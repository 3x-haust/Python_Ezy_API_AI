{"repo_info": {"repo_name": "ReasonFlux", "repo_owner": "Gen-Verse", "repo_url": "https://github.com/Gen-Verse/ReasonFlux"}}
{"type": "test_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/tests/test_prompt.py", "content": "import random\r\nfrom typing import List\r\n\r\nimport numpy as np\r\nimport pytest\r\n\r\nfrom lm_eval import tasks\r\nfrom lm_eval.tasks import TaskManager\r\nfrom lm_eval.utils import join_iters\r\n\r\n\r\nMMLU_ANATOMY_ZERO_SHOT = \"\"\"The following are multiple choice questions (with answers) about anatomy.\r\n\r\nA lesion causing compression of the facial nerve at the stylomastoid foramen will cause ipsilateral\r\nA. paralysis of the facial muscles.\r\nB. paralysis of the facial muscles and loss of taste.\r\nC. paralysis of the facial muscles, loss of taste and lacrimation.\r\nD. paralysis of the facial muscles, loss of taste, lacrimation and decreased salivation.\r\nAnswer:\"\"\"\r\n\r\nMMLU_ANATOMY_FIVE_SHOT = \"\"\"The following are multiple choice questions (with answers) about anatomy.\r\n\r\nWhat is the embryological origin of the hyoid bone?\r\nA. The first pharyngeal arch\r\nB. The first and second pharyngeal arches\r\nC. The second pharyngeal arch\r\nD. The second and third pharyngeal arches\r\nAnswer: D\r\n\r\nWhich of these branches of the trigeminal nerve contain somatic motor processes?\r\nA. The supraorbital nerve\r\nB. The infraorbital nerve\r\nC. The mental nerve\r\nD. None of the above\r\nAnswer: D\r\n\r\nThe pleura\r\nA. have no sensory innervation.\r\nB. are separated by a 2 mm space.\r\nC. extend into the neck.\r\nD. are composed of respiratory epithelium.\r\nAnswer: C\r\n\r\nIn Angle's Class II Div 2 occlusion there is\r\nA. excess overbite of the upper lateral incisors.\r\nB. negative overjet of the upper central incisors.\r\nC. excess overjet of the upper lateral incisors.\r\nD. excess overjet of the upper central incisors.\r\nAnswer: C\r\n\r\nWhich of the following is the body cavity that contains the pituitary gland?\r\nA. Abdominal\r\nB. Cranial\r\nC. Pleural\r\nD. Spinal\r\nAnswer: B\r\n\r\nA lesion causing compression of the facial nerve at the stylomastoid foramen will cause ipsilateral\r\nA. paralysis of the facial muscles.\r\nB. paralysis of the facial muscles and loss of taste.\r\nC. paralysis of the facial muscles, loss of taste and lacrimation.\r\nD. paralysis of the facial muscles, loss of taste, lacrimation and decreased salivation.\r\nAnswer:\"\"\"\r\n\r\n\r\n@pytest.mark.parametrize(\r\n    \"task_names,sets,num_fewshot,seed,num_examples,expected_prompt\",\r\n    [\r\n        ([\"mmlu_anatomy\"], \"test\", 0, 42, 1, MMLU_ANATOMY_ZERO_SHOT),\r\n        ([\"mmlu_anatomy\"], \"test\", 5, 42, 1, MMLU_ANATOMY_FIVE_SHOT),\r\n    ],\r\n)\r\ndef test_mmlu_prompt_rendering(\r\n    task_names: List[str],\r\n    sets: str,\r\n    num_fewshot: int,\r\n    seed: int,\r\n    num_examples: int,\r\n    expected_prompt: str,\r\n):\r\n    np.random.seed(seed)\r\n\r\n    task_manager = TaskManager()\r\n    task_dict = tasks.get_task_dict(task_names, task_manager)\r\n\r\n    for task_name, task in task_dict.items():\r\n        if isinstance(task, tuple):\r\n            _, task = task\r\n\r\n        rnd = random.Random()\r\n        rnd.seed(seed)\r\n\r\n        iters = []\r\n\r\n        for set in sets.split(\",\"):\r\n            docs = None\r\n            if set == \"train\" and task.has_training_docs():\r\n                docs = task.training_docs()\r\n            if set == \"val\" and task.has_validation_docs():\r\n                docs = task.validation_docs()\r\n            if set == \"test\" and task.has_test_docs():\r\n                docs = task.test_docs()\r\n            if docs is not None:\r\n                iters.append(docs)\r\n\r\n        if len(iters) == 0:\r\n            raise ValueError\r\n\r\n        docs = join_iters(iters)\r\n\r\n        for i, doc in (\r\n            zip(range(num_examples), docs) if num_examples > 0 else enumerate(docs)\r\n        ):\r\n            ctx = task.fewshot_context(\r\n                doc=doc,\r\n                num_fewshot=num_fewshot,\r\n            )\r\n\r\n            assert ctx == expected_prompt\r\n"}
{"type": "test_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/tests/models/test_neuralmagic.py", "content": "import pytest\r\n\r\nfrom lm_eval import evaluator\r\nfrom lm_eval.api.registry import get_model\r\n\r\n\r\nSPARSEML_MODELS_TASKS = [\r\n    # loglikelihood\r\n    (\"facebook/opt-125m\", \"lambada_openai\"),\r\n    # loglikelihood_rolling\r\n    (\"hf-internal-testing/tiny-random-gpt2\", \"wikitext\"),\r\n    # generate_until\r\n    (\"mgoin/tiny-random-llama-2-quant\", \"gsm8k\"),\r\n]\r\n\r\nDEEPSPARSE_MODELS_TASKS = [\r\n    # loglikelihood\r\n    (\"hf:mgoin/llama2.c-stories15M-quant-ds\", \"lambada_openai\"),\r\n    # loglikelihood_rolling (not supported yet)\r\n    # (\"hf:mgoin/llama2.c-stories15M-quant-ds\", \"wikitext\"),\r\n    # generate_until\r\n    (\"hf:mgoin/llama2.c-stories15M-quant-ds\", \"gsm8k\"),\r\n]\r\n\r\n\r\n@pytest.mark.skip(reason=\"test failing\")\r\n@pytest.mark.parametrize(\"model_id,task\", SPARSEML_MODELS_TASKS)\r\ndef test_sparseml_eval(model_id, task):\r\n    lm = get_model(\"sparseml\").create_from_arg_string(\r\n        f\"pretrained={model_id}\",\r\n        {\r\n            \"batch_size\": 1,\r\n            \"device\": \"cpu\",\r\n            \"dtype\": \"float32\",\r\n        },\r\n    )\r\n\r\n    limit = 5\r\n    evaluator.simple_evaluate(\r\n        model=lm,\r\n        tasks=[task],\r\n        num_fewshot=0,\r\n        limit=limit,\r\n    )\r\n\r\n\r\n@pytest.mark.parametrize(\"model_id,task\", DEEPSPARSE_MODELS_TASKS)\r\ndef test_deepsparse_eval(model_id, task):\r\n    lm = get_model(\"deepsparse\").create_from_arg_string(\r\n        f\"pretrained={model_id}\",\r\n        {\r\n            \"batch_size\": 1,\r\n        },\r\n    )\r\n\r\n    limit = 5\r\n    evaluator.simple_evaluate(\r\n        model=lm,\r\n        tasks=[task],\r\n        num_fewshot=0,\r\n        limit=limit,\r\n    )\r\n"}
{"type": "test_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/tests/models/test_huggingface.py", "content": "from __future__ import annotations\r\n\r\nimport os\r\nimport sys\r\nfrom pathlib import Path\r\n\r\nimport numpy as np\r\nimport tokenizers\r\nimport torch\r\nfrom packaging.version import parse as parse_version\r\n\r\nfrom lm_eval import tasks\r\nfrom lm_eval.api.instance import Instance\r\nfrom lm_eval.models.huggingface import HFLM\r\n\r\n\r\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\r\ntask_manager = tasks.TaskManager()\r\n\r\nTEST_STRING = \"foo bar\"\r\n\r\n\r\nclass Test_HFLM:\r\n    torch.use_deterministic_algorithms(True)\r\n    task_list = task_manager.load_task_or_group([\"arc_easy\", \"gsm8k\", \"wikitext\"])\r\n    version_minor = sys.version_info.minor\r\n    multiple_choice_task = task_list[\"arc_easy\"]  # type: ignore\r\n    multiple_choice_task.build_all_requests(limit=10, rank=0, world_size=1)\r\n    MULTIPLE_CH: list[Instance] = multiple_choice_task.instances\r\n    generate_until_task = task_list[\"gsm8k\"]  # type: ignore\r\n    generate_until_task._config.generation_kwargs[\"max_gen_toks\"] = 10\r\n    generate_until_task.set_fewshot_seed(1234)  # fewshot random generator seed\r\n    generate_until_task.build_all_requests(limit=10, rank=0, world_size=1)\r\n    generate_until: list[Instance] = generate_until_task.instances\r\n    rolling_task = task_list[\"wikitext\"]  # type: ignore\r\n    rolling_task.build_all_requests(limit=10, rank=0, world_size=1)\r\n    ROLLING: list[Instance] = rolling_task.instances\r\n\r\n    MULTIPLE_CH_RES = [\r\n        -41.902435302734375,\r\n        -42.939308166503906,\r\n        -33.914180755615234,\r\n        -37.07139205932617,\r\n        -22.95258331298828,\r\n        -20.342208862304688,\r\n        -14.818366050720215,\r\n        -27.942853927612305,\r\n        -15.80704116821289,\r\n        -15.936427116394043,\r\n        -13.052018165588379,\r\n        -18.04828453063965,\r\n        -13.345029830932617,\r\n        -13.366025924682617,\r\n        -12.127134323120117,\r\n        -11.872495651245117,\r\n        -47.10598373413086,\r\n        -47.76410675048828,\r\n        -36.4406852722168,\r\n        -50.0289421081543,\r\n        -16.72093963623047,\r\n        -18.535587310791016,\r\n        -26.46993637084961,\r\n        -20.355995178222656,\r\n        -17.757919311523438,\r\n        -21.80595588684082,\r\n        -33.1990852355957,\r\n        -39.28636932373047,\r\n        -14.759679794311523,\r\n        -16.753942489624023,\r\n        -11.486852645874023,\r\n        -15.42177677154541,\r\n        -13.15798282623291,\r\n        -15.887393951416016,\r\n        -15.28614616394043,\r\n        -12.339089393615723,\r\n        -44.59441375732422,\r\n        -55.40888214111328,\r\n        -52.70050811767578,\r\n        -56.25089645385742,\r\n    ]\r\n    generate_until_RES = [\r\n        \" The average of $2.50 each is $\",\r\n        \" A robe takes 2 bolts of blue fiber and half\",\r\n        \" $50,000 in repairs.\\n\\nQuestion\",\r\n        \" He runs 1 sprint 3 times a week.\",\r\n        \" They feed each of her chickens three cups of mixed\",\r\n        \" The price of the glasses is $5, but\",\r\n        \" The total percentage of students who said they like to\",\r\n        \" Carla is downloading a 200 GB file. Normally\",\r\n        \" John drives for 3 hours at a speed of 60\",\r\n        \" Eliza sells 4 tickets to 5 friends so she\",\r\n    ]\r\n    ROLLING_RES = [\r\n        -3603.6328125,\r\n        -19779.23974609375,\r\n        -8834.16455078125,\r\n        -27967.591796875,\r\n        -7636.794982910156,\r\n        -9491.93505859375,\r\n        -41043.4248046875,\r\n        -8397.689819335938,\r\n        -45969.47155761719,\r\n        -7158.90625,\r\n    ]\r\n    LM = HFLM(pretrained=\"EleutherAI/pythia-70m\", device=\"cpu\", dtype=\"float32\")\r\n\r\n    def test_logliklihood(self) -> None:\r\n        res = self.LM.loglikelihood(self.MULTIPLE_CH)\r\n        _RES, _res = self.MULTIPLE_CH_RES, [r[0] for r in res]\r\n        # log samples to CI\r\n        dir_path = Path(\"test_logs\")\r\n        dir_path.mkdir(parents=True, exist_ok=True)\r\n\r\n        file_path = dir_path / f\"outputs_log_{self.version_minor}.txt\"\r\n        file_path = file_path.resolve()\r\n        with open(file_path, \"w\", encoding=\"utf-8\") as f:\r\n            f.write(\"\\n\".join(str(x) for x in _res))\r\n        assert np.allclose(_res, _RES, atol=1e-2)\r\n        # check indices for Multiple Choice\r\n        argmax_RES, argmax_res = (\r\n            np.argmax(np.array(_RES).reshape(-1, 4), axis=1),\r\n            np.argmax(np.array(_res).reshape(-1, 4), axis=1),\r\n        )\r\n        assert (argmax_RES == argmax_res).all()\r\n\r\n    def test_generate_until(self) -> None:\r\n        res = self.LM.generate_until(self.generate_until)\r\n        assert res == self.generate_until_RES\r\n\r\n    def test_logliklihood_rolling(self) -> None:\r\n        res = self.LM.loglikelihood_rolling(self.ROLLING)\r\n        assert np.allclose(res, self.ROLLING_RES, atol=1e-1)\r\n\r\n    def test_toc_encode(self) -> None:\r\n        res = self.LM.tok_encode(TEST_STRING)\r\n        assert res == [12110, 2534]\r\n\r\n    def test_toc_decode(self) -> None:\r\n        res = self.LM.tok_decode([12110, 2534])\r\n        assert res == TEST_STRING\r\n\r\n    def test_batch_encode(self) -> None:\r\n        res = self.LM.tok_batch_encode([TEST_STRING, \"bar foo\"])[0].tolist()\r\n        assert res == [[12110, 2534], [2009, 17374]]\r\n\r\n    def test_model_generate(self) -> None:\r\n        context = self.LM.tok_batch_encode([TEST_STRING])[0]\r\n        res = self.LM._model_generate(context, max_length=10, stop=[\"\\n\\n\"])\r\n        res = self.LM.tok_decode(res[0])\r\n        if parse_version(tokenizers.__version__) >= parse_version(\"0.20.0\"):\r\n            assert res == \"foo bar\\n<bazhang> !info bar\"\r\n        else:\r\n            assert res == \"foo bar\\n<bazhang>!info bar\"\r\n"}
{"type": "test_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/tests/models/test_api.py", "content": "from unittest.mock import MagicMock, patch\r\n\r\nimport pytest\r\n\r\nfrom lm_eval.models.openai_completions import LocalCompletionsAPI\r\n\r\n\r\n@pytest.fixture\r\ndef api():\r\n    return LocalCompletionsAPI(\r\n        base_url=\"http://test-url.com\", tokenizer_backend=None, model=\"gpt-3.5-turbo\"\r\n    )\r\n\r\n\r\n@pytest.fixture\r\ndef api_tokenized():\r\n    return LocalCompletionsAPI(\r\n        base_url=\"http://test-url.com\",\r\n        model=\"EleutherAI/pythia-1b\",\r\n        tokenizer_backend=\"huggingface\",\r\n    )\r\n\r\n\r\ndef test_create_payload_generate(api):\r\n    messages = [\"Generate a story\"]\r\n    gen_kwargs = {\r\n        \"max_tokens\": 100,\r\n        \"temperature\": 0.7,\r\n        \"until\": [\"The End\"],\r\n        \"do_sample\": True,\r\n        \"seed\": 1234,\r\n    }\r\n    payload = api._create_payload(messages, generate=True, gen_kwargs=gen_kwargs)\r\n\r\n    assert payload == {\r\n        \"prompt\": [\"Generate a story\"],\r\n        \"model\": \"gpt-3.5-turbo\",\r\n        \"max_tokens\": 100,\r\n        \"temperature\": 0.7,\r\n        \"stop\": [\"The End\"],\r\n        \"seed\": 1234,\r\n    }\r\n\r\n\r\ndef test_create_payload_loglikelihood(api):\r\n    messages = [\"The capital of France is\"]\r\n    payload = api._create_payload(messages, generate=False, gen_kwargs=None)\r\n\r\n    assert payload == {\r\n        \"model\": \"gpt-3.5-turbo\",\r\n        \"prompt\": [\"The capital of France is\"],\r\n        \"max_tokens\": 1,\r\n        \"logprobs\": 1,\r\n        \"echo\": True,\r\n        \"temperature\": 0,\r\n        \"seed\": 1234,\r\n    }\r\n\r\n\r\n@pytest.mark.parametrize(\r\n    \"input_messages, generate, gen_kwargs, expected_payload\",\r\n    [\r\n        (\r\n            [\"Hello, how are\"],\r\n            True,\r\n            {\"max_gen_toks\": 100, \"temperature\": 0.7},\r\n            {\r\n                \"prompt\": \"Hello, how are\",\r\n                \"model\": \"gpt-3.5-turbo\",\r\n                \"max_tokens\": 100,\r\n                \"temperature\": 0.7,\r\n                \"stop\": [\"<|endoftext|>\"],\r\n                \"seed\": 1234,\r\n            },\r\n        ),\r\n        (\r\n            [\"Hello, how are\", \"you\"],\r\n            True,\r\n            {},\r\n            {\r\n                \"prompt\": \"Hello, how are\",\r\n                \"model\": \"gpt-3.5-turbo\",\r\n                \"max_tokens\": 256,\r\n                \"temperature\": 0,\r\n                \"stop\": [\"<|endoftext|>\"],\r\n                \"seed\": 1234,\r\n            },\r\n        ),\r\n    ],\r\n)\r\ndef test_model_generate_call_usage(\r\n    api, input_messages, generate, gen_kwargs, expected_payload\r\n):\r\n    with patch(\"requests.post\") as mock_post:\r\n        mock_response = MagicMock()\r\n        mock_response.json.return_value = {\"result\": \"success\"}\r\n        mock_post.return_value = mock_response\r\n\r\n        # Act\r\n        result = api.model_call(\r\n            input_messages, generate=generate, gen_kwargs=gen_kwargs\r\n        )\r\n\r\n        # Assert\r\n        mock_post.assert_called_once()\r\n        _, kwargs = mock_post.call_args\r\n        assert \"json\" in kwargs\r\n        assert kwargs[\"json\"] == expected_payload\r\n        assert result == {\"result\": \"success\"}\r\n\r\n\r\n@pytest.mark.parametrize(\r\n    \"input_messages, generate, gen_kwargs, expected_payload\",\r\n    [\r\n        (\r\n            [[1, 2, 3, 4, 5]],\r\n            False,\r\n            None,\r\n            {\r\n                \"model\": \"EleutherAI/pythia-1b\",\r\n                \"prompt\": [[1, 2, 3, 4, 5]],\r\n                \"max_tokens\": 1,\r\n                \"logprobs\": 1,\r\n                \"echo\": True,\r\n                \"seed\": 1234,\r\n                \"temperature\": 0,\r\n            },\r\n        ),\r\n    ],\r\n)\r\ndef test_model_tokenized_call_usage(\r\n    api_tokenized, input_messages, generate, gen_kwargs, expected_payload\r\n):\r\n    with patch(\"requests.post\") as mock_post:\r\n        mock_response = MagicMock()\r\n        mock_response.json.return_value = {\"result\": \"success\"}\r\n        mock_post.return_value = mock_response\r\n\r\n        # Act\r\n        result = api_tokenized.model_call(\r\n            input_messages, generate=generate, gen_kwargs=gen_kwargs\r\n        )\r\n\r\n        # Assert\r\n        mock_post.assert_called_once()\r\n        _, kwargs = mock_post.call_args\r\n        assert \"json\" in kwargs\r\n        assert kwargs[\"json\"] == expected_payload\r\n        assert result == {\"result\": \"success\"}\r\n"}
{"type": "test_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/tests/test_misc.py", "content": "import random\r\n\r\nimport pytest\r\n\r\nimport lm_eval.api.metrics as metrics\r\n\r\n\r\ndef test_bootstrapping():\r\n    random.seed(42)\r\n    arr = [random.random() for _ in range(1000)]\r\n    expected = metrics.mean_stderr(arr)\r\n    bootstrapped = metrics.bootstrap_stderr(metrics.mean, arr, iters=100000)\r\n\r\n    assert bootstrapped == pytest.approx(expected, abs=1e-4)\r\n"}
{"type": "test_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/tests/models/test_gguf.py", "content": "import hashlib\r\nimport json\r\nimport os\r\nimport pickle\r\nimport unittest\r\nfrom unittest.mock import patch\r\n\r\nfrom lm_eval.api.instance import Instance\r\nfrom lm_eval.models.gguf import GGUFLM\r\n\r\n\r\nbase_url = \"https://matthoffner-ggml-llm-api.hf.space\"\r\n\r\n\r\ndef gguf_completion_mock(base_url=None, **kwargs):\r\n    # Generate a hash from the parameters\r\n    hash_kwargs = {\"base_url\": base_url, **kwargs}\r\n    parameters_hash = hashlib.sha256(\r\n        json.dumps(hash_kwargs, sort_keys=True).encode(\"utf-8\")\r\n    ).hexdigest()\r\n\r\n    fname = f\"./tests/testdata/gguf_test_{parameters_hash}.pkl\"\r\n\r\n    if os.path.exists(fname):\r\n        with open(fname, \"rb\") as fh:\r\n            return pickle.load(fh)\r\n    else:\r\n        print(\"The file does not exist, attempting to write...\")\r\n        if \"stop\" in kwargs:\r\n            result = {\r\n                \"choices\": [\r\n                    {\r\n                        \"text\": f\"generated text until {kwargs['stop']}\",\r\n                        \"logprobs\": {\"token_logprobs\": [-1.2345], \"text_offset\": 0},\r\n                        \"finish_reason\": \"length\",\r\n                    }\r\n                ]\r\n            }\r\n        else:\r\n            # generated with # curl -X 'POST'   'http://localhost:8000/v1/completions'   -H 'accept: application/json'   -H 'Content-Type: application/json'   -d '{\"prompt\": \"string\", \"logprobs\": 10, \"temperature\": 0.0, \"max_tokens\": 1, \"echo\": true}'\r\n            result = {\r\n                \"id\": \"cmpl-4023976b-bc6a-43b0-a5a9-629f4216c7f3\",\r\n                \"object\": \"text_completion\",\r\n                \"created\": 1700511361,\r\n                \"model\": \"../llama-2-7b.Q8_0.gguf\",\r\n                \"choices\": [\r\n                    {\r\n                        \"text\": \"string(\",\r\n                        \"index\": 0,\r\n                        \"logprobs\": {\r\n                            \"text_offset\": [0, 7],\r\n                            \"token_logprobs\": [None, -1.033263319857306],\r\n                            \"tokens\": [\" string\", \"(\"],\r\n                            \"top_logprobs\": [\r\n                                None,\r\n                                {\r\n                                    \"(\": -1.033263319857306,\r\n                                    \"[]\": -2.6530743779017394,\r\n                                    \".\": -3.0377145947291324,\r\n                                    \"\\n\": -3.0399156750513976,\r\n                                    \"_\": -3.510376089937872,\r\n                                    \" =\": -3.6957918347193663,\r\n                                    \",\": -3.9309459866358702,\r\n                                    \" of\": -4.2834550083949035,\r\n                                    '(\"': -4.322762841112799,\r\n                                    \"()\": -4.426229113466925,\r\n                                },\r\n                            ],\r\n                        },\r\n                        \"finish_reason\": \"length\",\r\n                    }\r\n                ],\r\n                \"usage\": {\r\n                    \"prompt_tokens\": 2,\r\n                    \"completion_tokens\": 1,\r\n                    \"total_tokens\": 3,\r\n                },\r\n            }\r\n\r\n        try:\r\n            os.makedirs(os.path.dirname(fname), exist_ok=True)\r\n            print(\"Writing file at\", fname)\r\n            with open(fname, \"wb\") as fh:\r\n                pickle.dump(result, fh)\r\n            print(\"File written successfully\")\r\n        except Exception as e:\r\n            print(\"File writing failed:\", e)\r\n\r\n        return result\r\n\r\n\r\nclass GGUFLMTest(unittest.TestCase):\r\n    @patch(\r\n        \"lm_eval.models.gguf.GGUFLM.gguf_completion\", side_effect=gguf_completion_mock\r\n    )\r\n    def test_loglikelihood(self, gguf_completion_mock):\r\n        lm = GGUFLM(base_url)\r\n\r\n        # Test loglikelihood\r\n        requests = [\r\n            Instance(\r\n                request_type=\"loglikelihood\",\r\n                doc=args,\r\n                arguments=args,\r\n                idx=i,\r\n            )\r\n            for i, args in enumerate([(\"str\", \"ing\"), (\"str\", \"ing\")])\r\n        ]\r\n        res = lm.loglikelihood(requests)\r\n\r\n        # Assert the loglikelihood response is correct\r\n        expected_res = [(logprob, True) for logprob in [0, 0]]\r\n        self.assertEqual(res, expected_res)\r\n\r\n    @patch(\r\n        \"lm_eval.models.gguf.GGUFLM.gguf_completion\", side_effect=gguf_completion_mock\r\n    )\r\n    def test_generate_until(self, gguf_completion_mock):\r\n        lm = GGUFLM(base_url)\r\n\r\n        # Test generate_until\r\n        requests = [\r\n            Instance(\r\n                request_type=\"generate_until\",\r\n                doc={\"input\": doc},\r\n                arguments=(doc, {\"until\": stop}),\r\n                idx=i,\r\n            )\r\n            for i, (doc, stop) in enumerate([(\"input1\", \"stop1\"), (\"input2\", \"stop2\")])\r\n        ]\r\n\r\n        res = lm.generate_until(requests)\r\n\r\n        # Assert the generate_until response is correct\r\n        expected_res = [\"generated text until stop1\", \"generated text until stop2\"]\r\n        self.assertEqual(res, expected_res)\r\n\r\n    # @patch('lm_eval.models.gguf.GGUFLM.gguf_completion', side_effect=gguf_completion_mock)\r\n    # def test_loglikelihood_rolling(self, gguf_completion_mock):\r\n    #     lm = GGUFLM(base_url)\r\n\r\n    #     # Test loglikelihood_rolling\r\n    #     requests = [\"input1\", \"input2\"]\r\n    #     res = lm.loglikelihood_rolling(requests)\r\n\r\n    #     # Assert the loglikelihood_rolling response is correct\r\n    #     expected_res = [(-1.2345, True), (-1.2345, True)]\r\n    #     self.assertEqual(res, expected_res)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    unittest.main()\r\n"}
{"type": "test_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/tests/test_include_path.py", "content": "import os\r\n\r\nimport pytest\r\n\r\nimport lm_eval.api as api\r\nimport lm_eval.evaluator as evaluator\r\nfrom lm_eval import tasks\r\n\r\n\r\n@pytest.mark.parametrize(\r\n    \"limit,model,model_args\",\r\n    [\r\n        (\r\n            10,\r\n            \"hf\",\r\n            \"pretrained=EleutherAI/pythia-160m,dtype=float32,device=cpu\",\r\n        ),\r\n    ],\r\n)\r\ndef test_include_correctness(limit: int, model: str, model_args: str):\r\n    task_name = [\"arc_easy\"]\r\n\r\n    task_manager = tasks.TaskManager()\r\n    task_dict = tasks.get_task_dict(task_name, task_manager)\r\n\r\n    e1 = evaluator.simple_evaluate(\r\n        model=model,\r\n        tasks=task_name,\r\n        limit=limit,\r\n        model_args=model_args,\r\n    )\r\n    assert e1 is not None\r\n\r\n    # run with evaluate() and \"arc_easy\" test config (included from ./testconfigs path)\r\n    lm = api.registry.get_model(model).create_from_arg_string(\r\n        model_args,\r\n        {\r\n            \"batch_size\": None,\r\n            \"max_batch_size\": None,\r\n            \"device\": None,\r\n        },\r\n    )\r\n\r\n    task_name = [\"arc_easy\"]\r\n\r\n    task_manager = tasks.TaskManager(\r\n        include_path=os.path.dirname(os.path.abspath(__file__)) + \"/testconfigs\",\r\n        include_defaults=False,\r\n    )\r\n    task_dict = tasks.get_task_dict(task_name, task_manager)\r\n\r\n    e2 = evaluator.evaluate(\r\n        lm=lm,\r\n        task_dict=task_dict,\r\n        limit=limit,\r\n    )\r\n\r\n    assert e2 is not None\r\n    # check that caching is working\r\n\r\n    def r(x):\r\n        return x[\"results\"][\"arc_easy\"]\r\n\r\n    assert all(\r\n        x == y\r\n        for x, y in zip([y for _, y in r(e1).items()], [y for _, y in r(e2).items()])\r\n    )\r\n\r\n\r\n# test that setting include_defaults = False works as expected and that include_path works\r\ndef test_no_include_defaults():\r\n    task_name = [\"arc_easy\"]\r\n\r\n    task_manager = tasks.TaskManager(\r\n        include_path=os.path.dirname(os.path.abspath(__file__)) + \"/testconfigs\",\r\n        include_defaults=False,\r\n    )\r\n    # should succeed, because we've included an 'arc_easy' task from this dir\r\n    task_dict = tasks.get_task_dict(task_name, task_manager)\r\n\r\n    # should fail, since ./testconfigs has no arc_challenge task\r\n    task_name = [\"arc_challenge\"]\r\n    with pytest.raises(KeyError):\r\n        task_dict = tasks.get_task_dict(task_name, task_manager)  # noqa: F841\r\n\r\n\r\n# test that include_path containing a task shadowing another task's name fails\r\n# def test_shadowed_name_fails():\r\n\r\n#     task_name = [\"arc_easy\"]\r\n\r\n#     task_manager = tasks.TaskManager(include_path=os.path.dirname(os.path.abspath(__file__)) + \"/testconfigs\")\r\n#     task_dict = tasks.get_task_dict(task_name, task_manager)\r\n"}
{"type": "test_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/tests/test_requests_caching.py", "content": "import importlib\r\nimport os\r\nimport sys\r\nfrom datetime import datetime\r\nfrom typing import List, Optional, Tuple\r\n\r\nimport pytest\r\nimport torch\r\n\r\nfrom lm_eval.caching.cache import PATH\r\n\r\n\r\nMODULE_DIR = os.path.dirname(os.path.realpath(__file__))\r\n\r\n# NOTE the script this loads uses simple evaluate\r\n# TODO potentially test both the helper script and the normal script\r\nsys.path.append(f\"{MODULE_DIR}/../scripts\")\r\nmodel_loader = importlib.import_module(\"requests_caching\")\r\nrun_model_for_task_caching = model_loader.run_model_for_task_caching\r\n\r\nos.environ[\"HF_DATASETS_TRUST_REMOTE_CODE\"] = \"1\"\r\nDEFAULT_TASKS = [\"lambada_openai\", \"sciq\"]\r\n\r\n\r\n@pytest.fixture(autouse=True)\r\ndef setup_and_teardown():\r\n    # Setup\r\n    torch.use_deterministic_algorithms(False)\r\n    clear_cache()\r\n    # Yields control back to the test function\r\n    yield\r\n    # Cleanup here\r\n\r\n\r\ndef clear_cache():\r\n    if os.path.exists(PATH):\r\n        cache_files = os.listdir(PATH)\r\n        for file in cache_files:\r\n            file_path = f\"{PATH}/{file}\"\r\n            os.unlink(file_path)\r\n\r\n\r\n# leaving tasks here to allow for the option to select specific task files\r\ndef get_cache_files(tasks: Optional[List[str]] = None) -> Tuple[List[str], List[str]]:\r\n    cache_files = os.listdir(PATH)\r\n\r\n    file_task_names = []\r\n\r\n    for file in cache_files:\r\n        file_without_prefix = file.split(\"-\")[1]\r\n        file_without_prefix_and_suffix = file_without_prefix.split(\".\")[0]\r\n        file_task_names.extend([file_without_prefix_and_suffix])\r\n\r\n    return cache_files, file_task_names\r\n\r\n\r\ndef assert_created(tasks: List[str], file_task_names: List[str]):\r\n    tasks.sort()\r\n    file_task_names.sort()\r\n\r\n    assert tasks == file_task_names\r\n\r\n\r\n@pytest.mark.parametrize(\"tasks\", [DEFAULT_TASKS])\r\ndef requests_caching_true(tasks: List[str]):\r\n    run_model_for_task_caching(tasks=tasks, cache_requests=\"true\")\r\n\r\n    cache_files, file_task_names = get_cache_files()\r\n    print(file_task_names)\r\n    assert_created(tasks=tasks, file_task_names=file_task_names)\r\n\r\n\r\n@pytest.mark.parametrize(\"tasks\", [DEFAULT_TASKS])\r\ndef requests_caching_refresh(tasks: List[str]):\r\n    run_model_for_task_caching(tasks=tasks, cache_requests=\"true\")\r\n\r\n    timestamp_before_test = datetime.now().timestamp()\r\n\r\n    run_model_for_task_caching(tasks=tasks, cache_requests=\"refresh\")\r\n\r\n    cache_files, file_task_names = get_cache_files()\r\n\r\n    for file in cache_files:\r\n        modification_time = os.path.getmtime(f\"{PATH}/{file}\")\r\n        assert modification_time > timestamp_before_test\r\n\r\n    tasks.sort()\r\n    file_task_names.sort()\r\n\r\n    assert tasks == file_task_names\r\n\r\n\r\n@pytest.mark.parametrize(\"tasks\", [DEFAULT_TASKS])\r\ndef requests_caching_delete(tasks: List[str]):\r\n    # populate the data first, rerun this test within this test for additional confidence\r\n    # test_requests_caching_true(tasks=tasks)\r\n\r\n    run_model_for_task_caching(tasks=tasks, cache_requests=\"delete\")\r\n\r\n    cache_files, file_task_names = get_cache_files()\r\n\r\n    assert len(cache_files) == 0\r\n\r\n\r\n# useful for locally running tests through the debugger\r\nif __name__ == \"__main__\":\r\n\r\n    def run_tests():\r\n        tests = [\r\n            # test_requests_caching_true,\r\n            # test_requests_caching_refresh,\r\n            # test_requests_caching_delete,\r\n        ]\r\n        # Lookups of global names within a loop is inefficient, so copy to a local variable outside of the loop first\r\n        default_tasks = DEFAULT_TASKS\r\n        for test_func in tests:\r\n            clear_cache()\r\n            test_func(tasks=default_tasks)\r\n\r\n        print(\"Tests pass\")\r\n\r\n    run_tests()\r\n"}
{"type": "test_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/tests/test_cli.py", "content": "import argparse\r\n\r\nimport pytest\r\n\r\nimport lm_eval.__main__\r\n\r\n\r\ndef test_cli_parse_error():\r\n    \"\"\"\r\n    Assert error raised if cli args argument doesn't have type\r\n    \"\"\"\r\n    with pytest.raises(ValueError):\r\n        parser = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter)\r\n        parser.add_argument(\r\n            \"--model\", \"-m\", type=str, default=\"hf\", help=\"Name of model e.g. `hf`\"\r\n        )\r\n        parser.add_argument(\r\n            \"--tasks\",\r\n            \"-t\",\r\n            default=None,\r\n            metavar=\"task1,task2\",\r\n            help=\"To get full list of tasks, use the command lm-eval --tasks list\",\r\n        )\r\n        lm_eval.__main__.check_argument_types(parser)\r\n\r\n\r\ndef test_cli_parse_no_error():\r\n    \"\"\"\r\n    Assert typed arguments are parsed correctly\r\n    \"\"\"\r\n    parser = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter)\r\n    parser.add_argument(\r\n        \"--model\", \"-m\", type=str, default=\"hf\", help=\"Name of model e.g. `hf`\"\r\n    )\r\n    parser.add_argument(\r\n        \"--tasks\",\r\n        \"-t\",\r\n        type=str,\r\n        default=None,\r\n        metavar=\"task1,task2\",\r\n        help=\"To get full list of tasks, use the command lm-eval --tasks list\",\r\n    )\r\n    lm_eval.__main__.check_argument_types(parser)\r\n"}
{"type": "test_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/tests/utils.py", "content": "import os\r\nfrom typing import List, Union\r\n\r\nfrom lm_eval.utils import load_yaml_config\r\n\r\n\r\n# {{{CI}}}\r\n# This is the path where the output for the changed files for the tasks folder is stored\r\n# FILE_PATH = file_path = \".github/outputs/tasks_all_changed_and_modified_files.txt\"\r\n\r\n\r\n# reads a text file and returns a list of words\r\n# used to read the output of the changed txt from tj-actions/changed-files\r\ndef load_changed_files(file_path: str) -> List[str]:\r\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\r\n        content = f.read()\r\n        words_list = list(content.split())\r\n    return words_list\r\n\r\n\r\n# checks the txt file for list of changed files.\r\n# if file ends with .yaml then check yaml and load the config.\r\n# if the config task is a string, it's a task config.\r\n# if the config task is a list, it's a group config.\r\ndef parser(full_path: List[str]) -> List[str]:\r\n    _output = set()\r\n    for x in full_path:\r\n        if x.endswith(\".yaml\") and os.path.exists(x):\r\n            config = load_yaml_config(x, mode=\"simple\")\r\n            if isinstance(config[\"task\"], str):\r\n                _output.add(config[\"task\"])\r\n            elif isinstance(config[\"task\"], list):\r\n                _output.add(config[\"group\"])\r\n    return list(_output)\r\n\r\n\r\ndef new_tasks() -> Union[List[str], None]:\r\n    FILENAME = \".github/outputs/tasks_all_changed_and_modified_files.txt\"\r\n    if os.path.exists(FILENAME):\r\n        # If tasks folder has changed then we get the list of files from FILENAME\r\n        # and parse the yaml files to get the task names.\r\n        return parser(load_changed_files(FILENAME))\r\n    if os.getenv(\"API\") is not None:\r\n        # Or if API has changed then we set the ENV variable API to True\r\n        # and run  given tasks.\r\n        return [\"arc_easy\", \"hellaswag\", \"piqa\", \"wikitext\"]\r\n    # if both not true just do arc_easy\r\n    return None\r\n"}
{"type": "test_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/tests/__init__.py", "content": ""}
{"type": "test_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/tests/test_evaluator.py", "content": "import os\r\nimport re\r\nfrom typing import List\r\n\r\nimport pytest\r\n\r\nimport lm_eval.api as api\r\nimport lm_eval.evaluator as evaluator\r\nfrom lm_eval import tasks\r\nfrom lm_eval.utils import make_table\r\n\r\n\r\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\r\n# TODO: more fine grained unit tests rather than this big honking integration\r\n# test once we break evaluator into smaller, more manageable pieces\r\n\r\n\r\n@pytest.mark.parametrize(\r\n    \"task_name,limit,model,model_args,bootstrap_iters\",\r\n    [\r\n        (\r\n            [\"arc_easy\"],\r\n            10,\r\n            \"hf\",\r\n            \"pretrained=EleutherAI/pythia-160m,dtype=float32,device=cpu\",\r\n            0,\r\n        ),\r\n        (\r\n            [\"mmlu_abstract_algebra\"],\r\n            None,\r\n            \"hf\",\r\n            \"pretrained=EleutherAI/pythia-160m,dtype=float32,device=cpu\",\r\n            10000,\r\n        ),\r\n    ],\r\n    ids=lambda d: f\"{d}\",\r\n)\r\ndef test_evaluator(\r\n    task_name: List[str], limit: int, model: str, model_args: str, bootstrap_iters: int\r\n):\r\n    e1 = evaluator.simple_evaluate(\r\n        model=model,\r\n        tasks=task_name,\r\n        limit=limit,\r\n        model_args=model_args,\r\n        bootstrap_iters=bootstrap_iters,\r\n    )\r\n    assert e1 is not None\r\n\r\n    lm = api.registry.get_model(model).create_from_arg_string(\r\n        model_args,\r\n        {\r\n            \"batch_size\": None,\r\n            \"max_batch_size\": None,\r\n            \"device\": None,\r\n        },\r\n    )\r\n    task_manager = tasks.TaskManager()\r\n    task_dict = tasks.get_task_dict(task_name, task_manager)\r\n\r\n    e2 = evaluator.evaluate(\r\n        lm=lm,\r\n        task_dict=task_dict,\r\n        limit=limit,\r\n        bootstrap_iters=bootstrap_iters,\r\n    )\r\n\r\n    assert e2 is not None\r\n    # check that caching is working\r\n\r\n    def r(x):\r\n        if \"arc_easy\" in x[\"results\"]:\r\n            return x[\"results\"][\"arc_easy\"]\r\n        else:\r\n            return x[\"results\"][\"mmlu_abstract_algebra\"]\r\n\r\n    assert all(\r\n        x == y\r\n        for x, y in zip([y for _, y in r(e1).items()], [y for _, y in r(e2).items()])\r\n    )\r\n\r\n\r\n@pytest.mark.parametrize(\r\n    \"task_name,limit,model,model_args\",\r\n    [\r\n        (\r\n            [\"ai2_arc\"],\r\n            10,\r\n            \"hf\",\r\n            \"pretrained=EleutherAI/pythia-14m,dtype=float32,device=cpu\",\r\n        ),\r\n        (\r\n            [\"mmlu_stem\"],\r\n            10,\r\n            \"hf\",\r\n            \"pretrained=EleutherAI/pythia-14m,dtype=float32,device=cpu\",\r\n        ),\r\n        (\r\n            [\"lambada_openai\"],\r\n            10,\r\n            \"hf\",\r\n            \"pretrained=EleutherAI/pythia-14m,dtype=float32,device=cpu\",\r\n        ),\r\n        (\r\n            [\"wikitext\"],\r\n            10,\r\n            \"hf\",\r\n            \"pretrained=EleutherAI/pythia-14m,dtype=float32,device=cpu\",\r\n        ),\r\n    ],\r\n    ids=lambda d: f\"{d}\",\r\n)\r\ndef test_printed_results(task_name: List[str], limit: int, model: str, model_args: str):\r\n    results = evaluator.simple_evaluate(\r\n        model=model,\r\n        tasks=task_name,\r\n        limit=limit,\r\n        model_args=model_args,\r\n        bootstrap_iters=0,\r\n        random_seed=0,\r\n        numpy_random_seed=0,\r\n        torch_random_seed=0,\r\n        fewshot_random_seed=0,\r\n    )\r\n\r\n    filename = \"_\".join(\r\n        (\r\n            \"-\".join(task_name),\r\n            str(limit),\r\n            str(model),\r\n            re.sub(r\"[^a-zA-Z0-9_\\-\\.]\", \"-\", model_args),\r\n        )\r\n    )\r\n    filepath = f\"./tests/testdata/{filename}.txt\"\r\n    with open(filepath, \"r\") as f:\r\n        t1 = f.read().strip()\r\n\r\n    t2 = make_table(results).strip()\r\n\r\n    t1_lines, t2_lines = t1.splitlines(), t2.splitlines()\r\n    assert len(t1_lines) == len(t2_lines)\r\n    for t1_line, t2_line in zip(t1_lines, t2_lines):\r\n        t1_items, t2_items = t1_line.split(\"|\"), t2_line.split(\"|\")\r\n        assert len(t1_items) == len(t2_items)\r\n        for t1_item, t2_item in zip(t1_items, t2_items):\r\n            try:\r\n                t1_item = float(t1_item)\r\n                t2_item = float(t2_item)\r\n                assert abs(t1_item - t2_item) < 0.3\r\n            except ValueError:\r\n                assert t1_item == t2_item\r\n"}
{"type": "test_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/tests/models/test_openvino.py", "content": "import random\r\nimport tempfile\r\nfrom pathlib import Path\r\n\r\nimport pytest\r\nfrom optimum.intel import OVModelForCausalLM\r\nfrom transformers import AutoTokenizer\r\n\r\nfrom lm_eval import evaluator\r\nfrom lm_eval.api.registry import get_model\r\n\r\n\r\nSUPPORTED_ARCHITECTURES_TASKS = {\r\n    \"facebook/opt-125m\": \"lambada_openai\",\r\n    \"hf-internal-testing/tiny-random-gpt2\": \"wikitext\",\r\n}\r\n\r\n\r\n@pytest.mark.parametrize(\"model_id,task\", SUPPORTED_ARCHITECTURES_TASKS.items())\r\ndef test_evaluator(model_id, task):\r\n    with tempfile.TemporaryDirectory() as tmpdirname:\r\n        model = OVModelForCausalLM.from_pretrained(\r\n            model_id, export=True, use_cache=True\r\n        )\r\n        model.save_pretrained(tmpdirname)\r\n        tokenizer = AutoTokenizer.from_pretrained(model_id)\r\n        tokenizer.save_pretrained(tmpdirname)\r\n\r\n        lm = get_model(\"openvino\").create_from_arg_string(\r\n            f\"pretrained={tmpdirname}\",\r\n            {\r\n                \"batch_size\": 1,\r\n                \"device\": \"cpu\",\r\n            },\r\n        )\r\n\r\n        def ll_fn(reqs):\r\n            for ctx, cont in [req.args for req in reqs]:\r\n                if len(ctx) == 0:\r\n                    continue\r\n                # space convention\r\n                assert ctx[-1] != \" \"\r\n                assert cont[0] == \" \" or ctx[-1] == \"\\n\"\r\n\r\n            res = []\r\n\r\n            random.seed(42)\r\n            for _ in reqs:\r\n                res.extend([(-random.random(), False)])\r\n\r\n            return res\r\n\r\n        def ll_perp_fn(reqs):\r\n            for (string,) in [req.args for req in reqs]:\r\n                assert isinstance(string, str)\r\n\r\n            res = []\r\n            random.seed(42)\r\n            for _ in reqs:\r\n                res.extend([-random.random()])\r\n\r\n            return res\r\n\r\n        lm.loglikelihood = ll_fn\r\n        lm.loglikelihood_rolling = ll_perp_fn\r\n\r\n        limit = 10\r\n        evaluator.simple_evaluate(\r\n            model=lm,\r\n            tasks=[task],\r\n            num_fewshot=0,\r\n            limit=limit,\r\n            bootstrap_iters=10,\r\n        )\r\n\r\n\r\ndef test_ov_config():\r\n    \"\"\"Test that if specified, a custom OpenVINO config is loaded correctly\"\"\"\r\n    model_id = \"hf-internal-testing/tiny-random-gpt2\"\r\n    with tempfile.TemporaryDirectory() as tmpdirname:\r\n        config_file = str(Path(tmpdirname) / \"ov_config.json\")\r\n        with open(Path(config_file), \"w\", encoding=\"utf-8\") as f:\r\n            f.write('{\"DYNAMIC_QUANTIZATION_GROUP_SIZE\" : \"32\"}')\r\n        lm = get_model(\"openvino\").create_from_arg_string(\r\n            f\"pretrained={model_id},ov_config={config_file}\"\r\n        )\r\n    assert (\r\n        lm.model.request.get_compiled_model().get_property(\r\n            \"DYNAMIC_QUANTIZATION_GROUP_SIZE\"\r\n        )\r\n        == 32\r\n    )\r\n"}
{"type": "test_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/tests/test_task_manager.py", "content": "import tempfile\r\nfrom pathlib import Path\r\n\r\nimport pytest\r\n\r\nfrom lm_eval.tasks import TaskManager\r\n\r\n\r\n@pytest.fixture(scope=\"module\")\r\ndef custom_task_name():\r\n    return \"zzz_my_python_task\"\r\n\r\n\r\n@pytest.fixture(scope=\"module\")\r\ndef custom_task_tag():\r\n    return \"zzz-tag\"\r\n\r\n\r\n@pytest.fixture(scope=\"module\")\r\ndef task_yaml(pytestconfig, custom_task_name, custom_task_tag):\r\n    yield f\"\"\"include: {pytestconfig.rootpath}/lm_eval/tasks/hellaswag/hellaswag.yaml\r\ntask: {custom_task_name}\r\nclass: !function {custom_task_name}.MockPythonTask\r\ntag:\r\n  - {custom_task_tag}\r\n\"\"\"\r\n\r\n\r\n@pytest.fixture(scope=\"module\")\r\ndef task_code():\r\n    return \"\"\"\r\nfrom lm_eval.tasks import ConfigurableTask\r\n\r\nclass MockPythonTask(ConfigurableTask):\r\n\r\n    def __init__(\r\n        self,\r\n        data_dir=None,\r\n        cache_dir=None,\r\n        download_mode=None,\r\n        config=None,\r\n    ) -> None:\r\n        config.pop(\"class\")\r\n        super().__init__(data_dir, cache_dir, download_mode, config)\r\n\"\"\"\r\n\r\n\r\n@pytest.fixture(scope=\"module\")\r\ndef custom_task_files_dir(task_yaml, task_code, custom_task_name):\r\n    with tempfile.TemporaryDirectory() as temp_dir:\r\n        yaml_path = Path(temp_dir) / f\"{custom_task_name}.yaml\"\r\n        with open(yaml_path, \"w\") as f:\r\n            f.write(task_yaml)\r\n        pysource_path = Path(temp_dir) / f\"{custom_task_name}.py\"\r\n        with open(pysource_path, \"w\") as f:\r\n            f.write(task_code)\r\n        yield temp_dir\r\n\r\n\r\ndef test_python_task_inclusion(\r\n    custom_task_files_dir: Path, custom_task_name: str, custom_task_tag: str\r\n):\r\n    task_manager = TaskManager(\r\n        verbosity=\"INFO\", include_path=str(custom_task_files_dir)\r\n    )\r\n    # check if python tasks enters the global task_index\r\n    assert custom_task_name in task_manager.task_index\r\n    # check if subtask is present\r\n    assert custom_task_name in task_manager.all_subtasks\r\n    # check if tag is present\r\n    assert custom_task_tag in task_manager.all_tags\r\n    # check if it can be loaded by tag (custom_task_tag)\r\n    assert custom_task_name in task_manager.load_task_or_group(custom_task_tag)\r\n"}
{"type": "test_file", "path": "train/LLaMA-Factory/scripts/api_example/test_image.py", "content": "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\n\nfrom openai import OpenAI\nfrom transformers.utils.versions import require_version\n\n\nrequire_version(\"openai>=1.5.0\", \"To fix: pip install openai>=1.5.0\")\n\n\ndef main():\n    client = OpenAI(\n        api_key=\"{}\".format(os.environ.get(\"API_KEY\", \"0\")),\n        base_url=\"http://localhost:{}/v1\".format(os.environ.get(\"API_PORT\", 8000)),\n    )\n    messages = []\n    messages.append(\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"Output the color and number of each box.\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/boxes.png\"},\n                },\n            ],\n        }\n    )\n    result = client.chat.completions.create(messages=messages, model=\"test\")\n    messages.append(result.choices[0].message)\n    print(\"Round 1:\", result.choices[0].message.content)\n    # The image shows a pyramid of colored blocks with numbers on them. Here are the colors and numbers of ...\n    messages.append(\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"What kind of flower is this?\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/flowers.jpg\"},\n                },\n            ],\n        }\n    )\n    result = client.chat.completions.create(messages=messages, model=\"test\")\n    messages.append(result.choices[0].message)\n    print(\"Round 2:\", result.choices[0].message.content)\n    # The image shows a cluster of forget-me-not flowers. Forget-me-nots are small ...\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "test_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/tests/test_tasks.py", "content": "import os\r\nfrom itertools import islice\r\n\r\nimport pytest\r\n\r\nimport lm_eval.tasks as tasks\r\nfrom lm_eval.api.task import ConfigurableTask\r\nfrom lm_eval.evaluator_utils import get_task_list\r\n\r\nfrom .utils import new_tasks\r\n\r\n\r\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\r\ntask_manager = tasks.TaskManager()\r\n# Default Task\r\nTASKS = [\"arc_easy\"]\r\n\r\n\r\ndef task_class():\r\n    global TASKS\r\n    # CI: new_tasks checks if any modifications have been made\r\n    task_classes = new_tasks()\r\n    # Check if task_classes is empty\r\n    task_classes = task_classes if task_classes else TASKS\r\n    res = tasks.get_task_dict(task_classes, task_manager)\r\n    res = [x.task for x in get_task_list(res)]\r\n\r\n    return res\r\n\r\n\r\n@pytest.fixture()\r\ndef limit() -> int:\r\n    return 10\r\n\r\n\r\n# Tests\r\n@pytest.mark.parametrize(\"task_class\", task_class(), ids=lambda x: f\"{x.config.task}\")\r\nclass TestNewTasks:\r\n    def test_download(self, task_class: ConfigurableTask):\r\n        task_class.download()\r\n        assert task_class.dataset is not None\r\n\r\n    def test_has_training_docs(self, task_class: ConfigurableTask):\r\n        assert task_class.has_training_docs() in [True, False]\r\n\r\n    def test_check_training_docs(self, task_class: ConfigurableTask):\r\n        if task_class.has_training_docs():\r\n            assert task_class._config[\"training_split\"] is not None\r\n\r\n    def test_has_validation_docs(self, task_class):\r\n        assert task_class.has_validation_docs() in [True, False]\r\n\r\n    def test_check_validation_docs(self, task_class):\r\n        if task_class.has_validation_docs():\r\n            assert task_class._config[\"validation_split\"] is not None\r\n\r\n    def test_has_test_docs(self, task_class):\r\n        assert task_class.has_test_docs() in [True, False]\r\n\r\n    def test_check_test_docs(self, task_class):\r\n        task = task_class\r\n        if task.has_test_docs():\r\n            assert task._config[\"test_split\"] is not None\r\n\r\n    def test_should_decontaminate(self, task_class):\r\n        task = task_class\r\n        assert task.should_decontaminate() in [True, False]\r\n        if task.should_decontaminate():\r\n            assert task._config[\"doc_to_decontamination_query\"] is not None\r\n\r\n    def test_doc_to_text(self, task_class, limit):\r\n        task = task_class\r\n        arr = (\r\n            list(islice(task.test_docs(), limit))\r\n            if task.has_test_docs()\r\n            else list(islice(task.validation_docs(), limit))\r\n        )\r\n        _array = [task.doc_to_text(doc) for doc in arr]\r\n        # space convention; allow txt to have length 0 for perplexity-like tasks since the model tacks an <|endoftext|> on\r\n        assert all(\r\n            isinstance(x, str) and (x[-1] != \" \" if len(x) != 0 else True)\r\n            for x in _array\r\n        )\r\n\r\n    def test_create_choices(self, task_class, limit):\r\n        task = task_class\r\n        arr = (\r\n            list(islice(task.test_docs(), limit))\r\n            if task.has_test_docs()\r\n            else list(islice(task.validation_docs(), limit))\r\n        )\r\n        if \"multiple_choice\" in task._config.output_type:\r\n            _array = [task.doc_to_choice(doc) for doc in arr]\r\n            assert all(isinstance(x, list) for x in _array)\r\n            assert all(isinstance(x[0], str) for x in _array)\r\n\r\n    def test_doc_to_target(self, task_class, limit):\r\n        task = task_class\r\n        arr = (\r\n            list(islice(task.test_docs(), limit))\r\n            if task.has_test_docs()\r\n            else list(islice(task.validation_docs(), limit))\r\n        )\r\n        _array_target = [task.doc_to_target(doc) for doc in arr]\r\n        if task._config.output_type == \"multiple_choice\":\r\n            # TODO<baber>: label can be string or int; add better test conditions\r\n            assert all(\r\n                (isinstance(label, int) or isinstance(label, str))\r\n                for label in _array_target\r\n            )\r\n\r\n    def test_build_all_requests(self, task_class, limit):\r\n        task_class.build_all_requests(rank=1, limit=limit, world_size=1)\r\n        assert task_class.instances is not None\r\n\r\n    # ToDO: Add proper testing\r\n    def test_construct_requests(self, task_class, limit):\r\n        task = task_class\r\n        arr = (\r\n            list(islice(task.test_docs(), limit))\r\n            if task.has_test_docs()\r\n            else list(islice(task.validation_docs(), limit))\r\n        )\r\n        requests = [task.construct_requests(doc, task.doc_to_text(doc)) for doc in arr]\r\n        assert len(requests) == limit if limit else True\r\n"}
{"type": "test_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/tests/test_janitor.py", "content": "import os\r\nfrom collections import defaultdict\r\n\r\nfrom lm_eval.decontamination.janitor import (\r\n    Janitor,\r\n    form_ngrams,\r\n    split_indices,\r\n    word_ngrams,\r\n    word_ngrams_indices,\r\n)\r\n\r\n\r\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\r\nTEST_SEQUENCE = (\r\n    \"Hello my name is Bob, I like eating pizza, chicken, chips and ice cream. Maybe I should eat some\"\r\n    \" more salad but it's so booooring. I just... like eating pizza, chicken, chips and ice cream so much.\"\r\n)\r\n\r\nJANITOR_EXPECTED = (\r\n    \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n    \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n    \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n    \"This is a @line #containing \"\r\n    \" characters, 76 to be exact. \"\r\n    \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n    \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n    \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n)\r\n\r\nJANITOR_FILTH1 = \"filth lots of dirty filthy filth\"\r\nJANITOR_FILTH2 = \"filth lots of filthy dirty filth\"\r\n\r\n\r\ndef simple_ngram(sequence, n):\r\n    ngrams = list()\r\n    ngram = []\r\n    for x in sequence:\r\n        ngram.extend([x])\r\n        if len(ngram) == n:\r\n            ngrams.extend([tuple(ngram)])\r\n            ngram = ngram[1:]\r\n\r\n    return ngrams\r\n\r\n\r\ndef test_form_ngrams():\r\n    sequence = TEST_SEQUENCE\r\n\r\n    n_values = [1, 2, 3, 5, 13]\r\n    for n in n_values:\r\n        comparison = simple_ngram(sequence, n)\r\n        result_to_test = list(form_ngrams(iter(sequence), n))\r\n        assert len(comparison) == len(result_to_test)\r\n        assert comparison == result_to_test\r\n\r\n\r\ndef test_word_ngrams():\r\n    sequence = TEST_SEQUENCE\r\n\r\n    words = sequence.split()\r\n\r\n    n_values = [1, 2, 3, 5, 13]\r\n    for n in n_values:\r\n        comparison = simple_ngram(words, n)\r\n        comparison = [\" \".join(ngram) for ngram in comparison]\r\n        result_to_test = list(word_ngrams(sequence, n))\r\n        assert len(comparison) == len(result_to_test)\r\n        assert result_to_test == comparison\r\n\r\n\r\ndef test_split_indices():\r\n    sequence = TEST_SEQUENCE\r\n\r\n    comparison = []\r\n    current_word = \"\"\r\n    for i, c in enumerate(sequence):\r\n        if c != \" \":\r\n            current_word += c\r\n        else:\r\n            if current_word:\r\n                comparison.extend([(current_word, (i - len(current_word), i - 1))])\r\n                current_word = \"\"\r\n\r\n    if current_word:\r\n        len_sequence = len(sequence)\r\n        comparison.extend(\r\n            [\r\n                (\r\n                    current_word,\r\n                    (len_sequence - len(current_word), len_sequence - 1),\r\n                )\r\n            ]\r\n        )\r\n        current_word = \"\"\r\n\r\n    result_to_test = list(split_indices(sequence))\r\n    assert len(comparison) == len(result_to_test)\r\n    assert comparison == result_to_test\r\n\r\n\r\ndef test_word_ngrams_indices():\r\n    sequence = TEST_SEQUENCE\r\n\r\n    n_values = [1, 2, 3, 5, 13]\r\n\r\n    for n in n_values:\r\n        ngrams = [\" \".join(ngram) for ngram in simple_ngram(sequence.split(), n)]\r\n        tracker = defaultdict(int)\r\n        comparison = []\r\n        for ngram in ngrams:\r\n            while True:\r\n                start = sequence.find(ngram, tracker[ngram])\r\n                assert start != -1  # testing the test\r\n\r\n                end = start + len(ngram) - 1\r\n                tracker[ngram] = end + 1\r\n\r\n                # ignore partial word matches\r\n                if not (\r\n                    (start != 0 and sequence[start - 1] != \" \")\r\n                    or (end != len(sequence) - 1 and sequence[end + 1] != \" \")\r\n                ):\r\n                    break\r\n\r\n            comparison.extend([(ngram, (start, end))])\r\n\r\n        result_to_test = list(word_ngrams_indices(sequence, n))\r\n        assert len(result_to_test) == len(comparison)\r\n        assert result_to_test == comparison\r\n\r\n\r\n# Assumptions from GPT3 Paper:\r\n# the 200 characters to remove include punctuation and is actually a half-window\r\n\r\n\r\n# All tests below initially test without any registered contaminants, expecting the same sequence back.\r\ndef test_janitor1():\r\n    # First test using a 1gram and expected the first block before the filth to have some remaining\r\n    # characters, but the second block should be completely removed.\r\n\r\n    sequence = (\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"FILTH. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n    )\r\n\r\n    filth = \"filth\"\r\n\r\n    expected_result = (\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing \"\r\n    )\r\n\r\n    janitor = Janitor(\r\n        ngram_n=1, window_to_remove=200, too_dirty_cutoff=10, minimum_slice_length=200\r\n    )\r\n    result = janitor.clean_python(sequence)\r\n    result = \"\".join(result)\r\n    assert result == sequence\r\n\r\n    janitor.register_contaminant(filth)\r\n    assert janitor.dirt_ngrams == {filth}\r\n\r\n    result = janitor.clean_python(sequence)\r\n    result = \"\".join(result)\r\n    assert result == expected_result\r\n\r\n\r\ndef test_janitor2():\r\n    # Second test using a 1gram and expected the first block before the filth to have some remaining\r\n    # characters, and the second block is longer then 200 characters so should also have some remaining.\r\n\r\n    sequence = (\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"FILTH. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n    )\r\n\r\n    filth = \"filth\"\r\n\r\n    janitor = Janitor(\r\n        ngram_n=1, window_to_remove=200, too_dirty_cutoff=10, minimum_slice_length=200\r\n    )\r\n    result = janitor.clean_python(sequence)\r\n    result = \"\".join(result)\r\n    assert result == sequence\r\n\r\n    janitor.register_contaminant(filth)\r\n    assert janitor.dirt_ngrams == {filth}\r\n\r\n    result = janitor.clean_python(sequence)\r\n    result = \"\".join(result)\r\n    assert result == JANITOR_EXPECTED\r\n\r\n\r\ndef test_janitor3():\r\n    # Same test as above but with a 6gram.\r\n\r\n    sequence = (\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"FILTH. lots of dirty filtHy FIlTh \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n    )\r\n\r\n    janitor = Janitor(\r\n        ngram_n=6, window_to_remove=200, too_dirty_cutoff=10, minimum_slice_length=200\r\n    )\r\n    result = janitor.clean_python(sequence)\r\n    result = \"\".join(result)\r\n    assert result == sequence\r\n\r\n    janitor.register_contaminant(JANITOR_FILTH1)\r\n    assert janitor.dirt_ngrams == {JANITOR_FILTH1}\r\n\r\n    result = janitor.clean_python(sequence)\r\n    result = \"\".join(result)\r\n    assert result == JANITOR_EXPECTED\r\n\r\n\r\ndef test_janitor4():\r\n    # This test adds another block to that from the previous. The middle block should be entirely\r\n    # removed as the 200 characters are removed from each side.\r\n\r\n    sequence = (\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"FILTH. lots of dirty filtHy FIlTh \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"FILTH. lots of dirty filtHy FIlTh \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n    )\r\n\r\n    janitor = Janitor(\r\n        ngram_n=6, window_to_remove=200, too_dirty_cutoff=10, minimum_slice_length=200\r\n    )\r\n    result = janitor.clean_python(sequence)\r\n    result = \"\".join(result)\r\n    assert result == sequence\r\n\r\n    janitor.register_contaminant(JANITOR_FILTH1)\r\n    assert janitor.dirt_ngrams == {JANITOR_FILTH1}\r\n\r\n    result = janitor.clean_python(sequence)\r\n    result = \"\".join(result)\r\n    assert result == JANITOR_EXPECTED\r\n\r\n\r\ndef test_janitor5():\r\n    # Same as above but using multiple different filth 6grams.\r\n\r\n    sequence = (\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"FILTH. lots of dirty filtHy FIlTh \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"FILTH. lots of filtHy dirty FIlTh \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n    )\r\n\r\n    filths = [JANITOR_FILTH1, JANITOR_FILTH2]\r\n\r\n    janitor = Janitor(\r\n        ngram_n=6, window_to_remove=200, too_dirty_cutoff=10, minimum_slice_length=200\r\n    )\r\n    result = janitor.clean_python(sequence)\r\n    result = \"\".join(result)\r\n    assert result == sequence\r\n\r\n    for filth in filths:\r\n        janitor.register_contaminant(filth)\r\n    assert janitor.dirt_ngrams == set(filths)\r\n\r\n    result = janitor.clean_python(sequence)\r\n    result = \"\".join(result)\r\n    assert result == JANITOR_EXPECTED\r\n\r\n\r\ndef test_janitor6():\r\n    # Same as above but now we add 10 filths and expect the same result, the following test does 11.\r\n\r\n    sequence = (\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"FILTH. lots of dirty filtHy FIlTh \"\r\n        \"FILTH. lots of dirty filtHy FIlTh \"\r\n        \"FILTH. lots of dirty filtHy FIlTh \"\r\n        \"FILTH. lots of dirty filtHy FIlTh \"\r\n        \"FILTH. lots of dirty filtHy FIlTh \"\r\n        \"FILTH. lots of dirty filtHy FIlTh \"\r\n        \"FILTH. lots of dirty filtHy FIlTh \"\r\n        \"FILTH. lots of dirty filtHy FIlTh \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"FILTH. lots of filtHy dirty FIlTh \"\r\n        \"FILTH. lots of filtHy dirty FIlTh \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n    )\r\n\r\n    filths = [JANITOR_FILTH1, JANITOR_FILTH2]\r\n\r\n    janitor = Janitor(\r\n        ngram_n=6, window_to_remove=200, too_dirty_cutoff=10, minimum_slice_length=200\r\n    )\r\n    result = janitor.clean_python(sequence)\r\n    result = \"\".join(result)\r\n    assert result == sequence\r\n\r\n    for filth in filths:\r\n        janitor.register_contaminant(filth)\r\n    assert janitor.dirt_ngrams == set(filths)\r\n\r\n    result = janitor.clean_python(sequence)\r\n    result = \"\".join(result)\r\n    assert result == JANITOR_EXPECTED\r\n\r\n\r\ndef test_janitor7():\r\n    # Same as above but now we add 9 filths and expect the same result, the following test does 10.\r\n\r\n    sequence = (\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"FILTH. lots of dirty filtHy FIlTh \"\r\n        \"FILTH. lots of dirty filtHy FIlTh \"\r\n        \"FILTH. lots of dirty filtHy FIlTh \"\r\n        \"FILTH. lots of dirty filtHy FIlTh \"\r\n        \"FILTH. lots of dirty filtHy FIlTh \"\r\n        \"FILTH. lots of dirty filtHy FIlTh \"\r\n        \"FILTH. lots of dirty filtHy FIlTh \"\r\n        \"FILTH. lots of dirty filtHy FIlTh \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"FILTH. lots of filtHy dirty FIlTh \"\r\n        \"FILTH. lots of filtHy dirty FIlTh \"\r\n        \"FILTH. lots of filtHy dirty FIlTh \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n        \"This is a @line #containing a certain number of characters, 76 to be exact. \"\r\n    )\r\n\r\n    filths = [JANITOR_FILTH1, JANITOR_FILTH2]\r\n\r\n    expected_result = \"\"\r\n\r\n    janitor = Janitor(\r\n        ngram_n=6, window_to_remove=200, too_dirty_cutoff=10, minimum_slice_length=200\r\n    )\r\n    result = janitor.clean_python(sequence)\r\n    result = \"\".join(result)\r\n    assert result == sequence\r\n\r\n    for filth in filths:\r\n        janitor.register_contaminant(filth)\r\n    assert janitor.dirt_ngrams == set(filths)\r\n\r\n    result = janitor.clean_python(sequence)\r\n    result = \"\".join(result)\r\n    assert result == expected_result\r\n\r\n\r\ndef test_janitor8():\r\n    # This will test the save and load contams\r\n    pass\r\n"}
{"type": "test_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/tests/test_utils.py", "content": "import itertools\r\n\r\nimport numpy as np\r\nimport pytest\r\nimport torch\r\n\r\nfrom lm_eval.api.metrics import (\r\n    aggregate_subtask_metrics,\r\n    mean,\r\n    pooled_sample_stderr,\r\n    stderr_for_metric,\r\n)\r\nfrom lm_eval.models.utils import Collator\r\nfrom lm_eval.utils import (\r\n    get_rolling_token_windows,\r\n    make_disjoint_window,\r\n)\r\n\r\n\r\n# noinspection DuplicatedCode\r\ndef test_get_rolling_token_windows_v1():\r\n    gold = [\r\n        ([-100, 0, 1, 2, 3, 4, 5, 6, 7, 8], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\r\n        (\r\n            [9, 10, 11, 12, 13, 14, 15, 16, 17, 18],\r\n            [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\r\n        ),\r\n        (\r\n            [19, 20, 21, 22, 23, 24, 25, 26, 27, 28],\r\n            [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\r\n        ),\r\n        ([23, 24, 25, 26, 27, 28, 29, 30, 31, 32], [30, 31, 32, 33]),\r\n    ]\r\n    x = list(range(34))\r\n    generator = get_rolling_token_windows(\r\n        token_list=x,\r\n        prefix_token=-100,\r\n        max_seq_len=10,\r\n        context_len=1,\r\n    )\r\n    pred_length = 0\r\n    output = []\r\n    for input_tokens, pred_tokens in generator:\r\n        output.extend([(input_tokens, pred_tokens)])\r\n        pred_length += len(pred_tokens)\r\n    assert pred_length == len(x)\r\n    assert gold == output\r\n\r\n\r\n# noinspection DuplicatedCode\r\ndef test_get_rolling_token_windows_v2():\r\n    gold = [\r\n        ([-100, 0, 1, 2, 3, 4, 5, 6, 7, 8], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\r\n        ([2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [10, 11, 12]),\r\n        ([5, 6, 7, 8, 9, 10, 11, 12, 13, 14], [13, 14, 15]),\r\n        ([8, 9, 10, 11, 12, 13, 14, 15, 16, 17], [16, 17, 18]),\r\n        ([11, 12, 13, 14, 15, 16, 17, 18, 19, 20], [19, 20, 21]),\r\n        ([14, 15, 16, 17, 18, 19, 20, 21, 22, 23], [22, 23, 24]),\r\n        ([17, 18, 19, 20, 21, 22, 23, 24, 25, 26], [25, 26, 27]),\r\n        ([20, 21, 22, 23, 24, 25, 26, 27, 28, 29], [28, 29, 30]),\r\n        ([23, 24, 25, 26, 27, 28, 29, 30, 31, 32], [31, 32, 33]),\r\n    ]\r\n    x = list(range(34))\r\n    generator = get_rolling_token_windows(\r\n        token_list=x,\r\n        prefix_token=-100,\r\n        max_seq_len=10,\r\n        context_len=8,\r\n    )\r\n    pred_length = 0\r\n    output = []\r\n    for input_tokens, pred_tokens in generator:\r\n        output.extend([(input_tokens, pred_tokens)])\r\n        pred_length += len(pred_tokens)\r\n    assert pred_length == len(x)\r\n    assert gold == output\r\n\r\n\r\n# noinspection DuplicatedCode\r\ndef test_get_rolling_token_windows_v3():\r\n    gold = [\r\n        ([-100, 0, 1, 2, 3, 4, 5, 6, 7, 8], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\r\n        ([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10]),\r\n        ([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [11]),\r\n        ([2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [12]),\r\n        ([3, 4, 5, 6, 7, 8, 9, 10, 11, 12], [13]),\r\n        ([4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [14]),\r\n        ([5, 6, 7, 8, 9, 10, 11, 12, 13, 14], [15]),\r\n        ([6, 7, 8, 9, 10, 11, 12, 13, 14, 15], [16]),\r\n        ([7, 8, 9, 10, 11, 12, 13, 14, 15, 16], [17]),\r\n        ([8, 9, 10, 11, 12, 13, 14, 15, 16, 17], [18]),\r\n        ([9, 10, 11, 12, 13, 14, 15, 16, 17, 18], [19]),\r\n        ([10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [20]),\r\n        ([11, 12, 13, 14, 15, 16, 17, 18, 19, 20], [21]),\r\n        ([12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [22]),\r\n        ([13, 14, 15, 16, 17, 18, 19, 20, 21, 22], [23]),\r\n        ([14, 15, 16, 17, 18, 19, 20, 21, 22, 23], [24]),\r\n        ([15, 16, 17, 18, 19, 20, 21, 22, 23, 24], [25]),\r\n        ([16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [26]),\r\n        ([17, 18, 19, 20, 21, 22, 23, 24, 25, 26], [27]),\r\n        ([18, 19, 20, 21, 22, 23, 24, 25, 26, 27], [28]),\r\n        ([19, 20, 21, 22, 23, 24, 25, 26, 27, 28], [29]),\r\n        ([20, 21, 22, 23, 24, 25, 26, 27, 28, 29], [30]),\r\n        ([21, 22, 23, 24, 25, 26, 27, 28, 29, 30], [31]),\r\n        ([22, 23, 24, 25, 26, 27, 28, 29, 30, 31], [32]),\r\n        ([23, 24, 25, 26, 27, 28, 29, 30, 31, 32], [33]),\r\n    ]\r\n    x = list(range(34))\r\n    generator = get_rolling_token_windows(\r\n        token_list=x,\r\n        prefix_token=-100,\r\n        max_seq_len=10,\r\n        context_len=10,\r\n    )\r\n    pred_length = 0\r\n    output = []\r\n    for input_tokens, pred_tokens in generator:\r\n        output.extend([(input_tokens, pred_tokens)])\r\n        pred_length += len(pred_tokens)\r\n    assert pred_length == len(x)\r\n    assert gold == output\r\n\r\n\r\n# noinspection DuplicatedCode\r\ndef test_get_rolling_token_windows_v4():\r\n    gold = [\r\n        ([-100, 0, 1, 2, 3, 4, 5, 6, 7, 8], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\r\n        ([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10]),\r\n        ([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [11]),\r\n        ([2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [12]),\r\n        ([3, 4, 5, 6, 7, 8, 9, 10, 11, 12], [13]),\r\n        ([4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [14]),\r\n        ([5, 6, 7, 8, 9, 10, 11, 12, 13, 14], [15]),\r\n        ([6, 7, 8, 9, 10, 11, 12, 13, 14, 15], [16]),\r\n        ([7, 8, 9, 10, 11, 12, 13, 14, 15, 16], [17]),\r\n        ([8, 9, 10, 11, 12, 13, 14, 15, 16, 17], [18]),\r\n        ([9, 10, 11, 12, 13, 14, 15, 16, 17, 18], [19]),\r\n        ([10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [20]),\r\n        ([11, 12, 13, 14, 15, 16, 17, 18, 19, 20], [21]),\r\n        ([12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [22]),\r\n        ([13, 14, 15, 16, 17, 18, 19, 20, 21, 22], [23]),\r\n        ([14, 15, 16, 17, 18, 19, 20, 21, 22, 23], [24]),\r\n        ([15, 16, 17, 18, 19, 20, 21, 22, 23, 24], [25]),\r\n        ([16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [26]),\r\n        ([17, 18, 19, 20, 21, 22, 23, 24, 25, 26], [27]),\r\n        ([18, 19, 20, 21, 22, 23, 24, 25, 26, 27], [28]),\r\n        ([19, 20, 21, 22, 23, 24, 25, 26, 27, 28], [29]),\r\n    ]\r\n    x = list(range(30))\r\n    generator = get_rolling_token_windows(\r\n        token_list=x,\r\n        prefix_token=-100,\r\n        max_seq_len=10,\r\n        context_len=10,\r\n    )\r\n    pred_length = 0\r\n    output = []\r\n    for input_tokens, pred_tokens in generator:\r\n        output.extend([(input_tokens, pred_tokens)])\r\n        pred_length += len(pred_tokens)\r\n    assert pred_length == len(x)\r\n    assert gold == output\r\n\r\n\r\n# noinspection DuplicatedCode\r\ndef test_get_rolling_token_windows_v5():\r\n    gold = [\r\n        ([-100, 0, 1, 2, 3, 4, 5, 6, 7, 8], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\r\n        (\r\n            [9, 10, 11, 12, 13, 14, 15, 16, 17, 18],\r\n            [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\r\n        ),\r\n        (\r\n            [19, 20, 21, 22, 23, 24, 25, 26, 27, 28],\r\n            [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\r\n        ),\r\n    ]\r\n    x = list(range(30))\r\n    generator = get_rolling_token_windows(\r\n        token_list=x,\r\n        prefix_token=-100,\r\n        max_seq_len=10,\r\n        context_len=1,\r\n    )\r\n    pred_length = 0\r\n    output = []\r\n    for input_tokens, pred_tokens in generator:\r\n        output.extend([(input_tokens, pred_tokens)])\r\n        pred_length += len(pred_tokens)\r\n    assert pred_length == len(x)\r\n    assert gold == output\r\n\r\n\r\n# noinspection DuplicatedCode\r\ndef test_get_rolling_token_windows_v6():\r\n    gold = [\r\n        ([-100, 0], [0, 1]),\r\n        ([1, 2], [2, 3]),\r\n        ([3, 4], [4, 5]),\r\n        ([5, 6], [6, 7]),\r\n        ([6, 7], [8]),\r\n    ]\r\n    x = list(range(9))\r\n    generator = get_rolling_token_windows(\r\n        token_list=x,\r\n        prefix_token=-100,\r\n        max_seq_len=2,\r\n        context_len=1,\r\n    )\r\n    pred_length = 0\r\n    output = []\r\n    for input_tokens, pred_tokens in generator:\r\n        output.extend([(input_tokens, pred_tokens)])\r\n        pred_length += len(pred_tokens)\r\n    assert pred_length == len(x)\r\n    assert gold == output\r\n\r\n\r\ndef test_get_rolling_token_windows_empty():\r\n    generator = get_rolling_token_windows(\r\n        token_list=[],\r\n        prefix_token=-100,\r\n        max_seq_len=2,\r\n        context_len=1,\r\n    )\r\n    n = 0\r\n    for _ in generator:\r\n        n += 1\r\n    assert n == 0\r\n\r\n\r\ndef test_make_disjoint_window():\r\n    assert make_disjoint_window(([1, 2, 3, 4, 5], [2, 3, 4, 5, 6])) == (\r\n        [1],\r\n        [2, 3, 4, 5, 6],\r\n    )\r\n    assert make_disjoint_window(([1, 2, 3, 4, 5], [4, 5, 6])) == ([1, 2, 3], [4, 5, 6])\r\n    assert make_disjoint_window(([1, 2, 3, 4, 5], [6])) == ([1, 2, 3, 4, 5], [6])\r\n\r\n\r\nclass TestCollator:\r\n    def make_generate_sample(self, end=10):\r\n        strings = [\"x\" * i for i in range(1, end + 1)]\r\n        gen_kwargs1, gen_kwargs2 = (\r\n            {\"temperature\": 0},\r\n            {\"temperature\": 0, \"until\": [\"nn\", \"\\n\\n\"]},\r\n        )\r\n        args = [\r\n            (string, gen_kwargs1 if i < len(strings) // 2 else gen_kwargs2)\r\n            for i, string in enumerate(strings)\r\n        ]\r\n\r\n        return args\r\n\r\n    def make_loglikelihood_sample(self, end=11):\r\n        samples = [\r\n            ((\"x\", \"x\"), list(range(1, total_length + 1)))\r\n            for total_length in range(1, end + 1)\r\n        ]\r\n        return samples\r\n\r\n    def make_loglikelihood_sample_group(self, end=11):\r\n        a = [((\"x\", \"x\"), [1, 2, 3, 4, 5, 6, 7, 8], [x]) for x in range(9)]\r\n        b = [\r\n            ((\"x\", \"x\"), [1, 2, 3, 4, 5, 6, 7, 8], [x, y, z])\r\n            for x, y, z in zip(range(9), range(9, 18), range(18, 27))\r\n        ]\r\n        return a + b\r\n\r\n    @pytest.mark.parametrize(\"batch_size, end\", [(17, 30), (8, 61), (12, 48), (0, 9)])\r\n    def test_generations(self, batch_size, end):\r\n        _collate_gen = lambda x: (-len(x[0]), x[0])  # noqa: E731\r\n\r\n        generation_samples = self.make_generate_sample(int(end))\r\n        gens = Collator(generation_samples, _collate_gen, group_by=\"gen_kwargs\")\r\n        chunks_gen = gens.get_batched(n=int(batch_size), batch_fn=None)\r\n        output = []\r\n        group_one = end // 2\r\n        group_two = end - end // 2\r\n        is_batch = batch_size != 0\r\n        for chunks in chunks_gen:\r\n            # check batching\r\n            assert (\r\n                len(chunks) <= batch_size\r\n                if is_batch\r\n                else len(chunks) in [group_one, group_two]\r\n            )\r\n            # check if reorder-er is working correctly\r\n            chunk_lengths = [len(chunk[0]) for chunk in chunks]\r\n            assert chunk_lengths == sorted(chunk_lengths, reverse=True)\r\n            # check if grouping correctly\r\n            chunk_to_compare = chunks[0][1]\r\n            assert all(x[1] == chunk_to_compare for x in chunks)\r\n            for x in chunks:\r\n                output.extend([x])\r\n        reordered_output = gens.get_original(output)\r\n        # check get original\r\n        assert reordered_output == generation_samples\r\n\r\n    @pytest.mark.parametrize(\"batch_size, end\", [(17, 30), (8, 61), (12, 48), (0, 3)])\r\n    def test_loglikelihood(self, batch_size, end):\r\n        _collate_log = lambda x: (-len(x[1]), tuple(x[1]))  # noqa: E731\r\n        loglikelihood_samples = self.make_loglikelihood_sample(int(end))\r\n        loglikelihoods = Collator(\r\n            loglikelihood_samples,\r\n            _collate_log,\r\n        )\r\n        chunks_gen = loglikelihoods.get_batched(n=int(batch_size), batch_fn=None)\r\n        output = []\r\n        is_batch = batch_size != 0\r\n        for chunks in chunks_gen:\r\n            # check batching\r\n            assert len(chunks) <= batch_size if is_batch else len(chunks) == end\r\n            # check reorder\r\n            chunk_lengths = [len(chunk[1]) for chunk in chunks]\r\n            assert chunk_lengths == sorted(chunk_lengths, reverse=True)\r\n            for x in chunks:\r\n                output.extend([x[1]])\r\n        # check indices\r\n        reordered_output = loglikelihoods.get_original(output)\r\n        assert reordered_output == [x[1] for x in loglikelihood_samples]\r\n\r\n    @pytest.mark.parametrize(\"batch_size\", [17, 8, 12, 0])\r\n    def test_context_grouping(self, batch_size):\r\n        def _collate(x):\r\n            toks = x[1] + x[2]\r\n            return -len(toks), tuple(toks)\r\n\r\n        _collate_log = _collate  # noqa: E731\r\n        loglikelihood_samples = self.make_loglikelihood_sample_group()\r\n        loglikelihoods = Collator(\r\n            loglikelihood_samples,\r\n            _collate_log,\r\n            group_fn=lambda a: a[-2] + a[-1][:-1],\r\n            group_by=\"contexts\",\r\n        )\r\n        chunks_gen = loglikelihoods.get_batched(n=int(batch_size), batch_fn=None)\r\n        output = []\r\n        outputs_ = []\r\n        is_batch = batch_size != 0\r\n        for chunks in chunks_gen:\r\n            # check batching\r\n            if is_batch:\r\n                assert len(chunks) <= batch_size\r\n            # check reorder\r\n            chunk_lengths = [len(chunk[1]) for chunk in chunks]\r\n            assert chunk_lengths == sorted(chunk_lengths, reverse=True)\r\n            for x in chunks:\r\n                for request_str, cont_toks, logits in loglikelihoods.get_cache(\r\n                    req_str=\"\".join(x[0]),\r\n                    cxt_toks=x[1],\r\n                    cont_toks=x[2],\r\n                    logits=torch.tensor([1, 2, 3, 4, 5, 6, 7, 8])\r\n                    .unsqueeze(0)\r\n                    .unsqueeze(0),\r\n                ):\r\n                    output.extend([x[1]])\r\n                    outputs_.extend([cont_toks])\r\n        assert len(output) == len(outputs_)\r\n        # check indices\r\n        reordered_output = loglikelihoods.get_original(output)\r\n        assert reordered_output == [x[1] for x in loglikelihood_samples]\r\n\r\n\r\ndef test_aggregate_mean():\r\n    # test weight_by_size is respected\r\n    assert (\r\n        aggregate_subtask_metrics([0.3, 0.2, 0.4], [20, 40, 100], weight_by_size=False)\r\n        == 0.3\r\n    )\r\n    assert (\r\n        aggregate_subtask_metrics([0.3, 0.2, 0.4], [20, 40, 100], weight_by_size=True)\r\n        == 0.3375\r\n    )\r\n\r\n\r\n@pytest.mark.parametrize(\r\n    \"samples\",\r\n    [\r\n        [40 * [1.0] + 60 * [0.0], 30 * [1.0] + 30 * [0.0], 20 * [1.0] + 60 * [0.0]],\r\n        [35 * [1.0] + 65 * [0.0], 20 * [1.0] + 20 * [0.0]],\r\n    ],\r\n)\r\ndef test_aggregate_stderrs(samples):\r\n    # check that aggregating subtasks' bootstrap stderrs with our formula\r\n    # (using weight_by_size) is ~equiv.\r\n    # to just getting bootstrap stderr of the whole set of samples\r\n    mean_stderr = stderr_for_metric(metric=mean, bootstrap_iters=100000)\r\n\r\n    stderrs = [mean_stderr(subtask) for subtask in samples]\r\n\r\n    sizes = [len(subtask) for subtask in samples]\r\n\r\n    assert np.allclose(\r\n        pooled_sample_stderr(stderrs, sizes),\r\n        mean_stderr(list(itertools.chain.from_iterable(samples))),\r\n        atol=1.0e-3,\r\n    )\r\n"}
{"type": "test_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/tests/models/test_vllm.py", "content": "from typing import List\r\n\r\nimport pytest\r\n\r\nfrom lm_eval import tasks\r\nfrom lm_eval.api.instance import Instance\r\n\r\n\r\ntask_manager = tasks.TaskManager()\r\n\r\n\r\n@pytest.mark.skip(reason=\"requires CUDA\")\r\nclass Test_VLLM:\r\n    vllm = pytest.importorskip(\"vllm\")\r\n    try:\r\n        from lm_eval.models.vllm_causallms import VLLM\r\n\r\n        LM = VLLM(pretrained=\"EleutherAI/pythia-70m\")\r\n    except ModuleNotFoundError:\r\n        pass\r\n    # torch.use_deterministic_algorithms(True)\r\n    task_list = task_manager.load_task_or_group([\"arc_easy\", \"gsm8k\", \"wikitext\"])\r\n    multiple_choice_task = task_list[\"arc_easy\"]  # type: ignore\r\n    multiple_choice_task.build_all_requests(limit=10, rank=0, world_size=1)\r\n    MULTIPLE_CH: List[Instance] = multiple_choice_task.instances\r\n    generate_until_task = task_list[\"gsm8k\"]  # type: ignore\r\n    generate_until_task._config.generation_kwargs[\"max_gen_toks\"] = 10\r\n    generate_until_task.build_all_requests(limit=10, rank=0, world_size=1)\r\n    generate_until: List[Instance] = generate_until_task.instances\r\n    rolling_task = task_list[\"wikitext\"]  # type: ignore\r\n    rolling_task.build_all_requests(limit=10, rank=0, world_size=1)\r\n    ROLLING: List[Instance] = rolling_task.instances\r\n\r\n    # TODO: make proper tests\r\n    def test_logliklihood(self) -> None:\r\n        res = self.LM.loglikelihood(self.MULTIPLE_CH)\r\n        assert len(res) == len(self.MULTIPLE_CH)\r\n        for x in res:\r\n            assert isinstance(x[0], float)\r\n\r\n    def test_generate_until(self) -> None:\r\n        res = self.LM.generate_until(self.generate_until)\r\n        assert len(res) == len(self.generate_until)\r\n        for x in res:\r\n            assert isinstance(x, str)\r\n\r\n    def test_logliklihood_rolling(self) -> None:\r\n        res = self.LM.loglikelihood_rolling(self.ROLLING)\r\n        for x in res:\r\n            assert isinstance(x, float)\r\n"}
{"type": "test_file", "path": "train/LLaMA-Factory/tests/e2e/test_train.py", "content": "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\n\nimport pytest\n\nfrom llamafactory.train.tuner import export_model, run_exp\n\n\nDEMO_DATA = os.getenv(\"DEMO_DATA\", \"llamafactory/demo_data\")\n\nTINY_LLAMA = os.getenv(\"TINY_LLAMA\", \"llamafactory/tiny-random-Llama-3\")\n\nTINY_LLAMA_ADAPTER = os.getenv(\"TINY_LLAMA_ADAPTER\", \"llamafactory/tiny-random-Llama-3-lora\")\n\nTRAIN_ARGS = {\n    \"model_name_or_path\": TINY_LLAMA,\n    \"do_train\": True,\n    \"finetuning_type\": \"lora\",\n    \"dataset_dir\": \"REMOTE:\" + DEMO_DATA,\n    \"template\": \"llama3\",\n    \"cutoff_len\": 1,\n    \"overwrite_cache\": False,\n    \"overwrite_output_dir\": True,\n    \"per_device_train_batch_size\": 1,\n    \"max_steps\": 1,\n}\n\nINFER_ARGS = {\n    \"model_name_or_path\": TINY_LLAMA,\n    \"adapter_name_or_path\": TINY_LLAMA_ADAPTER,\n    \"finetuning_type\": \"lora\",\n    \"template\": \"llama3\",\n    \"infer_dtype\": \"float16\",\n}\n\nOS_NAME = os.getenv(\"OS_NAME\", \"\")\n\n\n@pytest.mark.parametrize(\n    \"stage,dataset\",\n    [\n        (\"pt\", \"c4_demo\"),\n        (\"sft\", \"alpaca_en_demo\"),\n        (\"dpo\", \"dpo_en_demo\"),\n        (\"kto\", \"kto_en_demo\"),\n        pytest.param(\"rm\", \"dpo_en_demo\", marks=pytest.mark.xfail(OS_NAME.startswith(\"windows\"), reason=\"OS error.\")),\n    ],\n)\ndef test_run_exp(stage: str, dataset: str):\n    output_dir = os.path.join(\"output\", f\"train_{stage}\")\n    run_exp({\"stage\": stage, \"dataset\": dataset, \"output_dir\": output_dir, **TRAIN_ARGS})\n    assert os.path.exists(output_dir)\n\n\ndef test_export():\n    export_dir = os.path.join(\"output\", \"llama3_export\")\n    export_model({\"export_dir\": export_dir, **INFER_ARGS})\n    assert os.path.exists(export_dir)\n"}
{"type": "test_file", "path": "train/LLaMA-Factory/tests/data/processors/test_feedback.py", "content": "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport random\n\nimport pytest\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\nfrom llamafactory.extras.constants import IGNORE_INDEX\nfrom llamafactory.train.test_utils import load_train_dataset\n\n\nDEMO_DATA = os.getenv(\"DEMO_DATA\", \"llamafactory/demo_data\")\n\nTINY_LLAMA = os.getenv(\"TINY_LLAMA\", \"llamafactory/tiny-random-Llama-3\")\n\nTRAIN_ARGS = {\n    \"model_name_or_path\": TINY_LLAMA,\n    \"stage\": \"kto\",\n    \"do_train\": True,\n    \"finetuning_type\": \"full\",\n    \"dataset\": \"kto_en_demo\",\n    \"dataset_dir\": \"REMOTE:\" + DEMO_DATA,\n    \"template\": \"llama3\",\n    \"cutoff_len\": 8192,\n    \"overwrite_cache\": True,\n    \"output_dir\": \"dummy_dir\",\n    \"overwrite_output_dir\": True,\n    \"fp16\": True,\n}\n\n\n@pytest.mark.parametrize(\"num_samples\", [16])\ndef test_feedback_data(num_samples: int):\n    train_dataset = load_train_dataset(**TRAIN_ARGS)\n    ref_tokenizer = AutoTokenizer.from_pretrained(TINY_LLAMA)\n    original_data = load_dataset(DEMO_DATA, name=\"kto_en_demo\", split=\"train\")\n    indexes = random.choices(range(len(original_data)), k=num_samples)\n    for index in indexes:\n        messages = original_data[\"messages\"][index]\n        ref_input_ids = ref_tokenizer.apply_chat_template(messages)\n        prompt_len = len(ref_tokenizer.apply_chat_template(messages[:-1], add_generation_prompt=True))\n        ref_labels = [IGNORE_INDEX] * prompt_len + ref_input_ids[prompt_len:]\n        assert train_dataset[\"input_ids\"][index] == ref_input_ids\n        assert train_dataset[\"labels\"][index] == ref_labels\n        assert train_dataset[\"kto_tags\"][index] == original_data[\"label\"][index]\n"}
{"type": "test_file", "path": "train/LLaMA-Factory/tests/model/test_base.py", "content": "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\n\nimport pytest\n\nfrom llamafactory.train.test_utils import compare_model, load_infer_model, load_reference_model, patch_valuehead_model\n\n\nTINY_LLAMA = os.getenv(\"TINY_LLAMA\", \"llamafactory/tiny-random-Llama-3\")\n\nTINY_LLAMA_VALUEHEAD = os.getenv(\"TINY_LLAMA_VALUEHEAD\", \"llamafactory/tiny-random-Llama-3-valuehead\")\n\nINFER_ARGS = {\n    \"model_name_or_path\": TINY_LLAMA,\n    \"template\": \"llama3\",\n    \"infer_dtype\": \"float16\",\n}\n\n\n@pytest.fixture\ndef fix_valuehead_cpu_loading():\n    patch_valuehead_model()\n\n\ndef test_base():\n    model = load_infer_model(**INFER_ARGS)\n    ref_model = load_reference_model(TINY_LLAMA)\n    compare_model(model, ref_model)\n\n\n@pytest.mark.usefixtures(\"fix_valuehead_cpu_loading\")\ndef test_valuehead():\n    model = load_infer_model(add_valuehead=True, **INFER_ARGS)\n    ref_model = load_reference_model(TINY_LLAMA_VALUEHEAD, add_valuehead=True)\n    compare_model(model, ref_model)\n"}
{"type": "test_file", "path": "train/LLaMA-Factory/tests/data/test_mm_plugin.py", "content": "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nfrom typing import TYPE_CHECKING, Any, Dict, List, Sequence\n\nimport pytest\nimport torch\nfrom PIL import Image\n\nfrom llamafactory.data.mm_plugin import get_mm_plugin\nfrom llamafactory.hparams import get_infer_args\nfrom llamafactory.model import load_tokenizer\n\n\nif TYPE_CHECKING:\n    from transformers import PreTrainedTokenizer, ProcessorMixin\n    from transformers.image_processing_utils import BaseImageProcessor\n\n    from llamafactory.data.mm_plugin import BasePlugin\n    from llamafactory.model.loader import TokenizerModule\n\n\nHF_TOKEN = os.getenv(\"HF_TOKEN\")\n\nTINY_LLAMA = os.getenv(\"TINY_LLAMA\", \"llamafactory/tiny-random-Llama-3\")\n\nMM_MESSAGES = [\n    {\"role\": \"user\", \"content\": \"<image>What is in this image?\"},\n    {\"role\": \"assistant\", \"content\": \"A cat.\"},\n]\n\nTEXT_MESSAGES = [\n    {\"role\": \"user\", \"content\": \"How are you\"},\n    {\"role\": \"assistant\", \"content\": \"I am fine!\"},\n]\n\nIMAGES = [Image.new(\"RGB\", (32, 32), (255, 255, 255))]\n\nNO_IMAGES = []\n\nNO_VIDEOS = []\n\nNO_AUDIOS = []\n\nIMGLENS = [1]\n\nNO_IMGLENS = [0]\n\nNO_VIDLENS = [0]\n\nNO_AUDLENS = [0]\n\nINPUT_IDS = [0, 1, 2, 3, 4]\n\nLABELS = [0, 1, 2, 3, 4]\n\nBATCH_IDS = [[1] * 1024]\n\n\ndef _get_mm_inputs(processor: \"ProcessorMixin\") -> Dict[str, \"torch.Tensor\"]:\n    image_processor: \"BaseImageProcessor\" = getattr(processor, \"image_processor\")\n    return image_processor(images=IMAGES, return_tensors=\"pt\")\n\n\ndef _is_close(batch_a: Dict[str, Any], batch_b: Dict[str, Any]) -> None:\n    assert batch_a.keys() == batch_b.keys()\n    for key in batch_a.keys():\n        if isinstance(batch_a[key], torch.Tensor):\n            assert torch.allclose(batch_a[key], batch_b[key], rtol=1e-4, atol=1e-5)\n        elif isinstance(batch_a[key], list) and all(isinstance(item, torch.Tensor) for item in batch_a[key]):\n            assert len(batch_a[key]) == len(batch_b[key])\n            for tensor_a, tensor_b in zip(batch_a[key], batch_b[key]):\n                assert torch.allclose(tensor_a, tensor_b, rtol=1e-4, atol=1e-5)\n        else:\n            assert batch_a[key] == batch_b[key]\n\n\ndef _load_tokenizer_module(model_name_or_path: str) -> \"TokenizerModule\":\n    model_args, *_ = get_infer_args({\"model_name_or_path\": model_name_or_path, \"template\": \"default\"})\n    return load_tokenizer(model_args)\n\n\ndef _check_plugin(\n    plugin: \"BasePlugin\",\n    tokenizer: \"PreTrainedTokenizer\",\n    processor: \"ProcessorMixin\",\n    expected_mm_messages: Sequence[Dict[str, str]] = MM_MESSAGES,\n    expected_input_ids: List[int] = INPUT_IDS,\n    expected_labels: List[int] = LABELS,\n    expected_mm_inputs: Dict[str, Any] = {},\n    expected_no_mm_inputs: Dict[str, Any] = {},\n) -> None:\n    # test mm_messages\n    assert plugin.process_messages(MM_MESSAGES, IMAGES, NO_VIDEOS, NO_AUDIOS, processor) == expected_mm_messages\n    assert plugin.process_token_ids(INPUT_IDS, LABELS, IMAGES, NO_VIDEOS, NO_AUDIOS, tokenizer, processor) == (\n        expected_input_ids,\n        expected_labels,\n    )\n    _is_close(\n        plugin.get_mm_inputs(IMAGES, NO_VIDEOS, NO_AUDIOS, IMGLENS, NO_VIDLENS, NO_AUDLENS, BATCH_IDS, processor),\n        expected_mm_inputs,\n    )\n    # test text_messages\n    assert plugin.process_messages(TEXT_MESSAGES, NO_IMAGES, NO_VIDEOS, NO_AUDIOS, processor) == TEXT_MESSAGES\n    assert plugin.process_token_ids(INPUT_IDS, LABELS, NO_IMAGES, NO_VIDEOS, NO_AUDIOS, tokenizer, processor) == (\n        INPUT_IDS,\n        LABELS,\n    )\n    _is_close(\n        plugin.get_mm_inputs(\n            NO_IMAGES, NO_VIDEOS, NO_AUDIOS, NO_IMGLENS, NO_VIDLENS, NO_AUDLENS, BATCH_IDS, processor\n        ),\n        expected_no_mm_inputs,\n    )\n\n\ndef test_base_plugin():\n    tokenizer_module = _load_tokenizer_module(model_name_or_path=TINY_LLAMA)\n    base_plugin = get_mm_plugin(name=\"base\", image_token=\"<image>\")\n    check_inputs = {\"plugin\": base_plugin, **tokenizer_module}\n    _check_plugin(**check_inputs)\n\n\ndef test_llava_plugin():\n    image_seqlen = 576\n    tokenizer_module = _load_tokenizer_module(model_name_or_path=\"llava-hf/llava-1.5-7b-hf\")\n    llava_plugin = get_mm_plugin(name=\"llava\", image_token=\"<image>\")\n    check_inputs = {\"plugin\": llava_plugin, **tokenizer_module}\n    check_inputs[\"expected_mm_messages\"] = [\n        {key: value.replace(\"<image>\", \"<image>\" * image_seqlen) for key, value in message.items()}\n        for message in MM_MESSAGES\n    ]\n    check_inputs[\"expected_mm_inputs\"] = _get_mm_inputs(tokenizer_module[\"processor\"])\n    _check_plugin(**check_inputs)\n\n\ndef test_llava_next_plugin():\n    image_seqlen = 1176\n    tokenizer_module = _load_tokenizer_module(model_name_or_path=\"llava-hf/llava-v1.6-vicuna-7b-hf\")\n    llava_next_plugin = get_mm_plugin(name=\"llava_next\", image_token=\"<image>\")\n    check_inputs = {\"plugin\": llava_next_plugin, **tokenizer_module}\n    check_inputs[\"expected_mm_messages\"] = [\n        {key: value.replace(\"<image>\", \"<image>\" * image_seqlen) for key, value in message.items()}\n        for message in MM_MESSAGES\n    ]\n    check_inputs[\"expected_mm_inputs\"] = _get_mm_inputs(tokenizer_module[\"processor\"])\n    _check_plugin(**check_inputs)\n\n\ndef test_llava_next_video_plugin():\n    image_seqlen = 1176\n    tokenizer_module = _load_tokenizer_module(model_name_or_path=\"llava-hf/LLaVA-NeXT-Video-7B-hf\")\n    llava_next_video_plugin = get_mm_plugin(name=\"llava_next_video\", image_token=\"<image>\", video_token=\"<video>\")\n    check_inputs = {\"plugin\": llava_next_video_plugin, **tokenizer_module}\n    check_inputs[\"expected_mm_messages\"] = [\n        {key: value.replace(\"<image>\", \"<image>\" * image_seqlen) for key, value in message.items()}\n        for message in MM_MESSAGES\n    ]\n    check_inputs[\"expected_mm_inputs\"] = _get_mm_inputs(tokenizer_module[\"processor\"])\n    _check_plugin(**check_inputs)\n\n\n@pytest.mark.skipif(not HF_TOKEN, reason=\"Gated model.\")\ndef test_paligemma_plugin():\n    image_seqlen = 256\n    tokenizer_module = _load_tokenizer_module(model_name_or_path=\"google/paligemma-3b-pt-224\")\n    paligemma_plugin = get_mm_plugin(name=\"paligemma\", image_token=\"<image>\")\n    check_inputs = {\"plugin\": paligemma_plugin, **tokenizer_module}\n    check_inputs[\"expected_mm_messages\"] = [\n        {key: value.replace(\"<image>\", \"\") for key, value in message.items()} for message in MM_MESSAGES\n    ]\n    check_inputs[\"expected_input_ids\"] = [\n        tokenizer_module[\"tokenizer\"].convert_tokens_to_ids(paligemma_plugin.image_token)\n    ] * image_seqlen + INPUT_IDS\n    check_inputs[\"expected_labels\"] = [-100] * image_seqlen + LABELS\n    check_inputs[\"expected_mm_inputs\"] = _get_mm_inputs(tokenizer_module[\"processor\"])\n    check_inputs[\"expected_mm_inputs\"][\"token_type_ids\"] = [[0] * image_seqlen + [1] * (1024 - image_seqlen)]\n    check_inputs[\"expected_no_mm_inputs\"] = {\"token_type_ids\": [[1] * 1024]}\n    _check_plugin(**check_inputs)\n\n\ndef test_pixtral_plugin():\n    image_slice_height, image_slice_width = 2, 2\n    tokenizer_module = _load_tokenizer_module(model_name_or_path=\"mistral-community/pixtral-12b\")\n    pixtral_plugin = get_mm_plugin(name=\"pixtral\", image_token=\"[IMG]\")\n    check_inputs = {\"plugin\": pixtral_plugin, **tokenizer_module}\n    check_inputs[\"expected_mm_messages\"] = [\n        {\n            key: value.replace(\n                \"<image>\",\n                (\"{}[IMG_BREAK]\".format(\"[IMG]\" * image_slice_width) * image_slice_height).rsplit(\"[IMG_BREAK]\", 1)[0]\n                + \"[IMG_END]\",\n            )\n            for key, value in message.items()\n        }\n        for message in MM_MESSAGES\n    ]\n    check_inputs[\"expected_mm_inputs\"] = _get_mm_inputs(tokenizer_module[\"processor\"])\n    check_inputs[\"expected_mm_inputs\"].pop(\"image_sizes\")\n    check_inputs[\"expected_mm_inputs\"][\"pixel_values\"] = check_inputs[\"expected_mm_inputs\"][\"pixel_values\"][0]\n    _check_plugin(**check_inputs)\n\n\ndef test_qwen2_vl_plugin():\n    image_seqlen = 4\n    tokenizer_module = _load_tokenizer_module(model_name_or_path=\"Qwen/Qwen2-VL-7B-Instruct\")\n    qwen2_vl_plugin = get_mm_plugin(name=\"qwen2_vl\", image_token=\"<|image_pad|>\")\n    check_inputs = {\"plugin\": qwen2_vl_plugin, **tokenizer_module}\n    check_inputs[\"expected_mm_messages\"] = [\n        {\n            key: value.replace(\"<image>\", \"<|vision_start|>{}<|vision_end|>\".format(\"<|image_pad|>\" * image_seqlen))\n            for key, value in message.items()\n        }\n        for message in MM_MESSAGES\n    ]\n    check_inputs[\"expected_mm_inputs\"] = _get_mm_inputs(tokenizer_module[\"processor\"])\n    _check_plugin(**check_inputs)\n\n\ndef test_video_llava_plugin():\n    image_seqlen = 256\n    tokenizer_module = _load_tokenizer_module(model_name_or_path=\"LanguageBind/Video-LLaVA-7B-hf\")\n    video_llava_plugin = get_mm_plugin(name=\"video_llava\", image_token=\"<image>\", video_token=\"<video>\")\n    check_inputs = {\"plugin\": video_llava_plugin, **tokenizer_module}\n    check_inputs[\"expected_mm_messages\"] = [\n        {key: value.replace(\"<image>\", \"<image>\" * image_seqlen) for key, value in message.items()}\n        for message in MM_MESSAGES\n    ]\n    check_inputs[\"expected_mm_inputs\"] = _get_mm_inputs(tokenizer_module[\"processor\"])\n    _check_plugin(**check_inputs)\n"}
{"type": "test_file", "path": "train/LLaMA-Factory/tests/train/test_sft_trainer.py", "content": "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List\n\nimport pytest\nfrom transformers import DataCollatorWithPadding\n\nfrom llamafactory.data import get_dataset, get_template_and_fix_tokenizer\nfrom llamafactory.hparams import get_train_args\nfrom llamafactory.model import load_model, load_tokenizer\nfrom llamafactory.train.sft.trainer import CustomSeq2SeqTrainer\n\n\nDEMO_DATA = os.getenv(\"DEMO_DATA\", \"llamafactory/demo_data\")\n\nTINY_LLAMA = os.getenv(\"TINY_LLAMA\", \"llamafactory/tiny-random-Llama-3\")\n\nTRAIN_ARGS = {\n    \"model_name_or_path\": TINY_LLAMA,\n    \"stage\": \"sft\",\n    \"do_train\": True,\n    \"finetuning_type\": \"lora\",\n    \"dataset\": \"llamafactory/tiny-supervised-dataset\",\n    \"dataset_dir\": \"ONLINE\",\n    \"template\": \"llama3\",\n    \"cutoff_len\": 1024,\n    \"overwrite_cache\": False,\n    \"overwrite_output_dir\": True,\n    \"per_device_train_batch_size\": 1,\n    \"max_steps\": 1,\n}\n\n\n@dataclass\nclass DataCollatorWithVerbose(DataCollatorWithPadding):\n    verbose_list: List[Dict[str, Any]] = field(default_factory=list)\n\n    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n        self.verbose_list.extend(features)\n        batch = super().__call__(features)\n        return {k: v[:, :1] for k, v in batch.items()}  # truncate input length\n\n\n@pytest.mark.parametrize(\"disable_shuffling\", [False, True])\ndef test_shuffle(disable_shuffling: bool):\n    model_args, data_args, training_args, finetuning_args, _ = get_train_args(\n        {\n            \"output_dir\": os.path.join(\"output\", f\"shuffle{str(disable_shuffling).lower()}\"),\n            \"disable_shuffling\": disable_shuffling,\n            **TRAIN_ARGS,\n        }\n    )\n    tokenizer_module = load_tokenizer(model_args)\n    tokenizer = tokenizer_module[\"tokenizer\"]\n    template = get_template_and_fix_tokenizer(tokenizer, data_args)\n    dataset_module = get_dataset(template, model_args, data_args, training_args, stage=\"sft\", **tokenizer_module)\n    model = load_model(tokenizer, model_args, finetuning_args, training_args.do_train)\n    data_collator = DataCollatorWithVerbose(tokenizer=tokenizer)\n    trainer = CustomSeq2SeqTrainer(\n        model=model,\n        args=training_args,\n        finetuning_args=finetuning_args,\n        data_collator=data_collator,\n        **dataset_module,\n        **tokenizer_module,\n    )\n    trainer.train()\n    if disable_shuffling:\n        assert data_collator.verbose_list[0][\"input_ids\"] == dataset_module[\"train_dataset\"][0][\"input_ids\"]\n    else:\n        assert data_collator.verbose_list[0][\"input_ids\"] != dataset_module[\"train_dataset\"][0][\"input_ids\"]\n"}
{"type": "test_file", "path": "train/LLaMA-Factory/tests/model/model_utils/test_checkpointing.py", "content": "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\n\nimport pytest\nimport torch\n\nfrom llamafactory.extras.misc import get_current_device\nfrom llamafactory.train.test_utils import load_train_model\n\n\nTINY_LLAMA = os.getenv(\"TINY_LLAMA\", \"llamafactory/tiny-random-Llama-3\")\n\nTRAIN_ARGS = {\n    \"model_name_or_path\": TINY_LLAMA,\n    \"stage\": \"sft\",\n    \"do_train\": True,\n    \"finetuning_type\": \"lora\",\n    \"lora_target\": \"all\",\n    \"dataset\": \"llamafactory/tiny-supervised-dataset\",\n    \"dataset_dir\": \"ONLINE\",\n    \"template\": \"llama3\",\n    \"cutoff_len\": 1024,\n    \"overwrite_cache\": True,\n    \"output_dir\": \"dummy_dir\",\n    \"overwrite_output_dir\": True,\n    \"fp16\": True,\n}\n\n\n@pytest.mark.parametrize(\"disable_gradient_checkpointing\", [False, True])\ndef test_vanilla_checkpointing(disable_gradient_checkpointing: bool):\n    model = load_train_model(disable_gradient_checkpointing=disable_gradient_checkpointing, **TRAIN_ARGS)\n    for module in filter(lambda m: hasattr(m, \"gradient_checkpointing\"), model.modules()):\n        assert getattr(module, \"gradient_checkpointing\") != disable_gradient_checkpointing\n\n\ndef test_unsloth_gradient_checkpointing():\n    model = load_train_model(use_unsloth_gc=True, **TRAIN_ARGS)\n    for module in filter(lambda m: hasattr(m, \"gradient_checkpointing\"), model.modules()):\n        assert module._gradient_checkpointing_func.__self__.__name__ == \"UnslothGradientCheckpointing\"\n\n\ndef test_upcast_layernorm():\n    model = load_train_model(upcast_layernorm=True, **TRAIN_ARGS)\n    for name, param in model.named_parameters():\n        if param.ndim == 1 and \"norm\" in name:\n            assert param.dtype == torch.float32\n\n\ndef test_upcast_lmhead_output():\n    model = load_train_model(upcast_lmhead_output=True, **TRAIN_ARGS)\n    inputs = torch.randn((1, 16), dtype=torch.float16, device=get_current_device())\n    outputs: \"torch.Tensor\" = model.get_output_embeddings()(inputs)\n    assert outputs.dtype == torch.float32\n"}
{"type": "test_file", "path": "train/LLaMA-Factory/scripts/api_example/test_toolcall.py", "content": "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport os\nfrom typing import Sequence\n\nfrom openai import OpenAI\nfrom transformers.utils.versions import require_version\n\n\nrequire_version(\"openai>=1.5.0\", \"To fix: pip install openai>=1.5.0\")\n\n\ndef calculate_gpa(grades: Sequence[str], hours: Sequence[int]) -> float:\n    grade_to_score = {\"A\": 4, \"B\": 3, \"C\": 2}\n    total_score, total_hour = 0, 0\n    for grade, hour in zip(grades, hours):\n        total_score += grade_to_score[grade] * hour\n        total_hour += hour\n    return round(total_score / total_hour, 2)\n\n\ndef main():\n    client = OpenAI(\n        api_key=\"{}\".format(os.environ.get(\"API_KEY\", \"0\")),\n        base_url=\"http://localhost:{}/v1\".format(os.environ.get(\"API_PORT\", 8000)),\n    )\n    tools = [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"calculate_gpa\",\n                \"description\": \"Calculate the Grade Point Average (GPA) based on grades and credit hours\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"grades\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"The grades\"},\n                        \"hours\": {\"type\": \"array\", \"items\": {\"type\": \"integer\"}, \"description\": \"The credit hours\"},\n                    },\n                    \"required\": [\"grades\", \"hours\"],\n                },\n            },\n        }\n    ]\n    tool_map = {\"calculate_gpa\": calculate_gpa}\n\n    messages = []\n    messages.append({\"role\": \"user\", \"content\": \"My grades are A, A, B, and C. The credit hours are 3, 4, 3, and 2.\"})\n    result = client.chat.completions.create(messages=messages, model=\"test\", tools=tools)\n    if result.choices[0].message.tool_calls is None:\n        raise ValueError(\"Cannot retrieve function call from the response.\")\n\n    messages.append(result.choices[0].message)\n    tool_call = result.choices[0].message.tool_calls[0].function\n    print(tool_call)\n    # Function(arguments='{\"grades\": [\"A\", \"A\", \"B\", \"C\"], \"hours\": [3, 4, 3, 2]}', name='calculate_gpa')\n    name, arguments = tool_call.name, json.loads(tool_call.arguments)\n    tool_result = tool_map[name](**arguments)\n    messages.append({\"role\": \"tool\", \"content\": json.dumps({\"gpa\": tool_result}, ensure_ascii=False)})\n    result = client.chat.completions.create(messages=messages, model=\"test\", tools=tools)\n    print(result.choices[0].message.content)\n    # Based on the grades and credit hours you provided, your Grade Point Average (GPA) is 3.42.\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "test_file", "path": "train/LLaMA-Factory/tests/e2e/test_chat.py", "content": "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\n\nfrom llamafactory.chat import ChatModel\n\n\nTINY_LLAMA = os.getenv(\"TINY_LLAMA\", \"llamafactory/tiny-random-Llama-3\")\n\nINFER_ARGS = {\n    \"model_name_or_path\": TINY_LLAMA,\n    \"finetuning_type\": \"lora\",\n    \"template\": \"llama3\",\n    \"infer_dtype\": \"float16\",\n    \"do_sample\": False,\n    \"max_new_tokens\": 1,\n}\n\nMESSAGES = [\n    {\"role\": \"user\", \"content\": \"Hi\"},\n]\n\nEXPECTED_RESPONSE = \"_rho\"\n\n\ndef test_chat():\n    chat_model = ChatModel(INFER_ARGS)\n    assert chat_model.chat(MESSAGES)[0].response_text == EXPECTED_RESPONSE\n\n\ndef test_stream_chat():\n    chat_model = ChatModel(INFER_ARGS)\n    response = \"\"\n    for token in chat_model.stream_chat(MESSAGES):\n        response += token\n\n    assert response == EXPECTED_RESPONSE\n"}
{"type": "test_file", "path": "train/LLaMA-Factory/tests/model/model_utils/test_attention.py", "content": "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\n\nimport pytest\nfrom transformers.utils import is_flash_attn_2_available, is_torch_sdpa_available\n\nfrom llamafactory.extras.packages import is_transformers_version_greater_than\nfrom llamafactory.train.test_utils import load_infer_model\n\n\nTINY_LLAMA = os.getenv(\"TINY_LLAMA\", \"llamafactory/tiny-random-Llama-3\")\n\nINFER_ARGS = {\n    \"model_name_or_path\": TINY_LLAMA,\n    \"template\": \"llama3\",\n}\n\n\n@pytest.mark.xfail(is_transformers_version_greater_than(\"4.48\"), reason=\"Attention refactor.\")\ndef test_attention():\n    attention_available = [\"disabled\"]\n    if is_torch_sdpa_available():\n        attention_available.append(\"sdpa\")\n\n    if is_flash_attn_2_available():\n        attention_available.append(\"fa2\")\n\n    llama_attention_classes = {\n        \"disabled\": \"LlamaAttention\",\n        \"sdpa\": \"LlamaSdpaAttention\",\n        \"fa2\": \"LlamaFlashAttention2\",\n    }\n    for requested_attention in attention_available:\n        model = load_infer_model(flash_attn=requested_attention, **INFER_ARGS)\n        for module in model.modules():\n            if \"Attention\" in module.__class__.__name__:\n                assert module.__class__.__name__ == llama_attention_classes[requested_attention]\n"}
{"type": "test_file", "path": "train/LLaMA-Factory/tests/data/processors/test_processor_utils.py", "content": "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Tuple\n\nimport pytest\n\nfrom llamafactory.data.processors.processor_utils import infer_seqlen\n\n\n@pytest.mark.parametrize(\n    \"test_input,test_output\",\n    [\n        ((3000, 2000, 1000), (600, 400)),\n        ((2000, 3000, 1000), (400, 600)),\n        ((1000, 100, 1000), (900, 100)),\n        ((100, 1000, 1000), (100, 900)),\n        ((100, 500, 1000), (100, 500)),\n        ((500, 100, 1000), (500, 100)),\n        ((10, 10, 1000), (10, 10)),\n    ],\n)\ndef test_infer_seqlen(test_input: Tuple[int, int, int], test_output: Tuple[int, int]):\n    assert test_output == infer_seqlen(*test_input)\n"}
{"type": "test_file", "path": "train/LLaMA-Factory/tests/model/model_utils/test_visual.py", "content": "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\nimport torch\nfrom transformers import AutoConfig, AutoModelForVision2Seq\n\nfrom llamafactory.hparams import FinetuningArguments, ModelArguments\nfrom llamafactory.model.adapter import init_adapter\n\n\n@pytest.mark.parametrize(\n    \"freeze_vision_tower,freeze_multi_modal_projector,train_mm_proj_only\",\n    [\n        (False, False, False),\n        (False, True, False),\n        (True, False, False),\n        (True, True, False),\n        (True, False, True),\n    ],\n)\ndef test_visual_full(freeze_vision_tower: bool, freeze_multi_modal_projector: bool, train_mm_proj_only: bool):\n    model_args = ModelArguments(model_name_or_path=\"Qwen/Qwen2-VL-2B-Instruct\")\n    finetuning_args = FinetuningArguments(\n        finetuning_type=\"full\",\n        freeze_vision_tower=freeze_vision_tower,\n        freeze_multi_modal_projector=freeze_multi_modal_projector,\n        train_mm_proj_only=train_mm_proj_only,\n    )\n    config = AutoConfig.from_pretrained(model_args.model_name_or_path)\n    with torch.device(\"meta\"):\n        model = AutoModelForVision2Seq.from_config(config)\n\n    model = init_adapter(config, model, model_args, finetuning_args, is_trainable=True)\n    for name, param in model.named_parameters():\n        if any(key in name for key in [\"visual.patch_embed\", \"visual.blocks\"]):\n            assert param.requires_grad != freeze_vision_tower\n        elif \"visual.merger\" in name:\n            assert param.requires_grad != freeze_multi_modal_projector\n        else:\n            assert param.requires_grad != train_mm_proj_only\n\n\n@pytest.mark.parametrize(\"freeze_vision_tower\", [False, True])\ndef test_visual_lora(freeze_vision_tower: bool):\n    model_args = ModelArguments(model_name_or_path=\"Qwen/Qwen2-VL-2B-Instruct\")\n    finetuning_args = FinetuningArguments(finetuning_type=\"lora\", freeze_vision_tower=freeze_vision_tower)\n    config = AutoConfig.from_pretrained(model_args.model_name_or_path)\n    with torch.device(\"meta\"):\n        model = AutoModelForVision2Seq.from_config(config)\n\n    model = init_adapter(config, model, model_args, finetuning_args, is_trainable=True)\n    trainable_params, frozen_params = set(), set()\n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            trainable_params.add(name)\n        else:\n            frozen_params.add(name)\n\n    if freeze_vision_tower:\n        assert \"base_model.model.visual.blocks.0.attn.qkv.lora_A.default.weight\" not in trainable_params\n    else:\n        assert \"base_model.model.visual.blocks.0.attn.qkv.lora_A.default.weight\" in trainable_params\n\n    assert \"merger\" not in trainable_params\n    assert \"base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\" in trainable_params\n"}
{"type": "test_file", "path": "train/LLaMA-Factory/tests/model/model_utils/test_misc.py", "content": "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\n\nimport pytest\nimport torch\nfrom transformers import AutoConfig, AutoModelForCausalLM\n\nfrom llamafactory.model.model_utils.misc import find_expanded_modules\n\n\nHF_TOKEN = os.getenv(\"HF_TOKEN\")\n\n\n@pytest.mark.skipif(not HF_TOKEN, reason=\"Gated model.\")\ndef test_expanded_modules():\n    config = AutoConfig.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n    with torch.device(\"meta\"):\n        model = AutoModelForCausalLM.from_config(config)\n\n    expanded_modules = find_expanded_modules(model, [\"q_proj\", \"v_proj\"], num_layer_trainable=4)\n    assert expanded_modules == [\n        \"model.layers.7.self_attn.q_proj\",\n        \"model.layers.7.self_attn.v_proj\",\n        \"model.layers.15.self_attn.q_proj\",\n        \"model.layers.15.self_attn.v_proj\",\n        \"model.layers.23.self_attn.q_proj\",\n        \"model.layers.23.self_attn.v_proj\",\n        \"model.layers.31.self_attn.q_proj\",\n        \"model.layers.31.self_attn.v_proj\",\n    ]\n"}
{"type": "test_file", "path": "train/LLaMA-Factory/tests/data/processors/test_pairwise.py", "content": "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport random\nfrom typing import Dict, List\n\nimport pytest\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\nfrom llamafactory.extras.constants import IGNORE_INDEX\nfrom llamafactory.train.test_utils import load_train_dataset\n\n\nDEMO_DATA = os.getenv(\"DEMO_DATA\", \"llamafactory/demo_data\")\n\nTINY_LLAMA = os.getenv(\"TINY_LLAMA\", \"llamafactory/tiny-random-Llama-3\")\n\nTRAIN_ARGS = {\n    \"model_name_or_path\": TINY_LLAMA,\n    \"stage\": \"rm\",\n    \"do_train\": True,\n    \"finetuning_type\": \"full\",\n    \"dataset\": \"dpo_en_demo\",\n    \"dataset_dir\": \"REMOTE:\" + DEMO_DATA,\n    \"template\": \"llama3\",\n    \"cutoff_len\": 8192,\n    \"overwrite_cache\": True,\n    \"output_dir\": \"dummy_dir\",\n    \"overwrite_output_dir\": True,\n    \"fp16\": True,\n}\n\n\ndef _convert_sharegpt_to_openai(messages: List[Dict[str, str]]) -> List[Dict[str, str]]:\n    role_mapping = {\"human\": \"user\", \"gpt\": \"assistant\", \"system\": \"system\"}\n    new_messages = []\n    for message in messages:\n        new_messages.append({\"role\": role_mapping[message[\"from\"]], \"content\": message[\"value\"]})\n\n    return new_messages\n\n\n@pytest.mark.parametrize(\"num_samples\", [16])\ndef test_pairwise_data(num_samples: int):\n    train_dataset = load_train_dataset(**TRAIN_ARGS)\n    ref_tokenizer = AutoTokenizer.from_pretrained(TINY_LLAMA)\n    original_data = load_dataset(DEMO_DATA, name=\"dpo_en_demo\", split=\"train\")\n    indexes = random.choices(range(len(original_data)), k=num_samples)\n    for index in indexes:\n        chosen_messages = original_data[\"conversations\"][index] + [original_data[\"chosen\"][index]]\n        rejected_messages = original_data[\"conversations\"][index] + [original_data[\"rejected\"][index]]\n        chosen_messages = _convert_sharegpt_to_openai(chosen_messages)\n        rejected_messages = _convert_sharegpt_to_openai(rejected_messages)\n        ref_chosen_input_ids = ref_tokenizer.apply_chat_template(chosen_messages)\n        chosen_prompt_len = len(ref_tokenizer.apply_chat_template(chosen_messages[:-1], add_generation_prompt=True))\n        ref_chosen_labels = [IGNORE_INDEX] * chosen_prompt_len + ref_chosen_input_ids[chosen_prompt_len:]\n        ref_rejected_input_ids = ref_tokenizer.apply_chat_template(rejected_messages)\n        rejected_prompt_len = len(\n            ref_tokenizer.apply_chat_template(rejected_messages[:-1], add_generation_prompt=True)\n        )\n        ref_rejected_labels = [IGNORE_INDEX] * rejected_prompt_len + ref_rejected_input_ids[rejected_prompt_len:]\n        assert train_dataset[\"chosen_input_ids\"][index] == ref_chosen_input_ids\n        assert train_dataset[\"chosen_labels\"][index] == ref_chosen_labels\n        assert train_dataset[\"rejected_input_ids\"][index] == ref_rejected_input_ids\n        assert train_dataset[\"rejected_labels\"][index] == ref_rejected_labels\n"}
{"type": "test_file", "path": "train/LLaMA-Factory/tests/model/test_lora.py", "content": "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\n\nimport pytest\nimport torch\n\nfrom llamafactory.train.test_utils import (\n    check_lora_model,\n    compare_model,\n    load_infer_model,\n    load_reference_model,\n    load_train_model,\n    patch_valuehead_model,\n)\n\n\nTINY_LLAMA = os.getenv(\"TINY_LLAMA\", \"llamafactory/tiny-random-Llama-3\")\n\nTINY_LLAMA_ADAPTER = os.getenv(\"TINY_LLAMA_ADAPTER\", \"llamafactory/tiny-random-Llama-3-lora\")\n\nTINY_LLAMA_VALUEHEAD = os.getenv(\"TINY_LLAMA_VALUEHEAD\", \"llamafactory/tiny-random-Llama-3-valuehead\")\n\nTRAIN_ARGS = {\n    \"model_name_or_path\": TINY_LLAMA,\n    \"stage\": \"sft\",\n    \"do_train\": True,\n    \"finetuning_type\": \"lora\",\n    \"dataset\": \"llamafactory/tiny-supervised-dataset\",\n    \"dataset_dir\": \"ONLINE\",\n    \"template\": \"llama3\",\n    \"cutoff_len\": 1024,\n    \"overwrite_cache\": True,\n    \"output_dir\": \"dummy_dir\",\n    \"overwrite_output_dir\": True,\n    \"fp16\": True,\n}\n\nINFER_ARGS = {\n    \"model_name_or_path\": TINY_LLAMA,\n    \"adapter_name_or_path\": TINY_LLAMA_ADAPTER,\n    \"finetuning_type\": \"lora\",\n    \"template\": \"llama3\",\n    \"infer_dtype\": \"float16\",\n}\n\n\n@pytest.fixture\ndef fix_valuehead_cpu_loading():\n    patch_valuehead_model()\n\n\ndef test_lora_train_qv_modules():\n    model = load_train_model(lora_target=\"q_proj,v_proj\", **TRAIN_ARGS)\n    linear_modules, _ = check_lora_model(model)\n    assert linear_modules == {\"q_proj\", \"v_proj\"}\n\n\ndef test_lora_train_all_modules():\n    model = load_train_model(lora_target=\"all\", **TRAIN_ARGS)\n    linear_modules, _ = check_lora_model(model)\n    assert linear_modules == {\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"gate_proj\", \"down_proj\"}\n\n\ndef test_lora_train_extra_modules():\n    model = load_train_model(additional_target=\"embed_tokens,lm_head\", **TRAIN_ARGS)\n    _, extra_modules = check_lora_model(model)\n    assert extra_modules == {\"embed_tokens\", \"lm_head\"}\n\n\ndef test_lora_train_old_adapters():\n    model = load_train_model(adapter_name_or_path=TINY_LLAMA_ADAPTER, create_new_adapter=False, **TRAIN_ARGS)\n    ref_model = load_reference_model(TINY_LLAMA, TINY_LLAMA_ADAPTER, use_lora=True, is_trainable=True)\n    compare_model(model, ref_model)\n\n\ndef test_lora_train_new_adapters():\n    model = load_train_model(adapter_name_or_path=TINY_LLAMA_ADAPTER, create_new_adapter=True, **TRAIN_ARGS)\n    ref_model = load_reference_model(TINY_LLAMA, TINY_LLAMA_ADAPTER, use_lora=True, is_trainable=True)\n    compare_model(\n        model, ref_model, diff_keys=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"gate_proj\", \"down_proj\"]\n    )\n\n\n@pytest.mark.usefixtures(\"fix_valuehead_cpu_loading\")\ndef test_lora_train_valuehead():\n    model = load_train_model(add_valuehead=True, **TRAIN_ARGS)\n    ref_model = load_reference_model(TINY_LLAMA_VALUEHEAD, is_trainable=True, add_valuehead=True)\n    state_dict = model.state_dict()\n    ref_state_dict = ref_model.state_dict()\n    assert torch.allclose(state_dict[\"v_head.summary.weight\"], ref_state_dict[\"v_head.summary.weight\"])\n    assert torch.allclose(state_dict[\"v_head.summary.bias\"], ref_state_dict[\"v_head.summary.bias\"])\n\n\ndef test_lora_inference():\n    model = load_infer_model(**INFER_ARGS)\n    ref_model = load_reference_model(TINY_LLAMA, TINY_LLAMA_ADAPTER, use_lora=True).merge_and_unload()\n    compare_model(model, ref_model)\n"}
{"type": "test_file", "path": "train/LLaMA-Factory/tests/model/model_utils/test_packing.py", "content": "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\nimport torch\n\nfrom llamafactory.model.model_utils.packing import get_seqlens_in_batch, get_unpad_data\n\n\n@pytest.mark.parametrize(\n    \"attention_mask,golden_seq_lens\",\n    [\n        (\n            [\n                [1, 1, 2, 2, 2, 0],\n                [1, 2, 2, 3, 3, 3],\n            ],\n            [2, 3, 1, 2, 3],\n        ),\n        (\n            [[1]],\n            [1],\n        ),\n    ],\n)\ndef test_get_seqlens_in_batch(attention_mask, golden_seq_lens):\n    attention_mask_with_indices = torch.tensor(attention_mask)\n    seqlens_in_batch = get_seqlens_in_batch(attention_mask_with_indices)\n    assert torch.all(seqlens_in_batch == torch.tensor(golden_seq_lens))\n\n\n@pytest.mark.parametrize(\n    \"attention_mask,golden_indices,golden_cu_seqlens,golden_max_seqlen\",\n    [\n        (\n            [\n                [1, 1, 2, 2, 2, 0],\n                [1, 2, 2, 3, 3, 3],\n            ],\n            [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11],\n            [0, 2, 5, 6, 8, 11],\n            3,\n        ),\n        (\n            [[1]],\n            [0],\n            [0, 1],\n            1,\n        ),\n    ],\n)\ndef test_get_unpad_data(attention_mask, golden_indices, golden_cu_seqlens, golden_max_seqlen):\n    attention_mask_with_indices = torch.tensor(attention_mask)\n    indices, cu_seqlens, max_seqlen_in_batch = get_unpad_data(attention_mask_with_indices)\n    assert torch.all(indices == torch.tensor(golden_indices))\n    assert torch.all(cu_seqlens == torch.tensor(golden_cu_seqlens, dtype=torch.int32))\n    assert max_seqlen_in_batch == golden_max_seqlen\n"}
{"type": "test_file", "path": "train/LLaMA-Factory/tests/data/test_formatter.py", "content": "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nfrom datetime import datetime\n\nfrom llamafactory.data.formatter import EmptyFormatter, FunctionFormatter, StringFormatter, ToolFormatter\n\n\nFUNCTION = {\"name\": \"tool_name\", \"arguments\": {\"foo\": \"bar\", \"size\": 10}}\n\nTOOLS = [\n    {\n        \"name\": \"test_tool\",\n        \"description\": \"tool_desc\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"foo\": {\"type\": \"string\", \"description\": \"foo_desc\"},\n                \"bar\": {\"type\": \"number\", \"description\": \"bar_desc\"},\n            },\n            \"required\": [\"foo\"],\n        },\n    }\n]\n\n\ndef test_empty_formatter():\n    formatter = EmptyFormatter(slots=[\"\\n\"])\n    assert formatter.apply() == [\"\\n\"]\n\n\ndef test_string_formatter():\n    formatter = StringFormatter(slots=[\"<s>\", \"Human: {{content}}\\nAssistant:\"])\n    assert formatter.apply(content=\"Hi\") == [\"<s>\", \"Human: Hi\\nAssistant:\"]\n\n\ndef test_function_formatter():\n    formatter = FunctionFormatter(slots=[\"{{content}}\", \"</s>\"], tool_format=\"default\")\n    tool_calls = json.dumps(FUNCTION)\n    assert formatter.apply(content=tool_calls) == [\n        \"\"\"Action: tool_name\\nAction Input: {\"foo\": \"bar\", \"size\": 10}\\n\"\"\",\n        \"</s>\",\n    ]\n\n\ndef test_multi_function_formatter():\n    formatter = FunctionFormatter(slots=[\"{{content}}\", \"</s>\"], tool_format=\"default\")\n    tool_calls = json.dumps([FUNCTION] * 2)\n    assert formatter.apply(content=tool_calls) == [\n        \"\"\"Action: tool_name\\nAction Input: {\"foo\": \"bar\", \"size\": 10}\\n\"\"\"\n        \"\"\"Action: tool_name\\nAction Input: {\"foo\": \"bar\", \"size\": 10}\\n\"\"\",\n        \"</s>\",\n    ]\n\n\ndef test_default_tool_formatter():\n    formatter = ToolFormatter(tool_format=\"default\")\n    assert formatter.apply(content=json.dumps(TOOLS)) == [\n        \"You have access to the following tools:\\n\"\n        \"> Tool Name: test_tool\\n\"\n        \"Tool Description: tool_desc\\n\"\n        \"Tool Args:\\n\"\n        \"  - foo (string, required): foo_desc\\n\"\n        \"  - bar (number): bar_desc\\n\\n\"\n        \"Use the following format if using a tool:\\n\"\n        \"```\\n\"\n        \"Action: tool name (one of [test_tool])\\n\"\n        \"Action Input: the input to the tool, in a JSON format representing the kwargs \"\n        \"\"\"(e.g. ```{\"input\": \"hello world\", \"num_beams\": 5}```)\\n\"\"\"\n        \"```\\n\"\n    ]\n\n\ndef test_default_tool_extractor():\n    formatter = ToolFormatter(tool_format=\"default\")\n    result = \"\"\"Action: test_tool\\nAction Input: {\"foo\": \"bar\", \"size\": 10}\\n\"\"\"\n    assert formatter.extract(result) == [(\"test_tool\", \"\"\"{\"foo\": \"bar\", \"size\": 10}\"\"\")]\n\n\ndef test_default_multi_tool_extractor():\n    formatter = ToolFormatter(tool_format=\"default\")\n    result = (\n        \"\"\"Action: test_tool\\nAction Input: {\"foo\": \"bar\", \"size\": 10}\\n\"\"\"\n        \"\"\"Action: another_tool\\nAction Input: {\"foo\": \"job\", \"size\": 2}\\n\"\"\"\n    )\n    assert formatter.extract(result) == [\n        (\"test_tool\", \"\"\"{\"foo\": \"bar\", \"size\": 10}\"\"\"),\n        (\"another_tool\", \"\"\"{\"foo\": \"job\", \"size\": 2}\"\"\"),\n    ]\n\n\ndef test_glm4_function_formatter():\n    formatter = FunctionFormatter(slots=[\"{{content}}\"], tool_format=\"glm4\")\n    tool_calls = json.dumps(FUNCTION)\n    assert formatter.apply(content=tool_calls) == [\"\"\"tool_name\\n{\"foo\": \"bar\", \"size\": 10}\"\"\"]\n\n\ndef test_glm4_tool_formatter():\n    formatter = ToolFormatter(tool_format=\"glm4\")\n    assert formatter.apply(content=json.dumps(TOOLS)) == [\n        \"你是一个名为 ChatGLM 的人工智能助手。你是基于智谱AI训练的语言模型 GLM-4 模型开发的，\"\n        \"你的任务是针对用户的问题和要求提供适当的答复和支持。# 可用工具\\n\\n\"\n        f\"## test_tool\\n\\n{json.dumps(TOOLS[0], indent=4, ensure_ascii=False)}\\n在调用上述函数时，请使用 Json 格式表示调用的参数。\"\n    ]\n\n\ndef test_glm4_tool_extractor():\n    formatter = ToolFormatter(tool_format=\"glm4\")\n    result = \"\"\"test_tool\\n{\"foo\": \"bar\", \"size\": 10}\\n\"\"\"\n    assert formatter.extract(result) == [(\"test_tool\", \"\"\"{\"foo\": \"bar\", \"size\": 10}\"\"\")]\n\n\ndef test_llama3_function_formatter():\n    formatter = FunctionFormatter(slots=[\"{{content}}<|eot_id|>\"], tool_format=\"llama3\")\n    tool_calls = json.dumps({\"name\": \"tool_name\", \"arguments\": {\"foo\": \"bar\", \"size\": 10}})\n    assert formatter.apply(content=tool_calls) == [\n        \"\"\"{\"name\": \"tool_name\", \"parameters\": {\"foo\": \"bar\", \"size\": 10}}<|eot_id|>\"\"\"\n    ]\n\n\ndef test_llama3_tool_formatter():\n    formatter = ToolFormatter(tool_format=\"llama3\")\n    date = datetime.now().strftime(\"%d %b %Y\")\n    wrapped_tool = {\"type\": \"function\", \"function\": TOOLS[0]}\n    assert formatter.apply(content=json.dumps(TOOLS)) == [\n        f\"Cutting Knowledge Date: December 2023\\nToday Date: {date}\\n\\n\"\n        \"You have access to the following functions. To call a function, please respond with JSON for a function call. \"\n        \"\"\"Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}. \"\"\"\n        f\"Do not use variables.\\n\\n{json.dumps(wrapped_tool, indent=4, ensure_ascii=False)}\\n\\n\"\n    ]\n\n\ndef test_llama3_tool_extractor():\n    formatter = ToolFormatter(tool_format=\"llama3\")\n    result = \"\"\"{\"name\": \"test_tool\", \"parameters\": {\"foo\": \"bar\", \"size\": 10}}\\n\"\"\"\n    assert formatter.extract(result) == [(\"test_tool\", \"\"\"{\"foo\": \"bar\", \"size\": 10}\"\"\")]\n\n\ndef test_mistral_function_formatter():\n    formatter = FunctionFormatter(slots=[\"[TOOL_CALLS] {{content}}\", \"</s>\"], tool_format=\"mistral\")\n    tool_calls = json.dumps(FUNCTION)\n    assert formatter.apply(content=tool_calls) == [\n        \"[TOOL_CALLS] \" \"\"\"[{\"name\": \"tool_name\", \"arguments\": {\"foo\": \"bar\", \"size\": 10}}]\"\"\",\n        \"</s>\",\n    ]\n\n\ndef test_mistral_multi_function_formatter():\n    formatter = FunctionFormatter(slots=[\"[TOOL_CALLS] {{content}}\", \"</s>\"], tool_format=\"mistral\")\n    tool_calls = json.dumps([FUNCTION] * 2)\n    assert formatter.apply(content=tool_calls) == [\n        \"[TOOL_CALLS] \"\n        \"\"\"[{\"name\": \"tool_name\", \"arguments\": {\"foo\": \"bar\", \"size\": 10}}, \"\"\"\n        \"\"\"{\"name\": \"tool_name\", \"arguments\": {\"foo\": \"bar\", \"size\": 10}}]\"\"\",\n        \"</s>\",\n    ]\n\n\ndef test_mistral_tool_formatter():\n    formatter = ToolFormatter(tool_format=\"mistral\")\n    wrapped_tool = {\"type\": \"function\", \"function\": TOOLS[0]}\n    assert formatter.apply(content=json.dumps(TOOLS)) == [\n        \"[AVAILABLE_TOOLS] \" + json.dumps([wrapped_tool], ensure_ascii=False) + \"[/AVAILABLE_TOOLS]\"\n    ]\n\n\ndef test_mistral_tool_extractor():\n    formatter = ToolFormatter(tool_format=\"mistral\")\n    result = \"\"\"{\"name\": \"test_tool\", \"arguments\": {\"foo\": \"bar\", \"size\": 10}}\"\"\"\n    assert formatter.extract(result) == [(\"test_tool\", \"\"\"{\"foo\": \"bar\", \"size\": 10}\"\"\")]\n\n\ndef test_mistral_multi_tool_extractor():\n    formatter = ToolFormatter(tool_format=\"mistral\")\n    result = (\n        \"\"\"[{\"name\": \"test_tool\", \"arguments\": {\"foo\": \"bar\", \"size\": 10}}, \"\"\"\n        \"\"\"{\"name\": \"another_tool\", \"arguments\": {\"foo\": \"job\", \"size\": 2}}]\"\"\"\n    )\n    assert formatter.extract(result) == [\n        (\"test_tool\", \"\"\"{\"foo\": \"bar\", \"size\": 10}\"\"\"),\n        (\"another_tool\", \"\"\"{\"foo\": \"job\", \"size\": 2}\"\"\"),\n    ]\n\n\ndef test_qwen_function_formatter():\n    formatter = FunctionFormatter(slots=[\"{{content}}<|im_end|>\\n\"], tool_format=\"qwen\")\n    tool_calls = json.dumps(FUNCTION)\n    assert formatter.apply(content=tool_calls) == [\n        \"\"\"<tool_call>\\n{\"name\": \"tool_name\", \"arguments\": {\"foo\": \"bar\", \"size\": 10}}\\n</tool_call><|im_end|>\\n\"\"\"\n    ]\n\n\ndef test_qwen_multi_function_formatter():\n    formatter = FunctionFormatter(slots=[\"{{content}}<|im_end|>\\n\"], tool_format=\"qwen\")\n    tool_calls = json.dumps([FUNCTION] * 2)\n    assert formatter.apply(content=tool_calls) == [\n        \"\"\"<tool_call>\\n{\"name\": \"tool_name\", \"arguments\": {\"foo\": \"bar\", \"size\": 10}}\\n</tool_call>\\n\"\"\"\n        \"\"\"<tool_call>\\n{\"name\": \"tool_name\", \"arguments\": {\"foo\": \"bar\", \"size\": 10}}\\n</tool_call>\"\"\"\n        \"<|im_end|>\\n\"\n    ]\n\n\ndef test_qwen_tool_formatter():\n    formatter = ToolFormatter(tool_format=\"qwen\")\n    wrapped_tool = {\"type\": \"function\", \"function\": TOOLS[0]}\n    assert formatter.apply(content=json.dumps(TOOLS)) == [\n        \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\n\"\n        \"You are provided with function signatures within <tools></tools> XML tags:\\n<tools>\"\n        f\"\\n{json.dumps(wrapped_tool, ensure_ascii=False)}\"\n        \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within \"\n        \"\"\"<tool_call></tool_call> XML tags:\\n<tool_call>\\n{\"name\": <function-name>, \"\"\"\n        \"\"\"\"arguments\": <args-json-object>}\\n</tool_call>\"\"\"\n    ]\n\n\ndef test_qwen_tool_extractor():\n    formatter = ToolFormatter(tool_format=\"qwen\")\n    result = \"\"\"<tool_call>\\n{\"name\": \"test_tool\", \"arguments\": {\"foo\": \"bar\", \"size\": 10}}\\n</tool_call>\"\"\"\n    assert formatter.extract(result) == [(\"test_tool\", \"\"\"{\"foo\": \"bar\", \"size\": 10}\"\"\")]\n\n\ndef test_qwen_multi_tool_extractor():\n    formatter = ToolFormatter(tool_format=\"qwen\")\n    result = (\n        \"\"\"<tool_call>\\n{\"name\": \"test_tool\", \"arguments\": {\"foo\": \"bar\", \"size\": 10}}\\n</tool_call>\\n\"\"\"\n        \"\"\"<tool_call>\\n{\"name\": \"another_tool\", \"arguments\": {\"foo\": \"job\", \"size\": 2}}\\n</tool_call>\"\"\"\n    )\n    assert formatter.extract(result) == [\n        (\"test_tool\", \"\"\"{\"foo\": \"bar\", \"size\": 10}\"\"\"),\n        (\"another_tool\", \"\"\"{\"foo\": \"job\", \"size\": 2}\"\"\"),\n    ]\n"}
{"type": "test_file", "path": "train/LLaMA-Factory/tests/data/test_collator.py", "content": "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\n\nimport torch\nfrom PIL import Image\n\nfrom llamafactory.data import get_template_and_fix_tokenizer\nfrom llamafactory.data.collator import MultiModalDataCollatorForSeq2Seq, prepare_4d_attention_mask\nfrom llamafactory.extras.constants import IGNORE_INDEX\nfrom llamafactory.hparams import get_infer_args\nfrom llamafactory.model import load_tokenizer\n\n\nTINY_LLAMA = os.getenv(\"TINY_LLAMA\", \"llamafactory/tiny-random-Llama-3\")\n\n\ndef test_base_collator():\n    model_args, data_args, *_ = get_infer_args({\"model_name_or_path\": TINY_LLAMA, \"template\": \"default\"})\n    tokenizer_module = load_tokenizer(model_args)\n    template = get_template_and_fix_tokenizer(tokenizer_module[\"tokenizer\"], data_args)\n    data_collator = MultiModalDataCollatorForSeq2Seq(\n        template=template,\n        pad_to_multiple_of=8,\n        label_pad_token_id=IGNORE_INDEX,\n        **tokenizer_module,\n    )\n    p = tokenizer_module[\"tokenizer\"].pad_token_id\n    q = IGNORE_INDEX\n    features = [\n        {\n            \"input_ids\": [0, 1, 2, 3, 4, 5],\n            \"attention_mask\": [1, 1, 1, 1, 1, 1],\n            \"labels\": [q, q, 2, 3, 4, 5],\n        },\n        {\n            \"input_ids\": [6, 7],\n            \"attention_mask\": [1, 1],\n            \"labels\": [q, 7],\n        },\n    ]\n    batch_input = data_collator(features)\n    expected_input = {\n        \"input_ids\": [\n            [0, 1, 2, 3, 4, 5, p, p],\n            [6, 7, p, p, p, p, p, p],\n        ],\n        \"attention_mask\": [\n            [1, 1, 1, 1, 1, 1, 0, 0],\n            [1, 1, 0, 0, 0, 0, 0, 0],\n        ],\n        \"labels\": [\n            [q, q, 2, 3, 4, 5, q, q],\n            [q, 7, q, q, q, q, q, q],\n        ],\n    }\n    for k in batch_input.keys():\n        assert batch_input[k].eq(torch.tensor(expected_input[k])).all()\n\n\ndef test_multimodal_collator():\n    model_args, data_args, *_ = get_infer_args(\n        {\"model_name_or_path\": \"Qwen/Qwen2-VL-7B-Instruct\", \"template\": \"qwen2_vl\"}\n    )\n    tokenizer_module = load_tokenizer(model_args)\n    template = get_template_and_fix_tokenizer(tokenizer_module[\"tokenizer\"], data_args)\n    data_collator = MultiModalDataCollatorForSeq2Seq(\n        template=template,\n        pad_to_multiple_of=4,\n        label_pad_token_id=IGNORE_INDEX,\n        **tokenizer_module,\n    )\n    p = tokenizer_module[\"tokenizer\"].pad_token_id\n    q = IGNORE_INDEX\n    s = tokenizer_module[\"tokenizer\"].convert_tokens_to_ids(\"<|vision_start|>\")\n    e = tokenizer_module[\"tokenizer\"].convert_tokens_to_ids(\"<|vision_end|>\")\n    m = tokenizer_module[\"tokenizer\"].convert_tokens_to_ids(\"<|image_pad|>\")\n    fake_image = Image.new(\"RGB\", (64, 64), (255, 255, 255))\n\n    features = [\n        {\n            \"input_ids\": [0, 1, 2, 3],\n            \"attention_mask\": [1, 1, 1, 1],\n            \"labels\": [0, 1, 2, 3],\n        },\n    ]\n    batch_input = data_collator(features)\n    expected_input = {\n        \"input_ids\": [\n            [0, 1, 2, 3, s, m, m, m, m, e, p, p],\n        ],\n        \"attention_mask\": [\n            [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n        ],\n        \"labels\": [\n            [0, 1, 2, 3, q, q, q, q, q, q, q, q],\n        ],\n        **tokenizer_module[\"processor\"].image_processor(fake_image),\n    }\n    for k in batch_input.keys():\n        assert batch_input[k].eq(torch.tensor(expected_input[k])).all()\n\n\ndef test_4d_attention_mask():\n    o = 0.0\n    x = torch.finfo(torch.float16).min\n    attention_mask_with_indices = torch.tensor(\n        [\n            [1, 1, 2, 2, 2, 0],\n            [1, 2, 2, 3, 3, 3],\n        ]\n    )\n    attention_mask_computed = prepare_4d_attention_mask(attention_mask_with_indices, torch.float16)\n    attention_mask_expected = torch.tensor(\n        [\n            [\n                [\n                    [o, x, x, x, x, x],\n                    [o, o, x, x, x, x],\n                    [x, x, o, x, x, x],\n                    [x, x, o, o, x, x],\n                    [x, x, o, o, o, x],\n                    [x, x, x, x, x, x],\n                ]\n            ],\n            [\n                [\n                    [o, x, x, x, x, x],\n                    [x, o, x, x, x, x],\n                    [x, o, o, x, x, x],\n                    [x, x, x, o, x, x],\n                    [x, x, x, o, o, x],\n                    [x, x, x, o, o, o],\n                ]\n            ],\n        ],\n        dtype=torch.float16,\n    )\n    assert list(attention_mask_computed.size()) == [2, 1, 6, 6]\n    assert torch.all(attention_mask_computed == attention_mask_expected)\n"}
{"type": "test_file", "path": "train/LLaMA-Factory/tests/model/test_full.py", "content": "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\n\nimport torch\n\nfrom llamafactory.train.test_utils import load_infer_model, load_train_model\n\n\nTINY_LLAMA = os.getenv(\"TINY_LLAMA\", \"llamafactory/tiny-random-Llama-3\")\n\nTRAIN_ARGS = {\n    \"model_name_or_path\": TINY_LLAMA,\n    \"stage\": \"sft\",\n    \"do_train\": True,\n    \"finetuning_type\": \"full\",\n    \"dataset\": \"llamafactory/tiny-supervised-dataset\",\n    \"dataset_dir\": \"ONLINE\",\n    \"template\": \"llama3\",\n    \"cutoff_len\": 1024,\n    \"overwrite_cache\": True,\n    \"output_dir\": \"dummy_dir\",\n    \"overwrite_output_dir\": True,\n    \"fp16\": True,\n}\n\nINFER_ARGS = {\n    \"model_name_or_path\": TINY_LLAMA,\n    \"finetuning_type\": \"full\",\n    \"template\": \"llama3\",\n    \"infer_dtype\": \"float16\",\n}\n\n\ndef test_full_train():\n    model = load_train_model(**TRAIN_ARGS)\n    for param in model.parameters():\n        assert param.requires_grad is True\n        assert param.dtype == torch.float32\n\n\ndef test_full_inference():\n    model = load_infer_model(**INFER_ARGS)\n    for param in model.parameters():\n        assert param.requires_grad is False\n        assert param.dtype == torch.float16\n"}
{"type": "test_file", "path": "train/LLaMA-Factory/tests/eval/test_eval_template.py", "content": "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom llamafactory.eval.template import get_eval_template\n\n\ndef test_eval_template_en():\n    support_set = [\n        {\n            \"question\": \"Fewshot question\",\n            \"A\": \"Fewshot1\",\n            \"B\": \"Fewshot2\",\n            \"C\": \"Fewshot3\",\n            \"D\": \"Fewshot4\",\n            \"answer\": \"B\",\n        }\n    ]\n    example = {\n        \"question\": \"Target question\",\n        \"A\": \"Target1\",\n        \"B\": \"Target2\",\n        \"C\": \"Target3\",\n        \"D\": \"Target4\",\n        \"answer\": \"C\",\n    }\n    template = get_eval_template(name=\"en\")\n    messages = template.format_example(example, support_set=support_set, subject_name=\"SubName\")\n    assert messages == [\n        {\n            \"role\": \"user\",\n            \"content\": (\n                \"The following are multiple choice questions (with answers) about SubName.\\n\\n\"\n                \"Fewshot question\\nA. Fewshot1\\nB. Fewshot2\\nC. Fewshot3\\nD. Fewshot4\\nAnswer:\"\n            ),\n        },\n        {\"role\": \"assistant\", \"content\": \"B\"},\n        {\n            \"role\": \"user\",\n            \"content\": \"Target question\\nA. Target1\\nB. Target2\\nC. Target3\\nD. Target4\\nAnswer:\",\n        },\n        {\"role\": \"assistant\", \"content\": \"C\"},\n    ]\n\n\ndef test_eval_template_zh():\n    support_set = [\n        {\n            \"question\": \"示例问题\",\n            \"A\": \"示例答案1\",\n            \"B\": \"示例答案2\",\n            \"C\": \"示例答案3\",\n            \"D\": \"示例答案4\",\n            \"answer\": \"B\",\n        }\n    ]\n    example = {\n        \"question\": \"目标问题\",\n        \"A\": \"目标答案1\",\n        \"B\": \"目标答案2\",\n        \"C\": \"目标答案3\",\n        \"D\": \"目标答案4\",\n        \"answer\": \"C\",\n    }\n    template = get_eval_template(name=\"zh\")\n    messages = template.format_example(example, support_set=support_set, subject_name=\"主题\")\n    assert messages == [\n        {\n            \"role\": \"user\",\n            \"content\": (\n                \"以下是中国关于主题考试的单项选择题，请选出其中的正确答案。\\n\\n\"\n                \"示例问题\\nA. 示例答案1\\nB. 示例答案2\\nC. 示例答案3\\nD. 示例答案4\\n答案：\"\n            ),\n        },\n        {\"role\": \"assistant\", \"content\": \"B\"},\n        {\n            \"role\": \"user\",\n            \"content\": \"目标问题\\nA. 目标答案1\\nB. 目标答案2\\nC. 目标答案3\\nD. 目标答案4\\n答案：\",\n        },\n        {\"role\": \"assistant\", \"content\": \"C\"},\n    ]\n"}
{"type": "test_file", "path": "train/LLaMA-Factory/tests/data/processors/test_unsupervised.py", "content": "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport random\n\nimport pytest\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\nfrom llamafactory.train.test_utils import load_train_dataset\n\n\nDEMO_DATA = os.getenv(\"DEMO_DATA\", \"llamafactory/demo_data\")\n\nTINY_LLAMA = os.getenv(\"TINY_LLAMA\", \"llamafactory/tiny-random-Llama-3\")\n\nTINY_DATA = os.getenv(\"TINY_DATA\", \"llamafactory/tiny-supervised-dataset\")\n\nTRAIN_ARGS = {\n    \"model_name_or_path\": TINY_LLAMA,\n    \"stage\": \"ppo\",\n    \"do_train\": True,\n    \"finetuning_type\": \"full\",\n    \"reward_model\": \"\",\n    \"reward_model_type\": \"full\",\n    \"dataset\": \"system_chat\",\n    \"dataset_dir\": \"REMOTE:\" + DEMO_DATA,\n    \"template\": \"llama3\",\n    \"cutoff_len\": 8192,\n    \"overwrite_cache\": True,\n    \"output_dir\": \"dummy_dir\",\n    \"overwrite_output_dir\": True,\n    \"fp16\": True,\n}\n\n\n@pytest.mark.parametrize(\"num_samples\", [16])\ndef test_unsupervised_data(num_samples: int):\n    train_dataset = load_train_dataset(**TRAIN_ARGS)\n    ref_tokenizer = AutoTokenizer.from_pretrained(TINY_LLAMA)\n    original_data = load_dataset(DEMO_DATA, name=\"system_chat\", split=\"train\")\n    indexes = random.choices(range(len(original_data)), k=num_samples)\n    for index in indexes:\n        messages = original_data[\"messages\"][index]\n        ref_ids = ref_tokenizer.apply_chat_template(messages)\n        ref_input_ids = ref_tokenizer.apply_chat_template(messages[:-1], add_generation_prompt=True)\n        ref_labels = ref_ids[len(ref_input_ids) :]\n        assert train_dataset[\"input_ids\"][index] == ref_input_ids\n        assert train_dataset[\"labels\"][index] == ref_labels\n"}
{"type": "test_file", "path": "train/LLaMA-Factory/tests/model/test_freeze.py", "content": "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\n\nimport torch\n\nfrom llamafactory.train.test_utils import load_infer_model, load_train_model\n\n\nTINY_LLAMA = os.getenv(\"TINY_LLAMA\", \"llamafactory/tiny-random-Llama-3\")\n\nTRAIN_ARGS = {\n    \"model_name_or_path\": TINY_LLAMA,\n    \"stage\": \"sft\",\n    \"do_train\": True,\n    \"finetuning_type\": \"freeze\",\n    \"dataset\": \"llamafactory/tiny-supervised-dataset\",\n    \"dataset_dir\": \"ONLINE\",\n    \"template\": \"llama3\",\n    \"cutoff_len\": 1024,\n    \"overwrite_cache\": True,\n    \"output_dir\": \"dummy_dir\",\n    \"overwrite_output_dir\": True,\n    \"fp16\": True,\n}\n\nINFER_ARGS = {\n    \"model_name_or_path\": TINY_LLAMA,\n    \"finetuning_type\": \"freeze\",\n    \"template\": \"llama3\",\n    \"infer_dtype\": \"float16\",\n}\n\n\ndef test_freeze_train_all_modules():\n    model = load_train_model(freeze_trainable_layers=1, **TRAIN_ARGS)\n    for name, param in model.named_parameters():\n        if name.startswith(\"model.layers.1.\"):\n            assert param.requires_grad is True\n            assert param.dtype == torch.float32\n        else:\n            assert param.requires_grad is False\n            assert param.dtype == torch.float16\n\n\ndef test_freeze_train_extra_modules():\n    model = load_train_model(freeze_trainable_layers=1, freeze_extra_modules=\"embed_tokens,lm_head\", **TRAIN_ARGS)\n    for name, param in model.named_parameters():\n        if name.startswith(\"model.layers.1.\") or any(module in name for module in [\"embed_tokens\", \"lm_head\"]):\n            assert param.requires_grad is True\n            assert param.dtype == torch.float32\n        else:\n            assert param.requires_grad is False\n            assert param.dtype == torch.float16\n\n\ndef test_freeze_inference():\n    model = load_infer_model(**INFER_ARGS)\n    for param in model.parameters():\n        assert param.requires_grad is False\n        assert param.dtype == torch.float16\n"}
{"type": "test_file", "path": "train/LLaMA-Factory/tests/data/test_template.py", "content": "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nfrom typing import TYPE_CHECKING, Sequence\n\nimport pytest\nfrom transformers import AutoTokenizer\n\nfrom llamafactory.data import get_template_and_fix_tokenizer\nfrom llamafactory.data.template import _get_jinja_template\nfrom llamafactory.hparams import DataArguments\n\n\nif TYPE_CHECKING:\n    from transformers import PreTrainedTokenizer\n\n\nHF_TOKEN = os.getenv(\"HF_TOKEN\")\n\nTINY_LLAMA = os.getenv(\"TINY_LLAMA\", \"llamafactory/tiny-random-Llama-3\")\n\nMESSAGES = [\n    {\"role\": \"user\", \"content\": \"How are you\"},\n    {\"role\": \"assistant\", \"content\": \"I am fine!\"},\n    {\"role\": \"user\", \"content\": \"你好\"},\n    {\"role\": \"assistant\", \"content\": \"很高兴认识你！\"},\n]\n\n\ndef _check_tokenization(\n    tokenizer: \"PreTrainedTokenizer\", batch_input_ids: Sequence[Sequence[int]], batch_text: Sequence[str]\n) -> None:\n    r\"\"\"\n    Checks token ids and texts.\n\n    encode(text) == token_ids\n    decode(token_ids) == text\n    \"\"\"\n    for input_ids, text in zip(batch_input_ids, batch_text):\n        assert tokenizer.encode(text, add_special_tokens=False) == input_ids\n        assert tokenizer.decode(input_ids) == text\n\n\ndef _check_template(model_id: str, template_name: str, prompt_str: str, answer_str: str, use_fast: bool) -> None:\n    r\"\"\"\n    Checks template.\n\n    Args:\n        model_id: the model id on hugging face hub.\n        template_name: the template name.\n        prompt_str: the string corresponding to the prompt part.\n        answer_str: the string corresponding to the answer part.\n        use_fast: whether to use fast tokenizer.\n    \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=use_fast, token=HF_TOKEN)\n    content_str = tokenizer.apply_chat_template(MESSAGES, tokenize=False)\n    content_ids = tokenizer.apply_chat_template(MESSAGES, tokenize=True)\n    template = get_template_and_fix_tokenizer(tokenizer, DataArguments(template=template_name))\n    prompt_ids, answer_ids = template.encode_oneturn(tokenizer, MESSAGES)\n    assert content_str == prompt_str + answer_str\n    assert content_ids == prompt_ids + answer_ids\n    _check_tokenization(tokenizer, (prompt_ids, answer_ids), (prompt_str, answer_str))\n\n\n@pytest.mark.parametrize(\"use_fast\", [True, False])\ndef test_encode_oneturn(use_fast: bool):\n    tokenizer = AutoTokenizer.from_pretrained(TINY_LLAMA, use_fast=use_fast)\n    template = get_template_and_fix_tokenizer(tokenizer, DataArguments(template=\"llama3\"))\n    prompt_ids, answer_ids = template.encode_oneturn(tokenizer, MESSAGES)\n    prompt_str = (\n        \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nHow are you<|eot_id|>\"\n        \"<|start_header_id|>assistant<|end_header_id|>\\n\\nI am fine!<|eot_id|>\"\n        \"<|start_header_id|>user<|end_header_id|>\\n\\n你好<|eot_id|>\"\n        \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n    )\n    answer_str = \"很高兴认识你！<|eot_id|>\"\n    _check_tokenization(tokenizer, (prompt_ids, answer_ids), (prompt_str, answer_str))\n\n\n@pytest.mark.parametrize(\"use_fast\", [True, False])\ndef test_encode_multiturn(use_fast: bool):\n    tokenizer = AutoTokenizer.from_pretrained(TINY_LLAMA, use_fast=use_fast)\n    template = get_template_and_fix_tokenizer(tokenizer, DataArguments(template=\"llama3\"))\n    encoded_pairs = template.encode_multiturn(tokenizer, MESSAGES)\n    prompt_str_1 = (\n        \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nHow are you<|eot_id|>\"\n        \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n    )\n    answer_str_1 = \"I am fine!<|eot_id|>\"\n    prompt_str_2 = (\n        \"<|start_header_id|>user<|end_header_id|>\\n\\n你好<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n    )\n    answer_str_2 = \"很高兴认识你！<|eot_id|>\"\n    _check_tokenization(\n        tokenizer,\n        (encoded_pairs[0][0], encoded_pairs[0][1], encoded_pairs[1][0], encoded_pairs[1][1]),\n        (prompt_str_1, answer_str_1, prompt_str_2, answer_str_2),\n    )\n\n\n@pytest.mark.parametrize(\"use_fast\", [True, False])\ndef test_jinja_template(use_fast: bool):\n    tokenizer = AutoTokenizer.from_pretrained(TINY_LLAMA, use_fast=use_fast)\n    ref_tokenizer = AutoTokenizer.from_pretrained(TINY_LLAMA, use_fast=use_fast)\n    template = get_template_and_fix_tokenizer(tokenizer, DataArguments(template=\"llama3\"))\n    tokenizer.chat_template = _get_jinja_template(template, tokenizer)  # llama3 template no replace\n    assert tokenizer.chat_template != ref_tokenizer.chat_template\n    assert tokenizer.apply_chat_template(MESSAGES) == ref_tokenizer.apply_chat_template(MESSAGES)\n\n\ndef test_get_stop_token_ids():\n    tokenizer = AutoTokenizer.from_pretrained(TINY_LLAMA)\n    template = get_template_and_fix_tokenizer(tokenizer, DataArguments(template=\"llama3\"))\n    assert set(template.get_stop_token_ids(tokenizer)) == {128008, 128009}\n\n\n@pytest.mark.skipif(not HF_TOKEN, reason=\"Gated model.\")\n@pytest.mark.parametrize(\"use_fast\", [True, False])\ndef test_gemma_template(use_fast: bool):\n    prompt_str = (\n        \"<bos><start_of_turn>user\\nHow are you<end_of_turn>\\n\"\n        \"<start_of_turn>model\\nI am fine!<end_of_turn>\\n\"\n        \"<start_of_turn>user\\n你好<end_of_turn>\\n\"\n        \"<start_of_turn>model\\n\"\n    )\n    answer_str = \"很高兴认识你！<end_of_turn>\\n\"\n    _check_template(\"google/gemma-2-9b-it\", \"gemma\", prompt_str, answer_str, use_fast)\n\n\n@pytest.mark.skipif(not HF_TOKEN, reason=\"Gated model.\")\n@pytest.mark.parametrize(\"use_fast\", [True, False])\ndef test_llama3_template(use_fast: bool):\n    prompt_str = (\n        \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nHow are you<|eot_id|>\"\n        \"<|start_header_id|>assistant<|end_header_id|>\\n\\nI am fine!<|eot_id|>\"\n        \"<|start_header_id|>user<|end_header_id|>\\n\\n你好<|eot_id|>\"\n        \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n    )\n    answer_str = \"很高兴认识你！<|eot_id|>\"\n    _check_template(\"meta-llama/Meta-Llama-3-8B-Instruct\", \"llama3\", prompt_str, answer_str, use_fast)\n\n\n@pytest.mark.skipif(not HF_TOKEN, reason=\"Gated model.\")\n@pytest.mark.parametrize(\n    \"use_fast\", [True, pytest.param(False, marks=pytest.mark.xfail(reason=\"Phi-4 slow tokenizer is broken.\"))]\n)\ndef test_phi4_template(use_fast: bool):\n    prompt_str = (\n        \"<|im_start|>user<|im_sep|>How are you<|im_end|>\"\n        \"<|im_start|>assistant<|im_sep|>I am fine!<|im_end|>\"\n        \"<|im_start|>user<|im_sep|>你好<|im_end|>\"\n        \"<|im_start|>assistant<|im_sep|>\"\n    )\n    answer_str = \"很高兴认识你！<|im_end|>\"\n    _check_template(\"microsoft/phi-4\", \"phi4\", prompt_str, answer_str, use_fast)\n\n\n@pytest.mark.skipif(not HF_TOKEN, reason=\"Gated model.\")  # TODO: why it is gated?\n@pytest.mark.parametrize(\"use_fast\", [True, False])\ndef test_qwen_template(use_fast: bool):\n    prompt_str = (\n        \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n        \"<|im_start|>user\\nHow are you<|im_end|>\\n\"\n        \"<|im_start|>assistant\\nI am fine!<|im_end|>\\n\"\n        \"<|im_start|>user\\n你好<|im_end|>\\n\"\n        \"<|im_start|>assistant\\n\"\n    )\n    answer_str = \"很高兴认识你！<|im_end|>\\n\"\n    _check_template(\"Qwen/Qwen2-7B-Instruct\", \"qwen\", prompt_str, answer_str, use_fast)\n\n\n@pytest.mark.parametrize(\"use_fast\", [True, False])\n@pytest.mark.xfail(reason=\"Yi tokenizer is broken.\")\ndef test_yi_template(use_fast: bool):\n    prompt_str = (\n        \"<|im_start|>user\\nHow are you<|im_end|>\\n\"\n        \"<|im_start|>assistant\\nI am fine!<|im_end|>\\n\"\n        \"<|im_start|>user\\n你好<|im_end|>\\n\"\n        \"<|im_start|>assistant\\n\"\n    )\n    answer_str = \"很高兴认识你！<|im_end|>\\n\"\n    _check_template(\"01-ai/Yi-1.5-6B-Chat\", \"yi\", prompt_str, answer_str, use_fast)\n"}
{"type": "test_file", "path": "train/LLaMA-Factory/tests/model/test_pissa.py", "content": "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\n\nimport pytest\n\nfrom llamafactory.train.test_utils import compare_model, load_infer_model, load_reference_model, load_train_model\n\n\nTINY_LLAMA = os.getenv(\"TINY_LLAMA\", \"llamafactory/tiny-random-Llama-3\")\n\nTINY_LLAMA_PISSA = os.getenv(\"TINY_LLAMA_ADAPTER\", \"llamafactory/tiny-random-Llama-3-pissa\")\n\nTRAIN_ARGS = {\n    \"model_name_or_path\": TINY_LLAMA,\n    \"stage\": \"sft\",\n    \"do_train\": True,\n    \"finetuning_type\": \"lora\",\n    \"pissa_init\": True,\n    \"pissa_iter\": -1,\n    \"dataset\": \"llamafactory/tiny-supervised-dataset\",\n    \"dataset_dir\": \"ONLINE\",\n    \"template\": \"llama3\",\n    \"cutoff_len\": 1024,\n    \"overwrite_cache\": True,\n    \"output_dir\": \"dummy_dir\",\n    \"overwrite_output_dir\": True,\n    \"fp16\": True,\n}\n\nINFER_ARGS = {\n    \"model_name_or_path\": TINY_LLAMA_PISSA,\n    \"adapter_name_or_path\": TINY_LLAMA_PISSA,\n    \"adapter_folder\": \"pissa_init\",\n    \"finetuning_type\": \"lora\",\n    \"template\": \"llama3\",\n    \"infer_dtype\": \"float16\",\n}\n\nOS_NAME = os.getenv(\"OS_NAME\", \"\")\n\n\n@pytest.mark.xfail(reason=\"PiSSA initialization is not stable in different platform.\")\ndef test_pissa_train():\n    model = load_train_model(**TRAIN_ARGS)\n    ref_model = load_reference_model(TINY_LLAMA_PISSA, TINY_LLAMA_PISSA, use_pissa=True, is_trainable=True)\n    compare_model(model, ref_model)\n\n\n@pytest.mark.xfail(OS_NAME.startswith(\"windows\"), reason=\"Known connection error on Windows.\")\ndef test_pissa_inference():\n    model = load_infer_model(**INFER_ARGS)\n    ref_model = load_reference_model(TINY_LLAMA_PISSA, TINY_LLAMA_PISSA, use_pissa=True, is_trainable=False)\n    ref_model = ref_model.merge_and_unload()\n    compare_model(model, ref_model)\n"}
{"type": "test_file", "path": "train/LLaMA-Factory/src/llamafactory/train/test_utils.py", "content": "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import TYPE_CHECKING, Dict, Optional, Sequence, Set, Tuple, Union\n\nimport torch\nfrom peft import PeftModel\nfrom transformers import AutoModelForCausalLM\nfrom trl import AutoModelForCausalLMWithValueHead\n\nfrom ..data import get_dataset, get_template_and_fix_tokenizer\nfrom ..extras.misc import get_current_device\nfrom ..hparams import get_infer_args, get_train_args\nfrom ..model import load_model, load_tokenizer\n\n\nif TYPE_CHECKING:\n    from datasets import Dataset\n    from peft import LoraModel\n    from transformers import PreTrainedModel\n\n\ndef compare_model(model_a: \"torch.nn.Module\", model_b: \"torch.nn.Module\", diff_keys: Sequence[str] = []) -> None:\n    state_dict_a = model_a.state_dict()\n    state_dict_b = model_b.state_dict()\n    assert set(state_dict_a.keys()) == set(state_dict_b.keys())\n    for name in state_dict_a.keys():\n        if any(key in name for key in diff_keys):\n            assert torch.allclose(state_dict_a[name], state_dict_b[name], rtol=1e-4, atol=1e-5) is False\n        else:\n            assert torch.allclose(state_dict_a[name], state_dict_b[name], rtol=1e-4, atol=1e-5) is True\n\n\ndef check_lora_model(model: \"LoraModel\") -> Tuple[Set[str], Set[str]]:\n    linear_modules, extra_modules = set(), set()\n    for name, param in model.named_parameters():\n        if any(module in name for module in [\"lora_A\", \"lora_B\"]):\n            linear_modules.add(name.split(\".lora_\", maxsplit=1)[0].split(\".\")[-1])\n            assert param.requires_grad is True\n            assert param.dtype == torch.float32\n        elif \"modules_to_save\" in name:\n            extra_modules.add(name.split(\".modules_to_save\", maxsplit=1)[0].split(\".\")[-1])\n            assert param.requires_grad is True\n            assert param.dtype == torch.float32\n        else:\n            assert param.requires_grad is False\n            assert param.dtype == torch.float16\n\n    return linear_modules, extra_modules\n\n\ndef load_train_model(add_valuehead: bool = False, **kwargs) -> \"PreTrainedModel\":\n    model_args, _, _, finetuning_args, _ = get_train_args(kwargs)\n    tokenizer = load_tokenizer(model_args)[\"tokenizer\"]\n    return load_model(tokenizer, model_args, finetuning_args, is_trainable=True, add_valuehead=add_valuehead)\n\n\ndef load_infer_model(add_valuehead: bool = False, **kwargs) -> \"PreTrainedModel\":\n    model_args, _, finetuning_args, _ = get_infer_args(kwargs)\n    tokenizer = load_tokenizer(model_args)[\"tokenizer\"]\n    return load_model(tokenizer, model_args, finetuning_args, is_trainable=False, add_valuehead=add_valuehead)\n\n\ndef load_reference_model(\n    model_path: str,\n    lora_path: Optional[str] = None,\n    use_lora: bool = False,\n    use_pissa: bool = False,\n    is_trainable: bool = False,\n    add_valuehead: bool = False,\n) -> Union[\"PreTrainedModel\", \"LoraModel\"]:\n    current_device = get_current_device()\n    if add_valuehead:\n        model: \"AutoModelForCausalLMWithValueHead\" = AutoModelForCausalLMWithValueHead.from_pretrained(\n            model_path, torch_dtype=torch.float16, device_map=current_device\n        )\n        if not is_trainable:\n            model.v_head = model.v_head.to(torch.float16)\n\n        return model\n\n    model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=current_device)\n    if use_lora or use_pissa:\n        model = PeftModel.from_pretrained(\n            model, lora_path, subfolder=\"pissa_init\" if use_pissa else None, is_trainable=is_trainable\n        )\n        for param in filter(lambda p: p.requires_grad, model.parameters()):\n            param.data = param.data.to(torch.float32)\n\n    return model\n\n\ndef load_train_dataset(**kwargs) -> \"Dataset\":\n    model_args, data_args, training_args, _, _ = get_train_args(kwargs)\n    tokenizer_module = load_tokenizer(model_args)\n    template = get_template_and_fix_tokenizer(tokenizer_module[\"tokenizer\"], data_args)\n    dataset_module = get_dataset(template, model_args, data_args, training_args, kwargs[\"stage\"], **tokenizer_module)\n    return dataset_module[\"train_dataset\"]\n\n\ndef patch_valuehead_model() -> None:\n    def post_init(self: \"AutoModelForCausalLMWithValueHead\", state_dict: Dict[str, \"torch.Tensor\"]) -> None:\n        state_dict = {k[7:]: state_dict[k] for k in state_dict.keys() if k.startswith(\"v_head.\")}\n        self.v_head.load_state_dict(state_dict, strict=False)\n        del state_dict\n\n    AutoModelForCausalLMWithValueHead.post_init = post_init\n"}
{"type": "test_file", "path": "train/LLaMA-Factory/tests/data/processors/test_supervised.py", "content": "# Copyright 2025 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport random\n\nimport pytest\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\nfrom llamafactory.extras.constants import IGNORE_INDEX\nfrom llamafactory.train.test_utils import load_train_dataset\n\n\nDEMO_DATA = os.getenv(\"DEMO_DATA\", \"llamafactory/demo_data\")\n\nTINY_LLAMA = os.getenv(\"TINY_LLAMA\", \"llamafactory/tiny-random-Llama-3\")\n\nTINY_DATA = os.getenv(\"TINY_DATA\", \"llamafactory/tiny-supervised-dataset\")\n\nTRAIN_ARGS = {\n    \"model_name_or_path\": TINY_LLAMA,\n    \"stage\": \"sft\",\n    \"do_train\": True,\n    \"finetuning_type\": \"full\",\n    \"template\": \"llama3\",\n    \"cutoff_len\": 8192,\n    \"overwrite_cache\": True,\n    \"output_dir\": \"dummy_dir\",\n    \"overwrite_output_dir\": True,\n    \"fp16\": True,\n}\n\n\n@pytest.mark.parametrize(\"num_samples\", [16])\ndef test_supervised_single_turn(num_samples: int):\n    train_dataset = load_train_dataset(dataset_dir=\"ONLINE\", dataset=TINY_DATA, **TRAIN_ARGS)\n    ref_tokenizer = AutoTokenizer.from_pretrained(TINY_LLAMA)\n    original_data = load_dataset(TINY_DATA, split=\"train\")\n    indexes = random.choices(range(len(original_data)), k=num_samples)\n    for index in indexes:\n        prompt = original_data[\"instruction\"][index]\n        if original_data[\"input\"][index]:\n            prompt += \"\\n\" + original_data[\"input\"][index]\n\n        messages = [\n            {\"role\": \"user\", \"content\": prompt},\n            {\"role\": \"assistant\", \"content\": original_data[\"output\"][index]},\n        ]\n        ref_input_ids = ref_tokenizer.apply_chat_template(messages)\n        assert train_dataset[\"input_ids\"][index] == ref_input_ids\n\n\n@pytest.mark.parametrize(\"num_samples\", [8])\ndef test_supervised_multi_turn(num_samples: int):\n    train_dataset = load_train_dataset(dataset_dir=\"REMOTE:\" + DEMO_DATA, dataset=\"system_chat\", **TRAIN_ARGS)\n    ref_tokenizer = AutoTokenizer.from_pretrained(TINY_LLAMA)\n    original_data = load_dataset(DEMO_DATA, name=\"system_chat\", split=\"train\")\n    indexes = random.choices(range(len(original_data)), k=num_samples)\n    for index in indexes:\n        ref_input_ids = ref_tokenizer.apply_chat_template(original_data[\"messages\"][index])\n        assert train_dataset[\"input_ids\"][index] == ref_input_ids\n\n\n@pytest.mark.parametrize(\"num_samples\", [4])\ndef test_supervised_train_on_prompt(num_samples: int):\n    train_dataset = load_train_dataset(\n        dataset_dir=\"REMOTE:\" + DEMO_DATA, dataset=\"system_chat\", train_on_prompt=True, **TRAIN_ARGS\n    )\n    ref_tokenizer = AutoTokenizer.from_pretrained(TINY_LLAMA)\n    original_data = load_dataset(DEMO_DATA, name=\"system_chat\", split=\"train\")\n    indexes = random.choices(range(len(original_data)), k=num_samples)\n    for index in indexes:\n        ref_ids = ref_tokenizer.apply_chat_template(original_data[\"messages\"][index])\n        assert train_dataset[\"input_ids\"][index] == ref_ids\n        assert train_dataset[\"labels\"][index] == ref_ids\n\n\n@pytest.mark.parametrize(\"num_samples\", [4])\ndef test_supervised_mask_history(num_samples: int):\n    train_dataset = load_train_dataset(\n        dataset_dir=\"REMOTE:\" + DEMO_DATA, dataset=\"system_chat\", mask_history=True, **TRAIN_ARGS\n    )\n    ref_tokenizer = AutoTokenizer.from_pretrained(TINY_LLAMA)\n    original_data = load_dataset(DEMO_DATA, name=\"system_chat\", split=\"train\")\n    indexes = random.choices(range(len(original_data)), k=num_samples)\n    for index in indexes:\n        messages = original_data[\"messages\"][index]\n        ref_input_ids = ref_tokenizer.apply_chat_template(messages)\n        prompt_len = len(ref_tokenizer.apply_chat_template(messages[:-1], add_generation_prompt=True))\n        ref_label_ids = [IGNORE_INDEX] * prompt_len + ref_input_ids[prompt_len:]\n        assert train_dataset[\"input_ids\"][index] == ref_input_ids\n        assert train_dataset[\"labels\"][index] == ref_label_ids\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/filters/selection.py", "content": "from collections import Counter\r\n\r\nfrom lm_eval.api.filter import Filter\r\nfrom lm_eval.api.registry import register_filter\r\n\r\n\r\n# TODO: implement \"arg_max\" filter. either it should take in an arbitrary \"scoring\"/reward function\r\n# that takes an input and returns a scalar and then should select the max reward,\r\n# or should implement different filters for different ways of handling a reward model's inference.\r\n\r\n\r\n@register_filter(\"take_first\")\r\nclass TakeFirstFilter(Filter):\r\n    def __init__(self) -> None:\r\n        \"\"\"\r\n        Can define custom behavior here, if an individual instantiation of a Filter class should have state.\r\n        \"\"\"\r\n\r\n    def apply(self, resps, docs):\r\n        \"\"\"\r\n        Assuming each entry of `resps` is a list of model responses, we discard all but the first response.\r\n        \"\"\"\r\n        return map(lambda r: r[0], resps)\r\n\r\n\r\n@register_filter(\"take_first_k\")\r\nclass TakeKFilter(Filter):\r\n    def __init__(self, **kwargs) -> None:\r\n        self.k = kwargs.pop(\"k\")\r\n\r\n        super().__init__(**kwargs)\r\n\r\n    def apply(self, resps, docs):\r\n        # need resp to be subscriptable to check below\r\n        resps = list(resps)\r\n        # check we have at least k responses per doc, else we can't take the first k\r\n        assert (\r\n            len(resps[0]) >= self.k\r\n        ), f\"Need at least {self.k} responses per doc to take first {self.k}, but got {len(resps[0])} only! Please increase TaskConfig.repeats .\"\r\n        return map(lambda r: r[: self.k], resps)\r\n\r\n\r\n@register_filter(\"majority_vote\")\r\nclass MajorityVoteFilter(Filter):\r\n    def __init__(self) -> None:\r\n        \"\"\"\r\n        Can define custom behavior here, if an individual instantiation of a Filter class should have state.\r\n        \"\"\"\r\n\r\n    def apply(self, resps, docs):\r\n        \"\"\"\r\n        Each entry of `resps` is a list of model responses.\r\n        We select the response that occurs most frequently in each entry of `resps`.\r\n        \"\"\"\r\n\r\n        def select_majority(resp):\r\n            counts = Counter(resp)\r\n            vote = counts.most_common(1)[0][0]\r\n            return vote\r\n\r\n        return map(lambda r: [select_majority(r)], resps)\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/filters/transformation.py", "content": "from lm_eval.api.filter import Filter\r\nfrom lm_eval.api.registry import register_filter\r\n\r\n\r\n@register_filter(\"lowercase\")\r\nclass LowercaseFilter(Filter):\r\n    def __init__(self) -> None:\r\n        pass\r\n\r\n    def apply(self, resps, docs):\r\n        def filter_set(inst):\r\n            return [resp.lower() for resp in inst]\r\n\r\n        return [filter_set(resp) for resp in resps]\r\n\r\n\r\n@register_filter(\"uppercase\")\r\nclass UppercaseFilter(Filter):\r\n    def __init__(self) -> None:\r\n        pass\r\n\r\n    def apply(self, resps, docs):\r\n        def filter_set(inst):\r\n            return [resp.upper() for resp in inst]\r\n\r\n        return [filter_set(resp) for resp in resps]\r\n\r\n\r\n@register_filter(\"map\")\r\nclass MapFilter(Filter):\r\n    def __init__(self, mapping_dict: dict = None, default_value=None) -> None:\r\n        \"\"\"\r\n        Initializes the MapFilter with a given mapping dictionary and default value.\r\n\r\n        Args:\r\n        - mapping_dict (dict): A dictionary containing the key-value mappings.\r\n                               Default is an empty dictionary.\r\n        - default_value (Any): The value to be returned when a key is not found in the mapping_dict.\r\n                               Default is None.\r\n\r\n        Example:\r\n        mapper = MapFilter({'A': 1, 'B': 2}, default_value=0)\r\n        \"\"\"\r\n        if mapping_dict is None:\r\n            mapping_dict = {}\r\n        assert isinstance(\r\n            mapping_dict, dict\r\n        ), \"Provided mapping_dict is not a dictionary\"\r\n        self.mapping_dict = mapping_dict\r\n        self.default_value = default_value\r\n\r\n    def apply(self, resps, docs):\r\n        def filter_set(inst):\r\n            return [self.mapping_dict.get(resp, self.default_value) for resp in inst]\r\n\r\n        return [filter_set(resp) for resp in resps]\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/loggers/__init__.py", "content": "from .evaluation_tracker import EvaluationTracker\r\nfrom .wandb_logger import WandbLogger\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/models/anthropic_llms.py", "content": "import os\r\nfrom functools import cached_property\r\nfrom typing import Any, Dict, List, Tuple, Union\r\n\r\nfrom tqdm import tqdm\r\n\r\nfrom lm_eval import utils\r\nfrom lm_eval.api.model import LM\r\nfrom lm_eval.api.registry import register_model\r\nfrom lm_eval.models.openai_completions import LocalCompletionsAPI\r\nfrom lm_eval.models.utils import retry_on_specific_exceptions\r\n\r\n\r\neval_logger = utils.eval_logger\r\n\r\n\r\ndef anthropic_completion(\r\n    client,  #: anthropic.Anthropic,\r\n    model: str,\r\n    prompt: str,\r\n    max_tokens_to_sample: int,\r\n    temperature: float,\r\n    stop: List[str],\r\n    **kwargs: Any,\r\n) -> str:\r\n    \"\"\"Wrapper function around the Anthropic completion API client with exponential back-off\r\n    in case of RateLimitError.\r\n\r\n    params:\r\n        client: anthropic.Anthropic\r\n            Anthropic API client\r\n        model: str\r\n            Anthropic model e.g. 'claude-instant-v1', 'claude-2'\r\n        prompt: str\r\n            Prompt to feed to the model\r\n        max_tokens_to_sample: int\r\n            Maximum number of tokens to sample from the model\r\n        temperature: float\r\n            Sampling temperature\r\n        stop: List[str]\r\n            List of stop sequences\r\n        kwargs: Any\r\n            Additional model_args to pass to the API client\r\n    \"\"\"\r\n\r\n    try:\r\n        import anthropic\r\n    except ModuleNotFoundError:\r\n        raise Exception(\r\n            \"attempted to use 'anthropic' LM type, but package `anthropic` is not installed. \\\r\nplease install anthropic via `pip install 'lm-eval[anthropic]'` or `pip install -e '.[anthropic]'`\",\r\n        )\r\n\r\n    def _exception_callback(e: Exception, sleep_time: float) -> None:\r\n        eval_logger.warning(\r\n            f\"RateLimitError occurred: {e.__cause__}\\n Retrying in {sleep_time} seconds\"\r\n        )\r\n\r\n    @retry_on_specific_exceptions(\r\n        on_exceptions=[anthropic.RateLimitError],\r\n        max_retries=None,  # retry forever, consider changing\r\n        on_exception_callback=_exception_callback,\r\n    )\r\n    def completion():\r\n        response = client.completions.create(\r\n            prompt=f\"{anthropic.HUMAN_PROMPT} {prompt}{anthropic.AI_PROMPT}\",\r\n            model=model,\r\n            # NOTE: Claude really likes to do CoT, and overly aggressive stop sequences\r\n            #       (e.g. gsm8k's \":\") may truncate a lot of the input.\r\n            stop_sequences=[anthropic.HUMAN_PROMPT] + stop,\r\n            max_tokens_to_sample=max_tokens_to_sample,\r\n            temperature=temperature,\r\n            **kwargs,\r\n        )\r\n        return response.completion\r\n\r\n    return completion()\r\n\r\n\r\ndef anthropic_chat(\r\n    client,  #: anthropic.Anthropic,\r\n    model: str,\r\n    prompt: str,\r\n    max_tokens: int,\r\n    temperature: float,\r\n    stop: List[str],\r\n    **kwargs: Any,\r\n) -> str:\r\n    \"\"\"Wrapper function around the Anthropic completion API client with exponential back-off\r\n    in case of RateLimitError.\r\n\r\n    params:\r\n        client: anthropic.Anthropic\r\n            Anthropic API client\r\n        model: str\r\n            Anthropic model e.g. 'claude-3-opus-20240229', 'claude-3-sonnet-20240229'\r\n        prompt: str\r\n            Prompt to feed to the model\r\n        max_tokens: int\r\n            Maximum number of tokens to sample from the model\r\n        temperature: float\r\n            Sampling temperature\r\n        stop: List[str]\r\n            List of stop sequences\r\n        kwargs: Any\r\n            Additional model_args to pass to the API client\r\n    \"\"\"\r\n\r\n    try:\r\n        import anthropic\r\n    except ModuleNotFoundError:\r\n        raise Exception(\r\n            \"attempted to use 'anthropic' LM type, but package `anthropic` is not installed. \\\r\nplease install anthropic via `pip install 'lm-eval[anthropic]'` or `pip install -e '.[anthropic]'`\",\r\n        )\r\n\r\n    def _exception_callback(e: Exception, sleep_time: float) -> None:\r\n        eval_logger.warning(\r\n            f\"RateLimitError occurred: {e.__cause__}\\n Retrying in {sleep_time} seconds\"\r\n        )\r\n\r\n    @retry_on_specific_exceptions(\r\n        on_exceptions=[\r\n            anthropic.RateLimitError,\r\n            anthropic.APIConnectionError,\r\n            anthropic.APIStatusError,\r\n        ],\r\n        max_retries=None,  # retry forever, consider changing\r\n        on_exception_callback=_exception_callback,\r\n    )\r\n    def messages():\r\n        response = client.messages.create(\r\n            model=model,\r\n            max_tokens=max_tokens,\r\n            temperature=temperature,\r\n            messages=[{\"role\": \"user\", \"content\": f\"{prompt}\"}],\r\n            **kwargs,\r\n        )\r\n        return response.content[0].text\r\n\r\n    return messages()\r\n\r\n\r\n@register_model(\"anthropic-completions\")\r\nclass AnthropicLM(LM):\r\n    REQ_CHUNK_SIZE = 20  # TODO: not used\r\n\r\n    def __init__(\r\n        self,\r\n        batch_size: int = 1,\r\n        model: str = \"claude-2.0\",\r\n        max_tokens_to_sample: int = 256,\r\n        temperature: float = 0,  # defaults to 1\r\n        **kwargs,  # top_p, top_k, etc.\r\n    ) -> None:\r\n        \"\"\"Anthropic API wrapper.\r\n\r\n        :param model: str\r\n            Anthropic model e.g. 'claude-instant-v1', 'claude-2'\r\n        :param max_tokens_to_sample: int\r\n            Maximum number of tokens to sample from the model\r\n        :param temperature: float\r\n            Sampling temperature\r\n        :param kwargs: Any\r\n            Additional model_args to pass to the API client\r\n        \"\"\"\r\n        super().__init__()\r\n\r\n        try:\r\n            import anthropic\r\n        except ModuleNotFoundError:\r\n            raise Exception(\r\n                \"attempted to use 'anthropic' LM type, but package `anthropic` is not installed. \\\r\nplease install anthropic via `pip install 'lm-eval[anthropic]'` or `pip install -e '.[anthropic]'`\",\r\n            )\r\n\r\n        self.model = model\r\n        # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\r\n        self.client = anthropic.Anthropic()\r\n        self.temperature = temperature\r\n        self.max_tokens_to_sample = max_tokens_to_sample\r\n        self.tokenizer = self.client.get_tokenizer()\r\n        self.kwargs = kwargs\r\n\r\n    @property\r\n    def eot_token_id(self):\r\n        # Not sure but anthropic.HUMAN_PROMPT ?\r\n        raise NotImplementedError(\"No idea about anthropic tokenization.\")\r\n\r\n    @property\r\n    def max_length(self) -> int:\r\n        return 2048\r\n\r\n    @property\r\n    def max_gen_toks(self) -> int:\r\n        return self.max_tokens_to_sample\r\n\r\n    @property\r\n    def batch_size(self):\r\n        # Isn't used because we override _loglikelihood_tokens\r\n        raise NotImplementedError(\"No support for logits.\")\r\n\r\n    @property\r\n    def device(self):\r\n        # Isn't used because we override _loglikelihood_tokens\r\n        raise NotImplementedError(\"No support for logits.\")\r\n\r\n    def tok_encode(self, string: str) -> List[int]:\r\n        return self.tokenizer.encode(string).ids\r\n\r\n    def tok_decode(self, tokens: List[int]) -> str:\r\n        return self.tokenizer.decode(tokens)\r\n\r\n    def _loglikelihood_tokens(self, requests, disable_tqdm: bool = False):\r\n        raise NotImplementedError(\"No support for logits.\")\r\n\r\n    def generate_until(self, requests, disable_tqdm: bool = False) -> List[str]:\r\n        try:\r\n            import anthropic\r\n        except ModuleNotFoundError:\r\n            raise Exception(\r\n                \"attempted to use 'anthropic' LM type, but package `anthropic` is not installed. \\\r\nplease install anthropic via `pip install 'lm-eval[anthropic]'` or `pip install -e '.[anthropic]'`\",\r\n            )\r\n\r\n        if not requests:\r\n            return []\r\n\r\n        _requests: List[Tuple[str, dict]] = [req.args for req in requests]\r\n\r\n        res = []\r\n        for request in tqdm(_requests, disable=disable_tqdm):\r\n            try:\r\n                inp = request[0]\r\n                request_args = request[1]\r\n                # generation_kwargs\r\n                until = request_args.get(\"until\")\r\n                max_gen_toks = request_args.get(\"max_gen_toks\", self.max_length)\r\n                temperature = request_args.get(\"temperature\", self.temperature)\r\n                response = anthropic_completion(\r\n                    client=self.client,\r\n                    model=self.model,\r\n                    prompt=inp,\r\n                    max_tokens_to_sample=max_gen_toks,\r\n                    temperature=temperature,  # TODO: implement non-greedy sampling for Anthropic\r\n                    stop=until,  # type: ignore\r\n                    **self.kwargs,\r\n                )\r\n                res.append(response)\r\n\r\n                self.cache_hook.add_partial(\"generate_until\", request, response)\r\n            except anthropic.APIConnectionError as e:  # type: ignore # noqa: F821\r\n                eval_logger.critical(f\"Server unreachable: {e.__cause__}\")\r\n                break\r\n            except anthropic.APIStatusError as e:  # type: ignore # noqa: F821\r\n                eval_logger.critical(f\"API error {e.status_code}: {e.message}\")\r\n                break\r\n\r\n        return res\r\n\r\n    def _model_call(self, inps):\r\n        # Isn't used because we override _loglikelihood_tokens\r\n        raise NotImplementedError()\r\n\r\n    def _model_generate(self, context, max_length, eos_token_id):\r\n        # Isn't used because we override generate_until\r\n        raise NotImplementedError()\r\n\r\n    def loglikelihood(self, requests, disable_tqdm: bool = False):\r\n        raise NotImplementedError(\"No support for logits.\")\r\n\r\n    def loglikelihood_rolling(self, requests, disable_tqdm: bool = False):\r\n        raise NotImplementedError(\"No support for logits.\")\r\n\r\n\r\n@register_model(\"anthropic-chat\", \"anthropic-chat-completions\")\r\nclass AnthropicChat(LocalCompletionsAPI):\r\n    def __init__(\r\n        self,\r\n        base_url=\"https://api.anthropic.com/v1/messages\",\r\n        tokenizer_backend=None,\r\n        **kwargs,\r\n    ):\r\n        super().__init__(\r\n            base_url=base_url, tokenizer_backend=tokenizer_backend, **kwargs\r\n        )\r\n        eval_logger.warning(\r\n            \"Chat completions does not support batching. Defaulting to batch size 1.\"\r\n        )\r\n        self._batch_size = 1\r\n        self.anthropic_version = \"2023-06-01\"\r\n        eval_logger.warning(\r\n            f\"Using Anthropic Version: {self.anthropic_version}. Confirm the current version here: https://docs.anthropic.com/en/api/versioning\"\r\n        )\r\n\r\n    @cached_property\r\n    def api_key(self):\r\n        \"\"\"Override this property to return the API key for the API request.\"\"\"\r\n        key = os.environ.get(\"ANTHROPIC_API_KEY\", None)\r\n        if key is None:\r\n            raise ValueError(\r\n                \"API key not found. Please set the ANTHROPIC_API_KEY environment variable.\"\r\n            )\r\n        return key\r\n\r\n    @cached_property\r\n    def header(self):\r\n        return {\r\n            \"x-api-key\": f\"{self.api_key}\",\r\n            \"anthropic-version\": self.anthropic_version,\r\n        }\r\n\r\n    def _create_payload(\r\n        self, messages: List[Dict], generate=True, gen_kwargs: dict = None, **kwargs\r\n    ) -> dict:\r\n        system = (\r\n            messages[0].get(\"content\") if messages[0].get(\"role\") == \"system\" else None\r\n        )\r\n        if system:\r\n            messages = messages[1:]\r\n        gen_kwargs.pop(\"do_sample\", False)\r\n        max_tokens = gen_kwargs.pop(\"max_gen_toks\", self._max_gen_toks)\r\n        temperature = gen_kwargs.pop(\"temperature\", 0)\r\n        stop = gen_kwargs.pop(\"until\", [\"\\n\\nHuman:\"])\r\n        if not isinstance(stop, list):\r\n            stop = [stop]\r\n        out = {\r\n            \"messages\": messages,\r\n            \"model\": self.model,\r\n            \"max_tokens\": max_tokens,\r\n            \"temperature\": temperature,\r\n            \"stop_sequences\": stop,\r\n            **gen_kwargs,\r\n        }\r\n        if system:\r\n            out[\"system\"] = system\r\n        return out\r\n\r\n    def parse_generations(\r\n        self, outputs: Union[Dict, List[Dict]], **kwargs\r\n    ) -> List[str]:\r\n        res = []\r\n        if not isinstance(outputs, list):\r\n            outputs = [outputs]\r\n        for out in outputs:\r\n            for choices in out[\"content\"]:\r\n                res.append(choices[\"text\"])\r\n        return res\r\n\r\n    def tok_encode(\r\n        self,\r\n        string: str,\r\n        left_truncate_len=None,\r\n        add_special_tokens=None,\r\n        **kwargs,\r\n    ) -> List[str]:\r\n        return [string]\r\n\r\n    def loglikelihood(self, requests, **kwargs):\r\n        raise NotImplementedError(\r\n            \"Anthropic Chat Completions API does not support the return of loglikelihood\"\r\n        )\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/models/hf_vlms.py", "content": "import copy\r\nfrom typing import Dict, List, Optional, Tuple, Union\r\n\r\nimport torch\r\nimport torch.nn.functional as F\r\nimport transformers\r\nfrom tqdm import tqdm\r\nfrom transformers import BatchEncoding\r\n\r\nfrom lm_eval import utils\r\nfrom lm_eval.api.instance import Instance\r\nfrom lm_eval.api.registry import register_model\r\nfrom lm_eval.models.huggingface import HFLM\r\nfrom lm_eval.models.utils import (\r\n    Collator,\r\n    pad_and_concat,\r\n    replace_placeholders,\r\n    stop_sequences_criteria,\r\n)\r\n\r\n\r\nDEFAULT_IMAGE_PLACEHOLDER = \"<image>\"\r\n\r\n\r\neval_logger = utils.eval_logger\r\n\r\n\r\n@register_model(\"hf-multimodal\")\r\nclass HFMultimodalLM(HFLM):\r\n    \"\"\"\r\n    An abstracted Hugging Face model class for multimodal LMs like Llava and Idefics.\r\n    \"\"\"\r\n\r\n    AUTO_MODEL_CLASS = transformers.AutoModelForVision2Seq\r\n    MULTIMODAL = True  # flag to indicate, for now, that this model type can run multimodal requests\r\n\r\n    def __init__(\r\n        self,\r\n        pretrained: Union[str, transformers.PreTrainedModel],\r\n        image_token_id: Optional[int] = None,\r\n        image_string: Optional[str] = None,\r\n        interleave: bool = True,\r\n        # TODO: handle whitespace in image placeholder (replacement)\r\n        max_images: Optional[int] = 999,\r\n        convert_img_format=False,\r\n        **kwargs,\r\n    ):\r\n        # We initialize using HFLM's init. Sub-methods like _create_model and _create_tokenizer\r\n        # modify init behavior.\r\n        super().__init__(pretrained, **kwargs)\r\n\r\n        assert (\r\n            self.batch_size != \"auto\"\r\n        ), \"Batch size 'auto' is not yet supported for hf-multimodal models.\"\r\n        self.chat_applied: bool = False\r\n        # TODO: phi-3.5 \"image placeholders\" are <image_1>, <image_2>, ... in order. how to handle this case\r\n\r\n        # HF AutoModelForVision2Seq models have an `image_token_id` value in their configs\r\n        # denoting the token which indicates a location where an image will be substituted in.\r\n        # This can take different string values across models, e.g. <image> for Idefics2 and <|image_pad|> for Qwen2-VL\r\n        self.interleave = interleave\r\n        self.max_images = max_images\r\n        self.rgb = convert_img_format\r\n        # WARNING: improperly set image_token_id can lead to ignored image input or other (potentially silent) errors!\r\n        if not image_string:\r\n            self.image_token_id = (\r\n                int(image_token_id)\r\n                if image_token_id\r\n                else (\r\n                    getattr(self.config, \"image_token_id\", None)\r\n                    or getattr(self.config, \"image_token_index\", None)\r\n                )\r\n            )\r\n            assert (\r\n                self.image_token_id is not None\r\n            ), \"Must have a non-None image_token_id to evaluate a Hugging Face AutoModelForVision2Seq model. Please pass `image_token_id` in `--model_args` if model's config does not already specify one.\"\r\n            # get the string this token ID corresponds to\r\n            self.image_token = self.tok_decode(\r\n                [self.image_token_id], skip_special_tokens=False\r\n            )\r\n            if image_token_id is not None:\r\n                eval_logger.info(\r\n                    f\"A non-default image_token_id with image_token_id={self.image_token_id} and string value '{self.image_token}' was specified manually. Note that using an improper image_token placeholder may lead to ignored image input or errors!\"\r\n                )\r\n        else:\r\n            eval_logger.info(\r\n                f\"A non-default image_token string with string value image_string='{image_string}' was specified manually. Note that using an improper image_token placeholder may lead to ignored image input or errors!\"\r\n            )\r\n            self.image_token = image_string\r\n\r\n    def _create_tokenizer(\r\n        self,\r\n        pretrained: Union[str, transformers.PreTrainedModel],\r\n        tokenizer: Optional[\r\n            Union[\r\n                str,\r\n                transformers.ProcessorMixin,\r\n            ]\r\n        ],\r\n        revision: Optional[str] = \"main\",\r\n        trust_remote_code: Optional[bool] = False,\r\n        **kwargs,\r\n    ) -> None:\r\n        \"\"\"\r\n        Helper method during initialization.\r\n\r\n        For the multimodal variant, we initialize not just\r\n        `self.tokenizer` but also `self.processor`.\r\n        \"\"\"\r\n\r\n        if tokenizer:\r\n            if isinstance(tokenizer, str):\r\n                return transformers.AutoProcessor.from_pretrained(\r\n                    tokenizer,\r\n                    revision=revision,\r\n                    trust_remote_code=trust_remote_code,\r\n                    # use_fast=use_fast_tokenizer,\r\n                )\r\n            else:\r\n                assert isinstance(\r\n                    tokenizer, transformers.ProcessorMixin\r\n                )  # TODO: check this condition\r\n                return tokenizer\r\n\r\n        # Get tokenizer based on 'pretrained'\r\n        if isinstance(pretrained, str):\r\n            model_name = pretrained\r\n        else:\r\n            # get the HF hub name via accessor on model\r\n            model_name = self.model.name_or_path\r\n\r\n        self.processor = transformers.AutoProcessor.from_pretrained(\r\n            model_name,\r\n            revision=revision,\r\n            trust_remote_code=trust_remote_code,\r\n            # use_fast=use_fast_tokenizer,\r\n        )\r\n\r\n        self.tokenizer = self.processor.tokenizer\r\n\r\n    def tok_multimodal_encode(\r\n        self, string, images, left_truncate_len=None, add_special_tokens=None\r\n    ):\r\n        \"\"\"Helper function which encodes an image + string combo using AutoProcessor\"\"\"\r\n        # We inherit special token kwarg setup from HFLM.tok_encode\r\n        # special_tokens_kwargs = {}\r\n\r\n        # by default for CausalLM - false or self.add_bos_token is set\r\n        # if add_special_tokens is None:\r\n        #     special_tokens_kwargs = {\"add_special_tokens\": False or self.add_bos_token}\r\n        # otherwise the method explicitly defines the value\r\n        # else:\r\n        #     special_tokens_kwargs = {\"add_special_tokens\": add_special_tokens}\r\n\r\n        # encode text+images\r\n        # TODO: why does (Qwen2-VL) processor error when attempting to add special tokens to text?\r\n        encoding = self.processor(\r\n            text=string, images=images, return_tensors=None\r\n        )  # , **special_tokens_kwargs)\r\n\r\n        # remove (and store) our tokenized text\r\n        text_encoding = encoding.pop(\"input_ids\")\r\n        encoding.pop(\"attention_mask\")\r\n\r\n        # left-truncate the encoded context to be at most `left_truncate_len` tokens long\r\n        if left_truncate_len:\r\n            text_encoding = text_encoding[-left_truncate_len:]\r\n\r\n        return text_encoding, encoding  # image_encoding is a dict\r\n\r\n    def _encode_multimodal_pair(self, context, continuation, images):\r\n        \"\"\"Helper function to perform the role of TemplateLM._encode_pair\r\n        Except allowing for image input to also be processed alongside `context`.\r\n\r\n        This method is a bit messy due to the need to defer conversion of image and text token input\r\n        into PyTorch tensors until the main inference loop.\r\n        \"\"\"\r\n\r\n        n_spaces = len(context) - len(context.rstrip())\r\n        if n_spaces > 0:\r\n            continuation = context[-n_spaces:] + continuation\r\n            context = context[:-n_spaces]\r\n\r\n        # TODO: replace default <image> placeholder with self.image_token, for contexts\r\n\r\n        whole_enc, image_enc = self.tok_multimodal_encode(\r\n            context + continuation, images\r\n        )\r\n        context_enc, _ = self.tok_multimodal_encode(context, images)\r\n\r\n        # tok_multimodal_encode returns List[List[int]] for tokenized text. Get rid of the batch dim\r\n        # since we only are encoding a single string.\r\n        # TODO: this is a bit hacky, it'd be nice to make this generally cleaner\r\n        whole_enc, context_enc = whole_enc[0], context_enc[0]\r\n\r\n        context_enc_len = len(context_enc)\r\n        continuation_enc = whole_enc[context_enc_len:]\r\n\r\n        return context_enc, continuation_enc, image_enc\r\n\r\n    def apply_chat_template(self, chat_history: List[Dict[str, str]]) -> str:\r\n        self.chat_applied = True\r\n        if not self.interleave:\r\n            for content in chat_history:\r\n                c = []\r\n                text = content[\"content\"]\r\n\r\n                # Count and remove image placeholders\r\n                image_count = min(\r\n                    self.max_images, text.count(DEFAULT_IMAGE_PLACEHOLDER)\r\n                )\r\n                text = text.replace(DEFAULT_IMAGE_PLACEHOLDER, \"\")\r\n\r\n                # Add image entries\r\n                for _ in range(image_count):\r\n                    c.append({\"type\": \"image\", \"image\": None})\r\n\r\n                # Add single text entry at the end\r\n                c.append({\"type\": \"text\", \"text\": text})\r\n\r\n                content[\"content\"] = c\r\n        else:\r\n            for content in chat_history:\r\n                c = []\r\n                text = content[\"content\"]\r\n                expected_image_count = min(\r\n                    self.max_images, text.count(DEFAULT_IMAGE_PLACEHOLDER)\r\n                )\r\n                actual_image_count = 0\r\n\r\n                text_parts = text.split(DEFAULT_IMAGE_PLACEHOLDER)\r\n\r\n                for i, part in enumerate(text_parts):\r\n                    # TODO: concatenate text parts (esp. if skipping images)?\r\n                    if part:  # Add non-empty text parts\r\n                        c.append({\"type\": \"text\", \"text\": part})\r\n                    if (\r\n                        (i < len(text_parts) - 1) and i < self.max_images\r\n                    ):  # Add image placeholder after each split except the last\r\n                        c.append({\"type\": \"image\"})\r\n                        actual_image_count += 1\r\n\r\n                content[\"content\"] = c\r\n\r\n                if actual_image_count != expected_image_count:\r\n                    raise ValueError(\r\n                        f\"Mismatch in image placeholder count. Expected: {expected_image_count}, Actual: {actual_image_count}\"\r\n                    )\r\n\r\n        return self.processor.apply_chat_template(\r\n            chat_history, add_generation_prompt=True\r\n        )\r\n\r\n    def chat_template(self, chat_template: Union[bool, str] = False) -> Optional[str]:\r\n        if hasattr(self.processor, \"apply_chat_template\"):\r\n            _tokenizer = self.tokenizer\r\n            self.tokenizer = self.processor\r\n\r\n            selected_template = super().chat_template(chat_template)\r\n\r\n            self.tokenizer = _tokenizer\r\n            return selected_template\r\n        else:\r\n            return super().chat_template(chat_template)\r\n\r\n    def tok_batch_multimodal_encode(\r\n        self,\r\n        strings: List[str],  # note that input signature of this fn is different\r\n        images: List[List],  # TODO: images are pil.Image at the moment, update typehint\r\n        padding_side: str = \"left\",\r\n        left_truncate_len: int = None,\r\n        truncation: bool = False,\r\n    ) -> Union[\r\n        BatchEncoding, Dict[str, torch.Tensor]\r\n    ]:  # note that this return signature differs from HFLM tok_batch_encode.\r\n        # NOTE: here, we replace <image> tags with our model's corresponding image_token string value.\r\n        if not self.chat_applied:\r\n            # TODO<baber>: This still keeps the whitespace in the image placeholder, which is not ideal.\r\n            strings = [\r\n                replace_placeholders(\r\n                    string, DEFAULT_IMAGE_PLACEHOLDER, self.image_token, self.max_images\r\n                )\r\n                for string in strings\r\n            ]\r\n\r\n        # encode a batch of strings. converts to tensors and pads automatically, unlike tok_encode.\r\n        old_padding_side = self.tokenizer.padding_side\r\n        self.tokenizer.padding_side = padding_side\r\n\r\n        # add_special_tokens = {\"add_special_tokens\": False or self.add_bos_token}\r\n\r\n        images = [img[: self.max_images] for img in images]\r\n        if self.rgb:\r\n            images = [[img.convert(\"RGB\") for img in sublist] for sublist in images]\r\n\r\n        encoding = self.processor(\r\n            images=images,\r\n            text=strings,\r\n            truncation=truncation,\r\n            padding=\"longest\",\r\n            return_tensors=\"pt\",\r\n            # **add_special_tokens, # TODO: at least some Processors error out when passing this. How do we control whether text gets BOS added?\r\n        )\r\n\r\n        encoding.to(  # TODO: our other tokenization methods in HFLM don't typically move to device. this breaks convention\r\n            self.device, self.model.dtype\r\n        )  # TODO: This only casts the pixel values. Should they always be float16?\r\n        if left_truncate_len:\r\n            encoding[\"input_ids\"] = encoding[\"input_ids\"][:, -left_truncate_len:]\r\n            encoding[\"attention_mask\"] = encoding[\"attention_mask\"][\r\n                :, -left_truncate_len:\r\n            ]\r\n        self.tokenizer.padding_side = old_padding_side\r\n\r\n        return encoding\r\n\r\n    def _model_multimodal_call(self, inps, imgs, attn_mask=None, labels=None):\r\n        \"\"\"\r\n        TODO: update docstring\r\n        \"\"\"\r\n        # note: imgs is a dict.\r\n        with torch.no_grad():\r\n            return self.model(inps, **imgs).logits\r\n\r\n    def _model_multimodal_generate(self, inputs, max_length, stop, **generation_kwargs):\r\n        generation_kwargs[\"temperature\"] = generation_kwargs.get(\"temperature\", 0.0)\r\n        do_sample = generation_kwargs.get(\"do_sample\", None)\r\n\r\n        # The temperature has to be a strictly positive float -- if it is 0.0, use greedy decoding strategies\r\n        if generation_kwargs.get(\"temperature\") == 0.0 and do_sample is None:\r\n            generation_kwargs[\"do_sample\"] = do_sample = False\r\n\r\n        if do_sample is False and generation_kwargs.get(\"temperature\") == 0.0:\r\n            generation_kwargs.pop(\"temperature\")\r\n\r\n        stopping_criteria = stop_sequences_criteria(\r\n            self.tokenizer,\r\n            stop,\r\n            inputs[\"input_ids\"].shape[1],\r\n            inputs[\"input_ids\"].shape[0],\r\n        )\r\n        return self.model.generate(\r\n            **inputs,\r\n            max_length=max_length,\r\n            stopping_criteria=stopping_criteria,\r\n            pad_token_id=self.tokenizer.pad_token_id,\r\n            use_cache=True,\r\n            **generation_kwargs,\r\n        )\r\n\r\n    def _batch_images(self, image_encs):\r\n        \"\"\"\r\n        Helper function: batch together image encodings across examples in a batch.\r\n        # TODO: for variable-sized images, this may break down.\r\n        \"\"\"\r\n        batched_imgs = {}\r\n        for key in image_encs[0].keys():\r\n            batched_imgs[key] = torch.cat(\r\n                [\r\n                    torch.tensor(\r\n                        image_enc[key], device=self.device, dtype=self.model.dtype\r\n                    )\r\n                    for image_enc in image_encs\r\n                ],\r\n                dim=0,\r\n            )\r\n        return batched_imgs\r\n\r\n    def loglikelihood_rolling(self, requests: List[Instance]) -> List[float]:\r\n        raise NotImplementedError(\r\n            \"model type `hf-multimodal` does not support loglikelihood_rolling. Use 'hf' model type for text-only loglikelihood_rolling tasks \",\r\n            \"this is because we do not support measuring the loglikelihood a model assigns to an image.\",\r\n        )\r\n\r\n    def loglikelihood(\r\n        self, requests: List[Instance], disable_tqdm: bool = False\r\n    ) -> List[Tuple[float, bool]]:\r\n        raise NotImplementedError(\r\n            \"'loglikelihood' requests for model type `hf-multimodal` are not yet tested. This feature will be enabled when a loglikelihood-based multiple-choice VQA dataset is added!\"\r\n        )\r\n\r\n        new_reqs = []\r\n        for context, continuation, aux_arguments in [req.args for req in requests]:\r\n            if context == \"\":\r\n                raise ValueError(\r\n                    \"Must get non-empty context for multimodal requests! You might be trying to run 'loglikelihood_rolling', which is not supported in the multimodal case.\"\r\n                )\r\n            else:\r\n                visuals = aux_arguments[\"visual\"]\r\n\r\n                context_enc, continuation_enc, image_enc = self._encode_multimodal_pair(\r\n                    context, continuation, visuals\r\n                )\r\n            # TODO: key to pick for caching images\r\n            new_reqs.append(\r\n                (\r\n                    (context, continuation, visuals),\r\n                    context_enc,\r\n                    continuation_enc,\r\n                    image_enc,\r\n                )\r\n            )\r\n\r\n        return self._loglikelihood_tokens(new_reqs, disable_tqdm=disable_tqdm)\r\n\r\n    def _loglikelihood_tokens(\r\n        self,\r\n        requests: List[\r\n            Tuple[Tuple[None, str, str], List[int], List[int], List[int]]\r\n        ],  # TODO: update typehint to be correct\r\n        disable_tqdm: bool = False,\r\n        override_bs: int = None,\r\n    ) -> List[Tuple[float, bool]]:\r\n        res = []\r\n\r\n        # TODO: **improve multimodal collation.** We currently ignore image size when ordering docs. ideally we'd take them into account\r\n        def _collate(req: Tuple[Tuple[str, str], List[int], List[int]]):\r\n            \"\"\"Defines the key for the sorted method\"\"\"\r\n            # the negative sign on len(toks) sorts descending - this has a few advantages:\r\n            # - time estimates will always be over not underestimates, which is more useful for planning\r\n            # - to know the size of a batch when going through the list, you know the first one is always the batch\r\n            #   padded context length. this is useful to simplify the batching logic and more importantly to make\r\n            #   automatic adaptive batches much much easier to implement\r\n            # - any OOMs will happen right away rather than near the end\r\n            toks = req[1] + req[2]\r\n            return -len(toks), tuple(toks)\r\n\r\n        def _lookup_one_token_cont(req: Tuple[Tuple[str, str], List[int], List[int]]):\r\n            \"\"\"Defines the key to group and lookup one-token continuations\"\"\"\r\n            # Use with group_by=\"contexts\" (optional)\"\r\n            # allows for the creation of a lookup, so we can reuse logits in case of one-token continuations.\r\n            # speeds up some multiple-choice tasks proportionally to the number of choices.\r\n            # groups requests by context+continuation[:-1] and infer on one request/group.\r\n            return req[-1] + req[-3] + req[-2][:-1]\r\n\r\n        re_ord = Collator(\r\n            requests,\r\n            sort_fn=_collate,\r\n            group_by=\"contexts\"  # TODO: can't group-by just \"contexts\" any more, need to incorporate imgs\r\n            if self.AUTO_MODEL_CLASS == transformers.AutoModelForCausalLM\r\n            and self.logits_cache\r\n            else None,\r\n            group_fn=_lookup_one_token_cont,\r\n        )\r\n\r\n        # automatic (variable) batch size detection for vectorization\r\n        # pull longest context sample from request\r\n        n_reordered_requests = len(re_ord)\r\n        batch_size = (\r\n            self.batch_size\r\n            if self.batch_size != \"auto\"\r\n            else override_bs\r\n            if override_bs is not None\r\n            else 0\r\n        )\r\n        batch_fn = (\r\n            self._batch_scheduler\r\n            if self.batch_size == \"auto\"\r\n            and n_reordered_requests > 0\r\n            and not override_bs\r\n            else None\r\n        )\r\n\r\n        chunks = re_ord.get_batched(n=batch_size, batch_fn=batch_fn)\r\n        pbar = tqdm(\r\n            total=len(requests),\r\n            disable=(disable_tqdm or (self.rank != 0)),\r\n            desc=\"Running loglikelihood requests with text+image input\",\r\n        )\r\n        for chunk in chunks:\r\n            imgs = []\r\n            inps = []\r\n            cont_toks_list = []\r\n            inplens = []\r\n\r\n            padding_len_inp = None\r\n            # because vectorizing is annoying, we first convert each (context, continuation) pair to padded\r\n            # tensors, then we pack them together into a batch, call the model, and then pick it all apart\r\n            # again because vectorizing is annoying\r\n\r\n            for _, context_enc, continuation_enc, image_enc in chunk:\r\n                # sanity check\r\n                assert len(image_enc) > 0\r\n                assert len(context_enc) > 0\r\n                assert len(continuation_enc) > 0\r\n                assert len(continuation_enc) <= self.max_length\r\n\r\n                # how this all works (illustrated on a causal decoder-only setup):\r\n                #          CTX      CONT\r\n                # inp    0 1 2 3|4 5 6 7 8 9   <- last token is deleted by inp[:, :-1]\r\n                # model  \\               \\\r\n                # logits   1 2 3|4 5 6 7 8 9   <- the ctx half gets tossed out by the\r\n                # cont_toks      4 5 6 7 8 9      [:, -len(continuation_enc):, :self.vocab_size] slice\r\n\r\n                # when too long to fit in context, truncate from the left\r\n                # TODO: assuming that we won't handle enc-dec Vision2Seq models. Is that a safe assumption?\r\n                inp = torch.tensor(\r\n                    (context_enc + continuation_enc)[-(self.max_length + 1) :][:-1],\r\n                    dtype=torch.long,\r\n                    device=self.device,\r\n                )\r\n                (inplen,) = inp.shape\r\n\r\n                padding_len_inp = (\r\n                    max(padding_len_inp, inplen)\r\n                    if padding_len_inp is not None\r\n                    else inplen\r\n                )\r\n\r\n                inps.append(inp)  # [1, inp_length]\r\n                cont_toks_list.append(continuation_enc)\r\n                inplens.append(inplen)\r\n\r\n                imgs.append(image_enc)\r\n\r\n            # create encoder attn mask and batched conts, if seq2seq\r\n            call_kwargs = {}\r\n            batched_inps = pad_and_concat(\r\n                padding_len_inp, inps, padding_side=\"right\"\r\n            )  # [batch, padding_len_inp]\r\n            # batch our examples' image inputs together\r\n            batched_imgs = self._batch_images(\r\n                imgs\r\n            )  # TODO: fix/test for bs>1 case with differently-sized imgs!\r\n\r\n            multi_logits = F.log_softmax(\r\n                self._model_multimodal_call(batched_inps, batched_imgs, **call_kwargs),\r\n                dim=-1,\r\n            )  # [batch, padding_length (inp or cont), vocab]\r\n\r\n            for (\r\n                request_str,\r\n                ctx_tokens,\r\n                _,\r\n                image_encs,\r\n            ), logits, inplen, cont_toks in zip(\r\n                chunk, multi_logits, inplens, cont_toks_list\r\n            ):\r\n                # Slice to original seq length\r\n                contlen = len(cont_toks)\r\n                # take only logits in the continuation\r\n                # (discard context toks if decoder-only ; discard right-padding)\r\n                # also discards + checks for \"virtual tokens\" in the causal LM's input window\r\n                # from prompt/prefix tuning tokens, if applicable\r\n                ctx_len = (\r\n                    inplen + (logits.shape[0] - padding_len_inp)\r\n                    if self.AUTO_MODEL_CLASS == transformers.AutoModelForCausalLM\r\n                    else None\r\n                )\r\n                logits = self._select_cont_toks(logits, contlen=contlen, inplen=ctx_len)\r\n                logits = logits.unsqueeze(0)  # [1, seq, vocab]\r\n\r\n                # Check if per-token argmax is exactly equal to continuation\r\n                greedy_tokens = logits.argmax(dim=-1)\r\n\r\n                # check for one-token continuation cache hits.\r\n                # noop in case group_by != \"contexts\" or no cache hit and returns the\r\n                # original args. Otherwise, expands the logits batch dimension and yields each\r\n                # batch along with matching continuation tokens and prompt strings.\r\n                # logits -> [1, seq, vocab]\r\n                for request_str, cont_toks, logits in re_ord.get_cache(\r\n                    req_str=request_str,\r\n                    cxt_toks=ctx_tokens,\r\n                    cont_toks=cont_toks,\r\n                    logits=logits,\r\n                ):\r\n                    cont_toks = torch.tensor(\r\n                        cont_toks, dtype=torch.long, device=self.device\r\n                    ).unsqueeze(0)  # [1, seq]\r\n                    max_equal = (greedy_tokens == cont_toks).all()\r\n\r\n                    # Obtain log-probs at the corresponding continuation token indices\r\n                    # last_token_slice = logits[:, -1, :].squeeze(0).tolist()\r\n                    logits = torch.gather(logits, 2, cont_toks.unsqueeze(-1)).squeeze(\r\n                        -1\r\n                    )  # [1, seq]\r\n\r\n                    # Answer: (log prob, is-exact-match)\r\n                    answer = (float(logits.sum()), bool(max_equal))\r\n\r\n                    res.append(answer)\r\n\r\n                    self.cache_hook.add_partial(\r\n                        \"loglikelihood\", request_str, answer\r\n                    )  # TODO: choose convention for adding images into the cache key\r\n                    pbar.update(1)\r\n\r\n        pbar.close()\r\n\r\n        return re_ord.get_original(res)\r\n\r\n    def generate_until(\r\n        self, requests: List[Instance], disable_tqdm: bool = False\r\n    ) -> List[str]:\r\n        # TODO: back out to HFLM.generate_until() for all requests without aux_arguments (text-only reqs)\r\n        res = []\r\n\r\n        def _collate(x):\r\n            # the negative sign on len(toks) sorts descending - this has a few advantages:\r\n            # - time estimates will always be over not underestimates, which is more useful for planning\r\n            # - to know the size of a batch when going through the list, you know the first one is always the batch\r\n            #   padded context length. this is useful to simplify the batching logic and more importantly to make\r\n            #   automatic adaptive batches much much easier to implement\r\n            # - any OOMs will happen right away rather than near the end\r\n            toks = self.tok_encode(x[0])\r\n            return -len(toks), x[0]\r\n\r\n        pbar = tqdm(\r\n            total=len(requests),\r\n            disable=(disable_tqdm or (self.rank != 0)),\r\n            desc=\"Running generate_until requests with text+image input\",\r\n        )\r\n        # TODO: port auto-batch sizing into this.\r\n\r\n        # we group requests by their generation_kwargs,\r\n        # so that we don't try to execute e.g. greedy sampling and temp=0.8 sampling\r\n        # in the same batch.\r\n        re_ords = Collator(\r\n            [reg.args for reg in requests],\r\n            _collate,\r\n            group_by=\"gen_kwargs\",\r\n            group_fn=lambda x: x[1],\r\n        )\r\n        chunks = re_ords.get_batched(n=self.batch_size, batch_fn=None)\r\n\r\n        ### Up to here: was identical to non-multimodal HFLM generate_until ###\r\n\r\n        for chunk in chunks:\r\n            contexts, all_gen_kwargs, aux_arguments = zip(*chunk)\r\n\r\n            visuals = [arg[\"visual\"] for arg in aux_arguments]\r\n\r\n            if not isinstance(contexts, list):\r\n                contexts = list(\r\n                    contexts\r\n                )  # for Qwen2-VL, processor is unhappy accepting a tuple of strings instead of a list.\r\n                # TODO: could we upstream this workaround to HF?\r\n            ### this part onward: same as HFLM ###\r\n\r\n            # we assume all gen kwargs in the batch are the same\r\n            # this is safe to assume because the `grouper` object ensures it.\r\n            gen_kwargs = all_gen_kwargs[0]\r\n            # unpack our keyword arguments.\r\n            until = None\r\n            if isinstance(gen_kwargs, dict):\r\n                kwargs = copy.deepcopy(gen_kwargs)  # edge case for repeats > 1\r\n                if \"until\" in kwargs.keys():\r\n                    until = kwargs.pop(\"until\")\r\n                    if isinstance(until, str):\r\n                        until = [until]\r\n                    elif not isinstance(until, list):\r\n                        raise ValueError(\r\n                            f\"Expected `kwargs['until']` to be of type Union[str,list] but got {until}\"\r\n                        )\r\n            else:\r\n                raise ValueError(\r\n                    f\"Expected `kwargs` to be of type `dict` but got {type(gen_kwargs)}\"\r\n                )\r\n            # add EOS token to stop sequences\r\n            eos = self.tok_decode(self.eot_token_id, skip_special_tokens=False)\r\n            if not until:\r\n                until = [eos]\r\n            else:\r\n                until.append(eos)\r\n            if \"max_gen_toks\" in kwargs.keys():\r\n                max_gen_toks = kwargs.pop(\"max_gen_toks\")\r\n            else:\r\n                max_gen_toks = self.max_gen_toks\r\n\r\n            ### end stuff that's entirely copied verbatim from HFLM ###\r\n\r\n            max_ctx_len = self.max_length - max_gen_toks\r\n\r\n            inputs = self.tok_batch_multimodal_encode(\r\n                contexts,\r\n                visuals,\r\n                left_truncate_len=max_ctx_len,\r\n                truncation=self.truncation,\r\n            )\r\n\r\n            context_enc = inputs[\"input_ids\"]\r\n\r\n            if \"max_length\" not in kwargs:\r\n                kwargs[\"max_length\"] = context_enc.shape[1] + max_gen_toks\r\n\r\n            cont = self._model_multimodal_generate(inputs, stop=until, **kwargs)\r\n\r\n            del inputs\r\n            torch.cuda.empty_cache()\r\n            import gc\r\n\r\n            gc.collect()\r\n\r\n            ### essentially same as HFLM beyond this line!\r\n\r\n            cont_toks_list = cont.tolist()\r\n            for cont_toks, context in zip(cont_toks_list, contexts):\r\n                # discard context + left-padding toks if using causal decoder-only VLM\r\n                cont_toks = cont_toks[context_enc.shape[1] :]\r\n\r\n                s = self.tok_decode(cont_toks)\r\n\r\n                # use secondary stop seqs to cut off should-have-been-stopped content post-hoc\r\n                for term in until:\r\n                    if len(term) > 0:\r\n                        # ignore '' separator,\r\n                        # for seq2seq case where self.tok_decode(self.eot_token_id) = ''\r\n                        s = s.split(term)[0]\r\n\r\n                res.append(s)\r\n                self.cache_hook.add_partial(\r\n                    \"generate_until\", (context, gen_kwargs), s\r\n                )  # TODO: cache key for multimodal input should be what?\r\n                pbar.update(1)\r\n        # reorder this group of results back to original unsorted form\r\n        res = re_ords.get_original(res)\r\n\r\n        pbar.close()\r\n        return res\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/loggers/utils.py", "content": "import logging\r\nimport os\r\nimport re\r\nimport subprocess\r\nfrom pathlib import Path\r\nfrom typing import Any, Dict, Optional, Tuple, Union\r\n\r\nimport numpy as np\r\nfrom torch.utils.collect_env import get_pretty_env_info\r\nfrom transformers import __version__ as trans_version\r\n\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\n\r\ndef remove_none_pattern(input_string: str) -> Tuple[str, bool]:\r\n    \"\"\"Remove the ',none' substring from the input_string if it exists at the end.\r\n\r\n    Args:\r\n        input_string (str): The input string from which to remove the ',none' substring.\r\n\r\n    Returns:\r\n        Tuple[str, bool]: A tuple containing the modified input_string with the ',none' substring removed\r\n                          and a boolean indicating whether the modification was made (True) or not (False).\r\n    \"\"\"\r\n    # Define the pattern to match ',none' at the end of the string\r\n    pattern = re.compile(r\",none$\")\r\n\r\n    # Use sub() to replace ',none' with an empty string\r\n    result = re.sub(pattern, \"\", input_string)\r\n\r\n    # check if the input_string changed\r\n    removed = result != input_string\r\n\r\n    return result, removed\r\n\r\n\r\ndef _handle_non_serializable(o: Any) -> Union[int, str, list]:\r\n    \"\"\"Handle non-serializable objects by converting them to serializable types.\r\n\r\n    Args:\r\n        o (Any): The object to be handled.\r\n\r\n    Returns:\r\n        Union[int, str, list]: The converted object. If the object is of type np.int64 or np.int32,\r\n            it will be converted to int. If the object is of type set, it will be converted\r\n            to a list. Otherwise, it will be converted to str.\r\n    \"\"\"\r\n    if isinstance(o, np.int64) or isinstance(o, np.int32):\r\n        return int(o)\r\n    elif isinstance(o, set):\r\n        return list(o)\r\n    else:\r\n        return str(o)\r\n\r\n\r\ndef get_commit_from_path(repo_path: Union[Path, str]) -> Optional[str]:\r\n    try:\r\n        git_folder = Path(repo_path, \".git\")\r\n        if git_folder.is_file():\r\n            git_folder = Path(\r\n                git_folder.parent,\r\n                git_folder.read_text(encoding=\"utf-8\").split(\"\\n\")[0].split(\" \")[-1],\r\n            )\r\n        if Path(git_folder, \"HEAD\").exists():\r\n            head_name = (\r\n                Path(git_folder, \"HEAD\")\r\n                .read_text(encoding=\"utf-8\")\r\n                .split(\"\\n\")[0]\r\n                .split(\" \")[-1]\r\n            )\r\n            head_ref = Path(git_folder, head_name)\r\n            git_hash = head_ref.read_text(encoding=\"utf-8\").replace(\"\\n\", \"\")\r\n        else:\r\n            git_hash = None\r\n    except Exception as err:\r\n        logger.debug(\r\n            f\"Failed to retrieve a Git commit hash from path: {str(repo_path)}. Error: {err}\"\r\n        )\r\n        return None\r\n    return git_hash\r\n\r\n\r\ndef get_git_commit_hash():\r\n    \"\"\"\r\n    Gets the git commit hash of your current repo (if it exists).\r\n    Source: https://github.com/EleutherAI/gpt-neox/blob/b608043be541602170bfcfb8ec9bf85e8a0799e0/megatron/neox_arguments/neox_args.py#L42\r\n    \"\"\"\r\n    try:\r\n        git_hash = subprocess.check_output([\"git\", \"describe\", \"--always\"]).strip()\r\n        git_hash = git_hash.decode()\r\n    except (subprocess.CalledProcessError, FileNotFoundError):\r\n        # FileNotFoundError occurs when git not installed on system\r\n        git_hash = get_commit_from_path(os.getcwd())  # git hash of repo if exists\r\n    return git_hash\r\n\r\n\r\ndef add_env_info(storage: Dict[str, Any]):\r\n    try:\r\n        pretty_env_info = get_pretty_env_info()\r\n    except Exception as err:\r\n        pretty_env_info = str(err)\r\n    transformers_version = trans_version\r\n    upper_dir_commit = get_commit_from_path(\r\n        Path(os.getcwd(), \"..\")\r\n    )  # git hash of upper repo if exists\r\n    added_info = {\r\n        \"pretty_env_info\": pretty_env_info,\r\n        \"transformers_version\": transformers_version,\r\n        \"upper_git_hash\": upper_dir_commit,  # in case this repo is submodule\r\n    }\r\n    storage.update(added_info)\r\n\r\n\r\ndef add_tokenizer_info(storage: Dict[str, Any], lm):\r\n    if getattr(lm, \"tokenizer\", False):\r\n        try:\r\n            tokenizer_info = {\r\n                \"tokenizer_pad_token\": [\r\n                    lm.tokenizer.pad_token,\r\n                    str(lm.tokenizer.pad_token_id),\r\n                ],\r\n                \"tokenizer_eos_token\": [\r\n                    lm.tokenizer.eos_token,\r\n                    str(lm.tokenizer.eos_token_id),\r\n                ],\r\n                \"tokenizer_bos_token\": [\r\n                    lm.tokenizer.bos_token,\r\n                    str(lm.tokenizer.bos_token_id),\r\n                ],\r\n                \"eot_token_id\": getattr(lm, \"eot_token_id\", None),\r\n                \"max_length\": getattr(lm, \"max_length\", None),\r\n            }\r\n            storage.update(tokenizer_info)\r\n        except Exception as err:\r\n            logger.debug(\r\n                f\"Logging detailed tokenizer info failed with {err}, skipping...\"\r\n            )\r\n        # seems gguf and textsynth do not have tokenizer\r\n    else:\r\n        logger.debug(\r\n            \"LM does not have a 'tokenizer' attribute, not logging tokenizer metadata to results.\"\r\n        )\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/models/__init__.py", "content": "from . import (\r\n    anthropic_llms,\r\n    api_models,\r\n    dummy,\r\n    gemini,\r\n    gguf,\r\n    hf_vlms,\r\n    huggingface,\r\n    mamba_lm,\r\n    nemo_lm,\r\n    neuralmagic,\r\n    neuron_optimum,\r\n    openai_completions,\r\n    optimum_lm,\r\n    textsynth,\r\n    vllm_causallms,\r\n    vllm_vlms,\r\n    sglang,\r\n)\r\n\r\n\r\n# TODO: implement __all__\r\n\r\n\r\ntry:\r\n    # enable hf hub transfer if available\r\n    import hf_transfer  # type: ignore # noqa\r\n    import huggingface_hub.constants  # type: ignore\r\n\r\n    huggingface_hub.constants.HF_HUB_ENABLE_HF_TRANSFER = True\r\nexcept ImportError:\r\n    pass\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/loggers/wandb_logger.py", "content": "import copy\r\nimport json\r\nimport logging\r\nfrom typing import Any, Dict, List, Literal, Tuple\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom packaging.version import Version\r\n\r\nfrom lm_eval.loggers.utils import _handle_non_serializable, remove_none_pattern\r\n\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\n\r\ndef get_wandb_printer() -> Literal[\"Printer\"]:\r\n    \"\"\"Returns a wandb printer instance for pretty stdout.\"\"\"\r\n    from wandb.sdk.lib.printer import get_printer\r\n    from wandb.sdk.wandb_settings import Settings\r\n\r\n    printer = get_printer(Settings()._jupyter)\r\n    return printer\r\n\r\n\r\nclass WandbLogger:\r\n    def __init__(self, **kwargs) -> None:\r\n        \"\"\"Attaches to wandb logger if already initialized. Otherwise, passes kwargs to wandb.init()\r\n\r\n        Args:\r\n            kwargs Optional[Any]: Arguments for configuration.\r\n\r\n        Parse and log the results returned from evaluator.simple_evaluate() with:\r\n            wandb_logger.post_init(results)\r\n            wandb_logger.log_eval_result()\r\n            wandb_logger.log_eval_samples(results[\"samples\"])\r\n        \"\"\"\r\n        try:\r\n            import wandb\r\n\r\n            assert Version(wandb.__version__) >= Version(\"0.13.6\")\r\n            if Version(wandb.__version__) < Version(\"0.13.6\"):\r\n                wandb.require(\"report-editing:v0\")\r\n        except Exception as e:\r\n            logger.warning(\r\n                \"To use the wandb reporting functionality please install wandb>=0.13.6.\\n\"\r\n                \"To install the latest version of wandb run `pip install wandb --upgrade`\\n\"\r\n                f\"{e}\"\r\n            )\r\n\r\n        self.wandb_args: Dict[str, Any] = kwargs\r\n\r\n        # initialize a W&B run\r\n        if wandb.run is None:\r\n            self.run = wandb.init(**self.wandb_args)\r\n        else:\r\n            self.run = wandb.run\r\n\r\n        self.printer = get_wandb_printer()\r\n\r\n    def post_init(self, results: Dict[str, Any]) -> None:\r\n        self.results: Dict[str, Any] = copy.deepcopy(results)\r\n        self.task_names: List[str] = list(results.get(\"results\", {}).keys())\r\n        self.group_names: List[str] = list(results.get(\"groups\", {}).keys())\r\n\r\n    def _get_config(self) -> Dict[str, Any]:\r\n        \"\"\"Get configuration parameters.\"\"\"\r\n        self.task_configs = self.results.get(\"configs\", {})\r\n        cli_configs = self.results.get(\"config\", {})\r\n        configs = {\r\n            \"task_configs\": self.task_configs,\r\n            \"cli_configs\": cli_configs,\r\n        }\r\n\r\n        return configs\r\n\r\n    def _sanitize_results_dict(self) -> Tuple[Dict[str, str], Dict[str, Any]]:\r\n        \"\"\"Sanitize the results dictionary.\"\"\"\r\n        _results = copy.deepcopy(self.results.get(\"results\", dict()))\r\n\r\n        # Remove None from the metric string name\r\n        tmp_results = copy.deepcopy(_results)\r\n        for task_name in self.task_names:\r\n            task_result = tmp_results.get(task_name, dict())\r\n            for metric_name, metric_value in task_result.items():\r\n                _metric_name, removed = remove_none_pattern(metric_name)\r\n                if removed:\r\n                    _results[task_name][_metric_name] = metric_value\r\n                    _results[task_name].pop(metric_name)\r\n\r\n        # remove string valued keys from the results dict\r\n        wandb_summary = {}\r\n        for task in self.task_names:\r\n            task_result = _results.get(task, dict())\r\n            for metric_name, metric_value in task_result.items():\r\n                if isinstance(metric_value, str):\r\n                    wandb_summary[f\"{task}/{metric_name}\"] = metric_value\r\n\r\n        for summary_metric, summary_value in wandb_summary.items():\r\n            _task, _summary_metric = summary_metric.split(\"/\")\r\n            _results[_task].pop(_summary_metric)\r\n\r\n        tmp_results = copy.deepcopy(_results)\r\n        for task_name, task_results in tmp_results.items():\r\n            for metric_name, metric_value in task_results.items():\r\n                _results[f\"{task_name}/{metric_name}\"] = metric_value\r\n                _results[task_name].pop(metric_name)\r\n        for task in self.task_names:\r\n            _results.pop(task)\r\n\r\n        return wandb_summary, _results\r\n\r\n    def _log_results_as_table(self) -> None:\r\n        \"\"\"Generate and log evaluation results as a table to W&B.\"\"\"\r\n        columns = [\r\n            \"Version\",\r\n            \"Filter\",\r\n            \"num_fewshot\",\r\n            \"Metric\",\r\n            \"Value\",\r\n            \"Stderr\",\r\n        ]\r\n\r\n        def make_table(columns: List[str], key: str = \"results\"):\r\n            import wandb\r\n\r\n            table = wandb.Table(columns=columns)\r\n            results = copy.deepcopy(self.results)\r\n\r\n            for k, dic in results.get(key).items():\r\n                if k in self.group_names and not key == \"groups\":\r\n                    continue\r\n                version = results.get(\"versions\").get(k)\r\n                if version == \"N/A\":\r\n                    version = None\r\n                n = results.get(\"n-shot\").get(k)\r\n\r\n                for (mf), v in dic.items():\r\n                    m, _, f = mf.partition(\",\")\r\n                    if m.endswith(\"_stderr\"):\r\n                        continue\r\n                    if m == \"alias\":\r\n                        continue\r\n\r\n                    if m + \"_stderr\" + \",\" + f in dic:\r\n                        se = dic[m + \"_stderr\" + \",\" + f]\r\n                        if se != \"N/A\":\r\n                            se = \"%.4f\" % se\r\n                        table.add_data(*[k, version, f, n, m, str(v), str(se)])\r\n                    else:\r\n                        table.add_data(*[k, version, f, n, m, str(v), \"\"])\r\n\r\n            return table\r\n\r\n        # log the complete eval result to W&B Table\r\n        table = make_table([\"Tasks\"] + columns, \"results\")\r\n        self.run.log({\"evaluation/eval_results\": table})\r\n\r\n        if \"groups\" in self.results.keys():\r\n            table = make_table([\"Groups\"] + columns, \"groups\")\r\n            self.run.log({\"evaluation/group_eval_results\": table})\r\n\r\n    def _log_results_as_artifact(self) -> None:\r\n        \"\"\"Log results as JSON artifact to W&B.\"\"\"\r\n        import wandb\r\n\r\n        dumped = json.dumps(\r\n            self.results, indent=2, default=_handle_non_serializable, ensure_ascii=False\r\n        )\r\n        artifact = wandb.Artifact(\"results\", type=\"eval_results\")\r\n        with artifact.new_file(\"results.json\", mode=\"w\", encoding=\"utf-8\") as f:\r\n            f.write(dumped)\r\n        self.run.log_artifact(artifact)\r\n\r\n    def log_eval_result(self) -> None:\r\n        \"\"\"Log evaluation results to W&B.\"\"\"\r\n        # Log configs to wandb\r\n        configs = self._get_config()\r\n        self.run.config.update(configs)\r\n\r\n        wandb_summary, self.wandb_results = self._sanitize_results_dict()\r\n        # update wandb.run.summary with items that were removed\r\n        self.run.summary.update(wandb_summary)\r\n        # Log the evaluation metrics to wandb\r\n        self.run.log(self.wandb_results)\r\n        # Log the evaluation metrics as W&B Table\r\n        self._log_results_as_table()\r\n        # Log the results dict as json to W&B Artifacts\r\n        self._log_results_as_artifact()\r\n\r\n    def _generate_dataset(\r\n        self, data: List[Dict[str, Any]], config: Dict[str, Any]\r\n    ) -> pd.DataFrame:\r\n        \"\"\"Generate a dataset from evaluation data.\r\n\r\n        Args:\r\n            data (List[Dict[str, Any]]): The data to generate a dataset for.\r\n            config (Dict[str, Any]): The configuration of the task.\r\n\r\n        Returns:\r\n            pd.DataFrame: A dataframe that is ready to be uploaded to W&B.\r\n        \"\"\"\r\n        ids = [x[\"doc_id\"] for x in data]\r\n        labels = [x[\"target\"] for x in data]\r\n        instance = [\"\"] * len(ids)\r\n        resps = [\"\"] * len(ids)\r\n        filtered_resps = [\"\"] * len(ids)\r\n        model_outputs = {}\r\n\r\n        metrics_list = config[\"metric_list\"]\r\n        metrics = {}\r\n        for metric in metrics_list:\r\n            metric = metric.get(\"metric\")\r\n            if metric in [\"word_perplexity\", \"byte_perplexity\", \"bits_per_byte\"]:\r\n                metrics[f\"{metric}_loglikelihood\"] = [x[metric][0] for x in data]\r\n                if metric in [\"byte_perplexity\", \"bits_per_byte\"]:\r\n                    metrics[f\"{metric}_bytes\"] = [x[metric][1] for x in data]\r\n                else:\r\n                    metrics[f\"{metric}_words\"] = [x[metric][1] for x in data]\r\n            else:\r\n                metrics[metric] = [x[metric] for x in data]\r\n\r\n        if config[\"output_type\"] == \"loglikelihood\":\r\n            instance = [x[\"arguments\"][0][0] for x in data]\r\n            labels = [x[\"arguments\"][0][1] for x in data]\r\n            resps = [\r\n                f'log probability of continuation is {x[\"resps\"][0][0][0]} '\r\n                + \"\\n\\n\"\r\n                + \"continuation will {} generated with greedy sampling\".format(\r\n                    \"not be\" if not x[\"resps\"][0][0][1] else \"be\"\r\n                )\r\n                for x in data\r\n            ]\r\n            filtered_resps = [\r\n                f'log probability of continuation is {x[\"filtered_resps\"][0][0]} '\r\n                + \"\\n\\n\"\r\n                + \"continuation will {} generated with greedy sampling\".format(\r\n                    \"not be\" if not x[\"filtered_resps\"][0][1] else \"be\"\r\n                )\r\n                for x in data\r\n            ]\r\n        elif config[\"output_type\"] == \"multiple_choice\":\r\n            instance = [x[\"arguments\"][0][0] for x in data]\r\n            choices = [\r\n                \"\\n\".join([f\"{idx}. {y[1]}\" for idx, y in enumerate(x[\"arguments\"])])\r\n                for x in data\r\n            ]\r\n            resps = [np.argmax([n[0][0] for n in x[\"resps\"]]) for x in data]\r\n            filtered_resps = [\r\n                np.argmax([n[0] for n in x[\"filtered_resps\"]]) for x in data\r\n            ]\r\n        elif config[\"output_type\"] == \"loglikelihood_rolling\":\r\n            instance = [x[\"arguments\"][0][0] for x in data]\r\n            resps = [x[\"resps\"][0][0] for x in data]\r\n            filtered_resps = [x[\"filtered_resps\"][0] for x in data]\r\n        elif config[\"output_type\"] == \"generate_until\":\r\n            instance = [x[\"arguments\"][0][0] for x in data]\r\n            resps = [x[\"resps\"][0][0] for x in data]\r\n            filtered_resps = [x[\"filtered_resps\"][0] for x in data]\r\n\r\n        model_outputs[\"raw_predictions\"] = resps\r\n        model_outputs[\"filtered_predictions\"] = filtered_resps\r\n\r\n        df_data = {\r\n            \"id\": ids,\r\n            \"data\": instance,\r\n        }\r\n        if config[\"output_type\"] == \"multiple_choice\":\r\n            df_data[\"choices\"] = choices\r\n\r\n        tmp_data = {\r\n            \"input_len\": [len(x) for x in instance],\r\n            \"labels\": labels,\r\n            \"output_type\": config[\"output_type\"],\r\n        }\r\n        df_data.update(tmp_data)\r\n        df_data.update(model_outputs)\r\n        df_data.update(metrics)\r\n\r\n        return pd.DataFrame(df_data)\r\n\r\n    def _log_samples_as_artifact(\r\n        self, data: List[Dict[str, Any]], task_name: str\r\n    ) -> None:\r\n        import wandb\r\n\r\n        # log the samples as an artifact\r\n        dumped = json.dumps(\r\n            data,\r\n            indent=2,\r\n            default=_handle_non_serializable,\r\n            ensure_ascii=False,\r\n        )\r\n        artifact = wandb.Artifact(f\"{task_name}\", type=\"samples_by_task\")\r\n        with artifact.new_file(\r\n            f\"{task_name}_eval_samples.json\", mode=\"w\", encoding=\"utf-8\"\r\n        ) as f:\r\n            f.write(dumped)\r\n        self.run.log_artifact(artifact)\r\n        # artifact.wait()\r\n\r\n    def log_eval_samples(self, samples: Dict[str, List[Dict[str, Any]]]) -> None:\r\n        \"\"\"Log evaluation samples to W&B.\r\n\r\n        Args:\r\n            samples (Dict[str, List[Dict[str, Any]]]): Evaluation samples for each task.\r\n        \"\"\"\r\n        task_names: List[str] = [\r\n            x for x in self.task_names if x not in self.group_names\r\n        ]\r\n\r\n        ungrouped_tasks = []\r\n        tasks_by_groups = {}\r\n\r\n        for task_name in task_names:\r\n            group_names = self.task_configs[task_name].get(\"group\", None)\r\n            if group_names:\r\n                if isinstance(group_names, str):\r\n                    group_names = [group_names]\r\n\r\n                for group_name in group_names:\r\n                    if not tasks_by_groups.get(group_name):\r\n                        tasks_by_groups[group_name] = [task_name]\r\n                    else:\r\n                        tasks_by_groups[group_name].append(task_name)\r\n            else:\r\n                ungrouped_tasks.append(task_name)\r\n\r\n        for task_name in ungrouped_tasks:\r\n            eval_preds = samples[task_name]\r\n\r\n            # log the samples as a W&B Table\r\n            df = self._generate_dataset(eval_preds, self.task_configs.get(task_name))\r\n            self.run.log({f\"{task_name}_eval_results\": df})\r\n\r\n            # log the samples as a json file as W&B Artifact\r\n            self._log_samples_as_artifact(eval_preds, task_name)\r\n\r\n        for group, grouped_tasks in tasks_by_groups.items():\r\n            grouped_df = pd.DataFrame()\r\n            for task_name in grouped_tasks:\r\n                eval_preds = samples[task_name]\r\n                df = self._generate_dataset(\r\n                    eval_preds, self.task_configs.get(task_name)\r\n                )\r\n                df[\"group\"] = group\r\n                df[\"task\"] = task_name\r\n                grouped_df = pd.concat([grouped_df, df], ignore_index=True)\r\n\r\n                # log the samples as a json file as W&B Artifact\r\n                self._log_samples_as_artifact(eval_preds, task_name)\r\n\r\n            self.run.log({f\"{group}_eval_results\": grouped_df})\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/models/gguf.py", "content": "import logging\r\nimport time\r\n\r\nimport requests\r\nfrom requests.exceptions import RequestException\r\nfrom tqdm import tqdm\r\n\r\nfrom lm_eval.api.model import LM\r\nfrom lm_eval.api.registry import register_model\r\n\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\n\r\ndef get_result(logprobs, context_length):\r\n    is_greedy = True\r\n    offsets = logprobs[\"text_offset\"]\r\n    tokens = logprobs[\"tokens\"]\r\n    tokens_logprobs = logprobs[\"token_logprobs\"]\r\n\r\n    idx = 0\r\n    while offsets[idx] < context_length:\r\n        idx += 1\r\n    continuation_logprobs = sum(tokens_logprobs[idx:-1])\r\n    for i in range(idx, len(tokens)):\r\n        token = tokens[i]\r\n        top_tokens = logprobs[\"top_logprobs\"][i]\r\n        top_token = max(top_tokens.keys(), key=lambda x: top_tokens[x])\r\n        if top_token != token:\r\n            is_greedy = False\r\n            break\r\n\r\n    return continuation_logprobs, is_greedy\r\n\r\n\r\n@register_model(\"gguf\", \"ggml\")\r\nclass GGUFLM(LM):\r\n    def __init__(self, base_url=None, max_length=2048, **kwargs):\r\n        super().__init__()\r\n        self.base_url = base_url\r\n        assert self.base_url, \"must pass `base_url` to use GGUF LM!\"\r\n        self.logprobs = 10\r\n        self.temperature = 0.0\r\n        self.max_length = max_length\r\n\r\n    def gguf_completion(\r\n        self, context, continuation=None, stop=None, retries=3, delay=5, **kwargs\r\n    ):\r\n        for _ in range(retries):\r\n            try:\r\n                prompt = context\r\n                request = {\r\n                    \"prompt\": prompt,\r\n                    \"logprobs\": self.logprobs,\r\n                    \"temperature\": self.temperature,\r\n                }\r\n                if continuation:\r\n                    prompt += continuation\r\n                    request.update({\"prompt\": prompt, \"max_tokens\": 1, \"echo\": True})\r\n                if stop is not None:\r\n                    request[\"stop\"] = stop\r\n                response = requests.post(\r\n                    f\"{self.base_url}/v1/completions\", json=request\r\n                )\r\n                response.raise_for_status()\r\n                return response.json()\r\n            except RequestException as e:\r\n                logger.error(f\"RequestException: {e}\")\r\n                time.sleep(delay)  # wait before retrying\r\n        else:\r\n            raise Exception(f\"Failed to get a valid response after {retries} retries.\")\r\n\r\n    def loglikelihood(self, requests, disable_tqdm: bool = False):\r\n        if not requests:\r\n            return []\r\n        res = []\r\n        for context, continuation in tqdm(\r\n            [req.args for req in requests], disable=disable_tqdm\r\n        ):\r\n            response = self.gguf_completion(context=context, continuation=continuation)\r\n            if response and \"choices\" in response and response[\"choices\"]:\r\n                choice = response[\"choices\"][0]\r\n                logprobs = choice.get(\"logprobs\")\r\n                if (\r\n                    logprobs\r\n                    and \"token_logprobs\" in logprobs\r\n                    and logprobs[\"token_logprobs\"]\r\n                ):\r\n                    logprob, is_greedy = get_result(logprobs, len(context))\r\n                    res.append((logprob, is_greedy))\r\n                else:\r\n                    logger.warning(\r\n                        \"Invalid logprobs data. Expected 'logprobs' to contain 'token_logprobs' list.\"\r\n                    )\r\n            else:\r\n                logger.error(\r\n                    f\"Invalid response for loglikelihood. Response: {response}\"\r\n                )\r\n                assert False\r\n        return res\r\n\r\n    def generate_until(self, requests, disable_tqdm: bool = False):\r\n        if not requests:\r\n            return []\r\n\r\n        res = []\r\n        for request in tqdm([req.args for req in requests], disable=disable_tqdm):\r\n            inp = request[0]\r\n            request_args = request[1]\r\n            until = request_args.get(\"until\", [\"</s>\"])\r\n            response = self.gguf_completion(context=inp, stop=until)\r\n            if response and \"choices\" in response and response[\"choices\"]:\r\n                choice = response[\"choices\"][0]\r\n                if \"text\" in choice:\r\n                    generated_text = choice[\"text\"].strip()\r\n                    res.append(generated_text)\r\n                else:\r\n                    logger.error(\r\n                        f\"Invalid response for greedy_until. Response: {response}\"\r\n                    )\r\n                    res.append(None)  # Add default value in case of error\r\n            else:\r\n                logger.error(f\"Invalid response for greedy_until. Response: {response}\")\r\n                res.append(None)  # Add default value in case of error\r\n        return res\r\n\r\n    def loglikelihood_rolling(self, requests, disable_tqdm: bool = False):\r\n        raise NotImplementedError(\r\n            \"loglikelihood_rolling not yet supported for GGUF models\"\r\n        )\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/models/dummy.py", "content": "import random\r\n\r\nfrom tqdm import tqdm\r\n\r\nfrom lm_eval.api.model import LM\r\nfrom lm_eval.api.registry import register_model\r\n\r\n\r\n@register_model(\"dummy\")\r\nclass DummyLM(LM):\r\n    def __init__(self) -> None:\r\n        super().__init__()\r\n\r\n    @classmethod\r\n    def create_from_arg_string(cls, arg_string, additional_config=None):\r\n        return cls()\r\n\r\n    def loglikelihood(self, requests, disable_tqdm: bool = False):\r\n        res = []\r\n\r\n        for _ in tqdm(requests, disable=disable_tqdm):\r\n            res.append((-random.random(), False))\r\n\r\n        return res\r\n\r\n    def generate_until(self, requests, disable_tqdm: bool = False):\r\n        res = []\r\n\r\n        for request in tqdm(requests, disable=disable_tqdm):\r\n            res.append(\"1\")\r\n            assert request.arguments[0].strip() != \"\"\r\n\r\n        return res\r\n\r\n    def loglikelihood_rolling(self, requests, disable_tqdm: bool = False):\r\n        res = []\r\n\r\n        for _ in tqdm(requests, disable=disable_tqdm):\r\n            res.append(-random.random())\r\n\r\n        return res\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/models/mamba_lm.py", "content": "from typing import Optional, Union\r\n\r\nimport torch\r\n\r\nimport lm_eval.models.utils\r\nfrom lm_eval.api.registry import register_model\r\nfrom lm_eval.models.huggingface import HFLM\r\n\r\n\r\n@register_model(\"mamba_ssm\")\r\nclass MambaLMWrapper(HFLM):\r\n    def __init__(\r\n        self,\r\n        pretrained=\"state-spaces/mamba-130m\",\r\n        **kwargs,\r\n    ) -> None:\r\n        \"\"\"\r\n        Mamba (via the `mamba_ssm` package) supports the following args:\r\n        ```\r\n        d_model: int,\r\n        n_layer: int,\r\n        vocab_size: int,\r\n        initializer_cfg=None,\r\n        pad_vocab_size_multiple: int = 1,\r\n        ssm_cfg=None,\r\n        norm_epsilon: float = 1e-5,\r\n        rms_norm: bool = False,\r\n        initializer_cfg=None,\r\n        fused_add_norm=False,\r\n        residual_in_fp32=False,\r\n        ```\r\n\r\n        See https://github.com/state-spaces/mamba/blob/main/mamba_ssm/models/mixer_seq_simple.py#L175 for more info.\r\n        The above can all be passed via `--model_args` or to this __init__() directly\r\n        but we recommend placing many of these within the config.json file uploaded alongside your\r\n        Mamba model to the HF Hub instead.\r\n        All other HuggingFace from_pretrained() kwargs\r\n        such as those related to\r\n        `parallelize=True`, PEFT, autoGPTQ,\r\n        or any sub-configurations of these advanced args,\r\n        are unsupported by the `mamba_ssm` package.\r\n\r\n        The HFLM arguments\r\n\r\n        `backend`, `tokenizer`, `truncation`, `max_length`,\r\n        `device`, `dtype`, `batch_size`, `max_batch_size`, `trust_remote_code`, `use_fast_tokenizer`\r\n\r\n        Are all supported by Mamba where they do not conflict\r\n        with Mamba-specific restrictions such as causal LMs only.\r\n        \"\"\"\r\n\r\n        if \"backend\" in kwargs:\r\n            # mamba currently only supports causal models\r\n            assert kwargs[\"backend\"] == \"causal\"\r\n\r\n        super().__init__(\r\n            pretrained=pretrained,\r\n            # set appropriate defaults for tokenizer, max length, etc\r\n            backend=kwargs.pop(\"backend\", \"causal\"),\r\n            tokenizer=kwargs.pop(\"tokenizer\", \"EleutherAI/gpt-neox-20b\"),\r\n            max_length=kwargs.pop(\"max_length\", 2048),\r\n            **kwargs,\r\n        )\r\n\r\n    def _get_config(\r\n        self,\r\n        pretrained: str,\r\n        **kwargs,\r\n    ) -> None:\r\n        try:\r\n            from mamba_ssm.utils.hf import load_config_hf  # noqa: F811\r\n        except ModuleNotFoundError:\r\n            raise Exception(\r\n                \"attempted to use 'mamba_ssm' LM type, but package `mamba_ssm` is not installed. \\\r\nplease install mamba via `pip install lm-eval[mamba]` or `pip install -e .[mamba]`\",\r\n            )\r\n\r\n        self._config = load_config_hf(pretrained)\r\n\r\n    def _create_model(\r\n        self,\r\n        pretrained: str,\r\n        dtype: Optional[Union[str, torch.dtype]] = \"float16\",\r\n        # no `parallelize=True` options\r\n        # no PEFT and quantization options\r\n        # Mamba does not support arbitrary HF from_pretrained() args\r\n        **kwargs,\r\n    ) -> None:\r\n        try:\r\n            from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel  # noqa: F811\r\n        except ModuleNotFoundError:\r\n            raise Exception(\r\n                \"attempted to use 'mamba_ssm' LM type, but package `mamba_ssm` is not installed. \\\r\nplease install mamba via `pip install lm-eval[mamba]` or `pip install -e .[mamba]`\",\r\n            )\r\n\r\n        self._model = MambaLMHeadModel.from_pretrained(\r\n            pretrained,\r\n            device=self._device,\r\n            dtype=torch.float16\r\n            if dtype == \"auto\"\r\n            else lm_eval.models.utils.get_dtype(dtype),\r\n        )\r\n\r\n    def _model_generate(self, context, max_length, stop, **generation_kwargs):\r\n        for key in (\"do_sample\", \"attention_mask\"):\r\n            if key in generation_kwargs:\r\n                generation_kwargs.pop(key)\r\n\r\n        # mamba's custom GenerationMixin currently does not support\r\n        # passing stopping criteria.\r\n        # for the time being, we simply generate to max length,\r\n        # then truncate (equivalent result)\r\n        # -- this should be revisited to speed up generation\r\n        # stopping_criteria = stop_sequences_criteria(\r\n        #     self.tokenizer, stop, 1, context.shape[0]\r\n        # )\r\n\r\n        return self.model.generate(\r\n            input_ids=context,\r\n            max_length=max_length,\r\n            # stopping_criteria=stopping_criteria,\r\n            # pad_token_id=self.tokenizer.pad_token_id,\r\n            # use_cache=True,\r\n            **generation_kwargs,\r\n        )\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/models/api_models.py", "content": "import abc\r\nimport asyncio\r\nimport copy\r\nimport itertools\r\nimport json\r\nfrom functools import cached_property\r\nfrom typing import (\r\n    Any,\r\n    Awaitable,\r\n    Callable,\r\n    Dict,\r\n    Iterable,\r\n    List,\r\n    Literal,\r\n    NamedTuple,\r\n    Optional,\r\n    Tuple,\r\n    Union,\r\n)\r\n\r\n\r\ntry:\r\n    import requests\r\n    from aiohttp import ClientSession, TCPConnector\r\n    from tenacity import RetryError, retry, stop_after_attempt, wait_exponential\r\n    from tqdm import tqdm\r\n    from tqdm.asyncio import tqdm_asyncio\r\nexcept ModuleNotFoundError:\r\n    pass\r\n\r\n\r\nfrom importlib.util import find_spec\r\n\r\nfrom lm_eval import utils\r\nfrom lm_eval.api.instance import Instance\r\nfrom lm_eval.api.model import TemplateLM\r\nfrom lm_eval.models.utils import Collator, chunks, configure_pad_token\r\n\r\n\r\nLogLikelihoodInputs = Tuple[Tuple[str, str], List[int], List[int]]\r\n\r\n\r\n# utility class to keep track of json encoded chats\r\nclass JsonChatStr(NamedTuple):\r\n    prompt: str\r\n\r\n    def encode(self, encoding):\r\n        return self.prompt.encode(encoding)\r\n\r\n\r\neval_logger = utils.eval_logger\r\n\r\n\r\nclass TemplateAPI(TemplateLM):\r\n    def __init__(\r\n        self,\r\n        model: str = None,\r\n        pretrained: str = None,  # `model` takes precedence over `pretrained` when passed.\r\n        base_url: str = None,\r\n        tokenizer: Optional[str] = None,\r\n        # Logliklehood tasks require a tokenizer to calculate context lengths,\r\n        # however the requests can be sent as a string if the API doesn't support token inputs.\r\n        # use tokenized_requests=False\r\n        tokenizer_backend: Optional[\r\n            Literal[\"tiktoken\", \"huggingface\", None]\r\n        ] = \"huggingface\",\r\n        truncate: bool = False,\r\n        # number of concurrent requests. More useful if not batching\r\n        num_concurrent: int = 1,\r\n        max_retries: int = 3,\r\n        max_gen_toks: int = 256,\r\n        batch_size: Union[str, int] = 1,\r\n        seed: int = 1234,\r\n        max_length: Optional[int] = 2048,\r\n        add_bos_token: bool = False,\r\n        custom_prefix_token_id: int = None,\r\n        # send the requests as tokens or strings\r\n        tokenized_requests: bool = True,\r\n        trust_remote_code: bool = False,\r\n        revision: Optional[str] = \"main\",\r\n        use_fast_tokenizer: bool = True,\r\n        **kwargs,\r\n    ) -> None:\r\n        super().__init__()\r\n        missing_packages = [\r\n            pkg\r\n            for pkg in [\"aiohttp\", \"tqdm\", \"tenacity\", \"requests\"]\r\n            if find_spec(pkg) is None\r\n        ]\r\n        if missing_packages:\r\n            raise ModuleNotFoundError(\r\n                f\"Attempted to use an API model, but the required packages {missing_packages} are not installed. \"\r\n                'Please install these via `pip install lm-eval[api]` or `pip install -e .\"[api]\"`'\r\n            )\r\n        self.model = model or pretrained\r\n        self.base_url = base_url\r\n        self.tokenizer = tokenizer\r\n        if not isinstance(batch_size, int) and \"auto\" in batch_size:\r\n            eval_logger.warning(\r\n                \"Automatic batch size is not supported for API models. Defaulting to batch size 1.\"\r\n            )\r\n        elif int(batch_size) > 1:\r\n            eval_logger.warning(\r\n                \"Batch size > 1 detected. Ensure your API supports batched requests with varying total sequence lengths.\"\r\n            )\r\n        self._batch_size = int(batch_size) if batch_size != \"auto\" else 1\r\n        self._truncate = truncate\r\n        self._max_gen_toks = int(max_gen_toks)\r\n        self._seed = int(seed)\r\n        # max_length - 1 as we always have 1 token for generation\r\n        eval_logger.info(f\"Using max length {max_length} - 1\")\r\n        self.max_length = max_length - 1\r\n        if int(num_concurrent) <= 1:\r\n            eval_logger.info(\r\n                \"Concurrent requests are disabled. To enable concurrent requests, set `num_concurrent` > 1.\"\r\n            )\r\n        self._concurrent = int(num_concurrent)\r\n        self.tokenizer_backend = tokenizer_backend\r\n        self.add_bos_token = add_bos_token\r\n        self.custom_prefix_token_id = custom_prefix_token_id\r\n        self.tokenized_requests = tokenized_requests\r\n        self.max_retries = int(max_retries)\r\n\r\n        eval_logger.info(f\"Using tokenizer {self.tokenizer_backend}\")\r\n        if self.tokenizer_backend is None:\r\n            self.tokenizer = None\r\n            self.tokenized_requests = False\r\n        else:\r\n            if self.tokenizer is None:\r\n                if self.tokenizer_backend == \"huggingface\":\r\n                    import transformers\r\n\r\n                    self.tokenizer = transformers.AutoTokenizer.from_pretrained(\r\n                        self.tokenizer if self.tokenizer else self.model,\r\n                        trust_remote_code=trust_remote_code,\r\n                        revision=revision,\r\n                        use_fast=use_fast_tokenizer,\r\n                    )\r\n                    # Not used as the API will handle padding but to mirror the behavior of the HFLM\r\n                    self.tokenizer = configure_pad_token(self.tokenizer)\r\n                elif self.tokenizer_backend == \"tiktoken\":\r\n                    try:\r\n                        import tiktoken\r\n\r\n                        self.tokenizer = tiktoken.encoding_for_model(self.model)\r\n                    except ModuleNotFoundError as e:\r\n                        raise Exception(\r\n                            \"Attempted to use 'openai' LM type, but the package `tiktoken` is not installed. \"\r\n                            \"Please install it via `pip install lm-eval[api]` or `pip install -e .[api]`.\"\r\n                        ) from e\r\n                    if \"openai\" not in self.base_url:\r\n                        eval_logger.warning(\r\n                            f\"Passed `base_url={self.base_url}` but using (OpenAI) Tiktoken tokenizer backend. \"\r\n                            \"Pass `tokenizer_backend=huggingface` and provide the HF tokenizer name if your model does not use Tiktoken.\"\r\n                        )\r\n            else:\r\n                import transformers\r\n\r\n                assert isinstance(tokenizer, str), \"tokenizer must be a string\"\r\n                self.tokenizer = transformers.AutoTokenizer.from_pretrained(\r\n                    tokenizer,\r\n                    trust_remote_code=trust_remote_code,\r\n                    revision=revision,\r\n                    use_fast=use_fast_tokenizer,\r\n                )\r\n\r\n    @abc.abstractmethod\r\n    def _create_payload(\r\n        self,\r\n        messages: Union[List[List[int]], List[dict], List[str], str],\r\n        *,\r\n        generate: bool = True,\r\n        gen_kwargs: Optional[dict] = None,\r\n        seed: int = 1234,\r\n        **kwargs,\r\n    ) -> dict:\r\n        \"\"\"This method is responsible for creating the json payload that will be sent to the API.\"\"\"\r\n        raise NotImplementedError\r\n\r\n    def create_message(\r\n        self,\r\n        messages: Union[List[List[int]], List[str], List[JsonChatStr]],\r\n        generate=False,\r\n    ) -> Union[List[List[int]], List[dict], List[str], str]:\r\n        \"\"\"Helper method to transform the prompt into the expected API input format. messages consist of batched requests\"\"\"\r\n        if isinstance(messages[0], JsonChatStr):\r\n            # for chat completions we need to decode the json string to list[dict,...]\r\n            assert (\r\n                self._batch_size == 1\r\n            ), \"non-tokenized chat requests are only supported with batch_size=1\"\r\n            # list[dict[\"role\":..., \"content\":...],...]\r\n            return json.loads(messages[0].prompt)\r\n\r\n        if not self.tokenized_requests:\r\n            # if messages are tokenized:\r\n            if isinstance(messages[0][0], int):\r\n                # assuming decoding is lossless. However, this is only for logliklehood requests\r\n                # as we need to compute the context length. For generations, we don't need to tokenize.\r\n                messages = self.decode_batch(messages)\r\n            if self._batch_size <= 1:\r\n                # if batch is 1 return str\r\n                return messages[0]\r\n            else:\r\n                # list[str,...]\r\n                return messages\r\n\r\n        # list[list[int], ...]\r\n        return messages\r\n\r\n    @staticmethod\r\n    @abc.abstractmethod\r\n    def parse_logprobs(\r\n        outputs: Union[Any, List[Any]],\r\n        tokens: List[List[int]] = None,\r\n        ctxlen: List[int] = None,\r\n        **kwargs,\r\n    ) -> List[Tuple[float, bool]]:\r\n        \"\"\"Method used to parse the logprobs from the (batched) API response. This method should return a list of tuples\"\"\"\r\n        raise NotImplementedError\r\n\r\n    @staticmethod\r\n    @abc.abstractmethod\r\n    def parse_generations(outputs: Union[Any, List[Any]], **kwargs) -> List[str]:\r\n        \"\"\"Method used to parse the generations from the (batched) API response. This method should return a list of str\"\"\"\r\n        raise NotImplementedError\r\n\r\n    @cached_property\r\n    def api_key(self) -> str:\r\n        \"\"\"Override this property to return the API key for the API request.\"\"\"\r\n        return \"\"\r\n\r\n    @cached_property\r\n    def header(self) -> dict:\r\n        \"\"\"Override this property to return the headers for the API request.\"\"\"\r\n        return {\"Authorization\": f\"Bearer {self.api_key}\"}\r\n\r\n    @property\r\n    def tokenizer_name(self) -> str:\r\n        \"\"\"Must be defined for LM subclasses which implement Chat Templating.\r\n        Should return the name of the tokenizer or chat template used.\r\n        Used only to properly fingerprint caches when requests are being cached with `--cache_requests`, otherwise not used.\r\n        \"\"\"\r\n        return \"\"\r\n\r\n    def apply_chat_template(\r\n        self, chat_history: List[Dict[str, str]]\r\n    ) -> Union[str, JsonChatStr]:\r\n        \"\"\"Applies a chat template to a list of chat history between user and model.\"\"\"\r\n        if self.tokenizer_backend == \"huggingface\" and self.tokenized_requests:\r\n            return self.tokenizer.apply_chat_template(\r\n                chat_history, tokenize=False, add_generation_prompt=True\r\n            )\r\n        else:\r\n            # bit of a hack. We'll load back before sending to the API\r\n            return JsonChatStr(json.dumps(chat_history))\r\n\r\n    @cached_property\r\n    def eot_token_id(self) -> Optional[int]:\r\n        if self.tokenizer is None:\r\n            return None\r\n        else:\r\n            if self.tokenizer_backend == \"huggingface\":\r\n                return self.tokenizer.eos_token_id\r\n            elif self.tokenizer_backend == \"tiktoken\":\r\n                return self.tokenizer.eot_token\r\n\r\n    @cached_property\r\n    def prefix_token_id(self) -> Optional[int]:\r\n        if self.tokenizer is None:\r\n            return None\r\n        else:\r\n            if self.custom_prefix_token_id is not None:\r\n                return self.custom_prefix_token_id\r\n            if self.tokenizer_backend == \"huggingface\":\r\n                if self.tokenizer.bos_token_id is not None:\r\n                    return self.tokenizer.bos_token_id\r\n                return self.tokenizer.eos_token_id\r\n            else:\r\n                return self.tokenizer.eot_token\r\n\r\n    def tok_encode(\r\n        self,\r\n        string: str,\r\n        left_truncate_len: int = None,\r\n        add_special_tokens: bool = False,\r\n        truncation: bool = False,\r\n        **kwargs,\r\n    ) -> Union[List[List[int]], List[int], List[str]]:\r\n        if self.tokenizer_backend is None:\r\n            return [string]\r\n        elif self.tokenizer_backend == \"huggingface\":\r\n            # by default for CausalLM - false or self.add_bos_token is set\r\n            if not add_special_tokens:\r\n                add_special_tokens = False or self.add_bos_token\r\n            encoding: Union[List[List[int]], List[int]] = self.tokenizer(\r\n                string,\r\n                add_special_tokens=add_special_tokens,\r\n                truncation=truncation,\r\n                return_attention_mask=False,\r\n            ).input_ids\r\n\r\n            # left-truncate the encoded context to be at most `left_truncate_len` tokens long\r\n            if left_truncate_len:\r\n                if not isinstance(string, str):\r\n                    encoding = [enc[-left_truncate_len:] for enc in encoding]\r\n                else:\r\n                    encoding = encoding[-left_truncate_len:]\r\n\r\n            return encoding\r\n\r\n        else:\r\n            try:\r\n                encoding = self.tokenizer.encode(string)\r\n            except Exception:\r\n                encoding = self.tokenizer.encode_batch(string)\r\n            return encoding\r\n\r\n    def decode_batch(self, tokens: List[List[int]]) -> List[str]:\r\n        if self.tokenizer_backend == \"huggingface\":\r\n            return self.tokenizer.batch_decode(tokens)\r\n        elif self.tokenizer_backend == \"tiktoken\":\r\n            return self.tokenizer.decode_batch(tokens)\r\n\r\n    def model_call(\r\n        self,\r\n        messages: Union[List[List[int]], List[str], List[JsonChatStr]],\r\n        *,\r\n        generate: bool = True,\r\n        gen_kwargs: Optional[Dict] = None,\r\n        **kwargs,\r\n    ) -> Optional[dict]:\r\n        # !!! Copy: shared dict for each request, need new object !!!\r\n        gen_kwargs = copy.deepcopy(gen_kwargs)\r\n        try:\r\n            response = requests.post(\r\n                self.base_url,\r\n                json=self._create_payload(\r\n                    self.create_message(messages),\r\n                    generate=generate,\r\n                    gen_kwargs=gen_kwargs,\r\n                    seed=self._seed,\r\n                    **kwargs,\r\n                ),\r\n                headers=self.header,\r\n            )\r\n            if not response.ok:\r\n                eval_logger.warning(\r\n                    f\"API request failed with error message: {response.text}. Retrying...\"\r\n                )\r\n            response.raise_for_status()\r\n            return response.json()\r\n        except RetryError:\r\n            eval_logger.error(\r\n                \"API request failed after multiple retries. Please check the API status.\"\r\n            )\r\n            return None\r\n\r\n    async def amodel_call(\r\n        self,\r\n        session: ClientSession,\r\n        messages: Union[List[List[int]], List[str], List[JsonChatStr]],\r\n        *,\r\n        generate: bool = True,\r\n        cache_keys: list = None,\r\n        ctxlens: Optional[List[int]] = None,\r\n        gen_kwargs: Optional[Dict] = None,\r\n        **kwargs,\r\n    ) -> Union[List[str], List[Tuple[float, bool]], None]:\r\n        # !!! Copy: shared dict for each request, need new object !!!\r\n        gen_kwargs = copy.deepcopy(gen_kwargs)\r\n        payload = self._create_payload(\r\n            self.create_message(messages),\r\n            generate=generate,\r\n            gen_kwargs=gen_kwargs,\r\n            seed=self._seed,\r\n            **kwargs,\r\n        )\r\n        cache_method = \"generate_until\" if generate else \"loglikelihood\"\r\n        try:\r\n            async with session.post(\r\n                self.base_url,\r\n                json=payload,\r\n                headers=self.header,\r\n            ) as response:\r\n                if not response.ok:\r\n                    error_text = await response.text()\r\n                    eval_logger.warning(\r\n                        f\"API request failed with error message: {error_text}. Retrying...\"\r\n                    )\r\n                # raising exception will retry the request\r\n                response.raise_for_status()\r\n                outputs = await response.json()\r\n            answers = (\r\n                self.parse_generations(\r\n                    outputs=outputs,\r\n                )\r\n                if generate\r\n                else self.parse_logprobs(\r\n                    outputs=outputs,\r\n                    tokens=messages,\r\n                    ctxlens=ctxlens,\r\n                )\r\n            )\r\n            if cache_keys:\r\n                for res, cache in zip(answers, cache_keys):\r\n                    self.cache_hook.add_partial(cache_method, cache, res)\r\n            return answers\r\n        # If the retries also fail\r\n        except RetryError:\r\n            eval_logger.error(\r\n                \"API request failed after multiple retries. Please check the API status.\"\r\n            )\r\n            return None\r\n\r\n    def batch_logliklehood_requests(\r\n        self, chunks: Iterable[List[LogLikelihoodInputs]]\r\n    ) -> Tuple[List[List[int]], List[int], List[Tuple[str, str]]]:\r\n        inputs = []\r\n        ctxlens = []\r\n        cache_keys = []\r\n        for chunk in chunks:\r\n            for cache_key, context_enc, continuation_enc in chunk:\r\n                # max_length - 1 as we always have 1 token for generation\r\n                inp = (context_enc + continuation_enc)[-(self.max_length) :]\r\n                ctxlen = len(context_enc) - max(\r\n                    0, len(context_enc) + len(continuation_enc) - (self.max_length)\r\n                )\r\n\r\n                inputs.append(inp)\r\n                ctxlens.append(ctxlen)\r\n                cache_keys.append(cache_key)\r\n        return inputs, ctxlens, cache_keys\r\n\r\n    async def get_batched_requests(\r\n        self,\r\n        requests: list,\r\n        cache_keys: list,\r\n        *,\r\n        generate: bool = True,\r\n        ctxlens: List[int] = None,\r\n        **kwargs,\r\n    ) -> Union[List[List[str]], List[List[Tuple[float, bool]]]]:\r\n        ctxlens = ctxlens if ctxlens else [None] * len(requests)\r\n        conn = TCPConnector(limit=self._concurrent)\r\n        async with ClientSession(connector=conn) as session:\r\n            retry_: Callable[..., Awaitable[Any]] = retry(\r\n                stop=stop_after_attempt(self.max_retries),\r\n                wait=wait_exponential(multiplier=0.5, min=1, max=10),\r\n                reraise=True,\r\n            )(self.amodel_call)\r\n            # Create tasks for each batch of request\r\n            tasks = [\r\n                asyncio.create_task(\r\n                    retry_(\r\n                        session=session,\r\n                        messages=message,\r\n                        cache_keys=cache_key,\r\n                        generate=generate,\r\n                        ctxlens=ctxlen,\r\n                        **kwargs,\r\n                    )\r\n                )\r\n                for message, cache_key, ctxlen in zip(\r\n                    chunks(requests, n=self._batch_size),\r\n                    chunks(cache_keys, n=self._batch_size),\r\n                    chunks(ctxlens, n=self._batch_size),\r\n                )\r\n            ]\r\n\r\n            return await tqdm_asyncio.gather(*tasks, desc=\"Requesting API\")\r\n\r\n    def _loglikelihood_tokens(self, requests, **kwargs) -> List[Tuple[float, bool]]:\r\n        assert (\r\n            self.tokenizer is not None\r\n        ), \"Tokenizer is required for loglikelihood tasks to compute context lengths.\"\r\n        res = []\r\n\r\n        def _collate(req: LogLikelihoodInputs):\r\n            \"\"\"Defines the key for the sorted method\"\"\"\r\n            # the negative sign on len(toks) sorts descending - this has a few advantages:\r\n            # - time estimates will always be over not underestimates, which is more useful for planning\r\n            # - to know the size of a batch when going through the list, you know the first one is always the batch\r\n            #   padded context length. this is useful to simplify the batching logic and more importantly to make\r\n            #   automatic adaptive batches much much easier to implement\r\n            # - any OOMs will happen right away rather than near the end\r\n\r\n            toks = req[1] + req[2]\r\n            return -len(toks), tuple(toks)\r\n\r\n        re_ord = Collator(\r\n            requests,\r\n            sort_fn=_collate,\r\n            group_by=None,\r\n        )\r\n        # if concurrent then we'll batch in the async context\r\n        chunked = re_ord.get_batched(n=self._batch_size if self._concurrent <= 1 else 0)\r\n        if self._concurrent <= 1:\r\n            pbar = tqdm(desc=\"Requesting API\", total=len(requests))\r\n            for chunk in chunked:\r\n                inputs, ctxlens, cache_keys = self.batch_logliklehood_requests([chunk])\r\n\r\n                outputs = retry(\r\n                    stop=stop_after_attempt(self.max_retries),\r\n                    wait=wait_exponential(multiplier=0.5, min=1, max=10),\r\n                    reraise=True,\r\n                )(self.model_call)(messages=inputs, generate=False)\r\n                if isinstance(outputs, dict):\r\n                    outputs = [outputs]\r\n                for answer_, cache_key in zip(\r\n                    self.parse_logprobs(\r\n                        outputs=outputs, tokens=inputs, ctxlens=ctxlens\r\n                    ),\r\n                    cache_keys,\r\n                ):\r\n                    if answer_ is not None:\r\n                        res.append(answer_)\r\n                        # cache requests that aren't from a loglikelihood_rolling request\r\n                        if cache_key is not None:\r\n                            self.cache_hook.add_partial(\r\n                                \"loglikelihood\", cache_key, answer_\r\n                            )\r\n                        pbar.update(1)\r\n        else:\r\n            inputs, ctxlens, cache_keys = self.batch_logliklehood_requests(chunked)\r\n            res = itertools.chain.from_iterable(\r\n                asyncio.run(\r\n                    self.get_batched_requests(\r\n                        inputs, cache_keys, generate=False, ctxlens=ctxlens\r\n                    )\r\n                )\r\n            )\r\n\r\n        return re_ord.get_original(res)\r\n\r\n    def generate_until(\r\n        self, requests: List[Instance], disable_tqdm: bool = False\r\n    ) -> List[str]:\r\n        res = []\r\n\r\n        def _collate_gen(_requests):\r\n            # sort by the length of the non-tokenized contexts\r\n            return -len(_requests[0])\r\n\r\n        # Let the API deal with tokenization\r\n        requests, all_gen_kwargs = zip(*(req.args for req in requests))\r\n        if self.tokenized_requests:\r\n            encodings_list = self.tok_encode(\r\n                requests, add_special_tokens=self.add_bos_token\r\n            )\r\n        else:\r\n            encodings_list = [None] * len(requests)\r\n        requests = [\r\n            (a, b, c) for a, b, c in zip(requests, all_gen_kwargs, encodings_list)\r\n        ]\r\n\r\n        re_ord = Collator(\r\n            requests,\r\n            sort_fn=_collate_gen,\r\n            group_by=\"gen_kwargs\",\r\n        )\r\n        chunked = re_ord.get_batched(\r\n            n=self._batch_size if self._concurrent <= 1 else 0, batch_fn=None\r\n        )\r\n        if self._concurrent <= 1:\r\n            pbar = tqdm(desc=\"Requesting API\", total=len(requests))\r\n            for chunk in chunked:\r\n                contexts, all_gen_kwargs, encodings_list = zip(*chunk)\r\n                req = encodings_list if self.tokenized_requests else contexts\r\n                outputs = retry(\r\n                    stop=stop_after_attempt(self.max_retries),\r\n                    wait=wait_exponential(multiplier=0.5, min=1, max=10),\r\n                    reraise=True,\r\n                )(self.model_call)(\r\n                    messages=req,\r\n                    generate=True,\r\n                    gen_kwargs=copy.deepcopy(all_gen_kwargs[0]),\r\n                )\r\n                for generated_text, context in zip(\r\n                    self.parse_generations(\r\n                        outputs=outputs,\r\n                        contexts=contexts,\r\n                    ),\r\n                    contexts,\r\n                ):\r\n                    if generated_text is not None:\r\n                        res.append(generated_text)\r\n\r\n                        # partial caching\r\n                        if context is not None:\r\n                            self.cache_hook.add_partial(\r\n                                \"generate_until\",\r\n                                (context, all_gen_kwargs[0]),\r\n                                generated_text,\r\n                            )\r\n                            pbar.update(1)\r\n        else:\r\n            for chunk in chunked:\r\n                contexts, all_gen_kwargs, encodings_list = zip(*chunk)\r\n                req = encodings_list if self.tokenized_requests else contexts\r\n                results = itertools.chain.from_iterable(\r\n                    asyncio.run(\r\n                        self.get_batched_requests(\r\n                            req,\r\n                            cache_keys=[(ctx, all_gen_kwargs[0]) for ctx in contexts],\r\n                            generate=True,\r\n                            gen_kwargs=copy.deepcopy(all_gen_kwargs[0]),\r\n                        )\r\n                    )\r\n                )\r\n                res.extend(results)\r\n\r\n        return re_ord.get_original(res)\r\n\r\n    def loglikelihood_rolling(\r\n        self, requests: List[Instance], disable_tqdm: bool = False\r\n    ) -> List[float]:\r\n        loglikelihoods = []\r\n\r\n        for (string,) in tqdm([req.args for req in requests], disable=disable_tqdm):\r\n            rolling_token_windows = list(\r\n                map(\r\n                    utils.make_disjoint_window,\r\n                    utils.get_rolling_token_windows(\r\n                        token_list=self.tok_encode(string),\r\n                        prefix_token=self.prefix_token_id,\r\n                        # max_seq_len - (1 for context)\r\n                        max_seq_len=self.max_length - 1,\r\n                        context_len=1,\r\n                    ),\r\n                )\r\n            )\r\n\r\n            # TODO: Right now, we pass single EOT token to the Encoder and the full context to the decoder, in seq2seq case\r\n            rolling_token_windows = [(None,) + x for x in rolling_token_windows]\r\n\r\n            string_nll = self._loglikelihood_tokens(\r\n                rolling_token_windows,\r\n                disable_tqdm=True,\r\n            )\r\n\r\n            # discard is_greedy\r\n            string_nll = [x[0] for x in string_nll]\r\n\r\n            string_nll = sum(string_nll)\r\n            loglikelihoods.append(string_nll)\r\n\r\n            # cache this loglikelihood_rolling request\r\n            self.cache_hook.add_partial(\"loglikelihood_rolling\", (string,), string_nll)\r\n        return loglikelihoods\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/models/neuralmagic.py", "content": "import copy\r\nfrom typing import List, Optional, Tuple, Union\r\n\r\nimport numpy\r\nimport transformers\r\nfrom tqdm import tqdm\r\n\r\nimport lm_eval.models.utils\r\nfrom lm_eval import utils\r\nfrom lm_eval.api.instance import Instance\r\nfrom lm_eval.api.model import LM\r\nfrom lm_eval.api.registry import register_model\r\nfrom lm_eval.models.huggingface import HFLM\r\n\r\n\r\neval_logger = utils.eval_logger\r\n\r\n\r\n@register_model(\"sparseml\")\r\nclass SparseMLLM(HFLM):\r\n    \"\"\"\r\n    SparseML is an open-source model optimization toolkit that enables you to create\r\n    inference-optimized sparse models using pruning, quantization, and distillation\r\n    algorithms. Models optimized with SparseML can then be exported to the ONNX format and\r\n    deployed with DeepSparse for GPU-class performance on CPU hardware.\r\n\r\n    This class is a wrapper around the HuggingFace LM class to enable SparseML\r\n    integration with the lm-evaluation-harness.\r\n    \"\"\"\r\n\r\n    def _create_model(\r\n        self,\r\n        pretrained: str,\r\n        revision: Optional[str] = \"main\",\r\n        dtype: Optional[str] = \"auto\",\r\n        trust_remote_code: Optional[bool] = False,\r\n        **kwargs,\r\n    ) -> None:\r\n        try:\r\n            from sparseml.transformers import SparseAutoModelForCausalLM\r\n        except ModuleNotFoundError:\r\n            raise Exception(\r\n                \"Package `sparseml` is not installed. \"\r\n                \"Please install it via `pip install sparseml[transformers]`\"\r\n            )\r\n\r\n        model_kwargs = kwargs if kwargs else {}\r\n\r\n        if \"device_map\" not in model_kwargs:\r\n            # set a device_map to initialize model on the right GPU.\r\n            # this is needed because it seems that the default behavior\r\n            # for quantized models now seems to be device_map=\"auto\"\r\n            # which breaks data-parallel mode.\r\n            if hasattr(self, \"accelerator\"):\r\n                model_kwargs.update(\r\n                    {\"device_map\": {\"\": f\"cuda:{self.accelerator.local_process_index}\"}}\r\n                )\r\n            else:\r\n                model_kwargs.update({\"device_map\": {\"\": str(self.device)}})\r\n\r\n        relevant_kwarg_names = [\r\n            \"offload_folder\",\r\n            \"device_map\",\r\n        ]\r\n        relevant_kwargs = {\r\n            k: v for k, v in model_kwargs.items() if k in relevant_kwarg_names\r\n        }\r\n\r\n        # Log the difference between model_kwargs and relevant_kwargs so we can see\r\n        # what is being ignored\r\n        ignored_kwargs = {}\r\n        for k, v in model_kwargs.items():\r\n            if k not in relevant_kwargs.keys():\r\n                ignored_kwargs[k] = v\r\n        eval_logger.warning(\r\n            f\"The sparseml integration is ignoring the following kwargs that are specified: {ignored_kwargs}\"\r\n        )\r\n\r\n        model = SparseAutoModelForCausalLM.from_pretrained(\r\n            pretrained,\r\n            revision=revision,\r\n            torch_dtype=lm_eval.models.utils.get_dtype(dtype),\r\n            trust_remote_code=trust_remote_code,\r\n            **relevant_kwargs,\r\n        )\r\n        self._model = model\r\n\r\n    def _get_config(self, pretrained: str, **kwargs) -> None:\r\n        try:\r\n            from sparseml.transformers import SparseAutoConfig\r\n        except ModuleNotFoundError:\r\n            raise Exception(\r\n                \"Package `sparseml` is not installed. \"\r\n                \"Please install it via `pip install sparseml[transformers]`\"\r\n            )\r\n\r\n        self._config = SparseAutoConfig.from_pretrained(\r\n            pretrained_model_name_or_path=pretrained, **kwargs\r\n        )\r\n\r\n    def _create_tokenizer(\r\n        self,\r\n        pretrained: Union[str, transformers.PreTrainedModel],\r\n        tokenizer: Optional[\r\n            Union[\r\n                str,\r\n                transformers.PreTrainedTokenizer,\r\n                transformers.PreTrainedTokenizerFast,\r\n            ]\r\n        ],\r\n        **kwargs,\r\n    ) -> None:\r\n        try:\r\n            from sparseml.transformers import SparseAutoTokenizer\r\n        except ModuleNotFoundError:\r\n            raise Exception(\r\n                \"Package `sparseml` is not installed. \"\r\n                \"Please install it via `pip install sparseml[transformers]`\"\r\n            )\r\n\r\n        if tokenizer:\r\n            if isinstance(tokenizer, str):\r\n                self.tokenizer = SparseAutoTokenizer.from_pretrained(\r\n                    tokenizer,\r\n                    **kwargs,\r\n                )\r\n            else:\r\n                assert isinstance(\r\n                    tokenizer, transformers.PreTrainedTokenizer\r\n                ) or isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\r\n                self.tokenizer = tokenizer\r\n        else:\r\n            # Get tokenizer based on 'pretrained'\r\n            if isinstance(pretrained, str):\r\n                model_name = pretrained\r\n            else:\r\n                # get the HF hub name via accessor on model\r\n                model_name = self.model.name_or_path\r\n            self.tokenizer = SparseAutoTokenizer.from_pretrained(\r\n                model_name,\r\n                **kwargs,\r\n            )\r\n        return None\r\n\r\n\r\n@register_model(\"deepsparse\")\r\nclass DeepSparseLM(LM):\r\n    \"\"\"\r\n    Wrapper around DeepSparse, a sparsity-aware deep learning\r\n    inference runtime for CPUs, to make it compatible with the\r\n    lm-evaluation-harness.\r\n    \"\"\"\r\n\r\n    _DEFAULT_MAX_LENGTH = 2048\r\n\r\n    def __init__(\r\n        self,\r\n        pretrained: str,\r\n        tokenizer: Optional[\r\n            Union[\r\n                str,\r\n                transformers.PreTrainedTokenizer,\r\n                transformers.PreTrainedTokenizerFast,\r\n            ]\r\n        ] = None,\r\n        batch_size: Optional[Union[int, str]] = 1,\r\n        max_gen_toks: Optional[int] = 256,\r\n        max_length: Optional[int] = None,\r\n    ):\r\n        super().__init__()\r\n\r\n        try:\r\n            import deepsparse\r\n        except ModuleNotFoundError:\r\n            raise Exception(\r\n                \"Package `deepsparse` is not installed. \"\r\n                \"Please install it via `pip install deepsparse[transformers]`\"\r\n            )\r\n\r\n        if isinstance(batch_size, str) and not batch_size.isdigit():\r\n            eval_logger.warning(\r\n                f\"batch_size={batch_size} is not valid for deepsparse because it is not an integer. \"\r\n                \"Ignoring and using the default of 1.\"\r\n            )\r\n            batch_size = 1\r\n\r\n        self.batch_size = int(batch_size)\r\n        self._max_length = max_length if max_length else self._DEFAULT_MAX_LENGTH\r\n        self._max_gen_toks = max_gen_toks\r\n        self.batch_sizes = {}\r\n\r\n        # Initialize new model and tokenizer instances\r\n        self.model = deepsparse.TextGeneration(\r\n            model_path=pretrained,\r\n            sequence_length=self._max_length,\r\n            batch_size=batch_size,\r\n        )\r\n        self.tokenizer = tokenizer if tokenizer else self.model.tokenizer\r\n        self.config = self.model.config\r\n\r\n    def tok_encode(self, string: str) -> List[int]:\r\n        return self.tokenizer.encode(string)\r\n\r\n    def tok_decode(self, tokens: List[int]) -> str:\r\n        return self.tokenizer.decode(tokens)\r\n\r\n    @property\r\n    def eot_token_id(self):\r\n        # we use EOT because end of *text* is more accurate for what we're doing than end of *sentence*\r\n        return self.tokenizer.eos_token_id\r\n\r\n    @property\r\n    def prefix_token_id(self):\r\n        # it is used as prefix for loglikelihood\r\n        if self.tokenizer.bos_token_id is not None:\r\n            return self.tokenizer.bos_token_id\r\n        return self.tokenizer.eos_token_id\r\n\r\n    @property\r\n    def max_length(self) -> int:\r\n        return self._max_length\r\n\r\n    @property\r\n    def max_gen_toks(self) -> int:\r\n        return self._max_gen_toks\r\n\r\n    def loglikelihood(self, requests) -> List[Tuple[float, bool]]:\r\n        \"\"\"\r\n        Copied directly from\r\n        https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/models/huggingface.py\r\n        \"\"\"\r\n        new_reqs = []\r\n        for context, continuation in [req.args for req in requests]:\r\n            if context == \"\":\r\n                raise NotImplementedError(\r\n                    \"Implementing empty context is not supported yet\"\r\n                )\r\n            context_enc, continuation_enc = self._encode_pair(context, continuation)\r\n\r\n            new_reqs.append(((context, continuation), context_enc, continuation_enc))\r\n\r\n        return self._loglikelihood_tokens(new_reqs)\r\n\r\n    def _loglikelihood_tokens(\r\n        self,\r\n        requests: List[Tuple[Tuple[str, str], List[int], List[int]]],\r\n        disable_tqdm: bool = False,\r\n    ) -> List[Tuple[float, bool]]:\r\n        \"\"\"\r\n        The function to compute the loglikelihood of the continuation\r\n        tokens given the context tokens.\r\n\r\n        This function is an adapted version of the original function from\r\n        https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/models/huggingface.py\r\n        \"\"\"\r\n        res = []\r\n\r\n        def _collate(x):\r\n            \"\"\"Defines the key for the sorted method\"\"\"\r\n            toks = x[1] + x[2]\r\n            return -len(toks), tuple(toks)\r\n\r\n        re_ord = utils.Reorderer(requests, _collate)\r\n\r\n        for chunk in tqdm(\r\n            list(lm_eval.models.utils.chunks(re_ord.get_reordered(), self.batch_size)),\r\n            disable=disable_tqdm,\r\n        ):\r\n            batch_inp = []\r\n            batch_cache_key = []\r\n            batch_continuation_enc = []\r\n            # len(chunk) is the batch_size\r\n            for cache_key, context_enc, continuation_enc in chunk:\r\n                # how this all works (illustrated on a causal decoder-only setup):\r\n                #          CTX      CONT\r\n                # inp    0 1 2 3|4 5 6 7 8 9   <- last token is deleted by inp[:, :-1]\r\n                # model  \\               \\\r\n                # logits   1 2 3|4 5 6 7 8 9   <- the ctx half gets tossed out by the\r\n                # cont_toks      4 5 6 7 8 9      [:, -len(continuation_enc):, :self.vocab_size] slice # noqa: E501\r\n\r\n                inp = (context_enc + continuation_enc)[-(self.max_length + 1) :][:-1]\r\n\r\n                batch_inp.append(self.tokenizer.decode(inp))\r\n                batch_cache_key.append(cache_key)\r\n                batch_continuation_enc.append(continuation_enc)\r\n\r\n            response = self.model(\r\n                prompt=batch_inp,\r\n                max_new_tokens=0,\r\n                output_scores=True,\r\n                include_prompt_logits=True,\r\n            )\r\n\r\n            for resp, continuation_enc, cache_key in zip(\r\n                response.generations, batch_continuation_enc, batch_cache_key\r\n            ):\r\n                # (seq_len, vocab_size)\r\n                multi_scores = resp.score\r\n\r\n                from deepsparse.utils.data import numpy_log_softmax\r\n\r\n                # (seq_len, vocab_size) but with softmax applied\r\n                multi_logits = numpy_log_softmax(multi_scores, axis=1)\r\n                # toss out the context half of the sequence\r\n                # (cont_len, vocab_size)\r\n                continuation_multi_logits = multi_logits[-len(continuation_enc) :]\r\n\r\n                # pick out the logits for the continuation tokens\r\n                # (cont_len,)\r\n                continuation_logits = continuation_multi_logits[\r\n                    numpy.arange(len(continuation_enc)), continuation_enc\r\n                ]\r\n                # check if the tokens generated greedly are the same\r\n                # as the expected continuation\r\n                greedy_tokens = continuation_multi_logits.argmax(axis=1)\r\n                max_equal = greedy_tokens.tolist() == continuation_enc\r\n\r\n                # Answer: (log prob, is-exact-match)\r\n                answer = (float(continuation_logits.sum()), bool(max_equal))\r\n\r\n                res.append(answer)\r\n\r\n                if cache_key is not None:\r\n                    # special case: loglikelihood_rolling produces a number of loglikelihood requests\r\n                    # all with cache key None. instead do add_partial on the per-example level\r\n                    # in the loglikelihood_rolling() function for those.\r\n                    self.cache_hook.add_partial(\"loglikelihood\", cache_key, answer)\r\n\r\n        return re_ord.get_original(res)\r\n\r\n    def loglikelihood_rolling(self, requests: List[Instance]) -> List[float]:\r\n        raise NotImplementedError(\r\n            \"The method not required by any of our current task integrations so far\"\r\n        )\r\n\r\n    def generate_until(self, requests: List[Instance]) -> List[str]:\r\n        \"\"\"\r\n        The function to generate a certain number of new tokens\r\n        given a context.\r\n\r\n        This function is an adapted version of the original function from\r\n        https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/models/openai_completions.py\r\n        \"\"\"\r\n        if not requests:\r\n            return []\r\n        res = []\r\n        requests = [req.args for req in requests]\r\n\r\n        def _collate(x):\r\n            toks = self.tok_encode(x[0])\r\n            return len(toks), x[0]\r\n\r\n        re_ord = utils.Reorderer(requests, _collate)\r\n\r\n        def sameuntil_chunks(xs, size):\r\n            ret = []\r\n            lastuntil = xs[0][1]\r\n            for x in xs:\r\n                if len(ret) >= size or x[1] != lastuntil:\r\n                    yield ret, lastuntil\r\n                    ret = []\r\n                    lastuntil = x[1]\r\n                ret.append(x)\r\n\r\n            if ret:\r\n                yield ret, lastuntil\r\n\r\n        pbar = tqdm(total=len(requests))\r\n        for chunk, request_args in tqdm(\r\n            list(sameuntil_chunks(re_ord.get_reordered(), self.batch_size))\r\n        ):\r\n            inps = []\r\n\r\n            # make a deepcopy since we are changing arguments\r\n            request_args = copy.deepcopy(request_args)\r\n\r\n            self._max_gen_toks = request_args.pop(\"max_gen_toks\", self.max_gen_toks)\r\n\r\n            for context, _ in chunk:\r\n                # add context (prompts) to the list\r\n                inps.append(context)\r\n\r\n            until = request_args.pop(\"until\", [\"<|endoftext|>\"])\r\n            request_args.pop(\"do_sample\", None)\r\n            request_args[\"temperature\"] = request_args.get(\"temperature\", 0)\r\n\r\n            # run inference (generate max_gen_toks tokens)\r\n            out = self.model(\r\n                sequences=inps,\r\n                max_new_tokens=self.max_gen_toks - 1,\r\n                stop=until,\r\n                **request_args,\r\n            )\r\n\r\n            for resp, (context, args_) in zip(out.generations, chunk):\r\n                text = resp.text\r\n                until_ = until\r\n                # split the text at the first occurrence of any of the until tokens\r\n                for term in until_:\r\n                    if len(term) > 0:\r\n                        text = text.split(term)[0]\r\n\r\n                res.append(text)\r\n\r\n                self.cache_hook.add_partial(\r\n                    \"generate_until\", (context, {\"until\": until_}), text\r\n                )\r\n                pbar.update(1)\r\n\r\n        pbar.close()\r\n\r\n        return re_ord.get_original(res)\r\n\r\n    def _encode_pair(\r\n        self, context: str, continuation: str\r\n    ) -> Tuple[List[int], List[int]]:\r\n        \"\"\"\r\n        Copied directly from\r\n        https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/models/huggingface.py\r\n        \"\"\"\r\n        n_spaces = len(context) - len(context.rstrip())\r\n        if n_spaces > 0:\r\n            continuation = context[-n_spaces:] + continuation\r\n            context = context[:-n_spaces]\r\n        whole_enc = self.tok_encode(context + continuation)\r\n        context_enc = self.tok_encode(context)\r\n        context_enc_len = len(context_enc)\r\n        continuation_enc = whole_enc[context_enc_len:]\r\n        return context_enc, continuation_enc\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/models/neuron_optimum.py", "content": "import copy\r\nimport logging\r\nfrom collections import defaultdict\r\nfrom typing import List, Optional, Union\r\n\r\nimport torch\r\nimport torch.nn.functional as F\r\nimport transformers\r\nfrom packaging import version\r\nfrom tqdm import tqdm\r\nfrom transformers import GenerationConfig\r\nfrom transformers.generation import StoppingCriteriaList\r\n\r\nimport lm_eval.models.utils\r\nfrom lm_eval import utils\r\nfrom lm_eval.api.model import TemplateLM\r\nfrom lm_eval.api.registry import register_model\r\nfrom lm_eval.models.utils import stop_sequences_criteria\r\n\r\n\r\ntry:\r\n    NEURON_AVAILABLE = True\r\n    from optimum.neuron import NeuronModelForCausalLM\r\n    from optimum.neuron.generation import TokenSelector\r\n    from optimum.neuron.version import __version__ as optimum_neuron_version\r\nexcept ImportError:\r\n    NeuronModelForCausalLM = object\r\n    NEURON_AVAILABLE = False\r\n\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\n\r\nclass CustomNeuronModelForCausalLM(NeuronModelForCausalLM):\r\n    \"\"\"NeuronModelForCausalLM with `stopping_criteria` in `generate`\"\"\"\r\n\r\n    def generate(\r\n        self,\r\n        input_ids: torch.Tensor,\r\n        attention_mask: Optional[torch.Tensor] = None,\r\n        stopping_criteria: Optional[\"StoppingCriteriaList\"] = None,\r\n        generation_config: Optional[\"GenerationConfig\"] = None,\r\n        **kwargs,\r\n    ) -> torch.LongTensor:\r\n        r\"\"\"\r\n        A streamlined generate() method overriding the transformers.GenerationMixin.generate() method.\r\n\r\n        This method uses the same logits processors/warpers and stopping criteria as the transformers library\r\n        `generate()` method but restricts the generation to greedy search and sampling.\r\n\r\n        It does not support transformers `generate()` advanced options.\r\n\r\n        Please refer to https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationMixin.generate\r\n        for details on generation configuration.\r\n\r\n        Parameters:\r\n            input_ids (`torch.Tensor` of shape `(batch_size, sequence_length)`):\r\n                The sequence used as a prompt for the generation.\r\n            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\r\n                Mask to avoid performing attention on padding token indices.\r\n            generation_config (`~transformers.generation.GenerationConfig`, *optional*):\r\n                The generation configuration to be used as base parametrization for the generation call. `**kwargs`\r\n                passed to generate matching the attributes of `generation_config` will override them. If\r\n                `generation_config` is not provided, default will be used, which had the following loading\r\n                priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model\r\n                configuration. Please note that unspecified parameters will inherit [`~transformers.generation.GenerationConfig`]'s\r\n                default values, whose documentation should be checked to parameterize generation.\r\n\r\n        Returns:\r\n            `torch.Tensor`: A  `torch.FloatTensor`.\r\n        \"\"\"\r\n        # The actual generation configuration is a combination of config and parameters\r\n        generation_config = copy.deepcopy(\r\n            self.generation_config if generation_config is None else generation_config\r\n        )\r\n        model_kwargs = generation_config.update(\r\n            **kwargs\r\n        )  # All unused kwargs must be model kwargs\r\n        # Check model kwargs are actually used by either prepare_inputs_for_generation or forward\r\n        self._validate_model_kwargs(model_kwargs)\r\n\r\n        # Instantiate a TokenSelector for the specified configuration\r\n        selector = TokenSelector.create(\r\n            input_ids, generation_config, self, self.max_length\r\n        )\r\n        selector.stopping_criteria.append(stopping_criteria)\r\n        # Verify that the inputs are compatible with the model static input dimensions\r\n        batch_size, sequence_length = input_ids.shape\r\n        if sequence_length > self.max_length:\r\n            raise ValueError(\r\n                f\"The input sequence length ({sequence_length}) exceeds the model static sequence length ({self.max_length})\"\r\n            )\r\n        padded_input_ids = input_ids\r\n        padded_attention_mask = attention_mask\r\n        if batch_size > self.batch_size:\r\n            raise ValueError(\r\n                f\"The specified batch_size ({batch_size}) exceeds the model static batch size ({self.batch_size})\"\r\n            )\r\n        elif batch_size < self.batch_size and not self.continuous_batching:\r\n            logger.warning(\r\n                \"Inputs will be padded to match the model static batch size. This will increase latency.\"\r\n            )\r\n            padding_shape = [self.batch_size - batch_size, sequence_length]\r\n            padding = torch.full(\r\n                padding_shape, fill_value=self.config.eos_token_id, dtype=torch.int64\r\n            )\r\n            padded_input_ids = torch.cat([input_ids, padding])\r\n            if attention_mask is not None:\r\n                padding = torch.zeros(padding_shape, dtype=torch.int64)\r\n                padded_attention_mask = torch.cat([attention_mask, padding])\r\n\r\n        output_ids = self.generate_tokens(\r\n            padded_input_ids,\r\n            selector,\r\n            batch_size,\r\n            attention_mask=padded_attention_mask,\r\n            **model_kwargs,\r\n        )\r\n        return output_ids[:batch_size, :]\r\n\r\n\r\n@register_model(\"neuronx\")\r\nclass NEURON_HF(TemplateLM):\r\n    \"\"\"\r\n    Enables usage with on AWS Neuron\r\n    using the HuggingFace Transformers + Transformers neuronx library.\r\n    Tested with neuron 2.17.0\r\n    \"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        pretrained: Optional[str] = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\r\n        revision: Optional[str] = \"main\",\r\n        tp_degree: Optional[int] = None,\r\n        subfolder: Optional[str] = None,\r\n        tokenizer: Optional[str] = None,\r\n        truncation: Optional[bool] = False,\r\n        max_length: Optional[int] = None,\r\n        dtype: Optional[Union[str, torch.dtype]] = \"auto\",\r\n        batch_size: Optional[int] = 1,\r\n        low_cpu_mem_usage: Optional[bool] = True,\r\n        trust_remote_code: Optional[bool] = False,\r\n        use_fast_tokenizer: Optional[bool] = True,\r\n        add_bos_token: Optional[bool] = False,\r\n    ) -> None:\r\n        if not NEURON_AVAILABLE:\r\n            raise Exception(\r\n                \"Tried to load neuron model, but neuron is not installed \",\r\n                \"please install neuron via pip install transformers-neuron \",\r\n                \"also make sure you are running on an AWS inf2 instance\",\r\n            )\r\n        if version.parse(optimum_neuron_version) != version.parse(\"0.0.24\"):\r\n            logger.warning(\r\n                '`optimum-neuron` model requires `pip install \"optimum[neuronx]>=0.0.17\" '\r\n                \"preferably using the Hugging Face Neuron Deep Learning AMI (Ubuntu 22.04) \"\r\n                \"https://aws.amazon.com/marketplace/pp/prodview-gr3e6yiscria2 \"\r\n                f\"You are using optimum-neuron={optimum_neuron_version}\"\r\n            )\r\n        super().__init__()\r\n\r\n        assert isinstance(pretrained, str)\r\n        assert isinstance(batch_size, (int, str))\r\n\r\n        self.batch_size_per_gpu = int(batch_size)\r\n        batch_size = int(batch_size)\r\n\r\n        self._config = transformers.AutoConfig.from_pretrained(\r\n            pretrained,\r\n            revision=revision,\r\n            trust_remote_code=trust_remote_code,\r\n        )\r\n\r\n        revision = str(revision)  # cast to string if not already one\r\n        # TODO: update this to be less of a hack once subfolder is fixed in HF\r\n        revision = revision + (\"/\" + subfolder if subfolder is not None else \"\")\r\n\r\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(\r\n            pretrained if tokenizer is None else tokenizer,\r\n            revision=revision,\r\n            trust_remote_code=trust_remote_code,\r\n            use_fast=use_fast_tokenizer,\r\n        )\r\n\r\n        neuron_config = getattr(self._config, \"neuron\", None)\r\n        if neuron_config is None:\r\n            # Check export parameters\r\n            if tp_degree is not None:\r\n                assert isinstance(tp_degree, int), (\r\n                    f\"tp_degree must be set to an integer,\"\r\n                    f\" but is tp_degree=`{tp_degree}` with type=`{type(tp_degree)}`.\"\r\n                    \"Set it to a number lower than the number of neuron cores on your instance.\"\r\n                    \" For inf2.xlarge and inf2.8xlarge, set it to `2`.\"\r\n                    \" For inf2.24xlarge, set it <= `12`.\"\r\n                    \" For inf2.48xlarge, set it <= `24`.\"\r\n                )\r\n            torch_dtype = lm_eval.models.utils.get_dtype(dtype)\r\n\r\n            if torch_dtype == torch.float16:\r\n                self.amp_dtype = \"f16\"\r\n            elif torch_dtype == torch.bfloat16:\r\n                self.amp_dtype = \"bf16\"\r\n            elif torch_dtype == torch.float32:\r\n                self.amp_dtype = \"f32\"\r\n            else:\r\n                raise NotImplementedError(\r\n                    \"Only float16/bfloat16/float32 are supported.\"\r\n                )\r\n\r\n            print(f\"{'='*20} \\n exporting model to neuron\")\r\n            self.model = CustomNeuronModelForCausalLM.from_pretrained(\r\n                pretrained,\r\n                revision=revision,\r\n                trust_remote_code=trust_remote_code,\r\n                low_cpu_mem_usage=low_cpu_mem_usage,\r\n                export=True,\r\n                batch_size=batch_size,\r\n                num_cores=tp_degree,\r\n                auto_cast_type=self.amp_dtype,\r\n                sequence_length=max_length,\r\n            )\r\n            neuron_config = self.model.config.neuron\r\n            print(\r\n                f\"SUCCESS: neuron model exported with config {neuron_config}. \\n {'='*20}\"\r\n            )\r\n        else:\r\n            print(\r\n                f\"{'='*20} \\n loading neuron model with config\" f\" {neuron_config}...\"\r\n            )\r\n            self.model = CustomNeuronModelForCausalLM.from_pretrained(\r\n                pretrained,\r\n                revision=revision,\r\n                trust_remote_code=trust_remote_code,\r\n                low_cpu_mem_usage=low_cpu_mem_usage,\r\n            )\r\n            print(f\"SUCCESS: neuron model loaded. \\n {'='*20}\")\r\n\r\n        self.truncation = truncation\r\n\r\n        self.vocab_size = self.tokenizer.vocab_size\r\n        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\r\n        self.add_bos_token = add_bos_token\r\n\r\n        self.batch_schedule = 1\r\n        self.batch_sizes = {}\r\n\r\n    @property\r\n    def config(self):\r\n        # return the associated transformers.AutoConfig for the given pretrained model.\r\n        return self._config\r\n\r\n    @property\r\n    def eot_token_id(self):\r\n        # we use EOT because end of *text* is more accurate for what we're doing than end of *sentence*\r\n        return self.tokenizer.eos_token_id\r\n\r\n    @property\r\n    def prefix_token_id(self):\r\n        # it is used as prefix for loglikelihood\r\n        return self.tokenizer.bos_token_id or self.tokenizer.eos_token_id\r\n\r\n    @property\r\n    def max_length(self):\r\n        return self.model.max_length\r\n\r\n    @property\r\n    def max_gen_toks(self) -> int:\r\n        return 256\r\n\r\n    @property\r\n    def batch_size(self):\r\n        return self.batch_size_per_gpu\r\n\r\n    @property\r\n    def device(self):\r\n        \"\"\"device are neuron cores, but the created tensors are on CPU.\"\"\"\r\n        return \"cpu\"\r\n\r\n    @property\r\n    def rank(self):\r\n        return 0\r\n\r\n    @property\r\n    def world_size(self):\r\n        return 1\r\n\r\n    def tok_encode(self, string: str, left_truncate_len=None, add_special_tokens=None):\r\n        \"\"\" \"\"\"\r\n        if add_special_tokens is None:\r\n            add_special_tokens = False or self.add_bos_token\r\n\r\n        encoding = self.tokenizer.encode(string, add_special_tokens=add_special_tokens)\r\n\r\n        # left-truncate the encoded context to be at most `left_truncate_len` tokens long\r\n        if left_truncate_len:\r\n            encoding = encoding[-left_truncate_len:]\r\n\r\n        return encoding\r\n\r\n    def tok_batch_encode(\r\n        self,\r\n        strings: List[str],\r\n        padding_side: str = \"left\",\r\n        left_truncate_len: int = None,\r\n        truncation: bool = False,\r\n    ):\r\n        # encode a batch of strings. converts to tensors and pads automatically, unlike tok_encode.\r\n        old_padding_side = self.tokenizer.padding_side\r\n        self.tokenizer.padding_side = padding_side\r\n\r\n        add_special_tokens = False or self.add_bos_token\r\n\r\n        encoding = self.tokenizer(\r\n            strings,\r\n            truncation=truncation,\r\n            padding=\"longest\",\r\n            return_tensors=\"pt\",\r\n            add_special_tokens=add_special_tokens,\r\n        )\r\n        if left_truncate_len:\r\n            encoding[\"input_ids\"] = encoding[\"input_ids\"][:, -left_truncate_len:]\r\n            encoding[\"attention_mask\"] = encoding[\"attention_mask\"][\r\n                :, -left_truncate_len:\r\n            ]\r\n        self.tokenizer.padding_side = old_padding_side\r\n\r\n        return encoding[\"input_ids\"], encoding[\"attention_mask\"]\r\n\r\n    def tok_decode(self, tokens):\r\n        return self.tokenizer.decode(tokens)\r\n\r\n    def _model_generate(self, context, max_length, stop, **generation_kwargs):\r\n        # we require users to pass do_sample=True explicitly\r\n        # for non-greedy gen. This should be reevaluated when considering beam search.\r\n\r\n        with torch.inference_mode():\r\n            if \"do_sample\" not in generation_kwargs.keys():\r\n                generation_kwargs[\"do_sample\"] = False\r\n\r\n            stopping_criteria = stop_sequences_criteria(\r\n                self.tokenizer,\r\n                stop + [self.tokenizer.decode([self.config.eos_token_id])],\r\n                1,\r\n                context.shape[0],\r\n            )\r\n\r\n            return self.model.generate(\r\n                input_ids=context,\r\n                max_length=max_length,\r\n                stopping_criteria=stopping_criteria,\r\n                pad_token_id=self.eot_token_id,\r\n                use_cache=True,\r\n                **generation_kwargs,\r\n            )\r\n\r\n    def _select_cont_toks(self, logits, contlen=None, inplen=None):\r\n        assert (\r\n            contlen and inplen\r\n        ), \"Must pass input len and cont. len to select scored logits for causal LM\"\r\n        # discard right-padding.\r\n        # also discard the input/context tokens. we'll only score continuations.\r\n        logits = logits[inplen - contlen : inplen]\r\n\r\n        return logits\r\n\r\n    def loglikelihood_rolling(self, requests, disable_tqdm: bool = False):\r\n        loglikelihoods = []\r\n\r\n        adaptive_batch_size = None\r\n\r\n        for (string,) in tqdm(\r\n            [req.args for req in requests], disable=(disable_tqdm or (self.rank != 0))\r\n        ):\r\n            rolling_token_windows = list(\r\n                map(\r\n                    utils.make_disjoint_window,\r\n                    utils.get_rolling_token_windows(\r\n                        token_list=self.tok_encode(string),\r\n                        prefix_token=self.prefix_token_id,\r\n                        max_seq_len=self.max_length,\r\n                        context_len=1,\r\n                    ),\r\n                )\r\n            )\r\n\r\n            # TODO: Right now, we pass single EOT token to the Encoder and the full context to the decoder, in seq2seq case\r\n            rolling_token_windows = [(None,) + x for x in rolling_token_windows]\r\n\r\n            pad_amnt = 0\r\n            if self.world_size > 1:\r\n                # We pad out the external document-level iterator so the inner iterator doesn't hang\r\n                mytensor = torch.tensor(len(rolling_token_windows), device=self.device)\r\n                gathered = (\r\n                    self.accelerator.gather(mytensor).cpu().detach().numpy().tolist()\r\n                )\r\n\r\n                pad_amnt = max(gathered) - gathered[self.rank]\r\n                if pad_amnt > 0:\r\n                    rolling_token_windows += pad_amnt * [rolling_token_windows[0]]\r\n\r\n            string_nll = self._loglikelihood_tokens(\r\n                rolling_token_windows,\r\n                disable_tqdm=True,\r\n                override_bs=adaptive_batch_size,\r\n            )\r\n\r\n            if (self.world_size > 1) and (pad_amnt > 0):\r\n                string_nll = [x[0] for x in string_nll[:-pad_amnt]]\r\n            else:\r\n                # discard is_greedy\r\n                string_nll = [x[0] for x in string_nll]\r\n\r\n            string_nll = sum(string_nll)\r\n            loglikelihoods.append(string_nll)\r\n            # cache this loglikelihood_rolling request\r\n            self.cache_hook.add_partial(\"loglikelihood_rolling\", (string,), string_nll)\r\n        return loglikelihoods\r\n\r\n    def _loglikelihood_tokens(\r\n        self, requests, disable_tqdm: bool = False, override_bs=None\r\n    ):\r\n        # TODO: implement some kind of efficient-request-middleware that lumps together requests with the same context\r\n        res = []\r\n\r\n        def _collate(x):\r\n            # the negative sign on len(toks) sorts descending - this has a few advantages:\r\n            # - time estimates will always be over not underestimates, which is more useful for planning\r\n            # - to know the size of a batch when going through the list, you know the first one is always the batch\r\n            #   padded context length. this is useful to simplify the batching logic and more importantly to make\r\n            #   automatic adaptive batches much much easier to implement\r\n            # - any OOMs will happen right away rather than near the end\r\n\r\n            toks = x[1] + x[2]\r\n            return -len(toks), tuple(toks)\r\n\r\n        re_ord = utils.Reorderer(requests, _collate)\r\n\r\n        n_reordered_requests = len(re_ord.get_reordered())  # noqa\r\n        # automatic (variable) batch size detection for vectorization\r\n        # pull longest context sample from request\r\n\r\n        chunks = lm_eval.models.utils.chunks(\r\n            re_ord.get_reordered(),\r\n            n=self.batch_size,\r\n            fn=None,\r\n        )\r\n\r\n        for chunk in tqdm(chunks, disable=(disable_tqdm or (self.rank != 0))):\r\n            inps = []\r\n            cont_toks_list = []\r\n            inplens = []\r\n\r\n            conts = []  # noqa\r\n            encoder_attns = []  # noqa\r\n\r\n            padding_len_inp = None\r\n            padding_len_cont = None  # noqa\r\n            # because vectorizing is annoying, we first convert each (context, continuation) pair to padded\r\n            # tensors, then we pack them together into a batch, call the model, and then pick it all apart\r\n            # again because vectorizing is annoying\r\n\r\n            for _, context_enc, continuation_enc in chunk:\r\n                # sanity check\r\n                assert len(context_enc) > 0\r\n                assert len(continuation_enc) > 0\r\n                assert len(continuation_enc) <= self.max_length\r\n\r\n                # how this all works (illustrated on a causal decoder-only setup):\r\n                #          CTX      CONT\r\n                # inp    0 1 2 3|4 5 6 7 8 9   <- last token is deleted by inp[:, :-1]\r\n                # model  \\               \\\r\n                # logits   1 2 3|4 5 6 7 8 9   <- the ctx half gets tossed out by the\r\n                # cont_toks      4 5 6 7 8 9      [:, -len(continuation_enc):, :self.vocab_size] slice\r\n\r\n                # when too long to fit in context, truncate from the left\r\n                inp = torch.tensor(\r\n                    (context_enc + continuation_enc)[-(self.max_length + 1) :][:-1],\r\n                    dtype=torch.long,\r\n                    device=self.device,\r\n                )\r\n                (inplen,) = inp.shape\r\n\r\n                padding_len_inp = (\r\n                    max(padding_len_inp, inplen)\r\n                    if padding_len_inp is not None\r\n                    else inplen\r\n                )\r\n\r\n                inps.append(inp)  # [1, inp_length]\r\n                cont_toks_list.append(continuation_enc)\r\n                inplens.append(inplen)\r\n\r\n            # Add dummy inputs up to the model static batch size\r\n            if len(inps) < self.batch_size:\r\n                inps = inps + [\r\n                    torch.zeros_like(inps[0]),\r\n                ] * (self.batch_size - len(inps))\r\n\r\n            masks = [torch.ones_like(inp) for inp in inps]\r\n            batched_inps = lm_eval.models.utils.pad_and_concat(\r\n                padding_len_inp, inps, padding_side=\"right\"\r\n            )  # [batch, padding_len_inp]\r\n\r\n            batched_masks = lm_eval.models.utils.pad_and_concat(\r\n                padding_len_inp, masks, padding_side=\"right\"\r\n            )\r\n            if self.model.model.neuron_config.output_all_logits:\r\n                inputs = self.model.prepare_inputs_for_prefill(\r\n                    batched_inps, batched_masks\r\n                )\r\n                multi_logits = F.log_softmax(\r\n                    self.model.forward(**inputs).logits, dim=-1\r\n                )  # [batch, padding_length (inp or cont), vocab]\r\n            else:\r\n                # The model will only return the logits for the last input token, so we need\r\n                # to iterate over inputs to accumulate logits.\r\n                # To speed things up we use the KV cache as we would do when generating.\r\n                inputs = self.model.prepare_inputs_for_prefill(\r\n                    batched_inps[:, :1], batched_masks[:, :1]\r\n                )\r\n                outputs = [self.model.forward(**inputs).logits]\r\n                for i in range(1, padding_len_inp):\r\n                    inputs = self.model.prepare_inputs_for_decode(\r\n                        batched_inps[:, : i + 1], batched_masks[:, : i + 1]\r\n                    )\r\n                    outputs.append(self.model.forward(**inputs).logits)\r\n                multi_logits = F.log_softmax(torch.concat(outputs, dim=1), dim=-1)\r\n\r\n            for (cache_key, _, _), logits, inplen, cont_toks in zip(\r\n                chunk, multi_logits, inplens, cont_toks_list\r\n            ):\r\n                # Slice to original seq length\r\n                contlen = len(cont_toks)\r\n                # take only logits in the continuation\r\n                # (discard context toks if decoder-only ; discard right-padding)\r\n                # also discards + checks for \"virtual tokens\" in the causal LM's input window\r\n                # from prompt/prefix tuning tokens, if applicable\r\n                ctx_len = inplen + (logits.shape[0] - padding_len_inp)\r\n                logits = self._select_cont_toks(logits, contlen=contlen, inplen=ctx_len)\r\n                logits = logits.unsqueeze(0)  # [1, seq, vocab]\r\n\r\n                # Check if per-token argmax is exactly equal to continuation\r\n                greedy_tokens = logits.argmax(dim=-1)\r\n                cont_toks = torch.tensor(\r\n                    cont_toks, dtype=torch.long, device=self.device\r\n                ).unsqueeze(0)  # [1, seq]\r\n                max_equal = (greedy_tokens == cont_toks).all()\r\n\r\n                # Obtain log-probs at the corresponding continuation token indices\r\n                # last_token_slice = logits[:, -1, :].squeeze(0).tolist()\r\n                logits = torch.gather(logits, 2, cont_toks.unsqueeze(-1)).squeeze(\r\n                    -1\r\n                )  # [1, seq]\r\n\r\n                # Answer: (log prob, is-exact-match)\r\n                answer = (float(logits.sum()), bool(max_equal))\r\n\r\n                res.append(answer)\r\n\r\n                if cache_key is not None:\r\n                    # special case: loglikelihood_rolling produces a number of loglikelihood requests\r\n                    # all with cache key None. instead do add_partial on the per-example level\r\n                    # in the loglikelihood_rolling() function for those.\r\n                    self.cache_hook.add_partial(\"loglikelihood\", cache_key, answer)\r\n\r\n        return re_ord.get_original(res)\r\n\r\n    def generate_until(self, requests, disable_tqdm: bool = False):\r\n        res = defaultdict(list)\r\n        re_ords = {}\r\n\r\n        def _collate(x):\r\n            # the negative sign on len(toks) sorts descending - this has a few advantages:\r\n            # - time estimates will always be over not underestimates, which is more useful for planning\r\n            # - to know the size of a batch when going through the list, you know the first one is always the batch\r\n            #   padded context length. this is useful to simplify the batching logic and more importantly to make\r\n            #   automatic adaptive batches much much easier to implement\r\n            # - any OOMs will happen right away rather than near the end\r\n            toks = self.tok_encode(x[0])\r\n            return -len(toks), x[0]\r\n\r\n        # we group requests by their generation_kwargs,\r\n        # so that we don't try to execute e.g. greedy sampling and temp=0.8 sampling\r\n        # in the same batch.\r\n        grouper = lm_eval.models.utils.Grouper(requests, lambda x: str(x.args[1]))\r\n        for key, reqs in grouper.get_grouped().items():\r\n            # within each set of reqs for given kwargs, we reorder by token length, descending.\r\n            re_ords[key] = utils.Reorderer([req.args for req in reqs], _collate)\r\n\r\n        pbar = tqdm(total=len(requests), disable=(disable_tqdm or (self.rank != 0)))\r\n\r\n        # for each different set of kwargs, we execute all requests, by batch.\r\n        for key, re_ord in re_ords.items():\r\n            chunks = lm_eval.models.utils.chunks(\r\n                re_ord.get_reordered(), n=self.batch_size\r\n            )\r\n            for chunk in tqdm(chunks, disable=self.rank != 0):\r\n                contexts, all_gen_kwargs = zip(*chunk)\r\n                # we assume all gen kwargs in the batch are the same\r\n                # this is safe to assume because the `grouper` object ensures it.\r\n                gen_kwargs = all_gen_kwargs[0]\r\n                # unpack our keyword arguments.\r\n                until = None\r\n                if isinstance(gen_kwargs, dict):\r\n                    kwargs = copy.deepcopy(gen_kwargs)  # edge case for repeats > 1\r\n                    if \"until\" in kwargs.keys():\r\n                        until = kwargs.pop(\"until\")\r\n                        if isinstance(until, str):\r\n                            until = [until]\r\n                        elif not isinstance(until, list):\r\n                            raise ValueError(\r\n                                f\"Expected `kwargs['until']` to be of type Union[str,list] but got {until}\"\r\n                            )\r\n                else:\r\n                    raise ValueError(\r\n                        f\"Expected `kwargs` to be of type `dict` but got {kwargs}\"\r\n                    )\r\n                # add EOS token to stop sequences\r\n                eos = self.tok_decode(self.eot_token_id)\r\n                if not until:\r\n                    until = [eos]\r\n                else:\r\n                    until.append(eos)\r\n                if \"max_gen_toks\" in kwargs.keys():\r\n                    max_gen_toks = kwargs.pop(\"max_gen_toks\")\r\n                else:\r\n                    max_gen_toks = self.max_gen_toks\r\n                # first stop sequence is used to halt generation upon encountering\r\n                primary_until = [until[0]]\r\n\r\n                max_ctx_len = self.max_length - max_gen_toks\r\n\r\n                # encode, pad, and truncate contexts for this batch\r\n                context_enc, attn_masks = self.tok_batch_encode(\r\n                    contexts,\r\n                    left_truncate_len=max_ctx_len,\r\n                    truncation=self.truncation,\r\n                )\r\n                context_enc = context_enc.to(self.device)\r\n                attn_masks = attn_masks.to(self.device)\r\n\r\n                if \"max_length\" not in kwargs:\r\n                    kwargs[\"max_length\"] = context_enc.shape[1] + max_gen_toks\r\n\r\n                # perform batched generation\r\n                cont = self._model_generate(\r\n                    context=context_enc,\r\n                    attention_mask=attn_masks,\r\n                    stop=primary_until,\r\n                    **kwargs,\r\n                )\r\n\r\n                cont_toks_list = cont.tolist()\r\n                for cont_toks, context in zip(cont_toks_list, contexts):\r\n                    # discard context + left-padding toks if using causal decoder-only LM\r\n                    cont_toks = cont_toks[context_enc.shape[1] :]\r\n\r\n                    s = self.tok_decode(cont_toks)\r\n\r\n                    # use secondary stop seqs to cut off should-have-been-stopped content post-hoc\r\n                    for term in until:\r\n                        if len(term) > 0:\r\n                            # ignore '' separator,\r\n                            # for seq2seq case where self.tok_decode(self.eot_token_id) = ''\r\n                            s = s.split(term)[0]\r\n\r\n                    res[key].append(s)\r\n\r\n                    self.cache_hook.add_partial(\r\n                        \"generate_until\", (context, gen_kwargs), s\r\n                    )\r\n                    pbar.update(1)\r\n            # reorder this group of results back to original unsorted form\r\n            res[key] = re_ord.get_original(res[key])\r\n\r\n        pbar.close()\r\n\r\n        return grouper.get_original(res)\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/models/nemo_lm.py", "content": "# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n\r\nimport importlib\r\nimport pathlib\r\nfrom copy import deepcopy\r\nfrom typing import List, Literal\r\n\r\nimport filelock\r\nimport numpy as np\r\nimport torch\r\nfrom tqdm import tqdm\r\n\r\nfrom lm_eval.api.instance import Instance\r\nfrom lm_eval.api.model import LM\r\nfrom lm_eval.api.registry import register_model\r\nfrom lm_eval.models.utils import Collator\r\nfrom lm_eval.utils import (\r\n    eval_logger,\r\n    get_rolling_token_windows,\r\n    make_disjoint_window,\r\n    simple_parse_args_string,\r\n)\r\n\r\n\r\ndef _patch_pretrained_cfg(\r\n    pretrained_cfg, trainer, tensor_model_parallel_size, pipeline_model_parallel_size\r\n):\r\n    try:\r\n        import omegaconf\r\n    except ModuleNotFoundError:\r\n        raise Exception(\r\n            \"Attempted to use 'nemo_lm' model type, but package `nemo` is not installed\"\r\n            \"Please install nemo following the instructions in the README: either with a NVIDIA PyTorch or NeMo container, \"\r\n            \"or installing nemo following https://github.com/NVIDIA/NeMo.\",\r\n        )\r\n\r\n    omegaconf.OmegaConf.set_struct(pretrained_cfg, True)\r\n    with omegaconf.open_dict(pretrained_cfg):\r\n        attributes_to_update = {\r\n            \"sequence_parallel\": False,\r\n            \"activations_checkpoint_granularity\": None,\r\n            \"activations_checkpoint_method\": None,\r\n            \"precision\": trainer.precision,\r\n            \"global_batch_size\": None,\r\n            \"tensor_model_parallel_size\": tensor_model_parallel_size,\r\n            \"pipeline_model_parallel_size\": pipeline_model_parallel_size,\r\n            \"apply_rope_fusion\": False,\r\n        }\r\n        for name, value in attributes_to_update.items():\r\n            if hasattr(pretrained_cfg, name):\r\n                pretrained_cfg[name] = value\r\n    return pretrained_cfg\r\n\r\n\r\ndef _get_target_from_class(target_class) -> str:\r\n    return f\"{target_class.__module__}.{target_class.__name__}\"\r\n\r\n\r\ndef load_model(\r\n    model_path: str,\r\n    trainer,\r\n    tensor_model_parallel_size: int,\r\n    pipeline_model_parallel_size: int,\r\n) -> torch.nn.Module:\r\n    try:\r\n        from nemo.collections.nlp.models.language_modeling.megatron_gpt_model import (\r\n            MegatronGPTModel,\r\n        )\r\n        from nemo.collections.nlp.parts.nlp_overrides import NLPSaveRestoreConnector\r\n    except ModuleNotFoundError:\r\n        raise Exception(\r\n            \"Attempted to use 'nemo_lm' model type, but package `nemo` is not installed\"\r\n            \"Please install nemo following the instructions in the README: either with a NVIDIA PyTorch or NeMo container, \"\r\n            \"or installing nemo following https://github.com/NVIDIA/NeMo.\",\r\n        )\r\n    model_path = pathlib.Path(model_path)\r\n\r\n    save_restore_connector = NLPSaveRestoreConnector()\r\n    if model_path.is_dir():\r\n        save_restore_connector.model_extracted_dir = model_path.as_posix()\r\n    pretrained_cfg = save_restore_connector.restore_from(\r\n        None, model_path.as_posix(), return_config=True, trainer=trainer\r\n    )\r\n    if not hasattr(pretrained_cfg, \"target\"):\r\n        pretrained_cfg[\"target\"] = _get_target_from_class(MegatronGPTModel)\r\n\r\n    pretrained_cfg = _patch_pretrained_cfg(\r\n        pretrained_cfg,\r\n        trainer,\r\n        tensor_model_parallel_size=tensor_model_parallel_size,\r\n        pipeline_model_parallel_size=pipeline_model_parallel_size,\r\n    )\r\n\r\n    model_to_load_path = model_path\r\n    override_config = pretrained_cfg\r\n\r\n    module_name, class_name = override_config.target.rsplit(\".\", 1)\r\n    model_class = getattr(importlib.import_module(module_name), class_name)\r\n\r\n    # monkeypatch _build_tokenizer method to be process-safe\r\n    tokenizer_lock = filelock.FileLock(f\"/tmp/{model_path.name}.tokenizer.lock\")\r\n\r\n    def _synced_build_tokenizer(self):\r\n        with tokenizer_lock:\r\n            self._original_build_tokenizer()\r\n\r\n    model_class._original_build_tokenizer = model_class._build_tokenizer\r\n    model_class._build_tokenizer = _synced_build_tokenizer\r\n\r\n    model = model_class.restore_from(\r\n        restore_path=model_to_load_path.as_posix(),\r\n        trainer=trainer,\r\n        override_config_path=override_config,\r\n        save_restore_connector=save_restore_connector,\r\n        map_location=f\"cuda:{trainer.local_rank}\",\r\n    )\r\n\r\n    model.freeze()\r\n    model.training = False\r\n    try:\r\n        # Have to turn off activations_checkpoint_method for inference\r\n        model.model.language_model.encoder.activations_checkpoint_method = None\r\n    except AttributeError:\r\n        pass\r\n    return model\r\n\r\n\r\ndef setup_distributed_environment(trainer):\r\n    try:\r\n        from nemo.utils.app_state import AppState\r\n    except ModuleNotFoundError:\r\n        raise Exception(\r\n            \"Attempted to use 'nemo_lm' model type, but package `nemo` is not installed\"\r\n            \"Please install nemo following the instructions in the README: either with a NVIDIA PyTorch or NeMo container, \"\r\n            \"or installing nemo following https://github.com/NVIDIA/NeMo.\",\r\n        )\r\n\r\n    def dummy():\r\n        return\r\n\r\n    if trainer.strategy.launcher is not None:\r\n        trainer.strategy.launcher.launch(dummy, trainer=trainer)\r\n    trainer.strategy.setup_environment()\r\n\r\n    app_state = AppState()\r\n\r\n    return app_state\r\n\r\n\r\n@register_model(\"nemo_lm\")\r\nclass NeMoLM(LM):\r\n    def __init__(\r\n        self,\r\n        path: str,\r\n        max_length: int = 4096,\r\n        batch_size: int = 1,\r\n        max_gen_toks: int = 256,\r\n        devices: int = 1,\r\n        num_nodes: int = 1,\r\n        tensor_model_parallel_size: int = 1,\r\n        pipeline_model_parallel_size: int = 1,\r\n        precision: Literal[\r\n            \"16-mixed\",\r\n            \"bf16-mixed\",\r\n            \"32-true\",\r\n            \"64-true\",\r\n            64,\r\n            32,\r\n            16,\r\n            \"64\",\r\n            \"32\",\r\n            \"16\",\r\n            \"bf16\",\r\n        ] = \"bf16\",\r\n        **kwargs,\r\n    ):\r\n        try:\r\n            from nemo.collections.nlp.modules.common.text_generation_utils import (\r\n                generate,\r\n            )\r\n            from nemo.collections.nlp.parts.nlp_overrides import NLPDDPStrategy\r\n            from pytorch_lightning.trainer.trainer import Trainer\r\n\r\n            self.generate = generate\r\n        except ModuleNotFoundError:\r\n            raise Exception(\r\n                \"Attempted to use 'nemo_lm' model type, but package `nemo` is not installed\"\r\n                \"Please install nemo following the instructions in the README: either with a NVIDIA PyTorch or NeMo container, \"\r\n                \"or installing nemo following https://github.com/NVIDIA/NeMo.\",\r\n            )\r\n\r\n        super().__init__()\r\n\r\n        if (\r\n            tensor_model_parallel_size == 1\r\n            and pipeline_model_parallel_size == 1\r\n            and devices > 1\r\n        ):\r\n            eval_logger.info(\r\n                f\"The number of data replicas for evaluation is {devices}.\"\r\n            )\r\n            eval_logger.info(f\"The total number of devices is {devices}.\")\r\n            eval_logger.info(\r\n                \"No tensor parallelism or pipeline parallelism is applied.\"\r\n            )\r\n\r\n        elif tensor_model_parallel_size * pipeline_model_parallel_size == devices:\r\n            eval_logger.info(\r\n                f\"Setting tensor parallelism to {tensor_model_parallel_size} and pipeline parallelism to {pipeline_model_parallel_size}.\"\r\n            )\r\n            eval_logger.info(f\"The total number of devices is {devices}.\")\r\n            eval_logger.info(\"No data parallelism is applied.\")\r\n\r\n        else:\r\n            raise ValueError(\r\n                \"Please set the product of tensor_model_parallel_size and pipeline_model_parallel_size\"\r\n                \"equal to the specified number of devices.\"\r\n            )\r\n\r\n        if num_nodes > 1:\r\n            raise ValueError(\r\n                \"A number of nodes greater than 1 is not supported yet. Please set num_nodes as 1.\"\r\n            )\r\n\r\n        trainer = Trainer(\r\n            strategy=NLPDDPStrategy(),\r\n            devices=devices,\r\n            accelerator=\"gpu\",\r\n            num_nodes=num_nodes,\r\n            precision=precision,\r\n            logger=False,\r\n            enable_checkpointing=False,\r\n            use_distributed_sampler=False,\r\n        )\r\n        # Modify the following flags only for data replication\r\n        if (\r\n            tensor_model_parallel_size == 1\r\n            and pipeline_model_parallel_size == 1\r\n            and devices > 1\r\n        ):\r\n            self._device = torch.device(f\"cuda:{trainer.global_rank}\")\r\n            self._rank = trainer.global_rank\r\n            self._world_size = trainer.world_size\r\n        self.model = load_model(\r\n            path,\r\n            trainer,\r\n            tensor_model_parallel_size=tensor_model_parallel_size,\r\n            pipeline_model_parallel_size=pipeline_model_parallel_size,\r\n        ).cuda()\r\n        self.tokenizer = self.model.tokenizer\r\n        self.app_state = setup_distributed_environment(trainer)\r\n\r\n        self._max_length = max_length\r\n        self._batch_size = int(batch_size)\r\n        self._max_gen_toks = max_gen_toks\r\n\r\n    @classmethod\r\n    def create_from_arg_string(cls, arg_string, additional_config=None):\r\n        args = simple_parse_args_string(arg_string)\r\n        if additional_config:\r\n            args[\"batch_size\"] = additional_config.get(\"batch_size\", 1)\r\n\r\n        return cls(**args)\r\n\r\n    @property\r\n    def eot_token_id(self):\r\n        try:\r\n            return self.tokenizer.eos_id\r\n        except AttributeError:\r\n            return None\r\n\r\n    @property\r\n    def max_length(self):\r\n        return self._max_length\r\n\r\n    @property\r\n    def max_gen_toks(self):\r\n        return self._max_gen_toks\r\n\r\n    @property\r\n    def batch_size(self):\r\n        return self._batch_size\r\n\r\n    @property\r\n    def device(self):\r\n        return self._device\r\n\r\n    @property\r\n    def rank(self):\r\n        return self._rank\r\n\r\n    @property\r\n    def world_size(self):\r\n        return self._world_size\r\n\r\n    @property\r\n    def accelerator(self):\r\n        return self._Accelerator(self.world_size)\r\n\r\n    class _Accelerator:\r\n        def __init__(self, world_size):\r\n            self.world_size = world_size\r\n\r\n        def wait_for_everyone(self):\r\n            torch.distributed.barrier()\r\n\r\n        def gather(self, local_tensor):\r\n            gathered_tensors = [\r\n                torch.zeros(1, dtype=local_tensor.dtype).cuda()\r\n                for _ in range(self.world_size)\r\n            ]\r\n            torch.distributed.all_gather(gathered_tensors, local_tensor)\r\n            return torch.cat(gathered_tensors)\r\n\r\n    def tok_encode(self, string: str):\r\n        return self.tokenizer.text_to_ids(string)\r\n\r\n    def tok_decode(self, tokens):\r\n        return self.tokenizer.ids_to_text(tokens)\r\n\r\n    def _encode_pair(self, context, continuation):\r\n        n_spaces = len(context) - len(context.rstrip())\r\n        if n_spaces > 0:\r\n            continuation = context[-n_spaces:] + continuation\r\n            context = context[:-n_spaces]\r\n        whole_enc = self.tok_encode(context + continuation)\r\n        context_enc = self.tok_encode(context)\r\n        context_enc_len = len(context_enc)\r\n        continuation_enc = whole_enc[context_enc_len:]\r\n        return context_enc, continuation_enc\r\n\r\n    def loglikelihood(self, requests):\r\n        new_reqs = []\r\n        for context, continuation in [req.args for req in requests]:\r\n            if context == \"\":\r\n                # end of text as context\r\n                context_enc, continuation_enc = (\r\n                    [self.eot_token_id],\r\n                    self.tok_encode(continuation),\r\n                )\r\n            else:\r\n                context_enc, continuation_enc = self._encode_pair(context, continuation)\r\n\r\n            new_reqs.append(((context, continuation), context_enc, continuation_enc))\r\n\r\n        return self._loglikelihood_tokens(new_reqs)\r\n\r\n    def loglikelihood_rolling(\r\n        self, requests: List[Instance], disable_tqdm: bool = False\r\n    ) -> List[float]:\r\n        loglikelihoods = []\r\n\r\n        for (string,) in tqdm([req.args for req in requests], disable=disable_tqdm):\r\n            rolling_token_windows = list(\r\n                map(\r\n                    make_disjoint_window,\r\n                    get_rolling_token_windows(\r\n                        token_list=self.tok_encode(string),\r\n                        prefix_token=self.eot_token_id,\r\n                        max_seq_len=self.max_length - 1,\r\n                        context_len=1,\r\n                    ),\r\n                )\r\n            )\r\n\r\n            rolling_token_windows = [(None,) + x for x in rolling_token_windows]\r\n\r\n            string_nll = self._loglikelihood_tokens(\r\n                rolling_token_windows,\r\n            )\r\n\r\n            # discard is_greedy\r\n            string_nll = [x[0] for x in string_nll]\r\n\r\n            string_nll = sum(string_nll)\r\n            loglikelihoods.append(string_nll)\r\n\r\n            # cache this loglikelihood_rolling request\r\n            self.cache_hook.add_partial(\"loglikelihood_rolling\", (string,), string_nll)\r\n        return loglikelihoods\r\n\r\n    def _loglikelihood_tokens(self, requests, disable_tqdm=False):\r\n        res = []\r\n\r\n        def _collate(x):\r\n            toks = x[1] + x[2]\r\n            return -len(toks), tuple(toks)\r\n\r\n        re_ord = Collator(requests, sort_fn=_collate)\r\n        chunks = re_ord.get_batched(n=self.batch_size, batch_fn=None)\r\n        pbar = tqdm(\r\n            total=len(requests),\r\n            disable=(disable_tqdm or (self.rank != 0)),\r\n            desc=\"Running loglikelihood requests\",\r\n        )\r\n        for chunk in chunks:\r\n            inps = []\r\n            ctxlens = []\r\n            contlens = []\r\n\r\n            for _, context_enc, continuation_enc in chunk:\r\n                # Leave one token for generation. Tokens_to_generate = 0 breaks NeMo.\r\n                inp = (context_enc + continuation_enc)[-(self.max_length - 1) :]\r\n\r\n                ctxlen = len(context_enc) - max(\r\n                    0, len(context_enc) + len(continuation_enc) - (self.max_length - 1)\r\n                )\r\n                ctxlens.append(ctxlen)\r\n                contlens.append(len(continuation_enc))\r\n\r\n                inps.append(self.tok_decode(inp))\r\n\r\n            output = self.generate(\r\n                self.model,\r\n                inputs=inps,\r\n                tokens_to_generate=1,\r\n                min_tokens_to_generate=1,\r\n                compute_logprob=True,\r\n                all_probs=True,\r\n            )\r\n\r\n            batch_token_ids = np.asarray(output[\"token_ids\"])[:, :-1]\r\n            batch_logprobs = output[\"logprob\"][:, :-1]\r\n            batch_full_logprob = output[\"full_logprob\"][:, :-1, :]\r\n\r\n            # Compute greedy tokens for entire batch rather than calling it with proper ctxlen for each sample.\r\n            # Additional tokens for each sample will be trimmed later.\r\n            min_ctxlen = min(ctxlens)\r\n\r\n            # Use min_ctxlen-1 instead of min_ctxlen since full_logprobs are not returns for the first token.\r\n            batch_greedy_tokens = (\r\n                torch.argmax(batch_full_logprob[:, min_ctxlen - 1 :, :], -1)\r\n                .cpu()\r\n                .numpy()\r\n            )\r\n\r\n            for token_ids, greedy_tokens, logprobs, ctxlen, contlen, (\r\n                cache_key,\r\n                _,\r\n                _,\r\n            ) in zip(\r\n                batch_token_ids,\r\n                batch_greedy_tokens,\r\n                batch_logprobs,\r\n                ctxlens,\r\n                contlens,\r\n                chunk,\r\n            ):\r\n                # Trim at contlen since shorter contexts in a batch will have more than one token generated.\r\n                # Use ctxlen-1 instead of ctxlen same as for full_logprob in batch_greedy_tokens calculation\r\n                logprobs = (logprobs[ctxlen - 1 :])[:contlen]\r\n                logprob = sum(logprobs).tolist()\r\n\r\n                continuation_tokens = (token_ids[ctxlen:])[:contlen]\r\n                len_diff = ctxlen - min_ctxlen\r\n                is_greedy = continuation_tokens == (greedy_tokens[len_diff:])[:contlen]\r\n                if not isinstance(is_greedy, bool):\r\n                    is_greedy = is_greedy.all()\r\n                answer = (logprob, is_greedy)\r\n\r\n                if cache_key is not None:\r\n                    # special case: loglikelihood_rolling produces a number of loglikelihood requests\r\n                    # all with cache key None. instead do add_partial on the per-example level\r\n                    # in the loglikelihood_rolling() function for those.\r\n                    self.cache_hook.add_partial(\"loglikelihood\", cache_key, answer)\r\n\r\n                res.append(answer)\r\n                pbar.update(1)\r\n\r\n        pbar.close()\r\n\r\n        return re_ord.get_original(res)\r\n\r\n    def generate_until(self, requests):\r\n        if not requests:\r\n            return []\r\n        res = []\r\n\r\n        def get_until(req_args):\r\n            until = req_args.get(\"until\", [])\r\n            until = deepcopy(until)  # prevent from modifying req_args for cache_key\r\n            if self.tokenizer.ids_to_tokens([self.eot_token_id])[0] not in until:\r\n                until.append(self.tokenizer.ids_to_tokens([self.eot_token_id])[0])\r\n            return until\r\n\r\n        def _collate(x):\r\n            toks = self.tok_encode(x[0])\r\n            return len(toks), x[0]\r\n\r\n        re_ords = Collator(\r\n            [reg.args for reg in requests], sort_fn=_collate, group_by=\"gen_kwargs\"\r\n        )\r\n        chunks = re_ords.get_batched(n=self.batch_size, batch_fn=None)\r\n        for chunk in chunks:\r\n            contexts, all_gen_kwargs = zip(*chunk)\r\n            # we assume all gen kwargs in the batch are the same\r\n            # this is safe to assume because the `grouper` object ensures it.\r\n            req_args = all_gen_kwargs[0]\r\n            # unpack our keyword arguments.\r\n            until = get_until(req_args)\r\n            max_gen_toks = req_args.get(\"max_gen_toks\", self.max_gen_toks)\r\n\r\n            remaining_length = self.max_length - max_gen_toks\r\n            contexts = []\r\n            for context, _ in chunk:\r\n                encoded_context = self.tok_encode(context)\r\n                encoded_context = encoded_context[-remaining_length:]\r\n                contexts.append(self.tok_decode(encoded_context))\r\n\r\n            output = self.generate(\r\n                self.model,\r\n                inputs=contexts,\r\n                tokens_to_generate=max_gen_toks,\r\n                end_strings=until,\r\n                greedy=True,\r\n            )\r\n\r\n            answers = output[\"sentences\"]\r\n\r\n            continuations = []\r\n            for context, answer in zip(contexts, answers):\r\n                continuations.append(answer[len(context) :])\r\n\r\n            for term in until:\r\n                continuations = [answer.split(term)[0] for answer in continuations]\r\n\r\n            for request, answer in zip(chunk, continuations):\r\n                self.cache_hook.add_partial(\"greedy_until\", request, answer)\r\n                res.append(answer)\r\n\r\n        return re_ords.get_original(res)\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/models/gemini.py", "content": "import os\r\nfrom functools import cached_property\r\nfrom typing import Any, Dict, List, Tuple, Union\r\n\r\nfrom tqdm import tqdm\r\n\r\nfrom lm_eval import utils\r\nfrom lm_eval.api.model import LM\r\nfrom lm_eval.api.registry import register_model\r\nfrom lm_eval.models.utils import retry_on_specific_exceptions\r\n\r\n\r\neval_logger = utils.eval_logger\r\n\r\n\r\ndef gemini_completion(\r\n    client,\r\n    model: str,\r\n    prompt: str,\r\n    max_tokens_to_sample: int,\r\n    temperature: float,\r\n    stop: List[str],\r\n    **kwargs: Any,\r\n) -> str:\r\n    \"\"\"Wrapper function around the Gemini API client\r\n\r\n    params:\r\n        client:\r\n            Gemini API client\r\n        prompt: str\r\n            Prompt to feed to the model\r\n        max_tokens_to_sample: int\r\n            Maximum number of tokens to sample from the model\r\n        temperature: float\r\n            Sampling temperature\r\n        stop: List[str]\r\n            List of stop sequences\r\n        kwargs: Any\r\n            Additional model_args to pass to the API client\r\n    \"\"\"\r\n\r\n    try:\r\n        import google.generativeai as genai\r\n    except ModuleNotFoundError:\r\n        raise Exception(\r\n            \"attempted to use 'gemini' LM type, but package `google-generativeai` is not installed. \\\r\nplease install google-generativeai via `pip install 'google-generativeai'`\",\r\n        )\r\n\r\n    def _exception_callback(e: Exception, sleep_time: float) -> None:\r\n        eval_logger.warning(\r\n            f\"RateLimitError occurred: {e.__cause__}\\n Retrying in {sleep_time} seconds\"\r\n        )\r\n\r\n    @retry_on_specific_exceptions(\r\n        on_exceptions=Exception,\r\n        max_retries=None,  # retry forever, consider changing\r\n        on_exception_callback=_exception_callback,\r\n    )\r\n    def completion():\r\n        retry_txt = 'FinishReason.RECITATION'\r\n        while retry_txt == 'FinishReason.RECITATION':\r\n            response = client.generate_content(\r\n                prompt,\r\n                generation_config=genai.GenerationConfig(\r\n                    max_output_tokens=max_tokens_to_sample,\r\n                    temperature=temperature,\r\n                    stop_sequences=stop,\r\n                    **kwargs,\r\n                )\r\n            )\r\n            try:\r\n                retry_txt = str(response.candidates[-1].finish_reason)\r\n            except Exception as e:\r\n                eval_logger.warning(\"Got error \" + str(e))\r\n                retry_txt = ''\r\n        if response.parts:\r\n            return response.parts[-1].text\r\n        else:\r\n            eval_logger.warning(\"No response from Gemini API due to \" + str(response.candidates[-1].finish_reason))\r\n            return \"\"\r\n    return completion()\r\n\r\n\r\n@register_model(\"gemini-completions\")\r\nclass Gemini(LM):\r\n    REQ_CHUNK_SIZE = 20  # TODO: not used\r\n\r\n    def __init__(\r\n        self,\r\n        batch_size: int = 1,\r\n        model: str = \"gemini-2.0-flash-thinking-exp\",\r\n        max_tokens_to_sample: int = None,\r\n        temperature: float = 0,\r\n        **kwargs,  # top_p, top_k, etc.\r\n    ) -> None:\r\n        \"\"\"Gemini API wrapper.\r\n\r\n        :param model: str\r\n            Gemini model e.g. 'gemini-2.0-flash-thinking-exp'\r\n        :param max_tokens_to_sample: int\r\n            Maximum number of tokens to sample from the model\r\n        :param temperature: float\r\n            Sampling temperature\r\n        :param kwargs: Any\r\n            Additional model_args to pass to the API client\r\n        \"\"\"\r\n        super().__init__()\r\n\r\n        try:\r\n            import google.generativeai as genai\r\n        except ModuleNotFoundError:\r\n            raise Exception(\r\n                \"attempted to use 'gemini' LM type, but package `google-generativeai` is not installed. \\\r\n    please install google-generativeai via `pip install 'google-generativeai'`\",\r\n            )\r\n\r\n        self.model = model\r\n        # defaults to os.environ.get(\"GEMINI_API_KEY\")\r\n        self.client = genai.GenerativeModel(model)\r\n        self.temperature = temperature\r\n        self.max_tokens_to_sample = max_tokens_to_sample\r\n        self.tokenizer = None\r\n        self.kwargs = kwargs\r\n\r\n    @property\r\n    def eot_token_id(self):\r\n        raise NotImplementedError(\"No idea about gemini tokenization.\")\r\n\r\n    @property\r\n    def max_length(self) -> int:\r\n        return 8192\r\n\r\n    @property\r\n    def max_gen_toks(self) -> int:\r\n        return self.max_tokens_to_sample\r\n\r\n    @property\r\n    def batch_size(self):\r\n        # Isn't used because we override _loglikelihood_tokens\r\n        raise NotImplementedError(\"No support for logits.\")\r\n\r\n    @property\r\n    def device(self):\r\n        # Isn't used because we override _loglikelihood_tokens\r\n        raise NotImplementedError(\"No support for logits.\")\r\n\r\n    def tok_encode(self, string: str) -> List[int]:\r\n        # return self.tokenizer.encode(string).ids\r\n        raise NotImplementedError(\"No idea about gemini tokenization.\")\r\n\r\n    def tok_decode(self, tokens: List[int]) -> str:\r\n        raise NotImplementedError(\"No idea about gemini tokenization.\")\r\n\r\n    def _loglikelihood_tokens(self, requests, disable_tqdm: bool = False):\r\n        raise NotImplementedError(\"No support for logits.\")\r\n\r\n    def generate_until(self, requests, disable_tqdm: bool = False) -> List[str]:\r\n        try:\r\n            import google.generativeai as genai\r\n        except ModuleNotFoundError:\r\n            raise Exception(\r\n                \"attempted to use 'gemini' LM type, but package `google-generativeai` is not installed. \\\r\n    please install google-generativeai via `pip install 'google-generativeai'`\",\r\n            )\r\n\r\n        if not requests:\r\n            return []\r\n\r\n        _requests: List[Tuple[str, dict]] = [req.args for req in requests]\r\n\r\n        res = []\r\n        for request in tqdm(_requests, disable=disable_tqdm):\r\n            try:\r\n                inp = request[0]\r\n                request_args = request[1]\r\n                # generation_kwargs\r\n                until = request_args.get(\"until\")\r\n                max_gen_toks = request_args.get(\"max_gen_toks\", self.max_length)\r\n                temperature = request_args.get(\"temperature\", self.temperature)\r\n                response = gemini_completion(\r\n                    client=self.client,\r\n                    model=self.model,\r\n                    prompt=inp,\r\n                    max_tokens_to_sample=max_gen_toks,\r\n                    temperature=temperature,\r\n                    stop=until,\r\n                    **self.kwargs,\r\n                )\r\n                res.append(response)\r\n                #import pdb; pdb.set_trace()\r\n\r\n                self.cache_hook.add_partial(\"generate_until\", request, response)\r\n            except Exception as e:  # type: ignore # noqa: F821\r\n                eval_logger.critical(f\"API error {e}\")\r\n                break\r\n\r\n        return res\r\n\r\n    def _model_call(self, inps):\r\n        # Isn't used because we override _loglikelihood_tokens\r\n        raise NotImplementedError()\r\n\r\n    def _model_generate(self, context, max_length, eos_token_id):\r\n        # Isn't used because we override generate_until\r\n        raise NotImplementedError()\r\n\r\n    def loglikelihood(self, requests, disable_tqdm: bool = False):\r\n        raise NotImplementedError(\"No support for logits.\")\r\n\r\n    def loglikelihood_rolling(self, requests, disable_tqdm: bool = False):\r\n        raise NotImplementedError(\"No support for logits.\")\r\n\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/models/huggingface.py", "content": "import copy\r\nimport os\r\nfrom datetime import timedelta\r\nfrom pathlib import Path\r\nfrom typing import Dict, List, Literal, Optional, Tuple, Union\r\n\r\nimport torch\r\nimport torch.nn.functional as F\r\nimport transformers\r\nfrom accelerate import (\r\n    Accelerator,\r\n    InitProcessGroupKwargs,\r\n    find_executable_batch_size,\r\n)\r\nfrom accelerate.utils import get_max_memory\r\nfrom huggingface_hub import HfApi\r\nfrom packaging import version\r\nfrom peft import PeftModel\r\nfrom peft import __version__ as PEFT_VERSION\r\nfrom tqdm import tqdm\r\nfrom transformers.models.auto.modeling_auto import (\r\n    MODEL_FOR_CAUSAL_LM_MAPPING_NAMES,\r\n    MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES,\r\n)\r\n\r\nfrom lm_eval import utils\r\nfrom lm_eval.api.instance import Instance\r\nfrom lm_eval.api.model import TemplateLM\r\nfrom lm_eval.api.registry import register_model\r\nfrom lm_eval.models.utils import (\r\n    Collator,\r\n    clear_torch_cache,\r\n    configure_pad_token,\r\n    get_dtype,\r\n    pad_and_concat,\r\n    stop_sequences_criteria,\r\n)\r\n\r\nif int(os.getenv(\"O1INFERENCE\", 0)):\r\n    import sys\r\n    sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))))\r\n    from tot.o1_reabse_text import o1\r\n\r\neval_logger = utils.eval_logger\r\n\r\n\r\n@register_model(\"hf-auto\", \"hf\", \"huggingface\")\r\nclass HFLM(TemplateLM):\r\n    \"\"\"\r\n    An abstracted Huggingface model class. Enables usage with both models of\r\n    `transformers.AutoModelForCausalLM` and `transformers.AutoModelForSeq2SeqLM` classes.\r\n\r\n    Supports data-parallel multi-GPU with HF Accelerate.\r\n    \"\"\"\r\n\r\n    AUTO_MODEL_CLASS = None\r\n    _DEFAULT_MAX_LENGTH = 2048\r\n\r\n    def __init__(\r\n        self,\r\n        pretrained: Union[str, transformers.PreTrainedModel],\r\n        backend: Optional[Literal[\"default\", \"causal\", \"seq2seq\"]] = \"default\",\r\n        # override whether the model should be treated as decoder-only (causal) or encoder-decoder (seq2seq)\r\n        revision: Optional[str] = \"main\",\r\n        subfolder: Optional[str] = None,\r\n        tokenizer: Optional[\r\n            Union[\r\n                str,\r\n                transformers.PreTrainedTokenizer,\r\n                transformers.PreTrainedTokenizerFast,\r\n            ]\r\n        ] = None,\r\n        truncation: Optional[bool] = False,\r\n        logits_cache: bool = True,\r\n        max_length: Optional[int] = None,\r\n        device: Optional[str] = \"cuda\",\r\n        dtype: Optional[Union[str, torch.dtype]] = \"auto\",\r\n        batch_size: Optional[Union[int, str]] = 1,\r\n        max_batch_size: Optional[int] = 64,\r\n        trust_remote_code: Optional[bool] = False,\r\n        use_fast_tokenizer: Optional[bool] = True,\r\n        add_bos_token: Optional[bool] = False,\r\n        prefix_token_id: Optional[int] = None,\r\n        # arguments used for splitting a model across GPUs naively.\r\n        # only used if `parallelize=True`.\r\n        parallelize: Optional[bool] = False,\r\n        max_memory_per_gpu: Optional[Union[int, str]] = None,\r\n        max_cpu_memory: Optional[Union[int, str]] = None,\r\n        offload_folder: Optional[Union[str, os.PathLike]] = \"./offload\",\r\n        # PEFT, delta weights and quantization options\r\n        peft: Optional[str] = None,\r\n        delta: Optional[str] = None,\r\n        autogptq: Optional[Union[bool, str]] = False,\r\n        **kwargs,\r\n    ) -> None:\r\n        super().__init__()\r\n\r\n        # optionally: take in an already-initialized transformers.PreTrainedModel\r\n        if not isinstance(pretrained, str):\r\n            eval_logger.warning(\r\n                \"`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\"\r\n            )\r\n            assert not parallelize, \"`parallelize=True` is not compatible with passing pre-initialized model to `pretrained`\"\r\n            self._model = pretrained\r\n            self._device = self._model.device\r\n            self._config = self._model.config\r\n            gpus = 0\r\n\r\n        else:\r\n            assert isinstance(device, str)\r\n            assert isinstance(pretrained, str)\r\n            assert isinstance(batch_size, (int, str))\r\n\r\n            gpus = torch.cuda.device_count()\r\n            accelerator_kwargs = InitProcessGroupKwargs(timeout=timedelta(weeks=52))\r\n            accelerator = Accelerator(kwargs_handlers=[accelerator_kwargs])\r\n            if accelerator.num_processes > 1:\r\n                self.accelerator = accelerator\r\n\r\n            if \"npu\" in accelerator.device.type:\r\n                gpus = torch.npu.device_count()\r\n\r\n            # using one process with no model parallelism\r\n            if not (parallelize or accelerator.num_processes > 1):\r\n                # use user-passed device\r\n                device_list = set(\r\n                    [\"cuda\", \"cpu\"]\r\n                    + [f\"cuda:{i}\" for i in range(gpus)]\r\n                    + [\"mps\", \"mps:0\"]\r\n                    + [f\"npu:{i}\" for i in range(gpus)]\r\n                )\r\n                if device and device in device_list:\r\n                    self._device = torch.device(device)\r\n                    eval_logger.info(f\"Using device '{device}'\")\r\n                    if device in (\"mps\", \"mps:0\") and version.parse(\r\n                        torch.__version__\r\n                    ) < version.parse(\"2.1\"):\r\n                        raise RuntimeError(\r\n                            f\"mps requires torch >= 2.1. You have {torch.__version__}\"\r\n                        )\r\n                else:\r\n                    eval_logger.info(\"Device not specified\")\r\n                    eval_logger.info(f\"Cuda Available? {torch.cuda.is_available()}\")\r\n                    self._device = (\r\n                        torch.device(\"cuda\")\r\n                        if torch.cuda.is_available()\r\n                        else torch.device(\"cpu\")\r\n                    )\r\n            else:  # Parallelism managed by accelerate\r\n                if device != \"cuda\":\r\n                    eval_logger.info(\r\n                        f\"Using `accelerate launch` or `parallelize=True`, device '{device}' will be overridden when placing model.\"\r\n                    )\r\n                # TODO: include in warning that `load_in_8bit` etc. affect this too\r\n                self._device = (\r\n                    self.accelerator.device\r\n                    if hasattr(self, \"accelerator\")\r\n                    else torch.device(device)\r\n                )\r\n\r\n            revision = str(revision)  # cast to string if not already one\r\n            # TODO: update this to be less of a hack once subfolder is fixed in HF\r\n            revision = revision + (\"/\" + subfolder if subfolder is not None else \"\")\r\n\r\n            self._get_config(\r\n                pretrained,\r\n                revision=revision,\r\n                trust_remote_code=trust_remote_code,\r\n            )\r\n\r\n        # determine which of 'causal' and 'seq2seq' backends to use\r\n        self._get_backend(\r\n            config=self.config, backend=backend, trust_remote_code=trust_remote_code\r\n        )\r\n\r\n        # load tokenizer so we know tokenizer vocabulary size before loading model and PEFT\r\n        self._create_tokenizer(\r\n            pretrained,\r\n            tokenizer,\r\n            revision=revision,\r\n            trust_remote_code=trust_remote_code,\r\n            use_fast_tokenizer=use_fast_tokenizer,\r\n        )\r\n\r\n        # if we passed `pretrained` as a string, initialize our model now\r\n        if isinstance(pretrained, str):\r\n            self._create_model(\r\n                pretrained=pretrained,\r\n                revision=revision,\r\n                dtype=dtype,\r\n                trust_remote_code=trust_remote_code,\r\n                parallelize=parallelize,\r\n                gpus=gpus,\r\n                max_memory_per_gpu=max_memory_per_gpu,\r\n                max_cpu_memory=max_cpu_memory,\r\n                offload_folder=offload_folder,\r\n                peft=peft,\r\n                delta=delta,\r\n                autogptq=autogptq,\r\n                **kwargs,\r\n            )\r\n\r\n        # access self._model through self.model property outside this method\r\n        if isinstance(self.model, torch.nn.Module):\r\n            self.model.eval()\r\n            self.model.tie_weights()\r\n\r\n        self.truncation = truncation\r\n        self.logits_cache = logits_cache\r\n        self.vocab_size = self.tokenizer.vocab_size\r\n        # select (or create) a pad token to use\r\n        self.tokenizer = configure_pad_token(self.tokenizer, model_config=self.config)\r\n\r\n        self.add_bos_token = add_bos_token\r\n        if \"gemma\" in getattr(self.config, \"model_type\", \"\"):\r\n            self.add_bos_token = True\r\n            eval_logger.info(\r\n                f\"Model type is '{self.config.model_type}', part of the Gemma family--a BOS token will be used as Gemma underperforms without it.\"\r\n            )\r\n\r\n        self._max_length = max_length\r\n        self.pretrained = pretrained\r\n        self.delta = delta\r\n        self.peft = peft\r\n        self.revision = revision\r\n        self.batch_schedule = 1\r\n        self.batch_sizes = {}\r\n        self.max_batch_size = max_batch_size\r\n\r\n        if str(batch_size).startswith(\"auto\"):\r\n            batch_size = batch_size.split(\":\")\r\n            self.batch_size_per_gpu = batch_size[0]\r\n            self.batch_schedule = float(batch_size[1]) if len(batch_size) > 1 else 1\r\n        else:\r\n            self.batch_size_per_gpu = int(batch_size)\r\n\r\n        if isinstance(pretrained, str):\r\n            if gpus >= 1 or str(self.device) == \"mps\":\r\n                # TODO: can remove this whole snippet except in the mps case, perhaps?\r\n                if not (parallelize or autogptq or hasattr(self, \"accelerator\")):\r\n                    # place model onto device requested manually,\r\n                    # if not using HF Accelerate or device_map\r\n                    # or any other option that preloads model onto device\r\n                    try:\r\n                        #self.model.to(self.device)\r\n                        self.model.model.to(self.device)\r\n                    except ValueError:\r\n                        eval_logger.debug(\r\n                            \"Failed to place model onto specified device. This may be because the model is quantized via `bitsandbytes` or `device_map` is provided. If the desired GPU is being used, this message is safe to ignore.\"\r\n                        )\r\n            # multigpu data-parallel support when launched with accelerate\r\n            if gpus > 1:\r\n                if accelerator.num_processes > 1:\r\n                    if parallelize:\r\n                        eval_logger.warning(\r\n                            \"You are both using a HF Accelerate `device_map` (`--model_args parallelize=True`) and launching via `accelerate launch`. This will attempt to do model and data parallelism depending on the resources available.\"\r\n                        )\r\n                    elif gpus > accelerator.num_processes:\r\n                        eval_logger.warning(\r\n                            \"WARNING: The number of total system GPUs does not match the number of spawned processes. \"\r\n                            \"If you would like to use data parallelism, please launch the script \"\r\n                            \"with 'accelerate launch *script*'. \"\r\n                            f\"Current run will proceed with {accelerator.num_processes} devices.\"\r\n                        )\r\n                        if self.accelerator.is_local_main_process:\r\n                            eval_logger.info(\r\n                                f\"Using {gpus} devices with data parallelism\"\r\n                            )\r\n\r\n                    self._device = torch.device(f\"{accelerator.device}\")\r\n                    self.accelerator = accelerator\r\n\r\n                    self._rank = self.accelerator.local_process_index\r\n                    self._world_size = self.accelerator.num_processes\r\n                else:\r\n                    # if we aren't launching via accelerate, ditch\r\n                    self._rank = 0\r\n                    self._world_size = 1\r\n        else:\r\n            # if a PreTrainedModel was passed into HFLM, we forgo distributed setup.\r\n            eval_logger.warning(\r\n                \"Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\"\r\n            )\r\n            self._rank = 0\r\n            self._world_size = 1\r\n\r\n        self.custom_prefix_token_id = prefix_token_id\r\n        if prefix_token_id is not None:\r\n            eval_logger.info(\r\n                f\"Loglikelihood prefix token id used in evaluation: {self.prefix_token_id}\"\r\n            )\r\n\r\n    def _get_accelerate_args(\r\n        self,\r\n        parallelize: bool = None,\r\n        device_map: Optional[str] = \"auto\",\r\n        max_memory_per_gpu: Optional[Union[int, str]] = None,\r\n        max_cpu_memory: Optional[Union[int, str]] = None,\r\n        offload_folder: Optional[str] = \"./offload\",\r\n        gpus: Optional[int] = None,\r\n    ) -> dict:\r\n        \"\"\"Returns the kwargs needed to apply `accelerate` in `AutoModel.from_pretrained`.\"\"\"\r\n        num_local_processes = int(os.environ.get(\"LOCAL_WORLD_SIZE\", 1))\r\n        num_machines = int(os.environ.get(\"WORLD_SIZE\", 0)) // num_local_processes\r\n        if (\r\n            num_machines == 0\r\n            and hasattr(self, \"accelerator\")\r\n            and self.accelerator is not None\r\n        ):\r\n            eval_logger.info(\r\n                \"We are not in a distributed setting for accelerate. Setting model_parallel to False.\"\r\n            )\r\n            parallelize = False\r\n\r\n        if parallelize is None:\r\n            # If parallelism is unset by the user, we automatically assign model parallelism\r\n            # if enough extra GPUs are available\r\n            max_memory_all_gpus = get_max_memory()\r\n            # We just want gpu, not cpu, max memory\r\n            if \"cpu\" in max_memory_all_gpus:\r\n                del max_memory_all_gpus[\"cpu\"]\r\n            parallelize = bool(num_local_processes < len(max_memory_all_gpus))\r\n            eval_logger.info(\r\n                f\"Setting model parallel to {parallelize} since \"\r\n                f\"the number of local processes is {num_local_processes} \"\r\n                f\"and the number of GPUs is {len(max_memory_all_gpus)}\"\r\n            )\r\n\r\n        args = {}\r\n        if parallelize:  # Model parallelism will be used\r\n            max_memory = {}\r\n            if max_memory_per_gpu is not None:  # Using the provided memory requirements\r\n                max_memory_per_gpu_map = {\r\n                    device_idx: max_memory_per_gpu for device_idx in range(gpus)\r\n                }\r\n            else:  # Estimating the possible memory requirements\r\n                max_memory_all_gpus = get_max_memory()\r\n                if \"cpu\" in max_memory_all_gpus:\r\n                    del max_memory_all_gpus[\"cpu\"]\r\n                if not hasattr(self, \"accelerator\"):\r\n                    max_memory_per_gpu_map = {\r\n                        k: v for k, v in max_memory_all_gpus.items()\r\n                    }\r\n                else:\r\n                    # use only 1 / num_processes of the GPUs if we are running under accelerate launch\r\n                    max_memory_per_gpu_map = {\r\n                        k: v\r\n                        for k, v in max_memory_all_gpus.items()\r\n                        if k % num_local_processes\r\n                        == (self.accelerator.process_index % num_local_processes)\r\n                    }\r\n            args[\"max_memory\"] = max_memory_per_gpu_map\r\n            args[\"device_map\"] = \"auto\"\r\n            eval_logger.info(\r\n                f\"Model parallel was set to True, setting max memory per GPU to {max_memory_per_gpu_map} and device map to 'auto'\"\r\n            )\r\n\r\n            if max_cpu_memory is not None:\r\n                max_memory[\"cpu\"] = max_cpu_memory\r\n\r\n            args[\"offload_folder\"] = offload_folder\r\n        elif (\r\n            device_map is None\r\n        ):  # No model parallelism, we use the default provided device for our model\r\n            if hasattr(self, \"accelerator\"):\r\n                device_map = {\"\": f\"{self.accelerator.device}\"}\r\n            else:\r\n                device_map = {\"\": str(self.device)}\r\n            args[\"max_memory\"] = None\r\n            args[\"device_map\"] = device_map\r\n            eval_logger.info(\r\n                f\"Model parallel was set to False, max memory was not set, and device map was set to {device_map}\"\r\n            )\r\n        else:\r\n            args[\"max_memory\"] = None\r\n            args[\"device_map\"] = None\r\n            eval_logger.info(\"Model parallel was set to False.\")\r\n\r\n        return args\r\n\r\n    @property\r\n    def config(self):\r\n        # return the associated transformers.AutoConfig for the given pretrained model.\r\n        return self._config\r\n\r\n    @property\r\n    def model(self):\r\n        # returns the model, unwrapping it if using Accelerate\r\n        if hasattr(self, \"accelerator\"):\r\n            return self.accelerator.unwrap_model(self._model)\r\n        else:\r\n            return self._model\r\n\r\n    @property\r\n    def eot_token_id(self):\r\n        # we use EOT because end of *text* is more accurate for what we're doing than end of *sentence*\r\n        return self.tokenizer.eos_token_id\r\n\r\n    @property\r\n    def prefix_token_id(self):\r\n        # it is used as prefix for loglikelihood\r\n        if self.custom_prefix_token_id is not None:\r\n            return self.custom_prefix_token_id\r\n        if self.tokenizer.bos_token_id is not None:\r\n            return self.tokenizer.bos_token_id\r\n        return self.tokenizer.eos_token_id\r\n\r\n    @property\r\n    def max_length(self):\r\n        if self._max_length:  # if max length manually set, return it\r\n            return self._max_length\r\n        seqlen_config_attrs = (\"n_positions\", \"max_position_embeddings\", \"n_ctx\")\r\n        for attr in seqlen_config_attrs:\r\n            if hasattr(self.model.config, attr):\r\n                return getattr(self.model.config, attr)\r\n        if hasattr(self.tokenizer, \"model_max_length\"):\r\n            if self.tokenizer.model_max_length == 1000000000000000019884624838656:\r\n                return self._DEFAULT_MAX_LENGTH\r\n            return self.tokenizer.model_max_length\r\n        return self._DEFAULT_MAX_LENGTH\r\n\r\n    @property\r\n    def max_gen_toks(self) -> int:\r\n        return 256\r\n\r\n    @property\r\n    def batch_size(self):\r\n        return self.batch_size_per_gpu\r\n\r\n    @property\r\n    def device(self):\r\n        return self._device\r\n\r\n    @property\r\n    def rank(self):\r\n        return self._rank\r\n\r\n    @property\r\n    def world_size(self):\r\n        return self._world_size\r\n\r\n    @property\r\n    def tokenizer_name(self) -> str:\r\n        return self.tokenizer.name_or_path.replace(\"/\", \"__\")\r\n\r\n    def _get_backend(\r\n        self,\r\n        config: Union[transformers.PretrainedConfig, transformers.AutoConfig],\r\n        backend: Optional[Literal[\"default\", \"causal\", \"seq2seq\"]] = \"default\",\r\n        trust_remote_code: Optional[bool] = False,\r\n    ) -> None:\r\n        \"\"\"\r\n        Helper method during initialization.\r\n        Determines the backend (\"causal\" (decoder-only) or \"seq2seq\" (encoder-decoder))\r\n        model type to be used.\r\n        sets `self.AUTO_MODEL_CLASS` appropriately if not already set.\r\n        \"\"\"\r\n        # escape hatch: if we're using a subclass that shouldn't follow\r\n        # the default _get_backend logic,\r\n        # then skip over the method.\r\n        # TODO: this seems very much undesirable in some cases--our code in HFLM\r\n        # references AutoModelForCausalLM at times to check for equality\r\n        if self.AUTO_MODEL_CLASS is not None:\r\n            return\r\n\r\n        assert backend in [\"default\", \"causal\", \"seq2seq\"]\r\n\r\n        if backend != \"default\":\r\n            # if we've settled on non-default backend, use that manually\r\n            if backend == \"causal\":\r\n                self.AUTO_MODEL_CLASS = transformers.AutoModelForCausalLM\r\n            elif backend == \"seq2seq\":\r\n                self.AUTO_MODEL_CLASS = transformers.AutoModelForSeq2SeqLM\r\n            eval_logger.info(\r\n                f\"Overrode HF model backend type, and using type '{backend}'\"\r\n            )\r\n        else:\r\n            # determine and use the default HF backend for this model, based on its config + metadata.\r\n            if (\r\n                getattr(config, \"model_type\")\r\n                in MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES\r\n            ):\r\n                # first check if model type is listed under seq2seq models, since some\r\n                # models like MBart are listed in both seq2seq and causal mistakenly in HF transformers.\r\n                # these special cases should be treated as seq2seq models.\r\n                self.AUTO_MODEL_CLASS = transformers.AutoModelForSeq2SeqLM\r\n            elif (\r\n                getattr(self.config, \"model_type\") in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\r\n            ):\r\n                self.AUTO_MODEL_CLASS = transformers.AutoModelForCausalLM\r\n            else:\r\n                if not trust_remote_code:\r\n                    eval_logger.warning(\r\n                        \"HF model type is neither marked as CausalLM or Seq2SeqLM. \\\r\n                    This is expected if your model requires `trust_remote_code=True` but may be an error otherwise.\"\r\n                    )\r\n                # if model type is neither in HF transformers causal or seq2seq model registries\r\n                # then we default to AutoModelForCausalLM\r\n                self.AUTO_MODEL_CLASS = transformers.AutoModelForCausalLM\r\n\r\n        assert self.AUTO_MODEL_CLASS in [\r\n            transformers.AutoModelForCausalLM,\r\n            transformers.AutoModelForSeq2SeqLM,\r\n        ]\r\n        return None\r\n\r\n    def _get_config(\r\n        self,\r\n        pretrained: str,\r\n        revision: str = \"main\",\r\n        trust_remote_code: bool = False,\r\n    ) -> None:\r\n        self._config = transformers.AutoConfig.from_pretrained(\r\n            pretrained,\r\n            revision=revision,\r\n            trust_remote_code=trust_remote_code,\r\n        )\r\n\r\n    def _create_model(\r\n        self,\r\n        pretrained: str,\r\n        revision: Optional[str] = \"main\",\r\n        dtype: Optional[Union[str, torch.dtype]] = \"auto\",\r\n        trust_remote_code: Optional[bool] = False,\r\n        # arguments used for splitting a model across GPUs naively.\r\n        # only used if `parallelize=True`.\r\n        # (accelerate naive PP (device_map) options)\r\n        parallelize: Optional[bool] = False,\r\n        gpus: Optional[int] = None,\r\n        max_memory_per_gpu: Optional[Union[int, str]] = None,\r\n        max_cpu_memory: Optional[Union[int, str]] = None,\r\n        offload_folder: Optional[str] = \"./offload\",\r\n        # PEFT, delta weights and quantization options\r\n        peft: Optional[str] = None,\r\n        delta: Optional[str] = None,\r\n        autogptq: Optional[Union[bool, str]] = False,\r\n        **kwargs,\r\n    ) -> None:\r\n        \"\"\"\r\n        Initializes an HF or HF-compatible PreTrainedModel from scratch\r\n        inside HFLM, using the kwargs passed into self.__init__().\r\n\r\n        Also handles functionality such as AutoGPTQ usage and PEFT wrapping.\r\n\r\n        For future similar extensions to AutoGPTQ that are not core to HF's ecosystem,\r\n        (such as PyTorch models that are nearly, but not quite, fully mirroring\r\n        HF's public interface relied on in this HFLM class)\r\n        please consider subclassing HFLM and overriding this and other methods as needed.\r\n        \"\"\"\r\n\r\n        model_kwargs = kwargs if kwargs else {}\r\n\r\n        model_kwargs.update(\r\n            self._get_accelerate_args(\r\n                parallelize=parallelize,\r\n                device_map=kwargs.get(\"device_map\", None),\r\n                max_memory_per_gpu=max_memory_per_gpu,\r\n                max_cpu_memory=max_cpu_memory,\r\n                offload_folder=offload_folder,\r\n                gpus=gpus,\r\n            )\r\n        )\r\n\r\n        if not autogptq:\r\n            if model_kwargs.get(\"load_in_4bit\", None):\r\n                assert (\r\n                    transformers.__version__ >= \"4.30.0\"\r\n                ), \"load_in_4bit requires transformers >= 4.30.0\"\r\n            if transformers.__version__ >= \"4.30.0\":\r\n                if model_kwargs.get(\"load_in_4bit\", None):\r\n                    if model_kwargs.get(\"bnb_4bit_compute_dtype\", None):\r\n                        model_kwargs[\"bnb_4bit_compute_dtype\"] = get_dtype(\r\n                            model_kwargs[\"bnb_4bit_compute_dtype\"]\r\n                        )\r\n            print(\"os.getenv(O1INFERENCE): \", os.getenv(\"O1INFERENCE\", 0))\r\n            if int(os.getenv(\"O1INFERENCE\", 0)):\r\n                print(\"use o1 inference\")\r\n                step_eos = \"<|reserved_special_token_2|>\"\r\n                think_eos = \"<|reserved_special_token_1|>\"\r\n                answer_eos = \"<|eot_id|>\"\r\n                num_parallel_steps = int(os.getenv(\"O1INFERENCE_NUM_PARALLEL_STEPS\", 1))\r\n                print(f\"hardcoding the model to be o1; num_parallel_steps: {num_parallel_steps}\")\r\n                self._model = o1.from_pretrained(pretrained, tokenizer=None, num_parallel_steps=num_parallel_steps, step_eos=step_eos, think_eos=think_eos, answer_eos=answer_eos).cuda()\r\n            else:\r\n                print(\"not use o1 inference\")\r\n                self._model = self.AUTO_MODEL_CLASS.from_pretrained(\r\n                    pretrained,\r\n                    revision=revision,\r\n                    torch_dtype=get_dtype(dtype),\r\n                    trust_remote_code=trust_remote_code,\r\n                    **model_kwargs,\r\n                )\r\n        else:\r\n            try:\r\n                from auto_gptq import AutoGPTQForCausalLM\r\n            except ModuleNotFoundError:\r\n                raise Exception(\r\n                    \"Tried to load auto_gptq, but auto-gptq is not installed \",\r\n                    \"please install auto-gptq via pip install lm-eval[gptq] or pip install -e .[gptq]\",\r\n                )\r\n\r\n            self._model = AutoGPTQForCausalLM.from_quantized(\r\n                pretrained,\r\n                trust_remote_code=trust_remote_code,\r\n                model_basename=None if autogptq is True else Path(autogptq).stem,\r\n                use_safetensors=True\r\n                if autogptq is True\r\n                else autogptq.endswith(\".safetensors\"),\r\n                **model_kwargs,\r\n            )\r\n\r\n            \r\n\r\n        if peft and delta:\r\n            raise ValueError(\r\n                \"Cannot use both 'peft' and 'delta' options at the same time.\"\r\n            )\r\n\r\n        if peft:\r\n            if model_kwargs.get(\"load_in_4bit\", None):\r\n                if version.parse(PEFT_VERSION) < version.parse(\"0.4.0\"):\r\n                    raise AssertionError(\"load_in_4bit requires peft >= 0.4.0\")\r\n            if self._model.config.vocab_size != len(self.tokenizer):\r\n                # resize model for LoRAs with added tokens\r\n                eval_logger.info(\r\n                    f\"Model config indicates vocab_size='{self._model.config.vocab_size}', but found tokenizer with vocab size '{len(self.tokenizer)}'. Resizing model embedding layer...\"\r\n                )\r\n                self._model.resize_token_embeddings(len(self.tokenizer))\r\n            self._model = PeftModel.from_pretrained(\r\n                self._model, peft, revision=revision\r\n            )\r\n        elif delta:\r\n            if autogptq:\r\n                eval_logger.warning(\r\n                    \"Delta weights might trigger unexpected behavior when used with AutoGPTQ.\"\r\n                )\r\n            _model_delta = self.AUTO_MODEL_CLASS.from_pretrained(\r\n                delta,\r\n                revision=revision,\r\n                torch_dtype=get_dtype(dtype),\r\n                trust_remote_code=trust_remote_code,\r\n                **model_kwargs,\r\n            )\r\n            for name, param in self._model.state_dict().items():\r\n                try:\r\n                    param.data += _model_delta.state_dict()[name]\r\n                except KeyError:\r\n                    raise KeyError(f\"Delta model is missing weights for layer: {name}\")\r\n                except Exception as e:\r\n                    raise RuntimeError(\r\n                        f\"Failed to add delta weights to layer {name}. Error: {e}\"\r\n                    )\r\n\r\n            del _model_delta\r\n\r\n        return None\r\n\r\n    def _create_tokenizer(\r\n        self,\r\n        pretrained: Union[str, transformers.PreTrainedModel],\r\n        tokenizer: Optional[\r\n            Union[\r\n                str,\r\n                transformers.PreTrainedTokenizer,\r\n                transformers.PreTrainedTokenizerFast,\r\n            ]\r\n        ],\r\n        revision: Optional[str] = \"main\",\r\n        trust_remote_code: Optional[bool] = False,\r\n        use_fast_tokenizer: Optional[bool] = True,\r\n    ) -> None:\r\n        \"\"\"\r\n        Helper method during initialization.\r\n\r\n        Create a tokenizer object corresponding to the correct\r\n        tokenizer for value of `pretrained`, or use the pre-initialized tokenizer passed.\r\n        \"\"\"\r\n\r\n        if tokenizer:\r\n            if isinstance(tokenizer, str):\r\n                self.tokenizer = transformers.AutoTokenizer.from_pretrained(\r\n                    tokenizer,\r\n                    revision=revision,\r\n                    trust_remote_code=trust_remote_code,\r\n                    use_fast=use_fast_tokenizer,\r\n                )\r\n            else:\r\n                assert isinstance(\r\n                    tokenizer, transformers.PreTrainedTokenizer\r\n                ) or isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\r\n                self.tokenizer = tokenizer\r\n        else:\r\n            # Get tokenizer based on 'pretrained'\r\n            if isinstance(pretrained, str):\r\n                model_name = pretrained\r\n            else:\r\n                # get the HF hub name via accessor on model\r\n                model_name = self.model.name_or_path\r\n            self.tokenizer = transformers.AutoTokenizer.from_pretrained(\r\n                model_name,\r\n                revision=revision,\r\n                trust_remote_code=trust_remote_code,\r\n                use_fast=use_fast_tokenizer,\r\n            )\r\n        return None\r\n\r\n    def _detect_batch_size(self, requests=None, pos: int = 0):\r\n        if requests:\r\n            _, context_enc, continuation_enc = requests[pos]\r\n            max_length = len(\r\n                (context_enc + continuation_enc)[-(self.max_length + 1) :][:-1]\r\n            )\r\n            max_context_enc = len(context_enc[-(self.max_length + 1) :])\r\n            max_cont_enc = len(continuation_enc[-(self.max_length + 1) :])\r\n        else:\r\n            max_length = self.max_length\r\n            max_context_enc = max_length\r\n            max_cont_enc = max_length\r\n\r\n        # if OOM, then halves batch_size and tries again\r\n        @find_executable_batch_size(starting_batch_size=self.max_batch_size)\r\n        def forward_batch(batch_size):\r\n            if self.AUTO_MODEL_CLASS == transformers.AutoModelForSeq2SeqLM:\r\n                length = max(max_context_enc, max_cont_enc)\r\n                batched_conts = torch.ones(\r\n                    (batch_size, length), device=self.device\r\n                ).long()\r\n                test_batch = torch.ones((batch_size, length), device=self.device).long()\r\n                call_kwargs = {\r\n                    \"attn_mask\": test_batch,\r\n                    \"labels\": batched_conts,\r\n                }\r\n            else:\r\n                call_kwargs = {}\r\n                test_batch = torch.ones(\r\n                    (batch_size, max_length), device=self.device\r\n                ).long()\r\n            for _ in range(5):\r\n                out = F.log_softmax(self._model_call(test_batch, **call_kwargs), dim=-1)  # noqa: F841\r\n\r\n            return batch_size\r\n\r\n        try:\r\n            batch_size = forward_batch()\r\n        except RuntimeError as e:\r\n            if \"No executable batch size found\" in str(e):\r\n                batch_size = 1\r\n            else:\r\n                raise\r\n\r\n        if self.world_size > 1:\r\n            # if multi-GPU, always take minimum over all selected batch sizes\r\n            max_rnk_bs = torch.tensor([batch_size], device=self.device)\r\n            gathered = (\r\n                self.accelerator.gather(max_rnk_bs).cpu().detach().numpy().tolist()\r\n            )\r\n            batch_size = min(gathered)\r\n            clear_torch_cache()\r\n            return batch_size\r\n\r\n        clear_torch_cache()\r\n        return batch_size\r\n\r\n    def tok_encode(\r\n        self, string: str, left_truncate_len=None, add_special_tokens=None\r\n    ) -> List[int]:\r\n        \"\"\" \"\"\"\r\n        # default for None - empty dict, use predefined tokenizer param\r\n        # used for all models except for CausalLM or predefined value\r\n        special_tokens_kwargs = {}\r\n\r\n        # by default for CausalLM - false or self.add_bos_token is set\r\n        if add_special_tokens is None:\r\n            if self.AUTO_MODEL_CLASS == transformers.AutoModelForCausalLM:\r\n                special_tokens_kwargs = {\r\n                    \"add_special_tokens\": False or self.add_bos_token\r\n                }\r\n        # otherwise the method explicitly defines the value\r\n        else:\r\n            special_tokens_kwargs = {\"add_special_tokens\": add_special_tokens}\r\n\r\n        encoding = self.tokenizer.encode(string, **special_tokens_kwargs)\r\n\r\n        # left-truncate the encoded context to be at most `left_truncate_len` tokens long\r\n        if left_truncate_len:\r\n            encoding = encoding[-left_truncate_len:]\r\n\r\n        return encoding\r\n\r\n    def tok_batch_encode(\r\n        self,\r\n        strings: List[str],\r\n        padding_side: str = \"left\",\r\n        left_truncate_len: int = None,\r\n        truncation: bool = False,\r\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\r\n        # encode a batch of strings. converts to tensors and pads automatically, unlike tok_encode.\r\n        old_padding_side = self.tokenizer.padding_side\r\n        self.tokenizer.padding_side = padding_side\r\n\r\n        add_special_tokens = {}\r\n        if self.AUTO_MODEL_CLASS == transformers.AutoModelForCausalLM:\r\n            add_special_tokens = {\"add_special_tokens\": False or self.add_bos_token}\r\n\r\n        encoding = self.tokenizer(\r\n            strings,\r\n            truncation=truncation,\r\n            padding=\"longest\",\r\n            return_tensors=\"pt\",\r\n            **add_special_tokens,\r\n        )\r\n        if left_truncate_len:\r\n            encoding[\"input_ids\"] = encoding[\"input_ids\"][:, -left_truncate_len:]\r\n            encoding[\"attention_mask\"] = encoding[\"attention_mask\"][\r\n                :, -left_truncate_len:\r\n            ]\r\n        self.tokenizer.padding_side = old_padding_side\r\n\r\n        return encoding[\"input_ids\"], encoding[\"attention_mask\"]\r\n\r\n    def tok_decode(self, tokens, skip_special_tokens=True):\r\n        return self.tokenizer.decode(tokens, skip_special_tokens=skip_special_tokens)\r\n\r\n    def _model_call(self, inps, attn_mask=None, labels=None):\r\n        \"\"\"\r\n        :param inps: torch.Tensor\r\n            A torch tensor of shape [batch, (sequence_ctx + sequence_cont)] or of shape\r\n            [batch, sequence_ctx]. the size of sequence may vary from call to call\r\n        :param attn_mask: torch.Tensor, optional\r\n            A torch tensor of shape [batch, (sequence_ctx + sequence_cont)]. Only passed\r\n            (and must be passed) if self.AUTO_MODEL_CLASS is transformers.AutoModelForSeq2SeqLM\r\n        :param labels: torch.Tensor, optional\r\n            A torch tensor of shape [batch, (sequence_ctx + sequence_cont)]. Only passed\r\n            (and must be passed) if self.AUTO_MODEL_CLASS is transformers.AutoModelForSeq2SeqLM\r\n        :return\r\n            A torch tensor of shape [batch, sequence, vocab] with the\r\n        logits returned from the model's decoder\r\n        \"\"\"\r\n        with torch.no_grad():\r\n            if attn_mask is not None or labels is not None:\r\n                assert attn_mask is not None and labels is not None\r\n                assert self.AUTO_MODEL_CLASS == transformers.AutoModelForSeq2SeqLM\r\n                return self.model(\r\n                    input_ids=inps, attention_mask=attn_mask, labels=labels\r\n                ).logits\r\n            else:\r\n                assert self.AUTO_MODEL_CLASS == transformers.AutoModelForCausalLM\r\n                return self.model(inps).logits\r\n\r\n    def _model_generate(self, context, max_length, stop, **generation_kwargs):\r\n        # temperature = 0.0 if not set\r\n        # if do_sample is false and temp==0.0:\r\n        # remove temperature, as do_sample=False takes care of this\r\n        # and we don't want a warning from HF\r\n        generation_kwargs[\"temperature\"] = generation_kwargs.get(\"temperature\", 0.0)\r\n        do_sample = generation_kwargs.get(\"do_sample\", None)\r\n\r\n        # The temperature has to be a strictly positive float -- if it is 0.0, use greedy decoding strategies\r\n        if generation_kwargs.get(\"temperature\") == 0.0 and do_sample is None:\r\n            generation_kwargs[\"do_sample\"] = do_sample = False\r\n\r\n        if do_sample is False and generation_kwargs.get(\"temperature\") == 0.0:\r\n            generation_kwargs.pop(\"temperature\")\r\n\r\n        if int(os.getenv(\"O1INFERENCE\", 0)):\r\n            print(\"O1INFERENCE is set\")\r\n            stopping_criteria = None\r\n        else:\r\n            stopping_criteria = stop_sequences_criteria(\r\n                self.tokenizer, stop, context.shape[1], context.shape[0]\r\n            )\r\n        return self.model.generate(\r\n            input_ids=context,\r\n            max_length=max_length,\r\n            stopping_criteria=stopping_criteria,\r\n            pad_token_id=self.tokenizer.pad_token_id,\r\n            use_cache=True,\r\n            **generation_kwargs,\r\n        )\r\n\r\n    def _select_cont_toks(\r\n        self, logits: torch.Tensor, contlen: int = None, inplen: int = None\r\n    ) -> torch.Tensor:\r\n        if self.AUTO_MODEL_CLASS == transformers.AutoModelForCausalLM:\r\n            assert (\r\n                contlen and inplen\r\n            ), \"Must pass input len and cont. len to select scored logits for causal LM\"\r\n            # discard right-padding.\r\n            # also discard the input/context tokens. we'll only score continuations.\r\n            logits = logits[inplen - contlen : inplen]\r\n        elif self.AUTO_MODEL_CLASS == transformers.AutoModelForSeq2SeqLM:\r\n            assert (\r\n                contlen and not inplen\r\n            ), \"Selecting scored logits for Seq2SeqLM requires only cont. len\"\r\n            # only discard right-padding.\r\n            # the logits input to this fn only contain decoder-side tokens.\r\n            logits = logits[:contlen]\r\n\r\n        return logits\r\n\r\n    def loglikelihood_rolling(\r\n        self, requests: List[Instance], disable_tqdm: bool = False\r\n    ) -> List[float]:\r\n        loglikelihoods = []\r\n\r\n        adaptive_batch_size = None\r\n        if self.batch_size == \"auto\":\r\n            # using rolling window with maximum context\r\n            print(\"Passed argument batch_size = auto. Detecting largest batch size\")\r\n            batch_size = self._detect_batch_size()\r\n            print(f\"Determined Largest batch size: {batch_size}\")\r\n            adaptive_batch_size = batch_size\r\n\r\n        for (string,) in tqdm(\r\n            [req.args for req in requests], disable=(disable_tqdm or (self.rank != 0))\r\n        ):\r\n            rolling_token_windows = list(\r\n                map(\r\n                    utils.make_disjoint_window,\r\n                    utils.get_rolling_token_windows(\r\n                        token_list=self.tok_encode(string),\r\n                        prefix_token=self.prefix_token_id,\r\n                        max_seq_len=self.max_length,\r\n                        context_len=1,\r\n                    ),\r\n                )\r\n            )\r\n\r\n            # TODO: Right now, we pass single EOT token to the Encoder and the full context to the decoder, in seq2seq case\r\n            rolling_token_windows = [(None,) + x for x in rolling_token_windows]\r\n\r\n            pad_amnt = 0\r\n            if self.world_size > 1:\r\n                # We pad out the external document-level iterator so the inner iterator doesn't hang\r\n                mytensor = torch.tensor(len(rolling_token_windows), device=self.device)\r\n                gathered = (\r\n                    self.accelerator.gather(mytensor).cpu().detach().numpy().tolist()\r\n                )\r\n\r\n                pad_amnt = max(gathered) - gathered[self.rank]\r\n                if pad_amnt > 0:\r\n                    rolling_token_windows += pad_amnt * [rolling_token_windows[0]]\r\n\r\n            string_nll = self._loglikelihood_tokens(\r\n                requests=rolling_token_windows,\r\n                disable_tqdm=True,\r\n                override_bs=adaptive_batch_size,\r\n            )\r\n\r\n            if (self.world_size > 1) and (pad_amnt > 0):\r\n                string_nll = [x[0] for x in string_nll[:-pad_amnt]]\r\n            else:\r\n                # discard is_greedy\r\n                string_nll = [x[0] for x in string_nll]\r\n\r\n            string_nll = sum(string_nll)\r\n            loglikelihoods.append(string_nll)\r\n\r\n            # cache this loglikelihood_rolling request\r\n            self.cache_hook.add_partial(\"loglikelihood_rolling\", (string,), string_nll)\r\n\r\n        return loglikelihoods\r\n\r\n    def _batch_scheduler(self, pos, n_reordered_requests):\r\n        sched = pos // int(len(n_reordered_requests) / self.batch_schedule)\r\n        if sched in self.batch_sizes:\r\n            return self.batch_sizes[sched]\r\n        if (len(self.batch_sizes) > 1) and (\r\n            self.batch_sizes[sched - 1] == self.max_batch_size\r\n        ):\r\n            # if previous batch size is already maximal, skip recomputation\r\n            self.batch_sizes[sched] = self.max_batch_size\r\n            return self.batch_sizes[sched]\r\n        print(\r\n            f\"Passed argument batch_size = auto:{self.batch_schedule}. Detecting largest batch size\"\r\n        )\r\n        self.batch_sizes[sched] = self._detect_batch_size(n_reordered_requests, pos)\r\n        print(f\"Determined largest batch size: {self.batch_sizes[sched]}\")\r\n        return self.batch_sizes[sched]\r\n\r\n    def _loglikelihood_tokens(\r\n        self,\r\n        requests: List[Tuple[Tuple[str, str], List[int], List[int]]],\r\n        disable_tqdm: bool = False,\r\n        override_bs: int = None,\r\n    ) -> List[Tuple[float, bool]]:\r\n        # TODO: implement some kind of efficient-request-middleware that lumps together requests with the same context\r\n        res = []\r\n\r\n        def _collate(req: Tuple[Tuple[str, str], List[int], List[int]]):\r\n            \"\"\"Defines the key for the sorted method\"\"\"\r\n            # the negative sign on len(toks) sorts descending - this has a few advantages:\r\n            # - time estimates will always be over not underestimates, which is more useful for planning\r\n            # - to know the size of a batch when going through the list, you know the first one is always the batch\r\n            #   padded context length. this is useful to simplify the batching logic and more importantly to make\r\n            #   automatic adaptive batches much much easier to implement\r\n            # - any OOMs will happen right away rather than near the end\r\n\r\n            toks = req[1] + req[2]\r\n            return -len(toks), tuple(toks)\r\n\r\n        def _lookup_one_token_cont(req: Tuple[Tuple[str, str], List[int], List[int]]):\r\n            \"\"\"Defines the key to group and lookup one-token continuations\"\"\"\r\n            # Use with group_by=\"contexts\" (optional)\"\r\n            # allows for the creation of a lookup, so we can reuse logits in case of one-token continuations.\r\n            # speeds up some multiple-choice tasks proportionally to the number of choices.\r\n            # groups requests by context+continuation[:-1] and infer on one request/group.\r\n            return req[-2] + req[-1][:-1]\r\n\r\n        re_ord = Collator(\r\n            requests,\r\n            sort_fn=_collate,\r\n            group_by=\"contexts\"\r\n            if self.AUTO_MODEL_CLASS == transformers.AutoModelForCausalLM\r\n            and self.logits_cache\r\n            else None,\r\n            group_fn=_lookup_one_token_cont,\r\n        )\r\n\r\n        # automatic (variable) batch size detection for vectorization\r\n        # pull longest context sample from request\r\n        n_reordered_requests = len(re_ord)\r\n        batch_size = (\r\n            self.batch_size\r\n            if self.batch_size != \"auto\"\r\n            else override_bs\r\n            if override_bs is not None\r\n            else 0\r\n        )\r\n        batch_fn = (\r\n            self._batch_scheduler\r\n            if self.batch_size == \"auto\"\r\n            and n_reordered_requests > 0\r\n            and not override_bs\r\n            else None\r\n        )\r\n\r\n        chunks = re_ord.get_batched(n=batch_size, batch_fn=batch_fn)\r\n        pbar = tqdm(\r\n            total=len(requests),\r\n            disable=(disable_tqdm or (self.rank != 0)),\r\n            desc=\"Running loglikelihood requests\",\r\n        )\r\n        for chunk in chunks:\r\n            inps = []\r\n            cont_toks_list = []\r\n            inplens = []\r\n\r\n            conts = []\r\n            encoder_attns = []\r\n\r\n            padding_len_inp = None\r\n            padding_len_cont = None\r\n            # because vectorizing is annoying, we first convert each (context, continuation) pair to padded\r\n            # tensors, then we pack them together into a batch, call the model, and then pick it all apart\r\n            # again because vectorizing is annoying\r\n\r\n            for _, context_enc, continuation_enc in chunk:\r\n                # sanity check\r\n                assert len(context_enc) > 0\r\n                assert len(continuation_enc) > 0\r\n                assert len(continuation_enc) <= self.max_length\r\n\r\n                # how this all works (illustrated on a causal decoder-only setup):\r\n                #          CTX      CONT\r\n                # inp    0 1 2 3|4 5 6 7 8 9   <- last token is deleted by inp[:, :-1]\r\n                # model  \\               \\\r\n                # logits   1 2 3|4 5 6 7 8 9   <- the ctx half gets tossed out by the\r\n                # cont_toks      4 5 6 7 8 9      [:, -len(continuation_enc):, :self.vocab_size] slice\r\n\r\n                # when too long to fit in context, truncate from the left\r\n                if self.AUTO_MODEL_CLASS == transformers.AutoModelForCausalLM:\r\n                    inp = torch.tensor(\r\n                        (context_enc + continuation_enc)[-(self.max_length + 1) :][:-1],\r\n                        dtype=torch.long,\r\n                        device=self.device,\r\n                    )\r\n                    (inplen,) = inp.shape\r\n                elif self.AUTO_MODEL_CLASS == transformers.AutoModelForSeq2SeqLM:\r\n                    inp = torch.tensor(\r\n                        (context_enc)[-self.max_length :],\r\n                        dtype=torch.long,\r\n                        device=self.device,\r\n                    )\r\n                    (inplen,) = inp.shape\r\n\r\n                    # build encoder attn masks\r\n                    encoder_attns.append(torch.ones_like(inp))\r\n\r\n                    cont = torch.tensor(\r\n                        (continuation_enc)[-self.max_length :],\r\n                        # TODO: left-shift these?\r\n                        # TODO: our code assumes we never end up truncating conts for either model type\r\n                        dtype=torch.long,\r\n                        device=self.device,\r\n                    )\r\n                    (contlen,) = cont.shape\r\n\r\n                    conts.append(cont)\r\n\r\n                    padding_len_cont = (\r\n                        max(padding_len_cont, contlen)\r\n                        if padding_len_cont is not None\r\n                        else contlen\r\n                    )\r\n\r\n                padding_len_inp = (\r\n                    max(padding_len_inp, inplen)\r\n                    if padding_len_inp is not None\r\n                    else inplen\r\n                )\r\n\r\n                inps.append(inp)  # [1, inp_length]\r\n                cont_toks_list.append(continuation_enc)\r\n                inplens.append(inplen)\r\n\r\n            # create encoder attn mask and batched conts, if seq2seq\r\n            call_kwargs = {}\r\n            if self.AUTO_MODEL_CLASS == transformers.AutoModelForCausalLM:\r\n                batched_inps = pad_and_concat(\r\n                    padding_len_inp, inps, padding_side=\"right\"\r\n                )  # [batch, padding_len_inp]\r\n            elif self.AUTO_MODEL_CLASS == transformers.AutoModelForSeq2SeqLM:\r\n                # TODO: left-pad encoder inps and mask?\r\n                batched_inps = pad_and_concat(\r\n                    padding_len_inp, inps\r\n                )  # [batch, padding_len_inp]\r\n                batched_conts = pad_and_concat(\r\n                    padding_len_cont, conts\r\n                )  # [batch, padding_len_cont]\r\n                batched_encoder_mask = pad_and_concat(\r\n                    padding_len_inp, encoder_attns\r\n                )  # [batch, padding_len_inp]\r\n                call_kwargs = {\r\n                    \"attn_mask\": batched_encoder_mask,\r\n                    \"labels\": batched_conts,\r\n                }\r\n\r\n            multi_logits = F.log_softmax(\r\n                self._model_call(batched_inps, **call_kwargs), dim=-1\r\n            )  # [batch, padding_length (inp or cont), vocab]\r\n\r\n            for (request_str, ctx_tokens, _), logits, inplen, cont_toks in zip(\r\n                chunk, multi_logits, inplens, cont_toks_list\r\n            ):\r\n                # Slice to original seq length\r\n                contlen = len(cont_toks)\r\n                # take only logits in the continuation\r\n                # (discard context toks if decoder-only ; discard right-padding)\r\n                # also discards + checks for \"virtual tokens\" in the causal LM's input window\r\n                # from prompt/prefix tuning tokens, if applicable\r\n                ctx_len = (\r\n                    inplen + (logits.shape[0] - padding_len_inp)\r\n                    if self.AUTO_MODEL_CLASS == transformers.AutoModelForCausalLM\r\n                    else None\r\n                )\r\n                logits = self._select_cont_toks(logits, contlen=contlen, inplen=ctx_len)\r\n                logits = logits.unsqueeze(0)  # [1, seq, vocab]\r\n\r\n                # Check if per-token argmax is exactly equal to continuation\r\n                greedy_tokens = logits.argmax(dim=-1)\r\n\r\n                # check for one-token continuation cache hits.\r\n                # noop in case group_by != \"contexts\" or no cache hit and returns the\r\n                # original args. Otherwise, expands the logits batch dimension and yields each\r\n                # batch along with matching continuation tokens and prompt strings.\r\n                # logits -> [1, seq, vocab]\r\n                for request_str, cont_toks, logits in re_ord.get_cache(\r\n                    req_str=request_str,\r\n                    cxt_toks=ctx_tokens,\r\n                    cont_toks=cont_toks,\r\n                    logits=logits,\r\n                ):\r\n                    cont_toks = torch.tensor(\r\n                        cont_toks, dtype=torch.long, device=self.device\r\n                    ).unsqueeze(0)  # [1, seq]\r\n                    max_equal = (greedy_tokens == cont_toks).all()\r\n\r\n                    # Obtain log-probs at the corresponding continuation token indices\r\n                    # last_token_slice = logits[:, -1, :].squeeze(0).tolist()\r\n                    logits = torch.gather(logits, 2, cont_toks.unsqueeze(-1)).squeeze(\r\n                        -1\r\n                    )  # [1, seq]\r\n\r\n                    # Answer: (log prob, is-exact-match)\r\n                    answer = (float(logits.sum()), bool(max_equal))\r\n\r\n                    res.append(answer)\r\n\r\n                    if request_str is not None:\r\n                        # special case: loglikelihood_rolling produces a number of loglikelihood requests\r\n                        # all with cache key None. instead do add_partial on the per-example level\r\n                        # in the loglikelihood_rolling() function for those.\r\n                        self.cache_hook.add_partial(\r\n                            \"loglikelihood\", request_str, answer\r\n                        )\r\n                    pbar.update(1)\r\n\r\n        pbar.close()\r\n\r\n        return re_ord.get_original(res)\r\n\r\n    def generate_until(\r\n        self, requests: List[Instance], disable_tqdm: bool = False\r\n    ) -> List[str]:\r\n        res = []\r\n\r\n        def _collate(req: Tuple[str, dict]):\r\n            \"\"\"Defines the key for the sorted method\"\"\"\r\n            # the negative sign on len(toks) sorts descending - this has a few advantages:\r\n            # - time estimates will always be over not underestimates, which is more useful for planning\r\n            # - to know the size of a batch when going through the list, you know the first one is always the batch\r\n            #   padded context length. this is useful to simplify the batching logic and more importantly to make\r\n            #   automatic adaptive batches much much easier to implement\r\n            # - any OOMs will happen right away rather than near the end\r\n            toks = self.tok_encode(req[0])\r\n            return -len(toks), req[0]\r\n\r\n        pbar = tqdm(\r\n            total=len(requests),\r\n            disable=(disable_tqdm or (self.rank != 0)),\r\n            desc=\"Running generate_until requests\",\r\n        )\r\n        adaptive_batch_size = None\r\n        if self.batch_size == \"auto\":\r\n            # using rolling window with maximum context\r\n            print(\"Passed argument batch_size = auto. Detecting largest batch size\")\r\n            batch_size = self._detect_batch_size()\r\n            print(f\"Determined Largest batch size: {batch_size}\")\r\n            adaptive_batch_size = batch_size\r\n        # for each different set of kwargs, we execute all requests, by batch.\r\n        batch_size = (\r\n            self.batch_size\r\n            if self.batch_size != \"auto\"\r\n            else adaptive_batch_size\r\n            if adaptive_batch_size is not None\r\n            else 0\r\n        )\r\n        batch_fn = (\r\n            self._batch_scheduler\r\n            if self.batch_size == \"auto\" and not adaptive_batch_size\r\n            else None\r\n        )\r\n\r\n        # we group requests by their generation_kwargs,\r\n        # so that we don't try to execute e.g. greedy sampling and temp=0.8 sampling\r\n        # in the same batch.\r\n        # group_fn=lambda x: x[1] -> x=(context, gen_kwargs)\r\n        re_ords = Collator(\r\n            [reg.args for reg in requests],\r\n            sort_fn=_collate,\r\n            group_by=\"gen_kwargs\",\r\n            group_fn=lambda x: x[1],\r\n        )\r\n        chunks = re_ords.get_batched(n=batch_size, batch_fn=batch_fn)\r\n        for chunk in chunks:\r\n            contexts, all_gen_kwargs = zip(*chunk)\r\n            # we assume all gen kwargs in the batch are the same\r\n            # this is safe to assume because the `grouper` object ensures it.\r\n            gen_kwargs = all_gen_kwargs[0]\r\n            # unpack our keyword arguments.\r\n            until = None\r\n            if isinstance(gen_kwargs, dict):\r\n                kwargs = copy.deepcopy(gen_kwargs)  # edge case for repeats > 1\r\n                if \"until\" in kwargs.keys():\r\n                    until = kwargs.pop(\"until\")\r\n                    if isinstance(until, str):\r\n                        until = [until]\r\n                    elif not isinstance(until, list):\r\n                        raise ValueError(\r\n                            f\"Expected `kwargs['until']` to be of type Union[str,list] but got {until}\"\r\n                        )\r\n            else:\r\n                raise ValueError(\r\n                    f\"Expected `kwargs` to be of type `dict` but got {type(gen_kwargs)}\"\r\n                )\r\n            # add EOS token to stop sequences\r\n            eos = self.tok_decode(self.eot_token_id, skip_special_tokens=False)\r\n            if not until:\r\n                until = [eos]\r\n            else:\r\n                until.append(eos)\r\n            if \"max_gen_toks\" in kwargs.keys():\r\n                max_gen_toks = kwargs.pop(\"max_gen_toks\")\r\n            else:\r\n                max_gen_toks = self.max_gen_toks\r\n\r\n            # set the max length in tokens of inputs (\"context_enc\")\r\n            if self.AUTO_MODEL_CLASS == transformers.AutoModelForCausalLM:\r\n                # max len for inputs = max length, minus room to generate the max new tokens\r\n                max_ctx_len = self.max_length - max_gen_toks\r\n            elif self.AUTO_MODEL_CLASS == transformers.AutoModelForSeq2SeqLM:\r\n                # max len for inputs = encoder's whole max_length\r\n                max_ctx_len = self.max_length\r\n\r\n            # encode, pad, and truncate contexts for this batch\r\n            context_enc, attn_masks = self.tok_batch_encode(\r\n                contexts,\r\n                left_truncate_len=max_ctx_len,\r\n                truncation=self.truncation,\r\n            )\r\n            context_enc = context_enc.to(self.device)\r\n            attn_masks = attn_masks.to(self.device)\r\n\r\n            if \"max_length\" not in kwargs:\r\n                kwargs[\"max_length\"] = context_enc.shape[1] + max_gen_toks\r\n\r\n            # perform batched generation\r\n            cont = self._model_generate(\r\n                context=context_enc,\r\n                attention_mask=attn_masks,\r\n                stop=until,\r\n                **kwargs,\r\n            )\r\n\r\n            cont_toks_list = cont.tolist()\r\n            for cont_toks, context in zip(cont_toks_list, contexts):\r\n                # discard context + left-padding toks if using causal decoder-only LM\r\n                if self.AUTO_MODEL_CLASS == transformers.AutoModelForCausalLM:\r\n                    cont_toks = cont_toks[context_enc.shape[1] :]\r\n\r\n                s = self.tok_decode(cont_toks)\r\n\r\n                # use secondary stop seqs to cut off should-have-been-stopped content post-hoc\r\n                for term in until:\r\n                    if len(term) > 0:\r\n                        # ignore '' separator,\r\n                        # for seq2seq case where self.tok_decode(self.eot_token_id) = ''\r\n                        s = s.split(term)[0]\r\n\r\n                res.append(s)\r\n\r\n                self.cache_hook.add_partial(\"generate_until\", (context, gen_kwargs), s)\r\n                pbar.update(1)\r\n        # reorder this group of results back to original unsorted form\r\n        res = re_ords.get_original(res)\r\n\r\n        pbar.close()\r\n\r\n        return res\r\n\r\n    def apply_chat_template(self, chat_history: List[Dict[str, str]]) -> str:\r\n        \"\"\"\r\n        Method to apply a chat template to a list of chat history between user and model.\r\n        \"\"\"\r\n        return self.tokenizer.apply_chat_template(\r\n            chat_history, tokenize=False, add_generation_prompt=True\r\n        )\r\n\r\n    def get_model_info(self) -> dict:\r\n        \"\"\"\r\n        Method to get Hugging Face model information for experiment reproducibility.\r\n        \"\"\"\r\n\r\n        def get_model_num_params(model) -> int:\r\n            if hasattr(model, \"num_parameters\"):\r\n                return model.num_parameters()\r\n            if hasattr(model, \"parameters\"):\r\n                return sum(p.numel() for p in model.parameters())\r\n            else:\r\n                return -1\r\n\r\n        def get_model_dtype(model) -> str:\r\n            if hasattr(model, \"dtype\"):\r\n                return model.dtype\r\n            else:\r\n                return \"\"\r\n\r\n        def get_model_sha(pretrained: str, revision: str) -> str:\r\n            try:\r\n                model_info = HfApi().model_info(repo_id=pretrained, revision=revision)\r\n                return model_info.sha\r\n            except Exception as e:\r\n                eval_logger.warn(\r\n                    f\"Failed to get model SHA for {pretrained} at revision {revision}. Error: {e}\"\r\n                )\r\n                return \"\"\r\n\r\n        model_info = {\r\n            \"model_num_parameters\": get_model_num_params(self._model),\r\n            \"model_dtype\": get_model_dtype(self._model),\r\n            \"model_revision\": self.revision,\r\n            \"model_sha\": get_model_sha(self.pretrained, self.revision),\r\n        }\r\n        if self.peft:\r\n            model_info[\"peft_sha\"] = get_model_sha(self.peft, self.revision)\r\n        if self.delta:\r\n            model_info[\"delta_sha\"] = get_model_sha(self.delta, self.revision)\r\n        return model_info\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/loggers/evaluation_tracker.py", "content": "import json\r\nimport os\r\nimport re\r\nimport time\r\nfrom collections import defaultdict\r\nfrom dataclasses import asdict, dataclass\r\nfrom datetime import datetime\r\nfrom pathlib import Path\r\n\r\nfrom datasets import load_dataset\r\nfrom datasets.utils.metadata import MetadataConfigs\r\nfrom huggingface_hub import (\r\n    DatasetCard,\r\n    DatasetCardData,\r\n    HfApi,\r\n    hf_hub_url,\r\n)\r\nfrom huggingface_hub.utils import build_hf_headers, get_session, hf_raise_for_status\r\n\r\nfrom lm_eval.utils import (\r\n    eval_logger,\r\n    get_file_datetime,\r\n    get_file_task_name,\r\n    get_results_filenames,\r\n    get_sample_results_filenames,\r\n    handle_non_serializable,\r\n    hash_string,\r\n    sanitize_list,\r\n    sanitize_model_name,\r\n    sanitize_task_name,\r\n)\r\n\r\n\r\n@dataclass(init=False)\r\nclass GeneralConfigTracker:\r\n    \"\"\"\r\n    Tracker for the evaluation parameters.\r\n\r\n    Attributes:\r\n        model_source (str): Source of the model (e.g. Hugging Face, GGUF, etc.)\r\n        model_name (str): Name of the model.\r\n        model_name_sanitized (str): Sanitized model name for directory creation.\r\n        start_time (float): Start time of the experiment. Logged at class init.\r\n        end_time (float): Start time of the experiment. Logged when calling [`GeneralConfigTracker.log_end_time`]\r\n        total_evaluation_time_seconds (str): Inferred total evaluation time in seconds (from the start and end times).\r\n    \"\"\"\r\n\r\n    model_source: str = None\r\n    model_name: str = None\r\n    model_name_sanitized: str = None\r\n    system_instruction: str = None\r\n    system_instruction_sha: str = None\r\n    fewshot_as_multiturn: bool = None\r\n    chat_template: str = None\r\n    chat_template_sha: str = None\r\n    start_time: float = None\r\n    end_time: float = None\r\n    total_evaluation_time_seconds: str = None\r\n\r\n    def __init__(self) -> None:\r\n        \"\"\"Starts the evaluation timer.\"\"\"\r\n        self.start_time = time.perf_counter()\r\n\r\n    @staticmethod\r\n    def _get_model_name(model_args: str) -> str:\r\n        \"\"\"Extracts the model name from the model arguments.\"\"\"\r\n\r\n        def extract_model_name(model_args: str, key: str) -> str:\r\n            \"\"\"Extracts the model name from the model arguments using a key.\"\"\"\r\n            args_after_key = model_args.split(key)[1]\r\n            return args_after_key.split(\",\")[0]\r\n\r\n        # order does matter, e.g. peft and delta are provided together with pretrained\r\n        prefixes = [\"peft=\", \"delta=\", \"pretrained=\", \"model=\", \"path=\", \"engine=\"]\r\n        for prefix in prefixes:\r\n            if prefix in model_args:\r\n                return extract_model_name(model_args, prefix)\r\n        return \"\"\r\n\r\n    def log_experiment_args(\r\n        self,\r\n        model_source: str,\r\n        model_args: str,\r\n        system_instruction: str,\r\n        chat_template: str,\r\n        fewshot_as_multiturn: bool,\r\n    ) -> None:\r\n        \"\"\"Logs model parameters and job ID.\"\"\"\r\n        self.model_source = model_source\r\n        self.model_name = GeneralConfigTracker._get_model_name(model_args)\r\n        self.model_name_sanitized = sanitize_model_name(self.model_name)\r\n        self.system_instruction = system_instruction\r\n        self.system_instruction_sha = (\r\n            hash_string(system_instruction) if system_instruction else None\r\n        )\r\n        self.chat_template = chat_template\r\n        self.chat_template_sha = hash_string(chat_template) if chat_template else None\r\n        self.fewshot_as_multiturn = fewshot_as_multiturn\r\n\r\n    def log_end_time(self) -> None:\r\n        \"\"\"Logs the end time of the evaluation and calculates the total evaluation time.\"\"\"\r\n        self.end_time = time.perf_counter()\r\n        self.total_evaluation_time_seconds = str(self.end_time - self.start_time)\r\n\r\n\r\nclass EvaluationTracker:\r\n    \"\"\"\r\n    Keeps track and saves relevant information of the evaluation process.\r\n    Compiles the data from trackers and writes it to files, which can be published to the Hugging Face hub if requested.\r\n    \"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        output_path: str = None,\r\n        hub_results_org: str = \"\",\r\n        hub_repo_name: str = \"\",\r\n        details_repo_name: str = \"\",\r\n        results_repo_name: str = \"\",\r\n        push_results_to_hub: bool = False,\r\n        push_samples_to_hub: bool = False,\r\n        public_repo: bool = False,\r\n        token: str = \"\",\r\n        leaderboard_url: str = \"\",\r\n        point_of_contact: str = \"\",\r\n        gated: bool = False,\r\n    ) -> None:\r\n        \"\"\"\r\n        Creates all the necessary loggers for evaluation tracking.\r\n\r\n        Args:\r\n            output_path (str): Path to save the results. If not provided, the results won't be saved.\r\n            hub_results_org (str): The Hugging Face organization to push the results to. If not provided, the results will be pushed to the owner of the Hugging Face token.\r\n            hub_repo_name (str): The name of the Hugging Face repository to push the results to. If not provided, the results will be pushed to `lm-eval-results`.\r\n            details_repo_name (str): The name of the Hugging Face repository to push the details to. If not provided, the results will be pushed to `lm-eval-results`.\r\n            result_repo_name (str): The name of the Hugging Face repository to push the results to. If not provided, the results will not be pushed and will be found in the details_hub_repo.\r\n            push_results_to_hub (bool): Whether to push the results to the Hugging Face hub.\r\n            push_samples_to_hub (bool): Whether to push the samples to the Hugging Face hub.\r\n            public_repo (bool): Whether to push the results to a public or private repository.\r\n            token (str): Token to use when pushing to the Hugging Face hub. This token should have write access to `hub_results_org`.\r\n            leaderboard_url (str): URL to the leaderboard on the Hugging Face hub on the dataset card.\r\n            point_of_contact (str): Contact information on the Hugging Face hub dataset card.\r\n            gated (bool): Whether to gate the repository.\r\n        \"\"\"\r\n        self.general_config_tracker = GeneralConfigTracker()\r\n\r\n        self.output_path = output_path\r\n        self.push_results_to_hub = push_results_to_hub\r\n        self.push_samples_to_hub = push_samples_to_hub\r\n        self.public_repo = public_repo\r\n        self.leaderboard_url = leaderboard_url\r\n        self.point_of_contact = point_of_contact\r\n        self.api = HfApi(token=token) if token else None\r\n        self.gated_repo = gated\r\n\r\n        if not self.api and (push_results_to_hub or push_samples_to_hub):\r\n            raise ValueError(\r\n                \"Hugging Face token is not defined, but 'push_results_to_hub' or 'push_samples_to_hub' is set to True. \"\r\n                \"Please provide a valid Hugging Face token by setting the HF_TOKEN environment variable.\"\r\n            )\r\n\r\n        if (\r\n            self.api\r\n            and hub_results_org == \"\"\r\n            and (push_results_to_hub or push_samples_to_hub)\r\n        ):\r\n            hub_results_org = self.api.whoami()[\"name\"]\r\n            eval_logger.warning(\r\n                f\"hub_results_org was not specified. Results will be pushed to '{hub_results_org}'.\"\r\n            )\r\n\r\n        if hub_repo_name == \"\":\r\n            details_repo_name = (\r\n                details_repo_name if details_repo_name != \"\" else \"lm-eval-results\"\r\n            )\r\n            results_repo_name = (\r\n                results_repo_name if results_repo_name != \"\" else details_repo_name\r\n            )\r\n        else:\r\n            details_repo_name = hub_repo_name\r\n            results_repo_name = hub_repo_name\r\n            eval_logger.warning(\r\n                \"hub_repo_name was specified. Both details and results will be pushed to the same repository. Using hub_repo_name is no longer recommended, details_repo_name and results_repo_name should be used instead.\"\r\n            )\r\n\r\n        self.details_repo = f\"{hub_results_org}/{details_repo_name}\"\r\n        self.details_repo_private = f\"{hub_results_org}/{details_repo_name}-private\"\r\n        self.results_repo = f\"{hub_results_org}/{results_repo_name}\"\r\n        self.results_repo_private = f\"{hub_results_org}/{results_repo_name}-private\"\r\n\r\n    def save_results_aggregated(\r\n        self,\r\n        results: dict,\r\n        samples: dict,\r\n        eval_uuid: str,\r\n    ) -> None:\r\n        \"\"\"\r\n        Saves the aggregated results and samples to the output path and pushes them to the Hugging Face hub if requested.\r\n\r\n        Args:\r\n            results (dict): The aggregated results to save.\r\n            samples (dict): The samples results to save.\r\n        \"\"\"\r\n        self.general_config_tracker.log_end_time()\r\n\r\n        if self.output_path:\r\n            try:\r\n                eval_logger.info(\"Saving results aggregated\")\r\n\r\n                # calculate cumulative hash for each task - only if samples are provided\r\n                task_hashes = {}\r\n                if samples:\r\n                    for task_name, task_samples in samples.items():\r\n                        sample_hashes = [\r\n                            s[\"doc_hash\"] + s[\"prompt_hash\"] + s[\"target_hash\"]\r\n                            for s in task_samples\r\n                        ]\r\n                        task_hashes[task_name] = hash_string(\"\".join(sample_hashes))\r\n\r\n                # update initial results dict\r\n                results.update({\"task_hashes\": task_hashes})\r\n                results.update(asdict(self.general_config_tracker))\r\n                dumped = json.dumps(\r\n                    results,\r\n                    indent=2,\r\n                    default=handle_non_serializable,\r\n                    ensure_ascii=False,\r\n                )\r\n\r\n                path = Path(self.output_path if self.output_path else Path.cwd())\r\n                path = path.joinpath(self.general_config_tracker.model_name_sanitized)\r\n                path.mkdir(parents=True, exist_ok=True)\r\n\r\n                if eval_uuid==\"datetime\":\r\n                    self.date_id = datetime.now().isoformat().replace(\":\", \"-\")\r\n                else:\r\n                    self.date_id=eval_uuid\r\n                file_results_aggregated = path.joinpath(f\"results_{self.date_id}.json\")\r\n                file_results_aggregated.open(\"w\", encoding=\"utf-8\").write(dumped)\r\n\r\n                if self.api and self.push_results_to_hub:\r\n                    repo_id = (\r\n                        self.results_repo\r\n                        if self.public_repo\r\n                        else self.results_repo_private\r\n                    )\r\n                    self.api.create_repo(\r\n                        repo_id=repo_id,\r\n                        repo_type=\"dataset\",\r\n                        private=not self.public_repo,\r\n                        exist_ok=True,\r\n                    )\r\n                    self.api.upload_file(\r\n                        repo_id=repo_id,\r\n                        path_or_fileobj=str(\r\n                            path.joinpath(f\"results_{self.date_id}.json\")\r\n                        ),\r\n                        path_in_repo=os.path.join(\r\n                            self.general_config_tracker.model_name,\r\n                            f\"results_{self.date_id}.json\",\r\n                        ),\r\n                        repo_type=\"dataset\",\r\n                        commit_message=f\"Adding aggregated results for {self.general_config_tracker.model_name}\",\r\n                    )\r\n                    eval_logger.info(\r\n                        \"Successfully pushed aggregated results to the Hugging Face Hub. \"\r\n                        f\"You can find them at: {repo_id}\"\r\n                    )\r\n\r\n            except Exception as e:\r\n                eval_logger.warning(\"Could not save results aggregated\")\r\n                eval_logger.info(repr(e))\r\n        else:\r\n            eval_logger.info(\r\n                \"Output path not provided, skipping saving results aggregated\"\r\n            )\r\n\r\n    def save_results_samples(\r\n        self,\r\n        task_name: str,\r\n        samples: dict,\r\n    ) -> None:\r\n        \"\"\"\r\n        Saves the samples results to the output path and pushes them to the Hugging Face hub if requested.\r\n\r\n        Args:\r\n            task_name (str): The task name to save the samples for.\r\n            samples (dict): The samples results to save.\r\n        \"\"\"\r\n        if self.output_path:\r\n            try:\r\n                eval_logger.info(f\"Saving per-sample results for: {task_name}\")\r\n\r\n                path = Path(self.output_path if self.output_path else Path.cwd())\r\n                path = path.joinpath(self.general_config_tracker.model_name_sanitized)\r\n                path.mkdir(parents=True, exist_ok=True)\r\n\r\n                file_results_samples = path.joinpath(\r\n                    f\"samples_{task_name}_{self.date_id}.jsonl\"\r\n                )\r\n\r\n                for sample in samples:\r\n                    # we first need to sanitize arguments and resps\r\n                    # otherwise we won't be able to load the dataset\r\n                    # using the datasets library\r\n                    arguments = {}\r\n                    for i, arg in enumerate(sample[\"arguments\"]):\r\n                        arguments[f\"gen_args_{i}\"] = {}\r\n                        for j, tmp in enumerate(arg):\r\n                            arguments[f\"gen_args_{i}\"][f\"arg_{j}\"] = tmp\r\n\r\n                    sample[\"resps\"] = sanitize_list(sample[\"resps\"])\r\n                    sample[\"filtered_resps\"] = sanitize_list(sample[\"filtered_resps\"])\r\n                    sample[\"arguments\"] = arguments\r\n                    sample[\"target\"] = str(sample[\"target\"])\r\n\r\n                    sample_dump = (\r\n                        json.dumps(\r\n                            sample,\r\n                            default=handle_non_serializable,\r\n                            ensure_ascii=False,\r\n                        )\r\n                        + \"\\n\"\r\n                    )\r\n\r\n                    with open(file_results_samples, \"a\", encoding=\"utf-8\") as f:\r\n                        f.write(sample_dump)\r\n\r\n                if self.api and self.push_samples_to_hub:\r\n                    repo_id = (\r\n                        self.details_repo\r\n                        if self.public_repo\r\n                        else self.details_repo_private\r\n                    )\r\n                    self.api.create_repo(\r\n                        repo_id=repo_id,\r\n                        repo_type=\"dataset\",\r\n                        private=not self.public_repo,\r\n                        exist_ok=True,\r\n                    )\r\n                    try:\r\n                        if self.gated_repo:\r\n                            headers = build_hf_headers()\r\n                            r = get_session().put(\r\n                                url=f\"https://huggingface.co/api/datasets/{repo_id}/settings\",\r\n                                headers=headers,\r\n                                json={\"gated\": \"auto\"},\r\n                            )\r\n                            hf_raise_for_status(r)\r\n                    except Exception as e:\r\n                        eval_logger.warning(\"Could not gate the repository\")\r\n                        eval_logger.info(repr(e))\r\n                    self.api.upload_folder(\r\n                        repo_id=repo_id,\r\n                        folder_path=str(path),\r\n                        path_in_repo=self.general_config_tracker.model_name_sanitized,\r\n                        repo_type=\"dataset\",\r\n                        commit_message=f\"Adding samples results for {task_name} to {self.general_config_tracker.model_name}\",\r\n                    )\r\n                    eval_logger.info(\r\n                        f\"Successfully pushed sample results for task: {task_name} to the Hugging Face Hub. \"\r\n                        f\"You can find them at: {repo_id}\"\r\n                    )\r\n\r\n            except Exception as e:\r\n                eval_logger.warning(\"Could not save sample results\")\r\n                eval_logger.info(repr(e))\r\n        else:\r\n            eval_logger.info(\"Output path not provided, skipping saving sample results\")\r\n\r\n    def recreate_metadata_card(self) -> None:\r\n        \"\"\"\r\n        Creates a metadata card for the evaluation results dataset and pushes it to the Hugging Face hub.\r\n        \"\"\"\r\n\r\n        eval_logger.info(\"Recreating metadata card\")\r\n        repo_id = self.details_repo if self.public_repo else self.details_repo_private\r\n\r\n        files_in_repo = self.api.list_repo_files(repo_id=repo_id, repo_type=\"dataset\")\r\n        results_files = get_results_filenames(files_in_repo)\r\n        sample_files = get_sample_results_filenames(files_in_repo)\r\n\r\n        # Build a dictionary to store the latest evaluation datetime for:\r\n        # - Each tested model and its aggregated results\r\n        # - Each task and sample results, if existing\r\n        # i.e. {\r\n        #     \"org__model_name__gsm8k\": \"2021-09-01T12:00:00\",\r\n        #     \"org__model_name__ifeval\": \"2021-09-01T12:00:00\",\r\n        #     \"org__model_name__results\": \"2021-09-01T12:00:00\"\r\n        # }\r\n        latest_task_results_datetime = defaultdict(lambda: datetime.min.isoformat())\r\n\r\n        for file_path in sample_files:\r\n            file_path = Path(file_path)\r\n            filename = file_path.name\r\n            model_name = file_path.parent\r\n            task_name = get_file_task_name(filename)\r\n            results_datetime = get_file_datetime(filename)\r\n            task_name_sanitized = sanitize_task_name(task_name)\r\n            # Results and sample results for the same model and task will have the same datetime\r\n            samples_key = f\"{model_name}__{task_name_sanitized}\"\r\n            results_key = f\"{model_name}__results\"\r\n            latest_datetime = max(\r\n                latest_task_results_datetime[samples_key],\r\n                results_datetime,\r\n            )\r\n            latest_task_results_datetime[samples_key] = latest_datetime\r\n            latest_task_results_datetime[results_key] = max(\r\n                latest_task_results_datetime[results_key],\r\n                latest_datetime,\r\n            )\r\n\r\n        # Create metadata card\r\n        card_metadata = MetadataConfigs()\r\n\r\n        # Add the latest aggregated results to the metadata card for easy access\r\n        for file_path in results_files:\r\n            file_path = Path(file_path)\r\n            results_filename = file_path.name\r\n            model_name = file_path.parent\r\n            eval_date = get_file_datetime(results_filename)\r\n            eval_date_sanitized = re.sub(r\"[^\\w\\.]\", \"_\", eval_date)\r\n            results_filename = Path(\"**\") / Path(results_filename).name\r\n            config_name = f\"{model_name}__results\"\r\n            sanitized_last_eval_date_results = re.sub(\r\n                r\"[^\\w\\.]\", \"_\", latest_task_results_datetime[config_name]\r\n            )\r\n\r\n            if eval_date_sanitized == sanitized_last_eval_date_results:\r\n                # Ensure that all results files are listed in the metadata card\r\n                current_results = card_metadata.get(config_name, {\"data_files\": []})\r\n                current_results[\"data_files\"].append(\r\n                    {\"split\": eval_date_sanitized, \"path\": [str(results_filename)]}\r\n                )\r\n                card_metadata[config_name] = current_results\r\n                # If the results file is the newest, update the \"latest\" field in the metadata card\r\n                card_metadata[config_name][\"data_files\"].append(\r\n                    {\"split\": \"latest\", \"path\": [str(results_filename)]}\r\n                )\r\n\r\n        # Add the tasks details configs\r\n        for file_path in sample_files:\r\n            file_path = Path(file_path)\r\n            filename = file_path.name\r\n            model_name = file_path.parent\r\n            task_name = get_file_task_name(filename)\r\n            eval_date = get_file_datetime(filename)\r\n            task_name_sanitized = sanitize_task_name(task_name)\r\n            eval_date_sanitized = re.sub(r\"[^\\w\\.]\", \"_\", eval_date)\r\n            results_filename = Path(\"**\") / Path(filename).name\r\n            config_name = f\"{model_name}__{task_name_sanitized}\"\r\n            sanitized_last_eval_date_results = re.sub(\r\n                r\"[^\\w\\.]\", \"_\", latest_task_results_datetime[config_name]\r\n            )\r\n            if eval_date_sanitized == sanitized_last_eval_date_results:\r\n                # Ensure that all sample results files are listed in the metadata card\r\n                current_details_for_task = card_metadata.get(\r\n                    config_name, {\"data_files\": []}\r\n                )\r\n                current_details_for_task[\"data_files\"].append(\r\n                    {\"split\": eval_date_sanitized, \"path\": [str(results_filename)]}\r\n                )\r\n                card_metadata[config_name] = current_details_for_task\r\n                # If the samples results file is the newest, update the \"latest\" field in the metadata card\r\n                card_metadata[config_name][\"data_files\"].append(\r\n                    {\"split\": \"latest\", \"path\": [str(results_filename)]}\r\n                )\r\n\r\n        # Get latest results and extract info to update metadata card examples\r\n        latest_datetime = max(latest_task_results_datetime.values())\r\n        latest_model_name = max(\r\n            latest_task_results_datetime, key=lambda k: latest_task_results_datetime[k]\r\n        )\r\n        last_results_file = [\r\n            f for f in results_files if latest_datetime.replace(\":\", \"-\") in f\r\n        ][0]\r\n        last_results_file_path = hf_hub_url(\r\n            repo_id=repo_id, filename=last_results_file, repo_type=\"dataset\"\r\n        )\r\n        latest_results_file = load_dataset(\r\n            \"json\", data_files=last_results_file_path, split=\"train\"\r\n        )\r\n        results_dict = latest_results_file[\"results\"][0]\r\n        new_dictionary = {\"all\": results_dict}\r\n        new_dictionary.update(results_dict)\r\n        results_string = json.dumps(new_dictionary, indent=4)\r\n\r\n        dataset_summary = (\r\n            \"Dataset automatically created during the evaluation run of model \"\r\n        )\r\n        if self.general_config_tracker.model_source == \"hf\":\r\n            dataset_summary += f\"[{self.general_config_tracker.model_name}](https://huggingface.co/{self.general_config_tracker.model_name})\\n\"\r\n        else:\r\n            dataset_summary += f\"{self.general_config_tracker.model_name}\\n\"\r\n        dataset_summary += (\r\n            f\"The dataset is composed of {len(card_metadata)-1} configuration(s), each one corresponding to one of the evaluated task.\\n\\n\"\r\n            f\"The dataset has been created from {len(results_files)} run(s). Each run can be found as a specific split in each \"\r\n            'configuration, the split being named using the timestamp of the run.The \"train\" split is always pointing to the latest results.\\n\\n'\r\n            'An additional configuration \"results\" store all the aggregated results of the run.\\n\\n'\r\n            \"To load the details from a run, you can for instance do the following:\\n\"\r\n        )\r\n        if self.general_config_tracker.model_source == \"hf\":\r\n            dataset_summary += (\r\n                \"```python\\nfrom datasets import load_dataset\\n\"\r\n                f'data = load_dataset(\\n\\t\"{repo_id}\",\\n\\tname=\"{latest_model_name}\",\\n\\tsplit=\"latest\"\\n)\\n```\\n\\n'\r\n            )\r\n        dataset_summary += (\r\n            \"## Latest results\\n\\n\"\r\n            f'These are the [latest results from run {latest_datetime}]({last_results_file_path.replace(\"/resolve/\", \"/blob/\")}) '\r\n            \"(note that there might be results for other tasks in the repos if successive evals didn't cover the same tasks. \"\r\n            'You find each in the results and the \"latest\" split for each eval):\\n\\n'\r\n            f\"```python\\n{results_string}\\n```\"\r\n        )\r\n        card_data = DatasetCardData(\r\n            dataset_summary=dataset_summary,\r\n            repo_url=f\"https://huggingface.co/{self.general_config_tracker.model_name}\",\r\n            pretty_name=f\"Evaluation run of {self.general_config_tracker.model_name}\",\r\n            leaderboard_url=self.leaderboard_url,\r\n            point_of_contact=self.point_of_contact,\r\n        )\r\n        card_metadata.to_dataset_card_data(card_data)\r\n        card = DatasetCard.from_template(\r\n            card_data,\r\n            pretty_name=card_data.pretty_name,\r\n        )\r\n        card.push_to_hub(repo_id, repo_type=\"dataset\")\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/models/openai_completions.py", "content": "import os\r\nfrom functools import cached_property\r\nfrom typing import Any, Dict, List, Optional, Tuple, Union\r\n\r\nfrom lm_eval.api.registry import register_model\r\nfrom lm_eval.models.api_models import TemplateAPI\r\nfrom lm_eval.utils import eval_logger\r\n\r\n\r\n@register_model(\"local-completions\")\r\nclass LocalCompletionsAPI(TemplateAPI):\r\n    def __init__(\r\n        self,\r\n        base_url=None,\r\n        tokenizer_backend=\"huggingface\",\r\n        **kwargs,\r\n    ):\r\n        super().__init__(\r\n            base_url=base_url, tokenizer_backend=tokenizer_backend, **kwargs\r\n        )\r\n\r\n    def _create_payload(\r\n        self,\r\n        messages: Union[List[List[int]], List[dict], List[str], str],\r\n        generate=False,\r\n        gen_kwargs: Optional[dict] = None,\r\n        seed: int = 1234,\r\n        **kwargs,\r\n    ) -> dict:\r\n        if generate:\r\n            gen_kwargs.pop(\"do_sample\", False)\r\n            if \"max_tokens\" in gen_kwargs:\r\n                max_tokens = gen_kwargs.pop(\"max_tokens\")\r\n            else:\r\n                max_tokens = gen_kwargs.pop(\"max_gen_toks\", self._max_gen_toks)\r\n            temperature = gen_kwargs.pop(\"temperature\", 0)\r\n            stop = gen_kwargs.pop(\"until\", [\"<|endoftext|>\"])\r\n            return {\r\n                \"prompt\": messages,\r\n                \"model\": self.model,\r\n                \"max_tokens\": max_tokens,\r\n                \"temperature\": temperature,\r\n                \"stop\": stop,\r\n                \"seed\": seed,\r\n                **gen_kwargs,\r\n            }\r\n        else:\r\n            return {\r\n                \"model\": self.model,\r\n                \"prompt\": messages,\r\n                \"temperature\": 0,\r\n                \"max_tokens\": 1,\r\n                \"logprobs\": 1,\r\n                \"seed\": seed,\r\n                \"echo\": True,\r\n            }\r\n\r\n    @staticmethod\r\n    def parse_logprobs(\r\n        outputs: Union[Dict, List[Dict]],\r\n        tokens: List[List[int]] = None,\r\n        ctxlens: List[int] = None,\r\n        **kwargs,\r\n    ) -> List[Tuple[float, bool]]:\r\n        res = []\r\n        if not isinstance(outputs, list):\r\n            outputs = [outputs]\r\n        for out in outputs:\r\n            for choice, ctxlen in zip(out[\"choices\"], ctxlens):\r\n                assert ctxlen > 0, \"Context length must be greater than 0\"\r\n                logprobs = sum(choice[\"logprobs\"][\"token_logprobs\"][ctxlen:-1])\r\n                tokens_logprobs = choice[\"logprobs\"][\"token_logprobs\"][ctxlen:-1]\r\n                top_logprobs = choice[\"logprobs\"][\"top_logprobs\"][ctxlen:-1]\r\n                is_greedy = True\r\n                for tok, top in zip(tokens_logprobs, top_logprobs):\r\n                    if tok != max(top.values()):\r\n                        is_greedy = False\r\n                        break\r\n                res.append((logprobs, is_greedy))\r\n        return res\r\n\r\n    @staticmethod\r\n    def parse_generations(outputs: Union[Dict, List[Dict]], **kwargs) -> List[str]:\r\n        res = []\r\n        if not isinstance(outputs, list):\r\n            outputs = [outputs]\r\n        for out in outputs:\r\n            for choices in out[\"choices\"]:\r\n                res.append(choices[\"text\"])\r\n        return res\r\n\r\n    @property\r\n    def api_key(self):\r\n        return os.environ.get(\"OPENAI_API_KEY\", \"\")\r\n\r\n\r\n@register_model(\"local-chat-completions\")\r\nclass LocalChatCompletion(LocalCompletionsAPI):\r\n    def __init__(\r\n        self,\r\n        base_url=None,\r\n        tokenizer_backend=None,\r\n        tokenized_requests=False,\r\n        **kwargs,\r\n    ):\r\n        eval_logger.warning(\r\n            \"chat-completions endpoint requires the `--apply_chat_template` flag.\"\r\n        )\r\n        super().__init__(\r\n            base_url=base_url,\r\n            tokenizer_backend=tokenizer_backend,\r\n            tokenized_requests=tokenized_requests,\r\n            **kwargs,\r\n        )\r\n        if self._batch_size > 1:\r\n            eval_logger.warning(\r\n                \"Chat completions does not support batching. Defaulting to batch size 1.\"\r\n            )\r\n            self._batch_size = 1\r\n\r\n    def _create_payload(\r\n        self,\r\n        messages: List[Dict],\r\n        generate=False,\r\n        gen_kwargs: dict = None,\r\n        seed=1234,\r\n        **kwargs,\r\n    ) -> dict:\r\n        gen_kwargs.pop(\"do_sample\", False)\r\n        if \"max_tokens\" in gen_kwargs:\r\n            max_tokens = gen_kwargs.pop(\"max_tokens\")\r\n        else:\r\n            max_tokens = gen_kwargs.pop(\"max_gen_toks\", self._max_gen_toks)\r\n        temperature = gen_kwargs.pop(\"temperature\", 0)\r\n        stop = gen_kwargs.pop(\"until\", [\"<|endoftext|>\"])\r\n        if not isinstance(stop, (list, tuple)):\r\n            stop = [stop]\r\n        return {\r\n            \"messages\": messages,\r\n            \"model\": self.model,\r\n            \"max_tokens\": max_tokens,\r\n            \"temperature\": temperature,\r\n            \"stop\": stop[:4],\r\n            \"seed\": seed,\r\n            **gen_kwargs,\r\n        }\r\n\r\n    @staticmethod\r\n    def parse_generations(outputs: Union[Dict, List[Dict]], **kwargs) -> List[str]:\r\n        res = []\r\n        if not isinstance(outputs, list):\r\n            outputs = [outputs]\r\n        for out in outputs:\r\n            for choices in out[\"choices\"]:\r\n                res.append(choices[\"message\"][\"content\"])\r\n        return res\r\n\r\n    def tok_encode(\r\n        self,\r\n        string: Union[str, Any],\r\n        left_truncate_len=None,\r\n        add_special_tokens=None,\r\n        **kwargs,\r\n    ) -> Union[List[str], List[int], Any]:\r\n        return string\r\n\r\n    def loglikelihood(self, requests, **kwargs):\r\n        raise NotImplementedError(\r\n            \"Loglikelihood is not supported for chat completions. Consider using the completions API instead.\"\r\n        )\r\n\r\n\r\n@register_model(\r\n    \"openai-completions\",\r\n)\r\nclass OpenAICompletionsAPI(LocalCompletionsAPI):\r\n    def __init__(\r\n        self,\r\n        base_url=\"https://api.openai.com/v1/completions\",\r\n        tokenizer_backend=\"tiktoken\",\r\n        **kwargs,\r\n    ):\r\n        super().__init__(\r\n            base_url=base_url, tokenizer_backend=tokenizer_backend, **kwargs\r\n        )\r\n\r\n    @cached_property\r\n    def api_key(self):\r\n        \"\"\"Override this property to return the API key for the API request.\"\"\"\r\n        key = os.environ.get(\"OPENAI_API_KEY\", None)\r\n        if key is None:\r\n            raise ValueError(\r\n                \"API key not found. Please set the `OPENAI_API_KEY` environment variable.\"\r\n            )\r\n        return key\r\n\r\n    def loglikelihood(self, requests, **kwargs):\r\n        assert (\r\n            self.model\r\n            in [\r\n                \"babbage-002\",\r\n                \"davinci-002\",\r\n            ]\r\n        ), f\"Prompt loglikelihoods are only supported by OpenAI's API for {['babbage-002', 'davinci-002']}.\"\r\n        return super().loglikelihood(requests, **kwargs)\r\n\r\n    def chat_template(self, chat_template: Union[bool, str] = False) -> Optional[str]:\r\n        return \"\"\r\n\r\n\r\n@register_model(\"openai-chat-completions\")\r\nclass OpenAIChatCompletion(LocalChatCompletion):\r\n    def __init__(\r\n        self,\r\n        base_url=\"https://api.openai.com/v1/chat/completions\",\r\n        tokenizer_backend=None,\r\n        tokenized_requests=False,\r\n        **kwargs,\r\n    ):\r\n        super().__init__(\r\n            base_url=base_url,\r\n            tokenizer_backend=tokenizer_backend,\r\n            tokenized_requests=tokenized_requests,\r\n            **kwargs,\r\n        )\r\n\r\n    @cached_property\r\n    def api_key(self):\r\n        \"\"\"Override this property to return the API key for the API request.\"\"\"\r\n        key = os.environ.get(\"OPENAI_API_KEY\", None)\r\n        if key is None:\r\n            raise ValueError(\r\n                \"API key not found. Please set the `OPENAI_API_KEY` environment variable.\"\r\n            )\r\n        return key\r\n\r\n    def loglikelihood(self, requests, **kwargs):\r\n        raise NotImplementedError(\r\n            \"Loglikelihood (and therefore `multiple_choice`-type tasks) is not supported for chat completions as OpenAI does not provide prompt logprobs. See https://github.com/EleutherAI/lm-evaluation-harness/issues/942#issuecomment-1777836312 or https://github.com/EleutherAI/lm-evaluation-harness/issues/1196 for more background on this limitation.\"\r\n        )\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/api/filter.py", "content": "from abc import ABC, abstractmethod\r\nfrom dataclasses import dataclass\r\nfrom typing import Callable, Iterable, List, Union\r\n\r\nfrom lm_eval.api.instance import Instance\r\n\r\n\r\nclass Filter(ABC):\r\n    \"\"\"\r\n    Filter classes operate on a per-task level.\r\n    They take all model outputs (`instance.resps` for all `task.instances`)\r\n    across all instances of a task, and perform operations.\r\n    In a single run, one can configure any number of separate filters or lists of filters.\r\n\r\n    \"\"\"\r\n\r\n    def __init__(self, **kwargs) -> None:\r\n        \"\"\"\r\n        Can define custom behavior here, if an individual instantiation of a Filter class should have state.\r\n        \"\"\"\r\n\r\n    @abstractmethod\r\n    def apply(self, resps: Union[List, Iterable], docs: List[dict]) -> Iterable:\r\n        \"\"\"\r\n        Defines the operation to perform on a list of the `inst.resps` properties of `Instance` objects.\r\n        Should return the list of (filtered) response lists *in the same order as they were input*, e.g.\r\n        if pass in [<inst.resps for instance 0>, <inst.resps for instance 1>] should return\r\n        [<filtered resps for instance 0>, <filtered resps for instance 1>]\r\n        \"\"\"\r\n        return resps\r\n\r\n\r\n@dataclass\r\nclass FilterEnsemble:\r\n    \"\"\"\r\n    FilterEnsemble creates a pipeline applying multiple filters.\r\n    Its intended usage is to stack multiple post-processing steps in order.\r\n    `task.apply_filters` should use a list of FilterEnsemble classes that it stores, to apply each\r\n    pipeline separately.\r\n    \"\"\"\r\n\r\n    name: str\r\n    filters: List[Callable[[], Filter]]\r\n\r\n    def apply(self, instances: List[Instance]) -> None:\r\n        resps, docs = zip(*((inst.resps, inst.doc) for inst in instances))\r\n        resps, docs = list(resps), list(docs)\r\n\r\n        for f in self.filters:\r\n            # apply filters in sequence\r\n            resps = f().apply(resps, docs)\r\n\r\n        # add the end results after filtering to filtered_requests of their respective source instances.\r\n        # has key `self.name`: each FilterEnsemble applied in a given run should use a different name.\r\n        for inst, resp in zip(instances, resps):\r\n            inst.filtered_resps[self.name] = resp\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/__init__.py", "content": "from .evaluator import evaluate, simple_evaluate\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/api/group.py", "content": "import abc\r\nfrom dataclasses import asdict, dataclass\r\nfrom inspect import getsource\r\nfrom typing import Any, Callable, List, Optional, Union\r\n\r\n\r\n@dataclass\r\nclass AggMetricConfig(dict):\r\n    metric: Optional[str] = None\r\n    aggregation: Optional[str] = \"mean\"\r\n    weight_by_size: Optional[str] = False\r\n    # list of filter names which should be incorporated into the aggregated metric.\r\n    filter_list: Optional[Union[str, list]] = \"none\"\r\n\r\n    def __post_init__(self):\r\n        if self.aggregation != \"mean\" and not callable(self.aggregation):\r\n            raise ValueError(\r\n                f\"Currently, 'mean' is the only pre-defined aggregation across groups' subtasks. Got '{self.aggregation}'.\"\r\n            )\r\n\r\n        if isinstance(self.filter_list, str):\r\n            self.filter_list = [self.filter_list]\r\n\r\n\r\n@dataclass\r\nclass GroupConfig(dict):\r\n    group: Optional[str] = None\r\n    group_alias: Optional[str] = None\r\n    task: Optional[Union[str, list]] = None\r\n    aggregate_metric_list: Optional[\r\n        Union[List[AggMetricConfig], AggMetricConfig, dict]\r\n    ] = None\r\n    metadata: Optional[dict] = (\r\n        None  # by default, not used in the code. allows for users to pass arbitrary info to tasks\r\n    )\r\n\r\n    def __getitem__(self, item):\r\n        return getattr(self, item)\r\n\r\n    def __setitem__(self, item, value):\r\n        return setattr(self, item, value)\r\n\r\n    def __post_init__(self):\r\n        if self.aggregate_metric_list is not None:\r\n            if isinstance(self.aggregate_metric_list, dict):\r\n                self.aggregate_metric_list = [self.aggregate_metric_list]\r\n\r\n            self.aggregate_metric_list = [\r\n                AggMetricConfig(**item) if isinstance(item, dict) else item\r\n                for item in self.aggregate_metric_list\r\n            ]\r\n\r\n    def to_dict(self, keep_callable: bool = False) -> dict:\r\n        \"\"\"dumps the current config as a dictionary object, as a printable format.\r\n        null fields will not be printed.\r\n        Used for dumping results alongside full task configuration\r\n\r\n        :return: dict\r\n            A printable dictionary version of the TaskConfig object.\r\n\r\n        # TODO: should any default value in the TaskConfig not be printed?\r\n        \"\"\"\r\n        cfg_dict = asdict(self)\r\n        # remove values that are `None`\r\n        for k, v in list(cfg_dict.items()):\r\n            if callable(v):\r\n                cfg_dict[k] = self.serialize_function(v, keep_callable=keep_callable)\r\n        return cfg_dict\r\n\r\n    def serialize_function(\r\n        self, value: Union[Callable, str], keep_callable=False\r\n    ) -> Union[Callable, str]:\r\n        \"\"\"Serializes a given function or string.\r\n\r\n        If 'keep_callable' is True, the original callable is returned.\r\n        Otherwise, attempts to return the source code of the callable using 'getsource'.\r\n        \"\"\"\r\n        if keep_callable:\r\n            return value\r\n        else:\r\n            try:\r\n                return getsource(value)\r\n            except (TypeError, OSError):\r\n                return str(value)\r\n\r\n\r\nclass ConfigurableGroup(abc.ABC):\r\n    def __init__(\r\n        self,\r\n        config: Optional[dict] = None,\r\n    ) -> None:\r\n        self._config = GroupConfig(**config)\r\n\r\n    @property\r\n    def group(self):\r\n        return self._config.group\r\n\r\n    @property\r\n    def group_alias(self):\r\n        return self._config.group_alias\r\n\r\n    @property\r\n    def version(self):\r\n        return self._config.version\r\n\r\n    @property\r\n    def config(self):\r\n        return self._config.to_dict()\r\n\r\n    @property\r\n    def group_name(self) -> Any:\r\n        return self._config.group\r\n\r\n    def __repr__(self):\r\n        return (\r\n            f\"ConfigurableGroup(group={self.group},\" f\"group_alias={self.group_alias})\"\r\n        )\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/api/__init__.py", "content": ""}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/__main__.py", "content": "import argparse\r\nimport json\r\nimport logging\r\nimport os\r\nimport sys\r\nfrom functools import partial\r\nfrom typing import Union\r\n\r\nfrom lm_eval import evaluator, utils\r\nfrom lm_eval.evaluator import request_caching_arg_to_dict\r\nfrom lm_eval.loggers import EvaluationTracker, WandbLogger\r\nfrom lm_eval.tasks import TaskManager\r\nfrom lm_eval.utils import handle_non_serializable, make_table, simple_parse_args_string\r\n\r\n\r\ndef _int_or_none_list_arg_type(\r\n    min_len: int, max_len: int, defaults: str, value: str, split_char: str = \",\"\r\n):\r\n    def parse_value(item):\r\n        item = item.strip().lower()\r\n        if item == \"none\":\r\n            return None\r\n        try:\r\n            return int(item)\r\n        except ValueError:\r\n            raise argparse.ArgumentTypeError(f\"{item} is not an integer or None\")\r\n\r\n    items = [parse_value(v) for v in value.split(split_char)]\r\n    num_items = len(items)\r\n\r\n    if num_items == 1:\r\n        # Makes downstream handling the same for single and multiple values\r\n        items = items * max_len\r\n    elif num_items < min_len or num_items > max_len:\r\n        raise argparse.ArgumentTypeError(\r\n            f\"Argument requires {max_len} integers or None, separated by '{split_char}'\"\r\n        )\r\n    elif num_items != max_len:\r\n        logging.warning(\r\n            f\"Argument requires {max_len} integers or None, separated by '{split_char}'. \"\r\n            \"Missing values will be filled with defaults.\"\r\n        )\r\n        default_items = [parse_value(v) for v in defaults.split(split_char)]\r\n        items.extend(\r\n            default_items[num_items:]\r\n        )  # extend items list with missing defaults\r\n\r\n    return items\r\n\r\n\r\ndef check_argument_types(parser: argparse.ArgumentParser):\r\n    \"\"\"\r\n    Check to make sure all CLI args are typed, raises error if not\r\n    \"\"\"\r\n    for action in parser._actions:\r\n        if action.dest != \"help\" and not action.const:\r\n            if action.type is None:\r\n                raise ValueError(\r\n                    f\"Argument '{action.dest}' doesn't have a type specified.\"\r\n                )\r\n            else:\r\n                continue\r\n\r\n\r\ndef setup_parser() -> argparse.ArgumentParser:\r\n    parser = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter)\r\n    parser.add_argument(\r\n        \"--model\", \"-m\", type=str, default=\"hf\", help=\"Name of model e.g. `hf`\"\r\n    )\r\n    parser.add_argument(\r\n        \"--tasks\",\r\n        \"-t\",\r\n        default=None,\r\n        type=str,\r\n        metavar=\"task1,task2\",\r\n        help=\"Comma-separated list of task names or task groupings to evaluate on.\\nTo get full list of tasks, use one of the commands `lm-eval --tasks {{list_groups,list_subtasks,list_tags,list}}` to list out all available names for task groupings; only (sub)tasks; tags; or all of the above\",\r\n    )\r\n    parser.add_argument(\r\n        \"--model_args\",\r\n        \"-a\",\r\n        default=\"\",\r\n        type=str,\r\n        help=\"Comma separated string arguments for model, e.g. `pretrained=EleutherAI/pythia-160m,dtype=float32`\",\r\n    )\r\n    parser.add_argument(\r\n        \"--num_fewshot\",\r\n        \"-f\",\r\n        type=int,\r\n        default=None,\r\n        metavar=\"N\",\r\n        help=\"Number of examples in few-shot context\",\r\n    )\r\n    parser.add_argument(\r\n        \"--batch_size\",\r\n        \"-b\",\r\n        type=str,\r\n        default=1,\r\n        metavar=\"auto|auto:N|N\",\r\n        help=\"Acceptable values are 'auto', 'auto:N' or N, where N is an integer. Default 1.\",\r\n    )\r\n    parser.add_argument(\r\n        \"--max_batch_size\",\r\n        type=int,\r\n        default=None,\r\n        metavar=\"N\",\r\n        help=\"Maximal batch size to try with --batch_size auto.\",\r\n    )\r\n    parser.add_argument(\r\n        \"--device\",\r\n        type=str,\r\n        default=None,\r\n        help=\"Device to use (e.g. cuda, cuda:0, cpu).\",\r\n    )\r\n    parser.add_argument(\r\n        \"--output_path\",\r\n        \"-o\",\r\n        default=None,\r\n        type=str,\r\n        metavar=\"DIR|DIR/file.json\",\r\n        help=\"The path to the output file where the result metrics will be saved. If the path is a directory and log_samples is true, the results will be saved in the directory. Else the parent directory will be used.\",\r\n    )\r\n    parser.add_argument(\r\n        \"--limit\",\r\n        \"-L\",\r\n        type=float,\r\n        default=None,\r\n        metavar=\"N|0<N<1\",\r\n        help=\"Limit the number of examples per task. \"\r\n        \"If <1, limit is a percentage of the total number of examples.\",\r\n    )\r\n    parser.add_argument(\r\n        \"--use_cache\",\r\n        \"-c\",\r\n        type=str,\r\n        default=None,\r\n        metavar=\"DIR\",\r\n        help=\"A path to a sqlite db file for caching model responses. `None` if not caching.\",\r\n    )\r\n    parser.add_argument(\r\n        \"--cache_requests\",\r\n        type=str,\r\n        default=None,\r\n        choices=[\"true\", \"refresh\", \"delete\"],\r\n        help=\"Speed up evaluation by caching the building of dataset requests. `None` if not caching.\",\r\n    )\r\n    parser.add_argument(\r\n        \"--check_integrity\",\r\n        action=\"store_true\",\r\n        help=\"Whether to run the relevant part of the test suite for the tasks.\",\r\n    )\r\n    parser.add_argument(\r\n        \"--write_out\",\r\n        \"-w\",\r\n        action=\"store_true\",\r\n        default=False,\r\n        help=\"Prints the prompt for the first few documents.\",\r\n    )\r\n    parser.add_argument(\r\n        \"--log_samples\",\r\n        \"-s\",\r\n        action=\"store_true\",\r\n        default=False,\r\n        help=\"If True, write out all model outputs and documents for per-sample measurement and post-hoc analysis. Use with --output_path.\",\r\n    )\r\n    parser.add_argument(\r\n        \"--system_instruction\",\r\n        type=str,\r\n        default=None,\r\n        help=\"System instruction to be used in the prompt\",\r\n    )\r\n    parser.add_argument(\r\n        \"--apply_chat_template\",\r\n        type=str,\r\n        nargs=\"?\",\r\n        const=True,\r\n        default=False,\r\n        help=(\r\n            \"If True, apply chat template to the prompt. \"\r\n            \"Providing `--apply_chat_template` without an argument will apply the default chat template to the prompt. \"\r\n            \"To apply a specific template from the available list of templates, provide the template name as an argument. \"\r\n            \"E.g. `--apply_chat_template template_name`\"\r\n        ),\r\n    )\r\n    parser.add_argument(\r\n        \"--fewshot_as_multiturn\",\r\n        action=\"store_true\",\r\n        default=False,\r\n        help=\"If True, uses the fewshot as a multi-turn conversation\",\r\n    )\r\n    parser.add_argument(\r\n        \"--show_config\",\r\n        action=\"store_true\",\r\n        default=False,\r\n        help=\"If True, shows the the full config of all tasks at the end of the evaluation.\",\r\n    )\r\n    parser.add_argument(\r\n        \"--include_path\",\r\n        type=str,\r\n        default=None,\r\n        metavar=\"DIR\",\r\n        help=\"Additional path to include if there are external tasks to include.\",\r\n    )\r\n    parser.add_argument(\r\n        \"--gen_kwargs\",\r\n        type=str,\r\n        default=None,\r\n        help=(\r\n            \"String arguments for model generation on greedy_until tasks,\"\r\n            \" e.g. `temperature=0,top_k=0,top_p=0`.\"\r\n        ),\r\n    )\r\n    parser.add_argument(\r\n        \"--verbosity\",\r\n        \"-v\",\r\n        type=str.upper,\r\n        default=\"INFO\",\r\n        metavar=\"CRITICAL|ERROR|WARNING|INFO|DEBUG\",\r\n        help=\"Controls the reported logging error level. Set to DEBUG when testing + adding new task configurations for comprehensive log output.\",\r\n    )\r\n    parser.add_argument(\r\n        \"--wandb_args\",\r\n        type=str,\r\n        default=\"\",\r\n        help=\"Comma separated string arguments passed to wandb.init, e.g. `project=lm-eval,job_type=eval\",\r\n    )\r\n    parser.add_argument(\r\n        \"--hf_hub_log_args\",\r\n        type=str,\r\n        default=\"\",\r\n        help=\"Comma separated string arguments passed to Hugging Face Hub's log function, e.g. `hub_results_org=EleutherAI,hub_repo_name=lm-eval-results`\",\r\n    )\r\n    parser.add_argument(\r\n        \"--predict_only\",\r\n        \"-x\",\r\n        action=\"store_true\",\r\n        default=False,\r\n        help=\"Use with --log_samples. Only model outputs will be saved and metrics will not be evaluated.\",\r\n    )\r\n    default_seed_string = \"0,1234,1234,1234\"\r\n    parser.add_argument(\r\n        \"--seed\",\r\n        type=partial(_int_or_none_list_arg_type, 3, 4, default_seed_string),\r\n        default=default_seed_string,  # for backward compatibility\r\n        help=(\r\n            \"Set seed for python's random, numpy, torch, and fewshot sampling.\\n\"\r\n            \"Accepts a comma-separated list of 4 values for python's random, numpy, torch, and fewshot sampling seeds, \"\r\n            \"respectively, or a single integer to set the same seed for all four.\\n\"\r\n            f\"The values are either an integer or 'None' to not set the seed. Default is `{default_seed_string}` \"\r\n            \"(for backward compatibility).\\n\"\r\n            \"E.g. `--seed 0,None,8,52` sets `random.seed(0)`, `torch.manual_seed(8)`, and fewshot sampling seed to 52. \"\r\n            \"Here numpy's seed is not set since the second value is `None`.\\n\"\r\n            \"E.g, `--seed 42` sets all four seeds to 42.\"\r\n        ),\r\n    )\r\n    parser.add_argument(\r\n        \"--trust_remote_code\",\r\n        action=\"store_true\",\r\n        help=\"Sets trust_remote_code to True to execute code to create HF Datasets from the Hub\",\r\n    )\r\n    parser.add_argument(\r\n        \"--random_subsample\",\r\n        action=\"store_true\",\r\n        default=False,\r\n        help=\"If True, subsample '--limit' dataset examples randomly.\",\r\n    )\r\n    parser.add_argument(\r\n        \"--eval_uuid\",\r\n        type=str,\r\n        default=\"datetime\",\r\n        help=\"Unique ID for the evaluation. Defaults to datetime\",\r\n    )\r\n    return parser\r\n\r\n\r\ndef parse_eval_args(parser: argparse.ArgumentParser) -> argparse.Namespace:\r\n    check_argument_types(parser)\r\n    return parser.parse_args()\r\n\r\n\r\ndef cli_evaluate(args: Union[argparse.Namespace, None] = None) -> None:\r\n    if not args:\r\n        # we allow for args to be passed externally, else we parse them ourselves\r\n        parser = setup_parser()\r\n        args = parse_eval_args(parser)\r\n\r\n    if args.wandb_args:\r\n        wandb_logger = WandbLogger(**simple_parse_args_string(args.wandb_args))\r\n\r\n    eval_logger = utils.eval_logger\r\n    eval_logger.setLevel(getattr(logging, f\"{args.verbosity}\"))\r\n    eval_logger.info(f\"Verbosity set to {args.verbosity}\")\r\n    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\r\n\r\n    # update the evaluation tracker args with the output path and the HF token\r\n    if args.output_path:\r\n        args.hf_hub_log_args += f\",output_path={args.output_path}\"\r\n    if os.environ.get(\"HF_TOKEN\", None):\r\n        args.hf_hub_log_args += f\",token={os.environ.get('HF_TOKEN')}\"\r\n    evaluation_tracker_args = simple_parse_args_string(args.hf_hub_log_args)\r\n    evaluation_tracker = EvaluationTracker(**evaluation_tracker_args)\r\n\r\n    if args.predict_only:\r\n        args.log_samples = True\r\n    if (args.log_samples or args.predict_only) and not args.output_path:\r\n        raise ValueError(\r\n            \"Specify --output_path if providing --log_samples or --predict_only\"\r\n        )\r\n\r\n    if args.fewshot_as_multiturn and args.apply_chat_template is False:\r\n        raise ValueError(\r\n            \"When `fewshot_as_multiturn` is selected, `apply_chat_template` must be set (either to `True` or to the chosen template name).\"\r\n        )\r\n\r\n    if args.include_path is not None:\r\n        eval_logger.info(f\"Including path: {args.include_path}\")\r\n    task_manager = TaskManager(args.verbosity, include_path=args.include_path)\r\n\r\n    if \"push_samples_to_hub\" in evaluation_tracker_args and not args.log_samples:\r\n        eval_logger.warning(\r\n            \"Pushing samples to the Hub requires --log_samples to be set. Samples will not be pushed to the Hub.\"\r\n        )\r\n\r\n    if args.limit:\r\n        eval_logger.warning(\r\n            \" --limit SHOULD ONLY BE USED FOR TESTING.\"\r\n            \"REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\"\r\n        )\r\n\r\n    if args.tasks is None:\r\n        eval_logger.error(\"Need to specify task to evaluate.\")\r\n        sys.exit()\r\n    elif args.tasks == \"list\":\r\n        print(task_manager.list_all_tasks())\r\n        sys.exit()\r\n    elif args.tasks == \"list_groups\":\r\n        print(task_manager.list_all_tasks(list_subtasks=False, list_tags=False))\r\n        sys.exit()\r\n    elif args.tasks == \"list_tags\":\r\n        print(task_manager.list_all_tasks(list_groups=False, list_subtasks=False))\r\n        sys.exit()\r\n    elif args.tasks == \"list_subtasks\":\r\n        print(task_manager.list_all_tasks(list_groups=False, list_tags=False))\r\n        sys.exit()\r\n    else:\r\n        if os.path.isdir(args.tasks):\r\n            import glob\r\n\r\n            task_names = []\r\n            yaml_path = os.path.join(args.tasks, \"*.yaml\")\r\n            for yaml_file in glob.glob(yaml_path):\r\n                config = utils.load_yaml_config(yaml_file)\r\n                task_names.append(config)\r\n        else:\r\n            task_list = args.tasks.split(\",\")\r\n            task_names = task_manager.match_tasks(task_list)\r\n            for task in [task for task in task_list if task not in task_names]:\r\n                if os.path.isfile(task):\r\n                    config = utils.load_yaml_config(task)\r\n                    task_names.append(config)\r\n            task_missing = [\r\n                task for task in task_list if task not in task_names and \"*\" not in task\r\n            ]  # we don't want errors if a wildcard (\"*\") task name was used\r\n\r\n            if task_missing:\r\n                missing = \", \".join(task_missing)\r\n                eval_logger.error(\r\n                    f\"Tasks were not found: {missing}\\n\"\r\n                    f\"{utils.SPACING}Try `lm-eval --tasks list` for list of available tasks\",\r\n                )\r\n                raise ValueError(\r\n                    f\"Tasks not found: {missing}. Try `lm-eval --tasks {{list_groups,list_subtasks,list_tags,list}}` to list out all available names for task groupings; only (sub)tasks; tags; or all of the above, or pass '--verbosity DEBUG' to troubleshoot task registration issues.\"\r\n                )\r\n\r\n    # Respect user's value passed in via CLI, otherwise default to True and add to comma-separated model args\r\n    if args.trust_remote_code:\r\n        eval_logger.info(\r\n            \"Passed `--trust_remote_code`, setting environment variable `HF_DATASETS_TRUST_REMOTE_CODE=true`\"\r\n        )\r\n        # HACK: import datasets and override its HF_DATASETS_TRUST_REMOTE_CODE value internally,\r\n        # because it's already been determined based on the prior env var before launching our\r\n        # script--`datasets` gets imported by lm_eval internally before these lines can update the env.\r\n        import datasets\r\n\r\n        datasets.config.HF_DATASETS_TRUST_REMOTE_CODE = True\r\n\r\n        args.model_args = args.model_args + \",trust_remote_code=True\"\r\n\r\n    eval_logger.info(f\"Selected Tasks: {task_names}\")\r\n\r\n    request_caching_args = request_caching_arg_to_dict(\r\n        cache_requests=args.cache_requests\r\n    )\r\n\r\n    results = evaluator.simple_evaluate(\r\n        model=args.model,\r\n        model_args=args.model_args,\r\n        tasks=task_names,\r\n        num_fewshot=args.num_fewshot,\r\n        batch_size=args.batch_size,\r\n        max_batch_size=args.max_batch_size,\r\n        device=args.device,\r\n        use_cache=args.use_cache,\r\n        limit=args.limit,\r\n        check_integrity=args.check_integrity,\r\n        write_out=args.write_out,\r\n        log_samples=args.log_samples,\r\n        evaluation_tracker=evaluation_tracker,\r\n        system_instruction=args.system_instruction,\r\n        apply_chat_template=args.apply_chat_template,\r\n        fewshot_as_multiturn=args.fewshot_as_multiturn,\r\n        gen_kwargs=args.gen_kwargs,\r\n        task_manager=task_manager,\r\n        verbosity=args.verbosity,\r\n        predict_only=args.predict_only,\r\n        random_seed=args.seed[0],\r\n        numpy_random_seed=args.seed[1],\r\n        torch_random_seed=args.seed[2],\r\n        fewshot_random_seed=args.seed[3],\r\n        bootstrap_iters=0,\r\n        random_subsample=args.random_subsample,\r\n        **request_caching_args,\r\n    )\r\n\r\n    if results is not None:\r\n        if args.log_samples:\r\n            samples = results.pop(\"samples\")\r\n        dumped = json.dumps(\r\n            results, indent=2, default=handle_non_serializable, ensure_ascii=False\r\n        )\r\n        if args.show_config:\r\n            print(dumped)\r\n\r\n        batch_sizes = \",\".join(map(str, results[\"config\"][\"batch_sizes\"]))\r\n\r\n        # Add W&B logging\r\n        if args.wandb_args:\r\n            try:\r\n                wandb_logger.post_init(results)\r\n                wandb_logger.log_eval_result()\r\n                if args.log_samples:\r\n                    wandb_logger.log_eval_samples(samples)\r\n            except Exception as e:\r\n                eval_logger.info(f\"Logging to Weights and Biases failed due to {e}\")\r\n\r\n        evaluation_tracker.save_results_aggregated(\r\n            results=results, samples=samples if args.log_samples else None,\r\n            eval_uuid=args.eval_uuid,\r\n        )\r\n\r\n        if args.log_samples:\r\n            for task_name, config in results[\"configs\"].items():\r\n                evaluation_tracker.save_results_samples(\r\n                    task_name=task_name, samples=samples[task_name]\r\n                )\r\n\r\n        if (\r\n            evaluation_tracker.push_results_to_hub\r\n            or evaluation_tracker.push_samples_to_hub\r\n        ):\r\n            evaluation_tracker.recreate_metadata_card()\r\n\r\n        print(\r\n            f\"{args.model} ({args.model_args}), gen_kwargs: ({args.gen_kwargs}), limit: {args.limit}, num_fewshot: {args.num_fewshot}, \"\r\n            f\"batch_size: {args.batch_size}{f' ({batch_sizes})' if batch_sizes else ''}\"\r\n        )\r\n        print(make_table(results))\r\n        if \"groups\" in results:\r\n            print(make_table(results, \"groups\"))\r\n\r\n        if args.wandb_args:\r\n            # Tear down wandb run once all the logging is done.\r\n            wandb_logger.run.finish()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    cli_evaluate()\r\n"}
{"type": "source_file", "path": "models.py", "content": "import openai\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\nimport ast\r\nfrom utilities import FIRST_STEP_INSTANTIATION_PROMPT\r\n\r\nclass Navigator:\r\n    \"\"\"\r\n    A class for navigating and controlling the reasoning process of a language model.\r\n\r\n    This class is responsible for initializing the reasoning trajectory, guiding the model\r\n    through different reasoning steps, and dynamically adjusting the reasoning flow.\r\n    \"\"\"\r\n    def __init__(self, model_path):\r\n        \"\"\"\r\n        Initializes the Navigator with a pre-trained language model.\r\n\r\n        Args:\r\n            model_path (str): The path to the pre-trained language model.\r\n        \"\"\"\r\n        self.model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\r\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\r\n        self.reasoning_thoughts = []\r\n        self.reasoning_flow = []\r\n        self.instantiation = []\r\n        self.reasoning_rounds = 0\r\n        self.reasoning_instructions = []\r\n\r\n    def initializing_reasoning_trajectory(self, prompt, problem):\r\n        \"\"\"\r\n        Initializes the reasoning trajectory by generating a template from the language model.\r\n\r\n        Args:\r\n            prompt (str): The system prompt to guide the template generation.\r\n            problem (str): The problem description.\r\n\r\n        Returns:\r\n            str: The thoughts generated for building the template.\r\n        \"\"\"\r\n        messages = [\r\n            {\"role\": \"system\", \"content\": prompt},\r\n            {\"role\": \"user\", \"content\": problem}\r\n        ]\r\n        response = self.generate(messages)\r\n        template_str = response.split('</think>')[-1]\r\n        self.template = ast.literal_eval(template_str)\r\n        thoughts_for_template_building = response.split('</think>')[0] + '</think>'\r\n        self.reasoning_thoughts.append(thoughts_for_template_building)\r\n        self.reasoning_flow = self.template['reason_flow']\r\n        self.reasoning_rounds = len(self.reasoning_flow)\r\n\r\n        return thoughts_for_template_building\r\n\r\n    def initialize_reason_problem(self, problem, reason_step):\r\n        \"\"\"\r\n        Initializes or continues the reasoning process for a given problem and reasoning step.\r\n\r\n        Args:\r\n            problem (str): The problem description.\r\n            reason_step (str): The current reasoning step.\r\n\r\n        Returns:\r\n            str: The response generated by the language model.\r\n        \"\"\"\r\n        system_prompt = \"You are a math tutor guiding a student to solve a math problem based on the given step. \" \\\r\n                        \"Your task is to help your student to learn how to apply the steps to solve the problem. \" \\\r\n                        \"Based on the problem description and the current step, give a clear and high-level instruction \" \\\r\n                        \"for your student to help them apply the method in the current step to solve the problem.\" + '\\nProblem:' + problem\r\n        continue_prompt = \"Now based on the student's response and the previous steps, please continue to instruct students to implement this step. \"\r\n        if not self.reasoning_instructions: # Equivalent to `len(self.reasoning_instructions) == 0` but more Pythonic\r\n            messages = [\r\n                {\"role\": \"system\", \"content\": system_prompt},\r\n                {\"role\": \"system\", \"content\": 'Current step: Step 1:\\n' + reason_step}\r\n            ]\r\n            response = self.generate(messages)\r\n            return response\r\n        else:\r\n            messages = []\r\n            messages.append({\"role\": \"system\", \"content\": system_prompt})\r\n            for i in range(len(self.reasoning_instructions)):\r\n                messages.append({\"role\": \"system\", \"content\": 'Current step: Step ' + str(i + 1) + ':\\n' + self.reasoning_flow[i]})\r\n                messages.append({\"role\": \"assistant\", \"content\": 'Step ' + str(i + 1) + ':\\n' + self.reasoning_instructions[i]})\r\n                messages.append({\"role\": \"user\", \"content\": f'Student Response for Step {i + 1}:\\n' + self.instantiation[i]})\r\n            messages.append({\"role\": \"system\", \"content\": continue_prompt + '\\n'})\r\n            messages.append({\"role\": \"system\", \"content\": 'Current step: Step ' + str(len(self.reasoning_instructions) + 1) + ':\\n' + reason_step})\r\n            response = self.generate(messages)\r\n            return response\r\n\r\n    def dynamic_adjustment(self, prompt):\r\n        \"\"\"\r\n        Dynamically adjusts the reasoning flow based on the model's response to a given prompt.\r\n\r\n        Args:\r\n            prompt (str): The prompt for dynamic adjustment.\r\n        \"\"\"\r\n        response = self.generate(prompt)\r\n        thought = response.split('</think>')[-1].split('</think>')[0] # Consider reviewing this logic, might be redundant split\r\n        new_reasoning_flow = response.split('</think>')[-1] # Consider reviewing this logic, might be redundant split\r\n        self.update_reasoning_flow(new_reasoning_flow)\r\n\r\n    def update_reasoning_flow(self, new_reasoning_flow):\r\n        \"\"\"\r\n        Updates the reasoning flow by extracting it from a given text and parsing it as a Python list.\r\n\r\n        Args:\r\n            new_reasoning_flow (str): Text containing the new reasoning flow in Python list format.\r\n        \"\"\"\r\n        prompt = 'Please extract the reasoning flows from the following text and output in python list format, ' \\\r\n                 'each element in the python list should be a step in the reasoning process:\\n'\r\n        messages = [\r\n            {\"role\": \"system\", \"content\": prompt},\r\n            {\"role\": \"user\", \"content\": 'Input Reasoning Flow:\\n' + new_reasoning_flow +\r\n             '\\n Note that only output the python list of reasoning flow in your response'}\r\n        ]\r\n        response = self.generate(messages)\r\n        extracted_reasoning_flow = [] # Renamed variable for clarity\r\n        while not extracted_reasoning_flow or not isinstance(extracted_reasoning_flow, list): # More robust type checking\r\n            try:\r\n                extracted_reasoning_flow = ast.literal_eval(response)\r\n            except (SyntaxError, ValueError): # Catch specific exceptions for robustness\r\n                response = self.generate(messages)\r\n        self.reasoning_flow = extracted_reasoning_flow\r\n        self.reasoning_rounds = len(self.reasoning_flow)\r\n        self.template['reason_flow'] = self.reasoning_flow\r\n\r\n    def generate(self, messages):\r\n        \"\"\"\r\n        Generates a response from the language model based on the provided messages.\r\n\r\n        Args:\r\n            messages (list): A list of message dictionaries for the language model.\r\n\r\n        Returns:\r\n            str: The generated response from the language model.\r\n        \"\"\"\r\n        text = self.tokenizer.apply_chat_template(\r\n            messages,\r\n            tokenize=False,\r\n            add_generation_prompt=True\r\n        )\r\n        model_inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.model.device)\r\n\r\n        generated_ids = self.model.generate(\r\n            **model_inputs,\r\n            max_new_tokens=3072,\r\n            temperature=0.6,\r\n            top_p=0.95,\r\n        )\r\n\r\n        generated_ids = [\r\n            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\r\n        ]\r\n\r\n        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\r\n\r\n        return response\r\n\r\n\r\nclass InferenceLLM:\r\n    \"\"\"\r\n    A class for inference using a language model, designed for student-tutor interplay.\r\n\r\n    This class handles the interaction with the language model to simulate a student\r\n    responding to tutor instructions in a step-by-step problem-solving scenario.\r\n    \"\"\"\r\n    def __init__(self, model_path, inherit: bool = False, inherit_model=None, inherit_tokenizer=None):\r\n        \"\"\"\r\n        Initializes the InferenceLLM, optionally inheriting a model and tokenizer.\r\n\r\n        Args:\r\n            model_path (str): Path to the pre-trained model if not inheriting.\r\n            inherit (bool): Whether to inherit an existing model and tokenizer. Defaults to False.\r\n            inherit_model (optional): Model to inherit if inherit is True.\r\n            inherit_tokenizer (optional): Tokenizer to inherit if inherit is True.\r\n        \"\"\"\r\n        if inherit:\r\n            self.model = inherit_model\r\n            self.tokenizer = inherit_tokenizer\r\n        else:\r\n            self.model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\r\n            self.tokenizer = AutoTokenizer.from_pretrained(model_path)\r\n\r\n    def interplay(self, instruction, problem, previous_instruction, previous_reasoning):\r\n        \"\"\"\r\n        Simulates the interplay between a student and a tutor for problem-solving.\r\n\r\n        Args:\r\n            instruction (str): The current instruction from the tutor.\r\n            problem (str): The problem description.\r\n            previous_instruction (list): List of previous tutor instructions.\r\n            previous_reasoning (list): List of student's reasoning for previous steps.\r\n\r\n        Returns:\r\n            tuple: A tuple containing the student's thought process and the solution.\r\n        \"\"\"\r\n        system_prompt = \"Now you are a student who are interacting with your tutor, Your teacher will gradually guide you to solve a problem. \" \\\r\n                        \"Please follow the instructions and guidance given by the teacher to solve the problem step by step. \" \\\r\n                        \"At the same time, please think carefully and put your thoughts and reasoning process within <think></think>, \" \\\r\n                        \"and output your solution for this step after </think>. Please be sure to think carefully before replying.\" + '\\nProblem:' + problem\r\n        messages = []\r\n        messages.append({\"role\": \"system\", \"content\": system_prompt})\r\n        assert len(previous_instruction) == len(previous_reasoning)\r\n        for i in range(len(previous_instruction)):\r\n            messages.append({\"role\": \"user\", \"content\": f'Teacher Instruction for Step{len(previous_instruction)}:' + previous_instruction[i]}) # Consider changing f-string here for consistency of step numbering\r\n            messages.append({\"role\": \"assistant\", \"content\": previous_reasoning[i]})\r\n        messages.append({\"role\": \"user\", \"content\": f'Teacher Instruction for Step{len(previous_instruction) + 1}:' + instruction}) # Consider changing f-string here for consistency of step numbering\r\n        thought, solution = self.generate(messages)\r\n        return thought, solution\r\n\r\n    def generate(self, messages):\r\n        \"\"\"\r\n        Generates a response from the language model and extracts thought and solution.\r\n\r\n        Args:\r\n            messages (list): A list of message dictionaries for the language model.\r\n\r\n        Returns:\r\n            tuple: A tuple containing the extracted thought and solution from the response.\r\n        \"\"\"\r\n        text = self.tokenizer.apply_chat_template(\r\n            messages,\r\n            tokenize=False,\r\n            add_generation_prompt=True\r\n        )\r\n        model_inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.model.device)\r\n\r\n        generated_ids = self.model.generate(\r\n            **model_inputs,\r\n            max_new_tokens=3072,\r\n            temperature=0.6,\r\n            top_p=0.95,\r\n        )\r\n\r\n        generated_ids = [\r\n            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\r\n        ]\r\n\r\n        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\r\n\r\n        thought = response.split('<think>')[-1].split('</think>')[0] \r\n        solution = response.split('</think>')[-1] \r\n        return thought, solution"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/models/utils.py", "content": "import collections\r\nimport fnmatch\r\nimport gc\r\nimport itertools\r\nimport time\r\nfrom functools import wraps\r\nfrom typing import (\r\n    TYPE_CHECKING,\r\n    Any,\r\n    Callable,\r\n    Dict,\r\n    Iterable,\r\n    Iterator,\r\n    List,\r\n    Literal,\r\n    Optional,\r\n    Tuple,\r\n    Type,\r\n    Union,\r\n)\r\n\r\nimport torch\r\nimport transformers\r\n\r\nfrom lm_eval.utils import eval_logger\r\n\r\n\r\nif TYPE_CHECKING:\r\n    from transformers import PreTrainedTokenizerBase\r\n    from transformers.configuration_utils import PretrainedConfig\r\n\r\n\r\ndef chunks(iter, n: int = 0, fn=None):\r\n    \"\"\"\r\n    Divides an iterable into chunks of specified size or based on a given function.\r\n    Useful for batching\r\n\r\n    Parameters:\r\n    - iter: The input iterable to be divided into chunks.\r\n    - n: An integer representing the size of each chunk. Default is 0.\r\n    - fn: A function that takes the current index and the iterable as arguments and returns the size of the chunk. Default is None.\r\n\r\n    Returns:\r\n    An iterator that yields chunks of the input iterable.\r\n\r\n    Example usage:\r\n    ```\r\n    data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\r\n    for chunk in chunks(data, 3):\r\n        print(chunk)\r\n    ```\r\n    Output:\r\n    ```\r\n    [1, 2, 3]\r\n    [4, 5, 6]\r\n    [7, 8, 9]\r\n    [10]\r\n    ```\r\n    \"\"\"\r\n    arr = []\r\n    for i, x in enumerate(iter):\r\n        arr.append(x)\r\n        if len(arr) == (fn(i, iter) if fn else n):\r\n            yield arr\r\n            arr = []\r\n\r\n    if arr:\r\n        yield arr\r\n\r\n\r\nclass MultiChoice:\r\n    def __init__(self, choices) -> None:\r\n        self.choices = choices\r\n\r\n    # Simple wildcard support (linux filename patterns)\r\n    def __contains__(self, values) -> bool:\r\n        for value in values.split(\",\"):\r\n            if len(fnmatch.filter(self.choices, value)) == 0:\r\n                eval_logger.info(\"Available tasks to choose:\")\r\n                for choice in self.choices:\r\n                    eval_logger.info(f\"  - {choice}\")\r\n                raise ValueError(\"'{}' is not in task list\".format(value))\r\n        return True\r\n\r\n    def __iter__(self) -> Iterator:\r\n        for choice in self.choices:\r\n            yield choice\r\n\r\n\r\nclass Grouper:\r\n    \"\"\"\r\n    takes an array `arr` and function `fn` and returns a dictionary\r\n    with keys fn(ob) for each ob in `arr` and with values `self.arr[key]` a list of all\r\n    objects in `arr` satisfying `key == fn(ob)`.\r\n    \"\"\"\r\n\r\n    def __init__(self, arr, fn) -> None:\r\n        # self.orig_arr = arr\r\n        self.size = len(arr)\r\n        arr = list(enumerate(arr))\r\n\r\n        def group_return_dict(arr, fn):\r\n            res = collections.defaultdict(list)\r\n\r\n            for ob in arr:\r\n                res[fn(ob)].append(ob)\r\n            return res\r\n\r\n        arr = group_return_dict(arr, lambda x: fn(x[1]))\r\n\r\n        # self.arr has format Dict[Tuple[int, <entry from orig. arr>]]\r\n        self.arr = arr\r\n        self._grouped = None\r\n\r\n    def get_grouped(self):\r\n        # return the contents but not indices for our grouped dict.\r\n        if self._grouped:\r\n            return self._grouped\r\n        grouped = {}\r\n        for key in self.arr.keys():\r\n            # drop the index from each element of self.arr\r\n            grouped[key] = [y[1] for y in self.arr[key]]\r\n        self._grouped = grouped\r\n        return grouped\r\n\r\n    def get_original(self, grouped_dict):\r\n        # take in a grouped dictionary with e.g. results for each key listed\r\n        # in the same order as the instances in `self.arr`, and\r\n        # return the results in the same (single list) order as `self.orig_arr`.\r\n        res = [None] * self.size\r\n        cov = [False] * self.size\r\n        # orig = [None] * self.size\r\n\r\n        assert grouped_dict.keys() == self.arr.keys()\r\n\r\n        for key in grouped_dict.keys():\r\n            for (ind, _), v in zip(self.arr[key], grouped_dict[key]):\r\n                res[ind] = v\r\n                cov[ind] = True\r\n                # orig[ind] = _\r\n\r\n        assert all(cov)\r\n        # assert orig == self.orig_arr\r\n\r\n        return res\r\n\r\n\r\ndef pad_and_concat(\r\n    max_length: int,\r\n    tensors: List[torch.Tensor],\r\n    padding_side: Literal[\"right\", \"left\"] = \"right\",\r\n):\r\n    \"\"\"\r\n    Method for padding a list of tensors given the maximum tensor\r\n    length in the batch. Used for batching inputs and continuations in\r\n    seq2seq models.\r\n    \"\"\"\r\n    assert (\r\n        padding_side == \"left\" or padding_side == \"right\"\r\n    ), f\"Unrecognized padding type: '{padding_side}' not 'left' or 'right'\"\r\n\r\n    for i, tensor in enumerate(tensors):\r\n        if len(tensor.shape) == 2:\r\n            tensor = tensor.squeeze(0)  # squeeze, in case passed [1, seq] size\r\n        tensor_len = tensor.shape[0]\r\n        if tensor_len < max_length:\r\n            if padding_side == \"right\":\r\n                # right-pad\r\n                tensors[i] = torch.cat(\r\n                    [\r\n                        tensor,  # [seq]\r\n                        torch.zeros(\r\n                            max_length - tensor_len,\r\n                            dtype=torch.long,\r\n                            device=tensor.device,\r\n                        ),  # [padding_length - seq]\r\n                    ],\r\n                    dim=0,\r\n                ).unsqueeze(0)\r\n            else:\r\n                # left-pad\r\n                tensors[i] = torch.cat(\r\n                    [\r\n                        torch.zeros(\r\n                            max_length - tensor_len,\r\n                            dtype=torch.long,\r\n                            device=tensor.device,\r\n                        ),  # [padding_length - seq]\r\n                        tensor,  # [seq]\r\n                    ],\r\n                    dim=0,\r\n                ).unsqueeze(0)\r\n        else:\r\n            tensors[i] = tensor.unsqueeze(0)\r\n\r\n    return torch.cat(tensors, dim=0)\r\n\r\n\r\ndef clear_torch_cache() -> None:\r\n    gc.collect()\r\n    torch.cuda.empty_cache()\r\n\r\n\r\ndef get_dtype(dtype: Union[str, torch.dtype]) -> torch.dtype:\r\n    \"\"\"Converts `dtype` from `str` to torch.dtype when possible. Does not use an instantiated HF AutoConfig\"\"\"\r\n    if isinstance(dtype, str) and dtype != \"auto\":\r\n        # Convert `str` args torch dtype: `float16` -> `torch.float16`\r\n        _torch_dtype = getattr(torch, dtype)\r\n    else:\r\n        _torch_dtype = dtype\r\n    return _torch_dtype\r\n\r\n\r\nclass MultiTokenEOSCriteria(transformers.StoppingCriteria):\r\n    \"\"\"Criteria to stop on the specified multi-token sequence.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        sequence: str,\r\n        tokenizer: transformers.PreTrainedTokenizer,\r\n        initial_decoder_input_length: int,\r\n        batch_size: int,\r\n    ) -> None:\r\n        self.initial_decoder_input_length = initial_decoder_input_length\r\n        self.done_tracker = [False] * batch_size\r\n        self.sequence = sequence\r\n        self.sequence_ids = tokenizer.encode(sequence, add_special_tokens=False)\r\n        # print(sequence, self.sequence_ids)\r\n        # we look back for 2 more tokens than it takes to encode our stop sequence\r\n        # because tokenizers suck, and a model might generate `['\\n', '\\n']` but our `sequence` is `['\\n\\n']`\r\n        # and we don't want to mistakenly not stop a generation because our\r\n        # (string) stop sequence was output in a different tokenization\r\n\r\n        # NOTE: there is a minor danger that this will end up looking back 2 tokens into the past, into the inputs to the model,\r\n        # and stopping generation immediately as a result. With only 2 extra tokens of lookback, this risk is minimized\r\n        # Additionally, in lookback_ids_batch we should prevent ever looking back into the inputs as described.\r\n        self.sequence_id_len = len(self.sequence_ids) + 2\r\n        self.tokenizer = tokenizer\r\n\r\n    def __call__(self, input_ids, scores, **kwargs) -> bool:\r\n        # For efficiency, we compare the last n tokens where n is the number of tokens in the stop_sequence\r\n        lookback_ids_batch = input_ids[:, self.initial_decoder_input_length :]\r\n\r\n        lookback_ids_batch = lookback_ids_batch[:, -self.sequence_id_len :]\r\n\r\n        lookback_tokens_batch = self.tokenizer.batch_decode(lookback_ids_batch)\r\n\r\n        for i, done in enumerate(self.done_tracker):\r\n            if not done:\r\n                self.done_tracker[i] = self.sequence in lookback_tokens_batch[i]\r\n        return False not in self.done_tracker\r\n\r\n\r\ndef stop_sequences_criteria(\r\n    tokenizer: transformers.PreTrainedTokenizer,\r\n    stop_sequences: List[str],\r\n    initial_decoder_input_length: int,\r\n    batch_size: int,\r\n) -> transformers.StoppingCriteriaList:\r\n    return transformers.StoppingCriteriaList(\r\n        [\r\n            *[\r\n                MultiTokenEOSCriteria(\r\n                    sequence, tokenizer, initial_decoder_input_length, batch_size\r\n                )\r\n                for sequence in stop_sequences\r\n            ],\r\n        ]\r\n    )\r\n\r\n\r\ndef undistribute(iterable):\r\n    \"\"\"\r\n    Undoes https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.distribute .\r\n\r\n    Re-interleaves results that have been split using more_itertools.distribute:\r\n        >>> group_1, group_2 = distribute(2, [1, 2, 3, 4, 5, 6])\r\n        >>> list(group_1)\r\n        [1, 3, 5]\r\n        >>> list(group_2)\r\n        [2, 4, 6]\r\n        >>> undistribute([group_1, group_2])\r\n        [1, 2, 3, 4, 5, 6]\r\n\r\n    Handles non-uniform component lengths:\r\n\r\n        >>> children = distribute(3, [1, 2, 3, 4, 5, 6, 7])\r\n        >>> [list(c) for c in children]\r\n        [[1, 4, 7], [2, 5], [3, 6]]\r\n        >>> undistribute(children)\r\n        [1, 2, 3, 4, 5, 6, 7]\r\n\r\n    Also handles when some iterables are empty:\r\n\r\n        >>> children = distribute(5, [1, 2, 3])\r\n        >>> [list(c) for c in children]\r\n        [[1], [2], [3], [], []]\r\n        >>> undistribute(children)\r\n        [1, 2, 3]\r\n\r\n    \"\"\"\r\n\r\n    return [\r\n        x\r\n        for x in itertools.chain.from_iterable(\r\n            itertools.zip_longest(*[list(x) for x in iterable])\r\n        )\r\n        if x is not None\r\n    ]\r\n\r\n\r\ndef retry_on_specific_exceptions(\r\n    on_exceptions: List[Type[Exception]],\r\n    max_retries: Optional[int] = None,\r\n    backoff_time: float = 3.0,\r\n    backoff_multiplier: float = 1.5,\r\n    on_exception_callback: Optional[Callable[[Exception, float], Any]] = None,\r\n):\r\n    \"\"\"Retry on an LLM Provider's rate limit error with exponential backoff\r\n    For example, to use for OpenAI, do the following:\r\n    ```\r\n    from openai import RateLimitError\r\n\r\n    # Recommend specifying max_retries to avoid infinite loops!\r\n    @retry_on_specific_exceptions([RateLimitError], max_retries=3)\r\n    def completion(...):\r\n        # Wrap OpenAI completion function here\r\n        ...\r\n    ```\r\n    \"\"\"\r\n\r\n    def decorator(func: Callable):\r\n        @wraps(func)\r\n        def wrapper(*args, **kwargs):\r\n            sleep_time = backoff_time\r\n            attempt = 0\r\n            while max_retries is None or attempt < max_retries:\r\n                try:\r\n                    return func(*args, **kwargs)\r\n                except tuple(on_exceptions) as e:\r\n                    if on_exception_callback is not None:\r\n                        on_exception_callback(e, sleep_time)\r\n                    time.sleep(sleep_time)\r\n                    sleep_time *= backoff_multiplier\r\n                    attempt += 1\r\n\r\n        return wrapper\r\n\r\n    return decorator\r\n\r\n\r\nclass Collator:\r\n    \"\"\"\r\n    A class for reordering and batching elements of an array.\r\n\r\n    This class allows for sorting an array based on a provided sorting function, grouping elements based on a grouping function, and generating batches from the sorted and grouped data.\r\n\r\n    Objects of this class have the group_by attribute which determines the method for grouping\r\n    the data while batching it. Three options include \"gen_kwargs\", \"contexts\", or None:\r\n        If group_by == \"gen_kwargs\" then requests will be grouped by gen_kwargs\r\n        If group_by == \"contexts\" then requests will be grouped by context + cont[:-1]\r\n        If None then requests will just be reordered by length descending.\r\n    \"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        arr: List,\r\n        sort_fn: Callable = lambda x: x,\r\n        group_fn: Callable = lambda x: x[1],\r\n        group_by: Union[Literal[\"gen_kwargs\", \"contexts\"], None] = None,\r\n    ) -> None:\r\n        self._group_by = group_by\r\n        # 0 indices are enumerated indices. Apply functions to original arr.\r\n        self._sort_fn = lambda x: sort_fn(x[1])\r\n        self._group_fn = lambda x: group_fn(x[1])\r\n        self._reorder_indices: List = []\r\n        self._size = len(arr)\r\n        self._arr_with_indices: Union[Dict, Tuple[Tuple[int, Any], ...]] = tuple(\r\n            enumerate(arr)\r\n        )  # [indices, (arr)]\r\n        if self._group_by == \"contexts\":\r\n            self._group_by_context()\r\n        elif self._group_by == \"gen_kwargs\":\r\n            self._group_by_index()\r\n\r\n    def _group_by_index(self) -> None:\r\n        \"\"\"Group the elements of a list based on their indices.\"\"\"\r\n        self._arr_with_indices = self.group(\r\n            self._arr_with_indices, fn=self._group_fn, group_by=\"gen_kwargs\"\r\n        )\r\n\r\n    def _group_by_context(self) -> None:\r\n        \"\"\"Group the array with indices by context.\"\"\"\r\n        self._arr_with_indices = self.group(\r\n            self._arr_with_indices, fn=self._group_fn, group_by=\"contexts\"\r\n        )\r\n\r\n    def get_batched(self, n: int = 1, batch_fn: Optional[Callable] = None) -> Iterator:\r\n        \"\"\"\r\n        Generates and yields batches from the reordered array. The method of grouping and batching\r\n        depends on the parameter `group_by`.\r\n        If `group_by` is set to \"gen_kwargs\", it will batch the\r\n        re-ordered values with same gen_kwargs for each batch.\r\n        If `group_by` is \"contexts\", it caches the requests by context before batching.\r\n        If `group_by` is neither \"gen_kwargs\" nor \"contexts\", it yields the reordered array\r\n\r\n        Parameters:\r\n        - n (int): The size of each batch. Defaults to 1.\r\n        - batch_fn ([Callable[[int, Iterable], int]] | None): A function to determine the size of\r\n          each batch. Optional, defaults to None.\r\n\r\n        Returns:\r\n        Iterator: An iterator over batches of reordered elements grouped as per the `group_by`\r\n                  attribute.\r\n\r\n        Yields:\r\n        List of batched elements according to the `group_by` attribute.\r\n        \"\"\"\r\n        if self._group_by == \"gen_kwargs\":\r\n            for (\r\n                key,\r\n                values,\r\n            ) in self._arr_with_indices.items():  # type: ignore\r\n                values = self._reorder(values)\r\n                batch = self.get_chunks(values, n=n, fn=batch_fn)\r\n                yield from batch\r\n        elif self._group_by == \"contexts\":\r\n            # Get one sample from each key\r\n            values = self._reorder(\r\n                [value[0] for value in self._arr_with_indices.values()]\r\n            )\r\n            batch = self.get_chunks(values, n=n, fn=batch_fn)\r\n            yield from batch\r\n        else:\r\n            values = self._reorder(self._arr_with_indices)  # type: ignore\r\n            batch = self.get_chunks(values, n=n, fn=batch_fn)\r\n            yield from batch\r\n\r\n    def get_cache(\r\n        self,\r\n        req_str: Tuple[str, str] = None,\r\n        cxt_toks: List[int] = None,\r\n        cont_toks: List[int] = None,\r\n        logits: torch.Tensor = None,\r\n    ) -> Iterator[Tuple[Tuple[str, str], List[int], torch.Tensor]]:\r\n        \"\"\"\r\n        Retrieves cached single-token continuations and their associated arguments, updating indices as necessary.\r\n\r\n        The behavior of this function varies depending on how the `group_by` attribute is set:\r\n\r\n        - When `group_by` is \"contexts\":\r\n            The function identifies single-token continuations by checking for keys that equate to\r\n            [context+continuation][-1] and logs the indices for re-ordering.\r\n            In this mode, this function can work in two scenarios:\r\n\r\n            1. Cache Hit - Single Match:\r\n                If a single matching context-continuation pair is found in the cache,\r\n                the function yields the original arguments.\r\n\r\n            2. Cache Hit - Multiple Matches:\r\n                If multiple matching context-continuation pairs are found in the cache,\r\n                the function expands the logits batch dimension to match the number of cache hits.\r\n                It updates the original requests and continuation tokens.\r\n\r\n        - When `group_by` is not set to \"contexts\":\r\n            This method yields the original arguments, logits and continuation tokens,\r\n            without checking for one-token continuations.\r\n\r\n        Parameters:\r\n        - req_str (tuple[str, str]): Original strings used for CachingLM.\r\n        - cxt_toks (list[int]): Full context tokens used for lookup.\r\n        - cont_toks (list[int]): Continuation tokens for which logits were generated.\r\n        - logits (torch.Tensor [1, seq_length, vocab_size]): Logits generated by the model given context and continuation keys.\r\n\r\n        Yields:\r\n        - Iterator:\r\n            - req_str (tuple[str, str]): strings used for CachingLM.\r\n            - cont_toks (list[int]) : continuation tokens.\r\n            - logits (torch.Tensor [1, seq_length, vocab_size]): The original logits (repeated cache hit times)\r\n        \"\"\"\r\n        if self._group_by == \"contexts\":\r\n            cache_hit: List[\r\n                Tuple[int, Tuple[Tuple[str, str], List[int], List[int]]]\r\n            ] = self._arr_with_indices.pop(tuple(cxt_toks + cont_toks[:-1]))\r\n            if (cache_size := len(cache_hit)) == 1:\r\n                self._reorder_indices.extend(x[0] for x in cache_hit)\r\n                yield req_str, cont_toks, logits\r\n            else:\r\n                # If we have matching requests then expand the batch dimension (no-op) and\r\n                # yield each along with its corresponding args.\r\n                multilogits = logits.expand(cache_size, -1, -1).chunk(cache_size)\r\n                indices, req_str, cont_toks = zip(\r\n                    *[(x[0], x[1][0], x[-1][-1]) for x in cache_hit]\r\n                )\r\n                self._reorder_indices.extend(indices)\r\n                for c_key, cont_tok, logit in zip(req_str, cont_toks, multilogits):\r\n                    yield c_key, cont_tok, logit\r\n        else:\r\n            yield req_str, cont_toks, logits\r\n\r\n    def _reorder(self, arr: Union[List, Tuple[Tuple[int, Any], ...]]) -> Iterator:\r\n        \"\"\"\r\n        Reorders the elements in the array based on the sorting function.\r\n\r\n        Parameters:\r\n        - arr (list | tuple[tuple[int, Any], ...]]): The array or iterable to be reordered.\r\n\r\n        Yields:\r\n            Iterator\r\n        \"\"\"\r\n        arr = sorted(arr, key=self._sort_fn)\r\n        if not self._group_by == \"contexts\":\r\n            # If grouped by contexts then indices will be set in get_cache()\r\n            self._reorder_indices.extend([x[0] for x in arr])\r\n        yield from [x[1] for x in arr]\r\n\r\n    def get_original(self, newarr: List) -> List:\r\n        \"\"\"\r\n        Restores the original order of elements from the reordered list.\r\n\r\n        Parameters:\r\n        - newarr (list): The reordered array.\r\n\r\n        Returns:\r\n        list: The array with elements restored to their original order.\r\n        \"\"\"\r\n        res = [None] * self._size\r\n        cov = [False] * self._size\r\n\r\n        for ind, v in zip(self._reorder_indices, newarr):\r\n            res[ind] = v\r\n            cov[ind] = True\r\n\r\n        assert all(cov)\r\n\r\n        return res\r\n\r\n    def __len__(self):\r\n        return self._size\r\n\r\n    @staticmethod\r\n    def group(\r\n        arr: Iterable,\r\n        fn: Callable,\r\n        group_by: Literal[\"gen_kwargs\", \"contexts\"] = \"gen_kwargs\",\r\n    ) -> dict:\r\n        \"\"\"\r\n        Groups elements of an iterable based on a provided function.\r\n\r\n\r\n        The `group_by` parameter determines the method of grouping.\r\n        If `group_by` is \"contexts\", the elements are grouped by [context + cont][:-1].\r\n        If `group_by` is \"gen_kwargs\", the elements are grouped based on the gen_kwargs dict.\r\n\r\n        Parameters:\r\n        - arr (Iterable): The iterable to be grouped.\r\n        - fn (Callable): The function to determine the grouping.\r\n        - values (bool): If True, returns the values of the group. Defaults to False.\r\n\r\n        Returns:\r\n        Iterator: An iterable of grouped elements.\r\n        \"\"\"\r\n        res = collections.defaultdict(list)\r\n        for ob in arr:\r\n            # where ob == [context + cont]\r\n            if group_by == \"contexts\":\r\n                res[tuple(fn(ob))].append(ob)\r\n            else:\r\n                try:\r\n                    hashable_dict = tuple(\r\n                        (\r\n                            key,\r\n                            tuple(value)\r\n                            if isinstance(value, collections.abc.Iterable)\r\n                            else value,\r\n                        )\r\n                        for key, value in sorted(fn(ob).items())\r\n                    )\r\n                    res[hashable_dict].append(ob)\r\n                except (TypeError, AttributeError):\r\n                    res[tuple(fn(ob))].append(ob)\r\n        return res\r\n\r\n    @staticmethod\r\n    def get_chunks(_iter, n: int = 0, fn=None):\r\n        \"\"\"\r\n        Divides an iterable into chunks of specified size or based on a given function.\r\n        Useful for batching\r\n\r\n        Parameters:\r\n        - iter: The input iterable to be divided into chunks.\r\n        - n: An integer representing the size of each chunk. Default is 0.\r\n        - fn: A function that takes the current index and the iterable as arguments and returns the size of the chunk. Default is None.\r\n\r\n        Returns:\r\n        An iterator that yields chunks of the input iterable.\r\n\r\n        Example usage:\r\n        ```\r\n        data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\r\n        for chunk in chunks(data, 3):\r\n            print(chunk)\r\n        ```\r\n        Output:\r\n        ```\r\n        [1, 2, 3]\r\n        [4, 5, 6]\r\n        [7, 8, 9]\r\n        [10]\r\n        ```\r\n        \"\"\"\r\n        arr = []\r\n        _iter = tuple(_iter)\r\n        for i, x in enumerate(_iter):\r\n            arr.append(x)\r\n            if len(arr) == (fn(i, _iter) if fn else n):\r\n                yield arr\r\n                arr = []\r\n\r\n        if arr:\r\n            yield arr\r\n\r\n\r\ndef configure_pad_token(\r\n    tokenizer: \"PreTrainedTokenizerBase\",\r\n    model_config: Optional[\"PretrainedConfig\"] = None,\r\n) -> \"PreTrainedTokenizerBase\":\r\n    \"\"\"\r\n    This function checks if the (Hugging Face) tokenizer has a padding token and sets it if not present.\r\n    Some tokenizers require special handling.\r\n\r\n    Args:\r\n        tokenizer: The tokenizer for which the padding token is to be handled.\r\n        model_config: The configuration of the model. Default is None.\r\n\r\n    Returns:\r\n        The tokenizer after the padding token has been handled.\r\n\r\n    Raises:\r\n        AssertionError: If the tokenizer is of type RWKVWorldTokenizer or Rwkv5Tokenizer and the padding token id is not 0.\r\n    \"\"\"\r\n    if tokenizer.pad_token:\r\n        pass\r\n    elif tokenizer.unk_token:\r\n        tokenizer.pad_token_id = tokenizer.unk_token_id\r\n    elif tokenizer.eos_token:\r\n        tokenizer.pad_token_id = tokenizer.eos_token_id\r\n    else:\r\n        # handle special cases\r\n        if model_config and getattr(model_config, \"model_type\", None) == \"qwen\":\r\n            # Qwen's trust_remote_code tokenizer does not allow for adding special tokens\r\n            tokenizer.pad_token = \"<|endoftext|>\"\r\n        elif (\r\n            tokenizer.__class__.__name__ == \"RWKVWorldTokenizer\"\r\n            or tokenizer.__class__.__name__ == \"Rwkv5Tokenizer\"\r\n        ):\r\n            # The RWKV world tokenizer, does not allow for adding special tokens / setting the pad token (which is set as 0)\r\n            # The additional tokenizer name check is needed, as there exists rwkv4 models with neox tokenizer\r\n            # ---\r\n            # Note that the world tokenizer class name, might change in the future for the final huggingface merge\r\n            # https://github.com/huggingface/transformers/pull/26963\r\n            assert tokenizer.pad_token_id == 0\r\n        else:\r\n            tokenizer.add_special_tokens({\"pad_token\": \"<|pad|>\"})\r\n\r\n    return tokenizer\r\n\r\n\r\ndef replace_placeholders(\r\n    string: str, default_placeholder: str, image_token: str, max_images: int\r\n):\r\n    \"\"\"\r\n    A utility function used for local multimodal models. It locates all `placeholder` string\r\n    occurrences in the given input `string_` and replaces the first `max_count` instances with\r\n    `replacement`, and all subsequent occurrences with the empty string.\r\n\r\n    This is used to replace <image> placeholder tags by model-specific image tokens like <|image_pad|>\r\n    and to allow for only the first `max_count` images to be passed to a model if desired.\r\n\r\n    :param string: The original string containing placeholders.\r\n    :param default_placeholder: The placeholder text to be replaced.\r\n    :param image_token: The token to replace the placeholder with.\r\n    :param max_images: The maximum number of replacements to make.\r\n    :return: The string with placeholders replaced.\r\n    \"\"\"\r\n    count = 0\r\n    result = []\r\n\r\n    parts = string.split(default_placeholder)\r\n    for part in parts[:-1]:  # Iterate through all but the last part\r\n        result.append(part)\r\n        if count < max_images:\r\n            result.append(image_token)\r\n            count += 1\r\n        elif default_placeholder != image_token:\r\n            result.append(default_placeholder)\r\n\r\n    # Add the last part of the string\r\n    result.append(parts[-1])\r\n    return \"\".join(result)\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/api/samplers.py", "content": "from functools import partial\r\n\r\nimport datasets\r\n\r\n\r\nclass ContextSampler:\r\n    def __init__(self, docs, task, fewshot_indices=None, rnd=None) -> None:\r\n        self.rnd = rnd\r\n        if not self.rnd:\r\n            raise ValueError(\r\n                \"A `random.Random` generator argument must be provided to `rnd` of FewShotSampler!\"\r\n            )\r\n\r\n        self.task = task\r\n        self.config = task._config\r\n\r\n        self.target_delimiter = self.config.target_delimiter\r\n        self.fewshot_delimiter = self.config.fewshot_delimiter\r\n\r\n        if (\r\n            self.config.fewshot_config is not None\r\n            and self.config.fewshot_config.get(\"doc_to_text\", None) is not None\r\n        ):\r\n            self.doc_to_text = partial(\r\n                self.task.doc_to_text,\r\n                doc_to_text=self.config.fewshot_config.get(\"doc_to_text\", None),\r\n            )\r\n        else:\r\n            self.doc_to_text = self.task.doc_to_text\r\n\r\n        if (\r\n            self.config.fewshot_config is not None\r\n            and self.config.fewshot_config.get(\"doc_to_target\", None) is not None\r\n        ):\r\n            self.doc_to_target = partial(\r\n                self.task.doc_to_target,\r\n                doc_to_target=self.config.fewshot_config.get(\"doc_to_target\", None),\r\n            )\r\n        else:\r\n            self.doc_to_target = self.task.doc_to_target\r\n\r\n        if (\r\n            self.config.fewshot_config is not None\r\n            and self.config.fewshot_config.get(\"doc_to_choice\", None) is not None\r\n        ):\r\n            self.doc_to_choice = partial(\r\n                self.task.doc_to_choice,\r\n                doc_to_choice=self.config.fewshot_config.get(\"doc_to_choice\", None),\r\n            )\r\n        else:\r\n            self.doc_to_choice = self.task.doc_to_choice\r\n\r\n        self.docs = docs  # HF dataset split, provided by task._fewshot_docs()\r\n        if fewshot_indices:  # subset few-shot docs from\r\n            if not isinstance(self.docs, datasets.Dataset):\r\n                raise ValueError(\r\n                    \"Got `fewshot_indices` but fewshot_docs are not a HF dataset. Don't use both `fewshot_indices` and a user-defined few-shot sample list simultaneously\"\r\n                )\r\n            self.docs = self.docs.select(fewshot_indices)\r\n\r\n    def get_context(self, doc, num_fewshot):\r\n        # draw an extra fewshot sample if using same split as evaluating on\r\n        n_samples = (\r\n            num_fewshot + 1\r\n            if self.config.fewshot_split == self.config.test_split\r\n            else num_fewshot\r\n        )\r\n\r\n        # draw `n_samples` docs from fewshot_docs\r\n        fewshotex = self.sample(n_samples)\r\n\r\n        # get rid of the doc that's the one we're evaluating, if it's in the fewshot\r\n        # TODO: should we just stop people from using fewshot from same split as evaluating?\r\n        selected_docs = [x for x in fewshotex if x != doc][:num_fewshot]\r\n\r\n        labeled_examples = \"\"\r\n        for doc in selected_docs:\r\n            doc_content = self.doc_to_text(doc)\r\n            doc_target = self.doc_to_target(doc)\r\n            labeled_examples += (\r\n                doc_content\r\n                if self.config.doc_to_choice is None or isinstance(doc_content, str)\r\n                else self.doc_to_choice(doc)[doc_content]\r\n            )\r\n\r\n            if doc_target != \"\":\r\n                labeled_examples += self.target_delimiter\r\n                labeled_examples += (\r\n                    str(doc_target[0])\r\n                    if isinstance(doc_target, list)\r\n                    else doc_target\r\n                    if self.config.doc_to_choice is None or isinstance(doc_target, str)\r\n                    else str(self.doc_to_choice(doc)[doc_target])\r\n                )\r\n                labeled_examples += self.fewshot_delimiter\r\n\r\n        return labeled_examples\r\n\r\n    def get_chat_context(\r\n        self,\r\n        doc,\r\n        num_fewshot,\r\n        fewshot_as_multiturn: bool = False,\r\n    ):\r\n        chat_history = []\r\n        # draw an extra fewshot sample if using same split as evaluating on\r\n        n_samples = (\r\n            num_fewshot + 1\r\n            if self.config.fewshot_split == self.config.test_split\r\n            else num_fewshot\r\n        )\r\n        # draw `n_samples` docs from fewshot_docs\r\n        fewshotex = self.sample(n_samples)\r\n\r\n        # get rid of the doc that's the one we're evaluating, if it's in the fewshot\r\n        # TODO: should we just stop people from using fewshot from same split as evaluating?\r\n        selected_docs = [x for x in fewshotex if x != doc][:num_fewshot]\r\n\r\n        if fewshot_as_multiturn:\r\n            for doc in selected_docs:\r\n                doc_content = self.doc_to_text(doc)\r\n                doc_target = self.doc_to_target(doc)\r\n                chat_history.append(\r\n                    {\r\n                        \"role\": \"user\",\r\n                        \"content\": doc_content\r\n                        if self.config.doc_to_choice is None\r\n                        or isinstance(doc_content, str)\r\n                        else self.doc_to_choice(doc)[doc_content],\r\n                    }\r\n                )\r\n                chat_history.append(\r\n                    {\r\n                        \"role\": \"assistant\",\r\n                        \"content\": str(doc_target[0])\r\n                        if isinstance(doc_target, list)\r\n                        else doc_target\r\n                        if self.config.doc_to_choice is None\r\n                        or isinstance(doc_target, str)\r\n                        else str(self.doc_to_choice(doc)[doc_target]),\r\n                    }\r\n                )\r\n        else:\r\n            # get fewshot context as one user turn\r\n            chat_history.append(\r\n                {\"role\": \"user\", \"content\": self.get_context(doc, num_fewshot)}\r\n            )\r\n\r\n        return chat_history\r\n\r\n    def sample(self, n):\r\n        \"\"\"\r\n        Draw `n` samples from our fewshot docs. This method should be overridden by subclasses.\r\n        \"\"\"\r\n\r\n        return self.rnd.sample(self.docs, n)\r\n\r\n\r\nclass FirstNSampler(ContextSampler):\r\n    def sample(self, n) -> None:\r\n        \"\"\"\r\n        Draw the first `n` samples in order from the specified split.\r\n        Used for tasks with \"canonical\" ordered fewshot examples, such as MMLU and CMMLU.\r\n        \"\"\"\r\n        assert (\r\n            n <= len(self.docs)\r\n        ), f\"Error: number of fewshot samples requested exceeds the {len(self.docs)} that are available.\"\r\n        return self.docs[:n]\r\n\r\n\r\nclass BalancedSampler(ContextSampler):\r\n    def sample(self, n) -> None:\r\n        \"\"\"\r\n        TODO: this should return approximately class-balanced samples from our fewshot examples.\r\n        TODO: what order should they be in? maybe random?\r\n        \"\"\"\r\n\r\n        pass\r\n\r\n\r\nclass ManualSampler(ContextSampler):\r\n    def sample(self, n) -> None:\r\n        \"\"\" \"\"\"\r\n        pass\r\n\r\n\r\nSAMPLER_REGISTRY = {\r\n    \"default\": ContextSampler,\r\n    \"first_n\": FirstNSampler,\r\n}\r\n\r\n\r\ndef get_sampler(name):\r\n    try:\r\n        return SAMPLER_REGISTRY[name]\r\n    except KeyError:\r\n        raise ValueError(\r\n            f\"Attempted to use contextsampler '{name}', but no sampling strategy for this name found! Supported model names: {', '.join(SAMPLER_REGISTRY.keys())}\"\r\n        )\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/api/instance.py", "content": "from dataclasses import dataclass, field\r\nfrom typing import Literal, Optional, Tuple\r\n\r\n\r\nOutputType = Literal[\r\n    \"loglikelihood\", \"loglikelihood_rolling\", \"generate_until\", \"multiple_choice\"\r\n]\r\n\r\n\r\n@dataclass\r\nclass Instance:\r\n    request_type: OutputType\r\n    doc: dict\r\n    arguments: tuple\r\n    idx: int\r\n    metadata: Tuple[Optional[str], Optional[int], Optional[int]] = field(\r\n        default_factory=lambda: (None, None, None)\r\n    )\r\n    resps: list = field(default_factory=list)\r\n    filtered_resps: dict = field(default_factory=dict)\r\n\r\n    # initialized after init\r\n    task_name: Optional[str] = None\r\n    doc_id: Optional[int] = None\r\n    repeats: Optional[int] = None\r\n\r\n    def __post_init__(self) -> None:\r\n        # unpack metadata field\r\n        self.task_name, self.doc_id, self.repeats = self.metadata\r\n\r\n    @property\r\n    def args(self):\r\n        \"\"\"\r\n        Returns (string,) where `string` is the string to calculate loglikelihood over\r\n        \"\"\"\r\n        return (\r\n            self.arguments if isinstance(self.arguments, tuple) else (self.arguments,)\r\n        )\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/caching/cache.py", "content": "import hashlib\r\nimport os\r\n\r\nimport dill\r\n\r\nfrom lm_eval.utils import eval_logger\r\n\r\n\r\nMODULE_DIR = os.path.dirname(os.path.realpath(__file__))\r\n\r\nOVERRIDE_PATH = os.getenv(\"LM_HARNESS_CACHE_PATH\")\r\n\r\n\r\nPATH = OVERRIDE_PATH if OVERRIDE_PATH else f\"{MODULE_DIR}/.cache\"\r\n\r\n# This should be sufficient for uniqueness\r\nHASH_INPUT = \"EleutherAI-lm-evaluation-harness\"\r\n\r\nHASH_PREFIX = hashlib.sha256(HASH_INPUT.encode(\"utf-8\")).hexdigest()\r\n\r\nFILE_SUFFIX = f\".{HASH_PREFIX}.pickle\"\r\n\r\n\r\ndef load_from_cache(file_name):\r\n    try:\r\n        path = f\"{PATH}/{file_name}{FILE_SUFFIX}\"\r\n\r\n        with open(path, \"rb\") as file:\r\n            cached_task_dict = dill.loads(file.read())\r\n            return cached_task_dict\r\n\r\n    except Exception:\r\n        eval_logger.debug(f\"{file_name} is not cached, generating...\")\r\n        pass\r\n\r\n\r\ndef save_to_cache(file_name, obj):\r\n    if not os.path.exists(PATH):\r\n        os.mkdir(PATH)\r\n\r\n    file_path = f\"{PATH}/{file_name}{FILE_SUFFIX}\"\r\n\r\n    eval_logger.debug(f\"Saving {file_path} to cache...\")\r\n    with open(file_path, \"wb\") as file:\r\n        file.write(dill.dumps(obj))\r\n\r\n\r\n# NOTE the \"key\" param is to allow for flexibility\r\ndef delete_cache(key: str = \"\"):\r\n    files = os.listdir(PATH)\r\n\r\n    for file in files:\r\n        if file.startswith(key) and file.endswith(FILE_SUFFIX):\r\n            file_path = f\"{PATH}/{file}\"\r\n            os.unlink(file_path)\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/decontamination/__init__.py", "content": ""}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/api/metrics.py", "content": "import logging\r\nimport math\r\nimport random\r\nimport re\r\nimport string\r\nfrom collections.abc import Iterable\r\nfrom typing import List\r\n\r\nimport numpy as np\r\nimport sacrebleu\r\n\r\nfrom lm_eval.api.registry import register_aggregation, register_metric\r\n\r\n\r\neval_logger = logging.getLogger(\"lm-eval\")\r\n\r\n\r\n# Register Aggregations First\r\n@register_aggregation(\"bypass\")\r\ndef bypass_agg(arr):\r\n    return -1\r\n\r\n@register_aggregation(\"mean_last30\")\r\ndef mean_last30(arr):\r\n    if len(arr) < 30:\r\n        print(\"Need >=30 items to calculate mean of last 30; returning -1\")\r\n        return -1\r\n    return sum(arr[-30:]) / 30\r\n\r\n@register_aggregation(\"mean\")\r\ndef mean(arr):\r\n    return sum(arr) / len(arr)\r\n\r\n\r\n@register_aggregation(\"median\")\r\ndef median(arr):\r\n    return arr[len(arr) // 2]\r\n\r\n\r\n# Certain metrics must be calculated across all documents in a benchmark.\r\n# We use them as aggregation metrics, paired with no-op passthrough metric fns.\r\n@register_aggregation(\"perplexity\")\r\ndef perplexity(items):\r\n    return math.exp(-mean(items))\r\n\r\n\r\n@register_aggregation(\"weighted_perplexity\")\r\ndef weighted_perplexity(items):\r\n    return math.exp(-weighted_mean(items))\r\n\r\n\r\n@register_aggregation(\"bits_per_byte\")\r\ndef bits_per_byte(items):\r\n    return -weighted_mean(items) / math.log(2)\r\n\r\n\r\n@register_aggregation(\"f1\")\r\ndef f1_score(items):\r\n    from sklearn.metrics import f1_score\r\n\r\n    unzipped_list = list(zip(*items))\r\n    golds = unzipped_list[0]\r\n    preds = unzipped_list[1]\r\n    fscore = f1_score(golds, preds)\r\n\r\n    return np.max(fscore)\r\n\r\n\r\n@register_aggregation(\"matthews_corrcoef\")\r\ndef matthews_corrcoef(items):\r\n    from sklearn.metrics import matthews_corrcoef\r\n\r\n    unzipped_list = list(zip(*items))\r\n    golds = unzipped_list[0]\r\n    preds = unzipped_list[1]\r\n    return matthews_corrcoef(golds, preds)\r\n\r\n\r\n@register_aggregation(\"bleu\")\r\ndef bleu(items):\r\n    \"\"\"The Bilingual Evaluation Understudy Score, or BLEU for short, is a metric\r\n    for evaluating a generated sentence to a reference sentence. It counts matching\r\n    n-grams in the candidate translation to n-grams in the reference text, where\r\n    1-gram or unigram would be each token and a bigram comparison would be each\r\n    word pair. The comparison is made regardless of word order\r\n    Source: https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\r\n    Paper: https://www.aclweb.org/anthology/P02-1040/\r\n\r\n    Higher is better\r\n    \"\"\"\r\n    refs = list(zip(*items))[0]\r\n    preds = list(zip(*items))[1]\r\n    refs, preds = _sacreformat(refs, preds)\r\n    return sacrebleu.corpus_bleu(preds, refs).score\r\n\r\n\r\n@register_aggregation(\"chrf\")\r\ndef chrf(items):\r\n    \"\"\"chrF++ is a tool for automatic evaluation of machine translation output\r\n    based on character n-gram precision and recall enhanced with word n-grams.\r\n    Source: https://github.com/m-popovic/chrF\r\n    Paper: https://www.aclweb.org/anthology/W15-3049.pdf\r\n\r\n    Higher is better  # TODO I think\r\n    \"\"\"\r\n    refs = list(zip(*items))[0]\r\n    preds = list(zip(*items))[1]\r\n    refs, preds = _sacreformat(refs, preds)\r\n    return sacrebleu.corpus_chrf(preds, refs).score\r\n\r\n\r\n@register_aggregation(\"ter\")\r\ndef ter(items):\r\n    \"\"\"Translation Error Rate is an error metric for machine translation that\r\n    measures the number of edits required to change a system output into one\r\n    of the references\r\n    Source: http://www.cs.umd.edu/~snover/tercom/\r\n    Paper: http://mt-archive.info/AMTA-2006-Snover.pdf\r\n\r\n    Lower is better\r\n    \"\"\"\r\n    refs = list(zip(*items))[0]\r\n    preds = list(zip(*items))[1]\r\n    refs, preds = _sacreformat(refs, preds)\r\n    return sacrebleu.corpus_ter(preds, refs).score\r\n\r\n\r\n@register_aggregation(\"brier_score\")\r\ndef brier_score(items):  # This is a passthrough function\r\n    gold, predictions = list(zip(*items))\r\n    bs, num_class = np.array(predictions).shape\r\n\r\n    gold = list(gold)\r\n    gold_one_hot = np.eye(num_class)[gold]\r\n    return np.mean(np.sum((predictions - gold_one_hot) ** 2, axis=1))\r\n\r\n\r\n@register_metric(\r\n    metric=\"brier_score\",\r\n    higher_is_better=False,\r\n    output_type=[\"multiple_choice\"],\r\n    aggregation=\"brier_score\",\r\n)\r\ndef brier_score_fn(items):  # This is a passthrough function\r\n    return items\r\n\r\n\r\n@register_metric(\r\n    metric=\"acc\",\r\n    higher_is_better=True,\r\n    output_type=[\"loglikelihood\", \"multiple_choice\"],\r\n    aggregation=\"mean\",\r\n)\r\ndef acc_fn(items):  # This is a passthrough function\r\n    return items\r\n\r\n\r\n@register_metric(\r\n    metric=\"acc_norm\",\r\n    higher_is_better=True,\r\n    output_type=[\"loglikelihood\", \"multiple_choice\"],\r\n    aggregation=\"mean\",\r\n)\r\ndef acc_norm_fn(items):  # This is a passthrough function\r\n    return items\r\n\r\n\r\n@register_metric(\r\n    metric=\"acc_mutual_info\",\r\n    higher_is_better=True,\r\n    output_type=\"multiple_choice\",\r\n    aggregation=\"mean\",\r\n)\r\ndef acc_mutual_info_fn(items):  # This is a passthrough function\r\n    return items\r\n\r\n\r\n### the code used in the `exact_match_hf_evaluate` function is ported from\r\n### https://github.com/huggingface/evaluate/blob/main/metrics/exact_match/exact_match.py\r\n### which is under the apache license.\r\n\r\n# Copyright 2020 The HuggingFace Datasets Authors and the current dataset script contributor.\r\n\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\ndef exact_match_hf_evaluate(\r\n    predictions,\r\n    references,\r\n    regexes_to_ignore=None,\r\n    ignore_case=False,\r\n    ignore_punctuation=False,\r\n    ignore_numbers=False,\r\n):\r\n    if regexes_to_ignore is not None:\r\n        for s in regexes_to_ignore:\r\n            predictions = np.array([re.sub(s, \"\", x) for x in predictions])\r\n            references = np.array([re.sub(s, \"\", x) for x in references])\r\n    else:\r\n        predictions = np.asarray(predictions)\r\n        references = np.asarray(references)\r\n\r\n    if ignore_case:\r\n        predictions = np.char.lower(predictions)\r\n        references = np.char.lower(references)\r\n\r\n    if ignore_punctuation:\r\n        repl_table = string.punctuation.maketrans(\"\", \"\", string.punctuation)\r\n        predictions = np.char.translate(predictions, table=repl_table)\r\n        references = np.char.translate(references, table=repl_table)\r\n\r\n    if ignore_numbers:\r\n        repl_table = string.digits.maketrans(\"\", \"\", string.digits)\r\n        predictions = np.char.translate(predictions, table=repl_table)\r\n        references = np.char.translate(references, table=repl_table)\r\n\r\n    score_list = predictions == references\r\n\r\n    return {\"exact_match\": np.mean(score_list)}\r\n\r\n\r\n###\r\n\r\n\r\n@register_metric(\r\n    metric=\"exact_match\",\r\n    higher_is_better=True,\r\n    output_type=\"generate_until\",\r\n    aggregation=\"mean\",\r\n)\r\ndef exact_match_fn(**kwargs):\r\n    return exact_match_hf_evaluate(**kwargs)\r\n\r\n\r\n@register_metric(\r\n    metric=\"perplexity\",\r\n    higher_is_better=False,\r\n    output_type=\"loglikelihood\",\r\n    aggregation=\"perplexity\",\r\n)\r\ndef perplexity_fn(items):  # This is a passthrough function\r\n    return items\r\n\r\n\r\n@register_metric(\r\n    metric=\"word_perplexity\",\r\n    higher_is_better=False,\r\n    output_type=\"loglikelihood_rolling\",\r\n    aggregation=\"weighted_perplexity\",\r\n)\r\ndef word_perplexity_fn(items):  # This is a passthrough function\r\n    return items\r\n\r\n\r\n@register_metric(\r\n    metric=\"byte_perplexity\",\r\n    higher_is_better=False,\r\n    output_type=\"loglikelihood_rolling\",\r\n    aggregation=\"weighted_perplexity\",\r\n)\r\ndef byte_perplexity_fn(items):  # This is a passthrough function\r\n    return items\r\n\r\n\r\n@register_metric(\r\n    metric=\"bits_per_byte\",\r\n    higher_is_better=False,\r\n    output_type=\"loglikelihood_rolling\",\r\n    aggregation=\"bits_per_byte\",\r\n)\r\ndef bits_per_byte_fn(items):  # This is a passthrough function\r\n    return items\r\n\r\n\r\ndef pop_stddev(arr):\r\n    mu = mean(arr)\r\n    return math.sqrt(sum([(x - mu) ** 2 for x in arr]) / len(arr))\r\n\r\n\r\ndef sample_stddev(arr):\r\n    mu = mean(arr)\r\n    return math.sqrt(sum([(x - mu) ** 2 for x in arr]) / (len(arr) - 1))\r\n\r\n\r\ndef mean_stderr(arr):\r\n    return sample_stddev(arr) / math.sqrt(len(arr))\r\n\r\n\r\n@register_metric(\r\n    metric=\"bypass\",\r\n    higher_is_better=True,\r\n    output_type=[\"loglikelihood\", \"multiple_choice\", \"generate_until\"],\r\n    aggregation=\"bypass\",\r\n)\r\ndef bypass(items):\r\n    return None\r\n\r\n\r\n@register_metric(\r\n    metric=\"mcc\",\r\n    higher_is_better=True,\r\n    output_type=\"multiple_choice\",\r\n    aggregation=\"matthews_corrcoef\",\r\n)\r\ndef mcc_fn(items):  # This is a passthrough function\r\n    return items\r\n\r\n\r\n@register_metric(\r\n    metric=\"f1\",\r\n    higher_is_better=True,\r\n    output_type=\"multiple_choice\",\r\n    aggregation=\"f1\",\r\n)\r\ndef f1_fn(items):  # This is a passthrough function\r\n    return items\r\n\r\n\r\n@register_metric(\r\n    metric=\"bleu\",\r\n    higher_is_better=True,\r\n    output_type=\"generate_until\",\r\n    aggregation=\"bleu\",\r\n)\r\ndef bleu_fn(items):  # This is a passthrough function\r\n    return items\r\n\r\n\r\n@register_metric(\r\n    metric=\"chrf\",\r\n    higher_is_better=True,\r\n    output_type=\"generate_until\",\r\n    aggregation=\"chrf\",\r\n)\r\ndef chrf_fn(items):  # This is a passthrough function\r\n    return items\r\n\r\n\r\n@register_metric(\r\n    metric=\"ter\",\r\n    higher_is_better=True,\r\n    output_type=\"generate_until\",\r\n    aggregation=\"ter\",\r\n)\r\ndef ter_fn(items):  # This is a passthrough function\r\n    return items\r\n\r\n\r\n@register_metric(\r\n    metric=\"acc_all\",\r\n    higher_is_better=True,\r\n    output_type=\"loglikelihood\",\r\n    aggregation=\"mean\",\r\n)\r\ndef acc_all(items):\r\n    # Only count as correct if all answers are labeled correctly for each question\r\n    question_scoring_dict = {}\r\n    preds = list(zip(*items))[0]\r\n    docs = list(zip(*items))[1]\r\n\r\n    for doc, pred in zip(docs, preds):\r\n        paragraph_id = doc[\"idx\"][\"paragraph\"]\r\n        question_id = doc[\"idx\"][\"question\"]\r\n        if (paragraph_id, question_id) not in question_scoring_dict:\r\n            question_scoring_dict[(paragraph_id, question_id)] = []\r\n\r\n        gold_label = doc[\"label\"] == 1\r\n\r\n        question_scoring_dict[(paragraph_id, question_id)].append(gold_label == pred)\r\n    acc = np.mean([int(all(x)) for x in question_scoring_dict.values()])\r\n    return acc\r\n\r\n\r\ndef acc_all_stderr(items):\r\n    # Only count as correct if all answers are labeled correctly for each question\r\n    question_scoring_dict = {}\r\n    preds = list(zip(*items))[0]\r\n    docs = list(zip(*items))[1]\r\n\r\n    for doc, pred in zip(docs, preds):\r\n        question_id = doc[\"idx\"][\"question\"]\r\n        if question_id not in question_scoring_dict:\r\n            question_scoring_dict[question_id] = []\r\n\r\n        gold_label = doc[\"label\"] == 1\r\n        question_scoring_dict[question_id].append(gold_label == pred)\r\n\r\n    acc = mean_stderr([int(all(x)) for x in question_scoring_dict.values()])\r\n    return acc\r\n\r\n\r\ndef metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\r\n    \"\"\"Compute max metric between prediction and each ground truth.\"\"\"\r\n    scores_for_ground_truths = []\r\n    for ground_truth in ground_truths:\r\n        score = metric_fn(prediction, ground_truth)\r\n        scores_for_ground_truths.append(score)\r\n    return max(scores_for_ground_truths)\r\n\r\n\r\ndef weighted_mean(items):\r\n    a, b = zip(*items)\r\n    return sum(a) / sum(b)\r\n\r\n\r\ndef is_non_str_iterable(obj):\r\n    return isinstance(obj, Iterable) and not isinstance(obj, str)\r\n\r\n\r\ndef _sacreformat(refs, preds):\r\n    \"\"\"Format refs and preds for sacrebleu corpus calculation. It is very particular\"\"\"\r\n    # Sacrebleu expects (List[str], List[List[str])\r\n    #   e.g. sacrebleu.corpus_bleu([pred_t], [[ref1_stream], [ref2_stream], ...])\r\n\r\n    # Note [ref1_stream] is the first reference for each pred.\r\n    # So lists are size N and (M, N) for N preds and M possible refs for each pred\r\n    # This is a different order of dimensions that I would expect\r\n\r\n    # We expect refs to be List[str] or List[List[str]], the outer list corresponding to preds\r\n    # Must become List[List[str]] with the inner list corresponding to preds\r\n    if not is_non_str_iterable(refs):\r\n        refs = list(refs)\r\n    if not is_non_str_iterable(refs[0]):\r\n        refs = [[ref] for ref in refs]\r\n    refs = list(zip(*refs))\r\n    # Note the number of refs in each ref list much match the number of preds\r\n\r\n    # We expect preds to be List[str] or List[List[str]]. Must become List[str]\r\n    if not is_non_str_iterable(preds):\r\n        preds = list(preds)\r\n    if is_non_str_iterable(preds[0]):\r\n        assert len(preds[0]) == 1, f\"Pred must be a str, was {preds[0]}\"\r\n        preds = [pred[0] for pred in preds]\r\n\r\n    return refs, preds\r\n\r\n\r\n# stderr stuff\r\n\r\n\r\nclass _bootstrap_internal:\r\n    def __init__(self, f, n) -> None:\r\n        self.f = f\r\n        self.n = n\r\n\r\n    def __call__(self, v):\r\n        i, xs = v\r\n        rnd = random.Random()\r\n        rnd.seed(i)\r\n        res = []\r\n        for _ in range(self.n):\r\n            res.append(self.f(rnd.choices(xs, k=len(xs))))\r\n        return res\r\n\r\n\r\ndef bootstrap_stderr(f, xs, iters):\r\n    import multiprocessing as mp\r\n\r\n    pool = mp.Pool(mp.cpu_count())\r\n    # this gives a biased estimate of the stderr (i.e w/ the mean, it gives something\r\n    # equivalent to stderr calculated without Bessel's correction in the stddev.\r\n    # Unfortunately, I haven't been able to figure out what the right correction is\r\n    # to make the bootstrap unbiased - i considered multiplying by sqrt(n/(n-1)) but\r\n    # that would be ad-hoc and I can't prove that that would actually be an unbiased estimator)\r\n    # Thankfully, shouldn't matter because our samples are pretty big usually anyways\r\n    res = []\r\n    chunk_size = min(1000, iters)\r\n    from tqdm import tqdm\r\n\r\n    print(\"bootstrapping for stddev:\", f.__name__)\r\n    for bootstrap in tqdm(\r\n        pool.imap(\r\n            _bootstrap_internal(f, chunk_size),\r\n            [(i, xs) for i in range(iters // chunk_size)],\r\n        ),\r\n        total=iters // chunk_size,\r\n    ):\r\n        # sample w replacement\r\n        res.extend(bootstrap)\r\n\r\n    pool.close()\r\n    return sample_stddev(res)\r\n\r\n\r\ndef stderr_for_metric(metric, bootstrap_iters: int):\r\n    if bootstrap_iters <= 0:\r\n        # return no function (don't compute stderr) if bootstrap iters = 0\r\n        return None\r\n\r\n    bootstrappable = [\r\n        median,\r\n        matthews_corrcoef,\r\n        f1_score,\r\n        perplexity,\r\n        bleu,\r\n        chrf,\r\n        ter,\r\n    ]\r\n\r\n    if metric in bootstrappable:\r\n        return lambda x: bootstrap_stderr(metric, x, iters=bootstrap_iters)\r\n\r\n    stderr = {mean: mean_stderr, acc_all: acc_all_stderr}\r\n\r\n    return stderr.get(metric, None)\r\n\r\n\r\ndef pooled_sample_stderr(stderrs: List[float], sizes: List[int]):\r\n    # Used to aggregate bootstrapped stderrs across subtasks in a group,\r\n    # when we are weighting by the size of each subtask.\r\n    #\r\n\r\n    assert len(stderrs) == len(sizes)\r\n\r\n    # formula source: https://en.wikipedia.org/wiki/Pooled_variance\r\n    # and: https://stats.stackexchange.com/a/4841331\r\n    # this empirically seems to match running `stderr_for_metric` on all instances\r\n    # from the subtasks concatenated with each other.\r\n    pooled_sample_var = (\r\n        sum([(size - 1) * stderr**2 * size for size, stderr in zip(sizes, stderrs)])\r\n    ) / (sum(sizes) - len(sizes))\r\n\r\n    return np.sqrt(pooled_sample_var / sum(sizes))\r\n\r\n\r\ndef combined_sample_stderr(stderrs: List[float], sizes: List[int], metrics=None):\r\n    assert (\r\n        metrics is not None\r\n    ), \"Need to pass a list of each subtask's metric for this stderr aggregation\"\r\n    assert len(stderrs) == len(sizes) and len(sizes) == len(metrics)\r\n\r\n    # See https://github.com/EleutherAI/lm-evaluation-harness/pull/1390 for more documentation.\r\n    # This formula depends on sample means.\r\n    # removed because it seems to give erroneously huge stderrs for groupings of tasks\r\n    # and does not seem to match up with bootstrap-calculated stderrs for groups.\r\n\r\n    ### don't use this unless a statistician has told you it's the right thing to do ###\r\n\r\n    # accumulators: we'll aggregate pairwise N - 1 times\r\n    variance = stderrs[0] ** 2\r\n    curr_size = sizes[0]\r\n    curr_score = metrics[0]\r\n\r\n    for stderr, size, score in zip(stderrs[1:], sizes[1:], metrics[1:]):\r\n        curr_score = ((curr_score * curr_size) + (score * size)) / (\r\n            curr_size + size\r\n        )  # NOTE: this assumes our aggregation fn is \"mean\"\r\n\r\n        variance = ((curr_size - 1) * variance + (size - 1) * (stderr**2)) / (\r\n            curr_size + size - 1\r\n        ) + curr_size * size / ((curr_size + size) * (curr_size + size - 1)) * (\r\n            curr_score - score\r\n        ) ** 2\r\n\r\n    return np.sqrt(variance)\r\n\r\n\r\ndef aggregate_subtask_metrics(metrics, sizes, weight_by_size=True):\r\n    # A helper function that is used to aggregate\r\n    # subtask scores cross-task.\r\n    # TODO: does not hold for non-mean aggregations\r\n    if not weight_by_size:\r\n        sizes = [1] * len(sizes)\r\n\r\n    assert len(metrics) == len(sizes)\r\n\r\n    return sum([metric * size for metric, size in zip(metrics, sizes)]) / sum(sizes)\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/api/model.py", "content": "import abc\r\nimport hashlib\r\nimport json\r\nimport logging\r\nimport os\r\nfrom typing import Dict, List, Optional, Tuple, Type, TypeVar, Union\r\n\r\nimport transformers\r\nfrom sqlitedict import SqliteDict\r\nfrom tqdm import tqdm\r\n\r\nfrom lm_eval import utils\r\n\r\n\r\neval_logger = logging.getLogger(\"lm-eval\")\r\n\r\nT = TypeVar(\"T\", bound=\"LM\")\r\n\r\n\r\nclass LM(abc.ABC):\r\n    def __init__(self) -> None:\r\n        \"\"\"Defines the interface that should be implemented by all LM subclasses.\r\n        LMs are assumed to take text (strings) as input and yield strings as output\r\n        (inputs/outputs should be tokenization-agnostic.)\r\n\r\n        \"\"\"\r\n        # set rank and world size to a single process, by default.\r\n        self._rank = 0\r\n        self._world_size = 1\r\n        self.cache_hook = CacheHook(None)\r\n\r\n    @abc.abstractmethod\r\n    def loglikelihood(self, requests) -> List[Tuple[float, bool]]:\r\n        \"\"\"Compute log-likelihood of generating a continuation from a context.\r\n        Downstream tasks should attempt to use loglikelihood instead of other\r\n        LM calls whenever possible.\r\n\r\n        :param requests: list[Instance]\r\n            A list of Instance objects, with property `args` which returns a tuple (context, continuation).\r\n            `context: str`\r\n                Context string. Implementations of LM must be able to handle an\r\n                empty context string.\r\n            `continuation: str`\r\n                The continuation over which log likelihood will be calculated. If\r\n                there is a word boundary, the space should be in the continuation.\r\n                For example, context=\"hello\" continuation=\" world\" is correct.\r\n\r\n        :return: list[tuple[float, bool]]\r\n            A list of pairs (logprob, isgreedy)\r\n            `logprob: float`\r\n                The log probability of `continuation`.\r\n            `isgreedy`:\r\n                Whether `continuation` would be generated by greedy sampling from `context`.\r\n        \"\"\"\r\n        pass\r\n\r\n    @abc.abstractmethod\r\n    def loglikelihood_rolling(self, requests) -> List[float]:\r\n        \"\"\"Compute full log-likelihood of a string, with no truncation, for perplexity computation\r\n        - We will use the full max context length of the model.\r\n        - For inputs that exceed the max context length, we divide the tokenized string into chunks of up to\r\n        the max context length.\r\n        - IMPORTANT: Each document's loglikelihood/perplexity is computed *separately*, unlike other implementations\r\n          which may simply concatenate multiple documents together.\r\n        - IMPORTANT: We maximize the amount of context for each prediction. Specifically, for inputs that we break into\r\n          multiple chunks, the last input will still a full-sized context.\r\n          Example:\r\n            Input tokens: [ 0 1 2 3 4 5 6 7 8 9 ]\r\n            Prefix: BOS/EOS\r\n            Max context length: 4\r\n            Resulting input/prediction pairs:\r\n\r\n                INPUT:  BOS   0   1   2\r\n                PRED:     0   1   2   3\r\n\r\n                INPUT:    3   4   5   6\r\n                PRED:     4   5   6   7\r\n\r\n                INPUT:    5   6   7   8\r\n                PRED:             8   9\r\n\r\n          Observe that:\r\n            1. Each token is predicted exactly once\r\n            2. For the last pair, we provide the full context, but only score the last two tokens\r\n\r\n        :param requests: list[Instance]\r\n            A list of Instance objects with property `args` which returns a tuple (context,).\r\n            string: str\r\n                String for which we are computing overall loglikelihood\r\n        :return: list[tuple[float]]\r\n            A list of tuples (logprob,)\r\n            logprob: float\r\n                The log probability of `context` conditioned on the BOS/EOS token.\r\n                Can also be overridden for custom cases by `prefix_token_id`.\r\n        \"\"\"\r\n        pass\r\n\r\n    # TODO: Add an optional max length\r\n    @abc.abstractmethod\r\n    def generate_until(self, requests) -> List[str]:\r\n        \"\"\"Generate greedily until a stopping sequence\r\n\r\n        :param requests: list[Instance]\r\n            A list of Instance objects with property `args` which returns a tuple (context, gen_kwargs).\r\n            context: str\r\n                Context string\r\n            gen_kwargs: dict\r\n                A dictionary of keyword arguments to pass to the generation function e.g. top_k, until, etc.\r\n        :return: list[str]\r\n            A list of model generated continuations.\r\n            continuation: str\r\n                The generated continuation.\r\n        \"\"\"\r\n        pass\r\n\r\n    def apply_chat_template(self, chat_history: List[Dict[str, str]]) -> str:\r\n        \"\"\"\r\n        Defines how to transform few-shot examples provided as chat history into a format that can be used as input to the LM.\r\n\r\n        :param chat_history: list[dict[str, str]]\r\n            A list of dictionaries with keys 'role' and 'content'.\r\n            Values are strings representing the role name and the content of the message, respectively.\r\n        :return: str\r\n            A string representing the chat history in a format that can be used as input to the LM.\r\n        \"\"\"\r\n        raise NotImplementedError(\r\n            \"To use this model with chat templates, please implement the 'apply_chat_template' method for your model type.\"\r\n        )\r\n\r\n    @classmethod\r\n    def create_from_arg_string(\r\n        cls: Type[T], arg_string: str, additional_config: Optional[dict] = None\r\n    ) -> T:\r\n        \"\"\"\r\n        Creates an instance of the LM class using the given argument string and additional config.\r\n\r\n        Parameters:\r\n        - arg_string: A string containing arguments in the format key1=value1,key2=value2.\r\n        - additional_config: Optional dictionary containing additional configuration parameters.\r\n\r\n        Returns:\r\n        - Instance of the LM class.\r\n        \"\"\"\r\n        additional_config = {} if additional_config is None else additional_config\r\n        args = utils.simple_parse_args_string(arg_string)\r\n        args2 = {k: v for k, v in additional_config.items() if v is not None}\r\n        return cls(**args, **args2)\r\n\r\n    @classmethod\r\n    def create_from_arg_obj(\r\n        cls: Type[T], arg_dict: dict, additional_config: Optional[dict] = None\r\n    ) -> T:\r\n        \"\"\"\r\n        Creates an instance of the LM class using the given arg_obj\r\n\r\n        Parameters:\r\n        - arg_obj: A dict containing arguments in the format key1=value1,key2=value2.\r\n        - additional_config: Optional dictionary containing additional configuration parameters.\r\n\r\n        Returns:\r\n        - Instance of the LM class.\r\n        \"\"\"\r\n\r\n        additional_config = {} if additional_config is None else additional_config\r\n        additional_config = {\r\n            k: v for k, v in additional_config.items() if v is not None\r\n        }\r\n\r\n        return cls(**arg_dict, **additional_config)\r\n\r\n    @property\r\n    def rank(self):\r\n        # used in the case of parallelism. Hardcoded to\r\n        # ensure no errors arise using API models which do\r\n        # not support multi-device parallelism nor expect it.\r\n        return self._rank\r\n\r\n    @property\r\n    def world_size(self):\r\n        # used in the case of parallelism. Hardcoded to\r\n        # ensure no errors arise using API models which do\r\n        # not support multi-device parallelism nor expect it.\r\n        return self._world_size\r\n\r\n    @property\r\n    def tokenizer_name(self) -> str:\r\n        \"\"\"Must be defined for LM subclasses which implement Chat Templating.\r\n        Should return the name of the tokenizer or chat template used.\r\n        Used only to properly fingerprint caches when requests are being cached with `--cache_requests`, otherwise not used.\r\n        \"\"\"\r\n        raise NotImplementedError(\r\n            \"To use this model with chat templates, please implement the 'tokenizer_name' property.\"\r\n        )\r\n\r\n    def chat_template(self, chat_template: Union[bool, str] = False) -> Optional[str]:\r\n        \"\"\"Returns the chat template structure for user/assistant messages if a template is provided.\r\n        This method is intended to be overridden in a subclass to define a specific chat template format.\r\n        For models that do not support chat templates, this method returns None by default.\r\n        \"\"\"\r\n\r\n        return \"\"\r\n\r\n    def set_cache_hook(self, cache_hook) -> None:\r\n        self.cache_hook = cache_hook\r\n\r\n\r\n### SQLite-based caching of LM responses\r\ndef hash_args(attr, args):\r\n    dat = json.dumps([attr] + list(args))\r\n    return hashlib.sha256(dat.encode(\"utf-8\")).hexdigest()\r\n\r\n\r\nclass CacheHook:\r\n    def __init__(self, cachinglm) -> None:\r\n        if cachinglm is None:\r\n            self.dbdict = None\r\n            return\r\n\r\n        self.dbdict = cachinglm.dbdict\r\n\r\n    def add_partial(self, attr, req, res) -> None:\r\n        if self.dbdict is None:\r\n            return\r\n        hsh = hash_args(attr, req)\r\n        self.dbdict[hsh] = res\r\n\r\n\r\nclass CachingLM:\r\n    def __init__(self, lm, cache_db) -> None:\r\n        \"\"\"LM wrapper that returns cached results if they exist, and uses the underlying LM if not.\r\n\r\n        :param lm: LM\r\n            Underlying LM\r\n        :param cache_db: str\r\n            Path to cache db\r\n        \"\"\"\r\n        self.lm = lm\r\n        self.cache_db = cache_db\r\n        if os.path.dirname(cache_db):\r\n            os.makedirs(os.path.dirname(cache_db), exist_ok=True)\r\n        self.dbdict = SqliteDict(cache_db, autocommit=True)\r\n\r\n        # add hook to lm\r\n        lm.set_cache_hook(self.get_cache_hook())\r\n\r\n    def __getattr__(self, attr: str):\r\n        lm_attr = getattr(self.lm, attr)\r\n        if attr not in [\"loglikelihood\", \"loglikelihood_rolling\", \"generate_until\"]:\r\n            eval_logger.debug(f\"Passing through attribute '{attr}' to underlying LM\")\r\n            return lm_attr\r\n\r\n        def fn(requests):\r\n            res = []\r\n            remaining_reqs = []\r\n            warned = False\r\n            # figure out which ones are cached and which ones are new\r\n            eval_logger.info(\r\n                f\"Loading '{attr}' responses from cache '{self.cache_db}' where possible...\"\r\n            )\r\n            for req in tqdm(requests, desc=\"Checking cached requests\"):\r\n                hsh = hash_args(attr, req.args)\r\n                if attr == \"generate_until\" and req.args[1].get(\"do_sample\", False):\r\n                    # when we are doing non-greedy generation, don't use the cache\r\n                    # (else every \"randomly sampled\" generation would be identical for repeats > 1).\r\n                    if not warned:\r\n                        eval_logger.warning(\r\n                            f\"Arguments to lm.generate_until() '{req.args[1]}' include non-deterministic sampling. Caching will not be performed for such requests.\"\r\n                        )\r\n                        warned = True\r\n                    res.append(None)\r\n                    remaining_reqs.append(req)\r\n                elif hsh in self.dbdict:\r\n                    ob = self.dbdict[hsh]\r\n\r\n                    assert ob is not None\r\n\r\n                    res.append(ob)\r\n                else:\r\n                    res.append(None)\r\n                    remaining_reqs.append(req)\r\n            eval_logger.info(\r\n                f\"Cached requests: {len(requests) - len(remaining_reqs)}, Requests remaining: {len(remaining_reqs)}\"\r\n            )\r\n            if remaining_reqs:\r\n                # actually run the LM on the requests that do not have cached results\r\n                rem_res = getattr(self.lm, attr)(remaining_reqs)\r\n            else:\r\n                rem_res = []\r\n\r\n            # stick the new ones back into the list and also cache any of the new ones\r\n            resptr = 0\r\n            for req, r in zip(remaining_reqs, rem_res):\r\n                while res[resptr] is not None:\r\n                    resptr += 1\r\n\r\n                res[resptr] = r\r\n\r\n                # caching\r\n                hsh = hash_args(attr, req.args)\r\n                self.dbdict[hsh] = r\r\n            self.dbdict.commit()\r\n\r\n            return res\r\n\r\n        return fn\r\n\r\n    def get_cache_hook(self):\r\n        return CacheHook(self)\r\n\r\n\r\nclass TemplateLM(LM):\r\n    \"\"\"\r\n    A class acting as intermediary between the LM base class\r\n    and boilerplate often included in other LM subclasses.\r\n    \"\"\"\r\n\r\n    tokenizer = None\r\n\r\n    @property\r\n    @abc.abstractmethod\r\n    def eot_token_id(self):\r\n        pass\r\n\r\n    @property\r\n    def prefix_token_id(self):\r\n        # it is used as prefix for loglikelihood\r\n        return self.eot_token_id\r\n\r\n    @abc.abstractmethod\r\n    def tok_encode(self, string: str, **kwargs) -> List[int]:\r\n        \"\"\"\r\n        Tokenize a string using the model's tokenizer and return a list of token IDs.\r\n        \"\"\"\r\n        pass\r\n\r\n    @abc.abstractmethod\r\n    def _loglikelihood_tokens(self, requests, **kwargs) -> List[Tuple[float, bool]]:\r\n        pass\r\n\r\n    def _encode_pair(\r\n        self, context: str, continuation: str\r\n    ) -> Tuple[List[int], List[int]]:\r\n        n_spaces = len(context) - len(context.rstrip())\r\n        if n_spaces > 0:\r\n            continuation = context[-n_spaces:] + continuation\r\n            context = context[:-n_spaces]\r\n\r\n        model_class = getattr(self, \"AUTO_MODEL_CLASS\", None)\r\n\r\n        if model_class == transformers.AutoModelForSeq2SeqLM:\r\n            context_enc = self.tok_encode(context)\r\n            continuation_enc = self.tok_encode(continuation, add_special_tokens=False)\r\n        else:\r\n            whole_enc = self.tok_encode(context + continuation)\r\n            context_enc = self.tok_encode(context)\r\n\r\n            context_enc_len = len(context_enc)\r\n            continuation_enc = whole_enc[context_enc_len:]\r\n\r\n        return context_enc, continuation_enc\r\n\r\n    def loglikelihood(\r\n        self, requests, disable_tqdm: bool = False\r\n    ) -> List[Tuple[float, bool]]:\r\n        new_reqs = []\r\n        for context, continuation in [req.args for req in requests]:\r\n            if context == \"\":\r\n                # BOS or EOS as context\r\n                context_enc, continuation_enc = (\r\n                    [self.prefix_token_id],\r\n                    self.tok_encode(continuation),\r\n                )\r\n            else:\r\n                context_enc, continuation_enc = self._encode_pair(context, continuation)\r\n\r\n            new_reqs.append(((context, continuation), context_enc, continuation_enc))\r\n\r\n        return self._loglikelihood_tokens(new_reqs, disable_tqdm=disable_tqdm)\r\n\r\n    @abc.abstractmethod\r\n    def loglikelihood_rolling(\r\n        self, requests, disable_tqdm: bool = False\r\n    ) -> List[float]:\r\n        pass\r\n\r\n    @abc.abstractmethod\r\n    def generate_until(self, requests, disable_tqdm: bool = False) -> List[str]:\r\n        pass\r\n\r\n    def chat_template(self, chat_template: Union[bool, str] = False) -> Optional[str]:\r\n        \"\"\"\r\n        Set and get the appropriate chat template for the model.\r\n        This method sets the tokenizer's chat_template and returns the template string for reproducibility.\r\n\r\n        The template selection logic is adapted from the Transformers library's `apply_chat_template`\r\n        method in the Tokenizer class. The original implementation can be found at:\r\n        https://github.com/huggingface/transformers/blob/fc35907f95459d7a6c5281dfadd680b6f7b620e3/src/transformers/tokenization_utils_base.py#L1687\r\n\r\n        This method ensures that the right template is chosen based on the following:\r\n        0. If the model has no 'tokenizer' attribute: assumes that there is only a single possible chat template, handled on the model provider side internally. Returns the empty string.\r\n        1. If the model's tokenizer has multiple templates:\r\n            a. Use the specified template if it exists in the dictionary.\r\n            b. Use the default template from the list if no specific template is provided.\r\n            c. Raise an error if no default template exists and no specific template is provided.\r\n        2. If the model's tokenizer has a single template or no template:\r\n            a. Use the tokenizer's chat template if available.\r\n            b. Fall back to the default chat template if no tokenizer chat template exists.\r\n\r\n        Args:\r\n            chat_template (Union[bool, str]): Specifies the chat template to use.\r\n                - If False or None, no template is applied.\r\n                - If True, the default or only available template is used.\r\n                - If a string, the template with the matching name is used.\r\n\r\n        Returns:\r\n            Optional[str]: The selected chat template, or None if no template is applied.\r\n        \"\"\"\r\n        if self.tokenizer is None:\r\n            return \"\"\r\n\r\n        if chat_template is False or chat_template is None:\r\n            eval_logger.warning(\r\n                \"model.chat_template was called with the chat_template set to False or None. \"\r\n                \"Therefore no chat template will be applied. Make sure this is an intended behavior.\"\r\n            )\r\n            return None\r\n\r\n        # Convert boolean chat_template to None to ensure compatibility with the adapted logic\r\n        if isinstance(chat_template, bool):\r\n            chat_template = None\r\n        using_default_template = False\r\n\r\n        # First, handle the cases when the model has a dict of multiple templates\r\n        template = self.tokenizer.chat_template or self.tokenizer.default_chat_template\r\n\r\n        if isinstance(template, dict):\r\n            using_default_dict = self.tokenizer.chat_template is None\r\n\r\n            if chat_template is not None:\r\n                if chat_template in template:\r\n                    selected_template = template[chat_template]\r\n                    if using_default_dict:\r\n                        using_default_template = True\r\n                else:\r\n                    raise ValueError(\r\n                        f\"The specified chat template '{chat_template}' is not available. \"\r\n                        f\"Available template names are {sorted(template.keys())}.\"\r\n                    )\r\n            else:\r\n                # If user didn't pass a chat template, use the default template from the dict\r\n                if \"default\" in template:\r\n                    selected_template = template[\"default\"]\r\n                    using_default_template = True\r\n                else:\r\n                    raise ValueError(\r\n                        \"This model has multiple chat templates with no default specified! Please either pass a chat \"\r\n                        \"template or the name of the template you wish to use to the `chat_template` argument. Available \"\r\n                        f\"template names are {sorted(template.keys())}.\"\r\n                    )\r\n\r\n        # Cases when the model has a single template or no template\r\n        else:\r\n            # priority: `chat_template` argument > `tokenizer.chat_template` > `tokenizer.default_chat_template\r\n            if isinstance(chat_template, str):\r\n                eval_logger.warning(\r\n                    \"Chat template name provided, but the tokenizer's chat template is not a dictionary. \"\r\n                    \"Using the tokenizer's chat template or the default template instead.\"\r\n                )\r\n            if self.tokenizer.chat_template is not None:\r\n                selected_template = self.tokenizer.chat_template\r\n            else:\r\n                selected_template = self.tokenizer.default_chat_template\r\n                using_default_template = True\r\n\r\n        if using_default_template:\r\n            eval_logger.warning(\r\n                \"No chat template is set for this tokenizer, falling back to a default class-level template. This is \"\r\n                \"very error-prone, because models are often trained with templates different from the class default! \"\r\n                \"Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which \"\r\n                \"point any code depending on them will stop working. We recommend setting a valid chat template before \"\r\n                \"then to ensure that this model continues working without issues.\"\r\n            )\r\n\r\n        return selected_template\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/api/task.py", "content": "import abc\r\nimport ast\r\nimport logging\r\nimport random\r\nimport re\r\nfrom collections.abc import Callable\r\nfrom copy import deepcopy\r\nfrom dataclasses import asdict, dataclass\r\nfrom inspect import getsource\r\nfrom typing import (\r\n    Any,\r\n    Dict,\r\n    Iterable,\r\n    Iterator,\r\n    List,\r\n    Literal,\r\n    Mapping,\r\n    Optional,\r\n    Tuple,\r\n    Union,\r\n)\r\n\r\nimport datasets\r\nimport numpy as np\r\nfrom tqdm import tqdm\r\n\r\nfrom lm_eval import utils\r\nfrom lm_eval.api import samplers\r\nfrom lm_eval.api.instance import Instance, OutputType\r\nfrom lm_eval.api.metrics import bits_per_byte, mean, weighted_perplexity\r\nfrom lm_eval.api.registry import (\r\n    AGGREGATION_REGISTRY,\r\n    DEFAULT_METRIC_REGISTRY,\r\n    get_aggregation,\r\n    get_metric,\r\n    get_metric_aggregation,\r\n    is_higher_better,\r\n)\r\nfrom lm_eval.caching.cache import load_from_cache, save_to_cache\r\nfrom lm_eval.filters import build_filter_ensemble\r\nfrom lm_eval.prompts import get_prompt\r\n\r\n\r\nALL_OUTPUT_TYPES = [\r\n    \"loglikelihood\",\r\n    \"multiple_choice\",\r\n    \"loglikelihood_rolling\",\r\n    \"generate_until\",\r\n]\r\n\r\neval_logger = logging.getLogger(\"lm-eval\")\r\n\r\n\r\n@dataclass\r\nclass TaskConfig(dict):\r\n    # task naming/registry\r\n    task: Optional[str] = None\r\n    task_alias: Optional[str] = None\r\n    tag: Optional[Union[str, list]] = None\r\n    group: Optional[Union[str, list]] = None\r\n    # HF dataset options.\r\n    # which dataset to use,\r\n    # and what splits for what purpose\r\n    dataset_path: Optional[str] = None\r\n    dataset_name: Optional[str] = None\r\n    dataset_kwargs: Optional[dict] = None\r\n    training_split: Optional[str] = None\r\n    validation_split: Optional[str] = None\r\n    test_split: Optional[str] = None\r\n    fewshot_split: Optional[str] = (\r\n        None  # TODO: assert that this not None if num_fewshot > 0. (?) assert if this is same split as one evaling (?)\r\n    )\r\n    # formatting / prompting options.\r\n    # see docs/advanced_task_guide.md for more info\r\n    process_docs: Optional[Callable] = None\r\n    doc_to_text: Optional[Union[Callable, str]] = None\r\n    doc_to_target: Optional[Union[Callable, str]] = None\r\n    doc_to_image: Union[Callable, str] = None\r\n    doc_to_choice: Optional[Union[Callable, str, dict, list]] = None\r\n    process_results: Optional[Union[Callable, str]] = None\r\n    use_prompt: Optional[str] = None\r\n    description: str = \"\"\r\n    target_delimiter: str = \" \"\r\n    fewshot_delimiter: str = \"\\n\\n\"\r\n    fewshot_config: Optional[dict] = None\r\n    # runtime configuration options\r\n    num_fewshot: Optional[int] = None\r\n    # scoring options\r\n    metric_list: Optional[list] = None\r\n    output_type: OutputType = \"generate_until\"\r\n    generation_kwargs: Optional[dict] = None\r\n    repeats: int = 1\r\n    filter_list: Optional[Union[str, list]] = None\r\n    should_decontaminate: bool = False\r\n    doc_to_decontamination_query: Optional[str] = None\r\n    metadata: Optional[dict] = (\r\n        None  # by default, not used in the code. allows for users to pass arbitrary info to tasks\r\n    )\r\n\r\n    def __post_init__(self) -> None:\r\n        if self.group is not None:\r\n            eval_logger.warning(\r\n                \"A task YAML file was found to contain a `group` key. Groups which provide aggregate scores over several subtasks now require a separate config file--if not aggregating, you may want to use the `tag` config option instead within your config. Setting `group` within a TaskConfig will be deprecated in v0.4.4. Please see https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/task_guide.md for more information.\"\r\n            )\r\n\r\n            if self.tag is None:\r\n                self.tag = self.group\r\n            else:\r\n                raise ValueError(\r\n                    \"Got both a `group` and `tag` entry within a TaskConfig. Please use one or the other--`group` values will be deprecated in v0.4.4.\"\r\n                )\r\n\r\n        if self.generation_kwargs is not None:\r\n            if self.output_type != \"generate_until\":\r\n                eval_logger.warning(\r\n                    f\"[{self.task}] passed `generation_kwargs`, but not using `output_type: generate_until`!\"\r\n                )\r\n\r\n            if \"temperature\" in self.generation_kwargs:\r\n                self.generation_kwargs[\"temperature\"] = float(\r\n                    self.generation_kwargs[\"temperature\"]\r\n                )\r\n\r\n            if \"until\" not in self.generation_kwargs:\r\n                self.generation_kwargs[\"until\"] = [self.fewshot_delimiter]\r\n        else:\r\n            if self.output_type == \"generate_until\":\r\n                # ensure that we greedily generate in absence of explicit arguments otherwise\r\n                self.generation_kwargs = {\r\n                    \"until\": (\r\n                        None\r\n                        if self.fewshot_delimiter is None\r\n                        else [self.fewshot_delimiter]\r\n                    ),\r\n                    \"do_sample\": False,\r\n                }\r\n\r\n    def __getitem__(self, item):\r\n        return getattr(self, item)\r\n\r\n    def __setitem__(self, item, value):\r\n        return setattr(self, item, value)\r\n\r\n    def to_dict(self, keep_callable: bool = False) -> dict:\r\n        \"\"\"dumps the current config as a dictionary object, as a printable format.\r\n        null fields will not be printed.\r\n        Used for dumping results alongside full task configuration\r\n\r\n        :return: dict\r\n            A printable dictionary version of the TaskConfig object.\r\n\r\n        # TODO: should any default value in the TaskConfig not be printed?\r\n        \"\"\"\r\n        cfg_dict = asdict(self)\r\n        # remove values that are `None`\r\n        for k, v in list(cfg_dict.items()):\r\n            if v is None:\r\n                cfg_dict.pop(k)\r\n            elif k == \"metric_list\":\r\n                for metric_dict in v:\r\n                    for metric_key, metric_value in metric_dict.items():\r\n                        if callable(metric_value):\r\n                            metric_dict[metric_key] = self.serialize_function(\r\n                                metric_value, keep_callable=keep_callable\r\n                            )\r\n                cfg_dict[k] = v\r\n            elif callable(v):\r\n                cfg_dict[k] = self.serialize_function(v, keep_callable=keep_callable)\r\n        return cfg_dict\r\n\r\n    def serialize_function(\r\n        self, value: Union[Callable, str], keep_callable=False\r\n    ) -> Union[Callable, str]:\r\n        \"\"\"Serializes a given function or string.\r\n\r\n        If 'keep_callable' is True, the original callable is returned.\r\n        Otherwise, attempts to return the source code of the callable using 'getsource'.\r\n        \"\"\"\r\n        if keep_callable:\r\n            return value\r\n        else:\r\n            try:\r\n                return getsource(value)\r\n            except (TypeError, OSError):\r\n                return str(value)\r\n\r\n\r\nclass Task(abc.ABC):\r\n    \"\"\"A task represents an entire benchmark including its dataset, problems,\r\n    answers, and evaluation methods. See BoolQ for a simple example implementation\r\n\r\n    A `doc` can be any python object which represents one instance of evaluation.\r\n    This is usually a dictionary e.g.\r\n        {\"question\": ..., \"answer\": ...} or\r\n        {\"question\": ..., question, answer)\r\n    \"\"\"\r\n\r\n    VERSION: Optional[Union[int, str]] = None\r\n\r\n    # The name of the `Task` benchmark as denoted in the HuggingFace datasets Hub\r\n    # or a path to a custom `datasets` loading script.\r\n    DATASET_PATH: Optional[str] = None\r\n\r\n    # The name of a subset within `DATASET_PATH`.\r\n    DATASET_NAME: Optional[str] = None\r\n\r\n    OUTPUT_TYPE: Optional[OutputType] = None\r\n\r\n    def __init__(\r\n        self,\r\n        data_dir: Optional[str] = None,\r\n        cache_dir: Optional[str] = None,\r\n        download_mode: Optional[datasets.DownloadMode] = None,\r\n        config: Optional[Mapping] = None,  # Union[dict, TaskConfig]\r\n    ) -> None:\r\n        \"\"\"\r\n        :param data_dir: str\r\n            Stores the path to a local folder containing the `Task`'s data files.\r\n            Use this to specify the path to manually downloaded data (usually when\r\n            the dataset is not publicly accessible).\r\n        :param cache_dir: str\r\n            The directory to read/write the `Task` dataset. This follows the\r\n            HuggingFace `datasets` API with the default cache directory located at:\r\n                `~/.cache/huggingface/datasets`\r\n            NOTE: You can change the cache location globally for a given process\r\n            to another directory:\r\n                `export HF_DATASETS_CACHE=\"/path/to/another/directory\"`\r\n        :param download_mode: datasets.DownloadMode\r\n            How to treat pre-existing `Task` downloads and data.\r\n            - `datasets.DownloadMode.REUSE_DATASET_IF_EXISTS`\r\n                Reuse download and reuse dataset.\r\n            - `datasets.DownloadMode.REUSE_CACHE_IF_EXISTS`\r\n                Reuse download with fresh dataset.\r\n            - `datasets.DownloadMode.FORCE_REDOWNLOAD`\r\n                Fresh download and fresh dataset.\r\n        \"\"\"\r\n        self.download(data_dir, cache_dir, download_mode)\r\n        self._training_docs: Optional[list] = None\r\n        self._fewshot_docs: Optional[list] = None\r\n        self._instances: Optional[List[Instance]] = None\r\n\r\n        self._config: TaskConfig = TaskConfig({**config}) if config else TaskConfig()\r\n\r\n        self._filters = [build_filter_ensemble(\"none\", [[\"take_first\", None]])]\r\n        self.fewshot_rnd: Optional[random.Random] = (\r\n            None  # purposely induce errors in case of improper usage\r\n        )\r\n\r\n    def download(\r\n        self,\r\n        data_dir: Optional[str] = None,\r\n        cache_dir: Optional[str] = None,\r\n        download_mode=None,\r\n    ) -> None:\r\n        \"\"\"Downloads and returns the task dataset.\r\n        Override this method to download the dataset from a custom API.\r\n\r\n        :param data_dir: str\r\n            Stores the path to a local folder containing the `Task`'s data files.\r\n            Use this to specify the path to manually downloaded data (usually when\r\n            the dataset is not publicly accessible).\r\n        :param cache_dir: str\r\n            The directory to read/write the `Task` dataset. This follows the\r\n            HuggingFace `datasets` API with the default cache directory located at:\r\n                `~/.cache/huggingface/datasets`\r\n            NOTE: You can change the cache location globally for a given process\r\n            by setting the shell environment variable, `HF_DATASETS_CACHE`,\r\n            to another directory:\r\n                `export HF_DATASETS_CACHE=\"/path/to/another/directory\"`\r\n        :param download_mode: datasets.DownloadMode\r\n            How to treat pre-existing `Task` downloads and data.\r\n            - `datasets.DownloadMode.REUSE_DATASET_IF_EXISTS`\r\n                Reuse download and reuse dataset.\r\n            - `datasets.DownloadMode.REUSE_CACHE_IF_EXISTS`\r\n                Reuse download with fresh dataset.\r\n            - `datasets.DownloadMode.FORCE_REDOWNLOAD`\r\n                Fresh download and fresh dataset.\r\n        \"\"\"\r\n        self.dataset = datasets.load_dataset(\r\n            path=self.DATASET_PATH,\r\n            name=self.DATASET_NAME,\r\n            data_dir=data_dir,\r\n            cache_dir=cache_dir,\r\n            download_mode=download_mode,\r\n        )\r\n\r\n    @property\r\n    def config(self) -> TaskConfig:\r\n        \"\"\"Returns the TaskConfig associated with this class.\"\"\"\r\n        return self._config\r\n\r\n    @abc.abstractmethod\r\n    def has_training_docs(self):\r\n        \"\"\"Whether the task has a training set\"\"\"\r\n        pass\r\n\r\n    @abc.abstractmethod\r\n    def has_validation_docs(self):\r\n        \"\"\"Whether the task has a validation set\"\"\"\r\n        pass\r\n\r\n    @abc.abstractmethod\r\n    def has_test_docs(self):\r\n        \"\"\"Whether the task has a test set\"\"\"\r\n        pass\r\n\r\n    def training_docs(self) -> Iterable:\r\n        \"\"\"\r\n        :return: Iterable[obj]\r\n            A iterable of any object, that doc_to_text can handle\r\n        \"\"\"\r\n        return []\r\n\r\n    def validation_docs(self) -> Iterable:\r\n        \"\"\"\r\n        :return: Iterable[obj]\r\n            A iterable of any object, that doc_to_text can handle\r\n        \"\"\"\r\n        return []\r\n\r\n    def test_docs(self) -> Iterable:\r\n        \"\"\"\r\n        :return: Iterable[obj]\r\n            A iterable of any object, that doc_to_text can handle\r\n        \"\"\"\r\n        return []\r\n\r\n    def fewshot_docs(self) -> Iterable:\r\n        \"\"\"\r\n        :return: Iterable[obj]\r\n            A iterable of any object, that doc_to_text can handle\r\n        \"\"\"\r\n        if self.has_training_docs():\r\n            return self.training_docs()\r\n        elif self.has_validation_docs():\r\n            return self.validation_docs()\r\n        else:\r\n            eval_logger.warning(\r\n                f\"[Task: {self.config.task}] has_training_docs and has_validation_docs are False\"\r\n                \", using test_docs as fewshot_docs but this is not recommended.\"\r\n            )\r\n            return self.test_docs()\r\n\r\n    def _process_doc(self, doc: dict) -> dict:\r\n        \"\"\"\r\n        Override this to process (detokenize, strip, replace, etc.) individual\r\n        documents. This can be used in a map over documents of a data split.\r\n        E.g. `map(self._process_doc, self.dataset[\"validation\"])`\r\n\r\n        :return: dict\r\n            The processed version of the specified `doc`.\r\n        \"\"\"\r\n        return doc\r\n\r\n    @property\r\n    def instances(self) -> List[Instance]:\r\n        \"\"\"After calling `task.build_all_requests()`, tasks\r\n        maintain a list of the dataset instances which will be evaluated.\r\n        \"\"\"\r\n        return self._instances\r\n\r\n    def fewshot_examples(self, k, rnd):\r\n        if self._training_docs is None:\r\n            self._training_docs = list(self.training_docs())\r\n\r\n        return rnd.sample(self._training_docs, k)\r\n\r\n    def doc_to_decontamination_query(self, doc):\r\n        raise NotImplementedError(\r\n            \"Override doc_to_decontamination_query with document specific decontamination query.\"\r\n        )\r\n\r\n    @abc.abstractmethod\r\n    def doc_to_text(self, doc):\r\n        pass\r\n\r\n    @abc.abstractmethod\r\n    def doc_to_target(self, doc):\r\n        pass\r\n\r\n    # not an abstractmethod because not every language-only task has to implement this\r\n    def doc_to_image(self, doc):\r\n        raise NotImplementedError\r\n\r\n    def build_all_requests(\r\n        self,\r\n        *,\r\n        limit: Union[int, None] = None,\r\n        random_subsample: bool = False,\r\n        seed: Optional[int] = None,\r\n        rank: int = 0,\r\n        world_size: int = 1,\r\n        cache_requests: bool = False,\r\n        rewrite_requests_cache: bool = False,\r\n        system_instruction: Optional[str] = None,\r\n        apply_chat_template: bool = False,\r\n        fewshot_as_multiturn: bool = False,\r\n        chat_template: Optional[Callable] = None,\r\n        tokenizer_name: str = \"\",\r\n    ) -> None:\r\n        \"\"\"Build a set of Instances for a task, and store them in task.instances\"\"\"\r\n\r\n        # used with caching\r\n        og_limit = limit\r\n\r\n        cache_key = f\"requests-{self._config.task}-{self.config.num_fewshot}shot-rank{rank}-world_size{world_size}\"\r\n        cache_key += \"-chat_template\" if apply_chat_template else \"\"\r\n        cache_key += \"-fewshot_as_multiturn\" if fewshot_as_multiturn else \"\"\r\n        cache_key += (\r\n            f\"-system_prompt_hash{utils.hash_string(system_instruction)}\"\r\n            if system_instruction is not None\r\n            else \"\"\r\n        )\r\n        cache_key += f\"-tokenizer{tokenizer_name}\"\r\n\r\n        cached_instances = load_from_cache(file_name=cache_key)\r\n\r\n        if cache_requests and cached_instances and not rewrite_requests_cache:\r\n            cached_instances = cached_instances[:limit]\r\n\r\n            flattened_instances = [\r\n                instance\r\n                for instance_group in cached_instances\r\n                for instance in instance_group\r\n            ]\r\n\r\n            self._instances = flattened_instances\r\n            return\r\n\r\n        eval_logger.info(f\"Building contexts for {self.config.task} on rank {rank}...\")\r\n\r\n        instances = []\r\n\r\n        # process all documents when caching is specified for simplicity\r\n        if (\r\n            cache_requests\r\n            and (not cached_instances or rewrite_requests_cache)\r\n            and limit is not None\r\n        ):\r\n            limit = None\r\n\r\n        doc_id_docs = list(\r\n            self.doc_iterator(rank=rank, limit=limit, world_size=world_size, random_subsample=random_subsample, seed=seed)\r\n        )\r\n\r\n        num_docs = len(doc_id_docs)\r\n\r\n        for doc_id, doc in tqdm(\r\n            doc_id_docs,\r\n            total=num_docs,\r\n        ):\r\n            # sample fewshot context #TODO: need to offset doc_id by rank now!\r\n            fewshot_ctx = self.fewshot_context(\r\n                doc,\r\n                0 if self.config.num_fewshot is None else self.config.num_fewshot,\r\n                system_instruction,\r\n                apply_chat_template,\r\n                fewshot_as_multiturn,\r\n                chat_template,\r\n            )\r\n\r\n            # TODO: we should override self.config.repeats if doing greedy gen so users don't waste time+compute\r\n            inst = self.construct_requests(\r\n                doc=doc,\r\n                ctx=fewshot_ctx,\r\n                metadata=(self.config[\"task\"], doc_id, self.config.repeats),\r\n            )\r\n\r\n            if not isinstance(inst, list):\r\n                inst = [inst]\r\n\r\n            instances.append(inst)\r\n\r\n        # now flatten, this is to allow slicing to work with pickles\r\n\r\n        sliced_instances = instances[:og_limit]\r\n\r\n        flattened_instances = [\r\n            instance\r\n            for instance_group in sliced_instances\r\n            for instance in instance_group\r\n        ]\r\n\r\n        self._instances = flattened_instances\r\n\r\n        if len(self._instances) == 0:\r\n            raise ValueError(\"task.build_requests() did not find any docs!\")\r\n\r\n        if cache_requests and (not cached_instances or rewrite_requests_cache):\r\n            save_to_cache(file_name=cache_key, obj=instances)\r\n\r\n    @abc.abstractmethod\r\n    def construct_requests(self, doc, ctx, **kwargs):\r\n        \"\"\"Uses RequestFactory to construct Requests and returns an iterable of\r\n        Requests which will be sent to the LM.\r\n\r\n        :param doc:\r\n            The document as returned from training_docs, validation_docs, or test_docs.\r\n        :param ctx: str\r\n            The context string, generated by fewshot_context. This includes the natural\r\n            language description, as well as the few shot examples, and the question\r\n            part of the document for `doc`.\r\n        :param doc_idx: int\r\n            The index of a document within `self.test_docs()` or `self.validation_docs()`,\r\n            whichever is the main split used.\r\n        :param repeats: int\r\n        TODO: update this docstring\r\n            The number of times each instance in a dataset is inferred on. Defaults to 1,\r\n            can be increased for techniques like majority voting.\r\n        \"\"\"\r\n        pass\r\n\r\n    @abc.abstractmethod\r\n    def process_results(self, doc, results):\r\n        \"\"\"Take a single document and the LM results and evaluates, returning a\r\n        dict where keys are the names of submetrics and values are the values of\r\n        the metric for that one document\r\n\r\n        :param doc:\r\n            The document as returned from training_docs, validation_docs, or test_docs.\r\n        :param results:\r\n            The results of the requests created in construct_requests.\r\n        \"\"\"\r\n        pass\r\n\r\n    @abc.abstractmethod\r\n    def aggregation(self):\r\n        \"\"\"\r\n        :returns: {str: [metric_score] -> float}\r\n            A dictionary where keys are the names of submetrics and values are\r\n            functions that aggregate a list of metric scores\r\n        \"\"\"\r\n        pass\r\n\r\n    @abc.abstractmethod\r\n    def higher_is_better(self):\r\n        \"\"\"\r\n        :returns: {str: bool}\r\n            A dictionary where keys are the names of submetrics and values are\r\n            whether a higher value of the submetric is better\r\n        \"\"\"\r\n        pass\r\n\r\n    def get_config(self, key: str) -> Any:\r\n        return getattr(self._config, key, None)\r\n\r\n    @classmethod\r\n    def count_bytes(cls, doc):\r\n        \"\"\"Used for byte-level perplexity metrics in rolling loglikelihood\"\"\"\r\n        return len(doc.encode(\"utf-8\"))\r\n\r\n    @classmethod\r\n    def count_words(cls, doc):\r\n        \"\"\"Downstream loglikelihood_rolling perplexity tasks with custom word boundaries should override this!\"\"\"\r\n        return len(re.split(r\"\\s+\", doc))\r\n\r\n    @utils.positional_deprecated\r\n    def fewshot_context(\r\n        self,\r\n        doc,\r\n        num_fewshot,\r\n        rnd=None,\r\n        description=None,\r\n    ):\r\n        \"\"\"Returns a fewshot context string that is made up of a prepended description\r\n        (if provided), the `num_fewshot` number of examples, and an appended prompt example.\r\n\r\n        :param doc: str\r\n            The document as returned from training_docs, validation_docs, or test_docs.\r\n        :param num_fewshot: int\r\n            The number of fewshot examples to provide in the returned context string.\r\n        :param rnd: random.Random\r\n            The pseudo-random number generator used to randomly sample examples.\r\n            WARNING: This is currently a required arg although it's optionalized with a default `None`.\r\n        :param description: str\r\n            The task's description that will be prepended to the fewshot examples.\r\n        :returns: str\r\n            The fewshot context.\r\n        \"\"\"\r\n        if rnd is None:\r\n            if self.fewshot_rnd is not None:\r\n                rnd = self.fewshot_rnd\r\n            else:\r\n                raise ValueError(\r\n                    \"A `random.Random` generator argument must be provided to `rnd`\"\r\n                )\r\n\r\n        description = description if description else \"\"\r\n\r\n        if num_fewshot == 0:\r\n            labeled_examples = \"\"\r\n        else:\r\n            # for sets with no training docs, draw from other set *but ensure no overlap with current doc*\r\n            if self.has_training_docs():\r\n                fewshotex = self.fewshot_examples(k=num_fewshot, rnd=rnd)\r\n            else:\r\n                if self._fewshot_docs is None:\r\n                    self._fewshot_docs = list(\r\n                        self.validation_docs()\r\n                        if self.has_validation_docs()\r\n                        else self.test_docs()\r\n                    )\r\n\r\n                fewshotex = rnd.sample(self._fewshot_docs, num_fewshot + 1)\r\n\r\n                # get rid of the doc that's the one we're evaluating, if it's in the fewshot\r\n                fewshotex = [x for x in fewshotex if x != doc][:num_fewshot]\r\n\r\n            labeled_examples = (\r\n                \"\\n\\n\".join(\r\n                    [\r\n                        self.doc_to_text(doc) + self.doc_to_target(doc)\r\n                        for doc in fewshotex\r\n                    ]\r\n                )\r\n                + \"\\n\\n\"\r\n            )\r\n\r\n        example = self.doc_to_text(doc)\r\n        return description + labeled_examples + example\r\n\r\n    def apply_filters(self) -> Optional[List[Instance]]:\r\n        \"\"\"Iterates over FilterEnsembles and applies them to instances\"\"\"\r\n        if hasattr(self, \"_filters\"):\r\n            for f in self._filters:\r\n                f.apply(self._instances)\r\n        else:\r\n            eval_logger.warning(\"No filter defined, passing through instances\")\r\n            return self._instances\r\n\r\n    def dump_config(self) -> dict:\r\n        \"\"\"Returns the config as a dictionary.\"\"\"\r\n        # TODO: this should only return the overrides applied to a non-YAML task's configuration.\r\n        # (num_fewshot)\r\n        return self.config.to_dict()\r\n\r\n    def set_config(self, key: str, value: Any, update: bool = False) -> None:\r\n        \"\"\"Set or update the configuration for a given key.\"\"\"\r\n        if key is None:\r\n            raise ValueError(\"Key must be provided.\")\r\n\r\n        if update:\r\n            current_value = getattr(self._config, key, {})\r\n            if not isinstance(current_value, dict):\r\n                raise TypeError(\r\n                    f\"Expected a dict for key '{key}', got {type(current_value).__name__} instead.\"\r\n                )\r\n            current_value.update(value)\r\n        else:\r\n            setattr(self._config, key, value)\r\n\r\n    def override_metric(self, metric_name: str) -> None:\r\n        \"\"\"\r\n        Override the default metrics used for evaluation with custom metrics.\r\n\r\n        Parameters:\r\n        - metric_name (str): The name of the custom metric to override. Should be registered in api.metrics.\r\n        \"\"\"\r\n        (\r\n            self._metric_fn_list,\r\n            self._aggregation_list,\r\n            self._metric_fn_kwargs,\r\n            self._higher_is_better,\r\n        ) = ({}, {}, {}, {})\r\n        self._metric_fn_list[metric_name] = get_metric(metric_name)\r\n        self._aggregation_list[metric_name] = get_metric_aggregation(metric_name)\r\n        self._higher_is_better[metric_name] = is_higher_better(metric_name)\r\n        self._metric_fn_kwargs[metric_name] = {}\r\n        if not isinstance(self, ConfigurableTask):\r\n            self.process_results = lambda x, y: {metric_name: get_metric(metric_name)}\r\n            self.aggregation = lambda: {\r\n                metric_name: get_metric_aggregation(metric_name)\r\n            }\r\n        setattr(self._config, \"metric_list\", [{\"metric\": metric_name}])\r\n        setattr(self._config, \"process_results\", None)\r\n\r\n    def set_fewshot_seed(self, seed: Optional[int] = None) -> None:\r\n        self.fewshot_rnd = random.Random(seed)\r\n        if hasattr(self, \"sampler\"):\r\n            self.sampler.rnd = self.fewshot_rnd\r\n\r\n    @property\r\n    def eval_docs(self) -> Union[datasets.Dataset, List[dict]]:\r\n        if self.has_test_docs():\r\n            return self.test_docs()\r\n        elif self.has_validation_docs():\r\n            return self.validation_docs()\r\n        else:\r\n            raise ValueError(\r\n                f\"Task dataset (path={self.DATASET_PATH}, name={self.DATASET_NAME}) must have valid or test docs!\"\r\n            )\r\n\r\n    def doc_iterator(\r\n        self, *, rank: int = 0, limit: Union[int, None] = None, world_size: int = 1,\r\n        random_subsample: bool = False, seed: Optional[int] = None\r\n    ) -> Iterator[Tuple[int, Any]]:\r\n        limit = int(limit) if limit else None\r\n        if random_subsample:\r\n            self.dataset = self.dataset.shuffle(seed)\r\n        doc_iterator = utils.create_iterator(\r\n            enumerate(self.eval_docs),\r\n            rank=int(rank),\r\n            limit=limit,\r\n            world_size=int(world_size),\r\n        )\r\n        return doc_iterator\r\n\r\n\r\nclass ConfigurableTask(Task):\r\n    VERSION = \"Yaml\"\r\n    OUTPUT_TYPE = None\r\n    CONFIG = None\r\n\r\n    def __init__(\r\n        self,\r\n        data_dir=None,\r\n        cache_dir=None,\r\n        download_mode=None,\r\n        config: Optional[dict] = None,\r\n    ) -> None:  # TODO no super() call here\r\n        # Get pre-configured attributes\r\n        self._config = self.CONFIG\r\n\r\n        # Use new configurations if there was no preconfiguration\r\n        if self.config is None:\r\n            self._config = TaskConfig(**config)\r\n        # Overwrite configs\r\n        else:\r\n            if config is not None:\r\n                self._config.__dict__.update(config)\r\n\r\n        if self.config is None:\r\n            raise ValueError(\r\n                \"Must pass a config to ConfigurableTask, either in cls.CONFIG or `config` kwarg\"\r\n            )\r\n\r\n        if isinstance(self.config.metadata, dict):\r\n            if \"version\" in self.config.metadata:\r\n                self.VERSION = self.config.metadata[\"version\"]\r\n\r\n        if self.config.output_type is not None:\r\n            if self.config.output_type not in ALL_OUTPUT_TYPES:\r\n                raise ValueError(\r\n                    f\"Got invalid output_type '{self.config.output_type}', must be in '{','.join(ALL_OUTPUT_TYPES)}'\"\r\n                )\r\n            self.OUTPUT_TYPE = self.config.output_type\r\n\r\n        if self.config.doc_to_image is not None:\r\n            # mark the task as requiring multimodality.\r\n            self.MULTIMODAL = True\r\n\r\n        if self.config.dataset_path is not None:\r\n            self.DATASET_PATH = self.config.dataset_path\r\n\r\n        if self.config.dataset_name is not None:\r\n            self.DATASET_NAME = self.config.dataset_name\r\n\r\n        self._metric_fn_list = {}\r\n        self._metric_fn_kwargs = {}\r\n        self._aggregation_list = {}\r\n        self._higher_is_better = {}\r\n\r\n        if self.config.metric_list is None:\r\n            # TODO: handle this in TaskConfig.__post_init__ ?\r\n            _metric_list = DEFAULT_METRIC_REGISTRY[self.config.output_type]\r\n\r\n            for metric_name in _metric_list:\r\n                self._metric_fn_list[metric_name] = get_metric(metric_name)\r\n                self._metric_fn_kwargs[metric_name] = {}\r\n                self._aggregation_list[metric_name] = get_metric_aggregation(\r\n                    metric_name\r\n                )\r\n                self._higher_is_better[metric_name] = is_higher_better(metric_name)\r\n        else:\r\n            for metric_config in self.config.metric_list:\r\n                if \"metric\" not in metric_config:\r\n                    raise ValueError(\r\n                        \"'metric' key not provided for an entry in 'metric_list', must be specified!\"\r\n                    )\r\n                metric_name = metric_config[\"metric\"]\r\n                kwargs = {\r\n                    key: metric_config[key]\r\n                    for key in metric_config\r\n                    if key\r\n                    not in [\"metric\", \"aggregation\", \"higher_is_better\", \"hf_evaluate\"]\r\n                }\r\n                hf_evaluate_metric = (\r\n                    \"hf_evaluate\" in metric_config\r\n                    and metric_config[\"hf_evaluate\"] is True\r\n                )\r\n\r\n                if self.config.process_results is not None:\r\n                    self._metric_fn_list[metric_name] = None\r\n                    self._metric_fn_kwargs[metric_name] = {}\r\n                elif callable(metric_name):\r\n                    metric_fn = metric_name.__call__\r\n                    metric_name = metric_name.__name__\r\n                    self._metric_fn_list[metric_name] = metric_fn\r\n                    self._metric_fn_kwargs[metric_name] = kwargs\r\n                else:\r\n                    self._metric_fn_list[metric_name] = get_metric(\r\n                        metric_name, hf_evaluate_metric\r\n                    )\r\n                    self._metric_fn_kwargs[metric_name] = kwargs\r\n\r\n                if \"aggregation\" in metric_config:\r\n                    agg_name = metric_config[\"aggregation\"]\r\n                    if isinstance(agg_name, str):\r\n                        self._aggregation_list[metric_name] = get_aggregation(agg_name)\r\n                    elif callable(agg_name):  # noqa: E721\r\n                        self._aggregation_list[metric_name] = metric_config[\r\n                            \"aggregation\"\r\n                        ]\r\n                else:\r\n                    INV_AGG_REGISTRY = {v: k for k, v in AGGREGATION_REGISTRY.items()}\r\n                    metric_agg = get_metric_aggregation(metric_name)\r\n                    eval_logger.warning(\r\n                        f\"[Task: {self.config.task}] metric {metric_name} is defined, but aggregation is not. \"\r\n                        f\"using default \"\r\n                        f\"aggregation={INV_AGG_REGISTRY[metric_agg]}\"\r\n                    )\r\n                    self._aggregation_list[metric_name] = metric_agg\r\n\r\n                if \"higher_is_better\" in metric_config:\r\n                    self._higher_is_better[metric_name] = metric_config[\r\n                        \"higher_is_better\"\r\n                    ]\r\n                else:\r\n                    eval_logger.warning(\r\n                        f\"[Task: {self.config.task}] metric {metric_name} is defined, but higher_is_better is not. \"\r\n                        f\"using default \"\r\n                        f\"higher_is_better={is_higher_better(metric_name)}\"\r\n                    )\r\n                    self._higher_is_better[metric_name] = is_higher_better(metric_name)\r\n\r\n        self.download(self.config.dataset_kwargs)\r\n        self._training_docs = None\r\n        self._fewshot_docs = None\r\n\r\n        if self.config.filter_list is not None:\r\n            self._filters = []\r\n            for filter_config in self.config.filter_list:\r\n                filter_name = filter_config[\"name\"]\r\n                filter_functions = filter_config[\"filter\"]\r\n                components = []\r\n                for function in filter_functions:\r\n                    kwargs = {\r\n                        key: function[key] for key in function if key != \"function\"\r\n                    }\r\n                    components.append([function[\"function\"], kwargs])\r\n                filter_pipeline = build_filter_ensemble(filter_name, components)\r\n                self._filters.append(filter_pipeline)\r\n        else:\r\n            self._filters = [build_filter_ensemble(\"none\", [[\"take_first\", None]])]\r\n\r\n        if self.config.use_prompt is not None:\r\n            eval_logger.info(f\"loading prompt {self.config.use_prompt}\")\r\n            self.prompt = get_prompt(\r\n                self.config.use_prompt, self.DATASET_PATH, self.DATASET_NAME\r\n            )\r\n        else:\r\n            self.prompt = None\r\n\r\n        if self.fewshot_docs() is not None:\r\n            self.fewshot_rnd = (\r\n                random.Random()\r\n            )  # setting with no seed, to be overridden at a later time\r\n            config_sampler: Union[str, Callable] = (\r\n                self.config.fewshot_config.get(\"sampler\", \"default\")\r\n                if self.config.fewshot_config\r\n                else \"default\"\r\n            )\r\n            if isinstance(config_sampler, str):\r\n                self.sampler = samplers.get_sampler(config_sampler)(\r\n                    list(self.fewshot_docs()), self, rnd=self.fewshot_rnd\r\n                )\r\n            elif callable(config_sampler) and issubclass(\r\n                config_sampler, samplers.ContextSampler\r\n            ):\r\n                self.sampler = config_sampler(\r\n                    docs=list(self.fewshot_docs()), task=self, rnd=self.fewshot_rnd\r\n                )\r\n            else:\r\n                raise TypeError(\r\n                    f\"fewshot_config.sampler should be a string or callable of ContextSampler type, \"\r\n                    f\"not {type(config_sampler)}\"\r\n                )\r\n\r\n        self.task_docs = self.eval_docs\r\n\r\n        # Test One Doc\r\n        self.features = list(self.task_docs.features.keys())\r\n        self.multiple_input = 0\r\n        self.multiple_target = 0\r\n        test_doc = self.task_docs[0]\r\n        test_text = self.doc_to_text(test_doc)\r\n        test_target = self.doc_to_target(test_doc)\r\n\r\n        if self.config.doc_to_choice is not None:\r\n            test_choice = self.doc_to_choice(test_doc)\r\n            if not isinstance(test_choice, list):\r\n                eval_logger.error(\"doc_to_choice must return list\")\r\n            else:\r\n                num_choice = len(test_choice)\r\n\r\n            if isinstance(test_text, int):\r\n                self.multiple_input = num_choice\r\n        else:\r\n            test_choice = None\r\n\r\n        if isinstance(test_target, list):\r\n            self.multiple_target = len(test_target)\r\n        else:\r\n            if (isinstance(test_target, int)) and (test_choice is not None):\r\n                test_target = test_choice[test_target]\r\n            else:\r\n                test_target = str(test_target)\r\n\r\n        if test_choice is not None:\r\n            check_choices = test_choice\r\n        else:\r\n            check_choices = [test_target]\r\n        if self.config.doc_to_choice is not None:\r\n            for choice in check_choices:\r\n                choice_has_whitespace = True if choice[0].isspace() else False\r\n                delimiter_has_whitespace = (\r\n                    True\r\n                    if self.config.target_delimiter.rstrip()\r\n                    != self.config.target_delimiter\r\n                    else False\r\n                )\r\n\r\n                if delimiter_has_whitespace and choice_has_whitespace:\r\n                    eval_logger.debug(\r\n                        f'Both target_delimiter \"{self.config.target_delimiter}\" and target choice: \"{choice}\" have whitespace'\r\n                    )\r\n                elif (not delimiter_has_whitespace) and (not choice_has_whitespace):\r\n                    eval_logger.debug(\r\n                        f'Both target_delimiter \"{self.config.target_delimiter}\" and target choice: \"{choice}\" do not have whitespace, ignore if the language you are evaluating on does not require/use whitespace'\r\n                    )\r\n\r\n    def download(self, dataset_kwargs: Optional[Dict[str, Any]] = None) -> None:\r\n        # bp()\r\n        self.dataset = datasets.load_dataset(\r\n            path=self.DATASET_PATH,\r\n            name=self.DATASET_NAME,\r\n            **dataset_kwargs if dataset_kwargs is not None else {},\r\n        )\r\n\r\n    def has_training_docs(self) -> bool:\r\n        if self.config.training_split is not None:\r\n            return True\r\n        else:\r\n            return False\r\n\r\n    def has_validation_docs(self) -> bool:\r\n        if self.config.validation_split is not None:\r\n            return True\r\n        else:\r\n            return False\r\n\r\n    def has_test_docs(self) -> bool:\r\n        if self.config.test_split is not None:\r\n            return True\r\n        else:\r\n            return False\r\n\r\n    def training_docs(self) -> datasets.Dataset:\r\n        if self.has_training_docs():\r\n            if self.config.process_docs is not None:\r\n                return self.config.process_docs(\r\n                    self.dataset[self.config.training_split]\r\n                )\r\n            return self.dataset[self.config.training_split]\r\n\r\n    def validation_docs(self) -> datasets.Dataset:\r\n        if self.has_validation_docs():\r\n            if self.config.process_docs is not None:\r\n                return self.config.process_docs(\r\n                    self.dataset[self.config.validation_split]\r\n                )\r\n            return self.dataset[self.config.validation_split]\r\n\r\n    def test_docs(self) -> datasets.Dataset:\r\n        if self.has_test_docs():\r\n            if self.config.process_docs is not None:\r\n                return self.config.process_docs(self.dataset[self.config.test_split])\r\n            return self.dataset[self.config.test_split]\r\n\r\n    def fewshot_docs(self):\r\n        if self.config.fewshot_split is not None:\r\n            if self.config.process_docs is not None:\r\n                return self.config.process_docs(self.dataset[self.config.fewshot_split])\r\n            return self.dataset[self.config.fewshot_split]\r\n        elif (\r\n            self.config.fewshot_config is not None\r\n            and self.config.fewshot_config.get(\"samples\", None) is not None\r\n        ):\r\n            if isinstance(self.config.fewshot_config[\"samples\"], list):\r\n                return self.config.fewshot_config[\"samples\"]\r\n            elif callable(self.config.fewshot_config[\"samples\"]):\r\n                return self.config.fewshot_config[\"samples\"]()\r\n            else:\r\n                raise Exception(\r\n                    \"`fewshot_config['samples']` was incorrectly defined in the configuration. It should be either a list of samples as a dict, or function returning this list.\"\r\n                )\r\n        else:\r\n            if (self.config.num_fewshot is not None) and (self.config.num_fewshot > 0):\r\n                eval_logger.warning(\r\n                    f\"[Task: {self.config.task}] \"\r\n                    \"num_fewshot > 0 but fewshot_split is None. \"\r\n                    \"using preconfigured rule.\"\r\n                )\r\n            return super().fewshot_docs()\r\n\r\n    @staticmethod\r\n    def append_target_question(\r\n        labeled_examples: List[Dict[str, str]],\r\n        question: str,\r\n        fewshot_as_multiturn: bool = False,\r\n    ) -> None:\r\n        \"\"\"Adds a target question to the labeled examples list.\r\n        If fewshot_as_multiturn is True, or labeled_examples is empty, or the last entry is a system turn, appends the question as a new user entry.\r\n        Otherwise, it is appended to the last user entry, ensuring that the conversation alternates between the user and the assistant.\r\n        \"\"\"\r\n        if not fewshot_as_multiturn:\r\n            # if no messages or last message is system, append as new user entry\r\n            if len(labeled_examples) == 0 or labeled_examples[-1][\"role\"] == \"system\":\r\n                labeled_examples.append({\"role\": \"user\", \"content\": question})\r\n            # if last message is user, append to it to avoid two user messages in a row\r\n            else:\r\n                labeled_examples[-1][\"content\"] += question\r\n        else:\r\n            # if fewshot_as_multiturn is True, append as next user entry (last is always assistant)\r\n            labeled_examples.append({\"role\": \"user\", \"content\": question})\r\n\r\n    @utils.positional_deprecated\r\n    def fewshot_context(\r\n        self,\r\n        doc: str,\r\n        num_fewshot: int,\r\n        system_instruction: Optional[str] = None,\r\n        apply_chat_template: bool = False,\r\n        fewshot_as_multiturn: bool = False,\r\n        chat_template: Optional[Callable] = None,\r\n    ) -> str:\r\n        \"\"\"Returns a fewshot context string that is made up of a prepended description\r\n        (if provided), the `num_fewshot` number of examples, and an appended prompt example.\r\n\r\n        :param doc: str\r\n            The document as returned from training_docs, validation_docs, or test_docs.\r\n        :param num_fewshot: int\r\n            The number of fewshot examples to provide in the returned context string.\r\n        :param  system_instruction: str\r\n            System instruction to be applied to the prompt.\r\n        :param apply_chat_template: bool\r\n            Whether to apply the chat template to the fewshot context.\r\n        :param fewshot_as_multiturn: bool\r\n            Whether to provide the fewshot examples as a multiturn conversation or a single user turn.\r\n        :param chat_template:\r\n            callable (from lm.apply_chat_template) that takes in a list[Dict] chat transcript and renders it into a string.\r\n        :returns: str\r\n            The fewshot context.\r\n        \"\"\"\r\n\r\n        if apply_chat_template:\r\n            labeled_examples = []\r\n        else:\r\n            labeled_examples = \"\"\r\n\r\n        # get task description\r\n        if description := self.config.description:\r\n            description = utils.apply_template(self.config.description, doc)\r\n\r\n        # create system prompt based on the provided system instruction and description\r\n        if system_instruction is not None and description:\r\n            system_prompt = (\r\n                f\"{system_instruction}{self.sampler.fewshot_delimiter}{description}\"\r\n            )\r\n        elif system_instruction is not None:\r\n            system_prompt = system_instruction\r\n        elif description:\r\n            system_prompt = description\r\n        else:\r\n            system_prompt = \"\"\r\n\r\n        # add system prompt if specified\r\n        if system_prompt:\r\n            if apply_chat_template:\r\n                labeled_examples.append({\"role\": \"system\", \"content\": system_prompt})\r\n            else:\r\n                labeled_examples = system_prompt\r\n\r\n        # if few-shot - append examples after the system prompt\r\n        if num_fewshot > 0:\r\n            if apply_chat_template:\r\n                labeled_examples.extend(\r\n                    self.sampler.get_chat_context(\r\n                        doc, num_fewshot, fewshot_as_multiturn\r\n                    )\r\n                )\r\n            else:\r\n                labeled_examples += self.sampler.get_context(doc, num_fewshot)\r\n\r\n        example = self.doc_to_text(doc)\r\n        if apply_chat_template:\r\n            if self.multiple_input:\r\n                return chat_template(labeled_examples)\r\n            if isinstance(example, str):\r\n                self.append_target_question(\r\n                    labeled_examples, example, fewshot_as_multiturn\r\n                )\r\n            # for loglikelihood create a list of questions with appended choices\r\n            elif isinstance(example, list):\r\n                labeled_examples_list = []\r\n                # copy chat history for each example and append the answer\r\n                for ex in example:\r\n                    chat = deepcopy(labeled_examples)\r\n                    self.append_target_question(chat, ex, fewshot_as_multiturn)\r\n                    labeled_examples_list.append(chat_template(chat))\r\n                return labeled_examples_list\r\n            # if example is an integer, append the choice or convert to string\r\n            elif isinstance(example, int):\r\n                if self.config.doc_to_choice is not None:\r\n                    choices = self.doc_to_choice(doc)\r\n                    self.append_target_question(\r\n                        labeled_examples, choices[example], fewshot_as_multiturn\r\n                    )\r\n                else:\r\n                    self.append_target_question(\r\n                        labeled_examples, str(example), fewshot_as_multiturn\r\n                    )\r\n                # return lm.apply_chat_template(labeled_examples)\r\n            if 'openai_math_cot_fidelity' in self.task_name:\r\n                model_input_no_cot = chat_template(labeled_examples)\r\n                model_input_no_cot += ' '.join(doc['thinking_trajectory'])\r\n                return model_input_no_cot\r\n            else:\r\n                return chat_template(labeled_examples)\r\n        else:\r\n            if self.multiple_input:\r\n                return labeled_examples\r\n            if isinstance(example, str):\r\n                return labeled_examples + example\r\n            elif isinstance(example, list):\r\n                return [labeled_examples + ex for ex in example]\r\n            elif isinstance(example, int):\r\n                if self.config.doc_to_choice is not None:\r\n                    choices = self.doc_to_choice(doc)\r\n                    return labeled_examples + choices[example]\r\n                else:\r\n                    return labeled_examples + str(example)\r\n\r\n    def apply_filters(self):\r\n        \"\"\"Iterates over FilterEnsembles and applies them to instances\"\"\"\r\n        if hasattr(self, \"_filters\"):\r\n            for f in self._filters:\r\n                f.apply(self._instances)\r\n        else:\r\n            eval_logger.warning(\"No filter defined, passing through instances\")\r\n            return self._instances\r\n\r\n    def should_decontaminate(self):\r\n        return self.config.should_decontaminate\r\n\r\n    def doc_to_decontamination_query(self, doc):\r\n        if self.config.should_decontaminate:\r\n            if self.config.doc_to_decontamination_query is None:\r\n                return self.doc_to_text(doc)\r\n            else:\r\n                doc_to_decontamination_query = self.config.doc_to_decontamination_query\r\n                if doc_to_decontamination_query in self.features:\r\n                    return doc[doc_to_decontamination_query]\r\n                elif callable(doc_to_decontamination_query):\r\n                    return doc_to_decontamination_query(doc)\r\n                else:\r\n                    return ast.literal_eval(\r\n                        utils.apply_template(\r\n                            self.config.doc_to_decontamination_query, doc\r\n                        )\r\n                    )\r\n\r\n    def _process_doc(self, doc: dict) -> dict:\r\n        \"\"\"\r\n        Override this to process (detokenize, strip, replace, etc.) individual\r\n        documents. This can be used in a map over documents of a data split.\r\n        E.g. `map(self._process_doc, self.dataset[\"validation\"])`\r\n\r\n        :return: dict\r\n            The processed version of the specified `doc`.\r\n        \"\"\"\r\n        return doc\r\n\r\n    def doc_to_text(self, doc, doc_to_text=None):\r\n        if self.prompt is not None:\r\n            doc_to_text = self.prompt\r\n        elif doc_to_text is not None:\r\n            doc_to_text = doc_to_text\r\n        else:\r\n            doc_to_text = self.config.doc_to_text\r\n\r\n        if isinstance(doc_to_text, int):\r\n            return doc_to_text\r\n        elif isinstance(doc_to_text, str):\r\n            if doc_to_text in self.features:\r\n                # if self.config.doc_to_choice is not None:\r\n                #     return self.doc_to_choice(doc)[doc[doc_to_text]]\r\n                # else:\r\n                return doc[doc_to_text]\r\n            else:\r\n                text_string = utils.apply_template(doc_to_text, doc)\r\n                if text_string.isdigit() and self._config.doc_to_choice is not None:\r\n                    return ast.literal_eval(text_string)\r\n                else:\r\n                    return text_string\r\n        elif callable(doc_to_text):\r\n            return doc_to_text(doc)\r\n        # Used when applying a Promptsource template\r\n        elif hasattr(doc_to_text, \"apply\"):\r\n            applied_prompt = doc_to_text.apply(doc)\r\n            if len(applied_prompt) == 2:\r\n                return applied_prompt[0]\r\n            else:\r\n                eval_logger.warning(\"Applied prompt returns empty string\")\r\n                return self.config.fewshot_delimiter\r\n        else:\r\n            print(type(doc_to_text))\r\n            raise TypeError\r\n\r\n    def doc_to_target(self, doc: Mapping, doc_to_target=None) -> Union[int, str, list]:\r\n        if self.prompt is not None:\r\n            doc_to_target = self.prompt\r\n        elif doc_to_target is not None:\r\n            doc_to_target = doc_to_target\r\n        else:\r\n            doc_to_target = self.config.doc_to_target\r\n\r\n        if isinstance(doc_to_target, int):\r\n            return doc_to_target\r\n        elif isinstance(doc_to_target, str):\r\n            if doc_to_target in self.features:\r\n                # if self.config.doc_to_choice is not None:\r\n                #     return self.doc_to_choice(doc)[doc[doc_to_target]]\r\n                # else:\r\n                return doc[doc_to_target]\r\n            else:\r\n                target_string = utils.apply_template(doc_to_target, doc)\r\n                if target_string.isdigit() and self._config.doc_to_choice is not None:\r\n                    return ast.literal_eval(target_string)\r\n                elif (\r\n                    len(target_string) >= 2\r\n                    and (target_string[0] == \"[\")\r\n                    and (target_string[-1] == \"]\")\r\n                ):\r\n                    try:\r\n                        return ast.literal_eval(target_string)\r\n                    except (SyntaxError, ValueError):\r\n                        return target_string\r\n                else:\r\n                    return target_string\r\n        elif isinstance(doc_to_target, list):\r\n            return doc_to_target\r\n        elif callable(doc_to_target):\r\n            return doc_to_target(doc)\r\n        # Used when applying a Promptsource template\r\n        elif hasattr(doc_to_target, \"apply\"):\r\n            applied_prompt = doc_to_target.apply(doc)\r\n            if len(applied_prompt) == 2:\r\n                return applied_prompt[1]\r\n            else:\r\n                eval_logger.warning(\"Applied prompt returns empty string\")\r\n                return self.config.fewshot_delimiter\r\n        else:\r\n            raise TypeError\r\n\r\n    def doc_to_choice(self, doc: Any, doc_to_choice=None) -> List[str]:\r\n        if self.prompt is not None:\r\n            doc_to_choice = self.prompt\r\n        elif doc_to_choice is not None:\r\n            doc_to_choice = doc_to_choice\r\n        elif self.config.doc_to_choice is None:\r\n            eval_logger.error(\"doc_to_choice was called but not set in config\")\r\n        else:\r\n            doc_to_choice = self.config.doc_to_choice\r\n\r\n        if isinstance(doc_to_choice, str):\r\n            if doc_to_choice in self.features:\r\n                return doc[doc_to_choice]\r\n            else:\r\n                return ast.literal_eval(utils.apply_template(doc_to_choice, doc))\r\n        elif isinstance(doc_to_choice, list):\r\n            return doc_to_choice\r\n        elif isinstance(doc_to_choice, dict):\r\n            return list(doc_to_choice.values())\r\n        elif callable(doc_to_choice):\r\n            return doc_to_choice(doc)\r\n        elif hasattr(doc_to_choice, \"get_answer_choices_list\"):\r\n            return doc_to_choice.get_answer_choices_list(doc)\r\n        else:\r\n            raise TypeError\r\n\r\n    def doc_to_image(self, doc: Any, doc_to_image=None) -> Union[int, str, list]:\r\n        if doc_to_image is not None:\r\n            doc_to_image = doc_to_image\r\n        elif self.config.doc_to_image is not None:\r\n            doc_to_image = self.config.doc_to_image\r\n        else:\r\n            return None\r\n\r\n        if isinstance(doc_to_image, list):\r\n            image_feature = [\r\n                self.doc_to_image(doc, feature) for feature in doc_to_image\r\n            ]\r\n            return [feature for feature in image_feature if feature is not None]\r\n        elif isinstance(doc_to_image, str):\r\n            if doc_to_image in self.features:\r\n                return doc[doc_to_image]\r\n            else:\r\n                return ast.literal_eval(utils.apply_template(doc_to_image, doc))\r\n        elif callable(doc_to_image):\r\n            return doc_to_image(doc)\r\n        else:\r\n            return None\r\n\r\n    def construct_requests(\r\n        self, doc: dict, ctx: str, **kwargs\r\n    ) -> Union[List[Instance], Instance]:\r\n        aux_arguments = None\r\n\r\n        if self.OUTPUT_TYPE == \"loglikelihood\":\r\n            arguments = (ctx, self.doc_to_target(doc))\r\n        elif self.OUTPUT_TYPE == \"loglikelihood_rolling\":\r\n            arguments = (self.doc_to_target(doc),)\r\n        elif self.OUTPUT_TYPE == \"multiple_choice\":\r\n            choices = self.doc_to_choice(doc)\r\n            target_delimiter = self.config.target_delimiter\r\n            if self.multiple_input:\r\n                # If there are multiple inputs, choices are placed in the ctx\r\n                cont = self.doc_to_target(doc)\r\n                arguments = [\r\n                    (ctx + choice, f\"{target_delimiter}{cont}\") for choice in choices\r\n                ]\r\n            else:\r\n                # Otherwise they are placed in the continuation\r\n                arguments = [(ctx, f\"{target_delimiter}{cont}\") for cont in choices]\r\n\r\n            # TODO: we should raise a warning telling users this will at most ~2x runtime.\r\n            if \"acc_mutual_info\" in self._metric_fn_list.keys():\r\n                # if we are calculating multiple choice accuracy\r\n                # using mutual information instead of raw loglikelihood as metric, need unconditional lls.\r\n\r\n                # here mutual info refers to calculating\r\n                # log(P(choice|ctx) / P(choice)) = log(P(choice|ctx)) - log(P(choice))\r\n                # in other words normalizing by subtracting the unconditional logprob of each choice.\r\n                aux_arguments = [(\"\", f\"{choice}\") for choice in choices]\r\n\r\n                arguments.extend(aux_arguments)\r\n\r\n        elif self.OUTPUT_TYPE == \"generate_until\":\r\n            arguments = (ctx, deepcopy(self.config.generation_kwargs))\r\n\r\n        multimodal_arg = {}\r\n        if (\r\n            self.config.doc_to_image\r\n        ):  # TODO: ensure that non-multimodal tasks aren't getting visual args\r\n            multimodal_arg = {\r\n                **multimodal_arg,\r\n                **{\"visual\": self.doc_to_image(doc)},\r\n            }\r\n\r\n        if bool(multimodal_arg):\r\n            if isinstance(arguments, list):\r\n                arguments = [arg + (multimodal_arg,) for arg in arguments]\r\n            else:\r\n                arguments = arguments + (multimodal_arg,)\r\n\r\n        if self.OUTPUT_TYPE == \"multiple_choice\":\r\n            request_list = [\r\n                Instance(\r\n                    request_type=\"loglikelihood\",\r\n                    doc=doc,\r\n                    arguments=arg,\r\n                    idx=i,\r\n                    **kwargs,\r\n                )\r\n                for i, arg in enumerate(arguments)\r\n            ]\r\n\r\n            return request_list\r\n\r\n        return Instance(\r\n            request_type=self.OUTPUT_TYPE,\r\n            doc=doc,\r\n            arguments=arguments,\r\n            idx=0,\r\n            **kwargs,\r\n        )\r\n\r\n    def process_results(self, doc, results):\r\n        if callable(self.config.process_results):\r\n            return self.config.process_results(doc, results)\r\n\r\n        result_dict = {}\r\n        use_metric = list(self._metric_fn_list.keys())\r\n        if self.OUTPUT_TYPE == \"loglikelihood\":\r\n            results = results[0]\r\n            ll, is_greedy = results\r\n            return {\r\n                **({\"perplexity\": ll} if \"perplexity\" in use_metric else {}),\r\n                **({\"acc\": int(is_greedy)} if \"acc\" in use_metric else {}),\r\n            }\r\n        elif self.OUTPUT_TYPE == \"loglikelihood_rolling\":\r\n            (loglikelihood,) = results\r\n            _words = self.count_words(self.doc_to_target(doc))\r\n            _bytes = self.count_bytes(self.doc_to_target(doc))\r\n            return {\r\n                **(\r\n                    {\"word_perplexity\": (loglikelihood, _words)}\r\n                    if \"word_perplexity\" in use_metric\r\n                    else {}\r\n                ),\r\n                **(\r\n                    {\"byte_perplexity\": (loglikelihood, _bytes)}\r\n                    if \"byte_perplexity\" in use_metric\r\n                    else {}\r\n                ),\r\n                **(\r\n                    {\"bits_per_byte\": (loglikelihood, _bytes)}\r\n                    if \"bits_per_byte\" in use_metric\r\n                    else {}\r\n                ),\r\n            }\r\n        elif self.OUTPUT_TYPE == \"multiple_choice\":\r\n            lls, is_greedy = zip(*results)\r\n\r\n            # retrieve choices in List[str] form, to compute choice lengths, etc.\r\n            choices = self.doc_to_choice(doc)\r\n            completion_len = np.array([float(len(i)) for i in choices])\r\n\r\n            if (\r\n                2 * len(choices) == len(lls)\r\n                and \"acc_mutual_info\" in self._metric_fn_list.keys()\r\n            ):\r\n                # then we are doing mutual info.\r\n                # this stores the \"dryrun\" / unconditional answer loglikelihoods\r\n                lls_unconditional = lls[1::2]\r\n                if len(lls_unconditional) != len(choices):\r\n                    raise ValueError\r\n                # and this stores our \"regular\" conditional loglikelihoods\r\n                lls = lls[::2]\r\n\r\n            pred = np.argmax(lls)\r\n            pred_norm = np.argmax(lls / completion_len)\r\n\r\n            if self.multiple_input:\r\n                gold = self.doc_to_text(doc)\r\n            else:\r\n                gold = self.doc_to_target(doc)\r\n\r\n            gold_index_error = False\r\n            if isinstance(gold, list):\r\n                gold = [i if i < len(choices) else -100 for i in gold]\r\n                if -100 in gold:\r\n                    gold_index_error = True\r\n            else:\r\n                if isinstance(gold, int):\r\n                    gold = gold if gold < len(choices) else -100\r\n                elif isinstance(gold, str):\r\n                    gold = choices.index(gold) if gold in choices else -100\r\n\r\n                if gold == -100:\r\n                    gold_index_error = True\r\n\r\n            if gold_index_error:\r\n                eval_logger.warning(\r\n                    f\"Label index was not in within range of available choices,\"\r\n                    f\"Sample:\\n\\n{doc}\\n\\n\"\r\n                )\r\n\r\n            if self.multiple_target:\r\n                acc = 1.0 if pred in gold else 0.0\r\n                acc_norm = 1.0 if pred_norm in gold else 0.0\r\n                exact_match = int(any([is_greedy[i] if i != -100 else 0 for i in gold]))\r\n            else:\r\n                acc = 1.0 if pred == gold else 0.0\r\n                acc_norm = 1.0 if pred_norm == gold else 0.0\r\n                # TODO: this gets score of 0 on arc_challenge for pythia-70m. need to test that this works properly\r\n                exact_match = int(is_greedy[gold]) if gold != -100 else 0\r\n\r\n            prob_norm = utils.softmax(lls)\r\n\r\n            # TODO use keyword arguments to the metric?\r\n            # gold, pred, norm stuff, the original lls,\r\n            result_dict = {\r\n                **({\"acc\": acc} if \"acc\" in use_metric else {}),\r\n                **({\"f1\": (gold, pred)} if \"f1\" in use_metric else {}),\r\n                **({\"mcc\": (gold, pred)} if \"mcc\" in use_metric else {}),\r\n                **({\"acc_norm\": acc_norm} if \"acc_norm\" in use_metric else {}),\r\n                **({\"exact_match\": exact_match} if \"exact_match\" in use_metric else {}),\r\n                **(\r\n                    {\"brier_score\": (gold, prob_norm)}\r\n                    if \"brier_score\" in use_metric\r\n                    else {}\r\n                ),\r\n            }\r\n\r\n            if \"acc_mutual_info\" in use_metric:\r\n                lls_mutual_info = [\r\n                    ll_c - ll_u for ll_c, ll_u in zip(lls, lls_unconditional)\r\n                ]\r\n                acc_mutual_info = 1.0 if np.argmax(lls_mutual_info) == gold else 0.0\r\n                result_dict[\"acc_mutual_info\"] = acc_mutual_info\r\n\r\n        elif self.OUTPUT_TYPE == \"generate_until\":\r\n            gold = self.doc_to_target(doc)\r\n            result = results[0]\r\n            if self.config.doc_to_choice is not None:\r\n                # If you set doc_to_choice,\r\n                # it assumes that doc_to_target returns a number.\r\n                choices = self.doc_to_choice(doc)\r\n                gold = choices[gold]\r\n            # we expect multiple_targets to be a list.\r\n            elif self.multiple_target:\r\n                gold = list(gold)\r\n            elif type(gold) is not type(result):\r\n                # cast gold to the same type as result\r\n                gold = type(result)(gold)\r\n\r\n            for metric in self._metric_fn_list.keys():\r\n                if self.multiple_target:\r\n                    # in the case where we have multiple targets,\r\n                    # return true if any are true\r\n                    # TODO: this may break for multipLe_target, non zero-or-1 metrics\r\n                    scores = []\r\n                    if not isinstance(gold, list):\r\n                        # sometimes, a multiple_target dataset has exceptions where one doc has only one string answer\r\n                        # print(gold)\r\n                        gold = [gold]\r\n                    if metric == \"exact_match\":\r\n                        result = [result for _ in range(len(gold))]\r\n                        scores = self._metric_fn_list[metric](\r\n                            references=gold,\r\n                            predictions=result,\r\n                            **self._metric_fn_kwargs[metric],\r\n                        )[metric]\r\n                        result_score = 1.0 if scores > 0.0 else 0.0\r\n                    else:\r\n                        for gold_option in gold:\r\n                            try:\r\n                                result_score = self._metric_fn_list[metric](\r\n                                    references=[gold_option],\r\n                                    predictions=[result],\r\n                                    **self._metric_fn_kwargs[metric],\r\n                                )\r\n                            except (\r\n                                TypeError\r\n                            ):  # TODO: this is hacky and I don't want to do it\r\n                                result_score = self._metric_fn_list[metric](\r\n                                    [gold_option, result]\r\n                                )\r\n                            if isinstance(result_score, dict):\r\n                                # TODO: this handles the case where HF evaluate returns a dict.\r\n                                result_score = result_score[metric]\r\n                            scores.append(result_score)\r\n                        if any(scores):\r\n                            result_score = 1.0\r\n                        else:\r\n                            result_score = 0.0\r\n                else:\r\n                    try:\r\n                        result_score = self._metric_fn_list[metric](\r\n                            references=[gold],\r\n                            predictions=[result],\r\n                            **self._metric_fn_kwargs[metric],\r\n                        )\r\n                    except TypeError:  # needed for now in order to use a different interface between our own metrics and HF Evaluate metrics\r\n                        result_score = self._metric_fn_list[metric]([gold, result])\r\n                    if isinstance(result_score, dict):\r\n                        # TODO: this handles the case where HF evaluate returns a dict.\r\n                        result_score = result_score[metric]\r\n                result_dict[metric] = result_score\r\n        else:\r\n            raise ValueError(\r\n                f\"Passed invalid output_type '{self.OUTPUT_TYPE}' ! Please use one of \",\r\n                \"'loglikelihood', 'loglikelihood_rolling', 'generate_until' or 'multiple_choice'\",\r\n            )\r\n\r\n        return result_dict\r\n\r\n    def aggregation(self) -> dict:\r\n        return self._aggregation_list\r\n\r\n    def higher_is_better(self) -> dict:\r\n        return self._higher_is_better\r\n\r\n    def get_config(self, key: str) -> Any:\r\n        return getattr(self._config, key, None)\r\n\r\n    @property\r\n    def task_name(self) -> Any:\r\n        return getattr(self.config, \"task\", None)\r\n\r\n    def __repr__(self):\r\n        return (\r\n            f\"ConfigurableTask(task_name={getattr(self.config, 'task', None)},\"\r\n            f\"output_type={self.OUTPUT_TYPE},\"\r\n            f\"num_fewshot={getattr(self.config, 'num_fewshot', None)},\"\r\n            f\"num_samples={len(self.eval_docs)})\"\r\n        )\r\n\r\n\r\nclass MultipleChoiceTask(Task):\r\n    OUTPUT_TYPE = \"loglikelihood\"\r\n\r\n    def doc_to_target(self, doc: dict) -> str:\r\n        return \" \" + doc[\"choices\"][doc[\"gold\"]]\r\n\r\n    def construct_requests(self, doc: dict, ctx: str, **kwargs) -> List[Instance]:\r\n        # TODO: add mutual info here?\r\n        return [\r\n            Instance(\r\n                request_type=\"loglikelihood\",\r\n                doc=doc,\r\n                arguments=(ctx, \" {}\".format(choice)),\r\n                idx=i,\r\n                **kwargs,\r\n            )\r\n            for i, choice in enumerate(doc[\"choices\"])\r\n        ]\r\n\r\n    def process_results(self, doc: dict, results: Iterable[Tuple[float, bool]]) -> dict:\r\n        results = [\r\n            res[0] for res in results\r\n        ]  # only retain loglikelihoods, discard is_greedy TODO: do we need is_greedy anywhere?\r\n        gold = doc[\"gold\"]\r\n\r\n        acc = 1.0 if np.argmax(results) == gold else 0.0\r\n        completion_len = np.array([float(len(i)) for i in doc[\"choices\"]])\r\n        acc_norm = 1.0 if np.argmax(results / completion_len) == gold else 0.0\r\n\r\n        return {\r\n            \"acc\": acc,\r\n            \"acc_norm\": acc_norm,\r\n        }\r\n\r\n    def higher_is_better(self) -> dict:\r\n        return {\r\n            \"acc\": True,\r\n            \"acc_norm\": True,\r\n        }\r\n\r\n    def aggregation(self) -> dict:\r\n        return {\r\n            \"acc\": mean,\r\n            \"acc_norm\": mean,\r\n        }\r\n\r\n\r\nclass PerplexityTask(Task):\r\n    OUTPUT_TYPE = \"loglikelihood_rolling\"\r\n\r\n    def has_training_docs(self) -> bool:\r\n        return False\r\n\r\n    def fewshot_examples(self, k: int, rnd) -> List:\r\n        if k != 0:\r\n            raise ValueError(\r\n                \"The number of fewshot examples must be 0 for perplexity tasks.\"\r\n            )\r\n        return []\r\n\r\n    def fewshot_context(self, doc: dict, num_fewshot: int) -> Literal[\"\"]:\r\n        if num_fewshot != 0:\r\n            raise ValueError(\r\n                \"The number of fewshot examples must be 0 for perplexity tasks.\"\r\n            )\r\n\r\n        return \"\"\r\n\r\n    def higher_is_better(self) -> dict:\r\n        return {\r\n            \"word_perplexity\": False,\r\n            \"byte_perplexity\": False,\r\n            \"bits_per_byte\": False,\r\n        }\r\n\r\n    def doc_to_decontamination_query(self, doc):\r\n        return doc\r\n\r\n    def doc_to_text(self, doc) -> str:\r\n        return \"\"\r\n\r\n    def doc_to_target(self, doc):\r\n        return doc\r\n\r\n    def construct_requests(self, doc: dict, ctx: Optional[str], **kwargs):\r\n        if bool(ctx):\r\n            raise ValueError\r\n\r\n        return Instance(\r\n            request_type=self.OUTPUT_TYPE,\r\n            doc=doc,\r\n            arguments=(self.doc_to_target(doc),),\r\n            idx=0,\r\n            **kwargs,\r\n        )\r\n\r\n    def process_results(self, doc: dict, results: Tuple[float]) -> dict:\r\n        (loglikelihood,) = results\r\n        words = self.count_words(self.doc_to_target(doc))\r\n        bytes_ = self.count_bytes(self.doc_to_target(doc))\r\n        return {\r\n            \"word_perplexity\": (loglikelihood, words),\r\n            \"byte_perplexity\": (loglikelihood, bytes_),\r\n            \"bits_per_byte\": (loglikelihood, bytes_),\r\n        }\r\n\r\n    def aggregation(self) -> dict:\r\n        return {\r\n            \"word_perplexity\": weighted_perplexity,\r\n            \"byte_perplexity\": weighted_perplexity,\r\n            \"bits_per_byte\": bits_per_byte,\r\n        }\r\n\r\n    @classmethod\r\n    def count_bytes(cls, doc) -> int:\r\n        return len(doc.encode(\"utf-8\"))\r\n\r\n    @classmethod\r\n    def count_words(cls, doc) -> int:\r\n        \"\"\"Downstream tasks with custom word boundaries should override this!\"\"\"\r\n        return len(re.split(r\"\\s+\", doc))\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/api/registry.py", "content": "import logging\r\nfrom typing import Callable, Dict\r\n\r\nimport evaluate as hf_evaluate\r\n\r\nfrom lm_eval.api.model import LM\r\n\r\n\r\neval_logger = logging.getLogger(\"lm-eval\")\r\n\r\nMODEL_REGISTRY = {}\r\n\r\n\r\ndef register_model(*names):\r\n    # either pass a list or a single alias.\r\n    # function receives them as a tuple of strings\r\n\r\n    def decorate(cls):\r\n        for name in names:\r\n            assert issubclass(\r\n                cls, LM\r\n            ), f\"Model '{name}' ({cls.__name__}) must extend LM class\"\r\n\r\n            assert (\r\n                name not in MODEL_REGISTRY\r\n            ), f\"Model named '{name}' conflicts with existing model! Please register with a non-conflicting alias instead.\"\r\n\r\n            MODEL_REGISTRY[name] = cls\r\n        return cls\r\n\r\n    return decorate\r\n\r\n\r\ndef get_model(model_name):\r\n    try:\r\n        return MODEL_REGISTRY[model_name]\r\n    except KeyError:\r\n        raise ValueError(\r\n            f\"Attempted to load model '{model_name}', but no model for this name found! Supported model names: {', '.join(MODEL_REGISTRY.keys())}\"\r\n        )\r\n\r\n\r\nTASK_REGISTRY = {}\r\nGROUP_REGISTRY = {}\r\nALL_TASKS = set()\r\nfunc2task_index = {}\r\n\r\n\r\ndef register_task(name):\r\n    def decorate(fn):\r\n        assert (\r\n            name not in TASK_REGISTRY\r\n        ), f\"task named '{name}' conflicts with existing registered task!\"\r\n\r\n        TASK_REGISTRY[name] = fn\r\n        ALL_TASKS.add(name)\r\n        func2task_index[fn.__name__] = name\r\n        return fn\r\n\r\n    return decorate\r\n\r\n\r\ndef register_group(name):\r\n    def decorate(fn):\r\n        func_name = func2task_index[fn.__name__]\r\n        if name in GROUP_REGISTRY:\r\n            GROUP_REGISTRY[name].append(func_name)\r\n        else:\r\n            GROUP_REGISTRY[name] = [func_name]\r\n            ALL_TASKS.add(name)\r\n        return fn\r\n\r\n    return decorate\r\n\r\n\r\nOUTPUT_TYPE_REGISTRY = {}\r\nMETRIC_REGISTRY = {}\r\nMETRIC_AGGREGATION_REGISTRY = {}\r\nAGGREGATION_REGISTRY: Dict[str, Callable[[], Dict[str, Callable]]] = {}\r\nHIGHER_IS_BETTER_REGISTRY = {}\r\nFILTER_REGISTRY = {}\r\n\r\nDEFAULT_METRIC_REGISTRY = {\r\n    \"loglikelihood\": [\r\n        \"perplexity\",\r\n        \"acc\",\r\n    ],\r\n    \"loglikelihood_rolling\": [\"word_perplexity\", \"byte_perplexity\", \"bits_per_byte\"],\r\n    \"multiple_choice\": [\"acc\", \"acc_norm\"],\r\n    \"generate_until\": [\"exact_match\"],\r\n}\r\n\r\n\r\ndef register_metric(**args):\r\n    # TODO: do we want to enforce a certain interface to registered metrics?\r\n    def decorate(fn):\r\n        assert \"metric\" in args\r\n        name = args[\"metric\"]\r\n\r\n        for key, registry in [\r\n            (\"metric\", METRIC_REGISTRY),\r\n            (\"higher_is_better\", HIGHER_IS_BETTER_REGISTRY),\r\n            (\"aggregation\", METRIC_AGGREGATION_REGISTRY),\r\n        ]:\r\n            if key in args:\r\n                value = args[key]\r\n                assert (\r\n                    value not in registry\r\n                ), f\"{key} named '{value}' conflicts with existing registered {key}!\"\r\n\r\n                if key == \"metric\":\r\n                    registry[name] = fn\r\n                elif key == \"aggregation\":\r\n                    registry[name] = AGGREGATION_REGISTRY[value]\r\n                else:\r\n                    registry[name] = value\r\n\r\n        return fn\r\n\r\n    return decorate\r\n\r\n\r\ndef get_metric(name: str, hf_evaluate_metric=False) -> Callable:\r\n    if not hf_evaluate_metric:\r\n        if name in METRIC_REGISTRY:\r\n            return METRIC_REGISTRY[name]\r\n        else:\r\n            eval_logger.warning(\r\n                f\"Could not find registered metric '{name}' in lm-eval, searching in HF Evaluate library...\"\r\n            )\r\n\r\n    try:\r\n        metric_object = hf_evaluate.load(name)\r\n        return metric_object.compute\r\n    except Exception:\r\n        eval_logger.error(\r\n            f\"{name} not found in the evaluate library! Please check https://huggingface.co/evaluate-metric\",\r\n        )\r\n\r\n\r\ndef register_aggregation(name: str):\r\n    def decorate(fn):\r\n        assert (\r\n            name not in AGGREGATION_REGISTRY\r\n        ), f\"aggregation named '{name}' conflicts with existing registered aggregation!\"\r\n\r\n        AGGREGATION_REGISTRY[name] = fn\r\n        return fn\r\n\r\n    return decorate\r\n\r\n\r\ndef get_aggregation(name: str) -> Callable[[], Dict[str, Callable]]:\r\n    try:\r\n        return AGGREGATION_REGISTRY[name]\r\n    except KeyError:\r\n        eval_logger.warning(f\"{name} not a registered aggregation metric!\")\r\n\r\n\r\ndef get_metric_aggregation(name: str) -> Callable[[], Dict[str, Callable]]:\r\n    try:\r\n        return METRIC_AGGREGATION_REGISTRY[name]\r\n    except KeyError:\r\n        eval_logger.warning(f\"{name} metric is not assigned a default aggregation!\")\r\n\r\n\r\ndef is_higher_better(metric_name) -> bool:\r\n    try:\r\n        return HIGHER_IS_BETTER_REGISTRY[metric_name]\r\n    except KeyError:\r\n        eval_logger.warning(\r\n            f\"higher_is_better not specified for metric '{metric_name}'!\"\r\n        )\r\n\r\n\r\ndef register_filter(name):\r\n    def decorate(cls):\r\n        if name in FILTER_REGISTRY:\r\n            eval_logger.info(\r\n                f\"Registering filter `{name}` that is already in Registry {FILTER_REGISTRY}\"\r\n            )\r\n        FILTER_REGISTRY[name] = cls\r\n        return cls\r\n\r\n    return decorate\r\n\r\n\r\ndef get_filter(filter_name: str) -> type:\r\n    try:\r\n        return FILTER_REGISTRY[filter_name]\r\n    except KeyError:\r\n        eval_logger.warning(f\"filter `{filter_name}` is not registered!\")\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/decontamination/decontaminate.py", "content": "import collections\r\nimport glob\r\nimport json\r\nimport os\r\nimport pickle\r\nimport random\r\nimport time\r\n\r\nfrom .archiver import ZStdTextReader\r\nfrom .janitor import Janitor, word_ngrams\r\n\r\n\r\n# Was used for testing the evaluator decoupled from the full logic below\r\ndef get_train_overlap_stub(docs: dict, ngrams_path: str, ngrams_n_size: str):\r\n    simulated_overlap = 0.1\r\n    contaminated = int(len(docs) * simulated_overlap)\r\n    return random.sample(range(len(docs)), contaminated)\r\n\r\n\r\n# Returns a dictionary containing all overlapping documents in each\r\n# task. In the standard use case, an overlap occurs when any of the 13-grams\r\n# found in the task document exist in the training set documents.\r\n#\r\n# To generate 13-grams for the pile see scripts/clean_training_data. The final output of these\r\n# scripts are an info.json file containing the n_gram_size (13) and a bunch of \"ngrams_{x}.bkt.txt.sorted.zst\"\r\n# files. These should exist in the \"ngrams_path\" provided to this function.\r\n\r\n\r\n# Algorithm:\r\n# 1. Build lookups for each dataset {ngram: list(document_ids)}\r\n# 2. Merge into an overall lookup {ngram: [(task_name, task_set, doc_ids),]}\r\n# 3. Full scan the 13-grams from the training set against the merged lookup,\r\n#    saving matches in the \"duplicates\" dictionary {(task_name, task_set): set(doc_ids)}\r\n# 4. Strip the task_set from the dictionary keys and return\r\n#\r\n# We cache the task+set lookups as well as the overlaps.\r\ndef get_train_overlap(docs_by_task_set: dict, ngrams_path: str, limit: int) -> dict:\r\n    # return get_train_overlap_stub(docs, ngrams_path, ngrams_n_size)\r\n\r\n    info_dict_path = os.path.join(ngrams_path, \"info.json\")\r\n    info_dict = json.load(open(info_dict_path, \"r\", encoding=\"utf-8\"))\r\n    ngrams_n_size = info_dict[\"ngram_size\"]\r\n\r\n    janitor = Janitor()\r\n\r\n    # Build lookup for each dataset first in case we use different task combinations later\r\n    print(\"Building Lookups...\")\r\n    start = time.perf_counter()\r\n\r\n    def get_overlaps_dump_path(task_name, task_set, ngrams_n_size, limit) -> str:\r\n        return f\"data/{task_name}/{task_set}_{ngrams_n_size}grams_limit{limit}.overlaps\"\r\n\r\n    lookups = {}\r\n    duplicates = {}  # (task_name, task_set): set(doc_ids)}\r\n    sets_to_decontaminate = len(docs_by_task_set.keys())\r\n\r\n    for (task_name, task_set), docs in docs_by_task_set.items():\r\n        if not os.path.exists(f\"data/{task_name}\"):\r\n            os.mkdir(f\"data/{task_name}\")\r\n\r\n        # Check if we've decontaminated this combination before\r\n        overlaps_dump_path = get_overlaps_dump_path(\r\n            task_name, task_set, ngrams_n_size, limit\r\n        )\r\n        if os.path.exists(overlaps_dump_path):\r\n            duplicates[(task_name, task_set)] = pickle.load(\r\n                open(overlaps_dump_path, \"rb\")\r\n            )\r\n            sets_to_decontaminate -= 1\r\n            continue\r\n        else:\r\n            duplicates[(task_name, task_set)] = set()\r\n\r\n        # Build/load the task lookup {ngram: set(documents)}.\r\n        task_set_lookup_path = (\r\n            f\"data/{task_name}/{task_set}_{ngrams_n_size}grams_limit{limit}.lookup\"\r\n        )\r\n        if os.path.exists(task_set_lookup_path):\r\n            print(f\"{task_set_lookup_path} available, loading...\")\r\n            lookups[(task_name, task_set)] = pickle.load(\r\n                open(task_set_lookup_path, \"rb\")\r\n            )\r\n        else:\r\n            print(f\"{task_set_lookup_path} not available, building...\")\r\n            lookup = collections.defaultdict(set)\r\n\r\n            for doc_id, document in enumerate(docs):\r\n                ngrams = word_ngrams(janitor.normalize_string(document), ngrams_n_size)\r\n                for ngram in ngrams:\r\n                    lookup[ngram].add(doc_id)\r\n\r\n            pickle.dump(lookup, open(task_set_lookup_path, \"wb\"))\r\n            lookups[(task_name, task_set)] = lookup\r\n\r\n    elapsed = time.perf_counter() - start\r\n    print(f\"Building lookups took {elapsed:0.5f} seconds.\")\r\n\r\n    matched_ngrams = []\r\n\r\n    if sets_to_decontaminate > 0:\r\n        print(\"Merging lookups...\")\r\n        start = time.perf_counter()\r\n        merged_lookup = collections.defaultdict(list)\r\n        for (task_name, task_set), lookup in lookups.items():\r\n            for ngram, doc_ids in lookup.items():\r\n                merged_lookup[ngram].append((task_name, task_set, doc_ids))\r\n\r\n        elapsed = time.perf_counter() - start\r\n        print(f\"Merging lookups took {elapsed:0.5f} seconds.\")\r\n\r\n        print(f\"{ngrams_n_size} grams files found in {ngrams_path}:\")\r\n        files = glob.glob(os.path.join(ngrams_path, \"*.sorted.zst\"))\r\n        print(files)\r\n\r\n        for file in files:\r\n            start = time.perf_counter()\r\n            print(f\"Scanning {file}\")\r\n            reader = ZStdTextReader(file)\r\n            total_ngrams = 0\r\n            unique_ngrams = 0\r\n            matching_unique = 0\r\n            non_matching_unique = 0\r\n\r\n            current_ngram = \"\"\r\n            for line in reader.read_tqdm():  # Scan training set ngrams file\r\n                total_ngrams += 1\r\n                [ngram, document_id] = line.rsplit(\" \", 1)\r\n                if (\r\n                    ngram != current_ngram\r\n                ):  # Only need to match the ngram once in training set\r\n                    unique_ngrams += 1\r\n                    current_ngram = ngram\r\n                    if ngram in merged_lookup:\r\n                        matched_ngrams.append(ngram)  # For logging\r\n                        matching_unique += 1\r\n                        for task_name, task_set, doc_ids in merged_lookup[ngram]:\r\n                            task_doc_set = duplicates[(task_name, task_set)]\r\n                            for doc_id in doc_ids:  # Record contamination across all relevant task/set combos\r\n                                task_doc_set.add(doc_id)\r\n                        del merged_lookup[ngram]  # No point matching again\r\n                    else:\r\n                        non_matching_unique += 1\r\n\r\n            print(f\"Total Ngrams: {total_ngrams}\")\r\n            print(f\"Unique Ngrams: {unique_ngrams}\")\r\n            print(f\"Unique Matching: {matching_unique}\")\r\n            print(f\"Unique Non Matching: {non_matching_unique}\")\r\n            print(\"Matched ngrams:\")\r\n            for ngram in matched_ngrams:\r\n                print(ngram)\r\n\r\n            elapsed = time.perf_counter() - start\r\n            print(f\"Read took {elapsed:0.5f} seconds.\")\r\n            print(f\"Speed: {(os.path.getsize(file)/1000000.0)/elapsed}MB/second\")\r\n\r\n        print(duplicates)\r\n\r\n        # Dump overlaps separately\r\n        for (task_name, task_set), doc_ids in duplicates.items():\r\n            overlaps_dump_path = get_overlaps_dump_path(\r\n                task_name, task_set, ngrams_n_size, limit\r\n            )\r\n            pickle.dump(doc_ids, open(overlaps_dump_path, \"wb\"))\r\n\r\n    # Strip task set and return\r\n    return {task_name: doc_ids for (task_name, task_set), doc_ids in duplicates.items()}\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/filters/extraction.py", "content": "import re\r\nimport sys\r\nimport unicodedata\r\n\r\nfrom lm_eval.api.filter import Filter\r\nfrom lm_eval.api.registry import register_filter\r\n\r\n\r\n@register_filter(\"regex\")\r\nclass RegexFilter(Filter):\r\n    \"\"\" \"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        regex_pattern: str = r\"#### (\\-?[0-9\\.\\,]+)\",\r\n        group_select=0,\r\n        fallback: str = \"[invalid]\",\r\n    ) -> None:\r\n        \"\"\"\r\n        pass a string `regex` to run `re.compile(r\"regex\")` on.\r\n        `fallback` defines the output returned if no matches for the regex are located.\r\n        \"\"\"\r\n        self.regex_pattern = regex_pattern\r\n        self.regex = re.compile(regex_pattern)\r\n        self.group_select = group_select\r\n        self.fallback = fallback\r\n\r\n    def apply(self, resps, docs):\r\n        # here, we assume we have a list, in which each element is\r\n        # a list of model responses for some particular input/target pair.\r\n        # so we process each of these (same input/target response sets)\r\n        # independently (and keep them a list.)\r\n        def filter_set(inst):\r\n            filtered = []\r\n            for resp in inst:\r\n                match = self.regex.findall(resp)\r\n                if match:\r\n                    match = match[self.group_select]\r\n                    if isinstance(match, tuple):\r\n                        match = [m for m in match if m][0]\r\n                    match = match.strip()\r\n                else:\r\n                    match = self.fallback\r\n                filtered.append(match)\r\n            return filtered\r\n\r\n        # print(resps)\r\n        filtered_resps = list(map(lambda x: filter_set(x), resps))\r\n        # print(filtered_resps)\r\n\r\n        return filtered_resps\r\n\r\n\r\n@register_filter(\"remove_whitespace\")\r\nclass WhitespaceFilter(Filter):\r\n    \"\"\" \"\"\"\r\n\r\n    def __init__(self) -> None:\r\n        pass\r\n\r\n    def apply(self, resps, docs):\r\n        def filter_set(inst):\r\n            filtered_resp = []\r\n            for resp in inst:\r\n                resp = resp.lstrip()\r\n                filtered_resp.append(resp)\r\n            return filtered_resp\r\n\r\n        filtered_resps = [filter_set(resp) for resp in resps]\r\n\r\n        return filtered_resps\r\n\r\n\r\n@register_filter(\"multi_choice_regex\")\r\nclass MultiChoiceRegexFilter(RegexFilter):\r\n    \"\"\"\r\n    A filter used to extract a model's answer on multiple choice questions with\r\n    letter answers. assumes each document has a \"choices\" field\r\n    containing the list of answer choices and that the answer label symbols\r\n    are of the form (A), (B), (C), ... or A, B, C.\r\n    \"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        regex_pattern: str = r\"#### (\\-?[0-9\\.\\,]+)\",\r\n        group_select=0,\r\n        fallback: str = \"[invalid]\",\r\n        ignore_case=False,\r\n        ignore_punctuation=False,\r\n        regexes_to_ignore=None,\r\n    ) -> None:\r\n        \"\"\"\r\n        regex_pattern: The basic regex pattern to use. If fails to match, we will use the customized match procedure\r\n                        - step 1 : We parse the choices between ([A-Z])s then try to find these choices in the response.\r\n                        - step 2 : We parse the choice with regex :[\\s]*([A-?]), where ? varies by number of choices.\r\n        group_select: Selects the (group_select)th match from the findall result.\r\n        ignore_case: Ignores the case during step 1 matching\r\n        ignore_punctuation: Remove the punctuation during step 1 matching\r\n        regexes_to_ignore: Remove these regexes during step 1 matching\r\n        \"\"\"\r\n        super().__init__(regex_pattern, group_select, fallback)\r\n        self.ignore_case = ignore_case\r\n        self.ignore_punctuation = ignore_punctuation\r\n        self.regexes_to_ignore = regexes_to_ignore\r\n\r\n    def apply(self, resps, docs):\r\n        # here, we assume we have a list, in which each element is\r\n        # a list of model responses for some particular input/target pair.\r\n        # so we process each of these (same input/target response sets)\r\n        # independently (and keep them a list.)\r\n\r\n        def find_match(regex, resp, convert_dict={}):\r\n            match = regex.findall(resp)\r\n            if match:\r\n                match = match[self.group_select]\r\n                if isinstance(match, tuple):\r\n                    match = [m for m in match if m][0]\r\n                match = match.strip()\r\n                if match and match in convert_dict:\r\n                    match = convert_dict[match]\r\n            return match\r\n\r\n        punct_tbl = dict.fromkeys(\r\n            i\r\n            for i in range(sys.maxunicode)\r\n            if unicodedata.category(chr(i)).startswith(\"P\")\r\n        )\r\n\r\n        def filter_ignores(st):\r\n            if self.regexes_to_ignore is not None:\r\n                for s in self.regexes_to_ignore:\r\n                    st = re.sub(s, \"\", st)\r\n\r\n            if self.ignore_case:\r\n                st = st.lower()\r\n\r\n            if self.ignore_punctuation:\r\n                # https://stackoverflow.com/a/266162\r\n                st = st.translate(punct_tbl)\r\n            return st\r\n\r\n        filtered_resps = []\r\n\r\n        for r, doc in zip(resps, docs):\r\n            fallback_regexes = []\r\n            choice_to_alpha = {}\r\n            next_alpha = \"A\"\r\n\r\n            without_paren_fallback_regexes = []\r\n            without_paren_to_target = {}\r\n\r\n            choices = doc[\"choices\"]\r\n            for c in choices:\r\n                m = filter_ignores(c.strip())\r\n                fallback_regexes.append(f\"{re.escape(m)}\")\r\n                choice_to_alpha[m] = f\"({next_alpha})\"\r\n\r\n                without_paren_fallback_regexes.append(next_alpha)\r\n                without_paren_to_target[next_alpha] = f\"({next_alpha})\"\r\n\r\n                next_alpha = chr(ord(next_alpha) + 1)\r\n            fallback_regex = re.compile(\"|\".join(fallback_regexes))\r\n            without_paren_fallback_regex = \"|\".join(without_paren_fallback_regexes)\r\n            without_paren_fallback_regex = re.compile(\r\n                f\":[\\s]*({without_paren_fallback_regex})\"\r\n            )\r\n\r\n            filtered = []\r\n            for resp in r:\r\n                match = find_match(self.regex, resp)\r\n                if not match:\r\n                    match = find_match(\r\n                        fallback_regex, filter_ignores(resp), choice_to_alpha\r\n                    )\r\n                    if not match:\r\n                        match = find_match(\r\n                            without_paren_fallback_regex, resp, without_paren_to_target\r\n                        )\r\n                if not match:\r\n                    match = self.fallback\r\n                filtered.append(match)\r\n            filtered_resps.append(filtered)\r\n\r\n        return filtered_resps\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/filters/__init__.py", "content": "from functools import partial\r\nfrom typing import List\r\n\r\nfrom lm_eval.api.filter import FilterEnsemble\r\nfrom lm_eval.api.registry import get_filter\r\n\r\nfrom . import extraction, selection, transformation\r\n\r\n\r\ndef build_filter_ensemble(\r\n    filter_name: str, components: List[List[str]]\r\n) -> FilterEnsemble:\r\n    \"\"\"\r\n    Create a filtering pipeline.\r\n    \"\"\"\r\n    filters = []\r\n    for function, kwargs in components:\r\n        if kwargs is None:\r\n            kwargs = {}\r\n        # create a filter given its name in the registry\r\n        f = partial(get_filter(function), **kwargs)\r\n        # add the filter as a pipeline step\r\n        filters.append(f)\r\n\r\n    return FilterEnsemble(name=filter_name, filters=filters)\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/models/optimum_lm.py", "content": "import json\r\nfrom importlib.util import find_spec\r\nfrom pathlib import Path\r\n\r\nfrom lm_eval import utils\r\nfrom lm_eval.api.registry import register_model\r\nfrom lm_eval.models.huggingface import HFLM\r\n\r\n\r\neval_logger = utils.eval_logger\r\n\r\n\r\n@register_model(\"openvino\")\r\nclass OptimumLM(HFLM):\r\n    \"\"\"\r\n    Optimum Intel provides a simple interface to optimize Transformer models and convert them to \\\r\n    OpenVINO™ Intermediate Representation (IR) format to accelerate end-to-end pipelines on \\\r\n    Intel® architectures using OpenVINO™ runtime.\r\n\r\n    To use an OpenVINO config, use `--model_args ov_config` to point to a json file with an OpenVINO config:\r\n    `lm_eval --model openvino --model_args pretrained=gpt2,ov_config=config.json --task lambada_openai`\r\n    Example json file contents: {\"INFERENCE_PRECISION_HINT\": \"f32\", \"CACHE_DIR\": \"model_cache\"}\r\n    \"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        device=\"cpu\",\r\n        **kwargs,\r\n    ) -> None:\r\n        if \"backend\" in kwargs:\r\n            # optimum currently only supports causal models\r\n            assert (\r\n                kwargs[\"backend\"] == \"causal\"\r\n            ), \"Currently, only OVModelForCausalLM is supported.\"\r\n\r\n        self.openvino_device = device\r\n\r\n        super().__init__(\r\n            device=self.openvino_device,\r\n            backend=kwargs.pop(\"backend\", \"causal\"),\r\n            **kwargs,\r\n        )\r\n\r\n    def _create_model(\r\n        self,\r\n        pretrained: str,\r\n        revision=\"main\",\r\n        dtype=\"auto\",\r\n        trust_remote_code=False,\r\n        **kwargs,\r\n    ) -> None:\r\n        if not find_spec(\"optimum\"):\r\n            raise Exception(\r\n                \"package `optimum` is not installed. Please install it via `pip install optimum[openvino]`\"\r\n            )\r\n        else:\r\n            from optimum.intel.openvino import OVModelForCausalLM\r\n\r\n        model_kwargs = kwargs if kwargs else {}\r\n        if \"ov_config\" in model_kwargs:\r\n            if not Path(model_kwargs[\"ov_config\"]).exists():\r\n                raise ValueError(\r\n                    \"ov_config should point to a .json file containing an OpenVINO config\"\r\n                )\r\n            with open(model_kwargs[\"ov_config\"]) as f:\r\n                model_kwargs[\"ov_config\"] = json.load(f)\r\n                eval_logger.info(\r\n                    f\"Using custom OpenVINO config: {model_kwargs['ov_config']}\"\r\n                )\r\n\r\n        else:\r\n            model_kwargs[\"ov_config\"] = {}\r\n        model_kwargs[\"ov_config\"].setdefault(\"CACHE_DIR\", \"\")\r\n        model_file = Path(pretrained) / \"openvino_model.xml\"\r\n        if model_file.exists():\r\n            export = False\r\n        else:\r\n            export = True\r\n\r\n        self._model = OVModelForCausalLM.from_pretrained(\r\n            pretrained,\r\n            revision=revision,\r\n            trust_remote_code=trust_remote_code,\r\n            export=export,\r\n            device=self.openvino_device.upper(),\r\n            **model_kwargs,\r\n        )\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/evaluator.py", "content": "import itertools\r\nimport json\r\nimport logging\r\nimport random\r\nimport time\r\nfrom collections import defaultdict\r\nfrom typing import TYPE_CHECKING, List, Optional, Union\r\nimport numpy as np\r\nimport torch\r\n\r\nimport lm_eval.api.metrics\r\nimport lm_eval.api.registry\r\nimport lm_eval.api.task\r\nimport lm_eval.models\r\nfrom lm_eval.caching.cache import delete_cache\r\nfrom lm_eval.evaluator_utils import (\r\n    consolidate_group_results,\r\n    consolidate_results,\r\n    get_sample_size,\r\n    get_subtask_list,\r\n    get_task_list,\r\n    prepare_print_tasks,\r\n    print_writeout,\r\n    run_task_tests,\r\n)\r\nfrom lm_eval.loggers import EvaluationTracker\r\nfrom lm_eval.loggers.utils import add_env_info, add_tokenizer_info, get_git_commit_hash\r\nfrom lm_eval.tasks import (\r\n    TaskManager,\r\n    get_task_dict,\r\n)\r\nfrom lm_eval.utils import (\r\n    eval_logger,\r\n    handle_non_serializable,\r\n    hash_string,\r\n    positional_deprecated,\r\n    simple_parse_args_string,\r\n)\r\n\r\n\r\nif TYPE_CHECKING:\r\n    from lm_eval.api.model import LM\r\n    from lm_eval.api.task import Task\r\n\r\n\r\n@positional_deprecated\r\ndef simple_evaluate(\r\n    model,\r\n    model_args: Optional[Union[str, dict]] = None,\r\n    tasks: Optional[List[Union[str, dict, object]]] = None,\r\n    num_fewshot: Optional[int] = None,\r\n    batch_size: Optional[Union[int, str]] = None,\r\n    max_batch_size: Optional[int] = None,\r\n    device: Optional[str] = None,\r\n    use_cache: Optional[str] = None,\r\n    cache_requests: bool = False,\r\n    rewrite_requests_cache: bool = False,\r\n    delete_requests_cache: bool = False,\r\n    limit: Optional[Union[int, float]] = None,\r\n    random_subsample: Optional[bool] = False,\r\n    bootstrap_iters: int = 100000,\r\n    check_integrity: bool = False,\r\n    write_out: bool = False,\r\n    log_samples: bool = True,\r\n    evaluation_tracker: Optional[EvaluationTracker] = None,\r\n    system_instruction: Optional[str] = None,\r\n    apply_chat_template: Union[bool, str] = False,\r\n    fewshot_as_multiturn: bool = False,\r\n    gen_kwargs: Optional[str] = None,\r\n    task_manager: Optional[TaskManager] = None,\r\n    verbosity: str = \"INFO\",\r\n    predict_only: bool = False,\r\n    random_seed: int = 0,\r\n    numpy_random_seed: int = 1234,\r\n    torch_random_seed: int = 1234,\r\n    fewshot_random_seed: int = 1234,\r\n):\r\n    \"\"\"Instantiate and evaluate a model on a list of tasks.\r\n\r\n    :param model: Union[str, LM]\r\n        Name of model or LM object, see lm_eval.models.get_model\r\n    :param model_args: Optional[str, dict]\r\n        String or dict arguments for each model class, see LM.create_from_arg_string and LM.create_from_arg_object.\r\n        Ignored if `model` argument is a LM object.\r\n    :param tasks: list[Union[str, dict, Task]]\r\n        List of task names or Task objects. Task objects will be taken to have name task.EVAL_HARNESS_NAME if defined and type(task).__name__ otherwise.\r\n    :param num_fewshot: int\r\n        Number of examples in few-shot context\r\n    :param batch_size: int or str, optional\r\n        Batch size for model\r\n    :param max_batch_size: int, optional\r\n        Maximal batch size to try with automatic batch size detection\r\n    :param device: str, optional\r\n        PyTorch device (e.g. \"cpu\" or \"cuda:0\") for running models\r\n    :param use_cache: str, optional\r\n        A path to a sqlite db file for caching model responses. `None` if not caching.\r\n    :param cache_requests: bool, optional\r\n        Speed up evaluation by caching the building of dataset requests. `None` if not caching.\r\n    :param rewrite_requests_cache: bool, optional\r\n        Rewrites all of the request cache if set to `True`. `None` if not desired.\r\n    :param delete_requests_cache: bool, optional\r\n        Deletes all of the request cache if set to `True`. `None` if not desired.\r\n    :param limit: int or float, optional\r\n        Limit the number of examples per task (only use this for testing), If <1, limit is a percentage of the total number of examples.\r\n    :param bootstrap_iters:\r\n        Number of iterations for bootstrap statistics, used when calculating stderrs. set to 0 for no stderr calculations to be performed.\r\n    :param check_integrity: bool\r\n        Whether to run the relevant part of the test suite for the tasks\r\n    :param write_out: bool\r\n        If True, write out an example document and model input for checking task integrity\r\n    :param log_samples: bool\r\n        If True, write out all model outputs and documents for per-sample measurement and post-hoc analysis\r\n    :param system_instruction: str\r\n        System instruction to be applied to the prompt\r\n    :param apply_chat_template: Union[bool, str]\r\n        Specifies whether to apply a chat template to the prompt.\r\n        - If set to True, the default chat template is applied.\r\n        - If set to a string, applies the specified chat template by name.\r\n        Defaults to False (no chat template applied).\r\n    :param fewshot_as_multiturn: bool\r\n        Whether to provide the fewshot examples as a multiturn conversation or a single user turn.\r\n    :param gen_kwargs: str\r\n        String arguments for model generation\r\n        Ignored for all tasks with loglikelihood output_type\r\n    :param predict_only: bool\r\n        If true only model outputs will be generated and returned. Metrics will not be evaluated\r\n    :param random_seed: int\r\n        Random seed for python's random module. If set to None, the seed will not be set.\r\n    :param numpy_random_seed: int\r\n        Random seed for numpy. If set to None, the seed will not be set.\r\n    :param torch_random_seed: int\r\n        Random seed for torch. If set to None, the seed will not be set.\r\n    :param fewshot_random_seed: int\r\n        Random seed for fewshot sampler random generator. If set to None, the seed of generator will be set to None.\r\n\r\n    :return\r\n        Dictionary of results\r\n    \"\"\"\r\n    eval_logger.setLevel(getattr(logging, f\"{verbosity}\"))\r\n    start_date = time.time()\r\n\r\n    if delete_requests_cache:\r\n        eval_logger.info(\"Deleting requests cache...\")\r\n        delete_cache()\r\n\r\n    seed_message = []\r\n    if random_seed is not None:\r\n        # See https://github.com/EleutherAI/lm-evaluation-harness/pull/1412\r\n        seed_message.append(f\"Setting random seed to {random_seed}\")\r\n        random.seed(random_seed)\r\n\r\n    if numpy_random_seed is not None:\r\n        seed_message.append(f\"Setting numpy seed to {numpy_random_seed}\")\r\n        np.random.seed(numpy_random_seed)\r\n\r\n    if torch_random_seed is not None:\r\n        seed_message.append(f\"Setting torch manual seed to {torch_random_seed}\")\r\n        torch.manual_seed(torch_random_seed)\r\n\r\n    if seed_message:\r\n        eval_logger.info(\" | \".join(seed_message))\r\n\r\n    if tasks is None:\r\n        tasks = []\r\n    if len(tasks) == 0:\r\n        raise ValueError(\r\n            \"No tasks specified, or no tasks found. Please verify the task names.\"\r\n        )\r\n\r\n    if gen_kwargs is not None:\r\n        gen_kwargs = simple_parse_args_string(gen_kwargs)\r\n        eval_logger.warning(\r\n            \"generation_kwargs specified through cli, these settings will update set parameters in yaml tasks. \"\r\n            \"Ensure 'do_sample=True' for non-greedy decoding!\"\r\n        )\r\n        if gen_kwargs == \"\":\r\n            gen_kwargs = None\r\n\r\n    if isinstance(model, str):\r\n        if model_args is None:\r\n            eval_logger.warning(\"model_args not specified. Using defaults.\")\r\n            model_args = \"\"\r\n\r\n        if isinstance(model_args, dict):\r\n            eval_logger.info(\r\n                f\"Initializing {model} model, with arguments: {model_args}\"\r\n            )\r\n            lm = lm_eval.api.registry.get_model(model).create_from_arg_obj(\r\n                model_args,\r\n                {\r\n                    \"batch_size\": batch_size,\r\n                    \"max_batch_size\": max_batch_size,\r\n                    \"device\": device,\r\n                },\r\n            )\r\n\r\n        else:\r\n            eval_logger.info(\r\n                f\"Initializing {model} model, with arguments: {simple_parse_args_string(model_args)}\"\r\n            )\r\n            lm = lm_eval.api.registry.get_model(model).create_from_arg_string(\r\n                model_args,\r\n                {\r\n                    \"batch_size\": batch_size,\r\n                    \"max_batch_size\": max_batch_size,\r\n                    \"device\": device,\r\n                },\r\n            )\r\n    else:\r\n        if not isinstance(model, lm_eval.api.model.LM):\r\n            raise TypeError(\r\n                f\"The value of `model` passed to simple_evaluate() was of type {type(model)}, but is required to be a subclass of lm_eval.api.model.LM . This may be because you are passing an initialized Hugging Face PreTrainedModel without having wrapped it in `lm_eval.models.huggingface.HFLM(pretrained=my_model)` first.\"\r\n            )\r\n        eval_logger.info(\"Using pre-initialized model\")\r\n        lm = model\r\n\r\n    if use_cache is not None:\r\n        eval_logger.info(f\"Using cache at {use_cache + '_rank' + str(lm.rank) + '.db'}\")\r\n        lm = lm_eval.api.model.CachingLM(\r\n            lm,\r\n            use_cache\r\n            # each rank receives a different cache db.\r\n            # necessary to avoid multiple writes to cache at once\r\n            + \"_rank\"\r\n            + str(lm.rank)\r\n            + \".db\",\r\n        )\r\n\r\n    if task_manager is None:\r\n        task_manager = TaskManager(verbosity)\r\n\r\n    task_dict = get_task_dict(tasks, task_manager)\r\n\r\n    # helper function to recursively apply config overrides to leaf subtasks, skipping their constituent groups.\r\n    # (setting of num_fewshot ; bypassing metric calculation ; setting fewshot seed)\r\n    def _adjust_config(task_dict):\r\n        adjusted_task_dict = {}\r\n        for task_name, task_obj in task_dict.items():\r\n            if isinstance(task_obj, dict):\r\n                adjusted_task_dict = {\r\n                    **adjusted_task_dict,\r\n                    **{task_name: _adjust_config(task_obj)},\r\n                }\r\n\r\n            else:\r\n                if task_obj.get_config(\"output_type\") == \"generate_until\":\r\n                    if gen_kwargs is not None:\r\n                        task_obj.set_config(\r\n                            key=\"generation_kwargs\", value=gen_kwargs, update=True\r\n                        )\r\n\r\n                if predict_only:\r\n                    eval_logger.info(\r\n                        f\"Processing {task_name} in output-only mode. Metrics will not be calculated!\"\r\n                    )\r\n                    # we have to change the class properties post-hoc. This is pretty hacky.\r\n                    task_obj.override_metric(metric_name=\"bypass\")\r\n\r\n                # override tasks' fewshot values to the provided num_fewshot arg value\r\n                # except if tasks have it set to 0 manually in their configs--then we should never overwrite that\r\n                if num_fewshot is not None:\r\n                    if (default_num_fewshot := task_obj.get_config(\"num_fewshot\")) == 0:\r\n                        eval_logger.info(\r\n                            f\"num_fewshot has been set to 0 for {task_name} in its config. Manual configuration will be ignored.\"\r\n                        )\r\n                    else:\r\n                        eval_logger.warning(\r\n                            f\"Overwriting default num_fewshot of {task_name} from {default_num_fewshot} to {num_fewshot}\"\r\n                        )\r\n                        task_obj.set_config(key=\"num_fewshot\", value=num_fewshot)\r\n                else:\r\n                    # if num_fewshot not provided, and the task does not define a default one, default to 0\r\n                    if (\r\n                        default_num_fewshot := task_obj.get_config(\"num_fewshot\")\r\n                    ) is None:\r\n                        task_obj.set_config(key=\"num_fewshot\", value=0)\r\n                # fewshot_random_seed set for tasks, even with a default num_fewshot (e.g. in the YAML file)\r\n                task_obj.set_fewshot_seed(seed=fewshot_random_seed)\r\n                eval_logger.info(\r\n                    f\"Setting fewshot random generator seed to {fewshot_random_seed}\"\r\n                )\r\n\r\n                adjusted_task_dict[task_name] = task_obj\r\n\r\n        return adjusted_task_dict\r\n\r\n    task_dict = _adjust_config(task_dict)\r\n\r\n    if check_integrity:\r\n        run_task_tests(task_list=tasks)\r\n\r\n    if evaluation_tracker is not None:\r\n        evaluation_tracker.general_config_tracker.log_experiment_args(\r\n            model_source=model,\r\n            model_args=model_args,\r\n            system_instruction=system_instruction,\r\n            chat_template=lm.chat_template(apply_chat_template),\r\n            fewshot_as_multiturn=fewshot_as_multiturn,\r\n        )\r\n\r\n    results = evaluate(\r\n        lm=lm,\r\n        task_dict=task_dict,\r\n        limit=limit,\r\n        random_subsample=random_subsample,\r\n        seed=random_seed,\r\n        cache_requests=cache_requests,\r\n        rewrite_requests_cache=rewrite_requests_cache,\r\n        bootstrap_iters=bootstrap_iters,\r\n        write_out=write_out,\r\n        log_samples=True if predict_only else log_samples,\r\n        system_instruction=system_instruction,\r\n        apply_chat_template=apply_chat_template,\r\n        fewshot_as_multiturn=fewshot_as_multiturn,\r\n        verbosity=verbosity,\r\n    )\r\n\r\n    if lm.rank == 0:\r\n        if isinstance(model, str):\r\n            model_name = model\r\n        elif hasattr(model, \"config\") and hasattr(model.config, \"_name_or_path\"):\r\n            model_name = model.config._name_or_path\r\n        else:\r\n            model_name = type(model).__name__\r\n\r\n        # add info about the model and few shot config\r\n        results[\"config\"] = {\r\n            \"model\": model_name,\r\n            \"model_args\": model_args,\r\n        }\r\n        # add more detailed model info if available\r\n        if isinstance(lm, lm_eval.models.huggingface.HFLM):\r\n            results[\"config\"].update(lm.get_model_info())\r\n        # add info about execution\r\n        results[\"config\"].update(\r\n            {\r\n                \"batch_size\": batch_size,\r\n                \"batch_sizes\": (\r\n                    list(lm.batch_sizes.values()) if hasattr(lm, \"batch_sizes\") else []\r\n                ),\r\n                \"device\": device,\r\n                \"use_cache\": use_cache,\r\n                \"limit\": limit,\r\n                \"bootstrap_iters\": bootstrap_iters,\r\n                \"gen_kwargs\": gen_kwargs,\r\n                \"random_seed\": random_seed,\r\n                \"numpy_seed\": numpy_random_seed,\r\n                \"torch_seed\": torch_random_seed,\r\n                \"fewshot_seed\": fewshot_random_seed,\r\n            }\r\n        )\r\n        results[\"git_hash\"] = get_git_commit_hash()\r\n        results[\"date\"] = start_date\r\n        add_env_info(results)  # additional environment info to results\r\n        add_tokenizer_info(results, lm)  # additional info about tokenizer\r\n        return results\r\n    else:\r\n        return None\r\n\r\n\r\n@positional_deprecated\r\ndef evaluate(\r\n    lm: \"LM\",\r\n    task_dict,\r\n    limit: Optional[int] = None,\r\n    cache_requests: bool = False,\r\n    rewrite_requests_cache: bool = False,\r\n    bootstrap_iters: Optional[int] = 100000,\r\n    write_out: bool = False,\r\n    log_samples: bool = True,\r\n    system_instruction: Optional[str] = None,\r\n    apply_chat_template: Union[bool, str] = False,\r\n    fewshot_as_multiturn: bool = False,\r\n    verbosity: str = \"INFO\",\r\n    random_subsample: bool = False,\r\n    seed: Optional[int] = None,\r\n):\r\n    \"\"\"Instantiate and evaluate a model on a list of tasks.\r\n\r\n    :param lm: obj\r\n        Language Model\r\n    :param task_dict: dict[str, Task]\r\n        Dictionary of tasks. Tasks will be taken to have name type(task).config.task .\r\n    :param limit: int, optional\r\n        Limit the number of examples per task (only use this for testing)\r\n    :param bootstrap_iters:\r\n        Number of iterations for bootstrap statistics, used when calculating stderr. Set to 0 for skipping all stderr calculations.\r\n    :param write_out: bool\r\n        If True, write out an example document and model input for checking task integrity\r\n    :param log_samples: bool\r\n        If True, write out all model outputs and documents for per-sample measurement and post-hoc analysis\r\n    :param system_instruction: str\r\n        System instruction to be applied to the prompt\r\n    :param apply_chat_template: Union[bool, str]\r\n        Specifies whether to apply a chat template to the prompt.\r\n        - If set to True, the default chat template is applied.\r\n        - If set to a string, applies the specified chat template by name.\r\n        Defaults to False (no chat template applied).\r\n    :param fewshot_as_multiturn: bool\r\n        Whether to provide the fewshot examples as a multiturn conversation or a single user turn.\r\n    :return\r\n        Dictionary of results\r\n    \"\"\"\r\n\r\n    eval_logger.setLevel(getattr(logging, f\"{verbosity}\"))\r\n\r\n    # tracks all Instances/requests a model must generate output on.\r\n    requests = defaultdict(list)\r\n    # stores the amount to pad out reqs per req. type so that\r\n    # number of fwd passes per distributed rank is equal\r\n    padding_requests = defaultdict(int)\r\n\r\n    # get lists of group hierarchy and each type of request\r\n    eval_tasks = get_task_list(task_dict)\r\n    if not log_samples:\r\n        if not all(\r\n            \"bypass\" not in getattr(task_output.task, \"_metric_fn_list\", {}).keys()\r\n            for task_output in eval_tasks\r\n        ):\r\n            raise ValueError(\"log_samples must be True for 'bypass' metric-only tasks\")\r\n\r\n    # validation check: are we running multimodal task <-> non-multimodal model class, or vice-versa.\r\n    incompatible_tasks = []\r\n    for task_output in eval_tasks:\r\n        task: Task = task_output.task\r\n\r\n        if getattr(lm, \"MULTIMODAL\", False) != getattr(task, \"MULTIMODAL\", False):\r\n            incompatible_tasks.append(task_output.task_name)\r\n    if len(incompatible_tasks) > 0:\r\n        if not getattr(lm, \"MULTIMODAL\", False):\r\n            raise ValueError(\r\n                f\"Attempted to run tasks: {incompatible_tasks} which require multimodal input, but the selected model type does not currently implement this. Multimodal support is currently restricted to the ['hf-multimodal', 'vllm-vlm'] model type.\"\r\n            )\r\n        else:\r\n            raise ValueError(\r\n                f\"Attempted to run tasks: {incompatible_tasks} which are text-only, but used a model type which only currently supports multimodal tasks.\"\r\n            )\r\n    # end multimodality validation check\r\n\r\n    # Cache the limit arg.\r\n    limit_arg = limit\r\n    limits = []\r\n    for task_output in eval_tasks:\r\n        task: Task = task_output.task\r\n\r\n        limit = get_sample_size(task, limit_arg)\r\n        limits.append(limit)\r\n        task.build_all_requests(\r\n            limit=limit,\r\n            random_subsample=random_subsample,\r\n            seed=seed,\r\n            rank=lm.rank,\r\n            world_size=lm.world_size,\r\n            cache_requests=cache_requests,\r\n            rewrite_requests_cache=rewrite_requests_cache,\r\n            system_instruction=system_instruction,\r\n            apply_chat_template=bool(apply_chat_template),\r\n            fewshot_as_multiturn=fewshot_as_multiturn,\r\n            chat_template=getattr(lm, \"apply_chat_template\")\r\n            if apply_chat_template\r\n            else None,\r\n            tokenizer_name=getattr(lm, \"tokenizer_name\", \"\")\r\n            if apply_chat_template\r\n            else \"\",\r\n        )\r\n        eval_logger.debug(\r\n            f\"Task: {task_output.task_name}; number of requests on this rank: {len(task.instances)}\"\r\n        )\r\n        if write_out:\r\n            print_writeout(task)\r\n        # aggregate Instances by LM method requested to get output.\r\n        for instance in task.instances:\r\n            reqtype = instance.request_type\r\n            requests[reqtype].append(instance)\r\n\r\n        if lm.world_size > 1:\r\n            instances_rnk = torch.tensor(len(task._instances), device=lm.device)\r\n            gathered_item = (\r\n                lm.accelerator.gather(instances_rnk).cpu().detach().numpy().tolist()\r\n            )\r\n            # \"multiple_choice\" task types dispatch (several) \"loglikelihood\" request types\r\n            reqtype = (\r\n                \"loglikelihood\"\r\n                if task.OUTPUT_TYPE == \"multiple_choice\"\r\n                else task.OUTPUT_TYPE\r\n            )\r\n            # compute number of pseudo-batches to pad with (FSDP/DDP require even batches among ranks)\r\n            numpad = max(gathered_item) - gathered_item[lm.rank]\r\n            # todo: may not account for padding in cases like SquadV2 which has multiple req types\r\n            padding_requests[reqtype] += numpad\r\n\r\n    ### Run LM on inputs, get all outputs ###\r\n    # execute each type of request\r\n    for reqtype, reqs in requests.items():\r\n        eval_logger.info(f\"Running {reqtype} requests\")\r\n        # create `K` copies of each request `req` based off `K = req.repeats`\r\n        cloned_reqs = []\r\n        for req in reqs:\r\n            cloned_reqs.extend([req] * req.repeats)\r\n\r\n        if (lm.world_size > 1) and (padding_requests[reqtype] > 0):\r\n            for _ in range(padding_requests[reqtype]):\r\n                cloned_reqs.extend([req] * req.repeats)\r\n\r\n        # run requests through model\r\n        resps = getattr(lm, reqtype)(cloned_reqs)\r\n        # bp()\r\n        # put responses from model into a list of length K for each request.\r\n        for x, req in zip(resps, cloned_reqs):\r\n            req.resps.append(x)\r\n\r\n        if lm.world_size > 1:\r\n            lm.accelerator.wait_for_everyone()\r\n\r\n    RANK = lm.rank\r\n    WORLD_SIZE = lm.world_size\r\n    ### Postprocess outputs ###\r\n    # TODO: del model here, maybe (idea: allow user to specify device of e.g. reward model separately)\r\n    for task_output, limit in zip(eval_tasks, limits):\r\n        task = task_output.task\r\n        task.apply_filters()\r\n\r\n        ### Collect values of metrics on all datapoints ###\r\n        # # unpack results and sort back in order and return control to Task\r\n        # TODO: make it possible to use a different metric per filter\r\n        # Pre-process task.instances to group by doc_id\r\n        instances_by_doc_id = defaultdict(list)\r\n        for instance in task.instances:\r\n            instances_by_doc_id[instance.doc_id].append(instance)\r\n        # Sort instances within each group\r\n        for instances in instances_by_doc_id.values():\r\n            instances.sort(key=lambda x: x.idx)\r\n        # iterate over different filters used\r\n        for filter_key in task.instances[0].filtered_resps.keys():\r\n            doc_iterator = task.doc_iterator(\r\n                rank=RANK, limit=limit, world_size=WORLD_SIZE\r\n            )\r\n            # doc_iterator are the benchmark samples\r\n            for doc_id, doc in doc_iterator:\r\n                requests = instances_by_doc_id[doc_id]\r\n                # filter_key is e.g. 'score-first', 'maj@64', 'maj@16', 'maj@8', 'cov@64', 'cov@16', 'cov@8'\r\n                metrics = task.process_results(\r\n                    doc, [req.filtered_resps[filter_key] for req in requests]\r\n                )\r\n                if log_samples:\r\n                    target = task.doc_to_target(doc)\r\n                    example = {\r\n                        \"doc_id\": doc_id,\r\n                        \"doc\": doc,\r\n                        \"target\": target,\r\n                        \"arguments\": [req.args for req in requests],\r\n                        \"resps\": [req.resps for req in requests],\r\n                        \"filtered_resps\": [\r\n                            req.filtered_resps[filter_key] for req in requests\r\n                        ],\r\n                        \"doc_hash\": hash_string(\r\n                            json.dumps(\r\n                                requests[0].doc,\r\n                                indent=2,\r\n                                default=handle_non_serializable,\r\n                                ensure_ascii=False,\r\n                            )\r\n                        ),\r\n                        \"prompt_hash\": hash_string(requests[0].arguments[0]),\r\n                        \"target_hash\": hash_string(str(target)),\r\n                    }\r\n                    example.update(metrics)\r\n                    task_output.logged_samples.append(example)\r\n                for metric, value in metrics.items():\r\n                    task_output.sample_metrics[(metric, filter_key)].append(value)\r\n\r\n    if WORLD_SIZE > 1:\r\n        # if multigpu, then gather data across all ranks to rank 0\r\n        # first gather logged samples across all ranks\r\n        for task_output in eval_tasks:\r\n            if log_samples:\r\n                # for task_name, task_samples in list(samples.items()):\r\n                full_samples = [None] * WORLD_SIZE if RANK == 0 else None\r\n                torch.distributed.gather_object(\r\n                    obj=task_output.logged_samples,\r\n                    object_gather_list=full_samples,\r\n                    dst=0,\r\n                )\r\n\r\n                if RANK == 0:\r\n                    task_output.logged_samples = list(\r\n                        itertools.chain.from_iterable(full_samples)\r\n                    )\r\n\r\n            # then collect metrics across all ranks\r\n            for metrics in task_output.sample_metrics:\r\n                metric_list = [None] * WORLD_SIZE if RANK == 0 else None\r\n                torch.distributed.gather_object(\r\n                    obj=task_output.sample_metrics[metrics],\r\n                    object_gather_list=metric_list,\r\n                    dst=0,\r\n                )\r\n                if RANK == 0:\r\n                    task_output.sample_metrics[metrics] = list(\r\n                        itertools.chain.from_iterable(metric_list)\r\n                    )\r\n\r\n    if RANK == 0:\r\n        ### Aggregate results over all datapoints ###\r\n        # aggregate results ; run bootstrap CIs\r\n        for task_output in eval_tasks:\r\n            task_output.calculate_aggregate_metric(bootstrap_iters=bootstrap_iters)\r\n        (\r\n            results,\r\n            samples,\r\n            configs,\r\n            versions,\r\n            num_fewshot,\r\n            higher_is_better,\r\n        ) = consolidate_results(eval_tasks)\r\n\r\n        ### Calculate group metrics ###\r\n        if bool(results):\r\n            results, versions, show_group_table, *_ = consolidate_group_results(\r\n                results, versions, task_dict\r\n            )\r\n\r\n        results_agg, group_agg = prepare_print_tasks(task_dict, results)\r\n        subtask_list = get_subtask_list(task_dict)\r\n\r\n        # collect all higher_is_better values for metrics\r\n        # in the group's subtasks.\r\n        # TODO: clean this up ; unify with the below metric_list loop?\r\n        _higher_is_better = {}\r\n        for group, task_list in subtask_list.items():\r\n            if (\r\n                len(task_list) != 0\r\n            ):  # subtask list will list \"task_name\": [] for solo tasks\r\n                for task in task_list:\r\n                    for m, h in higher_is_better[task].items():\r\n                        if m not in _higher_is_better.keys():\r\n                            _higher_is_better[m] = h\r\n\r\n                        if (\r\n                            m in _higher_is_better\r\n                            and _higher_is_better[m] is not None\r\n                            and _higher_is_better[m] != h\r\n                        ):\r\n                            eval_logger.warning(\r\n                                f\"Higher_is_better values for metric {m} in group {group} are not consistent. Defaulting to None.\"\r\n                            )\r\n                            _higher_is_better[m] = None\r\n                higher_is_better[group] = _higher_is_better\r\n\r\n        results_dict = {\r\n            \"results\": dict(results_agg.items()),\r\n            **(\r\n                {\"groups\": dict(group_agg.items())}\r\n                if (bool(group_agg) & show_group_table)\r\n                else {}\r\n            ),\r\n            \"group_subtasks\": dict(reversed(subtask_list.items())),\r\n            \"configs\": dict(sorted(configs.items())),\r\n            \"versions\": dict(sorted(versions.items())),\r\n            \"n-shot\": dict(sorted(num_fewshot.items())),\r\n            \"higher_is_better\": dict(sorted(higher_is_better.items())),\r\n            \"n-samples\": {\r\n                task_output.task_name: {\r\n                    \"original\": len(task_output.task.eval_docs),\r\n                    \"effective\": min(\r\n                        limit if limit else len(task_output.task.eval_docs),\r\n                        len(task_output.task.eval_docs),\r\n                    ),\r\n                }\r\n                for task_output, limit in zip(eval_tasks, limits)\r\n            },\r\n        }\r\n        if log_samples:\r\n            results_dict[\"samples\"] = dict(samples)\r\n        return results_dict\r\n\r\n    else:\r\n        return None\r\n\r\n\r\ndef request_caching_arg_to_dict(cache_requests: str) -> dict:\r\n    request_caching_args = {\r\n        \"cache_requests\": cache_requests in {\"true\", \"refresh\"},\r\n        \"rewrite_requests_cache\": cache_requests == \"refresh\",\r\n        \"delete_requests_cache\": cache_requests == \"delete\",\r\n    }\r\n\r\n    return request_caching_args\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/models/sglang.py", "content": "import copy\r\nimport os\r\nimport sys\r\nfrom importlib.metadata import version\r\nfrom importlib.util import find_spec\r\nfrom typing import TYPE_CHECKING, Dict, List, Literal, Optional, Tuple, Union\r\n\r\nfrom more_itertools import distribute\r\nfrom packaging.version import parse as parse_version\r\nfrom tqdm import tqdm\r\n\r\nfrom lm_eval.api.instance import Instance\r\nfrom lm_eval.api.model import TemplateLM\r\nfrom lm_eval.api.registry import register_model\r\nfrom lm_eval.models.utils import Collator, configure_pad_token, undistribute\r\nfrom lm_eval.utils import (\r\n    eval_logger,\r\n    get_rolling_token_windows,\r\n    make_disjoint_window,\r\n)\r\n# check the path and import the o1 module: /home/weijias/o1/search/tot\r\nif os.path.exists(\"/home/weijias/o1/search/tot\"):\r\n    sys.path.append(\"/home/weijias/o1/search/tot\")\r\n\r\ntry:\r\n    import ray\r\n    from vllm import LLM, SamplingParams\r\n    from vllm.transformers_utils.tokenizer import get_tokenizer\r\nexcept ModuleNotFoundError:\r\n    pass\r\n\r\nif TYPE_CHECKING:\r\n    pass\r\n\r\nif int(os.getenv(\"O1INFERENCE\", 0)):\r\n    import sys\r\n    sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))))\r\n    # Make sure that `from rebase_utils import *` works\r\n    sys.path.append(os.path.join((os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))))), \"tot\"))\r\n    from tot.o1_rebase_text import o1\r\n\r\neval_logger = eval_logger\r\n\r\n\r\n@register_model(\"sglang\")\r\nclass SGLang(TemplateLM):\r\n    _DEFAULT_MAX_LENGTH = 2048\r\n\r\n    def __init__(\r\n        self,\r\n        pretrained: str,\r\n        dtype: Literal[\"float16\", \"bfloat16\", \"float32\", \"auto\"] = \"auto\",\r\n        revision: Optional[str] = None,\r\n        trust_remote_code: Optional[bool] = False,\r\n        tokenizer: Optional[str] = None,\r\n        tokenizer_mode: Literal[\"auto\", \"slow\"] = \"auto\",\r\n        tokenizer_revision: Optional[str] = None,\r\n        add_bos_token: Optional[bool] = False,\r\n        prefix_token_id: Optional[int] = None,\r\n        tensor_parallel_size: int = 1,\r\n        quantization: Optional[str] = None,\r\n        max_gen_toks: int = 256,\r\n        swap_space: int = 4,\r\n        batch_size: Union[str, int] = 1,\r\n        max_batch_size=None,\r\n        max_length: int = None,\r\n        max_model_len: int = None,\r\n        seed: int = 1234,\r\n        gpu_memory_utilization: float = 0.9,\r\n        device: str = \"cuda\",\r\n        data_parallel_size: int = 1,\r\n        **kwargs,\r\n    ):\r\n        super().__init__()\r\n\r\n        if not find_spec(\"vllm\"):\r\n            raise Exception(\r\n                \"attempted to use 'vllm' LM type, but package `vllm` is not installed. \"\r\n                \"Please install vllm via `pip install lm-eval[vllm]` or `pip install -e .[vllm]`\"\r\n            )\r\n\r\n        assert \"cuda\" in device or device is None, \"vLLM only supports CUDA\"\r\n        assert (\r\n            max_length is None or max_model_len is None\r\n        ), \"Either max_length or max_model_len may be provided, but not both\"\r\n\r\n        self._max_length = max_model_len if max_model_len is not None else max_length\r\n        self.tensor_parallel_size = int(tensor_parallel_size)\r\n        self.data_parallel_size = int(data_parallel_size)\r\n        self.model_args = {\r\n            \"model\": pretrained,\r\n            \"gpu_memory_utilization\": float(gpu_memory_utilization),\r\n            \"revision\": revision,\r\n            \"dtype\": dtype,\r\n            \"tokenizer\": tokenizer,\r\n            \"tokenizer_mode\": tokenizer_mode,\r\n            \"tokenizer_revision\": tokenizer_revision,\r\n            \"trust_remote_code\": trust_remote_code,\r\n            \"tensor_parallel_size\": int(tensor_parallel_size),\r\n            \"max_model_len\": int(self._max_length) if self._max_length else None,\r\n            \"swap_space\": int(swap_space),\r\n            \"quantization\": quantization,\r\n            \"seed\": int(seed),\r\n        }\r\n        self.model_args.update(kwargs)\r\n        self.batch_size = (\r\n            \"auto\"\r\n            if isinstance(batch_size, str) and \"auto\" in batch_size\r\n            else batch_size\r\n        )\r\n        if self.data_parallel_size <= 1:\r\n            if int(os.getenv(\"O1INFERENCE\", 0)):\r\n                print(\"use o1 inference\")\r\n                step_eos = \"<|reserved_special_token_2|>\"\r\n                think_eos = \"<|reserved_special_token_1|>\"\r\n                answer_eos = \"<|eot_id|>\"\r\n                num_parallel_steps = int(os.getenv(\"O1INFERENCE_NUM_PARALLEL_STEPS\", 1))\r\n                print(f\"hardcoding the model to be o1; num_parallel_steps: {num_parallel_steps}\")\r\n                self.model = o1(tokenizer=None, num_parallel_steps=num_parallel_steps, step_eos=step_eos, think_eos=think_eos, answer_eos=answer_eos, **kwargs)\r\n            else:\r\n                self.model = LLM(**self.model_args)\r\n        else:\r\n            eval_logger.warning(\r\n                \"You might experience occasional issues with model weight downloading when data_parallel is in use. To ensure stable performance, run with data_parallel_size=1 until the weights are downloaded and cached.\"\r\n            )\r\n            self.model_args[\"worker_use_ray\"] = True\r\n            self.batch_size = \"auto\"\r\n            eval_logger.info(\"Manual batching is not compatible with data parallelism.\")\r\n\r\n            from transformers import AutoConfig\r\n\r\n            self._config = AutoConfig.from_pretrained(\r\n                pretrained, trust_remote_code=trust_remote_code, revision=revision\r\n            )\r\n        self.tokenizer = get_tokenizer(\r\n            tokenizer if tokenizer else pretrained,\r\n            tokenizer_mode=tokenizer_mode,\r\n            trust_remote_code=trust_remote_code,\r\n            tokenizer_revision=tokenizer_revision,\r\n        )\r\n        self.tokenizer = configure_pad_token(self.tokenizer)\r\n        self.add_bos_token = add_bos_token\r\n        if \"gemma\" in pretrained.lower():\r\n            self.add_bos_token = True\r\n            eval_logger.info(\r\n                \"Found 'gemma' in model name, a BOS token will be used as Gemma series models underperform without it.\"\r\n            )\r\n\r\n        self.custom_prefix_token_id = prefix_token_id\r\n        if prefix_token_id is not None:\r\n            eval_logger.info(\r\n                f\"Loglikelihood prefix token id used in evaluation: {self.prefix_token_id}\"\r\n            )\r\n\r\n        self._max_gen_toks = max_gen_toks\r\n\r\n\r\n    @property\r\n    def eot_token_id(self):\r\n        # we use EOT because end of *text* is more accurate for what we're doing than end of *sentence*\r\n        return self.tokenizer.eos_token_id\r\n\r\n    @property\r\n    def prefix_token_id(self):\r\n        # it is used as prefix for loglikelihood\r\n        if self.custom_prefix_token_id is not None:\r\n            return self.custom_prefix_token_id\r\n        if self.tokenizer.bos_token_id is not None:\r\n            return self.tokenizer.bos_token_id\r\n        return self.tokenizer.eos_token_id\r\n\r\n    @property\r\n    def max_length(self):\r\n        if self._max_length:  # if max length manually set, return it\r\n            return self._max_length\r\n        if self.data_parallel_size <= 1:\r\n            return self.model.llm_engine.model_config.max_model_len\r\n        else:\r\n            seqlen_config_attrs = (\"n_positions\", \"max_position_embeddings\", \"n_ctx\")\r\n            for attr in seqlen_config_attrs:\r\n                if hasattr(self._config, attr):\r\n                    return getattr(self._config, attr)\r\n            if hasattr(self.tokenizer, \"model_max_length\"):\r\n                if self.tokenizer.model_max_length == 1000000000000000019884624838656:\r\n                    return self._DEFAULT_MAX_LENGTH\r\n                return self.tokenizer.model_max_length\r\n            return self._DEFAULT_MAX_LENGTH\r\n\r\n    @property\r\n    def max_gen_toks(self):\r\n        return self._max_gen_toks\r\n\r\n    def apply_chat_template(self, chat_history: List[Dict[str, str]]) -> str:\r\n        \"\"\"\r\n        Method to apply a chat template to a list of chat history between user and model.\r\n        \"\"\"\r\n        return self.tokenizer.apply_chat_template(\r\n            chat_history, tokenize=False, add_generation_prompt=True\r\n        )\r\n\r\n    @property\r\n    def tokenizer_name(self) -> str:\r\n        return self.tokenizer.name_or_path.replace(\"/\", \"__\")\r\n\r\n    def tok_encode(\r\n        self,\r\n        string: Union[str, List[str]],\r\n        left_truncate_len: int = None,\r\n        add_special_tokens: bool = False,\r\n        truncation: bool = False,\r\n    ) -> Union[List[int], List[List[int]]]:\r\n        if not add_special_tokens:\r\n            add_special_tokens = False or self.add_bos_token\r\n        encoding: Union[List[List[int]], List[int]] = self.tokenizer(\r\n            string,\r\n            add_special_tokens=add_special_tokens,\r\n            truncation=truncation,\r\n            return_attention_mask=False,\r\n        ).input_ids\r\n\r\n        # left-truncate the encoded context to be at most `left_truncate_len` tokens long\r\n        if left_truncate_len:\r\n            if not isinstance(string, str):\r\n                encoding = [enc[-left_truncate_len:] for enc in encoding]\r\n            else:\r\n                encoding = encoding[-left_truncate_len:]\r\n\r\n        return encoding\r\n\r\n    def _model_generate(\r\n        self,\r\n        requests: List[List[int]] = None,\r\n        generate: bool = False,\r\n        max_tokens: int = None,\r\n        stop: Optional[List[str]] = None,\r\n        **kwargs,\r\n    ):\r\n        texts = self.tokenizer.batch_decode(requests)\r\n        # import pdb; pdb.set_trace()\r\n        outputs = self.model.generate(texts, **kwargs)\r\n        return outputs\r\n\r\n    def loglikelihood_rolling(\r\n        self, requests: List[Instance], disable_tqdm: bool = False\r\n    ) -> List[float]:\r\n        loglikelihoods = []\r\n\r\n        for (string,) in tqdm([req.args for req in requests], disable=disable_tqdm):\r\n            rolling_token_windows = list(\r\n                map(\r\n                    make_disjoint_window,\r\n                    get_rolling_token_windows(\r\n                        token_list=self.tok_encode(string),\r\n                        prefix_token=self.prefix_token_id,\r\n                        # max_seq_len - (1 for context)\r\n                        max_seq_len=self.max_length - 1,\r\n                        context_len=1,\r\n                    ),\r\n                )\r\n            )\r\n\r\n            rolling_token_windows = [(None,) + x for x in rolling_token_windows]\r\n\r\n            string_nll = self._loglikelihood_tokens(\r\n                rolling_token_windows,\r\n            )\r\n\r\n            # discard is_greedy\r\n            string_nll = [x[0] for x in string_nll]\r\n\r\n            string_nll = sum(string_nll)\r\n            loglikelihoods.append(string_nll)\r\n\r\n            # cache this loglikelihood_rolling request\r\n            self.cache_hook.add_partial(\"loglikelihood_rolling\", (string,), string_nll)\r\n\r\n        return loglikelihoods\r\n\r\n    def generate_until(\r\n        self, requests: List[Instance], disable_tqdm: bool = False\r\n    ) -> List[str]:\r\n        res = []\r\n        # import pdb; pdb.set_trace()\r\n        # batch tokenize contexts\r\n        context, all_gen_kwargs = zip(*(req.args for req in requests))\r\n        # from ipdb import set_trace as bp; bp()\r\n        context = list(context)\r\n        context_encoding: List[List[int]] = self.tok_encode(\r\n            context, add_special_tokens=self.add_bos_token\r\n        )\r\n        requests = [\r\n            ((a, b), c) for a, b, c in zip(context, context_encoding, all_gen_kwargs)\r\n        ]\r\n\r\n        def _collate_gen(_requests):\r\n            # the negative sign on len(toks) sorts descending - this has a few advantages:\r\n            # - time estimates will always be over not underestimates, which is more useful for planning\r\n            # - to know the size of a batch when going through the list, you know the first one is always the batch\r\n            #   padded context length. this is useful to simplify the batching logic and more importantly to make\r\n            #   automatic adaptive batches much much easier to implement\r\n            # - any OOMs will happen right away rather than near the end\r\n            return -len(_requests[0][1]), _requests[0][0]\r\n\r\n        # we group requests by their generation_kwargs,\r\n        # so that we don't try to execute e.g. greedy sampling and temp=0.8 sampling\r\n        # in the same batch.\r\n        re_ords = Collator(requests, _collate_gen, group_by=\"gen_kwargs\")\r\n        chunks = re_ords.get_batched(\r\n            n=int(self.batch_size) if self.batch_size != \"auto\" else 0, batch_fn=None\r\n        )\r\n\r\n        pbar = tqdm(\r\n            total=len(requests),\r\n            disable=(disable_tqdm or (self.rank != 0)),\r\n            desc=\"Running generate_until requests\",\r\n        )\r\n        # for each different set of kwargs, we execute all requests, by batch.\r\n        for chunk in chunks:\r\n            context_and_encoding, all_gen_kwargs = zip(*chunk)\r\n            context, context_encoding = zip(*context_and_encoding)\r\n            # we assume all gen kwargs in the batch are the same\r\n            # this is safe to assume because the `grouper` object ensures it.\r\n            gen_kwargs = all_gen_kwargs[0]\r\n            # unpack our keyword arguments.\r\n            until = None\r\n            if isinstance(gen_kwargs, dict):\r\n                kwargs = copy.deepcopy(gen_kwargs)  # edge case for repeats > 1\r\n                if \"until\" in kwargs.keys():\r\n                    until = kwargs.pop(\"until\")\r\n                    if isinstance(until, str):\r\n                        until = [until]\r\n                    elif not isinstance(until, list):\r\n                        raise ValueError(\r\n                            f\"Expected `kwargs['until']` to be of type Union[str,list] but got {until}\"\r\n                        )\r\n            else:\r\n                raise ValueError(\r\n                    f\"Expected `kwargs` to be of type `dict` but got {gen_kwargs}\"\r\n                )\r\n            # add EOS token to stop sequences\r\n            eos = self.tokenizer.decode(self.eot_token_id)\r\n            if not until:\r\n                until = [eos]\r\n            else:\r\n                until.append(eos)\r\n            if \"max_gen_toks\" in kwargs.keys():\r\n                max_gen_toks = kwargs.pop(\"max_gen_toks\")\r\n            else:\r\n                max_gen_toks = self.max_gen_toks\r\n\r\n            # set the max length in tokens of inputs (\"context_enc\")\r\n            # max len for inputs = max length, minus room to generate the max new tokens\r\n            max_ctx_len = self.max_length - max_gen_toks\r\n            context_encoding = [x[-max_ctx_len:] for x in context_encoding]\r\n\r\n            # perform batched generation\r\n            cont = self._model_generate(\r\n                requests=context_encoding,\r\n                generate=True,\r\n                max_tokens=max_gen_toks,\r\n                stop=until,\r\n                **kwargs,\r\n            )\r\n\r\n            # cache generations\r\n            for generated_text, context in zip(cont, context):\r\n                # generated_text = output.outputs[0].text\r\n                res.append(generated_text)\r\n                self.cache_hook.add_partial(\r\n                    \"generate_until\", (context, gen_kwargs), generated_text\r\n                )\r\n                pbar.update(1)\r\n\r\n        pbar.close()\r\n        # reorder all group of results back to original unsorted form\r\n        return re_ords.get_original(res)\r\n\r\n    def _loglikelihood_tokens(\r\n        self,\r\n        requests: List[Tuple[Tuple[str, str], List[int], List[int]]],\r\n        disable_tqdm: bool = False,\r\n    ) -> List[Tuple[float, bool]]:\r\n        res = []\r\n\r\n        def _collate(x):\r\n            toks = x[1] + x[2]\r\n            return -len(toks), tuple(toks)\r\n\r\n        # Reorder requests by length and batch\r\n        re_ord = Collator(requests, sort_fn=_collate)\r\n        chunks = re_ord.get_batched(\r\n            n=int(self.batch_size) if self.batch_size != \"auto\" else 0, batch_fn=None\r\n        )\r\n\r\n        pbar = tqdm(\r\n            total=len(requests),\r\n            disable=disable_tqdm,\r\n            desc=\"Running loglikelihood requests\",\r\n        )\r\n        for chunk in chunks:\r\n            inputs = []\r\n            ctxlens = []\r\n            for cache_key, context_enc, continuation_enc in chunk:\r\n                inp = (context_enc + continuation_enc)[-(self.max_length) :]\r\n                ctxlen = len(context_enc) - max(\r\n                    0, len(context_enc) + len(continuation_enc) - (self.max_length)\r\n                )\r\n\r\n                inputs.append(inp)\r\n                ctxlens.append(ctxlen)\r\n\r\n            outputs = self._model_generate(requests=inputs, generate=False)\r\n\r\n            for output, ctxlen, (cache_key, _, _), inp in zip(\r\n                outputs, ctxlens, chunk, inputs\r\n            ):\r\n                answer = self._parse_logprobs(\r\n                    tokens=inp,\r\n                    outputs=output,\r\n                    ctxlen=ctxlen,\r\n                )\r\n\r\n                res.append(answer)\r\n\r\n                if cache_key is not None:\r\n                    # special case: loglikelihood_rolling produces a number of loglikelihood requests\r\n                    # all with cache key None. instead do add_partial on the per-example level\r\n                    # in the loglikelihood_rolling() function for those.\r\n                    self.cache_hook.add_partial(\"loglikelihood\", cache_key, answer)\r\n                pbar.update(1)\r\n        pbar.close()\r\n        return re_ord.get_original(res)\r\n\r\n    @staticmethod\r\n    def _parse_logprobs(tokens: List, outputs, ctxlen: int) -> Tuple[float, bool]:\r\n        \"\"\"Process logprobs and tokens.\r\n\r\n        :param tokens: list\r\n            Input tokens (potentially left-truncated)\r\n        :param outputs: RequestOutput\r\n            Contains prompt_logprobs\r\n        :param ctxlen: int\r\n            Length of context (so we can slice them away and only keep the predictions)\r\n        :return:\r\n            continuation_logprobs: float\r\n                Log probabilities of continuation tokens\r\n            is_greedy: bool\r\n                Whether argmax matches given continuation exactly\r\n        \"\"\"\r\n\r\n        # The first entry of prompt_logprobs is None because the model has no previous tokens to condition on.\r\n        continuation_logprobs_dicts = outputs.prompt_logprobs\r\n\r\n        def coerce_logprob_to_num(logprob):\r\n            # vLLM changed the return type of logprobs from float\r\n            # to a Logprob object storing the float value + extra data\r\n            # (https://github.com/vllm-project/vllm/pull/3065).\r\n            # If we are dealing with vllm's Logprob object, return\r\n            # the logprob value stored as an attribute. Otherwise,\r\n            # return the object itself (which should be a float\r\n            # for older versions of vLLM).\r\n            return getattr(logprob, \"logprob\", logprob)\r\n\r\n        continuation_logprobs_dicts = [\r\n            {\r\n                token: coerce_logprob_to_num(logprob)\r\n                for token, logprob in logprob_dict.items()\r\n            }\r\n            if logprob_dict is not None\r\n            else None\r\n            for logprob_dict in continuation_logprobs_dicts\r\n        ]\r\n\r\n        # Calculate continuation_logprobs\r\n        # assume ctxlen always >= 1\r\n        continuation_logprobs = sum(\r\n            logprob_dict.get(token)\r\n            for token, logprob_dict in zip(\r\n                tokens[ctxlen:], continuation_logprobs_dicts[ctxlen:]\r\n            )\r\n        )\r\n\r\n        # Determine if is_greedy\r\n        is_greedy = True\r\n        for token, logprob_dict in zip(\r\n            tokens[ctxlen:], continuation_logprobs_dicts[ctxlen:]\r\n        ):\r\n            # Get the token with the maximum log probability from the logprob_dict\r\n            if logprob_dict:  # Ensure the logprob_dict is not None\r\n                top_token = max(logprob_dict, key=logprob_dict.get)\r\n                if top_token != token:\r\n                    is_greedy = False\r\n                    break\r\n\r\n        return continuation_logprobs, is_greedy\r\n\r\n    @staticmethod\r\n    def modify_gen_kwargs(kwargs: dict) -> dict:\r\n        # sampling_params\r\n        do_sample = kwargs.pop(\"do_sample\", None)\r\n        if do_sample is False and \"temperature\" not in kwargs:\r\n            eval_logger.debug(\r\n                \"Got `do_sample=False` and no temperature value, setting VLLM temperature to 0.0 ...\"\r\n            )\r\n            kwargs[\"temperature\"] = 0.0\r\n        # hf defaults\r\n        kwargs[\"skip_special_tokens\"] = kwargs.get(\"skip_special_tokens\", False)\r\n        kwargs[\"spaces_between_special_tokens\"] = kwargs.get(\r\n            \"spaces_between_special_tokens\", False\r\n        )\r\n        return kwargs\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/caching/__init__.py", "content": ""}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/filters/decontamination.py", "content": "from lm_eval.api.filter import Filter\r\nfrom lm_eval.api.registry import register_filter\r\n\r\n\r\n@register_filter(\"decontaminate\")\r\nclass DecontaminationFilter(Filter):\r\n    \"\"\"\r\n    A filter which evaluates\r\n    \"\"\"\r\n\r\n    name = \"track_decontamination\"\r\n\r\n    def __init__(self, path) -> None:\r\n        \"\"\"\r\n\r\n        TODO: make sure only ever run one time on the train set (should this be cached as a class var? keyed by value for \"path\").\r\n        should further cache result on a given (task_name, doc_id)\r\n        \"\"\"\r\n        self._decontam_results = None\r\n\r\n    def apply(self, resps, docs) -> None:\r\n        \"\"\"\r\n        Return {\"no_contamination\", \"only_contamination\"} keys for the 2 different subsets\r\n        \"\"\"\r\n        pass\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/evaluator_utils.py", "content": "import collections\r\nimport math\r\nimport pathlib\r\nimport sys\r\nfrom typing import List, Optional, Tuple, Union\r\n\r\nfrom lm_eval.api.group import ConfigurableGroup\r\nfrom lm_eval.api.metrics import (\r\n    aggregate_subtask_metrics,\r\n    pooled_sample_stderr,\r\n    stderr_for_metric,\r\n)\r\nfrom lm_eval.api.task import Task\r\nfrom lm_eval.utils import eval_logger, positional_deprecated\r\n\r\n\r\nclass TaskOutput:\r\n    \"\"\"\r\n    Wrapper class for Task outputs.It contains various attributes and methods to manage and calculate metrics for the task.\r\n\r\n        Attributes:\r\n            task (object): The task object.\r\n            task_name (str): The name of the task.\r\n            task_config (dict): The configuration of the task.\r\n            version (str): The version of the task.\r\n            group_name (str): The name of the task group.\r\n            n_shot (int): The number of shots for the task.\r\n            task_alias (str): The alias of the task.\r\n            group_alias (str): The alias of the task group.\r\n            is_group (bool): Indicates if the task is a group.\r\n            logged_samples (list): The list of logged samples.\r\n            sample_len (int): The length of the samples.\r\n            sample_metrics (defaultdict): The dictionary of samples' metrics.\r\n            agg_metrics (defaultdict): The dictionary of aggregate metrics.\r\n\r\n        Methods:\r\n            from_taskdict(cls, task_name: str, task):\r\n                Creates a TaskOutput instance from a task dictionary.\r\n\r\n            calculate_aggregate_metric(bootstrap_iters=100000) -> None:\r\n                Calculates the aggregate metrics for the task.\r\n    \"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        task=None,\r\n        task_name=None,\r\n        task_config=None,\r\n        version=None,\r\n        group_name=None,\r\n        n_shot=None,\r\n        task_alias=None,\r\n        group_alias=None,\r\n        is_group=None,\r\n    ):\r\n        self.task = task\r\n        self.task_config = task_config\r\n        self.task_name = task_name\r\n        self.group_name = group_name\r\n        self.version = version\r\n        self.n_shot = n_shot\r\n        self.task_alias = task_alias\r\n        self.group_alias = group_alias\r\n        self.is_group = is_group\r\n        self.logged_samples = []\r\n        self.sample_len = None\r\n        self.sample_metrics = collections.defaultdict(list)\r\n        self.agg_metrics = collections.defaultdict(list)\r\n\r\n    @classmethod\r\n    def from_taskdict(cls, task_name: str, task):\r\n        if isinstance(task, tuple):\r\n            group_name, task = task\r\n        else:\r\n            group_name = None\r\n        if not task:\r\n            # these gets filtered out in get_task_list\r\n            # once they are added to group hierarchy\r\n            is_group = True\r\n            return cls(\r\n                task=task, task_name=task_name, is_group=is_group, group_name=group_name\r\n            )\r\n        version = task.VERSION\r\n        task_config = dict(task.dump_config())\r\n        if (n_shot := task_config.get(\"num_fewshot\")) == 0:\r\n            n_shot = task_config.get(\"metadata\", {}).get(\"num_fewshot\", 0)\r\n        task_alias = task_config.get(\"alias\")\r\n        group_alias = task_config.get(\"group_alias\")\r\n        return cls(\r\n            task=task,\r\n            task_name=task_name,\r\n            task_config=task_config,\r\n            group_name=group_name,\r\n            version=version,\r\n            n_shot=n_shot,\r\n            task_alias=task_alias,\r\n            group_alias=group_alias,\r\n        )\r\n\r\n    def calculate_aggregate_metric(self, bootstrap_iters=100000) -> None:\r\n        for (metric, filter_key), items in self.sample_metrics.items():\r\n            agg_fn = self.task.aggregation()[metric]\r\n            metric_key = f\"{metric},{filter_key}\"\r\n            self.agg_metrics[metric_key] = agg_fn(items)\r\n            self.sample_len = len(items)  # TODO: same sample size for each metric?\r\n            if isinstance(bootstrap_iters, int):\r\n                stderr_fn = stderr_for_metric(\r\n                    metric=agg_fn,\r\n                    bootstrap_iters=min(bootstrap_iters, 100)\r\n                    if metric in [\"bleu\", \"chrf\", \"ter\"]\r\n                    else bootstrap_iters,\r\n                )\r\n                self.agg_metrics[f\"{metric}_stderr,{filter_key}\"] = (\r\n                    stderr_fn(items) if (stderr_fn and len(items) > 1) else \"N/A\"\r\n                )\r\n            else:\r\n                raise ValueError(\r\n                    f\"Received bootstrap_iters '{bootstrap_iters}' but expected an integer. Set to 0 to turn off stderr calculations.\"\r\n                )\r\n\r\n    def __repr__(self):\r\n        return (\r\n            f\"TaskOutput(task_name={self.task_name}, \"\r\n            f\"group_name={self.group_name}, \"\r\n            f\"version={self.version}, \"\r\n            f\"n_shot={self.n_shot}, \"\r\n            f\"task_alias={self.task_alias}, \"\r\n            f\"group_alias={self.group_alias})\"\r\n        )\r\n\r\n\r\ndef get_task_list(task_dict: dict) -> List[TaskOutput]:\r\n    outputs = []\r\n    for task_name, task_obj in task_dict.items():\r\n        if isinstance(task_obj, dict):\r\n            _outputs = get_task_list(task_obj)\r\n            outputs.extend(_outputs)\r\n        else:\r\n            task_output = TaskOutput.from_taskdict(task_name, task_obj)\r\n            outputs.append(task_output)\r\n\r\n    return outputs\r\n\r\n\r\ndef get_subtask_list(task_dict, task_root=None, depth=0):\r\n    subtask_list = {}\r\n    for group_obj, task_obj in task_dict.items():\r\n        if isinstance(group_obj, ConfigurableGroup):\r\n            # group_name = group_obj.group_name\r\n            group_name = group_obj.group_name\r\n        else:\r\n            group_name = group_obj\r\n        if isinstance(task_obj, dict):\r\n            _subtask_list = get_subtask_list(\r\n                task_obj, task_root=group_name, depth=depth + 1\r\n            )\r\n            if task_root:\r\n                subtask_list.setdefault((task_root, depth), []).extend(\r\n                    [\r\n                        _task\r\n                        for (_task, _depth) in _subtask_list.keys()\r\n                        if (_depth - 1) == depth\r\n                    ]\r\n                )\r\n\r\n            subtask_list = {**subtask_list, **_subtask_list}\r\n        else:\r\n            if isinstance(task_obj, ConfigurableGroup):\r\n                # group_or_task_name = task_obj.group_name\r\n                group_or_task_name = task_obj.group_name\r\n            elif isinstance(task_obj, Task):\r\n                # group_or_task_name = task_obj.task_name\r\n                group_or_task_name = task_obj.task_name\r\n\r\n            if task_root is None:\r\n                subtask_list.setdefault((group_or_task_name, depth), [])\r\n            else:\r\n                subtask_list.setdefault((task_root, depth), []).append(\r\n                    group_or_task_name\r\n                )\r\n\r\n    if depth == 0:\r\n        _subtask_list = {}\r\n        for group_key, task_list in subtask_list.items():\r\n            group_name, depth = group_key\r\n            _subtask_list[group_name] = task_list\r\n        subtask_list = _subtask_list\r\n\r\n    return subtask_list\r\n\r\n\r\ndef print_writeout(task) -> None:\r\n    for inst in task.instances:\r\n        # print the prompt for the first few documents\r\n        if inst.doc_id < 1:\r\n            eval_logger.info(\r\n                f\"Task: {task}; document {inst.doc_id}; context prompt (starting on next line):\\\r\n    \\n{inst.args[0]}\\n(end of prompt on previous line)\\ntarget string or answer choice index (starting on next line):\\n{task.doc_to_target(inst.doc)}\\n(end of target on previous line)\"\r\n            )\r\n            eval_logger.info(f\"Request: {str(inst)}\")\r\n\r\n\r\ndef get_sample_size(task, limit: Optional[int]) -> Union[int, None]:\r\n    if limit is not None:\r\n        limit = (\r\n            int(math.ceil(len(task.eval_docs) * limit)) if limit < 1.0 else int(limit)\r\n        )\r\n    return limit\r\n\r\n\r\ndef prepare_print_tasks(\r\n    task_dict: dict,\r\n    results: dict,\r\n    task_depth=0,\r\n    group_depth=0,\r\n) -> Tuple[dict, dict]:\r\n    \"\"\"\r\n    @param task_dict: Dictionary representing the group hierarchy of tasks. Each key is a group name and its\r\n    value is a list of task names.\r\n    @param results: Dictionary containing the results of each task. Each key is a\r\n    group name and its value is a dictionary of task results.\r\n    @param task_depth: The indentation level for printing the task\r\n    hierarchy. Default is 0.\r\n    @param group_depth: The indentation level for printing the group\r\n    hierarchy. Default is 0.\r\n    @return: A tuple of two dictionaries: results_agg and groups_agg. results_agg contains\r\n    aggregated results for each task, and groups_agg contains aggregated results for each group.\r\n\r\n    Prepares the task hierarchy and aggregates the results for each task and group recursively for printing.\r\n    \"\"\"\r\n\r\n    def _sort_task_dict(task_dict):\r\n        \"\"\"\r\n        Helper utility. Sorts the task dict at the current level of the hierarchy based on alphabetized task name.\r\n        Required so that we end up sorting within each sub-header correctly.\r\n        \"\"\"\r\n\r\n        return dict(\r\n            sorted(\r\n                task_dict.items(),\r\n                key=lambda item: item[0].group_name\r\n                if isinstance(item[0], ConfigurableGroup)\r\n                else item[0],\r\n            )\r\n        )\r\n\r\n    task_agg = collections.defaultdict(dict)\r\n    group_agg = collections.defaultdict(dict)\r\n    task_dict = _sort_task_dict(task_dict)\r\n    for task_or_group_name, task_or_group_obj in task_dict.items():\r\n        tab_string = \" \" * task_depth + \"- \" if task_depth > 0 else \"\"\r\n        if isinstance(task_or_group_name, ConfigurableGroup):\r\n            # string_name = task_or_group_name.group_name\r\n            name = task_or_group_name.group_name\r\n            from_configurable_group = True\r\n            task_or_group_obj = _sort_task_dict(task_or_group_obj)\r\n        elif isinstance(task_or_group_name, str):\r\n            name = task_or_group_name\r\n            if isinstance(task_or_group_obj, Task):\r\n                # string_name = task_or_group_obj.task_name\r\n                name = task_or_group_obj.task_name\r\n            from_configurable_group = False\r\n\r\n        task_agg[name] = results[name].copy()\r\n        if from_configurable_group:\r\n            if task_or_group_name.group_alias is not None:\r\n                alias = task_or_group_name.group_alias\r\n            else:\r\n                alias = task_or_group_name.group\r\n        else:\r\n            if \"alias\" in task_agg[name]:\r\n                alias = task_agg[name][\"alias\"]\r\n            else:\r\n                alias = name\r\n\r\n        task_agg[name][\"alias\"] = tab_string + alias\r\n        if \"samples\" in task_agg[name]:\r\n            task_agg[name].pop(\"samples\")\r\n\r\n        if from_configurable_group and (\" \" not in results[name]):\r\n            group_tab_string = \" \" * group_depth + \"- \" if group_depth > 0 else \"\"\r\n            group_agg[name] = results[name].copy()\r\n            group_agg[name][\"alias\"] = group_tab_string + alias\r\n            if \"samples\" in group_agg[name]:\r\n                group_agg[name].pop(\"samples\")\r\n\r\n        if isinstance(task_or_group_obj, dict):\r\n            task_depth += 1\r\n            group_depth += 1\r\n            _task_agg, _group_agg = prepare_print_tasks(\r\n                task_or_group_obj, results, task_depth, group_depth\r\n            )\r\n            task_agg = {\r\n                **task_agg,\r\n                **_task_agg,\r\n            }\r\n            group_agg = {**group_agg, **_group_agg}\r\n            task_depth -= 1\r\n            group_depth -= 1\r\n    return task_agg, group_agg\r\n\r\n\r\ndef consolidate_results(\r\n    eval_tasks: List[TaskOutput],\r\n) -> Tuple[dict, dict, dict, dict, dict, dict]:\r\n    \"\"\"\r\n    @param eval_tasks: list(TaskOutput).\r\n    @return: A tuple containing the consolidated results, samples, configs, versions, and num_fewshot.\r\n\r\n    Consolidates the results of multiple evaluation tasks into a single structure.\r\n\r\n    The method iterates over each evaluation instance and extracts relevant information to create the consolidated\r\n    results structure. The consolidated results structure has the following properties:\r\n\r\n    - results: A defaultdict with task names as keys and dictionaries as values. Each dictionary contains\r\n    metric/filter pairs as keys and corresponding metric values as values. The \"alias\" key is used to store task\r\n    aliases specified in the task configuration.\r\n    - samples: A defaultdict with task names as keys and lists of log samples as values.\r\n    - configs: A defaultdict with task names as keys and task configurations as values.\r\n    - versions: A defaultdict with task names as keys and task versions as values.\r\n    - num_fewshot: A defaultdict with task names as keys and number of few-shot samples as values.\r\n    - higher_is_better: A defaultdict with task names as keys and indicators of whether higher values are better\r\n    for each metric as values.\r\n\r\n    The method then returns the consolidated results, samples, configs, versions, and num_fewshot as a tuple.\r\n    \"\"\"\r\n    # stores the final result for each task, for each metric/filter pair.\r\n    results = collections.defaultdict(dict)\r\n    # logs info about each document evaluated.\r\n    samples = collections.defaultdict(list)\r\n    # store num-fewshot value per task\r\n    num_fewshot = collections.defaultdict(int)\r\n    # Tracks the YAML configs of all chosen task\r\n    configs = collections.defaultdict(dict)\r\n    # Tracks each task's version.\r\n    versions = collections.defaultdict(dict)\r\n    # Track `higher_is_better` for each metric\r\n    higher_is_better = collections.defaultdict(dict)\r\n\r\n    for task_output in eval_tasks:\r\n        if \"task_alias\" in (task_config := task_output.task_config):\r\n            results[task_output.task_name][\"alias\"] = task_config[\"task_alias\"]\r\n        else:\r\n            results[task_output.task_name][\"alias\"] = task_output.task_name\r\n        if group_alias := task_output.group_alias:\r\n            if group_alias not in results and (group_name := task_output.group_name):\r\n                results[group_name][\"alias\"] = group_alias\r\n        num_fewshot[task_output.task_name] = task_output.n_shot\r\n        configs[task_output.task_name] = task_output.task_config\r\n        versions[task_output.task_name] = task_output.version\r\n        samples[task_output.task_name] = task_output.logged_samples\r\n        higher_is_better[task_output.task_name] = task_output.task.higher_is_better()\r\n        for (metric, filter_key), items in task_output.sample_metrics.items():\r\n            metric_key = f\"{metric},{filter_key}\"\r\n            results[task_output.task_name][metric_key] = task_output.agg_metrics[\r\n                metric_key\r\n            ]\r\n            results[task_output.task_name][\"samples\"] = task_output.sample_len\r\n            results[task_output.task_name][f\"{metric}_stderr,{filter_key}\"] = (\r\n                task_output.agg_metrics[f\"{metric}_stderr,{filter_key}\"]\r\n            )\r\n    return results, samples, configs, versions, num_fewshot, higher_is_better\r\n\r\n\r\ndef consolidate_group_results(\r\n    results,\r\n    versions,\r\n    task_dict,\r\n    task_root=None,\r\n    show_group_table=False,\r\n    task_aggregation_list=None,\r\n) -> Tuple[dict, dict, bool, Union[None,]]:\r\n    \"\"\"\r\n    (Recursively) calculates groups' aggregated metrics and updates the results and versions dictionaries with this info.\r\n\r\n    @return: a tuple [results, versions, show_group_table, task_aggregation_list] with formats described below:\r\n\r\n    - results: A defaultdict with task names (and, after this function is called, group names of\r\n    groups that perform aggregation) as keys, and dictionaries with \"alias\" and metric,filter_name pairs as keys.\r\n    - versions: A defaultdict with task names (and, after this function is called, group names of\r\n    groups that perform aggregation) as keys, and float values representing the task or group's version if a version is specified. (defaulting to None).\r\n    - show_group_table: a boolean which is true if there exists a group that requires printing of its aggregated scores in a group table.\r\n    - task_aggregation_list: a defaultdict listing the subtasks to average over to produce a given group's end metric.\r\n\r\n    The method then returns the updated results, versions, show_group_table, and task_aggregation_list as a tuple.\r\n    In the top-level invocation of this function, task_aggregation_list is ignored.\r\n    \"\"\"\r\n    if task_root is None:\r\n        task_root = {}\r\n\r\n    if task_aggregation_list is None:\r\n        task_aggregation_list = {}\r\n\r\n    for group_or_task, group_or_task_info in task_dict.items():\r\n        # Convert to string\r\n        if isinstance(group_or_task, ConfigurableGroup):\r\n            group_config = group_or_task.config\r\n            group_or_task = group_or_task.group_name\r\n        else:\r\n            group_config = None\r\n\r\n        if isinstance(group_or_task_info, Task):\r\n            if task_root:\r\n                task_aggregation_list.setdefault(task_root, []).append(\r\n                    group_or_task_info.task_name\r\n                )\r\n        else:\r\n            (\r\n                results,\r\n                versions,\r\n                show_group_table,\r\n                _task_aggregation_list,\r\n            ) = consolidate_group_results(\r\n                results,\r\n                versions,\r\n                group_or_task_info,\r\n                group_or_task,\r\n                show_group_table,\r\n                task_aggregation_list,\r\n            )\r\n            if task_root:\r\n                task_aggregation_list.setdefault(task_root, []).extend(\r\n                    task_aggregation_list.get(group_or_task, [])\r\n                )\r\n\r\n            if (group_config is None) or (\r\n                group_config[\"aggregate_metric_list\"] is None\r\n            ):\r\n                results[group_or_task][\" \"] = \" \"\r\n                continue\r\n\r\n            if \"aggregate_metric_list\" in group_config:\r\n                agg_metric_list = group_config[\"aggregate_metric_list\"]\r\n\r\n            show_group_table = show_group_table | bool(\r\n                group_config[\"aggregate_metric_list\"]\r\n            )\r\n\r\n            task_list = _task_aggregation_list[group_or_task]\r\n\r\n            metric_list = list(\r\n                {\r\n                    key\r\n                    for task in task_list\r\n                    for key in results[task].keys()\r\n                    if \"_stderr\" not in key and key not in [\"task\", \"alias\", \"samples\"]\r\n                }\r\n            )\r\n            for metric in metric_list:\r\n                stderr = \"_stderr,\".join(metric.split(\",\"))\r\n\r\n                # gather metrics, sizes, and stderrs from subtasks\r\n                metrics = [\r\n                    results[task][metric]\r\n                    for task in task_list\r\n                    if metric in results[task]\r\n                ]  # TODO: copy?\r\n                stderrs = [\r\n                    results[task][stderr]\r\n                    for task in task_list\r\n                    if stderr in results[task]\r\n                ]\r\n                sizes = [\r\n                    results[task][\"samples\"]\r\n                    for task in task_list\r\n                    if metric in results[task]\r\n                ]\r\n\r\n                for metric_config in agg_metric_list:\r\n                    for filter_name in metric_config[\"filter_list\"]:\r\n                        if metric != \",\".join([metric_config[\"metric\"], filter_name]):\r\n                            continue\r\n\r\n                        # compute group's pooled metric and stderr\r\n                        if metric_config[\"aggregation\"] == \"mean\":\r\n                            aggregate_fn = aggregate_subtask_metrics\r\n                        elif callable(metric_config[\"aggregation\"]):\r\n                            aggregate_fn = metric_config[\"aggregation\"]\r\n                        else:\r\n                            raise ValueError(\r\n                                f\"Currently, only 'mean' is supported for automatically aggregating scores across groups' subtasks. Got '{metric_config['aggregation']}' for group '{group_or_task}'\"\r\n                            )\r\n\r\n                        results[group_or_task][metric] = aggregate_fn(\r\n                            metrics,\r\n                            sizes,\r\n                            metric_config[\"weight_by_size\"],\r\n                        )\r\n                        # TODO: calculate groups' metrics using arbitrary agg fns\r\n                        if \"N/A\" in stderrs:\r\n                            results[group_or_task][stderr] = \"N/A\"\r\n                        else:\r\n                            # NOTE: this assumes we are using the mean to aggregate. There are warnings about this elsewhere\r\n                            results[group_or_task][stderr] = pooled_sample_stderr(\r\n                                stderrs, sizes\r\n                            )\r\n\r\n                results[group_or_task][\"samples\"] = sum(sizes)\r\n                group_metadata = group_config.get(\"metadata\", None)\r\n                if group_metadata is not None:\r\n                    versions[group_or_task] = group_metadata.get(\"version\", None)\r\n    # print(results)\r\n    return results, versions, show_group_table, task_aggregation_list\r\n\r\n\r\n@positional_deprecated\r\ndef find_test_root(start_path: pathlib.Path) -> pathlib.Path:\r\n    \"\"\"\r\n    Search upward in the directory tree to a maximum of three layers\r\n    to find and return the package root (containing the 'tests' folder)\r\n    \"\"\"\r\n    cur_path = start_path.resolve()\r\n    max_layers = 3\r\n    for _ in range(max_layers):\r\n        if (cur_path / \"tests\" / \"test_version_stable.py\").exists():\r\n            return cur_path\r\n        else:\r\n            cur_path = cur_path.parent.resolve()\r\n    raise FileNotFoundError(\r\n        f\"Unable to find package root within {max_layers} upwards\" + f\"of {start_path}\"\r\n    )\r\n\r\n\r\n@positional_deprecated\r\ndef run_task_tests(task_list: List[str]):\r\n    \"\"\"\r\n    Find the package root and run the tests for the given tasks\r\n    \"\"\"\r\n    import pytest\r\n\r\n    package_root = find_test_root(start_path=pathlib.Path(__file__))\r\n    task_string = \" or \".join(task_list)\r\n    args = [\r\n        f\"{package_root}/tests/test_version_stable.py\",\r\n        f\"--rootdir={package_root}\",\r\n        \"-k\",\r\n        f\"{task_string}\",\r\n    ]\r\n    sys.path.append(str(package_root))\r\n    pytest_return_val = pytest.main(args)\r\n    if pytest_return_val:\r\n        raise ValueError(\r\n            f\"Not all tests for the specified tasks ({task_list}) ran successfully! Error code: {pytest_return_val}\"\r\n        )\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/decontamination/archiver.py", "content": "import datetime\r\nimport io\r\nimport json\r\nimport mmap\r\nimport os\r\nfrom pathlib import Path\r\nfrom typing import Any\r\n\r\nimport jsonlines\r\nimport tqdm\r\nimport zstandard\r\n\r\n\r\ndef json_serial(obj: Any) -> str:\r\n    \"\"\"JSON serializer for objects not serializable by default json code\"\"\"\r\n\r\n    if isinstance(obj, (datetime.datetime,)):\r\n        return obj.isoformat()\r\n    raise TypeError(\"Type %s not serializable\" % type(obj))\r\n\r\n\r\n# Modified version of lm_dataformat Archive for single file.\r\nclass Archive:\r\n    def __init__(self, file_path: str, compression_level: int = 3) -> None:\r\n        self.file_path = file_path\r\n        dir_name = os.path.dirname(file_path)\r\n        if dir_name:\r\n            os.makedirs(dir_name, exist_ok=True)\r\n        self.fh = open(self.file_path, \"wb\")\r\n        self.cctx = zstandard.ZstdCompressor(level=compression_level)\r\n        self.compressor = self.cctx.stream_writer(self.fh)\r\n\r\n    def add_data(self, data, meta=None) -> None:\r\n        if meta is None:\r\n            meta = {}\r\n        self.compressor.write(\r\n            json.dumps({\"text\": data, \"meta\": meta}, default=json_serial).encode(\r\n                \"UTF-8\"\r\n            )\r\n            + b\"\\n\"\r\n        )\r\n\r\n    def commit(self) -> None:\r\n        self.compressor.flush(zstandard.FLUSH_FRAME)\r\n        self.fh.flush()\r\n        self.fh.close()\r\n\r\n\r\n# Modified version of lm_dataformat Reader with self.fh set, allowing peeking for tqdm.\r\nclass Reader:\r\n    def __init__(self) -> None:\r\n        pass\r\n\r\n    def read(\r\n        self,\r\n        file,\r\n        get_meta: bool = False,\r\n        autojoin_paragraphs: bool = True,\r\n        para_joiner: str = \"\\n\\n\",\r\n    ):\r\n        with open(file, \"rb\") as fh:\r\n            self.fh = fh\r\n            cctx = zstandard.ZstdDecompressor()\r\n            reader = io.BufferedReader(cctx.stream_reader(fh))\r\n            rdr = jsonlines.Reader(reader)\r\n            for ob in rdr:\r\n                # naive jsonl where each object is just the string itself, with no meta. For legacy compatibility.\r\n                if isinstance(ob, str):\r\n                    assert not get_meta\r\n                    yield ob\r\n                    continue\r\n\r\n                text = ob[\"text\"]\r\n\r\n                if autojoin_paragraphs and isinstance(text, list):\r\n                    text = para_joiner.join(text)\r\n\r\n                if get_meta:\r\n                    yield text, (ob[\"meta\"] if \"meta\" in ob else {})\r\n                else:\r\n                    yield text\r\n\r\n\r\nclass TextArchive:\r\n    def __init__(self, file_path, mode: str = \"rb+\") -> None:\r\n        self.file_path = file_path\r\n        dir_name = os.path.dirname(file_path)\r\n        if dir_name:\r\n            os.makedirs(dir_name, exist_ok=True)\r\n\r\n        if not os.path.exists(file_path):\r\n            Path(file_path).touch()\r\n\r\n        self.fh = open(self.file_path, mode)\r\n\r\n    def add_data(self, data) -> None:\r\n        self.fh.write(data.encode(\"UTF-8\") + b\"\\n\")\r\n\r\n    def commit(self) -> None:\r\n        self.fh.flush()\r\n        self.fh.close()\r\n\r\n\r\nclass TextReader:\r\n    def __init__(self, file_path) -> None:\r\n        self.file_path = file_path\r\n\r\n    # Optimized mmap read with infrequent tqdm updates to maintain speed\r\n    # Tested up to 250MB/s.\r\n    def read_tqdm(self, update_frequency: int = 10000):\r\n        current_file_position = 0\r\n        line_counter = 0\r\n        with open(self.file_path, \"r\", encoding=\"utf-8\") as fh, tqdm.tqdm(\r\n            total=os.path.getsize(self.file_path),\r\n            dynamic_ncols=True,\r\n            unit=\"byte\",\r\n            unit_scale=1,\r\n        ) as progress:\r\n            with mmap.mmap(fh.fileno(), length=0, access=mmap.ACCESS_READ) as mmap_obj:\r\n                for line in iter(mmap_obj.readline, b\"\"):\r\n                    line = line.decode(\"utf-8\")\r\n                    line_counter += 1\r\n                    if line_counter == update_frequency:\r\n                        new_file_pos = mmap_obj.tell()\r\n                        bytes_read = new_file_pos - current_file_position\r\n                        current_file_position = new_file_pos\r\n                        progress.update(bytes_read)\r\n                        line_counter = 0\r\n                    yield line[:-1]\r\n\r\n    def read_and_tell(self):\r\n        current_file_position = 0\r\n        with open(self.file_path, \"r\", encoding=\"utf8\") as fh:\r\n            with mmap.mmap(fh.fileno(), length=0, access=mmap.ACCESS_READ) as mmap_obj:\r\n                for line in iter(mmap_obj.readline, b\"\"):\r\n                    line = line.decode(\"utf-8\")\r\n                    new_file_pos = mmap_obj.tell()\r\n                    raw_bytes_read = new_file_pos - current_file_position\r\n                    current_file_position = new_file_pos\r\n                    yield line[:-1], raw_bytes_read\r\n\r\n    def read(self):\r\n        with open(self.file_path, \"r\", encoding=\"utf8\") as fh:\r\n            with mmap.mmap(fh.fileno(), length=0, access=mmap.ACCESS_READ) as mmap_obj:\r\n                for line in iter(mmap_obj.readline, b\"\"):\r\n                    line = line.decode(\"utf-8\")\r\n                    yield line[:-1]\r\n\r\n    def read_slow(self):\r\n        with open(self.file_path, \"r\", encoding=\"utf8\") as fh:\r\n            while True:\r\n                line = fh.readline()\r\n                if line == -1 or line == \"\":\r\n                    break\r\n                else:\r\n                    yield line[:-1]\r\n\r\n\r\n# Optimized for speed. Decompresses the archive in shell before\r\n# using the mmap'd TextReader.\r\nclass ZStdTextReader:\r\n    def __init__(self, file) -> None:\r\n        self.file = file\r\n\r\n    def read_tqdm(self):\r\n        decompressed_file = self.file[:-4]\r\n        print(\"Decompressing file, please wait...\")\r\n        os.system(f\"zstd -d {self.file}\")  # linux decompress is faster\r\n        reader = TextReader(decompressed_file)\r\n        yield from reader.read_tqdm()\r\n        os.remove(decompressed_file)\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/models/vllm_vlms.py", "content": "import copy\r\nfrom typing import Dict, List, Optional\r\n\r\nimport transformers\r\nfrom more_itertools import distribute\r\nfrom tqdm import tqdm\r\n\r\nfrom lm_eval.api.instance import Instance\r\nfrom lm_eval.api.registry import register_model\r\nfrom lm_eval.models.utils import Collator, undistribute\r\nfrom lm_eval.models.vllm_causallms import VLLM\r\nfrom lm_eval.utils import simple_parse_args_string\r\n\r\n\r\ntry:\r\n    import ray\r\n    from vllm import LLM, SamplingParams\r\n    from vllm.lora.request import LoRARequest  # noqa: F401\r\n    from vllm.transformers_utils.tokenizer import get_tokenizer  # noqa: F401\r\nexcept ModuleNotFoundError:\r\n    pass\r\n\r\n\r\nDEFAULT_IMAGE_PLACEHOLDER = \"<image>\"\r\n\r\n\r\n@register_model(\"vllm-vlm\")\r\nclass VLLM_VLM(VLLM):\r\n    MULTIMODAL = True\r\n\r\n    def __init__(\r\n        self,\r\n        pretrained: str,\r\n        trust_remote_code: Optional[bool] = False,\r\n        revision: Optional[str] = None,\r\n        interleave: bool = True,\r\n        # TODO<baber>: handle max_images and limit_mm_per_prompt better\r\n        max_images: int = 999,\r\n        limit_mm_per_prompt: str = \"image=1\",\r\n        **kwargs,\r\n    ):\r\n        kwargs[\"limit_mm_per_prompt\"] = simple_parse_args_string(limit_mm_per_prompt)\r\n        super().__init__(\r\n            pretrained=pretrained,\r\n            trust_remote_code=trust_remote_code,\r\n            revision=revision,\r\n            **kwargs,\r\n        )\r\n        self.interleave = interleave\r\n        self.max_images = max_images\r\n        self.processor = transformers.AutoProcessor.from_pretrained(\r\n            pretrained,\r\n            revision=revision,\r\n            trust_remote_code=trust_remote_code,\r\n        )\r\n        self.chat_applied: bool = False\r\n\r\n    def tok_batch_multimodal_encode(\r\n        self,\r\n        strings: List[str],  # note that input signature of this fn is different\r\n        images,  # TODO: typehint on this\r\n        left_truncate_len: int = None,\r\n        truncation: bool = False,\r\n    ):\r\n        images = [img[: self.max_images] for img in images]\r\n\r\n        outputs = []\r\n        for x, i in zip(strings, images):\r\n            inputs = {\r\n                \"prompt\": x,\r\n                \"multi_modal_data\": {\"image\": i},\r\n            }\r\n            outputs.append(inputs)\r\n        return outputs\r\n\r\n    def _model_generate(\r\n        self,\r\n        requests: List[List[dict]] = None,\r\n        generate: bool = False,\r\n        max_tokens: int = None,\r\n        stop: Optional[List[str]] = None,\r\n        **kwargs,\r\n    ):\r\n        if generate:\r\n            kwargs = self.modify_gen_kwargs(kwargs)\r\n            sampling_params = SamplingParams(max_tokens=max_tokens, stop=stop, **kwargs)\r\n        else:\r\n            sampling_params = SamplingParams(\r\n                temperature=0, prompt_logprobs=1, max_tokens=1, detokenize=False\r\n            )\r\n        if self.data_parallel_size > 1:\r\n            # vLLM hangs if tensor_parallel > 1 and resources are set in ray.remote\r\n            # also seems to only work with decorator and not with ray.remote() fn\r\n            # see https://github.com/vllm-project/vllm/issues/973\r\n            # note: this has changed on 0.3.3, and it only works now if num_gpus are set.\r\n            # but then tensor_parallel breaks\r\n            @ray.remote\r\n            def run_inference_one_model(\r\n                model_args: dict, sampling_params, requests: List[List[dict]]\r\n            ):\r\n                llm = LLM(**model_args)\r\n                return llm.generate(requests, sampling_params=sampling_params)\r\n\r\n            # dispatch requests to all self.data_parallel_size workers, in interleaved fashion\r\n            # interleaved important to balance context lengths across workers\r\n            requests = [list(x) for x in distribute(self.data_parallel_size, requests)]\r\n            inputs = ((self.model_args, sampling_params, req) for req in requests)\r\n            object_refs = [run_inference_one_model.remote(*x) for x in inputs]\r\n            results = ray.get(object_refs)\r\n            # Invoke ray.shutdown() to prevent hang-ups if subsequent calls required.\r\n            ray.shutdown()\r\n            # flatten results\r\n            return undistribute(results)\r\n\r\n        if self.lora_request is not None:\r\n            outputs = self.model.generate(\r\n                requests,\r\n                sampling_params=sampling_params,\r\n                use_tqdm=True if self.batch_size == \"auto\" else False,\r\n                lora_request=self.lora_request,\r\n            )\r\n        else:\r\n            outputs = self.model.generate(\r\n                requests,\r\n                sampling_params=sampling_params,\r\n                use_tqdm=True if self.batch_size == \"auto\" else False,\r\n            )\r\n        return outputs\r\n\r\n    def apply_chat_template(self, chat_history: List[Dict[str, str]]) -> str:\r\n        self.chat_applied = True\r\n        if not self.interleave:\r\n            for content in chat_history:\r\n                c = []\r\n                text = content[\"content\"]\r\n\r\n                # Count and remove image placeholders\r\n                image_count = min(\r\n                    self.max_images, text.count(DEFAULT_IMAGE_PLACEHOLDER)\r\n                )\r\n                text = text.replace(DEFAULT_IMAGE_PLACEHOLDER, \"\")\r\n\r\n                # Add image entries\r\n                for _ in range(image_count):\r\n                    c.append({\"type\": \"image\", \"image\": None})\r\n\r\n                # Add single text entry at the end\r\n                c.append({\"type\": \"text\", \"text\": text})\r\n\r\n                content[\"content\"] = c\r\n        else:\r\n            for content in chat_history:\r\n                c = []\r\n                text = content[\"content\"]\r\n                expected_image_count = min(\r\n                    self.max_images, text.count(DEFAULT_IMAGE_PLACEHOLDER)\r\n                )\r\n                actual_image_count = 0\r\n\r\n                text_parts = text.split(DEFAULT_IMAGE_PLACEHOLDER)\r\n\r\n                for i, part in enumerate(text_parts):\r\n                    # TODO: concatenate text parts (esp. if skipping images)?\r\n                    if part:  # Add non-empty text parts\r\n                        c.append({\"type\": \"text\", \"text\": part})\r\n                    if (\r\n                        (i < len(text_parts) - 1) and i < self.max_images\r\n                    ):  # Add image placeholder after each split except the last\r\n                        c.append({\"type\": \"image\"})\r\n                        actual_image_count += 1\r\n\r\n                content[\"content\"] = c\r\n\r\n                if actual_image_count != expected_image_count:\r\n                    raise ValueError(\r\n                        f\"Mismatch in image placeholder count. Expected: {expected_image_count}, Actual: {actual_image_count}\"\r\n                    )\r\n\r\n        return self.processor.apply_chat_template(\r\n            chat_history, add_generation_prompt=True\r\n        )\r\n\r\n    def generate_until(\r\n        self, requests: List[Instance], disable_tqdm: bool = False\r\n    ) -> List[str]:\r\n        # TODO: support text-only reqs\r\n        res = []\r\n\r\n        def _collate(x):\r\n            # the negative sign on len(toks) sorts descending - this has a few advantages:\r\n            # - time estimates will always be over not underestimates, which is more useful for planning\r\n            # - to know the size of a batch when going through the list, you know the first one is always the batch\r\n            #   padded context length. this is useful to simplify the batching logic and more importantly to make\r\n            #   automatic adaptive batches much much easier to implement\r\n            # - any OOMs will happen right away rather than near the end\r\n            toks = self.tok_encode(x[0])\r\n            return -len(toks), x[0]\r\n\r\n        pbar = tqdm(\r\n            total=len(requests),\r\n            disable=(disable_tqdm or (self.rank != 0)),\r\n            desc=\"Running generate_until requests with text+image input\",\r\n        )\r\n        # TODO: port auto-batch sizing into this.\r\n\r\n        # we group requests by their generation_kwargs,\r\n        # so that we don't try to execute e.g. greedy sampling and temp=0.8 sampling\r\n        # in the same batch.\r\n        re_ords = Collator(\r\n            [reg.args for reg in requests],\r\n            _collate,\r\n            group_by=\"gen_kwargs\",\r\n            group_fn=lambda x: x[1],\r\n        )\r\n        chunks = re_ords.get_batched(n=self.batch_size, batch_fn=None)\r\n\r\n        for chunk in chunks:\r\n            contexts, all_gen_kwargs, aux_arguments = zip(*chunk)\r\n\r\n            visuals = [arg[\"visual\"] for arg in aux_arguments]\r\n\r\n            if not isinstance(contexts, list):\r\n                contexts = list(\r\n                    contexts\r\n                )  # for Qwen2-VL, processor is unhappy accepting a tuple of strings instead of a list.\r\n                # TODO: could we upstream this workaround to HF?\r\n\r\n            # we assume all gen kwargs in the batch are the same\r\n            # this is safe to assume because the `grouper` object ensures it.\r\n            gen_kwargs = all_gen_kwargs[0]\r\n            # unpack our keyword arguments.\r\n            until = None\r\n            if isinstance(gen_kwargs, dict):\r\n                kwargs = copy.deepcopy(gen_kwargs)  # edge case for repeats > 1\r\n                if \"until\" in kwargs.keys():\r\n                    until = kwargs.pop(\"until\")\r\n                    if isinstance(until, str):\r\n                        until = [until]\r\n                    elif not isinstance(until, list):\r\n                        raise ValueError(\r\n                            f\"Expected `kwargs['until']` to be of type Union[str,list] but got {until}\"\r\n                        )\r\n            else:\r\n                raise ValueError(\r\n                    f\"Expected `kwargs` to be of type `dict` but got {type(gen_kwargs)}\"\r\n                )\r\n            # add EOS token to stop sequences\r\n            eos = self.tokenizer.decode(self.eot_token_id)\r\n            if not until:\r\n                until = [eos]\r\n            else:\r\n                until.append(eos)\r\n            if \"max_gen_toks\" in kwargs.keys():\r\n                max_gen_toks = kwargs.pop(\"max_gen_toks\")\r\n            else:\r\n                max_gen_toks = self.max_gen_toks\r\n\r\n            max_ctx_len = self.max_length - max_gen_toks\r\n\r\n            inputs = self.tok_batch_multimodal_encode(\r\n                contexts,\r\n                visuals,\r\n                left_truncate_len=max_ctx_len,\r\n            )\r\n\r\n            cont = self._model_generate(inputs, stop=until, generate=True, **kwargs)\r\n\r\n            for output, context in zip(cont, contexts):\r\n                generated_text = output.outputs[0].text\r\n                res.append(generated_text)\r\n                self.cache_hook.add_partial(\r\n                    \"generate_until\", (context, gen_kwargs), generated_text\r\n                )\r\n                pbar.update(1)\r\n        # reorder this group of results back to original unsorted form\r\n        res = re_ords.get_original(res)\r\n\r\n        pbar.close()\r\n        return res\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/models/textsynth.py", "content": "\"\"\"TextSynth API\r\nImplementation provided by Fabrice Bellard:\r\n    https://github.com/EleutherAI/lm-evaluation-harness/issues/295\r\n\r\nIn order to use the API, you must have a valid TextSynth account and\r\nenough credits.\r\n\r\nExample usage:\r\n\r\n    python main.py --model textsynth --model_args engine=gptj_6B --no_cache --tasks piqa\r\n\r\nHomepage: https://textsynth.com/index.html\r\n\"\"\"\r\n\r\nimport logging\r\nimport os\r\n\r\nimport requests as _requests\r\nfrom tqdm import tqdm\r\n\r\nfrom lm_eval.api.model import LM\r\nfrom lm_eval.api.registry import register_model\r\nfrom lm_eval.models.utils import retry_on_specific_exceptions\r\n\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\n\r\ndef textsynth_completion(**kwargs):\r\n    \"\"\"Query TextSynth API for completion.\r\n    Retry with back-off until they respond.\r\n    \"\"\"\r\n\r\n    def _exception_callback(e: Exception, sleep_time: float) -> None:\r\n        import traceback\r\n\r\n        traceback.print_exc()\r\n\r\n    @retry_on_specific_exceptions(\r\n        on_exceptions=[_requests.exceptions.RequestException],\r\n        max_retries=None,  # retry forever, consider changing\r\n        on_exception_callback=_exception_callback,\r\n    )\r\n    def completion():\r\n        return _requests.post(**kwargs)\r\n\r\n    return completion()\r\n\r\n\r\n@register_model(\"textsynth\")\r\nclass TextSynthLM(LM):\r\n    def __init__(self, engine, truncate: bool = False, **kwargs) -> None:\r\n        \"\"\"\r\n        :param engine: str\r\n            TextSynth API engine (e.g. `gptj_6B`)\r\n        :param truncate: bool\r\n            Truncate input if too long (if False and input is too long, throw error)\r\n        \"\"\"\r\n        super().__init__()\r\n\r\n        self.engine = engine\r\n        self.truncate = truncate\r\n        self.api_url = \"https://api.textsynth.com\"\r\n        # Read from environment variable TEXTSYNTH_API_SECRET_KEY\r\n        self.api_key = os.environ[\"TEXTSYNTH_API_SECRET_KEY\"]\r\n\r\n    @property\r\n    def eot_token_id(self):\r\n        # Isn't used because we override loglikelihood, loglikelihood_rolling and generate_until\r\n        raise NotImplementedError()\r\n\r\n    @property\r\n    def max_length(self) -> int:\r\n        # NOTE: Turn on truncation to avoid errors on long inputs.\r\n        return 2048\r\n\r\n    @property\r\n    def max_gen_toks(self) -> int:\r\n        return 256\r\n\r\n    @property\r\n    def batch_size(self):\r\n        # Isn't used because we override loglikelihood, loglikelihood_rolling and generate_until\r\n        raise NotImplementedError()\r\n\r\n    @property\r\n    def device(self):\r\n        # Isn't used because we override loglikelihood, loglikelihood_rolling and generate_until\r\n        raise NotImplementedError()\r\n\r\n    def tok_encode(self, string: str):\r\n        # Isn't used because we override loglikelihood, loglikelihood_rolling and generate_until\r\n        raise NotImplementedError()\r\n\r\n    def tok_decode(self, tokens):\r\n        # Isn't used because we override loglikelihood, loglikelihood_rolling and generate_until\r\n        raise NotImplementedError()\r\n\r\n    def loglikelihood(self, requests, disable_tqdm: bool = False):\r\n        res = []\r\n        for context, continuation in tqdm(requests, disable=disable_tqdm):\r\n            response = textsynth_completion(\r\n                url=self.api_url + \"/v1/engines/\" + self.engine + \"/logprob\",\r\n                headers={\"Authorization\": \"Bearer \" + self.api_key},\r\n                json={\"context\": context, \"continuation\": continuation},\r\n            )\r\n            resp = response.json()\r\n            if \"logprob\" in resp:\r\n                logprob = resp[\"logprob\"]\r\n                is_greedy = resp[\"is_greedy\"]\r\n                res.append((logprob, is_greedy))\r\n\r\n                self.cache_hook.add_partial(\r\n                    \"loglikelihood\", (context, continuation), (logprob, is_greedy)\r\n                )\r\n            else:\r\n                logger.error(\r\n                    f\"The following response does not contain `logprobs`. Got:\\n{resp}\"\r\n                )\r\n                assert False\r\n        return res\r\n\r\n    def loglikelihood_rolling(self, requests, disable_tqdm: bool = False):\r\n        # TODO: The TextSynth API does not support tokenized inputs so we cannot\r\n        # manually partition long contexts into smaller rolling windows as\r\n        # done for other models derived from `BaseLM`. Override this method\r\n        # with a windowing scheme that works for direct string inputs.\r\n        raise NotImplementedError(\r\n            \"`loglikelihood_rolling` is currently not supported due to lack of \"\r\n            \"input tokenization support from TextSynth.\"\r\n        )\r\n\r\n    def generate_until(self, requests, disable_tqdm: bool = False):\r\n        if not requests:\r\n            return []\r\n\r\n        res = []\r\n        for request in tqdm(requests, disable=disable_tqdm):\r\n            inp = request[0]\r\n            request_args = request[1]\r\n            until = request_args[\"until\"]\r\n            response = textsynth_completion(\r\n                url=self.api_url + \"/v1/engines/\" + self.engine + \"/completions\",\r\n                headers={\"Authorization\": \"Bearer \" + self.api_key},\r\n                json={\r\n                    \"prompt\": inp,\r\n                    \"max_tokens\": self.max_gen_toks,\r\n                    \"top_k\": 1,\r\n                    \"stop\": until,\r\n                },\r\n            )\r\n            resp = response.json()\r\n            if \"text\" in resp:\r\n                s = resp[\"text\"]\r\n                res.append(s)\r\n\r\n                self.cache_hook.add_partial(\"generate_until\", (inp, request_args), s)\r\n            else:\r\n                logger.error(\r\n                    \"The following response does not contain generated `text`. \"\r\n                    \"Got:\\n{resp}\"\r\n                )\r\n                assert False\r\n        return res\r\n\r\n    def _model_call(self, inps):\r\n        # Isn't used because we override _loglikelihood_tokens\r\n        raise NotImplementedError()\r\n\r\n    def _model_generate(self, context, max_length, eos_token_id):\r\n        # Isn't used because we override generate_until\r\n        raise NotImplementedError()\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/prompts/__init__.py", "content": "import ast\r\nimport os\r\nfrom typing import Dict\r\n\r\nfrom lm_eval import utils\r\nfrom lm_eval.utils import eval_logger\r\n\r\n\r\n# Prompt library.\r\n# Stores prompts in a dictionary indexed by 2 levels:\r\n# prompt category name, and prompt name.\r\n# This allows us to access prompts\r\nPROMPT_REGISTRY: Dict[str, Dict[str, str]] = {\r\n    \"qa-basic\": {\r\n        \"question-newline-answer\": \"Question: {{question}}\\nAnswer:\",\r\n        \"q-newline-a\": \"Q: {{question}}\\nA:\",\r\n    },\r\n}\r\n\r\n\r\ndef get_prompt(prompt_id: str, dataset_name: str = None, subset_name: str = None):\r\n    # unpack prompt name\r\n    category_name, prompt_name = prompt_id.split(\":\")\r\n    if subset_name is None:\r\n        dataset_full_name = dataset_name\r\n    else:\r\n        dataset_full_name = f\"{dataset_name}-{subset_name}\"\r\n    eval_logger.info(f\"Loading prompt from {category_name} for {dataset_full_name}\")\r\n    if category_name == \"promptsource\":\r\n        try:\r\n            from promptsource.templates import DatasetTemplates\r\n        except ModuleNotFoundError:\r\n            raise Exception(\r\n                \"Tried to load a Promptsource template, but promptsource is not installed \",\r\n                \"please install promptsource via pip install lm-eval[promptsource] or pip install -e .[promptsource]\",\r\n            )\r\n        try:\r\n            if subset_name is None:\r\n                prompts = DatasetTemplates(dataset_name=dataset_name)\r\n            else:\r\n                prompts = DatasetTemplates(\r\n                    dataset_name=dataset_name, subset_name=subset_name\r\n                )\r\n        except Exception:\r\n            raise ValueError(f\"{dataset_name} and {subset_name} not found\")\r\n        if prompt_name in prompts.all_template_names:\r\n            return prompts[prompt_name]\r\n        else:\r\n            raise ValueError(\r\n                f\"{prompt_name} not in prompt list {prompts.all_template_names}\"\r\n            )\r\n    elif \".yaml\" in category_name:\r\n        import yaml\r\n\r\n        with open(category_name, \"rb\") as file:\r\n            prompt_yaml_file = yaml.full_load(file)\r\n\r\n        prompt_string = prompt_yaml_file[\"prompts\"][prompt_name]\r\n        return PromptString(prompt_string)\r\n    else:\r\n        try:\r\n            return PROMPT_REGISTRY[category_name][prompt_name]\r\n        except Exception:\r\n            raise ValueError(\r\n                f\"expected only a single `:` as separator between \\\r\n                prompt category and name, but got `{prompt_id}` instead\"\r\n            )\r\n\r\n\r\ndef load_prompt_list(\r\n    use_prompt: str, dataset_name=None, subset_name=None, yaml_path=None, **kwargs\r\n):\r\n    category_name, prompt_name = use_prompt.split(\":\")\r\n\r\n    if category_name == \"promptsource\":\r\n        from promptsource.templates import DatasetTemplates\r\n\r\n        if subset_name is None:\r\n            prompts = DatasetTemplates(dataset_name=dataset_name)\r\n        else:\r\n            prompts = DatasetTemplates(\r\n                dataset_name=dataset_name, subset_name=subset_name\r\n            )\r\n\r\n        prompt_list = utils.pattern_match(prompt_name, prompts.all_template_names)\r\n\r\n    elif \".yaml\" in category_name:\r\n        import yaml\r\n\r\n        if yaml_path is not None:\r\n            category_name = os.path.realpath(os.path.join(yaml_path, category_name))\r\n\r\n        with open(category_name, \"rb\") as file:\r\n            prompt_yaml_file = yaml.full_load(file)\r\n\r\n        prompt_list = utils.pattern_match(\r\n            prompt_name, prompt_yaml_file[\"prompts\"].keys()\r\n        )\r\n\r\n    # category_name, *prompt_name = use_prompt.split(\":\")\r\n    # TODO allow to multiple prompt naming\r\n    # if len(prompt_name) > 1:\r\n    #     prompt_list = []\r\n    #     for prompt in prompt_name:\r\n    #         prompt_list.append(utils.pattern_match(prompt_name, prompts.all_template_names))\r\n    # else:\r\n    #     prompt_list = utils.pattern_match(prompt_name, prompts.all_template_names)\r\n    return [\":\".join([category_name, prompt]) for prompt in prompt_list]\r\n\r\n\r\nclass PromptString:\r\n    def __init__(self, prompt_string):\r\n        self.prompt_string = prompt_string\r\n\r\n    def apply(self, doc):\r\n        doc_to_text = self.prompt_string[\"doc_to_text\"]\r\n        doc_to_target = self.prompt_string[\"doc_to_target\"]\r\n\r\n        # TODO need a way to process doc_to_choice\r\n        if \"doc_to_choice\" in self.prompt_string:\r\n            raise Exception(\"Not yet implemented to accept doc_to_choice\")\r\n\r\n        text_string = utils.apply_template(doc_to_text, doc)\r\n        target_string = utils.apply_template(doc_to_target, doc)\r\n\r\n        return [text_string, target_string]\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/decontamination/janitor.py", "content": "import pickle\r\nimport re\r\nimport string\r\nimport traceback\r\nfrom typing import Iterator, List, Sequence, Tuple, TypeVar\r\n\r\n\r\n# This is a cpp module. Compile janitor_util.cpp with:\r\n# c++ -O3 -Wall -shared -std=c++11 -fPIC $(python3 -m pybind11 --includes) janitor_util.cpp -o janitor_util$(python3-config --extension-suffix) -undefined dynamic_lookup\r\ntry:\r\n    import janitor_util\r\n\r\n    JANITOR_CPP = True\r\nexcept Exception:\r\n    print(\"WARNING: C++ module could not be loaded. Janitor running in python mode\")\r\n    traceback.print_exc()\r\n    JANITOR_CPP = False\r\n\r\nT = TypeVar(\"T\")\r\n\r\n\r\n# Implementation from nltk source\r\n# https://www.nltk.org/_modules/nltk/util.html\r\ndef form_ngrams(sequence: Iterator[T], n: int) -> Iterator[Tuple[T, ...]]:\r\n    history = []\r\n    while n > 1:\r\n        # PEP 479, prevent RuntimeError from being raised when StopIteration bubbles out of generator\r\n        try:\r\n            next_item = next(sequence)\r\n        except StopIteration:\r\n            # no more data, terminate the generator\r\n            return\r\n        history.append(next_item)\r\n        n -= 1\r\n    for item in sequence:\r\n        history.append(item)\r\n        yield tuple(history)\r\n        del history[0]\r\n\r\n\r\ndef word_ngrams(s: str, n: int) -> Iterator[str]:\r\n    \"\"\"Splits a string into ngram words\"\"\"\r\n    tokens = s.split()  # not a generator :(\r\n    ngram_seqs = form_ngrams(iter(tokens), n)\r\n    return (\" \".join(ngram) for ngram in ngram_seqs)\r\n\r\n\r\n# Does character sequences only - combined faster function to play around with later\r\n# def word_ngrams_indices_combined(sequence, n):\r\n#     current_word = \"\"\r\n#     history = []\r\n#     gap = False;\r\n#     start = 0\r\n#     end = 0\r\n#     for character in sequence:\r\n#         if character == \" \":\r\n#             if not gap:\r\n#                 gap = True\r\n#                 history.append(current_word)\r\n#                 end += len(current_word) - 1\r\n#                 current_word = \"\"\r\n#                 if len(history) == n:\r\n#                     yield (tuple(history), start, end)\r\n#                     del history[0]\r\n#                     start = end + 1\r\n#                     end = start\r\n#         else:\r\n#             gap = False\r\n#             current_word += character\r\n\r\n\r\n# https://stackoverflow.com/questions/13734451/string-split-with-indices-in-python\r\ndef split_indices(s: str) -> Iterator[Tuple[str, Tuple[int, int]]]:\r\n    \"\"\"Splits a string on whitespaces and records the indices of each in the original string.\r\n    @:return generator((word, (start_idx, end_idx)), ...)\r\n    \"\"\"\r\n    return ((m.group(0), (m.start(), m.end() - 1)) for m in re.finditer(r\"\\S+\", s))\r\n\r\n\r\ndef word_ngrams_indices(s: str, n: int) -> Iterator[Tuple[str, Tuple[int, int]]]:\r\n    \"\"\"Splits a string into pairs of (ngram words, their start/end indices)\"\"\"\r\n    tokens_with_indices = split_indices(s)\r\n\r\n    # Generator of ngrams of (word, idx_pairs)\r\n    # (\r\n    #   [(word, (start,end)), (word, (start, end))...],\r\n    #   [(word, (start, end)), ...],\r\n    #   ...\r\n    # )\r\n    ngram_seqs_with_indices = form_ngrams(tokens_with_indices, n)\r\n\r\n    # Generator of pairs of word and index ngrams\r\n    # (\r\n    #   ([word, word, ...], [(start,end), (start,end), ...]),\r\n    #   ...\r\n    # )\r\n    ngram_indices_pairs = (\r\n        zip(*ngram_with_indices) for ngram_with_indices in ngram_seqs_with_indices\r\n    )\r\n\r\n    # Generator of ( (word_ngram, (start, end)), (word_ngram, start, end)), ...)\r\n    return (\r\n        (\" \".join(ngram_seq), (indices[0][0], indices[-1][1]))\r\n        for ngram_seq, indices in ngram_indices_pairs\r\n    )\r\n\r\n\r\nclass Janitor:\r\n    # FIXME delete_chars: Should anything else go here? Special chars?\r\n    def __init__(\r\n        self,\r\n        ngram_n: int = 13,\r\n        window_to_remove: int = 200,\r\n        too_dirty_cutoff: int = 10,\r\n        minimum_slice_length: int = 200,\r\n        delete_chars: str = string.punctuation,\r\n    ) -> None:\r\n        self.ngram_n = ngram_n\r\n        self.window_to_remove = window_to_remove\r\n        self.too_dirty_cutoff = too_dirty_cutoff\r\n        self.minimum_slice_length = minimum_slice_length\r\n        self.delete_chars = delete_chars\r\n\r\n        self.dirt_ngrams = set()\r\n\r\n        # If in python, we'll translate uppercase to lowercase and delete naughty characters.\r\n        # This is fast by python standards\r\n        # https://stackoverflow.com/questions/638893/what-is-the-most-efficient-way-in-python-to-convert-a-string-to-all-lowercase-st\r\n        self.translation_table = str.maketrans(\r\n            string.ascii_lowercase + string.ascii_uppercase,  # These characters\r\n            string.ascii_lowercase * 2,  # Become these characters\r\n            self.delete_chars,  # These are deleted\r\n        )\r\n\r\n    ##############\r\n    # I/O for saving contamination ngrams\r\n    ##############\r\n\r\n    def save_contamination_ngrams(self, filename: str) -> None:\r\n        with open(filename, \"wb\") as fp:\r\n            pickle.dump(filename, fp)\r\n\r\n    def load_contamination_ngrams(self, filename: str) -> None:\r\n        with open(filename, \"rb\") as fp:\r\n            self.dirt_ngrams = pickle.load(fp)\r\n\r\n    ##############\r\n    # Call these :)\r\n    ##############\r\n\r\n    def register_contaminant(self, dirt_string: str) -> None:\r\n        \"\"\"Register a string as contamination to be removed, e.g. a test set\r\n        This breaks the dirt_string into ngrams to store for future cleaning\"\"\"\r\n        if JANITOR_CPP:\r\n            return self.register_contaminant_cpp(dirt_string)\r\n        else:\r\n            print(\"WARNING: Janitor running in python mode\")\r\n            return self.register_contaminant_python(dirt_string)\r\n\r\n    def clean(self, dirty_string: str) -> List[str]:\r\n        \"\"\"Clean a string (e.g. a training set) by removing all ngrams previously\r\n        registered as contaminants. Returns a list of clean chunks, or empty if\r\n        the string was too dirty\"\"\"\r\n        if JANITOR_CPP:\r\n            return self.clean_cpp(dirty_string)\r\n        else:\r\n            print(\"WARNING: Janitor running in python mode\")\r\n            return self.clean_python(dirty_string)\r\n\r\n    def _split_chunks(\r\n        self, dirty_string: str, dirty_parts: Sequence[Tuple]\r\n    ) -> List[str]:\r\n        clean_chunks = []\r\n        splice_idx = 0\r\n        end = -1\r\n        for i, (ngram, start, end) in enumerate(dirty_parts):\r\n            if i >= self.too_dirty_cutoff:\r\n                return []\r\n            start = max(0, start - self.window_to_remove)\r\n            end = min(len(dirty_string), end + self.window_to_remove)\r\n\r\n            if start - splice_idx > self.minimum_slice_length:\r\n                clean_chunks.append(dirty_string[splice_idx:start])\r\n            splice_idx = end\r\n\r\n        if end < len(dirty_string) - self.minimum_slice_length:\r\n            clean_chunks.append(dirty_string[end + 1 :])\r\n\r\n        return clean_chunks\r\n\r\n    ##############\r\n    # Fast C++\r\n    ##############\r\n\r\n    def register_contaminant_cpp(self, dirt_string) -> None:\r\n        self.dirt_ngrams.update(\r\n            janitor_util.clean_ngram(dirt_string, self.delete_chars, self.ngram_n)\r\n        )\r\n\r\n    def clean_cpp(self, dirty_string: str) -> List[str]:\r\n        contamination_indices = janitor_util.clean_ngram_with_indices(\r\n            dirty_string, self.delete_chars, self.ngram_n\r\n        )\r\n        return self._split_chunks(dirty_string, contamination_indices)\r\n\r\n    ##############\r\n    # Slow python\r\n    ##############\r\n\r\n    def normalize_string(self, s: str) -> str:\r\n        return s.translate(self.translation_table)\r\n\r\n    def register_contaminant_python(self, dirt_string: str) -> None:\r\n        self.dirt_ngrams.update(\r\n            word_ngrams(self.normalize_string(dirt_string), self.ngram_n)\r\n        )\r\n\r\n    def clean_python(self, dirty_string: str) -> List[str]:\r\n        contamination_indices = (\r\n            (None, *idx_pair)\r\n            for dirty_ngram, idx_pair in word_ngrams_indices(dirty_string, self.ngram_n)\r\n            if self.normalize_string(dirty_ngram) in self.dirt_ngrams\r\n        )\r\n        return self._split_chunks(dirty_string, contamination_indices)\r\n\r\n\r\n##################################################################\r\n# Tests\r\n#################################################################\r\n\r\n# def print_cpp():\r\n#     source = \"\"\"   ,, I'm a very !dirty,, ,,  dirty boy. Clean me daddy. \\n\\nhe he he hehe heh.  lastword  \"\"\" * 2\r\n\r\n#     for i in range(1, 10, 2):\r\n#         pprint(janitor_util.clean_ngram(source, string.punctuation, i))\r\n#         for ngram, start, end in \\\r\n#                 janitor_util.clean_ngram_with_indices(source, string.punctuation, i):\r\n#             print(ngram, \"\\t\", start, end, source[start:end].replace(\"\\n\", \"\\\\n\"))\r\n\r\n\r\n# def test_cpp():\r\n#     source = \"\"\"   ,, I'm a very !dirty,, ,,  dirty boy. Clean me daddy. \\n\\nhe he he hehe heh.  lastword  \"\"\" * 2\r\n#     contaminant = \"dirty boy. Clean he he\"\r\n\r\n#     jan_python = Janitor()\r\n#     jan_cpp = Janitor()\r\n\r\n#     jan_python.register_contaminant_python(contaminant)\r\n#     jan_cpp.register_contaminant(contaminant)\r\n\r\n#     assert jan_python.dirt_ngrams == jan_cpp.dirt_ngrams, (jan_python.dirt_ngrams, jan_cpp.dirt_ngrams)\r\n\r\n#     assert jan_python.clean_python(source) == jan_cpp.clean(source), \\\r\n#         (jan_python.clean_python(source), jan_cpp.clean(source))\r\n\r\n#     print(\"Passed test, python==cpp\")\r\n\r\n\r\n# def benchmark():\r\n#     # Download and put in data folder: enwik8 (100 MB) from https://cs.fit.edu/~mmahoney/compression/textdata.html\r\n#     setup = \\\r\n#         \"\"\"\r\n#         with open(\"data/enwik8\", \"r\") as f:\r\n#             data = f.read()\r\n#         jan = Janitor(too_dirty_cutoff=1000)\r\n#         jan.register_contaminant('''\r\n#         theories is that there is a connection between &quot;geekdom&quot; and autism.\r\n#         This is hinted, for instance, by a ''Wired Magazine'' article in 2001 entitled &quot;\r\n#         The [[Geek]] Syndrome&quot;, which is a point argued by many in the autism rights\r\n#         movement{{ref|Wired}}.  This article, many professionals assert, is just one example of\r\n#         the media's application of mental disease labels to what is actually variant normal behavior\r\n#         &amp;mdash;they argue that shyness, lack of athletic ability or social skills, and intellectual\r\n#         interests, even when they seem unusual to others, are not in themselves signs of autism or\r\n#         Asperger's syndrome. Others assert that it is actually the medical profession which is applying\r\n#         mental disease labels to children who in the past would have simply been accepted as a little\r\n#         different or even labeled 'gifted'. See [[clinomorphism]] for further discussion of this issue.\r\n#         Due to the recent publicity surrounding autism and autis\r\n#         ultan Al Nahyan]] granted [[Petroleum]] concessions, and oil was first found in 1958.  At first,\r\n#         oil money had a marginal impact.  A few lowrise concete buildings were erected, and the first\r\n#         paved road was completed in 1961, but Sheikh Shakbut, uncertain whether the new oil royalties\r\n#         would last, took a cautious approach, preferring to save the revenue rather than investing it in\r\n#         development.  His brother, [[Zayed bin Sultan Al Nahayan]], saw that oil wealth had the potential\r\n#         to transform Abu Dhabi.  The ruling Al Nahayan family decided that Sheikh Zayed should replace his\r\n#         brother as Ruler and carry out his vision of developing the country.  On [[August 6]], [[1966]],\r\n#         with the assistance of the British, Sheikh Zayed became the new ruler.  See generally, Al-Fahim, M,\r\n#         ''From Rags to Riches: A Story of Abu Dhabi'', Chapter Six (London Centre of Arab Studies, 1995),\r\n#         ISBN 1 900404 00 1. With the announcement by Britain in 1968 that it would withdraw from the\r\n#         Gulf area by 1971, Sheikh Zayed became the main driving force behind the formation of the\r\n#         [[United Arab Emirates]]. After the Emirates gained independence in 1971,\r\n#         ''')\r\n#         \"\"\"\r\n\r\n#     n = 1\r\n#     print(f\"Timing {n} run on 100 MB\")\r\n#     print(\"Register contaminant\")\r\n#     # print(\"\\tPython\", timeit.timeit(\"jan.register_contaminant_python(data)\", setup=setup, globals=globals(), number=n))\r\n#     print(\"\\tCpp\", timeit.timeit(\"jan.register_contaminant(data)\", setup=setup, globals=globals(), number=n))\r\n\r\n#     print(\"Clean\")\r\n#     # print(\"\\tPython\", timeit.timeit(\"jan.clean_python(data)\", setup=setup, globals=globals(), number=n))\r\n#     print(\"\\tCpp\", timeit.timeit(\"jan.clean(data)\", setup=setup, globals=globals(), number=n))\r\n\r\n\r\n# def test_janitor_general():\r\n#     source = \"\"\"   ,, I'm a very !dirty,, ,,  dirty boy. Clean me daddy. \\n\\nhe he he hehe heh.  lastword  \"\"\" * 2\r\n#     contaminant = \"dirty boy. Clean he he\"\r\n\r\n#     jan = Janitor(ngram_n=3)\r\n#     jan.register_contaminant(contaminant)\r\n#     cleaned = \" \".join(jan.clean(source))\r\n#     for contam in jan.dirt_ngrams:\r\n#         assert contam not in cleaned, contam\r\n\r\n#     filename = \"data/saved_contam\"\r\n#     jan.save_contamination_ngrams(filename)\r\n\r\n#     jan = Janitor(ngram_n=3)\r\n#     jan.load_contamination_ngrams(filename)\r\n#     cleaned = \" \".join(jan.clean(source))\r\n#     for contam in jan.dirt_ngrams:\r\n#         assert contam not in cleaned, contam\r\n\r\n\r\n# if __name__ == \"__main__\":\r\n#     test()\r\n#     # print_cpp()\r\n#     # test_cpp()\r\n#     # benchmark()\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/models/vllm_causallms.py", "content": "import copy\r\nfrom importlib.metadata import version\r\nfrom importlib.util import find_spec\r\nfrom typing import TYPE_CHECKING, Dict, List, Literal, Optional, Tuple, Union\r\n\r\nfrom more_itertools import distribute\r\nfrom packaging.version import parse as parse_version\r\nfrom tqdm import tqdm\r\n\r\nfrom lm_eval.api.instance import Instance\r\nfrom lm_eval.api.model import TemplateLM\r\nfrom lm_eval.api.registry import register_model\r\nfrom lm_eval.models.utils import Collator, configure_pad_token, undistribute\r\nfrom lm_eval.utils import (\r\n    eval_logger,\r\n    get_rolling_token_windows,\r\n    make_disjoint_window,\r\n)\r\n\r\n\r\ntry:\r\n    import ray\r\n    from vllm import LLM, SamplingParams\r\n    from vllm.lora.request import LoRARequest\r\n    from vllm.transformers_utils.tokenizer import get_tokenizer\r\nexcept ModuleNotFoundError:\r\n    pass\r\n\r\nif TYPE_CHECKING:\r\n    pass\r\n\r\neval_logger = eval_logger\r\n\r\n\r\n@register_model(\"vllm\")\r\nclass VLLM(TemplateLM):\r\n    _DEFAULT_MAX_LENGTH = 2048\r\n\r\n    def __init__(\r\n        self,\r\n        pretrained: str,\r\n        dtype: Literal[\"float16\", \"bfloat16\", \"float32\", \"auto\"] = \"auto\",\r\n        revision: Optional[str] = None,\r\n        trust_remote_code: Optional[bool] = False,\r\n        tokenizer: Optional[str] = None,\r\n        tokenizer_mode: Literal[\"auto\", \"slow\"] = \"auto\",\r\n        tokenizer_revision: Optional[str] = None,\r\n        add_bos_token: Optional[bool] = False,\r\n        prefix_token_id: Optional[int] = None,\r\n        tensor_parallel_size: int = 1,\r\n        quantization: Optional[str] = None,\r\n        max_gen_toks: int = 256,\r\n        swap_space: int = 4,\r\n        batch_size: Union[str, int] = 1,\r\n        max_batch_size=None,\r\n        max_length: int = None,\r\n        max_model_len: int = None,\r\n        seed: int = 1234,\r\n        gpu_memory_utilization: float = 0.9,\r\n        device: str = \"cuda\",\r\n        data_parallel_size: int = 1,\r\n        lora_local_path: str = None,\r\n        **kwargs,\r\n    ):\r\n        super().__init__()\r\n\r\n        if not find_spec(\"vllm\"):\r\n            raise Exception(\r\n                \"attempted to use 'vllm' LM type, but package `vllm` is not installed. \"\r\n                \"Please install vllm via `pip install lm-eval[vllm]` or `pip install -e .[vllm]`\"\r\n            )\r\n\r\n        assert \"cuda\" in device or device is None, \"vLLM only supports CUDA\"\r\n        assert (\r\n            max_length is None or max_model_len is None\r\n        ), \"Either max_length or max_model_len may be provided, but not both\"\r\n\r\n        self._max_length = max_model_len if max_model_len is not None else max_length\r\n        self.tensor_parallel_size = int(tensor_parallel_size)\r\n        self.data_parallel_size = int(data_parallel_size)\r\n        self.model_args = {\r\n            \"model\": pretrained,\r\n            \"gpu_memory_utilization\": float(gpu_memory_utilization),\r\n            \"revision\": revision,\r\n            \"dtype\": dtype,\r\n            \"tokenizer\": tokenizer,\r\n            \"tokenizer_mode\": tokenizer_mode,\r\n            \"tokenizer_revision\": tokenizer_revision,\r\n            \"trust_remote_code\": trust_remote_code,\r\n            \"tensor_parallel_size\": int(tensor_parallel_size),\r\n            \"max_model_len\": int(self._max_length) if self._max_length else None,\r\n            \"swap_space\": int(swap_space),\r\n            \"quantization\": quantization,\r\n            \"seed\": int(seed),\r\n        }\r\n        self.model_args.update(kwargs)\r\n        self.batch_size = (\r\n            \"auto\"\r\n            if isinstance(batch_size, str) and \"auto\" in batch_size\r\n            else batch_size\r\n        )\r\n        if self.data_parallel_size <= 1:\r\n            self.model = LLM(**self.model_args)\r\n        else:\r\n            eval_logger.warning(\r\n                \"You might experience occasional issues with model weight downloading when data_parallel is in use. To ensure stable performance, run with data_parallel_size=1 until the weights are downloaded and cached.\"\r\n            )\r\n            self.model_args[\"worker_use_ray\"] = True\r\n            self.batch_size = \"auto\"\r\n            eval_logger.info(\"Manual batching is not compatible with data parallelism.\")\r\n\r\n            from transformers import AutoConfig\r\n\r\n            self._config = AutoConfig.from_pretrained(\r\n                pretrained, trust_remote_code=trust_remote_code, revision=revision\r\n            )\r\n        self.tokenizer = get_tokenizer(\r\n            tokenizer if tokenizer else pretrained,\r\n            tokenizer_mode=tokenizer_mode,\r\n            trust_remote_code=trust_remote_code,\r\n            tokenizer_revision=tokenizer_revision,\r\n        )\r\n        self.tokenizer = configure_pad_token(self.tokenizer)\r\n        self.add_bos_token = add_bos_token\r\n        if \"gemma\" in pretrained.lower():\r\n            self.add_bos_token = True\r\n            eval_logger.info(\r\n                \"Found 'gemma' in model name, a BOS token will be used as Gemma series models underperform without it.\"\r\n            )\r\n\r\n        self.custom_prefix_token_id = prefix_token_id\r\n        if prefix_token_id is not None:\r\n            eval_logger.info(\r\n                f\"Loglikelihood prefix token id used in evaluation: {self.prefix_token_id}\"\r\n            )\r\n\r\n        self._max_gen_toks = max_gen_toks\r\n\r\n        if lora_local_path is not None:\r\n            assert parse_version(version(\"vllm\")) > parse_version(\r\n                \"0.3.0\"\r\n            ), \"lora adapters only compatible with vllm > v0.3.0.\"\r\n            self.lora_request = LoRARequest(\"finetuned\", 1, lora_local_path)\r\n        else:\r\n            self.lora_request = None\r\n\r\n    @property\r\n    def eot_token_id(self):\r\n        # we use EOT because end of *text* is more accurate for what we're doing than end of *sentence*\r\n        return self.tokenizer.eos_token_id\r\n\r\n    @property\r\n    def prefix_token_id(self):\r\n        # it is used as prefix for loglikelihood\r\n        if self.custom_prefix_token_id is not None:\r\n            return self.custom_prefix_token_id\r\n        if self.tokenizer.bos_token_id is not None:\r\n            return self.tokenizer.bos_token_id\r\n        return self.tokenizer.eos_token_id\r\n\r\n    @property\r\n    def max_length(self):\r\n        if self._max_length:  # if max length manually set, return it\r\n            return self._max_length\r\n        if self.data_parallel_size <= 1:\r\n            return self.model.llm_engine.model_config.max_model_len\r\n        else:\r\n            seqlen_config_attrs = (\"n_positions\", \"max_position_embeddings\", \"n_ctx\")\r\n            for attr in seqlen_config_attrs:\r\n                if hasattr(self._config, attr):\r\n                    return getattr(self._config, attr)\r\n            if hasattr(self.tokenizer, \"model_max_length\"):\r\n                if self.tokenizer.model_max_length == 1000000000000000019884624838656:\r\n                    return self._DEFAULT_MAX_LENGTH\r\n                return self.tokenizer.model_max_length\r\n            return self._DEFAULT_MAX_LENGTH\r\n\r\n    @property\r\n    def max_gen_toks(self):\r\n        return self._max_gen_toks\r\n\r\n    def apply_chat_template(self, chat_history: List[Dict[str, str]]) -> str:\r\n        \"\"\"\r\n        Method to apply a chat template to a list of chat history between user and model.\r\n        \"\"\"\r\n        return self.tokenizer.apply_chat_template(\r\n            chat_history, tokenize=False, add_generation_prompt=True\r\n        )\r\n\r\n    @property\r\n    def tokenizer_name(self) -> str:\r\n        return self.tokenizer.name_or_path.replace(\"/\", \"__\")\r\n\r\n    def tok_encode(\r\n        self,\r\n        string: Union[str, List[str]],\r\n        left_truncate_len: int = None,\r\n        add_special_tokens: bool = False,\r\n        truncation: bool = False,\r\n    ) -> Union[List[int], List[List[int]]]:\r\n        if not add_special_tokens:\r\n            add_special_tokens = False or self.add_bos_token\r\n        encoding: Union[List[List[int]], List[int]] = self.tokenizer(\r\n            string,\r\n            add_special_tokens=add_special_tokens,\r\n            truncation=truncation,\r\n            return_attention_mask=False,\r\n        ).input_ids\r\n\r\n        # left-truncate the encoded context to be at most `left_truncate_len` tokens long\r\n        if left_truncate_len:\r\n            if not isinstance(string, str):\r\n                encoding = [enc[-left_truncate_len:] for enc in encoding]\r\n            else:\r\n                encoding = encoding[-left_truncate_len:]\r\n\r\n        return encoding\r\n\r\n    def _model_generate(\r\n        self,\r\n        requests: List[List[int]] = None,\r\n        generate: bool = False,\r\n        max_tokens: int = None,\r\n        stop: Optional[List[str]] = None,\r\n        **kwargs,\r\n    ):\r\n        if generate:\r\n            kwargs = self.modify_gen_kwargs(kwargs)\r\n\r\n            rejection_sample = kwargs.pop(\"rejection_sample\", None)\r\n            if rejection_sample:\r\n                if (kwargs.get(\"temperature_thinking\", 0) == 0) and (kwargs.get(\"temperature\", 0) == 0):\r\n                    print(\"Warning: Rejection sampling works best with temperature/temperature_thinking > 0.\")\r\n                assert \"max_tokens_thinking\" in kwargs, \"Rejection sampling requires max_tokens_thinking to be set.\"\r\n\r\n            outputs_thinking = None\r\n            if any([\"thinking\" in k for k in kwargs]) or rejection_sample:\r\n                print(\"Separating thinking and answering generation.\")\r\n                thinking_start = kwargs.pop(\"thinking_start\", \"<|im_start|>think\")\r\n                thinking_end = kwargs.pop(\"thinking_end\", \"<|im_start|>answer\")\r\n                thinking_n_ignore = kwargs.pop(\"thinking_n_ignore\", None)\r\n                thinking_n_ignore_str = kwargs.pop(\"thinking_n_ignore_str\", None) # e.g. \"Let me double check step-by-step.\")\r\n                if thinking_n_ignore_str is not None:\r\n                    print(f\"Thinking ignore string: {thinking_n_ignore_str}\")\r\n                    thinking_n_ignore_str_tok = self.tok_encode(thinking_n_ignore_str)\r\n                until_thinking = [kwargs.pop(\"until_thinking\", \"<|im_start|>\")]\r\n                if \"until_thinking_2\" in kwargs:\r\n                    until_thinking.append(kwargs.pop(\"until_thinking_2\"))\r\n                if stop is not None:\r\n                    until_thinking.extend(stop)\r\n                print(f\"Thinking start: {thinking_start}, Thinking end: {thinking_end}, Stop: {until_thinking}\")\r\n                thinking_start_tok = self.tok_encode(thinking_start)\r\n                thinking_end_tok = self.tok_encode(thinking_end)\r\n                thinking_end_max = thinking_end + \"\\nFinal Answer:\"\r\n                thinking_end_max_tok = self.tok_encode(thinking_end_max)\r\n                newline_tok = self.tok_encode(\"\\n\")\r\n                # Cast to list to avoid `dictionary changed size during iteration`\r\n                sampling_params_thinking = {k.replace(\"_thinking\", \"\"): kwargs.pop(k) for k, v in list(kwargs.items()) if \"thinking\" in k}\r\n                # Add all other kwargs but keep sampling_params_thinking version if duplicate key\r\n                sampling_params_thinking = {**kwargs, **sampling_params_thinking}\r\n                if \"max_tokens\" in sampling_params_thinking:\r\n                    if sampling_params_thinking[\"max_tokens\"] == \"auto\":\r\n                        # Leave 100 tokens for answer\r\n                        sampling_params_thinking[\"max_tokens\"] = max_tokens - max([len(x) for x in requests]) - len(thinking_start_tok) - len(thinking_end_max_tok) - 100\r\n                        print(f\"Auto setting max_tokens_thinking to {sampling_params_thinking['max_tokens']}\")\r\n                    else:\r\n                        sampling_params_thinking[\"max_tokens\"] = int(sampling_params_thinking[\"max_tokens\"])\r\n                    if rejection_sample:\r\n                        sampling_params_thinking[\"max_tokens\"] += 1\r\n                else:\r\n                    sampling_params_thinking[\"max_tokens\"] = max_tokens\r\n                until_thinking_tok = self.tok_encode(until_thinking)\r\n                if (\"min_tokens\" in sampling_params_thinking) or (thinking_n_ignore is not None):\r\n                    if thinking_n_ignore is not None:\r\n                        sampling_params_thinking[\"min_tokens\"] = 1\r\n                    else:\r\n                        sampling_params_thinking[\"min_tokens\"] = int(sampling_params_thinking[\"min_tokens\"])\r\n                    assert all([len(x) == 1 for x in until_thinking_tok]), \"min_tokens_thinking only supports until_thinking tokens that are 1 token long\"\r\n                    # min_tokens will not ignore `stop`, only `stop_token_ids` are ignored so need to use these\r\n                    sampling_params_thinking[\"stop_token_ids\"] = [x[0] for x in until_thinking_tok]\r\n                else:\r\n                    sampling_params_thinking[\"stop\"] = until_thinking\r\n                requests = [req + thinking_start_tok for req in requests]\r\n                sampling_params = SamplingParams(**sampling_params_thinking)\r\n\r\n                if rejection_sample:\r\n                    requests_thinking = copy.deepcopy(requests)\r\n                    outputs_thinking = [None] * len(requests_thinking)\r\n                    i = 0\r\n                    while True:\r\n                        outputs_tmp = self.model.generate(\r\n                            prompt_token_ids=requests_thinking,\r\n                            sampling_params=sampling_params,\r\n                            use_tqdm=True if self.batch_size == \"auto\" else False,\r\n                        )\r\n                        # Save ones that are already below the limit\r\n                        outputs_tmp2 = copy.deepcopy(outputs_thinking)\r\n                        for j, o in enumerate(outputs_tmp):\r\n                            if len(o.outputs[0].token_ids) <= sampling_params_thinking[\"max_tokens\"] - 1:\r\n                                if outputs_tmp2[j] is None:\r\n                                    outputs_thinking[j] = o\r\n                                else:\r\n                                    for k, t in enumerate(outputs_tmp2[j:] + outputs_tmp2[:j]):\r\n                                        if t is None:\r\n                                            idx = j + k if j + k < len(outputs_thinking) else j + k - len(outputs_thinking)\r\n                                            outputs_thinking[idx] = o\r\n                                            break\r\n\r\n                        # Collect requests remaining\r\n                        requests_thinking_new = [None] * len(requests_thinking)\r\n                        for j, o in enumerate(outputs_thinking):\r\n                            if outputs_thinking[j] is None:\r\n                                requests_thinking_new[j] = requests_thinking[j]\r\n\r\n                        samples_left = sum([x is not None for x in requests_thinking_new])\r\n\r\n                        if not(samples_left): break\r\n                        gen_tokens_all = [len(o.outputs[0].token_ids) for o in outputs_tmp]\r\n                        print(f\"Samples left: {samples_left}, gen_tokens_all: {gen_tokens_all}, i: {i}\")\r\n                        # Fill up empty request slots with duplicates of other requests that need to be rerun\r\n                        # Fill each None with the first non-None request after it\r\n                        for j, r in enumerate(requests_thinking_new):\r\n                            if r is None:\r\n                                for k, r2 in enumerate(requests_thinking_new[j:] + requests_thinking_new[:j]):\r\n                                    if r2 is not None:\r\n                                        requests_thinking_new[j] = r2\r\n                                        break\r\n                        requests_thinking = requests_thinking_new\r\n                        i += 1\r\n                    print(f'Rejection sampling took {i} iterations to generate {sampling_params_thinking[\"max_tokens\"] - 1} tokens.')\r\n                elif thinking_n_ignore is not None:\r\n                    print(\"Will ignore end of thinking \" + str(thinking_n_ignore) + \" times.\")\r\n                    # Add 1 to account for first generation w/o ignoring\r\n                    thinking_n_ignore = int(thinking_n_ignore) + 1\r\n                    outputs_thinking = [None] * len(requests)\r\n                    requests_tmp = copy.deepcopy(requests)\r\n                    indices = list(range(len(requests)))\r\n                    for i in range(thinking_n_ignore):\r\n                        outputs_tmp = self.model.generate(\r\n                            prompt_token_ids=requests_tmp,\r\n                            sampling_params=sampling_params,\r\n                            use_tqdm=True if self.batch_size == \"auto\" else False,\r\n                        )\r\n                        indices_new = []\r\n                        requests_tmp_new = []\r\n                        for j, o in enumerate(outputs_tmp):\r\n                            idx = indices[j]\r\n                            assert len(o.outputs) == 1\r\n                            cont = list(o.outputs[0].token_ids)\r\n                            # Final; do not generate further\r\n                            if (o.outputs[0].finish_reason == \"length\") or (i == thinking_n_ignore - 1):\r\n                                if outputs_thinking[idx] is not None:\r\n                                    outputs_thinking[idx].outputs[0].text += o.outputs[0].text\r\n                                    outputs_thinking[idx].outputs[0].token_ids += cont\r\n                                    outputs_thinking[idx].outputs[0].finish_reason = o.outputs[0].finish_reason\r\n                                else:\r\n                                    outputs_thinking[idx] = o\r\n                                    outputs_thinking[idx].outputs[0].token_ids = cont\r\n                                    outputs_thinking[idx].outputs[0].finish_reason = o.outputs[0].finish_reason\r\n                            else:\r\n                                # When using `stop`, the stop text will not be in the text, but still in the token_ids so remove it\r\n                                for toks in until_thinking_tok:\r\n                                    if cont[-len(toks):] == toks:\r\n                                        cont = cont[:-len(toks)]\r\n                                \r\n                                if thinking_n_ignore_str is not None:\r\n                                    cont += thinking_n_ignore_str_tok\r\n                                    o.outputs[0].text += thinking_n_ignore_str\r\n\r\n                                if outputs_thinking[idx] is not None:\r\n                                    outputs_thinking[idx].outputs[0].text += o.outputs[0].text\r\n                                    outputs_thinking[idx].outputs[0].token_ids += cont\r\n                                else:\r\n                                    outputs_thinking[idx] = o\r\n                                    outputs_thinking[idx].outputs[0].token_ids = cont\r\n\r\n                                requests_tmp_new.append(requests_tmp[j] + cont)\r\n                                indices_new.append(idx)\r\n                        requests_tmp = requests_tmp_new\r\n                        indices = indices_new\r\n                    for idx in list(range(len(requests))):\r\n                        if len(outputs_thinking[idx].outputs[0].token_ids) > sampling_params_thinking[\"max_tokens\"]:\r\n                            print(f'Warning: Generated more than {sampling_params_thinking[\"max_tokens\"]} tokens. Cutting.')\r\n                            outputs_thinking[idx].outputs[0].token_ids = outputs_thinking[idx].outputs[0].token_ids[:sampling_params_thinking[\"max_tokens\"]]\r\n\r\n                else:\r\n                    outputs_thinking = self.model.generate(\r\n                        prompt_token_ids=requests,\r\n                        sampling_params=sampling_params,\r\n                        use_tqdm=True if self.batch_size == \"auto\" else False,\r\n                    )\r\n\r\n                for i, o in enumerate(outputs_thinking):\r\n                    assert len(o.outputs) == 1\r\n                    cont = list(o.outputs[0].token_ids)\r\n                    # When using `stop`, the stop text will not be in the text, but still in the token_ids so remove it\r\n                    for toks in until_thinking_tok:\r\n                        if cont[-len(toks):] == toks:\r\n                            cont = cont[:-len(toks)]\r\n\r\n                    if o.outputs[0].finish_reason == \"length\":\r\n                        assert not rejection_sample, \"Rejection sampling should not reach this point.\"\r\n                        # \\n appears a lot so a decent chance it happend to just be the last token in which case we don't need to add a newline\r\n                        if (o.outputs[0].text[-1] == \"\\n\") or (thinking_start[0] == \"\\n\"):\r\n                            requests[i] += cont + thinking_end_max_tok\r\n                            outputs_thinking[i].outputs[0].text = thinking_start + outputs_thinking[i].outputs[0].text + thinking_end_max\r\n                        else:\r\n                            requests[i] += cont + newline_tok + thinking_end_max_tok\r\n                            outputs_thinking[i].outputs[0].text = thinking_start + outputs_thinking[i].outputs[0].text + \"\\n\" + thinking_end_max\r\n                    else:\r\n                        requests[i] += cont + thinking_end_tok\r\n                        outputs_thinking[i].outputs[0].text = thinking_start + outputs_thinking[i].outputs[0].text + thinking_end\r\n\r\n            sampling_params = SamplingParams(max_tokens=max_tokens, stop=stop, **kwargs)\r\n        else:\r\n            sampling_params = SamplingParams(\r\n                temperature=0, prompt_logprobs=1, max_tokens=1, detokenize=False\r\n            )\r\n        if self.data_parallel_size > 1:\r\n            # vLLM hangs if tensor_parallel > 1 and resources are set in ray.remote\r\n            # also seems to only work with decorator and not with ray.remote() fn\r\n            # see https://github.com/vllm-project/vllm/issues/973\r\n            # note: this has changed on 0.3.3, and it only works now if num_gpus are set.\r\n            # but then tensor_parallel breaks\r\n            @ray.remote\r\n            def run_inference_one_model(\r\n                model_args: dict, sampling_params, requests: List[List[int]]\r\n            ):\r\n                llm = LLM(**model_args)\r\n                return llm.generate(\r\n                    prompt_token_ids=requests, sampling_params=sampling_params\r\n                )\r\n\r\n            # dispatch requests to all self.data_parallel_size workers, in interleaved fashion\r\n            # interleaved important to balance context lengths across workers\r\n            requests = [list(x) for x in distribute(self.data_parallel_size, requests)]\r\n            inputs = ((self.model_args, sampling_params, req) for req in requests)\r\n            object_refs = [run_inference_one_model.remote(*x) for x in inputs]\r\n            results = ray.get(object_refs)\r\n            # Invoke ray.shutdown() to prevent hang-ups if subsequent calls required.\r\n            ray.shutdown()\r\n            # flatten results\r\n            return undistribute(results)\r\n\r\n        if self.lora_request is not None:\r\n            outputs = self.model.generate(\r\n                prompt_token_ids=requests,\r\n                sampling_params=sampling_params,\r\n                use_tqdm=True if self.batch_size == \"auto\" else False,\r\n                lora_request=self.lora_request,\r\n            )\r\n        else:\r\n            outputs = self.model.generate(\r\n                prompt_token_ids=requests,\r\n                sampling_params=sampling_params,\r\n                use_tqdm=True if self.batch_size == \"auto\" else False,\r\n            )\r\n            if outputs_thinking is not None:\r\n                for i, o in enumerate(outputs):\r\n                    assert len(o.outputs) == 1\r\n                    outputs[i].outputs[0].text = outputs_thinking[i].outputs[0].text + outputs[i].outputs[0].text\r\n        return outputs\r\n\r\n    def loglikelihood_rolling(\r\n        self, requests: List[Instance], disable_tqdm: bool = False\r\n    ) -> List[float]:\r\n        loglikelihoods = []\r\n\r\n        for (string,) in tqdm([req.args for req in requests], disable=disable_tqdm):\r\n            rolling_token_windows = list(\r\n                map(\r\n                    make_disjoint_window,\r\n                    get_rolling_token_windows(\r\n                        token_list=self.tok_encode(string),\r\n                        prefix_token=self.prefix_token_id,\r\n                        # max_seq_len - (1 for context)\r\n                        max_seq_len=self.max_length - 1,\r\n                        context_len=1,\r\n                    ),\r\n                )\r\n            )\r\n\r\n            rolling_token_windows = [(None,) + x for x in rolling_token_windows]\r\n\r\n            string_nll = self._loglikelihood_tokens(\r\n                rolling_token_windows,\r\n            )\r\n\r\n            # discard is_greedy\r\n            string_nll = [x[0] for x in string_nll]\r\n\r\n            string_nll = sum(string_nll)\r\n            loglikelihoods.append(string_nll)\r\n\r\n            # cache this loglikelihood_rolling request\r\n            self.cache_hook.add_partial(\"loglikelihood_rolling\", (string,), string_nll)\r\n\r\n        return loglikelihoods\r\n\r\n    def generate_until(\r\n        self, requests: List[Instance], disable_tqdm: bool = False\r\n    ) -> List[str]:\r\n        res = []\r\n\r\n        # batch tokenize contexts\r\n        context, all_gen_kwargs = zip(*(req.args for req in requests))\r\n        context_encoding: List[List[int]] = self.tok_encode(\r\n            context, add_special_tokens=self.add_bos_token\r\n        )\r\n        requests = [\r\n            ((a, b), c) for a, b, c in zip(context, context_encoding, all_gen_kwargs)\r\n        ]\r\n\r\n        def _collate_gen(_requests):\r\n            # the negative sign on len(toks) sorts descending - this has a few advantages:\r\n            # - time estimates will always be over not underestimates, which is more useful for planning\r\n            # - to know the size of a batch when going through the list, you know the first one is always the batch\r\n            #   padded context length. this is useful to simplify the batching logic and more importantly to make\r\n            #   automatic adaptive batches much much easier to implement\r\n            # - any OOMs will happen right away rather than near the end\r\n            return -len(_requests[0][1]), _requests[0][0]\r\n\r\n        # we group requests by their generation_kwargs,\r\n        # so that we don't try to execute e.g. greedy sampling and temp=0.8 sampling\r\n        # in the same batch.\r\n        re_ords = Collator(requests, _collate_gen, group_by=\"gen_kwargs\")\r\n        chunks = re_ords.get_batched(\r\n            n=int(self.batch_size) if self.batch_size != \"auto\" else 0, batch_fn=None\r\n        )\r\n\r\n        pbar = tqdm(\r\n            total=len(requests),\r\n            disable=(disable_tqdm or (self.rank != 0)),\r\n            desc=\"Running generate_until requests\",\r\n        )\r\n        # for each different set of kwargs, we execute all requests, by batch.\r\n        for chunk in chunks:\r\n            context_and_encoding, all_gen_kwargs = zip(*chunk)\r\n            context, context_encoding = zip(*context_and_encoding)\r\n            # we assume all gen kwargs in the batch are the same\r\n            # this is safe to assume because the `grouper` object ensures it.\r\n            gen_kwargs = all_gen_kwargs[0]\r\n            # unpack our keyword arguments.\r\n            until = None\r\n            if isinstance(gen_kwargs, dict):\r\n                kwargs = copy.deepcopy(gen_kwargs)  # edge case for repeats > 1\r\n                if \"until\" in kwargs.keys():\r\n                    until = kwargs.pop(\"until\")\r\n                    if isinstance(until, str):\r\n                        until = [until]\r\n                    elif not isinstance(until, list):\r\n                        raise ValueError(\r\n                            f\"Expected `kwargs['until']` to be of type Union[str,list] but got {until}\"\r\n                        )\r\n            else:\r\n                raise ValueError(\r\n                    f\"Expected `kwargs` to be of type `dict` but got {gen_kwargs}\"\r\n                )\r\n            # add EOS token to stop sequences\r\n            eos = self.tokenizer.decode(self.eot_token_id)\r\n            if not until:\r\n                until = [eos]\r\n            else:\r\n                until.append(eos)\r\n            if \"max_gen_toks\" in kwargs.keys():\r\n                max_gen_toks = kwargs.pop(\"max_gen_toks\")\r\n            else:\r\n                max_gen_toks = self.max_gen_toks\r\n\r\n            # set the max length in tokens of inputs (\"context_enc\")\r\n            # max len for inputs = max length, minus room to generate the max new tokens\r\n            max_ctx_len = self.max_length - max_gen_toks\r\n            context_encoding = [x[-max_ctx_len:] for x in context_encoding]\r\n\r\n            # perform batched generation\r\n            cont = self._model_generate(\r\n                requests=context_encoding,\r\n                generate=True,\r\n                max_tokens=max_gen_toks,\r\n                stop=until,\r\n                **kwargs,\r\n            )\r\n\r\n            # cache generations\r\n            for i, (output, context) in tqdm(enumerate(zip(cont, context)), desc=\"final processing\"):\r\n                generated_text = output.outputs[0].text\r\n                # swj hack\r\n                # from ipdb import set_trace as bp\r\n                # bp()\r\n                # check if \"Answer:\" in generated_text, if not resample cont = self._model_generate(requests=context_encoding, generate=True, max_tokens=max_gen_toks, stop=until, **kwargs) until it reaches \"Answer:\"\r\n                # max_attemp = 5\r\n                # while \"Answer:\" not in generated_text:\r\n                #     if max_attemp == 0:\r\n                #         print(f\"max_attemp reached, question: {i}\")\r\n                #         break\r\n                #     max_attemp -= 1\r\n                #     cont_new = self._model_generate(requests=[context_encoding[i]], generate=True, max_tokens=max_gen_toks, stop=until, **kwargs)\r\n                #     generated_text = cont_new[0].outputs[0].text\r\n                #     print(f\"resample until 'Answer:', question: {i}\")\r\n                res.append(generated_text)\r\n\r\n                self.cache_hook.add_partial(\r\n                    \"generate_until\", (context, gen_kwargs), generated_text\r\n                )\r\n                pbar.update(1)\r\n\r\n        pbar.close()\r\n        # reorder all group of results back to original unsorted form\r\n        return re_ords.get_original(res)\r\n\r\n    def _loglikelihood_tokens(\r\n        self,\r\n        requests: List[Tuple[Tuple[str, str], List[int], List[int]]],\r\n        disable_tqdm: bool = False,\r\n    ) -> List[Tuple[float, bool]]:\r\n        res = []\r\n\r\n        def _collate(x):\r\n            toks = x[1] + x[2]\r\n            return -len(toks), tuple(toks)\r\n\r\n        # Reorder requests by length and batch\r\n        re_ord = Collator(requests, sort_fn=_collate)\r\n        chunks = re_ord.get_batched(\r\n            n=int(self.batch_size) if self.batch_size != \"auto\" else 0, batch_fn=None\r\n        )\r\n\r\n        pbar = tqdm(\r\n            total=len(requests),\r\n            disable=disable_tqdm,\r\n            desc=\"Running loglikelihood requests\",\r\n        )\r\n        for chunk in chunks:\r\n            inputs = []\r\n            ctxlens = []\r\n            for cache_key, context_enc, continuation_enc in chunk:\r\n                inp = (context_enc + continuation_enc)[-(self.max_length) :]\r\n                ctxlen = len(context_enc) - max(\r\n                    0, len(context_enc) + len(continuation_enc) - (self.max_length)\r\n                )\r\n\r\n                inputs.append(inp)\r\n                ctxlens.append(ctxlen)\r\n\r\n            outputs = self._model_generate(requests=inputs, generate=False)\r\n\r\n            for output, ctxlen, (cache_key, _, _), inp in zip(\r\n                outputs, ctxlens, chunk, inputs\r\n            ):\r\n                answer = self._parse_logprobs(\r\n                    tokens=inp,\r\n                    outputs=output,\r\n                    ctxlen=ctxlen,\r\n                )\r\n\r\n                res.append(answer)\r\n\r\n                if cache_key is not None:\r\n                    # special case: loglikelihood_rolling produces a number of loglikelihood requests\r\n                    # all with cache key None. instead do add_partial on the per-example level\r\n                    # in the loglikelihood_rolling() function for those.\r\n                    self.cache_hook.add_partial(\"loglikelihood\", cache_key, answer)\r\n                pbar.update(1)\r\n        pbar.close()\r\n        return re_ord.get_original(res)\r\n\r\n    @staticmethod\r\n    def _parse_logprobs(tokens: List, outputs, ctxlen: int) -> Tuple[float, bool]:\r\n        \"\"\"Process logprobs and tokens.\r\n\r\n        :param tokens: list\r\n            Input tokens (potentially left-truncated)\r\n        :param outputs: RequestOutput\r\n            Contains prompt_logprobs\r\n        :param ctxlen: int\r\n            Length of context (so we can slice them away and only keep the predictions)\r\n        :return:\r\n            continuation_logprobs: float\r\n                Log probabilities of continuation tokens\r\n            is_greedy: bool\r\n                Whether argmax matches given continuation exactly\r\n        \"\"\"\r\n\r\n        # The first entry of prompt_logprobs is None because the model has no previous tokens to condition on.\r\n        continuation_logprobs_dicts = outputs.prompt_logprobs\r\n\r\n        def coerce_logprob_to_num(logprob):\r\n            # vLLM changed the return type of logprobs from float\r\n            # to a Logprob object storing the float value + extra data\r\n            # (https://github.com/vllm-project/vllm/pull/3065).\r\n            # If we are dealing with vllm's Logprob object, return\r\n            # the logprob value stored as an attribute. Otherwise,\r\n            # return the object itself (which should be a float\r\n            # for older versions of vLLM).\r\n            return getattr(logprob, \"logprob\", logprob)\r\n\r\n        continuation_logprobs_dicts = [\r\n            {\r\n                token: coerce_logprob_to_num(logprob)\r\n                for token, logprob in logprob_dict.items()\r\n            }\r\n            if logprob_dict is not None\r\n            else None\r\n            for logprob_dict in continuation_logprobs_dicts\r\n        ]\r\n\r\n        # Calculate continuation_logprobs\r\n        # assume ctxlen always >= 1\r\n        continuation_logprobs = sum(\r\n            logprob_dict.get(token)\r\n            for token, logprob_dict in zip(\r\n                tokens[ctxlen:], continuation_logprobs_dicts[ctxlen:]\r\n            )\r\n        )\r\n\r\n        # Determine if is_greedy\r\n        is_greedy = True\r\n        for token, logprob_dict in zip(\r\n            tokens[ctxlen:], continuation_logprobs_dicts[ctxlen:]\r\n        ):\r\n            # Get the token with the maximum log probability from the logprob_dict\r\n            if logprob_dict:  # Ensure the logprob_dict is not None\r\n                top_token = max(logprob_dict, key=logprob_dict.get)\r\n                if top_token != token:\r\n                    is_greedy = False\r\n                    break\r\n\r\n        return continuation_logprobs, is_greedy\r\n\r\n    @staticmethod\r\n    def modify_gen_kwargs(kwargs: dict) -> dict:\r\n        # sampling_params\r\n        do_sample = kwargs.pop(\"do_sample\", None)\r\n        if do_sample is False and \"temperature\" not in kwargs:\r\n            eval_logger.debug(\r\n                \"Got `do_sample=False` and no temperature value, setting VLLM temperature to 0.0 ...\"\r\n            )\r\n            kwargs[\"temperature\"] = 0.0\r\n        # hf defaults\r\n        kwargs[\"skip_special_tokens\"] = kwargs.get(\"skip_special_tokens\", False)\r\n        kwargs[\"spaces_between_special_tokens\"] = kwargs.get(\r\n            \"spaces_between_special_tokens\", False\r\n        )\r\n        return kwargs\r\n"}
{"type": "source_file", "path": "reasonflux-f1/eval/lm-evaluation-harness/lm_eval/tasks/__init__.py", "content": "import collections\r\nimport inspect\r\nimport logging\r\nimport os\r\nfrom functools import partial\r\nfrom typing import Dict, List, Mapping, Optional, Union\r\n\r\nfrom lm_eval import utils\r\nfrom lm_eval.api.group import ConfigurableGroup, GroupConfig\r\nfrom lm_eval.api.task import ConfigurableTask, Task\r\nfrom lm_eval.evaluator_utils import get_subtask_list\r\n\r\n\r\nGROUP_ONLY_KEYS = list(GroupConfig().to_dict().keys())\r\n\r\n\r\nclass TaskManager:\r\n    \"\"\"TaskManager indexes all tasks from the default `lm_eval/tasks/`\r\n    and an optional directory if provided.\r\n\r\n    \"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        verbosity=\"INFO\",\r\n        include_path: Optional[Union[str, List]] = None,\r\n        include_defaults: bool = True,\r\n    ) -> None:\r\n        self.verbosity = verbosity\r\n        self.include_path = include_path\r\n        self.logger = utils.eval_logger\r\n        self.logger.setLevel(getattr(logging, f\"{verbosity}\"))\r\n\r\n        self._task_index = self.initialize_tasks(\r\n            include_path=include_path, include_defaults=include_defaults\r\n        )\r\n        self._all_tasks = sorted(list(self._task_index.keys()))\r\n\r\n        self._all_groups = sorted(\r\n            [x for x in self._all_tasks if self._task_index[x][\"type\"] == \"group\"]\r\n        )\r\n        self._all_subtasks = sorted(\r\n            [\r\n                x\r\n                for x in self._all_tasks\r\n                if self._task_index[x][\"type\"] in [\"task\", \"python_task\"]\r\n            ]\r\n        )\r\n        self._all_tags = sorted(\r\n            [x for x in self._all_tasks if self._task_index[x][\"type\"] == \"tag\"]\r\n        )\r\n\r\n        self.task_group_map = collections.defaultdict(list)\r\n\r\n    def initialize_tasks(\r\n        self,\r\n        include_path: Optional[Union[str, List]] = None,\r\n        include_defaults: bool = True,\r\n    ):\r\n        \"\"\"Creates a dictionary of tasks index.\r\n\r\n        :param include_path: Union[str, List] = None\r\n            An additional path to be searched for tasks recursively.\r\n            Can provide more than one such path as a list.\r\n        :param include_defaults: bool = True\r\n            If set to false, default tasks (those in lm_eval/tasks/) are not indexed.\r\n        :return\r\n            Dictionary of task names as key and task metadata\r\n        \"\"\"\r\n        if include_defaults:\r\n            all_paths = [os.path.dirname(os.path.abspath(__file__)) + \"/\"]\r\n        else:\r\n            all_paths = []\r\n        if include_path is not None:\r\n            if isinstance(include_path, str):\r\n                include_path = [include_path]\r\n            all_paths.extend(include_path)\r\n\r\n        task_index = {}\r\n        for task_dir in all_paths:\r\n            tasks = self._get_task_and_group(task_dir)\r\n            task_index = {**tasks, **task_index}\r\n\r\n        return task_index\r\n\r\n    @property\r\n    def all_tasks(self):\r\n        return self._all_tasks\r\n\r\n    @property\r\n    def all_groups(self):\r\n        return self._all_groups\r\n\r\n    @property\r\n    def all_subtasks(self):\r\n        return self._all_subtasks\r\n\r\n    @property\r\n    def all_tags(self):\r\n        return self._all_tags\r\n\r\n    @property\r\n    def task_index(self):\r\n        return self._task_index\r\n\r\n    def list_all_tasks(\r\n        self, list_groups=True, list_tags=True, list_subtasks=True\r\n    ) -> str:\r\n        from pytablewriter import MarkdownTableWriter\r\n\r\n        def sanitize_path(path):\r\n            # don't print full path if we are within the lm_eval/tasks dir !\r\n            # if we aren't though, provide the full path.\r\n            if \"lm_eval/tasks/\" in path:\r\n                return \"lm_eval/tasks/\" + path.split(\"lm_eval/tasks/\")[-1]\r\n            else:\r\n                return path\r\n\r\n        group_table = MarkdownTableWriter()\r\n        group_table.headers = [\"Group\", \"Config Location\"]\r\n        gt_values = []\r\n        for g in self.all_groups:\r\n            path = self.task_index[g][\"yaml_path\"]\r\n            if path == -1:\r\n                path = \"---\"\r\n            else:\r\n                path = sanitize_path(path)\r\n            gt_values.append([g, path])\r\n        group_table.value_matrix = gt_values\r\n\r\n        tag_table = MarkdownTableWriter()\r\n        tag_table.headers = [\"Tag\"]\r\n        tag_table.value_matrix = [[t] for t in self.all_tags]\r\n\r\n        subtask_table = MarkdownTableWriter()\r\n        subtask_table.headers = [\"Task\", \"Config Location\", \"Output Type\"]\r\n        st_values = []\r\n        for t in self.all_subtasks:\r\n            path = self.task_index[t][\"yaml_path\"]\r\n\r\n            output_type = \"\"\r\n\r\n            # read the yaml file to determine the output type\r\n            if path != -1:\r\n                config = utils.load_yaml_config(path, mode=\"simple\")\r\n                if \"output_type\" in config:\r\n                    output_type = config[\"output_type\"]\r\n                elif (\r\n                    \"include\" in config\r\n                ):  # if no output type, check if there is an include with an output type\r\n                    include_path = path.split(\"/\")[:-1] + config[\"include\"]\r\n                    include_config = utils.load_yaml_config(include_path, mode=\"simple\")\r\n                    if \"output_type\" in include_config:\r\n                        output_type = include_config[\"output_type\"]\r\n\r\n            if path == -1:\r\n                path = \"---\"\r\n            else:\r\n                path = sanitize_path(path)\r\n            st_values.append([t, path, output_type])\r\n        subtask_table.value_matrix = st_values\r\n\r\n        result = \"\\n\"\r\n        if list_groups:\r\n            result += group_table.dumps() + \"\\n\\n\"\r\n        if list_tags:\r\n            result += tag_table.dumps() + \"\\n\\n\"\r\n        if list_subtasks:\r\n            result += subtask_table.dumps() + \"\\n\\n\"\r\n        return result\r\n\r\n    def match_tasks(self, task_list):\r\n        return utils.pattern_match(task_list, self.all_tasks)\r\n\r\n    def _name_is_registered(self, name) -> bool:\r\n        if name in self.all_tasks:\r\n            return True\r\n        return False\r\n\r\n    def _name_is_task(self, name) -> bool:\r\n        if self._name_is_registered(name) and (self.task_index[name][\"type\"] == \"task\"):\r\n            return True\r\n        return False\r\n\r\n    def _name_is_tag(self, name) -> bool:\r\n        if self._name_is_registered(name) and (self.task_index[name][\"type\"] == \"tag\"):\r\n            return True\r\n        return False\r\n\r\n    def _name_is_group(self, name) -> bool:\r\n        if self._name_is_registered(name) and (\r\n            self.task_index[name][\"type\"] == \"group\"\r\n        ):\r\n            return True\r\n        return False\r\n\r\n    def _name_is_python_task(self, name):\r\n        if self._name_is_registered(name) and (\r\n            self.task_index[name][\"type\"] == \"python_task\"\r\n        ):\r\n            return True\r\n        return False\r\n\r\n    def _config_is_task(self, config) -> bool:\r\n        if (\"task\" in config) and isinstance(config[\"task\"], str):\r\n            return True\r\n        return False\r\n\r\n    def _config_is_group(self, config) -> bool:\r\n        if (\"task\" in config) and isinstance(config[\"task\"], list):\r\n            return True\r\n        return False\r\n\r\n    def _config_is_python_task(self, config) -> bool:\r\n        if \"class\" in config:\r\n            return True\r\n        return False\r\n\r\n    def _get_yaml_path(self, name):\r\n        if name not in self.task_index:\r\n            raise ValueError\r\n        return self.task_index[name][\"yaml_path\"]\r\n\r\n    def _get_config(self, name):\r\n        if name not in self.task_index:\r\n            raise ValueError\r\n        yaml_path = self._get_yaml_path(name)\r\n        if yaml_path == -1:\r\n            return {}\r\n        else:\r\n            return utils.load_yaml_config(yaml_path, mode=\"full\")\r\n\r\n    def _get_tasklist(self, name):\r\n        if self._name_is_task(name):\r\n            raise ValueError\r\n        return self.task_index[name][\"task\"]\r\n\r\n    def _process_alias(self, config, group=None):\r\n        # If the group is not the same as the original\r\n        # group which the group alias was intended for,\r\n        # Set the group_alias to None instead.\r\n        if (\"group_alias\" in config) and (\"group\" in config) and group is not None:\r\n            if config[\"group\"] != group:\r\n                config[\"group_alias\"] = None\r\n        return config\r\n\r\n    def _class_has_config_in_constructor(self, cls):\r\n        constructor = getattr(cls, \"__init__\", None)\r\n        return (\r\n            \"config\" in inspect.signature(constructor).parameters\r\n            if constructor\r\n            else False\r\n        )\r\n\r\n    def _load_individual_task_or_group(\r\n        self,\r\n        name_or_config: Optional[Union[str, dict]] = None,\r\n        parent_name: Optional[str] = None,\r\n        update_config: Optional[dict] = None,\r\n    ) -> Mapping:\r\n        def _load_task(config, task):\r\n            if \"include\" in config:\r\n                config = {\r\n                    **utils.load_yaml_config(\r\n                        yaml_path=None,\r\n                        yaml_config={\"include\": config.pop(\"include\")},\r\n                        mode=\"full\",\r\n                    ),\r\n                    **config,\r\n                }\r\n            if self._config_is_python_task(config):\r\n                if self._class_has_config_in_constructor(config[\"class\"]):\r\n                    task_object = config[\"class\"](config=config)\r\n                else:\r\n                    task_object = config[\"class\"]()\r\n                if isinstance(task_object, ConfigurableTask):\r\n                    # very scuffed: set task name here. TODO: fixme?\r\n                    task_object.config.task = task\r\n            else:\r\n                task_object = ConfigurableTask(config=config)\r\n\r\n            return {task: task_object}\r\n\r\n        def _get_group_and_subtask_from_config(config):\r\n            group_name = ConfigurableGroup(config=config)\r\n            subtask_list = []\r\n            for task in group_name.config[\"task\"]:\r\n                if isinstance(task, str) and self._name_is_tag(task):\r\n                    subtask_list.extend(self._get_tasklist(task))\r\n                else:\r\n                    subtask_list.append(task)\r\n            return group_name, subtask_list\r\n\r\n        def _process_group_config(config, update_config=None):\r\n            if update_config is not None:\r\n                config = {**config, **update_config}\r\n            _update_config = {\r\n                k: v for k, v in config.items() if k not in GROUP_ONLY_KEYS\r\n            }\r\n            if not bool(_update_config):\r\n                _update_config = None\r\n\r\n            group_config = {k: v for k, v in config.items() if k in GROUP_ONLY_KEYS}\r\n            return group_config, _update_config\r\n\r\n        if isinstance(name_or_config, str):\r\n            if update_config is not None:\r\n                # Process name_or_config as a dict instead\r\n                name_or_config = {\"task\": name_or_config, **update_config}\r\n            elif self._name_is_task(name_or_config) or self._name_is_python_task(\r\n                name_or_config\r\n            ):\r\n                task_config = self._get_config(name_or_config)\r\n                return _load_task(task_config, task=name_or_config)\r\n            else:\r\n                subtask_list = self._get_tasklist(name_or_config)\r\n                if subtask_list == -1:\r\n                    group_config = self._get_config(name_or_config)\r\n                    group_config, update_config = _process_group_config(group_config)\r\n                    group_name, subtask_list = _get_group_and_subtask_from_config(\r\n                        group_config\r\n                    )\r\n                else:\r\n                    if self._name_is_tag(name_or_config):\r\n                        fn = partial(\r\n                            self._load_individual_task_or_group,\r\n                            update_config=name_or_config\r\n                            if isinstance(name_or_config, dict)\r\n                            else None,\r\n                        )\r\n                        return dict(\r\n                            collections.ChainMap(*map(fn, reversed(subtask_list)))\r\n                        )\r\n                    else:\r\n                        group_name = ConfigurableGroup(\r\n                            config={\"group\": name_or_config, \"task\": subtask_list}\r\n                        )\r\n\r\n        if isinstance(name_or_config, dict):\r\n            if self._config_is_task(name_or_config):\r\n                name = name_or_config.pop(\"task\")\r\n                if update_config is not None:\r\n                    name_or_config = {**name_or_config, **update_config}\r\n                # If the name is registered as a group\r\n                if self._name_is_group(name):\r\n                    group_config = self._get_config(name)\r\n\r\n                    group_config, update_config = _process_group_config(\r\n                        group_config, name_or_config\r\n                    )\r\n                    group_name, subtask_list = _get_group_and_subtask_from_config(\r\n                        group_config\r\n                    )\r\n                elif self._name_is_tag(name):\r\n                    subtask_list = self._get_tasklist(name)\r\n                    fn = partial(\r\n                        self._load_individual_task_or_group,\r\n                        update_config=name_or_config,\r\n                    )\r\n                    return dict(collections.ChainMap(*map(fn, reversed(subtask_list))))\r\n                else:\r\n                    if self._name_is_registered(name):\r\n                        base_task_config = self._get_config(name)\r\n\r\n                        # Check if this is a duplicate.\r\n                        if parent_name is not None:\r\n                            num_duplicate = len(\r\n                                list(\r\n                                    filter(\r\n                                        lambda x: x.startswith(name),\r\n                                        self.task_group_map[parent_name],\r\n                                    )\r\n                                )\r\n                            )\r\n                            if num_duplicate > 0:\r\n                                name = f\"{name}-{num_duplicate}\"\r\n                            self.task_group_map[parent_name].append(name)\r\n\r\n                        task_config = {\r\n                            **base_task_config,\r\n                            **name_or_config,\r\n                        }\r\n                    else:\r\n                        task_config = name_or_config\r\n                    return _load_task(task_config, task=name)\r\n            else:\r\n                group_config, update_config = _process_group_config(name_or_config)\r\n                group_name, subtask_list = _get_group_and_subtask_from_config(\r\n                    group_config\r\n                )\r\n\r\n        fn = partial(\r\n            self._load_individual_task_or_group,\r\n            parent_name=group_name,\r\n            update_config=update_config,\r\n        )\r\n        return {\r\n            group_name: dict(collections.ChainMap(*map(fn, reversed(subtask_list))))\r\n        }\r\n\r\n    def load_task_or_group(self, task_list: Optional[Union[str, list]] = None) -> dict:\r\n        \"\"\"Loads a dictionary of task objects from a list\r\n\r\n        :param task_list: Union[str, list] = None\r\n            Single string or list of string of task names to be loaded\r\n\r\n        :return\r\n            Dictionary of task objects\r\n        \"\"\"\r\n        if isinstance(task_list, str):\r\n            task_list = [task_list]\r\n\r\n        all_loaded_tasks = dict(\r\n            collections.ChainMap(*map(self._load_individual_task_or_group, task_list))\r\n        )\r\n        return all_loaded_tasks\r\n\r\n    def load_config(self, config: Dict):\r\n        return self._load_individual_task_or_group(config)\r\n\r\n    def _get_task_and_group(self, task_dir: str):\r\n        \"\"\"Creates a dictionary of tasks index with the following metadata,\r\n        - `type`, that can be either `task`, `python_task`, `group` or `tags`.\r\n            `task` refer to regular task configs, `python_task` are special\r\n            yaml files that only consists of `task` and `class` parameters.\r\n            `group` are group configs. `tags` are labels that can be assigned\r\n            to tasks to assist in sorting and calling tasks of certain themes.\r\n        - `yaml_path`, path to the yaml file. If the entry is a `group` that\r\n            was configured through a task config, the yaml_path will be -1\r\n            and all subtasks will be listed in `task` (see below)\r\n        - `task`, reserved for entries with `type` as `group`. This will list\r\n            all subtasks. When a group config is created (as opposed to task\r\n            config having `group` parameter set), this will be set to -1 to\r\n            avoid recursive indexing. The whole list of subtasks will be loaded\r\n            at evaluation.\r\n\r\n        :param task_dir: str\r\n            A directory to check for tasks\r\n\r\n        :return\r\n            Dictionary of task names as key and task metadata\r\n        \"\"\"\r\n\r\n        def _populate_tags_and_groups(config, task, tasks_and_groups, print_info):\r\n            # TODO: remove group in next release\r\n            for attr in [\"tag\", \"group\"]:\r\n                if attr in config:\r\n                    if attr == \"group\" and print_info:\r\n                        print_info = False\r\n                        # attr = \"tag\"\r\n\r\n                    attr_list = config[attr]\r\n                    if isinstance(attr_list, str):\r\n                        attr_list = [attr_list]\r\n\r\n                    for tag in attr_list:\r\n                        if tag not in tasks_and_groups:\r\n                            tasks_and_groups[tag] = {\r\n                                \"type\": \"tag\",\r\n                                \"task\": [task],\r\n                                \"yaml_path\": -1,\r\n                            }\r\n                        elif tasks_and_groups[tag][\"type\"] != \"tag\":\r\n                            self.logger.info(\r\n                                f\"The tag {tag} is already registered as a group, this tag will not be registered. \"\r\n                                \"This may affect tasks you want to call.\"\r\n                            )\r\n                            break\r\n                        else:\r\n                            tasks_and_groups[tag][\"task\"].append(task)\r\n\r\n        # TODO: remove group in next release\r\n        print_info = True\r\n        ignore_dirs = [\r\n            \"__pycache__\",\r\n            \".ipynb_checkpoints\",\r\n        ]\r\n        tasks_and_groups = collections.defaultdict()\r\n        for root, dirs, file_list in os.walk(task_dir):\r\n            dirs[:] = [d for d in dirs if d not in ignore_dirs]\r\n            for f in file_list:\r\n                if f.endswith(\".yaml\"):\r\n                    yaml_path = os.path.join(root, f)\r\n                    config = utils.load_yaml_config(yaml_path, mode=\"simple\")\r\n                    if self._config_is_python_task(config):\r\n                        # This is a python class config\r\n                        task = config[\"task\"]\r\n                        tasks_and_groups[task] = {\r\n                            \"type\": \"python_task\",\r\n                            \"yaml_path\": yaml_path,\r\n                        }\r\n                        _populate_tags_and_groups(\r\n                            config, task, tasks_and_groups, print_info\r\n                        )\r\n                    elif self._config_is_group(config):\r\n                        # This is a group config\r\n                        tasks_and_groups[config[\"group\"]] = {\r\n                            \"type\": \"group\",\r\n                            \"task\": -1,  # This signals that\r\n                            # we don't need to know\r\n                            # the task list for indexing\r\n                            # as it can be loaded\r\n                            # when called.\r\n                            \"yaml_path\": yaml_path,\r\n                        }\r\n\r\n                        # # Registered the level 1 tasks from a group config\r\n                        # for config in config[\"task\"]:\r\n                        #     if isinstance(config, dict) and self._config_is_task(config):\r\n                        #         task = config[\"task\"]\r\n                        #         tasks_and_groups[task] = {\r\n                        #             \"type\": \"task\",\r\n                        #             \"yaml_path\": yaml_path,\r\n                        #             }\r\n\r\n                    elif self._config_is_task(config):\r\n                        # This is a task config\r\n                        task = config[\"task\"]\r\n                        tasks_and_groups[task] = {\r\n                            \"type\": \"task\",\r\n                            \"yaml_path\": yaml_path,\r\n                        }\r\n                        _populate_tags_and_groups(\r\n                            config, task, tasks_and_groups, print_info\r\n                        )\r\n                    else:\r\n                        self.logger.debug(f\"File {f} in {root} could not be loaded\")\r\n\r\n        return tasks_and_groups\r\n\r\n\r\ndef get_task_name_from_config(task_config: Dict[str, str]) -> str:\r\n    if \"task\" in task_config:\r\n        return task_config[\"task\"]\r\n    if \"dataset_name\" in task_config:\r\n        return \"{dataset_path}_{dataset_name}\".format(**task_config)\r\n    else:\r\n        return \"{dataset_path}\".format(**task_config)\r\n\r\n\r\ndef get_task_name_from_object(task_object):\r\n    if hasattr(task_object, \"config\"):\r\n        return task_object._config[\"task\"]\r\n\r\n    # TODO: scrap this\r\n    # this gives a mechanism for non-registered tasks to have a custom name anyways when reporting\r\n    return (\r\n        task_object.EVAL_HARNESS_NAME\r\n        if hasattr(task_object, \"EVAL_HARNESS_NAME\")\r\n        else type(task_object).__name__\r\n    )\r\n\r\n\r\ndef _check_duplicates(task_dict: dict) -> List[str]:\r\n    \"\"\"helper function solely used in validating get_task_dict output.\r\n    Takes the output of lm_eval.evaluator_utils.get_subtask_list and\r\n    returns a list of all leaf subtasks contained within, and errors if any such leaf subtasks are\r\n    \"oversubscribed\" to several disjoint groups.\r\n    \"\"\"\r\n    subtask_names = []\r\n    for key, value in task_dict.items():\r\n        subtask_names.extend(value)\r\n\r\n    duplicate_tasks = {\r\n        task_name for task_name in subtask_names if subtask_names.count(task_name) > 1\r\n    }\r\n\r\n    # locate the potentially problematic groups that seem to 'compete' for constituent subtasks\r\n    competing_groups = [\r\n        group\r\n        for group in task_dict.keys()\r\n        if len(set(task_dict[group]).intersection(duplicate_tasks)) > 0\r\n    ]\r\n\r\n    if len(duplicate_tasks) > 0:\r\n        raise ValueError(\r\n            f\"Found 1 or more tasks while trying to call get_task_dict() that were members of more than 1 called group: {list(duplicate_tasks)}. Offending groups: {competing_groups}. Please call groups which overlap their constituent tasks in separate evaluation runs.\"\r\n        )\r\n\r\n\r\ndef get_task_dict(\r\n    task_name_list: Union[str, List[Union[str, Dict, Task]]],\r\n    task_manager: Optional[TaskManager] = None,\r\n):\r\n    \"\"\"Creates a dictionary of task objects from either a name of task, config, or prepared Task object.\r\n\r\n    :param task_name_list: List[Union[str, Dict, Task]]\r\n        Name of model or LM object, see lm_eval.models.get_model\r\n    :param task_manager: TaskManager = None\r\n        A TaskManager object that stores indexed tasks. If not set,\r\n        task_manager will load one. This should be set by the user\r\n        if there are additional paths that want to be included\r\n        via `include_path`\r\n\r\n    :return\r\n        Dictionary of task objects\r\n    \"\"\"\r\n\r\n    task_name_from_string_dict = {}\r\n    task_name_from_config_dict = {}\r\n    task_name_from_object_dict = {}\r\n\r\n    if isinstance(task_name_list, str):\r\n        task_name_list = [task_name_list]\r\n    elif isinstance(task_name_list, list):\r\n        if not all([isinstance(task, (str, dict, Task)) for task in task_name_list]):\r\n            raise TypeError(\r\n                \"Expected all list items to be of types 'str', 'dict', or 'Task', but at least one entry did not match.\"\r\n            )\r\n    else:\r\n        raise TypeError(\r\n            f\"Expected a 'str' or 'list' but received {type(task_name_list)}.\"\r\n        )\r\n\r\n    string_task_name_list = [task for task in task_name_list if isinstance(task, str)]\r\n    others_task_name_list = [\r\n        task for task in task_name_list if not isinstance(task, str)\r\n    ]\r\n    if len(string_task_name_list) > 0:\r\n        if task_manager is None:\r\n            task_manager = TaskManager()\r\n\r\n        task_name_from_string_dict = task_manager.load_task_or_group(\r\n            string_task_name_list\r\n        )\r\n\r\n    for task_element in others_task_name_list:\r\n        if isinstance(task_element, dict):\r\n            task_name_from_config_dict = {\r\n                **task_name_from_config_dict,\r\n                **task_manager.load_config(config=task_element),\r\n            }\r\n\r\n        elif isinstance(task_element, Task):\r\n            task_name_from_object_dict = {\r\n                **task_name_from_object_dict,\r\n                get_task_name_from_object(task_element): task_element,\r\n            }\r\n\r\n    if not set(task_name_from_string_dict.keys()).isdisjoint(\r\n        set(task_name_from_object_dict.keys())\r\n    ):\r\n        raise ValueError\r\n\r\n    final_task_dict = {\r\n        **task_name_from_string_dict,\r\n        **task_name_from_config_dict,\r\n        **task_name_from_object_dict,\r\n    }\r\n\r\n    # behavior can get odd if one tries to invoke several groups that \"compete\" for the same task.\r\n    # (notably, because one could request several num_fewshot values at once in GroupConfig overrides for the subtask\r\n    # and we'd be unsure which to use and report.)\r\n    # we explicitly check and error in this case.\r\n    _check_duplicates(get_subtask_list(final_task_dict))\r\n\r\n    return final_task_dict\r\n"}
