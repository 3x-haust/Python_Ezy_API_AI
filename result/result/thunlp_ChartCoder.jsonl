{"repo_info": {"repo_name": "ChartCoder", "repo_owner": "thunlp", "repo_url": "https://github.com/thunlp/ChartCoder"}}
{"type": "source_file", "path": "data_generator/entry.py", "content": "import argparse\nimport time\nfrom datetime import datetime\nimport json\nimport os\nimport random\nfrom multiprocessing import Process, Manager, Lock, Value\n\n# from generate_data.dashboard.generate_data import dashboard_batch_worker\n# from generate_data.organization.generate_data import organization_batch_worker\n# from generate_data.algorithm.generate_data import algorithm_batch_worker\n# from generate_data.flowchart.generate_data import flowchart_batch_worker\nfrom generate_data.chart.generate_data import chart_batch_worker\n\nchart_types = ['pie', 'line', 'bar', 'bar_num', \"3d\", \"area\", \"box\", \"bubble\", \n               \"candlestick\", \"funnel\", \"heatmap\", \"multi-axes\", \"radar\", \"ring\", \"rose\", \n               \"treemap\"]\n\nweights = [10, 10, 8, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6]\n\ndef worker(queue, lock, success_num, fail_num, batch_worker, args):\n    while not queue.empty():\n        chart_type = random.choices(chart_types, weights=weights, k=1)[0]\n\n        success, fail = batch_worker(queue.get(), chart_type, args)\n        with lock:\n            success_num.value += success\n            fail_num.value += fail\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--type\",\n        type=str,\n        required=True,\n        help=\"The type of data to generate. Options: organization, algorithm, flowchart, dashboard.\",\n    )\n    parser.add_argument(\n        \"--batch_dir\",\n        type=str,\n        default=\"generate_data/?/data/\",\n        help=\"The directory where the batch is stored.\",\n    )\n    parser.add_argument(\n        \"--seed_domains_path\",\n        type=str,\n        default=\"generate_data/?/data/seeds/seeds.json\",\n        help=\"The path to the human written domains.\",\n    )\n    parser.add_argument(\n        \"--num_data\",\n        type=int,\n        default=20,\n        help=\"Number of data to generate.\",\n    )\n    parser.add_argument(\n        \"--batch_size\",\n        type=int,\n        default=8,\n        help=\"Number of data for each batch.\",\n    )\n    return parser.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    args.batch_dir = args.batch_dir.replace(\"?\", args.type)\n    args.seed_domains_path = args.seed_domains_path.replace(\"?\", args.type)\n    batch_worker = None\n    if args.type == \"organization\":\n        batch_worker = organization_batch_worker\n    elif args.type == \"flowchart\":\n        batch_worker = flowchart_batch_worker\n    elif args.type == \"dashboard\":\n        batch_worker = dashboard_batch_worker\n    elif args.type == \"algorithm\":\n        batch_worker = algorithm_batch_worker\n    elif args.type == \"chart\":\n        batch_worker = chart_batch_worker\n\n    random.seed(os.getpid())\n    start_time = datetime.now()\n\n    entity_words = []\n    with open(os.path.join(args.batch_dir, \"lm_generated_seed_domains.json\"), \"r\") as f:\n        for line in f:\n            data = json.loads(line)\n            entity_words.extend(data[\"entity_words\"])\n\n    with open(args.seed_domains_path, \"r\") as f:\n        data = json.load(f)\n    entity_words.extend(data[\"entity_words\"])\n\n    # set sample size\n    random.shuffle(entity_words)\n    sampled_entity_words = random.sample(entity_words, min(1000, len(entity_words)))\n\n    success = Value(\"i\", 0)\n    fail = Value(\"i\", 0)\n    lock = Lock()\n\n    domain_queue = []\n    for domain in sampled_entity_words:\n        domain_queue.append(domain)\n\n    k = 100\n    if k > len(domain_queue):\n        k = len(domain_queue)\n\n    # Create k lists, each equal to the elements in domain_queue\n    sublists = []\n    with Manager() as manager:\n        for i in range(k):\n            start_index = i * (len(domain_queue) // k)\n            end_index = (i + 1) * (len(domain_queue) // k)\n            # If it's not the last sublist, make sure it's not out of range\n            if i < k - 1:\n                sublist = domain_queue[start_index:end_index]\n            else:\n                # If it is the last sub-list, contains all remaining elements\n                sublist = domain_queue[start_index:]\n\n            queue = manager.Queue()\n            for domain in sublist:\n                queue.put(domain)\n\n            sublists.append(queue)\n\n        processes = []\n        for i in range(k):\n            time.sleep(random.randint(5, 10) / 10)\n            p = Process(target=worker, args=(sublists[i], lock, success, fail, batch_worker, args))\n            p.start()\n            processes.append(p)\n\n        for p in processes:\n            p.join()\n\n        day = datetime.now().strftime(\"%Y%m%d\")\n        batch_dir = os.path.join(args.batch_dir, f\"batch{day}\")\n        data_dict = []\n        if os.path.exists(os.path.join(batch_dir, \"lm_generated_data.json\")):\n            with open(os.path.join(batch_dir, \"lm_generated_data.json\"), \"r\") as fin:\n                for line in fin:\n                    data_dict.append(json.loads(line))\n\n        with open(os.path.join(batch_dir, f\"{day}.json\"), \"w\") as fout:\n            fout.write(json.dumps(data_dict, indent=4))\n\n    # calculate time\n    end_time = datetime.now()\n    print(\n        f\"\\033[1;34mSuccess: {success.value}, \"\n        f\"Fail: {fail.value}, \"\n        f\"Time used: {end_time - start_time}\\033[0m\"\n    )\n"}
{"type": "source_file", "path": "llava/model/language_model/llava_mpt.py", "content": "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom typing import Optional, Tuple\n\nimport torch\n\nfrom transformers import AutoConfig, AutoModelForCausalLM, MptConfig, MptForCausalLM, MptModel, GenerationConfig\nfrom llava.model.llava_arch import LlavaMetaModel, LlavaMetaForCausalLM\n\n\nclass LlavaMptConfig(MptConfig):\n    model_type = \"llava_mpt\"\n\n\nclass LlavaMptModel(LlavaMetaModel, MptModel):\n    config_class = LlavaMptConfig\n\n    def __init__(self, config: MptConfig):\n        config.hidden_size = config.d_model\n        super(LlavaMptModel, self).__init__(config)\n\n    def embed_tokens(self, x):\n        return self.wte(x)\n\n\nclass LlavaMptForCausalLM(MptForCausalLM, LlavaMetaForCausalLM):\n    config_class = LlavaMptConfig\n    supports_gradient_checkpointing = True\n\n    def __init__(self, config):\n        super(MptForCausalLM, self).__init__(config)\n\n        config.model_type = \"llava_mpt\"\n        config.rope_scaling = None\n        self.generation_config = GenerationConfig(\n            temperature=0.0,\n            max_new_tokens=1024,\n            do_sample=False,\n            top_p=None,\n        )\n\n        self.transformer = LlavaMptModel(config)\n        self.lm_head = torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.transformer\n\n    def _set_gradient_checkpointing(self, module, value=False):\n        if isinstance(module, LlavaMptModel):\n            module.gradient_checkpointing = value\n\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        inputs_embeds: Optional[torch.Tensor] = None,\n        labels: Optional[torch.Tensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        cache_position=None,\n        images=None,\n    ):\n\n        input_ids, attention_mask, past_key_values, inputs_embeds, labels = self.prepare_inputs_labels_for_multimodal(input_ids, attention_mask, past_key_values, labels, images)\n\n        return super().forward(\n            input_ids,\n            past_key_values=past_key_values,\n            attention_mask=attention_mask,\n            inputs_embeds=inputs_embeds,\n            labels=labels,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n        images = kwargs.pop(\"images\", None)\n        _inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs)\n        _inputs[\"images\"] = images\n        return _inputs\n\n\nAutoConfig.register(\"llava_mpt\", LlavaMptConfig)\nAutoModelForCausalLM.register(LlavaMptConfig, LlavaMptForCausalLM)\n"}
{"type": "source_file", "path": "data_generator/generate_data/chart/generate_data.py", "content": "import os\nimport time\nfrom datetime import datetime\nimport random\nimport re\nimport json\nfrom multiprocessing import Process, Manager\n\nfrom .templates import prompts\n\nfrom ..utils import gpt4_tools as gpt4\n# from ..utils.steps_processor import CalculationProcessor\n\nchart_types = ['pie', 'line', 'bar', 'bar_num', \"3d\", \"area\", \"box\", \"bubble\", \n               \"candlestick\", \"funnel\", \"heatmap\", \"multi-axes\", \"radar\", \"ring\", \"rose\", \n               \"treemap\"]\n\ndef format_parser(data, format=\"json\") -> str:\n    pattern = None\n    if format == \"json\":\n        pattern = r\"```json(.*?)```\"\n    elif format == \"python\":\n        pattern = r\"```python(.*?)```\"\n    raw_data = re.findall(pattern, data, re.DOTALL)\n    # maybe ```format``` pattern\n    if raw_data:\n        raw_data = raw_data[0]\n    else:\n        raw_data = data\n    return raw_data\n\ndef generate_raw_data(domain, context):\n    \"\"\"\n    Generate raw data by `prompt`\n    \"\"\"\n    prompt = prompts.disk_data_prompt(domain)\n    context.append({\"role\": \"user\", \"content\": prompt})\n    data, _ = gpt4.send_chat_request_azure(\n        context, temp=0.7, sample_n=1\n    )\n    data = format_parser(data, format=\"json\")\n    context.append({\"role\": \"assistant\", \"content\": data})\n    return data\n\ndef generate_caption_summary(domain, context, chart_type):\n    \"\"\"\n    Generate caption & summary for the data\n    \"\"\"\n    prompt = prompts.disk_caption_prompt(chart_type)\n    context.append({\"role\": \"user\", \"content\": prompt})\n    caption, _ = gpt4.send_chat_request_azure(\n        context, temp=0.6, sample_n=1\n    )\n    caption = format_parser(caption, format=\"json\")\n    context.append({\"role\": \"assistant\", \"content\": caption})\n\n    return caption, None\n\ndef generate_data(domain, context, chart_type, semi_finished_list, args):\n    # create a new batch directory\n    batch_dir = (\n        args.batch_dir + \"batch\" + datetime.now().strftime(\"%Y%m%d\") + \"/\"\n    )\n    os.makedirs(batch_dir, exist_ok=True)\n    os.makedirs(os.path.join(batch_dir, \"plots\"), exist_ok=True)\n\n    # generate data\n    try:\n        raw_data = generate_raw_data(domain, context)\n        data = json.loads(raw_data)\n\n        caption, summary = generate_caption_summary(domain, context, chart_type)\n        caption_text = json.loads(caption)[\"caption\"]\n\n        now = datetime.now()\n        microsecond = f\"{int(now.microsecond / 1000):03d}\"\n        nanosecond = f\"{now.microsecond % 1000:03d}\"\n\n        semi_finished_data_point = {\n            \"domain\": domain,\n            \"csv_data\": data[\"csv_data\"],\n            \"caption\": caption_text,\n            \"id\": now.strftime(\"%d%H%M%S\") + str(microsecond),\n        }\n\n        # TODO: change the path to args.output_file\n        with open(os.path.join(batch_dir, \"lm_generated_semi.json\"), \"a\") as fout:\n            fout.write(json.dumps(semi_finished_data_point) + \"\\n\")\n\n        return semi_finished_data_point\n    except Exception as e:\n        print(f\"Error: {e}, the semi-finished data for {domain} will be discarded.\")\n        return None\n    \ndef execute_code(code, result):\n    try:\n        exec(code, globals())\n    except Exception as e:\n        result[\"error\"] = e\n\ndef generate_plots(domain: str, data_point: dict, context: list[dict], chart_type, args, retries=3):\n    \"\"\"\n    Use python code to generate plots\n    \"\"\"\n\n    context.append({\"role\": \"user\", \"content\": \"The data is:\\n\"})\n    context.append({\"role\": \"user\", \"content\": json.dumps(data_point)})\n\n    # TODO: change the path to args.batch_dir\n    batch_dir = (\n        args.batch_dir + \"batch\" + datetime.now().strftime(\"%Y%m%d\") + \"/plots/\"\n    )\n\n    additional_requirements = {\n        \"save path\": f\"{batch_dir}{data_point['id']}.png\",\n        \"save parameters\": \"bbox_inches='tight', dpi=80\",\n        \"show plot\": \"do not show the plot\",\n    }\n\n    prompt = prompts.disk_code_prompt(\n        data_point[\"csv_data\"], data_point[\"caption\"], additional_requirements, chart_type\n    )\n    context.append({\"role\": \"user\", \"content\": prompt})\n\n    pattern = r\"```python(.*?)```\"\n    code = None\n    raw_code = None\n\n    for i in range(retries):\n        try:\n            code, _ = gpt4.send_chat_request_azure(\n                context, temp=0.6, sample_n=1\n            )\n            raw_code = re.findall(pattern, code, re.DOTALL)\n            if not raw_code:\n                return None\n            else:\n                with Manager() as manager:\n                    res = manager.dict()\n                    p = Process(target=execute_code, args=(raw_code[0], res))\n                    p.start()\n                    p.join()\n\n                    if \"error\" in res.keys():\n                        raise Exception(res[\"error\"])\n\n                    data_point[\"code\"] = raw_code[0]\n                    context.append({\"role\": \"assistant\", \"content\": code})\n                    break\n\n        except Exception as e:\n            context.append({\"role\": \"assistant\", \"content\": code})\n            context.append(\n                {\"role\": \"user\", \"content\": f\"Error: {e}, correct your code.\\n\"}\n            )\n            print(f\"Error: {e}, correct your code.\")\n            print(f\"Trying correcting code for {i + 1} times for {domain}\")\n            if i + 1 >= retries:\n                print(f\"Error: {e}, the plot for {domain} will be discarded.\")\n                return None\n            \n    save_path = f\"{batch_dir}{data_point['id']}.png\"\n    return raw_code[0], save_path\n\ndef chart_batch_worker(domain, chart_type, args):\n    semi_finished_list = []\n    context_msgs = []\n    random.seed(os.getpid())\n\n    # number of data generated for each domain\n    while len(semi_finished_list) < 1:\n        semi_finished_data_point = generate_data(\n            domain, context_msgs, chart_type, semi_finished_list, args\n        )\n        if semi_finished_data_point is not None:\n            semi_finished_list.append(semi_finished_data_point)\n        context_msgs.clear()\n        time.sleep(random.randint(1, 10) / 10)\n\n    # generate plots and qa\n    success = 0\n    for data_point in semi_finished_list:\n        context_msgs.clear()\n        code, savepath = generate_plots(domain, data_point, context_msgs, chart_type, args)\n        if code is None:\n            continue\n        else:\n            data_point['code'] = code\n            data_point['image'] = savepath\n            data_point['chart_type'] = chart_type\n            with open(os.path.join(\"lm_generated_data_with_code_type.json\"), \"a\") as fout:\n                fout.write(json.dumps(semi_finished_data_point) + \"\\n\")\n        success += 1\n\n    return success, len(semi_finished_list) - success"}
{"type": "source_file", "path": "llava/model/consolidate.py", "content": "\"\"\"\nUsage:\npython3 -m llava.model.consolidate --src ~/model_weights/llava-7b --dst ~/model_weights/llava-7b_consolidate\n\"\"\"\n\nimport argparse\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom llava.model import *\nfrom llava.model.utils import auto_upgrade\n\n\ndef consolidate_ckpt(src_path, dst_path):\n    print(\"Loading model\")\n    auto_upgrade(src_path)\n    src_model = AutoModelForCausalLM.from_pretrained(src_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n    src_tokenizer = AutoTokenizer.from_pretrained(src_path, use_fast=False)\n    src_model.save_pretrained(dst_path)\n    src_tokenizer.save_pretrained(dst_path)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--src\", type=str, required=True)\n    parser.add_argument(\"--dst\", type=str, required=True)\n\n    args = parser.parse_args()\n\n    consolidate_ckpt(args.src, args.dst)\n"}
{"type": "source_file", "path": "llava/__init__.py", "content": "from .model import LlavaLlamaForCausalLM\n"}
{"type": "source_file", "path": "data_generator/generate_data/utils/steps_processor.py", "content": "import json\n\nfrom . import gpt4_tools as gpt4\n\n\nclass CalculationProcessor:\n    def __init__(self, data):\n        self.context = []\n        self.context.append({\"role\": \"user\", \"content\": f\"The information: {data}\"})\n        self.context.append({\"role\": \"user\", \"content\": \"You are given some information and several QA pairs having calculations.\"\n                                                        \"You need to get the answer step by step.\\n\"\n                                                        \"Do not generate any other content except {\\\"steps\\\": [\\\"...\\\"]}\"})\n\n    def process(self, query):\n\n        self.context.append({\"role\": \"user\", \"content\": f\"{query}\"})\n        solution, _ = gpt4.send_chat_request_azure(self.context, temp=0.2, sample_n=1, engine='gpt-3.5-turbo-0125')\n        self.context.pop()\n\n        solution = json.loads(solution)\n        steps = solution['steps']\n\n        return steps\n"}
{"type": "source_file", "path": "inference.py", "content": "from llava.model.builder import load_pretrained_model\nfrom llava.mm_utils import tokenizer_image_token, process_images, get_model_name_from_path\nfrom llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\nfrom llava.conversation import conv_templates, SeparatorStyle\n\nfrom llava.model import *\nimport torch\n\nimport os\nfrom PIL import Image\n\nclass ChartCoder:\n    def __init__(\n        self,\n        temperature=0.1,\n        max_tokens=2048,\n        top_p=0.95,\n        context_length=2048,\n    ):\n        self.temperature = temperature\n        self.max_tokens = max_tokens\n        self.top_p = top_p\n        self.context_length = context_length\n\n        # Note: change to you path\n        pretrained = \"/mnt/afs/chartcoder\"\n        model_name = \"llava_deepseekcoder\"\n        device_map = \"auto\"\n        self.system_message = \"\"\n\n        tokenizer, model, image_processor, max_length = load_pretrained_model(pretrained, None, model_name, device_map=device_map)\n        model.eval()\n\n        self.tokenizer = tokenizer   \n        self.model = model\n        self.image_processor = image_processor\n        self.IMAGE_TOKEN_INDEX = IMAGE_TOKEN_INDEX\n\n    def generate(self, instruction, image_path):\n        image = Image.open(image_path).convert('RGB')\n        prompt = self.system_message + f\"### Instruction:\\n{DEFAULT_IMAGE_TOKEN}\\n{instruction}\\n### Response:\\n\"\n        input_ids = tokenizer_image_token(prompt, self.tokenizer, self.IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\n        image_tensor = process_images([image], self.image_processor, self.model.config)[0]\n\n        with torch.inference_mode():\n            output_ids = self.model.generate(\n                input_ids,\n                images=image_tensor.unsqueeze(0).half().cuda(),\n                image_sizes=[image.size],\n                do_sample=True if self.temperature > 0 else False,\n                temperature=self.temperature,\n                top_p=self.top_p,\n                max_new_tokens=self.max_tokens,\n                use_cache=True)\n        outputs = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n        return outputs\n\n"}
{"type": "source_file", "path": "llava/model/language_model/llava_llama.py", "content": "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\n\nfrom transformers import AutoConfig, AutoModelForCausalLM, LlamaConfig\n\nfrom torch.nn import CrossEntropyLoss\n\n\n# , LlamaModel, LlamaForCausalLM, GenerationConfig\n# from .modeling_llama import LlamaModel, LlamaForCausalLM\nfrom transformers import LlamaModel, LlamaForCausalLM\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nfrom transformers.generation.utils import GenerateOutput\n\nfrom llava.model.llava_arch import LlavaMetaModel, LlavaMetaForCausalLM\n\n\nclass LlavaConfig(LlamaConfig):\n    model_type = \"llava_llama\"\n    temperature: float = 0.0  # reset to 0.0, previously 0.9 for Vicuna\n    max_new_tokens: int = 1024\n    do_sample: bool = False\n    top_p: Optional[float] = None\n    # rope_scaling: Optional[dict] = {}\n\n\nclass LlavaLlamaModel(LlavaMetaModel, LlamaModel):\n    config_class = LlavaConfig\n\n    def __init__(self, config: LlamaConfig):\n        super(LlavaLlamaModel, self).__init__(config)\n\n\nclass LlavaLlamaForCausalLM(LlamaForCausalLM, LlavaMetaForCausalLM):\n    config_class = LlavaConfig\n\n    def __init__(self, config):\n        LlamaForCausalLM.__init__(self, config)\n\n        # configure default generation settings\n        config.model_type = \"llava_llama\"\n        # config.rope_scaling = None\n\n        self.model = LlavaLlamaModel(config)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.model\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        images: Optional[torch.FloatTensor] = None,\n        image_sizes: Optional[List[List[int]]] = None,\n        return_dict: Optional[bool] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        dpo_forward: Optional[bool] = None,\n        cache_position=None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n\n        if inputs_embeds is None:\n            (input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels) = self.prepare_inputs_labels_for_multimodal(input_ids, position_ids, attention_mask, past_key_values, labels, images, modalities, image_sizes)\n\n        if dpo_forward:\n            outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n            hidden_states = outputs[0]\n            logits = self.lm_head(hidden_states)\n            return logits, labels\n\n        else:\n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                labels=labels,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs: Optional[torch.Tensor] = None,\n        images: Optional[torch.Tensor] = None,\n        image_sizes: Optional[torch.Tensor] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        **kwargs,\n    ) -> Union[GenerateOutput, torch.LongTensor]:\n        modalities = kwargs.pop(\"modalities\", None) if \"modalities\" in kwargs and modalities is None else modalities\n        position_ids = kwargs.pop(\"position_ids\", None)\n        attention_mask = kwargs.pop(\"attention_mask\", None)\n        if \"inputs_embeds\" in kwargs:\n            raise NotImplementedError(\"`inputs_embeds` is not supported\")\n\n        if images is not None:\n            (inputs, position_ids, attention_mask, _, inputs_embeds, _) = self.prepare_inputs_labels_for_multimodal(inputs, position_ids, attention_mask, None, None, images, modalities, image_sizes=image_sizes)\n        else:\n            inputs_embeds = self.get_model().embed_tokens(inputs)\n\n        return super().generate(position_ids=position_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, **kwargs)\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n        images = kwargs.pop(\"images\", None)\n        image_sizes = kwargs.pop(\"image_sizes\", None)\n        inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs)\n        if images is not None:\n            inputs[\"images\"] = images\n        if image_sizes is not None:\n            inputs[\"image_sizes\"] = image_sizes\n        return inputs\n\n\nAutoConfig.register(\"llava_llama\", LlavaConfig)\nAutoModelForCausalLM.register(LlavaConfig, LlavaLlamaForCausalLM)\n"}
{"type": "source_file", "path": "data_generator/generate_data/chart/templates/plot_params.py", "content": "import random\n\n# packages\n# packages = ['matplotlib==3.8.0', 'pandas==2.1.2', 'textwrap']\npackages = ['matplotlib==3.8.0']\n\nplot_styles = [\n    'default',\n    'classic',\n    'Solarize_Light2',\n    'dark_background',\n    'ggplot',\n    'fivethirtyeight',\n    'fast',\n    'bmh'\n]\n\n# tab20 palette\ncolors = [\n    \"#F0F8FF\",\n    \"#FAEBD7\",\n    \"#00FFFF\",\n    \"#7FFFD4\",\n    \"#F0FFFF\",\n    \"#F5F5DC\",\n    \"#FFE4C4\",\n    \"#000000\",\n    \"#FFEBCD\",\n    \"#0000FF\",\n    \"#8A2BE2\",\n    \"#A52A2A\",\n    \"#DEB887\",\n    \"#5F9EA0\",\n    \"#7FFF00\",\n    \"#D2691E\",\n    \"#FF7F50\",\n    \"#6495ED\",\n    \"#FFF8DC\",\n    \"#DC143C\",\n    \"#00FFFF\",\n    \"#00008B\",\n    \"#008B8B\",\n    \"#B8860B\",\n    \"#A9A9A9\",\n    \"#006400\",\n    \"#BDB76B\",\n    \"#9932CC\",\n    \"#8B0000\",\n    \"#E9967A\",\n    \"#8FBC8F\",\n    \"#483D8B\",\n    \"#2F4F4F\",\n    \"#00CED1\",\n    \"#9400D3\",\n    \"#FF1493\",\n    \"#00BFFF\",\n    \"#696969\",\n    \"#1E90FF\",\n    \"#B22222\",\n]\n\nchart_types = ['pie', 'line', 'bar', 'bar_num', \"3d\", \"area\", \"box\", \"bubble\", \n               \"candlestick\", \"funnel\", \"heatmap\", \"multi-axes\", \"radar\", \"ring\", \"rose\", \n               \"treemap\", \"violin\", \"scatter\", \"quiver\",\"inset\", \"counter\"]\n# chart_types = ['bar']\n\ngrid_visibility = [True, False]\n\ngrid_line_styles = ['-', '--', '-.', ':']\n\nline_styles = ['solid', 'dashed', 'dotted', 'dashdot']\n\nmarker_styles = [\".\", \",\", \"v\", \"^\", \"<\", \">\", \"s\", \"p\", \"P\", \"*\", \"h\", \"H\", \"+\", \"x\", \"X\", \"D\", \"d\", \"|\", \"_\"]\n\nbar_styles = ['grouped', 'stacked']\n\nbar_arrangement = ['vertical', 'horizontal']\n\nfont_types = ['serif', 'sans-serif', 'monospace']\n\ntick_label_styles = ['sci', 'scientific', 'plain']\n\nhatch = ['/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*']\n\nlegend_positions = {\n    \"upper-right\": {\n        \"loc\": 2,  # upper left\n        \"bbox_to_anchor\": \"(1.1, 1.1)\"\n    },\n    \"lower-right\": {\n        \"loc\": 3,  # lower left\n        \"bbox_to_anchor\": \"(1.1, -0.1)\"\n    },\n    \"upper-left\": {\n        \"loc\": 1,  # upper right\n        \"bbox_to_anchor\": \"(-0.1, 1.1)\"\n    },\n    \"lower-left\": {\n        \"loc\": 4,  # lower right\n        \"bbox_to_anchor\": \"(-0.1, -0.1)\"\n    }\n}\n\n\ndef legend_position():\n    return random.choice(list(legend_positions.keys()))\n"}
{"type": "source_file", "path": "llava/model/language_model/llava_mixtral.py", "content": "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\n\nfrom transformers import AutoConfig, AutoModelForCausalLM, MixtralConfig, MixtralModel, MixtralForCausalLM, GenerationConfig\n\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nfrom transformers.generation.utils import GenerateOutput\n\nfrom ..llava_arch import LlavaMetaModel, LlavaMetaForCausalLM\n\n\nclass LlavaMixtralConfig(MixtralConfig):\n    model_type = \"llava_mixtral\"\n\n\nclass LlavaMixtralModel(LlavaMetaModel, MixtralModel):\n    config_class = LlavaMixtralConfig\n\n    def __init__(self, config: MixtralConfig):\n        super(LlavaMixtralModel, self).__init__(config)\n\n\nclass LlavaMixtralForCausalLM(MixtralForCausalLM, LlavaMetaForCausalLM):\n    config_class = LlavaMixtralConfig\n\n    def __init__(self, config):\n        super(MixtralForCausalLM, self).__init__(config)\n\n        config.model_type = \"llava_mixtral\"\n        config.rope_scaling = None\n        self.model = LlavaMixtralModel(config)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.model\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        images: Optional[torch.FloatTensor] = None,\n        image_sizes: Optional[List[List[int]]] = None,\n        return_dict: Optional[bool] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        dpo_forward: Optional[bool] = None,\n        cache_position=None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n\n        if inputs_embeds is None:\n            (input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels) = self.prepare_inputs_labels_for_multimodal(input_ids, position_ids, attention_mask, past_key_values, labels, images, modalities, image_sizes)\n\n        if dpo_forward:\n            outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n            hidden_states = outputs[0]\n            logits = self.lm_head(hidden_states)\n            return logits, labels\n\n        else:\n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                labels=labels,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs: Optional[torch.Tensor] = None,\n        images: Optional[torch.Tensor] = None,\n        image_sizes: Optional[torch.Tensor] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        **kwargs,\n    ) -> Union[GenerateOutput, torch.LongTensor]:\n        position_ids = kwargs.pop(\"position_ids\", None)\n        attention_mask = kwargs.pop(\"attention_mask\", None)\n        if \"inputs_embeds\" in kwargs:\n            raise NotImplementedError(\"`inputs_embeds` is not supported\")\n\n        if images is not None:\n            (inputs, position_ids, attention_mask, _, inputs_embeds, _) = self.prepare_inputs_labels_for_multimodal(inputs, position_ids, attention_mask, None, None, images, modalities, image_sizes=image_sizes)\n        else:\n            inputs_embeds = self.get_model().embed_tokens(inputs)\n\n        return super().generate(position_ids=position_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, **kwargs)\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n        images = kwargs.pop(\"images\", None)\n        image_sizes = kwargs.pop(\"image_sizes\", None)\n        inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs)\n        if images is not None:\n            inputs[\"images\"] = images\n        if image_sizes is not None:\n            inputs[\"image_sizes\"] = image_sizes\n        return inputs\n\n\nAutoConfig.register(\"llava_mixtral\", LlavaMixtralConfig)\nAutoModelForCausalLM.register(LlavaMixtralConfig, LlavaMixtralForCausalLM)\n"}
{"type": "source_file", "path": "llava/model/language_model/llava_gemma.py", "content": "#    Copyright 2024 Duc Q. Nguyen, Haotian Liu and Bo Li\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\n\nfrom transformers import AutoConfig, AutoModelForCausalLM, GemmaConfig, GemmaModel, GemmaForCausalLM\n\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nfrom transformers.generation.utils import GenerateOutput\n\nfrom ..llava_arch import LlavaMetaModel, LlavaMetaForCausalLM\n\n\nclass LlavaGemmaConfig(GemmaConfig):\n    model_type = \"llava_gemma\"\n\n\nclass LlavaGemmaModel(LlavaMetaModel, GemmaModel):\n    config_class = LlavaGemmaConfig\n\n    def __init__(self, config: GemmaConfig):\n        super(LlavaGemmaModel, self).__init__(config)\n\n\nclass LlavaGemmaForCausalLM(GemmaForCausalLM, LlavaMetaForCausalLM):\n    config_class = LlavaGemmaConfig\n\n    def __init__(self, config):\n        super(GemmaForCausalLM, self).__init__(config)\n        self.model = LlavaGemmaModel(config)\n\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.model\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        images: Optional[torch.FloatTensor] = None,\n        image_sizes: Optional[List[List[int]]] = None,\n        return_dict: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n\n        if inputs_embeds is None:\n            (input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels) = self.prepare_inputs_labels_for_multimodal(input_ids, position_ids, attention_mask, past_key_values, labels, images, image_sizes)\n\n        return super().forward(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            labels=labels,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            cache_position=cache_position,\n        )\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs: Optional[torch.Tensor] = None,\n        images: Optional[torch.Tensor] = None,\n        image_sizes: Optional[torch.Tensor] = None,\n        **kwargs,\n    ) -> Union[GenerateOutput, torch.LongTensor]:\n        position_ids = kwargs.pop(\"position_ids\", None)\n        attention_mask = kwargs.pop(\"attention_mask\", None)\n        if \"inputs_embeds\" in kwargs:\n            raise NotImplementedError(\"`inputs_embeds` is not supported\")\n\n        if images is not None:\n            (inputs, position_ids, attention_mask, _, inputs_embeds, _) = self.prepare_inputs_labels_for_multimodal(inputs, position_ids, attention_mask, None, None, images, image_sizes=image_sizes)\n        else:\n            inputs_embeds = self.get_model().embed_tokens(inputs)\n\n        return super().generate(position_ids=position_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, **kwargs)\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n        images = kwargs.pop(\"images\", None)\n        image_sizes = kwargs.pop(\"image_sizes\", None)\n        inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs)\n        if images is not None:\n            inputs[\"images\"] = images\n        if image_sizes is not None:\n            inputs[\"image_sizes\"] = image_sizes\n        return inputs\n\n\nAutoConfig.register(\"llava_gemma\", LlavaGemmaConfig)\nAutoModelForCausalLM.register(LlavaGemmaConfig, LlavaGemmaForCausalLM)\n"}
{"type": "source_file", "path": "llava/mm_utils.py", "content": "from PIL import Image\nfrom io import BytesIO\nimport base64\nimport math\nimport ast\nimport re\nimport torch\nfrom transformers import StoppingCriteria\nfrom llava.constants import IMAGE_TOKEN_INDEX\n\n\ndef resize_and_center_crop(image, shortest_edge_length):\n    # Calculate new dimensions and resize\n    aspect_ratio = float(image.width) / float(image.height)\n    if aspect_ratio > 1:\n        new_width = int(shortest_edge_length * aspect_ratio)\n        new_height = shortest_edge_length\n    else:\n        new_width = shortest_edge_length\n        new_height = int(shortest_edge_length / aspect_ratio)\n    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)\n\n    # Calculate the position and perform the center crop\n    left = (new_width - shortest_edge_length) / 2\n    top = (new_height - shortest_edge_length) / 2\n    right = (new_width + shortest_edge_length) / 2\n    bottom = (new_height + shortest_edge_length) / 2\n    cropped_image = resized_image.crop((left, top, right, bottom))\n\n    return cropped_image\n\n\ndef auto_pad_images(image, grid_params):\n    assert isinstance(image, Image.Image), \"Input should be a Pillow Image\"\n    assert len(grid_params) > 0, \"Grid parameters should not be empty\"\n\n    # Step 1: Calculate and find the closest aspect ratio\n    input_width, input_height = image.size\n    input_aspect_ratio = input_width / input_height\n    candidate_resolutions = [(w / h, w, h) for w in grid_params for h in grid_params]\n    closest_aspect_ratio = min(candidate_resolutions, key=lambda x: abs(input_aspect_ratio - x[0]))\n\n    candidate_resolutions = [(x[1], x[2]) for x in candidate_resolutions if abs(x[0] - closest_aspect_ratio[0]) < 1e-3]\n\n    target_resolution = min(candidate_resolutions, key=lambda res: abs(max(input_width, input_height) / max(res) - 1))\n\n    resize_width, resize_height = target_resolution\n    if input_width > input_height:\n        resize_height = int(resize_width / input_aspect_ratio)\n    else:\n        resize_width = int(resize_height * input_aspect_ratio)\n    resized_image = image.resize((resize_width, resize_height), Image.ANTIALIAS)\n\n    # Step 5: Pad the resized image if necessary to match the target resolution\n    pad_width = target_resolution[0] - resize_width\n    pad_height = target_resolution[1] - resize_height\n    padded_image = Image.new(\"RGB\", target_resolution, color=(0, 0, 0))\n    padded_image.paste(resized_image, (pad_width // 2, pad_height // 2))\n\n    return padded_image\n\n\ndef extract_patches(image, patch_size, overlap_ratio):\n    assert isinstance(image, Image.Image), \"Input should be a Pillow Image\"\n    assert patch_size > 0, \"Patch size should be greater than 0\"\n    assert 0 <= overlap_ratio < 1, \"Overlap ratio should be between 0 and 1\"\n\n    W, H = image.size\n    patches = []\n\n    stride = int(patch_size * (1 - overlap_ratio))\n\n    num_patches_y = (H - patch_size) // stride + 1\n    num_patches_x = (W - patch_size) // stride + 1\n\n    y_start = (H - (num_patches_y - 1) * stride - patch_size) // 2\n    x_start = (W - (num_patches_x - 1) * stride - patch_size) // 2\n\n    for y in range(y_start, y_start + num_patches_y * stride, stride):\n        for x in range(x_start, x_start + num_patches_x * stride, stride):\n            patch = image.crop((x, y, x + patch_size, y + patch_size))\n            patches.append(patch)\n\n    return patches\n\n\ndef process_highres_image_crop_split(image, data_args, processor=None):\n    crop_resolution = data_args.image_crop_resolution\n    split_resolution = data_args.image_split_resolution\n    if processor is None:\n        processor = data_args.image_processor\n    image_crop = resize_and_center_crop(image, crop_resolution)\n    image_patches = extract_patches(image_crop, patch_size=split_resolution, overlap_ratio=0)\n    image_patches = [processor.preprocess(image_patch, return_tensors=\"pt\")[\"pixel_values\"][0] for image_patch in image_patches]\n    return torch.stack(image_patches, dim=0)\n\n\ndef process_highres_image(image, processor, grid_pinpoints):\n    grid_params = [int(x) for x in grid_pinpoints.split(\",\")]\n    width_height = max(image.size)\n    fit_grid_params = [x for x in grid_params if x >= width_height]\n    if len(fit_grid_params) == 0:\n        select_size = max(grid_params)\n    else:\n        select_size = min(fit_grid_params)\n    # FIXME: always select the 448\n    select_size = max(grid_params)\n    image_padded = expand2square(image, tuple(int(x * 255) for x in processor.image_mean))\n\n    # FIXME: this seems to be a bug that it always resizes instead of padding\n    image_original_resize = image.resize((processor.size[\"shortest_edge\"], processor.size[\"shortest_edge\"]))\n    image_padded = image_padded.resize((select_size, select_size))\n    image_patches = extract_patches(image_padded, patch_size=processor.size[\"shortest_edge\"], overlap_ratio=0)\n    image_patches = [image_original_resize] + image_patches\n    image_patches = [processor.preprocess(image_patch, return_tensors=\"pt\")[\"pixel_values\"][0] for image_patch in image_patches]\n    return torch.stack(image_patches, dim=0)\n\n\ndef select_best_resolution(original_size, possible_resolutions):\n    \"\"\"\n    Selects the best resolution from a list of possible resolutions based on the original size.\n\n    Args:\n        original_size (tuple): The original size of the image in the format (width, height).\n        possible_resolutions (list): A list of possible resolutions in the format [(width1, height1), (width2, height2), ...].\n\n    Returns:\n        tuple: The best fit resolution in the format (width, height).\n    \"\"\"\n    original_width, original_height = original_size\n    best_fit = None\n    max_effective_resolution = 0\n    min_wasted_resolution = float(\"inf\")\n\n    for width, height in possible_resolutions:\n        # Calculate the downscaled size to keep the aspect ratio\n        scale = min(width / original_width, height / original_height)\n        downscaled_width, downscaled_height = int(original_width * scale), int(original_height * scale)\n\n        # Calculate effective and wasted resolutions\n        effective_resolution = min(downscaled_width * downscaled_height, original_width * original_height)\n        wasted_resolution = (width * height) - effective_resolution\n\n        if effective_resolution > max_effective_resolution or (effective_resolution == max_effective_resolution and wasted_resolution < min_wasted_resolution):\n            max_effective_resolution = effective_resolution\n            min_wasted_resolution = wasted_resolution\n            best_fit = (width, height)\n\n    return best_fit\n\n\ndef resize_and_pad_image(image, target_resolution):\n    \"\"\"\n    Resize and pad an image to a target resolution while maintaining aspect ratio.\n\n    Args:\n        image (PIL.Image.Image): The input image.\n        target_resolution (tuple): The target resolution (width, height) of the image.\n\n    Returns:\n        PIL.Image.Image: The resized and padded image.\n    \"\"\"\n    original_width, original_height = image.size\n    target_width, target_height = target_resolution\n\n    # Determine which dimension (width or height) to fill\n    scale_w = target_width / original_width\n    scale_h = target_height / original_height\n\n    if scale_w < scale_h:\n        # Width will be filled completely\n        new_width = target_width\n        new_height = min(math.ceil(original_height * scale_w), target_height)\n    else:\n        # Height will be filled completely\n        new_height = target_height\n        new_width = min(math.ceil(original_width * scale_h), target_width)\n\n    # Resize the image\n    resized_image = image.resize((new_width, new_height))\n\n    # Create a new image with the target size and paste the resized image onto it\n    new_image = Image.new(\"RGB\", (target_width, target_height), (0, 0, 0))\n    paste_x = (target_width - new_width) // 2\n    paste_y = (target_height - new_height) // 2\n    new_image.paste(resized_image, (paste_x, paste_y))\n\n    return new_image\n\n\ndef divide_to_patches(image, patch_size):\n    \"\"\"\n    Divides an image into patches of a specified size.\n\n    Args:\n        image (PIL.Image.Image): The input image.\n        patch_size (int): The size of each patch.\n\n    Returns:\n        list: A list of PIL.Image.Image objects representing the patches.\n    \"\"\"\n    patches = []\n    width, height = image.size\n    for i in range(0, height, patch_size):\n        for j in range(0, width, patch_size):\n            box = (j, i, j + patch_size, i + patch_size)\n            patch = image.crop(box)\n            patches.append(patch)\n\n    return patches\n\n\ndef  get_anyres_image_grid_shape(image_size, grid_pinpoints, patch_size):\n    \"\"\"\n    Calculate the shape of the image patch grid after the preprocessing for images of any resolution.\n\n    Args:\n        image_size (tuple): The size of the input image in the format (width, height).\n        grid_pinpoints (str): A string representation of a list of possible resolutions.\n        patch_size (int): The size of each image patch.\n\n    Returns:\n        tuple: The shape of the image patch grid in the format (width, height).\n    \"\"\"\n    if isinstance(grid_pinpoints, str) and \"x\" in grid_pinpoints:\n        assert patch_size in [224, 336, 384, 448, 512], \"patch_size should be in [224, 336, 384, 448, 512]\"\n        # Use regex to extract the range from the input string\n        matches = re.findall(r\"\\((\\d+)x(\\d+)\\)\", grid_pinpoints)\n        range_start = tuple(map(int, matches[0]))\n        range_end = tuple(map(int, matches[-1]))\n        # Generate a matrix of tuples from (range_start[0], range_start[1]) to (range_end[0], range_end[1])\n        grid_pinpoints = [(i, j) for i in range(range_start[0], range_end[0] + 1) for j in range(range_start[1], range_end[1] + 1)]\n        # Multiply all elements by patch_size\n        grid_pinpoints = [[dim * patch_size for dim in pair] for pair in grid_pinpoints]\n    if type(grid_pinpoints) is list:\n        possible_resolutions = grid_pinpoints\n    else:\n        possible_resolutions = ast.literal_eval(grid_pinpoints)\n    width, height = select_best_resolution(image_size, possible_resolutions)\n    return width // patch_size, height // patch_size\n\n\ndef process_anyres_image(image, processor, grid_pinpoints):\n    \"\"\"\n    Process an image with variable resolutions.\n\n    Args:\n        image (PIL.Image.Image): The input image to be processed.\n        processor: The image processor object.\n        grid_pinpoints (str): A string representation of a list of possible resolutions.\n\n    Returns:\n        torch.Tensor: A tensor containing the processed image patches.\n    \"\"\"\n    # Convert grid_pinpoints from string to list\n    if isinstance(grid_pinpoints, str) and \"x\" in grid_pinpoints:\n        try:\n            patch_size = processor.size[0]\n        except Exception as e:\n            patch_size = processor.size[\"shortest_edge\"]\n        assert patch_size in [224, 336, 384, 448, 512], \"patch_size should be in [224, 336, 384, 448, 512]\"\n        # Use regex to extract the range from the input string\n        matches = re.findall(r\"\\((\\d+)x(\\d+)\\)\", grid_pinpoints)\n        range_start = tuple(map(int, matches[0]))\n        range_end = tuple(map(int, matches[-1]))\n        # Generate a matrix of tuples from (range_start[0], range_start[1]) to (range_end[0], range_end[1])\n        grid_pinpoints = [(i, j) for i in range(range_start[0], range_end[0] + 1) for j in range(range_start[1], range_end[1] + 1)]\n        # Multiply all elements by patch_size\n        grid_pinpoints = [[dim * patch_size for dim in pair] for pair in grid_pinpoints]\n\n    if type(grid_pinpoints) is list:\n        possible_resolutions = grid_pinpoints\n    else:\n        possible_resolutions = ast.literal_eval(grid_pinpoints)\n    best_resolution = select_best_resolution(image.size, possible_resolutions)\n    image_padded = resize_and_pad_image(image, best_resolution)\n\n    patches = divide_to_patches(image_padded, processor.crop_size[\"height\"])\n\n    # FIXME: this seems to be a bug that it resizes instead of pad.\n    # but to keep it consistent with previous, i will keep it as it is\n    # TODO: uncomment below to ablate with the padding\n    if isinstance(processor.size, dict):\n        shortest_edge = processor.size[\"shortest_edge\"]\n    else:\n        shortest_edge = min(processor.size)\n    image_original_resize = image.resize((shortest_edge, shortest_edge))\n\n    # image_padded_square = expand2square(image, tuple(int(x*255) for x in processor.image_mean))\n    # image_original_resize = image_padded_square.resize((processor.size['shortest_edge'], processor.size['shortest_edge']))\n\n    image_patches = [image_original_resize] + patches\n\n    image_patches = [processor.preprocess(image_patch, return_tensors=\"pt\")[\"pixel_values\"][0] for image_patch in image_patches]\n    return torch.stack(image_patches, dim=0)\n\n\ndef load_image_from_base64(image):\n    return Image.open(BytesIO(base64.b64decode(image)))\n\n\ndef expand2square(pil_img, background_color):\n    width, height = pil_img.size\n    if width == height:\n        return pil_img\n    elif width > height:\n        result = Image.new(pil_img.mode, (width, width), background_color)\n        result.paste(pil_img, (0, (width - height) // 2))\n        return result\n    else:\n        result = Image.new(pil_img.mode, (height, height), background_color)\n        result.paste(pil_img, ((height - width) // 2, 0))\n        return result\n\n\ndef process_images(images, image_processor, model_cfg):\n    image_aspect_ratio = getattr(model_cfg, \"image_aspect_ratio\", None)\n    new_images = []\n    if image_aspect_ratio == \"highres\":\n        for image in images:\n            image = process_highres_image(image, image_processor, model_cfg.image_grid_pinpoints)\n            new_images.append(image)\n    elif image_aspect_ratio == \"anyres\" or \"anyres_max\" in image_aspect_ratio:\n        for image in images:\n            image = process_anyres_image(image, image_processor, model_cfg.image_grid_pinpoints)\n            new_images.append(image)\n    elif image_aspect_ratio == \"crop_split\":\n        for image in images:\n            image = process_highres_image_crop_split(image, model_cfg, image_processor)\n            new_images.append(image)\n    elif image_aspect_ratio == \"pad\":\n        for image in images:\n            image = expand2square(image, tuple(int(x * 255) for x in image_processor.image_mean))\n            image = image_processor.preprocess(image, return_tensors=\"pt\")[\"pixel_values\"][0]\n            new_images.append(image)\n    else:\n        return image_processor.preprocess(images, return_tensors=\"pt\")[\"pixel_values\"]\n    if all(x.shape == new_images[0].shape for x in new_images):\n        new_images = torch.stack(new_images, dim=0)\n    return new_images\n\n\ndef tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None):\n    prompt_chunks = [tokenizer(chunk).input_ids for chunk in prompt.split(\"<image>\")]\n\n    def insert_separator(X, sep):\n        return [ele for sublist in zip(X, [sep] * len(X)) for ele in sublist][:-1]\n\n    input_ids = []\n    offset = 0\n    if len(prompt_chunks) > 0 and len(prompt_chunks[0]) > 0 and prompt_chunks[0][0] == tokenizer.bos_token_id:\n        offset = 1\n        input_ids.append(prompt_chunks[0][0])\n\n    for x in insert_separator(prompt_chunks, [image_token_index] * (offset + 1)):\n        input_ids.extend(x[offset:])\n\n    if return_tensors is not None:\n        if return_tensors == \"pt\":\n            return torch.tensor(input_ids, dtype=torch.long)\n        raise ValueError(f\"Unsupported tensor type: {return_tensors}\")\n    return input_ids\n\n\ndef get_model_name_from_path(model_path):\n    model_path = model_path.strip(\"/\")\n    model_paths = model_path.split(\"/\")\n    if model_paths[-1].startswith(\"checkpoint-\"):\n        return model_paths[-2] + \"_\" + model_paths[-1]\n    else:\n        return model_paths[-1]\n\n\nclass KeywordsStoppingCriteria(StoppingCriteria):\n    def __init__(self, keywords, tokenizer, input_ids):\n        self.keywords = keywords\n        self.keyword_ids = []\n        for keyword in keywords:\n            cur_keyword_ids = tokenizer(keyword).input_ids\n            if len(cur_keyword_ids) > 1 and cur_keyword_ids[0] == tokenizer.bos_token_id:\n                cur_keyword_ids = cur_keyword_ids[1:]\n            self.keyword_ids.append(torch.tensor(cur_keyword_ids))\n        self.tokenizer = tokenizer\n        self.start_len = input_ids.shape[1]\n\n    def __call__(self, output_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n        assert output_ids.shape[0] == 1, \"Only support batch size 1 (yet)\"  # TODO\n        offset = min(output_ids.shape[1] - self.start_len, 3)\n        self.keyword_ids = [keyword_id.to(output_ids.device) for keyword_id in self.keyword_ids]\n        for keyword_id in self.keyword_ids:\n            if output_ids[0, -keyword_id.shape[0] :] == keyword_id:\n                return True\n        outputs = self.tokenizer.batch_decode(output_ids[:, -offset:], skip_special_tokens=True)[0]\n        for keyword in self.keywords:\n            if keyword in outputs:\n                return True\n        return False\n"}
{"type": "source_file", "path": "llava/model/language_model/llava_mistral.py", "content": "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\n\nfrom transformers import AutoConfig, AutoModelForCausalLM, MistralConfig, MistralModel, MistralForCausalLM, GenerationConfig\n\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nfrom transformers.generation.utils import GenerateOutput\n\nfrom ..llava_arch import LlavaMetaModel, LlavaMetaForCausalLM\n\n\nclass LlavaMistralConfig(MistralConfig):\n    model_type = \"llava_mistral\"\n    temperature: float = 0.0  # reset to 0.0, previously 0.9 for Vicuna\n    max_new_tokens: int = 1024\n    do_sample: bool = False\n    top_p: Optional[float] = None\n\n\nclass LlavaMistralModel(LlavaMetaModel, MistralModel):\n    config_class = LlavaMistralConfig\n\n    def __init__(self, config: MistralConfig):\n        super(LlavaMistralModel, self).__init__(config)\n\n\nclass LlavaMistralForCausalLM(MistralForCausalLM, LlavaMetaForCausalLM):\n    config_class = LlavaMistralConfig\n\n    def __init__(self, config):\n        super(MistralForCausalLM, self).__init__(config)\n\n        config.model_type = \"llava_mistral\"\n        config.rope_scaling = None\n\n        self.model = LlavaMistralModel(config)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.model\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        images: Optional[torch.FloatTensor] = None,\n        image_sizes: Optional[List[List[int]]] = None,\n        return_dict: Optional[bool] = None,\n        cache_position=None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n\n        if inputs_embeds is None:\n            (input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels) = self.prepare_inputs_labels_for_multimodal(input_ids, position_ids, attention_mask, past_key_values, labels, images, image_sizes)\n\n        return super().forward(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            labels=labels,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs: Optional[torch.Tensor] = None,\n        images: Optional[torch.Tensor] = None,\n        image_sizes: Optional[torch.Tensor] = None,\n        **kwargs,\n    ) -> Union[GenerateOutput, torch.LongTensor]:\n        position_ids = kwargs.pop(\"position_ids\", None)\n        attention_mask = kwargs.pop(\"attention_mask\", None)\n        if \"inputs_embeds\" in kwargs:\n            raise NotImplementedError(\"`inputs_embeds` is not supported\")\n\n        if images is not None:\n            (inputs, position_ids, attention_mask, _, inputs_embeds, _) = self.prepare_inputs_labels_for_multimodal(inputs, position_ids, attention_mask, None, None, images, image_sizes=image_sizes)\n        else:\n            inputs_embeds = self.get_model().embed_tokens(inputs)\n\n        return super().generate(position_ids=position_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, **kwargs)\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n        images = kwargs.pop(\"images\", None)\n        image_sizes = kwargs.pop(\"image_sizes\", None)\n        inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs)\n        if images is not None:\n            inputs[\"images\"] = images\n        if image_sizes is not None:\n            inputs[\"image_sizes\"] = image_sizes\n        return inputs\n\n\nAutoConfig.register(\"llava_mistral\", LlavaMistralConfig)\nAutoModelForCausalLM.register(LlavaMistralConfig, LlavaMistralForCausalLM)\n"}
{"type": "source_file", "path": "data_generator/generate_data/utils/gpt4_tools.py", "content": "import time\nfrom functools import wraps\nimport threading\n\nfrom openai import OpenAI\n\n\ndef retry(exception_to_check, tries=3, delay=5, backoff=1):\n    \"\"\"\n    Decorator used to automatically retry a failed function. Parameters:\n\n    exception_to_check: The type of exception to catch.\n    tries: Maximum number of retry attempts.\n    delay: Waiting time between each retry.\n    backoff: Multiplicative factor to increase the waiting time after each retry.\n    \"\"\"\n\n    def deco_retry(f):\n        @wraps(f)\n        def f_retry(*args, **kwargs):\n            mtries, mdelay = tries, delay\n            while mtries > 1:\n                try:\n                    return f(*args, **kwargs)\n                except exception_to_check as e:\n                    print(f\"{str(e)}, Retrying in {mdelay} seconds...\")\n                    time.sleep(mdelay)\n                    mtries -= 1\n                    mdelay *= backoff\n            return f(*args, **kwargs)\n\n        return f_retry  # true decorator\n\n    return deco_retry\n\n\ndef timeout_decorator(timeout):\n    class TimeoutException(Exception):\n        pass\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            result = [\n                TimeoutException(\"Function call timed out\")\n            ]  # Nonlocal mutable variable\n\n            def target():\n                try:\n                    result[0] = func(*args, **kwargs)\n                except Exception as e:\n                    result[0] = e\n\n            thread = threading.Thread(target=target)\n            thread.start()\n            thread.join(timeout)\n            if thread.is_alive():\n                print(f\"Function {func.__name__} timed out, retrying...\")\n                return wrapper(*args, **kwargs)\n            if isinstance(result[0], Exception):\n                raise result[0]\n            return result[0]\n\n        return wrapper\n\n    return decorator\n\n\n@timeout_decorator(180)\n@retry(Exception, tries=3, delay=5, backoff=1)\ndef send_chat_request_azure(\n        message_text,\n        engine=\"gpt-4o-2024-08-06\",  #o1-preview-2024-09-12\n        temp=0.2,\n        logit_bias: dict = {},\n        max_new_token=4096,\n        sample_n=1,\n):\n    data_res_list = []\n    # Config your api_key, base_url and model\n    client = OpenAI(api_key=\"add your api key\")\n\n    response = client.chat.completions.create(\n        model=engine,\n        messages=message_text,\n        temperature=temp,\n        max_tokens=max_new_token,\n        top_p=0.95,\n        frequency_penalty=0,\n        presence_penalty=0,\n        stop=None,\n        n=sample_n,\n    )\n\n    for index in range(sample_n):\n        data_res = response.choices[index].message.content\n        data_res_list.append(data_res)\n\n    return data_res_list[0], data_res_list\n"}
{"type": "source_file", "path": "llava/constants.py", "content": "CONTROLLER_HEART_BEAT_EXPIRATION = 30\nWORKER_HEART_BEAT_INTERVAL = 15\n\nLOGDIR = \".\"\n\n# Model Constants\nIGNORE_INDEX = -100\nIMAGE_TOKEN_INDEX = -200\nDEFAULT_IMAGE_TOKEN = \"<image>\"\nDEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\nDEFAULT_IM_START_TOKEN = \"<im_start>\"\nDEFAULT_IM_END_TOKEN = \"<im_end>\"\n"}
{"type": "source_file", "path": "data_generator/generate_data/chart/templates/prompts.py", "content": "import json\nimport random\n\nfrom .plot_params import *\n\n\nchart_examples = {\n\"bar\": [\n    {\n    \"import matplotlib.pyplot as plt\\nimport numpy as np\\n\\n# Data\\ncategories = ['CNN', 'RNN', 'Transformer', 'GAN']\\nabr_labels = ['Algorithm', 'Benchmark', 'Result']\\nvalues = {\\n    'CNN': [85, 78, 92],\\n    'RNN': [80, 75, 89],\\n    'Transformer': [90, 85, 95],\\n    'GAN': [82, 80, 88]\\n}\\n\\n# Set y positions for the bars\\ny_pos = np.arange(len(categories))\\n\\n# Set the width for each bar\\nbar_width = 0.2\\n\\n# Plot bars for each category\\nfor i, abr in enumerate(abr_labels):\\n    # Calculate the position for each ABR bar\\n    plt.barh(y_pos + i * bar_width, \\n             [values[cat][i] for cat in categories], \\n             height=bar_width, \\n             label=abr)\\n\\n# Add labels to y-axis\\nplt.yticks(y_pos + bar_width, categories)\\n\\n# Add title and labels\\nplt.xlabel('Scores')\\nplt.title('Performance of AI Models')\\n\\n# Show legend\\nplt.legend()\\n\\n# Display the plot\\nplt.show()\"\n    }, \n    {\n    \"import matplotlib.pyplot as plt\\n\\nfig = plt.figure(figsize=(10, 6))\\nax = fig.add_subplot()\\n\\ngrade = [\\\"5th\\\", \\\"6th\\\", \\\"7th\\\", \\\"8th\\\"]\\nmath = [75, 80, 85, 90]\\nscience = [80, 85, 90, 95]\\nenglish = [90, 95, 98, 100]\\n\\nax.bar(grade, math, color=\\\"blue\\\", label=\\\"Math\\\")\\nax.bar(grade, science, bottom=math, color=\\\"orange\\\", label=\\\"Science\\\")\\nax.bar(grade, english, bottom=[math[i]+science[i] for i in range(len(math))], color=\\\"green\\\", label=\\\"English\\\")\\n\\nplt.title('Academic scores in three subjects for grades 5th to 8th in 2021')\\nplt.xlabel('Grade')\\nplt.ylabel('Score (%)')\\nplt.xticks(rotation=0)\\nplt.legend(loc=\\\"upper left\\\")\\nplt.tight_layout()\\nplt.show()\\nplt.clf()\"\n    },\n    {\n    \"\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nCountry = ['USA','UK','Germany','France']\\nManufacturing_Output = [800,700,600,500]\\nAgricultural_Output = [200,400,350,450]\\nService_Output = [500,600,400,350]\\n\\nfig = plt.figure(figsize=(12,8))\\nax = fig.add_subplot(111)\\n\\nax.bar(Country, Manufacturing_Output, width=0.5, label=\\\"Manufacturing Output\\\")\\nax.bar(Country, Agricultural_Output, bottom=Manufacturing_Output, width=0.5, label=\\\"Agricultural Output\\\")\\nax.bar(Country, Service_Output, bottom=[x + y for x, y in zip(Manufacturing_Output,Agricultural_Output)], width=0.5, label=\\\"Service Output\\\")\\n\\nax.set_title(\\\"Manufacturing, Agricultural, and Service Output in four countries in 2021\\\")\\nax.set_ylabel(\\\"Output (million)\\\")\\nax.set_xlabel(\\\"Country\\\")\\nax.legend(loc=\\\"best\\\")\\nax.grid(True)\\n\\nfor i, v in enumerate(Manufacturing_Output):\\n    ax.text(i - 0.2, v + 10, str(v))\\n\\nfor i, v in enumerate(Agricultural_Output):\\n    ax.text(i - 0.2, v + Manufacturing_Output[i] + 10, str(v))\\n\\nfor i, v in enumerate(Service_Output):\\n    ax.text(i - 0.2, v + Manufacturing_Output[i] + Agricultural_Output[i] + 10, str(v))\\n\\nplt.tight_layout()\\nplt.xticks(Country)\\nplt.savefig('Bar Chart/png/514.png')\\nplt.clf()\" \n    },\n    {\n    \"import matplotlib.pyplot as plt\\nimport numpy as np\\n\\n# Data for the first subplot\\ncategories1 = ['CNN', 'RNN', 'Transformer', 'GAN']\\nvalues1 = [85, 80, 90, 82]\\ncolors1 = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']\\n\\n# Data for the second subplot\\ncategories2 = ['DNN', 'LSTM', 'BERT', 'VAE']\\nvalues2 = [88, 83, 93, 87]\\ncolors2 = ['#ffb3e6', '#c2c2f0', '#ffb3b3', '#c2f0c2']\\n\\n# Create a figure and two subplots (1 row, 2 columns)\\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\\n\\n# First subplot\\nbars1 = ax1.barh(np.arange(len(categories1)), values1, color=colors1, edgecolor='black', height=0.6)\\nax1.set_yticks(np.arange(len(categories1)))\\nax1.set_yticklabels(categories1)\\nax1.set_xlabel('Scores', fontsize=12)\\nax1.set_title('Model Performance (Group 1)', fontsize=14)\\nax1.grid(True, which='both', axis='x', linestyle='--', alpha=0.7)\\n\\n# Add values at the end of each bar in the first subplot\\nfor bar in bars1:\\n    ax1.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2, \\n             f'{bar.get_width():.1f}', va='center', fontsize=12, color='black')\\n\\n# Second subplot\\nbars2 = ax2.barh(np.arange(len(categories2)), values2, color=colors2, edgecolor='black', height=0.6)\\nax2.set_yticks(np.arange(len(categories2)))\\nax2.set_yticklabels(categories2)\\nax2.set_xlabel('Scores', fontsize=12)\\nax2.set_title('Model Performance (Group 2)', fontsize=14)\\nax2.grid(True, which='both', axis='x', linestyle='--', alpha=0.7)\\n\\n# Add values at the end of each bar in the second subplot\\nfor bar in bars2:\\n    ax2.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2, \\n             f'{bar.get_width():.1f}', va='center', fontsize=12, color='black')\\n\\n# Add a common title for the entire figure\\nfig.suptitle('AI Model Performance Comparison', fontsize=16, weight='bold')\\n\\n# Adjust layout for better spacing\\nplt.tight_layout(rect=[0, 0, 1, 0.95])\\n\\n# Display the plot\\nplt.show()\"\n    }\n],\n\n\"line\": [\n    {\n    \"\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n \\nx = np.array([2001, 2002, 2003, 2004])\\ny1 = np.array([200, 220, 190, 250])\\ny2 = np.array([180, 210, 230, 220])\\ny3 = np.array([100, 120, 110, 130])\\n \\nplt.figure(figsize=(10, 6))\\nplt.plot(x, y1, label=\\\"Music Albums Sold(million)\\\", color='red', marker='o', linestyle='--', linewidth=2)\\nplt.plot(x, y2, label=\\\"Movie Tickets Sold(million)\\\", color='blue', marker='^', linestyle='-', linewidth=2)\\nplt.plot(x, y3, label=\\\"Art Gallery Visits(million)\\\", color='green', marker='s', linestyle=':', linewidth=2)\\n \\nplt.title(\\\"Arts and culture industry visits and sales in the early 2000s\\\")\\nplt.xlabel('Year')\\nplt.ylabel('Number')\\nplt.xticks(x)\\nplt.legend()\\n \\nfor a, b, c in zip(x, y1, y2):\\n    plt.text(a, b + 0.05, '%.0f' % b, ha='center', va='bottom', fontsize=10)\\n    plt.text(a, c + 0.05, '%.0f' % c, ha='center', va='bottom', fontsize=10)\\nplt.text(2004, y3[-1] + 0.05, '%.0f' % y3[-1], ha='center', va='bottom', fontsize=10)\\n \\nplt.savefig('line chart/png/134.png')\\nplt.tight_layout()\\nplt.clf()\"\n    },\n    {\n    \"\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\n# data\\nx = np.arange(1, 6) # Month\\nPrice_A = [20, 25, 22, 30, 27]\\nPrice_B = [30, 35, 32, 40, 37]\\nPrice_C = [18, 20, 15, 20, 17]\\nPrice_D = [25, 30, 27, 35, 32]\\n\\n# create figure\\nplt.figure(figsize=(10,6))\\n\\n# create a subplot\\nax=plt.subplot()\\n\\n# plot the data\\nax.plot(x, Price_A, color='red', marker='o', label='Price A')\\nax.plot(x, Price_B, color='blue', marker='o', label='Price B')\\nax.plot(x, Price_C, color='green', marker='o', label='Price C')\\nax.plot(x, Price_D, color='black', marker='o', label='Price D')\\n\\n# set x ticks\\nax.set_xticks(x)\\n\\n# setting the limit of x axis\\nax.set_xlim(1, 5)\\n\\n# setting the grid\\nax.grid(linestyle='--', alpha=0.5)\\n\\n# set the title\\nax.set_title('Average prices of four food items in the US in 2021')\\n\\n# set the xlabel and ylabel\\nax.set_xlabel('Month')\\nax.set_ylabel('Price (dollars)')\\n\\n# set the legend\\nax.legend(loc='upper left', bbox_to_anchor=(1, 1))\\n\\n# tight the layout\\nplt.tight_layout()\\n\\n# save the figure\\nplt.savefig('line chart/png/546.png')\\n\\n# clear the current figure\\nplt.clf()\"\n    },\n    {   \n    \"\\\"\\n# Part 1: Importing Libraries\\n\\\"\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport matplotlib.colors as mcolors\\n\\n\\\"\\n# Part 2: Data Preparation\\n\\\"\\n# Sample data for plotting\\ntasks = np.arange(1, 11)\\nmodel_ResNet50 = np.random.uniform(60, 95, size=len(tasks))\\nmodel_VGG16 = np.random.uniform(55, 90, size=len(tasks))\\nmodel_InceptionV3 = np.random.uniform(50, 85, size=len(tasks))\\nmodel_EfficientNet = np.random.uniform(65, 100, size=len(tasks))\\n\\n# Labels for lines\\nline_label_ResNet50 = \\\"ResNet50\\\\n(Residual Network)\\\"\\nline_label_VGG16 = \\\"VGG16\\\\n(Deep CNN)\\\"\\nline_label_InceptionV3 = \\\"InceptionV3\\\\n(Inception Network)\\\"\\nline_label_EfficientNet = \\\"EfficientNet\\\\n(Scaled Network)\\\"\\n\\n# Plot configuration\\nxlim_values = [1, 10]\\nylim_values = [0, 110]\\nxlabel_value = \\\"Tasks\\\"\\nylabel_value = \\\"Performance (%)\\\"\\nxticks_values = np.arange(1, 11, 1)\\nyticks_values = np.arange(0, 111, 10)\\n\\n\\\"\\n# Part 3: Plot Configuration and Rendering\\n\\\"\\n# Create a figure and axes for two subplots\\nfig, axs = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\\n\\n# Plot on the first subplot\\naxs[0].plot(\\n    tasks, model_ResNet50, marker=\\\"o\\\", color=\\\"#1f77b4\\\", label=line_label_ResNet50, linestyle=\\\"-\\\", linewidth=2\\n)\\naxs[0].plot(\\n    tasks, model_VGG16, marker=\\\"s\\\", color=\\\"#ff7f0e\\\", label=line_label_VGG16, linestyle=\\\"--\\\", linewidth=2\\n)\\naxs[0].plot(\\n    tasks, model_InceptionV3, marker=\\\"^\\\", color=\\\"#2ca02c\\\", label=line_label_InceptionV3, linestyle=\\\"-.\\\", linewidth=2\\n)\\naxs[0].plot(\\n    tasks, model_EfficientNet, marker=\\\"D\\\", color=\\\"#d62728\\\", label=line_label_EfficientNet, linestyle=\\\":\\\" , linewidth=2\\n)\\naxs[0].set_title('Performance of Different Models (Subplot 1)')\\naxs[0].set_xlim(xlim_values)\\naxs[0].set_ylim(ylim_values)\\naxs[0].set_xlabel(xlabel_value, fontsize=12)\\naxs[0].set_ylabel(ylabel_value, fontsize=12)\\naxs[0].set_xticks(xticks_values)\\naxs[0].set_yticks(yticks_values)\\naxs[0].legend()\\naxs[0].grid(True, linestyle=\\\"--\\\", linewidth=0.5)\\n\\n# Plot on the second subplot\\naxs[1].plot(\\n    tasks, model_ResNet50, marker=\\\"o\\\", color=\\\"#1f77b4\\\", label=line_label_ResNet50, linestyle=\\\"-\\\", linewidth=2\\n)\\naxs[1].plot(\\n    tasks, model_VGG16, marker=\\\"s\\\", color=\\\"#ff7f0e\\\", label=line_label_VGG16, linestyle=\\\"--\\\", linewidth=2\\n)\\naxs[1].plot(\\n    tasks, model_InceptionV3, marker=\\\"^\\\", color=\\\"#2ca02c\\\", label=line_label_InceptionV3, linestyle=\\\"-.\\\", linewidth=2\\n)\\naxs[1].plot(\\n    tasks, model_EfficientNet, marker=\\\"D\\\", color=\\\"#d62728\\\", label=line_label_EfficientNet, linestyle=\\\":\\\" , linewidth=2\\n)\\naxs[1].set_title('Performance of Different Models (Subplot 2)')\\naxs[1].set_xlim(xlim_values)\\naxs[1].set_ylim(ylim_values)\\naxs[1].set_xlabel(xlabel_value, fontsize=12)\\naxs[1].set_xticks(xticks_values)\\naxs[1].legend()\\naxs[1].grid(True, linestyle=\\\"--\\\", linewidth=0.5)\\n\\n# Set the background color of the plots\\nfor ax in axs:\\n    ax.set_facecolor(\\n        mcolors.LinearSegmentedColormap.from_list(\\\"custom\\\", [\\\"#f0f0f0\\\", \\\"#ffffff\\\"])(0.8)\\n    )\\n\\n# Adjust layout for better spacing\\nplt.tight_layout()\\n\\n# Save the figure\\nplt.savefig('xxx.pdf', bbox_inches='tight')\\n\\n# Show the plot\\nplt.show()\"\n    },\n    {\n    \"import matplotlib.pyplot as plt\\nimport numpy as np\\n\\n#  Part 1: Data Preparation \\n# Time data (representing months)\\nmonths = np.arange(1, 13)\\n\\n# Stock prices of three companies with some random fluctuations and errors\\ncompany_A = np.array([100, 105, 102, 107, 110, 115, 120, 125, 123, 130, 128, 135])\\ncompany_B = np.array([95, 98, 100, 102, 105, 107, 110, 112, 115, 118, 120, 122])\\ncompany_C = np.array([90, 92, 94, 97, 95, 100, 105, 108, 110, 112, 115, 117])\\n\\n# Error values (for error bars)\\nerror_A = np.random.uniform(2, 4, len(months))\\nerror_B = np.random.uniform(1, 3, len(months))\\nerror_C = np.random.uniform(1, 2, len(months))\\n\\n# Interest rates of three countries\\ncountry_X = np.array([1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.3, 2.4, 2.5, 2.6, 2.7])\\ncountry_Y = np.array([0.5, 0.6, 0.65, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5])\\ncountry_Z = np.array([3.0, 3.2, 3.1, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 4.0, 4.1, 4.2])\\n\\n# Error values for interest rates\\nerror_X = np.random.uniform(0.05, 0.1, len(months))\\nerror_Y = np.random.uniform(0.02, 0.07, len(months))\\nerror_Z = np.random.uniform(0.08, 0.12, len(months))\\n\\n#  Part 2: Plotting the Data \\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\\n\\n# Plotting the stock prices with error bars (Top plot)\\nax1.errorbar(months, company_A, yerr=error_A, label='Company A', fmt='-o', capsize=5)\\nax1.errorbar(months, company_B, yerr=error_B, label='Company B', fmt='-s', capsize=5)\\nax1.errorbar(months, company_C, yerr=error_C, label='Company C', fmt='-^', capsize=5)\\nax1.set_title('Stock Prices Over Time')\\nax1.set_xlabel('Months')\\nax1.set_ylabel('Stock Price ($)')\\nax1.legend()\\nax1.grid(True)\\n\\n# Plotting the interest rates with error bars (Bottom plot)\\nax2.errorbar(months, country_X, yerr=error_X, label='Country X', fmt='-o', capsize=5)\\nax2.errorbar(months, country_Y, yerr=error_Y, label='Country Y', fmt='-s', capsize=5)\\nax2.errorbar(months, country_Z, yerr=error_Z, label='Country Z', fmt='-^', capsize=5)\\nax2.set_title('Interest Rates Over Time')\\nax2.set_xlabel('Months')\\nax2.set_ylabel('Interest Rate (%)')\\nax2.legend()\\nax2.grid(True)\\n\\n#  Part 3: Final Adjustments \\nplt.tight_layout()\\nplt.savefig('xxxx.png')\\nplt.show()\"\n    },\n    {\n    \"import numpy as np\\nimport matplotlib.pyplot as plt\\n\\n# Example data for computer science domain (e.g., Comparison of Two Algorithms Efficiency Over Time)\\n# Simulating computational tasks where the efficiency of two algorithms improves over time with fluctuations\\nx = np.linspace(0, 10, 100)  # Time (e.g., in seconds)\\n\\n# Algorithm A - more fluctuations in efficiency\\nmean_efficiency_A = np.log1p(x) + 0.2 * np.sin(2 * np.pi * x / 5)  # Simulated mean efficiency with fluctuations\\nstd_dev_efficiency_A = 0.2 + 0.05 * np.abs(np.sin(x))  # Simulated standard deviation for Algorithm A\\n\\n# Algorithm B - more fluctuations in efficiency\\nmean_efficiency_B = 0.5 * x + 0.3 * np.sin(2 * np.pi * x / 3)  # Simulated mean efficiency with fluctuations\\nstd_dev_efficiency_B = 0.3 + 0.03 * np.abs(np.cos(x))  # Simulated standard deviation for Algorithm B\\n\\n# Create the plot\\nplt.figure(figsize=(10, 6))\\n\\n# Plot the mean line of efficiency for Algorithm A\\nplt.plot(x, mean_efficiency_A, label='Algorithm A - Mean Efficiency', color='green')\\nplt.fill_between(x, mean_efficiency_A - std_dev_efficiency_A, mean_efficiency_A + std_dev_efficiency_A, \\n                 color='green', alpha=0.3, label='Algorithm A - Standard Deviation')\\n\\n# Plot the mean line of efficiency for Algorithm B\\nplt.plot(x, mean_efficiency_B, label='Algorithm B - Mean Efficiency', color='blue')\\nplt.fill_between(x, mean_efficiency_B - std_dev_efficiency_B, mean_efficiency_B + std_dev_efficiency_B, \\n                 color='blue', alpha=0.3, label='Algorithm B - Standard Deviation')\\n\\n# Add labels and title\\nplt.title('Algorithm Efficiency Comparison Over Time with Variance and Fluctuations', fontsize=16)\\nplt.xlabel('Time (seconds)', fontsize=14)\\nplt.ylabel('Efficiency', fontsize=14)\\n\\n# Show the legend\\nplt.legend()\\n\\n# Show the plot\\nplt.tight_layout()\\nplt.show()\\n\"\n    }\n\n],\n\n\"pie\": [\n    {\n    \"\\nimport matplotlib.pyplot as plt\\nplt.figure(figsize=(8,8))\\nlabels = ['Automotive', 'Electronics', 'Industrial Machinery', 'Food & Beverage', 'Chemicals', 'Textiles', 'Plastics', 'Metals', 'Other']\\nsizes = [30,20,15,10,10,10,5,5,5]\\nexplode = (0.1, 0, 0, 0, 0, 0, 0, 0, 0)\\nplt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=True, startangle=90)\\nplt.title(\\\"Distribution of Manufacturing and Production Output in 2023\\\")\\nplt.xticks(rotation=90, wrap=True)\\nplt.tight_layout()\\nplt.savefig('distribution_pie.png')\\nplt.clf()\"\n    },\n    {\n    \"import matplotlib.pyplot as plt\\n\\n# Creating figure with two subplots\\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 8))\\n\\n# Data for Fossil Fuels (left pie chart)\\nlabels_fossil = ['Coal', 'Oil', 'Natural Gas']\\nsizes_fossil = [40, 35, 25]  # Example data in percentage\\nwedges_fossil, texts_fossil, autotexts_fossil = ax1.pie(sizes_fossil, labels=labels_fossil, autopct='%1.1f%%', textprops={'fontsize': 12}, startangle=90)\\nax1.axis('equal')  # Equal aspect ratio ensures the pie is drawn as a circle\\nax1.set_title('Global Fossil Fuel Consumption, 2023', fontsize=14)\\n\\n# Adding legend to the first pie chart\\nax1.legend(wedges_fossil, labels_fossil, loc='upper left', bbox_to_anchor=(-0.1, 1.05), fontsize=12)\\n\\n# Data for Renewable Energy (right pie chart)\\nlabels_renewable = ['Solar Energy', 'Wind Energy', 'Hydropower', 'Geothermal Energy', 'Biomass']\\nsizes_renewable = [25, 25, 20, 15, 15]  # Example data in percentage\\nwedges_renewable, texts_renewable, autotexts_renewable = ax2.pie(sizes_renewable, labels=labels_renewable, autopct='%1.1f%%', textprops={'fontsize': 12}, startangle=90)\\nax2.axis('equal')\\nax2.set_title('Global Renewable Energy Consumption, 2023', fontsize=14)\\n\\n# Adding legend to the second pie chart\\nax2.legend(wedges_renewable, labels_renewable, loc='upper left', bbox_to_anchor=(1.0, 1.05), fontsize=12)\\n\\n# Adding tight layout and saving figure\\nplt.tight_layout()\\nplt.savefig('two_pie_charts_with_legend.png')\\nplt.show()\"\n    },\n    {\n    \"\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.pyplot import pie, axis, title, legend\\n\\n#Creating figure\\nfig = plt.figure(figsize=(10,8))\\n\\n#Data\\nlabels = ['Solar Energy','Wind Energy','Hydropower','Geothermal Energy','Biomass']\\nsizes = [25,25,20,15,15]\\n\\n#Plotting Pie Chart\\nax = fig.add_subplot()\\nax.pie(sizes, labels=labels, autopct='%1.1f%%', textprops={'fontsize':14},startangle=90)\\nax.axis('equal')\\ntitle('Distribution of Renewable Energy Sources in the USA, 2023')\\nlegend(labels, loc='upper left', bbox_to_anchor=(-0.1, 1.), fontsize=14)\\n\\n#Saving Figure\\nplt.tight_layout()\\nplt.savefig('pie chart/png/228.png')\\nplt.clf()\"\n    }\n],\n\n\"bar_num\":[\n    {\n    \"\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\nyears = [2018,2019,2020,2021]\\nPsychology = [180,200,190,200]\\nSociology = [150,180,190,240]\\nAnthropology = [100,120,140,160]\\n\\nfig = plt.figure(figsize=(12, 9))\\nax = fig.add_subplot(111)\\n\\nax.set_title('Number of publications in Social Sciences and Humanities from 2018 to 2021')\\n\\nbar_width = 0.2\\n\\nbars1 = ax.bar(np.array(years)-bar_width,Psychology,width=bar_width,align='center',label='Psychology',color='#1f77b4')\\nbars2 = ax.bar(np.array(years),Sociology,width=bar_width,align='center',label='Sociology',color='#ff7f0e')\\nbars3 = ax.bar(np.array(years)+bar_width,Anthropology,width=bar_width,align='center',label='Anthropology',color='#2ca02c')\\n\\nax.set_xticks(years)\\n\\nax.set_ylabel('Number of Publications')\\nax.set_xlabel('Years')\\n\\nax.legend()\\n\\nfor bar1, bar2,bar3 in zip(bars1, bars2,bars3):\\n    ax.annotate(\\\"%.0f\\\" % bar1.get_height(),\\n                xy=(bar1.get_x() + bar1.get_width() / 2, bar1.get_height()),\\n                xytext=(0, 3),  # 3 points vertical offset\\n                textcoords=\\\"offset points\\\",\\n                ha='center', va='bottom')\\n    ax.annotate(\\\"%.0f\\\" % bar2.get_height(),\\n                xy=(bar2.get_x() + bar2.get_width() / 2, bar2.get_height()),\\n                xytext=(0, 3),  # 3 points vertical offset\\n                textcoords=\\\"offset points\\\",\\n                ha='center', va='bottom')\\n    ax.annotate(\\\"%.0f\\\" % bar3.get_height(),\\n                xy=(bar3.get_x() + bar3.get_width() / 2, bar3.get_height()),\\n                xytext=(0, 3),  # 3 points vertical offset\\n                textcoords=\\\"offset points\\\",\\n                ha='center', va='bottom')\\n\\nplt.tight_layout()\\nplt.savefig(r'xxx.png')\\nplt.clf()\"\n    },\n    {\n    \"\\nimport matplotlib.pyplot as plt\\n\\nCountry = [\\\"USA\\\", \\\"UK\\\", \\\"Germany\\\", \\\"France\\\"]\\nOnline_Shopping = [430, 500, 700, 650]\\nRetail_Shopping = [400, 450, 410, 350]\\n\\nfig = plt.figure(figsize=(6, 5))\\nax = fig.add_subplot()\\nax.bar(Country, Online_Shopping, label='Online Shopping', bottom=Retail_Shopping)\\nax.bar(Country, Retail_Shopping, label='Retail Shopping')\\nfor i, v in enumerate(Online_Shopping):\\n    ax.text(i-0.2, v/2+Retail_Shopping[i], str(v), color='black', fontsize=12, fontweight='bold')\\nfor i, v in enumerate(Retail_Shopping):\\n    ax.text(i-0.2, v/2, str(v), color='black', fontsize=12, fontweight='bold')\\nplt.title(\\\"Comparison of online and retail shopping in four countries in 2021\\\")\\nplt.legend()\\nplt.xticks(Country)\\nplt.tight_layout()\\nplt.savefig('xxx.png')\\nplt.clf()\"\n    },\n    {\n    \"import numpy as np\\nimport matplotlib.pyplot as plt\\n\\n# Data for the left subplot (AI Publications per year)\\nyears = [2018, 2019, 2020, 2021, 2022]\\npublications = [1500, 1800, 2200, 3000, 3500]\\n\\n# Data for the right subplot (AI Funding per year in billions)\\nfunding = [50, 65, 85, 110, 140]\\n\\n# Setup figure and subplots\\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 7))\\n\\n# Bar colors and edge colors\\ncolors_publications = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#c2c2f0']\\ncolors_funding = ['#ffb3e6', '#c2f0c2', '#ff6666', '#66b3ff', '#ffcc99']\\nedge_color = 'black'  # Outline color for the bars\\n\\n# Left subplot - AI Publications\\nbars1 = ax1.bar(years, publications, color=colors_publications, edgecolor=edge_color)\\nax1.set_title('AI Publications by Year', fontsize=14, pad=20)\\nax1.set_xlabel('Year', fontsize=12)\\nax1.set_ylabel('Number of Publications', fontsize=12)\\nax1.legend(['Publications'], fontsize=12)\\n\\n# Adding annotations for AI Publications\\nfor i, bar in enumerate(bars1):\\n    ax1.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 50, \\n             f'{publications[i]}', ha='center', va='bottom', fontsize=12, color='black')\\n\\n# Right subplot - AI Funding\\nbars2 = ax2.bar(years, funding, color=colors_funding, edgecolor=edge_color)\\nax2.set_title('AI Funding by Year (in Billions)', fontsize=14, pad=20)\\nax2.set_xlabel('Year', fontsize=12)\\nax2.set_ylabel('Funding (Billions $)', fontsize=12)\\nax2.legend(['Funding'], fontsize=12)\\n\\n# Adding annotations for AI Funding\\nfor i, bar in enumerate(bars2):\\n    ax2.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 2, \\n             f'{funding[i]}B', ha='center', va='bottom', fontsize=12, color='black')\\n\\n# Layout adjustments\\nplt.tight_layout()\\n\\n# Save the figure\\nplt.savefig('ai_publications_funding_with_outline.png', bbox_inches='tight')\\n\\n# Show plot\\nplt.show()\"\n    },\n    {\n    \"import numpy as np\\nimport matplotlib.pyplot as plt\\n\\n# Data for the top subplot (Number of Research Papers)\\nmodels = ['ViT', 'BERT', 'GPT', 'ResNet']\\npapers_2021 = [1200, 2500, 1800, 1600]\\npapers_2022 = [1500, 2700, 1900, 1800]\\n\\n# Data for the bottom subplot (Model Training Time in hours)\\ntraining_time_2021 = [50, 75, 100, 45]\\ntraining_time_2022 = [55, 80, 110, 50]\\n\\n# Setup figure and subplots (2 rows, 1 column)\\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\\n\\n# Bar width and spacing\\nbar_width = 0.35\\nx = np.arange(len(models))  # Label locations for models\\n\\n# Bar colors and edge colors (same color for the same year)\\ncolor_2021 = '#66b3ff'  # Blue for 2021\\ncolor_2022 = '#ff9999'  # Red for 2022\\nedge_color = 'black'  # Outline color for the bars\\n\\n# Top subplot - Number of Research Papers\\nbars1 = ax1.bar(x - bar_width/2, papers_2021, width=bar_width, color=color_2021, edgecolor=edge_color, label='2021')\\nbars2 = ax1.bar(x + bar_width/2, papers_2022, width=bar_width, color=color_2022, edgecolor=edge_color, label='2022')\\n\\n# Title and labels\\nax1.set_title('Number of Research Papers on AI Models (2021 vs 2022)', fontsize=14, pad=20)\\nax1.set_xlabel('Models', fontsize=12)\\nax1.set_ylabel('Number of Papers', fontsize=12)\\nax1.set_xticks(x)\\nax1.set_xticklabels(models)\\nax1.legend()\\n\\n# Adding annotations for Number of Research Papers\\nfor bar in bars1 + bars2:\\n    height = bar.get_height()\\n    ax1.text(bar.get_x() + bar.get_width() / 2, height + 30, f'{int(height)}', ha='center', va='bottom', fontsize=11)\\n\\n# Bottom subplot - Model Training Time\\nbars3 = ax2.bar(x - bar_width/2, training_time_2021, width=bar_width, color=color_2021, edgecolor=edge_color, label='2021')\\nbars4 = ax2.bar(x + bar_width/2, training_time_2022, width=bar_width, color=color_2022, edgecolor=edge_color, label='2022')\\n\\n# Title and labels\\nax2.set_title('Model Training Time (in Hours, 2021 vs 2022)', fontsize=14, pad=20)\\nax2.set_xlabel('Models', fontsize=12)\\nax2.set_ylabel('Training Time (hours)', fontsize=12)\\nax2.set_xticks(x)\\nax2.set_xticklabels(models)\\nax2.legend()\\n\\n# Adding annotations for Model Training Time\\nfor bar in bars3 + bars4:\\n    height = bar.get_height()\\n    ax2.text(bar.get_x() + bar.get_width() / 2, height + 2, f'{int(height)}h', ha='center', va='bottom', fontsize=11)\\n\\n# Adjust layout to prevent overlap\\nplt.tight_layout()\\n\\n# Save the figure\\nplt.savefig('ai_models_research_training_same_color.png', bbox_inches='tight')\\n\\n# Show plot\\nplt.show()\"\n    }\n],\n\n\"3d\":[\n    {\n    \"import numpy as np\\nimport matplotlib.pyplot as plt\\nfrom mpl_toolkits.mplot3d import Axes3D\\n\\n# Data\\nx_values = ['USA', 'UK', 'France', 'Spain', 'Italy']\\ny_values = ['NoT (Millions)', 'ASpT ($)', 'TRfT ($Billions)']\\ndata = np.array([[15, 20, 30], [10, 15, 15], [12, 18, 21.6], [8, 14, 11.2], [11, 16, 17.6]])\\n\\nfig = plt.figure(figsize=(12, 10))\\nax = fig.add_subplot(111, projection='3d')\\n\\ncolors = ['r', 'g', 'b']\\nyticks = np.arange(len(y_values))\\n\\n# Plot bars\\nfor k, color in enumerate(colors):\\n    xs = np.arange(len(x_values))\\n    ys = data[:, k]\\n    \\n    # Plot bars with color\\n    bars = ax.bar(xs, ys, zs=k, zdir='y', color=color, alpha=0.8, edgecolor='black')\\n\\n\\n# Set labels\\nax.set_xlabel('Country', labelpad=20)\\nax.set_ylabel('Metrics', labelpad=20)\\nax.set_zlabel('Values')\\nax.set_xticks(np.arange(len(x_values)))\\nax.set_xticklabels(x_values, rotation=0, horizontalalignment=\\\"right\\\")\\nax.set_yticks(yticks)\\nax.set_yticklabels(y_values, rotation=45, ha='right')\\n\\n\\n# Title\\nplt.title('International Tourism and Hospitality Revenue Analysis by Country')\\n\\n# Adjust layout to prevent overlap\\nplt.tight_layout()\\n\\n# Save the figure\\nplt.savefig('xxx', bbox_inches='tight')\\n\\n# Clear the current figure\\nplt.clf()\"\n    },\n    {\n    \"import matplotlib.pyplot as plt\\nfrom mpl_toolkits.mplot3d import Axes3D\\nimport numpy as np\\n\\n# dataset\\ndata = np.array([[60.5, 105, 14.8, 270, 290],\\n                 [68.7, 110.3, 16.2, 280, 305],\\n                 [71.5, 114.2, 18.3, 290, 320],\\n                 [73.1, 115.7, 19.8, 300, 340],\\n                 [75.4, 118.0, 20.5, 310, 355]], dtype=np.float32)\\n\\n# y\\nfood_categories = ['Beef (lbs)', 'Poultry (lbs)', 'Fish (lbs)', 'Fruit (lbs)', 'Vegetables (lbs)']\\n\\n# x-axis data\\nyears = np.array([2018, 2019, 2020, 2021, 2022], dtype=np.int32)\\n\\n# create 3D figure\\nfig = plt.figure(figsize=(10, 7))\\nax = fig.add_subplot(111, projection='3d')\\n\\n# \\ncolors = ['#FF6347', '#4682B4', '#32CD32', '#FFD700', '#8A2BE2']\\n\\n# 3D\\nfor idx in range(len(food_categories)):\\n    ax.bar3d(np.arange(len(years)), [idx]*len(years), np.zeros(len(years)),\\n             0.5, 0.5, data[:, idx], color=colors[idx], alpha=0.9)\\n\\n# y\\nax.yaxis.set_tick_params(pad=10)\\nax.set_yticks(np.arange(len(food_categories)))\\nax.set_yticklabels(food_categories, va='center')\\n\\n# Set the scale and label on the x-axis\\nax.set_xticks(np.arange(len(years)))\\nax.set_xticklabels(years, rotation=30, ha='center')\\n\\n# set title\\nax.set_title('Annual Food Consumption Trends (2018-2022)', fontsize=14)\\n\\n# layout adjustment\\nplt.tight_layout()\\n\\n# save image\\nplt.savefig('xxxx.png')\\n\\nplt.cla()\\nplt.clf()\\nplt.close()\"\n    },\n    {\n    \"import matplotlib.pyplot as plt\\nimport numpy as np\\n\\ny_values = ['Coal Production (Million Tonnes)', 'Oil Production (Million Barrels)', 'Nuclear Energy Generation (GWh)', 'Renewable Energy Generation (GWh)']\\ndata = np.array([[2500, 2900, 3100, 3900], \\n                 [2400, 3100, 3200, 4100],\\n                 [2550, 3300, 3400, 4500],\\n                 [2600, 3500, 3600, 4700],\\n                 [2700, 3800, 3800, 5100]])\\nx_values = ['2019', '2020', '2021', '2022', '2023']\\n\\n# set graph\\nfig = plt.figure(figsize=(15, 8))\\nax = fig.add_subplot(111, projection='3d')\\nX = np.arange(len(x_values))\\n\\n# \\ncolors = ['b', 'g', 'orange', 'purple']\\n\\n# Draw 3D bar plots one by one\\nfor i in range(len(y_values)):\\n    ax.bar3d(X, [i]*len(x_values), np.zeros(len(x_values)), 1, 1, data[:, i], shade=True, color=colors[i])\\n\\n# Set axes\\nax.set_xticks(X)\\nax.set_yticks(range(len(y_values)))\\nax.set_xticklabels(x_values, rotation=45)\\nax.set_yticklabels(y_values, ha='left')\\n\\n# Set title\\nax.set_title('Energy Production and Generation Trends - 2019 to 2023')\\n\\n# layout adjustment\\nplt.tight_layout()\\n\\n# save figure\\nplt.savefig('xxxx.png')\\nplt.clf()\\n\"\n    },\n    {\n    \"import matplotlib.pyplot as plt\\nimport numpy as np\\nfrom mpl_toolkits.mplot3d import Axes3D\\n\\n# Generate some example data\\nx = np.arange(5)\\ny = np.arange(4)\\nX, Y = np.meshgrid(x, y)\\nZ1 = np.random.rand(4, 5) * 10\\nZ2 = np.random.rand(4, 5) * 20\\n\\n# Create a figure and two subplots\\nfig = plt.figure(figsize=(15, 7))\\n\\n# First subplot\\nax1 = fig.add_subplot(121, projection='3d')\\nax1.bar3d(X.flatten(), Y.flatten(), np.zeros_like(X.flatten()), 1, 1, Z1.flatten(), shade=True, color='r')\\nax1.set_title('3D Bar Plot 1')\\nax1.set_xlabel('X axis')\\nax1.set_ylabel('Y axis')\\nax1.set_zlabel('Z axis')\\n\\n# Second subplot\\nax2 = fig.add_subplot(122, projection='3d')\\nax2.bar3d(X.flatten(), Y.flatten(), np.zeros_like(X.flatten()), 1, 1, Z2.flatten(), shade=True, color='g')\\nax2.set_title('3D Bar Plot 2')\\nax2.set_xlabel('X axis')\\nax2.set_ylabel('Y axis')\\nax2.set_zlabel('Z axis')\\n\\n# Adjust layout\\nplt.tight_layout()\\n\\n# Save the figure\\nplt.savefig('2d_3d_bar_plots.png')\\n\\n# Show the figure\\nplt.show()\"\n    },\n    {\n    \"import matplotlib.pyplot as plt\\nimport numpy as np\\nfrom mpl_toolkits.mplot3d import Axes3D\\n\\n# Generate some example data\\nx = np.linspace(0, 10, 100)\\ny = np.sin(x)\\nz = np.cos(x)\\nx2 = np.linspace(0, 10, 100)\\ny2 = np.exp(x2 / 3)\\nz2 = np.log(x2 + 1)\\n\\n# Create a figure and two subplots\\nfig = plt.figure(figsize=(15, 7))\\n\\n# First subplot\\nax1 = fig.add_subplot(121, projection='3d')\\nax1.scatter(x, y, z, c='r', marker='o')\\nax1.set_title('3D Scatter Plot 1')\\nax1.set_xlabel('X axis')\\nax1.set_ylabel('Y axis')\\nax1.set_zlabel('Z axis')\\n\\n# Second subplot\\nax2 = fig.add_subplot(122, projection='3d')\\nax2.scatter(x2, y2, z2, c='b', marker='^')\\nax2.set_title('3D Scatter Plot 2')\\nax2.set_xlabel('X axis')\\nax2.set_ylabel('Y axis')\\nax2.set_zlabel('Z axis')\\n\\n# Adjust layout\\nplt.tight_layout()\\n\\n# Save the figure\\nplt.savefig('3d_scatter_plots.png')\\n\\n# Show the figure\\nplt.show()\"\n    }\n],\n\n\"area\":[\n    {\n    \"import pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\n# Create dictionary of data\\ndata = {\\n    'Category': ['Sociology', 'History', 'Psychology', 'Political Science', 'Literature'],\\n    '2019': [300, 250, 280, 200, 150],\\n    '2020': [330, 270, 300, 230, 160],\\n    '2021': [350, 290, 320, 250, 180],\\n    '2022': [360, 310, 330, 260, 200],\\n    '2023': [370, 320, 340, 270, 210]\\n}\\n\\n# Convert dictionary to DataFrame\\ndf = pd.DataFrame(data)\\n\\n# Set figure size\\nfig, ax = plt.subplots(figsize=(12, 7))\\n\\n# Set x and y axis ticks and labels\\nax.set_xticks(np.arange(len(df['Category'])))\\nax.set_xticklabels(df['Category'], rotation=45, ha='right')\\nax.set_xlim(-0.5, len(df['Category']) - 0.5)\\n\\n# Calculate the maximum total value for y-axis limits\\nmax_total_value = df.iloc[:, 1:].sum(axis=1).max()\\nax.set_ylim(0, np.ceil(max_total_value / 100) * 100)\\nax.set_yticks(np.linspace(0, max_total_value, num=6, dtype=int))\\n\\n# Plot area chart with custom colors\\nax.stackplot(df['Category'], df['2019'], df['2020'], df['2021'], df['2022'], df['2023'],\\n             labels=df.columns[1:], \\n             colors=['#FF9999', '#66B2FF', '#99FF99', '#FFCC99', '#FFCCFF'], \\n             alpha=0.7)\\n\\n# Set background grid lines\\nax.grid(color='grey', linestyle='--', linewidth=0.5, alpha=0.5)\\n\\n# Set legend and its properties\\nlegend = ax.legend(loc='upper left')\\nlegend.get_frame().set_alpha(0.8)\\n\\n# Add title and labels\\nax.set_title('Publications by Social Sciences and Humanities Categories (2019 - 2023)')\\nax.set_xlabel('Category')\\nax.set_ylabel('Number of Publications')\\n\\n# Automatically adjust layout\\nfig.tight_layout()\\n\\n# Save the figure\\nplt.savefig('output/final/area_chart/png/20231228-140159_66.png', bbox_inches='tight')\\n\\n# Clear the current figure state\\nplt.clf()\"\n    },\n    {\n    \"import pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\n# Create dictionary of data with computing-related content\\ndata = {\\n    'Category': ['Software Development', 'Data Science', 'Cybersecurity', 'AI & Machine Learning', 'Networking'],\\n    '2019': [500, 600, 400, 450, 350],\\n    '2020': [550, 650, 420, 480, 370],\\n    '2021': [600, 700, 450, 500, 400],\\n    '2022': [650, 750, 480, 520, 430],\\n    '2023': [700, 800, 500, 540, 460]\\n}\\n\\n# Convert dictionary to DataFrame\\ndf = pd.DataFrame(data)\\n\\n# Set figure size\\nfig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(12, 14))\\n\\n# Define colors for the plots\\ncolors = ['#FF6347', '#FFD700', '#4682B4', '#32CD32', '#FF4500']  # Colors: red, yellow, blue, green, orange\\n\\n# Set x and y axis ticks and labels for the first subplot\\nax1.set_xticks(np.arange(len(df['Category'])))\\nax1.set_xticklabels(df['Category'], rotation=45, ha='right')\\nax1.set_xlim(-0.5, len(df['Category']) - 0.5)\\n\\n# Calculate the maximum total value for y-axis limits for the first subplot\\nmax_total_value = df.iloc[:, 1:].sum(axis=1).max()\\nax1.set_ylim(0, np.ceil(max_total_value / 100) * 100)\\nax1.set_yticks(np.linspace(0, max_total_value, num=6, dtype=int))\\n\\n# Plot area chart with updated colors in the first subplot\\nax1.stackplot(df['Category'], df['2019'], df['2020'], df['2021'], df['2022'], df['2023'],\\n             labels=df.columns[1:], \\n             colors=colors, \\n             alpha=0.7)\\n\\n# Add star markers for the lines in the first subplot\\nfor i in range(1, len(df.columns) - 1):\\n    ax1.plot(df['Category'], df[df.columns[i]], marker='*', linestyle='-', color=colors[i - 1])\\n\\n# Set background grid lines for the first subplot\\nax1.grid(color='grey', linestyle='--', linewidth=0.5, alpha=0.5)\\n\\n# Set legend and its properties for the first subplot\\nlegend1 = ax1.legend(loc='upper left')\\nlegend1.get_frame().set_alpha(0.8)\\n\\n# Add title and labels for the first subplot\\nax1.set_title('Computing-Related Publications by Category (2019 - 2023)')\\nax1.set_xlabel('Category')\\nax1.set_ylabel('Number of Publications')\\n\\n# Set x and y axis ticks and labels for the second subplot\\nax2.set_xticks(np.arange(len(df['Category'])))\\nax2.set_xticklabels(df['Category'], rotation=45, ha='right')\\nax2.set_xlim(-0.5, len(df['Category']) - 0.5)\\n\\n# Calculate the maximum total value for y-axis limits for the second subplot\\nax2.set_ylim(0, np.ceil(max_total_value / 100) * 100)\\nax2.set_yticks(np.linspace(0, max_total_value, num=6, dtype=int))\\n\\n# Plot area chart with updated colors in the second subplot\\nax2.stackplot(df['Category'], df['2019'], df['2020'], df['2021'], df['2022'], df['2023'],\\n             labels=df.columns[1:], \\n             colors=colors, \\n             alpha=0.7)\\n\\n# Add star markers for the lines in the second subplot\\nfor i in range(1, len(df.columns) - 1):\\n    ax2.plot(df['Category'], df[df.columns[i]], marker='*', linestyle='-', color=colors[i - 1])\\n\\n# Set background grid lines for the second subplot\\nax2.grid(color='grey', linestyle='--', linewidth=0.5, alpha=0.5)\\n\\n# Set legend and its properties for the second subplot\\nlegend2 = ax2.legend(loc='upper left')\\nlegend2.get_frame().set_alpha(0.8)\\n\\n# Add title and labels for the second subplot\\nax2.set_title('Computing-Related Publications by Category (2019 - 2023)')\\nax2.set_xlabel('Category')\\nax2.set_ylabel('Number of Publications')\\n\\n# Automatically adjust layout\\nfig.tight_layout()\\n\\n# Save the figure\\nplt.savefig('xxx.png', bbox_inches='tight')\\n\\n# Clear the current figure state\\nplt.clf()\"\n    },\n    {\n    \"# Part 1: Importing Libraries\\nimport matplotlib.pyplot as plt\\n\\n# Part 2: Data Preparation\\n# Data for plotting\\nx = [1, 2, 3, 4, 5]\\ny1 = [33, 31, 27, 29, 29.5]\\ny2 = [29, 27, 21, 23, 24]\\ny3 = [26, 25, 12, 14, 15]\\ny4 = [21, 22, 17, 10, 8.5]\\n\\n# Labels for legend\\nlabel_activity_net_mIoU = \\\"ActivityNet mIoU\\\"\\nlabel_breakfast_mof = \\\"Breakfast MoF\\\"\\nlabel_activity_net_cider = \\\"ActivityNet CIDEr\\\"\\nlabel_qvhighlights_map = \\\"QVHighlights mAP\\\"\\n\\n# Plot limits\\nxlim_values = (1, 5)\\nylim_values = (0, 35)\\n\\n# Axis labels\\nxlabel_values = [\\\"10K\\\", \\\"50K\\\", \\\"1M\\\", \\\"5M\\\", \\\"10M\\\"]\\nylabel_values = [0, 10, 20, 30, 34]\\n\\n# Axis ticks\\nxticks_values = x\\nyticks_values = [0, 10, 20, 30, 34]\\n\\n# Horizontal line value\\naxhline_value = 30\\n\\n# Part 3: Plot Configuration and Rendering\\n# Plotting the data\\nplt.figure(figsize=(9, 8))  # Adjusting figure size to match original image dimensions\\nplt.plot(\\n    x,\\n    y1,\\n    \\\"s-\\\",\\n    clip_on=False,\\n    zorder=10,\\n    markerfacecolor=\\\"#ff9999\\\",\\n    markeredgecolor=\\\"#cc3333\\\",\\n    markersize=10,\\n    color=\\\"#cc3333\\\",\\n    label=label_activity_net_mIoU,\\n)\\nplt.plot(\\n    x,\\n    y2,\\n    \\\"s-\\\",\\n    clip_on=False,\\n    zorder=10,\\n    markerfacecolor=\\\"#ffcc99\\\",\\n    markeredgecolor=\\\"#cc9966\\\",\\n    markersize=10,\\n    color=\\\"#cc9966\\\",\\n    label=label_breakfast_mof,\\n)\\nplt.plot(\\n    x,\\n    y3,\\n    \\\"s-\\\",\\n    clip_on=False,\\n    zorder=10,\\n    markerfacecolor=\\\"#99cc99\\\",\\n    markeredgecolor=\\\"#339966\\\",\\n    markersize=10,\\n    color=\\\"#339966\\\",\\n    label=label_activity_net_cider,\\n)\\nplt.plot(\\n    x,\\n    y4,\\n    \\\"s-\\\",\\n    clip_on=False,\\n    zorder=10,\\n    markerfacecolor=\\\"#cccccc\\\",\\n    markeredgecolor=\\\"#666666\\\",\\n    markersize=10,\\n    color=\\\"#666666\\\",\\n    label=label_qvhighlights_map,\\n)\\n\\n# Filling the area under the curves\\nplt.fill_between(x, y1, y2, color=\\\"#ff9999\\\", alpha=0.5)\\nplt.fill_between(x, y2, y3, color=\\\"#ffcc99\\\", alpha=0.5)\\nplt.fill_between(x, y3, y4, color=\\\"#99cc99\\\", alpha=0.5)\\nplt.fill_between(x, y4, color=\\\"#cccccc\\\", alpha=0.5)\\n\\n# Adding a horizontal dashed line at y=axhline_value\\nplt.axhline(axhline_value, color=\\\"black\\\", linestyle=\\\"dotted\\\")\\n\\n# Setting the x-axis and y-axis limits\\nplt.xlim(*xlim_values)\\nplt.ylim(*ylim_values)\\n\\n# Setting the x-axis tick labels\\nplt.xticks(xticks_values, xlabel_values)\\nplt.yticks(yticks_values, ylabel_values)\\n\\n# Adding a legend\\nplt.legend(loc=\\\"lower center\\\", ncol=4, bbox_to_anchor=(0.5, -0.1), frameon=False)\\nplt.gca().tick_params(axis=\\\"both\\\", which=\\\"both\\\", length=0)\\n\\n# Part 4: Saving Output\\nplt.tight_layout()\\nplt.savefig(\\\"area_1.pdf\\\", bbox_inches=\\\"tight\\\")\\nplt.clf()\"\n    }\n],\n\n\"box\":[\n    {\n    \"import matplotlib.pyplot as plt\\nimport numpy as np\\n\\n# Restructure the data\\ndata = [[45000, 50000, 70000, 90000, 120000],\\n        [25000, 30000, 40000, 60000, 80000],\\n        [30000, 35000, 45000, 60000, 75000],\\n        [15000, 20000, 25000, 30000, 40000],\\n        [10000, 15000, 20000, 25000, 30000]]\\noutliers = [[],\\n            [120000, 150000],\\n            [90000, 100000],\\n            [45000],\\n            [35000]]\\n\\n# Plot the chart\\nfig = plt.figure(figsize=(10, 7))\\nax = fig.add_subplot(111)\\nboxprops = dict(linewidth=1.5, color='steelblue')\\nmedianprops = dict(linewidth=2, color='darkblue')\\nwhiskerprops = dict(linewidth=1.5, color='darkorange')\\ncapprops = dict(linewidth=1.5, color='darkorange')\\n\\nax.boxplot(data, \\n           whis=1.5, \\n           boxprops=boxprops, \\n           medianprops=medianprops, \\n           whiskerprops=whiskerprops, \\n           capprops=capprops, \\n           flierprops=dict(markerfacecolor='r', marker='o'))\\n\\n# Plot Outliers\\nfor i, outlier in enumerate(outliers):\\n    if len(outlier) > 0:\\n        ax.plot(np.repeat(i+1, len(outlier)), outlier, 'ro', markersize=8, alpha=0.7)\\n\\n# Adjust the chart\\nax.set_title('Salary Distribution by Employee Type (2020)', fontsize=14, fontweight='bold')\\nax.set_xlabel('Employee Type', fontsize=12)\\nax.set_ylabel('Salary (USD)', fontsize=12)\\nax.set_xticklabels(['Full Time', 'Part Time', 'Contract', 'Internship', 'Seasonal'], fontsize=10)\\n\\n# Add grids\\nax.yaxis.grid(True, linestyle='--', which='major', color='grey', alpha=0.6)\\nax.set_axisbelow(True)\\n\\n# Resize the chart\\nplt.tight_layout()\\n\\n# Save the chart\\nplt.savefig('salary_box.png')\\n\\n# Clear the current image state\\nplt.cla()\"\n    },\n    {\n    \"import matplotlib.pyplot as plt\\nimport numpy as np\\n\\ncategories = ['E-commerce', 'Social Media', 'Streaming Services', 'Online Gaming', 'EdTech']\\nvalue_lists = [[61, 65, 80, 95, 110], [87, 70, 92, 100, 115], [49, 72, 48, 105, 120], [42, 68, 82, 94, 108], [68, 77, 90, 132, 112]]\\noutliers_lists = [[], [110, 125], [125], [115, 120], [45]]\\n\\n# Create figure\\nfig = plt.figure(figsize=(12, 8))\\nax = fig.add_subplot()\\n\\n# Box plotting with updated style\\nbp = ax.boxplot(value_lists, whis=1.5, notch=True, patch_artist=True, boxprops=dict(edgecolor='black', linewidth=1.5), whiskerprops=dict(color='darkblue', linewidth=1.5), medianprops=dict(color='orange', linewidth=2), capprops=dict(color='darkblue', linewidth=1.5))\\n\\n# Set the colors of the boxes\\ncolors = ['#00008B', '#BDB76B', '#FF1493', '#696969', '#F0F8FF']\\nfor patch, color in zip(bp['boxes'], colors):\\n    patch.set_facecolor(color)\\n    patch.set_edgecolor('black')\\n\\n# Outliers plotting with updated style\\nfor i, outliers in enumerate(outliers_lists):\\n    if outliers:\\n        x = [i + 1] * len(outliers)\\n        ax.scatter(x, outliers, color='red', edgecolor='black', s=50, zorder=5)\\n\\n# Adding grid with a different style\\nax.grid(True, linestyle='--', color='grey', alpha=0.7)\\n\\n# Set axes labels and title\\nax.set_xticklabels(categories, rotation=45, ha=\\\"right\\\", fontsize=10)\\nax.set_ylabel('User Experience Score', fontsize=12)\\nax.set_title('User Experience Score Distribution by Internet Aspect (2022)', fontsize=14, fontweight='bold')\\n\\nplt.tight_layout()\\n\\n# Save and show the figure\\nplt.savefig('experience_score_box.png')\\nplt.show()\\n\\n# Clear current image state\\nplt.clf()\"\n    },\n    {\n    \"\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\n\\n# New data for the two subplots\\n\\ndata_1 = [[800, 1200, 1800, 2500, 3200], [600, 900, 1400, 2000, 2800], [700, 1100, 1500, 2200, 2900], [400, 700, 1000, 1600, 2300], [900, 1400, 1900, 2600, 3500]]\\n\\noutliers_1 = [[], [4000], [300, 5000], [2500], [4100]]\\n\\n\\ndata_2 = [[500, 700, 1100, 1500, 1900], [300, 400, 600, 800, 1000], [450, 650, 900, 1300, 1700], [350, 500, 700, 900, 1200], [800, 1200, 1600, 2200, 3000]]\\n\\noutliers_2 = [[], [2500], [200, 4000], [1500], [3200]]\\n\\n\\n# Create the figure and subplots with left and right arrangement\\n\\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\\n\\n\\n# Plot the first boxplot (left side)\\n\\nax1.boxplot(data_1, whis=1.5)\\n\\nfor i, outlier in enumerate(outliers_1):\\n\\n    if len(outlier) > 0:\\n\\n        ax1.plot([i + 1] * len(outlier), outlier, 'bo')\\n\\n\\nax1.set_title('Employee Salaries in 2023')\\n\\nax1.set_ylabel('Salary (USD)')\\n\\nax1.set_xticklabels(['HR', 'Marketing', 'Finance', 'IT', 'Operations'], fontsize=10, rotation=30)\\n\\nax1.yaxis.grid(True)\\n\\n\\n# Plot the second boxplot (right side)\\n\\nax2.boxplot(data_2, whis=1.5)\\n\\nfor i, outlier in enumerate(outliers_2):\\n\\n    if len(outlier) > 0:\\n\\n        ax2.plot([i + 1] * len(outlier), outlier, 'go')\\n\\n\\nax2.set_title('Project Budgets in 2023')\\n\\nax2.set_ylabel('Budget (Million USD)')\\n\\nax2.set_xticklabels(['Project A', 'Project B', 'Project C', 'Project D', 'Project E'], fontsize=10, rotation=30)\\n\\nax2.yaxis.grid(True)\\n\\n\\n# Adjust layout and display the plots\\n\\nplt.tight_layout()\\n\\n\\n# Display the generated plot\\n\\nplt.show()\\n\\n\"\n    }\n],\n\n\"bubble\":[\n    {\n    \"\\nimport matplotlib.pyplot as plt\\nimport matplotlib.cm as cm\\nimport numpy as np\\n\\n\\n# New data labels and dataset related to economic sectors\\n\\ndata_labels = ['Investment (Billion $)', 'Market Share (%)', 'Profit Margin (Score)', 'Employees (Millions)']\\n\\ndata = [['Finance', 950, 25, 8.1, 5.5], ['Energy', 1200, 40, 7.8, 6.3], ['Healthcare', 800, 15, 7.5, 4.7], \\n\\n        ['Technology', 1100, 30, 9.0, 7.2], ['Real Estate', 650, 10, 6.8, 3.9], ['Telecom', 700, 20, 8.5, 4.3]]\\n\\nline_labels = [data[i][0] + ' ' + str(data[i][3]) for i in range(len(data))]\\n\\n\\nfig = plt.figure(figsize=(8, 6))\\n\\nax = fig.add_subplot(1, 1, 1)\\n\\n\\n# Normalize data for color mapping\\n\\nsc = None\\n\\nnorm = cm.colors.Normalize(vmin=np.min(np.array([data[i][4] for i in range(len(data))])), \\n\\n                           vmax=np.array([data[i][4] for i in range(len(data))]).max())\\n\\ns_m = cm.ScalarMappable(cmap=cm.get_cmap('viridis'), norm=norm)\\n\\ns_m.set_array([])\\n\\n\\n# Scatter plot with size and color variation based on employees and profit margin\\n\\nfor i in range(len(data)):\\n\\n    sc = ax.scatter(data[i][1], data[i][2], \\n\\n                    s=(data[i][3] - 6.8) / (9.0 - 6.8) * (5000 - 600) + 600, \\n\\n                    c=s_m.to_rgba(data[i][4]), label=None)\\n\\n    ax.scatter([], [], c=s_m.to_rgba(data[i][4]), label=line_labels[i], s=20)\\n\\n\\n# Add legend and color bar\\n\\nax.legend(title=data_labels[2])\\n\\n\\ncbar = fig.colorbar(s_m, ax=ax)\\n\\ncbar.set_label(data_labels[3], rotation=90)\\n\\n\\n# Set axis labels, ticks, and grid\\n\\nax.set_xticks(np.arange(200, 1301, 200))\\n\\nax.set_xlabel(data_labels[0])\\n\\nax.set_ylabel(data_labels[1])\\n\\nax.grid(linestyle='--')\\n\\n\\n# Set title and layout\\n\\nplt.title('Global Economic Sector Performance in 2023')\\n\\nplt.tight_layout()\\n\\nplt.show()\\n\"\n    },\n    {\n    \"\\nimport matplotlib.pyplot as plt\\nimport matplotlib.cm as cm\\nimport numpy as np\\n\\n\\n# New data labels and dataset related to economic sectors\\n\\ndata_labels = ['Investment (Billion $)', 'Market Share (%)', 'Profit Margin (Score)', 'Employees (Millions)']\\n\\ndata = [['Finance', 950, 25, 8.1, 5.5], ['Energy', 1200, 40, 7.8, 6.3], ['Healthcare', 800, 15, 7.5, 4.7], \\n\\n        ['Technology', 1100, 30, 9.0, 7.2], ['Real Estate', 650, 10, 6.8, 3.9], ['Telecom', 700, 20, 8.5, 4.3]]\\n\\nline_labels = [data[i][0] + ' ' + str(data[i][3]) for i in range(len(data))]\\n\\n\\nfig = plt.figure(figsize=(8, 6))\\n\\nax = fig.add_subplot(1, 1, 1)\\n\\n\\n# Normalize data for color mapping\\n\\nsc = None\\n\\nnorm = cm.colors.Normalize(vmin=np.min(np.array([data[i][4] for i in range(len(data))])), \\n\\n                           vmax=np.array([data[i][4] for i in range(len(data))]).max())\\n\\ns_m = cm.ScalarMappable(cmap=cm.get_cmap('viridis'), norm=norm)\\n\\ns_m.set_array([])\\n\\n\\n# Scatter plot with size and color variation based on employees and profit margin, using solid circles ('o')\\n\\nfor i in range(len(data)):\\n\\n    sc = ax.scatter(data[i][1], data[i][2], \\n\\n                    s=(data[i][3] - 6.8) / (9.0 - 6.8) * (5000 - 600) + 600, \\n\\n                    c=s_m.to_rgba(data[i][4]), marker='o', label=None)\\n\\n    ax.scatter([], [], c=s_m.to_rgba(data[i][4]), marker='o', label=line_labels[i], s=20)\\n\\n\\n# Add legend and color bar\\n\\nax.legend(title=data_labels[2])\\n\\n\\ncbar = fig.colorbar(s_m, ax=ax)\\n\\ncbar.set_label(data_labels[3], rotation=90)\\n\\n\\n# Set axis labels, ticks, and grid\\n\\nax.set_xticks(np.arange(200, 1301, 200))\\n\\nax.set_xlabel(data_labels[0])\\n\\nax.set_ylabel(data_labels[1])\\n\\nax.grid(linestyle='--')\\n\\n\\n# Set title and layout\\n\\nplt.title('Global Economic Sector Performance in 2023')\\n\\nplt.tight_layout()\\n\\nplt.show()\\n\"\n    }\n],\n\"candlestick\": [\n    {\n    \"\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport mplfinance as mpf\\n\\n\\n# Updated data related to the tech sector\\n\\ndata = {\\n    'Date': ['2023-01-10', '2023-01-17', '2023-01-24', '2023-01-31', '2023-02-07', '2023-02-14', '2023-02-21', '2023-02-28', '2023-03-07', '2023-03-14'],\\n    'Opening Price ($)': [105.0, 110.5, 112.0, 115.0, 113.3, 109.0, 112.0, 109.5, 108.0, 109.0],\\n    'Closing Price ($)': [110.6, 112.5, 115.0, 113.3, 109.0, 112.0, 109.5, 108.0, 109.0, 110.8],\\n    'High Price ($)': [112.3, 113.5, 117.0, 119.0, 115.0, 116.0, 117.0, 111.8, 112.0, 114.0],\\n    'Low Price ($)': [102.7, 107.0, 109.0, 111.5, 105.0, 106.0, 108.0, 106.5, 106.0, 107.2]\\n}\\n\\n\\ndf = pd.DataFrame(data)\\n\\ndf['Date'] = pd.to_datetime(df['Date'])\\n\\ndf.set_index('Date', inplace=True)\\n\\ndf.rename(columns={'Opening Price ($)': 'Open', 'Closing Price ($)': 'Close', 'High Price ($)': 'High', 'Low Price ($)': 'Low'}, inplace=True)\\n\\n\\n# Plot candlestick chart for the tech sector stock prices\\n\\nfig = plt.figure(figsize=(10, 6))\\nmpf.plot(df, type='candle', title='Weekly Stock Price Trend in Tech Sector', figratio=(8, 6), \\n         savefig=dict(fname='xxxx.png'))\\nplt.close(fig)\\n\"\n    },\n    {\n    \"\\nimport plotly.graph_objects as go\\nimport pandas as pd\\n\\n# Updated data related to the biotechnology sector\\n\\ndata = [['2023-04-26', 120.5, 125.2, 127.8, 119.4],\\n        ['2023-04-27', 125.0, 126.4, 128.7, 123.9],\\n        ['2023-04-28', 126.4, 124.9, 127.5, 122.3],\\n        ['2023-04-29', 124.9, 128.2, 129.5, 123.7],\\n        ['2023-04-30', 128.5, 130.7, 132.3, 127.0],\\n        ['2023-05-01', 130.7, 131.5, 133.8, 129.5],\\n        ['2023-05-02', 131.5, 130.0, 132.0, 128.9],\\n        ['2023-05-03', 130.0, 131.0, 132.4, 129.5]]\\n\\n\\ndf = pd.DataFrame(data, columns=['Date', 'Open Price ($)', 'Close Price ($)', 'High Price ($)', 'Low Price ($)'])\\n\\n# Plot candlestick chart for biotechnology sector\\n\\nfig = go.Figure(data=[go.Candlestick(x=df['Date'],\\n                                    open=df['Open Price ($)'],\\n                                    high=df['High Price ($)'],\\n                                    low=df['Low Price ($)'],\\n                                    close=df['Close Price ($)'])])\\nfig.update_layout(title='Biotechnology Stock Price Trend Overview',\\n                  yaxis_range=[min(df['Low Price ($)']), max(df['High Price ($)'])],\\n                  width=1800,\\n                  height=1000,\\n                  font=dict(family='Courier New, monospace', size=18, color='#7f7f7f'))\\n\\n# Save the figure to a new file related to biotechnology sector\\nfig.write_image('biotech_sector_2024.png')\\n\"\n    },\n],\n\n\"funnel\": [\n    {\n    \"\\nimport plotly.graph_objects as go\\n\\n# Create a funnel chart with updated data for the biology field\\nfig = go.Figure(go.Funnel(\\n    y=[\\\"Biology Basics\\\", \\\"Genetics\\\", \\\"Molecular Biology\\\", \\\"Biotechnology\\\", \\\"Biomedicine\\\"],\\n    x=[150, 120, 90, 60, 30],\\n    textinfo=\\\"value+percent initial\\\",\\n    orientation=\\\"h\\\",\\n    marker_color='green'\\n))\\n\\n# Update layout with a new title and font settings\\nfig.update_layout(\\n    title_text='Education Level in Biology in 2023',\\n    font=dict(\\n        size=14\\n    )\\n)\\n\\n# Save the figure with a new filename related to biology field\\nfig.write_image(\\\"funnel_biology_updated.png\\\")\\n\"\n    },\n    {\n    \"\\nimport plotly.graph_objects as go\\n\\n# Create a funnel chart with updated data for the chemistry field\\nfig = go.Figure(go.Funnel(\\n    y=[\\\"Chemical Synthesis\\\", \\\"Purification\\\", \\\"Compound Characterization\\\", \\\"Toxicity Testing\\\", \\\"Scaling Up\\\", \\\"Product Delivery\\\"],\\n    x=[3000, 2500, 2100, 1700, 1000, 600],\\n    textinfo=\\\"value\\\",\\n    textfont_size=14,\\n    opacity=0.8,\\n    marker={\\\"color\\\": [\\\"blue\\\", \\\"green\\\", \\\"purple\\\", \\\"orange\\\", \\\"yellow\\\", \\\"red\\\"]},\\n))\\n\\n# Update layout with a new title and font settings\\nfig.update_layout(\\n    title={\\\"text\\\": \\\"Processes Completed in Chemical Production in 2023\\\"},\\n    font={\\\"family\\\": \\\"Courier New, monospace\\\", \\\"size\\\": 14},\\n    autosize=False,\\n    width=800,\\n    height=800,\\n    showlegend=True,\\n    xaxis_title=\\\"Number of Processes\\\",\\n    yaxis_title=\\\"Stage\\\",\\n    margin={\\\"l\\\": 140, \\\"b\\\": 40, \\\"t\\\": 140, \\\"r\\\": 40},\\n    paper_bgcolor=\\\"white\\\",\\n    plot_bgcolor=\\\"white\\\",\\n    legend={\\\"x\\\": 0.82, \\\"y\\\": 0.95},\\n)\\n\\n# Save the figure with a new filename related to chemistry field\\nfig.write_image(\\\"funnel_chemistry_2023.png\\\")\\n\"\n    }\n],\n\"heatmap\": [\n    {\n    \"\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport pandas as pd\\n\\n# Process the updated data for political field\\n\\ndata = {'Country': ['Germany', 'United Kingdom', 'United States', 'Russia', 'China'], \\n        'Political Stability (Index)': [70, 75, 60, 50, 65], \\n        'Government Effectiveness (Index)': [80, 85, 78, 55, 70], \\n        'Corruption Control (Index)': [65, 80, 68, 40, 55], \\n        'Press Freedom (Index)': [72, 75, 60, 30, 50], \\n        'Judicial Independence (Index)': [78, 82, 70, 45, 60]}\\ndf = pd.DataFrame(data)\\n\\n# Plot the heatmap chart\\n\\nfig, ax = plt.subplots(figsize=(10, 6))\\nim = ax.imshow(df.iloc[:, 1:].values, cmap='Blues')\\n\\n# Set the ticks and ticklabels for x and y axis\\nax.set_xticks(np.arange(len(df.columns[1:])))\\nax.set_yticks(np.arange(len(df['Country'])))\\nax.set_xticklabels(df.columns[1:])\\nax.set_yticklabels(df['Country'])\\n\\n# Rotate the x-axis tick labels by 45 degrees and align them to the right\\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\\\"right\\\", rotation_mode=\\\"anchor\\\")\\n\\n# Center the ticks and ticklabels\\nax.tick_params(axis='both', which='both', length=0.5, pad=10)\\nax.tick_params(axis='x', which='major', pad=20)\\nax.tick_params(axis='y', which='major', pad=10)\\n\\n# Create a colorbar and add the label\\ncbar = ax.figure.colorbar(im)\\ncbar.ax.set_ylabel('Index Values', rotation=-90, va=\\\"bottom\\\", labelpad=20)\\n\\n# Show the value of each cell\\nfor i in range(len(df['Country'])):\\n    for j in range(len(df.columns[1:])):\\n        text = ax.text(j, i, df.iloc[i, j+1], ha=\\\"center\\\", va=\\\"center\\\", color=\\\"black\\\")\\n\\n# Set the title\\nax.set_title('Political Metrics by Country')\\n\\n# Automatically resize the image and save it\\nfig.tight_layout()\\nplt.savefig('political_metrics_heatmap_2023.png', bbox_inches='tight')\\n\\n# Clear the current image state\\nplt.clf()\\nplt.close()\\n\"\n    },\n    {\n    \"\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport pandas as pd\\nimport seaborn as sns\\n\\n# Updated random data for the technology sector\\n\\ndata = {'Category': ['AI', 'Blockchain', 'Cloud Computing', 'Cybersecurity', 'IoT'], \\n        'Research': [70, 65, 90, 85, 75], \\n        'Development': [60, 70, 85, 80, 65], \\n        'Adoption': [50, 45, 75, 65, 60], \\n        'Market Growth': [80, 75, 95, 85, 70], \\n        'User Engagement': [75, 70, 80, 90, 65], \\n        'Investment': [85, 80, 95, 90, 70]}\\ndf = pd.DataFrame(data, columns = ['Category', 'Research', 'Development', 'Adoption', 'Market Growth', 'User Engagement', 'Investment'])\\ndf = df.set_index('Category')\\n\\n# Plotting the heatmap\\n\\nfig, ax = plt.subplots(figsize=(10, 6))\\nsns.heatmap(df, annot=True, cmap='coolwarm', linewidths=.5, cbar=True, ax=ax)\\n\\n# Setting ticks and ticklabels\\nax.set_xticks(np.arange(len(df.columns)) + 0.5, minor=False)\\nax.set_yticks(np.arange(len(df)) + 0.5, minor=False)\\nax.set_xticklabels(df.columns, minor=False)\\nax.set_yticklabels(df.index, minor=False)\\n\\n# Rotating and wrapping the labels\\nplt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\\nplt.setp(ax.get_yticklabels(), wrap=True)\\n\\n# Setting ticks and ticklabels in the center\\nax.tick_params(axis='both', which='both', length=0, labelsize=12)\\nax.tick_params(axis='x', which='major', pad=15)\\nax.tick_params(axis='y', which='major', pad=15)\\n\\n# Title and labels\\nplt.title('Technology Sector Performance Metrics', fontsize=15)\\nplt.xlabel('Metrics', fontsize=12)\\nplt.ylabel('Technology Areas', fontsize=12)\\n\\n# Resizing and saving the figure\\nplt.tight_layout()\\nplt.savefig('technology_sector_heatmap_2023.png', bbox_inches='tight')\\n\\n# Clearing the plot\\nplt.clf()\\n\"\n    }\n],\n\"multi-axes\":[\n    {\n    \"\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\n\\n# Generate data for the bar chart and line plot\\n\\ncategories = ['A', 'B', 'C', 'D', 'E']\\n\\nbar_values = [30, 50, 45, 70, 65]\\n\\nline_values1 = [20, 35, 50, 65, 80]\\n\\nline_values2 = [15, 30, 55, 60, 85]\\n\\nline_values3 = [10, 40, 30, 50, 60]\\n\\n\\n# Set up the figure and axes\\n\\nfig, ax1 = plt.subplots(figsize=(10, 6))\\n\\n\\n# Plot the bar chart\\n\\nbar_width = 0.4\\n\\nindex = np.arange(len(categories))\\n\\nbars = ax1.bar(index, bar_values, bar_width, color='b', label='Bar Data')\\n\\n\\n# Set up a secondary axis for the line plot\\n\\nax2 = ax1.twinx()\\n\\n\\n# Plot three lines on top of the bars to make the plot more complex\\n\\nline1 = ax2.plot(index, line_values1, color='r', marker='o', label='Line Data 1', linestyle='--', linewidth=2)\\n\\nline2 = ax2.plot(index, line_values2, color='g', marker='x', label='Line Data 2', linestyle=':', linewidth=2)\\n\\nline3 = ax2.plot(index, line_values3, color='purple', marker='s', label='Line Data 3', linestyle='-.', linewidth=2)\\n\\n\\n# Add data labels to the bar chart\\n\\nfor bar in bars:\\n\\n    height = bar.get_height()\\n\\n    ax1.text(bar.get_x() + bar.get_width() / 2.0, height, f'{height}', ha='center', va='bottom', fontsize=10, color='black')\\n\\n\\n# Add data labels to the line plots\\n\\nfor i, txt in enumerate(line_values1):\\n\\n    ax2.annotate(txt, (index[i], line_values1[i]), textcoords=\\\"offset points\\\", xytext=(0,5), ha='center', fontsize=8, color='r')\\n\\nfor i, txt in enumerate(line_values2):\\n\\n    ax2.annotate(txt, (index[i], line_values2[i]), textcoords=\\\"offset points\\\", xytext=(0,5), ha='center', fontsize=8, color='g')\\n\\nfor i, txt in enumerate(line_values3):\\n\\n    ax2.annotate(txt, (index[i], line_values3[i]), textcoords=\\\"offset points\\\", xytext=(0,5), ha='center', fontsize=8, color='purple')\\n\\n\\n# Customize the appearance\\n\\nax1.set_xlabel('Categories', fontsize=12)\\n\\nax1.set_ylabel('Bar Values', fontsize=12, color='b')\\n\\nax2.set_ylabel('Line Values', fontsize=12, color='r')\\n\\nax1.set_title('Complex Bar and Line Plot', fontsize=16)\\n\\n\\n# Set the ticks and gridlines for both axes\\n\\nax1.set_xticks(index)\\n\\nax1.set_xticklabels(categories, fontsize=10)\\n\\nax1.grid(True, which='major', axis='y', linestyle='--', linewidth=0.7)\\n\\nax2.grid(True, which='major', axis='y', linestyle=':', linewidth=0.5)\\n\\n\\n# Add legends for both bar and line data\\n\\nbars_legend = ax1.legend(loc='upper left')\\n\\nlines_legend = ax2.legend(loc='upper right')\\n\\n\\n# Adjust layout and show the plot\\n\\nplt.tight_layout()\\n\\nplt.show()\\n\\n\"\n    },\n    {\n    \"\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\n\\n# Generate data for the bar chart and line plots\\n\\ncategories = ['A', 'B', 'C', 'D', 'E']\\n\\nbar_values1 = [30, 50, 45, 70, 65]\\n\\nline_values1 = [20, 35, 50, 65, 80]\\n\\n\\nbar_values2 = [40, 60, 55, 75, 85]\\n\\nline_values2 = [25, 40, 60, 70, 90]\\n\\n\\n# Set up the figure with two subplots, side by side\\n\\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\\n\\n\\n# First subplot: bar chart and line plot\\n\\nbar_width = 0.4\\n\\nindex = np.arange(len(categories))\\n\\nbars1 = ax1.bar(index, bar_values1, bar_width, color='blue', label='Bar Data 1')\\n\\nax3 = ax1.twinx()\\n\\nline1 = ax3.plot(index, line_values1, color='red', marker='o', label='Line Data 1', linestyle='--', linewidth=2)\\n\\n\\n# Customize the first subplot\\n\\nax1.set_xlabel('Categories', fontsize=12)\\n\\nax1.set_ylabel('Bar Values', fontsize=12, color='blue')\\n\\nax3.set_ylabel('Line Values', fontsize=12, color='red')\\n\\nax1.set_title('Bar and Line Plot (Left)', fontsize=16)\\n\\nax1.set_xticks(index)\\n\\nax1.set_xticklabels(categories, fontsize=10)\\n\\nax1.grid(True, which='major', axis='y', linestyle='--', linewidth=0.7)\\n\\nax3.grid(True, which='major', axis='y', linestyle=':', linewidth=0.5)\\n\\n\\n# Add legends for the first subplot\\n\\nbars1_legend = ax1.legend(loc='upper left')\\n\\nline1_legend = ax3.legend(loc='upper right')\\n\\n\\n# Second subplot: bar chart and line plot\\n\\nbars2 = ax2.bar(index, bar_values2, bar_width, color='green', label='Bar Data 2')\\n\\nax4 = ax2.twinx()\\n\\nline2 = ax4.plot(index, line_values2, color='purple', marker='s', label='Line Data 2', linestyle='-', linewidth=2)\\n\\n\\n# Customize the second subplot\\n\\nax2.set_xlabel('Categories', fontsize=12)\\n\\nax2.set_ylabel('Bar Values', fontsize=12, color='green')\\n\\nax4.set_ylabel('Line Values', fontsize=12, color='purple')\\n\\nax2.set_title('Bar and Line Plot (Right)', fontsize=16)\\n\\nax2.set_xticks(index)\\n\\nax2.set_xticklabels(categories, fontsize=10)\\n\\nax2.grid(True, which='major', axis='y', linestyle='--', linewidth=0.7)\\n\\nax4.grid(True, which='major', axis='y', linestyle=':', linewidth=0.5)\\n\\n\\n# Add legends for the second subplot\\n\\nbars2_legend = ax2.legend(loc='upper left')\\n\\nline2_legend = ax4.legend(loc='upper right')\\n\\n\\n# Adjust layout and display the plot\\n\\nplt.tight_layout()\\n\\nplt.show()\\n\\n\"\n    }\n],\n\n\"radar\": [\n    {\n    \"\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\n\\n# Updated data related to the environmental sector\\n\\ndata_labels = ['Air Quality', 'Water Quality', 'Biodiversity', 'Waste Management']\\n\\nline_labels = ['Urban Areas', 'Rural Areas', 'Industrial Areas', 'Protected Areas', 'Residential Areas']\\n\\ndata = np.array([[65, 70, 75, 80], [80, 78, 82, 85], [55, 60, 65, 68], [75, 73, 70, 72], [60, 58, 62, 65]])\\n\\n\\n# Plot\\n\\nfig = plt.figure()\\n\\nax = fig.add_subplot(111, polar=True)\\n\\nangles = np.linspace(0, 2*np.pi, len(data_labels)+1, endpoint=True)\\n\\ndata = np.concatenate((data, data[:, 0:1]), axis=1)\\n\\nax.set_thetagrids(angles[:-1] * 180/np.pi, data_labels)\\n\\nax.set_ylim(0, 100)\\n\\n\\nfor i, line in enumerate(data):\\n\\n    ax.plot(angles, line, linewidth=1.5, label=line_labels[i])\\n\\n    ax.fill(angles, line, alpha=0.2)\\n\\n\\n# Customize the plot\\n\\nhandles, labels = ax.get_legend_handles_labels()\\n\\nax.legend(handles, labels, loc=(0.9, 0.95), fontsize=10)\\n\\nax.set_title('Environmental Performance - 2023', va='bottom', fontsize=15)\\n\\n\\n# Save the figure with the new filename\\n\\nplt.tight_layout()\\n\\nplt.savefig('/cpfs01/user/environment_performance_2023.png')\\n\\nplt.clf()\\n\\n\"\n    },\n    {\n    \"\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\n\\n# Random data for healthcare performance metrics\\n\\ndata = np.array([[85, 80, 90, 88, 82],\\n                 [78, 75, 85, 80, 75],\\n                 [90, 92, 88, 85, 87],\\n                 [80, 78, 85, 83, 79],\\n                 [75, 80, 85, 82, 77],\\n                 [88, 85, 90, 89, 84]])\\n\\n\\ndata_labels = [\\\"Patient Satisfaction (Score)\\\", \\\"Treatment Success (Score)\\\", \\\"Efficiency (Score)\\\",\\n               \\\"Accessibility (Score)\\\", \\\"Innovation (Score)\\\"]\\n\\nline_labels = [\\\"General Care\\\", \\\"Surgery\\\", \\\"Pharmaceuticals\\\", \\\"Pediatrics\\\", \\\"Mental Health\\\", \\\"Diagnostics\\\"]\\n\\n\\nfig = plt.figure(figsize=(10, 10))\\n\\nax = fig.add_subplot(111, polar=True)\\n\\n\\nangles = np.linspace(0, 2 * np.pi, len(data_labels) + 1, endpoint=True)\\n\\ndata = np.concatenate((data, data[:, 0:1]), axis=1)\\n\\n\\n# Colors for the lines\\n\\ncolors = ['#FF6347', '#4682B4', '#32CD32', '#FFD700', '#8A2BE2', '#FF69B4']\\n\\nfor i in range(len(data)):\\n\\n    ax.plot(angles, data[i], color=colors[i], label=line_labels[i])\\n\\n    ax.fill(angles, data[i], color=colors[i], alpha=0.25)\\n\\n\\n# Customizing the ticks and grid\\n\\nax.set_xticks(angles[:-1])\\n\\nax.set_xticklabels(data_labels)\\n\\nax.set_yticklabels([])\\n\\n\\n# Adding the y-axis grid and labels\\n\\nmax_value = np.amax(data)\\n\\nstep_size = max_value / 5\\n\\nax.set_rgrids([step_size * i for i in range(1, 6)], labels=[f'{step_size * i:.1f}' for i in range(1, 6)], angle=0)\\n\\n\\n# Setting the title\\n\\nax.set_title(\\\"Healthcare Performance Metrics - 2023\\\", fontsize=16)\\n\\n\\n# Add legend\\n\\nhandles, labels = ax.get_legend_handles_labels()\\n\\nax.legend(handles, labels, loc='upper right')\\n\\n\\n# Saving and displaying the plot\\n\\nplt.tight_layout()\\n\\nplt.savefig('healthcare_performance_2023.png')\\n\\nplt.show()\\n\\n\"\n    },\n    {\n    \"\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\n\\n# Updated data for the education sector\\n\\ndata_labels = ['Student Engagement', 'Teaching Quality', 'Infrastructure', 'Extracurricular Activities', 'Funding']\\n\\nline_labels = ['School A', 'School B', 'School C', 'School D']\\n\\ndata = np.array([[88, 92, 85, 87], [82, 85, 78, 80], [78, 75, 80, 82], [85, 88, 82, 84], [90, 92, 88, 89]])\\n\\n\\n# Create figure before plotting\\n\\nfig = plt.figure(figsize=(10, 10))\\n\\nax = fig.add_subplot(111, polar=True)\\n\\n\\n# Evenly space the axes for the number of data points\\n\\nangles = np.linspace(0, 2 * np.pi, len(data_labels) + 1, endpoint=True)\\n\\n\\n# Iterate over each row in the data array, append the first numerical element of that row to its end for close-loop plotting of data lines\\n\\nfor i in range(len(line_labels)):\\n\\n    data_row = np.append(data[:, i], data[0, i])\\n\\n    ax.plot(angles, data_row, linewidth=2, label=line_labels[i])\\n\\n    ax.fill(angles, data_row, alpha=0.25)\\n\\n\\n    # Generate a angle-like radius vector with constant values\\n\\n    angle_r = np.full_like(angles, (i+1) * np.max(data) / len(line_labels))\\n\\n    ax.plot(angles, angle_r, '--', alpha=0.25, color='black')\\n\\n\\n# Plot the axis label by using set_thetagrids\\n\\nax.set_thetagrids(angles[:-1] * 180/np.pi, data_labels, fontsize=14)\\n\\n\\n# Adjust the radial limits to accommodate the maximum of data\\n\\nax.set_ylim(0, np.max(data))\\n\\n\\n# Plot the data lines legend\\n\\nhandles, labels = ax.get_legend_handles_labels()\\n\\nax.legend(handles, labels, loc=(0.9, 0.95), fontsize=14)\\n\\n\\n# Remove the circular gridlines and background\\n\\nax.yaxis.grid(False)\\n\\nax.spines['polar'].set_visible(False)\\n\\n\\n# Set the title of the figure\\n\\nplt.title('Education Performance Evaluation', fontsize=18)\\n\\n\\n# Automatically resize the image by tight_layout\\n\\nplt.tight_layout()\\n\\n\\n# Save the image with the new filename\\n\\nplt.savefig('education_performance_2023.png')\\n\\n\\n# Clear the current image state\\n\\nplt.clf()\\n\"\n    }\n],\n\n\"ring\": [\n    {\n    \"\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\n\\n# Updated random data for the healthcare sector\\n\\ndata_labels = ['Patient Satisfaction', 'Treatment Time', 'Resource Usage', 'Cost Efficiency', 'Success Rate']\\n\\nline_labels = ['Metric', 'ratio']\\n\\ndata = np.array([[25, 20, 15, 30, 10]])\\n\\n\\n# Create the pie chart\\n\\nfig, ax = plt.subplots(figsize=(8, 8))\\n\\nax.pie(data.flatten(), labels=data_labels, startangle=90, counterclock=False, colors=['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#c2c2f0'])\\n\\n\\n# Add a white circle in the center to create a donut chart effect\\n\\ncircle = plt.Circle((0, 0), 0.6, color='white')\\n\\nax.add_artist(circle)\\n\\n\\n# Add legend and title\\n\\nax.legend(data_labels, loc='upper left')\\n\\nax.set_title('Healthcare Performance Overview - 2023')\\n\\n\\n# Save and display the plot\\n\\nplt.tight_layout()\\n\\nplt.savefig('healthcare_performance_overview_2023.png')\\n\\nplt.show()\\n\\n\"\n    },\n    {\n    \"import matplotlib.pyplot as plt\\nlabels = ['CPU Utilization', 'Memory Usage', 'Disk I/O', 'Network Latency']\\nmodel_a_data = [30, 40, 20, 10]\\nmodel_b_data = [25, 35, 30, 10]\\ncolors = ['#ff7f7f', '#4682b4', '#32cd32', '#deb887']\\ntitles = ['Model A Performance', 'Model B Performance']\\nfig, ax = plt.subplots(1, 2, figsize=(12, 6))\\nax[0].pie(model_a_data, labels=labels, colors=colors, startangle=90, counterclock=False, autopct='%1.1f%%', wedgeprops=dict(width=0.3))\\nax[0].set_title(titles[0])\\nax[1].pie(model_b_data, labels=labels, colors=colors, startangle=90, counterclock=False, autopct='%1.1f%%', wedgeprops=dict(width=0.3))\\nax[1].set_title(titles[1])\\nplt.tight_layout()\\nplt.savefig('computer_model_performance_pie_chart.png', bbox_inches='tight')\\nplt.show()\"\n    },\n    {\n    \"import matplotlib.pyplot as plt\\nimport matplotlib as mpl\\nimport numpy as np\\n\\n# Data for Environmental Protection Measures\\ndata_labels = ['Air Pollution Control', 'Water Conservation', 'Waste Management', 'Forest Preservation', 'Renewable Energy']\\ndata = [20, 25, 15, 25, 15]\\nline_labels = ['Category', 'Ratio']\\n\\n# Create figure and axis\\nfig = plt.figure(figsize=(12, 9))\\nax = fig.add_subplot(111)\\n\\n# New color scheme (using 'viridis' colormap for a modern look)\\ncolors = plt.cm.viridis(np.linspace(0, 1, len(data_labels)))\\n\\n# Explode the slices (only highlight the second and fourth slice)\\nexplode = [0, 0.1, 0, 0.1, 0]\\n\\n# Create a pie chart with updated colors and explode effect\\nax.pie(data, labels=data_labels, explode=explode, colors=colors, autopct='%1.1f%%',\\n       textprops={'fontsize': 14}, startangle=140, counterclock=False)\\n\\n# Adding a white circle in the center for a \\\"donut\\\" effect\\ncentre_circle = plt.Circle((0, 0), 0.65, fc='white')\\nax.add_artist(centre_circle)\\n\\n# Add a legend in the upper left corner\\nax.legend(data_labels, loc='upper left', bbox_to_anchor=(-0.15, 1.05), fontsize=12)\\n\\n# Set title for the chart\\nax.set_title('Environmental Protection Measures - 2023', fontsize=18)\\n\\n# Make layout tight to avoid overlap\\nplt.tight_layout()\\n\\n# Save the figure to a specific path\\nplt.savefig('environmental_protection_ring_chart.png')\\n\\n# Clear the figure to free memory\\nplt.clf()\"\n    }\n],\n\n\"rose\":[\n    {\n    \"import matplotlib.pyplot as plt\\nimport numpy as np\\n\\n# Data for Renewable Energy Production\\ndata_labels = ['Solar', 'Wind', 'Hydropower', 'Geothermal', 'Biomass', 'Tidal', 'Nuclear', 'Wave']\\ndata = [85, 70, 90, 65, 60, 40, 75, 30]\\n\\n# Create the figure\\nfig = plt.figure(figsize=(10, 10))\\nax = fig.add_subplot(1, 1, 1, projection='polar')\\n\\n# Calculate the angle for each sector\\nsector_angle = (2 * np.pi) / len(data_labels)\\n\\n# Plot each bar in the polar chart\\nfor i, datum in enumerate(data):\\n    ax.bar(sector_angle * i, datum, width=sector_angle, alpha=0.7, label=data_labels[i], color=plt.cm.Paired(i))\\n\\n# Set the tick marks and labels\\nax.set_xticks(np.arange(0, 2 * np.pi, sector_angle))\\nax.set_xticklabels(data_labels, fontsize=12)\\n\\n# Adjust label positioning and alignment based on their angle\\nfor label, angle in zip(ax.get_xticklabels(), np.arange(0, 2 * np.pi, sector_angle)):\\n    if 0 <= angle < np.pi / 2 or 3 * np.pi / 2 <= angle <= 2 * np.pi:\\n        label.set_horizontalalignment('left')  # Right side of the plot\\n    else:\\n        label.set_horizontalalignment('right')  # Left side of the plot\\n\\n# Adjust the legend position and avoid label overlap\\nax.legend(bbox_to_anchor=(1.1, 1.05), loc='upper left', fontsize=10)\\n\\n# Set chart title with larger font\\nax.set_title('Global Renewable Energy Production in 2023', fontsize=16, pad=20)\\n\\n# Adjust layout to prevent overlap\\nplt.tight_layout()\\n\\n# Save the figure with a new file name\\nplt.savefig('/mnt/data/global_renewable_energy_polar_chart_aligned.png')\\n\\n# Clear the plot to free memory\\nplt.clf()\"\n    },\n],\n\n\"treemap\":[\n    {\n    \"import plotly.express as px\\nimport os\\nimport pandas as pd\\n\\n# New data for a different professional field (e.g., Research Funding Allocation in Energy Sectors)\\ndata_labels = ['Energy Research Funding Allocation (%)']\\nline_labels = ['Solar Energy', 'Wind Energy', 'Hydropower', 'Geothermal Energy', \\n               'Biomass Energy', 'Nuclear Energy', 'Hydrogen', 'Energy Storage']\\ndata = [25, 20, 15, 10, 10, 8, 7, 5]\\n\\n# Create a DataFrame for Plotly\\ndf = pd.DataFrame(list(zip(line_labels, data)), columns=['Energy Sector', 'Funding Allocation'])\\n\\n# Create treemap using Plotly Express\\nfig = px.treemap(df, path=['Energy Sector'], values='Funding Allocation',\\n                 title='Research Funding Allocation Across Various Energy Sectors in 2023',\\n                 color='Funding Allocation', color_continuous_scale='Viridis')\\n\\n# Customize the layout\\nfig.update_layout(margin=dict(t=50, l=25, r=25, b=25))\\n\\n# Define the save path for the figure\\nsave_path = 'energy_funding_treemap.png'\\n\\n# Save the figure\\nfig.write_image(save_path)\\n\"\n    },\n    {\n    \"import matplotlib.pyplot as plt\\nimport squarify\\nimport os\\n\\n# New data for a different professional field (e.g., Global Market Share in Renewable Energy)\\ndata_labels = ['Solar Power', 'Wind Power', 'Hydropower', 'Geothermal Energy', 'Biomass', 'Tidal Energy', 'Hydrogen', 'Nuclear Energy']\\nline_labels = ['Market Share (%)']\\ndata = [35, 25, 15, 10, 5, 5, 3, 2]\\n\\n# Define a new color palette with better contrast (Set3)\\ncolors = plt.cm.Set3.colors[:len(data_labels)]\\n\\n# Create a larger figure to prevent overlap and enhance readability\\nplt.figure(figsize=(12, 8))\\n\\n# Create a treemap with the new data and color scheme\\nsquarify.plot(sizes=data, label=data_labels, color=colors, alpha=0.7, text_kwargs={'fontsize':12})\\n\\n# Add a title with updated font size and style\\nplt.title('Global Market Share in Renewable Energy Sectors (2023)', fontsize=16, pad=20)\\n\\n# Remove the axes for a cleaner look\\nplt.axis('off')\\n\\n# Adjust layout to prevent content from getting cut off\\nplt.tight_layout()\\n\\n# Define the save path for the figure\\nsave_path = 'renewable_energy_treemap_set.png'\\n\\n# Save the figure\\nplt.savefig(save_path, dpi=300)\\n\\n# Clear the current image state to keep the environment clean\\nplt.clf()\\n\"\n    }\n],\n}\n\ncode_none = {\n    \"Hide axis\":\n    \"\"\"\nax.axis('off')\n\"\"\"\n}\n\npackages = {\n    \"matplotlib\": \"matplotlib\",\n    \"none\": \"whatever you want\"\n}\n\ndef disk_data_prompt(domain):\n    prompt = f\"Generate data related to {domain}, pay attention to requirements above and below:\\n\"\n    requirements = (\n        \"Requirements:\\n\"\n        \"The data generated should in a following format. Here is an example: Label,item,item,item\\nlabel,1000,4000,10000\\nlabel,1200,4500,11000\\nlabel,1500,4800,12000\\nlabel,1800,5000,13000\\n\"\n        \"The imitation is as irrelevant as possible to the example. For example, you should generated data from all kinds of different fields and modify the labels and items with corresponding components. The data must have at least but not limited to 1 rows and 1 columns, 3, 4, 5 are all okay\\n\"\n        \"Generate only one set of json data.\\n\"\n        'Don\\'t generate any other content except {\"csv_data\": \"...\"}\\n'\n    )\n    return prompt + requirements\n\n\ndef disk_caption_prompt(chart_type):\n    prompt = f\"Generate caption for the {chart_type} chart data.\\n\"\n    requirements = (\n        \"Requirements:\\n\"\n        \"The caption should be brief and concise. It's best to keep it under five words\\n\"\n        \"The caption should describe the general content of the line chart data.\\n\"\n        'Don\\'t generate any other content except {\"caption\": \"...\" }'\n    )\n    return prompt + requirements\n\n\ndef disk_code_prompt(csv_data, title, additional_format_spec, chart_type):\n    # color control\n    random.shuffle(colors)\n    prompt = f\"Generate high quality python code for plotting {chart_type} chart.\\n\"\n\n    # external = random.choice([\"pyecharts\", \"matplotlib\"])\n    # external = random.choice([\"pyecharts\", \"none\"])\n    external = random.choice([\"matplotlib\"])\n    # chart_type = random.choice(chart_types)\n    code_matplotlib = random.choice(chart_examples[chart_type])\n    if external == \"matplotlib\":\n        code_examples = code_matplotlib\n        spec_req = {\n            \"figure size\": \"no larger than 1200 * 1200 pixels\",\n            \"font size\": \"use medium fonts (for scale labels and title)\",\n        }\n    else:\n        code_examples = code_none\n        spec_req = {\n            \"font size\": \"use large fonts (for scale labels and title)\",\n        }\n\n    # general requirements\n    general_spec = {\n        \"The generated code should not too complicated and the words cannot overlap each other\"\n        \"title\": f\"{title}\",\n        \"palette\": f\"{colors}\",\n        \"grid visibility\": f\"{grid_visibility}\",\n        \"grid line style\":  f\"{grid_line_styles}\",\n        \"line style\":  f\"{line_styles}\",\n        \"marker style\": f\"{marker_styles}\",\n        \"bar styles\": f\"{bar_styles}\",\n        \"arrangement\":  f\"{bar_arrangement}\",\n        \"font type\": f\"{font_types}\",\n        \"lengend postion\": f\"{legend_positions}\",\n        \"tick label styles\": f\"{tick_label_styles}\",\n        \"error filling symbol\": \"plt.fill_between()\",\n        \"components\": \"x-axis, y-axis, bars, label, title, legends, line style, marker style, grid visibility, bar styles, arrangement, font type, palette, gridlines(optional), axis titles, ticks. No overlap between all components\",\n        \"color\": \"the color used in the palette should be recognizable, so it's a little bit darker\",\n        \"modification\": \"the components in the sample code chould be replace, usually use the solid line but not limited by items in palette, bar style, arrangement, font type, lengend postion, tick label styles, line style, marker style, grid visibility\", \n        \"IMPORTANT NOTE\": \"further modification should be made in the number of the subfigures (choose from {1, 2, 3}), do not always generate code for one subfigures, generate two subfigures horizontally or vertically\",\n        \"bar/pie chart visualization\": \"to better show the bar/pie chart, the bar/pie filling symbol and tick label styles coule be further modified, but in most cases, you do not need to modify these parameters\",\n        \"line chart visualization\": \"to better show the line chart, the line could be filled using error filling symbol for further modification, the error could be generated directly without data, but in most cases, you do not need to modify these parameters\",\n        \"font family\": f\"replace the font type in the sample code with {random.choice(font_types)}\",\n    }\n\n    requirements = (\n        \"Requirements:\\n\"\n        f\"The code must present data: {csv_data} in a reasonable way\"\n        f\"The code should use packages {packages[external]}.\\n\"\n        f\"The code example (given in JSON format) is {code_examples}.\\n\"\n        \"You must not be limited by the code sample and draw different styles of dials.\\n\"\n        f\"The code must conform general requirements (given in JSON format):\\n{json.dumps(general_spec, indent=2)}.\\n\"\n        f\"The code must conform specific requirements (given in JSON format):\\n{json.dumps(spec_req, indent=2)}.\\n\"\n    )\n    if additional_format_spec is not None:\n        requirements += f\"The code must conform additional requirements (given in JSON format):\\n{json.dumps(additional_format_spec, indent=2)}\\n\"\n\n    requirements += \"Output format: ```python ... ```\\n\"\n\n    return prompt + requirements\n\n"}
{"type": "source_file", "path": "llava/model/builder.py", "content": "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nimport os\nimport warnings\nimport shutil\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\nimport torch\nfrom llava.model import *\nfrom llava.constants import DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\nfrom llava.utils import rank0_print\n\n\ndef load_pretrained_model(model_path, model_base, model_name, load_8bit=False, load_4bit=False, device_map=\"auto\", attn_implementation=\"flash_attention_2\", customized_config=None, overwrite_config=None, **kwargs):\n    kwargs[\"device_map\"] = device_map\n\n    if load_8bit:\n        kwargs[\"load_in_8bit\"] = True\n    elif load_4bit:\n        kwargs[\"load_in_4bit\"] = True\n        kwargs[\"quantization_config\"] = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\")\n    else:\n        kwargs[\"torch_dtype\"] = torch.float16\n\n    if customized_config is not None:\n        kwargs[\"config\"] = customized_config\n\n    if \"multimodal\" in kwargs:\n        if kwargs[\"multimodal\"] is True:\n            is_multimodal = True\n            kwargs.pop(\"multimodal\")\n    else:\n        is_multimodal = False\n\n    if \"llava\" in model_name.lower() or is_multimodal:\n        # Load LLaVA model\n        if \"lora\" in model_name.lower() and model_base is None:\n            warnings.warn(\n                \"There is `lora` in model name but no `model_base` is provided. If you are loading a LoRA model, please provide the `model_base` argument. Detailed instruction: https://github.com/haotian-liu/LLaVA#launch-a-model-worker-lora-weights-unmerged.\"\n            )\n        if \"lora\" in model_name.lower() and model_base is not None:\n            lora_cfg_pretrained = AutoConfig.from_pretrained(model_path)\n            tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n            rank0_print(\"Loading LLaVA from base model...\")\n            if \"mixtral\" in model_name.lower():\n                from llava.model.language_model.llava_mixtral import LlavaMixtralConfig\n\n                lora_cfg_pretrained = LlavaMixtralConfig.from_pretrained(model_path)\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                model = LlavaMixtralForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n            elif \"mistral\" in model_name.lower():\n                from llava.model.language_model.llava_mistral import LlavaMistralConfig\n\n                lora_cfg_pretrained = LlavaMistralConfig.from_pretrained(model_path)\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                model = LlavaMistralForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n            # elif \"gemma\" in model_name.lower():\n            #     from llava.model.language_model.llava_gemma import LlavaGemmaConfig\n\n            #     lora_cfg_pretrained = LlavaGemmaConfig.from_pretrained(model_path)\n            #     tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n            #     model = LlavaGemmaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n            elif \"deepseekcoder\" in model_name.lower():\n                from llava.model.language_model.llava_deepseekcoder import LlavaDeepSeekConfig\n                lora_cfg_pretrained = LlavaDeepSeekConfig.from_pretrained(model_path)\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                model = LlavaDeepSeekForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n            else:\n                from llava.model.language_model.llava_llama import LlavaConfig\n\n                lora_cfg_pretrained = LlavaConfig.from_pretrained(model_path)\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n\n            token_num, tokem_dim = model.lm_head.out_features, model.lm_head.in_features\n            if model.lm_head.weight.shape[0] != token_num:\n                model.lm_head.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\n                model.model.embed_tokens.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\n\n            rank0_print(\"Loading additional LLaVA weights...\")\n            if os.path.exists(os.path.join(model_path, \"non_lora_trainables.bin\")):\n                non_lora_trainables = torch.load(os.path.join(model_path, \"non_lora_trainables.bin\"), map_location=\"cpu\")\n            else:\n                # this is probably from HF Hub\n                from huggingface_hub import hf_hub_download\n\n                def load_from_hf(repo_id, filename, subfolder=None):\n                    cache_file = hf_hub_download(repo_id=repo_id, filename=filename, subfolder=subfolder)\n                    return torch.load(cache_file, map_location=\"cpu\")\n\n                non_lora_trainables = load_from_hf(model_path, \"non_lora_trainables.bin\")\n            non_lora_trainables = {(k[11:] if k.startswith(\"base_model.\") else k): v for k, v in non_lora_trainables.items()}\n            if any(k.startswith(\"model.model.\") for k in non_lora_trainables):\n                non_lora_trainables = {(k[6:] if k.startswith(\"model.\") else k): v for k, v in non_lora_trainables.items()}\n            model.load_state_dict(non_lora_trainables, strict=False)\n\n            from peft import PeftModel\n\n            rank0_print(\"Loading LoRA weights...\")\n            model = PeftModel.from_pretrained(model, model_path)\n            rank0_print(\"Merging LoRA weights...\")\n            model = model.merge_and_unload()\n            rank0_print(\"Model is loaded...\")\n        elif model_base is not None:  # this may be mm projector only, loading projector with preset language mdoel\n            rank0_print(f\"Loading LLaVA from base model {model_base}...\")\n            if \"mixtral\" in model_name.lower():\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                cfg_pretrained = AutoConfig.from_pretrained(model_path)\n                model = LlavaMixtralForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n            elif \"mistral\" in model_name.lower() or \"zephyr\" in model_name.lower():\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                cfg_pretrained = AutoConfig.from_pretrained(model_path)\n                model = LlavaMistralForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n            # elif \"gemma\" in model_name.lower():\n            #     tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n            #     cfg_pretrained = AutoConfig.from_pretrained(model_path)\n            #     model = LlavaGemmaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n            elif \"deepseekcoder\" in model_name.lower():\n                from llava.model.language_model.llava_deepseekcoder import LlavaDeepSeekConfig\n                cfg_pretrained = LlavaDeepSeekConfig.from_pretrained(model_path)\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                model = LlavaDeepSeekForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n            elif (\n                \"wizardlm-2\" in model_name.lower()\n                and \"vicuna\" in model_name.lower()\n                or \"llama\" in model_name.lower()\n                or \"yi\" in model_name.lower()\n                or \"nous-hermes\" in model_name.lower()\n                or \"llava-v1.6-34b\" in model_name.lower()\n                or \"llava-v1.5\" in model_name.lower()\n            ):\n                from llava.model.language_model.llava_llama import LlavaConfig\n\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n                if customized_config is None:\n                    llava_cfg = LlavaConfig.from_pretrained(model_path)\n                    if \"v1.5\" in model_name.lower():\n                        llava_cfg.delay_load = True  # a workaround for correctly loading v1.5 models\n                else:\n                    llava_cfg = customized_config\n\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                llava_cfg = LlavaConfig.from_pretrained(model_path)\n                model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=llava_cfg, **kwargs)\n            else:\n                raise ValueError(f\"Model {model_name} not supported\")\n\n            if os.path.exists(os.path.join(model_path, \"mm_projector.bin\")):\n                mm_projector_weights = torch.load(os.path.join(model_path, \"mm_projector.bin\"), map_location=\"cpu\")\n                mm_projector_weights = {k: v.to(torch.float16) for k, v in mm_projector_weights.items()}\n                model.load_state_dict(mm_projector_weights, strict=False)\n        else:\n            rank0_print(f\"Loaded LLaVA model: {model_path}\")\n            if \"mixtral\" in model_name.lower():\n                from llava.model.language_model.llava_mixtral import LlavaMixtralConfig\n\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n                if customized_config is None:\n                    llava_cfg = LlavaMixtralConfig.from_pretrained(model_path)\n                else:\n                    llava_cfg = customized_config\n\n                if overwrite_config is not None:\n                    rank0_print(f\"Overwriting config with {overwrite_config}\")\n                    for k, v in overwrite_config.items():\n                        setattr(llava_cfg, k, v)\n\n                tokenizer = AutoTokenizer.from_pretrained(model_path)\n                model = LlavaMixtralForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, config=llava_cfg, **kwargs)\n\n            elif \"mistral\" in model_name.lower() or \"zephyr\" in model_name.lower():\n                tokenizer = AutoTokenizer.from_pretrained(model_path)\n                model = LlavaMistralForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, **kwargs)\n            elif (\n                \"wizardlm-2\" in model_name.lower()\n                and \"vicuna\" in model_name.lower()\n                or \"llama\" in model_name.lower()\n                or \"yi\" in model_name.lower()\n                or \"nous-hermes\" in model_name.lower()\n                or \"llava-v1.6-34b\" in model_name.lower()\n                or \"llava-v1.5\" in model_name.lower()\n            ):\n                from llava.model.language_model.llava_llama import LlavaConfig\n\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n                if customized_config is None:\n                    llava_cfg = LlavaConfig.from_pretrained(model_path)\n                    if \"v1.5\" in model_name.lower():\n                        llava_cfg.delay_load = True  # a workaround for correctly loading v1.5 models\n                else:\n                    llava_cfg = customized_config\n\n                if overwrite_config is not None:\n                    rank0_print(f\"Overwriting config with {overwrite_config}\")\n                    for k, v in overwrite_config.items():\n                        setattr(llava_cfg, k, v)\n\n                model = LlavaLlamaForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, config=llava_cfg, **kwargs)\n\n            elif \"qwen\" in model_name.lower() or \"quyen\" in model_name.lower():\n                tokenizer = AutoTokenizer.from_pretrained(model_path)\n                # if \"moe\" in model_name.lower() or \"A14B\" in model_name.lower():\n                #     from llava.model.language_model.llava_qwen_moe import LlavaQwenMoeConfig\n                #     if overwrite_config is not None:\n                #         llava_cfg = LlavaQwenMoeConfig.from_pretrained(model_path)\n                #         rank0_print(f\"Overwriting config with {overwrite_config}\")\n                #         for k, v in overwrite_config.items():\n                #             setattr(llava_cfg, k, v)\n                #         model = LlavaQwenMoeForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, config=llava_cfg, **kwargs)\n                #     else:\n                #         model = LlavaQwenMoeForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, **kwargs)\n\n                # else:\n                from llava.model.language_model.llava_qwen import LlavaQwenConfig\n                if overwrite_config is not None:\n                    llava_cfg = LlavaQwenConfig.from_pretrained(model_path)\n                    rank0_print(f\"Overwriting config with {overwrite_config}\")\n                    for k, v in overwrite_config.items():\n                        setattr(llava_cfg, k, v)\n                    model = LlavaQwenForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, config=llava_cfg, **kwargs)\n                else:\n                    model = LlavaQwenForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, **kwargs)\n\n            # elif \"gemma\" in model_name.lower():\n            #     tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n            #     cfg_pretrained = AutoConfig.from_pretrained(model_path)\n            #     model = LlavaGemmaForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, config=cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n            elif \"deepseekcoder\" in model_name.lower():\n                tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, use_fast=False)\n                cfg_pretrained = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n                model = LlavaDeepSeekForCausalLM.from_pretrained(model_path, trust_remote_code=True, low_cpu_mem_usage=True, config=cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n            else:\n                try:\n                    from llava.model.language_model.llava_llama import LlavaConfig\n\n                    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n                    if customized_config is None:\n                        llava_cfg = LlavaConfig.from_pretrained(model_path)\n                        if \"v1.5\" in model_path.lower():\n                            llava_cfg.delay_load = True  # a workaround for correctly loading v1.5 models\n                    else:\n                        llava_cfg = customized_config\n\n                    if overwrite_config is not None:\n                        rank0_print(f\"Overwriting config with {overwrite_config}\")\n                        for k, v in overwrite_config.items():\n                            setattr(llava_cfg, k, v)\n                    model = LlavaLlamaForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, config=llava_cfg, **kwargs)\n                except:\n                    raise ValueError(f\"Model {model_name} not supported\")\n\n    else:\n        # Load language model\n        if model_base is not None:\n            # PEFT model\n            from peft import PeftModel\n\n            tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n            model = AutoModelForCausalLM.from_pretrained(model_base, torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map=\"auto\")\n            print(f\"Loading LoRA weights from {model_path}\")\n            model = PeftModel.from_pretrained(model, model_path)\n            print(f\"Merging weights\")\n            model = model.merge_and_unload()\n            print(\"Convert to FP16...\")\n            model.to(torch.float16)\n        else:\n            use_fast = False\n            if \"mpt\" in model_name.lower().replace(\"prompt\", \"\"):\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n                model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, trust_remote_code=True, **kwargs)\n            else:\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n                model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\n\n    rank0_print(f\"Model Class: {model.__class__.__name__}\")\n    image_processor = None\n\n    if \"llava\" in model_name.lower() or is_multimodal:\n        mm_use_im_start_end = getattr(model.config, \"mm_use_im_start_end\", False)\n        mm_use_im_patch_token = getattr(model.config, \"mm_use_im_patch_token\", True)\n        if mm_use_im_patch_token:\n            tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\n        if mm_use_im_start_end:\n            tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\n        model.resize_token_embeddings(len(tokenizer))\n\n        vision_tower = model.get_vision_tower()\n        if not vision_tower.is_loaded:\n            vision_tower.load_model(device_map=device_map)\n        if device_map != \"auto\":\n            vision_tower.to(device=\"cuda\", dtype=torch.float16)\n        image_processor = vision_tower.image_processor\n\n    if hasattr(model.config, \"max_sequence_length\"):\n        context_len = model.config.max_sequence_length\n    elif hasattr(model.config, \"max_position_embeddings\"):\n        context_len = model.config.max_position_embeddings\n    elif hasattr(model.config, \"tokenizer_model_max_length\"):\n        context_len = model.config.tokenizer_model_max_length\n    else:\n        context_len = 2048\n\n    return tokenizer, model, image_processor, context_len\n"}
{"type": "source_file", "path": "llava/model/__init__.py", "content": "try:\n    from .language_model.llava_llama import LlavaLlamaForCausalLM, LlavaConfig\n    from .language_model.llava_deepseekcoder import LlavaDeepSeekForCausalLM, LlavaDeepSeekConfig\n    from .language_model.llava_mpt import LlavaMptForCausalLM, LlavaMptConfig\n    from .language_model.llava_mistral import LlavaMistralForCausalLM, LlavaMistralConfig\n    from .language_model.llava_mixtral import LlavaMixtralForCausalLM, LlavaMixtralConfig\n    from .language_model.llava_qwen import LlavaQwenForCausalLM, LlavaQwenConfig\nexcept:\n    pass\n\nimport os\n\nAVAILABLE_MODELS = {\n    \"llava_qwen\": \"LlavaQwenForCausalLM, LlavaQwenConfig\",\n    \"llava_mistral\": \"LlavaMistralForCausalLM, LlavaMistralConfig\",\n    \"llava_mixtral\": \"LlavaMixtralForCausalLM, LlavaMixtralConfig\",\n    # \"llava_qwen_moe\": \"LlavaQwenMoeForCausalLM, LlavaQwenMoeConfig\",    \n    # Add other models as needed\n}\n\nfor model_name, model_classes in AVAILABLE_MODELS.items():\n    try:\n        exec(f\"from .language_model.{model_name} import {model_classes}\")\n    except Exception as e:\n        print(f\"Failed to import {model_name} from llava.language_model.{model_name}. Error: {e}\")\n"}
{"type": "source_file", "path": "llava/model/apply_delta.py", "content": "\"\"\"\nUsage:\npython3 -m fastchat.model.apply_delta --base ~/model_weights/llama-7b --target ~/model_weights/vicuna-7b --delta lmsys/vicuna-7b-delta\n\"\"\"\n\nimport argparse\n\nimport torch\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom llava import LlavaLlamaForCausalLM\n\n\ndef apply_delta(base_model_path, target_model_path, delta_path):\n    print(\"Loading base model\")\n    base = AutoModelForCausalLM.from_pretrained(base_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n\n    print(\"Loading delta\")\n    delta = LlavaLlamaForCausalLM.from_pretrained(delta_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n    delta_tokenizer = AutoTokenizer.from_pretrained(delta_path)\n\n    print(\"Applying delta\")\n    for name, param in tqdm(delta.state_dict().items(), desc=\"Applying delta\"):\n        if name not in base.state_dict():\n            assert name in [\"model.mm_projector.weight\", \"model.mm_projector.bias\"], f\"{name} not in base model\"\n            continue\n        if param.data.shape == base.state_dict()[name].shape:\n            param.data += base.state_dict()[name]\n        else:\n            assert name in [\"model.embed_tokens.weight\", \"lm_head.weight\"], f\"{name} dimension mismatch: {param.data.shape} vs {base.state_dict()[name].shape}\"\n            bparam = base.state_dict()[name]\n            param.data[: bparam.shape[0], : bparam.shape[1]] += bparam\n\n    print(\"Saving target model\")\n    delta.save_pretrained(target_model_path)\n    delta_tokenizer.save_pretrained(target_model_path)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--base-model-path\", type=str, required=True)\n    parser.add_argument(\"--target-model-path\", type=str, required=True)\n    parser.add_argument(\"--delta-path\", type=str, required=True)\n\n    args = parser.parse_args()\n\n    apply_delta(args.base_model_path, args.target_model_path, args.delta_path)\n"}
{"type": "source_file", "path": "llava/model/language_model/llava_deepseekcoder.py", "content": "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\n\nfrom transformers import AutoConfig, AutoModelForCausalLM, \\\n                         LlamaConfig, LlamaModel, LlamaForCausalLM\n\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nfrom transformers.generation.utils import GenerateOutput\nfrom ..llava_arch import LlavaMetaModel, LlavaMetaForCausalLM\n\n\nclass LlavaDeepSeekConfig(LlamaConfig):\n    model_type = \"llava_deepseek\"\n\n\nclass LlavaDeepSeekModel(LlavaMetaModel, LlamaModel):\n    config_class = LlavaDeepSeekConfig\n\n    def __init__(self, config: LlamaConfig):\n        super(LlavaDeepSeekModel, self).__init__(config)\n\n\nclass LlavaDeepSeekForCausalLM(LlamaForCausalLM, LlavaMetaForCausalLM):\n    config_class = LlavaDeepSeekConfig\n\n    def __init__(self, config):\n        super(LlamaForCausalLM, self).__init__(config)\n        self.model = LlavaDeepSeekModel(config)\n        self.pretraining_tp = config.pretraining_tp\n        self.vocab_size = config.vocab_size\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.model\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        images: Optional[torch.FloatTensor] = None,\n        image_sizes: Optional[List[List[int]]] = None,\n        return_dict: Optional[bool] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        dpo_forward: Optional[bool] = None,\n        cache_position=None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n        \n\n        if inputs_embeds is None:\n            (input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels) = self.prepare_inputs_labels_for_multimodal(input_ids, position_ids, attention_mask, past_key_values, labels, images, modalities, image_sizes)\n\n        if dpo_forward:\n            outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n            hidden_states = outputs[0]\n            logits = self.lm_head(hidden_states)\n            return logits, labels\n\n        else:\n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                labels=labels,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs: Optional[torch.Tensor] = None,\n        images: Optional[torch.Tensor] = None,\n        image_sizes: Optional[torch.Tensor] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        **kwargs,\n    ) -> Union[GenerateOutput, torch.LongTensor]:\n        modalities = kwargs.pop(\"modalities\", None) if \"modalities\" in kwargs and modalities is None else modalities\n        position_ids = kwargs.pop(\"position_ids\", None)\n        attention_mask = kwargs.pop(\"attention_mask\", None)\n        if \"inputs_embeds\" in kwargs:\n            raise NotImplementedError(\"`inputs_embeds` is not supported\")\n\n        if images is not None:\n            (inputs, position_ids, attention_mask, _, inputs_embeds, _) = self.prepare_inputs_labels_for_multimodal(inputs, position_ids, attention_mask, None, None, images, modalities, image_sizes=image_sizes)\n        else:\n            inputs_embeds = self.get_model().embed_tokens(inputs)\n\n        return super().generate(position_ids=position_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, **kwargs)\n\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n        images = kwargs.pop(\"images\", None)\n        image_sizes = kwargs.pop(\"image_sizes\", None)\n        inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs)\n        if images is not None:\n            inputs[\"images\"] = images\n        if image_sizes is not None:\n            inputs[\"image_sizes\"] = image_sizes\n        return inputs\n\nAutoConfig.register(\"llava_deepseek\", LlavaDeepSeekConfig)\nAutoModelForCausalLM.register(LlavaDeepSeekConfig, LlavaDeepSeekForCausalLM)\n"}
{"type": "source_file", "path": "llava/conversation.py", "content": "import dataclasses\nfrom enum import auto, Enum\nfrom typing import List, Any, Dict, Union, Tuple\nimport re\nimport base64\nfrom io import BytesIO\nfrom PIL import Image\nfrom transformers import AutoTokenizer\n\n\nclass SeparatorStyle(Enum):\n    \"\"\"Different separator style.\"\"\"\n\n    SINGLE = auto()\n    TWO = auto()\n    MPT = auto()\n    PLAIN = auto()\n    CHATML = auto()\n    LLAMA_2 = auto()\n    LLAMA_3 = auto()\n    QWEN = auto()\n    GEMMA = auto()\n    DeepSeekCoder = auto()\n\n\n@dataclasses.dataclass\nclass Conversation:\n    \"\"\"A class that keeps all conversation history.\"\"\"\n\n    system: str\n    roles: List[str]\n    messages: List[List[str]]\n    offset: int\n    sep_style: SeparatorStyle = SeparatorStyle.SINGLE\n    sep: str = \"###\"\n    sep2: str = None\n    version: str = \"Unknown\"\n\n    tokenizer_id: str = \"\"\n    tokenizer: Any = None\n    # Stop criteria (the default one is EOS token)\n    stop_str: Union[str, List[str]] = None\n    # Stops generation if meeting any token in this list\n    stop_token_ids: List[int] = None\n\n    skip_next: bool = False\n\n    def get_prompt(self):\n        messages = self.messages\n        if len(messages) > 0 and type(messages[0][1]) is tuple:\n            messages = self.messages.copy()\n            init_role, init_msg = messages[0].copy()\n            init_msg = init_msg[0]\n            if \"mmtag\" in self.version:\n                init_msg = init_msg.replace(\"<image>\", \"\").strip()\n                messages[0] = (init_role, init_msg)\n                messages.insert(0, (self.roles[0], \"<Image><image></Image>\"))\n                messages.insert(1, (self.roles[1], \"Received.\"))\n            elif not init_msg.startswith(\"<image>\"):\n                init_msg = init_msg.replace(\"<image>\", \"\").strip()\n                messages[0] = (init_role, \"<image>\\n\" + init_msg)\n            else:\n                messages[0] = (init_role, init_msg)\n\n        if self.sep_style == SeparatorStyle.SINGLE:\n            ret = self.system + self.sep\n            for role, message in messages:\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    ret += role + \": \" + message + self.sep\n                else:\n                    ret += role + \":\"\n\n        elif self.sep_style == SeparatorStyle.TWO:\n            seps = [self.sep, self.sep2]\n            ret = self.system + seps[0]\n            for i, (role, message) in enumerate(messages):\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    ret += role + \": \" + message + seps[i % 2]\n                else:\n                    ret += role + \":\"\n\n        elif self.sep_style == SeparatorStyle.CHATML:\n            ret = \"\" if self.system == \"\" else self.system + self.sep + \"\\n\"\n            for role, message in messages:\n                if message:\n                    if type(message) is tuple:\n                        message, images, _ = message\n                        message = \"<image>\" * len(images) + message\n                    ret += role + \"\\n\" + message + self.sep + \"\\n\"\n                else:\n                    ret += role + \"\\n\"\n            return ret\n\n        elif self.sep_style == SeparatorStyle.LLAMA_3:\n            chat_template_messages = [{\"role\": \"system\", \"content\": self.system}]\n            for role, message in messages:\n                if message:\n                    if type(message) is tuple:\n                        message, images = message\n                        message = \"<image>\" * len(images) + message\n                    chat_template_messages.append({\"role\": role, \"content\": message})\n\n            # print(chat_template_messages)\n            return self.tokenizer.apply_chat_template(chat_template_messages, tokenize=False, add_generation_prompt=True)\n            # ret = \"\" if self.system == \"\" else self.system + self.sep + \"\\n\"\n            # for role, message in messages:\n            #     if message:\n            #         if type(message) is tuple:\n            #             message, images = message\n            #             message = \"<image>\" * len(images) + message\n            #         ret += role + \"\\n\" + message + self.sep + \"\\n\"\n            #     else:\n            #         ret += role + \"\\n\"\n            # return ret\n\n        elif self.sep_style == SeparatorStyle.MPT:\n            ret = self.system + self.sep\n            for role, message in messages:\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    ret += role + message + self.sep\n                else:\n                    ret += role\n\n        elif self.sep_style == SeparatorStyle.GEMMA:\n            ret = \"\"\n            for i, (role, message) in enumerate(messages):\n                assert role == self.roles[i % 2], \"Conversation should alternate user/assistant/user/assistant/...\"\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    ret += role + message + self.sep\n                else:\n                    ret += role\n\n        elif self.sep_style == SeparatorStyle.LLAMA_2:\n            wrap_sys = lambda msg: f\"<<SYS>>\\n{msg}\\n<</SYS>>\\n\\n\" if len(msg) > 0 else msg\n            wrap_inst = lambda msg: f\"[INST] {msg} [/INST]\"\n            ret = \"\"\n\n            for i, (role, message) in enumerate(messages):\n                if i == 0:\n                    assert message, \"first message should not be none\"\n                    assert role == self.roles[0], \"first message should come from user\"\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    if i == 0:\n                        message = wrap_sys(self.system) + message\n                    if i % 2 == 0:\n                        message = wrap_inst(message)\n                        ret += self.sep + message\n                    else:\n                        ret += \" \" + message + \" \" + self.sep2\n                else:\n                    ret += \"\"\n            ret = ret.lstrip(self.sep)\n\n        elif self.sep_style == SeparatorStyle.PLAIN:\n            seps = [self.sep, self.sep2]\n            ret = self.system\n            for i, (role, message) in enumerate(messages):\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    ret += message + seps[i % 2]\n                else:\n                    ret += \"\"\n\n        elif self.sep_style == SeparatorStyle.DeepSeekCoder:\n            wrap_user = lambda msg: f\"### Instruction:\\n{msg}\\n### Response:\\n\"\n            wrap_assis = lambda msg: f\"{msg}\\n<|EOT|>\"\n            ret = self.system\n            for i, (role, message) in enumerate(messages):\n                if i == 0:\n                    assert message, \"first message should not be none\"\n                    assert role == self.roles[0], \"first message should come from user\"\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    # if i == 0: message = wrap_sys(self.system) + message\n                    if i % 2 == 0:\n                        message = wrap_user(message) #user\n                        ret += message\n                    else:\n                        message = wrap_assis(message) #assistant\n                        ret += message\n        else:\n            raise ValueError(f\"Invalid style: {self.sep_style}\")\n\n        return ret\n\n    def append_message(self, role, message):\n        self.messages.append([role, message])\n\n    def process_image(self, image, image_process_mode, return_pil=False, image_format=\"PNG\"):\n        if image_process_mode == \"Pad\":\n\n            def expand2square(pil_img, background_color=(122, 116, 104)):\n                width, height = pil_img.size\n                if width == height:\n                    return pil_img\n                elif width > height:\n                    result = Image.new(pil_img.mode, (width, width), background_color)\n                    result.paste(pil_img, (0, (width - height) // 2))\n                    return result\n                else:\n                    result = Image.new(pil_img.mode, (height, height), background_color)\n                    result.paste(pil_img, ((height - width) // 2, 0))\n                    return result\n\n            image = expand2square(image)\n        elif image_process_mode in [\"Default\", \"Crop\"]:\n            pass\n        elif image_process_mode == \"Resize\":\n            image = image.resize((336, 336))\n        else:\n            raise ValueError(f\"Invalid image_process_mode: {image_process_mode}\")\n\n        if type(image) is not Image.Image:\n            image = Image.open(image).convert(\"RGB\")\n\n        max_hw, min_hw = max(image.size), min(image.size)\n        aspect_ratio = max_hw / min_hw\n        max_len, min_len = 672, 448\n        shortest_edge = int(min(max_len / aspect_ratio, min_len, min_hw))\n        longest_edge = int(shortest_edge * aspect_ratio)\n        W, H = image.size\n        if H > W:\n            H, W = longest_edge, shortest_edge\n        else:\n            H, W = shortest_edge, longest_edge\n        image = image.resize((W, H))\n        if return_pil:\n            return image\n        else:\n            buffered = BytesIO()\n            image.save(buffered, format=image_format)\n            img_b64_str = base64.b64encode(buffered.getvalue()).decode()\n            return img_b64_str\n\n    def get_images(self, return_pil=False, return_path=False):\n        images = []\n        for i, (role, msg) in enumerate(self.messages[self.offset :]):\n            if i % 2 == 0:\n                if type(msg) is tuple:\n                    msg, image, image_process_mode = msg\n                    if type(image) != list:\n                        image = [image]\n                    for img in image:\n                        if not return_path and self.is_image_file(img):\n                            img = self.process_image(img, image_process_mode, return_pil=return_pil)\n                        else:\n                            images.append(img)\n        return images\n\n    def is_image_file(self, filename):\n        image_extensions = [\".png\", \".jpg\", \".jpeg\", \".gif\", \".bmp\", \".tiff\", \".webp\"]\n        return any(filename.lower().endswith(ext) for ext in image_extensions)\n\n    def is_video_file(self, filename):\n        video_extensions = [\".mp4\", \".mov\", \".avi\", \".mkv\", \".wmv\", \".flv\", \".mpeg\", \".mpg\"]\n        return any(filename.lower().endswith(ext) for ext in video_extensions)\n\n    def to_gradio_chatbot(self):\n        ret = []\n        for i, (role, msg) in enumerate(self.messages[self.offset :]):\n            if i % 2 == 0:\n                if type(msg) is tuple:\n                    msg, image, image_process_mode = msg\n                    if type(image) != list:\n                        image = [image]\n                    if len(image) == 1:\n                        msg = \"<image>\\n\" + msg.replace(\"<image>\", \"\").strip()\n                    else:\n                        msg = re.sub(r\"(<image>)\\n(?=<image>)\", r\"\\1 \", msg)\n\n                    img_str_list = []                         \n                    for img in image:\n                        if self.is_image_file(img):\n                            img_b64_str = self.process_image(img, \"Default\", return_pil=False, image_format=\"JPEG\")\n                            img_str = f'<img src=\"data:image/jpeg;base64,{img_b64_str}\" style=\"max-width: 256px; max-height: 256px; width: auto; height: auto; object-fit: contain;\"/>'\n                            img_str_list.append(img_str)\n                        elif self.is_video_file(img):\n                            ret.append(((img,), None))\n\n                    msg = msg.strip()\n                    img_place_holder = \"\"\n                    for img_str in img_str_list:\n                        img_place_holder += f\"{img_str}\\n\\n\"\n\n                    if len(img_str_list) > 0:\n                        msg = f\"{img_place_holder}\\n\\n{msg}\"\n\n                    if len(msg) > 0:\n                        ret.append([msg, None])\n                else:\n                    ret.append([msg, None])\n            else:\n                ret[-1][-1] = msg\n        return ret\n\n    def copy(self):\n        return Conversation(system=self.system, roles=self.roles, messages=[[x, y] for x, y in self.messages], offset=self.offset, sep_style=self.sep_style, sep=self.sep, sep2=self.sep2, version=self.version)\n\n    def dict(self):\n        if len(self.get_images()) > 0:\n            return {\n                \"system\": self.system,\n                \"roles\": self.roles,\n                \"messages\": [[x, y[0] if type(y) is tuple else y] for x, y in self.messages],\n                \"offset\": self.offset,\n                \"sep\": self.sep,\n                \"sep2\": self.sep2,\n            }\n        return {\n            \"system\": self.system,\n            \"roles\": self.roles,\n            \"messages\": self.messages,\n            \"offset\": self.offset,\n            \"sep\": self.sep,\n            \"sep2\": self.sep2,\n        }\n\n\nconv_vicuna_v0 = Conversation(\n    system=\"A chat between a curious human and an artificial intelligence assistant. \" \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n    roles=(\"Human\", \"Assistant\"),\n    messages=[\n        [\"Human\", \"What are the key differences between renewable and non-renewable energy sources?\"],\n        [\n            \"Assistant\",\n            \"Renewable energy sources are those that can be replenished naturally in a relatively \"\n            \"short amount of time, such as solar, wind, hydro, geothermal, and biomass. \"\n            \"Non-renewable energy sources, on the other hand, are finite and will eventually be \"\n            \"depleted, such as coal, oil, and natural gas. Here are some key differences between \"\n            \"renewable and non-renewable energy sources:\\n\"\n            \"1. Availability: Renewable energy sources are virtually inexhaustible, while non-renewable \"\n            \"energy sources are finite and will eventually run out.\\n\"\n            \"2. Environmental impact: Renewable energy sources have a much lower environmental impact \"\n            \"than non-renewable sources, which can lead to air and water pollution, greenhouse gas emissions, \"\n            \"and other negative effects.\\n\"\n            \"3. Cost: Renewable energy sources can be more expensive to initially set up, but they typically \"\n            \"have lower operational costs than non-renewable sources.\\n\"\n            \"4. Reliability: Renewable energy sources are often more reliable and can be used in more remote \"\n            \"locations than non-renewable sources.\\n\"\n            \"5. Flexibility: Renewable energy sources are often more flexible and can be adapted to different \"\n            \"situations and needs, while non-renewable sources are more rigid and inflexible.\\n\"\n            \"6. Sustainability: Renewable energy sources are more sustainable over the long term, while \"\n            \"non-renewable sources are not, and their depletion can lead to economic and social instability.\\n\",\n        ],\n    ],\n    offset=2,\n    sep_style=SeparatorStyle.SINGLE,\n    sep=\"###\",\n)\n\nconv_vicuna_v1 = Conversation(\n    system=\"A chat between a curious user and an artificial intelligence assistant. \" \"The assistant gives helpful, detailed, and polite answers to the user's questions.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"v1\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.TWO,\n    sep=\" \",\n    sep2=\"</s>\",\n)\n\nconv_llama_2 = Conversation(\n    system=\"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"llama_v2\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.LLAMA_2,\n    sep=\"<s>\",\n    sep2=\"</s>\",\n)\n\nconv_llava_llama_2 = Conversation(\n    system=\"You are a helpful language and vision assistant. \" \"You are able to understand the visual content that the user provides, \" \"and assist the user with a variety of tasks using natural language.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"llama_v2\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.LLAMA_2,\n    sep=\"<s>\",\n    sep2=\"</s>\",\n)\n\n# conv_llava_llama_3 = Conversation(\n#     system=\"You are a helpful language and vision assistant. \" \"You are able to understand the visual content that the user provides, \" \"and assist the user with a variety of tasks using natural language.\",\n#     roles=(\"user\", \"assistant\"),\n#     version=\"llama_v3\",\n#     messages=[],\n#     offset=0,\n#     sep=\"<|eot_id|>\",\n#     sep_style=SeparatorStyle.LLAMA_3,\n#     tokenizer_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n#     tokenizer=AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\"),\n#     stop_token_ids=[128009],\n# )\n\nconv_mistral_instruct = Conversation(\n    system=\"\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"llama_v2\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.LLAMA_2,\n    sep=\"\",\n    sep2=\"</s>\",\n)\n\nconv_llava_llama_2_simple = Conversation(\n    system=\"Answer the questions about the visual content that the user provides.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"llama_v2\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.LLAMA_2,\n    sep=\"<s>\",\n    sep2=\"</s>\",\n)\n\nconv_llava_llama_2_mmtag = Conversation(\n    system=\"Answer the questions about the visual content that the user provides.\" \"The visual content will be provided with the following format: <Image>visual content</Image>.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"llama_v2_mmtag\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.LLAMA_2,\n    sep=\"<s>\",\n    sep2=\"</s>\",\n)\n\nconv_mpt = Conversation(\n    system=\"\"\"<|im_start|>system\nA conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers.\"\"\",\n    roles=(\"<|im_start|>user\\n\", \"<|im_start|>assistant\\n\"),\n    version=\"mpt\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.MPT,\n    sep=\"<|im_end|>\",\n)\n\nconv_qwen = Conversation(\n    system=\"\"\"<|im_start|>system\nYou are a helpful assistant.\"\"\",\n    roles=(\"<|im_start|>user\", \"<|im_start|>assistant\"),\n    version=\"qwen\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.CHATML,\n    sep=\"<|im_end|>\",\n)\n\nconv_gemma_instruct = Conversation(system=\"\", roles=(\"<start_of_turn>user\\n\", \"<start_of_turn>model\\n\"), version=\"gemma\", messages=[], offset=0, sep_style=SeparatorStyle.GEMMA, sep=\"<end_of_turn>\\n\")\n\nconv_llava_plain = Conversation(\n    system=\"\",\n    roles=(\"\", \"\"),\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.PLAIN,\n    sep=\"\\n\",\n)\n\nconv_llava_v0 = Conversation(\n    system=\"A chat between a curious human and an artificial intelligence assistant. \" \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n    roles=(\"Human\", \"Assistant\"),\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.SINGLE,\n    sep=\"###\",\n)\n\nconv_llava_v0_mmtag = Conversation(\n    system=\"A chat between a curious user and an artificial intelligence assistant. \"\n    \"The assistant is able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\"\n    \"The visual content will be provided with the following format: <Image>visual content</Image>.\",\n    roles=(\"Human\", \"Assistant\"),\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.SINGLE,\n    sep=\"###\",\n    version=\"v0_mmtag\",\n)\n\nconv_llava_v1 = Conversation(\n    system=\"A chat between a curious human and an artificial intelligence assistant. \" \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"v1\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.TWO,\n    sep=\" \",\n    sep2=\"</s>\",\n)\n\nconv_llava_v1_mmtag = Conversation(\n    system=\"A chat between a curious user and an artificial intelligence assistant. \"\n    \"The assistant is able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\"\n    \"The visual content will be provided with the following format: <Image>visual content</Image>.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.TWO,\n    sep=\" \",\n    sep2=\"</s>\",\n    version=\"v1_mmtag\",\n)\n\nconv_mistral_orca = Conversation(\n    system=\"\"\"<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI. Write out your reasoning step-by-step to be sure you get the right answers!\"\"\",\n    roles=(\"<|im_start|>user\\n\", \"<|im_start|>assistant\\n\"),\n    version=\"mpt\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.MPT,\n    sep=\"<|im_end|>\",\n)\n\nconv_mistral_zephyr = Conversation(\n    system=\"\"\"<|system|>\nYou are a helpful AI assistant.\"\"\",\n    roles=(\"<|user|>\\n\", \"<|assistant|>\\n\"),\n    version=\"mpt\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.MPT,\n    sep=\"</s>\",\n)\n\nconv_mistral_direct = Conversation(\n    system=\"\"\"<|im_start|>system\nAnswer the questions.\"\"\",\n    roles=(\"<|im_start|>user\\n\", \"<|im_start|>assistant\\n\"),\n    version=\"mpt\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.MPT,\n    sep=\"<|im_end|>\",\n)\n\nconv_chatml_direct = Conversation(\n    system=\"\"\"<|im_start|>system\nAnswer the questions.\"\"\",\n    roles=(\"<|im_start|>user\\n\", \"<|im_start|>assistant\\n\"),\n    version=\"mpt\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.MPT,\n    sep=\"<|im_end|>\",\n)\n\nconv_llava_deepseekcoder = Conversation(\n    # system=\"You are an AI programming assistant, utilizing the DeepSeek Coder model, \"\n    #     \"developed by DeepSeek Company, and you only answer questions related to computer science. \"\n    #     \"For politically sensitive questions, security and privacy issues, \"\n    #     \"and other non-computer science questions, you will refuse to answer.\\n\",\n    system=\"\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"deepseekcoder\",\n    messages=(),\n    offset=0,\n    sep_style=SeparatorStyle.DeepSeekCoder,\n    sep=\"<|EOT|>\",\n)\n\ndefault_conversation = conv_vicuna_v0\nconv_templates = {\n    \"default\": conv_vicuna_v0,\n    \"v0\": conv_vicuna_v0,\n    \"v1\": conv_vicuna_v1,\n    \"vicuna_v1\": conv_vicuna_v1,\n    \"llama_2\": conv_llama_2,\n    \"mistral_instruct\": conv_mistral_instruct,\n    \"mistral_orca\": conv_mistral_orca,\n    \"mistral_zephyr\": conv_mistral_zephyr,\n    \"mistral_direct\": conv_mistral_direct,\n    \"plain\": conv_llava_plain,\n    \"v0_plain\": conv_llava_plain,\n    \"chatml_direct\": conv_chatml_direct,\n    \"llava_v0\": conv_llava_v0,\n    \"llava_v0_mmtag\": conv_llava_v0_mmtag,\n    \"llava_v1\": conv_llava_v1,\n    \"llava_v1_mmtag\": conv_llava_v1_mmtag,\n    \"llava_llama_2\": conv_llava_llama_2,\n    # \"llava_llama_3\": conv_llava_llama_3,\n    \"llava_llama_2_simple\": conv_llava_llama_2_simple,\n    \"llava_llama_2_mmtag\": conv_llava_llama_2_mmtag,\n    \"llava_mistral_instruct\": conv_mistral_instruct,\n    \"mpt\": conv_mpt,\n    \"qwen_1_5\": conv_qwen,\n    \"qwen_2\": conv_qwen,\n    \"gemma_instruct\": conv_gemma_instruct,\n    \"llava_deepseekcoder\":conv_llava_deepseekcoder,\n}\n\n\nif __name__ == \"__main__\":\n    print(default_conversation.get_prompt())\n"}
{"type": "source_file", "path": "llava/model/language_model/llava_qwen_moe.py", "content": "#    Copyright 2024 Hao Zhang\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom typing import List, Optional, Tuple, Union, Dict\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\n\nimport transformers\nfrom transformers import AutoConfig, AutoModelForCausalLM\n\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nfrom transformers.generation.utils import GenerateOutput\n\n# from ...constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\nfrom llava.model.llava_arch import LlavaMetaModel, LlavaMetaForCausalLM\nfrom transformers import Qwen2MoeConfig, Qwen2MoeModel, Qwen2MoeForCausalLM\n\n# from .qwen.modeling_qwen import QWenLMHeadModel, QWenModel\n# from .qwen.configuration_qwen import QWenConfig\n\n\nclass LlavaQwenMoeConfig(Qwen2MoeConfig):\n    model_type = \"llava_qwen_moe\"\n\n\nclass LlavaQwenMoeModel(LlavaMetaModel, Qwen2MoeModel):\n    config_class = LlavaQwenMoeConfig\n\n    def __init__(self, config: Qwen2MoeConfig):\n        super(LlavaQwenMoeModel, self).__init__(config)\n\n\nclass LlavaQwenMoeForCausalLM(Qwen2MoeForCausalLM, LlavaMetaForCausalLM):\n    config_class = LlavaQwenMoeConfig\n\n    def __init__(self, config):\n        # super(Qwen2MoeForCausalLM, self).__init__(config)\n        Qwen2MoeForCausalLM.__init__(self, config)\n        config.model_type = \"llava_qwen_moe\"\n        config.rope_scaling = None\n\n        self.model = LlavaQwenMoeModel(config)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.model\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        images: Optional[torch.FloatTensor] = None,\n        image_sizes: Optional[List[List[int]]] = None,\n        return_dict: Optional[bool] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        dpo_forward: Optional[bool] = False,\n        cache_position=None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n\n        if inputs_embeds is None:\n            (input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels) = self.prepare_inputs_labels_for_multimodal(input_ids, position_ids, attention_mask, past_key_values, labels, images, modalities, image_sizes)\n\n        if dpo_forward:\n            outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n            hidden_states = outputs[0]\n            logits = self.lm_head(hidden_states)\n            return logits, labels\n\n        else:\n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                labels=labels,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs: Optional[torch.Tensor] = None,\n        images: Optional[torch.Tensor] = None,\n        image_sizes: Optional[torch.Tensor] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        **kwargs,\n    ) -> Union[GenerateOutput, torch.LongTensor]:\n        position_ids = kwargs.pop(\"position_ids\", None)\n        attention_mask = kwargs.pop(\"attention_mask\", None)\n        if \"inputs_embeds\" in kwargs:\n            raise NotImplementedError(\"`inputs_embeds` is not supported\")\n\n        if images is not None:\n            (inputs, position_ids, attention_mask, _, inputs_embeds, _) = self.prepare_inputs_labels_for_multimodal(inputs, position_ids, attention_mask, None, None, images, modalities, image_sizes=image_sizes)\n        else:\n            inputs_embeds = self.get_model().embed_tokens(inputs)\n\n        return super().generate(position_ids=position_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, **kwargs)\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n        images = kwargs.pop(\"images\", None)\n        image_sizes = kwargs.pop(\"image_sizes\", None)\n        inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs)\n        if images is not None:\n            inputs[\"images\"] = images\n        if image_sizes is not None:\n            inputs[\"image_sizes\"] = image_sizes\n        return inputs\n\n\nAutoConfig.register(\"llava_qwen_moe\", LlavaQwenMoeConfig)\nAutoModelForCausalLM.register(LlavaQwenMoeConfig, LlavaQwenMoeForCausalLM)\n"}
{"type": "source_file", "path": "llava/model/language_model/modeling_llama.py", "content": "# coding=utf-8\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" PyTorch LLaMA model.\"\"\"\nimport math\nimport warnings\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.activations import ACT2FN\nfrom transformers.cache_utils import Cache, DynamicCache, StaticCache\nfrom transformers.modeling_outputs import (\n    BaseModelOutputWithPast,\n    CausalLMOutputWithPast,\n    QuestionAnsweringModelOutput,\n    SequenceClassifierOutputWithPast,\n)\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.pytorch_utils import ALL_LAYERNORM_LAYERS\nfrom transformers.utils import (\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    is_flash_attn_2_available,\n    is_flash_attn_greater_or_equal_2_10,\n    logging,\n    replace_return_docstrings,\n)\nfrom transformers.models.llama.configuration_llama import LlamaConfig\n\nif is_flash_attn_2_available():\n    from flash_attn import flash_attn_func, flash_attn_varlen_func\n    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa\n\n\nlogger = logging.get_logger(__name__)\n\n_CONFIG_FOR_DOC = \"LlamaConfig\"\n\n\ndef _get_unpad_data(attention_mask):\n    seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)\n    indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()\n    max_seqlen_in_batch = seqlens_in_batch.max().item()\n    cu_seqlens = F.pad(torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0))\n    return (\n        indices,\n        cu_seqlens,\n        max_seqlen_in_batch,\n    )\n\n\nclass LlamaRMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        LlamaRMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        input_dtype = hidden_states.dtype\n        hidden_states = hidden_states.to(torch.float32)\n        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * hidden_states.to(input_dtype)\n\n\nALL_LAYERNORM_LAYERS.append(LlamaRMSNorm)\n\n\nclass LlamaRotaryEmbedding(nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n        super().__init__()\n        self.scaling_factor = scaling_factor\n        self.dim = dim\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n        # For BC we register cos and sin cached\n        self.max_seq_len_cached = max_position_embeddings\n        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n        t = t / self.scaling_factor\n        freqs = torch.outer(t, self.inv_freq)\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.register_buffer(\"_cos_cached\", emb.cos().to(torch.get_default_dtype()), persistent=False)\n        self.register_buffer(\"_sin_cached\", emb.sin().to(torch.get_default_dtype()), persistent=False)\n\n    @property\n    def sin_cached(self):\n        logger.warning_once(\"The sin_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use \" \"the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\")\n        return self._sin_cached\n\n    @property\n    def cos_cached(self):\n        logger.warning_once(\"The cos_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use \" \"the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\")\n        return self._cos_cached\n\n    @torch.no_grad()\n    def forward(self, x, position_ids, seq_len=None):\n        if seq_len is not None:\n            logger.warning_once(\"The `seq_len` argument is deprecated and unused. It will be removed in v4.39.\")\n\n        # x: [bs, num_attention_heads, seq_len, head_size]\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n        position_ids_expanded = position_ids[:, None, :].float()\n        # Force float32 since bfloat16 loses precision on long contexts\n        # See https://github.com/huggingface/transformers/pull/29285\n        device_type = x.device.type\n        device_type = device_type if isinstance(device_type, str) else \"cpu\"\n        with torch.autocast(device_type=device_type, enabled=False):\n            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n            emb = torch.cat((freqs, freqs), dim=-1)\n            cos = emb.cos()\n            sin = emb.sin()\n        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n\n\nclass LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):\n    \"\"\"LlamaRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n\n    def forward(self, x, position_ids, seq_len=None):\n        # difference to the original RoPE: a scaling factor is aplied to the position ids\n        position_ids = position_ids.float() / self.scaling_factor\n        cos, sin = super().forward(x, position_ids, seq_len)\n        return cos, sin\n\n\nclass LlamaDynamicNTKScalingRotaryEmbedding(LlamaRotaryEmbedding):\n    \"\"\"LlamaRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n\n    def forward(self, x, position_ids, seq_len=None):\n        # difference to the original RoPE: inv_freq is recomputed when the sequence length > original length\n        seq_len = torch.max(position_ids) + 1\n        if seq_len > self.max_position_embeddings:\n            base = self.base * ((self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)) ** (self.dim / (self.dim - 2))\n            inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(x.device) / self.dim))\n            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: this may break with compilation\n\n        cos, sin = super().forward(x, position_ids, seq_len)\n        return cos, sin\n\n\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n\n    Args:\n        q (`torch.Tensor`): The query tensor.\n        k (`torch.Tensor`): The key tensor.\n        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n        sin (`torch.Tensor`): The sine part of the rotary embedding.\n        position_ids (`torch.Tensor`, *optional*):\n            Deprecated and unused.\n        unsqueeze_dim (`int`, *optional*, defaults to 1):\n            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n    Returns:\n        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n    \"\"\"\n    cos = cos.unsqueeze(unsqueeze_dim)\n    sin = sin.unsqueeze(unsqueeze_dim)\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\n\nclass LlamaMLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.intermediate_size = config.intermediate_size\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n        self.act_fn = ACT2FN[config.hidden_act]\n\n    def forward(self, x):\n        if self.config.pretraining_tp > 1:\n            slice = self.intermediate_size // self.config.pretraining_tp\n            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)\n            up_proj_slices = self.up_proj.weight.split(slice, dim=0)\n            down_proj_slices = self.down_proj.weight.split(slice, dim=1)\n\n            gate_proj = torch.cat([F.linear(x, gate_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1)\n            up_proj = torch.cat([F.linear(x, up_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1)\n\n            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)\n            down_proj = [F.linear(intermediate_states[i], down_proj_slices[i]) for i in range(self.config.pretraining_tp)]\n            down_proj = sum(down_proj)\n        else:\n            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n\n        return down_proj\n\n\ndef repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n    \"\"\"\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n\n\nclass LlamaAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        if layer_idx is None:\n            logger.warning_once(\n                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n                \"when creating this class.\"\n            )\n\n        self.attention_dropout = config.attention_dropout\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\" f\" and `num_heads`: {self.num_heads}).\")\n\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)\n        self._init_rope()\n\n    def _init_rope(self):\n        if self.config.rope_scaling is None:\n            self.rotary_emb = LlamaRotaryEmbedding(\n                self.head_dim,\n                max_position_embeddings=self.max_position_embeddings,\n                base=self.rope_theta,\n            )\n        else:\n            scaling_type = self.config.rope_scaling[\"type\"]\n            scaling_factor = self.config.rope_scaling[\"factor\"]\n            if scaling_type == \"linear\":\n                self.rotary_emb = LlamaLinearScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    scaling_factor=scaling_factor,\n                    base=self.rope_theta,\n                )\n            elif scaling_type == \"dynamic\":\n                self.rotary_emb = LlamaDynamicNTKScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    scaling_factor=scaling_factor,\n                    base=self.rope_theta,\n                )\n            else:\n                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n\n        if self.config.pretraining_tp > 1:\n            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n            query_slices = self.q_proj.weight.split((self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0)\n            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n\n            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n            query_states = torch.cat(query_states, dim=-1)\n\n            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n            key_states = torch.cat(key_states, dim=-1)\n\n            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n            value_states = torch.cat(value_states, dim=-1)\n\n        else:\n            query_states = self.q_proj(hidden_states)\n            key_states = self.k_proj(hidden_states)\n            value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        past_key_value = getattr(self, \"past_key_value\", past_key_value)\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; position_ids needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n        if attention_mask is not None:  # no matter the length, we just slice it\n            causal_mask = attention_mask\n            if cache_position is not None:\n                causal_mask = attention_mask[:, :, cache_position, : key_states.shape[-2]]\n            attn_weights = attn_weights + causal_mask\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\" f\" {attn_output.size()}\")\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        if self.config.pretraining_tp > 1:\n            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n        else:\n            attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\nclass LlamaRingFlashAttention2(LlamaAttention):\n    \"\"\"\n    Llama flash attention module. This module inherits from `LlamaAttention` as the weights of the module stays\n    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n    flash attention and deal with padding tokens in case the input contains any of them.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        output_attentions = False\n\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        # Flash attention requires the input to have the shape\n        # batch_size x seq_length x head_dim x hidden_dim\n        # therefore we just need to keep the original shape\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        past_key_value = getattr(self, \"past_key_value\", past_key_value)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; position_ids needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n        # to be able to avoid many of these transpose/reshape/view.\n        query_states = query_states.transpose(1, 2)\n        key_states = key_states.transpose(1, 2)\n        value_states = value_states.transpose(1, 2)\n\n        dropout_rate = self.attention_dropout if self.training else 0.0\n\n        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n        # therefore the input hidden states gets silently casted in float32. Hence, we need\n        # cast them back in the correct dtype just to be sure everything works as expected.\n        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n        # in fp32. (LlamaRMSNorm handles it correctly)\n\n        input_dtype = query_states.dtype\n        if input_dtype == torch.float32:\n            if torch.is_autocast_enabled():\n                target_dtype = torch.get_autocast_gpu_dtype()\n            # Handle the case where the model is quantized\n            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                target_dtype = self.config._pre_quantization_dtype\n            else:\n                target_dtype = self.q_proj.weight.dtype\n\n            logger.warning_once(\n                f\"The input hidden states seems to be silently casted in float32, this might be related to\" f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\" f\" {target_dtype}.\"\n            )\n\n            query_states = query_states.to(target_dtype)\n            key_states = key_states.to(target_dtype)\n            value_states = value_states.to(target_dtype)\n\n        attn_output = self._flash_attention_forward(query_states, key_states, value_states, attention_mask, q_len, dropout=dropout_rate)\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n    def _flash_attention_forward(self, query_states, key_states, value_states, attention_mask, query_length, dropout=0.0, softmax_scale=None):\n        \"\"\"\n        Calls the forward method of Flash Attention - if the input hidden states contain at least one padding token\n        first unpad the input, then computes the attention scores and pad the final attention scores.\n\n        Args:\n            query_states (`torch.Tensor`):\n                Input query states to be passed to Flash Attention API\n            key_states (`torch.Tensor`):\n                Input key states to be passed to Flash Attention API\n            value_states (`torch.Tensor`):\n                Input value states to be passed to Flash Attention API\n            attention_mask (`torch.Tensor`):\n                The padding mask - corresponds to a tensor of size `(batch_size, seq_len)` where 0 stands for the\n                position of padding tokens and 1 for the position of non-padding tokens.\n            dropout (`int`, *optional*):\n                Attention dropout\n            softmax_scale (`float`, *optional*):\n                The scaling of QK^T before applying softmax. Default to 1 / sqrt(head_dim)\n        \"\"\"\n        if not self._flash_attn_uses_top_left_mask:\n            causal = self.is_causal\n        else:\n            # TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1. For details, please see the comment in LlamaFlashAttention2 __init__.\n            causal = self.is_causal and query_length != 1\n\n        # Contains at least one padding token in the sequence\n        if attention_mask is not None:\n            batch_size = query_states.shape[0]\n            query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = self._upad_input(query_states, key_states, value_states, attention_mask, query_length)\n\n            cu_seqlens_q, cu_seqlens_k = cu_seq_lens\n            max_seqlen_in_batch_q, max_seqlen_in_batch_k = max_seq_lens\n\n            attn_output_unpad = zigzag_ring_flash_attn_varlen_func(\n                query_states,\n                key_states,\n                value_states,\n                cu_seqlens_q=cu_seqlens_q,\n                cu_seqlens_k=cu_seqlens_k,\n                max_seqlen_q=max_seqlen_in_batch_q,\n                max_seqlen_k=max_seqlen_in_batch_k,\n                dropout_p=dropout,\n                softmax_scale=softmax_scale,\n                causal=causal,\n            )\n\n            attn_output = pad_input(attn_output_unpad, indices_q, batch_size, query_length)\n        else:\n            # pack qkv\n            # query_states: (batch_size, seqlen, nheads, headdim)\n            # qkv: (batch_size, seqlen, 3, nheads, headdim)\n            qkv = torch.stack([query_states, key_states, value_states], dim=2)\n            attn_output = zigzag_ring_flash_attn_qkvpacked_func(qkv, dropout, softmax_scale, causal=causal)\n\n        return attn_output\n\n    def _upad_input(self, query_layer, key_layer, value_layer, attention_mask, query_length):\n        indices_k, cu_seqlens_k, max_seqlen_in_batch_k = _get_unpad_data(attention_mask)\n        batch_size, kv_seq_len, num_key_value_heads, head_dim = key_layer.shape\n\n        key_layer = index_first_axis(key_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k)\n        value_layer = index_first_axis(value_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k)\n        if query_length == kv_seq_len:\n            query_layer = index_first_axis(query_layer.reshape(batch_size * kv_seq_len, self.num_heads, head_dim), indices_k)\n            cu_seqlens_q = cu_seqlens_k\n            max_seqlen_in_batch_q = max_seqlen_in_batch_k\n            indices_q = indices_k\n        elif query_length == 1:\n            max_seqlen_in_batch_q = 1\n            cu_seqlens_q = torch.arange(batch_size + 1, dtype=torch.int32, device=query_layer.device)  # There is a memcpy here, that is very bad.\n            indices_q = cu_seqlens_q[:-1]\n            query_layer = query_layer.squeeze(1)\n        else:\n            # The -q_len: slice assumes left padding.\n            attention_mask = attention_mask[:, -query_length:]\n            query_layer, indices_q, cu_seqlens_q, max_seqlen_in_batch_q = unpad_input(query_layer, attention_mask)\n\n        return (\n            query_layer,\n            key_layer,\n            value_layer,\n            indices_q,\n            (cu_seqlens_q, cu_seqlens_k),\n            (max_seqlen_in_batch_q, max_seqlen_in_batch_k),\n        )\n\n\nclass LlamaFlashAttention2(LlamaAttention):\n    \"\"\"\n    Llama flash attention module. This module inherits from `LlamaAttention` as the weights of the module stays\n    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n    flash attention and deal with padding tokens in case the input contains any of them.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        output_attentions = False\n\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        # Flash attention requires the input to have the shape\n        # batch_size x seq_length x head_dim x hidden_dim\n        # therefore we just need to keep the original shape\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        past_key_value = getattr(self, \"past_key_value\", past_key_value)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; position_ids needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n        # to be able to avoid many of these transpose/reshape/view.\n        query_states = query_states.transpose(1, 2)\n        key_states = key_states.transpose(1, 2)\n        value_states = value_states.transpose(1, 2)\n\n        dropout_rate = self.attention_dropout if self.training else 0.0\n\n        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n        # therefore the input hidden states gets silently casted in float32. Hence, we need\n        # cast them back in the correct dtype just to be sure everything works as expected.\n        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n        # in fp32. (LlamaRMSNorm handles it correctly)\n\n        input_dtype = query_states.dtype\n        if input_dtype == torch.float32:\n            if torch.is_autocast_enabled():\n                target_dtype = torch.get_autocast_gpu_dtype()\n            # Handle the case where the model is quantized\n            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                target_dtype = self.config._pre_quantization_dtype\n            else:\n                target_dtype = self.q_proj.weight.dtype\n\n            logger.warning_once(\n                f\"The input hidden states seems to be silently casted in float32, this might be related to\" f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\" f\" {target_dtype}.\"\n            )\n\n            query_states = query_states.to(target_dtype)\n            key_states = key_states.to(target_dtype)\n            value_states = value_states.to(target_dtype)\n\n        attn_output = self._flash_attention_forward(query_states, key_states, value_states, attention_mask, q_len, dropout=dropout_rate)\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n    def _flash_attention_forward(self, query_states, key_states, value_states, attention_mask, query_length, dropout=0.0, softmax_scale=None):\n        \"\"\"\n        Calls the forward method of Flash Attention - if the input hidden states contain at least one padding token\n        first unpad the input, then computes the attention scores and pad the final attention scores.\n\n        Args:\n            query_states (`torch.Tensor`):\n                Input query states to be passed to Flash Attention API\n            key_states (`torch.Tensor`):\n                Input key states to be passed to Flash Attention API\n            value_states (`torch.Tensor`):\n                Input value states to be passed to Flash Attention API\n            attention_mask (`torch.Tensor`):\n                The padding mask - corresponds to a tensor of size `(batch_size, seq_len)` where 0 stands for the\n                position of padding tokens and 1 for the position of non-padding tokens.\n            dropout (`int`, *optional*):\n                Attention dropout\n            softmax_scale (`float`, *optional*):\n                The scaling of QK^T before applying softmax. Default to 1 / sqrt(head_dim)\n        \"\"\"\n        if not self._flash_attn_uses_top_left_mask:\n            causal = self.is_causal\n        else:\n            # TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1. For details, please see the comment in LlamaFlashAttention2 __init__.\n            causal = self.is_causal and query_length != 1\n\n        # Contains at least one padding token in the sequence\n        if attention_mask is not None:\n            batch_size = query_states.shape[0]\n            query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = self._upad_input(query_states, key_states, value_states, attention_mask, query_length)\n\n            cu_seqlens_q, cu_seqlens_k = cu_seq_lens\n            max_seqlen_in_batch_q, max_seqlen_in_batch_k = max_seq_lens\n\n            attn_output_unpad = flash_attn_varlen_func(\n                query_states,\n                key_states,\n                value_states,\n                cu_seqlens_q=cu_seqlens_q,\n                cu_seqlens_k=cu_seqlens_k,\n                max_seqlen_q=max_seqlen_in_batch_q,\n                max_seqlen_k=max_seqlen_in_batch_k,\n                dropout_p=dropout,\n                softmax_scale=softmax_scale,\n                causal=causal,\n            )\n\n            attn_output = pad_input(attn_output_unpad, indices_q, batch_size, query_length)\n        else:\n            attn_output = flash_attn_func(query_states, key_states, value_states, dropout, softmax_scale=softmax_scale, causal=causal)\n\n        return attn_output\n\n    def _upad_input(self, query_layer, key_layer, value_layer, attention_mask, query_length):\n        indices_k, cu_seqlens_k, max_seqlen_in_batch_k = _get_unpad_data(attention_mask)\n        batch_size, kv_seq_len, num_key_value_heads, head_dim = key_layer.shape\n\n        key_layer = index_first_axis(key_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k)\n        value_layer = index_first_axis(value_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k)\n        if query_length == kv_seq_len:\n            query_layer = index_first_axis(query_layer.reshape(batch_size * kv_seq_len, self.num_heads, head_dim), indices_k)\n            cu_seqlens_q = cu_seqlens_k\n            max_seqlen_in_batch_q = max_seqlen_in_batch_k\n            indices_q = indices_k\n        elif query_length == 1:\n            max_seqlen_in_batch_q = 1\n            cu_seqlens_q = torch.arange(batch_size + 1, dtype=torch.int32, device=query_layer.device)  # There is a memcpy here, that is very bad.\n            indices_q = cu_seqlens_q[:-1]\n            query_layer = query_layer.squeeze(1)\n        else:\n            # The -q_len: slice assumes left padding.\n            attention_mask = attention_mask[:, -query_length:]\n            query_layer, indices_q, cu_seqlens_q, max_seqlen_in_batch_q = unpad_input(query_layer, attention_mask)\n\n        return (\n            query_layer,\n            key_layer,\n            value_layer,\n            indices_q,\n            (cu_seqlens_q, cu_seqlens_k),\n            (max_seqlen_in_batch_q, max_seqlen_in_batch_k),\n        )\n\n\nclass LlamaSdpaAttention(LlamaAttention):\n    \"\"\"\n    Llama attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n    `LlamaAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n    SDPA API.\n    \"\"\"\n\n    # Adapted from LlamaAttention.forward\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        if output_attentions:\n            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n            logger.warning_once(\n                \"LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n            )\n            return super().forward(\n                hidden_states=hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_value=past_key_value,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n                cache_position=cache_position,\n            )\n\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        # In case static cache is used, it is an instance attribute.\n        past_key_value = getattr(self, \"past_key_value\", past_key_value)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; position_ids needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        causal_mask = attention_mask\n        if attention_mask is not None and cache_position is not None:\n            causal_mask = causal_mask[:, :, cache_position, : key_states.shape[-2]]\n\n        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n        if query_states.device.type == \"cuda\" and causal_mask is not None:\n            query_states = query_states.contiguous()\n            key_states = key_states.contiguous()\n            value_states = value_states.contiguous()\n\n        attn_output = torch.nn.functional.scaled_dot_product_attention(\n            query_states,\n            key_states,\n            value_states,\n            attn_mask=causal_mask,\n            dropout_p=self.attention_dropout if self.training else 0.0,\n        )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        attn_output = attn_output.view(bsz, q_len, self.hidden_size)\n\n        attn_output = self.o_proj(attn_output)\n\n        return attn_output, None, past_key_value\n\n\ntry:\n    from ring_flash_attn import zigzag_ring_flash_attn_qkvpacked_func, zigzag_ring_flash_attn_varlen_func\nexcept ImportError:\n    print(\"Please install the ring-flash-attn package\")\n\nLLAMA_ATTENTION_CLASSES = {\n    \"eager\": LlamaAttention,\n    \"flash_attention_2\": LlamaFlashAttention2,\n    \"ring_flash_attention_2\": LlamaRingFlashAttention2,\n    \"sdpa\": LlamaSdpaAttention,\n}\n\n\nclass LlamaDecoderLayer(nn.Module):\n    def __init__(self, config: LlamaConfig, layer_idx: int):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n\n        self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n\n        self.mlp = LlamaMLP(config)\n        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n            attention_mask (`torch.FloatTensor`, *optional*):\n                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n                query_sequence_length, key_sequence_length)` if default attention is used.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            use_cache (`bool`, *optional*):\n                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                (see `past_key_values`).\n            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n        \"\"\"\n        if \"padding_mask\" in kwargs:\n            warnings.warn(\"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\")\n\n        residual = hidden_states\n\n        hidden_states = self.input_layernorm(hidden_states)\n\n        # Self Attention\n        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_value=past_key_value,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n            cache_position=cache_position,\n            **kwargs,\n        )\n        hidden_states = residual + hidden_states\n\n        # Fully Connected\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (self_attn_weights,)\n\n        if use_cache:\n            outputs += (present_key_value,)\n\n        return outputs\n\n\nLLAMA_START_DOCSTRING = r\"\"\"\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Parameters:\n        config ([`LlamaConfig`]):\n            Model configuration class with all the parameters of the model. Initializing with a config file does not\n            load the weights associated with the model, only the configuration. Check out the\n            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaPreTrainedModel(PreTrainedModel):\n    config_class = LlamaConfig\n    base_model_prefix = \"model\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"LlamaDecoderLayer\"]\n    _skip_keys_device_placement = [\"past_key_values\", \"causal_mask\"]\n    _supports_flash_attn_2 = True\n    _supports_sdpa = True\n    _supports_cache_class = True\n\n    def _init_weights(self, module):\n        std = self.config.initializer_range\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n\n    def _setup_cache(self, cache_cls, max_batch_size, max_cache_len: Optional[int] = None):\n        if self.config._attn_implementation == \"flash_attention_2\" and cache_cls == StaticCache:\n            raise ValueError(\"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \" \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\")\n\n        if max_cache_len > self.model.causal_mask.shape[-1] or self.device != self.model.causal_mask.device:\n            causal_mask = torch.full((max_cache_len, max_cache_len), fill_value=True, device=self.device, dtype=torch.bool)\n            self.register_buffer(\"causal_mask\", torch.triu(causal_mask, diagonal=1), persistent=False)\n\n        for layer in self.model.layers:\n            device = layer.input_layernorm.weight.device\n            if hasattr(self.config, \"_pre_quantization_dtype\"):\n                dtype = self.config._pre_quantization_dtype\n            else:\n                dtype = layer.self_attn.o_proj.weight.dtype\n            layer.self_attn.past_key_value = cache_cls(self.config, max_batch_size, max_cache_len, device=device, dtype=dtype)\n\n    def _reset_cache(self):\n        for layer in self.model.layers:\n            layer.self_attn.past_key_value = None\n\n\nLLAMA_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n            it.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n            `past_key_values`).\n\n            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n            information on the default strategy.\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n            config.n_positions - 1]`.\n\n            [What are position IDs?](../glossary#position-ids)\n        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n\n            Two formats are allowed:\n            - a [`~cache_utils.Cache`] instance;\n            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n            cache format.\n\n            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n            legacy cache format will be returned.\n\n            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n            of shape `(batch_size, sequence_length)`.\n        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n            model's internal embedding lookup matrix.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaModel(LlamaPreTrainedModel):\n    \"\"\"\n    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n\n    Args:\n        config: LlamaConfig\n    \"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n        self.layers = nn.ModuleList([LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)])\n        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.gradient_checkpointing = False\n\n        # Register a causal mask to separate causal and padding mask creation. Merging happens in the attention class.\n        # NOTE: This is not friendly with TorchScript, ONNX, ExportedProgram serialization for very large `max_position_embeddings`.\n        causal_mask = torch.full((config.max_position_embeddings, config.max_position_embeddings), fill_value=True, dtype=torch.bool)\n        self.register_buffer(\"causal_mask\", torch.triu(causal_mask, diagonal=1), persistent=False)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Union[Tuple, BaseModelOutputWithPast]:\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if (input_ids is None) ^ (inputs_embeds is not None):\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\")\n\n        if self.gradient_checkpointing and self.training and use_cache:\n            logger.warning_once(\"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\")\n            use_cache = False\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)\n\n        past_seen_tokens = 0\n        if use_cache:  # kept for BC (cache positions)\n            if not isinstance(past_key_values, StaticCache):\n                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n                past_seen_tokens = past_key_values.get_seq_length()\n\n        if cache_position is None:\n            if isinstance(past_key_values, StaticCache):\n                raise ValueError(\"cache_position is a required argument when using StaticCache.\")\n            cache_position = torch.arange(past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device)\n\n        if position_ids is None:\n            position_ids = cache_position.unsqueeze(0)\n\n        causal_mask = self._update_causal_mask(attention_mask, inputs_embeds)\n\n        # embed positions\n        hidden_states = inputs_embeds\n\n        # decoder layers\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n        next_decoder_cache = None\n\n        for decoder_layer in self.layers:\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            if self.gradient_checkpointing and self.training:\n                layer_outputs = self._gradient_checkpointing_func(\n                    decoder_layer.__call__,\n                    hidden_states,\n                    causal_mask,\n                    position_ids,\n                    past_key_values,\n                    output_attentions,\n                    use_cache,\n                    cache_position,\n                )\n            else:\n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    attention_mask=causal_mask,\n                    position_ids=position_ids,\n                    past_key_value=past_key_values,\n                    output_attentions=output_attentions,\n                    use_cache=use_cache,\n                    cache_position=cache_position,\n                )\n\n            hidden_states = layer_outputs[0]\n\n            if use_cache:\n                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n\n        hidden_states = self.norm(hidden_states)\n\n        # add hidden states from the last decoder layer\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        next_cache = None\n        if use_cache:\n            next_cache = next_decoder_cache.to_legacy_cache() if isinstance(next_decoder_cache, Cache) else next_decoder_cache\n        if not return_dict:\n            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n        return BaseModelOutputWithPast(\n            last_hidden_state=hidden_states,\n            past_key_values=next_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n        )\n\n    # TODO: As of torch==2.2.0, the `attention_mask` passed to the model in `generate` is 2D and of dynamic length even when the static\n    # KV cache is used. This is an issue for torch.compile which then recaptures cudagraphs at each decode steps due to the dynamic shapes.\n    # (`recording cudagraph tree for symint key 13`, etc.), which is VERY slow. A workaround is `@torch.compiler.disable`, but this prevents using\n    # `fullgraph=True`. See more context in https://github.com/huggingface/transformers/pull/29114\n    def _update_causal_mask(self, attention_mask, input_tensor):\n        if self.config._attn_implementation == \"flash_attention_2\":\n            if attention_mask is not None and 0.0 in attention_mask:\n                return attention_mask\n            return None\n\n        batch_size, seq_length = input_tensor.shape[:2]\n        dtype = input_tensor.dtype\n        device = input_tensor.device\n\n        # support going beyond cached `max_position_embedding`\n        if seq_length > self.causal_mask.shape[-1]:\n            causal_mask = torch.full((2 * self.causal_mask.shape[-1], 2 * self.causal_mask.shape[-1]), fill_value=1)\n            self.register_buffer(\"causal_mask\", torch.triu(causal_mask, diagonal=1), persistent=False)\n\n        # We use the current dtype to avoid any overflows\n        min_dtype = torch.finfo(dtype).min\n        causal_mask = self.causal_mask[None, None, :, :].repeat(batch_size, 1, 1, 1).to(dtype) * min_dtype\n\n        causal_mask = causal_mask.to(dtype=dtype, device=device)\n        if attention_mask is not None and attention_mask.dim() == 2:\n            mask_length = attention_mask.shape[-1]\n            padding_mask = causal_mask[..., :mask_length].eq(0.0) * attention_mask[:, None, None, :].eq(0.0)\n            causal_mask[..., :mask_length] = causal_mask[..., :mask_length].masked_fill(padding_mask, min_dtype)\n\n        if self.config._attn_implementation == \"sdpa\" and attention_mask is not None and attention_mask.device.type == \"cuda\":\n            # TODO: For dynamo, rather use a check on fullgraph=True once this is possible (https://github.com/pytorch/pytorch/pull/120400).\n            is_tracing = torch.jit.is_tracing() or isinstance(input_tensor, torch.fx.Proxy) or (hasattr(torch, \"_dynamo\") and torch._dynamo.is_compiling())\n            if not is_tracing and torch.any(attention_mask != 1):\n                # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n                # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n                # Details: https://github.com/pytorch/pytorch/issues/110213\n                causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n\n        return causal_mask\n\n\nclass LlamaForCausalLM(LlamaPreTrainedModel):\n    _tied_weights_keys = [\"lm_head.weight\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = LlamaModel(config)\n        self.vocab_size = config.vocab_size\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def set_decoder(self, decoder):\n        self.model = decoder\n\n    def get_decoder(self):\n        return self.model\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n        r\"\"\"\n        Args:\n            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n\n        Returns:\n\n        Example:\n\n        ```python\n        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n\n        >>> model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\n        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n        >>> # Generate\n        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n        ```\"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            cache_position=cache_position,\n        )\n\n        hidden_states = outputs[0]\n        if self.config.pretraining_tp > 1:\n            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n            logits = torch.cat(logits, dim=-1)\n        else:\n            logits = self.lm_head(hidden_states)\n        logits = logits.float()\n\n        loss = None\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss()\n            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n            shift_labels = shift_labels.view(-1)\n            # Enable model parallelism\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return CausalLMOutputWithPast(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs):\n        past_length = 0\n        if past_key_values is not None:\n            if isinstance(past_key_values, Cache):\n                cache_length = past_key_values.get_seq_length()\n                past_length = past_key_values.seen_tokens\n                max_cache_length = past_key_values.get_max_length()\n            else:\n                cache_length = past_length = past_key_values[0][0].shape[2]\n                max_cache_length = None\n\n            # Keep only the unprocessed tokens:\n            # 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where\n            # some of the inputs are exclusively passed as part of the cache (e.g. when passing input_embeds as\n            # input)\n            if attention_mask is not None and attention_mask.shape[1] > input_ids.shape[1]:\n                input_ids = input_ids[:, -(attention_mask.shape[1] - past_length) :]\n            # 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard\n            # input_ids based on the past_length.\n            elif past_length < input_ids.shape[1]:\n                input_ids = input_ids[:, past_length:]\n            # 3 - Otherwise (past_length >= input_ids.shape[1]), let's assume input_ids only has unprocessed tokens.\n\n            # If we are about to go beyond the maximum cache length, we need to crop the input attention mask.\n            if max_cache_length is not None and attention_mask is not None and cache_length + input_ids.shape[1] > max_cache_length:\n                attention_mask = attention_mask[:, -max_cache_length:]\n\n        position_ids = kwargs.get(\"position_ids\", None)\n        if attention_mask is not None and position_ids is None:\n            # create position_ids on the fly for batch generation\n            position_ids = attention_mask.long().cumsum(-1) - 1\n            position_ids.masked_fill_(attention_mask == 0, 1)\n            if past_key_values:\n                position_ids = position_ids[:, -input_ids.shape[1] :]\n\n        if self.generation_config.cache_implementation == \"static\":\n            # generation with static cache\n            cache_position = kwargs.get(\"cache_position\", None)\n            if cache_position is None:\n                past_length = 0\n            else:\n                past_length = cache_position[-1] + 1\n            input_ids = input_ids[:, past_length:]\n            position_ids = position_ids[:, past_length:]\n\n        # TODO @gante we should only keep a `cache_position` in generate, and do +=1.\n        # same goes for position ids. Could also help with continued generation.\n        input_length = position_ids.shape[-1] if position_ids is not None else input_ids.shape[-1]\n        cache_position = torch.arange(past_length, past_length + input_length, device=input_ids.device)\n        position_ids = position_ids.contiguous() if position_ids is not None else None\n\n        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n        if inputs_embeds is not None and past_key_values is None:\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            # The `contiguous()` here is necessary to have a static stride during decoding. torchdynamo otherwise\n            # recompiles graphs as the stride of the inputs is a guard. Ref: https://github.com/huggingface/transformers/pull/29114\n            # TODO: use `next_tokens` directly instead.\n            model_inputs = {\"input_ids\": input_ids.contiguous()}\n\n        model_inputs.update(\n            {\n                \"position_ids\": position_ids,\n                \"cache_position\": cache_position,\n                \"past_key_values\": past_key_values,\n                \"use_cache\": kwargs.get(\"use_cache\"),\n                \"attention_mask\": attention_mask,\n            }\n        )\n        return model_inputs\n\n    @staticmethod\n    def _reorder_cache(past_key_values, beam_idx):\n        reordered_past = ()\n        for layer_past in past_key_values:\n            reordered_past += (tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),)\n        return reordered_past\n\n\n@add_start_docstrings(\n    \"\"\"\n    The LLaMa Model transformer with a sequence classification head on top (linear layer).\n\n    [`LlamaForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n    (e.g. GPT-2) do.\n\n    Since it does classification on the last token, it requires to know the position of the last token. If a\n    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n    each row of the batch).\n    \"\"\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaForSequenceClassification(LlamaPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.model = LlamaModel(config)\n        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        transformer_outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        hidden_states = transformer_outputs[0]\n        logits = self.score(hidden_states)\n\n        if input_ids is not None:\n            batch_size = input_ids.shape[0]\n        else:\n            batch_size = inputs_embeds.shape[0]\n\n        if self.config.pad_token_id is None and batch_size != 1:\n            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n        if self.config.pad_token_id is None:\n            sequence_lengths = -1\n        else:\n            if input_ids is not None:\n                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n                sequence_lengths = sequence_lengths.to(logits.device)\n            else:\n                sequence_lengths = -1\n\n        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n\n        loss = None\n        if labels is not None:\n            labels = labels.to(logits.device)\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = \"regression\"\n                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\"\n                else:\n                    self.config.problem_type = \"multi_label_classification\"\n\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss()\n                if self.num_labels == 1:\n                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n                else:\n                    loss = loss_fct(pooled_logits, labels)\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n            elif self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(pooled_logits, labels)\n        if not return_dict:\n            output = (pooled_logits,) + transformer_outputs[1:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutputWithPast(\n            loss=loss,\n            logits=pooled_logits,\n            past_key_values=transformer_outputs.past_key_values,\n            hidden_states=transformer_outputs.hidden_states,\n            attentions=transformer_outputs.attentions,\n        )\n\n\n@add_start_docstrings(\n    \"\"\"\nThe Llama Model transformer with a span classification head on top for extractive question-answering tasks like\nSQuAD (a linear layer on top of the hidden-states output to compute `span start logits` and `span end logits`).\n    \"\"\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaForQuestionAnswering(LlamaPreTrainedModel):\n    base_model_prefix = \"transformer\"\n\n    # Copied from transformers.models.bloom.modeling_bloom.BloomForQuestionAnswering.__init__ with Bloom->Llama\n    def __init__(self, config):\n        super().__init__(config)\n        self.transformer = LlamaModel(config)\n        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.transformer.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.transformer.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        start_positions: Optional[torch.LongTensor] = None,\n        end_positions: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n        r\"\"\"\n        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n            are not taken into account for computing the loss.\n        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n            are not taken into account for computing the loss.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.transformer(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1).contiguous()\n        end_logits = end_logits.squeeze(-1).contiguous()\n\n        total_loss = None\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1).to(start_logits.device)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1).to(end_logits.device)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions = start_positions.clamp(0, ignored_index)\n            end_positions = end_positions.clamp(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n\n        if not return_dict:\n            output = (start_logits, end_logits) + outputs[2:]\n            return ((total_loss,) + output) if total_loss is not None else output\n\n        return QuestionAnsweringModelOutput(\n            loss=total_loss,\n            start_logits=start_logits,\n            end_logits=end_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/loss.py", "content": "import math\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\ntry:\n    import torch.distributed.nn\n    from torch import distributed as dist\n\n    has_distributed = True\nexcept ImportError:\n    has_distributed = False\n\ntry:\n    import horovod.torch as hvd\nexcept ImportError:\n    hvd = None\n\nfrom timm.loss import LabelSmoothingCrossEntropy\n\n\ndef gather_features(image_features, text_features, local_loss=False, gather_with_grad=False, rank=0, world_size=1, use_horovod=False):\n    assert has_distributed, \"torch.distributed did not import correctly, please use a PyTorch version with support.\"\n    if use_horovod:\n        assert hvd is not None, \"Please install horovod\"\n        if gather_with_grad:\n            all_image_features = hvd.allgather(image_features)\n            all_text_features = hvd.allgather(text_features)\n        else:\n            with torch.no_grad():\n                all_image_features = hvd.allgather(image_features)\n                all_text_features = hvd.allgather(text_features)\n            if not local_loss:\n                # ensure grads for local rank when all_* features don't have a gradient\n                gathered_image_features = list(all_image_features.chunk(world_size, dim=0))\n                gathered_text_features = list(all_text_features.chunk(world_size, dim=0))\n                gathered_image_features[rank] = image_features\n                gathered_text_features[rank] = text_features\n                all_image_features = torch.cat(gathered_image_features, dim=0)\n                all_text_features = torch.cat(gathered_text_features, dim=0)\n    else:\n        # We gather tensors from all gpus\n        if gather_with_grad:\n            all_image_features = torch.cat(torch.distributed.nn.all_gather(image_features), dim=0)\n            all_text_features = torch.cat(torch.distributed.nn.all_gather(text_features), dim=0)\n            # all_image_features = torch.cat(torch.distributed.nn.all_gather(image_features, async_op=True), dim=0)\n            # all_text_features = torch.cat(torch.distributed.nn.all_gather(text_features, async_op=True), dim=0)\n        else:\n            gathered_image_features = [torch.zeros_like(image_features) for _ in range(world_size)]\n            gathered_text_features = [torch.zeros_like(text_features) for _ in range(world_size)]\n            dist.all_gather(gathered_image_features, image_features)\n            dist.all_gather(gathered_text_features, text_features)\n            if not local_loss:\n                # ensure grads for local rank when all_* features don't have a gradient\n                gathered_image_features[rank] = image_features\n                gathered_text_features[rank] = text_features\n            all_image_features = torch.cat(gathered_image_features, dim=0)\n            all_text_features = torch.cat(gathered_text_features, dim=0)\n\n    return all_image_features, all_text_features\n\n\nclass ClipLoss(nn.Module):\n\n    def __init__(\n        self,\n        local_loss=False,\n        gather_with_grad=False,\n        cache_labels=False,\n        rank=0,\n        world_size=1,\n        use_horovod=False,\n        smoothing=0.0,\n    ):\n        super().__init__()\n        self.local_loss = local_loss\n        self.gather_with_grad = gather_with_grad\n        self.cache_labels = cache_labels\n        self.rank = rank\n        self.world_size = world_size\n        self.use_horovod = use_horovod\n        self.label_smoothing_cross_entropy = LabelSmoothingCrossEntropy(smoothing=smoothing) if smoothing > 0 else None\n\n        # cache state\n        self.prev_num_logits = 0\n        self.labels = {}\n\n    def forward(self, image_features, text_features, logit_scale=1.0):\n        device = image_features.device\n        if self.world_size > 1:\n            all_image_features, all_text_features = gather_features(image_features, text_features, self.local_loss, self.gather_with_grad, self.rank, self.world_size, self.use_horovod)\n\n            if self.local_loss:\n                logits_per_image = logit_scale * image_features @ all_text_features.T\n                logits_per_text = logit_scale * text_features @ all_image_features.T\n            else:\n                logits_per_image = logit_scale * all_image_features @ all_text_features.T\n                logits_per_text = logits_per_image.T\n        else:\n            logits_per_image = logit_scale * image_features @ text_features.T\n            logits_per_text = logit_scale * text_features @ image_features.T\n        # calculated ground-truth and cache if enabled\n        num_logits = logits_per_image.shape[0]\n        if self.prev_num_logits != num_logits or device not in self.labels:\n            labels = torch.arange(num_logits, device=device, dtype=torch.long)\n            if self.world_size > 1 and self.local_loss:\n                labels = labels + num_logits * self.rank\n            if self.cache_labels:\n                self.labels[device] = labels\n                self.prev_num_logits = num_logits\n        else:\n            labels = self.labels[device]\n\n        if self.label_smoothing_cross_entropy:\n            total_loss = (self.label_smoothing_cross_entropy(logits_per_image, labels) + self.label_smoothing_cross_entropy(logits_per_text, labels)) / 2\n        else:\n            total_loss = (F.cross_entropy(logits_per_image, labels) + F.cross_entropy(logits_per_text, labels)) / 2\n\n        acc = None\n        i2t_acc = (logits_per_image.argmax(-1) == labels).sum() / len(logits_per_image)\n        t2i_acc = (logits_per_text.argmax(-1) == labels).sum() / len(logits_per_text)\n        acc = {\"i2t\": i2t_acc, \"t2i\": t2i_acc}\n        return total_loss, acc\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/builder.py", "content": "import os\nfrom .clip_encoder import CLIPVisionTower\nfrom .imagebind import ImageBindWrapper\nfrom .open_clip_encoder import OpenCLIPVisionTower\nfrom .hf_vision import HFVisionTower\nfrom .siglip_encoder import SigLipVisionTower\nfrom .clip_encoder import CLIPVisionTower, CLIPVisionTowerS2\n\n# from .eva_clip.eva_clip_encoder import EvaClipVisionTower\n# from .dev_eva_clip.eva_vit import EvaViTWrapper\n\n\ndef build_vision_tower(vision_tower_cfg, **kwargs):\n    vision_tower = getattr(vision_tower_cfg, \"mm_vision_tower\", getattr(vision_tower_cfg, \"vision_tower\", None))\n    is_absolute_path_exists = os.path.exists(vision_tower)\n    use_s2 = getattr(vision_tower_cfg, \"s2\", False)\n    if vision_tower.endswith(\"clip-vit-large-patch14-336\") or vision_tower.startswith(\"openai\") or vision_tower.startswith(\"laion\") or \"ShareGPT4V\" in vision_tower:\n        if use_s2:\n            return CLIPVisionTowerS2(vision_tower, args=vision_tower_cfg, **kwargs)\n        else:\n            return CLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n    elif vision_tower.endswith(\"siglip-so400m-patch14-384\") or \"siglip\" in vision_tower:\n        return SigLipVisionTower(vision_tower, vision_tower_cfg=vision_tower_cfg, **kwargs)\n    elif vision_tower.startswith(\"hf:\"):\n        return HFVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n    elif vision_tower in [\"imagebind_huge\"]:\n        return ImageBindWrapper(vision_tower, args=vision_tower_cfg, **kwargs)\n    elif vision_tower.startswith(\"open_clip_hub\"):\n        return OpenCLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n    # elif \"internal-eva\" in vision_tower.lower() or \"eva02\" in vision_tower.lower():\n    #     return EvaClipVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n    # elif vision_tower in [\"EVA-CLIP-8B\", \"EVA-CLIP-8B-plus\"]:\n    #     return EvaViTWrapper(vision_tower, args=vision_tower_cfg, **kwargs)\n\n    raise ValueError(f\"Unknown vision tower: {vision_tower}\")\n"}
{"type": "source_file", "path": "llava/model/make_delta.py", "content": "\"\"\"\nUsage:\npython3 -m llava.model.make_delta --base ~/model_weights/llama-7b --target ~/model_weights/llava-7b --delta ~/model_weights/llava-7b-delta --hub-repo-id liuhaotian/llava-7b-delta\n\"\"\"\n\nimport argparse\n\nimport torch\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom llava.model.utils import auto_upgrade\n\n\ndef make_delta(base_model_path, target_model_path, delta_path, hub_repo_id):\n    print(\"Loading base model\")\n    base = AutoModelForCausalLM.from_pretrained(base_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n\n    print(\"Loading target model\")\n    auto_upgrade(target_model_path)\n    target = AutoModelForCausalLM.from_pretrained(target_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n\n    print(\"Calculating delta\")\n    for name, param in tqdm(target.state_dict().items(), desc=\"Calculating delta\"):\n        if name not in base.state_dict():\n            assert name in [\"model.mm_projector.weight\", \"model.mm_projector.bias\"], f\"{name} not in base model\"\n            continue\n        if param.data.shape == base.state_dict()[name].shape:\n            param.data -= base.state_dict()[name]\n        else:\n            assert name in [\"model.embed_tokens.weight\", \"lm_head.weight\"], f\"{name} dimension mismatch: {param.data.shape} vs {base.state_dict()[name].shape}\"\n            bparam = base.state_dict()[name]\n            param.data[: bparam.shape[0], : bparam.shape[1]] -= bparam\n\n    print(\"Saving delta\")\n    if hub_repo_id:\n        kwargs = {\"push_to_hub\": True, \"repo_id\": hub_repo_id}\n    else:\n        kwargs = {}\n    target.save_pretrained(delta_path, **kwargs)\n    target_tokenizer = AutoTokenizer.from_pretrained(target_model_path)\n    target_tokenizer.save_pretrained(delta_path, **kwargs)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--base-model-path\", type=str, required=True)\n    parser.add_argument(\"--target-model-path\", type=str, required=True)\n    parser.add_argument(\"--delta-path\", type=str, required=True)\n    parser.add_argument(\"--hub-repo-id\", type=str, default=None)\n    args = parser.parse_args()\n\n    make_delta(args.base_model_path, args.target_model_path, args.delta_path, args.hub_repo_id)\n"}
{"type": "source_file", "path": "llava/model/language_model/llava_qwen.py", "content": "#    Copyright 2024 Hao Zhang\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom typing import List, Optional, Tuple, Union, Dict\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\n\nimport transformers\nfrom transformers import AutoConfig, AutoModelForCausalLM, LlamaConfig, LlamaModel, LlamaForCausalLM\n\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nfrom transformers.generation.utils import GenerateOutput\n\n# from ...constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\nfrom llava.model.llava_arch import LlavaMetaModel, LlavaMetaForCausalLM\nfrom transformers import Qwen2Config, Qwen2Model, Qwen2ForCausalLM\n\n# from .qwen.modeling_qwen import QWenLMHeadModel, QWenModel\n# from .qwen.configuration_qwen import QWenConfig\n\n\nclass LlavaQwenConfig(Qwen2Config):\n    model_type = \"llava_qwen\"\n\n\nclass LlavaQwenModel(LlavaMetaModel, Qwen2Model):\n    config_class = LlavaQwenConfig\n\n    def __init__(self, config: Qwen2Config):\n        super(LlavaQwenModel, self).__init__(config)\n\n\nclass LlavaQwenForCausalLM(Qwen2ForCausalLM, LlavaMetaForCausalLM):\n    config_class = LlavaQwenConfig\n\n    def __init__(self, config):\n        # super(Qwen2ForCausalLM, self).__init__(config)\n        Qwen2ForCausalLM.__init__(self, config)\n        config.model_type = \"llava_qwen\"\n        config.rope_scaling = None\n\n        self.model = LlavaQwenModel(config)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.model\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        images: Optional[torch.FloatTensor] = None,\n        image_sizes: Optional[List[List[int]]] = None,\n        return_dict: Optional[bool] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        dpo_forward: Optional[bool] = False,\n        cache_position=None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n\n        if inputs_embeds is None:\n            (input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels) = self.prepare_inputs_labels_for_multimodal(input_ids, position_ids, attention_mask, past_key_values, labels, images, modalities, image_sizes)\n\n        if dpo_forward:\n            outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n            hidden_states = outputs[0]\n            logits = self.lm_head(hidden_states)\n            return logits, labels\n\n        else:\n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                labels=labels,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs: Optional[torch.Tensor] = None,\n        images: Optional[torch.Tensor] = None,\n        image_sizes: Optional[torch.Tensor] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        **kwargs,\n    ) -> Union[GenerateOutput, torch.LongTensor]:\n        position_ids = kwargs.pop(\"position_ids\", None)\n        attention_mask = kwargs.pop(\"attention_mask\", None)\n        if \"inputs_embeds\" in kwargs:\n            raise NotImplementedError(\"`inputs_embeds` is not supported\")\n\n        if images is not None:\n            (inputs, position_ids, attention_mask, _, inputs_embeds, _) = self.prepare_inputs_labels_for_multimodal(inputs, position_ids, attention_mask, None, None, images, modalities, image_sizes=image_sizes)\n        else:\n            inputs_embeds = self.get_model().embed_tokens(inputs)\n\n        return super().generate(position_ids=position_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, **kwargs)\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n        images = kwargs.pop(\"images\", None)\n        image_sizes = kwargs.pop(\"image_sizes\", None)\n        inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs)\n        if images is not None:\n            inputs[\"images\"] = images\n        if image_sizes is not None:\n            inputs[\"image_sizes\"] = image_sizes\n        return inputs\n\n\nAutoConfig.register(\"llava_qwen\", LlavaQwenConfig)\nAutoModelForCausalLM.register(LlavaQwenConfig, LlavaQwenForCausalLM)\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/eva_vit_model.py", "content": "# --------------------------------------------------------\n# Adapted from  https://github.com/microsoft/unilm/tree/master/beit\n# --------------------------------------------------------\nimport math\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntry:\n    from timm.models.layers import drop_path, to_2tuple, trunc_normal_\nexcept:\n    from timm.layers import drop_path, to_2tuple, trunc_normal_\n\nfrom .transformer import PatchDropout\nfrom .rope import VisionRotaryEmbedding, VisionRotaryEmbeddingFast\n\nif os.getenv(\"ENV_TYPE\") == \"deepspeed\":\n    try:\n        from deepspeed.runtime.activation_checkpointing.checkpointing import checkpoint\n    except:\n        from torch.utils.checkpoint import checkpoint\nelse:\n    from torch.utils.checkpoint import checkpoint\n\ntry:\n    import xformers.ops as xops\nexcept ImportError:\n    xops = None\n    # print(\"Please 'pip install xformers'\")\n\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n\n    def extra_repr(self) -> str:\n        return \"p={}\".format(self.drop_prob)\n\n\nclass Mlp(nn.Module):\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        drop=0.0,\n        subln=False,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n\n        self.ffn_ln = norm_layer(hidden_features) if subln else nn.Identity()\n\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        # x = self.drop(x)\n        # commit this for the orignal BERT implement\n        x = self.ffn_ln(x)\n\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass SwiGLU(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.SiLU, drop=0.0, norm_layer=nn.LayerNorm, subln=False):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n\n        self.w1 = nn.Linear(in_features, hidden_features)\n        self.w2 = nn.Linear(in_features, hidden_features)\n\n        self.act = act_layer()\n        self.ffn_ln = norm_layer(hidden_features) if subln else nn.Identity()\n        self.w3 = nn.Linear(hidden_features, out_features)\n\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x1 = self.w1(x)\n        x2 = self.w2(x)\n        hidden = self.act(x1) * x2\n        x = self.ffn_ln(hidden)\n        x = self.w3(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.0, proj_drop=0.0, window_size=None, attn_head_dim=None, xattn=False, rope=None, subln=False, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        if attn_head_dim is not None:\n            head_dim = attn_head_dim\n        all_head_dim = head_dim * self.num_heads\n        self.scale = qk_scale or head_dim**-0.5\n\n        self.subln = subln\n        if self.subln:\n            self.q_proj = nn.Linear(dim, all_head_dim, bias=False)\n            self.k_proj = nn.Linear(dim, all_head_dim, bias=False)\n            self.v_proj = nn.Linear(dim, all_head_dim, bias=False)\n        else:\n            self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n\n        if qkv_bias:\n            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n        else:\n            self.q_bias = None\n            self.v_bias = None\n\n        if window_size:\n            self.window_size = window_size\n            self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n            self.relative_position_bias_table = nn.Parameter(torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n            # cls to token & token 2 cls & cls to cls\n\n            # get pair-wise relative position index for each token inside the window\n            coords_h = torch.arange(window_size[0])\n            coords_w = torch.arange(window_size[1])\n            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n            relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n            relative_coords[:, :, 1] += window_size[1] - 1\n            relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n            relative_position_index = torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n            relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n            relative_position_index[0, 0:] = self.num_relative_distance - 3\n            relative_position_index[0:, 0] = self.num_relative_distance - 2\n            relative_position_index[0, 0] = self.num_relative_distance - 1\n\n            self.register_buffer(\"relative_position_index\", relative_position_index)\n        else:\n            self.window_size = None\n            self.relative_position_bias_table = None\n            self.relative_position_index = None\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.inner_attn_ln = norm_layer(all_head_dim) if subln else nn.Identity()\n        # self.proj = nn.Linear(all_head_dim, all_head_dim)\n        self.proj = nn.Linear(all_head_dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.xattn = xattn\n        self.xattn_drop = attn_drop\n\n        self.rope = rope\n\n    def forward(self, x, rel_pos_bias=None, attn_mask=None):\n        B, N, C = x.shape\n        if self.subln:\n            q = F.linear(input=x, weight=self.q_proj.weight, bias=self.q_bias)\n            k = F.linear(input=x, weight=self.k_proj.weight, bias=None)\n            v = F.linear(input=x, weight=self.v_proj.weight, bias=self.v_bias)\n\n            q = q.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)  # B, num_heads, N, C\n            k = k.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n            v = v.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n        else:\n\n            qkv_bias = None\n            if self.q_bias is not None:\n                qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n\n            qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n            qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)  # 3, B, num_heads, N, C\n            q, k, v = qkv[0], qkv[1], qkv[2]\n\n        if self.rope:\n            # slightly fast impl\n            q_t = q[:, :, 1:, :]\n            ro_q_t = self.rope(q_t)\n            q = torch.cat((q[:, :, :1, :], ro_q_t), -2).type_as(v)\n\n            k_t = k[:, :, 1:, :]\n            ro_k_t = self.rope(k_t)\n            k = torch.cat((k[:, :, :1, :], ro_k_t), -2).type_as(v)\n\n        if self.xattn:\n            q = q.permute(0, 2, 1, 3)  # B, num_heads, N, C -> B, N, num_heads, C\n            k = k.permute(0, 2, 1, 3)\n            v = v.permute(0, 2, 1, 3)\n\n            x = xops.memory_efficient_attention(\n                q,\n                k,\n                v,\n                p=self.xattn_drop,\n                scale=self.scale,\n            )\n            x = x.reshape(B, N, -1)\n            x = self.inner_attn_ln(x)\n            x = self.proj(x)\n            x = self.proj_drop(x)\n        else:\n            q = q * self.scale\n            attn = q @ k.transpose(-2, -1)\n\n            if self.relative_position_bias_table is not None:\n                relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(self.window_size[0] * self.window_size[1] + 1, self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n                relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n                attn = attn + relative_position_bias.unsqueeze(0).type_as(attn)\n\n            if rel_pos_bias is not None:\n                attn = attn + rel_pos_bias.type_as(attn)\n\n            if attn_mask is not None:\n                attn_mask = attn_mask.bool()\n                attn = attn.masked_fill(~attn_mask[:, None, None, :], float(\"-inf\"))\n\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n\n            x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n            x = self.inner_attn_ln(x)\n            x = self.proj(x)\n            x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        init_values=None,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        window_size=None,\n        attn_head_dim=None,\n        xattn=False,\n        rope=None,\n        postnorm=False,\n        subln=False,\n        naiveswiglu=False,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim, xattn=xattn, rope=rope, subln=subln, norm_layer=norm_layer\n        )\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n\n        if naiveswiglu:\n            self.mlp = SwiGLU(\n                in_features=dim,\n                hidden_features=mlp_hidden_dim,\n                subln=subln,\n                norm_layer=norm_layer,\n            )\n        else:\n            self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, subln=subln, drop=drop)\n\n        if init_values is not None and init_values > 0:\n            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n        else:\n            self.gamma_1, self.gamma_2 = None, None\n\n        self.postnorm = postnorm\n\n    def forward(self, x, rel_pos_bias=None, attn_mask=None):\n        if self.gamma_1 is None:\n            if self.postnorm:\n                x = x + self.drop_path(self.norm1(self.attn(x, rel_pos_bias=rel_pos_bias, attn_mask=attn_mask)))\n                x = x + self.drop_path(self.norm2(self.mlp(x)))\n            else:\n                x = x + self.drop_path(self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias, attn_mask=attn_mask))\n                x = x + self.drop_path(self.mlp(self.norm2(x)))\n        else:\n            if self.postnorm:\n                x = x + self.drop_path(self.gamma_1 * self.norm1(self.attn(x, rel_pos_bias=rel_pos_bias, attn_mask=attn_mask)))\n                x = x + self.drop_path(self.gamma_2 * self.norm2(self.mlp(x)))\n            else:\n                x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias, attn_mask=attn_mask))\n                x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n        return x\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\"Image to Patch Embedding\"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x, **kwargs):\n        B, C, H, W = x.shape\n        # FIXME look at relaxing size constraints\n        assert H == self.img_size[0] and W == self.img_size[1], f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x\n\n\nclass RelativePositionBias(nn.Module):\n\n    def __init__(self, window_size, num_heads):\n        super().__init__()\n        self.window_size = window_size\n        self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n        self.relative_position_bias_table = nn.Parameter(torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n        # cls to token & token 2 cls & cls to cls\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(window_size[0])\n        coords_w = torch.arange(window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n        relative_position_index = torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n        relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        relative_position_index[0, 0:] = self.num_relative_distance - 3\n        relative_position_index[0:, 0] = self.num_relative_distance - 2\n        relative_position_index[0, 0] = self.num_relative_distance - 1\n\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n    def forward(self):\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(self.window_size[0] * self.window_size[1] + 1, self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n        return relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n\n\nclass EVAVisionTransformer(nn.Module):\n    \"\"\"Vision Transformer with support for patch or hybrid CNN input stage\"\"\"\n\n    def __init__(\n        self,\n        img_size=224,\n        patch_size=16,\n        in_chans=3,\n        num_classes=1000,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.0,\n        norm_layer=nn.LayerNorm,\n        init_values=None,\n        patch_dropout=0.0,\n        use_abs_pos_emb=True,\n        use_rel_pos_bias=False,\n        use_shared_rel_pos_bias=False,\n        rope=False,\n        use_mean_pooling=True,\n        init_scale=0.001,\n        grad_checkpointing=False,\n        xattn=False,\n        postnorm=False,\n        pt_hw_seq_len=16,\n        intp_freq=False,\n        naiveswiglu=False,\n        subln=False,\n    ):\n        super().__init__()\n        self.image_size = img_size\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n\n        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        # self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        if use_abs_pos_emb:\n            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        else:\n            self.pos_embed = None\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        if use_shared_rel_pos_bias:\n            self.rel_pos_bias = RelativePositionBias(window_size=self.patch_embed.patch_shape, num_heads=num_heads)\n        else:\n            self.rel_pos_bias = None\n\n        if rope:\n            half_head_dim = embed_dim // num_heads // 2\n            hw_seq_len = img_size // patch_size\n            self.rope = VisionRotaryEmbeddingFast(\n                dim=half_head_dim,\n                pt_seq_len=pt_hw_seq_len,\n                ft_seq_len=hw_seq_len if intp_freq else None,\n                # patch_dropout=patch_dropout\n            )\n        else:\n            self.rope = None\n\n        self.naiveswiglu = naiveswiglu\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        self.use_rel_pos_bias = use_rel_pos_bias\n        self.blocks = nn.ModuleList(\n            [\n                Block(\n                    dim=embed_dim,\n                    num_heads=num_heads,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    drop=drop_rate,\n                    attn_drop=attn_drop_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                    init_values=init_values,\n                    window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None,\n                    xattn=xattn,\n                    rope=self.rope,\n                    postnorm=postnorm,\n                    subln=subln,\n                    naiveswiglu=naiveswiglu,\n                )\n                for i in range(depth)\n            ]\n        )\n        self.norm = nn.Identity() if use_mean_pooling else norm_layer(embed_dim)\n        self.fc_norm = norm_layer(embed_dim) if use_mean_pooling else None\n        self.head = nn.Linear(embed_dim, num_classes, bias=qkv_bias) if num_classes > 0 else nn.Identity()\n\n        if self.pos_embed is not None:\n            trunc_normal_(self.pos_embed, std=0.02)\n\n        trunc_normal_(self.cls_token, std=0.02)\n\n        self.apply(self._init_weights)\n        self.fix_init_weight()\n\n        if isinstance(self.head, nn.Linear):\n            trunc_normal_(self.head.weight, std=0.02)\n            self.head.weight.data.mul_(init_scale)\n            if self.head.bias is not None:\n                self.head.bias.data.mul_(init_scale)\n\n        # setting a patch_dropout of 0. would mean it is disabled and this function would be the identity fn\n        self.patch_dropout = PatchDropout(patch_dropout) if patch_dropout > 0.0 else nn.Identity()\n\n        self.grad_checkpointing = grad_checkpointing\n\n    def fix_init_weight(self):\n        def rescale(param, layer_id):\n            param.div_(math.sqrt(2.0 * layer_id))\n\n        for layer_id, layer in enumerate(self.blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            if self.naiveswiglu:\n                rescale(layer.mlp.w3.weight.data, layer_id + 1)\n            else:\n                rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n\n    def get_cast_dtype(self) -> torch.dtype:\n        return self.blocks[0].mlp.fc2.weight.dtype\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def get_num_layers(self):\n        return len(self.blocks)\n\n    def lock(self, unlocked_groups=0, freeze_bn_stats=False):\n        assert unlocked_groups == 0, \"partial locking not currently supported for this model\"\n        for param in self.parameters():\n            param.requires_grad = False\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\"pos_embed\", \"cls_token\"}\n\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=\"\"):\n        self.num_classes = num_classes\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x, return_all_features=False):\n\n        x = self.patch_embed(x)\n        batch_size, seq_len, _ = x.size()\n\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        x = torch.cat((cls_tokens, x), dim=1)\n        if self.pos_embed is not None:\n            x = x + self.pos_embed\n        x = self.pos_drop(x)\n\n        # a patch_dropout of 0. would mean it is disabled and this function would do nothing but return what was passed in\n        # if os.getenv(\"RoPE\") == \"1\":\n        #     if self.training and not isinstance(self.patch_dropout, nn.Identity):\n        #         x, patch_indices_keep = self.patch_dropout(x)\n        #         self.rope.forward = partial(self.rope.forward, patch_indices_keep=patch_indices_keep)\n        #     else:\n        #         self.rope.forward = partial(self.rope.forward, patch_indices_keep=None)\n        #         x = self.patch_dropout(x)\n        # else:\n        x = self.patch_dropout(x)\n\n        rel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None\n        for blk in self.blocks:\n            if self.grad_checkpointing:\n                x = checkpoint(blk, x, (rel_pos_bias,))\n            else:\n                x = blk(x, rel_pos_bias=rel_pos_bias)\n\n        if not return_all_features:\n            x = self.norm(x)\n            if self.fc_norm is not None:\n                return self.fc_norm(x.mean(1))\n            else:\n                return x[:, 0]\n        return x\n\n    def forward(self, x, return_all_features=False):\n        if return_all_features:\n            return self.forward_features(x, return_all_features)\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x\n"}
{"type": "source_file", "path": "llava/model/llava_arch.py", "content": "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom abc import ABC, abstractmethod\n\nimport math\nimport re\nimport time\nimport torch\nimport torch.nn as nn\nfrom .multimodal_encoder.builder import build_vision_tower\nfrom .multimodal_resampler.builder import build_vision_resampler\nfrom .multimodal_projector.builder import build_vision_projector\n\nfrom llava.constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n\nfrom llava.mm_utils import get_anyres_image_grid_shape\nfrom llava.utils import rank0_print, rank_print\nimport random\n\n\nclass LlavaMetaModel:\n\n    def __init__(self, config):\n        super(LlavaMetaModel, self).__init__(config)\n\n        if hasattr(config, \"mm_vision_tower\"):\n            delay_load = getattr(config, \"delay_load\", False)\n            self.vision_tower = build_vision_tower(config, delay_load=delay_load)\n            self.vision_resampler = build_vision_resampler(config, vision_tower=self.vision_tower)\n            self.mm_projector = build_vision_projector(config, vision_cfg=self.vision_tower.config)\n\n            if \"unpad\" in getattr(config, \"mm_patch_merge_type\", \"\"):\n                self.image_newline = nn.Parameter(torch.empty(config.hidden_size, dtype=self.dtype))\n\n    def get_vision_tower(self):\n        vision_tower = getattr(self, \"vision_tower\", None)\n        if type(vision_tower) is list:\n            vision_tower = vision_tower[0]\n        return vision_tower\n\n    def initialize_vision_modules(self, model_args, fsdp=None):\n        vision_tower = model_args.vision_tower\n        mm_vision_select_layer = model_args.mm_vision_select_layer\n        mm_vision_select_feature = model_args.mm_vision_select_feature\n        pretrain_mm_mlp_adapter = model_args.pretrain_mm_mlp_adapter\n        mm_patch_merge_type = model_args.mm_patch_merge_type\n        pretrain_vision_tower = model_args.pretrain_vision_tower\n\n        self.config.mm_vision_tower = vision_tower\n        self.config.vision_tower_pretrained = getattr(model_args, \"vision_tower_pretrained\", \"\")\n\n        if self.get_vision_tower() is None:\n            vision_tower = build_vision_tower(model_args)\n            vision_resampler = build_vision_resampler(model_args, vision_tower=vision_tower)\n            for k, v in vision_resampler.config.items():\n                setattr(self.config, k, v)\n\n            if fsdp is not None and len(fsdp) > 0:\n                self.vision_tower = [vision_tower]\n                self.vision_resampler = [vision_resampler]\n            else:\n                self.vision_tower = vision_tower\n                self.vision_resampler = vision_resampler\n        else:\n            if fsdp is not None and len(fsdp) > 0:\n                vision_resampler = self.vision_resampler[0]\n                vision_tower = self.vision_tower[0]\n            else:\n                vision_resampler = self.vision_resampler\n                vision_tower = self.vision_tower\n            vision_tower.load_model()\n\n            # In case it is frozen by LoRA\n            for p in self.vision_resampler.parameters():\n                p.requires_grad = True\n\n        self.config.use_mm_proj = True\n        self.config.mm_projector_type = getattr(model_args, \"mm_projector_type\", \"linear\")\n        self.config.mm_hidden_size = getattr(vision_resampler, \"hidden_size\", vision_tower.hidden_size)\n        self.config.mm_vision_select_layer = mm_vision_select_layer\n        self.config.mm_vision_select_feature = mm_vision_select_feature\n        self.config.mm_patch_merge_type = mm_patch_merge_type\n\n        if getattr(self, \"mm_projector\", None) is None:\n            self.mm_projector = build_vision_projector(self.config, vision_cfg=vision_tower.config)\n\n            if \"unpad\" in mm_patch_merge_type:\n                embed_std = 1 / torch.sqrt(torch.tensor(self.config.hidden_size, dtype=self.dtype))\n                self.image_newline = nn.Parameter(torch.randn(self.config.hidden_size, dtype=self.dtype) * embed_std)\n        else:\n            # In case it is frozen by LoRA\n            for p in self.mm_projector.parameters():\n                p.requires_grad = True\n\n        if pretrain_mm_mlp_adapter is not None:\n            mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location=\"cpu\")\n\n            def get_w(weights, keyword):\n                return {k.split(keyword + \".\")[1]: v for k, v in weights.items() if keyword in k}\n\n            incompatible_keys = self.mm_projector.load_state_dict(get_w(mm_projector_weights, \"mm_projector\"))\n            rank0_print(f\"Loaded mm projector weights from {pretrain_mm_mlp_adapter}. Incompatible keys: {incompatible_keys}\")\n            incompatible_keys = self.vision_resampler.load_state_dict(get_w(mm_projector_weights, \"vision_resampler\"), strict=False)\n            rank0_print(f\"Loaded vision resampler weights from {pretrain_mm_mlp_adapter}. Incompatible keys: {incompatible_keys}\")\n\n        # if pretrain_vision_tower is not None:\n        #     vision_tower_weights = torch.load(pretrain_vision_tower, map_location='cpu')\n        #     new_vision_tower_weights = {}\n        #     for key in vision_tower_weights.keys():\n        #         new_key = key.replace('model.vision_tower.vision_tower.', 'vision_tower.')  # \n        #         new_vision_tower_weights[new_key] = vision_tower_weights[key]\n        #     incompatible_keys = self.vision_tower.load_state_dict(new_vision_tower_weights)\n        #     rank0_print(f\"Loaded vision tower weights from {pretrain_vision_tower}. Incompatible keys: {incompatible_keys}\")\n\ndef unpad_image(tensor, original_size):\n    \"\"\"\n    Unpads a PyTorch tensor of a padded and resized image.\n\n    Args:\n    tensor (torch.Tensor): The image tensor, assumed to be in CxHxW format.\n    original_size (tuple): The original size of the image (height, width).\n\n    Returns:\n    torch.Tensor: The unpadded image tensor.\n    \"\"\"\n    original_width, original_height = original_size\n    current_height, current_width = tensor.shape[1:]\n\n    # Compute aspect ratios\n    original_aspect_ratio = original_width / original_height\n    current_aspect_ratio = current_width / current_height\n\n    # Determine padding size and direction\n    if original_aspect_ratio > current_aspect_ratio:\n        # Padding was added to the height\n        scale_factor = current_width / original_width\n        new_height = int(original_height * scale_factor)\n        padding = (current_height - new_height) // 2\n        unpadded_tensor = tensor[:, padding : current_height - padding, :]\n    else:\n        # Padding was added to the width\n        scale_factor = current_height / original_height\n        new_width = int(original_width * scale_factor)\n        padding = (current_width - new_width) // 2\n        unpadded_tensor = tensor[:, :, padding : current_width - padding]\n\n    return unpadded_tensor\n\n\nclass LlavaMetaForCausalLM(ABC):\n\n    @abstractmethod\n    def get_model(self):\n        pass\n\n    def get_vision_tower(self):\n        return self.get_model().get_vision_tower()\n\n    def get_2dPool(self, image_feature):\n        height = width = self.get_vision_tower().num_patches_per_side\n        num_frames, num_tokens, num_dim = image_feature.shape\n        image_feature = image_feature.view(num_frames, height, width, -1)\n        image_feature = image_feature.permute(0, 3, 1, 2).contiguous()\n        # image_feature = nn.functional.max_pool2d(image_feature, self.config.mm_spatial_pool_stride)\n        if self.config.mm_spatial_pool_mode == \"average\":\n            image_feature = nn.functional.avg_pool2d(image_feature, self.config.mm_spatial_pool_stride)\n        elif self.config.mm_spatial_pool_mode == \"max\":\n            image_feature = nn.functional.max_pool2d(image_feature, self.config.mm_spatial_pool_stride)\n        elif self.config.mm_spatial_pool_mode == \"bilinear\":\n            height, weight = image_feature.shape[2:]\n            scaled_shape = [math.ceil(height / 2), math.ceil(weight / 2)]\n            image_feature = nn.functional.interpolate(image_feature, size=scaled_shape, mode='bilinear')\n\n        else:\n            raise ValueError(f\"Unexpected mm_spatial_pool_mode: {self.config.mm_spatial_pool_mode}\")\n        image_feature = image_feature.permute(0, 2, 3, 1)\n        image_feature = image_feature.view(num_frames, -1, num_dim)\n        return image_feature\n\n    def encode_images(self, images):\n        image_features = self.get_model().get_vision_tower()(images)\n        # image_features = self.get_model().vision_resampler(image_features, images=images)\n        image_features = self.get_model().mm_projector(image_features)\n        return image_features\n    \n    def encode_multimodals(self, videos_or_images, video_idx_in_batch, split_sizes=None):\n        videos_or_images_features = self.get_model().get_vision_tower()(videos_or_images)\n        per_videos_or_images_features = torch.split(videos_or_images_features, split_sizes, dim=0)  # tuple, (dim_1, 576, 4096)\n        all_videos_or_images_features = []\n\n        for idx, feat in enumerate(per_videos_or_images_features):\n            feat = self.get_model().mm_projector(feat)\n            if idx in video_idx_in_batch:\n                feat = self.get_2dPool(feat)\n            all_videos_or_images_features.append(feat)\n        return all_videos_or_images_features\n\n    def prepare_inputs_labels_for_multimodal(self, input_ids, position_ids, attention_mask, past_key_values, labels, images, modalities=[\"image\"], image_sizes=None):\n        vision_tower = self.get_vision_tower()\n        # rank_print(modalities)\n        if vision_tower is None or images is None or input_ids.shape[1] == 1:\n            return input_ids, position_ids, attention_mask, past_key_values, None, labels\n\n        if type(images) is list or images.ndim == 5:\n            if type(images) is list:\n                images = [x.unsqueeze(0) if x.ndim == 3 else x for x in images]\n\n            video_idx_in_batch = []\n            for _ in range(len(modalities)):\n                if modalities[_] == \"video\":\n                    video_idx_in_batch.append(_)\n\n            images_list = []\n            for image in images:\n                if image.ndim == 4:\n                    images_list.append(image)\n                else:\n                    images_list.append(image.unsqueeze(0))\n\n            concat_images = torch.cat([image for image in images_list], dim=0)\n            split_sizes = [image.shape[0] for image in images_list]\n            encoded_image_features = self.encode_images(concat_images)\n\n            # This is a list, each element is [num_images, patch * patch, dim]\n            # rank_print(f\"Concat images : {concat_images.shape}\")\n            encoded_image_features = torch.split(encoded_image_features, split_sizes)\n            image_features = []\n            for idx, image_feat in enumerate(encoded_image_features):\n                if idx in video_idx_in_batch:\n                    image_features.append(self.get_2dPool(image_feat))\n                else:\n                    image_features.append(image_feat)\n            # image_features = self.encode_multimodals(concat_images, video_idx_in_batch, split_sizes)\n            # rank_print(f\"Encoded image feats : {[x.shape for x in image_features]}\")\n            # image_features = torch.split(image_features, split_sizes, dim=0)\n            mm_patch_merge_type = getattr(self.config, \"mm_patch_merge_type\", \"flat\")\n            image_aspect_ratio = getattr(self.config, \"image_aspect_ratio\", \"square\")\n\n            if mm_patch_merge_type == \"flat\":\n                image_features = [x.flatten(0, 1) for x in image_features]\n\n            elif mm_patch_merge_type.startswith(\"spatial\"):\n                new_image_features = []\n                for image_idx, image_feature in enumerate(image_features):\n\n                    # FIXME: now assume the image is square, and split to 2x2 patches\n                    # num_patches = h * w, where h = w = sqrt(num_patches)\n                    # currently image_feature is a tensor of shape (4, num_patches, hidden_size)\n                    # we want to first unflatten it to (2, 2, h, w, hidden_size)\n                    # rank0_print(\"At least we are reaching here\")\n                    if image_idx in video_idx_in_batch:  # video operations\n                        # rank0_print(\"Video\")\n                        if \"unpad\" in mm_patch_merge_type:\n                            # image_feature = image_feature.permute(2, 0, 1).contiguous()\n                            # image_feature =  torch.cat((image_feature, self.model.image_newline[:, None, None].expand(*image_feature.shape[:-1], 1).to(image_feature.device)), dim=-1)\n                            # image_feature = image_feature.permute(1, 2, 0).contiguous()\n                            image_feature = image_feature.flatten(0, 1)\n                            image_feature = torch.cat((image_feature, self.model.image_newline[None].to(image_feature.device)), dim=0)\n\n                    elif image_feature.shape[0] > 1:  # multi patches and multi images operations\n                        # rank0_print(\"Single-images\")\n                        base_image_feature = image_feature[0]\n                        image_feature = image_feature[1:]\n                        height = width = self.get_vision_tower().num_patches_per_side\n                        assert height * width == base_image_feature.shape[0]\n\n                        if \"anyres_max\" in image_aspect_ratio:\n                            matched_anyres_max_num_patches = re.match(r\"anyres_max_(\\d+)\", image_aspect_ratio)\n                            if matched_anyres_max_num_patches:\n                                max_num_patches = int(matched_anyres_max_num_patches.group(1))\n\n                        if image_aspect_ratio == \"anyres\" or \"anyres_max\" in image_aspect_ratio:\n\n                            if hasattr(self.get_vision_tower(), \"image_size\"):\n                                vision_tower_image_size = self.get_vision_tower().image_size\n                            else:\n                                raise ValueError(\"vision_tower_image_size is not found in the vision tower.\")\n                            try:\n                                num_patch_width, num_patch_height = get_anyres_image_grid_shape(image_sizes[image_idx], self.config.image_grid_pinpoints, vision_tower_image_size)\n                            except Exception as e:\n                                rank0_print(f\"Error: {e}\")\n                                num_patch_width, num_patch_height = 2, 2\n                            image_feature = image_feature.view(num_patch_height, num_patch_width, height, width, -1)\n                        else:\n                            image_feature = image_feature.view(2, 2, height, width, -1)\n\n                        if \"maxpool2x2\" in mm_patch_merge_type:\n                            image_feature = image_feature.permute(4, 0, 2, 1, 3).contiguous()\n                            image_feature = image_feature.flatten(1, 2).flatten(2, 3)\n                            image_feature = nn.functional.max_pool2d(image_feature, 2)\n                            image_feature = image_feature.flatten(1, 2).transpose(0, 1)\n                        elif \"unpad\" in mm_patch_merge_type and \"anyres_max\" in image_aspect_ratio and matched_anyres_max_num_patches:\n                            unit = image_feature.shape[2]\n                            image_feature = image_feature.permute(4, 0, 2, 1, 3).contiguous()\n                            image_feature = image_feature.flatten(1, 2).flatten(2, 3)\n                            image_feature = unpad_image(image_feature, image_sizes[image_idx])\n                            c, h, w = image_feature.shape\n                            times = math.sqrt(h * w / (max_num_patches * unit**2))\n                            if times > 1.1:\n                                image_feature = image_feature[None]\n                                image_feature = nn.functional.interpolate(image_feature, [int(h // times), int(w // times)], mode=\"bilinear\")[0]\n                            image_feature = torch.cat((image_feature, self.model.image_newline[:, None, None].expand(*image_feature.shape[:-1], 1).to(image_feature.device)), dim=-1)\n                            image_feature = image_feature.flatten(1, 2).transpose(0, 1)\n                        elif \"unpad\" in mm_patch_merge_type:\n                            image_feature = image_feature.permute(4, 0, 2, 1, 3).contiguous()\n                            image_feature = image_feature.flatten(1, 2).flatten(2, 3)\n                            image_feature = unpad_image(image_feature, image_sizes[image_idx])\n                            image_feature = torch.cat((image_feature, self.model.image_newline[:, None, None].expand(*image_feature.shape[:-1], 1).to(image_feature.device)), dim=-1)\n                            image_feature = image_feature.flatten(1, 2).transpose(0, 1)\n                        else:\n                            image_feature = image_feature.permute(0, 2, 1, 3, 4).contiguous()\n                            image_feature = image_feature.flatten(0, 3)\n                        if \"nobase\" in mm_patch_merge_type:\n                            pass\n                        else:\n                            image_feature = torch.cat((base_image_feature, image_feature), dim=0)\n                    else:  # single image operations\n                        image_feature = image_feature[0]\n                        if \"unpad\" in mm_patch_merge_type:\n                            image_feature = torch.cat((image_feature, self.model.image_newline[None]), dim=0)\n\n                    new_image_features.append(image_feature)\n                image_features = new_image_features\n            else:\n                raise ValueError(f\"Unexpected mm_patch_merge_type: {self.config.mm_patch_merge_type}\")\n        else:\n            image_features = self.encode_images(images)\n\n        # TODO: image start / end is not implemented here to support pretraining.\n        if getattr(self.config, \"tune_mm_mlp_adapter\", False) and getattr(self.config, \"mm_use_im_start_end\", False):\n            raise NotImplementedError\n        # rank_print(f\"Total images : {len(image_features)}\")\n\n        # Let's just add dummy tensors if they do not exist,\n        # it is a headache to deal with None all the time.\n        # But it is not ideal, and if you have a better idea,\n        # please open an issue / submit a PR, thanks.\n        _labels = labels\n        _position_ids = position_ids\n        _attention_mask = attention_mask\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids, dtype=torch.bool)\n        else:\n            attention_mask = attention_mask.bool()\n        if position_ids is None:\n            position_ids = torch.arange(0, input_ids.shape[1], dtype=torch.long, device=input_ids.device)\n        if labels is None:\n            labels = torch.full_like(input_ids, IGNORE_INDEX)\n\n        # remove the padding using attention_mask -- FIXME\n        _input_ids = input_ids\n        input_ids = [cur_input_ids[cur_attention_mask] for cur_input_ids, cur_attention_mask in zip(input_ids, attention_mask)]\n        labels = [cur_labels[cur_attention_mask] for cur_labels, cur_attention_mask in zip(labels, attention_mask)]\n\n        new_input_embeds = []\n        new_labels = []\n        cur_image_idx = 0\n        # rank_print(\"Inserting Images embedding\")\n        for batch_idx, cur_input_ids in enumerate(input_ids):\n            num_images = (cur_input_ids == IMAGE_TOKEN_INDEX).sum()\n            # rank0_print(num_images)\n            if num_images == 0:\n                cur_image_features = image_features[cur_image_idx]\n                cur_input_embeds_1 = self.get_model().embed_tokens(cur_input_ids)\n                cur_input_embeds = torch.cat([cur_input_embeds_1, cur_image_features[0:0]], dim=0)\n                new_input_embeds.append(cur_input_embeds)\n                new_labels.append(labels[batch_idx])\n                cur_image_idx += 1\n                continue\n\n            image_token_indices = [-1] + torch.where(cur_input_ids == IMAGE_TOKEN_INDEX)[0].tolist() + [cur_input_ids.shape[0]]\n            cur_input_ids_noim = []\n            cur_labels = labels[batch_idx]\n            cur_labels_noim = []\n            for i in range(len(image_token_indices) - 1):\n                cur_input_ids_noim.append(cur_input_ids[image_token_indices[i] + 1 : image_token_indices[i + 1]])\n                cur_labels_noim.append(cur_labels[image_token_indices[i] + 1 : image_token_indices[i + 1]])\n            split_sizes = [x.shape[0] for x in cur_labels_noim]\n            cur_input_embeds = self.get_model().embed_tokens(torch.cat(cur_input_ids_noim))\n            cur_input_embeds_no_im = torch.split(cur_input_embeds, split_sizes, dim=0)\n            cur_new_input_embeds = []\n            cur_new_labels = []\n\n            for i in range(num_images + 1):\n                cur_new_input_embeds.append(cur_input_embeds_no_im[i])\n                cur_new_labels.append(cur_labels_noim[i])\n                if i < num_images:\n                    try:\n                        cur_image_features = image_features[cur_image_idx]\n                    except IndexError:\n                        cur_image_features = image_features[cur_image_idx - 1]\n                    cur_image_idx += 1\n                    cur_new_input_embeds.append(cur_image_features)\n                    cur_new_labels.append(torch.full((cur_image_features.shape[0],), IGNORE_INDEX, device=cur_labels.device, dtype=cur_labels.dtype))\n\n            cur_new_input_embeds = [x.to(self.device) for x in cur_new_input_embeds]\n\n            # import pdb; pdb.set_trace()\n            cur_new_input_embeds = torch.cat(cur_new_input_embeds)\n            cur_new_labels = torch.cat(cur_new_labels)\n\n            new_input_embeds.append(cur_new_input_embeds)\n            new_labels.append(cur_new_labels)\n\n        # Truncate sequences to max length as image embeddings can make the sequence longer\n        tokenizer_model_max_length = getattr(self.config, \"tokenizer_model_max_length\", None)\n        # rank_print(\"Finishing Inserting\")\n\n        new_input_embeds = [x[:tokenizer_model_max_length] for x, modality in zip(new_input_embeds, modalities)]\n        new_labels = [x[:tokenizer_model_max_length] for x, modality in zip(new_labels, modalities)]\n        # TODO: Hard code for control loss spike\n        # if tokenizer_model_max_length is not None:\n        #     new_input_embeds = [x[:4096] if modality != \"video\" else x[:tokenizer_model_max_length] for x, modality in zip(new_input_embeds, modalities)]\n        #     new_labels = [x[:4096] if modality != \"video\" else x[:tokenizer_model_max_length] for x, modality in zip(new_labels, modalities)]\n\n        # Combine them\n        max_len = max(x.shape[0] for x in new_input_embeds)\n        batch_size = len(new_input_embeds)\n\n        new_input_embeds_padded = []\n        new_labels_padded = torch.full((batch_size, max_len), IGNORE_INDEX, dtype=new_labels[0].dtype, device=new_labels[0].device)\n        attention_mask = torch.zeros((batch_size, max_len), dtype=attention_mask.dtype, device=attention_mask.device)\n        position_ids = torch.zeros((batch_size, max_len), dtype=position_ids.dtype, device=position_ids.device)\n        # rank0_print(\"Prepare pos id\")\n\n        for i, (cur_new_embed, cur_new_labels) in enumerate(zip(new_input_embeds, new_labels)):\n            cur_len = cur_new_embed.shape[0]\n            if getattr(self.config, \"tokenizer_padding_side\", \"right\") == \"left\":\n                new_input_embeds_padded.append(torch.cat((torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device), cur_new_embed), dim=0))\n                if cur_len > 0:\n                    new_labels_padded[i, -cur_len:] = cur_new_labels\n                    attention_mask[i, -cur_len:] = True\n                    position_ids[i, -cur_len:] = torch.arange(0, cur_len, dtype=position_ids.dtype, device=position_ids.device)\n            else:\n                new_input_embeds_padded.append(torch.cat((cur_new_embed, torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device)), dim=0))\n                if cur_len > 0:\n                    new_labels_padded[i, :cur_len] = cur_new_labels\n                    attention_mask[i, :cur_len] = True\n                    position_ids[i, :cur_len] = torch.arange(0, cur_len, dtype=position_ids.dtype, device=position_ids.device)\n\n        new_input_embeds = torch.stack(new_input_embeds_padded, dim=0)\n        # rank0_print(\"tokenizer padding\")\n\n        if _labels is None:\n            new_labels = None\n        else:\n            new_labels = new_labels_padded\n\n        if _attention_mask is None:\n            attention_mask = None\n        else:\n            attention_mask = attention_mask.to(dtype=_attention_mask.dtype)\n\n        if _position_ids is None:\n            position_ids = None\n        if getattr(self.config, \"use_pos_skipping\", False) and self.training:\n            position_ids = torch.arange(new_input_embeds.size(1), device=new_input_embeds.device).unsqueeze(0).to(new_input_embeds.device)\n            split_position = random.randint(0, new_input_embeds.size(1))\n            left_add = random.randint(0, self.config.pos_skipping_range)\n            right_add = random.randint(left_add, self.config.pos_skipping_range)\n            position_ids[:, :split_position] += left_add\n            position_ids[:, split_position:] += right_add\n        # import pdb; pdb.set_trace()\n        # rank0_print(\"Finish preparing\")\n        return None, position_ids, attention_mask, past_key_values, new_input_embeds, new_labels\n\n    def initialize_vision_tokenizer(self, model_args, tokenizer):\n        if model_args.mm_use_im_patch_token:\n            tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\n            self.resize_token_embeddings(len(tokenizer))\n\n        if model_args.mm_use_im_start_end:\n            num_new_tokens = tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\n            self.resize_token_embeddings(len(tokenizer))\n\n            if num_new_tokens > 0:\n                input_embeddings = self.get_input_embeddings().weight.data\n                output_embeddings = self.get_output_embeddings().weight.data\n\n                input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n                output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n\n                input_embeddings[-num_new_tokens:] = input_embeddings_avg\n                output_embeddings[-num_new_tokens:] = output_embeddings_avg\n\n            if model_args.tune_mm_mlp_adapter:\n                for p in self.get_input_embeddings().parameters():\n                    p.requires_grad = True\n                for p in self.get_output_embeddings().parameters():\n                    p.requires_grad = False\n\n            if model_args.pretrain_mm_mlp_adapter:\n                mm_projector_weights = torch.load(model_args.pretrain_mm_mlp_adapter, map_location=\"cpu\")\n                embed_tokens_weight = mm_projector_weights[\"model.embed_tokens.weight\"]\n                assert num_new_tokens == 2\n                if input_embeddings.shape == embed_tokens_weight.shape:\n                    input_embeddings[-num_new_tokens:] = embed_tokens_weight[-num_new_tokens:]\n                elif embed_tokens_weight.shape[0] == num_new_tokens:\n                    input_embeddings[-num_new_tokens:] = embed_tokens_weight\n                else:\n                    raise ValueError(f\"Unexpected embed_tokens_weight shape. Pretrained: {embed_tokens_weight.shape}. Current: {input_embeddings.shape}. Numer of new tokens: {num_new_tokens}.\")\n        elif model_args.mm_use_im_patch_token:\n            if model_args.tune_mm_mlp_adapter:\n                for p in self.get_input_embeddings().parameters():\n                    p.requires_grad = False\n                for p in self.get_output_embeddings().parameters():\n                    p.requires_grad = False"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/modified_resnet.py", "content": "from collections import OrderedDict\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom .utils import freeze_batch_norm_2d\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1):\n        super().__init__()\n\n        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.act1 = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.act2 = nn.ReLU(inplace=True)\n\n        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()\n\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.act3 = nn.ReLU(inplace=True)\n\n        self.downsample = None\n        self.stride = stride\n\n        if stride > 1 or inplanes != planes * Bottleneck.expansion:\n            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1\n            self.downsample = nn.Sequential(OrderedDict([(\"-1\", nn.AvgPool2d(stride)), (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)), (\"1\", nn.BatchNorm2d(planes * self.expansion))]))\n\n    def forward(self, x: torch.Tensor):\n        identity = x\n\n        out = self.act1(self.bn1(self.conv1(x)))\n        out = self.act2(self.bn2(self.conv2(out)))\n        out = self.avgpool(out)\n        out = self.bn3(self.conv3(out))\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.act3(out)\n        return out\n\n\nclass AttentionPool2d(nn.Module):\n    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):\n        super().__init__()\n        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim**2 + 1, embed_dim) / embed_dim**0.5)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n        self.num_heads = num_heads\n\n    def forward(self, x):\n        x = x.reshape(x.shape[0], x.shape[1], x.shape[2] * x.shape[3]).permute(2, 0, 1)  # NCHW -> (HW)NC\n        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC\n        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC\n        x, _ = F.multi_head_attention_forward(\n            query=x,\n            key=x,\n            value=x,\n            embed_dim_to_check=x.shape[-1],\n            num_heads=self.num_heads,\n            q_proj_weight=self.q_proj.weight,\n            k_proj_weight=self.k_proj.weight,\n            v_proj_weight=self.v_proj.weight,\n            in_proj_weight=None,\n            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),\n            bias_k=None,\n            bias_v=None,\n            add_zero_attn=False,\n            dropout_p=0.0,\n            out_proj_weight=self.c_proj.weight,\n            out_proj_bias=self.c_proj.bias,\n            use_separate_proj_weight=True,\n            training=self.training,\n            need_weights=False,\n        )\n\n        return x[0]\n\n\nclass ModifiedResNet(nn.Module):\n    \"\"\"\n    A ResNet class that is similar to torchvision's but contains the following changes:\n    - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\n    - The final pooling layer is a QKV attention instead of an average pool\n    \"\"\"\n\n    def __init__(self, layers, output_dim, heads, image_size=224, width=64):\n        super().__init__()\n        self.output_dim = output_dim\n        self.image_size = image_size\n\n        # the 3-layer stem\n        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(width // 2)\n        self.act1 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(width // 2)\n        self.act2 = nn.ReLU(inplace=True)\n        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(width)\n        self.act3 = nn.ReLU(inplace=True)\n        self.avgpool = nn.AvgPool2d(2)\n\n        # residual layers\n        self._inplanes = width  # this is a *mutable* variable used during construction\n        self.layer1 = self._make_layer(width, layers[0])\n        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)\n        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)\n        self.layer4 = self._make_layer(width * 8, layers[3], stride=2)\n\n        embed_dim = width * 32  # the ResNet feature dimension\n        self.attnpool = AttentionPool2d(image_size // 32, embed_dim, heads, output_dim)\n\n        self.init_parameters()\n\n    def _make_layer(self, planes, blocks, stride=1):\n        layers = [Bottleneck(self._inplanes, planes, stride)]\n\n        self._inplanes = planes * Bottleneck.expansion\n        for _ in range(1, blocks):\n            layers.append(Bottleneck(self._inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def init_parameters(self):\n        if self.attnpool is not None:\n            std = self.attnpool.c_proj.in_features**-0.5\n            nn.init.normal_(self.attnpool.q_proj.weight, std=std)\n            nn.init.normal_(self.attnpool.k_proj.weight, std=std)\n            nn.init.normal_(self.attnpool.v_proj.weight, std=std)\n            nn.init.normal_(self.attnpool.c_proj.weight, std=std)\n\n        for resnet_block in [self.layer1, self.layer2, self.layer3, self.layer4]:\n            for name, param in resnet_block.named_parameters():\n                if name.endswith(\"bn3.weight\"):\n                    nn.init.zeros_(param)\n\n    def lock(self, unlocked_groups=0, freeze_bn_stats=False):\n        assert unlocked_groups == 0, \"partial locking not currently supported for this model\"\n        for param in self.parameters():\n            param.requires_grad = False\n        if freeze_bn_stats:\n            freeze_batch_norm_2d(self)\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        # FIXME support for non-transformer\n        pass\n\n    def stem(self, x):\n        x = self.act1(self.bn1(self.conv1(x)))\n        x = self.act2(self.bn2(self.conv2(x)))\n        x = self.act3(self.bn3(self.conv3(x)))\n        x = self.avgpool(x)\n        return x\n\n    def forward(self, x):\n        x = self.stem(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.attnpool(x)\n\n        return x\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/openai.py", "content": "\"\"\" OpenAI pretrained model functions\n\nAdapted from https://github.com/openai/CLIP. Originally MIT License, Copyright (c) 2021 OpenAI.\n\"\"\"\n\nimport os\nimport warnings\nfrom typing import List, Optional, Union\n\nimport torch\n\nfrom .model import build_model_from_openai_state_dict, convert_weights_to_lp, get_cast_dtype\nfrom .pretrained import get_pretrained_url, list_pretrained_models_by_tag, download_pretrained_from_url\n\n__all__ = [\"list_openai_models\", \"load_openai_model\"]\n\n\ndef list_openai_models() -> List[str]:\n    \"\"\"Returns the names of available CLIP models\"\"\"\n    return list_pretrained_models_by_tag(\"openai\")\n\n\ndef load_openai_model(\n    name: str,\n    precision: Optional[str] = None,\n    device: Optional[Union[str, torch.device]] = None,\n    jit: bool = True,\n    cache_dir: Optional[str] = None,\n):\n    \"\"\"Load a CLIP model\n\n    Parameters\n    ----------\n    name : str\n        A model name listed by `clip.available_models()`, or the path to a model checkpoint containing the state_dict\n    precision: str\n        Model precision, if None defaults to 'fp32' if device == 'cpu' else 'fp16'.\n    device : Union[str, torch.device]\n        The device to put the loaded model\n    jit : bool\n        Whether to load the optimized JIT model (default) or more hackable non-JIT model.\n    cache_dir : Optional[str]\n        The directory to cache the downloaded model weights\n\n    Returns\n    -------\n    model : torch.nn.Module\n        The CLIP model\n    preprocess : Callable[[PIL.Image], torch.Tensor]\n        A torchvision transform that converts a PIL image into a tensor that the returned model can take as its input\n    \"\"\"\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    if precision is None:\n        precision = \"fp32\" if device == \"cpu\" else \"fp16\"\n\n    if get_pretrained_url(name, \"openai\"):\n        model_path = download_pretrained_from_url(get_pretrained_url(name, \"openai\"), cache_dir=cache_dir)\n    elif os.path.isfile(name):\n        model_path = name\n    else:\n        raise RuntimeError(f\"Model {name} not found; available models = {list_openai_models()}\")\n\n    try:\n        # loading JIT archive\n        model = torch.jit.load(model_path, map_location=device if jit else \"cpu\").eval()\n        state_dict = None\n    except RuntimeError:\n        # loading saved state dict\n        if jit:\n            warnings.warn(f\"File {model_path} is not a JIT archive. Loading as a state dict instead\")\n            jit = False\n        state_dict = torch.load(model_path, map_location=\"cpu\")\n\n    if not jit:\n        # Build a non-jit model from the OpenAI jitted model state dict\n        cast_dtype = get_cast_dtype(precision)\n        try:\n            model = build_model_from_openai_state_dict(state_dict or model.state_dict(), cast_dtype=cast_dtype)\n        except KeyError:\n            sd = {k[7:]: v for k, v in state_dict[\"state_dict\"].items()}\n            model = build_model_from_openai_state_dict(sd, cast_dtype=cast_dtype)\n\n        # model from OpenAI state dict is in manually cast fp16 mode, must be converted for AMP/fp32/bf16 use\n        model = model.to(device)\n        if precision.startswith(\"amp\") or precision == \"fp32\":\n            model.float()\n        elif precision == \"bf16\":\n            convert_weights_to_lp(model, dtype=torch.bfloat16)\n\n        return model\n\n    # patch the device names\n    device_holder = torch.jit.trace(lambda: torch.ones([]).to(torch.device(device)), example_inputs=[])\n    device_node = [n for n in device_holder.graph.findAllNodes(\"prim::Constant\") if \"Device\" in repr(n)][-1]\n\n    def patch_device(module):\n        try:\n            graphs = [module.graph] if hasattr(module, \"graph\") else []\n        except RuntimeError:\n            graphs = []\n\n        if hasattr(module, \"forward1\"):\n            graphs.append(module.forward1.graph)\n\n        for graph in graphs:\n            for node in graph.findAllNodes(\"prim::Constant\"):\n                if \"value\" in node.attributeNames() and str(node[\"value\"]).startswith(\"cuda\"):\n                    node.copyAttributes(device_node)\n\n    model.apply(patch_device)\n    patch_device(model.encode_image)\n    patch_device(model.encode_text)\n\n    # patch dtype to float32 (typically for CPU)\n    if precision == \"fp32\":\n        float_holder = torch.jit.trace(lambda: torch.ones([]).float(), example_inputs=[])\n        float_input = list(float_holder.graph.findNode(\"aten::to\").inputs())[1]\n        float_node = float_input.node()\n\n        def patch_float(module):\n            try:\n                graphs = [module.graph] if hasattr(module, \"graph\") else []\n            except RuntimeError:\n                graphs = []\n\n            if hasattr(module, \"forward1\"):\n                graphs.append(module.forward1.graph)\n\n            for graph in graphs:\n                for node in graph.findAllNodes(\"aten::to\"):\n                    inputs = list(node.inputs())\n                    for i in [1, 2]:  # dtype can be the second or third argument to aten::to()\n                        if inputs[i].node()[\"value\"] == 5:\n                            inputs[i].node().copyAttributes(float_node)\n\n        model.apply(patch_float)\n        patch_float(model.encode_image)\n        patch_float(model.encode_text)\n        model.float()\n\n    # ensure image_size attr available at consistent location for both jit and non-jit\n    model.visual.image_size = model.input_resolution.item()\n    return model\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/tokenizer.py", "content": "\"\"\" CLIP tokenizer\n\nCopied from https://github.com/openai/CLIP. Originally MIT License, Copyright (c) 2021 OpenAI.\n\"\"\"\n\nimport gzip\nimport html\nimport os\nfrom functools import lru_cache\nfrom typing import Union, List\n\nimport ftfy\nimport regex as re\nimport torch\n\n# https://stackoverflow.com/q/62691279\nimport os\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n\n@lru_cache()\ndef default_bpe():\n    return os.path.join(os.path.dirname(os.path.abspath(__file__)), \"bpe_simple_vocab_16e6.txt.gz\")\n\n\n@lru_cache()\ndef bytes_to_unicode():\n    \"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    \"\"\"\n    bs = list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"\"), ord(\"\") + 1)) + list(range(ord(\"\"), ord(\"\") + 1))\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8 + n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\n\ndef get_pairs(word):\n    \"\"\"Return set of symbol pairs in a word.\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    \"\"\"\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\n\ndef basic_clean(text):\n    text = ftfy.fix_text(text)\n    text = html.unescape(html.unescape(text))\n    return text.strip()\n\n\ndef whitespace_clean(text):\n    text = re.sub(r\"\\s+\", \" \", text)\n    text = text.strip()\n    return text\n\n\nclass SimpleTokenizer(object):\n    def __init__(self, bpe_path: str = default_bpe(), special_tokens=None):\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split(\"\\n\")\n        merges = merges[1 : 49152 - 256 - 2 + 1]\n        merges = [tuple(merge.split()) for merge in merges]\n        vocab = list(bytes_to_unicode().values())\n        vocab = vocab + [v + \"</w>\" for v in vocab]\n        for merge in merges:\n            vocab.append(\"\".join(merge))\n        if not special_tokens:\n            special_tokens = [\"<start_of_text>\", \"<end_of_text>\"]\n        else:\n            special_tokens = [\"<start_of_text>\", \"<end_of_text>\"] + special_tokens\n        vocab.extend(special_tokens)\n        self.encoder = dict(zip(vocab, range(len(vocab))))\n        self.decoder = {v: k for k, v in self.encoder.items()}\n        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n        self.cache = {t: t for t in special_tokens}\n        special = \"|\".join(special_tokens)\n        self.pat = re.compile(special + r\"\"\"|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\", re.IGNORECASE)\n\n        self.vocab_size = len(self.encoder)\n        self.all_special_ids = [self.encoder[t] for t in special_tokens]\n\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token[:-1]) + (token[-1] + \"</w>\",)\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token + \"</w>\"\n\n        while True:\n            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n                    new_word.append(first + second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = \" \".join(word)\n        self.cache[token] = word\n        return word\n\n    def encode(self, text):\n        bpe_tokens = []\n        text = whitespace_clean(basic_clean(text)).lower()\n        for token in re.findall(self.pat, text):\n            token = \"\".join(self.byte_encoder[b] for b in token.encode(\"utf-8\"))\n            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(\" \"))\n        return bpe_tokens\n\n    def decode(self, tokens):\n        text = \"\".join([self.decoder[token] for token in tokens])\n        text = bytearray([self.byte_decoder[c] for c in text]).decode(\"utf-8\", errors=\"replace\").replace(\"</w>\", \" \")\n        return text\n\n\n_tokenizer = SimpleTokenizer()\n\n\ndef tokenize(texts: Union[str, List[str]], context_length: int = 77) -> torch.LongTensor:\n    \"\"\"\n    Returns the tokenized representation of given input string(s)\n\n    Parameters\n    ----------\n    texts : Union[str, List[str]]\n        An input string or a list of input strings to tokenize\n    context_length : int\n        The context length to use; all CLIP models use 77 as the context length\n\n    Returns\n    -------\n    A two-dimensional tensor containing the resulting tokens, shape = [number of input strings, context_length]\n    \"\"\"\n    if isinstance(texts, str):\n        texts = [texts]\n\n    sot_token = _tokenizer.encoder[\"<start_of_text>\"]\n    eot_token = _tokenizer.encoder[\"<end_of_text>\"]\n    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n\n    for i, tokens in enumerate(all_tokens):\n        if len(tokens) > context_length:\n            tokens = tokens[:context_length]  # Truncate\n            tokens[-1] = eot_token\n        result[i, : len(tokens)] = torch.tensor(tokens)\n\n    return result\n\n\nclass HFTokenizer:\n    \"HuggingFace tokenizer wrapper\"\n\n    def __init__(self, tokenizer_name: str):\n        from transformers import AutoTokenizer\n\n        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n\n    def __call__(self, texts: Union[str, List[str]], context_length: int = 77) -> torch.Tensor:\n        # same cleaning as for default tokenizer, except lowercasing\n        # adding lower (for case-sensitive tokenizers) will make it more robust but less sensitive to nuance\n        if isinstance(texts, str):\n            texts = [texts]\n        texts = [whitespace_clean(basic_clean(text)) for text in texts]\n        input_ids = self.tokenizer(texts, return_tensors=\"pt\", max_length=context_length, padding=\"max_length\", truncation=True).input_ids\n        return input_ids\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/__init__.py", "content": "from .constants import OPENAI_DATASET_MEAN, OPENAI_DATASET_STD\nfrom .factory import create_model, create_model_and_transforms, create_model_from_pretrained, get_tokenizer\nfrom .factory import list_models, add_model_config, get_model_config, load_checkpoint\nfrom .loss import ClipLoss\nfrom .model import CLIP, CustomCLIP, CLIPTextCfg, CLIPVisionCfg, convert_weights_to_lp, convert_weights_to_fp16, trace_model, get_cast_dtype\nfrom .openai import load_openai_model, list_openai_models\nfrom .pretrained import list_pretrained, list_pretrained_models_by_tag, list_pretrained_tags_by_model, get_pretrained_url, download_pretrained_from_url, is_pretrained_cfg, get_pretrained_cfg, download_pretrained\nfrom .tokenizer import SimpleTokenizer, tokenize\nfrom .transform import image_transform\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/transformer.py", "content": "import os\nimport logging\nfrom collections import OrderedDict\nimport math\nfrom typing import Callable, Optional, Sequence\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ntry:\n    from timm.models.layers import trunc_normal_\nexcept:\n    from timm.layers import trunc_normal_\n\nfrom .rope import VisionRotaryEmbedding, VisionRotaryEmbeddingFast\nfrom .utils import to_2tuple\n\nif os.getenv(\"ENV_TYPE\") == \"deepspeed\":\n    try:\n        import deepspeed\n        from deepspeed.runtime.activation_checkpointing.checkpointing import checkpoint\n    except:\n        print(\"Please 'pip install deepspeed'\")\n        deepspeed = None\n        from torch.utils.checkpoint import checkpoint\nelse:\n    from torch.utils.checkpoint import checkpoint\n\ntry:\n    import xformers.ops as xops\nexcept ImportError:\n    xops = None\n    # print(\"Please 'pip install xformers'\")\n\n\nclass LayerNormFp32(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm to handle fp16 (by casting to float32 and back).\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def forward(self, x: torch.Tensor):\n        output = F.layer_norm(\n            x.float(),\n            self.normalized_shape,\n            self.weight.float() if self.weight is not None else None,\n            self.bias.float() if self.bias is not None else None,\n            self.eps,\n        )\n        return output.type_as(x)\n\n\nclass LayerNorm(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm (with cast back to input dtype).\"\"\"\n\n    def forward(self, x: torch.Tensor):\n        orig_type = x.dtype\n        x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n        return x.to(orig_type)\n\n\nclass QuickGELU(nn.Module):\n    # NOTE This is slower than nn.GELU or nn.SiLU and uses more GPU memory\n    def forward(self, x: torch.Tensor):\n        return x * torch.sigmoid(1.702 * x)\n\n\nclass LayerScale(nn.Module):\n    def __init__(self, dim, init_values=1e-5, inplace=False):\n        super().__init__()\n        self.inplace = inplace\n        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n\n    def forward(self, x):\n        return x.mul_(self.gamma) if self.inplace else x * self.gamma\n\n\nclass PatchDropout(nn.Module):\n    \"\"\"\n    https://arxiv.org/abs/2212.00794\n    \"\"\"\n\n    def __init__(self, prob, exclude_first_token=True):\n        super().__init__()\n        assert 0 <= prob < 1.0\n        self.prob = prob\n        self.exclude_first_token = exclude_first_token  # exclude CLS token\n        logging.info(f\"os.getenv('RoPE')={os.getenv('RoPE')}\")\n\n    def forward(self, x):\n        if not self.training or self.prob == 0.0:\n            return x\n\n        if self.exclude_first_token:\n            cls_tokens, x = x[:, :1], x[:, 1:]\n        else:\n            cls_tokens = torch.jit.annotate(torch.Tensor, x[:, :1])\n\n        batch = x.size()[0]\n        num_tokens = x.size()[1]\n\n        batch_indices = torch.arange(batch)\n        batch_indices = batch_indices[..., None]\n\n        keep_prob = 1 - self.prob\n        num_patches_keep = max(1, int(num_tokens * keep_prob))\n\n        rand = torch.randn(batch, num_tokens)\n        patch_indices_keep = rand.topk(num_patches_keep, dim=-1).indices\n\n        x = x[batch_indices, patch_indices_keep]\n\n        if self.exclude_first_token:\n            x = torch.cat((cls_tokens, x), dim=1)\n\n        if self.training and os.getenv(\"RoPE\") == \"1\":\n            return x, patch_indices_keep\n\n        return x\n\n\ndef _in_projection_packed(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    w: torch.Tensor,\n    b: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    https://github.com/pytorch/pytorch/blob/db2a237763eb8693a20788be94f8c192e762baa8/torch/nn/functional.py#L4726\n    \"\"\"\n    E = q.size(-1)\n    if k is v:\n        if q is k:\n            # self-attention\n            return F.linear(q, w, b).chunk(3, dim=-1)\n        else:\n            # encoder-decoder attention\n            w_q, w_kv = w.split([E, E * 2])\n            if b is None:\n                b_q = b_kv = None\n            else:\n                b_q, b_kv = b.split([E, E * 2])\n            return (F.linear(q, w_q, b_q),) + F.linear(k, w_kv, b_kv).chunk(2, dim=-1)\n    else:\n        w_q, w_k, w_v = w.chunk(3)\n        if b is None:\n            b_q = b_k = b_v = None\n        else:\n            b_q, b_k, b_v = b.chunk(3)\n        return F.linear(q, w_q, b_q), F.linear(k, w_k, b_k), F.linear(v, w_v, b_v)\n\n\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=True, scaled_cosine=False, scale_heads=False, logit_scale_max=math.log(1.0 / 0.01), attn_drop=0.0, proj_drop=0.0, xattn=False, rope=False):\n        super().__init__()\n        self.scaled_cosine = scaled_cosine\n        self.scale_heads = scale_heads\n        assert dim % num_heads == 0, \"dim should be divisible by num_heads\"\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim**-0.5\n        self.logit_scale_max = logit_scale_max\n\n        # keeping in_proj in this form (instead of nn.Linear) to match weight scheme of original\n        self.in_proj_weight = nn.Parameter(torch.randn((dim * 3, dim)) * self.scale)\n        if qkv_bias:\n            self.in_proj_bias = nn.Parameter(torch.zeros(dim * 3))\n        else:\n            self.in_proj_bias = None\n\n        if self.scaled_cosine:\n            self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))))\n        else:\n            self.logit_scale = None\n        self.attn_drop = nn.Dropout(attn_drop)\n        if self.scale_heads:\n            self.head_scale = nn.Parameter(torch.ones((num_heads, 1, 1)))\n        else:\n            self.head_scale = None\n        self.out_proj = nn.Linear(dim, dim)\n        self.out_drop = nn.Dropout(proj_drop)\n        self.xattn = xattn\n        self.xattn_drop = attn_drop\n        self.rope = rope\n\n    def forward(self, x, attn_mask: Optional[torch.Tensor] = None):\n        L, N, C = x.shape\n        q, k, v = F.linear(x, self.in_proj_weight, self.in_proj_bias).chunk(3, dim=-1)\n        if self.xattn:\n            q = q.contiguous().view(L, N, self.num_heads, -1).transpose(0, 1)\n            k = k.contiguous().view(L, N, self.num_heads, -1).transpose(0, 1)\n            v = v.contiguous().view(L, N, self.num_heads, -1).transpose(0, 1)\n\n            x = xops.memory_efficient_attention(\n                q,\n                k,\n                v,\n                p=self.xattn_drop,\n                scale=self.scale if self.logit_scale is None else None,\n                attn_bias=xops.LowerTriangularMask() if attn_mask is not None else None,\n            )\n        else:\n            q = q.contiguous().view(L, N * self.num_heads, -1).transpose(0, 1)\n            k = k.contiguous().view(L, N * self.num_heads, -1).transpose(0, 1)\n            v = v.contiguous().view(L, N * self.num_heads, -1).transpose(0, 1)\n\n            if self.logit_scale is not None:\n                attn = torch.bmm(F.normalize(q, dim=-1), F.normalize(k, dim=-1).transpose(-1, -2))\n                logit_scale = torch.clamp(self.logit_scale, max=self.logit_scale_max).exp()\n                attn = attn.view(N, self.num_heads, L, L) * logit_scale\n                attn = attn.view(-1, L, L)\n            else:\n                q = q * self.scale\n                attn = torch.bmm(q, k.transpose(-1, -2))\n\n            if attn_mask is not None:\n                if attn_mask.dtype == torch.bool:\n                    new_attn_mask = torch.zeros_like(attn_mask, dtype=q.dtype)\n                    new_attn_mask.masked_fill_(attn_mask, float(\"-inf\"))\n                    attn_mask = new_attn_mask\n                attn += attn_mask\n\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n\n            x = torch.bmm(attn, v)\n\n        if self.head_scale is not None:\n            x = x.view(N, self.num_heads, L, C) * self.head_scale\n            x = x.view(-1, L, C)\n        x = x.transpose(0, 1).reshape(L, N, C)\n        x = self.out_proj(x)\n        x = self.out_drop(x)\n        return x\n\n\nclass CustomAttention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=True, scaled_cosine=True, scale_heads=False, logit_scale_max=math.log(1.0 / 0.01), attn_drop=0.0, proj_drop=0.0, xattn=False):\n        super().__init__()\n        self.scaled_cosine = scaled_cosine\n        self.scale_heads = scale_heads\n        assert dim % num_heads == 0, \"dim should be divisible by num_heads\"\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim**-0.5\n        self.logit_scale_max = logit_scale_max\n\n        # keeping in_proj in this form (instead of nn.Linear) to match weight scheme of original\n        self.in_proj_weight = nn.Parameter(torch.randn((dim * 3, dim)) * self.scale)\n        if qkv_bias:\n            self.in_proj_bias = nn.Parameter(torch.zeros(dim * 3))\n        else:\n            self.in_proj_bias = None\n\n        if self.scaled_cosine:\n            self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))))\n        else:\n            self.logit_scale = None\n        self.attn_drop = nn.Dropout(attn_drop)\n        if self.scale_heads:\n            self.head_scale = nn.Parameter(torch.ones((num_heads, 1, 1)))\n        else:\n            self.head_scale = None\n        self.out_proj = nn.Linear(dim, dim)\n        self.out_drop = nn.Dropout(proj_drop)\n        self.xattn = xattn\n        self.xattn_drop = attn_drop\n\n    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n        q, k, v = _in_projection_packed(query, key, value, self.in_proj_weight, self.in_proj_bias)\n        N_q, B_q, C_q = q.shape\n        N_k, B_k, C_k = k.shape\n        N_v, B_v, C_v = v.shape\n        if self.xattn:\n            # B, N, C -> B, N, num_heads, C\n            q = q.permute(1, 0, 2).reshape(B_q, N_q, self.num_heads, -1)\n            k = k.permute(1, 0, 2).reshape(B_k, N_k, self.num_heads, -1)\n            v = v.permute(1, 0, 2).reshape(B_v, N_v, self.num_heads, -1)\n\n            x = xops.memory_efficient_attention(q, k, v, p=self.xattn_drop, scale=self.scale if self.logit_scale is None else None, attn_bias=xops.LowerTriangularMask() if attn_mask is not None else None)\n        else:\n            # B*H, L, C\n            q = q.contiguous().view(N_q, B_q * self.num_heads, -1).transpose(0, 1)\n            k = k.contiguous().view(N_k, B_k * self.num_heads, -1).transpose(0, 1)\n            v = v.contiguous().view(N_v, B_v * self.num_heads, -1).transpose(0, 1)\n\n            if self.logit_scale is not None:\n                # B*H, N_q, N_k\n                attn = torch.bmm(F.normalize(q, dim=-1), F.normalize(k, dim=-1).transpose(-1, -2))\n                logit_scale = torch.clamp(self.logit_scale, max=self.logit_scale_max).exp()\n                attn = attn.view(B_q, self.num_heads, N_q, N_k) * logit_scale\n                attn = attn.view(-1, N_q, N_k)\n            else:\n                q = q * self.scale\n                attn = torch.bmm(q, k.transpose(-1, -2))\n\n            if attn_mask is not None:\n                if attn_mask.dtype == torch.bool:\n                    new_attn_mask = torch.zeros_like(attn_mask, dtype=q.dtype)\n                    new_attn_mask.masked_fill_(attn_mask, float(\"-inf\"))\n                    attn_mask = new_attn_mask\n                attn += attn_mask\n\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n\n            x = torch.bmm(attn, v)\n\n        if self.head_scale is not None:\n            x = x.view(B_q, self.num_heads, N_q, C_q) * self.head_scale\n            x = x.view(-1, N_q, C_q)\n        x = x.transpose(0, 1).reshape(N_q, B_q, C_q)\n        x = self.out_proj(x)\n        x = self.out_drop(x)\n        return x\n\n\nclass CustomResidualAttentionBlock(nn.Module):\n    def __init__(\n        self,\n        d_model: int,\n        n_head: int,\n        mlp_ratio: float = 4.0,\n        ls_init_value: float = None,\n        act_layer: Callable = nn.GELU,\n        norm_layer: Callable = LayerNorm,\n        scale_cosine_attn: bool = False,\n        scale_heads: bool = False,\n        scale_attn: bool = False,\n        scale_fc: bool = False,\n        cross_attn: bool = False,\n        xattn: bool = False,\n    ):\n        super().__init__()\n\n        self.ln_1 = norm_layer(d_model)\n        self.ln_1_k = norm_layer(d_model) if cross_attn else self.ln_1\n        self.ln_1_v = norm_layer(d_model) if cross_attn else self.ln_1\n        self.attn = CustomAttention(d_model, n_head, qkv_bias=True, attn_drop=0.0, proj_drop=0.0, scaled_cosine=scale_cosine_attn, scale_heads=scale_heads, xattn=xattn)\n\n        self.ln_attn = norm_layer(d_model) if scale_attn else nn.Identity()\n        self.ls_1 = LayerScale(d_model, ls_init_value) if ls_init_value is not None else nn.Identity()\n\n        self.ln_2 = norm_layer(d_model)\n        mlp_width = int(d_model * mlp_ratio)\n        self.mlp = nn.Sequential(OrderedDict([(\"c_fc\", nn.Linear(d_model, mlp_width)), (\"ln\", norm_layer(mlp_width) if scale_fc else nn.Identity()), (\"gelu\", act_layer()), (\"c_proj\", nn.Linear(mlp_width, d_model))]))\n\n        self.ls_2 = LayerScale(d_model, ls_init_value) if ls_init_value is not None else nn.Identity()\n\n    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n        q = q + self.ls_1(self.ln_attn(self.attn(self.ln_1(q), self.ln_1_k(k), self.ln_1_v(v), attn_mask=attn_mask)))\n        q = q + self.ls_2(self.mlp(self.ln_2(q)))\n        return q\n\n\nclass CustomTransformer(nn.Module):\n    def __init__(\n        self,\n        width: int,\n        layers: int,\n        heads: int,\n        mlp_ratio: float = 4.0,\n        ls_init_value: float = None,\n        act_layer: Callable = nn.GELU,\n        norm_layer: Callable = LayerNorm,\n        scale_cosine_attn: bool = True,\n        scale_heads: bool = False,\n        scale_attn: bool = False,\n        scale_fc: bool = False,\n        cross_attn: bool = False,\n        xattn: bool = False,\n    ):\n        super().__init__()\n        self.width = width\n        self.layers = layers\n        self.grad_checkpointing = False\n        self.xattn = xattn\n\n        self.resblocks = nn.ModuleList(\n            [\n                CustomResidualAttentionBlock(\n                    width,\n                    heads,\n                    mlp_ratio,\n                    ls_init_value=ls_init_value,\n                    act_layer=act_layer,\n                    norm_layer=norm_layer,\n                    scale_cosine_attn=scale_cosine_attn,\n                    scale_heads=scale_heads,\n                    scale_attn=scale_attn,\n                    scale_fc=scale_fc,\n                    cross_attn=cross_attn,\n                    xattn=xattn,\n                )\n                for _ in range(layers)\n            ]\n        )\n\n    def get_cast_dtype(self) -> torch.dtype:\n        return self.resblocks[0].mlp.c_fc.weight.dtype\n\n    def forward(self, q: torch.Tensor, k: torch.Tensor = None, v: torch.Tensor = None, attn_mask: Optional[torch.Tensor] = None):\n        if k is None and v is None:\n            k = v = q\n        for r in self.resblocks:\n            if self.grad_checkpointing and not torch.jit.is_scripting():\n                q = checkpoint(r, q, k, v, attn_mask)\n            else:\n                q = r(q, k, v, attn_mask=attn_mask)\n        return q\n\n\nclass ResidualAttentionBlock(nn.Module):\n    def __init__(\n        self,\n        d_model: int,\n        n_head: int,\n        mlp_ratio: float = 4.0,\n        ls_init_value: float = None,\n        act_layer: Callable = nn.GELU,\n        norm_layer: Callable = LayerNorm,\n        xattn: bool = False,\n    ):\n        super().__init__()\n\n        self.ln_1 = norm_layer(d_model)\n        if xattn:\n            self.attn = Attention(d_model, n_head, xattn=True)\n        else:\n            self.attn = nn.MultiheadAttention(d_model, n_head)\n        self.ls_1 = LayerScale(d_model, ls_init_value) if ls_init_value is not None else nn.Identity()\n\n        self.ln_2 = norm_layer(d_model)\n        mlp_width = int(d_model * mlp_ratio)\n        self.mlp = nn.Sequential(OrderedDict([(\"c_fc\", nn.Linear(d_model, mlp_width)), (\"gelu\", act_layer()), (\"c_proj\", nn.Linear(mlp_width, d_model))]))\n\n        self.ls_2 = LayerScale(d_model, ls_init_value) if ls_init_value is not None else nn.Identity()\n        self.xattn = xattn\n\n    def attention(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n        attn_mask = attn_mask.to(x.dtype) if attn_mask is not None else None\n        if self.xattn:\n            return self.attn(x, attn_mask=attn_mask)\n        return self.attn(x, x, x, need_weights=False, attn_mask=attn_mask)[0]\n\n    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n        x = x + self.ls_1(self.attention(self.ln_1(x), attn_mask=attn_mask))\n        x = x + self.ls_2(self.mlp(self.ln_2(x)))\n        return x\n\n\nclass Transformer(nn.Module):\n    def __init__(\n        self,\n        width: int,\n        layers: int,\n        heads: int,\n        mlp_ratio: float = 4.0,\n        ls_init_value: float = None,\n        act_layer: Callable = nn.GELU,\n        norm_layer: Callable = LayerNorm,\n        xattn: bool = False,\n    ):\n        super().__init__()\n        self.width = width\n        self.layers = layers\n        self.grad_checkpointing = False\n\n        self.resblocks = nn.ModuleList([ResidualAttentionBlock(width, heads, mlp_ratio, ls_init_value=ls_init_value, act_layer=act_layer, norm_layer=norm_layer, xattn=xattn) for _ in range(layers)])\n\n    def get_cast_dtype(self) -> torch.dtype:\n        return self.resblocks[0].mlp.c_fc.weight.dtype\n\n    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n        for r in self.resblocks:\n            if self.grad_checkpointing and not torch.jit.is_scripting():\n                x = checkpoint(r, x, attn_mask)\n            else:\n                x = r(x, attn_mask=attn_mask)\n        return x\n\n\nclass VisionTransformer(nn.Module):\n    def __init__(\n        self,\n        image_size: int,\n        patch_size: int,\n        width: int,\n        layers: int,\n        heads: int,\n        mlp_ratio: float,\n        ls_init_value: float = None,\n        patch_dropout: float = 0.0,\n        global_average_pool: bool = False,\n        output_dim: int = 512,\n        act_layer: Callable = nn.GELU,\n        norm_layer: Callable = LayerNorm,\n        xattn: bool = False,\n    ):\n        super().__init__()\n        self.image_size = to_2tuple(image_size)\n        self.patch_size = to_2tuple(patch_size)\n        self.grid_size = (self.image_size[0] // self.patch_size[0], self.image_size[1] // self.patch_size[1])\n        self.output_dim = output_dim\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)\n\n        scale = width**-0.5\n        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n        self.positional_embedding = nn.Parameter(scale * torch.randn(self.grid_size[0] * self.grid_size[1] + 1, width))\n\n        # setting a patch_dropout of 0. would mean it is disabled and this function would be the identity fn\n        self.patch_dropout = PatchDropout(patch_dropout) if patch_dropout > 0.0 else nn.Identity()\n        self.ln_pre = norm_layer(width)\n\n        self.transformer = Transformer(width, layers, heads, mlp_ratio, ls_init_value=ls_init_value, act_layer=act_layer, norm_layer=norm_layer, xattn=xattn)\n\n        self.global_average_pool = global_average_pool\n        self.ln_post = norm_layer(width)\n        self.proj = nn.Parameter(scale * torch.randn(width, output_dim))\n\n    def lock(self, unlocked_groups=0, freeze_bn_stats=False):\n        for param in self.parameters():\n            param.requires_grad = False\n\n        if unlocked_groups != 0:\n            groups = [\n                [\n                    self.conv1,\n                    self.class_embedding,\n                    self.positional_embedding,\n                    self.ln_pre,\n                ],\n                *self.transformer.resblocks[:-1],\n                [\n                    self.transformer.resblocks[-1],\n                    self.ln_post,\n                ],\n                self.proj,\n            ]\n\n            def _unlock(x):\n                if isinstance(x, Sequence):\n                    for g in x:\n                        _unlock(g)\n                else:\n                    if isinstance(x, torch.nn.Parameter):\n                        x.requires_grad = True\n                    else:\n                        for p in x.parameters():\n                            p.requires_grad = True\n\n            _unlock(groups[-unlocked_groups:])\n\n    def get_num_layers(self):\n        return self.transformer.layers\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.transformer.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\"positional_embedding\", \"class_embedding\"}\n\n    def forward(self, x: torch.Tensor, return_all_features: bool = False):\n        x = self.conv1(x)  # shape = [*, width, grid, grid]\n        x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n        x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n        x = x + self.positional_embedding.to(x.dtype)\n\n        # a patch_dropout of 0. would mean it is disabled and this function would do nothing but return what was passed in\n        x = self.patch_dropout(x)\n        x = self.ln_pre(x)\n\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.transformer(x)\n        x = x.permute(1, 0, 2)  # LND -> NLD\n\n        if not return_all_features:\n            if self.global_average_pool:\n                x = x.mean(dim=1)  # x = x[:,1:,:].mean(dim=1)\n            else:\n                x = x[:, 0]\n\n            x = self.ln_post(x)\n\n            if self.proj is not None:\n                x = x @ self.proj\n\n        return x\n\n\nclass TextTransformer(nn.Module):\n    def __init__(\n        self,\n        context_length: int = 77,\n        vocab_size: int = 49408,\n        width: int = 512,\n        heads: int = 8,\n        layers: int = 12,\n        ls_init_value: float = None,\n        output_dim: int = 512,\n        act_layer: Callable = nn.GELU,\n        norm_layer: Callable = LayerNorm,\n        xattn: bool = False,\n        attn_mask: bool = True,\n    ):\n        super().__init__()\n        self.context_length = context_length\n        self.vocab_size = vocab_size\n        self.width = width\n        self.output_dim = output_dim\n\n        self.token_embedding = nn.Embedding(vocab_size, width)\n        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, width))\n        self.transformer = Transformer(width=width, layers=layers, heads=heads, ls_init_value=ls_init_value, act_layer=act_layer, norm_layer=norm_layer, xattn=xattn)\n\n        self.xattn = xattn\n        self.ln_final = norm_layer(width)\n        self.text_projection = nn.Parameter(torch.empty(width, output_dim))\n\n        if attn_mask:\n            self.register_buffer(\"attn_mask\", self.build_attention_mask(), persistent=False)\n        else:\n            self.attn_mask = None\n\n        self.init_parameters()\n\n    def init_parameters(self):\n        nn.init.normal_(self.token_embedding.weight, std=0.02)\n        nn.init.normal_(self.positional_embedding, std=0.01)\n\n        proj_std = (self.transformer.width**-0.5) * ((2 * self.transformer.layers) ** -0.5)\n        attn_std = self.transformer.width**-0.5\n        fc_std = (2 * self.transformer.width) ** -0.5\n        for block in self.transformer.resblocks:\n            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n\n        if self.text_projection is not None:\n            nn.init.normal_(self.text_projection, std=self.transformer.width**-0.5)\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.transformer.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        # return {'positional_embedding', 'token_embedding'}\n        return {\"positional_embedding\"}\n\n    def get_num_layers(self):\n        return self.transformer.layers\n\n    def build_attention_mask(self):\n        # lazily create causal attention mask, with full attention between the vision tokens\n        # pytorch uses additive attention mask; fill with -inf\n        mask = torch.empty(self.context_length, self.context_length)\n        mask.fill_(float(\"-inf\"))\n        mask.triu_(1)  # zero out the lower diagonal\n        return mask\n\n    def forward(self, text, return_all_features: bool = False):\n        cast_dtype = self.transformer.get_cast_dtype()\n        x = self.token_embedding(text).to(cast_dtype)  # [batch_size, n_ctx, d_model]\n\n        x = x + self.positional_embedding.to(cast_dtype)\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.transformer(x, attn_mask=self.attn_mask)\n        # x = self.transformer(x) # no attention mask is applied\n        x = x.permute(1, 0, 2)  # LND -> NLD\n        x = self.ln_final(x)\n\n        if not return_all_features:\n            # x.shape = [batch_size, n_ctx, transformer.width]\n            # take features from the eot embedding (eot_token is the highest number in each sequence)\n            x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n        return x\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/factory.py", "content": "import json\nimport logging\nimport os\nimport pathlib\nimport re\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom typing import Optional, Tuple, Union, Dict, Any\nimport torch\n\ntry:\n    import deepspeed\nexcept ImportError:\n    deepspeed = None\n\nfrom .constants import OPENAI_DATASET_MEAN, OPENAI_DATASET_STD\nfrom .model import CLIP, CustomCLIP, convert_weights_to_lp, convert_to_custom_text_state_dict, get_cast_dtype\nfrom .openai import load_openai_model\nfrom .pretrained import is_pretrained_cfg, get_pretrained_cfg, download_pretrained, list_pretrained_tags_by_model\nfrom .transform import image_transform\nfrom .tokenizer import HFTokenizer, tokenize\nfrom .utils import resize_clip_pos_embed, resize_evaclip_pos_embed, resize_visual_pos_embed, resize_eva_pos_embed\n\n\n_MODEL_CONFIG_PATHS = [Path(__file__).parent / f\"model_configs/\"]\n_MODEL_CONFIGS = {}  # directory (model_name: config) of model architecture configs\n\n\ndef _natural_key(string_):\n    return [int(s) if s.isdigit() else s for s in re.split(r\"(\\d+)\", string_.lower())]\n\n\ndef _rescan_model_configs():\n    global _MODEL_CONFIGS\n\n    config_ext = (\".json\",)\n    config_files = []\n    for config_path in _MODEL_CONFIG_PATHS:\n        if config_path.is_file() and config_path.suffix in config_ext:\n            config_files.append(config_path)\n        elif config_path.is_dir():\n            for ext in config_ext:\n                config_files.extend(config_path.glob(f\"*{ext}\"))\n\n    for cf in config_files:\n        with open(cf, \"r\", encoding=\"utf8\") as f:\n            model_cfg = json.load(f)\n            if all(a in model_cfg for a in (\"embed_dim\", \"vision_cfg\", \"text_cfg\")):\n                _MODEL_CONFIGS[cf.stem] = model_cfg\n\n    _MODEL_CONFIGS = dict(sorted(_MODEL_CONFIGS.items(), key=lambda x: _natural_key(x[0])))\n\n\n_rescan_model_configs()  # initial populate of model config registry\n\n\ndef list_models():\n    \"\"\"enumerate available model architectures based on config files\"\"\"\n    return list(_MODEL_CONFIGS.keys())\n\n\ndef add_model_config(path):\n    \"\"\"add model config path or file and update registry\"\"\"\n    if not isinstance(path, Path):\n        path = Path(path)\n    _MODEL_CONFIG_PATHS.append(path)\n    _rescan_model_configs()\n\n\ndef get_model_config(model_name):\n    if model_name in _MODEL_CONFIGS:\n        return deepcopy(_MODEL_CONFIGS[model_name])\n    else:\n        return None\n\n\ndef get_tokenizer(model_name):\n    config = get_model_config(model_name)\n    tokenizer = HFTokenizer(config[\"text_cfg\"][\"hf_tokenizer_name\"]) if \"hf_tokenizer_name\" in config[\"text_cfg\"] else tokenize\n    return tokenizer\n\n\n# loading openai CLIP weights when is_openai=True for training\ndef load_state_dict(checkpoint_path: str, map_location: str = \"cpu\", model_key: str = \"model|module|state_dict\", is_openai: bool = False, skip_list: list = []):\n    if is_openai:\n        model = torch.jit.load(checkpoint_path, map_location=\"cpu\").eval()\n        state_dict = model.state_dict()\n        for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n            state_dict.pop(key, None)\n    else:\n        checkpoint = torch.load(checkpoint_path, map_location=map_location)\n        for mk in model_key.split(\"|\"):\n            if isinstance(checkpoint, dict) and mk in checkpoint:\n                state_dict = checkpoint[mk]\n                break\n            else:\n                state_dict = checkpoint\n        if next(iter(state_dict.items()))[0].startswith(\"module\"):\n            state_dict = {k[7:]: v for k, v in state_dict.items()}\n\n    for k in skip_list:\n        if k in list(state_dict.keys()):\n            logging.info(f\"Removing key {k} from pretrained checkpoint\")\n            del state_dict[k]\n\n    if os.getenv(\"RoPE\") == \"1\":\n        for k in list(state_dict.keys()):\n            if \"freqs_cos\" in k or \"freqs_sin\" in k:\n                del state_dict[k]\n    return state_dict\n\n\ndef load_checkpoint(model, checkpoint_path, model_key=\"model|module|state_dict\", strict=True):\n    state_dict = load_state_dict(checkpoint_path, model_key=model_key, is_openai=False)\n    # detect old format and make compatible with new format\n    if \"positional_embedding\" in state_dict and not hasattr(model, \"positional_embedding\"):\n        state_dict = convert_to_custom_text_state_dict(state_dict)\n    if \"text.logit_scale\" in state_dict and hasattr(model, \"logit_scale\"):\n        state_dict[\"logit_scale\"] = state_dict[\"text.logit_scale\"]\n        del state_dict[\"text.logit_scale\"]\n\n    # resize_clip_pos_embed for CLIP and open CLIP\n    if \"visual.positional_embedding\" in state_dict:\n        resize_clip_pos_embed(state_dict, model)\n    # specified to eva_vit_model\n    elif \"visual.pos_embed\" in state_dict:\n        resize_evaclip_pos_embed(state_dict, model)\n\n    # resize_clip_pos_embed(state_dict, model)\n    incompatible_keys = model.load_state_dict(state_dict, strict=strict)\n    logging.info(f\"incompatible_keys.missing_keys: {incompatible_keys.missing_keys}\")\n    return incompatible_keys\n\n\ndef load_clip_visual_state_dict(checkpoint_path: str, map_location: str = \"cpu\", is_openai: bool = False, skip_list: list = []):\n    state_dict = load_state_dict(checkpoint_path, map_location=map_location, is_openai=is_openai, skip_list=skip_list)\n\n    for k in list(state_dict.keys()):\n        if not k.startswith(\"visual.\"):\n            del state_dict[k]\n    for k in list(state_dict.keys()):\n        if k.startswith(\"visual.\"):\n            new_k = k[7:]\n            state_dict[new_k] = state_dict[k]\n            del state_dict[k]\n    return state_dict\n\n\ndef load_clip_text_state_dict(checkpoint_path: str, map_location: str = \"cpu\", is_openai: bool = False, skip_list: list = []):\n    state_dict = load_state_dict(checkpoint_path, map_location=map_location, is_openai=is_openai, skip_list=skip_list)\n\n    for k in list(state_dict.keys()):\n        if k.startswith(\"visual.\"):\n            del state_dict[k]\n    return state_dict\n\n\ndef get_pretrained_tag(pretrained_model):\n    pretrained_model = pretrained_model.lower()\n    if \"laion\" in pretrained_model or \"open_clip\" in pretrained_model:\n        return \"open_clip\"\n    elif \"openai\" in pretrained_model:\n        return \"clip\"\n    elif \"eva\" in pretrained_model and \"clip\" in pretrained_model:\n        return \"eva_clip\"\n    else:\n        return \"other\"\n\n\ndef load_zero_partitions(model, state_dict, is_deepspeed_zero3_enabled, pretrained_model_path, ignore_mismatched_sizes=False):\n    \"\"\"\n    adept from pytorch lightning and transformers\n    with deepspeed.zero.Init():\n        model = MyModel()\n    state_dict = torch.load(model_path, map_location=\"cpu\")\n    load_zero_partitions(model, prefix=\"\")\n    \"\"\"\n\n    # because zero3 puts placeholders in model params, this context\n    # manager gathers (unpartitions) the params of the current layer, then loads from\n    # the state dict and then re-partitions them again\n    model_state_dict = model.state_dict()\n    expected_keys = list(model_state_dict.keys())\n    loaded_keys = list(state_dict.keys())\n    missing_keys = list(set(expected_keys) - set(loaded_keys))\n    unexpected_keys = list(set(loaded_keys) - set(expected_keys))\n\n    # Mistmatched keys contains tuples key/shape1/shape2 of weights in the checkpoint that have a shape not\n    # matching the weights in the model.\n    mismatched_keys = []\n    if ignore_mismatched_sizes:\n        for checkpoint_key in loaded_keys:\n            model_key = checkpoint_key\n\n            if model_key in model_state_dict and state_dict[checkpoint_key].shape != model_state_dict[model_key].shape:\n                mismatched_keys.append((checkpoint_key, state_dict[checkpoint_key].shape, model_state_dict[model_key].shape))\n                del state_dict[checkpoint_key]\n    # copy state_dict so _load_from_state_dict can modify it\n    metadata = getattr(state_dict, \"_metadata\", None)\n    state_dict = state_dict.copy()\n    if metadata is not None:\n        state_dict._metadata = metadata\n\n    error_msgs = []\n\n    # PyTorch's `_load_from_state_dict` does not copy parameters in a module's descendants\n    # so we need to apply the function recursively.\n    def load(module, prefix=\"\"):\n        local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n        args = (state_dict, prefix, local_metadata, True, [], [], error_msgs)\n        if is_deepspeed_zero3_enabled:\n            # because zero3 puts placeholders in model params, this context\n            # manager gathers (unpartitions) the params of the current layer, then loads from\n            # the state dict and then re-partitions them again\n            with deepspeed.zero.GatheredParameters(list(module.parameters(recurse=False)), modifier_rank=0):\n                if torch.distributed.get_rank() == 0:\n                    module._load_from_state_dict(*args)\n        else:\n            module._load_from_state_dict(*args)\n\n        for name, child in module._modules.items():\n            if child is not None:\n                load(child, prefix + name + \".\")\n\n    # Make sure we are able to load base models as well as derived models (with heads)\n    start_prefix = \"\"\n    model_to_load = model\n    load(model_to_load, prefix=start_prefix)\n    del state_dict\n    if len(error_msgs) > 0:\n        error_msg = \"\\n\\t\".join(error_msgs)\n        if \"size mismatch\" in error_msg:\n            error_msg += \"\\n\\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\"\n        raise RuntimeError(f\"Error(s) in loading state_dict for {model.__class__.__name__}:\\n\\t{error_msg}\")\n    if len(unexpected_keys) > 0:\n        logging.warning(\n            f\"Some weights of the model checkpoint at {pretrained_model_path} were not used when\"\n            f\" initializing {model.__class__.__name__}: {unexpected_keys}\\n- This IS expected if you are\"\n            f\" initializing {model.__class__.__name__} from the checkpoint of a model trained on another task or\"\n            \" with another architecture (e.g. initializing a BertForSequenceClassification model from a\"\n            \" BertForPreTraining model).\\n- This IS NOT expected if you are initializing\"\n            f\" {model.__class__.__name__} from the checkpoint of a model that you expect to be exactly identical\"\n            \" (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\"\n        )\n    else:\n        logging.info(f\"All model checkpoint weights were used when initializing {model.__class__.__name__}.\\n\")\n    if len(missing_keys) > 0:\n        logging.warning(\n            f\"Some weights of {model.__class__.__name__} were not initialized from the model checkpoint at\"\n            f\" {pretrained_model_path} and are newly initialized: {missing_keys}\\nYou should probably\"\n            \" TRAIN this model on a down-stream task to be able to use it for predictions and inference.\"\n        )\n    elif len(mismatched_keys) == 0:\n        logging.info(\n            f\"All the weights of {model.__class__.__name__} were initialized from the model checkpoint at\"\n            f\" {pretrained_model_path}.\\nIf your task is similar to the task the model of the checkpoint\"\n            f\" was trained on, you can already use {model.__class__.__name__} for predictions without further\"\n            \" training.\"\n        )\n    if len(mismatched_keys) > 0:\n        mismatched_warning = \"\\n\".join([f\"- {key}: found shape {shape1} in the checkpoint and {shape2} in the model instantiated\" for key, shape1, shape2 in mismatched_keys])\n        logging.warning(\n            f\"Some weights of {model.__class__.__name__} were not initialized from the model checkpoint at\"\n            f\" {pretrained_model_path} and are newly initialized because the shapes did not\"\n            f\" match:\\n{mismatched_warning}\\nYou should probably TRAIN this model on a down-stream task to be able\"\n            \" to use it for predictions and inference.\"\n        )\n\n\ndef load_pretrained_checkpoint(model, visual_checkpoint_path, text_checkpoint_path, strict=True, visual_model=None, text_model=None, model_key=\"model|module|state_dict\", skip_list=[]):\n    visual_tag = get_pretrained_tag(visual_model)\n    text_tag = get_pretrained_tag(text_model)\n\n    logging.info(f\"num of model state_dict keys: {len(model.state_dict().keys())}\")\n    visual_incompatible_keys, text_incompatible_keys = None, None\n    if visual_checkpoint_path:\n        if visual_tag == \"eva_clip\" or visual_tag == \"open_clip\":\n            visual_state_dict = load_clip_visual_state_dict(visual_checkpoint_path, is_openai=False, skip_list=skip_list)\n        elif visual_tag == \"clip\":\n            visual_state_dict = load_clip_visual_state_dict(visual_checkpoint_path, is_openai=True, skip_list=skip_list)\n        else:\n            visual_state_dict = load_state_dict(visual_checkpoint_path, model_key=model_key, is_openai=False, skip_list=skip_list)\n\n        # resize_clip_pos_embed for CLIP and open CLIP\n        if \"positional_embedding\" in visual_state_dict:\n            resize_visual_pos_embed(visual_state_dict, model)\n        # specified to EVA model\n        elif \"pos_embed\" in visual_state_dict:\n            resize_eva_pos_embed(visual_state_dict, model)\n\n        visual_incompatible_keys = model.visual.load_state_dict(visual_state_dict, strict=strict)\n        logging.info(f\"num of loaded visual_state_dict keys: {len(visual_state_dict.keys())}\")\n        logging.info(f\"visual_incompatible_keys.missing_keys: {visual_incompatible_keys.missing_keys}\")\n\n    if text_checkpoint_path:\n        if text_tag == \"eva_clip\" or text_tag == \"open_clip\":\n            text_state_dict = load_clip_text_state_dict(text_checkpoint_path, is_openai=False, skip_list=skip_list)\n        elif text_tag == \"clip\":\n            text_state_dict = load_clip_text_state_dict(text_checkpoint_path, is_openai=True, skip_list=skip_list)\n        else:\n            text_state_dict = load_state_dict(visual_checkpoint_path, model_key=model_key, is_openai=False, skip_list=skip_list)\n\n        text_incompatible_keys = model.text.load_state_dict(text_state_dict, strict=strict)\n\n        logging.info(f\"num of loaded text_state_dict keys: {len(text_state_dict.keys())}\")\n        logging.info(f\"text_incompatible_keys.missing_keys: {text_incompatible_keys.missing_keys}\")\n\n    return visual_incompatible_keys, text_incompatible_keys\n\n\ndef create_model(\n    model_name: str,\n    pretrained: Optional[str] = None,\n    precision: str = \"fp32\",\n    device: Union[str, torch.device] = \"cpu\",\n    jit: bool = False,\n    force_quick_gelu: bool = False,\n    force_custom_clip: bool = False,\n    force_patch_dropout: Optional[float] = None,\n    pretrained_image: str = \"\",\n    pretrained_text: str = \"\",\n    pretrained_hf: bool = True,\n    pretrained_visual_model: str = None,\n    pretrained_text_model: str = None,\n    cache_dir: Optional[str] = None,\n    skip_list: list = [],\n):\n    model_name = model_name.replace(\"/\", \"-\")  # for callers using old naming with / in ViT names\n    if isinstance(device, str):\n        device = torch.device(device)\n\n    if pretrained and pretrained.lower() == \"openai\":\n        logging.info(f\"Loading pretrained {model_name} from OpenAI.\")\n        model = load_openai_model(\n            model_name,\n            precision=precision,\n            device=device,\n            jit=jit,\n            cache_dir=cache_dir,\n        )\n    else:\n        model_cfg = get_model_config(model_name)\n        if model_cfg is not None:\n            logging.info(f\"Loaded {model_name} model config.\")\n        else:\n            logging.error(f\"Model config for {model_name} not found; available models {list_models()}.\")\n            raise RuntimeError(f\"Model config for {model_name} not found.\")\n\n        if \"rope\" in model_cfg.get(\"vision_cfg\", {}):\n            if model_cfg[\"vision_cfg\"][\"rope\"]:\n                os.environ[\"RoPE\"] = \"1\"\n        else:\n            os.environ[\"RoPE\"] = \"0\"\n\n        if force_quick_gelu:\n            # override for use of QuickGELU on non-OpenAI transformer models\n            model_cfg[\"quick_gelu\"] = True\n\n        if force_patch_dropout is not None:\n            # override the default patch dropout value\n            model_cfg[\"vision_cfg\"][\"patch_dropout\"] = force_patch_dropout\n\n        cast_dtype = get_cast_dtype(precision)\n        custom_clip = model_cfg.pop(\"custom_text\", False) or force_custom_clip or (\"hf_model_name\" in model_cfg[\"text_cfg\"])\n\n        if custom_clip:\n            if \"hf_model_name\" in model_cfg.get(\"text_cfg\", {}):\n                model_cfg[\"text_cfg\"][\"hf_model_pretrained\"] = pretrained_hf\n            model = CustomCLIP(**model_cfg, cast_dtype=cast_dtype)\n        else:\n            model = CLIP(**model_cfg, cast_dtype=cast_dtype)\n\n        pretrained_cfg = {}\n        if pretrained:\n            checkpoint_path = \"\"\n            pretrained_cfg = get_pretrained_cfg(model_name, pretrained)\n            if pretrained_cfg:\n                checkpoint_path = download_pretrained(pretrained_cfg, cache_dir=cache_dir)\n            elif os.path.exists(pretrained):\n                checkpoint_path = pretrained\n\n            if checkpoint_path:\n                logging.info(f\"Loading pretrained {model_name} weights ({pretrained}).\")\n                load_checkpoint(model, checkpoint_path, model_key=\"model|module|state_dict\", strict=False)\n            else:\n                error_str = f\"Pretrained weights ({pretrained}) not found for model {model_name}.\" f\"Available pretrained tags ({list_pretrained_tags_by_model(model_name)}.\"\n                logging.warning(error_str)\n                raise RuntimeError(error_str)\n        else:\n            visual_checkpoint_path = \"\"\n            text_checkpoint_path = \"\"\n\n            if pretrained_image:\n                pretrained_visual_model = pretrained_visual_model.replace(\"/\", \"-\")  # for callers using old naming with / in ViT names\n                pretrained_image_cfg = get_pretrained_cfg(pretrained_visual_model, pretrained_image)\n                if \"timm_model_name\" in model_cfg.get(\"vision_cfg\", {}):\n                    # pretrained weight loading for timm models set via vision_cfg\n                    model_cfg[\"vision_cfg\"][\"timm_model_pretrained\"] = True\n                elif pretrained_image_cfg:\n                    visual_checkpoint_path = download_pretrained(pretrained_image_cfg, cache_dir=cache_dir)\n                elif os.path.exists(pretrained_image):\n                    visual_checkpoint_path = pretrained_image\n                else:\n                    logging.warning(f\"Pretrained weights ({visual_checkpoint_path}) not found for model {model_name}.visual.\")\n                    raise RuntimeError(f\"Pretrained weights ({visual_checkpoint_path}) not found for model {model_name}.visual.\")\n\n            if pretrained_text:\n                pretrained_text_model = pretrained_text_model.replace(\"/\", \"-\")  # for callers using old naming with / in ViT names\n                pretrained_text_cfg = get_pretrained_cfg(pretrained_text_model, pretrained_text)\n                if pretrained_image_cfg:\n                    text_checkpoint_path = download_pretrained(pretrained_text_cfg, cache_dir=cache_dir)\n                elif os.path.exists(pretrained_text):\n                    text_checkpoint_path = pretrained_text\n                else:\n                    logging.warning(f\"Pretrained weights ({text_checkpoint_path}) not found for model {model_name}.text.\")\n                    raise RuntimeError(f\"Pretrained weights ({text_checkpoint_path}) not found for model {model_name}.text.\")\n\n            if visual_checkpoint_path:\n                logging.info(f\"Loading pretrained {model_name}.visual weights ({visual_checkpoint_path}).\")\n            if text_checkpoint_path:\n                logging.info(f\"Loading pretrained {model_name}.text weights ({text_checkpoint_path}).\")\n\n            if visual_checkpoint_path or text_checkpoint_path:\n                load_pretrained_checkpoint(model, visual_checkpoint_path, text_checkpoint_path, strict=False, visual_model=pretrained_visual_model, text_model=pretrained_text_model, model_key=\"model|module|state_dict\", skip_list=skip_list)\n\n        if \"fp16\" in precision or \"bf16\" in precision:\n            logging.info(f\"convert precision to {precision}\")\n            model = model.to(torch.bfloat16) if \"bf16\" in precision else model.to(torch.float16)\n\n        # model.to(device=device)\n\n        # set image / mean metadata from pretrained_cfg if available, or use default\n        model.visual.image_mean = pretrained_cfg.get(\"mean\", None) or OPENAI_DATASET_MEAN\n        model.visual.image_std = pretrained_cfg.get(\"std\", None) or OPENAI_DATASET_STD\n\n        if jit:\n            model = torch.jit.script(model)\n\n    return model\n\n\ndef create_model_and_transforms(\n    model_name: str,\n    pretrained: Optional[str] = None,\n    precision: str = \"fp32\",\n    device: Union[str, torch.device] = \"cpu\",\n    jit: bool = False,\n    force_quick_gelu: bool = False,\n    force_custom_clip: bool = False,\n    force_patch_dropout: Optional[float] = None,\n    pretrained_image: str = \"\",\n    pretrained_text: str = \"\",\n    pretrained_hf: bool = True,\n    pretrained_visual_model: str = None,\n    pretrained_text_model: str = None,\n    image_mean: Optional[Tuple[float, ...]] = None,\n    image_std: Optional[Tuple[float, ...]] = None,\n    cache_dir: Optional[str] = None,\n    skip_list: list = [],\n):\n    model = create_model(\n        model_name,\n        pretrained,\n        precision=precision,\n        device=device,\n        jit=jit,\n        force_quick_gelu=force_quick_gelu,\n        force_custom_clip=force_custom_clip,\n        force_patch_dropout=force_patch_dropout,\n        pretrained_image=pretrained_image,\n        pretrained_text=pretrained_text,\n        pretrained_hf=pretrained_hf,\n        pretrained_visual_model=pretrained_visual_model,\n        pretrained_text_model=pretrained_text_model,\n        cache_dir=cache_dir,\n        skip_list=skip_list,\n    )\n\n    image_mean = image_mean or getattr(model.visual, \"image_mean\", None)\n    image_std = image_std or getattr(model.visual, \"image_std\", None)\n    preprocess_train = image_transform(model.visual.image_size, is_train=True, mean=image_mean, std=image_std)\n    preprocess_val = image_transform(model.visual.image_size, is_train=False, mean=image_mean, std=image_std)\n\n    return model, preprocess_train, preprocess_val\n\n\ndef create_model_from_pretrained(\n    model_name: str,\n    pretrained: str,\n    precision: str = \"fp32\",\n    device: Union[str, torch.device] = \"cpu\",\n    jit: bool = False,\n    force_quick_gelu: bool = False,\n    force_custom_clip: bool = False,\n    force_patch_dropout: Optional[float] = None,\n    return_transform: bool = True,\n    image_mean: Optional[Tuple[float, ...]] = None,\n    image_std: Optional[Tuple[float, ...]] = None,\n    cache_dir: Optional[str] = None,\n    is_frozen: bool = False,\n):\n    if not is_pretrained_cfg(model_name, pretrained) and not os.path.exists(pretrained):\n        raise RuntimeError(f\"{pretrained} is not a valid pretrained cfg or checkpoint for {model_name}.\" f\" Use open_clip.list_pretrained() to find one.\")\n\n    model = create_model(\n        model_name,\n        pretrained,\n        precision=precision,\n        device=device,\n        jit=jit,\n        force_quick_gelu=force_quick_gelu,\n        force_custom_clip=force_custom_clip,\n        force_patch_dropout=force_patch_dropout,\n        cache_dir=cache_dir,\n    )\n\n    if is_frozen:\n        for param in model.parameters():\n            param.requires_grad = False\n\n    if not return_transform:\n        return model\n\n    image_mean = image_mean or getattr(model.visual, \"image_mean\", None)\n    image_std = image_std or getattr(model.visual, \"image_std\", None)\n    preprocess = image_transform(model.visual.image_size, is_train=False, mean=image_mean, std=image_std)\n\n    return model, preprocess\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/model.py", "content": "\"\"\" CLIP Model\n\nAdapted from https://github.com/openai/CLIP. Originally MIT License, Copyright (c) 2021 OpenAI.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, Union\nfrom functools import partial\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\ntry:\n    from .hf_model import HFTextEncoder\nexcept:\n    HFTextEncoder = None\nfrom .modified_resnet import ModifiedResNet\nfrom .timm_model import TimmModel\nfrom .eva_vit_model import EVAVisionTransformer\nfrom .transformer import LayerNorm, QuickGELU, Attention, VisionTransformer, TextTransformer\n\ntry:\n    from apex.normalization import FusedLayerNorm\nexcept:\n    FusedLayerNorm = LayerNorm\n    # print(\"Please 'pip install apex'\")\n\ntry:\n    import xformers.ops as xops\nexcept ImportError:\n    xops = None\n    # print(\"Please 'pip install xformers'\")\n\n\nclass RMSnorm(nn.Module):\n    \"\"\"\n    adepted from transformers T5LayerNorm\n    \"\"\"\n\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        Construct a layernorm module in the T5 style. No bias and no subtraction of mean.\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        # T5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean\n        # Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated\n        # w/o mean and there is no bias. Additionally we want to make sure that the accumulation for\n        # half-precision inputs is done in fp32\n\n        variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n\n        # convert into half-precision if necessary\n        if self.weight.dtype in [torch.float16, torch.bfloat16]:\n            hidden_states = hidden_states.to(self.weight.dtype)\n\n        return self.weight * hidden_states\n\n\n@dataclass\nclass CLIPVisionCfg:\n    layers: Union[Tuple[int, int, int, int], int] = 12\n    width: int = 768\n    head_width: int = 64\n    mlp_ratio: float = 4.0\n    patch_size: int = 16\n    image_size: Union[Tuple[int, int], int] = 224\n    ls_init_value: Optional[float] = None  # layer scale initial value\n    patch_dropout: float = 0.0  # what fraction of patches to dropout during training (0 would mean disabled and no patches dropped) - 0.5 to 0.75 recommended in the paper for optimal results\n    global_average_pool: bool = False  # whether to global average pool the last embedding layer, instead of using CLS token (https://arxiv.org/abs/2205.01580)\n    drop_path_rate: Optional[float] = None  # drop path rate\n    timm_model_name: str = None  # a valid model name overrides layers, width, patch_size\n    timm_model_pretrained: bool = False  # use (imagenet) pretrained weights for named model\n    timm_pool: str = \"avg\"  # feature pooling for timm model ('abs_attn', 'rot_attn', 'avg', '')\n    timm_proj: str = \"linear\"  # linear projection for timm model output ('linear', 'mlp', '')\n    timm_proj_bias: bool = False  # enable bias final projection\n    eva_model_name: str = None  # a valid eva model name overrides layers, width, patch_size\n    qkv_bias: bool = True\n    fusedLN: bool = False\n    xattn: bool = False\n    postnorm: bool = False\n    rope: bool = False\n    pt_hw_seq_len: int = 16  # 224/14\n    intp_freq: bool = False\n    naiveswiglu: bool = False\n    subln: bool = False\n    use_rms_norm: bool = False\n\n\n@dataclass\nclass CLIPTextCfg:\n    context_length: int = 77\n    vocab_size: int = 49408\n    width: int = 512\n    heads: int = 8\n    layers: int = 12\n    ls_init_value: Optional[float] = None  # layer scale initial value\n    hf_model_name: str = None\n    hf_tokenizer_name: str = None\n    hf_model_pretrained: bool = True\n    proj: str = \"mlp\"\n    pooler_type: str = \"mean_pooler\"\n    masked_language_modeling: bool = False\n    fusedLN: bool = False\n    xattn: bool = False\n    attn_mask: bool = True\n\n\ndef get_cast_dtype(precision: str):\n    cast_dtype = None\n    if precision == \"bf16\":\n        cast_dtype = torch.bfloat16\n    elif precision == \"fp16\":\n        cast_dtype = torch.float16\n    return cast_dtype\n\n\ndef _build_vision_tower(embed_dim: int, vision_cfg: CLIPVisionCfg, quick_gelu: bool = False, cast_dtype: Optional[torch.dtype] = None):\n    if isinstance(vision_cfg, dict):\n        vision_cfg = CLIPVisionCfg(**vision_cfg)\n\n    # OpenAI models are pretrained w/ QuickGELU but native nn.GELU is both faster and more\n    # memory efficient in recent PyTorch releases (>= 1.10).\n    # NOTE: timm models always use native GELU regardless of quick_gelu flag.\n    act_layer = QuickGELU if quick_gelu else nn.GELU\n\n    if vision_cfg.eva_model_name:\n        vision_heads = vision_cfg.width // vision_cfg.head_width\n\n        norm_layer = RMSnorm if vision_cfg.use_rms_norm else LayerNorm\n\n        visual = EVAVisionTransformer(\n            img_size=vision_cfg.image_size,\n            patch_size=vision_cfg.patch_size,\n            num_classes=embed_dim,\n            use_mean_pooling=vision_cfg.global_average_pool,  # False\n            init_values=vision_cfg.ls_init_value,\n            patch_dropout=vision_cfg.patch_dropout,\n            embed_dim=vision_cfg.width,\n            depth=vision_cfg.layers,\n            num_heads=vision_heads,\n            mlp_ratio=vision_cfg.mlp_ratio,\n            qkv_bias=vision_cfg.qkv_bias,\n            drop_path_rate=vision_cfg.drop_path_rate,\n            norm_layer=partial(norm_layer, eps=1e-6),\n            xattn=vision_cfg.xattn,\n            rope=vision_cfg.rope,\n            postnorm=vision_cfg.postnorm,\n            pt_hw_seq_len=vision_cfg.pt_hw_seq_len,  # 224/14\n            intp_freq=vision_cfg.intp_freq,\n            naiveswiglu=vision_cfg.naiveswiglu,\n            subln=vision_cfg.subln,\n        )\n    elif vision_cfg.timm_model_name:\n        visual = TimmModel(\n            vision_cfg.timm_model_name, pretrained=vision_cfg.timm_model_pretrained, pool=vision_cfg.timm_pool, proj=vision_cfg.timm_proj, proj_bias=vision_cfg.timm_proj_bias, embed_dim=embed_dim, image_size=vision_cfg.image_size\n        )\n        act_layer = nn.GELU  # so that text transformer doesn't use QuickGELU w/ timm models\n    elif isinstance(vision_cfg.layers, (tuple, list)):\n        vision_heads = vision_cfg.width * 32 // vision_cfg.head_width\n        visual = ModifiedResNet(layers=vision_cfg.layers, output_dim=embed_dim, heads=vision_heads, image_size=vision_cfg.image_size, width=vision_cfg.width)\n    else:\n        vision_heads = vision_cfg.width // vision_cfg.head_width\n        norm_layer = LayerNormFp32 if cast_dtype in (torch.float16, torch.bfloat16) else LayerNorm\n        visual = VisionTransformer(\n            image_size=vision_cfg.image_size,\n            patch_size=vision_cfg.patch_size,\n            width=vision_cfg.width,\n            layers=vision_cfg.layers,\n            heads=vision_heads,\n            mlp_ratio=vision_cfg.mlp_ratio,\n            ls_init_value=vision_cfg.ls_init_value,\n            patch_dropout=vision_cfg.patch_dropout,\n            global_average_pool=vision_cfg.global_average_pool,\n            output_dim=embed_dim,\n            act_layer=act_layer,\n            norm_layer=norm_layer,\n        )\n\n    return visual\n\n\ndef _build_text_tower(\n    embed_dim: int,\n    text_cfg: CLIPTextCfg,\n    quick_gelu: bool = False,\n    cast_dtype: Optional[torch.dtype] = None,\n):\n    if isinstance(text_cfg, dict):\n        text_cfg = CLIPTextCfg(**text_cfg)\n\n    if text_cfg.hf_model_name:\n        text = HFTextEncoder(text_cfg.hf_model_name, output_dim=embed_dim, tokenizer_name=text_cfg.hf_tokenizer_name, proj=text_cfg.proj, pooler_type=text_cfg.pooler_type, masked_language_modeling=text_cfg.masked_language_modeling)\n    else:\n        act_layer = QuickGELU if quick_gelu else nn.GELU\n        norm_layer = LayerNorm\n\n        text = TextTransformer(\n            context_length=text_cfg.context_length,\n            vocab_size=text_cfg.vocab_size,\n            width=text_cfg.width,\n            heads=text_cfg.heads,\n            layers=text_cfg.layers,\n            ls_init_value=text_cfg.ls_init_value,\n            output_dim=embed_dim,\n            act_layer=act_layer,\n            norm_layer=FusedLayerNorm if text_cfg.fusedLN else norm_layer,\n            xattn=text_cfg.xattn,\n            attn_mask=text_cfg.attn_mask,\n        )\n    return text\n\n\nclass CLIP(nn.Module):\n    def __init__(\n        self,\n        embed_dim: int,\n        vision_cfg: CLIPVisionCfg,\n        text_cfg: CLIPTextCfg,\n        quick_gelu: bool = False,\n        cast_dtype: Optional[torch.dtype] = None,\n    ):\n        super().__init__()\n        self.visual = _build_vision_tower(embed_dim, vision_cfg, quick_gelu, cast_dtype)\n\n        text = _build_text_tower(embed_dim, text_cfg, quick_gelu, cast_dtype)\n        self.transformer = text.transformer\n        self.vocab_size = text.vocab_size\n        self.token_embedding = text.token_embedding\n        self.positional_embedding = text.positional_embedding\n        self.ln_final = text.ln_final\n        self.text_projection = text.text_projection\n        self.register_buffer(\"attn_mask\", text.attn_mask, persistent=False)\n\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n\n    def lock_image_tower(self, unlocked_groups=0, freeze_bn_stats=False):\n        # lock image tower as per LiT - https://arxiv.org/abs/2111.07991\n        self.visual.lock(unlocked_groups=unlocked_groups, freeze_bn_stats=freeze_bn_stats)\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.visual.set_grad_checkpointing(enable)\n        self.transformer.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\"logit_scale\"}\n\n    def encode_image(self, image, normalize: bool = False):\n        features = self.visual(image)\n        return F.normalize(features, dim=-1) if normalize else features\n\n    def encode_text(self, text, normalize: bool = False):\n        cast_dtype = self.transformer.get_cast_dtype()\n\n        x = self.token_embedding(text).to(cast_dtype)  # [batch_size, n_ctx, d_model]\n\n        x = x + self.positional_embedding.to(cast_dtype)\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.transformer(x, attn_mask=self.attn_mask)\n        x = x.permute(1, 0, 2)  # LND -> NLD\n        x = self.ln_final(x)  # [batch_size, n_ctx, transformer.width]\n        # take features from the eot embedding (eot_token is the highest number in each sequence)\n        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n        return F.normalize(x, dim=-1) if normalize else x\n\n    def forward(self, image, text):\n        image_features = self.encode_image(image, normalize=True)\n        text_features = self.encode_text(text, normalize=True)\n        return image_features, text_features, self.logit_scale.exp()\n\n\nclass CustomCLIP(nn.Module):\n    def __init__(\n        self,\n        embed_dim: int,\n        vision_cfg: CLIPVisionCfg,\n        text_cfg: CLIPTextCfg,\n        quick_gelu: bool = False,\n        cast_dtype: Optional[torch.dtype] = None,\n        itm_task: bool = False,\n    ):\n        super().__init__()\n        self.visual = _build_vision_tower(embed_dim, vision_cfg, quick_gelu, cast_dtype)\n        self.text = _build_text_tower(embed_dim, text_cfg, quick_gelu, cast_dtype)\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n\n    def lock_image_tower(self, unlocked_groups=0, freeze_bn_stats=False):\n        # lock image tower as per LiT - https://arxiv.org/abs/2111.07991\n        self.visual.lock(unlocked_groups=unlocked_groups, freeze_bn_stats=freeze_bn_stats)\n\n    def lock_text_tower(self, unlocked_layers: int = 0, freeze_layer_norm: bool = True):\n        self.text.lock(unlocked_layers, freeze_layer_norm)\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.visual.set_grad_checkpointing(enable)\n        self.text.set_grad_checkpointing(enable)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\"logit_scale\"}\n\n    def encode_image(self, image, normalize: bool = False):\n        features = self.visual(image)\n        return F.normalize(features, dim=-1) if normalize else features\n\n    def encode_text(self, text, normalize: bool = False):\n        features = self.text(text)\n        return F.normalize(features, dim=-1) if normalize else features\n\n    def forward(self, image, text):\n        image_features = self.encode_image(image, normalize=True)\n        text_features = self.encode_text(text, normalize=True)\n        return image_features, text_features, self.logit_scale.exp()\n\n\ndef convert_weights_to_lp(model: nn.Module, dtype=torch.float16):\n    \"\"\"Convert applicable model parameters to low-precision (bf16 or fp16)\"\"\"\n\n    def _convert_weights(l):\n\n        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n            l.weight.data = l.weight.data.to(dtype)\n            if l.bias is not None:\n                l.bias.data = l.bias.data.to(dtype)\n\n        if isinstance(l, (nn.MultiheadAttention, Attention)):\n            for attr in [*[f\"{s}_proj_weight\" for s in [\"in\", \"q\", \"k\", \"v\"]], \"in_proj_bias\", \"bias_k\", \"bias_v\"]:\n                tensor = getattr(l, attr, None)\n                if tensor is not None:\n                    tensor.data = tensor.data.to(dtype)\n\n        if isinstance(l, nn.Parameter):\n            l.data = l.data.to(dtype)\n\n        for name in [\"text_projection\", \"proj\"]:\n            if hasattr(l, name) and isinstance(l, nn.Parameter):\n                attr = getattr(l, name, None)\n                if attr is not None:\n                    attr.data = attr.data.to(dtype)\n\n    model.apply(_convert_weights)\n\n\nconvert_weights_to_fp16 = convert_weights_to_lp  # backwards compat\n\n\n# used to maintain checkpoint compatibility\ndef convert_to_custom_text_state_dict(state_dict: dict):\n    if \"text_projection\" in state_dict:\n        # old format state_dict, move text tower -> .text\n        new_state_dict = {}\n        for k, v in state_dict.items():\n            if any(k.startswith(p) for p in (\"text_projection\", \"positional_embedding\", \"token_embedding\", \"transformer\", \"ln_final\", \"logit_scale\")):\n                k = \"text.\" + k\n            new_state_dict[k] = v\n        return new_state_dict\n    return state_dict\n\n\ndef build_model_from_openai_state_dict(\n    state_dict: dict,\n    quick_gelu=True,\n    cast_dtype=torch.float16,\n):\n    vit = \"visual.proj\" in state_dict\n\n    if vit:\n        vision_width = state_dict[\"visual.conv1.weight\"].shape[0]\n        vision_layers = len([k for k in state_dict.keys() if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")])\n        vision_patch_size = state_dict[\"visual.conv1.weight\"].shape[-1]\n        grid_size = round((state_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5)\n        image_size = vision_patch_size * grid_size\n    else:\n        counts: list = [len(set(k.split(\".\")[2] for k in state_dict if k.startswith(f\"visual.layer{b}\"))) for b in [1, 2, 3, 4]]\n        vision_layers = tuple(counts)\n        vision_width = state_dict[\"visual.layer1.0.conv1.weight\"].shape[0]\n        output_width = round((state_dict[\"visual.attnpool.positional_embedding\"].shape[0] - 1) ** 0.5)\n        vision_patch_size = None\n        assert output_width**2 + 1 == state_dict[\"visual.attnpool.positional_embedding\"].shape[0]\n        image_size = output_width * 32\n\n    embed_dim = state_dict[\"text_projection\"].shape[1]\n    context_length = state_dict[\"positional_embedding\"].shape[0]\n    vocab_size = state_dict[\"token_embedding.weight\"].shape[0]\n    transformer_width = state_dict[\"ln_final.weight\"].shape[0]\n    transformer_heads = transformer_width // 64\n    transformer_layers = len(set(k.split(\".\")[2] for k in state_dict if k.startswith(f\"transformer.resblocks\")))\n\n    vision_cfg = CLIPVisionCfg(\n        layers=vision_layers,\n        width=vision_width,\n        patch_size=vision_patch_size,\n        image_size=image_size,\n    )\n    text_cfg = CLIPTextCfg(context_length=context_length, vocab_size=vocab_size, width=transformer_width, heads=transformer_heads, layers=transformer_layers)\n    model = CLIP(\n        embed_dim,\n        vision_cfg=vision_cfg,\n        text_cfg=text_cfg,\n        quick_gelu=quick_gelu,  # OpenAI models were trained with QuickGELU\n        cast_dtype=cast_dtype,\n    )\n\n    for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n        state_dict.pop(key, None)\n\n    convert_weights_to_fp16(model)  # OpenAI state dicts are partially converted to float16\n    model.load_state_dict(state_dict)\n    return model.eval()\n\n\ndef trace_model(model, batch_size=256, device=torch.device(\"cpu\")):\n    model.eval()\n    image_size = model.visual.image_size\n    example_images = torch.ones((batch_size, 3, image_size, image_size), device=device)\n    example_text = torch.zeros((batch_size, model.context_length), dtype=torch.int, device=device)\n    model = torch.jit.trace_module(model, inputs=dict(forward=(example_images, example_text), encode_text=(example_text,), encode_image=(example_images,)))\n    model.visual.image_size = image_size\n    return model\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/transform.py", "content": "from typing import Optional, Sequence, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms.functional as F\n\nfrom torchvision.transforms import Normalize, Compose, RandomResizedCrop, InterpolationMode, ToTensor, Resize, CenterCrop\n\nfrom .constants import OPENAI_DATASET_MEAN, OPENAI_DATASET_STD\n\n\nclass ResizeMaxSize(nn.Module):\n\n    def __init__(self, max_size, interpolation=InterpolationMode.BICUBIC, fn=\"max\", fill=0):\n        super().__init__()\n        if not isinstance(max_size, int):\n            raise TypeError(f\"Size should be int. Got {type(max_size)}\")\n        self.max_size = max_size\n        self.interpolation = interpolation\n        self.fn = min if fn == \"min\" else min\n        self.fill = fill\n\n    def forward(self, img):\n        if isinstance(img, torch.Tensor):\n            height, width = img.shape[:2]\n        else:\n            width, height = img.size\n        scale = self.max_size / float(max(height, width))\n        if scale != 1.0:\n            new_size = tuple(round(dim * scale) for dim in (height, width))\n            img = F.resize(img, new_size, self.interpolation)\n            pad_h = self.max_size - new_size[0]\n            pad_w = self.max_size - new_size[1]\n            img = F.pad(img, padding=[pad_w // 2, pad_h // 2, pad_w - pad_w // 2, pad_h - pad_h // 2], fill=self.fill)\n        return img\n\n\ndef _convert_to_rgb(image):\n    return image.convert(\"RGB\")\n\n\n# class CatGen(nn.Module):\n#     def __init__(self, num=4):\n#         self.num = num\n#     def mixgen_batch(image, text):\n#         batch_size = image.shape[0]\n#         index = np.random.permutation(batch_size)\n\n#         cat_images = []\n#         for i in range(batch_size):\n#             # image mixup\n#             image[i,:] = lam * image[i,:] + (1 - lam) * image[index[i],:]\n#             # text concat\n#             text[i] = tokenizer((str(text[i]) + \" \" + str(text[index[i]])))[0]\n#         text = torch.stack(text)\n#         return image, text\n\n\ndef image_transform(\n    image_size: int,\n    is_train: bool,\n    mean: Optional[Tuple[float, ...]] = None,\n    std: Optional[Tuple[float, ...]] = None,\n    resize_longest_max: bool = False,\n    fill_color: int = 0,\n):\n    mean = mean or OPENAI_DATASET_MEAN\n    if not isinstance(mean, (list, tuple)):\n        mean = (mean,) * 3\n\n    std = std or OPENAI_DATASET_STD\n    if not isinstance(std, (list, tuple)):\n        std = (std,) * 3\n\n    if isinstance(image_size, (list, tuple)) and image_size[0] == image_size[1]:\n        # for square size, pass size as int so that Resize() uses aspect preserving shortest edge\n        image_size = image_size[0]\n\n    normalize = Normalize(mean=mean, std=std)\n    if is_train:\n        return Compose(\n            [\n                RandomResizedCrop(image_size, scale=(0.9, 1.0), interpolation=InterpolationMode.BICUBIC),\n                _convert_to_rgb,\n                ToTensor(),\n                normalize,\n            ]\n        )\n    else:\n        if resize_longest_max:\n            transforms = [ResizeMaxSize(image_size, fill=fill_color)]\n        else:\n            transforms = [\n                Resize(image_size, interpolation=InterpolationMode.BICUBIC),\n                CenterCrop(image_size),\n            ]\n        transforms.extend(\n            [\n                _convert_to_rgb,\n                ToTensor(),\n                normalize,\n            ]\n        )\n        return Compose(transforms)\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/clip_encoder.py", "content": "import torch\nimport torch.nn as nn\nfrom llava.utils import rank0_print\nfrom transformers import CLIPVisionModel, CLIPImageProcessor, CLIPVisionConfig\n\ntry:\n    from s2wrapper import forward as multiscale_forward\nexcept:\n    pass\n\n\nclass CLIPVisionTower(nn.Module):\n    def __init__(self, vision_tower, args, delay_load=False):\n        super().__init__()\n\n        self.is_loaded = False\n\n        self.vision_tower_name = vision_tower\n        self.select_layer = args.mm_vision_select_layer\n        self.select_feature = getattr(args, \"mm_vision_select_feature\", \"patch\")\n\n        # if not delay_load:\n        #     rank0_print(f\"Loading vision tower: {vision_tower}\")\n        self.load_model()\n        # elif getattr(args, \"unfreeze_mm_vision_tower\", False):\n        #     # TODO: better detector is needed.\n        #     rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `unfreeze_mm_vision_tower`: True.\")\n        #     self.load_model()\n        # elif hasattr(args, \"mm_tunable_parts\") and \"mm_vision_tower\" in args.mm_tunable_parts:\n        #     rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `mm_tunable_parts` contains `mm_vision_tower`.\")\n        #     self.load_model()\n        # else:\n        #     self.cfg_only = CLIPVisionConfig.from_pretrained(self.vision_tower_name)\n\n    def load_model(self, device_map=None):\n        if self.is_loaded:\n            rank0_print(\"{} is already loaded, `load_model` called again, skipping.\".format(self.vision_tower_name))\n            return\n\n        self.image_processor = CLIPImageProcessor.from_pretrained(self.vision_tower_name)\n        self.vision_tower = CLIPVisionModel.from_pretrained(self.vision_tower_name, device_map=device_map)\n        self.vision_tower.requires_grad_(False)\n\n        self.is_loaded = True\n\n    def feature_select(self, image_forward_outs):\n        select_feature_type = self.select_feature\n\n        if self.select_feature in [\"slicefour_patch\", \"slicefour_cls_patch\"]:\n            select_every_k_layer = len(image_forward_outs.hidden_states) // 4\n            image_features = torch.cat([image_forward_outs.hidden_states[i] for i in range(select_every_k_layer + self.select_layer, len(image_forward_outs.hidden_states), select_every_k_layer)], dim=-1)\n            select_feature_type = select_feature_type.replace(\"slicefour_\", \"\")\n        elif self.select_feature in [\"slice_m25811_f6_patch\", \"slice_m25811_f6_cls_patch\"]:\n            select_layers = [-2, -5, -8, -11, 6]\n            image_features = torch.cat([image_forward_outs.hidden_states[i] for i in select_layers], dim=-1)\n            select_feature_type = select_feature_type.replace(\"slice_m25811_f6_\", \"\")\n        else:\n            image_features = image_forward_outs.hidden_states[self.select_layer]\n\n        if select_feature_type == \"patch\":\n            image_features = image_features[:, 1:]\n        elif select_feature_type == \"cls_patch\":\n            image_features = image_features\n        else:\n            raise ValueError(f\"Unexpected select feature: {select_feature_type}\")\n        return image_features\n\n    def forward(self, images):\n        if type(images) is list:\n            image_features = []\n            for image in images:\n                image_forward_out = self.vision_tower(image.to(device=self.device, dtype=self.dtype).unsqueeze(0), output_hidden_states=True)\n                image_feature = self.feature_select(image_forward_out).to(image.dtype)\n                image_features.append(image_feature)\n        else:\n            image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\n            image_features = self.feature_select(image_forward_outs).to(images.dtype)\n\n        return image_features\n\n    @property\n    def dummy_feature(self):\n        return torch.zeros(1, self.hidden_size, device=self.device, dtype=self.dtype)\n\n    @property\n    def dtype(self):\n        return self.vision_tower.dtype\n\n    @property\n    def device(self):\n        return self.vision_tower.device\n\n    @property\n    def config(self):\n        if self.is_loaded:\n            return self.vision_tower.config\n        else:\n            return self.cfg_only\n\n    @property\n    def hidden_size(self):\n        _hidden_size = self.config.hidden_size\n        if \"slicefour\" in self.select_feature:\n            _hidden_size *= 4\n        if \"slice_m25811_f6\" in self.select_feature:\n            _hidden_size *= 5\n        return _hidden_size\n\n    @property\n    def num_patches_per_side(self):\n        return self.config.image_size // self.config.patch_size\n\n    @property\n    def num_patches(self):\n        _num_patches = (self.config.image_size // self.config.patch_size) ** 2\n        if \"cls_patch\" in self.select_feature:\n            _num_patches += 1\n        return _num_patches\n\n    @property\n    def image_size(self):\n        return self.config.image_size\n\n\nclass CLIPVisionTowerS2(CLIPVisionTower):\n    def __init__(self, vision_tower, args, delay_load=False):\n\n        self.s2_scales = getattr(args, \"s2_scales\", \"336,672,1008\")\n        self.s2_scales = list(map(int, self.s2_scales.split(\",\")))\n        self.s2_scales.sort()\n        self.s2_split_size = self.s2_scales[0]\n        self.s2_image_size = self.s2_scales[-1]\n\n        super().__init__(vision_tower, args, delay_load)\n\n        # change resize/crop size in preprocessing to the largest image size in s2_scale\n        if not delay_load or getattr(args, \"unfreeze_mm_vision_tower\", False):\n            self.image_processor.size[\"shortest_edge\"] = self.s2_image_size\n            self.image_processor.crop_size[\"height\"] = self.image_processor.crop_size[\"width\"] = self.s2_image_size\n\n    def load_model(self, device_map=None):\n        if self.is_loaded:\n            rank0_print(\"{} is already loaded, `load_model` called again, skipping.\".format(self.vision_tower_name))\n            return\n\n        self.image_processor = CLIPImageProcessor.from_pretrained(self.vision_tower_name)\n        self.vision_tower = CLIPVisionModel.from_pretrained(self.vision_tower_name, device_map=device_map)\n        self.vision_tower.requires_grad_(False)\n\n        self.image_processor.size[\"shortest_edge\"] = self.s2_image_size\n        self.image_processor.crop_size[\"height\"] = self.image_processor.crop_size[\"width\"] = self.s2_image_size\n\n        self.is_loaded = True\n\n    def forward_feature(self, images):\n        image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\n        image_features = self.feature_select(image_forward_outs).to(images.dtype)\n        return image_features\n\n    def forward(self, images):\n        if type(images) is list:\n            image_features = []\n            for image in images:\n                image_feature = multiscale_forward(self.forward_feature, image.unsqueeze(0), img_sizes=self.s2_scales, max_split_size=self.s2_split_size, split_forward=True)\n                image_features.append(image_feature)\n        else:\n            image_features = multiscale_forward(self.forward_feature, images, img_sizes=self.s2_scales, max_split_size=self.s2_split_size, split_forward=True)\n\n        return image_features\n\n    @property\n    def hidden_size(self):\n        return self.config.hidden_size * len(self.s2_scales)\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/hf_model.py", "content": "\"\"\" huggingface model adapter\n\nWraps HuggingFace transformers (https://github.com/huggingface/transformers) models for use as a text tower in CLIP model.\n\"\"\"\n\nimport re\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch import TensorType\n\ntry:\n    import transformers\n    from transformers import AutoModel, AutoModelForMaskedLM, AutoTokenizer, AutoConfig, PretrainedConfig\n    from transformers.modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, BaseModelOutputWithPoolingAndCrossAttentions\nexcept ImportError as e:\n    transformers = None\n\n    class BaseModelOutput:\n        pass\n\n    class PretrainedConfig:\n        pass\n\n\nfrom .hf_configs import arch_dict\n\n\n# utils\ndef _camel2snake(s):\n    return re.sub(r\"(?<!^)(?=[A-Z])\", \"_\", s).lower()\n\n\n# TODO: ?last - for gpt-like models\n_POOLERS = {}\n\n\ndef register_pooler(cls):\n    \"\"\"Decorator registering pooler class\"\"\"\n    _POOLERS[_camel2snake(cls.__name__)] = cls\n    return cls\n\n\n@register_pooler\nclass MeanPooler(nn.Module):\n    \"\"\"Mean pooling\"\"\"\n\n    def forward(self, x: BaseModelOutput, attention_mask: TensorType):\n        masked_output = x.last_hidden_state * attention_mask.unsqueeze(-1)\n        return masked_output.sum(dim=1) / attention_mask.sum(-1, keepdim=True)\n\n\n@register_pooler\nclass MaxPooler(nn.Module):\n    \"\"\"Max pooling\"\"\"\n\n    def forward(self, x: BaseModelOutput, attention_mask: TensorType):\n        masked_output = x.last_hidden_state.masked_fill(attention_mask.unsqueeze(-1), -torch.inf)\n        return masked_output.max(1).values\n\n\n@register_pooler\nclass ClsPooler(nn.Module):\n    \"\"\"CLS token pooling\"\"\"\n\n    def __init__(self, use_pooler_output=True):\n        super().__init__()\n        self.cls_token_position = 0\n        self.use_pooler_output = use_pooler_output\n\n    def forward(self, x: BaseModelOutput, attention_mask: TensorType):\n\n        if self.use_pooler_output and isinstance(x, (BaseModelOutputWithPooling, BaseModelOutputWithPoolingAndCrossAttentions)) and (x.pooler_output is not None):\n            return x.pooler_output\n\n        return x.last_hidden_state[:, self.cls_token_position, :]\n\n\nclass HFTextEncoder(nn.Module):\n    \"\"\"HuggingFace model adapter\"\"\"\n\n    def __init__(self, model_name_or_path: str, output_dim: int, tokenizer_name: str = None, config: PretrainedConfig = None, pooler_type: str = None, proj: str = None, pretrained: bool = True, masked_language_modeling: bool = False):\n        super().__init__()\n\n        self.output_dim = output_dim\n\n        # TODO: find better way to get this information\n        uses_transformer_pooler = pooler_type == \"cls_pooler\"\n\n        if transformers is None:\n            raise RuntimeError(\"Please `pip install transformers` to use pre-trained HuggingFace models\")\n        if config is None:\n            self.config = AutoConfig.from_pretrained(model_name_or_path)\n            if masked_language_modeling:\n                create_func, model_args = (AutoModelForMaskedLM.from_pretrained, model_name_or_path) if pretrained else (AutoModelForMaskedLM.from_config, self.config)\n            else:\n                create_func, model_args = (AutoModel.from_pretrained, model_name_or_path) if pretrained else (AutoModel.from_config, self.config)\n            # TODO: do all model configs have this attribute? PretrainedConfig does so yes??\n            if hasattr(self.config, \"is_encoder_decoder\") and self.config.is_encoder_decoder:\n                self.transformer = create_func(model_args)\n                self.transformer = self.transformer.encoder\n            else:\n                self.transformer = create_func(model_args, add_pooling_layer=uses_transformer_pooler)\n        else:\n            self.config = config\n            if masked_language_modeling:\n                self.transformer = AutoModelForMaskedLM.from_config(config)\n            else:\n                self.transformer = AutoModel.from_config(config)\n\n        if pooler_type is None:  # get default arch pooler\n            self.pooler = _POOLERS[(arch_dict[self.config.model_type][\"pooler\"])]()\n        else:\n            self.pooler = _POOLERS[pooler_type]()\n\n        d_model = getattr(self.config, arch_dict[self.config.model_type][\"config_names\"][\"width\"])\n        if (d_model == output_dim) and (proj is None):  # do we always need a proj?\n            self.proj = nn.Identity()\n        elif proj == \"linear\":\n            self.proj = nn.Linear(d_model, output_dim, bias=False)\n        elif proj == \"mlp\":\n            hidden_size = (d_model + output_dim) // 2\n            self.proj = nn.Sequential(\n                nn.Linear(d_model, hidden_size, bias=False),\n                nn.GELU(),\n                nn.Linear(hidden_size, output_dim, bias=False),\n            )\n\n        # self.itm_proj = nn.Linear(d_model, 2, bias=False)\n        # self.mlm_proj = nn.Linear(d_model, self.config.vocab_size), bias=False)\n        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n\n    # def forward_itm(self, x:TensorType, image_embeds:TensorType) -> TensorType:\n    #     image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(x.device)\n    #     attn_mask = (x != self.config.pad_token_id).long()\n    #     out = self.transformer(\n    #         input_ids=x,\n    #         attention_mask=attn_mask,\n    #         encoder_hidden_states = image_embeds,\n    #         encoder_attention_mask = image_atts,\n    #         )\n    #     pooled_out = self.pooler(out, attn_mask)\n\n    #     return self.itm_proj(pooled_out)\n\n    def mask(self, input_ids, vocab_size, device, targets=None, masked_indices=None, probability_matrix=None):\n        if masked_indices is None:\n            masked_indices = torch.bernoulli(probability_matrix).bool()\n\n        masked_indices[input_ids == self.tokenizer.pad_token_id] = False\n        masked_indices[input_ids == self.tokenizer.cls_token_id] = False\n\n        if targets is not None:\n            targets[~masked_indices] = -100  # We only compute loss on masked tokens\n\n        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n        indices_replaced = torch.bernoulli(torch.full(input_ids.shape, 0.8)).bool() & masked_indices\n        input_ids[indices_replaced] = self.tokenizer.mask_token_id\n\n        # 10% of the time, we replace masked input tokens with random word\n        indices_random = torch.bernoulli(torch.full(input_ids.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n        random_words = torch.randint(vocab_size, input_ids.shape, dtype=torch.long).to(device)\n        input_ids[indices_random] = random_words[indices_random]\n        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n\n        if targets is not None:\n            return input_ids, targets\n        else:\n            return input_ids\n\n    def forward_mlm(self, input_ids, image_embeds, mlm_probability=0.25):\n        labels = input_ids.clone()\n        attn_mask = (input_ids != self.config.pad_token_id).long()\n        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(input_ids.device)\n        vocab_size = getattr(self.config, arch_dict[self.config.model_type][\"config_names\"][\"vocab_size\"])\n        probability_matrix = torch.full(labels.shape, mlm_probability)\n        input_ids, labels = self.mask(input_ids, vocab_size, input_ids.device, targets=labels, probability_matrix=probability_matrix)\n        mlm_output = self.transformer(\n            input_ids,\n            attention_mask=attn_mask,\n            encoder_hidden_states=image_embeds,\n            encoder_attention_mask=image_atts,\n            return_dict=True,\n            labels=labels,\n        )\n        return mlm_output.loss\n        # mlm_output = self.transformer(input_ids,\n        #                 attention_mask = attn_mask,\n        #                 encoder_hidden_states = image_embeds,\n        #                 encoder_attention_mask = image_atts,\n        #                 return_dict = True,\n        #             ).last_hidden_state\n        # logits = self.mlm_proj(mlm_output)\n\n        # # logits = logits[:, :-1, :].contiguous().view(-1, vocab_size)\n        # logits = logits[:, 1:, :].contiguous().view(-1, vocab_size)\n        # labels = labels[:, 1:].contiguous().view(-1)\n\n        # mlm_loss = F.cross_entropy(\n        #     logits,\n        #     labels,\n        #     # label_smoothing=0.1,\n        # )\n        # return mlm_loss\n\n    def forward(self, x: TensorType) -> TensorType:\n        attn_mask = (x != self.config.pad_token_id).long()\n        out = self.transformer(input_ids=x, attention_mask=attn_mask)\n        pooled_out = self.pooler(out, attn_mask)\n\n        return self.proj(pooled_out)\n\n    def lock(self, unlocked_layers: int = 0, freeze_layer_norm: bool = True):\n        if not unlocked_layers:  # full freezing\n            for n, p in self.transformer.named_parameters():\n                p.requires_grad = (not freeze_layer_norm) if \"LayerNorm\" in n.split(\".\") else False\n            return\n\n        encoder = self.transformer.encoder if hasattr(self.transformer, \"encoder\") else self.transformer\n        layer_list = getattr(encoder, arch_dict[self.config.model_type][\"config_names\"][\"layer_attr\"])\n        print(f\"Unlocking {unlocked_layers}/{len(layer_list) + 1} layers of hf model\")\n        embeddings = getattr(self.transformer, arch_dict[self.config.model_type][\"config_names\"][\"token_embeddings_attr\"])\n        modules = [embeddings, *layer_list][:-unlocked_layers]\n        # freeze layers\n        for module in modules:\n            for n, p in module.named_parameters():\n                p.requires_grad = (not freeze_layer_norm) if \"LayerNorm\" in n.split(\".\") else False\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.transformer.gradient_checkpointing_enable()\n\n    def get_num_layers(self):\n        encoder = self.transformer.encoder if hasattr(self.transformer, \"encoder\") else self.transformer\n        layer_list = getattr(encoder, arch_dict[self.config.model_type][\"config_names\"][\"layer_attr\"])\n        return len(layer_list)\n\n    def init_parameters(self):\n        pass\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/rope.py", "content": "from math import pi\nimport torch\nfrom torch import nn\nfrom einops import rearrange, repeat\nimport logging\n\n\ndef broadcat(tensors, dim=-1):\n    num_tensors = len(tensors)\n    shape_lens = set(list(map(lambda t: len(t.shape), tensors)))\n    assert len(shape_lens) == 1, \"tensors must all have the same number of dimensions\"\n    shape_len = list(shape_lens)[0]\n    dim = (dim + shape_len) if dim < 0 else dim\n    dims = list(zip(*map(lambda t: list(t.shape), tensors)))\n    expandable_dims = [(i, val) for i, val in enumerate(dims) if i != dim]\n    assert all([*map(lambda t: len(set(t[1])) <= 2, expandable_dims)]), \"invalid dimensions for broadcastable concatentation\"\n    max_dims = list(map(lambda t: (t[0], max(t[1])), expandable_dims))\n    expanded_dims = list(map(lambda t: (t[0], (t[1],) * num_tensors), max_dims))\n    expanded_dims.insert(dim, (dim, dims[dim]))\n    expandable_shapes = list(zip(*map(lambda t: t[1], expanded_dims)))\n    tensors = list(map(lambda t: t[0].expand(*t[1]), zip(tensors, expandable_shapes)))\n    return torch.cat(tensors, dim=dim)\n\n\ndef rotate_half(x):\n    x = rearrange(x, \"... (d r) -> ... d r\", r=2)\n    x1, x2 = x.unbind(dim=-1)\n    x = torch.stack((-x2, x1), dim=-1)\n    return rearrange(x, \"... d r -> ... (d r)\")\n\n\nclass VisionRotaryEmbedding(nn.Module):\n    def __init__(\n        self,\n        dim,\n        pt_seq_len,\n        ft_seq_len=None,\n        custom_freqs=None,\n        freqs_for=\"lang\",\n        theta=10000,\n        max_freq=10,\n        num_freqs=1,\n    ):\n        super().__init__()\n        if custom_freqs:\n            freqs = custom_freqs\n        elif freqs_for == \"lang\":\n            freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n        elif freqs_for == \"pixel\":\n            freqs = torch.linspace(1.0, max_freq / 2, dim // 2) * pi\n        elif freqs_for == \"constant\":\n            freqs = torch.ones(num_freqs).float()\n        else:\n            raise ValueError(f\"unknown modality {freqs_for}\")\n\n        if ft_seq_len is None:\n            ft_seq_len = pt_seq_len\n        t = torch.arange(ft_seq_len) / ft_seq_len * pt_seq_len\n\n        freqs_h = torch.einsum(\"..., f -> ... f\", t, freqs)\n        freqs_h = repeat(freqs_h, \"... n -> ... (n r)\", r=2)\n\n        freqs_w = torch.einsum(\"..., f -> ... f\", t, freqs)\n        freqs_w = repeat(freqs_w, \"... n -> ... (n r)\", r=2)\n\n        freqs = broadcat((freqs_h[:, None, :], freqs_w[None, :, :]), dim=-1)\n\n        self.register_buffer(\"freqs_cos\", freqs.cos())\n        self.register_buffer(\"freqs_sin\", freqs.sin())\n\n        logging.info(f\"Shape of rope freq: {self.freqs_cos.shape}\")\n\n    def forward(self, t, start_index=0):\n        rot_dim = self.freqs_cos.shape[-1]\n        end_index = start_index + rot_dim\n        assert rot_dim <= t.shape[-1], f\"feature dimension {t.shape[-1]} is not of sufficient size to rotate in all the positions {rot_dim}\"\n        t_left, t, t_right = t[..., :start_index], t[..., start_index:end_index], t[..., end_index:]\n        t = (t * self.freqs_cos) + (rotate_half(t) * self.freqs_sin)\n\n        return torch.cat((t_left, t, t_right), dim=-1)\n\n\nclass VisionRotaryEmbeddingFast(nn.Module):\n    def __init__(self, dim, pt_seq_len, ft_seq_len=None, custom_freqs=None, freqs_for=\"lang\", theta=10000, max_freq=10, num_freqs=1, patch_dropout=0.0):\n        super().__init__()\n        if custom_freqs:\n            freqs = custom_freqs\n        elif freqs_for == \"lang\":\n            freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n        elif freqs_for == \"pixel\":\n            freqs = torch.linspace(1.0, max_freq / 2, dim // 2) * pi\n        elif freqs_for == \"constant\":\n            freqs = torch.ones(num_freqs).float()\n        else:\n            raise ValueError(f\"unknown modality {freqs_for}\")\n\n        if ft_seq_len is None:\n            ft_seq_len = pt_seq_len\n        t = torch.arange(ft_seq_len) / ft_seq_len * pt_seq_len\n\n        freqs = torch.einsum(\"..., f -> ... f\", t, freqs)\n        freqs = repeat(freqs, \"... n -> ... (n r)\", r=2)\n        freqs = broadcat((freqs[:, None, :], freqs[None, :, :]), dim=-1)\n\n        freqs_cos = freqs.cos().view(-1, freqs.shape[-1])\n        freqs_sin = freqs.sin().view(-1, freqs.shape[-1])\n\n        self.patch_dropout = patch_dropout\n\n        self.register_buffer(\"freqs_cos\", freqs_cos)\n        self.register_buffer(\"freqs_sin\", freqs_sin)\n\n        logging.info(f\"Shape of rope freq: {self.freqs_cos.shape}\")\n\n    def forward(self, t, patch_indices_keep=None):\n        if patch_indices_keep is not None:\n            batch = t.size()[0]\n            batch_indices = torch.arange(batch)\n            batch_indices = batch_indices[..., None]\n\n            freqs_cos = repeat(self.freqs_cos, \"i j -> n i m j\", n=t.shape[0], m=t.shape[1])\n            freqs_sin = repeat(self.freqs_sin, \"i j -> n i m j\", n=t.shape[0], m=t.shape[1])\n\n            freqs_cos = freqs_cos[batch_indices, patch_indices_keep]\n            freqs_cos = rearrange(freqs_cos, \"n i m j -> n m i j\")\n            freqs_sin = freqs_sin[batch_indices, patch_indices_keep]\n            freqs_sin = rearrange(freqs_sin, \"n i m j -> n m i j\")\n\n            return t * freqs_cos + rotate_half(t) * freqs_sin\n\n        return t * self.freqs_cos + rotate_half(t) * self.freqs_sin\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/constants.py", "content": "OPENAI_DATASET_MEAN = (0.48145466, 0.4578275, 0.40821073)\nOPENAI_DATASET_STD = (0.26862954, 0.26130258, 0.27577711)\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/eva_clip/eva_clip_processors.py", "content": "\"\"\"\n# Adapted from https://github.com/baaivision/EVA/tree/master/EVA-CLIP\n\"\"\"\n\nfrom torchvision import transforms\nfrom torchvision.transforms.functional import InterpolationMode\nfrom transformers.image_processing_utils import BatchFeature\nfrom PIL import Image\nfrom transformers.image_transforms import convert_to_rgb\n\n\nclass BaseProcessor:\n    def __init__(self):\n        self.transform = lambda x: x\n        return\n\n    def __call__(self, item):\n        return self.transform(item)\n\n\nclass EvaClipImageBaseProcessor(BaseProcessor):\n    def __init__(self, mean=None, std=None):\n        self.mean = (0.48145466, 0.4578275, 0.40821073) if mean is None else mean\n        self.std = (0.26862954, 0.26130258, 0.27577711) if std is None else std\n\n        self.normalize = transforms.Normalize(self.mean, self.std)\n\n    @property\n    def image_mean(self):\n        return self.mean\n\n\nclass EvaClipImageTrainProcessor(EvaClipImageBaseProcessor):\n    def __init__(self, image_size=224, mean=None, std=None, min_scale=0.5, max_scale=1.0):\n        super().__init__(mean=mean, std=std)\n\n        self.transform = transforms.Compose(\n            [\n                convert_to_rgb,\n                transforms.Resize(\n                    image_size,\n                    interpolation=InterpolationMode.BICUBIC,\n                ),\n                transforms.CenterCrop(image_size),\n                transforms.ToTensor(),\n                self.normalize,\n            ]\n        )\n\n        self.image_size = image_size\n\n    def preprocess(self, images, return_tensors):\n        if isinstance(images, Image.Image):\n            images = [images]\n        else:\n            assert isinstance(images, list)\n\n        transformed_images = [self.transform(image).numpy() for image in images]\n        data = {\"pixel_values\": transformed_images}\n\n        return BatchFeature(data=data, tensor_type=return_tensors)\n\n    def __call__(self, item):\n        return self.transform(item)\n\n    @property\n    def crop_size(self):\n        return {\"height\": self.image_size, \"width\": self.image_size}\n\n    @property\n    def size(self):\n        return {\"shortest_edge\": self.image_size}\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_vit.py", "content": "# Based on EVA, BEIT, timm and DeiT code bases\n# https://github.com/baaivision/EVA\n# https://github.com/rwightman/pytorch-image-models/tree/master/timm\n# https://github.com/microsoft/unilm/tree/master/beit\n# https://github.com/facebookresearch/deit/\n# https://github.com/facebookresearch/dino\n# --------------------------------------------------------'\n# not tested yet\nimport math\nfrom transformers import CLIPImageProcessor\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint\nfrom timm.models.layers import drop_path, to_2tuple, trunc_normal_\nfrom .eva_clip import create_model_and_transforms, get_model_config\nimport torch\nimport torchvision\nimport time\n\nfrom llava.utils import rank0_print\n\n\nclass EvaViTWrapper(nn.Module):\n    def __init__(self, vision_tower, args, delay_load=False):\n        super().__init__()\n\n        self.is_loaded = False\n        self.vision_tower_name = vision_tower\n        self.pretrained = args.vision_tower_pretrained\n        self.args = args\n\n        self.select_layer = args.mm_vision_select_layer\n        if self.select_layer < -1:\n            self.select_layer += 1\n        self.select_feature = getattr(args, \"mm_vision_select_feature\", \"patch\")\n\n        self.model_config = get_model_config(self.vision_tower_name)\n\n        if not delay_load:\n            rank0_print(f\"Loading vision tower: {vision_tower}\")\n            self.load_model()\n        elif getattr(args, \"unfreeze_mm_vision_tower\", False):\n            # TODO: better detector is needed.\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `unfreeze_mm_vision_tower`: True.\")\n            self.load_model()\n        elif hasattr(args, \"mm_tunable_parts\") and \"mm_vision_tower\" in args.mm_tunable_parts:\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `mm_tunable_parts` contains `mm_vision_tower`.\")\n            self.load_model()\n\n    def load_model(self):\n        rank0_print(f\"Loading: {self.vision_tower_name}\")\n        rank0_print(f\"Pretrained: {self.pretrained}\")\n        time_start = time.time()\n        model, _, image_processor = create_model_and_transforms(self.vision_tower_name, self.pretrained, force_custom_clip=True, precision=\"fp16\")\n        time_end = time.time()\n        rank0_print(f\"Loaded: {self.vision_tower_name} in {time_end - time_start:.2f}s\")\n        self.device = next(model.parameters()).device\n        self.dtype = next(model.parameters()).dtype\n        if self.device.type != \"meta\":\n            model = model.to(\"cuda\")\n        self.vision_tower = model.visual\n        resize_transform = [t for t in image_processor.transforms if isinstance(t, torchvision.transforms.Resize)][0]\n        normalize_transform = [t for t in image_processor.transforms if isinstance(t, torchvision.transforms.Normalize)][0]\n        self.resize_transform_size = resize_transform.size\n        self.image_processor = CLIPImageProcessor.from_pretrained(\n            \"openai/clip-vit-large-patch14\",\n            crop_size=resize_transform.size,\n            size={\"shortest_edge\": resize_transform.size},\n            image_mean=list(normalize_transform.mean),\n            image_std=list(normalize_transform.std),\n        )\n        rank0_print(f\"Loaded image processor: {self.image_processor}\")\n        self.vision_tower.requires_grad_(False)\n        self.is_loaded = True\n\n    def feature_select(self, image_features):\n        select_feature_type = self.select_feature\n\n        # if self.select_feature in [\"slicefour_patch\", \"slicefour_cls_patch\"]:\n        #     select_every_k_layer = len(image_features) // 4\n        #     image_features = torch.cat([image_features[i] for i in range(select_every_k_layer + self.select_layer, len(image_features), select_every_k_layer)], dim=-1)\n        #     select_feature_type = select_feature_type.replace(\"slicefour_\", \"\")\n        # elif self.select_feature in [\"slice_m25811_f6_patch\", \"slice_m25811_f6_cls_patch\"]:\n        #     select_layers = [-1, -4, -7, -10, 6]\n        #     image_features = torch.cat([image_features[i] for i in select_layers], dim=-1)\n        #     select_feature_type = select_feature_type.replace(\"slice_m25811_f6_\", \"\")\n        # else:\n        #     image_features = image_features[self.select_layer]\n\n        if select_feature_type == \"patch\":\n            image_features = image_features[:, 1:]\n        elif select_feature_type == \"cls_patch\":\n            image_features = image_features\n        else:\n            raise ValueError(f\"Unexpected select feature: {select_feature_type}\")\n        return image_features\n\n    def train(self, mode=True):\n        self.training = mode\n\n        if self.is_loaded:\n            self.vision_tower.eval()\n\n    def forward(self, images):\n        if type(images) is list:\n            image_features = []\n            for image in images:\n                image_features = self.vision_tower.forward_features(image.to(self.dtype), return_all_features=True)\n                image_features = self.feature_select(image_features).to(self.dtype)\n                image_features.append(image_features)\n        else:\n            image_features = self.vision_tower.forward_features(images.to(self.dtype), return_all_features=True)\n            image_features = self.feature_select(image_features).to(self.dtype)\n\n        return image_features\n\n    @property\n    def dummy_feature(self):\n        return torch.zeros(1, self.hidden_size, device=self.device, dtype=self.dtype)\n\n    @property\n    def hidden_size(self):\n        return self.model_config[\"vision_cfg\"][\"width\"]\n\n    @property\n    def num_patches(self):\n        return (self.model_config[\"vision_cfg\"][\"image_size\"] // self.model_config[\"vision_cfg\"][\"patch_size\"]) ** 2\n\n    @property\n    def num_patches_per_side(self):\n        return self.model_config[\"vision_cfg\"][\"image_size\"] // self.model_config[\"vision_cfg\"][\"patch_size\"]\n\n    @property\n    def config(self):\n        return self.model_config\n\n    @property\n    def image_size(self):\n        return self.model_config[\"vision_cfg\"][\"image_size\"]\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/timm_model.py", "content": "\"\"\" timm model adapter\n\nWraps timm (https://github.com/rwightman/pytorch-image-models) models for use as a vision tower in CLIP model.\n\"\"\"\n\nimport logging\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\n\ntry:\n    import timm\n    from timm.models.layers import Mlp, to_2tuple\n\n    try:\n        # old timm imports < 0.8.1\n        from timm.models.layers.attention_pool2d import RotAttentionPool2d\n        from timm.models.layers.attention_pool2d import AttentionPool2d as AbsAttentionPool2d\n    except ImportError:\n        # new timm imports >= 0.8.1\n        from timm.layers import RotAttentionPool2d\n        from timm.layers import AttentionPool2d as AbsAttentionPool2d\nexcept ImportError:\n    timm = None\n\nfrom .utils import freeze_batch_norm_2d\n\n\nclass TimmModel(nn.Module):\n    \"\"\"timm model adapter\n    # FIXME this adapter is a work in progress, may change in ways that break weight compat\n    \"\"\"\n\n    def __init__(self, model_name, embed_dim, image_size=224, pool=\"avg\", proj=\"linear\", proj_bias=False, drop=0.0, pretrained=False):\n        super().__init__()\n        if timm is None:\n            raise RuntimeError(\"Please `pip install timm` to use timm models.\")\n\n        self.image_size = to_2tuple(image_size)\n        self.trunk = timm.create_model(model_name, pretrained=pretrained)\n        feat_size = self.trunk.default_cfg.get(\"pool_size\", None)\n        feature_ndim = 1 if not feat_size else 2\n        if pool in (\"abs_attn\", \"rot_attn\"):\n            assert feature_ndim == 2\n            # if attn pooling used, remove both classifier and default pool\n            self.trunk.reset_classifier(0, global_pool=\"\")\n        else:\n            # reset global pool if pool config set, otherwise leave as network default\n            reset_kwargs = dict(global_pool=pool) if pool else {}\n            self.trunk.reset_classifier(0, **reset_kwargs)\n        prev_chs = self.trunk.num_features\n\n        head_layers = OrderedDict()\n        if pool == \"abs_attn\":\n            head_layers[\"pool\"] = AbsAttentionPool2d(prev_chs, feat_size=feat_size, out_features=embed_dim)\n            prev_chs = embed_dim\n        elif pool == \"rot_attn\":\n            head_layers[\"pool\"] = RotAttentionPool2d(prev_chs, out_features=embed_dim)\n            prev_chs = embed_dim\n        else:\n            assert proj, \"projection layer needed if non-attention pooling is used.\"\n\n        # NOTE attention pool ends with a projection layer, so proj should usually be set to '' if such pooling is used\n        if proj == \"linear\":\n            head_layers[\"drop\"] = nn.Dropout(drop)\n            head_layers[\"proj\"] = nn.Linear(prev_chs, embed_dim, bias=proj_bias)\n        elif proj == \"mlp\":\n            head_layers[\"mlp\"] = Mlp(prev_chs, 2 * embed_dim, embed_dim, drop=drop, bias=(True, proj_bias))\n\n        self.head = nn.Sequential(head_layers)\n\n    def lock(self, unlocked_groups=0, freeze_bn_stats=False):\n        \"\"\"lock modules\n        Args:\n            unlocked_groups (int): leave last n layer groups unlocked (default: 0)\n        \"\"\"\n        if not unlocked_groups:\n            # lock full model\n            for param in self.trunk.parameters():\n                param.requires_grad = False\n            if freeze_bn_stats:\n                freeze_batch_norm_2d(self.trunk)\n        else:\n            # NOTE: partial freeze requires latest timm (master) branch and is subject to change\n            try:\n                # FIXME import here until API stable and in an official release\n                from timm.models.helpers import group_parameters, group_modules\n            except ImportError:\n                raise RuntimeError(\"Please install latest timm `pip install git+https://github.com/rwightman/pytorch-image-models`\")\n            matcher = self.trunk.group_matcher()\n            gparams = group_parameters(self.trunk, matcher)\n            max_layer_id = max(gparams.keys())\n            max_layer_id = max_layer_id - unlocked_groups\n            for group_idx in range(max_layer_id + 1):\n                group = gparams[group_idx]\n                for param in group:\n                    self.trunk.get_parameter(param).requires_grad = False\n            if freeze_bn_stats:\n                gmodules = group_modules(self.trunk, matcher, reverse=True)\n                gmodules = {k for k, v in gmodules.items() if v <= max_layer_id}\n                freeze_batch_norm_2d(self.trunk, gmodules)\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        try:\n            self.trunk.set_grad_checkpointing(enable)\n        except Exception as e:\n            logging.warning(\"grad checkpointing not supported for this timm image tower, continuing without...\")\n\n    def forward(self, x):\n        x = self.trunk(x)\n        x = self.head(x)\n        return x\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/eva_clip/eva_clip_encoder.py", "content": "import torch\nimport torch.nn as nn\n\nfrom .eva_clip_processors import EvaClipImageTrainProcessor\nfrom .eva_vit import EVAEncoderWrapper\nfrom .factory import list_models, add_model_config, get_model_config\n\nfrom llava.utils import rank0_print\n\n\nclass EvaClipVisionTower(nn.Module):\n    def __init__(self, vision_tower, args, delay_load=False):\n        super().__init__()\n\n        self.is_loaded = False\n        self.vision_tower_name = vision_tower\n        self.vision_tower_pretrained = args.vision_tower_pretrained\n        self.config = get_model_config(vision_tower)\n\n        if not delay_load:\n            rank0_print(f\"Loading EVA ViT: {self.vision_tower_name}\")\n            self.load_model()\n        elif getattr(args, \"unfreeze_mm_vision_tower\", False):\n            # TODO: better detector is needed.\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `unfreeze_mm_vision_tower`: True.\")\n            self.load_model()\n        elif hasattr(args, \"mm_tunable_parts\") and \"mm_vision_tower\" in args.mm_tunable_parts:\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `mm_tunable_parts` contains `mm_vision_tower`.\")\n            self.load_model()\n        else:\n            self.cfg_only = self.config\n\n    def load_model(self, device_map=None):\n        rank0_print(f\"Pretrained: {self.vision_tower_pretrained}\")\n        self.image_processor = EvaClipImageTrainProcessor(self.config[\"vision_cfg\"][\"image_size\"])\n        self.vision_tower = EVAEncoderWrapper(self.vision_tower_pretrained, self.config)\n        rank0_print(f\"Loaded image processor: {self.image_processor}\")\n        self.vision_tower.requires_grad_(False)\n        self.is_loaded = True\n\n    def forward(self, images):\n        if type(images) is list:\n            image_features = []\n            for image in images:\n                image_feature = self.vision_tower(image.to(device=self.device, dtype=self.dtype).unsqueeze(0)).to(image.dtype)\n                image_features.append(image_feature)\n        else:\n            image_features = self.vision_tower(images.to(device=self.device, dtype=self.dtype)).to(images.dtype)\n\n        return image_features\n\n    @property\n    def dtype(self):\n        return self.vision_tower.dtype\n\n    @property\n    def device(self):\n        return self.vision_tower.device\n\n    @property\n    def hidden_size(self):\n        return self.config[\"vision_cfg\"][\"width\"]\n\n    @property\n    def num_patches(self):\n        return (self.config[\"vision_cfg\"][\"image_size\"] // self.config[\"vision_cfg\"][\"patch_size\"]) ** 2\n\n    @property\n    def num_patches_per_side(self):\n        return self.config[\"vision_cfg\"][\"image_size\"] // self.config[\"vision_cfg\"][\"patch_size\"]\n\n    @property\n    def image_size(self):\n        return self.config[\"vision_cfg\"][\"image_size\"]\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/hf_configs.py", "content": "# HF architecture dict:\narch_dict = {\n    # https://huggingface.co/docs/transformers/model_doc/roberta#roberta\n    \"roberta\": {\n        \"config_names\": {\n            \"context_length\": \"max_position_embeddings\",\n            \"vocab_size\": \"vocab_size\",\n            \"width\": \"hidden_size\",\n            \"heads\": \"num_attention_heads\",\n            \"layers\": \"num_hidden_layers\",\n            \"layer_attr\": \"layer\",\n            \"token_embeddings_attr\": \"embeddings\",\n        },\n        \"pooler\": \"mean_pooler\",\n    },\n    # https://huggingface.co/docs/transformers/model_doc/xlm-roberta#transformers.XLMRobertaConfig\n    \"xlm-roberta\": {\n        \"config_names\": {\n            \"context_length\": \"max_position_embeddings\",\n            \"vocab_size\": \"vocab_size\",\n            \"width\": \"hidden_size\",\n            \"heads\": \"num_attention_heads\",\n            \"layers\": \"num_hidden_layers\",\n            \"layer_attr\": \"layer\",\n            \"token_embeddings_attr\": \"embeddings\",\n        },\n        \"pooler\": \"mean_pooler\",\n    },\n    # https://huggingface.co/docs/transformers/model_doc/mt5#mt5\n    \"mt5\": {\n        \"config_names\": {\n            # unlimited seqlen\n            # https://github.com/google-research/text-to-text-transfer-transformer/issues/273\n            # https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/t5/modeling_t5.py#L374\n            \"context_length\": \"\",\n            \"vocab_size\": \"vocab_size\",\n            \"width\": \"d_model\",\n            \"heads\": \"num_heads\",\n            \"layers\": \"num_layers\",\n            \"layer_attr\": \"block\",\n            \"token_embeddings_attr\": \"embed_tokens\",\n        },\n        \"pooler\": \"mean_pooler\",\n    },\n    \"bert\": {\n        \"config_names\": {\n            \"context_length\": \"max_position_embeddings\",\n            \"vocab_size\": \"vocab_size\",\n            \"width\": \"hidden_size\",\n            \"heads\": \"num_attention_heads\",\n            \"layers\": \"num_hidden_layers\",\n            \"layer_attr\": \"layer\",\n            \"token_embeddings_attr\": \"embeddings\",\n        },\n        \"pooler\": \"mean_pooler\",\n    },\n}\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/eva_clip/eva_vit.py", "content": "\"\"\"\n# Adapted from https://github.com/baaivision/EVA/tree/master/EVA-CLIP\n\"\"\"\n\nfrom math import pi\nimport torch\nfrom torch import nn\nfrom einops import rearrange, repeat\nimport logging\nfrom llava.utils import rank0_print\n\n\ndef broadcat(tensors, dim=-1):\n    num_tensors = len(tensors)\n    shape_lens = set(list(map(lambda t: len(t.shape), tensors)))\n    assert len(shape_lens) == 1, \"tensors must all have the same number of dimensions\"\n    shape_len = list(shape_lens)[0]\n    dim = (dim + shape_len) if dim < 0 else dim\n    dims = list(zip(*map(lambda t: list(t.shape), tensors)))\n    expandable_dims = [(i, val) for i, val in enumerate(dims) if i != dim]\n    assert all([*map(lambda t: len(set(t[1])) <= 2, expandable_dims)]), \"invalid dimensions for broadcastable concatentation\"\n    max_dims = list(map(lambda t: (t[0], max(t[1])), expandable_dims))\n    expanded_dims = list(map(lambda t: (t[0], (t[1],) * num_tensors), max_dims))\n    expanded_dims.insert(dim, (dim, dims[dim]))\n    expandable_shapes = list(zip(*map(lambda t: t[1], expanded_dims)))\n    tensors = list(map(lambda t: t[0].expand(*t[1]), zip(tensors, expandable_shapes)))\n    return torch.cat(tensors, dim=dim)\n\n\ndef rotate_half(x):\n    x = rearrange(x, \"... (d r) -> ... d r\", r=2)\n    x1, x2 = x.unbind(dim=-1)\n    x = torch.stack((-x2, x1), dim=-1)\n    return rearrange(x, \"... d r -> ... (d r)\")\n\n\nclass VisionRotaryEmbeddingFast(nn.Module):\n    def __init__(self, dim, pt_seq_len, ft_seq_len=None, custom_freqs=None, freqs_for=\"lang\", theta=10000, max_freq=10, num_freqs=1, patch_dropout=0.0):\n        super().__init__()\n        if custom_freqs:\n            freqs = custom_freqs\n        elif freqs_for == \"lang\":\n            freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n        elif freqs_for == \"pixel\":\n            freqs = torch.linspace(1.0, max_freq / 2, dim // 2) * pi\n        elif freqs_for == \"constant\":\n            freqs = torch.ones(num_freqs).float()\n        else:\n            raise ValueError(f\"unknown modality {freqs_for}\")\n\n        if ft_seq_len is None:\n            ft_seq_len = pt_seq_len\n        t = torch.arange(ft_seq_len) / ft_seq_len * pt_seq_len\n\n        freqs = torch.einsum(\"..., f -> ... f\", t, freqs)\n        freqs = repeat(freqs, \"... n -> ... (n r)\", r=2)\n        freqs = broadcat((freqs[:, None, :], freqs[None, :, :]), dim=-1)\n\n        freqs_cos = freqs.cos().view(-1, freqs.shape[-1])\n        freqs_sin = freqs.sin().view(-1, freqs.shape[-1])\n\n        self.patch_dropout = patch_dropout\n\n        self.register_buffer(\"freqs_cos\", freqs_cos)\n        self.register_buffer(\"freqs_sin\", freqs_sin)\n\n        logging.info(f\"Shape of rope freq: {self.freqs_cos.shape}\")\n\n    def forward(self, t, patch_indices_keep=None):\n        if patch_indices_keep is not None:\n            batch = t.size()[0]\n            batch_indices = torch.arange(batch)\n            batch_indices = batch_indices[..., None]\n\n            freqs_cos = repeat(self.freqs_cos, \"i j -> n i m j\", n=t.shape[0], m=t.shape[1])\n            freqs_sin = repeat(self.freqs_sin, \"i j -> n i m j\", n=t.shape[0], m=t.shape[1])\n\n            freqs_cos = freqs_cos[batch_indices, patch_indices_keep]\n            freqs_cos = rearrange(freqs_cos, \"n i m j -> n m i j\")\n            freqs_sin = freqs_sin[batch_indices, patch_indices_keep]\n            freqs_sin = rearrange(freqs_sin, \"n i m j -> n m i j\")\n\n            return t * freqs_cos + rotate_half(t) * freqs_sin\n\n        return t * self.freqs_cos + rotate_half(t) * self.freqs_sin\n\n\nclass LayerNorm(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm (with cast back to input dtype).\"\"\"\n\n    def forward(self, x: torch.Tensor):\n        orig_type = x.dtype\n        x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n        return x.to(orig_type)\n\n\nclass PatchDropout(nn.Module):\n    \"\"\"\n    https://arxiv.org/abs/2212.00794\n    \"\"\"\n\n    def __init__(self, prob, exclude_first_token=True):\n        super().__init__()\n        assert 0 <= prob < 1.0\n        self.prob = prob\n        self.exclude_first_token = exclude_first_token  # exclude CLS token\n        logging.info(f\"os.getenv('RoPE')={os.getenv('RoPE')}\")\n\n    def forward(self, x):\n        if not self.training or self.prob == 0.0:\n            return x\n\n        if self.exclude_first_token:\n            cls_tokens, x = x[:, :1], x[:, 1:]\n        else:\n            cls_tokens = torch.jit.annotate(torch.Tensor, x[:, :1])\n\n        batch = x.size()[0]\n        num_tokens = x.size()[1]\n\n        batch_indices = torch.arange(batch)\n        batch_indices = batch_indices[..., None]\n\n        keep_prob = 1 - self.prob\n        num_patches_keep = max(1, int(num_tokens * keep_prob))\n\n        rand = torch.randn(batch, num_tokens)\n        patch_indices_keep = rand.topk(num_patches_keep, dim=-1).indices\n\n        x = x[batch_indices, patch_indices_keep]\n\n        if self.exclude_first_token:\n            x = torch.cat((cls_tokens, x), dim=1)\n\n        if self.training and os.getenv(\"RoPE\") == \"1\":\n            return x, patch_indices_keep\n\n        return x\n\n\n# --------------------------------------------------------\n# Adapted from  https://github.com/microsoft/unilm/tree/master/beit\n# --------------------------------------------------------\nimport math\nimport os\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntry:\n    from timm.models.layers import drop_path, to_2tuple, trunc_normal_\nexcept:\n    from timm.layers import drop_path, to_2tuple, trunc_normal_\n\nif os.getenv(\"ENV_TYPE\") == \"deepspeed\":\n    try:\n        from deepspeed.runtime.activation_checkpointing.checkpointing import checkpoint\n    except:\n        from torch.utils.checkpoint import checkpoint\nelse:\n    from torch.utils.checkpoint import checkpoint\n\ntry:\n    import xformers.ops as xops\nexcept ImportError:\n    xops = None\n    # print(\"Please 'pip install xformers'\")\n\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n\n    def extra_repr(self) -> str:\n        return \"p={}\".format(self.drop_prob)\n\n\nclass Mlp(nn.Module):\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        drop=0.0,\n        subln=False,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n\n        self.ffn_ln = norm_layer(hidden_features) if subln else nn.Identity()\n\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        # x = self.drop(x)\n        # commit this for the orignal BERT implement\n        x = self.ffn_ln(x)\n\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass SwiGLU(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.SiLU, drop=0.0, norm_layer=nn.LayerNorm, subln=False):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n\n        self.w1 = nn.Linear(in_features, hidden_features)\n        self.w2 = nn.Linear(in_features, hidden_features)\n\n        self.act = act_layer()\n        self.ffn_ln = norm_layer(hidden_features) if subln else nn.Identity()\n        self.w3 = nn.Linear(hidden_features, out_features)\n\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x1 = self.w1(x)\n        x2 = self.w2(x)\n        hidden = self.act(x1) * x2\n        x = self.ffn_ln(hidden)\n        x = self.w3(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.0, proj_drop=0.0, window_size=None, attn_head_dim=None, xattn=False, rope=None, subln=False, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        if attn_head_dim is not None:\n            head_dim = attn_head_dim\n        all_head_dim = head_dim * self.num_heads\n        self.scale = qk_scale or head_dim**-0.5\n\n        self.subln = subln\n        if self.subln:\n            self.q_proj = nn.Linear(dim, all_head_dim, bias=False)\n            self.k_proj = nn.Linear(dim, all_head_dim, bias=False)\n            self.v_proj = nn.Linear(dim, all_head_dim, bias=False)\n        else:\n            self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n\n        if qkv_bias:\n            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n        else:\n            self.q_bias = None\n            self.v_bias = None\n\n        if window_size:\n            self.window_size = window_size\n            self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n            self.relative_position_bias_table = nn.Parameter(torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n            # cls to token & token 2 cls & cls to cls\n\n            # get pair-wise relative position index for each token inside the window\n            coords_h = torch.arange(window_size[0])\n            coords_w = torch.arange(window_size[1])\n            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n            relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n            relative_coords[:, :, 1] += window_size[1] - 1\n            relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n            relative_position_index = torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n            relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n            relative_position_index[0, 0:] = self.num_relative_distance - 3\n            relative_position_index[0:, 0] = self.num_relative_distance - 2\n            relative_position_index[0, 0] = self.num_relative_distance - 1\n\n            self.register_buffer(\"relative_position_index\", relative_position_index)\n        else:\n            self.window_size = None\n            self.relative_position_bias_table = None\n            self.relative_position_index = None\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.inner_attn_ln = norm_layer(all_head_dim) if subln else nn.Identity()\n        # self.proj = nn.Linear(all_head_dim, all_head_dim)\n        self.proj = nn.Linear(all_head_dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.xattn = xattn\n        self.xattn_drop = attn_drop\n\n        self.rope = rope\n\n    def forward(self, x, rel_pos_bias=None, attn_mask=None):\n        B, N, C = x.shape\n        if self.subln:\n            q = F.linear(input=x, weight=self.q_proj.weight, bias=self.q_bias)\n            k = F.linear(input=x, weight=self.k_proj.weight, bias=None)\n            v = F.linear(input=x, weight=self.v_proj.weight, bias=self.v_bias)\n\n            q = q.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)  # B, num_heads, N, C\n            k = k.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n            v = v.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n        else:\n\n            qkv_bias = None\n            if self.q_bias is not None:\n                qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n\n            qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n            qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)  # 3, B, num_heads, N, C\n            q, k, v = qkv[0], qkv[1], qkv[2]\n\n        if self.rope:\n            # slightly fast impl\n            q_t = q[:, :, 1:, :]\n            ro_q_t = self.rope(q_t)\n            q = torch.cat((q[:, :, :1, :], ro_q_t), -2).type_as(v)\n\n            k_t = k[:, :, 1:, :]\n            ro_k_t = self.rope(k_t)\n            k = torch.cat((k[:, :, :1, :], ro_k_t), -2).type_as(v)\n\n        if self.xattn and xops is not None:\n            q = q.permute(0, 2, 1, 3)  # B, num_heads, N, C -> B, N, num_heads, C\n            k = k.permute(0, 2, 1, 3)\n            v = v.permute(0, 2, 1, 3)\n\n            x = xops.memory_efficient_attention(\n                q,\n                k,\n                v,\n                p=self.xattn_drop,\n                scale=self.scale,\n            )\n            x = x.reshape(B, N, -1)\n            x = self.inner_attn_ln(x)\n            x = self.proj(x)\n            x = self.proj_drop(x)\n        else:\n            q = q * self.scale\n            attn = q @ k.transpose(-2, -1)\n\n            if self.relative_position_bias_table is not None:\n                relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(self.window_size[0] * self.window_size[1] + 1, self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n                relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n                attn = attn + relative_position_bias.unsqueeze(0).type_as(attn)\n\n            if rel_pos_bias is not None:\n                attn = attn + rel_pos_bias.type_as(attn)\n\n            if attn_mask is not None:\n                attn_mask = attn_mask.bool()\n                attn = attn.masked_fill(~attn_mask[:, None, None, :], float(\"-inf\"))\n\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n\n            x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n            x = self.inner_attn_ln(x)\n            x = self.proj(x)\n            x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        init_values=None,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        window_size=None,\n        attn_head_dim=None,\n        xattn=False,\n        rope=None,\n        postnorm=False,\n        subln=False,\n        naiveswiglu=False,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim, xattn=xattn, rope=rope, subln=subln, norm_layer=norm_layer\n        )\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n\n        if naiveswiglu:\n            self.mlp = SwiGLU(\n                in_features=dim,\n                hidden_features=mlp_hidden_dim,\n                subln=subln,\n                norm_layer=norm_layer,\n            )\n        else:\n            self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, subln=subln, drop=drop)\n\n        if init_values is not None and init_values > 0:\n            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n        else:\n            self.gamma_1, self.gamma_2 = None, None\n\n        self.postnorm = postnorm\n\n    def forward(self, x, rel_pos_bias=None, attn_mask=None):\n        if self.gamma_1 is None:\n            if self.postnorm:\n                x = x + self.drop_path(self.norm1(self.attn(x, rel_pos_bias=rel_pos_bias, attn_mask=attn_mask)))\n                x = x + self.drop_path(self.norm2(self.mlp(x)))\n            else:\n                x = x + self.drop_path(self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias, attn_mask=attn_mask))\n                x = x + self.drop_path(self.mlp(self.norm2(x)))\n        else:\n            if self.postnorm:\n                x = x + self.drop_path(self.gamma_1 * self.norm1(self.attn(x, rel_pos_bias=rel_pos_bias, attn_mask=attn_mask)))\n                x = x + self.drop_path(self.gamma_2 * self.norm2(self.mlp(x)))\n            else:\n                x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias, attn_mask=attn_mask))\n                x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n        return x\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\"Image to Patch Embedding\"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x, **kwargs):\n        B, C, H, W = x.shape\n        # FIXME look at relaxing size constraints\n        assert H == self.img_size[0] and W == self.img_size[1], f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x\n\n\nclass RelativePositionBias(nn.Module):\n\n    def __init__(self, window_size, num_heads):\n        super().__init__()\n        self.window_size = window_size\n        self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n        self.relative_position_bias_table = nn.Parameter(torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n        # cls to token & token 2 cls & cls to cls\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(window_size[0])\n        coords_w = torch.arange(window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n        relative_position_index = torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n        relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        relative_position_index[0, 0:] = self.num_relative_distance - 3\n        relative_position_index[0:, 0] = self.num_relative_distance - 2\n        relative_position_index[0, 0] = self.num_relative_distance - 1\n\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n    def forward(self):\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(self.window_size[0] * self.window_size[1] + 1, self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n        return relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n\n\nclass EVAVisionTransformer(nn.Module):\n    \"\"\"Vision Transformer with support for patch or hybrid CNN input stage\"\"\"\n\n    def __init__(\n        self,\n        img_size=224,\n        patch_size=16,\n        in_chans=3,\n        num_classes=1000,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.0,\n        norm_layer=nn.LayerNorm,\n        init_values=None,\n        patch_dropout=0.0,\n        use_abs_pos_emb=True,\n        use_rel_pos_bias=False,\n        use_shared_rel_pos_bias=False,\n        rope=False,\n        use_mean_pooling=True,\n        init_scale=0.001,\n        grad_checkpointing=False,\n        xattn=False,\n        postnorm=False,\n        pt_hw_seq_len=16,\n        intp_freq=False,\n        naiveswiglu=False,\n        subln=False,\n    ):\n        super().__init__()\n        self.image_size = img_size\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n\n        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        # self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        if use_abs_pos_emb:\n            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        else:\n            self.pos_embed = None\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        if use_shared_rel_pos_bias:\n            self.rel_pos_bias = RelativePositionBias(window_size=self.patch_embed.patch_shape, num_heads=num_heads)\n        else:\n            self.rel_pos_bias = None\n\n        if rope:\n            half_head_dim = embed_dim // num_heads // 2\n            hw_seq_len = img_size // patch_size\n            self.rope = VisionRotaryEmbeddingFast(\n                dim=half_head_dim,\n                pt_seq_len=pt_hw_seq_len,\n                ft_seq_len=hw_seq_len if intp_freq else None,\n                # patch_dropout=patch_dropout\n            )\n        else:\n            self.rope = None\n\n        self.naiveswiglu = naiveswiglu\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        self.use_rel_pos_bias = use_rel_pos_bias\n        self.blocks = nn.ModuleList(\n            [\n                Block(\n                    dim=embed_dim,\n                    num_heads=num_heads,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    drop=drop_rate,\n                    attn_drop=attn_drop_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                    init_values=init_values,\n                    window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None,\n                    xattn=xattn,\n                    rope=self.rope,\n                    postnorm=postnorm,\n                    subln=subln,\n                    naiveswiglu=naiveswiglu,\n                )\n                for i in range(depth)\n            ]\n        )\n        self.norm = nn.Identity() if use_mean_pooling else norm_layer(embed_dim)\n        self.fc_norm = norm_layer(embed_dim) if use_mean_pooling else None\n        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n        if self.pos_embed is not None:\n            trunc_normal_(self.pos_embed, std=0.02)\n\n        trunc_normal_(self.cls_token, std=0.02)\n        # trunc_normal_(self.mask_token, std=.02)\n\n        self.apply(self._init_weights)\n        self.fix_init_weight()\n\n        if isinstance(self.head, nn.Linear):\n            trunc_normal_(self.head.weight, std=0.02)\n            self.head.weight.data.mul_(init_scale)\n            self.head.bias.data.mul_(init_scale)\n\n        # setting a patch_dropout of 0. would mean it is disabled and this function would be the identity fn\n        self.patch_dropout = PatchDropout(patch_dropout) if patch_dropout > 0.0 else nn.Identity()\n\n        self.grad_checkpointing = grad_checkpointing\n\n    def fix_init_weight(self):\n        def rescale(param, layer_id):\n            param.div_(math.sqrt(2.0 * layer_id))\n\n        for layer_id, layer in enumerate(self.blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            if self.naiveswiglu:\n                rescale(layer.mlp.w3.weight.data, layer_id + 1)\n            else:\n                rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n\n    def get_cast_dtype(self) -> torch.dtype:\n        return self.blocks[0].mlp.fc2.weight.dtype\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def get_num_layers(self):\n        return len(self.blocks)\n\n    def lock(self, unlocked_groups=0, freeze_bn_stats=False):\n        assert unlocked_groups == 0, \"partial locking not currently supported for this model\"\n        for param in self.parameters():\n            param.requires_grad = False\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\"pos_embed\", \"cls_token\"}\n\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=\"\"):\n        self.num_classes = num_classes\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x, return_all_features=False):\n\n        x = self.patch_embed(x)\n        batch_size, seq_len, _ = x.size()\n\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        x = torch.cat((cls_tokens, x), dim=1)\n        if self.pos_embed is not None:\n            x = x + self.pos_embed\n        x = self.pos_drop(x)\n\n        # a patch_dropout of 0. would mean it is disabled and this function would do nothing but return what was passed in\n        if os.getenv(\"RoPE\") == \"1\":\n            if self.training and not isinstance(self.patch_dropout, nn.Identity):\n                x, patch_indices_keep = self.patch_dropout(x)\n                # Directly pass patch_indices_keep to self.rope.forward\n                x = self.rope.forward(x, patch_indices_keep=patch_indices_keep)\n            else:\n                # Pass None or omit the patch_indices_keep argument for default behavior\n                x = self.rope.forward(x, patch_indices_keep=None)\n                x = self.patch_dropout(x)\n        else:\n            x = self.patch_dropout(x)\n\n        rel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None\n        for i, blk in enumerate(self.blocks):\n            if i == len(self.blocks) - 1:\n                continue\n            if self.grad_checkpointing:\n                x = checkpoint(blk, x, (rel_pos_bias,))\n            else:\n                x = blk(x, rel_pos_bias=rel_pos_bias)\n\n        if not return_all_features:\n            x = self.norm(x)\n            if self.fc_norm is not None:\n                return self.fc_norm(x.mean(1))\n            else:\n                return x[:, 0]\n        return x\n\n    def forward(self, x, return_all_features=False):\n        if return_all_features:\n            return self.forward_features(x, return_all_features)\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x\n\n\ndef load_state_dict(checkpoint_path: str, map_location: str = \"cpu\", model_key: str = \"model|module|state_dict\", is_openai: bool = False, skip_list: list = []):\n    if is_openai:\n        model = torch.jit.load(checkpoint_path, map_location=\"cpu\").eval()\n        state_dict = model.state_dict()\n        for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n            state_dict.pop(key, None)\n    else:\n        checkpoint = torch.load(checkpoint_path, map_location=map_location)\n        for mk in model_key.split(\"|\"):\n            if isinstance(checkpoint, dict) and mk in checkpoint:\n                state_dict = checkpoint[mk]\n                break\n            else:\n                state_dict = checkpoint\n        if next(iter(state_dict.items()))[0].startswith(\"module\"):\n            state_dict = {k[7:]: v for k, v in state_dict.items()}\n\n    for k in skip_list:\n        if k in list(state_dict.keys()):\n            logging.info(f\"Removing key {k} from pretrained checkpoint\")\n            del state_dict[k]\n\n    if os.getenv(\"RoPE\") == \"1\":\n        for k in list(state_dict.keys()):\n            if \"freqs_cos\" in k or \"freqs_sin\" in k:\n                del state_dict[k]\n    return state_dict\n\n\ndef load_clip_visual_state_dict(checkpoint_path: str, map_location: str = \"cpu\", is_openai: bool = False, skip_list: list = []):\n    state_dict = load_state_dict(checkpoint_path, map_location=map_location, is_openai=is_openai, skip_list=skip_list)\n    # for k in list(state_dict.keys()):\n    #     if not k.startswith(\"visual.\"):\n    #         del state_dict[k]\n    # for k in list(state_dict.keys()):\n    #     if k.startswith(\"visual.\"):\n    #         new_k = k[7:]\n    #         state_dict[new_k] = state_dict[k]\n    #         del state_dict[k]\n    return state_dict\n\n\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, Union\n\ntry:\n    from apex.normalization import FusedLayerNorm\nexcept:\n    FusedLayerNorm = LayerNorm\n    # print(\"Please build and install Nvidia apex package with option '--cuda_ext' according to https://github.com/NVIDIA/apex#from-source .\")\n\n\n@dataclass\nclass CLIPVisionCfg:\n    layers: Union[Tuple[int, int, int, int], int] = 12\n    width: int = 768\n    head_width: int = 64\n    mlp_ratio: float = 4.0\n    patch_size: int = 16\n    image_size: Union[Tuple[int, int], int] = 224\n    ls_init_value: Optional[float] = None  # layer scale initial value\n    patch_dropout: float = 0.0  # what fraction of patches to dropout during training (0 would mean disabled and no patches dropped) - 0.5 to 0.75 recommended in the paper for optimal results\n    global_average_pool: bool = False  # whether to global average pool the last embedding layer, instead of using CLS token (https://arxiv.org/abs/2205.01580)\n    drop_path_rate: Optional[float] = None  # drop path rate\n    timm_model_name: str = None  # a valid model name overrides layers, width, patch_size\n    timm_model_pretrained: bool = False  # use (imagenet) pretrained weights for named model\n    timm_pool: str = \"avg\"  # feature pooling for timm model ('abs_attn', 'rot_attn', 'avg', '')\n    timm_proj: str = \"linear\"  # linear projection for timm model output ('linear', 'mlp', '')\n    timm_proj_bias: bool = False  # enable bias final projection\n    eva_model_name: str = None  # a valid eva model name overrides layers, width, patch_size\n    qkv_bias: bool = True\n    fusedLN: bool = False\n    xattn: bool = False\n    postnorm: bool = False\n    rope: bool = False\n    pt_hw_seq_len: int = 16  # 224/14\n    intp_freq: bool = False\n    naiveswiglu: bool = False\n    subln: bool = False\n\n\ndef create_norm_layer_factory(use_fused_ln, eps=1e-6):\n    # Otherwise, use the standard LayerNorm\n    return lambda num_features: nn.LayerNorm(num_features, eps=eps)\n\n\ndef _build_vision_tower(vision_tower_path: str, embed_dim: int, vision_cfg: CLIPVisionCfg, **kwargs):\n    if isinstance(vision_cfg, dict):\n        vision_cfg = CLIPVisionCfg(**vision_cfg)\n\n    if vision_cfg.eva_model_name:\n        vision_heads = vision_cfg.width // vision_cfg.head_width\n        # Determine the appropriate norm layer factory based on the configuration\n        norm_layer_factory = create_norm_layer_factory(vision_cfg.fusedLN, eps=1e-6)\n\n        visual = EVAVisionTransformer(\n            img_size=vision_cfg.image_size,\n            patch_size=vision_cfg.patch_size,\n            num_classes=embed_dim,\n            use_mean_pooling=vision_cfg.global_average_pool,  # False\n            init_values=vision_cfg.ls_init_value,\n            patch_dropout=vision_cfg.patch_dropout,\n            embed_dim=vision_cfg.width,\n            depth=vision_cfg.layers,\n            num_heads=vision_heads,\n            mlp_ratio=vision_cfg.mlp_ratio,\n            qkv_bias=vision_cfg.qkv_bias,\n            drop_path_rate=vision_cfg.drop_path_rate,\n            norm_layer=norm_layer_factory,\n            xattn=vision_cfg.xattn,\n            rope=vision_cfg.rope,\n            postnorm=vision_cfg.postnorm,\n            pt_hw_seq_len=vision_cfg.pt_hw_seq_len,  # 224/14\n            intp_freq=vision_cfg.intp_freq,\n            naiveswiglu=vision_cfg.naiveswiglu,\n            subln=vision_cfg.subln,\n        )\n\n        state_dict = load_clip_visual_state_dict(vision_tower_path)\n        incompatible_keys = visual.load_state_dict(state_dict, strict=False)\n        rank0_print(\"EVA-CLIP incompatible_keys:\", incompatible_keys)\n\n    return visual\n\n\nclass EVAEncoderWrapper(nn.Module):\n    def __init__(self, vision_tower_pretrained, config):\n        super(EVAEncoderWrapper, self).__init__()\n        self.config = config\n        self.config[\"vision_tower_path\"] = vision_tower_pretrained\n        self.model = _build_vision_tower(**self.config)\n\n    def forward(self, image, **kwargs):\n        encode = self.model(image, return_all_features=True)[:, 1:, :]  # remove the CLS token\n        return encode\n\n    @property\n    def dtype(self):\n        return list(self.parameters())[-1].dtype\n\n    @property\n    def device(self):\n        return list(self.parameters())[-1].device\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/pretrained.py", "content": "import hashlib\nimport os\nimport urllib\nimport warnings\nfrom typing import Dict, Union\n\nfrom tqdm import tqdm\n\ntry:\n    from huggingface_hub import hf_hub_download\n\n    _has_hf_hub = True\nexcept ImportError:\n    hf_hub_download = None\n    _has_hf_hub = False\n\n\ndef _pcfg(url=\"\", hf_hub=\"\", filename=\"\", mean=None, std=None):\n    return dict(\n        url=url,\n        hf_hub=hf_hub,\n        mean=mean,\n        std=std,\n    )\n\n\n_VITB32 = dict(\n    openai=_pcfg(\"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\"),\n    laion400m_e31=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-quickgelu-laion400m_e31-d867053b.pt\"),\n    laion400m_e32=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-quickgelu-laion400m_e32-46683a32.pt\"),\n    laion2b_e16=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-laion2b_e16-af8dbd0c.pth\"),\n    laion2b_s34b_b79k=_pcfg(hf_hub=\"laion/CLIP-ViT-B-32-laion2B-s34B-b79K/\"),\n)\n\n_VITB32_quickgelu = dict(\n    openai=_pcfg(\"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\"),\n    laion400m_e31=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-quickgelu-laion400m_e31-d867053b.pt\"),\n    laion400m_e32=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-quickgelu-laion400m_e32-46683a32.pt\"),\n)\n\n_VITB16 = dict(\n    openai=_pcfg(\"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\"),\n    laion400m_e31=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_16-laion400m_e31-00efa78f.pt\"),\n    laion400m_e32=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_16-laion400m_e32-55e67d44.pt\"),\n    laion2b_s34b_b88k=_pcfg(hf_hub=\"laion/CLIP-ViT-B-16-laion2B-s34B-b88K/\"),\n)\n\n_EVAB16 = dict(\n    eva=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_B_psz14to16.pt\"),\n    eva02=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_B_psz14to16.pt\"),\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_B_psz16_s8B.pt\"),\n    eva02_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_B_psz16_s8B.pt\"),\n)\n\n_VITB16_PLUS_240 = dict(\n    laion400m_e31=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_16_plus_240-laion400m_e31-8fb26589.pt\"),\n    laion400m_e32=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_16_plus_240-laion400m_e32-699c4b84.pt\"),\n)\n\n_VITL14 = dict(\n    openai=_pcfg(\"https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt\"),\n    laion400m_e31=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_l_14-laion400m_e31-69988bb6.pt\"),\n    laion400m_e32=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_l_14-laion400m_e32-3d133497.pt\"),\n    laion2b_s32b_b82k=_pcfg(hf_hub=\"laion/CLIP-ViT-L-14-laion2B-s32B-b82K/\", mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n)\n\n_EVAL14 = dict(\n    eva=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_L_psz14.pt\"),\n    eva02=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_L_psz14.pt\"),\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_L_psz14_s4B.pt\"),\n    eva02_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_L_psz14_s4B.pt\"),\n)\n\n_VITL14_336 = dict(\n    openai=_pcfg(\"https://openaipublic.azureedge.net/clip/models/3035c92b350959924f9f00213499208652fc7ea050643e8b385c2dac08641f02/ViT-L-14-336px.pt\"),\n)\n\n_EVAL14_336 = dict(\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_L_336_psz14_s6B.pt\"),\n    eva02_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_L_336_psz14_s6B.pt\"),\n    eva_clip_224to336=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_L_psz14_224to336.pt\"),\n    eva02_clip_224to336=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_L_psz14_224to336.pt\"),\n)\n\n_VITH14 = dict(\n    laion2b_s32b_b79k=_pcfg(hf_hub=\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K/\"),\n)\n\n_VITg14 = dict(\n    laion2b_s12b_b42k=_pcfg(hf_hub=\"laion/CLIP-ViT-g-14-laion2B-s12B-b42K/\"),\n    laion2b_s34b_b88k=_pcfg(hf_hub=\"laion/CLIP-ViT-g-14-laion2B-s34B-b88K/\"),\n)\n\n_EVAg14 = dict(\n    eva=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/\"),\n    eva01=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA01_g_psz14.pt\"),\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA01_CLIP_g_14_psz14_s11B.pt\"),\n    eva01_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA01_CLIP_g_14_psz14_s11B.pt\"),\n)\n\n_EVAg14_PLUS = dict(\n    eva=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/\"),\n    eva01=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA01_g_psz14.pt\"),\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA01_CLIP_g_14_plus_psz14_s11B.pt\"),\n    eva01_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA01_CLIP_g_14_plus_psz14_s11B.pt\"),\n)\n\n_VITbigG14 = dict(\n    laion2b_s39b_b160k=_pcfg(hf_hub=\"laion/CLIP-ViT-bigG-14-laion2B-39B-b160k/\"),\n)\n\n_EVAbigE14 = dict(\n    eva=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_E_psz14.pt\"),\n    eva02=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_E_psz14.pt\"),\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_E_psz14_s4B.pt\"),\n    eva02_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_E_psz14_s4B.pt\"),\n)\n\n_EVAbigE14_PLUS = dict(\n    eva=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_E_psz14.pt\"),\n    eva02=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_E_psz14.pt\"),\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_E_psz14_plus_s9B.pt\"),\n    eva02_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_E_psz14_plus_s9B.pt\"),\n)\n\n_EVA_8B = dict(\n    eva=_pcfg(hf_hub=\"BAAI/EVA-CLIP-8B/EVA_8B_psz14.bin\"),\n    eva_clip=_pcfg(hf_hub=\"BAAI/EVA-CLIP-8B/EVA_CLIP_8B_psz14_s9B.pt\"),\n)\n\n_EVA_8B_PLUS = dict(\n    eva_clip=_pcfg(hf_hub=\"BAAI/EVA-CLIP-8B-448/EVA_CLIP_8B_psz14_plus_s0.6B.pt\"),\n)\n\n\n_PRETRAINED = {\n    # \"ViT-B-32\": _VITB32,\n    \"OpenaiCLIP-B-32\": _VITB32,\n    \"OpenCLIP-B-32\": _VITB32,\n    # \"ViT-B-32-quickgelu\": _VITB32_quickgelu,\n    \"OpenaiCLIP-B-32-quickgelu\": _VITB32_quickgelu,\n    \"OpenCLIP-B-32-quickgelu\": _VITB32_quickgelu,\n    # \"ViT-B-16\": _VITB16,\n    \"OpenaiCLIP-B-16\": _VITB16,\n    \"OpenCLIP-B-16\": _VITB16,\n    \"EVA02-B-16\": _EVAB16,\n    \"EVA02-CLIP-B-16\": _EVAB16,\n    # \"ViT-B-16-plus-240\": _VITB16_PLUS_240,\n    \"OpenCLIP-B-16-plus-240\": _VITB16_PLUS_240,\n    # \"ViT-L-14\": _VITL14,\n    \"OpenaiCLIP-L-14\": _VITL14,\n    \"OpenCLIP-L-14\": _VITL14,\n    \"EVA02-L-14\": _EVAL14,\n    \"EVA02-CLIP-L-14\": _EVAL14,\n    # \"ViT-L-14-336\": _VITL14_336,\n    \"OpenaiCLIP-L-14-336\": _VITL14_336,\n    \"EVA02-CLIP-L-14-336\": _EVAL14_336,\n    # \"ViT-H-14\": _VITH14,\n    # \"ViT-g-14\": _VITg14,\n    \"OpenCLIP-H-14\": _VITH14,\n    \"OpenCLIP-g-14\": _VITg14,\n    \"EVA01-CLIP-g-14\": _EVAg14,\n    \"EVA01-CLIP-g-14-plus\": _EVAg14_PLUS,\n    # \"ViT-bigG-14\": _VITbigG14,\n    \"OpenCLIP-bigG-14\": _VITbigG14,\n    \"EVA02-CLIP-bigE-14\": _EVAbigE14,\n    \"EVA02-CLIP-bigE-14-plus\": _EVAbigE14_PLUS,\n    \"EVA-CLIP-8B\": _EVA_8B,\n    \"EVA-CLIP-8B-448\": _EVA_8B_PLUS,\n    \"EVA-CLIP-8B-plus\": _EVA_8B_PLUS,\n}\n\n\ndef _clean_tag(tag: str):\n    # normalize pretrained tags\n    return tag.lower().replace(\"-\", \"_\")\n\n\ndef list_pretrained(as_str: bool = False):\n    \"\"\"returns list of pretrained models\n    Returns a tuple (model_name, pretrain_tag) by default or 'name:tag' if as_str == True\n    \"\"\"\n    return [\":\".join([k, t]) if as_str else (k, t) for k in _PRETRAINED.keys() for t in _PRETRAINED[k].keys()]\n\n\ndef list_pretrained_models_by_tag(tag: str):\n    \"\"\"return all models having the specified pretrain tag\"\"\"\n    models = []\n    tag = _clean_tag(tag)\n    for k in _PRETRAINED.keys():\n        if tag in _PRETRAINED[k]:\n            models.append(k)\n    return models\n\n\ndef list_pretrained_tags_by_model(model: str):\n    \"\"\"return all pretrain tags for the specified model architecture\"\"\"\n    tags = []\n    if model in _PRETRAINED:\n        tags.extend(_PRETRAINED[model].keys())\n    return tags\n\n\ndef is_pretrained_cfg(model: str, tag: str):\n    if model not in _PRETRAINED:\n        return False\n    return _clean_tag(tag) in _PRETRAINED[model]\n\n\ndef get_pretrained_cfg(model: str, tag: str):\n    if model not in _PRETRAINED:\n        return {}\n    model_pretrained = _PRETRAINED[model]\n    return model_pretrained.get(_clean_tag(tag), {})\n\n\ndef get_pretrained_url(model: str, tag: str):\n    cfg = get_pretrained_cfg(model, _clean_tag(tag))\n    return cfg.get(\"url\", \"\")\n\n\ndef download_pretrained_from_url(\n    url: str,\n    cache_dir: Union[str, None] = None,\n):\n    if not cache_dir:\n        cache_dir = os.path.expanduser(\"~/.cache/clip\")\n    os.makedirs(cache_dir, exist_ok=True)\n    filename = os.path.basename(url)\n\n    if \"openaipublic\" in url:\n        expected_sha256 = url.split(\"/\")[-2]\n    elif \"mlfoundations\" in url:\n        expected_sha256 = os.path.splitext(filename)[0].split(\"-\")[-1]\n    else:\n        expected_sha256 = \"\"\n\n    download_target = os.path.join(cache_dir, filename)\n\n    if os.path.exists(download_target) and not os.path.isfile(download_target):\n        raise RuntimeError(f\"{download_target} exists and is not a regular file\")\n\n    if os.path.isfile(download_target):\n        if expected_sha256:\n            if hashlib.sha256(open(download_target, \"rb\").read()).hexdigest().startswith(expected_sha256):\n                return download_target\n            else:\n                warnings.warn(f\"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file\")\n        else:\n            return download_target\n\n    with urllib.request.urlopen(url) as source, open(download_target, \"wb\") as output:\n        with tqdm(total=int(source.headers.get(\"Content-Length\")), ncols=80, unit=\"iB\", unit_scale=True) as loop:\n            while True:\n                buffer = source.read(8192)\n                if not buffer:\n                    break\n\n                output.write(buffer)\n                loop.update(len(buffer))\n\n    if expected_sha256 and not hashlib.sha256(open(download_target, \"rb\").read()).hexdigest().startswith(expected_sha256):\n        raise RuntimeError(f\"Model has been downloaded but the SHA256 checksum does not not match\")\n\n    return download_target\n\n\ndef has_hf_hub(necessary=False):\n    if not _has_hf_hub and necessary:\n        # if no HF Hub module installed, and it is necessary to continue, raise error\n        raise RuntimeError(\"Hugging Face hub model specified but package not installed. Run `pip install huggingface_hub`.\")\n    return _has_hf_hub\n\n\ndef download_pretrained_from_hf(\n    model_id: str,\n    filename: str = \"open_clip_pytorch_model.bin\",\n    revision=None,\n    cache_dir: Union[str, None] = None,\n):\n    has_hf_hub(True)\n    cached_file = hf_hub_download(model_id, filename, revision=revision, cache_dir=cache_dir)\n    return cached_file\n\n\ndef download_pretrained(\n    cfg: Dict,\n    force_hf_hub: bool = False,\n    cache_dir: Union[str, None] = None,\n):\n    target = \"\"\n    if not cfg:\n        return target\n\n    download_url = cfg.get(\"url\", \"\")\n    download_hf_hub = cfg.get(\"hf_hub\", \"\")\n    if download_hf_hub and force_hf_hub:\n        # use HF hub even if url exists\n        download_url = \"\"\n\n    if download_url:\n        target = download_pretrained_from_url(download_url, cache_dir=cache_dir)\n    elif download_hf_hub:\n        has_hf_hub(True)\n        # we assume the hf_hub entries in pretrained config combine model_id + filename in\n        # 'org/model_name/filename.pt' form. To specify just the model id w/o filename and\n        # use 'open_clip_pytorch_model.bin' default, there must be a trailing slash 'org/model_name/'.\n        model_id, filename = os.path.split(download_hf_hub)\n        if filename:\n            target = download_pretrained_from_hf(model_id, filename=filename, cache_dir=cache_dir)\n        else:\n            target = download_pretrained_from_hf(model_id, cache_dir=cache_dir)\n\n    return target\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/eva_clip/factory.py", "content": "import json\nimport logging\nimport os\nimport pathlib\nimport re\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom typing import Optional, Tuple, Union, Dict, Any\nimport torch\n\n_MODEL_CONFIG_PATHS = [Path(__file__).parent / f\"model_configs/\"]\n_MODEL_CONFIGS = {}  # directory (model_name: config) of model architecture configs\n\n\ndef _natural_key(string_):\n    return [int(s) if s.isdigit() else s for s in re.split(r\"(\\d+)\", string_.lower())]\n\n\ndef _rescan_model_configs():\n    global _MODEL_CONFIGS\n\n    config_ext = (\".json\",)\n    config_files = []\n    for config_path in _MODEL_CONFIG_PATHS:\n        if config_path.is_file() and config_path.suffix in config_ext:\n            config_files.append(config_path)\n        elif config_path.is_dir():\n            for ext in config_ext:\n                config_files.extend(config_path.glob(f\"*{ext}\"))\n\n    for cf in config_files:\n        with open(cf, \"r\", encoding=\"utf8\") as f:\n            model_cfg = json.load(f)\n            if all(a in model_cfg for a in (\"embed_dim\", \"vision_cfg\", \"text_cfg\")):\n                _MODEL_CONFIGS[cf.stem] = model_cfg\n\n    _MODEL_CONFIGS = dict(sorted(_MODEL_CONFIGS.items(), key=lambda x: _natural_key(x[0])))\n\n\n_rescan_model_configs()  # initial populate of model config registry\n\n\ndef list_models():\n    \"\"\"enumerate available model architectures based on config files\"\"\"\n    return list(_MODEL_CONFIGS.keys())\n\n\ndef add_model_config(path):\n    \"\"\"add model config path or file and update registry\"\"\"\n    if not isinstance(path, Path):\n        path = Path(path)\n    _MODEL_CONFIG_PATHS.append(path)\n    _rescan_model_configs()\n\n\ndef get_model_config(model_name):\n    if model_name in _MODEL_CONFIGS:\n        return deepcopy(_MODEL_CONFIGS[model_name])\n    else:\n        return None\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/utils.py", "content": "from itertools import repeat\nimport collections.abc\nimport logging\nimport math\nimport numpy as np\n\nimport torch\nfrom torch import nn as nn\nfrom torchvision.ops.misc import FrozenBatchNorm2d\nimport torch.nn.functional as F\n\n\n# open CLIP\ndef resize_clip_pos_embed(state_dict, model, interpolation: str = \"bicubic\", seq_dim=1):\n    # Rescale the grid of position embeddings when loading from state_dict\n    old_pos_embed = state_dict.get(\"visual.positional_embedding\", None)\n    if old_pos_embed is None or not hasattr(model.visual, \"grid_size\"):\n        return\n    grid_size = to_2tuple(model.visual.grid_size)\n    extra_tokens = 1  # FIXME detect different token configs (ie no class token, or more)\n    new_seq_len = grid_size[0] * grid_size[1] + extra_tokens\n    if new_seq_len == old_pos_embed.shape[0]:\n        return\n\n    if extra_tokens:\n        pos_emb_tok, pos_emb_img = old_pos_embed[:extra_tokens], old_pos_embed[extra_tokens:]\n    else:\n        pos_emb_tok, pos_emb_img = None, old_pos_embed\n    old_grid_size = to_2tuple(int(math.sqrt(len(pos_emb_img))))\n\n    logging.info(\"Resizing position embedding grid-size from %s to %s\", old_grid_size, grid_size)\n    pos_emb_img = pos_emb_img.reshape(1, old_grid_size[0], old_grid_size[1], -1).permute(0, 3, 1, 2)\n    pos_emb_img = F.interpolate(\n        pos_emb_img,\n        size=grid_size,\n        mode=interpolation,\n        align_corners=True,\n    )\n    pos_emb_img = pos_emb_img.permute(0, 2, 3, 1).reshape(1, grid_size[0] * grid_size[1], -1)[0]\n    if pos_emb_tok is not None:\n        new_pos_embed = torch.cat([pos_emb_tok, pos_emb_img], dim=0)\n    else:\n        new_pos_embed = pos_emb_img\n    state_dict[\"visual.positional_embedding\"] = new_pos_embed\n\n\ndef resize_visual_pos_embed(state_dict, model, interpolation: str = \"bicubic\", seq_dim=1):\n    # Rescale the grid of position embeddings when loading from state_dict\n    old_pos_embed = state_dict.get(\"positional_embedding\", None)\n    if old_pos_embed is None or not hasattr(model.visual, \"grid_size\"):\n        return\n    grid_size = to_2tuple(model.visual.grid_size)\n    extra_tokens = 1  # FIXME detect different token configs (ie no class token, or more)\n    new_seq_len = grid_size[0] * grid_size[1] + extra_tokens\n    if new_seq_len == old_pos_embed.shape[0]:\n        return\n\n    if extra_tokens:\n        pos_emb_tok, pos_emb_img = old_pos_embed[:extra_tokens], old_pos_embed[extra_tokens:]\n    else:\n        pos_emb_tok, pos_emb_img = None, old_pos_embed\n    old_grid_size = to_2tuple(int(math.sqrt(len(pos_emb_img))))\n\n    logging.info(\"Resizing position embedding grid-size from %s to %s\", old_grid_size, grid_size)\n    pos_emb_img = pos_emb_img.reshape(1, old_grid_size[0], old_grid_size[1], -1).permute(0, 3, 1, 2)\n    pos_emb_img = F.interpolate(\n        pos_emb_img,\n        size=grid_size,\n        mode=interpolation,\n        align_corners=True,\n    )\n    pos_emb_img = pos_emb_img.permute(0, 2, 3, 1).reshape(1, grid_size[0] * grid_size[1], -1)[0]\n    if pos_emb_tok is not None:\n        new_pos_embed = torch.cat([pos_emb_tok, pos_emb_img], dim=0)\n    else:\n        new_pos_embed = pos_emb_img\n    state_dict[\"positional_embedding\"] = new_pos_embed\n\n\ndef resize_evaclip_pos_embed(state_dict, model, interpolation: str = \"bicubic\", seq_dim=1):\n    all_keys = list(state_dict.keys())\n    # interpolate position embedding\n    if \"visual.pos_embed\" in state_dict:\n        pos_embed_checkpoint = state_dict[\"visual.pos_embed\"]\n        embedding_size = pos_embed_checkpoint.shape[-1]\n        num_patches = model.visual.patch_embed.num_patches\n        # num_extra_tokens = model.visual.pos_embed.shape[-2] - num_patches\n        num_extra_tokens = 1  # FIXME detect different token configs (ie no class token, or more)\n        # height (== width) for the checkpoint position embedding\n        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n        # height (== width) for the new position embedding\n        new_size = int(num_patches**0.5)\n        # class_token and dist_token are kept unchanged\n        if orig_size != new_size:\n            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n            # only the position tokens are interpolated\n            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n            pos_tokens = torch.nn.functional.interpolate(pos_tokens, size=(new_size, new_size), mode=\"bicubic\", align_corners=False)\n            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n            state_dict[\"visual.pos_embed\"] = new_pos_embed\n\n            patch_embed_proj = state_dict[\"visual.patch_embed.proj.weight\"]\n            patch_size = model.visual.patch_embed.patch_size\n            state_dict[\"visual.patch_embed.proj.weight\"] = torch.nn.functional.interpolate(patch_embed_proj.float(), size=patch_size, mode=\"bicubic\", align_corners=False)\n\n\ndef resize_eva_pos_embed(state_dict, model, interpolation: str = \"bicubic\", seq_dim=1):\n    all_keys = list(state_dict.keys())\n    # interpolate position embedding\n    if \"pos_embed\" in state_dict:\n        pos_embed_checkpoint = state_dict[\"pos_embed\"]\n        embedding_size = pos_embed_checkpoint.shape[-1]\n        num_patches = model.visual.patch_embed.num_patches\n        # num_extra_tokens = model.visual.pos_embed.shape[-2] - num_patches\n        num_extra_tokens = 1  # FIXME detect different token configs (ie no class token, or more)\n        # height (== width) for the checkpoint position embedding\n        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n        # height (== width) for the new position embedding\n        new_size = int(num_patches**0.5)\n        # class_token and dist_token are kept unchanged\n        if orig_size != new_size:\n            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n            # only the position tokens are interpolated\n            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n            pos_tokens = torch.nn.functional.interpolate(pos_tokens, size=(new_size, new_size), mode=\"bicubic\", align_corners=False)\n            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n            state_dict[\"pos_embed\"] = new_pos_embed\n\n            patch_embed_proj = state_dict[\"patch_embed.proj.weight\"]\n            patch_size = model.visual.patch_embed.patch_size\n            state_dict[\"patch_embed.proj.weight\"] = torch.nn.functional.interpolate(patch_embed_proj.float(), size=patch_size, mode=\"bicubic\", align_corners=False)\n\n\ndef resize_rel_pos_embed(state_dict, model, interpolation: str = \"bicubic\", seq_dim=1):\n    all_keys = list(state_dict.keys())\n    for key in all_keys:\n        if \"relative_position_index\" in key:\n            state_dict.pop(key)\n\n        if \"relative_position_bias_table\" in key:\n            rel_pos_bias = state_dict[key]\n            src_num_pos, num_attn_heads = rel_pos_bias.size()\n            dst_num_pos, _ = model.visual.state_dict()[key].size()\n            dst_patch_shape = model.visual.patch_embed.patch_shape\n            if dst_patch_shape[0] != dst_patch_shape[1]:\n                raise NotImplementedError()\n            num_extra_tokens = dst_num_pos - (dst_patch_shape[0] * 2 - 1) * (dst_patch_shape[1] * 2 - 1)\n            src_size = int((src_num_pos - num_extra_tokens) ** 0.5)\n            dst_size = int((dst_num_pos - num_extra_tokens) ** 0.5)\n            if src_size != dst_size:\n                print(\"Position interpolate for %s from %dx%d to %dx%d\" % (key, src_size, src_size, dst_size, dst_size))\n                extra_tokens = rel_pos_bias[-num_extra_tokens:, :]\n                rel_pos_bias = rel_pos_bias[:-num_extra_tokens, :]\n\n                def geometric_progression(a, r, n):\n                    return a * (1.0 - r**n) / (1.0 - r)\n\n                left, right = 1.01, 1.5\n                while right - left > 1e-6:\n                    q = (left + right) / 2.0\n                    gp = geometric_progression(1, q, src_size // 2)\n                    if gp > dst_size // 2:\n                        right = q\n                    else:\n                        left = q\n\n                # if q > 1.090307:\n                #     q = 1.090307\n\n                dis = []\n                cur = 1\n                for i in range(src_size // 2):\n                    dis.append(cur)\n                    cur += q ** (i + 1)\n\n                r_ids = [-_ for _ in reversed(dis)]\n\n                x = r_ids + [0] + dis\n                y = r_ids + [0] + dis\n\n                t = dst_size // 2.0\n                dx = np.arange(-t, t + 0.1, 1.0)\n                dy = np.arange(-t, t + 0.1, 1.0)\n\n                print(\"Original positions = %s\" % str(x))\n                print(\"Target positions = %s\" % str(dx))\n\n                all_rel_pos_bias = []\n\n                for i in range(num_attn_heads):\n                    z = rel_pos_bias[:, i].view(src_size, src_size).float().numpy()\n                    f = F.interpolate.interp2d(x, y, z, kind=\"cubic\")\n                    all_rel_pos_bias.append(torch.Tensor(f(dx, dy)).contiguous().view(-1, 1).to(rel_pos_bias.device))\n\n                rel_pos_bias = torch.cat(all_rel_pos_bias, dim=-1)\n\n                new_rel_pos_bias = torch.cat((rel_pos_bias, extra_tokens), dim=0)\n                state_dict[key] = new_rel_pos_bias\n\n    # interpolate position embedding\n    if \"pos_embed\" in state_dict:\n        pos_embed_checkpoint = state_dict[\"pos_embed\"]\n        embedding_size = pos_embed_checkpoint.shape[-1]\n        num_patches = model.visual.patch_embed.num_patches\n        num_extra_tokens = model.visual.pos_embed.shape[-2] - num_patches\n        # height (== width) for the checkpoint position embedding\n        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n        # height (== width) for the new position embedding\n        new_size = int(num_patches**0.5)\n        # class_token and dist_token are kept unchanged\n        if orig_size != new_size:\n            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n            # only the position tokens are interpolated\n            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n            pos_tokens = torch.nn.functional.interpolate(pos_tokens, size=(new_size, new_size), mode=\"bicubic\", align_corners=False)\n            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n            state_dict[\"pos_embed\"] = new_pos_embed\n\n            patch_embed_proj = state_dict[\"patch_embed.proj.weight\"]\n            patch_size = model.visual.patch_embed.patch_size\n            state_dict[\"patch_embed.proj.weight\"] = torch.nn.functional.interpolate(patch_embed_proj.float(), size=patch_size, mode=\"bicubic\", align_corners=False)\n\n\ndef freeze_batch_norm_2d(module, module_match={}, name=\"\"):\n    \"\"\"\n    Converts all `BatchNorm2d` and `SyncBatchNorm` layers of provided module into `FrozenBatchNorm2d`. If `module` is\n    itself an instance of either `BatchNorm2d` or `SyncBatchNorm`, it is converted into `FrozenBatchNorm2d` and\n    returned. Otherwise, the module is walked recursively and submodules are converted in place.\n\n    Args:\n        module (torch.nn.Module): Any PyTorch module.\n        module_match (dict): Dictionary of full module names to freeze (all if empty)\n        name (str): Full module name (prefix)\n\n    Returns:\n        torch.nn.Module: Resulting module\n\n    Inspired by https://github.com/pytorch/pytorch/blob/a5895f85be0f10212791145bfedc0261d364f103/torch/nn/modules/batchnorm.py#L762\n    \"\"\"\n    res = module\n    is_match = True\n    if module_match:\n        is_match = name in module_match\n    if is_match and isinstance(module, (nn.modules.batchnorm.BatchNorm2d, nn.modules.batchnorm.SyncBatchNorm)):\n        res = FrozenBatchNorm2d(module.num_features)\n        res.num_features = module.num_features\n        res.affine = module.affine\n        if module.affine:\n            res.weight.data = module.weight.data.clone().detach()\n            res.bias.data = module.bias.data.clone().detach()\n        res.running_mean.data = module.running_mean.data\n        res.running_var.data = module.running_var.data\n        res.eps = module.eps\n    else:\n        for child_name, child in module.named_children():\n            full_child_name = \".\".join([name, child_name]) if name else child_name\n            new_child = freeze_batch_norm_2d(child, module_match, full_child_name)\n            if new_child is not child:\n                res.add_module(child_name, new_child)\n    return res\n\n\n# From PyTorch internals\ndef _ntuple(n):\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return x\n        return tuple(repeat(x, n))\n\n    return parse\n\n\nto_1tuple = _ntuple(1)\nto_2tuple = _ntuple(2)\nto_3tuple = _ntuple(3)\nto_4tuple = _ntuple(4)\nto_ntuple = lambda n, x: _ntuple(n)(x)\n\n\ndef is_logging(args):\n    def is_global_master(args):\n        return args.rank == 0\n\n    def is_local_master(args):\n        return args.local_rank == 0\n\n    def is_master(args, local=False):\n        return is_local_master(args) if local else is_global_master(args)\n\n    return is_master\n\n\nclass AllGather(torch.autograd.Function):\n    \"\"\"An autograd function that performs allgather on a tensor.\n    Performs all_gather operation on the provided tensors.\n    *** Warning ***: torch.distributed.all_gather has no gradient.\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, tensor, rank, world_size):\n        tensors_gather = [torch.empty_like(tensor) for _ in range(world_size)]\n        torch.distributed.all_gather(tensors_gather, tensor)\n        ctx.rank = rank\n        ctx.batch_size = tensor.shape[0]\n        return torch.cat(tensors_gather, 0)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return (grad_output[ctx.batch_size * ctx.rank : ctx.batch_size * (ctx.rank + 1)], None, None)\n\n\nallgather = AllGather.apply\n"}
