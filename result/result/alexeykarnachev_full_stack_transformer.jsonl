{"repo_info": {"repo_name": "full_stack_transformer", "repo_owner": "alexeykarnachev", "repo_url": "https://github.com/alexeykarnachev/full_stack_transformer"}}
{"type": "source_file", "path": "examples/language_generation.py", "content": "import pathlib\n\nimport torch\n\nfrom full_stack_transformer import (\n    LanguageGenerator,\n    LanguageGeneratorParams,\n    Document,\n    load_language_model_from_checkpoint,\n    load_tokenizer_from_checkpoint\n)\n\nif __name__ == '__main__':\n    device = 'cuda:0'\n    experiment_dir = pathlib.Path('../static/experiments/nietzsche_v0/')\n    ckpt_path = experiment_dir / 'models' / 'epoch=9.ckpt'\n\n    generator_params = LanguageGeneratorParams(\n        max_number_of_generated_tokens=64,\n        num_return_sequences=8,\n        repetition_penalty=3.0,\n        temperature=0.5,\n        top_k=50,\n        top_p=1.0\n    )\n\n    ckpt = torch.load(f=str(ckpt_path), map_location='cpu')\n\n    model = load_language_model_from_checkpoint(\n        ckpt=ckpt, device=device, unlikelihood_alpha=None\n    )\n\n    tokenizer = load_tokenizer_from_checkpoint(\n        ckpt=ckpt, max_body_len=64, max_meta_len=0\n    )\n\n    generator = LanguageGenerator(\n        model=model, eos_token_id=tokenizer.eos_token_id\n    )\n\n    document = Document(body='The best filosopher of the 19th century is')\n\n    inp_encoding = tokenizer.encode_document(\n        document=document, with_eos=False\n    )\n\n    out_encodings = generator(inp_encoding, params=generator_params)\n\n    for enc in out_encodings:\n        text = tokenizer.decode_encoding(enc)\n        print(text + '\\n\\n')\n"}
{"type": "source_file", "path": "full_stack_transformer/core/__init__.py", "content": ""}
{"type": "source_file", "path": "full_stack_transformer/core/constants.py", "content": "LOSS_IGNORE = -100"}
{"type": "source_file", "path": "full_stack_transformer/core/data/text_lines_parsers.py", "content": "import abc\nfrom typing import Optional\n\nfrom full_stack_transformer.core.text_input import TextInput\n\n\nclass TextLinesParser:\n    @abc.abstractmethod\n    def parse(self, text: str) -> Optional[TextInput]:\n        pass\n"}
{"type": "source_file", "path": "full_stack_transformer/core/model_output.py", "content": "from dataclasses import dataclass\nfrom typing import Optional\n\nimport torch\n\n\n@dataclass\nclass ModelOutput:\n    \"\"\"Base data container for model output tensors.\"\"\"\n    logits: torch.Tensor\n    loss: Optional[torch.Tensor]\n\n\n@dataclass\nclass LanguageModelOutput(ModelOutput):\n    \"\"\"Data container for language model output tensors.\"\"\"\n    hidden: torch.Tensor\n    past: torch.Tensor\n    lm_loss: Optional[torch.Tensor] = None\n    ul_loss: Optional[torch.Tensor] = None\n"}
{"type": "source_file", "path": "full_stack_transformer/core/modelling/__init__.py", "content": ""}
{"type": "source_file", "path": "full_stack_transformer/core/tokenizer.py", "content": "import abc\nimport re\nfrom typing import List\n\nfrom tokenizers.implementations import BaseTokenizer\nfrom transformers.tokenization_utils import PreTrainedTokenizerFast\n\nfrom full_stack_transformer.core.encoding import Encoding\nfrom full_stack_transformer.core.text_input import TextInput\nfrom full_stack_transformer.utilities.factory import get_object\n\n_END = '[END]'\n\n_SPECIAL_TOKENS = [_END]\n\n\nclass Tokenizer(PreTrainedTokenizerFast):\n    @property\n    def eos_token_id(self):\n        return self.convert_tokens_to_ids(self.eos_token)\n\n    @property\n    def eos_token(self):\n        return _END\n\n    @property\n    def vocab_size(self):\n        return max(self.all_special_ids) + 1\n\n    def __init__(self, tokenizer: BaseTokenizer, **kwargs):\n        super().__init__(tokenizer, **kwargs)\n\n        self.add_special_tokens({'additional_special_tokens': _SPECIAL_TOKENS})\n\n    def encode_for_train(self, text_input: TextInput) -> List[Encoding]:\n        text_input = self._preprocess_input(text_input=text_input)\n        encodings = self._encode_for_train(text_input=text_input)\n\n        return encodings\n\n    def encode_for_inference(self, text_input: TextInput) -> List[Encoding]:\n        text_input = self._preprocess_input(text_input=text_input)\n        encodings = self._encode_for_inference(text_input=text_input)\n\n        return encodings\n\n    @abc.abstractmethod\n    def _encode_for_train(self, text_input: TextInput) -> List[Encoding]:\n        pass\n\n    @abc.abstractmethod\n    def _encode_for_inference(self, text_input: TextInput) -> List[Encoding]:\n        pass\n\n    @abc.abstractmethod\n    def _preprocess_input(self, text_input: TextInput) -> TextInput:\n        pass\n\n    @abc.abstractmethod\n    def _postprocess_text(self, text: str) -> str:\n        pass\n\n    def decode_encoding(self, encoding: Encoding) -> str:\n        token_ids = encoding.token_ids\n        text = self.decode(token_ids=token_ids, skip_special_tokens=True)\n\n        for token in _SPECIAL_TOKENS:\n            text = re.sub(re.escape(token), '', text)\n\n        text = self._postprocess_text(text)\n\n        text = text.strip()\n\n        return text\n"}
{"type": "source_file", "path": "full_stack_transformer/tasks/document_lm/data/encodings_collate.py", "content": "from typing import Sequence, Union, Optional\n\nimport torch\n\nfrom full_stack_transformer.core.constants import LOSS_IGNORE\nfrom full_stack_transformer.core.encoding import Encoding\nfrom full_stack_transformer.core.model_input import ModelInput\nfrom full_stack_transformer.utilities.sequences import pad_sequences_from_right\n\n\nclass DocumentEncodingsCollate:\n    def __init__(self, pad_value: int):\n        self._pad_value = pad_value\n\n    def __call__(\n            self,\n            encodings: Sequence[Encoding],\n            device: Optional[Union[torch.device, str]] = None\n    ) -> ModelInput:\n        model_input = _collate_encodings(\n            encodings=encodings,\n            pad_value=self._pad_value,\n            device=device\n        )\n\n        return model_input\n\n\ndef _collate_encodings(\n        encodings: Sequence[Encoding],\n        pad_value: int,\n        device: Optional[Union[torch.device, str]] = None\n) -> ModelInput:\n    token_ids = pad_sequences_from_right(\n        sequences=[e.token_ids for e in encodings],\n        pad_value=pad_value,\n        max_len=None,\n    )\n    lm_labels = pad_sequences_from_right(\n        sequences=[e.lm_labels for e in encodings],\n        pad_value=LOSS_IGNORE,\n        max_len=None\n    )\n\n    input_ids = torch.tensor(token_ids, dtype=torch.long, device=device)\n    lm_labels = torch.tensor(lm_labels, dtype=torch.long, device=device)\n    model_input = ModelInput(input_ids=input_ids, lm_labels=lm_labels)\n\n    return model_input\n"}
{"type": "source_file", "path": "full_stack_transformer/core/data/dataset.py", "content": "import abc\nimport pathlib\nfrom multiprocessing import Queue\nfrom typing import Optional, Callable\n\nimport torch.utils.data\n\nfrom full_stack_transformer.core.data.encodings_collate import EncodingsCollate\nfrom full_stack_transformer.core.data.encodings_producer import EncodingsProducer\nfrom full_stack_transformer.core.data.encodings_sampler import EncodingsSampler\nfrom full_stack_transformer.core.data.text_inputs_producer import TextInputsProducer\nfrom full_stack_transformer.core.data.text_lines_parsers import TextLinesParser\nfrom full_stack_transformer.core.tokenizer import Tokenizer\nfrom full_stack_transformer.utilities.queue_iterable_dataset import \\\n    QueueIterableDataset\n\n_QUEUE_MAX_SIZE = 10\n\n\nclass Dataset(QueueIterableDataset):\n    def __init__(\n            self,\n            file_path: pathlib.Path,\n            tokenizer: Tokenizer,\n            text_lines_parser: TextLinesParser,\n            encodings_collate: Callable,\n            n_producer_workers: int = 4,\n            chunk_size: Optional[int] = 10000\n    ):\n        self._file_path = file_path\n        self._tokenizer = tokenizer\n        self._n_producer_workers = n_producer_workers\n        self._chunk_size = chunk_size\n        self._text_lines_parser = text_lines_parser\n        self._collate = encodings_collate\n\n        sorted_samples_queue = self._initialize()\n\n        super().__init__(inp_queue=sorted_samples_queue,\n)\n\n    def _initialize(self):\n        text_inputs_queue = Queue(maxsize=_QUEUE_MAX_SIZE)\n        encodings_queue = Queue(maxsize=_QUEUE_MAX_SIZE)\n        sorted_encodings_queue = Queue(maxsize=_QUEUE_MAX_SIZE)\n\n        text_inputs_producer = TextInputsProducer(\n            file_path=self._file_path,\n            out_text_inputs_queue=text_inputs_queue,\n            out_chunk_size=self._chunk_size,\n            text_lines_parser=self._text_lines_parser\n        )\n\n        model_inputs_producers = []\n\n        for _ in range(self._n_producer_workers):\n            samples_producer = EncodingsProducer(\n                inp_text_inputs_queue=text_inputs_queue,\n                out_encodings_queue=encodings_queue,\n                tokenizer=self._tokenizer\n            )\n            model_inputs_producers.append(samples_producer)\n\n        encodings_sampler = EncodingsSampler(\n            inp_encodings_queue=encodings_queue,\n            out_encodings_queue=sorted_encodings_queue\n        )\n\n        text_inputs_producer.start()\n        [p.start() for p in model_inputs_producers]\n        encodings_sampler.start()\n\n        return sorted_encodings_queue\n\n    @abc.abstractmethod\n    def __len__(self) -> int:\n        pass\n\n    def get_data_loader(\n            self,\n            batch_size: int,\n            num_workers: int\n    ) -> 'DataLoader':\n        dl = DataLoader(\n            dataset=self,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            encodings_collate=self._collate\n        )\n        return dl\n\n\nclass DataLoader(torch.utils.data.DataLoader):\n    def __init__(\n            self,\n            dataset: Dataset,\n            batch_size: int,\n            num_workers: int,\n            encodings_collate: EncodingsCollate\n    ):\n        self._dataset = dataset\n        self._batch_size = batch_size\n        self._collate = encodings_collate\n\n        super().__init__(\n            dataset=self._dataset,\n            batch_size=self._batch_size,\n            num_workers=num_workers,\n            collate_fn=self._collate\n        )\n\n    def __len__(self):\n        return (len(self._dataset) // self._batch_size) - 1\n"}
{"type": "source_file", "path": "full_stack_transformer/tasks/__init__.py", "content": ""}
{"type": "source_file", "path": "full_stack_transformer/core/model_input.py", "content": "from dataclasses import dataclass\nfrom typing import Optional\n\nimport torch\n\n\nclass ModelInputError(Exception):\n    pass\n\n\n@dataclass\nclass ModelInput(dict):\n    \"\"\"Base data container which represents model input.\"\"\"\n    input_ids: torch.Tensor\n    token_type_ids: Optional[torch.Tensor] = None\n    lm_labels: Optional[torch.Tensor] = None\n    past: Optional[torch.Tensor] = None\n\n    def __post_init__(self):\n        super().__init__(**self.__dict__)\n\n        self._check_inputs_validity()\n\n    def _check_inputs_validity(self):\n        if self.token_type_ids is not None and \\\n                self.input_ids.size() != self.token_type_ids.size():\n            raise ModelInputError(\n                '`input_ids` and `token_type_ids` shapes must be equal.'\n            )\n        elif self.lm_labels is not None and \\\n                self.lm_labels.size() != self.input_ids.size():\n            raise ModelInputError(\n                '`lm_labels` were passed, bu there shape does not match '\n                '`input_ids` shape.'\n            )\n        elif self.past is not None and self.input_ids.size()[1] != 1:\n            raise ModelInputError(\n                'If `past` passed, `input_ids` must contain only one token, '\n                'i.e sequence length must be equal to 1.'\n            )\n\n    def cuda(self, gpu_id):\n        \"\"\"Sends object field to cuda.\"\"\"\n        fields = self.__dict__\n\n        for name, field in fields.items():\n            if isinstance(field, torch.Tensor):\n                self.__dict__[name] = field.cuda(gpu_id)\n\n        return self\n"}
{"type": "source_file", "path": "full_stack_transformer/core/data/text_inputs_producer.py", "content": "import pathlib\nfrom multiprocessing import Process, Queue\n\nfrom full_stack_transformer.core.data.text_lines_parsers import TextLinesParser\n\n\nclass TextInputsProducer(Process):\n    def __init__(\n            self,\n            file_path: pathlib.Path,\n            out_text_inputs_queue: Queue,\n            out_chunk_size: int,\n            text_lines_parser: TextLinesParser\n    ):\n        super().__init__()\n\n        self._file_path = file_path\n        self._chunk_size = out_chunk_size\n        self._out_queue = out_text_inputs_queue\n        self._parser = text_lines_parser\n\n    def run(self) -> None:\n        while True:\n            chunk = []\n            with self._file_path.open() as file:\n                for line in file:\n\n                    text_input = self._parser.parse(line)\n                    if text_input is not None:\n                        chunk.append(text_input)\n\n                        if len(chunk) >= self._chunk_size:\n                            self._out_queue.put(chunk)\n                            chunk = []\n\n            if len(chunk) > 0:\n                self._out_queue.put(chunk)\n"}
{"type": "source_file", "path": "full_stack_transformer/core/modelling/loading.py", "content": "import copy\nimport pathlib\nfrom typing import Optional, Union\n\nimport transformers\n\n\nclass ModelLoadingError(Exception):\n    pass\n\n\ndef load_transformer_model_from_path(\n        model_path: Union[str, pathlib.Path],\n        vocab_size: Optional[int]\n) -> transformers.PreTrainedModel:\n    config = transformers.AutoConfig.from_pretrained(model_path)\n    modified_config = _modify_transformers_config(config)\n\n    model = transformers.AutoModelForPreTraining.from_pretrained(\n        pretrained_model_name_or_path=model_path,\n        config=modified_config\n    )\n\n    _resize_embeddings_if_needed(model, vocab_size)\n\n    return model\n\n\ndef initialize_transformer_model_from_config(\n        config: transformers.PretrainedConfig,\n        vocab_size: Optional[int]\n) -> transformers.PreTrainedModel:\n    modified_config = _modify_transformers_config(config)\n    model = transformers.AutoModelForPreTraining.from_config(modified_config)\n\n    _resize_embeddings_if_needed(model, vocab_size)\n\n    return model\n\n\ndef _modify_transformers_config(\n        config: transformers.PretrainedConfig\n) -> transformers.PretrainedConfig:\n    config_copy = copy.deepcopy(config)\n    config_copy.output_past = True\n    config_copy.output_hidden_states = True\n    return config_copy\n\n\ndef _resize_embeddings_if_needed(\n        model: transformers.PreTrainedModel,\n        vocab_size: int\n) -> None:\n    if vocab_size is not None:\n        mean_emb = model.base_model.wte.weight.data.mean(0)\n        old_size = model.base_model.wte.weight.data.size()[0]\n        n_new = vocab_size - old_size\n\n        if n_new < 0:\n            raise ModelLoadingError(\n                \"Can't resize embeddings: new vocab size can not be less than \"\n                \"the old embeddings number (old vocab size).\"\n            )\n\n        model.resize_token_embeddings(vocab_size)\n        idx = vocab_size - n_new\n        model.base_model.wte.weight.data[idx:] = mean_emb.unsqueeze(0)\n"}
{"type": "source_file", "path": "full_stack_transformer/tasks/document_lm/__init__.py", "content": "from full_stack_transformer.tasks.document_lm.hf_gpt2_tokenizer import \\\n    HFGPT2DocumentTokenizer\nfrom full_stack_transformer.tasks.document_lm.language_generator.generator import (\n    LanguageGeneratorParams,\n    LanguageGenerator\n)\nfrom full_stack_transformer.tasks.document_lm.modelling.loading import (\n    load_model_from_checkpoint,\n    load_tokenizer_from_checkpoint\n)\nfrom full_stack_transformer.tasks.document_lm.ru_transformers_tokenizer import \\\n    RuTransformersDocumentTokenizer\nfrom full_stack_transformer.tasks.document_lm.text_input import DocumentInput\n"}
{"type": "source_file", "path": "full_stack_transformer/core/modelling/model.py", "content": "import abc\n\nimport torch\nimport torch.nn as nn\n\nfrom full_stack_transformer.core.model_input import ModelInput\nfrom full_stack_transformer.core.model_output import ModelOutput\n\n\nclass Model(nn.Module):\n\n    @property\n    def device(self):\n        return self.parameters().__next__().device\n\n    @abc.abstractmethod\n    def forward(self, inp: ModelInput) -> ModelOutput:\n        pass\n\n    @torch.no_grad()\n    @abc.abstractmethod\n    def infer(self, inp: ModelInput) -> ModelOutput:\n        pass\n"}
{"type": "source_file", "path": "full_stack_transformer/core/data/encodings_producer.py", "content": "from multiprocessing import Process, Queue\n\nfrom full_stack_transformer.core.tokenizer import Tokenizer\n\n\nclass EncodingsProducer(Process):\n    def __init__(\n            self,\n            tokenizer: Tokenizer,\n            inp_text_inputs_queue: Queue,\n            out_encodings_queue: Queue\n    ):\n        super().__init__()\n\n        self._tokenizer = tokenizer\n        self._inp_queue = inp_text_inputs_queue\n        self._out_queue = out_encodings_queue\n\n    def run(self) -> None:\n        while True:\n            input_chunk = self._inp_queue.get()\n            encodings = []\n            for text_input in input_chunk:\n                enc = self._tokenizer.encode_for_train(text_input)\n                encodings.extend(enc)\n\n            self._out_queue.put(encodings)\n"}
{"type": "source_file", "path": "full_stack_transformer/core/task_runner.py", "content": "import abc\nimport argparse\nimport copy\nfrom typing import Dict, Type, List\n\nfrom pytorch_lightning import Trainer, Callback\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.loggers import TensorBoardLogger\n\nfrom full_stack_transformer.core.modelling.lightning import PLModule\nfrom full_stack_transformer.utilities.experiment import Workspace\n\n\nclass TaskRunner(abc.ABC):\n    def __init__(self, pl_module_cls: Type[PLModule]):\n        self.pl_module_cls = pl_module_cls\n        args = self._parse_args()\n\n        self.workspace = Workspace(\n            experiments_root=args['experiments_root'],\n            experiment_name=args['experiment_name']\n        )\n\n    def _parse_args(self) -> Dict:\n        parser = argparse.ArgumentParser()\n        parser = Trainer.add_argparse_args(parser)\n        parser = Workspace.add_argparse_args(parser)\n        parser = self.pl_module_cls.add_argparse_args(parser)\n\n        args = parser.parse_args()\n\n        return args.__dict__\n\n    def run(self):\n        args = self._parse_args()\n\n        module = self.pl_module_cls(**args)\n\n        description = {\n            'Arguments': args,\n            'Module': module.get_description()\n        }\n        self.workspace.save_description(description)\n\n        trainer = self._prepare_trainer(args=args)\n\n        trainer.fit(model=module)\n\n    def _prepare_trainer(self, args):\n        trainer_args = copy.deepcopy(args)\n\n        _fix_trainer_args(args=trainer_args)\n\n        trainer_args.update(\n            {\n                'logger': TensorBoardLogger(\n                    save_dir=self.workspace.logs_dir,\n                    name=self.workspace.name,\n                    version=self.workspace.version\n                ),\n                'checkpoint_callback': ModelCheckpoint(\n                    filepath=self.workspace.models_dir,\n                    verbose=True,\n                    save_top_k=2,\n                    period=0\n                )\n            }\n        )\n\n        trainer_args['callbacks'] = self._get_trainer_callbacks()\n\n        trainer = Trainer(**trainer_args)\n\n        return trainer\n\n    @abc.abstractmethod\n    def _get_trainer_callbacks(self) -> List[Callback]:\n        pass\n\n\ndef _fix_trainer_args(args: Dict) -> None:\n    val_check_interval = args['val_check_interval']\n    if val_check_interval <= 1:\n        val_check_interval = float(val_check_interval)\n    else:\n        val_check_interval = int(val_check_interval)\n\n    args['val_check_interval'] = val_check_interval\n"}
{"type": "source_file", "path": "full_stack_transformer/core/encoding.py", "content": "from dataclasses import dataclass\nfrom typing import List\n\n\n@dataclass\nclass Encoding:\n    \"\"\"Base data container which represents encoded text.\"\"\"\n    token_ids: List[int]\n    lm_labels: List[int]"}
{"type": "source_file", "path": "full_stack_transformer/tasks/document_lm/data/__init__.py", "content": ""}
{"type": "source_file", "path": "full_stack_transformer/core/data/encodings_sampler.py", "content": "from multiprocessing import Process, Queue\n\n\nclass EncodingsSampler(Process):\n    def __init__(self, inp_encodings_queue: Queue, out_encodings_queue: Queue):\n        super().__init__()\n\n        self._inp_queue = inp_encodings_queue\n        self._out_queue = out_encodings_queue\n\n    def run(self) -> None:\n        while True:\n            encodings = self._inp_queue.get()\n            encodings = sorted(encodings, key=lambda e: -len(e.token_ids))\n            for enc in encodings:\n                self._out_queue.put(enc)\n"}
{"type": "source_file", "path": "full_stack_transformer/__init__.py", "content": "import sys\n\nfrom full_stack_transformer.utilities import log_config\n\nsys.excepthook = log_config.handle_unhandled_exception\n\n__version__ = '0.2.0'\n"}
{"type": "source_file", "path": "full_stack_transformer/core/data/encodings_collate.py", "content": "import abc\nfrom typing import Sequence\n\nfrom full_stack_transformer.core.encoding import Encoding\nfrom full_stack_transformer.core.model_input import ModelInput\n\n\nclass EncodingsCollate:\n    @abc.abstractmethod\n    def __call__(self, encodings: Sequence[Encoding]) -> ModelInput:\n        pass\n"}
{"type": "source_file", "path": "full_stack_transformer/core/text_input.py", "content": "from dataclasses import dataclass\n\n\n@dataclass\nclass TextInput:\n    pass\n"}
{"type": "source_file", "path": "full_stack_transformer/core/nn/__init__.py", "content": ""}
{"type": "source_file", "path": "full_stack_transformer/core/modelling/lightning.py", "content": "import abc\nfrom typing import Mapping, Tuple, Dict, Sequence\n\nimport torch\nfrom pytorch_lightning import LightningModule\nfrom pytorch_lightning.loggers.base import merge_dicts\n\nfrom full_stack_transformer.core.model_input import ModelInput\nfrom full_stack_transformer.core.model_output import ModelOutput\nfrom full_stack_transformer.core.modelling.model import Model\nfrom full_stack_transformer.utilities.arguments import ArgparserExtender\n\n\nclass PLModule(LightningModule, ArgparserExtender):\n    def __init__(self, model: Model):\n        super().__init__()\n\n        self.model = model\n\n    def forward(self, model_inp: ModelInput) -> ModelOutput:\n        output = self.model(model_inp)\n        return output\n\n    def training_step(self, model_inp: ModelInput, batch_idx: int) -> Dict:\n        loss, log = self._step(model_inp=model_inp)\n        return {'loss': loss, 'log': log}\n\n    def validation_step(self, model_inp: ModelInput, batch_idx: int) -> Dict:\n        loss, log = self._step(model_inp=model_inp)\n        return {'val_loss': loss, 'log': log}\n\n    def validation_epoch_end(self, val_step_results: Sequence):\n        validation_epoch_result = merge_dicts(\n            dicts=val_step_results,\n            default_func=lambda x: torch.stack(x).mean().item()\n        )\n\n        return validation_epoch_result\n\n    def configure_optimizers(self):\n        optimizer = self._get_optimizer()\n        scheduler = self._get_lr_scheduler(optimizer=optimizer)\n\n        return [optimizer], [scheduler]\n\n    def _step(self, model_inp: ModelInput) -> Tuple[torch.Tensor, Mapping]:\n        output = self.forward(model_inp=model_inp)\n        log = self._get_step_log(model_output=output)\n        return output.loss, log\n\n    @abc.abstractmethod\n    def _get_optimizer(self):\n        pass\n\n    @abc.abstractmethod\n    def _get_lr_scheduler(self, optimizer):\n        pass\n\n    @abc.abstractmethod\n    def _get_step_log(self, model_output: ModelOutput) -> Dict:\n        pass\n\n    @abc.abstractmethod\n    def get_description(self) -> Dict:\n        pass\n"}
{"type": "source_file", "path": "full_stack_transformer/core/nn/unlikelihood_candidates_loss.py", "content": "import torch\nimport torch.nn.functional\n\n\ndef unlikelihood_candidates_loss(logits, target):\n    \"\"\"Loss which helps model not to predict already appeared tokens.\n    Args:\n        logits (tensor):\n            Torch tensor of shape (bs, seq_len, vocab_size), output language\n            model scores.\n        target (tensor):\n            Torch tensor of shape (bs, seq_len), language model target (model\n            input tokens itself).\n    Returns:\n        Not-scaled unlikelihood candidates loss-value.\n    Notes:\n        This loss is based on penalizing of the previous context tokens.\n        Original paper - Welleck et al. https://arxiv.org/pdf/1908.04319.pdf.\n    \"\"\"\n\n    candidates_logp = _get_candidates_logp(logits=logits, target=target)\n    loss = _get_loss(candidates_logp)\n    return loss\n\n\ndef _get_candidates_logp(logits, target):\n    logp = torch.nn.functional.log_softmax(logits, 2)\n    bs = logits.size()[0]\n    seq_len = logits.size()[1]\n\n    logp_flat = logp.view(bs * seq_len, -1)\n    tril = torch.tril(torch.ones((seq_len, seq_len), device=logits.device), diagonal=-1)\n\n    cols = target.repeat(1, seq_len).view(seq_len * bs, -1)\n    rows = torch.arange(0, cols.size()[0]).unsqueeze(-1).repeat(1, seq_len)\n\n    not_ignore_mask = ~(cols == target.flatten().unsqueeze(-1).repeat(1, seq_len))\n    candidates_mask = tril.repeat(bs, 1).bool()\n    candidates_mask *= not_ignore_mask\n\n    cols_flat = cols[candidates_mask]\n    rows_flat = rows[candidates_mask]\n\n    logp = [] if len(cols_flat) == 0 else logp_flat[rows_flat, cols_flat]\n\n    return logp\n\n\ndef _get_loss(candidates_logp):\n    if len(candidates_logp) == 0:\n        return 0\n\n    probs = torch.clamp((1.0 - candidates_logp.exp()), min=1e-5)\n\n    loss = -torch.log(probs)\n    loss = loss.mean()\n\n    return loss\n"}
{"type": "source_file", "path": "full_stack_transformer/core/data/__init__.py", "content": ""}
{"type": "source_file", "path": "full_stack_transformer/tasks/document_lm/examples/language_generation.py", "content": "import pathlib\n\nimport torch\n\nfrom full_stack_transformer.tasks.document_lm import (\n    LanguageGeneratorParams,\n    load_model_from_checkpoint,\n    load_tokenizer_from_checkpoint,\n    LanguageGenerator,\n    DocumentInput\n)\n\nif __name__ == '__main__':\n    device = 'cuda:0'\n    experiment_dir = pathlib.Path('../../../../data/experiments/nietzsche_v0/')\n    ckpt_path = experiment_dir / 'models' / 'epoch=6.ckpt'\n\n    generator_params = LanguageGeneratorParams(\n        max_number_of_generated_tokens=64,\n        num_return_sequences=8,\n        repetition_penalty=3.0,\n        temperature=1.0,\n        top_k=50,\n        top_p=1.0\n    )\n\n    ckpt = torch.load(f=str(ckpt_path), map_location='cpu')\n    model = load_model_from_checkpoint(ckpt=ckpt, device=device)\n    tokenizer = load_tokenizer_from_checkpoint(ckpt=ckpt)\n    generator = LanguageGenerator(\n        model=model, eos_token_id=tokenizer.eos_token_id\n    )\n\n    document = DocumentInput(body='The best filosopher of the 19th century is')\n    inp_encoding = tokenizer.encode_for_inference(text_input=document)[0]\n\n    out_encodings = generator(inp_encoding, params=generator_params)\n    for enc in out_encodings:\n        text = tokenizer.decode_encoding(enc)\n        print(text + '\\n\\n')\n"}
{"type": "source_file", "path": "full_stack_transformer/tasks/document_lm/data/text_lines_parsers.py", "content": "import json\nfrom typing import Optional\n\nfrom full_stack_transformer.core.data.text_lines_parsers import TextLinesParser\nfrom full_stack_transformer.tasks.document_lm.text_input import DocumentInput\n\n\nclass DocumentLinesParser(TextLinesParser):\n    def __init__(self):\n        pass\n\n    def parse(self, text: str) -> Optional[DocumentInput]:\n        inp = DocumentInput(**json.loads(text))\n        return inp\n"}
{"type": "source_file", "path": "full_stack_transformer/tasks/document_lm/language_generator/progress.py", "content": "import torch\n\n\nclass GenerationProgressTrackerError(Exception):\n    pass\n\n\ndef _return_value_if_not_initialized(value):\n    def _return_value_if_not_initialized_inner(function_or_prop):\n        def decorated(self, *args, **kwargs):\n            if self._n_samples is None:\n                return value\n            else:\n                return function_or_prop(self, *args, **kwargs)\n\n        return decorated\n\n    return _return_value_if_not_initialized_inner\n\n\nclass GenerationProgressTracker:\n    \"\"\"Tracks generation progress.\"\"\"\n\n    @property\n    @_return_value_if_not_initialized(value=False)\n    def max_length_reached(self):\n        \"\"\"Will be True, when all sequence lengths will reach max_length.\"\"\"\n        return self.current_length >= self._max_length\n\n    @property\n    @_return_value_if_not_initialized(value=False)\n    def all_samples_finished(self):\n        \"\"\"Will be True, when eos_token_id will appear in every sequence.\"\"\"\n        return self._unfinished_mask.max() == 0\n\n    @property\n    @_return_value_if_not_initialized(value=False)\n    def finished(self):\n        return self.max_length_reached or self.all_samples_finished\n\n    @property\n    def generated_sample_lengths(self):\n        return self._gen_lengths\n\n    def __init__(self, eos_token_id: int, max_length: int):\n        \"\"\"\n        Args:\n            eos_token_id:\n                End of string token id. It's needed for GeneratorProgress to\n                understand which sample is finished.\n            max_length:\n                Maximum length of the generated sequences.\n        \"\"\"\n        self._eos_token_id = eos_token_id\n        self._max_length = max_length\n        self.current_length = 0\n\n        self._n_samples = None\n        self._unfinished_mask = None\n        self._gen_lengths = None\n\n        self._check_arguments_validity()\n\n    def _check_arguments_validity(self) -> None:\n        if self._max_length < 1:\n            raise GenerationProgressTrackerError(\"`max_length` must be >= 1.\")\n        elif self._eos_token_id < 0:\n            raise GenerationProgressTrackerError(\"`eos_token_id` must be >= 0.\")\n\n    def update(self, next_token_ids) -> None:\n        \"\"\"Updates generation progress status.\"\"\"\n        self._assert_update_is_possible()\n        self._initialize_if_needed(next_token_ids)\n\n        not_eos_tokens_mask = next_token_ids.ne(self._eos_token_id).bool()\n        self._gen_lengths[self._unfinished_mask] += 1\n\n        self._unfinished_mask *= not_eos_tokens_mask\n        self.current_length += 1\n\n    def _assert_update_is_possible(self):\n        if self.finished:\n            raise GenerationProgressTrackerError(\n                \"Can't update generation progress, because it's already \"\n                \"finished.\")\n\n    def _initialize_if_needed(self, next_tokens):\n        if self._n_samples is None:\n            device = next_tokens.device\n            self._n_samples = len(next_tokens)\n            self._unfinished_mask = torch.ones(\n                self._n_samples,\n                dtype=torch.bool,\n                device=device\n            )\n            self._gen_lengths = torch.zeros(\n                self._n_samples,\n                dtype=torch.long,\n                device=device\n            )\n"}
{"type": "source_file", "path": "full_stack_transformer/tasks/document_lm/serving/__init__.py", "content": ""}
{"type": "source_file", "path": "full_stack_transformer/tasks/document_lm/serving/views.py", "content": "import re\nfrom typing import Mapping\n\nfrom fastapi import FastAPI\n\nfrom full_stack_transformer.tasks.document_lm.language_generator.generator import LanguageGenerator, \\\n    LanguageGeneratorParams\nfrom full_stack_transformer.tasks.document_lm.serving.schemas import (\n    GeneratedTexts,\n    LanguageGeneratorAppParams\n)\nfrom full_stack_transformer.tasks.document_lm.text_input import DocumentInput\nfrom full_stack_transformer.tasks.document_lm.tokenizer import DocumentTokenizer\n\n\nclass ViewsRegister:\n    def __init__(\n            self,\n            app: FastAPI,\n            generator: LanguageGenerator,\n            tokenizer: DocumentTokenizer,\n            version: Mapping\n    ):\n        self._app = app\n        self._generator = generator\n        self._version = version\n        self._tokenizer = tokenizer\n\n    def register_generated_texts_view(self):\n        @self._app.post(\"/generated_texts/\", response_model=GeneratedTexts)\n        def generated_texts(\n                app_params: LanguageGeneratorAppParams\n        ) -> GeneratedTexts:\n            params = LanguageGeneratorParams(\n                app_params.max_number_of_generated_tokens,\n                num_return_sequences=app_params.num_return_sequences,\n                repetition_penalty=app_params.repetition_penalty,\n                temperature=app_params.temperature,\n                top_p=app_params.top_p,\n                top_k=app_params.top_k\n            )\n            document = DocumentInput(\n                body=app_params.body,\n                meta=app_params.meta\n            )\n            inp_encoding = self._tokenizer.encode_for_inference(\n                text_input=document\n            )[0]\n            encodings = self._generator(\n                encoding=inp_encoding,\n                params=params\n            )\n            texts = [self._tokenizer.decode_encoding(e) for e in encodings]\n            return GeneratedTexts(texts=texts)\n\n        return generated_texts\n\n    def register_health_check_view(self):\n        @self._app.get(\"/health_check/\")\n        def health_check():\n            return \"ok\"\n\n        return health_check\n\n    def register_version_view(self):\n        @self._app.get(\"/version/\")\n        def version():\n            return self._version\n\n        return version\n\n    def register_all_views(self):\n        for field in dir(self):\n            if re.match('^register.+view$', field):\n                getattr(self, field)()"}
{"type": "source_file", "path": "full_stack_transformer/tasks/document_lm/serving/schemas.py", "content": "from typing import List, Optional\n\nfrom pydantic import BaseModel, Field\n\n\nclass GeneratedTexts(BaseModel):\n    texts: List[str] = Field()\n\n\nclass LanguageGeneratorAppParams(BaseModel):\n    body: str = Field(default='', max_length=1024)\n    meta: Optional[str] = Field(default=None, max_length=1024)\n\n    max_number_of_generated_tokens: int = Field(default=128, ge=1, le=512)\n    temperature: float = Field(default=0.7, gt=0, le=100)\n    top_k: int = Field(default=50, ge=0)\n    top_p: float = Field(default=1.0, gt=0.0, le=1.0)\n    repetition_penalty: float = Field(default=5.0, ge=1.0, le=100)\n    num_return_sequences: int = Field(default=1, ge=1, le=64)"}
{"type": "source_file", "path": "full_stack_transformer/tasks/document_lm/language_generator/callback.py", "content": "import json\nimport pathlib\nfrom collections import Mapping\n\nfrom pytorch_lightning import Callback, Trainer\n\nfrom full_stack_transformer.tasks.document_lm.language_generator.generator import LanguageGeneratorParams, \\\n    LanguageGenerator\nfrom full_stack_transformer.tasks.document_lm.modelling.lightning import DocumentPLModule\nfrom full_stack_transformer.tasks.document_lm.text_input import DocumentInput\nfrom full_stack_transformer.utilities.experiment import Workspace\n\n\nclass LanguageGeneratorCallback(Callback):\n    _OUTPUT_FILE_NAME = 'generated.txt'\n\n    @property\n    def _default_params(self):\n        return LanguageGeneratorParams(\n            max_number_of_generated_tokens=128,\n            num_return_sequences=8,\n            repetition_penalty=1.0,\n            temperature=0.7,\n            top_k=0,\n            top_p=1.0\n        )\n\n    @property\n    def _default_document(self):\n        return DocumentInput(body='', meta=None)\n\n    @property\n    def _out_file(self) -> pathlib.Path:\n        return self._workspace.experiment_dir / self._OUTPUT_FILE_NAME\n\n    def __init__(self, experiment_workspace: Workspace):\n        self._workspace = experiment_workspace\n\n    def on_validation_end(\n            self,\n            trainer: Trainer,\n            pl_module: DocumentPLModule\n    ):\n        params = self._default_params\n        tokenizer = pl_module.tokenizer\n        generator = LanguageGenerator(\n            model=pl_module.model,\n            eos_token_id=tokenizer.eos_token_id\n        )\n\n        inp_encoding = tokenizer.encode_for_inference(\n            text_input=self._default_document,\n        )[0]\n\n        encodings = generator(encoding=inp_encoding, params=params)\n\n        text_samples = [tokenizer.decode_encoding(e) for e in encodings]\n\n        result = {\n            'Global step': trainer.global_step,\n            'Current epoch': trainer.current_epoch,\n            'Generator params': params.__dict__,\n            'Generated samples': text_samples\n        }\n\n        self._dump_result(result=result)\n\n    def _dump_result(self, result: Mapping):\n        with self._out_file.open('a') as file:\n            out_str = json.dumps(obj=result, ensure_ascii=False, indent=4)\n            out_str += '\\n'\n            file.write(out_str)\n"}
{"type": "source_file", "path": "full_stack_transformer/tasks/document_lm/examples/__init__.py", "content": ""}
{"type": "source_file", "path": "full_stack_transformer/tasks/document_lm/hf_gpt2_tokenizer.py", "content": "import pathlib\n\nfrom tokenizers import ByteLevelBPETokenizer\n\nfrom full_stack_transformer.tasks.document_lm.text_input import DocumentInput\nfrom full_stack_transformer.tasks.document_lm.tokenizer import \\\n    DocumentTokenizer\n\n_THIS_DIR = pathlib.Path(__file__).parent\n_VOCAB = _THIS_DIR / '..' / 'static' / 'gpt2_bpe' / 'vocab.json'\n_MERGES = _THIS_DIR / '..' / 'static' / 'gpt2_bpe' / 'merges.txt'\n\n\nclass HFGPT2DocumentTokenizer(DocumentTokenizer):\n    def __init__(\n            self,\n            max_meta_len: int,\n            max_body_len: int,\n            ignore_meta_prob: float\n    ):\n        tokenizer = ByteLevelBPETokenizer(\n            vocab_file=str(_VOCAB),\n            merges_file=str(_MERGES)\n        )\n\n        super().__init__(\n            tokenizer=tokenizer,\n            pad_token='<pad>',\n            max_meta_len=max_meta_len,\n            max_body_len=max_body_len,\n            ignore_meta_prob=ignore_meta_prob\n        )\n\n    def _preprocess_input(self, text_input: DocumentInput) -> DocumentInput:\n        return text_input\n\n    def _postprocess_text(self, text: str) -> str:\n        return text\n"}
{"type": "source_file", "path": "full_stack_transformer/tasks/document_lm/modelling/model.py", "content": "from typing import Optional\n\nimport torch\nimport transformers\n\nfrom full_stack_transformer.core.model_input import ModelInput\nfrom full_stack_transformer.core.model_output import LanguageModelOutput\nfrom full_stack_transformer.core.modelling.model import Model\nfrom full_stack_transformer.core.nn.unlikelihood_candidates_loss import \\\n    unlikelihood_candidates_loss\n\n\nclass DocumentModel(Model):\n\n    @property\n    def device(self):\n        return self.parameters().__next__().device\n\n    def __init__(\n            self,\n            lm_head_model: transformers.GPT2LMHeadModel,\n            unlikelihood_alpha: Optional[float]\n    ):\n        super().__init__()\n\n        self.lm_head_model = lm_head_model\n        self._ul_alpha = unlikelihood_alpha\n\n    def forward(self, inp: ModelInput) -> LanguageModelOutput:\n        lm_loss, logits, past, hidden = self.lm_head_model(\n            input_ids=inp.input_ids,\n            token_type_ids=inp.token_type_ids,\n            labels=inp.lm_labels,\n            past=inp.past\n        )\n\n        if self._ul_alpha is not None:\n            ul_loss = unlikelihood_candidates_loss(\n                logits=logits,\n                target=inp.input_ids\n            )\n\n            loss = lm_loss + self._ul_alpha * ul_loss\n        else:\n            loss = lm_loss\n            ul_loss = None\n\n        output = LanguageModelOutput(\n            lm_loss=lm_loss,\n            ul_loss=ul_loss,\n            loss=loss,\n            logits=logits,\n            past=past,\n            hidden=hidden\n        )\n\n        return output\n\n    @torch.no_grad()\n    def infer(self, inp: ModelInput) -> LanguageModelOutput:\n        \"\"\"Performs forward pass without loss calculation.\"\"\"\n\n        logits, past, hidden = self.lm_head_model(\n            input_ids=inp.input_ids,\n            token_type_ids=inp.token_type_ids,\n            past=inp.past\n        )\n\n        output = LanguageModelOutput(\n            logits=logits,\n            past=past,\n            hidden=hidden,\n            loss=None\n        )\n\n        return output\n"}
{"type": "source_file", "path": "full_stack_transformer/tasks/document_lm/language_generator/generator.py", "content": "from dataclasses import dataclass\nfrom typing import Sequence\n\nimport torch\nimport torch.nn.functional\n\nfrom full_stack_transformer.core.encoding import Encoding\nfrom full_stack_transformer.core.model_input import ModelInput\nfrom full_stack_transformer.tasks.document_lm.data.encodings_collate import DocumentEncodingsCollate\nfrom full_stack_transformer.tasks.document_lm.language_generator.logits_modifiers import (\n    IgnoredTokensModifier,\n    RepetitiveTokensModifier,\n    TemperatureModifier,\n    TopKNucleusModifier\n)\nfrom full_stack_transformer.tasks.document_lm.language_generator.progress import GenerationProgressTracker\nfrom full_stack_transformer.tasks.document_lm.modelling.model import DocumentModel\n\n\n@dataclass\nclass LanguageGeneratorParams:\n    max_number_of_generated_tokens: int\n    num_return_sequences: int\n\n    repetition_penalty: float\n    temperature: float\n    top_k: float\n    top_p: float\n\n\nclass LanguageGenerator:\n    def __init__(self, model: DocumentModel, eos_token_id: int):\n        self._model = model\n        self._eos_token_ids = eos_token_id\n\n        # LanguageGenerator never needs to pad sequences, so the `pad_value`\n        # could be any here (e.g. 0).\n        self._collator = DocumentEncodingsCollate(pad_value=0)\n\n    def __call__(\n            self,\n            encoding: Encoding,\n            params: LanguageGeneratorParams\n    ) -> Sequence[Encoding]:\n\n        self._model.eval()\n\n        encodings = [encoding] * params.num_return_sequences\n        model_inp = self._collator(\n            encodings=encodings,\n            device=self._model.device\n        )\n\n        progress = GenerationProgressTracker(\n            eos_token_id=self._eos_token_ids,\n            max_length=params.max_number_of_generated_tokens\n        )\n\n        generated_token_ids = torch.zeros(\n            params.num_return_sequences,\n            params.max_number_of_generated_tokens,\n            dtype=torch.long\n        )\n\n        generated_token_ids = generated_token_ids.to(self._model.device)\n\n        past_token_ids = model_inp.input_ids.detach().clone()\n        not_eos_mask = ~(past_token_ids == self._eos_token_ids).all(0)\n        past_token_ids = past_token_ids[:, not_eos_mask]\n\n        while not progress.finished:\n            model_out = self._model.infer(inp=model_inp)\n            next_token_logits = model_out.logits[:, -1, :]\n            past_token_ids = torch.cat(\n                tensors=[past_token_ids, generated_token_ids],\n                dim=1\n            )\n            _modify_next_token_logits(\n                next_token_logits=next_token_logits,\n                ignored_token_ids=[],\n                token_ids_to_penalize=past_token_ids,\n                repetition_penalty=params.repetition_penalty,\n                temperature=params.temperature,\n                top_k=params.top_k,\n                top_p=params.top_p\n            )\n            next_token_ids = _sample_next_token_ids(next_token_logits)\n            progress.update(next_token_ids)\n\n            generated_token_ids[:, progress.current_length - 1] = next_token_ids\n\n            input_ids = next_token_ids.unsqueeze(1)\n\n            if model_inp.token_type_ids is not None:\n                token_type_ids = model_inp.token_type_ids[:, -1:]\n            else:\n                token_type_ids = None\n\n            model_inp = ModelInput(\n                input_ids=input_ids,\n                token_type_ids=token_type_ids,\n                past=model_out.past\n            )\n\n        candidates = _get_candidates(\n            generated_tokens=generated_token_ids,\n            generated_sample_lengths=progress.generated_sample_lengths\n        )\n\n        return candidates\n\n\ndef _modify_next_token_logits(\n        next_token_logits,\n        ignored_token_ids,\n        token_ids_to_penalize,\n        repetition_penalty,\n        temperature,\n        top_k,\n        top_p\n):\n    modifiers = [\n        IgnoredTokensModifier(\n            ignored_token_ids=ignored_token_ids\n        ),\n        RepetitiveTokensModifier(\n            penalty=repetition_penalty,\n            token_ids_to_penalize=token_ids_to_penalize\n        ),\n        TemperatureModifier(\n            temperature=temperature\n        ),\n        TopKNucleusModifier(\n            top_k=top_k,\n            top_p=top_p\n        )\n    ]\n\n    _ = [modifier(next_token_logits) for modifier in modifiers]\n\n\ndef _sample_next_token_ids(next_token_logits: torch.tensor) -> torch.tensor:\n    probabilities = torch.nn.functional.softmax(\n        input=next_token_logits,\n        dim=-1\n    )\n\n    next_tokens = torch.multinomial(\n        probabilities, num_samples=1\n    )\n    return next_tokens.squeeze(1)\n\n\ndef _get_candidates(generated_tokens, generated_sample_lengths):\n    candidates = []\n    for i in range(generated_tokens.size()[0]):\n        token_ids = generated_tokens[i, :generated_sample_lengths[i]]\n        token_ids = token_ids.detach().cpu().numpy().tolist()\n        candidate = Encoding(token_ids=token_ids, lm_labels=token_ids)\n        candidates.append(candidate)\n\n    return candidates\n"}
{"type": "source_file", "path": "full_stack_transformer/tasks/document_lm/modelling/loading.py", "content": "import json\nimport re\n\nfrom transformers import GPT2Config\n\nfrom full_stack_transformer.core.modelling.loading import initialize_transformer_model_from_config\nfrom full_stack_transformer.tasks.document_lm.modelling.model import DocumentModel\nfrom full_stack_transformer.tasks.document_lm.tokenizer import get_tokenizer, DocumentTokenizer\n\n\ndef load_model_from_checkpoint(ckpt, device):\n    state_dict = dict()\n\n    for k, v in ckpt['state_dict'].items():\n        new_key = re.search(r'^model\\.(.+)$', k).group(1)\n        state_dict[new_key] = v\n\n    vocab_size = state_dict['lm_head_model.transformer.wte.weight'].size()[0]\n\n    transformer_config = json.loads(ckpt['hparams']['transformer_config'])\n    transformer_config = GPT2Config(**transformer_config)\n\n    lm_head_model = initialize_transformer_model_from_config(\n        config=transformer_config,\n        vocab_size=vocab_size\n    )\n\n    model = DocumentModel(\n        lm_head_model=lm_head_model,\n        unlikelihood_alpha=None\n    )\n\n    model.load_state_dict(state_dict=state_dict)\n    model = model.to(device)\n\n    return model\n\n\ndef load_tokenizer_from_checkpoint(ckpt) -> DocumentTokenizer:\n    config = ckpt['hparams']['tokenizer_config']\n    config['ignore_meta_prob'] = 0\n    tokenizer = get_tokenizer(**config)\n    return tokenizer\n"}
{"type": "source_file", "path": "full_stack_transformer/tasks/document_lm/modelling/lightning.py", "content": "import json\nimport pathlib\nfrom typing import Dict, Optional\n\nimport transformers\nfrom transformers import get_cosine_with_hard_restarts_schedule_with_warmup\n\nfrom full_stack_transformer.core.data.dataset import DataLoader\nfrom full_stack_transformer.core.model_output import LanguageModelOutput\nfrom full_stack_transformer.core.modelling.lightning import PLModule\nfrom full_stack_transformer.core.modelling.loading import load_transformer_model_from_path\nfrom full_stack_transformer.tasks.document_lm.data.dataset import DocumentDataset\nfrom full_stack_transformer.tasks.document_lm.modelling.model import DocumentModel\nfrom full_stack_transformer.tasks.document_lm.tokenizer import get_tokenizer\nfrom full_stack_transformer.utilities.arguments import get_func_arg_values_as_namespace\n\n\nclass DocumentPLModule(PLModule):\n    def __init__(\n            self,\n            model_path: str,\n            tokenizer_class_name: str,\n            batch_size: int,\n            max_meta_len: int,\n            max_body_len: int,\n            ignore_meta_prob: float,\n            train_file: str,\n            valid_file: str,\n            learning_rate: float,\n            num_warmup_steps: int,\n            num_cycles: int,\n            unlikelihood_alpha: float,\n            **kwargs\n    ):\n        \"\"\"\n        Args:\n            model_path (str):\n                Path to the pre-trained transformer model.\n            tokenizer_class_name (str):\n                Name of the tokenizer class which is importable from\n                `dialog_models.dialog_data`.\n            batch_size (int):\n                Batch size (the same for training and validation).\n            max_meta_len (int):\n                Max number of tokens for `meta` field encoding. Longer meta\n                field will be cut from the left side (right side will be\n                remained).\n            max_body_len (int):\n                Max number of tokens for `body` field encoding. Longer bodies\n                will be chunked. Encoding of the `meta` will be appended to\n                each part of the encoded body.\n            ignore_meta_prob (float):\n                The probability to ignore `meta` field in the document and don't\n                add it to the final encoding.\n            train_file (str):\n                Path to the training raw dialog samples file.\n            valid_file (str):\n                Path to the validation raw dialog samples file.\n            learning_rate (float):\n                Base learning rate for AdamW optimizer.\n            num_warmup_steps (int):\n                Number of warmup steps for the cosine with hard restarts\n                scheduler.\n            num_cycles (int):\n                Number of cycles for the cosine with hard restarts scheduler.\n                If 0, the scheduler will perform as a constant scheduler with\n                warmup.\n            unlikelihood_alpha (float):\n                Unlikelihood loss multiplier. If None, no unlikelihood loss will\n                be used.\n        \"\"\"\n        self.train_file = pathlib.Path(train_file)\n        self.valid_file = pathlib.Path(valid_file)\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.num_warmup_steps = num_warmup_steps\n        self.num_cycles = num_cycles\n\n        tokenizer_config = dict(\n            name=tokenizer_class_name,\n            max_meta_len=max_meta_len,\n            max_body_len=max_body_len,\n            ignore_meta_prob=ignore_meta_prob\n        )\n\n        self.tokenizer = get_tokenizer(**tokenizer_config)\n\n        lm_head_model = load_transformer_model_from_path(\n            model_path=model_path,\n            vocab_size=self.tokenizer.vocab_size\n        )\n        model = DocumentModel(\n            lm_head_model=lm_head_model,\n            unlikelihood_alpha=unlikelihood_alpha\n        )\n\n        self.train_dataset: Optional[DocumentDataset] = None\n        self.valid_dataset: Optional[DocumentDataset] = None\n\n        locals_ = locals()\n        self.transformer_config = json.dumps(\n            lm_head_model.config.__dict__,\n            ensure_ascii=False\n        )\n\n        super().__init__(model=model)\n\n        self.hparams = get_func_arg_values_as_namespace(\n            locals_=locals_,\n            func=self.__init__,\n            transformer_config=self.transformer_config,\n            tokenizer_config=tokenizer_config\n        )\n\n    def prepare_data(self) -> None:\n        self.train_dataset = DocumentDataset(\n            file_path=self.train_file,\n            tokenizer=self.tokenizer,\n        )\n\n        self.valid_dataset = DocumentDataset(\n            file_path=self.valid_file,\n            tokenizer=self.tokenizer\n        )\n\n    def train_dataloader(self) -> DataLoader:\n        return self.train_dataset.get_data_loader(\n            batch_size=self.batch_size,\n            num_workers=4\n        )\n\n    def val_dataloader(self) -> DataLoader:\n        return self.valid_dataset.get_data_loader(\n            batch_size=self.batch_size,\n            num_workers=1\n        )\n\n    def _get_optimizer(self):\n        parameters = self.model.parameters()\n        optimizer = transformers.AdamW(\n            params=parameters,\n            lr=self.learning_rate\n        )\n\n        return optimizer\n\n    def _get_lr_scheduler(self, optimizer):\n        total_steps = len(self.train_dataloader()) * self.trainer.max_epochs\n        training_steps = total_steps // self.trainer.accumulate_grad_batches\n\n        lr_scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n            optimizer=optimizer,\n            num_warmup_steps=self.num_warmup_steps,\n            num_training_steps=training_steps,\n            num_cycles=self.num_cycles\n        )\n        scheduler = {\n            'scheduler': lr_scheduler,\n            'interval': 'step',\n            'frequency': self.trainer.accumulate_grad_batches,\n            'monitor': 'Loss/valid'\n        }\n\n        return scheduler\n\n    def _get_step_log(self, model_output: LanguageModelOutput) -> Dict:\n        postfix = 'train' if self.training else 'valid'\n        log = {\n            f'LM-Loss/{postfix}': model_output.lm_loss,\n            f'UL-Loss/{postfix}': model_output.ul_loss,\n            f'Loss/{postfix}': model_output.loss\n        }\n\n        if self.training:\n            current_lr = self.trainer.optimizers[0].param_groups[0]['lr']\n            log['Learning-Rate'] = current_lr\n\n        return log\n\n    def get_description(self) -> Dict:\n        return {'Transformer': self.transformer_config}\n"}
{"type": "source_file", "path": "full_stack_transformer/tasks/document_lm/serving/app.py", "content": "import logging\nimport pathlib\n\nimport torch\nfrom fastapi import FastAPI\n\nimport full_stack_transformer\nfrom full_stack_transformer.tasks.document_lm.language_generator.generator import LanguageGenerator\nfrom full_stack_transformer.tasks.document_lm.modelling.loading import (\n    load_tokenizer_from_checkpoint,\n    load_model_from_checkpoint\n)\nfrom full_stack_transformer.tasks.document_lm.serving.views import ViewsRegister\nfrom full_stack_transformer.utilities.log_config import prepare_logging\n\n_LOGGER = logging.getLogger(__name__)\n\n\ndef prepare(checkpoint_path, device, logs_dir) -> FastAPI:\n    prepare_logging(pathlib.Path(logs_dir))\n    ckpt = torch.load(f=checkpoint_path, map_location='cpu')\n\n    tokenizer = load_tokenizer_from_checkpoint(ckpt=ckpt)\n\n    model = load_model_from_checkpoint(ckpt=ckpt, device=device)\n    generator = LanguageGenerator(\n        model=model,\n        eos_token_id=tokenizer.eos_token_id\n    )\n    version = _get_version_from_ckpt(\n        ckpt=ckpt,\n        checkpoint_path=checkpoint_path\n    )\n    app = _prepare_app(\n        generator=generator,\n        tokenizer=tokenizer,\n        version=version\n    )\n\n    _LOGGER.info(\n        'All text_generator_service components were successfully initialized.'\n    )\n\n    return app\n\n\ndef _get_version_from_ckpt(ckpt, **kwargs):\n    version = dict()\n\n    version['package_version'] = full_stack_transformer.__version__\n    version['epoch'] = ckpt['epoch']\n    version['global_step'] = ckpt['global_step']\n    version['hparams'] = ckpt['hparams']\n    version.update(kwargs)\n\n    return version\n\n\ndef _prepare_app(generator, tokenizer, version):\n    app = FastAPI()\n    views_register = ViewsRegister(\n        app=app,\n        generator=generator,\n        version=version,\n        tokenizer=tokenizer\n    )\n    views_register.register_all_views()\n\n    return app\n"}
{"type": "source_file", "path": "full_stack_transformer/tasks/document_lm/task_runner.py", "content": "from typing import List\n\nfrom pytorch_lightning import Callback\n\nfrom full_stack_transformer.core.task_runner import TaskRunner\nfrom full_stack_transformer.tasks.document_lm.language_generator.callback import LanguageGeneratorCallback\nfrom full_stack_transformer.tasks.document_lm.modelling.lightning import DocumentPLModule\n\n\nclass DocumentLMTaskRunner(TaskRunner):\n    def __init__(self):\n        super().__init__(pl_module_cls=DocumentPLModule)\n\n    def _get_trainer_callbacks(self) -> List[Callback]:\n        return [LanguageGeneratorCallback(experiment_workspace=self.workspace)]\n\n\nif __name__ == '__main__':\n    runner = DocumentLMTaskRunner()\n    runner.run()"}
{"type": "source_file", "path": "full_stack_transformer/tasks/document_lm/language_generator/__init__.py", "content": ""}
{"type": "source_file", "path": "full_stack_transformer/tasks/document_lm/data/dataset.py", "content": "import pathlib\n\nfrom full_stack_transformer.core.data.dataset import Dataset\nfrom full_stack_transformer.tasks.document_lm.data.encodings_collate import \\\n    DocumentEncodingsCollate\nfrom full_stack_transformer.tasks.document_lm.data.text_lines_parsers import \\\n    DocumentLinesParser\nfrom full_stack_transformer.tasks.document_lm.tokenizer import DocumentTokenizer\nfrom full_stack_transformer.utilities.files import count_lines_in_file\n\n\nclass DocumentDataset(Dataset):\n    def __init__(\n            self,\n            file_path: pathlib.Path,\n            tokenizer: DocumentTokenizer,\n    ):\n        collate = DocumentEncodingsCollate(pad_value=tokenizer.pad_token_id)\n        text_lines_parser = DocumentLinesParser()\n\n        super().__init__(\n            file_path=file_path,\n            tokenizer=tokenizer,\n            text_lines_parser=text_lines_parser,\n            encodings_collate=collate\n        )\n\n    def __len__(self) -> int:\n        return count_lines_in_file(self._file_path)\n"}
{"type": "source_file", "path": "full_stack_transformer/tasks/document_lm/ru_transformers_tokenizer.py", "content": "import pathlib\nimport re\nfrom typing import Optional\n\nfrom tokenizers import SentencePieceBPETokenizer\n\nfrom full_stack_transformer.tasks.document_lm.text_input import DocumentInput\nfrom full_stack_transformer.tasks.document_lm.tokenizer import \\\n    DocumentTokenizer\n\n_THIS_DIR = pathlib.Path(__file__).parent\n_VOCAB = _THIS_DIR / '..' / 'static' / 'ru_transformers_sp' / 'vocab.json'\n_MERGES = _THIS_DIR / '..' / 'static' / 'ru_transformers_sp' / 'merges.txt'\n\n_NEW_LINE_REP = '<|n|>'\n\n\nclass RuTransformersDocumentTokenizer(DocumentTokenizer):\n    def __init__(\n            self,\n            max_meta_len: int,\n            max_body_len: int,\n            ignore_meta_prob: float\n    ):\n        tokenizer = SentencePieceBPETokenizer(\n            vocab_file=str(_VOCAB),\n            merges_file=str(_MERGES)\n        )\n\n        super().__init__(\n            tokenizer=tokenizer,\n            max_meta_len=max_meta_len,\n            max_body_len=max_body_len,\n            ignore_meta_prob=ignore_meta_prob,\n            pad_token='<pad>'\n        )\n\n    def _preprocess_input(self, text_input: DocumentInput) -> DocumentInput:\n        body = _preprocess(text_input.body)\n        meta = _preprocess(text_input.meta)\n\n        new_input = DocumentInput(body=body, meta=meta)\n\n        return new_input\n\n    def _postprocess_text(self, text: str) -> str:\n        return _postrpocess(text=text)\n\n\ndef _preprocess(text: Optional[str]) -> Optional[str]:\n    if text is None:\n        return None\n\n    if text and text[0] != ' ':\n        text = ' ' + text\n\n    text = re.sub(r'(?=[^ ])([\\W])([\\w])', r'\\g<1> \\g<2>', text)\n    text = text.replace('\\n', f' {_NEW_LINE_REP}')\n    return text\n\n\ndef _postrpocess(text: str) -> str:\n    text = re.sub(re.escape(_NEW_LINE_REP), '\\n', text)\n    text = re.sub(r'([\\n(]) (\\w)', r'\\g<1>\\g<2>', text)\n    text = re.sub(r'(\\W|^)([«\"''\\n(]|^) (\\w)', r'\\g<1>\\g<2>\\g<3>', text)\n    text = re.sub(r'(\\w)- (\\w)', r'\\g<1>-\\g<2>', text)\n    return text\n"}
{"type": "source_file", "path": "full_stack_transformer/tasks/document_lm/language_generator/logits_modifiers.py", "content": "import abc\nfrom typing import Sequence, Optional\n\nimport torch\nfrom transformers import modeling_utils\n\n_MINUS_INF = -float(\"Inf\")\n\n\nclass NextTokenLogitsModifierError(Exception):\n    pass\n\n\nclass NextTokenLogitsModifier(abc.ABC):\n    @abc.abstractmethod\n    def modify(self, logits: torch.tensor):\n        \"\"\"Modifies next token logits. Should modify them inplace.\"\"\"\n\n    def __call__(self, logits: torch.tensor):\n        return self.modify(logits)\n\n\nclass TemperatureModifier(NextTokenLogitsModifier):\n    \"\"\"Classic temperature logits distribution modifier.\"\"\"\n\n    def __init__(self, temperature: float):\n        \"\"\"\n        Args:\n            temperature:\n                The value used to module the next token probabilities.\n                Must be >= 0. 0 Means, that the token with maximum logits value\n                will be taken (no sampling).\n        Raises:\n            NextTokenLogitsModifierError: in case of incorrect input arguments.\n        \"\"\"\n        self._temperature = temperature\n        self._check_arguments_validity()\n\n    def _check_arguments_validity(self) -> None:\n        if self._temperature < 0:\n            raise NextTokenLogitsModifierError('`temperature` must be >= 0.')\n\n    def modify(self, logits: torch.tensor):\n        logits.mul_(1 / self._temperature)\n\n\nclass TopKNucleusModifier(NextTokenLogitsModifier):\n    \"\"\"Filters a distribution of logits using top-k and/or top-p filtering\"\"\"\n\n    def __init__(self, top_k: int, top_p: float):\n        \"\"\"\n        Args:\n            top_k:\n                The number of highest probability vocabulary tokens to keep for\n                top-k-filtering. Between 0 and infinity. If 0, top-k filtering\n                will not be using.\n            top_p:\n                The cumulative probability of parameter highest probability\n                vocabulary tokens to keep for nucleus sampling. Must be between\n                0 and 1.\n        Notes:\n            `top_k` is performed before `top_p`. It means, that cumulative\n            probability will be counted only for top k tokens.\n        Raises:\n            NextTokenLogitsModifierError: in case of incorrect input arguments.\n        \"\"\"\n        self._top_k = top_k\n        self._top_p = top_p\n\n        self._check_arguments_validity()\n\n    def _check_arguments_validity(self) -> None:\n        if self._top_k < 0:\n            raise NextTokenLogitsModifierError(\n                '`top_k` must be >= 0. Use `top_k` = 0 if you want to switch '\n                'off the top-k filtering.')\n        elif not (0 <= self._top_p <= 1):\n            raise NextTokenLogitsModifierError(\n                '`top_p` must be between 0 and 1.')\n\n    def modify(self, logits: torch.tensor):\n        \"\"\"Delegates call to the transformers `top_k_top_p_filtering` func.\"\"\"\n        modeling_utils.top_k_top_p_filtering(\n            logits=logits,\n            top_k=self._top_k,\n            top_p=self._top_p,\n            filter_value=_MINUS_INF)\n\n\nclass IgnoredTokensModifier(NextTokenLogitsModifier):\n    \"\"\"Assigns zero probabilities logits to the ignored tokens.\"\"\"\n\n    def __init__(self, ignored_token_ids: Optional[Sequence[int]]):\n        \"\"\"\n        Args:\n            ignored_token_ids:\n                Ignored token indexes sequence.\n        \"\"\"\n        self._ignored_token_ids = list(set(ignored_token_ids))\n\n    def modify(self, logits: torch.tensor):\n        if self._ignored_token_ids:\n            logits[:, self._ignored_token_ids] = _MINUS_INF\n\n\nclass RepetitiveTokensModifier(NextTokenLogitsModifier):\n    \"\"\"Decreases probability (and logits) for tokens which have already been.\"\"\"\n\n    def __init__(\n            self,\n            penalty: float,\n            token_ids_to_penalize: torch.tensor\n    ):\n        \"\"\"\n        Args:\n            penalty:\n                Repetitive tokens penalization strength (must be >= 1.0).\n                1.0 means no penalty.\n            token_ids_to_penalize:\n                Tensor with shape (batch_size, seq_len).\n        Raises:\n            NextTokenLogitsModifierError: in case of incorrect input arguments.\n        \"\"\"\n        self._token_ids_to_penalize = token_ids_to_penalize\n        self._penalty = penalty\n        self._check_arguments_validity()\n\n    def _check_arguments_validity(self) -> None:\n        if self._penalty < 1.0:\n            raise NextTokenLogitsModifierError(\n                \"`penalty` must be >= 1.0. Use 1.0 if you don't want to apply \"\n                \"repetition penalty.\")\n\n    def modify(self, logits: torch.tensor):\n        for i in range(logits.size()[0]):\n            ids_to_penalize = self._token_ids_to_penalize[i]\n            _penalize_logits_tensor(logits[i], ids_to_penalize, self._penalty)\n\n\ndef _penalize_logits_tensor(logits, penalty_idx, penalty):\n    if penalty == 1.0:\n        return\n\n    idx = torch.unique(penalty_idx)\n    logits -= logits.max()\n\n    full_exp = torch.exp(logits)\n\n    e = full_exp[idx]\n    sum_e = torch.sum(e)\n    s = torch.sum(full_exp) - sum_e\n\n    n = torch.log((e * s) / (penalty * s + penalty * sum_e - sum_e))\n    logits[idx] = n\n"}
{"type": "source_file", "path": "full_stack_transformer/tasks/document_lm/telegram/handlers.py", "content": "import json\nimport logging\nimport os\nimport pathlib\nfrom http import HTTPStatus\nfrom typing import Optional, Tuple\n\nimport aiohttp\nfrom aiogram import Dispatcher, types\nfrom aiogram.types import InlineKeyboardMarkup, InlineKeyboardButton, ParseMode\nfrom aiohttp import ServerDisconnectedError\n\nfrom full_stack_transformer.utilities.strings import get_string_md5\n\n\nclass HandlersRegister:\n    \"\"\"Class which registers all text generator telegram client handlers.\"\"\"\n\n    WELCOME_MESSAGE = \"Hello, send me a message and I'll continue it.\"\n    TEXT_GENERATOR_SERVICE_ERROR_MESSAGE = \"I'm broken, or tired...\"\n    CANT_REPEAT_MESSAGE = \"Can't repeat, please send me a new message.\"\n\n    REPEAT_CALLBACK_DATA_PREFIX = '__repeat__'\n\n    DEFAULT_PARAMS = {\n        'max_number_of_generated_tokens': 128,\n        'temperature': 1.0,\n        'top_k': 0,\n        'top_p': 1.0,\n        'repetition_penalty': 1.0\n    }\n\n    def __init__(\n            self,\n            dispatcher: Dispatcher,\n            text_generator_service_url: str,\n            text_generator_service_auth: Optional,\n            logs_dir: pathlib.Path\n    ):\n        self._dispatcher = dispatcher\n        self._msg_cache = MessagesCache()\n        self._url = text_generator_service_url\n        self._auth = text_generator_service_auth\n        self._log_handler = LoggingHandler(logs_dir=logs_dir)\n\n    def register_start_message_handler(self):\n        \"\"\"Handles `/start` command and sends welcome message.\"\"\"\n\n        @self._dispatcher.message_handler(commands=['start'])\n        async def start(message: types.Message):\n            await message.answer(self.WELCOME_MESSAGE)\n\n    def register_send_reply_message_handler(self):\n        \"\"\"Replies on user input message.\"\"\"\n\n        @self._dispatcher.message_handler()\n        async def send_reply(message: types.Message):\n            message_hash = self._msg_cache.add_message(message.text)\n\n            await self._get_response_and_send_reply(\n                message=message,\n                seed_string=message.text,\n                callback_data=message_hash\n            )\n\n    def register_send_reply_callback_query_handler(self):\n        \"\"\"Replies with user's previous seed text.\"\"\"\n\n        @self._dispatcher.callback_query_handler(\n            lambda q: q.data.startswith(self.REPEAT_CALLBACK_DATA_PREFIX)\n        )\n        async def send_reply(callback_query: types.CallbackQuery):\n            message_hash = callback_query.data.split(':', 1)[1]\n            message_text = self._msg_cache.get_message(message_hash)\n\n            if message_text is None:\n                await callback_query.message.answer(self.CANT_REPEAT_MESSAGE)\n            else:\n                await self._get_response_and_send_reply(\n                    message=callback_query.message,\n                    seed_string=message_text,\n                    callback_data=message_hash\n                )\n\n    async def _get_response_and_send_reply(\n            self,\n            message,\n            seed_string,\n            callback_data\n    ):\n        response = await self._get_text_generator_service_response(\n            seed_string=seed_string\n        )\n\n        reply_text = _prepare_reply_text(\n            text_generator_service_response=response,\n            prefix_string=seed_string\n        )\n\n        keyboard = _get_inline_keyboard(callback_data=callback_data)\n\n        self._log_handler.log(\n            user_id=_get_user_id_from_message(message=message),\n            log_msg=f'\\n{reply_text}\\n'\n        )\n\n        await message.answer(\n            text=reply_text,\n            reply_markup=keyboard,\n            parse_mode=ParseMode.HTML\n        )\n\n    async def _get_text_generator_service_response(\n            self,\n            seed_string: str\n    ) -> Tuple[Optional[str], int]:\n        url = os.path.join(self._url, 'generated_texts')\n\n        payload = {'body': seed_string}\n        payload.update(self.DEFAULT_PARAMS)\n\n        headers = {'Content-Type': 'application/json'}\n        async with aiohttp.ClientSession(auth=self._auth) as session:\n            try:\n                async with session.post(\n                        url=url,\n                        data=json.dumps(payload),\n                        headers=headers\n                ) as response:\n                    status = response.status\n                    reply = await response.text()\n                    return reply, status\n            except ServerDisconnectedError:\n                return None, HTTPStatus.INTERNAL_SERVER_ERROR\n\n    def register_all_handlers(self):\n        self.register_start_message_handler()\n        self.register_send_reply_message_handler()\n        self.register_send_reply_callback_query_handler()\n\n\ndef _prepare_reply_text(\n        text_generator_service_response: Tuple[Optional[str], int],\n        prefix_string: str\n) -> str:\n    response_text, status = text_generator_service_response\n    if status == 200:\n        response_dict = json.loads(response_text)\n        generated_text = response_dict['texts'][0]\n        generated_text = f'<b>{prefix_string}</b> {generated_text}'\n    else:\n        generated_text = HandlersRegister.TEXT_GENERATOR_SERVICE_ERROR_MESSAGE\n\n    return generated_text\n\n\ndef _get_user_id_from_message(message):\n    username = str(message.chat['username'])\n    chat_id = str(message.chat['id'])\n    user_name = \"\".join(x for x in username if x.isalnum())\n    user_id = user_name + '_' + chat_id\n    return user_id\n\n\ndef _get_inline_keyboard(callback_data: str) -> InlineKeyboardMarkup:\n    buttons = []\n\n    data = f'{HandlersRegister.REPEAT_CALLBACK_DATA_PREFIX}:{callback_data}'\n    repeat_button = InlineKeyboardButton(text='Repeat', callback_data=data)\n    buttons.append(repeat_button)\n    keyboard = InlineKeyboardMarkup(inline_keyboard=[buttons])\n\n    return keyboard\n\n\nclass MessagesCache:\n    def __init__(self):\n        self._cache = dict()\n\n    def add_message(self, message: str) -> str:\n        message_hash = get_string_md5(message)[:16]\n        self._cache[message_hash] = message\n        return message_hash\n\n    def get_message(self, message_hash: str) -> str:\n        message = self._cache.get(message_hash)\n        return message\n\n\nclass LoggingHandler:\n    \"\"\"Performs logging for users in their individual files.\"\"\"\n\n    FORMATTER = '%(asctime)s %(message)s'\n\n    def __init__(self, logs_dir: pathlib.Path):\n        self._logs_dir = logs_dir\n        self._cache = dict()\n\n        self._logs_dir.mkdir(parents=True, exist_ok=True)\n\n    def _get_logger(self, user_id: str):\n        if user_id not in self._cache:\n            log_file = self._logs_dir / f'{user_id}.log'\n            formatter = logging.Formatter(self.FORMATTER)\n            handler = logging.FileHandler(str(log_file))\n            handler.setFormatter(formatter)\n\n            logger = logging.getLogger(f'{user_id}')\n            logger.setLevel(logging.INFO)\n            logger.addHandler(handler)\n            self._cache[user_id] = logger\n        else:\n            logger = self._cache[user_id]\n\n        return logger\n\n    def log(self, user_id: str, log_msg: str):\n        logger = self._get_logger(user_id=user_id)\n        logger.info(log_msg)\n"}
{"type": "source_file", "path": "full_stack_transformer/tasks/document_lm/modelling/__init__.py", "content": ""}
{"type": "source_file", "path": "full_stack_transformer/tasks/document_lm/telegram/__init__.py", "content": ""}
{"type": "source_file", "path": "full_stack_transformer/tasks/document_lm/telegram/app.py", "content": "import argparse\nimport pathlib\nfrom typing import Optional\n\nimport aiohttp\nfrom aiogram import Dispatcher, Bot\nfrom aiogram.utils import executor\n\nfrom full_stack_transformer.tasks.document_lm.telegram.handlers import HandlersRegister\n\nTHIS_DIR = pathlib.Path(__file__).parent\n\n\ndef _parse_args():\n    logs_dir = THIS_DIR / '../../data/telegram_logs/'\n\n    parser = argparse.ArgumentParser(\n        description='This script runs telegram client for the text generator '\n                    'service'\n    )\n\n    parser.add_argument(\n        '--telegram_api_token', type=str, required=True,\n        help='Telegram client API token. Could be obtained via `@BotFather` '\n             'bot in telegram.'\n    )\n    parser.add_argument(\n        '--text_generator_service_url', type=str, required=True\n    )\n    parser.add_argument(\n        '--text_generator_service_login', type=str, required=False\n    )\n    parser.add_argument(\n        '--text_generator_service_password', type=str, required=False\n    )\n    parser.add_argument(\n        '--logs_dir', type=str, required=False, default=logs_dir\n    )\n\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = _parse_args()\n\n    dispatcher = prepare(\n        logs_dir=args.logs_dir,\n        telegram_api_token=args.telegram_api_token,\n        text_generator_service_url=args.text_generator_service_url,\n        text_generator_service_login=args.text_generator_service_login,\n        text_generator_service_password=args.text_generator_service_password\n    )\n\n    executor.start_polling(dispatcher, skip_updates=True)\n\n\ndef prepare(\n        telegram_api_token: str,\n        text_generator_service_url: str,\n        logs_dir: pathlib.Path,\n        text_generator_service_login: Optional[str] = None,\n        text_generator_service_password: Optional[str] = None\n) -> Dispatcher:\n    \"\"\"Prepares dispatcher object.\"\"\"\n\n    bot = Bot(token=telegram_api_token)\n    dispatcher = Dispatcher(bot)\n\n    if text_generator_service_login and text_generator_service_password:\n        text_generator_service_auth = aiohttp.BasicAuth(\n            login=text_generator_service_login,\n            password=text_generator_service_password\n        )\n    else:\n        text_generator_service_auth = None\n\n    handlers_register = HandlersRegister(\n        dispatcher=dispatcher,\n        text_generator_service_url=text_generator_service_url,\n        text_generator_service_auth=text_generator_service_auth,\n        logs_dir=logs_dir\n    )\n\n    handlers_register.register_all_handlers()\n\n    return dispatcher\n\n\nif __name__ == '__main__':\n    main()"}
