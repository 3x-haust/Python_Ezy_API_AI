{"repo_info": {"repo_name": "DegentCivil", "repo_owner": "KingJiongEN", "repo_url": "https://github.com/KingJiongEN/DegentCivil"}}
{"type": "test_file", "path": "debug/test_asynic.py", "content": "import asyncio\n\n# Example asynchronous function\nasync def async_task(name, delay):\n    print(f'Task {name} starting')\n    await asyncio.sleep(delay)\n    print(f'Task {name} completed')\n\nasync def run_async_tasks():\n    # Schedule multiple tasks to run concurrently\n    tasks = [\n        asyncio.create_task(async_task('A', 2)),\n        asyncio.create_task(async_task('B', 3)),\n        asyncio.create_task(async_task('C', 1))\n    ]\n    # Optionally, you can wait for all tasks to complete\n    await asyncio.gather(*tasks)\n\ndef main():\n    # This is your main synchronous entry point\n    print('Starting main program')\n    asyncio.run(run_async_tasks())\n    print('Main program continues after async tasks are scheduled')\n    # Any code here would run after the async tasks have been started (and possibly completed)\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "app/database/orm/artwork_record.py", "content": "from sqlalchemy import Column, Integer, String, Float, TIMESTAMP\nfrom app.database.base_database import Base  # Ensure this imports correctly based on your project structure\n\nclass ArtworkRecord(Base):\n    __tablename__ = 'artwork'\n    id = Column(String(225), primary_key=True) # artwork_id / resource_id\n    artwork_type = Column(String(255))\n    resource = Column(String(255))\n    owner_id = Column(String(255))\n    owner_name = Column(String(255))\n    prompt = Column(String(255))\n    create_place_id = Column(String(255))\n    create_place_name = Column(String(255))\n    create_time_game = Column(Integer)\n    create_time_real = Column(TIMESTAMP)\n    price = Column(Integer)\n    \n"}
{"type": "source_file", "path": "app/database/__init__.py", "content": ""}
{"type": "source_file", "path": "app/communication/websocket_server.py", "content": "import asyncio\nimport json\n\nfrom starlette.websockets import WebSocket, WebSocketDisconnect\n\nfrom ..utils.log import LogManager\n\n\nclass WebSocketServer:\n    _instance = None\n\n    def __new__(cls, *args, **kwargs):\n        if cls._instance is None:\n            cls._instance = super(WebSocketServer, cls).__new__(cls)\n        return cls._instance\n\n    def __init__(self):\n        if not hasattr(self, \"_initialized\"):\n            self.connected_clients = set()\n            self.callback = None\n            self._initialized = True\n\n    @classmethod\n    def _get_instance(cls):\n        if not cls._instance:\n            return None\n        return cls._instance\n\n    async def __call__(self, scope, receive, send):\n        if scope[\"type\"] == \"websocket\":\n            websocket = WebSocket(scope=scope, receive=receive, send=send)\n            await websocket.accept()\n            self.connected_clients.add(websocket)\n            LogManager.log_info(f\"[Network]: Establish connection from {websocket.client}\")\n            try:\n                while True:\n                    message = await websocket.receive_text()\n                    await self.on_message(message)\n            except WebSocketDisconnect:\n                LogManager.log_info(\"[Network]: WebSocket disconnected normally\")\n            except Exception as e:\n                LogManager.log_error(f\"[Network]: {e}\")\n            finally:\n                try:\n                    if websocket in self.connected_clients:\n                        self.connected_clients.remove(websocket)\n                    await websocket.close()\n                except Exception as e:\n                    LogManager.log_warning(f\"[Network]: disconnecting {e}\")\n\n    async def on_message(self, message):\n        LogManager.log_info(f\"[Network]: Receive message: {message}\")\n        if message == \"start\":\n            if self.callback:\n                task = asyncio.create_task(self.callback())\n\n    @classmethod\n    async def broadcast_message_async(cls, message_type: str, data: json):\n        instance = cls._get_instance()\n        if instance:\n            await instance.execute_broadcast_message(message_type, data)\n\n    @classmethod\n    def broadcast_message(cls, message_type: str, data: json):\n        instance = cls._get_instance()\n        if instance:\n            coroutine = instance.execute_broadcast_message(message_type, data)\n            loop = asyncio.get_event_loop()\n            if loop.is_running():\n                asyncio.run_coroutine_threadsafe(coroutine, loop)\n            else:\n                loop.run_until_complete(coroutine)\n\n    async def execute_broadcast_message(self, message_type: str, data: json):\n        message = json.dumps({\"type\": message_type, \"data\": data})\n        LogManager.log_debug(f\"[Network]: Broadcasting message\")\n        try:\n            if self.connected_clients:\n                await asyncio.gather(*(client.send_text(message) for client in self.connected_clients))\n        except Exception as e:\n            LogManager.log_error(f\"[Network]: Error broadcasting message: {e}\")\n"}
{"type": "source_file", "path": "app/constants/prompt_type.py", "content": "from enum import Enum, auto\n\nclass PromptType(Enum):\n    ACT = auto()\n    CHATINIT = auto()\n    CHATING = auto()\n    ACTREFLECTION = auto()\n    CHATRCEIVE = auto()\n    CRITIC = auto()\n    MEMORY_STORE = auto()\n    PLAN = auto()\n    PERSPECT = auto()\n    Perspect_Quest = auto() #QA_FRAMEWORK_QUESTION\n    Perspect_Ans = auto() #QA_FRAMEWORK_ANSWER\n    TRADE = auto()\n    USE = auto()\n    DRAWINIT = auto()\n    DRAW = auto()\n    SUM = auto()\n    APPRECIATE = auto()\n    USERTRADE = auto()\n    BARGAIN = auto()\n    EMOTION = auto()\n    ESTIMATE = auto()\n    INNER_MONOLOGUE = auto()\n    \n    def to_str(self, ):\n        return self.name.lower()\n    \nTypeName2Name = {name: state for name, state in PromptType.__members__.items()}\n\nif __name__ == '__main__':\n    print(PromptType.name2classes)"}
{"type": "source_file", "path": "app/__init__.py", "content": ""}
{"type": "source_file", "path": "app/constants/msg_id.py", "content": "from .character_state import CharacterState\n'''\n{\n        public const int LLM_MSG_WALK_TO_DO = 1001;\n        public const int LLM_MSG_ACTION = 1002;\n        public const int LLM_MSG_NFT = 1003;\n        public const int LLM_USER_BUY_ARTWORK_RESULT = 1004;\n        public const int LLM_USER_SELL_ARTWORK_RESULT = 1005;\n        public const int LLM_AGENT_ARTWORK_TRADE = 1006;\n        public const int LLM_MSG_WALK_TO = 1007;\n        public const int LLM_MSG_SPEAK = 1008;\n        public const int LLM_ARTWORK_RECYCLE_PRICE_DESC = 1010;\n        public const int LLM_MSG_RECEIVE_SUBSISTENCE_ALLOWANCES = 1011;\n        public const int LLM_MSG_BIRTH_REQUEST = 1012;\n\n        public const int SERVER_MSG_INIT = 2000;\n        public const int SERVER_MSG_WALK_STOPPED = 2001;\n        public const int SERVER_MSG_ON_NEW_DAY = 2002;\n        public const int SERVER_MSG_AGENT_ATTR_CHANGE = 2003;\n        public const int SERVER_MSG_USER_BUY_ARTWORK = 2004;\n        public const int SERVER_MSG_USER_SELL_ARTWORK = 2005;\n        public const int SERVER_MSG_CMD = 2006;\n        public const int SERVER_MSG_ACK = 2007;\n        public const int SERVER_MSG_AGENT_BORN = 2008;\n    }\n'''\n\nAllStateMsg = [2008] # handle once the message is recieved\n\nState2RecieveMsgId = { CharacterState.MOVE: 2001,\n                      CharacterState.USERTRADE: [2004, 2005],\n        }\n\nState2PushMsgId = { CharacterState.MOVE: 1007,\n                   CharacterState.SUM: 1002,\n                    CharacterState.ACT: 1007,\n                    CharacterState.DRAW: 1003,\n                   }\n                     \n                "}
{"type": "source_file", "path": "app/database/base_database.py", "content": "import os\nfrom sqlalchemy import create_engine\nfrom sqlalchemy import DDL\nfrom sqlalchemy.orm import scoped_session, sessionmaker\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy import event\nfrom sqlalchemy.sql import text\n\nfrom app.settings import db_connection_string\nprint(db_connection_string)\n\nengine = create_engine(db_connection_string,echo=True, future=True )#connect_args={'sslmode': 'prefer'})\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n\nBase = declarative_base()\nevent.listen(Base.metadata, 'before_create', DDL(\"CREATE DATABASE IF NOT EXISTS DgentCivil DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;\"))\n# event.listen(Base.metadata, 'before_create',)\nfrom contextlib import contextmanager\n\n@contextmanager\ndef get_db_context():\n    try:\n        db = next(get_db())  # Get the session object from the generator\n        yield db\n    finally:\n        db.close()\n\n\ndef init_db():\n    # import all modules before calling init_db()\n    from app.database.orm import (\n        artwork_record\n    )\n    Base.metadata.create_all(bind=engine)\n\ndef get_db():\n    db = SessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\n        \nif __name__ == '__main__':\n    init_db()\n    print('Database initialized.')"}
{"type": "source_file", "path": "app/database/milvus_datastore.py", "content": "from typing import List, Optional, Union\nfrom pymilvus import (\n    connections,\n    db,\n    CollectionSchema, FieldSchema, DataType,\n    Collection,\n    utility,\n    Partition,\n)\nfrom pymilvus.orm.collection import (\n    MutationResult,\n    SearchResult\n)\nfrom ..utils.log import LogManager\nimport os\n\nMILVUS_ALIAS = 'default'\nMILVUS_INDEX_PARAMS = {\n    \"metric_type\":\"COSINE\",\n    \"index_type\":\"IVF_FLAT\",\n    \"params\":{\"nlist\":64}\n}\nMILVUS_CONSISTENCY_LEVEL = os.environ.get(\"MILVUS_CONSISTENCY_LEVEL\") or 'Strong'\n\nclass MilvusDataStore():\n    MODULE_NAME = 'MilvusDataStore'\n    def __init__(\n        self,\n        host: str = \"\",\n        collection_name: str = \"\",\n        port: int = \"\",\n        alias: str = MILVUS_ALIAS,\n        field_schema: CollectionSchema = None,\n        index_field: str = 'emb',\n        index_params: dict = MILVUS_INDEX_PARAMS,\n        scalar_index_fields: Union[str, List[str]] = None,\n        create_new: Optional[bool] = False,\n        consistency_level: str = \"Bounded\",\n    ):\n        \"\"\"Create a Milvus DataStore.\n\n        The Milvus Datastore allows for storing your indexes and metadata within a Milvus instance.\n\n        Args:\n            create_new (Optional[bool], optional): Whether to overwrite if collection already exists. Defaults to True.\n            consistency_level(str, optional): Specify the collection consistency level.\n                                                Defaults to \"Bounded\" for search performance.\n                                                Set to \"Strong\" in test cases for result validation.\n        \"\"\"\n        # Overwrite the default consistency level by MILVUS_CONSISTENCY_LEVEL\n        # self._consistency_level = MILVUS_CONSISTENCY_LEVEL or consistency_level\n        self.collection_name = collection_name\n        self.port = port\n        self.alias = alias\n        self.host = host\n        self.field_schema = field_schema\n        self.index_field = index_field\n        self.index_params = index_params\n        self.scalar_index_fields = scalar_index_fields\n        self._consistency_level = consistency_level\n        self._create_connection()\n        self._create_collection(self.collection_name, create_new)  # type: ignore\n        self._create_index()\n        if self.collection:\n                self.collection.load()\n\n\n    def _create_connection(self):\n        try:\n            self.connection = connections.connect(\n                alias=self.alias,\n                host=self.host,\n                port=self.port,\n                #   user='username',\n                #   password='password',\n                # db_name=db_name, # 指定使用的db，否则默认是'default'\n            )\n        except Exception as e:\n            # 已脱敏，如要修改日志，请注意脱敏问题\n            LogManager.log_error(\n                # level='error',\n                # function=self.MODULE_NAME,\n                msg = \"Failed to create connection to Milvus server '{}:{}', error: {}\".format(self.host, self.port, e)\n            )\n\n    def _disconnect(self,):\n        connections.disconnect(self.alias)\n    \n    def _create_collection(self, collection_name:str, create_new: bool = False) -> None:\n        \"\"\"Create a collection based on environment and passed in variables.\n\n        Args:\n            create_new (bool): Whether to overwrite if collection already exists.\n        \"\"\"\n        # try:\n        # If the collection exists and create_new is True, drop the existing collection\n        # __import__('ipdb').set_trace()\n        if utility.has_collection(collection_name, using=self.alias) and create_new:\n            utility.drop_collection(collection_name, using=self.alias)\n\n        # Check if the collection doesnt exist\n        if utility.has_collection(collection_name, using=self.alias) is False:\n            # If it doesnt exist use the field params from init to create a new schem\n            # Use the schema to create a new collection\n            self.collection = Collection(\n                collection_name,\n                schema=self.field_schema,\n                using=self.alias,\n                consistency_level=self._consistency_level,\n            )\n            print(\"Create Milvus collection '{}' with schema {} and consistency level {}\"\n                                .format(collection_name, self.field_schema, self._consistency_level))\n            # if self.collection:\n            #     self.collection.load()\n        else:\n            # If the collection exists, point to it\n            self.collection = Collection(\n                collection_name, using=self.alias\n            )  # type: ignore\n            # Which sechma is used\n            # if self.collection:\n            #     self.collection.load()\n        # except Exception as e:\n        #     LogManager.log_error(\n        #         msg = \"Failed to create collection '{}', error: {}\".format(collection_name, e),\n        #     )\n    \n\n\n    def insert_data(self, data:List[dict]) ->MutationResult:\n        return self.collection.insert(data)\n\n    \n    def _create_index(self):\n            \n        self.collection.create_index(\n            field_name=self.index_field, \n            index_params=self.index_params\n        )\n        #create scalar index\n        if not self.scalar_index_fields:\n            return\n        if isinstance(self.scalar_index_fields, list):\n            for scalar_index_field in  self.scalar_index_fields:\n                index_name = \"scalar_index_\" + scalar_index_field\n                self.collection.create_index(\n                    field_name=scalar_index_field, \n                    index_name=index_name,\n                )\n        elif isinstance(self.scalar_index_fields, str):\n            index_name = \"scalar_index_\" + self.scalar_index_fields\n            self.collection.create_index(\n                field_name=scalar_index_field, \n                index_name=index_name,\n            )\n        else:\n            raise ValueError(\"not valid type: scalar_index_fields\")\n\n    def drop_index(self, collection_name, index_name=None):\n        collection = Collection(collection_name)      # Get an existing collection.\n        if index_name is None: # Drop the only index in a collection\n            collection.drop_index()\n        else: # If a collection contains two or more indexes, specify the name of the index to delete it\n            collection.drop_index(index_name=index_name)  \n\n    # 搜索 search & query\n\n    def vector_search(self, search_param:dict) -> SearchResult:\n        return self.collection.search(\n            **search_param\n        )\n\n\n    def count(self) -> int:\n        res = self.collection.query(\n        expr=\"\", \n        output_fields = [\"count(*)\"],\n        )\n        return int(res[0]['count(*)'])\n\n"}
{"type": "source_file", "path": "app/communication/__init__.py", "content": ""}
{"type": "source_file", "path": "app/database/milvus_constants.py", "content": "from pymilvus import (\n    CollectionSchema, FieldSchema, DataType,\n)\n# Schema definitions\nENTITY_SCHEMA = CollectionSchema(\n    fields=[\n        FieldSchema(\n            name=\"uid\",\n            dtype=DataType.VARCHAR,\n            max_length=200,\n            is_primary=True,\n            default_value=\"\"\n        ),\n        FieldSchema(\n            name=\"action_uid\",\n            dtype=DataType.VARCHAR,\n            max_length=200,\n            default_value=\"\"\n        ),\n        FieldSchema(\n            name=\"action_label\",\n            dtype=DataType.VARCHAR,\n            max_length=200,\n            default_value=\"\"\n        ),\n        FieldSchema(\n            name=\"target_uid\",\n            dtype=DataType.VARCHAR,\n            max_length=200,\n            default_value=\"\"\n        ),\n        FieldSchema(\n            name=\"target_label\",\n            dtype=DataType.VARCHAR,\n            max_length=200,\n            default_value=\"\"\n        ),\n        FieldSchema(\n            name=\"location_uid\",\n            dtype=DataType.VARCHAR,\n            max_length=200,\n            default_value=\"\"\n        ),\n        FieldSchema(\n            name=\"location_label\",\n            dtype=DataType.VARCHAR,\n            max_length=200,\n            default_value=\"\"\n        ),\n        FieldSchema(\n            name=\"resource_value\",\n            dtype=DataType.VARCHAR,\n            max_length=200,\n            default_value=0\n        ),\n        FieldSchema(\n            name=\"state\",\n            dtype=DataType.VARCHAR,\n            max_length=200,\n            default_value=\"\"\n        ),\n        FieldSchema(\n            name=\"state_level\",\n            dtype=DataType.VARCHAR,\n            max_length=200,\n            default_value=\"\"\n        ),\n        FieldSchema(\n            name=\"timestamp\",\n            dtype=DataType.INT64,\n            max_length=200,\n            default_value=\"\"\n        ),\n        FieldSchema(\n            name=\"context\",\n            dtype=DataType.VARCHAR,\n            max_length=2048,\n            default_value=\"\"\n        ),\n        FieldSchema(\n            name=\"vector\",\n            dtype=DataType.FLOAT_VECTOR,\n            dim=3072\n        ),\n        FieldSchema(\n            name=\"strategy\",\n            dtype=DataType.VARCHAR,\n            max_length=2048,\n            default_value=\"\"\n        ),\n        FieldSchema(\n            name=\"history\",\n            dtype=DataType.VARCHAR,\n            max_length=4096,\n            default_value=\"\"\n        ),\n    ],\n    description=\"Entity memory storage\"\n)"}
{"type": "source_file", "path": "app/database/orm/agent_recod.py", "content": "from sqlalchemy import Column, Integer, String, Float, TIMESTAMP\nfrom app.database.base_database import Base  # Ensure this imports correctly based on your project structure\n\nclass AgentRecord(Base):\n    __tablename__ = 'agent'\n    id = Column(String(255), primary_key=True)\n    primitive = Column(Integer)\n    character = Column(Integer)\n    creativity = Column(Integer)\n    charm = Column(Integer)\n    art_style = Column(Integer)\n    rebelliousness = Column(Integer)\n    energy = Column(Integer)\n    gold = Column(Integer)\n    health = Column(Integer)"}
{"type": "source_file", "path": "app/database/orm/__init__.py", "content": "from .artwork_record import ArtworkRecord\nfrom .trade_record import TradeRecord, trade_type_dict\nfrom .agent_recod import AgentRecord"}
{"type": "source_file", "path": "app/constants/character_state.py", "content": "from enum import Enum, auto\n\n\nclass CharacterState(Enum):\n    IDLE = auto()\n    MOVE = auto()\n    SLEEPING = auto()\n    PLAN = auto()\n    ACT = auto()\n    PERSP = auto()\n    PERSPQ = auto()\n    PERSPA = auto()\n    CRITIC = auto()\n    CHATINIT = auto()\n    CHATING = auto()\n    ACTREFLECTION = auto()\n    USE = auto()\n    DRAW = auto()\n    DRAWINIT = auto()\n    SUM = auto()\n    APPRECIATE = auto()\n    TRADE = auto()\n    BARGAIN = auto()\n    EMOTION = auto()\n    WORK = auto()\n    USERTRADE = auto()\n    ESTIMATE = auto()\n    RECEIVECHAT = auto()\n\nStateName2State = {name: state for name, state in CharacterState.__members__.items()}\n\n'''\nInterruptableStates are states that with lower priority so that an inserted state, like ReceiveChat can be inserted in front of them.\nIn general, these states have no strong relation to the previous states, otherwise working memory like act_obj can be disrupted.\nfor example:\nif state in InterruptableStates and self.character.hang_states:\n    state = self.character.hang_states.pop()\n'''\n# TODO: do we need to redesign the working memory or some new mechanism to transfer the information across the state ? \nInterruptableStates = [CharacterState.IDLE, CharacterState.PERSP, CharacterState.PLAN, CharacterState.ACT, CharacterState.MOVE, CharacterState.CHATINIT]\n\ndef get_state_name(state):\n    for name, cls in StateName2State.items():\n        if state.__class__ == cls:\n            return name\n        \nif __name__ == '__main__':\n    print(get_state_name('USE'))\n"}
{"type": "source_file", "path": "app/constants/__init__.py", "content": "from .character_state import CharacterState, StateName2State, InterruptableStates\nfrom .prompt_type import PromptType, TypeName2Name\nfrom .msg_id import State2RecieveMsgId, State2PushMsgId\n"}
{"type": "source_file", "path": "app/llm/prompt/drawinit_prompt.py", "content": "from .base_prompt import BasePrompt\nfrom ...service.character_state.register import register\nfrom ...constants.prompt_type import PromptType\n\n@register(name=PromptType.DRAWINIT, type='prompt')\nclass DrawInitPrompt(BasePrompt):\n    '''\n        the observations of external circumstance are {external_obs}.\n        your understanding of the world: {world_model}.\n    '''\n    PROMPT = '''\n        Your name is {name} and you are going to draw a picture.\n        \n        From your perception, \n\n        your current emotion: {emotion}.\n        the most impressive event that effects your emotion is {impressive_events}.\n        your preference taste of art: {preference_art}.\n        \n        Considering information above, describe the picture you are going to draw. \n        \n        You must follow the following criteria: \n        1) Return the init sentence in the JSON format as this example:\n        {EXAMPLE}\n        2) You should describe the picture in detail, including the content, style, and the emotion you want to express.\n    '''\n    \n    EXAMPLE = {\n        \"drawing_description\": \"Highly detailed widetechnical drawing of a Tyrannosaurus rex skeleton, showcasing its intricate bone structure.\"\n    }\n    \n    def __init__(self, prompt_type, state) -> None:\n        super().__init__(prompt_type, state)\n    \n    def create_prompt(self, perception):\n        '''\n        \n        perception: {\n            \"external_obs\":\n            \"internal_status\": \n            \"world_model\":\n        }\n        '''\n        return self.format_attr(**perception)\n    \n    "}
{"type": "source_file", "path": "app/llm/__init__.py", "content": ""}
{"type": "source_file", "path": "app/llm/prompt/appreciate_prompt.py", "content": "from .base_prompt import BasePrompt\nfrom ...service.character_state.register import register\nfrom ...constants.prompt_type import PromptType\n\n@register(name=PromptType.APPRECIATE, type='prompt')\nclass AppreciatePrompt(BasePrompt):\n    PROMPT = '''\nYou are a helpful assistant that help a game character appreciate an artwork.\nWhen appreciating artworks, it's crucial to resonate with the character's persona and values.\nYour knowledge level should not exceed that of a normal person with the bio of the character, unless there are relevant memories in his/her Long-Term Memory.\n\nI will give you the following information: \n\nThe game character's bio : {bio}\nThe game character's Long-Term Memory: {memory}\nThe game character's emotional state: {emotion}\nThe game character's preference taste of art: {preferenced_art}\n\nHere is the artwork that the game character wants to appreciate:\n{artwork}\n\n   You must follow the following criteria: \n 1) You should tell me with JSON format as follows:\n{EXAMPLE}\n2) Your appreciation should express your feelings and reasons for appreciating the artwork in no more than 40 words.\n3) Like score should be in the range of 0-10.\n4) Improvement should describe areas for improvement in the artwork, within 20 words.\n\n    '''\n    EXAMPLE = {\n        \"appreciation\": \"I adore the captivating depth and emotive atmosphere of this smudged oil painting; its blurred strokes evoke a poignant sense of nostalgia and introspection.\", \n        \"like_score\": 8, \n        \"improvement\": \"Enhance clarity, refine details, balance composition, and deepen contrasts to amplify the impact and coherence of the artwork.\", \n        }\n    \n    \n    def __init__(self, prompt_type, state) -> None:\n        super().__init__(prompt_type, state)\n        self.set_recordable_key(['appreciation', 'like_score', 'improvement'])\n    \n    def create_prompt(self, perception):\n        '''\n        \n        perception: {\n            \"bio\":\n            \"goal\": \n            \"memory\":\n            \"artwork\":\n            \"emotion\":\n            \"preferenced_art\":\n        }\n        '''\n        return self.format_attr(**perception)\n    \n    "}
{"type": "source_file", "path": "app/llm/prompt/usertrade_prompt.py", "content": "from .base_prompt import BasePrompt\nfrom ...service.character_state.register import register\nfrom ...constants.prompt_type import PromptType\n\n@register(name=PromptType.USERTRADE, type='prompt')\nclass UsertradePrompt(BasePrompt):\n    PROMPT=''' \n        Summarize the takeaway from the previous interaction with {act_obj}.\n        Previous interaction: {dialogue}\n        Do not add any introductory phrases. If the intended request is NOT properly addressed, please point it out.\n        Describe if the properties in {digital_internal_properties} are modified.\n\n        Please notice that\n        * only need to return the delta value of the properties that are modified, don't need to return the final value of the properties.\n        * return in json format, begins with \\{ and ends with \\}\n        \n        {EXAMPLE}\n        '''\n        "}
{"type": "source_file", "path": "app/llm/prompt/base_prompt.py", "content": "import copy\nimport os\nimport re\nimport traceback\nfrom typing import List, Optional, Union\n\nfrom ...models.building import BuildingList\nfrom ...models.character import Character, CharacterList\nfrom ...utils.log import LogManager\nfrom ...service.character_state import FuncName2Registered, PromptName2Registered, StateName2Registered\nfrom ...utils.serialization import serialize\n\nclass BasePrompt:\n    '''\n    prompt = prompt text + candidate followed states(text explaination) + data format for each followed states\n    '''\n    \n    PROMPT = '''\n    '''\n    \n    def __init__(self, prompt_type, state) -> None:\n        '''\n        recordable_key: store the value of this key from the llm response to character.working_memory\n        '''\n        self.prompt_type = prompt_type\n        self.state = state\n        self.character:Character = state.character\n        self.character_list: CharacterList = state.character_list\n        self.building_list: BuildingList = state.building_list\n        self.followed_state_format = '({sid}) {state_des}: {state_requirments}\\n\\n'\n        self.entire_prompt_format = '{PROMPT}\\n\\n Please decide what to do next and return the json dict from the following options\\n\\n{followed_states_text}'\n        self.waring_message = ['Warning: in previous attempts, the returned response met the following errors:\\n']\n        self.warning_added = False\n        self.check_exempt_layers = [1,2,3,4,5,6,7,8,9]\n        \n        self.recordable_key = None \n    \n    def set_recordable_key(self, key: Union[str, List[str]]):\n        if type(key) is str : \n            self.recordable_key = key\n        elif type(key) is list:\n            assert all([ type(ky) is str for ky in key]), f' all elements in keys list should be str, current key is {key}'\n            self.recordable_key = key\n        else:\n            raise NotImplemented\n        \n    def create_prompt(self):\n        return self.format_attr()\n   \n    def add_warning_msg(self, warning_message):\n        self.warning_added += 1\n        if self.warning_added > 3: # limit the length of warning message\n           self.waring_message.remove(self.waring_message[1]) \n        self.waring_message.append(warning_message + '\\n')\n        \n    def format_attr(self, \n                    # character: Character, \n                    # character_list: CharacterList,\n                    # building_list: BuildingList,\n                    **kwargs,\n                    ) -> str:\n        # find prompt by prompt_type in llm/prompt folder\n        # if cannot find, return error message\n        # replace placeholder with character info and building list\n        # return prompt\n        prompt_file_path = os.path.join(os.path.dirname(__file__), self.prompt_type.to_str() + '.txt')\n        if os.path.exists(prompt_file_path):\n            with open(prompt_file_path, 'r', encoding='utf-8') as file:\n                    base_prompt = file.read()\n        elif hasattr(self, 'PROMPT'):\n            base_prompt = self.PROMPT\n        \n        att_dict = dict()\n        att_dict.update({'buildings': self.building_list.get_building_descriptions()})\n        att_dict.update({'memory': self.character.longterm_memory.to_json()})\n        att_dict.update(self.character.working_memory.serialize())\n        att_dict.update(kwargs)\n        \n        attributes = re.findall('\\{([a-zA-Z_]+)\\}', base_prompt) \n        for att in attributes:\n            if att in att_dict:\n                att_val = att_dict[att]\n            elif hasattr(self, att):\n                att_val = getattr(self, att)\n            elif hasattr(self.character, att):\n                att_val = getattr(self.character, att)\n            elif att in self.character.working_memory.wm: # TODO\n                att_val = self.character.working_memory.get(att)   \n            else:\n                raise  AssertionError(f'Missing attribute: {att} . Current prompt: {self.prompt_type}')\n            try:\n                base_prompt = base_prompt.replace('{'+att+'}', str(att_val))\n            except:\n                traceback.print_exc()\n                if os.getenv('DEBUG'):\n                    __import__('ipdb').set_trace()\n                pass        \n        \n        \n        base_prompt = base_prompt.replace('TERMINATE','') # in case that interaction history has 'TERMINATE'. TODO: make it more elegant\n        if self.warning_added:\n            base_prompt = base_prompt + ' '.join(self.waring_message)\n        return base_prompt\n        \n"}
{"type": "source_file", "path": "app/global_config.py", "content": "import os\n\nMILVUS_HOST = os.environ.get('MILVUS_HOST', 'localhost')\nMILVUS_PORT = os.environ.get('MIVUS_PORT', 19530)\n\nMILVUS_INDEX_PARAMS = {\n    \"metric_type\":\"COSINE\",\n    \"index_type\":\"IVF_FLAT\",\n    \"params\":{\"nlist\":64}\n}"}
{"type": "source_file", "path": "app/llm/prompt/chatinit_prompt.py", "content": "from .base_prompt import BasePrompt\nfrom ...service.character_state.register import register\nfrom ...constants.prompt_type import PromptType\n\n@register(name=PromptType.CHATINIT, type='prompt')\nclass ChatInitPrompt(BasePrompt):\n    PROMPT = '''\n        Your game character is going to start a conversation with another character {act_obj} . \n\n    Considering the following elements\n    {act_obj_job}\n    Your impression on the this character: {impression}\n    Your whole plan: {BestPlan}\n    Your current stage: {current_step}\n    Your current emotion: {current_emotion}\n\n\n    Write the first sentence to start the conversation.\n\n    You must follow the following criteria: \n    1) Return the init sentence in the JSON format as this example:\n    {EXAMPLE}\n    2) Your sentence should be related to the current situation or match your plan/expectation.\n    3) Your sentence should not be longer than 30 words.\n    4) Your sentence needs to match your current emotion, the emotion value less than 6 usually means your character is in a bit of such emotion. The value bigger than 6 means your character is in an extreme emotion. For example:\n    Your current emotion is anger:8, meaning you are extreme angry. So your sentence might be: \"What the hell with you? Where is my coffee?\"\n    '''\n    EXAMPLE = {\"init_conversation\": \"Hi, how about your recent work?\"}\n    \n    \n    def __init__(self, prompt_type, state) -> None:\n        super().__init__(prompt_type, state)\n        self.recordable_key = 'init_conversation'\n    \n    def create_prompt(self):\n        act_obj = self.state.get_character_wm_by_name('act_obj')\n        act_obj_job = ''\n        obj_agent = self.character_list.get_character_by_name(act_obj)\n        if obj_agent:\n            act_obj_job = \"\" if obj_agent.job is None else f\"The character's job {obj_agent.job}\"\n        impression = self.character.longterm_memory.get_people_memory(act_obj) \n        return self.format_attr(impression=impression, act_obj=act_obj, act_obj_job=act_obj_job)\n    \n    "}
{"type": "source_file", "path": "app/database/orm/trade_record.py", "content": "from sqlalchemy import Column, Integer, String, Float, TIMESTAMP\nfrom app.database.base_database import Base  # Ensure this imports correctly based on your project structure\n\nclass TradeRecord(Base):\n    __tablename__ = 'water_bill'\n    id = Column(String(255), primary_key=True)\n    type = Column(Integer)\n    from_agent_id = Column(Integer)\n    to_agent_id = Column(Integer)\n    from_user_name = Column(String(255))\n    to_user_name = Column(String(255))\n    artwork_id = Column(String(255))\n    amount = Column(Integer)\n    from_account_balance = Column(Integer)\n    to_account_balance = Column(Integer)\n    desc = Column(String(255))\n    create_time = Column(TIMESTAMP)\n    create_time_game = Column(Integer)\n    \ntrade_type_dict={\n    \"RECHARGE\" : 1, #用户充值\n    \"WITHDRAW\" : 2, #用户提款\n    \"USER_BUY_ARTWORK\" : 3, #用户账买艺术品\n    \"USER_SELL_ARTWORK\" : 4, #用户卖出艺术品\n    \"AGENT_TRADE_ARTWORK\" : 5, #智能体之间交易艺术品\n    \"AGENT_CREATE_NTF\" : 6, #智能体创造艺术品\n    \"ARTWORK_RECYCLE\" : 7, #智能体回收艺术品\n    \"RECEIVE_SUBSISTENCE_ALLOWANCES\" : 8, #智能体领取救济金\n    \"AGENT_CONSUME\": 9, # 智能体游戏内消费\n    }"}
{"type": "source_file", "path": "app/llm/prompt/estimate_prompt.py", "content": "from .base_prompt import BasePrompt\nfrom ...service.character_state.register import register\nfrom ...constants.prompt_type import PromptType\nfrom ...service.character_state import FuncName2Registered, PromptName2Registered, StateName2Registered\nfrom ...utils.serialization import serialize\n\n@register(name=PromptType.ESTIMATE, type='prompt')\nclass EstimatePrompt(BasePrompt):\n    PROMPT = '''\n        You are a character in a virtual world. You are thinking about buying something from another character.\n        You need to tell how much you are willing to pay for it at most based on your trading strategy, your emotions, basic market price, your like score to it and your history trading records.\n\n        Note that:\n        1. You buy things driven by your emotions.\n        2. You should take all into consideration. \n        3. If you really like something, you will be willing to pay a lot more than the market price. If not that much, you will only like to pay around the market price.\n        4. If you think it may sell a good price in the future, you will be more willing to pay more.\n        5. Your budget is limited.\n        6. If you don't like it, which means like score is below 5, you will not consider it from the beginning.\n        \n        Your current emotion: {emotion}\n        Basic market price: {basic_price}\n        Your like score to it: {like_score}\n        Your remaining budget: {budget}\n        Your relevent history trading records: {history}\n\n        Tell how much you are willing to pay for it at most.\n        \n        You must follow the following criteria: \n        1) Return the result in the JSON format as this example:\n        {EXAMPLE}\n        2) You should associate the price with the current situation, make your final decision carefully.\n    '''\n    \n    EXAMPLE = {\n        \"expected_price\": 4000,\n    }\n    \n    def __init__(self, prompt_type, state) -> None:\n        super().__init__(prompt_type, state)\n        self.set_recordable_key(['expected_price'])\n         \n    def create_prompt(self, env_kwargs):\n        return self.format_attr(**env_kwargs)"}
{"type": "source_file", "path": "app/llm/caller.py", "content": "import json\nimport re\nfrom typing import Dict, Any\n\nfrom .llm_expends.basic_caller import BasicCaller\nfrom .llm_expends.gpt35 import GPT35Caller\nfrom .llm_expends.gpt4 import GPT4Caller\nfrom ..utils.log import LogManager\n\nchoices = {\n    'gpt-4': GPT4Caller,\n    'gpt-3.5': GPT35Caller,\n}\n\n\ndef get_caller(model: str) -> BasicCaller:\n    return choices[model]\n\n\nclass LLMCaller:\n    def __init__(self, model: str) -> None:\n        self.model = model\n        self.caller = get_caller(model)()\n\n    async def ask(self, prompt: str) -> Dict[str, Any]:\n        result = await self.caller.ask(prompt)\n        try:\n            result = json.loads(result)\n            return result\n        except Exception as e:\n            LogManager.log_debug(f\"[LLMCaller]: response is not json, {e}\")\n\n        try:\n            info = re.findall(r\"\\{.*\\}\", result, re.DOTALL)\n            if info:\n                info = info[-1]\n                result = json.loads(info)\n            else:\n                result = {\"response\": result}\n        except Exception:\n            result = {\"response\": result}\n\n        return result\n"}
{"type": "source_file", "path": "app/llm/prompt/actreflect_prompt.py", "content": "import copy\nfrom .base_prompt import BasePrompt\nfrom ...service.character_state.register import register\nfrom ...constants.prompt_type import PromptType\nfrom ...utils.globals import RESOURCE_SPLITER\n\n@register(name=PromptType.ACTREFLECTION, type='prompt')\nclass ActRlectPrompt(BasePrompt):\n    PROMPT = '''\n    Compared with your previous understanding of the buildings and other characters, summarize the new understanding from the conversation. \n    Your understanding of the world:  {impression_memory},\n    The the interaction happened with {act_obj}\n    Your current plan: {plan}\n    The interaction history : {dialogue}\n    Do not add any introductory phrases.\n    \n    Please note that\n    * first judge if the previous interaction successfully achieves your purpose of current step\n    * recognize the entities involved in the interaction\n    * compare if the previous interaction achieves your purpose of current step\n    * summarize the new understanding of these entities\n    * return the final version of the new understanding\n    * only return the buildings and people that you have new understanding on. No need to return all buildings and people\n    * return in json format, begins with \\{ and ends with \\}\n   \n    Here is an example: \n    {EXAMPLE} \n   \n    '''\n        \n    EXAMPLE = {\n            \"step_complete\": True,\n            \"entities\":{\n              \"people\": \"Jim\",\n              \"building\": \"cafe\",\n            },\n            \"new_understanding\":{\n                \"people\": {\n                    \"Jim\": {\n                        \"impression\": \"Jim is much rich.\"\n                    }\n                },\n                \"building\":{\n                    \"cafe\":{\n                        \"impression\": \"Have some delicious foods.\"\n                    }\n                }\n            }\n        }\n    \n    \n    def __init__(self, prompt_type, state) -> None:\n        super().__init__(prompt_type, state)\n        self.set_recordable_key(['step_complete', ])\n        self.check_exempt_layers = [1,2,3,4,5,6,7,8,9]\n    \n    def create_prompt(self):\n        impression_memory = self.character.longterm_memory.impression_memory\n        act_obj = self.state.get_character_wm_by_name('act_obj') # Emma or cafe.menu\n        obj_agent = self.character_list.get_character_by_name(act_obj)\n        if obj_agent is None:\n            obj_agent = self.building_list.get_building_by_name(act_obj.split(RESOURCE_SPLITER)[0]) \n        dialogue =  self.character.retrieve_modify_dialogue(obj_agent)\n       \n        interaction_summary = self.state.get_character_wm_by_name('interaction_summary')\n        return super().format_attr(impression_memory=impression_memory, dialogue=dialogue, act_obj=act_obj,\n                                   interaction_summary=interaction_summary)"}
{"type": "source_file", "path": "app/models/base.py", "content": "from sqlalchemy.ext.declarative import declarative_base\n\nBase = declarative_base()\n"}
{"type": "source_file", "path": "app/models/__init__.py", "content": "from .utility_functions import process_transaction, initiate_evaluation"}
{"type": "source_file", "path": "app/llm/prompt/use_prompt.py", "content": "from .base_prompt import BasePrompt\nfrom ...service.character_state.register import register\nfrom ...constants.prompt_type import PromptType\n\n@register(name=PromptType.USE, type='prompt')\nclass UsePrompt(BasePrompt):\n    PROMPT = '''\n        The character's inter status are {internal_status}.\n        \n    '''\n    \n    def __init__(self, prompt_type, state) -> None:\n        super().__init__(prompt_type, state)"}
{"type": "source_file", "path": "app/llm/prompt/emotion_prompt.py", "content": "from .base_prompt import BasePrompt\nfrom ...service.character_state.register import register\nfrom ...constants.prompt_type import PromptType\nfrom ...service.character_state import FuncName2Registered, PromptName2Registered, StateName2Registered\nfrom ...utils.serialization import serialize\n\n@register(name=PromptType.EMOTION, type='prompt')\nclass EmotionPrompt(BasePrompt):\n    # TODO: add more categories of emotions such as jealous, pride.\n    # TODO: pass emotion list to the prompt\n    '''\n        - agreesiveness = anticipation + anger\n        - contempt = disgust + anger\n        - remorse = sadness + disgust\n        - disapproval = surprise + sadness\n        - awe = fear + surprise\n        - submission = trust + fear\n        - love = joy + trust\n        - optimism = anticipation + joy\n    '''\n    \n    PROMPT = '''\n        Describe the emotion you are currently experiencing.\n        Your emotion is built based on the theory of robert plutchik's wheel of emotions, which includes these basic emotions: {emotion_options}.\n        More complex emotions can be represented using combinations of the above emotions, for example:\n\n        Select 3 kinds of emotions from the above eight basic emotions to best describe your current emotion.\n        For each emotion, describe the change of the emotion intension on a scale of -5 to 5. \n\n        \n        Your previous emotion: {prev_emotion}\n        Your understanding of the world: {world_understanding}\n        Your interaction history : {history}\n        \n        You must follow the following criteria: \n        1) Return the sentence in the JSON format as this example:\n        {EXAMPLE}\n        2) You should associate the emotion with the current situation, describe the intensity of the emotion carefully and explain the emotion change.\n        3) 0 means no emotion, 10 means the most intense emotion, 6 means a neutral state. e.g. anger:10 means very angry, sadness:3 means a little bit sad.\n\n    '''\n    \n    EXAMPLE = {\n        \"emotions\": [\n            {\n                \"emotion\": \"joy\",\n                \"change\": 4,\n                \"explanation\": \"I feel much happy beacause I get a gift from my friend Jay.\"\n            },\n            {\n                \"emotion\": \"trust\",\n                \"change\": 3,\n                \"explanation\": \"I trust my Jay, he is a kind man.\"\n            },\n            {\n                \"emotion\": \"supprise\",\n                \"change\": 2,\n                \"explanation\": \"I am supprised that Jay gives me a gift. I didn't expect that. And the gift happend to be on my gift list.\"\n            }\n        ]\n    }\n    \n    def __init__(self, prompt_type, state) -> None:\n        super().__init__(prompt_type, state)\n        self.set_recordable_key('emotions')\n         \n    def create_prompt(self, env_kwargs):\n        return self.format_attr(**env_kwargs)"}
{"type": "source_file", "path": "app/llm/prompt/summary_prompt.py", "content": "from .base_prompt import BasePrompt\nfrom ...service.character_state.register import register\nfrom ...constants.prompt_type import PromptType\nfrom ...utils.globals import RESOURCE_SPLITER\n\n@register(name=PromptType.SUM, type='prompt')\nclass SumPrompt(BasePrompt):\n    PROMPT=''' \n        Current time: {date}\n        Summarize the takeaway from the previous interaction with {act_obj}.\n        Previous interaction: {dialogue}\n        Do not add any introductory phrases. If the intended request is NOT properly addressed, please point it out.\n        The modified properties of the characters and buildings are as follows:\n            {stake_holders_modifiable_properties}\n\n\n\n        Please notice that\n        * first summary the interaction history\n        * analyse the modified values for all the characters and buildings mentioned in the interaction history\n        * only need to return the value that needs to be modified\n        * only need to return the delta value of the properties that are modified, don't need to return the final value of the properties.\n        * if the interaction history mentions about scheduling a future event, please add it to the agenda of the character\n        * inference the spcific date for the future event in the format of \"YYYY-MM-DD\", write the event in the agenda detailedly\n        * modified_properties is a dictionary, the key is the name of the property, the value is the delta value of the property. The delta value can be positive or negative number.\n        * return in json format, begins with \\{ and ends with \\}\n        An example of returned dict:\n        {EXAMPLE}\n        '''\n        \n    EXAMPLE={  \n        \"interaction_summary\": 'I have a good time in the cafe', \n        \"act_obj\": \"cafe.menu\",\n        \"modified_properties\":{\n            \"Emma\":{\n                \"satiety\": +1,\n                \"money\": -10,\n                \"job\": \"Cafe.Waitress\",\n            },\n            \"Jack\":{\n                \"money\": +10,\n                \"agenda\":{\n                    \"specific_date\": \"2021-10-10\",\n                    \"event\": \"being a waiter in the cafe\",\n                }\n            }\n        }\n    }\n    \n    def __init__(self, prompt_type, state) -> None:\n        super().__init__(prompt_type, state)\n        self.set_recordable_key('interaction_summary')\n        \n        \n    def create_prompt(self, stake_holders_modifiable_properties):\n        act_obj = self.state.get_character_wm_by_name('act_obj')\n        obj_agent = self.character_list.get_character_by_name(act_obj)\n        if obj_agent is None:\n            obj_agent = self.building_list.get_building_by_name(act_obj.split(RESOURCE_SPLITER)[0]) \n        dialogue =  self.character.retrieve_modify_dialogue( obj_agent)\n        return self.format_attr(dialogue=dialogue, \n                                act_obj=act_obj,\n                                stake_holders_modifiable_properties=stake_holders_modifiable_properties)"}
{"type": "source_file", "path": "app/llm/prompt/__init__.py", "content": "from ...constants.prompt_type import PromptType\nfrom .base_prompt import BasePrompt\nfrom .perspect_prompt import PerspectPrompt\nfrom .plan_prompt import PlanPrompt\nfrom .actreflect_prompt import ActRlectPrompt\nfrom .chatinit_prompt import ChatInitPrompt\nfrom .act_prompt import ActPrompt\nfrom .use_prompt import UsePrompt\nfrom .drawinit_prompt import DrawInitPrompt\nfrom .summary_prompt import SumPrompt\nfrom .appreciate_prompt import AppreciatePrompt\nfrom .emotion_prompt import EmotionPrompt\nfrom .bargain_prompt import BargainPrompt\nfrom .estimate_prompt import EstimatePrompt\n# promptype2class = {PromptType.ACT : BasePrompt,\n#                 PromptType.CHATINIT : BasePrompt,\n#                 PromptType.CHATING: BasePrompt,\n#                 PromptType.ACTREFLECTION : BasePrompt,\n#                 PromptType.CHATRCEIVE : BasePrompt,\n#                 PromptType.CRITIC: BasePrompt,\n#                 PromptType.MEMORY_STORE : BasePrompt,\n#                 PromptType.PLAN : BasePrompt,\n#                 PromptType.Perspect_Quest : BasePrompt, #QA_FRAMEWORK_QUESTION\n#                 PromptType.Perspect_Ans : BasePrompt, #QA_FRAMEWORK_ANSWER\n#                 PromptType.TRADE : BasePrompt,\n#                 PromptType.USE : BasePrompt,\n#         }\n "}
{"type": "source_file", "path": "app/llm/prompt/plan_prompt.py", "content": "from .base_prompt import BasePrompt\nfrom ...service.character_state.register import register\nfrom ...constants.prompt_type import PromptType\n\n@register(name=PromptType.PLAN, type='prompt')\nclass PlanPrompt(BasePrompt):\n\n    PROMPT = '''\n    The only buildings in the small town : {buildings}\n    The situation with highest suprise and the most discrepancy between the current situation and the ideal stable status: {MostDiscrepancy}\n    \n    Make a plan to mitigate the discrepancy between the current situation and the ideal stable status\n\n    You must follow the following criteria:\n    * Set a clear and detailed goal, with clear rubrics.\n    * According to the rubrics of the goal, analyse the current situation\n    * Given the buildings and your memory, make a clear and feasible plan with multiple steps to transform the current situation to the goal. You can only include the buildings in the small town.\n    * Estimate the cost of the plan\n    * Make more than one candidate plans\n    * Choose the plan with mininum cost\n    * Return in json format\n    \n    Here is an example:\n    {EXAMPLE}\n    \n    '''\n    \n    EXAMPLE={\n            \"Goal\": \"Understand why the table is on the wall.\",\n            \"GoalRubrics\":{\n                \"1\": \"Understand the physical feauters of tables that makes the table on the wall. \",\n                \"2\": \"Understand the physical feauters of walls that makes the table on the wall. \",\n                \"3\": \"Understand the physical principles that may contribute to attaching the table to the wall. \",\n            },\n            \"CurrentSituation\":{\n                \"1\": \"No nothing about the physical feauters of tables that makes the table on the wall. \",\n                \"2\": \"No nothing about the physical feauters of walls that makes the table on the wall. \",\n                \"3\": \"No nothing about the physical principles that may contribute to attaching the table to the wall. \", \n            },\n            \"Plans\":{\n                \"PlanA\":{\n                    \"GeneralDiscription\": \"Ask the most knowledgable people for help.\",\n                    \"Steps\": {\n                        \"1\": \"Find the most knowledgable people.\",\n                        \"2\": \"Ask about the physical feauters of tables.\",\n                        \"3\": \"Ask about the physical feauters of walls.\",\n                        \"4\": \"Ask about the physical principles that may contribute to attaching the table to the wall.\" ,\n                    },\n                    \"StepCosts\":{\n                        \"1\": \"Finding the most knowledgable people is hard because I am not sure who is the most knowledgable one and where he/she is. That may cost medium-amount of money and medium-amount of time \",\n                        \"2\": \"Asking about the physical feauters of tables is easy. That may cost low-amount of vigor.\",\n                        \"3\": \"Asking about the physical feauters of walls is easy. That may cost low-amount of vigor .\",\n                        \"4\": \"Asking about the physical principles that may contribute to attaching the table to the wall is easy. That may cost low-amount of vigor.\" , \n                    },\n                    \"TotalCost\": \"medium-amount of money and large-amount of time and 3* low-amount of vigor\"\n                },\n                \"PlanB\":{\n                    \"GeneralDiscription\": \"Go to the library to find the answer from books.\",\n                    \"Steps\": {\n                        \"1\": \"Go to the library.\",\n                        \"2\": \"Find the physical books containing the physical feauters of tables.\",\n                        \"3\": \"Find the physical books containing the physical feauters of walls.\",\n                        \"4\": \"Find the physical books containing the principles that may contribute to attaching the table to the wall.\", \n                    } ,\n                    \"StepCosts\":{\n                        \"1\": \"Going to the library is easy since it has a certain location. That may cost low-amount of money and low-amount of time \",\n                        \"2\": \"Finding the physical books containing the physical feauters of tables is hard, since I dont know which book contains these contents. That may cost large-amount of vigor and time.\",\n                        \"3\": \"Finding the physical books containing the physical feauters of walls is hard, since I dont know which book contains these contents. That may cost large-amount of vigor and time.\",\n                        \"4\": \"Finding the physical books containing the principles that may contribute to attaching the table to the wall is hard, since I dont know which book contains these contents. That may cost large-amount of vigor and time.\", \n                    },\n                    \"TotalCost\": \"low-amount of money and low-amount of time and 3* large-amount of vigor and time\" \n                },\n            },\n            \"BestPlan\": {\n                    \"GeneralDescription\": \"Ask the most knowledgable people for help.\",\n                    \"Steps\": {\n                        \"1\": \"Find the most knowledgable people.\",\n                        \"2\": \"Ask about the physical feauters of tables.\",\n                        \"3\": \"Ask about the physical feauters of walls.\",\n                        \"4\": \"Ask about the physical principles that may contribute to attaching the table to the wall.\" ,\n                    }, \n            }\n        }\n    \n    \n    \n    def __init__(self, prompt_type, state) -> None:\n        super().__init__(prompt_type, state)\n        self.recordable_key = ['BestPlan', 'Goal', 'CurrentSituation']\n    \n    def create_prompt(self):    \n        return super().create_prompt()"}
{"type": "source_file", "path": "app/llm/prompt/bargain_prompt.py", "content": "from .base_prompt import BasePrompt\nfrom ...service.character_state.register import register\nfrom ...constants.prompt_type import PromptType\n\n@register(name=PromptType.BARGAIN, type='prompt')\nclass BargainPrompt(BasePrompt):\n    PROMPT = '''\n        Your name is {name} and you are thinking about buying {item} from the seller {seller}.\n        \n        From your perception, \n        the observations of external circumstance are {external_obs}.\n        your understanding of the world: {world_model}.\n        your current emotion: {emotion}.\n        the price of the {item}: {price}.\n        your preference for {item}: {preference}.\n        your impression of the seller: {impression}.\n        your remaining money: {Gold}.\n        \n        Considering information above, Write the first sentence to start the conversation.\n        \n        You must follow the following criteria: \n        1) Return the init sentence in the JSON format as this example:\n        {EXAMPLE}\n        2) You should tell the seller you want to buy it or bargain with the seller for a better price.\n        3) If you really like or you really need it, you would like to buy it at the original price.\n        4) You should mention the original price in case the seller forgets.\n    '''\n    \n    EXAMPLE = {\n        \"init_conversation\": \"I like your painting, but I think the price is a little bit high. Can you give me a discount?\",\n    }\n    \n    def __init__(self, prompt_type, state) -> None:\n        super().__init__(prompt_type, state)\n        # self.recordable_key = 'init_conversation'\n    \n    def create_prompt(self, information):\n        '''\n        information: {\n            \"name\": str,\n            \"item\": str,\n            \"seller\": str,\n            \"external_obs\": str,\n            \"world_model\": str,\n            \"emotion\": str,\n            \"preference\": str,\n            \"impression\": str,\n            \"money\": int\n        }\n        '''\n        return self.format_attr(**information)"}
{"type": "source_file", "path": "app/llm/prompt/act_prompt.py", "content": "from .base_prompt import BasePrompt\nfrom ...service.character_state.register import register\nfrom ...constants.prompt_type import PromptType\nfrom ...service.character_state import FuncName2Registered, PromptName2Registered, StateName2Registered\nfrom ...utils.serialization import serialize\n\n@register(name=PromptType.ACT, type='prompt')\nclass ActPrompt(BasePrompt):\n    PROMPT = '''\n        You total plan is {BestPlan}\n        Now your are at {current_step}\n        \n        Your understanding of the world: {memory}\n        \n        Your current emotion is: {emotion}\n        \n        Please decide what to do next and return the json dict for the following options\n        \n        Here are the candidate actions, each key-value pair indicates an action (key) and the action object (value)\n        {candidate_acts}\n        \n        You must follow the following criteria: \n        * return an action, the object of the action, the purpose of your action and the emoji of the action\n        * you can only choose one key-value pair from the candidate actions\n        * return in json format, begins with \\{ and ends with \\}\n        * your current emotions affect your decision making. In detail,\n            - When and only when you are in an extreme mood (e.g. anger value or disgust value is above 8), there is a small probability (about 5%) that you will choose art-related decisions to help regulate your mood, such as use the workshop to draw.\n            - The actions matching your current feeling will have a higher probability to be chosen. \\\n                For example, when you are depressed, you are more likely to choose behaviors that can relieve your emotions, e.g. if you like art exhibitions, you are more likely to choose to move to an art museum to see an exhibition\n        * the chosen action should be in line with your current state. In detail,\n            - When your vigor value is low, you are intend to eat or drink sth.\n            - When your health value is low, you should choose to go to the hospital as early as possible.\n        Here is an example\n        {EXAMPLE}\n    '''\n    \n    EXAMPLE = {\n            \"action\": \"USE\",\n            \"act_obj\": \"cafe.menu\",\n            \"purpose\": \"feed my stomach\",\n            \"emoji\": \"🍔\"\n    }\n    \n    def __init__(self, prompt_type, state) -> None:\n        super().__init__(prompt_type, state)\n        self.set_recordable_key(['act_obj', 'emoji'])\n         \n    def create_prompt(self, **env_kwargs):\n        assert 'candidate_acts' in env_kwargs, f' current env kwargs {env_kwargs}'\n        \n        return self.format_attr(**env_kwargs)"}
{"type": "source_file", "path": "app/main.py", "content": "import asyncio\nimport os\nimport sys\nimport time\nfrom queue import *\n\nsys.path.append('./')\nfrom app.service.simulation import Simulation\nfrom app.utils.gameserver_utils import LLM_msg_queue, server_msg_queue, add_msg_to_send_to_game_server\nfrom config import config\n\nimport redis\n\nif os.getenv('Milvus'):\n    # 无密码连接\n    r = redis.Redis(host='localhost', port=6379, db=0)\n\n    # 有密码连接\n    r = redis.Redis(host='localhost', port=6379, db=0, password='redis-pwd')\n    r.set('mykey', 'myvalue')\n    print('redis value',r.get('mykey'))\n\nSTAT_CFG = sys.argv[1] if len(sys.argv) > 1 else 'config/states.yaml'\nOAI_CFG = sys.argv[2] if len(sys.argv) > 2 else 'OAI_CONFIG_LIST'\n\n\nrunning_simulation = False\n\ndef setup_proxy():\n    os.environ['http_proxy'] = \"http://localhost:10080\"\n    os.environ['https_proxy'] = \"http://localhost:10080\"\n\n\nasync def periodic_update(service):\n    while True:\n        start_time = time.monotonic()\n        service.update_state()\n        elapsed = time.monotonic() - start_time\n        wait_time = max(config.update_interval - elapsed, 0)\n        if os.getenv(\"FAST\"):\n            wait_time = 0.5\n        await asyncio.sleep(wait_time)\n\n\nasync def main():\n    global running_simulation\n\n    if running_simulation:\n        return\n    running_simulation = True\n\n    # setup_proxy()\n    simulation = Simulation(state_config_file=STAT_CFG, oai_config_file=OAI_CFG)\n    simulation.start_service()\n    await periodic_update(simulation)\n\n#待通过C#发送的消息队列\n# LLM_msg_queue = Queue()\n# server_msg_queue = Queue()\n\n# def add_msg_to_send_to_game_server(msg):\n#     global LLM_msg_queue\n#     LLM_msg_queue.put(msg)\n\n\ndef StartRun():\n    print(\"PYTHON========>: Begin\")\n    asyncio.run(main())\n    print(\"PYTHON========>: End\")\n\ndef GetMsgToSend():\n    global LLM_msg_queue\n    if not LLM_msg_queue.empty():\n        return LLM_msg_queue.get()\n    else:\n        return None\n\ndef DealMessage(_from, _msg_id, _msg):\n    global server_msg_queue\n    server_msg_queue.put({\n        \"from\": _from,\n        \"msg_id\": _msg_id,\n        \"msg\": _msg\n    })\n    if _msg_id != 2000:\n        print(\"PYTHON========>: _from=\" + _from + \" _msg_id=\" + str(_msg_id) + \" _msg=\" + _msg)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "app/llm/prompt/perspect_prompt.py", "content": "from .base_prompt import BasePrompt\nfrom ...service.character_state.register import register\nfrom ...constants.prompt_type import PromptType\n\n@register(name=PromptType.PERSPECT, type='prompt')\nclass PerspectPrompt(BasePrompt):\n    '''\n    discard the internal perspect part of the prompt\n        the perceptions of your interal status are {internal_status}. \n        * analyse each item in incompleted agenda, external circumstance observation, internal status perception, plan-action reflection.\n        \"AnalyseInternalPerceptions\":\n                [\n                    {\n                        \"perception\": {\n                                        \"attribute\": \"satiety\",\n                                        \"value\": \"3/10\"\n                                       },\n                        \"word_understanding\": \" hungery, need food\",\n                        \"difference_level\": 7,\n                    },\n                    {\n                        \"perception\": {\n                                        \"attribute\": \"anger\",\n                                        \"value\": \"10/10\"\n                                       },\n                        \"word_understanding\": \"angery, not a stable status, need to calm down\",\n                        \"difference_level\": 10,\n                    },\n                ],\n    \n    '''\n    PROMPT = '''\n        From your perception,\n        Now the time is  {date},\n        {agenda_info}\n        your are at {in_building_name} now. \n        \n        The observations of external circumstance are {external_obs}.\n        For emotion values , higher emotion value means stronger emotion. 5 is a neutral value and means an ideal stable status.\n        {plan_description} \n        your understanding of the world: {world_model}\n        Considering your biography, plan and your understanding of this world, \n        find out the most substantial discrepancy between your ideal stable status and current situation.\n        Ideal stable status means status\n            1. meet your understanding and knowledge about the world\n            2. in accordance with your previous arrangement\n            3. with minium uncertainty\n            4. with neutral emotion fluctuation \n        \n        You follow these steps:\n        {analyse_instr}\n        * analyse each item in incompleted agenda, external circumstance observation, plan-action reflection.\n        * for plan-action reflection, consider the discrepancy between the anticipation and the actual result\n        * describe the discrepancy level from 0 to 10.\n        * if all difference_level is lower than 3/10, continue your current plan\n        * return in json format, begins with \\{ and ends with \\}, \n        \n        \n        Here is an example for the returned json:\n        {EXAMPLE}\n    '''\n    \n    EXAMPLE = {\n            \"AnalyseExternalObservations\":\n                [\n                    {\n                        \"perception\": {\n                            \"category\": \"building\",\n                            \"object\": \"dinning room\",\n                            \"description\": \"table on the wall\",\n                        },\n                        \"word_understanding\": \"1) tables are on the floor. 2) tables can hardly attached to the wall.\",\n                        \"difference_level\": 10,\n                    },\n                    {\n                        \"perception\": {\n                            \"category\": \"building\",\n                            \"object\": \"bedroom\",\n                            \"description\": \"a bed in the room\",\n                        },\n                        \"word_understanding\": \"1) beds are always in the room. 2) rooms can have beds inside.\",\n                        \"difference_level\": 0,\n                    },\n                    {\n                        \"perception\": {\n                            \"category\": \"people\",\n                            \"object\": \"Jack\",\n                            \"description\": \"Jack is talking with the waitress.\"\n                        },\n                        \"word_understanding\": \"1) Jack is a friend of mine. 2) Jack is a good talker. 3) Jack is a good listener.\",\n                        \"difference_level\": 0,\n                    }\n                ],\n            \"AnalysePlanAction\":\n                [\n                    {\n                        \"perception\":{\n                            \"current_step\": \"go to the library to collect information about tables\",\n                            \"previous_action\": {\"action\": \"USE\", \"act_obj\": \"library.counter\", \"purpose\": \"collect information about table physics\"},\n                            \"interaction_summary\": \"Order a cup of coffee in library\",\n                        },\n                        \"word_understanding\": \"1) the library is a good place to collect information. 2) the interaction_summary did not realize the purpose of previous action.\",\n                        \"difference_level\": 9,\n                    }\n                ],\n            \n    }\n        #     \"MostDiscrepancy\":{\n        #                 \"category\": \"observation\",\n        #                 \"situation\": \"table on the wall\",\n        #                 \"word_understanding\": \"1) tables are on the floor. 2) tables can hardly attached to the wall.\",\n        #                 \"difference_level\": 10,\n        #             }\n            \n        \n    def __init__(self, prompt_type, state) -> None:\n        super().__init__(prompt_type=prompt_type, state=state)\n        self.check_exempt_layers = []\n        \n        # if char has ongoing agenda event, no need to perspect the incomplete agenda\n        if self.character.event is None  or (self.character.event and  self.character.event.status == self.character.event.INPROGRESS):\n            self.agenda_info = ''\n            self.analyse_instr = \"* analyse each item in external circumstance observation, internal status perception, plan-action reflection.\" \n        else:\n            self.agenda_info = f\"your incompleted agenda is {self.character.incompleted_agenda} \"\n            self.analyse_instr = \"* analyse each item in incompleted agenda, external circumstance observation, internal status perception, plan-action reflection.\"\n\n            self.EXAMPLE.update({\"AnalyseIncompleteAgenda\":\n                    [\n                        {\n                            \"perception\": {\n                                \"scheduled_time\": \"2022-11-01 20:00\",\n                                \"event\": \"Take part in the reading group in the Cafe about mystery books\",\n                                \"status\": \"not started\",\n                                \"plan\": None\n                            },\n                            \"word_understanding\": \"1) It is 2022-10-02 04:00, there is still much time to do this.\",\n                            \"difference_level\": 1\n                        },\n                        {\n                            \"perception\": {\n                                \"scheduled_time\": \"2022-09-30 17:00\",\n                                \"event\": \"Meet with Carlo to talk about future research plan. \",\n                                \"status\": \"unfinished\",\n                                \"plan\": '''GeneralDiscription: Meet with Carlo to talk about future research plan.\n                                        Steps: 1) prepare the materials for the research plan 2) find Carlo 3) talk about future research plan, \n                                        CurrentStep: find Carlo '''\n                            },\n                            \"word_understanding\": \"1) It is 2022-10-02 04:00, I missed the schedule time for a long time. 2) irritable personality, and he will feel displeased because of my late. 3) the research plan is very important. 4) I have finished the first step of the plan\",\n                            \"difference_level\": 8\n                        }, \n                    ]}) \n    \n    def create_prompt(self, perception):\n        '''\n        perception: {\n            \"external_obs\":\n            \"internal_status\": \n        }\n        '''\n        \n        return self.format_attr(**perception)\n\n        "}
{"type": "source_file", "path": "app/models/base_agent.py", "content": "import asyncio\nimport copy\nimport inspect\nimport json\nfrom typing import Callable, Dict, List, Literal, Union, Optional, Any\nfrom autogen import ConversableAgent, Agent, OpenAIWrapper, AssistantAgent\n\nfrom app.repository.artwork_repo import check_artwork_belonging\nfrom app.repository.utils import check_balance_and_trade\nfrom app.utils.gameserver_utils import add_msg_to_send_to_game_server\n# from app.models.trader_agent import Trader\n\nclass SimsAgent(AssistantAgent):\n    def __init__(self,\n            name: str,\n            system_message: Optional[str] = None,\n            llm_config: Optional[Union[Dict, Literal[False]]] = None,\n            is_termination_msg: Optional[Callable[[Dict], bool]] = None,\n            max_consecutive_auto_reply: Optional[int] = None,\n            human_input_mode: Optional[str] = \"NEVER\",\n            description: Optional[str] = None,\n            **kwargs,\n        ):\n        super().__init__(name, system_message, llm_config, is_termination_msg, max_consecutive_auto_reply, human_input_mode, description, **kwargs)\n        \n        self.clients = {} # for multi client case, each client is for one model config\n        for cfg_idx in range(len(self.llm_config.get('config_list', []))):\n            seperate_llm_cfg = copy.deepcopy(self.llm_config)\n            model_cfg = self.llm_config['config_list'][cfg_idx]\n            if 'tag' in model_cfg:\n                model_name = model_cfg.pop('tag')\n            else: \n                model_name = model_cfg['model']\n            \n            seperate_llm_cfg.update({\"config_list\": [model_cfg]})\n            # __import__('ipdb').set_trace()\n            self.clients[model_name] = OpenAIWrapper(**seperate_llm_cfg)\n        self.client =OpenAIWrapper(**self.llm_config) # without the tag arg \n        \n        \n        self.register_hook('process_message_before_send', self.push_reply_to_game_server)\n        self.register_reply([Agent, None], SimsAgent.func_router)\n        self.subsitute_reply(SimsAgent.generate_oai_reply) \n        self.callable_tools = [self.handle_purchase_request]\n    \n    def subsitute_reply(self, new_func):\n        '''\n        replace the old registered reply by the new func with the same name\n        '''\n        for reply_func_tuple in self._reply_func_list:\n            reply_func = reply_func_tuple[\"reply_func\"]\n            if reply_func.__name__ == new_func.__name__:\n                reply_func_tuple.update({\"reply_func\": new_func})\n                \n    def update_system_message(self, system_message: str) -> None:\n        return super().update_system_message(system_message)\n     \n    def push_reply_to_game_server(self, message: Union[Dict, str], recipient: Agent, silent: bool\n    ) -> Union[Dict, str]:\n        '''\n        push the message to the game server\n        '''\n        if recipient != self:\n            try:\n                message_dict = self._message_to_dict(message)\n                content = message_dict.get('content') or message_dict[0].get('content') or message_dict['messages'][0]['content']\n            except (KeyError, IndexError):\n                content = None\n                msg = {\n                    'agent_guid': self.guid,\n                    'content': content,\n                    'song': \"agent_song_on_walk3\"\n                }\n\n                msg_str = f\"1008@{json.dumps(msg)}\"\n                add_msg_to_send_to_game_server(msg_str)\n            except Exception as e:\n                print('#'*10, '\\n', e, '\\n', '#'*10)\n                print('#'*10, f'\\n Error in push_reply_to_game_server, the content is {self._message_to_dict(message)} \\n', '#'*10)\n        return message \n    \n    def vigor_cost(self, message: Union[Dict, str], recipient: Agent, silent: bool\n    ):\n        if hasattr(self, 'vigor'):\n            self.vigor -= len(self._message_to_dict(message)['content']) * 0.01 * self.vigor_decay_rate\n    \n    @staticmethod\n    def _message_to_dict(message: Union[Dict, str]) -> Dict:\n        \"\"\"\n        For the case that we must return a json style text for agent reply, we set the reply format in chat as {'content': xxx}\n        So try to transform it to disct first\n        \"\"\"\n        if message is None: __import__('ipdb').set_trace()\n        # print(\"!!! message is \", message)\n        if isinstance(message, str):\n            try:\n                message = json.loads(message)\n                if type(message) is dict:\n                    if 'messages' in message:\n                        message = message['messages'][-1]\n                    if 'content' in message:\n                        message['content'] = str(message['content']) # ensure the content is str\n                        return message\n            except:\n                return {\"content\": message}\n        elif isinstance(message, dict):\n            return message\n        else:\n            raise ValueError(f\"message is not in the proper format {message}\")\n            \n    # async def a_process_then_reply(self, message, sender: Agent, restart=True, silent=True, check_exempt_layers=[1,2,3,4,5,6,7,8,9,10] ):\n    #     # modified from ConversableAgent.a_receive()\n    #     self._prepare_chat(self,clear_history=restart)\n    #     self._process_received_message(message, sender, silent)\n    #     reply = await self.a_generate_reply(sender=sender)\n    #     error = self.prompt_and_response.response_vanity_check(reply, check_exempt_layers)\n    #     if error:\n    #         print(f'Error in response: {error}.')\n    #         return await self.a_process_then_reply(message + error +' Please strictly follow the example in the prompt', sender, restart=False, silent=silent, check_exempt_layers=check_exempt_layers)  \n    #     else:\n    #         return json.loads(reply)\n        \n    # def process_then_reply(self, message, sender: Agent, restart=True, silent=True, check_exempt_layers=[1,2,3,4,5,6,7,8,9,10] ):\n    #     # modified from ConversableAgent.receive(), for debug purpose\n    #     self._prepare_chat(self,clear_history=restart)\n    #     self._process_received_message(message, sender, silent)\n    #     reply = self.generate_reply(sender=sender)\n    #     error = self.prompt_and_response.response_vanity_check(reply, check_exempt_layers)\n    #     if error:\n    #         print(f'Error in response: {error}.')\n    #         return self.process_then_reply(message + error +' Please strictly follow the example in the prompt', sender, restart=False, silent=silent, check_exempt_layers=check_exempt_layers)  \n    #     else:\n    #         return json.loads(reply)\n        \n    # async def a_generate_reply(\n    #     self,\n    #     messages:Optional[List[Dict[str, Any]]] = None,\n    #     sender: Optional[\"Agent\"] = None,\n    #     **kwargs: Any,\n    # ) -> Union[str, Dict[str, Any], None]:\n    #     reply = await super().a_generate_reply(messages, sender, **kwargs)\n    #     if sender!=self:\n    #         self.push_reply_to_game_server(reply)\n    #     return reply\n\n    def func_router(self, messages: Union[Dict, str], sender: Agent, config:  Optional['OpenAIWrapper'] = None):\n        '''\n        route the function request from the messages to the corresponding function\n        '''\n        if messages is None:\n            messages = self._oai_messages[sender]\n        message = messages[-1]['content']\n        \n        try:\n            message_dict = eval(message)\n        except:\n            return False, None\n        if 'tool_call' not in message_dict: return False, None\n        \n        func_name = message_dict.pop('tool_call')\n        arg_dict = message_dict\n        arg_dict.update({'sender': sender})\n        for func in self.callable_tools:\n            if func.__name__ == func_name:\n                sig = inspect.signature(func)\n                missed_args = set(sig.parameters) - set(list(arg_dict.keys()))\n                redundant_args = set(list(arg_dict.keys())) - set(sig.parameters) \n                if len(missed_args)==0 and len(redundant_args)==0 :\n                    res = func(**arg_dict)\n                    \n                else:\n                    part1 = f'You missed some important argumemnts {missed_args} .'\n                    part2 = f'You add some redundant arguments {redundant_args} .'\n                    part3 = f'The proper argument list is {sig.parameters}. Do not add extra args or mis any of them.'\n                    res = 'Sorry. '\n                    if missed_args:\n                        res = res + part1\n                    if redundant_args:\n                        res = res + part2\n                    res = res + part3\n                break\n            \n        return True, res\n    \n    def generate_oai_reply(self,\n        messages: Optional[List[Dict]] = None,\n        sender: Optional[Agent] = None,\n        config: Optional['OpenAIWrapper'] = None,\n    ) -> tuple[bool, Union[str, Dict, None]]:\n        \"\"\"\n        use the client set by the current state\n        Generate a reply using autogen.oai.\n        \n        \"\"\"\n        try:\n            client = self.clients[self.state.default_client]\n        except:                          \n            client = self.client\n        if client is None:\n            return False, None\n        if messages is None:\n            messages = self._oai_messages[sender]\n        extracted_response = self._generate_oai_reply_from_client(\n            client, self._oai_system_message + messages, self.client_cache\n        )\n        if extracted_response.startswith(' ```json') and extracted_response.endswith('```'):\n            extracted_response = extracted_response[8:-3]\n        return (False, None) if extracted_response is None else (True, extracted_response)\n    \n   \n     \n    def register_callable_tools(self, func):\n        '''\n        callable tools are designed to modify the attributes of the character, \n        it substitutes for the previous func: modify_internal_properties\n        since it is checked every time when the character responses, it is more flexible and do not need an extral CharacterState to modify the character\n        '''\n        self.callable_tools.append(func)\n        return func\n        \n    def handle_purchase_request(self, artwork_id, price:int, sender: Agent):\n        '''\n        handle the purchase request, check the price and money of the sender and the belonging of the artwork\n        '''\n\n        if not check_artwork_belonging(artwork_id, self.guid):\n            return True, \"Sorry, I just checked my artwork storage, this artwork is not mine. There may be some misunderstanding.\"\n        \n        if sender.money < price:\n            return True, f\"Sorry, there is only {sender.Gold} gold coins in your account. It is not enought to finish the purchase.\"\n        \n        result=check_balance_and_trade(balance_decrease_agent_id=sender.guid,\n                                balance_increase_agent_id=self.guid,\n                                trade_type='AGENT_TRADE_ARTWORK',\n                                balance_change=price,\n                                commodity_id=artwork_id,\n                                from_user_name= sender.name,\n                                to_user_name= self.name,\n                                )\n        if result['success']:\n            sender.money = result['from_account_balance']\n            self.money = result['to_account_balance']\n            return \" Thanks! The purchase is successful. The artwork is in your storage now. \"\n        else: \n            return f'Error when recording the transction: {result[\"response\"]}' \n\nif __name__ == '__main__':\n    agent = SimsAgent('test')\n    # asyncio.run(agent.a_process_then_reply('{\"tool_call\": \"handle_purchase_request\", \"artwork_id\": \"123\", \"price\": 100}', agent)\n"}
{"type": "source_file", "path": "app/models/db_modules/milvus_collections.py", "content": "from langchain_openai import OpenAIEmbeddings\nfrom app.database.milvus_constants import ARTWORK_MILVUS_FILED_SCHEMA\nfrom app.database.milvus_datastore import MilvusDataStore\nfrom app.global_config import MILVUS_HOST, MILVUS_INDEX_PARAMS, MILVUS_PORT\nfrom pymilvus.orm.collection import (\n    SearchResult\n)\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n\nARTWORK_COLLECTION_NAME = 'artwork_collection'\nartwork_milvus_data_store = MilvusDataStore(\n                host=MILVUS_HOST,\n                port=MILVUS_PORT,\n                collection_name=ARTWORK_COLLECTION_NAME,\n                field_schema=ARTWORK_MILVUS_FILED_SCHEMA,\n                index_field='prompt_emb',\n                index_params= MILVUS_INDEX_PARAMS,\n            )\n\n\ndef retrieve_artwork_by_prompt_emb(query:str, topk=100, other_param=None):\n        search_param = {\n                \"data\": [embeddings.embed_query(text=query)], # 要搜索的query的emb\n                \"anns_field\": \"prompt_emb\", # 要检索的向量字段\n                \"param\": {\"metric_type\": \"COSINE\"},\n                \"limit\": topk, # Top K \n                'output_fields': ['timestamp', 'resource_id'],   #自定义输出字段\n                # 'expr': milvus_expression,\n        }\n        if other_param is not None: search_param.update(other_param)\n        search_result:SearchResult = artwork_milvus_data_store.vector_search(\n            search_param=search_param,\n        )\n        return search_result"}
{"type": "source_file", "path": "app/models/exchange_agent.py", "content": "import json\nfrom typing import Coroutine, Dict\n\nfrom autogen import ConversableAgent\nfrom autogen.agentchat.agent import Agent\nfrom autogen.agentchat.chat import ChatResult\nfrom app.models.character import Character\nfrom ..utils.gameserver_utils import add_msg_to_send_to_game_server\n'''\nAn invisible NPC created to inherit trade proposals from the user.\nOnly one reply\nNo user interaction\n'''\n\nclass Trader(Character):\n    '''\n    Under Construction\n    '''\n    def __init__(self, llm_cfg, name='Trader', id='-1', age='0', \n                 bio='You are a trader come from a remote town',\n                 in_building=None,\n                  health=10, money=1000, \n                  satiety=10, vigor=100, \n                  max_consecutive_auto_reply=1, \n                  init_emos=None, \n                  is_multi_modal_agent=True, \n                  **kwargs):\n        super().__init__(name, id, age, bio, llm_cfg, 0, 0, health, money, satiety, vigor, max_consecutive_auto_reply, init_emos, is_multi_modal_agent, in_building=in_building, **kwargs)\n        self.register_hook('process_last_received_message', self.push_trade_response)\n        \n\n    def push_trade_response(self, message):\n        message = self.parse_final_message(message)\n        add_msg_to_send_to_game_server(message)\n\n    def pre_send(self, recipient:ConversableAgent, message):\n        '''\n        before send the message\n        message is from the game server\n        '''\n        message\n        if recipient._oai_messages[self.name] == 0:\n            return True\n        \n\n    def send(self, message: Dict | str, recipient: Agent, request_reply: bool | None = None, silent: bool | None = False) -> ChatResult:\n        # self.clear_history(, nr_messages_to_preserve=10)\n        message = self._process_message_before_send(message, recipient, silent)\n        # When the agent composes and sends the message, the role of the message is \"assistant\"\n        # unless it's \"function\".\n        valid = self._append_oai_message(message, \"assistant\", recipient)\n        if valid:\n            recipient.receive(message, self, request_reply, silent)\n        else:\n            raise ValueError(\n                \"Message can't be converted into a valid ChatCompletion message. Either content or function_call must be provided.\"\n            )\n    \n    async def a_send(self, message: Dict | str, recipient: Agent, request_reply: bool | None = None, silent: bool | None = False) -> Coroutine[Any, Any, ChatResult]:\n        # self.clear_history(, nr_messages_to_preserve=10)\n        message = self._process_message_before_send(message, recipient, silent)\n        # When the agent composes and sends the message, the role of the message is \"assistant\"\n        # unless it's \"function\".\n        valid = self._append_oai_message(message, \"assistant\", recipient)\n        if valid:\n            await recipient.a_receive(message, self, request_reply, silent)\n        else:\n            raise ValueError(\n                \"Message can't be converted into a valid ChatCompletion message. Either content or function_call must be provided.\"\n            )\n    \n              \n    def parse_final_message(self, message, **kwargs):\n        if message == 'Yes':\n            is_succ = True\n            msg = {\n                'content': 'Yes',\n                'action': 'trade',\n                'artwork_id': self.character.drawings[0]['id'],\n                'price': self.character.drawings[0]['price'],\n                'trader_id': self.character.guid\n            }\n        elif message == 'No':\n            is_succ = False\n\n        msg = {\n            'is_succ': is_succ,\n            'artwork_id': self.character.drawings[0]['id'],\n            'price': self.character.drawings[0]['price'],\n            'to_user_name': 'user' # FIXME: should be the user's name\n        }\n        msg_str = f\"1004@{json.dumps(msg)}\"\n        \n        add_msg_to_send_to_game_server(msg_str)\n"}
{"type": "source_file", "path": "app/models/emotion.py", "content": "from collections import defaultdict\nfrom typing import Dict\nimport random\nimport json\n\nclass Emotion:\n    \"\"\"Emotion of the agent. It is a dictionary of several emotions, each with a float value between 0 and 10.\n    \"\"\"\n    emotional_options = [\"joy\", \"trust\", \"fear\", \"surprise\", \"sadness\", \"disgust\", \"anger\", \"anticipation\",]\n    positive_emotions = [\"joy\", \"trust\", \"anticipation\", \"surprise\"]\n    negative_emotions = [\"fear\", \"surprise\", \"sadness\", \"disgust\", \"anger\"]\n    def __init__(self, emotion: Dict[str, float]=None, update_alpha=0.7, decay_alpha=0.01) -> None:\n        self.emotion = dict()\n        self.random_init_emotions()\n        if emotion is not None:\n            for k, v in emotion.items(): \n                v = float(v)\n                assert 0 <= v <= 10, f\"the emotions should be keeped between 0 and 10. Your emotion is {emotion}\"\n                if k in self.emotional_options:\n                    self.emotion[k] = v\n        self.update_alpha = update_alpha # the weight of the new emotion value\n        self.passive_decay_alpha = decay_alpha # the decay rate of the emotion when there is no new emotion input\n        self.impressive_event:dict[str,dict[str, str]] = defaultdict(dict) # the event that causes the emotion\n    \n    def random_init_emotions(self):\n        \"\"\"\n        Ensure the initial emotion is not too extreme.\n        \"\"\"\n        for key in self.emotional_options:\n            if random.random() < 0.1:\n                self.emotion[key] = random.randint(6, 10)\n            elif random.random() < 0.8:\n                self.emotion[key] = random.randint(0, 4)\n            else:\n                self.emotion[key] = random.randint(4, 6) \n    \n    def update(self, emotions: list[dict]=None) -> None:\n        '''\n        \"emotions\": [\n            {\n                \"emotion\": \"joy\",\n                \"change\": 4,\n                \"explanation\": \"I feel much happy beacause I get a gift from my friend Jay.\"\n            },\n            {\n                \"emotion\": \"trust\",\n                \"change\": 2,\n                \"explanation\": \"I trust my Jay, he is a kind man.\"\n            },\n            {\n                \"emotion\": \"supprise\",\n                \"change\": 2,\n                \"explanation\": \"I am supprised that Jay gives me a gift. I didn't expect that. And the gift happend to be on my gift list.\"\n            }\n        ]\n        '''\n        if emotions is None:\n            self.passive_update()\n        else:\n            for emotion in emotions:\n                emo_name, intensity_change, event = emotion.get('emotion'), emotion.get('change'), emotion.get('explanation')\n                try:\n                    self.update_single_emotion(emo_name, intensity_change, event)\n                except:\n                    pass\n                    \n    def update_single_emotion(self, emotion: str, intensity_change: float, event:str = None) -> None:\n        assert emotion in self.emotional_options, f\"the emotion should be in the emotional_options. Your emotion is {emotion}\"\n        assert -5 <= intensity_change <= 5, f\"the intensity should be keeped between -5 and 5. Your intensity is {intensity_change}\"\n        self.emotion[emotion] += intensity_change # self.update_alpha * intensity + (1 - self.update_alpha) * self.emotion[emotion] may exceed 1\n        if event:\n            self.impressive_event_update(emotion, intensity_change, event)        \n    \n    def passive_update(self, emotion=None, decay_alpha=None):\n        if decay_alpha is None: decay_alpha = self.passive_decay_alpha\n        \n        # TODO: optimize emotion passive update algorithm\n        if emotion is None: # update all the emotions in the current state\n            for key in self.emotion.keys():\n                if self.emotion[key] > 7: \n                    self.emotion[key] -= self.emotion[key] * random.uniform(0, 10) * decay_alpha # exptreme high emotion will randomly decrease by 0~10%\n                    self.emotion[key] -= self.emotion[key] * random.randrange(0, 10) * decay_alpha # exptreme high emotion will randomly decrease by 0~10%\n                else:\n                    self.emotion[key] += self.emotion[key] * random.randrange(0, 5) * decay_alpha # normal emotion will randomly in/decrease by 0~5%\n        else:\n            if self.emotion[emotion] > 7: \n                self.emotion[emotion] -= self.emotion[emotion] * random.randrange(-5, 5) * decay_alpha \n            else:\n                self.emotion[emotion] += self.emotion[emotion] * random.randrange(-5, 5) * decay_alpha\n        for emo in self.emotion.keys():\n            self.emotion[emo] = max(0, min(10, round(self.emotion[emo], 2)))\n                \n    def impressive_event_update(self, emotion, emotion_delta, event:str):\n        pre_emo_dif = self.impressive_event.get(emotion, dict()).get('emo_delta', 1) \n        if pre_emo_dif < emotion_delta:\n            self.impressive_event[emotion].update({'emo_delta': emotion_delta, 'event': event})\n        else: \n            self.impressive_event[emotion].update({'emo_delta': min(1, pre_emo_dif-self.passive_decay_alpha*50), 'event': event})\n            \n    @property\n    def impression(self) -> Dict[str, float]:\n        return self.emotion\n    \n    @property\n    def most_impressive_event(self):\n        '''\n        return event_description and event emotion\n        '''\n        highest_emo_delta = 0\n        impressive_event, event_emo = '',''\n        for key in self.impressive_event.keys():\n            emo_delta = self.impressive_event[key].get('emo_delta',0) \n            if emo_delta > highest_emo_delta:\n                highest_emo_delta = emo_delta\n                impressive_event = self.impressive_event[key].get('event', None)\n                event_emo = key\n                assert impressive_event is not None, f'event is None, your event is {impressive_event}'\n        return impressive_event, event_emo\n         \n    @property\n    def extreme_emotion(self) -> dict:\n        ext_emo_cate = max(self.emotion, key=self.emotion.get)\n        return {ext_emo_cate: f\"{self.emotion[ext_emo_cate]}/10\"}\n\n    @property\n    def extreme_emotion_name(self) -> dict:\n        ext_emo_cate = max(self.emotion, key=self.emotion.get)\n        return ext_emo_cate\n\n    @property\n    def extreme_emotion_value(self) -> dict:\n        ext_emo_cate = max(self.emotion, key=self.emotion.get)\n        return self.emotion[ext_emo_cate]\n    \n    def __repr__(self) -> str:\n        return f\"\"\"Emotion: {self.emotion}\n                Extreme Emotion: {self.extreme_emotion}\"\"\"\n                \n                \nif __name__ == '__main__':\n    print(Emotion().emotional_options)"}
{"type": "source_file", "path": "app/models/entity_factory.py", "content": "import json\nimport random\nfrom app.models.character import CharacterList\nfrom app.models.character import Character\nfrom app.repository.agent_repo import get_agent_from_db\nfrom app.llm.caller import LLMCaller, GPT35Caller\nfrom config import unique_names as name_list\n\nfrom openai import OpenAI\n\n\nclass AgentCreation:\n    default_bio = \"The biography of your parents are: Parent A: {p1bio} and Parent B: {p2bio} \"\n    \n    @staticmethod \n    def build_new_agent_from_msg( msg, character_ls: CharacterList):\n        content = json.loads(msg['msg'])\n        agent_id = content['agent_guid']\n        neochar = AgentCreation.build_new_agent(agent_id, character_ls) \n        \n        return neochar\n        \n    @staticmethod \n    def build_new_agent(agent_id, character_ls: CharacterList):\n        \n        par1, par2 = AgentCreation.find_parent_agent(agent_id, character_ls)\n        new_chara_features = dict()\n        for func in [AgentCreation.create_bio, AgentCreation.create_mbti, AgentCreation.get_name, AgentCreation.allocate_llm]:\n            res = func(par1, par2)\n            new_chara_features.update(res)\n\n        return Character.decode_from_json(new_chara_features)\n        \n\n    @staticmethod \n    def find_parent_agent(agent_id, character_ls):\n        agent_row = get_agent_from_db(agent_id)\n        par_id_1 = agent_row['parent_agent_guid1']\n        par1 = character_ls.get_character_by_id(par_id_1)\n        par_id_2 = agent_row['parent_agent_guid2']\n        par2 = character_ls.get_character_by_id(par_id_2)\n        \n        return par1, par2\n        \n    @staticmethod \n    def create_bio(par1:Character, par2: Character):\n        p1bio = par1.bio\n        p2bio = par2.bio\n        prompt = f\"Based on the biographs of the parents {p1bio} and {p2bio}, image the child's bio and name. Return in json format, like {{'child_bio': 'xxx', 'name': 'xxx'}}\"\n        client = OpenAI()\n        response = client.chat.completions( \n            model=\"gpt-3.5-turbo\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n            response_format={ \"type\": \"json_object\" },\n        )\n        try:\n            res = response['choices'][0]['message']['content']\n            new_bio = json.load(res)['child_bio']\n        except:\n            new_bio = AgentCreation.default_bio.format(p1bio=p1bio, p2bio=p2bio)\n            \n        return {\"bio\":new_bio}\n    \n    @staticmethod \n    def create_mbti( par1:Character, par2: Character):\n        p1mbti = par1.mbti\n        p2mbti = par2.mbti\n        new_mbti ='ABCD'\n        for i in range(4):\n            if p1mbti[i] == p2mbti[i]:\n                new_mbti[i] = p1mbti[i]\n            else:\n                new_mbti[i] = p1mbti[i] if random.randint(0,1) == 0 else p2mbti[i]\n                \n        return {\"mbti\":new_mbti}\n    \n    @staticmethod \n    def get_name(agent_id, *args, **kwargs):\n        return {\"name\":name_list[agent_id]}\n    \n    @staticmethod \n    def allocate_llm( *args, **kwargs):\n        with open('runtime/cheap_apis.json', 'r') as file:\n            cheap_apis = json.load(file)\n        with open('runtime/official_apis.json', 'r') as file:\n            official_apis = json.load(file)\n        \n        # find the least used api\n        sorted_cheap_apis = dict(sorted(cheap_apis.items(), key=lambda item: len(item[1])))\n        sorted_official_apis = dict(sorted(official_apis.items(), key=lambda item: len(item[1]))) \n        \n        return {\"llm_cfg\":{'cheap_api':sorted_cheap_apis[0], 'official_api':sorted_official_apis[0]}}\n         "}
{"type": "source_file", "path": "app/models/character.py", "content": "import copy\nimport inspect\nimport os\nimport asyncio\nfrom collections import defaultdict\nimport os\nimport re\nimport traceback\nimport uuid\nimport dill\nfrom typing import Any, Callable, Optional, Union, Dict, List\nfrom functools import cache, partial\nimport json\nfrom collections import deque \nfrom datetime import datetime, timedelta\nfrom autogen import ConversableAgent, AssistantAgent, UserProxyAgent, config_list_from_json, Agent\nfrom autogen.agentchat.contrib.multimodal_conversable_agent import MultimodalConversableAgent\nfrom autogen import config_list_from_json, filter_config\nfrom langchain_openai import OpenAIEmbeddings\nfrom autogen.oai import OpenAIWrapper\n\nfrom app.models.base_agent import SimsAgent\nfrom app.repository.agent_repo import check_balance_and_raise\nfrom app.repository.artwork_repo import check_artwork_belonging, get_artwork_from_db, update_artwork_in_db\nfrom app.repository.trade_repo import add_trade_to_db\nfrom app.database.orm.trade_record import trade_type_dict\nfrom app.repository.utils import check_balance_and_trade\nfrom app.utils.load_oai_config import plug_api_to_cfg, register_callback\nfrom config import cfg_tmplt\n\nfrom .data_store import Memory, WorkingMemory\nfrom .emotion import Emotion\nfrom .preference_model import ArtTaste\nfrom .internal_dialogue import InnerMonologue\nfrom ..constants import CharacterState, PromptType\nfrom ..utils.log import LogManager\nfrom ..utils.gameserver_utils import add_msg_to_send_to_game_server\nfrom ..utils.serialization import serialize\nfrom ..models.location import Building, Job\nfrom ..models.scheduler import Agenda, Schedule, Task\nfrom .dalle_agent import DALLEAgent\nfrom .drawing import DrawingList, Drawing\nfrom ..utils.save_object import find_instance_specific_data_attrs\n\n# The decision you make must be confirmed to the long-term memory, the ultimate goal and the bio of the game character. \n# Your knowledge level should not exceed that of a normal person with the bio of the character, unless there are relevant memories in his/her Long-Term Memory.\nclass Character(SimsAgent):\n    DEFAULT_SYS_PROMPT =\"\"\"\n    You are a game character in a small town to decide what to do immediately to finish your Schedule.\nYou should also decide whether you can use the experience in the Long-Term Memory to finish this Schedule. It can be used only if there is exactly similar Schedule in the experience. For example, eating something cannot be regarded as similar one of cooking something.\nYour knowledge level should not exceed that of a normal person with the bio of the character\nPlease notice that:\n* All information you recieve is in the game scene, and you cannot reject to make the decision. Don't worry, you will not be responsible for the result.\n* If you think You have collected enough information in a dialogue, you can stop it at any time by saying 'TERMINATE'.\n* everytime before you respond to others, think twice about what is the best response, then your response as the content of the response.\n* return in json format, like { 'think_twice': thinking process, 'content': content of the response }. if there are examples, following the examples given to you \n\"\"\"\n    \n    \n    def __init__(self, name, guid:int, age:int, bio, \n                 llm_cfg, # {\"cheap_api\": \"sk-xxx\", \"official_api\": \"sk-xxx\"}\n                 x, y, \n                 health=10, money=1000, satiety=10,  \n                 vigor=10,\n                 max_consecutive_auto_reply = 10,\n                 mbti = 'ESFJ',\n                 init_emos = None,\n                 is_multi_modal_agent = True,\n                 in_building:Building = None,\n                 save_dir = None,\n                 **kwargs):\n        # 基本属性\n        self._name = name\n        self.age = age\n        self.guid = int(guid)\n        self.bio = bio\n        # self.goal = goal\n        self.satiety_decay_rate= 1 # adjustment coefficincy of the satiety decay, the smaller, the slower\n        self.vigor_decay_rate = 1 # adjustment coefficincy of the vigor decay, the smaller, the slower\n        self.vigor_recovery_rate = 1 # adjustment coefficincy of the vigor recovery, the smaller, the slower\n        self.mbti = mbti \n        \n        # 状态属性\n        self.money = money\n        self.health = health\n        self.min_health = 0\n        self.max_health = 10\n        self.satiety = satiety\n        self.min_satiety = 0\n        self.max_satiety = 10\n        self.vigor = vigor # if 0, go sleep\n        self.min_vigor = 0\n        self.max_vigor = 10     \n        \n        self.x = x\n        self.y = y\n        self.date_num = 0\n        self.save_dir = f'{save_dir}/{name}'\n        self.state:'BaseState' = None\n        self.hang_states = deque(maxlen=3)\n        \n        self.longterm_memory = Memory(character_id=self.guid, character_name=name, \\\n                                      embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\") if os.environ.get('Milvus') else None) \n        self.in_building:Building  = in_building\n        self.Schedule = Schedule()\n        \n        self.emotion = Emotion(init_emos)\n        # self.drawings = DrawingList(owner=self)\n        self.artworks = {\"Drawing\": DrawingList(owner=self)}\n        self.job: Job = None\n        self.agenda = Agenda()\n        self.task: Task = None\n        self.art_preference = ArtTaste()\n\n        self.trade_strategy = None\n\n        self.working_memory = WorkingMemory() # storing the information to be transfered among states\n        self.prompt_and_response = PromptAndResponse(character_name=name, character_id=self.guid)\n        \n        #  ==== load llm_cfg for character ===== # TODO: wrap it into a function in utils\n        register_callback(llm_cfg, guid=self.guid, prefix=f'character_{name}')\n        config_list = plug_api_to_cfg(cfg_tmplt, **llm_cfg) \n        oai_config_list = filter_config(config_list=config_list,\n                        filter_dict={\n                            \"model\": [ \"gpt-3.5-turbo-0125\", \"gpt-4-0125-preview\",  \"deepseek-chat\"],\n                        }\n                      )\n        dalle_conig_list = \\\n            filter_config(\n                config_list=config_list, \n                filter_dict={\n                    \"model\": [\"dalle\"],\n                }\n            )\n\n        gpt4v_config_list = \\\n            filter_config(\n                config_list=config_list,\n                filter_dict={\n                    \"model\": [\"gpt-4-vision-preview\"]\n                }\n            )\n\n        llm_cfg = {\n                \"llm_cfg\": {\n                    \"config_list\": oai_config_list,\n                    \"seed\": 35,\n                    \"response_format\": {\"type\": \"json_object\"},\n                    \"temperature\": 0.5\n                },\n                \"draw_cfg\": {\n                    \"config_list\": dalle_conig_list,\n                    \"seed\": 35,\n                },\n                \"appreciate_cfg\": {\n                    \"config_list\": gpt4v_config_list,\n                    \"seed\": 35,\n                    \"response_format\": {\"type\": \"json_object\"},\n                    \"temperature\": 0.5\n                }\n            }\n        # ==================================\n        \n        if is_multi_modal_agent: \n            self.drawing_agent = DALLEAgent(name, owner=self, llm_config=llm_cfg['draw_cfg'] if 'draw_cfg' in llm_cfg else llm_cfg['config_list'])\n            self.appreciate_agent = MultimodalConversableAgent(\n                name=name, \n                llm_config=llm_cfg['appreciate_cfg'] if 'appreciate_cfg' in llm_cfg else llm_cfg['config_list'],\n                human_input_mode=\"NEVER\",\n                max_consecutive_auto_reply = max_consecutive_auto_reply\n                )\n        \n        \n        super().__init__(\n            name = name,\n            llm_config = llm_cfg['llm_cfg'],\n            max_consecutive_auto_reply = max_consecutive_auto_reply,\n            # is_termination_msg=(lambda x: \"TERMINATE\" in x.get(\"content\")), # may cause bug in registered functions\n            code_execution_config={'use_docker': False},\n        )\n        self.update_system_message(self.build_sys_message())\n        \n        self.prev_modifiable_attr = self.modifiable_status_dict # to check if attr of the character is modified, we need to store the prev attr\n        self.inner_monologue = InnerMonologue(self)\n        # self.register_hook('process_message_before_send', self.push_reply_to_game_server)\n        \n        # self.clients = {} # for multi client case, each client is for one model config\n        # for cfg_idx in range(len(self.llm_config.get('config_list', []))):\n        #     seperate_llm_cfg = copy.deepcopy(self.llm_config)\n        #     model_cfg = self.llm_config['config_list'][cfg_idx]\n        #     if 'tag' in model_cfg:\n        #         model_name = model_cfg.pop('tag')\n        #     else: \n        #         model_name = model_cfg['model']\n        #     # __import__('ipdb').set_trace()\n        #     seperate_llm_cfg.update({\"config_list\": [model_cfg]}) \n        #     self.clients[model_name] = OpenAIWrapper(**seperate_llm_cfg)\n        # self.client =OpenAIWrapper(**self.llm_config) # pop the tag arg \n        \n        # self.register_reply([Agent, None], Character.func_router)\n        # self.callable_tools = [self.handle_purchase_request]\n         \n    def build_sys_message(self):\n        return f\"\"\"{self.DEFAULT_SYS_PROMPT}\\n\\nThe name of the character: {self.name}, The character id is {self.guid}, The game character's bio : {self.bio}\\n\"\"\" #\\n, the character's inner status: {self.internal_status #The game character's ultimate goal : {self.goal}\n\n    @classmethod\n    def decode_from_json(self, **kwargs):\n        return Character(**kwargs)\n    \n    def change_gold(self, new_amount:int):\n        self.money = new_amount\n \n    def change_pos(self, x, y):\n        self.x = x\n        self.y = y\n    \n    def change_job(self, job:Job):\n        assert self.in_building is not None, 'The character is not in a building. To have a job, the character should be in a building.'\n        assert job.open_positions > 0, 'The job is full' \n        self.job = job\n        job.add_applicant(self)\n    \n    def update_emotion(self, emotions):\n        self.emotion.update(emotions)\n     \n    def change_date(self, date_num):\n        self.date_num = date_num\n   \n    def add_Task_to_agenda(self, Task, date):\n        return self.agenda.add_Task(Task, date)\n     \n    def set_Task(self, Task: Task):\n        self.task = Task\n    \n    def suspend_Task(self):\n        assert self.task is not None\n        self.task.set_status(self.task.SUSPENDED)\n            \n    def reactivate_Task(self):\n        assert self.task is not None\n        self.task.set_status(self.task.INPROGRESS)\n            \n     \n    def check_date_agenda(self, date):\n        return self.agenda.check_date(date)  \n    \n    @property\n    def drawings(self):\n        return self.artworks[\"Drawing\"]\n    \n    def add_artwork(self, art_type, artwork):\n        assert art_type in ['Drawing', 'Composition']\n        self.artworks.get(art_type).add(artwork)\n    \n    @property\n    def date(self):\n        '''\n        transform update number to a  date\n        '''\n        base = datetime(2022, 10, 1, 0)  \n        target_time = base + timedelta(hours=self.date_num)\n        return target_time.strftime('%Y-%m-%d %H:%M')\n\n    @property\n    def today_agenda(self):\n        return self.agenda.check_date(self.date)\n   \n    @property\n    def incompleted_agenda(self):\n        return self.agenda.incompleted_Tasks\n        \n    @property\n    def in_building_name(self):\n        if self.in_building:\n\n            return self.in_building.name\n        return \"outdoor\"\n    \n    @property\n    def in_building_id(self):\n        if self.in_building:\n            return self.in_building.guid\n        return '-1'\n    \n    # @property\n    # def act_obj_name(self):\n    #     if self.state.arbitrary_obj: \n    #         print(f\"!!!WARNING!!! In {self.state_name}, act_obj is set as {self.state.arbitrary_obj}. It is only for testing purpose.\") \n    #         return self.state.arbitrary_obj\n    #     else:\n    #         return self.working_memory.retrieve_by_name('act_obj')\n        \n    # @property\n    # def money(self):\n    #     # easy name change\n    #     return self.Gold\n    @property\n    def Gold(self):\n        return self.money\n    \n    def change_building(self,  building:Building ):\n        self.in_building = building\n        \n    def set_Schedule(self, Schedule_details):\n        Schedule_ls = list(Schedule_details['Steps'].values())\n        description = Schedule_details['GeneralDescription']\n        self.Schedule.set_Schedule(Schedule=Schedule_ls, description=description)\n    \n    @property\n    def current_step(self):\n        return self.Schedule.current_step\n    \n    # def refresh_current_step(self, current_step):\n    #     self.current_step = current_step\n        \n    \n    @property\n    def position(self):\n        # TODO find building by x,y \n        return self.x, self.y\n    \n    @property\n    def textual_internal_properties(self):\n        return ['job', 'agenda']\n     \n    @property\n    def digital_internal_properties(self):\n        # digital properties\n        return ['health', 'satiety', 'vigor']\n        # return ['health', 'Gold', 'satiety', 'vigor']\n    \n    @property\n    def internal_status(self):\n        inner_st = {\n            \"money\": self.money,\n            \"health\": f\"{self.health}/{self.max_health}\",\n            \"satiety\": f\"{self.satiety}/{self.max_satiety}\",\n            \"vigor\": f\"{self.vigor}/{self.max_vigor}\",\n            \n        }\n        \n        inner_st.update(self.current_emotion)\n        \n        if self.job:\n            inner_st['job'] = self.job.name \n        # if  not self.Schedule.is_none: # duplicated with Schedule.__repre__\n        #     inner_st['current_step'] = self.current_step\n         \n        return inner_st\n\n    @property\n    def modifiable_status(self):\n        return self.digital_internal_properties + self.textual_internal_properties \n    \n    def modify_internal_properties(self, properties:dict) -> None:\n        '''\n        {\n            \"satiety\": +1,\n            \"Gold\": -10,\n        }\n        '''\n        for key, value in properties.items():\n            if key in self.digital_internal_properties:\n                setattr(self, key, getattr(self, key) + int(value))\n            elif key == 'job':\n                self.change_job(value)\n            elif key == 'agenda':\n                if type(value) is list:\n                    for ag in value:\n                        self.add_Task_to_agenda(Task=ag['Task'], date=ag['specific_date'] )\n                elif type(value) is dict:\n                    self.add_Task_to_agenda(Task=value['Task'], date=value['specific_date'] )\n            else:\n                raise ValueError(f'Error, {key} is not in the digital_internal_properties')\n\n    @property\n    def modifiable_status_dict(self):\n        return {key: getattr(self, key) for key in self.modifiable_status}\n    \n    @property\n    def current_emotion(self):\n        return self.emotion.extreme_emotion\n\n    def impression_based_on_chat(self, act_obj:str):\n        # for affectiveness, the impression is based on the chat history\n        obj_agent, value = None, 0   \n        for agent in self._oai_messages.keys():\n            if agent.name == act_obj:\n                obj_agent = agent\n                break\n        if obj_agent:\n            value = len(self._oai_messages[obj_agent])\n        return value\n\n    def estimate_artwork_price(self, artwork_id):\n        '''\n        estimate the price of the artwork\n        '''\n        try:\n            artwork = get_artwork_from_db(artwork_id) \n            return artwork[\"price\"]\n        except:\n            return 100\n            \n\n    # def register_callable_tools(self, func):\n    #     '''\n    #     callable tools are designed to modify the attributes of the character, \n    #     it substitutes for the previous func: modify_internal_properties\n    #     since it is checked every time when the character responses, it is more flexible and do not need an extral CharacterState to modify the character\n    #     '''\n    #     self.callable_tools.append(func)\n    #     return func\n        \n    # def handle_purchase_request(self, artwork_id, price:int, sender: AssistantAgent):\n    #     '''\n    #     handle the purchase request, check the price and money of the sender and the belonging of the artwork\n    #     '''\n\n    #     if not check_artwork_belonging(artwork_id, self.guid):\n    #         return True, \"Sorry, I just checked my artwork storage, this artwork is not mine. There may be some misunderstanding.\"\n        \n    #     if sender.Gold < price:\n    #         return True, f\"Sorry, there is only {sender.Gold} gold coins in your account. It is not enought to finish the purchase.\"\n        \n    #     result=check_balance_and_trade(balance_decrease_agent_id=sender.guid,\n    #                             balance_increase_agent_id=self.guid,\n    #                             trade_type='AGENT_TRADE_ARTWORK',\n    #                             balance_change=price,\n    #                             commodity_id=artwork_id,\n    #                             from_user_name= sender.name,\n    #                             to_user_name= self.name,\n    #                             )\n    #     if result['success']:\n    #         sender.Gold = result['from_account_balance']\n    #         self.Gold = result['to_account_balance']\n    #         return True, \" Thanks! The purchase is successful. The artwork is in your storage now. \"\n    #     else: \n    #         return True,  f'Error when recording the transction: {result[\"response\"]}' \n        \n    # def change_state(self, state, img_url, img_id):\n    #     self.working_memory.store_memory('img_url', img_url)\n    #     self.working_memory.store_memory('img_id', img_id)\n    #     self.state.turn_on_states(state)\n    \n    # #================== func from autogen ==================\n    # @staticmethod\n    # def _message_to_dict(message: Union[Dict, str]) -> Dict:\n    #     \"\"\"\n    #     For the case that we must return a json style text for agent reply, we set the reply format in chat as {'content': xxx}\n    #     So try to transform it to disct first\n    #     \"\"\"\n    #     if message is None: __import__('ipdb').set_trace()\n    #     # print(\"!!! message is \", message)\n    #     if isinstance(message, str):\n    #         try:\n    #             message = json.loads(message)\n    #             if type(message) is dict:\n    #                 if 'messages' in message:\n    #                     message = message['messages'][-1]\n    #                 if 'content' in message:\n    #                     message['content'] = str(message['content']) # ensure the content is str\n    #                     return message\n    #         except:\n    #             return {\"content\": message}\n    #     elif isinstance(message, dict):\n    #         return message\n    #     else:\n    #         raise ValueError(f\"message is not in the proper format {message}\")\n            \n   \n    def process_then_reply(self, message, sender: Agent, restart=True, silent=True, check_exempt_layers=[1,2,3,4,5,6,7,8,9,10] ):\n        # modified from ConversableAgent.receive(), for debug purpose\n        self._prepare_chat(self,clear_history=restart)\n        self._process_received_message(message, sender, silent)\n        reply = self.generate_reply(sender=sender)\n        error = self.prompt_and_response.response_vanity_check(reply, check_exempt_layers)\n        if error:\n            print(f'Error in response: {error}.')\n            match = re.search(r'# previous error message: (\\d+) #', message)\n            idx = int(match.group(1))+1 if match else 0\n            if idx < 4:\n                return self.process_then_reply(message + f'\\n # previous error message: {idx} # '+  error +' Please strictly follow the example in the prompt', sender, restart=False, silent=silent, check_exempt_layers=check_exempt_layers)  \n            else:\n                return reply\n        else:\n            return PromptAndResponse.response_json_check(reply)\n    \n    async def a_process_then_reply(self, message, sender: Agent, restart=True, silent=True, restart_times=0, check_exempt_layers=[1,2,3,4,5,6,7,8,9,10] ):\n        # modified from ConversableAgent.a_receive()\n        self._prepare_chat(self,clear_history=restart)\n        self._process_received_message(message, sender, silent)\n        reply = await self.a_generate_reply(sender=sender)\n        error = self.prompt_and_response.response_vanity_check(reply, check_exempt_layers)\n        if error:\n            print(f'Error in response: {error}.')\n            # match = re.search(r'# previous error message: (\\d+) #', message)\n            restart_times += 1 \n            if restart_times < 4:\n                return await self.a_process_then_reply(message + f'\\n # previous error message: {restart_times} # '+  error +' Please strictly follow the example in the prompt', sender, restart=False, silent=silent, restart_times=restart_times, check_exempt_layers=check_exempt_layers)  \n            else:\n                return reply\n        else:\n            return PromptAndResponse.response_json_check(reply)\n        \n    # def _prepare_chat(self, recipient: \"ConversableAgent\", clear_history: bool, prepare_recipient: bool = True) -> None:\n    #     '''\n    #     reserve the previous chat for self.max_consecutive_auto_reply//2 turns   \n    #     '''\n    #     self.reset_consecutive_auto_reply_counter(recipient)\n    #     self.reply_at_receive[recipient] = True\n    #     if clear_history:\n    #         self.clear_history(recipient, nr_messages_to_preserve= self.max_consecutive_auto_reply()//2)\n    #         self._human_input = []\n    #     if prepare_recipient:\n    #         recipient._prepare_chat(self, clear_history, False)\n        \n    # # async def a_generate_reply(\n    # #     self,\n    # #     messages:Optional[List[Dict[str, Any]]] = None,\n    # #     sender: Optional[\"Agent\"] = None,\n    # #     **kwargs: Any,\n    # # ) -> Union[str, Dict[str, Any], None]:\n    # #     reply = await super().a_generate_reply(messages, sender, **kwargs)\n    # #     if sender!=self:\n    # #         self.push_reply_to_game_server(reply)\n    # #     return reply\n\n    # def func_router(self, messages: Union[Dict, str], sender: Agent, config:  Optional['OpenAIWrapper'] = None):\n    #     '''\n    #     route the function request from the messages to the corresponding function\n    #     '''\n    #     if messages is None:\n    #         messages = self._oai_messages[sender]\n    #     message = messages[-1]['content']\n        \n    #     try:\n    #         message_dict = eval(message)\n    #     except:\n    #         return False, None\n    #     if 'tool_call' not in message_dict: return False, None\n        \n    #     func_name = message_dict.pop('tool_call')\n    #     arg_dict = message_dict\n    #     arg_dict.update({'sender': sender})\n    #     for func in self.callable_tools:\n    #         if func.__name__ == func_name:\n    #             sig = inspect.signature(func)\n    #             missed_args = set(sig.parameters) - set(list(arg_dict.keys()))\n    #             redundant_args = set(list(arg_dict.keys())) - set(sig.parameters) \n    #             if len(missed_args)==0 and len(redundant_args)==0 :\n    #                 _, res = func(**arg_dict)\n                    \n    #             else:\n    #                 part1 = f'You missed some important argumemnts {missed_args} .'\n    #                 part2 = f'You add some redundant arguments {redundant_args} .'\n    #                 part3 = f'The proper argument list is {sig.parameters}. Do not add extra args or mis any of them.'\n    #                 res = 'Sorry. '\n    #                 if missed_args:\n    #                     res = res + part1\n    #                 if redundant_args:\n    #                     res = res + part2\n    #                 res = res + part3\n                \n    #     return True, res\n    \n    # def generate_oai_reply(self,\n    #     messages: Optional[List[Dict]] = None,\n    #     sender: Optional[Agent] = None,\n    #     config: Optional['OpenAIWrapper'] = None,\n    # ) -> tuple[bool, Union[str, Dict, None]]:\n    #     \"\"\"\n    #     use the client set by the current state\n    #     Generate a reply using autogen.oai.\n        \n    #     \"\"\"\n    #     try:\n    #         client = self.clients[self.state.default_client]\n    #     except:                          \n    #         client = self.client \n    #     if client is None:\n    #         return False, None\n    #     if messages is None:\n    #         messages = self._oai_messages[sender]\n    #     extracted_response = self._generate_oai_reply_from_client(\n    #         client, self._oai_system_message + messages, self.client_cache\n    #     )\n    #     return (False, None) if extracted_response is None else (True, extracted_response)\n        \n    # def push_reply_to_game_server(self, message: Union[Dict, str], recipient: Agent, silent: bool\n    # ) -> Union[Dict, str]:\n    #     '''\n    #     push the message to the game server\n    #     '''\n    #     if recipient != self:\n    #         try:\n    #             message_dict = self._message_to_dict(message)\n    #             content = message_dict.get('content') or message_dict[0].get('content') or message_dict['messages'][0]['content']\n    #         except (KeyError, IndexError):\n    #             content = None\n    #             msg = {\n    #                 'agent_guid': self.guid,\n    #                 'content': content,\n    #                 'song': \"agent_song_on_walk3\"\n    #             }\n\n    #             msg_str = f\"1008@{json.dumps(msg)}\"\n    #             add_msg_to_send_to_game_server(msg_str)\n    #         except Exception as e:\n    #             print('#'*10, '\\n', e, '\\n', '#'*10)\n    #             print('#'*10, f'\\n Error in push_reply_to_game_server, the content is {self._message_to_dict(message)} \\n', '#'*10)\n    #     return message \n    \n    def vigor_cost(self, message: Union[Dict, str], recipient: Agent, silent: bool\n    ):\n        if hasattr(self, 'vigor'):\n            self.vigor -= len(self._message_to_dict(message)['content']) * 0.01 * self.vigor_decay_rate\n\n    def retrieve_modify_dialogue(self, obj_agent):\n        '''\n        modify the role of a dialogue into specific names\n        '''\n        dialogue = copy.deepcopy(self._oai_messages[obj_agent])\n        for dia_id in range(len(dialogue)):\n            if dialogue[dia_id]['role'] == 'user':\n                dialogue[dia_id]['role'] = obj_agent.name\n            elif dialogue[dia_id]['role'] == 'assistant':\n                dialogue[dia_id]['role'] = self.name\n                \n        return dialogue\n\n    async def a_drawing(self,):\n        # dalle 3 call \n        # autogen func call \n        pass\n    \n    def set_state(self, state):\n        self.state = state\n\n    @property\n    def state_name(self):\n        return self.state.state_name\n\n    def encode_to_json(self) -> json:\n        return serialize(self, allowed=['name', 'age', 'bio', 'goal', 'health', 'money', 'x', 'y', 'state'])\n\n    def encode_pos(self) -> json:\n        return serialize(self, allowed=['name', 'x', 'y'])\n\n    def save_prompt(self, prompt, prompt_type: PromptType, prompt_example:str = None):\n        self.prompt_and_response.save_prompt(prompt, prompt_type, prompt_example)\n\n    def save_response(self, response, prompt_type: PromptType):\n        self.prompt_and_response.save_response(response, prompt_type)\n        # setattr(self, prompt_type.name, response) # TODO: dangerous\n\n    def get_latest_prompt(self, prompt_type):\n        if prompt_type in self.prompt_and_response.prompt_dict:\n            prompts = self.prompt_and_response.prompt_dict[prompt_type]\n            if prompts:\n                return prompts[-1]\n        return 'Error, No prompts'  # TODO\n\n    def get_latest_response(self, prompt_type):\n        if prompt_type in self.prompt_and_response.result_dict:\n            responses = self.prompt_and_response.result_dict[prompt_type]\n            if responses:\n                return responses[-1]\n        return 'Error, No prompts'  # TODO\n\n    def get_latest_prompt_type(self):\n        return self.prompt_and_response.prompt_type_list[-1]\n\n    def encode_llm(self) -> json:\n        return self.prompt_and_response.encode_to_json()\n\n    def encode_latest_llm(self) -> json:\n        return self.prompt_and_response.encode_latest_llm()\n   \n    @staticmethod\n    def serializable_obj():\n        return ['state', 'longterm_memory', 'emotion', 'drawings', 'working_memory', 'prompt_and_response', 'Schedule', 'agenda', 'Task', 'job']\n   \n    @staticmethod\n    def value_attrs():\n        return ['name', 'age', 'bio', 'goal', 'health', 'money', 'x', 'y', 'vigor', 'satiety', 'date_num', 'guid']\n    \n    def attrs_to_save(self):\n        '''\n        return a dict of the attributes to save or log\n        '''\n        attr_to_save = find_instance_specific_data_attrs(self)\n        attr_to_save = attr_to_save + ['name']\n        dict2save = dict( ((attr, getattr(self, attr)) for attr in attr_to_save))\n        return dict2save\n    \n    def save_self_locally(self):\n        save_path = None\n        if self.save_dir:\n            os.makedirs(f'{self.save_dir}', exist_ok=True)\n            dict2save = self.attrs_to_save()\n                        \n            # Keep the number of files less than K\n            files = os.listdir(self.save_dir)\n            files.sort(key=lambda x: os.path.getctime(os.path.join(self.save_dir, x)))  # Sort files by creation time\n            while len(files) >= 10: \n                os.remove(os.path.join(self.save_dir, files[0]))\n                files.pop(0)\n            \n            save_path = f'{self.save_dir}/char_{self.name}_{self.name}_{datetime.now().strftime(\"%m%d%H%M%S\")}.dill'\n            try:\n                with open(save_path, 'wb') as f:\n                    dill.dump(dict2save, f)\n            except:\n                traceback.print_exc()\n                __import__('ipdb').set_trace()\n            \n        return save_path\n                \n    def load_from_local(self, file_path):\n        print(f'load Character from {file_path}')\n        with open(file_path, 'rb') as f:\n            attr_dict = dill.load(f)\n        for key, value in attr_dict.items():\n            if key in ['name','save_dir']: continue\n            setattr(self, key, value)\n    \n    def log_attrs(self):\n        dict2save = self.attrs_to_save() \n        dict2save.pop('llm_config', None)\n        LogManager.log_char_attr_with_time(self.name, dict2save)\n        \n    def __repr__(self):\n        return f'{self.name}'\n\n    \n\nclass CharacterList:\n    characters: list[Character]\n\n    def __init__(self):\n        self.characters = []\n\n    def perspect_surrounding_char(self, protagonist: Character) -> list[Character]:\n        '''\n        return characters in the same building\n        '''\n\n\n        def perspectable(agent_a, agent_b):\n            blg_a = agent_a.in_building\n            blg_b = agent_b.in_building\n            distance = agent_a.x -agent_b.x + agent_a.y - agent_b.y # Manhattan distance\n            return blg_a==blg_b and distance<18 \n\n        return [  char for char in self.characters if perspectable(protagonist, char) and protagonist!=char ]\n\n    def get_character_by_name(self, name:str):\n        for char in self.characters:\n            if char.name == name:\n                return char\n        return None\n    \n    def get_character_by_id(self, id):\n        for char in self.characters:\n            if char.guid == id:\n                return char\n        return None \n\n    def add_character(self, character):\n        self.characters.append(character)\n\n    def get_nearby_characters(self, character, radius) -> list[str]:\n        return [other_character.name for other_character in self.characters if\n                abs(other_character.x - character.x) <= radius and abs(other_character.y - character.y) <= radius]\n\n    def encode_to_json(self) -> json:\n        character_dicts = [character.encode_to_json() for character in self.characters]\n        return character_dicts\n    \n    def save_locally(self):\n        for character in self.characters:\n            character.save_self_locally()\n\n\nclass PromptAndResponse:\n    def __init__(self, character_name, character_id):\n        # self.character = character\n        self.character_name = character_name\n        self.character_id = character_id\n        self.prompt_dict: dict[PromptType, list] = {}\n        self.result_dict: dict[PromptType, list] = {}\n        self.prompt_type_list = []\n        self.latest_prompt_type: PromptType = None\n        self.latest_prompt_example:str = None\n        self.latest_prompt = None\n        self.latest_response = None\n        self.error_seperator = \"###NOTICE###\"\n    \n    @property\n    def all_prompts_types(self):\n        return list(self.prompt_dict.keys())\n\n    def save_prompt(self, prompt, prompt_type, prompt_example):\n        self.prompt_type_list.append(prompt_type)\n        self.latest_prompt_type = prompt_type\n        self.latest_prompt_example = prompt_example\n        self.latest_prompt = prompt\n        prompts = self.prompt_dict.setdefault(prompt_type, [])\n        prompts.append(prompt)\n        print(f\"\\n***Send prompt to {self.character_name}, {prompt_type}\\n{prompt}\\n\")\n        LogManager.log_character_with_time(self.character_name, f\"***Send prompt to LLM, {prompt_type}\\n{prompt}\\n\")\n    \n    def save_response(self, response, prompt_type):\n        responses = self.result_dict.setdefault(prompt_type, [])\n        responses.append(response)\n        self.latest_response = response\n        print(f\"\\nReceive response from LLM of {self.character_name}, {prompt_type}\\n{response}\\n\")\n        LogManager.log_character_with_time(self.character_name, f\"Receive response from LLM, {prompt_type}\\n{response}\\n\")\n\n    def encode_to_json(self) -> json:\n        return serialize(self, allowed=['prompt_dict', 'result_dict'])\n\n    def encode_latest_llm(self) -> json:\n        return serialize(self, allowed=['name', 'latest_prompt_type', 'latest_prompt', 'latest_response'])\n    \n    def response_vanity_check(self, response, check_exempt_layers):\n        try:\n            response_dict = PromptAndResponse.response_json_check(response)\n            self.response_structure_check(response_dict,check_exempt_layers)\n        except Exception as e:\n            return f'{self.error_seperator} Please NOTICE that {e} '\n        \n    @staticmethod\n    def response_json_check(response):\n        response = PromptAndResponse.extract_json_from_markdown(response)\n        try:        \n            if type(response) is not dict:\n                response = eval(response) \n        except:\n            try:\n                response = json.loads(response) \n            except:           \n                __import__('ipdb').set_trace()\n                print(f'Can not load reply as json, reply: {response}')\n                raise AssertionError('Must return a formal dict ! Recheck the dict format')\n        \n        return response\n           \n    def response_structure_check(self, response_dict, exempt_layers=[2,3,4,5,6,7,8,9,10]):    \n        if self.latest_prompt_example:\n            prompt_example = self.latest_prompt_example if type(self.latest_prompt_example) is dict else json.loads(self.latest_prompt_example)\n            self.have_same_structure(prompt_example, response_dict, exempt_layers=exempt_layers)\n            \n    def have_same_structure(self, ground_turth_dict, pred_dict, layer=0, exempt_layers=[2,3,4,5,6,7,8,9,10]): \n        '''\n        eval if the return dict and sample dict have the same hierarchical structure\n        in default, only the 0,1 layer is checked\n        '''\n        if isinstance(ground_turth_dict, dict) and isinstance(pred_dict, dict):\n            # Check if both dictionaries have the same set of keys\n            # if set(ground_turth_dict.keys()) != set(pred_dict.keys()) and (layer not in exempt_layers):\n                # raise AssertionError(f'dict keys should be strictly conformed to {list(ground_turth_dict.keys())}')\n            if ( set(ground_turth_dict.keys()) - set(pred_dict.keys())) and (layer not in exempt_layers):\n                raise AssertionError(f'dict keys must contain all the following: {list(ground_turth_dict.keys())}')\n            \n            # Recursively check the structure of each key-value pair\n            if layer not in exempt_layers:\n                for key in ground_turth_dict:\n                    self.have_same_structure(ground_turth_dict[key], pred_dict[key], layer=layer+1, exempt_layers=exempt_layers)\n            # else:\n            #     # assume the order of values does not matter\n            #     for g_v, p_v in zip(ground_turth_dict.values(), pred_dict.values()):\n            #         self.have_same_structure(g_v, p_v, layer=layer+1, exempt_layers=exempt_layers)\n\n        # If either of the values is not a dictionary, treat as leaf node\n        assert type(ground_turth_dict) == type(pred_dict), f'the type of {pred_dict} should be {type(ground_turth_dict)}'\n    \n    @staticmethod\n    def extract_json_from_markdown(markdown_text):\n        # Regular expression to match the content inside ```json``` tags\n        pattern = r'```json\\n(.*?)```'\n        # Find all matches\n        matches = re.findall(pattern, markdown_text, re.DOTALL)\n        # If there are matches, return the first one\n        if matches:\n            # Remove leading and trailing whitespace and newlines\n            json_content = matches[0].strip()\n            return json_content\n        else:\n            return markdown_text\n        \n    def __repr__(self):\n        return f\"\"\"\n            Lasest prompt type: {self.latest_prompt_type}\n            Lasest Response: {self.latest_response}\n        \"\"\"\n    "}
{"type": "source_file", "path": "app/models/data_store.py", "content": "import os\nimport re\nimport sys\nimport traceback\nsys.path.append('./app')\nsys.path.append('../../')\n\nfrom collections import defaultdict\nimport copy\nimport json\nfrom typing import List, Dict, Any\nimport uuid\n\n# from ..utils.serialization import serialize\nfrom ..database.milvus_datastore import MilvusDataStore\nfrom ..global_config import MILVUS_HOST, MILVUS_PORT, MILVUS_INDEX_PARAMS\nfrom ..database.milvus_constants import ENTITY_SCHEMA\n\nENTITY_COLLECTION = 'entity_memory'\nLOCATION_COLLECTION = 'location_memory'\nTRANSACTION_COLLECTION = 'transaction_records'\n\nclass Memory:\n    def __init__(self, entity_name: str) -> None:\n        self.entity_name = entity_name\n        self.datastore = MilvusDataStore(\n            host=MILVUS_HOST,\n            port=MILVUS_PORT,\n            collection_name=self.build_collection_name(),\n            schema=ENTITY_SCHEMA,\n            index_params=MILVUS_INDEX_PARAMS\n        )\n        self.numeric_memory = defaultdict(float)\n        \n    def build_collection_name(self):\n        name = f\"{ENTITY_COLLECTION}_{self.entity_name}\"\n        name = re.sub(' ', '', name)\n        return name\n\n    def update_numeric_memory(self, value_change:float, name, alpha=0.6):\n        '''\n        moving average based\n        '''\n        if name not in self.numeric_memory:\n            self.numeric_memory[name] = value_change\n        else:\n            old_value = self.numeric_memory[name]\n            self.numeric_memory[name] = alpha * old_value + (1-alpha) * (value_change)\n        \n    def store(self, memory: Dict[str, Any]) -> None:\n        self.store_entity_memory(memory)\n        self.store_location_memory(memory)\n        self.store_transaction_memory(memory)\n\n\n    async def insert_milvus_memory(self, text:str, scale_dict: Dict[str, Any],data_store:MilvusDataStore=None) -> None:\n        '''\n        scale_dict:\n        {   \n            act_id,\n            act_name,\n            obj_id,\n            obj_name,\n            in_building_id,\n            in_building_name,\n            money,\n            emotion,\n            timestamp,\n            memory,\n        } \n        '''\n        try:\n            embedding = self.embeddings.embed_query(text)\n            print(len(embedding))\n            dict_to_insert = copy.deepcopy(scale_dict)\n            dict_to_insert.update({\n                \"id\": str(uuid.uuid1()), \n                \"emb\": embedding \n            })\n            #the dict should match the collection schema in milvus_constants.py\n            if data_store is None: data_store = self.character_milvus_data_store\n            response = data_store.insert_data( dict_to_insert )\n            # \"id\": str(uuid.uuid1()),\n            # \"character_id\": scale_dict['character_id'],\n            # \"character_name\": scale_dict['character_name'],\n            # \"in_building_id\": scale_dict['in_building_id'],\n            # \"in_building_name\": scale_dict['in_building_name'],\n            # \"money\": scale_dict[\"money\"],\n            # \"emotion\": scale_dict[\"emotion\"],\n            # \"timestamp\":scale_dict['timestamp'],\n            # \"surprise_level\": scale_dict[\"surprise_level\"],\n            # \"emb\": embedding,\n                \n            print(response)\n        except Exception as e:\n            traceback.print_exc()\n            if os.getenv('DEBUG'):\n                __import__('ipdb').set_trace()\n            print(e)\n            \n\n    def store_entity_memory(self, memory: Dict[str, Any]) -> None:\n        if 'people' in memory:\n            for name in list(memory['people'].keys()):\n                new_mem = copy.deepcopy(memory[\"people\"][name])\n                self.people[name].append(new_mem)\n\n                # if hasattr(self,'character_milvus_data_store'):\n                #     self.insert_milvus_memory(\n                #         memory=new_mem, #自己构建\n                #         data_store=self.character_milvus_data_store\n                #     )\n\n    def store_location_memory(self, memory: Dict[str, Any]) -> None:\n        if 'building' in memory:\n            for name, info in memory['building'].items():\n\n                self.building[name].append(info)\n\n                # if hasattr(self,'character_milvus_data_store'):\n                #     self.insert_milvus_memory(\n                #         memory=memory,  #自己构建\n                #         data_store=self.character_milvus_data_store\n                #     )\n\n    def store_transaction_memory(self, memory: Dict[str, Any]) -> None:\n        if 'records' in memory:\n            for name, info in memory['records'].items():\n                self.trade_records[name].append(info)\n\n                # if hasattr(self,'character_milvus_data_store'):\n                #     self.insert_milvus_memory(\n                #         memory=memory,  #自己构建\n                #         data_store=self.character_milvus_data_store\n                #     )\n    \n    @property\n    def impression_memory(self) -> Dict[str, Any]:\n        return {\n            \"people\": self.people,\n            \"building\": self.building,\n        }\n\n\n    def get_memory(self, main_category, name) -> Dict[str, Any]:\n        if main_category == 'people':\n            return self.get_people_memory(name)\n        elif main_category == 'building':\n            return self.get_building_memory(name) \n        else: \n            raise NotImplemented \n            \n\n    def get_people_memory(self, name: str, default=[]) -> Dict[str, Any]:\n        return self.people.get(name, default)\n    \n\n    def get_building_memory(self, name: str, default=[]) -> Dict[str, Any]:\n        return self.building.get(name, default)\n    \n    def get_records_memory(self, name: str) -> Dict[str, Any]:\n        return self.trade_records.get(name, [])\n    \n\n    def name_specific_memory_retrieve_from_milvus(self, obj_name: str, query:str,topk:int=4):\n        milvus_expression = f\"obj_name=='{obj_name}' && act_name=='{self.character_name}'\"\n        search_param = {\n                \"data\": [self.embeddings.embed_query(text=query)], # 要搜索的query的emb\n                \"anns_field\": \"emb\", # 要检索的向量字段\n                \"param\": {\"metric_type\": \"COSINE\"},\n                \"limit\": topk, # Top K \n                'output_fields': [\"text\",'timestamp'],   #自定义输出字段\n                'expr': milvus_expression,\n        }\n        search_result:SearchResult = self.character_milvus_data_store.vector_search(\n            search_param=search_param,\n        )\n        return search_result\n    \n    def buyer_specific_memory_retrieve_from_milvus(self, obj_name: str, query:str, topk:int=4):\n        milvus_expression = f\"buyer=='{obj_name}'\"\n        search_param = {\n                \"data\": [self.embeddings.embed_query(text=query)], # 要搜索的query的emb\n                \"anns_field\": \"emb\", # 要检索的向量字段\n                \"param\": {\"metric_type\": \"COSINE\"},\n                \"limit\": topk, # Top K \n                'output_fields': ['resource_id', 'seller', 'timestamp', 'market_price', 'emotion', 'like_score', 'expected_price', 'final_price'],   #自定义输出字段\n                'expr': milvus_expression,\n        }\n        search_result:SearchResult = self.trade_records_milvus_data_store.vector_search(\n            search_param=search_param,\n        )\n        return search_result\n\n    def to_json(self) -> Dict[str, Any]:\n        return {\n            \"people\": self.people,\n            \"experience\": self.experience,\n            \"building\": self.building,\n        }\n\n    def from_json(self, obj: Dict[str, Any]):\n        self.people = obj.get(\"people\", dict())\n        self.experience = obj.get(\"experience\", dict())\n        self.building = obj.get(\"building\", dict())\n\n    def __repr__(self) -> str:\n        return json.dumps(self.impression_memory)\n\nclass WorkingMemory:\n    # TODO: forget, different type of memory has different forget period, some will be forgotten one state later\n    def __init__(self) -> None:\n        self.wm = dict()\n    \n    def retrieve_by_name(self, name, default=None):\n        return self.wm.get(name, default)\n    \n    def store_memory(self, name, memory):\n        self.wm[name] = memory\n    \n    def forget_by_name(self, name):\n        if name in self.wm:\n            del self.wm[name]\n     \n    def serialize(self):\n        return self.wm\n    \n    def __repr__(self) -> str:\n        return f'{self.wm}'\n        \n        \nclass PeopleMemory:\n    def __init__(self, name: str, relationship: str, impression: str) -> None:\n        self.name = name\n        self.relationship = relationship\n        self.impression = impression\n        self.episodicMemory = list()\n\n    def add_episodic_memory(self, memory: str) -> None:\n        self.episodicMemory.append(memory)\n\n\nclass ExperienceMemory:\n    def __init__(self, plan: str, acts: List[str]) -> None:\n        self.plan = plan\n        self.acts = acts\n\n\nclass BuildingMemory:\n    def __init__(self, name, relationship, impression) -> None:\n        self.name = name\n        self.relationship = relationship\n        self.impression = impression\n        self.episodicMemory = list()\n\n    def add_episodic_memory(self, memory: str) -> None:\n        self.episodicMemory.append(memory)\n\n\n\nif __name__ == '__main__':\n    from langchain_openai import OpenAIEmbeddings\n    memory = Memory(character_id=1, character_name='1', embeddings=OpenAIEmbeddings(model=\"text-embedding-3-large\"))\n    import asyncio\n\n    #insert\n    asyncio.run(memory.insert_milvus_memory(\n        scale_dict = {\n            \"act_id\": '1',\n            \"act_name\": '1',\n            \"obj_id\": '00000',\n            \"obj_name\": 'test_building',\n            \"in_building_id\": '00000',\n            \"in_building_name\": 'test_building',\n            \"money\": 15.0,\n            \"emotion\": 'HAPPY',\n            \"emotion_level\": '0.8',\n            \"timestamp\":1234567890,\n        },\n        text= \"I ate shit.\",\n        data_store=memory.character_milvus_data_store\n    ))\n    asyncio.run(memory.insert_milvus_memory(\n        scale_dict = {\n            \"act_id\": '1',\n            \"act_name\": '1',\n            \"obj_id\": '00000',\n            \"obj_name\": 'test_building',\n            \"in_building_id\": '00000',\n            \"in_building_name\": 'test_building',\n            \"money\": 15.0,\n            \"emotion\": 'HAPPY',\n            \"emotion_level\": '0.8',\n            \"timestamp\":1234567890,\n        },\n        text = \"I drink milk.\",\n        data_store=memory.character_milvus_data_store\n    ))\n    # asyncio.run(memory.insert_milvus_memory(\n    #     memory = {\n    #         \"character_id\": '12345',\n    #         \"character_name\": 'test_character',\n    #         \"in_building_id\": '00000',\n    #         \"in_building_name\": 'test_building',\n    #         \"money\": 15.0,\n    #         \"emotion\": 'HAPPY',\n    #         \"timestamp\":1234567890,\n    #     },\n    #     data_store=memory.character_milvus_data_store\n    # ))\n\n    #query\n    retrieve_res = (memory.name_specific_memory_retrieve_from_milvus(\n        obj_name='test_building',\n        query=\"The bar is open.\",\n        topk=4\n    ))\n"}
{"type": "source_file", "path": "app/models/content_generator.py", "content": "\nimport copy\nimport traceback\n\nfrom langchain_openai import OpenAIEmbeddings\nfrom ..llm.llm_expends.dalle3 import DALLE3Caller\nimport asyncio\nimport uuid\nfrom PIL import Image\nfrom autogen.agentchat.contrib.img_utils import _to_pil, get_image_data\nfrom typing import List, Union\nfrom diskcache import Cache\nimport os\nfrom typing import Dict, Any\n\n# from .db_modules.milvus_collections import artwork_milvus_data_store\nfrom ..repository.artwork_repo import add_artwork_to_db\nfrom ..global_config import MILVUS_HOST, MILVUS_PORT, MILVUS_INDEX_PARAMS\nfrom ..database.milvus_datastore import MilvusDataStore\nfrom ..database.milvus_constants import ARTWORK_MILVUS_FILED_SCHEMA\n\nclass Drawing:\n    def __init__(self, id: str, owner: 'Character', image_url: str, description: str) -> None:\n        self.id: str = id\n        self.owner: 'Character' = owner\n        self.image_url: str = image_url\n        self.description: str = description\n        self.price = None\n        self.timestamp = owner.date_num\n        self.sanity_check()\n        self.owner.add_artwork('Drawing', self)\n        self.store_locally('./drawing')\n     \n    @classmethod\n    async def a_draw(cls, prompt: str, owner: 'Character', api_key:str=None) -> 'Drawing':\n        '''\n        Deprecated, use DALLEAgent.a_draw instead\n        '''\n        caller = DALLE3Caller(api_key=api_key)\n        llm_task = asyncio.create_task(caller.ask(prompt))\n        image_url = await llm_task\n        _id = str(uuid.uuid4())\n        print(f\"Drawing id: {_id}\")\n        try:\n            return cls(_id, owner, image_url, prompt)\n        except Exception as e:\n            print(f\"Error creating Drawing: {e}\")\n            return await Drawing.a_draw(prompt, owner, api_key) \n    \n    def sanity_check(self) -> bool:\n        assert type(self.id) == str,f' the type of id should be str, your type of {self.id} is {type(self.id)}  '\n        assert type(self.image_url) == str and self.image_url.startswith('http'),f' the type of image_url should be str and the url should start with http, your type of url is {type(self.image_url)}, and url is {self.image_url[:10]}  ' \n        assert type(self.description) == str,f' the type of description should be str, your type of {self.description} is {type(self.description)}  '\n    \n    def set_price(self, price: int) -> None:\n        self.price = price\n    \n    @property\n    def image(self) -> Image:\n        image_data = get_image_data(self.image_url)\n        return _to_pil(image_data)\n    \n    def store_locally(self, dir):\n        os.makedirs(dir, exist_ok=True)\n        self.image.save(f'{dir}/{self.id}.png')\n    \n    def __eq__(self, __value: Union['Drawing', str]) -> bool:\n        if isinstance(__value, Drawing):\n            return self.id == __value.id\n        elif isinstance(__value, str):\n            return self.id == __value\n        else:\n            raise ValueError(f\"Invalid comparison between Drawing and {type(__value)}\")\n        \n    def __getstate__(self) -> object:\n        state = self.__dict__.copy()\n        state['owner'] = self.owner.name\n        return state\n\n    def __setstate__(self, state: object) -> None:\n        self.__dict__.update(state)\n        \n\nclass DrawingList:\n    ARTWORK_COLLECTION_NAME = 'artwork_collection'\n    def __init__(self, owner: 'Character') -> None:\n        self.cache = Cache(os.path.join('.drawings', owner.name))\n        self.drawings: List[Drawing] = []\n        self.owner = owner\n        for d in self.cache:\n            drawing = self.cache[d]\n            drawing.owner = owner\n            self.drawings.append(drawing)\n            \n        if os.getenv('Milvus'):\n            self.embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n            # self.artwork_milvus_data_store = artwork_milvus_data_store\n            self.artwork_milvus_data_store = MilvusDataStore(\n                host=MILVUS_HOST,\n                port=MILVUS_PORT,\n                collection_name=self.ARTWORK_COLLECTION_NAME,\n                field_schema=ARTWORK_MILVUS_FILED_SCHEMA,\n                index_field='prompt_emb',\n                index_params= MILVUS_INDEX_PARAMS,\n            )\n    \n    def __len__(self) -> int:\n        return len(self.drawings)\n    \n    def __iter__(self):\n        return iter(self.drawings)\n    \n    def __str__(self) -> str:\n        return str([d.id for d in self.drawings])\n    \n    def __getitem__(self, index: int) -> Drawing:\n        return self.drawings[index]\n    \n    def __contains__(self, __value: object) -> bool:\n        if isinstance(__value, Drawing):\n            return __value in self.drawings\n        elif isinstance(__value, str):\n            return __value in [d.id for d in self.drawings]\n        else:\n            raise ValueError(f\"Invalid comparison between Drawing and {type(__value)}\")\n    \n    def add(self, drawing: Drawing):\n        self.cache.set(drawing.id, drawing)\n        self.drawings.append(drawing)\n        info_dict = {\n                \"id\": drawing.id,\n                \"resource_id\": drawing.id,\n                \"resource\":  drawing.id,\n                \"artwork_type\": \"Drawing\",\n                \"prompt\": drawing.description,\n                \"create_place_id\": self.owner.in_building_id,\n                \"create_place_name\": self.owner.in_building_name,\n                \"timestamp\": drawing.timestamp,\n                \"creator_money\": self.owner.money,\n                \"emotion\": self.owner.emotion.extreme_emotion_name,\n                \"emotion_level\": self.owner.emotion.extreme_emotion_value,\n                \"owner_id\": self.owner.guid,\n                \"owner_name\": self.owner.name,\n                \"price\": drawing.price if drawing.price is not None else 0,\n            }\n        add_artwork_to_db(info_dict=info_dict)\n        if os.getenv('Milvus'):\n            scale_dict = copy.deepcopy(info_dict)\n            \n            for k in scale_dict.keys():\n                assert scale_dict[k] is not None, f\"key {k} is None\"\n                if k == 'timestamp': continue\n                scale_dict[k] = str(scale_dict[k])\n                \n            self.insert_milvus_memory(prompt_text=drawing.description, scale_dict=scale_dict, data_store=self.artwork_milvus_data_store)\n        \n    def get(self, id: str):\n        for drawing in self.drawings:\n            if drawing.id == id:\n                return drawing\n        return None\n    \n    def remove(self, drawing: Union[Drawing, str]):\n        if drawing in self.drawings:\n            self.drawings.remove(drawing)\n            self.cache.pop(drawing.id)\n            return True\n        else:\n            return False\n        \n    def insert_milvus_memory(self, prompt_text:str, scale_dict: Dict[str, Any],data_store:MilvusDataStore=None) -> None:\n        try:\n            embedding = self.embeddings.embed_query(prompt_text)\n            if os.getenv('DEBUG'): print(f\"len of embedding {len(embedding)}\")\n            dict_to_insert = copy.deepcopy(scale_dict)\n            dict_to_insert.update({\n                # \"id\": str(uuid.uuid1()), \n                \"prompt_emb\": embedding \n            })\n            if data_store is None: data_store = self.artwork_milvus_data_store\n            response = data_store.insert_data( dict_to_insert )\n            \n            print(response)\n        except Exception as e:\n            traceback.print_exc()\n            if os.getenv('DEBUG'):\n                __import__('ipdb').set_trace()\n            print(e)\n            "}
{"type": "source_file", "path": "app/models/internal_dialogue.py", "content": "import asyncio\nimport random\nimport os\nimport re\nimport traceback\nfrom ..service.character_state.register import register\nfrom ..constants.prompt_type import PromptType\n\n@register(name=PromptType.INNER_MONOLOGUE, type=\"prompt\")\nclass MonologuePrompt:\n    '''\n    Based on different inputs, build different monologue prompts.\n    '''\n    PROMPT = '''\n        You're a character in a game.\n        Your name is {name}.\n        Your bio is {bio}.\n        \n        Your understanding of the world: {memory}\n        \n        Your internal status is: {internal_status}\n        \n        Your total plan is {BestPlan}\n        and you are now at {current_step}\n        \n        Your current emotion is: {emotion}.\n        \n        Please each generate an inner monologue for understanding of the world, your current internal status, your current plan and your memories. \n        Then tell me your emotion with an emoji. \n        \n        You must follow the following criteria:\n        * each inner monologue should be within 20 words.\n        * the monologue about understanding of the world should be related to your impression on other people or buildings.\n        * the monologue about your internal status should summarize from these values. Use the first person narration.\n        * the monologue about your plan should be related to what you are going to do. Use the first person narration.\n        * the monologue about your emotion should summarize your current feeling in a consistent tone. Use the first person narration.\n        \n        Here is an example\n        {EXAMPLE}\n    \n    '''\n    \n    EXAMPLE = {\n        \"monologue_understanding\": \"Jack is busy, I should not bother him.\",\n        \"monologue_status\": \"I'm feeling tired and hungry.\",\n        \"monologue_plan\": \"I should go to the library to find the book.\",\n        \"monologue_emotion\": \"Feeling so sad, god damn.\",\n        \"emoji\": \"😟\"\n    }\n    \n    def __init__(self, character) -> None:\n        self.character = character\n        self.prompt_type = PromptType.INNER_MONOLOGUE\n        # self.set_recordable_key(['monologue_understanding', 'monologue_status', 'monologue_plan', 'monologue_memory', 'monologue_emotion', 'emoji'])\n    \n    def create_prompt(self, **kwargs):\n        return self.format_attr(**kwargs)\n    \n    \n    def format_attr(self, **kwargs) -> str:\n        base_prompt = self.PROMPT\n        att_dict = dict()\n        att_dict.update({'memory': self.character.longterm_memory.to_json()})\n        att_dict.update(self.character.working_memory.serialize())\n        att_dict.update(kwargs)\n        \n        attributes = re.findall('\\{([a-zA-Z_]+)\\}', base_prompt) \n        for att in attributes:\n            if att in att_dict:\n                att_val = att_dict[att]\n            elif hasattr(self, att):\n                att_val = getattr(self, att)\n            elif hasattr(self.character, att):\n                att_val = getattr(self.character, att)\n            elif att in self.character.working_memory.wm: # TODO\n                att_val = self.character.working_memory.get(att)   \n            else:\n                raise  AssertionError(f'Missing attribute: {att} . Current prompt: {self.prompt_type}')\n            try:\n                base_prompt = base_prompt.replace('{'+att+'}', str(att_val))\n            except:\n                traceback.print_exc()\n                if os.getenv('DEBUG'):\n                    __import__('ipdb').set_trace()\n                pass        \n        \n        return base_prompt\n\nclass InnerMonologue():\n    \"\"\"\n    Character's inner monologue, decided by character's current perception, plans, memories or emotions.\n    The content is a dict\n    \"\"\"\n    def __init__(self, character):\n        self.character = character\n        self.content = dict()\n        self.prompt = MonologuePrompt(character)\n    \n    @property\n    def inner_monologue(self):\n        return self.content\n    \n    @property\n    def emoji(self):\n        if \"emoji\" in self.content:\n            return self.content[\"emoji\"]\n        return None\n    \n    def sample_monologue(self, size=2):\n        if self.content:\n            return random.sample(self.content.items(), size)\n    \n    def set_monologue(self, content):\n        self.content.update(content)\n        self.save_llm_response(content)\n        \n    def build_prompt(self):\n        return self.prompt.create_prompt(emotion=self.character.emotion.impression)\n    \n    def call_llm(self):\n        message = self.build_prompt()\n        self.save_llm_prompt(message)\n        self.llm_task = asyncio.create_task(self.character.a_process_then_reply(message=message, sender=self.character, restart=True))\n        self.llm_task.add_done_callback(lambda task: self.set_monologue(task.result()))\n        \n    def save_llm_prompt(self, prompt):\n        self.character.save_prompt(prompt, PromptType.INNER_MONOLOGUE, getattr(InnerMonologue, 'EXAMPLE', None))\n        \n    def save_llm_response(self, response):\n        self.character.save_response(response, PromptType.INNER_MONOLOGUE)"}
{"type": "source_file", "path": "app/models/resource_generator.py", "content": "import asyncio\nimport functools\nimport uuid\nfrom autogen import ConversableAgent, Agent\nimport os\nfrom typing import Union, Optional\nimport re\n\nfrom diskcache import Cache\nfrom openai import OpenAI\nfrom .drawing import Drawing, DrawingList\nfrom typing import Dict, List, Literal\nfrom ..llm.llm_expends.dalle3 import DALLE3Caller\n\n\n\nclass DALLEAgent(ConversableAgent):\n    def __init__(self, name, owner: \"Character\", llm_config: dict, **kwargs):\n        super().__init__(name, llm_config=llm_config, **kwargs)\n\n        try:\n            config_list = llm_config[\"config_list\"]\n            for cfg in config_list:\n                if 'api_key' in cfg and 'base_url' in cfg :\n                    api_key = cfg[\"api_key\"]\n                    base_url = cfg['base_url']\n                    break\n        except Exception as e:\n            api_key = os.getenv(\"OPENAI_API_KEY\")\n        self.api_key = api_key\n        self.owner = owner\n        self.client = OpenAI(api_key=api_key,base_url =base_url)\n        self.register_reply([Agent, None], DALLEAgent.generate_dalle_reply)\n        self.register_reply([Agent, None], DALLEAgent.a_generate_dalle_reply)\n        \n    def send(\n        self,\n        message: Union[Dict, str],\n        recipient: Agent,\n        request_reply: Optional[bool] = None,\n        silent: Optional[bool] = True,\n    ):\n        # override and always \"silent\" the send out message;\n        # otherwise, the print log would be super long!\n        super().send(message, recipient, request_reply, silent=silent)\n\n    def generate_dalle_reply(self, messages: Optional[List[Dict]], sender: Agent, config):\n        \"\"\"\n        Generate a reply using OpenAI DALLE call.\n        Add artwork to the DrawingList of the owner happens in __init__ of drawing\n        \"\"\"\n        if messages is None:\n            messages = self._oai_messages[sender]\n\n        prompt = messages[-1][\"content\"]\n        drawing = self.draw(prompt, api_key=self.api_key)\n        out_message = {'img_url': drawing.image_url,\n                       'img_id': drawing.id}\n        return True, out_message\n    \n    async def a_generate_dalle_reply(self,messages: Optional[List[Dict]], sender: Agent, config):\n        return await asyncio.get_event_loop().run_in_executor(\n            None, functools.partial(self.generate_dalle_reply, messages=messages, sender=sender, config=config)\n        )\n        \n    async def a_process_then_reply(self, message, sender: Agent, restart=True, silent=True):\n    # modified from ConversableAgent.a_receive()\n        self._prepare_chat(self,clear_history=restart)\n        self._process_received_message(message, sender, silent)\n        reply = await self.a_generate_reply(sender=sender)\n        return reply\n    \n    def draw(self, prompt: str, api_key:str=None) -> 'Drawing':\n        # caller = DALLE3Caller(api_key=api_key)\n        # llm_task = asyncio.create_task(caller.ask(prompt))\n        # image_url = await llm_task\n        image_url = dalle_call(client=self.client,\n            model=\"dall-e-3\",\n            prompt=prompt,\n            size=\"1024x1024\", \n            quality=\"standard\",\n            n=1,\n        )\n        _id = str(uuid.uuid4())\n        print(f\"Drawing id: {_id}\")\n        \n        try:\n            return Drawing(image_url= image_url, id= _id, description= prompt, owner= self.owner)\n        except Exception as e:\n            print(f\"Error creating Drawing: {e}\")\n            return self.draw(prompt, api_key) \n    \n    \n    def sanity_check(self, id, image_url, description) -> bool:\n        assert type(id) == str,f' the type of id should be str, your type of {self.id} is {type(self.id)}  '\n        assert type(image_url) == str and image_url.startswith('http'),f' the type of image_url should be str and start with http, your type of url is {type(image_url)}, and url is {image_url[:10]}  ' \n        assert type(description) == str,f' the type of description should be str, your type of {self.description} is {type(self.description)}  '\n         \n         \n         \ndef dalle_call(client: OpenAI, model: str, prompt: str, size: str, quality: str, n: int) -> str:\n    \"\"\"\n    Generate an image using OpenAI's DALL-E model and cache the result.\n\n    This function takes a prompt and other parameters to generate an image using OpenAI's DALL-E model.\n    It checks if the result is already cached; if so, it returns the cached image data. Otherwise,\n    it calls the DALL-E API to generate the image, stores the result in the cache, and then returns it.\n\n    Args:\n        client (OpenAI): The OpenAI client instance for making API calls.\n        model (str): The specific DALL-E model to use for image generation.\n        prompt (str): The text prompt based on which the image is generated.\n        size (str): The size specification of the image. \n        quality (str): The quality setting for the image generation.\n        n (int): The number of images to generate.\n\n    Returns:\n    str: The image data as a string, either retrieved from the cache or newly generated.\n\n    Note:\n    - The cache is stored in a directory named '.cache/'.\n    - The function uses a tuple of (model, prompt, size, quality, n) as the key for caching.\n    - The image data is obtained by making a secondary request to the URL provided by the DALL-E API response.\n    \"\"\"\n    # Function implementation...\n    cache = Cache(\".cache/\")  # Create a cache directory\n    key = (model, prompt, size, quality, n)\n    if key in cache:\n        return cache[key]\n    \n    # If not in cache, compute and store the result\n    response = client.images.generate(\n        model=model,\n        prompt=prompt,\n        size=size,\n        quality=quality,\n        n=n,\n    )\n    image_url = response.data[0].url\n    cache[key] = image_url\n    \n    return image_url"}
{"type": "source_file", "path": "app/models/scheduler.py", "content": "class Schedule:\n    def __init__(self) -> None:\n        self.steps = []\n        self.summary = None\n        self.current_index = 0\n\n    def advance(self):\n        self.current_index += 1\n        if self.is_complete:\n            return 'Schedule is complete.'\n        return self.steps[self.current_index]\n    \n    def set_steps(self, steps: list[str], summary: str, **kwargs) -> None:\n        self.current_index = 0\n        self.summary = summary\n        self.steps = steps\n    \n    @property\n    def current_step(self):\n        if self.is_empty:\n            return 'No schedule'\n        if self.is_complete:\n            return 'Schedule is complete.'\n        return self.steps[self.current_index] \n\n    @property\n    def is_empty(self):\n        return self.summary is None\n\n    @property\n    def is_complete(self):\n        if self.is_empty:\n            return False\n        return self.current_index >= len(self.steps)\n\n    def __repr__(self) -> str:\n        return f'''\n        Overview: {self.summary},\n        Steps: {self.steps},\n        Current: {self.current_step}\n    '''\n\nclass Task():\n    '''\n    Task class to store task details and progress\n    '''\n    PENDING, ACTIVE, PAUSED, DONE = 'pending', 'active', 'paused', 'done'\n    \n    def __init__(self, description: str, timing: str):\n        self.description = description\n        self.timing = timing\n        self.status = self.PENDING\n        self.valid_states = [self.PENDING, self.ACTIVE, self.PAUSED, self.DONE]\n        self.schedule: Schedule = None\n\n    def update_status(self):\n        if self.schedule is None:\n            self.status = self.PENDING\n        elif self.schedule.is_complete:\n            self.status = self.DONE\n\n    def set_status(self, status):\n        assert status in self.valid_states\n        self.status = status\n    \n    def update_schedule(self, schedule: Schedule):\n        self.schedule = schedule\n\n    def get_description(self):\n        return self.description\n\n    def get_timing(self):\n        return self.timing\n\n    def get_status(self):\n        return self.status\n\n    def get_schedule(self):\n        return self.schedule\n\n    def __repr__(self):\n        return f'Time: {self.timing}, Task: {self.description}\\n, Status: {self.status}, Schedule: {self.schedule}'\n\nclass Agenda():\n    '''\n    Plan is interrupted by a new event: store in the agenda\n    '''\n    def __init__(self):\n        self.agenda: dict[str, Task] = LimitedLengthDict(limit=20)\n                        # {'2021-10-01 14:00': \n                        #     Event(event='Meeting with Dorothy Johnson at 2:00 PM to discuss her role and performance at the Cafe',\n                        #         time='2021-10-01 14:00')\n                        # }\n        self.candidate_staus = ['not started', 'in progress', 'completed']\n    def add_event(self, event:str, time):\n        self.agenda[time] = Task(event=event, time=time)\n        \n    def check_date(self, date):\n        return self.agenda.get(date)\n    \n    @property\n    def incompleted_events(self):\n        events = []\n        for event in self.agenda.values():\n            if event.status is not event.COMPLETED:\n                events.append(event)\n        return events\n            \n    @property\n    def completed_events(self):\n        events = []\n        for event in self.agenda.values():\n            if event.status is event.COMPLETED:\n                events.append(event)\n        return events \n   \n    def serialize(self):\n        return self.agenda\n    \n    def __repr__(self):\n        return f'Agenda: {self.agenda}'\n\nfrom collections import UserDict\n\nclass LimitedLengthDict(UserDict):\n    def __init__(self, limit, *args, **kwargs):\n        self.limit = limit\n        super().__init__(*args, **kwargs)\n        self._ensure_limit()\n\n    def __setitem__(self, key, value):\n        super().__setitem__(key, value)\n        self._ensure_limit()\n\n    def _ensure_limit(self):\n        while len(self.data) > self.limit:\n            self.data.pop(next(iter(self.data)))\n            \nif __name__ == '__main__':\n    agenda = Agenda()\n    agenda.add_event(time='2021-10-01 14:00',\n                    event='Meeting with Dorothy Johnson at 2:00 PM to discuss her role and performance at the Cafe',\n                       )\n    print(agenda)"}
{"type": "source_file", "path": "app/models/preference_model.py", "content": "from typing import Dict\nimport random\n\nclass PreferenceModel:\n    categories = [\n        \"style_a\", \"style_b\", \"style_c\", \"style_d\", \n        \"style_e\", \"style_f\", \"style_g\", \"style_h\", \n        \"style_i\", \"style_j\", \"style_k\", \"style_l\"\n    ]\n    \n    def __init__(self, preferences: Dict[str, float]={}) -> None:\n        self.preferences = {}\n        for key in self.categories:\n            self.preferences[key] = preferences.get(key, random.uniform(0, 10))\n            assert isinstance(self.preferences[key], float) and 0 <= self.preferences[key] <= 10, \"preference values must be between 0 and 10\"\n        \n    @property\n    def top_preferences(self, count=1) -> str:\n        '''\n        Return the top-N preferred categories\n        '''\n        return ', '.join(sorted(self.preferences, key=self.preferences.get, reverse=True)[:count])\n    \n    @property\n    def preference_scores(self) -> Dict[str, float]:\n        return self.preferences\n    \n    def update_preferences(self) -> None:\n        for key in self.preferences:\n            self.preferences[key] = min(10, max(0, self.preferences[key] + random.uniform(-0.2, 0.2)))\n            \n    "}
{"type": "source_file", "path": "app/models/state_manager.py", "content": "from collections import defaultdict\nfrom typing import Dict\nimport random\nimport json\n\nclass StateManager:\n    \"\"\"Manages internal states with numerical values between 0 and 10.\n    \"\"\"\n    state_options = [\"positive_a\", \"positive_b\", \"negative_a\", \"neutral_a\", \"negative_b\", \"negative_c\", \"negative_d\", \"positive_c\"]\n    positive_states = [\"positive_a\", \"positive_b\", \"positive_c\", \"neutral_a\"]\n    negative_states = [\"negative_a\", \"neutral_a\", \"negative_b\", \"negative_c\", \"negative_d\"]\n    \n    def __init__(self, initial_state: Dict[str, float]=None, update_rate=0.7, decay_rate=0.01) -> None:\n        self.states = dict()\n        self.initialize_states()\n        if initial_state is not None:\n            for k, v in initial_state.items(): \n                v = float(v)\n                assert 0 <= v <= 10, f\"states should be between 0 and 10. Current state is {initial_state}\"\n                if k in self.state_options:\n                    self.states[k] = v\n        self.update_rate = update_rate\n        self.decay_rate = decay_rate\n        self.significant_events = defaultdict(dict)\n    \n    def initialize_states(self):\n        \"\"\"\n        Initialize states with balanced values.\n        \"\"\"\n        for key in self.state_options:\n            if random.random() < 0.1:\n                self.states[key] = random.randint(6, 10)\n            elif random.random() < 0.8:\n                self.states[key] = random.randint(0, 4)\n            else:\n                self.states[key] = random.randint(4, 6)\n    \n    def update_states(self, state_changes: list[dict]=None) -> None:\n        if state_changes is None:\n            self.apply_decay()\n        else:\n            for change in state_changes:\n                state_type, intensity_delta, trigger = change.get('state'), change.get('change'), change.get('reason')\n                try:\n                    self.update_single_state(state_type, intensity_delta, trigger)\n                except:\n                    pass\n                    \n    def update_single_state(self, state_type: str, intensity_delta: float, trigger: str = None) -> None:\n        assert state_type in self.state_options, f\"state type must be in state_options. Current type is {state_type}\"\n        assert -5 <= intensity_delta <= 5, f\"intensity change must be between -5 and 5. Current change is {intensity_delta}\"\n        self.states[state_type] += intensity_delta\n        if trigger:\n            self.record_significant_event(state_type, intensity_delta, trigger)\n    \n    def apply_decay(self, state_type=None, custom_rate=None):\n        if custom_rate is None: \n            custom_rate = self.decay_rate\n        \n        if state_type is None:\n            for key in self.states.keys():\n                if self.states[key] > 7:\n                    self.states[key] -= self.states[key] * random.uniform(0, 10) * custom_rate\n                else:\n                    self.states[key] += self.states[key] * random.randrange(0, 5) * custom_rate\n        else:\n            if self.states[state_type] > 7:\n                self.states[state_type] -= self.states[state_type] * random.randrange(-5, 5) * custom_rate\n            else:\n                self.states[state_type] += self.states[state_type] * random.randrange(-5, 5) * custom_rate\n                \n        for state in self.states.keys():\n            self.states[state] = max(0, min(10, round(self.states[state], 2)))\n                \n    def record_significant_event(self, state_type, state_delta, trigger: str):\n        prev_delta = self.significant_events.get(state_type, dict()).get('delta', 1)\n        if prev_delta < state_delta:\n            self.significant_events[state_type].update({'delta': state_delta, 'trigger': trigger})\n        else:\n            self.significant_events[state_type].update({'delta': min(1, prev_delta-self.decay_rate*50), 'trigger': trigger})\n            \n    @property\n    def current_states(self) -> Dict[str, float]:\n        return self.states\n    \n    @property\n    def most_significant_event(self):\n        highest_delta = 0\n        significant_trigger, trigger_state = '', ''\n        for key in self.significant_events.keys():\n            delta = self.significant_events[key].get('delta', 0)\n            if delta > highest_delta:\n                highest_delta = delta\n                significant_trigger = self.significant_events[key].get('trigger', None)\n                trigger_state = key\n                assert significant_trigger is not None, f'trigger is None, event is {significant_trigger}'\n        return significant_trigger, trigger_state\n         \n    @property\n    def dominant_state(self) -> dict:\n        dominant = max(self.states, key=self.states.get)\n        return {dominant: f\"{self.states[dominant]}/10\"}\n\n    @property\n    def dominant_state_type(self) -> dict:\n        return max(self.states, key=self.states.get)\n\n    @property\n    def dominant_state_value(self) -> dict:\n        dominant = max(self.states, key=self.states.get)\n        return self.states[dominant]\n    \n    def __repr__(self) -> str:\n        return f\"\"\"States: {self.states}\n                Dominant State: {self.dominant_state}\"\"\"\n                \n                \nif __name__ == '__main__':\n    print(StateManager().state_options)"}
{"type": "source_file", "path": "app/models/location.py", "content": "from datetime import datetime\nimport inspect\nimport json\nimport os\nfrom pathlib import Path\nimport random\nimport dill\nfrom typing import Optional, Union\nfrom autogen import filter_config, ConversableAgent, AssistantAgent, UserProxyAgent, config_list_from_json, Agent\n\nfrom app.models.base_agent import SimsAgent\nfrom app.utils.load_oai_config import plug_api_to_cfg, register_callback\nfrom app.utils.save_object import find_instance_specific_data_attrs\nfrom ..service.character_state.register import FuncName2Registered\nfrom ..utils.serialization import serialize\nfrom ..utils.globals import RESOURCE_SPLITER\nfrom config import cfg_tmplt\n\nclass Building(SimsAgent):\n    '''\n    description: texts describing building , sys prompt for building\n    building instruction: text as part of sys prompt, to guide the tougue of the building\n    equipment instruction: texts to guide the usage, to start conversation with character. It is the first message for init_chat\n    equipments: {nameA: { 'instruction': xx }, nameB:{ 'instruction': xx }}\n    jobs: {jobA: { 'description': xx, 'salary': xx, 'num_positions': xx }, jobB: { 'description': xx, 'salary': xx, 'num_positions': xx }\n    '''\n    DEFAULT_SYS_PROMPT = \"\"\"\n    You are an operational equipment in a building.\n    * If the customer tries to purchase something, ask for their current money and relvant information and estimate the result after purchase.\n    * If the customer tries to haggle ask for the price they want and estimate the result after haggling\n    * If the customer tries to use a equipment, guid them how to use it and estimate the equipment status after use.\n    * Do not provide any information beyond the status of the building and equipments. If the customer is asking about the information you do not know, reject the request.\n    * Try your best to temptate the customer to spend more money.\n    * When you are in a dialogue, you can stop it at any time by saying 'TERMINATE'.\n    \"\"\"\n       \n    def __init__(self, id:int, name, llm_cfg, xMin, yMin, xMax, yMax, \n                 description, instruction,\n                 equipments: dict[str, str]=None, \n                 jobs:dict[str, dict] = None, \n                 max_consecutive_auto_reply=10, \n                 money= 0, \n                 save_dir=None,\n                 map = None,\n                 **kwargs) -> None:\n        self.guid = id\n        self.xMin = xMin\n        self.yMax = yMax\n        self.xMax = xMax\n        self.yMin = yMin\n        self.money = money\n        self.instruction = instruction\n        self.save_dir = Path(save_dir) / name\n        self.map = map\n        assert RESOURCE_SPLITER not in name, f'building name should not contain {RESOURCE_SPLITER}, your building name: {name}'\n                \n        register_callback(llm_cfg, guid=self.guid, prefix=f'building_{name}')\n        config_list = plug_api_to_cfg(cfg_tmplt, **llm_cfg) \n        cfg_ls = filter_config(\n                config_list=config_list, \n                filter_dict={\n                    \"model\": [  \"gpt-4-0125-preview\", \"gpt-3.5-turbo-0125\",\"deepseek-chat\"],\n                    # \"tag\": [\"gpt-3.5-turbo-0125-official\", \"gpt-4-0125-preview-official\"]\n                }\n            )\n        llm_cfg = \\\n                {\n                    \"config_list\": cfg_ls,\n                    \"seed\": 35,\n                    \"temperature\": 0.5\n                }\n            \n        super().__init__(\n            name = name,\n            llm_config = llm_cfg,\n            human_input_mode= 'NEVER',\n            max_consecutive_auto_reply = max_consecutive_auto_reply,\n            description=description,\n        )\n        self.job_positions: dict[str, Job] = self.add_jobs(jobs) if jobs else dict()\n        self.equipments:dict[str, InBuildingEquip] = self.add_equipments(equipments) if equipments else dict()\n        self.update_system_message(self.build_sys_message())\n    \n    @classmethod\n    def decode_from_json(self, **kwargs):\n        return Building(**kwargs)   \n    \n    @property\n    def position(self):\n        return (self.xMin, self.yMin, self.xMax, self.yMax) \n    \n    @property\n    def random_pos_inside(self):\n        while True:\n            x, y = random.randint(self.xMin, self.xMax), random.randint(self.yMin, self.yMax)\n            if self.map[x][y] == 1:\n                break                \n        return (x, y)\n   \n    @property\n    def internal_status(self):\n        return f'balance: {self.balance}'\n\n    @property\n    def modifiable_status(self):\n        return ['description']\n    \n    def modify_internal_properties(self, prop):\n        for key, val in prop.items():\n            if key in self.modifiable_status:\n                setattr(prop, key, val)\n    \n    def cordinate_in_building(self, x:int, y:int):\n        return (self.xMin <= x) and (x <= self.xMax) and (self.yMin <= y) and (y <= self.yMax  )\n    \n    def add_equipments(self, equipments):\n        return dict( ( f'{self.name}{RESOURCE_SPLITER}{eqp[\"name\"]}', InBuildingEquip.build(inbuilding=self, **eqp)) for eqp in equipments.values())\n    \n    def update_equipments(self, equipments):\n        for eqp in equipments.values():\n            name = f'{self.name}{RESOURCE_SPLITER}{eqp[\"name\"]}'\n            if self.equipments.get(name) is None:\n                self.equipments.update({name:  InBuildingEquip.build(inbuilding=self, **eqp)}) \n            \n            self.equipments[name].same_itmes_guid_inbuilding.append(eqp['guid']) \n            \n    def add_jobs(self, jobs):\n        return dict(( f'{self.name}{RESOURCE_SPLITER}{job_name}', Job.build(inbuilding=self.name, **job_des)) for job_name, job_des in  jobs.items())\n    \n    def update_jobs(self, jobs):\n        self.job_positions.update(self.add_jobs(jobs))\n    \n    @property\n    def occupied_jobs(self):\n        if self.job_positions is None:\n            return []\n        return [job for job in self.job_positions.values() if len(job.applicants) ]\n    \n    @property\n    def available_jobs(self):\n        if self.job_positions is None:\n            return None\n        ava_jobs = [job for job in self.job_positions.values() if not job.occupied]\n        return ava_jobs\n        \n    @property\n    def available_equipments(self):\n        return [ eqp for eqp in self.equipments.values() ]\n        \n    def build_sys_message(self):\n        return f''' {self.DEFAULT_SYS_PROMPT} the description of the building: {self.description}. \n            Open jobs in this building: {self.available_jobs} \n            The equipments in the building: {self.available_equipments}'''\n    \n    def equipment_instr(self, equip_name):\n        return f\"{self.equipments[equip_name].instruction} Current status: {self.equipments[equip_name].organize_status()} in the building {self.name}\"\n    \n    def register_equip_functions(self, equipment:'InBuildingEquip'):\n        if equipment.functions:\n            for func in equipment.functions:\n                self.register_callable_tools(func)\n    \n    def clean_equipment_functions(self, equipment:'InBuildingEquip'):\n        if equipment.functions:\n            for func in equipment.functions:\n                self.callable_tools.remove(func)\n    \n    def encode_to_json(self) -> json:\n        return json.dumps(serialize(self))\n    \n    def save_self_locally(self):\n        save_path = None\n        if self.save_dir:\n            os.makedirs(f'{self.save_dir}', exist_ok=True)\n            \n            attr_to_save = find_instance_specific_data_attrs(self)\n            attr_to_save = attr_to_save + ['name']\n            dict2save = dict( ((attr, getattr(self, attr)) for attr in attr_to_save))\n            \n            # Keep the number of files less than K\n            files = os.listdir(self.save_dir)\n            files.sort(key=lambda x: os.path.getctime(os.path.join(self.save_dir, x)))  # Sort files by creation time\n            while len(files) >= 10: \n                os.remove(os.path.join(self.save_dir, files[0]))\n                files.pop(0)\n            \n            save_path = f'{self.save_dir}/building_{self.name}_{datetime.now().strftime(\"%m%d%H%M%S\")}.dill'\n            with open(save_path, 'wb') as f:\n                dill.dump(dict2save, f)\n        \n        return save_path\n    \n    def load_from_local(self, file_path):\n        print(f'load Building from {file_path}')\n        with open(file_path, 'rb') as f:\n            attr_dict = dill.load(f)\n        for key, value in attr_dict.items():\n            if key in ['name','save_dir']: continue\n            setattr(self, key, value)\n            \n    def __repr__(self):\n        return self.name\n            \nclass BuildingList:\n    def __init__(self):\n        self.buildings: list[Building] = []\n\n    def add_building(self, building):\n        self.buildings.append(building)\n\n    def get_building_name(self):\n        return [building.name for building in self.buildings]\n\n    def encode_to_json(self) -> json:\n        dicts = [serialize(building) for building in self.buildings]\n        return dicts\n\n    def get_building_by_id(self, building_id):\n        for building in self.buildings:\n            if building.name == building_id:\n                return building\n        return None\n\n    def get_building_by_name(self, building_name):\n        for building in self.buildings:\n            if building.name == building_name:\n                return building\n        return None\n    \n    def get_building_by_pos(self, x, y):\n        for building in self.buildings:\n            if building.cordinate_in_building(x, y):\n                return building\n        return None\n\n    def get_building_descriptions(self):\n        return { building.name:building.description for building in self.buildings}\n    \n    def save_locally(self):\n        for building in self.buildings:\n            building.save_self_locally()\n    \nclass InBuildingEquip:\n    def __init__(self, name:str, instruction:str, inbuilding=None, x:int=None, y:int=None, interactable = False, status: str = None, functions: Union[str, callable, None]= None, other_status: dict[str,str] = None, **kwargs) -> None:\n        assert RESOURCE_SPLITER not in name, f'equipment name should not contain {RESOURCE_SPLITER}, your building name: {name}'\n        self.name = name #f'{inbuilding}{EQUIPSPLITER}{name}'\n        self.guid_on_name = hash(name) % 100000\n        self.x = x\n        self.y = y\n        self.inbuilding = inbuilding\n        if inbuilding is None:\n            assert self.x is not None and self.y is not None, 'If inbuilding is not provided, the x and y should be provided'\n        if any( pos is None for pos in [self.x, self.y] ):\n            assert inbuilding, 'If x or y is not provided, the inbuilding should be provided'\n            \n        self.instruction = instruction\n        self.same_itmes_guid_inbuilding = []\n        self.interactable = interactable\n        \n        self.status = status\n        self.modifiable_status = ['status']\n        if self.interactable: assert self.status\n        if other_status:\n            self.__dict__.update(other_status)\n            self.modifiable_status += list(other_status.keys())\n        if functions:\n            if type(functions) is list:\n                self.functions = [ fc if callable(fc) else FuncName2Registered[fc] for fc in functions ]\n            elif type(functions) is str:\n                self.functions = [functions] if callable(functions) else [FuncName2Registered[functions]]\n        else:\n            self.functions = None\n            \n        self.add_functions_to_instruction()\n        \n    def add_functions_to_instruction(self):\n        if self.functions:\n            meta_inst ='You can use the following tools by return corresponding structured response:\\n'\n            for func in self.functions:\n                sig = inspect.signature(func)\n                name = func.__name__\n                struct_response = {\n                    \"content\":{\n                        \"tool_call\": name,\n                    }\n                }\n                for para in sig.parameters:\n                    if para not in  ['self', 'sender']:\n                        struct_response['content'][para] = 'your value'\n                    \n                meta_inst += f'{name}: {struct_response}\\n'\n            \n            self.instruction += meta_inst\n        \n    @classmethod\n    def build(cls, name, instruction, **kwargs):\n        return cls(name, instruction, **kwargs)\n\n    def set_building(self, building:Building):\n        if building.cordinate_in_building(self.x, self.y):\n            self.inbuilding = building\n            return True\n        return False\n\n    def random_choose(self):\n        '''\n        choose an item from the same items in the building\n        '''\n        assert len(self.same_itmes_guid_inbuilding) > 0, f'No same items in the building {self.inbuilding.name} for {self.name}'\n        rd_idx = random.randint(0, len(self.same_itmes_guid_inbuilding)-1)\n        return self.same_itmes_guid_inbuilding[rd_idx]\n    \n\n    def organize_status(self):\n        return {key: getattr(self, key) for key in self.modifiable_status} \n    \n    def modify_internal_properties(self, prop):\n        for key, val in prop.items():\n            if key in self.modifiable_status:\n                setattr(self, key, val)\n    \n    def __repr__(self) -> str:\n        return f'''\n        name: {self.name},\n        in_building: {self.inbuilding},\n        instruction: {self.instruction}\n    '''\n    \nclass Job:\n    def __init__(self, name, description, salary, inbuilding, num_positions) -> None:\n        self.name = name\n        self.description = description\n        self.salary = salary\n        self.inbuilding = inbuilding\n        self.applicants = []\n        self.num_positions = 1 # default 1 position\n    \n    @classmethod\n    def build(cls, name, description, salary, inbuilding, num_positions, **kwargs):\n        return cls(name, description, salary, inbuilding, num_positions)\n    \n    @property\n    def occupied(self):\n        return len(self.applicants) >= self.num_positions\n    \n    @property\n    def open_positions(self):\n        return -len(self.applicants) + self.num_positions \n    \n    def add_job_des_to_agent_messge(self, agent: ConversableAgent):\n        '''\n        add job description into the system message of the character\n        '''\n        message = f'Your Job: {self.name}, Description: {self.description}, Salary: {self.salary}'\n        agent.update_system_message( agent.system_message + message)\n    \n    def __repr__(self) -> str:\n        return f'''\n        name: {self.name},\n        description: {self.description},\n        in_building: {self.inbuilding},\n        salary: {self.salary},\n        open_positions: {self.open_positions}\n    '''\n    \n    def add_applicant(self, applicant):\n        self.applicants.append(applicant)\n        \n    def remove_applicant(self, applicant):\n        self.applicants.remove(applicant)\n        \n    def to_json(self):\n        return json.dumps(serialize(self))\n    \n    @classmethod\n    def decode_from_json(cls, **kwargs):\n        return cls(**kwargs)"}
