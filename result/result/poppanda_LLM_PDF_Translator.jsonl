{"repo_info": {"repo_name": "LLM_PDF_Translator", "repo_owner": "poppanda", "repo_url": "https://github.com/poppanda/LLM_PDF_Translator"}}
{"type": "source_file", "path": "batch.py", "content": "import warnings\n\nwarnings.filterwarnings(\"ignore\")\nimport server\nimport os\nfrom pathlib import Path\nimport tempfile\nfrom loguru import logger\nimport sys\nfrom utils.api_utils import TranslateRequest\n\nif __name__ == \"__main__\":\n    translator = server.TranslateApi()\n    pdf_dirs = sys.argv[1:]\n    try:\n        files = []\n        for pdf_dir in pdf_dirs:\n            files.extend(list(os.scandir(pdf_dir)))\n    except Exception as e:\n        print(e)\n        exit(1)\n    for file in files:\n        if file.is_dir():\n            files.extend(list(os.scandir(file.path)))\n        elif file.is_file() and file.name.endswith(\".pdf\"):\n            if file.name.endswith(\"_translated.pdf\") or os.path.exists(\n                file.path.replace(\".pdf\", \"_translated.pdf\")\n            ):\n                logger.info(f\"Skip {file.path}\")\n                continue\n            logger.info(f\"Translating {file.path}\")\n            req = TranslateRequest(\n                pdf_path=file.path,\n                temp_output_dir=translator.temp_dir_name,\n                from_lang=\"English\",\n                to_lang=\"Chinese\",\n                translate_all=True,\n                p_from=0,\n                p_to=0,\n                output_file_path=file.path.replace(\".pdf\", \"_translated.pdf\"),\n                add_blank_page=True,\n            )\n            \n            translator._translate_pdf(req)\n            # response = translator._translate_pdf(\n            #     pdf_path=file.path,\n            #     output_dir=translator.temp_dir_name,\n            #     from_lang=\"English\",\n            #     to_lang=\"Chinese\",\n            #     translate_all=True,\n            #     p_from=0,\n            #     p_to=0,\n            #     output_file_path=file.path.replace(\".pdf\", \"_translated.pdf\"),\n            #     add_blank_page=True,\n            # )\n"}
{"type": "source_file", "path": "__init__.py", "content": ""}
{"type": "source_file", "path": "modules/layout/ditod.py", "content": "import cv2\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom pdf2image import convert_from_bytes, convert_from_path\nfrom .base import LayoutBase \nfrom utils import LayoutAnalyzer\n\nclass DiTLayout(LayoutBase):\n    def init(self, cfg: dict):\n        self.layout_model = LayoutAnalyzer(\n            model_root_dir= Path(\"models/unilm\"), device=cfg['device']\n        )\n\n        self.DPI = cfg['DPI'] if 'DPI' in cfg else 200\n \n    def get_layout(self, pdf_path_or_bytes: str, p_from, p_to) -> str:\n        \n        if isinstance(pdf_path_or_bytes, Path):\n            pdf_images = convert_from_path(pdf_path_or_bytes, dpi=self.DPI)\n        else:\n            pdf_images = convert_from_bytes(pdf_path_or_bytes, dpi=self.DPI)\n\n        data = []\n        images = []\n\n        for i, image in tqdm(enumerate(pdf_images)):\n            if i < p_from: continue\n            if i > p_to and p_to != 0: break\n\n            result = self.get_single_layout(image)\n            images.append(image)\n            data.append(result)\n        return data, images\n    \n    def get_single_layout(self, image):\n        img = np.array(image, dtype=np.uint8)\n        # NOTE: delete RGB2BGR operation\n        result = self.layout_model(img) \n        return result\n                "}
{"type": "source_file", "path": "modules/translate/LLMTranslateBase.py", "content": "from abc import ABC, abstractmethod\nfrom tqdm import tqdm\nfrom typing import List\nfrom utils.layout_model import Layout\nfrom threading import Thread\nfrom .base import TranslateBase\nfrom openai import OpenAI\nfrom loguru import logger\nfrom textdistance import levenshtein\n\n\nlangs = [\n    \"Albanian\",\n    \"Arabic\",\n    \"Armenian\",\n    \"Awadhi\",\n    \"Azerbaijani\",\n    \"Bashkir\",\n    \"Basque\",\n    \"Belarusian\",\n    \"Bengali\",\n    \"Bhojpuri\",\n    \"Bosnian\",\n    \"Brazilian Portuguese\",\n    \"Bulgarian\",\n    \"Cantonese (Yue)\",\n    \"Catalan\",\n    \"Chhattisgarhi\",\n    \"Chinese\",\n    \"Croatian\",\n    \"Czech\",\n    \"Danish\",\n    \"Dogri\",\n    \"Dutch\",\n    \"English\",\n    \"Estonian\",\n    \"Faroese\",\n    \"Finnish\",\n    \"French\",\n    \"Galician\",\n    \"Georgian\",\n    \"German\",\n    \"Greek\",\n    \"Gujarati\",\n    \"Haryanvi\",\n    \"Hindi\",\n    \"Hungarian\",\n    \"Indonesian\",\n    \"Irish\",\n    \"Italian\",\n    \"Japanese\",\n    \"Javanese\",\n    \"Kannada\",\n    \"Kashmiri\",\n    \"Kazakh\",\n    \"Konkani\",\n    \"Korean\",\n    \"Kyrgyz\",\n    \"Latvian\",\n    \"Lithuanian\",\n    \"Macedonian\",\n    \"Maithili\",\n    \"Malay\",\n    \"Maltese\",\n    \"Mandarin\",\n    \"Mandarin Chinese\",\n    \"Marathi\",\n    \"Marwari\",\n    \"Min Nan\",\n    \"Moldovan\",\n    \"Mongolian\",\n    \"Montenegrin\",\n    \"Nepali\",\n    \"Norwegian\",\n    \"Oriya\",\n    \"Pashto\",\n    \"Persian (Farsi)\",\n    \"Polish\",\n    \"Portuguese\",\n    \"Punjabi\",\n    \"Rajasthani\",\n    \"Romanian\",\n    \"Russian\",\n    \"Sanskrit\",\n    \"Santali\",\n    \"Serbian\",\n    \"Sindhi\",\n    \"Sinhala\",\n    \"Slovak\",\n    \"Slovene\",\n    \"Slovenian\",\n    \"Ukrainian\",\n    \"Urdu\",\n    \"Uzbek\",\n    \"Vietnamese\",\n    \"Welsh\",\n    \"Wu\",\n]\n\n\nclass LLMTranslateBase(TranslateBase):\n    def init(self, cfg: dict):\n        self.client: OpenAI = self.init_client(cfg)\n        self.model = cfg[\"model\"]\n        self.from_lang = None\n        self.to_lang = None\n        self.check_response = None\n\n    @abstractmethod\n    def init_client(self, cfg: dict) -> OpenAI:\n        pass\n\n    def get_languages(self):\n        return langs\n\n    def get_response(self, messages: list):\n        while True:\n            try:\n                response = self.client.chat.completions.create(\n                    model=self.model, messages=messages\n                )\n                return response.choices[0].message.content\n            except Exception as e:\n                logger.warning(f\"Failed to get response: {e}, retrying...\")\n\n    def reformat_text(self, text):\n        sys_prompt = \"\"\"You are a text format checker now. The user will give you some reformatting tasks, you should just complete the task without any additional response. Return the result only.\"\"\"\n        prompt = (\n            \"\"\"I will give you some text which are recognized as list. But there are no \\\\n(newline) symbols in the text. What I want you do is reformatting the text with adding newline symbol into the text.\n        Here is the text (pay attention to the numbering):\\n\"\"\"\n            + text\n        )\n        trial_time = 0\n        while True:\n            response: str = self.get_response(\n                [\n                    {\"role\": \"system\", \"content\": sys_prompt},\n                    {\"role\": \"user\", \"content\": prompt},\n                ]\n            )\n            _response = response.replace(\"\\n\", \"\")\n            _text = text.replace(\"\\n\", \"\")\n            if len(_response) != len(_text):\n                dist = levenshtein.distance(_response, _text)\n                if dist < max(len(text) * 0.05, 5):\n                    break\n                logger.warning(\n                    f\"Reformatting the text again(trial time {trial_time} / 3)\"\n                )\n                trial_time += 1\n                if trial_time == 3:\n                    response = text\n                    break\n            else:\n                break\n        return response\n\n    def model_check(self, text, translation):\n        sys_prompt = f\"You are a judger of {self.from_lang}-to-{self.to_lang} translation. Please check the translation of the following text and tell the user if the translation is correct.\\nThe translation request is:\\n- Keep all special characters / HTML tags / links as in the source text.\\n- Return only {self.to_lang} translation.\\n- The text may contain multiple lines.\\nYou should check the translation by the rules before.\\n And the judge request for you is:\\n- If the translation is correct, answer 'correct' only, without any other contents.\\n- If the translation is incorrect, asnwer 'incorrect' and provide the reason.\"\n        user_prompt = f\"<The text is> {text}\\n<The translation is> {translation}\"\n        response = self.get_response(\n            [\n                {\"role\": \"system\", \"content\": sys_prompt},\n                {\"role\": \"user\", \"content\": user_prompt},\n            ]\n        )\n        if \"incorrect\" in response:\n            self.check_response = response\n            return False\n        elif \"correct\" in response:\n            return True\n        else:\n            logger.error(f\"Unexpected response: {response}\")\n            return False\n\n    def check_translation(self, text, translation):\n        splitted_text = [line for line in text.split(\"\\n\") if line != \"\"]\n        splitted_translation = [line for line in translation.split(\"\\n\") if line != \"\"]\n        if len(splitted_text) != len(splitted_translation):\n            return self.model_check(text, translation)\n        return True\n\n    def _check_reference_once(self, text):\n        prompts = [\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are a content checker now. The user will give you some reference checking tasks, you should just complete the task without any additional response.\\nMore specifically:\\n- references usually contain a list of links or citations. Some times they are listed with numbers or bullet points.\\n- answer \"yes\" if the text is a reference, \"no\" if it is not.\\n- do not add any additional information to the text.\\n\\nFor example:\\n\"1. https://example.com\" -> yes\\n\"[81] Qiyang Zhang, Xiang Li, Xiangying Che, Xiao Ma, Ao Zhou, Mengwei Xu, Shangguang Wang, Yun Ma, and Xuanzhe Liu. A comprehensive benchmark of deep learning libraries on mobile devices. In Proceedings of ACM WWW, 2022.\\n[82] Pengfei Zhou, Yuanqing Zheng, and Mo Li. How long to wait? predicting bus arrival time with mobile phone based participatory sensing. Proceedings of ACM MobiSys, 2012.\" -> yes\\nNote that all you output should be exactly one \"yes\" or \"no\".\"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Please check if the following text is a reference or not, remember to answer 'yes' or 'no' first:\\n\\n{text}\",\n            },\n        ]\n        response = self.get_response(prompts)\n        if \"yes\" in response.lower():\n            return True\n        elif \"no\" in response.lower():\n            return False\n        else:\n            raise ValueError(f\"Invalid response: {response}\")\n\n    def check_reference(self, text):\n        try:\n            is_reference = self._check_reference_once(text)\n        except Exception as e:\n            is_reference = True\n        # logger.info(f\"{text} is reference: {is_reference}\")\n        if is_reference:\n            _is, _is_not = 1, 0\n            while (_is + _is_not) < 2 or _is == _is_not:\n                try:\n                    if self._check_reference_once(text):\n                        _is += 1\n                    else:\n                        _is_not += 1\n                except Exception as e:\n                    logger.error(f\"Failed to check the reference: {e}\")\n            if _is > _is_not:\n                logger.info(\"Is reference, skip...\")\n                return True\n        return False\n\n    def translate(\n        self, text: str, from_lang=\"ENGLISH\", to_lang=\"SLOVENIAN\"\n    ) -> str | None:\n        \"\"\"\n        Translates a given string into another language.\n\n        Parameters:\n        - text (str): The text to be translated.\n\n        Returns:\n        - str: The translated text.\n        - None: If the translation failed or it should not be translated(eg. it is a reference).\n\n        This method needs to be implemented by subclasses.\n        \"\"\"\n        self.from_lang = from_lang\n        self.to_lang = to_lang\n        check_time = 0\n        base_prompt = f\"You are an {from_lang}-to-{to_lang} translator. \\n - Keep all special characters / HTML tags / links as in the source text. \\n - Do not pay any attention to the http links in the text\\n - Return the {to_lang} translation only.\\n\"\n        while True:\n            if self.check_response:\n                prompt = f\"{base_prompt}You have translated once before, but the feedback of your translation is bad, the feedback is {self.check_response}. Pay attention to your translation later.\\nHere is the text to translate, return the translation only:\\n\\n{text}\"\n                self.check_response = None\n            else:\n                prompt = f\"{base_prompt}Here is the text to translate, return the translation only:\\n\\n{text}\"\n            translated_text = self.get_response([{\"role\": \"user\", \"content\": prompt}])\n            logger.debug(f\"Translated text: {translated_text}\")\n            if self.check_translation(text, translated_text):\n                return translated_text\n            else:\n                logger.warning(\n                    f\"Translating the text again:\\nText: {text}\\nTranslated text: {translated_text}\"\n                )\n                check_time += 1\n                if check_time >= 2:\n                    logger.error(f\"Failed to translate the text\")\n                    return None\n\n    def translate_all(\n        self, layout: List[Layout], from_lang, to_lang, multi_thread=False\n    ):\n        threads = []\n\n        def translate_single_layout(i):\n            line: Layout = layout[i]\n            # Skip the reference\n            if self.check_reference(line.text):\n                layout[i].translated_text = None\n                return\n            # Reformat the list\n            if line.type == \"list\":\n                line.text = self.reformat_text(line.text)\n            layout[i].translated_text = self.translate(line.text, from_lang, to_lang)\n\n        progress = tqdm(\n            total=len(layout), desc=\"Translating text\", leave=False, dynamic_ncols=True\n        )\n        for i in range(len(layout)):\n            if not layout[i].text:\n                continue\n            if multi_thread:\n                t = Thread(target=translate_single_layout, args=(i,))\n                threads.append(t)\n                t.start()\n            else:\n                progress.update(1)\n                translate_single_layout(i)\n        if multi_thread:\n            for t in threads:\n                t.join()\n        return layout\n"}
{"type": "source_file", "path": "cli_operation/single.py", "content": "import warnings\n\nwarnings.filterwarnings(\"ignore\")\nimport server\nimport os\nfrom pathlib import Path\nimport tempfile\nfrom loguru import logger\nimport sys\nimport argparse\n\nif __name__ == \"__main__\":\n    translator = server.TranslateApi()\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--file\", type=str, help=\"PDF file to translate\")\n    parser.add_argument(\n        \"--overwrite\",\n        action=\"store_true\",\n        help=\"Overwrite the existing translated file\",\n    )\n    parser.add_argument(\n        \"--suffix\",\n        type=str,\n        default=\"_translated\",\n        help=\"Suffix for the translated file (without .pdf)\",\n    )\n    # get the arguments\n    args = parser.parse_args()\n    file: str = args.file # source file\n    suffix = args.suffix + \".pdf\" # suffix for the translated file\n    overwrite_flag: bool = args.overwrite # overwrite the existing translated file\n\n    # make sure the file exists and is a PDF file\n    assert os.path.exists(file) and file.endswith(\n        \".pdf\"\n    ), \"Please provide a valid PDF file\"\n\n    # make sure the translation action should be done\n    file: Path = Path(file)\n    is_translated_file = file.name.endswith(suffix)\n    has_translated_file = os.path.exists(file.as_posix().replace(\".pdf\", suffix))\n    if is_translated_file or (has_translated_file and not overwrite_flag):\n        logger.info(f\"Skip {file.as_posix()}\")\n        exit(0)\n        \n    # start translation\n    logger.info(f\"Translating {file.as_posix()}\")\n    response = translator._translate_pdf(\n        pdf_path_or_bytes=file.as_posix(),\n        output_dir=translator.temp_dir_name,\n        from_lang=\"English\",\n        to_lang=\"Chinese\",\n        translate_all=True,\n        p_from=0,\n        p_to=0,\n        output_file_path=file.as_posix().replace(\".pdf\", suffix),\n    )\n"}
{"type": "source_file", "path": "modules/translate/openai_gpt.py", "content": "from .LLMTranslateBase import LLMTranslateBase\nfrom openai import OpenAI\n\nclass TranslateOpenAIGPT(LLMTranslateBase):\n    def init(self, cfg: dict):\n        super().init(cfg)\n        \n    def init_client(self, cfg: dict) -> OpenAI:\n        return OpenAI(api_key=cfg[\"api_key\"])"}
{"type": "source_file", "path": "cli_operation/__init__.py", "content": ""}
{"type": "source_file", "path": "modules/translate/google_translate.py", "content": "from .base import TranslateBase \nfrom googletrans import Translator, LANGUAGES\n\nlanguage_codes = []\nlanguages = []\n\n# Iterate over the LANGUAGES dictionary\nfor code, name in LANGUAGES.items():\n    language_codes.append(code)  # Append the code to the language_codes list\n    languages.append(name)  # Append the name to the languages list\n\n\n\nclass TranslateGoogleTranslate(TranslateBase):\n    def init(self, cfg: dict):\n        self.client = Translator()\n\n    def get_languages(self):\n        return language_codes\n\n    def translate(self, text: str, from_lang='en', to_lang='sl') -> str:\n        print(f\"translating {text} from {from_lang} to {to_lang}\")\n        trans = self.client.translate(text, src=from_lang, dest=to_lang)\n        print(trans)\n        return trans.text"}
{"type": "source_file", "path": "utils/__init__.py", "content": "import os\nfrom .textwrap_local import fw_fill, fw_wrap\nfrom .ocr_model import OCRModel\nfrom .layout_model import LayoutAnalyzer\nfrom .gui import GradioApp\nfrom PIL import Image, ImageDraw, ImageFont\nfrom loguru import logger\n\nimport yaml\n\n__all__ = [\"fw_fill\", \"fw_wrap\", \"OCRModel\", \"LayoutAnalyzer\"]\n\ndef load_config(base_config_path, override_config_path):\n    with open(base_config_path, 'r') as base_file:\n        base_config = yaml.safe_load(base_file)\n    \n    final_config = base_config\n\n    # Update the base config with the override config\n    # This recursively updates nested dictionaries\n    def update(d, u):\n        for k, v in u.items():\n            if isinstance(v, dict):\n                d[k] = update(d.get(k, {}), v)\n            else:\n                d[k] = v\n        return d\n\n    if os.path.exists(override_config_path):\n        with open(override_config_path, 'r') as override_file:\n            override_config = yaml.safe_load(override_file)\n            final_config = update(base_config, override_config)\n\n    return final_config\n\n\ndef draw_text(draw: ImageDraw.ImageDraw, processed_text, current_fnt, font_size, width, ygain):\n    y = 0\n    if isinstance(processed_text, str):\n        processed_text = processed_text.split('\\n')\n    first = len(processed_text) > 1\n    for l in processed_text:\n        words = l.split(\" \")\n        words_length = sum(draw.textlength(w, font=current_fnt) for w in words)\n        if first: words_length += 40\n        space_length = (width - words_length) / (len(words))\n        if (space_length > 40):\n            logger.debug(f\"Space length too wide, Setting to : {space_length}\")\n            space_length = font_size/2.4\n        elif space_length < 0:\n            space_length = 0\n        x = 0\n        if first: x+= 40\n        for word in words:\n            logger.debug(f\"Drawing word at {x}, {y}: \\n {word}\")\n            draw.text((x, y), word, font=current_fnt, fill=\"black\")\n            x += draw.textlength(word, font=current_fnt) + space_length\n        y += ygain\n        first = False"}
{"type": "source_file", "path": "modules/translate/ollama_translate.py", "content": "from .LLMTranslateBase import LLMTranslateBase\nfrom openai import OpenAI\n\n\nclass TranslateOllama(LLMTranslateBase):\n    def init(self, cfg: dict):\n        super().init(cfg)\n\n    def init_client(self, cfg: dict) -> OpenAI:\n        return OpenAI(\n            base_url=\"http://localhost:11434/v1/\",\n            # required but ignored\n            api_key=\"ollama\",\n        )\n"}
{"type": "source_file", "path": "modules/translate/qwen_translate.py", "content": "from .LLMTranslateBase import LLMTranslateBase\nfrom openai import OpenAI\n\n\nclass TranslateQwen(LLMTranslateBase):\n    def init(self, cfg: dict):\n        super().init(cfg)\n\n    def init_client(self, cfg: dict) -> OpenAI:\n        return OpenAI(\n            api_key=cfg[\"api_key\"],\n            base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n        )\n"}
{"type": "source_file", "path": "server.py", "content": "import sys\nimport os\nfrom threading import Thread\nfrom multiprocessing import Pool\nimport tempfile\nfrom pathlib import Path\nfrom typing import List, Tuple\nimport uvicorn\nfrom fastapi import FastAPI, File, Form, UploadFile\nfrom fastapi.responses import FileResponse, JSONResponse\nfrom typing import Optional\nfrom PyPDF2 import PdfReader, PdfWriter\nfrom io import BytesIO\nimport time\nimport asyncio\n# from starlette.middleware.wsgi import WSGIMiddleware\nfrom pdf2image import convert_from_bytes, convert_from_path\nfrom PIL import Image\nfrom pydantic import BaseModel, Field\nfrom modules.render.base import RenderMode\nfrom modules.render.simple import SimpleRender\nfrom modules.render.reportlab import ReportLabRender\nfrom tqdm import tqdm\nimport gradio as gr\nfrom loguru import logger\nfrom concurrent.futures import ThreadPoolExecutor\nfrom utils.layout_model import Layout\nfrom utils.database.file_db import FileDatabase, FileStatus\nfrom utils.api_utils import TranslateRequest\nfrom threading import Thread\n\nlogger.remove()\nlogger.add(sys.stderr, level=\"INFO\")\n\n\nfrom utils import GradioApp, load_config\nfrom modules import (\n    load_translator,\n    load_layout_engine,\n    load_ocr_engine,\n    load_render_engine,\n)\n\n\ncfg = load_config(\"config.yaml\", \"config.dev.yaml\")\ntranslator = load_translator(cfg[\"translator\"])\nlogger.info(f\"Got translator {translator}\")\nrender_engine = load_render_engine(cfg[\"render\"])\n\n\nclass InputPdf(BaseModel):\n    \"\"\"Input PDF file.\"\"\"\n\n    input_pdf: UploadFile = Field(..., title=\"Input PDF file\")\n\ndef layout_and_ocr_process(cfg: dict, pdf_images: list):\n    \"\"\"Process the layout and OCR for the PDF images.\n    Restart the Ollama container if it is provided.(For lower vram usage)\n    Input:\n        cfg: dict: Configurations\n        pdf_images: list: List of PDF images\n        ollama_container: str: Ollama container name\n    \"\"\"\n    if (\n        cfg[\"translator\"].get(\"restart_container\") is not None\n        and cfg[\"translator\"][\"restart_container\"]\n    ):\n        ollama_container = cfg[\"translator\"][\"container_name\"]\n        logger.info(f\"\\tRestarting the Ollama container: {ollama_container}\")\n        os.system(f\"docker restart {ollama_container}\")\n    # Initialize the layout engine / OCR engine\n    layout_engine = load_layout_engine(cfg[\"layout\"])\n    ocr_engine = load_ocr_engine(cfg[\"ocr\"])\n    results = []\n    for i, image in tqdm(enumerate(pdf_images), desc=\"Getting layout and texts\"):\n        result = layout_engine.get_single_layout(image)\n        result = ocr_engine.get_all_text(result)\n        results.append(result)\n    return results\n\n\nclass TranslateApi:\n    \"\"\"Translator API class.\n\n    Attributes\n    ----------\n    app: FastAPI\n        FastAPI instance\n    temp_dir: tempfile.TemporaryDirectory\n        Temporary directory for storing translated PDF files\n    temp_dir_name: Path\n        Path to the temporary directory\n    layout_model: PPStructure\n        Layout model for detecting text blocks\n    ocr_model: PaddleOCR\n        OCR model for detecting text in the text blocks\n    translate_model: MarianMTModel\n        Translation model for translating text\n    translate_tokenizer: MarianTokenizer\n        Tokenizer for the translation model\n    \"\"\"\n\n    DPI = 200\n\n    def __init__(\n        self,\n        model_root_dir: Path = Path(\"/app/models/\"),\n        enable_api: bool = False,\n        enable_gui: bool = False,\n    ):\n        # The database\n        self.database_name = cfg['gui']['database_name']\n        self.file_db = FileDatabase(self.database_name)\n        self.temp_dir = tempfile.TemporaryDirectory()\n        self.temp_dir_name = Path(self.temp_dir.name)\n\n        self.use_multi_thread = cfg[\"multi_thread\"][\"enable\"]\n        \n        self.pending_requests: list[TranslateRequest] = []\n\n        self.translate_thread = Thread(target=self.scan_and_translate)\n        self.translate_thread.start()\n\n        if enable_api or enable_gui:\n            self.app = FastAPI()\n            self.app.add_api_route(\n                \"/translate_pdf/\",\n                self.translate_pdf,\n                methods=[\"POST\"],\n                response_class=JSONResponse,\n            )\n            self.app.add_api_route(\n                \"/clear_temp_dir/\",\n                self.clear_temp_dir,\n                methods=[\"GET\"],\n            )\n            logger.info(\"GETFILES API ENABLED\")\n            self.app.add_api_route(\n                \"/get_files/\",\n                self.get_files,\n                methods=[\"POST\"],\n                response_class=JSONResponse,\n            )\n            self.app.add_api_route(\n                \"/download_file/\",\n                self.download_file,\n                methods=[\"POST\"],\n                response_class=FileResponse,\n            )\n\n        if enable_gui:\n            gradioapp_ = GradioApp(translator.get_languages(), cfg['gui'])\n            gradioapp = gradioapp_.create_gradio_app()\n            gr.mount_gradio_app(self.app, gradioapp, \"/\")\n\n    def run(self):\n        \"\"\"Run the API server\"\"\"\n        uvicorn.run(self.app, host=\"0.0.0.0\", port=8765)\n        \n    def scan_and_translate(self):\n        \"\"\"Scan the pending requests and translate them.\"\"\"\n        file_db = FileDatabase(self.database_name)\n        while True:\n            time.sleep(1)\n            # logger.info(f\"Scanning the pending requests, there are {len(self.pending_requests)} requests\")\n            if len(self.pending_requests) > 0:\n                req = self.pending_requests.pop(0)\n                file_db.set_translating(str(req.pdf_path).split(\"/\")[-1])\n                self._translate_pdf(req)\n                file_db.set_translated(str(req.pdf_path).split(\"/\")[-1])\n\n    async def translate_pdf(\n        self,\n        input_pdf: UploadFile = File(None),\n        input_pdf_path: str = Form(None),\n        from_lang: str = Form(...),\n        to_lang: str = Form(...),\n        translate_all: bool = Form(...),\n        p_from: int = Form(...),\n        p_to: int = Form(...),\n        render_mode: str = Form(...),\n        output_file_path: str = Form(None),\n        add_blank_page: bool = Form(...),\n    ) -> FileResponse:\n        \"\"\"API endpoint for translating PDF files.\"\"\"\n        logger.info(\n            f\"Got request to translate PDF, the args are:\\nfrom_lang: {from_lang} to_lang: {to_lang}\\ntranslate_all: {translate_all} p_from: {p_from}, p_to: {p_to}\\nrender_mode: {render_mode}\\noutput_file_path: {output_file_path}\\ninput_pdf_path: {input_pdf_path}\\nadd_blank_page: {add_blank_page}\\ninput_pdf: {input_pdf is None}\"\n        )\n\n        if input_pdf:\n            # conver to Path\n            input_pdf_data = await input_pdf.read()\n            input_pdf_data = BytesIO(input_pdf_data)\n            # input_pdf_data = PdfReader(input_pdf_data)\n            # save the PDF file\n            logger.info(f\"The filename is {input_pdf.filename}\")\n            if input_pdf_path is None:\n                input_pdf_path = self.temp_dir_name / input_pdf.filename\n                output_file_path = self.temp_dir_name / input_pdf.filename.replace(\".pdf\", \"_translated.pdf\")\n            else:\n                input_pdf_path = Path(input_pdf_path)\n            with open(input_pdf_path, \"wb\") as f:\n                writer = PdfWriter()\n                writer.append(input_pdf_data)\n                writer.write(f)\n            input_pdf_data = Path(input_pdf_path)\n        elif input_pdf_path:\n            input_pdf_data = Path(input_pdf_path)\n        else:\n            raise ValueError(\"No input PDF file provided\")\n        response: str = self._submit(\n            input_pdf_data,\n            self.temp_dir_name,\n            from_lang,\n            to_lang,\n            translate_all,\n            p_from,\n            p_to,\n            output_file_path=output_file_path,\n            render_mode=render_mode,\n            add_blank_page=add_blank_page,\n        )\n        return JSONResponse(content={\"message\": response})\n\n    def _submit(\n        self,\n        pdf_path: Path,\n        temp_output_dir: Path,\n        from_lang: str,\n        to_lang: str,\n        translate_all: bool,\n        p_from: int,\n        p_to: int,\n        output_file_path: Optional[Path | str] = None,\n        render_mode: Optional[str] = None,\n        add_blank_page: bool = False,\n    ) -> str:\n        \"\"\"Submit a translation request.\"\"\"\n        req = TranslateRequest(\n            pdf_path=pdf_path,\n            temp_output_dir=temp_output_dir,\n            from_lang=from_lang,\n            to_lang=to_lang,\n            translate_all=translate_all,\n            p_from=p_from,\n            p_to=p_to,\n            output_file_path=output_file_path,\n            render_mode=render_mode,\n            add_blank_page=add_blank_page,\n        )\n        # self.req_db.add_request(req)\n        # self.lock.acquire()\n        self.pending_requests.append(req)\n        # self.lock.release()\n        self.file_db.add_file(\n            str(req.pdf_path).split(\"/\")[-1], \n            str(req.pdf_path), \n            str(req.output_file_path),\n            FileStatus.NOT_TRANSLATED\n        )\n        if len(self.pending_requests) == 1:\n            return \"Request submitted, translating...\"\n        else:\n            return f\"Request submitted, there are {len(self.pending_requests) - 1} requests before.\"\n\n    async def get_files(self, target_status: Optional[FileStatus]=Form(None)):\n        logger.info(f\"Getting files with status {target_status}\")\n        file_status: list[tuple] = self.file_db.get_files(target_status)\n        ret_status = []\n        for file, src_path, target_path, status in file_status:\n            target_file_disappeared = status == FileStatus.TRANSLATED.value and not os.path.exists(target_path)\n            src_path_disappeared = status == FileStatus.NOT_TRANSLATED.value and not os.path.exists(src_path)\n            if target_file_disappeared or src_path_disappeared:\n                self.file_db.remove_file(file)\n                continue\n            status = {\n                \"file\": file,\n                \"src_path\": src_path,\n                \"target_path\": target_path,\n                \"status\": status,\n            }\n            ret_status.append(status)\n        return JSONResponse(content=ret_status)\n    \n    async def download_file(self, file_path: str=Form(...)):\n        logger.info(f\"Downloading file {file_path}\")\n        if not os.path.exists(file_path):\n            return JSONResponse(content={\"message\": \"File not found\"})\n        return FileResponse(file_path)\n\n    async def clear_temp_dir(self):\n        \"\"\"API endpoint for clearing the temporary directory.\"\"\"\n        self.temp_dir.cleanup()\n        self.temp_dir = tempfile.TemporaryDirectory()\n        self.temp_dir_name = Path(self.temp_dir.name)\n        return {\"message\": \"temp dir cleared\"}\n\n    def _init_translation(\n        self,\n        pdf_path: Path,\n        render_mode: Optional[str],\n        p_from: int,\n        p_to: int,\n        translate_all: bool,\n    ) -> Tuple[List[Image.Image], Optional[RenderMode]]:\n        # Check if the input is a file or bytes\n        if isinstance(pdf_path, str):\n            pdf_path = Path(pdf_path)\n            # check if the path is a file\n            assert pdf_path.is_file(), f\"{pdf_path} is not a file\"\n\n        if isinstance(pdf_path, Path):\n            pdf_images = convert_from_path(pdf_path, dpi=self.DPI)\n        else:\n            raise ValueError(\"Invalid input type\")\n\n        # Get the render mode of the output file\n        if isinstance(render_mode, str):\n            render_mode = RenderMode.get_mode(render_mode)\n        else:\n            render_mode = None\n\n        total_pages = len(pdf_images)\n        if translate_all:\n            p_from = 0\n            p_to = total_pages\n        elif p_to > p_from:\n            total_pages = p_to - p_from\n        else:\n            logger.error(\"Invalid page range, the range will be [from_page, to_page)\")\n            raise ValueError(\n                \"Invalid page range, the range will be [from_page, to_page)\"\n            )\n        pdf_images = pdf_images[p_from:p_to]\n        logger.info(\n            f\"Total pages: {total_pages} / Translating pages: from {p_from} to {p_to} / Translate all: {translate_all}\"\n        )\n\n        return pdf_images, render_mode\n\n    def _translate_pdf(\n        self,\n        req: TranslateRequest,\n    ) -> None:\n        \"\"\"Backend function for translating PDF files.\n\n        Translation is performed in the following steps:\n            1. Getting the layout and text\n                1.1 Convert the PDF file to images\n                1.2 Detect text blocks in the images (layout detection)\n                1.3 For each text block, detect text (ocr)\n            2. translate the text\n            3. Setting the render font, render each page with the translated text\n            4. Merge all PDF files into one PDF file\n\n        At 3, this function does not translate the text after\n        the references section. Instead, saves the image as it is.\n\n        Parameters\n        ----------\n        pdf_path: Path\n            Path to the input PDF file or bytes of the input PDF file\n        temp_output_dir: Path\n            Path to the output directory for temporal files\n        from_lang: str\n            The source language\n        to_lang: str\n            The target language\n        translate_all: bool\n            Translate all the pages\n        p_from: int\n            (Won't take effect when translate_all is True) The begin index of the range of translation\n        p_to: int\n            (Won't take effect when translate_all is True) The end index of the range of translation\n        output_file_path: Optional[Path | str] = None\n            The path of output file\n        render_mode: Optional[str] = None,\n            The render mode\n        add_blank_page: bool = False,\n            Add blank page at the begining and the end of the pdf, only take effects when the render mode is RenderMode.INTERLEAVE and the render backend is ReportLab\n        \"\"\"\n        # 0. Initialize\n        logger.info(f\"Translate PDF: {req.pdf_path}\")\n        pdf_path, temp_output_dir, from_lang, to_lang, translate_all, p_from, p_to, output_file_path, render_mode, add_blank_page = req.extract()\n        pdf_images, render_mode = self._init_translation(\n            pdf_path, render_mode, p_from, p_to, translate_all\n        )\n        logger.info(f\"Translate from {from_lang} to {to_lang}\")\n\n        pdf_files = []\n        total_pages = len(pdf_images)\n        logger.info(f\"Step 1/2: processing {total_pages} pages\")\n\n        if isinstance(output_file_path, str):\n            if Path(output_file_path).is_dir():\n                output_file_path = os.path.join(output_file_path, pdf_path.name.replace(\".pdf\", \"_translated.pdf\"))\n            output_file_path = Path(output_file_path)\n        if isinstance(render_engine, SimpleRender):\n            render_engine.init_pdf()\n        elif isinstance(render_engine, ReportLabRender):\n            render_engine.init_pdf(output_file_path, self.temp_dir_name)\n\n        results, threads = [], []\n        logger.info(f\"Getting layout and texts\")\n\n        def translate_one_page(i, layouts):\n            results[i] = translator.translate_all(\n                layouts, from_lang, to_lang, multi_thread=True\n            )\n\n        # 1. Getting layout and text\n        if not self.use_multi_thread:\n            # Use multi-processing to control the vram usage\n            # This will free the vram after each page is processed\n            # On 3090, the vram usage is around 5GB\n            logger.info(f\"\\tUsing single-threading\")\n            self.pool = Pool(1)\n            res = self.pool.apply_async(layout_and_ocr_process, args=(cfg, pdf_images))\n            self.pool.close()\n            self.pool.join()\n            results = res.get()\n        else:\n            # Initialize the layout engine / OCR engine\n            layout_engine = load_layout_engine(cfg[\"layout\"])\n            ocr_engine = load_ocr_engine(cfg[\"ocr\"])\n\n            for i, image in enumerate(\n                zip(range(p_to - p_from), pdf_images), desc=\"Getting layout and texts\"\n            ):\n                result: list[Layout] = layout_engine.get_single_layout(\n                    image\n                )  # Getting layout\n                result = ocr_engine.get_all_text(result)  # Getting text\n                results.append(result)\n                # translate the text in parallel\n                t = Thread(target=translate_one_page, args=(i, result))\n                threads.append(t)\n                t.start()\n\n        # 2. Translate the text\n        logger.info(f\"Translating pages\")\n        if self.use_multi_thread:\n            logger.info(f\"\\tUsing multi-threading\")\n            for t in threads:\n                t.join()\n        else:\n            # translate the text sequentially\n            for i, result in tqdm(\n                enumerate(results), leave=False, desc=\"Translating pages\"\n            ):\n                result = translator.translate_all(result, from_lang, to_lang)\n                results[i] = result\n\n        # 3. Setting render font and render each page\n        logger.info(f\"Render the pages\")\n        for i, (image, result) in tqdm(\n            enumerate(zip(pdf_images, results)), leave=False, desc=\"Setting render font\"\n        ):\n            result = render_engine.get_all_fonts(result)\n            output_path = temp_output_dir / f\"{i:03}.pdf\"\n\n            if isinstance(render_engine, SimpleRender):\n                if not render_engine.reached_references:\n                    render_engine.translate_one_page(image=image, result=result)\n                render_engine.post_process(image, render_mode, output_path, self.DPI)\n                pdf_files.append(str(output_path))\n            elif isinstance(render_engine, ReportLabRender):\n                render_engine.translate_one_page(image=image, result=result)\n                render_engine.post_process()\n            else:\n                raise NotImplementedError(\"Font engine not implemented\")\n\n        # 4. Merge the result and save the PDF\n        logger.info(\"Step 2/2: Merging PDF files\")\n        if isinstance(render_engine, SimpleRender):\n            render_engine.merge_pdfs(pdf_files, output_file_path, self.temp_dir_name)\n        elif isinstance(render_engine, ReportLabRender):\n            if render_mode is None:\n                render_mode = render_engine.render_mode\n            render_engine.save_pdf(\n                render_mode, pdf_path, p_from, add_blank_page\n            )\n        else:\n            raise NotImplementedError(\"Render engine not implemented\")\n\nif __name__ == \"__main__\":\n    translate_api = TranslateApi(enable_gui=True)\n    translate_api.run()\n"}
{"type": "source_file", "path": "modules/ocr/base.py", "content": "from abc import ABC, abstractmethod\n\nclass OCRBase(ABC):\n    @abstractmethod\n    def init(self, cfg: dict):\n        pass\n\n\n    @abstractmethod\n    def get_text(self, image):\n        \"\"\"\n        Translates a given string into another language.\n\n        Parameters:\n        - text (str): The text to be translated.\n\n        Returns:\n        - str: The translated text.\n\n        This method needs to be implemented by subclasses.\n        \"\"\"\n        pass\n\n\n    @abstractmethod\n    def get_all_text(self, layout):\n        \"\"\"\n        Translates a given string into another language.\n\n        Parameters:\n        - text (str): The text to be translated.\n\n        Returns:\n        - str: The translated text.\n\n        This method needs to be implemented by subclasses.\n        \"\"\"\n        pass\n\n"}
{"type": "source_file", "path": "modules/translate/base.py", "content": "from abc import ABC, abstractmethod\nfrom tqdm import tqdm\nfrom typing import List\nfrom utils.layout_model import Layout\nfrom threading import Thread\n\nclass TranslateBase(ABC):\n    @abstractmethod\n    def init(self, cfg: dict):\n        pass\n\n    @abstractmethod\n    def get_languages(self):\n        pass\n\n    def translate_all(self, layout: List[Layout], from_lang, to_lang, multi_thread = False):\n        if not multi_thread:\n            for line in tqdm(layout, desc=\"Translating\", leave=False):\n                if line.text:\n                    if line.type == 'list':\n                        line.text = self.reformat_text(line.text)\n                    line.translated_text = self.translate(line.text, from_lang, to_lang)\n        else:\n            threads = []\n            def translate_single_layout(i):\n                line = layout[i]\n                if line.type == 'list':\n                    line.text = self.reformat_text(line.text)\n                layout[i].translated_text = self.translate(line.text, from_lang, to_lang)\n            for i in range(len(layout)):\n                if layout[i].text:\n                    t = Thread(target=translate_single_layout, args=(i,))\n                    threads.append(t)\n                    t.start()\n            for t in threads:\n                t.join()\n        return layout\n\n    @abstractmethod\n    def reformat_text(self, text: str) -> str:\n        pass\n\n    @abstractmethod\n    def translate(self, text: str) -> str:\n        \"\"\"\n        Translates a given string into another language.\n\n        Parameters:\n        - text (str): The text to be translated.\n\n        Returns:\n        - str: The translated text.\n\n        This method needs to be implemented by subclasses.\n        \"\"\"\n        pass\n"}
{"type": "source_file", "path": "modules/render/reportlab.py", "content": "import re\nfrom loguru import logger\nimport numpy as np\nfrom tqdm import tqdm\nfrom .base import RenderBase, RenderMode\nfrom utils.layout_model import Layout\nfrom reportlab.pdfgen.canvas import Canvas\nfrom reportlab.pdfbase.ttfonts import TTFont\nfrom reportlab.pdfbase import pdfmetrics\nfrom reportlab.lib.utils import ImageReader\nfrom typing import Tuple, Optional\nfrom PIL import Image\nfrom pathlib import Path\nimport io\nfrom PyPDF2 import PdfReader, PdfWriter, Transformation\nimport sys\nfrom loguru import logger\n\nsys.setrecursionlimit(100000000)\n\n\nclass ReportLabRender(RenderBase):\n    FONT_SIZE = 29\n\n    def init(self, cfg: dict):\n        self.cfg = cfg\n        self.font_path = cfg[\"font_path\"]\n        self.font_name = cfg[\"font_name\"]\n        pdfmetrics.registerFont(TTFont(self.font_name, self.font_path))\n        self.render_mode = RenderMode.get_mode(cfg[\"render_mode\"])\n        logger.info(f\"Render mode: {self.render_mode}\")\n        self.pdf: Optional[Canvas] = None\n\n    def init_pdf(self, output_dir, temp_dir_name: Path):\n        output_file = output_dir if output_dir else (temp_dir_name / \"translated.pdf\")\n        if isinstance(output_file, Path):\n            output_file = str(output_file)\n        self.output_file = output_file\n        self.packet = io.BytesIO()\n        self.pdf = Canvas(self.packet, pagesize=(1000, 1000))\n        # self.pdf.setFont(self.font_name, self.FONT_SIZE)\n        self.reached_references = False\n\n    def get_all_fonts(self, layout: list[Layout]):\n        for line in tqdm(\n            layout, desc=\"Getting font info\", leave=False, dynamic_ncols=True\n        ):\n            if line.type in [\"text\", \"list\", \"title\"]:\n                # update this so image is created from images and layout bbox info\n                image = line.image\n                height = line.bbox[3] - line.bbox[1]\n                width = line.bbox[2] - line.bbox[0]\n                temp_canvas = Canvas(\"temp.pdf\", pagesize=(width, height))\n                family, font_size, ygain, processed_text = self.get_font_info(\n                    line, temp_canvas\n                )\n                line.font = {\"family\": family, \"size\": font_size, \"ygain\": ygain}\n                line.processed_text = processed_text\n\n        return layout\n\n    def get_font_info(self, line: Layout, draw: Canvas):\n        logger.debug(f\"Getting font info for text:\\n{line.text}\")\n        if line.translated_text is None:\n            return \"SourceHanSerifSC-Medium.otf\", self.FONT_SIZE, 30, \"\"\n        font_family = \"SourceHanSerifSC-Medium.otf\"\n        text = line.translated_text.strip()\n        height = line.bbox[3] - line.bbox[1]\n        width = line.bbox[2] - line.bbox[0]\n\n        def split_text(text: str, width: int, fontsize: int):\n            logger.debug(f\"Splitting text with width {width}/text: \\n{text}\")\n            lines = []\n            while text:\n                # logger.debug(f\"Remaining text for width {width}: \\n{text}\")\n                initial_indent = 40 if len(lines) == 0 else 0\n                index = 0\n                if text[0] == \"\\n\":\n                    text = text[1:]\n                    lines.append(\"\\n\")\n                    continue\n                while (\n                    (index < len(text))\n                    and (text[index] != \"\\n\")\n                    and (\n                        initial_indent\n                        + draw.stringWidth(\n                            text[0 : index + 1],\n                            fontName=self.font_name,\n                            fontSize=fontsize,\n                        )\n                        < width\n                    )\n                ):\n                    index += 1\n                # logger.debug(f\"Index: {index} / Text: {text[0:index]}\")\n                lines.append(text[0:index])\n                text = text[index:]\n            return lines\n\n        font_size = self.FONT_SIZE\n        processed_text = None\n\n        while True:\n            logger.debug(f\"Try Font size: {font_size}\")\n            if font_size < 4:\n                logger.error(\"Font size too small!\")\n                break\n            text_s = split_text(text, width, font_size)\n            text_l = split_text(text, width, font_size + 1)\n            height_s = len(text_s) * 1.1 * font_size\n            height_l = len(text_l) * 1.1 * (font_size + 1)\n            if height_s < height and height_l >= height:\n                # logger.debug(f\"Width {width} / Render width {bbox_s[2] - bbox_s[0]}\")\n                processed_text = text_s\n                break\n            elif height_s >= height:\n                logger.debug(f\"Decreasing font size from {font_size}\")\n                font_size -= 1\n            else:\n                assert (\n                    height_s < height and height_l < height\n                ), f\"Font size too large! S:{height_s} L:{height_l} Height:{height}, condition 1: {height_s < height and height_l >= height}\"\n                if text_s.count(\"\\n\") == 0:\n                    processed_text = text_s\n                    break\n                logger.debug(\n                    f\"Increasing font size from {font_size}, count newline:\"\n                    + str(text_s.count(\"\\n\"))\n                )\n                font_size += 1\n\n        ygain = int(font_size * 1.1)\n\n        # return 'TimesNewRoman.ttf', font_size, ygain\n        return \"SourceHanSerifSC-Medium.otf\", font_size, ygain, processed_text\n\n    def fill_unrendered_region(self, image: np.ndarray):\n        radius = self.FONT_SIZE // 2\n        if len(image.shape) == 3:\n            not_white = np.any(image != 255, axis=-1)\n        else:\n            not_white = image != 255\n        vis = np.zeros_like(not_white, dtype=bool)\n\n        def dfs(x, y, res):\n            if vis[x, y]:\n                return res\n            if not_white[x, y]:\n                vis[x, y] = True\n            for nx in range(x - radius, x + radius):\n                for ny in range(y - radius, y + radius):\n                    if (\n                        nx >= 0\n                        and nx < image.shape[0]\n                        and ny >= 0\n                        and ny < image.shape[1]\n                        and not_white[nx, ny]\n                        and not vis[nx, ny]\n                    ):\n                        res[0] = min(res[0], nx)\n                        res[1] = max(res[1], nx)\n                        res[2] = min(res[2], ny)\n                        res[3] = max(res[3], ny)\n                        res = dfs(nx, ny, res)\n            return res\n\n        for i in range(image.shape[0]):\n            for j in range(image.shape[1]):\n                if not_white[i, j] and not vis[i, j]:\n                    (x_min, x_max, y_min, y_max) = dfs(i, j, np.array([i, i, j, j]))\n                    if x_max - x_min > 0 and y_max - y_min > 0:\n                        img_to_render = Image.fromarray(image[x_min:x_max, y_min:y_max])\n                        self.pdf.drawImage(\n                            ImageReader(img_to_render),\n                            y_min,\n                            image.shape[0] - x_max,\n                            y_max - y_min,\n                            x_max - x_min,\n                        )\n\n    def translate_one_page(\n        self, image, result: list[Layout]\n    ) -> Tuple[np.ndarray, np.ndarray, bool]:\n        \"\"\"Translate one page of the PDF file.\"\"\"\n        image = np.array(image, dtype=np.uint8)\n        # white out all the recognized region\n        for line in result:\n            image[\n                max(0, line.bbox[1] - 3) : line.bbox[3] + 3,\n                max(0, line.bbox[0] - 3) : line.bbox[2] + 3,\n            ] = 255\n            line.bbox[1], line.bbox[3] = (\n                image.shape[0] - line.bbox[3],\n                image.shape[0] - line.bbox[1],\n            )\n\n        self.pdf.setPageSize((image.shape[1], image.shape[0]))\n        self.pdf.setFont(self.font_name, self.FONT_SIZE)\n\n        self.fill_unrendered_region(image)\n        # self.pdf.drawImage(ImageReader(Image.fromarray(image)), 0, 0, image.shape[1], image.shape[0])\n\n        for line in result:\n            if line.type in [\"text\", \"list\"]:\n                logger.debug(f\"Rendering text: {line.text}\")\n                if len(line.text) > 0 and (\n                    line.processed_text is not None and len(line.processed_text) > 0\n                ):\n                    x, y = line.bbox[0], line.bbox[3] - line.font[\"ygain\"]\n\n                    processed_text = line.processed_text\n                    self.pdf.setFont(self.font_name, line.font[\"size\"])\n\n                    # copy over original image\n                    offset = 40\n                    for render_line in processed_text:\n                        if render_line != \"\\n\":\n                            self.pdf.drawString(x + offset, y, render_line)\n                        y -= line.font[\"ygain\"]\n                        offset = 0\n                else:\n                    self.pdf.drawImage(\n                        ImageReader(Image.fromarray(line.image)),\n                        line.bbox[0],\n                        line.bbox[1],\n                        line.bbox[2] - line.bbox[0],\n                        line.bbox[3] - line.bbox[1],\n                    )\n\n            elif line.type == \"title\":\n                title = line.text\n                self.pdf.drawImage(\n                    ImageReader(Image.fromarray(line.image)),\n                    line.bbox[0],\n                    line.bbox[1],\n                    line.bbox[2] - line.bbox[0],\n                    line.bbox[3] - line.bbox[1],\n                )\n                if title.lower() == \"references\" or title.lower() == \"reference\":\n                    self.reached_references = True\n            elif line.type == \"figure\":\n                self.pdf.drawImage(\n                    ImageReader(Image.fromarray(line.image)),\n                    line.bbox[0],\n                    line.bbox[1],\n                    line.bbox[2] - line.bbox[0],\n                    line.bbox[3] - line.bbox[1],\n                )\n            else:\n                # TODO: add list, table and image translation support\n                self.pdf.drawImage(\n                    ImageReader(Image.fromarray(line.image)),\n                    line.bbox[0],\n                    line.bbox[1],\n                    line.bbox[2] - line.bbox[0],\n                    line.bbox[3] - line.bbox[1],\n                )\n                logger.warning(f\"Unknown type: {line.type}\")\n        self.pdf.showPage()\n        # self.pdf.save()\n\n    def post_process(self):\n        pass\n\n    def save_pdf(\n        self,\n        render_mode: RenderMode,\n        src_pdf_path: Optional[Path] = None,\n        p_from: Optional[int] = None,\n        add_blank_page: bool = False,\n    ):\n        self.pdf.save()\n        self.packet.seek(0)\n        new_pdf = PdfReader(self.packet)\n        if (render_mode is RenderMode.SIDE_BY_SIDE) or (\n            render_mode is RenderMode.INTERLEAVE\n        ):\n            # if isinstance(src_pdf_path, bytes):\n            #     src_pdf = PdfReader(io.BytesIO(src_pdf_path))\n            # else:\n            src_pdf = PdfReader(open(src_pdf_path, \"rb\"))\n\n        # existing_pdf = PdfReader(open(\"samplePDF.pdf\", \"rb\"))\n        output = PdfWriter()\n\n        width, height = new_pdf.pages[0].mediabox.width, new_pdf.pages[0].mediabox.height\n\n\n        for i, page in enumerate(new_pdf.pages):\n            if render_mode is RenderMode.SIDE_BY_SIDE:\n                src_page = src_pdf.pages[i + p_from]\n                src_page.scale_to(width, height)\n                page.add_transformation(\n                    Transformation().translate(width, 0), expand=True\n                )\n                page.merge_page(src_page, True)\n            elif render_mode is RenderMode.INTERLEAVE:\n                src_page = src_pdf.pages[i + p_from]\n                src_page.scale_to(width, height)\n                output.add_page(src_page)\n            logger.debug(\n                f\"Merging {i} / {len(new_pdf.pages)}, page is None: {page is None}\"\n            )\n            output.add_page(page)\n\n        outputStream = open(self.output_file, \"wb\")\n        output.write(outputStream)\n        outputStream.close()\n            \n        if render_mode is RenderMode.INTERLEAVE and add_blank_page:\n            previous_pdf = PdfReader(self.output_file)\n            output = PdfWriter()\n            output.add_blank_page(width, height)\n            for i, page in enumerate(previous_pdf.pages):\n                output.add_page(page)\n            output.add_blank_page(width, height)\n            outputStream = open(self.output_file.replace(\".pdf\", \"_blank.pdf\"), \"wb\")\n            output.write(outputStream)\n            outputStream.close()"}
{"type": "source_file", "path": "utils/database/basic_info_db.py", "content": "from .base import Database\nfrom loguru import logger\n\nclass BasicInfoDatabase(Database):\n    def __init__(\n        self,\n        database_name,\n        table_name=\"basic_info\",\n        table_format={\n            \"key\": str,\n            \"value\": str\n        }\n    ):\n        super().__init__(database_name, table_name, table_format=table_format)\n        self.check_table()\n\n    def get_value(self, key: str):\n        conn, c = self.new_cursor()\n        c.execute(\n            f\"SELECT value FROM {self.table_name} WHERE key = ?\", (key,)\n        )\n        value = c.fetchone()\n        conn.commit()\n        conn.close()\n        logger.info(f\"The value of {key} is {value}\")\n        if value:\n            return value[0]\n        return None\n    \n    def update_value(self, key: str, value: str):\n        conn, c = self.new_cursor()\n        c.execute(\n            f\"UPDATE {self.table_name} SET value = ? WHERE key = ?\",\n            (value, key),\n        )\n        conn.commit()\n        conn.close()\n        logger.info(f\"Updated {key} to {value}\")\n\n    def set_value(self, key: str, value: str):\n        conn, c = self.new_cursor()\n        c.execute(\n            f\"INSERT INTO {self.table_name} VALUES (?, ?)\",\n            (key, value),\n        )\n        conn.commit()\n        conn.close()\n        logger.info(f\"Set {key} to {value}\")"}
{"type": "source_file", "path": "modules/layout/base.py", "content": "from abc import ABC, abstractmethod\n\nclass LayoutBase(ABC):\n    @abstractmethod\n    def init(self, cfg: dict):\n        pass\n\n\n    @abstractmethod\n    def get_layout(self, text: str):\n        \"\"\"\n        Translates a given string into another language.\n\n        Parameters:\n        - text (str): The text to be translated.\n\n        Returns:\n        - str: The translated text.\n\n        This method needs to be implemented by subclasses.\n        \"\"\"\n        pass\n"}
{"type": "source_file", "path": "cli.py", "content": "import argparse\nimport subprocess\nfrom pathlib import Path\n\nimport requests\n\nTRANSLATE_URL = \"http://localhost:8765/translate_pdf/\"\nCLEAR_TEMP_URL = \"http://localhost:8765/clear_temp_dir/\"\n\n\ndef translate_request(input_pdf_path: Path, output_dir: Path) -> None:\n    \"\"\"Sends a POST request to the translator server to translate a PDF.\n\n    Parameters\n    ----------\n    input_pdf_path : Path\n        Path to the PDF to be translated.\n    output_dir : Path\n        Path to the directory where the translated PDF will be saved.\n    \"\"\"\n    print(f\"Translating {input_pdf_path}...\")\n    with open(input_pdf_path, \"rb\") as input_pdf:\n        response = requests.post(TRANSLATE_URL, files={\"input_pdf\": input_pdf})\n\n    if response.status_code == 200:\n        with open(output_dir / input_pdf_path.name, \"wb\") as output_pdf:\n            output_pdf.write(response.content)\n        print(f\"Converted PDF saved to {output_dir / input_pdf_path.name}\")\n        requests.get(CLEAR_TEMP_URL)\n    else:\n        print(f\"An error occurred: {response.status_code}\")\n\n\ndef main(args: argparse.Namespace) -> None:\n    \"\"\"Translates a PDF or all PDFs in a directory.\n\n    Parameters\n    ----------\n    args : argparse.Namespace\n        Arguments passed to the script.\n\n    Raises\n    ------\n     ValueError\n        If the input path is not a valid path to file or directory.\n\n    Notes\n    -----\n    args must have the following attributes:\n        input_pdf_path_or_dir : Path\n            Path to the PDF or directory of PDFs to be translated.\n        output_dir : Path\n            Path to the directory where the translated PDFs\n            will be saved.\n    \"\"\"\n    args.output_dir.mkdir(parents=True, exist_ok=True)\n\n    if args.input_pdf_path_or_dir.is_file():\n        if args.input_pdf_path_or_dir.suffix != \".pdf\":\n            raise ValueError(\n                f\"Input file must be a PDF or directory: {args.input_pdf_path_or_dir}\"\n            )\n\n        translate_request(args.input_pdf_path_or_dir, args.output_dir)\n    elif args.input_pdf_path_or_dir.is_dir():\n        input_pdf_paths = args.input_pdf_path_or_dir.glob(\"*.pdf\")\n\n        if not input_pdf_paths:\n            raise ValueError(f\"Input directory is empty: {args.input_pdf_path_or_dir}\")\n\n        for input_pdf_path in input_pdf_paths:\n            translate_request(input_pdf_path, args.output_dir)\n    else:\n        raise ValueError(\n            f\"Input path must be a file or directory: {args.input_pdf_path_or_dir}\"\n        )\n\n    print(\"Done.\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"-i\",\n        \"--input_pdf_path_or_dir\",\n        type=Path,\n        required=True,\n        help=\"Path to the PDF or directory of PDFs to be translated.\",\n    )\n    parser.add_argument(\n        \"-o\",\n        \"--output_dir\",\n        type=Path,\n        default=\"./outputs\",\n        help=\"Path to the directory where the translated PDFs will be saved. (default: ./outputs)\",\n    )\n    args = parser.parse_args()\n    main(args)\n"}
{"type": "source_file", "path": "modules/__init__.py", "content": "def load_translator(cfg: dict):\n    if cfg['type'] == 'openai':\n        from .translate.openai_gpt import TranslateOpenAIGPT\n        translator = TranslateOpenAIGPT()\n    elif cfg['type'] == 'google_translate':\n        from .translate.google_translate import TranslateGoogleTranslate\n        translator = TranslateGoogleTranslate()\n    elif cfg['type'] == 'ollama':\n        from .translate.ollama_translate import TranslateOllama\n        translator = TranslateOllama()\n    elif cfg['type'] == 'qwen':\n        from .translate.qwen_translate import TranslateQwen\n        translator = TranslateQwen()\n    else:\n        raise(\"unknown translator\")\n    translator.init(cfg)\n    return translator\n\ndef load_layout_engine(cfg: dict):\n    if cfg['type'] == 'dit':\n        from .layout.ditod import DiTLayout\n        engine = DiTLayout()\n        engine.init(cfg)\n        return engine\n    \n    raise(\"unknown layout engine\")\n\ndef load_ocr_engine(cfg: dict):\n    if cfg['type'] == 'paddle':\n        from .ocr.paddle import PaddleOCR\n        engine = PaddleOCR()\n        engine.init(cfg)\n        return engine\n    \n    raise(\"unknown ocr engine\")\n\ndef load_render_engine(cfg: dict):\n    if cfg['type'] == 'simple':\n        from .render.simple import SimpleRender\n        engine = SimpleRender()\n        engine.init(cfg)\n        return engine\n    elif cfg['type'] == 'reportlab':\n        from .render.reportlab import ReportLabRender\n        engine = ReportLabRender()\n        engine.init(cfg)\n        return engine\n    \n    raise(\"unknown ocr engine\")"}
{"type": "source_file", "path": "utils/api_utils.py", "content": "from dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Optional\n\n@dataclass\nclass TranslateRequest:\n    pdf_path: Path\n    temp_output_dir: Path\n    from_lang: str\n    to_lang: str\n    translate_all: bool\n    p_from: int\n    p_to: int\n    output_file_path: Optional[Path | str] = None,\n    render_mode: Optional[str] = None,\n    add_blank_page: bool = False,\n    def extract(self):\n        if isinstance(self.pdf_path, str):\n            self.pdf_path = Path(self.pdf_path)\n        return (\n            self.pdf_path,\n            self.temp_output_dir,\n            self.from_lang,\n            self.to_lang,\n            self.translate_all,\n            self.p_from,\n            self.p_to,\n            self.output_file_path,\n            self.render_mode,\n            self.add_blank_page,\n        )"}
{"type": "source_file", "path": "utils/database/__init__.py", "content": ""}
{"type": "source_file", "path": "utils/database/base.py", "content": "# create a database to save (file, path, status)\n# file: the name of the file\n# path: the path of the file\n# status: the status of the file (translated, not translated) -> True, False\nimport sqlite3\n\nclass Database:\n    def __init__(self, database_name, table_name, table_format: dict):\n        self.database = database_name\n        self.table_name = table_name\n        self.table_format = \"\"\n        for key, value in table_format.items():\n            if value == str:\n                self.table_format += f\"{key} text, \"\n            elif value == bool:\n                self.table_format += f\"{key} boolean, \"\n            elif value == int:\n                self.table_format += f\"{key} integer, \"\n            elif value == float:\n                self.table_format += f\"{key} real, \"\n            elif value == bytes:\n                self.table_format += f\"{key} blob, \"\n            else:\n                print(\"Invalid data type\")\n\n    def new_cursor(self) -> tuple[sqlite3.Connection, sqlite3.Cursor]:\n        conn = sqlite3.connect(self.database)\n        return conn, conn.cursor()\n\n    def check_table(self):\n        # Check if the table exists\n        conn, c = self.new_cursor()\n        c.execute(\n            \"SELECT name FROM sqlite_master WHERE type='table' AND name=?\",\n            (self.table_name,),\n        )\n        if c.fetchone() is None:\n            c.execute(\n                f\"\"\"CREATE TABLE {self.table_name} ({self.table_format[:-2]})\"\"\"\n            )\n            conn.commit()\n        conn.close()\n\n    def delete_db(self):\n        conn, c = self.new_cursor()\n        c.execute(f\"DROP TABLE {self.table_name}\")\n        conn.commit()\n        conn.close()\n\n    def recreate_db(\n        self,\n    ):\n        # Check if the table exists\n        conn, c = self.new_cursor()\n        c.execute(\n            f\"SELECT name FROM sqlite_master WHERE type='table' AND name=?\", (self.table_name,)\n        )\n        if c.fetchone() is not None:\n            c.execute(\"DROP TABLE files\")\n        c.execute(\n            f\"\"\"CREATE TABLE {self.table_name}\n                        (file text, src_path text, target_path text, status boolean)\"\"\"\n        )\n        conn.commit()\n        conn.close()\n"}
{"type": "source_file", "path": "modules/render/base.py", "content": "from abc import ABC, abstractmethod\nfrom typing import Tuple\nfrom utils.layout_model import Layout\nimport numpy as np\nfrom enum import Enum\n\nclass RenderMode(Enum):\n    SIDE_BY_SIDE = 1\n    TRANSLATION_ONLY = 2\n    INTERLEAVE = 3\n    @staticmethod\n    def get_mode(mode: str):\n        if mode.lower() == \"side_by_side\":\n            return RenderMode.SIDE_BY_SIDE\n        elif mode.lower() == \"translation_only\":\n            return RenderMode.TRANSLATION_ONLY\n        elif mode.lower() == \"interleave\":\n            return RenderMode.INTERLEAVE\n    \n\nclass RenderBase(ABC):\n    @abstractmethod\n    def init(self, cfg: dict):\n        pass\n\n\n    @abstractmethod\n    def get_font_info(self, image, line_cnt):\n        \"\"\"\n        Translates a given string into another language.\n\n        Parameters:\n        - text (str): The text to be translated.\n\n        Returns:\n        - str: The translated text.\n\n        This method needs to be implemented by subclasses.\n        \"\"\"\n        pass\n\n\n    @abstractmethod\n    def get_all_fonts(self, layout):\n        \"\"\"\n        Translates a given string into another language.\n\n        Parameters:\n        - text (str): The text to be translated.\n\n        Returns:\n        - str: The translated text.\n\n        This method needs to be implemented by subclasses.\n        \"\"\"\n        pass\n\n    \n    @abstractmethod\n    def translate_one_page(\n        self,\n        image,\n        result: list[Layout],\n        reached_references: bool,\n    ) -> Tuple[np.ndarray, np.ndarray, bool]:\n        \"\"\"Translate one page of the PDF file.\"\"\"\n        pass"}
{"type": "source_file", "path": "modules/ocr/paddle.py", "content": "import re\nimport numpy as np\nfrom pathlib import Path\nfrom tqdm import tqdm\nfrom .base import OCRBase \nfrom utils import OCRModel\n\nclass PaddleOCR(OCRBase):\n    def init(self, cfg: dict):\n        self.ocr_model = OCRModel(\n            model_root_dir= Path(\"models/paddle-ocr\"), device=cfg['device']\n        )\n \n    def get_all_text(self, layout) -> str:\n\n        for i, line in enumerate(layout):\n            if line.type in [\"text\", \"list\", \"title\"]:\n                # update this so image is created from images and layout bbox info\n                image = line.image\n                ocr_results = self.get_text(image)\n                text = list(map(lambda x: x[0],ocr_results[1]))\n                text = \" \".join(text)\n                clean_text = re.sub(r\"\\n|\\t\", \" \", text)\n                line.text = clean_text\n\n                lasty = 0\n                cnt = 0\n                for x in ocr_results[0]:\n                    if x[0][1] > lasty:\n                        cnt+=1\n                        lasty = x[2][1]\n\n                line.line_cnt = cnt\n\n\n        return layout\n    \n    def get_text(self, image):\n        return self.ocr_model(image)\n                "}
{"type": "source_file", "path": "modules/render/simple.py", "content": "import re\nimport PyPDF2\nfrom loguru import logger\nimport numpy as np\nfrom tqdm import tqdm\nfrom .base import RenderBase, RenderMode\nfrom utils.layout_model import Layout\nfrom PIL import Image, ImageFont\nfrom PIL.ImageFont import FreeTypeFont\nfrom PIL import ImageDraw\nfrom typing import List, Optional, Tuple\nfrom utils import draw_text\nimport matplotlib.pyplot as plt\n\nclass SimpleRender(RenderBase):\n    FONT_SIZE = 29\n\n    def init(self, cfg: dict):\n        self.cfg = cfg\n    \n    def init_pdf(self):\n        self.reached_references = False\n\n    def get_all_fonts(self, layout: list[Layout]):\n        for line in tqdm(layout, desc=\"Getting font info\", leave=False, dynamic_ncols=True):\n            if line.type in [\"text\", \"list\", \"title\"]:\n                # update this so image is created from images and layout bbox info\n                image = line.image\n                height = line.bbox[3] - line.bbox[1]\n                width = line.bbox[2] - line.bbox[0]\n                new_block = Image.new(\"RGB\", (width, height), color=(255, 255, 255))\n                draw = ImageDraw.ImageDraw(new_block)\n                family, font_size, ygain, processed_text = self.get_font_info(line, draw)\n                line.font = {\"family\": family, \"size\": font_size, \"ygain\": ygain}\n                line.processed_text = processed_text\n\n        return layout\n\n    def get_font_info(self, \n                      line: Layout,\n                      draw: ImageDraw.ImageDraw):\n        logger.debug(f\"Getting font info for text:\\n{line.text}\")\n        if line.translated_text is None:\n            return \"SourceHanSerifSC-Medium.otf\", self.FONT_SIZE, 30, \"\"\n        font_family = \"SourceHanSerifSC-Medium.otf\"\n        image, text, line_cnt = line.image, line.translated_text.strip(), line.line_cnt\n        # if image.ndim == 3:  # If the image has channels (e.g., RGB)\n        #     height, width, _ = image.shape\n        # else:  # For a 2D image (grayscale)\n        #     height, width = image.shape\n        height = line.bbox[3] - line.bbox[1]\n        width = line.bbox[2] - line.bbox[0]\n\n        def split_text(text: str, width: int, fnt: FreeTypeFont):\n            logger.debug(f\"Splitting text with width {width}/text: \\n{text}\")\n            lines = []\n            while text:\n                # logger.debug(f\"Remaining text for width {width}: \\n{text}\")\n                initial_indent = 40 if len(lines) == 0 else 0\n                index = 0\n                if text[0] == '\\n':\n                    text = text[1:]\n                    continue\n                while (index < len(text)) and \\\n                      (text[index] != '\\n') and \\\n                      (initial_indent + draw.textlength(text[0:index + 1], font=fnt) < width):\n                    index += 1\n                # logger.debug(f\"Index: {index} / Text: {text[0:index]}\")\n                lines.append(text[0:index])\n                text = text[index:]\n            return '\\n'.join(lines)\n\n        font_size = self.FONT_SIZE\n        processed_text = None\n        render_dict = {}\n\n        while True:\n            logger.debug(f\"Try Font size: {font_size}\")\n            if(font_size < 4):\n                logger.error(\"Font size too small!\")\n                break\n            fnt_s = ImageFont.truetype(f\"fonts/{font_family}\", font_size)\n            fnt_l = ImageFont.truetype(f\"fonts/{font_family}\", font_size + 1)\n            text_s = split_text(text, width, fnt_s)\n            text_l = split_text(text, width, fnt_l)\n            bbox_s = draw.textbbox((0, 0), text_s, font=fnt_s)\n            bbox_l = draw.textbbox((0, 0), text_l, font=fnt_l)\n            if bbox_s[3] < height and bbox_l[3] >= height:\n                logger.debug(f\"Width {width} / Render width {bbox_s[2] - bbox_s[0]}\")\n                processed_text = text_s\n                break\n            elif bbox_s[3] >= height:\n                logger.debug(f\"Decreasing font size from {font_size}\")\n                font_size -= 1\n            else:\n                assert (bbox_s[3] < height and bbox_l[3] < height), f\"Font size too large! S:{bbox_s[3]} L:{bbox_l[3]} Height:{height}, condition 1: {bbox_s[3] < height and bbox_l[3] >= height}\"\n                if text_s.count('\\n') == 0:\n                    processed_text = text_s\n                    break\n                logger.debug(f\"Increasing font size from {font_size}, count newline:\"+str(text_s.count('\\n')))\n                font_size += 1\n                \n        ygain = int(font_size * 1.1)\n\n        # return 'TimesNewRoman.ttf', font_size, ygain\n        return \"SourceHanSerifSC-Medium.otf\", font_size, ygain, processed_text\n\n    def translate_one_page(\n        self,\n        image,\n        result: list[Layout],\n    ) -> Tuple[np.ndarray, np.ndarray, bool]:\n        \"\"\"Translate one page of the PDF file.\"\"\"\n        img = np.array(image, dtype=np.uint8)\n        for line in result:\n            if line.type in [\"text\", \"list\"]:\n                if line.text:\n                    height = line.bbox[3] - line.bbox[1]\n                    width = line.bbox[2] - line.bbox[0]\n\n                    # calculate text wrapping\n                    processed_text = line.processed_text\n\n                    fnt = ImageFont.truetype(\n                        \"fonts/\" + line.font[\"family\"], line.font[\"size\"]\n                    )\n                    # create new image block with new text\n                    new_block = Image.new(\"RGB\", (width, height), color=(255, 255, 255))\n                    draw = ImageDraw.Draw(new_block)\n                    draw_text(\n                        draw,\n                        processed_text,\n                        fnt,\n                        line.font[\"size\"],\n                        width,\n                        line.font[\"ygain\"],\n                    )\n\n                    # copy over original image\n\n                    new_block = np.array(new_block)\n                    img[\n                        int(line.bbox[1]) : int(line.bbox[3]),\n                        int(line.bbox[0]) : int(line.bbox[2]),\n                    ] = new_block\n\n            elif line.type == \"title\":\n                title = line.text\n                if title.lower() == \"references\" or title.lower() == \"reference\":\n                    self.reached_references = True\n            elif line.type == \"figure\":\n                continue\n            else:\n                # TODO: add list, table and image translation support\n                # print(f\"\\n\\n\\nunknown: {line.type}\")\n                logger.warning(f\"Unknown type: {line.type}\")\n        self.one_page_img = img\n    \n    def post_process(self, image, render_mode: RenderMode, output_path: str, DPI: int):\n        if self.reached_references:\n            (\n                image.convert(\"RGB\")\n                .resize((int(1400 / image.size[1] * image.size[0]), 1400))\n                .save(output_path, format=\"pdf\")\n            )\n        else:\n            if render_mode is RenderMode.SIDE_BY_SIDE:\n                fig, ax = plt.subplots(1, 2, figsize=(20, 14))\n                ax[0].imshow(image)\n                ax[1].imshow(self.one_page_img)\n                ax[0].axis(\"off\")\n                ax[1].axis(\"off\")\n            else:\n                fig, ax = plt.subplots(1, 1, figsize=(10, 14))\n                ax.imshow(self.one_page_img)\n                ax.axis(\"off\")\n            plt.tight_layout()\n            plt.savefig(output_path, format=\"pdf\", dpi=DPI)\n            plt.close(fig)\n            \n    def merge_pdfs(self, pdf_files: List[str], output_dir: Optional[str] = None, temp_dir_name=None) -> None:\n        \"\"\"Merge translated PDF files into one file.\n\n        Merged file will be stored in the temp directory\n        as \"translated.pdf\".\n\n        Parameters\n        ----------\n        pdf_files: List[str]\n            List of paths to translated PDF files stored in\n            the temp directory.\n        \"\"\"\n        pdf_merger = PyPDF2.PdfMerger()\n\n        for pdf_file in sorted(pdf_files):\n            pdf_merger.append(pdf_file)\n        if output_dir:\n            pdf_merger.write(output_dir)\n        else:\n            pdf_merger.write(temp_dir_name / \"translated.pdf\")"}
{"type": "source_file", "path": "pdf_operation/merge_pdf.py", "content": "# merge the pdf files in the output_dir\nfrom PyPDF2 import PdfFileMerger, PdfReader, PdfWriter\nimport sys\nimport os\n\nif __name__ == \"__main__\":\n    scan_dir = sys.argv[1]\n    writer = PdfWriter()\n    for file in os.listdir(scan_dir):\n        if file.endswith(\".pdf\"):\n            pdf_path = os.path.join(scan_dir, file)\n            pdf = PdfReader(pdf_path)\n            for page in pdf.pages:\n                writer.add_page(page)\n            writer.add_blank_page()\n    with open(\"output.pdf\", \"wb\") as f:\n        writer.write(f)\n        "}
{"type": "source_file", "path": "pdf_operation/add_front_blank_page.py", "content": "from PyPDF2 import PdfReader, PdfWriter\nimport os\nimport sys\n\ndef add_blank_page(input_pdf_path: str, output_pdf_path: str, front=True, back=True):\n    reader = PdfReader(input_pdf_path)\n    writer = PdfWriter()\n    if front:\n        writer.add_blank_page(reader.pages[0].mediabox.width, reader.pages[0].mediabox.height)\n    for page in reader.pages:\n        writer.add_page(page)\n    if back:\n        writer.add_blank_page(reader.pages[0].mediabox.width, reader.pages[0].mediabox.height)\n    with open(output_pdf_path, \"wb\") as f:\n        writer.write(f)\n        \nif __name__ == \"__main__\":\n    input_pdf_path = sys.argv[1]\n    output_pdf_path = sys.argv[2]\n    add_blank_page_at_front(input_pdf_path, output_pdf_path)"}
{"type": "source_file", "path": "utils/ditod/backbone.py", "content": "# --------------------------------------------------------------------------------\n# VIT: Multi-Path Vision Transformer for Dense Prediction\n# Copyright (c) 2022 Electronics and Telecommunications Research Institute (ETRI).\n# All Rights Reserved.\n# Written by Youngwan Lee\n# This source code is licensed(Dual License(GPL3.0 & Commercial)) under the license found in the\n# LICENSE file in the root directory of this source tree.\n# --------------------------------------------------------------------------------\n# References:\n# timm: https://github.com/rwightman/pytorch-image-models/tree/master/timm\n# CoaT: https://github.com/mlpc-ucsd/CoaT\n# --------------------------------------------------------------------------------\n\n\nimport torch\n\nfrom detectron2.layers import (\n    ShapeSpec,\n)\nfrom detectron2.modeling import Backbone, BACKBONE_REGISTRY, FPN\nfrom detectron2.modeling.backbone.fpn import LastLevelP6P7, LastLevelMaxPool\n\nfrom .beit import beit_base_patch16, dit_base_patch16, dit_large_patch16, beit_large_patch16\nfrom .deit import deit_base_patch16, mae_base_patch16\n\n__all__ = [\n    \"build_vit_fpn_backbone\",\n]\n\n\nclass VIT_Backbone(Backbone):\n    \"\"\"\n    Implement VIT backbone.\n    \"\"\"\n\n    def __init__(self, name, out_features, drop_path, img_size, pos_type, model_kwargs):\n        super().__init__()\n        self._out_features = out_features\n        if 'base' in name:\n            self._out_feature_strides = {\"layer3\": 4, \"layer5\": 8, \"layer7\": 16, \"layer11\": 32}\n        else:\n            self._out_feature_strides = {\"layer7\": 4, \"layer11\": 8, \"layer15\": 16, \"layer23\": 32}\n\n        if name == 'beit_base_patch16':\n            model_func = beit_base_patch16\n            self._out_feature_channels = {\"layer3\": 768, \"layer5\": 768, \"layer7\": 768, \"layer11\": 768}\n        elif name == 'dit_base_patch16':\n            model_func = dit_base_patch16\n            self._out_feature_channels = {\"layer3\": 768, \"layer5\": 768, \"layer7\": 768, \"layer11\": 768}\n        elif name == \"deit_base_patch16\":\n            model_func = deit_base_patch16\n            self._out_feature_channels = {\"layer3\": 768, \"layer5\": 768, \"layer7\": 768, \"layer11\": 768}\n        elif name == \"mae_base_patch16\":\n            model_func = mae_base_patch16\n            self._out_feature_channels = {\"layer3\": 768, \"layer5\": 768, \"layer7\": 768, \"layer11\": 768}\n        elif name == \"dit_large_patch16\":\n            model_func = dit_large_patch16\n            self._out_feature_channels = {\"layer7\": 1024, \"layer11\": 1024, \"layer15\": 1024, \"layer23\": 1024}\n        elif name == \"beit_large_patch16\":\n            model_func = beit_large_patch16\n            self._out_feature_channels = {\"layer7\": 1024, \"layer11\": 1024, \"layer15\": 1024, \"layer23\": 1024}\n        else:\n            raise ValueError(\"Unsupported VIT name yet.\")\n\n        if 'beit' in name or 'dit' in name:\n            if pos_type == \"abs\":\n                self.backbone = model_func(img_size=img_size,\n                                           out_features=out_features,\n                                           drop_path_rate=drop_path,\n                                           use_abs_pos_emb=True,\n                                           **model_kwargs)\n            elif pos_type == \"shared_rel\":\n                self.backbone = model_func(img_size=img_size,\n                                           out_features=out_features,\n                                           drop_path_rate=drop_path,\n                                           use_shared_rel_pos_bias=True,\n                                           **model_kwargs)\n            elif pos_type == \"rel\":\n                self.backbone = model_func(img_size=img_size,\n                                           out_features=out_features,\n                                           drop_path_rate=drop_path,\n                                           use_rel_pos_bias=True,\n                                           **model_kwargs)\n            else:\n                raise ValueError()\n        else:\n            self.backbone = model_func(img_size=img_size,\n                                       out_features=out_features,\n                                       drop_path_rate=drop_path,\n                                       **model_kwargs)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Tensor of shape (N,C,H,W). H, W must be a multiple of ``self.size_divisibility``.\n\n        Returns:\n            dict[str->Tensor]: names and the corresponding features\n        \"\"\"\n        assert x.dim() == 4, f\"VIT takes an input of shape (N, C, H, W). Got {x.shape} instead!\"\n        return self.backbone.forward_features(x)\n\n    def output_shape(self):\n        return {\n            name: ShapeSpec(\n                channels=self._out_feature_channels[name], stride=self._out_feature_strides[name]\n            )\n            for name in self._out_features\n        }\n\n\ndef build_VIT_backbone(cfg):\n    \"\"\"\n    Create a VIT instance from config.\n\n    Args:\n        cfg: a detectron2 CfgNode\n\n    Returns:\n        A VIT backbone instance.\n    \"\"\"\n    # fmt: off\n    name = cfg.MODEL.VIT.NAME\n    out_features = cfg.MODEL.VIT.OUT_FEATURES\n    drop_path = cfg.MODEL.VIT.DROP_PATH\n    img_size = cfg.MODEL.VIT.IMG_SIZE\n    pos_type = cfg.MODEL.VIT.POS_TYPE\n\n    model_kwargs = eval(str(cfg.MODEL.VIT.MODEL_KWARGS).replace(\"`\", \"\"))\n\n    return VIT_Backbone(name, out_features, drop_path, img_size, pos_type, model_kwargs)\n\n\n@BACKBONE_REGISTRY.register()\ndef build_vit_fpn_backbone(cfg, input_shape: ShapeSpec):\n    \"\"\"\n    Create a VIT w/ FPN backbone.\n\n    Args:\n        cfg: a detectron2 CfgNode\n\n    Returns:\n        backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.\n    \"\"\"\n    bottom_up = build_VIT_backbone(cfg)\n    in_features = cfg.MODEL.FPN.IN_FEATURES\n    out_channels = cfg.MODEL.FPN.OUT_CHANNELS\n    backbone = FPN(\n        bottom_up=bottom_up,\n        in_features=in_features,\n        out_channels=out_channels,\n        norm=cfg.MODEL.FPN.NORM,\n        top_block=LastLevelMaxPool(),\n        fuse_type=cfg.MODEL.FPN.FUSE_TYPE,\n    )\n    return backbone\n"}
{"type": "source_file", "path": "utils/ditod/table_evaluation/__init__.py", "content": "from .evaluate import calc_table_score"}
{"type": "source_file", "path": "utils/ditod/mycheckpointer.py", "content": "from detectron2.checkpoint import DetectionCheckpointer\n\nfrom typing import Any\nimport torch\nimport torch.nn as nn\nfrom fvcore.common.checkpoint import _IncompatibleKeys, _strip_prefix_if_present, TORCH_VERSION, quantization, \\\n    ObserverBase, FakeQuantizeBase\nfrom torch import distributed as dist\nfrom scipy import interpolate\nimport numpy as np\nimport torch.nn.functional as F\n\n\ndef append_prefix(k):\n    prefix = 'backbone.bottom_up.backbone.'\n    return prefix + k if not k.startswith(prefix) else k\n\n\ndef modify_ckpt_state(model, state_dict, logger=None):\n    # reshape absolute position embedding for Swin\n    if state_dict.get(append_prefix('absolute_pos_embed')) is not None:\n        absolute_pos_embed = state_dict[append_prefix('absolute_pos_embed')]\n        N1, L, C1 = absolute_pos_embed.size()\n        N2, C2, H, W = model.backbone.bottom_up.backbone.absolute_pos_embed.size()\n        if N1 != N2 or C1 != C2 or L != H * W:\n            logger.warning(\"Error in loading absolute_pos_embed, pass\")\n        else:\n            state_dict[append_prefix('absolute_pos_embed')] = absolute_pos_embed.view(N2, H, W, C2).permute(0, 3, 1, 2)\n\n    def get_dist_info():\n        if dist.is_available() and dist.is_initialized():\n            rank = dist.get_rank()\n            world_size = dist.get_world_size()\n        else:\n            rank = 0\n            world_size = 1\n        return rank, world_size\n\n    rank, _ = get_dist_info()\n    all_keys = list(state_dict.keys())\n    for key in all_keys:\n        if \"relative_position_index\" in key:\n            state_dict.pop(key)\n\n        if \"relative_position_bias_table\" in key:\n            rel_pos_bias = state_dict[key]\n            src_num_pos, num_attn_heads = rel_pos_bias.size()\n            if key not in model.state_dict():\n                continue\n            dst_num_pos, _ = model.state_dict()[key].size()\n            dst_patch_shape = model.backbone.bottom_up.backbone.patch_embed.patch_shape\n            if dst_patch_shape[0] != dst_patch_shape[1]:\n                raise NotImplementedError()\n            num_extra_tokens = dst_num_pos - (dst_patch_shape[0] * 2 - 1) * (dst_patch_shape[1] * 2 - 1)\n            src_size = int((src_num_pos - num_extra_tokens) ** 0.5)\n            dst_size = int((dst_num_pos - num_extra_tokens) ** 0.5)\n            if src_size != dst_size:\n                if rank == 0:\n                    print(\"Position interpolate for %s from %dx%d to %dx%d\" % (\n                        key, src_size, src_size, dst_size, dst_size))\n                extra_tokens = rel_pos_bias[-num_extra_tokens:, :]\n                rel_pos_bias = rel_pos_bias[:-num_extra_tokens, :]\n\n                def geometric_progression(a, r, n):\n                    return a * (1.0 - r ** n) / (1.0 - r)\n\n                left, right = 1.01, 1.5\n                while right - left > 1e-6:\n                    q = (left + right) / 2.0\n                    gp = geometric_progression(1, q, src_size // 2)\n                    if gp > dst_size // 2:\n                        right = q\n                    else:\n                        left = q\n\n                # if q > 1.13492:\n                #     q = 1.13492\n\n                dis = []\n                cur = 1\n                for i in range(src_size // 2):\n                    dis.append(cur)\n                    cur += q ** (i + 1)\n\n                r_ids = [-_ for _ in reversed(dis)]\n\n                x = r_ids + [0] + dis\n                y = r_ids + [0] + dis\n\n                t = dst_size // 2.0\n                dx = np.arange(-t, t + 0.1, 1.0)\n                dy = np.arange(-t, t + 0.1, 1.0)\n                if rank == 0:\n                    print(\"x = {}\".format(x))\n                    print(\"dx = {}\".format(dx))\n\n                all_rel_pos_bias = []\n\n                for i in range(num_attn_heads):\n                    z = rel_pos_bias[:, i].view(src_size, src_size).float().numpy()\n                    f = interpolate.interp2d(x, y, z, kind='cubic')\n                    all_rel_pos_bias.append(\n                        torch.Tensor(f(dx, dy)).contiguous().view(-1, 1).to(rel_pos_bias.device))\n\n                rel_pos_bias = torch.cat(all_rel_pos_bias, dim=-1)\n                new_rel_pos_bias = torch.cat((rel_pos_bias, extra_tokens), dim=0)\n                state_dict[key] = new_rel_pos_bias\n\n    if append_prefix('pos_embed') in state_dict:\n        pos_embed_checkpoint = state_dict[append_prefix('pos_embed')]\n        embedding_size = pos_embed_checkpoint.shape[-1]\n        num_patches = model.backbone.bottom_up.backbone.patch_embed.num_patches\n        num_extra_tokens = model.backbone.bottom_up.backbone.pos_embed.shape[-2] - num_patches\n        # height (== width) for the checkpoint position embedding\n        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n        # height (== width) for the new position embedding\n        # new_size = int(num_patches ** 0.5)\n        new_size_w = model.backbone.bottom_up.backbone.patch_embed.num_patches_w\n        new_size_h = model.backbone.bottom_up.backbone.patch_embed.num_patches_h\n        # class_token and dist_token are kept unchanged\n        if orig_size != new_size_h or orig_size != new_size_w:\n            if rank == 0:\n                print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size_w, new_size_h))\n            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n            # only the position tokens are interpolated\n            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n            pos_tokens = torch.nn.functional.interpolate(\n                pos_tokens, size=(new_size_w, new_size_h), mode='bicubic', align_corners=False)\n            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n            state_dict[append_prefix('pos_embed')] = new_pos_embed\n\n    # interpolate position bias table if needed\n    relative_position_bias_table_keys = [k for k in state_dict.keys() if \"relative_position_bias_table\" in k]\n    for table_key in relative_position_bias_table_keys:\n        table_pretrained = state_dict[table_key]\n        if table_key not in model.state_dict():\n            continue\n        table_current = model.state_dict()[table_key]\n        L1, nH1 = table_pretrained.size()\n        L2, nH2 = table_current.size()\n        if nH1 != nH2:\n            logger.warning(f\"Error in loading {table_key}, pass\")\n        else:\n            if L1 != L2:\n                S1 = int(L1 ** 0.5)\n                S2 = int(L2 ** 0.5)\n                table_pretrained_resized = F.interpolate(\n                    table_pretrained.permute(1, 0).view(1, nH1, S1, S1),\n                    size=(S2, S2), mode='bicubic')\n                state_dict[table_key] = table_pretrained_resized.view(nH2, L2).permute(1, 0)\n\n    if append_prefix('rel_pos_bias.relative_position_bias_table') in state_dict and \\\n            model.backbone.bottom_up.backbone.use_rel_pos_bias and \\\n            not model.backbone.bottom_up.backbone.use_shared_rel_pos_bias and \\\n            append_prefix('blocks.0.attn.relative_position_bias_table') not in state_dict:\n        logger.info(\"[BEIT] Expand the shared relative position embedding to each transformer block. \")\n        num_layers = model.backbone.bottom_up.backbone.get_num_layers()\n        rel_pos_bias = state_dict[append_prefix(\"rel_pos_bias.relative_position_bias_table\")]\n        for i in range(num_layers):\n            state_dict[\"blocks.%d.attn.relative_position_bias_table\" % i] = rel_pos_bias.clone()\n        state_dict.pop(append_prefix(\"rel_pos_bias.relative_position_bias_table\"))\n\n    return state_dict\n\n\nclass MyDetectionCheckpointer(DetectionCheckpointer):\n    def _load_model(self, checkpoint: Any) -> _IncompatibleKeys:\n        \"\"\"\n        Load weights from a checkpoint.\n\n        Args:\n            checkpoint (Any): checkpoint contains the weights.\n\n        Returns:\n            ``NamedTuple`` with ``missing_keys``, ``unexpected_keys``,\n                and ``incorrect_shapes`` fields:\n                * **missing_keys** is a list of str containing the missing keys\n                * **unexpected_keys** is a list of str containing the unexpected keys\n                * **incorrect_shapes** is a list of (key, shape in checkpoint, shape in model)\n\n            This is just like the return value of\n            :func:`torch.nn.Module.load_state_dict`, but with extra support\n            for ``incorrect_shapes``.\n        \"\"\"\n        checkpoint_state_dict = checkpoint.pop(\"model\")\n        self._convert_ndarray_to_tensor(checkpoint_state_dict)\n\n        # if the state_dict comes from a model that was wrapped in a\n        # DataParallel or DistributedDataParallel during serialization,\n        # remove the \"module\" prefix before performing the matching.\n        _strip_prefix_if_present(checkpoint_state_dict, \"module.\")\n\n        # workaround https://github.com/pytorch/pytorch/issues/24139\n        model_state_dict = self.model.state_dict()\n        incorrect_shapes = []\n\n        # rename the para in checkpoint_state_dict\n        # some bug here, do not support re load\n\n        checkpoint_state_dict = {\n            append_prefix(k): checkpoint_state_dict[k]\n            for k in checkpoint_state_dict.keys()\n        }\n\n        checkpoint_state_dict = modify_ckpt_state(self.model, checkpoint_state_dict, logger=self.logger)\n\n        for k in list(checkpoint_state_dict.keys()):\n            if k in model_state_dict:\n                model_param = model_state_dict[k]\n                # Allow mismatch for uninitialized parameters\n                if TORCH_VERSION >= (1, 8) and isinstance(\n                        model_param, nn.parameter.UninitializedParameter\n                ):\n                    continue\n                shape_model = tuple(model_param.shape)\n                shape_checkpoint = tuple(checkpoint_state_dict[k].shape)\n                if shape_model != shape_checkpoint:\n\n                    has_observer_base_classes = (\n                            TORCH_VERSION >= (1, 8)\n                            and hasattr(quantization, \"ObserverBase\")\n                            and hasattr(quantization, \"FakeQuantizeBase\")\n                    )\n                    if has_observer_base_classes:\n                        # Handle the special case of quantization per channel observers,\n                        # where buffer shape mismatches are expected.\n                        def _get_module_for_key(\n                                model: torch.nn.Module, key: str\n                        ) -> torch.nn.Module:\n                            # foo.bar.param_or_buffer_name -> [foo, bar]\n                            key_parts = key.split(\".\")[:-1]\n                            cur_module = model\n                            for key_part in key_parts:\n                                cur_module = getattr(cur_module, key_part)\n                            return cur_module\n\n                        cls_to_skip = (\n                            ObserverBase,\n                            FakeQuantizeBase,\n                        )\n                        target_module = _get_module_for_key(self.model, k)\n                        if isinstance(target_module, cls_to_skip):\n                            # Do not remove modules with expected shape mismatches\n                            # them from the state_dict loading. They have special logic\n                            # in _load_from_state_dict to handle the mismatches.\n                            continue\n\n                    incorrect_shapes.append((k, shape_checkpoint, shape_model))\n                    checkpoint_state_dict.pop(k)\n        incompatible = self.model.load_state_dict(checkpoint_state_dict, strict=False)\n        return _IncompatibleKeys(\n            missing_keys=incompatible.missing_keys,\n            unexpected_keys=incompatible.unexpected_keys,\n            incorrect_shapes=incorrect_shapes,\n        )\n"}
{"type": "source_file", "path": "utils/ditod/dataset_mapper.py", "content": "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n# from https://github.com/facebookresearch/detr/blob/main/d2/detr/dataset_mapper.py\n\n\nimport copy\nimport logging\n\nimport numpy as np\nimport torch\n\nfrom detectron2.data import detection_utils as utils\nfrom detectron2.data import transforms as T\n\n__all__ = [\"DetrDatasetMapper\"]\n\n\ndef build_transform_gen(cfg, is_train):\n    \"\"\"\n    Create a list of :class:`TransformGen` from config.\n    Returns:\n        list[TransformGen]\n    \"\"\"\n    if is_train:\n        min_size = cfg.INPUT.MIN_SIZE_TRAIN\n        max_size = cfg.INPUT.MAX_SIZE_TRAIN\n        sample_style = cfg.INPUT.MIN_SIZE_TRAIN_SAMPLING\n    else:\n        min_size = cfg.INPUT.MIN_SIZE_TEST\n        max_size = cfg.INPUT.MAX_SIZE_TEST\n        sample_style = \"choice\"\n    if sample_style == \"range\":\n        assert len(min_size) == 2, \"more than 2 ({}) min_size(s) are provided for ranges\".format(len(min_size))\n\n    logger = logging.getLogger(__name__)\n    tfm_gens = []\n    if is_train:\n        tfm_gens.append(T.RandomFlip())\n    tfm_gens.append(T.ResizeShortestEdge(min_size, max_size, sample_style))\n    if is_train:\n        logger.info(\"TransformGens used in training: \" + str(tfm_gens))\n    return tfm_gens\n\n\nclass DetrDatasetMapper:\n    \"\"\"\n    A callable which takes a dataset dict in Detectron2 Dataset format,\n    and map it into a format used by DETR.\n\n    The callable currently does the following:\n\n    1. Read the image from \"file_name\"\n    2. Applies geometric transforms to the image and annotation\n    3. Find and applies suitable cropping to the image and annotation\n    4. Prepare image and annotation to Tensors\n    \"\"\"\n\n    def __init__(self, cfg, is_train=True):\n        if cfg.INPUT.CROP.ENABLED and is_train:\n            self.crop_gen = [\n                T.ResizeShortestEdge([400, 500, 600], sample_style=\"choice\"),\n                T.RandomCrop(cfg.INPUT.CROP.TYPE, cfg.INPUT.CROP.SIZE),\n            ]\n        else:\n            self.crop_gen = None\n\n        self.mask_on = cfg.MODEL.MASK_ON\n        self.tfm_gens = build_transform_gen(cfg, is_train)\n        logging.getLogger(__name__).info(\n            \"Full TransformGens used in training: {}, crop: {}\".format(str(self.tfm_gens), str(self.crop_gen))\n        )\n\n        self.img_format = cfg.INPUT.FORMAT\n        self.is_train = is_train\n\n    def __call__(self, dataset_dict):\n        \"\"\"\n        Args:\n            dataset_dict (dict): Metadata of one image, in Detectron2 Dataset format.\n\n        Returns:\n            dict: a format that builtin models in detectron2 accept\n        \"\"\"\n        dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n        image = utils.read_image(dataset_dict[\"file_name\"], format=self.img_format)\n        utils.check_image_size(dataset_dict, image)\n\n        if self.crop_gen is None:\n            image, transforms = T.apply_transform_gens(self.tfm_gens, image)\n        else:\n            if np.random.rand() > 0.5:\n                image, transforms = T.apply_transform_gens(self.tfm_gens, image)\n            else:\n                image, transforms = T.apply_transform_gens(\n                    self.tfm_gens[:-1] + self.crop_gen + self.tfm_gens[-1:], image\n                )\n\n        image_shape = image.shape[:2]  # h, w\n\n        # Pytorch's dataloader is efficient on torch.Tensor due to shared-memory,\n        # but not efficient on large generic data structures due to the use of pickle & mp.Queue.\n        # Therefore it's important to use torch.Tensor.\n        dataset_dict[\"image\"] = torch.as_tensor(np.ascontiguousarray(image.transpose(2, 0, 1)))\n\n        if not self.is_train:\n            # USER: Modify this if you want to keep them for some reason.\n            dataset_dict.pop(\"annotations\", None)\n            return dataset_dict\n\n        if \"annotations\" in dataset_dict:\n            # USER: Modify this if you want to keep them for some reason.\n            for anno in dataset_dict[\"annotations\"]:\n                if not self.mask_on:\n                    anno.pop(\"segmentation\", None)\n                anno.pop(\"keypoints\", None)\n\n            # USER: Implement additional transformations if you have other types of data\n            annos = [\n                utils.transform_instance_annotations(obj, transforms, image_shape)\n                for obj in dataset_dict.pop(\"annotations\")\n                if obj.get(\"iscrowd\", 0) == 0\n            ]\n            instances = utils.annotations_to_instances(annos, image_shape)\n            dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n        return dataset_dict"}
{"type": "source_file", "path": "utils/ditod/config.py", "content": "from detectron2.config import CfgNode as CN\n\n\ndef add_vit_config(cfg):\n    \"\"\"\n    Add config for VIT.\n    \"\"\"\n    _C = cfg\n\n    _C.MODEL.VIT = CN()\n\n    # CoaT model name.\n    _C.MODEL.VIT.NAME = \"\"\n\n    # Output features from CoaT backbone.\n    _C.MODEL.VIT.OUT_FEATURES = [\"layer3\", \"layer5\", \"layer7\", \"layer11\"]\n\n    _C.MODEL.VIT.IMG_SIZE = [224, 224]\n\n    _C.MODEL.VIT.POS_TYPE = \"shared_rel\"\n\n    _C.MODEL.VIT.DROP_PATH = 0.\n\n    _C.MODEL.VIT.MODEL_KWARGS = \"{}\"\n\n    _C.SOLVER.OPTIMIZER = \"ADAMW\"\n\n    _C.SOLVER.BACKBONE_MULTIPLIER = 1.0\n\n    _C.AUG = CN()\n\n    _C.AUG.DETR = False\n"}
{"type": "source_file", "path": "utils/ditod_vgt/Wordnn_embedding.py", "content": "import numpy as np\nimport torch\nfrom torch import nn\nfrom .tokenization_bros import BrosTokenizer\n\ndef _init_weights(m):\n    if isinstance(m, nn.Linear):\n        # we use xavier_uniform following official JAX ViT:\n        torch.nn.init.xavier_uniform_(m.weight)\n        if isinstance(m, nn.Linear) and m.bias is not None:\n            nn.init.constant_(m.bias, 0)\n    elif isinstance(m, nn.LayerNorm):\n        nn.init.constant_(m.bias, 0)\n        nn.init.constant_(m.weight, 1.0)\n        \nclass WordnnEmbedding(nn.Module):\n    \"\"\"Generate chargrid embedding feature map.\n    \"\"\"\n    def __init__(self,\n                 vocab_size=30552,\n                 hidden_size=768,\n                 embedding_dim=64,\n                 bros_embedding_path=\"/bros-base-uncased/\",\n                 use_pretrain_weight=True,\n                 use_UNK_text=False):\n        \"\"\"\n        Args\n            vocab_size (int): size of vocabulary.\n            embedding_dim (int): dim of input features\n        \"\"\"\n        super().__init__()\n\n        self.embedding = nn.Embedding(vocab_size, hidden_size)\n        self.embedding_proj = nn.Linear(hidden_size, embedding_dim, bias=False)\n        # self.tokenizer = BrosTokenizer.from_pretrained(bros_embedding_path)\n        self.use_pretrain_weight = use_pretrain_weight\n        self.use_UNK_text = use_UNK_text\n        \n        self.init_weights(bros_embedding_path)\n        self.apply(_init_weights)\n\n    def init_weights(self, bros_embedding_path):\n        if self.use_pretrain_weight:\n            state_dict = torch.load(bros_embedding_path + \"pytorch_model.bin\", map_location='cpu')\n            if 'bert' in bros_embedding_path:\n                word_embs = state_dict[\"bert.embeddings.word_embeddings.weight\"]\n            elif 'bros' in bros_embedding_path:\n                word_embs = state_dict[\"embeddings.word_embeddings.weight\"]\n            elif 'layoutlm' in bros_embedding_path:\n                word_embs = state_dict[\"layoutlm.embeddings.word_embeddings.weight\"]\n            else:\n                print(\"Wrong bros_embedding_path!\")\n            self.embedding = nn.Embedding.from_pretrained(word_embs)\n            print(\"use_pretrain_weight: load model from:\", bros_embedding_path)\n        \n    def forward(self, img, batched_inputs, stride = 1):\n        \"\"\" Forward computation\n        Args:\n            img (Tensor): in shape of [B x 3 x H x W]\n            batched_inputs (list[dict]): \n        Returns:\n            Tensor: in shape of [B x N x L x D], where D is the embedding_dim.\n        \"\"\"\n        device = img.device\n        batch_b, _, batch_h, batch_w = img.size()\n\n        chargrid_map = torch.zeros((batch_b, batch_h // stride, batch_w // stride ), dtype=torch.int64).to(device)\n        \n        for iter_b in range(batch_b):\n            per_input_ids = batched_inputs[iter_b][\"input_ids\"]   \n            per_input_bbox = batched_inputs[iter_b][\"bbox\"]\n            \n            short_length_w = min(len(per_input_ids), len(per_input_bbox)) \n            \n            if short_length_w > 0 : \n                for word_idx in range(short_length_w): \n                    per_id = per_input_ids[word_idx]\n                    \n                    bbox = per_input_bbox[word_idx] / stride\n                    w_start, h_start, w_end, h_end = bbox.round().astype(np.int).tolist()\n                            \n                    if self.use_UNK_text:\n                        chargrid_map[iter_b, h_start:h_end, w_start: w_end] = 100\n                    else:\n                        chargrid_map[iter_b, h_start:h_end, w_start: w_end] = per_id\n\n        chargrid_map = self.embedding(chargrid_map)\n        chargrid_map = self.embedding_proj(chargrid_map)\n        \n        return chargrid_map.permute(0, 3, 1, 2).contiguous()\n        \n    "}
{"type": "source_file", "path": "utils/ditod/icdar_evaluation.py", "content": "import copy\nimport itertools\nimport os\nimport os.path as osp\nimport shutil\nfrom collections import OrderedDict\nfrom xml.dom.minidom import Document\n\nimport detectron2.utils.comm as comm\nimport torch\nfrom detectron2.evaluation import COCOEvaluator\nfrom detectron2.utils.file_io import PathManager\n\nfrom .table_evaluation.evaluate import calc_table_score\n\n\nclass ICDAREvaluator(COCOEvaluator):\n    def evaluate(self, img_ids=None):\n        \"\"\"\n        Args:\n            img_ids: a list of image IDs to evaluate on. Default to None for the whole dataset\n        \"\"\"\n        if self._distributed:\n            comm.synchronize()\n            predictions = comm.gather(self._predictions, dst=0)\n            predictions = list(itertools.chain(*predictions))\n\n            if not comm.is_main_process():\n                return {}\n        else:\n            predictions = self._predictions\n\n        if len(predictions) == 0:\n            self._logger.warning(\"[COCOEvaluator] Did not receive valid predictions.\")\n            return {}\n\n        if self._output_dir:\n            PathManager.mkdirs(self._output_dir)\n            file_path = os.path.join(self._output_dir, \"instances_predictions.pth\")\n            with PathManager.open(file_path, \"wb\") as f:\n                torch.save(predictions, f)\n\n        self._results = OrderedDict()\n        if \"proposals\" in predictions[0]:\n            self._eval_box_proposals(predictions)\n        if \"instances\" in predictions[0]:\n            self._eval_predictions(predictions, img_ids=img_ids)\n            self.evaluate_table(predictions)\n        # Copy so the caller can do whatever with results\n        return copy.deepcopy(self._results)\n\n    def evaluate_table(self, predictions):\n        xml_dir = self.convert_to_xml(predictions)\n        results = calc_table_score(xml_dir)\n        self._results[\"wF1\"] = results['wF1']\n\n    def convert_to_xml(self, predictions):\n        output_dir = osp.join(self._output_dir, \"xml_results\")\n        if os.path.exists(output_dir):\n            shutil.rmtree(output_dir)\n        os.makedirs(output_dir, exist_ok=True)\n        coco_results = list(itertools.chain(*[x[\"instances\"] for x in predictions]))\n        results_dict = {}\n        for result in coco_results:\n            if result[\"score\"] < 0.7:\n                continue\n            image_id = result[\"image_id\"]\n            if image_id not in results_dict:\n                results_dict[image_id] = []\n\n            results_dict[image_id].append(result)\n\n        for image_id, tables in results_dict.items():\n            file_name = f\"cTDaR_t{image_id:05d}.jpg\"\n            doc = Document()\n            root = doc.createElement('document')\n            root.setAttribute('filename', file_name)\n            doc.appendChild(root)\n            for table_id, table in enumerate(tables, start=1):\n                nodeManager = doc.createElement('table')\n                nodeManager.setAttribute('id', str(table_id))\n                bbox = list(map(int, table['bbox']))\n                bbox_str = '{},{} {},{} {},{} {},{}'.format(bbox[0], bbox[1],\n                                                            bbox[0], bbox[1] + bbox[3],\n                                                            bbox[0] + bbox[2], bbox[1] + bbox[3],\n                                                            bbox[0] + bbox[2], bbox[1])\n                nodeCoords = doc.createElement('Coords')\n                nodeCoords.setAttribute('points', bbox_str)\n                nodeManager.appendChild(nodeCoords)\n                root.appendChild(nodeManager)\n            filename = '{}-result.xml'.format(file_name[:-4])\n            fp = open(os.path.join(output_dir, filename), 'w')\n            doc.writexml(fp, indent='', addindent='\\t', newl='\\n', encoding=\"utf-8\")\n            fp.flush()\n            fp.close()\n        return output_dir\n\n\nif __name__ == '__main__':\n    pass\n"}
{"type": "source_file", "path": "utils/ditod/beit.py", "content": "\"\"\" Vision Transformer (ViT) in PyTorch\n\nA PyTorch implement of Vision Transformers as described in\n'An Image Is Worth 16 x 16 Words: Transformers for Image Recognition at Scale' - https://arxiv.org/abs/2010.11929\n\nThe official jax code is released and available at https://github.com/google-research/vision_transformer\n\nStatus/TODO:\n* Models updated to be compatible with official impl. Args added to support backward compat for old PyTorch weights.\n* Weights ported from official jax impl for 384x384 base and small models, 16x16 and 32x32 patches.\n* Trained (supervised on ImageNet-1k) my custom 'small' patch model to 77.9, 'base' to 79.4 top-1 with this code.\n* Hopefully find time and GPUs for SSL or unsupervised pretraining on OpenImages w/ ImageNet fine-tune in future.\n\nAcknowledgments:\n* The paper authors for releasing code and weights, thanks!\n* I fixed my class token impl based on Phil Wang's https://github.com/lucidrains/vit-pytorch ... check it out\nfor some einops/einsum fun\n* Simple transformer style inspired by Andrej Karpathy's https://github.com/karpathy/minGPT\n* Bert reference code checks against Huggingface Transformers and Tensorflow Bert\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport warnings\nimport math\nimport torch\nfrom functools import partial\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint\nfrom timm.models.layers import drop_path, to_2tuple, trunc_normal_\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n        'crop_pct': .9, 'interpolation': 'bicubic',\n        'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5),\n        **kwargs\n    }\n\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    \"\"\"\n\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n\n    def extra_repr(self) -> str:\n        return 'p={}'.format(self.drop_prob)\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        # x = self.drop(x)\n        # commit this for the orignal BERT implement\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(\n            self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.,\n            proj_drop=0., window_size=None, attn_head_dim=None):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        if attn_head_dim is not None:\n            head_dim = attn_head_dim\n        all_head_dim = head_dim * self.num_heads\n        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n        if qkv_bias:\n            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n        else:\n            self.q_bias = None\n            self.v_bias = None\n\n        if window_size:\n            self.window_size = window_size\n            self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n            self.relative_position_bias_table = nn.Parameter(\n                torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n            # cls to token & token 2 cls & cls to cls\n\n            # get pair-wise relative position index for each token inside the window\n            coords_h = torch.arange(window_size[0])\n            coords_w = torch.arange(window_size[1])\n            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n            relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n            relative_coords[:, :, 1] += window_size[1] - 1\n            relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n            relative_position_index = \\\n                torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n            relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n            relative_position_index[0, 0:] = self.num_relative_distance - 3\n            relative_position_index[0:, 0] = self.num_relative_distance - 2\n            relative_position_index[0, 0] = self.num_relative_distance - 1\n\n            self.register_buffer(\"relative_position_index\", relative_position_index)\n\n            # trunc_normal_(self.relative_position_bias_table, std=.0)\n        else:\n            self.window_size = None\n            self.relative_position_bias_table = None\n            self.relative_position_index = None\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(all_head_dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x, rel_pos_bias=None, training_window_size=None):\n        B, N, C = x.shape\n        qkv_bias = None\n        if self.q_bias is not None:\n            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n        # qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))\n\n        if self.relative_position_bias_table is not None:\n            if training_window_size == self.window_size:\n                relative_position_bias = \\\n                    self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n                        self.window_size[0] * self.window_size[1] + 1,\n                        self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n                relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n                attn = attn + relative_position_bias.unsqueeze(0)\n            else:\n                training_window_size = tuple(training_window_size.tolist())\n                new_num_relative_distance = (2 * training_window_size[0] - 1) * (2 * training_window_size[1] - 1) + 3\n                # new_num_relative_dis  cls-clstok-clscls-tok\n                new_relative_position_bias_table = F.interpolate(\n                    self.relative_position_bias_table[:-3, :].permute(1, 0).view(1, self.num_heads,\n                                                                                 2 * self.window_size[0] - 1,\n                                                                                 2 * self.window_size[1] - 1),\n                    size=(2 * training_window_size[0] - 1, 2 * training_window_size[1] - 1), mode='bicubic',\n                    align_corners=False)\n                new_relative_position_bias_table = new_relative_position_bias_table.view(self.num_heads,\n                                                                                         new_num_relative_distance - 3).permute(\n                    1, 0)\n                new_relative_position_bias_table = torch.cat(\n                    [new_relative_position_bias_table, self.relative_position_bias_table[-3::]], dim=0)\n\n                # get pair-wise relative position index for each token inside the window\n                coords_h = torch.arange(training_window_size[0])\n                coords_w = torch.arange(training_window_size[1])\n                coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n                coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n                relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n                relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n                relative_coords[:, :, 0] += training_window_size[0] - 1  # shift to start from 0\n                relative_coords[:, :, 1] += training_window_size[1] - 1\n                relative_coords[:, :, 0] *= 2 * training_window_size[1] - 1\n                relative_position_index = \\\n                    torch.zeros(size=(training_window_size[0] * training_window_size[1] + 1,) * 2,\n                                dtype=relative_coords.dtype)\n                relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n                relative_position_index[0, 0:] = new_num_relative_distance - 3\n                relative_position_index[0:, 0] = new_num_relative_distance - 2\n                relative_position_index[0, 0] = new_num_relative_distance - 1\n\n                relative_position_bias = \\\n                    new_relative_position_bias_table[relative_position_index.view(-1)].view(\n                        training_window_size[0] * training_window_size[1] + 1,\n                        training_window_size[0] * training_window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n                relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n                attn = attn + relative_position_bias.unsqueeze(0)\n\n        if rel_pos_bias is not None:\n            attn = attn + rel_pos_bias\n\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n                 window_size=None, attn_head_dim=None):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        if init_values is not None:\n            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n        else:\n            self.gamma_1, self.gamma_2 = None, None\n\n    def forward(self, x, rel_pos_bias=None, training_window_size=None):\n        if self.gamma_1 is None:\n            x = x + self.drop_path(\n                self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias, training_window_size=training_window_size))\n            x = x + self.drop_path(self.mlp(self.norm2(x)))\n        else:\n            x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias,\n                                                            training_window_size=training_window_size))\n            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n        return x\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n\n    def __init__(self, img_size=[224, 224], patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n        self.num_patches_w = self.patch_shape[0]\n        self.num_patches_h = self.patch_shape[1]\n        # the so-called patch_shape is the patch shape during pre-training\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x, position_embedding=None, **kwargs):\n        # FIXME look at relaxing size constraints\n        # assert H == self.img_size[0] and W == self.img_size[1], \\\n        #     f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x)\n        Hp, Wp = x.shape[2], x.shape[3]\n\n        if position_embedding is not None:\n            # interpolate the position embedding to the corresponding size\n            position_embedding = position_embedding.view(1, self.patch_shape[0], self.patch_shape[1], -1).permute(0, 3,\n                                                                                                                  1, 2)\n            position_embedding = F.interpolate(position_embedding, size=(Hp, Wp), mode='bicubic')\n            x = x + position_embedding\n\n        x = x.flatten(2).transpose(1, 2)\n        return x, (Hp, Wp)\n\n\nclass HybridEmbed(nn.Module):\n    \"\"\" CNN Feature Map Embedding\n    Extract feature map from CNN, flatten, project to embedding dim.\n    \"\"\"\n\n    def __init__(self, backbone, img_size=[224, 224], feature_size=None, in_chans=3, embed_dim=768):\n        super().__init__()\n        assert isinstance(backbone, nn.Module)\n        img_size = to_2tuple(img_size)\n        self.img_size = img_size\n        self.backbone = backbone\n        if feature_size is None:\n            with torch.no_grad():\n                # FIXME this is hacky, but most reliable way of determining the exact dim of the output feature\n                # map for all networks, the feature metadata has reliable channel and stride info, but using\n                # stride to calc feature dim requires info about padding of each stage that isn't captured.\n                training = backbone.training\n                if training:\n                    backbone.eval()\n                o = self.backbone(torch.zeros(1, in_chans, img_size[0], img_size[1]))[-1]\n                feature_size = o.shape[-2:]\n                feature_dim = o.shape[1]\n                backbone.train(training)\n        else:\n            feature_size = to_2tuple(feature_size)\n            feature_dim = self.backbone.feature_info.channels()[-1]\n        self.num_patches = feature_size[0] * feature_size[1]\n        self.proj = nn.Linear(feature_dim, embed_dim)\n\n    def forward(self, x):\n        x = self.backbone(x)[-1]\n        x = x.flatten(2).transpose(1, 2)\n        x = self.proj(x)\n        return x\n\n\nclass RelativePositionBias(nn.Module):\n\n    def __init__(self, window_size, num_heads):\n        super().__init__()\n        self.window_size = window_size\n        self.num_heads = num_heads\n        self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n        # cls to token & token 2 cls & cls to cls\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(window_size[0])\n        coords_w = torch.arange(window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n        relative_position_index = \\\n            torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n        relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        relative_position_index[0, 0:] = self.num_relative_distance - 3\n        relative_position_index[0:, 0] = self.num_relative_distance - 2\n        relative_position_index[0, 0] = self.num_relative_distance - 1\n\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n        # trunc_normal_(self.relative_position_bias_table, std=.02)\n\n    def forward(self, training_window_size):\n        if training_window_size == self.window_size:\n            relative_position_bias = \\\n                self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n                    self.window_size[0] * self.window_size[1] + 1,\n                    self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n        else:\n            training_window_size = tuple(training_window_size.tolist())\n            new_num_relative_distance = (2 * training_window_size[0] - 1) * (2 * training_window_size[1] - 1) + 3\n            # new_num_relative_dis  cls-clstok-clscls-tok\n            new_relative_position_bias_table = F.interpolate(\n                self.relative_position_bias_table[:-3, :].permute(1, 0).view(1, self.num_heads,\n                                                                             2 * self.window_size[0] - 1,\n                                                                             2 * self.window_size[1] - 1),\n                size=(2 * training_window_size[0] - 1, 2 * training_window_size[1] - 1), mode='bicubic',\n                align_corners=False)\n            new_relative_position_bias_table = new_relative_position_bias_table.view(self.num_heads,\n                                                                                     new_num_relative_distance - 3).permute(\n                1, 0)\n            new_relative_position_bias_table = torch.cat(\n                [new_relative_position_bias_table, self.relative_position_bias_table[-3::]], dim=0)\n\n            # get pair-wise relative position index for each token inside the window\n            coords_h = torch.arange(training_window_size[0])\n            coords_w = torch.arange(training_window_size[1])\n            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n            relative_coords[:, :, 0] += training_window_size[0] - 1  # shift to start from 0\n            relative_coords[:, :, 1] += training_window_size[1] - 1\n            relative_coords[:, :, 0] *= 2 * training_window_size[1] - 1\n            relative_position_index = \\\n                torch.zeros(size=(training_window_size[0] * training_window_size[1] + 1,) * 2,\n                            dtype=relative_coords.dtype)\n            relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n            relative_position_index[0, 0:] = new_num_relative_distance - 3\n            relative_position_index[0:, 0] = new_num_relative_distance - 2\n            relative_position_index[0, 0] = new_num_relative_distance - 1\n\n            relative_position_bias = \\\n                new_relative_position_bias_table[relative_position_index.view(-1)].view(\n                    training_window_size[0] * training_window_size[1] + 1,\n                    training_window_size[0] * training_window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n\n        return relative_position_bias\n\n\nclass BEiT(nn.Module):\n    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n    \"\"\"\n\n    def __init__(self,\n                 img_size=[224, 224],\n                 patch_size=16,\n                 in_chans=3,\n                 num_classes=80,\n                 embed_dim=768,\n                 depth=12,\n                 num_heads=12,\n                 mlp_ratio=4.,\n                 qkv_bias=False,\n                 qk_scale=None,\n                 drop_rate=0.,\n                 attn_drop_rate=0.,\n                 drop_path_rate=0.,\n                 hybrid_backbone=None,\n                 norm_layer=None,\n                 init_values=None,\n                 use_abs_pos_emb=False,\n                 use_rel_pos_bias=False,\n                 use_shared_rel_pos_bias=False,\n                 use_checkpoint=True,\n                 pretrained=None,\n                 out_features=None,\n                 ):\n\n        super(BEiT, self).__init__()\n\n        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n        self.use_checkpoint = use_checkpoint\n\n        if hybrid_backbone is not None:\n            self.patch_embed = HybridEmbed(\n                hybrid_backbone, img_size=img_size, in_chans=in_chans, embed_dim=embed_dim)\n        else:\n            self.patch_embed = PatchEmbed(\n                img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n        num_patches = self.patch_embed.num_patches\n        self.out_features = out_features\n        self.out_indices = [int(name[5:]) for name in out_features]\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        # self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        if use_abs_pos_emb:\n            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        else:\n            self.pos_embed = None\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        self.use_shared_rel_pos_bias = use_shared_rel_pos_bias\n        if use_shared_rel_pos_bias:\n            self.rel_pos_bias = RelativePositionBias(window_size=self.patch_embed.patch_shape, num_heads=num_heads)\n        else:\n            self.rel_pos_bias = None\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        self.use_rel_pos_bias = use_rel_pos_bias\n        self.blocks = nn.ModuleList([\n            Block(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n                init_values=init_values, window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None)\n            for i in range(depth)])\n\n        # trunc_normal_(self.mask_token, std=.02)\n\n        if patch_size == 16:\n            self.fpn1 = nn.Sequential(\n                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n                # nn.SyncBatchNorm(embed_dim),\n                nn.BatchNorm2d(embed_dim),\n                nn.GELU(),\n                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n            )\n\n            self.fpn2 = nn.Sequential(\n                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n            )\n\n            self.fpn3 = nn.Identity()\n\n            self.fpn4 = nn.MaxPool2d(kernel_size=2, stride=2)\n        elif patch_size == 8:\n            self.fpn1 = nn.Sequential(\n                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n            )\n\n            self.fpn2 = nn.Identity()\n\n            self.fpn3 = nn.Sequential(\n                nn.MaxPool2d(kernel_size=2, stride=2),\n            )\n\n            self.fpn4 = nn.Sequential(\n                nn.MaxPool2d(kernel_size=4, stride=4),\n            )\n\n        if self.pos_embed is not None:\n            trunc_normal_(self.pos_embed, std=.02)\n        trunc_normal_(self.cls_token, std=.02)\n        self.apply(self._init_weights)\n        self.fix_init_weight()\n\n    def fix_init_weight(self):\n        def rescale(param, layer_id):\n            param.div_(math.sqrt(2.0 * layer_id))\n\n        for layer_id, layer in enumerate(self.blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    '''\n    def init_weights(self):\n        \"\"\"Initialize the weights in backbone.\n\n        Args:\n            pretrained (str, optional): Path to pre-trained weights.\n                Defaults to None.\n        \"\"\"\n        logger = get_root_logger()\n\n        if self.pos_embed is not None:\n            trunc_normal_(self.pos_embed, std=.02)\n        trunc_normal_(self.cls_token, std=.02)\n        self.apply(self._init_weights)\n        self.fix_init_weight()\n\n        if self.init_cfg is None:\n            logger.warn(f'No pre-trained weights for '\n                        f'{self.__class__.__name__}, '\n                        f'training start from scratch')\n        else:\n            assert 'checkpoint' in self.init_cfg, f'Only support ' \\\n                                                  f'specify `Pretrained` in ' \\\n                                                  f'`init_cfg` in ' \\\n                                                  f'{self.__class__.__name__} '\n            logger.info(f\"Will load ckpt from {self.init_cfg['checkpoint']}\")\n            load_checkpoint(self,\n                            filename=self.init_cfg['checkpoint'],\n                            strict=False,\n                            logger=logger,\n                            beit_spec_expand_rel_pos = self.use_rel_pos_bias,\n                            )\n    '''\n\n    def get_num_layers(self):\n        return len(self.blocks)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'pos_embed', 'cls_token'}\n\n    def forward_features(self, x):\n        B, C, H, W = x.shape\n        x, (Hp, Wp) = self.patch_embed(x, self.pos_embed[:, 1:, :] if self.pos_embed is not None else None)\n        # Hp, Wp are HW for patches\n        batch_size, seq_len, _ = x.size()\n\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        if self.pos_embed is not None:\n            cls_tokens = cls_tokens + self.pos_embed[:, :1, :]\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = self.pos_drop(x)\n\n        features = []\n        training_window_size = torch.tensor([Hp, Wp])\n\n        rel_pos_bias = self.rel_pos_bias(training_window_size) if self.rel_pos_bias is not None else None\n\n        for i, blk in enumerate(self.blocks):\n            if self.use_checkpoint:\n                x = checkpoint.checkpoint(blk, x, rel_pos_bias, training_window_size)\n            else:\n                x = blk(x, rel_pos_bias=rel_pos_bias, training_window_size=training_window_size)\n            if i in self.out_indices:\n                xp = x[:, 1:, :].permute(0, 2, 1).reshape(B, -1, Hp, Wp)\n                features.append(xp.contiguous())\n\n        ops = [self.fpn1, self.fpn2, self.fpn3, self.fpn4]\n        for i in range(len(features)):\n            features[i] = ops[i](features[i])\n\n        feat_out = {}\n\n        for name, value in zip(self.out_features, features):\n            feat_out[name] = value\n\n        return feat_out\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        return x\n\n\ndef beit_base_patch16(pretrained=False, **kwargs):\n    model = BEiT(\n        patch_size=16,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_values=None,\n        **kwargs)\n    model.default_cfg = _cfg()\n    return model\n\ndef beit_large_patch16(pretrained=False, **kwargs):\n    model = BEiT(\n        patch_size=16,\n        embed_dim=1024,\n        depth=24,\n        num_heads=16,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_values=None,\n        **kwargs)\n    model.default_cfg = _cfg()\n    return model\n\ndef dit_base_patch16(pretrained=False, **kwargs):\n    model = BEiT(\n        patch_size=16,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_values=0.1,\n        **kwargs)\n    model.default_cfg = _cfg()\n    return model\n\ndef dit_large_patch16(pretrained=False, **kwargs):\n    model = BEiT(\n        patch_size=16,\n        embed_dim=1024,\n        depth=24,\n        num_heads=16,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_values=1e-5,\n        **kwargs)\n    model.default_cfg = _cfg()\n    return model\n\nif __name__ == '__main__':\n    model = BEiT(use_checkpoint=True, use_shared_rel_pos_bias=True)\n    model = model.to(\"cuda:0\")\n    input1 = torch.rand(2, 3, 512, 762).to(\"cuda:0\")\n    input2 = torch.rand(2, 3, 800, 1200).to(\"cuda:0\")\n    input3 = torch.rand(2, 3, 720, 1000).to(\"cuda:0\")\n    output1 = model(input1)\n    output2 = model(input2)\n    output3 = model(input3)\n    print(\"all done\")\n"}
{"type": "source_file", "path": "utils/database/request_db.py", "content": "from .base import Database\nfrom ..api_utils import TranslateRequest\n\nclass RequestDatabase(Database):\n    def __init__(\n        self,\n        database_name,\n        table_name=\"pdf_translator_requests\",\n        table_format={\n            \"pdf_path\": str,\n            \"temp_output_dir\": str,\n            \"from_lang\": str,\n            \"to_lang\": str,\n            \"translate_all\": bool,\n            \"p_from\": int,\n            \"p_to\": int,\n            \"output_file_path\": str,\n            \"render_mode\": str,\n            \"add_blank_page\": bool,\n        },\n    ):\n        super().__init__(database_name, table_name, table_format=table_format)\n        \n    def add_request(self, request: TranslateRequest):\n        self.c.execute(\n            f\"INSERT INTO {self.table_name} VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n            (\n                str(request.pdf_path),\n                str(request.temp_output_dir),\n                request.from_lang,\n                request.to_lang,\n                request.translate_all,\n                request.p_from,\n                request.p_to,\n                str(request.output_file_path),\n                request.render_mode,\n                request.add_blank_page,\n            ),\n        )\n        self.conn.commit()"}
{"type": "source_file", "path": "utils/ditod_vgt/config.py", "content": "from detectron2.config import CfgNode as CN\n\n\ndef add_vit_config(cfg):\n    \"\"\"\n    Add config for VIT.\n    \"\"\"\n    _C = cfg\n\n    _C.MODEL.VIT = CN()\n\n    # CoaT model name.\n    _C.MODEL.VIT.NAME = \"\"\n\n    # Output features from CoaT backbone.\n    _C.MODEL.VIT.OUT_FEATURES = [\"layer3\", \"layer5\", \"layer7\", \"layer11\"]\n\n    _C.MODEL.VIT.IMG_SIZE = [224, 224]\n\n    _C.MODEL.VIT.POS_TYPE = \"shared_rel\"\n    \n    _C.MODEL.VIT.MERGE_TYPE = \"Sum\"\n\n    _C.MODEL.VIT.DROP_PATH = 0.\n\n    _C.MODEL.VIT.MODEL_KWARGS = \"{}\"\n\n    _C.SOLVER.OPTIMIZER = \"ADAMW\"\n\n    _C.SOLVER.BACKBONE_MULTIPLIER = 1.0\n\n    _C.AUG = CN()\n\n    _C.AUG.DETR = False\n    \n    _C.MODEL.WORDGRID = CN()\n    \n    _C.MODEL.WORDGRID.VOCAB_SIZE = 30552\n    \n    _C.MODEL.WORDGRID.EMBEDDING_DIM = 64 \n    \n    _C.MODEL.WORDGRID.MODEL_PATH = ''\n    \n    _C.MODEL.WORDGRID.HIDDEN_SIZE = 768\n    \n    _C.MODEL.WORDGRID.USE_PRETRAIN_WEIGHT = True\n    \n    _C.MODEL.WORDGRID.USE_UNK_TEXT = False\n"}
{"type": "source_file", "path": "utils/ditod_vgt/VGTbeit.py", "content": "\"\"\" Vision Transformer (ViT) in PyTorch\n\nA PyTorch implement of Vision Transformers as described in\n'An Image Is Worth 16 x 16 Words: Transformers for Image Recognition at Scale' - https://arxiv.org/abs/2010.11929\n\nThe official jax code is released and available at https://github.com/google-research/vision_transformer\n\nStatus/TODO:\n* Models updated to be compatible with official impl. Args added to support backward compat for old PyTorch weights.\n* Weights ported from official jax impl for 384x384 base and small models, 16x16 and 32x32 patches.\n* Trained (supervised on ImageNet-1k) my custom 'small' patch model to 77.9, 'base' to 79.4 top-1 with this code.\n* Hopefully find time and GPUs for SSL or unsupervised pretraining on OpenImages w/ ImageNet fine-tune in future.\n\nAcknowledgments:\n* The paper authors for releasing code and weights, thanks!\n* I fixed my class token impl based on Phil Wang's https://github.com/lucidrains/vit-pytorch ... check it out\nfor some einops/einsum fun\n* Simple transformer style inspired by Andrej Karpathy's https://github.com/karpathy/minGPT\n* Bert reference code checks against Huggingface Transformers and Tensorflow Bert\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport warnings\nimport math\nimport torch\nfrom functools import partial\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint\nfrom timm.models.layers import drop_path, to_2tuple, trunc_normal_\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n        'crop_pct': .9, 'interpolation': 'bicubic',\n        'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5),\n        **kwargs\n    }\n\ndef torch_memory(device, tag=\"\"):\n    # Checks and prints GPU memory\n    print(tag, f'{torch.cuda.memory_allocated(device)/1024/1024:.2f} MB USED')\n    print(tag, f'{torch.cuda.memory_reserved(device)/1024/1024:.2f} MB RESERVED')\n    print(tag, f'{torch.cuda.max_memory_allocated(device)/1024/1024:.2f} MB USED MAX')\n    print(tag, f'{torch.cuda.max_memory_reserved(device)/1024/1024:.2f} MB RESERVED MAX')\n    print('')\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    \"\"\"\n\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n\n    def extra_repr(self) -> str:\n        return 'p={}'.format(self.drop_prob)\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        # x = self.drop(x)\n        # commit this for the orignal BERT implement\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\nclass CrossAttention(nn.Module):\n    def __init__(\n            self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.,\n            proj_drop=0., window_size=None, attn_head_dim=None):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        if attn_head_dim is not None:\n            head_dim = attn_head_dim\n        all_head_dim = head_dim * self.num_heads\n        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.q = nn.Linear(dim, all_head_dim, bias=False)\n        self.kv = nn.Linear(dim, all_head_dim * 2, bias=False)\n\n        if qkv_bias:\n            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n        else:\n            self.q_bias = None\n            self.v_bias = None\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(all_head_dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x, y):\n        B, N, C = x.shape\n        qkv_bias = None\n        if self.q_bias is not None:\n            kv_bias = torch.cat((torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n        # qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        kv = F.linear(input=y, weight=self.kv.weight, bias=kv_bias)\n        kv = kv.reshape(B, N, 2, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n        k, v = kv[0], kv[1]  # make torchscript happy (cannot use tensor as tuple)\n\n        q = F.linear(input=x, weight=self.q.weight, bias=self.q_bias)\n        q = q.reshape(B, N, 1, self.num_heads, -1).permute(2, 0, 3, 1, 4)[0]\n\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))\n\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\nclass CrossBlock(nn.Module):\n\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n                 window_size=None, attn_head_dim=None):\n        super().__init__()\n        self.norm_vis = norm_layer(dim)\n        self.norm_grid = norm_layer(dim)\n        self.vis_attn = CrossAttention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim)\n        self.grid_attn = CrossAttention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2_vis = norm_layer(dim)\n        self.norm2_grid = norm_layer(dim)\n\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.vis_mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n        self.grid_mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        self.self_block = CrossSelfBlock(dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop, attn_drop=attn_drop, drop_path=drop_path, init_values=init_values, act_layer=act_layer, norm_layer=norm_layer, window_size=window_size, attn_head_dim=attn_head_dim)\n\n        if init_values is not None:\n            self.gamma_vis = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n            self.gamma_grid = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n        else:\n            self.gamma_vis, self.gamma_grid, self.gamma_1, self.gamma_2 = None, None, None, None\n\n    def cross_att(self, vis_input, grid_input):\n        # Cross Attention\n        if self.gamma_vis is None:\n            vis_att_output = vis_input + self.drop_path(self.vis_attn(self.norm_vis(vis_input), self.norm_grid(grid_input)))\n            grid_att_output = grid_input + self.drop_path(self.grid_attn(self.norm_grid(grid_input), self.norm_vis(vis_input)))\n        else:\n            vis_att_output = vis_input + self.drop_path(self.gamma_vis * self.vis_attn(self.norm_vis(vis_input), self.norm_grid(grid_input)))\n            grid_att_output = grid_input + self.drop_path(self.gamma_grid * self.grid_attn(self.norm_grid(grid_input), self.norm_vis(vis_input)))\n        return vis_att_output, grid_att_output\n\n    def forward(self, vis_input, grid_input):\n        vis_att_output, grid_att_output = self.cross_att(vis_input, grid_input)\n        vis_output, grid_output = self.self_block(vis_att_output, grid_att_output)\n\n        if self.gamma_1 is None:\n            vis_output = vis_output + self.drop_path(self.vis_mlp(self.norm2_vis(vis_output)))\n            grid_output = grid_output + self.drop_path(self.grid_mlp(self.norm2_grid(grid_output)))\n        else:\n            vis_output = vis_output + self.drop_path(self.gamma_1 * self.vis_mlp(self.norm2_vis(vis_output)))\n            grid_output = grid_output + self.drop_path(self.gamma_2 * self.grid_mlp(self.norm2_grid(grid_output)))\n\n        return vis_output, grid_output\n\nclass CrossSelfBlock(nn.Module):\n\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n                 window_size=None, attn_head_dim=None):\n        super().__init__()\n        self.norm_vis = norm_layer(dim)\n        self.norm_grid = norm_layer(dim)\n        self.vis_attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim)\n        self.grid_attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        mlp_hidden_dim = int(dim * mlp_ratio)\n\n        if init_values is not None:\n            self.gamma_vis = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n            self.gamma_grid = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n        else:\n            self.gamma_vis, self.gamma_grid = None, None\n\n    def self_att(self, vis_input, grid_input):\n        # Cross Attention\n        if self.gamma_vis is None:\n            vis_att_output = vis_input + self.drop_path(self.vis_attn(self.norm_vis(vis_input)))\n            grid_att_output = grid_input + self.drop_path(self.grid_attn(self.norm_grid(grid_input)))\n        else:\n            vis_att_output = vis_input + self.drop_path(self.gamma_vis * self.vis_attn(self.norm_vis(vis_input)))\n            grid_att_output = grid_input + self.drop_path(self.gamma_grid * self.grid_attn(self.norm_grid(grid_input)))\n        return vis_att_output, grid_att_output\n\n    def forward(self, vis_input, grid_input):\n        vis_att_output, grid_att_output = self.self_att(vis_input, grid_input)\n\n        return vis_att_output, grid_att_output\n    \nclass Attention(nn.Module):\n    def __init__(\n            self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.,\n            proj_drop=0., window_size=None, attn_head_dim=None):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        if attn_head_dim is not None:\n            head_dim = attn_head_dim\n        all_head_dim = head_dim * self.num_heads\n        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n        if qkv_bias:\n            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n        else:\n            self.q_bias = None\n            self.v_bias = None\n\n        if window_size:\n            self.window_size = window_size\n            self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n            self.relative_position_bias_table = nn.Parameter(\n                torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n            # cls to token & token 2 cls & cls to cls\n\n            # get pair-wise relative position index for each token inside the window\n            coords_h = torch.arange(window_size[0])\n            coords_w = torch.arange(window_size[1])\n            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n            relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n            relative_coords[:, :, 1] += window_size[1] - 1\n            relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n            relative_position_index = \\\n                torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n            relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n            relative_position_index[0, 0:] = self.num_relative_distance - 3\n            relative_position_index[0:, 0] = self.num_relative_distance - 2\n            relative_position_index[0, 0] = self.num_relative_distance - 1\n\n            self.register_buffer(\"relative_position_index\", relative_position_index)\n\n            # trunc_normal_(self.relative_position_bias_table, std=.0)\n        else:\n            self.window_size = None\n            self.relative_position_bias_table = None\n            self.relative_position_index = None\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(all_head_dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x, rel_pos_bias=None, training_window_size=None):\n        B, N, C = x.shape\n        qkv_bias = None\n        if self.q_bias is not None:\n            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n        # qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))\n\n        if self.relative_position_bias_table is not None:\n            if training_window_size == self.window_size:\n                relative_position_bias = \\\n                    self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n                        self.window_size[0] * self.window_size[1] + 1,\n                        self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n                relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n                attn = attn + relative_position_bias.unsqueeze(0)\n            else:\n                training_window_size = tuple(training_window_size.tolist())\n                new_num_relative_distance = (2 * training_window_size[0] - 1) * (2 * training_window_size[1] - 1) + 3\n                # new_num_relative_dis  cls-clstok-clscls-tok\n                new_relative_position_bias_table = F.interpolate(\n                    self.relative_position_bias_table[:-3, :].permute(1, 0).view(1, self.num_heads,\n                                                                                 2 * self.window_size[0] - 1,\n                                                                                 2 * self.window_size[1] - 1),\n                    size=(2 * training_window_size[0] - 1, 2 * training_window_size[1] - 1), mode='bicubic',\n                    align_corners=False)\n                new_relative_position_bias_table = new_relative_position_bias_table.view(self.num_heads,\n                                                                                         new_num_relative_distance - 3).permute(\n                    1, 0)\n                new_relative_position_bias_table = torch.cat(\n                    [new_relative_position_bias_table, self.relative_position_bias_table[-3::]], dim=0)\n\n                # get pair-wise relative position index for each token inside the window\n                coords_h = torch.arange(training_window_size[0])\n                coords_w = torch.arange(training_window_size[1])\n                coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n                coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n                relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n                relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n                relative_coords[:, :, 0] += training_window_size[0] - 1  # shift to start from 0\n                relative_coords[:, :, 1] += training_window_size[1] - 1\n                relative_coords[:, :, 0] *= 2 * training_window_size[1] - 1\n                relative_position_index = \\\n                    torch.zeros(size=(training_window_size[0] * training_window_size[1] + 1,) * 2,\n                                dtype=relative_coords.dtype)\n                relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n                relative_position_index[0, 0:] = new_num_relative_distance - 3\n                relative_position_index[0:, 0] = new_num_relative_distance - 2\n                relative_position_index[0, 0] = new_num_relative_distance - 1\n\n                relative_position_bias = \\\n                    new_relative_position_bias_table[relative_position_index.view(-1)].view(\n                        training_window_size[0] * training_window_size[1] + 1,\n                        training_window_size[0] * training_window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n                relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n                attn = attn + relative_position_bias.unsqueeze(0)\n\n        if rel_pos_bias is not None:\n            attn = attn + rel_pos_bias\n\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n                 window_size=None, attn_head_dim=None):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        if init_values is not None:\n            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n        else:\n            self.gamma_1, self.gamma_2 = None, None\n\n    def forward(self, x, rel_pos_bias=None, training_window_size=None):\n        if self.gamma_1 is None:\n            x = x + self.drop_path(\n                self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias, training_window_size=training_window_size))\n            x = x + self.drop_path(self.mlp(self.norm2(x)))\n        else:\n            x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias,\n                                                            training_window_size=training_window_size))\n            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n        return x\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n\n    def __init__(self, img_size=[224, 224], patch_size=16, in_chans=3, embed_dim=768, bias=True):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n        self.num_patches_w = self.patch_shape[0]\n        self.num_patches_h = self.patch_shape[1]\n        # the so-called patch_shape is the patch shape during pre-training\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=bias)\n\n    def forward(self, x, position_embedding=None, **kwargs):\n        # FIXME look at relaxing size constraints\n        # assert H == self.img_size[0] and W == self.img_size[1], \\\n        #     f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x)\n        Hp, Wp = x.shape[2], x.shape[3]\n\n        if position_embedding is not None:\n            # interpolate the position embedding to the corresponding size\n            position_embedding = position_embedding.view(1, self.patch_shape[0], self.patch_shape[1], -1).permute(0, 3,\n                                                                                                                  1, 2)\n            position_embedding = F.interpolate(position_embedding, size=(Hp, Wp), mode='bicubic')\n            x = x + position_embedding\n\n        x = x.flatten(2).transpose(1, 2)\n        return x, (Hp, Wp)\n    \n\nclass HybridEmbed(nn.Module):\n    \"\"\" CNN Feature Map Embedding\n    Extract feature map from CNN, flatten, project to embedding dim.\n    \"\"\"\n\n    def __init__(self, backbone, img_size=[224, 224], feature_size=None, in_chans=3, embed_dim=768):\n        super().__init__()\n        assert isinstance(backbone, nn.Module)\n        img_size = to_2tuple(img_size)\n        self.img_size = img_size\n        self.backbone = backbone\n        if feature_size is None:\n            with torch.no_grad():\n                # FIXME this is hacky, but most reliable way of determining the exact dim of the output feature\n                # map for all networks, the feature metadata has reliable channel and stride info, but using\n                # stride to calc feature dim requires info about padding of each stage that isn't captured.\n                training = backbone.training\n                if training:\n                    backbone.eval()\n                o = self.backbone(torch.zeros(1, in_chans, img_size[0], img_size[1]))[-1]\n                feature_size = o.shape[-2:]\n                feature_dim = o.shape[1]\n                backbone.train(training)\n        else:\n            feature_size = to_2tuple(feature_size)\n            feature_dim = self.backbone.feature_info.channels()[-1]\n        self.num_patches = feature_size[0] * feature_size[1]\n        self.proj = nn.Linear(feature_dim, embed_dim)\n\n    def forward(self, x):\n        x = self.backbone(x)[-1]\n        x = x.flatten(2).transpose(1, 2)\n        x = self.proj(x)\n        return x\n\n\nclass RelativePositionBias(nn.Module):\n\n    def __init__(self, window_size, num_heads):\n        super().__init__()\n        self.window_size = window_size\n        self.num_heads = num_heads\n        self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n        # cls to token & token 2 cls & cls to cls\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(window_size[0])\n        coords_w = torch.arange(window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n        relative_position_index = \\\n            torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n        relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        relative_position_index[0, 0:] = self.num_relative_distance - 3\n        relative_position_index[0:, 0] = self.num_relative_distance - 2\n        relative_position_index[0, 0] = self.num_relative_distance - 1\n\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n        # trunc_normal_(self.relative_position_bias_table, std=.02)\n\n    def forward(self, training_window_size):\n        if training_window_size == self.window_size:\n            relative_position_bias = \\\n                self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n                    self.window_size[0] * self.window_size[1] + 1,\n                    self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n        else:\n            training_window_size = tuple(training_window_size.tolist())\n            new_num_relative_distance = (2 * training_window_size[0] - 1) * (2 * training_window_size[1] - 1) + 3\n            # new_num_relative_dis  cls-clstok-clscls-tok\n            new_relative_position_bias_table = F.interpolate(\n                self.relative_position_bias_table[:-3, :].permute(1, 0).view(1, self.num_heads,\n                                                                             2 * self.window_size[0] - 1,\n                                                                             2 * self.window_size[1] - 1),\n                size=(2 * training_window_size[0] - 1, 2 * training_window_size[1] - 1), mode='bicubic',\n                align_corners=False)\n            new_relative_position_bias_table = new_relative_position_bias_table.view(self.num_heads,\n                                                                                     new_num_relative_distance - 3).permute(\n                1, 0)\n            new_relative_position_bias_table = torch.cat(\n                [new_relative_position_bias_table, self.relative_position_bias_table[-3::]], dim=0)\n\n            # get pair-wise relative position index for each token inside the window\n            coords_h = torch.arange(training_window_size[0])\n            coords_w = torch.arange(training_window_size[1])\n            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n            relative_coords[:, :, 0] += training_window_size[0] - 1  # shift to start from 0\n            relative_coords[:, :, 1] += training_window_size[1] - 1\n            relative_coords[:, :, 0] *= 2 * training_window_size[1] - 1\n            relative_position_index = \\\n                torch.zeros(size=(training_window_size[0] * training_window_size[1] + 1,) * 2,\n                            dtype=relative_coords.dtype)\n            relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n            relative_position_index[0, 0:] = new_num_relative_distance - 3\n            relative_position_index[0:, 0] = new_num_relative_distance - 2\n            relative_position_index[0, 0] = new_num_relative_distance - 1\n\n            relative_position_bias = \\\n                new_relative_position_bias_table[relative_position_index.view(-1)].view(\n                    training_window_size[0] * training_window_size[1] + 1,\n                    training_window_size[0] * training_window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n\n        return relative_position_bias\n\n\nclass BEiT(nn.Module):\n    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n    \"\"\"\n\n    def __init__(self,\n                 img_size=[224, 224],\n                 patch_size=16,\n                 in_chans=3,\n                 grid_chans=64,\n                 num_classes=80,\n                 embed_dim=768,\n                 self_depth=7,\n                 cross_depth=5,\n                 num_heads=12,\n                 mlp_ratio=4.,\n                 qkv_bias=False,\n                 qk_scale=None,\n                 drop_rate=0.,\n                 attn_drop_rate=0.,\n                 drop_path_rate=0.,\n                 hybrid_backbone=None,\n                 norm_layer=None,\n                 init_values=None,\n                 use_abs_pos_emb=False,\n                 use_rel_pos_bias=False,\n                 use_shared_rel_pos_bias=False,\n                 use_checkpoint=True,\n                 pretrained=None,\n                 out_features=None,\n                 ):\n\n        super(BEiT, self).__init__()\n\n        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n        self.use_checkpoint = use_checkpoint\n\n        if hybrid_backbone is not None:\n            self.patch_embed = HybridEmbed(\n                hybrid_backbone, img_size=img_size, in_chans=in_chans, embed_dim=embed_dim)\n        else:\n            self.patch_embed = PatchEmbed(\n                img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n            self.grid_patch_embed = PatchEmbed(\n                img_size=img_size, patch_size=patch_size, in_chans=grid_chans, embed_dim=embed_dim, bias=True)\n        num_patches = self.patch_embed.num_patches\n        self.out_features = out_features\n        self.out_indices = [int(name[5:]) for name in out_features]\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.grid_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        # self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        if use_abs_pos_emb:\n            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n            self.grid_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        else:\n            self.pos_embed = None\n            self.grid_pos_embed = None\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        self.use_shared_rel_pos_bias = use_shared_rel_pos_bias\n        if use_shared_rel_pos_bias:\n            self.rel_pos_bias = RelativePositionBias(window_size=self.patch_embed.patch_shape, num_heads=num_heads)\n        else:\n            self.rel_pos_bias = None\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, self_depth + cross_depth)]  # stochastic depth decay rule\n        self.use_rel_pos_bias = use_rel_pos_bias\n        self.blocks = nn.ModuleList([\n            Block(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n                init_values=init_values, window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None)\n            for i in range(self_depth)])\n        \n        self.grid_blocks = nn.ModuleList([\n            Block(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n                init_values=init_values, window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None)\n            for i in range(self_depth)])\n        \n        self.cross_blocks = nn.ModuleList([\n            CrossBlock(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i + self_depth], norm_layer=norm_layer,\n                init_values=init_values, window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None)\n            for i in range(cross_depth)])\n\n        # trunc_normal_(self.mask_token, std=.02)\n\n        if patch_size == 16:\n            self.fpn1 = nn.Sequential(\n                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n                # nn.SyncBatchNorm(embed_dim),\n                nn.BatchNorm2d(embed_dim),\n                nn.GELU(),\n                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n            )\n            self.fpn2 = nn.Sequential(\n                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n            )\n            self.fpn3 = nn.Identity()\n            self.fpn4 = nn.MaxPool2d(kernel_size=2, stride=2)\n            \n            \n            self.grid_fpn1 = nn.Sequential(\n                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n                # nn.SyncBatchNorm(embed_dim),\n                nn.BatchNorm2d(embed_dim),\n                nn.GELU(),\n                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n            )\n            self.grid_fpn2 = nn.Sequential(\n                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n            )\n            self.grid_fpn3 = nn.Identity()\n            self.grid_fpn4 = nn.MaxPool2d(kernel_size=2, stride=2)\n            \n            \n        elif patch_size == 8:\n            self.fpn1 = nn.Sequential(\n                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n            )\n            self.fpn2 = nn.Identity()\n            self.fpn3 = nn.Sequential(\n                nn.MaxPool2d(kernel_size=2, stride=2),\n            )\n            self.fpn4 = nn.Sequential(\n                nn.MaxPool2d(kernel_size=4, stride=4),\n            )\n            \n            self.grid_fpn1 = nn.Sequential(\n                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n            )\n            self.grid_fpn2 = nn.Identity()\n            self.grid_fpn3 = nn.Sequential(\n                nn.MaxPool2d(kernel_size=2, stride=2),\n            )\n            self.grid_fpn4 = nn.Sequential(\n                nn.MaxPool2d(kernel_size=4, stride=4),\n            )\n\n        if self.pos_embed is not None:\n            trunc_normal_(self.pos_embed, std=.02)\n            trunc_normal_(self.grid_pos_embed, std=.02)\n        trunc_normal_(self.cls_token, std=.02)\n        trunc_normal_(self.grid_token, std=.02)\n        self.apply(self._init_weights)\n        self.fix_init_weight()\n        \n    def fix_init_weight(self):\n        def rescale(param, layer_id):\n            param.div_(math.sqrt(2.0 * layer_id))\n\n        for layer_id, layer in enumerate(self.blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    '''\n    def init_weights(self):\n        \"\"\"Initialize the weights in backbone.\n\n        Args:\n            pretrained (str, optional): Path to pre-trained weights.\n                Defaults to None.\n        \"\"\"\n        logger = get_root_logger()\n\n        if self.pos_embed is not None:\n            trunc_normal_(self.pos_embed, std=.02)\n        trunc_normal_(self.cls_token, std=.02)\n        self.apply(self._init_weights)\n        self.fix_init_weight()\n\n        if self.init_cfg is None:\n            logger.warn(f'No pre-trained weights for '\n                        f'{self.__class__.__name__}, '\n                        f'training start from scratch')\n        else:\n            assert 'checkpoint' in self.init_cfg, f'Only support ' \\\n                                                  f'specify `Pretrained` in ' \\\n                                                  f'`init_cfg` in ' \\\n                                                  f'{self.__class__.__name__} '\n            logger.info(f\"Will load ckpt from {self.init_cfg['checkpoint']}\")\n            load_checkpoint(self,\n                            filename=self.init_cfg['checkpoint'],\n                            strict=False,\n                            logger=logger,\n                            beit_spec_expand_rel_pos = self.use_rel_pos_bias,\n                            )\n    '''\n\n    def get_num_layers(self):\n        return len(self.blocks)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'pos_embed', 'cls_token'}\n\n    def forward_features(self, x, grid):\n        B, C, H, W = x.shape\n        vis_x, (Hp, Wp) = self.patch_embed(x, self.pos_embed[:, 1:, :] if self.pos_embed is not None else None)\n        grid_x, (grid_Hp, grid_Wp) = self.grid_patch_embed(grid, self.grid_pos_embed[:, 1:, :] if self.grid_pos_embed is not None else None)\n        \n        # Hp, Wp are HW for patches\n        batch_size, seq_len, _ = grid_x.size()\n\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        grid_tokens = self.grid_token.expand(batch_size, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        if self.pos_embed is not None:\n            cls_tokens = cls_tokens + self.pos_embed[:, :1, :]\n            grid_tokens = grid_tokens + self.grid_pos_embed[:, :1, :]\n        vis_x = torch.cat((cls_tokens, vis_x), dim=1)\n        vis_x = self.pos_drop(vis_x)\n        \n        grid_x = torch.cat((grid_tokens, grid_x), dim=1)\n        grid_x = self.pos_drop(grid_x)\n\n        features = []\n        grid_features = []\n        training_window_size = torch.tensor([Hp, Wp])\n        grid_training_window_size = torch.tensor([grid_Hp, grid_Wp])\n\n        rel_pos_bias = self.rel_pos_bias(training_window_size) if self.rel_pos_bias is not None else None\n        \n        for i, blk in enumerate(self.blocks):\n            if self.use_checkpoint:\n                vis_x = checkpoint.checkpoint(blk, vis_x, rel_pos_bias, training_window_size)\n            else:\n                vis_x = blk(vis_x, rel_pos_bias=rel_pos_bias, training_window_size=training_window_size)\n            if i in self.out_indices:\n                xp = vis_x[:, 1:, :].permute(0, 2, 1).reshape(B, -1, Hp, Wp)\n                features.append(xp.contiguous())\n        \n        for i, grid_blk in enumerate(self.grid_blocks):\n            if self.use_checkpoint:\n                grid_x = checkpoint.checkpoint(grid_blk, grid_x, rel_pos_bias, grid_training_window_size)\n            else:\n                grid_x = grid_blk(grid_x, rel_pos_bias=rel_pos_bias, training_window_size=grid_training_window_size)\n            if i in self.out_indices:\n                gp = grid_x[:, 1:, :].permute(0, 2, 1).reshape(B, -1, grid_Hp, grid_Wp)\n                grid_features.append(gp.contiguous())\n        \n        # import ipdb;ipdb.set_trace()\n        for i, cross_blk in enumerate(self.cross_blocks):\n            if self.use_checkpoint:\n                vis_x, grid_x = checkpoint.checkpoint(cross_blk, vis_x, grid_x)\n            else:\n                vis_x, grid_x = cross_blk(vis_input = vis_x, grid_input = grid_x)\n                \n            if 1:\n                xp = vis_x[:, 1:, :].permute(0, 2, 1).reshape(B, -1, Hp, Wp)\n                features.append(xp.contiguous())\n                \n                gp = grid_x[:, 1:, :].permute(0, 2, 1).reshape(B, -1, grid_Hp, grid_Wp)\n                grid_features.append(gp.contiguous())\n                \n        ops = [self.fpn1, self.fpn2, self.fpn3, self.fpn4]\n        grid_ops = [self.grid_fpn1, self.grid_fpn2, self.grid_fpn3, self.grid_fpn4]\n        \n        for i in range(len(features)):\n            features[i] = ops[i](features[i])\n        \n        for i in range(len(grid_features)):\n            grid_features[i] = grid_ops[i](grid_features[i])   \n\n        feat_out = {}\n        grid_feat_out = {}\n\n        for name, vis_value, grid_value in zip(self.out_features, features, grid_features):\n            feat_out[name] = vis_value\n            grid_feat_out[name] = grid_value\n\n        return feat_out, grid_feat_out\n\n    def forward(self, x, grid):\n        x,y = self.forward_features(x, grid)\n        return x,y\n\n\ndef beit_base_patch16(pretrained=False, **kwargs):\n    model = BEiT(\n        patch_size=16,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_values=None,\n        **kwargs)\n    model.default_cfg = _cfg()\n    return model\n\ndef beit_large_patch16(pretrained=False, **kwargs):\n    model = BEiT(\n        patch_size=16,\n        embed_dim=1024,\n        depth=24,\n        num_heads=16,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_values=None,\n        **kwargs)\n    model.default_cfg = _cfg()\n    return model\n\ndef VGT_dit_base_patch16(pretrained=False, **kwargs):\n    model = BEiT(\n        patch_size=16,\n        embed_dim=768,\n        self_depth=12,\n        cross_depth=0,\n        num_heads=12,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_values=0.1,\n        in_chans=3,\n        grid_chans=64,\n        **kwargs)\n    model.default_cfg = _cfg()\n    return model\n\ndef dit_base_patch16(pretrained=False, **kwargs):\n    model = BEiT(\n        patch_size=16,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_values=0.1,\n        in_chans=3,\n        **kwargs)\n    model.default_cfg = _cfg()\n    return model\n\ndef dit_large_patch16(pretrained=False, **kwargs):\n    model = BEiT(\n        patch_size=16,\n        embed_dim=1024,\n        depth=24,\n        num_heads=16,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_values=1e-5,\n        **kwargs)\n    model.default_cfg = _cfg()\n    return model\n\nif __name__ == '__main__':\n    model = BEiT(use_checkpoint=True, use_shared_rel_pos_bias=True)\n    model = model.to(\"cuda:0\")\n    input1 = torch.rand(2, 3, 512, 762).to(\"cuda:0\")\n    input2 = torch.rand(2, 3, 800, 1200).to(\"cuda:0\")\n    input3 = torch.rand(2, 3, 720, 1000).to(\"cuda:0\")\n    output1 = model(input1)\n    output2 = model(input2)\n    output3 = model(input3)\n    print(\"all done\")\n"}
{"type": "source_file", "path": "utils/ditod/__init__.py", "content": "# --------------------------------------------------------------------------------\n# MPViT: Multi-Path Vision Transformer for Dense Prediction\n# Copyright (c) 2022 Electronics and Telecommunications Research Institute (ETRI).\n# All Rights Reserved.\n# Written by Youngwan Lee\n# This source code is licensed(Dual License(GPL3.0 & Commercial)) under the license found in the\n# LICENSE file in the root directory of this source tree.\n# --------------------------------------------------------------------------------\n\nfrom .config import add_vit_config\nfrom .backbone import build_vit_fpn_backbone\nfrom .dataset_mapper import DetrDatasetMapper\nfrom .mycheckpointer import MyDetectionCheckpointer\nfrom .icdar_evaluation import ICDAREvaluator\nfrom .mytrainer import MyTrainer\nfrom .table_evaluation import calc_table_score"}
{"type": "source_file", "path": "utils/ditod/deit.py", "content": "\"\"\"\nMostly copy-paste from DINO and timm library:\nhttps://github.com/facebookresearch/dino\nhttps://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n\"\"\"\nimport warnings\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.utils.checkpoint as checkpoint\nfrom timm.models.layers import trunc_normal_, drop_path, to_2tuple\nfrom functools import partial\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n        'crop_pct': .9, 'interpolation': 'bicubic',\n        'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5),\n        **kwargs\n    }\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    \"\"\"\n\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n\n    def extra_repr(self) -> str:\n        return 'p={}'.format(self.drop_prob)\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        q, k, v = self.qkv(x).reshape(B, N, 3, self.num_heads,\n                                      C // self.num_heads).permute(2, 0, 3, 1, 4)\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(\n            drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,\n                       act_layer=act_layer, drop=drop)\n\n    def forward(self, x):\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n\n        self.window_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n\n        self.num_patches_w, self.num_patches_h = self.window_size\n\n        self.num_patches = self.window_size[0] * self.window_size[1]\n        self.img_size = img_size\n        self.patch_size = patch_size\n\n        self.proj = nn.Conv2d(in_chans, embed_dim,\n                              kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):\n        x = self.proj(x)\n        return x\n\n\nclass HybridEmbed(nn.Module):\n    \"\"\" CNN Feature Map Embedding\n    Extract feature map from CNN, flatten, project to embedding dim.\n    \"\"\"\n\n    def __init__(self, backbone, img_size=224, feature_size=None, in_chans=3, embed_dim=768):\n        super().__init__()\n        assert isinstance(backbone, nn.Module)\n        img_size = to_2tuple(img_size)\n        self.img_size = img_size\n        self.backbone = backbone\n        if feature_size is None:\n            with torch.no_grad():\n                # FIXME this is hacky, but most reliable way of determining the exact dim of the output feature\n                # map for all networks, the feature metadata has reliable channel and stride info, but using\n                # stride to calc feature dim requires info about padding of each stage that isn't captured.\n                training = backbone.training\n                if training:\n                    backbone.eval()\n                o = self.backbone(torch.zeros(\n                    1, in_chans, img_size[0], img_size[1]))[-1]\n                feature_size = o.shape[-2:]\n                feature_dim = o.shape[1]\n                backbone.train(training)\n        else:\n            feature_size = to_2tuple(feature_size)\n            feature_dim = self.backbone.feature_info.channels()[-1]\n        self.num_patches = feature_size[0] * feature_size[1]\n        self.proj = nn.Linear(feature_dim, embed_dim)\n\n    def forward(self, x):\n        x = self.backbone(x)[-1]\n        x = x.flatten(2).transpose(1, 2)\n        x = self.proj(x)\n        return x\n\n\nclass ViT(nn.Module):\n    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n    \"\"\"\n\n    def __init__(self,\n                 model_name='vit_base_patch16_224',\n                 img_size=384,\n                 patch_size=16,\n                 in_chans=3,\n                 embed_dim=1024,\n                 depth=24,\n                 num_heads=16,\n                 num_classes=19,\n                 mlp_ratio=4.,\n                 qkv_bias=True,\n                 qk_scale=None,\n                 drop_rate=0.1,\n                 attn_drop_rate=0.,\n                 drop_path_rate=0.,\n                 hybrid_backbone=None,\n                 norm_layer=partial(nn.LayerNorm, eps=1e-6),\n                 norm_cfg=None,\n                 pos_embed_interp=False,\n                 random_init=False,\n                 align_corners=False,\n                 use_checkpoint=False,\n                 num_extra_tokens=1,\n                 out_features=None,\n                 **kwargs,\n                 ):\n\n        super(ViT, self).__init__()\n        self.model_name = model_name\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n        self.depth = depth\n        self.num_heads = num_heads\n        self.num_classes = num_classes\n        self.mlp_ratio = mlp_ratio\n        self.qkv_bias = qkv_bias\n        self.qk_scale = qk_scale\n        self.drop_rate = drop_rate\n        self.attn_drop_rate = attn_drop_rate\n        self.drop_path_rate = drop_path_rate\n        self.hybrid_backbone = hybrid_backbone\n        self.norm_layer = norm_layer\n        self.norm_cfg = norm_cfg\n        self.pos_embed_interp = pos_embed_interp\n        self.random_init = random_init\n        self.align_corners = align_corners\n        self.use_checkpoint = use_checkpoint\n        self.num_extra_tokens = num_extra_tokens\n        self.out_features = out_features\n        self.out_indices = [int(name[5:]) for name in out_features]\n\n        # self.num_stages = self.depth\n        # self.out_indices = tuple(range(self.num_stages))\n\n        if self.hybrid_backbone is not None:\n            self.patch_embed = HybridEmbed(\n                self.hybrid_backbone, img_size=self.img_size, in_chans=self.in_chans, embed_dim=self.embed_dim)\n        else:\n            self.patch_embed = PatchEmbed(\n                img_size=self.img_size, patch_size=self.patch_size, in_chans=self.in_chans, embed_dim=self.embed_dim)\n        self.num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n\n        if self.num_extra_tokens == 2:\n            self.dist_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n\n        self.pos_embed = nn.Parameter(torch.zeros(\n            1, self.num_patches + self.num_extra_tokens, self.embed_dim))\n        self.pos_drop = nn.Dropout(p=self.drop_rate)\n\n        # self.num_extra_tokens = self.pos_embed.shape[-2] - self.num_patches\n        dpr = [x.item() for x in torch.linspace(0, self.drop_path_rate,\n                                                self.depth)]  # stochastic depth decay rule\n        self.blocks = nn.ModuleList([\n            Block(\n                dim=self.embed_dim, num_heads=self.num_heads, mlp_ratio=self.mlp_ratio, qkv_bias=self.qkv_bias,\n                qk_scale=self.qk_scale,\n                drop=self.drop_rate, attn_drop=self.attn_drop_rate, drop_path=dpr[i], norm_layer=self.norm_layer)\n            for i in range(self.depth)])\n\n        # NOTE as per official impl, we could have a pre-logits representation dense layer + tanh here\n        # self.repr = nn.Linear(embed_dim, representation_size)\n        # self.repr_act = nn.Tanh()\n\n        if patch_size == 16:\n            self.fpn1 = nn.Sequential(\n                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n                nn.SyncBatchNorm(embed_dim),\n                nn.GELU(),\n                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n            )\n\n            self.fpn2 = nn.Sequential(\n                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n            )\n\n            self.fpn3 = nn.Identity()\n\n            self.fpn4 = nn.MaxPool2d(kernel_size=2, stride=2)\n        elif patch_size == 8:\n            self.fpn1 = nn.Sequential(\n                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n            )\n\n            self.fpn2 = nn.Identity()\n\n            self.fpn3 = nn.Sequential(\n                nn.MaxPool2d(kernel_size=2, stride=2),\n            )\n\n            self.fpn4 = nn.Sequential(\n                nn.MaxPool2d(kernel_size=4, stride=4),\n            )\n\n        trunc_normal_(self.pos_embed, std=.02)\n        trunc_normal_(self.cls_token, std=.02)\n        if self.num_extra_tokens==2:\n            trunc_normal_(self.dist_token, std=0.2)\n        self.apply(self._init_weights)\n        # self.fix_init_weight()\n\n    def fix_init_weight(self):\n        def rescale(param, layer_id):\n            param.div_(math.sqrt(2.0 * layer_id))\n\n        for layer_id, layer in enumerate(self.blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    '''\n    def init_weights(self):\n        logger = get_root_logger()\n\n        trunc_normal_(self.pos_embed, std=.02)\n        trunc_normal_(self.cls_token, std=.02)\n        self.apply(self._init_weights)\n\n        if self.init_cfg is None:\n            logger.warn(f'No pre-trained weights for '\n                        f'{self.__class__.__name__}, '\n                        f'training start from scratch')\n        else:\n            assert 'checkpoint' in self.init_cfg, f'Only support ' \\\n                                                  f'specify `Pretrained` in ' \\\n                                                  f'`init_cfg` in ' \\\n                                                  f'{self.__class__.__name__} '\n            logger.info(f\"Will load ckpt from {self.init_cfg['checkpoint']}\")\n            load_checkpoint(self, filename=self.init_cfg['checkpoint'], strict=False, logger=logger)\n    '''\n\n    def get_num_layers(self):\n        return len(self.blocks)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'pos_embed', 'cls_token'}\n\n    def _conv_filter(self, state_dict, patch_size=16):\n        \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n        out_dict = {}\n        for k, v in state_dict.items():\n            if 'patch_embed.proj.weight' in k:\n                v = v.reshape((v.shape[0], 3, patch_size, patch_size))\n            out_dict[k] = v\n        return out_dict\n\n    def to_2D(self, x):\n        n, hw, c = x.shape\n        h = w = int(math.sqrt(hw))\n        x = x.transpose(1, 2).reshape(n, c, h, w)\n        return x\n\n    def to_1D(self, x):\n        n, c, h, w = x.shape\n        x = x.reshape(n, c, -1).transpose(1, 2)\n        return x\n\n    def interpolate_pos_encoding(self, x, w, h):\n        npatch = x.shape[1] - self.num_extra_tokens\n        N = self.pos_embed.shape[1] - self.num_extra_tokens\n        if npatch == N and w == h:\n            return self.pos_embed\n\n        class_ORdist_pos_embed = self.pos_embed[:, 0:self.num_extra_tokens]\n\n        patch_pos_embed = self.pos_embed[:, self.num_extra_tokens:]\n\n        dim = x.shape[-1]\n        w0 = w // self.patch_embed.patch_size[0]\n        h0 = h // self.patch_embed.patch_size[1]\n        # we add a small number to avoid floating point error in the interpolation\n        # see discussion at https://github.com/facebookresearch/dino/issues/8\n        w0, h0 = w0 + 0.1, h0 + 0.1\n        patch_pos_embed = nn.functional.interpolate(\n            patch_pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n            mode='bicubic',\n        )\n        assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]\n        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n\n        return torch.cat((class_ORdist_pos_embed, patch_pos_embed), dim=1)\n\n    def prepare_tokens(self, x, mask=None):\n        B, nc, w, h = x.shape\n        # patch linear embedding\n        x = self.patch_embed(x)\n\n        # mask image modeling\n        if mask is not None:\n            x = self.mask_model(x, mask)\n        x = x.flatten(2).transpose(1, 2)\n\n        # add the [CLS] token to the embed patch tokens\n        all_tokens = [self.cls_token.expand(B, -1, -1)]\n\n        if self.num_extra_tokens == 2:\n            dist_tokens = self.dist_token.expand(B, -1, -1)\n            all_tokens.append(dist_tokens)\n        all_tokens.append(x)\n\n        x = torch.cat(all_tokens, dim=1)\n\n        # add positional encoding to each token\n        x = x + self.interpolate_pos_encoding(x, w, h)\n\n        return self.pos_drop(x)\n\n    def forward_features(self, x):\n        # print(f\"==========shape of x is {x.shape}==========\")\n        B, _, H, W = x.shape\n        Hp, Wp = H // self.patch_size, W // self.patch_size\n        x = self.prepare_tokens(x)\n\n        features = []\n        for i, blk in enumerate(self.blocks):\n            if self.use_checkpoint:\n                x = checkpoint.checkpoint(blk, x)\n            else:\n                x = blk(x)\n            if i in self.out_indices:\n                xp = x[:, self.num_extra_tokens:, :].permute(0, 2, 1).reshape(B, -1, Hp, Wp)\n                features.append(xp.contiguous())\n\n        ops = [self.fpn1, self.fpn2, self.fpn3, self.fpn4]\n        for i in range(len(features)):\n            features[i] = ops[i](features[i])\n\n        feat_out = {}\n\n        for name, value in zip(self.out_features, features):\n            feat_out[name] = value\n\n        return feat_out\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        return x\n\n\ndef deit_base_patch16(pretrained=False, **kwargs):\n    model = ViT(\n        patch_size=16,\n        drop_rate=0.,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        num_classes=1000,\n        mlp_ratio=4.,\n        qkv_bias=True,\n        use_checkpoint=True,\n        num_extra_tokens=2,\n        **kwargs)\n    model.default_cfg = _cfg()\n    return model\n\ndef mae_base_patch16(pretrained=False, **kwargs):\n    model = ViT(\n        patch_size=16,\n        drop_rate=0.,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        num_classes=1000,\n        mlp_ratio=4.,\n        qkv_bias=True,\n        use_checkpoint=True,\n        num_extra_tokens=1,\n        **kwargs)\n    model.default_cfg = _cfg()\n    return model"}
{"type": "source_file", "path": "utils/ditod_vgt/VGTTrainer.py", "content": "# -*- coding: utf-8 -*-\n# Copyright (c) Facebook, Inc. and its affiliates.\n\n\"\"\"\nThis file contains components with some default boilerplate logic user may need\nin training / testing. They will not work for everyone, but many users may find them useful.\n\nThe behavior of functions/classes in this file is subject to change,\nsince they are meant to represent the \"common default behavior\" people need in their projects.\n\"\"\"\n\nimport argparse\nimport logging\nimport os\nimport sys\nimport weakref\nfrom collections import OrderedDict\nfrom typing import Optional\nimport torch\nfrom fvcore.nn.precise_bn import get_bn_modules\nfrom omegaconf import OmegaConf\nfrom torch.nn.parallel import DistributedDataParallel\nimport numpy as np\n\nimport detectron2.data.transforms as T\nfrom detectron2.checkpoint import DetectionCheckpointer\nfrom detectron2.config import CfgNode, LazyConfig\nfrom detectron2.data import (\n    MetadataCatalog,\n    build_detection_test_loader,\n    build_detection_train_loader,\n)\nfrom detectron2.evaluation import (\n    DatasetEvaluator,\n    inference_on_dataset,\n    print_csv_format,\n    verify_results,\n)\nfrom detectron2.modeling import build_model\nfrom detectron2.solver import build_lr_scheduler, build_optimizer\nfrom detectron2.utils import comm\nfrom detectron2.utils.collect_env import collect_env_info\nfrom detectron2.utils.env import seed_all_rng\nfrom detectron2.utils.events import CommonMetricPrinter, JSONWriter, TensorboardXWriter\nfrom detectron2.utils.file_io import PathManager\nfrom detectron2.utils.logger import setup_logger\n\nfrom detectron2.engine import hooks\nfrom detectron2.engine.train_loop import AMPTrainer, SimpleTrainer, TrainerBase\n\nfrom .VGTcheckpointer import MyDetectionCheckpointer\nfrom typing import Any, Dict, List, Set\nimport itertools\nfrom detectron2.solver.build import maybe_add_gradient_clipping\nfrom .dataset_mapper import DetrDatasetMapper\nfrom detectron2.evaluation import COCOEvaluator\n\nimport pickle\nfrom detectron2.data import detection_utils as utils\nfrom detectron2.structures import (\n    BitMasks,\n    Boxes,\n    BoxMode,\n    Instances,\n    Keypoints,\n    PolygonMasks,\n    RotatedBoxes,\n    polygons_to_bitmask,\n)\n\n__all__ = [\n    \"create_ddp_model\",\n    \"default_argument_parser\",\n    \"default_setup\",\n    \"default_writers\",\n    \"DefaultPredictor\",\n    \"GridTextTrainer\",\n]\n\ndef torch_memory(device, tag=\"\"):\n    # Checks and prints GPU memory\n    print(tag, f'{torch.cuda.memory_allocated(device)/1024/1024:.2f} MB USED')\n    print(tag, f'{torch.cuda.memory_reserved(device)/1024/1024:.2f} MB RESERVED')\n    print(tag, f'{torch.cuda.max_memory_allocated(device)/1024/1024:.2f} MB USED MAX')\n    print(tag, f'{torch.cuda.max_memory_reserved(device)/1024/1024:.2f} MB RESERVED MAX')\n    print('')\n\ndef create_ddp_model(model, *, fp16_compression=False, **kwargs):\n    \"\"\"\n    Create a DistributedDataParallel model if there are >1 processes.\n\n    Args:\n        model: a torch.nn.Module\n        fp16_compression: add fp16 compression hooks to the ddp object.\n            See more at https://pytorch.org/docs/stable/ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook\n        kwargs: other arguments of :module:`torch.nn.parallel.DistributedDataParallel`.\n    \"\"\"  # noqa\n    if comm.get_world_size() == 1:\n        return model\n    if \"device_ids\" not in kwargs:\n        kwargs[\"device_ids\"] = [comm.get_local_rank()]\n    ddp = DistributedDataParallel(model, **kwargs)\n    if fp16_compression:\n        from torch.distributed.algorithms.ddp_comm_hooks import default as comm_hooks\n\n        ddp.register_comm_hook(state=None, hook=comm_hooks.fp16_compress_hook)\n    return ddp\n\n\ndef default_argument_parser(epilog=None):\n    \"\"\"\n    Create a parser with some common arguments used by detectron2 users.\n\n    Args:\n        epilog (str): epilog passed to ArgumentParser describing the usage.\n\n    Returns:\n        argparse.ArgumentParser:\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        epilog=epilog\n        or f\"\"\"\nExamples:\n\nRun on single machine:\n    $ {sys.argv[0]} --num-gpus 8 --config-file cfg.yaml\n\nChange some config options:\n    $ {sys.argv[0]} --config-file cfg.yaml MODEL.WEIGHTS /path/to/weight.pth SOLVER.BASE_LR 0.001\n\nRun on multiple machines:\n    (machine0)$ {sys.argv[0]} --machine-rank 0 --num-machines 2 --dist-url <URL> [--other-flags]\n    (machine1)$ {sys.argv[0]} --machine-rank 1 --num-machines 2 --dist-url <URL> [--other-flags]\n\"\"\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n    )\n    parser.add_argument(\"--config-file\", default=\"\", metavar=\"FILE\", help=\"path to config file\")\n    parser.add_argument(\n        \"--resume\",\n        action=\"store_true\",\n        help=\"Whether to attempt to resume from the checkpoint directory. \"\n        \"See documentation of `MyTrainer.resume_or_load()` for what it means.\",\n    )\n    parser.add_argument(\"--eval-only\", action=\"store_true\", help=\"perform evaluation only\")\n    parser.add_argument(\"--num-gpus\", type=int, default=1, help=\"number of gpus *per machine*\")\n    parser.add_argument(\"--num-machines\", type=int, default=1, help=\"total number of machines\")\n    parser.add_argument(\n        \"--machine-rank\", type=int, default=0, help=\"the rank of this machine (unique per machine)\"\n    )\n\n    # PyTorch still may leave orphan processes in multi-gpu training.\n    # Therefore we use a deterministic way to obtain port,\n    # so that users are aware of orphan processes by seeing the port occupied.\n    port = 2 ** 15 + 2 ** 14 + hash(os.getuid() if sys.platform != \"win32\" else 1) % 2 ** 14\n    parser.add_argument(\n        \"--dist-url\",\n        default=\"tcp://127.0.0.1:{}\".format(port),\n        help=\"initialization URL for pytorch distributed backend. See \"\n        \"https://pytorch.org/docs/stable/distributed.html for details.\",\n    )\n    parser.add_argument(\n        \"opts\",\n        help=\"\"\"\nModify config options at the end of the command. For Yacs configs, use\nspace-separated \"PATH.KEY VALUE\" pairs.\nFor python-based LazyConfig, use \"path.key=value\".\n        \"\"\".strip(),\n        default=None,\n        nargs=argparse.REMAINDER,\n    )\n    return parser\n\n\ndef _try_get_key(cfg, *keys, default=None):\n    \"\"\"\n    Try select keys from cfg until the first key that exists. Otherwise return default.\n    \"\"\"\n    if isinstance(cfg, CfgNode):\n        cfg = OmegaConf.create(cfg.dump())\n    for k in keys:\n        none = object()\n        p = OmegaConf.select(cfg, k, default=none)\n        if p is not none:\n            return p\n    return default\n\n\ndef _highlight(code, filename):\n    try:\n        import pygments\n    except ImportError:\n        return code\n\n    from pygments.lexers import Python3Lexer, YamlLexer\n    from pygments.formatters import Terminal256Formatter\n\n    lexer = Python3Lexer() if filename.endswith(\".py\") else YamlLexer()\n    code = pygments.highlight(code, lexer, Terminal256Formatter(style=\"monokai\"))\n    return code\n\n\ndef default_setup(cfg, args):\n    \"\"\"\n    Perform some basic common setups at the beginning of a job, including:\n\n    1. Set up the detectron2 logger\n    2. Log basic information about environment, cmdline arguments, and config\n    3. Backup the config to the output directory\n\n    Args:\n        cfg (CfgNode or omegaconf.DictConfig): the full config to be used\n        args (argparse.NameSpace): the command line arguments to be logged\n    \"\"\"\n    output_dir = _try_get_key(cfg, \"OUTPUT_DIR\", \"output_dir\", \"train.output_dir\")\n    if comm.is_main_process() and output_dir:\n        PathManager.mkdirs(output_dir)\n\n    rank = comm.get_rank()\n    setup_logger(output_dir, distributed_rank=rank, name=\"fvcore\")\n    logger = setup_logger(output_dir, distributed_rank=rank)\n\n    logger.info(\"Rank of current process: {}. World size: {}\".format(rank, comm.get_world_size()))\n    logger.info(\"Environment info:\\n\" + collect_env_info())\n\n    logger.info(\"Command line arguments: \" + str(args))\n    if hasattr(args, \"config_file\") and args.config_file != \"\":\n        logger.info(\n            \"Contents of args.config_file={}:\\n{}\".format(\n                args.config_file,\n                _highlight(PathManager.open(args.config_file, \"r\").read(), args.config_file),\n            )\n        )\n\n    if comm.is_main_process() and output_dir:\n        # Note: some of our scripts may expect the existence of\n        # config.yaml in output directory\n        path = os.path.join(output_dir, \"config.yaml\")\n        if isinstance(cfg, CfgNode):\n            logger.info(\"Running with full config:\\n{}\".format(_highlight(cfg.dump(), \".yaml\")))\n            with PathManager.open(path, \"w\") as f:\n                f.write(cfg.dump())\n        else:\n            LazyConfig.save(cfg, path)\n        logger.info(\"Full config saved to {}\".format(path))\n\n    # make sure each worker has a different, yet deterministic seed if specified\n    seed = _try_get_key(cfg, \"SEED\", \"train.seed\", default=-1)\n    seed_all_rng(None if seed < 0 else seed + rank)\n\n    # cudnn benchmark has large overhead. It shouldn't be used considering the small size of\n    # typical validation set.\n    if not (hasattr(args, \"eval_only\") and args.eval_only):\n        torch.backends.cudnn.benchmark = _try_get_key(\n            cfg, \"CUDNN_BENCHMARK\", \"train.cudnn_benchmark\", default=False\n        )\n\n\ndef default_writers(output_dir: str, max_iter: Optional[int] = None):\n    \"\"\"\n    Build a list of :class:`EventWriter` to be used.\n    It now consists of a :class:`CommonMetricPrinter`,\n    :class:`TensorboardXWriter` and :class:`JSONWriter`.\n\n    Args:\n        output_dir: directory to store JSON metrics and tensorboard events\n        max_iter: the total number of iterations\n\n    Returns:\n        list[EventWriter]: a list of :class:`EventWriter` objects.\n    \"\"\"\n    PathManager.mkdirs(output_dir)\n    return [\n        # It may not always print what you want to see, since it prints \"common\" metrics only.\n        CommonMetricPrinter(max_iter),\n        JSONWriter(os.path.join(output_dir, \"metrics.json\")),\n        TensorboardXWriter(output_dir),\n    ]\n\n\nclass DefaultPredictor:\n    \"\"\"\n    Create a simple end-to-end predictor with the given config that runs on\n    single device for a single input image.\n\n    Compared to using the model directly, this class does the following additions:\n\n    1. Load checkpoint from `cfg.MODEL.WEIGHTS`.\n    2. Always take BGR image as the input and apply conversion defined by `cfg.INPUT.FORMAT`.\n    3. Apply resizing defined by `cfg.INPUT.{MIN,MAX}_SIZE_TEST`.\n    4. Take one input image and produce a single output, instead of a batch.\n\n    This is meant for simple demo purposes, so it does the above steps automatically.\n    This is not meant for benchmarks or running complicated inference logic.\n    If you'd like to do anything more complicated, please refer to its source code as\n    examples to build and use the model manually.\n\n    Attributes:\n        metadata (Metadata): the metadata of the underlying dataset, obtained from\n            cfg.DATASETS.TEST.\n\n    Examples:\n    ::\n        pred = DefaultPredictor(cfg)\n        inputs = cv2.imread(\"input.jpg\")\n        outputs = pred(inputs)\n    \"\"\"\n    \n    def __init__(self, cfg):\n        self.cfg = cfg.clone()  # cfg can be modified by model\n        self.model = build_model(self.cfg)\n        self.model.eval()\n        if len(cfg.DATASETS.TEST):\n            self.metadata = MetadataCatalog.get(cfg.DATASETS.TEST[0])\n\n        checkpointer = DetectionCheckpointer(self.model)\n        checkpointer.load(cfg.MODEL.WEIGHTS)\n\n        self.aug = T.ResizeShortestEdge(\n            [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST\n        )\n\n        self.input_format = cfg.INPUT.FORMAT\n        assert self.input_format in [\"RGB\", \"BGR\"], self.input_format\n\n    def __call__(self, original_image, grid_path):\n        \"\"\"\n        Args:\n            original_image (np.ndarray): an image of shape (H, W, C) (in BGR order).\n\n        Returns:\n            predictions (dict):\n                the output of the model for one image only.\n                See :doc:`/tutorials/models` for details about the format.\n        \"\"\"\n        with torch.no_grad():  # https://github.com/sphinx-doc/sphinx/issues/4258\n            # Apply pre-processing to image.\n            \n            # if self.input_format == \"RGB\":\n            #     # whether the model expects BGR inputs or RGB\n            #     import ipdb;ipdb.set_trace() \n            #     original_image = original_image[:, :, ::-1]\n            \n            height, width = original_image.shape[:2]\n            image, transforms = T.apply_transform_gens([self.aug], original_image)\n            \n            # add grid    \n            image_shape = image.shape[:2]  # h, w\n            image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n            \n            with open(grid_path, \"rb\") as f:\n                sample_inputs = pickle.load(f)\n            input_ids = sample_inputs[\"input_ids\"]\n            bbox_subword_list = sample_inputs[\"bbox_subword_list\"]\n                \n            # word bbox\n            bbox = []\n            for bbox_per_subword in bbox_subword_list:\n                text_word = {}\n                text_word['bbox'] = bbox_per_subword.tolist()\n                text_word['bbox_mode'] = BoxMode.XYWH_ABS\n                utils.transform_instance_annotations(text_word, transforms, image_shape)\n                bbox.append(text_word['bbox'])\n                \n            dataset_dict = {}\n            dataset_dict[\"input_ids\"] = input_ids \n            dataset_dict[\"bbox\"] = bbox\n            dataset_dict[\"image\"] = image \n            dataset_dict[\"height\"] = height \n            dataset_dict[\"width\"] = width \n\n            predictions = self.model([dataset_dict])[0]\n            return predictions\n        \n\nclass VGTTrainer(TrainerBase):\n    \"\"\"\n    A trainer with default training logic. It does the following:\n\n    1. Create a :class:`SimpleTrainer` using model, optimizer, dataloader\n       defined by the given config. Create a LR scheduler defined by the config.\n    2. Load the last checkpoint or `cfg.MODEL.WEIGHTS`, if exists, when\n       `resume_or_load` is called.\n    3. Register a few common hooks defined by the config.\n\n    It is created to simplify the **standard model training workflow** and reduce code boilerplate\n    for users who only need the standard training workflow, with standard features.\n    It means this class makes *many assumptions* about your training logic that\n    may easily become invalid in a new research. In fact, any assumptions beyond those made in the\n    :class:`SimpleTrainer` are too much for research.\n\n    The code of this class has been annotated about restrictive assumptions it makes.\n    When they do not work for you, you're encouraged to:\n\n    1. Overwrite methods of this class, OR:\n    2. Use :class:`SimpleTrainer`, which only does minimal SGD training and\n       nothing else. You can then add your own hooks if needed. OR:\n    3. Write your own training loop similar to `tools/plain_train_net.py`.\n\n    See the :doc:`/tutorials/training` tutorials for more details.\n\n    Note that the behavior of this class, like other functions/classes in\n    this file, is not stable, since it is meant to represent the \"common default behavior\".\n    It is only guaranteed to work well with the standard models and training workflow in detectron2.\n    To obtain more stable behavior, write your own training logic with other public APIs.\n\n    Examples:\n    ::\n        trainer = MyTrainer(cfg)\n        trainer.resume_or_load()  # load last checkpoint or MODEL.WEIGHTS\n        trainer.train()\n\n    Attributes:\n        scheduler:\n        checkpointer (DetectionCheckpointer):\n        cfg (CfgNode):\n    \"\"\"\n\n    def __init__(self, cfg):\n        \"\"\"\n        Args:\n            cfg (CfgNode):\n        \"\"\"\n        super().__init__()\n        logger = logging.getLogger(\"detectron2\")\n        if not logger.isEnabledFor(logging.INFO):  # setup_logger is not called for d2\n            setup_logger()\n        cfg = VGTTrainer.auto_scale_workers(cfg, comm.get_world_size())\n\n        self.cfg = cfg\n\n        # Assume these objects must be constructed in this order.\n        model = self.build_model(cfg)\n        optimizer = self.build_optimizer(cfg, model)\n        data_loader = self.build_train_loader(cfg)\n\n        model = create_ddp_model(model, broadcast_buffers=False)\n        self._trainer = (AMPTrainer if cfg.SOLVER.AMP.ENABLED else SimpleTrainer)(\n            model, data_loader, optimizer\n        )\n\n        self.scheduler = self.build_lr_scheduler(cfg, optimizer)\n        self.checkpointer = MyDetectionCheckpointer(\n            # Assume you want to save checkpoints together with logs/statistics\n            model,\n            cfg.OUTPUT_DIR,\n            trainer=weakref.proxy(self),\n        )\n        self.start_iter = 0\n        self.max_iter = cfg.SOLVER.MAX_ITER\n        self.cfg = cfg\n\n        self.register_hooks(self.build_hooks())\n\n    def resume_or_load(self, resume=True):\n        \"\"\"\n        If `resume==True` and `cfg.OUTPUT_DIR` contains the last checkpoint (defined by\n        a `last_checkpoint` file), resume from the file. Resuming means loading all\n        available states (eg. optimizer and scheduler) and update iteration counter\n        from the checkpoint. ``cfg.MODEL.WEIGHTS`` will not be used.\n\n        Otherwise, this is considered as an independent training. The method will load model\n        weights from the file `cfg.MODEL.WEIGHTS` (but will not load other states) and start\n        from iteration 0.\n\n        Args:\n            resume (bool): whether to do resume or not\n        \"\"\"\n        self.checkpointer.resume_or_load(self.cfg.MODEL.WEIGHTS, resume=resume)\n        if resume and self.checkpointer.has_checkpoint():\n            # The checkpoint stores the training iteration that just finished, thus we start\n            # at the next iteration\n            self.start_iter = self.iter + 1\n\n    def build_hooks(self):\n        \"\"\"\n        Build a list of default hooks, including timing, evaluation,\n        checkpointing, lr scheduling, precise BN, writing events.\n\n        Returns:\n            list[HookBase]:\n        \"\"\"\n        cfg = self.cfg.clone()\n        cfg.defrost()\n        cfg.DATALOADER.NUM_WORKERS = 0  # save some memory and time for PreciseBN\n\n        ret = [\n            hooks.IterationTimer(),\n            hooks.LRScheduler(),\n            hooks.PreciseBN(\n                # Run at the same freq as (but before) evaluation.\n                cfg.TEST.EVAL_PERIOD,\n                self.model,\n                # Build a new data loader to not affect training\n                self.build_train_loader(cfg),\n                cfg.TEST.PRECISE_BN.NUM_ITER,\n            )\n            if cfg.TEST.PRECISE_BN.ENABLED and get_bn_modules(self.model)\n            else None,\n        ]\n\n        # Do PreciseBN before checkpointer, because it updates the model and need to\n        # be saved by checkpointer.\n        # This is not always the best: if checkpointing has a different frequency,\n        # some checkpoints may have more precise statistics than others.\n        if comm.is_main_process():\n            ret.append(hooks.PeriodicCheckpointer(self.checkpointer, cfg.SOLVER.CHECKPOINT_PERIOD))\n\n        def test_and_save_results():\n            self._last_eval_results = self.test(self.cfg, self.model)\n            return self._last_eval_results\n\n        # Do evaluation after checkpointer, because then if it fails,\n        # we can use the saved checkpoint to debug.\n        ret.append(hooks.EvalHook(cfg.TEST.EVAL_PERIOD, test_and_save_results))\n\n        if comm.is_main_process():\n            # Here the default print/log frequency of each writer is used.\n            # run writers in the end, so that evaluation metrics are written\n            ret.append(hooks.PeriodicWriter(self.build_writers(), period=20))\n        return ret\n\n    def build_writers(self):\n        \"\"\"\n        Build a list of writers to be used using :func:`default_writers()`.\n        If you'd like a different list of writers, you can overwrite it in\n        your trainer.\n\n        Returns:\n            list[EventWriter]: a list of :class:`EventWriter` objects.\n        \"\"\"\n        return default_writers(self.cfg.OUTPUT_DIR, self.max_iter)\n\n    def train(self):\n        \"\"\"\n        Run training.\n\n        Returns:\n            OrderedDict of results, if evaluation is enabled. Otherwise None.\n        \"\"\"\n        super().train(self.start_iter, self.max_iter)\n        if len(self.cfg.TEST.EXPECTED_RESULTS) and comm.is_main_process():\n            assert hasattr(\n                self, \"_last_eval_results\"\n            ), \"No evaluation results obtained during training!\"\n            verify_results(self.cfg, self._last_eval_results)\n            return self._last_eval_results\n\n    def run_step(self):\n        try:\n            self._trainer.iter = self.iter\n            self._trainer.run_step()\n        except RuntimeError as exception:\n            if \"out of memory\" in str(exception):\n                logger = logging.getLogger(\"detectron2\")\n                logger.warn(\"Out of memory\")\n                # import ipdb;ipdb.set_trace()\n                if hasattr(torch.cuda, 'empty_cache'):\n                    torch.cuda.empty_cache()\n            else:\n                raise exception\n\n    @classmethod\n    def build_model(cls, cfg):\n        \"\"\"\n        Returns:\n            torch.nn.Module:\n\n        It now calls :func:`detectron2.modeling.build_model`.\n        Overwrite it if you'd like a different model.\n        \"\"\"\n        model = build_model(cfg)\n        \n        def compute_para(model):\n            params_num = []\n            filtered_parameters = []\n            for p in filter(lambda p: p.requires_grad, model.parameters()):\n                filtered_parameters.append(p)\n                params_num.append(np.prod(p.size()))\n            total_params = int(sum(params_num))\n            total_params = f'Trainable network params num : {total_params:,}'\n            # print(total_params)\n            return total_params\n        \n        logger = logging.getLogger(\"detectron2\")\n        logger.info(\"Model: {}\".format(compute_para(model)))\n        return model\n\n    @classmethod\n    def build_optimizer(cls, cfg, model):\n        params: List[Dict[str, Any]] = []\n        memo: Set[torch.nn.parameter.Parameter] = set()\n        for key, value in model.named_parameters(recurse=True):\n            if not value.requires_grad:\n                continue\n            # Avoid duplicating parameters\n            if value in memo:\n                continue\n            memo.add(value)\n            lr = cfg.SOLVER.BASE_LR\n            weight_decay = cfg.SOLVER.WEIGHT_DECAY\n            if \"backbone\" in key:\n                lr = lr * cfg.SOLVER.BACKBONE_MULTIPLIER\n            params += [{\"params\": [value], \"lr\": lr, \"weight_decay\": weight_decay}]\n\n        def maybe_add_full_model_gradient_clipping(optim):  # optim: the optimizer class\n            # detectron2 doesn't have full model gradient clipping now\n            clip_norm_val = cfg.SOLVER.CLIP_GRADIENTS.CLIP_VALUE\n            enable = (\n                    cfg.SOLVER.CLIP_GRADIENTS.ENABLED\n                    and cfg.SOLVER.CLIP_GRADIENTS.CLIP_TYPE == \"full_model\"\n                    and clip_norm_val > 0.0\n            )\n\n            class FullModelGradientClippingOptimizer(optim):\n                def step(self, closure=None):\n                    all_params = itertools.chain(*[x[\"params\"] for x in self.param_groups])\n                    torch.nn.utils.clip_grad_norm_(all_params, clip_norm_val)\n                    super().step(closure=closure)\n\n            return FullModelGradientClippingOptimizer if enable else optim\n\n        optimizer_type = cfg.SOLVER.OPTIMIZER\n        if optimizer_type == \"SGD\":\n            optimizer = maybe_add_full_model_gradient_clipping(torch.optim.SGD)(\n                params, cfg.SOLVER.BASE_LR, momentum=cfg.SOLVER.MOMENTUM\n            )\n        elif optimizer_type == \"ADAMW\":\n            optimizer = maybe_add_full_model_gradient_clipping(torch.optim.AdamW)(\n                params, cfg.SOLVER.BASE_LR\n            )\n        else:\n            raise NotImplementedError(f\"no optimizer type {optimizer_type}\")\n        if not cfg.SOLVER.CLIP_GRADIENTS.CLIP_TYPE == \"full_model\":\n            optimizer = maybe_add_gradient_clipping(cfg, optimizer)\n        return optimizer\n\n    @classmethod\n    def build_lr_scheduler(cls, cfg, optimizer):\n        \"\"\"\n        It now calls :func:`detectron2.solver.build_lr_scheduler`.\n        Overwrite it if you'd like a different scheduler.\n        \"\"\"\n        return build_lr_scheduler(cfg, optimizer)\n\n    @classmethod\n    def build_train_loader(cls, cfg):\n        if cfg.AUG.DETR:\n            mapper = DetrDatasetMapper(cfg, is_train=True)\n        else:\n            mapper = None\n        return build_detection_train_loader(cfg, mapper=mapper)\n\n    @classmethod\n    def build_test_loader(cls, cfg, dataset_name):\n        \"\"\"\n        Returns:\n            iterable\n\n        It now calls :func:`detectron2.data.build_detection_test_loader`.\n        Overwrite it if you'd like a different data loader.\n        \"\"\"\n        mapper = DetrDatasetMapper(cfg, is_train=False)\n        return build_detection_test_loader(cfg, dataset_name, mapper=mapper)\n    \n        # return build_detection_test_loader(cfg, dataset_name)\n\n    @classmethod\n    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n        if output_folder is None:\n            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n            return COCOEvaluator(dataset_name, output_dir=output_folder)\n        \n        # if 'icdar' not in dataset_name:\n        #     return COCOEvaluator(dataset_name, output_dir=output_folder)\n        # else:\n        #     return ICDAREvaluator(dataset_name, output_dir=output_folder)\n\n    @classmethod\n    def test(cls, cfg, model, evaluators=None):\n        \"\"\"\n        Evaluate the given model. The given model is expected to already contain\n        weights to evaluate.\n\n        Args:\n            cfg (CfgNode):\n            model (nn.Module):\n            evaluators (list[DatasetEvaluator] or None): if None, will call\n                :meth:`build_evaluator`. Otherwise, must have the same length as\n                ``cfg.DATASETS.TEST``.\n\n        Returns:\n            dict: a dict of result metrics\n        \"\"\"\n        logger = logging.getLogger(__name__)\n        if isinstance(evaluators, DatasetEvaluator):\n            evaluators = [evaluators]\n        if evaluators is not None:\n            assert len(cfg.DATASETS.TEST) == len(evaluators), \"{} != {}\".format(\n                len(cfg.DATASETS.TEST), len(evaluators)\n            )\n\n        results = OrderedDict()\n        for idx, dataset_name in enumerate(cfg.DATASETS.TEST):\n            data_loader = cls.build_test_loader(cfg, dataset_name)\n            # When evaluators are passed in as arguments,\n            # implicitly assume that evaluators can be created before data_loader.\n            if evaluators is not None:\n                evaluator = evaluators[idx]\n            else:\n                try:\n                    evaluator = cls.build_evaluator(cfg, dataset_name)\n                except NotImplementedError:\n                    logger.warn(\n                        \"No evaluator found. Use `MyTrainer.test(evaluators=)`, \"\n                        \"or implement its `build_evaluator` method.\"\n                    )\n                    results[dataset_name] = {}\n                    continue\n            results_i = inference_on_dataset(model, data_loader, evaluator)\n            results[dataset_name] = results_i\n            if comm.is_main_process():\n                assert isinstance(\n                    results_i, dict\n                ), \"Evaluator must return a dict on the main process. Got {} instead.\".format(\n                    results_i\n                )\n                logger.info(\"Evaluation results for {} in csv format:\".format(dataset_name))\n                print_csv_format(results_i)\n\n        if len(results) == 1:\n            results = list(results.values())[0]\n        return results\n\n    @staticmethod\n    def auto_scale_workers(cfg, num_workers: int):\n        \"\"\"\n        When the config is defined for certain number of workers (according to\n        ``cfg.SOLVER.REFERENCE_WORLD_SIZE``) that's different from the number of\n        workers currently in use, returns a new cfg where the total batch size\n        is scaled so that the per-GPU batch size stays the same as the\n        original ``IMS_PER_BATCH // REFERENCE_WORLD_SIZE``.\n\n        Other config options are also scaled accordingly:\n        * training steps and warmup steps are scaled inverse proportionally.\n        * learning rate are scaled proportionally, following :paper:`ImageNet in 1h`.\n\n        For example, with the original config like the following:\n\n        .. code-block:: yaml\n\n            IMS_PER_BATCH: 16\n            BASE_LR: 0.1\n            REFERENCE_WORLD_SIZE: 8\n            MAX_ITER: 5000\n            STEPS: (4000,)\n            CHECKPOINT_PERIOD: 1000\n\n        When this config is used on 16 GPUs instead of the reference number 8,\n        calling this method will return a new config with:\n\n        .. code-block:: yaml\n\n            IMS_PER_BATCH: 32\n            BASE_LR: 0.2\n            REFERENCE_WORLD_SIZE: 16\n            MAX_ITER: 2500\n            STEPS: (2000,)\n            CHECKPOINT_PERIOD: 500\n\n        Note that both the original config and this new config can be trained on 16 GPUs.\n        It's up to user whether to enable this feature (by setting ``REFERENCE_WORLD_SIZE``).\n\n        Returns:\n            CfgNode: a new config. Same as original if ``cfg.SOLVER.REFERENCE_WORLD_SIZE==0``.\n        \"\"\"\n        old_world_size = cfg.SOLVER.REFERENCE_WORLD_SIZE\n        if old_world_size == 0 or old_world_size == num_workers:\n            return cfg\n        cfg = cfg.clone()\n        frozen = cfg.is_frozen()\n        cfg.defrost()\n\n        assert (\n            cfg.SOLVER.IMS_PER_BATCH % old_world_size == 0\n        ), \"Invalid REFERENCE_WORLD_SIZE in config!\"\n        scale = num_workers / old_world_size\n        bs = cfg.SOLVER.IMS_PER_BATCH = int(round(cfg.SOLVER.IMS_PER_BATCH * scale))\n        lr = cfg.SOLVER.BASE_LR = cfg.SOLVER.BASE_LR * scale\n        max_iter = cfg.SOLVER.MAX_ITER = int(round(cfg.SOLVER.MAX_ITER / scale))\n        warmup_iter = cfg.SOLVER.WARMUP_ITERS = int(round(cfg.SOLVER.WARMUP_ITERS / scale))\n        cfg.SOLVER.STEPS = tuple(int(round(s / scale)) for s in cfg.SOLVER.STEPS)\n        cfg.TEST.EVAL_PERIOD = int(round(cfg.TEST.EVAL_PERIOD / scale))\n        cfg.SOLVER.CHECKPOINT_PERIOD = int(round(cfg.SOLVER.CHECKPOINT_PERIOD / scale))\n        cfg.SOLVER.REFERENCE_WORLD_SIZE = num_workers  # maintain invariant\n        logger = logging.getLogger(__name__)\n        logger.info(\n            f\"Auto-scaling the config to batch_size={bs}, learning_rate={lr}, \"\n            f\"max_iter={max_iter}, warmup={warmup_iter}.\"\n        )\n\n        if frozen:\n            cfg.freeze()\n        return cfg\n\n\n# Access basic attributes from the underlying trainer\nfor _attr in [\"model\", \"data_loader\", \"optimizer\"]:\n    setattr(\n        VGTTrainer,\n        _attr,\n        property(\n            # getter\n            lambda self, x=_attr: getattr(self._trainer, x),\n            # setter\n            lambda self, value, x=_attr: setattr(self._trainer, x, value),\n        ),\n    )\n"}
{"type": "source_file", "path": "utils/ditod_vgt/VGTcheckpointer.py", "content": "from detectron2.checkpoint import DetectionCheckpointer\n\nfrom typing import Any\nimport torch\nimport torch.nn as nn\nfrom fvcore.common.checkpoint import _IncompatibleKeys, _strip_prefix_if_present, TORCH_VERSION, quantization, \\\n    ObserverBase, FakeQuantizeBase\nfrom torch import distributed as dist\nfrom scipy import interpolate\nimport numpy as np\nimport torch.nn.functional as F\n\n\ndef append_prefix(k):\n    prefix = 'backbone.'\n    if \"Wordgrid_embedding\" in k:\n        return k[10:]\n    elif \"myFPN\" in k:\n        return prefix + k[16:]\n    else:\n        return prefix + k if not k.startswith(prefix) else k\n    \ndef DiT_append_prefix(k):\n    prefix = 'backbone.bottom_up.backbone.'\n    return prefix + k if not k.startswith(prefix) else k\n\ndef modify_ckpt_state(model, state_dict, logger=None):\n    # reshape absolute position embedding for Swin\n    if state_dict.get(append_prefix('absolute_pos_embed')) is not None:\n        absolute_pos_embed = state_dict[append_prefix('absolute_pos_embed')]\n        N1, L, C1 = absolute_pos_embed.size()\n        N2, C2, H, W = model.backbone.bottom_up.backbone.absolute_pos_embed.size()\n        if N1 != N2 or C1 != C2 or L != H * W:\n            logger.warning(\"Error in loading absolute_pos_embed, pass\")\n        else:\n            state_dict[append_prefix('absolute_pos_embed')] = absolute_pos_embed.view(N2, H, W, C2).permute(0, 3, 1, 2)\n\n    def get_dist_info():\n        if dist.is_available() and dist.is_initialized():\n            rank = dist.get_rank()\n            world_size = dist.get_world_size()\n        else:\n            rank = 0\n            world_size = 1\n        return rank, world_size\n\n    rank, _ = get_dist_info()\n    all_keys = list(state_dict.keys())\n    for key in all_keys:\n        if \"relative_position_index\" in key:\n            state_dict.pop(key)\n\n        if \"relative_position_bias_table\" in key:\n            rel_pos_bias = state_dict[key]\n            src_num_pos, num_attn_heads = rel_pos_bias.size()\n            if key not in model.state_dict():\n                continue\n            dst_num_pos, _ = model.state_dict()[key].size()\n            dst_patch_shape = model.backbone.bottom_up.backbone.patch_embed.patch_shape\n            if dst_patch_shape[0] != dst_patch_shape[1]:\n                raise NotImplementedError()\n            num_extra_tokens = dst_num_pos - (dst_patch_shape[0] * 2 - 1) * (dst_patch_shape[1] * 2 - 1)\n            src_size = int((src_num_pos - num_extra_tokens) ** 0.5)\n            dst_size = int((dst_num_pos - num_extra_tokens) ** 0.5)\n            if src_size != dst_size:\n                if rank == 0:\n                    print(\"Position interpolate for %s from %dx%d to %dx%d\" % (\n                        key, src_size, src_size, dst_size, dst_size))\n                extra_tokens = rel_pos_bias[-num_extra_tokens:, :]\n                rel_pos_bias = rel_pos_bias[:-num_extra_tokens, :]\n\n                def geometric_progression(a, r, n):\n                    return a * (1.0 - r ** n) / (1.0 - r)\n\n                left, right = 1.01, 1.5\n                while right - left > 1e-6:\n                    q = (left + right) / 2.0\n                    gp = geometric_progression(1, q, src_size // 2)\n                    if gp > dst_size // 2:\n                        right = q\n                    else:\n                        left = q\n\n                # if q > 1.13492:\n                #     q = 1.13492\n\n                dis = []\n                cur = 1\n                for i in range(src_size // 2):\n                    dis.append(cur)\n                    cur += q ** (i + 1)\n\n                r_ids = [-_ for _ in reversed(dis)]\n\n                x = r_ids + [0] + dis\n                y = r_ids + [0] + dis\n\n                t = dst_size // 2.0\n                dx = np.arange(-t, t + 0.1, 1.0)\n                dy = np.arange(-t, t + 0.1, 1.0)\n                if rank == 0:\n                    print(\"x = {}\".format(x))\n                    print(\"dx = {}\".format(dx))\n\n                all_rel_pos_bias = []\n\n                for i in range(num_attn_heads):\n                    z = rel_pos_bias[:, i].view(src_size, src_size).float().numpy()\n                    f = interpolate.interp2d(x, y, z, kind='cubic')\n                    all_rel_pos_bias.append(\n                        torch.Tensor(f(dx, dy)).contiguous().view(-1, 1).to(rel_pos_bias.device))\n\n                rel_pos_bias = torch.cat(all_rel_pos_bias, dim=-1)\n                new_rel_pos_bias = torch.cat((rel_pos_bias, extra_tokens), dim=0)\n                state_dict[key] = new_rel_pos_bias\n\n    if append_prefix('pos_embed') in state_dict:\n        pos_embed_checkpoint = state_dict[append_prefix('pos_embed')]\n        embedding_size = pos_embed_checkpoint.shape[-1]\n        num_patches = model.backbone.bottom_up.backbone.patch_embed.num_patches\n        num_extra_tokens = model.backbone.bottom_up.backbone.pos_embed.shape[-2] - num_patches\n        # height (== width) for the checkpoint position embedding\n        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n        # height (== width) for the new position embedding\n        # new_size = int(num_patches ** 0.5)\n        new_size_w = model.backbone.bottom_up.backbone.patch_embed.num_patches_w\n        new_size_h = model.backbone.bottom_up.backbone.patch_embed.num_patches_h\n        # class_token and dist_token are kept unchanged\n        if orig_size != new_size_h or orig_size != new_size_w:\n            if rank == 0:\n                print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size_w, new_size_h))\n            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n            # only the position tokens are interpolated\n            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n            pos_tokens = torch.nn.functional.interpolate(\n                pos_tokens, size=(new_size_w, new_size_h), mode='bicubic', align_corners=False)\n            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n            state_dict[append_prefix('pos_embed')] = new_pos_embed\n\n    # interpolate position bias table if needed\n    relative_position_bias_table_keys = [k for k in state_dict.keys() if \"relative_position_bias_table\" in k]\n    for table_key in relative_position_bias_table_keys:\n        table_pretrained = state_dict[table_key]\n        if table_key not in model.state_dict():\n            continue\n        table_current = model.state_dict()[table_key]\n        L1, nH1 = table_pretrained.size()\n        L2, nH2 = table_current.size()\n        if nH1 != nH2:\n            logger.warning(f\"Error in loading {table_key}, pass\")\n        else:\n            if L1 != L2:\n                S1 = int(L1 ** 0.5)\n                S2 = int(L2 ** 0.5)\n                table_pretrained_resized = F.interpolate(\n                    table_pretrained.permute(1, 0).view(1, nH1, S1, S1),\n                    size=(S2, S2), mode='bicubic')\n                state_dict[table_key] = table_pretrained_resized.view(nH2, L2).permute(1, 0)\n\n    if append_prefix('rel_pos_bias.relative_position_bias_table') in state_dict and \\\n            model.backbone.bottom_up.backbone.use_rel_pos_bias and \\\n            not model.backbone.bottom_up.backbone.use_shared_rel_pos_bias and \\\n            append_prefix('blocks.0.attn.relative_position_bias_table') not in state_dict:\n        logger.info(\"[BEIT] Expand the shared relative position embedding to each transformer block. \")\n        num_layers = model.backbone.bottom_up.backbone.get_num_layers()\n        rel_pos_bias = state_dict[append_prefix(\"rel_pos_bias.relative_position_bias_table\")]\n        for i in range(num_layers):\n            state_dict[\"blocks.%d.attn.relative_position_bias_table\" % i] = rel_pos_bias.clone()\n        state_dict.pop(append_prefix(\"rel_pos_bias.relative_position_bias_table\"))\n\n    return state_dict\n\n\nclass MyDetectionCheckpointer(DetectionCheckpointer):\n    def _load_model(self, checkpoint: Any) -> _IncompatibleKeys:\n        \"\"\"\n        Load weights from a checkpoint.\n\n        Args:\n            checkpoint (Any): checkpoint contains the weights.\n\n        Returns:\n            ``NamedTuple`` with ``missing_keys``, ``unexpected_keys``,\n                and ``incorrect_shapes`` fields:\n                * **missing_keys** is a list of str containing the missing keys\n                * **unexpected_keys** is a list of str containing the unexpected keys\n                * **incorrect_shapes** is a list of (key, shape in checkpoint, shape in model)\n\n            This is just like the return value of\n            :func:`torch.nn.Module.load_state_dict`, but with extra support\n            for ``incorrect_shapes``.\n        \"\"\"\n        DiT_checkpoint_state_dict = torch.load(\"/path/dit-base-224-p16-500k-62d53a.pth\", map_location=torch.device(\"cpu\"))[\"model\"]\n        checkpoint_state_dict = checkpoint.pop(\"model\")\n        # import ipdb;ipdb.set_trace()\n        self._convert_ndarray_to_tensor(checkpoint_state_dict)\n\n        # if the state_dict comes from a model that was wrapped in a\n        # DataParallel or DistributedDataParallel during serialization,\n        # remove the \"module\" prefix before performing the matching.\n        _strip_prefix_if_present(checkpoint_state_dict, \"module.\")\n\n        # workaround https://github.com/pytorch/pytorch/issues/24139\n        model_state_dict = self.model.state_dict()\n        incorrect_shapes = []\n        \n        new_checkpoint_state_dict = {}\n        for k in checkpoint_state_dict.keys():\n            new_checkpoint_state_dict[append_prefix(k)] = checkpoint_state_dict[k]\n\n        for k in DiT_checkpoint_state_dict.keys():\n            new_checkpoint_state_dict[DiT_append_prefix(k)] = DiT_checkpoint_state_dict[k]\n            \n        checkpoint_state_dict = new_checkpoint_state_dict\n        \n        for k in list(checkpoint_state_dict.keys()):\n            if k in model_state_dict:\n                model_param = model_state_dict[k]\n                # Allow mismatch for uninitialized parameters\n                if TORCH_VERSION >= (1, 8) and isinstance(\n                        model_param, nn.parameter.UninitializedParameter\n                ):\n                    continue\n                shape_model = tuple(model_param.shape)\n                shape_checkpoint = tuple(checkpoint_state_dict[k].shape)\n                if shape_model != shape_checkpoint:\n\n                    has_observer_base_classes = (\n                            TORCH_VERSION >= (1, 8)\n                            and hasattr(quantization, \"ObserverBase\")\n                            and hasattr(quantization, \"FakeQuantizeBase\")\n                    )\n                    if has_observer_base_classes:\n                        # Handle the special case of quantization per channel observers,\n                        # where buffer shape mismatches are expected.\n                        def _get_module_for_key(\n                                model: torch.nn.Module, key: str\n                        ) -> torch.nn.Module:\n                            # foo.bar.param_or_buffer_name -> [foo, bar]\n                            key_parts = key.split(\".\")[:-1]\n                            cur_module = model\n                            for key_part in key_parts:\n                                cur_module = getattr(cur_module, key_part)\n                            return cur_module\n\n                        cls_to_skip = (\n                            ObserverBase,\n                            FakeQuantizeBase,\n                        )\n                        target_module = _get_module_for_key(self.model, k)\n                        if isinstance(target_module, cls_to_skip):\n                            # Do not remove modules with expected shape mismatches\n                            # them from the state_dict loading. They have special logic\n                            # in _load_from_state_dict to handle the mismatches.\n                            continue\n\n                    incorrect_shapes.append((k, shape_checkpoint, shape_model))\n                    checkpoint_state_dict.pop(k)\n        incompatible = self.model.load_state_dict(checkpoint_state_dict, strict=False)\n        return _IncompatibleKeys(\n            missing_keys=incompatible.missing_keys,\n            unexpected_keys=incompatible.unexpected_keys,\n            incorrect_shapes=incorrect_shapes,\n        )\n"}
{"type": "source_file", "path": "utils/ditod_vgt/__init__.py", "content": "# --------------------------------------------------------------------------------\n# MPViT: Multi-Path Vision Transformer for Dense Prediction\n# Copyright (c) 2022 Electronics and Telecommunications Research Institute (ETRI).\n# All Rights Reserved.\n# Written by Youngwan Lee\n# This source code is licensed(Dual License(GPL3.0 & Commercial)) under the license found in the\n# LICENSE file in the root directory of this source tree.\n# --------------------------------------------------------------------------------\n\nfrom .config import add_vit_config\nfrom .VGTbackbone import build_VGT_fpn_backbone\nfrom .dataset_mapper import DetrDatasetMapper\nfrom .VGTTrainer import VGTTrainer\nfrom .VGT import VGT\n\nfrom .utils import eval_and_show, load_gt_from_json, pub_load_gt_from_json\n"}
{"type": "source_file", "path": "utils/ditod_vgt/VGT.py", "content": "# -*- coding: utf-8 -*-\n# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\nimport logging\nimport numpy as np\nfrom typing import Dict, List, Optional, Tuple\nimport torch\nfrom torch import nn\n\nfrom detectron2.config import configurable\nfrom detectron2.data.detection_utils import convert_image_to_rgb\nfrom detectron2.structures import ImageList, Instances\nfrom detectron2.utils.events import get_event_storage\nfrom detectron2.utils.logger import log_first_n\n\nfrom detectron2.modeling.meta_arch.build import META_ARCH_REGISTRY\nfrom detectron2.modeling.meta_arch.rcnn import GeneralizedRCNN\n\nfrom .Wordnn_embedding import WordnnEmbedding\n\n__all__ = [\"VGT\"]\n\ndef torch_memory(device, tag=\"\"):\n    # Checks and prints GPU memory\n    print(tag, f'{torch.cuda.memory_allocated(device)/1024/1024:.2f} MB USED')\n    print(tag, f'{torch.cuda.memory_reserved(device)/1024/1024:.2f} MB RESERVED')\n    print(tag, f'{torch.cuda.max_memory_allocated(device)/1024/1024:.2f} MB USED MAX')\n    print(tag, f'{torch.cuda.max_memory_reserved(device)/1024/1024:.2f} MB RESERVED MAX')\n    print('') \n                    \n@META_ARCH_REGISTRY.register()\nclass VGT(GeneralizedRCNN):\n    \n    @configurable\n    def __init__(\n        self,\n        *,\n        vocab_size: int = 30552,\n        hidden_size: int = 768,\n        embedding_dim: int = 64,\n        bros_embedding_path: str = '',\n        use_pretrain_weight: bool = True,\n        use_UNK_text: bool = False,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.Wordgrid_embedding = WordnnEmbedding(vocab_size, hidden_size, embedding_dim, \\\n                                                    bros_embedding_path, use_pretrain_weight, use_UNK_text)\n    @classmethod\n    def from_config(cls, cfg):\n        ret = super().from_config(cfg)\n        ret.update(\n            {\n                \"vocab_size\": cfg.MODEL.WORDGRID.VOCAB_SIZE,\n                \"hidden_size\": cfg.MODEL.WORDGRID.HIDDEN_SIZE,\n                \"embedding_dim\": cfg.MODEL.WORDGRID.EMBEDDING_DIM,\n                \"bros_embedding_path\": cfg.MODEL.WORDGRID.MODEL_PATH,\n                \"use_pretrain_weight\": cfg.MODEL.WORDGRID.USE_PRETRAIN_WEIGHT,\n                \"use_UNK_text\": cfg.MODEL.WORDGRID.USE_UNK_TEXT,\n            }\n        )\n        return ret\n    \n    def forward(self, batched_inputs: List[Dict[str, torch.Tensor]]):\n        \"\"\"\n        Args:\n            batched_inputs: a list, batched outputs of :class:`DatasetMapper` .\n                Each item in the list contains the inputs for one image.\n                For now, each item in the list is a dict that contains:\n\n                * image: Tensor, image in (C, H, W) format.\n                * instances (optional): groundtruth :class:`Instances`\n                * proposals (optional): :class:`Instances`, precomputed proposals.\n\n                Other information that's included in the original dicts, such as:\n\n                * \"height\", \"width\" (int): the output resolution of the model, used in inference.\n                  See :meth:`postprocess` for details.\n\n        Returns:\n            list[dict]:\n                Each dict is the output for one input image.\n                The dict contains one key \"instances\" whose value is a :class:`Instances`.\n                The :class:`Instances` object has the following keys:\n                \"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\", \"pred_keypoints\"\n        \"\"\"\n        if not self.training:\n            return self.inference(batched_inputs)\n\n        images = self.preprocess_image(batched_inputs)\n        \n        if \"instances\" in batched_inputs[0]:\n            gt_instances = [x[\"instances\"].to(self.device) for x in batched_inputs]\n        else:\n            gt_instances = None\n\n        chargrid = self.Wordgrid_embedding(images.tensor, batched_inputs)\n        features = self.backbone(images.tensor, chargrid)\n\n        if self.proposal_generator is not None:\n            proposals, proposal_losses = self.proposal_generator(images, features, gt_instances)\n        else:\n            assert \"proposals\" in batched_inputs[0]\n            proposals = [x[\"proposals\"].to(self.device) for x in batched_inputs]\n            proposal_losses = {}\n            \n        _, detector_losses = self.roi_heads(images, features, proposals, gt_instances)\n        if self.vis_period > 0:\n            storage = get_event_storage()\n            if storage.iter % self.vis_period == 0:\n                self.visualize_training(batched_inputs, proposals)\n\n        losses = {}\n        losses.update(detector_losses)\n        losses.update(proposal_losses)\n        \n        return losses\n\n    \n    def inference(\n        self,\n        batched_inputs: List[Dict[str, torch.Tensor]],\n        detected_instances: Optional[List[Instances]] = None,\n        do_postprocess: bool = True,\n    ):\n        \"\"\"\n        Run inference on the given inputs.\n\n        Args:\n            batched_inputs (list[dict]): same as in :meth:`forward`\n            detected_instances (None or list[Instances]): if not None, it\n                contains an `Instances` object per image. The `Instances`\n                object contains \"pred_boxes\" and \"pred_classes\" which are\n                known boxes in the image.\n                The inference will then skip the detection of bounding boxes,\n                and only predict other per-ROI outputs.\n            do_postprocess (bool): whether to apply post-processing on the outputs.\n\n        Returns:\n            When do_postprocess=True, same as in :meth:`forward`.\n            Otherwise, a list[Instances] containing raw network outputs.\n        \"\"\"\n        assert not self.training\n\n        images = self.preprocess_image(batched_inputs)\n        \n        chargrid = self.Wordgrid_embedding(images.tensor, batched_inputs)\n        features = self.backbone(images.tensor, chargrid)\n\n        if detected_instances is None:\n            if self.proposal_generator is not None:\n                proposals, _ = self.proposal_generator(images, features, None)\n            else:\n                assert \"proposals\" in batched_inputs[0]\n                proposals = [x[\"proposals\"].to(self.device) for x in batched_inputs]\n\n            results, _ = self.roi_heads(images, features, proposals, None)\n        else:\n            detected_instances = [x.to(self.device) for x in detected_instances]\n            results = self.roi_heads.forward_with_given_boxes(features, detected_instances)\n        \n        if do_postprocess:\n            assert not torch.jit.is_scripting(), \"Scripting is not supported for postprocess.\"\n            return GeneralizedRCNN._postprocess(results, batched_inputs, images.image_sizes)\n        else:\n            return results"}
{"type": "source_file", "path": "utils/database/file_db.py", "content": "from .base import Database\nfrom enum import Enum\nfrom typing import Optional\nfrom loguru import logger\n\n# database_name = \"pdf_translator_files.db\"\n\nclass FileStatus(Enum):\n    NOT_TRANSLATED = 0\n    TRANSLATING = 1\n    TRANSLATED = 2\n\nclass FileDatabase(Database):\n    def __init__(\n        self,\n        database_name,\n        table_name=\"pdf_translator_files\",\n        table_format={\n            \"file\": str,\n            \"src_path\": str,\n            \"target_path\": str,\n            \"status\": int,\n        },\n    ):\n        super().__init__(database_name, table_name, table_format=table_format)\n        self.check_table()\n        self.clear_unfinished_files()\n\n    def clear_unfinished_files(self):\n        conn, c = self.new_cursor()\n        c.execute(\n            f\"DELETE FROM {self.table_name} WHERE status = ?\", (FileStatus.TRANSLATING.value,)\n        )\n        conn.commit()\n        # delete NOT_TRANSLATED files\n        c.execute(\n            f\"DELETE FROM {self.table_name} WHERE status = ?\", (FileStatus.NOT_TRANSLATED.value,)\n        )\n        conn.commit()\n        conn.close()\n        \n    def set_translating_to_not_translated(self):\n        conn, c = self.new_cursor()\n        c.execute(\n            f\"UPDATE {self.table_name} SET status = ? WHERE status = ?\",\n            (FileStatus.NOT_TRANSLATED.value, FileStatus.TRANSLATING.value),\n        )\n        conn.commit()\n        conn.close()\n        \n    def set_translating(self, file: str):\n        conn, c = self.new_cursor()\n        while True:\n            try:\n                c.execute(\n                    f\"UPDATE {self.table_name} SET status = ? WHERE file = ?\",\n                    (FileStatus.TRANSLATING.value, file),\n                )\n                conn.commit()\n                break\n            except Exception as e:\n                logger.error(f\"Error updating {file} status to TRANSLATING: {e}\")\n                continue\n        conn.close()\n        \n    def set_translated(self, file: str):\n        conn, c = self.new_cursor()\n        c.execute(\n            f\"UPDATE {self.table_name} SET status = ? WHERE file = ?\",\n            (FileStatus.TRANSLATED.value, file),\n        )\n        conn.commit()\n        conn.close()\n\n    def add_file(self, file, src_path, target_path, status: FileStatus | int):\n        if isinstance(status, FileStatus):\n            status = status.value\n        conn, c = self.new_cursor()\n        c.execute(\n            f\"INSERT INTO {self.table_name} VALUES (?, ?, ?, ?)\",\n            (file, src_path, target_path, status),\n        )\n        conn.commit()\n        conn.close()\n    \n    def remove_file(self, file):\n        conn, c = self.new_cursor()\n        c.execute(f\"DELETE FROM {self.table_name} WHERE file = ?\", (file,))\n        conn.commit()\n        conn.close()\n\n    def update_file_status(self, file, status):\n        conn, c = self.new_cursor()\n        c.execute(\n            f\"UPDATE {self.table_name} SET status = ? WHERE file = ?\", (status, file)\n        )\n        conn.commit()\n        conn.close()\n    \n    def get_files(self, status: Optional[FileStatus]):\n        conn, c = self.new_cursor()\n        if status is None:\n            c.execute(f\"SELECT * FROM {self.table_name}\")\n        else:\n            c.execute(f\"SELECT * FROM {self.table_name} WHERE status = ?\", (status.value,))\n        result = c.fetchall()\n        conn.commit()\n        conn.close()\n        return result\n    \n    def check_file_exists(self, file):\n        conn, c = self.new_cursor()\n        c.execute(f\"SELECT * FROM {self.table_name} WHERE file = ?\", (file,))\n        result = c.fetchone() is not None\n        conn.commit()\n        conn.close()\n        return result"}
{"type": "source_file", "path": "utils/ditod/mytrainer.py", "content": "# -*- coding: utf-8 -*-\n# Copyright (c) Facebook, Inc. and its affiliates.\n\n\"\"\"\nThis file contains components with some default boilerplate logic user may need\nin training / testing. They will not work for everyone, but many users may find them useful.\n\nThe behavior of functions/classes in this file is subject to change,\nsince they are meant to represent the \"common default behavior\" people need in their projects.\n\"\"\"\n\nimport argparse\nimport logging\nimport os\nimport sys\nimport weakref\nfrom collections import OrderedDict\nfrom typing import Optional\nimport torch\nfrom fvcore.nn.precise_bn import get_bn_modules\nfrom omegaconf import OmegaConf\nfrom torch.nn.parallel import DistributedDataParallel\n\nimport detectron2.data.transforms as T\nfrom detectron2.checkpoint import DetectionCheckpointer\nfrom detectron2.config import CfgNode, LazyConfig\nfrom detectron2.data import (\n    MetadataCatalog,\n    build_detection_test_loader,\n    build_detection_train_loader,\n)\nfrom detectron2.evaluation import (\n    DatasetEvaluator,\n    inference_on_dataset,\n    print_csv_format,\n    verify_results,\n)\nfrom detectron2.modeling import build_model\nfrom detectron2.solver import build_lr_scheduler, build_optimizer\nfrom detectron2.utils import comm\nfrom detectron2.utils.collect_env import collect_env_info\nfrom detectron2.utils.env import seed_all_rng\nfrom detectron2.utils.events import CommonMetricPrinter, JSONWriter, TensorboardXWriter\nfrom detectron2.utils.file_io import PathManager\nfrom detectron2.utils.logger import setup_logger\n\nfrom detectron2.engine import hooks\nfrom detectron2.engine.train_loop import AMPTrainer, SimpleTrainer, TrainerBase\n\nfrom .mycheckpointer import MyDetectionCheckpointer\nfrom typing import Any, Dict, List, Set\nimport itertools\nfrom detectron2.solver.build import maybe_add_gradient_clipping\nfrom .dataset_mapper import DetrDatasetMapper\nfrom .icdar_evaluation import ICDAREvaluator\nfrom detectron2.evaluation import COCOEvaluator\n\n__all__ = [\n    \"create_ddp_model\",\n    \"default_argument_parser\",\n    \"default_setup\",\n    \"default_writers\",\n    \"DefaultPredictor\",\n    \"MyTrainer\",\n]\n\n\ndef create_ddp_model(model, *, fp16_compression=False, **kwargs):\n    \"\"\"\n    Create a DistributedDataParallel model if there are >1 processes.\n\n    Args:\n        model: a torch.nn.Module\n        fp16_compression: add fp16 compression hooks to the ddp object.\n            See more at https://pytorch.org/docs/stable/ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook\n        kwargs: other arguments of :module:`torch.nn.parallel.DistributedDataParallel`.\n    \"\"\"  # noqa\n    if comm.get_world_size() == 1:\n        return model\n    if \"device_ids\" not in kwargs:\n        kwargs[\"device_ids\"] = [comm.get_local_rank()]\n    ddp = DistributedDataParallel(model, **kwargs)\n    if fp16_compression:\n        from torch.distributed.algorithms.ddp_comm_hooks import default as comm_hooks\n\n        ddp.register_comm_hook(state=None, hook=comm_hooks.fp16_compress_hook)\n    return ddp\n\n\ndef default_argument_parser(epilog=None):\n    \"\"\"\n    Create a parser with some common arguments used by detectron2 users.\n\n    Args:\n        epilog (str): epilog passed to ArgumentParser describing the usage.\n\n    Returns:\n        argparse.ArgumentParser:\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        epilog=epilog\n        or f\"\"\"\nExamples:\n\nRun on single machine:\n    $ {sys.argv[0]} --num-gpus 8 --config-file cfg.yaml\n\nChange some config options:\n    $ {sys.argv[0]} --config-file cfg.yaml MODEL.WEIGHTS /path/to/weight.pth SOLVER.BASE_LR 0.001\n\nRun on multiple machines:\n    (machine0)$ {sys.argv[0]} --machine-rank 0 --num-machines 2 --dist-url <URL> [--other-flags]\n    (machine1)$ {sys.argv[0]} --machine-rank 1 --num-machines 2 --dist-url <URL> [--other-flags]\n\"\"\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n    )\n    parser.add_argument(\"--config-file\", default=\"\", metavar=\"FILE\", help=\"path to config file\")\n    parser.add_argument(\n        \"--resume\",\n        action=\"store_true\",\n        help=\"Whether to attempt to resume from the checkpoint directory. \"\n        \"See documentation of `MyTrainer.resume_or_load()` for what it means.\",\n    )\n    parser.add_argument(\"--eval-only\", action=\"store_true\", help=\"perform evaluation only\")\n    parser.add_argument(\"--num-gpus\", type=int, default=1, help=\"number of gpus *per machine*\")\n    parser.add_argument(\"--num-machines\", type=int, default=1, help=\"total number of machines\")\n    parser.add_argument(\n        \"--machine-rank\", type=int, default=0, help=\"the rank of this machine (unique per machine)\"\n    )\n\n    # PyTorch still may leave orphan processes in multi-gpu training.\n    # Therefore we use a deterministic way to obtain port,\n    # so that users are aware of orphan processes by seeing the port occupied.\n    port = 2 ** 15 + 2 ** 14 + hash(os.getuid() if sys.platform != \"win32\" else 1) % 2 ** 14\n    parser.add_argument(\n        \"--dist-url\",\n        default=\"tcp://127.0.0.1:{}\".format(port),\n        help=\"initialization URL for pytorch distributed backend. See \"\n        \"https://pytorch.org/docs/stable/distributed.html for details.\",\n    )\n    parser.add_argument(\n        \"opts\",\n        help=\"\"\"\nModify config options at the end of the command. For Yacs configs, use\nspace-separated \"PATH.KEY VALUE\" pairs.\nFor python-based LazyConfig, use \"path.key=value\".\n        \"\"\".strip(),\n        default=None,\n        nargs=argparse.REMAINDER,\n    )\n    return parser\n\n\ndef _try_get_key(cfg, *keys, default=None):\n    \"\"\"\n    Try select keys from cfg until the first key that exists. Otherwise return default.\n    \"\"\"\n    if isinstance(cfg, CfgNode):\n        cfg = OmegaConf.create(cfg.dump())\n    for k in keys:\n        none = object()\n        p = OmegaConf.select(cfg, k, default=none)\n        if p is not none:\n            return p\n    return default\n\n\ndef _highlight(code, filename):\n    try:\n        import pygments\n    except ImportError:\n        return code\n\n    from pygments.lexers import Python3Lexer, YamlLexer\n    from pygments.formatters import Terminal256Formatter\n\n    lexer = Python3Lexer() if filename.endswith(\".py\") else YamlLexer()\n    code = pygments.highlight(code, lexer, Terminal256Formatter(style=\"monokai\"))\n    return code\n\n\ndef default_setup(cfg, args):\n    \"\"\"\n    Perform some basic common setups at the beginning of a job, including:\n\n    1. Set up the detectron2 logger\n    2. Log basic information about environment, cmdline arguments, and config\n    3. Backup the config to the output directory\n\n    Args:\n        cfg (CfgNode or omegaconf.DictConfig): the full config to be used\n        args (argparse.NameSpace): the command line arguments to be logged\n    \"\"\"\n    output_dir = _try_get_key(cfg, \"OUTPUT_DIR\", \"output_dir\", \"train.output_dir\")\n    if comm.is_main_process() and output_dir:\n        PathManager.mkdirs(output_dir)\n\n    rank = comm.get_rank()\n    setup_logger(output_dir, distributed_rank=rank, name=\"fvcore\")\n    logger = setup_logger(output_dir, distributed_rank=rank)\n\n    logger.info(\"Rank of current process: {}. World size: {}\".format(rank, comm.get_world_size()))\n    logger.info(\"Environment info:\\n\" + collect_env_info())\n\n    logger.info(\"Command line arguments: \" + str(args))\n    if hasattr(args, \"config_file\") and args.config_file != \"\":\n        logger.info(\n            \"Contents of args.config_file={}:\\n{}\".format(\n                args.config_file,\n                _highlight(PathManager.open(args.config_file, \"r\").read(), args.config_file),\n            )\n        )\n\n    if comm.is_main_process() and output_dir:\n        # Note: some of our scripts may expect the existence of\n        # config.yaml in output directory\n        path = os.path.join(output_dir, \"config.yaml\")\n        if isinstance(cfg, CfgNode):\n            logger.info(\"Running with full config:\\n{}\".format(_highlight(cfg.dump(), \".yaml\")))\n            with PathManager.open(path, \"w\") as f:\n                f.write(cfg.dump())\n        else:\n            LazyConfig.save(cfg, path)\n        logger.info(\"Full config saved to {}\".format(path))\n\n    # make sure each worker has a different, yet deterministic seed if specified\n    seed = _try_get_key(cfg, \"SEED\", \"train.seed\", default=-1)\n    seed_all_rng(None if seed < 0 else seed + rank)\n\n    # cudnn benchmark has large overhead. It shouldn't be used considering the small size of\n    # typical validation set.\n    if not (hasattr(args, \"eval_only\") and args.eval_only):\n        torch.backends.cudnn.benchmark = _try_get_key(\n            cfg, \"CUDNN_BENCHMARK\", \"train.cudnn_benchmark\", default=False\n        )\n\n\ndef default_writers(output_dir: str, max_iter: Optional[int] = None):\n    \"\"\"\n    Build a list of :class:`EventWriter` to be used.\n    It now consists of a :class:`CommonMetricPrinter`,\n    :class:`TensorboardXWriter` and :class:`JSONWriter`.\n\n    Args:\n        output_dir: directory to store JSON metrics and tensorboard events\n        max_iter: the total number of iterations\n\n    Returns:\n        list[EventWriter]: a list of :class:`EventWriter` objects.\n    \"\"\"\n    PathManager.mkdirs(output_dir)\n    return [\n        # It may not always print what you want to see, since it prints \"common\" metrics only.\n        CommonMetricPrinter(max_iter),\n        JSONWriter(os.path.join(output_dir, \"metrics.json\")),\n        TensorboardXWriter(output_dir),\n    ]\n\n\nclass DefaultPredictor:\n    \"\"\"\n    Create a simple end-to-end predictor with the given config that runs on\n    single device for a single input image.\n\n    Compared to using the model directly, this class does the following additions:\n\n    1. Load checkpoint from `cfg.MODEL.WEIGHTS`.\n    2. Always take BGR image as the input and apply conversion defined by `cfg.INPUT.FORMAT`.\n    3. Apply resizing defined by `cfg.INPUT.{MIN,MAX}_SIZE_TEST`.\n    4. Take one input image and produce a single output, instead of a batch.\n\n    This is meant for simple demo purposes, so it does the above steps automatically.\n    This is not meant for benchmarks or running complicated inference logic.\n    If you'd like to do anything more complicated, please refer to its source code as\n    examples to build and use the model manually.\n\n    Attributes:\n        metadata (Metadata): the metadata of the underlying dataset, obtained from\n            cfg.DATASETS.TEST.\n\n    Examples:\n    ::\n        pred = DefaultPredictor(cfg)\n        inputs = cv2.imread(\"input.jpg\")\n        outputs = pred(inputs)\n    \"\"\"\n\n    def __init__(self, cfg):\n        self.cfg = cfg.clone()  # cfg can be modified by model\n        self.model = build_model(self.cfg)\n        self.model.eval()\n        if len(cfg.DATASETS.TEST):\n            self.metadata = MetadataCatalog.get(cfg.DATASETS.TEST[0])\n\n        checkpointer = DetectionCheckpointer(self.model)\n        checkpointer.load(cfg.MODEL.WEIGHTS)\n\n        self.aug = T.ResizeShortestEdge(\n            [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST\n        )\n\n        self.input_format = cfg.INPUT.FORMAT\n        assert self.input_format in [\"RGB\", \"BGR\"], self.input_format\n\n    def __call__(self, original_image):\n        \"\"\"\n        Args:\n            original_image (np.ndarray): an image of shape (H, W, C) (in BGR order).\n\n        Returns:\n            predictions (dict):\n                the output of the model for one image only.\n                See :doc:`/tutorials/models` for details about the format.\n        \"\"\"\n        with torch.no_grad():  # https://github.com/sphinx-doc/sphinx/issues/4258\n            # Apply pre-processing to image.\n            if self.input_format == \"RGB\":\n                # whether the model expects BGR inputs or RGB\n                original_image = original_image[:, :, ::-1]\n            height, width = original_image.shape[:2]\n            image = self.aug.get_transform(original_image).apply_image(original_image)\n            image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n\n            inputs = {\"image\": image, \"height\": height, \"width\": width}\n            predictions = self.model([inputs])[0]\n            return predictions\n\n\nclass MyTrainer(TrainerBase):\n    \"\"\"\n    A trainer with default training logic. It does the following:\n\n    1. Create a :class:`SimpleTrainer` using model, optimizer, dataloader\n       defined by the given config. Create a LR scheduler defined by the config.\n    2. Load the last checkpoint or `cfg.MODEL.WEIGHTS`, if exists, when\n       `resume_or_load` is called.\n    3. Register a few common hooks defined by the config.\n\n    It is created to simplify the **standard model training workflow** and reduce code boilerplate\n    for users who only need the standard training workflow, with standard features.\n    It means this class makes *many assumptions* about your training logic that\n    may easily become invalid in a new research. In fact, any assumptions beyond those made in the\n    :class:`SimpleTrainer` are too much for research.\n\n    The code of this class has been annotated about restrictive assumptions it makes.\n    When they do not work for you, you're encouraged to:\n\n    1. Overwrite methods of this class, OR:\n    2. Use :class:`SimpleTrainer`, which only does minimal SGD training and\n       nothing else. You can then add your own hooks if needed. OR:\n    3. Write your own training loop similar to `tools/plain_train_net.py`.\n\n    See the :doc:`/tutorials/training` tutorials for more details.\n\n    Note that the behavior of this class, like other functions/classes in\n    this file, is not stable, since it is meant to represent the \"common default behavior\".\n    It is only guaranteed to work well with the standard models and training workflow in detectron2.\n    To obtain more stable behavior, write your own training logic with other public APIs.\n\n    Examples:\n    ::\n        trainer = MyTrainer(cfg)\n        trainer.resume_or_load()  # load last checkpoint or MODEL.WEIGHTS\n        trainer.train()\n\n    Attributes:\n        scheduler:\n        checkpointer (DetectionCheckpointer):\n        cfg (CfgNode):\n    \"\"\"\n\n    def __init__(self, cfg):\n        \"\"\"\n        Args:\n            cfg (CfgNode):\n        \"\"\"\n        super().__init__()\n        logger = logging.getLogger(\"detectron2\")\n        if not logger.isEnabledFor(logging.INFO):  # setup_logger is not called for d2\n            setup_logger()\n        cfg = MyTrainer.auto_scale_workers(cfg, comm.get_world_size())\n\n        self.cfg = cfg\n\n        # Assume these objects must be constructed in this order.\n        model = self.build_model(cfg)\n        optimizer = self.build_optimizer(cfg, model)\n        data_loader = self.build_train_loader(cfg)\n\n        model = create_ddp_model(model, broadcast_buffers=False)\n        self._trainer = (AMPTrainer if cfg.SOLVER.AMP.ENABLED else SimpleTrainer)(\n            model, data_loader, optimizer\n        )\n\n        self.scheduler = self.build_lr_scheduler(cfg, optimizer)\n        self.checkpointer = MyDetectionCheckpointer(\n            # Assume you want to save checkpoints together with logs/statistics\n            model,\n            cfg.OUTPUT_DIR,\n            trainer=weakref.proxy(self),\n        )\n        self.start_iter = 0\n        self.max_iter = cfg.SOLVER.MAX_ITER\n        self.cfg = cfg\n\n        self.register_hooks(self.build_hooks())\n\n    def resume_or_load(self, resume=True):\n        \"\"\"\n        If `resume==True` and `cfg.OUTPUT_DIR` contains the last checkpoint (defined by\n        a `last_checkpoint` file), resume from the file. Resuming means loading all\n        available states (eg. optimizer and scheduler) and update iteration counter\n        from the checkpoint. ``cfg.MODEL.WEIGHTS`` will not be used.\n\n        Otherwise, this is considered as an independent training. The method will load model\n        weights from the file `cfg.MODEL.WEIGHTS` (but will not load other states) and start\n        from iteration 0.\n\n        Args:\n            resume (bool): whether to do resume or not\n        \"\"\"\n        self.checkpointer.resume_or_load(self.cfg.MODEL.WEIGHTS, resume=resume)\n        if resume and self.checkpointer.has_checkpoint():\n            # The checkpoint stores the training iteration that just finished, thus we start\n            # at the next iteration\n            self.start_iter = self.iter + 1\n\n    def build_hooks(self):\n        \"\"\"\n        Build a list of default hooks, including timing, evaluation,\n        checkpointing, lr scheduling, precise BN, writing events.\n\n        Returns:\n            list[HookBase]:\n        \"\"\"\n        cfg = self.cfg.clone()\n        cfg.defrost()\n        cfg.DATALOADER.NUM_WORKERS = 0  # save some memory and time for PreciseBN\n\n        ret = [\n            hooks.IterationTimer(),\n            hooks.LRScheduler(),\n            hooks.PreciseBN(\n                # Run at the same freq as (but before) evaluation.\n                cfg.TEST.EVAL_PERIOD,\n                self.model,\n                # Build a new data loader to not affect training\n                self.build_train_loader(cfg),\n                cfg.TEST.PRECISE_BN.NUM_ITER,\n            )\n            if cfg.TEST.PRECISE_BN.ENABLED and get_bn_modules(self.model)\n            else None,\n        ]\n\n        # Do PreciseBN before checkpointer, because it updates the model and need to\n        # be saved by checkpointer.\n        # This is not always the best: if checkpointing has a different frequency,\n        # some checkpoints may have more precise statistics than others.\n        if comm.is_main_process():\n            ret.append(hooks.PeriodicCheckpointer(self.checkpointer, cfg.SOLVER.CHECKPOINT_PERIOD))\n\n        def test_and_save_results():\n            self._last_eval_results = self.test(self.cfg, self.model)\n            return self._last_eval_results\n\n        # Do evaluation after checkpointer, because then if it fails,\n        # we can use the saved checkpoint to debug.\n        ret.append(hooks.EvalHook(cfg.TEST.EVAL_PERIOD, test_and_save_results))\n\n        if comm.is_main_process():\n            # Here the default print/log frequency of each writer is used.\n            # run writers in the end, so that evaluation metrics are written\n            ret.append(hooks.PeriodicWriter(self.build_writers(), period=20))\n        return ret\n\n    def build_writers(self):\n        \"\"\"\n        Build a list of writers to be used using :func:`default_writers()`.\n        If you'd like a different list of writers, you can overwrite it in\n        your trainer.\n\n        Returns:\n            list[EventWriter]: a list of :class:`EventWriter` objects.\n        \"\"\"\n        return default_writers(self.cfg.OUTPUT_DIR, self.max_iter)\n\n    def train(self):\n        \"\"\"\n        Run training.\n\n        Returns:\n            OrderedDict of results, if evaluation is enabled. Otherwise None.\n        \"\"\"\n        super().train(self.start_iter, self.max_iter)\n        if len(self.cfg.TEST.EXPECTED_RESULTS) and comm.is_main_process():\n            assert hasattr(\n                self, \"_last_eval_results\"\n            ), \"No evaluation results obtained during training!\"\n            verify_results(self.cfg, self._last_eval_results)\n            return self._last_eval_results\n\n    def run_step(self):\n        self._trainer.iter = self.iter\n        self._trainer.run_step()\n\n    @classmethod\n    def build_model(cls, cfg):\n        \"\"\"\n        Returns:\n            torch.nn.Module:\n\n        It now calls :func:`detectron2.modeling.build_model`.\n        Overwrite it if you'd like a different model.\n        \"\"\"\n        model = build_model(cfg)\n        logger = logging.getLogger(__name__)\n        logger.info(\"Model:\\n{}\".format(model))\n        return model\n\n    @classmethod\n    def build_optimizer(cls, cfg, model):\n        params: List[Dict[str, Any]] = []\n        memo: Set[torch.nn.parameter.Parameter] = set()\n        for key, value in model.named_parameters(recurse=True):\n            if not value.requires_grad:\n                continue\n            # Avoid duplicating parameters\n            if value in memo:\n                continue\n            memo.add(value)\n            lr = cfg.SOLVER.BASE_LR\n            weight_decay = cfg.SOLVER.WEIGHT_DECAY\n            if \"backbone\" in key:\n                lr = lr * cfg.SOLVER.BACKBONE_MULTIPLIER\n            params += [{\"params\": [value], \"lr\": lr, \"weight_decay\": weight_decay}]\n\n        def maybe_add_full_model_gradient_clipping(optim):  # optim: the optimizer class\n            # detectron2 doesn't have full model gradient clipping now\n            clip_norm_val = cfg.SOLVER.CLIP_GRADIENTS.CLIP_VALUE\n            enable = (\n                    cfg.SOLVER.CLIP_GRADIENTS.ENABLED\n                    and cfg.SOLVER.CLIP_GRADIENTS.CLIP_TYPE == \"full_model\"\n                    and clip_norm_val > 0.0\n            )\n\n            class FullModelGradientClippingOptimizer(optim):\n                def step(self, closure=None):\n                    all_params = itertools.chain(*[x[\"params\"] for x in self.param_groups])\n                    torch.nn.utils.clip_grad_norm_(all_params, clip_norm_val)\n                    super().step(closure=closure)\n\n            return FullModelGradientClippingOptimizer if enable else optim\n\n        optimizer_type = cfg.SOLVER.OPTIMIZER\n        if optimizer_type == \"SGD\":\n            optimizer = maybe_add_full_model_gradient_clipping(torch.optim.SGD)(\n                params, cfg.SOLVER.BASE_LR, momentum=cfg.SOLVER.MOMENTUM\n            )\n        elif optimizer_type == \"ADAMW\":\n            optimizer = maybe_add_full_model_gradient_clipping(torch.optim.AdamW)(\n                params, cfg.SOLVER.BASE_LR\n            )\n        else:\n            raise NotImplementedError(f\"no optimizer type {optimizer_type}\")\n        if not cfg.SOLVER.CLIP_GRADIENTS.CLIP_TYPE == \"full_model\":\n            optimizer = maybe_add_gradient_clipping(cfg, optimizer)\n        return optimizer\n\n    @classmethod\n    def build_lr_scheduler(cls, cfg, optimizer):\n        \"\"\"\n        It now calls :func:`detectron2.solver.build_lr_scheduler`.\n        Overwrite it if you'd like a different scheduler.\n        \"\"\"\n        return build_lr_scheduler(cfg, optimizer)\n\n    @classmethod\n    def build_train_loader(cls, cfg):\n        if cfg.AUG.DETR:\n            mapper = DetrDatasetMapper(cfg, is_train=True)\n        else:\n            mapper = None\n        return build_detection_train_loader(cfg, mapper=mapper)\n\n    @classmethod\n    def build_test_loader(cls, cfg, dataset_name):\n        \"\"\"\n        Returns:\n            iterable\n\n        It now calls :func:`detectron2.data.build_detection_test_loader`.\n        Overwrite it if you'd like a different data loader.\n        \"\"\"\n        return build_detection_test_loader(cfg, dataset_name)\n\n    @classmethod\n    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n        if output_folder is None:\n            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n        if 'icdar' not in dataset_name:\n            return COCOEvaluator(dataset_name, output_dir=output_folder)\n        else:\n            return ICDAREvaluator(dataset_name, output_dir=output_folder)\n\n    @classmethod\n    def test(cls, cfg, model, evaluators=None):\n        \"\"\"\n        Evaluate the given model. The given model is expected to already contain\n        weights to evaluate.\n\n        Args:\n            cfg (CfgNode):\n            model (nn.Module):\n            evaluators (list[DatasetEvaluator] or None): if None, will call\n                :meth:`build_evaluator`. Otherwise, must have the same length as\n                ``cfg.DATASETS.TEST``.\n\n        Returns:\n            dict: a dict of result metrics\n        \"\"\"\n        logger = logging.getLogger(__name__)\n        if isinstance(evaluators, DatasetEvaluator):\n            evaluators = [evaluators]\n        if evaluators is not None:\n            assert len(cfg.DATASETS.TEST) == len(evaluators), \"{} != {}\".format(\n                len(cfg.DATASETS.TEST), len(evaluators)\n            )\n\n        results = OrderedDict()\n        for idx, dataset_name in enumerate(cfg.DATASETS.TEST):\n            data_loader = cls.build_test_loader(cfg, dataset_name)\n            # When evaluators are passed in as arguments,\n            # implicitly assume that evaluators can be created before data_loader.\n            if evaluators is not None:\n                evaluator = evaluators[idx]\n            else:\n                try:\n                    evaluator = cls.build_evaluator(cfg, dataset_name)\n                except NotImplementedError:\n                    logger.warn(\n                        \"No evaluator found. Use `MyTrainer.test(evaluators=)`, \"\n                        \"or implement its `build_evaluator` method.\"\n                    )\n                    results[dataset_name] = {}\n                    continue\n            results_i = inference_on_dataset(model, data_loader, evaluator)\n            results[dataset_name] = results_i\n            if comm.is_main_process():\n                assert isinstance(\n                    results_i, dict\n                ), \"Evaluator must return a dict on the main process. Got {} instead.\".format(\n                    results_i\n                )\n                logger.info(\"Evaluation results for {} in csv format:\".format(dataset_name))\n                print_csv_format(results_i)\n\n        if len(results) == 1:\n            results = list(results.values())[0]\n        return results\n\n    @staticmethod\n    def auto_scale_workers(cfg, num_workers: int):\n        \"\"\"\n        When the config is defined for certain number of workers (according to\n        ``cfg.SOLVER.REFERENCE_WORLD_SIZE``) that's different from the number of\n        workers currently in use, returns a new cfg where the total batch size\n        is scaled so that the per-GPU batch size stays the same as the\n        original ``IMS_PER_BATCH // REFERENCE_WORLD_SIZE``.\n\n        Other config options are also scaled accordingly:\n        * training steps and warmup steps are scaled inverse proportionally.\n        * learning rate are scaled proportionally, following :paper:`ImageNet in 1h`.\n\n        For example, with the original config like the following:\n\n        .. code-block:: yaml\n\n            IMS_PER_BATCH: 16\n            BASE_LR: 0.1\n            REFERENCE_WORLD_SIZE: 8\n            MAX_ITER: 5000\n            STEPS: (4000,)\n            CHECKPOINT_PERIOD: 1000\n\n        When this config is used on 16 GPUs instead of the reference number 8,\n        calling this method will return a new config with:\n\n        .. code-block:: yaml\n\n            IMS_PER_BATCH: 32\n            BASE_LR: 0.2\n            REFERENCE_WORLD_SIZE: 16\n            MAX_ITER: 2500\n            STEPS: (2000,)\n            CHECKPOINT_PERIOD: 500\n\n        Note that both the original config and this new config can be trained on 16 GPUs.\n        It's up to user whether to enable this feature (by setting ``REFERENCE_WORLD_SIZE``).\n\n        Returns:\n            CfgNode: a new config. Same as original if ``cfg.SOLVER.REFERENCE_WORLD_SIZE==0``.\n        \"\"\"\n        old_world_size = cfg.SOLVER.REFERENCE_WORLD_SIZE\n        if old_world_size == 0 or old_world_size == num_workers:\n            return cfg\n        cfg = cfg.clone()\n        frozen = cfg.is_frozen()\n        cfg.defrost()\n\n        assert (\n            cfg.SOLVER.IMS_PER_BATCH % old_world_size == 0\n        ), \"Invalid REFERENCE_WORLD_SIZE in config!\"\n        scale = num_workers / old_world_size\n        bs = cfg.SOLVER.IMS_PER_BATCH = int(round(cfg.SOLVER.IMS_PER_BATCH * scale))\n        lr = cfg.SOLVER.BASE_LR = cfg.SOLVER.BASE_LR * scale\n        max_iter = cfg.SOLVER.MAX_ITER = int(round(cfg.SOLVER.MAX_ITER / scale))\n        warmup_iter = cfg.SOLVER.WARMUP_ITERS = int(round(cfg.SOLVER.WARMUP_ITERS / scale))\n        cfg.SOLVER.STEPS = tuple(int(round(s / scale)) for s in cfg.SOLVER.STEPS)\n        cfg.TEST.EVAL_PERIOD = int(round(cfg.TEST.EVAL_PERIOD / scale))\n        cfg.SOLVER.CHECKPOINT_PERIOD = int(round(cfg.SOLVER.CHECKPOINT_PERIOD / scale))\n        cfg.SOLVER.REFERENCE_WORLD_SIZE = num_workers  # maintain invariant\n        logger = logging.getLogger(__name__)\n        logger.info(\n            f\"Auto-scaling the config to batch_size={bs}, learning_rate={lr}, \"\n            f\"max_iter={max_iter}, warmup={warmup_iter}.\"\n        )\n\n        if frozen:\n            cfg.freeze()\n        return cfg\n\n\n# Access basic attributes from the underlying trainer\nfor _attr in [\"model\", \"data_loader\", \"optimizer\"]:\n    setattr(\n        MyTrainer,\n        _attr,\n        property(\n            # getter\n            lambda self, x=_attr: getattr(self._trainer, x),\n            # setter\n            lambda self, value, x=_attr: setattr(self._trainer, x, value),\n        ),\n    )\n"}
{"type": "source_file", "path": "utils/ditod/table_evaluation/evaluate.py", "content": "\"\"\"\nEvaluation of -.tar.gz file.\nYu Fang - March 2019\n\"\"\"\n\nimport os\nimport xml.dom.minidom\n\n# from eval import eval\n\nreg_gt_path = os.path.abspath(\"data/test\")\nreg_gt_path_archival = os.path.abspath(\"data/test\")\nreg_gt_path_modern = os.path.abspath(\"data/test\")\nstr_gt_path_1 = os.path.abspath(\"data/test\")\nstr_gt_path_2 = os.path.abspath(\"data/test\")\nstr_gt_path_archival = os.path.abspath(\"data/test\")\nstr_gt_path_modern = os.path.abspath(\"data/test\")\n\nimport xml.dom.minidom\n# from functools import cmp_to_key\nfrom os.path import join as osj\nfrom .data_structure import *\n\n\nclass eval:\n    STR = \"-str\"\n    REG = \"-reg\"\n    DEFAULT_ENCODING = \"UTF-8\"\n    # reg_gt_path = \"./annotations/trackA/\"\n    # str_gt_path = \"./annotations/trackB/\"\n    # reg_gt_path = os.path.abspath(\"data/test\")\n    # reg_gt_path_archival = os.path.abspath(\"data/test\")\n    # reg_gt_path_modern = os.path.abspath(\"data/test\")\n    # str_gt_path_1 = os.path.abspath(\"data/test\")\n    # str_gt_path_2 = os.path.abspath(\"data/test\")\n    # str_gt_path_archival = os.path.abspath(\"data/test\")\n    # str_gt_path_modern = os.path.abspath(\"data/test\")\n\n    # dummyDom = xml.dom.minidom.parse(\"./dummyXML.xml\")\n\n    def __init__(self, track, res_path):\n        self.return_result = None\n        self.reg = True\n        self.str = False\n\n        self.resultFile = res_path\n        self.inPrefix = os.path.split(res_path)[-1].split(\".\")[0][:-7]\n\n        if track == \"-trackA\":\n            self.reg = True\n            self.GTFile = osj(reg_gt_path, self.inPrefix + \".xml\")\n            # self.GTFile = osj(self.reg_gt_path, self.inPrefix)\n        elif track == \"-trackA1\":  # archival documents\n            self.reg = True\n            self.GTFile = osj(reg_gt_path_archival, self.inPrefix + \".xml\")\n        elif track == \"-trackA2\":  # modern documents\n            self.reg = True\n            self.GTFile = osj(reg_gt_path_modern, self.inPrefix + \".xml\")\n        elif track == \"-trackB1\":\n            self.str = True\n            self.GTFile = osj(str_gt_path_1, self.inPrefix + \".xml\")\n            # self.GTFile = osj(self.str_gt_path_1, self.inPrefix)\n        elif track == \"-trackB2\":\n            self.str = True\n            self.GTFile = osj(str_gt_path_2, self.inPrefix + \".xml\")\n            # print(self.GTFile)\n            # self.GTFile = osj(self.str_gt_path_2, self.inPrefix)\n        elif track == \"-trackB2_a\":\n            self.str = True\n            self.GTFile = osj(str_gt_path_archival, self.inPrefix + \".xml\")\n        elif track == \"-trackB2_m\":\n            self.str = True\n            self.GTFile = osj(str_gt_path_modern, self.inPrefix + \".xml\")\n        else:\n            print(track)\n            print(\"Not a valid track, please check your spelling.\")\n\n        # self.resultFile = res_path\n        # self.inPrefix = os.path.split(res_path)[-1].split(\"-\")[0]\n\n        # if self.str:\n        #     # self.GTFile = osj(self.str_gt_path, self.inPrefix + \"-str.xml\")\n        #     self.GTFile = osj(self.str_gt_path, self.inPrefix + \".xml\")\n        # elif self.reg:\n        #     # self.GTFile = osj(self.reg_gt_path, self.inPrefix + \"-reg.xml\")\n        #     self.GTFile = osj(self.reg_gt_path, self.inPrefix + \".xml\")\n        # else:\n        #     print(\"Not a valid track, please check your spelling.\")\n\n        self.gene_ret_lst()\n\n    @property\n    def result(self):\n        return self.return_result\n\n    def gene_ret_lst(self):\n        ret_lst = []\n        for iou in [0.6, 0.7, 0.8, 0.9]:\n            temp = self.compute_retVal(iou)\n            ret_lst.append(temp)\n            # ret_lst.append(self.compute_retVal(iou))\n\n        ret_lst.append(self.inPrefix + \".xml\")\n        # ret_lst.append(self.inPrefix)\n        # print(\"Done processing {}\\n\".format(self.resultFile))\n        self.return_result = ret_lst\n\n    def compute_retVal(self, iou):\n        gt_dom = xml.dom.minidom.parse(self.GTFile)\n        # incorrect submission format handling\n        try:\n            result_dom = xml.dom.minidom.parse(self.resultFile)\n        except Exception as e:\n            # result_dom = xml.dom.minidom.parse(dummyDom)\n            gt_tables = eval.get_table_list(gt_dom)\n            retVal = ResultStructure(truePos=0, gtTotal=len(gt_tables), resTotal=0)\n            return retVal\n\n        # result_dom = xml.dom.minidom.parse(self.resultFile)\n        if self.reg:\n            ret = self.evaluate_result_reg(gt_dom, result_dom, iou)\n            return ret\n        if self.str:\n            ret = self.evaluate_result_str(gt_dom, result_dom, iou)\n            return ret\n\n    @staticmethod\n    def get_table_list(dom):\n        \"\"\"\n        return a list of Table objects corresponding to the table element of the DOM.\n        \"\"\"\n        return [Table(_nd) for _nd in dom.documentElement.getElementsByTagName(\"table\")]\n\n    @staticmethod\n    def evaluate_result_reg(gt_dom, result_dom, iou_value):\n        # parse the tables in input elements\n        gt_tables = eval.get_table_list(gt_dom)\n        result_tables = eval.get_table_list(result_dom)\n        # duplicate result table list\n        remaining_tables = result_tables.copy()\n\n        # map the tables in gt and result file\n        table_matches = []  # @param: table_matches - list of mapping of tables in gt and res file, in order (gt, res)\n        for gtt in gt_tables:\n            for rest in remaining_tables:\n                if gtt.compute_table_iou(rest) >= iou_value:\n                    remaining_tables.remove(rest)\n                    table_matches.append((gtt, rest))\n                    break\n\n        assert len(table_matches) <= len(gt_tables)\n        assert len(table_matches) <= len(result_tables)\n\n        retVal = ResultStructure(truePos=len(table_matches), gtTotal=len(gt_tables), resTotal=len(result_tables))\n        return retVal\n\n    @staticmethod\n    def evaluate_result_str(gt_dom, result_dom, iou_value, table_iou_value=0.8):\n        # parse the tables in input elements\n        gt_tables = eval.get_table_list(gt_dom)\n        result_tables = eval.get_table_list(result_dom)\n\n        # duplicate result table list\n        remaining_tables = result_tables.copy()\n        gt_remaining = gt_tables.copy()\n\n        # map the tables in gt and result file\n        table_matches = []  # @param: table_matches - list of mapping of tables in gt and res file, in order (gt, res)\n        for gtt in gt_remaining:\n            for rest in remaining_tables:\n                # note: for structural analysis, use 0.8 for table mapping\n                if gtt.compute_table_iou(rest) >= table_iou_value:\n                    table_matches.append((gtt, rest))\n                    remaining_tables.remove(rest)  # unsafe... should be ok with the break below\n                    gt_remaining.remove(gtt)\n                    break\n\n        total_gt_relation, total_res_relation, total_correct_relation = 0, 0, 0\n        for gt_table, ress_table in table_matches:\n\n            # set up the cell mapping for matching tables\n            cell_mapping = gt_table.find_cell_mapping(ress_table, iou_value)\n            # set up the adj relations, convert the one for result table to a dictionary for faster searching\n            gt_AR = gt_table.find_adj_relations()\n            total_gt_relation += len(gt_AR)\n\n            res_AR = ress_table.find_adj_relations()\n            total_res_relation += len(res_AR)\n\n            if False:  # for DEBUG\n                Table.printCellMapping(cell_mapping)\n                Table.printAdjacencyRelationList(gt_AR, \"GT\")\n                Table.printAdjacencyRelationList(res_AR, \"run\")\n\n            # Now map GT adjacency relations to result\n            lMappedAR = []\n            for ar in gt_AR:\n                try:\n                    resFromCell = cell_mapping[ar.fromText]\n                    resToCell = cell_mapping[ar.toText]\n                    # make a mapped adjacency relation\n                    lMappedAR.append(AdjRelation(resFromCell, resToCell, ar.direction))\n                except:\n                    # no mapping is possible\n                    pass\n\n            # compare two list of adjacency relation\n            correct_dect = 0\n            for ar1 in res_AR:\n                for ar2 in lMappedAR:\n                    if ar1.isEqual(ar2):\n                        correct_dect += 1\n                        break\n\n            total_correct_relation += correct_dect\n\n        # handle gt_relations in unmatched gt table\n        for gtt_remain in gt_remaining:\n            total_gt_relation += len(gtt_remain.find_adj_relations())\n\n        # handle gt_relation in unmatched res table\n        for res_remain in remaining_tables:\n            total_res_relation += len(res_remain.find_adj_relations())\n\n        retVal = ResultStructure(truePos=total_correct_relation, gtTotal=total_gt_relation, resTotal=total_res_relation)\n        return retVal\n\n# calculate the gt adj_relations of the missing file\n# @param: file_lst - list of missing ground truth file\n# @param: cur_gt_num - current total of ground truth objects (tables / cells)\ndef process_missing_files(track, gt_file_lst, cur_gt_num):\n    if track in [\"-trackA\", \"-trackA1\", \"-trackA2\"]:\n        gt_file_lst_full = [osj(reg_gt_path, filename) for filename in gt_file_lst]\n        for file in gt_file_lst_full:\n            if os.path.split(file)[-1].split(\".\")[-1] == \"xml\":\n                gt_dom = xml.dom.minidom.parse(file)\n                gt_root = gt_dom.documentElement\n                # tables = []\n                table_elements = gt_root.getElementsByTagName(\"table\")\n                for res_table in table_elements:\n                    # t = Table(res_table)\n                    # tables.append(t)\n                    cur_gt_num += 1\n        return cur_gt_num\n    elif track == \"-trackB1\":\n        gt_file_lst_full = [osj(str_gt_path_1, filename) for filename in gt_file_lst]\n        for file in gt_file_lst_full:\n            if os.path.split(file)[-1].split(\".\")[-1] == \"xml\":\n                gt_dom = xml.dom.minidom.parse(file)\n                gt_root = gt_dom.documentElement\n                tables = []\n                table_elements = gt_root.getElementsByTagName(\"table\")\n                for res_table in table_elements:\n                    t = Table(res_table)\n                    tables.append(t)\n                for table in tables:\n                    cur_gt_num += len(table.find_adj_relations())\n        return cur_gt_num\n    elif track == \"-trackB2\":\n        gt_file_lst_full = [osj(str_gt_path_2, filename) for filename in gt_file_lst]\n        for file in gt_file_lst_full:\n            if os.path.split(file)[-1].split(\".\")[-1] == \"xml\":\n                gt_dom = xml.dom.minidom.parse(file)\n                gt_root = gt_dom.documentElement\n                tables = []\n                table_elements = gt_root.getElementsByTagName(\"table\")\n                for res_table in table_elements:\n                    t = Table(res_table)\n                    tables.append(t)\n                for table in tables:\n                    cur_gt_num += len(table.find_adj_relations())\n        return cur_gt_num\n\ndef calc(F1):\n    sum_a = 0.6 * F1[0] + 0.7 * F1[1] + 0.8 * F1[2] + 0.9 * F1[3]\n    sum_b = 0.6 + 0.7 + 0.8 + 0.9\n\n    return sum_a / sum_b\n\ndef calc_table_score(result_path):\n    # measure = eval(*sys.argv[1:])\n\n    gt_file_lst = os.listdir(reg_gt_path_archival)\n    track = \"-trackA1\"\n    untar_path = result_path\n\n    res_lst = []\n    for root, files, dirs in os.walk(untar_path):\n        for name in dirs:\n            if name.split(\".\")[-1] == \"xml\":\n                cur_filepath = osj(os.path.abspath(root), name)\n                res_lst.append(eval(track, cur_filepath))\n                # printing for debug\n                # print(\"Processing... {}\".format(name))\n    # print(\"DONE WITH FILE PROCESSING\\n\")\n    # note: results are stored as list of each when iou at [0.6, 0.7, 0.8, 0.9, gt_filename]\n    # gt number should be the same for all files\n    gt_num = 0\n    correct_six, res_six = 0, 0\n    correct_seven, res_seven = 0, 0\n    correct_eight, res_eight = 0, 0\n    correct_nine, res_nine = 0, 0\n\n\n    for each_file in res_lst:\n        # print(each_file)\n        try:\n            gt_file_lst.remove(each_file.result[-1])\n            if each_file.result[-1].replace('.xml', '.jpg') in gt_file_lst:\n                gt_file_lst.remove(each_file.result[-1].replace('.xml', '.jpg'))\n            correct_six += each_file.result[0].truePos\n            gt_num += each_file.result[0].gtTotal\n            res_six += each_file.result[0].resTotal\n            # print(\"{} {} {}\".format(each_file.result[0].truePos, each_file.result[0].gtTotal, each_file.result[0].resTotal))\n\n            correct_seven += each_file.result[1].truePos\n            res_seven += each_file.result[1].resTotal\n\n            correct_eight += each_file.result[2].truePos\n            res_eight += each_file.result[2].resTotal\n\n            correct_nine += each_file.result[3].truePos\n            res_nine += each_file.result[3].resTotal\n        except:\n            print(\"Error occur in processing result list.\")\n            print(each_file.result[-1])\n            break\n            # print(each_file.result[-1])\n            # print(each_file)\n\n    # for file in gt_file_lst:\n    #     if file.split(\".\") != \"xml\":\n    #         gt_file_lst.remove(file)\n    #     # print(gt_file_lst)\n\n    for i in range(len(gt_file_lst) - 1, -1, -1):\n        if gt_file_lst[i].split(\".\")[-1] != \"xml\":\n            del gt_file_lst[i]\n\n    if len(gt_file_lst) > 0:\n        print(\"\\nWarning: missing result annotations for file: {}\\n\".format(gt_file_lst))\n        gt_total = process_missing_files(track, gt_file_lst, gt_num)\n    else:\n        gt_total = gt_num\n\n\n    try:\n        # print(\"Evaluation of {}\".format(track.replace(\"-\", \"\")))\n        # iou @ 0.6\n        p_six = correct_six / res_six\n        r_six = correct_six / gt_total\n        f1_six = 2 * p_six * r_six / (p_six + r_six)\n        print(\"IOU @ 0.6 -\\nprecision: {}\\nrecall: {}\\nf1: {}\".format(p_six, r_six, f1_six))\n        print(\"correct: {}, gt: {}, res: {}\\n\".format(correct_six, gt_total, res_six))\n\n        # iou @ 0.7\n        p_seven = correct_seven / res_seven\n        r_seven = correct_seven / gt_total\n        f1_seven = 2 * p_seven * r_seven / (p_seven + r_seven)\n        print(\"IOU @ 0.7 -\\nprecision: {}\\nrecall: {}\\nf1: {}\".format(p_seven, r_seven, f1_seven))\n        print(\"correct: {}, gt: {}, res: {}\\n\".format(correct_seven, gt_total, res_seven))\n\n        # iou @ 0.8\n        p_eight = correct_eight / res_eight\n        r_eight = correct_eight / gt_total\n        f1_eight = 2 * p_eight * r_eight / (p_eight + r_eight)\n        print(\"IOU @ 0.8 -\\nprecision: {}\\nrecall: {}\\nf1: {}\".format(p_eight, r_eight, f1_eight))\n        print(\"correct: {}, gt: {}, res: {}\\n\".format(correct_eight, gt_total, res_eight))\n\n        # iou @ 0.9\n        p_nine = correct_nine / res_nine\n        r_nine = correct_nine / gt_total\n        f1_nine = 2 * p_nine * r_nine / (p_nine + r_nine)\n        print(\"IOU @ 0.9 -\\nprecision: {}\\nrecall: {}\\nf1: {}\".format(p_nine, r_nine, f1_nine))\n        print(\"correct: {}, gt: {}, res: {}\".format(correct_nine, gt_total, res_nine))\n\n        F1 = [f1_six, f1_seven, f1_eight, f1_nine]\n        wF1 = calc(F1)\n\n        print(\"Average weight F1: {}\".format(wF1))\n\n        return {\n            'p_six':p_six * 100,\n            \"r_six\":r_six * 100,\n            \"f1_six\":f1_six * 100,\n            \"p_seven\":p_seven * 100,\n            \"r_seven\":r_seven * 100,\n            \"f1_seven\":f1_seven * 100,\n            \"p_eight\":p_eight * 100,\n            \"r_eight\":r_eight * 100,\n            \"f1_eight\":f1_eight * 100,\n            \"p_nine\":p_nine * 100,\n            \"r_nine\":r_nine * 100,\n            \"f1_nine\":f1_nine * 100,\n            \"wF1\":wF1 * 100\n        }\n    except ZeroDivisionError:\n        print(\n            \"Error: zero devision error found, (possible that no adjacency relations are found), please check the file input.\")\n        return {\"wF1\": 0}\n\n\nif __name__==\"__main__\":\n    pass\n"}
{"type": "source_file", "path": "utils/ditod_vgt/FeatureMerge.py", "content": "import torch\nfrom torch import nn\n\nclass FeatureMerge(nn.Module):\n    \"\"\"Multimodal feature fusion used in VSR.\"\"\"\n    def __init__(self,\n                 feature_names,\n                 visual_dim,\n                 semantic_dim,\n                 merge_type='Sum',\n                 dropout_ratio=0.1,\n                 with_extra_fc=True,\n                 shortcut=False\n                 ):\n        \"\"\"Multimodal feature merge used in VSR.\n        Args:\n            visual_dim (list): the dim of visual features, e.g. [256]\n            semantic_dim (list): the dim of semantic features, e.g. [256]\n            merge_type (str): fusion type, e.g. 'Sum', 'Concat', 'Weighted'\n            dropout_ratio (float): dropout ratio of fusion features\n            with_extra_fc (bool): whether add extra fc layers for adaptation\n            shortcut (bool): whether add shortcut connection\n        \"\"\"\n        super().__init__()\n\n        # merge param\n        self.feature_names = feature_names\n        self.merge_type = merge_type\n        self.visual_dim = visual_dim\n        self.textual_dim = semantic_dim\n        self.with_extra_fc = with_extra_fc\n        self.shortcut = shortcut\n        self.relu = nn.ReLU(inplace=True)\n        \n        if self.merge_type == 'Sum':\n            assert len(self.visual_dim) == len(self.textual_dim)\n        elif self.merge_type == 'Concat':\n            assert len(self.visual_dim) == len(self.textual_dim)\n            # self.concat_proj = nn.ModuleList()\n            \n            self.vis_proj = nn.ModuleList()\n            self.text_proj = nn.ModuleList()\n            self.alpha_proj = nn.ModuleList()\n            \n            for idx in range(len(self.visual_dim)):\n                # self.concat_proj.append(nn.Conv2d(self.visual_dim[idx] + self.textual_dim[idx], self.visual_dim[idx], kernel_size = (1,1), stride=1))\n                if self.with_extra_fc:\n                    self.vis_proj.append(nn.Linear(self.visual_dim[idx], self.visual_dim[idx]))\n                    self.text_proj.append(nn.Linear(self.textual_dim[idx], self.textual_dim[idx]))\n                self.alpha_proj.append(nn.Linear(self.visual_dim[idx] + self.textual_dim[idx], self.visual_dim[idx]))\n            \n        elif self.merge_type == 'Weighted':\n            assert len(self.visual_dim) == len(self.textual_dim)\n            self.total_num = len(self.visual_dim)\n\n            # vis projection\n            self.vis_proj = nn.ModuleList()\n            self.vis_proj_relu = nn.ModuleList()\n\n            # text projection\n            self.text_proj = nn.ModuleList()\n            self.text_proj_relu = nn.ModuleList()\n\n            self.alpha_proj = nn.ModuleList()\n            for idx in range(self.total_num):\n                if self.with_extra_fc:\n                    self.vis_proj.append(nn.Linear(self.visual_dim[idx], self.visual_dim[idx]))\n                    self.text_proj.append(nn.Linear(self.textual_dim[idx], self.textual_dim[idx]))\n                self.alpha_proj.append(nn.Linear(self.visual_dim[idx] + self.textual_dim[idx], self.visual_dim[idx]))\n\n        else:\n            raise \"Unknown merge type {}\".format(self.merge_type)\n\n        self.dropout = nn.Dropout(dropout_ratio)\n\n        # visual context\n        # self.visual_ap = nn.AdaptiveAvgPool2d((1, 1))\n\n\n    def forward(self, visual_feat=None, textual_feat=None):\n        \"\"\" Forward computation\n        Args:\n            visual_feat (list(Tensor)): visual feature maps, in shape of [L x C x H x W] x B\n            textual_feat (Tensor): textual feature maps, in shape of B x L x C\n        Returns:\n            Tensor: fused feature maps, in shape of [B x L x C]\n        \"\"\"\n        assert len(visual_feat) == len(textual_feat)\n\n        # feature merge\n        merged_feat = {}\n        if self.merge_type == 'Sum':\n            for name in self.feature_names:\n                merged_feat[name] = visual_feat[name] + textual_feat[name]\n        elif self.merge_type == 'Concat':\n            for idx, name in enumerate(self.feature_names):\n                # merged_feat[name] = self.concat_proj[idx](torch.cat((visual_feat[name],textual_feat[name]),1))\n                per_vis = visual_feat[name].permute(0, 2, 3, 1)\n                per_text = textual_feat[name].permute(0, 2, 3, 1)\n                if self.with_extra_fc:\n                    per_vis = self.relu(self.vis_proj[idx](per_vis))\n                    per_text = self.relu(self.text_proj[idx](per_text))\n                x_sentence = self.alpha_proj[idx](torch.cat((per_vis, per_text), -1))\n                x_sentence = x_sentence.permute(0,3,1,2).contiguous()\n                merged_feat[name] = x_sentence\n        else:\n            assert self.total_num == len(visual_feat) or self.total_num == 1\n            # for per_vis, per_text in zip(visual_feat, textual_feat):\n            for idx, name in enumerate(self.feature_names):\n                per_vis = visual_feat[name].permute(0, 2, 3, 1)\n                per_text = textual_feat[name].permute(0, 2, 3, 1)\n                if self.with_extra_fc:\n                    per_vis = self.relu(self.vis_proj[idx](per_vis))\n                    per_text = self.relu(self.text_proj[idx](per_text))\n\n                alpha = torch.sigmoid(self.alpha_proj[idx](torch.cat((per_vis, per_text), -1)))\n                if self.shortcut:\n                    # shortcut\n                    x_sentence = per_vis + alpha * per_text\n                else:\n                    # selection\n                    x_sentence = alpha * per_vis + (1 - alpha) * per_text\n\n                x_sentence = x_sentence.permute(0,3,1,2).contiguous()\n                merged_feat[name] = x_sentence\n\n        return merged_feat"}
{"type": "source_file", "path": "utils/ditod/table_evaluation/data_structure.py", "content": "\"\"\"\nData structures used by the evaluation process.\nYu Fang - March 2019\n\"\"\"\n\nfrom collections.abc import Iterable\n\nimport numpy as np\nfrom shapely.geometry import Polygon\n\n\n# helper functions\ndef flatten(lis):\n    for item in lis:\n        if isinstance(item, Iterable) and not isinstance(item, str):\n            for x in flatten(item):\n                yield x\n        else:\n            yield item\n\n# derived from https://blog.csdn.net/u012433049/article/details/82909484\ndef compute_poly_iou(list1, list2):\n    a1 = np.array(list1, dtype=int).reshape(-1, 2)\n    poly1 = Polygon(a1)\n    poly1_clean = poly1.buffer(0)\n\n    a2 = np.array(list2, dtype=int).reshape(-1, 2)\n    poly2 = Polygon(a2)\n    poly2_clean = poly2.buffer(0)\n\n    try:\n        # iou = poly1.intersection(poly2).area / poly1.union(poly2).area\n        iou = poly1_clean.intersection(poly2_clean).area / poly1_clean.union(poly2_clean).area\n    except ZeroDivisionError:\n        iou = 0\n    return iou\n\n\nclass Cell(object):\n    # @:param start_row : start row index of the Cell\n    # @:param start_col : start column index of the Cell\n    # @:param end-row : end row index of the Cell\n    # @:param end-col : end column index of the Cell\n    # @:param cell_box: bounding-box of the Cell (coordinates are saved as a string)\n    # @:param content_box: bounding-box of the text content within Cell (unused variable)\n    # @:param cell_id: unique id of the Cell\n\n    def __init__(self, table_id, start_row, start_col, cell_box, end_row, end_col, content_box=\"\"):\n        self._start_row = int(start_row)\n        self._start_col = int(start_col)\n        self._cell_box = cell_box\n        self._content_box = content_box\n        self._table_id = table_id    # the table_id this cell belongs to\n        # self._cell_name = cell_id    # specify the cell using passed-in cell_id\n        self._cell_id = id(self)\n        # self._region = region\n\n        # check for end-row and end-col special case\n        if end_row == -1:\n            self._end_row = self.start_row\n        else:\n            self._end_row = int(end_row)\n        if end_col == -1:\n            self._end_col = self._start_col\n        else:\n            self._end_col = int(end_col)\n\n    @property\n    def start_row(self):\n        return self._start_row\n\n    @property\n    def start_col(self):\n        return self._start_col\n\n    @property\n    def end_row(self):\n        return self._end_row\n\n    @property\n    def end_col(self):\n        return self._end_col\n\n    @property\n    def cell_box(self):\n        return self._cell_box\n\n    @property\n    def content_box(self):\n        return self._content_box\n\n    @property\n    def cell_id(self):\n        return self._cell_id\n\n    @property\n    def table_id(self):\n        return self._table_id\n\n    def __str__(self):\n        return \"CELL row=[%d, %d] col=[%d, %d] (coords=%s)\" %(self.start_row, self.end_row\n                                                              , self.start_col, self.end_col\n                                                              , self.cell_box)\n\n    # return the IoU value of two cell blocks\n    def compute_cell_iou(self, another_cell):\n        cell_box_1_temp = []\n        for el in self.cell_box.split():\n            cell_box_1_temp.append((el.split(\",\")))\n        cell_box_1 = list(flatten(cell_box_1_temp))\n        cell_box_1 = [int(x) for x in cell_box_1]\n\n        cell_box_2_temp = []\n        for el in another_cell.cell_box.split():\n            cell_box_2_temp.append((el.split(\",\")))\n        cell_box_2 = list(flatten(cell_box_2_temp))\n        cell_box_2 = [int(x) for x in cell_box_2]\n\n        return compute_poly_iou(cell_box_1, cell_box_2)\n\n    # check if the two cell object denotes same cell area in table\n    def check_same(self, another_cell):\n        return self._start_row == another_cell.start_row and self._end_row == another_cell.end_row and \\\n               self._start_col == another_cell.start_col and self._end_col == another_cell.end_col\n\n\n# Note: currently save the relation with two cell object involved,\n# can be replaced by cell_id in follow-up memory clean up\nclass AdjRelation:\n\n    DIR_HORIZ = 1\n    DIR_VERT = 2\n\n    def __init__(self, fromText, toText, direction):\n        # @param: fromText, toText are Cell objects may be changed to cell-ID for further development\n        self._fromText = fromText\n        self._toText = toText\n        self._direction = direction\n\n    @property\n    def fromText(self):\n        return self._fromText\n\n    @property\n    def toText(self):\n        return self._toText\n\n    @property\n    def direction(self):\n        return self._direction\n\n    def __str__(self):\n        if self.direction == self.DIR_VERT:\n            dir = \"vertical\"\n        else:\n            dir = \"horizontal\"\n        return 'ADJ_RELATION: ' + str(self._fromText) + '  ' + str(self._toText) + '    ' + dir\n\n    def isEqual(self, otherRelation):\n        return self.fromText.cell_id == otherRelation.fromText.cell_id and \\\n               self.toText.cell_id == otherRelation.toText.cell_id and self.direction == otherRelation.direction\n\n\nclass Table:\n\n    def __init__(self, tableNode):\n        self._root = tableNode\n        self._id = id(self)\n        self._table_coords = \"\"\n        self._maxRow = 0    # PS: indexing from 0\n        self._maxCol = 0\n        self._cells = []    # save a table as list of <Cell>s\n        self.adj_relations = []    # save the adj_relations for the table\n        self.parsed = False\n        self.found = False    # check if the find_adj_relations() has been called once\n\n        self.parse_table()\n\n    def __str__(self):\n        return \"TABLE object - {} row x {} col\".format(self._maxRow+1, self._maxCol+1)\n\n    @property\n    def id(self):\n        return self._id\n\n    @property\n    def table_coords(self):\n        return self._table_coords\n\n    @property\n    def table_cells(self):\n        return self._cells\n\n    # parse input xml to cell lists\n    def parse_table(self):\n        # get the table bbox\n        self._table_coords = str(self._root.getElementsByTagName(\"Coords\")[0].getAttribute(\"points\"))\n\n        # get info for each cell\n        cells = self._root.getElementsByTagName(\"cell\")\n        max_row = max_col = 0\n        for cell in cells:\n            sr = cell.getAttribute(\"start-row\")\n            sc = cell.getAttribute(\"start-col\")\n            cell_id = cell.getAttribute(\"id\")\n            b_points = str(cell.getElementsByTagName(\"Coords\")[0].getAttribute(\"points\"))\n            # try:\n            #     try:\n            #         text = cell.getElementsByTagName(\"content\")[0].firstChild.nodeValue\n            #     except AttributeError:\n            #         text = \"\"\n            # except IndexError:\n            #     text = \"initialized cell as no content\"\n            er = cell.getAttribute(\"end-row\") if cell.hasAttribute(\"end-row\") else -1\n            ec = cell.getAttribute(\"end-col\") if cell.hasAttribute(\"end-col\") else -1\n            new_cell = Cell(table_id=str(self.id), start_row=sr, start_col=sc, cell_box=b_points,\n                            end_row=er, end_col=ec)\n            max_row = max(max_row, int(sr), int(er))\n            max_col = max(max_col, int(sc), int(ec))\n            self._cells.append(new_cell)\n        self._maxCol = max_col\n        self._maxRow = max_row\n        self.parsed = True\n\n    # generate a table-like structure for finding adj_relations\n    def convert_2d(self):\n        table = [[0 for x in range(self._maxCol+1)] for y in range(self._maxRow+1)]    # init blank cell with int 0\n        for cell in self._cells:\n            cur_row = cell.start_row\n            while cur_row <= cell.end_row:\n                cur_col = cell.start_col\n                while cur_col <= cell.end_col:\n                    temp = table[cur_row][cur_col]\n                    if temp == 0:\n                        table[cur_row][cur_col] = cell\n                    elif type(temp) == list:\n                        temp.append(cell)\n                        table[cur_row][cur_col] = temp\n                    else:\n                        table[cur_row][cur_col] = [temp, cell]\n                    cur_col += 1\n                cur_row += 1\n\n        return table\n\n    def find_adj_relations(self):\n        if self.found:\n            return self.adj_relations\n        else:\n            # if len(self._cells) == 0:\n            if self.parsed == False:\n                # fix: cases where there's no cell in table?\n                print(\"table is not parsed for further steps.\")\n                self.parse_table()\n                self.find_adj_relations()\n            else:\n                retVal = []\n                tab = self.convert_2d()\n\n                # find horizontal relations\n                for r in range(self._maxRow+1):\n                    for c_from in range(self._maxCol):\n                        temp_pos = tab[r][c_from]\n                        if temp_pos == 0:\n                            continue\n                        elif type(temp_pos) == list:\n                            for cell in temp_pos:\n                                c_to = c_from + 1\n                                if tab[r][c_to] != 0:\n                                    # find relation between two adjacent cells\n                                    if type(tab[r][c_to]) == list:\n                                        for cell_to in tab[r][c_to]:\n                                            if cell != cell_to and (not cell.check_same(cell_to)):\n                                                adj_relation = AdjRelation(cell, cell_to, AdjRelation.DIR_HORIZ)\n                                                retVal.append(adj_relation)\n                                    else:\n                                        if cell != tab[r][c_to]:\n                                            adj_relation = AdjRelation(cell, tab[r][c_to], AdjRelation.DIR_HORIZ)\n                                            retVal.append(adj_relation)\n                                else:\n                                    # find the next non-blank cell, if exists\n                                    for temp in range(c_from + 1, self._maxCol + 1):\n                                        if tab[r][temp] != 0:\n                                            if type(tab[r][temp]) == list:\n                                                for cell_to in tab[r][temp]:\n                                                    adj_relation = AdjRelation(cell, cell_to,\n                                                                               AdjRelation.DIR_HORIZ)\n                                                    retVal.append(adj_relation)\n                                            else:\n                                                adj_relation = AdjRelation(cell, tab[r][temp],\n                                                                           AdjRelation.DIR_HORIZ)\n                                                retVal.append(adj_relation)\n                                            break\n                        else:\n                            c_to = c_from + 1\n                            if tab[r][c_to] != 0:\n                                # find relation between two adjacent cells\n                                if type(tab[r][c_to]) == list:\n                                    for cell_to in tab[r][c_to]:\n                                        if temp_pos != cell_to:\n                                            adj_relation = AdjRelation(temp_pos, cell_to, AdjRelation.DIR_HORIZ)\n                                            retVal.append(adj_relation)\n                                else:\n                                    if temp_pos != tab[r][c_to]:\n                                        adj_relation = AdjRelation(temp_pos, tab[r][c_to], AdjRelation.DIR_HORIZ)\n                                        retVal.append(adj_relation)\n                            else:\n                                # find the next non-blank cell, if exists\n                                for temp in range(c_from + 1, self._maxCol + 1):\n                                    if tab[r][temp] != 0:\n                                        if type(tab[r][temp]) == list:\n                                            for cell_to in tab[r][temp]:\n                                                adj_relation = AdjRelation(temp_pos, cell_to,\n                                                                           AdjRelation.DIR_HORIZ)\n                                                retVal.append(adj_relation)\n                                        else:\n                                            adj_relation = AdjRelation(temp_pos, tab[r][temp], AdjRelation.DIR_HORIZ)\n                                            retVal.append(adj_relation)\n                                        break\n\n                # find vertical relations\n                for c in range(self._maxCol+1):\n                    for r_from in range(self._maxRow):\n                        temp_pos = tab[r_from][c]\n                        if temp_pos == 0:\n                            continue\n                        elif type(temp_pos) == list:\n                            for cell in temp_pos:\n                                r_to = r_from + 1\n                                if tab[r_to][c] != 0:\n                                    # find relation between two adjacent cells\n                                    if type(tab[r_to][c]) == list:\n                                        for cell_to in tab[r_to][c]:\n                                            if cell != cell_to and (not cell.check_same(cell_to)):\n                                                adj_relation = AdjRelation(cell, cell_to, AdjRelation.DIR_VERT)\n                                                retVal.append(adj_relation)\n                                    else:\n                                        if cell != tab[r_to][c]:\n                                            adj_relation = AdjRelation(cell, tab[r_to][c], AdjRelation.DIR_VERT)\n                                            retVal.append(adj_relation)\n                                else:\n                                    # find the next non-blank cell, if exists\n                                    for temp in range(r_from + 1, self._maxRow + 1):\n                                        if tab[temp][c] != 0:\n                                            if type(tab[temp][c]) == list:\n                                                for cell_to in tab[temp][c]:\n                                                    adj_relation = AdjRelation(cell, cell_to,\n                                                                               AdjRelation.DIR_VERT)\n                                                    retVal.append(adj_relation)\n                                            else:\n                                                adj_relation = AdjRelation(cell, tab[temp][c],\n                                                                           AdjRelation.DIR_VERT)\n                                                retVal.append(adj_relation)\n                                            break\n                        else:\n                            r_to = r_from + 1\n                            if tab[r_to][c] != 0:\n                                # find relation between two adjacent cells\n                                if type(tab[r_to][c]) == list:\n                                    for cell_to in tab[r_to][c]:\n                                        if temp_pos != cell_to:\n                                            adj_relation = AdjRelation(temp_pos, cell_to, AdjRelation.DIR_VERT)\n                                            retVal.append(adj_relation)\n                                else:\n                                    if temp_pos != tab[r_to][c]:\n                                        adj_relation = AdjRelation(temp_pos, tab[r_to][c], AdjRelation.DIR_VERT)\n                                        retVal.append(adj_relation)\n                            else:\n                                # find the next non-blank cell, if exists\n                                for temp in range(r_from + 1, self._maxRow + 1):\n                                    if tab[temp][c] != 0:\n                                        if type(tab[temp][c]) == list:\n                                            for cell_to in tab[temp][c]:\n                                                adj_relation = AdjRelation(temp_pos, cell_to, AdjRelation.DIR_VERT)\n                                                retVal.append(adj_relation)\n                                        else:\n                                            adj_relation = AdjRelation(temp_pos, tab[temp][c], AdjRelation.DIR_VERT)\n                                            retVal.append(adj_relation)\n                                        break\n\n                # eliminate duplicates\n                repeat = True\n                while repeat:\n                    repeat = False\n                    duplicates = []\n\n                    for ar1 in retVal:\n                        for ar2 in retVal:\n                            if ar1 != ar2:\n                                if ar1.direction == ar2.direction and ar1.fromText == ar2.fromText and\\\n                                        ar1.toText == ar2.toText:\n                                    duplicates.append(ar2)\n                                    break\n                        else:\n                            continue\n                        break\n\n                    if len(duplicates) > 0:\n                        repeat = True\n                        retVal.remove(duplicates[0])\n\n                self.found = True\n                self.adj_relations = retVal\n            return self.adj_relations\n\n    # compute the IOU of table, pass-in var is another Table object\n    def compute_table_iou(self, another_table):\n        table_box_1_temp = []\n        for el in self.table_coords.split():\n            table_box_1_temp.append((el.split(\",\")))\n        table_box_1 = list(flatten(table_box_1_temp))\n        table_box_1 = [int(x) for x in table_box_1]\n\n        table_box_2_temp = []\n        for el in another_table.table_coords.split():\n            table_box_2_temp.append((el.split(\",\")))\n        table_box_2 = list(flatten(table_box_2_temp))\n        table_box_2 = [int(x) for x in table_box_2]\n\n        return compute_poly_iou(table_box_1, table_box_2)\n\n    # find the cell mapping of tables as dictionary, pass-in var is another table and the desired IOU value\n    def find_cell_mapping(self, target_table, iou_value):\n        mapped_cell = []    # store the matches as tuples - (gt, result) mind the order of table when passing in\n        for cell_1 in self.table_cells:\n            for cell_2 in target_table.table_cells:\n                if cell_1.compute_cell_iou(cell_2) >= iou_value:\n                    mapped_cell.append((cell_1, cell_2))\n                    break\n        ret = dict(mapped_cell)\n        # print(ret)\n        return ret\n\n    # to print a table cell mapping\n    @classmethod\n    def printCellMapping(cls, dMappedCell):\n        print(\"-\"*25)\n        for cell1, cell2 in dMappedCell.items():\n            print(\"  \", cell1, \" --> \", cell2)\n\n    # to print a table set of adjacency relations\n    @classmethod\n    def printAdjacencyRelationList(cls, lAdjRel, title=\"\"):\n        print(\"--- %s \"%title + \"-\"*25)\n        for adj in lAdjRel:\n            print(adj)\n\n\nclass ResultStructure:\n\n    def __init__(self, truePos, gtTotal, resTotal):\n        self._truePos = truePos\n        self._gtTotal = gtTotal\n        self._resTotal = resTotal\n\n    @property\n    def truePos(self):\n        return self._truePos\n\n    @property\n    def gtTotal(self):\n        return self._gtTotal\n\n    @property\n    def resTotal(self):\n        return self._resTotal\n\n    def __str__(self):\n        return \"true: {}, gt: {}, res: {}\".format(self._truePos, self._gtTotal, self._resTotal)"}
{"type": "source_file", "path": "utils/ditod_vgt/VGTbackbone.py", "content": "# --------------------------------------------------------------------------------\n# VIT: Multi-Path Vision Transformer for Dense Prediction\n# Copyright (c) 2022 Electronics and Telecommunications Research Institute (ETRI).\n# All Rights Reserved.\n# Written by Youngwan Lee\n# This source code is licensed(Dual License(GPL3.0 & Commercial)) under the license found in the\n# LICENSE file in the root directory of this source tree.\n# --------------------------------------------------------------------------------\n# References:\n# timm: https://github.com/rwightman/pytorch-image-models/tree/master/timm\n# CoaT: https://github.com/mlpc-ucsd/CoaT\n# --------------------------------------------------------------------------------\n\n\nimport torch\nimport torch.nn.functional as F\nimport logging\n\nfrom detectron2.layers import (\n    ShapeSpec,\n)\nfrom detectron2.modeling import Backbone, BACKBONE_REGISTRY, FPN\nfrom detectron2.modeling.backbone.fpn import LastLevelP6P7, LastLevelMaxPool\n\nfrom .VGTbeit import beit_base_patch16, dit_base_patch16, dit_large_patch16, beit_large_patch16, VGT_dit_base_patch16\nfrom .FeatureMerge import FeatureMerge\n\n__all__ = [\n    \"build_VGT_fpn_backbone\",\n]\n\n\nclass PTM_VIT_Backbone(Backbone):\n    \"\"\"\n    Implement VIT backbone.\n    \"\"\"\n\n    def __init__(self, name, out_features, drop_path, img_size, pos_type, merge_type, model_kwargs):\n        super().__init__()\n        self._out_features = out_features\n        if 'base' in name:\n            self._out_feature_strides = {\"layer3\": 4, \"layer5\": 8, \"layer7\": 16, \"layer11\": 32}\n        else:\n            self._out_feature_strides = {\"layer7\": 4, \"layer11\": 8, \"layer15\": 16, \"layer23\": 32}\n\n        if name == 'beit_base_patch16':\n            model_func = beit_base_patch16\n            self._out_feature_channels = {\"layer3\": 768, \"layer5\": 768, \"layer7\": 768, \"layer11\": 768}\n        elif name == 'dit_base_patch16':\n            model_func = dit_base_patch16\n            self._out_feature_channels = {\"layer3\": 768, \"layer5\": 768, \"layer7\": 768, \"layer11\": 768}\n        elif name == \"deit_base_patch16\":\n            model_func = deit_base_patch16\n            self._out_feature_channels = {\"layer3\": 768, \"layer5\": 768, \"layer7\": 768, \"layer11\": 768}\n        elif name == 'VGT_dit_base_patch16':\n            model_func = VGT_dit_base_patch16\n            self._out_feature_channels = {\"layer3\": 768, \"layer5\": 768, \"layer7\": 768, \"layer11\": 768}\n        elif name == \"mae_base_patch16\":\n            model_func = mae_base_patch16\n            self._out_feature_channels = {\"layer3\": 768, \"layer5\": 768, \"layer7\": 768, \"layer11\": 768}\n        elif name == \"dit_large_patch16\":\n            model_func = dit_large_patch16\n            self._out_feature_channels = {\"layer7\": 1024, \"layer11\": 1024, \"layer15\": 1024, \"layer23\": 1024}\n        elif name == \"beit_large_patch16\":\n            model_func = beit_large_patch16\n            self._out_feature_channels = {\"layer7\": 1024, \"layer11\": 1024, \"layer15\": 1024, \"layer23\": 1024}\n        else:\n            raise ValueError(\"Unsupported VIT name yet.\")\n\n        if 'beit' in name or 'dit' in name:\n            if pos_type == \"abs\":\n                self.backbone = model_func(img_size=img_size,\n                                           out_features=out_features,\n                                           drop_path_rate=drop_path,\n                                           use_abs_pos_emb=True,\n                                           **model_kwargs)\n            elif pos_type == \"shared_rel\":\n                self.backbone = model_func(img_size=img_size,\n                                           out_features=out_features,\n                                           drop_path_rate=drop_path,\n                                           use_shared_rel_pos_bias=True,\n                                           **model_kwargs)\n            elif pos_type == \"rel\":\n                self.backbone = model_func(img_size=img_size,\n                                           out_features=out_features,\n                                           drop_path_rate=drop_path,\n                                           use_rel_pos_bias=True,\n                                           **model_kwargs)\n            else:\n                raise ValueError()\n        else:\n            self.backbone = model_func(img_size=img_size,\n                                       out_features=out_features,\n                                       drop_path_rate=drop_path,\n                                       **model_kwargs)\n        \n        logger = logging.getLogger(\"detectron2\")\n        logger.info(\"Merge using: {}\".format(merge_type))\n        self.FeatureMerge = FeatureMerge(feature_names = self._out_features, visual_dim=[768,768,768,768], semantic_dim=[768,768,768,768], merge_type = merge_type)\n\n    def forward(self, x, grid):\n        \"\"\"\n        Args:\n            x: Tensor of shape (N,C,H,W). H, W must be a multiple of ``self.size_divisibility``.\n\n        Returns:\n            dict[str->Tensor]: names and the corresponding features\n        \"\"\"\n        assert x.dim() == 4, f\"VIT takes an input of shape (N, C, H, W). Got {x.shape} instead!\"\n        \n        vis_feat_out, grid_feat_out = self.backbone.forward_features(x, grid)\n        return self.FeatureMerge.forward(vis_feat_out, grid_feat_out)\n        # return self.backbone.forward_features(x)\n\n    def output_shape(self):\n        return {\n            name: ShapeSpec(\n                channels=self._out_feature_channels[name], stride=self._out_feature_strides[name]\n            )\n            for name in self._out_features\n        }\n\n\nclass GridFPN(FPN):\n    def forward(self, x, grid):\n        \"\"\"\n        Args:\n            input (dict[str->Tensor]): mapping feature map name (e.g., \"res5\") to\n                feature map tensor for each feature level in high to low resolution order.\n        Returns:\n            dict[str->Tensor]:\n                mapping from feature map name to FPN feature map tensor\n                in high to low resolution order. Returned feature names follow the FPN\n                paper convention: \"p<stage>\", where stage has stride = 2 ** stage e.g.,\n                [\"p2\", \"p3\", ..., \"p6\"].\n        \"\"\"\n        bottom_up_features = self.bottom_up(x, grid)\n        results = []\n        prev_features = self.lateral_convs[0](bottom_up_features[self.in_features[-1]])\n        results.append(self.output_convs[0](prev_features))\n\n        # Reverse feature maps into top-down order (from low to high resolution)\n        for idx, (lateral_conv, output_conv) in enumerate(\n            zip(self.lateral_convs, self.output_convs)\n        ):\n            # Slicing of ModuleList is not supported https://github.com/pytorch/pytorch/issues/47336\n            # Therefore we loop over all modules but skip the first one\n            if idx > 0:\n                features = self.in_features[-idx - 1]\n                features = bottom_up_features[features]\n                top_down_features = F.interpolate(prev_features, scale_factor=2.0, mode=\"nearest\")\n                lateral_features = lateral_conv(features)\n                prev_features = lateral_features + top_down_features\n                if self._fuse_type == \"avg\":\n                    prev_features /= 2\n                results.insert(0, output_conv(prev_features))\n\n        if self.top_block is not None:\n            if self.top_block.in_feature in bottom_up_features:\n                top_block_in_feature = bottom_up_features[self.top_block.in_feature]\n            else:\n                top_block_in_feature = results[self._out_features.index(self.top_block.in_feature)]\n            results.extend(self.top_block(top_block_in_feature))\n        assert len(self._out_features) == len(results)\n        return {f: res for f, res in zip(self._out_features, results)}\n    \ndef build_PTM_VIT_Backbone(cfg):\n    \"\"\"\n    Create a VIT instance from config.\n\n    Args:\n        cfg: a detectron2 CfgNode\n\n    Returns:\n        A VIT backbone instance.\n    \"\"\"\n    # fmt: off\n    name = cfg.MODEL.VIT.NAME\n    out_features = cfg.MODEL.VIT.OUT_FEATURES\n    drop_path = cfg.MODEL.VIT.DROP_PATH\n    img_size = cfg.MODEL.VIT.IMG_SIZE\n    pos_type = cfg.MODEL.VIT.POS_TYPE\n    merge_type = cfg.MODEL.VIT.MERGE_TYPE\n    \n    model_kwargs = eval(str(cfg.MODEL.VIT.MODEL_KWARGS).replace(\"`\", \"\"))\n\n    return PTM_VIT_Backbone(name, out_features, drop_path, img_size, pos_type, merge_type, model_kwargs)\n\n\n@BACKBONE_REGISTRY.register()\ndef build_VGT_fpn_backbone(cfg, input_shape: ShapeSpec):\n    \"\"\"\n    Create a VIT w/ FPN backbone.\n\n    Args:\n        cfg: a detectron2 CfgNode\n\n    Returns:\n        backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.\n    \"\"\"\n    bottom_up = build_PTM_VIT_Backbone(cfg)\n    in_features = cfg.MODEL.FPN.IN_FEATURES\n    out_channels = cfg.MODEL.FPN.OUT_CHANNELS\n    backbone = GridFPN(\n        bottom_up=bottom_up,\n        in_features=in_features,\n        out_channels=out_channels,\n        norm=cfg.MODEL.FPN.NORM,\n        top_block=LastLevelMaxPool(),\n        fuse_type=cfg.MODEL.FPN.FUSE_TYPE,\n    )\n    return backbone\n"}
