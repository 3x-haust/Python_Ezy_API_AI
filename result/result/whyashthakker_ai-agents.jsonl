{"repo_info": {"repo_name": "ai-agents", "repo_owner": "whyashthakker", "repo_url": "https://github.com/whyashthakker/ai-agents"}}
{"type": "source_file", "path": "agents/environment/aqi/main.py", "content": "from typing import Dict, Optional\nfrom dataclasses import dataclass\nfrom pydantic import BaseModel, Field\nfrom agno.agent import Agent\nfrom agno.models.openai import OpenAIChat\nfrom firecrawl import FirecrawlApp\nimport streamlit as st\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\nclass AQIResponse(BaseModel):\n    success: bool\n    data: Dict[str, float]\n    status: str\n    expiresAt: str\n\nclass ExtractSchema(BaseModel):\n    aqi: float = Field(description=\"Air Quality Index\")\n    temperature: float = Field(description=\"Temperature in degrees Celsius\")\n    humidity: float = Field(description=\"Humidity percentage\")\n    wind_speed: float = Field(description=\"Wind speed in kilometers per hour\")\n    pm25: float = Field(description=\"Particulate Matter 2.5 micrometers\")\n    pm10: float = Field(description=\"Particulate Matter 10 micrometers\")\n    co: float = Field(description=\"Carbon Monoxide level\")\n\n@dataclass\nclass UserInput:\n    city: str\n    state: str\n    country: str\n    medical_conditions: Optional[str]\n    planned_activity: str\n\nclass AQIAnalyzer:\n    \n    def __init__(self, firecrawl_key: str) -> None:\n        self.firecrawl = FirecrawlApp(api_key=firecrawl_key)\n    \n    def _format_url(self, country: str, state: str, city: str) -> str:\n        \"\"\"Format URL based on location, handling cases with and without state\"\"\"\n        country_clean = country.lower().replace(' ', '-')\n        city_clean = city.lower().replace(' ', '-')\n        \n        if not state or state.lower() == 'none':\n            return f\"https://www.aqi.in/dashboard/{country_clean}/{city_clean}\"\n        \n        state_clean = state.lower().replace(' ', '-')\n        return f\"https://www.aqi.in/dashboard/{country_clean}/{state_clean}/{city_clean}\"\n    \n    def fetch_aqi_data(self, city: str, state: str, country: str) -> Dict[str, float]:\n        \"\"\"Fetch AQI data using Firecrawl\"\"\"\n        try:\n            url = self._format_url(country, state, city)\n            st.info(f\"Accessing URL: {url}\")  # Display URL being accessed\n            \n            response = self.firecrawl.extract(\n                urls=[f\"{url}/*\"],\n                params={\n                    'prompt': 'Extract the current real-time AQI, temperature, humidity, wind speed, PM2.5, PM10, and CO levels from the page. Also extract the timestamp of the data.',\n                    'schema': ExtractSchema.model_json_schema()\n                }\n            )\n            \n            aqi_response = AQIResponse(**response)\n            if not aqi_response.success:\n                raise ValueError(f\"Failed to fetch AQI data: {aqi_response.status}\")\n            \n            with st.expander(\"üì¶ Raw AQI Data\", expanded=False):\n                st.json({\n                    \"url_accessed\": url,\n                    \"timestamp\": aqi_response.expiresAt,\n                    \"data\": aqi_response.data\n                })\n                \n                st.warning(\"\"\"\n                    ‚ö†Ô∏è Note: The data shown may not match real-time values on the website. \n                    This could be due to:\n                    - Cached data in Firecrawl\n                    - Rate limiting\n                    - Website updates not being captured\n                    \n                    Consider refreshing or checking the website directly for real-time values.\n                \"\"\")\n                \n            return aqi_response.data\n            \n        except Exception as e:\n            st.error(f\"Error fetching AQI data: {str(e)}\")\n            return {\n                'aqi': 0,\n                'temperature': 0,\n                'humidity': 0,\n                'wind_speed': 0,\n                'pm25': 0,\n                'pm10': 0,\n                'co': 0\n            }\n\nclass HealthRecommendationAgent:\n    \n    def __init__(self, openai_key: str) -> None:\n        self.agent = Agent(\n            model=OpenAIChat(\n                id=\"gpt-4o\",\n                name=\"Health Recommendation Agent\",\n                api_key=openai_key\n            )\n        )\n    \n    def get_recommendations(\n        self,\n        aqi_data: Dict[str, float],\n        user_input: UserInput\n    ) -> str:\n        prompt = self._create_prompt(aqi_data, user_input)\n        response = self.agent.run(prompt)\n        return response.content\n    \n    def _create_prompt(self, aqi_data: Dict[str, float], user_input: UserInput) -> str:\n        return f\"\"\"\n        Based on the following air quality conditions in {user_input.city}, {user_input.state}, {user_input.country}:\n        - Overall AQI: {aqi_data['aqi']}\n        - PM2.5 Level: {aqi_data['pm25']} ¬µg/m¬≥\n        - PM10 Level: {aqi_data['pm10']} ¬µg/m¬≥\n        - CO Level: {aqi_data['co']} ppb\n        \n        Weather conditions:\n        - Temperature: {aqi_data['temperature']}¬∞C\n        - Humidity: {aqi_data['humidity']}%\n        - Wind Speed: {aqi_data['wind_speed']} km/h\n        \n        User's Context:\n        - Medical Conditions: {user_input.medical_conditions or 'None'}\n        - Planned Activity: {user_input.planned_activity}\n        **Comprehensive Health Recommendations:**\n        1. **Impact of Current Air Quality on Health:**\n        2. **Necessary Safety Precautions for Planned Activity:**\n        3. **Advisability of Planned Activity:**\n        4. **Best Time to Conduct the Activity:**\n        \"\"\"\n\ndef analyze_conditions(\n    user_input: UserInput,\n    api_keys: Dict[str, str]\n) -> str:\n    aqi_analyzer = AQIAnalyzer(firecrawl_key=api_keys['firecrawl'])\n    health_agent = HealthRecommendationAgent(openai_key=api_keys['openai'])\n    \n    aqi_data = aqi_analyzer.fetch_aqi_data(\n        city=user_input.city,\n        state=user_input.state,\n        country=user_input.country\n    )\n    \n    return health_agent.get_recommendations(aqi_data, user_input)\n\ndef initialize_session_state():\n    if 'api_keys' not in st.session_state:\n        # Get API keys from environment variables\n        firecrawl_key = os.environ.get('FIRECRAWL_API_KEY', '')\n        openai_key = os.environ.get('OPENAI_API_KEY', '')\n        \n        st.session_state.api_keys = {\n            'firecrawl': firecrawl_key,\n            'openai': openai_key\n        }\n        \n        # Check if API keys are available\n        if not firecrawl_key:\n            st.warning(\"‚ö†Ô∏è FIRECRAWL_API_KEY not found in environment variables. You can still set it in the sidebar.\")\n        if not openai_key:\n            st.warning(\"‚ö†Ô∏è OPENAI_API_KEY not found in environment variables. You can still set it in the sidebar.\")\n\ndef setup_page():\n    st.set_page_config(\n        page_title=\"AQI Analysis Agent\",\n        page_icon=\"üåç\",\n        layout=\"wide\"\n    )\n    \n    st.title(\"üåç AQI Analysis Agent\")\n    st.info(\"Get personalized health recommendations based on air quality conditions.\")\n\ndef render_sidebar():\n    \"\"\"Render sidebar with API configuration\"\"\"\n    with st.sidebar:\n        st.header(\"üîë API Configuration\")\n        st.info(\"API keys are loaded from environment variables. You can override them here if needed.\")\n        \n        new_firecrawl_key = st.text_input(\n            \"Firecrawl API Key\",\n            type=\"password\",\n            value=st.session_state.api_keys['firecrawl'],\n            help=\"Enter your Firecrawl API key or set FIRECRAWL_API_KEY environment variable\"\n        )\n        new_openai_key = st.text_input(\n            \"OpenAI API Key\",\n            type=\"password\",\n            value=st.session_state.api_keys['openai'],\n            help=\"Enter your OpenAI API key or set OPENAI_API_KEY environment variable\"\n        )\n        \n        if (new_firecrawl_key and new_openai_key and\n            (new_firecrawl_key != st.session_state.api_keys['firecrawl'] or \n             new_openai_key != st.session_state.api_keys['openai'])):\n            st.session_state.api_keys.update({\n                'firecrawl': new_firecrawl_key,\n                'openai': new_openai_key\n            })\n            st.success(\"‚úÖ API keys updated!\")\n            \n        # Display environment variable setup instructions\n        with st.expander(\"How to set environment variables\"):\n            st.markdown(\"\"\"\n            ### Setting up environment variables\n            \n            1. Create a `.env` file in the project root with:\n            ```\n            FIRECRAWL_API_KEY=your_firecrawl_key_here\n            OPENAI_API_KEY=your_openai_key_here\n            ```\n            \n            2. Or set them in your terminal:\n            ```bash\n            export FIRECRAWL_API_KEY=your_firecrawl_key_here\n            export OPENAI_API_KEY=your_openai_key_here\n            ```\n            \"\"\")\n\ndef render_main_content():\n    st.header(\"üìç Location Details\")\n    col1, col2 = st.columns(2)\n    \n    with col1:\n        city = st.text_input(\"City\", placeholder=\"e.g., Mumbai\")\n        state = st.text_input(\"State\", placeholder=\"If it's a Union Territory or a city in the US, leave it blank\")\n        country = st.text_input(\"Country\", value=\"India\", placeholder=\"United States\")\n    \n    with col2:\n        st.header(\"üë§ Personal Details\")\n        medical_conditions = st.text_area(\n            \"Medical Conditions (optional)\",\n            placeholder=\"e.g., asthma, allergies\"\n        )\n        planned_activity = st.text_area(\n            \"Planned Activity\",\n            placeholder=\"e.g., morning jog for 2 hours\"\n        )\n    \n    return UserInput(\n        city=city,\n        state=state,\n        country=country,\n        medical_conditions=medical_conditions,\n        planned_activity=planned_activity\n    )\n\ndef create_aqi_gauge(aqi_value: float) -> go.Figure:\n    \"\"\"Create a gauge chart for AQI value with color zones\"\"\"\n    \n    # Define AQI categories and colors\n    categories = ['Good', 'Moderate', 'Unhealthy for Sensitive Groups', 'Unhealthy', 'Very Unhealthy', 'Hazardous']\n    colors = ['#00e400', '#ffff00', '#ff7e00', '#ff0000', '#99004c', '#7e0023']\n    \n    # Create thresholds for AQI categories\n    thresholds = [0, 50, 100, 150, 200, 300, 500]\n    \n    # Determine the category based on AQI value\n    category_index = 0\n    for i, threshold in enumerate(thresholds[1:]):\n        if aqi_value <= threshold:\n            category_index = i\n            break\n    \n    fig = go.Figure(go.Indicator(\n        mode=\"gauge+number+delta\",\n        value=aqi_value,\n        domain={'x': [0, 1], 'y': [0, 1]},\n        title={'text': f\"Air Quality Index<br><span style='font-size:0.8em;color:{colors[category_index]}'>{categories[category_index]}</span>\"},\n        gauge={\n            'axis': {'range': [None, 500], 'tickwidth': 1, 'tickcolor': \"darkblue\"},\n            'bar': {'color': colors[category_index]},\n            'bgcolor': \"white\",\n            'borderwidth': 2,\n            'bordercolor': \"gray\",\n            'steps': [\n                {'range': [0, 50], 'color': '#00e400'},\n                {'range': [50, 100], 'color': '#ffff00'},\n                {'range': [100, 150], 'color': '#ff7e00'},\n                {'range': [150, 200], 'color': '#ff0000'},\n                {'range': [200, 300], 'color': '#99004c'},\n                {'range': [300, 500], 'color': '#7e0023'}\n            ],\n        }\n    ))\n    \n    fig.update_layout(\n        height=300,\n        margin=dict(l=20, r=20, t=50, b=20),\n    )\n    \n    return fig\n\ndef create_pollutant_comparison(aqi_data: Dict[str, float]) -> go.Figure:\n    \"\"\"Create a bar chart comparing different pollutants\"\"\"\n    \n    pollutants = ['pm25', 'pm10', 'co']\n    values = [aqi_data.get(p, 0) for p in pollutants]\n    labels = ['PM2.5 (¬µg/m¬≥)', 'PM10 (¬µg/m¬≥)', 'CO (ppb)']\n    \n    # Define thresholds for each pollutant (simplified)\n    thresholds = {\n        'pm25': [0, 12, 35.4, 55.4, 150.4, 250.4],  # EPA standards\n        'pm10': [0, 54, 154, 254, 354, 424],        # EPA standards\n        'co': [0, 4400, 9400, 12400, 15400, 30400]  # Simplified CO thresholds\n    }\n    \n    colors = []\n    for i, pollutant in enumerate(pollutants):\n        value = values[i]\n        thresh = thresholds[pollutant]\n        \n        if value <= thresh[1]:\n            colors.append('#00e400')  # Good\n        elif value <= thresh[2]:\n            colors.append('#ffff00')  # Moderate\n        elif value <= thresh[3]:\n            colors.append('#ff7e00')  # Unhealthy for Sensitive Groups\n        elif value <= thresh[4]:\n            colors.append('#ff0000')  # Unhealthy\n        elif value <= thresh[5]:\n            colors.append('#99004c')  # Very Unhealthy\n        else:\n            colors.append('#7e0023')  # Hazardous\n    \n    fig = go.Figure(data=[\n        go.Bar(\n            x=labels,\n            y=values,\n            marker_color=colors\n        )\n    ])\n    \n    fig.update_layout(\n        title=\"Pollutant Levels\",\n        xaxis_title=\"Pollutant Type\",\n        yaxis_title=\"Concentration\",\n        height=400,\n        margin=dict(l=20, r=20, t=50, b=20),\n    )\n    \n    return fig\n\ndef create_weather_indicators(aqi_data: Dict[str, float]) -> Dict[str, go.Figure]:\n    \"\"\"Create indicator charts for weather conditions\"\"\"\n    \n    indicators = {}\n    \n    # Temperature indicator\n    temp_fig = go.Figure(go.Indicator(\n        mode=\"gauge+number\",\n        value=aqi_data.get('temperature', 0),\n        domain={'x': [0, 1], 'y': [0, 1]},\n        title={'text': \"Temperature (¬∞C)\"},\n        gauge={\n            'axis': {'range': [-10, 50], 'tickwidth': 1},\n            'bar': {'color': \"darkred\"},\n            'steps': [\n                {'range': [-10, 0], 'color': \"#6ECFF6\"},\n                {'range': [0, 15], 'color': \"#85E0F9\"},\n                {'range': [15, 25], 'color': \"#FFFF00\"},\n                {'range': [25, 35], 'color': \"#FFA500\"},\n                {'range': [35, 50], 'color': \"#FF4500\"}\n            ],\n        }\n    ))\n    temp_fig.update_layout(height=250, margin=dict(l=20, r=20, t=50, b=20))\n    indicators['temperature'] = temp_fig\n    \n    # Humidity indicator\n    humidity_fig = go.Figure(go.Indicator(\n        mode=\"gauge+number\",\n        value=aqi_data.get('humidity', 0),\n        domain={'x': [0, 1], 'y': [0, 1]},\n        title={'text': \"Humidity (%)\"},\n        gauge={\n            'axis': {'range': [0, 100], 'tickwidth': 1},\n            'bar': {'color': \"blue\"},\n            'steps': [\n                {'range': [0, 30], 'color': \"#FFFACD\"},\n                {'range': [30, 60], 'color': \"#87CEEB\"},\n                {'range': [60, 100], 'color': \"#1E90FF\"}\n            ],\n        }\n    ))\n    humidity_fig.update_layout(height=250, margin=dict(l=20, r=20, t=50, b=20))\n    indicators['humidity'] = humidity_fig\n    \n    # Wind speed indicator\n    wind_fig = go.Figure(go.Indicator(\n        mode=\"gauge+number\",\n        value=aqi_data.get('wind_speed', 0),\n        domain={'x': [0, 1], 'y': [0, 1]},\n        title={'text': \"Wind Speed (km/h)\"},\n        gauge={\n            'axis': {'range': [0, 50], 'tickwidth': 1},\n            'bar': {'color': \"lightblue\"},\n            'steps': [\n                {'range': [0, 5], 'color': \"#E0FFFF\"},\n                {'range': [5, 20], 'color': \"#87CEEB\"},\n                {'range': [20, 35], 'color': \"#4682B4\"},\n                {'range': [35, 50], 'color': \"#000080\"}\n            ],\n        }\n    ))\n    wind_fig.update_layout(height=250, margin=dict(l=20, r=20, t=50, b=20))\n    indicators['wind_speed'] = wind_fig\n    \n    return indicators\n\ndef render_aqi_dashboard(aqi_data: Dict[str, float], user_input: UserInput):\n    \"\"\"Render a dashboard with AQI and weather visualizations\"\"\"\n    \n    st.header(\"üìä Air Quality Dashboard\")\n    \n    # Display location and timestamp\n    st.subheader(f\"üìç {user_input.city}, {user_input.state or ''} {user_input.country}\")\n    st.caption(f\"Data as of: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n    \n    # Main AQI gauge\n    st.plotly_chart(create_aqi_gauge(aqi_data.get('aqi', 0)), use_container_width=True)\n    \n    # Pollutant comparison\n    st.subheader(\"üî¨ Pollutant Levels\")\n    st.plotly_chart(create_pollutant_comparison(aqi_data), use_container_width=True)\n    \n    # Weather indicators\n    st.subheader(\"üå§Ô∏è Weather Conditions\")\n    weather_indicators = create_weather_indicators(aqi_data)\n    \n    cols = st.columns(3)\n    with cols[0]:\n        st.plotly_chart(weather_indicators['temperature'], use_container_width=True)\n    with cols[1]:\n        st.plotly_chart(weather_indicators['humidity'], use_container_width=True)\n    with cols[2]:\n        st.plotly_chart(weather_indicators['wind_speed'], use_container_width=True)\n    \n    # Health impact summary\n    st.subheader(\"ü©∫ Health Impact Summary\")\n    \n    aqi_value = aqi_data.get('aqi', 0)\n    if aqi_value <= 50:\n        impact = \"Good air quality with minimal health concerns.\"\n        color = \"#00e400\"\n    elif aqi_value <= 100:\n        impact = \"Moderate air quality. Unusually sensitive individuals should consider limiting prolonged outdoor exertion.\"\n        color = \"#ffff00\"\n    elif aqi_value <= 150:\n        impact = \"Unhealthy for sensitive groups. People with respiratory or heart disease, the elderly and children should limit prolonged outdoor exertion.\"\n        color = \"#ff7e00\"\n    elif aqi_value <= 200:\n        impact = \"Unhealthy. Everyone may begin to experience health effects. Sensitive groups should limit outdoor exertion.\"\n        color = \"#ff0000\"\n    elif aqi_value <= 300:\n        impact = \"Very Unhealthy. Health warnings of emergency conditions. The entire population is more likely to be affected.\"\n        color = \"#99004c\"\n    else:\n        impact = \"Hazardous. Health alert: everyone may experience more serious health effects.\"\n        color = \"#7e0023\"\n    \n    st.markdown(f\"<div style='background-color:{color}20; padding:10px; border-radius:5px; border-left:5px solid {color};'>{impact}</div>\", unsafe_allow_html=True)\n\ndef main():\n    \"\"\"Main application entry point\"\"\"\n    initialize_session_state()\n    setup_page()\n    render_sidebar()\n    user_input = render_main_content()\n    \n    result = None\n    aqi_data = None\n    \n    if st.button(\"üîç Analyze & Get Recommendations\"):\n        if not all([user_input.city, user_input.planned_activity]):\n            st.error(\"Please fill in all required fields (state and medical conditions are optional)\")\n        elif not all(st.session_state.api_keys.values()):\n            st.error(\"Please provide both API keys in the sidebar\")\n        else:\n            try:\n                with st.spinner(\"üîÑ Analyzing conditions...\"):\n                    # First fetch AQI data\n                    aqi_analyzer = AQIAnalyzer(firecrawl_key=st.session_state.api_keys['firecrawl'])\n                    aqi_data = aqi_analyzer.fetch_aqi_data(\n                        city=user_input.city,\n                        state=user_input.state,\n                        country=user_input.country\n                    )\n                    \n                    # Then get recommendations\n                    health_agent = HealthRecommendationAgent(openai_key=st.session_state.api_keys['openai'])\n                    result = health_agent.get_recommendations(aqi_data, user_input)\n                    \n                    st.success(\"‚úÖ Analysis completed!\")\n            \n            except Exception as e:\n                st.error(f\"‚ùå Error: {str(e)}\")\n\n    if aqi_data:\n        # Render the AQI dashboard with visualizations\n        render_aqi_dashboard(aqi_data, user_input)\n        \n    if result:\n        st.markdown(\"### üì¶ Recommendations\")\n        st.markdown(result)\n        \n        st.download_button(\n            \"üíæ Download Recommendations\",\n            data=result,\n            file_name=f\"aqi_recommendations_{user_input.city}_{user_input.state}.txt\",\n            mime=\"text/plain\"\n        )\n\nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "agents/newsletter/main.py", "content": "import os\nfrom dotenv import load_dotenv\nfrom crewai import Agent, Task, Crew, LLM\nfrom crewai_tools import SerperDevTool\nfrom langchain_openai import ChatOpenAI\nfrom langchain_community.llms import Ollama\n\nload_dotenv()\nos.environ[\"SERPER_API_KEY\"] = os.getenv(\"SERPER_API_KEY\")\n\nsearch_tool = SerperDevTool()\n\ndef get_llm(use_gpt=True):\n    if use_gpt:\n        return LLM(model=\"openai/gpt-4o-mini\", temperature=0.7)\n    return Ollama(\n        model=\"deepseek-r1:latest\",\n        base_url=\"http://localhost:11434\",\n        temperature=0.7\n    )\n\ndef create_agents(use_gpt=True):\n    llm = get_llm(use_gpt)\n    \n    researcher = Agent(\n        role='Research Specialist',\n        goal='Find comprehensive and up-to-date information on topics',\n        backstory='Expert researcher skilled at discovering reliable information from various sources',\n        tools=[search_tool],\n        llm=llm,\n        verbose=True\n    )\n    \n    fact_checker = Agent(\n        role='Fact Verification Specialist',\n        goal='Verify accuracy of information and cross-reference sources',\n        backstory='Meticulous fact-checker with years of experience in verification and validation',\n        tools=[search_tool],\n        llm=llm,\n        verbose=True\n    )\n    \n    writer = Agent(\n        role='Newsletter Writer',\n        goal='Create engaging and well-structured newsletters',\n        backstory='Professional writer specializing in creating compelling newsletters with clear structure and engaging content',\n        llm=llm,\n        verbose=True\n    )\n    \n    return researcher, fact_checker, writer\n\ndef create_tasks(researcher, fact_checker, writer, topic):\n    research_task = Task(\n        description=f\"Research the latest developments, key trends, and important insights about: {topic}\",\n        agent=researcher,\n        expected_output=\"A detailed summary of the topic with key points and references\"\n    )\n    \n    verify_task = Task(\n        description=\"Verify the accuracy of the research findings and identify any conflicting information\",\n        agent=fact_checker,\n        context=[research_task],\n        expected_output=\"A comprehensive report on the accuracy of the research findings and any conflicting information\"\n    )\n    \n    newsletter_task = Task(\n        description=f\"\"\"Create a newsletter about {topic} with the following format:\n        - Title\n        - Subtitle\n        - Topic overview\n        - H1, H2, H3 headers for main points\n        - 500-word blog post\n        Make it engaging and well-structured.\"\"\",\n        agent=writer,\n        context=[research_task, verify_task],\n        expected_output=\"A well-structured newsletter in HTML format\"\n    )\n    \n    return [research_task, verify_task, newsletter_task]\n\ndef create_crew(agents, tasks):\n    return Crew(\n        agents=agents,\n        tasks=tasks,\n        verbose=True\n    )\n\ndef main():\n    print(\"Welcome to the Newsletter Creation Crew!\")\n    use_gpt = input(\"Use GPT-4? (yes/no): \").lower() == 'yes'\n    \n    if not use_gpt:\n        print(\"\\nUsing Ollama - Ensure it's running on http://localhost:11434\")\n        print(\"Start with: ollama run deepseek-r1:latest\")\n    \n    topic = input(\"\\nNewsletter topic: \")\n    \n    try:\n        researcher, fact_checker, writer = create_agents(use_gpt)\n        tasks = create_tasks(researcher, fact_checker, writer, topic)\n        crew = create_crew([researcher, fact_checker, writer], tasks)\n        \n        result = crew.kickoff()\n        print(\"\\nNewsletter Result:\")\n        print(result)\n        \n    except Exception as e:\n        print(f\"\\nError: {str(e)}\")\n        if not use_gpt:\n            print(\"\\nTip: Ensure Ollama is running with the deepseek-r1:latest model\")\n            print(\"Run: ollama pull deepseek-r1:latest\")\n\nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "agents/jobs/job-hunt-agent-new.py", "content": "from typing import Dict, List\nfrom pydantic import BaseModel, Field\nfrom agno.agent import Agent\nfrom agno.models.openai import OpenAIChat\nfrom firecrawl import FirecrawlApp\nimport streamlit as st\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file if it exists\nload_dotenv()\n\nclass NestedModel1(BaseModel):\n    \"\"\"Schema for job posting data\"\"\"\n    region: str = Field(description=\"Region or area where the job is located\", default=None)\n    role: str = Field(description=\"Specific role or function within the job category\", default=None)\n    job_title: str = Field(description=\"Title of the job position\", default=None)\n    experience: str = Field(description=\"Experience required for the position\", default=None)\n    job_link: str = Field(description=\"Link to the job posting\", default=None)\nclass ExtractSchema(BaseModel):\n    \"\"\"Schema for job postings extraction\"\"\"\n    job_postings: List[NestedModel1] = Field(description=\"List of job postings\")\n\nclass IndustryTrend(BaseModel):\n    \"\"\"Schema for industry trend data\"\"\"\n    industry: str = Field(description=\"Industry name\", default=None)\n    avg_salary: float = Field(description=\"Average salary in the industry\", default=None)\n    growth_rate: float = Field(description=\"Growth rate of the industry\", default=None)\n    demand_level: str = Field(description=\"Demand level in the industry\", default=None)\n    top_skills: List[str] = Field(description=\"Top skills in demand for this industry\", default=None)\n\nclass IndustryTrendsSchema(BaseModel):\n    \"\"\"Schema for industry trends extraction\"\"\"\n    industry_trends: List[IndustryTrend] = Field(description=\"List of industry trends\")\n\nclass FirecrawlResponse(BaseModel):\n    \"\"\"Schema for Firecrawl API response\"\"\"\n    success: bool\n    data: Dict\n    status: str\n    expiresAt: str\n\nclass JobHuntingAgent:\n    \"\"\"Agent responsible for finding jobs and providing recommendations\"\"\"\n    \n    def __init__(self, firecrawl_api_key: str, openai_api_key: str, model_id: str = \"o3-mini\"):\n        self.agent = Agent(\n            model=OpenAIChat(id=model_id, api_key=openai_api_key),\n            markdown=True,\n            description=\"I am a career expert who helps find and analyze job opportunities based on user preferences.\"\n        )\n        self.firecrawl = FirecrawlApp(api_key=firecrawl_api_key)\n\n    def find_jobs(\n        self, \n        job_title: str,\n        location: str,\n        experience_years: int,\n        skills: List[str]\n    ) -> str:\n        \"\"\"Find and analyze jobs based on user preferences\"\"\"\n        formatted_job_title = job_title.lower().replace(\" \", \"-\")\n        formatted_location = location.lower().replace(\" \", \"-\")\n        skills_string = \", \".join(skills)\n        \n        urls = [\n            f\"https://www.naukri.com/{formatted_job_title}-jobs-in-{formatted_location}\",\n            f\"https://www.indeed.com/jobs?q={formatted_job_title}&l={formatted_location}\",\n            f\"https://www.monster.com/jobs/search/?q={formatted_job_title}&where={formatted_location}\",\n        ]\n        \n        print(f\"Searching for jobs with URLs: {urls}\")\n        \n        try:\n            raw_response = self.firecrawl.extract(\n                urls=urls,\n                params={\n                    'prompt': f\"\"\"Extract job postings by region, roles, job titles, and experience from these job sites.\n                    \n                    Look for jobs that match these criteria:\n                    - Job Title: Should be related to {job_title}\n                    - Location: {location} (include remote jobs if available)\n                    - Experience: Around {experience_years} years\n                    - Skills: Should match at least some of these skills: {skills_string}\n                    - Job Type: Full-time, Part-time, Contract, Temporary, Internship\n                    \n                    For each job posting, extract:\n                    - region: The broader region or area where the job is located (e.g., \"Northeast\", \"West Coast\", \"Midwest\")\n                    - role: The specific role or function (e.g., \"Frontend Developer\", \"Data Analyst\")\n                    - job_title: The exact title of the job\n                    - experience: The experience requirement in years or level (e.g., \"3-5 years\", \"Senior\")\n                    - job_link: The link to the job posting\n                    \n                    IMPORTANT: Return data for at least 3 different job opportunities. MAXIMUM 10.\n                    \"\"\",\n                    'schema': ExtractSchema.model_json_schema()\n                }\n            )\n            \n            print(\"Raw Job Response:\", raw_response)\n            \n            if isinstance(raw_response, dict) and raw_response.get('success'):\n                jobs = raw_response['data'].get('job_postings', [])\n            else:\n                jobs = []\n                \n            print(\"Processed Jobs:\", jobs)\n            \n            if not jobs:\n                return \"No job listings found matching your criteria. Try adjusting your search parameters or try different job sites.\"\n            \n            analysis = self.agent.run(\n                f\"\"\"As a career expert, analyze these job opportunities:\n\n                Jobs Found in json format:\n                {jobs}\n\n                **IMPORTANT INSTRUCTIONS:**\n                1. ONLY analyze jobs from the above JSON data that match the user's requirements:\n                   - Job Title: Related to {job_title}\n                   - Location/Region: Near {location}\n                   - Experience: Around {experience_years} years\n                   - Skills: {skills_string}\n                   - Job Type: Full-time, Part-time, Contract, Temporary, Internship\n                2. DO NOT create new job listings\n                3. From the matching jobs, select 5-6 jobs that best match the user's skills and experience\n\n                Please provide your analysis in this format:\n                \n                üíº SELECTED JOB OPPORTUNITIES\n                ‚Ä¢ List only 5-6 best matching jobs\n                ‚Ä¢ For each job include:\n                  - Job Title and Role\n                  - Region/Location\n                  - Experience Required\n                  - Pros and Cons\n                  - Job Link\n                üîç SKILLS MATCH ANALYSIS\n                ‚Ä¢ Compare the selected jobs based on:\n                  - Skills match with user's profile\n                  - Experience requirements\n                  - Growth potential\n\n                üí° RECOMMENDATIONS\n                ‚Ä¢ Top 3 jobs from the selection with reasoning\n                ‚Ä¢ Career growth potential\n                ‚Ä¢ Points to consider before applying\n\n                üìù APPLICATION TIPS\n                ‚Ä¢ Job-specific application strategies\n                ‚Ä¢ Resume customization tips for these roles\n\n                Format your response in a clear, structured way using the above sections.\n                \"\"\"\n            )\n            \n            return analysis.content\n        except Exception as e:\n            print(f\"Error in find_jobs: {str(e)}\")\n            return f\"An error occurred while searching for jobs: {str(e)}\\n\\nPlease try again with different search parameters or check if the job sites are supported by Firecrawl.\"\n\n    def get_industry_trends(self, job_category: str) -> str:\n        \"\"\"Get trends for the specified job category/industry\"\"\"\n        urls = [\n            f\"https://www.payscale.com/research/US/Job={job_category.replace(' ', '_')}/Salary\",\n            f\"https://www.glassdoor.com/Salaries/{job_category.lower().replace(' ', '-')}-salary-SRCH_KO0,{len(job_category)}.htm\"\n        ]\n        \n        print(f\"Searching for industry trends with URLs: {urls}\")\n        \n        try:\n            raw_response = self.firecrawl.extract(\n                urls=urls,\n                params={\n                    'prompt': f\"\"\"Extract industry trends data for the {job_category} industry. \n                    \n                    For each industry trend, extract:\n                    - industry: The specific industry or sub-category\n                    - avg_salary: The average salary in this industry (as a number)\n                    - growth_rate: The growth rate of this industry (as a number)\n                    - demand_level: The demand level (e.g., \"High\", \"Medium\", \"Low\")\n                    - top_skills: A list of top skills in demand for this industry\n                    \n                    IMPORTANT: \n                    - Extract data for at least 3-5 different roles or sub-categories within this industry\n                    - Include salary trends, growth rate, and demand level\n                    - Identify top skills in demand for this industry\n                    \"\"\",\n                    'schema': IndustryTrendsSchema.model_json_schema(),\n                }\n            )\n            \n            print(\"Raw Industry Trends Response:\", raw_response)\n            \n            if isinstance(raw_response, dict) and raw_response.get('success'):\n                industries = raw_response['data'].get('industry_trends', [])\n        \n                if not industries:\n                    return f\"No industry trends data available for {job_category}. Try a different industry category.\"\n                \n                analysis = self.agent.run(\n                    f\"\"\"As a career expert, analyze these industry trends for {job_category}:\n\n                    {industries}\n\n                    Please provide:\n                    1. A bullet-point summary of the salary and demand trends\n                    2. Identify the top skills in demand for this industry\n                    3. Career growth opportunities:\n                       - Roles with highest growth potential\n                       - Emerging specializations\n                       - Skills with increasing demand\n                    4. Specific advice for job seekers based on these trends\n\n                    Format the response as follows:\n                    \n                    üìä INDUSTRY TRENDS SUMMARY\n                    ‚Ä¢ [Bullet points for salary and demand trends]\n\n                    üî• TOP SKILLS IN DEMAND\n                    ‚Ä¢ [Bullet points for most sought-after skills]\n\n                    üìà CAREER GROWTH OPPORTUNITIES\n                    ‚Ä¢ [Bullet points with growth insights]\n\n                    üéØ RECOMMENDATIONS FOR JOB SEEKERS\n                    ‚Ä¢ [Bullet points with specific advice]\n                    \"\"\"\n                )\n                \n                return analysis.content\n            \n            return f\"No industry trends data available for {job_category}. Try a different industry category.\"\n        except Exception as e:\n            print(f\"Error in get_industry_trends: {str(e)}\")\n            return f\"An error occurred while fetching industry trends: {str(e)}\\n\\nPlease try again with a different industry category or check if the sites are supported by Firecrawl.\"\n\ndef create_job_agent():\n    \"\"\"Create JobHuntingAgent with API keys from session state\"\"\"\n    if 'job_agent' not in st.session_state:\n        st.session_state.job_agent = JobHuntingAgent(\n            firecrawl_api_key=st.session_state.firecrawl_key,\n            openai_api_key=st.session_state.openai_key,\n            model_id=st.session_state.model_id\n        )\n\ndef main():\n    st.set_page_config(\n        page_title=\"AI Job Hunting Assistant\",\n        page_icon=\"üíº\",\n        layout=\"wide\"\n    )\n\n    # Get API keys from environment variables\n    env_firecrawl_key = os.getenv(\"FIRECRAWL_API_KEY\", \"\")\n    env_openai_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n    default_model = os.getenv(\"OPENAI_MODEL_ID\", \"o3-mini\")\n\n    with st.sidebar:\n        st.title(\"üîë API Configuration\")\n        \n        st.subheader(\"ü§ñ Model Selection\")\n        model_id = st.selectbox(\n            \"Choose OpenAI Model\",\n            options=[\"o3-mini\", \"gpt-4o-mini\"],\n            index=0 if default_model == \"o3-mini\" else 1,\n            help=\"Select the AI model to use. Choose gpt-4o if your api doesn't have access to o3-mini\"\n        )\n        st.session_state.model_id = model_id\n        \n        st.divider()\n        \n        st.subheader(\"üîê API Keys\")\n        \n        # Show environment variable status\n        if env_firecrawl_key:\n            st.success(\"‚úÖ Firecrawl API Key found in environment variables\")\n        if env_openai_key:\n            st.success(\"‚úÖ OpenAI API Key found in environment variables\")\n            \n        # Allow UI override of environment variables\n        firecrawl_key = st.text_input(\n            \"Firecrawl API Key (optional if set in environment)\",\n            type=\"password\",\n            help=\"Enter your Firecrawl API key or set FIRECRAWL_API_KEY in environment\",\n            value=\"\" if env_firecrawl_key else \"\"\n        )\n        openai_key = st.text_input(\n            \"OpenAI API Key (optional if set in environment)\",\n            type=\"password\",\n            help=\"Enter your OpenAI API key or set OPENAI_API_KEY in environment\",\n            value=\"\" if env_openai_key else \"\"\n        )\n        \n        # Use environment variables if UI inputs are empty\n        firecrawl_key = firecrawl_key or env_firecrawl_key\n        openai_key = openai_key or env_openai_key\n        \n        if firecrawl_key and openai_key:\n            st.session_state.firecrawl_key = firecrawl_key\n            st.session_state.openai_key = openai_key\n            create_job_agent()\n        else:\n            missing_keys = []\n            if not firecrawl_key:\n                missing_keys.append(\"Firecrawl API Key\")\n            if not openai_key:\n                missing_keys.append(\"OpenAI API Key\")\n            if missing_keys:\n                st.warning(f\"‚ö†Ô∏è Missing required API keys: {', '.join(missing_keys)}\")\n                st.info(\"Please provide the missing keys in the fields above or set them as environment variables.\")\n\n    st.title(\"üíº AI Job Hunting Assistant\")\n    st.info(\n        \"\"\"\n        Welcome to the AI Job Hunting Assistant! \n        Enter your job search criteria below to get job recommendations \n        and industry insights.\n        \"\"\"\n    )\n\n    col1, col2 = st.columns(2)\n    \n    with col1:\n        job_title = st.text_input(\n            \"Job Title\",\n            placeholder=\"Enter job title (e.g., Software Engineer)\",\n            help=\"Enter the job title you're looking for\"\n        )\n        \n        location = st.text_input(\n            \"Location\",\n            placeholder=\"Enter location (e.g., Bangalore, Remote)\",\n            help=\"Enter the location where you want to work\"\n        )\n\n    with col2:\n        experience_years = st.number_input(\n            \"Experience (in years)\",\n            min_value=0,\n            max_value=30,\n            value=2,\n            step=1,\n            help=\"Enter your years of experience\"\n        )\n        \n        skills_input = st.text_area(\n            \"Skills (comma separated)\",\n            placeholder=\"e.g., Python, JavaScript, React, SQL\",\n            help=\"Enter your skills separated by commas\"\n        )\n        \n        skills = [skill.strip() for skill in skills_input.split(\",\")] if skills_input else []\n\n    job_category = st.selectbox(\n        \"Industry/Job Category\",\n        options=[\n            \"Information Technology\", \n            \"Software Development\", \n            \"Data Science\", \n            \"Marketing\", \n            \"Finance\", \n            \"Healthcare\",\n            \"Education\",\n            \"Engineering\",\n            \"Sales\",\n            \"Human Resources\"\n        ],\n        help=\"Select the industry or job category you're interested in\"\n    )\n\n    if st.button(\"üîç Start Job Search\", use_container_width=True):\n        if 'job_agent' not in st.session_state:\n            st.error(\"‚ö†Ô∏è Please enter your API keys in the sidebar first!\")\n            return\n            \n        if not job_title or not location:\n            st.error(\"‚ö†Ô∏è Please enter both job title and location!\")\n            return\n            \n        if not skills:\n            st.warning(\"‚ö†Ô∏è No skills provided. Adding skills will improve job matching.\")\n            \n        try:\n            with st.spinner(\"üîç Searching for jobs...\"):\n                job_results = st.session_state.job_agent.find_jobs(\n                    job_title=job_title,\n                    location=location,\n                    experience_years=experience_years,\n                    skills=skills\n                )\n                \n                if \"An error occurred\" in job_results:\n                    st.error(job_results)\n                else:\n                    st.success(\"‚úÖ Job search completed!\")\n                    st.subheader(\"üíº Job Recommendations\")\n                    st.markdown(job_results)\n                    \n                    st.divider()\n                    \n                    with st.spinner(\"üìä Analyzing industry trends...\"):\n                        industry_trends = st.session_state.job_agent.get_industry_trends(job_category)\n                        \n                        if \"An error occurred\" in industry_trends:\n                            st.error(industry_trends)\n                        else:\n                            st.success(\"‚úÖ Industry analysis completed!\")\n                            with st.expander(f\"üìà {job_category} Industry Trends Analysis\"):\n                                st.markdown(industry_trends)\n                \n        except Exception as e:\n            error_message = str(e)\n            st.error(f\"‚ùå An error occurred: {error_message}\")\n            \n            if \"website is no longer supported\" in error_message.lower():\n                st.info(\"It appears one of the job sites is not supported by your Firecrawl API key. Please contact Firecrawl support to enable these sites for your account.\")\n            elif \"api key\" in error_message.lower():\n                st.info(\"Please check that your API keys are correct and have the necessary permissions.\")\n            else:\n                st.info(\"Please try again with different search parameters or check your internet connection.\")\n\nif __name__ == \"__main__\":\n    main() "}
{"type": "source_file", "path": "agents/gemini-agents/comparison.py", "content": "import os\nfrom dotenv import load_dotenv\nfrom crewai import Agent, Task, Crew, LLM\nfrom crewai_tools import SerperDevTool, WebsiteSearchTool\nimport streamlit as st\nimport concurrent.futures\nfrom datetime import datetime\nimport pandas as pd\nimport json\n# Load environment variables\nload_dotenv()\nos.environ[\"SERPER_API_KEY\"] = os.getenv(\"SERPER_API_KEY\")\nos.environ[\"GEMINI_API_KEY\"] = os.getenv(\"GEMINI_API_KEY\")\n\n# Initialize enhanced search tools\nsearch_tool = SerperDevTool()\nwebsite_tool = WebsiteSearchTool()\n\ndef get_llm(model_choice='gemini'):\n    \"\"\"Get the specified language model\"\"\"\n    if model_choice == 'openai':\n        return LLM(\n            model=\"openai/o1-mini\",\n            api_key=os.getenv(\"OPENAI_API_KEY\"),\n            verbose=True\n        )\n    elif model_choice == 'gemini':\n        return LLM(\n            model=\"gemini/gemini-2.0-flash\",\n            temperature=0.7,\n            google_api_key=os.getenv(\"GEMINI_API_KEY\"),\n            verbose=True\n        )\n    else:  # ollama\n        return LLM(\n            model=\"ollama/deepseek-r1:latest\",\n            base_url=\"http://localhost:11434\",\n        )\n\ndef create_agents(model_choice='gemini'):\n    \"\"\"Create specialized research and analysis agents\"\"\"\n    llm = get_llm(model_choice)\n    \n    deep_researcher = Agent(\n        role='Deep Research Specialist',\n        goal='Conduct comprehensive internet research and data gathering',\n        backstory=\"\"\"Expert at conducting deep, thorough research across multiple sources. \n        Skilled at finding hard-to-locate information and connecting disparate data points. \n        Specializes in complex research tasks that would typically take hours or days.\"\"\",\n        tools=[search_tool, website_tool],\n        llm=llm,\n        verbose=True,\n        max_iter=100,\n        allow_delegation=False,\n        max_rpm=50,\n        max_retry_limit=3\n    )\n    \n    analyst = Agent(\n        role='Research Analyst',\n        goal='Analyze and synthesize complex research findings',\n        backstory=\"\"\"Expert analyst skilled at processing large amounts of information,\n        identifying patterns, and drawing meaningful conclusions. Specializes in turning\n        raw research into actionable insights.\"\"\",\n        tools=[search_tool],\n        llm=llm,\n        verbose=True,\n        max_iter=75,\n        allow_delegation=False,\n        max_rpm=30,\n        max_retry_limit=2\n    )\n    \n    report_writer = Agent(\n        role='Research Report Writer',\n        goal='Create comprehensive, well-structured research reports',\n        backstory=\"\"\"Expert at transforming complex research and analysis into \n        clear, actionable reports. Skilled at maintaining detail while ensuring \n        accessibility and practical value.\"\"\",\n        llm=llm,\n        verbose=True,\n        max_iter=50,\n        allow_delegation=False,\n        max_rpm=20,\n        max_retry_limit=2\n    )\n    \n    return deep_researcher, analyst, report_writer\n\ndef create_tasks(researcher, analyst, writer, research_query):\n    \"\"\"Create research tasks with clear objectives\"\"\"\n    deep_research_task = Task(\n        description=f\"\"\"Conduct focused research on: {research_query}\n        \n        Step-by-step approach:\n        1. Initial broad search to identify key sources\n        2. Deep dive into most relevant sources\n        3. Extract specific details and evidence\n        4. Verify key findings across sources\n        5. Document sources and findings clearly\n        \n        Keep focused on specific, verified information.\"\"\",\n        agent=researcher,\n        expected_output=\"Detailed research findings with verified sources\"\n    )\n    \n    analysis_task = Task(\n        description=f\"\"\"Analyze the research findings about {research_query}:\n        \n        Follow these steps:\n        1. Review and categorize all findings\n        2. Identify main themes and patterns\n        3. Evaluate source credibility\n        4. Note any inconsistencies\n        5. Summarize key insights\n        \n        Focus on clear, actionable analysis.\"\"\",\n        agent=analyst,\n        context=[deep_research_task],\n        expected_output=\"Clear analysis of findings with key insights\"\n    )\n    \n    report_task = Task(\n        description=f\"\"\"Create a structured report about {research_query}:\n        \n        Include:\n        1. Executive summary (2-3 paragraphs)\n        2. Key findings (bullet points)\n        3. Supporting evidence\n        4. Conclusions\n        5. References\n        \n        Keep it clear and focused.\"\"\",\n        agent=writer,\n        context=[deep_research_task, analysis_task],\n        expected_output=\"Concise, well-structured report\"\n    )\n    \n    return [deep_research_task, analysis_task, report_task]\n\ndef create_crew(agents, tasks):\n    \"\"\"Create a crew with optimal settings\"\"\"\n    return Crew(\n        agents=agents,\n        tasks=tasks,\n        verbose=True,\n        max_rpm=100,\n        process=\"sequential\"\n    )\n\ndef run_research(model_choice, query):\n    \"\"\"Run research with specified model and return results\"\"\"\n    try:\n        start_time = datetime.now()\n        researcher, analyst, writer = create_agents(model_choice)\n        tasks = create_tasks(researcher, analyst, writer, query)\n        crew = create_crew([researcher, analyst, writer], tasks)\n        result = crew.kickoff()\n        execution_time = (datetime.now() - start_time).total_seconds()\n        return {'result': result, 'execution_time': execution_time}\n    except Exception as e:\n        return f\"Error with {model_choice}: {str(e)}\"\n\ndef save_results(query, results):\n    \"\"\"Save research results to JSON file\"\"\"\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    filename = f\"research_results_{timestamp}.json\"\n    \n    data = {\n        \"query\": query,\n        \"timestamp\": timestamp,\n        \"results\": results\n    }\n    \n    with open(filename, \"w\") as f:\n        json.dump(data, f, indent=4)\n    \n    return filename\n\ndef main():\n    st.set_page_config(page_title=\"Research Model Comparison\", layout=\"wide\")\n    \n    st.title(\"üîç Deep Research Model Comparison\")\n    \n    # Sidebar configuration\n    st.sidebar.header(\"Configuration\")\n    selected_models = st.sidebar.multiselect(\n        \"Select Models to Compare\",\n        [\"Gemini\", \"OpenAI\", \"Ollama\"],\n        default=[\"Gemini\"]\n    )\n    \n    # Convert display names to internal names\n    model_mapping = {\n        \"Gemini\": \"gemini\",\n        \"OpenAI\": \"openai\",\n        \"Ollama\": \"ollama\"\n    }\n    \n    # Main query input\n    query = st.text_area(\"Research Query\", height=100, placeholder=\"Enter your research query here...\")\n    \n    if st.button(\"Start Research\", type=\"primary\"):\n        if not query:\n            st.error(\"Please enter a research query\")\n            return\n        \n        if not selected_models:\n            st.error(\"Please select at least one model\")\n            return\n        \n        # Create progress containers\n        progress_bars = {model: st.progress(0) for model in selected_models}\n        status_containers = {model: st.empty() for model in selected_models}\n        timer_containers = {model: st.empty() for model in selected_models}\n        \n        # Initialize results dictionary\n        results = {}\n        \n        # Create columns for results\n        cols = st.columns(len(selected_models))\n        result_containers = {model: cols[i].container() for i, model in enumerate(selected_models)}\n        \n        start_times = {model: None for model in selected_models}\n        \n        # Run research for each selected model\n        with concurrent.futures.ThreadPoolExecutor() as executor:\n            future_to_model = {\n                executor.submit(run_research, model_mapping[model], query): model \n                for model in selected_models\n            }\n            \n            # Start times for each model\n            for model in selected_models:\n                start_times[model] = datetime.now()\n            \n            while future_to_model:\n                done, _ = concurrent.futures.wait(future_to_model.keys(), timeout=0.1)\n                \n                # Update running timers\n                for model in selected_models:\n                    if model in results:  # Skip if already completed\n                        continue\n                    current_time = (datetime.now() - start_times[model]).total_seconds()\n                    timer_containers[model].text(f\"‚è±Ô∏è Running Time: {current_time:.1f}s\")\n                \n                for future in done:\n                    model = future_to_model[future]\n                    try:\n                        result_data = future.result()\n                        if isinstance(result_data, dict):\n                            results[model] = result_data['result']\n                            execution_time = result_data['execution_time']\n                            \n                            progress_bars[model].progress(100)\n                            status_containers[model].success(f\"{model} Research Complete\")\n                            timer_containers[model].text(f\"‚è±Ô∏è Final Time: {execution_time:.2f}s\")\n                            \n                            with result_containers[model]:\n                                st.subheader(f\"{model} Results\")\n                                st.write(results[model])\n                        else:\n                            progress_bars[model].progress(100)\n                            status_containers[model].error(f\"Error with {model}: {result_data}\")\n                            timer_containers[model].empty()\n                    except Exception as e:\n                        progress_bars[model].progress(100)\n                        status_containers[model].error(f\"Error with {model}: {str(e)}\")\n                        timer_containers[model].empty()\n                    \n                    del future_to_model[future]\n\n        # Save results if any were generated\n        if results:\n            filename = save_results(query, results)\n            st.sidebar.success(f\"Results saved to {filename}\")\n            \n            # Create comparison table\n            st.subheader(\"Quick Comparison\")\n            comparison_data = {\n                \"Model\": list(results.keys()),\n                \"Response Length\": [len(str(r)) for r in results.values()],\n                \"Contains References\": [\"References\" in str(r) for r in results.values()],\n                \"Contains Analysis\": [\"Analysis\" in str(r) for r in results.values()]\n            }\n            comparison_df = pd.DataFrame(comparison_data)\n            st.dataframe(comparison_df)\n\nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "agents/legal/legal_team.py", "content": "import streamlit as st\nfrom agno.agent import Agent\nfrom agno.knowledge.pdf import PDFKnowledgeBase, PDFReader\nfrom agno.vectordb.qdrant import Qdrant\nfrom agno.tools.duckduckgo import DuckDuckGoTools\nfrom agno.models.openai import OpenAIChat\nfrom agno.embedder.openai import OpenAIEmbedder\nimport tempfile\nimport os\nfrom agno.document.chunking.document import DocumentChunking\n\ndef init_session_state():\n    \"\"\"Initialize session state variables\"\"\"\n    if 'openai_api_key' not in st.session_state:\n        st.session_state.openai_api_key = None\n    if 'qdrant_api_key' not in st.session_state:\n        st.session_state.qdrant_api_key = None\n    if 'qdrant_url' not in st.session_state:\n        st.session_state.qdrant_url = None\n    if 'vector_db' not in st.session_state:\n        st.session_state.vector_db = None\n    if 'legal_team' not in st.session_state:\n        st.session_state.legal_team = None\n    if 'knowledge_base' not in st.session_state:\n        st.session_state.knowledge_base = None\n    # Add a new state variable to track processed files\n    if 'processed_files' not in st.session_state:\n        st.session_state.processed_files = set()\n\nCOLLECTION_NAME = \"legal_documents\"  # Define your collection name\n\ndef init_qdrant():\n    \"\"\"Initialize Qdrant client with configured settings.\"\"\"\n    if not all([st.session_state.qdrant_api_key, st.session_state.qdrant_url]):\n        return None\n    try:\n        # Create Agno's Qdrant instance which implements VectorDb\n        vector_db = Qdrant(\n            collection=COLLECTION_NAME,\n            url=st.session_state.qdrant_url,\n            api_key=st.session_state.qdrant_api_key,\n            embedder=OpenAIEmbedder(\n                id=\"text-embedding-3-small\", \n                api_key=st.session_state.openai_api_key\n            )\n        )\n        return vector_db\n    except Exception as e:\n        st.error(f\"üî¥ Qdrant connection failed: {str(e)}\")\n        return None\n\ndef process_document(uploaded_file, vector_db: Qdrant):\n    \"\"\"\n    Process document, create embeddings and store in Qdrant vector database\n    \n    Args:\n        uploaded_file: Streamlit uploaded file object\n        vector_db (Qdrant): Initialized Qdrant instance from Agno\n    \n    Returns:\n        PDFKnowledgeBase: Initialized knowledge base with processed documents\n    \"\"\"\n    if not st.session_state.openai_api_key:\n        raise ValueError(\"OpenAI API key not provided\")\n        \n    os.environ['OPENAI_API_KEY'] = st.session_state.openai_api_key\n    \n    try:\n        # Save the uploaded file to a temporary location\n        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_file:\n            temp_file.write(uploaded_file.getvalue())\n            temp_file_path = temp_file.name\n        \n        st.info(\"Loading and processing document...\")\n        \n        # Create a PDFKnowledgeBase with the vector_db\n        knowledge_base = PDFKnowledgeBase(\n            path=temp_file_path,  # Single string path, not a list\n            vector_db=vector_db,\n            reader=PDFReader(),\n            chunking_strategy=DocumentChunking(\n                chunk_size=1000,\n                overlap=200\n            )\n        )\n        \n        # Load the documents into the knowledge base\n        with st.spinner('üì§ Loading documents into knowledge base...'):\n            try:\n                knowledge_base.load(recreate=True, upsert=True)\n                st.success(\"‚úÖ Documents stored successfully!\")\n            except Exception as e:\n                st.error(f\"Error loading documents: {str(e)}\")\n                raise\n        \n        # Clean up the temporary file\n        try:\n            os.unlink(temp_file_path)\n        except Exception:\n            pass\n            \n        return knowledge_base\n            \n    except Exception as e:\n        st.error(f\"Document processing error: {str(e)}\")\n        raise Exception(f\"Error processing document: {str(e)}\")\n\ndef main():\n    st.set_page_config(page_title=\"Legal Document Analyzer\", layout=\"wide\")\n    init_session_state()\n\n    st.title(\"AI Legal Agent Team üë®‚Äç‚öñÔ∏è\")\n\n    with st.sidebar:\n        st.header(\"üîë API Configuration\")\n   \n        openai_key = st.text_input(\n            \"OpenAI API Key\",\n            type=\"password\",\n            value=st.session_state.openai_api_key if st.session_state.openai_api_key else \"\",\n            help=\"Enter your OpenAI API key\"\n        )\n        if openai_key:\n            st.session_state.openai_api_key = openai_key\n\n        qdrant_key = st.text_input(\n            \"Qdrant API Key\",\n            type=\"password\",\n            value=st.session_state.qdrant_api_key if st.session_state.qdrant_api_key else \"\",\n            help=\"Enter your Qdrant API key\"\n        )\n        if qdrant_key:\n            st.session_state.qdrant_api_key = qdrant_key\n\n        qdrant_url = st.text_input(\n            \"Qdrant URL\",\n            value=st.session_state.qdrant_url if st.session_state.qdrant_url else \"\",\n            help=\"Enter your Qdrant instance URL\"\n        )\n        if qdrant_url:\n            st.session_state.qdrant_url = qdrant_url\n\n        if all([st.session_state.qdrant_api_key, st.session_state.qdrant_url]):\n            try:\n                if not st.session_state.vector_db:\n                    # Make sure we're initializing a QdrantClient here\n                    st.session_state.vector_db = init_qdrant()\n                    if st.session_state.vector_db:\n                        st.success(\"Successfully connected to Qdrant!\")\n            except Exception as e:\n                st.error(f\"Failed to connect to Qdrant: {str(e)}\")\n\n        st.divider()\n\n        if all([st.session_state.openai_api_key, st.session_state.vector_db]):\n            st.header(\"üìÑ Document Upload\")\n            uploaded_file = st.file_uploader(\"Upload Legal Document\", type=['pdf'])\n            \n            if uploaded_file:\n                # Check if this file has already been processed\n                if uploaded_file.name not in st.session_state.processed_files:\n                    with st.spinner(\"Processing document...\"):\n                        try:\n                            # Process the document and get the knowledge base\n                            knowledge_base = process_document(uploaded_file, st.session_state.vector_db)\n                            \n                            if knowledge_base:\n                                st.session_state.knowledge_base = knowledge_base\n                                # Add the file to processed files\n                                st.session_state.processed_files.add(uploaded_file.name)\n                                \n                                # Initialize agents\n                                legal_researcher = Agent(\n                                    name=\"Legal Researcher\",\n                                    role=\"Legal research specialist\",\n                                    model=OpenAIChat(id=\"gpt-4o\"),\n                                    tools=[DuckDuckGoTools()],\n                                    knowledge=st.session_state.knowledge_base,\n                                    search_knowledge=True,\n                                    instructions=[\n                                        \"Find and cite relevant legal cases and precedents\",\n                                        \"Provide detailed research summaries with sources\",\n                                        \"Reference specific sections from the uploaded document\",\n                                        \"Always search the knowledge base for relevant information\"\n                                    ],\n                                    show_tool_calls=True,\n                                    markdown=True\n                                )\n\n                                contract_analyst = Agent(\n                                    name=\"Contract Analyst\",\n                                    role=\"Contract analysis specialist\",\n                                    model=OpenAIChat(id=\"gpt-4o\"),\n                                    knowledge=st.session_state.knowledge_base,\n                                    search_knowledge=True,\n                                    instructions=[\n                                        \"Review contracts thoroughly\",\n                                        \"Identify key terms and potential issues\",\n                                        \"Reference specific clauses from the document\"\n                                    ],\n                                    markdown=True\n                                )\n\n                                legal_strategist = Agent(\n                                    name=\"Legal Strategist\", \n                                    role=\"Legal strategy specialist\",\n                                    model=OpenAIChat(id=\"gpt-4o\"),\n                                    knowledge=st.session_state.knowledge_base,\n                                    search_knowledge=True,\n                                    instructions=[\n                                        \"Develop comprehensive legal strategies\",\n                                        \"Provide actionable recommendations\",\n                                        \"Consider both risks and opportunities\"\n                                    ],\n                                    markdown=True\n                                )\n\n                                # Legal Agent Team\n                                st.session_state.legal_team = Agent(\n                                    name=\"Legal Team Lead\",\n                                    role=\"Legal team coordinator\",\n                                    model=OpenAIChat(id=\"gpt-4o\"),\n                                    team=[legal_researcher, contract_analyst, legal_strategist],\n                                    knowledge=st.session_state.knowledge_base,\n                                    search_knowledge=True,\n                                    instructions=[\n                                        \"Coordinate analysis between team members\",\n                                        \"Provide comprehensive responses\",\n                                        \"Ensure all recommendations are properly sourced\",\n                                        \"Reference specific parts of the uploaded document\",\n                                        \"Always search the knowledge base before delegating tasks\"\n                                    ],\n                                    show_tool_calls=True,\n                                    markdown=True\n                                )\n                                \n                                st.success(\"‚úÖ Document processed and team initialized!\")\n                                \n                        except Exception as e:\n                            st.error(f\"Error processing document: {str(e)}\")\n                else:\n                    # File already processed, just show a message\n                    st.success(\"‚úÖ Document already processed and team ready!\")\n\n            st.divider()\n            st.header(\"üîç Analysis Options\")\n            analysis_type = st.selectbox(\n                \"Select Analysis Type\",\n                [\n                    \"Contract Review\",\n                    \"Legal Research\",\n                    \"Risk Assessment\",\n                    \"Compliance Check\",\n                    \"Custom Query\"\n                ]\n            )\n        else:\n            st.warning(\"Please configure all API credentials to proceed\")\n\n    # Main content area\n    if not all([st.session_state.openai_api_key, st.session_state.vector_db]):\n        st.info(\"üëà Please configure your API credentials in the sidebar to begin\")\n    elif not uploaded_file:\n        st.info(\"üëà Please upload a legal document to begin analysis\")\n    elif st.session_state.legal_team:\n        # Create a dictionary for analysis type icons\n        analysis_icons = {\n            \"Contract Review\": \"üìë\",\n            \"Legal Research\": \"üîç\",\n            \"Risk Assessment\": \"‚ö†Ô∏è\",\n            \"Compliance Check\": \"‚úÖ\",\n            \"Custom Query\": \"üí≠\"\n        }\n\n        # Dynamic header with icon\n        st.header(f\"{analysis_icons[analysis_type]} {analysis_type} Analysis\")\n  \n        analysis_configs = {\n            \"Contract Review\": {\n                \"query\": \"Review this contract and identify key terms, obligations, and potential issues.\",\n                \"agents\": [\"Contract Analyst\"],\n                \"description\": \"Detailed contract analysis focusing on terms and obligations\"\n            },\n            \"Legal Research\": {\n                \"query\": \"Research relevant cases and precedents related to this document.\",\n                \"agents\": [\"Legal Researcher\"],\n                \"description\": \"Research on relevant legal cases and precedents\"\n            },\n            \"Risk Assessment\": {\n                \"query\": \"Analyze potential legal risks and liabilities in this document.\",\n                \"agents\": [\"Contract Analyst\", \"Legal Strategist\"],\n                \"description\": \"Combined risk analysis and strategic assessment\"\n            },\n            \"Compliance Check\": {\n                \"query\": \"Check this document for regulatory compliance issues.\",\n                \"agents\": [\"Legal Researcher\", \"Contract Analyst\", \"Legal Strategist\"],\n                \"description\": \"Comprehensive compliance analysis\"\n            },\n            \"Custom Query\": {\n                \"query\": None,\n                \"agents\": [\"Legal Researcher\", \"Contract Analyst\", \"Legal Strategist\"],\n                \"description\": \"Custom analysis using all available agents\"\n            }\n        }\n\n        st.info(f\"üìã {analysis_configs[analysis_type]['description']}\")\n        st.write(f\"ü§ñ Active Legal AI Agents: {', '.join(analysis_configs[analysis_type]['agents'])}\")  #dictionary!!\n\n        # Replace the existing user_query section with this:\n        if analysis_type == \"Custom Query\":\n            user_query = st.text_area(\n                \"Enter your specific query:\",\n                help=\"Add any specific questions or points you want to analyze\"\n            )\n        else:\n            user_query = None  # Set to None for non-custom queries\n\n\n        if st.button(\"Analyze\"):\n            if analysis_type == \"Custom Query\" and not user_query:\n                st.warning(\"Please enter a query\")\n            else:\n                with st.spinner(\"Analyzing document...\"):\n                    try:\n                        # Ensure OpenAI API key is set\n                        os.environ['OPENAI_API_KEY'] = st.session_state.openai_api_key\n                        \n                        # Combine predefined and user queries\n                        if analysis_type != \"Custom Query\":\n                            combined_query = f\"\"\"\n                            Using the uploaded document as reference:\n                            \n                            Primary Analysis Task: {analysis_configs[analysis_type]['query']}\n                            Focus Areas: {', '.join(analysis_configs[analysis_type]['agents'])}\n                            \n                            Please search the knowledge base and provide specific references from the document.\n                            \"\"\"\n                        else:\n                            combined_query = f\"\"\"\n                            Using the uploaded document as reference:\n                            \n                            {user_query}\n                            \n                            Please search the knowledge base and provide specific references from the document.\n                            Focus Areas: {', '.join(analysis_configs[analysis_type]['agents'])}\n                            \"\"\"\n\n                        response = st.session_state.legal_team.run(combined_query)\n                        \n                        # Display results in tabs\n                        tabs = st.tabs([\"Analysis\", \"Key Points\", \"Recommendations\"])\n                        \n                        with tabs[0]:\n                            st.markdown(\"### Detailed Analysis\")\n                            if response.content:\n                                st.markdown(response.content)\n                            else:\n                                for message in response.messages:\n                                    if message.role == 'assistant' and message.content:\n                                        st.markdown(message.content)\n                        \n                        with tabs[1]:\n                            st.markdown(\"### Key Points\")\n                            key_points_response = st.session_state.legal_team.run(\n                                f\"\"\"Based on this previous analysis:    \n                                {response.content}\n                                \n                                Please summarize the key points in bullet points.\n                                Focus on insights from: {', '.join(analysis_configs[analysis_type]['agents'])}\"\"\"\n                            )\n                            if key_points_response.content:\n                                st.markdown(key_points_response.content)\n                            else:\n                                for message in key_points_response.messages:\n                                    if message.role == 'assistant' and message.content:\n                                        st.markdown(message.content)\n                        \n                        with tabs[2]:\n                            st.markdown(\"### Recommendations\")\n                            recommendations_response = st.session_state.legal_team.run(\n                                f\"\"\"Based on this previous analysis:\n                                {response.content}\n                                \n                                What are your key recommendations based on the analysis, the best course of action?\n                                Provide specific recommendations from: {', '.join(analysis_configs[analysis_type]['agents'])}\"\"\"\n                            )\n                            if recommendations_response.content:\n                                st.markdown(recommendations_response.content)\n                            else:\n                                for message in recommendations_response.messages:\n                                    if message.role == 'assistant' and message.content:\n                                        st.markdown(message.content)\n\n                    except Exception as e:\n                        st.error(f\"Error during analysis: {str(e)}\")\n    else:\n        st.info(\"Please upload a legal document to begin analysis\")\n\nif __name__ == \"__main__\":\n    main() "}
{"type": "source_file", "path": "agents/openai/basic-agent.py", "content": "import os\nimport json\nimport streamlit as st\nimport time\nimport asyncio\nfrom datetime import datetime\nfrom typing import List, Dict, Any, Optional\nfrom pathlib import Path\nimport base64\nfrom io import BytesIO\n\n# Import OpenAI Python SDK\nimport openai\nfrom openai import OpenAI\n\n# Import OpenAI Agents SDK with proper imports based on documentation\nfrom openai import Agent, AgentRunner\nfrom openai.types.beta.thread import Message\nfrom openai.tool import function_tool\nfrom openai.tools.web_search import WebSearchTool\nfrom openai.tools.file_search import FileSearchTool\n\n# Create a directory for storing knowledge base files if it doesn't exist\nKNOWLEDGE_BASE_DIR = Path(\"knowledge_base\")\nKNOWLEDGE_BASE_DIR.mkdir(exist_ok=True)\n\n# Set up Vector Store for file search if not already created\ndef setup_vector_store():\n    client = OpenAI()\n    # Check if we already have a vector store\n    vector_stores = client.vector_stores.list()\n    vector_store_id = None\n    \n    # Look for our research agent vector store\n    for vs in vector_stores.data:\n        if vs.name == \"research_agent_files\":\n            vector_store_id = vs.id\n            break\n    \n    # Create a new vector store if one doesn't exist\n    if not vector_store_id:\n        vector_store = client.vector_stores.create(name=\"research_agent_files\")\n        vector_store_id = vector_store.id\n    \n    return vector_store_id\n\n# Upload a file to the vector store\ndef upload_file_to_vector_store(file_path, vector_store_id):\n    client = OpenAI()\n    \n    # First upload the file to get a file ID\n    with open(file_path, \"rb\") as file:\n        file_obj = client.files.create(\n            file=file,\n            purpose=\"assistants\"\n        )\n    \n    # Add the file to the vector store\n    client.vector_stores.files.create(\n        vector_store_id=vector_store_id,\n        file_id=file_obj.id\n    )\n    \n    return file_obj.id\n\n# Create Research Agent using Agents SDK\ndef create_research_agent(model=\"gpt-4o-mini\"):\n    # Set up tools\n    web_search_tool = WebSearchTool()\n    \n    # Create the agent with web search capability\n    agent = Agent(\n        name=\"Research Assistant\",\n        instructions=\"\"\"You are a highly efficient research assistant that helps users find and synthesize information. \n        Your goal is to provide comprehensive, accurate, and well-cited answers to research questions.\n        \n        When given a research task:\n        1. Break down complex questions into manageable parts\n        2. Use web search to find the most current and relevant information\n        3. If file search is available, look for relevant information in the user's knowledge base\n        4. Synthesize information from multiple sources\n        5. Always cite your sources with proper links\n        6. Provide balanced perspectives on controversial topics\n        7. Clarify when information might be outdated or uncertain\n        8. When appropriate, suggest follow-up questions or areas for further research\n        \n        Respond in a clear, organized manner with headings and sections for complex topics.\n        \"\"\",\n        model=model,\n        tools=[web_search_tool]\n    )\n    \n    return agent\n\n# Create a File Search Agent that specifically searches through the vector store\ndef create_file_search_agent(vector_store_id, model=\"gpt-4o-mini\"):\n    # Set up the file search tool with the vector store\n    file_search_tool = FileSearchTool(vector_store_ids=[vector_store_id])\n    \n    # Create the agent\n    agent = Agent(\n        name=\"File Search Assistant\",\n        instructions=\"\"\"You are a knowledge base search specialist that helps find information in the user's documents.\n        Use the file search tool to locate relevant information in the user's knowledge base.\n        Provide accurate information based only on the content of the files.\n        Always cite which file the information came from.\n        If you can't find relevant information in the files, clearly state that.\n        \"\"\",\n        model=model,\n        tools=[file_search_tool]\n    )\n    \n    return agent\n\n# Create a main orchestrator agent that can delegate to specialized agents\ndef create_orchestrator_agent(file_search_agent, model=\"gpt-4o-mini\"):\n    # Define a function to handle research with file search\n    @function_tool\n    async def research_with_files(query: str) -> str:\n        \"\"\"\n        Research a topic using the knowledge base files.\n        \"\"\"\n        trace = await AgentRunner.run(file_search_agent, query)\n        return trace.final_output\n    \n    # Set up web search\n    web_search_tool = WebSearchTool()\n    \n    # Create the orchestrator agent\n    agent = Agent(\n        name=\"Research Orchestrator\",\n        instructions=\"\"\"You are a comprehensive research assistant that coordinates different specialized tools to help users.\n        \n        For general research questions, use web search first to find up-to-date information.\n        \n        When a user asks about information that might be in their knowledge base or private documents, use the research_with_files tool.\n        \n        Always consider which tool is most appropriate for the task:\n        - Web search for current events, general knowledge, or public information\n        - File search for personal documents or specialized knowledge base content\n        \n        Provide comprehensive, well-organized answers with proper citations.\n        \"\"\",\n        model=model,\n        tools=[web_search_tool, research_with_files]\n    )\n    \n    return agent\n\n# Run a research query using the appropriate agent(s)\nasync def run_research(query, agent, include_trace=False):\n    # Execute the research\n    trace = await AgentRunner.run(agent, query)\n    \n    # Extract results\n    result = {\n        \"text\": trace.final_output,\n        \"sources\": []\n    }\n    \n    # Try to extract sources from the trace\n    try:\n        # Extract web search URLs if present\n        for event in trace.events:\n            if event.name == \"tool_call\" and event.data.get(\"tool_name\") == \"web_search\":\n                web_results = event.data.get(\"output\", {}).get(\"web_results\", [])\n                if web_results:\n                    for web_result in web_results:\n                        if web_result.get(\"url\") and web_result.get(\"title\"):\n                            result[\"sources\"].append({\n                                \"title\": web_result.get(\"title\"),\n                                \"url\": web_result.get(\"url\")\n                            })\n            \n            # Extract file search sources if present\n            elif event.name == \"tool_call\" and event.data.get(\"tool_name\") == \"file_search\":\n                file_results = event.data.get(\"output\", {}).get(\"results\", [])\n                if file_results:\n                    for file_result in file_results:\n                        if file_result.get(\"file_id\") and file_result.get(\"file_name\"):\n                            result[\"sources\"].append({\n                                \"title\": file_result.get(\"file_name\"),\n                                \"file_id\": file_result.get(\"file_id\")\n                            })\n    except Exception as e:\n        print(f\"Error extracting sources: {e}\")\n    \n    # Include the trace if requested\n    if include_trace:\n        result[\"trace\"] = trace\n    \n    return result\n\n# Streamlit UI\ndef main():\n    st.set_page_config(page_title=\"Research Agent\", page_icon=\"üîç\", layout=\"wide\")\n    \n    # Check if OpenAI API key is set\n    api_key = os.environ.get(\"OPENAI_API_KEY\")\n    if not api_key:\n        api_key = st.sidebar.text_input(\"Enter OpenAI API Key\", type=\"password\")\n        if api_key:\n            os.environ[\"OPENAI_API_KEY\"] = api_key\n            st.sidebar.success(\"API key set successfully!\")\n        else:\n            st.error(\"Please enter your OpenAI API key in the sidebar to continue.\")\n            return\n    \n    st.title(\"Research Agent with OpenAI Agents SDK\")\n    st.markdown(\"This application uses the OpenAI Agents SDK to conduct research using web search and file search capabilities.\")\n    \n    # Sidebar configuration options\n    st.sidebar.header(\"Configuration\")\n    \n    # Model selection\n    model = st.sidebar.selectbox(\n        \"Select Model\",\n        [\"gpt-4o-mini\", \"gpt-4o\"],\n        index=0\n    )\n    \n    # Initialize session state\n    if 'vector_store_id' not in st.session_state:\n        try:\n            with st.spinner(\"Setting up knowledge base...\"):\n                st.session_state.vector_store_id = setup_vector_store()\n        except Exception as e:\n            st.error(f\"Error setting up vector store: {str(e)}\")\n            st.session_state.vector_store_id = None\n    \n    if 'research_agent' not in st.session_state or st.session_state.get('current_model') != model:\n        with st.spinner(\"Initializing research agent...\"):\n            st.session_state.research_agent = create_research_agent(model=model)\n            if st.session_state.vector_store_id:\n                st.session_state.file_search_agent = create_file_search_agent(st.session_state.vector_store_id, model=model)\n                st.session_state.orchestrator_agent = create_orchestrator_agent(\n                    st.session_state.file_search_agent,\n                    model=model\n                )\n            st.session_state.current_model = model\n    \n    # Helper function to run async functions in Streamlit\n    def run_async(coroutine):\n        return asyncio.run(coroutine)\n    \n    # Tabs for different features\n    tab1, tab2, tab3 = st.tabs([\n        \"Research\", \n        \"Knowledge Base\", \n        \"Trace Viewer\"\n    ])\n    \n    # Research Tab\n    with tab1:\n        st.header(\"Research Assistant\")\n        \n        # Agent selection\n        agent_type = st.radio(\n            \"Select research mode:\",\n            [\"Web Search\", \"File Search\", \"Combined (Orchestrator)\"],\n            horizontal=True,\n            disabled=not st.session_state.vector_store_id and (agent_type in [\"File Search\", \"Combined (Orchestrator)\"])\n        )\n        \n        if not st.session_state.vector_store_id and agent_type in [\"File Search\", \"Combined (Orchestrator)\"]:\n            st.warning(\"Vector store not available. Please check your setup.\")\n            agent_type = \"Web Search\"\n        \n        query = st.text_area(\"Research query:\", height=100)\n        \n        col1, col2 = st.columns([1, 5])\n        with col1:\n            include_trace = st.checkbox(\"Include trace\", value=False)\n        \n        with col2:\n            if st.button(\"Research\", key=\"research_button\"):\n                if query:\n                    with st.spinner(\"Researching...\"):\n                        try:\n                            # Select the appropriate agent\n                            if agent_type == \"Web Search\":\n                                research_agent = st.session_state.research_agent\n                            elif agent_type == \"File Search\":\n                                if st.session_state.vector_store_id:\n                                    research_agent = st.session_state.file_search_agent\n                                else:\n                                    st.error(\"File search not available - falling back to web search\")\n                                    research_agent = st.session_state.research_agent\n                            else:  # Combined\n                                if st.session_state.vector_store_id:\n                                    research_agent = st.session_state.orchestrator_agent\n                                else:\n                                    st.error(\"Orchestrator not available - falling back to web search\")\n                                    research_agent = st.session_state.research_agent\n                            \n                            # Run the research\n                            results = run_async(run_research(query, research_agent, include_trace))\n                            \n                            # Display results\n                            st.markdown(\"## Research Results\")\n                            st.write(results[\"text\"])\n                            \n                            # Display sources if available\n                            if results[\"sources\"]:\n                                st.markdown(\"### Sources\")\n                                for i, source in enumerate(results[\"sources\"], 1):\n                                    if \"url\" in source:\n                                        st.markdown(f\"{i}. [{source['title']}]({source['url']})\")\n                                    elif \"file_id\" in source:\n                                        st.markdown(f\"{i}. File: {source['title']}\")\n                            \n                            # Store trace for viewing\n                            if include_trace and \"trace\" in results:\n                                st.session_state.last_trace = results[\"trace\"]\n                                st.info(\"Trace captured and available in the Trace Viewer tab.\")\n                        except Exception as e:\n                            st.error(f\"Error during research: {str(e)}\")\n                else:\n                    st.warning(\"Please enter a research query.\")\n    \n    # Knowledge Base Tab\n    with tab2:\n        st.header(\"Knowledge Base Management\")\n        \n        if not st.session_state.vector_store_id:\n            st.error(\"Vector store is not available. Please check your API key and permissions.\")\n        else:\n            # Display files in knowledge base\n            st.subheader(\"Upload Files to Knowledge Base\")\n            \n            uploaded_file = st.file_uploader(\"Choose a file\", type=[\"pdf\", \"txt\", \"docx\", \"csv\", \"json\", \"md\"])\n            \n            if uploaded_file is not None:\n                # Save the uploaded file temporarily\n                file_path = os.path.join(KNOWLEDGE_BASE_DIR, uploaded_file.name)\n                with open(file_path, \"wb\") as f:\n                    f.write(uploaded_file.getbuffer())\n                \n                # Upload to vector store\n                if st.button(\"Add to Knowledge Base\"):\n                    with st.spinner(\"Processing file...\"):\n                        try:\n                            file_id = upload_file_to_vector_store(file_path, st.session_state.vector_store_id)\n                            st.success(f\"File added to knowledge base! File ID: {file_id}\")\n                        except Exception as e:\n                            st.error(f\"Error adding file to knowledge base: {str(e)}\")\n            \n            # Display knowledge base files\n            st.subheader(\"Knowledge Base Files\")\n            \n            # Placeholder for listing files in the vector store\n            # (In a full implementation, we would list files from the vector store)\n            # For now, we'll list local files in the knowledge_base directory\n            \n            if os.path.exists(KNOWLEDGE_BASE_DIR):\n                files = list(KNOWLEDGE_BASE_DIR.glob(\"*\"))\n                if files:\n                    st.write(f\"Files in local knowledge base directory:\")\n                    for file in files:\n                        st.write(f\"- {file.name}\")\n                else:\n                    st.info(\"No files in local knowledge base directory.\")\n            \n            # Add a query box for testing file search\n            st.subheader(\"Test File Search\")\n            file_query = st.text_input(\"Query your knowledge base:\")\n            \n            if st.button(\"Search Files\"):\n                if file_query:\n                    with st.spinner(\"Searching knowledge base...\"):\n                        try:\n                            results = run_async(run_research(file_query, st.session_state.file_search_agent))\n                            \n                            st.markdown(\"### Search Results\")\n                            st.write(results[\"text\"])\n                        except Exception as e:\n                            st.error(f\"Error searching knowledge base: {str(e)}\")\n                else:\n                    st.warning(\"Please enter a search query.\")\n    \n    # Trace Viewer Tab\n    with tab3:\n        st.header(\"Trace Viewer\")\n        \n        if 'last_trace' in st.session_state:\n            trace = st.session_state.last_trace\n            \n            st.subheader(\"Trace Information\")\n            st.write(f\"Agent Name: {trace.agent_info.name}\")\n            st.write(f\"Model: {trace.agent_info.model}\")\n            \n            st.subheader(\"Events\")\n            for i, event in enumerate(trace.events):\n                with st.expander(f\"Event {i+1}: {event.name}\"):\n                    st.write(f\"Timestamp: {event.timestamp}\")\n                    st.write(f\"Event Type: {event.name}\")\n                    \n                    # Display different information based on event type\n                    if event.name == \"tool_call\":\n                        st.write(f\"Tool: {event.data.get('tool_name')}\")\n                        st.write(\"Input:\")\n                        st.code(json.dumps(event.data.get(\"input\"), indent=2), language=\"json\")\n                        \n                        if \"output\" in event.data:\n                            st.write(\"Output:\")\n                            st.code(json.dumps(event.data.get(\"output\"), indent=2), language=\"json\")\n                    \n                    elif event.name == \"agent_message\":\n                        role = event.data.get(\"role\")\n                        content = event.data.get(\"content\")\n                        st.write(f\"Role: {role}\")\n                        st.write(f\"Content: {content}\")\n                    \n                    # Show all event data for debugging\n                    with st.expander(\"Raw Event Data\"):\n                        st.code(json.dumps(event.data, indent=2), language=\"json\")\n            \n            # Display final output\n            st.subheader(\"Final Output\")\n            st.write(trace.final_output)\n        else:\n            st.info(\"No trace available. Run a research query with 'Include trace' checked to see trace data here.\")\n    \n    # Footer\n    st.sidebar.markdown(\"---\")\n    st.sidebar.markdown(\"### About\")\n    st.sidebar.info(\n        \"This research agent uses the OpenAI Agents SDK to provide comprehensive \"\n        \"research capabilities including web search and file search.\"\n    )\n    \n    # Instructions for setup\n    st.sidebar.markdown(\"### Setup Guide\")\n    with st.sidebar.expander(\"Installation Instructions\"):\n        st.markdown(\"\"\"\n        To set up this application correctly:\n        \n        1. Install the required packages:\n        ```bash\n        pip install openai openai-agents streamlit\n        ```\n        \n        2. Set your OpenAI API key as an environment variable:\n        ```bash\n        export OPENAI_API_KEY='your-api-key'\n        ```\n        \n        3. Run the Streamlit app:\n        ```bash\n        streamlit run app.py\n        ```\n        \"\"\")\n\nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "agents/gemini-agents/main.py", "content": "import os\nfrom dotenv import load_dotenv\nfrom crewai import Agent, Task, Crew, LLM\nfrom crewai_tools import SerperDevTool, WebsiteSearchTool\nfrom langchain_openai import ChatOpenAI\nfrom langchain_community.llms import Ollama\nfrom langchain_google_genai import ChatGoogleGenerativeAI\n\n# Load environment variables\nload_dotenv()\nos.environ[\"SERPER_API_KEY\"] = os.getenv(\"SERPER_API_KEY\")\nos.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n\n# Initialize enhanced search tools\nsearch_tool = SerperDevTool()\nwebsite_tool = WebsiteSearchTool()\n\ndef get_llm(model_choice='gemini'):\n    \"\"\"Get the specified language model\"\"\"\n    if model_choice == 'openai':\n        return ChatOpenAI(\n            model_name=\"o3-mini\",\n        )\n    elif model_choice == 'gemini':\n        return LLM(\n            model=\"gemini/gemini-2.0-flash\",\n            temperature=0.7,\n            google_api_key=os.getenv(\"GOOGLE_API_KEY\"),\n            verbose=True\n        )\n    else:  # ollama\n        return Ollama(\n            model=\"deepseek-r1:latest\",\n            base_url=\"http://localhost:11434\",\n            temperature=0.7\n        )\n\ndef create_agents(model_choice='gemini'):\n    \"\"\"Create specialized research and analysis agents\"\"\"\n    llm = get_llm(model_choice)\n    \n    deep_researcher = Agent(\n        role='Deep Research Specialist',\n        goal='Conduct comprehensive internet research and data gathering',\n        backstory=\"\"\"Expert at conducting deep, thorough research across multiple sources. \n        Skilled at finding hard-to-locate information and connecting disparate data points. \n        Specializes in complex research tasks that would typically take hours or days.\"\"\",\n        tools=[search_tool, website_tool],\n        llm=llm,\n        verbose=True,\n        max_iter=100,\n        allow_delegation=False,\n        max_rpm=50,\n        max_retry_limit=3\n    )\n    \n    analyst = Agent(\n        role='Research Analyst',\n        goal='Analyze and synthesize complex research findings',\n        backstory=\"\"\"Expert analyst skilled at processing large amounts of information,\n        identifying patterns, and drawing meaningful conclusions. Specializes in turning\n        raw research into actionable insights.\"\"\",\n        tools=[search_tool],\n        llm=llm,\n        verbose=True,\n        max_iter=75,\n        allow_delegation=False,\n        max_rpm=30,\n        max_retry_limit=2\n    )\n    \n    report_writer = Agent(\n        role='Research Report Writer',\n        goal='Create comprehensive, well-structured research reports',\n        backstory=\"\"\"Expert at transforming complex research and analysis into \n        clear, actionable reports. Skilled at maintaining detail while ensuring \n        accessibility and practical value.\"\"\",\n        llm=llm,\n        verbose=True,\n        max_iter=50,\n        allow_delegation=False,\n        max_rpm=20,\n        max_retry_limit=2\n    )\n    \n    return deep_researcher, analyst, report_writer\n\ndef create_tasks(researcher, analyst, writer, research_query):\n    \"\"\"Create research tasks with clear objectives\"\"\"\n    deep_research_task = Task(\n        description=f\"\"\"Conduct focused research on: {research_query}\n        \n        Step-by-step approach:\n        1. Initial broad search to identify key sources\n        2. Deep dive into most relevant sources\n        3. Extract specific details and evidence\n        4. Verify key findings across sources\n        5. Document sources and findings clearly\n        \n        Keep focused on specific, verified information.\"\"\",\n        agent=researcher,\n        expected_output=\"Detailed research findings with verified sources\"\n    )\n    \n    analysis_task = Task(\n        description=f\"\"\"Analyze the research findings about {research_query}:\n        \n        Follow these steps:\n        1. Review and categorize all findings\n        2. Identify main themes and patterns\n        3. Evaluate source credibility\n        4. Note any inconsistencies\n        5. Summarize key insights\n        \n        Focus on clear, actionable analysis.\"\"\",\n        agent=analyst,\n        context=[deep_research_task],\n        expected_output=\"Clear analysis of findings with key insights\"\n    )\n    \n    report_task = Task(\n        description=f\"\"\"Create a structured report about {research_query}:\n        \n        Include:\n        1. Executive summary (2-3 paragraphs)\n        2. Key findings (bullet points)\n        3. Supporting evidence\n        4. Conclusions\n        5. References\n        \n        Keep it clear and focused.\"\"\",\n        agent=writer,\n        context=[deep_research_task, analysis_task],\n        expected_output=\"Concise, well-structured report\"\n    )\n    \n    return [deep_research_task, analysis_task, report_task]\n\ndef create_crew(agents, tasks):\n    \"\"\"Create a crew with optimal settings\"\"\"\n    return Crew(\n        agents=agents,\n        tasks=tasks,\n        verbose=True,\n        max_rpm=100,\n        process=\"sequential\"\n    )\n\ndef main():\n    print(\"\\nüîç Welcome to Deep Research Crew!\")\n    print(\"\\nAvailable Models:\")\n    print(\"1. Google Gemini 1.5 Pro\")\n    print(\"2. OpenAI o3-mini (Requires API key)\")\n    print(\"3. Local DeepSeek-r1 (Requires Ollama)\")\n    \n    choice = input(\"\\nSelect model (1-3): \").strip()\n    model_choice = {\n        '1': 'gemini',\n        '2': 'openai',\n        '3': 'ollama'\n    }.get(choice, 'gemini')\n    \n    if model_choice == 'ollama':\n        print(\"\\nUsing Ollama with DeepSeek-r1\")\n        print(\"Ensure Ollama is running: ollama run deepseek-r1:latest\")\n    \n    query = input(\"\\nWhat would you like researched? (Be specific): \")\n    \n    try:\n        researcher, analyst, writer = create_agents(model_choice)\n        tasks = create_tasks(researcher, analyst, writer, query)\n        crew = create_crew([researcher, analyst, writer], tasks)\n        \n        print(\"\\nüîç Starting deep research process...\")\n        result = crew.kickoff()\n        \n        print(\"\\nüìä Research Report:\")\n        print(\"==================\")\n        print(result)\n        \n    except Exception as e:\n        print(f\"\\n‚ùå Error: {str(e)}\")\n        if model_choice == 'openai':\n            print(\"\\nTip: Check your OpenAI API key\")\n        elif model_choice == 'gemini':\n            print(\"\\nTip: Check your Google API key\")\n        else:\n            print(\"\\nTip: Ensure Ollama is running with deepseek-r1:latest\")\n            print(\"Run: ollama run deepseek-r1:latest\")\n\nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "agents/marketing/customer-support.py/customer-agent.py", "content": "import streamlit as st\nfrom openai import OpenAI\nfrom mem0 import Memory\nimport os\nimport json\nfrom datetime import datetime, timedelta\nfrom dotenv import load_dotenv, set_key\nimport pathlib\n\n# Get the absolute path to the .env file\nenv_path = pathlib.Path(os.path.join(os.getcwd(), '.env'))\n\n# Load environment variables from .env file\nload_dotenv(dotenv_path=env_path)\n\n# Set up the Streamlit App\nst.set_page_config(page_title=\"AI Customer Support Agent\", layout=\"wide\")\nst.title(\"AI Customer Support Agent with Memory üõí\")\nst.caption(\"Chat with a customer support assistant who remembers your past interactions.\")\n\n# Initialize session state for API keys if not already set\nif \"api_keys_initialized\" not in st.session_state:\n    # Get API keys from environment variables\n    st.session_state.env_openai_api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n    st.session_state.env_qdrant_url = os.getenv(\"QDRANT_URL\", \"localhost\")\n    st.session_state.env_qdrant_port = os.getenv(\"QDRANT_PORT\", \"6333\")\n    st.session_state.env_openai_model = os.getenv(\"OPENAI_MODEL_ID\", \"gpt-4\")\n    \n    # Initialize the working API keys with environment values\n    st.session_state.openai_api_key = st.session_state.env_openai_api_key\n    st.session_state.qdrant_url = st.session_state.env_qdrant_url\n    st.session_state.qdrant_port = st.session_state.env_qdrant_port\n    st.session_state.openai_model = st.session_state.env_openai_model\n    \n    st.session_state.api_keys_initialized = True\n\n# Function to save API keys to .env file\ndef save_api_keys_to_env():\n    try:\n        # Save OpenAI API key\n        if st.session_state.openai_api_key:\n            set_key(env_path, \"OPENAI_API_KEY\", st.session_state.openai_api_key)\n            \n        # Save Qdrant URL and port\n        if st.session_state.qdrant_url:\n            set_key(env_path, \"QDRANT_URL\", st.session_state.qdrant_url)\n        \n        if st.session_state.qdrant_port:\n            set_key(env_path, \"QDRANT_PORT\", st.session_state.qdrant_port)\n            \n        # Save OpenAI model\n        if st.session_state.openai_model:\n            set_key(env_path, \"OPENAI_MODEL_ID\", st.session_state.openai_model)\n            \n        # Update environment variables in session state\n        st.session_state.env_openai_api_key = st.session_state.openai_api_key\n        st.session_state.env_qdrant_url = st.session_state.qdrant_url\n        st.session_state.env_qdrant_port = st.session_state.qdrant_port\n        st.session_state.env_openai_model = st.session_state.openai_model\n        \n        return True\n    except Exception as e:\n        st.error(f\"Error saving API keys to .env file: {str(e)}\")\n        return False\n\n# Sidebar for API key management\nwith st.sidebar:\n    st.title(\"Configuration\")\n    \n    # API Key Management Section\n    with st.expander(\"API Key Management\", expanded=False):\n        st.info(\"API keys from .env file are used by default. You can override them here.\")\n        \n        # Function to handle API key updates\n        def update_api_key(key_name, env_key_name, password=True, help_text=\"\"):\n            input_type = \"password\" if password else \"default\"\n            new_value = st.text_input(\n                f\"{key_name}\", \n                value=st.session_state[env_key_name] if st.session_state[env_key_name] else \"\",\n                type=input_type,\n                help=help_text\n            )\n            \n            # Only update if user entered something or if we have an env value\n            if new_value:\n                st.session_state[key_name.lower()] = new_value\n                return True\n            elif st.session_state[env_key_name]:\n                st.session_state[key_name.lower()] = st.session_state[env_key_name]\n                return True\n            return False\n        \n        # API keys inputs\n        has_openai = update_api_key(\n            \"OpenAI API Key\", \n            \"env_openai_api_key\", \n            password=True,\n            help_text=\"Enter your OpenAI API key\"\n        )\n        \n        # OpenAI model selection\n        openai_models = [\"gpt-4\", \"gpt-4o\", \"gpt-4-turbo\", \"gpt-3.5-turbo\"]\n        selected_model = st.selectbox(\n            \"OpenAI Model\",\n            options=openai_models,\n            index=openai_models.index(st.session_state.openai_model) if st.session_state.openai_model in openai_models else 0,\n            help=\"Select the OpenAI model to use\"\n        )\n        st.session_state.openai_model = selected_model\n        has_model = True\n        \n        # Qdrant configuration\n        st.subheader(\"Qdrant Vector Store Settings\")\n        has_qdrant_url = update_api_key(\n            \"Qdrant Host\", \n            \"env_qdrant_url\", \n            password=False,\n            help_text=\"Enter your Qdrant host (default: localhost)\"\n        )\n        \n        has_qdrant_port = update_api_key(\n            \"Qdrant Port\", \n            \"env_qdrant_port\", \n            password=False,\n            help_text=\"Enter your Qdrant port (default: 6333)\"\n        )\n        \n        # Buttons for API key management\n        col1, col2 = st.columns(2)\n        with col1:\n            if st.button(\"Reset to .env values\"):\n                st.session_state.openai_api_key = st.session_state.env_openai_api_key\n                st.session_state.qdrant_url = st.session_state.env_qdrant_url\n                st.session_state.qdrant_port = st.session_state.env_qdrant_port\n                st.session_state.openai_model = st.session_state.env_openai_model\n                st.experimental_rerun()\n        \n        with col2:\n            if st.button(\"Save to .env file\"):\n                if save_api_keys_to_env():\n                    st.success(\"Configuration saved to .env file!\")\n                    st.experimental_rerun()\n    \n    # Display API status\n    api_status_ok = has_openai and has_qdrant_url and has_qdrant_port and has_model\n    \n    if api_status_ok:\n        st.success(\"‚úÖ All required configurations are set\")\n    else:\n        missing_keys = []\n        if not has_openai:\n            missing_keys.append(\"OpenAI API Key\")\n        if not has_qdrant_url:\n            missing_keys.append(\"Qdrant Host\")\n        if not has_qdrant_port:\n            missing_keys.append(\"Qdrant Port\")\n        \n        st.error(f\"‚ùå Missing configuration: {', '.join(missing_keys)}\")\n    \n    # Separator\n    st.markdown(\"---\")\n    \n    # Customer Management Section\n    st.subheader(\"Customer Management\")\n    previous_customer_id = st.session_state.get(\"previous_customer_id\", None)\n    customer_id = st.text_input(\"Enter Customer ID\")\n\n    if customer_id != previous_customer_id:\n        st.session_state.messages = []\n        st.session_state.previous_customer_id = customer_id\n        st.session_state.customer_data = None\n\n    # Customer data management buttons\n    if customer_id:\n        col1, col2 = st.columns(2)\n        with col1:\n            if st.button(\"Generate Data\"):\n                if api_status_ok:\n                    with st.spinner(\"Generating customer data...\"):\n                        # Initialize the agent here to ensure we're using the latest config\n                        if 'support_agent' in locals():\n                            st.session_state.customer_data = support_agent.generate_synthetic_data(customer_id)\n                        else:\n                            st.error(\"Agent not initialized. Please check your configuration.\")\n                    if st.session_state.customer_data:\n                        st.success(\"Data generated!\")\n                    else:\n                        st.error(\"Failed to generate data.\")\n                else:\n                    st.error(\"Configure API keys first.\")\n        \n        with col2:\n            if st.button(\"View Profile\"):\n                if st.session_state.get(\"customer_data\"):\n                    st.session_state.show_profile = True\n                else:\n                    st.info(\"Generate data first.\")\n    else:\n        st.info(\"Enter a customer ID to manage customer data.\")\n    \n    # Show customer profile if requested\n    if st.session_state.get(\"show_profile\", False) and st.session_state.get(\"customer_data\"):\n        with st.expander(\"Customer Profile\", expanded=True):\n            st.json(st.session_state.customer_data)\n            if st.button(\"Hide Profile\"):\n                st.session_state.show_profile = False\n                st.experimental_rerun()\n\n# Define the CustomerSupportAIAgent class outside the conditional block\nclass CustomerSupportAIAgent:\n    def __init__(self):\n        # Check if API keys are configured\n        if not st.session_state.get(\"openai_api_key\"):\n            st.error(\"OpenAI API key is not configured.\")\n            st.stop()\n            \n        # Set the OpenAI API key\n        os.environ['OPENAI_API_KEY'] = st.session_state.openai_api_key\n            \n        # Initialize Mem0 with Qdrant as the vector store\n        config = {\n            \"vector_store\": {\n                \"provider\": \"qdrant\",\n                \"config\": {\n                    \"host\": st.session_state.qdrant_url,\n                    \"port\": int(st.session_state.qdrant_port),\n                }\n            },\n        }\n        try:\n            self.memory = Memory.from_config(config)\n        except Exception as e:\n            st.error(f\"Failed to initialize memory: {e}\")\n            st.stop()  # Stop execution if memory initialization fails\n\n        self.client = OpenAI()\n        self.app_id = \"customer-support\"\n        self.model = st.session_state.openai_model\n\n    def handle_query(self, query, user_id=None):\n        try:\n            # Search for relevant memories\n            relevant_memories = self.memory.search(query=query, user_id=user_id)\n            \n            # Build context from relevant memories\n            context = \"Relevant past information:\\n\"\n            if relevant_memories and \"results\" in relevant_memories:\n                for memory in relevant_memories[\"results\"]:\n                    if \"memory\" in memory:\n                        context += f\"- {memory['memory']}\\n\"\n\n            # Generate a response using OpenAI\n            full_prompt = f\"{context}\\nCustomer: {query}\\nSupport Agent:\"\n            response = self.client.chat.completions.create(\n                model=self.model,\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are a customer support AI agent for TechGadgets.com, an online electronics store.\"},\n                    {\"role\": \"user\", \"content\": full_prompt}\n                ]\n            )\n            answer = response.choices[0].message.content\n\n            # Add the query and answer to memory\n            self.memory.add(query, user_id=user_id, metadata={\"app_id\": self.app_id, \"role\": \"user\"})\n            self.memory.add(answer, user_id=user_id, metadata={\"app_id\": self.app_id, \"role\": \"assistant\"})\n\n            return answer\n        except Exception as e:\n            st.error(f\"An error occurred while handling the query: {e}\")\n            return \"Sorry, I encountered an error. Please try again later.\"\n\n    def get_memories(self, user_id=None):\n        try:\n            # Retrieve all memories for a user\n            return self.memory.get_all(user_id=user_id)\n        except Exception as e:\n            st.error(f\"Failed to retrieve memories: {e}\")\n            return None\n\n    def generate_synthetic_data(self, user_id: str) -> dict | None:\n        try:\n            today = datetime.now()\n            order_date = (today - timedelta(days=10)).strftime(\"%B %d, %Y\")\n            expected_delivery = (today + timedelta(days=2)).strftime(\"%B %d, %Y\")\n\n            prompt = f\"\"\"Generate a detailed customer profile and order history for a TechGadgets.com customer with ID {user_id}. Include:\n            1. Customer name and basic info\n            2. A recent order of a high-end electronic device (placed on {order_date}, to be delivered by {expected_delivery})\n            3. Order details (product, price, order number)\n            4. Customer's shipping address\n            5. 2-3 previous orders from the past year\n            6. 2-3 customer service interactions related to these orders\n            7. Any preferences or patterns in their shopping behavior\n\n            Format the output as a JSON object.\"\"\"\n\n            response = self.client.chat.completions.create(\n                model=self.model,\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are a data generation AI that creates realistic customer profiles and order histories. Always respond with valid JSON.\"},\n                    {\"role\": \"user\", \"content\": prompt}\n                ]\n            )\n\n            customer_data = json.loads(response.choices[0].message.content)\n\n            # Add generated data to memory\n            for key, value in customer_data.items():\n                if isinstance(value, list):\n                    for item in value:\n                        self.memory.add(\n                            json.dumps(item), \n                            user_id=user_id, \n                            metadata={\"app_id\": self.app_id, \"role\": \"system\"}\n                        )\n                else:\n                    self.memory.add(\n                        f\"{key}: {json.dumps(value)}\", \n                        user_id=user_id, \n                        metadata={\"app_id\": self.app_id, \"role\": \"system\"}\n                    )\n\n            return customer_data\n        except Exception as e:\n            st.error(f\"Failed to generate synthetic data: {e}\")\n            return None\n\n# Main chat interface\nif api_status_ok:\n    # Initialize the CustomerSupportAIAgent\n    support_agent = CustomerSupportAIAgent()\n    \n    if customer_id:\n        # Memory viewer\n        with st.expander(\"Memory Viewer\", expanded=False):\n            if st.button(\"View Memory\"):\n                memories = support_agent.get_memories(user_id=customer_id)\n                if memories and \"results\" in memories and memories[\"results\"]:\n                    st.write(f\"Memory for customer **{customer_id}**:\")\n                    for memory in memories[\"results\"]:\n                        if \"memory\" in memory:\n                            st.write(f\"- {memory['memory']}\")\n                else:\n                    st.info(\"No memory found for this customer ID.\")\n        \n        # Initialize the chat history\n        if \"messages\" not in st.session_state:\n            st.session_state.messages = []\n\n        # Display the chat history\n        st.subheader(f\"Chat with Customer Support (ID: {customer_id})\")\n        for message in st.session_state.messages:\n            with st.chat_message(message[\"role\"]):\n                st.markdown(message[\"content\"])\n\n        # Accept user input\n        query = st.chat_input(\"How can I assist you today?\")\n\n        if query:\n            # Add user message to chat history\n            st.session_state.messages.append({\"role\": \"user\", \"content\": query})\n            with st.chat_message(\"user\"):\n                st.markdown(query)\n\n            # Generate and display response\n            with st.spinner(\"Generating response...\"):\n                answer = support_agent.handle_query(query, user_id=customer_id)\n\n            # Add assistant response to chat history\n            st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer})\n            with st.chat_message(\"assistant\"):\n                st.markdown(answer)\n    else:\n        st.info(\"üëà Please enter a customer ID in the sidebar to start chatting.\")\nelse:\n    st.warning(\"‚ö†Ô∏è Please configure your API keys in the sidebar to use the customer support agent.\")"}
{"type": "source_file", "path": "agents/marketing/lead-generation/agents.py", "content": "import streamlit as st\nimport requests\nfrom agno.agent import Agent\nfrom agno.tools.firecrawl import FirecrawlTools\nfrom agno.models.openai import OpenAIChat\nfrom firecrawl import FirecrawlApp\nfrom pydantic import BaseModel, Field\nfrom typing import List\nfrom composio_phidata import Action, ComposioToolSet\nimport json\nimport os\nfrom dotenv import load_dotenv, set_key\nimport pathlib\n\n# Get the absolute path to the .env file\nenv_path = pathlib.Path(os.path.join(os.getcwd(), '.env'))\n\n# Load environment variables from .env file\nload_dotenv(dotenv_path=env_path)\n\nclass QuoraUserInteractionSchema(BaseModel):\n    username: str = Field(description=\"The username of the user who posted the question or answer\")\n    bio: str = Field(description=\"The bio or description of the user\")\n    post_type: str = Field(description=\"The type of post, either 'question' or 'answer'\")\n    timestamp: str = Field(description=\"When the question or answer was posted\")\n    upvotes: int = Field(default=0, description=\"Number of upvotes received\")\n    links: List[str] = Field(default_factory=list, description=\"Any links included in the post\")\n\nclass QuoraPageSchema(BaseModel):\n    interactions: List[QuoraUserInteractionSchema] = Field(description=\"List of all user interactions (questions and answers) on the page\")\n\ndef search_for_urls(company_description: str, firecrawl_api_key: str, num_links: int) -> List[str]:\n    url = \"https://api.firecrawl.dev/v1/search\"\n    headers = {\n        \"Authorization\": f\"Bearer {firecrawl_api_key}\",\n        \"Content-Type\": \"application/json\"\n    }\n    query1 = f\"quora websites where people are looking for {company_description} services\"\n    payload = {\n        \"query\": query1,\n        \"limit\": num_links,\n        \"lang\": \"en\",\n        \"location\": \"United States\",\n        \"timeout\": 60000,\n    }\n    response = requests.post(url, json=payload, headers=headers)\n    if response.status_code == 200:\n        data = response.json()\n        if data.get(\"success\"):\n            results = data.get(\"data\", [])\n            return [result[\"url\"] for result in results]\n    return []\n\ndef extract_user_info_from_urls(urls: List[str], firecrawl_api_key: str) -> List[dict]:\n    user_info_list = []\n    firecrawl_app = FirecrawlApp(api_key=firecrawl_api_key)\n    \n    try:\n        for url in urls:\n            response = firecrawl_app.extract(\n                [url],\n                {\n                    'prompt': \"\"\"\n                    Extract information about all users who have posted questions or answers on this Quora page.\n                    For each user, extract their username, bio, the type of post (question or answer), \n                    when it was posted, number of upvotes, and any links they included.\n                    \"\"\",\n                    'schema': QuoraPageSchema.model_json_schema(),\n                }\n            )\n            \n            if response.get('success') and response.get('data'):\n                extracted_data = response['data']\n                if 'interactions' in extracted_data:\n                    for interaction in extracted_data['interactions']:\n                        user_info = {\n                            'url': url,\n                            'username': interaction.get('username', 'Unknown'),\n                            'bio': interaction.get('bio', 'No bio available'),\n                            'post_type': interaction.get('post_type', 'Unknown'),\n                            'timestamp': interaction.get('timestamp', 'Unknown'),\n                            'upvotes': interaction.get('upvotes', 0),\n                            'links': interaction.get('links', [])\n                        }\n                        user_info_list.append(user_info)\n    except Exception as e:\n        st.error(f\"Error extracting user info: {str(e)}\")\n    \n    return user_info_list\n\ndef format_user_info_to_flattened_json(user_info_list: List[dict]) -> List[dict]:\n    flattened_data = []\n    \n    for user_info in user_info_list:\n        flattened_user = {\n            'url': user_info.get('url', ''),\n            'username': user_info.get('username', ''),\n            'bio': user_info.get('bio', ''),\n            'post_type': user_info.get('post_type', ''),\n            'timestamp': user_info.get('timestamp', ''),\n            'upvotes': user_info.get('upvotes', 0),\n            'links': ', '.join(user_info.get('links', [])),\n        }\n        flattened_data.append(flattened_user)\n    \n    return flattened_data\n\ndef create_google_sheets_agent(composio_api_key: str, openai_api_key: str) -> Agent:\n    composio_tools = ComposioToolSet(\n        api_key=composio_api_key,\n        actions=[Action.GOOGLE_SHEETS_CREATE]\n    )\n    \n    return Agent(\n        model=OpenAIChat(id=\"gpt-4\", api_key=openai_api_key),\n        tools=[composio_tools],\n        show_tool_calls=True,\n        markdown=True\n    )\n\ndef write_to_google_sheets(flattened_data: List[dict], composio_api_key: str, openai_api_key: str) -> str:\n    if not flattened_data:\n        return \"\"\n    \n    agent = create_google_sheets_agent(composio_api_key, openai_api_key)\n    \n    json_data = json.dumps(flattened_data, indent=2)\n    \n    response = agent.run(\n        f\"\"\"Create a Google Sheet with the following data:\n        {json_data}\n        \n        Format it nicely with appropriate column headers and return the link to the sheet.\n        \"\"\"\n    )\n    \n    # Extract the Google Sheets link from the response\n    content = response.content\n    if \"https://docs.google.com/spreadsheets\" in content:\n        for line in content.split('\\n'):\n            if \"https://docs.google.com/spreadsheets\" in line:\n                return line.strip()\n    \n    return \"\"\n\ndef create_prompt_transformation_agent(openai_api_key: str) -> Agent:\n    return Agent(\n        model=OpenAIChat(id=\"gpt-4\", api_key=openai_api_key),\n        system_prompt=\"\"\"You are an expert at transforming verbose product/service descriptions into concise, targeted phrases for search queries.\nYour task is to take a detailed description and extract the core product or service being offered, condensing it into 3-4 words.\n\nExamples:\nInput: \"We're looking for businesses that need help with their social media marketing, especially those struggling with content creation and engagement\"\nOutput: \"social media marketing\"\n\nInput: \"Need to find businesses interested in implementing machine learning solutions for fraud detection\"\nOutput: \"ML fraud detection\"\n\nAlways focus on the core product/service and keep it concise but clear.\"\"\",\n        markdown=True\n    )\n\n# Function to save API keys to .env file\ndef save_api_keys_to_env():\n    try:\n        # Save OpenAI API key\n        if st.session_state.openai_api_key:\n            set_key(env_path, \"OPENAI_API_KEY\", st.session_state.openai_api_key)\n            \n        # Save Firecrawl API key\n        if st.session_state.firecrawl_api_key:\n            set_key(env_path, \"FIRECRAWL_API_KEY\", st.session_state.firecrawl_api_key)\n            \n        # Save Composio API key\n        if st.session_state.composio_api_key:\n            set_key(env_path, \"COMPOSIO_API_KEY\", st.session_state.composio_api_key)\n            \n        # Update environment variables in session state\n        st.session_state.env_openai_api_key = st.session_state.openai_api_key\n        st.session_state.env_firecrawl_api_key = st.session_state.firecrawl_api_key\n        st.session_state.env_composio_api_key = st.session_state.composio_api_key\n        \n        return True\n    except Exception as e:\n        st.error(f\"Error saving API keys to .env file: {str(e)}\")\n        return False\n\ndef main():\n    st.set_page_config(page_title=\"AI Lead Generation Agent\", layout=\"wide\")\n    st.title(\"üéØ AI Lead Generation Agent\")\n    st.info(\"This firecrawl powered agent helps you generate leads from Quora by searching for relevant posts and extracting user information.\")\n\n    # Initialize session state for API keys if not already set\n    if \"api_keys_initialized\" not in st.session_state:\n        # Get API keys from environment variables\n        st.session_state.env_openai_api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n        st.session_state.env_firecrawl_api_key = os.getenv(\"FIRECRAWL_API_KEY\", \"\")\n        st.session_state.env_composio_api_key = os.getenv(\"COMPOSIO_API_KEY\", \"\")\n        \n        # Initialize the working API keys with environment values\n        st.session_state.openai_api_key = st.session_state.env_openai_api_key\n        st.session_state.firecrawl_api_key = st.session_state.env_firecrawl_api_key\n        st.session_state.composio_api_key = st.session_state.env_composio_api_key\n        \n        st.session_state.api_keys_initialized = True\n\n    with st.sidebar:\n        st.header(\"API Keys\")\n        \n        # API Key Management Section\n        with st.expander(\"Configure API Keys\", expanded=False):\n            st.info(\"API keys from .env file are used by default. You can override them here.\")\n            \n            # Function to handle API key updates\n            def update_api_key(key_name, env_key_name, help_text=\"\"):\n                new_value = st.text_input(\n                    f\"{key_name}\", \n                    value=st.session_state[env_key_name] if st.session_state[env_key_name] else \"\",\n                    type=\"password\",\n                    help=help_text\n                )\n                \n                # Only update if user entered something or if we have an env value\n                if new_value:\n                    st.session_state[key_name.lower()] = new_value\n                    return True\n                elif st.session_state[env_key_name]:\n                    st.session_state[key_name.lower()] = st.session_state[env_key_name]\n                    return True\n                return False\n            \n            # API keys inputs\n            has_firecrawl = update_api_key(\n                \"Firecrawl API Key\", \n                \"env_firecrawl_api_key\", \n                help_text=\"Get your Firecrawl API key from [Firecrawl's website](https://www.firecrawl.dev/app/api-keys)\"\n            )\n            \n            has_openai = update_api_key(\n                \"OpenAI API Key\", \n                \"env_openai_api_key\", \n                help_text=\"Get your OpenAI API key from [OpenAI's website](https://platform.openai.com/api-keys)\"\n            )\n            \n            has_composio = update_api_key(\n                \"Composio API Key\", \n                \"env_composio_api_key\", \n                help_text=\"Get your Composio API key from [Composio's website](https://composio.ai)\"\n            )\n            \n            # Buttons for API key management\n            col1, col2 = st.columns(2)\n            with col1:\n                if st.button(\"Reset to .env values\"):\n                    st.session_state.openai_api_key = st.session_state.env_openai_api_key\n                    st.session_state.firecrawl_api_key = st.session_state.env_firecrawl_api_key\n                    st.session_state.composio_api_key = st.session_state.env_composio_api_key\n                    st.experimental_rerun()\n            \n            with col2:\n                if st.button(\"Save to .env file\"):\n                    if save_api_keys_to_env():\n                        st.success(\"API keys saved to .env file!\")\n                        st.experimental_rerun()\n        \n        # Display API status\n        api_status_ok = has_openai and has_firecrawl and has_composio\n        \n        if api_status_ok:\n            st.success(\"‚úÖ All required API keys are configured\")\n        else:\n            missing_keys = []\n            if not has_openai:\n                missing_keys.append(\"OpenAI API Key\")\n            if not has_firecrawl:\n                missing_keys.append(\"Firecrawl API Key\")\n            if not has_composio:\n                missing_keys.append(\"Composio API Key\")\n            \n            st.error(f\"‚ùå Missing API keys: {', '.join(missing_keys)}\")\n        \n        # Search settings\n        st.subheader(\"Search Settings\")\n        num_links = st.number_input(\"Number of links to search\", min_value=1, max_value=10, value=3)\n        \n        if st.button(\"Reset Session\"):\n            # Keep API keys but clear other session state\n            api_keys = {\n                \"api_keys_initialized\": st.session_state.api_keys_initialized,\n                \"env_openai_api_key\": st.session_state.env_openai_api_key,\n                \"env_firecrawl_api_key\": st.session_state.env_firecrawl_api_key,\n                \"env_composio_api_key\": st.session_state.env_composio_api_key,\n                \"openai_api_key\": st.session_state.openai_api_key,\n                \"firecrawl_api_key\": st.session_state.firecrawl_api_key,\n                \"composio_api_key\": st.session_state.composio_api_key\n            }\n            st.session_state.clear()\n            for key, value in api_keys.items():\n                st.session_state[key] = value\n            st.experimental_rerun()\n\n    user_query = st.text_area(\n        \"Describe what kind of leads you're looking for:\",\n        placeholder=\"e.g., Looking for users who need automated video editing software with AI capabilities\",\n        help=\"Be specific about the product/service and target audience. The AI will convert this into a focused search query.\"\n    )\n\n    if st.button(\"Generate Leads\"):\n        if not api_status_ok:\n            st.error(\"Please configure all required API keys in the sidebar.\")\n        elif not user_query:\n            st.error(\"Please describe what leads you're looking for.\")\n        else:\n            with st.spinner(\"Processing your query...\"):\n                transform_agent = create_prompt_transformation_agent(st.session_state.openai_api_key)\n                company_description = transform_agent.run(f\"Transform this query into a concise 3-4 word company description: {user_query}\")\n                st.write(\"üéØ Searching for:\", company_description.content)\n            \n            with st.spinner(\"Searching for relevant URLs...\"):\n                urls = search_for_urls(company_description.content, st.session_state.firecrawl_api_key, num_links)\n            \n            if urls:\n                st.subheader(\"Quora Links Used:\")\n                for url in urls:\n                    st.write(url)\n                \n                with st.spinner(\"Extracting user info from URLs...\"):\n                    user_info_list = extract_user_info_from_urls(urls, st.session_state.firecrawl_api_key)\n                \n                with st.spinner(\"Formatting user info...\"):\n                    flattened_data = format_user_info_to_flattened_json(user_info_list)\n                \n                with st.spinner(\"Writing to Google Sheets...\"):\n                    google_sheets_link = write_to_google_sheets(flattened_data, st.session_state.composio_api_key, st.session_state.openai_api_key)\n                \n                if google_sheets_link:\n                    st.success(\"Lead generation and data writing to Google Sheets completed successfully!\")\n                    st.subheader(\"Google Sheets Link:\")\n                    st.markdown(f\"[View Google Sheet]({google_sheets_link})\")\n                else:\n                    st.error(\"Failed to retrieve the Google Sheets link.\")\n            else:\n                st.warning(\"No relevant URLs found.\")\n\nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "agents/game/main.py", "content": "import streamlit as st\nfrom openai import OpenAI\nfrom agno.agent import Agent as AgnoAgent\nfrom agno.models.openai import OpenAIChat as AgnoOpenAIChat\nfrom langchain_openai import ChatOpenAI \nimport asyncio\nfrom browser_use import Browser\n\nst.set_page_config(page_title=\"PyGame Code Generator\", layout=\"wide\")\n\n# Initialize session state\nif \"api_keys\" not in st.session_state:\n    st.session_state.api_keys = {\n        \"deepseek\": \"\",\n        \"openai\": \"\"\n    }\n\n# Streamlit sidebar for API keys\nwith st.sidebar:\n    st.title(\"API Keys Configuration\")\n    st.session_state.api_keys[\"deepseek\"] = st.text_input(\n        \"DeepSeek API Key\",\n        type=\"password\",\n        value=st.session_state.api_keys[\"deepseek\"]\n    )\n    st.session_state.api_keys[\"openai\"] = st.text_input(\n        \"OpenAI API Key\",\n        type=\"password\",\n        value=st.session_state.api_keys[\"openai\"]\n    )\n    \n    st.markdown(\"---\")\n    st.info(\"\"\"\n    üìù How to use:\n    1. Enter your API keys above\n    2. Write your PyGame visualization query\n    3. Click 'Generate Code' to get the code\n    4. Click 'Generate Visualization' to:\n       - Open Trinket.io PyGame editor\n       - Copy and paste the generated code\n       - Watch it run automatically\n    \"\"\")\n\n# Main UI\nst.title(\"üéÆ AI 3D Visualizer with DeepSeek R1\")\nexample_query = \"Create a particle system simulation where 100 particles emit from the mouse position and respond to keyboard-controlled wind forces\"\nquery = st.text_area(\n    \"Enter your PyGame query:\",\n    height=70,\n    placeholder=f\"e.g.: {example_query}\"\n)\n\n# Split the buttons into columns\ncol1, col2 = st.columns(2)\ngenerate_code_btn = col1.button(\"Generate Code\")\ngenerate_vis_btn = col2.button(\"Generate Visualization\")\n\nif generate_code_btn and query:\n    if not st.session_state.api_keys[\"deepseek\"] or not st.session_state.api_keys[\"openai\"]:\n        st.error(\"Please provide both API keys in the sidebar\")\n        st.stop()\n\n    # Initialize Deepseek client\n    deepseek_client = OpenAI(\n        api_key=st.session_state.api_keys[\"deepseek\"],\n        base_url=\"https://api.deepseek.com\"\n    )\n\n    system_prompt = \"\"\"You are a Pygame and Python Expert that specializes in making games and visualisation through pygame and python programming. \n    During your reasoning and thinking, include clear, concise, and well-formatted Python code in your reasoning. \n    Always include explanations for the code you provide.\"\"\"\n\n    try:\n        # Get reasoning from Deepseek\n        with st.spinner(\"Generating solution...\"):\n            deepseek_response = deepseek_client.chat.completions.create(\n                model=\"deepseek-reasoner\",\n                messages=[\n                    {\"role\": \"system\", \"content\": system_prompt},\n                    {\"role\": \"user\", \"content\": query}\n                ],\n                max_tokens=1  \n            )\n\n        reasoning_content = deepseek_response.choices[0].message.reasoning_content\n        print(\"\\nDeepseek Reasoning:\\n\", reasoning_content)\n        with st.expander(\"R1's Reasoning\"):      \n            st.write(reasoning_content)\n\n        # Initialize Claude agent (using PhiAgent)\n        openai_agent = AgnoAgent(\n            model=AgnoOpenAIChat(\n                id=\"gpt-4o\",\n                api_key=st.session_state.api_keys[\"openai\"]\n            ),\n            show_tool_calls=True,\n            markdown=True\n        )\n\n        # Extract code\n        extraction_prompt = f\"\"\"Extract ONLY the Python code from the following content which is reasoning of a particular query to make a pygame script. \n        Return nothing but the raw code without any explanations, or markdown backticks:\n        {reasoning_content}\"\"\"\n\n        with st.spinner(\"Extracting code...\"):\n            code_response = openai_agent.run(extraction_prompt)\n            extracted_code = code_response.content\n\n        # Store the generated code in session state\n        st.session_state.generated_code = extracted_code\n        \n        # Display the code\n        with st.expander(\"Generated PyGame Code\", expanded=True):      \n            st.code(extracted_code, language=\"python\")\n            \n        st.success(\"Code generated successfully! Click 'Generate Visualization' to run it.\")\n\n    except Exception as e:\n        st.error(f\"An error occurred: {str(e)}\")\n\nelif generate_vis_btn:\n    if \"generated_code\" not in st.session_state:\n        st.warning(\"Please generate code first before visualization\")\n    else:\n        async def run_pygame_on_trinket(code: str) -> None:\n            browser = Browser()\n            from browser_use import Agent \n            async with await browser.new_context() as context:\n                model = ChatOpenAI(\n                    model=\"gpt-4o\", \n                    api_key=st.session_state.api_keys[\"openai\"]\n                )\n                \n                agent1 = Agent(\n                    task='Go to https://trinket.io/features/pygame, thats your only job.',\n                    llm=model,\n                    browser_context=context,\n                )\n                \n                executor = Agent(\n                    task='Executor. Execute the code written by the User by clicking on the run button on the right. ',\n                    llm=model,\n                    browser_context=context\n                )\n\n                coder = Agent(\n                    task='Coder. Your job is to wait for the user for 10 seconds to write the code in the code editor.',\n                    llm=model,\n                    browser_context=context\n                )\n                \n                viewer = Agent(\n                    task='Viewer. Your job is to just view the pygame window for 10 seconds.',\n                    llm=model,\n                    browser_context=context,\n                )\n\n                with st.spinner(\"Running code on Trinket...\"):\n                    try:\n                        await agent1.run()\n                        await coder.run()\n                        await executor.run()\n                        await viewer.run()\n                        st.success(\"Code is running on Trinket!\")\n                    except Exception as e:\n                        st.error(f\"Error running code on Trinket: {str(e)}\")\n                        st.info(\"You can still copy the code above and run it manually on Trinket\")\n\n        # Run the async function with the stored code\n        asyncio.run(run_pygame_on_trinket(st.session_state.generated_code))\n\nelif generate_code_btn and not query:\n    st.warning(\"Please enter a query before generating code\")"}
{"type": "source_file", "path": "agents/health/main.py", "content": "import os\nimport streamlit as st\nfrom dotenv import load_dotenv\nfrom crewai import Agent, Task, Crew, LLM\nfrom crewai_tools import SerperDevTool\nfrom langchain_openai import ChatOpenAI\n\n# Load environment variables\nload_dotenv()\nos.environ[\"SERPER_API_KEY\"] = os.getenv(\"SERPER_API_KEY\")\nos.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n\n# Initialize the search tool\nsearch_tool = SerperDevTool()\n\ndef get_llm():\n    return LLM(\n        model=\"openai/o1-mini\",\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        verbose=True\n    )\n\ndef create_agents():\n    \"\"\"Create the specialized nutrition agents.\"\"\"\n    llm = get_llm()\n    \n    # Nutrition Researcher\n    nutritionist = Agent(\n        role='Nutrition Specialist',\n        goal='Research and develop personalized nutritional recommendations based on scientific evidence',\n        backstory='''You are a highly qualified nutritionist with expertise in therapeutic diets, \n                    nutrient interactions, and dietary requirements across different health conditions. \n                    Your recommendations are always backed by peer-reviewed research.''',\n        tools=[search_tool],\n        llm=llm,\n        verbose=True\n    )\n    \n    # Medical Nutrition Specialist\n    medical_specialist = Agent(\n        role='Medical Nutrition Therapist',\n        goal='Analyze medical conditions and provide appropriate dietary modifications',\n        backstory='''With dual training in medicine and nutrition, you specialize in managing \n                    nutrition-related aspects of various medical conditions. You understand \n                    medication-food interactions and how to optimize nutrition within medical constraints.''',\n        tools=[search_tool],\n        llm=llm,\n        verbose=True\n    )\n    \n    # Diet Plan Creator\n    diet_planner = Agent(\n        role='Therapeutic Diet Planner',\n        goal='Create detailed, practical and enjoyable meal plans tailored to individual needs',\n        backstory='''You excel at transforming clinical nutrition requirements into delicious, \n                    practical eating plans. You have extensive knowledge of food preparation, \n                    nutrient preservation, and food combinations that optimize both health and enjoyment.''',\n        llm=llm,\n        verbose=True\n    )\n    \n    return nutritionist, medical_specialist, diet_planner\n\ndef create_tasks(nutritionist, medical_specialist, diet_planner, user_info):\n    \"\"\"Create tasks for each agent based on user information.\"\"\"\n    \n    # First task: Research nutrition needs based on demographics\n    demographics_research = Task(\n        description=f'''Research nutritional needs for an individual with the following demographics:\n            - Age: {user_info['age']}\n            - Gender: {user_info['gender']}\n            - Height: {user_info['height']}\n            - Weight: {user_info['weight']}\n            - Activity Level: {user_info['activity_level']}\n            - Goals: {user_info['goals']}\n            \n            Provide detailed nutritional requirements including:\n            1. Caloric needs (basal and adjusted for activity)\n            2. Macronutrient distribution (proteins, carbs, fats)\n            3. Key micronutrients particularly important for this demographic\n            4. Hydration requirements\n            5. Meal timing and frequency recommendations''',\n        agent=nutritionist,\n        expected_output=\"A comprehensive nutritional profile with scientific rationale\"\n    )\n    \n    # Second task: Analyze medical conditions and adjust nutritional recommendations\n    medical_analysis = Task(\n        description=f'''Analyze the following medical conditions and medications, then provide dietary modifications:\n            - Medical Conditions: {user_info['medical_conditions']}\n            - Medications: {user_info['medications']}\n            - Allergies/Intolerances: {user_info['allergies']}\n            \n            Consider the baseline nutritional profile and provide:\n            1. Specific nutrients to increase or limit based on each condition\n            2. Food-medication interactions to avoid\n            3. Potential nutrient deficiencies associated with these conditions/medications\n            4. Foods that may help manage symptoms or improve outcomes\n            5. Foods to strictly avoid''',\n        agent=medical_specialist,\n        context=[demographics_research],\n        expected_output=\"A detailed analysis of medical nutrition therapy adjustments\"\n    )\n    \n    # Third task: Create the comprehensive diet plan\n    diet_plan = Task(\n        description=f'''Create a detailed, practical diet plan incorporating all information:\n            - User's Food Preferences: {user_info['food_preferences']}\n            - Cooking Skills/Time: {user_info['cooking_ability']}\n            - Budget Constraints: {user_info['budget']}\n            - Cultural/Religious Factors: {user_info['cultural_factors']}\n            \n            Develop a comprehensive nutrition plan that includes:\n            1. Specific foods to eat daily, weekly, and occasionally with portion sizes\n            2. A 7-day meal plan with specific meals and recipes\n            3. Grocery shopping list with specific items\n            4. Meal preparation tips and simple recipes\n            5. Eating out guidelines and suggested restaurant options/orders\n            6. Supplement recommendations if necessary (with scientific justification)\n            7. Hydration schedule and recommended beverages\n            8. How to monitor progress and potential adjustments over time''',\n        agent=diet_planner,\n        context=[demographics_research, medical_analysis],\n        expected_output=\"A comprehensive, practical, and personalized nutrition plan\"\n    )\n    \n    return [demographics_research, medical_analysis, diet_plan]\n\ndef create_crew(agents, tasks):\n    \"\"\"Create the CrewAI crew with the specified agents and tasks.\"\"\"\n    return Crew(\n        agents=agents,\n        tasks=tasks,\n        verbose=True\n    )\n\ndef run_nutrition_advisor(user_info):\n    \"\"\"Run the nutrition advisor with the user information.\"\"\"\n    try:\n        # Create agents\n        nutritionist, medical_specialist, diet_planner = create_agents()\n        \n        # Create tasks\n        tasks = create_tasks(nutritionist, medical_specialist, diet_planner, user_info)\n        \n        # Create crew\n        crew = create_crew([nutritionist, medical_specialist, diet_planner], tasks)\n        \n        # Execute the crew\n        with st.spinner('Our nutrition team is creating your personalized plan. This may take a few minutes...'):\n            result = crew.kickoff()\n        \n        return result\n    except Exception as e:\n        st.error(f\"An error occurred: {str(e)}\")\n        return None\n\ndef app():\n    \"\"\"Main Streamlit application.\"\"\"\n    st.set_page_config(page_title=\"Personalized Nutrition Advisor\", page_icon=\"ü•ó\", layout=\"wide\")\n    \n    st.title(\"ü•ó Personalized Nutrition Advisor\")\n    st.markdown(\"\"\"\n    Get a detailed nutrition plan based on your demographics, health conditions, and preferences.\n    Our AI team of nutrition specialists will create a personalized recommendation just for you.\n    \"\"\")\n    \n    # Create tabs for organization\n    tab1, tab2, tab3 = st.tabs([\"Basic Information\", \"Health Details\", \"Preferences & Lifestyle\"])\n    \n    with tab1:\n        st.header(\"Personal Information\")\n        col1, col2 = st.columns(2)\n        \n        with col1:\n            age = st.number_input(\"Age\", min_value=1, max_value=120, value=30)\n            gender = st.selectbox(\"Gender\", [\"Male\", \"Female\", \"Non-binary/Other\"])\n            height = st.text_input(\"Height (e.g., 5'10\\\" or 178 cm)\", \"5'10\\\"\")\n            \n        with col2:\n            weight = st.text_input(\"Weight (e.g., 160 lbs or 73 kg)\", \"160 lbs\")\n            activity_level = st.select_slider(\n                \"Activity Level\",\n                options=[\"Sedentary\", \"Lightly Active\", \"Moderately Active\", \"Very Active\", \"Extremely Active\"]\n            )\n            goals = st.multiselect(\n                \"Nutrition Goals\",\n                [\"Weight Loss\", \"Weight Gain\", \"Maintenance\", \"Muscle Building\", \"Better Energy\", \n                 \"Improved Athletic Performance\", \"Disease Management\", \"General Health\"]\n            )\n    \n    with tab2:\n        st.header(\"Health Information\")\n        \n        medical_conditions = st.text_area(\n            \"Medical Conditions (separate with commas)\",\n            placeholder=\"E.g., Diabetes Type 2, Hypertension, Hypothyroidism...\"\n        )\n        \n        medications = st.text_area(\n            \"Current Medications (separate with commas)\",\n            placeholder=\"E.g., Metformin, Lisinopril, Levothyroxine...\"\n        )\n        \n        allergies = st.text_area(\n            \"Food Allergies/Intolerances (separate with commas)\",\n            placeholder=\"E.g., Lactose, Gluten, Shellfish, Peanuts...\"\n        )\n    \n    with tab3:\n        st.header(\"Preferences & Lifestyle\")\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            food_preferences = st.text_area(\n                \"Food Preferences & Dislikes\",\n                placeholder=\"E.g., Prefer plant-based, dislike seafood...\"\n            )\n            \n            cooking_ability = st.select_slider(\n                \"Cooking Skills & Available Time\",\n                options=[\"Very Limited\", \"Basic/Quick Meals\", \"Average\", \"Advanced/Can Spend Time\", \"Professional Level\"]\n            )\n        \n        with col2:\n            budget = st.select_slider(\n                \"Budget Considerations\",\n                options=[\"Very Limited\", \"Budget Conscious\", \"Moderate\", \"Flexible\", \"No Constraints\"]\n            )\n            \n            cultural_factors = st.text_area(\n                \"Cultural or Religious Dietary Factors\",\n                placeholder=\"E.g., Halal, Kosher, Mediterranean tradition...\"\n            )\n    \n    # Collect all user information\n    user_info = {\n        \"age\": age,\n        \"gender\": gender,\n        \"height\": height,\n        \"weight\": weight,\n        \"activity_level\": activity_level,\n        \"goals\": \", \".join(goals) if goals else \"General health improvement\",\n        \"medical_conditions\": medical_conditions or \"None reported\",\n        \"medications\": medications or \"None reported\",\n        \"allergies\": allergies or \"None reported\",\n        \"food_preferences\": food_preferences or \"No specific preferences\",\n        \"cooking_ability\": cooking_ability,\n        \"budget\": budget,\n        \"cultural_factors\": cultural_factors or \"No specific factors\"\n    }\n    \n    # Check if API keys are present\n    if not os.getenv(\"SERPER_API_KEY\") or not os.getenv(\"OPENAI_API_KEY\"):\n        st.warning(\"‚ö†Ô∏è API keys not detected. Please add your SERPER_API_KEY and OPENAI_API_KEY to your .env file.\")\n    \n    # Create a submission button\n    if st.button(\"Generate Nutrition Plan\"):\n        if not goals:\n            st.error(\"Please select at least one nutrition goal.\")\n            return\n        \n        # Display user information summary\n        with st.expander(\"Summary of Your Information\"):\n            st.json(user_info)\n        \n        # Run the nutrition advisor\n        result = run_nutrition_advisor(user_info)\n        \n        if result:\n            st.success(\"‚úÖ Your personalized nutrition plan is ready!\")\n            st.markdown(\"## Your Personalized Nutrition Plan\")\n            st.markdown(result)\n            \n            # Add download capability\n            st.download_button(\n                label=\"Download Nutrition Plan\",\n                data=result,\n                file_name=\"my_nutrition_plan.md\",\n                mime=\"text/markdown\"\n            )\n\nif __name__ == \"__main__\":\n    app()"}
{"type": "source_file", "path": "agents/jobs/job-hunt-agent.py", "content": "from typing import Dict, List\nfrom pydantic import BaseModel, Field\nfrom agno.agent import Agent\nfrom agno.models.openai import OpenAIChat\nfrom firecrawl import FirecrawlApp\nimport streamlit as st\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file if it exists\nload_dotenv()\n\nclass JobData(BaseModel):\n    \"\"\"Schema for job data extraction\"\"\"\n    job_title: str = Field(description=\"Title of the job position\", alias=\"Job_title\")\n    company_name: str = Field(description=\"Name of the company offering the job\", alias=\"Company_name\")\n    location: str = Field(description=\"Location of the job (city, remote, etc.)\")\n    region: str = Field(description=\"Region or area where the job is located\", default=\"\")\n    role: str = Field(description=\"Specific role or function within the job category\", default=\"\")\n    experience: str = Field(description=\"Experience required for the position\", default=\"\")\n    salary: str = Field(description=\"Salary range or compensation details\", alias=\"Salary\")\n    experience_required: str = Field(description=\"Experience required for the position\", alias=\"Experience_required\")\n    job_description: str = Field(description=\"Detailed description of the job\", alias=\"Job_description\")\n    skills_required: str = Field(description=\"Skills required for the job\", alias=\"Skills_required\")\n    posting_date: str = Field(description=\"Date when the job was posted\", alias=\"Posting_date\")\n    application_link: str = Field(description=\"Link to apply for the job\", alias=\"Application_link\")\n\nclass JobsResponse(BaseModel):\n    \"\"\"Schema for multiple jobs response\"\"\"\n    jobs: List[JobData] = Field(description=\"List of job details\")\n\nclass IndustryData(BaseModel):\n    \"\"\"Schema for industry trends\"\"\"\n    industry: str\n    avg_salary: float\n    growth_rate: float\n    demand_level: str\n    top_skills: List[str]\n\nclass IndustriesResponse(BaseModel):\n    \"\"\"Schema for multiple industries response\"\"\"\n    industries: List[IndustryData] = Field(description=\"List of industry data points\")\n\nclass FirecrawlResponse(BaseModel):\n    \"\"\"Schema for Firecrawl API response\"\"\"\n    success: bool\n    data: Dict\n    status: str\n    expiresAt: str\n\nclass NestedModel1(BaseModel):\n    \"\"\"Schema for job posting data\"\"\"\n    region: str = Field(description=\"Region or area where the job is located\", default=None)\n    role: str = Field(description=\"Specific role or function within the job category\", default=None)\n    job_title: str = Field(description=\"Title of the job position\", default=None)\n    experience: str = Field(description=\"Experience required for the position\", default=None)\n\nclass ExtractSchema(BaseModel):\n    \"\"\"Schema for job postings extraction\"\"\"\n    job_postings: List[NestedModel1] = Field(description=\"List of job postings\")\n\nclass IndustryTrend(BaseModel):\n    \"\"\"Schema for industry trend data\"\"\"\n    industry: str = Field(description=\"Industry name\", default=None)\n    avg_salary: float = Field(description=\"Average salary in the industry\", default=None)\n    growth_rate: float = Field(description=\"Growth rate of the industry\", default=None)\n    demand_level: str = Field(description=\"Demand level in the industry\", default=None)\n    top_skills: List[str] = Field(description=\"Top skills in demand for this industry\", default=None)\n\nclass IndustryTrendsSchema(BaseModel):\n    \"\"\"Schema for industry trends extraction\"\"\"\n    industry_trends: List[IndustryTrend] = Field(description=\"List of industry trends\")\n\nclass JobHuntingAgent:\n    \"\"\"Agent responsible for finding jobs and providing recommendations\"\"\"\n    \n    def __init__(self, firecrawl_api_key: str, openai_api_key: str, model_id: str = \"o3-mini\"):\n        self.agent = Agent(\n            model=OpenAIChat(id=model_id, api_key=openai_api_key),\n            markdown=True,\n            description=\"I am a career expert who helps find and analyze job opportunities based on user preferences.\"\n        )\n        self.firecrawl = FirecrawlApp(api_key=firecrawl_api_key)\n\n    def find_jobs(\n        self, \n        job_title: str,\n        location: str,\n        experience_years: int,\n        skills: List[str]\n    ) -> str:\n        \"\"\"Find and analyze jobs based on user preferences\"\"\"\n        formatted_job_title = job_title.lower().replace(\" \", \"-\")\n        formatted_location = location.lower().replace(\" \", \"-\")\n        skills_string = \", \".join(skills)\n        \n        urls = [\n            f\"https://www.naukri.com/{formatted_job_title}-jobs-in-{formatted_location}\",\n            f\"https://www.indeed.com/jobs?q={formatted_job_title}&l={formatted_location}\",\n            f\"https://www.monster.com/jobs/search/?q={formatted_job_title}&where={formatted_location}\"\n        ]\n        \n        print(f\"Searching for jobs with URLs: {urls}\")\n        \n        try:\n            raw_response = self.firecrawl.extract(\n                urls=urls,\n                params={\n                    'prompt': f\"\"\"Extract job postings by region, roles, job titles, and experience from these job sites.\n                    \n                    Look for jobs that match these criteria:\n                    - Job Title: Should be related to {job_title}\n                    - Location: {location} (include remote jobs if available)\n                    - Experience: Around {experience_years} years\n                    - Skills: Should match at least some of these skills: {skills_string}\n                    \n                    For each job posting, extract:\n                    - region: The broader region or area where the job is located (e.g., \"Northeast\", \"West Coast\", \"Midwest\")\n                    - role: The specific role or function (e.g., \"Frontend Developer\", \"Data Analyst\")\n                    - job_title: The exact title of the job\n                    - experience: The experience requirement in years or level (e.g., \"3-5 years\", \"Senior\")\n                    \n                    IMPORTANT: Return data for at least 3 different job opportunities. MAXIMUM 10.\n                    \"\"\",\n                    'schema': ExtractSchema.model_json_schema()\n                }\n            )\n            \n            print(\"Raw Job Response:\", raw_response)\n            \n            if isinstance(raw_response, dict) and raw_response.get('success'):\n                jobs = raw_response['data'].get('job_postings', [])\n            else:\n                jobs = []\n                \n            print(\"Processed Jobs:\", jobs)\n            \n            if not jobs:\n                return \"No job listings found matching your criteria. Try adjusting your search parameters or try different job sites.\"\n            \n            analysis = self.agent.run(\n                f\"\"\"As a career expert, analyze these job opportunities:\n\n                Jobs Found in json format:\n                {jobs}\n\n                **IMPORTANT INSTRUCTIONS:**\n                1. ONLY analyze jobs from the above JSON data that match the user's requirements:\n                   - Job Title: Related to {job_title}\n                   - Location/Region: Near {location}\n                   - Experience: Around {experience_years} years\n                   - Skills: {skills_string}\n                2. DO NOT create new job listings\n                3. From the matching jobs, select 5-6 jobs that best match the user's skills and experience\n\n                Please provide your analysis in this format:\n                \n                üíº SELECTED JOB OPPORTUNITIES\n                ‚Ä¢ List only 5-6 best matching jobs\n                ‚Ä¢ For each job include:\n                  - Job Title and Role\n                  - Region/Location\n                  - Experience Required\n                  - Pros and Cons\n\n                üîç SKILLS MATCH ANALYSIS\n                ‚Ä¢ Compare the selected jobs based on:\n                  - Skills match with user's profile\n                  - Experience requirements\n                  - Growth potential\n\n                üí° RECOMMENDATIONS\n                ‚Ä¢ Top 3 jobs from the selection with reasoning\n                ‚Ä¢ Career growth potential\n                ‚Ä¢ Points to consider before applying\n\n                üìù APPLICATION TIPS\n                ‚Ä¢ Job-specific application strategies\n                ‚Ä¢ Resume customization tips for these roles\n\n                Format your response in a clear, structured way using the above sections.\n                \"\"\"\n            )\n            \n            return analysis.content\n        except Exception as e:\n            print(f\"Error in find_jobs: {str(e)}\")\n            return f\"An error occurred while searching for jobs: {str(e)}\\n\\nPlease try again with different search parameters or check if the job sites are supported by Firecrawl.\"\n\n    def get_industry_trends(self, job_category: str) -> str:\n        \"\"\"Get trends for the specified job category/industry\"\"\"\n        urls = [\n            f\"https://www.payscale.com/research/US/Job={job_category.replace(' ', '_')}/Salary\",\n            f\"https://www.glassdoor.com/Salaries/{job_category.lower().replace(' ', '-')}-salary-SRCH_KO0,{len(job_category)}.htm\"\n        ]\n        \n        print(f\"Searching for industry trends with URLs: {urls}\")\n        \n        try:\n            raw_response = self.firecrawl.extract(\n                urls=urls,\n                params={\n                    'prompt': f\"\"\"Extract industry trends data for the {job_category} industry. \n                    \n                    For each industry trend, extract:\n                    - industry: The specific industry or sub-category\n                    - avg_salary: The average salary in this industry (as a number)\n                    - growth_rate: The growth rate of this industry (as a number)\n                    - demand_level: The demand level (e.g., \"High\", \"Medium\", \"Low\")\n                    - top_skills: A list of top skills in demand for this industry\n                    \n                    IMPORTANT: \n                    - Extract data for at least 3-5 different roles or sub-categories within this industry\n                    - Include salary trends, growth rate, and demand level\n                    - Identify top skills in demand for this industry\n                    \"\"\",\n                    'schema': IndustryTrendsSchema.model_json_schema(),\n                }\n            )\n            \n            print(\"Raw Industry Trends Response:\", raw_response)\n            \n            if isinstance(raw_response, dict) and raw_response.get('success'):\n                industries = raw_response['data'].get('industry_trends', [])\n        \n                if not industries:\n                    return f\"No industry trends data available for {job_category}. Try a different industry category.\"\n                \n                analysis = self.agent.run(\n                    f\"\"\"As a career expert, analyze these industry trends for {job_category}:\n\n                    {industries}\n\n                    Please provide:\n                    1. A bullet-point summary of the salary and demand trends\n                    2. Identify the top skills in demand for this industry\n                    3. Career growth opportunities:\n                       - Roles with highest growth potential\n                       - Emerging specializations\n                       - Skills with increasing demand\n                    4. Specific advice for job seekers based on these trends\n\n                    Format the response as follows:\n                    \n                    üìä INDUSTRY TRENDS SUMMARY\n                    ‚Ä¢ [Bullet points for salary and demand trends]\n\n                    üî• TOP SKILLS IN DEMAND\n                    ‚Ä¢ [Bullet points for most sought-after skills]\n\n                    üìà CAREER GROWTH OPPORTUNITIES\n                    ‚Ä¢ [Bullet points with growth insights]\n\n                    üéØ RECOMMENDATIONS FOR JOB SEEKERS\n                    ‚Ä¢ [Bullet points with specific advice]\n                    \"\"\"\n                )\n                \n                return analysis.content\n            \n            return f\"No industry trends data available for {job_category}. Try a different industry category.\"\n        except Exception as e:\n            print(f\"Error in get_industry_trends: {str(e)}\")\n            return f\"An error occurred while fetching industry trends: {str(e)}\\n\\nPlease try again with a different industry category or check if the sites are supported by Firecrawl.\"\n\ndef create_job_agent():\n    \"\"\"Create JobHuntingAgent with API keys from session state\"\"\"\n    if 'job_agent' not in st.session_state:\n        st.session_state.job_agent = JobHuntingAgent(\n            firecrawl_api_key=st.session_state.firecrawl_key,\n            openai_api_key=st.session_state.openai_key,\n            model_id=st.session_state.model_id\n        )\n\ndef main():\n    st.set_page_config(\n        page_title=\"AI Job Hunting Assistant\",\n        page_icon=\"üíº\",\n        layout=\"wide\"\n    )\n\n    # Get API keys from environment variables\n    env_firecrawl_key = os.getenv(\"FIRECRAWL_API_KEY\", \"\")\n    env_openai_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n    default_model = os.getenv(\"OPENAI_MODEL_ID\", \"o3-mini\")\n\n    with st.sidebar:\n        st.title(\"üîë API Configuration\")\n        \n        st.subheader(\"ü§ñ Model Selection\")\n        model_id = st.selectbox(\n            \"Choose OpenAI Model\",\n            options=[\"o3-mini\", \"gpt-4o-mini\"],\n            index=0 if default_model == \"o3-mini\" else 1,\n            help=\"Select the AI model to use. Choose gpt-4o if your api doesn't have access to o3-mini\"\n        )\n        st.session_state.model_id = model_id\n        \n        st.divider()\n        \n        st.subheader(\"üîê API Keys\")\n        \n        # Show environment variable status\n        if env_firecrawl_key:\n            st.success(\"‚úÖ Firecrawl API Key found in environment variables\")\n        if env_openai_key:\n            st.success(\"‚úÖ OpenAI API Key found in environment variables\")\n            \n        # Allow UI override of environment variables\n        firecrawl_key = st.text_input(\n            \"Firecrawl API Key (optional if set in environment)\",\n            type=\"password\",\n            help=\"Enter your Firecrawl API key or set FIRECRAWL_API_KEY in environment\",\n            value=\"\" if env_firecrawl_key else \"\"\n        )\n        openai_key = st.text_input(\n            \"OpenAI API Key (optional if set in environment)\",\n            type=\"password\",\n            help=\"Enter your OpenAI API key or set OPENAI_API_KEY in environment\",\n            value=\"\" if env_openai_key else \"\"\n        )\n        \n        # Use environment variables if UI inputs are empty\n        firecrawl_key = firecrawl_key or env_firecrawl_key\n        openai_key = openai_key or env_openai_key\n        \n        if firecrawl_key and openai_key:\n            st.session_state.firecrawl_key = firecrawl_key\n            st.session_state.openai_key = openai_key\n            create_job_agent()\n        else:\n            missing_keys = []\n            if not firecrawl_key:\n                missing_keys.append(\"Firecrawl API Key\")\n            if not openai_key:\n                missing_keys.append(\"OpenAI API Key\")\n            if missing_keys:\n                st.warning(f\"‚ö†Ô∏è Missing required API keys: {', '.join(missing_keys)}\")\n                st.info(\"Please provide the missing keys in the fields above or set them as environment variables.\")\n\n    st.title(\"üíº AI Job Hunting Assistant\")\n    st.info(\n        \"\"\"\n        Welcome to the AI Job Hunting Assistant! \n        Enter your job search criteria below to get job recommendations \n        and industry insights.\n        \"\"\"\n    )\n\n    col1, col2 = st.columns(2)\n    \n    with col1:\n        job_title = st.text_input(\n            \"Job Title\",\n            placeholder=\"Enter job title (e.g., Software Engineer)\",\n            help=\"Enter the job title you're looking for\"\n        )\n        \n        location = st.text_input(\n            \"Location\",\n            placeholder=\"Enter location (e.g., Bangalore, Remote)\",\n            help=\"Enter the location where you want to work\"\n        )\n\n    with col2:\n        experience_years = st.number_input(\n            \"Experience (in years)\",\n            min_value=0,\n            max_value=30,\n            value=2,\n            step=1,\n            help=\"Enter your years of experience\"\n        )\n        \n        skills_input = st.text_area(\n            \"Skills (comma separated)\",\n            placeholder=\"e.g., Python, JavaScript, React, SQL\",\n            help=\"Enter your skills separated by commas\"\n        )\n        \n        skills = [skill.strip() for skill in skills_input.split(\",\")] if skills_input else []\n\n    job_category = st.selectbox(\n        \"Industry/Job Category\",\n        options=[\n            \"Information Technology\", \n            \"Software Development\", \n            \"Data Science\", \n            \"Marketing\", \n            \"Finance\", \n            \"Healthcare\",\n            \"Education\",\n            \"Engineering\",\n            \"Sales\",\n            \"Human Resources\"\n        ],\n        help=\"Select the industry or job category you're interested in\"\n    )\n\n    if st.button(\"üîç Start Job Search\", use_container_width=True):\n        if 'job_agent' not in st.session_state:\n            st.error(\"‚ö†Ô∏è Please enter your API keys in the sidebar first!\")\n            return\n            \n        if not job_title or not location:\n            st.error(\"‚ö†Ô∏è Please enter both job title and location!\")\n            return\n            \n        if not skills:\n            st.warning(\"‚ö†Ô∏è No skills provided. Adding skills will improve job matching.\")\n            \n        try:\n            with st.spinner(\"üîç Searching for jobs...\"):\n                job_results = st.session_state.job_agent.find_jobs(\n                    job_title=job_title,\n                    location=location,\n                    experience_years=experience_years,\n                    skills=skills\n                )\n                \n                if \"An error occurred\" in job_results:\n                    st.error(job_results)\n                else:\n                    st.success(\"‚úÖ Job search completed!\")\n                    st.subheader(\"üíº Job Recommendations\")\n                    st.markdown(job_results)\n                    \n                    st.divider()\n                    \n                    with st.spinner(\"üìä Analyzing industry trends...\"):\n                        industry_trends = st.session_state.job_agent.get_industry_trends(job_category)\n                        \n                        if \"An error occurred\" in industry_trends:\n                            st.error(industry_trends)\n                        else:\n                            st.success(\"‚úÖ Industry analysis completed!\")\n                            with st.expander(f\"üìà {job_category} Industry Trends Analysis\"):\n                                st.markdown(industry_trends)\n                \n        except Exception as e:\n            error_message = str(e)\n            st.error(f\"‚ùå An error occurred: {error_message}\")\n            \n            if \"website is no longer supported\" in error_message.lower():\n                st.info(\"It appears one of the job sites is not supported by your Firecrawl API key. Please contact Firecrawl support to enable these sites for your account.\")\n            elif \"api key\" in error_message.lower():\n                st.info(\"Please check that your API keys are correct and have the necessary permissions.\")\n            else:\n                st.info(\"Please try again with different search parameters or check your internet connection.\")\n\nif __name__ == \"__main__\":\n    main() "}
{"type": "source_file", "path": "agents/marketing/competitor-study/ai-agent.py", "content": "import streamlit as st\nfrom exa_py import Exa\nfrom agno.agent import Agent\nfrom agno.tools.firecrawl import FirecrawlTools\nfrom agno.models.openai import OpenAIChat\nfrom agno.tools.duckduckgo import DuckDuckGoTools\nimport pandas as pd\nimport requests\nfrom firecrawl import FirecrawlApp\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional\nimport json\nimport os\nfrom dotenv import load_dotenv, set_key\nimport pathlib\n\n# Get the absolute path to the .env file\nenv_path = pathlib.Path(os.path.join(os.getcwd(), '.env'))\n\n# Load environment variables from .env file\nload_dotenv(dotenv_path=env_path)\n\n# Streamlit UI\nst.set_page_config(page_title=\"AI Competitor Intelligence Agent Team\", layout=\"wide\")\n\n# Initialize session state for API keys if not already set\nif \"api_keys_initialized\" not in st.session_state:\n    # Get API keys from environment variables\n    st.session_state.env_openai_api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n    st.session_state.env_firecrawl_api_key = os.getenv(\"FIRECRAWL_API_KEY\", \"\")\n    st.session_state.env_perplexity_api_key = os.getenv(\"PERPLEXITY_API_KEY\", \"\")\n    st.session_state.env_exa_api_key = os.getenv(\"EXA_API_KEY\", \"\")\n    \n    # Initialize the working API keys with environment values\n    st.session_state.openai_api_key = st.session_state.env_openai_api_key\n    st.session_state.firecrawl_api_key = st.session_state.env_firecrawl_api_key\n    st.session_state.perplexity_api_key = st.session_state.env_perplexity_api_key\n    st.session_state.exa_api_key = st.session_state.env_exa_api_key\n    \n    st.session_state.api_keys_initialized = True\n\n# Function to save API keys to .env file\ndef save_api_keys_to_env():\n    try:\n        # Save OpenAI API key\n        if st.session_state.openai_api_key:\n            set_key(env_path, \"OPENAI_API_KEY\", st.session_state.openai_api_key)\n            \n        # Save Firecrawl API key\n        if st.session_state.firecrawl_api_key:\n            set_key(env_path, \"FIRECRAWL_API_KEY\", st.session_state.firecrawl_api_key)\n            \n        # Save Perplexity API key\n        if st.session_state.perplexity_api_key:\n            set_key(env_path, \"PERPLEXITY_API_KEY\", st.session_state.perplexity_api_key)\n            \n        # Save Exa API key\n        if st.session_state.exa_api_key:\n            set_key(env_path, \"EXA_API_KEY\", st.session_state.exa_api_key)\n            \n        # Update environment variables in session state\n        st.session_state.env_openai_api_key = st.session_state.openai_api_key\n        st.session_state.env_firecrawl_api_key = st.session_state.firecrawl_api_key\n        st.session_state.env_perplexity_api_key = st.session_state.perplexity_api_key\n        st.session_state.env_exa_api_key = st.session_state.exa_api_key\n        \n        return True\n    except Exception as e:\n        st.error(f\"Error saving API keys to .env file: {str(e)}\")\n        return False\n\n# Sidebar for API keys\nwith st.sidebar:\n    st.title(\"AI Competitor Intelligence\")\n    \n    # Add search engine selection\n    search_engine = st.selectbox(\n        \"Select Search Endpoint\",\n        options=[\"Perplexity AI - Sonar Pro\", \"Exa AI\"],\n        help=\"Choose which AI service to use for finding competitor URLs\"\n    )\n    \n    # API Key Management Section\n    st.subheader(\"API Key Management\")\n    \n    # Add option to show/hide API key inputs with expander\n    with st.expander(\"Configure API Keys\", expanded=False):\n        st.info(\"API keys from .env file are used by default. You can override them here.\")\n        \n        # Function to handle API key updates\n        def update_api_key(key_name, env_key_name):\n            new_value = st.text_input(\n                f\"{key_name} API Key\", \n                value=st.session_state[env_key_name] if st.session_state[env_key_name] else \"\",\n                type=\"password\",\n                help=f\"Enter your {key_name} API key or leave blank to use the one from .env file\"\n            )\n            \n            # Only update if user entered something or if we have an env value\n            if new_value:\n                st.session_state[key_name.lower() + \"_api_key\"] = new_value\n                return True\n            elif st.session_state[env_key_name]:\n                st.session_state[key_name.lower() + \"_api_key\"] = st.session_state[env_key_name]\n                return True\n            return False\n        \n        # Always required API keys\n        has_openai = update_api_key(\"OpenAI\", \"env_openai_api_key\")\n        has_firecrawl = update_api_key(\"Firecrawl\", \"env_firecrawl_api_key\")\n        \n        # Search engine specific API keys\n        if search_engine == \"Perplexity AI - Sonar Pro\":\n            has_search_engine = update_api_key(\"Perplexity\", \"env_perplexity_api_key\")\n        else:  # Exa AI\n            has_search_engine = update_api_key(\"Exa\", \"env_exa_api_key\")\n        \n        # Buttons for API key management\n        col1, col2 = st.columns(2)\n        with col1:\n            if st.button(\"Reset to .env values\"):\n                st.session_state.openai_api_key = st.session_state.env_openai_api_key\n                st.session_state.firecrawl_api_key = st.session_state.env_firecrawl_api_key\n                st.session_state.perplexity_api_key = st.session_state.env_perplexity_api_key\n                st.session_state.exa_api_key = st.session_state.env_exa_api_key\n                st.experimental_rerun()\n        \n        with col2:\n            if st.button(\"Save to .env file\"):\n                if save_api_keys_to_env():\n                    st.success(\"API keys saved to .env file!\")\n                    st.experimental_rerun()\n    \n    # Display API status\n    api_status_ok = has_openai and has_firecrawl and has_search_engine\n    \n    if api_status_ok:\n        st.success(\"‚úÖ All required API keys are configured\")\n    else:\n        missing_keys = []\n        if not has_openai:\n            missing_keys.append(\"OpenAI\")\n        if not has_firecrawl:\n            missing_keys.append(\"Firecrawl\")\n        if not has_search_engine:\n            missing_keys.append(\"Search Engine\")\n        \n        st.error(f\"‚ùå Missing API keys: {', '.join(missing_keys)}\")\n\n# Main UI\nst.title(\"üß≤ AI Competitor Intelligence Agent Team\")\nst.info(\n    \"\"\"\n    This app helps businesses analyze their competitors by extracting structured data from competitor websites and generating insights using AI.\n    - Provide a **URL** or a **description** of your company.\n    - The app will fetch competitor URLs, extract relevant information, and generate a detailed analysis report.\n    \"\"\"\n)\nst.success(\"For better results, provide both URL and a 5-6 word description of your company!\")\n\n# Input fields for URL and description\nurl = st.text_input(\"Enter your company URL :\")\ndescription = st.text_area(\"Enter a description of your company (if URL is not available):\")\n\n# Initialize API keys and tools\nif api_status_ok:\n    # Initialize Exa only if selected\n    if search_engine == \"Exa AI\":\n        exa = Exa(api_key=st.session_state.exa_api_key)\n\n    firecrawl_tools = FirecrawlTools(\n        api_key=st.session_state.firecrawl_api_key,\n        scrape=False,\n        crawl=True,\n        limit=5\n    )\n\n    firecrawl_agent = Agent(\n        model=OpenAIChat(id=\"gpt-4o-mini\", api_key=st.session_state.openai_api_key),\n        tools=[firecrawl_tools, DuckDuckGoTools()],\n        show_tool_calls=True,\n        markdown=True\n    )\n\n    analysis_agent = Agent(\n        model=OpenAIChat(id=\"gpt-4o-mini\", api_key=st.session_state.openai_api_key),\n        show_tool_calls=True,\n        markdown=True\n    )\n\n    # New agent for comparing competitor data\n    comparison_agent = Agent(\n        model=OpenAIChat(id=\"gpt-4o-mini\", api_key=st.session_state.openai_api_key),\n        show_tool_calls=True,\n        markdown=True\n    )\n\n    def get_competitor_urls(url: str = None, description: str = None) -> list[str]:\n        if not url and not description:\n            raise ValueError(\"Please provide either a URL or a description.\")\n\n        if search_engine == \"Perplexity AI - Sonar Pro\":\n            perplexity_url = \"https://api.perplexity.ai/chat/completions\"\n            \n            content = \"Find me 3 competitor company URLs similar to the company with \"\n            if url and description:\n                content += f\"URL: {url} and description: {description}\"\n            elif url:\n                content += f\"URL: {url}\"\n            else:\n                content += f\"description: {description}\"\n            content += \". ONLY RESPOND WITH THE URLS, NO OTHER TEXT.\"\n\n            payload = {\n                \"model\": \"sonar-pro\",\n                \"messages\": [\n                    {\n                        \"role\": \"system\",\n                        \"content\": \"Be precise and only return 3 company URLs ONLY.\"\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": content\n                    }\n                ],\n                \"max_tokens\": 1000,\n                \"temperature\": 0.2,\n            }\n            \n            headers = {\n                \"Authorization\": f\"Bearer {st.session_state.perplexity_api_key}\",\n                \"Content-Type\": \"application/json\"\n            }\n\n            try:\n                response = requests.post(perplexity_url, json=payload, headers=headers)\n                response.raise_for_status()\n                urls = response.json()['choices'][0]['message']['content'].strip().split('\\n')\n                return [url.strip() for url in urls if url.strip()]\n            except Exception as e:\n                st.error(f\"Error fetching competitor URLs from Perplexity: {str(e)}\")\n                return []\n\n        else:  # Exa AI\n            try:\n                if url:\n                    result = exa.find_similar(\n                        url=url,\n                        num_results=3,\n                        exclude_source_domain=True,\n                        category=\"company\"\n                    )\n                else:\n                    result = exa.search(\n                        description,\n                        type=\"neural\",\n                        category=\"company\",\n                        use_autoprompt=True,\n                        num_results=3\n                    )\n                return [item.url for item in result.results]\n            except Exception as e:\n                st.error(f\"Error fetching competitor URLs from Exa: {str(e)}\")\n                return []\n\n    class CompetitorDataSchema(BaseModel):\n        company_name: str = Field(description=\"Name of the company\")\n        pricing: str = Field(description=\"Pricing details, tiers, and plans\")\n        key_features: List[str] = Field(description=\"Main features and capabilities of the product/service\")\n        tech_stack: List[str] = Field(description=\"Technologies, frameworks, and tools used\")\n        marketing_focus: str = Field(description=\"Main marketing angles and target audience\")\n        customer_feedback: str = Field(description=\"Customer testimonials, reviews, and feedback\")\n\n    def extract_competitor_info(competitor_url: str) -> Optional[dict]:\n        try:\n            # Initialize FirecrawlApp with API key\n            app = FirecrawlApp(api_key=st.session_state.firecrawl_api_key)\n            \n            # Add wildcard to crawl subpages\n            url_pattern = f\"{competitor_url}/*\"\n            \n            extraction_prompt = \"\"\"\n            Extract detailed information about the company's offerings, including:\n            - Company name and basic information\n            - Pricing details, plans, and tiers\n            - Key features and main capabilities\n            - Technology stack and technical details\n            - Marketing focus and target audience\n            - Customer feedback and testimonials\n            \n            Analyze the entire website content to provide comprehensive information for each field.\n            \"\"\"\n            \n            response = app.extract(\n                [url_pattern],\n                {\n                    'prompt': extraction_prompt,\n                    'schema': CompetitorDataSchema.model_json_schema(),\n                }\n            )\n            \n            if response.get('success') and response.get('data'):\n                extracted_info = response['data']\n                \n                # Create JSON structure\n                competitor_json = {\n                    \"competitor_url\": competitor_url,\n                    \"company_name\": extracted_info.get('company_name', 'N/A'),\n                    \"pricing\": extracted_info.get('pricing', 'N/A'),\n                    \"key_features\": extracted_info.get('key_features', [])[:5],  # Top 5 features\n                    \"tech_stack\": extracted_info.get('tech_stack', [])[:5],      # Top 5 tech stack items\n                    \"marketing_focus\": extracted_info.get('marketing_focus', 'N/A'),\n                    \"customer_feedback\": extracted_info.get('customer_feedback', 'N/A')\n                }\n                \n                return competitor_json\n                \n            else:\n                return None\n                \n        except Exception as e:\n            return None\n\n    def generate_comparison_report(competitor_data: list) -> None:\n        # Format the competitor data for the prompt\n        formatted_data = json.dumps(competitor_data, indent=2)\n        print(formatted_data)\n        \n        # Updated system prompt for more structured output\n        system_prompt = f\"\"\"\n        As an expert business analyst, analyze the following competitor data in JSON format and create a structured comparison.\n        Extract and summarize the key information into concise points.\n\n        {formatted_data}\n\n        Return the data in a structured format with EXACTLY these columns:\n        Company, Pricing, Key Features, Tech Stack, Marketing Focus, Customer Feedback\n\n        Rules:\n        1. For Company: Include company name and URL\n        2. For Key Features: List top 3 most important features only\n        3. For Tech Stack: List top 3 most relevant technologies only\n        4. Keep all entries clear and concise\n        5. Format feedback as brief quotes\n        6. Return ONLY the structured data, no additional text\n        \"\"\"\n\n        # Get comparison data from agent\n        comparison_response = comparison_agent.run(system_prompt)\n        \n        try:\n            # Split the response into lines and clean them\n            table_lines = [\n                line.strip() \n                for line in comparison_response.content.split('\\n') \n                if line.strip() and '|' in line\n            ]\n            \n            # Extract headers (first row)\n            headers = [\n                col.strip() \n                for col in table_lines[0].split('|') \n                if col.strip()\n            ]\n            \n            # Extract data rows (skip header and separator rows)\n            data_rows = []\n            for line in table_lines[2:]:  # Skip header and separator rows\n                row_data = [\n                    cell.strip() \n                    for cell in line.split('|') \n                    if cell.strip()\n                ]\n                if len(row_data) == len(headers):\n                    data_rows.append(row_data)\n            \n            # Create DataFrame\n            df = pd.DataFrame(\n                data_rows,\n                columns=headers\n            )\n            \n            # Display the table\n            st.subheader(\"Competitor Comparison\")\n            st.table(df)\n            \n        except Exception as e:\n            st.error(f\"Error creating comparison table: {str(e)}\")\n            st.write(\"Raw comparison data for debugging:\", comparison_response.content)\n\n    def generate_analysis_report(competitor_data: list):\n        # Format the competitor data for the prompt\n        formatted_data = json.dumps(competitor_data, indent=2)\n        print(\"Analysis Data:\", formatted_data)  # For debugging\n        \n        report = analysis_agent.run(\n            f\"\"\"Analyze the following competitor data in JSON format and identify market opportunities to improve my own company:\n            \n            {formatted_data}\n\n            Tasks:\n            1. Identify market gaps and opportunities based on competitor offerings\n            2. Analyze competitor weaknesses that we can capitalize on\n            3. Recommend unique features or capabilities we should develop\n            4. Suggest pricing and positioning strategies to gain competitive advantage\n            5. Outline specific growth opportunities in underserved market segments\n            6. Provide actionable recommendations for product development and go-to-market strategy\n\n            Focus on finding opportunities where we can differentiate and do better than competitors.\n            Highlight any unmet customer needs or pain points we can address.\n            \"\"\"\n        )\n        return report.content\n\n    # Run analysis when the user clicks the button\n    if st.button(\"Analyze Competitors\"):\n        if not api_status_ok:\n            st.error(\"‚ö†Ô∏è Please configure all required API keys in the sidebar before proceeding.\")\n        elif url or description:\n            with st.spinner(\"Fetching competitor URLs...\"):\n                competitor_urls = get_competitor_urls(url=url, description=description)\n                st.write(f\"Competitor URLs: {competitor_urls}\")\n            \n            competitor_data = []\n            for comp_url in competitor_urls:\n                with st.spinner(f\"Analyzing Competitor: {comp_url}...\"):\n                    competitor_info = extract_competitor_info(comp_url)\n                    if competitor_info is not None:\n                        competitor_data.append(competitor_info)\n            \n            if competitor_data:\n                # Generate and display comparison report\n                with st.spinner(\"Generating comparison table...\"):\n                    generate_comparison_report(competitor_data)\n                \n                # Generate and display final analysis report\n                with st.spinner(\"Generating analysis report...\"):\n                    analysis_report = generate_analysis_report(competitor_data)\n                    st.subheader(\"Competitor Analysis Report\")\n                    st.markdown(analysis_report)\n                \n                st.success(\"Analysis complete!\")\n            else:\n                st.error(\"Could not extract data from any competitor URLs\")\n        else:\n            st.error(\"Please provide either a URL or a description.\")\n    else:\n        # Display API key status message when the app first loads\n        if not api_status_ok:\n            st.warning(\"‚ö†Ô∏è Configure your API keys in the sidebar before analyzing competitors.\")"}
{"type": "source_file", "path": "agents/sales/with_st.py", "content": "from textwrap import dedent\nfrom crewai import Agent, Task, Crew, Process\nfrom crewai.tools import BaseTool\nfrom crewai_tools import SerperDevTool\nfrom pydantic import BaseModel, Field\nfrom typing import List, Dict, Type, ClassVar\nimport smtplib\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\nfrom langchain_community.llms import Ollama\nfrom langchain_openai import ChatOpenAI\nimport os\nimport json\n\n# Initialize SerperDev tool\nsearch_tool = SerperDevTool()\n\ndef get_llm(use_gpt=False):\n    \"\"\"Get the specified language model\"\"\"\n    if use_gpt:\n        return ChatOpenAI(\n            model_name=\"gpt-4o-mini\",\n            temperature=0.7\n        )\n    return Ollama(\n        model=\"deepseek-r1:latest\",\n        base_url=\"http://localhost:11434\",\n        temperature=0.7\n    )\n\nclass EmailInput(BaseModel):\n    \"\"\"Input schema for Email Tool\"\"\"\n    to: str = Field(..., description=\"Recipient email address\")\n    subject: str = Field(..., description=\"Email subject line\")\n    body: str = Field(..., description=\"Email body content\")\n\nclass EmailSender(BaseTool):\n    name: str = \"Email Sender\"\n    description: str = \"Sends personalized emails using Gmail SMTP\"\n    args_schema: Type[BaseModel] = EmailInput\n    \n    smtp_settings: ClassVar[Dict[str, str | int]] = {\n        'server': \"smtp.gmail.com\",\n        'port': 587,\n        'username': os.getenv('GMAIL_USER'),\n        'password': os.getenv('GMAIL_APP_PASSWORD')\n    }\n\n    def _run(self, to: str, subject: str, body: str) -> str:\n        if not self.smtp_settings['username'] or not self.smtp_settings['password']:\n            return json.dumps({\"error\": \"GMAIL_USER and GMAIL_APP_PASSWORD environment variables are required\"})\n        \n        try:\n            msg = MIMEMultipart()\n            msg['From'] = self.smtp_settings['username']\n            msg['To'] = to\n            msg['Subject'] = subject\n            msg.attach(MIMEText(body, 'plain'))\n            \n            with smtplib.SMTP(self.smtp_settings['server'], self.smtp_settings['port']) as server:\n                server.starttls()\n                server.login(self.smtp_settings['username'], self.smtp_settings['password'])\n                server.send_message(msg)\n            \n            return json.dumps({\n                \"status\": \"success\",\n                \"message\": f\"Email sent successfully to {to}\",\n                \"to\": to,\n                \"subject\": subject,\n                \"body\": body\n            })\n        except Exception as e:\n            return json.dumps({\n                \"error\": f\"Error sending email: {str(e)}\",\n                \"to\": to,\n                \"subject\": subject,\n                \"body\": body\n            })\n\ndef load_email_template(industry: str) -> str:\n    \"\"\"Load email template for the given industry\"\"\"\n    try:\n        with open(f'email_templates/{industry.lower()}.txt', 'r') as f:\n            return f.read()\n    except FileNotFoundError:\n        return \"\"\n\nclass DetailedSalesCrew:\n    def __init__(self, target_emails: List[dict], use_gpt: bool = False):\n        \"\"\"\n        Initialize with list of dicts containing email and industry\n        Example: [{\"email\": \"user@domain.com\", \"industry\": \"Technology\"}]\n        \"\"\"\n        # Validate email format\n        for email_data in target_emails:\n            if not isinstance(email_data, dict) or \"email\" not in email_data or \"industry\" not in email_data:\n                raise ValueError(\"Each target email must be a dictionary with 'email' and 'industry' keys\")\n        \n        self.target_emails = target_emails\n        self.llm = get_llm(use_gpt)\n        self.email_tool = EmailSender()\n        \n    def create_agents(self, industry: str):\n        # Research Agent\n        self.researcher = Agent(\n            role='Company Research Specialist',\n            goal='Analyze companies and gather comprehensive information',\n            backstory=dedent(f\"\"\"You are an expert researcher specializing in \n                {industry} company analysis. You excel at finding detailed information \n                about companies, their products, and market presence.\"\"\"),\n            tools=[search_tool],\n            verbose=True,\n            llm=self.llm,\n            max_iter=100,\n            allow_delegation=False\n        )\n        \n        # News Agent\n        self.news_analyst = Agent(\n            role='News and Trends Analyst',\n            goal='Find and analyze relevant news and industry trends',\n            backstory=dedent(f\"\"\"You are skilled at identifying relevant news \n                and understanding {industry} industry trends. You can connect company \n                activities to broader market movements.\"\"\"),\n            tools=[search_tool],\n            verbose=True,\n            llm=self.llm,\n            max_iter=75,\n            allow_delegation=False\n        )\n        \n        # Content Writer\n        template = load_email_template(industry)\n        template_context = f\"Use this template if available: {template}\" if template else \"\"\n        \n        self.writer = Agent(\n            role='Outreach Content Specialist',\n            goal='Create highly personalized email content',\n            backstory=dedent(f\"\"\"You are an expert at crafting personalized \n                outreach emails for {industry} companies that resonate with recipients. \n                You excel at combining company research with industry insights.\n                You are founder of explainx.ai and your name is Yash Thakker, which \n                is what should be mentioned in the email. {template_context}\"\"\"),\n            tools=[self.email_tool],\n            verbose=True,\n            llm=self.llm,\n            max_iter=50,\n            allow_delegation=False\n        )\n        \n        return [self.researcher, self.news_analyst, self.writer]\n    \n    def create_tasks(self, email: str, industry: str):\n        # Extract domain from email\n        domain = email.split('@')[1]\n        company_name = domain.split('.')[0]\n        \n        # Research Task\n        research_task = Task(\n            description=dedent(f\"\"\"Research {company_name} ({domain}) thoroughly.\n                Consider their position in the {industry} industry.\n                \n                Step-by-step approach:\n                1. Search for company overview and background\n                2. Research their products/services in detail\n                3. Find information about their team and leadership\n                4. Analyze their market position\n                5. Identify their tech stack and tools\n                \n                Focus on:\n                - Company's main products/services\n                - Value proposition\n                - Target market\n                - Team information\n                - Recent updates or changes\n                - Technology stack or tools mentioned\n                \n                Create a comprehensive profile of the company.\"\"\"),\n            agent=self.researcher,\n            expected_output=dedent(\"\"\"Detailed company profile including all \n                discovered information in a structured format.\"\"\")\n        )\n        \n        # News Analysis Task\n        news_task = Task(\n            description=dedent(f\"\"\"Research recent news and developments about \n                {company_name} and their position in the {industry} industry.\n                \n                Step-by-step approach:\n                1. Search for company news from the last 3 months\n                2. Research {industry} industry trends affecting them\n                3. Analyze competitor movements\n                4. Identify market opportunities\n                5. Find any company milestones or achievements\n                \n                Focus on:\n                - Recent company news and press releases\n                - Industry trends and developments\n                - Competitive landscape\n                - Market opportunities and challenges\n                - Recent achievements or notable events\"\"\"),\n            agent=self.news_analyst,\n            expected_output=dedent(\"\"\"Comprehensive news analysis including \n                company-specific news and relevant industry trends.\"\"\")\n        )\n        \n        # Email Creation Task\n        email_task = Task(\n            description=dedent(f\"\"\"Create and send a personalized email to {email} using \n                the research and news analysis. Consider their position in the \n                {industry} industry.\n                \n                Step-by-step approach:\n                1. Extract key insights from research\n                2. Identify compelling news points\n                3. Craft attention-grabbing subject\n                4. Write personalized introduction\n                5. Present value proposition\n                \n                Guidelines:\n                - Keep subject line engaging but professional\n                - Reference specific company details from research\n                - Mention relevant {industry} trends\n                - Focus on value proposition\n                - Keep email concise (150-200 words)\n                - Include clear call to action\n                - Sign as Yash Thakker, Founder at ExplainX.ai\n                \n                Use the email tool to send the email directly.\"\"\"),\n            agent=self.writer,\n            context=[research_task, news_task],\n            expected_output=dedent(\"\"\"Email sent successfully with response from \n                email tool in JSON format.\"\"\")\n        )\n        \n        return [research_task, news_task, email_task]\n    \n    def run(self):\n        \"\"\"Process each email and create personalized outreach\"\"\"\n        all_results = []\n        \n        for target in self.target_emails:\n            email = target[\"email\"]\n            industry = target[\"industry\"]\n            \n            print(f\"\\nProcessing email: {email} (Industry: {industry})\")\n            \n            # Create crew for this email\n            crew = Crew(\n                agents=self.create_agents(industry),\n                tasks=self.create_tasks(email, industry),\n                process=Process.sequential,\n                verbose=True,\n                max_rpm=100\n            )\n            \n            # Execute the crew's tasks\n            result = crew.kickoff()\n            all_results.append({\n                \"email\": email,\n                \"industry\": industry,\n                \"result\": result\n            })\n        \n        return all_results"}
{"type": "source_file", "path": "agents/sales/st-ui.py", "content": "import streamlit as st\nimport pandas as pd\nfrom typing import List\nfrom with_st import DetailedSalesCrew  # Import from your main implementation file\nimport json\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\n# Page config\nst.set_page_config(\n    page_title=\"AI Sales Outreach Platform\",\n    page_icon=\"ü§ñ\",\n    layout=\"wide\"\n)\n\n# Initialize session state\nif 'emails' not in st.session_state:\n    st.session_state.emails = []\nif 'results' not in st.session_state:\n    st.session_state.results = []\n\ndef validate_email(email: str) -> bool:\n    \"\"\"Simple email validation\"\"\"\n    return '@' in email and '.' in email.split('@')[1]\n\ndef save_email_template(industry: str, template: str):\n    \"\"\"Save email template to templates directory\"\"\"\n    os.makedirs('email_templates', exist_ok=True)\n    with open(f'email_templates/{industry.lower()}.txt', 'w') as f:\n        f.write(template)\n\ndef load_email_template(industry: str) -> str:\n    \"\"\"Load email template for given industry\"\"\"\n    try:\n        with open(f'email_templates/{industry.lower()}.txt', 'r') as f:\n            return f.read()\n    except FileNotFoundError:\n        return \"\"\n\ndef run_sales_crew(emails: List[dict], use_gpt: bool = False) -> List[dict]:\n    \"\"\"Run the sales crew with the given emails\"\"\"\n    sales_crew = DetailedSalesCrew(emails, use_gpt)\n    return sales_crew.run()\n\n# Sidebar\nwith st.sidebar:\n    st.title(\"‚öôÔ∏è Settings\")\n    \n    # Model selection\n    model_option = st.radio(\n        \"Select AI Model\",\n        [\"OpenAI GPT-4\", \"Local DeepSeek Coder\"],\n        help=\"Choose between OpenAI's GPT-4 or local DeepSeek Coder model\"\n    )\n    \n    # API Keys\n    with st.expander(\"API Configuration\"):\n        openai_key = st.text_input(\"OpenAI API Key\", type=\"password\")\n        serper_key = st.text_input(\"Serper API Key\", type=\"password\")\n        gmail_user = st.text_input(\"Gmail User\", type=\"password\", help=\"Your Gmail address\")\n        gmail_password = st.text_input(\"Gmail App Password\", type=\"password\", help=\"Gmail App Password (NOT your regular password)\")\n        \n        # Save credentials to environment\n        if openai_key:\n            os.environ[\"OPENAI_API_KEY\"] = openai_key\n        if serper_key:\n            os.environ[\"SERPER_API_KEY\"] = serper_key\n        if gmail_user:\n            os.environ[\"GMAIL_USER\"] = gmail_user\n        if gmail_password:\n            os.environ[\"GMAIL_APP_PASSWORD\"] = gmail_password\n\n# Main content\nst.title(\"ü§ñ AI Sales Outreach Platform\")\nst.caption(\"Personalized email outreach powered by AI\")\n\n# Tab selection\ntab1, tab2, tab3 = st.tabs([\"Add Prospects\", \"Email Templates\", \"Results\"])\n\n# Add Prospects Tab\nwith tab1:\n    st.header(\"Add Target Prospects\")\n    \n    col1, col2 = st.columns(2)\n    \n    with col1:\n        email = st.text_input(\"Email Address\", help=\"Enter the prospect's email address\")\n        industry = st.selectbox(\n            \"Industry\",\n            [\"Technology\", \"Finance\", \"Healthcare\", \"Education\", \"Other\"],\n            help=\"Select the prospect's industry\"\n        )\n        \n        upload_file = st.file_uploader(\"Or Upload CSV\", type=['csv'], \n            help=\"CSV should have 'email' and 'industry' columns\")\n        \n        if upload_file is not None:\n            try:\n                df = pd.read_csv(upload_file)\n                if 'email' in df.columns and 'industry' in df.columns:\n                    new_prospects = df[['email', 'industry']].to_dict('records')\n                    for prospect in new_prospects:\n                        if validate_email(prospect['email']) and \\\n                           not any(e[\"email\"] == prospect['email'] for e in st.session_state.emails):\n                            st.session_state.emails.append(prospect)\n                    st.success(f\"Added {len(new_prospects)} prospects from CSV!\")\n                else:\n                    st.error(\"CSV must contain 'email' and 'industry' columns\")\n            except Exception as e:\n                st.error(f\"Error processing CSV: {str(e)}\")\n    \n    with col2:\n        if st.button(\"Add Prospect\", help=\"Add single prospect to the list\"):\n            if validate_email(email):\n                if not any(e[\"email\"] == email for e in st.session_state.emails):\n                    st.session_state.emails.append({\"email\": email, \"industry\": industry})\n                    st.success(f\"Added {email} to the prospect list!\")\n                else:\n                    st.warning(\"This email is already in the list!\")\n            else:\n                st.error(\"Please enter a valid email address!\")\n    \n    # Display current prospects\n    if st.session_state.emails:\n        st.subheader(\"Current Prospects\")\n        df = pd.DataFrame(st.session_state.emails)\n        st.dataframe(df, hide_index=True)\n        \n        col3, col4 = st.columns(2)\n        \n        with col3:\n            if st.button(\"Clear List\", help=\"Remove all prospects from the list\"):\n                st.session_state.emails = []\n                st.success(\"Prospect list cleared!\")\n        \n        with col4:\n            if st.button(\"Run Outreach Campaign\", help=\"Start sending personalized emails\"):\n                if not os.getenv(\"GMAIL_USER\") or not os.getenv(\"GMAIL_APP_PASSWORD\"):\n                    st.error(\"Please configure Gmail credentials in the settings!\")\n                elif not os.getenv(\"SERPER_API_KEY\"):\n                    st.error(\"Please configure Serper API key in the settings!\")\n                elif model_option == \"OpenAI GPT-4\" and not os.getenv(\"OPENAI_API_KEY\"):\n                    st.error(\"Please configure OpenAI API key in the settings!\")\n                else:\n                    with st.spinner(\"Running outreach campaign...\"):\n                        try:\n                            results = run_sales_crew(\n                                st.session_state.emails,\n                                use_gpt=(model_option == \"OpenAI GPT-4\")\n                            )\n                            st.session_state.results = results\n                            st.success(\"Campaign completed successfully!\")\n                        except Exception as e:\n                            st.error(f\"Error running campaign: {str(e)}\")\n\n# Email Templates Tab\nwith tab2:\n    st.header(\"Email Templates\")\n    \n    template_industry = st.selectbox(\n        \"Select Industry for Template\",\n        [\"Technology\", \"Finance\", \"Healthcare\", \"Education\", \"Other\"],\n        key=\"template_industry\"\n    )\n    \n    existing_template = load_email_template(template_industry)\n    \n    template_content = st.text_area(\n        \"Email Template\",\n        value=existing_template,\n        height=300,\n        help=\"Use placeholders: {company}, {industry}, {name}, etc.\"\n    )\n    \n    with st.expander(\"Template Variables Help\"):\n        st.markdown(\"\"\"\n        Available template variables:\n        - `{company}`: Company name\n        - `{industry}`: Industry name\n        - `{domain}`: Company domain\n        - `{name}`: Recipient's name (if available)\n        \n        Example template:\n        ```\n        Subject: Enhancing {company}'s Capabilities\n        \n        Hi {name},\n        \n        I noticed {company}'s impressive work in the {industry} sector...\n        \n        Best regards,\n        Yash Thakker\n        Founder, ExplainX.ai\n        ```\n        \"\"\")\n    \n    if st.button(\"Save Template\"):\n        save_email_template(template_industry, template_content)\n        st.success(f\"Template saved for {template_industry} industry!\")\n\n# Results Tab\nwith tab3:\n    st.header(\"Campaign Results\")\n    \n    if st.session_state.results:\n        # Download results button\n        results_df = pd.DataFrame([{\n            'email': r['email'],\n            'industry': r['industry'],\n            'status': 'success' if json.loads(r['result'])['status'] == 'success' else 'error',\n            'subject': json.loads(r['result'])['subject'],\n            'body': json.loads(r['result'])['body']\n        } for r in st.session_state.results])\n        \n        csv = results_df.to_csv(index=False)\n        st.download_button(\n            \"Download Results CSV\",\n            csv,\n            \"campaign_results.csv\",\n            \"text/csv\",\n            key='download-csv'\n        )\n        \n        # Display individual results\n        for result in st.session_state.results:\n            with st.expander(f\"Results for {result['email']} ({result['industry']})\"):\n                try:\n                    # Parse the JSON result\n                    email_content = json.loads(result['result'])\n                    \n                    # Show status\n                    if \"status\" in email_content:\n                        st.success(email_content[\"message\"])\n                    elif \"error\" in email_content:\n                        st.error(email_content[\"error\"])\n                    \n                    # Show email content\n                    st.subheader(\"Subject\")\n                    st.write(email_content.get('subject', 'N/A'))\n                    st.subheader(\"Body\")\n                    st.write(email_content.get('body', 'N/A'))\n                    \n                except Exception as e:\n                    st.error(f\"Error displaying result: {str(e)}\")\n                    st.write(result['result'])\n    else:\n        st.info(\"No campaign results yet. Run an outreach campaign to see results here.\")\n\n# Footer\nst.markdown(\"---\")\nst.markdown(\"Made with ‚ù§Ô∏è by @goyashy\")\n\n# Add help/documentation tooltip\nwith st.sidebar:\n    with st.expander(\"‚ÑπÔ∏è Help & Documentation\"):\n        st.markdown(\"\"\"\n        ### Quick Start Guide\n        1. Configure your API keys in Settings\n        2. Add prospects individually or via CSV\n        3. Optionally set up email templates\n        4. Run the campaign\n        \n        ### Requirements\n        - Gmail account with App Password\n        - Serper API key for research\n        - OpenAI API key (if using GPT-4)\n        \n        ### Need Help?\n        Contact @goyashy for support\n        \"\"\")"}
{"type": "source_file", "path": "agents/thinking/streamlit-based.py", "content": "import streamlit as st\nimport os\nfrom dotenv import load_dotenv\nfrom crewai import Agent, Task, Crew, LLM\nfrom crewai_tools import SerperDevTool, WebsiteSearchTool\nfrom langchain_openai import ChatOpenAI\nimport requests\n\n# Load environment variables\nload_dotenv()\nos.environ[\"SERPER_API_KEY\"] = os.getenv(\"SERPER_API_KEY\")\n\n# Initialize enhanced search tools\nsearch_tool = SerperDevTool()\nwebsite_tool = WebsiteSearchTool()\n\ndef check_ollama_availability():\n    \"\"\"Check if Ollama server is running\"\"\"\n    try:\n        response = requests.get(\"http://localhost:11434/api/version\")\n        return response.status_code == 200\n    except requests.exceptions.ConnectionError:\n        return False\n\ndef get_llm(use_gpt=True):\n    \"\"\"Initialize the specified language model\"\"\"   \n    if use_gpt:\n        return ChatOpenAI(model_name=\"o3-mini\")\n    \n    # Use CrewAI's LLM class with correct provider format\n    return LLM(\n        model=\"ollama/deepseek-r1:latest\",\n        base_url=\"http://localhost:11434\",\n        temperature=0.7\n    )\n\ndef create_agents(use_gpt=True):\n    \"\"\"Create specialized research and analysis agents\"\"\"\n    try:\n        llm = get_llm(use_gpt)\n        \n        researcher = Agent(\n            role='Deep Research Specialist',\n            goal='Conduct comprehensive research and gather detailed information',\n            backstory=\"\"\"Expert researcher skilled at discovering hard-to-find information \n            and connecting complex data points. Specializes in thorough, detailed research.\"\"\",\n            tools=[search_tool, website_tool],\n            llm=llm,\n            verbose=True,\n            max_iter=15,\n            allow_delegation=False\n        )\n        \n        analyst = Agent(\n            role='Research Analyst',\n            goal='Analyze and synthesize research findings',\n            backstory=\"\"\"Expert analyst skilled at processing complex information and \n            identifying key patterns and insights. Specializes in clear, actionable analysis.\"\"\",\n            tools=[search_tool],\n            llm=llm,\n            verbose=True,\n            max_iter=10,\n            allow_delegation=False\n        )\n        \n        writer = Agent(\n            role='Content Synthesizer',\n            goal='Create clear, structured reports from analysis',\n            backstory=\"\"\"Expert writer skilled at transforming complex analysis into \n            clear, engaging content while maintaining technical accuracy.\"\"\",\n            llm=llm,\n            verbose=True,\n            max_iter=8,\n            allow_delegation=False\n        )\n        \n        return researcher, analyst, writer\n    except Exception as e:\n        st.error(f\"Error creating agents: {str(e)}\")\n        return None, None, None\n\ndef create_tasks(researcher, analyst, writer, topic):\n    \"\"\"Create research tasks with clear objectives\"\"\"\n    research_task = Task(\n        description=f\"\"\"Research this topic thoroughly: {topic}\n        \n        Follow these steps:\n        1. Find reliable sources and latest information\n        2. Extract key details and evidence\n        3. Verify information across sources\n        4. Document findings with references\"\"\",\n        agent=researcher,\n        expected_output=\"Detailed research findings with sources\"\n    )\n    \n    analysis_task = Task(\n        description=f\"\"\"Analyze the research findings about {topic}:\n        \n        Steps:\n        1. Review and categorize findings\n        2. Identify patterns and trends\n        3. Evaluate source credibility\n        4. Note key insights\"\"\",\n        agent=analyst,\n        context=[research_task],\n        expected_output=\"Analysis of findings and insights\"\n    )\n    \n    synthesis_task = Task(\n        description=f\"\"\"Create a clear report on {topic}:\n        \n        Include:\n        - Executive Summary\n        - Key Findings\n        - Evidence\n        - Conclusions\n        - Specific questions asked by the user\n        - search volume, demand, search converstion\n        - Top keywords\n        - References\"\"\",\n        agent=writer,\n        context=[research_task, analysis_task],\n        expected_output=\"Structured report with insights\"\n    )\n    \n    return [research_task, analysis_task, synthesis_task]\n\ndef run_research(topic, use_gpt):\n    \"\"\"Execute the research process\"\"\"\n    try:\n        if use_gpt and not os.getenv(\"OPENAI_API_KEY\"):\n            raise ValueError(\"OpenAI API key not found. Please set OPENAI_API_KEY in your environment.\")\n        \n        if not use_gpt and not check_ollama_availability():\n            raise ConnectionError(\"Ollama server not running. Start with: ollama run deepseek-r1\")\n        \n        researcher, analyst, writer = create_agents(use_gpt)\n        if not all([researcher, analyst, writer]):\n            raise Exception(\"Failed to create agents\")\n        \n        tasks = create_tasks(researcher, analyst, writer, topic)\n        crew = Crew(\n            agents=[researcher, analyst, writer],\n            tasks=tasks,\n            verbose=True\n        )\n        \n        result = crew.kickoff()\n        # Convert CrewOutput to string for consistency\n        return str(result)\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n\ndef main():\n    st.set_page_config(\n        page_title=\"Deep Research Assistant\",\n        page_icon=\"üîç\",\n        layout=\"wide\"\n    )\n\n    # Sidebar\n    st.sidebar.title(\"‚öôÔ∏è Settings\")\n    model_choice = st.sidebar.radio(\n        \"Choose Model\",\n        [\"OpenAI o3-mini\", \"Local DeepSeek-r1\"]\n    )\n    use_gpt = model_choice == \"OpenAI o3-mini\"\n\n    if use_gpt and not os.getenv(\"OPENAI_API_KEY\"):\n        st.sidebar.warning(\"‚ö†Ô∏è OpenAI API key not found\")\n    \n    if not use_gpt:\n        if not check_ollama_availability():\n            st.sidebar.warning(\"‚ö†Ô∏è Ollama not running. Run: `ollama pull deepseek-r1 && ollama run deepseek-r1`\")\n        else:\n            st.sidebar.success(\"‚úÖ Ollama running\")\n\n    # Main content\n    st.title(\"üîç Deep Research Assistant\")\n    st.markdown(\"\"\"\n    This AI-powered research assistant conducts comprehensive research on any topic.\n    It uses specialized agents to research, analyze, and synthesize information.\n    \"\"\")\n\n    # Input\n    query = st.text_area(\n        \"Research Topic\",\n        placeholder=\"Enter your research topic (be specific)...\",\n        help=\"More specific queries yield better results\"\n    )\n\n    col1, col2, col3 = st.columns([1, 1, 1])\n    with col2:\n        start_research = st.button(\"üöÄ Start Research\", type=\"primary\")\n\n    # Execute research\n    if start_research and query:\n        with st.spinner(\"üîç Conducting research...\"):\n            result = run_research(query, use_gpt)\n\n        if isinstance(result, str) and result.startswith(\"Error:\"):\n            st.error(result)\n        else:\n            st.success(\"‚úÖ Research Complete!\")\n            \n            tab1, tab2 = st.tabs([\"üìä Report\", \"‚ÑπÔ∏è About\"])\n            \n            with tab1:\n                st.markdown(\"### Research Report\")\n                st.markdown(\"---\")\n                st.markdown(str(result))  # Ensure result is converted to string\n                \n            with tab2:\n                st.markdown(f\"\"\"\n                ### Process:\n                1. **Research**: Comprehensive source search\n                2. **Analysis**: Pattern identification\n                3. **Synthesis**: Report creation\n                \n                **Details:**\n                - Model: {model_choice}\n                - Tools: Web search, content analysis\n                - Method: Multi-agent collaboration\n                \"\"\")\n    \n    st.divider()\n    st.markdown(\"*Built with CrewAI and Streamlit*\")\n\nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "docx.py", "content": "import os\nimport sys\nfrom docx import Document\n\ndef convert_docx_to_txt(input_folder):\n    # Ensure the input folder exists\n    if not os.path.exists(input_folder):\n        print(f\"The folder {input_folder} does not exist.\")\n        return\n\n    # Iterate through all files in the folder\n    for filename in os.listdir(input_folder):\n        if filename.endswith(\".docx\"):\n            docx_path = os.path.join(input_folder, filename)\n            txt_path = os.path.join(input_folder, f\"{os.path.splitext(filename)[0]}.txt\")\n\n            try:\n                # Open the docx file\n                doc = Document(docx_path)\n\n                # Extract text from the document\n                full_text = []\n                for para in doc.paragraphs:\n                    full_text.append(para.text)\n\n                # Write the extracted text to a new txt file\n                with open(txt_path, \"w\", encoding=\"utf-8\") as txt_file:\n                    txt_file.write(\"\\n\".join(full_text))\n\n                print(f\"Converted {filename} to TXT\")\n            except Exception as e:\n                print(f\"Error converting {filename}: {str(e)}\")\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 2:\n        print(\"Usage: python docx_to_txt_converter.py <folder_path>\")\n        sys.exit(1)\n\n    folder_path = sys.argv[1]\n    convert_docx_to_txt(folder_path)"}
{"type": "source_file", "path": "agents/thinking/o3-agent.py", "content": "import os\nfrom dotenv import load_dotenv\nfrom crewai import Agent, Task, Crew\nfrom crewai_tools import SerperDevTool, WebsiteSearchTool\nfrom langchain_openai import ChatOpenAI\nfrom langchain_community.llms import Ollama\n\n# Load environment variables\nload_dotenv()\nos.environ[\"SERPER_API_KEY\"] = os.getenv(\"SERPER_API_KEY\")\n\n# Initialize enhanced search tools\nsearch_tool = SerperDevTool()\nwebsite_tool = WebsiteSearchTool()\n\ndef get_llm(use_gpt=True):\n    \"\"\"Get the specified language model\"\"\"\n    if use_gpt:\n        return ChatOpenAI(\n            model_name=\"o3-mini\",\n        )\n    return Ollama(\n        model=\"deepseek-r1:latest\",\n        base_url=\"http://localhost:11434\",\n        temperature=0.7\n    )\n\ndef create_agents(use_gpt=True):\n    \"\"\"Create specialized research and analysis agents\"\"\"\n    llm = get_llm(use_gpt)\n    \n    deep_researcher = Agent(\n        role='Deep Research Specialist',\n        goal='Conduct comprehensive internet research and data gathering',\n        backstory=\"\"\"Expert at conducting deep, thorough research across multiple sources. \n        Skilled at finding hard-to-locate information and connecting disparate data points. \n        Specializes in complex research tasks that would typically take hours or days.\"\"\",\n        tools=[search_tool, website_tool],\n        llm=llm,\n        verbose=True,\n        max_iter=100,          # Increased iteration limit\n        allow_delegation=False, # Prevent unnecessary delegations\n        max_rpm=50,            # Rate limit for API calls\n        max_retry_limit=3      # Allow retries on failures\n    )\n    \n    analyst = Agent(\n        role='Research Analyst',\n        goal='Analyze and synthesize complex research findings',\n        backstory=\"\"\"Expert analyst skilled at processing large amounts of information,\n        identifying patterns, and drawing meaningful conclusions. Specializes in turning\n        raw research into actionable insights.\"\"\",\n        tools=[search_tool],\n        llm=llm,\n        verbose=True,\n        max_iter=75,\n        allow_delegation=False,\n        max_rpm=30,\n        max_retry_limit=2\n    )\n    \n    report_writer = Agent(\n        role='Research Report Writer',\n        goal='Create comprehensive, well-structured research reports',\n        backstory=\"\"\"Expert at transforming complex research and analysis into \n        clear, actionable reports. Skilled at maintaining detail while ensuring \n        accessibility and practical value.\"\"\",\n        llm=llm,\n        verbose=True,\n        max_iter=50,\n        allow_delegation=False,\n        max_rpm=20,\n        max_retry_limit=2\n    )\n    \n    return deep_researcher, analyst, report_writer\n\ndef create_tasks(researcher, analyst, writer, research_query):\n    \"\"\"Create research tasks with clear objectives\"\"\"\n    deep_research_task = Task(\n        description=f\"\"\"Conduct focused research on: {research_query}\n        \n        Step-by-step approach:\n        1. Initial broad search to identify key sources\n        2. Deep dive into most relevant sources\n        3. Extract specific details and evidence\n        4. Verify key findings across sources\n        5. Document sources and findings clearly\n        \n        Keep focused on specific, verified information.\"\"\",\n        agent=researcher,\n        expected_output=\"Detailed research findings with verified sources\"\n    )\n    \n    analysis_task = Task(\n        description=f\"\"\"Analyze the research findings about {research_query}:\n        \n        Follow these steps:\n        1. Review and categorize all findings\n        2. Identify main themes and patterns\n        3. Evaluate source credibility\n        4. Note any inconsistencies\n        5. Summarize key insights\n        \n        Focus on clear, actionable analysis.\"\"\",\n        agent=analyst,\n        context=[deep_research_task],\n        expected_output=\"Clear analysis of findings with key insights\"\n    )\n    \n    report_task = Task(\n        description=f\"\"\"Create a structured report about {research_query}:\n        \n        Include:\n        1. Executive summary (2-3 paragraphs)\n        2. Key findings (bullet points)\n        3. Supporting evidence\n        4. Conclusions\n        5. References\n        \n        Keep it clear and focused.\"\"\",\n        agent=writer,\n        context=[deep_research_task, analysis_task],\n        expected_output=\"Concise, well-structured report\"\n    )\n    \n    return [deep_research_task, analysis_task, report_task]\n\ndef create_crew(agents, tasks):\n    \"\"\"Create a crew with optimal settings\"\"\"\n    return Crew(\n        agents=agents,\n        tasks=tasks,\n        verbose=True,\n        max_rpm=100,  # Overall crew rate limit\n        process=\"sequential\"\n    )\n\ndef main():\n    print(\"\\nüîç Welcome to Deep Research Crew!\")\n    print(\"\\nAvailable Models:\")\n    print(\"1. OpenAI o3-mini (Requires API key)\")\n    print(\"2. Local DeepSeek-r1 (Requires Ollama)\")\n    \n    use_gpt = input(\"\\nUse OpenAI o3-mini? (yes/no): \").lower() == 'yes'\n    \n    if not use_gpt:\n        print(\"\\nUsing Ollama with DeepSeek-r1\")\n        print(\"Ensure Ollama is running: ollama run deepseek-r1:latest\")\n    \n    query = input(\"\\nWhat would you like researched? (Be specific): \")\n    \n    try:\n        researcher, analyst, writer = create_agents(use_gpt)\n        tasks = create_tasks(researcher, analyst, writer, query)\n        crew = create_crew([researcher, analyst, writer], tasks)\n        \n        print(\"\\nüîç Starting deep research process...\")\n        result = crew.kickoff()\n        \n        print(\"\\nüìä Research Report:\")\n        print(\"==================\")\n        print(result)\n        \n    except Exception as e:\n        print(f\"\\n‚ùå Error: {str(e)}\")\n        if use_gpt:\n            print(\"\\nTip: Check your OpenAI API key\")\n        else:\n            print(\"\\nTip: Ensure Ollama is running with deepseek-r1:latest\")\n            print(\"Run: ollama run deepseek-r1:latest\")\n\nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "agents/sales/run_email_preview.py", "content": "#!/usr/bin/env python3\n\nimport os\nimport sys\nimport subprocess\n\n# Get the current directory\ncurrent_dir = os.path.dirname(os.path.abspath(__file__))\n\ndef main():\n    \"\"\"Run the email preview Streamlit application\"\"\"\n    print(\"Starting Email Preview & Send application...\")\n    \n    # Use the Streamlit CLI to run the app\n    streamlit_cmd = [\"streamlit\", \"run\", os.path.join(current_dir, \"email_preview.py\")]\n    \n    try:\n        # Run the Streamlit command\n        subprocess.run(streamlit_cmd)\n    except KeyboardInterrupt:\n        print(\"\\nApplication stopped.\")\n    except Exception as e:\n        print(f\"Error running Streamlit application: {str(e)}\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main() "}
{"type": "source_file", "path": "agents/startup-ideas/main.py", "content": "import os\nimport streamlit as st\nfrom dotenv import load_dotenv\nfrom crewai import Agent, Task, Crew, LLM\nfrom langchain_openai import ChatOpenAI\nfrom crewai_tools import SerperDevTool, WebsiteSearchTool\n\n# Load environment variables\nload_dotenv()\nos.environ[\"SERPER_API_KEY\"] = os.getenv(\"SERPER_API_KEY\")\nos.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n\n# Initialize tools\nsearch_tool = SerperDevTool()\nweb_search_tool = WebsiteSearchTool()\n\ndef get_llm():\n    \"\"\"Setup LLM with appropriate configuration.\"\"\"\n    return LLM(\n        model=\"openai/gpt-4o-mini\",\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        verbose=True\n    )\n\ndef create_agents():\n    \"\"\"Create specialized startup validation agents.\"\"\"\n    # Get the LLM\n    llm = get_llm()\n    \n    # Market Research Analyst\n    market_analyst = Agent(\n        role='Market Research Analyst',\n        goal='Research market size, trends, and competitive landscape for startup ideas',\n        backstory='''You are an experienced market research analyst who specializes in emerging \n                    industries and technology trends. You have a keen eye for identifying market \n                    opportunities and potential red flags. You excel at analyzing TAM, SAM, and \n                    SOM for new ventures.''',\n        tools=[search_tool, web_search_tool],\n        llm=llm,\n        verbose=True\n    )\n    \n    # Startup Ecosystem Expert\n    ecosystem_expert = Agent(\n        role='Startup Ecosystem Expert',\n        goal='Analyze similar startups, funding patterns, and YC/accelerator trends',\n        backstory='''You have deep knowledge of the startup ecosystem, including Y Combinator, \n                    Techstars, and other major accelerators. You stay up-to-date with funding \n                    rounds, acquisitions, and startup failures. You can identify patterns in \n                    what kinds of startups get funded and succeed.''',\n        tools=[search_tool, web_search_tool],\n        llm=llm,\n        verbose=True\n    )\n    \n    # Business Model Strategist\n    business_strategist = Agent(\n        role='Business Model Strategist',\n        goal='Evaluate business models, revenue streams, and go-to-market strategies',\n        backstory='''You are a business strategist with experience in designing sustainable \n                    business models. You understand various revenue models, pricing strategies, \n                    customer acquisition channels, and go-to-market approaches. You can assess \n                    the viability of a business model and suggest improvements.''',\n        tools=[search_tool],\n        llm=llm,\n        verbose=True\n    )\n    \n    # Investment Analyst\n    investment_analyst = Agent(\n        role='Startup Investment Analyst',\n        goal='Assess investment potential, valuation aspects, and investor appeal',\n        backstory='''You've worked with venture capital firms and angel investors, evaluating \n                    hundreds of pitch decks and investment opportunities. You understand what \n                    investors look for in early-stage startups and can identify red flags as \n                    well as promising signals.''',\n        tools=[search_tool],\n        llm=llm,\n        verbose=True\n    )\n    \n    return market_analyst, ecosystem_expert, business_strategist, investment_analyst\n\ndef create_tasks(market_analyst, ecosystem_expert, business_strategist, investment_analyst, startup_info):\n    \"\"\"Create tasks for each agent based on startup information.\"\"\"\n    \n    # Task 1: Market Research Analysis\n    market_research = Task(\n        description=f'''Research and analyze the market for a startup with the following details:\n            - Industry: {startup_info['industry']}\n            - Product/Service: {startup_info['product_description']}\n            - Target Customers: {startup_info['target_customers']}\n            - Problem Statement: {startup_info['problem_statement']}\n            \n            Provide comprehensive market research including:\n            1. Total Addressable Market (TAM) size and growth rate\n            2. Serviceable Available Market (SAM) analysis\n            3. Key market trends and future outlook\n            4. Main competitors and their market share\n            5. Market entry barriers\n            6. Regulatory considerations\n            7. Market validation indicators or red flags''',\n        agent=market_analyst,\n        expected_output=\"A detailed market analysis report with quantitative and qualitative data\"\n    )\n    \n    # Task 2: Startup Ecosystem Analysis\n    ecosystem_analysis = Task(\n        description=f'''Analyze similar startups and ecosystem patterns for this idea:\n            - Industry: {startup_info['industry']}\n            - Product/Service: {startup_info['product_description']}\n            - Business Model: {startup_info['business_model']}\n            - Unique Value Proposition: {startup_info['value_proposition']}\n            \n            Research and provide:\n            1. Similar startups that have been funded in the last 3 years\n            2. Startups in this space that have been accepted to Y Combinator, Techstars, or other major accelerators\n            3. Success stories and failure cases in this domain\n            4. Current funding trends for this type of startup\n            5. Insights from YC partners or prominent VCs about this space\n            6. Potential acquirers if the startup succeeds\n            7. Red flags based on previous failures in this space''',\n        agent=ecosystem_expert,\n        context=[market_research],\n        expected_output=\"A comprehensive analysis of the startup ecosystem relevant to this idea\"\n    )\n    \n    # Task 3: Business Model Evaluation\n    business_model = Task(\n        description=f'''Evaluate the business model and go-to-market strategy:\n            - Revenue Model: {startup_info['revenue_model']}\n            - Pricing Strategy: {startup_info['pricing']}\n            - Customer Acquisition Strategy: {startup_info['acquisition_strategy']}\n            - Unit Economics: {startup_info['unit_economics']}\n            - Unique Value Proposition: {startup_info['value_proposition']}\n            \n            Analyze and provide:\n            1. Viability assessment of the proposed business model\n            2. Comparative analysis with business models of successful companies in this space\n            3. Potential alternative or complementary revenue streams\n            4. Go-to-market strategy recommendations\n            5. Customer acquisition cost (CAC) and lifetime value (LTV) considerations\n            6. Scalability assessment\n            7. Potential pivots if the original model faces challenges''',\n        agent=business_strategist,\n        context=[market_research, ecosystem_analysis],\n        expected_output=\"A detailed business model evaluation with actionable recommendations\"\n    )\n    \n    # Task 4: Investment Potential Assessment\n    investment_assessment = Task(\n        description=f'''Assess the investment potential and create a final recommendation:\n            - Founding Team: {startup_info['team_background']}\n            - Funding Needs: {startup_info['funding_needs']}\n            - Current Traction: {startup_info['current_traction']}\n            - Competitive Advantage: {startup_info['competitive_advantage']}\n            - Timeline: {startup_info['timeline']}\n            \n            Drawing on all previous analyses, provide:\n            1. Overall investment attractiveness on a scale of 1-10 with justification\n            2. Key strengths of the startup idea\n            3. Critical risks and weaknesses to address\n            4. Potential valuation range based on comparable companies\n            5. Recommended next steps for the founding team\n            6. Specific metrics investors would want to see before investing\n            7. Types of investors likely to be interested (angels, seed VCs, etc.)\n            8. Final recommendation: Proceed, Pivot, or Reconsider''',\n        agent=investment_analyst,\n        context=[market_research, ecosystem_analysis, business_model],\n        expected_output=\"A comprehensive investment assessment and final recommendation\"\n    )\n    \n    return [market_research, ecosystem_analysis, business_model, investment_assessment]\n\ndef create_crew(agents, tasks):\n    \"\"\"Create the CrewAI crew with the specified agents and tasks.\"\"\"\n    return Crew(\n        agents=agents,\n        tasks=tasks,\n        verbose=True,\n        planning=True,  # Enable planning for better coordination between agents\n        memory=True     # Enable memory for agents to remember context from previous tasks\n    )\n\ndef run_startup_validator(startup_info):\n    \"\"\"Run the startup validator with the provided information.\"\"\"\n    try:\n        # Create agents\n        market_analyst, ecosystem_expert, business_strategist, investment_analyst = create_agents()\n        \n        # Create tasks\n        tasks = create_tasks(market_analyst, ecosystem_expert, business_strategist, investment_analyst, startup_info)\n        \n        # Create crew with planning enabled for better coordination\n        crew = create_crew([market_analyst, ecosystem_expert, business_strategist, investment_analyst], tasks)\n        \n        # Execute the crew\n        with st.spinner('Our startup validation team is analyzing your idea. This may take a few minutes...'):\n            result = crew.kickoff()\n            \n            # Convert the CrewOutput to string if it's not already\n            if hasattr(result, 'raw'):\n                result = result.raw\n            elif not isinstance(result, str):\n                result = str(result)\n        \n        return result\n    except Exception as e:\n        st.error(f\"An error occurred: {str(e)}\")\n        return None\n\ndef app():\n    \"\"\"Main Streamlit application.\"\"\"\n    st.set_page_config(page_title=\"Startup Idea Validator\", page_icon=\"üöÄ\", layout=\"wide\")\n    \n    st.title(\"üöÄ Startup Idea Validator\")\n    st.markdown(\"\"\"\n    Get comprehensive validation of your startup idea through market research, ecosystem analysis, \n    business model evaluation, and investment potential assessment.\n    Our AI team will analyze your idea from multiple perspectives to provide insights and recommendations.\n    \"\"\")\n    \n    # Create tabs for organization\n    tab1, tab2, tab3 = st.tabs([\"Idea Basics\", \"Business Model\", \"Team & Traction\"])\n    \n    with tab1:\n        st.header(\"Startup Concept\")\n        col1, col2 = st.columns(2)\n        \n        with col1:\n            industry = st.selectbox(\n                \"Industry\", \n                [\"SaaS\", \"FinTech\", \"HealthTech\", \"EdTech\", \"E-commerce\", \"AI/ML\", \n                 \"CleanTech\", \"Consumer App\", \"Hardware\", \"Marketplace\", \"Other\"]\n            )\n            \n            if industry == \"Other\":\n                industry = st.text_input(\"Specify Industry\")\n                \n            problem_statement = st.text_area(\n                \"Problem Statement\",\n                placeholder=\"Describe the problem your startup aims to solve...\"\n            )\n            \n        with col2:\n            product_description = st.text_area(\n                \"Product/Service Description\",\n                placeholder=\"Describe your product or service in detail...\"\n            )\n            \n            target_customers = st.text_area(\n                \"Target Customers\",\n                placeholder=\"Describe your ideal customer segments...\"\n            )\n    \n    with tab2:\n        st.header(\"Business Model\")\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            business_model = st.selectbox(\n                \"Business Model Type\",\n                [\"SaaS\", \"Marketplace\", \"E-commerce\", \"Subscription\", \"Freemium\", \n                 \"Transaction Fee\", \"Advertising\", \"Hardware\", \"Data/API\", \"Other\"]\n            )\n            \n            if business_model == \"Other\":\n                business_model = st.text_input(\"Specify Business Model\")\n            \n            revenue_model = st.text_area(\n                \"Revenue Model Details\",\n                placeholder=\"Explain how your startup will generate revenue...\"\n            )\n            \n            pricing = st.text_area(\n                \"Pricing Strategy\",\n                placeholder=\"Explain your pricing structure and strategy...\"\n            )\n        \n        with col2:\n            acquisition_strategy = st.text_area(\n                \"Customer Acquisition Strategy\",\n                placeholder=\"How will you acquire customers? What channels will you use?\"\n            )\n            \n            unit_economics = st.text_area(\n                \"Unit Economics\",\n                placeholder=\"Share any information about CAC, LTV, margins, etc. (if available)\"\n            )\n            \n            value_proposition = st.text_area(\n                \"Unique Value Proposition\",\n                placeholder=\"What makes your solution unique? Why would customers choose you over alternatives?\"\n            )\n    \n    with tab3:\n        st.header(\"Team & Traction\")\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            team_background = st.text_area(\n                \"Team Background\",\n                placeholder=\"Describe founders' experience, skills, and relevant background...\"\n            )\n            \n            current_traction = st.text_area(\n                \"Current Traction\",\n                placeholder=\"Describe any users, revenue, pilots, waitlist, or other traction metrics...\"\n            )\n            \n            funding_needs = st.text_input(\n                \"Funding Needs (if any)\",\n                placeholder=\"E.g., $500K seed round\"\n            )\n        \n        with col2:\n            competitive_advantage = st.text_area(\n                \"Competitive Advantage\",\n                placeholder=\"What sustainable advantages do you have over potential competitors?\"\n            )\n            \n            timeline = st.text_area(\n                \"Timeline & Milestones\",\n                placeholder=\"Share your key milestones and timeline for the next 12-18 months\"\n            )\n            \n            additional_info = st.text_area(\n                \"Additional Information\",\n                placeholder=\"Any other relevant details you'd like our validation team to consider\"\n            )\n    \n    # Collect all startup information\n    startup_info = {\n        \"industry\": industry,\n        \"problem_statement\": problem_statement,\n        \"product_description\": product_description,\n        \"target_customers\": target_customers,\n        \"business_model\": business_model,\n        \"revenue_model\": revenue_model,\n        \"pricing\": pricing,\n        \"acquisition_strategy\": acquisition_strategy,\n        \"unit_economics\": unit_economics or \"Not provided\",\n        \"value_proposition\": value_proposition,\n        \"team_background\": team_background,\n        \"current_traction\": current_traction or \"Pre-launch\",\n        \"funding_needs\": funding_needs or \"Not specified\",\n        \"competitive_advantage\": competitive_advantage,\n        \"timeline\": timeline or \"Not provided\",\n        \"additional_info\": additional_info or \"None\"\n    }\n    \n    # Check if API keys are present\n    if not os.getenv(\"SERPER_API_KEY\") or not os.getenv(\"OPENAI_API_KEY\"):\n        st.warning(\"‚ö†Ô∏è API keys not detected. Please add your SERPER_API_KEY and OPENAI_API_KEY to your .env file.\")\n    \n    # Create a submission button\n    if st.button(\"Validate Startup Idea\"):\n        required_fields = [\"problem_statement\", \"product_description\", \"target_customers\", \"value_proposition\"]\n        missing_fields = [field for field in required_fields if not startup_info[field]]\n        \n        if missing_fields:\n            st.error(f\"Please fill in the following required fields: {', '.join(missing_fields)}\")\n            return\n        \n        # Display startup information summary\n        with st.expander(\"Summary of Your Startup Information\"):\n            st.json(startup_info)\n        \n        # Run the startup validator\n        result = run_startup_validator(startup_info)\n        \n        if result:\n            st.success(\"‚úÖ Your startup idea validation is complete!\")\n            st.markdown(\"## Startup Validation Report\")\n            st.markdown(result)\n            \n            # Add download capability\n            try:\n                st.download_button(\n                    label=\"Download Validation Report\",\n                    data=result,\n                    file_name=\"startup_validation_report.md\",\n                    mime=\"text/markdown\"\n                )\n            except Exception as e:\n                st.error(f\"Could not generate download button: {str(e)}\")\n                st.info(\"You can copy the report text manually from above.\")\n\nif __name__ == \"__main__\":\n    app()"}
{"type": "source_file", "path": "agents/thinking/model-comparison.py", "content": "import streamlit as st\nimport os\nfrom dotenv import load_dotenv\nfrom crewai import Agent, Task, Crew, LLM\nfrom crewai_tools import SerperDevTool, WebsiteSearchTool\nfrom langchain_openai import ChatOpenAI\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nimport requests\nimport asyncio\nfrom concurrent.futures import ThreadPoolExecutor\nimport google.generativeai as genai\n\n# Load environment variables\nload_dotenv()\nos.environ[\"SERPER_API_KEY\"] = os.getenv(\"SERPER_API_KEY\")\n\n# Initialize enhanced search tools\nsearch_tool = SerperDevTool()\nwebsite_tool = WebsiteSearchTool()\n\ndef check_ollama_availability():\n    \"\"\"Check if Ollama server is running\"\"\"\n    try:\n        response = requests.get(\"http://localhost:11434/api/version\")\n        return response.status_code == 200\n    except requests.exceptions.ConnectionError:\n        return False\n\ndef get_llm(model_type):\n    \"\"\"Initialize the specified language model\"\"\"   \n    if model_type == \"o3-mini\":\n        return ChatOpenAI(model_name=\"o3-mini\")\n    elif model_type == \"gemini\":\n        return LLM(\n            model=\"gemini/gemini-2.0-flash-001\",\n            temperature=0.7,\n            max_tokens=2048,\n            top_p=0.8,\n            vertex_credentials=None  # Will use GOOGLE_API_KEY environment variable\n        )\n    else:  # deepseek\n        return LLM(\n            model=\"ollama/deepseek-r1:latest\",\n            base_url=\"http://localhost:11434\",\n            temperature=0.7\n        )\n\ndef create_agents(model_type):\n    \"\"\"Create specialized research and analysis agents for a specific model\"\"\"\n    try:\n        llm = get_llm(model_type)\n        \n        researcher = Agent(\n            role=f'Deep Research Specialist ({model_type})',\n            goal='Conduct comprehensive research and gather detailed information',\n            backstory=\"\"\"Expert researcher skilled at discovering hard-to-find information \n            and connecting complex data points. Specializes in thorough, detailed research.\"\"\",\n            tools=[search_tool, website_tool],\n            llm=llm,\n            verbose=True,\n            max_iter=15,\n            allow_delegation=False\n        )\n        \n        analyst = Agent(\n            role=f'Research Analyst ({model_type})',\n            goal='Analyze and synthesize research findings',\n            backstory=\"\"\"Expert analyst skilled at processing complex information and \n            identifying key patterns and insights. Specializes in clear, actionable analysis.\"\"\",\n            tools=[search_tool],\n            llm=llm,\n            verbose=True,\n            max_iter=10,\n            allow_delegation=False\n        )\n        \n        writer = Agent(\n            role=f'Content Synthesizer ({model_type})',\n            goal='Create clear, structured reports from analysis',\n            backstory=\"\"\"Expert writer skilled at transforming complex analysis into \n            clear, engaging content while maintaining technical accuracy.\"\"\",\n            llm=llm,\n            verbose=True,\n            max_iter=8,\n            allow_delegation=False\n        )\n        \n        return researcher, analyst, writer\n    except Exception as e:\n        st.error(f\"Error creating agents for {model_type}: {str(e)}\")\n        return None, None, None\n\ndef create_tasks(researcher, analyst, writer, topic):\n    \"\"\"Create research tasks with clear objectives\"\"\"\n    research_task = Task(\n        description=f\"\"\"Research this topic thoroughly: {topic}\n        \n        Follow these steps:\n        1. Find reliable sources and latest information\n        2. Extract key details and evidence\n        3. Verify information across sources\n        4. Document findings with references\"\"\",\n        agent=researcher,\n        expected_output=\"Detailed research findings with sources\"\n    )\n    \n    analysis_task = Task(\n        description=f\"\"\"Analyze the research findings about {topic}:\n        \n        Steps:\n        1. Review and categorize findings\n        2. Identify patterns and trends\n        3. Evaluate source credibility\n        4. Note key insights\"\"\",\n        agent=analyst,\n        context=[research_task],\n        expected_output=\"Analysis of findings and insights\"\n    )\n    \n    synthesis_task = Task(\n        description=f\"\"\"Create a clear report on {topic}:\n        \n        Include:\n        - Executive Summary\n        - Key Findings\n        - Evidence\n        - Conclusions\n        - Specific questions asked by the user\n        - search volume, demand, search conversion\n        - Top keywords\n        - References\"\"\",\n        agent=writer,\n        context=[research_task, analysis_task],\n        expected_output=\"Structured report with insights\"\n    )\n    \n    return [research_task, analysis_task, synthesis_task]\n\ndef run_single_model_research(topic, model_type):\n    \"\"\"Execute research process for a single model\"\"\"\n    try:\n        researcher, analyst, writer = create_agents(model_type)\n        if not all([researcher, analyst, writer]):\n            raise Exception(f\"Failed to create agents for {model_type}\")\n        \n        tasks = create_tasks(researcher, analyst, writer, topic)\n        crew = Crew(\n            agents=[researcher, analyst, writer],\n            tasks=tasks,\n            verbose=True\n        )\n        \n        result = crew.kickoff()\n        return str(result)\n    except Exception as e:\n        return f\"Error with {model_type}: {str(e)}\"\n\n@st.cache_data(ttl=3600)\ndef run_parallel_research(topic, selected_models):\n    \"\"\"Execute research process in parallel for multiple models\"\"\"\n    results = {}\n    \n    for model in selected_models:\n        try:\n            with st.spinner(f\"üîç Researching with {model}...\"):\n                result = run_single_model_research(topic, model)\n                results[model] = result\n        except Exception as e:\n            results[model] = f\"Error with {model}: {str(e)}\"\n    \n    return results\n\ndef check_api_keys():\n    \"\"\"Check availability of required API keys\"\"\"\n    api_status = {\n        \"gpt-4o-mini\": bool(os.getenv(\"OPENAI_API_KEY\")),\n        \"gemini\": bool(os.getenv(\"GEMINI_API_KEY\")),\n        \"deepseek\": check_ollama_availability()\n    }\n    return api_status\n\ndef main():\n    st.set_page_config(\n        page_title=\"Multi-LLM Research Assistant\",\n        page_icon=\"üîç\",\n        layout=\"wide\"\n    )\n\n    # Sidebar settings\n    st.sidebar.title(\"‚öôÔ∏è Model Selection\")\n    \n    # Check API keys and model availability\n    api_status = check_api_keys()\n    \n    # Model selection with checkboxes\n    st.sidebar.markdown(\"### Choose Models\")\n    selected_models = []\n    \n    if api_status[\"gpt-4o-mini\"]:\n        if st.sidebar.checkbox(\"OpenAI GPT-4o-mini\", value=True):\n            selected_models.append(\"gpt-4o-mini\")\n    else:\n        st.sidebar.warning(\"‚ö†Ô∏è OpenAI API key not found\")\n        \n    if api_status[\"gemini\"]:\n        if st.sidebar.checkbox(\"Google Gemini-2.0\", value=True):\n            selected_models.append(\"gemini\")\n    else:\n        st.sidebar.warning(\"‚ö†Ô∏è Google API key not found\")\n        \n    if api_status[\"deepseek\"]:\n        if st.sidebar.checkbox(\"Local DeepSeek-r1\", value=True):\n            selected_models.append(\"deepseek\")\n    else:\n        st.sidebar.warning(\"‚ö†Ô∏è Ollama not running\")\n\n    # Main content\n    st.title(\"üîç Multi-LLM Research Assistant\")\n    st.markdown(\"\"\"\n    This enhanced research assistant uses multiple AI models in parallel to provide comprehensive, \n    multi-perspective research on any topic. Select one or more models to compare their analyses.\n    \"\"\")\n\n    # Input\n    query = st.text_area(\n        \"Research Topic\",\n        placeholder=\"Enter your research topic (be specific)...\",\n        help=\"More specific queries yield better results\"\n    )\n\n    col1, col2, col3 = st.columns([1, 1, 1])\n    with col2:\n        start_research = st.button(\n            f\"üöÄ Start Research with {len(selected_models)} Model{'s' if len(selected_models) != 1 else ''}\", \n            type=\"primary\",\n            disabled=len(selected_models) == 0\n        )\n\n    # Execute research\n    if start_research and query and selected_models:\n        with st.spinner(f\"üîç Conducting research using {len(selected_models)} models...\"):\n            results = run_parallel_research(query, selected_models)\n\n        st.success(\"‚úÖ Research Complete!\")\n        \n        # Create tabs for each model plus comparison\n        tabs = [f\"üìä {model.upper()}\" for model in selected_models]\n        if len(selected_models) > 1:\n            tabs.append(\"üîÑ Comparison\")\n            \n        tab_list = st.tabs(tabs)\n        \n        # Display individual results\n        for i, model in enumerate(selected_models):\n            with tab_list[i]:\n                st.markdown(f\"### Research Report from {model.upper()}\")\n                st.markdown(\"---\")\n                st.markdown(str(results[model]))\n        \n        # Display comparison if multiple models selected\n        if len(selected_models) > 1:\n            with tab_list[-1]:\n                st.markdown(\"### Model Comparison\")\n                st.markdown(\"---\")\n                for model in selected_models:\n                    with st.expander(f\"{model.upper()} Summary\"):\n                        # Extract and display key points from each model's results\n                        st.markdown(str(results[model]))\n    \n    st.divider()\n    st.markdown(\"*Built with CrewAI, Streamlit, and Multiple LLMs*\")\n\nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "agents/real-estate/main.py", "content": "from typing import Dict, List\nfrom pydantic import BaseModel, Field\nfrom agno.agent import Agent\nfrom agno.models.openai import OpenAIChat\nfrom firecrawl import FirecrawlApp\nimport streamlit as st\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file if it exists\nload_dotenv()\n\nclass PropertyData(BaseModel):\n    \"\"\"Schema for property data extraction\"\"\"\n    building_name: str = Field(description=\"Name of the building/property\", alias=\"Building_name\")\n    property_type: str = Field(description=\"Type of property (commercial, residential, etc)\", alias=\"Property_type\")\n    location_address: str = Field(description=\"Complete address of the property\")\n    price: str = Field(description=\"Price of the property\", alias=\"Price\")\n    description: str = Field(description=\"Detailed description of the property\", alias=\"Description\")\n\nclass PropertiesResponse(BaseModel):\n    \"\"\"Schema for multiple properties response\"\"\"\n    properties: List[PropertyData] = Field(description=\"List of property details\")\n\nclass LocationData(BaseModel):\n    \"\"\"Schema for location price trends\"\"\"\n    location: str\n    price_per_sqft: float\n    percent_increase: float\n    rental_yield: float\n\nclass LocationsResponse(BaseModel):\n    \"\"\"Schema for multiple locations response\"\"\"\n    locations: List[LocationData] = Field(description=\"List of location data points\")\n\nclass FirecrawlResponse(BaseModel):\n    \"\"\"Schema for Firecrawl API response\"\"\"\n    success: bool\n    data: Dict\n    status: str\n    expiresAt: str\n\nclass PropertyFindingAgent:\n    \"\"\"Agent responsible for finding properties and providing recommendations\"\"\"\n    \n    def __init__(self, firecrawl_api_key: str, openai_api_key: str, model_id: str = \"o3-mini\"):\n        self.agent = Agent(\n            model=OpenAIChat(id=model_id, api_key=openai_api_key),\n            markdown=True,\n            description=\"I am a real estate expert who helps find and analyze properties based on user preferences.\"\n        )\n        self.firecrawl = FirecrawlApp(api_key=firecrawl_api_key)\n\n    def find_properties(\n        self, \n        city: str,\n        max_price: float,\n        property_category: str = \"Residential\",\n        property_type: str = \"Flat\"\n    ) -> str:\n        \"\"\"Find and analyze properties based on user preferences\"\"\"\n        formatted_location = city.lower()\n        \n        urls = [\n            f\"https://www.squareyards.com/sale/property-for-sale-in-{formatted_location}/*\",\n            f\"https://www.99acres.com/property-in-{formatted_location}-ffid/*\",\n            f\"https://housing.com/in/buy/{formatted_location}/{formatted_location}\",\n            # f\"https://www.nobroker.in/property/sale/{city}/{formatted_location}\",\n        ]\n        \n        property_type_prompt = \"Flats\" if property_type == \"Flat\" else \"Individual Houses\"\n        \n        raw_response = self.firecrawl.extract(\n            urls=urls,\n            params={\n                'prompt': f\"\"\"Extract ONLY 10 OR LESS different {property_category} {property_type_prompt} from {city} that cost less than {max_price} crores.\n                \n                Requirements:\n                - Property Category: {property_category} properties only\n                - Property Type: {property_type_prompt} only\n                - Location: {city}\n                - Maximum Price: {max_price} crores\n                - Include complete property details with exact location\n                - IMPORTANT: Return data for at least 3 different properties. MAXIMUM 10.\n                - Format as a list of properties with their respective details\n                \"\"\",\n                'schema': PropertiesResponse.model_json_schema()\n            }\n        )\n        \n        print(\"Raw Property Response:\", raw_response)\n        \n        if isinstance(raw_response, dict) and raw_response.get('success'):\n            properties = raw_response['data'].get('properties', [])\n        else:\n            properties = []\n            \n        print(\"Processed Properties:\", properties)\n\n        \n        analysis = self.agent.run(\n            f\"\"\"As a real estate expert, analyze these properties and market trends:\n\n            Properties Found in json format:\n            {properties}\n\n            **IMPORTANT INSTRUCTIONS:**\n            1. ONLY analyze properties from the above JSON data that match the user's requirements:\n               - Property Category: {property_category}\n               - Property Type: {property_type}\n               - Maximum Price: {max_price} crores\n            2. DO NOT create new categories or property types\n            3. From the matching properties, select 5-6 properties with prices closest to {max_price} crores\n\n            Please provide your analysis in this format:\n            \n            üè† SELECTED PROPERTIES\n            ‚Ä¢ List only 5-6 best matching properties with prices closest to {max_price} crores\n            ‚Ä¢ For each property include:\n              - Name and Location\n              - Price (with value analysis)\n              - Key Features\n              - Pros and Cons\n\n            üí∞ BEST VALUE ANALYSIS\n            ‚Ä¢ Compare the selected properties based on:\n              - Price per sq ft\n              - Location advantage\n              - Amenities offered\n\n            üìç LOCATION INSIGHTS\n            ‚Ä¢ Specific advantages of the areas where selected properties are located\n\n            üí° RECOMMENDATIONS\n            ‚Ä¢ Top 3 properties from the selection with reasoning\n            ‚Ä¢ Investment potential\n            ‚Ä¢ Points to consider before purchase\n\n            ü§ù NEGOTIATION TIPS\n            ‚Ä¢ Property-specific negotiation strategies\n\n            Format your response in a clear, structured way using the above sections.\n            \"\"\"\n        )\n        \n        return analysis.content\n\n    def get_location_trends(self, city: str) -> str:\n        \"\"\"Get price trends for different localities in the city\"\"\"\n        raw_response = self.firecrawl.extract([\n            f\"https://www.99acres.com/property-rates-and-price-trends-in-{city.lower()}-prffid/*\"\n        ], {\n            'prompt': \"\"\"Extract price trends data for ALL major localities in the city. \n            IMPORTANT: \n            - Return data for at least 5-10 different localities\n            - Include both premium and affordable areas\n            - Do not skip any locality mentioned in the source\n            - Format as a list of locations with their respective data\n            \"\"\",\n            'schema': LocationsResponse.model_json_schema(),\n        })\n        \n        if isinstance(raw_response, dict) and raw_response.get('success'):\n            locations = raw_response['data'].get('locations', [])\n    \n            analysis = self.agent.run(\n                f\"\"\"As a real estate expert, analyze these location price trends for {city}:\n\n                {locations}\n\n                Please provide:\n                1. A bullet-point summary of the price trends for each location\n                2. Identify the top 3 locations with:\n                   - Highest price appreciation\n                   - Best rental yields\n                   - Best value for money\n                3. Investment recommendations:\n                   - Best locations for long-term investment\n                   - Best locations for rental income\n                   - Areas showing emerging potential\n                4. Specific advice for investors based on these trends\n\n                Format the response as follows:\n                \n                üìä LOCATION TRENDS SUMMARY\n                ‚Ä¢ [Bullet points for each location]\n\n                üèÜ TOP PERFORMING AREAS\n                ‚Ä¢ [Bullet points for best areas]\n\n                üí° INVESTMENT INSIGHTS\n                ‚Ä¢ [Bullet points with investment advice]\n\n                üéØ RECOMMENDATIONS\n                ‚Ä¢ [Bullet points with specific recommendations]\n                \"\"\"\n            )\n            \n            return analysis.content\n            \n        return \"No price trends data available\"\n\ndef create_property_agent():\n    \"\"\"Create PropertyFindingAgent with API keys from session state\"\"\"\n    if 'property_agent' not in st.session_state:\n        st.session_state.property_agent = PropertyFindingAgent(\n            firecrawl_api_key=st.session_state.firecrawl_key,\n            openai_api_key=st.session_state.openai_key,\n            model_id=st.session_state.model_id\n        )\n\ndef main():\n    st.set_page_config(\n        page_title=\"AI Real Estate Agent\",\n        page_icon=\"üè†\",\n        layout=\"wide\"\n    )\n\n    # Get API keys from environment variables\n    env_firecrawl_key = os.getenv(\"FIRECRAWL_API_KEY\", \"\")\n    env_openai_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n    default_model = os.getenv(\"OPENAI_MODEL_ID\", \"o3-mini\")\n\n    with st.sidebar:\n        st.title(\"üîë API Configuration\")\n        \n        st.subheader(\"ü§ñ Model Selection\")\n        model_id = st.selectbox(\n            \"Choose OpenAI Model\",\n            options=[\"o3-mini\", \"gpt-4o-mini\"],\n            index=0 if default_model == \"o3-mini\" else 1,\n            help=\"Select the AI model to use. Choose gpt-4o if your api doesn't have access to o3-mini\"\n        )\n        st.session_state.model_id = model_id\n        \n        st.divider()\n        \n        st.subheader(\"üîê API Keys\")\n        \n        # Show environment variable status\n        if env_firecrawl_key:\n            st.success(\"‚úÖ Firecrawl API Key found in environment variables\")\n        if env_openai_key:\n            st.success(\"‚úÖ OpenAI API Key found in environment variables\")\n            \n        # Allow UI override of environment variables\n        firecrawl_key = st.text_input(\n            \"Firecrawl API Key (optional if set in environment)\",\n            type=\"password\",\n            help=\"Enter your Firecrawl API key or set FIRECRAWL_API_KEY in environment\",\n            value=\"\" if env_firecrawl_key else \"\"\n        )\n        openai_key = st.text_input(\n            \"OpenAI API Key (optional if set in environment)\",\n            type=\"password\",\n            help=\"Enter your OpenAI API key or set OPENAI_API_KEY in environment\",\n            value=\"\" if env_openai_key else \"\"\n        )\n        \n        # Use environment variables if UI inputs are empty\n        firecrawl_key = firecrawl_key or env_firecrawl_key\n        openai_key = openai_key or env_openai_key\n        \n        if firecrawl_key and openai_key:\n            st.session_state.firecrawl_key = firecrawl_key\n            st.session_state.openai_key = openai_key\n            create_property_agent()\n        else:\n            missing_keys = []\n            if not firecrawl_key:\n                missing_keys.append(\"Firecrawl API Key\")\n            if not openai_key:\n                missing_keys.append(\"OpenAI API Key\")\n            if missing_keys:\n                st.warning(f\"‚ö†Ô∏è Missing required API keys: {', '.join(missing_keys)}\")\n                st.info(\"Please provide the missing keys in the fields above or set them as environment variables.\")\n\n    st.title(\"üè† AI Real Estate Agent\")\n    st.info(\n        \"\"\"\n        Welcome to the AI Real Estate Agent! \n        Enter your search criteria below to get property recommendations \n        and location insights.\n        \"\"\"\n    )\n\n    col1, col2 = st.columns(2)\n    \n    with col1:\n        city = st.text_input(\n            \"City\",\n            placeholder=\"Enter city name (e.g., Bangalore)\",\n            help=\"Enter the city where you want to search for properties\"\n        )\n        \n        property_category = st.selectbox(\n            \"Property Category\",\n            options=[\"Residential\", \"Commercial\"],\n            help=\"Select the type of property you're interested in\"\n        )\n\n    with col2:\n        max_price = st.number_input(\n            \"Maximum Price (in Crores)\",\n            min_value=0.1,\n            max_value=100.0,\n            value=5.0,\n            step=0.1,\n            help=\"Enter your maximum budget in Crores\"\n        )\n        \n        property_type = st.selectbox(\n            \"Property Type\",\n            options=[\"Flat\", \"Individual House\"],\n            help=\"Select the specific type of property\"\n        )\n\n    if st.button(\"üîç Start Search\", use_container_width=True):\n        if 'property_agent' not in st.session_state:\n            st.error(\"‚ö†Ô∏è Please enter your API keys in the sidebar first!\")\n            return\n            \n        if not city:\n            st.error(\"‚ö†Ô∏è Please enter a city name!\")\n            return\n            \n        try:\n            with st.spinner(\"üîç Searching for properties...\"):\n                property_results = st.session_state.property_agent.find_properties(\n                    city=city,\n                    max_price=max_price,\n                    property_category=property_category,\n                    property_type=property_type\n                )\n                \n                st.success(\"‚úÖ Property search completed!\")\n                \n                st.subheader(\"üèòÔ∏è Property Recommendations\")\n                st.markdown(property_results)\n                \n                st.divider()\n                \n                with st.spinner(\"üìä Analyzing location trends...\"):\n                    location_trends = st.session_state.property_agent.get_location_trends(city)\n                    \n                    st.success(\"‚úÖ Location analysis completed!\")\n                    \n                    with st.expander(\"üìà Location Trends Analysis of the city\"):\n                        st.markdown(location_trends)\n                \n        except Exception as e:\n            st.error(f\"‚ùå An error occurred: {str(e)}\")\n\nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "agents/social_media/thinking-ant-social-media-calendar.py", "content": "import os\nfrom dotenv import load_dotenv\nfrom crewai import Agent, Task, Crew, LLM\nfrom crewai_tools import SerperDevTool, WebsiteSearchTool\nimport streamlit as st\nfrom datetime import datetime\nimport pandas as pd\nimport json\n\n# Load environment variables\nload_dotenv()\nos.environ[\"SERPER_API_KEY\"] = os.getenv(\"SERPER_API_KEY\")\nos.environ[\"ANTHROPIC_API_KEY\"] = os.getenv(\"ANTHROPIC_API_KEY\")\n\n# Initialize enhanced search tools\nsearch_tool = SerperDevTool()\nwebsite_tool = WebsiteSearchTool()\n\ndef get_claude_llm():\n    \"\"\"Get the Claude 3.7 Sonnet language model\"\"\"\n    return LLM(\n        model=\"anthropic/claude-3-7-sonnet-20250219\",\n        api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n        temperature=0.7,\n        verbose=True\n    )\n\ndef create_content_calendar_agents():\n    \"\"\"Create specialized agents for content calendar creation\"\"\"\n    llm = get_claude_llm()\n    \n    trend_researcher = Agent(\n        role='Content Trend Researcher',\n        goal='Identify current and upcoming content trends relevant to the target audience',\n        backstory=\"Expert at discovering trending topics and viral content patterns. You find what resonates with audiences.\",\n        tools=[search_tool, website_tool],\n        llm=llm,\n        verbose=True,\n        allow_delegation=False,\n        max_tokens=100\n    )\n    \n    content_strategist = Agent(\n        role='Content Calendar Strategist',\n        goal='Develop a strategic 7-day content plan based on research findings',\n        backstory=\"Experienced content strategist who creates balanced, engaging content calendars.\",\n        tools=[search_tool],\n        llm=llm,\n        verbose=True,\n        allow_delegation=False,\n        max_tokens=100\n    )\n    \n    content_creator = Agent(\n        role='Content Creator',\n        goal='Generate brief content outlines for each day of the calendar',\n        backstory=\"Creative content developer who transforms plans into actionable content briefs.\",\n        tools=[search_tool],\n        llm=llm,\n        verbose=True,\n        allow_delegation=False,\n        max_tokens=100\n    )\n    \n    return trend_researcher, content_strategist, content_creator\n\ndef create_content_calendar_tasks(researcher, strategist, creator, industry, target_audience, content_goals):\n    \"\"\"Create content calendar tasks with clear objectives but limited scope to manage token usage\"\"\"\n    # Truncate inputs if they're too long\n    industry = industry[:100] if industry else \"\"\n    target_audience = target_audience[:200] if target_audience else \"\"\n    content_goals = content_goals[:200] if content_goals else \"\"\n    \n    trend_research_task = Task(\n        description=f\"\"\"Research current trends in the {industry} industry for {target_audience}.\n        \n        Focus on:\n        1. Top content formats (video, blog, etc.)\n        2. Trending topics and hashtags\n        3. Upcoming events in the next 2 weeks\n        4. 5-7 potential content topics that align with: {content_goals}\"\"\",\n        agent=researcher,\n        expected_output=\"List of content trends and topic ideas (max 500 words)\"\n    )\n    \n    strategy_task = Task(\n        description=f\"\"\"Create a simple 7-day content calendar for {target_audience} based on the research.\n        \n        Include:\n        1. Mix of content types (educational, promotional, etc.)\n        2. One main topic per day\n        3. Brief rationale for each day\n        \n        Format as Day 1: [Topic] - [Type] - [Brief rationale]\"\"\",\n        agent=strategist,\n        context=[trend_research_task],\n        expected_output=\"7-day content calendar outline (max 500 words)\"\n    )\n    \n    content_brief_task = Task(\n        description=f\"\"\"Create brief content outlines for each day of the 7-day calendar.\n        \n        For each day include:\n        1. Headline\n        2. Brief hook\n        3. 3-5 key points\n        4. Call-to-action\n        \n        Keep each day's brief concise and focused.\"\"\",\n        agent=creator,\n        context=[trend_research_task, strategy_task],\n        expected_output=\"Brief outlines for 7 days of content (max 1000 words)\"\n    )\n    \n    return [trend_research_task, strategy_task, content_brief_task]\n\ndef create_crew(agents, tasks):\n    \"\"\"Create a crew with optimal settings and token limits\"\"\"\n    return Crew(\n        agents=agents,\n        tasks=tasks,\n        verbose=True,\n        process=\"sequential\",\n        max_rpm=10  # Limiting requests per minute to avoid rate limits\n    )\n\ndef run_content_calendar_creation(industry, target_audience, content_goals):\n    \"\"\"Run the content calendar creation process and return results\"\"\"\n    try:\n        start_time = datetime.now()\n        researcher, strategist, creator = create_content_calendar_agents()\n        tasks = create_content_calendar_tasks(researcher, strategist, creator, industry, target_audience, content_goals)\n        crew = create_crew([researcher, strategist, creator], tasks)\n        result = crew.kickoff()\n        execution_time = (datetime.now() - start_time).total_seconds()\n        return {'result': result, 'execution_time': execution_time}\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\ndef save_content_calendar(industry, target_audience, content_goals, result):\n    \"\"\"Save content calendar to JSON file\"\"\"\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    filename = f\"content_calendar_{timestamp}.json\"\n    \n    data = {\n        \"industry\": industry,\n        \"target_audience\": target_audience,\n        \"content_goals\": content_goals,\n        \"timestamp\": timestamp,\n        \"content_calendar\": result\n    }\n    \n    with open(filename, \"w\") as f:\n        json.dump(data, f, indent=4)\n    \n    return filename\n\ndef main():\n    st.set_page_config(page_title=\"7-Day Content Calendar Creator\", layout=\"wide\")\n    \n    st.title(\"üìÖ AI Content Calendar Creator\")\n    st.subheader(\"Powered by Claude 3.7 Sonnet\")\n    \n    # Display token usage warning\n    st.warning(\"‚ö†Ô∏è Token Usage Management: Please keep inputs brief to avoid rate limits.\")\n    \n    # Input form with character counters\n    with st.form(\"content_calendar_form\"):\n        industry = st.text_input(\"Industry/Niche (max 100 chars)\", placeholder=\"e.g., Fitness, SaaS, Digital Marketing\")\n        st.caption(f\"Characters: {len(industry)}/100\")\n        \n        target_audience = st.text_area(\"Target Audience (max 200 chars)\", placeholder=\"Key demographics and interests...\", height=80)\n        st.caption(f\"Characters: {len(target_audience)}/200\")\n        \n        content_goals = st.text_area(\"Content Goals (max 200 chars)\", placeholder=\"e.g., Increase brand awareness...\", height=80)\n        st.caption(f\"Characters: {len(content_goals)}/200\")\n        \n        submit_button = st.form_submit_button(\"Generate 7-Day Content Calendar\")\n    \n    if submit_button:\n        if not industry or not target_audience or not content_goals:\n            st.error(\"Please fill out all fields\")\n            return\n        \n        # Create progress tracking\n        progress_container = st.empty()\n        progress_bar = st.progress(0)\n        status_container = st.empty()\n        timer_container = st.empty()\n        \n        status_container.info(\"Starting content calendar creation...\")\n        start_time = datetime.now()\n        \n        # Update timer in a separate area\n        def update_timer():\n            while True:\n                elapsed = (datetime.now() - start_time).total_seconds()\n                timer_container.text(f\"‚è±Ô∏è Time elapsed: {elapsed:.1f}s\")\n                time.sleep(0.5)\n        \n        import threading\n        import time\n        timer_thread = threading.Thread(target=update_timer)\n        timer_thread.daemon = True\n        timer_thread.start()\n        \n        # Run the content calendar creation\n        result = run_content_calendar_creation(industry, target_audience, content_goals)\n        \n        if isinstance(result, dict):\n            # Save results to file\n            filename = save_content_calendar(industry, target_audience, content_goals, result['result'])\n            \n            # Show results\n            progress_bar.progress(100)\n            status_container.success(\"Content Calendar Created!\")\n            timer_container.text(f\"‚è±Ô∏è Total time: {result['execution_time']:.2f}s\")\n            \n            st.subheader(\"Your 7-Day Content Calendar\")\n            st.write(result['result'])\n            \n            # Create a download button for the JSON file\n            with open(filename, \"r\") as f:\n                st.download_button(\n                    label=\"Download Content Calendar (JSON)\",\n                    data=f,\n                    file_name=filename,\n                    mime=\"application/json\"\n                )\n            \n            # Display the calendar in a more visual format if possible\n            try:\n                # This is a basic attempt to extract calendar data - actual format may vary\n                days = result['result'].split(\"Day \")\n                if len(days) > 1:\n                    calendar_data = []\n                    for day in days[1:]:  # Skip first empty split\n                        day_content = day.strip()\n                        if day_content:\n                            day_num = day_content[0]\n                            content = day_content[1:].strip()\n                            calendar_data.append({\"Day\": int(day_num), \"Content\": content})\n                    \n                    if calendar_data:\n                        st.subheader(\"Calendar View\")\n                        calendar_df = pd.DataFrame(calendar_data)\n                        st.dataframe(calendar_df)\n            except:\n                # If parsing fails, just show raw result\n                pass\n            \n        else:\n            progress_bar.progress(100)\n            status_container.error(f\"Error: {result}\")\n            timer_container.empty()\n\nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "agents/sales/email_preview.py", "content": "import streamlit as st\nimport pandas as pd\nfrom typing import Dict, List\nimport json\nimport os\nfrom dotenv import load_dotenv\nimport smtplib\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\nfrom crewai import Agent, Task, Crew, Process\nfrom crewai.tools import BaseTool\nfrom crewai_tools import SerperDevTool, ScrapeWebsiteTool\nimport requests\nfrom bs4 import BeautifulSoup\nfrom langchain_community.llms import Ollama\nfrom langchain_openai import ChatOpenAI\nfrom textwrap import dedent\nfrom pydantic import BaseModel, Field\nfrom typing import Type, ClassVar\n\n# Load environment variables\nload_dotenv()\n\n# Initialize SerperDev tool\nsearch_tool = SerperDevTool()\n\n# Initialize ScrapeWebsite tool\nscrape_tool = ScrapeWebsiteTool()\n\n# Page config\nst.set_page_config(\n    page_title=\"Email Preview & Send\",\n    page_icon=\"üìß\",\n    layout=\"wide\"\n)\n\n# Initialize session state\nif 'email_data' not in st.session_state:\n    st.session_state.email_data = None\nif 'research_data' not in st.session_state:\n    st.session_state.research_data = None\nif 'news_data' not in st.session_state:\n    st.session_state.news_data = None\nif 'website_data' not in st.session_state:\n    st.session_state.website_data = None\nif 'scraped_data' not in st.session_state:\n    st.session_state.scraped_data = None\nif 'draft_subject' not in st.session_state:\n    st.session_state.draft_subject = \"\"\nif 'draft_body' not in st.session_state:\n    st.session_state.draft_body = \"\"\nif 'draft_cc' not in st.session_state:\n    st.session_state.draft_cc = \"yash@explainx.ai\"\nif 'draft_reply_to' not in st.session_state:\n    st.session_state.draft_reply_to = \"yash@explainx.ai\"\nif 'email_sent' not in st.session_state:\n    st.session_state.email_sent = False\n\ndef get_llm(use_gpt=True):\n    \"\"\"Get the specified language model\"\"\"\n    if use_gpt:\n        return ChatOpenAI(\n            model_name=\"gpt-4o-mini\",\n            temperature=0.7\n        )\n    return Ollama(\n        model=\"deepseek-r1:latest\",\n        base_url=\"http://localhost:11434\",\n        temperature=0.7\n    )\n\ndef validate_email(email: str) -> bool:\n    \"\"\"Simple email validation\"\"\"\n    return '@' in email and '.' in email.split('@')[1]\n\ndef load_email_template(industry: str) -> str:\n    \"\"\"Load email template for given industry\"\"\"\n    try:\n        with open(f'email_templates/{industry.lower()}.txt', 'r') as f:\n            return f.read()\n    except FileNotFoundError:\n        return \"\"\n\ndef send_email(to: str, subject: str, body: str, cc: str = None, reply_to: str = None) -> Dict:\n    \"\"\"Send email using Gmail SMTP\"\"\"\n    smtp_settings = {\n        'server': \"smtp.gmail.com\",\n        'port': 587,\n        'username': os.getenv('GMAIL_USER'),\n        'password': os.getenv('GMAIL_APP_PASSWORD')\n    }\n    \n    if not smtp_settings['username'] or not smtp_settings['password']:\n        return {\"status\": \"error\", \"message\": \"GMAIL_USER and GMAIL_APP_PASSWORD environment variables are required\"}\n    \n    try:\n        msg = MIMEMultipart()\n        msg['From'] = smtp_settings['username']\n        msg['To'] = to\n        msg['Subject'] = subject\n        \n        # Add CC if provided\n        if cc:\n            msg['Cc'] = cc\n        \n        # Add Reply-To if provided\n        if reply_to:\n            msg['Reply-To'] = reply_to\n            \n        msg.attach(MIMEText(body, 'plain'))\n        \n        with smtplib.SMTP(smtp_settings['server'], smtp_settings['port']) as server:\n            server.starttls()\n            server.login(smtp_settings['username'], smtp_settings['password'])\n            \n            # Include CC recipients in the send_message recipients list if provided\n            recipients = [to]\n            if cc:\n                recipients.append(cc)\n                \n            server.send_message(msg, to_addrs=recipients)\n        \n        return {\n            \"status\": \"success\",\n            \"message\": f\"Email sent successfully to {to}\" + (f\" with CC to {cc}\" if cc else \"\"),\n            \"to\": to,\n            \"cc\": cc,\n            \"reply_to\": reply_to,\n            \"subject\": subject,\n            \"body\": body\n        }\n    except Exception as e:\n        return {\n            \"status\": \"error\",\n            \"message\": f\"Error sending email: {str(e)}\",\n            \"to\": to,\n            \"cc\": cc,\n            \"reply_to\": reply_to,\n            \"subject\": subject,\n            \"body\": body\n        }\n\nclass EmailDrafter:\n    def __init__(self, email: str, industry: str, use_gpt: bool = True):\n        self.email = email\n        self.industry = industry\n        self.llm = get_llm(use_gpt)\n        self.domain = email.split('@')[1]\n        self.company_name = self.domain.split('.')[0]\n        \n    def research_company(self):\n        \"\"\"Research the company and return findings\"\"\"\n        # First, try to get direct website data\n        website_url = f\"www.{self.domain}\"\n        website_data = extract_website_data(website_url)\n        \n        # Use the ScrapeWebsiteTool as well for more structured data\n        try:\n            scraped_data = scrape_tool._run(website_url)\n        except Exception as e:\n            scraped_data = f\"Error using ScrapeWebsiteTool: {str(e)}\"\n        \n        # Combine the data\n        direct_website_data = f\"\"\"\n        DIRECT WEBSITE SCRAPING RESULTS:\n        {website_data}\n        \n        SCRAPE WEBSITE TOOL RESULTS:\n        {scraped_data}\n        \"\"\"\n        \n        # Create research agent with both scraping and search tools\n        researcher = Agent(\n            role='Company Research Specialist',\n            goal='Analyze company and gather comprehensive information',\n            backstory=dedent(f\"\"\"You are an expert researcher specializing in \n                {self.industry} company analysis. You excel at finding detailed information \n                about companies, their products, and market presence. \n                Use the website data provided first, and supplement with search results.\"\"\"),\n            tools=[search_tool, scrape_tool],\n            verbose=True,\n            llm=self.llm,\n            max_iter=100,\n            allow_delegation=False\n        )\n        \n        research_task = Task(\n            description=dedent(f\"\"\"Research {self.company_name} ({self.domain}) thoroughly.\n                Consider their position in the {self.industry} industry.\n                \n                First, analyze this direct data from their website:\n                {direct_website_data}\n                \n                If the website data is insufficient, use your tools to search for more information.\n                \n                Step-by-step approach:\n                1. Analyze the website data provided\n                2. Search for additional company overview and background if needed\n                3. Research their products/services in detail\n                4. Find information about their team and leadership\n                5. Analyze their market position\n                6. Identify their tech stack and tools\n                \n                Focus on:\n                - Company's main products/services\n                - Value proposition\n                - Target market\n                - Team information\n                - Recent updates or changes\n                - Technology stack or tools mentioned\n                \n                Create a comprehensive profile of the company.\"\"\"),\n            agent=researcher,\n            expected_output=dedent(\"\"\"Detailed company profile including all \n                discovered information in a structured format.\"\"\")\n        )\n        \n        # Create and run crew\n        crew = Crew(\n            agents=[researcher],\n            tasks=[research_task],\n            process=Process.sequential,\n            verbose=True,\n            max_rpm=100\n        )\n        \n        result = crew.kickoff()\n        return result\n    \n    def analyze_news(self):\n        \"\"\"Research news and trends about the company\"\"\"\n        # First, try to extract news from the company website\n        website_url = f\"www.{self.domain}\"\n        \n        # Try to find common news/blog paths\n        news_paths = ['/news', '/blog', '/press', '/media', '/updates', '/articles']\n        news_data = \"\"\n        \n        for path in news_paths:\n            try:\n                news_url = f\"https://{website_url}{path}\"\n                headers = {\n                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n                }\n                response = requests.get(news_url, headers=headers, timeout=5)\n                if response.status_code == 200:\n                    soup = BeautifulSoup(response.text, 'html.parser')\n                    \n                    # Look for article titles and content\n                    articles = []\n                    \n                    # Method 1: Look for article elements\n                    for article in soup.find_all(['article', 'div'], class_=lambda c: c and any(x in c for x in ['post', 'article', 'news', 'blog'])):\n                        title_elem = article.find(['h1', 'h2', 'h3'])\n                        title = title_elem.get_text().strip() if title_elem else \"Untitled\"\n                        \n                        date_elem = article.find(['time', 'span', 'div'], class_=lambda c: c and any(x in c for x in ['date', 'time', 'published']))\n                        date = date_elem.get_text().strip() if date_elem else \"\"\n                        \n                        snippet_elem = article.find(['p', 'div'], class_=lambda c: c and any(x in c for x in ['excerpt', 'summary', 'content']))\n                        snippet = snippet_elem.get_text().strip() if snippet_elem else \"\"\n                        \n                        if title != \"Untitled\" or snippet:\n                            articles.append(f\"Title: {title}\\nDate: {date}\\nExcerpt: {snippet}\\n\")\n                    \n                    # Method 2: If no articles found, look for links that might be news items\n                    if not articles:\n                        for link in soup.find_all('a', href=True):\n                            if link.get_text().strip():\n                                articles.append(f\"Link: {link.get_text().strip()}\\n\")\n                    \n                    # Take only first 10 articles\n                    articles = articles[:10]\n                    if articles:\n                        news_data += f\"\\nNEWS FROM {news_url}:\\n\" + \"\\n\".join(articles) + \"\\n\"\n            except:\n                continue\n        \n        # Create news analyst agent with both website news and search capability\n        news_analyst = Agent(\n            role='News and Trends Analyst',\n            goal='Find and analyze relevant news and industry trends',\n            backstory=dedent(f\"\"\"You are skilled at identifying relevant news \n                and understanding {self.industry} industry trends. You can connect company \n                activities to broader market movements. Use website news data if available,\n                then supplement with search results.\"\"\"),\n            tools=[search_tool, scrape_tool],\n            verbose=True,\n            llm=self.llm,\n            max_iter=75,\n            allow_delegation=False\n        )\n        \n        news_task = Task(\n            description=dedent(f\"\"\"Research recent news and developments about \n                {self.company_name} and their position in the {self.industry} industry.\n                \n                First, analyze any news data from their website:\n                {news_data}\n                \n                Then use your tools to search for additional news and trends.\n                \n                Step-by-step approach:\n                1. Analyze any website news data provided\n                2. Search for additional company news from the last 3 months\n                3. Research {self.industry} industry trends affecting them\n                4. Analyze competitor movements\n                5. Identify market opportunities\n                6. Find any company milestones or achievements\n                \n                Focus on:\n                - Recent company news and press releases\n                - Industry trends and developments\n                - Competitive landscape\n                - Market opportunities and challenges\n                - Recent achievements or notable events\"\"\"),\n            agent=news_analyst,\n            expected_output=dedent(\"\"\"Comprehensive news analysis including \n                company-specific news and relevant industry trends.\"\"\")\n        )\n        \n        # Create and run crew\n        crew = Crew(\n            agents=[news_analyst],\n            tasks=[news_task],\n            process=Process.sequential,\n            verbose=True,\n            max_rpm=100\n        )\n        \n        result = crew.kickoff()\n        return result\n    \n    def draft_email(self, research_data: str, news_data: str):\n        \"\"\"Draft email based on research and news\"\"\"\n        writer = Agent(\n            role='Outreach Content Specialist',\n            goal='Create highly personalized email content',\n            backstory=dedent(f\"\"\"You are an expert at crafting personalized \n                outreach emails for {self.industry} companies that resonate with recipients. \n                You excel at combining company research with industry insights.\n                You are founder of explainx.ai and your name is Yash Thakker, which \n                is what should be mentioned in the email. Be sure to mention that this email \n                was generated by an AI agent, as a demonstration of your company's AI capabilities.\"\"\"),\n            verbose=True,\n            llm=self.llm\n        )\n        \n        # Load template if available\n        template = load_email_template(self.industry)\n        template_context = f\"Use this template as inspiration if available: {template}\" if template else \"\"\n        \n        email_task = Task(\n            description=dedent(f\"\"\"Draft a personalized email to {self.email} using \n                the research and news analysis provided. Consider their position in the \n                {self.industry} industry.\n                \n                The research data is: {research_data}\n                \n                The news analysis is: {news_data}\n                \n                Step-by-step approach:\n                1. Extract key insights from research\n                2. Identify compelling news points\n                3. Craft attention-grabbing subject\n                4. Write personalized introduction\n                5. Present value proposition\n                \n                Guidelines:\n                - Subject line must be a short question with a wave emoji (üëã), e.g., \"üëã Need AI Agents at {self.company_name}?\"\n                - Subject should be crisp and direct\n                - Keep tone crisp, minimal and professional throughout\n                - Be concise - avoid unnecessary words and phrases\n                - Get straight to the point quickly\n                - Reference specific company details from research\n                - Focus on how our AI agents can improve business efficiency for {self.company_name}\n                - DO NOT suggest replacing or competing with their existing tools\n                - Emphasize complementing and enhancing their current processes\n                - Mention relevant {self.industry} trends\n                - Focus on value proposition around time/cost savings and business optimization\n                - Keep email concise (100-150 words maximum)\n                - Include clear call to action\n                - Sign as \"Jane, AI Agent for Outreach\"\n                \n                IMPORTANT: Your email MUST include the following information:\n                1. Start with \"I'm Jane, Outreach AI Agent @ ExplainX.ai\" before getting into the main content\n                2. Clearly state that \"This email was drafted by an AI agent\" as a demonstration of our technology\n                3. Mention that we offer AI agents for business efficiency, including:\n                   - Sales automation\n                   - Marketing content creation\n                   - Customer outreach\n                   - Lead qualification\n                4. Include 1-2 specific examples of how our AI agents could improve efficiency in sales or marketing for {self.company_name} based on your research\n                5. Include a call to action inviting them to reach out if they're interested in implementing AI solutions to enhance their business operations\n                6. Do NOT mention any specific individual by name in the email body or signature\n                7. Sign simply as \"Jane, AI Agent for Outreach\"\n                \n                {template_context}\n                \n                IMPORTANT: Return your response in valid JSON format with the following structure:\n                {{\n                    \"subject\": \"Your email subject (5-7 words with emoji)\",\n                    \"body\": \"Your email body\",\n                    \"cc\": \"yash@explainx.ai\",\n                    \"reply_to\": \"yash@explainx.ai\"\n                }}\"\"\"),\n            agent=writer,\n            expected_output=dedent(\"\"\"JSON containing subject and body for email.\"\"\")\n        )\n        \n        # Create and run crew\n        crew = Crew(\n            agents=[writer],\n            tasks=[email_task],\n            process=Process.sequential,\n            verbose=True,\n            max_rpm=100\n        )\n        \n        result = crew.kickoff()\n        \n        # Handle CrewOutput object - get the string content from it\n        result_str = str(result)\n        \n        # Try to parse JSON from the result string\n        try:\n            # Look for JSON in the result string\n            import re\n            json_match = re.search(r'\\{.*\\}', result_str, re.DOTALL)\n            \n            if json_match:\n                json_str = json_match.group(0)\n                email_content = json.loads(json_str)\n                return email_content\n            \n            # If no JSON pattern found, do simple parsing\n            lines = result_str.strip().split('\\n')\n            subject = \"\"\n            body = \"\"\n            \n            for i, line in enumerate(lines):\n                if \"subject\" in line.lower() and \":\" in line:\n                    subject = line.split(\":\", 1)[1].strip().strip('\"').strip()\n                    body_start = i + 1\n                    # Look for the body after finding subject\n                    for j in range(body_start, len(lines)):\n                        if \"body\" in lines[j].lower() and \":\" in lines[j]:\n                            body = \"\\n\".join(lines[j+1:]).strip()\n                            break\n                    if not body:  # If we didn't find a body label, just use everything after subject\n                        body = \"\\n\".join(lines[body_start:]).strip()\n                    break\n            \n            # If we found at least a subject, return that\n            if subject:\n                return {\n                    \"subject\": subject,\n                    \"body\": body\n                }\n                \n            # Fallback to manually creating email content\n            return {\n                \"subject\": f\"üëã AI efficiency for {self.company_name}?\",\n                \"body\": f\"I'm Jane, Outreach AI Agent @ ExplainX.ai\\n\\nI'm reaching out about {self.company_name} in the {self.industry} space.\\n\\n**This email was drafted by an AI agent.**\\n\\nWe build AI agents to enhance business efficiency:\\n- Sales automation\\n- Marketing content creation\\n- Customer outreach\\n- Lead qualification\\n\\nOur AI solutions could streamline your sales and marketing operations while complementing your existing tools.\\n\\nInterested in boosting efficiency? Reply for more details.\\n\\nJane\\nAI Agent for Outreach\",\n                \"cc\": \"yash@explainx.ai\",\n                \"reply_to\": \"yash@explainx.ai\"\n            }\n            \n        except Exception as e:\n            print(f\"Error parsing result: {str(e)}\")\n            # Fallback email content\n            return {\n                \"subject\": f\"üëã AI efficiency for {self.company_name}?\",\n                \"body\": f\"I'm Jane, Outreach AI Agent @ ExplainX.ai\\n\\nI'm reaching out about {self.company_name} in the {self.industry} space.\\n\\n**This email was drafted by an AI agent.**\\n\\nWe build AI agents to enhance business efficiency:\\n- Sales automation\\n- Marketing content creation\\n- Customer outreach\\n- Lead qualification\\n\\nOur AI solutions could streamline your sales and marketing operations while complementing your existing tools.\\n\\nInterested in boosting efficiency? Reply for more details.\\n\\nJane\\nAI Agent for Outreach\",\n                \"cc\": \"yash@explainx.ai\",\n                \"reply_to\": \"yash@explainx.ai\"\n            }\n\ndef extract_website_data(url: str) -> str:\n    \"\"\"Extract data from a website using BeautifulSoup\"\"\"\n    if not url.startswith('http'):\n        url = f\"https://{url}\"\n    \n    try:\n        headers = {\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n        }\n        response = requests.get(url, headers=headers, timeout=10)\n        response.raise_for_status()\n        \n        soup = BeautifulSoup(response.text, 'html.parser')\n        \n        # Extract title\n        title = soup.title.string if soup.title else \"\"\n        \n        # Extract meta description\n        meta_desc = \"\"\n        meta_tag = soup.find('meta', attrs={'name': 'description'})\n        if meta_tag and 'content' in meta_tag.attrs:\n            meta_desc = meta_tag['content']\n        \n        # Extract heading text\n        headings = [h.get_text() for h in soup.find_all(['h1', 'h2', 'h3']) if h.get_text()]\n        heading_text = \"\\n\".join(headings[:10])  # Limit to top 10 headings\n        \n        # Extract paragraph text\n        paragraphs = [p.get_text().strip() for p in soup.find_all('p') if p.get_text().strip()]\n        paragraph_text = \"\\n\".join(paragraphs[:15])  # Limit to top 15 paragraphs\n        \n        # Check for about page link\n        about_links = soup.find_all('a', href=True, text=lambda t: t and 'about' in t.lower())\n        about_urls = [link['href'] for link in about_links]\n        \n        # Try to extract about page content if found\n        about_content = \"\"\n        for about_url in about_urls[:1]:  # Just try the first about link\n            if about_url.startswith('/'):\n                about_url = url.rstrip('/') + about_url\n            elif not about_url.startswith('http'):\n                about_url = f\"{url.rstrip('/')}/{about_url.lstrip('/')}\"\n                \n            try:\n                about_response = requests.get(about_url, headers=headers, timeout=10)\n                about_response.raise_for_status()\n                about_soup = BeautifulSoup(about_response.text, 'html.parser')\n                about_paragraphs = [p.get_text().strip() for p in about_soup.find_all('p') if p.get_text().strip()]\n                about_content = \"\\n\".join(about_paragraphs[:10])  # Limit to top 10 paragraphs\n            except:\n                pass\n        \n        # Compile all the data\n        website_data = f\"\"\"\n        WEBSITE DATA FOR {url}\n        \n        TITLE: {title}\n        \n        META DESCRIPTION: {meta_desc}\n        \n        MAIN HEADINGS:\n        {heading_text}\n        \n        MAIN CONTENT:\n        {paragraph_text}\n        \n        ABOUT PAGE CONTENT:\n        {about_content}\n        \"\"\"\n        \n        return website_data.strip()\n    \n    except Exception as e:\n        return f\"Error extracting data from {url}: {str(e)}\"\n\n# Sidebar with settings\nwith st.sidebar:\n    st.title(\"‚öôÔ∏è Settings\")\n    \n    # Model selection\n    model_option = st.radio(\n        \"Select AI Model\",\n        [\"OpenAI GPT-4\", \"Local DeepSeek Coder\"],\n        help=\"Choose between OpenAI's GPT-4 or local DeepSeek Coder model\"\n    )\n    \n    # API Keys\n    with st.expander(\"API Configuration\"):\n        openai_key = st.text_input(\"OpenAI API Key\", type=\"password\")\n        serper_key = st.text_input(\"Serper API Key\", type=\"password\")\n        gmail_user = st.text_input(\"Gmail User\", type=\"password\", help=\"Your Gmail address\")\n        gmail_password = st.text_input(\"Gmail App Password\", type=\"password\", help=\"Gmail App Password (NOT your regular password)\")\n        \n        # Save credentials to environment\n        if openai_key:\n            os.environ[\"OPENAI_API_KEY\"] = openai_key\n        if serper_key:\n            os.environ[\"SERPER_API_KEY\"] = serper_key\n        if gmail_user:\n            os.environ[\"GMAIL_USER\"] = gmail_user\n        if gmail_password:\n            os.environ[\"GMAIL_APP_PASSWORD\"] = gmail_password\n\n# Main content\nst.title(\"üìß Email Preview & Send\")\nst.caption(\"Research, draft, preview and send personalized emails\")\nst.title(f\"Email will be sent via {os.getenv('GMAIL_USER')}\")\n\n# Input form\nwith st.form(key=\"email_form\"):\n    col1, col2 = st.columns(2)\n    \n    with col1:\n        recipient_email = st.text_input(\"Recipient Email\", help=\"Enter the prospect's email address\")\n    \n    with col2:\n        industry = st.selectbox(\n            \"Industry\",\n            [\"Technology\", \"Finance\", \"Healthcare\", \"Education\", \"Other\"],\n            help=\"Select the prospect's industry\"\n        )\n    \n    submitted = st.form_submit_button(\"Research & Draft Email\")\n    \n    if submitted:\n        if not validate_email(recipient_email):\n            st.error(\"Please enter a valid email address!\")\n        elif not os.getenv(\"SERPER_API_KEY\"):\n            st.error(\"Please configure Serper API key in the settings!\")\n        elif model_option == \"OpenAI GPT-4\" and not os.getenv(\"OPENAI_API_KEY\"):\n            st.error(\"Please configure OpenAI API key in the settings!\")\n        else:\n            # Extract domain for website scraping\n            domain = recipient_email.split('@')[1]\n            website_url = f\"www.{domain}\"\n            \n            # Create email drafter\n            drafter = EmailDrafter(\n                recipient_email, \n                industry,\n                use_gpt=(model_option == \"OpenAI GPT-4\")\n            )\n            \n            # Check if website is accessible\n            try:\n                with st.spinner(f\"Checking website accessibility: {website_url}\"):\n                    headers = {\n                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n                    }\n                    response = requests.head(f\"https://{website_url}\", headers=headers, timeout=5)\n                    if response.status_code < 400:\n                        st.success(f\"‚úÖ Website {website_url} is accessible - will extract data directly\")\n                        \n                        # Extract website data and save to session state\n                        website_data = extract_website_data(website_url)\n                        st.session_state.website_data = website_data\n                        \n                        try:\n                            scraped_data = scrape_tool._run(website_url)\n                            st.session_state.scraped_data = scraped_data\n                        except Exception as e:\n                            st.session_state.scraped_data = f\"Error using ScrapeWebsiteTool: {str(e)}\"\n                    else:\n                        st.warning(f\"‚ö†Ô∏è Website {website_url} returned status code {response.status_code} - will rely more on search\")\n            except Exception as e:\n                st.warning(f\"‚ö†Ô∏è Could not access {website_url}: {str(e)} - will rely on search\")\n            \n            # Research phase\n            with st.spinner(\"Researching company...\"):\n                research_data = drafter.research_company()\n                st.session_state.research_data = research_data\n            \n            # News analysis phase\n            with st.spinner(\"Analyzing news and trends...\"):\n                news_data = drafter.analyze_news()\n                st.session_state.news_data = news_data\n            \n            # Draft email\n            with st.spinner(\"Drafting personalized email...\"):\n                email_content = drafter.draft_email(research_data, news_data)\n                st.session_state.draft_subject = email_content.get(\"subject\", \"\")\n                st.session_state.draft_body = email_content.get(\"body\", \"\")\n                st.session_state.draft_cc = email_content.get(\"cc\", \"yash@explainx.ai\")\n                st.session_state.draft_reply_to = email_content.get(\"reply_to\", \"yash@explainx.ai\")\n                st.session_state.email_data = {\n                    \"to\": recipient_email,\n                    \"industry\": industry\n                }\n            \n            st.success(\"Email drafted successfully! Preview it below.\")\n\n# Display preview if email is drafted\nif st.session_state.email_data and st.session_state.draft_subject and st.session_state.draft_body:\n    st.header(\"Email Preview\")\n    \n    # Research and News tabs for reference\n    tab1, tab2, tab3, tab4 = st.tabs([\"Email Draft\", \"Company Research\", \"News Analysis\", \"Website Data\"])\n    \n    with tab1:\n        # Editable email content\n        to_email = st.session_state.email_data[\"to\"]\n        edited_subject = st.text_input(\"Subject\", value=st.session_state.draft_subject)\n        edited_body = st.text_area(\"Body\", value=st.session_state.draft_body, height=300)\n        \n        # Add fields for CC and Reply-To\n        cc_email = st.text_input(\"CC\", value=st.session_state.draft_cc)\n        reply_to_email = st.text_input(\"Reply-To\", value=st.session_state.draft_reply_to)\n        \n        col1, col2, col3 = st.columns(3)\n        \n        with col1:\n            if st.button(\"Send Email\"):\n                if not os.getenv(\"GMAIL_USER\") or not os.getenv(\"GMAIL_APP_PASSWORD\"):\n                    st.error(\"Please configure Gmail credentials in the settings!\")\n                else:\n                    with st.spinner(\"Sending email...\"):\n                        result = send_email(\n                            st.session_state.email_data[\"to\"],\n                            edited_subject,\n                            edited_body,\n                            cc=cc_email,\n                            reply_to=reply_to_email\n                        )\n                        \n                        if result[\"status\"] == \"success\":\n                            st.success(result[\"message\"])\n                            st.session_state.email_sent = True\n                        else:\n                            st.error(result[\"message\"])\n        \n        with col2:\n            if st.button(\"Regenerate Email\"):\n                if st.session_state.research_data and st.session_state.news_data:\n                    with st.spinner(\"Regenerating email...\"):\n                        # Create new drafter\n                        drafter = EmailDrafter(\n                            st.session_state.email_data[\"to\"],\n                            st.session_state.email_data[\"industry\"],\n                            use_gpt=(model_option == \"OpenAI GPT-4\")\n                        )\n                        \n                        # Draft new email with existing research data\n                        email_content = drafter.draft_email(\n                            st.session_state.research_data, \n                            st.session_state.news_data\n                        )\n                        \n                        # Update session state\n                        st.session_state.draft_subject = email_content.get(\"subject\", \"\")\n                        st.session_state.draft_body = email_content.get(\"body\", \"\")\n                        st.session_state.draft_cc = email_content.get(\"cc\", \"yash@explainx.ai\")\n                        st.session_state.draft_reply_to = email_content.get(\"reply_to\", \"yash@explainx.ai\")\n                        \n                        # Rerun to update UI\n                        st.rerun()\n        \n        with col3:\n            if st.button(\"Reset\"):\n                st.session_state.email_data = None\n                st.session_state.research_data = None\n                st.session_state.news_data = None\n                st.session_state.website_data = None\n                st.session_state.scraped_data = None\n                st.session_state.draft_subject = \"\"\n                st.session_state.draft_body = \"\"\n                st.session_state.email_sent = False\n                st.rerun()\n    \n    with tab2:\n        if st.session_state.research_data:\n            st.markdown(\"### Company Research\")\n            st.markdown(st.session_state.research_data)\n    \n    with tab3:\n        if st.session_state.news_data:\n            st.markdown(\"### News Analysis\")\n            st.markdown(st.session_state.news_data)\n    \n    with tab4:\n        st.markdown(\"### Website Data\")\n        domain = st.session_state.email_data[\"to\"].split('@')[1]\n        website_url = f\"www.{domain}\"\n        \n        if st.button(\"Refresh Website Data\"):\n            with st.spinner(f\"Extracting data from {website_url}...\"):\n                website_data = extract_website_data(website_url)\n                st.session_state.website_data = website_data\n                \n                try:\n                    scraped_data = scrape_tool._run(website_url)\n                    st.session_state.scraped_data = scraped_data\n                except Exception as e:\n                    st.session_state.scraped_data = f\"Error using ScrapeWebsiteTool: {str(e)}\"\n        \n        # Direct BS4 extraction\n        if 'website_data' in st.session_state:\n            with st.expander(\"BS4 Extraction Results\", expanded=True):\n                st.text(st.session_state.website_data)\n        \n        # ScrapeWebsiteTool results\n        if 'scraped_data' in st.session_state:\n            with st.expander(\"ScrapeWebsiteTool Results\", expanded=True):\n                st.text(st.session_state.scraped_data)\n            \nelse:\n    st.info(\"Enter a valid email address and click 'Research & Draft Email' to get started.\")\n\n# Footer\nst.markdown(\"---\")\nst.markdown(\"Made with ‚ù§Ô∏è by @goyashy\")\n\n# Help section\nwith st.sidebar:\n    with st.expander(\"‚ÑπÔ∏è Help & Documentation\"):\n        st.markdown(\"\"\"\n        ### How to use this tool\n        1. Configure your API keys in Settings\n        2. Enter a prospect's email address\n        3. Select their industry\n        4. Click 'Research & Draft Email'\n        5. Review the research and draft\n        6. Edit the email if needed\n        7. Click 'Send Email' when ready\n        \n        ### Requirements\n        - Gmail account with App Password\n        - Serper API key for research\n        - OpenAI API key (if using GPT-4)\n        \n        ### Need Help?\n        Contact @goyashy for support\n        \"\"\") "}
{"type": "source_file", "path": "agents/sales/main.py", "content": "from textwrap import dedent\nfrom crewai import Agent, Task, Crew, Process\nfrom crewai.tools import BaseTool\nfrom crewai_tools import SerperDevTool\nfrom pydantic import BaseModel, Field\nfrom typing import List, Dict, Type, ClassVar\nimport requests\nimport smtplib\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\nfrom langchain_community.llms import Ollama\nfrom langchain_openai import ChatOpenAI\nfrom dotenv import load_dotenv\nimport os\nimport json\n\n# Load environment variables\nload_dotenv()\nos.environ[\"SERPER_API_KEY\"] = os.getenv(\"SERPER_API_KEY\")\n\n# Initialize SerperDev tool\nsearch_tool = SerperDevTool()\n\ndef get_llm(use_gpt=False):\n    \"\"\"Get the specified language model\"\"\"\n    if use_gpt:\n        return ChatOpenAI(\n            model_name=\"gpt-4o-mini\",\n            temperature=0.7\n        )\n    return Ollama(\n        model=\"deepseek-r1:latest\",\n        base_url=\"http://localhost:11434\",\n        temperature=0.7\n    )\n\nclass EmailInput(BaseModel):\n    \"\"\"Input schema for Email Tool\"\"\"\n    to: str = Field(..., description=\"Recipient email address\")\n    subject: str = Field(..., description=\"Email subject line\")\n    body: str = Field(..., description=\"Email body content\")\n\nclass EmailSender(BaseTool):\n    name: str = \"Email Sender\"\n    description: str = \"Sends personalized emails using Gmail SMTP\"\n    args_schema: Type[BaseModel] = EmailInput\n    \n    smtp_settings: ClassVar[Dict[str, str | int]] = {\n        'server': \"smtp.gmail.com\",\n        'port': 587,\n        'username': os.getenv('GMAIL_USER'),\n        'password': os.getenv('GMAIL_APP_PASSWORD')\n    }\n\n    def _run(self, to: str, subject: str, body: str) -> str:\n        if not self.smtp_settings['username'] or not self.smtp_settings['password']:\n            return json.dumps({\"error\": \"GMAIL_USER and GMAIL_APP_PASSWORD environment variables are required\"})\n        \n        try:\n            msg = MIMEMultipart()\n            msg['From'] = self.smtp_settings['username']\n            msg['To'] = to\n            msg['Subject'] = subject\n            msg.attach(MIMEText(body, 'plain'))\n            \n            with smtplib.SMTP(self.smtp_settings['server'], self.smtp_settings['port']) as server:\n                server.starttls()\n                server.login(self.smtp_settings['username'], self.smtp_settings['password'])\n                server.send_message(msg)\n            \n            return json.dumps({\"status\": \"success\", \"message\": f\"Email sent successfully to {to}\"})\n        except Exception as e:\n            return json.dumps({\"error\": f\"Error sending email: {str(e)}\"})\n\nclass DetailedSalesCrew:\n    def __init__(self, target_emails: List[str], use_gpt: bool = False):\n        self.target_emails = target_emails\n        self.llm = get_llm(use_gpt)\n        self.email_tool = EmailSender()\n        \n    def create_agents(self):\n        # Research Agent\n        self.researcher = Agent(\n            role='Company Research Specialist',\n            goal='Analyze companies and gather comprehensive information',\n            backstory=dedent(\"\"\"You are an expert researcher specializing in \n                company analysis. You excel at finding detailed information \n                about companies, their products, and market presence.\"\"\"),\n            tools=[search_tool],\n            verbose=True,\n            llm=self.llm,\n            max_iter=100,\n            allow_delegation=False,\n            max_rpm=50,\n            max_retry_limit=3\n        )\n        \n        # News Agent\n        self.news_analyst = Agent(\n            role='News and Trends Analyst',\n            goal='Find and analyze relevant news and industry trends',\n            backstory=dedent(\"\"\"You are skilled at identifying relevant news \n                and understanding industry trends. You can connect company \n                activities to broader market movements.\"\"\"),\n            tools=[search_tool],\n            verbose=True,\n            llm=self.llm,\n            max_iter=75,\n            allow_delegation=False,\n            max_rpm=30,\n            max_retry_limit=2\n        )\n        \n        # Content Writer\n        self.writer = Agent(\n            role='Outreach Content Specialist',\n            goal='Create highly personalized email content',\n            backstory=dedent(\"\"\"You are an expert at crafting personalized \n                outreach emails that resonate with recipients. You excel at \n                combining company research with industry insights. You are founder of explainx.ai and your name is Yash Thakker, which is what should be mentioned in the email.\"\"\"),\n            tools=[self.email_tool],\n            verbose=True,\n            llm=self.llm,\n            max_iter=50,\n            allow_delegation=False,\n            max_rpm=20,\n            max_retry_limit=2\n        )\n        \n        return [self.researcher, self.news_analyst, self.writer]\n    \n    def create_tasks(self, email: str):\n        # Extract domain from email\n        domain = email.split('@')[1]\n        company_name = domain.split('.')[0]\n        \n        # Research Task\n        research_task = Task(\n            description=dedent(f\"\"\"Research {company_name} ({domain}) thoroughly.\n                Step-by-step approach:\n                1. Search for company overview and background\n                2. Research their products/services in detail\n                3. Find information about their team and leadership\n                4. Analyze their market position\n                5. Identify their tech stack and tools\n                \n                Focus on:\n                - Company's main products/services\n                - Value proposition\n                - Target market\n                - Team information\n                - Recent updates or changes\n                - Technology stack or tools mentioned\n                \n                Create a comprehensive profile of the company.\"\"\"),\n            agent=self.researcher,\n            expected_output=dedent(\"\"\"Detailed company profile including all \n                discovered information in a structured format.\"\"\")\n        )\n        \n        # News Analysis Task\n        news_task = Task(\n            description=dedent(f\"\"\"Research recent news and developments about \n                {company_name} and their industry.\n                \n                Step-by-step approach:\n                1. Search for company news from the last 3 months\n                2. Research industry trends affecting them\n                3. Analyze competitor movements\n                4. Identify market opportunities\n                5. Find any company milestones or achievements\n                \n                Focus on:\n                - Recent company news and press releases\n                - Industry trends and developments\n                - Competitive landscape\n                - Market opportunities and challenges\n                - Recent achievements or notable events\"\"\"),\n            agent=self.news_analyst,\n            expected_output=dedent(\"\"\"Comprehensive news analysis including \n                company-specific news and relevant industry trends.\"\"\")\n        )\n        \n        # Email Creation Task\n        email_task = Task(\n            description=dedent(f\"\"\"Create a personalized email for {email} using \n                the research and news analysis.\n                \n                Step-by-step approach:\n                1. Extract key insights from research\n                2. Identify compelling news points\n                3. Craft attention-grabbing subject\n                4. Write personalized introduction\n                5. Present value proposition\n                \n                Guidelines:\n                - Keep subject line engaging but professional\n                - Reference specific company details from research\n                - Mention relevant news or trends\n                - Focus on value proposition\n                - Keep email concise (150-200 words)\n                - Include clear call to action\n                \n                Format the response as JSON with 'to', 'subject', and 'body' fields.\"\"\"),\n            agent=self.writer,\n            expected_output=dedent(\"\"\"JSON formatted email content with subject \n                line and body text.\"\"\"),\n            context=[research_task, news_task]\n        )\n        \n        return [research_task, news_task, email_task]\n    \n    def run(self):\n        \"\"\"Process each email and create personalized outreach\"\"\"\n        all_results = []\n        \n        for email in self.target_emails:\n            print(f\"\\nProcessing email: {email}\")\n            \n            # Create crew for this email\n            crew = Crew(\n                agents=self.create_agents(),\n                tasks=self.create_tasks(email),\n                process=Process.sequential,\n                verbose=True,\n                max_rpm=100\n            )\n            \n            # Execute the crew's tasks\n            result = crew.kickoff()\n            all_results.append({\n                \"email\": email,\n                \"result\": result\n            })\n        \n        return all_results\n\ndef main():\n    print(\"\\nüîç Welcome to Sales Outreach Crew!\")\n    print(\"\\nAvailable Models:\")\n    print(\"1. OpenAI GPT-4 Turbo (Requires API key)\")\n    print(\"2. Local DeepSeek Coder (Requires Ollama)\")\n    \n    use_gpt = input(\"\\nUse OpenAI GPT-4? (yes/no): \").lower() == 'yes'\n    \n    if not use_gpt:\n        print(\"\\nUsing Ollama with DeepSeek Coder\")\n        print(\"Ensure Ollama is running: ollama run deepseek-coder:latest\")\n    \n    target_emails = [\n        \"pratham@explainx.ai\"\n    ]\n    \n    try:\n        # Initialize and run the sales crew\n        sales_crew = DetailedSalesCrew(target_emails, use_gpt)\n        results = sales_crew.run()\n        \n        # Print results\n        for result in results:\n            print(f\"\\nResults for {result['email']}:\")\n            print(result['result'])\n            \n    except Exception as e:\n        print(f\"\\n‚ùå Error: {str(e)}\")\n        if use_gpt:\n            print(\"\\nTip: Check your OpenAI API key and SERPER_API_KEY\")\n        else:\n            print(\"\\nTip: Ensure Ollama is running with deepseek-coder:latest\")\n            print(\"Run: ollama run deepseek-coder:latest\")\n            print(\"Also check your SERPER_API_KEY\")\n\nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "agents/social_media/main.py", "content": "import os\nimport time\nfrom dotenv import load_dotenv\nfrom crewai import Agent, Task, Crew\nfrom crewai_tools import SerperDevTool\nfrom langchain_openai import ChatOpenAI\nfrom langchain_community.llms import Ollama\n\nload_dotenv()\n\nos.environ[\"SERPER_API_KEY\"] = os.getenv(\"SERPER_API_KEY\")\n\nsearch_tool = SerperDevTool()\n\ndef create_llm(use_gpt=True):\n    if use_gpt:\n        return ChatOpenAI(model=\"gpt-4o-mini\")\n    else:\n        return Ollama(model=\"llama3.1\")\n    \ndef create_agents(brand_name, llm):\n    researcher = Agent(\n        role=\"Social Media Researcher\",\n        goal=f\"Research and gather information about {brand_name} from various sources\",\n        backstory=\"You are an expert researcher with a knack for finding relevant information quickly.\",\n        verbose=True,\n        allow_delegation=False,\n        tools=[search_tool],\n        llm=llm,\n        max_iter=15  # Increased max iterations\n    )\n\n    social_media_monitor = Agent(\n        role=\"Social Media Monitor\",\n        goal=f\"Monitor social media platforms for mentions of {brand_name}\",\n        backstory=\"You are an experienced social media analyst with keen eyes for trends and mentions.\",\n        verbose=True,\n        allow_delegation=False,\n        tools=[search_tool],\n        llm=llm,\n        max_iter=15  # Increased max iterations\n    )\n\n    sentiment_analyzer = Agent(\n        role=\"Sentiment Analyzer\",\n        goal=f\"Analyze the sentiment of social media mentions about {brand_name}\",\n        backstory=\"You are an expert in natural language processing and sentiment analysis.\",\n        verbose=True,\n        allow_delegation=False,\n        llm=llm,\n        max_iter=15  # Increased max iterations\n    )\n\n    report_generator = Agent(\n        role=\"Report Generator\",\n        goal=f\"Generate comprehensive reports based on the analysis of {brand_name}\",\n        backstory=\"You are a skilled data analyst and report writer, adept at presenting insights clearly.\",\n        verbose=True,\n        allow_delegation=False,\n        llm=llm,\n        max_iter=15  # Increased max iterations\n    )\n\n    return [researcher, social_media_monitor, sentiment_analyzer, report_generator]\n\ndef create_tasks(brand_name, agents):\n    research_task = Task(\n        description=f\"Research {brand_name} and provide a summary of their online presence, key information, and recent activities.\",\n        agent=agents[0],\n        expected_output=\"A structured summary containing: \\n1. Brief overview of {brand_name}\\n2. Key online platforms and follower counts\\n3. Recent notable activities or campaigns\\n4. Main products or services\\n5. Any recent news or controversies\"\n    )\n\n    monitoring_task = Task(\n        description=f\"Monitor social media platforms for mentions of '{brand_name}' in the last 24 hours. Provide a summary of the mentions.\",\n        agent=agents[1],\n        expected_output=\"A structured report containing: \\n1. Total number of mentions\\n2. Breakdown by platform (e.g., Twitter, Instagram, Facebook)\\n3. Top 5 most engaging posts or mentions\\n4. Any trending hashtags associated with {brand_name}\\n5. Notable influencers or accounts mentioning {brand_name}\"\n    )\n\n    sentiment_analysis_task = Task(\n        description=f\"Analyze the sentiment of the social media mentions about {brand_name}. Categorize them as positive, negative, or neutral.\",\n        agent=agents[2],\n        expected_output=\"A sentiment analysis report containing: \\n1. Overall sentiment distribution (% positive, negative, neutral)\\n2. Key positive themes or comments\\n3. Key negative themes or comments\\n4. Any notable changes in sentiment compared to previous periods\\n5. Suggestions for sentiment improvement if necessary\"\n    )\n\n    report_generation_task = Task(\n        description=f\"Generate a comprehensive report about {brand_name} based on the research, social media mentions, and sentiment analysis. Include key insights and recommendations.\",\n        agent=agents[3],\n        expected_output=\"A comprehensive report structured as follows: \\n1. Executive Summary\\n2. Brand Overview\\n3. Social Media Presence Analysis\\n4. Sentiment Analysis\\n5. Key Insights\\n6. Recommendations for Improvement\\n7. Conclusion\"\n    )\n\n    return [research_task, monitoring_task, sentiment_analysis_task, report_generation_task]\n\ndef run_social_media_monitoring(brand_name, use_gpt=True, max_retries=3):\n    llm = create_llm(use_gpt)\n    agents = create_agents(brand_name, llm)\n    tasks = create_tasks(brand_name, agents)\n    \n    crew = Crew(\n        agents=agents,\n        tasks=tasks,\n        verbose=True\n    )\n\n    for attempt in range(max_retries):\n        try:\n            result = crew.kickoff()\n            return result\n        except Exception as e:\n            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n            if attempt < max_retries - 1:\n                print(\"Retrying...\")\n                time.sleep(5)  # Wait for 5 seconds before retrying\n            else:\n                print(\"Max retries reached. Unable to complete the task.\")\n                return None\n\nif __name__ == \"__main__\":\n    print(\"Welcome to the Social Media Monitoring Crew!\")\n    use_gpt = input(\"Do you want to use GPT? (yes/no): \").lower() == 'yes'\n    brand_name = input(\"Enter the name of the brand or influencer you want to research: \")\n    \n    result = run_social_media_monitoring(brand_name, use_gpt)\n    \n    if result:\n        print(\"\\n\", \"=\"*50, \"\\n\")\n        print(\"Final Report:\")\n        print(result)\n    else:\n        print(\"Failed to generate the report. Please try again later.\")"}
{"type": "source_file", "path": "agents/research/main.py", "content": "import os\nfrom dotenv import load_dotenv\nfrom crewai import Agent, Task, Crew\nfrom crewai_tools import SerperDevTool\nfrom langchain_openai import ChatOpenAI\nfrom langchain_community.llms import Ollama\n\nload_dotenv()\n\nos.environ[\"SERPER_API_KEY\"] = os.getenv(\"SERPER_API_KEY\")\n\nsearch_tool = SerperDevTool()\n\ndef create_research_agent(use_gpt=True):\n    if use_gpt:\n        llm = ChatOpenAI(model=\"o3-mini\")\n    else:\n        llm = Ollama(model=\"llama3.1\") \n\n    return Agent(\n        role='Research Specialist',\n        goal='Conduct thorough research on given topics',\n        backstory='You are an experienced researcher with expertise in finding and synthesizing information from various sources.',\n        verbose=True,\n        allow_delegation=False,\n        tools=[search_tool],\n        llm=llm\n    )\n\ndef create_research_task(agent, topic):\n    return Task(\n        description=f\"Research the following topic and provide a comprehensive summary: {topic}\",\n        agent=agent,\n        expected_output=\"A detailed summary of the research findings, including key points, trends, and insights related to the topic.\"\n    )\n\ndef run_research(topic, use_gpt=True):\n    agent = create_research_agent(use_gpt)\n    task = create_research_task(agent, topic)\n    crew = Crew(agents=[agent], tasks=[task])\n    result = crew.kickoff()\n    return result\n\nif __name__ == \"__main__\":\n    print(\"Welcome to the Research Agent!\")\n    use_gpt = input(\"Do you want to use GPT? (yes/no): \").lower() == 'yes'\n    topic = input(\"Enter the research topic: \")\n    \n    result = run_research(topic, use_gpt)\n    print(\"\\nResearch Result:\")\n    print(result)"}
{"type": "source_file", "path": "dynamic_newsletter.py", "content": "from crewai import Agent, Task, Crew\nfrom crewai_tools import SerperDevTool\nimport os\n\nsearch_tool = SerperDevTool()\n\nos.environ[\"OPENAI_MODEL_NAME\"] = \"gpt-4o-mini\"\n\ndef create_newsletter_crew(topic):\n    researcher = Agent(\n        role='Research Analyst',\n        goal=f'Find the latest and most relevant news about {topic}',\n        backstory=f\"You're an AI with a knack for discovering trending topics in {topic}.\",\n        tools=[search_tool]\n    )\n\n    writer = Agent(\n        role='Content Writer',\n        goal=f'Create engaging newsletter content about {topic} based on research',\n        backstory=f\"You're an AI with a talent for crafting compelling narratives about {topic}.\"\n    )\n\n    editor = Agent(\n        role='Copy Editor',\n        goal=f'Ensure the {topic} newsletter is polished and error-free',\n        backstory=\"You're an AI with an eye for detail and a mastery of language.\"\n    )\n\n    research_task = Task(\n        description=f'Find the top 3 trending topics in {topic} and provide brief summaries',\n        agent=researcher,\n        expected_output=f\"A list of 3 trending {topic} topics with brief summaries for each\"\n    )\n\n    writing_task = Task(\n        description=f'Write a 300-word article on each trending {topic}',\n        agent=writer,\n        expected_output=f\"Three 300-word articles about the trending {topic} topics\"\n    )\n\n    editing_task = Task(\n        description=f'Proofread and polish the {topic} articles, ensuring they flow well together',\n        agent=editor,\n        expected_output=f\"A final, polished newsletter about {topic} trends, ready for distribution\"\n    )\n\n    newsletter_crew = Crew(\n        agents=[researcher, writer, editor],\n        tasks=[research_task, writing_task, editing_task],\n        verbose=True\n    )\n\n    return newsletter_crew\n\ndef main():\n    topic = input(\"Enter the topic for the newsletter: \")\n    crew = create_newsletter_crew(topic)\n    result = crew.kickoff()\n    print(result)\n\nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "flappy_bird.py", "content": "import pygame\nimport random\nimport sys\n\n# Initialize Pygame\npygame.init()\n\n# Screen dimensions\nWIDTH = 400\nHEIGHT = 600\nSCREEN = pygame.display.set_mode((WIDTH, HEIGHT))\npygame.display.set_caption('Flappy Bird Clone')\n\n# Set up fonts\nFONT = pygame.font.SysFont('Arial', 32)\n\n# Define game variables\nGRAVITY = 0.5\nGAME_SPEED = 3\n\n# Colors\nWHITE = (255, 255, 255)\n\n# Load images\ntry:\n    BIRD_IMAGE = pygame.image.load('bird.png').convert_alpha()\n    # Scale down the bird image\n    BIRD_IMAGE = pygame.transform.scale(BIRD_IMAGE, (50, 35))\n    BG_IMAGE = pygame.image.load('bg.png').convert()\n    PIPE_IMAGE = pygame.image.load('pipe.png').convert_alpha()\nexcept pygame.error as e:\n    print(f\"Error loading images: {e}\")\n    pygame.quit()\n    sys.exit()\n\n# Player class representing the character\nclass Player(pygame.sprite.Sprite):\n    def __init__(self):\n        super().__init__()\n        # Use the scaled bird image for the player\n        self.image = BIRD_IMAGE\n        self.rect = self.image.get_rect()\n        self.rect.center = (50, HEIGHT // 2)  # Start position\n        self.velocity = 0  # Initial velocity\n\n    def update(self):\n        # Apply gravity to the player's velocity\n        self.velocity += GRAVITY\n        # Update the player's position\n        self.rect.y += int(self.velocity)\n\n        # Prevent the player from moving off-screen\n        if self.rect.top <= 0:\n            self.rect.top = 0\n            self.velocity = 0\n        if self.rect.bottom >= HEIGHT:\n            self.rect.bottom = HEIGHT\n            self.velocity = 0\n\n    def flap(self):\n        # Move the player upward when the spacebar is pressed\n        self.velocity = -10\n\n# Pipe class for obstacles\nclass Pipe(pygame.sprite.Sprite):\n    def __init__(self, x, y, orientation):\n        super().__init__()\n        # Flip the pipe image for the top pipe\n        if orientation == 'top':\n            self.image = pygame.transform.flip(PIPE_IMAGE, False, True)\n            self.rect = self.image.get_rect(midbottom=(x, y - 100))\n        else:\n            self.image = PIPE_IMAGE\n            self.rect = self.image.get_rect(midtop=(x, y + 100))\n        self.mask = pygame.mask.from_surface(self.image)\n\n    def update(self):\n        # Move the pipe leftward\n        self.rect.x -= GAME_SPEED\n        # Remove the pipe when it's off-screen\n        if self.rect.right < 0:\n            self.kill()\n\n# Function to create a new pair of pipes\ndef create_pipes():\n    # Randomly determine the gap's vertical position\n    gap_center = random.randint(150, HEIGHT - 150)\n    top_pipe = Pipe(WIDTH, gap_center, 'top')\n    bottom_pipe = Pipe(WIDTH, gap_center, 'bottom')\n    return top_pipe, bottom_pipe\n\n# Main game function\ndef main_game():\n    global GAME_SPEED\n    clock = pygame.time.Clock()\n    score = 0\n    running = True\n    game_over = False\n\n    # Sprite groups for efficient rendering and updates\n    all_sprites = pygame.sprite.Group()\n    pipe_group = pygame.sprite.Group()\n\n    # Create the player object and add it to the sprite group\n    player = Player()\n    all_sprites.add(player)\n\n    # Custom event for adding new pipes\n    ADDPIPE = pygame.USEREVENT + 1\n    pygame.time.set_timer(ADDPIPE, 1500)  # New pipe every 1.5 seconds\n\n    while running:\n        clock.tick(60)  # Limit the frame rate to 60 FPS\n\n        for event in pygame.event.get():\n            if event.type == pygame.QUIT:\n                running = False\n                pygame.quit()\n                sys.exit()\n\n            elif event.type == pygame.KEYDOWN:\n                if event.key == pygame.K_SPACE and not game_over:\n                    player.flap()  # Make the player jump\n                if event.key == pygame.K_r and game_over:\n                    main_game()  # Restart the game\n\n            elif event.type == ADDPIPE and not game_over:\n                # Add new pipes to the game\n                top_pipe, bottom_pipe = create_pipes()\n                all_sprites.add(top_pipe, bottom_pipe)\n                pipe_group.add(top_pipe, bottom_pipe)\n\n        if not game_over:\n            all_sprites.update()\n\n            # Check for collisions with pipes\n            if pygame.sprite.spritecollide(player, pipe_group, False, pygame.sprite.collide_mask):\n                game_over = True\n            # Check for collisions with the screen boundaries\n            if player.rect.top <= 0 or player.rect.bottom >= HEIGHT:\n                game_over = True\n\n            # Update the score and remove off-screen pipes\n            for pipe in pipe_group:\n                if pipe.rect.right < player.rect.left and not hasattr(pipe, 'scored'):\n                    score += 0.5  # Each pipe counts as 0.5 points\n                    pipe.scored = True  # Ensure each pipe is only counted once\n\n            # Gradually increase the game speed to raise difficulty\n            if int(score) % 5 == 0 and score != 0:\n                GAME_SPEED += 0.001  # Slightly increase the speed\n\n        # Draw the background\n        SCREEN.blit(BG_IMAGE, (0, 0))\n\n        # Draw all sprites (player and pipes)\n        all_sprites.draw(SCREEN)\n\n        # Display the current score\n        score_surface = FONT.render(f'Score: {int(score)}', True, WHITE)\n        SCREEN.blit(score_surface, (10, 10))\n\n        if game_over:\n            # Display 'Game Over' message\n            game_over_surface = FONT.render('Game Over!', True, WHITE)\n            SCREEN.blit(game_over_surface, (WIDTH // 2 - game_over_surface.get_width() // 2, HEIGHT // 2 - 50))\n            restart_surface = FONT.render('Press R to Restart', True, WHITE)\n            SCREEN.blit(restart_surface, (WIDTH // 2 - restart_surface.get_width() // 2, HEIGHT // 2))\n\n        # Update the display\n        pygame.display.flip()\n\n# Run the game\nif __name__ == '__main__':\n    main_game()\n"}
{"type": "source_file", "path": "research.py", "content": "from crewai import Agent, Task, Crew, Process\nfrom langchain_openai import OpenAI\nfrom crewai_tools import SerperDevTool\nimport os\n\n# Set up tools\nsearch_tool = SerperDevTool()\n\n# Set up language model\nllm = OpenAI(model_name=\"gpt-4o-mini\")\n\nos.environ[\"OPENAI_MODEL_NAME\"]=\"gpt-4o-mini\"\n\n# Create researcher agent\nresearcher = Agent(\n    role=\"Senior Research Analyst\",\n    goal=\"Uncover cutting-edge developments in AI\",\n    backstory=\"You are an experienced research analyst with a keen eye for emerging trends in technology. Your expertise lies in identifying groundbreaking AI innovations and their potential impact on various industries.\",\n    verbose=True,\n    allow_delegation=False,\n    tools=[search_tool],\n)\n\n# Create writer agent\nwriter = Agent(\n    role=\"Content Writer\",\n    goal=\"Create engaging articles about AI developments\",\n    backstory=\"You are a skilled writer with a passion for explaining complex technological concepts in simple terms. Your articles captivate readers while conveying accurate information about AI advancements.\",\n    verbose=True,\n    allow_delegation=False,\n)\n\nresearch_task = Task(\n    description=\"Research the latest advancements in AI and summarize the top 3 breakthroughs\",\n    agent=researcher,\n    expected_output=\"A bullet-point list of the top 3 AI breakthroughs with a brief explanation of each\"\n)\n\nwriting_task = Task(\n    description=\"Write a blog post about the top 3 AI breakthroughs\",\n    agent=writer,\n    expected_output=\"A 500-word blog post discussing the top 3 AI breakthroughs\",\n    context=[research_task]\n)\n\ncrew = Crew(\n    agents=[researcher, writer],\n    tasks=[research_task, writing_task],\n    process=Process.sequential,\n    verbose=True,\n)\n\nresult = crew.kickoff()\nprint(result)\n\n"}
{"type": "source_file", "path": "newsletter.py", "content": "from crewai import Agent, Task, Crew\nfrom crewai_tools import SerperDevTool\nimport os\n\nsearch_tool = SerperDevTool()\n\nos.environ[\"OPENAI_MODEL_NAME\"]=\"o1-mini\"\n\nresearcher = Agent(\n    role='Research Analyst',\n    goal='Find the latest and most relevant tech news',\n    backstory=\"You're an research analyst with a knack for discovering trending topics in tech.\",\n    tools=[search_tool]\n)\n\nwriter = Agent(\n    role='Content Writer',\n    goal='Create engaging newsletter content based on research',\n    backstory=\"You're a content writer with a talent for crafting compelling narratives.\"\n)\n\neditor = Agent(\n    role='Copy Editor',\n    goal='Ensure the newsletter is polished, error-free, and well-integrated',\n    backstory=\"You're a editor with an eye for detail, a mastery of language, and the ability to seamlessly integrate personalized content.\"\n)\n\npersonalizer = Agent(\n    role='Content Personalizer',\n    goal='Craft personalized introductions for different reader segments',\n    backstory=\"You're an content personalizer expert in analyzing reader preferences and creating engaging, personalized content.\",\n    tools=[search_tool]\n\n)\n\nresearch_task = Task(\n    description='Find the top 3 trending topics in AI and provide brief summaries',\n    agent=researcher,\n    expected_output=f\"A list of 3 trending topics with brief summaries for each\"\n)\n\nwriting_task = Task(\n    description='Write a 300-word article on each trending topic',\n    agent=writer,\n    expected_output=f\"Three 300-word articles about the trending topics\"\n)\n\nediting_task = Task(\n    description='Proofread and polish the articles, ensuring they flow well together. Integrate the personalized introduction seamlessly.',\n    agent=editor,\n    expected_output=\"Ensure newsletter is polished.\"\n)\n\npersonalization_task = Task(\n    description='Analyze reader data and create a personalized introduction for the newsletter',\n    agent=personalizer,\n    expected_output = 'Summary of a personalised intro'\n)\n\nnewsletter_crew = Crew(\n    agents=[researcher, writer, editor, personalizer],\n    tasks=[research_task, writing_task, editing_task, personalization_task],\n    verbose=True\n)\n\nresult = newsletter_crew.kickoff()\nprint(result)\n\n"}
{"type": "source_file", "path": "dynamic_research.py", "content": "from crewai import Agent, Task, Crew, Process\nfrom langchain_openai import OpenAI\nfrom crewai_tools import SerperDevTool\nimport os\n\n# Set up tools\nsearch_tool = SerperDevTool()\n\n# Set up language model\nllm = OpenAI(model_name=\"gpt-4o-mini\")\n\nos.environ[\"OPENAI_MODEL_NAME\"] = \"gpt-4o-mini\"\n\ndef create_crew(topic):\n    # Create researcher agent\n    researcher = Agent(\n        role=\"Senior Research Analyst\",\n        goal=f\"Uncover cutting-edge developments in {topic}\",\n        backstory=f\"You are an experienced research analyst with a keen eye for emerging trends in {topic}. Your expertise lies in identifying groundbreaking innovations and their potential impact on various industries.\",\n        verbose=True,\n        allow_delegation=False,\n        tools=[search_tool],\n    )\n\n    # Create writer agent\n    writer = Agent(\n        role=\"Content Writer\",\n        goal=f\"Create engaging articles about {topic} developments\",\n        backstory=f\"You are a skilled writer with a passion for explaining complex {topic} concepts in simple terms. Your articles captivate readers while conveying accurate information about advancements in {topic}.\",\n        verbose=True,\n        allow_delegation=False,\n    )\n\n    research_task = Task(\n        description=f\"Research the latest advancements in {topic} and summarize the top 3 breakthroughs\",\n        agent=researcher,\n        expected_output=f\"A bullet-point list of the top 3 {topic} breakthroughs with a brief explanation of each\"\n    )\n\n    writing_task = Task(\n        description=f\"Write a blog post about the top 3 {topic} breakthroughs\",\n        agent=writer,\n        expected_output=f\"A 500-word blog post discussing the top 3 {topic} breakthroughs\",\n        context=[research_task]\n    )\n\n    crew = Crew(\n        agents=[researcher, writer],\n        tasks=[research_task, writing_task],\n        process=Process.sequential,\n        verbose=True,\n    )\n\n    return crew\n\ndef main():\n    topic = input(\"Enter the topic you want to research: \")\n    crew = create_crew(topic)\n    result = crew.kickoff()\n    print(result)\n\nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "teleprompter.py", "content": "import tkinter as tk\nfrom tkinter import filedialog, messagebox, ttk\nimport docx\n\nclass Teleprompter:\n    def __init__(self, master):\n        self.master = master\n        master.title(\"Modern Teleprompter\")\n        master.geometry(\"800x600\")\n        master.resizable(False, False)  # Disable window resizing\n        \n        style = ttk.Style()\n        style.theme_use('clam')\n        \n        self.main_frame = ttk.Frame(master)\n        self.main_frame.pack(fill=tk.BOTH, expand=True)\n        \n        # Control frame at the top\n        self.control_frame = ttk.Frame(self.main_frame)\n        self.control_frame.pack(side=tk.TOP, fill=tk.X, padx=10, pady=10)\n        \n        self.load_button = ttk.Button(self.control_frame, text=\"Load Script\", command=self.load_script)\n        self.load_button.grid(row=0, column=0, padx=5, pady=5, sticky=\"ew\")\n        \n        self.start_button = ttk.Button(self.control_frame, text=\"Start\", command=self.start_scrolling)\n        self.start_button.grid(row=0, column=1, padx=5, pady=5, sticky=\"ew\")\n        \n        self.stop_button = ttk.Button(self.control_frame, text=\"Stop\", command=self.stop_scrolling)\n        self.stop_button.grid(row=0, column=2, padx=5, pady=5, sticky=\"ew\")\n        \n        self.restart_button = ttk.Button(self.control_frame, text=\"Restart\", command=self.restart_scrolling)\n        self.restart_button.grid(row=0, column=3, padx=5, pady=5, sticky=\"ew\")\n        \n        self.speed_label = ttk.Label(self.control_frame, text=\"Speed:\")\n        self.speed_label.grid(row=1, column=0, padx=5, pady=5, sticky=\"w\")\n        \n        self.speed_scale = ttk.Scale(self.control_frame, from_=0.1, to=50, orient=tk.HORIZONTAL)\n        self.speed_scale.set(5)\n        self.speed_scale.grid(row=1, column=1, columnspan=3, padx=5, pady=5, sticky=\"ew\")\n        \n        self.font_size_label = ttk.Label(self.control_frame, text=\"Font Size:\")\n        self.font_size_label.grid(row=2, column=0, padx=5, pady=5, sticky=\"w\")\n        \n        self.font_size_scale = ttk.Scale(self.control_frame, from_=12, to=72, orient=tk.HORIZONTAL, command=self.change_font_size)\n        self.font_size_scale.set(24)\n        self.font_size_scale.grid(row=2, column=1, columnspan=3, padx=5, pady=5, sticky=\"ew\")\n        \n        self.control_frame.grid_columnconfigure((0, 1, 2, 3), weight=1)\n        \n        # Text frame below the control frame\n        self.text_frame = ttk.Frame(self.main_frame)\n        self.text_frame.pack(fill=tk.BOTH, expand=True)\n        \n        self.text = tk.Text(self.text_frame, wrap=tk.WORD, font=(\"Arial\", 24), bg=\"black\", fg=\"white\")\n        self.text.pack(fill=tk.BOTH, expand=True)\n        \n        self.scrollbar = ttk.Scrollbar(self.text_frame, orient=\"vertical\", command=self.text.yview)\n        self.scrollbar.pack(side=\"right\", fill=\"y\")\n        self.text.configure(yscrollcommand=self.scrollbar.set)\n        \n        self.scrolling = False\n        self.scroll_position = 0.0\n        \n    def load_script(self):\n        file_path = filedialog.askopenfilename(filetypes=[(\"Word Document\", \"*.docx\"), (\"Text File\", \"*.txt\")])\n        if file_path:\n            try:\n                if file_path.endswith('.docx'):\n                    doc = docx.Document(file_path)\n                    full_text = \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n                else:  # Assume it's a .txt file\n                    with open(file_path, 'r', encoding='utf-8') as file:\n                        full_text = file.read()\n                self.text.delete(1.0, tk.END)\n                self.text.insert(tk.END, full_text)\n                self.scroll_position = 0.0\n                self.text.yview_moveto(0)\n            except Exception as e:\n                messagebox.showerror(\"Error\", f\"Failed to load the document: {str(e)}\")\n        \n    def start_scrolling(self):\n        self.scrolling = True\n        self.scroll_text()\n        \n    def stop_scrolling(self):\n        self.scrolling = False\n        \n    def restart_scrolling(self):\n        self.scroll_position = 0.0\n        self.text.yview_moveto(0)\n        self.start_scrolling()\n        \n    def scroll_text(self):\n        if self.scrolling:\n            self.scroll_position += 0.0001 * self.speed_scale.get()\n            self.text.yview_moveto(self.scroll_position)\n            if self.scroll_position >= 1.0:\n                self.stop_scrolling()\n                return\n            self.master.after(20, self.scroll_text)\n        \n    def change_font_size(self, size):\n        new_size = int(float(size))\n        self.text.configure(font=(\"Arial\", new_size))\n        # Adjust text widget height to maintain visibility of control frame\n        approx_lines = 500 // new_size  # Reduced from 600 to account for control frame\n        self.text.configure(height=approx_lines)\n\nroot = tk.Tk()\nteleprompter = Teleprompter(root)\nroot.mainloop()"}
