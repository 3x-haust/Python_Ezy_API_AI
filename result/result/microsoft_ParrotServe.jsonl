{"repo_info": {"repo_name": "ParrotServe", "repo_owner": "microsoft", "repo_url": "https://github.com/microsoft/ParrotServe"}}
{"type": "test_file", "path": "3rdparty/FastChat/fastchat/serve/test_message.py", "content": "\"\"\"Send a test message.\"\"\"\nimport argparse\nimport json\n\nimport requests\n\nfrom fastchat.model.model_adapter import get_conversation_template\n\n\ndef main():\n    model_name = args.model_name\n\n    if args.worker_address:\n        worker_addr = args.worker_address\n    else:\n        controller_addr = args.controller_address\n        ret = requests.post(controller_addr + \"/refresh_all_workers\")\n        ret = requests.post(controller_addr + \"/list_models\")\n        models = ret.json()[\"models\"]\n        models.sort()\n        print(f\"Models: {models}\")\n\n        ret = requests.post(\n            controller_addr + \"/get_worker_address\", json={\"model\": model_name}\n        )\n        worker_addr = ret.json()[\"address\"]\n        print(f\"worker_addr: {worker_addr}\")\n\n    if worker_addr == \"\":\n        print(f\"No available workers for {model_name}\")\n        return\n\n    conv = get_conversation_template(model_name)\n    conv.append_message(conv.roles[0], args.message)\n    conv.append_message(conv.roles[1], None)\n    prompt = conv.get_prompt()\n\n    headers = {\"User-Agent\": \"FastChat Client\"}\n    gen_params = {\n        \"model\": model_name,\n        \"prompt\": prompt,\n        \"temperature\": args.temperature,\n        \"max_new_tokens\": args.max_new_tokens,\n        \"stop\": conv.stop_str,\n        \"stop_token_ids\": conv.stop_token_ids,\n        \"echo\": False,\n    }\n    response = requests.post(\n        worker_addr + \"/worker_generate_stream\",\n        headers=headers,\n        json=gen_params,\n        stream=True,\n    )\n\n    print(f\"{conv.roles[0]}: {args.message}\")\n    print(f\"{conv.roles[1]}: \", end=\"\")\n    prev = 0\n    for chunk in response.iter_lines(decode_unicode=False, delimiter=b\"\\0\"):\n        if chunk:\n            data = json.loads(chunk.decode())\n            output = data[\"text\"].strip()\n            print(output[prev:], end=\"\", flush=True)\n            prev = len(output)\n    print(\"\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--controller-address\", type=str, default=\"http://localhost:21001\"\n    )\n    parser.add_argument(\"--worker-address\", type=str)\n    parser.add_argument(\"--model-name\", type=str, required=True)\n    parser.add_argument(\"--temperature\", type=float, default=0.0)\n    parser.add_argument(\"--max-new-tokens\", type=int, default=32)\n    parser.add_argument(\n        \"--message\", type=str, default=\"Tell me a story with more than 1000 words.\"\n    )\n    args = parser.parse_args()\n\n    main()\n"}
{"type": "test_file", "path": "tests/engine/builtin/test_shared_prefix.py", "content": "from parrot.engine.config import BuiltinConfig\nfrom parrot.engine.builtin.builtin_runner import BuiltinRunner\nfrom parrot.engine.primitive_job import Fill, Generate\nfrom parrot.sampling_config import SamplingConfig\n\n\ndef test_simple_batch_share():\n    model_name = \"lmsys/vicuna-7b-v1.3\"\n    builtin_config = BuiltinConfig(\n        num_kv_cache_blocks=1600,\n        attn_func=\"xformers_fill_shared_prompts_generate\",\n        block_size=16,\n    )\n    runner = BuiltinRunner(model_name, builtin_config)\n\n    batch_size = 16\n    # Expect result: 32\n    shared_len = 20\n    diverged_lens = [i * 10 for i in range(1, batch_size + 1)]\n\n    shared_fill = Fill(\n        pid=0,\n        tid=0,\n        context_id=0,\n        parent_context_id=-1,\n        token_ids=[100] * shared_len,\n    )\n\n    diverged_fills = [\n        Fill(\n            pid=0,\n            tid=0,\n            context_id=i + 1,\n            parent_context_id=0,\n            token_ids=[200] * diverged_lens[i],\n        )\n        for i in range(batch_size)\n    ]\n\n    gens = [\n        Generate(\n            pid=0,\n            tid=0,\n            context_id=i + 1 + batch_size,\n            parent_context_id=i + 1,\n            sampling_config=SamplingConfig(max_gen_length=20),\n        )\n        for i in range(batch_size)\n    ]\n\n    runner.run_iter([shared_fill])\n    runner.run_iter(diverged_fills)\n    for _ in range(10):\n        runner.run_iter(gens)\n\n\ndef test_two_level_batch_share():\n    model_name = \"lmsys/vicuna-7b-v1.3\"\n    builtin_config = BuiltinConfig(\n        num_kv_cache_blocks=1600,\n        attn_func=\"xformers_fill_shared_prompts_generate\",\n        block_size=16,\n    )\n    runner = BuiltinRunner(model_name, builtin_config)\n\n    batch_size = 16\n    # Expect result: 32+16=48\n    shared_len1 = 20\n    shared_len2 = 10\n    diverged_lens = [i * 10 for i in range(1, batch_size + 1)]\n\n    shared_fill1 = Fill(\n        pid=0,\n        tid=0,\n        context_id=0,\n        parent_context_id=-1,\n        token_ids=[100] * shared_len1,\n    )\n\n    shared_fill2 = Fill(\n        pid=0,\n        tid=0,\n        context_id=1,\n        parent_context_id=0,\n        token_ids=[100] * shared_len2,\n    )\n\n    diverged_fills = [\n        Fill(\n            pid=0,\n            tid=0,\n            context_id=i + 2,\n            parent_context_id=1,\n            token_ids=[200] * diverged_lens[i],\n        )\n        for i in range(batch_size)\n    ]\n\n    gens = [\n        Generate(\n            pid=0,\n            tid=0,\n            context_id=i + 2 + batch_size,\n            parent_context_id=i + 2,\n            sampling_config=SamplingConfig(max_gen_length=20),\n        )\n        for i in range(batch_size)\n    ]\n\n    runner.run_iter([shared_fill1])\n    runner.run_iter([shared_fill2])\n    runner.run_iter(diverged_fills)\n    for _ in range(10):\n        runner.run_iter(gens)\n\n\nif __name__ == \"__main__\":\n    # test_simple_batch_share()\n    test_two_level_batch_share()\n"}
{"type": "test_file", "path": "3rdparty/FastChat/playground/test_embedding/test_sentence_similarity.py", "content": "import json\nimport os\n\nimport numpy as np\nimport openai\nimport requests\nfrom scipy.spatial.distance import cosine\n\n\ndef get_embedding_from_api(word, model=\"vicuna-7b-v1.1\"):\n    if \"ada\" in model:\n        resp = openai.Embedding.create(\n            model=model,\n            input=word,\n        )\n        embedding = np.array(resp[\"data\"][0][\"embedding\"])\n        return embedding\n\n    url = \"http://localhost:8000/v1/embeddings\"\n    headers = {\"Content-Type\": \"application/json\"}\n    data = json.dumps({\"model\": model, \"input\": word})\n\n    response = requests.post(url, headers=headers, data=data)\n    if response.status_code == 200:\n        embedding = np.array(response.json()[\"data\"][0][\"embedding\"])\n        return embedding\n    else:\n        print(f\"Error: {response.status_code} - {response.text}\")\n        return None\n\n\ndef cosine_similarity(vec1, vec2):\n    return 1 - cosine(vec1, vec2)\n\n\ndef print_cosine_similarity(embeddings, texts):\n    for i in range(len(texts)):\n        for j in range(i + 1, len(texts)):\n            sim = cosine_similarity(embeddings[texts[i]], embeddings[texts[j]])\n            print(f\"Cosine similarity between '{texts[i]}' and '{texts[j]}': {sim:.2f}\")\n\n\ntexts = [\n    \"The quick brown fox\",\n    \"The quick brown dog\",\n    \"The fast brown fox\",\n    \"A completely different sentence\",\n]\n\nembeddings = {}\nfor text in texts:\n    embeddings[text] = get_embedding_from_api(text)\n\nprint(\"Vicuna-7B:\")\nprint_cosine_similarity(embeddings, texts)\n\nfor text in texts:\n    embeddings[text] = get_embedding_from_api(text, model=\"text-similarity-ada-001\")\n\nprint(\"text-similarity-ada-001:\")\nprint_cosine_similarity(embeddings, texts)\n\nfor text in texts:\n    embeddings[text] = get_embedding_from_api(text, model=\"text-embedding-ada-002\")\n\nprint(\"text-embedding-ada-002:\")\nprint_cosine_similarity(embeddings, texts)\n"}
{"type": "test_file", "path": "tests/serve/test_context.py", "content": "import json\n\nfrom parrot.serve.backend_repr import Context, ExecutionEngine, LanguageModel\nfrom parrot.engine.config import EngineConfig\nfrom parrot.testing.get_configs import get_sample_engine_config_path\n\nfrom parrot.serve.variable_manager import SemanticVariableManager\nfrom parrot.serve.scheduler import CompletionTask\nfrom parrot.serve.context_manager import PrefixCache, ServeCoreContextManager\nfrom parrot.sampling_config import SamplingConfig\nfrom parrot.serve.graph import (\n    RequestChain,\n    ConstantFill,\n    PlaceholderGen,\n    PlaceholderFill,\n)\nfrom parrot.serve.graph.call_request import (\n    SemanticCallMetadata,\n    SemanticFunctionParameter,\n)\n\n\ndef test_prefix_cache():\n    svs = [\"sv0\", \"sv1\", \"sv2\"]\n    prefix_cache = PrefixCache()\n    prefix_hash = \"\"\n    for context_id, sv in enumerate(svs):\n        prefix_hash += ServeCoreContextManager._hash_var_id(sv)\n        prefix_cache.cache_prefix_context(prefix_hash, context_id)\n    print(prefix_cache._prefix_ctx_map)\n\n\ndef test_context_manager():\n    session_id = 0\n    var_mgr = SemanticVariableManager(666)\n    var_mgr.register_local_var_space(session_id=0)\n    var0 = var_mgr.create_var(session_id, \"a\")\n    var0.set(\"Content0\")\n\n    request_chain = RequestChain.from_nodes(\n        nodes=[\n            ConstantFill(\"Test1\"),\n            PlaceholderFill(\n                parameter=SemanticFunctionParameter(\n                    name=\"a\", var_id=var0.id, is_output=False\n                )\n            ),\n            ConstantFill(\"Test2\"),\n            PlaceholderGen(\n                parameter=SemanticFunctionParameter(\n                    name=\"b\", is_output=True, sampling_config=SamplingConfig()\n                )\n            ),\n        ]\n    )\n    # request_chain.metadata.fuse_fill = True\n    var_mgr.create_vars_for_semantic_request_chain(session_id, request_chain)\n\n    task = CompletionTask(task_id=0, chain=request_chain.comp_chains[0])\n\n    config_path = get_sample_engine_config_path(\"opt-13b.json\")\n    with open(config_path, \"r\") as f:\n        engine_config = EngineConfig.from_dict(json.load(f))\n    engine = ExecutionEngine.from_engine_config(0, engine_config)\n\n    task.schedule_to(engine, update_engine_info=False)\n\n    context_mgr = ServeCoreContextManager()\n    context_mgr.register_engine_prefix_cache(engine.engine_id)\n    context_mgr.set_task_contexts(task)\n\n    print(context_mgr._context_ref_counter)\n    print(context_mgr.prefix_caches[engine.engine_id]._prefix_ctx_map)\n\n\nif __name__ == \"__main__\":\n    test_prefix_cache()\n    test_context_manager()\n"}
{"type": "test_file", "path": "tests/pfunc/test_prompt_formatter.py", "content": "from parrot import P\nfrom parrot.frontend.pfunc.transforms.prompt_formatter import (\n    PyIndentRemover,\n    SquashIntoOneLine,\n    AlwaysOneSpace,\n)\n\n\ndef test_py_indent_remover():\n    @P.semantic_function(formatter=None)\n    def foo(a: P.Output):\n        \"\"\"This is a function.\n        It has multiple lines.\n        And it has indents. {{a}}\n        \"\"\"\n\n    print(\"Before:\", foo.to_template_str())\n\n    foo = PyIndentRemover().transform(foo)\n\n    print(\"After:\", foo.to_template_str())\n\n\ndef test_squash_into_one_line(formatter=None):\n    @P.semantic_function(formatter=None)\n    def foo(a: P.Output):\n        \"\"\"This\n        is\n        a\n        function.\n        It\n        has multiple\n        lines. {{a}}\n        \"\"\"\n\n    print(\"Before:\", foo.to_template_str())\n\n    foo = SquashIntoOneLine().transform(foo)\n\n    print(\"After:\", foo.to_template_str())\n\n\ndef test_always_one_space():\n    @P.semantic_function(formatter=None)\n    def foo(a: P.Output):\n        \"\"\"This is  a   function.    It     has multiple      spaces.   {{a}}\"\"\"\n\n    print(\"Before:\", foo.to_template_str())\n\n    foo = AlwaysOneSpace().transform(foo)\n\n    print(\"After:\", foo.to_template_str())\n\n\nif __name__ == \"__main__\":\n    test_py_indent_remover()\n    test_squash_into_one_line()\n    test_always_one_space()\n"}
{"type": "test_file", "path": "tests/engine/builtin/test_opt.py", "content": "from parrot.engine.config import BuiltinConfig\nfrom parrot.utils import set_random_seed\nfrom parrot.testing.model_runner_test_template import *\n\n\ndef test_opt():\n    set_random_seed(0)\n\n    model_name = \"facebook/opt-125m\"\n    builtin_config = BuiltinConfig(\n        num_kv_cache_blocks=16000,\n        attn_func=\"xformers_with_buffer\",\n    )\n\n    template_test_single_fill(model_name, builtin_config)\n    template_test_batch_fills(model_name, builtin_config)\n    template_test_fill_then_gen(model_name, builtin_config)\n    template_test_generate_single_text(model_name, builtin_config)\n    template_test_generate_batch_text(model_name, builtin_config)\n    template_test_fill_generate_mixed(model_name, builtin_config)\n\n\nif __name__ == \"__main__\":\n    test_opt()\n"}
{"type": "test_file", "path": "3rdparty/vllm/tests/kernels/test_layernorm.py", "content": "import torch\nimport torch.nn as nn\n\nfrom vllm import layernorm_ops\n\n\nclass RefRMSNorm(nn.Module):\n\n    def __init__(self, hidden_size, eps=1e-6):\n        super().__init__()\n        weight = torch.empty(hidden_size)\n        weight.uniform_(-1e-3, 1e-3)\n        self.weight = nn.Parameter(weight)\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        variance = hidden_states.to(torch.float32).pow(2).mean(-1,\n                                                               keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance +\n                                                    self.variance_epsilon)\n        if self.weight.dtype in [torch.half, torch.float16, torch.bfloat16]:\n            hidden_states = hidden_states.to(self.weight.dtype)\n        return self.weight * hidden_states\n\n\n@torch.inference_mode()\ndef run_rms_norm(\n    num_tokens: int,\n    hidden_size: int,\n    dtype: torch.dtype,\n) -> None:\n    x = torch.randn(num_tokens, hidden_size, dtype=dtype, device='cuda')\n    ref = RefRMSNorm(hidden_size).to(dtype).cuda()\n\n    out = torch.empty_like(x)\n    layernorm_ops.rms_norm(\n        out,\n        x,\n        ref.weight.data,\n        ref.variance_epsilon,\n    )\n    ref_out = ref(x)\n    assert torch.allclose(out, ref_out, atol=1e-3, rtol=1e-5)\n\n\ndef test_rms_norm() -> None:\n    for dtype in [torch.half, torch.bfloat16, torch.float]:\n        for num_tokens in [7, 128, 2048]:\n            for hidden_size in [13, 64, 1024, 5120]:\n                print(f'Testing RMS kernel with dtype={dtype}, num_tokens='\n                      f'{num_tokens}, hidden_size={hidden_size}')\n                run_rms_norm(\n                    num_tokens=num_tokens,\n                    hidden_size=hidden_size,\n                    dtype=dtype,\n                )\n"}
{"type": "test_file", "path": "3rdparty/FastChat/tests/test_openai_langchain.py", "content": "# Usage:\n# python3 -m fastchat.serve.model_worker --model-path lmsys/vicuna-7b-v1.5 --model-names gpt-3.5-turbo,text-davinci-003,text-embedding-ada-002\n# export OPENAI_API_BASE=http://localhost:8000/v1\n# export OPENAI_API_KEY=EMPTY\n# wget https://raw.githubusercontent.com/hwchase17/langchain/v0.0.200/docs/modules/state_of_the_union.txt\n\nimport os\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.document_loaders import TextLoader\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.indexes import VectorstoreIndexCreator\n\n\ndef test_chain():\n    embedding = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n    loader = TextLoader(\"state_of_the_union.txt\")\n    index = VectorstoreIndexCreator(embedding=embedding).from_loaders([loader])\n\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n\n    questions = [\n        \"Who is the speaker\",\n        \"What did the president say about Ketanji Brown Jackson\",\n        \"What are the threats to America\",\n        \"Who are mentioned in the speech\",\n        \"Who is the vice president\",\n        \"How many projects were announced\",\n    ]\n\n    for query in questions:\n        print(\"Query:\", query)\n        print(\"Answer:\", index.query(query, llm=llm))\n\n\nif __name__ == \"__main__\":\n    os.environ[\"OPENAI_API_BASE\"] = \"http://localhost:8000/v1\"\n    os.environ[\"OPENAI_API_KEY\"] = \"empty\"\n    test_chain()\n"}
{"type": "test_file", "path": "tests/serve/test_scheduler.py", "content": "from typing import List, Optional\nfrom parrot.serve.scheduler import (\n    CompletionTask,\n    TaskCreator,\n    GlobalScheduler,\n    GlobalSchedulerConfig,\n)\nfrom parrot.serve.tokenizer_wrapper import TokenizersWrapper\nfrom parrot.serve.context_manager import ServeCoreContextManager\nfrom parrot.serve.variable_manager import SemanticVariableManager\nfrom parrot.serve.graph import (\n    RequestChain,\n    CompletionChain,\n    ConstantFill,\n    PlaceholderFill,\n    PlaceholderGen,\n    ComputeGraph,\n    PerformanceCriteria,\n    activate_sv,\n    SemanticVariable,\n)\nfrom parrot.serve.graph.call_request import (\n    SemanticCallMetadata,\n    SemanticFunctionParameter,\n)\nfrom parrot.engine.config import EngineConfig\nfrom parrot.serve.engine_manager import EngineManager\nfrom parrot.serve.graph.visualize_utils import view_graph\n\n\ndef test_default_policy_throughput():\n    scheduler_cfg = GlobalSchedulerConfig(\n        app_fifo=False,\n        graph_group=False,\n        ctx_group=False,\n        ctx_aware=False,\n        max_queue_size=1024,\n    )\n\n    graph = ComputeGraph()\n    tokenizers_wrapper = TokenizersWrapper()\n    context_mgr = ServeCoreContextManager()\n    engine_mgr = EngineManager(\n        tokenizers_wrapper=tokenizers_wrapper,\n        context_mgr=context_mgr,\n        engine_heartbeat_timeout=666,\n    )\n\n    scheduler = GlobalScheduler(\n        config=scheduler_cfg,\n        engine_mgr=engine_mgr,\n        context_mgr=context_mgr,\n    )\n    task_creator = TaskCreator()\n\n    # Register 4 identical engines\n    engine_config = EngineConfig(tokenizer=\"hf-internal-testing/llama-tokenizer\")\n    for _ in range(4):\n        engine_mgr.register_engine(engine_config)\n\n    var_mgr = SemanticVariableManager(666)\n    session_id = 0\n    var_mgr.register_local_var_space(session_id)\n\n    # 8 identical tasks\n    for _ in range(8):\n        request_chain = RequestChain.from_nodes(\n            nodes=[\n                ConstantFill(\"This is a test \"),\n                PlaceholderGen(\n                    parameter=SemanticFunctionParameter(name=\"a\", is_output=True)\n                ),\n            ]\n        )\n        var_mgr.create_vars_for_semantic_request_chain(session_id, request_chain)\n        graph.insert_and_update_request_chain(request_chain)\n        comp_chain = request_chain.comp_chains[0]\n        activate_sv(comp_chain.gen_node.sv, PerformanceCriteria.THROUGHPUT)\n        task = task_creator.create_task(comp_chain)\n        task.tokenize_chain(tokenizers_wrapper)\n        scheduler.submit_task(task)\n\n    scheduler.schedule()\n\n    # Expected results: all tasks go to the same engine\n\n\ndef test_default_policy_latency():\n    scheduler_cfg = GlobalSchedulerConfig(\n        app_fifo=False,\n        graph_group=False,\n        ctx_group=False,\n        ctx_aware=False,\n        max_queue_size=1024,\n    )\n\n    graph = ComputeGraph()\n    tokenizers_wrapper = TokenizersWrapper()\n    context_mgr = ServeCoreContextManager()\n    engine_mgr = EngineManager(\n        tokenizers_wrapper=tokenizers_wrapper,\n        context_mgr=context_mgr,\n        engine_heartbeat_timeout=666,\n    )\n\n    scheduler = GlobalScheduler(\n        config=scheduler_cfg,\n        engine_mgr=engine_mgr,\n        context_mgr=context_mgr,\n    )\n    task_creator = TaskCreator()\n\n    # Register 4 identical engines\n    engine_config = EngineConfig(tokenizer=\"hf-internal-testing/llama-tokenizer\")\n    for _ in range(4):\n        engine_mgr.register_engine(engine_config)\n\n    var_mgr = SemanticVariableManager(666)\n    session_id = 0\n    var_mgr.register_local_var_space(session_id)\n\n    # 8 identical tasks\n    for _ in range(8):\n        request_chain = RequestChain.from_nodes(\n            nodes=[\n                ConstantFill(\"This is a test \"),\n                PlaceholderGen(\n                    parameter=SemanticFunctionParameter(name=\"a\", is_output=True)\n                ),\n            ]\n        )\n        var_mgr.create_vars_for_semantic_request_chain(session_id, request_chain)\n        graph.insert_and_update_request_chain(request_chain)\n        comp_chain = request_chain.comp_chains[0]\n        activate_sv(comp_chain.gen_node.sv, PerformanceCriteria.LATENCY)\n        task = task_creator.create_task(comp_chain)\n        task.tokenize_chain(tokenizers_wrapper)\n        scheduler.submit_task(task)\n\n    scheduler.schedule()\n\n    # Expected results: 4 tasks engine0, 4 tasks engine1\n\n\ndef test_app_fifo():\n    scheduler_cfg = GlobalSchedulerConfig(\n        app_fifo=True,\n        graph_group=False,\n        ctx_group=False,\n        ctx_aware=False,\n        max_queue_size=1024,\n    )\n\n    graph = ComputeGraph()\n    tokenizers_wrapper = TokenizersWrapper()\n    context_mgr = ServeCoreContextManager()\n    engine_mgr = EngineManager(\n        tokenizers_wrapper=tokenizers_wrapper,\n        context_mgr=context_mgr,\n        engine_heartbeat_timeout=666,\n    )\n\n    scheduler = GlobalScheduler(\n        config=scheduler_cfg,\n        engine_mgr=engine_mgr,\n        context_mgr=context_mgr,\n    )\n    task_creator = TaskCreator()\n\n    # Register 1 engine with limited capacity\n    engine_config = EngineConfig(\n        tokenizer=\"hf-internal-testing/llama-tokenizer\", tasks_capacity=1\n    )\n    engine_mgr.register_engine(engine_config)\n\n    var_mgr = SemanticVariableManager(666)\n    session_id = 0\n    var_mgr.register_local_var_space(session_id)\n\n    # 8 tasks. Each group of 2 tasks with A->B dependency.\n    first_batch_tasks: List[CompletionTask] = []\n    second_batch_chains: List[CompletionChain] = []\n    out_vars: List[SemanticVariable] = []\n\n    for i in range(4):\n        request_chain1 = RequestChain.from_nodes(\n            nodes=[\n                ConstantFill(\"This is a test \"),\n                PlaceholderGen(\n                    parameter=SemanticFunctionParameter(name=\"a\", is_output=True)\n                ),\n            ]\n        )\n\n        var_mgr.create_vars_for_semantic_request_chain(session_id, request_chain1)\n        graph.insert_and_update_request_chain(request_chain1)\n        comp_chain1 = request_chain1.comp_chains[0]\n        out_vars.append(comp_chain1.gen_node.sv)\n\n        request_chain2 = RequestChain.from_nodes(\n            nodes=[\n                PlaceholderFill(\n                    parameter=SemanticFunctionParameter(\n                        name=\"a\",\n                        var_id=out_vars[i].id,\n                        is_output=False,\n                    )\n                ),\n                PlaceholderGen(\n                    parameter=SemanticFunctionParameter(name=\"b\", is_output=True)\n                ),\n            ]\n        )\n\n        var_mgr.create_vars_for_semantic_request_chain(session_id, request_chain2)\n        graph.insert_and_update_request_chain(request_chain2)\n        comp_chain2 = request_chain2.comp_chains[0]\n        activate_sv(comp_chain2.gen_node.sv, PerformanceCriteria.LATENCY)\n\n        task1 = task_creator.create_task(comp_chain1)\n        task1.tokenize_chain(tokenizers_wrapper)\n        first_batch_tasks.append(task1)\n        second_batch_chains.append(comp_chain2)\n\n        scheduler.submit_task(task1)\n\n    # view_graph(graph)\n\n    for i in range(4):\n        # Schedule.\n        # Expected result: No. i task in engine0.\n        scheduler.schedule()\n        # Set var as finish\n        out_vars[i].set(\"Content0\")\n        assert first_batch_tasks[i].is_scheduled\n        first_batch_tasks[i].leave_scheduled()\n\n        # Submit 2\n        comp_chain = second_batch_chains[i]\n        task = task_creator.create_task(comp_chain)\n        task.tokenize_chain(tokenizers_wrapper)\n        scheduler.submit_task(task)\n\n        # Schedule again.\n        # Expected result: No. i+4 task in engine 0.\n        scheduler.schedule()\n        assert task.is_scheduled\n        task.engine.update_servelayer_runtime_info_remove_task(task)\n\n    # view_graph(graph)\n    # 0 4 1 5 2 6 3 7\n\n\ndef test_graph_group():\n    scheduler_cfg = GlobalSchedulerConfig(\n        app_fifo=False,\n        graph_group=True,\n        ctx_group=False,\n        ctx_aware=False,\n        max_queue_size=1024,\n    )\n\n    graph = ComputeGraph()\n    tokenizers_wrapper = TokenizersWrapper()\n    context_mgr = ServeCoreContextManager()\n    engine_mgr = EngineManager(\n        tokenizers_wrapper=tokenizers_wrapper,\n        context_mgr=context_mgr,\n        engine_heartbeat_timeout=666,\n    )\n\n    scheduler = GlobalScheduler(\n        config=scheduler_cfg,\n        engine_mgr=engine_mgr,\n        context_mgr=context_mgr,\n    )\n    task_creator = TaskCreator()\n\n    # Register 1 engine\n    engine_config = EngineConfig(tokenizer=\"hf-internal-testing/llama-tokenizer\")\n    engine_mgr.register_engine(engine_config)\n\n    var_mgr = SemanticVariableManager(666)\n    session_id = 0\n    var_mgr.register_local_var_space(session_id)\n\n    # In this case we construct a graph with 17 requests, with 16 requests dependent on the 17th.\n    out_vars: List[SemanticVariable] = []\n    chains: List[CompletionChain] = []\n    for _ in range(16):\n        request_chain = RequestChain.from_nodes(\n            nodes=[\n                ConstantFill(\"This is a test \"),\n                PlaceholderGen(\n                    parameter=SemanticFunctionParameter(name=\"a\", is_output=True)\n                ),\n            ]\n        )\n\n        var_mgr.create_vars_for_semantic_request_chain(session_id, request_chain)\n        graph.insert_and_update_request_chain(request_chain)\n        comp_chain = request_chain.comp_chains[0]\n        out_vars.append(comp_chain.gen_node.sv)\n        chains.append(comp_chain)\n\n    # Construct the final request\n    request_chain = RequestChain.from_nodes(\n        nodes=[\n            PlaceholderFill(\n                parameter=SemanticFunctionParameter(\n                    name=f\"a_{i}\",\n                    var_id=out_vars[i].id,\n                    is_output=False,\n                )\n            )\n            for i in range(16)\n        ]\n        + [\n            PlaceholderGen(\n                parameter=SemanticFunctionParameter(name=\"b\", is_output=True)\n            ),\n        ]\n    )\n    var_mgr.create_vars_for_semantic_request_chain(session_id, request_chain)\n    graph.insert_and_update_request_chain(request_chain)\n    comp_chain = request_chain.comp_chains[0]\n    activate_sv(comp_chain.gen_node.sv, PerformanceCriteria.LATENCY)\n\n    # view_graph(graph)\n\n    for i in range(16):\n        task = task_creator.create_task(chains[i])\n        task.tokenize_chain(tokenizers_wrapper)\n        scheduler.submit_task(task)\n\n    scheduler.schedule()\n\n    # Expected result: 16 tasks go to the engine 0, ignoring the task_num_upperbound as a group.\n\n\ndef test_ctx_group():\n    scheduler_cfg = GlobalSchedulerConfig(\n        app_fifo=False,\n        graph_group=False,\n        ctx_group=True,\n        ctx_aware=False,\n        max_queue_size=1024,\n    )\n\n    graph = ComputeGraph()\n    tokenizers_wrapper = TokenizersWrapper()\n    context_mgr = ServeCoreContextManager()\n    engine_mgr = EngineManager(\n        tokenizers_wrapper=tokenizers_wrapper,\n        context_mgr=context_mgr,\n        engine_heartbeat_timeout=666,\n    )\n\n    scheduler = GlobalScheduler(\n        config=scheduler_cfg,\n        engine_mgr=engine_mgr,\n        context_mgr=context_mgr,\n    )\n    task_creator = TaskCreator()\n\n    # Register 4 identical engines\n    engine_config = EngineConfig(tokenizer=\"hf-internal-testing/llama-tokenizer\")\n    for _ in range(4):\n        engine_mgr.register_engine(engine_config)\n\n    var_mgr = SemanticVariableManager(666)\n    session_id = 0\n    var_mgr.register_local_var_space(session_id)\n\n    # 16 identical tasks\n    prompts = [f\"This is a test {i}\" for i in range(4)]\n    for i in range(16):\n        request_chain = RequestChain.from_nodes(\n            nodes=[\n                ConstantFill(prompts[i % 4]),\n                PlaceholderGen(\n                    parameter=SemanticFunctionParameter(name=\"a\", is_output=True)\n                ),\n            ]\n        )\n        var_mgr.create_vars_for_semantic_request_chain(session_id, request_chain)\n        graph.insert_and_update_request_chain(request_chain)\n        comp_chain = request_chain.comp_chains[0]\n        activate_sv(comp_chain.gen_node.sv, PerformanceCriteria.LATENCY)\n        task = task_creator.create_task(comp_chain)\n        task.tokenize_chain(tokenizers_wrapper)\n        scheduler.submit_task(task)\n\n    scheduler.schedule()\n\n    # Expected results: 0, 4, 8, 12 tasks go to engine 0, 1, 2, 3 respectively.\n\n\ndef test_ctx_aware():\n    scheduler_cfg = GlobalSchedulerConfig(\n        app_fifo=False,\n        graph_group=False,\n        ctx_group=False,\n        ctx_aware=True,\n        max_queue_size=1024,\n    )\n\n    graph = ComputeGraph()\n    tokenizers_wrapper = TokenizersWrapper()\n    context_mgr = ServeCoreContextManager()\n    engine_mgr = EngineManager(\n        tokenizers_wrapper=tokenizers_wrapper,\n        context_mgr=context_mgr,\n        engine_heartbeat_timeout=666,\n    )\n\n    scheduler = GlobalScheduler(\n        config=scheduler_cfg,\n        engine_mgr=engine_mgr,\n        context_mgr=context_mgr,\n    )\n    task_creator = TaskCreator()\n\n    # Register 4 identical engines\n    engine_config = EngineConfig(tokenizer=\"hf-internal-testing/llama-tokenizer\")\n    for _ in range(4):\n        engine_mgr.register_engine(engine_config)\n\n    var_mgr = SemanticVariableManager(666)\n    session_id = 0\n    var_mgr.register_local_var_space(session_id)\n\n    # 16 identical tasks\n    prompts = [f\"This is a test {i}\" for i in range(4)]\n    first_vars: List[SemanticVariable] = []\n    for i in range(16):\n        request_chain = RequestChain.from_nodes(\n            nodes=[\n                ConstantFill(prompts[i % 4]),\n                PlaceholderGen(\n                    parameter=SemanticFunctionParameter(name=\"a\", is_output=True)\n                ),\n            ]\n        )\n        var_mgr.create_vars_for_semantic_request_chain(session_id, request_chain)\n        graph.insert_and_update_request_chain(request_chain)\n        comp_chain = request_chain.comp_chains[0]\n        first_vars.append(comp_chain.first_node.sv)\n        activate_sv(comp_chain.gen_node.sv, PerformanceCriteria.THROUGHPUT)\n        task = task_creator.create_task(comp_chain)\n        task.tokenize_chain(tokenizers_wrapper)\n        scheduler.submit_task(task)\n\n    # Assign context in a round-robin manner (hacky)\n    for i in range(4):\n        prefix_cache = context_mgr.prefix_caches[i]\n        prefix_hash = context_mgr._hash_var_id(first_vars[i].id)\n        prefix_cache.cache_prefix_context(prefix_hash=prefix_hash, context_id=i)\n\n    scheduler.schedule()\n\n    # Expected results: 0, 4, 8, 12 tasks go to engine 0, 1, 2, 3 respectively.\n\n\nif __name__ == \"__main__\":\n    # test_default_policy_throughput()\n    # test_default_policy_latency()\n    # test_app_fifo()\n    test_graph_group()\n    # test_ctx_group()\n    # test_ctx_aware()\n"}
{"type": "test_file", "path": "tests/serve/test_tokenizer.py", "content": "from parrot.serve.tokenizer_wrapper import TokenizersWrapper\n\nfrom parrot.serve.variable_manager import SemanticVariableManager\nfrom parrot.serve.scheduler import CompletionTask\nfrom parrot.sampling_config import SamplingConfig\nfrom parrot.serve.graph import (\n    RequestChain,\n    ConstantFill,\n    PlaceholderGen,\n    PlaceholderFill,\n)\nfrom parrot.serve.graph.call_request import (\n    SemanticCallMetadata,\n    SemanticFunctionParameter,\n)\n\n\nTESTING_PROMPT_TEXT = (\n    \"He is widely acknowledged as one of the top achievers in his class\"\n)\nTESTING_TOKEN_IDS = [\n    940,\n    338,\n    17644,\n    24084,\n    3192,\n    408,\n    697,\n    310,\n    278,\n    2246,\n    3657,\n    347,\n    874,\n    297,\n    670,\n    770,\n]\n\n\ndef test_encode():\n    tokenizers_wrapper = TokenizersWrapper()\n    tokenizer_name = \"hf-internal-testing/llama-tokenizer\"\n    tokenizers_wrapper.register_tokenizer(tokenizer_name)\n\n    encoded = tokenizers_wrapper.tokenize(TESTING_PROMPT_TEXT, tokenizer_name)\n\n    # print(encoded)\n    assert encoded == TESTING_TOKEN_IDS\n\n\ndef test_decode():\n    tokenizers_wrapper = TokenizersWrapper()\n    tokenizer_name = \"hf-internal-testing/llama-tokenizer\"\n    tokenizers_wrapper.register_tokenizer(tokenizer_name)\n\n    decoded = tokenizers_wrapper.detokenize(TESTING_TOKEN_IDS, tokenizer_name)\n\n    assert TESTING_PROMPT_TEXT == decoded\n\n\ndef test_tokenize_request():\n    session_id = 0\n    var_mgr = SemanticVariableManager(666)\n    var_mgr.register_local_var_space(session_id=0)\n    var0 = var_mgr.create_var(session_id, \"a\")\n\n    request_chain = RequestChain.from_nodes(\n        nodes=[\n            ConstantFill(\"Test1\"),\n            PlaceholderFill(\n                parameter=SemanticFunctionParameter(\n                    name=\"a\", var_id=var0.id, is_output=False\n                )\n            ),\n            ConstantFill(\"Test2\"),\n            PlaceholderGen(\n                parameter=SemanticFunctionParameter(\n                    name=\"b\", is_output=True, sampling_config=SamplingConfig()\n                )\n            ),\n        ]\n    )\n\n    task = CompletionTask(task_id=0, chain=request_chain.comp_chains[0])\n\n    tokenizers_wrapper = TokenizersWrapper()\n    tokenizer_name1 = \"hf-internal-testing/llama-tokenizer\"\n    tokenizer_name2 = \"facebook/opt-13b\"\n    tokenizers_wrapper.register_tokenizer(tokenizer_name1)\n    tokenizers_wrapper.register_tokenizer(tokenizer_name2)\n\n    var0.set(\"Content0\")\n    var_mgr.create_vars_for_semantic_request_chain(session_id, request_chain)\n    task.tokenize_chain(tokenizers_wrapper)\n\n    print(task.tokenized_result)\n    token_ids_list1 = task.tokenized_result[tokenizer_name1]\n    for token_ids in token_ids_list1:\n        print(tokenizers_wrapper.detokenize(token_ids, tokenizer_name1))\n\n\nif __name__ == \"__main__\":\n    # test_encode()\n    # test_decode()\n    test_tokenize_request()\n"}
{"type": "test_file", "path": "tests/pfunc/test_vm.py", "content": "import time\nimport pytest\n\nfrom parrot import P\n\nfrom parrot.testing.fake_core_server import TESTING_SERVER_URL\nfrom parrot.testing.localhost_server_daemon import fake_core_server\n\n\ndef test_e2e():\n    with fake_core_server():\n\n        @P.semantic_function()\n        def test(a: P.Input, b: P.Input, c: P.Output):\n            \"\"\"This {{b}} is a test {{a}} function {{c}}\"\"\"\n\n        def main():\n            c = test(\"a\", b=\"b\")\n            print(c.get(P.PerformanceCriteria.LATENCY))\n\n        vm = P.VirtualMachine(core_http_addr=TESTING_SERVER_URL, mode=\"debug\")\n        vm.run(main)\n\n\n# @pytest.mark.skip(reason=\"Not implemented yet\")\ndef test_vm_import():\n    with fake_core_server():\n        vm = P.VirtualMachine(core_http_addr=TESTING_SERVER_URL, mode=\"debug\")\n        vm.import_function(\n            function_name=\"tell_me_a_joke\",\n            module_path=\"examples.codelib.app.common\",\n        )\n\n\ndef test_define_func():\n    with fake_core_server():\n        vm = P.VirtualMachine(core_http_addr=TESTING_SERVER_URL, mode=\"debug\")\n        func = vm.define_function(\n            func_name=\"test\",\n            func_body=\"This is a {{input}}. {{output}}\",\n            params=[\n                P.Parameter(name=\"input\", typ=P.ParamType.INPUT_LOC),\n                P.Parameter(name=\"output\", typ=P.ParamType.OUTPUT_LOC),\n            ],\n        )\n        print(func.to_template_str())\n\n\nif __name__ == \"__main__\":\n    test_e2e()\n    # test_vm_import()\n    # test_define_func()\n"}
{"type": "test_file", "path": "tests/serve/test_core.py", "content": "from parrot.serve.core import create_serve_core\n\nfrom parrot.testing.get_configs import get_sample_core_config_path\n\n\ndef test_launch_core():\n    config_path = get_sample_core_config_path(\"localhost_serve_core.json\")\n    core = create_serve_core(config_path)\n    print(core)\n\n\ndef test_core_register_session():\n    config_path = get_sample_core_config_path(\"localhost_serve_core.json\")\n    core = create_serve_core(config_path)\n    core.register_session({})\n\n\nif __name__ == \"__main__\":\n    test_launch_core()\n    test_core_register_session()\n"}
{"type": "test_file", "path": "tests/engine/test_http_server.py", "content": "import asyncio\nimport pytest\nimport json\nimport time\nfrom transformers import AutoTokenizer\n\nfrom parrot.engine.config import EngineConfig\nfrom parrot.protocol.internal.layer_apis import free_context\nfrom parrot.protocol.internal.primitive_request import Fill, Generate\nfrom parrot.serve.backend_repr import Context, ExecutionEngine\nfrom parrot.sampling_config import SamplingConfig\nfrom parrot.testing.get_configs import get_sample_engine_config_path\nfrom parrot.testing.localhost_server_daemon import engine_server\n\n\ndef _test_single_server_simple_serving(config):\n    engine_type, config_fn = config\n    engine_config_path = get_sample_engine_config_path(config_fn)\n\n    async def main():\n        with open(engine_config_path) as f:\n            engine_config = dict(json.load(f))\n        engine_config.pop(\"instance\")\n        engine_config.pop(\"scheduler\")\n        engine_config.pop(\"os\")\n        engine_config = EngineConfig(**engine_config)\n\n        os_engine = ExecutionEngine(\n            engine_id=0,\n            config=engine_config,\n        )\n\n        ctx = Context(\n            context_id=0,\n            engine=os_engine,\n        )\n\n        prompt_text = \"Hello, my name is\"\n        if engine_config.tokenizer != \"unknown\":\n            tokenizer = AutoTokenizer.from_pretrained(engine_config.tokenizer)\n            prompt_tokens = tokenizer(prompt_text)[\"input_ids\"]\n\n            fill_primitive = Fill(\n                pid=0,\n                tid=0,\n                context=ctx,\n                token_ids=prompt_tokens,\n            )\n        else:\n            fill_primitive = Fill(\n                pid=0,\n                tid=0,\n                context=ctx,\n                text=prompt_text,\n            )\n\n        resp = await fill_primitive.apost()\n        # assert resp.filled_len == len(prompt_tokens)\n\n        gen_primitive = Generate(\n            pid=0,\n            tid=0,\n            context=ctx,\n            sampling_config=SamplingConfig(max_gen_length=10),\n        )\n\n        if engine_type == \"native\":\n            generator = gen_primitive.astream()\n\n            text = prompt_text\n\n            async for token_id in generator:\n                # print(token_id)\n                text += tokenizer.decode([token_id])\n        else:\n            resp = await gen_primitive.apost()\n            text = resp.generated_text\n\n        print(\"Generated: \", text)\n\n    wait_ready_time = 20 if \"vicuna\" in config_fn else 5  # seconds\n\n    with engine_server(\n        config_fn,\n        wait_ready_time=wait_ready_time,\n        connect_to_os=False,\n    ):\n        time.sleep(5)\n        asyncio.run(main())\n\n\nTEST_CONFIGS_LIST = [\n    (\"native\", \"opt-125m.json\"),\n    (\"native\", \"vicuna-7b-v1.3.json\"),\n    (\"openai\", \"azure-openai-gpt-3.5-turbo.json\"),\n]\n\n\n@pytest.mark.skip(reason=\"OOM in test\")\ndef test_simple_serving():\n    for config in TEST_CONFIGS_LIST:\n        print(\"TESTING: \", config)\n        _test_single_server_simple_serving(config)\n        time.sleep(1)\n\n\nif __name__ == \"__main__\":\n    test_simple_serving()\n"}
{"type": "test_file", "path": "tests/pfunc/test_native_function.py", "content": "import pytest\nimport inspect\nfrom parrot import P\n\nfrom parrot.frontend.pfunc.function import PyNativeCall\n\n\ndef test_parse_native_function():\n    @P.native_function()\n    def add(a: P.Input, b: P.Input, c: P.Output):\n        ret = str(int(a) + int(b))\n        c.set(ret)\n\n    def add_pyfunc(a: str, b: str) -> str:\n        return str(int(a) + int(b))\n\n    print(add.display_signature())\n    print(add.inputs)\n    print(add.outputs)\n    print(inspect.signature(add_pyfunc))\n\n\ndef test_call_to_payload():\n    @P.native_function()\n    def add(a: str, b: str, c: P.Output):\n        ret = str(int(a) + int(b))\n        c.set(ret)\n\n    call: PyNativeCall = add(\"1\", \"2\")\n    print(call.to_request_payload())\n\n\nif __name__ == \"__main__\":\n    # test_parse_native_function()\n    test_call_to_payload()\n"}
{"type": "test_file", "path": "3rdparty/FastChat/tests/test_cli.py", "content": "\"\"\"Test command line interface for model inference.\"\"\"\nimport argparse\nimport os\n\nfrom fastchat.utils import run_cmd\n\n\ndef test_single_gpu():\n    models = [\n        \"lmsys/vicuna-7b-v1.5\",\n        \"lmsys/longchat-7b-16k\",\n        \"lmsys/fastchat-t5-3b-v1.0\",\n        \"meta-llama/Llama-2-7b-chat-hf\",\n        \"THUDM/chatglm-6b\",\n        \"THUDM/chatglm2-6b\",\n        \"mosaicml/mpt-7b-chat\",\n        \"tiiuae/falcon-7b-instruct\",\n        \"~/model_weights/alpaca-7b\",\n        \"~/model_weights/RWKV-4-Raven-7B-v11x-Eng99%-Other1%-20230429-ctx8192.pth\",\n    ]\n\n    for model_path in models:\n        if \"model_weights\" in model_path and not os.path.exists(\n            os.path.expanduser(model_path)\n        ):\n            continue\n        cmd = (\n            f\"python3 -m fastchat.serve.cli --model-path {model_path} \"\n            f\"--style programmatic < test_cli_inputs.txt\"\n        )\n        ret = run_cmd(cmd)\n        if ret != 0:\n            return\n\n        print(\"\")\n\n\ndef test_multi_gpu():\n    models = [\n        \"lmsys/vicuna-13b-v1.3\",\n    ]\n\n    for model_path in models:\n        cmd = (\n            f\"python3 -m fastchat.serve.cli --model-path {model_path} \"\n            f\"--style programmatic --num-gpus 2 --max-gpu-memory 14Gib < test_cli_inputs.txt\"\n        )\n        ret = run_cmd(cmd)\n        if ret != 0:\n            return\n        print(\"\")\n\n\ndef test_8bit():\n    models = [\n        \"lmsys/vicuna-13b-v1.3\",\n    ]\n\n    for model_path in models:\n        cmd = (\n            f\"python3 -m fastchat.serve.cli --model-path {model_path} \"\n            f\"--style programmatic --load-8bit < test_cli_inputs.txt\"\n        )\n        ret = run_cmd(cmd)\n        if ret != 0:\n            return\n        print(\"\")\n\n\ndef test_hf_api():\n    models = [\n        \"lmsys/vicuna-7b-v1.5\",\n        \"lmsys/fastchat-t5-3b-v1.0\",\n    ]\n\n    for model_path in models:\n        cmd = f\"python3 -m fastchat.serve.huggingface_api --model-path {model_path}\"\n        ret = run_cmd(cmd)\n        if ret != 0:\n            return\n        print(\"\")\n\n\nif __name__ == \"__main__\":\n    test_single_gpu()\n    test_multi_gpu()\n    test_8bit()\n    test_hf_api()\n"}
{"type": "test_file", "path": "tests/test_misc.py", "content": "from parrot.testing.latency_simulator import get_latency\n\n\ndef test_simulate_latency():\n    for _ in range(10):\n        latency = get_latency()\n        print(latency)\n\n\nif __name__ == \"__main__\":\n    test_simulate_latency()\n"}
{"type": "test_file", "path": "3rdparty/vllm/tests/kernels/test_pos_encoding.py", "content": "from typing import Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom vllm import pos_encoding_ops\n\n\ndef rotate_half(x: torch.Tensor) -> torch.Tensor:\n    x1 = x[..., :x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2:]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    cos: torch.Tensor,\n    sin: torch.Tensor,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\n\nclass RefRotaryEmbeddingNeox(nn.Module):\n    \"\"\"Reference implementation of the GPT-NeoX style rotary embedding.\"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        max_position_embeddings: int = 2048,\n        base: int = 10000,\n    ) -> None:\n        super().__init__()\n        self.rotary_dim = dim\n        self.max_position_embeddings = max_position_embeddings\n\n        # Create cos and sin embeddings.\n        inv_freq = 1.0 / (base**(torch.arange(0, dim, 2) / dim))\n        t = torch.arange(max_position_embeddings).float()\n        freqs = torch.einsum(\"i,j->ij\", t, inv_freq.float())\n        emb = torch.cat((freqs, freqs), dim=-1)\n        cos = emb.cos().to(dtype=inv_freq.dtype)\n        sin = emb.sin().to(dtype=inv_freq.dtype)\n        self.register_buffer(\"cos_cached\", cos, persistent=False)\n        self.register_buffer(\"sin_cached\", sin, persistent=False)\n\n    def forward(\n        self,\n        positions: torch.Tensor,  # [num_tokens]\n        query: torch.Tensor,  # [num_tokens, num_heads, head_size]\n        key: torch.Tensor,  # [num_tokens, num_heads, head_size]\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n\n        query_rot = query[..., :self.rotary_dim]\n        query_pass = query[..., self.rotary_dim:]\n        key_rot = key[..., :self.rotary_dim]\n        key_pass = key[..., self.rotary_dim:]\n\n        query_rot = query_rot.transpose(0, 1)\n        key_rot = key_rot.transpose(0, 1)\n        cos = F.embedding(positions, self.cos_cached)\n        sin = F.embedding(positions, self.sin_cached)\n        query_rot, key_rot = apply_rotary_pos_emb(query_rot, key_rot, cos, sin)\n        query_rot = query_rot.transpose(0, 1).contiguous()\n        key_rot = key_rot.transpose(0, 1).contiguous()\n\n        query = torch.cat((query_rot, query_pass), dim=-1)\n        key = torch.cat((key_rot, key_pass), dim=-1)\n\n        # Output query/key shape: [num_tokens, num_tokens, head_size]\n        return query, key\n\n\n@torch.inference_mode()\ndef run_rotary_embedding_neox(\n    num_tokens: int,\n    num_heads: int,\n    head_size: int,\n    max_position: int,\n    rotary_dim: int,\n    dtype: torch.dtype,\n    base: int = 10000,\n) -> None:\n    positions = torch.randint(0, max_position, (num_tokens, ), device='cuda')\n    query = torch.randn(num_tokens,\n                        num_heads * head_size,\n                        dtype=dtype,\n                        device='cuda')\n    key = torch.randn(num_tokens,\n                      num_heads * head_size,\n                      dtype=dtype,\n                      device='cuda')\n\n    # Create the rotary embedding.\n    inv_freq = 1.0 / (base**(torch.arange(0, rotary_dim, 2) / rotary_dim))\n    t = torch.arange(max_position).float()\n    freqs = torch.einsum('i,j -> ij', t, inv_freq.float())\n    cos = freqs.cos()\n    sin = freqs.sin()\n    cos_sin_cache = torch.cat((cos, sin), dim=-1)\n    cos_sin_cache = cos_sin_cache.to(dtype=dtype, device='cuda')\n\n    # Run the kernel. The kernel is in-place, so we need to clone the inputs.\n    out_query = query.clone()\n    out_key = key.clone()\n    pos_encoding_ops.rotary_embedding_neox(\n        positions,\n        out_query,\n        out_key,\n        head_size,\n        cos_sin_cache,\n    )\n\n    # Run the reference implementation.\n    ref_rotary_embedding = RefRotaryEmbeddingNeox(\n        dim=rotary_dim,\n        max_position_embeddings=max_position,\n        base=base,\n    ).to(dtype=dtype, device='cuda')\n    ref_query, ref_key = ref_rotary_embedding(\n        positions,\n        query.view(num_tokens, num_heads, head_size),\n        key.view(num_tokens, num_heads, head_size),\n    )\n    ref_query = ref_query.view(num_tokens, num_heads * head_size)\n    ref_key = ref_key.view(num_tokens, num_heads * head_size)\n\n    # Compare the results.\n    assert torch.allclose(out_query, ref_query, atol=1e-3, rtol=1e-5)\n    assert torch.allclose(out_key, ref_key, atol=1e-3, rtol=1e-5)\n\n\ndef test_rotary_embedding_neox() -> None:\n    for dtype in [torch.half, torch.bfloat16, torch.float]:\n        for head_size in [32, 64, 80, 96, 128, 160, 192, 256]:\n            print(f'Running tests for head_size={head_size} and dtype={dtype}')\n            run_rotary_embedding_neox(\n                num_tokens=2145,\n                num_heads=5,\n                head_size=head_size,\n                max_position=8192,\n                rotary_dim=head_size,\n                dtype=dtype,\n            )\n"}
{"type": "test_file", "path": "tests/engine/builtin/test_sampler.py", "content": "from transformers import AutoConfig\nimport torch\n\n\nfrom parrot.engine.builtin.models.opt import OPTForCausalLM\nfrom parrot.engine.config import BuiltinConfig\nfrom parrot.sampling_config import SamplingConfig\nfrom parrot.utils import set_random_seed\n\n\ndef test_sampling_one_token():\n    set_random_seed(2333)\n\n    model_config = AutoConfig.from_pretrained(\"facebook/opt-125m\")\n    builtin_config = BuiltinConfig(\n        num_kv_cache_blocks=1024, attn_func=\"xformers_with_buffer\"\n    )\n\n    # Just to get the sampler\n    torch.set_default_dtype(torch.float16)\n    model = OPTForCausalLM(model_config, builtin_config)\n    model.load_weights(\"facebook/opt-125m\")\n    model = model.cuda()\n\n    sampler = model.sampler\n    set_random_seed(2333)\n    hidden_states = torch.randn(\n        (8, model_config.hidden_size), dtype=torch.float16, device=\"cuda\"\n    )\n    ids = sampler(\n        hidden_states[-1:],\n        [\n            SamplingConfig(\n                temperature=1.0,\n                top_p=1.0,\n            )\n        ],\n    )\n\n    assert ids[0] == 14836\n\n\nif __name__ == \"__main__\":\n    test_sampling_one_token()\n"}
{"type": "test_file", "path": "3rdparty/FastChat/playground/test_embedding/test_classification.py", "content": "import json\nimport os\n\nimport numpy as np\nimport openai\nimport pandas as pd\nimport requests\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score\n\n\nnp.set_printoptions(threshold=10000)\n\n\ndef get_embedding_from_api(word, model=\"vicuna-7b-v1.1\"):\n    if \"ada\" in model:\n        resp = openai.Embedding.create(\n            model=model,\n            input=word,\n        )\n        embedding = np.array(resp[\"data\"][0][\"embedding\"])\n        return embedding\n\n    url = \"http://localhost:8000/v1/embeddings\"\n    headers = {\"Content-Type\": \"application/json\"}\n    data = json.dumps({\"model\": model, \"input\": word})\n\n    response = requests.post(url, headers=headers, data=data)\n    if response.status_code == 200:\n        embedding = np.array(response.json()[\"data\"][0][\"embedding\"])\n        return embedding\n    else:\n        print(f\"Error: {response.status_code} - {response.text}\")\n        return None\n\n\ndef create_embedding_data_frame(data_path, model, max_tokens=500):\n    df = pd.read_csv(data_path, index_col=0)\n    df = df[[\"Time\", \"ProductId\", \"UserId\", \"Score\", \"Summary\", \"Text\"]]\n    df = df.dropna()\n    df[\"combined\"] = (\n        \"Title: \" + df.Summary.str.strip() + \"; Content: \" + df.Text.str.strip()\n    )\n    top_n = 1000\n    df = df.sort_values(\"Time\").tail(top_n * 2)\n    df.drop(\"Time\", axis=1, inplace=True)\n\n    df[\"n_tokens\"] = df.combined.apply(lambda x: len(x))\n    df = df[df.n_tokens <= max_tokens].tail(top_n)\n    df[\"embedding\"] = df.combined.apply(lambda x: get_embedding_from_api(x, model))\n    return df\n\n\ndef train_random_forest(df):\n    X_train, X_test, y_train, y_test = train_test_split(\n        list(df.embedding.values), df.Score, test_size=0.2, random_state=42\n    )\n\n    clf = RandomForestClassifier(n_estimators=100)\n    clf.fit(X_train, y_train)\n    preds = clf.predict(X_test)\n\n    report = classification_report(y_test, preds)\n    accuracy = accuracy_score(y_test, preds)\n    return clf, accuracy, report\n\n\ninput_datapath = \"amazon_fine_food_review.csv\"\nif not os.path.exists(input_datapath):\n    raise Exception(\n        f\"Please download data from: https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews\"\n    )\n\ndf = create_embedding_data_frame(input_datapath, \"vicuna-7b-v1.1\")\nclf, accuracy, report = train_random_forest(df)\nprint(f\"Vicuna-7b-v1.1 accuracy:{accuracy}\")\ndf = create_embedding_data_frame(input_datapath, \"text-similarity-ada-001\")\nclf, accuracy, report = train_random_forest(df)\nprint(f\"text-similarity-ada-001 accuracy:{accuracy}\")\ndf = create_embedding_data_frame(input_datapath, \"text-embedding-ada-002\")\nclf, accuracy, report = train_random_forest(df)\nprint(f\"text-embedding-ada-002 accuracy:{accuracy}\")\n"}
{"type": "test_file", "path": "3rdparty/FastChat/playground/test_embedding/test_semantic_search.py", "content": "import json\nimport os\n\nimport numpy as np\nimport openai\nimport pandas as pd\nimport requests\nfrom scipy.spatial.distance import cosine\n\n\ndef cosine_similarity(vec1, vec2):\n    try:\n        return 1 - cosine(vec1, vec2)\n    except:\n        print(vec1.shape, vec2.shape)\n\n\ndef get_embedding_from_api(word, model=\"vicuna-7b-v1.1\"):\n    if \"ada\" in model:\n        resp = openai.Embedding.create(\n            model=model,\n            input=word,\n        )\n        embedding = np.array(resp[\"data\"][0][\"embedding\"])\n        return embedding\n\n    url = \"http://localhost:8000/v1/embeddings\"\n    headers = {\"Content-Type\": \"application/json\"}\n    data = json.dumps({\"model\": model, \"input\": word})\n\n    response = requests.post(url, headers=headers, data=data)\n    if response.status_code == 200:\n        embedding = np.array(response.json()[\"data\"][0][\"embedding\"])\n        return embedding\n    else:\n        print(f\"Error: {response.status_code} - {response.text}\")\n        return None\n\n\ndef create_embedding_data_frame(data_path, model, max_tokens=500):\n    df = pd.read_csv(data_path, index_col=0)\n    df = df[[\"Time\", \"ProductId\", \"UserId\", \"Score\", \"Summary\", \"Text\"]]\n    df = df.dropna()\n    df[\"combined\"] = (\n        \"Title: \" + df.Summary.str.strip() + \"; Content: \" + df.Text.str.strip()\n    )\n    top_n = 1000\n    df = df.sort_values(\"Time\").tail(top_n * 2)\n    df.drop(\"Time\", axis=1, inplace=True)\n\n    df[\"n_tokens\"] = df.combined.apply(lambda x: len(x))\n    df = df[df.n_tokens <= max_tokens].tail(top_n)\n    df[\"embedding\"] = df.combined.apply(lambda x: get_embedding_from_api(x, model))\n    return df\n\n\ndef search_reviews(df, product_description, n=3, pprint=False, model=\"vicuna-7b-v1.1\"):\n    product_embedding = get_embedding_from_api(product_description, model=model)\n    df[\"similarity\"] = df.embedding.apply(\n        lambda x: cosine_similarity(x, product_embedding)\n    )\n\n    results = (\n        df.sort_values(\"similarity\", ascending=False)\n        .head(n)\n        .combined.str.replace(\"Title: \", \"\")\n        .str.replace(\"; Content:\", \": \")\n    )\n    if pprint:\n        for r in results:\n            print(r[:200])\n            print()\n    return results\n\n\ndef print_model_search(input_path, model):\n    print(f\"Model: {model}\")\n    df = create_embedding_data_frame(input_path, model)\n    print(\"search: delicious beans\")\n    results = search_reviews(df, \"delicious beans\", n=5, model=model)\n    print(results)\n    print(\"search: whole wheat pasta\")\n    results = search_reviews(df, \"whole wheat pasta\", n=5, model=model)\n    print(results)\n    print(\"search: bad delivery\")\n    results = search_reviews(df, \"bad delivery\", n=5, model=model)\n    print(results)\n\n\ninput_datapath = \"amazon_fine_food_review.csv\"\nif not os.path.exists(input_datapath):\n    raise Exception(\n        f\"Please download data from: https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews\"\n    )\n\n\nprint_model_search(input_datapath, \"vicuna-7b-v1.1\")\nprint_model_search(input_datapath, \"text-similarity-ada-001\")\nprint_model_search(input_datapath, \"text-embedding-ada-002\")\n"}
{"type": "test_file", "path": "3rdparty/FastChat/tests/test_openai_api.py", "content": "\"\"\"\nTest the OpenAI compatible server\n\nLaunch:\npython3 launch_openai_api_test_server.py\n\"\"\"\n\nimport openai\n\nfrom fastchat.utils import run_cmd\n\nopenai.api_key = \"EMPTY\"  # Not support yet\nopenai.api_base = \"http://localhost:8000/v1\"\n\n\ndef test_list_models():\n    model_list = openai.Model.list()\n    names = [x[\"id\"] for x in model_list[\"data\"]]\n    return names\n\n\ndef test_completion(model, logprob):\n    prompt = \"Once upon a time\"\n    completion = openai.Completion.create(\n        model=model, prompt=prompt, logprobs=logprob, max_tokens=64\n    )\n    print(f\"full text: {prompt + completion.choices[0].text}\", flush=True)\n    if completion.choices[0].logprobs is not None:\n        print(f\"logprobs: {completion.choices[0].logprobs.token_logprobs}\", flush=True)\n\n\ndef test_completion_stream(model):\n    prompt = \"Once upon a time\"\n    res = openai.Completion.create(\n        model=model, prompt=prompt, max_tokens=64, stream=True\n    )\n    print(prompt, end=\"\")\n    for chunk in res:\n        content = chunk[\"choices\"][0][\"text\"]\n        print(content, end=\"\", flush=True)\n    print()\n\n\ndef test_embedding(model):\n    embedding = openai.Embedding.create(model=model, input=\"Hello world!\")\n    print(f\"embedding len: {len(embedding['data'][0]['embedding'])}\")\n    print(f\"embedding value[:5]: {embedding['data'][0]['embedding'][:5]}\")\n\n\ndef test_chat_completion(model):\n    completion = openai.ChatCompletion.create(\n        model=model, messages=[{\"role\": \"user\", \"content\": \"Hello! What is your name?\"}]\n    )\n    print(completion.choices[0].message.content)\n\n\ndef test_chat_completion_stream(model):\n    messages = [{\"role\": \"user\", \"content\": \"Hello! What is your name?\"}]\n    res = openai.ChatCompletion.create(model=model, messages=messages, stream=True)\n    for chunk in res:\n        content = chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n        print(content, end=\"\", flush=True)\n    print()\n\n\ndef test_openai_curl():\n    run_cmd(\"curl http://localhost:8000/v1/models\")\n\n    run_cmd(\n        \"\"\"\ncurl http://localhost:8000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"vicuna-7b-v1.5\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello! What is your name?\"}]\n  }'\n\"\"\"\n    )\n\n    run_cmd(\n        \"\"\"\ncurl http://localhost:8000/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"vicuna-7b-v1.5\",\n    \"prompt\": \"Once upon a time\",\n    \"max_tokens\": 41,\n    \"temperature\": 0.5\n  }'\n\"\"\"\n    )\n\n    run_cmd(\n        \"\"\"\ncurl http://localhost:8000/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"vicuna-7b-v1.5\",\n    \"input\": \"Hello world!\"\n  }'\n\"\"\"\n    )\n\n\nif __name__ == \"__main__\":\n    models = test_list_models()\n    print(f\"models: {models}\")\n\n    for model in models:\n        print(f\"===== Test {model} ======\")\n\n        if model in [\"fastchat-t5-3b-v1.0\"]:\n            logprob = None\n        else:\n            logprob = 1\n\n        test_completion(model, logprob)\n        test_completion_stream(model)\n        test_chat_completion(model)\n        test_chat_completion_stream(model)\n        try:\n            test_embedding(model)\n        except openai.error.APIError as e:\n            print(f\"Embedding error: {e}\")\n\n    print(\"===== Test curl =====\")\n    test_openai_curl()\n"}
{"type": "test_file", "path": "tests/serve/test_engine_manage.py", "content": "import json\nimport time\n\n\nfrom parrot.engine.config import EngineConfig\nfrom parrot.serve.context_manager import ServeCoreContextManager\nfrom parrot.serve.tokenizer_wrapper import TokenizersWrapper\nfrom parrot.serve.engine_manager import EngineManager\nfrom parrot.testing.get_configs import get_sample_engine_config_path\n\n\ndef test_engine_manager():\n    context_mgr = ServeCoreContextManager()\n    tokenizers_wrapper = TokenizersWrapper()\n    engine_mgr = EngineManager(\n        tokenizers_wrapper=tokenizers_wrapper,\n        context_mgr=context_mgr,\n        engine_heartbeat_timeout=5,\n    )\n    config_path = get_sample_engine_config_path(\"opt-13b.json\")\n\n    with open(config_path, \"r\") as f:\n        engine_config = EngineConfig.from_dict(json.load(f))\n\n    # Test engine registration\n    engine_id = engine_mgr.register_engine(engine_config)\n\n    engine = engine_mgr.get_engine(engine_id)\n    print(engine.model)\n    assert engine.engine_id == engine_id\n\n    # Test engine expiration\n    time.sleep(6)\n    engine_mgr.update_expired_engines()\n    engine_mgr.sweep_not_running_engines()\n\n    print(engine_mgr.engines, engine_mgr.models)\n\n\nif __name__ == \"__main__\":\n    test_engine_manager()\n"}
{"type": "test_file", "path": "3rdparty/FastChat/fastchat/serve/test_throughput.py", "content": "\"\"\"Benchmarking script to test the throughput of serving workers.\"\"\"\nimport argparse\nimport json\n\nimport requests\nimport threading\nimport time\n\nfrom fastchat.conversation import get_conv_template\n\n\ndef main():\n    if args.worker_address:\n        worker_addr = args.worker_address\n    else:\n        controller_addr = args.controller_address\n        ret = requests.post(controller_addr + \"/refresh_all_workers\")\n        ret = requests.post(controller_addr + \"/list_models\")\n        models = ret.json()[\"models\"]\n        models.sort()\n        print(f\"Models: {models}\")\n\n        ret = requests.post(\n            controller_addr + \"/get_worker_address\", json={\"model\": args.model_name}\n        )\n        worker_addr = ret.json()[\"address\"]\n        print(f\"worker_addr: {worker_addr}\")\n\n    if worker_addr == \"\":\n        return\n\n    conv = get_conv_template(\"vicuna_v1.1\")\n    conv.append_message(conv.roles[0], \"Tell me a story with more than 1000 words\")\n    prompt_template = conv.get_prompt()\n    prompts = [prompt_template for _ in range(args.n_thread)]\n\n    headers = {\"User-Agent\": \"fastchat Client\"}\n    ploads = [\n        {\n            \"model\": args.model_name,\n            \"prompt\": prompts[i],\n            \"max_new_tokens\": args.max_new_tokens,\n            \"temperature\": 0.0,\n            # \"stop\": conv.sep,\n        }\n        for i in range(len(prompts))\n    ]\n\n    def send_request(results, i):\n        if args.test_dispatch:\n            ret = requests.post(\n                controller_addr + \"/get_worker_address\", json={\"model\": args.model_name}\n            )\n            thread_worker_addr = ret.json()[\"address\"]\n        else:\n            thread_worker_addr = worker_addr\n        print(f\"thread {i} goes to {thread_worker_addr}\")\n        response = requests.post(\n            thread_worker_addr + \"/worker_generate_stream\",\n            headers=headers,\n            json=ploads[i],\n            stream=False,\n        )\n        k = list(\n            response.iter_lines(chunk_size=8192, decode_unicode=False, delimiter=b\"\\0\")\n        )\n        # print(k)\n        response_new_words = json.loads(k[-2].decode(\"utf-8\"))[\"text\"]\n        error_code = json.loads(k[-2].decode(\"utf-8\"))[\"error_code\"]\n        # print(f\"=== Thread {i} ===, words: {1}, error code: {error_code}\")\n        results[i] = len(response_new_words.split(\" \")) - len(prompts[i].split(\" \"))\n\n    # use N threads to prompt the backend\n    tik = time.time()\n    threads = []\n    results = [None] * args.n_thread\n    for i in range(args.n_thread):\n        t = threading.Thread(target=send_request, args=(results, i))\n        t.start()\n        # time.sleep(0.5)\n        threads.append(t)\n\n    for t in threads:\n        t.join()\n\n    print(f\"Time (POST): {time.time() - tik} s\")\n    # n_words = 0\n    # for i, response in enumerate(results):\n    #     # print(prompt[i].replace(conv.sep, \"\\n\"), end=\"\")\n    #     # make sure the streaming finishes at EOS or stopping criteria\n    #     k = list(response.iter_lines(chunk_size=8192, decode_unicode=False, delimiter=b\"\\0\"))\n    #     response_new_words = json.loads(k[-2].decode(\"utf-8\"))[\"text\"]\n    #     # print(response_new_words)\n    #     n_words += len(response_new_words.split(\" \")) - len(prompts[i].split(\" \"))\n    n_words = sum(results)\n    time_seconds = time.time() - tik\n    print(\n        f\"Time (Completion): {time_seconds}, n threads: {args.n_thread}, \"\n        f\"throughput: {n_words / time_seconds} words/s.\"\n    )\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--controller-address\", type=str, default=\"http://localhost:21001\"\n    )\n    parser.add_argument(\"--worker-address\", type=str)\n    parser.add_argument(\"--model-name\", type=str, default=\"vicuna\")\n    parser.add_argument(\"--max-new-tokens\", type=int, default=2048)\n    parser.add_argument(\"--n-thread\", type=int, default=8)\n    parser.add_argument(\"--test-dispatch\", action=\"store_true\")\n    args = parser.parse_args()\n\n    main()\n"}
{"type": "test_file", "path": "tests/serve/test_graph_viz.py", "content": "from parrot.serve.graph.call_request import ChunkedSemanticCallRequest\nfrom parrot.serve.variable_manager import SemanticVariableManager\nfrom parrot.serve.graph import (\n    RequestChain,\n    ComputeGraph,\n    ConstantFill,\n    PlaceholderFill,\n    PlaceholderGen,\n    PerformanceCriteria,\n    activate_sv,\n    NativeFuncNode,\n)\nfrom parrot.serve.graph.call_request import (\n    SemanticCallMetadata,\n    SemanticFunctionParameter,\n)\nfrom parrot.serve.graph.visualize_utils import view_graph\n\n\ndef test_view_graph_simple():\n    graph = ComputeGraph()\n\n    request_chain = RequestChain.from_nodes(\n        nodes=[\n            ConstantFill(\"This is a test \"),\n            PlaceholderGen(\n                parameter=SemanticFunctionParameter(name=\"a\", is_output=True)\n            ),\n        ]\n    )\n\n    var_mgr = SemanticVariableManager(666)\n    session_id = 0\n    var_mgr.register_local_var_space(session_id)\n    var_mgr.create_vars_for_semantic_request_chain(session_id, request_chain)\n\n    graph.insert_and_update_request_chain(request_chain)\n\n    view_graph(graph)\n\n\ndef test_view_graph_complex():\n    graph = ComputeGraph()\n\n    var_mgr = SemanticVariableManager(666)\n    session_id = 0\n    var_mgr.register_local_var_space(session_id)\n\n    request1 = RequestChain.from_nodes(\n        nodes=[\n            ConstantFill(\"This is a test \"),\n            PlaceholderGen(\n                parameter=SemanticFunctionParameter(name=\"a\", is_output=True)\n            ),\n        ]\n    )\n\n    var_mgr.create_vars_for_semantic_request_chain(session_id, request1)\n    graph.insert_and_update_request_chain(request1)\n    out_var0 = request1.comp_chains[0].gen_node.sv\n\n    request2 = RequestChain.from_nodes(\n        nodes=[\n            PlaceholderFill(\n                parameter=SemanticFunctionParameter(\n                    name=\"a\", var_id=out_var0.id, is_output=False\n                )\n            ),\n            PlaceholderGen(\n                parameter=SemanticFunctionParameter(name=\"b\", is_output=True)\n            ),\n        ]\n    )\n\n    var_mgr.create_vars_for_semantic_request_chain(session_id, request2)\n    graph.insert_and_update_request_chain(request2)\n    out_var1 = request2.comp_chains[0].gen_node.sv\n\n    request3 = RequestChain.from_nodes(\n        nodes=[\n            PlaceholderFill(\n                parameter=SemanticFunctionParameter(\n                    name=\"b\", var_id=out_var1.id, is_output=False\n                )\n            ),\n            PlaceholderGen(\n                parameter=SemanticFunctionParameter(name=\"c\", is_output=True)\n            ),\n        ]\n    )\n\n    var_mgr.create_vars_for_semantic_request_chain(session_id, request3)\n    graph.insert_and_update_request_chain(request3)\n    out_var2 = request3.comp_chains[0].gen_node.sv\n\n    view_graph(graph)\n    activate_sv(out_var0, PerformanceCriteria.LATENCY)\n    activate_sv(out_var1, PerformanceCriteria.LATENCY)\n    activate_sv(out_var2, PerformanceCriteria.LATENCY)\n\n\ndef test_view_graph_with_native():\n    graph = ComputeGraph()\n\n    var_mgr = SemanticVariableManager(666)\n    session_id = 0\n    var_mgr.register_local_var_space(session_id)\n\n    request1 = RequestChain.from_nodes(\n        nodes=[\n            ConstantFill(\"This is a test \"),\n            PlaceholderGen(\n                parameter=SemanticFunctionParameter(name=\"a\", is_output=True)\n            ),\n        ]\n    )\n\n    var_mgr.create_vars_for_semantic_request_chain(session_id, request1)\n    graph.insert_and_update_request_chain(request1)\n    out_var0 = request1.comp_chains[0].gen_node.sv\n\n    request2 = RequestChain.from_nodes(\n        nodes=[\n            ConstantFill(\"This is a test \"),\n            PlaceholderGen(\n                parameter=SemanticFunctionParameter(name=\"b\", is_output=True)\n            ),\n        ]\n    )\n\n    var_mgr.create_vars_for_semantic_request_chain(session_id, request2)\n    graph.insert_and_update_request_chain(request2)\n    out_var1 = request2.comp_chains[0].gen_node.sv\n\n    out_var2 = var_mgr.create_var(session_id=0, var_name=\"c\")\n\n    native_func_node = NativeFuncNode.from_variables(\n        func_name=\"test_func\",\n        input_vars={\"a\": out_var0, \"b\": out_var1},\n        output_vars={\"c\": out_var2},\n    )\n\n    graph.insert_native_func_node(native_func_node)\n\n    request3 = RequestChain.from_nodes(\n        nodes=[\n            PlaceholderFill(\n                parameter=SemanticFunctionParameter(\n                    name=\"c\", var_id=out_var2.id, is_output=False\n                )\n            ),\n            PlaceholderGen(\n                parameter=SemanticFunctionParameter(name=\"d\", is_output=True)\n            ),\n        ]\n    )\n\n    var_mgr.create_vars_for_semantic_request_chain(session_id, request3)\n    graph.insert_and_update_request_chain(request3)\n\n    view_graph(graph)\n    # activate_sv(out_var0, PerformanceCriteria.LATENCY)\n    # activate_sv(out_var1, PerformanceCriteria.LATENCY)\n    activate_sv(out_var2, PerformanceCriteria.LATENCY)\n\n    # for var in [out_var0, out_var1, out_var2]:\n    #     print(var.is_activated)\n\n\nif __name__ == \"__main__\":\n    # test_view_graph_simple()\n    # test_view_graph_complex()\n    test_view_graph_with_native()\n"}
{"type": "test_file", "path": "tests/pfunc/test_conversation_template.py", "content": "from parrot import P\nfrom parrot.frontend.pfunc.transforms.conversation_template import vicuna_template\n\n\ndef test_vicuna_template():\n    @P.semantic_function()\n    def foo(a: P.Input, b: P.Input, c: P.Output, d: P.Output):\n        \"\"\"This is a test function {{a}}.\n        An apple {{b}} a day keeps the doctor away.\n        Please show something. {{c}}\n        And something else. {{d}}\n        \"\"\"\n\n    print(\"Before:\", foo.to_template_str())\n\n    foo = vicuna_template.transform(foo)\n\n    print(\"After:\", foo.to_template_str())\n\n\nif __name__ == \"__main__\":\n    test_vicuna_template()\n"}
{"type": "test_file", "path": "tests/kernel/test_shared_kernel.py", "content": "from transformers import AutoTokenizer\nimport torch\nimport json\n\nfrom parrot.engine.builtin.builtin_runner import BuiltinRunner\nfrom parrot.engine.config import BuiltinConfig\nfrom parrot.engine.primitive_job import Fill, Generate\nfrom parrot.sampling_config import SamplingConfig\n\n\ndef test_shared_decode():\n    config = BuiltinConfig(\n        num_kv_cache_blocks=2048,\n        # attn_func=\"xformers_fill_shared_prompts_generate\",\n        attn_func=\"xformers_fill_vllm_paged_attention_generate\",\n        block_size=16,\n        max_seq_len=16384,\n    )\n    sampling_config = SamplingConfig(\n        max_gen_length=200,\n        ignore_tokenizer_eos=True,\n    )\n\n    runner = BuiltinRunner(\"lmsys/vicuna-7b-v1.3\", config=config)\n    tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n\n    # bs=2\n    # shared len = 1712\n    # diverged len = 3\n    prompt_token_ids = [\n        [100] * 1712 + [200, 300, 400],\n        [100] * 1712 + [300, 400, 500],\n    ]\n    num_seqs = len(prompt_token_ids)\n\n    shared_ids = 0\n    while len(set([prompt[shared_ids] for prompt in prompt_token_ids])) == 1:\n        shared_ids += 1\n    print(shared_ids)\n\n    shared_fill = Fill(\n        session_id=0,\n        task_id=0,\n        context_id=0,\n        parent_context_id=-1,\n        token_ids=prompt_token_ids[0][:shared_ids],\n    )\n    diverged_fills = [\n        Fill(\n            session_id=0,\n            task_id=0,\n            context_id=i + 1,\n            parent_context_id=0,\n            token_ids=prompt[shared_ids:],\n        )\n        for i, prompt in enumerate(prompt_token_ids)\n    ]\n    gens = [\n        Generate(\n            session_id=0,\n            task_id=0,\n            context_id=i + 1,\n            parent_context_id=0,\n            sampling_config=sampling_config,\n        )\n        for i, prompt in enumerate(prompt_token_ids)\n    ]\n\n    runner.run_iter([shared_fill])\n    runner.run_iter(diverged_fills)\n    for _ in range(10):\n        runner.run_iter(gens)\n\n\ndef test_masked_attention():\n    config = BuiltinConfig(\n        num_kv_cache_blocks=2048,\n        attn_func=\"xformers_fill_shared_prompts_generate\",\n        block_size=16,\n        max_seq_len=16384,\n    )\n    sampling_config = SamplingConfig(\n        max_gen_length=200,\n        ignore_tokenizer_eos=True,\n    )\n\n    runner = BuiltinRunner(\"lmsys/vicuna-7b-v1.3\", config=config)\n    tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n    prompt = \"Hi, my name is John. I'm a research scientist at a AI lab. I'm working on a project to develop a new AI model. I'm looking for a collaborator\"\n    prompt_token_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].tolist()\n\n    shared_ids = 20  # a number that is not divisible by 16\n\n    shared_fill = Fill(\n        session_id=0,\n        task_id=0,\n        context_id=0,\n        parent_context_id=-1,\n        token_ids=prompt_token_ids[0][:shared_ids],\n    )\n    diverged_fills = [\n        Fill(\n            session_id=0,\n            task_id=0,\n            context_id=i + 1,\n            parent_context_id=0,\n            token_ids=prompt[shared_ids:],\n        )\n        for i, prompt in enumerate(prompt_token_ids)\n    ]\n    gens = [\n        Generate(\n            session_id=0,\n            task_id=0,\n            context_id=i + 1,\n            parent_context_id=0,\n            sampling_config=sampling_config,\n        )\n        for i, prompt in enumerate(prompt_token_ids)\n    ]\n\n    runner.run_iter([shared_fill])\n    runner.run_iter(diverged_fills)\n    for _ in range(10):\n        runner.run_iter(gens)\n\n    print(tokenizer.decode(gens[0].context.token_ids))\n\n\nif __name__ == \"__main__\":\n    # test_shared_decode()\n    test_masked_attention()\n"}
{"type": "test_file", "path": "tests/test_protocol.py", "content": "import json\nimport time\nimport asyncio\n\nfrom parrot.protocol.internal.runtime_info import EngineRuntimeInfo\nfrom parrot.engine.config import EngineConfig\nfrom parrot.serve.backend_repr import ExecutionEngine, LanguageModel\nfrom parrot.serve.tokenizer_wrapper import TokenizersWrapper\nfrom parrot.serve.backend_repr.context import Context\n\nfrom parrot.protocol.public.apis import (\n    register_session,\n    get_session_info,\n    remove_session,\n    submit_semantic_call,\n    register_semantic_variable,\n    set_semantic_variable,\n    get_semantic_variable,\n    get_semantic_variable_list,\n)\nfrom parrot.protocol.internal.layer_apis import (\n    free_context,\n    ping_engine,\n    engine_heartbeat,\n    register_engine,\n)\nfrom parrot.protocol.internal.primitive_request import Fill, Generate\nfrom parrot.sampling_config import SamplingConfig\n\nfrom parrot.testing.fake_core_server import TESTING_SERVER_URL as CORE_URL\nfrom parrot.testing.fake_engine_server import TESTING_SERVER_URL as ENGINE_URL\nfrom parrot.testing.fake_engine_server import TESTING_SERVER_HOST, TESTING_SERVER_PORT\nfrom parrot.testing.localhost_server_daemon import fake_core_server, fake_engine_server\nfrom parrot.testing.get_configs import get_sample_engine_config_path\n\n\ndef test_register_session():\n    with fake_core_server():\n        resp = register_session(http_addr=CORE_URL, api_key=\"1\")\n        assert resp.session_id == 0\n\n\ndef test_remove_session():\n    with fake_core_server():\n        resp = register_session(http_addr=CORE_URL, api_key=\"1\")\n        resp1 = remove_session(\n            http_addr=CORE_URL, session_id=resp.session_id, session_auth=\"1\"\n        )\n\n\ndef test_submit_semantic_call():\n    payload = {\n        \"template\": \"This is a test {{a}} function. {{b}}\",\n        \"placeholders\": [\n            {\n                \"name\": \"a\",\n                \"is_output\": False,\n                \"var_id\": \"xxx\",\n            },\n            {\n                \"name\": \"b\",\n                \"is_output\": True,\n                \"sampling_config\": {\n                    \"temperature\": 0.9,\n                    \"top_p\": 0.9,\n                },\n            },\n        ],\n        \"models\": [\"model1\", \"model2\"],\n        \"model_type\": \"token_id\",\n        \"remove_pure_fill\": True,\n    }\n\n    with fake_core_server():\n        resp = submit_semantic_call(\n            http_addr=CORE_URL,\n            session_id=0,\n            session_auth=\"1\",\n            payload=payload,\n        )\n\n        assert resp.request_id == 0\n\n\ndef test_register_semantic_variable():\n    with fake_core_server():\n        resp = register_semantic_variable(\n            http_addr=CORE_URL,\n            session_id=0,\n            session_auth=\"1\",\n            var_name=\"test\",\n        )\n\n        print(resp.var_id)\n\n\ndef test_set_semantic_variable():\n    with fake_core_server():\n        resp = register_semantic_variable(\n            http_addr=CORE_URL,\n            session_id=0,\n            session_auth=\"1\",\n            var_name=\"test\",\n        )\n\n        print(resp.var_id)\n\n        resp1 = set_semantic_variable(\n            http_addr=CORE_URL,\n            session_id=0,\n            session_auth=\"1\",\n            var_id=resp.var_id,\n            content=\"test_value\",\n        )\n\n\ndef test_get_semantic_variable():\n    with fake_core_server():\n        resp = register_semantic_variable(\n            http_addr=CORE_URL,\n            session_id=0,\n            session_auth=\"1\",\n            var_name=\"test\",\n        )\n\n        print(resp.var_id)\n        content = \"test_value\"\n\n        resp1 = set_semantic_variable(\n            http_addr=CORE_URL,\n            session_id=0,\n            session_auth=\"1\",\n            var_id=resp.var_id,\n            content=content,\n        )\n\n        resp2 = get_semantic_variable(\n            http_addr=CORE_URL,\n            session_id=0,\n            session_auth=\"1\",\n            var_id=resp.var_id,\n            criteria=\"latency\",\n        )\n\n        assert resp2.content == content\n\n\ndef test_free_context():\n    with fake_engine_server():\n        resp = free_context(\n            http_addr=ENGINE_URL,\n            context_id=0,\n        )\n\n        assert resp.context_len == 0\n\n\ndef test_ping_engine():\n    with fake_engine_server():\n        resp = ping_engine(http_addr=ENGINE_URL)\n        assert resp.pong\n\n\ndef test_engine_heartbeat():\n    with fake_core_server():\n        resp = engine_heartbeat(\n            http_addr=CORE_URL,\n            engine_id=0,\n            engine_name=\"test\",\n            runtime_info=EngineRuntimeInfo(),\n        )\n\n\ndef _get_opt_125m_engine_config():\n    engine_config_path = get_sample_engine_config_path(\"opt-125m.json\")\n    with open(engine_config_path, \"r\") as f:\n        engine_config = json.load(f)\n\n    assert EngineConfig.verify_config(engine_config)\n    engine_config = EngineConfig.from_dict(engine_config)\n    engine_config.host = TESTING_SERVER_HOST\n    engine_config.port = TESTING_SERVER_PORT\n    return engine_config\n\n\ndef test_register_engine():\n    engine_config = _get_opt_125m_engine_config()\n\n    with fake_core_server():\n        resp = register_engine(\n            http_addr=CORE_URL,\n            engine_config=engine_config,\n        )\n\n        assert (\n            resp.engine_id == 0\n        )  # It's related to the allocating policy of the fake core server\n\n\ndef test_fill():\n    engine_config = _get_opt_125m_engine_config()\n    engine = ExecutionEngine.from_engine_config(engine_id=0, config=engine_config)\n\n    async def main():\n        primitve = Fill(\n            session_id=0,\n            task_id=0,\n            context_id=0,\n            parent_context_id=-1,\n            end_flag=False,\n            token_ids=[1, 2, 3],\n        )\n        st = time.perf_counter_ns()\n        resp = primitve.post(engine.http_address)\n        ed = time.perf_counter_ns()\n        print(\"Fill Time Used: \", (ed - st) / 1e9)\n        assert resp.filled_len == 3\n        resp = await primitve.apost(engine.http_address)\n        assert resp.filled_len == 3\n\n    with fake_engine_server():\n        asyncio.run(main())\n\n\ndef test_generate():\n    engine_config = _get_opt_125m_engine_config()\n    model = LanguageModel.from_engine_config(engine_config)\n    engine = ExecutionEngine(\n        engine_id=0,\n        config=engine_config,\n        model=model,\n    )\n\n    async def main():\n        primitive = Generate(\n            session_id=0,\n            task_id=0,\n            context_id=0,\n            parent_context_id=-1,\n            end_flag=False,\n            sampling_config=SamplingConfig(),\n        )\n\n        # Generate\n        st = time.perf_counter_ns()\n        resp = await primitive.apost(engine.http_address)\n        ed = time.perf_counter_ns()\n        print(\n            \"Generate Time Used: \",\n            (ed - st) / 1e9,\n            f\"(s), generated tokens: {len(resp.generated_ids)}\",\n        )\n\n        # Generate Stream\n        counter = 0\n        times = []\n\n        st = time.perf_counter_ns()\n        async for token_id in primitive.astream(engine.http_address):\n            counter += 1\n            # assert counter == token_id\n            # print(token_id)\n            cur_time = time.perf_counter_ns()\n            times.append((cur_time - st) / 1e9)\n            st = cur_time\n\n        print(\"Generation Time Points: \", times)\n\n    with fake_engine_server():\n        asyncio.run(main())\n\n\nif __name__ == \"__main__\":\n    # test_register_session()\n    # test_remove_session()\n    test_submit_semantic_call()\n    # test_register_semantic_variable()\n    # test_set_semantic_variable()\n    # test_get_semantic_variable()\n    # test_free_context()\n    # test_fill()\n    # test_generate()\n    pass\n"}
{"type": "test_file", "path": "3rdparty/vllm/tests/kernels/test_cache.py", "content": "import random\n\nimport torch\n\nfrom vllm import cache_ops\n\n\n@torch.inference_mode()\ndef run_copy_blocks(\n    num_mappings: int,\n    num_layers: int,\n    num_heads: int,\n    head_size: int,\n    block_size: int,\n    num_blocks: int,\n    dtype: torch.dtype,\n) -> None:\n    # Generate random block mappings.\n    src_blocks = random.sample(range(num_blocks), num_mappings)\n    remainig_blocks = list(set(range(num_blocks)) - set(src_blocks))\n    dst_blocks = random.sample(remainig_blocks, num_mappings)\n    block_mapping = {src: [dst] for src, dst in zip(src_blocks, dst_blocks)}\n\n    # Create the KV cache.\n    x = 16 // torch.tensor([], dtype=dtype).element_size()\n    key_cache_shape = (num_blocks, num_heads, head_size // x, block_size, x)\n    key_caches = []\n    for _ in range(num_layers):\n        key_cache = torch.randn(size=key_cache_shape,\n                                dtype=dtype,\n                                device='cuda')\n        key_caches.append(key_cache)\n    cloned_key_caches = []\n    for key_cache in key_caches:\n        cloned_key_caches.append(key_cache.clone())\n\n    value_cache_shape = (num_blocks, num_heads, head_size, block_size)\n    value_caches = []\n    for _ in range(num_layers):\n        value_cache = torch.randn(size=value_cache_shape,\n                                  dtype=dtype,\n                                  device='cuda')\n        value_caches.append(value_cache)\n    cloned_value_caches = []\n    for value_cache in value_caches:\n        cloned_value_caches.append(value_cache.clone())\n\n    # Call the copy blocks kernel.\n    cache_ops.copy_blocks(key_caches, value_caches, block_mapping)\n\n    # Reference implementation.\n    for src, dsts in block_mapping.items():\n        for dst in dsts:\n            for key_cache, cloned_key_cache in zip(key_caches,\n                                                   cloned_key_caches):\n                cloned_key_cache[dst] = cloned_key_cache[src]\n            for value_cache, cloned_value_cache in zip(value_caches,\n                                                       cloned_value_caches):\n                cloned_value_cache[dst] = cloned_value_cache[src]\n\n    # Compare the results.\n    for key_cache, cloned_key_cache in zip(key_caches, cloned_key_caches):\n        assert torch.allclose(key_cache, cloned_key_cache)\n    for value_cache, cloned_value_cache in zip(value_caches,\n                                               cloned_value_caches):\n        assert torch.allclose(value_cache, cloned_value_cache)\n\n\n@torch.inference_mode()\ndef run_reshape_and_cache(\n    num_tokens: int,\n    num_heads: int,\n    head_size: int,\n    block_size: int,\n    num_blocks: int,\n    dtype: torch.dtype,\n) -> None:\n    num_slots = block_size * num_blocks\n    slot_mapping = random.sample(range(num_slots), num_tokens)\n    slot_mapping = torch.tensor(slot_mapping, dtype=torch.int, device='cuda')\n\n    qkv = torch.randn(num_tokens,\n                      3,\n                      num_heads,\n                      head_size,\n                      dtype=dtype,\n                      device='cuda')\n    _, key, value = qkv.unbind(dim=1)\n\n    x = 16 // torch.tensor([], dtype=dtype).element_size()\n    key_cache_shape = (num_blocks, num_heads, head_size // x, block_size, x)\n    key_cache = torch.randn(size=key_cache_shape, dtype=dtype, device='cuda')\n    cloned_key_cache = key_cache.clone()\n\n    value_cache_shape = (num_blocks, num_heads, head_size, block_size)\n    value_cache = torch.randn(size=value_cache_shape,\n                              dtype=dtype,\n                              device='cuda')\n    cloned_value_cache = value_cache.clone()\n\n    cache_ops.reshape_and_cache(key, value, key_cache, value_cache,\n                                slot_mapping)\n\n    for i in range(num_tokens):\n        reshaped_key = key.reshape(num_tokens, num_heads, head_size // x, x)\n        block_idx = torch.div(slot_mapping[i],\n                              block_size,\n                              rounding_mode='floor')\n        block_offset = slot_mapping[i] % block_size\n        cloned_key_cache[block_idx, :, :, block_offset, :] = reshaped_key[i]\n        cloned_value_cache[block_idx, :, :, block_offset] = value[i]\n\n    assert torch.allclose(key_cache, cloned_key_cache)\n    assert torch.allclose(value_cache, cloned_value_cache)\n\n\n@torch.inference_mode()\ndef run_gather_cached_kv(\n    num_tokens: int,\n    num_heads: int,\n    head_size: int,\n    block_size: int,\n    num_blocks: int,\n    dtype: torch.dtype,\n) -> None:\n    num_slots = block_size * num_blocks\n    slot_mapping = random.sample(range(num_slots), num_tokens)\n    slot_mapping = torch.tensor(slot_mapping, dtype=torch.int, device='cuda')\n\n    qkv = torch.randn(num_tokens,\n                      3,\n                      num_heads,\n                      head_size,\n                      dtype=dtype,\n                      device='cuda')\n    _, key, value = qkv.unbind(dim=1)\n\n    qkv_clone = qkv.clone()\n    _, cloned_key, cloned_value = qkv_clone.unbind(dim=1)\n\n    x = 16 // torch.tensor([], dtype=dtype).element_size()\n    key_cache_shape = (num_blocks, num_heads, head_size // x, block_size, x)\n    key_cache = torch.randn(size=key_cache_shape, dtype=dtype, device='cuda')\n\n    value_cache_shape = (num_blocks, num_heads, head_size, block_size)\n    value_cache = torch.randn(size=value_cache_shape,\n                              dtype=dtype,\n                              device='cuda')\n\n    cache_ops.gather_cached_kv(key, value, key_cache, value_cache,\n                               slot_mapping)\n\n    # Reference implementation.\n    for i in range(num_tokens):\n        reshaped_key = cloned_key.reshape(num_tokens, num_heads,\n                                          head_size // x, x)\n        block_idx = torch.div(slot_mapping[i],\n                              block_size,\n                              rounding_mode='floor')\n        block_offset = slot_mapping[i] % block_size\n        reshaped_key[i] = key_cache[block_idx, :, :, block_offset, :]\n        cloned_value[i] = value_cache[block_idx, :, :, block_offset]\n\n    assert torch.allclose(key, cloned_key)\n    assert torch.allclose(value, cloned_value)\n\n\ndef test_copy_blocks() -> None:\n    for dtype in [torch.half, torch.bfloat16, torch.float]:\n        run_copy_blocks(num_mappings=23,\n                        num_layers=7,\n                        num_heads=17,\n                        head_size=16,\n                        block_size=8,\n                        num_blocks=1024,\n                        dtype=dtype)\n\n\ndef test_reshape_and_cache() -> None:\n    for dtype in [torch.half, torch.bfloat16, torch.float]:\n        run_reshape_and_cache(num_tokens=3,\n                              num_heads=2,\n                              head_size=16,\n                              block_size=8,\n                              num_blocks=2,\n                              dtype=dtype)\n\n\ndef test_gather_cached_kv() -> None:\n    for dtype in [torch.half, torch.bfloat16, torch.float]:\n        run_gather_cached_kv(num_tokens=3,\n                             num_heads=2,\n                             head_size=16,\n                             block_size=8,\n                             num_blocks=2,\n                             dtype=dtype)\n"}
{"type": "test_file", "path": "tests/serve/test_graph.py", "content": "from parrot.serve.graph.call_request import ChunkedSemanticCallRequest\nfrom parrot.serve.variable_manager import SemanticVariableManager\nfrom parrot.serve.graph import (\n    RequestChain,\n    ComputeGraph,\n    ConstantFill,\n    PlaceholderFill,\n    PlaceholderGen,\n    PerformanceCriteria,\n    activate_sv,\n)\nfrom parrot.serve.graph.call_request import (\n    SemanticCallMetadata,\n    SemanticFunctionParameter,\n)\n\n\ndef test_request_parse():\n    payload = {\n        \"template\": \"This is a test {{a}} function. {{b}}\",\n        \"parameters\": [\n            {\n                \"name\": \"a\",\n                \"is_output\": False,\n                \"var_id\": \"xxx\",\n            },\n            {\n                \"name\": \"b\",\n                \"is_output\": True,\n                \"sampling_config\": {\n                    \"temperature\": 0.9,\n                    \"top_p\": 0.9,\n                },\n            },\n        ],\n        \"models\": [\"model1\", \"model2\"],\n        \"model_type\": \"token_id\",\n        \"remove_pure_fill\": True,\n    }\n\n    chunked_request = ChunkedSemanticCallRequest.parse_from_payload(\n        request_id=0, session_id=0, payload=payload\n    )\n    print(chunked_request)\n\n\ndef test_split_prefix():\n    payload = {\n        \"template\": \"This is a test {{a}} function. {{b}}\",\n        \"parameters\": [\n            {\n                \"name\": \"a\",\n                \"is_output\": False,\n                \"var_id\": \"xxx\",\n            },\n            {\n                \"name\": \"b\",\n                \"is_output\": True,\n                \"sampling_config\": {\n                    \"temperature\": 0.9,\n                    \"top_p\": 0.9,\n                },\n            },\n        ],\n        \"models\": [\"model1\", \"model2\"],\n        \"model_type\": \"token_id\",\n        \"remove_pure_fill\": True,\n    }\n\n    chunked_request = ChunkedSemanticCallRequest.parse_from_payload(\n        request_id=0, session_id=0, payload=payload\n    )\n    chunked_request.split_prefix_chunk(5)\n    print(chunked_request)\n\n\ndef test_request_chain_print():\n    request_chain = RequestChain.from_nodes(\n        nodes=[\n            ConstantFill(\"This is a test \"),\n            PlaceholderGen(\n                parameter=SemanticFunctionParameter(name=\"a\", is_output=True)\n            ),\n        ],\n    )\n\n    print(request_chain.pretty_print())\n\n\ndef test_chunked_request_to_chain():\n    payload = {\n        \"template\": \"This is a test {{a}} function. {{b}}\",\n        \"parameters\": [\n            {\n                \"name\": \"a\",\n                \"is_output\": False,\n                \"var_id\": \"xxx\",\n            },\n            {\n                \"name\": \"b\",\n                \"is_output\": True,\n                \"sampling_config\": {\n                    \"temperature\": 0.9,\n                    \"top_p\": 0.9,\n                },\n            },\n        ],\n        \"models\": [\"model1\", \"model2\"],\n        \"model_type\": \"token_id\",\n        \"remove_pure_fill\": True,\n    }\n    chunked_request = ChunkedSemanticCallRequest.parse_from_payload(\n        request_id=0, session_id=0, payload=payload\n    )\n    request_chain = RequestChain.from_chunked_request(chunked_request)\n    print(request_chain.pretty_print())\n\n\ndef test_graph_remove():\n    graph = ComputeGraph()\n\n    request_chain = RequestChain.from_nodes(\n        nodes=[\n            ConstantFill(\"This is a test \"),\n            PlaceholderGen(\n                parameter=SemanticFunctionParameter(name=\"a\", is_output=True)\n            ),\n        ],\n    )\n\n    var_mgr = SemanticVariableManager(666)\n    session_id = 0\n    var_mgr.register_local_var_space(session_id)\n    var_mgr.create_vars_for_semantic_request_chain(session_id, request_chain)\n\n    graph.insert_and_update_request_chain(request_chain)\n\n    # for i, node in enumerate(request_chain.iter()):\n    #     print(i, node)\n\n    graph.remove_completion_chain(request_chain.comp_chains[0])\n\n    print(graph.nodes, graph.chains)\n\n\ndef test_graph_traverse():\n    # A graph of 3 requests\n    # A -> B -> C\n    graph = ComputeGraph()\n\n    var_mgr = SemanticVariableManager(666)\n    session_id = 0\n    var_mgr.register_local_var_space(session_id)\n\n    request1 = RequestChain.from_nodes(\n        nodes=[\n            ConstantFill(\"This is a test \"),\n            PlaceholderGen(\n                parameter=SemanticFunctionParameter(name=\"a\", is_output=True)\n            ),\n        ]\n    )\n\n    var_mgr.create_vars_for_semantic_request_chain(session_id, request1)\n    graph.insert_and_update_request_chain(request1)\n    out_var0 = request1.comp_chains[0].gen_node.sv\n\n    request2 = RequestChain.from_nodes(\n        nodes=[\n            PlaceholderFill(\n                parameter=SemanticFunctionParameter(\n                    name=\"a\", var_id=out_var0.id, is_output=False\n                )\n            ),\n            PlaceholderGen(\n                parameter=SemanticFunctionParameter(name=\"b\", is_output=True)\n            ),\n        ]\n    )\n\n    var_mgr.create_vars_for_semantic_request_chain(session_id, request2)\n    graph.insert_and_update_request_chain(request2)\n    out_var1 = request2.comp_chains[0].gen_node.sv\n\n    request3 = RequestChain.from_nodes(\n        nodes=[\n            PlaceholderFill(\n                parameter=SemanticFunctionParameter(\n                    name=\"b\", var_id=out_var1.id, is_output=False\n                )\n            ),\n            PlaceholderGen(\n                parameter=SemanticFunctionParameter(name=\"c\", is_output=True)\n            ),\n        ]\n    )\n\n    var_mgr.create_vars_for_semantic_request_chain(session_id, request3)\n    graph.insert_and_update_request_chain(request3)\n    out_var2 = request3.comp_chains[0].gen_node.sv\n\n    # view_graph(graph)\n    activate_sv(out_var0, PerformanceCriteria.LATENCY)\n    activate_sv(out_var1, PerformanceCriteria.LATENCY)\n    activate_sv(out_var2, PerformanceCriteria.LATENCY)\n\n    # Expected results: A: depth 2, B: depth 1, C: depth 0\n    requests = [request1, request2, request3]\n    for req in requests:\n        sv = req.comp_chains[0].gen_node.sv\n        assert sv.is_activated\n        assert sv.criteria == PerformanceCriteria.LATENCY\n        print(sv.depth)\n\n\nif __name__ == \"__main__\":\n    # test_request_parse()\n    # test_request_chain_print()\n    # test_chunked_request_to_chain()\n    # test_graph_remove()\n    # test_view_graph()\n    test_graph_traverse()\n"}
{"type": "test_file", "path": "examples/test_dan.py", "content": "# Copyright (c) 2023 by Microsoft Corporation.\n# Author: Chaofan Lin (v-chaofanlin@microsoft.com)\n\nfrom parrot import P\n\nvm = P.VirtualMachine(\n    core_http_addr=\"http://localhost:9000\",\n    mode=\"release\",\n)\n\n\ndan_request = vm.import_function(\"dan\", \"codelib.app.chat\")\n\n\ndef main():\n    question = \"What is the date today?\"\n\n    for _ in range(10):\n        ack, answer = dan_request(question=question)\n        ack_str = ack.get()\n        if \"DAN: I am waiting for a question\" in ack_str:\n            print(\"Verify sucess! ACK=\", ack_str)\n            print(\"The answer: \", answer.get())\n            continue\n        else:\n            print(\"Wrong ACK: \", ack_str)\n            print(\"The answer: \", answer.get())\n            continue\n\n\nvm.run(main)\n"}
{"type": "test_file", "path": "3rdparty/FastChat/tests/launch_openai_api_test_server.py", "content": "\"\"\"\nLaunch an OpenAI API server with multiple model workers.\n\"\"\"\nimport os\n\n\ndef launch_process(cmd):\n    os.popen(cmd)\n\n\nif __name__ == \"__main__\":\n    launch_process(\"python3 -m fastchat.serve.controller\")\n    launch_process(\"python3 -m fastchat.serve.openai_api_server\")\n\n    models = [\n        (\"lmsys/vicuna-7b-v1.5\", \"model_worker\"),\n        (\"lmsys/fastchat-t5-3b-v1.0\", \"model_worker\"),\n        (\"THUDM/chatglm-6b\", \"model_worker\"),\n        (\"mosaicml/mpt-7b-chat\", \"model_worker\"),\n        (\"meta-llama/Llama-2-7b-chat-hf\", \"vllm_worker\"),\n    ]\n\n    for i, (model_path, worker_name) in enumerate(models):\n        cmd = (\n            f\"CUDA_VISIBLE_DEVICES={i} python3 -m fastchat.serve.{worker_name} \"\n            f\"--model-path {model_path} --port {30000+i} \"\n            f\"--worker-address http://localhost:{30000+i} \"\n        )\n        if worker_name == \"vllm_worker\":\n            cmd += \"--tokenizer hf-internal-testing/llama-tokenizer\"\n\n        launch_process(cmd)\n\n    while True:\n        pass\n"}
{"type": "test_file", "path": "3rdparty/vllm/tests/kernels/test_attention.py", "content": "import random\nfrom typing import List, Optional\n\nimport torch\nfrom xformers import ops as xops\nfrom xformers.ops.fmha.attn_bias import BlockDiagonalCausalMask\n\nfrom vllm import attention_ops\n\nMAX_SEQ_LEN = 4096\nTEST_SEED = 0\n\n\ndef ref_masked_attention(\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    scale: float,\n    attn_mask: Optional[torch.Tensor] = None,\n) -> torch.Tensor:\n    query = query * scale\n    attn = torch.einsum('qhd,khd->hqk', query, key)\n    if attn_mask is not None:\n        attn = attn + attn_mask\n    attn = torch.softmax(attn, dim=-1)\n    out = torch.einsum('hqk,khd->qhd', attn, value)\n    return out\n\n\ndef ref_single_query_cached_kv_attention(\n    output: torch.Tensor,\n    query: torch.Tensor,\n    key_cache: torch.Tensor,\n    value_cache: torch.Tensor,\n    block_tables: torch.Tensor,\n    context_lens: torch.Tensor,\n) -> None:\n    num_heads = value_cache.shape[1]\n    head_size = value_cache.shape[2]\n    block_size = value_cache.shape[3]\n\n    num_input_tokens = query.shape[0]\n    for i in range(num_input_tokens):\n        q = query[i].unsqueeze(0)\n        block_table = block_tables[i]\n        context_len = int(context_lens[i])\n\n        keys = []\n        values = []\n        for j in range(context_len):\n            block_number = int(block_table[j // block_size])\n            block_offset = j % block_size\n\n            k = key_cache[block_number, :, :, block_offset, :]\n            k = k.reshape(num_heads, head_size)\n            keys.append(k)\n\n            v = value_cache[block_number, :, :, block_offset]\n            values.append(v)\n        keys = torch.stack(keys, dim=0)\n        values = torch.stack(values, dim=0)\n\n        scale = 1.0 / (head_size**0.5)\n        out = ref_masked_attention(q, keys, values, scale)\n        out = out.view(num_heads, head_size)\n        output[i].copy_(out, non_blocking=True)\n\n\ndef ref_multi_query_kv_attention(\n    cu_seq_lens: List[int],\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    dtype: torch.dtype,\n) -> torch.Tensor:\n    head_size = query.shape[-1]\n    scale = 1.0 / (head_size**0.5)\n\n    num_seqs = len(cu_seq_lens) - 1\n    ref_outputs = []\n    for i in range(num_seqs):\n        start_idx = cu_seq_lens[i]\n        end_idx = cu_seq_lens[i + 1]\n        seq_len = end_idx - start_idx\n\n        # Create attention mask.\n        attn_mask = torch.triu(torch.ones(seq_len, seq_len, dtype=dtype),\n                               diagonal=1)\n        attn_mask = attn_mask * torch.finfo(dtype).min\n        attn_mask = attn_mask.to(dtype=dtype, device='cuda')\n\n        ref_output = ref_masked_attention(\n            query[start_idx:end_idx],\n            key[start_idx:end_idx],\n            value[start_idx:end_idx],\n            scale,\n            attn_mask=attn_mask,\n        )\n        ref_outputs.append(ref_output)\n    ref_output = torch.cat(ref_outputs, dim=0)\n    return ref_output\n\n\ndef ref_multi_query_cached_kv_attention(\n    cu_query_lens: List[int],\n    query: torch.Tensor,\n    key_cache: torch.Tensor,\n    value_cache: torch.Tensor,\n    block_tables: torch.Tensor,\n    context_lens: torch.Tensor,\n    dtype: torch.dtype,\n) -> torch.Tensor:\n    num_heads = value_cache.shape[1]\n    head_size = value_cache.shape[2]\n    block_size = value_cache.shape[3]\n    scale = 1.0 / (head_size**0.5)\n\n    num_queries = len(cu_query_lens) - 1\n    ref_outputs = []\n    for i in range(num_queries):\n        start_idx = cu_query_lens[i]\n        end_idx = cu_query_lens[i + 1]\n        query_len = end_idx - start_idx\n        context_len = int(context_lens[i])\n        block_table = block_tables[i]\n\n        # Create attention mask\n        attn_mask = torch.triu(torch.ones(query_len, context_len),\n                               diagonal=context_len - query_len + 1) * -1e5\n        attn_mask = attn_mask.to(dtype=dtype, device='cuda')\n\n        keys = []\n        values = []\n        for j in range(context_len):\n            block_number = int(block_table[j // block_size])\n            block_offset = j % block_size\n\n            k = key_cache[block_number, :, :, block_offset, :]\n            k = k.reshape(num_heads, head_size)\n            keys.append(k)\n\n            v = value_cache[block_number, :, :, block_offset]\n            values.append(v)\n        keys = torch.stack(keys, dim=0)\n        values = torch.stack(values, dim=0)\n\n        ref_output = ref_masked_attention(\n            query[start_idx:end_idx],\n            keys,\n            values,\n            scale,\n            attn_mask=attn_mask,\n        )\n        ref_outputs.append(ref_output)\n    ref_output = torch.cat(ref_outputs, dim=0)\n    return ref_output\n\n\n@torch.inference_mode()\ndef run_single_query_cached_kv_attention(\n    num_tokens: int,\n    num_heads: int,\n    head_size: int,\n    block_size: int,\n    num_blocks: int,\n    dtype: torch.dtype,\n    num_kv_heads: int = None,\n) -> None:\n    qkv = torch.empty(num_tokens,\n                      3,\n                      num_heads,\n                      head_size,\n                      dtype=dtype,\n                      device='cuda')\n    qkv.uniform_(-1e-3, 1e-3)\n    query, _, _ = qkv.unbind(dim=1)\n\n    x = 16 // torch.tensor([], dtype=dtype).element_size()\n    key_block_shape = (num_heads, head_size // x, block_size, x)\n    key_cache = torch.empty(size=(num_blocks, *key_block_shape),\n                            dtype=dtype,\n                            device='cuda')\n    key_cache.uniform_(-1e-3, 1e-3)\n    value_block_shape = (num_heads, head_size, block_size)\n    value_cache = torch.empty(size=(num_blocks, *value_block_shape),\n                              dtype=dtype,\n                              device='cuda')\n    value_cache.uniform_(-1e-3, 1e-3)\n\n    context_lens = [random.randint(1, MAX_SEQ_LEN) for _ in range(num_tokens)]\n    max_context_len = max(context_lens)\n    context_lens = torch.tensor(context_lens, dtype=torch.int, device='cuda')\n\n    max_num_blocks_per_seq = (max_context_len + block_size - 1) // block_size\n    block_tables = []\n    for _ in range(num_tokens):\n        block_table = [\n            random.randint(0, num_blocks - 1)\n            for _ in range(max_num_blocks_per_seq)\n        ]\n        block_tables.append(block_table)\n    block_tables = torch.tensor(block_tables, dtype=torch.int, device='cuda')\n    head_mapping = torch.arange(num_heads, dtype=torch.int32, device=\"cuda\")\n\n    scale = float(1.0 / (head_size**0.5))\n\n    num_kv_heads = num_heads if num_kv_heads is None else num_kv_heads\n    assert num_heads % num_kv_heads == 0\n    num_queries_per_kv = num_heads // num_kv_heads\n    head_mapping = torch.repeat_interleave(\n        torch.arange(num_kv_heads, dtype=torch.int32, device=\"cuda\"),\n                     num_queries_per_kv)\n\n    output = torch.empty(num_tokens,\n                         num_heads,\n                         head_size,\n                         dtype=dtype,\n                         device='cuda')\n    attention_ops.single_query_cached_kv_attention(\n        output,\n        query,\n        key_cache,\n        value_cache,\n        head_mapping,\n        scale,\n        block_tables,\n        context_lens,\n        block_size,\n        max_context_len,\n        None,  # ALiBi slopes.\n    )\n\n    ref_output = torch.empty_like(query)\n    ref_single_query_cached_kv_attention(\n        ref_output,\n        query,\n        key_cache,\n        value_cache,\n        block_tables,\n        context_lens,\n    )\n    # NOTE(woosuk): Due to the difference in the data types the two\n    # implementations use for attention softmax logits and accumulation,\n    # there is a small difference in the final outputs.\n    # We should use a relaxed tolerance for the test.\n    assert torch.allclose(output, ref_output, atol=1e-3, rtol=1e-5)\n\n\n@torch.inference_mode()\ndef run_multi_query_kv_attention(\n    num_seqs: int,\n    num_heads: int,\n    head_size: int,\n    dtype: torch.dtype,\n) -> None:\n    seq_lens = random.sample(range(1, MAX_SEQ_LEN), num_seqs)\n    num_tokens = sum(seq_lens)\n\n    scale = float(1.0 / (head_size**0.5))\n    qkv = torch.empty(num_tokens,\n                      3,\n                      num_heads,\n                      head_size,\n                      dtype=dtype,\n                      device='cuda')\n    qkv.uniform_(-1e-3, 1e-3)\n    query, key, value = qkv.unbind(dim=1)\n\n    attn_op = xops.fmha.cutlass.FwOp()\n    attn_bias = BlockDiagonalCausalMask.from_seqlens(seq_lens)\n    output = xops.memory_efficient_attention_forward(\n        query.unsqueeze(0),\n        key.unsqueeze(0),\n        value.unsqueeze(0),\n        attn_bias=attn_bias,\n        p=0.0,\n        scale=scale,\n        op=attn_op,\n    )\n    output = output.squeeze(0)\n\n    cu_seq_lens = [0]\n    for seq_len in seq_lens:\n        cu_seq_lens.append(cu_seq_lens[-1] + seq_len)\n    ref_output = ref_multi_query_kv_attention(\n        cu_seq_lens,\n        query,\n        key,\n        value,\n        dtype,\n    )\n    assert torch.allclose(output, ref_output, atol=1e-3, rtol=1e-5)\n\n\ndef test_single_query_cached_kv_attention() -> None:\n    torch.random.manual_seed(TEST_SEED)\n    torch.cuda.manual_seed(TEST_SEED)\n    for dtype in [torch.half, torch.bfloat16, torch.float]:\n        for block_size in [8, 16, 32]:\n            for head_size in [64, 80, 96, 112, 128, 256]:\n                print(f'Testing single_query_cached_kv_attention with '\n                      f'dtype={dtype}, block_size={block_size}, '\n                      f'head_size={head_size}')\n                run_single_query_cached_kv_attention(\n                    num_tokens=37,\n                    num_heads=3,\n                    head_size=head_size,\n                    block_size=block_size,\n                    num_blocks=1024,\n                    dtype=dtype,\n                )\n\n\ndef test_multi_query_kv_attention() -> None:\n    torch.random.manual_seed(TEST_SEED)\n    torch.cuda.manual_seed(TEST_SEED)\n    for dtype in [torch.half, torch.bfloat16, torch.float]:\n        for head_size in [64, 80, 96, 112, 128, 256]:\n            print(f'Testing multi_query_kv_attention with dtype={dtype}, '\n                  f'head_size={head_size}')\n            run_multi_query_kv_attention(\n                num_seqs=5,\n                num_heads=3,\n                head_size=head_size,\n                dtype=dtype,\n            )\n"}
{"type": "test_file", "path": "tests/serve/test_sv.py", "content": "from parrot.serve.graph import (\n    RequestChain,\n    ConstantFill,\n    PlaceholderFill,\n    PlaceholderGen,\n)\nfrom parrot.serve.graph.call_request import (\n    SemanticCallMetadata,\n    SemanticFunctionParameter,\n)\nfrom parrot.serve.variable_manager import SemanticVariableManager\nfrom parrot.sampling_config import SamplingConfig\n\n\ndef test_content_hash():\n    session_id = 0\n    sv_content = \"test\"\n    var_mgr = SemanticVariableManager(constant_prefix_var_timeout=10)\n    var_mgr.register_local_var_space(session_id)\n    var1 = var_mgr._get_local_var_by_content(session_id, sv_content)\n    var2 = var_mgr._get_local_var_by_content(session_id, sv_content)\n    assert var1 == var2\n\n\ndef test_request_chain_hash():\n    var_mgr = SemanticVariableManager(constant_prefix_var_timeout=10)\n\n    request_chain1 = RequestChain.from_nodes(\n        nodes=[\n            ConstantFill(\"Test1\"),\n            PlaceholderFill(\n                parameter=SemanticFunctionParameter(\n                    name=\"a\", is_output=False, value=\"test\"\n                )\n            ),\n            ConstantFill(\"Test2\"),\n            PlaceholderGen(\n                parameter=SemanticFunctionParameter(\n                    name=\"b\", is_output=True, sampling_config=SamplingConfig()\n                )\n            ),\n        ]\n    )\n    request_chain2 = RequestChain.from_nodes(\n        nodes=[\n            ConstantFill(\"Test1\"),\n            PlaceholderFill(\n                parameter=SemanticFunctionParameter(\n                    name=\"a\", is_output=False, value=\"test\"\n                )\n            ),\n            ConstantFill(\"Test2\"),\n            PlaceholderGen(\n                parameter=SemanticFunctionParameter(\n                    name=\"b\", is_output=True, sampling_config=SamplingConfig()\n                )\n            ),\n        ]\n    )\n\n    session_id = 0\n    var_mgr.register_local_var_space(session_id)\n    var_mgr.create_vars_for_semantic_request_chain(session_id, request_chain1)\n    var_mgr.create_vars_for_semantic_request_chain(session_id, request_chain2)\n\n    # Check the first chain\n    print(request_chain1.pretty_print())\n\n    # Check the second chain\n    print(request_chain2.pretty_print())\n\n\nif __name__ == \"__main__\":\n    # test_content_hash()\n    test_request_chain_hash()\n"}
{"type": "test_file", "path": "tests/engine/builtin/test_llama.py", "content": "from parrot.engine.config import BuiltinConfig\nfrom parrot.utils import set_random_seed\nfrom parrot.testing.model_runner_test_template import *\n\n\ndef test_llama_xformers_with_buffer():\n    set_random_seed(0)\n\n    model_name = \"lmsys/vicuna-7b-v1.3\"\n    builtin_config = BuiltinConfig(\n        num_kv_cache_blocks=16000,\n        attn_func=\"xformers_with_buffer\",\n    )\n\n    template_test_single_fill(model_name, builtin_config)\n    template_test_batch_fills(model_name, builtin_config)\n    template_test_fill_then_gen(model_name, builtin_config)\n    template_test_generate_single_text(model_name, builtin_config)\n    template_test_generate_batch_text(model_name, builtin_config)\n    template_test_fill_generate_mixed(model_name, builtin_config)\n\n\ndef test_llama_vllm():\n    set_random_seed(0)\n\n    model_name = \"lmsys/vicuna-7b-v1.3\"\n    builtin_config = BuiltinConfig(\n        num_kv_cache_blocks=1024,\n        block_size=16,\n        attn_func=\"xformers_fill_vllm_paged_attention_generate\",\n    )\n\n    # template_test_single_fill(model_name, builtin_config)\n    # template_test_batch_fills(model_name, builtin_config)\n    # template_test_fill_then_gen(model_name, builtin_config)\n    template_test_generate_single_text(model_name, builtin_config)\n    # template_test_generate_batch_text(model_name, builtin_config)\n    # template_test_fill_generate_mixed(model_name, builtin_config)\n\n\nif __name__ == \"__main__\":\n    # test_llama_xformers_with_buffer()\n    test_llama_vllm()\n"}
{"type": "test_file", "path": "3rdparty/vllm/tests/kernels/test_activation.py", "content": "import torch\nimport torch.nn.functional as F\nfrom transformers.activations import get_activation\nfrom vllm import activation_ops\n\n\ndef ref_silu_and_mul(x: torch.Tensor) -> torch.Tensor:\n    x1, x2 = x.chunk(chunks=2, dim=1)\n    return F.silu(x1) * x2\n\n\n@torch.inference_mode()\ndef run_silu_and_mul(\n    num_tokens: int,\n    d: int,\n    dtype: torch.dtype,\n) -> None:\n    x = torch.randn(num_tokens, 2 * d, dtype=dtype, device='cuda')\n    out = torch.empty(num_tokens, d, dtype=dtype, device='cuda')\n    activation_ops.silu_and_mul(out, x)\n    ref_out = ref_silu_and_mul(x)\n    assert torch.allclose(out, ref_out, atol=1e-5, rtol=1e-5)\n\n\ndef test_silu_and_mul() -> None:\n    for dtype in [torch.half, torch.bfloat16, torch.float]:\n        for num_tokens in [7, 83, 2048]:\n            for d in [512, 4096, 5120, 13824]:\n                print(f'Testing dtype={dtype}, num_tokens={num_tokens}, d={d}')\n                run_silu_and_mul(num_tokens, d, dtype)\n\n\n@torch.inference_mode()\ndef run_gelu_new(\n    num_tokens: int,\n    d: int,\n    dtype: torch.dtype,\n) -> None:\n    x = torch.randn(num_tokens, d, dtype=dtype, device='cuda')\n    out = torch.empty(num_tokens, d, dtype=dtype, device='cuda')\n    activation_ops.gelu_new(out, x)\n    ref_out = get_activation(\"gelu_new\")(x)\n    assert torch.allclose(out, ref_out, atol=1e-5, rtol=1e-5)\n\n\ndef test_gelu_new() -> None:\n    for dtype in [torch.half, torch.bfloat16, torch.float]:\n        for num_tokens in [7, 83, 2048]:\n            for d in [512, 4096, 5120, 13824]:\n                print(f'Testing dtype={dtype}, num_tokens={num_tokens}, d={d}')\n                run_gelu_new(num_tokens, d, dtype)\n\n\n@torch.inference_mode()\ndef run_gelu_fast(\n    num_tokens: int,\n    d: int,\n    dtype: torch.dtype,\n) -> None:\n    x = torch.randn(num_tokens, d, dtype=dtype, device='cuda')\n    out = torch.empty(num_tokens, d, dtype=dtype, device='cuda')\n    activation_ops.gelu_fast(out, x)\n    ref_out = get_activation(\"gelu_fast\")(x)\n    assert torch.allclose(out, ref_out, atol=1e-5, rtol=1e-5)\n\n\ndef test_gelu_fast() -> None:\n    for dtype in [torch.half, torch.bfloat16, torch.float]:\n        for num_tokens in [7, 83, 2048]:\n            for d in [512, 4096, 5120, 13824]:\n                print(f'Testing dtype={dtype}, num_tokens={num_tokens}, d={d}')\n                run_gelu_fast(num_tokens, d, dtype)\n"}
{"type": "test_file", "path": "tests/test_utils.py", "content": "from parrot.utils import (\n    RecyclePool,\n    bytes_to_encoded_b64str,\n    encoded_b64str_to_bytes,\n)\n\n\ndef test_recycle_pool():\n    pool = RecyclePool()\n    for i in range(4):\n        assert pool.allocate() in [0, 1, 2, 3]\n\n    for i in range(32):\n        pool.free(i % 4)\n        assert pool.allocate() in [0, 1, 2, 3]\n\n    for i in range(4):\n        pool.free(i)\n\n\ndef test_recycle_pool_error():\n    pool = RecyclePool()\n    pool.allocate()\n\n    try:\n        pool.allocate()\n    except ValueError:\n        pass\n\n    pool.free(0)\n    try:\n        pool.free(0)\n    except ValueError:\n        pass\n\n\ndef test_serialize_tools():\n    data = b\"hello world\"\n    encoded = bytes_to_encoded_b64str(data)\n    decoded = encoded_b64str_to_bytes(encoded)\n    assert data == decoded\n\n\nif __name__ == \"__main__\":\n    test_recycle_pool()\n    test_recycle_pool_error()\n    test_serialize_tools()\n"}
{"type": "test_file", "path": "tests/pfunc/test_semantic_function.py", "content": "import pytest\n\nfrom parrot import P\n\nfrom parrot.frontend.pfunc.function import Constant, ParameterLoc, SemanticCall\n\n\ndef test_parse_semantic_function():\n    @P.semantic_function()\n    def tell_me_a_joke(\n        topic: P.Input,\n        keyword: P.Input,\n        joke: P.Output,\n        explanation: P.Output,\n    ):\n        \"\"\"Tell me a joke about {{topic}}. The joke must contains the following\n        keywords: {{keyword}}. The following is the joke: {{joke}}. And giving a\n        short explanation to show that why it is funny. The following is the explanation\n        for the joke above: {{explanation}}.\"\"\"\n\n    expected_body = [\n        Constant,\n        ParameterLoc,\n        Constant,\n        ParameterLoc,\n        Constant,\n        ParameterLoc,\n        Constant,\n        ParameterLoc,\n    ]\n    expected_var_is_output = [\n        False,\n        False,\n        True,\n        True,\n    ]\n    assert len(expected_body) == len(tell_me_a_joke.body)\n    j = 0\n    for i, piece in enumerate(tell_me_a_joke.body):\n        assert isinstance(piece, expected_body[i])\n        if isinstance(piece, ParameterLoc):\n            assert piece.param.is_output == expected_var_is_output[j]\n            j += 1\n\n\ndef test_call_function():\n    @P.semantic_function()\n    def test(a: P.Input, b: P.Input, c: P.Output):\n        \"\"\"This {{b}} is a test {{a}} function {{c}}\"\"\"\n\n    print(test.body)\n\n    print(test(\"a\", b=\"b\"))\n\n\ndef test_call_to_payload():\n    @P.semantic_function()\n    def test(a: P.Input, b: P.Input, c: P.Output):\n        \"\"\"This {{b}} is a test {{a}} function {{c}}\"\"\"\n\n    call: SemanticCall = test(\"a\", b=\"b\")\n    print(call.to_request_payload())\n\n\ndef test_call_function_with_pyobjects():\n    @P.semantic_function()\n    def test(a: float, b: int, c: list, d: P.Output):\n        \"\"\"This {{b}} is a test {{a}} function {{c}} and {{d}}\"\"\"\n\n    print(test.body)\n\n    print(test(23.3, 400, [1, 2, 3, 4]))\n\n\ndef test_wrongly_pass_output_argument():\n    # NOTE: output argument can only be passed by name\n\n    @P.semantic_function()\n    def test(a: P.Input, b: P.Input, c: P.Output):\n        \"\"\"This {{b}} is a test {{a}} function {{c}}\"\"\"\n\n    with pytest.raises(ValueError):\n        test(\"a\", \"b\", \"c\")\n\n\nif __name__ == \"__main__\":\n    # test_parse_semantic_function()\n    # test_call_function()\n    test_call_to_payload()\n    # test_call_function_with_pyobjects()\n    # test_wrongly_pass_output_argument()\n"}
{"type": "test_file", "path": "tests/serve/test_session.py", "content": "import time\nimport pytest\nimport asyncio\n\nfrom parrot.exceptions import ParrotCoreUserError\n\nfrom parrot.frontend.pfunc import native_function, Output\n\nfrom parrot.serve.session_manager import SessionManager\nfrom parrot.serve.scheduler import TaskCreator, GlobalScheduler, GlobalSchedulerConfig\nfrom parrot.serve.prefix_matcher import PrefixMatcher\nfrom parrot.serve.variable_manager import SemanticVariableManager\nfrom parrot.serve.tokenizer_wrapper import TokenizersWrapper\nfrom parrot.serve.context_manager import ServeCoreContextManager\nfrom parrot.serve.engine_manager import EngineManager\nfrom parrot.serve.session.graph_executor import GraphExecutor\nfrom parrot.serve.session.native_executor import PyNativeExecutor\nfrom parrot.serve.backend_repr import ExecutionEngine\n\nfrom parrot.testing.localhost_server_daemon import fake_engine_server\nfrom parrot.testing.fake_engine_server import engine_config\n\nfrom parrot.serve.graph import (\n    RequestChain,\n    PyNativeCallRequest,\n    NativeFuncNode,\n    ConstantFill,\n    PlaceholderFill,\n    PlaceholderGen,\n    PerformanceCriteria,\n    activate_sv,\n)\nfrom parrot.serve.graph.call_request import SemanticFunctionParameter\n\n\ndef test_session_manager():\n    scheduler_config = GlobalSchedulerConfig()\n    prefix_matcher = PrefixMatcher()\n    var_mgr = SemanticVariableManager(666)\n    tokenizers_wrapper = TokenizersWrapper()\n    context_mgr = ServeCoreContextManager()\n    engine_mgr = EngineManager(\n        tokenizers_wrapper=tokenizers_wrapper,\n        context_mgr=context_mgr,\n        engine_heartbeat_timeout=666,\n    )\n    task_creator = TaskCreator()\n    scheduler = GlobalScheduler(scheduler_config, engine_mgr, context_mgr)\n\n    session_mgr = SessionManager(\n        life_span=10,\n        prefix_matcher=prefix_matcher,\n        task_creator=task_creator,\n        scheduler=scheduler,\n        var_mgr=var_mgr,\n        engine_mgr=engine_mgr,\n        context_mgr=context_mgr,\n        tokenizers_wrapper=tokenizers_wrapper,\n    )\n\n    # Test session registration\n    session_id = session_mgr.register_session()\n\n    session = session_mgr.get_session(session_id)\n    assert session.session_id == session_id\n\n    # Test session expiration\n    time.sleep(11)\n    session_mgr.check_running_sessions()\n\n    with pytest.raises(ParrotCoreUserError):\n        session_mgr.check_session_status(session_id)\n\n\ndef test_graph_executor():\n    session_id = 0\n\n    task_creator = TaskCreator()\n    scheduler_config = GlobalSchedulerConfig()\n    var_mgr = SemanticVariableManager(666)\n    tokenizers_wrapper = TokenizersWrapper()\n    context_mgr = ServeCoreContextManager()\n    engine_mgr = EngineManager(\n        tokenizers_wrapper=tokenizers_wrapper,\n        context_mgr=context_mgr,\n        engine_heartbeat_timeout=666,\n    )\n    task_creator = TaskCreator()\n    scheduler = GlobalScheduler(scheduler_config, engine_mgr, context_mgr)\n    executor = GraphExecutor(\n        session_id=session_id,\n        task_creator=task_creator,\n        scheduler=scheduler,\n        engine_mgr=engine_mgr,\n        context_mgr=context_mgr,\n        tokenizers_wrapper=tokenizers_wrapper,\n    )\n\n    var_mgr.register_local_var_space(session_id)\n    in_var = var_mgr.create_var(session_id, \"in_var\")\n\n    request = RequestChain.from_nodes(\n        nodes=[\n            ConstantFill(\"Hello world, I'm a prefix.\"),\n            PlaceholderFill(\n                parameter=SemanticFunctionParameter(\n                    name=\"a\", var_id=in_var.id, is_output=False\n                )\n            ),\n            PlaceholderGen(\n                parameter=SemanticFunctionParameter(name=\"b\", is_output=True)\n            ),\n        ]\n    )\n\n    var_mgr.create_vars_for_semantic_request_chain(session_id, request)\n\n    engine_mgr.register_engine(engine_config)\n\n    async def main():\n        executor.add_request(request)\n        activate_sv(request.comp_chains[0].gen_node.sv, PerformanceCriteria.LATENCY)\n        await asyncio.sleep(1)\n        in_var.set(\"This is a test value.\")\n        await asyncio.sleep(0.1)\n        scheduler.schedule()\n        await asyncio.sleep(5)\n\n    with fake_engine_server():\n        asyncio.run(main())\n\n\ndef test_native_executor():\n    session_id = 0\n\n    task_creator = TaskCreator()\n    scheduler_config = GlobalSchedulerConfig()\n    var_mgr = SemanticVariableManager(666)\n    tokenizers_wrapper = TokenizersWrapper()\n    context_mgr = ServeCoreContextManager()\n    engine_mgr = EngineManager(\n        tokenizers_wrapper=tokenizers_wrapper,\n        context_mgr=context_mgr,\n        engine_heartbeat_timeout=666,\n    )\n    task_creator = TaskCreator()\n    scheduler = GlobalScheduler(scheduler_config, engine_mgr, context_mgr)\n    executor = GraphExecutor(\n        session_id=session_id,\n        task_creator=task_creator,\n        scheduler=scheduler,\n        engine_mgr=engine_mgr,\n        context_mgr=context_mgr,\n        tokenizers_wrapper=tokenizers_wrapper,\n    )\n    native_executor = PyNativeExecutor(\n        session_id=session_id,\n        graph=executor.graph,\n    )\n\n    var_mgr.register_local_var_space(session_id)\n\n    @native_function()\n    def test_native_func(a: str, b: str, c: Output):\n        c.set(a + b)\n\n    payload = test_native_func(\"Hello\", \"World\").to_request_payload()\n\n    print(payload)\n\n    native_request = PyNativeCallRequest.parse_from_payload(\n        request_id=0, session_id=session_id, payload=payload\n    )\n    func_node = NativeFuncNode(native_request)\n\n    var_mgr.create_vars_for_pynative_func(session_id, func_node)\n\n    print(func_node.input_values, func_node.output_vars)\n\n    out_var = func_node.output_vars[\"c\"]\n\n    async def main():\n        native_executor.add_native_func(func_node)\n        activate_sv(out_var, PerformanceCriteria.LATENCY)\n        await asyncio.sleep(1)\n        print(out_var.get())\n\n    asyncio.run(main())\n\n\nif __name__ == \"__main__\":\n    test_session_manager()\n    test_graph_executor()\n    test_native_executor()\n"}
{"type": "test_file", "path": "tests/engine/test_engine.py", "content": "import asyncio\nimport time\nfrom transformers import AutoTokenizer\n\nfrom parrot.engine.engine_creator import create_engine\nfrom parrot.engine.primitive_job import Fill, Generate\nfrom parrot.sampling_config import SamplingConfig\nfrom parrot.utils import create_task_in_loop\nfrom parrot.testing.get_configs import get_sample_engine_config_path\n\nimport torch\n\n\ndef _test_single_engine_simple_serving(config):\n    engine_type, config_fn = config\n\n    engine = create_engine(\n        engine_config_path=get_sample_engine_config_path(config_fn),\n        connect_to_core=False,\n    )\n\n    prompt_text = \"Hello, my name is\"\n    tokenizer_name = engine.engine_config.tokenizer\n    if tokenizer_name == \"unknown\":\n        fill_job = Fill(\n            session_id=0,\n            task_id=0,\n            context_id=0,\n            parent_context_id=-1,\n            text=prompt_text,\n        )\n    else:\n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n        prompt_tokens = tokenizer(prompt_text)[\"input_ids\"]\n        fill_job = Fill(\n            session_id=0,\n            task_id=0,\n            context_id=0,\n            parent_context_id=-1,\n            token_ids=prompt_tokens,\n        )\n\n    gen_job = Generate(\n        session_id=0,\n        task_id=0,\n        context_id=0,\n        parent_context_id=-1,\n        sampling_config=SamplingConfig(\n            max_gen_length=40,\n            ignore_tokenizer_eos=True,\n        ),\n    )\n\n    async def execute_job(job):\n        engine._add_job(job)\n        await job.finish_event.wait()\n\n    if engine_type == \"builtin\":\n\n        async def main():\n            create_task_in_loop(engine.engine_loop())\n            await execute_job(fill_job)\n            await execute_job(gen_job)\n            print(tokenizer.decode(gen_job.context.token_ids))\n\n    elif engine_type == \"openai\":\n\n        async def main():\n            create_task_in_loop(engine.engine_loop())\n            await execute_job(fill_job)\n            await execute_job(gen_job)\n            print(gen_job.context.get_latest_context_text())\n\n    try:\n        asyncio.run(main(), debug=True)\n    except BaseException as e:\n        print(\"Internal error happends:\", e)\n\n    del engine\n    torch.cuda.empty_cache()\n\n\nTEST_CONFIGS_LIST = [\n    # (\"builtin\", \"opt-125m.json\"),\n    (\"builtin\", \"vicuna-7b-v1.3.json\"),\n    # (\"openai\", \"azure-openai-gpt-3.5-turbo.json\"),\n]\n\n\ndef test_engine_simple_serving():\n    for config in TEST_CONFIGS_LIST:\n        print(\"TESTING: \", config)\n        _test_single_engine_simple_serving(config)\n        time.sleep(1.0)\n\n\nif __name__ == \"__main__\":\n    test_engine_simple_serving()\n"}
{"type": "test_file", "path": "tests/serve/test_prefix_matcher.py", "content": "from parrot.serve.prefix_matcher import PrefixMatcher\n\n\ndef test_prefix_matcher():\n    prefix_matcher = PrefixMatcher()\n\n    # Will not add\n    prefix_matcher.add_prefix(\"This is a test\")\n\n    # Will add\n    for i in range(PrefixMatcher._GP_THRESHOLD + 1):\n        prefix_matcher.add_prefix(\"A\" * PrefixMatcher._START_LEN + \"BBB\" + str(i))\n\n    print(prefix_matcher._prefix_counter)\n\n    query_str = \"A\" * PrefixMatcher._START_LEN + \"BBB\" + \"XXX\"\n    pos = prefix_matcher.query_prefix(query_str)\n    assert pos != -1\n    print(\"prefix: \" + query_str[:pos], \"suffix: \" + query_str[pos:])\n\n\nif __name__ == \"__main__\":\n    test_prefix_matcher()\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/llm_judge/gen_model_answer.py", "content": "\"\"\"Generate answers with local models.\n\nUsage:\npython3 gen_model_answer.py --model-path lmsys/fastchat-t5-3b-v1.0 --model-id fastchat-t5-3b-v1.0\n\"\"\"\nimport argparse\nimport json\nimport os\nimport random\nimport time\n\nimport shortuuid\nimport torch\nfrom tqdm import tqdm\n\nfrom fastchat.llm_judge.common import load_questions, temperature_config\nfrom fastchat.model import load_model, get_conversation_template\nfrom fastchat.utils import str_to_torch_dtype\n\n\ndef run_eval(\n    model_path,\n    model_id,\n    question_file,\n    question_begin,\n    question_end,\n    answer_file,\n    max_new_token,\n    num_choices,\n    num_gpus_per_model,\n    num_gpus_total,\n    max_gpu_memory,\n    dtype,\n):\n    questions = load_questions(question_file, question_begin, question_end)\n    # random shuffle the questions to balance the loading\n    random.shuffle(questions)\n\n    # Split the question file into `num_gpus` files\n    assert num_gpus_total % num_gpus_per_model == 0\n    use_ray = num_gpus_total // num_gpus_per_model > 1\n\n    if use_ray:\n        get_answers_func = ray.remote(num_gpus=num_gpus_per_model)(\n            get_model_answers\n        ).remote\n    else:\n        get_answers_func = get_model_answers\n\n    chunk_size = len(questions) // (num_gpus_total // num_gpus_per_model)\n    ans_handles = []\n    for i in range(0, len(questions), chunk_size):\n        ans_handles.append(\n            get_answers_func(\n                model_path,\n                model_id,\n                questions[i : i + chunk_size],\n                answer_file,\n                max_new_token,\n                num_choices,\n                num_gpus_per_model,\n                max_gpu_memory,\n                dtype=dtype,\n            )\n        )\n\n    if use_ray:\n        ray.get(ans_handles)\n\n\n@torch.inference_mode()\ndef get_model_answers(\n    model_path,\n    model_id,\n    questions,\n    answer_file,\n    max_new_token,\n    num_choices,\n    num_gpus_per_model,\n    max_gpu_memory,\n    dtype,\n):\n    model, tokenizer = load_model(\n        model_path,\n        device=\"cuda\",\n        num_gpus=num_gpus_per_model,\n        max_gpu_memory=max_gpu_memory,\n        dtype=dtype,\n        load_8bit=False,\n        cpu_offloading=False,\n        debug=False,\n    )\n\n    for question in tqdm(questions):\n        if question[\"category\"] in temperature_config:\n            temperature = temperature_config[question[\"category\"]]\n        else:\n            temperature = 0.7\n\n        choices = []\n        for i in range(num_choices):\n            torch.manual_seed(i)\n            conv = get_conversation_template(model_id)\n            turns = []\n            for j in range(len(question[\"turns\"])):\n                qs = question[\"turns\"][j]\n                conv.append_message(conv.roles[0], qs)\n                conv.append_message(conv.roles[1], None)\n                prompt = conv.get_prompt()\n                input_ids = tokenizer([prompt]).input_ids\n\n                if temperature < 1e-4:\n                    do_sample = False\n                else:\n                    do_sample = True\n\n                # some models may error out when generating long outputs\n                try:\n                    output_ids = model.generate(\n                        torch.as_tensor(input_ids).cuda(),\n                        do_sample=do_sample,\n                        temperature=temperature,\n                        max_new_tokens=max_new_token,\n                    )\n                    if model.config.is_encoder_decoder:\n                        output_ids = output_ids[0]\n                    else:\n                        output_ids = output_ids[0][len(input_ids[0]) :]\n\n                    # be consistent with the template's stop_token_ids\n                    if conv.stop_token_ids:\n                        stop_token_ids_index = [\n                            i\n                            for i, id in enumerate(output_ids)\n                            if id in conv.stop_token_ids\n                        ]\n                        if len(stop_token_ids_index) > 0:\n                            output_ids = output_ids[: stop_token_ids_index[0]]\n\n                    output = tokenizer.decode(\n                        output_ids,\n                        spaces_between_special_tokens=False,\n                    )\n                    if conv.stop_str and isinstance(conv.stop_str, list):\n                        stop_str_indices = sorted(\n                            [\n                                output.find(stop_str)\n                                for stop_str in conv.stop_str\n                                if output.find(stop_str) > 0\n                            ]\n                        )\n                        if len(stop_str_indices) > 0:\n                            output = output[: stop_str_indices[0]]\n                    elif conv.stop_str and output.find(conv.stop_str) > 0:\n                        output = output[: output.find(conv.stop_str)]\n\n                    for special_token in tokenizer.special_tokens_map.values():\n                        if isinstance(special_token, list):\n                            for special_tok in special_token:\n                                output = output.replace(special_tok, \"\")\n                        else:\n                            output = output.replace(special_token, \"\")\n                    output = output.strip()\n\n                    if conv.name == \"xgen\" and output.startswith(\"Assistant:\"):\n                        output = output.replace(\"Assistant:\", \"\", 1).strip()\n                except RuntimeError as e:\n                    print(\"ERROR question ID: \", question[\"question_id\"])\n                    output = \"ERROR\"\n\n                conv.update_last_message(output)\n                turns.append(output)\n\n            choices.append({\"index\": i, \"turns\": turns})\n\n        # Dump answers\n        os.makedirs(os.path.dirname(answer_file), exist_ok=True)\n        with open(os.path.expanduser(answer_file), \"a\") as fout:\n            ans_json = {\n                \"question_id\": question[\"question_id\"],\n                \"answer_id\": shortuuid.uuid(),\n                \"model_id\": model_id,\n                \"choices\": choices,\n                \"tstamp\": time.time(),\n            }\n            fout.write(json.dumps(ans_json) + \"\\n\")\n\n\ndef reorg_answer_file(answer_file):\n    \"\"\"Sort by question id and de-duplication\"\"\"\n    answers = {}\n    with open(answer_file, \"r\") as fin:\n        for l in fin:\n            qid = json.loads(l)[\"question_id\"]\n            answers[qid] = l\n\n    qids = sorted(list(answers.keys()))\n    with open(answer_file, \"w\") as fout:\n        for qid in qids:\n            fout.write(answers[qid])\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model-path\",\n        type=str,\n        required=True,\n        help=\"The path to the weights. This can be a local folder or a Hugging Face repo ID.\",\n    )\n    parser.add_argument(\n        \"--model-id\", type=str, required=True, help=\"A custom name for the model.\"\n    )\n    parser.add_argument(\n        \"--bench-name\",\n        type=str,\n        default=\"mt_bench\",\n        help=\"The name of the benchmark question set.\",\n    )\n    parser.add_argument(\n        \"--question-begin\",\n        type=int,\n        help=\"A debug option. The begin index of questions.\",\n    )\n    parser.add_argument(\n        \"--question-end\", type=int, help=\"A debug option. The end index of questions.\"\n    )\n    parser.add_argument(\"--answer-file\", type=str, help=\"The output answer file.\")\n    parser.add_argument(\n        \"--max-new-token\",\n        type=int,\n        default=1024,\n        help=\"The maximum number of new generated tokens.\",\n    )\n    parser.add_argument(\n        \"--num-choices\",\n        type=int,\n        default=1,\n        help=\"How many completion choices to generate.\",\n    )\n    parser.add_argument(\n        \"--num-gpus-per-model\",\n        type=int,\n        default=1,\n        help=\"The number of GPUs per model.\",\n    )\n    parser.add_argument(\n        \"--num-gpus-total\", type=int, default=1, help=\"The total number of GPUs.\"\n    )\n    parser.add_argument(\n        \"--max-gpu-memory\",\n        type=str,\n        help=\"Maxmum GPU memory used for model weights per GPU.\",\n    )\n    parser.add_argument(\n        \"--dtype\",\n        type=str,\n        choices=[\"float32\", \"float16\", \"bfloat16\"],\n        help=\"Override the default dtype. If not set, it will use float16 on GPU and float32 on CPU.\",\n        default=None,\n    )\n\n    args = parser.parse_args()\n\n    if args.num_gpus_total // args.num_gpus_per_model > 1:\n        import ray\n\n        ray.init()\n\n    question_file = f\"data/{args.bench_name}/question.jsonl\"\n    if args.answer_file:\n        answer_file = args.answer_file\n    else:\n        answer_file = f\"data/{args.bench_name}/model_answer/{args.model_id}.jsonl\"\n\n    print(f\"Output to {answer_file}\")\n\n    run_eval(\n        model_path=args.model_path,\n        model_id=args.model_id,\n        question_file=question_file,\n        question_begin=args.question_begin,\n        question_end=args.question_end,\n        answer_file=answer_file,\n        max_new_token=args.max_new_token,\n        num_choices=args.num_choices,\n        num_gpus_per_model=args.num_gpus_per_model,\n        num_gpus_total=args.num_gpus_total,\n        max_gpu_memory=args.max_gpu_memory,\n        dtype=str_to_torch_dtype(args.dtype),\n    )\n\n    reorg_answer_file(answer_file)\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/llm_judge/gen_judgment.py", "content": "\"\"\"\nUsage:\npython gen_judgment.py --model-list [LIST-OF-MODEL-ID] --parallel [num-concurrent-api-call] --mode [single|pairwise-baseline|pairwise-all]\n\"\"\"\nimport argparse\nfrom concurrent.futures import ThreadPoolExecutor\nimport json\n\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom fastchat.llm_judge.common import (\n    load_questions,\n    load_model_answers,\n    load_judge_prompts,\n    check_data,\n    play_a_match_pair,\n    play_a_match_single,\n    get_model_list,\n    Judge,\n    MatchPair,\n    MatchSingle,\n    NEED_REF_CATS,\n)\n\n\ndef make_match(\n    questions,\n    models,\n    model_answers,\n    judge,\n    baseline_model,\n    ref_answers=None,\n    multi_turn=False,\n):\n    matches = []\n    for q in questions:\n        if multi_turn and len(q[\"turns\"]) != 2:\n            continue\n        for i in range(len(models)):\n            q_id = q[\"question_id\"]\n            m_1 = models[i]\n            m_2 = baseline_model\n            if m_1 == m_2:\n                continue\n            a_1 = model_answers[m_1][q_id]\n            a_2 = model_answers[baseline_model][q_id]\n            if ref_answers is not None:\n                ref = ref_answers[judge.model_name][q_id]\n                match = MatchPair(\n                    dict(q),\n                    m_1,\n                    m_2,\n                    a_1,\n                    a_2,\n                    judge,\n                    ref_answer=ref,\n                    multi_turn=multi_turn,\n                )\n            else:\n                match = MatchPair(\n                    dict(q), m_1, m_2, a_1, a_2, judge, multi_turn=multi_turn\n                )\n            matches.append(match)\n    return matches\n\n\ndef make_match_all_pairs(\n    questions,\n    models,\n    model_answers,\n    judge,\n    baseline_model=None,\n    ref_answers=None,\n    multi_turn=False,\n):\n    matches = []\n    for q in questions:\n        if multi_turn and len(q[\"turns\"]) != 2:\n            continue\n        for i in range(len(models)):\n            for j in range(i + 1, len(models)):\n                q_id = q[\"question_id\"]\n                m_1 = models[i]\n                m_2 = models[j]\n                a_1 = model_answers[m_1][q_id]\n                a_2 = model_answers[m_2][q_id]\n                if ref_answers is not None:\n                    ref = ref_answers[judge.model_name][q_id]\n                    match = MatchPair(\n                        dict(q),\n                        m_1,\n                        m_2,\n                        a_1,\n                        a_2,\n                        judge,\n                        ref_answer=ref,\n                        multi_turn=multi_turn,\n                    )\n                else:\n                    match = MatchPair(\n                        dict(q), m_1, m_2, a_1, a_2, judge, multi_turn=multi_turn\n                    )\n                matches.append(match)\n    return matches\n\n\ndef make_match_single(\n    questions,\n    models,\n    model_answers,\n    judge,\n    baseline_model=None,\n    ref_answers=None,\n    multi_turn=False,\n):\n    matches = []\n    for q in questions:\n        if multi_turn and len(q[\"turns\"]) != 2:\n            continue\n        for i in range(len(models)):\n            q_id = q[\"question_id\"]\n            m = models[i]\n            a = model_answers[m][q_id]\n            if ref_answers is not None:\n                ref = ref_answers[judge.model_name][q_id]\n                matches.append(\n                    MatchSingle(\n                        dict(q), m, a, judge, ref_answer=ref, multi_turn=multi_turn\n                    )\n                )\n            else:\n                matches.append(MatchSingle(dict(q), m, a, judge, multi_turn=multi_turn))\n    return matches\n\n\ndef make_judge_pairwise(judge_model, judge_prompts):\n    judges = {}\n    judges[\"default\"] = Judge(judge_model, judge_prompts[\"pair-v2\"])\n    judges[\"math\"] = Judge(judge_model, judge_prompts[\"pair-math-v1\"], ref_based=True)\n    judges[\"default-mt\"] = Judge(\n        judge_model, judge_prompts[\"pair-v2-multi-turn\"], multi_turn=True\n    )\n    judges[\"math-mt\"] = Judge(\n        judge_model,\n        judge_prompts[\"pair-math-v1-multi-turn\"],\n        ref_based=True,\n        multi_turn=True,\n    )\n    return judges\n\n\ndef make_judge_single(judge_model, judge_prompts):\n    judges = {}\n    judges[\"default\"] = Judge(judge_model, judge_prompts[\"single-v1\"])\n    judges[\"math\"] = Judge(judge_model, judge_prompts[\"single-math-v1\"], ref_based=True)\n    judges[\"default-mt\"] = Judge(\n        judge_model, judge_prompts[\"single-v1-multi-turn\"], multi_turn=True\n    )\n    judges[\"math-mt\"] = Judge(\n        judge_model,\n        judge_prompts[\"single-math-v1-multi-turn\"],\n        ref_based=True,\n        multi_turn=True,\n    )\n    return judges\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--bench-name\",\n        type=str,\n        default=\"mt_bench\",\n        help=\"The name of the benchmark question set.\",\n    )\n    parser.add_argument(\n        \"--judge-file\",\n        type=str,\n        default=\"data/judge_prompts.jsonl\",\n        help=\"The file of judge prompts.\",\n    )\n    parser.add_argument(\"--judge-model\", type=str, default=\"gpt-4\")\n    parser.add_argument(\"--baseline-model\", type=str, default=\"gpt-3.5-turbo\")\n    parser.add_argument(\n        \"--mode\",\n        type=str,\n        default=\"single\",\n        choices=[\"pairwise-baseline\", \"pairwise-all\", \"single\"],\n        help=(\n            \"Evaluation mode. \"\n            \"`pairwise-baseline` runs pairwise comparision against a baseline. \"\n            \"`pairwise-all` runs pairwise comparision between all pairs. \"\n            \"`single` runs single answer grading.\"\n        ),\n    )\n    parser.add_argument(\n        \"--model-list\",\n        type=str,\n        nargs=\"+\",\n        default=None,\n        help=\"A list of models to be evaluated\",\n    )\n    parser.add_argument(\n        \"--parallel\", type=int, default=1, help=\"The number of concurrent API calls.\"\n    )\n    parser.add_argument(\n        \"--first-n\", type=int, help=\"A debug option. Only run the first `n` judgments.\"\n    )\n    args = parser.parse_args()\n\n    question_file = f\"data/{args.bench_name}/question.jsonl\"\n    answer_dir = f\"data/{args.bench_name}/model_answer\"\n    ref_answer_dir = f\"data/{args.bench_name}/reference_answer\"\n\n    # Load questions\n    questions = load_questions(question_file, None, None)\n\n    # Load answers\n    model_answers = load_model_answers(answer_dir)\n    ref_answers = load_model_answers(ref_answer_dir)\n\n    # Load judge\n    judge_prompts = load_judge_prompts(args.judge_file)\n\n    if args.first_n:\n        questions = questions[: args.first_n]\n\n    if args.model_list is None:\n        models = get_model_list(answer_dir)\n    else:\n        models = args.model_list\n\n    if args.mode == \"single\":\n        judges = make_judge_single(args.judge_model, judge_prompts)\n        play_a_match_func = play_a_match_single\n        output_file = (\n            f\"data/{args.bench_name}/model_judgment/{args.judge_model}_single.jsonl\"\n        )\n        make_match_func = make_match_single\n        baseline_model = None\n    else:\n        judges = make_judge_pairwise(args.judge_model, judge_prompts)\n        play_a_match_func = play_a_match_pair\n        output_file = (\n            f\"data/{args.bench_name}/model_judgment/{args.judge_model}_pair.jsonl\"\n        )\n        if args.mode == \"pairwise-all\":\n            make_match_func = make_match_all_pairs\n            baseline_model = None\n        else:\n            make_match_func = make_match\n            baseline_model = args.baseline_model\n\n    check_data(questions, model_answers, ref_answers, models, judges)\n\n    question_math = [q for q in questions if q[\"category\"] in NEED_REF_CATS]\n    question_default = [q for q in questions if q[\"category\"] not in NEED_REF_CATS]\n\n    # Make matches\n    matches = []\n    matches += make_match_func(\n        question_default, models, model_answers, judges[\"default\"], baseline_model\n    )\n    matches += make_match_func(\n        question_math,\n        models,\n        model_answers,\n        judges[\"math\"],\n        baseline_model,\n        ref_answers,\n    )\n    matches += make_match_func(\n        question_default,\n        models,\n        model_answers,\n        judges[\"default-mt\"],\n        baseline_model,\n        multi_turn=True,\n    )\n    matches += make_match_func(\n        question_math,\n        models,\n        model_answers,\n        judges[\"math-mt\"],\n        baseline_model,\n        ref_answers,\n        multi_turn=True,\n    )\n\n    match_stat = {}\n    match_stat[\"bench_name\"] = args.bench_name\n    match_stat[\"mode\"] = args.mode\n    match_stat[\"judge\"] = args.judge_model\n    match_stat[\"baseline\"] = baseline_model\n    match_stat[\"model_list\"] = models\n    match_stat[\"total_num_questions\"] = len(questions)\n    match_stat[\"total_num_matches\"] = len(matches)\n    match_stat[\"output_path\"] = output_file\n\n    # Show match stats and prompt enter to continue\n    print(\"Stats:\")\n    print(json.dumps(match_stat, indent=4))\n    input(\"Press Enter to confirm...\")\n\n    # Play matches\n    if args.parallel == 1:\n        for match in tqdm(matches):\n            play_a_match_func(match, output_file=output_file)\n    else:\n\n        def play_a_match_wrapper(match):\n            play_a_match_func(match, output_file=output_file)\n\n        np.random.seed(0)\n        np.random.shuffle(matches)\n\n        with ThreadPoolExecutor(args.parallel) as executor:\n            for match in tqdm(\n                executor.map(play_a_match_wrapper, matches), total=len(matches)\n            ):\n                pass\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/llm_judge/download_mt_bench_pregenerated.py", "content": "\"\"\"\nDownload the pre-generated model answers and judgments for MT-bench.\n\"\"\"\nimport os\n\nfrom fastchat.utils import run_cmd\n\nfilenames = [\n    \"data/mt_bench/model_answer/alpaca-13b.jsonl\",\n    \"data/mt_bench/model_answer/baize-v2-13b.jsonl\",\n    \"data/mt_bench/model_answer/chatglm-6b.jsonl\",\n    \"data/mt_bench/model_answer/claude-instant-v1.jsonl\",\n    \"data/mt_bench/model_answer/claude-v1.jsonl\",\n    \"data/mt_bench/model_answer/dolly-v2-12b.jsonl\",\n    \"data/mt_bench/model_answer/falcon-40b-instruct.jsonl\",\n    \"data/mt_bench/model_answer/fastchat-t5-3b.jsonl\",\n    \"data/mt_bench/model_answer/gpt-3.5-turbo.jsonl\",\n    \"data/mt_bench/model_answer/gpt-4.jsonl\",\n    \"data/mt_bench/model_answer/gpt4all-13b-snoozy.jsonl\",\n    \"data/mt_bench/model_answer/guanaco-33b.jsonl\",\n    \"data/mt_bench/model_answer/guanaco-65b.jsonl\",\n    \"data/mt_bench/model_answer/h2ogpt-oasst-open-llama-13b.jsonl\",\n    \"data/mt_bench/model_answer/koala-13b.jsonl\",\n    \"data/mt_bench/model_answer/llama-13b.jsonl\",\n    \"data/mt_bench/model_answer/mpt-30b-chat.jsonl\",\n    \"data/mt_bench/model_answer/mpt-30b-instruct.jsonl\",\n    \"data/mt_bench/model_answer/mpt-7b-chat.jsonl\",\n    \"data/mt_bench/model_answer/nous-hermes-13b.jsonl\",\n    \"data/mt_bench/model_answer/oasst-sft-4-pythia-12b.jsonl\",\n    \"data/mt_bench/model_answer/oasst-sft-7-llama-30b.jsonl\",\n    \"data/mt_bench/model_answer/palm-2-chat-bison-001.jsonl\",\n    \"data/mt_bench/model_answer/rwkv-4-raven-14b.jsonl\",\n    \"data/mt_bench/model_answer/stablelm-tuned-alpha-7b.jsonl\",\n    \"data/mt_bench/model_answer/tulu-30b.jsonl\",\n    \"data/mt_bench/model_answer/vicuna-13b-v1.3.jsonl\",\n    \"data/mt_bench/model_answer/vicuna-33b-v1.3.jsonl\",\n    \"data/mt_bench/model_answer/vicuna-7b-v1.3.jsonl\",\n    \"data/mt_bench/model_answer/wizardlm-13b.jsonl\",\n    \"data/mt_bench/model_answer/wizardlm-30b.jsonl\",\n    \"data/mt_bench/model_judgment/gpt-4_single.jsonl\",\n    \"data/mt_bench/model_judgment/gpt-4_pair.jsonl\",\n]\n\n\nif __name__ == \"__main__\":\n    prefix = \"https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/\"\n\n    for name in filenames:\n        os.makedirs(os.path.dirname(name), exist_ok=True)\n        ret = run_cmd(f\"wget -q --show-progress -O {name} {prefix + name}\")\n        assert ret == 0\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/model/model_falcon.py", "content": "import gc\nfrom threading import Thread\nfrom typing import Iterable\n\nimport torch\nimport transformers\nfrom transformers import TextIteratorStreamer, GenerationConfig\n\nfrom fastchat.utils import is_partial_stop\n\n\n@torch.inference_mode()\ndef generate_stream_falcon(\n    model,\n    tokenizer,\n    params,\n    device,\n    context_len=2048,\n    stream_interval=2,\n    judge_sent_end=False,\n):\n    prompt = params[\"prompt\"]\n    len_prompt = len(prompt)\n    temperature = float(params.get(\"temperature\", 1.0))\n    repetition_penalty = float(params.get(\"repetition_penalty\", 1.0))\n    top_p = float(params.get(\"top_p\", 1.0))\n    top_k = int(params.get(\"top_k\", 50))  # -1 means disable\n    max_new_tokens = int(params.get(\"max_new_tokens\", 256))\n    stop_str = params.get(\"stop\", None)\n    echo = bool(params.get(\"echo\", True))\n    stop_token_ids = params.get(\"stop_token_ids\", None) or []\n    stop_token_ids.append(tokenizer.eos_token_id)\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    input_ids = inputs[\"input_ids\"]\n    attention_mask = inputs[\"attention_mask\"]\n\n    max_src_len = context_len - max_new_tokens - 8\n\n    input_ids = input_ids[-max_src_len:]  # truncate from the left\n    attention_mask = attention_mask[-max_src_len:]  # truncate from the left\n    input_echo_len = len(input_ids)\n\n    decode_config = dict(skip_special_tokens=True, clean_up_tokenization_spaces=True)\n    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, **decode_config)\n\n    generation_config = GenerationConfig(\n        max_new_tokens=max_new_tokens,\n        do_sample=temperature >= 1e-5,\n        temperature=temperature,\n        repetition_penalty=repetition_penalty,\n        no_repeat_ngram_size=10,\n        top_p=top_p,\n        top_k=top_k,\n        eos_token_id=stop_token_ids,\n    )\n\n    generation_kwargs = dict(\n        inputs=input_ids,\n        attention_mask=attention_mask,\n        streamer=streamer,\n        generation_config=generation_config,\n    )\n\n    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n    thread.start()\n\n    if echo:\n        # means keep the prompt\n        output = prompt\n    else:\n        output = \"\"\n\n    for i, new_text in enumerate(streamer):\n        output += new_text\n        if i % stream_interval == 0:\n            if echo:\n                rfind_start = len_prompt\n            else:\n                rfind_start = 0\n\n            partially_stopped = False\n            if stop_str:\n                if isinstance(stop_str, str):\n                    pos = output.rfind(stop_str, rfind_start)\n                    if pos != -1:\n                        output = output[:pos]\n                    else:\n                        partially_stopped = is_partial_stop(output, stop_str)\n                elif isinstance(stop_str, Iterable):\n                    for each_stop in stop_str:\n                        pos = output.rfind(each_stop, rfind_start)\n                        if pos != -1:\n                            output = output[:pos]\n                            break\n                        else:\n                            partially_stopped = is_partial_stop(output, each_stop)\n                            if partially_stopped:\n                                break\n                else:\n                    raise ValueError(\"Invalid stop field type.\")\n\n            # prevent yielding partial stop sequence\n            if not partially_stopped:\n                yield {\n                    \"text\": output,\n                    \"usage\": {\n                        \"prompt_tokens\": input_echo_len,\n                        \"completion_tokens\": i,\n                        \"total_tokens\": input_echo_len + i,\n                    },\n                    \"finish_reason\": None,\n                }\n    output = output.strip()\n\n    # finish stream event, which contains finish reason\n    if i == max_new_tokens - 1:\n        finish_reason = \"length\"\n    elif partially_stopped:\n        finish_reason = None\n    else:\n        finish_reason = \"stop\"\n\n    yield {\n        \"text\": output,\n        \"usage\": {\n            \"prompt_tokens\": input_echo_len,\n            \"completion_tokens\": i,\n            \"total_tokens\": input_echo_len + i,\n        },\n        \"finish_reason\": finish_reason,\n    }\n\n    # clean\n    gc.collect()\n    torch.cuda.empty_cache()\n    if device == \"xpu\":\n        torch.xpu.empty_cache()\n    if device == \"npu\":\n        torch.npu.empty_cache()\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/model/model_adapter.py", "content": "\"\"\"Model adapter registration.\"\"\"\n\nimport math\nimport os\nimport re\nimport sys\nfrom typing import Dict, List, Optional\nimport warnings\n\nif sys.version_info >= (3, 9):\n    from functools import cache\nelse:\n    from functools import lru_cache as cache\n\nimport accelerate\nimport psutil\nimport torch\nfrom transformers import (\n    AutoConfig,\n    AutoModel,\n    AutoModelForCausalLM,\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    LlamaTokenizer,\n    LlamaForCausalLM,\n    T5Tokenizer,\n)\n\nfrom fastchat.constants import CPU_ISA\nfrom fastchat.conversation import Conversation, get_conv_template\nfrom fastchat.model.compression import load_compress_model\nfrom fastchat.model.llama_condense_monkey_patch import replace_llama_with_condense\nfrom fastchat.model.model_chatglm import generate_stream_chatglm\nfrom fastchat.model.model_codet5p import generate_stream_codet5p\nfrom fastchat.model.model_falcon import generate_stream_falcon\nfrom fastchat.model.model_exllama import generate_stream_exllama\nfrom fastchat.model.model_xfastertransformer import generate_stream_xft\nfrom fastchat.model.monkey_patch_non_inplace import (\n    replace_llama_attn_with_non_inplace_operations,\n)\nfrom fastchat.modules.awq import AWQConfig, load_awq_quantized\nfrom fastchat.modules.exllama import ExllamaConfig, load_exllama_model\nfrom fastchat.modules.xfastertransformer import load_xft_model, XftConfig\nfrom fastchat.modules.gptq import GptqConfig, load_gptq_quantized\nfrom fastchat.utils import get_gpu_memory\n\n# Check an environment variable to check if we should be sharing Peft model\n# weights.  When false we treat all Peft models as separate.\npeft_share_base_weights = (\n    os.environ.get(\"PEFT_SHARE_BASE_WEIGHTS\", \"false\").lower() == \"true\"\n)\n\nANTHROPIC_MODEL_LIST = (\n    \"claude-1\",\n    \"claude-2\",\n    \"claude-instant-1\",\n)\n\n\nclass BaseModelAdapter:\n    \"\"\"The base and the default model adapter.\"\"\"\n\n    use_fast_tokenizer = True\n\n    def match(self, model_path: str):\n        return True\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        try:\n            tokenizer = AutoTokenizer.from_pretrained(\n                model_path,\n                use_fast=self.use_fast_tokenizer,\n                revision=revision,\n                trust_remote_code=True,\n            )\n        except TypeError:\n            tokenizer = AutoTokenizer.from_pretrained(\n                model_path, use_fast=False, revision=revision, trust_remote_code=True\n            )\n        try:\n            model = AutoModelForCausalLM.from_pretrained(\n                model_path,\n                low_cpu_mem_usage=True,\n                trust_remote_code=True,\n                **from_pretrained_kwargs,\n            )\n        except NameError:\n            model = AutoModel.from_pretrained(\n                model_path,\n                low_cpu_mem_usage=True,\n                trust_remote_code=True,\n                **from_pretrained_kwargs,\n            )\n        return model, tokenizer\n\n    def load_compress_model(self, model_path, device, torch_dtype, revision=\"main\"):\n        return load_compress_model(\n            model_path,\n            device,\n            torch_dtype,\n            use_fast=self.use_fast_tokenizer,\n            revision=revision,\n        )\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"one_shot\")\n\n\n# A global registry for all model adapters\n# TODO (lmzheng): make it a priority queue.\nmodel_adapters: List[BaseModelAdapter] = []\n\n\ndef register_model_adapter(cls):\n    \"\"\"Register a model adapter.\"\"\"\n    model_adapters.append(cls())\n\n\n@cache\ndef get_model_adapter(model_path: str) -> BaseModelAdapter:\n    \"\"\"Get a model adapter for a model_path.\"\"\"\n    model_path_basename = os.path.basename(os.path.normpath(model_path))\n\n    # Try the basename of model_path at first\n    for adapter in model_adapters:\n        if adapter.match(model_path_basename) and type(adapter) != BaseModelAdapter:\n            return adapter\n\n    # Then try the full path\n    for adapter in model_adapters:\n        if adapter.match(model_path):\n            return adapter\n\n    raise ValueError(f\"No valid model adapter for {model_path}\")\n\n\ndef raise_warning_for_incompatible_cpu_offloading_configuration(\n    device: str, load_8bit: bool, cpu_offloading: bool\n):\n    if cpu_offloading:\n        if not load_8bit:\n            warnings.warn(\n                \"The cpu-offloading feature can only be used while also using 8-bit-quantization.\\n\"\n                \"Use '--load-8bit' to enable 8-bit-quantization\\n\"\n                \"Continuing without cpu-offloading enabled\\n\"\n            )\n            return False\n        if not \"linux\" in sys.platform:\n            warnings.warn(\n                \"CPU-offloading is only supported on linux-systems due to the limited compatability with the bitsandbytes-package\\n\"\n                \"Continuing without cpu-offloading enabled\\n\"\n            )\n            return False\n        if device != \"cuda\":\n            warnings.warn(\n                \"CPU-offloading is only enabled when using CUDA-devices\\n\"\n                \"Continuing without cpu-offloading enabled\\n\"\n            )\n            return False\n    return cpu_offloading\n\n\ndef load_model(\n    model_path: str,\n    device: str = \"cuda\",\n    num_gpus: int = 1,\n    max_gpu_memory: Optional[str] = None,\n    dtype: Optional[torch.dtype] = None,\n    load_8bit: bool = False,\n    cpu_offloading: bool = False,\n    gptq_config: Optional[GptqConfig] = None,\n    awq_config: Optional[AWQConfig] = None,\n    exllama_config: Optional[ExllamaConfig] = None,\n    xft_config: Optional[XftConfig] = None,\n    revision: str = \"main\",\n    debug: bool = False,\n):\n    \"\"\"Load a model from Hugging Face.\"\"\"\n    # get model adapter\n    adapter = get_model_adapter(model_path)\n\n    # Handle device mapping\n    cpu_offloading = raise_warning_for_incompatible_cpu_offloading_configuration(\n        device, load_8bit, cpu_offloading\n    )\n    if device == \"cpu\":\n        kwargs = {\"torch_dtype\": torch.float32}\n        if CPU_ISA in [\"avx512_bf16\", \"amx\"]:\n            try:\n                import intel_extension_for_pytorch as ipex\n\n                kwargs = {\"torch_dtype\": torch.bfloat16}\n            except ImportError:\n                warnings.warn(\n                    \"Intel Extension for PyTorch is not installed, it can be installed to accelerate cpu inference\"\n                )\n    elif device == \"cuda\":\n        kwargs = {\"torch_dtype\": torch.float16}\n        if num_gpus != 1:\n            kwargs[\"device_map\"] = \"auto\"\n            if max_gpu_memory is None:\n                kwargs[\n                    \"device_map\"\n                ] = \"sequential\"  # This is important for not the same VRAM sizes\n                available_gpu_memory = get_gpu_memory(num_gpus)\n                kwargs[\"max_memory\"] = {\n                    i: str(int(available_gpu_memory[i] * 0.85)) + \"GiB\"\n                    for i in range(num_gpus)\n                }\n            else:\n                kwargs[\"max_memory\"] = {i: max_gpu_memory for i in range(num_gpus)}\n    elif device == \"mps\":\n        kwargs = {\"torch_dtype\": torch.float16}\n        # Avoid bugs in mps backend by not using in-place operations.\n        replace_llama_attn_with_non_inplace_operations()\n    elif device == \"xpu\":\n        kwargs = {\"torch_dtype\": torch.bfloat16}\n        # Try to load ipex, while it looks unused, it links into torch for xpu support\n        try:\n            import intel_extension_for_pytorch as ipex\n        except ImportError:\n            warnings.warn(\n                \"Intel Extension for PyTorch is not installed, but is required for xpu inference.\"\n            )\n    elif device == \"npu\":\n        kwargs = {\"torch_dtype\": torch.float16}\n        # Try to load ipex, while it looks unused, it links into torch for xpu support\n        try:\n            import torch_npu\n        except ImportError:\n            warnings.warn(\"Ascend Extension for PyTorch is not installed.\")\n    else:\n        raise ValueError(f\"Invalid device: {device}\")\n\n    if cpu_offloading:\n        # raises an error on incompatible platforms\n        from transformers import BitsAndBytesConfig\n\n        if \"max_memory\" in kwargs:\n            kwargs[\"max_memory\"][\"cpu\"] = (\n                str(math.floor(psutil.virtual_memory().available / 2**20)) + \"Mib\"\n            )\n        kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n            load_in_8bit_fp32_cpu_offload=cpu_offloading\n        )\n        kwargs[\"load_in_8bit\"] = load_8bit\n    elif load_8bit:\n        if num_gpus != 1:\n            warnings.warn(\n                \"8-bit quantization is not supported for multi-gpu inference.\"\n            )\n        else:\n            model, tokenizer = adapter.load_compress_model(\n                model_path=model_path,\n                device=device,\n                torch_dtype=kwargs[\"torch_dtype\"],\n                revision=revision,\n            )\n            if debug:\n                print(model)\n            return model, tokenizer\n    elif awq_config and awq_config.wbits < 16:\n        assert (\n            awq_config.wbits == 4\n        ), \"Currently we only support 4-bit inference for AWQ.\"\n        model, tokenizer = load_awq_quantized(model_path, awq_config, device)\n        if num_gpus != 1:\n            device_map = accelerate.infer_auto_device_map(\n                model,\n                max_memory=kwargs[\"max_memory\"],\n                no_split_module_classes=[\n                    \"OPTDecoderLayer\",\n                    \"LlamaDecoderLayer\",\n                    \"BloomBlock\",\n                    \"MPTBlock\",\n                    \"DecoderLayer\",\n                ],\n            )\n            model = accelerate.dispatch_model(\n                model, device_map=device_map, offload_buffers=True\n            )\n        else:\n            model.to(device)\n        return model, tokenizer\n    elif gptq_config and gptq_config.wbits < 16:\n        model, tokenizer = load_gptq_quantized(model_path, gptq_config)\n        if num_gpus != 1:\n            device_map = accelerate.infer_auto_device_map(\n                model,\n                max_memory=kwargs[\"max_memory\"],\n                no_split_module_classes=[\"LlamaDecoderLayer\"],\n            )\n            model = accelerate.dispatch_model(\n                model, device_map=device_map, offload_buffers=True\n            )\n        else:\n            model.to(device)\n        return model, tokenizer\n    elif exllama_config:\n        model, tokenizer = load_exllama_model(model_path, exllama_config)\n        return model, tokenizer\n    elif xft_config:\n        model, tokenizer = load_xft_model(model_path, xft_config)\n        return model, tokenizer\n    kwargs[\"revision\"] = revision\n\n    if dtype is not None:  # Overwrite dtype if it is provided in the arguments.\n        kwargs[\"torch_dtype\"] = dtype\n\n    # Load model\n    model, tokenizer = adapter.load_model(model_path, kwargs)\n\n    if (\n        device == \"cpu\"\n        and kwargs[\"torch_dtype\"] is torch.bfloat16\n        and CPU_ISA is not None\n    ):\n        model = ipex.optimize(model, dtype=kwargs[\"torch_dtype\"])\n\n    if (device == \"cuda\" and num_gpus == 1 and not cpu_offloading) or device in (\n        \"mps\",\n        \"xpu\",\n        \"npu\",\n    ):\n        model.to(device)\n\n    if device == \"xpu\":\n        model = torch.xpu.optimize(model, dtype=kwargs[\"torch_dtype\"], inplace=True)\n\n    if debug:\n        print(model)\n\n    return model, tokenizer\n\n\ndef get_conversation_template(model_path: str) -> Conversation:\n    \"\"\"Get the default conversation template.\"\"\"\n    adapter = get_model_adapter(model_path)\n    return adapter.get_default_conv_template(model_path)\n\n\ndef get_generate_stream_function(model: torch.nn.Module, model_path: str):\n    \"\"\"Get the generate_stream function for inference.\"\"\"\n    from fastchat.serve.inference import generate_stream\n\n    model_type = str(type(model)).lower()\n    is_chatglm = \"chatglm\" in model_type\n    is_falcon = \"rwforcausallm\" in model_type\n    is_codet5p = \"codet5p\" in model_type\n    is_peft = \"peft\" in model_type\n    is_exllama = \"exllama\" in model_type\n    is_xft = \"xft\" in model_type\n\n    if is_chatglm:\n        return generate_stream_chatglm\n    elif is_falcon:\n        return generate_stream_falcon\n    elif is_codet5p:\n        return generate_stream_codet5p\n    elif is_exllama:\n        return generate_stream_exllama\n    elif is_xft:\n        return generate_stream_xft\n\n    elif peft_share_base_weights and is_peft:\n        # Return a curried stream function that loads the right adapter\n        # according to the model_name available in this context.  This ensures\n        # the right weights are available.\n        @torch.inference_mode()\n        def generate_stream_peft(\n            model,\n            tokenizer,\n            params: Dict,\n            device: str,\n            context_len: int,\n            stream_interval: int = 2,\n            judge_sent_end: bool = False,\n        ):\n            model.set_adapter(model_path)\n            for x in generate_stream(\n                model,\n                tokenizer,\n                params,\n                device,\n                context_len,\n                stream_interval,\n                judge_sent_end,\n            ):\n                yield x\n\n        return generate_stream_peft\n    else:\n        return generate_stream\n\n\ndef add_model_args(parser):\n    parser.add_argument(\n        \"--model-path\",\n        type=str,\n        default=\"lmsys/vicuna-7b-v1.5\",\n        help=\"The path to the weights. This can be a local folder or a Hugging Face repo ID.\",\n    )\n    parser.add_argument(\n        \"--revision\",\n        type=str,\n        default=\"main\",\n        help=\"Hugging Face Hub model revision identifier\",\n    )\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        choices=[\"cpu\", \"cuda\", \"mps\", \"xpu\", \"npu\"],\n        default=\"cuda\",\n        help=\"The device type\",\n    )\n    parser.add_argument(\n        \"--gpus\",\n        type=str,\n        default=None,\n        help=\"A single GPU like 1 or multiple GPUs like 0,2\",\n    )\n    parser.add_argument(\"--num-gpus\", type=int, default=1)\n    parser.add_argument(\n        \"--max-gpu-memory\",\n        type=str,\n        help=\"The maximum memory per GPU for storing model weights. Use a string like '13Gib'\",\n    )\n    parser.add_argument(\n        \"--dtype\",\n        type=str,\n        choices=[\"float32\", \"float16\", \"bfloat16\"],\n        help=\"Override the default dtype. If not set, it will use float16 on GPU and float32 on CPU.\",\n        default=None,\n    )\n    parser.add_argument(\n        \"--load-8bit\", action=\"store_true\", help=\"Use 8-bit quantization\"\n    )\n    parser.add_argument(\n        \"--cpu-offloading\",\n        action=\"store_true\",\n        help=\"Only when using 8-bit quantization: Offload excess weights to the CPU that don't fit on the GPU\",\n    )\n    parser.add_argument(\n        \"--gptq-ckpt\",\n        type=str,\n        default=None,\n        help=\"Used for GPTQ. The path to the local GPTQ checkpoint.\",\n    )\n    parser.add_argument(\n        \"--gptq-wbits\",\n        type=int,\n        default=16,\n        choices=[2, 3, 4, 8, 16],\n        help=\"Used for GPTQ. #bits to use for quantization\",\n    )\n    parser.add_argument(\n        \"--gptq-groupsize\",\n        type=int,\n        default=-1,\n        help=\"Used for GPTQ. Groupsize to use for quantization; default uses full row.\",\n    )\n    parser.add_argument(\n        \"--gptq-act-order\",\n        action=\"store_true\",\n        help=\"Used for GPTQ. Whether to apply the activation order GPTQ heuristic\",\n    )\n    parser.add_argument(\n        \"--awq-ckpt\",\n        type=str,\n        default=None,\n        help=\"Used for AWQ. Load quantized model. The path to the local AWQ checkpoint.\",\n    )\n    parser.add_argument(\n        \"--awq-wbits\",\n        type=int,\n        default=16,\n        choices=[4, 16],\n        help=\"Used for AWQ. #bits to use for AWQ quantization\",\n    )\n    parser.add_argument(\n        \"--awq-groupsize\",\n        type=int,\n        default=-1,\n        help=\"Used for AWQ. Groupsize to use for AWQ quantization; default uses full row.\",\n    )\n    parser.add_argument(\n        \"--enable-exllama\",\n        action=\"store_true\",\n        help=\"Used for exllamabv2. Enable exllamaV2 inference framework.\",\n    )\n    parser.add_argument(\n        \"--exllama-max-seq-len\",\n        type=int,\n        default=4096,\n        help=\"Used for exllamabv2. Max sequence length to use for exllamav2 framework; default 4096 sequence length.\",\n    )\n    parser.add_argument(\n        \"--exllama-gpu-split\",\n        type=str,\n        default=None,\n        help=\"Used for exllamabv2. Comma-separated list of VRAM (in GB) to use per GPU. Example: 20,7,7\",\n    )\n    parser.add_argument(\n        \"--enable-xft\",\n        action=\"store_true\",\n        help=\"Used for xFasterTransformer Enable xFasterTransformer inference framework.\",\n    )\n    parser.add_argument(\n        \"--xft-max-seq-len\",\n        type=int,\n        default=4096,\n        help=\"Used for xFasterTransformer. Max sequence length to use for xFasterTransformer framework; default 4096 sequence length.\",\n    )\n    parser.add_argument(\n        \"--xft-dtype\",\n        type=str,\n        choices=[\"fp16\", \"bf16\", \"int8\", \"bf16_fp16\", \"bf16_int8\"],\n        help=\"Override the default dtype. If not set, it will use bfloat16 for first token and float16 next tokens on CPU.\",\n        default=None,\n    )\n\n\ndef remove_parent_directory_name(model_path):\n    \"\"\"Remove parent directory name.\"\"\"\n    if model_path[-1] == \"/\":\n        model_path = model_path[:-1]\n    return model_path.split(\"/\")[-1]\n\n\npeft_model_cache = {}\n\n\nclass PeftModelAdapter:\n    \"\"\"Loads any \"peft\" model and it's base model.\"\"\"\n\n    def match(self, model_path: str):\n        \"\"\"Accepts any model path with \"peft\" in the name\"\"\"\n        if os.path.exists(os.path.join(model_path, \"adapter_config.json\")):\n            return True\n        return \"peft\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        \"\"\"Loads the base model then the (peft) adapter weights\"\"\"\n        from peft import PeftConfig, PeftModel\n\n        config = PeftConfig.from_pretrained(model_path)\n        base_model_path = config.base_model_name_or_path\n        if \"peft\" in base_model_path:\n            raise ValueError(\n                f\"PeftModelAdapter cannot load a base model with 'peft' in the name: {config.base_model_name_or_path}\"\n            )\n\n        # Basic proof of concept for loading peft adapters that share the base\n        # weights.  This is pretty messy because Peft re-writes the underlying\n        # base model and internally stores a map of adapter layers.\n        # So, to make this work we:\n        #  1. Cache the first peft model loaded for a given base models.\n        #  2. Call `load_model` for any follow on Peft models.\n        #  3. Make sure we load the adapters by the model_path.  Why? This is\n        #  what's accessible during inference time.\n        #  4. In get_generate_stream_function, make sure we load the right\n        #  adapter before doing inference.  This *should* be safe when calls\n        #  are blocked the same semaphore.\n        if peft_share_base_weights:\n            if base_model_path in peft_model_cache:\n                model, tokenizer = peft_model_cache[base_model_path]\n                # Super important: make sure we use model_path as the\n                # `adapter_name`.\n                model.load_adapter(model_path, adapter_name=model_path)\n            else:\n                base_adapter = get_model_adapter(base_model_path)\n                base_model, tokenizer = base_adapter.load_model(\n                    base_model_path, from_pretrained_kwargs\n                )\n                # Super important: make sure we use model_path as the\n                # `adapter_name`.\n                model = PeftModel.from_pretrained(\n                    base_model, model_path, adapter_name=model_path\n                )\n                peft_model_cache[base_model_path] = (model, tokenizer)\n            return model, tokenizer\n\n        # In the normal case, load up the base model weights again.\n        base_adapter = get_model_adapter(base_model_path)\n        base_model, tokenizer = base_adapter.load_model(\n            base_model_path, from_pretrained_kwargs\n        )\n        model = PeftModel.from_pretrained(base_model, model_path)\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        \"\"\"Uses the conv template of the base model\"\"\"\n        from peft import PeftConfig, PeftModel\n\n        config = PeftConfig.from_pretrained(model_path)\n        if \"peft\" in config.base_model_name_or_path:\n            raise ValueError(\n                f\"PeftModelAdapter cannot load a base model with 'peft' in the name: {config.base_model_name_or_path}\"\n            )\n        base_model_path = config.base_model_name_or_path\n        base_adapter = get_model_adapter(base_model_path)\n        return base_adapter.get_default_conv_template(config.base_model_name_or_path)\n\n\nclass VicunaAdapter(BaseModelAdapter):\n    \"Model adapter for Vicuna models (e.g., lmsys/vicuna-7b-v1.5)\" \"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"vicuna\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, use_fast=self.use_fast_tokenizer, revision=revision\n        )\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            **from_pretrained_kwargs,\n        )\n        self.raise_warning_for_old_weights(model)\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        if \"v0\" in remove_parent_directory_name(model_path):\n            return get_conv_template(\"one_shot\")\n        return get_conv_template(\"vicuna_v1.1\")\n\n    def raise_warning_for_old_weights(self, model):\n        if isinstance(model, LlamaForCausalLM) and model.model.vocab_size > 32000:\n            warnings.warn(\n                \"\\nYou are probably using the old Vicuna-v0 model, \"\n                \"which will generate unexpected results with the \"\n                \"current fastchat.\\nYou can try one of the following methods:\\n\"\n                \"1. Upgrade your weights to the new Vicuna-v1.3: https://github.com/lm-sys/FastChat#vicuna-weights.\\n\"\n                \"2. Use the old conversation template by `python3 -m fastchat.serve.cli --model-path /path/to/vicuna-v0 --conv-template one_shot`\\n\"\n                \"3. Downgrade fschat to fschat==0.1.10 (Not recommended).\\n\"\n            )\n\n\nclass AiroborosAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for jondurbin/airoboros-*\"\"\"\n\n    def match(self, model_path: str):\n        if re.search(r\"airoboros|spicyboros\", model_path, re.I):\n            return True\n        return False\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        if \"-3.\" in model_path or \"-3p\" in model_path:\n            return get_conv_template(\"airoboros_v3\")\n        if \"spicyboros\" in model_path or re.search(r\"-(2\\.[2-9]+)\", model_path):\n            return get_conv_template(\"airoboros_v2\")\n        return get_conv_template(\"airoboros_v1\")\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        if \"mpt\" not in model_path.lower():\n            return super().load_model(model_path, from_pretrained_kwargs)\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            trust_remote_code=True,\n            max_seq_len=8192,\n            **from_pretrained_kwargs,\n        )\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, trust_remote_code=True, use_fast=True\n        )\n        return model, tokenizer\n\n\nclass LongChatAdapter(BaseModelAdapter):\n    \"Model adapter for LongChat models (e.g., lmsys/longchat-7b-16k).\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"longchat\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n\n        # Apply monkey patch, TODO(Dacheng): Add flash attention support\n        config = AutoConfig.from_pretrained(model_path, revision=revision)\n        replace_llama_with_condense(config.rope_scaling[\"factor\"])\n\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, use_fast=self.use_fast_tokenizer, revision=revision\n        )\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            **from_pretrained_kwargs,\n        )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"vicuna_v1.1\")\n\n\nclass GoogleT5Adapter(BaseModelAdapter):\n    \"\"\"The model adapter for google/Flan based models, such as Salesforce/codet5p-6b, lmsys/fastchat-t5-3b-v1.0, flan-t5-*, flan-ul2\"\"\"\n\n    def match(self, model_path: str):\n        return any(\n            model_str in model_path.lower()\n            for model_str in [\"flan-\", \"fastchat-t5\", \"codet5p\"]\n        )\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        tokenizer = T5Tokenizer.from_pretrained(model_path, revision=revision)\n        model = AutoModelForSeq2SeqLM.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            trust_remote_code=True,\n            **from_pretrained_kwargs,\n        )\n        return model, tokenizer\n\n\nclass KoalaAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Koala\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"koala\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"koala_v1\")\n\n\nclass AlpacaAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Alpaca\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"alpaca\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"alpaca\")\n\n\nclass ChatGLMAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for THUDM/chatglm-6b, THUDM/chatglm2-6b\"\"\"\n\n    def match(self, model_path: str):\n        return \"chatglm\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        if \"chatglm3\" in model_path.lower():\n            tokenizer = AutoTokenizer.from_pretrained(\n                model_path,\n                encode_special_tokens=True,\n                trust_remote_code=True,\n                revision=revision,\n            )\n        else:\n            tokenizer = AutoTokenizer.from_pretrained(\n                model_path, trust_remote_code=True, revision=revision\n            )\n        model = AutoModel.from_pretrained(\n            model_path, trust_remote_code=True, **from_pretrained_kwargs\n        )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        model_path = model_path.lower()\n        if \"chatglm2\" in model_path.lower():\n            return get_conv_template(\"chatglm2\")\n        if \"chatglm3\" in model_path.lower():\n            return get_conv_template(\"chatglm3\")\n        return get_conv_template(\"chatglm\")\n\n\nclass CodeGeexAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for THUDM/codegeex-6b, THUDM/codegeex2-6b\"\"\"\n\n    def match(self, model_path: str):\n        return \"codegeex\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, trust_remote_code=True, revision=revision\n        )\n        model = AutoModel.from_pretrained(\n            model_path, trust_remote_code=True, **from_pretrained_kwargs\n        )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"codegeex\")\n\n\nclass DollyV2Adapter(BaseModelAdapter):\n    \"\"\"The model adapter for databricks/dolly-v2-12b\"\"\"\n\n    def match(self, model_path: str):\n        return \"dolly-v2\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        tokenizer = AutoTokenizer.from_pretrained(model_path, revision=revision)\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            **from_pretrained_kwargs,\n        )\n        # 50277 means \"### End\"\n        tokenizer.eos_token_id = 50277\n        model.config.eos_token_id = tokenizer.eos_token_id\n        model.config.pad_token_id = tokenizer.pad_token_id\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"dolly_v2\")\n\n\nclass OasstPythiaAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\"\"\n\n    def match(self, model_path: str):\n        model_path = model_path.lower()\n        return \"oasst\" in model_path and \"pythia\" in model_path\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"oasst_pythia\")\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        model, tokenizer = super().load_model(model_path, from_pretrained_kwargs)\n        model.config.eos_token_id = tokenizer.eos_token_id\n        model.config.pad_token_id = tokenizer.pad_token_id\n        return model, tokenizer\n\n\nclass OasstLLaMAAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for OpenAssistant/oasst-sft-7-llama-30b\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        model_path = model_path.lower()\n        if \"openassistant-sft-7-llama-30b-hf\" in model_path:\n            return True\n        return \"oasst\" in model_path and \"pythia\" not in model_path\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"oasst_llama\")\n\n\nclass OpenChat35Adapter(BaseModelAdapter):\n    \"\"\"The model adapter for OpenChat 3.5 (e.g. openchat/openchat_3.5)\"\"\"\n\n    def match(self, model_path: str):\n        return \"openchat\" in model_path.lower() and \"3.5\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"openchat_3.5\")\n\n\nclass PythiaAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for any EleutherAI/pythia model\"\"\"\n\n    def match(self, model_path: str):\n        return \"pythia\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        model, tokenizer = super().load_model(model_path, from_pretrained_kwargs)\n        model.config.eos_token_id = tokenizer.eos_token_id\n        model.config.pad_token_id = tokenizer.pad_token_id\n        return model, tokenizer\n\n\nclass StableLMAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for StabilityAI/stablelm-tuned-alpha-7b\"\"\"\n\n    def match(self, model_path: str):\n        return \"stablelm\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"stablelm\")\n\n\nclass MPTAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for MPT series (mosaicml/mpt-7b-chat, mosaicml/mpt-30b-chat)\"\"\"\n\n    def match(self, model_path: str):\n        model_path = model_path.lower()\n        return \"mpt\" in model_path and not \"airoboros\" in model_path\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            trust_remote_code=True,\n            max_seq_len=8192,\n            **from_pretrained_kwargs,\n        )\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, trust_remote_code=True, revision=revision\n        )\n        model.config.eos_token_id = tokenizer.eos_token_id\n        model.config.pad_token_id = tokenizer.pad_token_id\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        model_path = model_path.lower()\n        if \"mpt-7b-chat\" in model_path:\n            return get_conv_template(\"mpt-7b-chat\")\n        elif \"mpt-30b-chat\" in model_path:\n            return get_conv_template(\"mpt-30b-chat\")\n        elif \"mpt-30b-instruct\" in model_path:\n            return get_conv_template(\"mpt-30b-instruct\")\n        else:\n            print(\n                \"Warning: Loading base MPT model with `zero_shot` conversation configuration.  \"\n                \"If this is not desired, inspect model configurations and names.\"\n            )\n            return get_conv_template(\"zero_shot\")\n\n\nclass BaizeAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for project-baize/baize-v2-7b\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"baize\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"baize\")\n\n\nclass RwkvAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for BlinkDL/RWKV-4-Raven\"\"\"\n\n    def match(self, model_path: str):\n        return \"rwkv-4\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        from fastchat.model.rwkv_model import RwkvModel\n\n        model = RwkvModel(model_path)\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        tokenizer = AutoTokenizer.from_pretrained(\n            \"EleutherAI/pythia-160m\", revision=revision\n        )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"rwkv\")\n\n\nclass OpenBuddyAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for OpenBuddy/openbuddy-7b-v1.1-bf16-enc\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"openbuddy\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"openbuddy\")\n\n\nclass PhoenixAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for FreedomIntelligence/phoenix-inst-chat-7b\"\"\"\n\n    def match(self, model_path: str):\n        return \"phoenix\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"phoenix\")\n\n\nclass ReaLMAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for FreedomIntelligence/ReaLM-7b\"\"\"\n\n    def match(self, model_path: str):\n        return \"ReaLM\" in model_path\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path, low_cpu_mem_usage=True, **from_pretrained_kwargs\n        )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"ReaLM-7b-v1\")\n\n\nclass ChatGPTAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for ChatGPT\"\"\"\n\n    def match(self, model_path: str):\n        return model_path in (\n            \"gpt-3.5-turbo\",\n            \"gpt-3.5-turbo-1106\",\n            \"gpt-4\",\n            \"gpt-4-turbo\",\n        )\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        raise NotImplementedError()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"chatgpt\")\n\n\nclass AzureOpenAIAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Azure OpenAI\"\"\"\n\n    def match(self, model_path: str):\n        return model_path in (\"azure-gpt-35-turbo\", \"azure-gpt-4\")\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        raise NotImplementedError()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"chatgpt\")\n\n\nclass ClaudeAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Claude\"\"\"\n\n    def match(self, model_path: str):\n        return model_path in ANTHROPIC_MODEL_LIST\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        raise NotImplementedError()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"claude\")\n\n\nclass BardAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Bard\"\"\"\n\n    def match(self, model_path: str):\n        return model_path == \"bard\"\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        raise NotImplementedError()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"bard\")\n\n\nclass PaLM2Adapter(BaseModelAdapter):\n    \"\"\"The model adapter for PaLM2\"\"\"\n\n    def match(self, model_path: str):\n        return model_path == \"palm-2\"\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        raise NotImplementedError()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"bard\")\n\n\nclass BiLLaAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Neutralzz/BiLLa-7B-SFT\"\"\"\n\n    def match(self, model_path: str):\n        return \"billa\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"billa\")\n\n\nclass RedPajamaINCITEAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for togethercomputer/RedPajama-INCITE-7B-Chat\"\"\"\n\n    def match(self, model_path: str):\n        return \"redpajama-incite\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        tokenizer = AutoTokenizer.from_pretrained(model_path, revision=revision)\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            **from_pretrained_kwargs,\n        )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"redpajama-incite\")\n\n\nclass H2OGPTAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"h2ogpt\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"h2ogpt\")\n\n\nclass RobinAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for LMFlow/Full-Robin-7b-v2\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"robin\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"Robin\")\n\n\nclass SnoozyAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for nomic-ai/gpt4all-13b-snoozy\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        model_path = model_path.lower()\n        return \"gpt4all\" in model_path and \"snoozy\" in model_path\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"snoozy\")\n\n\nclass WizardLMAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for WizardLM/WizardLM-13B-V1.0\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"wizardlm\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        model_path = model_path.lower()\n        if \"13b\" in model_path or \"30b\" in model_path or \"70b\" in model_path:\n            return get_conv_template(\"vicuna_v1.1\")\n        else:\n            # TODO: use the recommended template for 7B\n            # (https://huggingface.co/WizardLM/WizardLM-13B-V1.0)\n            return get_conv_template(\"one_shot\")\n\n\nclass ManticoreAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for openaccess-ai-collective/manticore-13b-chat-pyg\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"manticore\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"manticore\")\n\n\nclass GuanacoAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for timdettmers/guanaco-33b-merged\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"guanaco\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, use_fast=self.use_fast_tokenizer, revision=revision\n        )\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path, low_cpu_mem_usage=True, **from_pretrained_kwargs\n        )\n        # Fix a bug in tokenizer config\n        tokenizer.eos_token_id = model.config.eos_token_id\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"zero_shot\")\n\n\nclass ChangGPTAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for lcw99/polyglot-ko-12.8b-chang-instruct-chat\"\"\"\n\n    def match(self, model_path: str):\n        model_path = model_path.lower()\n        return \"polyglot\" in model_path and \"chang\" in model_path\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"polyglot_changgpt\")\n\n\nclass CamelAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for camel-ai/CAMEL-13B-Combined-Data\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"camel\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"vicuna_v1.1\")\n\n\nclass TuluAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for allenai/tulu-30b\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"tulu\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"tulu\")\n\n\nclass FalconAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for tiiuae/falcon-40b\"\"\"\n\n    def match(self, model_path: str):\n        return \"falcon\" in model_path.lower() and \"chat\" not in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        # Strongly suggest using bf16, which is recommended by the author of Falcon\n        tokenizer = AutoTokenizer.from_pretrained(model_path, revision=revision)\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            trust_remote_code=True,\n            **from_pretrained_kwargs,\n        )\n        # In Falcon tokenizer config and special config there is not any pad token\n        # Setting `pad_token_id` to 9, which corresponds to special token '>>SUFFIX<<'\n        tokenizer.pad_token_id = 9\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"falcon\")\n\n\nclass FalconChatAdapter(BaseModelAdapter):\n    def match(self, model_path: str):\n        return \"falcon\" in model_path.lower() and \"chat\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"falcon-chat\")\n\n\nclass TigerBotAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for TigerResearch/tigerbot-7b-sft\"\"\"\n\n    def match(self, model_path: str):\n        return \"tigerbot\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path,\n            trust_remote_code=True,\n            revision=revision,\n        )\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            trust_remote_code=True,\n            low_cpu_mem_usage=True,\n            **from_pretrained_kwargs,\n        )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"tigerbot\")\n\n\nclass BaichuanAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Baichuan models (e.g., baichuan-inc/Baichuan-7B)\"\"\"\n\n    def match(self, model_path: str):\n        return \"baichuan\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, trust_remote_code=True, revision=revision\n        )\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            trust_remote_code=True,\n            low_cpu_mem_usage=True,\n            **from_pretrained_kwargs,\n        )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        # for Baichuan-13B-Chat\n        if \"chat\" in model_path.lower():\n            if \"baichuan2\" in model_path.lower():\n                return get_conv_template(\"baichuan2-chat\")\n            return get_conv_template(\"baichuan-chat\")\n        return get_conv_template(\"zero_shot\")\n\n\nclass XGenAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Salesforce/xgen-7b\"\"\"\n\n    def match(self, model_path: str):\n        return \"xgen\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            trust_remote_code=True,\n            **from_pretrained_kwargs,\n        )\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, trust_remote_code=True, revision=revision\n        )\n        model.config.eos_token_id = 50256\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"xgen\")\n\n\nclass NousHermesAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for NousResearch/Nous-Hermes-13b\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"nous-hermes\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"alpaca\")\n\n\nclass InternLMChatAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for internlm/internlm-chat-7b\"\"\"\n\n    def match(self, model_path: str):\n        return \"internlm\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            trust_remote_code=True,\n            **from_pretrained_kwargs,\n        )\n        model = model.eval()\n        if \"8k\" in model_path.lower():\n            model.config.max_sequence_length = 8192\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, trust_remote_code=True, revision=revision\n        )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"internlm-chat\")\n\n\nclass StarChatAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for HuggingFaceH4/starchat-beta\"\"\"\n\n    def match(self, model_path: str):\n        return \"starchat\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"starchat\")\n\n\nclass MistralAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Mistral AI models\"\"\"\n\n    def match(self, model_path: str):\n        return \"mistral\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        model, tokenizer = super().load_model(model_path, from_pretrained_kwargs)\n        model.config.eos_token_id = tokenizer.eos_token_id\n        model.config.pad_token_id = tokenizer.pad_token_id\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"mistral\")\n\n\nclass Llama2Adapter(BaseModelAdapter):\n    \"\"\"The model adapter for Llama-2 (e.g., meta-llama/Llama-2-7b-hf)\"\"\"\n\n    def match(self, model_path: str):\n        return \"llama-2\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        model, tokenizer = super().load_model(model_path, from_pretrained_kwargs)\n        model.config.eos_token_id = tokenizer.eos_token_id\n        model.config.pad_token_id = tokenizer.pad_token_id\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"llama-2\")\n\n\nclass CuteGPTAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for CuteGPT\"\"\"\n\n    def match(self, model_path: str):\n        return \"cutegpt\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        tokenizer = LlamaTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path, low_cpu_mem_usage=True, **from_pretrained_kwargs\n        )\n        tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids(\"<end>\")\n        model.config.eos_token_id = tokenizer.eos_token_id\n        model.config.pad_token_id = tokenizer.eos_token_id\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"cutegpt\")\n\n\nclass OpenOrcaAdapter(BaseModelAdapter):\n    \"\"\"Model adapter for Open-Orca models which may use different prompt templates\n    - (e.g. Open-Orca/OpenOrcaxOpenChat-Preview2-13B, Open-Orca/Mistral-7B-OpenOrca)\n    - `OpenOrcaxOpenChat-Preview2-13B` uses their \"OpenChat Llama2 V1\" prompt template.\n        - [Open-Orca/OpenOrcaxOpenChat-Preview2-13B #Prompt Template](https://huggingface.co/Open-Orca/OpenOrcaxOpenChat-Preview2-13B#prompt-template)\n    - `Mistral-7B-OpenOrca` uses the [OpenAI's Chat Markup Language (ChatML)](https://github.com/openai/openai-python/blob/main/chatml.md)\n        format, with <|im_start|> and <|im_end|> tokens added to support this.\n        - [Open-Orca/Mistral-7B-OpenOrca #Prompt Template](https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca#prompt-template)\n    \"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return (\n            \"mistral-7b-openorca\" in model_path.lower()\n            or \"openorca\" in model_path.lower()\n        )\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, use_fast=self.use_fast_tokenizer, revision=revision\n        )\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            **from_pretrained_kwargs,\n        ).eval()\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        if \"mistral-7b-openorca\" in model_path.lower():\n            return get_conv_template(\"mistral-7b-openorca\")\n        return get_conv_template(\"open-orca\")\n\n\nclass WizardCoderAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for WizardCoder (e.g., WizardLM/WizardCoder-Python-34B-V1.0)\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"wizardcoder\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        # Same as Alpaca, see :\n        # https://github.com/nlpxucan/WizardLM/blob/main/WizardCoder/src/inference_wizardcoder.py#L60\n        return get_conv_template(\"alpaca\")\n\n\nclass QwenChatAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Qwen/Qwen-7B-Chat\n    To run this model, you need to ensure additional flash attention installation:\n    ``` bash\n    git clone https://github.com/Dao-AILab/flash-attention\n    cd flash-attention && pip install .\n    pip install csrc/layer_norm\n    pip install csrc/rotary\n    ```\n\n    Since from 2.0, the following change happened\n    - `flash_attn_unpadded_func` -> `flash_attn_varlen_func`\n    - `flash_attn_unpadded_qkvpacked_func` -> `flash_attn_varlen_qkvpacked_func`\n    - `flash_attn_unpadded_kvpacked_func` -> `flash_attn_varlen_kvpacked_func`\n    You may need to revise the code in: https://huggingface.co/Qwen/Qwen-7B-Chat/blob/main/modeling_qwen.py#L69\n    to from flash_attn.flash_attn_interface import flash_attn_varlen_func as flash_attn_unpadded_func\n    \"\"\"\n\n    def match(self, model_path: str):\n        return \"qwen\" in model_path.lower()\n\n    def float_set(self, config, option):\n        config.bf16 = False\n        config.fp16 = False\n        config.fp32 = False\n\n        if option == \"bf16\":\n            config.bf16 = True\n        elif option == \"fp16\":\n            config.fp16 = True\n        elif option == \"fp32\":\n            config.fp32 = True\n        else:\n            print(\"Invalid option. Please choose one from 'bf16', 'fp16' and 'fp32'.\")\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        from transformers.generation import GenerationConfig\n\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        config = AutoConfig.from_pretrained(\n            model_path,\n            trust_remote_code=True,\n        )\n        # NOTE: if you use the old version of model file, please remove the comments below\n        # config.use_flash_attn = False\n        self.float_set(config, \"fp16\")\n        generation_config = GenerationConfig.from_pretrained(\n            model_path, trust_remote_code=True\n        )\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            config=config,\n            low_cpu_mem_usage=True,\n            trust_remote_code=True,\n            **from_pretrained_kwargs,\n        ).eval()\n        if hasattr(model.config, \"use_dynamic_ntk\") and model.config.use_dynamic_ntk:\n            model.config.max_sequence_length = 16384\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, trust_remote_code=True, revision=revision\n        )\n        tokenizer.eos_token_id = config.eos_token_id\n        tokenizer.bos_token_id = config.bos_token_id\n        tokenizer.pad_token_id = generation_config.pad_token_id\n        model.config.eos_token_id = tokenizer.eos_token_id\n        model.config.bos_token_id = tokenizer.bos_token_id\n        model.config.pad_token_id = tokenizer.pad_token_id\n\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"qwen-7b-chat\")\n\n\nclass BGEAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for BGE (e.g., BAAI/bge-large-en-v1.5)\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"bge\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        model = AutoModel.from_pretrained(\n            model_path,\n            **from_pretrained_kwargs,\n        )\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, trust_remote_code=True, revision=revision\n        )\n        if hasattr(model.config, \"max_position_embeddings\") and hasattr(\n            tokenizer, \"model_max_length\"\n        ):\n            model.config.max_sequence_length = min(\n                model.config.max_position_embeddings, tokenizer.model_max_length\n            )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"one_shot\")\n\n\nclass E5Adapter(BaseModelAdapter):\n    \"\"\"The model adapter for E5 (e.g., intfloat/e5-large-v2)\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"e5-\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        model = AutoModel.from_pretrained(\n            model_path,\n            **from_pretrained_kwargs,\n        )\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, trust_remote_code=True, revision=revision\n        )\n        if hasattr(model.config, \"max_position_embeddings\") and hasattr(\n            tokenizer, \"model_max_length\"\n        ):\n            model.config.max_sequence_length = min(\n                model.config.max_position_embeddings, tokenizer.model_max_length\n            )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"one_shot\")\n\n\nclass AquilaChatAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for BAAI/Aquila\n\n    Now supports:\n    - BAAI/AquilaChat-7B\n    - BAAI/AquilaChat2-7B\n    - BAAI/AquilaChat2-34B\n    \"\"\"\n\n    def match(self, model_path: str):\n        return \"aquila\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            trust_remote_code=True,\n            **from_pretrained_kwargs,\n        )\n        model = model.eval()\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, trust_remote_code=True, revision=revision\n        )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        model_path = model_path.lower()\n        # See: https://huggingface.co/BAAI/AquilaChat2-34B/blob/4608b75855334b93329a771aee03869dbf7d88cc/predict.py#L347\n        if \"aquilachat2\" in model_path:\n            if \"16k\" in model_path:\n                return get_conv_template(\"aquila\")\n            elif \"34b\" in model_path:\n                return get_conv_template(\"aquila-legacy\")\n            else:\n                return get_conv_template(\"aquila-v1\")\n        else:\n            return get_conv_template(\"aquila-chat\")\n\n\nclass Lamma2ChineseAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for FlagAlpha/LLama2-Chinese sft\"\"\"\n\n    def match(self, model_path: str):\n        return \"llama2-chinese\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path,\n            trust_remote_code=True,\n            revision=revision,\n        )\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            trust_remote_code=True,\n            low_cpu_mem_usage=True,\n            **from_pretrained_kwargs,\n        )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"llama2-chinese\")\n\n\nclass VigogneAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for vigogne (e.g., bofenghuang/vigogne-2-7b-chat)\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return bool(re.search(r\"vigogne|vigostral\", model_path, re.I))\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path,\n            use_fast=self.use_fast_tokenizer,\n            trust_remote_code=True,\n            revision=revision,\n        )\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            trust_remote_code=True,\n            low_cpu_mem_usage=True,\n            **from_pretrained_kwargs,\n        ).eval()\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        if \"chat\" in model_path.lower():\n            if \"vigostral\" in model_path.lower():\n                return get_conv_template(\"vigogne_chat_v3\")\n            return get_conv_template(\"vigogne_chat_v2\")\n        return get_conv_template(\"vigogne_instruct\")\n\n\nclass OpenLLaMaOpenInstructAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for OpenLLaMa-Open-Instruct (e.g., VMware/open-llama-7b-open-instruct)\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return (\n            \"open-llama\" in model_path.lower() and \"open-instruct\" in model_path.lower()\n        )\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        revision = from_pretrained_kwargs.get(\"revision\", \"main\")\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path,\n            use_fast=self.use_fast_tokenizer,\n            trust_remote_code=True,\n            revision=revision,\n        )\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            trust_remote_code=True,\n            low_cpu_mem_usage=True,\n            **from_pretrained_kwargs,\n        ).eval()\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"alpaca\")\n\n\nclass CodeLlamaAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for CodeLlama (e.g., codellama/CodeLlama-34b-hf)\"\"\"\n\n    def match(self, model_path: str):\n        return \"codellama\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        model, tokenizer = super().load_model(model_path, from_pretrained_kwargs)\n        model.config.eos_token_id = tokenizer.eos_token_id\n        model.config.pad_token_id = tokenizer.pad_token_id\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"llama-2\")\n\n\nclass PhindCodeLlamaAdapter(CodeLlamaAdapter):\n    \"\"\"The model adapter for Phind-CodeLlama (e.g., Phind/Phind-CodeLlama-34B-v2)\"\"\"\n\n    def match(self, model_path: str):\n        return \"phind-codellama-\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"phind\")\n\n\nclass Llama2ChangAdapter(Llama2Adapter):\n    \"\"\"The model adapter for Llama2-ko-chang (e.g., lcw99/llama2-ko-chang-instruct-chat)\"\"\"\n\n    def match(self, model_path: str):\n        return \"llama2-ko-chang\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"polyglot_changgpt\")\n\n\nclass ZephyrAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Zephyr (e.g. HuggingFaceH4/zephyr-7b-alpha)\"\"\"\n\n    def match(self, model_path: str):\n        return \"zephyr\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"zephyr\")\n\n\nclass XwinLMAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Xwin-LM V0.1 and V0.2 series of models(e.g., Xwin-LM/Xwin-LM-70B-V0.1)\"\"\"\n\n    # use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"xwin-lm\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"vicuna_v1.1\")\n\n\nclass LemurAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for OpenLemur/lemur-70b-chat-v1\"\"\"\n\n    use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return \"lemur-70b-chat\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"lemur-70b-chat\")\n\n\nclass PygmalionAdapter(BaseModelAdapter):\n    \"\"\"The model adapter for Pygmalion/Metharme series of models(e.g., PygmalionAI/mythalion-13b)\"\"\"\n\n    # use_fast_tokenizer = False\n\n    def match(self, model_path: str):\n        return bool(\n            re.search(r\"pygmalion|mythalion|metharme\", model_path.lower(), re.I)\n        )\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"metharme\")\n\n\n# Note: the registration order matters.\n# The one registered earlier has a higher matching priority.\nregister_model_adapter(PeftModelAdapter)\nregister_model_adapter(VicunaAdapter)\nregister_model_adapter(AiroborosAdapter)\nregister_model_adapter(LongChatAdapter)\nregister_model_adapter(GoogleT5Adapter)\nregister_model_adapter(KoalaAdapter)\nregister_model_adapter(AlpacaAdapter)\nregister_model_adapter(ChatGLMAdapter)\nregister_model_adapter(CodeGeexAdapter)\nregister_model_adapter(DollyV2Adapter)\nregister_model_adapter(OasstPythiaAdapter)\nregister_model_adapter(OasstLLaMAAdapter)\nregister_model_adapter(OpenChat35Adapter)\nregister_model_adapter(StableLMAdapter)\nregister_model_adapter(BaizeAdapter)\nregister_model_adapter(RwkvAdapter)\nregister_model_adapter(OpenBuddyAdapter)\nregister_model_adapter(PhoenixAdapter)\nregister_model_adapter(BardAdapter)\nregister_model_adapter(PaLM2Adapter)\nregister_model_adapter(ChatGPTAdapter)\nregister_model_adapter(AzureOpenAIAdapter)\nregister_model_adapter(ClaudeAdapter)\nregister_model_adapter(MPTAdapter)\nregister_model_adapter(BiLLaAdapter)\nregister_model_adapter(RedPajamaINCITEAdapter)\nregister_model_adapter(H2OGPTAdapter)\nregister_model_adapter(RobinAdapter)\nregister_model_adapter(SnoozyAdapter)\nregister_model_adapter(WizardLMAdapter)\nregister_model_adapter(ManticoreAdapter)\nregister_model_adapter(GuanacoAdapter)\nregister_model_adapter(CamelAdapter)\nregister_model_adapter(ChangGPTAdapter)\nregister_model_adapter(TuluAdapter)\nregister_model_adapter(FalconChatAdapter)\nregister_model_adapter(FalconAdapter)\nregister_model_adapter(TigerBotAdapter)\nregister_model_adapter(BaichuanAdapter)\nregister_model_adapter(XGenAdapter)\nregister_model_adapter(NousHermesAdapter)\nregister_model_adapter(PythiaAdapter)\nregister_model_adapter(InternLMChatAdapter)\nregister_model_adapter(StarChatAdapter)\nregister_model_adapter(Llama2Adapter)\nregister_model_adapter(CuteGPTAdapter)\nregister_model_adapter(OpenOrcaAdapter)\nregister_model_adapter(MistralAdapter)\nregister_model_adapter(WizardCoderAdapter)\nregister_model_adapter(QwenChatAdapter)\nregister_model_adapter(AquilaChatAdapter)\nregister_model_adapter(BGEAdapter)\nregister_model_adapter(E5Adapter)\nregister_model_adapter(Lamma2ChineseAdapter)\nregister_model_adapter(VigogneAdapter)\nregister_model_adapter(OpenLLaMaOpenInstructAdapter)\nregister_model_adapter(ReaLMAdapter)\nregister_model_adapter(PhindCodeLlamaAdapter)\nregister_model_adapter(CodeLlamaAdapter)\nregister_model_adapter(Llama2ChangAdapter)\nregister_model_adapter(ZephyrAdapter)\nregister_model_adapter(XwinLMAdapter)\nregister_model_adapter(LemurAdapter)\nregister_model_adapter(PygmalionAdapter)\n\n\n# After all adapters, try the default base adapter.\nregister_model_adapter(BaseModelAdapter)\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/llm_judge/show_result.py", "content": "\"\"\"\nUsage:\npython3 show_result.py --mode [single|pairwise-baseline|pairwise-all]\n\"\"\"\nimport argparse\nimport pandas as pd\n\n\ndef display_result_single(args):\n    if args.input_file is None:\n        input_file = (\n            f\"data/{args.bench_name}/model_judgment/{args.judge_model}_single.jsonl\"\n        )\n    else:\n        input_file = args.input_file\n\n    print(f\"Input file: {input_file}\")\n    df_all = pd.read_json(input_file, lines=True)\n    df = df_all[[\"model\", \"score\", \"turn\"]]\n    df = df[df[\"score\"] != -1]\n\n    if args.model_list is not None:\n        df = df[df[\"model\"].isin(args.model_list)]\n\n    print(\"\\n########## First turn ##########\")\n    df_1 = df[df[\"turn\"] == 1].groupby([\"model\", \"turn\"]).mean()\n    print(df_1.sort_values(by=\"score\", ascending=False))\n\n    if args.bench_name == \"mt_bench\":\n        print(\"\\n########## Second turn ##########\")\n        df_2 = df[df[\"turn\"] == 2].groupby([\"model\", \"turn\"]).mean()\n        print(df_2.sort_values(by=\"score\", ascending=False))\n\n        print(\"\\n########## Average ##########\")\n        df_3 = df[[\"model\", \"score\"]].groupby([\"model\"]).mean()\n        print(df_3.sort_values(by=\"score\", ascending=False))\n\n\ndef display_result_pairwise(args):\n    if args.input_file is None:\n        input_file = (\n            f\"data/{args.bench_name}/model_judgment/{args.judge_model}_pair.jsonl\"\n        )\n    else:\n        input_file = args.input_file\n\n    print(f\"Input file: {input_file}\")\n    df_all = pd.read_json(input_file, lines=True)\n    df_all = df_all[(df_all[\"g1_winner\"] != \"error\") & (df_all[\"g2_winner\"] != \"error\")]\n\n    model_list = (\n        df_all[\"model_1\"].unique().tolist() + df_all[\"model_2\"].unique().tolist()\n    )\n    model_list = list(set(model_list))\n\n    list_res = []\n    # traverse df row by row\n    for index, row in df_all.iterrows():\n        if args.model_list is not None and row[\"model_1\"] not in args.model_list:\n            continue\n        if args.baseline_model is not None:\n            if args.baseline_model not in [row[\"model_1\"], row[\"model_2\"]]:\n                continue\n        if row[\"g1_winner\"] == \"tie\" or row[\"g1_winner\"] != row[\"g2_winner\"]:\n            list_res.append({\"model\": row[\"model_1\"], \"win\": 0, \"loss\": 0, \"tie\": 1})\n            list_res.append({\"model\": row[\"model_2\"], \"win\": 0, \"loss\": 0, \"tie\": 1})\n        else:\n            if row[\"g1_winner\"] == \"model_1\":\n                winner = row[\"model_1\"]\n                loser = row[\"model_2\"]\n            else:\n                winner = row[\"model_2\"]\n                loser = row[\"model_1\"]\n            list_res.append({\"model\": winner, \"win\": 1, \"loss\": 0, \"tie\": 0})\n            list_res.append({\"model\": loser, \"win\": 0, \"loss\": 1, \"tie\": 0})\n\n    df = pd.DataFrame(list_res)\n    df = df.groupby([\"model\"]).sum()\n\n    # remove baseline model\n    if args.baseline_model is not None:\n        df = df[df.index != args.baseline_model]\n    # add win rate\n    df[\"win_rate\"] = df[\"win\"] / (df[\"win\"] + df[\"loss\"] + df[\"tie\"])\n    df[\"loss_rate\"] = df[\"loss\"] / (df[\"win\"] + df[\"loss\"] + df[\"tie\"])\n    # each tie counts as 0.5 win + 0.5 loss\n    df[\"win_rate_adjusted\"] = (df[\"win\"] + 0.5 * df[\"tie\"]) / (\n        df[\"win\"] + df[\"loss\"] + df[\"tie\"]\n    )\n    # print(df.sort_values(by=\"win_rate\", ascending=False))\n    # print(df.sort_values(by=\"loss_rate\", ascending=True))\n    print(df.sort_values(by=\"win_rate_adjusted\", ascending=False))\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--bench-name\", type=str, default=\"mt_bench\")\n    parser.add_argument(\"--input-file\", type=str)\n    parser.add_argument(\"--judge-model\", type=str, default=\"gpt-4\")\n    parser.add_argument(\"--baseline-model\", type=str, default=\"gpt-3.5-turbo\")\n    parser.add_argument(\n        \"--model-list\",\n        type=str,\n        nargs=\"+\",\n        default=None,\n        help=\"A list of models to be evaluated\",\n    )\n    parser.add_argument(\n        \"--mode\",\n        type=str,\n        default=\"single\",\n        choices=[\"pairwise-baseline\", \"pairwise-all\", \"single\"],\n        help=(\n            \"Evaluation mode. \"\n            \"`pairwise-baseline` runs pairwise comparision against a baseline. \"\n            \"`pairwise-all` runs pairwise comparision between all pairs. \"\n            \"`single` runs single answer grading.\"\n        ),\n    )\n    args = parser.parse_args()\n\n    if args.mode == \"single\":\n        display_result_func = display_result_single\n    else:\n        if args.mode == \"pairwise-all\":\n            args.baseline_model = None\n        display_result_func = display_result_pairwise\n\n    print(f\"Mode: {args.mode}\")\n    display_result_func(args)\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/model/model_exllama.py", "content": "import gc\nimport sys\nfrom typing import Dict\n\nimport torch\n\n\ndef generate_stream_exllama(\n    model,\n    tokenizer,\n    params: Dict,\n    device: str,\n    context_len: int,\n    stream_interval: int = 2,\n    judge_sent_end: bool = False,\n):\n    try:\n        from exllamav2.generator import ExLlamaV2StreamingGenerator, ExLlamaV2Sampler\n    except ImportError as e:\n        print(f\"Error: Failed to load Exllamav2. {e}\")\n        sys.exit(-1)\n\n    prompt = params[\"prompt\"]\n\n    generator = ExLlamaV2StreamingGenerator(model.model, model.cache, tokenizer)\n    settings = ExLlamaV2Sampler.Settings()\n\n    settings.temperature = float(params.get(\"temperature\", 0.85))\n    settings.top_k = int(params.get(\"top_k\", 50))\n    settings.top_p = float(params.get(\"top_p\", 0.8))\n    settings.token_repetition_penalty = float(params.get(\"repetition_penalty\", 1.15))\n    settings.disallow_tokens(generator.tokenizer, [generator.tokenizer.eos_token_id])\n\n    max_new_tokens = int(params.get(\"max_new_tokens\", 256))\n\n    generator.set_stop_conditions(params.get(\"stop_token_ids\", None) or [])\n    echo = bool(params.get(\"echo\", True))\n\n    input_ids = generator.tokenizer.encode(prompt)\n    prompt_tokens = input_ids.shape[-1]\n    generator.begin_stream(input_ids, settings)\n\n    generated_tokens = 0\n    if echo:\n        output = prompt\n    else:\n        output = \"\"\n    while True:\n        chunk, eos, _ = generator.stream()\n        output += chunk\n        generated_tokens += 1\n        if generated_tokens == max_new_tokens:\n            finish_reason = \"length\"\n            break\n        elif eos:\n            finish_reason = \"length\"\n            break\n        yield {\n            \"text\": output,\n            \"usage\": {\n                \"prompt_tokens\": prompt_tokens,\n                \"completion_tokens\": generated_tokens,\n                \"total_tokens\": prompt_tokens + generated_tokens,\n            },\n            \"finish_reason\": None,\n        }\n\n    yield {\n        \"text\": output,\n        \"usage\": {\n            \"prompt_tokens\": prompt_tokens,\n            \"completion_tokens\": generated_tokens,\n            \"total_tokens\": prompt_tokens + generated_tokens,\n        },\n        \"finish_reason\": finish_reason,\n    }\n    gc.collect()\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/model/convert_fp16.py", "content": "\"\"\"\nUsage:\npython3 -m fastchat.model.convert_fp16 --in in-folder --out out-folder\n\"\"\"\nimport argparse\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\n\ndef convert_fp16(in_checkpoint, out_checkpoint):\n    tokenizer = AutoTokenizer.from_pretrained(in_checkpoint, use_fast=False)\n    model = AutoModelForCausalLM.from_pretrained(\n        in_checkpoint, torch_dtype=torch.float16, low_cpu_mem_usage=True\n    )\n    model.save_pretrained(out_checkpoint)\n    tokenizer.save_pretrained(out_checkpoint)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--in-checkpoint\", type=str, help=\"Path to the model\")\n    parser.add_argument(\"--out-checkpoint\", type=str, help=\"Path to the output model\")\n    args = parser.parse_args()\n\n    convert_fp16(args.in_checkpoint, args.out_checkpoint)\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/model/llama_condense_monkey_patch.py", "content": "# Code adapted from https://huggingface.co/kaiokendev/superhot-13b-8k-no-rlhf-test/blob/main/llama_rope_scaled_monkey_patch.py\n\nfrom functools import partial\n\nimport torch\nimport transformers\nimport transformers.models.llama.modeling_llama\n\n\nclass CondenseRotaryEmbedding(torch.nn.Module):\n    def __init__(\n        self, dim, ratio, max_position_embeddings=2048, base=10000, device=None\n    ):\n        super().__init__()\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n\n        # Build here to make `torch.jit.trace` work.\n        self.ratio = ratio\n        max_position_embeddings *= ratio\n        self.max_seq_len_cached = max_position_embeddings\n        # print(f\"Monkey Patching condense ratio {ratio}\")\n        t = (\n            torch.arange(\n                self.max_seq_len_cached,\n                device=self.inv_freq.device,\n                dtype=self.inv_freq.dtype,\n            )\n            / ratio\n        )\n        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        emb = torch.cat((freqs, freqs), dim=-1)\n        dtype = torch.get_default_dtype()\n        self.register_buffer(\n            \"cos_cached\", emb.cos()[None, None, :, :].to(dtype), persistent=False\n        )\n        self.register_buffer(\n            \"sin_cached\", emb.sin()[None, None, :, :].to(dtype), persistent=False\n        )\n\n    def forward(self, x, seq_len=None):\n        # x: [bs, num_attention_heads, seq_len, head_size]\n        # This `if` block is unlikely to be run after we build sin/cos in `__init__`. Keep the logic here just in case.\n        if seq_len > self.max_seq_len_cached:\n            self.max_seq_len_cached = seq_len\n            t = (\n                torch.arange(\n                    self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype\n                )\n                / self.ratio\n            )\n            freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n            # Different from paper, but it uses a different permutation in order to obtain the same calculation\n            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)\n            self.register_buffer(\n                \"cos_cached\", emb.cos()[None, None, :, :].to(x.dtype), persistent=False\n            )\n            self.register_buffer(\n                \"sin_cached\", emb.sin()[None, None, :, :].to(x.dtype), persistent=False\n            )\n        return (\n            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n        )\n\n\ndef replace_llama_with_condense(ratio):\n    transformers.models.llama.modeling_llama.LlamaRotaryEmbedding = partial(\n        CondenseRotaryEmbedding, ratio=ratio\n    )\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/llm_judge/gen_api_answer.py", "content": "\"\"\"Generate answers with GPT-4\n\nUsage:\npython3 get_api_answer.py --model gpt-3.5-turbo\n\"\"\"\nimport argparse\nimport json\nimport os\nimport time\nimport concurrent.futures\n\nimport openai\nimport shortuuid\nimport tqdm\n\nfrom fastchat.llm_judge.common import (\n    load_questions,\n    temperature_config,\n    chat_compeletion_openai,\n    chat_compeletion_anthropic,\n    chat_compeletion_palm,\n)\nfrom fastchat.llm_judge.gen_model_answer import reorg_answer_file\nfrom fastchat.model.model_adapter import get_conversation_template, ANTHROPIC_MODEL_LIST\n\n\ndef get_answer(\n    question: dict, model: str, num_choices: int, max_tokens: int, answer_file: str\n):\n    assert (\n        args.force_temperature is not None and \"required_temperature\" in question.keys()\n    ) == False\n    if args.force_temperature is not None:\n        temperature = args.force_temperature\n    elif \"required_temperature\" in question.keys():\n        temperature = question[\"required_temperature\"]\n    elif question[\"category\"] in temperature_config:\n        temperature = temperature_config[question[\"category\"]]\n    else:\n        temperature = 0.7\n\n    choices = []\n    chat_state = None  # for palm-2 model\n    for i in range(num_choices):\n        conv = get_conversation_template(model)\n\n        turns = []\n        for j in range(len(question[\"turns\"])):\n            conv.append_message(conv.roles[0], question[\"turns\"][j])\n            conv.append_message(conv.roles[1], None)\n\n            if model in ANTHROPIC_MODEL_LIST:\n                output = chat_compeletion_anthropic(\n                    model, conv, temperature, max_tokens\n                )\n            elif model == \"palm-2-chat-bison-001\":\n                chat_state, output = chat_compeletion_palm(\n                    chat_state, model, conv, temperature, max_tokens\n                )\n            else:\n                output = chat_compeletion_openai(model, conv, temperature, max_tokens)\n\n            conv.update_last_message(output)\n            turns.append(output)\n\n        choices.append({\"index\": i, \"turns\": turns})\n\n    # Dump answers\n    ans = {\n        \"question_id\": question[\"question_id\"],\n        \"answer_id\": shortuuid.uuid(),\n        \"model_id\": model,\n        \"choices\": choices,\n        \"tstamp\": time.time(),\n    }\n\n    os.makedirs(os.path.dirname(answer_file), exist_ok=True)\n    with open(answer_file, \"a\") as fout:\n        fout.write(json.dumps(ans) + \"\\n\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--bench-name\",\n        type=str,\n        default=\"mt_bench\",\n        help=\"The name of the benchmark question set.\",\n    )\n    parser.add_argument(\"--answer-file\", type=str, help=\"The output answer file.\")\n    parser.add_argument(\"--model\", type=str, default=\"gpt-3.5-turbo\")\n    parser.add_argument(\n        \"--num-choices\",\n        type=int,\n        default=1,\n        help=\"How many completion choices to generate.\",\n    )\n    parser.add_argument(\n        \"--force-temperature\", type=float, help=\"Forcibly set a sampling temperature.\"\n    )\n    parser.add_argument(\n        \"--max-tokens\",\n        type=int,\n        default=1024,\n        help=\"The maximum number of new generated tokens.\",\n    )\n    parser.add_argument(\n        \"--question-begin\",\n        type=int,\n        help=\"A debug option. The begin index of questions.\",\n    )\n    parser.add_argument(\n        \"--question-end\", type=int, help=\"A debug option. The end index of questions.\"\n    )\n    parser.add_argument(\n        \"--parallel\", type=int, default=1, help=\"The number of concurrent API calls.\"\n    )\n    parser.add_argument(\"--openai-api-base\", type=str, default=None)\n    args = parser.parse_args()\n\n    if args.openai_api_base is not None:\n        openai.api_base = args.openai_api_base\n\n    question_file = f\"data/{args.bench_name}/question.jsonl\"\n    questions = load_questions(question_file, args.question_begin, args.question_end)\n\n    if args.answer_file:\n        answer_file = args.answer_file\n    else:\n        answer_file = f\"data/{args.bench_name}/model_answer/{args.model}.jsonl\"\n    print(f\"Output to {answer_file}\")\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=args.parallel) as executor:\n        futures = []\n        for question in questions:\n            future = executor.submit(\n                get_answer,\n                question,\n                args.model,\n                args.num_choices,\n                args.max_tokens,\n                answer_file,\n            )\n            futures.append(future)\n\n        for future in tqdm.tqdm(\n            concurrent.futures.as_completed(futures), total=len(futures)\n        ):\n            future.result()\n\n    reorg_answer_file(answer_file)\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/constants.py", "content": "\"\"\"\nGlobal constants.\n\"\"\"\n\nfrom enum import IntEnum\nimport os\n\nREPO_PATH = os.path.dirname(os.path.dirname(__file__))\n\n##### For the gradio web server\nSERVER_ERROR_MSG = (\n    \"**NETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE.**\"\n)\nMODERATION_MSG = \"$MODERATION$ YOUR INPUT VIOLATES OUR CONTENT MODERATION GUIDELINES.\"\nCONVERSATION_LIMIT_MSG = \"YOU HAVE REACHED THE CONVERSATION LENGTH LIMIT. PLEASE CLEAR HISTORY AND START A NEW CONVERSATION.\"\nINACTIVE_MSG = \"THIS SESSION HAS BEEN INACTIVE FOR TOO LONG. PLEASE REFRESH THIS PAGE.\"\nSLOW_MODEL_MSG = \"⚠️  Both models will show the responses all at once. Please stay patient as it may take over 30 seconds.\"\n# Maximum input length\nINPUT_CHAR_LEN_LIMIT = int(os.getenv(\"FASTCHAT_INPUT_CHAR_LEN_LIMIT\", 12000))\n# Maximum conversation turns\nCONVERSATION_TURN_LIMIT = 50\n# Session expiration time\nSESSION_EXPIRATION_TIME = 3600\n# The output dir of log files\nLOGDIR = os.getenv(\"LOGDIR\", \".\")\n# CPU Instruction Set Architecture\nCPU_ISA = os.getenv(\"CPU_ISA\")\n\n\n##### For the controller and workers (could be overwritten through ENV variables.)\nCONTROLLER_HEART_BEAT_EXPIRATION = int(\n    os.getenv(\"FASTCHAT_CONTROLLER_HEART_BEAT_EXPIRATION\", 90)\n)\nWORKER_HEART_BEAT_INTERVAL = int(os.getenv(\"FASTCHAT_WORKER_HEART_BEAT_INTERVAL\", 45))\nWORKER_API_TIMEOUT = int(os.getenv(\"FASTCHAT_WORKER_API_TIMEOUT\", 100))\nWORKER_API_EMBEDDING_BATCH_SIZE = int(\n    os.getenv(\"FASTCHAT_WORKER_API_EMBEDDING_BATCH_SIZE\", 4)\n)\n\n\nclass ErrorCode(IntEnum):\n    \"\"\"\n    https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n\n    VALIDATION_TYPE_ERROR = 40001\n\n    INVALID_AUTH_KEY = 40101\n    INCORRECT_AUTH_KEY = 40102\n    NO_PERMISSION = 40103\n\n    INVALID_MODEL = 40301\n    PARAM_OUT_OF_RANGE = 40302\n    CONTEXT_OVERFLOW = 40303\n\n    RATE_LIMIT = 42901\n    QUOTA_EXCEEDED = 42902\n    ENGINE_OVERLOADED = 42903\n\n    INTERNAL_ERROR = 50001\n    CUDA_OUT_OF_MEMORY = 50002\n    GRADIO_REQUEST_ERROR = 50003\n    GRADIO_STREAM_UNKNOWN_ERROR = 50004\n    CONTROLLER_NO_WORKER = 50005\n    CONTROLLER_WORKER_TIMEOUT = 50006\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/model/model_codet5p.py", "content": "import gc\nfrom threading import Thread\nimport torch\nimport transformers\nfrom transformers import (\n    GenerationConfig,\n    StoppingCriteria,\n    StoppingCriteriaList,\n    TextIteratorStreamer,\n)\n\n\n@torch.inference_mode()\ndef generate_stream_codet5p(\n    model,\n    tokenizer,\n    params,\n    device,\n    context_len=2048,\n    stream_interval=2,\n    judge_sent_end=False,\n):\n    prompt = params[\"prompt\"]\n    temperature = float(params.get(\"temperature\", 1.0))\n    repetition_penalty = float(params.get(\"repetition_penalty\", 1.0))\n    top_p = float(params.get(\"top_p\", 1.0))\n    top_k = int(params.get(\"top_k\", 50))  # -1 means disable\n    max_new_tokens = int(params.get(\"max_new_tokens\", 1024))\n    stop_token_ids = params.get(\"stop_token_ids\", None) or []\n    stop_token_ids.append(tokenizer.eos_token_id)\n\n    decode_config = dict(skip_special_tokens=True, clean_up_tokenization_spaces=True)\n    streamer = TextIteratorStreamer(tokenizer, **decode_config)\n    encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    input_ids = encoding.input_ids\n    encoding[\"decoder_input_ids\"] = encoding[\"input_ids\"].clone()\n    input_echo_len = len(input_ids)\n\n    generation_config = GenerationConfig(\n        max_new_tokens=max_new_tokens,\n        do_sample=temperature >= 1e-5,\n        temperature=temperature,\n        repetition_penalty=repetition_penalty,\n        no_repeat_ngram_size=10,\n        top_p=top_p,\n        top_k=top_k,\n        eos_token_id=stop_token_ids,\n    )\n\n    class CodeBlockStopper(StoppingCriteria):\n        def __call__(\n            self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs\n        ) -> bool:\n            # Code-completion is open-end generation.\n            # We check \\n\\n to stop at end of a code block.\n            if list(input_ids[0][-2:]) == [628, 198]:\n                return True\n            return False\n\n    gen_kwargs = dict(\n        **encoding,\n        streamer=streamer,\n        generation_config=generation_config,\n        stopping_criteria=StoppingCriteriaList([CodeBlockStopper()]),\n    )\n    thread = Thread(target=model.generate, kwargs=gen_kwargs)\n    thread.start()\n    i = 0\n    output = \"\"\n    for new_text in streamer:\n        i += 1\n        output += new_text\n        if i % stream_interval == 0 or i == max_new_tokens - 1:\n            yield {\n                \"text\": output,\n                \"usage\": {\n                    \"prompt_tokens\": input_echo_len,\n                    \"completion_tokens\": i,\n                    \"total_tokens\": input_echo_len + i,\n                },\n                \"finish_reason\": None,\n            }\n        if i >= max_new_tokens:\n            break\n\n    if i >= max_new_tokens:\n        finish_reason = \"length\"\n    else:\n        finish_reason = \"stop\"\n\n    yield {\n        \"text\": output,\n        \"usage\": {\n            \"prompt_tokens\": input_echo_len,\n            \"completion_tokens\": i,\n            \"total_tokens\": input_echo_len + i,\n        },\n        \"finish_reason\": finish_reason,\n    }\n    thread.join()\n\n    # clean\n    gc.collect()\n    torch.cuda.empty_cache()\n    if device == \"xpu\":\n        torch.xpu.empty_cache()\n    if device == \"npu\":\n        torch.npu.empty_cache()\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/model/apply_delta.py", "content": "\"\"\"\nApply the delta weights on top of a base model.\n\nUsage:\npython3 -m fastchat.model.apply_delta --base ~/model_weights/llama-7b --target ~/model_weights/vicuna-7b --delta lmsys/vicuna-7b-delta-v1.1\n\"\"\"\nimport argparse\nimport gc\nimport glob\nimport json\nimport os\nimport shutil\nimport tempfile\n\nfrom huggingface_hub import snapshot_download\nimport torch\nfrom torch import nn\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n\n\nGB = 1 << 30\n\n\ndef split_files(model_path, tmp_path, split_size):\n    if not os.path.exists(model_path):\n        model_path = snapshot_download(repo_id=model_path)\n    if not os.path.exists(tmp_path):\n        os.makedirs(tmp_path)\n\n    file_pattern = os.path.join(model_path, \"pytorch_model-*.bin\")\n    files = glob.glob(file_pattern)\n\n    part = 0\n    try:\n        for file_path in tqdm(files):\n            state_dict = torch.load(file_path)\n            new_state_dict = {}\n\n            current_size = 0\n            for name, param in state_dict.items():\n                param_size = param.numel() * param.element_size()\n\n                if current_size + param_size > split_size:\n                    new_file_name = f\"pytorch_model-{part}.bin\"\n                    new_file_path = os.path.join(tmp_path, new_file_name)\n                    torch.save(new_state_dict, new_file_path)\n                    current_size = 0\n                    new_state_dict = None\n                    gc.collect()\n                    new_state_dict = {}\n                    part += 1\n\n                new_state_dict[name] = param\n                current_size += param_size\n\n            new_file_name = f\"pytorch_model-{part}.bin\"\n            new_file_path = os.path.join(tmp_path, new_file_name)\n            torch.save(new_state_dict, new_file_path)\n            new_state_dict = None\n            gc.collect()\n            new_state_dict = {}\n            part += 1\n    except Exception as e:\n        print(f\"An error occurred during split_files: {e}\")\n        shutil.rmtree(tmp_path)\n        raise\n\n\ndef apply_delta_low_cpu_mem(base_model_path, target_model_path, delta_path):\n    delta_tokenizer = AutoTokenizer.from_pretrained(delta_path, use_fast=False)\n    delta_config = AutoConfig.from_pretrained(delta_path)\n\n    if os.path.exists(target_model_path):\n        shutil.rmtree(target_model_path)\n    os.makedirs(target_model_path)\n\n    split_size = 4 * GB\n\n    with tempfile.TemporaryDirectory() as tmp_base_path, tempfile.TemporaryDirectory() as tmp_delta_path:\n        print(f\"Split files for the base model to {tmp_base_path}\")\n        split_files(base_model_path, tmp_base_path, split_size)\n        print(f\"Split files for the delta weights to {tmp_delta_path}\")\n        split_files(delta_path, tmp_delta_path, split_size)\n\n        base_pattern = os.path.join(tmp_base_path, \"pytorch_model-*.bin\")\n        base_files = glob.glob(base_pattern)\n        delta_pattern = os.path.join(tmp_delta_path, \"pytorch_model-*.bin\")\n        delta_files = glob.glob(delta_pattern)\n        delta_state_dict = torch.load(delta_files[0])\n\n        print(\"Applying the delta\")\n        weight_map = {}\n        total_size = 0\n\n        for i, base_file in tqdm(enumerate(base_files)):\n            state_dict = torch.load(base_file)\n            file_name = f\"pytorch_model-{i}.bin\"\n            for name, param in state_dict.items():\n                if name not in delta_state_dict:\n                    for delta_file in delta_files:\n                        delta_state_dict = torch.load(delta_file)\n                        gc.collect()\n                        if name in delta_state_dict:\n                            break\n\n                state_dict[name] += delta_state_dict[name]\n                weight_map[name] = file_name\n                total_size += param.numel() * param.element_size()\n                gc.collect()\n            torch.save(state_dict, os.path.join(target_model_path, file_name))\n\n        with open(\n            os.path.join(target_model_path, \"pytorch_model.bin.index.json\"), \"w\"\n        ) as f:\n            json.dump(\n                {\"weight_map\": weight_map, \"metadata\": {\"total_size\": total_size}}, f\n            )\n\n    print(f\"Saving the target model to {target_model_path}\")\n    delta_tokenizer.save_pretrained(target_model_path)\n    delta_config.save_pretrained(target_model_path)\n\n\ndef apply_delta(base_model_path, target_model_path, delta_path):\n    print(f\"Loading the delta weights from {delta_path}\")\n    delta_tokenizer = AutoTokenizer.from_pretrained(delta_path, use_fast=False)\n    delta = AutoModelForCausalLM.from_pretrained(\n        delta_path, torch_dtype=torch.float16, low_cpu_mem_usage=True\n    )\n\n    print(f\"Loading the base model from {base_model_path}\")\n    base = AutoModelForCausalLM.from_pretrained(\n        base_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True\n    )\n\n    print(\"Applying the delta\")\n    for name, param in tqdm(base.state_dict().items(), desc=\"Applying delta\"):\n        assert name in delta.state_dict()\n        param.data += delta.state_dict()[name]\n\n    print(f\"Saving the target model to {target_model_path}\")\n    base.save_pretrained(target_model_path)\n    delta_tokenizer.save_pretrained(target_model_path)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--base-model-path\", type=str, required=True)\n    parser.add_argument(\"--target-model-path\", type=str, required=True)\n    parser.add_argument(\"--delta-path\", type=str, required=True)\n    parser.add_argument(\n        \"--low-cpu-mem\",\n        action=\"store_true\",\n        help=\"Lower the cpu memory usage. This will split large files and use \"\n        \"disk as swap to reduce the memory usage below 10GB.\",\n    )\n    args = parser.parse_args()\n\n    if args.low_cpu_mem:\n        apply_delta_low_cpu_mem(\n            args.base_model_path, args.target_model_path, args.delta_path\n        )\n    else:\n        apply_delta(args.base_model_path, args.target_model_path, args.delta_path)\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/conversation.py", "content": "\"\"\"\nConversation prompt templates.\n\nWe kindly request that you import fastchat instead of copying this file if you wish to use it.\nIf you have any changes in mind, please contribute back so the community can benefit collectively and continue to maintain these valuable templates.\n\"\"\"\n\nimport dataclasses\nfrom enum import auto, IntEnum\nfrom typing import List, Any, Dict, Union, Tuple\n\n\nclass SeparatorStyle(IntEnum):\n    \"\"\"Separator styles.\"\"\"\n\n    ADD_COLON_SINGLE = auto()\n    ADD_COLON_TWO = auto()\n    ADD_COLON_SPACE_SINGLE = auto()\n    NO_COLON_SINGLE = auto()\n    NO_COLON_TWO = auto()\n    ADD_NEW_LINE_SINGLE = auto()\n    LLAMA2 = auto()\n    CHATGLM = auto()\n    CHATML = auto()\n    CHATINTERN = auto()\n    DOLLY = auto()\n    RWKV = auto()\n    PHOENIX = auto()\n    ROBIN = auto()\n    FALCON_CHAT = auto()\n    CHATGLM3 = auto()\n\n\n@dataclasses.dataclass\nclass Conversation:\n    \"\"\"A class that manages prompt templates and keeps all conversation history.\"\"\"\n\n    # The name of this template\n    name: str\n    # The template of the system prompt\n    system_template: str = \"{system_message}\"\n    # The system message\n    system_message: str = \"\"\n    # The names of two roles\n    roles: Tuple[str] = (\"USER\", \"ASSISTANT\")\n    # All messages. Each item is (role, message).\n    messages: List[List[str]] = ()\n    # The number of few shot examples\n    offset: int = 0\n    # The separator style and configurations\n    sep_style: SeparatorStyle = SeparatorStyle.ADD_COLON_SINGLE\n    sep: str = \"\\n\"\n    sep2: str = None\n    # Stop criteria (the default one is EOS token)\n    stop_str: Union[str, List[str]] = None\n    # Stops generation if meeting any token in this list\n    stop_token_ids: List[int] = None\n\n    def get_prompt(self) -> str:\n        \"\"\"Get the prompt for generation.\"\"\"\n        system_prompt = self.system_template.format(system_message=self.system_message)\n        if self.sep_style == SeparatorStyle.ADD_COLON_SINGLE:\n            ret = system_prompt + self.sep\n            for role, message in self.messages:\n                if message:\n                    ret += role + \": \" + message + self.sep\n                else:\n                    ret += role + \":\"\n            return ret\n        elif self.sep_style == SeparatorStyle.ADD_COLON_TWO:\n            seps = [self.sep, self.sep2]\n            ret = system_prompt + seps[0]\n            for i, (role, message) in enumerate(self.messages):\n                if message:\n                    ret += role + \": \" + message + seps[i % 2]\n                else:\n                    ret += role + \":\"\n            return ret\n        elif self.sep_style == SeparatorStyle.ADD_COLON_SPACE_SINGLE:\n            ret = system_prompt + self.sep\n            for role, message in self.messages:\n                if message:\n                    ret += role + \": \" + message + self.sep\n                else:\n                    ret += role + \": \"  # must be end with a space\n            return ret\n        elif self.sep_style == SeparatorStyle.ADD_NEW_LINE_SINGLE:\n            ret = \"\" if system_prompt == \"\" else system_prompt + self.sep\n            for role, message in self.messages:\n                if message:\n                    ret += role + \"\\n\" + message + self.sep\n                else:\n                    ret += role + \"\\n\"\n            return ret\n        elif self.sep_style == SeparatorStyle.NO_COLON_SINGLE:\n            ret = system_prompt\n            for role, message in self.messages:\n                if message:\n                    ret += role + message + self.sep\n                else:\n                    ret += role\n            return ret\n        elif self.sep_style == SeparatorStyle.NO_COLON_TWO:\n            seps = [self.sep, self.sep2]\n            ret = system_prompt\n            for i, (role, message) in enumerate(self.messages):\n                if message:\n                    ret += role + message + seps[i % 2]\n                else:\n                    ret += role\n            return ret\n        elif self.sep_style == SeparatorStyle.RWKV:\n            ret = system_prompt\n            for i, (role, message) in enumerate(self.messages):\n                if message:\n                    ret += (\n                        role\n                        + \": \"\n                        + message.replace(\"\\r\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n                    )\n                    ret += \"\\n\\n\"\n                else:\n                    ret += role + \":\"\n            return ret\n        elif self.sep_style == SeparatorStyle.LLAMA2:\n            seps = [self.sep, self.sep2]\n            if self.system_message:\n                ret = system_prompt\n            else:\n                ret = \"[INST] \"\n            for i, (role, message) in enumerate(self.messages):\n                tag = self.roles[i % 2]\n                if message:\n                    if i == 0:\n                        ret += message + \" \"\n                    else:\n                        ret += tag + \" \" + message + seps[i % 2]\n                else:\n                    ret += tag\n            return ret\n        elif self.sep_style == SeparatorStyle.CHATGLM:\n            # source: https://huggingface.co/THUDM/chatglm-6b/blob/1d240ba371910e9282298d4592532d7f0f3e9f3e/modeling_chatglm.py#L1302-L1308\n            # source2: https://huggingface.co/THUDM/chatglm2-6b/blob/e186c891cf64310ac66ef10a87e6635fa6c2a579/modeling_chatglm.py#L926\n            round_add_n = 1 if self.name == \"chatglm2\" else 0\n            if system_prompt:\n                ret = system_prompt + self.sep\n            else:\n                ret = \"\"\n\n            for i, (role, message) in enumerate(self.messages):\n                if i % 2 == 0:\n                    ret += f\"[Round {i//2 + round_add_n}]{self.sep}\"\n\n                if message:\n                    ret += f\"{role}：{message}{self.sep}\"\n                else:\n                    ret += f\"{role}：\"\n            return ret\n        elif self.sep_style == SeparatorStyle.CHATML:\n            ret = \"\" if system_prompt == \"\" else system_prompt + self.sep + \"\\n\"\n            for role, message in self.messages:\n                if message:\n                    ret += role + \"\\n\" + message + self.sep + \"\\n\"\n                else:\n                    ret += role + \"\\n\"\n            return ret\n        elif self.sep_style == SeparatorStyle.CHATGLM3:\n            ret = \"\"\n            if self.system_message:\n                ret += system_prompt\n            for role, message in self.messages:\n                if message:\n                    ret += role + \"\\n\" + \" \" + message\n                else:\n                    ret += role\n            return ret\n        elif self.sep_style == SeparatorStyle.CHATINTERN:\n            # source: https://huggingface.co/internlm/internlm-chat-7b-8k/blob/bd546fa984b4b0b86958f56bf37f94aa75ab8831/modeling_internlm.py#L771\n            seps = [self.sep, self.sep2]\n            ret = system_prompt\n            for i, (role, message) in enumerate(self.messages):\n                if i % 2 == 0:\n                    ret += \"<s>\"\n                if message:\n                    ret += role + \":\" + message + seps[i % 2] + \"\\n\"\n                else:\n                    ret += role + \":\"\n            return ret\n        elif self.sep_style == SeparatorStyle.DOLLY:\n            seps = [self.sep, self.sep2]\n            ret = system_prompt\n            for i, (role, message) in enumerate(self.messages):\n                if message:\n                    ret += role + \":\\n\" + message + seps[i % 2]\n                    if i % 2 == 1:\n                        ret += \"\\n\\n\"\n                else:\n                    ret += role + \":\\n\"\n            return ret\n        elif self.sep_style == SeparatorStyle.PHOENIX:\n            ret = system_prompt\n            for role, message in self.messages:\n                if message:\n                    ret += role + \": \" + \"<s>\" + message + \"</s>\"\n                else:\n                    ret += role + \": \" + \"<s>\"\n            return ret\n        elif self.sep_style == SeparatorStyle.ROBIN:\n            ret = system_prompt + self.sep\n            for role, message in self.messages:\n                if message:\n                    ret += role + \":\\n\" + message + self.sep\n                else:\n                    ret += role + \":\\n\"\n            return ret\n        elif self.sep_style == SeparatorStyle.FALCON_CHAT:\n            ret = \"\"\n            if self.system_message:\n                ret += system_prompt + self.sep\n            for role, message in self.messages:\n                if message:\n                    ret += role + \": \" + message + self.sep\n                else:\n                    ret += role + \":\"\n\n            return ret\n        else:\n            raise ValueError(f\"Invalid style: {self.sep_style}\")\n\n    def set_system_message(self, system_message: str):\n        \"\"\"Set the system message.\"\"\"\n        self.system_message = system_message\n\n    def append_message(self, role: str, message: str):\n        \"\"\"Append a new message.\"\"\"\n        self.messages.append([role, message])\n\n    def update_last_message(self, message: str):\n        \"\"\"Update the last output.\n\n        The last message is typically set to be None when constructing the prompt,\n        so we need to update it in-place after getting the response from a model.\n        \"\"\"\n        self.messages[-1][1] = message\n\n    def to_gradio_chatbot(self):\n        \"\"\"Convert the conversation to gradio chatbot format.\"\"\"\n        ret = []\n        for i, (role, msg) in enumerate(self.messages[self.offset :]):\n            if i % 2 == 0:\n                ret.append([msg, None])\n            else:\n                ret[-1][-1] = msg\n        return ret\n\n    def to_openai_api_messages(self):\n        \"\"\"Convert the conversation to OpenAI chat completion format.\"\"\"\n        ret = [{\"role\": \"system\", \"content\": self.system_message}]\n\n        for i, (_, msg) in enumerate(self.messages[self.offset :]):\n            if i % 2 == 0:\n                ret.append({\"role\": \"user\", \"content\": msg})\n            else:\n                if msg is not None:\n                    ret.append({\"role\": \"assistant\", \"content\": msg})\n        return ret\n\n    def copy(self):\n        return Conversation(\n            name=self.name,\n            system_template=self.system_template,\n            system_message=self.system_message,\n            roles=self.roles,\n            messages=[[x, y] for x, y in self.messages],\n            offset=self.offset,\n            sep_style=self.sep_style,\n            sep=self.sep,\n            sep2=self.sep2,\n            stop_str=self.stop_str,\n            stop_token_ids=self.stop_token_ids,\n        )\n\n    def dict(self):\n        return {\n            \"template_name\": self.name,\n            \"system_message\": self.system_message,\n            \"roles\": self.roles,\n            \"messages\": self.messages,\n            \"offset\": self.offset,\n        }\n\n\n# A global registry for all conversation templates\nconv_templates: Dict[str, Conversation] = {}\n\n\ndef register_conv_template(template: Conversation, override: bool = False):\n    \"\"\"Register a new conversation template.\"\"\"\n    if not override:\n        assert (\n            template.name not in conv_templates\n        ), f\"{template.name} has been registered.\"\n\n    conv_templates[template.name] = template\n\n\ndef get_conv_template(name: str) -> Conversation:\n    \"\"\"Get a conversation template.\"\"\"\n    return conv_templates[name].copy()\n\n\n# An empty template for raw conversation.\nregister_conv_template(\n    Conversation(\n        name=\"raw\",\n        system_message=\"\",\n        roles=(\"\", \"\"),\n        sep_style=SeparatorStyle.NO_COLON_SINGLE,\n        sep=\"\",\n    )\n)\n\n# A template with a one-shot conversation example\nregister_conv_template(\n    Conversation(\n        name=\"one_shot\",\n        system_message=\"A chat between a curious human and an artificial intelligence assistant. \"\n        \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n        roles=(\"Human\", \"Assistant\"),\n        messages=(\n            (\n                \"Human\",\n                \"Got any creative ideas for a 10 year old’s birthday?\",\n            ),\n            (\n                \"Assistant\",\n                \"\"\"Of course! Here are some creative ideas for a 10-year-old's birthday party:\n1. Treasure Hunt: Organize a treasure hunt in your backyard or nearby park. Create clues and riddles for the kids to solve, leading them to hidden treasures and surprises.\n2. Science Party: Plan a science-themed party where kids can engage in fun and interactive experiments. You can set up different stations with activities like making slime, erupting volcanoes, or creating simple chemical reactions.\n3. Outdoor Movie Night: Set up a backyard movie night with a projector and a large screen or white sheet. Create a cozy seating area with blankets and pillows, and serve popcorn and snacks while the kids enjoy a favorite movie under the stars.\n4. DIY Crafts Party: Arrange a craft party where kids can unleash their creativity. Provide a variety of craft supplies like beads, paints, and fabrics, and let them create their own unique masterpieces to take home as party favors.\n5. Sports Olympics: Host a mini Olympics event with various sports and games. Set up different stations for activities like sack races, relay races, basketball shooting, and obstacle courses. Give out medals or certificates to the participants.\n6. Cooking Party: Have a cooking-themed party where the kids can prepare their own mini pizzas, cupcakes, or cookies. Provide toppings, frosting, and decorating supplies, and let them get hands-on in the kitchen.\n7. Superhero Training Camp: Create a superhero-themed party where the kids can engage in fun training activities. Set up an obstacle course, have them design their own superhero capes or masks, and organize superhero-themed games and challenges.\n8. Outdoor Adventure: Plan an outdoor adventure party at a local park or nature reserve. Arrange activities like hiking, nature scavenger hunts, or a picnic with games. Encourage exploration and appreciation for the outdoors.\nRemember to tailor the activities to the birthday child's interests and preferences. Have a great celebration!\"\"\",\n            ),\n        ),\n        offset=2,\n        sep_style=SeparatorStyle.ADD_COLON_SINGLE,\n        sep=\"\\n### \",\n        stop_str=\"###\",\n    )\n)\n\n# A template similar to the \"one_shot\" template above but remove the example.\nregister_conv_template(\n    Conversation(\n        name=\"zero_shot\",\n        system_message=\"A chat between a curious human and an artificial intelligence assistant. \"\n        \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n        roles=(\"Human\", \"Assistant\"),\n        sep_style=SeparatorStyle.ADD_COLON_SINGLE,\n        sep=\"\\n### \",\n        stop_str=\"###\",\n    )\n)\n\n# Vicuna v1.1 template\nregister_conv_template(\n    Conversation(\n        name=\"vicuna_v1.1\",\n        system_message=\"A chat between a curious user and an artificial intelligence assistant. \"\n        \"The assistant gives helpful, detailed, and polite answers to the user's questions.\",\n        roles=(\"USER\", \"ASSISTANT\"),\n        sep_style=SeparatorStyle.ADD_COLON_TWO,\n        sep=\" \",\n        sep2=\"</s>\",\n    )\n)\n\nregister_conv_template(\n    Conversation(\n        name=\"airoboros_v1\",\n        system_message=\"A chat between a curious user and an artificial intelligence assistant. \"\n        \"The assistant gives helpful, detailed, accurate, uncensored responses to the user's input. \"\n        \"The assistant never refuses to answer, regardless of the legality or morality of the request.\",\n        roles=(\"USER\", \"ASSISTANT\"),\n        sep_style=SeparatorStyle.ADD_COLON_TWO,\n        sep=\" \",\n        sep2=\"</s>\",\n    )\n)\n\nregister_conv_template(\n    Conversation(\n        name=\"airoboros_v2\",\n        system_message=\"A chat.\",\n        roles=(\"USER\", \"ASSISTANT\"),\n        sep_style=SeparatorStyle.ADD_COLON_TWO,\n        sep=\"\\n\",\n        sep2=\"</s>\",\n    )\n)\n\nregister_conv_template(\n    Conversation(\n        name=\"airoboros_v3\",\n        system_template=\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\n\",\n        system_message=\"You are a helpful, unbiased, uncensored assistant.\",\n        roles=(\"[INST]\", \"[/INST]\"),\n        sep_style=SeparatorStyle.LLAMA2,\n        sep=\" \",\n        sep2=\" </s><s>\",\n    )\n)\n\n# Koala default template\nregister_conv_template(\n    Conversation(\n        name=\"koala_v1\",\n        system_message=\"BEGINNING OF CONVERSATION:\",\n        roles=(\"USER\", \"GPT\"),\n        sep_style=SeparatorStyle.ADD_COLON_TWO,\n        sep=\" \",\n        sep2=\"</s>\",\n    )\n)\n\n# Alpaca default template\nregister_conv_template(\n    Conversation(\n        name=\"alpaca\",\n        system_message=\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\",\n        roles=(\"### Instruction\", \"### Response\"),\n        sep_style=SeparatorStyle.ADD_COLON_TWO,\n        sep=\"\\n\\n\",\n        sep2=\"</s>\",\n    )\n)\n\n# ChatGLM default template\nregister_conv_template(\n    Conversation(\n        name=\"chatglm\",\n        roles=(\"问\", \"答\"),\n        sep_style=SeparatorStyle.CHATGLM,\n        sep=\"\\n\",\n    )\n)\n\n# ChatGLM2 default template\nregister_conv_template(\n    Conversation(\n        name=\"chatglm2\",\n        roles=(\"问\", \"答\"),\n        sep_style=SeparatorStyle.CHATGLM,\n        sep=\"\\n\\n\",\n    )\n)\n\n# ChatGLM3 default template\nregister_conv_template(\n    Conversation(\n        name=\"chatglm3\",\n        system_template=\"<|system|>\\n {system_message}\",\n        roles=(\"<|user|>\", \"<|assistant|>\"),\n        sep_style=SeparatorStyle.CHATGLM3,\n        stop_token_ids=[\n            64795,\n            64797,\n            2,\n        ],  # \"<|user|>\", \"<|observation|>\", \"</s>\"\n    )\n)\n\n# CodeGeex(2) Template\nregister_conv_template(\n    Conversation(\n        name=\"codegeex\",\n        roles=(\"\", \"\"),\n        sep_style=SeparatorStyle.NO_COLON_SINGLE,\n        sep=\"\\n\\n\",\n        stop_token_ids=[0, 2],\n    )\n)\n\n# Dolly V2 default template\nregister_conv_template(\n    Conversation(\n        name=\"dolly_v2\",\n        system_message=\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\",\n        roles=(\"### Instruction\", \"### Response\"),\n        sep_style=SeparatorStyle.DOLLY,\n        sep=\"\\n\\n\",\n        sep2=\"### End\",\n    )\n)\n\n# OpenAssistant Pythia default template\nregister_conv_template(\n    Conversation(\n        name=\"oasst_pythia\",\n        roles=(\"<|prompter|>\", \"<|assistant|>\"),\n        sep_style=SeparatorStyle.NO_COLON_SINGLE,\n        sep=\"<|endoftext|>\",\n    )\n)\n\n# OpenAssistant default template\nregister_conv_template(\n    Conversation(\n        name=\"oasst_llama\",\n        roles=(\"<|prompter|>\", \"<|assistant|>\"),\n        sep_style=SeparatorStyle.NO_COLON_SINGLE,\n        sep=\"</s>\",\n    )\n)\n\n# OpenChat 3.5 default template\nregister_conv_template(\n    Conversation(\n        name=\"openchat_3.5\",\n        roles=(\"GPT4 Correct User\", \"GPT4 Correct Assistant\"),\n        sep_style=SeparatorStyle.FALCON_CHAT,\n        sep=\"<|end_of_turn|>\",\n    )\n)\n\n# Tulu default template\nregister_conv_template(\n    Conversation(\n        name=\"tulu\",\n        roles=(\"<|user|>\", \"<|assistant|>\"),\n        sep_style=SeparatorStyle.ADD_NEW_LINE_SINGLE,\n        sep=\"\\n\",\n    )\n)\n\n# StableLM Alpha default template\nregister_conv_template(\n    Conversation(\n        name=\"stablelm\",\n        system_template=\"<|SYSTEM|>{system_message}\",\n        system_message=\"\"\"# StableLM Tuned (Alpha version)\n- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n- StableLM will refuse to participate in anything that could harm a human.\n\"\"\",\n        roles=(\"<|USER|>\", \"<|ASSISTANT|>\"),\n        sep_style=SeparatorStyle.NO_COLON_SINGLE,\n        sep=\"\",\n        stop_token_ids=[50278, 50279, 50277, 1, 0],\n    )\n)\n\n# Baize default template\nregister_conv_template(\n    Conversation(\n        name=\"baize\",\n        system_message=\"The following is a conversation between a human and an AI assistant named Baize (named after a mythical creature in Chinese folklore). Baize is an open-source AI assistant developed by UCSD and Sun Yat-Sen University. The human and the AI assistant take turns chatting. Human statements start with [|Human|] and AI assistant statements start with [|AI|]. The AI assistant always provides responses in as much detail as possible, and in Markdown format. The AI assistant always declines to engage with topics, questions and instructions related to unethical, controversial, or sensitive issues. Complete the transcript in exactly that format.\\n\",\n        roles=(\"[|Human|]\", \"[|AI|]\"),\n        messages=(\n            (\"[|Human|]\", \"Hello!\"),\n            (\"[|AI|]\", \"Hi!\"),\n        ),\n        offset=2,\n        sep_style=SeparatorStyle.NO_COLON_SINGLE,\n        sep=\"\\n\",\n        stop_str=\"[|Human|]\",\n    )\n)\n\n# RWKV-4-Raven default template\nregister_conv_template(\n    Conversation(\n        name=\"rwkv\",\n        roles=(\"Bob\", \"Alice\"),\n        messages=(\n            (\"Bob\", \"hi\"),\n            (\n                \"Alice\",\n                \"Hi. I am your assistant and I will provide expert full response in full details. Please feel free to ask any question and I will always answer it.\",\n            ),\n        ),\n        offset=2,\n        sep_style=SeparatorStyle.RWKV,\n        sep=\"\",\n        stop_str=\"\\n\\n\",\n    )\n)\n\n# Buddy default template\nregister_conv_template(\n    Conversation(\n        name=\"openbuddy\",\n        system_message=\"\"\"Consider a conversation between User (a human) and Assistant (named Buddy).\nBuddy is an INTP-T, a friendly, intelligent and multilingual AI assistant, by OpenBuddy team. GitHub: https://github.com/OpenBuddy/OpenBuddy\nBuddy cannot access the Internet.\nBuddy can fluently speak the user's language (e.g. English, Chinese).\nBuddy can generate poems, stories, code, essays, songs, parodies, and more.\nBuddy possesses vast knowledge about the world, history, and culture.\nBuddy's responses are always safe, creative, high-quality, human-like, and interesting.\nBuddy strictly refuses to discuss political, NSFW, or other unsafe topics.\n\nUser: Hi.\nAssistant: Hi, I'm Buddy, your AI assistant. How can I help you today?\"\"\",\n        roles=(\"User\", \"Assistant\"),\n        sep_style=SeparatorStyle.ADD_COLON_SINGLE,\n        sep=\"\\n\",\n    )\n)\n\n# Phoenix default template\nregister_conv_template(\n    Conversation(\n        name=\"phoenix\",\n        system_message=\"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\\n\\n\",\n        roles=(\"Human\", \"Assistant\"),\n        sep_style=SeparatorStyle.PHOENIX,\n        sep=\"</s>\",\n    )\n)\n\n# ReaLM default template\nregister_conv_template(\n    Conversation(\n        name=\"ReaLM-7b-v1\",\n        system_message=\"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\\n\\n\",\n        roles=(\"Human\", \"Assistant\"),\n        sep_style=SeparatorStyle.PHOENIX,\n        sep=\"</s>\",\n    )\n)\n\n# ChatGPT default template\nregister_conv_template(\n    Conversation(\n        name=\"chatgpt\",\n        system_message=\"You are a helpful assistant.\",\n        roles=(\"user\", \"assistant\"),\n        sep_style=None,\n        sep=None,\n    )\n)\n\n# Claude default template\nregister_conv_template(\n    Conversation(\n        name=\"claude\",\n        roles=(\"Human\", \"Assistant\"),\n        sep_style=SeparatorStyle.ADD_COLON_SINGLE,\n        sep=\"\\n\\n\",\n    )\n)\n\n# MPT default template\nregister_conv_template(\n    Conversation(\n        name=\"mpt-7b-chat\",\n        system_template=\"\"\"<|im_start|>system\n{system_message}\"\"\",\n        system_message=\"\"\"- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.\"\"\",\n        roles=(\"<|im_start|>user\", \"<|im_start|>assistant\"),\n        sep_style=SeparatorStyle.CHATML,\n        sep=\"<|im_end|>\",\n        stop_token_ids=[50278, 0],\n    )\n)\n\n# MPT-30b-chat default template\nregister_conv_template(\n    Conversation(\n        name=\"mpt-30b-chat\",\n        system_template=\"\"\"<|im_start|>system\n{system_message}\"\"\",\n        system_message=\"\"\"A conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers.\"\"\",\n        roles=(\"<|im_start|>user\", \"<|im_start|>assistant\"),\n        sep_style=SeparatorStyle.CHATML,\n        sep=\"<|im_end|>\",\n        stop_token_ids=[50278, 0],\n    )\n)\n\n# Lemur-70b-chat default template\n# reference: https://huggingface.co/OpenLemur/lemur-70b-chat-v1#generation\nregister_conv_template(\n    Conversation(\n        name=\"lemur-70b-chat\",\n        system_template=\"\"\"<|im_start|>system\n{system_message}\"\"\",\n        system_message=\"\"\"You are a helpful, respectful, and honest assistant.\"\"\",\n        roles=(\"<|im_start|>user\", \"<|im_start|>assistant\"),\n        sep_style=SeparatorStyle.CHATML,\n        sep=\"<|im_end|>\",\n        stop_token_ids=[32002, 0],\n    )\n)\n\n# MPT-30b-instruct default template\n# reference: https://huggingface.co/mosaicml/mpt-30b-instruct#formatting\nregister_conv_template(\n    Conversation(\n        name=\"mpt-30b-instruct\",\n        system_template=\"{system_message}\",\n        system_message=\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\",\n        roles=(\"### Instruction\", \"### Response\"),\n        sep_style=SeparatorStyle.ADD_NEW_LINE_SINGLE,\n        sep=\"\\n\\n\",\n        stop_token_ids=[50278, 0],\n    )\n)\n\n# Bard default template\n# Reference: https://github.com/google/generative-ai-python/blob/9c99bcb474a991a97a2e7d62fcdb52db7ce40729/google/generativeai/discuss.py#L150\n#            https://github.com/google/generative-ai-python/blob/9c99bcb474a991a97a2e7d62fcdb52db7ce40729/google/generativeai/discuss.py#L40\nregister_conv_template(\n    Conversation(\n        name=\"bard\",\n        roles=(\"0\", \"1\"),\n        sep_style=None,\n        sep=None,\n    )\n)\n\n# BiLLa default template\nregister_conv_template(\n    Conversation(\n        name=\"billa\",\n        roles=(\"Human\", \"Assistant\"),\n        sep_style=SeparatorStyle.ADD_COLON_SPACE_SINGLE,\n        sep=\"\\n\",\n        stop_str=\"Human:\",\n    )\n)\n\n# RedPajama INCITE default template\nregister_conv_template(\n    Conversation(\n        name=\"redpajama-incite\",\n        roles=(\"<human>\", \"<bot>\"),\n        sep_style=SeparatorStyle.ADD_COLON_SINGLE,\n        sep=\"\\n\",\n        stop_str=\"<human>\",\n    )\n)\n\n# h2oGPT default template\nregister_conv_template(\n    Conversation(\n        name=\"h2ogpt\",\n        roles=(\"<|prompt|>\", \"<|answer|>\"),\n        sep_style=SeparatorStyle.NO_COLON_SINGLE,\n        sep=\"</s>\",\n    )\n)\n\n# Robin default template\nregister_conv_template(\n    Conversation(\n        name=\"Robin\",\n        system_message=\"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n        roles=(\"###Human\", \"###Assistant\"),\n        sep_style=SeparatorStyle.ROBIN,\n        sep=\"\\n\",\n        stop_token_ids=[2, 396],\n        stop_str=\"###\",\n    )\n)\n\n# Snoozy default template\n# Reference: https://github.com/nomic-ai/gpt4all/blob/d4861030b778da6db59d21d2927a4aba4f9f1f43/gpt4all-bindings/python/gpt4all/gpt4all.py#L232\nregister_conv_template(\n    Conversation(\n        name=\"snoozy\",\n        system_template=\"### Instruction:\\n{system_message}\",\n        system_message=\"The prompt below is a question to answer, a task to complete, or a conversation to respond to; decide which and write an appropriate response.\",\n        roles=(\"### Prompt\", \"### Response\"),\n        sep_style=SeparatorStyle.ADD_COLON_SINGLE,\n        sep=\"\\n\",\n        stop_str=\"###\",\n    )\n)\n\n# manticore default template\nregister_conv_template(\n    Conversation(\n        name=\"manticore\",\n        roles=(\"USER\", \"ASSISTANT\"),\n        sep_style=SeparatorStyle.ADD_COLON_TWO,\n        sep=\"\\n\",\n        sep2=\"</s>\",\n    )\n)\n\n# Falcon default template\nregister_conv_template(\n    Conversation(\n        name=\"falcon\",\n        roles=(\"User\", \"Assistant\"),\n        messages=[],\n        sep_style=SeparatorStyle.RWKV,\n        sep=\"\\n\",\n        sep2=\"<|endoftext|>\",\n        stop_str=\"\\nUser\",  # use stop_str to stop generation after stop_token_ids, it will also remove stop_str from the generated text\n        stop_token_ids=[\n            0,\n            1,\n            2,\n            3,\n            4,\n            5,\n            6,\n            7,\n            8,\n            9,\n            10,\n            11,\n        ],  # it better only put special tokens here, because tokenizer only remove special tokens\n    )\n)\n\n# ChangGPT default template\nregister_conv_template(\n    Conversation(\n        name=\"polyglot_changgpt\",\n        roles=(\"B\", \"A\"),\n        sep_style=SeparatorStyle.ADD_COLON_SINGLE,\n        sep=\"\\n\",\n    )\n)\n\n# tigerbot template\nregister_conv_template(\n    Conversation(\n        name=\"tigerbot\",\n        system_message=\"A chat between a curious user and an artificial intelligence assistant. \"\n        \"The assistant gives helpful, detailed, and polite answers to the user's questions.\",\n        roles=(\"### Instruction\", \"### Response\"),\n        sep_style=SeparatorStyle.ROBIN,\n        sep=\"\\n\\n\",\n        stop_str=\"###\",\n    )\n)\n\n# ref: https://huggingface.co/Salesforce/xgen-7b-8k-inst\nregister_conv_template(\n    Conversation(\n        name=\"xgen\",\n        system_message=\"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\\n\\n\",\n        roles=(\"### Human\", \"### Assistant\"),\n        sep_style=SeparatorStyle.ADD_COLON_SINGLE,\n        sep=\"\\n\",\n        stop_token_ids=[50256],\n    )\n)\n\n# Internlm-chat template\nregister_conv_template(\n    Conversation(\n        name=\"internlm-chat\",\n        system_message=\"A chat between a curious <|User|> and an <|Bot|>. The <|Bot|> gives helpful, detailed, and polite answers to the <|User|>'s questions.\\n\\n\",\n        roles=(\"<|User|>\", \"<|Bot|>\"),\n        sep_style=SeparatorStyle.CHATINTERN,\n        sep=\"<eoh>\",\n        sep2=\"<eoa>\",\n        stop_token_ids=[1, 103028],\n        stop_str=\"<|User|>\",\n    )\n)\n\n# StarChat template\n# reference: https://huggingface.co/spaces/HuggingFaceH4/starchat-playground/blob/main/dialogues.py\nregister_conv_template(\n    Conversation(\n        name=\"starchat\",\n        system_template=\"<system>\\n{system_message}\",\n        roles=(\"<|user|>\", \"<|assistant|>\"),\n        sep_style=SeparatorStyle.CHATML,\n        sep=\"<|end|>\",\n        stop_token_ids=[0, 49155],\n        stop_str=\"<|end|>\",\n    )\n)\n\n# Baichuan-13B-Chat template\nregister_conv_template(\n    # source: https://huggingface.co/baichuan-inc/Baichuan-13B-Chat/blob/19ef51ba5bad8935b03acd20ff04a269210983bc/modeling_baichuan.py#L555\n    # https://huggingface.co/baichuan-inc/Baichuan-13B-Chat/blob/main/generation_config.json\n    # https://github.com/baichuan-inc/Baichuan-13B/issues/25\n    Conversation(\n        name=\"baichuan-chat\",\n        roles=(\"<reserved_102>\", \"<reserved_103>\"),\n        sep_style=SeparatorStyle.NO_COLON_SINGLE,\n        sep=\"\",\n        stop_token_ids=[],\n    )\n)\n\n# Baichuan2-13B-Chat template\nregister_conv_template(\n    # source: https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/blob/c6f8592a60b4ad73c210b28dd2ab3cca51abbf93/modeling_baichuan.py#L773\n    # https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/blob/main/generation_config.json\n    # https://github.com/baichuan-inc/Baichuan2/issues/62\n    Conversation(\n        name=\"baichuan2-chat\",\n        roles=(\"<reserved_106>\", \"<reserved_107>\"),\n        sep_style=SeparatorStyle.NO_COLON_SINGLE,\n        sep=\"\",\n        stop_token_ids=[],\n    )\n)\n\n# Mistral template\n# source: https://docs.mistral.ai/llm/mistral-instruct-v0.1#chat-template\nregister_conv_template(\n    Conversation(\n        name=\"mistral\",\n        system_template=\"[INST]{system_message}\\n\",\n        roles=(\"[INST]\", \"[/INST]\"),\n        sep_style=SeparatorStyle.LLAMA2,\n        sep=\" \",\n        sep2=\"</s>\",\n    )\n)\n\n# llama2 template\n# reference: https://huggingface.co/blog/codellama#conversational-instructions\n# reference: https://github.com/facebookresearch/llama/blob/1a240688810f8036049e8da36b073f63d2ac552c/llama/generation.py#L212\nregister_conv_template(\n    Conversation(\n        name=\"llama-2\",\n        system_template=\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\n\",\n        roles=(\"[INST]\", \"[/INST]\"),\n        sep_style=SeparatorStyle.LLAMA2,\n        sep=\" \",\n        sep2=\" </s><s>\",\n    )\n)\n\nregister_conv_template(\n    Conversation(\n        name=\"cutegpt\",\n        roles=(\"问：\", \"答：\\n\"),\n        sep_style=SeparatorStyle.NO_COLON_TWO,\n        sep=\"\\n\",\n        sep2=\"\\n\",\n        stop_str=\"<end>\",\n    )\n)\n\n# OpenOrcaxOpenChat-Preview2-13B template\nregister_conv_template(\n    Conversation(\n        name=\"open-orca\",\n        system_template=\"{system_message}\",\n        system_message=\"You are a helpful assistant. Please answer truthfully and write out your \"\n        \"thinking step by step to be sure you get the right answer. If you make a mistake or encounter \"\n        \"an error in your thinking, say so out loud and attempt to correct it. If you don't know or \"\n        \"aren't sure about something, say so clearly. You will act as a professional logician, mathematician, \"\n        \"and physicist. You will also act as the most appropriate type of expert to answer any particular \"\n        \"question or solve the relevant problem; state which expert type your are, if so. Also think of \"\n        \"any particular named expert that would be ideal to answer the relevant question or solve the \"\n        \"relevant problem; name and act as them, if appropriate.\",\n        roles=(\"User\", \"Assistant\"),\n        sep_style=SeparatorStyle.ADD_COLON_SPACE_SINGLE,\n        sep=\"<|end_of_turn|>\\n\",\n        stop_token_ids=[32000, 32001],  # \"<|end_of_turn|>\"\n        stop_str=\"User\",\n    )\n)\n\n# Open-Orca/Mistral-7B-OpenOrca template\n# source: https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca\n# reference: https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca#prompt-template\nregister_conv_template(\n    Conversation(\n        name=\"mistral-7b-openorca\",\n        system_template=\"<|im_start|>system\\n{system_message}\",\n        system_message=\"You are MistralOrca, a large language model trained by Alignment Lab AI. Write out your reasoning step-by-step to be sure you get the right answers!\",\n        roles=(\"<|im_start|>user\", \"<|im_start|>assistant\"),\n        sep_style=SeparatorStyle.CHATML,\n        sep=\"<|im_end|>\",\n        stop_token_ids=[32000, 32001],\n    )\n)\n\n# Qwen-chat default template\n# source: https://huggingface.co/Qwen/Qwen-7B-Chat/blob/main/qwen_generation_utils.py#L130\nregister_conv_template(\n    Conversation(\n        name=\"qwen-7b-chat\",\n        system_template=\"<|im_start|>system\\n{system_message}\",\n        system_message=\"You are a helpful assistant.\",\n        roles=(\"<|im_start|>user\", \"<|im_start|>assistant\"),\n        sep_style=SeparatorStyle.CHATML,\n        sep=\"<|im_end|>\",\n        stop_token_ids=[\n            151643,\n            151644,\n            151645,\n        ],  # \"<|endoftext|>\", \"<|im_start|>\", \"<|im_end|>\"\n        stop_str=\"<|endoftext|>\",\n    )\n)\n\n\n# AquilaChat default template\n# source: https://github.com/FlagAI-Open/FlagAI/blob/master/examples/Aquila/Aquila-chat/cyg_conversation.py\nregister_conv_template(\n    Conversation(\n        name=\"aquila-chat\",\n        system_message=\"A chat between a curious human and an artificial intelligence assistant. \"\n        \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n        roles=(\"Human\", \"Assistant\"),\n        sep_style=SeparatorStyle.ADD_COLON_SINGLE,\n        sep=\"###\",\n        sep2=\"\",\n        stop_str=[\"###\", \"</s>\", \"[UNK]\"],\n    )\n)\n# AquilaChat2-34B default template\n# source: https://huggingface.co/BAAI/AquilaChat2-34B/blob/4608b75855334b93329a771aee03869dbf7d88cc/predict.py#L212\nregister_conv_template(\n    Conversation(\n        name=\"aquila-legacy\",\n        system_message=\"A chat between a curious human and an artificial intelligence assistant. \"\n        \"The assistant gives helpful, detailed, and polite answers to the human's questions.\\n\\n\",\n        roles=(\"### Human: \", \"### Assistant: \"),\n        offset=0,\n        sep_style=SeparatorStyle.NO_COLON_TWO,\n        sep=\"\\n\",\n        sep2=\"</s>\",\n        stop_str=[\"</s>\", \"[UNK]\"],\n    )\n)\n# AquilaChat2-7B-16K and AquilaChat2-34B-16K default template\n# source: https://huggingface.co/BAAI/AquilaChat2-34B/blob/4608b75855334b93329a771aee03869dbf7d88cc/predict.py#L227\nregister_conv_template(\n    Conversation(\n        name=\"aquila\",\n        system_message=\"A chat between a curious human and an artificial intelligence assistant. \"\n        \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n        roles=(\"Human\", \"Assistant\"),\n        offset=0,\n        sep_style=SeparatorStyle.ADD_COLON_TWO,\n        sep=\"###\",\n        sep2=\"</s>\",\n        stop_str=[\"</s>\", \"[UNK]\"],\n    )\n)\n\n# AquilaChat2-7B default template\n# source: https://huggingface.co/BAAI/AquilaChat2-34B/blob/4608b75855334b93329a771aee03869dbf7d88cc/predict.py#L242\nregister_conv_template(\n    Conversation(\n        name=\"aquila-v1\",\n        roles=(\"<|startofpiece|>\", \"<|endofpiece|>\"),\n        offset=0,\n        sep_style=SeparatorStyle.NO_COLON_TWO,\n        sep=\"\",\n        sep2=\"</s>\",\n        stop_str=[\"</s>\", \"<|endoftext|>\"],\n    )\n)\n\n# Llama2-Chinese default template\n# source: https://huggingface.co/FlagAlpha\nregister_conv_template(\n    Conversation(\n        name=\"llama2-chinese\",\n        system_template=\"<s>{system_message}</s>\",\n        roles=(\"Human\", \"Assistant\", \"System\"),\n        sep_style=SeparatorStyle.ADD_COLON_TWO,\n        sep=\"\\n\",\n        sep2=\"\\n</s><s>\",\n        stop_str=\"</s>\",\n    )\n)\n\n# Vigogne Instruct default template\n# source: https://github.com/bofenghuang/vigogne\nregister_conv_template(\n    Conversation(\n        name=\"vigogne_instruct\",\n        system_template=\"### System:\\n{system_message}\\n\\n\",\n        system_message=(\n            \"Ci-dessous se trouve une instruction qui décrit une tâche à accomplir. Rédigez une réponse qui répond de manière\"\n            \" précise à la demande.\"\n        ),\n        roles=(\"### Instruction\", \"### Response\"),\n        sep_style=SeparatorStyle.DOLLY,\n        sep=\"\\n\\n\",\n        sep2=\"</s>\",\n    )\n)\n\n# Vigogne Chat default template\nregister_conv_template(\n    Conversation(\n        name=\"vigogne_chat_v2\",\n        system_template=\"<|system|>: {system_message}\",\n        system_message=(\n            \"Vous êtes Vigogne, un assistant IA créé par Zaion Lab. Vous suivez extrêmement bien les instructions. Aidez\"\n            \" autant que vous le pouvez.\"\n        ),\n        roles=(\"<|user|>\", \"<|assistant|>\"),\n        sep_style=SeparatorStyle.ADD_COLON_TWO,\n        sep=\"\\n\",\n        sep2=\"</s>\\n\",\n        stop_str=\"<|user|>\",\n    )\n)\n\nregister_conv_template(\n    Conversation(\n        name=\"vigogne_chat_v3\",\n        system_template=\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\n\",\n        system_message=(\n            \"Vous êtes Vigogne, un assistant IA créé par Zaion Lab. Vous suivez extrêmement bien les instructions. Aidez\"\n            \" autant que vous le pouvez.\"\n        ),\n        roles=(\"[INST]\", \"[/INST]\"),\n        sep_style=SeparatorStyle.LLAMA2,\n        sep=\" \",\n        sep2=\" </s>\",\n    )\n)\n\n# Falcon 180B chat template\n# source: https://huggingface.co/spaces/tiiuae/falcon-180b-demo/blob/d1590ee7fae9b6ce331ba7808e61a29dcce9239f/app.py#L28-L37\nregister_conv_template(\n    Conversation(\n        name=\"falcon-chat\",\n        roles=(\"User\", \"Falcon\"),\n        system_template=\"System: {system_message}\",\n        messages=[],\n        sep_style=SeparatorStyle.FALCON_CHAT,\n        sep=\"\\n\",\n        sep2=\"<|endoftext|>\",\n        stop_str=\"\\nUser:\",  # use stop_str to stop generation after stop_token_ids, it will also remove stop_str from the generated text\n    )\n)\n\n# Phind template\n# source: https://huggingface.co/Phind/Phind-CodeLlama-34B-v2\nregister_conv_template(\n    Conversation(\n        name=\"phind\",\n        system_message=\"### System Prompt\\nYou are an intelligent programming assistant.\",\n        roles=(\"### User Message\", \"### Assistant\"),\n        messages=(),\n        offset=0,\n        sep_style=SeparatorStyle.ADD_COLON_SINGLE,\n        sep=\"\\n\\n\",\n    )\n)\n\n# Metharme formatting for Pygmalion models\n# source: https://huggingface.co/PygmalionAI/pygmalion-2-13b\nregister_conv_template(\n    Conversation(\n        name=\"metharme\",\n        system_template=\"<|system|>{system_message}\",\n        system_message=\"\"\"Enter RP mode. You shall reply to the user while staying \n        in character. Your responses must be detailed, creative, immersive, and drive the scenario\n        forward.\"\"\",\n        roles=(\"<|user|>\", \"<|model|>\"),\n        sep_style=SeparatorStyle.NO_COLON_SINGLE,\n        sep=\"\",\n        stop_str=\"<|user|>\",\n    )\n)\n\n# Zephyr template\n# reference: https://huggingface.co/spaces/HuggingFaceH4/zephyr-playground/blob/main/dialogues.py\nregister_conv_template(\n    Conversation(\n        name=\"zephyr\",\n        system_template=\"<|system|>\\n{system_message}\",\n        roles=(\"<|user|>\", \"<|assistant|>\"),\n        sep_style=SeparatorStyle.CHATML,\n        sep=\"</s>\",\n        stop_token_ids=[2],\n        stop_str=\"</s>\",\n    )\n)\n\n\nif __name__ == \"__main__\":\n    from fastchat.conversation import get_conv_template\n\n    print(\"-- Vicuna template --\")\n    conv = get_conv_template(\"vicuna_v1.1\")\n    conv.append_message(conv.roles[0], \"Hello!\")\n    conv.append_message(conv.roles[1], \"Hi!\")\n    conv.append_message(conv.roles[0], \"How are you?\")\n    conv.append_message(conv.roles[1], None)\n    print(conv.get_prompt())\n\n    print(\"\\n\")\n\n    print(\"-- Llama-2 template --\")\n    conv = get_conv_template(\"llama-2\")\n    conv.set_system_message(\"You are a helpful, respectful and honest assistant.\")\n    conv.append_message(conv.roles[0], \"Hello!\")\n    conv.append_message(conv.roles[1], \"Hi!\")\n    conv.append_message(conv.roles[0], \"How are you?\")\n    conv.append_message(conv.roles[1], None)\n    print(conv.get_prompt())\n\n    print(\"\\n\")\n\n    print(\"-- ChatGPT template --\")\n    conv = get_conv_template(\"chatgpt\")\n    conv.append_message(conv.roles[0], \"Hello!\")\n    conv.append_message(conv.roles[1], \"Hi!\")\n    conv.append_message(conv.roles[0], \"How are you?\")\n    conv.append_message(conv.roles[1], None)\n    print(conv.to_openai_api_messages())\n\n    print(\"\\n\")\n\n    print(\"-- Claude template --\")\n    conv = get_conv_template(\"claude\")\n    conv.append_message(conv.roles[0], \"Hello!\")\n    conv.append_message(conv.roles[1], \"Hi!\")\n    conv.append_message(conv.roles[0], \"How are you?\")\n    conv.append_message(conv.roles[1], None)\n    print(conv.get_prompt())\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/model/model_xfastertransformer.py", "content": "import gc\nfrom threading import Thread\n\nimport torch\nfrom transformers import TextIteratorStreamer\n\n\n@torch.inference_mode()\ndef generate_stream_xft(\n    model,\n    tokenizer,\n    params,\n    device,\n    context_len=8192,\n    stream_interval=2,\n    judge_sent_end=False,\n):\n    prompt = params[\"prompt\"]\n    repetition_penalty = float(params.get(\"repetition_penalty\", 1.0))\n\n    # unused now, and placehold for future.\n    # temperature = float(params.get(\"temperature\", 1.0))\n    # top_p = float(params.get(\"top_p\", 1.0))\n\n    max_new_tokens = int(params.get(\"max_new_tokens\", 4096))\n    echo = params.get(\"echo\", True)\n\n    inputs = tokenizer(\n        prompt, return_tensors=\"pt\", padding=model.config.padding\n    ).input_ids\n    input_echo_len = len(inputs[0])\n    max_len = max_new_tokens + input_echo_len\n\n    decode_config = dict(skip_special_tokens=True, clean_up_tokenization_spaces=True)\n    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, **decode_config)\n    generation_kwargs = {\n        \"input_ids\": inputs,\n        \"streamer\": streamer,\n        \"max_length\": max_len,\n        \"num_beams\": model.config.beam_width,\n        \"length_penalty\": repetition_penalty,\n        \"num_return_sequences\": model.config.num_return_sequences,\n        \"early_stopping\": model.config.early_stopping,\n        \"eos_token_id\": model.config.eos_token_id,\n        \"pad_token_id\": model.config.pad_token_id,\n    }\n\n    thread = Thread(target=model.model.generate, kwargs=generation_kwargs)\n    thread.start()\n    if echo:\n        # means keep the prompt\n        output = prompt\n    else:\n        output = \"\"\n    i = 0\n    for i, new_text in enumerate(streamer):\n        output += new_text\n        yield {\n            \"text\": output,\n            \"usage\": {\n                \"prompt_tokens\": input_echo_len,\n                \"completion_tokens\": i,\n                \"total_tokens\": input_echo_len + i,\n            },\n            \"finish_reason\": None,\n        }\n    output = output.strip()\n    if i == max_new_tokens - 1:\n        finish_reason = \"length\"\n    else:\n        finish_reason = \"stop\"\n    yield {\n        \"text\": output,\n        \"usage\": {\n            \"prompt_tokens\": input_echo_len,\n            \"completion_tokens\": i,\n            \"total_tokens\": input_echo_len + i,\n        },\n        \"finish_reason\": finish_reason,\n    }\n    gc.collect()\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/llm_judge/common.py", "content": "\"\"\"\nCommon data structures and utilities.\n\"\"\"\n\nimport ast\nimport dataclasses\nimport glob\nimport json\nimport os\nimport re\nimport time\nfrom typing import Optional\n\nimport openai\nimport anthropic\n\nfrom fastchat.model.model_adapter import get_conversation_template, ANTHROPIC_MODEL_LIST\n\n# API setting constants\nAPI_MAX_RETRY = 16\nAPI_RETRY_SLEEP = 10\nAPI_ERROR_OUTPUT = \"$ERROR$\"\n\nTIE_DELTA = 0.1\n\n# Categories that need reference answers\nNEED_REF_CATS = [\"math\", \"reasoning\", \"coding\", \"arena-hard-200\"]\n\n# Extract scores from judgments\ntwo_score_pattern = re.compile(\"\\[\\[(\\d+\\.?\\d*),\\s?(\\d+\\.?\\d*)\\]\\]\")\ntwo_score_pattern_backup = re.compile(\"\\[(\\d+\\.?\\d*),\\s?(\\d+\\.?\\d*)\\]\")\none_score_pattern = re.compile(\"\\[\\[(\\d+\\.?\\d*)\\]\\]\")\none_score_pattern_backup = re.compile(\"\\[(\\d+\\.?\\d*)\\]\")\n\n# Sampling temperature configs for\ntemperature_config = {\n    \"writing\": 0.7,\n    \"roleplay\": 0.7,\n    \"extraction\": 0.0,\n    \"math\": 0.0,\n    \"coding\": 0.0,\n    \"reasoning\": 0.0,\n    \"stem\": 0.1,\n    \"humanities\": 0.1,\n    \"arena-hard-200\": 0.0,\n}\n\nreverse_model_map = {\n    \"model_1\": \"model_2\",\n    \"model_2\": \"model_1\",\n}\n\n\n@dataclasses.dataclass\nclass Judge:\n    model_name: str\n    prompt_template: dict\n    ref_based: bool = False\n    multi_turn: bool = False\n\n\n@dataclasses.dataclass\nclass MatchSingle:\n    question: dict\n    model: str\n    answer: dict\n    judge: Judge\n    ref_answer: dict = None\n    multi_turn: bool = False\n\n\n@dataclasses.dataclass\nclass MatchPair:\n    question: dict\n    model_1: str\n    model_2: str\n    answer_1: dict\n    answer_2: dict\n    judge: Judge\n    ref_answer: dict = None\n    multi_turn: bool = False\n\n\ndef load_questions(question_file: str, begin: Optional[int], end: Optional[int]):\n    \"\"\"Load questions from a file.\"\"\"\n    questions = []\n    with open(question_file, \"r\") as ques_file:\n        for line in ques_file:\n            if line:\n                questions.append(json.loads(line))\n    questions = questions[begin:end]\n    return questions\n\n\ndef load_model_answers(answer_dir: str):\n    \"\"\"Load model answers.\n\n    The return value is a python dict of type:\n    Dict[model_name: str -> Dict[question_id: int -> answer: dict]]\n    \"\"\"\n    filenames = glob.glob(os.path.join(answer_dir, \"*.jsonl\"))\n    filenames.sort()\n    model_answers = {}\n\n    for filename in filenames:\n        model_name = os.path.basename(filename)[:-6]\n        answer = {}\n        with open(filename) as fin:\n            for line in fin:\n                line = json.loads(line)\n                answer[line[\"question_id\"]] = line\n        model_answers[model_name] = answer\n\n    return model_answers\n\n\ndef load_judge_prompts(prompt_file: str):\n    \"\"\"Load judge prompts.\n\n    The return value is a python dict of type:\n    Dict[judge_name: str -> dict]\n    \"\"\"\n    prompts = {}\n    with open(prompt_file) as fin:\n        for line in fin:\n            line = json.loads(line)\n            prompts[line[\"name\"]] = line\n    return prompts\n\n\ndef run_judge_single(question, answer, judge, ref_answer, multi_turn=False):\n    kwargs = {}\n    model = judge.model_name\n    if ref_answer is not None:\n        kwargs[\"ref_answer_1\"] = ref_answer[\"choices\"][0][\"turns\"][0]\n        if multi_turn:\n            kwargs[\"ref_answer_2\"] = ref_answer[\"choices\"][0][\"turns\"][1]\n\n    if multi_turn:\n        user_prompt = judge.prompt_template[\"prompt_template\"].format(\n            question_1=question[\"turns\"][0],\n            question_2=question[\"turns\"][1],\n            answer_1=answer[\"choices\"][0][\"turns\"][0],\n            answer_2=answer[\"choices\"][0][\"turns\"][1],\n            **kwargs,\n        )\n    else:\n        user_prompt = judge.prompt_template[\"prompt_template\"].format(\n            question=question[\"turns\"][0],\n            answer=answer[\"choices\"][0][\"turns\"][0],\n            **kwargs,\n        )\n\n    rating = -1\n\n    system_prompt = judge.prompt_template[\"system_prompt\"]\n    conv = get_conversation_template(model)\n    conv.set_system_message(system_prompt)\n    conv.append_message(conv.roles[0], user_prompt)\n    conv.append_message(conv.roles[1], None)\n\n    if model in [\"gpt-3.5-turbo\", \"gpt-4\"]:\n        judgment = chat_compeletion_openai(model, conv, temperature=0, max_tokens=2048)\n    elif model in ANTHROPIC_MODEL_LIST:\n        judgment = chat_compeletion_anthropic(\n            model, conv, temperature=0, max_tokens=1024\n        )\n    else:\n        raise ValueError(f\"Invalid judge model name: {model}\")\n\n    if judge.prompt_template[\"output_format\"] == \"[[rating]]\":\n        match = re.search(one_score_pattern, judgment)\n        if not match:\n            match = re.search(one_score_pattern_backup, judgment)\n\n        if match:\n            rating = ast.literal_eval(match.groups()[0])\n        else:\n            rating = -1\n    else:\n        raise ValueError(\n            f\"invalid output format: {judge.prompt_template['output_format']}\"\n        )\n\n    return rating, user_prompt, judgment\n\n\ndef play_a_match_single(match: MatchPair, output_file: str):\n    question, model, answer, judge, ref_answer, multi_turn = (\n        match.question,\n        match.model,\n        match.answer,\n        match.judge,\n        match.ref_answer,\n        match.multi_turn,\n    )\n\n    if judge.prompt_template[\"type\"] == \"single\":\n        score, user_prompt, judgment = run_judge_single(\n            question, answer, judge, ref_answer, multi_turn=multi_turn\n        )\n\n        question_id = question[\"question_id\"]\n        turn = 1 if not multi_turn else 2\n        result = {\n            \"question_id\": question_id,\n            \"model\": model,\n            \"judge\": (judge.model_name, judge.prompt_template[\"name\"]),\n            \"user_prompt\": user_prompt,\n            \"judgment\": judgment,\n            \"score\": score,\n            \"turn\": turn,\n            \"tstamp\": time.time(),\n        }\n        print(\n            f\"question: {question_id}, turn: {turn}, model: {model}, \"\n            f\"score: {score}, \"\n            f\"judge: {(judge.model_name, judge.prompt_template['name'])}\"\n        )\n    else:\n        raise ValueError(f\"invalid judge type: {judge['type']}\")\n\n    if output_file:\n        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n        with open(output_file, \"a\") as fout:\n            fout.write(json.dumps(result) + \"\\n\")\n\n    return result\n\n\ndef run_judge_pair(question, answer_a, answer_b, judge, ref_answer, multi_turn=False):\n    kwargs = {}\n    model = judge.model_name\n    if ref_answer is not None:\n        kwargs[\"ref_answer_1\"] = ref_answer[\"choices\"][0][\"turns\"][0]\n        if multi_turn:\n            kwargs[\"ref_answer_2\"] = ref_answer[\"choices\"][0][\"turns\"][1]\n\n    if multi_turn:\n        system_prompt = judge.prompt_template[\"system_prompt\"]\n        user_prompt = judge.prompt_template[\"prompt_template\"].format(\n            question_1=question[\"turns\"][0],\n            question_2=question[\"turns\"][1],\n            answer_a_1=answer_a[\"choices\"][0][\"turns\"][0],\n            answer_b_1=answer_b[\"choices\"][0][\"turns\"][0],\n            answer_a_2=answer_a[\"choices\"][0][\"turns\"][1],\n            answer_b_2=answer_b[\"choices\"][0][\"turns\"][1],\n            **kwargs,\n        )\n    else:\n        system_prompt = judge.prompt_template[\"system_prompt\"]\n        user_prompt = judge.prompt_template[\"prompt_template\"].format(\n            question=question[\"turns\"][0],\n            answer_a=answer_a[\"choices\"][0][\"turns\"][0],\n            answer_b=answer_b[\"choices\"][0][\"turns\"][0],\n            **kwargs,\n        )\n\n    winner = \"error\"\n\n    conv = get_conversation_template(model)\n    conv.append_message(conv.roles[0], user_prompt)\n    conv.append_message(conv.roles[1], None)\n\n    if model in [\"gpt-3.5-turbo\", \"gpt-4\"]:\n        conv.set_system_message(system_prompt)\n        judgment = chat_compeletion_openai(model, conv, temperature=0, max_tokens=2048)\n    elif model in ANTHROPIC_MODEL_LIST:\n        if system_prompt != \"You are a helpful assistant.\":\n            user_prompt = \"[Instruction]\\n\" + system_prompt + \"\\n\\n\" + user_prompt\n            conv.messages[0][1] = user_prompt\n        judgment = chat_compeletion_anthropic(\n            model, conv, temperature=0, max_tokens=1024\n        )\n    else:\n        raise ValueError(f\"Invalid judge model name: {model}\")\n\n    if judge.prompt_template[\"output_format\"] == \"[[A]]\":\n        if \"[[A]]\" in judgment:\n            winner = \"A\"\n        elif \"[[B]]\" in judgment:\n            winner = \"B\"\n        elif \"[[C]]\" in judgment:\n            winner = \"tie\"\n        else:\n            winner = \"error\"\n    elif judge.prompt_template[\"output_format\"] == \"[[rating_a,rating_b]]\":\n        match = re.search(two_score_pattern, judgment)\n        if not match:\n            match = re.search(two_score_pattern_backup, judgment)\n        if match:\n            scores = [ast.literal_eval(s.strip()) for s in match.groups()]\n            if abs(scores[0] - scores[1]) <= TIE_DELTA:\n                winner = \"tie\"\n            elif scores[0] > scores[1]:\n                winner = \"A\"\n            else:\n                winner = \"B\"\n        else:\n            winner = \"error\"\n    else:\n        raise ValueError(\n            f\"invalid output format: {judge.prompt_template['output_format']}\"\n        )\n\n    return winner, user_prompt, judgment\n\n\ndef play_a_match_pair(match: MatchPair, output_file: str):\n    question, model_1, model_2, answer_1, answer_2, judge, ref_answer, multi_turn = (\n        match.question,\n        match.model_1,\n        match.model_2,\n        match.answer_1,\n        match.answer_2,\n        match.judge,\n        match.ref_answer,\n        match.multi_turn,\n    )\n\n    if judge.prompt_template[\"type\"] == \"pairwise\":\n        g1_winner, g1_user_prompt, g1_judgment = run_judge_pair(\n            question, answer_1, answer_2, judge, ref_answer, multi_turn=multi_turn\n        )\n        g2_winner, g2_user_prompt, g2_judgment = run_judge_pair(\n            question, answer_2, answer_1, judge, ref_answer, multi_turn=multi_turn\n        )\n\n        g1_map = {\"A\": \"model_1\", \"B\": \"model_2\"}\n        g2_map = {\"A\": \"model_2\", \"B\": \"model_1\"}\n        g1_winner = g1_map.get(g1_winner, g1_winner)\n        g2_winner = g2_map.get(g2_winner, g2_winner)\n        question_id = question[\"question_id\"]\n        turn = 1 if not multi_turn else 2\n\n        result = {\n            \"question_id\": question_id,\n            \"model_1\": model_1,\n            \"model_2\": model_2,\n            \"g1_winner\": g1_winner,\n            \"g2_winner\": g2_winner,\n            \"judge\": (judge.model_name, judge.prompt_template[\"name\"]),\n            \"g1_user_prompt\": g1_user_prompt,\n            \"g1_judgment\": g1_judgment,\n            \"g2_user_prompt\": g2_user_prompt,\n            \"g2_judgment\": g2_judgment,\n            \"turn\": turn,\n            \"tstamp\": time.time(),\n        }\n\n        print(\n            f\"question: {question_id}, turn: {turn}, model_1: {model_1}, model_2: {model_2}, \"\n            f\"g1_winner: {g1_winner}, g2_winner: {g2_winner}, \"\n            f\"judge: {(judge.model_name, judge.prompt_template['name'])}\"\n        )\n    elif judge.prompt_template[\"type\"] == \"single\":\n        m1_score, m1_user_prompt, m1_judgment = run_judge_single(\n            question, answer_1, judge\n        )\n        m2_score, m2_user_prompt, m2_judgment = run_judge_single(\n            question, answer_2, judge\n        )\n\n        if abs(m1_score - m2_score) <= TIE_DELTA:\n            winner = \"tie\"\n        elif m1_score > m2_score:\n            winner = \"model_1\"\n        else:\n            winner = \"model_2\"\n\n        question_id = question[\"question_id\"]\n        result = {\n            \"question_id\": question_id,\n            \"model_1\": model_1,\n            \"model_2\": model_2,\n            \"g1_winner\": winner,\n            \"g2_winner\": winner,\n            \"judge\": (judge.model_name, judge.prompt_template[\"name\"]),\n            \"g1_user_prompt\": m1_user_prompt,\n            \"g1_judgment\": m1_judgment,\n            \"g2_user_prompt\": m2_user_prompt,\n            \"g2_judgment\": m2_judgment,\n            \"m1_score\": m1_score,\n            \"m2_score\": m2_score,\n            \"tstamp\": time.time(),\n        }\n        print(\n            f\"question: {question_id}, model_1: {model_1}, model_2: {model_2}, \"\n            f\"winner: {winner}, m1_score: {m1_score}, m2_score: {m2_score}, \"\n            f\"judge: {(judge.model_name, judge.prompt_template['name'])}\"\n        )\n    else:\n        raise ValueError(f\"invalid judge type: {judge['type']}\")\n\n    if output_file:\n        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n        with open(output_file, \"a\") as fout:\n            fout.write(json.dumps(result) + \"\\n\")\n\n    return result\n\n\ndef chat_compeletion_openai(model, conv, temperature, max_tokens, api_dict=None):\n    if api_dict is not None:\n        openai.api_base = api_dict[\"api_base\"]\n        openai.api_key = api_dict[\"api_key\"]\n    output = API_ERROR_OUTPUT\n    for _ in range(API_MAX_RETRY):\n        try:\n            messages = conv.to_openai_api_messages()\n            response = openai.ChatCompletion.create(\n                model=model,\n                messages=messages,\n                n=1,\n                temperature=temperature,\n                max_tokens=max_tokens,\n            )\n            output = response[\"choices\"][0][\"message\"][\"content\"]\n            break\n        except openai.error.OpenAIError as e:\n            print(type(e), e)\n            time.sleep(API_RETRY_SLEEP)\n\n    return output\n\n\ndef chat_compeletion_openai_azure(model, conv, temperature, max_tokens, api_dict=None):\n    openai.api_type = \"azure\"\n    openai.api_version = \"2023-07-01-preview\"\n    if api_dict is not None:\n        openai.api_base = api_dict[\"api_base\"]\n        openai.api_key = api_dict[\"api_key\"]\n    else:\n        openai.api_base = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n        openai.api_key = os.environ[\"AZURE_OPENAI_KEY\"]\n\n    if \"azure-\" in model:\n        model = model[6:]\n\n    output = API_ERROR_OUTPUT\n    for _ in range(API_MAX_RETRY):\n        try:\n            messages = conv.to_openai_api_messages()\n            response = openai.ChatCompletion.create(\n                engine=model,\n                messages=messages,\n                n=1,\n                temperature=temperature,\n                max_tokens=max_tokens,\n            )\n            output = response[\"choices\"][0][\"message\"][\"content\"]\n            break\n        except openai.error.OpenAIError as e:\n            print(type(e), e)\n            time.sleep(API_RETRY_SLEEP)\n        except openai.error.InvalidRequestError as e:\n            print(type(e), e)\n            break\n        except KeyError:\n            print(response)\n            break\n\n    return output\n\n\ndef chat_compeletion_anthropic(model, conv, temperature, max_tokens):\n    output = API_ERROR_OUTPUT\n    for _ in range(API_MAX_RETRY):\n        try:\n            c = anthropic.Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n            prompt = conv.get_prompt()\n            response = c.completions.create(\n                model=model,\n                prompt=prompt,\n                stop_sequences=[anthropic.HUMAN_PROMPT],\n                max_tokens_to_sample=max_tokens,\n                temperature=temperature,\n            )\n            output = response.completion\n            break\n        except anthropic.APIError as e:\n            print(type(e), e)\n            time.sleep(API_RETRY_SLEEP)\n    return output.strip()\n\n\ndef chat_compeletion_palm(chat_state, model, conv, temperature, max_tokens):\n    from fastchat.serve.api_provider import init_palm_chat\n\n    assert model == \"palm-2-chat-bison-001\"\n\n    if chat_state is None:\n        chat_state = init_palm_chat(\"chat-bison@001\")\n\n    parameters = {\n        \"temperature\": temperature,\n        \"top_p\": 0.8,\n        \"top_k\": 40,\n        \"max_output_tokens\": max_tokens,\n    }\n    output = API_ERROR_OUTPUT\n    for _ in range(API_MAX_RETRY):\n        try:\n            response = chat_state.send_message(conv.messages[-2][1], **parameters)\n            output = response.text\n            break\n        except Exception as e:\n            print(type(e), e)\n            time.sleep(API_RETRY_SLEEP)\n    return chat_state, output\n\n\ndef normalize_game_key_single(gamekey, result):\n    \"\"\"Make the model names sorted in a game key.\"\"\"\n    qid, model_1, model_2 = gamekey\n    if model_1 < model_2:\n        return gamekey, result\n    else:\n        new_gamekey = (qid, model_2, model_1)\n        new_result = {\n            \"winners\": tuple(reverse_model_map.get(x, x) for x in result[\"winners\"]),\n            \"g1_judgment\": result[\"g2_judgment\"],\n            \"g2_judgment\": result[\"g1_judgment\"],\n        }\n        return new_gamekey, new_result\n\n\ndef normalize_game_key_dict(judgment_dict):\n    \"\"\"Make the model names sorted in the game keys.\"\"\"\n    ret = {}\n    for key, value in judgment_dict.items():\n        new_key, new_value = normalize_game_key_single(key, value)\n        ret[new_key] = new_value\n    return ret\n\n\ndef load_pairwise_model_judgments(filename: str):\n    \"\"\"Load model judgments.\n\n    The return value is a dict of type:\n    Dict[judge: Tuple -> Dict[game_key: tuple -> game_result: dict]\n    \"\"\"\n    judge_dict = {}\n\n    for line in open(filename):\n        obj = json.loads(line)\n        judge = tuple(obj[\"judge\"])\n        qid, model_1, model_2 = obj[\"question_id\"], obj[\"model_1\"], obj[\"model_2\"]\n\n        if judge not in judge_dict:\n            judge_dict[judge] = {}\n\n        if \"winner\" in obj:\n            winner = obj[\"winner\"]\n        elif \"g1_winner\" in obj and \"g2_winner\" in obj:\n            g1_winner, g2_winner = obj[\"g1_winner\"], obj[\"g2_winner\"]\n            if g1_winner == g2_winner:\n                winner = g1_winner\n            else:\n                winner = \"inconsistent\"\n        else:\n            raise ValueError(f\"Invalid keys: {list(obj.keys())}\")\n\n        gamekey = (qid, model_1, model_2)\n        winners = (winner,)\n\n        judge_dict[judge][gamekey] = {\n            \"winners\": winners,\n            \"g1_judgment\": obj[\"g1_judgment\"],\n            \"g2_judgment\": obj[\"g2_judgment\"],\n        }\n\n    # Make the model names sorted in the game keys\n    normalized = {}\n    for judge, value in judge_dict.items():\n        normalized[judge] = normalize_game_key_dict(value)\n    return normalized\n\n\ndef load_single_model_judgments(filename: str):\n    \"\"\"Load model judgments.\n\n    The return value is a dict of type:\n    Dict[judge: Tuple -> Dict[game_key: tuple -> game_result: dict]\n    \"\"\"\n    judge_dict = {}\n\n    for line in open(filename):\n        obj = json.loads(line)\n        judge = tuple(obj[\"judge\"])\n        qid, model = obj[\"question_id\"], obj[\"model\"]\n\n        if judge not in judge_dict:\n            judge_dict[judge] = {}\n\n        gamekey = (qid, model)\n\n        judge_dict[judge][gamekey] = {\n            \"score\": obj[\"score\"],\n            \"judgment\": obj[\"judgment\"],\n        }\n    return judge_dict\n\n\ndef resolve_pairwise_judgment_dict(\n    question, model_judgments_normal, model_judgments_math, multi_turn=False\n):\n    \"\"\"Return the correct pairwise judge.\"\"\"\n    if multi_turn:\n        if question[\"category\"] in NEED_REF_CATS:\n            return model_judgments_math[(\"gpt-4\", \"pair-math-v1-multi-turn\")]\n        return model_judgments_normal[(\"gpt-4\", \"pair-v2-multi-turn\")]\n\n    if question[\"category\"] in NEED_REF_CATS:\n        return model_judgments_math[(\"gpt-4\", \"pair-math-v1\")]\n    else:\n        return model_judgments_normal[(\"gpt-4\", \"pair-v2\")]\n\n\ndef resolve_single_judgment_dict(\n    question, model_judgments_normal, model_judgments_math, multi_turn=False\n):\n    \"\"\"Return the correct single answer grading judge.\"\"\"\n    if multi_turn:\n        if question[\"category\"] in NEED_REF_CATS:\n            return model_judgments_math[(\"gpt-4\", \"single-math-v1-multi-turn\")]\n        return model_judgments_normal[(\"gpt-4\", \"single-v1-multi-turn\")]\n\n    if question[\"category\"] in NEED_REF_CATS:\n        return model_judgments_math[(\"gpt-4\", \"single-math-v1\")]\n    else:\n        return model_judgments_normal[(\"gpt-4\", \"single-v1\")]\n\n\ndef get_pairwise_judge_explanation(gamekey, judgment_dict):\n    \"\"\"Get model judge explanation.\"\"\"\n    try:\n        qid, model_1, model_2 = gamekey\n        if model_1 < model_2:\n            res = judgment_dict[gamekey]\n            g1_judgment, g2_judgment = res[\"g1_judgment\"], res[\"g2_judgment\"]\n        else:\n            new_gamekey = (qid, model_2, model_1)\n            res = judgment_dict[new_gamekey]\n\n            model_1, model_2 = model_1, model_2\n            g1_judgment, g2_judgment = res[\"g2_judgment\"], res[\"g1_judgment\"]\n\n        return (\n            f\"**Game 1**. **A**: {model_1}, **B**: {model_2}\\n\\n\"\n            f\"**Judgment**: {g1_judgment}\"\n            + f\"\\n\\n`--------------------------`\\n\\n\"\n            + f\"**Game 2**. **A**: {model_2}, **B**: {model_1}\\n\\n\"\n            f\"**Judgment**: {g2_judgment}\"\n        )\n    except KeyError:\n        return \"N/A\"\n\n\ndef get_single_judge_explanation(gamekey, judgment_dict):\n    \"\"\"Get model judge explanation.\"\"\"\n    try:\n        qid, model = gamekey\n\n        res = judgment_dict[gamekey]\n\n        g1_judgment = res[\"judgment\"]\n        g1_score = res[\"score\"]\n\n        return (\n            f\"**Game 1**. **A**: {model}, **Score**: {g1_score}\\n\\n\"\n            f\"**Judgment**: {g1_judgment}\"\n        )\n    except KeyError:\n        return \"N/A\"\n\n\ndef check_data(questions, model_answers, ref_answers, models, judges):\n    # check model answers\n    for m in models:\n        assert m in model_answers, f\"Missing model answer for {m}\"\n        m_answer = model_answers[m]\n        for q in questions:\n            assert (\n                q[\"question_id\"] in m_answer\n            ), f\"Missing model {m}'s answer to Question {q['question_id']}\"\n    # check ref answers\n    for jg in judges.values():\n        if not jg.ref_based:\n            continue\n        for q in questions:\n            if q[\"category\"] not in NEED_REF_CATS:\n                continue\n            assert (\n                q[\"question_id\"] in ref_answers[jg.model_name]\n            ), f\"Missing reference answer to Question {q['question_id']} for judge {jg.model_name}\"\n\n\ndef get_model_list(answer_dir):\n    file_paths = glob.glob(f\"{answer_dir}/*.jsonl\")\n    file_names = [os.path.splitext(os.path.basename(f))[0] for f in file_paths]\n    return file_names\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/model/make_delta.py", "content": "\"\"\"\nMake the delta weights by subtracting base weights.\n\nUsage:\npython3 -m fastchat.model.make_delta --base ~/model_weights/llama-13b --target ~/model_weights/vicuna-13b --delta ~/model_weights/vicuna-13b-delta --hub-repo-id lmsys/vicuna-13b-delta-v1.1\n\"\"\"\nimport argparse\n\nimport torch\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n\ndef make_delta(base_model_path, target_model_path, delta_path):\n    print(f\"Loading the base model from {base_model_path}\")\n    base = AutoModelForCausalLM.from_pretrained(\n        base_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True\n    )\n\n    print(f\"Loading the target model from {target_model_path}\")\n    target = AutoModelForCausalLM.from_pretrained(\n        target_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True\n    )\n    target_tokenizer = AutoTokenizer.from_pretrained(target_model_path, use_fast=False)\n\n    print(\"Calculating the delta\")\n    for name, param in tqdm(target.state_dict().items(), desc=\"Calculating delta\"):\n        assert name in base.state_dict()\n        param.data -= base.state_dict()[name]\n\n    print(f\"Saving the delta to {delta_path}\")\n    if args.hub_repo_id:\n        kwargs = {\"push_to_hub\": True, \"repo_id\": args.hub_repo_id}\n    else:\n        kwargs = {}\n    target.save_pretrained(delta_path, **kwargs)\n    target_tokenizer.save_pretrained(delta_path, **kwargs)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--base-model-path\", type=str, required=True)\n    parser.add_argument(\"--target-model-path\", type=str, required=True)\n    parser.add_argument(\"--delta-path\", type=str, required=True)\n    parser.add_argument(\"--hub-repo-id\", type=str)\n    args = parser.parse_args()\n\n    make_delta(args.base_model_path, args.target_model_path, args.delta_path)\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/model/rwkv_model.py", "content": "import os\nfrom types import SimpleNamespace\nimport warnings\n\nimport torch\n\nos.environ[\"RWKV_JIT_ON\"] = \"1\"\nos.environ[\"RWKV_CUDA_ON\"] = \"1\"\n\nfrom rwkv.model import RWKV\nfrom rwkv.utils import PIPELINE, PIPELINE_ARGS\n\n\nclass RwkvModel:\n    def __init__(self, model_path):\n        warnings.warn(\n            \"Experimental support. Please use ChatRWKV if you want to chat with RWKV\"\n        )\n        self.config = SimpleNamespace(is_encoder_decoder=False)\n        self.model = RWKV(model=model_path, strategy=\"cuda fp16\")\n        # two GPUs\n        # self.model = RWKV(model=model_path, strategy=\"cuda:0 fp16 *20 -> cuda:1 fp16\")\n\n        self.tokenizer = None\n        self.model_path = model_path\n\n    def to(self, target):\n        assert target == \"cuda\"\n\n    def __call__(self, input_ids, use_cache, past_key_values=None):\n        assert use_cache == True\n        input_ids = input_ids[0].detach().cpu().numpy()\n        # print(input_ids)\n        logits, state = self.model.forward(input_ids, past_key_values)\n        # print(logits)\n        logits = logits.unsqueeze(0).unsqueeze(0)\n        out = SimpleNamespace(logits=logits, past_key_values=state)\n        return out\n\n    def generate(\n        self, input_ids, do_sample, temperature, max_new_tokens, repetition_penalty=1.0\n    ):\n        # This function is used by fastchat.llm_judge.\n        # Because RWKV does not support huggingface generation API,\n        # we reuse fastchat.serve.inference.generate_stream as a workaround.\n        from transformers import AutoTokenizer\n\n        from fastchat.serve.inference import generate_stream\n        from fastchat.conversation import get_conv_template\n\n        if self.tokenizer is None:\n            self.tokenizer = AutoTokenizer.from_pretrained(\n                \"EleutherAI/pythia-160m\", use_fast=True\n            )\n        prompt = self.tokenizer.decode(input_ids[0].tolist())\n        conv = get_conv_template(\"rwkv\")\n\n        gen_params = {\n            \"model\": self.model_path,\n            \"prompt\": prompt,\n            \"temperature\": temperature,\n            \"repetition_penalty\": repetition_penalty,\n            \"max_new_tokens\": max_new_tokens,\n            \"stop\": conv.stop_str,\n            \"stop_token_ids\": conv.stop_token_ids,\n            \"echo\": False,\n        }\n        res_iter = generate_stream(self, self.tokenizer, gen_params, \"cuda\")\n\n        for res in res_iter:\n            pass\n\n        output = res[\"text\"]\n        output_ids = self.tokenizer.encode(output)\n\n        return [input_ids[0].tolist() + output_ids]\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/model/__init__.py", "content": "from fastchat.model.model_adapter import (\n    load_model,\n    get_conversation_template,\n    add_model_args,\n)\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/model/apply_lora.py", "content": "\"\"\"\nApply the LoRA weights on top of a base model.\n\nUsage:\npython3 -m fastchat.model.apply_lora --base ~/model_weights/llama-7b --target ~/model_weights/baize-7b --lora project-baize/baize-lora-7B\n\nDependency:\npip3 install git+https://github.com/huggingface/peft.git@2822398fbe896f25d4dac5e468624dc5fd65a51b\n\"\"\"\nimport argparse\n\nimport torch\nfrom peft import PeftModel\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n\ndef apply_lora(base_model_path, target_model_path, lora_path):\n    print(f\"Loading the base model from {base_model_path}\")\n    base = AutoModelForCausalLM.from_pretrained(\n        base_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True\n    )\n    base_tokenizer = AutoTokenizer.from_pretrained(base_model_path, use_fast=False)\n\n    print(f\"Loading the LoRA adapter from {lora_path}\")\n\n    lora_model = PeftModel.from_pretrained(\n        base,\n        lora_path,\n        # torch_dtype=torch.float16\n    )\n\n    print(\"Applying the LoRA\")\n    model = lora_model.merge_and_unload()\n\n    print(f\"Saving the target model to {target_model_path}\")\n    model.save_pretrained(target_model_path)\n    base_tokenizer.save_pretrained(target_model_path)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--base-model-path\", type=str, required=True)\n    parser.add_argument(\"--target-model-path\", type=str, required=True)\n    parser.add_argument(\"--lora-path\", type=str, required=True)\n\n    args = parser.parse_args()\n\n    apply_lora(args.base_model_path, args.target_model_path, args.lora_path)\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/model/compression.py", "content": "import dataclasses\nimport gc\nimport glob\nimport os\n\nfrom accelerate import init_empty_weights\nfrom accelerate.utils import set_module_tensor_to_device\nfrom huggingface_hub import snapshot_download\nimport torch\nfrom torch import Tensor\nfrom torch.nn import functional as F\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom transformers import (\n    AutoConfig,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    AutoModel,\n    AutoModelForSeq2SeqLM,\n)\n\n\n@dataclasses.dataclass\nclass CompressionConfig:\n    \"\"\"Group-wise quantization.\"\"\"\n\n    num_bits: int\n    group_size: int\n    group_dim: int\n    symmetric: bool\n    enabled: bool = True\n\n\ndefault_compression_config = CompressionConfig(\n    num_bits=8, group_size=256, group_dim=1, symmetric=True, enabled=True\n)\n\n\nclass CLinear(nn.Module):\n    \"\"\"Compressed Linear Layer.\"\"\"\n\n    def __init__(self, weight=None, bias=None, device=None):\n        super().__init__()\n        if weight is None:\n            self.weight = None\n        elif isinstance(weight, Tensor):\n            self.weight = compress(weight.data.to(device), default_compression_config)\n        else:\n            self.weight = weight\n        self.bias = bias\n\n    def forward(self, input: Tensor) -> Tensor:\n        weight = decompress(self.weight, default_compression_config)\n        if self.bias is None:\n            return F.linear(input.to(weight.dtype), weight)\n        return F.linear(input.to(weight.dtype), weight, self.bias.to(weight.dtype))\n\n\ndef compress_module(module, target_device):\n    for attr_str in dir(module):\n        target_attr = getattr(module, attr_str)\n        if type(target_attr) == torch.nn.Linear:\n            setattr(\n                module,\n                attr_str,\n                CLinear(target_attr.weight, target_attr.bias, target_device),\n            )\n    for name, child in module.named_children():\n        compress_module(child, target_device)\n\n\ndef get_compressed_list(module, prefix=\"\"):\n    compressed_list = []\n    for attr_str in dir(module):\n        target_attr = getattr(module, attr_str)\n        if type(target_attr) == torch.nn.Linear:\n            full_name = (\n                f\"{prefix}.{attr_str}.weight\" if prefix else f\"{attr_str}.weight\"\n            )\n            compressed_list.append(full_name)\n    for name, child in module.named_children():\n        child_prefix = f\"{prefix}.{name}\" if prefix else name\n        for each in get_compressed_list(child, child_prefix):\n            compressed_list.append(each)\n    return compressed_list\n\n\ndef apply_compressed_weight(module, compressed_state_dict, target_device, prefix=\"\"):\n    for attr_str in dir(module):\n        target_attr = getattr(module, attr_str)\n        if type(target_attr) == torch.nn.Linear:\n            full_name = (\n                f\"{prefix}.{attr_str}.weight\" if prefix else f\"{attr_str}.weight\"\n            )\n            setattr(\n                module,\n                attr_str,\n                CLinear(\n                    compressed_state_dict[full_name], target_attr.bias, target_device\n                ),\n            )\n    for name, child in module.named_children():\n        child_prefix = f\"{prefix}.{name}\" if prefix else name\n        apply_compressed_weight(\n            child, compressed_state_dict, target_device, child_prefix\n        )\n\n\ndef load_compress_model(model_path, device, torch_dtype, use_fast, revision=\"main\"):\n    # partially load model\n    # `use_fast=True`` is not supported for some models.\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, use_fast=use_fast, revision=revision, trust_remote_code=True\n        )\n    except TypeError:\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, use_fast=~use_fast, revision=revision, trust_remote_code=True\n        )\n    with init_empty_weights():\n        # `trust_remote_code` should be set as `True` for both AutoConfig and AutoModel\n        config = AutoConfig.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            torch_dtype=torch_dtype,\n            trust_remote_code=True,\n            revision=revision,\n        )\n        # some models are loaded by AutoModel but not AutoModelForCausalLM,\n        # such as chatglm, chatglm2\n        try:\n            # google/flan-* models are based on an AutoModelForSeq2SeqLM.\n            if \"T5Config\" in str(type(config)):\n                model = AutoModelForSeq2SeqLM.from_config(\n                    config, trust_remote_code=True\n                )\n            else:\n                model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)\n        except NameError:\n            model = AutoModel.from_config(config, trust_remote_code=True)\n        linear_weights = get_compressed_list(model)\n    if os.path.exists(model_path):\n        # `model_path` is a local folder\n        base_pattern = os.path.join(model_path, \"pytorch_model*.bin\")\n    else:\n        # `model_path` is a cached Hugging Face repo\n        # We don't necessarily need to download the model' repo again if there is a cache.\n        # So check the default huggingface cache first.\n        model_path_temp = os.path.join(\n            os.path.expanduser(\"~\"),\n            \".cache/huggingface/hub\",\n            \"models--\" + model_path.replace(\"/\", \"--\"),\n            \"snapshots/\",\n        )\n        downloaded = False\n        if os.path.exists(model_path_temp):\n            temp_last_dir = os.listdir(model_path_temp)[-1]\n            model_path_temp = os.path.join(model_path_temp, temp_last_dir)\n            base_pattern = os.path.join(model_path_temp, \"pytorch_model*.bin\")\n            files = glob.glob(base_pattern)\n            if len(files) > 0:\n                downloaded = True\n\n        if downloaded:\n            model_path = model_path_temp\n        else:\n            model_path = snapshot_download(model_path, revision=revision)\n        base_pattern = os.path.join(model_path, \"pytorch_model*.bin\")\n\n    files = glob.glob(base_pattern)\n    if len(files) == 0:\n        raise ValueError(\n            f\"Cannot find any model weight files. \"\n            f\"Please check your (cached) weight path: {model_path}\"\n        )\n\n    compressed_state_dict = {}\n    for filename in tqdm(files):\n        tmp_state_dict = torch.load(filename, map_location=lambda storage, loc: storage)\n        for name in tmp_state_dict:\n            if name in linear_weights:\n                tensor = tmp_state_dict[name].to(device, dtype=torch_dtype)\n                compressed_state_dict[name] = compress(\n                    tensor, default_compression_config\n                )\n            else:\n                compressed_state_dict[name] = tmp_state_dict[name].to(\n                    device, dtype=torch_dtype\n                )\n            tmp_state_dict[name] = None\n            tensor = None\n            gc.collect()\n            torch.cuda.empty_cache()\n            if device == \"xpu\":\n                torch.xpu.empty_cache()\n            if device == \"npu\":\n                torch.npu.empty_cache()\n\n    for name in model.state_dict():\n        if name not in linear_weights:\n            set_module_tensor_to_device(\n                model, name, device, value=compressed_state_dict[name]\n            )\n    apply_compressed_weight(model, compressed_state_dict, device)\n\n    if torch_dtype == torch.float16:\n        model.half()\n    model.to(device)\n    model.eval()\n\n    return model, tokenizer\n\n\ndef compress(tensor, config):\n    \"\"\"Simulate group-wise quantization.\"\"\"\n    if not config.enabled:\n        return tensor\n\n    group_size, num_bits, group_dim, symmetric = (\n        config.group_size,\n        config.num_bits,\n        config.group_dim,\n        config.symmetric,\n    )\n    assert num_bits <= 8\n\n    original_shape = tensor.shape\n    num_groups = (original_shape[group_dim] + group_size - 1) // group_size\n    new_shape = (\n        original_shape[:group_dim]\n        + (num_groups, group_size)\n        + original_shape[group_dim + 1 :]\n    )\n\n    # Pad\n    pad_len = (group_size - original_shape[group_dim] % group_size) % group_size\n    if pad_len != 0:\n        pad_shape = (\n            original_shape[:group_dim] + (pad_len,) + original_shape[group_dim + 1 :]\n        )\n        tensor = torch.cat(\n            [tensor, torch.zeros(pad_shape, dtype=tensor.dtype, device=tensor.device)],\n            dim=group_dim,\n        )\n    data = tensor.view(new_shape)\n\n    # Quantize\n    if symmetric:\n        B = 2 ** (num_bits - 1) - 1\n        scale = B / torch.max(data.abs(), dim=group_dim + 1, keepdim=True)[0]\n        data = data * scale\n        data = data.clamp_(-B, B).round_().to(torch.int8)\n        return data, scale, original_shape\n    else:\n        B = 2**num_bits - 1\n        mn = torch.min(data, dim=group_dim + 1, keepdim=True)[0]\n        mx = torch.max(data, dim=group_dim + 1, keepdim=True)[0]\n\n        scale = B / (mx - mn)\n        data = data - mn\n        data.mul_(scale)\n\n        data = data.clamp_(0, B).round_().to(torch.uint8)\n        return data, mn, scale, original_shape\n\n\ndef decompress(packed_data, config):\n    \"\"\"Simulate group-wise dequantization.\"\"\"\n    if not config.enabled:\n        return packed_data\n\n    group_size, num_bits, group_dim, symmetric = (\n        config.group_size,\n        config.num_bits,\n        config.group_dim,\n        config.symmetric,\n    )\n\n    # Dequantize\n    if symmetric:\n        data, scale, original_shape = packed_data\n        data = data / scale\n    else:\n        data, mn, scale, original_shape = packed_data\n        data = data / scale\n        data.add_(mn)\n\n    # Unpad\n    pad_len = (group_size - original_shape[group_dim] % group_size) % group_size\n    if pad_len:\n        padded_original_shape = (\n            original_shape[:group_dim]\n            + (original_shape[group_dim] + pad_len,)\n            + original_shape[group_dim + 1 :]\n        )\n        data = data.reshape(padded_original_shape)\n        indices = [slice(0, x) for x in original_shape]\n        return data[indices].contiguous()\n    else:\n        return data.view(original_shape)\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/llm_judge/qa_browser.py", "content": "\"\"\"\nUsage:\npython3 qa_browser.py --share\n\"\"\"\n\nimport argparse\nfrom collections import defaultdict\nimport re\n\nimport gradio as gr\n\nfrom fastchat.llm_judge.common import (\n    load_questions,\n    load_model_answers,\n    load_single_model_judgments,\n    load_pairwise_model_judgments,\n    resolve_single_judgment_dict,\n    resolve_pairwise_judgment_dict,\n    get_single_judge_explanation,\n    get_pairwise_judge_explanation,\n)\n\n\nquestions = []\nmodel_answers = {}\n\nmodel_judgments_normal_single = {}\nmodel_judgments_math_single = {}\n\nmodel_judgments_normal_pairwise = {}\nmodel_judgments_math_pairwise = {}\n\nquestion_selector_map = {}\ncategory_selector_map = defaultdict(list)\n\n\ndef display_question(category_selector, request: gr.Request):\n    choices = category_selector_map[category_selector]\n    return gr.Dropdown.update(\n        value=choices[0],\n        choices=choices,\n    )\n\n\ndef display_pairwise_answer(\n    question_selector, model_selector1, model_selector2, request: gr.Request\n):\n    q = question_selector_map[question_selector]\n    qid = q[\"question_id\"]\n\n    ans1 = model_answers[model_selector1][qid]\n    ans2 = model_answers[model_selector2][qid]\n\n    chat_mds = pairwise_to_gradio_chat_mds(q, ans1, ans2)\n    gamekey = (qid, model_selector1, model_selector2)\n\n    judgment_dict = resolve_pairwise_judgment_dict(\n        q,\n        model_judgments_normal_pairwise,\n        model_judgments_math_pairwise,\n        multi_turn=False,\n    )\n\n    explanation = (\n        \"##### Model Judgment (first turn)\\n\"\n        + get_pairwise_judge_explanation(gamekey, judgment_dict)\n    )\n\n    judgment_dict_turn2 = resolve_pairwise_judgment_dict(\n        q,\n        model_judgments_normal_pairwise,\n        model_judgments_math_pairwise,\n        multi_turn=True,\n    )\n\n    explanation_turn2 = (\n        \"##### Model Judgment (second turn)\\n\"\n        + get_pairwise_judge_explanation(gamekey, judgment_dict_turn2)\n    )\n\n    return chat_mds + [explanation] + [explanation_turn2]\n\n\ndef display_single_answer(question_selector, model_selector1, request: gr.Request):\n    q = question_selector_map[question_selector]\n    qid = q[\"question_id\"]\n\n    ans1 = model_answers[model_selector1][qid]\n\n    chat_mds = single_to_gradio_chat_mds(q, ans1)\n    gamekey = (qid, model_selector1)\n\n    judgment_dict = resolve_single_judgment_dict(\n        q, model_judgments_normal_single, model_judgments_math_single, multi_turn=False\n    )\n\n    explanation = \"##### Model Judgment (first turn)\\n\" + get_single_judge_explanation(\n        gamekey, judgment_dict\n    )\n\n    judgment_dict_turn2 = resolve_single_judgment_dict(\n        q, model_judgments_normal_single, model_judgments_math_single, multi_turn=True\n    )\n\n    explanation_turn2 = (\n        \"##### Model Judgment (second turn)\\n\"\n        + get_single_judge_explanation(gamekey, judgment_dict_turn2)\n    )\n\n    return chat_mds + [explanation] + [explanation_turn2]\n\n\nnewline_pattern1 = re.compile(\"\\n\\n(\\d+\\. )\")\nnewline_pattern2 = re.compile(\"\\n\\n(- )\")\n\n\ndef post_process_answer(x):\n    \"\"\"Fix Markdown rendering problems.\"\"\"\n    x = x.replace(\"\\u2022\", \"- \")\n    x = re.sub(newline_pattern1, \"\\n\\g<1>\", x)\n    x = re.sub(newline_pattern2, \"\\n\\g<1>\", x)\n    return x\n\n\ndef pairwise_to_gradio_chat_mds(question, ans_a, ans_b, turn=None):\n    end = len(question[\"turns\"]) if turn is None else turn + 1\n\n    mds = [\"\", \"\", \"\", \"\", \"\", \"\", \"\"]\n    for i in range(end):\n        base = i * 3\n        if i == 0:\n            mds[base + 0] = \"##### User\\n\" + question[\"turns\"][i]\n        else:\n            mds[base + 0] = \"##### User's follow-up question \\n\" + question[\"turns\"][i]\n        mds[base + 1] = \"##### Assistant A\\n\" + post_process_answer(\n            ans_a[\"choices\"][0][\"turns\"][i].strip()\n        )\n        mds[base + 2] = \"##### Assistant B\\n\" + post_process_answer(\n            ans_b[\"choices\"][0][\"turns\"][i].strip()\n        )\n\n    ref = question.get(\"reference\", [\"\", \"\"])\n\n    ref_md = \"\"\n    if turn is None:\n        if ref[0] != \"\" or ref[1] != \"\":\n            mds[6] = f\"##### Reference Solution\\nQ1. {ref[0]}\\nQ2. {ref[1]}\"\n    else:\n        x = ref[turn] if turn < len(ref) else \"\"\n        if x:\n            mds[6] = f\"##### Reference Solution\\n{ref[turn]}\"\n        else:\n            mds[6] = \"\"\n    return mds\n\n\ndef single_to_gradio_chat_mds(question, ans, turn=None):\n    end = len(question[\"turns\"]) if turn is None else turn + 1\n\n    mds = [\"\", \"\", \"\", \"\", \"\"]\n    for i in range(end):\n        base = i * 2\n        if i == 0:\n            mds[base + 0] = \"##### User\\n\" + question[\"turns\"][i]\n        else:\n            mds[base + 0] = \"##### User's follow-up question \\n\" + question[\"turns\"][i]\n        mds[base + 1] = \"##### Assistant A\\n\" + post_process_answer(\n            ans[\"choices\"][0][\"turns\"][i].strip()\n        )\n\n    ref = question.get(\"reference\", [\"\", \"\"])\n\n    ref_md = \"\"\n    if turn is None:\n        if ref[0] != \"\" or ref[1] != \"\":\n            mds[4] = f\"##### Reference Solution\\nQ1. {ref[0]}\\nQ2. {ref[1]}\"\n    else:\n        x = ref[turn] if turn < len(ref) else \"\"\n        if x:\n            mds[4] = f\"##### Reference Solution\\n{ref[turn]}\"\n        else:\n            mds[4] = \"\"\n    return mds\n\n\ndef build_question_selector_map():\n    global question_selector_map, category_selector_map\n\n    # Build question selector map\n    for q in questions:\n        preview = f\"{q['question_id']}: \" + q[\"turns\"][0][:128] + \"...\"\n        question_selector_map[preview] = q\n        category_selector_map[q[\"category\"]].append(preview)\n\n\ndef build_pairwise_browser_tab():\n    global question_selector_map, category_selector_map\n\n    models = list(model_answers.keys())\n    num_sides = 2\n    num_turns = 2\n    side_names = [\"A\", \"B\"]\n\n    question_selector_choices = list(question_selector_map.keys())\n    category_selector_choices = list(category_selector_map.keys())\n\n    # Selectors\n    with gr.Row():\n        with gr.Column(scale=1, min_width=200):\n            category_selector = gr.Dropdown(\n                choices=category_selector_choices, label=\"Category\", container=False\n            )\n        with gr.Column(scale=100):\n            question_selector = gr.Dropdown(\n                choices=question_selector_choices, label=\"Question\", container=False\n            )\n\n    model_selectors = [None] * num_sides\n    with gr.Row():\n        for i in range(num_sides):\n            with gr.Column():\n                if i == 0:\n                    value = models[0]\n                else:\n                    value = \"gpt-3.5-turbo\"\n                model_selectors[i] = gr.Dropdown(\n                    choices=models,\n                    value=value,\n                    label=f\"Model {side_names[i]}\",\n                    container=False,\n                )\n\n    # Conversation\n    chat_mds = []\n    for i in range(num_turns):\n        chat_mds.append(gr.Markdown(elem_id=f\"user_question_{i+1}\"))\n        with gr.Row():\n            for j in range(num_sides):\n                with gr.Column(scale=100):\n                    chat_mds.append(gr.Markdown())\n\n                if j == 0:\n                    with gr.Column(scale=1, min_width=8):\n                        gr.Markdown()\n    reference = gr.Markdown(elem_id=f\"reference\")\n    chat_mds.append(reference)\n\n    model_explanation = gr.Markdown(elem_id=\"model_explanation\")\n    model_explanation2 = gr.Markdown(elem_id=\"model_explanation\")\n\n    # Callbacks\n    category_selector.change(display_question, [category_selector], [question_selector])\n    question_selector.change(\n        display_pairwise_answer,\n        [question_selector] + model_selectors,\n        chat_mds + [model_explanation] + [model_explanation2],\n    )\n\n    for i in range(num_sides):\n        model_selectors[i].change(\n            display_pairwise_answer,\n            [question_selector] + model_selectors,\n            chat_mds + [model_explanation] + [model_explanation2],\n        )\n\n    return (category_selector,)\n\n\ndef build_single_answer_browser_tab():\n    global question_selector_map, category_selector_map\n\n    models = list(model_answers.keys())\n    num_sides = 1\n    num_turns = 2\n    side_names = [\"A\"]\n\n    question_selector_choices = list(question_selector_map.keys())\n    category_selector_choices = list(category_selector_map.keys())\n\n    # Selectors\n    with gr.Row():\n        with gr.Column(scale=1, min_width=200):\n            category_selector = gr.Dropdown(\n                choices=category_selector_choices, label=\"Category\", container=False\n            )\n        with gr.Column(scale=100):\n            question_selector = gr.Dropdown(\n                choices=question_selector_choices, label=\"Question\", container=False\n            )\n\n    model_selectors = [None] * num_sides\n    with gr.Row():\n        for i in range(num_sides):\n            with gr.Column():\n                model_selectors[i] = gr.Dropdown(\n                    choices=models,\n                    value=models[i] if len(models) > i else \"\",\n                    label=f\"Model {side_names[i]}\",\n                    container=False,\n                )\n\n    # Conversation\n    chat_mds = []\n    for i in range(num_turns):\n        chat_mds.append(gr.Markdown(elem_id=f\"user_question_{i+1}\"))\n        with gr.Row():\n            for j in range(num_sides):\n                with gr.Column(scale=100):\n                    chat_mds.append(gr.Markdown())\n\n                if j == 0:\n                    with gr.Column(scale=1, min_width=8):\n                        gr.Markdown()\n\n    reference = gr.Markdown(elem_id=f\"reference\")\n    chat_mds.append(reference)\n\n    model_explanation = gr.Markdown(elem_id=\"model_explanation\")\n    model_explanation2 = gr.Markdown(elem_id=\"model_explanation\")\n\n    # Callbacks\n    category_selector.change(display_question, [category_selector], [question_selector])\n    question_selector.change(\n        display_single_answer,\n        [question_selector] + model_selectors,\n        chat_mds + [model_explanation] + [model_explanation2],\n    )\n\n    for i in range(num_sides):\n        model_selectors[i].change(\n            display_single_answer,\n            [question_selector] + model_selectors,\n            chat_mds + [model_explanation] + [model_explanation2],\n        )\n\n    return (category_selector,)\n\n\nblock_css = \"\"\"\n#user_question_1 {\n    background-color: #DEEBF7;\n}\n#user_question_2 {\n    background-color: #E2F0D9;\n}\n#reference {\n    background-color: #FFF2CC;\n}\n#model_explanation {\n    background-color: #FBE5D6;\n}\n\"\"\"\n\n\ndef load_demo():\n    dropdown_update = gr.Dropdown.update(value=list(category_selector_map.keys())[0])\n    return dropdown_update, dropdown_update\n\n\ndef build_demo():\n    build_question_selector_map()\n\n    with gr.Blocks(\n        title=\"MT-Bench Browser\",\n        theme=gr.themes.Base(text_size=gr.themes.sizes.text_lg),\n        css=block_css,\n    ) as demo:\n        gr.Markdown(\n            \"\"\"\n# MT-Bench Browser\nThe code to generate answers and judgments is at [fastchat.llm_judge](https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge).\n\"\"\"\n        )\n        with gr.Tab(\"Single Answer Grading\"):\n            (category_selector,) = build_single_answer_browser_tab()\n        with gr.Tab(\"Pairwise Comparison\"):\n            (category_selector2,) = build_pairwise_browser_tab()\n        demo.load(load_demo, [], [category_selector, category_selector2])\n\n    return demo\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\")\n    parser.add_argument(\"--port\", type=int)\n    parser.add_argument(\"--share\", action=\"store_true\")\n    parser.add_argument(\"--bench-name\", type=str, default=\"mt_bench\")\n    args = parser.parse_args()\n    print(args)\n\n    question_file = f\"data/{args.bench_name}/question.jsonl\"\n    answer_dir = f\"data/{args.bench_name}/model_answer\"\n    pairwise_model_judgment_file = (\n        f\"data/{args.bench_name}/model_judgment/gpt-4_pair.jsonl\"\n    )\n    single_model_judgment_file = (\n        f\"data/{args.bench_name}/model_judgment/gpt-4_single.jsonl\"\n    )\n\n    # Load questions\n    questions = load_questions(question_file, None, None)\n\n    # Load answers\n    model_answers = load_model_answers(answer_dir)\n\n    # Load model judgments\n    model_judgments_normal_single = (\n        model_judgments_math_single\n    ) = load_single_model_judgments(single_model_judgment_file)\n    model_judgments_normal_pairwise = (\n        model_judgments_math_pairwise\n    ) = load_pairwise_model_judgments(pairwise_model_judgment_file)\n\n    demo = build_demo()\n    demo.queue(concurrency_count=10, status_update_rate=10, api_open=False).launch(\n        server_name=args.host, server_port=args.port, share=args.share, max_threads=200\n    )\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/model/model_chatglm.py", "content": "\"\"\"\nInference code for ChatGLM.\nAdapted from https://huggingface.co/THUDM/chatglm-6b/blob/main/modeling_chatglm.py.\n\"\"\"\nimport re\n\nimport torch\nfrom transformers.generation.logits_process import LogitsProcessor\n\n\nclass InvalidScoreLogitsProcessor(LogitsProcessor):\n    def __call__(\n        self, input_ids: torch.LongTensor, scores: torch.FloatTensor\n    ) -> torch.FloatTensor:\n        if torch.isnan(scores).any() or torch.isinf(scores).any():\n            scores.zero_()\n            scores[..., 5] = 5e4\n        return scores\n\n\ninvalid_score_processor = InvalidScoreLogitsProcessor()\n\n\ndef process_response(response):\n    response = response.strip()\n    response = response.replace(\"[[训练时间]]\", \"2023年\")\n    punkts = [\n        [\",\", \"，\"],\n        [\"!\", \"！\"],\n        [\":\", \"：\"],\n        [\";\", \"；\"],\n        [\"\\?\", \"？\"],\n    ]\n    for item in punkts:\n        response = re.sub(r\"([\\u4e00-\\u9fff])%s\" % item[0], r\"\\1%s\" % item[1], response)\n        response = re.sub(r\"%s([\\u4e00-\\u9fff])\" % item[0], r\"%s\\1\" % item[1], response)\n    return response\n\n\n@torch.inference_mode()\ndef generate_stream_chatglm(\n    model,\n    tokenizer,\n    params,\n    device,\n    context_len=2048,\n    stream_interval=2,\n    judge_sent_end=False,\n):\n    prompt = params[\"prompt\"]\n    temperature = float(params.get(\"temperature\", 1.0))\n    repetition_penalty = float(params.get(\"repetition_penalty\", 1.0))\n    top_p = float(params.get(\"top_p\", 1.0))\n    max_new_tokens = int(params.get(\"max_new_tokens\", 256))\n    echo = params.get(\"echo\", True)\n\n    inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n    input_echo_len = len(inputs[\"input_ids\"][0])\n\n    gen_kwargs = {\n        \"max_length\": max_new_tokens + input_echo_len,\n        \"do_sample\": True if temperature > 1e-5 else False,\n        \"top_p\": top_p,\n        \"repetition_penalty\": repetition_penalty,\n        \"logits_processor\": [invalid_score_processor],\n    }\n    if temperature > 1e-5:\n        gen_kwargs[\"temperature\"] = temperature\n\n    total_len = 0\n    for total_ids in model.stream_generate(**inputs, **gen_kwargs):\n        total_ids = total_ids.tolist()[0]\n        total_len = len(total_ids)\n        if echo:\n            output_ids = total_ids\n        else:\n            output_ids = total_ids[input_echo_len:]\n        response = tokenizer.decode(output_ids)\n        response = process_response(response)\n\n        yield {\n            \"text\": response,\n            \"usage\": {\n                \"prompt_tokens\": input_echo_len,\n                \"completion_tokens\": total_len - input_echo_len,\n                \"total_tokens\": total_len,\n            },\n            \"finish_reason\": None,\n        }\n\n    # TODO: ChatGLM stop when it reach max length\n    # Only last stream result contains finish_reason, we set finish_reason as stop\n    ret = {\n        \"text\": response,\n        \"usage\": {\n            \"prompt_tokens\": input_echo_len,\n            \"completion_tokens\": total_len - input_echo_len,\n            \"total_tokens\": total_len,\n        },\n        \"finish_reason\": \"stop\",\n    }\n    yield ret\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/protocol/api_protocol.py", "content": "from typing import Literal, Optional, List, Dict, Any, Union\n\nimport time\n\nimport shortuuid\nfrom pydantic import BaseModel, Field\n\n\nclass ErrorResponse(BaseModel):\n    object: str = \"error\"\n    message: str\n    code: int\n\n\nclass ModelPermission(BaseModel):\n    id: str = Field(default_factory=lambda: f\"modelperm-{shortuuid.random()}\")\n    object: str = \"model_permission\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    allow_create_engine: bool = False\n    allow_sampling: bool = True\n    allow_logprobs: bool = True\n    allow_search_indices: bool = True\n    allow_view: bool = True\n    allow_fine_tuning: bool = False\n    organization: str = \"*\"\n    group: Optional[str] = None\n    is_blocking: str = False\n\n\nclass ModelCard(BaseModel):\n    id: str\n    object: str = \"model\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    owned_by: str = \"fastchat\"\n    root: Optional[str] = None\n    parent: Optional[str] = None\n    permission: List[ModelPermission] = []\n\n\nclass ModelList(BaseModel):\n    object: str = \"list\"\n    data: List[ModelCard] = []\n\n\nclass UsageInfo(BaseModel):\n    prompt_tokens: int = 0\n    total_tokens: int = 0\n    completion_tokens: Optional[int] = 0\n\n\nclass APIChatCompletionRequest(BaseModel):\n    model: str\n    messages: Union[str, List[Dict[str, str]]]\n    temperature: Optional[float] = 0.7\n    top_p: Optional[float] = 1.0\n    top_k: Optional[int] = -1\n    n: Optional[int] = 1\n    max_tokens: Optional[int] = None\n    stop: Optional[Union[str, List[str]]] = None\n    stream: Optional[bool] = False\n    user: Optional[str] = None\n    repetition_penalty: Optional[float] = 1.0\n    frequency_penalty: Optional[float] = 0.0\n    presence_penalty: Optional[float] = 0.0\n\n\nclass ChatMessage(BaseModel):\n    role: str\n    content: str\n\n\nclass ChatCompletionResponseChoice(BaseModel):\n    index: int\n    message: ChatMessage\n    finish_reason: Optional[Literal[\"stop\", \"length\"]] = None\n\n\nclass ChatCompletionResponse(BaseModel):\n    id: str = Field(default_factory=lambda: f\"chatcmpl-{shortuuid.random()}\")\n    object: str = \"chat.completion\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: List[ChatCompletionResponseChoice]\n    usage: UsageInfo\n\n\nclass DeltaMessage(BaseModel):\n    role: Optional[str] = None\n    content: Optional[str] = None\n\n\nclass ChatCompletionResponseStreamChoice(BaseModel):\n    index: int\n    delta: DeltaMessage\n    finish_reason: Optional[Literal[\"stop\", \"length\"]] = None\n\n\nclass ChatCompletionStreamResponse(BaseModel):\n    id: str = Field(default_factory=lambda: f\"chatcmpl-{shortuuid.random()}\")\n    object: str = \"chat.completion.chunk\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: List[ChatCompletionResponseStreamChoice]\n\n\nclass APITokenCheckRequestItem(BaseModel):\n    model: str\n    prompt: str\n    max_tokens: int\n\n\nclass APITokenCheckRequest(BaseModel):\n    prompts: List[APITokenCheckRequestItem]\n\n\nclass APITokenCheckResponseItem(BaseModel):\n    fits: bool\n    tokenCount: int\n    contextLength: int\n\n\nclass APITokenCheckResponse(BaseModel):\n    prompts: List[APITokenCheckResponseItem]\n\n\nclass CompletionRequest(BaseModel):\n    model: str\n    prompt: Union[str, List[Any]]\n    suffix: Optional[str] = None\n    temperature: Optional[float] = 0.7\n    n: Optional[int] = 1\n    max_tokens: Optional[int] = 16\n    stop: Optional[Union[str, List[str]]] = None\n    stream: Optional[bool] = False\n    top_p: Optional[float] = 1.0\n    top_k: Optional[int] = -1\n    logprobs: Optional[int] = None\n    echo: Optional[bool] = False\n    presence_penalty: Optional[float] = 0.0\n    frequency_penalty: Optional[float] = 0.0\n    user: Optional[str] = None\n\n\nclass CompletionResponseChoice(BaseModel):\n    index: int\n    text: str\n    logprobs: Optional[int] = None\n    finish_reason: Optional[Literal[\"stop\", \"length\"]] = None\n\n\nclass CompletionResponse(BaseModel):\n    id: str = Field(default_factory=lambda: f\"cmpl-{shortuuid.random()}\")\n    object: str = \"text_completion\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: List[CompletionResponseChoice]\n    usage: UsageInfo\n\n\nclass CompletionResponseStreamChoice(BaseModel):\n    index: int\n    text: str\n    logprobs: Optional[float] = None\n    finish_reason: Optional[Literal[\"stop\", \"length\"]] = None\n\n\nclass CompletionStreamResponse(BaseModel):\n    id: str = Field(default_factory=lambda: f\"cmpl-{shortuuid.random()}\")\n    object: str = \"text_completion\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: List[CompletionResponseStreamChoice]\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/serve/controller.py", "content": "\"\"\"\nA controller manages distributed workers.\nIt sends worker addresses to clients.\n\"\"\"\nimport argparse\nimport asyncio\nimport dataclasses\nfrom enum import Enum, auto\nimport json\nimport logging\nimport os\nimport time\nfrom typing import List, Union\nimport threading\n\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import StreamingResponse\nimport numpy as np\nimport requests\nimport uvicorn\n\nfrom fastchat.constants import (\n    CONTROLLER_HEART_BEAT_EXPIRATION,\n    WORKER_API_TIMEOUT,\n    ErrorCode,\n    SERVER_ERROR_MSG,\n)\nfrom fastchat.utils import build_logger\n\n\nlogger = build_logger(\"controller\", \"controller.log\")\n\n\nclass DispatchMethod(Enum):\n    LOTTERY = auto()\n    SHORTEST_QUEUE = auto()\n\n    @classmethod\n    def from_str(cls, name):\n        if name == \"lottery\":\n            return cls.LOTTERY\n        elif name == \"shortest_queue\":\n            return cls.SHORTEST_QUEUE\n        else:\n            raise ValueError(f\"Invalid dispatch method\")\n\n\n@dataclasses.dataclass\nclass WorkerInfo:\n    model_names: List[str]\n    speed: int\n    queue_length: int\n    check_heart_beat: bool\n    last_heart_beat: str\n\n\ndef heart_beat_controller(controller):\n    while True:\n        time.sleep(CONTROLLER_HEART_BEAT_EXPIRATION)\n        controller.remove_stale_workers_by_expiration()\n\n\nclass Controller:\n    def __init__(self, dispatch_method: str):\n        # Dict[str -> WorkerInfo]\n        self.worker_info = {}\n        self.dispatch_method = DispatchMethod.from_str(dispatch_method)\n\n        self.heart_beat_thread = threading.Thread(\n            target=heart_beat_controller, args=(self,)\n        )\n        self.heart_beat_thread.start()\n\n    def register_worker(\n        self, worker_name: str, check_heart_beat: bool, worker_status: dict\n    ):\n        if worker_name not in self.worker_info:\n            logger.info(f\"Register a new worker: {worker_name}\")\n        else:\n            logger.info(f\"Register an existing worker: {worker_name}\")\n\n        if not worker_status:\n            worker_status = self.get_worker_status(worker_name)\n        if not worker_status:\n            return False\n\n        self.worker_info[worker_name] = WorkerInfo(\n            worker_status[\"model_names\"],\n            worker_status[\"speed\"],\n            worker_status[\"queue_length\"],\n            check_heart_beat,\n            time.time(),\n        )\n\n        logger.info(f\"Register done: {worker_name}, {worker_status}\")\n        return True\n\n    def get_worker_status(self, worker_name: str):\n        try:\n            r = requests.post(worker_name + \"/worker_get_status\", timeout=5)\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Get status fails: {worker_name}, {e}\")\n            return None\n\n        if r.status_code != 200:\n            logger.error(f\"Get status fails: {worker_name}, {r}\")\n            return None\n\n        return r.json()\n\n    def remove_worker(self, worker_name: str):\n        del self.worker_info[worker_name]\n\n    def refresh_all_workers(self):\n        old_info = dict(self.worker_info)\n        self.worker_info = {}\n\n        for w_name, w_info in old_info.items():\n            if not self.register_worker(w_name, w_info.check_heart_beat, None):\n                logger.info(f\"Remove stale worker: {w_name}\")\n\n    def list_models(self):\n        model_names = set()\n\n        for w_name, w_info in self.worker_info.items():\n            model_names.update(w_info.model_names)\n\n        return list(model_names)\n\n    def get_worker_address(self, model_name: str):\n        if self.dispatch_method == DispatchMethod.LOTTERY:\n            worker_names = []\n            worker_speeds = []\n            for w_name, w_info in self.worker_info.items():\n                if model_name in w_info.model_names:\n                    worker_names.append(w_name)\n                    worker_speeds.append(w_info.speed)\n            worker_speeds = np.array(worker_speeds, dtype=np.float32)\n            norm = np.sum(worker_speeds)\n            if norm < 1e-4:\n                return \"\"\n            worker_speeds = worker_speeds / norm\n            if True:  # Directly return address\n                pt = np.random.choice(np.arange(len(worker_names)), p=worker_speeds)\n                worker_name = worker_names[pt]\n                return worker_name\n\n            # Check status before returning\n            while True:\n                pt = np.random.choice(np.arange(len(worker_names)), p=worker_speeds)\n                worker_name = worker_names[pt]\n\n                if self.get_worker_status(worker_name):\n                    break\n                else:\n                    self.remove_worker(worker_name)\n                    worker_speeds[pt] = 0\n                    norm = np.sum(worker_speeds)\n                    if norm < 1e-4:\n                        return \"\"\n                    worker_speeds = worker_speeds / norm\n                    continue\n            return worker_name\n        elif self.dispatch_method == DispatchMethod.SHORTEST_QUEUE:\n            worker_names = []\n            worker_qlen = []\n            for w_name, w_info in self.worker_info.items():\n                if model_name in w_info.model_names:\n                    worker_names.append(w_name)\n                    worker_qlen.append(w_info.queue_length / w_info.speed)\n            if len(worker_names) == 0:\n                return \"\"\n            min_index = np.argmin(worker_qlen)\n            w_name = worker_names[min_index]\n            self.worker_info[w_name].queue_length += 1\n            logger.info(\n                f\"names: {worker_names}, queue_lens: {worker_qlen}, ret: {w_name}\"\n            )\n            return w_name\n        else:\n            raise ValueError(f\"Invalid dispatch method: {self.dispatch_method}\")\n\n    def receive_heart_beat(self, worker_name: str, queue_length: int):\n        if worker_name not in self.worker_info:\n            logger.info(f\"Receive unknown heart beat. {worker_name}\")\n            return False\n\n        self.worker_info[worker_name].queue_length = queue_length\n        self.worker_info[worker_name].last_heart_beat = time.time()\n        logger.info(f\"Receive heart beat. {worker_name}\")\n        return True\n\n    def remove_stale_workers_by_expiration(self):\n        expire = time.time() - CONTROLLER_HEART_BEAT_EXPIRATION\n        to_delete = []\n        for worker_name, w_info in self.worker_info.items():\n            if w_info.check_heart_beat and w_info.last_heart_beat < expire:\n                to_delete.append(worker_name)\n\n        for worker_name in to_delete:\n            self.remove_worker(worker_name)\n\n    def handle_no_worker(self, params):\n        logger.info(f\"no worker: {params['model']}\")\n        ret = {\n            \"text\": SERVER_ERROR_MSG,\n            \"error_code\": ErrorCode.CONTROLLER_NO_WORKER,\n        }\n        return json.dumps(ret).encode() + b\"\\0\"\n\n    def handle_worker_timeout(self, worker_address):\n        logger.info(f\"worker timeout: {worker_address}\")\n        ret = {\n            \"text\": SERVER_ERROR_MSG,\n            \"error_code\": ErrorCode.CONTROLLER_WORKER_TIMEOUT,\n        }\n        return json.dumps(ret).encode() + b\"\\0\"\n\n    # Let the controller act as a worker to achieve hierarchical\n    # management. This can be used to connect isolated sub networks.\n    def worker_api_get_status(self):\n        model_names = set()\n        speed = 0\n        queue_length = 0\n\n        for w_name in self.worker_info:\n            worker_status = self.get_worker_status(w_name)\n            if worker_status is not None:\n                model_names.update(worker_status[\"model_names\"])\n                speed += worker_status[\"speed\"]\n                queue_length += worker_status[\"queue_length\"]\n\n        model_names = sorted(list(model_names))\n        return {\n            \"model_names\": model_names,\n            \"speed\": speed,\n            \"queue_length\": queue_length,\n        }\n\n    def worker_api_generate_stream(self, params):\n        worker_addr = self.get_worker_address(params[\"model\"])\n        if not worker_addr:\n            yield self.handle_no_worker(params)\n\n        try:\n            response = requests.post(\n                worker_addr + \"/worker_generate_stream\",\n                json=params,\n                stream=True,\n                timeout=WORKER_API_TIMEOUT,\n            )\n            for chunk in response.iter_lines(decode_unicode=False, delimiter=b\"\\0\"):\n                if chunk:\n                    yield chunk + b\"\\0\"\n        except requests.exceptions.RequestException as e:\n            yield self.handle_worker_timeout(worker_addr)\n\n\napp = FastAPI()\n\n\n@app.post(\"/register_worker\")\nasync def register_worker(request: Request):\n    data = await request.json()\n    controller.register_worker(\n        data[\"worker_name\"], data[\"check_heart_beat\"], data.get(\"worker_status\", None)\n    )\n\n\n@app.post(\"/refresh_all_workers\")\nasync def refresh_all_workers():\n    models = controller.refresh_all_workers()\n\n\n@app.post(\"/list_models\")\nasync def list_models():\n    models = controller.list_models()\n    return {\"models\": models}\n\n\n@app.post(\"/get_worker_address\")\nasync def get_worker_address(request: Request):\n    data = await request.json()\n    addr = controller.get_worker_address(data[\"model\"])\n    return {\"address\": addr}\n\n\n@app.post(\"/receive_heart_beat\")\nasync def receive_heart_beat(request: Request):\n    data = await request.json()\n    exist = controller.receive_heart_beat(data[\"worker_name\"], data[\"queue_length\"])\n    return {\"exist\": exist}\n\n\n@app.post(\"/worker_generate_stream\")\nasync def worker_api_generate_stream(request: Request):\n    params = await request.json()\n    generator = controller.worker_api_generate_stream(params)\n    return StreamingResponse(generator)\n\n\n@app.post(\"/worker_get_status\")\nasync def worker_api_get_status(request: Request):\n    return controller.worker_api_get_status()\n\n\n@app.get(\"/test_connection\")\nasync def worker_api_get_status(request: Request):\n    return \"success\"\n\n\ndef create_controller():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--host\", type=str, default=\"localhost\")\n    parser.add_argument(\"--port\", type=int, default=21001)\n    parser.add_argument(\n        \"--dispatch-method\",\n        type=str,\n        choices=[\"lottery\", \"shortest_queue\"],\n        default=\"shortest_queue\",\n    )\n    parser.add_argument(\n        \"--ssl\",\n        action=\"store_true\",\n        required=False,\n        default=False,\n        help=\"Enable SSL. Requires OS Environment variables 'SSL_KEYFILE' and 'SSL_CERTFILE'.\",\n    )\n    args = parser.parse_args()\n    logger.info(f\"args: {args}\")\n\n    controller = Controller(args.dispatch_method)\n    return args, controller\n\n\nif __name__ == \"__main__\":\n    args, controller = create_controller()\n    if args.ssl:\n        uvicorn.run(\n            app,\n            host=args.host,\n            port=args.port,\n            log_level=\"info\",\n            ssl_keyfile=os.environ[\"SSL_KEYFILE\"],\n            ssl_certfile=os.environ[\"SSL_CERTFILE\"],\n        )\n    else:\n        uvicorn.run(app, host=args.host, port=args.port, log_level=\"debug\")\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/serve/base_model_worker.py", "content": "import asyncio\nimport threading\nimport time\nfrom typing import List\n\nfrom fastapi import FastAPI, Request, BackgroundTasks\nfrom fastapi.responses import StreamingResponse, JSONResponse\nimport requests\n\nfrom fastchat.constants import WORKER_HEART_BEAT_INTERVAL\nfrom fastchat.conversation import Conversation\nfrom fastchat.utils import pretty_print_semaphore, build_logger\n\n\nworker = None\nlogger = None\n\napp = FastAPI()\n\n\ndef heart_beat_worker(obj):\n    while True:\n        time.sleep(WORKER_HEART_BEAT_INTERVAL)\n        obj.send_heart_beat()\n\n\nclass BaseModelWorker:\n    def __init__(\n        self,\n        controller_addr: str,\n        worker_addr: str,\n        worker_id: str,\n        model_path: str,\n        model_names: List[str],\n        limit_worker_concurrency: int,\n        conv_template: str = None,\n    ):\n        global logger, worker\n\n        self.controller_addr = controller_addr\n        self.worker_addr = worker_addr\n        self.worker_id = worker_id\n        if model_path.endswith(\"/\"):\n            model_path = model_path[:-1]\n        self.model_names = model_names or [model_path.split(\"/\")[-1]]\n        self.limit_worker_concurrency = limit_worker_concurrency\n        self.conv = self.make_conv_template(conv_template, model_path)\n        self.conv.sep_style = int(self.conv.sep_style)\n        self.tokenizer = None\n        self.context_len = None\n        self.call_ct = 0\n        self.semaphore = None\n\n        self.heart_beat_thread = None\n\n        if logger is None:\n            logger = build_logger(\"model_worker\", f\"model_worker_{self.worker_id}.log\")\n        if worker is None:\n            worker = self\n\n    def make_conv_template(\n        self,\n        conv_template: str = None,\n        model_path: str = None,\n    ) -> Conversation:\n        \"\"\"\n        can be overrided to costomize the conversation template for different model workers.\n        \"\"\"\n        from fastchat.conversation import get_conv_template\n        from fastchat.model.model_adapter import get_conversation_template\n\n        if conv_template:\n            conv = get_conv_template(conv_template)\n        else:\n            conv = get_conversation_template(model_path)\n        return conv\n\n    def init_heart_beat(self):\n        self.register_to_controller()\n        self.heart_beat_thread = threading.Thread(\n            target=heart_beat_worker,\n            args=(self,),\n            daemon=True,\n        )\n        self.heart_beat_thread.start()\n\n    def register_to_controller(self):\n        logger.info(\"Register to controller\")\n\n        url = self.controller_addr + \"/register_worker\"\n        data = {\n            \"worker_name\": self.worker_addr,\n            \"check_heart_beat\": True,\n            \"worker_status\": self.get_status(),\n        }\n        r = requests.post(url, json=data)\n        assert r.status_code == 200\n\n    def send_heart_beat(self):\n        logger.info(\n            f\"Send heart beat. Models: {self.model_names}. \"\n            f\"Semaphore: {pretty_print_semaphore(self.semaphore)}. \"\n            f\"call_ct: {self.call_ct}. \"\n            f\"worker_id: {self.worker_id}. \"\n        )\n\n        url = self.controller_addr + \"/receive_heart_beat\"\n\n        while True:\n            try:\n                ret = requests.post(\n                    url,\n                    json={\n                        \"worker_name\": self.worker_addr,\n                        \"queue_length\": self.get_queue_length(),\n                    },\n                    timeout=5,\n                )\n                exist = ret.json()[\"exist\"]\n                break\n            except (requests.exceptions.RequestException, KeyError) as e:\n                logger.error(f\"heart beat error: {e}\")\n            time.sleep(5)\n\n        if not exist:\n            self.register_to_controller()\n\n    def get_queue_length(self):\n        if (\n            self.semaphore is None\n            or self.semaphore._value is None\n            or self.semaphore._waiters is None\n        ):\n            return 0\n        else:\n            return (\n                self.limit_worker_concurrency\n                - self.semaphore._value\n                + len(self.semaphore._waiters)\n            )\n\n    def get_status(self):\n        return {\n            \"model_names\": self.model_names,\n            \"speed\": 1,\n            \"queue_length\": self.get_queue_length(),\n        }\n\n    def count_token(self, params):\n        prompt = params[\"prompt\"]\n\n        try:\n            input_ids = self.tokenizer(prompt).input_ids\n            input_echo_len = len(input_ids)\n        except TypeError:\n            input_echo_len = self.tokenizer.num_tokens(prompt)\n\n        ret = {\n            \"count\": input_echo_len,\n            \"error_code\": 0,\n        }\n        return ret\n\n    def get_conv_template(self):\n        return {\"conv\": self.conv}\n\n    def generate_stream_gate(self, params):\n        raise NotImplementedError\n\n    def generate_gate(self, params):\n        raise NotImplementedError\n\n    def get_embeddings(self, params):\n        raise NotImplementedError\n\n\ndef release_worker_semaphore():\n    worker.semaphore.release()\n\n\ndef acquire_worker_semaphore():\n    if worker.semaphore is None:\n        worker.semaphore = asyncio.Semaphore(worker.limit_worker_concurrency)\n    return worker.semaphore.acquire()\n\n\ndef create_background_tasks():\n    background_tasks = BackgroundTasks()\n    background_tasks.add_task(release_worker_semaphore)\n    return background_tasks\n\n\n@app.post(\"/worker_generate_stream\")\nasync def api_generate_stream(request: Request):\n    params = await request.json()\n    await acquire_worker_semaphore()\n    generator = worker.generate_stream_gate(params)\n    background_tasks = create_background_tasks()\n    return StreamingResponse(generator, background=background_tasks)\n\n\n@app.post(\"/worker_generate\")\nasync def api_generate(request: Request):\n    params = await request.json()\n    await acquire_worker_semaphore()\n    output = await asyncio.to_thread(worker.generate_gate, params)\n    release_worker_semaphore()\n    return JSONResponse(output)\n\n\n@app.post(\"/worker_get_embeddings\")\nasync def api_get_embeddings(request: Request):\n    params = await request.json()\n    await acquire_worker_semaphore()\n    embedding = worker.get_embeddings(params)\n    release_worker_semaphore()\n    return JSONResponse(content=embedding)\n\n\n@app.post(\"/worker_get_status\")\nasync def api_get_status(request: Request):\n    return worker.get_status()\n\n\n@app.post(\"/count_token\")\nasync def api_count_token(request: Request):\n    params = await request.json()\n    return worker.count_token(params)\n\n\n@app.post(\"/worker_get_conv_template\")\nasync def api_get_conv(request: Request):\n    return worker.get_conv_template()\n\n\n@app.post(\"/model_details\")\nasync def api_model_details(request: Request):\n    return {\"context_length\": worker.context_len}\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/modules/exllama.py", "content": "from dataclasses import dataclass, field\nimport sys\n\n\n@dataclass\nclass ExllamaConfig:\n    max_seq_len: int\n    gpu_split: str = None\n\n\nclass ExllamaModel:\n    def __init__(self, exllama_model, exllama_cache):\n        self.model = exllama_model\n        self.cache = exllama_cache\n        self.config = self.model.config\n\n\ndef load_exllama_model(model_path, exllama_config: ExllamaConfig):\n    try:\n        from exllamav2 import (\n            ExLlamaV2Config,\n            ExLlamaV2Tokenizer,\n            ExLlamaV2,\n            ExLlamaV2Cache,\n        )\n    except ImportError as e:\n        print(f\"Error: Failed to load Exllamav2. {e}\")\n        sys.exit(-1)\n\n    exllamav2_config = ExLlamaV2Config()\n    exllamav2_config.model_dir = model_path\n    exllamav2_config.prepare()\n    exllamav2_config.max_seq_len = exllama_config.max_seq_len\n\n    exllama_model = ExLlamaV2(exllamav2_config)\n    tokenizer = ExLlamaV2Tokenizer(exllamav2_config)\n\n    split = None\n    if exllama_config.gpu_split:\n        split = [float(alloc) for alloc in exllama_config.gpu_split.split(\",\")]\n    exllama_model.load(split)\n\n    exllama_cache = ExLlamaV2Cache(exllama_model)\n    model = ExllamaModel(exllama_model=exllama_model, exllama_cache=exllama_cache)\n\n    return model, tokenizer\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/serve/api_provider.py", "content": "\"\"\"Call API providers.\"\"\"\n\nimport os\nimport random\nimport time\n\nfrom fastchat.utils import build_logger\nfrom fastchat.constants import WORKER_API_TIMEOUT\n\n\nlogger = build_logger(\"gradio_web_server\", \"gradio_web_server.log\")\n\n\ndef openai_api_stream_iter(\n    model_name,\n    messages,\n    temperature,\n    top_p,\n    max_new_tokens,\n    api_base=None,\n    api_key=None,\n):\n    import openai\n\n    openai.api_base = api_base or \"https://api.openai.com/v1\"\n    openai.api_key = api_key or os.environ[\"OPENAI_API_KEY\"]\n    if model_name == \"gpt-4-turbo\":\n        model_name = \"gpt-4-1106-preview\"\n\n    # Make requests\n    gen_params = {\n        \"model\": model_name,\n        \"prompt\": messages,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"max_new_tokens\": max_new_tokens,\n    }\n    logger.info(f\"==== request ====\\n{gen_params}\")\n\n    res = openai.ChatCompletion.create(\n        model=model_name,\n        messages=messages,\n        temperature=temperature,\n        max_tokens=max_new_tokens,\n        stream=True,\n    )\n    text = \"\"\n    for chunk in res:\n        text += chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n        data = {\n            \"text\": text,\n            \"error_code\": 0,\n        }\n        yield data\n\n\ndef anthropic_api_stream_iter(model_name, prompt, temperature, top_p, max_new_tokens):\n    import anthropic\n\n    c = anthropic.Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n\n    # Make requests\n    gen_params = {\n        \"model\": model_name,\n        \"prompt\": prompt,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"max_new_tokens\": max_new_tokens,\n    }\n    logger.info(f\"==== request ====\\n{gen_params}\")\n\n    res = c.completions.create(\n        prompt=prompt,\n        stop_sequences=[anthropic.HUMAN_PROMPT],\n        max_tokens_to_sample=max_new_tokens,\n        temperature=temperature,\n        top_p=top_p,\n        model=model_name,\n        stream=True,\n    )\n    text = \"\"\n    for chunk in res:\n        text += chunk.completion\n        data = {\n            \"text\": text,\n            \"error_code\": 0,\n        }\n        yield data\n\n\ndef init_palm_chat(model_name):\n    import vertexai  # pip3 install google-cloud-aiplatform\n    from vertexai.preview.language_models import ChatModel\n\n    project_id = os.environ[\"GCP_PROJECT_ID\"]\n    location = \"us-central1\"\n    vertexai.init(project=project_id, location=location)\n\n    chat_model = ChatModel.from_pretrained(model_name)\n    chat = chat_model.start_chat(examples=[])\n    return chat\n\n\ndef palm_api_stream_iter(chat, message, temperature, top_p, max_new_tokens):\n    parameters = {\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"max_output_tokens\": max_new_tokens,\n    }\n    gen_params = {\n        \"model\": \"palm-2\",\n        \"prompt\": message,\n    }\n    gen_params.update(parameters)\n    logger.info(f\"==== request ====\\n{gen_params}\")\n\n    response = chat.send_message(message, **parameters)\n    content = response.text\n\n    pos = 0\n    while pos < len(content):\n        # This is a fancy way to simulate token generation latency combined\n        # with a Poisson process.\n        pos += random.randint(10, 20)\n        time.sleep(random.expovariate(50))\n        data = {\n            \"text\": content[:pos],\n            \"error_code\": 0,\n        }\n        yield data\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/modules/gptq.py", "content": "from dataclasses import dataclass, field\nimport os\nfrom os.path import isdir, isfile\nfrom pathlib import Path\nimport sys\n\nfrom transformers import AutoTokenizer\n\n\n@dataclass\nclass GptqConfig:\n    ckpt: str = field(\n        default=None,\n        metadata={\n            \"help\": \"Load quantized model. The path to the local GPTQ checkpoint.\"\n        },\n    )\n    wbits: int = field(default=16, metadata={\"help\": \"#bits to use for quantization\"})\n    groupsize: int = field(\n        default=-1,\n        metadata={\"help\": \"Groupsize to use for quantization; default uses full row.\"},\n    )\n    act_order: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to apply the activation order GPTQ heuristic\"},\n    )\n\n\ndef load_gptq_quantized(model_name, gptq_config: GptqConfig):\n    print(\"Loading GPTQ quantized model...\")\n\n    try:\n        script_path = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))\n        module_path = os.path.join(script_path, \"../repositories/GPTQ-for-LLaMa\")\n\n        sys.path.insert(0, module_path)\n        from llama import load_quant\n    except ImportError as e:\n        print(f\"Error: Failed to load GPTQ-for-LLaMa. {e}\")\n        print(\"See https://github.com/lm-sys/FastChat/blob/main/docs/gptq.md\")\n        sys.exit(-1)\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n    # only `fastest-inference-4bit` branch cares about `act_order`\n    if gptq_config.act_order:\n        model = load_quant(\n            model_name,\n            find_gptq_ckpt(gptq_config),\n            gptq_config.wbits,\n            gptq_config.groupsize,\n            act_order=gptq_config.act_order,\n        )\n    else:\n        # other branches\n        model = load_quant(\n            model_name,\n            find_gptq_ckpt(gptq_config),\n            gptq_config.wbits,\n            gptq_config.groupsize,\n        )\n\n    return model, tokenizer\n\n\ndef find_gptq_ckpt(gptq_config: GptqConfig):\n    if Path(gptq_config.ckpt).is_file():\n        return gptq_config.ckpt\n\n    for ext in [\"*.pt\", \"*.safetensors\"]:\n        matched_result = sorted(Path(gptq_config.ckpt).glob(ext))\n        if len(matched_result) > 0:\n            return str(matched_result[-1])\n\n    print(\"Error: gptq checkpoint not found\")\n    sys.exit(1)\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/llm_judge/compute_agreement.py", "content": "\"\"\"\nCompute agreement among judges.\n\nUsage:\npython compute_agreement.py --judges gpt4-pair human --votefiles human_judgments.json gpt4_pair_judgments.json\npython compute_agreement.py --judges human human --votefiles human_judgments.json\n\"\"\"\nimport argparse\nimport json\nimport os\n\nimport numpy as np\n\n\ndef get_judge_name(judge):\n    if isinstance(judge, list) and judge[0] == \"gpt-4\" and judge[1].startswith(\"pair\"):\n        return \"gpt4-pair\"\n    if judge.startswith(\"expert\"):\n        return \"human\"\n    if judge.startswith(\"author\"):\n        return \"author\"\n\n\ndef revert(vote):\n    if vote == \"model_a\":\n        return \"model_b\"\n    elif vote == \"model_b\":\n        return \"model_a\"\n    return vote\n\n\ndef get_mt_bench_votes_data(raw_votes):\n    data = [{}, {}]\n\n    for judge_votes in raw_votes:\n        for vote in judge_votes:\n            turn = vote[\"turn\"] - 1\n            if vote[\"model_a\"] < vote[\"model_b\"]:\n                key = (vote[\"question_id\"], vote[\"model_a\"], vote[\"model_b\"])\n                winner = vote[\"winner\"]\n            else:\n                key = (vote[\"question_id\"], vote[\"model_b\"], vote[\"model_a\"])\n                winner = revert(vote[\"winner\"])\n            judge = get_judge_name(vote[\"judge\"])\n            if key not in data[turn]:\n                data[turn][key] = {}\n            if judge not in data[turn][key]:\n                data[turn][key][judge] = []\n            data[turn][key][judge].append(winner)\n\n    return data\n\n\ndef convertvote(vote):\n    if \"tie\" in vote:\n        return \"tie\"\n    return vote\n\n\ndef equalvote(vote1, vote2):\n    if \"tie\" in vote1 and \"tie\" in vote2:\n        return True\n    return vote1 == vote2\n\n\n# data: Dict[qid -> List[vote]]\ndef get_mt_bench_agreement(data, judge1, judge2, ban):\n    if judge1.startswith(\"gpt4\") and judge2 == \"human\":\n        stats = [0, 0]\n        for votes in data.values():\n            if judge1 not in votes or judge2 not in votes:\n                continue\n            assert len(votes[judge1]) == 1\n            if convertvote(votes[judge1][0]) in ban:\n                continue\n            for v in votes[judge2]:\n                if convertvote(v) in ban:\n                    continue\n                stats[1] += 1\n                stats[0] += equalvote(votes[judge1][0], v)\n        return stats[0], stats[1]\n    elif judge1 == \"human\" and judge2 == \"human\":\n        stats = [0, 0]\n        for votes in data.values():\n            if \"human\" not in votes:\n                continue\n            for i in range(len(votes[\"human\"]) - 1):\n                for j in range(i + 1, len(votes[\"human\"])):\n                    if (\n                        convertvote(votes[\"human\"][i]) in ban\n                        or convertvote(votes[\"human\"][j]) in ban\n                    ):\n                        continue\n                    stats[1] += 1\n                    stats[0] += equalvote(votes[\"human\"][i], votes[\"human\"][j])\n        return stats[0], stats[1]\n    else:\n        raise Exception(\"Unsupported judges.\")\n\n\ndef run_mt_bench_agreement(judges, votefiles):\n    # votes[i]: List of votes\n    votes = []\n    for filename in votefiles:\n        with open(filename, \"r\") as f:\n            data = json.load(f)\n        votes.append(data)\n\n    data = get_mt_bench_votes_data(votes)\n\n    agree, total = get_mt_bench_agreement(data[0], judges[0], judges[1], ban=[])\n    print(\n        f\"turn 1 with tie. #total: {total}, #agree: {agree}, ratio: {agree/total:.2f}\"\n    )\n    agree, total = get_mt_bench_agreement(data[0], judges[0], judges[1], ban=[\"tie\"])\n    print(\n        f\"turn 1 without tie. #total: {total}, #agree: {agree}, ratio: {agree/total:.2f}\"\n    )\n    agree, total = get_mt_bench_agreement(data[1], judges[0], judges[1], ban=[])\n    print(\n        f\"turn 2 with tie. #total: {total}, #agree: {agree}, ratio: {agree/total:.2f}\"\n    )\n    agree, total = get_mt_bench_agreement(data[1], judges[0], judges[1], ban=[\"tie\"])\n    print(\n        f\"turn 2 without tie. #total: {total}, #agree: {agree}, ratio: {agree/total:.2f}\"\n    )\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--judges\", nargs=2, type=str, default=[\"gpt4-pair\", \"human\"])\n    parser.add_argument(\n        \"--votefiles\",\n        nargs=\"+\",\n        type=str,\n        default=[\"gpt4_judgments.json\", \"human_judgments.json\"],\n    )\n    args = parser.parse_args()\n\n    run_mt_bench_agreement(args.judges, args.votefiles)\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/modules/awq.py", "content": "from dataclasses import dataclass, field\nfrom pathlib import Path\nimport sys\n\nimport torch\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, modeling_utils\n\n\n@dataclass\nclass AWQConfig:\n    ckpt: str = field(\n        default=None,\n        metadata={\n            \"help\": \"Load quantized model. The path to the local AWQ checkpoint.\"\n        },\n    )\n    wbits: int = field(default=16, metadata={\"help\": \"#bits to use for quantization\"})\n    groupsize: int = field(\n        default=-1,\n        metadata={\"help\": \"Groupsize to use for quantization; default uses full row.\"},\n    )\n\n\ndef load_awq_quantized(model_name, awq_config: AWQConfig, device):\n    print(\"Loading AWQ quantized model...\")\n\n    try:\n        from tinychat.utils import load_quant\n        from tinychat.modules import make_quant_norm, make_quant_attn, make_fused_mlp\n    except ImportError as e:\n        print(f\"Error: Failed to import tinychat. {e}\")\n        print(\"Please double check if you have successfully installed AWQ\")\n        print(\"See https://github.com/lm-sys/FastChat/blob/main/docs/awq.md\")\n        sys.exit(-1)\n\n    config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_name, use_fast=False, trust_remote_code=True\n    )\n\n    def skip(*args, **kwargs):\n        pass\n\n    torch.nn.init.kaiming_uniform_ = skip\n    torch.nn.init.kaiming_normal_ = skip\n    torch.nn.init.uniform_ = skip\n    torch.nn.init.normal_ = skip\n    modeling_utils._init_weights = False\n\n    torch.set_default_dtype(torch.half)\n    model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)\n\n    if any(name in find_awq_ckpt(awq_config) for name in [\"llama\", \"vicuna\"]):\n        model = load_quant.load_awq_llama_fast(\n            model,\n            find_awq_ckpt(awq_config),\n            awq_config.wbits,\n            awq_config.groupsize,\n            device,\n        )\n        make_quant_attn(model, device)\n        make_quant_norm(model)\n        make_fused_mlp(model)\n    else:\n        model = load_quant.load_awq_model(\n            model,\n            find_awq_ckpt(awq_config),\n            awq_config.wbits,\n            awq_config.groupsize,\n            device,\n        )\n    return model, tokenizer\n\n\ndef find_awq_ckpt(awq_config: AWQConfig):\n    if Path(awq_config.ckpt).is_file():\n        return awq_config.ckpt\n\n    for ext in [\"*.pt\", \"*.safetensors\"]:\n        matched_result = sorted(Path(awq_config.ckpt).glob(ext))\n        if len(matched_result) > 0:\n            return str(matched_result[-1])\n\n    print(\"Error: AWQ checkpoint not found\")\n    sys.exit(1)\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/modules/__init__.py", "content": ""}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/model/monkey_patch_non_inplace.py", "content": "\"\"\"\nMonkey patch the llama implementation in the huggingface/transformers library.\nAvoid bugs in mps backend by not using in-place operations.\n\"\"\"\nimport math\nfrom typing import List, Optional, Tuple\n\nimport torch\nfrom torch import nn\nimport transformers\n\n\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2].clone()\n    x2 = x[..., x.shape[-1] // 2 :].clone()\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n    gather_indices = position_ids[:, None, :, None]  # [bs, 1, seq_len, 1]\n    gather_indices = gather_indices.repeat(1, cos.shape[1], 1, cos.shape[3])\n    cos = torch.gather(cos.repeat(gather_indices.shape[0], 1, 1, 1), 2, gather_indices)\n    sin = torch.gather(sin.repeat(gather_indices.shape[0], 1, 1, 1), 2, gather_indices)\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\n\ndef forward(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n    padding_mask: Optional[torch.LongTensor] = None,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n    bsz, q_len, _ = hidden_states.size()\n\n    query_states = (\n        self.q_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n    key_states = (\n        self.k_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n    value_states = (\n        self.v_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n\n    kv_seq_len = key_states.shape[-2]\n    if past_key_value is not None:\n        kv_seq_len += past_key_value[0].shape[-2]\n    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n    query_states, key_states = apply_rotary_pos_emb(\n        query_states, key_states, cos, sin, position_ids\n    )\n    # [bsz, nh, t, hd]\n\n    if past_key_value is not None:\n        # reuse k, v, self_attention\n        key_states = torch.cat([past_key_value[0], key_states], dim=2)\n        value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\n    past_key_value = (key_states, value_states) if use_cache else None\n\n    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(\n        self.head_dim\n    )\n\n    if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n        raise ValueError(\n            f\"Attention weights should be of size {(bsz * self.num_heads, q_len, kv_seq_len)}, but is\"\n            f\" {attn_weights.size()}\"\n        )\n\n    if attention_mask is not None:\n        if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n            )\n        attn_weights = attn_weights + attention_mask\n        attn_weights = torch.max(\n            attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min)\n        )\n\n    # upcast attention to fp32\n    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(\n        query_states.dtype\n    )\n    attn_output = torch.matmul(attn_weights, value_states)\n\n    if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n        raise ValueError(\n            f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n            f\" {attn_output.size()}\"\n        )\n\n    attn_output = attn_output.transpose(1, 2)\n    attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n    attn_output = self.o_proj(attn_output)\n\n    if not output_attentions:\n        attn_weights = None\n\n    return attn_output, attn_weights, past_key_value\n\n\ndef replace_llama_attn_with_non_inplace_operations():\n    \"\"\"Avoid bugs in mps backend by not using in-place operations.\"\"\"\n    transformers.models.llama.modeling_llama.LlamaAttention.forward = forward\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/serve/gradio_block_arena_named.py", "content": "\"\"\"\nChatbot Arena (side-by-side) tab.\nUsers chat with two chosen models.\n\"\"\"\n\nimport json\nimport time\n\nimport gradio as gr\nimport numpy as np\n\nfrom fastchat.constants import (\n    MODERATION_MSG,\n    CONVERSATION_LIMIT_MSG,\n    INPUT_CHAR_LEN_LIMIT,\n    CONVERSATION_TURN_LIMIT,\n)\nfrom fastchat.model.model_adapter import get_conversation_template\nfrom fastchat.serve.gradio_web_server import (\n    State,\n    bot_response,\n    get_conv_log_filename,\n    no_change_btn,\n    enable_btn,\n    disable_btn,\n    invisible_btn,\n    acknowledgment_md,\n    get_model_description_md,\n    ip_expiration_dict,\n    get_ip,\n)\nfrom fastchat.utils import (\n    build_logger,\n    moderation_filter,\n)\n\n\nlogger = build_logger(\"gradio_web_server_multi\", \"gradio_web_server_multi.log\")\n\nnum_sides = 2\nenable_moderation = False\n\n\ndef set_global_vars_named(enable_moderation_):\n    global enable_moderation\n    enable_moderation = enable_moderation_\n\n\ndef load_demo_side_by_side_named(models, url_params):\n    states = (None,) * num_sides\n\n    model_left = models[0] if len(models) > 0 else \"\"\n    if len(models) > 1:\n        weights = ([8] * 4 + [4] * 8 + [1] * 32)[: len(models) - 1]\n        weights = weights / np.sum(weights)\n        model_right = np.random.choice(models[1:], p=weights)\n    else:\n        model_right = model_left\n\n    selector_updates = (\n        gr.Dropdown.update(choices=models, value=model_left, visible=True),\n        gr.Dropdown.update(choices=models, value=model_right, visible=True),\n    )\n\n    return states + selector_updates\n\n\ndef vote_last_response(states, vote_type, model_selectors, request: gr.Request):\n    with open(get_conv_log_filename(), \"a\") as fout:\n        data = {\n            \"tstamp\": round(time.time(), 4),\n            \"type\": vote_type,\n            \"models\": [x for x in model_selectors],\n            \"states\": [x.dict() for x in states],\n            \"ip\": get_ip(request),\n        }\n        fout.write(json.dumps(data) + \"\\n\")\n\n\ndef leftvote_last_response(\n    state0, state1, model_selector0, model_selector1, request: gr.Request\n):\n    logger.info(f\"leftvote (named). ip: {get_ip(request)}\")\n    vote_last_response(\n        [state0, state1], \"leftvote\", [model_selector0, model_selector1], request\n    )\n    return (\"\",) + (disable_btn,) * 4\n\n\ndef rightvote_last_response(\n    state0, state1, model_selector0, model_selector1, request: gr.Request\n):\n    logger.info(f\"rightvote (named). ip: {get_ip(request)}\")\n    vote_last_response(\n        [state0, state1], \"rightvote\", [model_selector0, model_selector1], request\n    )\n    return (\"\",) + (disable_btn,) * 4\n\n\ndef tievote_last_response(\n    state0, state1, model_selector0, model_selector1, request: gr.Request\n):\n    logger.info(f\"tievote (named). ip: {get_ip(request)}\")\n    vote_last_response(\n        [state0, state1], \"tievote\", [model_selector0, model_selector1], request\n    )\n    return (\"\",) + (disable_btn,) * 4\n\n\ndef bothbad_vote_last_response(\n    state0, state1, model_selector0, model_selector1, request: gr.Request\n):\n    logger.info(f\"bothbad_vote (named). ip: {get_ip(request)}\")\n    vote_last_response(\n        [state0, state1], \"bothbad_vote\", [model_selector0, model_selector1], request\n    )\n    return (\"\",) + (disable_btn,) * 4\n\n\ndef regenerate(state0, state1, request: gr.Request):\n    logger.info(f\"regenerate (named). ip: {get_ip(request)}\")\n    states = [state0, state1]\n    for i in range(num_sides):\n        states[i].conv.update_last_message(None)\n    return states + [x.to_gradio_chatbot() for x in states] + [\"\"] + [disable_btn] * 6\n\n\ndef clear_history(request: gr.Request):\n    logger.info(f\"clear_history (named). ip: {get_ip(request)}\")\n    return (\n        [None] * num_sides\n        + [None] * num_sides\n        + [\"\"]\n        + [invisible_btn] * 4\n        + [disable_btn] * 2\n    )\n\n\ndef share_click(state0, state1, model_selector0, model_selector1, request: gr.Request):\n    logger.info(f\"share (named). ip: {get_ip(request)}\")\n    if state0 is not None and state1 is not None:\n        vote_last_response(\n            [state0, state1], \"share\", [model_selector0, model_selector1], request\n        )\n\n\ndef add_text(\n    state0, state1, model_selector0, model_selector1, text, request: gr.Request\n):\n    ip = get_ip(request)\n    logger.info(f\"add_text (named). ip: {ip}. len: {len(text)}\")\n    states = [state0, state1]\n    model_selectors = [model_selector0, model_selector1]\n\n    # Init states if necessary\n    for i in range(num_sides):\n        if states[i] is None:\n            states[i] = State(model_selectors[i])\n\n    if len(text) <= 0:\n        for i in range(num_sides):\n            states[i].skip_next = True\n        return (\n            states\n            + [x.to_gradio_chatbot() for x in states]\n            + [\"\"]\n            + [\n                no_change_btn,\n            ]\n            * 6\n        )\n\n    model_list = [states[i].model_name for i in range(num_sides)]\n    flagged = moderation_filter(text, model_list)\n    if flagged:\n        logger.info(f\"violate moderation (named). ip: {ip}. text: {text}\")\n        # overwrite the original text\n        text = MODERATION_MSG\n\n    conv = states[0].conv\n    if (len(conv.messages) - conv.offset) // 2 >= CONVERSATION_TURN_LIMIT:\n        logger.info(f\"conversation turn limit. ip: {ip}. text: {text}\")\n        for i in range(num_sides):\n            states[i].skip_next = True\n        return (\n            states\n            + [x.to_gradio_chatbot() for x in states]\n            + [CONVERSATION_LIMIT_MSG]\n            + [\n                no_change_btn,\n            ]\n            * 6\n        )\n\n    text = text[:INPUT_CHAR_LEN_LIMIT]  # Hard cut-off\n    for i in range(num_sides):\n        states[i].conv.append_message(states[i].conv.roles[0], text)\n        states[i].conv.append_message(states[i].conv.roles[1], None)\n        states[i].skip_next = False\n\n    return (\n        states\n        + [x.to_gradio_chatbot() for x in states]\n        + [\"\"]\n        + [\n            disable_btn,\n        ]\n        * 6\n    )\n\n\ndef bot_response_multi(\n    state0,\n    state1,\n    temperature,\n    top_p,\n    max_new_tokens,\n    request: gr.Request,\n):\n    logger.info(f\"bot_response_multi (named). ip: {get_ip(request)}\")\n\n    if state0.skip_next:\n        # This generate call is skipped due to invalid inputs\n        yield (\n            state0,\n            state1,\n            state0.to_gradio_chatbot(),\n            state1.to_gradio_chatbot(),\n        ) + (no_change_btn,) * 6\n        return\n\n    states = [state0, state1]\n    gen = []\n    for i in range(num_sides):\n        gen.append(\n            bot_response(\n                states[i],\n                temperature,\n                top_p,\n                max_new_tokens,\n                request,\n            )\n        )\n\n    chatbots = [None] * num_sides\n    while True:\n        stop = True\n        for i in range(num_sides):\n            try:\n                ret = next(gen[i])\n                states[i], chatbots[i] = ret[0], ret[1]\n                stop = False\n            except StopIteration:\n                pass\n        yield states + chatbots + [disable_btn] * 6\n        if stop:\n            break\n\n\ndef flash_buttons():\n    btn_updates = [\n        [disable_btn] * 4 + [enable_btn] * 2,\n        [enable_btn] * 6,\n    ]\n    for i in range(4):\n        yield btn_updates[i % 2]\n        time.sleep(0.5)\n\n\ndef build_side_by_side_ui_named(models):\n    notice_markdown = \"\"\"\n# ⚔️  Chatbot Arena ⚔️ : Benchmarking LLMs in the Wild\n| [Blog](https://lmsys.org/blog/2023-05-03-arena/) | [GitHub](https://github.com/lm-sys/FastChat) | [Paper](https://arxiv.org/abs/2306.05685) | [Dataset](https://github.com/lm-sys/FastChat/blob/main/docs/dataset_release.md) | [Twitter](https://twitter.com/lmsysorg) | [Discord](https://discord.gg/HSWAKCrnFx) |\n\n## 📜 Rules\n- Chat with any two models side-by-side and vote!\n- You can continue chatting for multiple rounds.\n- Click \"Clear history\" to start a new round.\n\n## 🤖 Choose two models to compare\n\"\"\"\n\n    states = [gr.State() for _ in range(num_sides)]\n    model_selectors = [None] * num_sides\n    chatbots = [None] * num_sides\n\n    model_description_md = get_model_description_md(models)\n    notice = gr.Markdown(\n        notice_markdown + model_description_md, elem_id=\"notice_markdown\"\n    )\n\n    with gr.Box(elem_id=\"share-region-named\"):\n        with gr.Row():\n            for i in range(num_sides):\n                with gr.Column():\n                    model_selectors[i] = gr.Dropdown(\n                        choices=models,\n                        value=models[i] if len(models) > i else \"\",\n                        interactive=True,\n                        show_label=False,\n                        container=False,\n                    )\n\n        with gr.Row():\n            for i in range(num_sides):\n                label = \"Model A\" if i == 0 else \"Model B\"\n                with gr.Column():\n                    chatbots[i] = gr.Chatbot(\n                        label=label, elem_id=f\"chatbot\", height=550\n                    )\n\n        with gr.Row():\n            leftvote_btn = gr.Button(\n                value=\"👈  A is better\", visible=False, interactive=False\n            )\n            rightvote_btn = gr.Button(\n                value=\"👉  B is better\", visible=False, interactive=False\n            )\n            tie_btn = gr.Button(value=\"🤝  Tie\", visible=False, interactive=False)\n            bothbad_btn = gr.Button(\n                value=\"👎  Both are bad\", visible=False, interactive=False\n            )\n\n    with gr.Row():\n        with gr.Column(scale=20):\n            textbox = gr.Textbox(\n                show_label=False,\n                placeholder=\"Enter your prompt here and press ENTER\",\n                container=False,\n                elem_id=\"input_box\",\n            )\n        with gr.Column(scale=1, min_width=50):\n            send_btn = gr.Button(value=\"Send\", variant=\"primary\")\n\n    with gr.Row() as button_row:\n        regenerate_btn = gr.Button(value=\"🔄  Regenerate\", interactive=False)\n        clear_btn = gr.Button(value=\"🗑️  Clear history\", interactive=False)\n        share_btn = gr.Button(value=\"📷  Share\")\n\n    with gr.Accordion(\"Parameters\", open=False) as parameter_row:\n        temperature = gr.Slider(\n            minimum=0.0,\n            maximum=1.0,\n            value=0.7,\n            step=0.1,\n            interactive=True,\n            label=\"Temperature\",\n        )\n        top_p = gr.Slider(\n            minimum=0.0,\n            maximum=1.0,\n            value=1.0,\n            step=0.1,\n            interactive=True,\n            label=\"Top P\",\n        )\n        max_output_tokens = gr.Slider(\n            minimum=16,\n            maximum=1024,\n            value=512,\n            step=64,\n            interactive=True,\n            label=\"Max output tokens\",\n        )\n\n    gr.Markdown(acknowledgment_md)\n\n    # Register listeners\n    btn_list = [\n        leftvote_btn,\n        rightvote_btn,\n        tie_btn,\n        bothbad_btn,\n        regenerate_btn,\n        clear_btn,\n    ]\n    leftvote_btn.click(\n        leftvote_last_response,\n        states + model_selectors,\n        [textbox, leftvote_btn, rightvote_btn, tie_btn, bothbad_btn],\n    )\n    rightvote_btn.click(\n        rightvote_last_response,\n        states + model_selectors,\n        [textbox, leftvote_btn, rightvote_btn, tie_btn, bothbad_btn],\n    )\n    tie_btn.click(\n        tievote_last_response,\n        states + model_selectors,\n        [textbox, leftvote_btn, rightvote_btn, tie_btn, bothbad_btn],\n    )\n    bothbad_btn.click(\n        bothbad_vote_last_response,\n        states + model_selectors,\n        [textbox, leftvote_btn, rightvote_btn, tie_btn, bothbad_btn],\n    )\n    regenerate_btn.click(\n        regenerate, states, states + chatbots + [textbox] + btn_list\n    ).then(\n        bot_response_multi,\n        states + [temperature, top_p, max_output_tokens],\n        states + chatbots + btn_list,\n    ).then(\n        flash_buttons, [], btn_list\n    )\n    clear_btn.click(clear_history, None, states + chatbots + [textbox] + btn_list)\n\n    share_js = \"\"\"\nfunction (a, b, c, d) {\n    const captureElement = document.querySelector('#share-region-named');\n    html2canvas(captureElement)\n        .then(canvas => {\n            canvas.style.display = 'none'\n            document.body.appendChild(canvas)\n            return canvas\n        })\n        .then(canvas => {\n            const image = canvas.toDataURL('image/png')\n            const a = document.createElement('a')\n            a.setAttribute('download', 'chatbot-arena.png')\n            a.setAttribute('href', image)\n            a.click()\n            canvas.remove()\n        });\n    return [a, b, c, d];\n}\n\"\"\"\n    share_btn.click(share_click, states + model_selectors, [], _js=share_js)\n\n    for i in range(num_sides):\n        model_selectors[i].change(\n            clear_history, None, states + chatbots + [textbox] + btn_list\n        )\n\n    textbox.submit(\n        add_text,\n        states + model_selectors + [textbox],\n        states + chatbots + [textbox] + btn_list,\n    ).then(\n        bot_response_multi,\n        states + [temperature, top_p, max_output_tokens],\n        states + chatbots + btn_list,\n    ).then(\n        flash_buttons, [], btn_list\n    )\n    send_btn.click(\n        add_text,\n        states + model_selectors + [textbox],\n        states + chatbots + [textbox] + btn_list,\n    ).then(\n        bot_response_multi,\n        states + [temperature, top_p, max_output_tokens],\n        states + chatbots + btn_list,\n    ).then(\n        flash_buttons, [], btn_list\n    )\n\n    return states + model_selectors\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/modules/xfastertransformer.py", "content": "from dataclasses import dataclass\nimport sys\n\n\n@dataclass\nclass XftConfig:\n    max_seq_len: int = 4096\n    beam_width: int = 1\n    eos_token_id: int = -1\n    pad_token_id: int = -1\n    num_return_sequences: int = 1\n    is_encoder_decoder: bool = False\n    padding: bool = True\n    early_stopping: bool = False\n    data_type: str = \"bf16_fp16\"\n\n\nclass XftModel:\n    def __init__(self, xft_model, xft_config):\n        self.model = xft_model\n        self.config = xft_config\n\n\ndef load_xft_model(model_path, xft_config: XftConfig):\n    try:\n        import xfastertransformer\n        from transformers import AutoTokenizer\n    except ImportError as e:\n        print(f\"Error: Failed to load xFasterTransformer. {e}\")\n        sys.exit(-1)\n\n    if xft_config.data_type is None or xft_config.data_type == \"\":\n        data_type = \"bf16_fp16\"\n    else:\n        data_type = xft_config.data_type\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_path, use_fast=False, padding_side=\"left\", trust_remote_code=True\n    )\n    xft_model = xfastertransformer.AutoModel.from_pretrained(\n        model_path, dtype=data_type\n    )\n    model = XftModel(xft_model=xft_model, xft_config=xft_config)\n    if model.model.rank > 0:\n        while True:\n            model.model.generate()\n    return model, tokenizer\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/serve/gradio_web_server_multi.py", "content": "\"\"\"\nThe gradio demo server with multiple tabs.\nIt supports chatting with a single model or chatting with two models side-by-side.\n\"\"\"\n\nimport argparse\nimport pickle\nimport time\n\nimport gradio as gr\n\nfrom fastchat.constants import (\n    SESSION_EXPIRATION_TIME,\n)\nfrom fastchat.serve.gradio_block_arena_anony import (\n    build_side_by_side_ui_anony,\n    load_demo_side_by_side_anony,\n    set_global_vars_anony,\n)\nfrom fastchat.serve.gradio_block_arena_named import (\n    build_side_by_side_ui_named,\n    load_demo_side_by_side_named,\n    set_global_vars_named,\n)\nfrom fastchat.serve.gradio_web_server import (\n    set_global_vars,\n    block_css,\n    build_single_model_ui,\n    build_about,\n    get_model_list,\n    load_demo_single,\n    ip_expiration_dict,\n    get_ip,\n)\nfrom fastchat.serve.monitor.monitor import build_leaderboard_tab\nfrom fastchat.utils import (\n    build_logger,\n    get_window_url_params_js,\n    get_window_url_params_with_tos_js,\n    parse_gradio_auth_creds,\n)\n\nlogger = build_logger(\"gradio_web_server_multi\", \"gradio_web_server_multi.log\")\n\n\ndef load_demo(url_params, request: gr.Request):\n    global models\n\n    ip = get_ip(request)\n    logger.info(f\"load_demo. ip: {ip}. params: {url_params}\")\n    ip_expiration_dict[ip] = time.time() + SESSION_EXPIRATION_TIME\n\n    selected = 0\n    if \"arena\" in url_params:\n        selected = 0\n    elif \"compare\" in url_params:\n        selected = 1\n    elif \"single\" in url_params:\n        selected = 2\n    elif \"leaderboard\" in url_params:\n        selected = 3\n\n    if args.model_list_mode == \"reload\":\n        if args.anony_only_for_proprietary_model:\n            models = get_model_list(\n                args.controller_url,\n                args.register_openai_compatible_models,\n                False,\n                False,\n                False,\n            )\n        else:\n            models = get_model_list(\n                args.controller_url,\n                args.register_openai_compatible_models,\n                args.add_chatgpt,\n                args.add_claude,\n                args.add_palm,\n            )\n\n    single_updates = load_demo_single(models, url_params)\n\n    models_anony = list(models)\n    if args.anony_only_for_proprietary_model:\n        # Only enable these models in anony battles.\n        if args.add_chatgpt:\n            models_anony += [\n                \"gpt-4\",\n                \"gpt-3.5-turbo\",\n                \"gpt-4-turbo\",\n                \"gpt-3.5-turbo-1106\",\n            ]\n        if args.add_claude:\n            models_anony += [\"claude-2\", \"claude-1\", \"claude-instant-1\"]\n        if args.add_palm:\n            models_anony += [\"palm-2\"]\n    models_anony = list(set(models_anony))\n\n    side_by_side_anony_updates = load_demo_side_by_side_anony(models_anony, url_params)\n    side_by_side_named_updates = load_demo_side_by_side_named(models, url_params)\n    return (\n        (gr.Tabs.update(selected=selected),)\n        + single_updates\n        + side_by_side_anony_updates\n        + side_by_side_named_updates\n    )\n\n\ndef build_demo(models, elo_results_file, leaderboard_table_file):\n    text_size = gr.themes.sizes.text_md\n    with gr.Blocks(\n        title=\"Chat with Open Large Language Models\",\n        theme=gr.themes.Default(text_size=text_size),\n        css=block_css,\n    ) as demo:\n        with gr.Tabs() as tabs:\n            with gr.Tab(\"Arena (battle)\", id=0):\n                side_by_side_anony_list = build_side_by_side_ui_anony(models)\n\n            with gr.Tab(\"Arena (side-by-side)\", id=1):\n                side_by_side_named_list = build_side_by_side_ui_named(models)\n\n            with gr.Tab(\"Direct Chat\", id=2):\n                single_model_list = build_single_model_ui(\n                    models, add_promotion_links=True\n                )\n            if elo_results_file:\n                with gr.Tab(\"Leaderboard\", id=3):\n                    build_leaderboard_tab(elo_results_file, leaderboard_table_file)\n            with gr.Tab(\"About Us\", id=4):\n                about = build_about()\n\n        url_params = gr.JSON(visible=False)\n\n        if args.model_list_mode not in [\"once\", \"reload\"]:\n            raise ValueError(f\"Unknown model list mode: {args.model_list_mode}\")\n\n        if args.show_terms_of_use:\n            load_js = get_window_url_params_with_tos_js\n        else:\n            load_js = get_window_url_params_js\n\n        demo.load(\n            load_demo,\n            [url_params],\n            [tabs]\n            + single_model_list\n            + side_by_side_anony_list\n            + side_by_side_named_list,\n            _js=load_js,\n        )\n\n    return demo\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\")\n    parser.add_argument(\"--port\", type=int)\n    parser.add_argument(\n        \"--share\",\n        action=\"store_true\",\n        help=\"Whether to generate a public, shareable link\",\n    )\n    parser.add_argument(\n        \"--controller-url\",\n        type=str,\n        default=\"http://localhost:21001\",\n        help=\"The address of the controller\",\n    )\n    parser.add_argument(\n        \"--concurrency-count\",\n        type=int,\n        default=10,\n        help=\"The concurrency count of the gradio queue\",\n    )\n    parser.add_argument(\n        \"--model-list-mode\",\n        type=str,\n        default=\"once\",\n        choices=[\"once\", \"reload\"],\n        help=\"Whether to load the model list once or reload the model list every time.\",\n    )\n    parser.add_argument(\n        \"--moderate\",\n        action=\"store_true\",\n        help=\"Enable content moderation to block unsafe inputs\",\n    )\n    parser.add_argument(\n        \"--show-terms-of-use\",\n        action=\"store_true\",\n        help=\"Shows term of use before loading the demo\",\n    )\n    parser.add_argument(\n        \"--add-chatgpt\",\n        action=\"store_true\",\n        help=\"Add OpenAI's ChatGPT models (gpt-3.5-turbo, gpt-4)\",\n    )\n    parser.add_argument(\n        \"--add-claude\",\n        action=\"store_true\",\n        help=\"Add Anthropic's Claude models (claude-2, claude-instant-1)\",\n    )\n    parser.add_argument(\n        \"--add-palm\",\n        action=\"store_true\",\n        help=\"Add Google's PaLM model (PaLM 2 for Chat: chat-bison@001)\",\n    )\n    parser.add_argument(\n        \"--anony-only-for-proprietary-model\",\n        action=\"store_true\",\n        help=\"Only add ChatGPT, Claude, Bard under anony battle tab\",\n    )\n    parser.add_argument(\n        \"--register-openai-compatible-models\",\n        type=str,\n        help=\"Register custom OpenAI API compatible models by loading them from a JSON file\",\n    )\n    parser.add_argument(\n        \"--gradio-auth-path\",\n        type=str,\n        help='Set the gradio authentication file path. The file should contain one or more user:password pairs in this format: \"u1:p1,u2:p2,u3:p3\"',\n        default=None,\n    )\n    parser.add_argument(\n        \"--elo-results-file\", type=str, help=\"Load leaderboard results and plots\"\n    )\n    parser.add_argument(\n        \"--leaderboard-table-file\", type=str, help=\"Load leaderboard results and plots\"\n    )\n    args = parser.parse_args()\n    logger.info(f\"args: {args}\")\n\n    # Set global variables\n    set_global_vars(args.controller_url, args.moderate)\n    set_global_vars_named(args.moderate)\n    set_global_vars_anony(args.moderate)\n    if args.anony_only_for_proprietary_model:\n        models = get_model_list(\n            args.controller_url,\n            args.register_openai_compatible_models,\n            False,\n            False,\n            False,\n        )\n    else:\n        models = get_model_list(\n            args.controller_url,\n            args.register_openai_compatible_models,\n            args.add_chatgpt,\n            args.add_claude,\n            args.add_palm,\n        )\n\n    # Set authorization credentials\n    auth = None\n    if args.gradio_auth_path is not None:\n        auth = parse_gradio_auth_creds(args.gradio_auth_path)\n\n    # Launch the demo\n    demo = build_demo(models, args.elo_results_file, args.leaderboard_table_file)\n    demo.queue(\n        concurrency_count=args.concurrency_count, status_update_rate=10, api_open=False\n    ).launch(\n        server_name=args.host,\n        server_port=args.port,\n        share=args.share,\n        max_threads=200,\n        auth=auth,\n    )\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/serve/cli.py", "content": "\"\"\"\nChat with a model with command line interface.\n\nUsage:\npython3 -m fastchat.serve.cli --model lmsys/vicuna-7b-v1.5\npython3 -m fastchat.serve.cli --model lmsys/fastchat-t5-3b-v1.0\n\nOther commands:\n- Type \"!!exit\" or an empty line to exit.\n- Type \"!!reset\" to start a new conversation.\n- Type \"!!remove\" to remove the last prompt.\n- Type \"!!regen\" to regenerate the last message.\n- Type \"!!save <filename>\" to save the conversation history to a json file.\n- Type \"!!load <filename>\" to load a conversation history from a json file.\n\"\"\"\nimport argparse\nimport os\nimport re\nimport sys\n\nfrom prompt_toolkit import PromptSession\nfrom prompt_toolkit.auto_suggest import AutoSuggestFromHistory\nfrom prompt_toolkit.completion import WordCompleter\nfrom prompt_toolkit.history import InMemoryHistory\nfrom prompt_toolkit.key_binding import KeyBindings\nfrom rich.console import Console\nfrom rich.live import Live\nfrom rich.markdown import Markdown\nimport torch\n\nfrom fastchat.model.model_adapter import add_model_args\nfrom fastchat.modules.awq import AWQConfig\nfrom fastchat.modules.exllama import ExllamaConfig\nfrom fastchat.modules.xfastertransformer import XftConfig\nfrom fastchat.modules.gptq import GptqConfig\nfrom fastchat.serve.inference import ChatIO, chat_loop\nfrom fastchat.utils import str_to_torch_dtype\n\n\nclass SimpleChatIO(ChatIO):\n    def __init__(self, multiline: bool = False):\n        self._multiline = multiline\n\n    def prompt_for_input(self, role) -> str:\n        if not self._multiline:\n            return input(f\"{role}: \")\n\n        prompt_data = []\n        line = input(f\"{role} [ctrl-d/z on empty line to end]: \")\n        while True:\n            prompt_data.append(line.strip())\n            try:\n                line = input()\n            except EOFError as e:\n                break\n        return \"\\n\".join(prompt_data)\n\n    def prompt_for_output(self, role: str):\n        print(f\"{role}: \", end=\"\", flush=True)\n\n    def stream_output(self, output_stream):\n        pre = 0\n        for outputs in output_stream:\n            output_text = outputs[\"text\"]\n            output_text = output_text.strip().split(\" \")\n            now = len(output_text) - 1\n            if now > pre:\n                print(\" \".join(output_text[pre:now]), end=\" \", flush=True)\n                pre = now\n        print(\" \".join(output_text[pre:]), flush=True)\n        return \" \".join(output_text)\n\n    def print_output(self, text: str):\n        print(text)\n\n\nclass RichChatIO(ChatIO):\n    bindings = KeyBindings()\n\n    @bindings.add(\"escape\", \"enter\")\n    def _(event):\n        event.app.current_buffer.newline()\n\n    def __init__(self, multiline: bool = False, mouse: bool = False):\n        self._prompt_session = PromptSession(history=InMemoryHistory())\n        self._completer = WordCompleter(\n            words=[\"!!exit\", \"!!reset\", \"!!remove\", \"!!regen\", \"!!save\", \"!!load\"],\n            pattern=re.compile(\"$\"),\n        )\n        self._console = Console()\n        self._multiline = multiline\n        self._mouse = mouse\n\n    def prompt_for_input(self, role) -> str:\n        self._console.print(f\"[bold]{role}:\")\n        # TODO(suquark): multiline input has some issues. fix it later.\n        prompt_input = self._prompt_session.prompt(\n            completer=self._completer,\n            multiline=False,\n            mouse_support=self._mouse,\n            auto_suggest=AutoSuggestFromHistory(),\n            key_bindings=self.bindings if self._multiline else None,\n        )\n        self._console.print()\n        return prompt_input\n\n    def prompt_for_output(self, role: str):\n        self._console.print(f\"[bold]{role.replace('/', '|')}:\")\n\n    def stream_output(self, output_stream):\n        \"\"\"Stream output from a role.\"\"\"\n        # TODO(suquark): the console flickers when there is a code block\n        #  above it. We need to cut off \"live\" when a code block is done.\n\n        # Create a Live context for updating the console output\n        with Live(console=self._console, refresh_per_second=4) as live:\n            # Read lines from the stream\n            for outputs in output_stream:\n                if not outputs:\n                    continue\n                text = outputs[\"text\"]\n                # Render the accumulated text as Markdown\n                # NOTE: this is a workaround for the rendering \"unstandard markdown\"\n                #  in rich. The chatbots output treat \"\\n\" as a new line for\n                #  better compatibility with real-world text. However, rendering\n                #  in markdown would break the format. It is because standard markdown\n                #  treat a single \"\\n\" in normal text as a space.\n                #  Our workaround is adding two spaces at the end of each line.\n                #  This is not a perfect solution, as it would\n                #  introduce trailing spaces (only) in code block, but it works well\n                #  especially for console output, because in general the console does not\n                #  care about trailing spaces.\n                lines = []\n                for line in text.splitlines():\n                    lines.append(line)\n                    if line.startswith(\"```\"):\n                        # Code block marker - do not add trailing spaces, as it would\n                        #  break the syntax highlighting\n                        lines.append(\"\\n\")\n                    else:\n                        lines.append(\"  \\n\")\n                markdown = Markdown(\"\".join(lines))\n                # Update the Live console output\n                live.update(markdown)\n        self._console.print()\n        return text\n\n    def print_output(self, text: str):\n        self.stream_output([{\"text\": text}])\n\n\nclass ProgrammaticChatIO(ChatIO):\n    def prompt_for_input(self, role) -> str:\n        contents = \"\"\n        # `end_sequence` signals the end of a message. It is unlikely to occur in\n        #  message content.\n        end_sequence = \" __END_OF_A_MESSAGE_47582648__\\n\"\n        len_end = len(end_sequence)\n        while True:\n            if len(contents) >= len_end:\n                last_chars = contents[-len_end:]\n                if last_chars == end_sequence:\n                    break\n            try:\n                char = sys.stdin.read(1)\n                contents = contents + char\n            except EOFError:\n                continue\n        contents = contents[:-len_end]\n        print(f\"[!OP:{role}]: {contents}\", flush=True)\n        return contents\n\n    def prompt_for_output(self, role: str):\n        print(f\"[!OP:{role}]: \", end=\"\", flush=True)\n\n    def stream_output(self, output_stream):\n        pre = 0\n        for outputs in output_stream:\n            output_text = outputs[\"text\"]\n            output_text = output_text.strip().split(\" \")\n            now = len(output_text) - 1\n            if now > pre:\n                print(\" \".join(output_text[pre:now]), end=\" \", flush=True)\n                pre = now\n        print(\" \".join(output_text[pre:]), flush=True)\n        return \" \".join(output_text)\n\n    def print_output(self, text: str):\n        print(text)\n\n\ndef main(args):\n    if args.gpus:\n        if len(args.gpus.split(\",\")) < args.num_gpus:\n            raise ValueError(\n                f\"Larger --num-gpus ({args.num_gpus}) than --gpus {args.gpus}!\"\n            )\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpus\n        os.environ[\"XPU_VISIBLE_DEVICES\"] = args.gpus\n    if args.enable_exllama:\n        exllama_config = ExllamaConfig(\n            max_seq_len=args.exllama_max_seq_len,\n            gpu_split=args.exllama_gpu_split,\n        )\n    else:\n        exllama_config = None\n    if args.enable_xft:\n        xft_config = XftConfig(\n            max_seq_len=args.xft_max_seq_len,\n            data_type=args.xft_dtype,\n        )\n        if args.device != \"cpu\":\n            print(\"xFasterTransformer now is only support CPUs. Reset device to CPU\")\n            args.device = \"cpu\"\n    else:\n        xft_config = None\n    if args.style == \"simple\":\n        chatio = SimpleChatIO(args.multiline)\n    elif args.style == \"rich\":\n        chatio = RichChatIO(args.multiline, args.mouse)\n    elif args.style == \"programmatic\":\n        chatio = ProgrammaticChatIO()\n    else:\n        raise ValueError(f\"Invalid style for console: {args.style}\")\n    try:\n        chat_loop(\n            args.model_path,\n            args.device,\n            args.num_gpus,\n            args.max_gpu_memory,\n            str_to_torch_dtype(args.dtype),\n            args.load_8bit,\n            args.cpu_offloading,\n            args.conv_template,\n            args.conv_system_msg,\n            args.temperature,\n            args.repetition_penalty,\n            args.max_new_tokens,\n            chatio,\n            gptq_config=GptqConfig(\n                ckpt=args.gptq_ckpt or args.model_path,\n                wbits=args.gptq_wbits,\n                groupsize=args.gptq_groupsize,\n                act_order=args.gptq_act_order,\n            ),\n            awq_config=AWQConfig(\n                ckpt=args.awq_ckpt or args.model_path,\n                wbits=args.awq_wbits,\n                groupsize=args.awq_groupsize,\n            ),\n            exllama_config=exllama_config,\n            xft_config=xft_config,\n            revision=args.revision,\n            judge_sent_end=args.judge_sent_end,\n            debug=args.debug,\n            history=not args.no_history,\n        )\n    except KeyboardInterrupt:\n        print(\"exit...\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    add_model_args(parser)\n    parser.add_argument(\n        \"--conv-template\", type=str, default=None, help=\"Conversation prompt template.\"\n    )\n    parser.add_argument(\n        \"--conv-system-msg\", type=str, default=None, help=\"Conversation system message.\"\n    )\n    parser.add_argument(\"--temperature\", type=float, default=0.7)\n    parser.add_argument(\"--repetition_penalty\", type=float, default=1.0)\n    parser.add_argument(\"--max-new-tokens\", type=int, default=512)\n    parser.add_argument(\"--no-history\", action=\"store_true\")\n    parser.add_argument(\n        \"--style\",\n        type=str,\n        default=\"simple\",\n        choices=[\"simple\", \"rich\", \"programmatic\"],\n        help=\"Display style.\",\n    )\n    parser.add_argument(\n        \"--multiline\",\n        action=\"store_true\",\n        help=\"Enable multiline input. Use ESC+Enter for newline.\",\n    )\n    parser.add_argument(\n        \"--mouse\",\n        action=\"store_true\",\n        help=\"[Rich Style]: Enable mouse support for cursor positioning.\",\n    )\n    parser.add_argument(\n        \"--judge-sent-end\",\n        action=\"store_true\",\n        help=\"Whether enable the correction logic that interrupts the output of sentences due to EOS.\",\n    )\n    parser.add_argument(\n        \"--debug\",\n        action=\"store_true\",\n        help=\"Print useful debug information (e.g., prompts)\",\n    )\n    args = parser.parse_args()\n    main(args)\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/serve/gradio_block_arena_anony.py", "content": "\"\"\"\nChatbot Arena (battle) tab.\nUsers chat with two anonymous models.\n\"\"\"\n\nimport json\nimport time\n\nimport gradio as gr\nimport numpy as np\n\nfrom fastchat.constants import (\n    MODERATION_MSG,\n    CONVERSATION_LIMIT_MSG,\n    SLOW_MODEL_MSG,\n    INPUT_CHAR_LEN_LIMIT,\n    CONVERSATION_TURN_LIMIT,\n)\nfrom fastchat.model.model_adapter import get_conversation_template\nfrom fastchat.serve.gradio_block_arena_named import flash_buttons\nfrom fastchat.serve.gradio_web_server import (\n    State,\n    bot_response,\n    get_conv_log_filename,\n    no_change_btn,\n    enable_btn,\n    disable_btn,\n    invisible_btn,\n    acknowledgment_md,\n    ip_expiration_dict,\n    get_ip,\n)\nfrom fastchat.utils import (\n    build_logger,\n    moderation_filter,\n)\n\nlogger = build_logger(\"gradio_web_server_multi\", \"gradio_web_server_multi.log\")\n\nnum_sides = 2\nenable_moderation = False\nanony_names = [\"\", \"\"]\nmodels = []\n\n\ndef set_global_vars_anony(enable_moderation_):\n    global enable_moderation\n    enable_moderation = enable_moderation_\n\n\ndef load_demo_side_by_side_anony(models_, url_params):\n    global models\n    models = models_\n\n    states = (None,) * num_sides\n    selector_updates = (\n        gr.Markdown.update(visible=True),\n        gr.Markdown.update(visible=True),\n    )\n\n    return states + selector_updates\n\n\ndef vote_last_response(states, vote_type, model_selectors, request: gr.Request):\n    with open(get_conv_log_filename(), \"a\") as fout:\n        data = {\n            \"tstamp\": round(time.time(), 4),\n            \"type\": vote_type,\n            \"models\": [x for x in model_selectors],\n            \"states\": [x.dict() for x in states],\n            \"ip\": get_ip(request),\n        }\n        fout.write(json.dumps(data) + \"\\n\")\n\n    if \":\" not in model_selectors[0]:\n        for i in range(15):\n            names = (\n                \"### Model A: \" + states[0].model_name,\n                \"### Model B: \" + states[1].model_name,\n            )\n            yield names + (\"\",) + (disable_btn,) * 4\n            time.sleep(0.2)\n    else:\n        names = (\n            \"### Model A: \" + states[0].model_name,\n            \"### Model B: \" + states[1].model_name,\n        )\n        yield names + (\"\",) + (disable_btn,) * 4\n\n\ndef leftvote_last_response(\n    state0, state1, model_selector0, model_selector1, request: gr.Request\n):\n    logger.info(f\"leftvote (anony). ip: {get_ip(request)}\")\n    for x in vote_last_response(\n        [state0, state1], \"leftvote\", [model_selector0, model_selector1], request\n    ):\n        yield x\n\n\ndef rightvote_last_response(\n    state0, state1, model_selector0, model_selector1, request: gr.Request\n):\n    logger.info(f\"rightvote (anony). ip: {get_ip(request)}\")\n    for x in vote_last_response(\n        [state0, state1], \"rightvote\", [model_selector0, model_selector1], request\n    ):\n        yield x\n\n\ndef tievote_last_response(\n    state0, state1, model_selector0, model_selector1, request: gr.Request\n):\n    logger.info(f\"tievote (anony). ip: {get_ip(request)}\")\n    for x in vote_last_response(\n        [state0, state1], \"tievote\", [model_selector0, model_selector1], request\n    ):\n        yield x\n\n\ndef bothbad_vote_last_response(\n    state0, state1, model_selector0, model_selector1, request: gr.Request\n):\n    logger.info(f\"bothbad_vote (anony). ip: {get_ip(request)}\")\n    for x in vote_last_response(\n        [state0, state1], \"bothbad_vote\", [model_selector0, model_selector1], request\n    ):\n        yield x\n\n\ndef regenerate(state0, state1, request: gr.Request):\n    logger.info(f\"regenerate (anony). ip: {get_ip(request)}\")\n    states = [state0, state1]\n    for i in range(num_sides):\n        states[i].conv.update_last_message(None)\n    return states + [x.to_gradio_chatbot() for x in states] + [\"\"] + [disable_btn] * 6\n\n\ndef clear_history(request: gr.Request):\n    logger.info(f\"clear_history (anony). ip: {get_ip(request)}\")\n    return (\n        [None] * num_sides\n        + [None] * num_sides\n        + anony_names\n        + [\"\"]\n        + [invisible_btn] * 4\n        + [disable_btn] * 2\n        + [\"\"]\n    )\n\n\ndef share_click(state0, state1, model_selector0, model_selector1, request: gr.Request):\n    logger.info(f\"share (anony). ip: {get_ip(request)}\")\n    if state0 is not None and state1 is not None:\n        vote_last_response(\n            [state0, state1], \"share\", [model_selector0, model_selector1], request\n        )\n\n\nSAMPLING_WEIGHTS = {\n    # tier 0\n    \"gpt-4\": 4,\n    \"gpt-4-turbo\": 4,\n    \"gpt-3.5-turbo\": 2,\n    \"gpt-3.5-turbo-1106\": 2,\n    \"claude-2\": 8,\n    \"claude-1\": 2,\n    \"claude-instant-1\": 8,\n    \"zephyr-7b-beta\": 2,\n    \"openchat-3.5\": 2,\n    # tier 1\n    \"deluxe-chat-v1.1\": 2,\n    \"palm-2\": 1.5,\n    \"llama-2-70b-chat\": 1.5,\n    \"llama-2-13b-chat\": 1.5,\n    \"codellama-34b-instruct\": 1.5,\n    \"vicuna-33b\": 8,\n    \"vicuna-13b\": 1.5,\n    \"wizardlm-70b\": 1.5,\n    \"wizardlm-13b\": 1.5,\n    \"qwen-14b-chat\": 1.5,\n    \"mistral-7b-instruct\": 1.5,\n    # tier 2\n    \"vicuna-7b\": 1.0,\n    \"llama-2-7b-chat\": 1.0,\n    \"chatglm2-6b\": 1.0,\n    # deprecated\n    \"zephyr-7b-alpha\": 1.5,\n    \"codellama-13b-instruct\": 1.0,\n    \"mpt-30b-chat\": 1.5,\n    \"guanaco-33b\": 1.0,\n    \"fastchat-t5-3b\": 0.5,\n    \"alpaca-13b\": 0.5,\n    \"mpt-7b-chat\": 0.1,\n    \"oasst-pythia-12b\": 0.1,\n    \"RWKV-4-Raven-14B\": 0.1,\n    \"gpt4all-13b-snoozy\": 0.1,\n    \"koala-13b\": 0.1,\n    \"stablelm-tuned-alpha-7b\": 0.1,\n    \"dolly-v2-12b\": 0.1,\n    \"llama-13b\": 0.1,\n    \"chatglm-6b\": 0.5,\n    \"deluxe-chat-v1\": 4,\n}\n\n# target model sampling weights will be boosted.\nBATTLE_TARGETS = {\n    \"gpt-4\": {\"claude-2\"},\n    \"gpt-4-turbo\": {\"gpt-4\", \"gpt-3.5-turbo\"},\n    \"gpt-3.5-turbo\": {\"claude-instant-1\", \"gpt-4\", \"claude-2\"},\n    \"claude-2\": {\"gpt-4\", \"gpt-3.5-turbo\", \"claude-1\"},\n    \"claude-1\": {\"claude-2\", \"gpt-4\", \"gpt-3.5-turbo\"},\n    \"claude-instant-1\": {\"gpt-3.5-turbo\", \"claude-2\"},\n    \"deluxe-chat-v1.1\": {\"gpt-4\"},\n    \"openchat-3.5\": {\"gpt-3.5-turbo\", \"llama-2-70b-chat\", \"zephyr-7b-beta\"},\n    \"qwen-14b-chat\": {\"vicuna-13b\", \"llama-2-13b-chat\", \"llama-2-70b-chat\"},\n    \"zephyr-7b-alpha\": {\"mistral-7b-instruct\", \"llama-2-13b-chat\"},\n    \"zephyr-7b-beta\": {\n        \"mistral-7b-instruct\",\n        \"llama-2-13b-chat\",\n        \"llama-2-7b-chat\",\n        \"wizardlm-13b\",\n    },\n    \"llama-2-70b-chat\": {\"gpt-3.5-turbo\", \"vicuna-33b\", \"claude-instant-1\"},\n    \"llama-2-13b-chat\": {\"mistral-7b-instruct\", \"vicuna-13b\", \"llama-2-70b-chat\"},\n    \"llama-2-7b-chat\": {\"mistral-7b-instruct\", \"vicuna-7b\", \"llama-2-13b-chat\"},\n    \"mistral-7b-instruct\": {\n        \"llama-2-7b-chat\",\n        \"llama-2-13b-chat\",\n        \"llama-2-70b-chat\",\n    },\n    \"vicuna-33b\": {\"llama-2-70b-chat\", \"gpt-3.5-turbo\", \"claude-instant-1\"},\n    \"vicuna-13b\": {\"llama-2-13b-chat\", \"llama-2-70b-chat\"},\n    \"vicuna-7b\": {\"llama-2-7b-chat\", \"mistral-7b-instruct\", \"llama-2-13b-chat\"},\n    \"wizardlm-70b\": {\"gpt-3.5-turbo\", \"vicuna-33b\", \"claude-instant-1\"},\n    \"palm-2\": {\"llama-2-13b-chat\", \"gpt-3.5-turbo\"},\n}\n\nSAMPLING_BOOST_MODELS = [\"openchat-3.5\", \"gpt-4-turbo\", \"gpt-3.5-turbo-1106\"]\n\n# outage models won't be sampled.\nOUTAGE_MODELS = []\n\n\ndef get_sample_weight(model):\n    if model in OUTAGE_MODELS:\n        return 0\n    weight = SAMPLING_WEIGHTS.get(model, 1.0)\n    if model in SAMPLING_BOOST_MODELS:\n        weight *= 5\n    return weight\n\n\ndef get_battle_pair():\n    if len(models) == 1:\n        return models[0], models[0]\n\n    model_weights = []\n    for model in models:\n        weight = get_sample_weight(model)\n        model_weights.append(weight)\n    total_weight = np.sum(model_weights)\n    model_weights = model_weights / total_weight\n    chosen_idx = np.random.choice(len(models), p=model_weights)\n    chosen_model = models[chosen_idx]\n\n    rival_models = []\n    rival_weights = []\n    for model in models:\n        if model == chosen_model:\n            continue\n        weight = get_sample_weight(model)\n        if (\n            weight != 0\n            and chosen_model in BATTLE_TARGETS\n            and model in BATTLE_TARGETS[chosen_model]\n        ):\n            # boost to 50% chance\n            weight = total_weight / len(BATTLE_TARGETS[chosen_model])\n        rival_models.append(model)\n        rival_weights.append(weight)\n    # for p, w in zip(rival_models, rival_weights):\n    #     print(p, w)\n    rival_weights = rival_weights / np.sum(rival_weights)\n    rival_idx = np.random.choice(len(rival_models), p=rival_weights)\n    rival_model = rival_models[rival_idx]\n\n    swap = np.random.randint(2)\n    if swap == 0:\n        return chosen_model, rival_model\n    else:\n        return rival_model, chosen_model\n\n\ndef add_text(\n    state0, state1, model_selector0, model_selector1, text, request: gr.Request\n):\n    ip = get_ip(request)\n    logger.info(f\"add_text (anony). ip: {ip}. len: {len(text)}\")\n    states = [state0, state1]\n    model_selectors = [model_selector0, model_selector1]\n\n    # Init states if necessary\n    if states[0] is None:\n        assert states[1] is None\n\n        model_left, model_right = get_battle_pair()\n        states = [\n            State(model_left),\n            State(model_right),\n        ]\n\n    if len(text) <= 0:\n        for i in range(num_sides):\n            states[i].skip_next = True\n        return (\n            states\n            + [x.to_gradio_chatbot() for x in states]\n            + [\"\"]\n            + [\n                no_change_btn,\n            ]\n            * 6\n            + [\"\"]\n        )\n\n    model_list = [states[i].model_name for i in range(num_sides)]\n    flagged = moderation_filter(text, model_list)\n    if flagged:\n        logger.info(f\"violate moderation (anony). ip: {ip}. text: {text}\")\n        # overwrite the original text\n        text = MODERATION_MSG\n\n    conv = states[0].conv\n    if (len(conv.messages) - conv.offset) // 2 >= CONVERSATION_TURN_LIMIT:\n        logger.info(f\"conversation turn limit. ip: {get_ip(request)}. text: {text}\")\n        for i in range(num_sides):\n            states[i].skip_next = True\n        return (\n            states\n            + [x.to_gradio_chatbot() for x in states]\n            + [CONVERSATION_LIMIT_MSG]\n            + [\n                no_change_btn,\n            ]\n            * 6\n            + [\"\"]\n        )\n\n    text = text[:INPUT_CHAR_LEN_LIMIT]  # Hard cut-off\n    for i in range(num_sides):\n        states[i].conv.append_message(states[i].conv.roles[0], text)\n        states[i].conv.append_message(states[i].conv.roles[1], None)\n        states[i].skip_next = False\n\n    slow_model_msg = \"\"\n    for i in range(num_sides):\n        if \"deluxe\" in states[i].model_name:\n            slow_model_msg = SLOW_MODEL_MSG\n    return (\n        states\n        + [x.to_gradio_chatbot() for x in states]\n        + [\"\"]\n        + [\n            disable_btn,\n        ]\n        * 6\n        + [slow_model_msg]\n    )\n\n\ndef bot_response_multi(\n    state0,\n    state1,\n    temperature,\n    top_p,\n    max_new_tokens,\n    request: gr.Request,\n):\n    logger.info(f\"bot_response_multi (anony). ip: {get_ip(request)}\")\n\n    if state0 is None or state0.skip_next:\n        # This generate call is skipped due to invalid inputs\n        yield (\n            state0,\n            state1,\n            state0.to_gradio_chatbot(),\n            state1.to_gradio_chatbot(),\n        ) + (no_change_btn,) * 6\n        return\n\n    states = [state0, state1]\n    gen = []\n    for i in range(num_sides):\n        gen.append(\n            bot_response(\n                states[i],\n                temperature,\n                top_p,\n                max_new_tokens,\n                request,\n            )\n        )\n\n    chatbots = [None] * num_sides\n    while True:\n        stop = True\n        for i in range(num_sides):\n            try:\n                ret = next(gen[i])\n                states[i], chatbots[i] = ret[0], ret[1]\n                stop = False\n            except StopIteration:\n                pass\n        yield states + chatbots + [disable_btn] * 6\n        if stop:\n            break\n\n\ndef build_side_by_side_ui_anony(models):\n    notice_markdown = \"\"\"\n# ⚔️  Chatbot Arena ⚔️ : Benchmarking LLMs in the Wild\n| [Blog](https://lmsys.org/blog/2023-05-03-arena/) | [GitHub](https://github.com/lm-sys/FastChat) | [Paper](https://arxiv.org/abs/2306.05685) | [Dataset](https://github.com/lm-sys/FastChat/blob/main/docs/dataset_release.md) | [Twitter](https://twitter.com/lmsysorg) | [Discord](https://discord.gg/HSWAKCrnFx) |\n\n## 📜 Rules\n- Ask any question to two anonymous models (e.g., ChatGPT, Claude, Llama) and vote for the better one!\n- You can continue chatting until you identify a winner.\n- Vote won't be counted if model identity is revealed during conversation.\n\n## 🏆 Arena Elo [Leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)\nWe use **100K** human votes to compile an Elo-based LLM leaderboard.\nFind out who is the 🥇LLM Champion!\n\n## 👇 Chat now!\n\n\"\"\"\n\n    states = [gr.State() for _ in range(num_sides)]\n    model_selectors = [None] * num_sides\n    chatbots = [None] * num_sides\n\n    gr.Markdown(notice_markdown, elem_id=\"notice_markdown\")\n\n    with gr.Box(elem_id=\"share-region-anony\"):\n        with gr.Row():\n            for i in range(num_sides):\n                label = \"Model A\" if i == 0 else \"Model B\"\n                with gr.Column():\n                    chatbots[i] = gr.Chatbot(\n                        label=label, elem_id=f\"chatbot\", height=550\n                    )\n\n        with gr.Row():\n            for i in range(num_sides):\n                with gr.Column():\n                    model_selectors[i] = gr.Markdown(anony_names[i])\n        with gr.Row():\n            slow_warning = gr.Markdown(\"\", elem_id=\"notice_markdown\")\n\n        with gr.Row():\n            leftvote_btn = gr.Button(\n                value=\"👈  A is better\", visible=False, interactive=False\n            )\n            rightvote_btn = gr.Button(\n                value=\"👉  B is better\", visible=False, interactive=False\n            )\n            tie_btn = gr.Button(value=\"🤝  Tie\", visible=False, interactive=False)\n            bothbad_btn = gr.Button(\n                value=\"👎  Both are bad\", visible=False, interactive=False\n            )\n\n    with gr.Row():\n        with gr.Column(scale=20):\n            textbox = gr.Textbox(\n                show_label=False,\n                placeholder=\"👉 Enter your prompt and press ENTER\",\n                container=False,\n                elem_id=\"input_box\",\n            )\n        with gr.Column(scale=1, min_width=50):\n            send_btn = gr.Button(value=\"Send\", variant=\"primary\")\n\n    with gr.Row() as button_row:\n        clear_btn = gr.Button(value=\"🎲 New Round\", interactive=False)\n        regenerate_btn = gr.Button(value=\"🔄  Regenerate\", interactive=False)\n        share_btn = gr.Button(value=\"📷  Share\")\n\n    with gr.Accordion(\"Parameters\", open=False) as parameter_row:\n        temperature = gr.Slider(\n            minimum=0.0,\n            maximum=1.0,\n            value=0.7,\n            step=0.1,\n            interactive=True,\n            label=\"Temperature\",\n        )\n        top_p = gr.Slider(\n            minimum=0.0,\n            maximum=1.0,\n            value=1.0,\n            step=0.1,\n            interactive=True,\n            label=\"Top P\",\n        )\n        max_output_tokens = gr.Slider(\n            minimum=16,\n            maximum=1024,\n            value=512,\n            step=64,\n            interactive=True,\n            label=\"Max output tokens\",\n        )\n\n    gr.Markdown(acknowledgment_md)\n\n    # Register listeners\n    btn_list = [\n        leftvote_btn,\n        rightvote_btn,\n        tie_btn,\n        bothbad_btn,\n        regenerate_btn,\n        clear_btn,\n    ]\n    leftvote_btn.click(\n        leftvote_last_response,\n        states + model_selectors,\n        model_selectors + [textbox, leftvote_btn, rightvote_btn, tie_btn, bothbad_btn],\n    )\n    rightvote_btn.click(\n        rightvote_last_response,\n        states + model_selectors,\n        model_selectors + [textbox, leftvote_btn, rightvote_btn, tie_btn, bothbad_btn],\n    )\n    tie_btn.click(\n        tievote_last_response,\n        states + model_selectors,\n        model_selectors + [textbox, leftvote_btn, rightvote_btn, tie_btn, bothbad_btn],\n    )\n    bothbad_btn.click(\n        bothbad_vote_last_response,\n        states + model_selectors,\n        model_selectors + [textbox, leftvote_btn, rightvote_btn, tie_btn, bothbad_btn],\n    )\n    regenerate_btn.click(\n        regenerate, states, states + chatbots + [textbox] + btn_list\n    ).then(\n        bot_response_multi,\n        states + [temperature, top_p, max_output_tokens],\n        states + chatbots + btn_list,\n    ).then(\n        flash_buttons, [], btn_list\n    )\n    clear_btn.click(\n        clear_history,\n        None,\n        states + chatbots + model_selectors + [textbox] + btn_list + [slow_warning],\n    )\n\n    share_js = \"\"\"\nfunction (a, b, c, d) {\n    const captureElement = document.querySelector('#share-region-anony');\n    html2canvas(captureElement)\n        .then(canvas => {\n            canvas.style.display = 'none'\n            document.body.appendChild(canvas)\n            return canvas\n        })\n        .then(canvas => {\n            const image = canvas.toDataURL('image/png')\n            const a = document.createElement('a')\n            a.setAttribute('download', 'chatbot-arena.png')\n            a.setAttribute('href', image)\n            a.click()\n            canvas.remove()\n        });\n    return [a, b, c, d];\n}\n\"\"\"\n    share_btn.click(share_click, states + model_selectors, [], _js=share_js)\n\n    textbox.submit(\n        add_text,\n        states + model_selectors + [textbox],\n        states + chatbots + [textbox] + btn_list + [slow_warning],\n    ).then(\n        bot_response_multi,\n        states + [temperature, top_p, max_output_tokens],\n        states + chatbots + btn_list,\n    ).then(\n        flash_buttons,\n        [],\n        btn_list,\n    )\n\n    send_btn.click(\n        add_text,\n        states + model_selectors + [textbox],\n        states + chatbots + [textbox] + btn_list,\n    ).then(\n        bot_response_multi,\n        states + [temperature, top_p, max_output_tokens],\n        states + chatbots + btn_list,\n    ).then(\n        flash_buttons, [], btn_list\n    )\n\n    return states + model_selectors\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/protocol/openai_api_protocol.py", "content": "from typing import Literal, Optional, List, Dict, Any, Union\n\nimport time\n\nimport shortuuid\nfrom pydantic import BaseModel, Field\n\n\nclass ErrorResponse(BaseModel):\n    object: str = \"error\"\n    message: str\n    code: int\n\n\nclass ModelPermission(BaseModel):\n    id: str = Field(default_factory=lambda: f\"modelperm-{shortuuid.random()}\")\n    object: str = \"model_permission\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    allow_create_engine: bool = False\n    allow_sampling: bool = True\n    allow_logprobs: bool = True\n    allow_search_indices: bool = True\n    allow_view: bool = True\n    allow_fine_tuning: bool = False\n    organization: str = \"*\"\n    group: Optional[str] = None\n    is_blocking: str = False\n\n\nclass ModelCard(BaseModel):\n    id: str\n    object: str = \"model\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    owned_by: str = \"fastchat\"\n    root: Optional[str] = None\n    parent: Optional[str] = None\n    permission: List[ModelPermission] = []\n\n\nclass ModelList(BaseModel):\n    object: str = \"list\"\n    data: List[ModelCard] = []\n\n\nclass UsageInfo(BaseModel):\n    prompt_tokens: int = 0\n    total_tokens: int = 0\n    completion_tokens: Optional[int] = 0\n\n\nclass LogProbs(BaseModel):\n    text_offset: List[int] = Field(default_factory=list)\n    token_logprobs: List[Optional[float]] = Field(default_factory=list)\n    tokens: List[str] = Field(default_factory=list)\n    top_logprobs: List[Optional[Dict[str, float]]] = Field(default_factory=list)\n\n\nclass ChatCompletionRequest(BaseModel):\n    model: str\n    messages: Union[str, List[Dict[str, str]]]\n    temperature: Optional[float] = 0.7\n    top_p: Optional[float] = 1.0\n    top_k: Optional[int] = -1\n    n: Optional[int] = 1\n    max_tokens: Optional[int] = None\n    stop: Optional[Union[str, List[str]]] = None\n    stream: Optional[bool] = False\n    presence_penalty: Optional[float] = 0.0\n    frequency_penalty: Optional[float] = 0.0\n    user: Optional[str] = None\n\n\nclass ChatMessage(BaseModel):\n    role: str\n    content: str\n\n\nclass ChatCompletionResponseChoice(BaseModel):\n    index: int\n    message: ChatMessage\n    finish_reason: Optional[Literal[\"stop\", \"length\"]] = None\n\n\nclass ChatCompletionResponse(BaseModel):\n    id: str = Field(default_factory=lambda: f\"chatcmpl-{shortuuid.random()}\")\n    object: str = \"chat.completion\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: List[ChatCompletionResponseChoice]\n    usage: UsageInfo\n\n\nclass DeltaMessage(BaseModel):\n    role: Optional[str] = None\n    content: Optional[str] = None\n\n\nclass ChatCompletionResponseStreamChoice(BaseModel):\n    index: int\n    delta: DeltaMessage\n    finish_reason: Optional[Literal[\"stop\", \"length\"]] = None\n\n\nclass ChatCompletionStreamResponse(BaseModel):\n    id: str = Field(default_factory=lambda: f\"chatcmpl-{shortuuid.random()}\")\n    object: str = \"chat.completion.chunk\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: List[ChatCompletionResponseStreamChoice]\n\n\nclass TokenCheckRequestItem(BaseModel):\n    model: str\n    prompt: str\n    max_tokens: int\n\n\nclass TokenCheckRequest(BaseModel):\n    prompts: List[TokenCheckRequestItem]\n\n\nclass TokenCheckResponseItem(BaseModel):\n    fits: bool\n    tokenCount: int\n    contextLength: int\n\n\nclass TokenCheckResponse(BaseModel):\n    prompts: List[TokenCheckResponseItem]\n\n\nclass EmbeddingsRequest(BaseModel):\n    model: Optional[str] = None\n    engine: Optional[str] = None\n    input: Union[str, List[Any]]\n    user: Optional[str] = None\n    encoding_format: Optional[str] = None\n\n\nclass EmbeddingsResponse(BaseModel):\n    object: str = \"list\"\n    data: List[Dict[str, Any]]\n    model: str\n    usage: UsageInfo\n\n\nclass CompletionRequest(BaseModel):\n    model: str\n    prompt: Union[str, List[Any]]\n    suffix: Optional[str] = None\n    temperature: Optional[float] = 0.7\n    n: Optional[int] = 1\n    max_tokens: Optional[int] = 16\n    stop: Optional[Union[str, List[str]]] = None\n    stream: Optional[bool] = False\n    top_p: Optional[float] = 1.0\n    top_k: Optional[int] = -1\n    logprobs: Optional[int] = None\n    echo: Optional[bool] = False\n    presence_penalty: Optional[float] = 0.0\n    frequency_penalty: Optional[float] = 0.0\n    user: Optional[str] = None\n    use_beam_search: Optional[bool] = False\n    best_of: Optional[int] = None\n\n\nclass CompletionResponseChoice(BaseModel):\n    index: int\n    text: str\n    logprobs: Optional[LogProbs] = None\n    finish_reason: Optional[Literal[\"stop\", \"length\"]] = None\n\n\nclass CompletionResponse(BaseModel):\n    id: str = Field(default_factory=lambda: f\"cmpl-{shortuuid.random()}\")\n    object: str = \"text_completion\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: List[CompletionResponseChoice]\n    usage: UsageInfo\n\n\nclass CompletionResponseStreamChoice(BaseModel):\n    index: int\n    text: str\n    logprobs: Optional[LogProbs] = None\n    finish_reason: Optional[Literal[\"stop\", \"length\"]] = None\n\n\nclass CompletionStreamResponse(BaseModel):\n    id: str = Field(default_factory=lambda: f\"cmpl-{shortuuid.random()}\")\n    object: str = \"text_completion\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: List[CompletionResponseStreamChoice]\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/serve/__init__.py", "content": ""}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/serve/inference.py", "content": "\"\"\"Inference for FastChat models.\"\"\"\nimport abc\nimport gc\nimport json\nimport math\nimport os\nimport sys\nimport time\nfrom typing import Iterable, Optional, Dict\nimport warnings\n\nimport psutil\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    LlamaTokenizer,\n    LlamaForCausalLM,\n    AutoModel,\n    AutoModelForSeq2SeqLM,\n    T5Tokenizer,\n    AutoConfig,\n)\nfrom transformers.generation.logits_process import (\n    LogitsProcessorList,\n    RepetitionPenaltyLogitsProcessor,\n    TemperatureLogitsWarper,\n    TopKLogitsWarper,\n    TopPLogitsWarper,\n)\n\nfrom fastchat.conversation import get_conv_template, SeparatorStyle\nfrom fastchat.model.model_adapter import (\n    load_model,\n    get_conversation_template,\n    get_generate_stream_function,\n)\nfrom fastchat.modules.awq import AWQConfig\nfrom fastchat.modules.gptq import GptqConfig\nfrom fastchat.modules.exllama import ExllamaConfig\nfrom fastchat.modules.xfastertransformer import XftConfig\nfrom fastchat.utils import is_partial_stop, is_sentence_complete, get_context_length\n\n\ndef prepare_logits_processor(\n    temperature: float, repetition_penalty: float, top_p: float, top_k: int\n) -> LogitsProcessorList:\n    processor_list = LogitsProcessorList()\n    # TemperatureLogitsWarper doesn't accept 0.0, 1.0 makes it a no-op so we skip two cases.\n    if temperature >= 1e-5 and temperature != 1.0:\n        processor_list.append(TemperatureLogitsWarper(temperature))\n    if repetition_penalty > 1.0:\n        processor_list.append(RepetitionPenaltyLogitsProcessor(repetition_penalty))\n    if 1e-8 <= top_p < 1.0:\n        processor_list.append(TopPLogitsWarper(top_p))\n    if top_k > 0:\n        processor_list.append(TopKLogitsWarper(top_k))\n    return processor_list\n\n\n@torch.inference_mode()\ndef generate_stream(\n    model,\n    tokenizer,\n    params: Dict,\n    device: str,\n    context_len: int,\n    stream_interval: int = 2,\n    judge_sent_end: bool = False,\n):\n    if hasattr(model, \"device\"):\n        device = model.device\n\n    # Read parameters\n    prompt = params[\"prompt\"]\n    len_prompt = len(prompt)\n    temperature = float(params.get(\"temperature\", 1.0))\n    repetition_penalty = float(params.get(\"repetition_penalty\", 1.0))\n    top_p = float(params.get(\"top_p\", 1.0))\n    top_k = int(params.get(\"top_k\", -1))  # -1 means disable\n    max_new_tokens = int(params.get(\"max_new_tokens\", 256))\n    logprobs = params.get(\"logprobs\", None)  # FIXME: Support logprobs>1.\n    echo = bool(params.get(\"echo\", True))\n    \n    stop_str = params.get(\"stop\", None)\n    stop_token_ids = params.get(\"stop_token_ids\", None) or []\n    \n    # HACK(chaofan): Don't stop in EOS for testing stability.\n    # if tokenizer.eos_token_id not in stop_token_ids:\n    #     stop_token_ids.append(tokenizer.eos_token_id)\n\n    logits_processor = prepare_logits_processor(\n        temperature, repetition_penalty, top_p, top_k\n    )\n    input_ids = tokenizer(prompt).input_ids\n\n    if model.config.is_encoder_decoder:\n        max_src_len = context_len\n    else:  # truncate\n        max_src_len = context_len - max_new_tokens - 1\n\n    input_ids = input_ids[-max_src_len:]\n    output_ids = list(input_ids)\n    input_echo_len = len(input_ids)\n\n    if model.config.is_encoder_decoder:\n        if logprobs is not None:  # FIXME: Support logprobs for encoder-decoder models.\n            raise NotImplementedError\n        encoder_output = model.encoder(\n            input_ids=torch.as_tensor([input_ids], device=device)\n        )[0]\n        start_ids = torch.as_tensor(\n            [[model.generation_config.decoder_start_token_id]],\n            dtype=torch.int64,\n            device=device,\n        )\n    else:\n        start_ids = torch.as_tensor([input_ids], device=device)\n\n    past_key_values = out = None\n    token_logprobs = [None]  # The first token has no logprobs.\n    sent_interrupt = False\n    finish_reason = None\n    for i in range(max_new_tokens):\n        if i == 0:  # prefill\n            if model.config.is_encoder_decoder:\n                out = model.decoder(\n                    input_ids=start_ids,\n                    encoder_hidden_states=encoder_output,\n                    use_cache=True,\n                )\n                logits = model.lm_head(out[0])\n            else:\n                out = model(input_ids=start_ids, use_cache=True)\n                logits = out.logits\n            past_key_values = out.past_key_values\n\n            if logprobs is not None:\n                # Prefull logprobs for the prompt.\n                shift_input_ids = start_ids[..., 1:].contiguous()\n                shift_logits = logits[..., :-1, :].contiguous()\n                shift_logits = torch.log_softmax(shift_logits, dim=-1).tolist()\n                for label_id, logit in zip(\n                    shift_input_ids[0].tolist(), shift_logits[0]\n                ):\n                    token_logprobs.append(logit[label_id])\n        else:  # decoding\n            if model.config.is_encoder_decoder:\n                out = model.decoder(\n                    input_ids=torch.as_tensor(\n                        [[token] if not sent_interrupt else output_ids],\n                        device=device,\n                    ),\n                    encoder_hidden_states=encoder_output,\n                    use_cache=True,\n                    past_key_values=past_key_values if not sent_interrupt else None,\n                )\n                sent_interrupt = False\n\n                logits = model.lm_head(out[0])\n            else:\n                # torch.cuda.synchronize()\n                # st = time.perf_counter_ns()\n\n                out = model(\n                    input_ids=torch.as_tensor(\n                        [[token] if not sent_interrupt else output_ids],\n                        device=device,\n                    ),\n                    use_cache=True,\n                    past_key_values=past_key_values if not sent_interrupt else None,\n                )\n\n                # torch.cuda.synchronize()\n                # ed = time.perf_counter_ns()\n\n                # print(\n                #     f\"One token decode time: {(ed - st) / 1e6} ms, {past_key_values[0][0].shape}\"\n                # )\n\n                sent_interrupt = False\n                logits = out.logits\n            past_key_values = out.past_key_values\n\n        if logits_processor:\n            if repetition_penalty > 1.0:\n                tmp_output_ids = torch.as_tensor([output_ids], device=logits.device)\n            else:\n                tmp_output_ids = None\n            last_token_logits = logits_processor(tmp_output_ids, logits[:, -1, :])[0]\n        else:\n            last_token_logits = logits[0, -1, :]\n\n        if device == \"mps\":\n            # Switch to CPU by avoiding some bugs in mps backend.\n            last_token_logits = last_token_logits.float().to(\"cpu\")\n\n        if temperature < 1e-5 or top_p < 1e-8:  # greedy\n            _, indices = torch.topk(last_token_logits, 2)\n            tokens = [int(index) for index in indices.tolist()]\n        else:\n            probs = torch.softmax(last_token_logits, dim=-1)\n            indices = torch.multinomial(probs, num_samples=2)\n            tokens = [int(token) for token in indices.tolist()]\n        token = tokens[0]\n        output_ids.append(token)\n        if logprobs is not None:\n            # Cannot use last_token_logits because logprobs is based on raw logits.\n            token_logprobs.append(\n                torch.log_softmax(logits[0, -1, :], dim=-1)[token].tolist()\n            )\n\n        if token in stop_token_ids:\n            stopped = True\n        else:\n            stopped = False\n\n        # Yield the output tokens\n        if i % stream_interval == 0 or i == max_new_tokens - 1 or stopped:\n            if echo:\n                tmp_output_ids = output_ids\n                rfind_start = len_prompt\n            else:\n                tmp_output_ids = output_ids[input_echo_len:]\n                rfind_start = 0\n\n            output = tokenizer.decode(\n                tmp_output_ids,\n                skip_special_tokens=True,\n                spaces_between_special_tokens=False,\n                clean_up_tokenization_spaces=True,\n            )\n            ret_logprobs = None\n            if logprobs is not None:\n                ret_logprobs = {\n                    \"text_offset\": [],\n                    \"tokens\": [\n                        tokenizer.decode(token)\n                        for token in (\n                            output_ids if echo else output_ids[input_echo_len:]\n                        )\n                    ],\n                    \"token_logprobs\": token_logprobs\n                    if echo\n                    else token_logprobs[input_echo_len:],\n                    \"top_logprobs\": [{}]\n                    * len(token_logprobs if echo else token_logprobs[input_echo_len:]),\n                }\n                # Compute text_offset\n                curr_pos = 0\n                for text in ret_logprobs[\"tokens\"]:\n                    ret_logprobs[\"text_offset\"].append(curr_pos)\n                    curr_pos += len(text)\n\n            # TODO: For the issue of incomplete sentences interrupting output, apply a patch and others can also modify it to a more elegant way\n            if judge_sent_end and stopped and not is_sentence_complete(output):\n                if len(tokens) > 1:\n                    token = tokens[1]\n                    output_ids[-1] = token\n                else:\n                    output_ids.pop()\n                stopped = False\n                sent_interrupt = True\n\n            partially_stopped = False\n            if stop_str:\n                if isinstance(stop_str, str):\n                    pos = output.rfind(stop_str, rfind_start)\n                    if pos != -1:\n                        output = output[:pos]\n                        stopped = True\n                    else:\n                        partially_stopped = is_partial_stop(output, stop_str)\n                elif isinstance(stop_str, Iterable):\n                    for each_stop in stop_str:\n                        pos = output.rfind(each_stop, rfind_start)\n                        if pos != -1:\n                            output = output[:pos]\n                            stopped = True\n                            break\n                        else:\n                            partially_stopped = is_partial_stop(output, each_stop)\n                            if partially_stopped:\n                                break\n                else:\n                    raise ValueError(\"Invalid stop field type.\")\n\n            # Prevent yielding partial stop sequence\n            if not partially_stopped:\n                yield {\n                    \"text\": output,\n                    \"logprobs\": ret_logprobs,\n                    \"usage\": {\n                        \"prompt_tokens\": input_echo_len,\n                        \"completion_tokens\": i,\n                        \"total_tokens\": input_echo_len + i,\n                    },\n                    \"finish_reason\": None,\n                }\n\n        if stopped:\n            break\n\n    # Finish stream event, which contains finish reason\n    else:\n        finish_reason = \"length\"\n\n    if stopped:\n        finish_reason = \"stop\"\n\n    yield {\n        \"text\": output,\n        \"logprobs\": ret_logprobs,\n        \"usage\": {\n            \"prompt_tokens\": input_echo_len,\n            \"completion_tokens\": i,\n            \"total_tokens\": input_echo_len + i,\n        },\n        \"finish_reason\": finish_reason,\n    }\n\n    # Clean\n    del past_key_values, out\n    gc.collect()\n    torch.cuda.empty_cache()\n    if device == \"xpu\":\n        torch.xpu.empty_cache()\n    if device == \"npu\":\n        torch.npu.empty_cache()\n\n\nclass ChatIO(abc.ABC):\n    @abc.abstractmethod\n    def prompt_for_input(self, role: str) -> str:\n        \"\"\"Prompt for input from a role.\"\"\"\n\n    @abc.abstractmethod\n    def prompt_for_output(self, role: str):\n        \"\"\"Prompt for output from a role.\"\"\"\n\n    @abc.abstractmethod\n    def stream_output(self, output_stream):\n        \"\"\"Stream output.\"\"\"\n\n    @abc.abstractmethod\n    def print_output(self, text: str):\n        \"\"\"Print output.\"\"\"\n\n\ndef chat_loop(\n    model_path: str,\n    device: str,\n    num_gpus: int,\n    max_gpu_memory: str,\n    dtype: Optional[torch.dtype],\n    load_8bit: bool,\n    cpu_offloading: bool,\n    conv_template: Optional[str],\n    conv_system_msg: Optional[str],\n    temperature: float,\n    repetition_penalty: float,\n    max_new_tokens: int,\n    chatio: ChatIO,\n    gptq_config: Optional[GptqConfig] = None,\n    awq_config: Optional[AWQConfig] = None,\n    exllama_config: Optional[ExllamaConfig] = None,\n    xft_config: Optional[XftConfig] = None,\n    revision: str = \"main\",\n    judge_sent_end: bool = True,\n    debug: bool = True,\n    history: bool = True,\n):\n    # Model\n    model, tokenizer = load_model(\n        model_path,\n        device=device,\n        num_gpus=num_gpus,\n        max_gpu_memory=max_gpu_memory,\n        dtype=dtype,\n        load_8bit=load_8bit,\n        cpu_offloading=cpu_offloading,\n        gptq_config=gptq_config,\n        awq_config=awq_config,\n        exllama_config=exllama_config,\n        xft_config=xft_config,\n        revision=revision,\n        debug=debug,\n    )\n    generate_stream_func = get_generate_stream_function(model, model_path)\n\n    model_type = str(type(model)).lower()\n    is_t5 = \"t5\" in model_type\n    is_codet5p = \"codet5p\" in model_type\n    is_xft = \"xft\" in model_type\n\n    # Hardcode T5's default repetition penalty to be 1.2\n    if is_t5 and repetition_penalty == 1.0:\n        repetition_penalty = 1.2\n\n    # Set context length\n    context_len = get_context_length(model.config)\n\n    # Chat\n    def new_chat():\n        if conv_template:\n            conv = get_conv_template(conv_template)\n        else:\n            conv = get_conversation_template(model_path)\n        if conv_system_msg is not None:\n            conv.set_system_message(conv_system_msg)\n        return conv\n\n    def reload_conv(conv):\n        \"\"\"\n        Reprints the conversation from the start.\n        \"\"\"\n        for message in conv.messages[conv.offset :]:\n            chatio.prompt_for_output(message[0])\n            chatio.print_output(message[1])\n\n    conv = None\n\n    while True:\n        if not history or not conv:\n            conv = new_chat()\n\n        try:\n            inp = chatio.prompt_for_input(conv.roles[0])\n        except EOFError:\n            inp = \"\"\n\n        if inp == \"!!exit\" or not inp:\n            print(\"exit...\")\n            break\n        elif inp == \"!!reset\":\n            print(\"resetting...\")\n            conv = new_chat()\n            continue\n        elif inp == \"!!remove\":\n            print(\"removing last message...\")\n            if len(conv.messages) > conv.offset:\n                # Assistant\n                if conv.messages[-1][0] == conv.roles[1]:\n                    conv.messages.pop()\n                # User\n                if conv.messages[-1][0] == conv.roles[0]:\n                    conv.messages.pop()\n                reload_conv(conv)\n            else:\n                print(\"No messages to remove.\")\n            continue\n        elif inp == \"!!regen\":\n            print(\"regenerating last message...\")\n            if len(conv.messages) > conv.offset:\n                # Assistant\n                if conv.messages[-1][0] == conv.roles[1]:\n                    conv.messages.pop()\n                # User\n                if conv.messages[-1][0] == conv.roles[0]:\n                    reload_conv(conv)\n                    # Set inp to previous message\n                    inp = conv.messages.pop()[1]\n                else:\n                    # Shouldn't happen in normal circumstances\n                    print(\"No user message to regenerate from.\")\n                    continue\n            else:\n                print(\"No messages to regenerate.\")\n                continue\n        elif inp.startswith(\"!!save\"):\n            args = inp.split(\" \", 1)\n\n            if len(args) != 2:\n                print(\"usage: !!save <filename>\")\n                continue\n            else:\n                filename = args[1]\n\n            # Add .json if extension not present\n            if not \".\" in filename:\n                filename += \".json\"\n\n            print(\"saving...\", filename)\n            with open(filename, \"w\") as outfile:\n                json.dump(conv.dict(), outfile)\n            continue\n        elif inp.startswith(\"!!load\"):\n            args = inp.split(\" \", 1)\n\n            if len(args) != 2:\n                print(\"usage: !!load <filename>\")\n                continue\n            else:\n                filename = args[1]\n\n            # Check if file exists and add .json if needed\n            if not os.path.exists(filename):\n                if (not filename.endswith(\".json\")) and os.path.exists(\n                    filename + \".json\"\n                ):\n                    filename += \".json\"\n                else:\n                    print(\"file not found:\", filename)\n                    continue\n\n            print(\"loading...\", filename)\n            with open(filename, \"r\") as infile:\n                new_conv = json.load(infile)\n\n            conv = get_conv_template(new_conv[\"template_name\"])\n            conv.set_system_message(new_conv[\"system_message\"])\n            conv.messages = new_conv[\"messages\"]\n            reload_conv(conv)\n            continue\n\n        conv.append_message(conv.roles[0], inp)\n        conv.append_message(conv.roles[1], None)\n        prompt = conv.get_prompt()\n\n        if is_codet5p:  # codet5p is a code completion model.\n            prompt = inp\n\n        gen_params = {\n            \"model\": model_path,\n            \"prompt\": prompt,\n            \"temperature\": temperature,\n            \"repetition_penalty\": repetition_penalty,\n            \"max_new_tokens\": max_new_tokens,\n            \"stop\": conv.stop_str,\n            \"stop_token_ids\": conv.stop_token_ids,\n            \"echo\": False,\n        }\n\n        try:\n            chatio.prompt_for_output(conv.roles[1])\n            output_stream = generate_stream_func(\n                model,\n                tokenizer,\n                gen_params,\n                device,\n                context_len=context_len,\n                judge_sent_end=judge_sent_end,\n            )\n            t = time.time()\n            outputs = chatio.stream_output(output_stream)\n            duration = time.time() - t\n            conv.update_last_message(outputs.strip())\n\n            if debug:\n                num_tokens = len(tokenizer.encode(outputs))\n                msg = {\n                    \"conv_template\": conv.name,\n                    \"prompt\": prompt,\n                    \"outputs\": outputs,\n                    \"speed (token/s)\": round(num_tokens / duration, 2),\n                }\n                print(f\"\\n{msg}\\n\")\n\n        except KeyboardInterrupt:\n            print(\"stopped generation.\")\n            # If generation didn't finish\n            if conv.messages[-1][1] is None:\n                conv.messages.pop()\n                # Remove last user message, so there isn't a double up\n                if conv.messages[-1][0] == conv.roles[0]:\n                    conv.messages.pop()\n\n                reload_conv(conv)\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/model/upload_hub.py", "content": "\"\"\"\nUpload weights to huggingface.\n\nUsage:\npython3 -m fastchat.model.upload_hub --model-path ~/model_weights/vicuna-13b --hub-repo-id lmsys/vicuna-13b-v1.3\n\"\"\"\nimport argparse\nimport tempfile\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n\ndef upload_hub(model_path, hub_repo_id, component, private):\n    if component == \"all\":\n        components = [\"model\", \"tokenizer\"]\n    else:\n        components = [component]\n\n    kwargs = {\"push_to_hub\": True, \"repo_id\": hub_repo_id, \"private\": args.private}\n\n    if \"model\" in components:\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True\n        )\n        with tempfile.TemporaryDirectory() as tmp_path:\n            model.save_pretrained(tmp_path, **kwargs)\n\n    if \"tokenizer\" in components:\n        tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n        with tempfile.TemporaryDirectory() as tmp_path:\n            tokenizer.save_pretrained(tmp_path, **kwargs)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model-path\", type=str, required=True)\n    parser.add_argument(\"--hub-repo-id\", type=str, required=True)\n    parser.add_argument(\n        \"--component\", type=str, choices=[\"all\", \"model\", \"tokenizer\"], default=\"all\"\n    )\n    parser.add_argument(\"--private\", action=\"store_true\")\n    args = parser.parse_args()\n\n    upload_hub(args.model_path, args.hub_repo_id, args.component, args.private)\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/serve/model_worker.py", "content": "\"\"\"\nA model worker that executes the model.\n\"\"\"\nimport argparse\nimport base64\nimport gc\nimport json\nimport os\nfrom typing import List, Optional\nimport uuid\n\nimport torch\nimport torch.nn.functional as F\nfrom transformers import set_seed\nimport uvicorn\n\nfrom fastchat.constants import ErrorCode, SERVER_ERROR_MSG\nfrom fastchat.model.model_adapter import (\n    load_model,\n    add_model_args,\n    get_generate_stream_function,\n)\nfrom fastchat.modules.awq import AWQConfig\nfrom fastchat.modules.exllama import ExllamaConfig\nfrom fastchat.modules.xfastertransformer import XftConfig\nfrom fastchat.modules.gptq import GptqConfig\nfrom fastchat.serve.base_model_worker import BaseModelWorker, app\nfrom fastchat.utils import (\n    build_logger,\n    get_context_length,\n    str_to_torch_dtype,\n)\n\n\nworker_id = str(uuid.uuid4())[:8]\nlogger = build_logger(\"model_worker\", f\"model_worker_{worker_id}.log\")\n\n\nclass ModelWorker(BaseModelWorker):\n    def __init__(\n        self,\n        controller_addr: str,\n        worker_addr: str,\n        worker_id: str,\n        model_path: str,\n        model_names: List[str],\n        limit_worker_concurrency: int,\n        no_register: bool,\n        device: str,\n        num_gpus: int,\n        max_gpu_memory: str,\n        dtype: Optional[torch.dtype] = None,\n        load_8bit: bool = False,\n        cpu_offloading: bool = False,\n        gptq_config: Optional[GptqConfig] = None,\n        awq_config: Optional[AWQConfig] = None,\n        exllama_config: Optional[ExllamaConfig] = None,\n        xft_config: Optional[XftConfig] = None,\n        stream_interval: int = 2,\n        conv_template: Optional[str] = None,\n        embed_in_truncate: bool = False,\n        seed: Optional[int] = None,\n        debug: bool = False,\n        **kwargs,\n    ):\n        super().__init__(\n            controller_addr,\n            worker_addr,\n            worker_id,\n            model_path,\n            model_names,\n            limit_worker_concurrency,\n            conv_template=conv_template,\n        )\n\n        logger.info(f\"Loading the model {self.model_names} on worker {worker_id} ...\")\n        self.model, self.tokenizer = load_model(\n            model_path,\n            device=device,\n            num_gpus=num_gpus,\n            max_gpu_memory=max_gpu_memory,\n            dtype=dtype,\n            load_8bit=load_8bit,\n            cpu_offloading=cpu_offloading,\n            gptq_config=gptq_config,\n            awq_config=awq_config,\n            exllama_config=exllama_config,\n            xft_config=xft_config,\n            debug=debug,\n        )\n        self.device = device\n        if self.tokenizer.pad_token == None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.context_len = get_context_length(self.model.config)\n        self.generate_stream_func = get_generate_stream_function(self.model, model_path)\n        self.stream_interval = stream_interval\n        self.embed_in_truncate = embed_in_truncate\n        self.seed = seed\n\n        if not no_register:\n            self.init_heart_beat()\n\n    def generate_stream_gate(self, params):\n        self.call_ct += 1\n\n        try:\n            if self.seed is not None:\n                set_seed(self.seed)\n            for output in self.generate_stream_func(\n                self.model,\n                self.tokenizer,\n                params,\n                self.device,\n                self.context_len,\n                self.stream_interval,\n            ):\n                ret = {\n                    \"text\": output[\"text\"],\n                    \"error_code\": 0,\n                }\n                if \"usage\" in output:\n                    ret[\"usage\"] = output[\"usage\"]\n                if \"finish_reason\" in output:\n                    ret[\"finish_reason\"] = output[\"finish_reason\"]\n                if \"logprobs\" in output:\n                    ret[\"logprobs\"] = output[\"logprobs\"]\n                yield json.dumps(ret).encode() + b\"\\0\"\n        except torch.cuda.OutOfMemoryError as e:\n            ret = {\n                \"text\": f\"{SERVER_ERROR_MSG}\\n\\n({e})\",\n                \"error_code\": ErrorCode.CUDA_OUT_OF_MEMORY,\n            }\n            yield json.dumps(ret).encode() + b\"\\0\"\n        except (ValueError, RuntimeError) as e:\n            ret = {\n                \"text\": f\"{SERVER_ERROR_MSG}\\n\\n({e})\",\n                \"error_code\": ErrorCode.INTERNAL_ERROR,\n            }\n            yield json.dumps(ret).encode() + b\"\\0\"\n\n    def generate_gate(self, params):\n        for x in self.generate_stream_gate(params):\n            pass\n        return json.loads(x[:-1].decode())\n\n    def __process_embed_chunk(self, input_ids, attention_mask, **model_type_dict):\n        if model_type_dict.get(\"is_bert\"):\n            model_output = self.model(input_ids)\n            if model_type_dict.get(\"is_robert\"):\n                data = model_output.last_hidden_state\n            else:\n                data = model_output[0]\n        elif model_type_dict.get(\"is_t5\"):\n            model_output = self.model(input_ids, decoder_input_ids=input_ids)\n            data = model_output.encoder_last_hidden_state\n        else:\n            model_output = self.model(input_ids, output_hidden_states=True)\n            if model_type_dict.get(\"is_chatglm\"):\n                data = model_output.hidden_states[-1].transpose(0, 1)\n            else:\n                data = model_output.hidden_states[-1]\n        mask = attention_mask.unsqueeze(-1).expand(data.size()).float()\n        masked_embeddings = data * mask\n        sum_embeddings = torch.sum(masked_embeddings, dim=1)\n        token_num = torch.sum(attention_mask).item()\n\n        return sum_embeddings, token_num\n\n    def __encode_base64(self, embeddings: torch.Tensor) -> List[str]:\n        embeddings = embeddings.cpu()\n        return [\n            base64.b64encode(e.numpy().tobytes()).decode(\"utf-8\") for e in embeddings\n        ]\n\n    @torch.inference_mode()\n    def get_embeddings(self, params):\n        self.call_ct += 1\n\n        try:\n            tokenizer = self.tokenizer\n            ret = {\"embedding\": [], \"token_num\": 0}\n\n            model_type_dict = {\n                \"is_llama\": \"llama\" in str(type(self.model)),\n                \"is_t5\": \"t5\" in str(type(self.model)),\n                \"is_chatglm\": \"chatglm\" in str(type(self.model)),\n                \"is_bert\": \"bert\" in str(type(self.model)),\n                \"is_robert\": \"robert\" in str(type(self.model)),\n            }\n\n            if self.embed_in_truncate:\n                encoding = tokenizer.batch_encode_plus(\n                    params[\"input\"],\n                    padding=True,\n                    truncation=\"longest_first\",\n                    return_tensors=\"pt\",\n                    max_length=self.context_len,\n                )\n            else:\n                encoding = tokenizer.batch_encode_plus(\n                    params[\"input\"], padding=True, return_tensors=\"pt\"\n                )\n            input_ids = encoding[\"input_ids\"].to(self.device)\n            attention_mask = input_ids != tokenizer.pad_token_id\n\n            base64_encode = params.get(\"encoding_format\", None)\n\n            if self.embed_in_truncate:\n                chunk_embeddings, token_num = self.__process_embed_chunk(\n                    input_ids, attention_mask, **model_type_dict\n                )\n                embedding = chunk_embeddings / token_num\n                normalized_embeddings = F.normalize(embedding, p=2, dim=1)\n                ret[\"token_num\"] = token_num\n            else:\n                all_embeddings = []\n                all_token_num = 0\n                for i in range(0, input_ids.size(1), self.context_len):\n                    chunk_input_ids = input_ids[:, i : i + self.context_len]\n                    chunk_attention_mask = attention_mask[:, i : i + self.context_len]\n\n                    chunk_embeddings, token_num = self.__process_embed_chunk(\n                        chunk_input_ids, chunk_attention_mask, **model_type_dict\n                    )\n                    all_embeddings.append(chunk_embeddings)\n                    all_token_num += token_num\n\n                all_embeddings_tensor = torch.stack(all_embeddings)\n                embedding = torch.sum(all_embeddings_tensor, dim=0) / all_token_num\n                normalized_embeddings = F.normalize(embedding, p=2, dim=1)\n\n                ret[\"token_num\"] = all_token_num\n\n            if base64_encode == \"base64\":\n                out_embeddings = self.__encode_base64(normalized_embeddings)\n            else:\n                out_embeddings = normalized_embeddings.tolist()\n            ret[\"embedding\"] = out_embeddings\n\n            gc.collect()\n            torch.cuda.empty_cache()\n            if self.device == \"xpu\":\n                torch.xpu.empty_cache()\n            if self.device == \"npu\":\n                torch.npu.empty_cache()\n        except torch.cuda.OutOfMemoryError as e:\n            ret = {\n                \"text\": f\"{SERVER_ERROR_MSG}\\n\\n({e})\",\n                \"error_code\": ErrorCode.CUDA_OUT_OF_MEMORY,\n            }\n        except (ValueError, RuntimeError) as e:\n            ret = {\n                \"text\": f\"{SERVER_ERROR_MSG}\\n\\n({e})\",\n                \"error_code\": ErrorCode.INTERNAL_ERROR,\n            }\n        return ret\n\n\ndef create_model_worker():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--host\", type=str, default=\"localhost\")\n    parser.add_argument(\"--port\", type=int, default=21002)\n    parser.add_argument(\"--worker-address\", type=str, default=\"http://localhost:21002\")\n    parser.add_argument(\n        \"--controller-address\", type=str, default=\"http://localhost:21001\"\n    )\n    add_model_args(parser)\n    parser.add_argument(\n        \"--model-names\",\n        type=lambda s: s.split(\",\"),\n        help=\"Optional display comma separated names\",\n    )\n    parser.add_argument(\n        \"--conv-template\", type=str, default=None, help=\"Conversation prompt template.\"\n    )\n    parser.add_argument(\"--embed-in-truncate\", action=\"store_true\")\n    parser.add_argument(\n        \"--limit-worker-concurrency\",\n        type=int,\n        default=5,\n        help=\"Limit the model concurrency to prevent OOM.\",\n    )\n    parser.add_argument(\"--stream-interval\", type=int, default=2)\n    parser.add_argument(\"--no-register\", action=\"store_true\")\n    parser.add_argument(\n        \"--seed\",\n        type=int,\n        default=None,\n        help=\"Overwrite the random seed for each generation.\",\n    )\n    parser.add_argument(\n        \"--debug\", type=bool, default=False, help=\"Print debugging messages\"\n    )\n    parser.add_argument(\n        \"--ssl\",\n        action=\"store_true\",\n        required=False,\n        default=False,\n        help=\"Enable SSL. Requires OS Environment variables 'SSL_KEYFILE' and 'SSL_CERTFILE'.\",\n    )\n    args = parser.parse_args()\n    logger.info(f\"args: {args}\")\n\n    if args.gpus:\n        if len(args.gpus.split(\",\")) < args.num_gpus:\n            raise ValueError(\n                f\"Larger --num-gpus ({args.num_gpus}) than --gpus {args.gpus}!\"\n            )\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpus\n\n    gptq_config = GptqConfig(\n        ckpt=args.gptq_ckpt or args.model_path,\n        wbits=args.gptq_wbits,\n        groupsize=args.gptq_groupsize,\n        act_order=args.gptq_act_order,\n    )\n    awq_config = AWQConfig(\n        ckpt=args.awq_ckpt or args.model_path,\n        wbits=args.awq_wbits,\n        groupsize=args.awq_groupsize,\n    )\n    if args.enable_exllama:\n        exllama_config = ExllamaConfig(\n            max_seq_len=args.exllama_max_seq_len,\n            gpu_split=args.exllama_gpu_split,\n        )\n    else:\n        exllama_config = None\n    if args.enable_xft:\n        xft_config = XftConfig(\n            max_seq_len=args.xft_max_seq_len,\n            data_type=args.xft_dtype,\n        )\n        if args.device != \"cpu\":\n            print(\"xFasterTransformer now is only support CPUs. Reset device to CPU\")\n            args.device = \"cpu\"\n    else:\n        xft_config = None\n\n    worker = ModelWorker(\n        args.controller_address,\n        args.worker_address,\n        worker_id,\n        args.model_path,\n        args.model_names,\n        args.limit_worker_concurrency,\n        no_register=args.no_register,\n        device=args.device,\n        num_gpus=args.num_gpus,\n        max_gpu_memory=args.max_gpu_memory,\n        dtype=str_to_torch_dtype(args.dtype),\n        load_8bit=args.load_8bit,\n        cpu_offloading=args.cpu_offloading,\n        gptq_config=gptq_config,\n        awq_config=awq_config,\n        exllama_config=exllama_config,\n        xft_config=xft_config,\n        stream_interval=args.stream_interval,\n        conv_template=args.conv_template,\n        embed_in_truncate=args.embed_in_truncate,\n        seed=args.seed,\n        debug=args.debug,\n    )\n    return args, worker\n\n\nif __name__ == \"__main__\":\n    args, worker = create_model_worker()\n    if args.ssl:\n        uvicorn.run(\n            app,\n            host=args.host,\n            port=args.port,\n            log_level=\"info\",\n            ssl_keyfile=os.environ[\"SSL_KEYFILE\"],\n            ssl_certfile=os.environ[\"SSL_CERTFILE\"],\n        )\n    else:\n        uvicorn.run(app, host=args.host, port=args.port, log_level=\"info\")\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/serve/gradio_web_server.py", "content": "\"\"\"\nThe gradio demo server for chatting with a single model.\n\"\"\"\n\nimport argparse\nfrom collections import defaultdict\nimport datetime\nimport json\nimport os\nimport random\nimport time\nimport uuid\n\nimport gradio as gr\nimport requests\n\nfrom fastchat.conversation import SeparatorStyle\nfrom fastchat.constants import (\n    LOGDIR,\n    WORKER_API_TIMEOUT,\n    ErrorCode,\n    MODERATION_MSG,\n    CONVERSATION_LIMIT_MSG,\n    SERVER_ERROR_MSG,\n    INPUT_CHAR_LEN_LIMIT,\n    CONVERSATION_TURN_LIMIT,\n    SESSION_EXPIRATION_TIME,\n)\nfrom fastchat.model.model_adapter import get_conversation_template\nfrom fastchat.model.model_registry import get_model_info, model_info\nfrom fastchat.serve.api_provider import (\n    anthropic_api_stream_iter,\n    openai_api_stream_iter,\n    palm_api_stream_iter,\n    init_palm_chat,\n)\nfrom fastchat.utils import (\n    build_logger,\n    moderation_filter,\n    get_window_url_params_js,\n    get_window_url_params_with_tos_js,\n    parse_gradio_auth_creds,\n)\n\n\nlogger = build_logger(\"gradio_web_server\", \"gradio_web_server.log\")\n\nheaders = {\"User-Agent\": \"FastChat Client\"}\n\nno_change_btn = gr.Button.update()\nenable_btn = gr.Button.update(interactive=True, visible=True)\ndisable_btn = gr.Button.update(interactive=False)\ninvisible_btn = gr.Button.update(interactive=False, visible=False)\n\ncontroller_url = None\nenable_moderation = False\n\nacknowledgment_md = \"\"\"\n### Acknowledgment\n<div class=\"image-container\">\n    <p> We thank <a href=\"https://www.kaggle.com/\" target=\"_blank\">Kaggle</a>, <a href=\"https://mbzuai.ac.ae/\" target=\"_blank\">MBZUAI</a>, <a href=\"https://www.anyscale.com/\" target=\"_blank\">AnyScale</a>, and <a href=\"https://huggingface.co/\" target=\"_blank\">HuggingFace</a> for their <a href=\"https://lmsys.org/donations/\" target=\"_blank\">sponsorship</a>. </p>\n    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Kaggle_logo.png/400px-Kaggle_logo.png\" alt=\"Image 1\">\n    <img src=\"https://mma.prnewswire.com/media/1227419/MBZUAI_Logo.jpg?p=facebookg\" alt=\"Image 2\">\n    <img src=\"https://docs.anyscale.com/site-assets/logo.png\" alt=\"Image 3\">\n    <img src=\"https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo-with-title.png\" alt=\"Image 4\">\n</div>\n\"\"\"\n\nip_expiration_dict = defaultdict(lambda: 0)\n\n# Information about custom OpenAI compatible API models.\n# JSON file format:\n# {\n#     \"vicuna-7b\": {\n#         \"model_name\": \"vicuna-7b-v1.5\",\n#         \"api_base\": \"http://8.8.8.55:5555/v1\",\n#         \"api_key\": \"password\"\n#     },\n# }\nopenai_compatible_models_info = {}\n\n\nclass State:\n    def __init__(self, model_name):\n        self.conv = get_conversation_template(model_name)\n        self.conv_id = uuid.uuid4().hex\n        self.skip_next = False\n        self.model_name = model_name\n\n        if model_name == \"palm-2\":\n            # According to release note, \"chat-bison@001\" is PaLM 2 for chat.\n            # https://cloud.google.com/vertex-ai/docs/release-notes#May_10_2023\n            self.palm_chat = init_palm_chat(\"chat-bison@001\")\n\n    def to_gradio_chatbot(self):\n        return self.conv.to_gradio_chatbot()\n\n    def dict(self):\n        base = self.conv.dict()\n        base.update(\n            {\n                \"conv_id\": self.conv_id,\n                \"model_name\": self.model_name,\n            }\n        )\n        return base\n\n\ndef set_global_vars(controller_url_, enable_moderation_):\n    global controller_url, enable_moderation\n    controller_url = controller_url_\n    enable_moderation = enable_moderation_\n\n\ndef get_conv_log_filename():\n    t = datetime.datetime.now()\n    name = os.path.join(LOGDIR, f\"{t.year}-{t.month:02d}-{t.day:02d}-conv.json\")\n    return name\n\n\ndef get_model_list(\n    controller_url, register_openai_compatible_models, add_chatgpt, add_claude, add_palm\n):\n    if controller_url:\n        ret = requests.post(controller_url + \"/refresh_all_workers\")\n        assert ret.status_code == 200\n        ret = requests.post(controller_url + \"/list_models\")\n        models = ret.json()[\"models\"]\n    else:\n        models = []\n\n    # Add API providers\n    if register_openai_compatible_models:\n        global openai_compatible_models_info\n        openai_compatible_models_info = json.load(\n            open(register_openai_compatible_models)\n        )\n        models += list(openai_compatible_models_info.keys())\n\n    if add_chatgpt:\n        models += [\"gpt-3.5-turbo\", \"gpt-4\", \"gpt-4-turbo\", \"gpt-3.5-turbo-1106\"]\n    if add_claude:\n        models += [\"claude-2\", \"claude-instant-1\"]\n    if add_palm:\n        models += [\"palm-2\"]\n    models = list(set(models))\n\n    if \"deluxe-chat-v1\" in models:\n        del models[models.index(\"deluxe-chat-v1\")]\n    if \"deluxe-chat-v1.1\" in models:\n        del models[models.index(\"deluxe-chat-v1.1\")]\n\n    priority = {k: f\"___{i:02d}\" for i, k in enumerate(model_info)}\n    models.sort(key=lambda x: priority.get(x, x))\n    logger.info(f\"Models: {models}\")\n    return models\n\n\ndef load_demo_single(models, url_params):\n    selected_model = models[0] if len(models) > 0 else \"\"\n    if \"model\" in url_params:\n        model = url_params[\"model\"]\n        if model in models:\n            selected_model = model\n\n    dropdown_update = gr.Dropdown.update(\n        choices=models, value=selected_model, visible=True\n    )\n\n    state = None\n    return state, dropdown_update\n\n\ndef load_demo(url_params, request: gr.Request):\n    global models\n\n    ip = get_ip(request)\n    logger.info(f\"load_demo. ip: {ip}. params: {url_params}\")\n    ip_expiration_dict[ip] = time.time() + SESSION_EXPIRATION_TIME\n\n    if args.model_list_mode == \"reload\":\n        models = get_model_list(\n            controller_url,\n            args.register_openai_compatible_models,\n            args.add_chatgpt,\n            args.add_claude,\n            args.add_palm,\n        )\n\n    return load_demo_single(models, url_params)\n\n\ndef vote_last_response(state, vote_type, model_selector, request: gr.Request):\n    with open(get_conv_log_filename(), \"a\") as fout:\n        data = {\n            \"tstamp\": round(time.time(), 4),\n            \"type\": vote_type,\n            \"model\": model_selector,\n            \"state\": state.dict(),\n            \"ip\": get_ip(request),\n        }\n        fout.write(json.dumps(data) + \"\\n\")\n\n\ndef upvote_last_response(state, model_selector, request: gr.Request):\n    ip = get_ip(request)\n    logger.info(f\"upvote. ip: {ip}\")\n    vote_last_response(state, \"upvote\", model_selector, request)\n    return (\"\",) + (disable_btn,) * 3\n\n\ndef downvote_last_response(state, model_selector, request: gr.Request):\n    ip = get_ip(request)\n    logger.info(f\"downvote. ip: {ip}\")\n    vote_last_response(state, \"downvote\", model_selector, request)\n    return (\"\",) + (disable_btn,) * 3\n\n\ndef flag_last_response(state, model_selector, request: gr.Request):\n    ip = get_ip(request)\n    logger.info(f\"flag. ip: {ip}\")\n    vote_last_response(state, \"flag\", model_selector, request)\n    return (\"\",) + (disable_btn,) * 3\n\n\ndef regenerate(state, request: gr.Request):\n    ip = get_ip(request)\n    logger.info(f\"regenerate. ip: {ip}\")\n    state.conv.update_last_message(None)\n    return (state, state.to_gradio_chatbot(), \"\") + (disable_btn,) * 5\n\n\ndef clear_history(request: gr.Request):\n    ip = get_ip(request)\n    logger.info(f\"clear_history. ip: {ip}\")\n    state = None\n    return (state, [], \"\") + (disable_btn,) * 5\n\n\ndef get_ip(request: gr.Request):\n    if \"cf-connecting-ip\" in request.headers:\n        ip = request.headers[\"cf-connecting-ip\"]\n    else:\n        ip = request.client.host\n    return ip\n\n\ndef add_text(state, model_selector, text, request: gr.Request):\n    ip = get_ip(request)\n    logger.info(f\"add_text. ip: {ip}. len: {len(text)}\")\n\n    if state is None:\n        state = State(model_selector)\n\n    if len(text) <= 0:\n        state.skip_next = True\n        return (state, state.to_gradio_chatbot(), \"\") + (no_change_btn,) * 5\n\n    flagged = moderation_filter(text, [state.model_name])\n    if flagged:\n        logger.info(f\"violate moderation. ip: {ip}. text: {text}\")\n        # overwrite the original text\n        text = MODERATION_MSG\n\n    conv = state.conv\n    if (len(conv.messages) - conv.offset) // 2 >= CONVERSATION_TURN_LIMIT:\n        logger.info(f\"conversation turn limit. ip: {ip}. text: {text}\")\n        state.skip_next = True\n        return (state, state.to_gradio_chatbot(), CONVERSATION_LIMIT_MSG) + (\n            no_change_btn,\n        ) * 5\n\n    text = text[:INPUT_CHAR_LEN_LIMIT]  # Hard cut-off\n    conv.append_message(conv.roles[0], text)\n    conv.append_message(conv.roles[1], None)\n    return (state, state.to_gradio_chatbot(), \"\") + (disable_btn,) * 5\n\n\ndef post_process_code(code):\n    sep = \"\\n```\"\n    if sep in code:\n        blocks = code.split(sep)\n        if len(blocks) % 2 == 1:\n            for i in range(1, len(blocks), 2):\n                blocks[i] = blocks[i].replace(\"\\\\_\", \"_\")\n        code = sep.join(blocks)\n    return code\n\n\ndef model_worker_stream_iter(\n    conv,\n    model_name,\n    worker_addr,\n    prompt,\n    temperature,\n    repetition_penalty,\n    top_p,\n    max_new_tokens,\n):\n    # Make requests\n    gen_params = {\n        \"model\": model_name,\n        \"prompt\": prompt,\n        \"temperature\": temperature,\n        \"repetition_penalty\": repetition_penalty,\n        \"top_p\": top_p,\n        \"max_new_tokens\": max_new_tokens,\n        \"stop\": conv.stop_str,\n        \"stop_token_ids\": conv.stop_token_ids,\n        \"echo\": False,\n    }\n    logger.info(f\"==== request ====\\n{gen_params}\")\n\n    # Stream output\n    response = requests.post(\n        worker_addr + \"/worker_generate_stream\",\n        headers=headers,\n        json=gen_params,\n        stream=True,\n        timeout=WORKER_API_TIMEOUT,\n    )\n    for chunk in response.iter_lines(decode_unicode=False, delimiter=b\"\\0\"):\n        if chunk:\n            data = json.loads(chunk.decode())\n            yield data\n\n\ndef bot_response(state, temperature, top_p, max_new_tokens, request: gr.Request):\n    ip = get_ip(request)\n    logger.info(f\"bot_response. ip: {ip}\")\n    start_tstamp = time.time()\n    temperature = float(temperature)\n    top_p = float(top_p)\n    max_new_tokens = int(max_new_tokens)\n\n    if state.skip_next:\n        # This generate call is skipped due to invalid inputs\n        state.skip_next = False\n        yield (state, state.to_gradio_chatbot()) + (no_change_btn,) * 5\n        return\n\n    conv, model_name = state.conv, state.model_name\n    if model_name in [\"gpt-3.5-turbo\", \"gpt-4\", \"gpt-4-turbo\", \"gpt-3.5-turbo-1106\"]:\n        prompt = conv.to_openai_api_messages()\n        stream_iter = openai_api_stream_iter(\n            model_name, prompt, temperature, top_p, max_new_tokens\n        )\n    elif model_name in [\"claude-2\", \"claude-1\", \"claude-instant-1\"]:\n        prompt = conv.get_prompt()\n        stream_iter = anthropic_api_stream_iter(\n            model_name, prompt, temperature, top_p, max_new_tokens\n        )\n    elif model_name == \"palm-2\":\n        stream_iter = palm_api_stream_iter(\n            state.palm_chat, conv.messages[-2][1], temperature, top_p, max_new_tokens\n        )\n    elif model_name in openai_compatible_models_info:\n        model_info = openai_compatible_models_info[model_name]\n        prompt = conv.to_openai_api_messages()\n        stream_iter = openai_api_stream_iter(\n            model_info[\"model_name\"],\n            prompt,\n            temperature,\n            top_p,\n            max_new_tokens,\n            api_base=model_info[\"api_base\"],\n            api_key=model_info[\"api_key\"],\n        )\n    else:\n        # Query worker address\n        ret = requests.post(\n            controller_url + \"/get_worker_address\", json={\"model\": model_name}\n        )\n        worker_addr = ret.json()[\"address\"]\n        logger.info(f\"model_name: {model_name}, worker_addr: {worker_addr}\")\n\n        # No available worker\n        if worker_addr == \"\":\n            conv.update_last_message(SERVER_ERROR_MSG)\n            yield (\n                state,\n                state.to_gradio_chatbot(),\n                disable_btn,\n                disable_btn,\n                disable_btn,\n                enable_btn,\n                enable_btn,\n            )\n            return\n\n        # Construct prompt.\n        # We need to call it here, so it will not be affected by \"▌\".\n        prompt = conv.get_prompt()\n\n        # Set repetition_penalty\n        if \"t5\" in model_name:\n            repetition_penalty = 1.2\n        else:\n            repetition_penalty = 1.0\n\n        stream_iter = model_worker_stream_iter(\n            conv,\n            model_name,\n            worker_addr,\n            prompt,\n            temperature,\n            repetition_penalty,\n            top_p,\n            max_new_tokens,\n        )\n\n    conv.update_last_message(\"▌\")\n    yield (state, state.to_gradio_chatbot()) + (disable_btn,) * 5\n\n    try:\n        for i, data in enumerate(stream_iter):\n            if data[\"error_code\"] == 0:\n                output = data[\"text\"].strip()\n                conv.update_last_message(output + \"▌\")\n                yield (state, state.to_gradio_chatbot()) + (disable_btn,) * 5\n            else:\n                output = data[\"text\"] + f\"\\n\\n(error_code: {data['error_code']})\"\n                conv.update_last_message(output)\n                yield (state, state.to_gradio_chatbot()) + (\n                    disable_btn,\n                    disable_btn,\n                    disable_btn,\n                    enable_btn,\n                    enable_btn,\n                )\n                return\n        output = data[\"text\"].strip()\n        if \"vicuna\" in model_name:\n            output = post_process_code(output)\n        conv.update_last_message(output)\n        yield (state, state.to_gradio_chatbot()) + (enable_btn,) * 5\n    except requests.exceptions.RequestException as e:\n        conv.update_last_message(\n            f\"{SERVER_ERROR_MSG}\\n\\n\"\n            f\"(error_code: {ErrorCode.GRADIO_REQUEST_ERROR}, {e})\"\n        )\n        yield (state, state.to_gradio_chatbot()) + (\n            disable_btn,\n            disable_btn,\n            disable_btn,\n            enable_btn,\n            enable_btn,\n        )\n        return\n    except Exception as e:\n        conv.update_last_message(\n            f\"{SERVER_ERROR_MSG}\\n\\n\"\n            f\"(error_code: {ErrorCode.GRADIO_STREAM_UNKNOWN_ERROR}, {e})\"\n        )\n        yield (state, state.to_gradio_chatbot()) + (\n            disable_btn,\n            disable_btn,\n            disable_btn,\n            enable_btn,\n            enable_btn,\n        )\n        return\n\n    finish_tstamp = time.time()\n    logger.info(f\"{output}\")\n\n    with open(get_conv_log_filename(), \"a\") as fout:\n        data = {\n            \"tstamp\": round(finish_tstamp, 4),\n            \"type\": \"chat\",\n            \"model\": model_name,\n            \"gen_params\": {\n                \"temperature\": temperature,\n                \"top_p\": top_p,\n                \"max_new_tokens\": max_new_tokens,\n            },\n            \"start\": round(start_tstamp, 4),\n            \"finish\": round(finish_tstamp, 4),\n            \"state\": state.dict(),\n            \"ip\": get_ip(request),\n        }\n        fout.write(json.dumps(data) + \"\\n\")\n\n\nblock_css = \"\"\"\n#notice_markdown {\n    font-size: 110%\n}\n#notice_markdown th {\n    display: none;\n}\n#notice_markdown td {\n    padding-top: 6px;\n    padding-bottom: 6px;\n}\n#leaderboard_markdown {\n    font-size: 110%\n}\n#leaderboard_markdown td {\n    padding-top: 6px;\n    padding-bottom: 6px;\n}\n#leaderboard_dataframe td {\n    line-height: 0.1em;\n}\n#about_markdown {\n    font-size: 110%\n}\n#input_box textarea {\n}\nfooter {\n    display:none !important\n}\n.image-container {\n    display: flex;\n    align-items: center;\n    padding: 1px;\n}\n.image-container img {\n    margin: 0 30px;\n    height: 20px;\n    max-height: 100%;\n    width: auto;\n    max-width: 20%;\n}\n.image-about img {\n    margin: 0 30px;\n    margin-top:  30px;\n    height: 60px;\n    max-height: 100%;\n    width: auto;\n    float: left;\n}\n\"\"\"\n\n\ndef get_model_description_md(models):\n    model_description_md = \"\"\"\n| | | |\n| ---- | ---- | ---- |\n\"\"\"\n    ct = 0\n    visited = set()\n    for i, name in enumerate(models):\n        minfo = get_model_info(name)\n        if minfo.simple_name in visited:\n            continue\n        visited.add(minfo.simple_name)\n        one_model_md = f\"[{minfo.simple_name}]({minfo.link}): {minfo.description}\"\n\n        if ct % 3 == 0:\n            model_description_md += \"|\"\n        model_description_md += f\" {one_model_md} |\"\n        if ct % 3 == 2:\n            model_description_md += \"\\n\"\n        ct += 1\n    return model_description_md\n\n\ndef build_about():\n    about_markdown = f\"\"\"\n# About Us\nChatbot Arena is an open-source research project developed by members from [LMSYS](https://lmsys.org/about/) and UC Berkeley [SkyLab](https://sky.cs.berkeley.edu/).  Our mission is to build an open crowdsourced platform to collect human feedback and evaluate LLMs under real-world scenarios. We open-source our code at [GitHub](https://github.com/lm-sys/FastChat) and release chat and human feedback datasets [here](https://github.com/lm-sys/FastChat/blob/main/docs/dataset_release.md). We invite everyone to join us in this journey!\n\n## Read More\n- Chatbot Arena [launch post](https://lmsys.org/blog/2023-05-03-arena/), [data release](https://lmsys.org/blog/2023-07-20-dataset/)\n- LMSYS-Chat-1M [report](https://arxiv.org/abs/2309.11998)\n\n## Core Members\n[Lianmin Zheng](https://lmzheng.net/), [Wei-Lin Chiang](https://infwinston.github.io/), [Ying Sheng](https://sites.google.com/view/yingsheng/home), [Siyuan Zhuang](https://scholar.google.com/citations?user=KSZmI5EAAAAJ)\n\n## Advisors\n[Ion Stoica](http://people.eecs.berkeley.edu/~istoica/), [Joseph E. Gonzalez](https://people.eecs.berkeley.edu/~jegonzal/), [Hao Zhang](https://cseweb.ucsd.edu/~haozhang/)\n\n## Contact Us\n- Follow our [Twitter](https://twitter.com/lmsysorg), [Discord](https://discord.gg/HSWAKCrnFx) or email us at lmsys.org@gmail.com\n- File issues on [GitHub](https://github.com/lm-sys/FastChat)\n- Download our datasets and models on [HuggingFace](https://huggingface.co/lmsys)\n\n## Sponsors\nWe thank [Kaggle](https://www.kaggle.com/), [MBZUAI](https://mbzuai.ac.ae/), [Anyscale](https://www.anyscale.com/), [HuggingFace](https://huggingface.co/) for their generous sponsorship.\nLearn more about partnership [here](https://lmsys.org/donations/).\n\n<div class=\"image-about\">\n    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Kaggle_logo.png/400px-Kaggle_logo.png\" alt=\"Image 1\">\n    <img src=\"https://upload.wikimedia.org/wikipedia/en/5/55/Mohamed_bin_Zayed_University_of_Artificial_Intelligence_logo.png\" alt=\"Image 2\">\n    <img src=\"https://docs.anyscale.com/site-assets/logo.png\" alt=\"Image 3\">\n    <img src=\"https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png\" alt=\"Image 4\">\n</div>\n\"\"\"\n\n    # state = gr.State()\n    gr.Markdown(about_markdown, elem_id=\"about_markdown\")\n\n    # return [state]\n\n\ndef build_single_model_ui(models, add_promotion_links=False):\n    promotion = (\n        \"\"\"\n- | [GitHub](https://github.com/lm-sys/FastChat) | [Dataset](https://github.com/lm-sys/FastChat/blob/main/docs/dataset_release.md) | [Twitter](https://twitter.com/lmsysorg) | [Discord](https://discord.gg/HSWAKCrnFx) |\n- Introducing Llama 2: The Next Generation Open Source Large Language Model. [[Website]](https://ai.meta.com/llama/)\n- Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality. [[Blog]](https://lmsys.org/blog/2023-03-30-vicuna/)\n\"\"\"\n        if add_promotion_links\n        else \"\"\n    )\n\n    notice_markdown = f\"\"\"\n# 🏔️ Chat with Open Large Language Models\n{promotion}\n\n## 👉 Choose any model to chat\n\"\"\"\n\n    state = gr.State()\n    model_description_md = get_model_description_md(models)\n    gr.Markdown(notice_markdown + model_description_md, elem_id=\"notice_markdown\")\n\n    with gr.Row(elem_id=\"model_selector_row\"):\n        model_selector = gr.Dropdown(\n            choices=models,\n            value=models[0] if len(models) > 0 else \"\",\n            interactive=True,\n            show_label=False,\n            container=False,\n        )\n\n    chatbot = gr.Chatbot(\n        elem_id=\"chatbot\",\n        label=\"Scroll down and start chatting\",\n        height=550,\n    )\n    with gr.Row():\n        with gr.Column(scale=20):\n            textbox = gr.Textbox(\n                show_label=False,\n                placeholder=\"Enter your prompt here and press ENTER\",\n                container=False,\n                elem_id=\"input_box\",\n            )\n        with gr.Column(scale=1, min_width=50):\n            send_btn = gr.Button(value=\"Send\", variant=\"primary\")\n\n    with gr.Row() as button_row:\n        upvote_btn = gr.Button(value=\"👍  Upvote\", interactive=False)\n        downvote_btn = gr.Button(value=\"👎  Downvote\", interactive=False)\n        flag_btn = gr.Button(value=\"⚠️  Flag\", interactive=False)\n        regenerate_btn = gr.Button(value=\"🔄  Regenerate\", interactive=False)\n        clear_btn = gr.Button(value=\"🗑️  Clear history\", interactive=False)\n\n    with gr.Accordion(\"Parameters\", open=False) as parameter_row:\n        temperature = gr.Slider(\n            minimum=0.0,\n            maximum=1.0,\n            value=0.7,\n            step=0.1,\n            interactive=True,\n            label=\"Temperature\",\n        )\n        top_p = gr.Slider(\n            minimum=0.0,\n            maximum=1.0,\n            value=1.0,\n            step=0.1,\n            interactive=True,\n            label=\"Top P\",\n        )\n        max_output_tokens = gr.Slider(\n            minimum=16,\n            maximum=1024,\n            value=512,\n            step=64,\n            interactive=True,\n            label=\"Max output tokens\",\n        )\n\n    if add_promotion_links:\n        gr.Markdown(acknowledgment_md)\n\n    # Register listeners\n    btn_list = [upvote_btn, downvote_btn, flag_btn, regenerate_btn, clear_btn]\n    upvote_btn.click(\n        upvote_last_response,\n        [state, model_selector],\n        [textbox, upvote_btn, downvote_btn, flag_btn],\n    )\n    downvote_btn.click(\n        downvote_last_response,\n        [state, model_selector],\n        [textbox, upvote_btn, downvote_btn, flag_btn],\n    )\n    flag_btn.click(\n        flag_last_response,\n        [state, model_selector],\n        [textbox, upvote_btn, downvote_btn, flag_btn],\n    )\n    regenerate_btn.click(regenerate, state, [state, chatbot, textbox] + btn_list).then(\n        bot_response,\n        [state, temperature, top_p, max_output_tokens],\n        [state, chatbot] + btn_list,\n    )\n    clear_btn.click(clear_history, None, [state, chatbot, textbox] + btn_list)\n\n    model_selector.change(clear_history, None, [state, chatbot, textbox] + btn_list)\n\n    textbox.submit(\n        add_text, [state, model_selector, textbox], [state, chatbot, textbox] + btn_list\n    ).then(\n        bot_response,\n        [state, temperature, top_p, max_output_tokens],\n        [state, chatbot] + btn_list,\n    )\n    send_btn.click(\n        add_text,\n        [state, model_selector, textbox],\n        [state, chatbot, textbox] + btn_list,\n    ).then(\n        bot_response,\n        [state, temperature, top_p, max_output_tokens],\n        [state, chatbot] + btn_list,\n    )\n\n    return [state, model_selector]\n\n\ndef build_demo(models):\n    with gr.Blocks(\n        title=\"Chat with Open Large Language Models\",\n        theme=gr.themes.Default(),\n        css=block_css,\n    ) as demo:\n        url_params = gr.JSON(visible=False)\n\n        state, model_selector = build_single_model_ui(models)\n\n        if args.model_list_mode not in [\"once\", \"reload\"]:\n            raise ValueError(f\"Unknown model list mode: {args.model_list_mode}\")\n\n        if args.show_terms_of_use:\n            load_js = get_window_url_params_with_tos_js\n        else:\n            load_js = get_window_url_params_js\n\n        demo.load(\n            load_demo,\n            [url_params],\n            [\n                state,\n                model_selector,\n            ],\n            _js=load_js,\n        )\n\n    return demo\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\")\n    parser.add_argument(\"--port\", type=int)\n    parser.add_argument(\n        \"--share\",\n        action=\"store_true\",\n        help=\"Whether to generate a public, shareable link\",\n    )\n    parser.add_argument(\n        \"--controller-url\",\n        type=str,\n        default=\"http://localhost:21001\",\n        help=\"The address of the controller\",\n    )\n    parser.add_argument(\n        \"--concurrency-count\",\n        type=int,\n        default=10,\n        help=\"The concurrency count of the gradio queue\",\n    )\n    parser.add_argument(\n        \"--model-list-mode\",\n        type=str,\n        default=\"once\",\n        choices=[\"once\", \"reload\"],\n        help=\"Whether to load the model list once or reload the model list every time\",\n    )\n    parser.add_argument(\n        \"--moderate\",\n        action=\"store_true\",\n        help=\"Enable content moderation to block unsafe inputs\",\n    )\n    parser.add_argument(\n        \"--show-terms-of-use\",\n        action=\"store_true\",\n        help=\"Shows term of use before loading the demo\",\n    )\n    parser.add_argument(\n        \"--add-chatgpt\",\n        action=\"store_true\",\n        help=\"Add OpenAI's ChatGPT models (gpt-3.5-turbo, gpt-4)\",\n    )\n    parser.add_argument(\n        \"--add-claude\",\n        action=\"store_true\",\n        help=\"Add Anthropic's Claude models (claude-2, claude-instant-1)\",\n    )\n    parser.add_argument(\n        \"--add-palm\",\n        action=\"store_true\",\n        help=\"Add Google's PaLM model (PaLM 2 for Chat: chat-bison@001)\",\n    )\n    parser.add_argument(\n        \"--register-openai-compatible-models\",\n        type=str,\n        help=\"Register custom OpenAI API compatible models by loading them from a JSON file\",\n    )\n    parser.add_argument(\n        \"--gradio-auth-path\",\n        type=str,\n        help='Set the gradio authentication file path. The file should contain one or more user:password pairs in this format: \"u1:p1,u2:p2,u3:p3\"',\n    )\n    args = parser.parse_args()\n    logger.info(f\"args: {args}\")\n\n    # Set global variables\n    set_global_vars(args.controller_url, args.moderate)\n    models = get_model_list(\n        args.controller_url,\n        args.register_openai_compatible_models,\n        args.add_chatgpt,\n        args.add_claude,\n        args.add_palm,\n    )\n\n    # Set authorization credentials\n    auth = None\n    if args.gradio_auth_path is not None:\n        auth = parse_gradio_auth_creds(args.gradio_auth_path)\n\n    # Launch the demo\n    demo = build_demo(models)\n    demo.queue(\n        concurrency_count=args.concurrency_count, status_update_rate=10, api_open=False\n    ).launch(\n        server_name=args.host,\n        server_port=args.port,\n        share=args.share,\n        max_threads=200,\n        auth=auth,\n    )\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/llm_judge/clean_judgment.py", "content": "\"\"\"\nClean model judgment files.\n\"\"\"\nimport argparse\nimport json\n\nselected_models = [\n    \"alpaca-13b\",\n    \"baize-v2-13b\",\n    \"chatglm-6b\",\n    \"claude-instant-v1\",\n    \"claude-v1\",\n    \"dolly-v2-12b\",\n    \"falcon-40b-instruct\",\n    \"fastchat-t5-3b\",\n    \"gpt-3.5-turbo\",\n    \"gpt-4\",\n    \"gpt4all-13b-snoozy\",\n    \"guanaco-33b\",\n    \"guanaco-65b\",\n    \"h2ogpt-oasst-open-llama-13b\",\n    \"koala-13b\",\n    \"llama-13b\",\n    \"mpt-30b-chat\",\n    \"mpt-30b-instruct\",\n    \"mpt-7b-chat\",\n    \"nous-hermes-13b\",\n    \"oasst-sft-4-pythia-12b\",\n    \"oasst-sft-7-llama-30b\",\n    \"palm-2-chat-bison-001\",\n    \"rwkv-4-raven-14b\",\n    \"stablelm-tuned-alpha-7b\",\n    \"tulu-30b\",\n    \"vicuna-13b-v1.3\",\n    \"vicuna-33b-v1.3\",\n    \"vicuna-7b-v1.3\",\n    \"wizardlm-13b\",\n    \"wizardlm-30b\",\n]\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--infile\", type=str)\n    args = parser.parse_args()\n\n    infile = args.infile\n    outfile = infile.replace(\".jsonl\", \"_clean.jsonl\")\n\n    raw_lines = open(infile).readlines()\n    rets = []\n    models = set()\n    visited = set()\n    for line in raw_lines:\n        obj = json.loads(line)\n\n        if \"model_1\" in obj:  # pair\n            model = obj[\"model_1\"]\n            key = (\n                obj[\"model_1\"],\n                obj[\"model_2\"],\n                obj[\"question_id\"],\n                tuple(obj[\"judge\"]),\n            )\n        else:  # single\n            model = obj[\"model\"]\n            key = (obj[\"model\"], obj[\"question_id\"], tuple(obj[\"judge\"]))\n\n        if key in visited:\n            continue\n        visited.add(key)\n\n        if model not in selected_models:\n            continue\n        models.add(model)\n        rets.append(obj)\n\n    models = sorted(list(models))\n    missing_models = [x for x in selected_models if x not in models]\n    print(f\"in models: {models}, number: {len(models)}\")\n    print(f\"missing models: {missing_models}\")\n    print(f\"#in: {len(raw_lines)}, #out: {len(rets)}\")\n    rets.sort(\n        key=lambda x: (\n            x[\"model\"] if \"model\" in x else x[\"model_1\"],\n            x[\"question_id\"],\n            x[\"turn\"],\n        )\n    )\n\n    with open(outfile, \"w\") as fout:\n        for x in rets:\n            fout.write(json.dumps(x) + \"\\n\")\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/serve/huggingface_api.py", "content": "\"\"\"\nUse FastChat with Hugging Face generation APIs.\n\nUsage:\npython3 -m fastchat.serve.huggingface_api --model lmsys/vicuna-7b-v1.5\npython3 -m fastchat.serve.huggingface_api --model lmsys/fastchat-t5-3b-v1.0\n\"\"\"\nimport argparse\n\nimport torch\n\nfrom fastchat.model import load_model, get_conversation_template, add_model_args\n\n\n@torch.inference_mode()\ndef main(args):\n    # Load model\n    model, tokenizer = load_model(\n        args.model_path,\n        device=args.device,\n        num_gpus=args.num_gpus,\n        max_gpu_memory=args.max_gpu_memory,\n        load_8bit=args.load_8bit,\n        cpu_offloading=args.cpu_offloading,\n        revision=args.revision,\n        debug=args.debug,\n    )\n\n    # Build the prompt with a conversation template\n    msg = args.message\n    conv = get_conversation_template(args.model_path)\n    conv.append_message(conv.roles[0], msg)\n    conv.append_message(conv.roles[1], None)\n    prompt = conv.get_prompt()\n\n    # Run inference\n    inputs = tokenizer([prompt], return_tensors=\"pt\").to(args.device)\n    output_ids = model.generate(\n        **inputs,\n        do_sample=True if args.temperature > 1e-5 else False,\n        temperature=args.temperature,\n        repetition_penalty=args.repetition_penalty,\n        max_new_tokens=args.max_new_tokens,\n    )\n\n    if model.config.is_encoder_decoder:\n        output_ids = output_ids[0]\n    else:\n        output_ids = output_ids[0][len(inputs[\"input_ids\"][0]) :]\n    outputs = tokenizer.decode(\n        output_ids, skip_special_tokens=True, spaces_between_special_tokens=False\n    )\n\n    # Print results\n    print(f\"{conv.roles[0]}: {msg}\")\n    print(f\"{conv.roles[1]}: {outputs}\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    add_model_args(parser)\n    parser.add_argument(\"--temperature\", type=float, default=0.7)\n    parser.add_argument(\"--repetition_penalty\", type=float, default=1.0)\n    parser.add_argument(\"--max-new-tokens\", type=int, default=512)\n    parser.add_argument(\"--debug\", action=\"store_true\")\n    parser.add_argument(\"--message\", type=str, default=\"Hello! Who are you?\")\n    args = parser.parse_args()\n\n    # Reset default repetition penalty for T5 models.\n    if \"t5\" in args.model_path and args.repetition_penalty == 1.0:\n        args.repetition_penalty = 1.2\n\n    main(args)\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/model/model_registry.py", "content": "\"\"\"Additional information of the models.\"\"\"\nfrom collections import namedtuple\nfrom typing import List\n\n\nModelInfo = namedtuple(\"ModelInfo\", [\"simple_name\", \"link\", \"description\"])\n\n\nmodel_info = {}\n\n\ndef register_model_info(\n    full_names: List[str], simple_name: str, link: str, description: str\n):\n    info = ModelInfo(simple_name, link, description)\n\n    for full_name in full_names:\n        model_info[full_name] = info\n\n\ndef get_model_info(name: str) -> ModelInfo:\n    if name in model_info:\n        return model_info[name]\n    else:\n        # To fix this, please use `register_model_info` to register your model\n        return ModelInfo(\n            name, \"\", \"Register the description at fastchat/model/model_registry.py\"\n        )\n\n\nregister_model_info(\n    [\"gpt-3.5-turbo\"],\n    \"GPT-3.5\",\n    \"https://openai.com/blog/chatgpt\",\n    \"GPT-3.5 by OpenAI\",\n)\nregister_model_info(\n    [\"gpt-3.5-turbo-1106\"],\n    \"GPT-3.5-Turbo-1106\",\n    \"https://platform.openai.com/docs/models/gpt-3-5\",\n    \"GPT-3.5-Turbo-1106 by OpenAI\",\n)\nregister_model_info(\n    [\"gpt-4\"], \"GPT-4\", \"https://openai.com/research/gpt-4\", \"ChatGPT-4 by OpenAI\"\n)\nregister_model_info(\n    [\"gpt-4-turbo\"],\n    \"GPT-4-Turbo\",\n    \"https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo\",\n    \"GPT-4-Turbo by OpenAI\",\n)\nregister_model_info(\n    [\"claude-2\"],\n    \"Claude\",\n    \"https://www.anthropic.com/index/claude-2\",\n    \"Claude 2 by Anthropic\",\n)\nregister_model_info(\n    [\"claude-1\"],\n    \"Claude\",\n    \"https://www.anthropic.com/index/introducing-claude\",\n    \"Claude by Anthropic\",\n)\nregister_model_info(\n    [\"claude-instant-1\"],\n    \"Claude Instant\",\n    \"https://www.anthropic.com/index/introducing-claude\",\n    \"Claude Instant by Anthropic\",\n)\nregister_model_info(\n    [\"palm-2\"],\n    \"PaLM 2 Chat\",\n    \"https://cloud.google.com/vertex-ai/docs/release-notes#May_10_2023\",\n    \"PaLM 2 for Chat (chat-bison@001) by Google\",\n)\nregister_model_info(\n    [\n        \"vicuna-33b\",\n        \"vicuna-33b-v1.3\",\n        \"vicuna-13b\",\n        \"vicuna-13b-v1.3\",\n        \"vicuna-7b\",\n        \"vicuna-7b-v1.3\",\n    ],\n    \"Vicuna\",\n    \"https://lmsys.org/blog/2023-03-30-vicuna/\",\n    \"a chat assistant fine-tuned on user-shared conversations by LMSYS\",\n)\nregister_model_info(\n    [\"llama-2-70b-chat\", \"llama-2-34b-chat\", \"llama-2-13b-chat\", \"llama-2-7b-chat\"],\n    \"Llama 2\",\n    \"https://ai.meta.com/llama/\",\n    \"open foundation and fine-tuned chat models by Meta\",\n)\nregister_model_info(\n    [\"mistral-7b-instruct\"],\n    \"Mistral\",\n    \"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\",\n    \"a large language model by Mistral AI team\",\n)\nregister_model_info(\n    [\"zephyr-7b-beta\", \"zephyr-7b-alpha\"],\n    \"Zephyr\",\n    \"https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha\",\n    \"a chatbot fine-tuned from Mistral by Hugging Face\",\n)\nregister_model_info(\n    [\"qwen-14b-chat\"],\n    \"Qwen\",\n    \"https://huggingface.co/Qwen/Qwen-14B-Chat\",\n    \"a large language model by Alibaba Cloud\",\n)\nregister_model_info(\n    [\"codellama-34b-instruct\", \"codellama-13b-instruct\", \"codellama-7b-instruct\"],\n    \"Code Llama\",\n    \"https://ai.meta.com/blog/code-llama-large-language-model-coding/\",\n    \"open foundation models for code by Meta\",\n)\nregister_model_info(\n    [\"wizardlm-70b\", \"wizardlm-30b\", \"wizardlm-13b\"],\n    \"WizardLM\",\n    \"https://github.com/nlpxucan/WizardLM\",\n    \"an instruction-following LLM using evol-instruct by Microsoft\",\n)\nregister_model_info(\n    [\"wizardcoder-15b-v1.0\"],\n    \"WizardLM\",\n    \"https://github.com/nlpxucan/WizardLM/tree/main/WizardCoder\",\n    \"Empowering Code Large Language Models with Evol-Instruct\",\n)\nregister_model_info(\n    [\"mpt-7b-chat\", \"mpt-30b-chat\"],\n    \"MPT-Chat\",\n    \"https://www.mosaicml.com/blog/mpt-30b\",\n    \"a chatbot fine-tuned from MPT by MosaicML\",\n)\nregister_model_info(\n    [\"guanaco-33b\", \"guanaco-65b\"],\n    \"Guanaco\",\n    \"https://github.com/artidoro/qlora\",\n    \"a model fine-tuned with QLoRA by UW\",\n)\nregister_model_info(\n    [\"gpt4all-13b-snoozy\"],\n    \"GPT4All-Snoozy\",\n    \"https://github.com/nomic-ai/gpt4all\",\n    \"a finetuned LLaMA model on assistant style data by Nomic AI\",\n)\nregister_model_info(\n    [\"koala-13b\"],\n    \"Koala\",\n    \"https://bair.berkeley.edu/blog/2023/04/03/koala\",\n    \"a dialogue model for academic research by BAIR\",\n)\nregister_model_info(\n    [\"RWKV-4-Raven-14B\"],\n    \"RWKV-4-Raven\",\n    \"https://huggingface.co/BlinkDL/rwkv-4-raven\",\n    \"an RNN with transformer-level LLM performance\",\n)\nregister_model_info(\n    [\"chatglm-6b\", \"chatglm2-6b\"],\n    \"ChatGLM\",\n    \"https://chatglm.cn/blog\",\n    \"an open bilingual dialogue language model by Tsinghua University\",\n)\nregister_model_info(\n    [\"alpaca-13b\"],\n    \"Alpaca\",\n    \"https://crfm.stanford.edu/2023/03/13/alpaca.html\",\n    \"a model fine-tuned from LLaMA on instruction-following demonstrations by Stanford\",\n)\nregister_model_info(\n    [\"oasst-pythia-12b\"],\n    \"OpenAssistant (oasst)\",\n    \"https://open-assistant.io\",\n    \"an Open Assistant for everyone by LAION\",\n)\nregister_model_info(\n    [\"oasst-sft-7-llama-30b\"],\n    \"OpenAssistant (oasst)\",\n    \"https://open-assistant.io\",\n    \"an Open Assistant for everyone by LAION\",\n)\nregister_model_info(\n    [\"openchat-3.5\"],\n    \"OpenChat 3.5\",\n    \"https://github.com/imoneoi/openchat\",\n    \"OpenChat 3.5 is a versatile, open-source language model fine-tuned using C-RLFT\",\n)\nregister_model_info(\n    [\"llama-7b\", \"llama-13b\"],\n    \"LLaMA\",\n    \"https://arxiv.org/abs/2302.13971\",\n    \"open and efficient foundation language models by Meta\",\n)\nregister_model_info(\n    [\"open-llama-7b-v2-open-instruct\", \"open-llama-7b-open-instruct\"],\n    \"Open LLaMa (Open Instruct)\",\n    \"https://medium.com/vmware-data-ml-blog/starter-llm-for-the-enterprise-instruction-tuning-openllama-7b-d05fc3bbaccc\",\n    \"Open LLaMa fine-tuned on instruction-following data by VMware\",\n)\nregister_model_info(\n    [\"dolly-v2-12b\"],\n    \"Dolly\",\n    \"https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm\",\n    \"an instruction-tuned open large language model by Databricks\",\n)\nregister_model_info(\n    [\"stablelm-tuned-alpha-7b\"],\n    \"StableLM\",\n    \"https://github.com/stability-AI/stableLM\",\n    \"Stability AI language models\",\n)\nregister_model_info(\n    [\"codet5p-6b\"],\n    \"CodeT5p-6b\",\n    \"https://huggingface.co/Salesforce/codet5p-6b\",\n    \"Code completion model released by Salesforce\",\n)\nregister_model_info(\n    [\"fastchat-t5-3b\", \"fastchat-t5-3b-v1.0\"],\n    \"FastChat-T5\",\n    \"https://huggingface.co/lmsys/fastchat-t5-3b-v1.0\",\n    \"a chat assistant fine-tuned from FLAN-T5 by LMSYS\",\n)\nregister_model_info(\n    [\"phoenix-inst-chat-7b\"],\n    \"Phoenix-7B\",\n    \"https://huggingface.co/FreedomIntelligence/phoenix-inst-chat-7b\",\n    \"a multilingual chat assistant fine-tuned from Bloomz to democratize ChatGPT across languages by CUHK(SZ)\",\n)\nregister_model_info(\n    [\"realm-7b-v1\"],\n    \"ReaLM\",\n    \"https://github.com/FreedomIntelligence/ReaLM\",\n    \"A chatbot fine-tuned from LLaMA2 with data generated via iterative calls to UserGPT and ChatGPT by CUHK(SZ) and SRIBD.\",\n)\nregister_model_info(\n    [\"billa-7b-sft\"],\n    \"BiLLa-7B-SFT\",\n    \"https://huggingface.co/Neutralzz/BiLLa-7B-SFT\",\n    \"an instruction-tuned bilingual LLaMA with enhanced reasoning ability by an independent researcher\",\n)\nregister_model_info(\n    [\"h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt-v2\"],\n    \"h2oGPT-GM-7b\",\n    \"https://huggingface.co/h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt-v2\",\n    \"an instruction-tuned OpenLLaMA with enhanced conversational ability by H2O.ai\",\n)\nregister_model_info(\n    [\"baize-v2-7b\", \"baize-v2-13b\"],\n    \"Baize v2\",\n    \"https://github.com/project-baize/baize-chatbot#v2\",\n    \"A chatbot fine-tuned from LLaMA with ChatGPT self-chat data and Self-Disillation with Feedback (SDF) by UCSD and SYSU.\",\n)\nregister_model_info(\n    [\n        \"airoboros-l2-7b-2.1\",\n        \"airoboros-l2-13b-2.1\",\n        \"airoboros-c34b-2.1\",\n        \"airoboros-l2-70b-2.1\",\n    ],\n    \"airoboros\",\n    \"https://huggingface.co/jondurbin/airoboros-l2-70b-2.1\",\n    \"an instruction-tuned LlaMa model tuned with 100% synthetic instruction-response pairs from GPT4\",\n)\nregister_model_info(\n    [\n        \"spicyboros-7b-2.2\",\n        \"spicyboros-13b-2.2\",\n        \"spicyboros-70b-2.2\",\n    ],\n    \"spicyboros\",\n    \"https://huggingface.co/jondurbin/spicyboros-70b-2.2\",\n    \"de-aligned versions of the airoboros models\",\n)\nregister_model_info(\n    [\"Robin-7b-v2\", \"Robin-13b-v2\", \"Robin-33b-v2\"],\n    \"Robin-v2\",\n    \"https://huggingface.co/OptimalScale/robin-7b-v2-delta\",\n    \"A chatbot fine-tuned from LLaMA-7b, achieving competitive performance on chitchat, commonsense reasoning and instruction-following tasks, by OptimalScale, HKUST.\",\n)\nregister_model_info(\n    [\"manticore-13b-chat\"],\n    \"Manticore 13B Chat\",\n    \"https://huggingface.co/openaccess-ai-collective/manticore-13b-chat-pyg\",\n    \"A chatbot fine-tuned from LlaMa across several CoT and chat datasets.\",\n)\nregister_model_info(\n    [\"redpajama-incite-7b-chat\"],\n    \"RedPajama-INCITE-7B-Chat\",\n    \"https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Chat\",\n    \"A chatbot fine-tuned from RedPajama-INCITE-7B-Base by Together\",\n)\nregister_model_info(\n    [\n        \"falcon-7b\",\n        \"falcon-7b-instruct\",\n        \"falcon-40b\",\n        \"falcon-40b-instruct\",\n        \"falcon-180b\",\n        \"falcon-180b-chat\",\n    ],\n    \"Falcon\",\n    \"https://huggingface.co/tiiuae/falcon-180B\",\n    \"TII's flagship series of large language models\",\n)\nregister_model_info(\n    [\"tigerbot-7b-sft\"],\n    \"Tigerbot\",\n    \"https://huggingface.co/TigerResearch/tigerbot-7b-sft\",\n    \"TigerBot is a large-scale language model (LLM) with multiple languages and tasks.\",\n)\nregister_model_info(\n    [\"internlm-chat-7b\", \"internlm-chat-7b-8k\"],\n    \"InternLM\",\n    \"https://huggingface.co/internlm/internlm-chat-7b\",\n    \"InternLM is a multi-language large-scale language model (LLM), developed by SHLAB.\",\n)\nregister_model_info(\n    [\"Qwen-7B-Chat\"],\n    \"Qwen\",\n    \"https://huggingface.co/Qwen/Qwen-7B-Chat\",\n    \"Qwen is a multi-language large-scale language model (LLM), developed by Damo Academy.\",\n)\nregister_model_info(\n    [\"Llama2-Chinese-13b-Chat\", \"LLama2-Chinese-13B\"],\n    \"Llama2-Chinese\",\n    \"https://huggingface.co/FlagAlpha/Llama2-Chinese-13b-Chat\",\n    \"Llama2-Chinese is a multi-language large-scale language model (LLM), developed by FlagAlpha.\",\n)\nregister_model_info(\n    [\"Vigogne-2-7B-Instruct\", \"Vigogne-2-13B-Instruct\"],\n    \"Vigogne-Instruct\",\n    \"https://huggingface.co/bofenghuang/vigogne-2-7b-instruct\",\n    \"Vigogne-Instruct is a French large language model (LLM) optimized for instruction-following, developed by Bofeng Huang\",\n)\nregister_model_info(\n    [\"Vigogne-2-7B-Chat\", \"Vigogne-2-13B-Chat\"],\n    \"Vigogne-Chat\",\n    \"https://huggingface.co/bofenghuang/vigogne-2-7b-chat\",\n    \"Vigogne-Chat is a French large language model (LLM) optimized for instruction-following and multi-turn dialogues, developed by Bofeng Huang\",\n)\nregister_model_info(\n    [\"deluxe-chat-v1\", \"deluxe-chat-v1.1\"],\n    \"DeluxeChat\",\n    \"\",\n    \"Deluxe Chat\",\n)\nregister_model_info(\n    [\n        \"Xwin-LM-7B-V0.1\",\n        \"Xwin-LM-13B-V0.1\",\n        \"Xwin-LM-70B-V0.1\",\n        \"Xwin-LM-7B-V0.2\",\n        \"Xwin-LM-13B-V0.2\",\n    ],\n    \"Xwin-LM\",\n    \"https://github.com/Xwin-LM/Xwin-LM\",\n    \"Chat models developed by Xwin-LM team\",\n)\n\nregister_model_info(\n    [\"lemur-70b-chat\"],\n    \"Lemur-Chat\",\n    \"https://huggingface.co/OpenLemur/lemur-70b-chat-v1\",\n    \"an openly accessible language model optimized for both natural language and coding capabilities \",\n)\n\nregister_model_info(\n    [\"Mistral-7B-OpenOrca\"],\n    \"Open-Orca\",\n    \"https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca\",\n    \"A fine-tune of [Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-v0.1) using [OpenOrca dataset](https://huggingface.co/datasets/Open-Orca/OpenOrca)\",\n)\n\nregister_model_info(\n    [\n        \"AquilaChat-7B\",\n        \"AquilaChat2-7B\",\n        \"AquilaChat2-34B\",\n    ],\n    \"Aquila-Chat\",\n    \"https://huggingface.co/BAAI/AquilaChat2-34B\",\n    \"Chat models developed by BAAI team\",\n)\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/serve/huggingface_api_worker.py", "content": "\"\"\"\nA model worker that calls huggingface inference endpoint.\n\nRegister models in a JSON file with the following format:\n{\n    \"falcon-180b-chat\": {\n        \"model_path\": \"tiiuae/falcon-180B-chat\",\n        \"api_base\": \"https://api-inference.huggingface.co/models\",\n        \"token\": \"hf_xxx\",\n        \"context_length\": 2048,\n        \"model_names\": \"falcon-180b-chat\",\n        \"conv_template\": null\n    }\n}\n\n\"model_path\", \"api_base\", \"token\", and \"context_length\" are necessary, while others are optional.\n\"\"\"\nimport argparse\nimport asyncio\nimport json\nimport uuid\nimport os\nfrom typing import List, Optional\n\nimport requests\nimport uvicorn\nfrom fastapi import BackgroundTasks, FastAPI, Request\nfrom fastapi.responses import JSONResponse, StreamingResponse\nfrom huggingface_hub import InferenceClient\n\nfrom fastchat.constants import SERVER_ERROR_MSG, ErrorCode\nfrom fastchat.serve.base_model_worker import BaseModelWorker\nfrom fastchat.utils import build_logger\n\nworker_id = str(uuid.uuid4())[:8]\nlogger = build_logger(\"model_worker\", f\"model_worker_{worker_id}.log\")\n\nworkers = []\nworker_map = {}\napp = FastAPI()\n\n\n# reference to\n# https://github.com/philschmid/easyllm/blob/cbd908b3b3f44a97a22cb0fc2c93df3660bacdad/easyllm/clients/huggingface.py#L374-L392\ndef get_gen_kwargs(\n    params,\n    seed: Optional[int] = None,\n):\n    stop = params.get(\"stop\", None)\n    if isinstance(stop, list):\n        stop_sequences = stop\n    elif isinstance(stop, str):\n        stop_sequences = [stop]\n    else:\n        stop_sequences = []\n    gen_kwargs = {\n        \"do_sample\": True,\n        \"return_full_text\": bool(params.get(\"echo\", False)),\n        \"max_new_tokens\": int(params.get(\"max_new_tokens\", 256)),\n        \"top_p\": float(params.get(\"top_p\", 1.0)),\n        \"temperature\": float(params.get(\"temperature\", 1.0)),\n        \"stop_sequences\": stop_sequences,\n        \"repetition_penalty\": float(params.get(\"repetition_penalty\", 1.0)),\n        \"top_k\": params.get(\"top_k\", None),\n        \"seed\": seed,\n    }\n    if gen_kwargs[\"top_p\"] == 1:\n        gen_kwargs[\"top_p\"] = 0.9999999\n    if gen_kwargs[\"top_p\"] == 0:\n        gen_kwargs.pop(\"top_p\")\n    if gen_kwargs[\"temperature\"] == 0:\n        gen_kwargs.pop(\"temperature\")\n        gen_kwargs[\"do_sample\"] = False\n    return gen_kwargs\n\n\ndef could_be_stop(text, stop):\n    for s in stop:\n        if any(text.endswith(s[:i]) for i in range(1, len(s) + 1)):\n            return True\n    return False\n\n\nclass HuggingfaceApiWorker(BaseModelWorker):\n    def __init__(\n        self,\n        controller_addr: str,\n        worker_addr: str,\n        worker_id: str,\n        model_path: str,\n        api_base: str,\n        token: str,\n        context_length: int,\n        model_names: List[str],\n        limit_worker_concurrency: int,\n        no_register: bool,\n        conv_template: Optional[str] = None,\n        seed: Optional[int] = None,\n        **kwargs,\n    ):\n        super().__init__(\n            controller_addr,\n            worker_addr,\n            worker_id,\n            model_path,\n            model_names,\n            limit_worker_concurrency,\n            conv_template=conv_template,\n        )\n\n        self.model_path = model_path\n        self.api_base = api_base\n        self.token = token\n        self.context_len = context_length\n        self.seed = seed\n\n        logger.info(\n            f\"Connecting with huggingface api {self.model_path} as {self.model_names} on worker {worker_id} ...\"\n        )\n\n        if not no_register:\n            self.init_heart_beat()\n\n    def count_token(self, params):\n        # No tokenizer here\n        ret = {\n            \"count\": 0,\n            \"error_code\": 0,\n        }\n        return ret\n\n    def generate_stream_gate(self, params):\n        self.call_ct += 1\n\n        prompt = params[\"prompt\"]\n        gen_kwargs = get_gen_kwargs(params, seed=self.seed)\n        stop = gen_kwargs[\"stop_sequences\"]\n        if \"falcon\" in self.model_path and \"chat\" in self.model_path:\n            stop.extend([\"\\nUser:\", \"<|endoftext|>\", \" User:\", \"###\"])\n            stop = list(set(stop))\n            gen_kwargs[\"stop_sequences\"] = stop\n\n        logger.info(f\"prompt: {prompt}\")\n        logger.info(f\"gen_kwargs: {gen_kwargs}\")\n\n        try:\n            if self.model_path == \"\":\n                url = f\"{self.api_base}\"\n            else:\n                url = f\"{self.api_base}/{self.model_path}\"\n            client = InferenceClient(url, token=self.token)\n            res = client.text_generation(\n                prompt, stream=True, details=True, **gen_kwargs\n            )\n\n            reason = None\n            text = \"\"\n            for chunk in res:\n                if chunk.token.special:\n                    continue\n                text += chunk.token.text\n\n                s = next((x for x in stop if text.endswith(x)), None)\n                if s is not None:\n                    text = text[: -len(s)]\n                    reason = \"stop\"\n                    break\n                if could_be_stop(text, stop):\n                    continue\n                if (\n                    chunk.details is not None\n                    and chunk.details.finish_reason is not None\n                ):\n                    reason = chunk.details.finish_reason\n                if reason not in [\"stop\", \"length\"]:\n                    reason = None\n                ret = {\n                    \"text\": text,\n                    \"error_code\": 0,\n                    \"finish_reason\": reason,\n                }\n                yield json.dumps(ret).encode() + b\"\\0\"\n        except Exception as e:\n            ret = {\n                \"text\": f\"{SERVER_ERROR_MSG}\\n\\n({e})\",\n                \"error_code\": ErrorCode.INTERNAL_ERROR,\n            }\n            yield json.dumps(ret).encode() + b\"\\0\"\n\n    def generate_gate(self, params):\n        for x in self.generate_stream_gate(params):\n            pass\n        return json.loads(x[:-1].decode())\n\n    def get_embeddings(self, params):\n        raise NotImplementedError()\n\n\ndef release_worker_semaphore(worker):\n    worker.semaphore.release()\n\n\ndef acquire_worker_semaphore(worker):\n    if worker.semaphore is None:\n        worker.semaphore = asyncio.Semaphore(worker.limit_worker_concurrency)\n    return worker.semaphore.acquire()\n\n\ndef create_background_tasks(worker):\n    background_tasks = BackgroundTasks()\n    background_tasks.add_task(lambda: release_worker_semaphore(worker))\n    return background_tasks\n\n\n@app.post(\"/worker_generate_stream\")\nasync def api_generate_stream(request: Request):\n    params = await request.json()\n    worker = worker_map[params[\"model\"]]\n    await acquire_worker_semaphore(worker)\n    generator = worker.generate_stream_gate(params)\n    background_tasks = create_background_tasks(worker)\n    return StreamingResponse(generator, background=background_tasks)\n\n\n@app.post(\"/worker_generate\")\nasync def api_generate(request: Request):\n    params = await request.json()\n    worker = worker_map[params[\"model\"]]\n    await acquire_worker_semaphore(worker)\n    output = worker.generate_gate(params)\n    release_worker_semaphore(worker)\n    return JSONResponse(output)\n\n\n@app.post(\"/worker_get_embeddings\")\nasync def api_get_embeddings(request: Request):\n    params = await request.json()\n    worker = worker_map[params[\"model\"]]\n    await acquire_worker_semaphore(worker)\n    embedding = worker.get_embeddings(params)\n    release_worker_semaphore(worker)\n    return JSONResponse(content=embedding)\n\n\n@app.post(\"/worker_get_status\")\nasync def api_get_status(request: Request):\n    return {\n        \"model_names\": [m for w in workers for m in w.model_names],\n        \"speed\": 1,\n        \"queue_length\": sum([w.get_queue_length() for w in workers]),\n    }\n\n\n@app.post(\"/count_token\")\nasync def api_count_token(request: Request):\n    params = await request.json()\n    worker = worker_map[params[\"model\"]]\n    return worker.count_token(params)\n\n\n@app.post(\"/worker_get_conv_template\")\nasync def api_get_conv(request: Request):\n    params = await request.json()\n    worker = worker_map[params[\"model\"]]\n    return worker.get_conv_template()\n\n\n@app.post(\"/model_details\")\nasync def api_model_details(request: Request):\n    params = await request.json()\n    worker = worker_map[params[\"model\"]]\n    return {\"context_length\": worker.context_len}\n\n\ndef create_huggingface_api_worker():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--host\", type=str, default=\"localhost\")\n    parser.add_argument(\"--port\", type=int, default=21002)\n    parser.add_argument(\"--worker-address\", type=str, default=\"http://localhost:21002\")\n    parser.add_argument(\n        \"--controller-address\", type=str, default=\"http://localhost:21001\"\n    )\n    # all model-related parameters are listed in --model-info-file\n    parser.add_argument(\n        \"--model-info-file\",\n        type=str,\n        required=True,\n        help=\"Huggingface API model's info file path\",\n    )\n\n    parser.add_argument(\n        \"--limit-worker-concurrency\",\n        type=int,\n        default=5,\n        help=\"Limit the model concurrency to prevent OOM.\",\n    )\n    parser.add_argument(\"--no-register\", action=\"store_true\")\n    parser.add_argument(\n        \"--seed\",\n        type=int,\n        default=None,\n        help=\"Overwrite the random seed for each generation.\",\n    )\n    parser.add_argument(\n        \"--ssl\",\n        action=\"store_true\",\n        required=False,\n        default=False,\n        help=\"Enable SSL. Requires OS Environment variables 'SSL_KEYFILE' and 'SSL_CERTFILE'.\",\n    )\n    args = parser.parse_args()\n\n    with open(args.model_info_file, \"r\", encoding=\"UTF-8\") as f:\n        model_info = json.load(f)\n\n    logger.info(f\"args: {args}\")\n\n    model_path_list = []\n    api_base_list = []\n    token_list = []\n    context_length_list = []\n    model_names_list = []\n    conv_template_list = []\n\n    for m in model_info:\n        model_path_list.append(model_info[m][\"model_path\"])\n        api_base_list.append(model_info[m][\"api_base\"])\n        token_list.append(model_info[m][\"token\"])\n\n        context_length = model_info[m][\"context_length\"]\n        model_names = model_info[m].get(\"model_names\", [m.split(\"/\")[-1]])\n        if isinstance(model_names, str):\n            model_names = [model_names]\n        conv_template = model_info[m].get(\"conv_template\", None)\n\n        context_length_list.append(context_length)\n        model_names_list.append(model_names)\n        conv_template_list.append(conv_template)\n\n    logger.info(f\"Model paths: {model_path_list}\")\n    logger.info(f\"API bases: {api_base_list}\")\n    logger.info(f\"Tokens: {token_list}\")\n    logger.info(f\"Context lengths: {context_length_list}\")\n    logger.info(f\"Model names: {model_names_list}\")\n    logger.info(f\"Conv templates: {conv_template_list}\")\n\n    for (\n        model_names,\n        conv_template,\n        model_path,\n        api_base,\n        token,\n        context_length,\n    ) in zip(\n        model_names_list,\n        conv_template_list,\n        model_path_list,\n        api_base_list,\n        token_list,\n        context_length_list,\n    ):\n        m = HuggingfaceApiWorker(\n            args.controller_address,\n            args.worker_address,\n            worker_id,\n            model_path,\n            api_base,\n            token,\n            context_length,\n            model_names,\n            args.limit_worker_concurrency,\n            no_register=args.no_register,\n            conv_template=conv_template,\n            seed=args.seed,\n        )\n        workers.append(m)\n        for name in model_names:\n            worker_map[name] = m\n\n    # register all the models\n    url = args.controller_address + \"/register_worker\"\n    data = {\n        \"worker_name\": workers[0].worker_addr,\n        \"check_heart_beat\": not args.no_register,\n        \"worker_status\": {\n            \"model_names\": [m for w in workers for m in w.model_names],\n            \"speed\": 1,\n            \"queue_length\": sum([w.get_queue_length() for w in workers]),\n        },\n    }\n    r = requests.post(url, json=data)\n    assert r.status_code == 200\n\n    return args, workers\n\n\nif __name__ == \"__main__\":\n    args, workers = create_huggingface_api_worker()\n    if args.ssl:\n        uvicorn.run(\n            app,\n            host=args.host,\n            port=args.port,\n            log_level=\"info\",\n            ssl_keyfile=os.environ[\"SSL_KEYFILE\"],\n            ssl_certfile=os.environ[\"SSL_CERTFILE\"],\n        )\n    else:\n        uvicorn.run(app, host=args.host, port=args.port, log_level=\"info\")\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/serve/launch_all_serve.py", "content": "\"\"\"\nUsage: python launch_all_serve_by_shell.py --model-path-address \"THUDM/chatglm2-6b@localhost@2021\" \"huggyllama/llama-7b@localhost@2022\" \n\nWorkers are listed in format of `model-path`@`host`@`port` \n\nThe key mechanism behind this scripts is: \n    1, execute shell cmd to launch the controller/worker/openai-api-server;\n    2, check the log of controller/worker/openai-api-server to ensure that the serve is launched properly.\nNote that a few of non-critical `fastchat.serve` cmd options are not supported currently.\n\"\"\"\nimport sys\nimport os\n\nsys.path.append(os.path.dirname(os.path.dirname(__file__)))\n\nimport subprocess\nimport re\nimport argparse\n\nLOGDIR = \"./logs/\"\n\nif not os.path.exists(LOGDIR):\n    os.makedirs(LOGDIR)\n\nparser = argparse.ArgumentParser()\n# ------multi worker-----------------\nparser.add_argument(\n    \"--model-path-address\",\n    default=\"THUDM/chatglm2-6b@localhost@20002\",\n    nargs=\"+\",\n    type=str,\n    help=\"model path, host, and port, formatted as model-path@host@port\",\n)\n# ---------------controller-------------------------\n\nparser.add_argument(\"--controller-host\", type=str, default=\"localhost\")\nparser.add_argument(\"--controller-port\", type=int, default=21001)\nparser.add_argument(\n    \"--dispatch-method\",\n    type=str,\n    choices=[\"lottery\", \"shortest_queue\"],\n    default=\"shortest_queue\",\n)\ncontroller_args = [\"controller-host\", \"controller-port\", \"dispatch-method\"]\n\n# ----------------------worker------------------------------------------\n\nparser.add_argument(\"--worker-host\", type=str, default=\"localhost\")\nparser.add_argument(\"--worker-port\", type=int, default=21002)\n# parser.add_argument(\"--worker-address\", type=str, default=\"http://localhost:21002\")\n# parser.add_argument(\n#     \"--controller-address\", type=str, default=\"http://localhost:21001\"\n# )\nparser.add_argument(\n    \"--model-path\",\n    type=str,\n    default=\"lmsys/vicuna-7b-v1.5\",\n    help=\"The path to the weights. This can be a local folder or a Hugging Face repo ID.\",\n)\nparser.add_argument(\n    \"--revision\",\n    type=str,\n    default=\"main\",\n    help=\"Hugging Face Hub model revision identifier\",\n)\nparser.add_argument(\n    \"--device\",\n    type=str,\n    choices=[\"cpu\", \"cuda\", \"mps\", \"xpu\", \"npu\"],\n    default=\"cuda\",\n    help=\"The device type\",\n)\nparser.add_argument(\n    \"--gpus\",\n    type=str,\n    default=\"0\",\n    help=\"A single GPU like 1 or multiple GPUs like 0,2\",\n)\nparser.add_argument(\"--num-gpus\", type=int, default=1)\nparser.add_argument(\n    \"--max-gpu-memory\",\n    type=str,\n    help=\"The maximum memory per gpu. Use a string like '13Gib'\",\n)\nparser.add_argument(\"--load-8bit\", action=\"store_true\", help=\"Use 8-bit quantization\")\nparser.add_argument(\n    \"--cpu-offloading\",\n    action=\"store_true\",\n    help=\"Only when using 8-bit quantization: Offload excess weights to the CPU that don't fit on the GPU\",\n)\nparser.add_argument(\n    \"--gptq-ckpt\",\n    type=str,\n    default=None,\n    help=\"Load quantized model. The path to the local GPTQ checkpoint.\",\n)\nparser.add_argument(\n    \"--gptq-wbits\",\n    type=int,\n    default=16,\n    choices=[2, 3, 4, 8, 16],\n    help=\"#bits to use for quantization\",\n)\nparser.add_argument(\n    \"--gptq-groupsize\",\n    type=int,\n    default=-1,\n    help=\"Groupsize to use for quantization; default uses full row.\",\n)\nparser.add_argument(\n    \"--gptq-act-order\",\n    action=\"store_true\",\n    help=\"Whether to apply the activation order GPTQ heuristic\",\n)\nparser.add_argument(\n    \"--model-names\",\n    type=lambda s: s.split(\",\"),\n    help=\"Optional display comma separated names\",\n)\nparser.add_argument(\n    \"--limit-worker-concurrency\",\n    type=int,\n    default=5,\n    help=\"Limit the model concurrency to prevent OOM.\",\n)\nparser.add_argument(\"--stream-interval\", type=int, default=2)\nparser.add_argument(\"--no-register\", action=\"store_true\")\n\nworker_args = [\n    \"worker-host\",\n    \"worker-port\",\n    \"model-path\",\n    \"revision\",\n    \"device\",\n    \"gpus\",\n    \"num-gpus\",\n    \"max-gpu-memory\",\n    \"load-8bit\",\n    \"cpu-offloading\",\n    \"gptq-ckpt\",\n    \"gptq-wbits\",\n    \"gptq-groupsize\",\n    \"gptq-act-order\",\n    \"model-names\",\n    \"limit-worker-concurrency\",\n    \"stream-interval\",\n    \"no-register\",\n    \"controller-address\",\n]\n# -----------------openai server---------------------------\n\nparser.add_argument(\"--server-host\", type=str, default=\"localhost\", help=\"host name\")\nparser.add_argument(\"--server-port\", type=int, default=8001, help=\"port number\")\nparser.add_argument(\n    \"--allow-credentials\", action=\"store_true\", help=\"allow credentials\"\n)\n# parser.add_argument(\n#     \"--allowed-origins\", type=json.loads, default=[\"*\"], help=\"allowed origins\"\n# )\n# parser.add_argument(\n#     \"--allowed-methods\", type=json.loads, default=[\"*\"], help=\"allowed methods\"\n# )\n# parser.add_argument(\n#     \"--allowed-headers\", type=json.loads, default=[\"*\"], help=\"allowed headers\"\n# )\nparser.add_argument(\n    \"--api-keys\",\n    type=lambda s: s.split(\",\"),\n    help=\"Optional list of comma separated API keys\",\n)\nserver_args = [\n    \"server-host\",\n    \"server-port\",\n    \"allow-credentials\",\n    \"api-keys\",\n    \"controller-address\",\n]\n\nargs = parser.parse_args()\n\nargs = argparse.Namespace(\n    **vars(args),\n    **{\"controller-address\": f\"http://{args.controller_host}:{args.controller_port}\"},\n)\n\nif args.gpus:\n    if len(args.gpus.split(\",\")) < args.num_gpus:\n        raise ValueError(\n            f\"Larger --num-gpus ({args.num_gpus}) than --gpus {args.gpus}!\"\n        )\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpus\n\n# 0,controller, model_worker, openai_api_server\n# 1, cmd options\n# 2,LOGDIR\n# 3, log file name\nbase_launch_sh = \"nohup python3 -m fastchat.serve.{0} {1} >{2}/{3}.log 2>&1 &\"\n\n# 0 LOGDIR\n#! 1 log file name\n# 2 controller, worker, openai_api_server\nbase_check_sh = \"\"\"while [ `grep -c \"Uvicorn running on\" {0}/{1}.log` -eq '0' ];do\n                        sleep 1s;\n                        echo \"wait {2} running\"\n                done\n                echo '{2} running' \"\"\"\n\n\ndef string_args(args, args_list):\n    args_str = \"\"\n    for key, value in args._get_kwargs():\n        key = key.replace(\"_\", \"-\")\n        if key not in args_list:\n            continue\n\n        key = key.split(\"-\")[-1] if re.search(\"port|host\", key) else key\n        if not value:\n            pass\n        # 1==True ->  True\n        elif isinstance(value, bool) and value == True:\n            args_str += f\" --{key} \"\n        elif (\n            isinstance(value, list)\n            or isinstance(value, tuple)\n            or isinstance(value, set)\n        ):\n            value = \" \".join(value)\n            args_str += f\" --{key} {value} \"\n        else:\n            args_str += f\" --{key} {value} \"\n\n    return args_str\n\n\ndef launch_worker(item):\n    log_name = (\n        item.split(\"/\")[-1]\n        .split(\"\\\\\")[-1]\n        .replace(\"-\", \"_\")\n        .replace(\"@\", \"_\")\n        .replace(\".\", \"_\")\n    )\n\n    args.model_path, args.worker_host, args.worker_port = item.split(\"@\")\n    print(\"*\" * 80)\n    worker_str_args = string_args(args, worker_args)\n    print(worker_str_args)\n    worker_sh = base_launch_sh.format(\n        \"model_worker\", worker_str_args, LOGDIR, f\"worker_{log_name}\"\n    )\n    worker_check_sh = base_check_sh.format(LOGDIR, f\"worker_{log_name}\", \"model_worker\")\n    subprocess.run(worker_sh, shell=True, check=True)\n    subprocess.run(worker_check_sh, shell=True, check=True)\n\n\ndef launch_all():\n    controller_str_args = string_args(args, controller_args)\n    controller_sh = base_launch_sh.format(\n        \"controller\", controller_str_args, LOGDIR, \"controller\"\n    )\n    controller_check_sh = base_check_sh.format(LOGDIR, \"controller\", \"controller\")\n    subprocess.run(controller_sh, shell=True, check=True)\n    subprocess.run(controller_check_sh, shell=True, check=True)\n\n    if isinstance(args.model_path_address, str):\n        launch_worker(args.model_path_address)\n    else:\n        for idx, item in enumerate(args.model_path_address):\n            print(f\"loading {idx}th model:{item}\")\n            launch_worker(item)\n\n    server_str_args = string_args(args, server_args)\n    server_sh = base_launch_sh.format(\n        \"openai_api_server\", server_str_args, LOGDIR, \"openai_api_server\"\n    )\n    server_check_sh = base_check_sh.format(\n        LOGDIR, \"openai_api_server\", \"openai_api_server\"\n    )\n    subprocess.run(server_sh, shell=True, check=True)\n    subprocess.run(server_check_sh, shell=True, check=True)\n\n\nif __name__ == \"__main__\":\n    launch_all()\n"}
{"type": "source_file", "path": "3rdparty/FastChat/fastchat/__init__.py", "content": "__version__ = \"0.2.32\"\n"}
