{"repo_info": {"repo_name": "PlanGen", "repo_owner": "360CVGroup", "repo_url": "https://github.com/360CVGroup/PlanGen"}}
{"type": "source_file", "path": "project/base/base_system.py", "content": "import sys\nimport torch\nfrom torch import nn\nimport argparse\nimport logging\nimport math\nimport os\nimport shutil\nfrom copy import deepcopy\nimport types\nimport gc\nfrom time import time\n\nimport einops\nfrom rich import print\nimport os.path as osp\nimport datasets\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nimport transformers\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nfrom accelerate.utils import ProjectConfiguration, set_seed\nfrom tqdm.auto import tqdm\nfrom transformers import CLIPTextModel, CLIPTokenizer\nimport diffusers\nfrom diffusers import AutoencoderKL, DDPMScheduler, DDIMScheduler, DPMSolverMultistepScheduler, UNet2DConditionModel, UniPCMultistepScheduler, EulerAncestralDiscreteScheduler, DiffusionPipeline\nfrom diffusers.optimization import get_scheduler\nfrom diffusers.training_utils import EMAModel\nfrom diffusers.utils.torch_utils import randn_tensor\nfrom diffusers.models.attention import BasicTransformerBlock, _chunked_feed_forward\nfrom diffusers.models.attention_processor import Attention\nfrom PIL import Image\nfrom mmengine.config import Config, DictAction\nfrom diffusers import ControlNetModel\nimport safetensors\nfrom src.utils.funcs import *\nfrom src.utils.seg_palette import palette#151个\nimport pickle\nfrom src.utils.funcs import *\nfrom contextlib import contextmanager\nimport wandb\nfrom peft import LoraConfig, set_peft_model_state_dict, PeftModel, get_peft_model\nfrom time import time\nfrom torch.utils.data.dataloader import default_collate\n\nimport os\nimport PIL.Image\nimport torch\nimport numpy as np\nfrom transformers import AutoModelForCausalLM\nfrom src.utils.causal_loss import ForCausalLMLoss\nfrom tokenizers import AddedToken\n\nfrom abc import ABC, abstractmethod\n\nclass Base_System(nn.Module):\n    def __init__(self, \n    ) -> None:\n        super().__init__()\n\n    @property\n    def set_dataset(self,):\n        assert False\n    @abstractmethod\n    def prepare_trainable(self,):\n        pass\n    def get_child(self, model):\n        print([n for n,t in model.named_children()])\n    def get_dtype(self, model):\n        return next(model.parameters()).dtype\n    def get_device(self, model):\n        return next(model.parameters()).device\n    def load_dataset_and_loader(self,):\n        if self.args.test: self.args.train_data = self.args.test_data\n\n        train_dataset, test_dataset, train_dataloader, test_dataloader = self.set_dataset(##\n            self.args, \n            self.args.train_data, \n            self.args.test_data, \n            self.args.train_batch_size, \n            self.args.test_batch_size,\n            tokenizer=None, \n            accelerator=None,\n            collate_fn=None,\n        )\n        return train_dataset, test_dataset, train_dataloader, test_dataloader\n\n    def load_dataset_and_loader_2(self,train_dataset=None, test_dataset=None):\n        if self.args.test: self.args.train_data_2 = self.args.test_data_2\n\n        train_dataset, test_dataset, train_dataloader, test_dataloader = self.set_dataset(\n            self.args, \n            self.args.train_data_2, \n            self.args.test_data_2, \n            self.args.train_batch_size_2, \n            self.args.test_batch_size_2,\n            tokenizer=None,\n            accelerator=None,\n            collate_fn=None,\n            train_dataset=train_dataset, test_dataset=train_dataset\n        )\n        return train_dataset, test_dataset, train_dataloader, test_dataloader\n\n    def setup_data(self, accelerator):\n        args = self.args\n        train_dataset, test_dataset, train_dataloader, test_dataloader = self.load_dataset_and_loader()\n\n        print(f\"len(train_dataset): {len(train_dataset)}\", end='\\n\\n')\n        print(f\"len(test_dataset): {len(test_dataset)}\", end='\\n\\n')\n        \n        train_dataloader, test_dataloader = accelerator.prepare(train_dataloader, test_dataloader)\n        \n        self.train_dataloader = train_dataloader\n        self.test_dataloader = test_dataloader\n        self.train_dataset = train_dataset\n        self.test_dataset = test_dataset\n        return train_dataloader, train_dataset\n\n    @abstractmethod\n    def forward(self, batch):\n        pass\n\n    def resume(self, accelerator=None, stage1_path=None):\n        args = self.args\n        global_step = 0\n        if stage1_path is None:\n            if args.resume != \"latest\":\n                if isinstance(args.resume, int):\n                    path = f\"checkpoint-{args.resume}\"\n                    path = os.path.join(args.output_dir, path)\n                else:\n                    path = args.resume\n            else:\n                # Get the most recent checkpoint\n                dirs = os.listdir(args.output_dir)\n                dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n                dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n                path = dirs[-1] if len(dirs) > 0 else None\n                if path is not None:\n                    path = os.path.join(args.output_dir, path)\n\n            if path is None:\n                print(f\"Checkpoint '{args.resume}' does not exist. Starting a new training run.\")\n                args.resume = None\n            else:\n                print(f\"Resuming from checkpoint {path}\")\n                global_step = int(os.path.basename(path).split(\"-\")[1])\n\n                state_dict = torch.load(osp.join(path, 'trainable_model_parameters.pth'), map_location=torch.device('cpu'))\n                mesg = self.load_state_dict(state_dict, strict=False)\n                print(mesg)\n\n                # if accelerator is not None:\n                #     accelerator.load_state(path, map_location='cpu', strict=False)#key缺少增多问题不大，但是shape要一致\n        else:\n            state_dict = torch.load(osp.join(stage1_path, 'trainable_model_parameters.pth'), map_location=torch.device('cpu'))\n            mesg = self.load_state_dict(state_dict, strict=False)\n            print(mesg)\n\n        return global_step\n    \n    def save_para(self, global_step, accelerator):\n        args = self.args\n        if args.checkpoints_total_limit is not None:\n            checkpoints = os.listdir(args.output_dir)\n            checkpoints = [d for d in checkpoints if d.startswith(\"checkpoint\")]\n            checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n\n            if len(checkpoints) >= args.checkpoints_total_limit:\n                num_to_remove = len(checkpoints) - args.checkpoints_total_limit + 1\n                removing_checkpoints = checkpoints[0:num_to_remove]\n                print(f\"{len(checkpoints)} checkpoints already exist, removing {len(removing_checkpoints)} checkpoints\")\n                print(f\"removing checkpoints: {', '.join(removing_checkpoints)}\")\n                for removing_checkpoint in removing_checkpoints:\n                    removing_checkpoint = os.path.join(args.output_dir, removing_checkpoint)\n                    shutil.rmtree(removing_checkpoint)\n\n        save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n        mkdir(save_path)\n        # accelerator.save_state(save_path)\n        # print(f\"Saved state to {save_path}\")\n\n        # state_dict = {k:v for k,v in self.state_dict().items() if v.requires_grad} #是tensor不是parameter所以requires_grad都是false\n        state_dict = {k:v for k,v in self.named_parameters() if v.requires_grad}\n        torch.save(state_dict, osp.join(save_path, 'trainable_model_parameters.pth'))\n\n    def check_para(self, model):\n        params = [p for p in model.parameters()]\n        print('-------------------')\n        print(model.__class__, get_parameter_number_params(params))\n    \n    def get_trainable_para(self, accelerator):\n        params = [p for p in self.parameters() if p.requires_grad]\n        if accelerator.is_main_process:\n            paramsx = [p for p in self.parameters()]\n            print(self.__class__, get_parameter_number_params(paramsx))\n        return params\n    \n    def get_trainable_para_lr(self, accelerator):\n        names = []\n        params_with_lr = []\n        for name, p in self.named_parameters():\n            if p.requires_grad:\n                lr = self.args.learning_rate\n                names.append(name)\n                params_with_lr.append({'params': p, 'lr':lr})\n\n        params = [p for p in self.parameters() if p.requires_grad]\n\n        if accelerator.is_main_process:\n            path = osp.join(self.args.output_dir, 'params.jsonl')\n            save_jsonl(path, names)\n            for module in self.trainable:\n                part_params = [p for p in module.parameters()]\n                print('---trainable---')\n                print(module.__class__, get_parameter_number_params(part_params))\n\n            paramsx = [p for p in self.parameters()]\n            print('++++ sum ++++')\n            print(self.__class__, get_parameter_number_params(paramsx))\n\n        return params_with_lr, params\n\n    @property\n    def device(self,):\n        assert False\n\n    @property\n    def dtype(self):\n        assert False\n    \n    @staticmethod\n    def freeze_params(params):\n        for param in params:\n            param.requires_grad = False\n\n    @staticmethod\n    def unfreeze_params(params):\n        for param in params:\n            param.requires_grad = True\n\n    @staticmethod\n    def check_dropout_enable(model):\n        for name, module in model.named_modules():\n            if isinstance(module, nn.Dropout):\n                print(f\"Dropout layer '{name}' is {'enabled' if module.training else 'disabled'}\")\n\n    @abstractmethod\n    def validation(self, global_step=0, accelerator=None,):\n        pass\n"}
{"type": "source_file", "path": "project/plangen/dataset/layoutgpt/data_layoutgpt.py", "content": "import sys;sys.path.insert(0, '/home/jovyan/boomcheng-data-shcdt/herunze/code/base/')\nfrom torch.utils.data import Dataset\nimport random\nfrom copy import deepcopy\nfrom torchvision.transforms import Resize\nfrom glob import glob\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport random\nfrom copy import deepcopy\nfrom torchvision.transforms import Resize\nfrom datasets import load_dataset\nfrom src.utils.funcs import convert_to_np, load_jsonl\nimport numpy as np\nimport os\nfrom src.utils.funcs import *\n\n# three_party/LayoutGPT/dataset/NSR-1K/counting/counting.train.json\n# three_party/LayoutGPT/dataset/NSR-1K/spatial/spatial.train.json\n\nclass Dataset_layout(Dataset):\n    def __init__(\n        self,\n        args=None,\n        is_test=False,\n    ):\n        self.args = args\n        self.is_test = is_test\n\n        datas_counting = load_json('three_party/LayoutGPT/dataset/NSR-1K/counting/counting.train.json') + load_json('three_party/LayoutGPT/dataset/NSR-1K/counting/counting.val.json')#38k\n        datas_spatial = load_json('three_party/LayoutGPT/dataset/NSR-1K/spatial/spatial.train.json') + load_json('three_party/LayoutGPT/dataset/NSR-1K/spatial/spatial.val.json')#0.7k\n\n        items = []\n        for data in datas_counting:\n            object_list = data['object_list']\n            clas = [t[0] for t in object_list]\n            bboxes = torch.tensor([t[1] for t in object_list])\n            new_bboxes = self.convert_box(bboxes)\n            items.append(dict(\n                base_caption=data['prompt'],\n                obj_bbox=new_bboxes,\n                obj_class=clas,\n            ))\n        for data in datas_spatial:\n            object_list = [data['obj1'], data['obj2']]\n            clas = [t[0] for t in object_list]\n            bboxes = torch.tensor([t[1] for t in object_list])\n            new_bboxes = self.convert_box(bboxes)\n            # for i in range(1):\n            for i in range(10):\n                items.append(dict(\n                    base_caption=data['prompt'],\n                    obj_bbox=new_bboxes,\n                    obj_class=clas,\n                ))\n\n        self.items = items\n\n    def convert_box(self, bboxes):\n        # 提取 cx, cy, _w, _h\n        cx = bboxes[:, 0]\n        cy = bboxes[:, 1]\n        _w = bboxes[:, 2]\n        _h = bboxes[:, 3]\n        # x1,y1,h,w\n        # x1,y1,x2,y2\n\n        # 计算 x1, y1, x2, y2\n        x1 = cx\n        y1 = cy\n        x2 = cx + _w\n        y2 = cy + _h\n\n        # 将结果堆叠成一个新的 Tensor，形状为 (n, 4)\n        new_bboxes = torch.stack([x1, y1, x2, y2], dim=1)\n        return new_bboxes\n\n    def __len__(self):\n        return len(self.items)\n\n    def __getitem__(self, i):\n        return self.items[i]\n\nif __name__ == '__main__':\n    data = Dataset_layout(args=None)"}
{"type": "source_file", "path": "project/plangen/dataset/coco/data_coco_bef.py", "content": "import torchvision.datasets as dset\nfrom torchvision import transforms\nfrom pycocotools.coco import COCO\nfrom torch.utils.data import DataLoader\n\nfrom torch.utils.data import Dataset\nimport random\nfrom copy import deepcopy\nfrom torchvision.transforms import Resize\nfrom glob import glob\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport random\nfrom copy import deepcopy\nfrom torchvision.transforms import Resize\nfrom datasets import load_dataset\nfrom src.utils.funcs import convert_to_np, load_jsonl\nimport numpy as np\nimport os\nfrom src.utils.funcs import *\nfrom torch.utils.data import ConcatDataset\n\ndef resize_and_crop(image, bboxes, target_size=384):\n    \"\"\"\n    Resize the image with the short side to target_size, then center crop to target_size x target_size.\n    Adjust the bounding boxes accordingly.\n\n    :param image: PIL Image\n    :param bboxes: numpy array of shape (n, 4) where each row is [x1, y1, w, h]\n    :param target_size: int, the target size for the short side and crop\n    :return: resized and cropped image, adjusted bboxes\n    \"\"\"\n    # Get original image size\n    original_width, original_height = image.size\n    \n    # Determine the scaling factor\n    if original_width < original_height:\n        scale = target_size / original_width\n        new_width = target_size\n        new_height = int(original_height * scale)\n    else:\n        scale = target_size / original_height\n        new_height = target_size\n        new_width = int(original_width * scale)\n    \n    # Resize the image\n    cropped_image = image.resize((new_width, new_height), Image.BILINEAR)\n    \n    # Calculate the coordinates for center cropping\n    left = (new_width - target_size) // 2\n    top = (new_height - target_size) // 2\n    right = left + target_size\n    bottom = top + target_size\n    \n    cropped_image = cropped_image.crop((left, top, right, bottom))\n    \n    adjusted_bboxes = []\n    for bbox in bboxes:\n        x1, y1, w, h = bbox\n        x1_scaled = x1 * scale\n        y1_scaled = y1 * scale\n        w_scaled = w * scale\n        h_scaled = h * scale\n        \n        x1_cropped = x1_scaled - left\n        y1_cropped = y1_scaled - top\n        \n        adjusted_bboxes.append([x1_cropped, y1_cropped, w_scaled, h_scaled])\n    \n    return cropped_image, np.array(adjusted_bboxes)\n\n\ndef filter_box(all_bbox, all_class):\n    image_width = image_height = 384\n    filtered_bbox = []\n    filtered_class = []\n\n    for i, (x, y, w, h) in enumerate(all_bbox):\n        \n        # 调整框的坐标和宽高，确保它们在图像范围内\n        x2 = x + w\n        y2 = y + h\n        x = max(0, x)\n        y = max(0, y)\n\n        if x > 380 or y > 380:\n            pass\n        else:\n            x2 = min(384, x2)\n            y2 = min(384, y2)\n            # w = min(384-x2, w)\n            # h = min(384-y2, h)\n            w = x2 - x\n            h = y2 - y\n\n            if w*h < 200:\n                pass\n            else:\n                filtered_bbox.append([x, y, w, h])\n                filtered_class.append(all_class[i])\n\n    # 将列表转换为 numpy 数组\n    filtered_bbox = np.array(filtered_bbox)\n    filtered_class = filtered_class\n    return filtered_bbox, filtered_class\n\n\nclass Dataset_coco(Dataset):\n    def __init__(\n        self,\n        args=None,\n        is_test=False,\n        split='train',\n    ):\n        self.args = args\n        self.is_test = is_test\n        self.split = split\n\n        transform = transforms.Compose([\n            # transforms.Resize(384),\n            # transforms.CenterCrop(384),\n            transforms.ToTensor(),\n        ])\n\n        if self.split == 'train':\n            coco_data = dset.CocoDetection(\n                root='/home/jovyan/multi-modal-datasets/public/coco/train2017',\n                annFile='/home/jovyan/multi-modal-datasets/public/coco/annotations/instances_train2017.json',\n                transform=transform\n            )\n        elif self.split == 'val14':\n            coco_data = dset.CocoDetection(\n                root='/home/jovyan/multi-modal-datasets/public/coco/val2014',\n                annFile='/home/jovyan/multi-modal-datasets/public/coco/annotations/instances_val2014.json',\n                transform=transform\n            )\n        elif self.split == 'val17':\n            coco_data = dset.CocoDetection(\n                root='/home/jovyan/multi-modal-datasets/public/coco/val2017',\n                annFile='/home/jovyan/multi-modal-datasets/public/coco/annotations/instances_val2017.json',\n                transform=transform\n            )\n        else:\n            assert False\n\n        self.dataset = self.coco_train = self.coco_val = coco_data\n\n        self.coco_caps = COCO('/home/jovyan/multi-modal-datasets/public/coco/annotations/captions_val2017.json')\n\n        # self.dataset = ConcatDataset([coco_train, coco_val])\n        # self.coco_train = coco_train\n        # self.coco_val = coco_val\n\n        print(f\"coco数据集大小: {len(self)}\")\n        # print(f\"验证集大小: {len(coco_val)}\")\n\n\n    # 获取类别名称的函数\n    def get_name(self, category_id):\n        category_info = self.coco_train.coco.loadCats(category_id)[0]\n        category_name = category_info['name']\n        return category_name\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, i):\n        data = self.dataset[i]\n        image, annotations = data\n\n        if len(annotations) == 0:\n            print('len(anno)==0!!!')\n            return self.__getitem__((i+1)%len(self)) #要不然找不到image_id\n        \n        obj_bbox = []\n        obj_class = []\n        for ano in annotations:\n            bbox = ano['bbox']\n            image_id = ano['image_id']\n            category_id = ano['category_id']\n            clas = self.get_name(category_id)\n            obj_bbox.append(bbox)\n            obj_class.append(clas)\n\n        image_pil = transforms.ToPILImage()(image)\n        image_pil, obj_bbox = resize_and_crop(image_pil, obj_bbox)\n        image =  transforms.ToTensor()(image_pil)\n        image = image*2-1\n        obj_bbox, obj_class = filter_box(obj_bbox, obj_class)\n        obj_bbox = obj_bbox/384\n        obj_bbox = obj_bbox.reshape(-1,4)\n        obj_bbox[:,2] = obj_bbox[:,0] + obj_bbox[:,2]\n        obj_bbox[:,3] = obj_bbox[:,1] + obj_bbox[:,3]\n\n        caps = self.coco_caps.imgToAnns[image_id]\n\n        if len(caps) == 0:\n            print('no caps!!!')\n            cap = ''\n            # cap = \" and \".join([f\"a {t}\" for t in obj_class])\n        else:\n            cap = random.choice(caps)['caption']\n        \n        return dict(\n            base_caption=cap,\n            obj_bbox=obj_bbox,\n            obj_class=obj_class,\n            image=image,\n            image_id=f\"{image_id:012d}\",\n        )\n        "}
{"type": "source_file", "path": "project/plangen/dataset/code_hico/debug_grit.py", "content": "#!/usr/bin/python\n#\n# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport os\nfrom collections import defaultdict\nimport random\nimport PIL\nfrom PIL import Image, ImageFont, ImageDraw\nimport numpy as np\nimport torch\nimport torchvision.transforms as T\nfrom torch.utils.data import Dataset\n\n# 原文件：\n#from layout_diffusion.dataset.util import image_normalize\n#from layout_diffusion.dataset.augmentations import RandomSampleCrop, RandomMirror\nfrom .dataset.util import image_normalize\nfrom .dataset.augmentations import RandomMirror, RandomSampleCrop, CenterSampleCrop\n\n# 修改的\n#from LayoutDiffusion.layout_diffusion.dataset.util import image_normalize\n#from LayoutDiffusion.layout_diffusion.dataset.augmentations import RandomMirror, RandomSampleCrop\n#CenterSampleCrop=RandomSampleCrop## 没有CenterSampleCrop，先用RandomSampleCrop代替\n\n\n\n\nimport pdb\n\n\nImage.MAX_IMAGE_PIXELS = None\n\nclass GritSceneGraphDataset(Dataset):\n    def __init__(self, list_tokenizers, grit_json, \n                 image_dir, instances_json, stuff_json=None,\n                 stuff_only=True, image_size=(64, 64), mask_size=16,\n                 max_num_samples=None,proportion_empty_prompts=0.05,\n                 include_relationships=True, min_object_size=0.02,\n                 min_objects_per_image=3, max_objects_per_image=8, left_right_flip=False,\n                 include_other=False, instance_whitelist=None, stuff_whitelist=None, mode='train',\n                 use_deprecated_stuff2017=False, deprecated_coco_stuff_ids_txt='', filter_mode='LostGAN',\n                 use_MinIoURandomCrop=False,\n                 return_origin_image=False, specific_image_ids=None,\n                 args=None,\n                 ):\n        \"\"\"\n        A PyTorch Dataset for loading Coco and Coco-Stuff annotations and converting\n        them to scene graphs on the fly.\n\n        Inputs:\n        - image_dir: Path to a directory where images are held\n        - instances_json: Path to a JSON file giving COCO annotations\n        - stuff_json: (optional) Path to a JSON file giving COCO-Stuff annotations\n        - stuff_only: (optional, default True) If True then only iterate over\n          images which appear in stuff_json; if False then iterate over all images\n          in instances_json.\n        - image_size: Size (H, W) at which to load images. Default (64, 64).\n        - mask_size: Size M for object segmentation masks; default 16.\n        - max_num_samples: If None use all images. Other wise only use images in the\n          range [0, max_num_samples). Default None.\n        - include_relationships: If True then include spatial relationships; if\n          False then only include the trivial __in_image__ relationship.\n        - min_object_size: Ignore objects whose bounding box takes up less than\n          this fraction of the image.\n        - min_objects_per_image: Ignore images which have fewer than this many\n          object annotations.\n        - max_objects_per_image: Ignore images which have more than this many\n          object annotations.\n        - include_other: If True, include COCO-Stuff annotations which have category\n          \"other\". Default is False, because I found that these were really noisy\n          and pretty much impossible for the system to model.\n        - instance_whitelist: None means use all instance categories. Otherwise a\n          list giving a whitelist of instance category names to use.\n        - stuff_whitelist: None means use all stuff categories. Otherwise a list\n          giving a whitelist of stuff category names to use.\n        \"\"\"\n        super(Dataset, self).__init__()\n\n        self.args = args\n\n        self.return_origin_image = return_origin_image\n        if self.return_origin_image:\n            self.origin_transform = T.Compose([\n                T.ToTensor(),\n                image_normalize()\n            ])\n\n        if stuff_only and stuff_json is None:\n            print('WARNING: Got stuff_only=True but stuff_json=None.')\n            print('Falling back to stuff_only=False.')\n\n        self.proportion_empty_prompts = proportion_empty_prompts\n        self.use_deprecated_stuff2017 = use_deprecated_stuff2017\n        self.deprecated_coco_stuff_ids_txt = deprecated_coco_stuff_ids_txt\n        self.mode = mode\n        self.max_objects_per_image = max_objects_per_image\n        self.image_dir = image_dir\n        self.mask_size = mask_size\n        self.max_num_samples = max_num_samples\n        self.include_relationships = include_relationships\n        self.filter_mode = filter_mode\n        self.image_size = image_size\n        self.min_image_size = min(self.image_size)\n        self.min_object_size = min_object_size\n        self.left_right_flip = left_right_flip\n        if left_right_flip:\n            self.random_flip = RandomMirror()\n\n        self.layout_length = self.max_objects_per_image + 2\n\n        self.use_MinIoURandomCrop = use_MinIoURandomCrop\n        #self.use_MinIoURandomCrop = False\n        if use_MinIoURandomCrop:\n            self.MinIoURandomCrop = RandomSampleCrop()\n            self.MinIoUCenterCrop = CenterSampleCrop()\n\n        self.transform = T.Compose([\n            T.ToTensor(),\n            T.Resize(size=image_size, antialias=True),\n            image_normalize()\n        ])\n\n        self.transform_cond = T.Compose([\n            T.ToTensor(),\n            #T.Resize(size=image_size, antialias=True),\n            #image_normalize()\n        ])\n\n        self.total_num_bbox = 0\n        self.total_num_invalid_bbox = 0\n\n        #self.tokenizers = tokenizers\n        self.tokenizers_one, self.tokenizers_two = list_tokenizers\n\n        # read grit-20m data\n        with open(grit_json, 'r') as f:\n            grit_data = json.load(f)\n\n        self.image_ids = []\n        self.image_id_to_objects = {}\n        for idx, obj_data in grit_data.items():\n            f_img_path = obj_data[\"f_path\"]\n            #list_chunks = obj_data[\"noun_chunks\"]\n            list_exps = obj_data[\"ref_exps\"]\n            image_w = obj_data[\"width\"]\n            image_h = obj_data[\"height\"]\n            caption = obj_data[\"caption\"]\n            url = obj_data[\"url\"]\n\n            obj_nums = len(list_exps)\n            # get sub-caption\n            list_bbox_info = []\n            for box_info in list_exps:\n                phrase_s, phrase_e, x1_norm, y1_norm, x2_norm, y2_norm, score = box_info\n                phrase_s = int(phrase_s)\n                phrase_e = int(phrase_e)\n                phrase = caption[phrase_s:phrase_e]\n                x1, y1, x2, y2 = int(x1_norm * image_w), int(y1_norm * image_h), int(x2_norm * image_w), int(y2_norm * image_h)\n\n                x1, y1 = min(x1, image_w), min(y1, image_h)\n                x2, y2 = min(x2, image_w), min(y2, image_h)\n                if int(x2 - x1) < 0.05 * image_w or int(y2 - y1) < 0.05 * image_h:\n                    continue\n                \n                #list_bbox_info.append([phrase, [x1, y1, x2, y2]])\n                list_bbox_info.append([phrase, [x1, y1, int(x2 - x1), int(y2 - y1)]])\n                if len(list_bbox_info) >= self.max_objects_per_image:\n                    break\n            if len(list_bbox_info) == 0:\n                continue\n\n            self.image_ids.append([idx, f_img_path, obj_nums])\n            self.image_id_to_objects.setdefault(idx, [caption, image_w, image_h, list_bbox_info, url])\n\n        print (\"data nums : %s.\" % len(self.image_id_to_objects))\n\n    def filter_invalid_bbox(self, H, W, bbox, is_valid_bbox, verbose=False):\n        #pdb.set_trace()\n        for idx, obj_bbox in enumerate(bbox):\n            if not is_valid_bbox[idx]:\n                continue\n            self.total_num_bbox += 1\n\n            x, y, w, h = obj_bbox\n\n            if (x >= W) or (y >= H):\n                is_valid_bbox[idx] = False\n                self.total_num_invalid_bbox += 1\n                if verbose:\n                    print(\n                        'total_num = {}, invalid_num = {}, x = {}, y={}, w={}, h={}, W={}, H={}'.format(\n                            self.total_num_bbox, self.total_num_invalid_bbox, x, y, w, h, W, H,\n                        )\n                    )\n                continue\n\n            x0, y0, x1, y1 = x, y, x + w, y + h\n            x1 = np.clip(x + w, 1, W)\n            y1 = np.clip(y + h, 1, H)\n\n            if (y1 - y0 < self.min_object_size * H) or (x1 - x0 < self.min_object_size * W):\n                is_valid_bbox[idx] = False\n                self.total_num_invalid_bbox += 1\n                if verbose:\n                    print(\n                        'total_num = {}, invalid_num = {}, x = {}, y={}, w={}, h={}, W={}, H={}'.format(\n                            self.total_num_bbox, self.total_num_invalid_bbox, x, y, w, h, W, H,\n                        )\n                    )\n                continue\n            bbox[idx][0], bbox[idx][1], bbox[idx][2], bbox[idx][3] = x0, y0, x1, y1\n\n        return bbox, is_valid_bbox\n\n    def total_objects(self):\n        total_objs = 0\n        for i, image_info in enumerate(self.image_ids):\n            total_objs += image_info[2]\n        return total_objs\n\n    def get_init_meta_data(self, image_id, caption):\n        #self.layout_length = self.max_objects_per_image + 2\n        #pdb.set_trace()\n        layout_length = self.layout_length\n        #clip_text_ids = self.tokenize_caption(\"\")\n        list_clip_text_ids = self.tokenize_caption(\"\")\n        meta_data = {\n            'obj_bbox': torch.zeros([layout_length, 4]),\n            'obj_class': [\"\"] * layout_length,\n            'is_valid_obj': torch.zeros([layout_length]),\n            'upd_is_valid_obj': torch.zeros([layout_length]),\n            #'obj_class_text_ids': clip_text_ids.repeat(layout_length, 1),\n            'obj_class_text_ids': [list_clip_text_ids] * layout_length,\n            #'obj_class': torch.LongTensor(layout_length).fill_(self.vocab['object_name_to_idx']['__null__']),\n            #'filename': self.image_id_to_filename[image_id].replace('/', '_').split('.')[0]\n        }\n\n        # The first object will be the special __image__ object\n        meta_data['obj_bbox'][0] = torch.FloatTensor([0, 0, 1, 1])\n        #meta_data['obj_class'][0] = self.vocab['object_name_to_idx']['__image__']\n        meta_data['obj_class'][0] = caption\n        meta_data['is_valid_obj'][0] = 1.0\n        meta_data['upd_is_valid_obj'][0] = 1.0\n\n        #clip_text_ids = self.tokenize_caption(caption)\n        #meta_data['obj_class_text_ids'][0] = clip_text_ids\n        list_clip_text_ids = self.tokenize_caption(caption)\n        meta_data['obj_class_text_ids'][0] = list_clip_text_ids\n\n        return meta_data\n\n    def load_image(self, image_path):\n        with open(image_path, 'rb') as f:\n            with PIL.Image.open(f) as image:\n                image = image.convert('RGB')\n        return image\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def tokenize_caption(self, caption):\n        captions = []\n        #if random.random() < 0.05:\n        if random.random() < self.proportion_empty_prompts:\n            captions.append(\"\")\n        else:\n            captions.append(caption)\n        clip_inputs_one = self.tokenizers_one(\n            captions, max_length = self.tokenizers_one.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n        )\n        clip_inputs_two = self.tokenizers_two(\n            captions, max_length = self.tokenizers_two.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n        )\n        return [clip_inputs_one.input_ids, clip_inputs_two.input_ids]\n\n    def resize_img(self, image, obj_bbox, obj_class):\n        # resize to self.resolution\n        #pdb.set_trace()\n        ori_width, ori_height = image.size\n        #target_size = self.image_size\n        #res_height, res_width = self.image_size\n        res_min_size = self.min_image_size\n        if ori_height < ori_width:\n            resize_height = res_min_size\n            aspect_r = ori_width / ori_height\n            resize_width = int(resize_height * aspect_r)\n            im_resized = image.resize((resize_width, resize_height))\n\n            rescale = resize_height / ori_height\n            re_obj_bbox = obj_bbox * rescale\n        else:\n            resize_width = res_min_size\n            aspect_r = ori_height / ori_width\n            resize_height = int(resize_width * aspect_r)\n            im_resized = image.resize((resize_width, resize_height))\n\n            rescale = resize_height / ori_height\n            re_obj_bbox = obj_bbox * rescale\n\n        return im_resized, re_obj_bbox, obj_class\n\n    def draw_image(self, image, obj_bbox, obj_class, img_save):\n        dw_img = PIL.Image.fromarray(np.uint8(image * 255))\n        draw = PIL.ImageDraw.Draw(dw_img)\n        color = tuple(np.random.randint(0, 255, size=3).tolist())\n        #draw.rectangle([100, 100, 300, 300], outline = (0, 255, 255), fill = (255, 0, 0), width = 10)\n        for iix in range(len(obj_bbox)):\n            rec = obj_bbox[iix]\n            d_rec = [int(xx) for xx in rec]\n            draw.rectangle(d_rec, outline = color, width = 3)\n\n            text = obj_class[iix]\n            font = ImageFont.truetype(\"/home/jovyan/boomcheng-data/tools/font/msyh.ttf\", size=10)\n            draw.text((d_rec[0], d_rec[1]), text, font = font, fill=\"red\", align=\"left\")\n        dw_img.save(img_save)\n\n    def draw_image_xywh(self, image, obj_bbox, obj_class, img_save):\n        dw_img = PIL.Image.fromarray(np.uint8(image * 255))\n        draw = PIL.ImageDraw.Draw(dw_img)\n        color = tuple(np.random.randint(0, 255, size=3).tolist())\n        #draw.rectangle([100, 100, 300, 300], outline = (0, 255, 255), fill = (255, 0, 0), width = 10)\n        for iix in range(len(obj_bbox)):\n            rec = obj_bbox[iix]\n            d_rec = [int(xx) for xx in rec]\n            d_rec[2] += d_rec[0]\n            d_rec[3] += d_rec[1]\n            draw.rectangle(d_rec, outline = color, width = 3)\n\n            text = obj_class[iix]\n            font = ImageFont.truetype(\"/home/jovyan/boomcheng-data/tools/font/msyh.ttf\", size=10)\n            draw.text((d_rec[0], d_rec[1]), text, font = font, fill=\"red\", align=\"left\")\n        dw_img.save(img_save)\n\n\n    def __getitem__(self, index):\n        \"\"\"\n        Get the pixels of an image, and a random synthetic scene graph for that\n        image constructed on-the-fly from its COCO object annotations. We assume\n        that the image will have height H, width W, C channels; there will be O\n        object annotations, each of which will have both a bounding box and a\n        segmentation mask of shape (M, M). There will be T triples in the scene\n        graph.\n\n        Returns a tuple of:\n        - image: FloatTensor of shape (C, H, W)\n        - objs: LongTensor of shape (O,)\n        - boxes: FloatTensor of shape (O, 4) giving boxes for objects in\n          (x0, y0, x1, y1) format, in a [0, 1] coordinate system\n        - masks: LongTensor of shape (O, M, M) giving segmentation masks for\n          objects, where 0 is background and 0 is object.\n\n        \"\"\"\n        f_idx, f_image_path, f_obj_nums = self.image_ids[index]\n\n        image = self.load_image(f_image_path)\n        W, H = image.size\n        caption, image_w, image_h, list_bbox_info, url = self.image_id_to_objects[f_idx]\n\n        if W != image_w or H != image_h:\n            index = 0\n            f_idx, f_image_path, f_obj_nums = self.image_ids[index]\n            image = self.load_image(f_image_path)\n            caption, image_w, image_h, list_bbox_info, url = self.image_id_to_objects[f_idx]\n            \n\n        f_img_nm = f_image_path.split(\"/\")[-1]\n        #image.save(\"./image_demo/%s-s0-base.jpg\" % f_img_nm)\n        #print (f_img_nm, image_w, image_h)\n\n        num_obj = len(list_bbox_info)\n        obj_bbox = [obj[1] for obj in list_bbox_info]   # [x, y, w, h]\n        obj_bbox = np.array(obj_bbox)\n        obj_class = [obj[0] for obj in list_bbox_info]\n        is_valid_obj = [True for _ in range(num_obj)]\n\n        #pdb.set_trace()\n        #self.draw_image_xywh(np.array(image, dtype=np.float32) / 255.0, obj_bbox, obj_class, \"./image_demo/%s-s0-base-bbox.jpg\" % f_img_nm)\n\n        #pdb.set_trace()\n        # filter invalid bbox\n        # bbox : [x, y, w, h] -> [x1, y1, x2, y2]\n        if True:\n            W, H = image.size\n            obj_bbox, is_valid_obj = self.filter_invalid_bbox(H=H, W=W, bbox=obj_bbox, is_valid_bbox=is_valid_obj)\n\n        if True:\n            image, obj_bbox, obj_class = self.resize_img(image, obj_bbox, obj_class)\n\n        # import pdb;pdb.set_trace()\n        # \"a.png\"\n        # if self.args.debug_data:\n        #     # image.save(\"./image_demo/%s-s1-resize-out.jpg\" % f_img_nm)\n        #     self.draw_image(np.array(image, dtype=np.float32) / 255.0, obj_bbox, obj_class, \"./image_demo/%s-s1-resize-bbox.jpg\" % f_img_nm)\n\n        if self.return_origin_image:\n            origin_image = np.array(image, dtype=np.float32) / 255.0\n        image = np.array(image, dtype=np.float32) / 255.0\n\n        # self.draw_image(image, obj_bbox, obj_class, f\"./a_{f_img_nm}.jpg\")\n        # a_000000945.jpg.jpg\n\n        H, W, _ = image.shape\n        # get meta data\n        #pdb.set_trace()\n        meta_data = self.get_init_meta_data(f_idx, caption)\n        #meta_data['width'], meta_data['height'] = W, H\n        meta_data['width'], meta_data['height'] = image_w, image_h\n        meta_data['original_sizes_hw'] = (image_h, image_w)\n        meta_data['num_obj_ori'] = num_obj\n\n        #pdb.set_trace()\n        for iid in range(len(is_valid_obj)):\n            meta_data['is_valid_obj'][1+iid] = is_valid_obj[iid]\n\n        # flip\n        #if False:\n        if self.left_right_flip and random.random() < 0.5:\n            image, obj_bbox, obj_class = self.random_flip(image, obj_bbox, obj_class)\n        \n        base_class = obj_class\n        base_bbox = obj_bbox\n        base_image = PIL.Image.fromarray(np.uint8(image * 255))\n\n        #pdb.set_trace()\n        #self.draw_image(image, obj_bbox, obj_class, \"./image_demo/%s-s2-flip-bbox.jpg\" % f_img_nm)\n\n        # random crop image and its bbox\n        #if False:\n        #pdb.set_trace()\n        crop_top_left = (0,0)\n        if self.use_MinIoURandomCrop:#true\n            r_obj_bbox = obj_bbox[is_valid_obj]\n            r_obj_class = [obj_class[ii] for ii in range(len(is_valid_obj)) if is_valid_obj[ii]]\n\n            #try:\n            if True:\n                crop_top_left, image, upd_obj_bbox, upd_obj_class, upd_is_valid_obj = self.MinIoUCenterCrop(image, r_obj_bbox, r_obj_class)\n            #except:\n            #    print (f\"=======================, index:{index}, f_idx:{f_idx}\")\n            #    return self.__getitem__(0)\n                \n\n            meta_data['new_height'] = image.shape[0]\n            meta_data['new_width'] = image.shape[1]\n            H, W, _ = image.shape\n        else:\n            #### add\n            upd_is_valid_obj = is_valid_obj\n            upd_obj_bbox = obj_bbox\n            upd_obj_class = obj_class\n            # upd_is_valid_obj = [1]*len(r_obj_bbox)\n\n        meta_data[\"crop_top_lefts\"] = crop_top_left     # (x, y)\n        for iid in range(len(upd_is_valid_obj)):\n            meta_data['upd_is_valid_obj'][1+iid] = int(upd_is_valid_obj[iid])\n\n        obj_bbox, obj_class = upd_obj_bbox, upd_obj_class\n        #self.draw_image(image, obj_bbox, obj_class, \"./image_demo/%s-s3-crop-bbox.jpg\" % f_img_nm)\n\n        #pdb.set_trace()\n        H, W, C = image.shape\n        ############### condition_image #############\n        list_cond_image = []\n        cond_image = np.zeros_like(image, dtype=np.uint8)\n        list_cond_image.append(cond_image)\n        for iit in range(len(obj_bbox)):\n            dot_bbox = obj_bbox[iit]\n            dx1, dy1, dx2, dy2 = [int(xx) for xx in dot_bbox]\n            cond_image = np.zeros_like(image, dtype=np.uint8)\n            #cond_image[dy1:dy2, dx1:dx2] = 255\n            cond_image[dy1:dy2, dx1:dx2] = 1\n            list_cond_image.append(cond_image)\n\n            ##print (dot_bbox, image.shape, obj_class[iit])\n            #im = PIL.Image.fromarray(cond_image*255)\n            #im.save(\"./image_demo/%s-cond-%s.jpg\" % (f_img_nm, iit))\n\n        # PIL.Image.fromarray(np.uint8(meta_data['cond_image'][0])).save('a0.jpg')\n\n        #obj_bbox = torch.FloatTensor(obj_bbox[is_valid_obj])\n        #obj_class = [obj_class[iv] for iv in range(len(is_valid_obj)) if is_valid_obj[iv]]\n        obj_bbox = torch.FloatTensor(obj_bbox)\n\n        obj_bbox[:, 0::2] = obj_bbox[:, 0::2] / W\n        obj_bbox[:, 1::2] = obj_bbox[:, 1::2] / H\n\n        num_selected = min(obj_bbox.shape[0], self.max_objects_per_image)\n        selected_obj_idxs = random.sample(range(obj_bbox.shape[0]), num_selected)#[2, 0, 1]\n\n        meta_data['obj_bbox'][1:1 + num_selected] = obj_bbox[selected_obj_idxs]\n        list_text_select = [obj_class[iv] for iv in selected_obj_idxs]\n        meta_data['obj_class'][1:1 + num_selected] = list_text_select #['Pink Vans, with pink roses on the outer side', 'Pink Vans, with pink roses on the outer side', 'pink roses on the outer side', 'pink roses on the outer side', '', '', '', '', '', '']\n\n        obj_cond_image = np.stack(list_cond_image, axis=0)\n        meta_data['cond_image'] = np.zeros([self.layout_length, H, W, C])\n        meta_data['cond_image'][0:len(list_cond_image)] = obj_cond_image\n        # torch_resize = Resize([256,256])\n        # torch_resize(torch.from_numpy(meta_data['cond_image'].transpose(0,3,1,2))).permute(0,2,3,1)\n        ##meta_data['cond_image'] = self.transform_cond(meta_data['cond_image'])\n\n        #meta_data['cond_image'] = torch.from_numpy(meta_data['cond_image'].transpose(0,3,1,2)).permute(0,2,3,1)\n        #meta_data['cond_image'][1:1 + num_selected] = torch.from_numpy(obj_cond_image[1:][selected_obj_idxs])\n        meta_data['cond_image'][1:1 + num_selected] = obj_cond_image[1:][selected_obj_idxs]\n        meta_data['cond_image'] = torch.from_numpy(meta_data['cond_image'].transpose(0,3,1,2))\n\n        #pdb.set_trace()\n        # if self.args is not None and self.args.debug_data:\n        #     self.draw_image(image,  np.uint8(obj_bbox.numpy() * (self.min_image_size-1)), obj_class, \"./image_demo/%s-base.jpg\" % f_img_nm)\n        #for iii in range(num_selected):\n        #    PIL.Image.fromarray(np.uint8(meta_data['cond_image'][1+iii] * 255)).save(\"./image_demo/%s-cond-%s.jpg\" % (f_img_nm, iii))\n            \n        list_clip_text_ids = self.tokenize_caption(caption)\n        meta_data['base_caption'] = caption\n        meta_data['base_class_text_ids'] = list_clip_text_ids\n\n        #meta_data['is_valid_obj'][1:1 + num_selected] = 1.0\n        meta_data['num_selected'] =  1 + num_selected\n        meta_data['url'] = url\n        #meta_data['num_obj_select'] = num_selected\n\n        # tokenizer\n        #pdb.set_trace()\n        for iit in range(len(list_text_select)):\n            text = list_text_select[iit]\n            list_clip_text_ids = self.tokenize_caption(text)\n            meta_data['obj_class_text_ids'][1+iit] = list_clip_text_ids\n\n        if self.return_origin_image:\n            meta_data['origin_image'] = self.origin_transform(origin_image)\n\n        #trans_image = base_image\n        #tmp_save_name = \"./image_demo/%s-s4-ori-draw-crop.jpg\" % f_img_nm\n        #crop_bbox = [crop_top_left[0], crop_top_left[1], crop_top_left[0] + 512,crop_top_left[1]+512]\n        #self.draw_image(np.array(trans_image, dtype=np.float32) / 255.0, [crop_bbox], [caption], tmp_save_name)\n\n        ## upd_is_valid_obj\n        #tmp_save_name = \"./image_demo/%s-s5-ori-draw-crop-bbox.jpg\" % f_img_nm\n        ##pdb.set_trace()\n        #trans_image = self.load_image(tmp_save_name)\n        #upd_is_valid_obj = [int(x) for x in upd_is_valid_obj]\n        #upd_base_bbox = []\n        #upd_base_class = []\n        ##upd_base_class.append(caption)\n        ##crop_bbox = [crop_top_left[0], crop_top_left[1], crop_top_left[0] + 512,crop_top_left[1]+512]\n        ##upd_base_bbox.append(crop_bbox)\n        #for iid in range(len(upd_is_valid_obj)):\n        #    if upd_is_valid_obj[iid]:\n        #        upd_base_class.append(base_class[iid])\n        #        upd_base_bbox.append(base_bbox[iid])\n        #self.draw_image(np.array(trans_image, dtype=np.float32) / 255.0, upd_base_bbox, upd_base_class, tmp_save_name)\n\n        #return self.transform(image), meta_data\n        meta_data[\"pixel_values\"] = self.transform(image)\n\n        meta_data[\"image_path\"] = f_image_path\n\n        # meta_data[\"image_path\"] = \n        return meta_data\n        #return self.transform(image), meta_data\n\ndef grit_collate_fn(examples):\n    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()   # [bs, 3, 512, 512]\n\n    layo_cond_image = [example[\"cond_image\"] for example in examples]   # bs, [10, 3, 512, 512]\n    layo_cond_image = torch.stack(layo_cond_image)  # [bs, 10, 3, 512, 512]\n    layo_cond_image = layo_cond_image.to(memory_format=torch.contiguous_format)\n\n    original_sizes = [example[\"original_sizes_hw\"] for example in examples] # bs\n    crop_top_lefts = [example[\"crop_top_lefts\"] for example in examples]    # bs\n    base_caption = [example[\"base_caption\"] for example in examples]        # bs\n    num_selected = [example[\"num_selected\"] for example in examples]\n\n    base_input_ids_one = torch.concat([example[\"base_class_text_ids\"][0] for example in examples])  # bs, 77\n    base_input_ids_two = torch.concat([example[\"base_class_text_ids\"][1] for example in examples])  # bs, 77\n\n    list_input_ids_one = []\n    list_input_ids_two = []\n    for example in examples:\n        list_input_text_ids = example['obj_class_text_ids']\n        clip_input_ids_one = torch.concat([x[0] for x in list_input_text_ids])  # [10, 77]\n        clip_input_ids_two = torch.concat([x[1] for x in list_input_text_ids])  # [10, 77]\n        list_input_ids_one.append(clip_input_ids_one)\n        list_input_ids_two.append(clip_input_ids_two)\n\n    layo_input_ids_one = torch.stack(list_input_ids_one)    # bs, 10, 77\n    layo_input_ids_two = torch.stack(list_input_ids_two)    # bs, 10, 77\n\n    out_data = {\n        \"pixel_values\": pixel_values,\n        \"cond_image\": layo_cond_image,\n        \"original_sizes_hw\": original_sizes,\n        \"crop_top_lefts\": crop_top_lefts,\n        \"num_selected\": num_selected,\n        #\"base_caption\": base_caption,\n        \"base_input_ids_one\": base_input_ids_one,\n        \"base_input_ids_two\": base_input_ids_two,\n        \"layo_input_ids_one\": layo_input_ids_one,\n        \"layo_input_ids_two\": layo_input_ids_two,\n    }\n    return out_data\n\ndef grit_collate_fn_for_layout(batch):\n    all_meta_data = defaultdict(list)\n    all_imgs = []\n\n    #pdb.set_trace()\n    for i, (img, meta_data) in enumerate(batch):\n        all_imgs.append(img[None])\n        for key, value in meta_data.items():\n            all_meta_data[key].append(value)\n\n    all_imgs = torch.cat(all_imgs)\n    for key, value in all_meta_data.items():\n        #if key in ['obj_bbox', 'obj_class', 'is_valid_obj'] or key.startswith('labels_from_layout_to_image_at_resolution'):\n        if key in ['obj_bbox'] or key.startswith('labels_from_layout_to_image_at_resolution'):\n            all_meta_data[key] = torch.stack(value)\n\n    return all_imgs, all_meta_data\n\n\ndef build_grit_dsets(cfg, list_tokenizer, mode='train', args=None):\n    assert mode in ['train', 'val', 'test']\n    params = cfg.data.parameters\n    dataset = GritSceneGraphDataset(\n        list_tokenizers=list_tokenizer,\n        grit_json=params.grit_json,\n        mode=mode,\n        filter_mode=params.filter_mode,\n        stuff_only=params.stuff_only,\n        proportion_empty_prompts=params.proportion_empty_prompts,\n        image_size=(params.image_size, params.image_size),\n        mask_size=params.mask_size_for_layout_object,\n        min_object_size=params.min_object_size,\n        min_objects_per_image=params.min_objects_per_image,\n        max_objects_per_image=params.max_objects_per_image,\n        instance_whitelist=params.instance_whitelist,\n        stuff_whitelist=params.stuff_whitelist,\n        include_other=params.include_other,\n        include_relationships=params.include_relationships,\n        use_deprecated_stuff2017=params.use_deprecated_stuff2017,\n        deprecated_coco_stuff_ids_txt=os.path.join(params.root_dir, params[mode].deprecated_stuff_ids_txt),\n        image_dir=os.path.join(params.root_dir, params[mode].image_dir),\n        instances_json=os.path.join(params.root_dir, params[mode].instances_json),\n        stuff_json=os.path.join(params.root_dir, params[mode].stuff_json),\n        max_num_samples=params[mode].max_num_samples,\n        left_right_flip=params[mode].left_right_flip,\n        use_MinIoURandomCrop=params[mode].use_MinIoURandomCrop,\n        return_origin_image=params.return_origin_image,\n        specific_image_ids=params[mode].specific_image_ids,\n        args=args,\n    )\n\n    num_objs = dataset.total_objects()\n    num_imgs = len(dataset)\n    print('%s dataset has %d images and %d objects' % (mode, num_imgs, num_objs))\n    print('(%.2f objects per image)' % (float(num_objs) / num_imgs))\n\n    return dataset\n\nif __name__ == '__main__':\n\n    from omegaconf import OmegaConf\n    cfg_data = OmegaConf.load(\"/home/jovyan/boomcheng-data/aigc/LayoutProj/diffusers_0263/examples/controlnet/latent_LayoutDiffusion_large.yaml\")\n    # cfg_data = OmegaConf.load(\"./code_chengbo/latent_LayoutDiffusion_large.yaml\")\n    from transformers import AutoTokenizer\n    #pretrained_model = \"/home/jovyan/fast-data/stable-diffusion-xl-base-1.0\"\n    pretrained_model = \"/home/jovyan/boomcheng-data-shcdt/herunze/models/stable-diffusion-xl-base-1.0\"\n    tokenizer_one = AutoTokenizer.from_pretrained(\n                pretrained_model,\n                subfolder=\"tokenizer\",\n                revision=None,\n                use_fast=False,\n            )\n    tokenizer_two = AutoTokenizer.from_pretrained(\n                pretrained_model,\n                subfolder=\"tokenizer_2\",\n                revision=None,\n                use_fast=False,\n            )\n\n    dataset = build_grit_dsets(cfg_data, [tokenizer_one, tokenizer_two], mode='train')\n\n    if True:\n        #for ii in range(len(dataset)):\n        for ii in range(852, 860):\n            meta_data = dataset[ii]#dict_keys(['obj_bbox', 'obj_class', 'is_valid_obj', 'upd_is_valid_obj', 'obj_class_text_ids', 'width', 'height', 'original_sizes_hw', 'num_obj_ori', 'new_height', 'new_width', 'crop_top_lefts', 'cond_image', 'base_caption', 'base_class_text_ids', 'num_selected', 'url', 'pixel_values'])\n            # torch.Size([3, 512, 512]) -1~1\n\n            #image, meta_data = dataset[ii]\n            #pdb.set_trace()\n            #print (meta_data)\n            print (ii,meta_data[\"pixel_values\"].shape)\n            pdb.set_trace()\n            pass\n\n    \"\"\"\n    from prefetch_generator import BackgroundGenerator\n    class DataLoaderUpd(torch.utils.data.DataLoader):                                                                        \n\n        def __iter__(self):\n            return BackgroundGenerator(super().__iter__())\n\n    train_dataloader = DataLoaderUpd(\n            dataset,\n            #collate_fn=grit_collate_fn_for_layout,\n            collate_fn=grit_collate_fn,\n            batch_size=4,\n            num_workers=0,\n            pin_memory=True\n        )\n\n    for step, batch in enumerate(train_dataloader):\n        #batch_cond = batch\n        pdb.set_trace()\n        #print (step)\n        batch_images = batch[\"pixel_values\"]\n        bsize = 4\n        for cid in range(bsize):\n            bsid = min(batch[\"num_selected\"][cid], 4)\n            d_cond_image = batch[\"cond_image\"][cid][:bsid]     # [10, 3, 512, 512]\n            d_layo_input_ids_one = batch[\"layo_input_ids_one\"][cid][:bsid]      # 10, 77\n            d_layo_input_ids_two = batch[\"layo_input_ids_two\"][cid][:bsid]      # 10, 77\n\n            d_base_input_ids_one = batch[\"base_input_ids_one\"][cid]      # 77\n            d_base_input_ids_two = batch[\"base_input_ids_two\"][cid]      # 77 \n\n            d_base_input_ids_one = torch.repeat_interleave(torch.unsqueeze(d_base_input_ids_one, dim=0), repeats=bsid, dim=0)   # bsid, 77\n            d_base_input_ids_two = torch.repeat_interleave(torch.unsqueeze(d_base_input_ids_two, dim=0), repeats=bsid, dim=0)   # bsid, 77\n\n            d_original_sizes_hw = batch[\"original_sizes_hw\"][cid]   # [512, 768]\n            d_crop_top_lefts = batch[\"crop_top_lefts\"][cid]         # [88, 0]\n\n    \"\"\"\n\n\n\n"}
{"type": "source_file", "path": "project/plangen/dataset/edit/dataset_edit_coco_edit.py", "content": "from torch.utils.data import Dataset\nimport random\nfrom copy import deepcopy\nfrom torchvision.transforms import Resize\nfrom glob import glob\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport random\nfrom copy import deepcopy\nfrom torchvision.transforms import Resize\nfrom datasets import load_dataset\nfrom src.utils.funcs import convert_to_np, load_jsonl\nimport numpy as np\nimport os\nfrom src.utils.funcs import *\n\nclass Dataset_edit_coco_edit(Dataset):\n    def __init__(\n        self,\n        args=None,\n        is_test=False,\n    ):\n        self.args = args\n        self.is_test = is_test\n        # self.datas = load_json(\"project/janus/dataset/edit/edit.json\")\n\n    def __len__(self):\n        return 200\n\n    def __getitem__(self, i):\n        base_caption = ''\n\n        path = self.args.coco_200_path\n\n        image_path = f'{path}/image/{i}.png'\n        mask_path = f'{path}/mask/{i}.png'\n        box_path = f'{path}/box/{i}.json'\n        box_new_path = f'{path}/box_new/{i}.json'\n        image = load2ts(image_path)\n        data1 = load_json(box_path)\n        data2 = load_json(box_new_path)\n        obj_bbox_1, obj_class_1 = data1['obj_bbox'], data1['obj_class']\n        obj_bbox_2, obj_class_2 = data2['obj_bbox'], data2['obj_class']\n\n        obj_bbox_1 = torch.tensor(obj_bbox_1).reshape(1,4)\n        obj_bbox_2 = torch.tensor(obj_bbox_2).reshape(1,4)\n\n        obj_bbox_edit = torch.cat([obj_bbox_1, obj_bbox_2], dim=0)\n        obj_bbox = obj_bbox_2\n        obj_class = [obj_class_2]\n\n        obj_bbox_neg = torch.zeros((0,4))\n        obj_class_neg = []\n\n        ret = dict(\n            base_caption=base_caption,\n            image=image,\n            image_path=image_path,\n            obj_class=obj_class,\n            obj_bbox=obj_bbox,\n            obj_bbox_edit=obj_bbox_edit,\n            obj_class_neg=obj_class_neg,\n            obj_bbox_neg=obj_bbox_neg,\n        )\n        return ret"}
{"type": "source_file", "path": "project/plangen/dataset/edit/dataset_edit_coco_rm.py", "content": "from torch.utils.data import Dataset\nimport random\nfrom copy import deepcopy\nfrom torchvision.transforms import Resize\nfrom glob import glob\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport random\nfrom copy import deepcopy\nfrom torchvision.transforms import Resize\nfrom datasets import load_dataset\nfrom src.utils.funcs import convert_to_np, load_jsonl\nimport numpy as np\nimport os\nfrom src.utils.funcs import *\n\nclass Dataset_edit_coco_rm(Dataset):\n    def __init__(\n        self,\n        args=None,\n        is_test=False,\n    ):\n        self.args = args\n        self.is_test = is_test\n        # self.datas = load_json(\"project/janus/dataset/edit/edit.json\")\n\n    def __len__(self):\n        return 200\n\n    def __getitem__(self, i):\n        base_caption = ''\n\n        image_path = f'/home/jovyan/boomcheng-data-shcdt/herunze/code/base/gen_data/coco_rm_200/image/{i}.png'\n        mask_path = f'/home/jovyan/boomcheng-data-shcdt/herunze/code/base/gen_data/coco_rm_200/mask/{i}.png'\n        box_path = f'/home/jovyan/boomcheng-data-shcdt/herunze/code/base/gen_data/coco_rm_200/box/{i}.json'\n        box_new_path = f'/home/jovyan/boomcheng-data-shcdt/herunze/code/base/gen_data/coco_rm_200/box_new/{i}.json'\n        image = load2ts(image_path)\n        data1 = load_json(box_path)\n        obj_bbox_1, obj_class_1 = data1['obj_bbox'], data1['obj_class']\n        obj_bbox_1 = torch.tensor(obj_bbox_1).reshape(1,4)\n\n        obj_bbox_edit = obj_bbox_1\n\n        edit_region = np.array(Image.open(mask_path).resize((24,24)).convert('RGB'))[...,0]/255\n\n        obj_bbox = torch.zeros_like(obj_bbox_1)\n        obj_class = ['']*len(obj_bbox_1)\n\n        obj_class_neg = ['an object, a person, artifacts, disharmonious objects, abrupt objects, messy background, noisy environment']\n        obj_bbox_neg = obj_bbox_1\n\n        ret = dict(\n            base_caption=base_caption,\n            image=image,\n            image_path=image_path,\n            obj_class=obj_class,\n            obj_bbox=obj_bbox,\n            obj_bbox_edit=obj_bbox_edit,\n            obj_class_neg=obj_class_neg,\n            obj_bbox_neg=obj_bbox_neg,\n            edit_region=torch.tensor(edit_region), #mask or box\n        )\n        return ret"}
{"type": "source_file", "path": "project/plangen/cfg/uni/h_text_ump+oimsam_tiny.py", "content": "_base_ = '../base.py'\n\ntrain_data=[\n    dict(task_type='uni', data_name=['hico'], batch_size=1),\n    dict(task_type='mmu', data_name=['hico'], batch_size=1),\n    # dict(task_type='plan', data_name='layout', batch_size=1),\n]\ntest_data=dict(task_type='uni', data_name='hico', batch_size=8)\n\nuse_textual=True\nuse_special_tokens=True\n\ntuning_mode='stage3'\n\nmax_train_steps=200000\n\ngradient_accumulation_steps=1"}
{"type": "source_file", "path": "project/plangen/dataset/data_hico.py", "content": "import os\nos.environ[\"WANDB_MODE\"]=\"offline\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nimport datasets\nfrom datasets import load_dataset, ClassLabel, concatenate_datasets\nimport torch\nimport numpy as np\nimport random\nfrom PIL import Image\nimport json\nimport copy\nfrom torchvision import transforms\nimport pickle \nimport re\nimport cv2\n\nfrom .code_hico.debug_grit import build_grit_dsets\nfrom datasets import load_dataset\nimport json\nimport os\nfrom collections import defaultdict\nimport random\nimport PIL\nfrom PIL import Image, ImageFont, ImageDraw\nimport numpy as np\nimport torch\nimport torchvision.transforms as T\nfrom torch.utils.data import Dataset\nfrom omegaconf import OmegaConf\nfrom transformers import AutoTokenizer\nfrom src.utils.funcs import *\nfrom .sam.sam_traindata import BboxDataset_sam\nfrom .layoutgpt.data_layoutgpt import Dataset_layout\nfrom .edit.dataset_edit import Dataset_edit\nfrom .edit.dataset_edit_coco_rm import Dataset_edit_coco_rm\nfrom .edit.dataset_edit_coco_edit import Dataset_edit_coco_edit\nfrom .coco.data_coco import Dataset_coco\nfrom .oim.data_oim import Dataset_oim\nfrom .hico7k.data_7k import Dataset_7k\nfrom .plan.data_plan import Dataset_plan\n\n\nclass Hico_dataset(torch.utils.data.Dataset):\n    def __init__(\n        self,\n        args=None,\n        is_test=False,\n        tiny_hico_data=True,\n        hico_split='train',\n        is_sam=False,\n        is_mb=False,\n        is_7k=False,\n        is_plan=False,\n        is_creati=False,\n        is_layout=False,\n        is_edit=False,\n        is_coco=False,\n        for_rm=False,\n        is_oim=False,\n        is_gen=False,\n        use_1k=False,\n        can_dropout=False,\n        coco_split='train',\n        edit_split='',\n        mb_split='train',\n        oim_split='train',\n        plan_split='llama',\n        **kwargs,\n    ):\n        self.args = args\n        self.is_sam = is_sam\n        self.is_7k = is_7k\n        self.is_mb = is_mb\n        self.is_coco = is_coco\n        self.is_oim = is_oim\n        self.is_creati = is_creati\n        self.is_layout = is_layout\n        self.is_edit = is_edit\n        self.is_gen = is_gen\n        self.is_test = is_test\n        self.use_1k = use_1k\n        self.is_plan = is_plan\n        self.can_dropout = can_dropout\n        \n        if self.is_creati:\n            self.dataset = self.load_creati()\n        elif self.is_plan:\n            self.dataset = Dataset_plan(args, model=plan_split)\n        elif self.is_mb:\n            self.dataset = Dataset_mb(args, split=mb_split)\n        elif self.is_7k:\n            self.dataset = Dataset_7k(args)\n        elif self.is_coco:\n            self.dataset = Dataset_coco(split=coco_split, for_rm=for_rm)\n        elif self.is_oim:\n            self.dataset = Dataset_oim(args, split=oim_split)\n        elif self.is_sam:\n            self.dataset = self.load_sam()\n        elif self.is_layout:\n            self.dataset = Dataset_layout()\n        elif self.is_gen:\n            self.dataset = Dataset_gen()\n        elif self.is_edit:\n            if edit_split == 'rm_coco':\n                self.dataset = Dataset_edit_coco_rm()\n            elif edit_split == 'edit_coco':\n                self.dataset = Dataset_edit_coco_edit()\n            else:\n                assert False\n        else:\n            self.dataset = self.load_hico(tiny_hico_data, hico_split)\n\n    def load_sam(self):\n        dataset_path = self.args.layoutsam_path\n        dataset = load_dataset(dataset_path, split='train')\n        dataset = BboxDataset_sam(dataset)\n        return dataset\n\n    def load_creati(self):\n        dataset_path = self.args.layoutsam_eval_path\n        dataset = load_dataset(dataset_path, split='test')\n        dataset = BboxDataset_sam(dataset, is_testset=True)\n        return dataset\n\n    def load_hico(\n        self, \n        tiny_hico_data,\n        hico_split,\n    ):\n        args = self.args\n        if tiny_hico_data:\n            cfg_data = OmegaConf.load(\"/home/jovyan/boomcheng-data-shcdt/herunze/code/base/PlanGen/project/plangen/dataset/code_hico/latent_LayoutDiffusion_large.yaml\")\n        else:\n            cfg_data = OmegaConf.load(\"/home/jovyan/boomcheng-data/aigc/LayoutProj/diffusers_0263/examples/controlnet/latent_LayoutDiffusion_large.yaml\")\n        pretrained_model = self.args.sdxl_path\n        tokenizer_one = AutoTokenizer.from_pretrained(\n                    pretrained_model,\n                    subfolder=\"tokenizer\",\n                    revision=None,\n                    use_fast=False,\n                )\n        tokenizer_two = AutoTokenizer.from_pretrained(\n                    pretrained_model,\n                    subfolder=\"tokenizer_2\",\n                    revision=None,\n                    use_fast=False,\n                )\n        dataset = build_grit_dsets(cfg_data, [tokenizer_one, tokenizer_two], mode=hico_split, args=args)\n        return dataset\n        \n    def get_grounding(self, base_caption, obj_bbox, obj_class, upd_is_valid_obj=None):\n        try:\n            if obj_bbox.sum() == 0 or upd_is_valid_obj.sum() == 0:\n                return base_caption\n        except:\n            print('null box')\n            print(obj_bbox)\n            print(obj_class)\n            return base_caption\n\n        if len(base_caption) == 0:\n            full_prompt = f'<grounding>'\n        else:\n            full_prompt = f'{base_caption} <grounding>'\n\n        for i in range(len(obj_bbox)):\n            box = obj_bbox[i]\n            des = obj_class[i]\n            if upd_is_valid_obj is None or upd_is_valid_obj[i]:\n                if self.args.use_textual:\n                    nbox = [round(1000*t) for t in box.cpu().detach().numpy().tolist()]\n                    full_prompt += f'<ref>{des}</ref>'\n                    full_prompt += f'<box>{nbox}</box>'\n                else:\n                    nbox = [round(99*t) for t in box.cpu().detach().numpy().tolist()]\n                    nbox[0] = f'<h{nbox[0]}>'\n                    nbox[1] = f'<w{nbox[1]}>'\n                    nbox[2] = f'<h{nbox[2]}>'\n                    nbox[3] = f'<w{nbox[3]}>'   \n                    full_prompt += f'<ref>{des}</ref>'\n                    full_prompt += f'<box>{\",\".join(nbox)}</box>'\n        full_prompt += '</grounding>'\n        return full_prompt\n\n    def convert_creati_to_hico(self, meta_data):\n        meta_data['region_bboxes_list'] = meta_data['region_bboxes_list'][:10]\n        meta_data['region_caption_list'] = meta_data['region_caption_list'][:10]\n        meta_data['detail_region_caption_list'] = meta_data['detail_region_caption_list'][:10]\n\n        obj_bbox = torch.tensor(meta_data['region_bboxes_list'])\n        need_pad = 10-len(obj_bbox)\n        if len(obj_bbox) < 10:\n            pad_obj_bbox = torch.cat([obj_bbox, torch.zeros((need_pad, 4))], dim=0)\n        else:\n            pad_obj_bbox = obj_bbox\n            \n        obj_class = meta_data['detail_region_caption_list'] + ['']*need_pad\n        obj_class_simple = meta_data['region_caption_list'] + ['']*need_pad\n        upd_is_valid_obj = torch.tensor([1]*len(obj_bbox) + [0]*need_pad) #list不会collate进行concat\n        pixel_values = meta_data['image']\n        base_caption = meta_data['global_caption']\n        image_path = meta_data['file_name']\n\n        meta_data.update(dict(\n            obj_bbox=pad_obj_bbox,\n            obj_class=obj_class,\n            upd_is_valid_obj=upd_is_valid_obj,\n            pixel_values=pixel_values,\n            image_path=image_path,\n            base_caption=base_caption,\n            # cond_image=cond_image,\n        ))\n\n        if self.args.use_creati_detail:\n            meta_data.update(obj_class_simple=obj_class_simple)\n\n    def convert_layout_to_hico(self, meta_data):\n\n        # 现在都是x1,y1,x2,y2格式\n        obj_bbox = torch.tensor(meta_data['obj_bbox']).clamp(0,1)\n        need_pad = 10-len(obj_bbox)\n        if len(obj_bbox) < 10:\n            pad_obj_bbox = torch.cat([obj_bbox, torch.zeros((need_pad, 4))], dim=0)\n        else:\n            pad_obj_bbox = obj_bbox\n            \n        obj_class = meta_data['obj_class'] + ['']*need_pad\n        upd_is_valid_obj = torch.tensor([1]*len(obj_bbox) + [0]*need_pad) #list不会collate进行concat\n\n        pixel_values = meta_data.get('image', torch.zeros((3,self.args.janus_hw,self.args.janus_hw)))\n        base_caption = meta_data['base_caption']\n        image_path = ''\n\n        meta_data.update(dict(\n            obj_bbox=pad_obj_bbox,\n            obj_class=obj_class,\n            upd_is_valid_obj=upd_is_valid_obj,\n            pixel_values=pixel_values,\n            image_path=image_path,\n            base_caption=base_caption,\n            # cond_image=cond_image,\n        ))\n\n    def convert_edit_to_hico(self, meta_data):\n\n        obj_bbox = meta_data['obj_bbox']\n        need_pad = 10-len(obj_bbox)\n        if len(obj_bbox) < 10:\n            pad_obj_bbox = torch.cat([obj_bbox, torch.zeros((need_pad, 4))], dim=0)\n        else:\n            pad_obj_bbox = obj_bbox\n            \n        obj_class = meta_data['obj_class'] + ['']*need_pad\n        upd_is_valid_obj = torch.tensor([1]*len(obj_bbox) + [0]*need_pad) #list不会collate进行concat\n        pixel_values = meta_data['image']\n        base_caption = meta_data['base_caption']\n        image_path = meta_data['image_path']\n\n        meta_data.update(dict(\n            obj_bbox=pad_obj_bbox,\n            obj_class=obj_class,\n            upd_is_valid_obj=upd_is_valid_obj,\n            pixel_values=pixel_values,\n            image_path=image_path,\n            base_caption=base_caption,\n            # cond_image=cond_image,\n        ))\n\n    def preprocess_hico(self, meta_data):\n        obj_bbox = torch.cat([meta_data['obj_bbox'][1:], torch.zeros((1,4))])\n        \n        obj_class = meta_data['obj_class'][1:] + ['']\n        \n        upd_is_valid_obj = torch.cat([meta_data['upd_is_valid_obj'][1:], torch.tensor([0])])\n        # for i in range(10):\n        #     if obj_bbox[i].sum() == 0:\n        #         upd_is_valid_obj[i] = 0\n        #     if obj_class[i] == '':\n        #         upd_is_valid_obj[i] = 0\n        \n        # cond_image = meta_data['cond_image']\n        # cond_image = torch.cat([cond_image[1:], torch.zeros((1,*cond_image.shape[1:]))])\n        \n        meta_data.update(dict(\n            obj_bbox=obj_bbox,\n            obj_class=obj_class,\n            upd_is_valid_obj=upd_is_valid_obj,\n            # cond_image=cond_image,\n        ))\n\n    def __getitem__(self, index):\n        meta_data = self.dataset[index]\n        old_meta_data = deepcopy(meta_data)\n\n        if self.is_creati or self.is_sam:\n            self.convert_creati_to_hico(meta_data)\n        elif self.is_layout or self.is_gen or self.is_coco or self.is_oim or self.is_7k or self.is_plan:\n            self.convert_layout_to_hico(meta_data)\n        elif self.is_edit or self.is_mb:\n            self.convert_edit_to_hico(meta_data)\n        else:\n            self.preprocess_hico(meta_data)\n        # dict_keys(['obj_bbox', 'obj_class', 'is_valid_obj', 'upd_is_valid_obj', 'obj_class_text_ids', 'width', 'height', 'original_sizes_hw', 'num_obj_ori', 'new_height', 'new_width', 'crop_top_lefts', 'cond_image', 'base_caption', 'base_class_text_ids', 'num_selected', 'url', 'pixel_values'])\n\n        base_caption = meta_data['base_caption']\n        pixel_values = meta_data['pixel_values']\n        image_path = meta_data['image_path']\n\n        obj_bbox = meta_data['obj_bbox']\n        obj_class = meta_data['obj_class']\n        upd_is_valid_obj = meta_data['upd_is_valid_obj']\n\n        assert len(obj_bbox) == len(obj_class)\n        for i in range(len(obj_bbox)):\n            if obj_bbox[i].sum() == 0:\n                upd_is_valid_obj[i] = 0\n            if obj_class[i] == '':\n                upd_is_valid_obj[i] = 0\n\n        pixel_values = resize_pt(pixel_values, self.args.janus_hw)\n\n        prompt, gt_grounding = self.get_g_prompt(base_caption, obj_bbox, obj_class, upd_is_valid_obj)\n\n        neg_base_caption = self.args.neg_prompt\n        neg_gt_grounding = ''\n        if self.is_edit:\n            if 'edit_region' in meta_data:\n                edit_region = meta_data['edit_region'].reshape(-1)\n            else:\n                h = 384//16\n                edit_region = torch.zeros((h, h))\n                edit_boxes = meta_data['obj_bbox_edit']\n                if self.args.pad_edit_box != 0:\n                    dx = edit_boxes[:,2] - edit_boxes[:,0]\n                    dy = edit_boxes[:,3] - edit_boxes[:,1]\n                    edit_boxes[:,0] -= dx * self.args.pad_edit_box\n                    edit_boxes[:,1] -= dy * self.args.pad_edit_box\n                    edit_boxes[:,2] += dx * self.args.pad_edit_box\n                    edit_boxes[:,3] += dy * self.args.pad_edit_box\n                    edit_boxes = edit_boxes.clamp(0,1)\n                for box in edit_boxes:\n                    x1, y1, x2, y2 = map(lambda x: int(h * x), box)\n                    edit_region[y1:y2, x1:x2] = 1\n                edit_region = edit_region.reshape(-1)\n\n            if self.args.use_neg_box:\n                neg_prompt, neg_gt_grounding = self.get_g_prompt(\n                    neg_base_caption, \n                    meta_data['obj_bbox_neg'], \n                    meta_data['obj_class_neg'], \n                    upd_is_valid_obj=torch.ones((len(meta_data['obj_bbox_neg']))))##\n            else:\n                neg_prompt = neg_base_caption\n        else:\n            edit_region = torch.zeros((576))\n            neg_prompt = neg_base_caption\n\n        ret = dict(\n            image=pixel_values.to(torch.float),\n            base_caption=base_caption,\n            prompt=prompt,\n            neg_base_caption=neg_base_caption,\n            neg_prompt=neg_prompt,\n            gt_grounding=gt_grounding,\n            neg_gt_grounding=neg_gt_grounding,\n            image_path=image_path,\n            edit_region=edit_region.to(torch.long),\n            image_id=meta_data.get('image_id', ''),\n            H=meta_data.get('H', 0),\n            W=meta_data.get('W', 0),\n        )\n\n        if self.args.use_creati_detail:\n            ret.update(obj_class_simple=meta_data['obj_class_simple'])\n\n        return ret\n\n    def get_g_prompt(self, base_caption, obj_bbox, obj_class, upd_is_valid_obj):\n        if self.args.use_textual or self.args.use_numhw_tokens:\n            prompt = self.get_grounding(base_caption, obj_bbox, obj_class, upd_is_valid_obj)\n            gt_grounding = self.get_grounding(\"\", obj_bbox, obj_class, upd_is_valid_obj)\n        else:\n            prompt = base_caption\n            gt_grounding = \"\"\n        return prompt, gt_grounding\n\n    def __len__(self):\n        if self.use_1k:\n            return 1000\n        return len(self.dataset)\n"}
{"type": "source_file", "path": "project/plangen/dataset/code_hico/dataset/__init__.py", "content": ""}
{"type": "source_file", "path": "project/plangen/cfg/uni/h_text_ump+oimsam.py", "content": "_base_ = '../base.py'\n\ntrain_data=[\n    dict(task_type='uni', data_name=['hico_full', 'oim', 'sam'], batch_size=3),\n    dict(task_type='mmu', data_name=['hico_full', 'oim', 'sam'], batch_size=3),\n    dict(task_type='plan', data_name='layout', batch_size=2),\n]\ntest_data=dict(task_type='uni', data_name='hico', batch_size=8)\n\nuse_textual=True\nuse_special_tokens=True\n\ntuning_mode='stage3'\n\nmax_train_steps=200000\n\ngradient_accumulation_steps=1"}
{"type": "source_file", "path": "project/plangen/dataset/code_hico/dataset/augmentations.py", "content": "import torch\nfrom torchvision import transforms\nimport cv2\nimport numpy as np\nimport types\nfrom numpy import random\nimport pdb\n\ndef intersect(box_a, box_b):\n    max_xy = np.minimum(box_a[:, 2:], box_b[2:])\n    min_xy = np.maximum(box_a[:, :2], box_b[:2])\n    inter = np.clip((max_xy - min_xy), a_min=0, a_max=np.inf)\n    return inter[:, 0] * inter[:, 1]\n\n\ndef jaccard_numpy(box_a, box_b):\n    \"\"\"Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.\n    E.g.:\n        A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)\n    Args:\n        box_a: Multiple bounding boxes, Shape: [num_boxes,4]\n        box_b: Single bounding box, Shape: [4]\n    Return:\n        jaccard overlap: Shape: [box_a.shape[0], box_a.shape[1]]\n    \"\"\"\n    inter = intersect(box_a, box_b)\n    area_a = ((box_a[:, 2] - box_a[:, 0]) *\n              (box_a[:, 3] - box_a[:, 1]))  # [A,B]\n    area_b = ((box_b[2] - box_b[0]) *\n              (box_b[3] - box_b[1]))  # [A,B]\n    union = area_a + area_b - inter\n    return inter / union  # [A,B]\n\n\nclass Compose(object):\n    \"\"\"Composes several augmentations together.\n    Args:\n        transforms (List[Transform]): list of transforms to compose.\n    Example:\n        >>> augmentations.Compose([\n        >>>     transforms.CenterCrop(10),\n        >>>     transforms.ToTensor(),\n        >>> ])\n    \"\"\"\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img, boxes=None, labels=None):\n        for t in self.transforms:\n            img, boxes, labels = t(img, boxes, labels)\n        return img, boxes, labels\n\n\nclass Lambda(object):\n    \"\"\"Applies a lambda as a transform.\"\"\"\n\n    def __init__(self, lambd):\n        assert isinstance(lambd, types.LambdaType)\n        self.lambd = lambd\n\n    def __call__(self, img, boxes=None, labels=None):\n        return self.lambd(img, boxes, labels)\n\n\nclass ConvertFromInts(object):\n    def __call__(self, image, boxes=None, labels=None):\n        return image.astype(np.float32), boxes, labels\n\n\nclass SubtractMeans(object):\n    def __init__(self, mean):\n        self.mean = np.array(mean, dtype=np.float32)\n\n    def __call__(self, image, boxes=None, labels=None):\n        image = image.astype(np.float32)\n        image -= self.mean\n        return image.astype(np.float32), boxes, labels\n\n\nclass ToAbsoluteCoords(object):\n    def __call__(self, image, boxes=None, labels=None):\n        height, width, channels = image.shape\n        boxes[:, 0] *= width\n        boxes[:, 2] *= width\n        boxes[:, 1] *= height\n        boxes[:, 3] *= height\n\n        return image, boxes, labels\n\n\nclass ToPercentCoords(object):\n    def __call__(self, image, boxes=None, labels=None):\n        height, width, channels = image.shape\n        boxes[:, 0] /= width\n        boxes[:, 2] /= width\n        boxes[:, 1] /= height\n        boxes[:, 3] /= height\n\n        return image, boxes, labels\n\n\nclass Resize(object):\n    def __init__(self, size=300):\n        self.size = size\n\n    def __call__(self, image, boxes=None, labels=None):\n        image = cv2.resize(image, (self.size,\n                                   self.size))\n        return image, boxes, labels\n\n\nclass RandomSaturation(object):\n    def __init__(self, lower=0.5, upper=1.5):\n        self.lower = lower\n        self.upper = upper\n        assert self.upper >= self.lower, \"contrast upper must be >= lower.\"\n        assert self.lower >= 0, \"contrast lower must be non-negative.\"\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            image[:, :, 1] *= random.uniform(self.lower, self.upper)\n\n        return image, boxes, labels\n\n\nclass RandomHue(object):\n    def __init__(self, delta=18.0):\n        assert delta >= 0.0 and delta <= 360.0\n        self.delta = delta\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            image[:, :, 0] += random.uniform(-self.delta, self.delta)\n            image[:, :, 0][image[:, :, 0] > 360.0] -= 360.0\n            image[:, :, 0][image[:, :, 0] < 0.0] += 360.0\n        return image, boxes, labels\n\n\nclass RandomLightingNoise(object):\n    def __init__(self):\n        self.perms = ((0, 1, 2), (0, 2, 1),\n                      (1, 0, 2), (1, 2, 0),\n                      (2, 0, 1), (2, 1, 0))\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            swap = self.perms[random.randint(len(self.perms))]\n            shuffle = SwapChannels(swap)  # shuffle channels\n            image = shuffle(image)\n        return image, boxes, labels\n\n\nclass ConvertColor(object):\n    def __init__(self, current='BGR', transform='HSV'):\n        self.transform = transform\n        self.current = current\n\n    def __call__(self, image, boxes=None, labels=None):\n        if self.current == 'BGR' and self.transform == 'HSV':\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n        elif self.current == 'HSV' and self.transform == 'BGR':\n            image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n        else:\n            raise NotImplementedError\n        return image, boxes, labels\n\n\nclass RandomContrast(object):\n    def __init__(self, lower=0.5, upper=1.5):\n        self.lower = lower\n        self.upper = upper\n        assert self.upper >= self.lower, \"contrast upper must be >= lower.\"\n        assert self.lower >= 0, \"contrast lower must be non-negative.\"\n\n    # expects float image\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            alpha = random.uniform(self.lower, self.upper)\n            image *= alpha\n        return image, boxes, labels\n\n\nclass RandomBrightness(object):\n    def __init__(self, delta=32):\n        assert delta >= 0.0\n        assert delta <= 255.0\n        self.delta = delta\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            delta = random.uniform(-self.delta, self.delta)\n            image += delta\n        return image, boxes, labels\n\n\nclass ToCV2Image(object):\n    def __call__(self, tensor, boxes=None, labels=None):\n        return tensor.cpu().numpy().astype(np.float32).transpose((1, 2, 0)), boxes, labels\n\n\nclass ToTensor(object):\n    def __call__(self, cvimage, boxes=None, labels=None):\n        return torch.from_numpy(cvimage.astype(np.float32)).permute(2, 0, 1), boxes, labels\n\nclass CenterSampleCrop(object):\n    \"\"\"Crop\n    Arguments:\n        img (Image): the image being input during training\n        boxes (Tensor): the original bounding boxes in pt form\n        labels (Tensor): the class labels for each bbox\n        mode (float tuple): the min and max jaccard overlaps\n    Return:\n        (img, boxes, classes)\n            img (Image): the cropped image\n            boxes (Tensor): the adjusted bounding boxes in pt form\n            labels (Tensor): the class labels for each bbox\n    \"\"\"\n\n    def __init__(self):\n        self.sample_options = [\n            # using entire original input image\n            None,\n            # sample a patch s.t. MIN jaccard w/ obj in .1,.3,.4,.7,.9\n            (0.1, None),\n            (0.3, None),\n            (0.7, None),\n            (0.9, None),\n            # randomly sample a patch\n            (None, None),\n        ]\n\n    def __call__(self, image, boxes=None, labels=None):\n        height, width, _ = image.shape\n        while True:\n            # randomly choose a mode\n            mode = self.sample_options[random.choice([1,2,3,4,5])]\n            #mode = self.sample_options[random.choice([0,1,2,3,4,5])]\n            #if mode is None:\n            #    return image, boxes, labels, [True for i in range(boxes.shape[0])]\n\n            #pdb.set_trace()\n            min_iou, max_iou = mode\n            if min_iou is None:\n                min_iou = float('-inf')\n            if max_iou is None:\n                max_iou = float('inf')\n\n            # max trails (50)\n            for _ in range(50):\n                current_image = image\n\n                #w = random.uniform(0.3 * width, width)\n                #h = random.uniform(0.3 * height, height)\n                # aspect ratio constraint b/t .5 & 2\n                #if h / w < 0.5 or h / w > 2:\n                #    continue\n                min_asp = min(width, height)\n                if width == min_asp:\n                    w = width\n                    h = width\n                else:\n                    h = height\n                    w = height\n\n                left = random.uniform(0, width - w)\n                top = random.uniform(0, height - h)\n\n                # convert to integer rect x1,y1,x2,y2\n                rect = np.array([int(left), int(top), int(left + w), int(top + h)])\n\n                # calculate IoU (jaccard overlap) b/t the cropped and gt boxes\n                #pdb.set_trace()\n                overlap = jaccard_numpy(boxes, rect)\n\n                if len(overlap) == 0:\n                    continue\n\n                # is min and max overlap constraint satisfied? if not try again\n                if overlap.min() < min_iou and max_iou < overlap.max():\n                    continue\n\n                # cut the crop from the image\n                current_image = current_image[rect[1]:rect[3], rect[0]:rect[2],:]\n\n                # keep overlap with gt box IF center in sampled patch\n                centers = (boxes[:, :2] + boxes[:, 2:]) / 2.0\n\n                # mask in all gt boxes that above and to the left of centers\n                m1 = (rect[0] < centers[:, 0]) * (rect[1] < centers[:, 1])\n\n                # mask in all gt boxes that under and to the right of centers\n                m2 = (rect[2] > centers[:, 0]) * (rect[3] > centers[:, 1])\n\n                # mask in that both m1 and m2 are true\n                mask = m1 * m2\n\n                # have any valid boxes? try again if not\n                if not mask.any():\n                    continue\n\n                # take only matching gt boxes\n                current_boxes = boxes[mask, :].copy()\n\n                # take only matching gt labels\n                #current_labels = labels[mask]\n                current_labels = []\n                for iij in range(len(mask)):\n                    if mask[iij]:\n                        current_labels.append(labels[iij])\n\n                # should we use the box left and top corner or the crop's\n                current_boxes[:, :2] = np.maximum(current_boxes[:, :2],\n                                                  rect[:2])\n                # adjust to crop (by substracting crop's left,top)\n                current_boxes[:, :2] -= rect[:2]\n\n                current_boxes[:, 2:] = np.minimum(current_boxes[:, 2:],\n                                                  rect[2:])\n                # adjust to crop (by substracting crop's left,top)\n                current_boxes[:, 2:] -= rect[:2]\n\n                crop_top_left = [int(left), int(top)]\n                return crop_top_left, current_image, current_boxes, current_labels, mask\n\n\nclass RandomSampleCrop(object):\n    \"\"\"Crop\n    Arguments:\n        img (Image): the image being input during training\n        boxes (Tensor): the original bounding boxes in pt form\n        labels (Tensor): the class labels for each bbox\n        mode (float tuple): the min and max jaccard overlaps\n    Return:\n        (img, boxes, classes)\n            img (Image): the cropped image\n            boxes (Tensor): the adjusted bounding boxes in pt form\n            labels (Tensor): the class labels for each bbox\n    \"\"\"\n\n    def __init__(self):\n        self.sample_options = [\n            # using entire original input image\n            None,\n            # sample a patch s.t. MIN jaccard w/ obj in .1,.3,.4,.7,.9\n            (0.1, None),\n            (0.3, None),\n            (0.7, None),\n            (0.9, None),\n            # randomly sample a patch\n            (None, None),\n        ]\n\n    def __call__(self, image, boxes=None, labels=None):\n        height, width, _ = image.shape\n        while True:\n            # randomly choose a mode\n            mode = self.sample_options[random.choice([0,1,2,3,4,5])]\n            if mode is None:\n                return image, boxes, labels, [True for i in range(boxes.shape[0])]\n\n            min_iou, max_iou = mode\n            if min_iou is None:\n                min_iou = float('-inf')\n            if max_iou is None:\n                max_iou = float('inf')\n\n            # max trails (50)\n            for _ in range(50):\n                current_image = image\n\n                w = random.uniform(0.3 * width, width)\n                h = random.uniform(0.3 * height, height)\n\n                # aspect ratio constraint b/t .5 & 2\n                if h / w < 0.5 or h / w > 2:\n                    continue\n\n                left = random.uniform(width - w)\n                top = random.uniform(height - h)\n\n                # convert to integer rect x1,y1,x2,y2\n                rect = np.array([int(left), int(top), int(left + w), int(top + h)])\n\n                # calculate IoU (jaccard overlap) b/t the cropped and gt boxes\n                overlap = jaccard_numpy(boxes, rect)\n\n                # is min and max overlap constraint satisfied? if not try again\n                if overlap.min() < min_iou and max_iou < overlap.max():\n                    continue\n\n                # cut the crop from the image\n                current_image = current_image[rect[1]:rect[3], rect[0]:rect[2],:]\n\n                # keep overlap with gt box IF center in sampled patch\n                centers = (boxes[:, :2] + boxes[:, 2:]) / 2.0\n\n                # mask in all gt boxes that above and to the left of centers\n                m1 = (rect[0] < centers[:, 0]) * (rect[1] < centers[:, 1])\n\n                # mask in all gt boxes that under and to the right of centers\n                m2 = (rect[2] > centers[:, 0]) * (rect[3] > centers[:, 1])\n\n                # mask in that both m1 and m2 are true\n                mask = m1 * m2\n\n                # have any valid boxes? try again if not\n                if not mask.any():\n                    continue\n\n                # take only matching gt boxes\n                current_boxes = boxes[mask, :].copy()\n\n                # take only matching gt labels\n                current_labels = labels[mask]\n\n                # should we use the box left and top corner or the crop's\n                current_boxes[:, :2] = np.maximum(current_boxes[:, :2],\n                                                  rect[:2])\n                # adjust to crop (by substracting crop's left,top)\n                current_boxes[:, :2] -= rect[:2]\n\n                current_boxes[:, 2:] = np.minimum(current_boxes[:, 2:],\n                                                  rect[2:])\n                # adjust to crop (by substracting crop's left,top)\n                current_boxes[:, 2:] -= rect[:2]\n\n                return current_image, current_boxes, current_labels, mask\n\n\nclass Expand(object):\n    def __init__(self, mean):\n        self.mean = mean\n\n    def __call__(self, image, boxes, labels):\n        if random.randint(2):\n            return image, boxes, labels\n\n        height, width, depth = image.shape\n        ratio = random.uniform(1, 4)\n        left = random.uniform(0, width * ratio - width)\n        top = random.uniform(0, height * ratio - height)\n\n        expand_image = np.zeros(\n            (int(height * ratio), int(width * ratio), depth),\n            dtype=image.dtype)\n        expand_image[:, :, :] = self.mean\n        expand_image[int(top):int(top + height),\n        int(left):int(left + width)] = image\n        image = expand_image\n\n        boxes = boxes.copy()\n        boxes[:, :2] += (int(left), int(top))\n        boxes[:, 2:] += (int(left), int(top))\n\n        return image, boxes, labels\n\n\nclass RandomMirror(object):\n    def __call__(self, image, boxes, classes):\n        _, width, _ = image.shape\n        if random.randint(2):\n            image = image[:, ::-1].copy()\n            boxes = boxes.copy()\n            boxes[:, 0::2] = width - boxes[:, 2::-2]\n        return image, boxes, classes\n\n\nclass SwapChannels(object):\n    \"\"\"Transforms a tensorized image by swapping the channels in the order\n     specified in the swap tuple.\n    Args:\n        swaps (int triple): final order of channels\n            eg: (2, 1, 0)\n    \"\"\"\n\n    def __init__(self, swaps):\n        self.swaps = swaps\n\n    def __call__(self, image):\n        \"\"\"\n        Args:\n            image (Tensor): image tensor to be transformed\n        Return:\n            a tensor with channels swapped according to swap\n        \"\"\"\n        # if torch.is_tensor(image):\n        #     image = image.data.cpu().numpy()\n        # else:\n        #     image = np.array(image)\n        image = image[:, :, self.swaps]\n        return image\n\n\nclass PhotometricDistort(object):\n    def __init__(self):\n        self.pd = [\n            RandomContrast(),\n            ConvertColor(transform='HSV'),\n            RandomSaturation(),\n            RandomHue(),\n            ConvertColor(current='HSV', transform='BGR'),\n            RandomContrast()\n        ]\n        self.rand_brightness = RandomBrightness()\n        self.rand_light_noise = RandomLightingNoise()\n\n    def __call__(self, image, boxes, labels):\n        im = image.copy()\n        im, boxes, labels = self.rand_brightness(im, boxes, labels)\n        if random.randint(2):\n            distort = Compose(self.pd[:-1])\n        else:\n            distort = Compose(self.pd[1:])\n        im, boxes, labels = distort(im, boxes, labels)\n        return self.rand_light_noise(im, boxes, labels)\n\n\nclass SSDAugmentation(object):\n    def __init__(self, size=300, mean=(104, 117, 123)):\n        self.mean = mean\n        self.size = size\n        self.augment = Compose([\n            ConvertFromInts(),\n            ToAbsoluteCoords(),\n            PhotometricDistort(),\n            Expand(self.mean),\n            RandomSampleCrop(),\n            RandomMirror(),\n            ToPercentCoords(),\n            Resize(self.size),\n            SubtractMeans(self.mean)\n        ])\n\n    def __call__(self, img, boxes, labels):\n        return self.augment(img, boxes, labels)\n"}
{"type": "source_file", "path": "project/plangen/dataset/hico7k/data_7k.py", "content": "from torch.utils.data import Dataset\nimport random\nfrom copy import deepcopy\nfrom torchvision.transforms import Resize\nfrom glob import glob\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport random\nfrom copy import deepcopy\nfrom torchvision.transforms import Resize\nfrom datasets import load_dataset\nfrom src.utils.funcs import convert_to_np, load_jsonl\nimport numpy as np\nimport os\nfrom src.utils.funcs import *\n\nclass Dataset_7k(Dataset):\n    def __init__(\n        self,\n        args=None,\n        is_test=False,\n    ):\n        self.args = args\n        self.is_test = is_test\n\n        datas = load_json('/home/jovyan/boomcheng-data-shcdt/chengbo/grit-20m-512-obj3-val-7k-512s-clean-v3.json')\n        self.datas = datas\n\n\n    def __len__(self):\n        return len(self.datas)\n\n    def __getitem__(self, i):\n        data = self.datas[i]\n\n        obj_bbox = []\n        obj_class = []\n\n        prompt = data[1]\n\n        h = data[3]['H']\n        w = data[3]['W']\n\n        for t in data[5]:\n            obj_class.append(t[0])\n            obj_bbox.append(t[1])\n\n        obj_bbox = torch.tensor(obj_bbox)\n        obj_bbox[:,0::2] /= h\n        obj_bbox[:,1::2] /= w\n\n\n        ret = dict(\n            base_caption=prompt,\n            obj_bbox=obj_bbox,\n            obj_class=obj_class,\n        )\n        return ret"}
{"type": "source_file", "path": "project/plangen/dataset/data_toy.py", "content": "from torch.utils.data import Dataset\nimport random\nfrom copy import deepcopy\nfrom torchvision.transforms import Resize\nfrom glob import glob\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport random\nfrom copy import deepcopy\nfrom torchvision.transforms import Resize\nfrom datasets import load_dataset\nfrom src.utils.funcs import convert_to_np, load_jsonl\nimport numpy as np\nimport os\nfrom src.utils.funcs import *\n\nclass Dataset_toy(Dataset):\n    def __init__(\n        self,\n        args,\n        is_test=False,\n    ):\n        self.args = args\n        self.is_test = is_test\n\n    def __len__(self):\n        return 100\n\n    def __getitem__(self, i):\n        prompt = 'a sks meme'\n        image_path = '/home/jovyan/boomcheng-data-shcdt/herunze/code/base/Janus/images/doge.png'\n        image = (torch.tensor(np.array(Image.open(image_path).resize((self.args.janus_hw,self.args.janus_hw))))/255-0.5)*2\n        image = image.permute(2,0,1)\n\n        ret = dict(\n            prompt=prompt,\n            image=image,\n            image_path=image_path,\n        )\n        return ret"}
{"type": "source_file", "path": "project/plangen/dataset/code_hico/dataset/util.py", "content": "#!/usr/bin/python\n#\n# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport PIL\nimport torch\nimport torchvision.transforms as T\nfrom torchvision.transforms.functional import crop, resize\nimport numpy as np\n\n# IMAGENET_MEAN = [0.485, 0.456, 0.406]\n# IMAGENET_STD = [0.229, 0.224, 0.225]\n\n# IMAGENET_MEAN = [0., 0., 0.]\n# IMAGENET_STD = [1.0, 1.0, 1.0]\n\nIMAGENET_MEAN = [0.5, 0.5, 0.5]\nIMAGENET_STD = [0.5, 0.5, 0.5]\n\nINV_IMAGENET_MEAN = [-m for m in IMAGENET_MEAN]\nINV_IMAGENET_STD = [1.0 / s for s in IMAGENET_STD]\n\n\ndef rescale(x):\n    lo, hi = x.min(), x.max()\n    return x.sub(lo).div(hi - lo)\n\n\ndef blank(x):\n    return x\n\n\ndef image_normalize():\n    # return blank\n    return T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n\n\ndef image_unnormalize(rescale_image=False):\n    transforms = [\n        T.Normalize(mean=[0, 0, 0], std=INV_IMAGENET_STD),\n        T.Normalize(mean=INV_IMAGENET_MEAN, std=[1.0, 1.0, 1.0]),\n    ]\n    if rescale_image:\n        transforms.append(rescale)\n    return T.Compose(transforms)\n\n\ndef image_unnormalize_batch(imgs, rescale=False):\n    \"\"\"\n    Input:\n    - imgs: FloatTensor of shape (N, C, H, W) or (C, H, W) giving preprocessed images\n\n    Output:\n    - imgs_de: ByteTensor of shape (N, C, H, W) or (C, H, W) giving deprocessed images\n      in the range [0, 255]\n    \"\"\"\n    # if isinstance(imgs, torch.autograd.Variable):\n    #   imgs = imgs.data\n    # imgs = imgs.cpu().clone()\n    deprocess_fn = image_unnormalize(rescale_image=rescale)\n    imgs_de = []\n    if imgs.dim() == 4:\n        for i in range(imgs.size(0)):\n            img_de = deprocess_fn(imgs[i])[None]\n            # img_de = img_de.mul(255).clamp(0, 255).byte()\n            # img_de = img_de.mul(255).clamp(0, 255)\n            imgs_de.append(img_de)\n        imgs_de = torch.cat(imgs_de, dim=0)\n        return imgs_de\n    elif imgs.dim() == 3:\n        img_de = deprocess_fn(imgs)\n        return img_de\n    else:\n        raise NotImplementedError\n\n\nclass Resize(object):\n    def __init__(self, size, interp=PIL.Image.BILINEAR):\n        if isinstance(size, tuple):\n            H, W = size\n            self.size = (W, H)\n        else:\n            self.size = (size, size)\n        self.interp = interp\n\n    def __call__(self, img):\n        return img.resize(self.size, self.interp)\n\n\n\ndef get_cropped_image(obj_bboxes, images, image_size=256, cropped_size=32, antialias=True):\n    '''\n\n    :param obj_bboxes: # N * L * 4, (x0, y0, w, h)\n    :param images:    # N * 3 * H * W\n    :param cropped_size: mask_size\n    :return:\n    '''\n\n    rounded_obj_bbox = obj_bboxes.clone()\n    height, width = images.shape[2], images.shape[3]\n    rounded_obj_bbox[:,:, 0::2] = rounded_obj_bbox[:,:,0::2] * width\n    rounded_obj_bbox[:,:,1::2] = rounded_obj_bbox[:,:,1::2] * height\n    rounded_obj_bbox = torch.round(rounded_obj_bbox)\n    rounded_obj_bbox = rounded_obj_bbox.long()\n    # rounded_obj_bbox[:, :, 2] = torch.where(\n    #     rounded_obj_bbox[:, :, 2] >= 1,\n    #     rounded_obj_bbox[:, :, 2],\n    #     1\n    # )\n    # rounded_obj_bbox[:, :, 3] = torch.where(\n    #     rounded_obj_bbox[:, :, 3] >= 1,\n    #     rounded_obj_bbox[:, :, 3],\n    #     1\n    # )\n    bs, length = rounded_obj_bbox.shape[0], rounded_obj_bbox.shape[1]\n\n    cropped_images = []\n    device = obj_bboxes.device\n    for image_id in range(rounded_obj_bbox.shape[0]):\n        for object_id, object in enumerate(rounded_obj_bbox[image_id]):\n            if torch.equal(obj_bboxes[image_id][object_id], torch.zeros((4,), device=device)):\n                cropped_images.append(torch.zeros((3, cropped_size, cropped_size), device=device))\n                continue\n\n            x0, y0, x1, y1 = object\n\n            cropped_image = crop(images[image_id], top=y0, left=x0, height=max(y1 - y0, 1), width=max(x1 - x0, 1))\n\n            if antialias:\n                cropped_image = resize(cropped_image, size=[cropped_size, cropped_size], antialias=True)\n            else:\n                cropped_image = resize(cropped_image, size=[cropped_size, cropped_size])\n\n            cropped_images.append(cropped_image)\n\n    cropped_images = torch.stack(cropped_images).reshape(bs, length, 3, cropped_size, cropped_size)\n\n    return cropped_images\n\n\n"}
{"type": "source_file", "path": "project/plangen/dataset/edit/dataset_edit.py", "content": "from torch.utils.data import Dataset\nimport random\nfrom copy import deepcopy\nfrom torchvision.transforms import Resize\nfrom glob import glob\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport random\nfrom copy import deepcopy\nfrom torchvision.transforms import Resize\nfrom datasets import load_dataset\nfrom src.utils.funcs import convert_to_np, load_jsonl\nimport numpy as np\nimport os\nfrom src.utils.funcs import *\n\nclass Dataset_edit(Dataset):\n    def __init__(\n        self,\n        args=None,\n        is_test=False,\n    ):\n        self.args = args\n        self.is_test = is_test\n        self.datas = load_json(\"project/janus/dataset/edit/edit.json\")\n\n    def __len__(self):\n        return len(self.datas)\n\n    def __getitem__(self, i):\n        data = self.datas[i]\n        base_caption = data.get('base_caption', '')\n        new_grounding_prompt = data.get('new_grounding_prompt', '')\n        edited_grounding_prompt = data.get('edited_grounding_prompt', '')\n        neg_grounding_prompt = data.get('neg_grounding_prompt', '')\n        image_path = data.get('image_path', '')\n\n        def get_obj_from_grounding(new_grounding_prompt):\n            pattern = r\"<ref>(.*?)</ref><box>(.*?)</box>\"\n            matches = re.findall(pattern, new_grounding_prompt)\n            matches = [(x,convert_coordinates(y)) for x,y in matches]\n\n            obj_class = []\n            obj_bbox = []\n            for desc, box in matches:\n                ori_x1, ori_y1, ori_x2, ori_y2 = map(int, box.split(\",\"))\n                cx, cy, _h, _w = ori_x1, ori_y1, ori_x2, ori_y2\n                x1 = cx - _w / 2\n                y1 = cy - _h / 2\n                x2 = cx + _w / 2\n                y2 = cy + _h / 2\n                ori_x1, ori_y1, ori_x2, ori_y2 = x1, y1, x2, y2\n                obj_class.append(desc)\n                obj_bbox.append([x1, y1, x2, y2])\n            return obj_class, obj_bbox\n\n        obj_class, obj_bbox = get_obj_from_grounding(new_grounding_prompt)\n        obj_class_edit, obj_bbox_edit = get_obj_from_grounding(edited_grounding_prompt)\n        obj_class_neg, obj_bbox_neg = get_obj_from_grounding(neg_grounding_prompt)\n\n        obj_bbox=torch.tensor(obj_bbox).clamp(0,1000)/1000\n        obj_bbox_edit=torch.tensor(obj_bbox_edit).clamp(0,1000)/1000\n        obj_bbox_neg=torch.tensor(obj_bbox_neg).clamp(0,1000)/1000\n\n        image = load2ts(image_path)\n\n        ret = dict(\n            base_caption=base_caption,\n            image=image,\n            image_path=image_path,\n            obj_class=obj_class,\n            obj_bbox=obj_bbox,\n            obj_bbox_edit=obj_bbox_edit,\n            obj_class_neg=obj_class_neg,\n            obj_bbox_neg=obj_bbox_neg,\n        )\n        return ret"}
{"type": "source_file", "path": "project/plangen/cfg/base.py", "content": "# _base_ = './testset.py'\n\nseed=0\ndirname='janus'\noutput_dir=None\nlogging_dir='logging_dir'\nfind_unused_parameters=True\n# janus_path=\"deepseek-ai/Janus-Pro-1B\"\n# layoutsam_path=\"HuiZhang0812/LayoutSAM\"\n# layoutsam_eval_path=\"HuiZhang0812/LayoutSAM-eval\"\n# sdxl_path=\"stabilityai/stable-diffusion-xl-base-1.0\"\njanus_path=\"/home/jovyan/boomcheng-data-shcdt/herunze/models/Janus-Pro-1B\"\nlayoutsam_path=\"/home/jovyan/boomcheng-data-shcdt/herunze/datasets/LayoutSAM\"\nlayoutsam_eval_path=\"/home/jovyan/boomcheng-data-shcdt/herunze/datasets/LayoutSAM-eval\"\nsdxl_path=\"/home/jovyan/boomcheng-data-shcdt/herunze/models/stable-diffusion-xl-base-1.0\"\ncoco_200_path = '/home/jovyan/boomcheng-data-shcdt/herunze/code/base/PlanGen_coco_data'\n# coco_200_path = 'coco_data'\n\nsystem_cls_path='project.plangen.plangen_base'\nworking_dir='project/plangen/out'\n\ntrain_data=[\n    dict(task_type='t2i', data_name='hico', batch_size=8),\n]\ntest_data=dict(task_type='t2i', data_name='hico', batch_size=1)\npin_memory=True\ndataloader_num_workers=16\n\nmax_train_steps=1000000\ncheckpointing_steps=5000\nvalidation_steps=5000\nmetric_steps=10000\nmax_val_len=3\nmax_test_len=20\nuse_metric=True\nuse_teacher_forcing=False\ntune_token_when_lora=True\n\ntest=False\nval=False\nfunc=None\nonly_metric=False\nsam_debug=False\n\ngradient_accumulation_steps=1\ncheckpoints_total_limit=3\nresume='latest'\nreport_to='tensorboard'\n\nscale_lr=None\nlr_scheduler='constant'\nlr_warmup_steps=0\nmax_grad_norm=1.0\nadam_beta1=0.9\nadam_beta2=0.999\nadam_epsilon=1e-08\nadam_weight_decay=0.01\nlearning_rate=5e-5\n\nmixed_precision='bf16'\n# mixed_precision='fp16'\n# mixed_precision=None\ngradient_checkpointing_enable=False\n\nuse_numhw_tokens=False\nuse_textual=False\n\n# is_edit=False\nuse_special_tokens=False\ntuning_mode='all'\nlora_rank=256\nlora_alpha=128\n\n\nlocal_rank=-1\nval_batch_size=1\n\n\nnum_frames=25\nheight=576\nwidth=1024\n\njanus_hw=384\n\nO1=False\nO2=False\nO3=False\nO4=True\ndebug=False\nno_full=False\n\nnum_inference_steps=25\n\nnoise_offset=None\nconditioning_dropout_prob=None\nunuse_image_emb=False\nunet_tuning='temp' # all,lora\n\nuse_mmu_loss=False\nuse_centerhw=False\nuse_smooth_labels=False\n\nplan_lr_scale=None\nuse_blank_token=False\ndropout_grounding=0\ndropout_caption=0\n\nscale_emb_grad=None\n\neta_min=None\n\nuse_2d_rope=False\n\ndataset_same=False\n\nuse_bg_box=False\n\nis_edit=False\n\npad_edit_box=0\nuse_neg_box=True\ntrans_data_to_rm=False\n\nuse_grounding_in_user=False\n\noim_split=None\noim_start=0\n\nneg_prompt='low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers.'\n\nuse_info=False\nuse_creati_detail=False\n\nmax_seq_len=700\ndebug_max_seq_len=None\n\ntest_start=0\ncreati_path=None\ncal_creati=False\nprefetch_factor=None\n\nscore_plan=False\nscore_creati=False\n\nuse_edit_uni=False\nuse_local_edit_loss=False\nuse_des_for_edit_region=False\n\nuse_random_one_box=False\n\n\nbeam_search=False\n\nscore_edit=False\n\ngen=True\n\nparallel_size=1\n\nuse_showo=False\n\ncfg_weight=None\n\nsave_data=False"}
{"type": "source_file", "path": "project/plangen/dataset/coco/data_coco.py", "content": "import torchvision.datasets as dset\nfrom torchvision import transforms\nfrom pycocotools.coco import COCO\nfrom torch.utils.data import DataLoader\n\nfrom torch.utils.data import Dataset\nimport random\nfrom copy import deepcopy\nfrom torchvision.transforms import Resize\nfrom glob import glob\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport random\nfrom copy import deepcopy\nfrom torchvision.transforms import Resize\nfrom datasets import load_dataset\nfrom src.utils.funcs import convert_to_np, load_jsonl\nimport numpy as np\nimport os\nfrom src.utils.funcs import *\nfrom torch.utils.data import ConcatDataset\n\ndef resize_and_crop(image, bboxes, target_size=384):\n    \"\"\"\n    Resize the image with the short side to target_size, then center crop to target_size x target_size.\n    Adjust the bounding boxes accordingly.\n\n    :param image: PIL Image\n    :param bboxes: numpy array of shape (n, 4) where each row is [x1, y1, w, h]\n    :param target_size: int, the target size for the short side and crop\n    :return: resized and cropped image, adjusted bboxes\n    \"\"\"\n    # Get original image size\n    original_width, original_height = image.size\n    \n    # Determine the scaling factor\n    if original_width < original_height:\n        scale = target_size / original_width\n        new_width = target_size\n        new_height = int(original_height * scale)\n    else:\n        scale = target_size / original_height\n        new_height = target_size\n        new_width = int(original_width * scale)\n    \n    # Resize the image\n    cropped_image = image.resize((new_width, new_height), Image.BILINEAR)\n    \n    # Calculate the coordinates for center cropping\n    left = (new_width - target_size) // 2\n    top = (new_height - target_size) // 2\n    right = left + target_size\n    bottom = top + target_size\n    \n    cropped_image = cropped_image.crop((left, top, right, bottom))\n    \n    adjusted_bboxes = []\n    for bbox in bboxes:\n        x1, y1, w, h = bbox\n        x1_scaled = x1 * scale\n        y1_scaled = y1 * scale\n        w_scaled = w * scale\n        h_scaled = h * scale\n        \n        x1_cropped = x1_scaled - left\n        y1_cropped = y1_scaled - top\n        \n        adjusted_bboxes.append([x1_cropped, y1_cropped, w_scaled, h_scaled])\n    \n    return cropped_image, np.array(adjusted_bboxes)\n\n\ndef filter_box(all_bbox, all_class):\n    image_width = image_height = 384\n    filtered_bbox = []\n    filtered_class = []\n\n    for i, (x, y, w, h) in enumerate(all_bbox):\n        \n        # 调整框的坐标和宽高，确保它们在图像范围内\n        x2 = x + w\n        y2 = y + h\n        x = max(0, x)\n        y = max(0, y)\n\n        if x > 380 or y > 380:\n            pass\n        else:\n            x2 = min(384, x2)\n            y2 = min(384, y2)\n            # w = min(384-x2, w)\n            # h = min(384-y2, h)\n            w = x2 - x\n            h = y2 - y\n\n            if w*h < 200:\n                pass\n            else:\n                filtered_bbox.append([x, y, w, h])\n                filtered_class.append(all_class[i])\n\n    # 将列表转换为 numpy 数组\n    filtered_bbox = np.array(filtered_bbox)\n    filtered_class = filtered_class\n    return filtered_bbox, filtered_class\n\n\nclass Dataset_coco(Dataset):\n    def __init__(\n        self,\n        args=None,\n        is_test=False,\n        split='train',\n        for_rm=False,\n    ):\n        self.args = args\n        self.is_test = is_test\n        self.split = split\n        self.for_rm = for_rm\n\n        # read MSCOCO\n        ann_file = '/home/jovyan/multi-modal-datasets/public/coco/annotations/instances_val2017.json'\n        ann_file_captions = '/home/jovyan/multi-modal-datasets/public/coco/annotations/captions_val2017.json'\n        coco_caption = COCO(ann_file_captions)\n        coco = COCO(ann_file)\n\n        # sort indices for reproducible results\n        image_ids = coco.getImgIds()\n        image_ids.sort()\n\n        self.coco = coco\n        self.coco_caption = coco_caption\n        self.image_ids = image_ids\n\n    # 获取类别名称的函数\n    def get_name(self, category_id):\n        category_info = self.coco_train.coco.loadCats(category_id)[0]\n        category_name = category_info['name']\n        return category_name\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, i):\n        img_id = self.image_ids[i]\n        # Pick one image.\n        img_info = self.coco.loadImgs([img_id])[0]\n        file_name = img_info['file_name']\n        image_id = img_info['id']\n        height = img_info['height']\n        width = img_info['width']\n\n        # Get all the annotations for the specified image.\n        ann_ids = self.coco.getAnnIds(imgIds=[img_id], iscrowd=None)\n        annotations = self.coco.loadAnns(ann_ids)\n\n        ann_ids_captions = self.coco_caption.getAnnIds(imgIds=[img_id], iscrowd=None)\n        anns_caption = self.coco_caption.loadAnns(ann_ids_captions)[0]['caption']\n        \n        obj_bbox = [t['bbox'] for t in annotations]\n\n        mask = [self.coco.annToMask(t) for t in annotations]\n        mask = np.stack(mask, axis=0)\n        kernel = np.ones((8, 8), np.uint8)\n        dilated_array = np.zeros_like(mask)\n        for i in range(mask.shape[0]):\n            dilated_array[i] = cv2.dilate(mask[i], kernel, iterations=5)\n        mask = dilated_array\n        mask = resize_pt(torch.tensor(mask), 24)\n\n        obj_bbox = torch.tensor(obj_bbox)\n        obj_bbox = obj_bbox.reshape(-1,4)\n        obj_class_id = [t['category_id'] for t in annotations]\n        obj_class = [cat[\"name\"] for cat in self.coco.loadCats(obj_class_id)]\n        assert len(obj_class) == len(obj_bbox)\n\n        image_path = osp.join('/home/jovyan/multi-modal-datasets/public/coco/val2017', f\"{image_id:012d}.jpg\")\n        image = to_ts(Image.open(image_path).convert('RGB').resize((384,384)))\n        image = image*2-1\n        obj_bbox[:,0::2] /= width\n        obj_bbox[:,1::2] /= height\n        obj_bbox[:,2] += obj_bbox[:,0]\n        obj_bbox[:,3] += obj_bbox[:,1]\n\n        # if self.for_rm:\n        #     i = random.randint(0, len(obj_bbox))\n        #     obj_bbox = obj_bbox[i:i+1]\n        #     obj_class = obj_class[i:i+1]\n        #     mask = mask[i:i+1]\n\n        return dict(\n            base_caption=anns_caption,\n            obj_bbox=obj_bbox,\n            obj_class=obj_class,\n            image=image,\n            image_id=f\"{image_id:012d}\",\n            H=height,\n            W=width,\n            mask=mask,\n        )\n        "}
{"type": "source_file", "path": "project/plangen/dataset/coco/a.py", "content": "import sys;sys.path.insert(0, './')\nimport torchvision.datasets as dset\nfrom torchvision import transforms\nfrom pycocotools.coco import COCO\nfrom torch.utils.data import DataLoader\n\n\nimport torchvision.datasets as dset\nfrom torchvision import transforms\nfrom pycocotools.coco import COCO\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\nimport numpy as np\nfrom src.utils.funcs import *\n\n\n# 设置图像转换，通常是将图像调整为固定的大小并转换为张量\ntransform = transforms.Compose([\n    # transforms.Resize(384),  # 示例尺寸\n    # transforms.CenterCrop(384),\n    transforms.ToTensor(),\n])\n\n# 加载训练集\n# coco_train = dset.CocoDetection(\n#     root='/home/jovyan/multi-modal-datasets/public/coco/train2017',\n#     annFile='/home/jovyan/multi-modal-datasets/public/coco/annotations/instances_train2017.json',\n#     transform=transform\n# )\n\n# 加载验证集\ncoco_val = dset.CocoDetection(\n    root='/home/jovyan/multi-modal-datasets/public/coco/val2017',\n    annFile='/home/jovyan/multi-modal-datasets/public/coco/annotations/instances_val2017.json',\n    transform=transform\n)\n\n# 初始化COCO API用于读取caption标注\ncoco_caps = COCO('/home/jovyan/multi-modal-datasets/public/coco/annotations/captions_val2017.json')\n\n# 获取类别名称的函数\ndef get_name(category_id):\n    category_info = coco_val.coco.loadCats(category_id)[0]\n    category_name = category_info['name']\n    return category_name\n\n# 获取caption的函数\ndef get_cap(image_id):\n    annIds = coco_caps.getAnnIds(imgIds=image_id)\n    anns = coco_caps.loadAnns(annIds)\n    return [ann['caption'] for ann in anns]\n\n# 打印数据集大小\n# print(f\"训练集大小: {len(coco_train)}\")\nprint(f\"验证集大小: {len(coco_val)}\")\n\nfrom PIL import Image\nimport torchvision.transforms.functional as F\nimport torch\n\nfrom PIL import Image\nimport numpy as np\nfrom PIL import Image\nimport numpy as np\n\ndef resize_and_crop(image, bboxes, target_size=384):\n    \"\"\"\n    Resize the image with the short side to target_size, then center crop to target_size x target_size.\n    Adjust the bounding boxes accordingly.\n\n    :param image: PIL Image\n    :param bboxes: numpy array of shape (n, 4) where each row is [x1, y1, w, h]\n    :param target_size: int, the target size for the short side and crop\n    :return: resized and cropped image, adjusted bboxes\n    \"\"\"\n    # Get original image size\n    original_width, original_height = image.size\n    \n    # Determine the scaling factor\n    if original_width < original_height:\n        scale = target_size / original_width\n        new_width = target_size\n        new_height = int(original_height * scale)\n    else:\n        scale = target_size / original_height\n        new_height = target_size\n        new_width = int(original_width * scale)\n    \n    # Resize the image\n    cropped_image = image.resize((new_width, new_height), Image.BILINEAR)\n    \n    # Calculate the coordinates for center cropping\n    left = (new_width - target_size) // 2\n    top = (new_height - target_size) // 2\n    right = left + target_size\n    bottom = top + target_size\n    \n    cropped_image = cropped_image.crop((left, top, right, bottom))\n    \n    adjusted_bboxes = []\n    for bbox in bboxes:\n        x1, y1, w, h = bbox\n        x1_scaled = x1 * scale\n        y1_scaled = y1 * scale\n        w_scaled = w * scale\n        h_scaled = h * scale\n        \n        x1_cropped = x1_scaled - left\n        y1_cropped = y1_scaled - top\n        \n        adjusted_bboxes.append([x1_cropped, y1_cropped, w_scaled, h_scaled])\n    \n    return cropped_image, np.array(adjusted_bboxes)\n\n\ndef filter_box(all_bbox, all_class):\n    image_width = image_height = 384\n    filtered_bbox = []\n    filtered_class = []\n\n    for i, (x, y, w, h) in enumerate(all_bbox):\n        \n        # 调整框的坐标和宽高，确保它们在图像范围内\n        x2 = x + w\n        y2 = y + h\n        x = max(0, x)\n        y = max(0, y)\n\n        if x > 380 or y > 380:\n            pass\n        else:\n            x2 = min(384, x2)\n            y2 = min(384, y2)\n            # w = min(384-x2, w)\n            # h = min(384-y2, h)\n            w = x2 - x\n            h = y2 - y\n\n            if w*h < 200:\n                pass\n            else:\n                filtered_bbox.append([x, y, w, h])\n                filtered_class.append(all_class[i])\n\n    # 将列表转换为 numpy 数组\n    filtered_bbox = np.array(filtered_bbox)\n    filtered_class = np.array(filtered_class)\n    return filtered_bbox, filtered_class\n\n\n# 示例用法\n# image_path = 'path_to_your_image.jpg'\n# image = Image.open(image_path).convert(\"RGB\")\n# bboxes = torch.tensor([[40, 50, 100, 120], [60, 70, 80, 90]])  # 示例bbox [(x_min, y_min, width, height), ...]\n\n# cropped_image, adjusted_bboxes = resize_short_side_and_center_crop(image, bboxes)\n\n# print(f\"Adjusted bboxes:\\n{adjusted_bboxes}\")\n# 遍历训练集\nfor data in coco_val:\n    image, annotations = data\n    if len(annotations) == 0:\n        continue  # 跳过没有标注的图像\n\n    # 获取第一个标注的信息\n    # 将图像从张量转换为PIL图像\n\n\n    all_bbox = []\n    all_class = []\n    for ano in annotations:\n        category_id = ano['category_id']\n        bbox = ano['bbox']\n        image_id = ano['image_id']\n\n        # 打印caption和类别名称\n        print(f\"Image ID: {image_id}\")\n        # print(f\"Captions: {get_cap(image_id)}\")\n        captions = coco_caps.imgToAnns[image_id]\n        print(f\"Category Name: {get_name(category_id)}\")\n\n        all_class.append(get_name(category_id))\n        # all_bbox.append(bbox)\n    \n    all_bbox = [t['bbox'] for t in annotations]\n\n    all_bbox = torch.tensor(all_bbox).reshape(-1,4)\n    image_pil = transforms.ToPILImage()(image)\n    # import pdb;pdb.set_trace()\n    image_pil, all_bbox = resize_and_crop(image_pil, all_bbox)\n\n    all_bbox,all_class = filter_box(all_bbox,all_class)\n    # all_bbox, all_class\n    # image_pil = transforms.ToPILImage()(image)\n\n    # 创建画布\n    fig, ax = plt.subplots(1)\n    ax.imshow(image_pil)\n\n    for (bbox, name) in zip(all_bbox, all_class):\n        x, y, width, height = bbox\n        rect = patches.Rectangle((x, y), width, height, linewidth=2, edgecolor='r', facecolor='none')\n        ax.add_patch(rect)\n\n        # 添加类别名称\n        plt.text(x, y, name, color='red', fontsize=12, backgroundcolor='white')\n\n        # 显示图像\n    plt.axis('off')\n    plt.savefig('a.png')\n    import pdb;pdb.set_trace()\n"}
{"type": "source_file", "path": "three_party/Janus/generation_inference.py", "content": "# Copyright (c) 2023-2024 DeepSeek.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the \"Software\"), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n# the Software, and to permit persons to whom the Software is furnished to do so,\n# subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport torch\nfrom transformers import AutoModelForCausalLM\n\nfrom janus.models import MultiModalityCausalLM, VLChatProcessor\nimport numpy as np\nimport os\nimport PIL.Image\n\n# specify the path to the model\nmodel_path = \"deepseek-ai/Janus-1.3B\"\nvl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\ntokenizer = vl_chat_processor.tokenizer\n\nvl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n    model_path, trust_remote_code=True\n)\nvl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n\nconversation = [\n    {\n        \"role\": \"User\",\n        \"content\": \"A close-up high-contrast photo of Sydney Opera House sitting next to Eiffel tower, under a blue night sky of roiling energy, exploding yellow stars, and radiating swirls of blue.\",\n    },\n    {\"role\": \"Assistant\", \"content\": \"\"},\n]\n\nsft_format = vl_chat_processor.apply_sft_template_for_multi_turn_prompts(\n    conversations=conversation,\n    sft_format=vl_chat_processor.sft_format,\n    system_prompt=\"\",\n)\nprompt = sft_format + vl_chat_processor.image_start_tag\n\n\n@torch.inference_mode()\ndef generate(\n    mmgpt: MultiModalityCausalLM,\n    vl_chat_processor: VLChatProcessor,\n    prompt: str,\n    temperature: float = 1,\n    parallel_size: int = 16,\n    cfg_weight: float = 5,\n    image_token_num_per_image: int = 576,\n    img_size: int = 384,\n    patch_size: int = 16,\n):\n    input_ids = vl_chat_processor.tokenizer.encode(prompt)\n    input_ids = torch.LongTensor(input_ids)\n\n    tokens = torch.zeros((parallel_size*2, len(input_ids)), dtype=torch.int).cuda()\n    for i in range(parallel_size*2):\n        tokens[i, :] = input_ids\n        if i % 2 != 0:\n            tokens[i, 1:-1] = vl_chat_processor.pad_id\n\n    inputs_embeds = mmgpt.language_model.get_input_embeddings()(tokens)\n\n    generated_tokens = torch.zeros((parallel_size, image_token_num_per_image), dtype=torch.int).cuda()\n\n    for i in range(image_token_num_per_image):\n        outputs = mmgpt.language_model.model(inputs_embeds=inputs_embeds, use_cache=True, past_key_values=outputs.past_key_values if i != 0 else None)\n        hidden_states = outputs.last_hidden_state\n        \n        logits = mmgpt.gen_head(hidden_states[:, -1, :])\n        logit_cond = logits[0::2, :]\n        logit_uncond = logits[1::2, :]\n        \n        logits = logit_uncond + cfg_weight * (logit_cond-logit_uncond)\n        probs = torch.softmax(logits / temperature, dim=-1)\n\n        next_token = torch.multinomial(probs, num_samples=1)\n        generated_tokens[:, i] = next_token.squeeze(dim=-1)\n\n        next_token = torch.cat([next_token.unsqueeze(dim=1), next_token.unsqueeze(dim=1)], dim=1).view(-1)\n        img_embeds = mmgpt.prepare_gen_img_embeds(next_token)\n        inputs_embeds = img_embeds.unsqueeze(dim=1)\n\n\n    dec = mmgpt.gen_vision_model.decode_code(generated_tokens.to(dtype=torch.int), shape=[parallel_size, 8, img_size//patch_size, img_size//patch_size])\n    dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)\n\n    dec = np.clip((dec + 1) / 2 * 255, 0, 255)\n\n    visual_img = np.zeros((parallel_size, img_size, img_size, 3), dtype=np.uint8)\n    visual_img[:, :, :] = dec\n\n    os.makedirs('generated_samples', exist_ok=True)\n    for i in range(parallel_size):\n        save_path = os.path.join('generated_samples', \"img_{}.jpg\".format(i))\n        PIL.Image.fromarray(visual_img[i]).save(save_path)\n\n\ngenerate(\n    vl_gpt,\n    vl_chat_processor,\n    prompt,\n)"}
{"type": "source_file", "path": "project/plangen/dataset/oim/data_oim.py", "content": "from packaging import version\nfrom PIL import Image\nfrom torchvision import transforms\nimport os\nimport PIL\nfrom torch.utils.data import Dataset\nimport torchvision\nimport numpy as np\nimport torch\nimport random\nimport albumentations as A\nimport copy\nimport cv2\nimport pandas as pd\nimport glob\nfrom torchvision.transforms import Resize\nfrom tqdm import tqdm\n\nfrom src.utils.funcs import *\nfrom ..coco.data_coco import filter_box, resize_and_crop\n\ndef split_list(lst, n):\n    \"\"\"\n    将列表 lst 划分为 n 等份。\n\n    Args:\n        lst (list): 需要划分的列表。\n        n (int): 划分的份数。\n\n    Returns:\n        list: 包含 n 个子列表的列表。\n    \"\"\"\n    # 计算每份的长度\n    k, m = divmod(len(lst), n)\n    # 使用列表生成器划分列表\n    return [lst[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n)]\n\nclass Dataset_oim(Dataset):\n    def __init__(\n        self,\n        args=None,\n        data_root='/home/jovyan/multi-modal-datasets/public/OID',\n        split=\"train\",\n        easy_mode=False,\n    ):\n        self.args = args\n        self.data_root = data_root\n        self.split = split\n        self.easy_mode = easy_mode\n\n        self.bbox_path_list = []\n        if self.split == \"train\":\n            bboxs_path = os.path.join(data_root, 'anno', f'oidv6-train-annotations-bbox.csv')\n        elif self.split == \"validation\":\n            bboxs_path = os.path.join(data_root, 'anno', f'validation-annotations-bbox.csv')\n        else:\n            bboxs_path = os.path.join(data_root, 'anno', f'test-annotations-bbox.csv') # 93w\n\n        df_bbox = pd.read_csv(bboxs_path)\n        bbox_groups = df_bbox.groupby(df_bbox.LabelName)\n\n        #anno_files = pd.read_csv('project/janus/dataset/oim/class-descriptions-boxable.csv')\n        anno_files = pd.read_csv('project/plangen/dataset/oim/class-descriptions-boxable.csv')\n        self.anno_dict = anno_files.set_index(anno_files.columns[0])[anno_files.columns[1]].to_dict()\n\n        self.image_ids = df_bbox['ImageID'].unique()\n        self.df_bbox = df_bbox\n\n        if self.args.oim_split is not None:\n            self.image_ids = split_list(self.image_ids, 16)[self.args.oim_split]\n            self.image_ids = self.image_ids[self.args.oim_start*8:]\n\n        print(f\"openimage, len(image): {len(self.image_ids)}\")\n\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, i):\n\n        image_id = self.image_ids[i]\n        bboxes = self.df_bbox[self.df_bbox['ImageID']==image_id]\n\n        img_path = os.path.join(self.data_root, self.split, f'{image_id}.jpg')\n\n        boxes = torch.stack([\n            torch.tensor(bboxes['XMin'].tolist()), \n            torch.tensor(bboxes['YMin'].tolist()), \n            torch.tensor(bboxes['XMax'].tolist()), \n            torch.tensor(bboxes['YMax'].tolist())], \n        dim=-1)\n\n        try:\n            classes = [self.anno_dict[t].lower() for t in bboxes['LabelName']]\n        except:\n            return self.__getitem__((i+1)%len(self))\n\n        # print(\"len(boxes)\")\n        # print(len(boxes))\n        areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n        sorted_indices = torch.argsort(-areas)\n        # import pdb;pdb.set_trace()\n        boxes = boxes[sorted_indices][:10]\n        classes = np.array(classes)[sorted_indices.tolist()].tolist()[:10]\n\n        data = [img_path, boxes, classes]\n        image_path, obj_bbox, obj_class = data\n        obj_bbox = obj_bbox.reshape(-1,4)\n\n        image_pil = Image.open(image_path).convert('RGB')\n        w,h = image_pil.size\n        obj_bbox[:,0::2] *= w\n        obj_bbox[:,1::2] *= h\n        obj_bbox[:,2] = obj_bbox[:,2]-obj_bbox[:,0]\n        obj_bbox[:,3] = obj_bbox[:,3]-obj_bbox[:,1]\n\n        image_pil, obj_bbox = resize_and_crop(image_pil, obj_bbox)\n        image =  transforms.ToTensor()(image_pil)\n        image = image*2-1\n        obj_bbox, obj_class = filter_box(obj_bbox, obj_class)\n        obj_bbox = obj_bbox/384\n        obj_bbox = obj_bbox.reshape(-1,4)\n        obj_bbox[:,2] = obj_bbox[:,0] + obj_bbox[:,2]\n        obj_bbox[:,3] = obj_bbox[:,1] + obj_bbox[:,3]\n\n        # import pdb;pdb.set_trace()\n        try:\n            cap = load_jsonl(f\"gen_data/oim_cap2/{image_id}.jsonl\")[0]\n        except:\n            if self.split != 'test':\n                print('no caps!!!')\n                save_jsonl(f\"gen_data/oim_cap_nocap2/{image_id}.jsonl\", [])\n            cap = ''\n\n        # if len(cap) > 400:\n        #     print('len(cap)>400')\n        #     # print(cap)\n        #     cap = ''\n\n        if self.easy_mode:\n            return dict(\n                base_caption=cap,\n                image=image,\n                image_path=image_path,\n                # image_pil=image_pil,\n                image_id=image_id,\n            )\n\n        return dict(\n            base_caption=cap,\n            obj_bbox=obj_bbox,\n            obj_class=obj_class,\n            image=image,\n            image_path=image_path,\n            image_pil=image_pil,\n            image_id=image_id,\n        )\n\n\nif __name__ == '__main__':\n    dataset = OpenImagesDataset()\n    for data in dataset:\n        import pdb;pdb.set_trace()\n        pass\n"}
{"type": "source_file", "path": "three_party/Janus/inference.py", "content": "# Copyright (c) 2023-2024 DeepSeek.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the \"Software\"), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n# the Software, and to permit persons to whom the Software is furnished to do so,\n# subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport torch\nfrom transformers import AutoModelForCausalLM\n\nfrom janus.models import MultiModalityCausalLM, VLChatProcessor\nfrom janus.utils.io import load_pil_images\n\n# specify the path to the model\nmodel_path = \"deepseek-ai/Janus-1.3B\"\nvl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\ntokenizer = vl_chat_processor.tokenizer\n\nvl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n    model_path, trust_remote_code=True\n)\nvl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n\nconversation = [\n    {\n        \"role\": \"User\",\n        \"content\": \"<image_placeholder>\\nConvert the formula into latex code.\",\n        \"images\": [\"images/equation.png\"],\n    },\n    {\"role\": \"Assistant\", \"content\": \"\"},\n]\n\n# load images and prepare for inputs\npil_images = load_pil_images(conversation)\nprepare_inputs = vl_chat_processor(\n    conversations=conversation, images=pil_images, force_batchify=True\n).to(vl_gpt.device)\n\n# # run image encoder to get the image embeddings\ninputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n\n# # run the model to get the response\noutputs = vl_gpt.language_model.generate(\n    inputs_embeds=inputs_embeds,\n    attention_mask=prepare_inputs.attention_mask,\n    pad_token_id=tokenizer.eos_token_id,\n    bos_token_id=tokenizer.bos_token_id,\n    eos_token_id=tokenizer.eos_token_id,\n    max_new_tokens=512,\n    do_sample=False,\n    use_cache=True,\n)\n\nanswer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\nprint(f\"{prepare_inputs['sft_format'][0]}\", answer)\n"}
{"type": "source_file", "path": "three_party/Janus/janus/janusflow/models/clip_encoder.py", "content": "# Copyright (c) 2023-2024 DeepSeek.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the \"Software\"), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n# the Software, and to permit persons to whom the Software is furnished to do so,\n# subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom typing import Dict, List, Literal, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms\nfrom einops import rearrange\n\nfrom janus.janusflow.models.siglip_vit import create_siglip_vit\n\n\nclass CLIPVisionTower(nn.Module):\n    def __init__(\n        self,\n        model_name: str = \"siglip_large_patch16_384\",\n        image_size: Union[Tuple[int, int], int] = 336,\n        select_feature: str = \"patch\",\n        select_layer: int = -2,\n        select_layers: list = None,\n        ckpt_path: str = \"\",\n        pixel_mean: Optional[List[float]] = None,\n        pixel_std: Optional[List[float]] = None,\n        **kwargs,\n    ):\n        super().__init__()\n\n        self.model_name = model_name\n        self.select_feature = select_feature\n        self.select_layer = select_layer\n        self.select_layers = select_layers\n\n        vision_tower_params = {\n            \"model_name\": model_name,\n            \"image_size\": image_size,\n            \"ckpt_path\": ckpt_path,\n            \"select_layer\": select_layer,\n        }\n        vision_tower_params.update(kwargs)\n        self.vision_tower, self.forward_kwargs = self.build_vision_tower(\n            vision_tower_params\n        )\n\n        if pixel_mean is not None and pixel_std is not None:\n            image_norm = torchvision.transforms.Normalize(\n                mean=pixel_mean, std=pixel_std\n            )\n        else:\n            image_norm = None\n\n        self.image_norm = image_norm\n\n    def build_vision_tower(self, vision_tower_params):\n        if self.model_name.startswith(\"siglip\"):\n            self.select_feature = \"same\"\n            vision_tower = create_siglip_vit(**vision_tower_params)\n            forward_kwargs = dict()\n\n        elif self.model_name.startswith(\"sam\"):\n            vision_tower = create_sam_vit(**vision_tower_params)\n            forward_kwargs = dict()\n\n        else:  # huggingface\n            from transformers import CLIPVisionModel\n\n            vision_tower = CLIPVisionModel.from_pretrained(**vision_tower_params)\n            forward_kwargs = dict(output_hidden_states=True)\n\n        return vision_tower, forward_kwargs\n\n    def feature_select(self, image_forward_outs):\n        if isinstance(image_forward_outs, torch.Tensor):\n            # the output has been the self.select_layer\"s features\n            image_features = image_forward_outs\n        else:\n            image_features = image_forward_outs.hidden_states[self.select_layer]\n\n        if self.select_feature == \"patch\":\n            # if the output has cls_token\n            image_features = image_features[:, 1:]\n        elif self.select_feature == \"cls_patch\":\n            image_features = image_features\n        elif self.select_feature == \"same\":\n            image_features = image_features\n\n        else:\n            raise ValueError(f\"Unexpected select feature: {self.select_feature}\")\n        return image_features\n\n    def forward(self, images):\n        \"\"\"\n\n        Args:\n            images (torch.Tensor): [b, 3, H, W]\n\n        Returns:\n            image_features (torch.Tensor): [b, n_patch, d]\n        \"\"\"\n\n        if self.image_norm is not None:\n            images = self.image_norm(images)\n\n        image_forward_outs = self.vision_tower(images, **self.forward_kwargs)\n        image_features = self.feature_select(image_forward_outs)\n        return image_features\n"}
{"type": "source_file", "path": "project/plangen/dataset/set_dataset.py", "content": "import torch\nfrom copy import deepcopy\nfrom .data_toy import Dataset_toy\nfrom .data_hico import Hico_dataset\nfrom torch.utils.data import ConcatDataset\n\ndef get_one_dataset(args, data, is_test=False, **kwargs):\n    if data == 'data_toy':\n        return Dataset_toy(args, is_test=is_test)\n    elif data == 'sam':\n        return Hico_dataset(args, is_test=is_test, is_sam=True)\n    elif data == 'coco':\n        return Hico_dataset(args, is_test=is_test, is_coco=True)\n    elif data == 'coco_rm':\n        return Hico_dataset(args, is_test=is_test, is_coco=True, for_rm=True)\n    elif data == 'coco_val17':\n        return Hico_dataset(args, is_test=is_test, is_coco=True, coco_split='val17')\n    elif data == 'oim':\n        return Hico_dataset(args, is_test=is_test, is_oim=True)\n    elif data == 'oim_test':\n        return Hico_dataset(args, is_test=is_test, is_oim=True, oim_split='test')\n    elif data == 'mb':\n        return Hico_dataset(args, is_test=is_test, is_mb=True, mb_split='test')\n    elif data == 'mb_train':\n        return Hico_dataset(args, is_test=is_test, is_mb=True, mb_split='train')\n    elif data == 'ultra':\n        return Hico_dataset(args, is_test=is_test, is_mb=True, mb_split='ultra')\n    elif data == 'hico':\n        return Hico_dataset(args, is_test=is_test)\n    elif data == 'creati':\n        return Hico_dataset(args, is_test=is_test, is_creati=True)\n    elif data == '1k':\n        return Hico_dataset(args, is_test=is_test, is_creati=True, use_1k=True)\n    elif data == '1k_obj':\n        return Hico_dataset(args, is_test=is_test, is_creati=True, use_1k=True, obj_level=True)\n    elif data == 'layout':\n        return Hico_dataset(args, is_test=is_test, is_layout=True)\n    elif data == 'gen':\n        return Hico_dataset(args, is_test=is_test, is_gen=True)\n    elif data == 'edit':\n        return Hico_dataset(args, is_test=is_test, is_edit=True)\n    elif data == 'edit_coco':\n        return Hico_dataset(args, is_test=is_test, is_edit=True, edit_split='edit_coco')\n    elif data == 'rm_coco':\n        return Hico_dataset(args, is_test=is_test, is_edit=True, edit_split='rm_coco')\n    elif data == 'hico_full':\n        return Hico_dataset(args, is_test=is_test, tiny_hico_data=False)\n    elif data == 'hico_7k':\n        return Hico_dataset(args, is_test=is_test, is_7k=True)\n    elif data == 'plan_llama':\n        return Hico_dataset(args, is_test=is_test, is_plan=True, plan_split='llama')\n    elif data == 'plan_r1_qwen':\n        return Hico_dataset(args, is_test=is_test, is_plan=True, plan_split='r1_qwen')\n    elif data == 'plan_r1':\n        return Hico_dataset(args, is_test=is_test, is_plan=True, plan_split='r1')\n    elif data == 'plan_qwen':\n        return Hico_dataset(args, is_test=is_test, is_plan=True, plan_split='qwen')\n    elif data == 'plan_r1':\n        return Hico_dataset(args, is_test=is_test, is_plan=True, plan_split='r1')\n    elif data == 'hico_full_d':\n        return Hico_dataset(args, is_test=is_test, tiny_hico_data=False, can_dropout=True)\n    elif data == 'hico_d':\n        return Hico_dataset(args, is_test=is_test, can_dropout=True)\n    elif data == 'hico_test':\n        return Hico_dataset(args, is_test=is_test, tiny_hico_data=True, hico_split='test')\n    elif data == 'hico_val':\n        return Hico_dataset(args, is_test=is_test, tiny_hico_data=True, hico_split='val')\n        import pdb;pdb.set_trace()\n    elif isinstance(data, list):\n        datas = [] \n        for data_type in data:\n            dataset = get_one_dataset(args, data_type, is_test=is_test, **kwargs)\n            datas.append(dataset)\n        dataset = ConcatDataset(datas)\n        return dataset\n    else:\n        assert False\n\ndef loader(dataset, shuffle=True, batch_size=1, dataloader_num_workers=16, pin_memory=False, collate_fn=None, prefetch_factor=None):\n    return torch.utils.data.DataLoader(\n        dataset,\n        shuffle=shuffle,\n        batch_size=batch_size,\n        num_workers=dataloader_num_workers,\n        pin_memory=pin_memory,\n        collate_fn=collate_fn,\n        prefetch_factor=prefetch_factor,\n    )\n\n\ndef set_dataset(args, train_data, test_data, train_batch_size, test_batch_size, tokenizer=None, accelerator=None, collate_fn=None, train_dataset=None, test_dataset=None):\n    if train_dataset is None:\n        train_dataset = get_one_dataset(args, train_data)\n    if test_dataset is None:\n        test_dataset  = get_one_dataset(args, test_data, is_test=True)\n\n    if collate_fn is None:\n        collate_fn = torch.utils.data.dataloader.default_collate\n\n    train_dataloader = loader(train_dataset, True, train_batch_size, args.dataloader_num_workers, args.pin_memory, collate_fn=collate_fn)\n    test_dataloader  = loader(test_dataset, False, test_batch_size, args.dataloader_num_workers, args.pin_memory, collate_fn=collate_fn)\n\n    return train_dataset, test_dataset, train_dataloader, test_dataloader\n\ndef get_dataset(args, data_name, batch_size, tokenizer=None, accelerator=None, collate_fn=None, dataset=None, is_test=False, prefetch_factor=None):\n    if dataset is None:\n        dataset = get_one_dataset(args, data_name, is_test=is_test)\n\n    if collate_fn is None:\n        collate_fn = torch.utils.data.dataloader.default_collate\n\n    dataloader = loader(\n        dataset, \n        False if is_test else True, \n        batch_size, \n        args.dataloader_num_workers, \n        args.pin_memory, \n        collate_fn=collate_fn,\n        prefetch_factor=prefetch_factor,\n    )\n\n    return dataset, dataloader"}
{"type": "source_file", "path": "three_party/Janus/interactivechat.py", "content": "import os\nimport PIL.Image\nimport torch\nimport numpy as np\nfrom transformers import AutoModelForCausalLM\nfrom janus.models import MultiModalityCausalLM, VLChatProcessor\nimport time\nimport re\n\n# Specify the path to the model\nmodel_path = \"deepseek-ai/Janus-1.3B\"\nvl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\ntokenizer = vl_chat_processor.tokenizer\n\nvl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n    model_path, trust_remote_code=True\n)\nvl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n\n\ndef create_prompt(user_input: str) -> str:\n    conversation = [\n        {\n            \"role\": \"User\",\n            \"content\": user_input,\n        },\n        {\"role\": \"Assistant\", \"content\": \"\"},\n    ]\n\n    sft_format = vl_chat_processor.apply_sft_template_for_multi_turn_prompts(\n        conversations=conversation,\n        sft_format=vl_chat_processor.sft_format,\n        system_prompt=\"\",\n    )\n    prompt = sft_format + vl_chat_processor.image_start_tag\n    return prompt\n\n\n@torch.inference_mode()\ndef generate(\n    mmgpt: MultiModalityCausalLM,\n    vl_chat_processor: VLChatProcessor,\n    prompt: str,\n    short_prompt: str,\n    parallel_size: int = 16,\n    temperature: float = 1,\n    cfg_weight: float = 5,\n    image_token_num_per_image: int = 576,\n    img_size: int = 384,\n    patch_size: int = 16,\n):\n    input_ids = vl_chat_processor.tokenizer.encode(prompt)\n    input_ids = torch.LongTensor(input_ids)\n\n    tokens = torch.zeros((parallel_size * 2, len(input_ids)), dtype=torch.int).cuda()\n    for i in range(parallel_size * 2):\n        tokens[i, :] = input_ids\n        if i % 2 != 0:\n            tokens[i, 1:-1] = vl_chat_processor.pad_id\n\n    inputs_embeds = mmgpt.language_model.get_input_embeddings()(tokens)\n\n    generated_tokens = torch.zeros((parallel_size, image_token_num_per_image), dtype=torch.int).cuda()\n    outputs = None  # Initialize outputs for use in the loop\n\n    for i in range(image_token_num_per_image):\n        outputs = mmgpt.language_model.model(\n            inputs_embeds=inputs_embeds,\n            use_cache=True,\n            past_key_values=outputs.past_key_values if i != 0 else None\n        )\n        hidden_states = outputs.last_hidden_state\n\n        logits = mmgpt.gen_head(hidden_states[:, -1, :])\n        logit_cond = logits[0::2, :]\n        logit_uncond = logits[1::2, :]\n\n        logits = logit_uncond + cfg_weight * (logit_cond - logit_uncond)\n        probs = torch.softmax(logits / temperature, dim=-1)\n\n        next_token = torch.multinomial(probs, num_samples=1)\n        generated_tokens[:, i] = next_token.squeeze(dim=-1)\n\n        next_token = torch.cat([next_token.unsqueeze(dim=1), next_token.unsqueeze(dim=1)], dim=1).view(-1)\n        img_embeds = mmgpt.prepare_gen_img_embeds(next_token)\n        inputs_embeds = img_embeds.unsqueeze(dim=1)\n\n    dec = mmgpt.gen_vision_model.decode_code(\n        generated_tokens.to(dtype=torch.int),\n        shape=[parallel_size, 8, img_size // patch_size, img_size // patch_size]\n    )\n    dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)\n\n    dec = np.clip((dec + 1) / 2 * 255, 0, 255)\n\n    visual_img = np.zeros((parallel_size, img_size, img_size, 3), dtype=np.uint8)\n    visual_img[:, :, :] = dec\n\n    os.makedirs('generated_samples', exist_ok=True)\n\n    # Create a timestamp\n    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n\n    # Sanitize the short_prompt to ensure it's safe for filenames\n    short_prompt = re.sub(r'\\W+', '_', short_prompt)[:50]\n\n    # Save images with timestamp and part of the user prompt in the filename\n    for i in range(parallel_size):\n        save_path = os.path.join('generated_samples', f\"img_{timestamp}_{short_prompt}_{i}.jpg\")\n        PIL.Image.fromarray(visual_img[i]).save(save_path)\n\n\ndef interactive_image_generator():\n    print(\"Welcome to the interactive image generator!\")\n\n    # Ask for the number of images at the start of the session\n    while True:\n        num_images_input = input(\"How many images would you like to generate per prompt? (Enter a positive integer): \")\n        if num_images_input.isdigit() and int(num_images_input) > 0:\n            parallel_size = int(num_images_input)\n            break\n        else:\n            print(\"Invalid input. Please enter a positive integer.\")\n\n    while True:\n        user_input = input(\"Please describe the image you'd like to generate (or type 'exit' to quit): \")\n\n        if user_input.lower() == 'exit':\n            print(\"Exiting the image generator. Goodbye!\")\n            break\n\n        prompt = create_prompt(user_input)\n\n        # Create a sanitized version of user_input for the filename\n        short_prompt = re.sub(r'\\W+', '_', user_input)[:50]\n\n        print(f\"Generating {parallel_size} image(s) for: '{user_input}'\")\n        generate(\n            mmgpt=vl_gpt,\n            vl_chat_processor=vl_chat_processor,\n            prompt=prompt,\n            short_prompt=short_prompt,\n            parallel_size=parallel_size  # Pass the user-specified number of images\n        )\n\n        print(\"Image generation complete! Check the 'generated_samples' folder for the output.\\n\")\n\n\nif __name__ == \"__main__\":\n    interactive_image_generator()\n"}
{"type": "source_file", "path": "three_party/Janus/demo/app_januspro.py", "content": "import gradio as gr\nimport torch\nfrom transformers import AutoConfig, AutoModelForCausalLM\nfrom janus.models import MultiModalityCausalLM, VLChatProcessor\nfrom janus.utils.io import load_pil_images\nfrom PIL import Image\n\nimport numpy as np\nimport os\nimport time\n# import spaces  # Import spaces for ZeroGPU compatibility\n\n\n# Load model and processor\nmodel_path = \"deepseek-ai/Janus-Pro-7B\"\nconfig = AutoConfig.from_pretrained(model_path)\nlanguage_config = config.language_config\nlanguage_config._attn_implementation = 'eager'\nvl_gpt = AutoModelForCausalLM.from_pretrained(model_path,\n                                             language_config=language_config,\n                                             trust_remote_code=True)\nif torch.cuda.is_available():\n    vl_gpt = vl_gpt.to(torch.bfloat16).cuda()\nelse:\n    vl_gpt = vl_gpt.to(torch.float16)\n\nvl_chat_processor = VLChatProcessor.from_pretrained(model_path)\ntokenizer = vl_chat_processor.tokenizer\ncuda_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n@torch.inference_mode()\n# @spaces.GPU(duration=120) \n# Multimodal Understanding function\ndef multimodal_understanding(image, question, seed, top_p, temperature):\n    # Clear CUDA cache before generating\n    torch.cuda.empty_cache()\n    \n    # set seed\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    torch.cuda.manual_seed(seed)\n    \n    conversation = [\n        {\n            \"role\": \"<|User|>\",\n            \"content\": f\"<image_placeholder>\\n{question}\",\n            \"images\": [image],\n        },\n        {\"role\": \"<|Assistant|>\", \"content\": \"\"},\n    ]\n    \n    pil_images = [Image.fromarray(image)]\n    prepare_inputs = vl_chat_processor(\n        conversations=conversation, images=pil_images, force_batchify=True\n    ).to(cuda_device, dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16)\n    \n    \n    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n    \n    outputs = vl_gpt.language_model.generate(\n        inputs_embeds=inputs_embeds,\n        attention_mask=prepare_inputs.attention_mask,\n        pad_token_id=tokenizer.eos_token_id,\n        bos_token_id=tokenizer.bos_token_id,\n        eos_token_id=tokenizer.eos_token_id,\n        max_new_tokens=512,\n        do_sample=False if temperature == 0 else True,\n        use_cache=True,\n        temperature=temperature,\n        top_p=top_p,\n    )\n    \n    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n    return answer\n\n\ndef generate(input_ids,\n             width,\n             height,\n             temperature: float = 1,\n             parallel_size: int = 5,\n             cfg_weight: float = 5,\n             image_token_num_per_image: int = 576,\n             patch_size: int = 16):\n    # Clear CUDA cache before generating\n    torch.cuda.empty_cache()\n    \n    tokens = torch.zeros((parallel_size * 2, len(input_ids)), dtype=torch.int).to(cuda_device)\n    for i in range(parallel_size * 2):\n        tokens[i, :] = input_ids\n        if i % 2 != 0:\n            tokens[i, 1:-1] = vl_chat_processor.pad_id\n    inputs_embeds = vl_gpt.language_model.get_input_embeddings()(tokens)\n    generated_tokens = torch.zeros((parallel_size, image_token_num_per_image), dtype=torch.int).to(cuda_device)\n\n    pkv = None\n    for i in range(image_token_num_per_image):\n        with torch.no_grad():\n            outputs = vl_gpt.language_model.model(inputs_embeds=inputs_embeds,\n                                                use_cache=True,\n                                                past_key_values=pkv)\n            pkv = outputs.past_key_values\n            hidden_states = outputs.last_hidden_state\n            logits = vl_gpt.gen_head(hidden_states[:, -1, :])\n            logit_cond = logits[0::2, :]\n            logit_uncond = logits[1::2, :]\n            logits = logit_uncond + cfg_weight * (logit_cond - logit_uncond)\n            probs = torch.softmax(logits / temperature, dim=-1)\n            next_token = torch.multinomial(probs, num_samples=1)\n            generated_tokens[:, i] = next_token.squeeze(dim=-1)\n            next_token = torch.cat([next_token.unsqueeze(dim=1), next_token.unsqueeze(dim=1)], dim=1).view(-1)\n\n            img_embeds = vl_gpt.prepare_gen_img_embeds(next_token)\n            inputs_embeds = img_embeds.unsqueeze(dim=1)\n\n    \n\n    patches = vl_gpt.gen_vision_model.decode_code(generated_tokens.to(dtype=torch.int),\n                                                 shape=[parallel_size, 8, width // patch_size, height // patch_size])\n\n    return generated_tokens.to(dtype=torch.int), patches\n\ndef unpack(dec, width, height, parallel_size=5):\n    dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)\n    dec = np.clip((dec + 1) / 2 * 255, 0, 255)\n\n    visual_img = np.zeros((parallel_size, width, height, 3), dtype=np.uint8)\n    visual_img[:, :, :] = dec\n\n    return visual_img\n\n\n\n@torch.inference_mode()\n# @spaces.GPU(duration=120)  # Specify a duration to avoid timeout\ndef generate_image(prompt,\n                   seed=None,\n                   guidance=5,\n                   t2i_temperature=1.0):\n    # Clear CUDA cache and avoid tracking gradients\n    torch.cuda.empty_cache()\n    # Set the seed for reproducible results\n    if seed is not None:\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        np.random.seed(seed)\n    width = 384\n    height = 384\n    parallel_size = 5\n    \n    with torch.no_grad():\n        messages = [{'role': '<|User|>', 'content': prompt},\n                    {'role': '<|Assistant|>', 'content': ''}]\n        text = vl_chat_processor.apply_sft_template_for_multi_turn_prompts(conversations=messages,\n                                                                   sft_format=vl_chat_processor.sft_format,\n                                                                   system_prompt='')\n        text = text + vl_chat_processor.image_start_tag\n        \n        input_ids = torch.LongTensor(tokenizer.encode(text))\n        output, patches = generate(input_ids,\n                                   width // 16 * 16,\n                                   height // 16 * 16,\n                                   cfg_weight=guidance,\n                                   parallel_size=parallel_size,\n                                   temperature=t2i_temperature)\n        images = unpack(patches,\n                        width // 16 * 16,\n                        height // 16 * 16,\n                        parallel_size=parallel_size)\n\n        return [Image.fromarray(images[i]).resize((768, 768), Image.LANCZOS) for i in range(parallel_size)]\n        \n\n# Gradio interface\nwith gr.Blocks() as demo:\n    gr.Markdown(value=\"# Multimodal Understanding\")\n    with gr.Row():\n        image_input = gr.Image()\n        with gr.Column():\n            question_input = gr.Textbox(label=\"Question\")\n            und_seed_input = gr.Number(label=\"Seed\", precision=0, value=42)\n            top_p = gr.Slider(minimum=0, maximum=1, value=0.95, step=0.05, label=\"top_p\")\n            temperature = gr.Slider(minimum=0, maximum=1, value=0.1, step=0.05, label=\"temperature\")\n        \n    understanding_button = gr.Button(\"Chat\")\n    understanding_output = gr.Textbox(label=\"Response\")\n\n    examples_inpainting = gr.Examples(\n        label=\"Multimodal Understanding examples\",\n        examples=[\n            [\n                \"explain this meme\",\n                \"images/doge.png\",\n            ],\n            [\n                \"Convert the formula into latex code.\",\n                \"images/equation.png\",\n            ],\n        ],\n        inputs=[question_input, image_input],\n    )\n    \n        \n    gr.Markdown(value=\"# Text-to-Image Generation\")\n\n    \n    \n    with gr.Row():\n        cfg_weight_input = gr.Slider(minimum=1, maximum=10, value=5, step=0.5, label=\"CFG Weight\")\n        t2i_temperature = gr.Slider(minimum=0, maximum=1, value=1.0, step=0.05, label=\"temperature\")\n\n    prompt_input = gr.Textbox(label=\"Prompt. (Prompt in more detail can help produce better images!)\")\n    seed_input = gr.Number(label=\"Seed (Optional)\", precision=0, value=12345)\n\n    generation_button = gr.Button(\"Generate Images\")\n\n    image_output = gr.Gallery(label=\"Generated Images\", columns=2, rows=2, height=300)\n\n    examples_t2i = gr.Examples(\n        label=\"Text to image generation examples.\",\n        examples=[\n            \"Master shifu racoon wearing drip attire as a street gangster.\",\n            \"The face of a beautiful girl\",\n            \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\",\n            \"A glass of red wine on a reflective surface.\",\n            \"A cute and adorable baby fox with big brown eyes, autumn leaves in the background enchanting,immortal,fluffy, shiny mane,Petals,fairyism,unreal engine 5 and Octane Render,highly detailed, photorealistic, cinematic, natural colors.\",\n            \"The image features an intricately designed eye set against a circular backdrop adorned with ornate swirl patterns that evoke both realism and surrealism. At the center of attention is a strikingly vivid blue iris surrounded by delicate veins radiating outward from the pupil to create depth and intensity. The eyelashes are long and dark, casting subtle shadows on the skin around them which appears smooth yet slightly textured as if aged or weathered over time.\\n\\nAbove the eye, there's a stone-like structure resembling part of classical architecture, adding layers of mystery and timeless elegance to the composition. This architectural element contrasts sharply but harmoniously with the organic curves surrounding it. Below the eye lies another decorative motif reminiscent of baroque artistry, further enhancing the overall sense of eternity encapsulated within each meticulously crafted detail. \\n\\nOverall, the atmosphere exudes a mysterious aura intertwined seamlessly with elements suggesting timelessness, achieved through the juxtaposition of realistic textures and surreal artistic flourishes. Each component\\u2014from the intricate designs framing the eye to the ancient-looking stone piece above\\u2014contributes uniquely towards creating a visually captivating tableau imbued with enigmatic allure.\",\n        ],\n        inputs=prompt_input,\n    )\n    \n    understanding_button.click(\n        multimodal_understanding,\n        inputs=[image_input, question_input, und_seed_input, top_p, temperature],\n        outputs=understanding_output\n    )\n    \n    generation_button.click(\n        fn=generate_image,\n        inputs=[prompt_input, seed_input, cfg_weight_input, t2i_temperature],\n        outputs=image_output\n    )\n\ndemo.launch(share=True)\n# demo.queue(concurrency_count=1, max_size=10).launch(server_name=\"0.0.0.0\", server_port=37906, root_path=\"/path\")"}
{"type": "source_file", "path": "src/utils/funcs.py", "content": "import os\nimport PIL.Image\nimport torch\nfrom torch.cuda.amp import custom_bwd, custom_fwd\nimport csv\nimport numpy as np\nimport fire\nfrom torchvision.transforms import Resize, CenterCrop, Normalize\nimport PIL\nimport requests\nimport json\nimport cv2\nimport os.path as osp\nfrom tqdm import tqdm\nfrom PIL import Image, ImageDraw\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Union\nfrom omegaconf import OmegaConf\nimport random\nfrom copy import deepcopy\nfrom torchvision import transforms\nimport types\nfrom diffusers.utils.torch_utils import randn_tensor\nimport torch.nn.functional as F\nimport shutil\nfrom diffusers.optimization import get_scheduler\nfrom rich import print\nfrom rich.console import Console\nfrom rich.progress import Progress\nfrom rich.table import Table\nimport kornia\nfrom glob import glob\nfrom PIL import Image, ImageDraw, ImageFont\nimport re\nfrom torchvision.transforms import ToPILImage\nconsole = Console()\nfrom torchvision.transforms import ToPILImage, ToTensor\nto_pil = ToPILImage()\nto_ts = ToTensor()\n\nimport pickle\ndef save_pickle(data, name):\n    with open(name, 'wb') as f:\n        pickle.dump(data, f)\n\ndef load_pickle(name):\n    with open(name, 'rb') as f:\n        data = pickle.load(f)\n    return data\n\nimport matplotlib.pyplot as plt\n\ndef vis_mask(x, name='a.png'):\n    plt.imshow(x.cpu(), cmap='Greys', interpolation='nearest')\n    plt.savefig(name)\n    plt.close()\n\ndef pt2clipnp(refer_image):\n    refer_image = resize_pt(denorm_pt(refer_image), 224)\n    refer_image = norm_pt(refer_image, [0.48145466, 0.4578275, 0.40821073], [0.26862954, 0.26130258, 0.27577711])\n    return refer_image\n\ndef insert_path(path, new):\n    pre,post = path.rsplit('/',1)\n    new_path = osp.join(pre, new, post)\n    mkdir(osp.dirname(new_path))\n    return new_path\n\n##############################\n\n# 转换函数\ndef convert_coordinates(coord_str):\n    # 提取所有数字部分\n    parts = coord_str.split(',')\n    # 处理每个部分\n    result = []\n    for part in parts:\n        # 去掉 < 和 >，提取数字\n        num = int(part.strip('<>')[1:])  # 去掉 < 和 >，并去掉首字母（h 或 w）\n        # 乘以 10\n        num *= 10\n        result.append(str(num))\n    # 拼接成目标格式\n    return ','.join(result)\n\n\ndef draw_boxes_on_image(image, prompt, use_centerhw=False):\n    draw = ImageDraw.Draw(image)\n\n    try:\n        font = ImageFont.truetype(\"/home/jovyan/boomcheng-data/tools/font/msyh.ttf\", 20)\n    except IOError:\n        font = ImageFont.load_default()\n\n    pattern = r\"<ref>(.*?)</ref><box>\\[(.*?)\\]</box>\"\n    matches = re.findall(pattern, prompt)\n\n    if len(matches) == 0:\n        pattern = r\"<ref>(.*?)</ref><box>(.*?)</box>\"\n        matches = re.findall(pattern, prompt)\n        try:\n            matches = [(x,convert_coordinates(y)) for x,y in matches]\n        except:\n            matches = []\n\n    prompts = []\n    boxes = []\n    h,w = image.size\n    for desc, box in matches:\n        try:\n            ori_x1, ori_y1, ori_x2, ori_y2 = map(int, box.split(\",\"))\n        except:\n            continue\n\n        if use_centerhw:\n            cx, cy, _h, _w = ori_x1, ori_y1, ori_x2, ori_y2\n            x1 = cx - _w / 2\n            y1 = cy - _h / 2\n            x2 = cx + _w / 2\n            y2 = cy + _h / 2\n            ori_x1, ori_y1, ori_x2, ori_y2 = x1, y1, x2, y2\n\n        x1 = int(ori_x1/1000*h)\n        x2 = int(ori_x2/1000*h)\n        y1 = int(ori_y1/1000*h)\n        y2 = int(ori_y2/1000*h)\n\n        try:\n            draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=3)\n            draw.text((x1, y1 - 25), desc, fill=\"red\", font=font)\n            draw.text((x1, y1 - 50), str([int(ori_x1/100), int(ori_y1/100), int(ori_x2/100), int(ori_y2/100)]), fill=\"red\", font=font)\n        except:\n            pass\n\n    return np.array(image)\n\ndef mask_image(ref_image, refer_mask, bg_mode_for_refer='white'):\n    refer_mask = refer_mask.float()\n    if bg_mode_for_refer == 'random':\n        random_color = torch.randn(3)*2-1\n    elif bg_mode_for_refer == 'white':\n        random_color = torch.ones(3)*2-1\n    elif bg_mode_for_refer == 'black':\n        random_color = torch.zeros(3)*2-1\n    else:\n        assert False\n    ref_image = ref_image * refer_mask + (1-refer_mask) * random_color[...,None,None]\n    return ref_image\n\nimport torchvision.utils as tvu\ndef save_img(img, path='a.png', bs=8, pad=0):\n    if isinstance(img, torch.Tensor):\n        tvu.save_image(tvu.make_grid(img, nrow=bs, padding=pad), path)\n    elif isinstance(img, List):\n        if isinstance(img[0], torch.Tensor):\n            save_img(torch.stack(img), path, bs, pad)\n        elif isinstance(img[0], PIL.Image.Image):\n            img = torch.cat([pil2pt(t,norm=False) for t in img])\n            save_img(img, path, bs, pad)\n        else:\n            assert False, img[0].__class__\n\ndef get_params_opt_sch(trainable, args, accelerator):\n    params = []\n    for model in trainable:\n        params.extend([x for x in model.parameters()])\n\n    if accelerator.is_main_process:\n        print('trainable model:')\n        print([t.__class__ for t in trainable], end='\\n\\n')\n        print([get_parameter_number(t) for t in trainable], end='\\n\\n')\n        print(\"all in optimizer:\", get_parameter_number_params(params), end='\\n\\n')\n\n    params_with_lr = []\n    for model in trainable:\n        params_with_lr.append({'params': model.parameters(), 'lr':args.learning_rate})\n        \n    optimizer_cls = torch.optim.AdamW\n    optimizer = optimizer_cls(\n        params_with_lr,\n        # params,\n        # lr=args.learning_rate, d\n        betas=(args.adam_beta1, args.adam_beta2),\n        weight_decay=args.adam_weight_decay,\n        eps=args.adam_epsilon,\n    )\n\n    lr_scheduler = get_scheduler(\n        args.lr_scheduler,\n        optimizer=optimizer,\n        num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,\n        num_training_steps=args.max_train_steps * accelerator.num_processes,\n    )\n    return params, optimizer, lr_scheduler\n\ndef resume_model(args, accelerator):\n    global_step = 0\n    if args.resume_from_checkpoint != \"latest\":\n        if os.path.exists(args.resume_from_checkpoint):\n            path = args.resume_from_checkpoint\n        elif isinstance(args.resume_from_checkpoint, int):\n            path = f\"checkpoint-{args.resume_from_checkpoint}\"\n        else:\n            path = os.path.basename(args.resume_from_checkpoint)\n    else:\n        # Get the most recent checkpoint\n        dirs = os.listdir(args.output_dir)\n        dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n        dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n        path = dirs[-1] if len(dirs) > 0 else None\n\n    if path is None:\n        accelerator.print(\n            f\"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.\"\n        )\n        args.resume_from_checkpoint = None\n    else:\n        accelerator.print(f\"Resuming from checkpoint {path}\")\n        accelerator.load_state(os.path.join(args.output_dir, path), map_location='cpu')\n        global_step = int(os.path.basename(path).split(\"-\")[1])\n\n        # resume_global_step = global_step * args.gradient_accumulation_steps\n        # first_epoch = global_step // num_update_steps_per_epoch\n        # resume_step = resume_global_step % (num_update_steps_per_epoch * args.gradient_accumulation_steps)\n\n    return global_step\n\ndef get_latent_and_target(images, vae, scheduler, accelerator):\n    latents = vae.encode(images).latent_dist.sample()\n    latents = latents * vae.config.scaling_factor\n\n    noise = torch.randn_like(latents)\n    bsz = latents.shape[0]\n    timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (bsz,), device=accelerator.device).long()\n    latents = scheduler.add_noise(latents, noise, timesteps)\n\n    if scheduler.config.prediction_type == \"epsilon\":\n        target = noise\n    elif scheduler.config.prediction_type == \"v_prediction\":\n        target = scheduler.get_velocity(latents, noise, timesteps)\n    else:\n        raise ValueError(f\"Unknown prediction type {scheduler.config.prediction_type}\")\n    return latents, timesteps, target\n\ndef backward_and_step(args, params, loss, accelerator, optimizer, lr_scheduler):\n    # avg_loss = accelerator.gather(loss.repeat(args.train_batch_size)).mean()\n    # train_loss += avg_loss.item() / args.gradient_accumulation_steps\n\n    # Backpropagate\n    accelerator.backward(loss)\n    if accelerator.sync_gradients:\n        accelerator.clip_grad_norm_(params, args.max_grad_norm)\n\n    optimizer.step()\n    lr_scheduler.step()\n    optimizer.zero_grad()\n\ndef save_model(args, logger, accelerator, global_step):\n    if args.checkpoints_total_limit is not None:\n        checkpoints = os.listdir(args.output_dir)\n        checkpoints = [d for d in checkpoints if d.startswith(\"checkpoint\")]\n        checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n\n        if len(checkpoints) >= args.checkpoints_total_limit:\n            num_to_remove = len(checkpoints) - args.checkpoints_total_limit + 1\n            removing_checkpoints = checkpoints[0:num_to_remove]\n            logger.info(f\"{len(checkpoints)} checkpoints already exist, removing {len(removing_checkpoints)} checkpoints\")\n            logger.info(f\"removing checkpoints: {', '.join(removing_checkpoints)}\")\n            for removing_checkpoint in removing_checkpoints:\n                removing_checkpoint = os.path.join(args.output_dir, removing_checkpoint)\n                shutil.rmtree(removing_checkpoint)\n\n    save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n    accelerator.save_state(save_path)\n    logger.info(f\"Saved state to {save_path}\")\n\ndef init_latents(vae,scheduler,device,batch_size=1,height=512,width=512,generator=None):\n    vae_scale_factor = 2 ** (len(vae.config.block_out_channels) - 1)\n    shape = (batch_size, 4, height // vae_scale_factor, width // vae_scale_factor)\n    latents = randn_tensor(shape, generator=generator, device=device) * scheduler.init_noise_sigma\n    return latents\n\ndef vae_decode(vae, latents, generator=None):\n    images = []\n    for i in range(0,100,8):\n        if len(latents[i:i+8]) == 0:\n            break\n        image = vae.decode(latents[i:i+8] / vae.config.scaling_factor, return_dict=False, generator=generator)[0]\n        images.append(image)\n    return images\n\ndef lines(name):\n    with open(name, 'r') as f:\n        contents = f.readlines()\n        lines_without_newline = [line.rstrip('\\n') for line in contents]\n        return lines_without_newline\n    \ndef dilate(path, size=None, iters=1):\n    # gray_image = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n    gray_image = Image.open(path).convert('L')\n    if size is not None:\n        gray_image = gray_image.resize((size,size))\n    gray_image = np.array(gray_image)\n    _, binary_image = cv2.threshold(gray_image, 127, 255, cv2.THRESH_BINARY)\n    dilated_image = cv2.dilate(binary_image, kernel=np.ones((3,3),np.uint8), iterations=iters)\n    # cv2.imwrite('a.png', dilated_image)\n    return dilated_image\n\ndef dilate_ts_test(ts, size=None, iters=1):\n    # gray_image = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n    # gray_image = Image.open(path).convert('L')\n\n    # if size is not None:\n    #     gray_image = gray_image.resize((size,size))\n    gray_image = (ts[0].cpu().detach().numpy()*255).astype(np.uint8)\n    _, binary_image = cv2.threshold(gray_image, 127, 255, cv2.THRESH_BINARY)\n    dilated_image = cv2.dilate(binary_image, kernel=np.ones((7,7),np.uint8), iterations=iters)\n\n    # import pdb;pdb.set_trace()\n\n    dilated_image = (torch.tensor(np.array(dilated_image))/255).float()[None]\n\n    return dilated_image\n\ndef norm1(t):\n    t = t - t.min()\n    return t/(t.max()+1e-6)\n\ndef seq2mat(t):\n    bs, nseq = t.shape\n    h = int(np.sqrt(nseq))\n    t = t.reshape(bs,1,h,h)\n    return Resize((32,32))(t)\n\ndef seq2mat_ca(t):\n    t = t.permute(0,2,1)\n    bs, _77, nseq = t.shape\n    h = int(np.sqrt(nseq))\n    try:\n        t = t.reshape(bs,77,h,h)\n    except:\n        import pdb;pdb.set_trace()\n        pass\n    return Resize((32,32))(t)\n\ndef mkdir(path):\n    os.makedirs(path, exist_ok=True)\n\ndef save_np_img(img, filename):\n    Image.fromarray(img.astype(np.uint8)).save(filename)\n\ndef random_range(pad_range):\n    return random.choice(range(*pad_range))\n\nfrom PIL import Image, ImageDraw\n\ndef draw_rectangle_on_image(img, top_left, bottom_right):\n    # 创建一个可以在给定图像上绘图的对象\n    draw = ImageDraw.Draw(img)\n    \n    # 使用draw.rectangle()函数绘制矩形\n    # 参数是矩形两个对角点的坐标，fill是填充色，outline是边框色\n    draw.rectangle([top_left, bottom_right], outline=\"red\")\n    \n    # 保存修改后的图像\n    # img.save(output_path)\n    # return img\n\ndef get_mask_bounds(mask, pad_range=None):\n    # 获取mask最左上和最右下的坐标。\n\n    y_indices, x_indices = np.where(mask != 0)\n    h,w = mask.shape\n\n    if not len(x_indices) or not len(y_indices):\n        return None\n\n    x1,y1 = np.min(x_indices), np.min(y_indices)\n    x2,y2 = np.max(x_indices), np.max(y_indices)\n\n    if pad_range is not None:\n        left_top     = (max(0, x1-random_range(pad_range)),max(0, y1-random_range(pad_range)))\n        right_bottom = (min(h, x2+random_range(pad_range)),min(w, y2+random_range(pad_range)))\n    else:\n        left_top     = (x1, y1)\n        right_bottom = (x2, y2)\n    \n    return left_top, right_bottom\n\n\ndef smooth_transition_tensor(image_a1, image_a2, mask, transition_width=5):\n    \"\"\"\n    使用Tensor实现平滑过渡\n    :param image_a1: 图像a1，形状为(bs, 3, h, w)，值范围为[0, 1]\n    :param image_a2: 图像a2，形状为(bs, 3, h, w)，值范围为[0, 1]\n    :param mask: 掩码图像，形状为(bs, 1, h, w)，值范围为[0, 1]\n    :param transition_width: 过渡区域的宽度\n    :return: 混合后的图像，形状为(bs, 3, h, w)，值范围为[0, 1]\n    \"\"\"\n    # 使用高斯模糊对mask进行平滑处理，创建平滑过渡效果\n    # 注意：这里的高斯模糊需要自定义或使用现有库，如kornia\n    # 以下代码假设已有一个名为gaussian_blur的函数\n    blurred_mask = gaussian_blur(mask, kernel_size=transition_width)\n\n    blurred_mask = torch.clamp(blurred_mask, 0, 1)\n\n    # blurred_mask = (blurred_mask > 0.).float()\n    \n    # 计算反向掩码\n    reverse_blurred_mask = 1 - blurred_mask\n    \n    # 使用mask进行区域选择和混合\n    blended_image = image_a1 * blurred_mask + image_a2 * reverse_blurred_mask\n    \n    return blended_image\n\ndef gaussian_blur(mask, kernel_size):\n    \"\"\"\n    对mask应用高斯模糊\n    :param mask: 待模糊的mask，形状为(bs, 1, h, w)\n    :param kernel_size: 高斯核的大小\n    :return: 模糊后的mask\n    \"\"\"\n    # 计算padding，确保输出大小与输入相同\n    padding = kernel_size // 2\n    # 创建高斯核\n    gaussian_kernel = kornia.filters.GaussianBlur2d((kernel_size, kernel_size), (padding, padding))\n    # 应用高斯模糊\n    blurred_mask = gaussian_kernel(mask)\n    return blurred_mask\n\ndef crop_image_by_bounds(image, left_top, right_bottom):\n    # cropped_image = image.crop((left_top[0], left_top[1], right_bottom[0], right_bottom[1]))\n    cropped_image = image[left_top[1]:right_bottom[1], left_top[0]:right_bottom[0]]\n    return cropped_image\n\ndef crop_image_by_bounds_ts(image, left_top, right_bottom):\n    # cropped_image = image.crop((left_top[0], left_top[1], right_bottom[0], right_bottom[1]))\n    if len(image.shape) == 4:\n        cropped_image = image[:,:,left_top[1]:right_bottom[1], left_top[0]:right_bottom[0]]\n    else:\n        cropped_image = image[:,left_top[1]:right_bottom[1], left_top[0]:right_bottom[0]]\n    return cropped_image\n\ndef get_pg():\n    from rich.progress import Progress, BarColumn, TextColumn, TimeRemainingColumn, TimeElapsedColumn\n    progress_columns = [\n        BarColumn(bar_width=40),\n        TextColumn(\"{task.completed}/{task.total}, {task.percentage:.1f}%\"),\n        TimeElapsedColumn(),\n        TimeRemainingColumn(),\n    ]\n    return Progress(*progress_columns)\n\n    # with pg as progress:\n    # task = progress.add_task(\"[cyan]doing...\", total=100)\n    \n    # for i in range(100):\n    #     time.sleep(0.1)\n    #     progress.update(task, advance=1)\n\ndef split_list(lst, n_parts=8):\n    # 计算每份的大致长度，向上取整以确保所有元素都能被分配\n    n = len(lst)\n    part_size = -(-n // n_parts)  # 使用负数向上取整的技巧\n    # 使用列表推导式和range步长来拆分列表\n    return [lst[i:i + part_size] for i in range(0, n, part_size)]\n\ndef pil2pt(t, norm=True):\n    if norm:\n        return torch.tensor(np.array(t.convert('RGB'))).permute(2,0,1)[None]/127.5-1\n    else:\n        return torch.tensor(np.array(t.convert('RGB'))).permute(2,0,1)[None]/255\n\ndef set_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\ndef dummy_args():\n    a={}\n    return OmegaConf.create(a)\n\ndef post_processing(t, resize=None):\n    t = t.clamp(-1,1)\n    t = denorm_pt(t)\n    if resize:\n        t = resize_pt(t, 200)\n    # bs,3,h,w\n    t = t.permute(0,2,3,1).flatten(0,1)\n    # bs*h,3,w\n    t = pt2np(t)\n    return t\n    \ndef pt2np(x, to255=True):\n    if to255:\n        return (x.cpu().detach().numpy() * 255).astype(np.uint8)\n    else:\n        return x.cpu().detach().numpy()\n\ndef pt2pil(x, denorm=False, resize=None):\n    if denorm:\n        x = (x+1)/2\n    if resize is not None:\n        x = Resize((resize,resize))(x)\n    assert len(x.shape) == 4\n    imgs = (x.permute(0,2,3,1)*255).cpu().detach().numpy().astype(np.uint8)#bs,h,w,3\n    return [Image.fromarray(t) for t in imgs]\n\ndef denorm_pt(pt):\n    return (pt.clamp(-1,1)+1)/2\n\ndef donorm_pt(pt):\n    return pt*2-1\n\ndef norm_pt(pt, mean=[0.48145466, 0.4578275, 0.40821073], var=[0.26862954, 0.26130258, 0.27577711]):\n    device = pt.device\n    # ret = ((pt-torch.tensor(mean).to(device))/torch.tensor(var).to(device))\n    ret = ((pt.permute(0,2,3,1)-torch.tensor(mean).to(device))/torch.tensor(var).to(device)).permute(0,3,1,2)\n    return ret\n\ndef resize_pt(pt,s):\n    if isinstance(s, int):\n        return Resize((s,s))(pt)\n    else:\n        assert len(s) == 2\n        return Resize(s)(pt)\n\n\ndef pdb():\n    import pdb;pdb.set_trace()\n    pass\n\ndef loader(dataset, shuffle=True, batch_size=1, dataloader_num_workers=8, pin_memory=False):\n    return torch.utils.data.DataLoader(\n        dataset,\n        shuffle=shuffle,\n        batch_size=batch_size,\n        num_workers=dataloader_num_workers,\n        pin_memory=pin_memory,\n    )\n\n\n\ndef pillist2ts(imgs):\n    imgs = [pil2pt(t, norm=False) for t in imgs]\n    img = torch.cat(imgs)\n    return img\n\ndef ts2np(t):\n    return t.cpu().detach().numpy()\n\ndef ts2array(t):\n    # 3,h,w\n    # import pdb;pdb.set_trace()\n    t = t.clamp(0,1)\n    return (t.permute(1,2,0)*255).cpu().detach().numpy().astype(np.uint8)\n\ndef load_to_np(path, mode='RGB', size=None,):\n    if size is None:\n        img = np.array(Image.open(path).convert(mode))\n    else:\n        img = np.array(Image.open(path).resize((size,size)).convert(mode))\n    return img\n\ndef load2pil(path, mode='RGB', size=None,):\n    if size is None:\n        img = Image.open(path).convert(mode)\n    else:\n        img = Image.open(path).resize((size,size)).convert(mode)\n    return img\n\ndef comp_ts(imgs, texts=None, r=200):\n    # bs, dim, h, w\n    bs = len(imgs[0])\n\n    new_imgs = []\n    for t in imgs:\n        if t.ndim == 3:\n            t = t[:,None]\n        if t.shape[1] == 1:\n            t = t.repeat(1,3,1,1)\n        if t.min() < -0.8:\n            t = denorm_pt(t)\n        # 0~1\n        t = Resize((r,r))(t)\n        t = t.cpu()\n        new_imgs.append(t)\n\n    imgs_cat = torch.cat(new_imgs, dim=0)\n    img_grid = tvu.make_grid(imgs_cat, nrow=bs, padding=0)\n    return img_grid\n\ndef dilate_ts(tensor, kernel_size=3, iterations=1, padding=1):\n    \"\"\"\n    对输入的tensor执行dilate操作。\n    \n    参数:\n    - tensor: 输入的tensor，形状为[1, 1, H, W]\n    - kernel_size: 卷积核的大小\n    - iterations: 膨胀操作的迭代次数\n    - padding: 卷积操作的填充大小\n    \"\"\"\n    # 创建一个卷积核\n    kernel = torch.ones((1, 1, kernel_size, kernel_size)).to(tensor)\n    \n    # 执行指定次数的dilate操作\n    for _ in range(iterations):\n        tensor = F.conv2d(tensor, kernel, padding=padding)\n        tensor = (tensor > 0).to(kernel) # float的话是32\n    \n    return tensor\n\n# def load2ts(path, mode='RGB', resize=(256,256), norm=True):\n#     if resize is not None:\n#         if isinstance(resize, tuple):\n#             img = np.array(Image.open(path).resize(resize).convert(mode))\n#         else:\n#             img = np.array(Image.open(path).resize((resize,resize)).convert(mode))\n#     else:\n#         img = np.array(Image.open(path).convert(mode))\n#     if norm:\n#         img = torch.tensor(img).permute(2,0,1)/127.5-1\n#     else:\n#         img = torch.tensor(img).permute(2,0,1)/255\n#     return img\n\ndef load2np(path, mode='RGB', in_01=False):\n    if isinstance(path, str):\n        img = Image.open(path).convert(mode)\n    else:\n        img = path.convert(mode)\n    img = np.array(img)\n    if in_01:\n        img = img / 255\n    return img\n\nfrom typing import Dict\nimport numpy as np\nfrom PIL import Image, ImageDraw, ImageFont\nimport random\ndef scale_boxes(boxes, width, height):\n    scaled_boxes = []\n    for box in boxes:\n        x_min, y_min, x_max, y_max = box\n        scaled_box = [x_min * width, y_min * height, x_max * width, y_max * height]\n        scaled_boxes.append(scaled_box)\n    return scaled_boxes\n\ndef draw_mask(mask, draw, random_color=True):\n    if random_color:\n        color = (\n            random.randint(0, 255),\n            random.randint(0, 255),\n            random.randint(0, 255),\n            153,\n        )\n    else:\n        color = (30, 144, 255, 153)\n\n    nonzero_coords = np.transpose(np.nonzero(mask))\n    \n    for coord in nonzero_coords:\n        draw.point(coord[::-1], fill=color)\n        \ndef bbox_visualization(image_pil: Image,\n              result: Dict,\n              draw_width: float = 3.0,\n              return_mask=True,\n              font_size=20,\n) -> Image:\n    \"\"\"Plot bounding boxes and labels on an image.\n\n    Args:\n        image_pil (PIL.Image): The input image as a PIL Image object.\n        result (Dict[str, Union[torch.Tensor, List[torch.Tensor]]]): The target dictionary containing\n            the bounding boxes and labels. The keys are:\n                - boxes (List[int]): A list of bounding boxes in shape (N, 4), [x1, y1, x2, y2] format.\n                - scores (List[float]): A list of scores for each bounding box. shape (N)\n                - labels (List[str]): A list of labels for each object\n                - masks (List[PIL.Image]): A list of masks in the format of PIL.Image\n        draw_score (bool): Draw score on the image. Defaults to False.\n\n    Returns:\n        PIL.Image: The input image with plotted bounding boxes, labels, and masks.\n    \"\"\"\n    # Get the bounding boxes and labels from the target dictionary\n    boxes = result[\"boxes\"]\n    categorys = result[\"labels\"]\n    masks = result.get(\"masks\", [])\n\n    \n    color_list= [(177, 214, 144),(255, 162, 76),\n                (13, 146, 244),(249, 84, 84),(54, 186, 152),\n                (74, 36, 157),(0, 159, 189),\n                (80, 118, 135),(188, 90, 148),(119, 205, 255)]\n\n\n    np.random.seed(42)\n\n    # Find all unique categories and build a cate2color dictionary\n    cate2color = {}\n    unique_categorys = sorted(set(categorys))\n    for idx,cate in enumerate(unique_categorys):\n        cate2color[cate] = color_list[idx%len(color_list)]\n    \n    # Load a font with the specified size\n    # font = ImageFont.truetype(\"utils/arial.ttf\", font_size)\n    font = ImageFont.truetype(\"/home/jovyan/boomcheng-data/tools/font/msyh.ttf\", font_size)\n    \n    # Create a PIL ImageDraw object to draw on the input image\n    if isinstance(image_pil, np.ndarray):\n        image_pil = Image.fromarray(image_pil)\n    draw = ImageDraw.Draw(image_pil)\n    \n    # Create a new binary mask image with the same size as the input image\n    mask = Image.new(\"L\", image_pil.size, 0)\n    # Create a PIL ImageDraw object to draw on the mask image\n    mask_draw = ImageDraw.Draw(mask)\n\n    # Draw boxes, labels, and masks for each box and label in the target dictionary\n    for box, category in zip(boxes, categorys):\n        try:\n            # Extract the box coordinates\n            x0, y0, x1, y1 = box\n\n            x0, y0, x1, y1 = int(x0), int(y0), int(x1), int(y1)\n            color = cate2color[category]\n\n            # Draw the box outline on the input image\n            draw.rectangle([x0, y0, x1, y1], outline=color, width=int(draw_width))\n\n            # Draw the label and score on the input image\n            text = f\"{category}\"\n        \n            if hasattr(font, \"getbbox\"):\n                bbox = draw.textbbox((x0, y0), text, font)\n            else:\n                w, h = draw.textsize(text, font)\n                bbox = (x0, y0, w + x0, y0 + h)\n            draw.rectangle(bbox, fill=color)\n            draw.text((x0, y0), text, fill=\"white\",font=font)\n        except:\n            print('bug in draw')\n    # Draw the mask on the input image if masks are provided\n    if len(masks) > 0 and return_mask:\n        size = image_pil.size\n        mask_image = Image.new(\"RGBA\", size, color=(0, 0, 0, 0))\n        mask_draw = ImageDraw.Draw(mask_image)\n        for mask in masks:\n            mask = np.array(mask)[:, :, -1]\n            draw_mask(mask, mask_draw)\n\n        image_pil = Image.alpha_composite(image_pil.convert(\"RGBA\"), mask_image).convert(\"RGB\")\n    return image_pil\n\ndef load2ts(path, mode='RGB',resize=None, norm=True, aug_trans=None, mask=None):\n    if isinstance(path, str):\n        img = Image.open(path).convert(mode)\n    else:\n        img = path.convert(mode)\n    if resize:\n        if not isinstance(resize, tuple):\n            resize = (resize, resize)\n        img = img.resize(resize)\n    img = np.array(img)\n\n    if mask is not None:\n        mask = Image.open(mask).convert('RGB')\n        if resize:\n            if not isinstance(resize, tuple):\n                resize = (resize, resize)\n            mask = mask.resize(resize)\n        mask = np.array(mask)\n\n    if aug_trans is not None:\n        if mask is not None:\n            out = aug_trans(image=img,mask=mask)\n            img = out['image']\n            mask = out['mask']\n        else:\n            img = aug_trans(image=img)['image']\n        \n    img = torch.tensor(img).permute(2,0,1)\n    if norm:\n        img = img/127.5-1\n    else:\n        img = img/225\n\n    if mask is not None:\n        mask = torch.tensor(mask).permute(2,0,1)[:1]\n        mask = mask/225\n        return img,mask\n    else:\n        return img\n\ndef load2ts_ori(path, mode='RGB'):\n    img = np.array(Image.open(path).convert(mode))\n    img = torch.tensor(img).permute(2,0,1)/255\n    return img\n\ndef tensor2np_final(images, args=None):\n    if args is not None and 'tryon' in args.test_data:\n        images = (Resize((256,192))(images.clamp(-1,1)) + 1)/2\n    else:\n        images = (Resize((256,256))(images.clamp(-1,1)) + 1)/2\n    images = (images.permute(0,2,3,1).cpu().detach().numpy() * 255).astype(np.uint8) # bs,h,w,3\n    # images = (images.permute(0,2,3,1).flatten(0,1).cpu().detach().numpy() * 255).astype(np.uint8)\n    return images\n\ndef tensor2np_final_01(images, args=None):\n    if args is not None and 'tryon' in args.test_data:\n        images = Resize((256,192))(images)\n    else:\n        images = Resize((256,256))(images)\n    images = (images.permute(0,2,3,1).cpu().detach().numpy() * 255).astype(np.uint8) # bs,h,w,3\n    # images = (images.permute(0,2,3,1).flatten(0,1).cpu().detach().numpy() * 255).astype(np.uint8)\n    return images\n\ndef text_under_image(image: np.ndarray, text: str, text_color: Tuple[int, int, int] = (0, 0, 0)):\n    h, w, c = image.shape\n    offset = int(h * .2)\n    img = np.ones((h + offset, w, c), dtype=np.uint8) * 255\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    # font = ImageFont.truetype(\"/usr/share/fonts/truetype/noto/NotoMono-Regular.ttf\", font_size)\n    img[:h] = image\n    size = 0.75\n    thick = 1 # int\n    textsize = cv2.getTextSize(text, font, size, thick)[0]#int\n    text_x, text_y = (w - textsize[0]) // 2, h + offset - textsize[1] // 2\n    cv2.putText(img, text, (text_x, text_y ), font, size, text_color, thick)#0.5是size\n    return img\n\ndef convert_to_np(image, resolution, mode='RGB', p=1):\n    image = image.convert(mode).resize((resolution*p, resolution))\n    return np.array(image).transpose(2, 0, 1)\n\ndef convert_to_np_ct(image, resolution, mode='RGB', p=1, size=0):\n    image = image.convert(mode)\n    if size != 0:\n        image = CenterCrop(512-size*2)(image.resize((512,512)))\n    image = image.resize((resolution*p, resolution))\n    image = np.array(image)\n    return image.transpose(2, 0, 1)\n\ndef convert_to_np_mask(image, resolution, mode='L'):\n    image = image.convert('L').resize((resolution, resolution))\n    return np.array(image)[None]\n\ndef download_image(url):\n    image = PIL.Image.open(requests.get(url, stream=True).raw)\n    image = PIL.ImageOps.exif_transpose(image)\n    image = image.convert(\"RGB\")\n    return image\n\ndef tokenize_captions(captions, tokenizer):\n    inputs = tokenizer(\n        captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n    )\n    return inputs.input_ids\n\n# def get_parameter_number(model):\n#     total_num = sum(p.numel() for p in model.parameters())\n#     trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n#     return {'Total': f\"{total_num/1024/1024} MB\", 'Trainable': f\"{trainable_num/1024/1024} MB\"}\n\ndef get_parameter_number_params(params):\n    total_num = sum(p.numel() for p in params)/1024/1024\n    trainable_num = sum(p.numel() for p in params if p.requires_grad)/1024/1024\n    return f\"Parameter: {round(total_num,2)} MB, Trainable: {round(trainable_num,2)} MB\"\n\ndef tensor2np(images, norm=True, flatten=True, resize=True):\n    if norm:\n        images = (images.clamp(-1,1)+1)/2\n    if resize:\n        images = Resize((200,200))(images)\n    if flatten:\n        images = (images.permute(0,2,3,1).flatten(0,1).cpu().detach().numpy() * 255).astype(np.uint8)\n    else:\n        images = (images.permute(0,2,3,1).cpu().detach().numpy() * 255).astype(np.uint8)\n    return images\n\ndef torch_dfs(model: torch.nn.Module):\n    result = [model]\n    for child in model.children():\n        result += torch_dfs(child)\n    return result\n\nclass SpecifyGradient(torch.autograd.Function):\n    @staticmethod\n    @custom_fwd\n    def forward(ctx, input_tensor, gt_grad):\n        ctx.save_for_backward(gt_grad)\n\n        # dummy loss value\n        return torch.zeros([1], device=input_tensor.device, dtype=input_tensor.dtype)\n\n    @staticmethod\n    @custom_bwd\n    def backward(ctx, grad):\n        gt_grad, = ctx.saved_tensors\n        batch_size = len(gt_grad)\n        return gt_grad / batch_size, None\n    \n\ndef load_jsonl(filename):\n    data = []\n    with open(filename, 'r') as f:\n        for line in f:\n            data.append(json.loads(line))\n    return data\n\ndef load_json(filename):\n    with open(filename, \"r\") as file:\n        data = json.load(file)\n    return data\n\ndef default(obj):\n    if isinstance(obj, np.integer):\n        return int(obj)\n    raise TypeError\n\ndef save_json(filename, data):\n    with open(filename, \"w\") as file:\n        json.dump(data, file, default=default)\n\ndef load_txt(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        lines = [line.strip() for line in file]\n    return lines\n\ndef save_jsonl(filename, data_list):\n    with open(filename, 'w') as f:\n        for item in data_list:\n            f.write(json.dumps(item) + '\\n')\n\ndef load_csv(name):\n    with open(name, mode='r', encoding='utf-8') as file:\n        # 创建 CSV 读取器\n        reader = csv.reader(file)\n        \n        # 将每一行转换为列表，并存储到行的列表中\n        rows_list = list(reader)\n\n    # 打印结果\n    # print(rows_list)\n    return rows_list"}
{"type": "source_file", "path": "project/plangen/plangen_base.py", "content": "import sys;sys.path.insert(0, './three_party/Janus')\nimport torch\nfrom torch import nn\nimport argparse\nimport logging\nimport math\nimport os\nimport shutil\nfrom copy import deepcopy\nimport types\nimport gc\nfrom time import time\nimport einops\nfrom rich import print\nimport os.path as osp\nimport datasets\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nimport transformers\nfrom functools import partial\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nfrom accelerate.utils import ProjectConfiguration, set_seed\nfrom tqdm.auto import tqdm\nfrom transformers import CLIPTextModel, CLIPTokenizer\nimport diffusers\nfrom diffusers import AutoencoderKL, DDPMScheduler, DDIMScheduler, DPMSolverMultistepScheduler, UNet2DConditionModel, UniPCMultistepScheduler, EulerAncestralDiscreteScheduler, DiffusionPipeline\nfrom diffusers.optimization import get_scheduler\nfrom diffusers.training_utils import EMAModel\nfrom diffusers.utils.torch_utils import randn_tensor\nfrom diffusers.models.attention import BasicTransformerBlock, _chunked_feed_forward\nfrom diffusers.models.attention_processor import Attention\nfrom PIL import Image\nfrom mmengine.config import Config, DictAction\nfrom diffusers import ControlNetModel\nimport safetensors\nfrom janus.models import MultiModalityCausalLM, VLChatProcessor\nfrom janus.utils.io import load_pil_images\nfrom src.utils.funcs import *\nimport pickle\nfrom contextlib import contextmanager\nimport wandb\nfrom peft import LoraConfig, set_peft_model_state_dict, PeftModel, get_peft_model, TaskType\nfrom time import time\nfrom torch.utils.data.dataloader import default_collate\nfrom torchvision.transforms import ToPILImage, ToTensor\nto_pil = ToPILImage()\nto_ts = ToTensor()\nimport os\nimport PIL.Image\nimport torch\nimport numpy as np\nfrom transformers import AutoModelForCausalLM\nfrom src.utils.causal_loss import ForCausalLMLoss\nfrom tokenizers import AddedToken\nfrom .dataset.set_dataset import set_dataset\nimport traceback\n\nfrom transformers import AutoModel, AutoTokenizer\nimport fire\n\nimport json\nimport os\n\nfrom tqdm import tqdm, trange\nfrom einops import rearrange\nfrom torchvision.utils import make_grid\nfrom torchvision.transforms import ToTensor\nfrom pytorch_lightning import seed_everything\nfrom diffusers import DiffusionPipeline, StableDiffusionPipeline\nfrom project.base.base_system import Base_System\nfrom lightning.pytorch.utilities import CombinedLoader\nfrom .dataset.set_dataset import get_dataset\nfrom transformers.models.llama.modeling_llama import *\n\nclass System(Base_System):\n    def __init__(self, \n        args=None,\n        accelerator=None,\n    ) -> None:\n        super().__init__()\n        if args.test and args.test_data.data_name=='1k':\n            args.max_test_len=-1\n\n        self.args = args\n        self.accelerator = accelerator\n\n        model_path = self.args.janus_path\n        vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n        tokenizer = vl_chat_processor.tokenizer\n\n        vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n            model_path, trust_remote_code=True\n        )\n\n        self.vl_chat_processor = vl_chat_processor\n        self.ori_image_proc = self.vl_chat_processor.image_processor.__class__.__call__\n        self.vl_chat_processor.image_processor.__class__.__call__ = self.hack_image_proc\n\n        self.tokenizer = tokenizer\n        self.vl_gpt = vl_gpt\n        if self.vl_gpt.vision_model.vision_tower.ignore_head:\n            vl_gpt.vision_model.vision_tower.attn_pool = None\n        # ['vision_model', 'aligner', 'gen_vision_model', 'gen_aligner', 'gen_head', 'gen_embed', 'language_model']\n        # ['model', 'lm_head']\n\n        if self.args.use_special_tokens:\n            res = tokenizer.add_tokens([\n                AddedToken(\"<grounding>\", special=True),\n                AddedToken(\"</grounding>\", special=True),\n                AddedToken(\"<box>\", special=True),\n                AddedToken(\"</box>\", special=True),\n                AddedToken(\"<ref>\", special=True),\n                AddedToken(\"</ref>\", special=True),\n            ])\n            print('\\nadd special tokens', res)\n\n        if self.args.use_numhw_tokens:\n            hw_list = []\n            for i in range(100):\n                hw_list.append(AddedToken(f\"<h{i}>\", special=True))\n                hw_list.append(AddedToken(f\"<w{i}>\", special=True))\n            res = tokenizer.add_tokens(hw_list)\n            print('\\nadd hw_num tokens', res)\n\n        img_size = self.args.janus_hw\n        self.image_token_num_per_image = (self.args.janus_hw//16)**2\n        self.vl_chat_processor.num_image_tokens = self.image_token_num_per_image\n        self.vl_chat_processor.image_processor.image_size = self.args.janus_hw\n\n        self.prepare_trainable()\n\n    def hack_image_proc(self, image, return_tensors='pt'):\n        if isinstance(image, torch.Tensor):\n            class ImagesOutputs:\n                def __init__(self, pixel_values):\n                    self.pixel_values = pixel_values\n            return ImagesOutputs(image)\n        else:\n            return self.ori_image_proc(\n                self.vl_chat_processor.image_processor,#self\n                image, #images\n                return_tensors=return_tensors\n            )\n\n    def prepare_trainable(self,):\n        self.trainable = []\n        self.non_trainable = []\n\n        self.freeze_params(self.parameters())\n\n        if self.args.gradient_checkpointing_enable:\n            self.vl_gpt.language_model.gradient_checkpointing_enable()\n\n        if self.args.tuning_mode == 'all':\n            self.trainable.append(self)\n        elif self.args.tuning_mode == 'lm':\n            self.trainable.append(self.vl_gpt.language_model)\n        elif self.args.tuning_mode == 'lora':\n            transformer_lora_config = LoraConfig(\n                r=self.args.lora_rank,\n                lora_alpha=self.args.lora_alpha,\n                init_lora_weights=\"gaussian\",\n                target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n            )#15MB\n            self.vl_gpt.language_model.enable_input_require_grads()\n            self.vl_gpt = get_peft_model(self.vl_gpt, transformer_lora_config)\n\n            if self.args.tune_token_when_lora and (self.args.use_special_tokens or self.args.use_numhw_tokens):\n                self.unfreeze_params(self.vl_gpt.language_model.model.embed_tokens.parameters())\n\n        elif self.args.tuning_mode == 'lora_ranni':\n            peft_config = LoraConfig(\n                r=64,\n                lora_alpha=16,\n                target_modules=[\"q_proj\", \"v_proj\"],\n                task_type=TaskType.CAUSAL_LM,\n                inference_mode=False,\n                lora_dropout=0.05,\n                bias=\"none\",\n            )#30MB\n            self.vl_gpt = get_peft_model(self.vl_gpt, peft_config)\n        elif self.args.tuning_mode == 'stage1':\n            self.trainable.append(self.vl_gpt.aligner)\n            self.trainable.append(self.vl_gpt.gen_aligner)\n            self.trainable.append(self.vl_gpt.gen_head)\n        elif self.args.tuning_mode == 'stage2':\n            self.trainable.append(self)\n            self.non_trainable.append(self.vl_gpt.vision_model)\n            self.non_trainable.append(self.vl_gpt.gen_vision_model)\n        elif self.args.tuning_mode == 'stage2_lora':\n            self.trainable.append(self)\n            self.non_trainable.append(self.vl_gpt.vision_model)\n            self.non_trainable.append(self.vl_gpt.gen_vision_model)\n        elif self.args.tuning_mode == 'stage3':\n            self.trainable.append(self)\n            self.non_trainable.append(self.vl_gpt.gen_vision_model)\n        else:\n            assert False\n\n        for module in self.trainable:\n            self.unfreeze_params(module.parameters())\n\n        for module in self.non_trainable:\n            self.freeze_params(module.parameters())\n\n    def wrap_t2i_prompt(self, \n        caption=\"a yellow car in front of the tree\"\n    ):\n        conversation = [\n            {\n                \"role\": \"<|User|>\",\n                \"content\": caption,\n            },\n            {\"role\": \"<|Assistant|>\", \"content\": \"\"},\n        ]\n\n        sft_format = self.vl_chat_processor.apply_sft_template_for_multi_turn_prompts(\n            conversations=conversation,\n            sft_format=self.vl_chat_processor.sft_format,\n            system_prompt=\"\",\n        )\n        prompt = sft_format + self.vl_chat_processor.image_start_tag\n\n        inputs_ids = self.vl_chat_processor.tokenizer.encode(prompt)\n        inputs_ids = torch.LongTensor(inputs_ids)\n        return prompt, inputs_ids\n\n    def wrap_uni_prompt(self, \n        caption=\"a yellow car in front of the tree\",\n        grounding=None,\n        in_stage1=False,\n    ):\n        conversation = [\n            {\n                \"role\": \"<|User|>\",\n                \"content\": caption,\n            },\n            {\"role\": \"<|Assistant|>\", \"content\": f\"{grounding}\"},#可能dropout\n        ]\n\n        sft_format = self.vl_chat_processor.apply_sft_template_for_multi_turn_prompts(\n            conversations=conversation,\n            sft_format=self.vl_chat_processor.sft_format,\n            system_prompt=\"\",\n        )\n\n        if in_stage1:\n            prompt = sft_format\n        else:\n            prompt = sft_format + self.vl_chat_processor.image_start_tag\n\n        inputs_ids = self.vl_chat_processor.tokenizer.encode(prompt)\n        inputs_ids = torch.LongTensor(inputs_ids)\n\n        if in_stage1:\n            inputs_ids = inputs_ids[...,:-1]\n        return prompt, inputs_ids\n\n    def wrap_mmu_prompt(self, \n        question=\"a yellow car in front of the tree\",\n        image=None,\n        answer=\"\",\n    ):\n        conversation = [\n            {\n                \"role\": \"<|User|>\",\n                \"content\": f\"<image_placeholder>\\n{question}\",\n                \"images\": [image],\n            },\n            {\"role\": \"<|Assistant|>\", \"content\": f\"{answer}\"},\n        ]\n\n        if isinstance(image, torch.Tensor):\n            pil_images = image\n        else:\n            pil_images = load_pil_images(conversation)\n\n        prepare_inputs = self.vl_chat_processor(\n            conversations=conversation, images=pil_images, force_batchify=True\n        ).to(self.device)\n\n        prepare_inputs['pixel_values'] = prepare_inputs['pixel_values'].to(torch.bfloat16)\n\n        # # run image encoder to get the image embeddings\n        inputs_embeds = self.vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n\n        return prepare_inputs, inputs_embeds\n\n    def decode_text(self, inputs_ids):\n        return self.tokenizer.decode(inputs_ids, skip_special_tokens=False)\n\n    def decode_plan_text_batch(self, inputs_ids):\n        texts = [\"<grounding>\"+self.decode_text(t) for t in inputs_ids]\n        new_texts = []\n        for text in texts:\n            end_pos = text.find(\"</grounding>\")\n            if end_pos != -1:\n                result = text[:end_pos + len(\"</grounding>\")]\n            else:\n                result = \"<grounding>\"+\"</grounding>\"\n            new_texts.append(result)\n        return new_texts\n\n    def get_pr_grounding_part(self, text):\n        pos = text.find(\"<grounding>\")\n        if pos != -1:\n            text = text[pos:]\n        return text\n    \n    def decode_mmu_text_batch(self, inputs_ids):\n        new_ids = []\n        for ids in inputs_ids:\n            try:\n                pos = torch.where(ids==self.tokenizer.eos_token_id)[0][0].item()\n                ids = ids[:pos]\n            except:\n                pass\n            new_ids.append(ids)\n        inputs_ids = [t for t in new_ids]\n        texts = [self.decode_text(t) for t in inputs_ids]\n        return texts\n\n    @torch.inference_mode()\n    def uni_generate(\n        self,\n        batch = None,\n        gen_path = None,\n        batch_idx = None,\n        accelerator = None, ###\n        prompt: str = None,\n        temperature: float = 1,\n        parallel_size: int = 4,#16\n        cfg_weight: float = 5,\n        patch_size: int = 16,\n        pred_layout = True,\n        pred_image = True,\n        save_local = True,\n        use_uni_prompt_in_t2i = True,\n        is_mmu = False,\n        **kwargs,\n    ):\n        parallel_size = self.args.parallel_size\n        img_size = self.args.janus_hw\n        image_token_num_per_image = (self.args.janus_hw//16)**2\n\n        print('\\n uni...')\n\n        base_caption = batch['base_caption']\n        gt_image = batch['image']\n        gt_grounding = batch['gt_grounding']\n\n        bs = len(base_caption)\n\n        self.vl_gpt.eval()\n\n        with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n            print(base_caption)\n\n            if pred_layout:\n                if is_mmu:\n                    prepare_inputs = batch['prepare_inputs_infer']\n                    inputs_embeds = self.vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n                    attention_mask = prepare_inputs['attention_mask']\n                else:\n                    inputs_ids = batch['uni_stage1_inputs_ids']\n                    attention_mask = batch['uni_stage1_attention_mask']\n                    inputs_embeds = self.vl_gpt.language_model.get_input_embeddings()(inputs_ids.to(self.device))\n                outputs = self.x2t(inputs_embeds, attention_mask)\n\n                if is_mmu:\n                    pr_grounding = self.decode_mmu_text_batch(outputs)\n                else:\n                    pr_grounding = self.decode_plan_text_batch(outputs)\n\n                if pred_image:\n                    bs = len(pr_grounding)\n                    all_inputs_ids = []\n                    for base_caption_i, grounding_prompt in zip(base_caption, pr_grounding):\n                        _, inputs_ids = self.wrap_uni_prompt(base_caption_i, grounding_prompt)\n                        all_inputs_ids.append(inputs_ids)\n                    uni_inputs_ids, uni_attention_mask = self.pad_input_ids(all_inputs_ids)\n                    uni_attention_mask = torch.cat([uni_attention_mask, torch.ones((bs, self.image_token_num_per_image))], dim=-1)\n                    batch.update(dict(\n                        uni_inputs_ids=uni_inputs_ids.to(self.device),\n                        uni_attention_mask=uni_attention_mask.to(self.device)\n                    ))\n            else:\n                pr_grounding = gt_grounding\n\n            if pred_image:\n                batch_new = self.t2i_infer_collate_batch(batch, use_uni=use_uni_prompt_in_t2i)\n                cfg_emb=None\n                cfg_inputs_ids = batch_new['cfg_inputs_ids']\n                cfg_attention_mask = batch_new['cfg_attention_mask']\n\n                func = self.t2i\n                pr_image, edit_mask = func(\n                    None, parallel_size, image_token_num_per_image, cfg_weight, temperature, img_size, patch_size, gt_image, batch, \n                    mask=cfg_attention_mask, \n                    tokens=cfg_inputs_ids,\n                    emb=cfg_emb,\n                )\n                pr_image = pr_image.float()\n            else:\n                pr_image = gt_image\n                edit_mask = None\n\n        self.vl_gpt.train()\n        self.clean(accelerator)\n\n        if save_local:\n            data = dict(\n                base_caption=base_caption, gt_grounding=gt_grounding, pr_grounding=pr_grounding if pred_layout else ''\n            )\n            json_path = osp.join(gen_path, str(batch_idx)+'_layout.json')\n            save_json(json_path, data)\n\n            # if edit_mask is not None:\n            #     gt_image[:,0][edit_mask[:,0]==1] = 150\n\n            vis = torch.cat([gt_image, pr_image], dim=0)\n            x_grounding = [t for t in gt_grounding]\n            for i in range(parallel_size):\n                x_grounding += pr_grounding\n            assert len(vis) == len(x_grounding)\n            vis = torch.cat([vis, donorm_pt(torch.ones_like(gt_image))], dim=0)\n            x_grounding += gt_grounding\n            if pred_layout:\n                vis = torch.cat([vis, donorm_pt(torch.ones_like(pr_image))], dim=0)\n                x_grounding += pr_grounding\n            if edit_mask is not None:\n                vis = torch.cat([vis, edit_mask], dim=0)\n                x_grounding += gt_grounding\n            if 'edited_image' in batch:\n                vis = torch.cat([vis, batch['edited_image']], dim=0)\n                x_grounding += gt_grounding\n                vis = torch.cat([vis, gt_image], dim=0)\n                x_grounding += gt_grounding\n            vis = self.vis_image(vis, x_grounding)\n            vis = denorm_pt(vis)\n            img_path = osp.join(gen_path, str(batch_idx)+'.png')\n            save_img(vis, img_path, bs=bs)\n\n            img_each_path = osp.join(gen_path, str(batch_idx))\n            mkdir(img_each_path)\n            for i in range(len(vis)):\n                col = i % bs\n                row = i // bs\n                to_pil(vis[i]).save(f\"{img_each_path}/{row}_{col}.png\")\n\n        return dict(\n            pr_grounding=pr_grounding, \n            pr_image=pr_image,\n        )\n\n    def trans_gr_to_creati(self, prompt):\n        pattern = r\"<ref>(.*?)</ref><box>\\[(.*?)\\]</box>\"\n        matches = re.findall(pattern, prompt)\n        prompts = []\n        boxes = []\n        for desc, box in matches:\n            ori_x1, ori_y1, ori_x2, ori_y2 = map(int, box.split(\",\"))\n            x1 = ori_x1/1000\n            x2 = ori_x2/1000\n            y1 = ori_y1/1000\n            y2 = ori_y2/1000\n            prompts.append(desc)\n            boxes.append([x1,y1,x2,y2])\n        return boxes, prompts\n\n    def vis_image(self, vis, pr_grounding):\n        vis = denorm_pt(vis)\n        assert isinstance(pr_grounding, list)\n        try:\n            assert len(vis) == len(pr_grounding)\n        except:\n            import pdb;pdb.set_trace()\n\n        creati_style=True\n        if creati_style:\n            h = 384\n            out_vis = []\n            for i in range(len(vis)):\n                image = to_pil(vis[i])\n                boxes, caps = self.trans_gr_to_creati(pr_grounding[i])\n                show_input = {\"boxes\":scale_boxes(boxes,h,h), \"labels\":caps}\n                bbox_visualization_img = bbox_visualization(image,show_input)\n                out_vis.append(to_ts(bbox_visualization_img))\n            out_vis = torch.stack(out_vis, 0)\n            vis = donorm_pt(out_vis)\n        else:\n            for i in range(len(vis)):\n                img = self.draw_boxes_on_image(\n                    to_pil(vis[i]),\n                    pr_grounding[i],\n                )\n                vis[i] = donorm_pt(to_ts(img))\n        return vis\n\n    def clean(self, accelerator):\n        torch.cuda.empty_cache()\n        gc.collect()\n        if accelerator is not None:\n            accelerator.free_memory()\n\n    def draw_boxes_on_image(self, *args):\n        return draw_boxes_on_image(*args, use_centerhw=self.args.use_centerhw)\n\n    def x2t(self, inputs_embeds, attention_mask=None):\n        return self.vl_gpt.language_model.generate(\n                inputs_embeds=inputs_embeds,\n                attention_mask=attention_mask,\n                pad_token_id=self.tokenizer.eos_token_id,\n                bos_token_id=self.tokenizer.bos_token_id,\n                eos_token_id=self.tokenizer.eos_token_id,\n                max_new_tokens=512,\n                do_sample=False,\n                use_cache=True,\n            )\n\n    def t2i(self, inputs_ids, parallel_size, image_token_num_per_image, cfg_weight, temperature, img_size, patch_size, gt_image=None, batch=None, mask=None, tokens=None, emb=None):\n        generator = torch.Generator(device='cuda').manual_seed(self.args.seed)\n\n        if self.args.use_teacher_forcing and gt_image is not None:\n            with torch.no_grad():\n                gt_images = gt_image.bfloat16()\n                bs = gt_images.shape[0]\n                gt_labels = self.vl_gpt.gen_vision_model.encode(gt_images)[-1][-1].reshape(bs,-1) # torch.Size([self.image_token_num_per_image])\n        else:\n            gt_labels = None\n\n        if tokens is None and emb is None:\n            tokens = torch.zeros((parallel_size*2, len(inputs_ids)), dtype=torch.int).cuda()\n            for i in range(parallel_size*2):\n                tokens[i, :] = inputs_ids\n                if i % 2 != 0:\n                    tokens[i, 1:-1] = self.vl_chat_processor.pad_id\n            inputs_embeds = self.vl_gpt.language_model.get_input_embeddings()(tokens)\n        else:\n            if tokens is None:\n                inputs_embeds = emb\n            else:\n                tokens = torch.cat([tokens]*parallel_size)\n                inputs_embeds = self.vl_gpt.language_model.get_input_embeddings()(tokens)\n            mask = torch.cat([mask]*parallel_size)\n\n        num_gen = inputs_embeds.shape[0] // 2\n\n        generated_tokens = self.sample_image(inputs_embeds, num_gen, image_token_num_per_image, mask, cfg_weight, temperature, generator, batch, gt_labels)\n\n        dec = self.vl_gpt.gen_vision_model.decode_code(generated_tokens.to(dtype=torch.int), shape=[num_gen, 8, img_size//patch_size, img_size//patch_size])\n\n        if self.args.use_teacher_forcing:\n            mask_image = batch['edit_region']\n            bs = mask_image.shape[0]\n            mask_image = resize_pt(mask_image.reshape(bs,1,24,24).repeat(1,3,1,1), self.args.janus_hw).to(dec)\n        else:\n            mask_image = None\n\n\n        return dec, mask_image\n\n    def sample_image(self, inputs_embeds, num_gen, image_token_num_per_image, mask, cfg_weight, temperature, generator, batch, gt_labels, ):\n        generated_tokens = torch.zeros((num_gen, image_token_num_per_image), dtype=torch.int).cuda()\n\n        for i in tqdm(range(image_token_num_per_image)):\n            outputs = self.vl_gpt.language_model.model(\n                inputs_embeds=inputs_embeds, \n                attention_mask=mask.to(self.device) if mask is not None else None,\n                use_cache=True, \n                past_key_values=outputs.past_key_values if i != 0 else None,\n            )\n            hidden_states = outputs.last_hidden_state\n            \n            logits = self.vl_gpt.gen_head(hidden_states[:, -1, :])\n            logit_cond = logits[0::2, :]\n            logit_uncond = logits[1::2, :]\n\n            if self.args.cfg_weight is not None:\n                cfg_weight = self.args.cfg_weight\n                \n            \n            logits = logit_uncond + cfg_weight * (logit_cond-logit_uncond)\n            probs = torch.softmax(logits / temperature, dim=-1)\n            # 8,16384\n\n            next_token = torch.multinomial(probs, num_samples=1, generator=generator)#bs,1\n\n            if self.args.use_teacher_forcing:\n                edit_region = batch['edit_region']\n                bs = len(edit_region)\n                for bid in range(bs):\n                    if edit_region[bid,i].item() == 0:\n                        next_token[bid,0] = gt_labels[bid,i]\n\n            generated_tokens[:, i] = next_token.squeeze(dim=-1)\n\n            next_token = torch.cat([next_token.unsqueeze(dim=1), next_token.unsqueeze(dim=1)], dim=1).view(-1)\n            img_embeds = self.vl_gpt.prepare_gen_img_embeds(next_token)\n            inputs_embeds = img_embeds.unsqueeze(dim=1)\n            # inputs_embeds = torch.cat([inputs_embeds, img_embeds.unsqueeze(dim=1)], dim=1) ## todo\n\n        return generated_tokens\n\n    def t2i_infer_collate(self, batch):\n        batch = default_collate(batch)\n\n        ##### t2i\n        bs = len(batch['prompt'])\n        all_inputs_ids = []\n        for prompt in batch['prompt']:\n            wrapped_prompt, inputs_ids = self.wrap_t2i_prompt(prompt)\n            all_inputs_ids.append(inputs_ids)\n\n        max_length = max(map(len, all_inputs_ids))\n\n        padded_all_inputs_ids = torch.ones((bs, max_length))*self.vl_chat_processor.pad_id\n        padded_all_attention_mask = torch.zeros((bs, max_length))\n        for i, inputs_ids in enumerate(all_inputs_ids):\n            padded_all_inputs_ids[i, -len(inputs_ids):] = inputs_ids\n            padded_all_attention_mask[i, -len(inputs_ids):] = 1\n        padded_all_inputs_ids = padded_all_inputs_ids.int()\n        padded_all_attention_mask = torch.cat([padded_all_attention_mask, torch.ones((bs, self.image_token_num_per_image))], dim=-1)\n        padded_all_attention_mask = padded_all_attention_mask.int()\n        \n        batch.update(dict(\n            cfg_inputs_ids=padded_all_inputs_ids,\n            cfg_attention_mask=padded_all_attention_mask\n        ))\n        return batch\n\n    def t2i_infer_collate_batch(self, \n        batch, \n        use_uni=False,\n    ):\n        bs = len(batch['prompt'])\n\n        if use_uni:\n            t2i_inputs_ids = batch['uni_inputs_ids']\n            t2i_attention_mask = batch['uni_attention_mask']\n        else:\n            assert False\n            t2i_inputs_ids = batch['t2i_inputs_ids']\n            t2i_attention_mask = batch['t2i_attention_mask']\n\n        max_length = t2i_inputs_ids.shape[-1]\n\n        if self.args.use_neg_box:\n            neg_all_inputs_ids = []\n            for base_caption, grounding_prompt in zip(batch['neg_base_caption'], batch['neg_gt_grounding']):\n                _, inputs_ids = self.wrap_uni_prompt(base_caption, grounding_prompt)\n                neg_all_inputs_ids.append(inputs_ids)\n\n            max_length_neg = max([len(t) for t in neg_all_inputs_ids])\n            if max_length_neg > max_length:\n                need_pad = max_length_neg - max_length\n                \n                t2i_inputs_ids = torch.cat([torch.ones((bs,need_pad)).to(t2i_inputs_ids)*self.vl_chat_processor.pad_id, t2i_inputs_ids], dim=1)\n\n                t2i_attention_mask = torch.cat([torch.zeros((bs,need_pad)).to(t2i_attention_mask)*self.vl_chat_processor.pad_id, t2i_attention_mask], dim=1)\n                max_length = max_length_neg\n\n            uni_inputs_ids, uni_attention_mask = self.pad_input_ids(neg_all_inputs_ids, max_length)\n            uni_attention_mask_image = torch.cat([uni_attention_mask, torch.ones((bs, self.image_token_num_per_image))], dim=-1)\n            neg_ids = uni_inputs_ids\n            neg_mask = uni_attention_mask_image\n        else:\n            # _, neg_inputs_ids = self.wrap_uni_prompt(self.args.neg_prompt, '<grounding></grounding>')\n            _, neg_inputs_ids = self.wrap_uni_prompt(self.args.neg_prompt, '')\n            # _, neg_inputs_ids = self.wrap_t2i_prompt(self.args.neg_prompt)\n\n            max_length_neg = neg_inputs_ids.shape[-1]\n            if max_length_neg > max_length:\n                need_pad = max_length_neg - max_length\n                \n                t2i_inputs_ids = torch.cat([torch.ones((bs,need_pad)).to(t2i_inputs_ids)*self.vl_chat_processor.pad_id, t2i_inputs_ids], dim=1)\n\n                t2i_attention_mask = torch.cat([torch.zeros((bs,need_pad)).to(t2i_attention_mask)*self.vl_chat_processor.pad_id, t2i_attention_mask], dim=1)\n                max_length = max_length_neg\n\n            neg_ids, neg_mask = self.pad_input_ids([neg_inputs_ids]*bs, max_length=max_length)\n            neg_mask = torch.cat([neg_mask, torch.ones((bs, self.image_token_num_per_image))], dim=-1)\n\n        neg_mask2 = neg_mask\n\n        padded_all_inputs_ids = torch.stack([t2i_inputs_ids, neg_ids.to(self.device)], dim=1).view(bs*2,-1)\n        padded_all_attention_mask = torch.stack([t2i_attention_mask, neg_mask2.to(self.device)], dim=1).view(bs*2,-1)\n        \n        batch.update(dict(\n            cfg_inputs_ids=padded_all_inputs_ids.int(),\n            cfg_attention_mask=padded_all_attention_mask.int()\n        ))\n        return batch\n    \n    def pad_input_ids(self, all_inputs_ids, max_length=None):\n        bs = len(all_inputs_ids)\n\n        if self.args.debug_max_seq_len is not None:\n            # print('debugging...')\n            max_length = self.args.debug_max_seq_len\n        if max_length is None:\n            max_length = max(map(len, all_inputs_ids))\n\n        padded_all_inputs_ids = torch.ones((bs, max_length))*self.vl_chat_processor.pad_id\n        padded_all_attention_mask = torch.zeros((bs, max_length))\n        for i, inputs_ids in enumerate(all_inputs_ids):\n            padded_all_inputs_ids[i, -len(inputs_ids):] = inputs_ids\n            padded_all_attention_mask[i, -len(inputs_ids):] = 1\n\n        if self.args.test or self.args.func is not None:\n            pass\n        else:\n            if padded_all_inputs_ids.shape[1] > self.args.max_seq_len:\n                print('pad_input_ids: extend max_seq_len!!!') ## todo\n                print(padded_all_inputs_ids.shape)\n\n                num_start = padded_all_inputs_ids.shape[1] - self.args.max_seq_len\n                padded_all_inputs_ids = padded_all_inputs_ids[:, num_start:]\n                padded_all_attention_mask = padded_all_attention_mask[:, num_start:]\n\n        return padded_all_inputs_ids.int(), padded_all_attention_mask.int()\n\n\n    def mmu_collatexx(self, batch, pass_default=False):\n        if pass_default:\n            pass\n        else:\n            try:\n                batch = default_collate(batch) #cpu\n            except Exception as e:\n                print(e)\n                print(batch[0].keys())\n                print(batch[1].keys())\n                traceback.print_exc()\n                import pdb;pdb.set_trace()\n        return batch\n    \n    def mmu_collate(self, batch, pass_default=False):\n        if pass_default:\n            pass\n        else:\n            try:\n                batch = default_collate(batch) #cpu\n            except Exception as e:\n                print(e)\n                print(batch[0].keys())\n                print(batch[1].keys())\n                traceback.print_exc()\n                import pdb;pdb.set_trace()\n\n        if self.args.func == 'minicpm_cap':\n            return batch\n\n        bs = len(batch['prompt'])\n\n        ##### t2i\n        all_inputs_ids = []\n        for prompt in batch['prompt']:\n            _, inputs_ids = self.wrap_t2i_prompt(prompt)\n            all_inputs_ids.append(inputs_ids)\n        t2i_inputs_ids, t2i_attention_mask = self.pad_input_ids(all_inputs_ids)\n        t2i_attention_mask = torch.cat([t2i_attention_mask, torch.ones((bs, self.image_token_num_per_image))], dim=-1)\n        batch.update(dict(\n            t2i_inputs_ids=t2i_inputs_ids,\n            t2i_attention_mask=t2i_attention_mask\n        ))\n\n        ### uni\n        all_inputs_ids = []\n        for base_caption, grounding_prompt in zip(batch['base_caption'], batch['gt_grounding']):\n            _, inputs_ids = self.wrap_uni_prompt(base_caption, grounding_prompt)\n            all_inputs_ids.append(inputs_ids)\n        uni_inputs_ids, uni_attention_mask = self.pad_input_ids(all_inputs_ids)\n        uni_attention_mask_image = torch.cat([uni_attention_mask, torch.ones((bs, self.image_token_num_per_image))], dim=-1)\n        batch.update(dict(\n            uni_inputs_ids=uni_inputs_ids,\n            uni_attention_mask=uni_attention_mask_image\n        ))\n        # uni_attention_mask_image: bs, seq\n\n        # uni_stage1\n        all_inputs_ids = []\n        for base_caption, grounding_prompt in zip(batch['base_caption'], batch['gt_grounding']):\n            _, inputs_ids = self.wrap_uni_prompt(base_caption, \"<grounding>\", in_stage1=True)\n            all_inputs_ids.append(inputs_ids)\n        uni_inputs_ids, uni_attention_mask = self.pad_input_ids(all_inputs_ids)\n        batch.update(dict(\n            uni_stage1_inputs_ids=uni_inputs_ids,\n            uni_stage1_attention_mask=uni_attention_mask\n        ))\n\n        ### mmu\n        all_prepares = []\n        image = batch['image']\n        answer = batch['prompt']\n        question = \"Please describe this image and then give the description and bounding box of each object in the image.\"\n        for i in range(len(image)):\n            conversation = [\n                {\"role\": \"<|User|>\",\n                \"content\": f\"<image_placeholder>\\n{question}\",\n                \"images\": [image[i:i+1]],},\n                {\"role\": \"<|Assistant|>\", \"content\": f\"{answer[i]}\"},\n            ]\n            prepare = self.vl_chat_processor.process_one(\n                prompt=None, \n                conversations=conversation, \n                images=image[i:i+1]\n            )\n            all_prepares.append(prepare)\n        prepare_inputs = self.vl_chat_processor.batchify(all_prepares)\n        batch.update(dict(\n            prepare_inputs=prepare_inputs,\n        ))\n\n        ### mmu_infer\n        all_prepares = []\n        image = batch['image']\n        answer = batch['prompt']\n        question = \"Please describe this image and then give the description and bounding box of each object in the image.\"\n        for i in range(len(image)):\n            conversation = [\n                {\"role\": \"<|User|>\",\n                \"content\": f\"<image_placeholder>\\n{question}\",\n                \"images\": [image[i:i+1]],},\n                {\"role\": \"<|Assistant|>\", \"content\": \"\"},\n            ]\n            prepare = self.vl_chat_processor.process_one(\n                prompt=None, \n                conversations=conversation, \n                images=image[i:i+1]\n            )\n            all_prepares.append(prepare)\n        prepare_inputs = self.vl_chat_processor.batchify(all_prepares)\n        batch.update(dict(\n            prepare_inputs_infer=prepare_inputs,\n        ))\n        return batch\n\n    def forward_mmu(self, batch, is_plan=False):\n\n        bs = len(batch['prompt'])\n\n        if is_plan:\n            padded_all_inputs_ids = batch['uni_inputs_ids']\n            padded_all_attention_mask = batch['uni_attention_mask']\n\n            padded_all_inputs_embeds = self.vl_gpt.language_model.get_input_embeddings()(padded_all_inputs_ids)\n\n        else:\n            prepare_inputs = batch['prepare_inputs']\n            inputs_embeds = self.vl_gpt.prepare_inputs_embeds(**prepare_inputs) # dict_keys(['sft_format', 'input_ids', 'pixel_values', 'attention_mask', 'images_seq_mask', 'images_emb_mask'])\n\n            padded_all_inputs_embeds = inputs_embeds\n            padded_all_inputs_ids = prepare_inputs['input_ids']\n            padded_all_inputs_ids[padded_all_inputs_ids==100581] = self.vl_chat_processor.pad_id\n            padded_all_attention_mask = prepare_inputs['attention_mask']\n\n            if self.args.test or self.args.func is not None:\n                pass\n            else:\n                if padded_all_inputs_embeds.shape[1] > self.args.max_seq_len + 576:\n                    print('mmu exceeds maximum length')\n                    start = padded_all_inputs_embeds.shape[1] - (self.args.max_seq_len + 576)\n                    padded_all_inputs_embeds = padded_all_inputs_embeds[:, start:]\n                    padded_all_inputs_ids = padded_all_inputs_ids[:, start:]\n                    padded_all_attention_mask = padded_all_attention_mask[:, start:]\n\n        # run the model to get the response\n        outputs = self.vl_gpt.language_model.model(\n            inputs_embeds=padded_all_inputs_embeds,\n            attention_mask=padded_all_attention_mask.to(self.device),\n            past_key_values=None,\n            use_cache=False,\n        )\n        hidden_states = outputs.last_hidden_state\n\n        logits = self.vl_gpt.language_model.lm_head(hidden_states)# torch.Size([8, 896, 102400])\n        loss = self.cal_lm_loss(\n            logits=logits,\n            labels=padded_all_inputs_ids.to(torch.long), \n            ignore_index = self.vl_chat_processor.pad_id,\n            vocab_size=logits.shape[-1],\n        )\n\n        if is_plan:\n            return dict(loss_plan_lm=loss)\n        else:\n            return dict(loss_mmu=loss)\n\n    def cal_lm_loss(\n        self, \n        logits,\n        labels,\n        ignore_index,\n        vocab_size,\n        is_image=False,\n    ):\n        return ForCausalLMLoss(\n            logits=logits,\n            labels=labels,\n            ignore_index=ignore_index,\n            vocab_size=vocab_size,\n        )\n\n    def forward_t2i(self, batch, is_uni=False):\n        bs = len(batch['prompt'])\n        if is_uni:\n            padded_all_inputs_ids = batch['uni_inputs_ids']\n            padded_all_attention_mask = batch['uni_attention_mask']\n            inputs_embeds = self.vl_gpt.language_model.get_input_embeddings()(padded_all_inputs_ids)\n        else:\n            padded_all_inputs_ids = batch['t2i_inputs_ids']\n            padded_all_attention_mask = batch['t2i_attention_mask']\n            inputs_embeds = self.vl_gpt.language_model.get_input_embeddings()(padded_all_inputs_ids)\n\n        with torch.no_grad():\n            images = batch['image'].bfloat16()\n            all_labels = self.vl_gpt.gen_vision_model.encode(images)[-1][-1].reshape(bs, -1)\n\n        img_embeds = self.vl_gpt.prepare_gen_img_embeds(all_labels)\n\n        num_image_token = img_embeds.shape[1]\n\n        inputs_embeds = torch.cat([inputs_embeds, img_embeds], dim=1)\n\n        outputs = self.vl_gpt.language_model.model(\n            inputs_embeds=inputs_embeds, \n            attention_mask=padded_all_attention_mask,\n            use_cache=False, \n            past_key_values=None,\n            # image_position_ids=image_position_ids,\n        )\n        hidden_states = outputs.last_hidden_state\n\n        logits = self.vl_gpt.gen_head(hidden_states[:, -(num_image_token+1):, :])\n\n\n        if self.args.use_local_edit_loss:\n            edit_region = batch['edit_region'].bool()\n            all_labels = all_labels.clone()\n            all_labels[edit_region==0] = self.vl_chat_processor.pad_id\n            all_labels = all_labels.detach()\n\n        loss_t2i = self.cal_lm_loss(\n            logits=logits,\n            labels=torch.cat([torch.zeros((bs,1)).to(all_labels), all_labels], dim=1), \n            ignore_index = self.vl_chat_processor.pad_id,\n            vocab_size=logits.shape[-1],\n            is_image=True,\n        )\n\n        if is_uni:\n            logits_lm = self.vl_gpt.language_model.lm_head(hidden_states[:, :-(num_image_token), :])\n\n            loss_lm= self.cal_lm_loss(\n                logits=logits_lm,\n                labels=padded_all_inputs_ids.to(torch.long), \n                ignore_index = self.vl_chat_processor.pad_id,\n                vocab_size=logits_lm.shape[-1],\n            )\n            loss_dict = dict(\n                loss_uni_t2i=loss_t2i,\n                loss_uni_lm=loss_lm,\n            )\n        else:\n            loss_dict = dict(loss_t2i=loss_t2i)\n\n        return loss_dict\n\n    def forward_uni(self, batch):\n        return self.forward_t2i(batch, is_uni=True)\n\n    def forward_plan(self, batch):\n        return self.forward_mmu(batch, is_plan=True)\n    \n    def setup_data(self, accelerator):\n        args = self.args\n\n        if args.debug:\n            args.max_val_len = 1\n\n        test_dataset, test_dataloader = get_dataset(\n            args,\n            args.test_data.data_name, \n            args.test_data.batch_size, \n            is_test=True,\n            collate_fn=self.mmu_collate,\n        )\n\n        test_dataloader = accelerator.prepare(test_dataloader)\n        print(f\"test_dataloader: {args.test_data.data_name}, {len(test_dataloader)}\")\n        self.test_dataset = test_dataset\n        self.test_dataloader = test_dataloader\n\n        if self.args.test or self.args.func is not None:\n            train_dataset = test_dataset\n            train_dataloader = test_dataloader\n            self.train_dataset = train_dataset\n            self.train_dataloader = train_dataloader\n        else:\n            iterables_train = {}\n            flow2task = {}\n            train_datasets = []\n            dataset_dict = {}\n            for flow_id, data_item in enumerate(args.train_data):\n                if self.args.debug:\n                    data_item.batch_size = 2\n                if self.args.no_full or self.args.debug:\n                    if data_item.data_name == 'hico_full':\n                        data_item.data_name = 'hico'\n                    elif isinstance(data_item.data_name, list):\n                         for i in range(len(data_item.data_name)):\n                            if data_item.data_name[i] == 'hico_full':\n                                data_item.data_name[i] = 'hico'\n\n                dataset_same = dataset_dict.get(str(data_item.data_name), None)\n                train_dataset, train_dataloader = get_dataset(\n                    args,\n                    data_item.data_name, \n                    data_item.batch_size, \n                    collate_fn=self.mmu_collate,\n                    dataset=dataset_same,\n                )\n                dataset_dict[str(data_item.data_name)] = train_dataset\n\n                train_dataloader = accelerator.prepare(train_dataloader)\n\n                print(f\"\\ntrain_dataset_{flow_id}: {data_item.data_name}, {len(train_dataset)}\")\n                iterables_train[flow_id] = train_dataloader\n                flow2task[flow_id] = data_item.task_type\n\n                train_datasets.append(train_dataset)\n\n            self.flow2task = flow2task\n\n            train_dataloader = CombinedLoader(iterables_train, mode=\"min_size\")\n            train_dataloader = iter(train_dataloader)\n\n\n            print(f\"\\nAll len(train_dataloader): {len(train_dataloader)}\")\n\n            self.train_dataset = train_dataset\n            self.train_dataloader = train_dataloader\n\n        test_dataset[0]\n        test_dataset[1]\n        test_dataset[2]\n        return train_dataloader, train_dataset\n\n    def forward(self, batch):\n        if self.args.scale_emb_grad is not None:\n            a = self.args.scale_emb_grad\n            self.vl_gpt.language_model.model.embed_tokens.data = self.vl_gpt.language_model.model.embed_tokens.weight * a + self.vl_gpt.language_model.model.embed_tokens.weight.detach() * (1 - a)\n\n        batch, idx1, idx2 = batch\n\n        loss_dict = {}\n\n        forward_funcs = dict(\n            t2i=self.forward_t2i,\n            mmu=self.forward_mmu,\n            uni=self.forward_uni,\n            plan=self.forward_plan,\n        )\n\n        for flow_id in batch:\n            task_type = self.flow2task[flow_id]\n            func = forward_funcs[task_type]\n            loss_dict_sub = func(batch[flow_id])\n            loss_dict_sub = {f\"{k}_{flow_id}\":v for k,v in loss_dict_sub.items()}\n            loss_dict.update(loss_dict_sub)\n\n        loss = 0\n        for k in loss_dict:\n            loss_i = loss_dict[k] * getattr(self.args, f'{k}_scale', 1)\n            if 'lm' in k and self.args.plan_lr_scale is not None:\n                loss_i = loss_i * self.args.plan_lr_scale\n            loss += loss_i\n            loss_dict[k] = loss_i.detach().item()\n\n        return loss, loss_dict\n\n    def validation(\n        self, \n        global_step=0, \n        accelerator=None, \n        test_mode=False,\n        val_num=None\n    ):\n        args = self.args\n        test_mode = args.test or test_mode\n        val_num = val_num or args.max_test_len\n\n        if test_mode:\n            patha = osp.join(args.output_dir, 'test', f\"{args.test_data.data_name}_{args.test_data.task_type}_{val_num}\")\n            path = osp.join(patha, f\"{global_step}\")\n            batch_path = osp.join(patha, f\"{global_step}_batch\")\n            mkdir(osp.join(path, \"gt_image\"))\n            mkdir(osp.join(path, \"pr_image\"))\n            mkdir(osp.join(path, \"image_ids\"))\n            mkdir(osp.join(path, \"gt_image_ids\"))\n        else:\n            path = osp.join(args.output_dir, 'val')\n            batch_path = path\n        mkdir(path)\n        mkdir(batch_path)\n\n        kwargs = {}\n        func = self.uni_generate\n        if args.test_data.task_type == 't2i':\n            kwargs.update(pred_layout=False)\n            kwargs.update(use_uni_prompt_in_t2i=False)\n        elif args.test_data.task_type == 'uni_2stage':\n            pass\n        elif args.test_data.task_type == 'uni':\n            kwargs.update(pred_layout=False)\n        elif args.test_data.task_type == 'mmu':\n            kwargs.update(pred_image=False)\n            kwargs.update(is_mmu=True)\n        elif args.test_data.task_type == 'plan':\n            kwargs.update(pred_image=False)\n        else:\n            assert False\n\n        rets = []\n        for idx, batch in enumerate(tqdm(self.test_dataloader)):\n            if val_num != -1 and idx >= val_num: break\n            if idx >= self.args.test_start:\n                pass\n            else:\n                continue\n            \n            batch_str = f'{idx}' if test_mode else f'{global_step}_{idx}'\n            \n            out = func(\n                batch=batch, \n                batch_idx=batch_str,\n                gen_path=batch_path, \n                accelerator=accelerator,\n                parallel_size=1,\n                **kwargs,\n            )\n\n            if not test_mode:\n                break\n\n            gt_image = batch['image']\n            image_id = batch['image_id']\n            edited_image = batch.get('edited_image', None)\n            H = batch['H']\n            W = batch['W']\n            pr_image = out['pr_image']\n            pr_grounding = out['pr_grounding']\n\n            bs = gt_image.shape[0]\n\n            print(path)\n            for i in range(len(gt_image)):\n                if image_id[i] != '':\n                    # pil = to_pil(denorm_pt(pr_image[i]))\n                    # h,w = H[i],W[i]\n                    # if h * w != 0:\n                    #     pil = pil.resize((w,h))\n                    to_pil(denorm_pt(pr_image[i])).save(f\"{path}/image_ids/{image_id[i]}.jpg\")\n                    to_pil(denorm_pt(gt_image[i])).save(f\"{path}/gt_image_ids/{image_id[i]}.jpg\")\n\n                p = self.args.parallel_size\n                if p > 1:\n                    for t in range(self.args.parallel_size):\n                        to_pil(denorm_pt(pr_image[i*p+t])).save(f\"{path}/pr_image/{idx*bs+i}_{t}.png\")\n                else:\n                    to_pil(denorm_pt(pr_image[i*p])).save(f\"{path}/pr_image/{idx*bs+i}.png\")\n                to_pil(denorm_pt(gt_image[i])).save(f\"{path}/gt_image/{idx*bs+i}.png\")\n\n                if edited_image is not None:\n                    mkdir(f\"{path}/edited_image/\")\n                    to_pil(denorm_pt(edited_image[i])).save(f\"{path}/edited_image/{idx*bs+i}.png\")\n\n    @property\n    def device(self,):\n        return self.get_device(self.vl_gpt)\n\n    @property\n    def dtype(self):\n        return self.get_dtype(self.vl_gpt)"}
{"type": "source_file", "path": "src/utils/causal_loss.py", "content": "import torch\nfrom torch import nn\nfrom copy import deepcopy\n\ndef fixed_cross_entropy(source, target, num_items_in_batch: int = None, ignore_index: int = -100, **kwargs):\n    reduction = \"sum\" if num_items_in_batch is not None else \"mean\"\n    loss = nn.functional.cross_entropy(source, target, ignore_index=ignore_index, reduction=reduction)\n    if reduction == \"sum\":\n        loss = loss / num_items_in_batch\n    return loss\n\ndef ForCausalLMLoss(\n    logits, labels, vocab_size: int, num_items_in_batch: int = None, ignore_index: int = -100, **kwargs\n):\n    # Upcast to float if we need to compute the loss to avoid potential precision issues\n    logits = logits.float()\n    # Shift so that tokens < n predict n\n    shift_logits = logits[..., :-1, :].contiguous()\n    shift_labels = labels[..., 1:].contiguous()\n\n    # Flatten the tokens\n    shift_logits = shift_logits.view(-1, vocab_size)\n    shift_labels = shift_labels.view(-1)\n    # Enable model parallelism\n    shift_labels = shift_labels.to(shift_logits.device)\n    loss = fixed_cross_entropy(shift_logits, shift_labels, num_items_in_batch, ignore_index, **kwargs)\n    return loss"}
{"type": "source_file", "path": "three_party/Janus/demo/fastapi_app.py", "content": "from fastapi import FastAPI, File, Form, UploadFile, HTTPException\nfrom fastapi.responses import JSONResponse, StreamingResponse\nimport torch\nfrom transformers import AutoConfig, AutoModelForCausalLM\nfrom janus.models import MultiModalityCausalLM, VLChatProcessor\nfrom PIL import Image\nimport numpy as np\nimport io\n\napp = FastAPI()\n\n# Load model and processor\nmodel_path = \"deepseek-ai/Janus-1.3B\"\nconfig = AutoConfig.from_pretrained(model_path)\nlanguage_config = config.language_config\nlanguage_config._attn_implementation = 'eager'\nvl_gpt = AutoModelForCausalLM.from_pretrained(model_path,\n                                              language_config=language_config,\n                                              trust_remote_code=True)\nvl_gpt = vl_gpt.to(torch.bfloat16).cuda()\n\nvl_chat_processor = VLChatProcessor.from_pretrained(model_path)\ntokenizer = vl_chat_processor.tokenizer\ncuda_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n\n@torch.inference_mode()\ndef multimodal_understanding(image_data, question, seed, top_p, temperature):\n    torch.cuda.empty_cache()\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    torch.cuda.manual_seed(seed)\n\n    conversation = [\n        {\n            \"role\": \"User\",\n            \"content\": f\"<image_placeholder>\\n{question}\",\n            \"images\": [image_data],\n        },\n        {\"role\": \"Assistant\", \"content\": \"\"},\n    ]\n\n    pil_images = [Image.open(io.BytesIO(image_data))]\n    prepare_inputs = vl_chat_processor(\n        conversations=conversation, images=pil_images, force_batchify=True\n    ).to(cuda_device, dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16)\n    \n    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n    outputs = vl_gpt.language_model.generate(\n        inputs_embeds=inputs_embeds,\n        attention_mask=prepare_inputs.attention_mask,\n        pad_token_id=tokenizer.eos_token_id,\n        bos_token_id=tokenizer.bos_token_id,\n        eos_token_id=tokenizer.eos_token_id,\n        max_new_tokens=512,\n        do_sample=False if temperature == 0 else True,\n        use_cache=True,\n        temperature=temperature,\n        top_p=top_p,\n    )\n    \n    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n    return answer\n\n\n@app.post(\"/understand_image_and_question/\")\nasync def understand_image_and_question(\n    file: UploadFile = File(...),\n    question: str = Form(...),\n    seed: int = Form(42),\n    top_p: float = Form(0.95),\n    temperature: float = Form(0.1)\n):\n    image_data = await file.read()\n    response = multimodal_understanding(image_data, question, seed, top_p, temperature)\n    return JSONResponse({\"response\": response})\n\n\ndef generate(input_ids,\n             width,\n             height,\n             temperature: float = 1,\n             parallel_size: int = 5,\n             cfg_weight: float = 5,\n             image_token_num_per_image: int = 576,\n             patch_size: int = 16):\n    torch.cuda.empty_cache()\n    tokens = torch.zeros((parallel_size * 2, len(input_ids)), dtype=torch.int).to(cuda_device)\n    for i in range(parallel_size * 2):\n        tokens[i, :] = input_ids\n        if i % 2 != 0:\n            tokens[i, 1:-1] = vl_chat_processor.pad_id\n    inputs_embeds = vl_gpt.language_model.get_input_embeddings()(tokens)\n    generated_tokens = torch.zeros((parallel_size, image_token_num_per_image), dtype=torch.int).to(cuda_device)\n\n    pkv = None\n    for i in range(image_token_num_per_image):\n        outputs = vl_gpt.language_model.model(inputs_embeds=inputs_embeds, use_cache=True, past_key_values=pkv)\n        pkv = outputs.past_key_values\n        hidden_states = outputs.last_hidden_state\n        logits = vl_gpt.gen_head(hidden_states[:, -1, :])\n        logit_cond = logits[0::2, :]\n        logit_uncond = logits[1::2, :]\n        logits = logit_uncond + cfg_weight * (logit_cond - logit_uncond)\n        probs = torch.softmax(logits / temperature, dim=-1)\n        next_token = torch.multinomial(probs, num_samples=1)\n        generated_tokens[:, i] = next_token.squeeze(dim=-1)\n        next_token = torch.cat([next_token.unsqueeze(dim=1), next_token.unsqueeze(dim=1)], dim=1).view(-1)\n        img_embeds = vl_gpt.prepare_gen_img_embeds(next_token)\n        inputs_embeds = img_embeds.unsqueeze(dim=1)\n    patches = vl_gpt.gen_vision_model.decode_code(\n        generated_tokens.to(dtype=torch.int), \n        shape=[parallel_size, 8, width // patch_size, height // patch_size]\n    )\n\n    return generated_tokens.to(dtype=torch.int), patches\n\n\ndef unpack(dec, width, height, parallel_size=5):\n    dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)\n    dec = np.clip((dec + 1) / 2 * 255, 0, 255)\n\n    visual_img = np.zeros((parallel_size, width, height, 3), dtype=np.uint8)\n    visual_img[:, :, :] = dec\n\n    return visual_img\n\n\n@torch.inference_mode()\ndef generate_image(prompt, seed, guidance):\n    torch.cuda.empty_cache()\n    seed = seed if seed is not None else 12345\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed)\n    width = 384\n    height = 384\n    parallel_size = 5\n    \n    with torch.no_grad():\n        messages = [{'role': 'User', 'content': prompt}, {'role': 'Assistant', 'content': ''}]\n        text = vl_chat_processor.apply_sft_template_for_multi_turn_prompts(\n            conversations=messages,\n            sft_format=vl_chat_processor.sft_format,\n            system_prompt=''\n        )\n        text = text + vl_chat_processor.image_start_tag\n        input_ids = torch.LongTensor(tokenizer.encode(text))\n        _, patches = generate(input_ids, width // 16 * 16, height // 16 * 16, cfg_weight=guidance, parallel_size=parallel_size)\n        images = unpack(patches, width // 16 * 16, height // 16 * 16)\n\n        return [Image.fromarray(images[i]).resize((1024, 1024), Image.LANCZOS) for i in range(parallel_size)]\n\n\n@app.post(\"/generate_images/\")\nasync def generate_images(\n    prompt: str = Form(...),\n    seed: int = Form(None),\n    guidance: float = Form(5.0),\n):\n    try:\n        images = generate_image(prompt, seed, guidance)\n        def image_stream():\n            for img in images:\n                buf = io.BytesIO()\n                img.save(buf, format='PNG')\n                buf.seek(0)\n                yield buf.read()\n\n        return StreamingResponse(image_stream(), media_type=\"multipart/related\")\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Image generation failed: {str(e)}\")\n\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"}
{"type": "source_file", "path": "project/plangen/dataset/plan/data_plan.py", "content": "from torch.utils.data import Dataset\nimport random\nfrom copy import deepcopy\nfrom torchvision.transforms import Resize\nfrom glob import glob\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport random\nfrom copy import deepcopy\nfrom torchvision.transforms import Resize\nfrom datasets import load_dataset\nfrom src.utils.funcs import convert_to_np, load_jsonl\nimport numpy as np\nimport os\nfrom src.utils.funcs import *\n\nclass Dataset_plan(Dataset):\n    def __init__(\n        self,\n        args=None,\n        is_test=False,\n        model='llama',\n    ):\n        self.args = args\n        self.is_test = is_test\n\n        datas = load_json(f'gen_data/plan1k_{model}_out.json')\n        self.datas = datas\n\n        self.caps = load_jsonl('gen_data/1k_cap.jsonl')\n\n\n    def __len__(self):\n        return len(self.datas)\n\n    def __getitem__(self, i):\n        data = self.datas[i]\n        cap = self.caps[i]\n\n        prompt = cap\n\n        obj_class = data['obj_class']\n        obj_bbox = data['obj_bbox']\n\n        obj_bbox = torch.tensor(obj_bbox).to(torch.float)\n        obj_bbox = obj_bbox.reshape(-1,4)\n        obj_bbox[:,0::2] /= 512\n        obj_bbox[:,1::2] /= 512\n        obj_bbox[:,2:] += obj_bbox[:,:2]\n\n        if len(obj_bbox) > 10:\n            obj_bbox = obj_bbox[:10]\n            obj_class = obj_class[:10]\n\n        ret = dict(\n            base_caption=prompt,\n            obj_bbox=obj_bbox,\n            obj_class=obj_class,\n        )\n        return ret"}
{"type": "source_file", "path": "three_party/Janus/janus/__init__.py", "content": "# Copyright (c) 2023-2024 DeepSeek.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the \"Software\"), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n# the Software, and to permit persons to whom the Software is furnished to do so,\n# subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n# check if python version is above 3.10\nimport sys\n\nif sys.version_info >= (3, 10):\n    print(\"Python version is above 3.10, patching the collections module.\")\n    # Monkey patch collections\n    import collections\n    import collections.abc\n\n    for type_name in collections.abc.__all__:\n        setattr(collections, type_name, getattr(collections.abc, type_name))\n"}
{"type": "source_file", "path": "three_party/Janus/demo/app_janusflow.py", "content": "import gradio as gr\nimport torch\nfrom janus.janusflow.models import MultiModalityCausalLM, VLChatProcessor\nfrom PIL import Image\nfrom diffusers.models import AutoencoderKL\nimport numpy as np\n\ncuda_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Load model and processor\nmodel_path = \"deepseek-ai/JanusFlow-1.3B\"\nvl_chat_processor = VLChatProcessor.from_pretrained(model_path)\ntokenizer = vl_chat_processor.tokenizer\n\nvl_gpt = MultiModalityCausalLM.from_pretrained(model_path)\nvl_gpt = vl_gpt.to(torch.bfloat16).to(cuda_device).eval()\n\n# remember to use bfloat16 dtype, this vae doesn't work with fp16\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sdxl-vae\")\nvae = vae.to(torch.bfloat16).to(cuda_device).eval()\n\n# Multimodal Understanding function\n@torch.inference_mode()\n# Multimodal Understanding function\ndef multimodal_understanding(image, question, seed, top_p, temperature):\n    # Clear CUDA cache before generating\n    torch.cuda.empty_cache()\n    \n    # set seed\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    torch.cuda.manual_seed(seed)\n    \n    conversation = [\n        {\n            \"role\": \"User\",\n            \"content\": f\"<image_placeholder>\\n{question}\",\n            \"images\": [image],\n        },\n        {\"role\": \"Assistant\", \"content\": \"\"},\n    ]\n    \n    pil_images = [Image.fromarray(image)]\n    prepare_inputs = vl_chat_processor(\n        conversations=conversation, images=pil_images, force_batchify=True\n    ).to(cuda_device, dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16)\n    \n    \n    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n    \n    outputs = vl_gpt.language_model.generate(\n        inputs_embeds=inputs_embeds,\n        attention_mask=prepare_inputs.attention_mask,\n        pad_token_id=tokenizer.eos_token_id,\n        bos_token_id=tokenizer.bos_token_id,\n        eos_token_id=tokenizer.eos_token_id,\n        max_new_tokens=512,\n        do_sample=False if temperature == 0 else True,\n        use_cache=True,\n        temperature=temperature,\n        top_p=top_p,\n    )\n    \n    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n\n    return answer\n\n\n@torch.inference_mode()\ndef generate(\n    input_ids,\n    cfg_weight: float = 2.0,\n    num_inference_steps: int = 30\n):\n    # we generate 5 images at a time, *2 for CFG\n    tokens = torch.stack([input_ids] * 10).cuda()\n    tokens[5:, 1:] = vl_chat_processor.pad_id\n    inputs_embeds = vl_gpt.language_model.get_input_embeddings()(tokens)\n    print(inputs_embeds.shape)\n\n    # we remove the last <bog> token and replace it with t_emb later\n    inputs_embeds = inputs_embeds[:, :-1, :] \n    \n    # generate with rectified flow ode\n    # step 1: encode with vision_gen_enc\n    z = torch.randn((5, 4, 48, 48), dtype=torch.bfloat16).cuda()\n    \n    dt = 1.0 / num_inference_steps\n    dt = torch.zeros_like(z).cuda().to(torch.bfloat16) + dt\n    \n    # step 2: run ode\n    attention_mask = torch.ones((10, inputs_embeds.shape[1]+577)).to(vl_gpt.device)\n    attention_mask[5:, 1:inputs_embeds.shape[1]] = 0\n    attention_mask = attention_mask.int()\n    for step in range(num_inference_steps):\n        # prepare inputs for the llm\n        z_input = torch.cat([z, z], dim=0) # for cfg\n        t = step / num_inference_steps * 1000.\n        t = torch.tensor([t] * z_input.shape[0]).to(dt)\n        z_enc = vl_gpt.vision_gen_enc_model(z_input, t)\n        z_emb, t_emb, hs = z_enc[0], z_enc[1], z_enc[2]\n        z_emb = z_emb.view(z_emb.shape[0], z_emb.shape[1], -1).permute(0, 2, 1)\n        z_emb = vl_gpt.vision_gen_enc_aligner(z_emb)\n        llm_emb = torch.cat([inputs_embeds, t_emb.unsqueeze(1), z_emb], dim=1)\n\n        # input to the llm\n        # we apply attention mask for CFG: 1 for tokens that are not masked, 0 for tokens that are masked.\n        if step == 0:\n            outputs = vl_gpt.language_model.model(inputs_embeds=llm_emb, \n                                             use_cache=True, \n                                             attention_mask=attention_mask,\n                                             past_key_values=None)\n            past_key_values = []\n            for kv_cache in past_key_values:\n                k, v = kv_cache[0], kv_cache[1]\n                past_key_values.append((k[:, :, :inputs_embeds.shape[1], :], v[:, :, :inputs_embeds.shape[1], :]))\n            past_key_values = tuple(past_key_values)\n        else:\n            outputs = vl_gpt.language_model.model(inputs_embeds=llm_emb, \n                                             use_cache=True, \n                                             attention_mask=attention_mask,\n                                             past_key_values=past_key_values)\n        hidden_states = outputs.last_hidden_state\n        \n        # transform hidden_states back to v\n        hidden_states = vl_gpt.vision_gen_dec_aligner(vl_gpt.vision_gen_dec_aligner_norm(hidden_states[:, -576:, :]))\n        hidden_states = hidden_states.reshape(z_emb.shape[0], 24, 24, 768).permute(0, 3, 1, 2)\n        v = vl_gpt.vision_gen_dec_model(hidden_states, hs, t_emb)\n        v_cond, v_uncond = torch.chunk(v, 2)\n        v = cfg_weight * v_cond - (cfg_weight-1.) * v_uncond\n        z = z + dt * v\n        \n    # step 3: decode with vision_gen_dec and sdxl vae\n    decoded_image = vae.decode(z / vae.config.scaling_factor).sample\n    \n    images = decoded_image.float().clip_(-1., 1.).permute(0,2,3,1).cpu().numpy()\n    images = ((images+1) / 2. * 255).astype(np.uint8)\n    \n    return images\n    \ndef unpack(dec, width, height, parallel_size=5):\n    dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)\n    dec = np.clip((dec + 1) / 2 * 255, 0, 255)\n\n    visual_img = np.zeros((parallel_size, width, height, 3), dtype=np.uint8)\n    visual_img[:, :, :] = dec\n\n    return visual_img\n\n\n@torch.inference_mode()\ndef generate_image(prompt,\n                   seed=None,\n                   guidance=5,\n                   num_inference_steps=30):\n    # Clear CUDA cache and avoid tracking gradients\n    torch.cuda.empty_cache()\n    # Set the seed for reproducible results\n    if seed is not None:\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        np.random.seed(seed)\n    \n    with torch.no_grad():\n        messages = [{'role': 'User', 'content': prompt},\n                    {'role': 'Assistant', 'content': ''}]\n        text = vl_chat_processor.apply_sft_template_for_multi_turn_prompts(conversations=messages,\n                                                                   sft_format=vl_chat_processor.sft_format,\n                                                                   system_prompt='')\n        text = text + vl_chat_processor.image_start_tag\n        input_ids = torch.LongTensor(tokenizer.encode(text))\n        images = generate(input_ids,\n                                   cfg_weight=guidance,\n                                   num_inference_steps=num_inference_steps)\n        return [Image.fromarray(images[i]).resize((1024, 1024), Image.LANCZOS) for i in range(images.shape[0])]\n\n        \n\n# Gradio interface\nwith gr.Blocks() as demo:\n    gr.Markdown(value=\"# Multimodal Understanding\")\n    # with gr.Row():\n    with gr.Row():\n        image_input = gr.Image()\n        with gr.Column():\n            question_input = gr.Textbox(label=\"Question\")\n            und_seed_input = gr.Number(label=\"Seed\", precision=0, value=42)\n            top_p = gr.Slider(minimum=0, maximum=1, value=0.95, step=0.05, label=\"top_p\")\n            temperature = gr.Slider(minimum=0, maximum=1, value=0.1, step=0.05, label=\"temperature\")\n        \n    understanding_button = gr.Button(\"Chat\")\n    understanding_output = gr.Textbox(label=\"Response\")\n\n    examples_inpainting = gr.Examples(\n        label=\"Multimodal Understanding examples\",\n        examples=[\n            [\n                \"explain this meme\",\n                \"./images/doge.png\",\n            ],\n            [\n                \"Convert the formula into latex code.\",\n                \"./images/equation.png\",\n            ],\n        ],\n        inputs=[question_input, image_input],\n    )\n    \n        \n    gr.Markdown(value=\"# Text-to-Image Generation\")\n\n    \n    \n    with gr.Row():\n        cfg_weight_input = gr.Slider(minimum=1, maximum=10, value=2, step=0.5, label=\"CFG Weight\")\n        step_input = gr.Slider(minimum=1, maximum=50, value=30, step=1, label=\"Number of Inference Steps\")\n\n    prompt_input = gr.Textbox(label=\"Prompt\")\n    seed_input = gr.Number(label=\"Seed (Optional)\", precision=0, value=12345)\n\n    generation_button = gr.Button(\"Generate Images\")\n\n    image_output = gr.Gallery(label=\"Generated Images\", columns=2, rows=2, height=300)\n\n    examples_t2i = gr.Examples(\n        label=\"Text to image generation examples.\",\n        examples=[\n            \"Master shifu racoon wearing drip attire as a street gangster.\",\n            \"A cute and adorable baby fox with big brown eyes, autumn leaves in the background enchanting,immortal,fluffy, shiny mane,Petals,fairyism,unreal engine 5 and Octane Render,highly detailed, photorealistic, cinematic, natural colors.\",\n            \"The image features an intricately designed eye set against a circular backdrop adorned with ornate swirl patterns that evoke both realism and surrealism. At the center of attention is a strikingly vivid blue iris surrounded by delicate veins radiating outward from the pupil to create depth and intensity. The eyelashes are long and dark, casting subtle shadows on the skin around them which appears smooth yet slightly textured as if aged or weathered over time.\\n\\nAbove the eye, there's a stone-like structure resembling part of classical architecture, adding layers of mystery and timeless elegance to the composition. This architectural element contrasts sharply but harmoniously with the organic curves surrounding it. Below the eye lies another decorative motif reminiscent of baroque artistry, further enhancing the overall sense of eternity encapsulated within each meticulously crafted detail. \\n\\nOverall, the atmosphere exudes a mysterious aura intertwined seamlessly with elements suggesting timelessness, achieved through the juxtaposition of realistic textures and surreal artistic flourishes. Each component\\u2014from the intricate designs framing the eye to the ancient-looking stone piece above\\u2014contributes uniquely towards creating a visually captivating tableau imbued with enigmatic allure.\",\n        ],\n        inputs=prompt_input,\n    )\n    \n    understanding_button.click(\n        multimodal_understanding,\n        inputs=[image_input, question_input, und_seed_input, top_p, temperature],\n        outputs=understanding_output\n    )\n    \n    generation_button.click(\n        fn=generate_image,\n        inputs=[prompt_input, seed_input, cfg_weight_input, step_input],\n        outputs=image_output\n    )\n\ndemo.launch(share=True)"}
{"type": "source_file", "path": "three_party/Janus/janus/janusflow/__init__.py", "content": "# Copyright (c) 2023-2024 DeepSeek.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the \"Software\"), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n# the Software, and to permit persons to whom the Software is furnished to do so,\n# subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n# check if python version is above 3.10\nimport sys\n\nif sys.version_info >= (3, 10):\n    print(\"Python version is above 3.10, patching the collections module.\")\n    # Monkey patch collections\n    import collections\n    import collections.abc\n\n    for type_name in collections.abc.__all__:\n        setattr(collections, type_name, getattr(collections.abc, type_name))\n"}
{"type": "source_file", "path": "three_party/Janus/demo/fastapi_client.py", "content": "import requests\nfrom PIL import Image\nimport io\n# Endpoint URLs\nunderstand_image_url = \"http://localhost:8000/understand_image_and_question/\"\ngenerate_images_url = \"http://localhost:8000/generate_images/\"\n\n# Use your image file path here\nimage_path = \"images/equation.png\"\n\n# Function to call the image understanding endpoint\ndef understand_image_and_question(image_path, question, seed=42, top_p=0.95, temperature=0.1):\n    files = {'file': open(image_path, 'rb')}\n    data = {\n        'question': question,\n        'seed': seed,\n        'top_p': top_p,\n        'temperature': temperature\n    }\n    response = requests.post(understand_image_url, files=files, data=data)\n    response_data = response.json()\n    print(\"Image Understanding Response:\", response_data['response'])\n\n\n# Function to call the text-to-image generation endpoint\ndef generate_images(prompt, seed=None, guidance=5.0):\n    data = {\n        'prompt': prompt,\n        'seed': seed,\n        'guidance': guidance\n    }\n    response = requests.post(generate_images_url, data=data, stream=True)\n    \n    if response.ok:\n        img_idx = 1\n\n        # We will create a new BytesIO for each image\n        buffers = {}\n\n        try:\n            for chunk in response.iter_content(chunk_size=1024):\n                if chunk:\n                    # Use a boundary detection to determine new image start\n                    if img_idx not in buffers:\n                        buffers[img_idx] = io.BytesIO()\n\n                    buffers[img_idx].write(chunk)\n\n                    # Attempt to open the image\n                    try:\n                        buffer = buffers[img_idx]\n                        buffer.seek(0)\n                        image = Image.open(buffer)\n                        img_path = f\"generated_image_{img_idx}.png\"\n                        image.save(img_path)\n                        print(f\"Saved: {img_path}\")\n\n                        # Prepare the next image buffer\n                        buffer.close()\n                        img_idx += 1\n\n                    except Exception as e:\n                        # Continue loading data into the current buffer\n                        continue\n\n        except Exception as e:\n            print(\"Error processing image:\", e)\n    else:\n        print(\"Failed to generate images.\")\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Call the image understanding API\n    understand_image_and_question(image_path, \"What is this image about?\")\n    \n    # Call the image generation API\n    generate_images(\"A beautiful sunset over a mountain range, digital art.\")\n"}
{"type": "source_file", "path": "three_party/Janus/demo/app.py", "content": "import gradio as gr\nimport torch\nfrom transformers import AutoConfig, AutoModelForCausalLM\nfrom janus.models import MultiModalityCausalLM, VLChatProcessor\nfrom PIL import Image\n\nimport numpy as np\n\n\n# Load model and processor\nmodel_path = \"deepseek-ai/Janus-1.3B\"\nconfig = AutoConfig.from_pretrained(model_path)\nlanguage_config = config.language_config\nlanguage_config._attn_implementation = 'eager'\nvl_gpt = AutoModelForCausalLM.from_pretrained(model_path,\n                                             language_config=language_config,\n                                             trust_remote_code=True)\nvl_gpt = vl_gpt.to(torch.bfloat16).cuda()\n\nvl_chat_processor = VLChatProcessor.from_pretrained(model_path)\ntokenizer = vl_chat_processor.tokenizer\ncuda_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n# Multimodal Understanding function\n@torch.inference_mode()\n# Multimodal Understanding function\ndef multimodal_understanding(image, question, seed, top_p, temperature):\n    # Clear CUDA cache before generating\n    torch.cuda.empty_cache()\n    \n    # set seed\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    torch.cuda.manual_seed(seed)\n    \n    conversation = [\n        {\n            \"role\": \"User\",\n            \"content\": f\"<image_placeholder>\\n{question}\",\n            \"images\": [image],\n        },\n        {\"role\": \"Assistant\", \"content\": \"\"},\n    ]\n    \n    pil_images = [Image.fromarray(image)]\n    prepare_inputs = vl_chat_processor(\n        conversations=conversation, images=pil_images, force_batchify=True\n    ).to(cuda_device, dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16)\n    \n    \n    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n    \n    outputs = vl_gpt.language_model.generate(\n        inputs_embeds=inputs_embeds,\n        attention_mask=prepare_inputs.attention_mask,\n        pad_token_id=tokenizer.eos_token_id,\n        bos_token_id=tokenizer.bos_token_id,\n        eos_token_id=tokenizer.eos_token_id,\n        max_new_tokens=512,\n        do_sample=False if temperature == 0 else True,\n        use_cache=True,\n        temperature=temperature,\n        top_p=top_p,\n    )\n    \n    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n    return answer\n\n\ndef generate(input_ids,\n             width,\n             height,\n             temperature: float = 1,\n             parallel_size: int = 5,\n             cfg_weight: float = 5,\n             image_token_num_per_image: int = 576,\n             patch_size: int = 16):\n    # Clear CUDA cache before generating\n    torch.cuda.empty_cache()\n    \n    tokens = torch.zeros((parallel_size * 2, len(input_ids)), dtype=torch.int).to(cuda_device)\n    for i in range(parallel_size * 2):\n        tokens[i, :] = input_ids\n        if i % 2 != 0:\n            tokens[i, 1:-1] = vl_chat_processor.pad_id\n    inputs_embeds = vl_gpt.language_model.get_input_embeddings()(tokens)\n    generated_tokens = torch.zeros((parallel_size, image_token_num_per_image), dtype=torch.int).to(cuda_device)\n\n    pkv = None\n    for i in range(image_token_num_per_image):\n        outputs = vl_gpt.language_model.model(inputs_embeds=inputs_embeds,\n                                             use_cache=True,\n                                             past_key_values=pkv)\n        pkv = outputs.past_key_values\n        hidden_states = outputs.last_hidden_state\n        logits = vl_gpt.gen_head(hidden_states[:, -1, :])\n        logit_cond = logits[0::2, :]\n        logit_uncond = logits[1::2, :]\n        logits = logit_uncond + cfg_weight * (logit_cond - logit_uncond)\n        probs = torch.softmax(logits / temperature, dim=-1)\n        next_token = torch.multinomial(probs, num_samples=1)\n        generated_tokens[:, i] = next_token.squeeze(dim=-1)\n        next_token = torch.cat([next_token.unsqueeze(dim=1), next_token.unsqueeze(dim=1)], dim=1).view(-1)\n        img_embeds = vl_gpt.prepare_gen_img_embeds(next_token)\n        inputs_embeds = img_embeds.unsqueeze(dim=1)\n    patches = vl_gpt.gen_vision_model.decode_code(generated_tokens.to(dtype=torch.int),\n                                                 shape=[parallel_size, 8, width // patch_size, height // patch_size])\n\n    return generated_tokens.to(dtype=torch.int), patches\n\ndef unpack(dec, width, height, parallel_size=5):\n    dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)\n    dec = np.clip((dec + 1) / 2 * 255, 0, 255)\n\n    visual_img = np.zeros((parallel_size, width, height, 3), dtype=np.uint8)\n    visual_img[:, :, :] = dec\n\n    return visual_img\n\n\n\n@torch.inference_mode()\ndef generate_image(prompt,\n                   seed=None,\n                   guidance=5):\n    # Clear CUDA cache and avoid tracking gradients\n    torch.cuda.empty_cache()\n    # Set the seed for reproducible results\n    if seed is not None:\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        np.random.seed(seed)\n    width = 384\n    height = 384\n    parallel_size = 5\n    \n    with torch.no_grad():\n        messages = [{'role': 'User', 'content': prompt},\n                    {'role': 'Assistant', 'content': ''}]\n        text = vl_chat_processor.apply_sft_template_for_multi_turn_prompts(conversations=messages,\n                                                                   sft_format=vl_chat_processor.sft_format,\n                                                                   system_prompt='')\n        text = text + vl_chat_processor.image_start_tag\n        input_ids = torch.LongTensor(tokenizer.encode(text))\n        output, patches = generate(input_ids,\n                                   width // 16 * 16,\n                                   height // 16 * 16,\n                                   cfg_weight=guidance,\n                                   parallel_size=parallel_size)\n        images = unpack(patches,\n                        width // 16 * 16,\n                        height // 16 * 16)\n\n        return [Image.fromarray(images[i]).resize((1024, 1024), Image.LANCZOS) for i in range(parallel_size)]\n\n        \n\n# Gradio interface\nwith gr.Blocks() as demo:\n    gr.Markdown(value=\"# Multimodal Understanding\")\n    # with gr.Row():\n    with gr.Row():\n        image_input = gr.Image()\n        with gr.Column():\n            question_input = gr.Textbox(label=\"Question\")\n            und_seed_input = gr.Number(label=\"Seed\", precision=0, value=42)\n            top_p = gr.Slider(minimum=0, maximum=1, value=0.95, step=0.05, label=\"top_p\")\n            temperature = gr.Slider(minimum=0, maximum=1, value=0.1, step=0.05, label=\"temperature\")\n        \n    understanding_button = gr.Button(\"Chat\")\n    understanding_output = gr.Textbox(label=\"Response\")\n\n    examples_inpainting = gr.Examples(\n        label=\"Multimodal Understanding examples\",\n        examples=[\n            [\n                \"explain this meme\",\n                \"images/doge.png\",\n            ],\n            [\n                \"Convert the formula into latex code.\",\n                \"images/equation.png\",\n            ],\n        ],\n        inputs=[question_input, image_input],\n    )\n    \n        \n    gr.Markdown(value=\"# Text-to-Image Generation\")\n\n    \n    \n    with gr.Row():\n        cfg_weight_input = gr.Slider(minimum=1, maximum=10, value=5, step=0.5, label=\"CFG Weight\")\n\n    prompt_input = gr.Textbox(label=\"Prompt\")\n    seed_input = gr.Number(label=\"Seed (Optional)\", precision=0, value=12345)\n\n    generation_button = gr.Button(\"Generate Images\")\n\n    image_output = gr.Gallery(label=\"Generated Images\", columns=2, rows=2, height=300)\n\n    examples_t2i = gr.Examples(\n        label=\"Text to image generation examples. (Tips for designing prompts: Adding description like 'digital art' at the end of the prompt or writing the prompt in more detail can help produce better images!)\",\n        examples=[\n            \"Master shifu racoon wearing drip attire as a street gangster.\",\n            \"A cute and adorable baby fox with big brown eyes, autumn leaves in the background enchanting,immortal,fluffy, shiny mane,Petals,fairyism,unreal engine 5 and Octane Render,highly detailed, photorealistic, cinematic, natural colors.\",\n            \"The image features an intricately designed eye set against a circular backdrop adorned with ornate swirl patterns that evoke both realism and surrealism. At the center of attention is a strikingly vivid blue iris surrounded by delicate veins radiating outward from the pupil to create depth and intensity. The eyelashes are long and dark, casting subtle shadows on the skin around them which appears smooth yet slightly textured as if aged or weathered over time.\\n\\nAbove the eye, there's a stone-like structure resembling part of classical architecture, adding layers of mystery and timeless elegance to the composition. This architectural element contrasts sharply but harmoniously with the organic curves surrounding it. Below the eye lies another decorative motif reminiscent of baroque artistry, further enhancing the overall sense of eternity encapsulated within each meticulously crafted detail. \\n\\nOverall, the atmosphere exudes a mysterious aura intertwined seamlessly with elements suggesting timelessness, achieved through the juxtaposition of realistic textures and surreal artistic flourishes. Each component\\u2014from the intricate designs framing the eye to the ancient-looking stone piece above\\u2014contributes uniquely towards creating a visually captivating tableau imbued with enigmatic allure.\",\n        ],\n        inputs=prompt_input,\n    )\n    \n    understanding_button.click(\n        multimodal_understanding,\n        inputs=[image_input, question_input, und_seed_input, top_p, temperature],\n        outputs=understanding_output\n    )\n    \n    generation_button.click(\n        fn=generate_image,\n        inputs=[prompt_input, seed_input, cfg_weight_input],\n        outputs=image_output\n    )\n\ndemo.launch(share=True)"}
{"type": "source_file", "path": "three_party/Janus/janus/janusflow/models/__init__.py", "content": "# Copyright (c) 2023-2024 DeepSeek.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the \"Software\"), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n# the Software, and to permit persons to whom the Software is furnished to do so,\n# subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom .image_processing_vlm import VLMImageProcessor\nfrom .modeling_vlm import MultiModalityCausalLM\nfrom .processing_vlm import VLChatProcessor\n\n__all__ = [\n    \"VLMImageProcessor\",\n    \"VLChatProcessor\",\n    \"MultiModalityCausalLM\",\n]\n"}
{"type": "source_file", "path": "three_party/Janus/janus/janusflow/models/modeling_vlm.py", "content": "# Copyright (c) 2023-2024 DeepSeek.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the \"Software\"), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n# the Software, and to permit persons to whom the Software is furnished to do so,\n# subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom attrdict import AttrDict\nfrom einops import rearrange\nimport torch\nfrom transformers.configuration_utils import PretrainedConfig\nfrom transformers import (\n    AutoConfig,\n    AutoModelForCausalLM,\n    PreTrainedModel,\n    LlamaConfig,\n    LlamaForCausalLM,\n)\nfrom transformers.models.llama.modeling_llama import LlamaRMSNorm\nfrom janus.janusflow.models.clip_encoder import CLIPVisionTower\nfrom janus.janusflow.models.uvit import ShallowUViTEncoder, ShallowUViTDecoder\nimport torch.nn as nn\n\n\ndef model_name_to_cls(cls_name):\n\n    if \"CLIPVisionTower\" in cls_name:\n        cls = CLIPVisionTower\n    elif \"ShallowUViTEncoder\" in cls_name:\n        cls = ShallowUViTEncoder\n    elif \"ShallowUViTDecoder\" in cls_name:\n        cls = ShallowUViTDecoder\n    else:\n        raise ValueError(f\"class_name {cls_name} is invalid.\")\n\n    return cls\n\n\nclass VisionUnderstandEncoderConfig(PretrainedConfig):\n    model_type = \"vision_und_enc\"\n    cls: str = \"\"\n    params: AttrDict = {}\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        self.cls = kwargs.get(\"cls\", \"\")\n        if not isinstance(self.cls, str):\n            self.cls = self.cls.__name__\n\n        self.params = AttrDict(kwargs.get(\"params\", {}))\n\n\nclass VisionGenerationEncoderConfig(PretrainedConfig):\n    model_type = \"vision_gen_enc\"\n    cls: str = \"\"\n    params: AttrDict = {}\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        self.cls = kwargs.get(\"cls\", \"\")\n        if not isinstance(self.cls, str):\n            self.cls = self.cls.__name__\n\n        self.params = AttrDict(kwargs.get(\"params\", {}))\n\n\nclass VisionGenerationDecoderConfig(PretrainedConfig):\n    model_type = \"vision_gen_dec\"\n    cls: str = \"\"\n    params: AttrDict = {}\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        self.cls = kwargs.get(\"cls\", \"\")\n        if not isinstance(self.cls, str):\n            self.cls = self.cls.__name__\n\n        self.params = AttrDict(kwargs.get(\"params\", {}))\n\n\nclass MultiModalityConfig(PretrainedConfig):\n    model_type = \"multi_modality\"\n    vision_und_enc_config: VisionUnderstandEncoderConfig\n    language_config: LlamaConfig\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        vision_und_enc_config = kwargs.get(\"vision_und_enc_config\", {})\n        self.vision_und_enc_config = VisionUnderstandEncoderConfig(\n            **vision_und_enc_config\n        )\n\n        vision_gen_enc_config = kwargs.get(\"vision_gen_enc_config\", {})\n        self.vision_gen_enc_config = VisionGenerationEncoderConfig(\n            **vision_gen_enc_config\n        )\n\n        vision_gen_dec_config = kwargs.get(\"vision_gen_dec_config\", {})\n        self.vision_gen_dec_config = VisionGenerationDecoderConfig(\n            **vision_gen_dec_config\n        )\n\n        language_config = kwargs.get(\"language_config\", {})\n        if isinstance(language_config, LlamaConfig):\n            self.language_config = language_config\n        else:\n            self.language_config = LlamaConfig(**language_config)\n\n\nclass MultiModalityPreTrainedModel(PreTrainedModel):\n    config_class = MultiModalityConfig\n    base_model_prefix = \"multi_modality\"\n    _no_split_modules = []\n    _skip_keys_device_placement = \"past_key_values\"\n\n\nclass MultiModalityCausalLM(MultiModalityPreTrainedModel):\n\n    def __init__(self, config: MultiModalityConfig):\n        super().__init__(config)\n\n        # vision understanding encoder\n        vision_und_enc_config = config.vision_und_enc_config\n        vision_und_enc_cls = model_name_to_cls(vision_und_enc_config.cls)\n        self.vision_und_enc_model = vision_und_enc_cls(**vision_und_enc_config.params)\n\n        # vision understanding aligner\n        self.vision_und_enc_aligner = nn.Linear(1024, 2048, bias=True)\n\n        # begin of understanding embedding\n        self.beg_of_und_embed = nn.Parameter(torch.zeros(1, 2048))\n\n        # vision generation encoder\n        vision_gen_enc_config = config.vision_gen_enc_config\n        vision_gen_enc_cls = model_name_to_cls(vision_gen_enc_config.cls)\n        self.vision_gen_enc_model = vision_gen_enc_cls(**vision_gen_enc_config.params)\n\n        # vision generation encoder aligner\n        self.vision_gen_enc_aligner = nn.Linear(768, 2048, bias=True)\n\n        # vision generation decoder\n        vision_gen_dec_config = config.vision_gen_dec_config\n        vision_gen_dec_cls = model_name_to_cls(vision_gen_dec_config.cls)\n        self.vision_gen_dec_model = vision_gen_dec_cls(**vision_gen_dec_config.params)\n\n        # language model\n        language_config = config.language_config\n        self.language_model = LlamaForCausalLM(language_config)\n\n        # vision generation decoder aligner\n        self.vision_gen_dec_aligner_norm = LlamaRMSNorm(\n            2048, eps=language_config.rms_norm_eps\n        )\n        self.vision_gen_dec_aligner = nn.Linear(2048, 768, bias=True)\n\n    def prepare_inputs_embeds(\n        self,\n        input_ids: torch.LongTensor,\n        pixel_values: torch.FloatTensor,\n        images_seq_mask: torch.LongTensor,\n        images_emb_mask: torch.LongTensor,\n        **kwargs,\n    ):\n        \"\"\"\n\n        Args:\n            input_ids (torch.LongTensor): [b, T]\n            pixel_values (torch.FloatTensor):   [b, n_images, 3, h, w]\n            images_seq_mask (torch.BoolTensor): [b, T]\n            images_emb_mask (torch.BoolTensor): [b, n_images, n_image_tokens]\n\n            assert torch.sum(images_seq_mask) == torch.sum(images_emb_mask)\n\n        Returns:\n            input_embeds (torch.Tensor): [b, T, D]\n        \"\"\"\n\n        bs, n = pixel_values.shape[0:2]\n        images = rearrange(pixel_values, \"b n c h w -> (b n) c h w\")\n        # [b x n, T2, D]\n        images_embeds = self.vision_und_enc_model(images)\n        images_embeds = self.vision_und_enc_aligner(images_embeds)\n        # print(images_embeds.shape, self.beg_of_und_embed.shape, images_seq_mask.shape, input_ids.shape)\n        beg_of_und_embed = self.beg_of_und_embed[0].detach().clone()\n        images_embeds = torch.cat(\n            [\n                beg_of_und_embed.view(1, 1, -1).repeat(images_embeds.shape[0], 1, 1),\n                images_embeds,\n            ],\n            dim=1,\n        )\n        # [b x n, T2, D] -> [b, n x T2, D]\n        images_embeds = rearrange(images_embeds, \"(b n) t d -> b (n t) d\", b=bs, n=n)\n        # [b, n, T2] -> [b, n x T2]\n        images_emb_mask = rearrange(images_emb_mask, \"b n t -> b (n t)\")\n\n        # [b, T, D]\n        input_ids[input_ids < 0] = 0  # ignore the image embeddings\n        inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n\n        # replace with the image embeddings\n        inputs_embeds[images_seq_mask] = images_embeds[images_emb_mask]\n\n        return inputs_embeds\n\n\nAutoConfig.register(\"vision_und_enc\", VisionUnderstandEncoderConfig)\nAutoConfig.register(\"vision_gen_enc\", VisionGenerationEncoderConfig)\nAutoConfig.register(\"vision_gen_dec\", VisionGenerationDecoderConfig)\nAutoConfig.register(\"multi_modality\", MultiModalityConfig)\nAutoModelForCausalLM.register(MultiModalityConfig, MultiModalityCausalLM)\n"}
{"type": "source_file", "path": "project/plangen/dataset/sam/sam_traindata.py", "content": "from torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport ast\nimport numpy as np\nfrom src.utils.funcs import *\nfrom ..coco.data_coco import filter_box, resize_and_crop\n\ndef adjust_and_normalize_bboxes(bboxes, orig_width, orig_height):\n    normalized_bboxes = []\n    for bbox in bboxes:\n        x1, y1, x2, y2 = bbox\n        x1_norm = round(x1 / orig_width,3)  \n        y1_norm = round(y1 / orig_height,3)\n        x2_norm = round(x2 / orig_width,3)\n        y2_norm = round(y2 / orig_height,3)\n        normalized_bboxes.append([x1_norm, y1_norm, x2_norm, y2_norm])\n    \n    return normalized_bboxes\n    \nclass BboxDataset_sam(Dataset):\n    def __init__(self, dataset, resolution=1024, is_testset=False,):\n        self.is_testset = is_testset\n        self.dataset = dataset\n        self.resolution = resolution\n        if self.is_testset:\n            self.transform = transforms.Compose([\n                transforms.Resize(\n                    (resolution,resolution), interpolation=transforms.InterpolationMode.BILINEAR \n                ),\n                transforms.ToTensor(),\n                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n            ])\n        else:\n            self.transform = transforms.Compose([\n                transforms.ToTensor(),\n            ])\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def update_item(self, item):\n        #dict_keys(['bbox_info', 'global_caption', 'image_info'])\n        # item.update(image=x)\n        dirname, filename = item['image_path'][3:].split('/')\n        image_path = osp.join('/home/jovyan/myh-data-ceph-shcdt-1/data/SAM/', str(int(dirname)), filename)\n        image = Image.open(image_path).convert('RGB')\n        bbox_info = item['metadata']['bbox_info']\n        global_caption = item['metadata']['global_caption']\n        image_info = item['metadata']['image_info']\n\n        height = image_info['height']\n        width = image_info['width']\n        file_name = image_info['file_name']\n\n        bbox_list = []\n        region_captions = []\n        detail_region_captions = []\n        for box in bbox_info:\n            bbox_list.append(box['bbox'])\n            region_captions.append(box['description'])\n            detail_region_captions.append(box['detail_description'])\n\n        item.update(dict(\n            global_caption=global_caption,\n            image=image,\n            height=height,\n            width=width,\n            file_name=file_name,\n            bbox_list=str(bbox_list),\n            region_captions=str(region_captions),\n            detail_region_captions=str(detail_region_captions),\n        ))\n\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n\n        if self.is_testset:\n            pass\n        else:\n            self.update_item(item)\n\n        image = item['image']\n        image = self.transform(image)\n        # import pdb;pdb.set_trace()\n\n        height = int(item['height'])\n        width = int(item['width'])\n        global_caption = item['global_caption']\n        region_bboxes_list = item['bbox_list']\n        detail_region_caption_list = item['detail_region_captions']\n        region_caption_list = item['region_captions']\n        file_name = item['file_name']\n\n        region_bboxes_list = ast.literal_eval(region_bboxes_list)\n        region_bboxes_list = adjust_and_normalize_bboxes(region_bboxes_list,width,height)\n        region_bboxes_list = np.array(region_bboxes_list, dtype=np.float32)\n\n        region_caption_list = ast.literal_eval(region_caption_list)\n        detail_region_caption_list = ast.literal_eval(detail_region_caption_list)\n\n        if self.is_testset:\n            pass\n        else:\n            image_pil = to_pil(image)\n            obj_bbox = region_bboxes_list * [width, height, width, height]\n            obj_class = detail_region_caption_list\n\n            obj_bbox[:,2] = obj_bbox[:,2] - obj_bbox[:,0]\n            obj_bbox[:,3] = obj_bbox[:,3] - obj_bbox[:,1]\n            image_pil, obj_bbox = resize_and_crop(image_pil, obj_bbox)\n            image =  to_ts(image_pil)\n            image = image*2-1\n            obj_bbox, obj_class = filter_box(obj_bbox, obj_class)\n            obj_bbox = obj_bbox/384\n            obj_bbox = obj_bbox.reshape(-1,4)\n            obj_bbox[:,2] = obj_bbox[:,0] + obj_bbox[:,2]\n            obj_bbox[:,3] = obj_bbox[:,1] + obj_bbox[:,3]\n            \n            region_bboxes_list = obj_bbox\n            detail_region_caption_list = obj_class\n        \n        if None in detail_region_caption_list:\n            detail_region_caption_list = region_caption_list\n        if None in region_caption_list:\n            return self.__getitem__(self, idx+1)\n\n        return {\n            'image': image,\n            'global_caption': global_caption,\n            'detail_region_caption_list': detail_region_caption_list,\n            'region_bboxes_list': region_bboxes_list,\n            'region_caption_list': region_caption_list,\n            'file_name': file_name,\n            'height': height,\n            'width': width\n        }\n\n"}
{"type": "source_file", "path": "three_party/Janus/janus/models/projector.py", "content": "# Copyright (c) 2023-2024 DeepSeek.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the \"Software\"), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n# the Software, and to permit persons to whom the Software is furnished to do so,\n# subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom typing import Tuple, Union\n\nimport torch\nimport torch.nn as nn\nfrom attrdict import AttrDict\n\n\nclass MlpProjector(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n\n        self.cfg = cfg\n\n        if cfg.projector_type == \"identity\":\n            modules = nn.Identity()\n\n        elif cfg.projector_type == \"linear\":\n            modules = nn.Linear(cfg.input_dim, cfg.n_embed)\n\n        elif cfg.projector_type == \"mlp_gelu\":\n            mlp_depth = cfg.get(\"depth\", 1)\n            modules = [nn.Linear(cfg.input_dim, cfg.n_embed)]\n            for _ in range(1, mlp_depth):\n                modules.append(nn.GELU())\n                modules.append(nn.Linear(cfg.n_embed, cfg.n_embed))\n            modules = nn.Sequential(*modules)\n\n        elif cfg.projector_type == \"low_high_hybrid_split_mlp_gelu\":\n            mlp_depth = cfg.get(\"depth\", 1)\n            self.high_up_proj = nn.Linear(cfg.input_dim, cfg.n_embed // 2)\n            self.low_up_proj = nn.Linear(cfg.input_dim, cfg.n_embed // 2)\n\n            modules = []\n            for _ in range(1, mlp_depth):\n                modules.append(nn.GELU())\n                modules.append(nn.Linear(cfg.n_embed, cfg.n_embed))\n            modules = nn.Sequential(*modules)\n\n        else:\n            raise ValueError(f\"Unknown projector type: {cfg.projector_type}\")\n\n        self.layers = modules\n\n    def forward(\n        self, x_or_tuple: Union[Tuple[torch.Tensor, torch.Tensor], torch.Tensor]\n    ):\n        \"\"\"\n\n        Args:\n            x_or_tuple (Union[Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:  if it is a tuple of torch.Tensor,\n                then it comes from the hybrid vision encoder, and x = high_res_x, low_res_x);\n                otherwise it is the feature from the single vision encoder.\n\n        Returns:\n            x (torch.Tensor): [b, s, c]\n        \"\"\"\n\n        if isinstance(x_or_tuple, tuple):\n            # self.cfg.projector_type == \"low_high_hybrid_split_mlp_gelu\":\n            high_x, low_x = x_or_tuple\n            high_x = self.high_up_proj(high_x)\n            low_x = self.low_up_proj(low_x)\n            x = torch.concat([high_x, low_x], dim=-1)\n        else:\n            x = x_or_tuple\n\n        return self.layers(x)\n\n\nif __name__ == \"__main__\":\n    cfg = AttrDict(\n        input_dim=1024,\n        n_embed=2048,\n        depth=2,\n        projector_type=\"low_high_hybrid_split_mlp_gelu\",\n    )\n    inputs = (torch.rand(4, 576, 1024), torch.rand(4, 576, 1024))\n\n    m = MlpProjector(cfg)\n    out = m(inputs)\n    print(out.shape)\n"}
{"type": "source_file", "path": "three_party/Janus/janus/models/modeling_vlm copy.py", "content": "# Copyright (c) 2023-2024 DeepSeek.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the \"Software\"), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n# the Software, and to permit persons to whom the Software is furnished to do so,\n# subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport torch\nfrom attrdict import AttrDict\nfrom einops import rearrange\nfrom transformers import (\n    AutoConfig,\n    AutoModelForCausalLM,\n    LlamaConfig,\n    LlamaForCausalLM,\n    PreTrainedModel,\n)\nfrom transformers.configuration_utils import PretrainedConfig\n\nfrom janus.models.clip_encoder import CLIPVisionTower\nfrom janus.models.projector import MlpProjector\n\n\nclass vision_head(torch.nn.Module):\n    def __init__(self, params):\n        super().__init__()\n        self.output_mlp_projector = torch.nn.Linear(\n            params.n_embed, params.image_token_embed\n        )\n        self.vision_activation = torch.nn.GELU()\n        self.vision_head = torch.nn.Linear(\n            params.image_token_embed, params.image_token_size\n        )\n\n    def forward(self, x):\n        x = self.output_mlp_projector(x)\n        x = self.vision_activation(x)\n        x = self.vision_head(x)\n        return x\n\n\ndef model_name_to_cls(cls_name):\n    if \"MlpProjector\" in cls_name:\n        cls = MlpProjector\n\n    elif \"CLIPVisionTower\" in cls_name:\n        cls = CLIPVisionTower\n\n    elif \"VQ\" in cls_name:\n        from janus.models.vq_model import VQ_models\n\n        cls = VQ_models[cls_name]\n    elif \"vision_head\" in cls_name:\n        cls = vision_head\n    else:\n        raise ValueError(f\"class_name {cls_name} is invalid.\")\n\n    return cls\n\n\nclass VisionConfig(PretrainedConfig):\n    model_type = \"vision\"\n    cls: str = \"\"\n    params: AttrDict = {}\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        self.cls = kwargs.get(\"cls\", \"\")\n        if not isinstance(self.cls, str):\n            self.cls = self.cls.__name__\n\n        self.params = AttrDict(kwargs.get(\"params\", {}))\n\n\nclass AlignerConfig(PretrainedConfig):\n    model_type = \"aligner\"\n    cls: str = \"\"\n    params: AttrDict = {}\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        self.cls = kwargs.get(\"cls\", \"\")\n        if not isinstance(self.cls, str):\n            self.cls = self.cls.__name__\n\n        self.params = AttrDict(kwargs.get(\"params\", {}))\n\n\nclass GenVisionConfig(PretrainedConfig):\n    model_type = \"gen_vision\"\n    cls: str = \"\"\n    params: AttrDict = {}\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        self.cls = kwargs.get(\"cls\", \"\")\n        if not isinstance(self.cls, str):\n            self.cls = self.cls.__name__\n\n        self.params = AttrDict(kwargs.get(\"params\", {}))\n\n\nclass GenAlignerConfig(PretrainedConfig):\n    model_type = \"gen_aligner\"\n    cls: str = \"\"\n    params: AttrDict = {}\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        self.cls = kwargs.get(\"cls\", \"\")\n        if not isinstance(self.cls, str):\n            self.cls = self.cls.__name__\n\n        self.params = AttrDict(kwargs.get(\"params\", {}))\n\n\nclass GenHeadConfig(PretrainedConfig):\n    model_type = \"gen_head\"\n    cls: str = \"\"\n    params: AttrDict = {}\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        self.cls = kwargs.get(\"cls\", \"\")\n        if not isinstance(self.cls, str):\n            self.cls = self.cls.__name__\n\n        self.params = AttrDict(kwargs.get(\"params\", {}))\n\n\nclass MultiModalityConfig(PretrainedConfig):\n    model_type = \"multi_modality\"\n    vision_config: VisionConfig\n    aligner_config: AlignerConfig\n\n    gen_vision_config: GenVisionConfig\n    gen_aligner_config: GenAlignerConfig\n    gen_head_config: GenHeadConfig\n\n    language_config: LlamaConfig\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        vision_config = kwargs.get(\"vision_config\", {})\n        self.vision_config = VisionConfig(**vision_config)\n\n        aligner_config = kwargs.get(\"aligner_config\", {})\n        self.aligner_config = AlignerConfig(**aligner_config)\n\n        gen_vision_config = kwargs.get(\"gen_vision_config\", {})\n        self.gen_vision_config = GenVisionConfig(**gen_vision_config)\n\n        gen_aligner_config = kwargs.get(\"gen_aligner_config\", {})\n        self.gen_aligner_config = GenAlignerConfig(**gen_aligner_config)\n\n        gen_head_config = kwargs.get(\"gen_head_config\", {})\n        self.gen_head_config = GenHeadConfig(**gen_head_config)\n\n        language_config = kwargs.get(\"language_config\", {})\n        if isinstance(language_config, LlamaConfig):\n            self.language_config = language_config\n        else:\n            self.language_config = LlamaConfig(**language_config)\n\n\nclass MultiModalityPreTrainedModel(PreTrainedModel):\n    config_class = MultiModalityConfig\n    base_model_prefix = \"multi_modality\"\n    _no_split_modules = []\n    _skip_keys_device_placement = \"past_key_values\"\n\n\nclass MultiModalityCausalLM(MultiModalityPreTrainedModel):\n    def __init__(self, config: MultiModalityConfig):\n        super().__init__(config)\n\n        vision_config = config.vision_config\n        vision_cls = model_name_to_cls(vision_config.cls)\n        self.vision_model = vision_cls(**vision_config.params)\n\n        aligner_config = config.aligner_config\n        aligner_cls = model_name_to_cls(aligner_config.cls)\n        self.aligner = aligner_cls(aligner_config.params)\n\n        gen_vision_config = config.gen_vision_config\n        gen_vision_cls = model_name_to_cls(gen_vision_config.cls)\n        self.gen_vision_model = gen_vision_cls()\n        # janus.models.vq_model.VQModel\n\n        gen_aligner_config = config.gen_aligner_config\n        gen_aligner_cls = model_name_to_cls(gen_aligner_config.cls)\n        self.gen_aligner = gen_aligner_cls(gen_aligner_config.params)\n\n        gen_head_config = config.gen_head_config\n        gen_head_cls = model_name_to_cls(gen_head_config.cls)\n        self.gen_head = gen_head_cls(gen_head_config.params)\n\n        self.gen_embed = torch.nn.Embedding(\n            gen_vision_config.params.image_token_size, gen_vision_config.params.n_embed\n        )\n\n        language_config = config.language_config\n        self.language_model = LlamaForCausalLM(language_config)\n\n    def prepare_inputs_embeds(\n        self,\n        input_ids: torch.LongTensor,\n        pixel_values: torch.FloatTensor,\n        images_seq_mask: torch.LongTensor,\n        images_emb_mask: torch.LongTensor,\n        **kwargs,\n    ):\n        \"\"\"\n\n        Args:\n            input_ids (torch.LongTensor): [b, T]\n            pixel_values (torch.FloatTensor):   [b, n_images, 3, h, w]\n            images_seq_mask (torch.BoolTensor): [b, T]\n            images_emb_mask (torch.BoolTensor): [b, n_images, n_image_tokens]\n\n            assert torch.sum(images_seq_mask) == torch.sum(images_emb_mask)\n\n        Returns:\n            input_embeds (torch.Tensor): [b, T, D]\n        \"\"\"\n\n        bs, n = pixel_values.shape[0:2]\n        images = rearrange(pixel_values, \"b n c h w -> (b n) c h w\")\n        # [b x n, T2, D]\n\n        device = next(self.vision_model.parameters()).device\n        dtype = next(self.vision_model.parameters()).dtype\n        # import pdb;pdb.set_trace()\n\n        # with torch.no_grad():\n            # self.vision_model.bfloat16().eval()\n            # aa = self.vision_model(images.to(torch.bfloat16).to(device))\n        aa = self.vision_model(images.bfloat16().to(device))\n        images_embeds = self.aligner(aa)\n\n        # [b x n, T2, D] -> [b, n x T2, D]\n        images_embeds = rearrange(images_embeds, \"(b n) t d -> b (n t) d\", b=bs, n=n)\n        # [b, n, T2] -> [b, n x T2]\n        images_emb_mask = rearrange(images_emb_mask, \"b n t -> b (n t)\")\n\n        # [b, T, D]\n        input_ids[input_ids < 0] = 0  # ignore the image embeddings\n        inputs_embeds = self.language_model.get_input_embeddings()(input_ids.to(device))\n        # inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n\n        # replace with the image embeddings\n        inputs_embeds[images_seq_mask] = images_embeds[images_emb_mask].to(inputs_embeds)\n        # 因为混合精度可能后者是bfloat16\n\n        return inputs_embeds\n\n    def prepare_gen_img_embeds(self, image_ids: torch.LongTensor):\n        # import pdb;pdb.set_trace()\n        # torch.Size([bs=8])\n        # (Pdb) self.gen_embed(image_ids).shape\n        # torch.Size([8, chs=8])\n        # (Pdb) self.gen_aligner(self.gen_embed(image_ids)).shape\n        # torch.Size([8, 2048])\n        return self.gen_aligner(self.gen_embed(image_ids))\n        # (Pdb) p self.gen_embed\n        # Embedding(16384, 8) #每个emb都是8d的code，再进行编码\n        # (Pdb) p self.gen_aligner\n        # MlpProjector(\n        # (layers): Sequential(\n        #     (0): Linear(in_features=8, out_features=2048, bias=True)\n        #     (1): GELU(approximate='none')\n        #     (2): Linear(in_features=2048, out_features=2048, bias=True)\n        # )\n        # )\n\n\nAutoConfig.register(\"vision\", VisionConfig)\nAutoConfig.register(\"aligner\", AlignerConfig)\nAutoConfig.register(\"gen_vision\", GenVisionConfig)\nAutoConfig.register(\"gen_aligner\", GenAlignerConfig)\nAutoConfig.register(\"gen_head\", GenHeadConfig)\nAutoConfig.register(\"multi_modality\", MultiModalityConfig)\nAutoModelForCausalLM.register(MultiModalityConfig, MultiModalityCausalLM)\n"}
{"type": "source_file", "path": "three_party/Janus/janus/models/processing_vlm copy.py", "content": "# Copyright (c) 2023-2024 DeepSeek.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the \"Software\"), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n# the Software, and to permit persons to whom the Software is furnished to do so,\n# subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom dataclasses import dataclass\nfrom typing import Dict, List\n\nimport torch\nfrom PIL.Image import Image\nfrom transformers import LlamaTokenizerFast\nfrom transformers.processing_utils import ProcessorMixin\n\nfrom janus.models.image_processing_vlm import VLMImageProcessor\nfrom janus.utils.conversation import get_conv_template\n\n\nclass DictOutput(object):\n    def keys(self):\n        return self.__dict__.keys()\n\n    def __getitem__(self, item):\n        return self.__dict__[item]\n\n    def __setitem__(self, key, value):\n        self.__dict__[key] = value\n\n\n@dataclass\nclass VLChatProcessorOutput(DictOutput):\n    sft_format: str\n    input_ids: torch.Tensor\n    pixel_values: torch.Tensor\n    num_image_tokens: torch.IntTensor\n\n    def __len__(self):\n        return len(self.input_ids)\n\n\n@dataclass\nclass BatchedVLChatProcessorOutput(DictOutput):\n    sft_format: List[str]\n    input_ids: torch.Tensor\n    pixel_values: torch.Tensor\n    attention_mask: torch.Tensor\n    images_seq_mask: torch.BoolTensor\n    images_emb_mask: torch.BoolTensor\n\n    def to(self, device, dtype=torch.bfloat16):\n        self.input_ids = self.input_ids.to(device)\n        self.attention_mask = self.attention_mask.to(device)\n        self.images_seq_mask = self.images_seq_mask.to(device)\n        self.images_emb_mask = self.images_emb_mask.to(device)\n        self.pixel_values = self.pixel_values.to(device=device, dtype=dtype)\n        return self\n\n\nclass VLChatProcessor(ProcessorMixin):\n    image_processor_class = \"AutoImageProcessor\"\n    tokenizer_class = (\"LlamaTokenizer\", \"LlamaTokenizerFast\")\n\n    attributes = [\"image_processor\", \"tokenizer\"]\n\n    system_prompt = (\n        \"You are a helpful language and vision assistant. \"\n        \"You are able to understand the visual content that the user provides, \"\n        \"and assist the user with a variety of tasks using natural language.\"\n    )\n\n    def __init__(\n        self,\n        image_processor: VLMImageProcessor,\n        tokenizer: LlamaTokenizerFast,\n        image_tag: str = \"<image_placeholder>\",\n        image_start_tag: str = \"<begin_of_image>\",\n        image_end_tag: str = \"<end_of_image>\",\n        pad_tag: str = \"<｜▁pad▁｜>\",\n        num_image_tokens: int = 576,\n        add_special_token: bool = False,\n        sft_format: str = \"deepseek\",\n        mask_prompt: bool = True,\n        ignore_id: int = -100,\n        **kwargs,\n    ):\n        self.image_processor = image_processor\n        self.tokenizer = tokenizer\n\n        image_id = self.tokenizer.vocab.get(image_tag)\n        if image_id is None:\n            special_tokens = [image_tag]\n            special_tokens_dict = {\"additional_special_tokens\": special_tokens}\n            self.tokenizer.add_special_tokens(special_tokens_dict)\n            print(f\"Add image tag = {image_tag} to the tokenizer\")\n\n        self.image_tag = image_tag\n        self.image_start_tag = image_start_tag\n        self.image_end_tag = image_end_tag\n        self.pad_tag = pad_tag\n\n        self.num_image_tokens = num_image_tokens\n        self.add_special_token = add_special_token\n        self.sft_format = sft_format\n        self.mask_prompt = mask_prompt\n        self.ignore_id = ignore_id\n\n        super().__init__(\n            image_processor,\n            tokenizer,\n            image_tag,\n            num_image_tokens,\n            add_special_token,\n            sft_format,\n            mask_prompt,\n            ignore_id,\n            **kwargs,\n        )\n\n    def new_chat_template(self):\n        conv = get_conv_template(self.sft_format)\n        conv.set_system_message(self.system_prompt)\n        return conv\n\n    def apply_sft_template_for_multi_turn_prompts(\n        self,\n        conversations: List[Dict[str, str]],\n        sft_format: str = \"deepseek\",\n        system_prompt: str = \"\",\n    ):\n        \"\"\"\n        Applies the SFT template to conversation.\n\n        An example of conversation:\n        conversation = [\n            {\n                \"role\": \"User\",\n                \"content\": \"<image_placeholder> is Figure 1.\\n<image_placeholder> is Figure 2.\\nWhich image is brighter?\",\n                \"images\": [\n                    \"./multi-images/attribute_comparison_1.png\",\n                    \"./multi-images/attribute_comparison_2.png\"\n                ]\n            },\n            {\n                \"role\": \"Assistant\",\n                \"content\": \"\"\n            }\n        ]\n\n        Args:\n            conversations (List[Dict]): A conversation with a List of Dict[str, str] text.\n            sft_format (str, optional): The format of the SFT template to use. Defaults to \"deepseek\".\n            system_prompt (str, optional): The system prompt to use in the SFT template. Defaults to \"\".\n\n        Returns:\n            sft_prompt (str): The formatted text.\n        \"\"\"\n\n        conv = get_conv_template(sft_format)\n        conv.set_system_message(system_prompt)\n        for message in conversations:\n            conv.append_message(message[\"role\"], message[\"content\"].strip())\n        sft_prompt = conv.get_prompt().strip()\n\n        return sft_prompt\n\n    @property\n    def image_token(self):\n        return self.image_tag\n\n    @property\n    def image_id(self):\n        image_id = self.tokenizer.vocab.get(self.image_tag)\n        return image_id\n\n    @property\n    def image_start_id(self):\n        image_start_id = self.tokenizer.vocab.get(self.image_start_tag)\n        return image_start_id\n\n    @property\n    def image_end_id(self):\n        image_end_id = self.tokenizer.vocab.get(self.image_end_tag)\n        return image_end_id\n\n    @property\n    def image_start_token(self):\n        return self.image_start_tag\n\n    @property\n    def image_end_token(self):\n        return self.image_end_tag\n\n    @property\n    def pad_id(self):\n        pad_id = self.tokenizer.vocab.get(self.pad_tag)\n        # pad_id = self.tokenizer.pad_token_id\n        # if pad_id is None:\n        #     pad_id = self.tokenizer.eos_token_id\n\n        return pad_id\n\n    def add_image_token(\n        self,\n        image_indices: List[int],\n        input_ids: torch.LongTensor,\n    ):\n        \"\"\"\n\n        Args:\n            image_indices (List[int]): [index_0, index_1, ..., index_j]\n            input_ids (torch.LongTensor): [N]\n\n        Returns:\n            input_ids (torch.LongTensor): [N + image tokens]\n            num_image_tokens (torch.IntTensor): [n_images]\n        \"\"\"\n\n        input_slices = []\n        device = input_ids.device\n\n        start = 0\n        for index in image_indices:\n            if self.add_special_token:\n                end = index + 1\n            else:\n                end = index\n\n            # original text tokens\n            input_slices.append(input_ids[start:end])\n\n            # add boi, image tokens, eoi and set the mask as False\n            input_slices.append(self.image_start_id * torch.ones((1), dtype=torch.long, device=device))\n            input_slices.append(\n                self.image_id * torch.ones((self.num_image_tokens,), dtype=torch.long, device=device)\n            )\n            input_slices.append(self.image_end_id * torch.ones((1), dtype=torch.long, device=device))\n            start = index + 1\n\n        # the left part\n        input_slices.append(input_ids[start:])\n\n        # concat all slices\n        # import pdb;pdb.set_trace()\n        # input_slices = [t.to(device) for t in input_slices]###\n        input_ids = torch.cat(input_slices, dim=0)\n        num_image_tokens = torch.IntTensor([self.num_image_tokens] * len(image_indices)).to(device)\n\n        return input_ids, num_image_tokens\n\n    def process_one(\n        self,\n        prompt: str = None,\n        conversations: List[Dict[str, str]] = None,\n        images: List[Image] = None,\n        **kwargs,\n    ):\n        \"\"\"\n\n        Args:\n            prompt (str): the formatted prompt;\n            conversations (List[Dict]): conversations with a list of messages;\n            images (List[ImageType]): the list of images;\n            **kwargs:\n\n        Returns:\n            outputs (BaseProcessorOutput): the output of the processor,\n                - input_ids (torch.LongTensor): [N + image tokens]\n                - target_ids (torch.LongTensor): [N + image tokens]\n                - images (torch.FloatTensor): [n_images, 3, H, W]\n                - image_id (int): the id of the image token\n                - num_image_tokens (List[int]): the number of image tokens\n        \"\"\"\n\n        assert (\n            prompt is None or conversations is None\n        ), \"prompt and conversations cannot be used at the same time.\"\n\n        if prompt is None:\n            # apply sft format\n            sft_format = self.apply_sft_template_for_multi_turn_prompts(\n                conversations=conversations,\n                sft_format=self.sft_format,\n                system_prompt=self.system_prompt,\n            )\n        else:\n            sft_format = prompt\n\n        from time import time\n\n        t1 = time()\n\n        # tokenize\n        input_ids = self.tokenizer.encode(sft_format)\n        input_ids = torch.LongTensor(input_ids).cuda()\n        # input_ids = torch.LongTensor(input_ids)\n\n        t2 = time()\n        print(f\"t2-t1: {t2-t1:.2f}\") #0.02s\n\n        # add image tokens to the input_ids\n        image_token_mask: torch.BoolTensor = input_ids == self.image_id\n        image_indices = image_token_mask.nonzero()\n        input_ids, num_image_tokens = self.add_image_token(\n            image_indices=image_indices,\n            input_ids=input_ids,\n        )\n\n        t3 = time()\n        print(f\"t3-t2: {t3-t2:.2f}\") # 0.19s\n\n        # load images\n        images_outputs = self.image_processor(images, return_tensors=\"pt\")\n\n        # import pdb;pdb.set_trace()\n\n        # t4 = time()\n        # print(f\"t4-t3: {t4-t3:.2f}\")\n        # import pdb;pdb.set_trace() # 0.00s\n\n        prepare = VLChatProcessorOutput(\n            sft_format=sft_format,\n            input_ids=input_ids,\n            pixel_values=images_outputs.pixel_values,\n            num_image_tokens=num_image_tokens,\n        )\n\n        return prepare\n\n    def __call__(\n        self,\n        *,\n        prompt: str = None,\n        conversations: List[Dict[str, str]] = None,\n        images: List[Image] = None,\n        force_batchify: bool = True,\n        **kwargs,\n    ):\n        \"\"\"\n\n        Args:\n            prompt (str): the formatted prompt;\n            conversations (List[Dict]): conversations with a list of messages;\n            images (List[ImageType]): the list of images;\n            force_batchify (bool): force batchify the inputs;\n            **kwargs:\n\n        Returns:\n            outputs (BaseProcessorOutput): the output of the processor,\n                - input_ids (torch.LongTensor): [N + image tokens]\n                - images (torch.FloatTensor): [n_images, 3, H, W]\n                - image_id (int): the id of the image token\n                - num_image_tokens (List[int]): the number of image tokens\n        \"\"\"\n\n        prepare = self.process_one(\n            prompt=prompt, conversations=conversations, images=images\n        )\n\n        if force_batchify:\n            prepare = self.batchify([prepare])\n\n        return prepare\n\n    def batchify(\n        self, prepare_list: List[VLChatProcessorOutput]\n    ) -> BatchedVLChatProcessorOutput:\n        \"\"\"\n        Preprocesses the inputs for multimodal inference.\n\n        Args:\n            prepare_list (List[VLChatProcessorOutput]): A list of VLChatProcessorOutput.\n\n        Returns:\n            BatchedVLChatProcessorOutput: A dictionary of the inputs to use for multimodal inference.\n        \"\"\"\n\n        batch_size = len(prepare_list)\n        sft_format = []\n        n_images = []\n        seq_lens = []\n        for prepare in prepare_list:\n            n_images.append(len(prepare.num_image_tokens))\n            seq_lens.append(len(prepare))\n\n        input_token_max_len = max(seq_lens)\n        max_n_images = max(1, max(n_images))\n\n        device = prepare_list[0].input_ids.device\n\n        batched_input_ids = torch.full(\n            (batch_size, input_token_max_len), self.pad_id\n        ).long().to(device)  # FIXME\n        batched_attention_mask = torch.zeros((batch_size, input_token_max_len)).long().to(device)\n        batched_pixel_values = torch.zeros(\n            (batch_size, max_n_images, *self.image_processor.default_shape)\n        ).float().to(device)####\n        batched_images_seq_mask = torch.zeros((batch_size, input_token_max_len)).bool().to(device)\n        batched_images_emb_mask = torch.zeros(\n            (batch_size, max_n_images, self.num_image_tokens)\n        ).bool().to(device)\n\n        for i, prepare in enumerate(prepare_list):\n            input_ids = prepare.input_ids\n            seq_len = len(prepare)\n            n_image = len(prepare.num_image_tokens)\n            # left-padding\n            batched_attention_mask[i, -seq_len:] = 1\n            # import pdb;pdb.set_trace()\n            batched_input_ids[i, -seq_len:] = torch.LongTensor(input_ids.cpu()).to(device)\n            # batched_input_ids[i, -seq_len:] = input_ids.clone()\n            batched_images_seq_mask[i, -seq_len:] = input_ids == self.image_id\n\n            if n_image > 0:\n                batched_pixel_values[i, :n_image] = prepare.pixel_values\n                for j, n_image_tokens in enumerate(prepare.num_image_tokens):\n                    batched_images_emb_mask[i, j, :n_image_tokens] = True\n\n            sft_format.append(prepare.sft_format)\n\n        batched_prepares = BatchedVLChatProcessorOutput(\n            input_ids=batched_input_ids,\n            attention_mask=batched_attention_mask,\n            pixel_values=batched_pixel_values,\n            images_seq_mask=batched_images_seq_mask,\n            images_emb_mask=batched_images_emb_mask,\n            sft_format=sft_format,\n        )\n\n        return batched_prepares\n"}
{"type": "source_file", "path": "three_party/Janus/janus/models/modeling_vlm.py", "content": "# Copyright (c) 2023-2024 DeepSeek.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the \"Software\"), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n# the Software, and to permit persons to whom the Software is furnished to do so,\n# subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport torch\nfrom attrdict import AttrDict\nfrom einops import rearrange\nfrom transformers import (\n    AutoConfig,\n    AutoModelForCausalLM,\n    LlamaConfig,\n    LlamaForCausalLM,\n    PreTrainedModel,\n)\nfrom transformers.configuration_utils import PretrainedConfig\n\nfrom janus.models.clip_encoder import CLIPVisionTower\nfrom janus.models.projector import MlpProjector\n\n\nclass vision_head(torch.nn.Module):\n    def __init__(self, params):\n        super().__init__()\n        self.output_mlp_projector = torch.nn.Linear(\n            params.n_embed, params.image_token_embed\n        )\n        self.vision_activation = torch.nn.GELU()\n        self.vision_head = torch.nn.Linear(\n            params.image_token_embed, params.image_token_size\n        )\n\n    def forward(self, x):\n        x = self.output_mlp_projector(x)\n        x = self.vision_activation(x)\n        x = self.vision_head(x)\n        return x\n\n\ndef model_name_to_cls(cls_name):\n    if \"MlpProjector\" in cls_name:\n        cls = MlpProjector\n\n    elif \"CLIPVisionTower\" in cls_name:\n        cls = CLIPVisionTower\n\n    elif \"VQ\" in cls_name:\n        from janus.models.vq_model import VQ_models\n\n        cls = VQ_models[cls_name]\n    elif \"vision_head\" in cls_name:\n        cls = vision_head\n    else:\n        raise ValueError(f\"class_name {cls_name} is invalid.\")\n\n    return cls\n\n\nclass VisionConfig(PretrainedConfig):\n    model_type = \"vision\"\n    cls: str = \"\"\n    params: AttrDict = {}\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        self.cls = kwargs.get(\"cls\", \"\")\n        if not isinstance(self.cls, str):\n            self.cls = self.cls.__name__\n\n        self.params = AttrDict(kwargs.get(\"params\", {}))\n\n\nclass AlignerConfig(PretrainedConfig):\n    model_type = \"aligner\"\n    cls: str = \"\"\n    params: AttrDict = {}\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        self.cls = kwargs.get(\"cls\", \"\")\n        if not isinstance(self.cls, str):\n            self.cls = self.cls.__name__\n\n        self.params = AttrDict(kwargs.get(\"params\", {}))\n\n\nclass GenVisionConfig(PretrainedConfig):\n    model_type = \"gen_vision\"\n    cls: str = \"\"\n    params: AttrDict = {}\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        self.cls = kwargs.get(\"cls\", \"\")\n        if not isinstance(self.cls, str):\n            self.cls = self.cls.__name__\n\n        self.params = AttrDict(kwargs.get(\"params\", {}))\n\n\nclass GenAlignerConfig(PretrainedConfig):\n    model_type = \"gen_aligner\"\n    cls: str = \"\"\n    params: AttrDict = {}\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        self.cls = kwargs.get(\"cls\", \"\")\n        if not isinstance(self.cls, str):\n            self.cls = self.cls.__name__\n\n        self.params = AttrDict(kwargs.get(\"params\", {}))\n\n\nclass GenHeadConfig(PretrainedConfig):\n    model_type = \"gen_head\"\n    cls: str = \"\"\n    params: AttrDict = {}\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        self.cls = kwargs.get(\"cls\", \"\")\n        if not isinstance(self.cls, str):\n            self.cls = self.cls.__name__\n\n        self.params = AttrDict(kwargs.get(\"params\", {}))\n\n\nclass MultiModalityConfig(PretrainedConfig):\n    model_type = \"multi_modality\"\n    vision_config: VisionConfig\n    aligner_config: AlignerConfig\n\n    gen_vision_config: GenVisionConfig\n    gen_aligner_config: GenAlignerConfig\n    gen_head_config: GenHeadConfig\n\n    language_config: LlamaConfig\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        vision_config = kwargs.get(\"vision_config\", {})\n        self.vision_config = VisionConfig(**vision_config)\n\n        aligner_config = kwargs.get(\"aligner_config\", {})\n        self.aligner_config = AlignerConfig(**aligner_config)\n\n        gen_vision_config = kwargs.get(\"gen_vision_config\", {})\n        self.gen_vision_config = GenVisionConfig(**gen_vision_config)\n\n        gen_aligner_config = kwargs.get(\"gen_aligner_config\", {})\n        self.gen_aligner_config = GenAlignerConfig(**gen_aligner_config)\n\n        gen_head_config = kwargs.get(\"gen_head_config\", {})\n        self.gen_head_config = GenHeadConfig(**gen_head_config)\n\n        language_config = kwargs.get(\"language_config\", {})\n        if isinstance(language_config, LlamaConfig):\n            self.language_config = language_config\n        else:\n            self.language_config = LlamaConfig(**language_config)\n\n\nclass MultiModalityPreTrainedModel(PreTrainedModel):\n    config_class = MultiModalityConfig\n    base_model_prefix = \"multi_modality\"\n    _no_split_modules = []\n    _skip_keys_device_placement = \"past_key_values\"\n\n\nclass MultiModalityCausalLM(MultiModalityPreTrainedModel):\n    def __init__(self, config: MultiModalityConfig):\n        super().__init__(config)\n\n        vision_config = config.vision_config\n        vision_cls = model_name_to_cls(vision_config.cls)\n        self.vision_model = vision_cls(**vision_config.params)\n\n        aligner_config = config.aligner_config\n        aligner_cls = model_name_to_cls(aligner_config.cls)\n        self.aligner = aligner_cls(aligner_config.params)\n\n        gen_vision_config = config.gen_vision_config\n        gen_vision_cls = model_name_to_cls(gen_vision_config.cls)\n        self.gen_vision_model = gen_vision_cls()\n\n        gen_aligner_config = config.gen_aligner_config\n        gen_aligner_cls = model_name_to_cls(gen_aligner_config.cls)\n        self.gen_aligner = gen_aligner_cls(gen_aligner_config.params)\n\n        gen_head_config = config.gen_head_config\n        gen_head_cls = model_name_to_cls(gen_head_config.cls)\n        self.gen_head = gen_head_cls(gen_head_config.params)\n\n        self.gen_embed = torch.nn.Embedding(\n            gen_vision_config.params.image_token_size, gen_vision_config.params.n_embed\n        )\n\n        language_config = config.language_config\n        self.language_model = LlamaForCausalLM(language_config)\n\n    def prepare_inputs_embeds(\n        self,\n        input_ids: torch.LongTensor,\n        pixel_values: torch.FloatTensor,\n        images_seq_mask: torch.LongTensor,\n        images_emb_mask: torch.LongTensor,\n        **kwargs,\n    ):\n        \"\"\"\n\n        Args:\n            input_ids (torch.LongTensor): [b, T]\n            pixel_values (torch.FloatTensor):   [b, n_images, 3, h, w]\n            images_seq_mask (torch.BoolTensor): [b, T]\n            images_emb_mask (torch.BoolTensor): [b, n_images, n_image_tokens]\n\n            assert torch.sum(images_seq_mask) == torch.sum(images_emb_mask)\n\n        Returns:\n            input_embeds (torch.Tensor): [b, T, D]\n        \"\"\"\n\n        device = next(self.vision_model.parameters()).device\n        input_ids = input_ids.to(device) ##\n\n        bs, n = pixel_values.shape[0:2]\n        images = rearrange(pixel_values, \"b n c h w -> (b n) c h w\")\n        # [b x n, T2, D]\n        images = images.bfloat16().to(device) ###\n        images_embeds = self.aligner(self.vision_model(images))\n        # p next(self.vision_model.parameters()).requires_grad\n        # p images_embeds.grad\n        # import pdb;pdb.set_trace()\n\n        # [b x n, T2, D] -> [b, n x T2, D]\n        images_embeds = rearrange(images_embeds, \"(b n) t d -> b (n t) d\", b=bs, n=n)\n        # [b, n, T2] -> [b, n x T2]\n        images_emb_mask = rearrange(images_emb_mask, \"b n t -> b (n t)\")\n\n        # [b, T, D]\n        input_ids[input_ids < 0] = 0  # ignore the image embeddings\n        input_ids = input_ids.clone() ####\n        inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n\n        # replace with the image embeddings\n        inputs_embeds[images_seq_mask] = images_embeds[images_emb_mask].to(inputs_embeds.dtype)###\n\n        return inputs_embeds\n\n    def prepare_gen_img_embeds(self, image_ids: torch.LongTensor):\n        return self.gen_aligner(self.gen_embed(image_ids))\n\n\nAutoConfig.register(\"vision\", VisionConfig)\nAutoConfig.register(\"aligner\", AlignerConfig)\nAutoConfig.register(\"gen_vision\", GenVisionConfig)\nAutoConfig.register(\"gen_aligner\", GenAlignerConfig)\nAutoConfig.register(\"gen_head\", GenHeadConfig)\nAutoConfig.register(\"multi_modality\", MultiModalityConfig)\nAutoModelForCausalLM.register(MultiModalityConfig, MultiModalityCausalLM)"}
{"type": "source_file", "path": "three_party/Janus/janus/janusflow/models/processing_vlm.py", "content": "# Copyright (c) 2023-2024 DeepSeek.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the \"Software\"), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n# the Software, and to permit persons to whom the Software is furnished to do so,\n# subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom dataclasses import dataclass\nfrom typing import Dict, List\n\nimport torch\nfrom PIL.Image import Image\nfrom transformers import LlamaTokenizerFast\nfrom transformers.processing_utils import ProcessorMixin\n\nfrom janus.janusflow.models.image_processing_vlm import VLMImageProcessor\nfrom janus.utils.conversation import get_conv_template\n\n\nclass DictOutput(object):\n    def keys(self):\n        return self.__dict__.keys()\n\n    def __getitem__(self, item):\n        return self.__dict__[item]\n\n    def __setitem__(self, key, value):\n        self.__dict__[key] = value\n\n\n@dataclass\nclass VLChatProcessorOutput(DictOutput):\n    sft_format: str\n    input_ids: torch.Tensor\n    pixel_values: torch.Tensor\n    num_und_image_tokens: torch.IntTensor\n\n    def __len__(self):\n        return len(self.input_ids)\n\n\n@dataclass\nclass BatchedVLChatProcessorOutput(DictOutput):\n    sft_format: List[str]\n    input_ids: torch.Tensor\n    pixel_values: torch.Tensor\n    attention_mask: torch.Tensor\n    images_seq_mask: torch.BoolTensor\n    images_emb_mask: torch.BoolTensor\n\n    def to(self, device, dtype=torch.bfloat16):\n        self.input_ids = self.input_ids.to(device)\n        self.attention_mask = self.attention_mask.to(device)\n        self.images_seq_mask = self.images_seq_mask.to(device)\n        self.images_emb_mask = self.images_emb_mask.to(device)\n        self.pixel_values = self.pixel_values.to(device=device, dtype=dtype)\n        return self\n\n\nclass VLChatProcessor(ProcessorMixin):\n    image_processor_class = \"AutoImageProcessor\"\n    tokenizer_class = (\"LlamaTokenizer\", \"LlamaTokenizerFast\")\n\n    attributes = [\"image_processor\", \"tokenizer\"]\n\n    system_prompt = (\n        \"You are a helpful language and vision assistant. \"\n        \"You are able to understand the visual content that the user provides, \"\n        \"and assist the user with a variety of tasks using natural language.\"\n    )\n\n    def __init__(\n        self,\n        image_processor: VLMImageProcessor,\n        tokenizer: LlamaTokenizerFast,\n        image_tag: str = \"<image_placeholder>\",\n        image_start_tag: str = \"<begin_of_image>\",\n        image_end_tag: str = \"<end_of_image>\",\n        image_gen_tag: str = \"<｜begin▁of▁generation｜>\",\n        num_image_tokens: int = 576,\n        add_special_token: bool = False,\n        sft_format: str = \"deepseek\",\n        mask_prompt: bool = True,\n        ignore_id: int = -100,\n        **kwargs,\n    ):\n        self.image_processor = image_processor\n        self.tokenizer = tokenizer\n\n        image_id = self.tokenizer.vocab.get(image_tag)\n        if image_id is None:\n            special_tokens = [image_tag]\n            special_tokens_dict = {\"additional_special_tokens\": special_tokens}\n            self.tokenizer.add_special_tokens(special_tokens_dict)\n            print(f\"Add image tag = {image_tag} to the tokenizer\")\n\n        image_gen_id = self.tokenizer.vocab.get(image_gen_tag)\n        if image_gen_id is None:\n            special_tokens = [image_gen_tag]\n            special_tokens_dict = {\"additional_special_tokens\": special_tokens}\n            self.tokenizer.add_special_tokens(special_tokens_dict)\n            print(f\"Add generation tag = {image_gen_tag} to the tokenizer\")\n\n        assert image_start_tag is not None and image_end_tag is not None\n        boi_id = self.tokenizer.vocab.get(image_start_tag)\n        eoi_id = self.tokenizer.vocab.get(image_end_tag)\n        if boi_id is None:\n            special_tokens = [image_start_tag]\n            special_tokens_dict = {\"additional_special_tokens\": special_tokens}\n            self.tokenizer.add_special_tokens(special_tokens_dict)\n            print(f\"Add boi tag = {image_start_tag} to the tokenizer\")\n        if eoi_id is None:\n            special_tokens = [image_end_tag]\n            special_tokens_dict = {\"additional_special_tokens\": special_tokens}\n            self.tokenizer.add_special_tokens(special_tokens_dict)\n            print(f\"Add eoi tag = {image_end_tag} to the tokenizer\")\n\n        self.image_tag = image_tag\n        self.image_gen_tag = image_gen_tag\n        self.image_start_tag = image_start_tag\n        self.image_end_tag = image_end_tag\n\n        self.num_image_tokens = num_image_tokens\n        self.add_special_token = add_special_token\n        self.sft_format = sft_format\n        self.mask_prompt = mask_prompt\n        self.ignore_id = ignore_id\n        self.tokenizer.pad_token_id = self.tokenizer.vocab.get(\"<｜▁pad▁｜>\")\n\n        super().__init__(\n            image_processor,\n            tokenizer,\n            image_tag,\n            num_image_tokens,\n            add_special_token,\n            sft_format,\n            mask_prompt,\n            ignore_id,\n            **kwargs,\n        )\n\n    def new_chat_template(self):\n        conv = get_conv_template(self.sft_format)\n        conv.set_system_message(self.system_prompt)\n        return conv\n\n    def apply_sft_template_for_multi_turn_prompts(\n        self,\n        conversations: List[Dict[str, str]],\n        sft_format: str = \"deepseek\",\n        system_prompt: str = \"\",\n    ):\n        \"\"\"\n        Applies the SFT template to conversation.\n\n        An example of conversation:\n        conversation = [\n            {\n                \"role\": \"User\",\n                \"content\": \"<image_placeholder> is Figure 1.\\n<image_placeholder> is Figure 2.\\nWhich image is brighter?\",\n                \"images\": [\n                    \"./multi-images/attribute_comparison_1.png\",\n                    \"./multi-images/attribute_comparison_2.png\"\n                ]\n            },\n            {\n                \"role\": \"Assistant\",\n                \"content\": \"\"\n            }\n        ]\n\n        Args:\n            conversations (List[Dict]): A conversation with a List of Dict[str, str] text.\n            sft_format (str, optional): The format of the SFT template to use. Defaults to \"deepseek\".\n            system_prompt (str, optional): The system prompt to use in the SFT template. Defaults to \"\".\n\n        Returns:\n            sft_prompt (str): The formatted text.\n        \"\"\"\n\n        conv = get_conv_template(sft_format)\n        conv.set_system_message(system_prompt)\n        for message in conversations:\n            conv.append_message(message[\"role\"], message[\"content\"].strip())\n        sft_prompt = conv.get_prompt().strip()\n\n        return sft_prompt\n\n    @property\n    def image_token(self):\n        return self.image_tag\n\n    @property\n    def image_id(self):\n        image_id = self.tokenizer.vocab.get(self.image_tag)\n        return image_id\n\n    @property\n    def image_start_id(self):\n        image_start_id = self.tokenizer.vocab.get(self.image_start_tag)\n        return image_start_id\n\n    @property\n    def image_end_id(self):\n        image_end_id = self.tokenizer.vocab.get(self.image_end_tag)\n        return image_end_id\n\n    @property\n    def image_start_token(self):\n        return self.image_start_tag\n\n    @property\n    def image_end_token(self):\n        return self.image_end_tag\n\n    @property\n    def pad_id(self):\n        pad_id = self.tokenizer.pad_token_id\n        if pad_id is None:\n            pad_id = self.tokenizer.eos_token_id\n\n        return pad_id\n\n    @property\n    def image_gen_id(self):\n        image_gen_id = self.tokenizer.vocab.get(self.image_gen_tag)\n        return image_gen_id\n\n    def add_image_token(\n        self,\n        image_indices: List[int],\n        input_ids: torch.LongTensor,\n    ):\n        \"\"\"\n\n        Args:\n            image_indices (List[int]): [index_0, index_1, ..., index_j]\n            input_ids (torch.LongTensor): [N]\n\n        Returns:\n            input_ids (torch.LongTensor): [N + image tokens]\n            num_image_tokens (torch.IntTensor): [n_images]\n        \"\"\"\n\n        input_slices = []\n\n        start = 0\n        for index in image_indices:\n            if self.add_special_token:\n                end = index + 1\n            else:\n                end = index\n\n            # original text tokens\n            input_slices.append(input_ids[start:end])\n\n            # add boi, image tokens, eoi and set the mask as False\n            input_slices.append(self.image_start_id * torch.ones((1), dtype=torch.long))\n            input_slices.append(\n                self.image_id * torch.ones((self.num_image_tokens,), dtype=torch.long)\n            )\n            input_slices.append(self.image_end_id * torch.ones((1), dtype=torch.long))\n            start = index + 1\n\n        # the left part\n        input_slices.append(input_ids[start:])\n\n        # concat all slices\n        input_ids = torch.cat(input_slices, dim=0)\n        num_image_tokens = torch.IntTensor(\n            [self.num_image_tokens + 1] * len(image_indices)\n        )\n        # we add 1 to fit generation\n\n        return input_ids, num_image_tokens\n\n    def process_one(\n        self,\n        prompt: str = None,\n        conversations: List[Dict[str, str]] = None,\n        images: List[Image] = None,\n        **kwargs,\n    ):\n        \"\"\"\n\n        Args:\n            prompt (str): the formatted prompt;\n            conversations (List[Dict]): conversations with a list of messages;\n            images (List[ImageType]): the list of images;\n            **kwargs:\n\n        Returns:\n            outputs (BaseProcessorOutput): the output of the processor,\n                - input_ids (torch.LongTensor): [N + image tokens]\n                - target_ids (torch.LongTensor): [N + image tokens]\n                - images (torch.FloatTensor): [n_images, 3, H, W]\n                - image_id (int): the id of the image token\n                - num_image_tokens (List[int]): the number of image tokens\n        \"\"\"\n\n        assert (\n            prompt is None or conversations is None\n        ), \"prompt and conversations cannot be used at the same time.\"\n\n        if prompt is None:\n            # apply sft format\n            sft_format = self.apply_sft_template_for_multi_turn_prompts(\n                conversations=conversations,\n                sft_format=self.sft_format,\n                system_prompt=self.system_prompt,\n            )\n        else:\n            sft_format = prompt\n\n        # tokenize\n        input_ids = self.tokenizer.encode(sft_format)\n        input_ids = torch.LongTensor(input_ids)\n\n        # add image tokens to the input_ids\n        image_token_mask: torch.BoolTensor = input_ids == self.image_id\n        image_indices = image_token_mask.nonzero()\n\n        input_ids, num_und_image_tokens = self.add_image_token(\n            image_indices=image_indices,\n            input_ids=input_ids,\n        )\n\n        # load images\n        images_outputs = self.image_processor(images, return_tensors=\"pt\")\n\n        prepare = VLChatProcessorOutput(\n            sft_format=sft_format,\n            input_ids=input_ids,\n            pixel_values=images_outputs.pixel_values,\n            num_und_image_tokens=num_und_image_tokens,\n        )\n\n        return prepare\n\n    def __call__(\n        self,\n        *,\n        prompt: str = None,\n        conversations: List[Dict[str, str]] = None,\n        images: List[Image] = None,\n        force_batchify: bool = True,\n        **kwargs,\n    ):\n        \"\"\"\n\n        Args:\n            prompt (str): the formatted prompt;\n            conversations (List[Dict]): conversations with a list of messages;\n            images (List[ImageType]): the list of images;\n            force_batchify (bool): force batchify the inputs;\n            **kwargs:\n\n        Returns:\n            outputs (BaseProcessorOutput): the output of the processor,\n                - input_ids (torch.LongTensor): [N + image tokens]\n                - images (torch.FloatTensor): [n_images, 3, H, W]\n                - image_id (int): the id of the image token\n                - num_image_tokens (List[int]): the number of image tokens\n        \"\"\"\n\n        prepare = self.process_one(\n            prompt=prompt, conversations=conversations, images=images\n        )\n\n        if force_batchify:\n            prepare = self.batchify([prepare])\n\n        return prepare\n\n    def batchify(\n        self, prepare_list: List[VLChatProcessorOutput]\n    ) -> BatchedVLChatProcessorOutput:\n        \"\"\"\n        Preprocesses the inputs for multimodal inference.\n\n        Args:\n            prepare_list (List[VLChatProcessorOutput]): A list of VLChatProcessorOutput.\n\n        Returns:\n            BatchedVLChatProcessorOutput: A dictionary of the inputs to use for multimodal inference.\n        \"\"\"\n\n        batch_size = len(prepare_list)\n        sft_format = []\n        n_images = []\n        seq_lens = []\n        for prepare in prepare_list:\n            # we only fill the images for understanding tasks into the mask\n            n_images.append(len(prepare.num_und_image_tokens))\n            seq_lens.append(len(prepare))\n\n        input_token_max_len = max(seq_lens)\n        max_n_images = max(1, max(n_images))\n\n        batched_input_ids = torch.full(\n            (batch_size, input_token_max_len), self.pad_id\n        ).long()  # FIXME\n        batched_attention_mask = torch.zeros((batch_size, input_token_max_len)).long()\n        batched_pixel_values = torch.zeros(\n            (batch_size, max_n_images, *self.image_processor.default_shape)\n        ).float()\n        batched_images_seq_mask = torch.zeros((batch_size, input_token_max_len)).bool()\n        batched_images_emb_mask = torch.zeros(\n            (\n                batch_size,\n                max_n_images,\n                self.num_image_tokens + 1,\n            )  # add 1 to account for <image_beg>\n        ).bool()\n\n        for i, prepare in enumerate(prepare_list):\n            input_ids = prepare.input_ids\n            seq_len = len(prepare)\n            n_image = len(prepare.num_und_image_tokens)\n            # left-padding\n            batched_attention_mask[i, -seq_len:] = 1\n            batched_input_ids[i, -seq_len:] = torch.LongTensor(input_ids)\n            batched_images_seq_mask[i, -seq_len:] = (input_ids == self.image_id) | (\n                input_ids == self.image_start_id\n            )\n\n            if n_image > 0:\n                batched_pixel_values[i, :n_image] = prepare.pixel_values\n                for j, n_image_tokens in enumerate(prepare.num_und_image_tokens):\n                    batched_images_emb_mask[i, j, :n_image_tokens] = True\n\n            sft_format.append(prepare.sft_format)\n\n        batched_prepares = BatchedVLChatProcessorOutput(\n            input_ids=batched_input_ids,\n            attention_mask=batched_attention_mask,\n            pixel_values=batched_pixel_values,\n            images_seq_mask=batched_images_seq_mask,\n            images_emb_mask=batched_images_emb_mask,\n            sft_format=sft_format,\n        )\n\n        return batched_prepares\n"}
{"type": "source_file", "path": "three_party/Janus/janus/models/processing_vlm.py", "content": "# Copyright (c) 2023-2024 DeepSeek.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the \"Software\"), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n# the Software, and to permit persons to whom the Software is furnished to do so,\n# subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom dataclasses import dataclass\nfrom typing import Dict, List\n\nimport torch\nfrom PIL.Image import Image\nfrom transformers import LlamaTokenizerFast\nfrom transformers.processing_utils import ProcessorMixin\n\nfrom janus.models.image_processing_vlm import VLMImageProcessor\nfrom janus.utils.conversation import get_conv_template\n\n\nclass DictOutput(object):\n    def keys(self):\n        return self.__dict__.keys()\n\n    def __getitem__(self, item):\n        return self.__dict__[item]\n\n    def __setitem__(self, key, value):\n        self.__dict__[key] = value\n\n\n@dataclass\nclass VLChatProcessorOutput(DictOutput):\n    sft_format: str\n    input_ids: torch.Tensor\n    pixel_values: torch.Tensor\n    num_image_tokens: torch.IntTensor\n\n    def __len__(self):\n        return len(self.input_ids)\n\n\n@dataclass\nclass BatchedVLChatProcessorOutput(DictOutput):\n    sft_format: List[str]\n    input_ids: torch.Tensor\n    pixel_values: torch.Tensor\n    attention_mask: torch.Tensor\n    images_seq_mask: torch.BoolTensor\n    images_emb_mask: torch.BoolTensor\n\n    def to(self, device, dtype=torch.bfloat16):\n        self.input_ids = self.input_ids.to(device)\n        self.attention_mask = self.attention_mask.to(device)\n        self.images_seq_mask = self.images_seq_mask.to(device)\n        self.images_emb_mask = self.images_emb_mask.to(device)\n        self.pixel_values = self.pixel_values.to(device=device, dtype=dtype)\n        return self\n\n\nclass VLChatProcessor(ProcessorMixin):\n    image_processor_class = \"AutoImageProcessor\"\n    tokenizer_class = (\"LlamaTokenizer\", \"LlamaTokenizerFast\")\n\n    attributes = [\"image_processor\", \"tokenizer\"]\n\n    system_prompt = (\n        \"You are a helpful language and vision assistant. \"\n        \"You are able to understand the visual content that the user provides, \"\n        \"and assist the user with a variety of tasks using natural language.\"\n    )\n\n    def __init__(\n        self,\n        image_processor: VLMImageProcessor,\n        tokenizer: LlamaTokenizerFast,\n        image_tag: str = \"<image_placeholder>\",\n        image_start_tag: str = \"<begin_of_image>\",\n        image_end_tag: str = \"<end_of_image>\",\n        pad_tag: str = \"<｜▁pad▁｜>\",\n        num_image_tokens: int = 576,\n        add_special_token: bool = False,\n        sft_format: str = \"deepseek\",\n        mask_prompt: bool = True,\n        ignore_id: int = -100,\n        **kwargs,\n    ):\n        self.image_processor = image_processor\n        self.tokenizer = tokenizer\n\n        image_id = self.tokenizer.vocab.get(image_tag)\n        if image_id is None:\n            special_tokens = [image_tag]\n            special_tokens_dict = {\"additional_special_tokens\": special_tokens}\n            self.tokenizer.add_special_tokens(special_tokens_dict)\n            print(f\"Add image tag = {image_tag} to the tokenizer\")\n\n        self.image_tag = image_tag\n        self.image_start_tag = image_start_tag\n        self.image_end_tag = image_end_tag\n        self.pad_tag = pad_tag\n\n        self.num_image_tokens = num_image_tokens\n        self.add_special_token = add_special_token\n        self.sft_format = sft_format\n        self.mask_prompt = mask_prompt\n        self.ignore_id = ignore_id\n\n        super().__init__(\n            image_processor,\n            tokenizer,\n            image_tag,\n            num_image_tokens,\n            add_special_token,\n            sft_format,\n            mask_prompt,\n            ignore_id,\n            **kwargs,\n        )\n\n    def new_chat_template(self):\n        conv = get_conv_template(self.sft_format)\n        conv.set_system_message(self.system_prompt)\n        return conv\n\n    def apply_sft_template_for_multi_turn_prompts(\n        self,\n        conversations: List[Dict[str, str]],\n        sft_format: str = \"deepseek\",\n        system_prompt: str = \"\",\n    ):\n        \"\"\"\n        Applies the SFT template to conversation.\n\n        An example of conversation:\n        conversation = [\n            {\n                \"role\": \"User\",\n                \"content\": \"<image_placeholder> is Figure 1.\\n<image_placeholder> is Figure 2.\\nWhich image is brighter?\",\n                \"images\": [\n                    \"./multi-images/attribute_comparison_1.png\",\n                    \"./multi-images/attribute_comparison_2.png\"\n                ]\n            },\n            {\n                \"role\": \"Assistant\",\n                \"content\": \"\"\n            }\n        ]\n\n        Args:\n            conversations (List[Dict]): A conversation with a List of Dict[str, str] text.\n            sft_format (str, optional): The format of the SFT template to use. Defaults to \"deepseek\".\n            system_prompt (str, optional): The system prompt to use in the SFT template. Defaults to \"\".\n\n        Returns:\n            sft_prompt (str): The formatted text.\n        \"\"\"\n\n        conv = get_conv_template(sft_format)\n        conv.set_system_message(system_prompt)\n        for message in conversations:\n            conv.append_message(message[\"role\"], message[\"content\"].strip())\n        sft_prompt = conv.get_prompt().strip()\n\n        return sft_prompt\n\n    @property\n    def image_token(self):\n        return self.image_tag\n\n    @property\n    def image_id(self):\n        image_id = self.tokenizer.vocab.get(self.image_tag)\n        return image_id\n\n    @property\n    def image_start_id(self):\n        image_start_id = self.tokenizer.vocab.get(self.image_start_tag)\n        return image_start_id\n\n    @property\n    def image_end_id(self):\n        image_end_id = self.tokenizer.vocab.get(self.image_end_tag)\n        return image_end_id\n\n    @property\n    def image_start_token(self):\n        return self.image_start_tag\n\n    @property\n    def image_end_token(self):\n        return self.image_end_tag\n\n    @property\n    def pad_id(self):\n        pad_id = self.tokenizer.vocab.get(self.pad_tag)\n        # pad_id = self.tokenizer.pad_token_id\n        # if pad_id is None:\n        #     pad_id = self.tokenizer.eos_token_id\n\n        return pad_id\n\n    def add_image_token(\n        self,\n        image_indices: List[int],\n        input_ids: torch.LongTensor,\n    ):\n        \"\"\"\n\n        Args:\n            image_indices (List[int]): [index_0, index_1, ..., index_j]\n            input_ids (torch.LongTensor): [N]\n\n        Returns:\n            input_ids (torch.LongTensor): [N + image tokens]\n            num_image_tokens (torch.IntTensor): [n_images]\n        \"\"\"\n\n        input_slices = []\n\n        start = 0\n        for index in image_indices:\n            if self.add_special_token:\n                end = index + 1\n            else:\n                end = index\n\n            # original text tokens\n            input_slices.append(input_ids[start:end])\n\n            # add boi, image tokens, eoi and set the mask as False\n            input_slices.append(self.image_start_id * torch.ones((1), dtype=torch.long))\n            input_slices.append(\n                self.image_id * torch.ones((self.num_image_tokens,), dtype=torch.long)\n            )\n            input_slices.append(self.image_end_id * torch.ones((1), dtype=torch.long))\n            start = index + 1\n\n        # the left part\n        input_slices.append(input_ids[start:])\n\n        # concat all slices\n        input_ids = torch.cat(input_slices, dim=0)\n        num_image_tokens = torch.IntTensor([self.num_image_tokens] * len(image_indices))\n\n        return input_ids, num_image_tokens\n\n    def process_one(\n        self,\n        prompt: str = None,\n        conversations: List[Dict[str, str]] = None,\n        images: List[Image] = None,\n        **kwargs,\n    ):\n        \"\"\"\n\n        Args:\n            prompt (str): the formatted prompt;\n            conversations (List[Dict]): conversations with a list of messages;\n            images (List[ImageType]): the list of images;\n            **kwargs:\n\n        Returns:\n            outputs (BaseProcessorOutput): the output of the processor,\n                - input_ids (torch.LongTensor): [N + image tokens]\n                - target_ids (torch.LongTensor): [N + image tokens]\n                - images (torch.FloatTensor): [n_images, 3, H, W]\n                - image_id (int): the id of the image token\n                - num_image_tokens (List[int]): the number of image tokens\n        \"\"\"\n\n        assert (\n            prompt is None or conversations is None\n        ), \"prompt and conversations cannot be used at the same time.\"\n\n        if prompt is None:\n            # apply sft format\n            sft_format = self.apply_sft_template_for_multi_turn_prompts(\n                conversations=conversations,\n                sft_format=self.sft_format,\n                system_prompt=self.system_prompt,\n            )\n        else:\n            sft_format = prompt\n\n        # tokenize\n        input_ids = self.tokenizer.encode(sft_format)\n        input_ids = torch.LongTensor(input_ids)\n\n        ##\n        device = torch.tensor([0]).device\n        input_ids = input_ids.to(device)\n\n        # add image tokens to the input_ids\n        image_token_mask: torch.BoolTensor = input_ids == self.image_id\n        image_indices = image_token_mask.nonzero()\n        input_ids, num_image_tokens = self.add_image_token(\n            image_indices=image_indices,\n            input_ids=input_ids,\n        )\n\n        # load images\n        images_outputs = self.image_processor(images, return_tensors=\"pt\")\n\n        prepare = VLChatProcessorOutput(\n            sft_format=sft_format,\n            input_ids=input_ids,\n            pixel_values=images_outputs.pixel_values,\n            num_image_tokens=num_image_tokens,\n        )\n\n        return prepare\n\n    def __call__(\n        self,\n        *,\n        prompt: str = None,\n        conversations: List[Dict[str, str]] = None,\n        images: List[Image] = None,\n        force_batchify: bool = True,\n        **kwargs,\n    ):\n        \"\"\"\n\n        Args:\n            prompt (str): the formatted prompt;\n            conversations (List[Dict]): conversations with a list of messages;\n            images (List[ImageType]): the list of images;\n            force_batchify (bool): force batchify the inputs;\n            **kwargs:\n\n        Returns:\n            outputs (BaseProcessorOutput): the output of the processor,\n                - input_ids (torch.LongTensor): [N + image tokens]\n                - images (torch.FloatTensor): [n_images, 3, H, W]\n                - image_id (int): the id of the image token\n                - num_image_tokens (List[int]): the number of image tokens\n        \"\"\"\n\n        prepare = self.process_one(\n            prompt=prompt, conversations=conversations, images=images\n        )\n\n        if force_batchify:\n            prepare = self.batchify([prepare])\n\n        return prepare\n\n    def batchify(\n        self, prepare_list: List[VLChatProcessorOutput]\n    ) -> BatchedVLChatProcessorOutput:\n        \"\"\"\n        Preprocesses the inputs for multimodal inference.\n\n        Args:\n            prepare_list (List[VLChatProcessorOutput]): A list of VLChatProcessorOutput.\n\n        Returns:\n            BatchedVLChatProcessorOutput: A dictionary of the inputs to use for multimodal inference.\n        \"\"\"\n\n        batch_size = len(prepare_list)\n        sft_format = []\n        n_images = []\n        seq_lens = []\n        for prepare in prepare_list:\n            n_images.append(len(prepare.num_image_tokens))\n            seq_lens.append(len(prepare))\n\n        input_token_max_len = max(seq_lens)\n        max_n_images = max(1, max(n_images))\n\n        batched_input_ids = torch.full(\n            (batch_size, input_token_max_len), self.pad_id\n        ).long()  # FIXME\n        batched_attention_mask = torch.zeros((batch_size, input_token_max_len)).long()\n        batched_pixel_values = torch.zeros(\n            (batch_size, max_n_images, *self.image_processor.default_shape)\n        ).float()\n        batched_images_seq_mask = torch.zeros((batch_size, input_token_max_len)).bool()\n        batched_images_emb_mask = torch.zeros(\n            (batch_size, max_n_images, self.num_image_tokens)\n        ).bool()\n\n        for i, prepare in enumerate(prepare_list):\n            input_ids = prepare.input_ids\n            seq_len = len(prepare)\n            n_image = len(prepare.num_image_tokens)\n            # left-padding\n            batched_attention_mask[i, -seq_len:] = 1\n            # batched_input_ids[i, -seq_len:] = torch.LongTensor(input_ids)\n            # import pdb;pdb.set_trace()\n            batched_input_ids[i, -seq_len:] = input_ids ##\n            batched_images_seq_mask[i, -seq_len:] = input_ids == self.image_id\n\n            if n_image > 0:\n                batched_pixel_values[i, :n_image] = prepare.pixel_values\n                for j, n_image_tokens in enumerate(prepare.num_image_tokens):\n                    batched_images_emb_mask[i, j, :n_image_tokens] = True\n\n            sft_format.append(prepare.sft_format)\n\n        batched_prepares = BatchedVLChatProcessorOutput(\n            input_ids=batched_input_ids,\n            attention_mask=batched_attention_mask,\n            pixel_values=batched_pixel_values,\n            images_seq_mask=batched_images_seq_mask,\n            images_emb_mask=batched_images_emb_mask,\n            sft_format=sft_format,\n        )\n\n        return batched_prepares"}
{"type": "source_file", "path": "three_party/Janus/janus/janusflow/models/image_processing_vlm.py", "content": "# Copyright (c) 2023-2024 DeepSeek.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the \"Software\"), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n# the Software, and to permit persons to whom the Software is furnished to do so,\n# subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom typing import List, Tuple, Union\n\nimport numpy as np\nimport torch\nimport torchvision\nimport torchvision.transforms.functional\nfrom PIL import Image\nfrom transformers import AutoImageProcessor, PretrainedConfig\nfrom transformers.image_processing_utils import BaseImageProcessor, BatchFeature\nfrom transformers.image_utils import to_numpy_array\nfrom transformers.utils import logging\n\nlogger = logging.get_logger(__name__)\n\nImageType = Union[np.ndarray, torch.Tensor, Image.Image]\nIMAGENET_MEAN = (0.48145466, 0.4578275, 0.40821073)\nIMAGENET_STD = (0.26862954, 0.26130258, 0.27577711)\nIMAGENET_INCEPTION_MEAN = (0.5, 0.5, 0.5)\nIMAGENET_INCEPTION_STD = (0.5, 0.5, 0.5)\n\n\ndef expand2square(pil_img, background_color):\n    width, height = pil_img.size\n    if width == height:\n        return pil_img\n    elif width > height:\n        result = Image.new(pil_img.mode, (width, width), background_color)\n        result.paste(pil_img, (0, (width - height) // 2))\n        return result\n    else:\n        result = Image.new(pil_img.mode, (height, height), background_color)\n        result.paste(pil_img, ((height - width) // 2, 0))\n        return result\n\n\nclass VLMImageProcessorConfig(PretrainedConfig):\n    model_type = \"deepseek_vlm\"\n    image_size: int\n    min_size: int\n    image_mean: Union[Tuple[float, float, float], List[float]]\n    image_std: Union[Tuple[float, float, float], List[float]]\n    rescale_factor: float\n    do_normalize: bool\n\n    def __init__(\n        self,\n        image_size: int,\n        min_size: int = 14,\n        image_mean: Union[Tuple[float, float, float], List[float]] = (\n            0.48145466,\n            0.4578275,\n            0.40821073,\n        ),\n        image_std: Union[Tuple[float, float, float], List[float]] = (\n            0.26862954,\n            0.26130258,\n            0.27577711,\n        ),\n        rescale_factor: float = 1.0 / 255.0,\n        do_normalize: bool = True,\n        **kwargs,\n    ):\n        self.image_size = image_size\n        self.min_size = min_size\n        self.image_mean = image_mean\n        self.image_std = image_std\n        self.rescale_factor = rescale_factor\n        self.do_normalize = do_normalize\n\n        super().__init__(**kwargs)\n\n\nclass VLMImageProcessor(BaseImageProcessor):\n    model_input_names = [\"pixel_values\"]\n\n    def __init__(\n        self,\n        image_size: int,\n        min_size: int = 14,\n        image_mean: Union[Tuple[float, float, float], List[float]] = (\n            0.48145466,\n            0.4578275,\n            0.40821073,\n        ),\n        image_std: Union[Tuple[float, float, float], List[float]] = (\n            0.26862954,\n            0.26130258,\n            0.27577711,\n        ),\n        rescale_factor: float = 1.0 / 255.0,\n        do_normalize: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n\n        self.image_size = image_size\n        self.rescale_factor = rescale_factor\n        self.image_mean = image_mean\n        self.image_std = image_std\n        self.min_size = min_size\n        self.do_normalize = do_normalize\n\n        if image_mean is None:\n            self.background_color = (127, 127, 127)\n        else:\n            self.background_color = tuple([int(x * 255) for x in image_mean])\n\n    def resize(self, pil_img: Image) -> np.ndarray:\n        \"\"\"\n\n        Args:\n            pil_img (PIL.Image): [H, W, 3] in PIL.Image in RGB\n\n        Returns:\n            x (np.ndarray): [3, self.image_size, self.image_size]\n        \"\"\"\n\n        width, height = pil_img.size\n        max_size = max(width, height)\n\n        size = [\n            max(int(height / max_size * self.image_size), self.min_size),\n            max(int(width / max_size * self.image_size), self.min_size),\n        ]\n\n        if width <= 0 or height <= 0 or size[0] <= 0 or size[1] <= 0:\n            print(f\"orig size = {pil_img.size}, new size = {size}\")\n            raise ValueError(\"Invalid size!\")\n\n        pil_img = torchvision.transforms.functional.resize(\n            pil_img,\n            size,\n            interpolation=torchvision.transforms.functional.InterpolationMode.BICUBIC,\n            antialias=True,\n        )\n\n        pil_img = expand2square(pil_img, self.background_color)\n        x = to_numpy_array(pil_img)\n\n        # [H, W, 3] -> [3, H, W]\n        x = np.transpose(x, (2, 0, 1))\n\n        return x\n\n    def preprocess(self, images, return_tensors: str = \"pt\", **kwargs) -> BatchFeature:\n        # resize and pad to [self.image_size, self.image_size]\n        # then convert from [H, W, 3] to [3, H, W]\n        images: List[np.ndarray] = [self.resize(image) for image in images]\n\n        # resacle from [0, 255] -> [0, 1]\n        images = [\n            self.rescale(\n                image=image,\n                scale=self.rescale_factor,\n                input_data_format=\"channels_first\",\n            )\n            for image in images\n        ]\n\n        # normalize\n        if self.do_normalize:\n            images = [\n                self.normalize(\n                    image=image,\n                    mean=self.image_mean,\n                    std=self.image_std,\n                    input_data_format=\"channels_first\",\n                )\n                for image in images\n            ]\n\n        data = {\"pixel_values\": images}\n        return BatchFeature(data=data, tensor_type=return_tensors)\n\n    @property\n    def default_shape(self):\n        return [3, self.image_size, self.image_size]\n\n\nAutoImageProcessor.register(VLMImageProcessorConfig, VLMImageProcessor)\n\n\nif __name__ == \"__main__\":\n    image_processor = VLMImageProcessor(\n        image_size=1024,\n        image_mean=IMAGENET_INCEPTION_MEAN,\n        image_std=IMAGENET_INCEPTION_STD,\n        do_normalize=True,\n    )\n"}
{"type": "source_file", "path": "three_party/Janus/janus/janusflow/models/siglip_vit.py", "content": "# Copyright (c) 2023-2024 DeepSeek.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the \"Software\"), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n# the Software, and to permit persons to whom the Software is furnished to do so,\n# subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n# https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py\nimport math\nimport warnings\nfrom dataclasses import dataclass\nfrom functools import partial\nfrom typing import (\n    Callable,\n    Dict,\n    Final,\n    List,\n    Literal,\n    Optional,\n    Sequence,\n    Set,\n    Tuple,\n    Type,\n    Union,\n)\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom timm.layers import (\n    AttentionPoolLatent,\n    DropPath,\n    LayerType,\n    Mlp,\n    PatchDropout,\n    PatchEmbed,\n    resample_abs_pos_embed,\n)\nfrom timm.models._manipulate import checkpoint_seq, named_apply\n\n\ndef _no_grad_trunc_normal_(tensor, mean, std, a, b):\n    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n    def norm_cdf(x):\n        # Computes standard normal cumulative distribution function\n        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0\n\n    if (mean < a - 2 * std) or (mean > b + 2 * std):\n        warnings.warn(\n            \"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n            \"The distribution of values may be incorrect.\",\n            stacklevel=2,\n        )\n\n    with torch.no_grad():\n        # Values are generated by using a truncated uniform distribution and\n        # then using the inverse CDF for the normal distribution.\n        # Get upper and lower cdf values\n        l = norm_cdf((a - mean) / std)  # noqa: E741\n        u = norm_cdf((b - mean) / std)\n\n        # Uniformly fill tensor with values from [l, u], then translate to\n        # [2l-1, 2u-1].\n        tensor.uniform_(2 * l - 1, 2 * u - 1)\n\n        # Use inverse cdf transform for normal distribution to get truncated\n        # standard normal\n        tensor.erfinv_()\n\n        # Transform to proper mean, std\n        tensor.mul_(std * math.sqrt(2.0))\n        tensor.add_(mean)\n\n        # Clamp to ensure it's in the proper range\n        tensor.clamp_(min=a, max=b)\n        return tensor\n\n\ndef trunc_normal_(tensor, mean=0.0, std=1.0, a=-2.0, b=2.0):\n    # type: (torch.Tensor, float, float, float, float) -> torch.Tensor\n    r\"\"\"The original timm.models.layers.weight_init.trunc_normal_ can not handle bfloat16 yet, here we first\n    convert the tensor to float32, apply the trunc_normal_() in float32, and then convert it back to its original dtype.\n    Fills the input Tensor with values drawn from a truncated normal distribution. The values are effectively drawn\n    from the normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n    with values outside :math:`[a, b]` redrawn until they are within\n    the bounds. The method used for generating the random values works\n    best when :math:`a \\leq \\text{mean} \\leq b`.\n    Args:\n        tensor: an n-dimensional `torch.Tensor`\n        mean: the mean of the normal distribution\n        std: the standard deviation of the normal distribution\n        a: the minimum cutoff value\n        b: the maximum cutoff value\n    Examples:\n        >>> w = torch.empty(3, 5)\n        >>> nn.init.trunc_normal_(w)\n    \"\"\"\n\n    with torch.no_grad():\n        dtype = tensor.dtype\n        tensor_fp32 = tensor.float()\n        tensor_fp32 = _no_grad_trunc_normal_(tensor_fp32, mean, std, a, b)\n        tensor_dtype = tensor_fp32.to(dtype=dtype)\n        tensor.copy_(tensor_dtype)\n\n\ndef init_weights(self):\n    if self.pos_embed is not None:\n        trunc_normal_(self.pos_embed, std=self.pos_embed.shape[1] ** -0.5)\n    trunc_normal_(self.latent, std=self.latent_dim**-0.5)\n\n\ndef init_weights_vit_timm(module: nn.Module, name: str = \"\") -> None:\n    \"\"\"ViT weight initialization, original timm impl (for reproducibility)\"\"\"\n    if isinstance(module, nn.Linear):\n        trunc_normal_(module.weight, std=0.02)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif hasattr(module, \"init_weights\"):\n        module.init_weights()\n\n\nclass Attention(nn.Module):\n    fused_attn: Final[bool]\n\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int = 8,\n        qkv_bias: bool = False,\n        qk_norm: bool = False,\n        attn_drop: float = 0.0,\n        proj_drop: float = 0.0,\n        norm_layer: nn.Module = nn.LayerNorm,\n    ) -> None:\n        super().__init__()\n        assert dim % num_heads == 0, \"dim should be divisible by num_heads\"\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim**-0.5\n        # self.fused_attn = use_fused_attn()\n        self.fused_attn = True\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop) if proj_drop > 0.0 else nn.Identity()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        B, N, C = x.shape\n        qkv = (\n            self.qkv(x)\n            .reshape(B, N, 3, self.num_heads, self.head_dim)\n            .permute(2, 0, 3, 1, 4)\n        )\n        q, k, v = qkv.unbind(0)\n        q, k = self.q_norm(q), self.k_norm(k)\n\n        if self.fused_attn:\n            x = F.scaled_dot_product_attention(\n                q,\n                k,\n                v,\n                dropout_p=self.attn_drop.p if self.training else 0.0,\n            )\n        else:\n            q = q * self.scale\n            attn = q @ k.transpose(-2, -1)\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n            x = attn @ v\n\n        x = x.transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass LayerScale(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        init_values: float = 1e-5,\n        inplace: bool = False,\n    ) -> None:\n        super().__init__()\n        self.inplace = inplace\n        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return x.mul_(self.gamma) if self.inplace else x * self.gamma\n\n\nclass Block(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = False,\n        qk_norm: bool = False,\n        proj_drop: float = 0.0,\n        attn_drop: float = 0.0,\n        init_values: Optional[float] = None,\n        drop_path: float = 0.0,\n        act_layer: nn.Module = nn.GELU,\n        norm_layer: nn.Module = nn.LayerNorm,\n        mlp_layer: nn.Module = Mlp,\n    ) -> None:\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            qk_norm=qk_norm,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop,\n            norm_layer=norm_layer,\n        )\n        self.ls1 = (\n            LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n        )\n        self.drop_path1 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n\n        self.norm2 = norm_layer(dim)\n        self.mlp = mlp_layer(\n            in_features=dim,\n            hidden_features=int(dim * mlp_ratio),\n            act_layer=act_layer,\n            drop=proj_drop,\n        )\n        self.ls2 = (\n            LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n        )\n        self.drop_path2 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n        return x\n\n\nclass VisionTransformer(nn.Module):\n    \"\"\"Vision Transformer\n\n    A PyTorch impl of : `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale`\n        - https://arxiv.org/abs/2010.11929\n    \"\"\"\n\n    dynamic_img_size: Final[bool]\n\n    def __init__(\n        self,\n        img_size: Union[int, Tuple[int, int]] = 224,\n        patch_size: Union[int, Tuple[int, int]] = 16,\n        in_chans: int = 3,\n        num_classes: int = 1000,\n        global_pool: Literal[\"\", \"avg\", \"token\", \"map\"] = \"token\",\n        embed_dim: int = 768,\n        depth: int = 12,\n        num_heads: int = 12,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = True,\n        qk_norm: bool = False,\n        init_values: Optional[float] = None,\n        class_token: bool = True,\n        no_embed_class: bool = False,\n        reg_tokens: int = 0,\n        pre_norm: bool = False,\n        fc_norm: Optional[bool] = None,\n        dynamic_img_size: bool = False,\n        dynamic_img_pad: bool = False,\n        drop_rate: float = 0.0,\n        pos_drop_rate: float = 0.0,\n        patch_drop_rate: float = 0.0,\n        proj_drop_rate: float = 0.0,\n        attn_drop_rate: float = 0.0,\n        drop_path_rate: float = 0.0,\n        weight_init: Literal[\"skip\", \"jax\", \"jax_nlhb\", \"moco\", \"\"] = \"\",\n        embed_layer: Callable = PatchEmbed,\n        norm_layer: Optional[LayerType] = None,\n        act_layer: Optional[LayerType] = None,\n        block_fn: Type[nn.Module] = Block,\n        mlp_layer: Type[nn.Module] = Mlp,\n        ignore_head: bool = False,\n    ) -> None:\n        \"\"\"\n        Args:\n            img_size: Input image size.\n            patch_size: Patch size.\n            in_chans: Number of image input channels.\n            num_classes: Mumber of classes for classification head.\n            global_pool: Type of global pooling for final sequence (default: 'token').\n            embed_dim: Transformer embedding dimension.\n            depth: Depth of transformer.\n            num_heads: Number of attention heads.\n            mlp_ratio: Ratio of mlp hidden dim to embedding dim.\n            qkv_bias: Enable bias for qkv projections if True.\n            init_values: Layer-scale init values (layer-scale enabled if not None).\n            class_token: Use class token.\n            no_embed_class: Don't include position embeddings for class (or reg) tokens.\n            reg_tokens: Number of register tokens.\n            fc_norm: Pre head norm after pool (instead of before), if None, enabled when global_pool == 'avg'.\n            drop_rate: Head dropout rate.\n            pos_drop_rate: Position embedding dropout rate.\n            attn_drop_rate: Attention dropout rate.\n            drop_path_rate: Stochastic depth rate.\n            weight_init: Weight initialization scheme.\n            embed_layer: Patch embedding layer.\n            norm_layer: Normalization layer.\n            act_layer: MLP activation layer.\n            block_fn: Transformer block layer.\n        \"\"\"\n        super().__init__()\n        assert global_pool in (\"\", \"avg\", \"token\", \"map\")\n        assert class_token or global_pool != \"token\"\n        use_fc_norm = global_pool == \"avg\" if fc_norm is None else fc_norm\n        # norm_layer = get_norm_layer(norm_layer) or partial(nn.LayerNorm, eps=1e-6)\n        # act_layer = get_act_layer(act_layer) or nn.GELU\n        norm_layer = partial(nn.LayerNorm, eps=1e-6)\n        act_layer = nn.GELU\n\n        self.num_classes = num_classes\n        self.global_pool = global_pool\n        self.num_features = self.embed_dim = (\n            embed_dim  # num_features for consistency with other models\n        )\n        self.num_prefix_tokens = 1 if class_token else 0\n        self.num_prefix_tokens += reg_tokens\n        self.num_reg_tokens = reg_tokens\n        self.has_class_token = class_token\n        self.no_embed_class = (\n            no_embed_class  # don't embed prefix positions (includes reg)\n        )\n        self.dynamic_img_size = dynamic_img_size\n        self.grad_checkpointing = False\n        self.ignore_head = ignore_head\n\n        embed_args = {}\n        if dynamic_img_size:\n            # flatten deferred until after pos embed\n            embed_args.update(dict(strict_img_size=False, output_fmt=\"NHWC\"))\n        self.patch_embed = embed_layer(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n            bias=not pre_norm,  # disable bias if pre-norm is used (e.g. CLIP)\n            dynamic_img_pad=dynamic_img_pad,\n            **embed_args,\n        )\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = (\n            nn.Parameter(torch.zeros(1, 1, embed_dim)) if class_token else None\n        )\n        self.reg_token = (\n            nn.Parameter(torch.zeros(1, reg_tokens, embed_dim)) if reg_tokens else None\n        )\n        embed_len = (\n            num_patches if no_embed_class else num_patches + self.num_prefix_tokens\n        )\n        self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * 0.02)\n        self.pos_drop = nn.Dropout(p=pos_drop_rate)\n        if patch_drop_rate > 0:\n            self.patch_drop = PatchDropout(\n                patch_drop_rate,\n                num_prefix_tokens=self.num_prefix_tokens,\n            )\n        else:\n            self.patch_drop = nn.Identity()\n        self.norm_pre = norm_layer(embed_dim) if pre_norm else nn.Identity()\n\n        dpr = [\n            x.item() for x in torch.linspace(0, drop_path_rate, depth)\n        ]  # stochastic depth decay rule\n        self.blocks = nn.Sequential(\n            *[\n                block_fn(\n                    dim=embed_dim,\n                    num_heads=num_heads,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    qk_norm=qk_norm,\n                    init_values=init_values,\n                    proj_drop=proj_drop_rate,\n                    attn_drop=attn_drop_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                    act_layer=act_layer,\n                    mlp_layer=mlp_layer,\n                )\n                for i in range(depth)\n            ]\n        )\n        self.norm = norm_layer(embed_dim) if not use_fc_norm else nn.Identity()\n\n        # Classifier Head\n        if global_pool == \"map\":\n            AttentionPoolLatent.init_weights = init_weights\n            self.attn_pool = AttentionPoolLatent(\n                self.embed_dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                norm_layer=norm_layer,\n            )\n        else:\n            self.attn_pool = None\n        self.fc_norm = norm_layer(embed_dim) if use_fc_norm else nn.Identity()\n        self.head_drop = nn.Dropout(drop_rate)\n        self.head = (\n            nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n        )\n\n        if weight_init != \"skip\":\n            self.init_weights(weight_init)\n\n    def init_weights(self, mode: Literal[\"jax\", \"jax_nlhb\", \"moco\", \"\"] = \"\") -> None:\n        assert mode in (\"jax\", \"jax_nlhb\", \"moco\", \"\")\n        # head_bias = -math.log(self.num_classes) if \"nlhb\" in mode else 0.0\n        trunc_normal_(self.pos_embed, std=0.02)\n        if self.cls_token is not None:\n            nn.init.normal_(self.cls_token, std=1e-6)\n        named_apply(init_weights_vit_timm, self)\n\n    @torch.jit.ignore\n    def no_weight_decay(self) -> Set:\n        return {\"pos_embed\", \"cls_token\", \"dist_token\"}\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse: bool = False) -> Dict:\n        return dict(\n            stem=r\"^cls_token|pos_embed|patch_embed\",  # stem and embed\n            blocks=[(r\"^blocks\\.(\\d+)\", None), (r\"^norm\", (99999,))],\n        )\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable: bool = True) -> None:\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def get_classifier(self) -> nn.Module:\n        return self.head\n\n    def reset_classifier(self, num_classes: int, global_pool=None) -> None:\n        self.num_classes = num_classes\n        if global_pool is not None:\n            assert global_pool in (\"\", \"avg\", \"token\", \"map\")\n            if global_pool == \"map\" and self.attn_pool is None:\n                assert (\n                    False\n                ), \"Cannot currently add attention pooling in reset_classifier().\"\n            elif global_pool != \"map \" and self.attn_pool is not None:\n                self.attn_pool = None  # remove attention pooling\n            self.global_pool = global_pool\n        self.head = (\n            nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n        )\n\n    def _pos_embed(self, x: torch.Tensor) -> torch.Tensor:\n        if self.dynamic_img_size:\n            B, H, W, C = x.shape\n            pos_embed = resample_abs_pos_embed(\n                self.pos_embed,\n                (H, W),\n                num_prefix_tokens=0 if self.no_embed_class else self.num_prefix_tokens,\n            )\n            x = x.view(B, -1, C)\n        else:\n            pos_embed = self.pos_embed\n\n        to_cat = []\n        if self.cls_token is not None:\n            to_cat.append(self.cls_token.expand(x.shape[0], -1, -1))\n        if self.reg_token is not None:\n            to_cat.append(self.reg_token.expand(x.shape[0], -1, -1))\n\n        if self.no_embed_class:\n            # deit-3, updated JAX (big vision)\n            # position embedding does not overlap with class token, add then concat\n            x = x + pos_embed\n            if to_cat:\n                x = torch.cat(to_cat + [x], dim=1)\n        else:\n            # original timm, JAX, and deit vit impl\n            # pos_embed has entry for class token, concat then add\n            if to_cat:\n                x = torch.cat(to_cat + [x], dim=1)\n            x = x + pos_embed\n\n        return self.pos_drop(x)\n\n    def _intermediate_layers(\n        self,\n        x: torch.Tensor,\n        n: Union[int, Sequence] = 1,\n    ) -> List[torch.Tensor]:\n        outputs, num_blocks = [], len(self.blocks)\n        take_indices = set(\n            range(num_blocks - n, num_blocks) if isinstance(n, int) else n\n        )\n\n        # forward pass\n        x = self.patch_embed(x)\n        x = self._pos_embed(x)\n        x = self.patch_drop(x)\n        x = self.norm_pre(x)\n        for i, blk in enumerate(self.blocks):\n            x = blk(x)\n            if i in take_indices:\n                outputs.append(x)\n\n        return outputs\n\n    def get_intermediate_layers(\n        self,\n        x: torch.Tensor,\n        n: Union[int, Sequence] = 1,\n        reshape: bool = False,\n        return_prefix_tokens: bool = False,\n        norm: bool = False,\n    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]]]:\n        \"\"\"Intermediate layer accessor (NOTE: This is a WIP experiment).\n        Inspired by DINO / DINOv2 interface\n        \"\"\"\n        # take last n blocks if n is an int, if in is a sequence, select by matching indices\n        outputs = self._intermediate_layers(x, n)\n        if norm:\n            outputs = [self.norm(out) for out in outputs]\n        prefix_tokens = [out[:, 0 : self.num_prefix_tokens] for out in outputs]\n        outputs = [out[:, self.num_prefix_tokens :] for out in outputs]\n\n        if reshape:\n            grid_size = self.patch_embed.grid_size\n            outputs = [\n                out.reshape(x.shape[0], grid_size[0], grid_size[1], -1)\n                .permute(0, 3, 1, 2)\n                .contiguous()\n                for out in outputs\n            ]\n\n        if return_prefix_tokens:\n            return tuple(zip(outputs, prefix_tokens))\n        return tuple(outputs)\n\n    def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.patch_embed(x)\n        x = self._pos_embed(x)\n        x = self.patch_drop(x)\n        x = self.norm_pre(x)\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.blocks, x)\n        else:\n            x = self.blocks(x)\n        x = self.norm(x)\n        return x\n\n    def forward_head(self, x: torch.Tensor, pre_logits: bool = False) -> torch.Tensor:\n        if self.attn_pool is not None:\n            x = self.attn_pool(x)\n        elif self.global_pool == \"avg\":\n            x = x[:, self.num_prefix_tokens :].mean(dim=1)\n        elif self.global_pool:\n            x = x[:, 0]  # class token\n        x = self.fc_norm(x)\n        x = self.head_drop(x)\n        return x if pre_logits else self.head(x)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.forward_features(x)\n        if not self.ignore_head:\n            x = self.forward_head(x)\n        return x\n\n\n@dataclass\nclass SigLIPVisionCfg:\n    width: int = 1152\n    layers: Union[Tuple[int, int, int, int], int] = 27\n    heads: int = 16\n    patch_size: int = 14\n    image_size: Union[Tuple[int, int], int] = 336\n    global_pool: str = \"map\"\n    mlp_ratio: float = 3.7362\n    class_token: bool = False\n    num_classes: int = 0\n    use_checkpoint: bool = False\n\n\nSigLIP_MODEL_CONFIG = {\n    \"siglip_so400m_patch14_384\": {\n        \"image_size\": 336,\n        \"patch_size\": 14,\n        \"width\": 1152,\n        \"layers\": 27,\n        \"heads\": 16,\n        \"mlp_ratio\": 3.7362,\n        \"global_pool\": \"map\",\n        \"use_checkpoint\": False,\n    },\n    \"siglip_so400m_patch14_224\": {\n        \"image_size\": 224,\n        \"patch_size\": 14,\n        \"width\": 1152,\n        \"layers\": 27,\n        \"heads\": 16,\n        \"mlp_ratio\": 3.7362,\n        \"global_pool\": \"map\",\n        \"use_checkpoint\": False,\n    },\n    \"siglip_large_patch16_384\": {\n        \"image_size\": 384,\n        \"patch_size\": 16,\n        \"width\": 1024,\n        \"layers\": 24,\n        \"heads\": 16,\n        \"mlp_ratio\": 4,\n        \"global_pool\": \"map\",\n        \"use_checkpoint\": False,\n    },\n    \"siglip_large_patch16_256\": {\n        \"image_size\": 256,\n        \"patch_size\": 16,\n        \"width\": 1024,\n        \"layers\": 24,\n        \"heads\": 16,\n        \"mlp_ratio\": 4,\n        \"global_pool\": \"map\",\n        \"use_checkpoint\": False,\n    },\n}\n\n\ndef create_siglip_vit(\n    model_name: str = \"siglip_so400m_patch14_384\",\n    image_size: int = 384,\n    select_layer: int = -1,\n    ckpt_path: str = \"\",\n    **kwargs,\n):\n    assert (\n        model_name in SigLIP_MODEL_CONFIG.keys()\n    ), f\"model name should be in {SigLIP_MODEL_CONFIG.keys()}\"\n\n    vision_cfg = SigLIPVisionCfg(**SigLIP_MODEL_CONFIG[model_name])\n\n    if select_layer <= 0:\n        layers = min(vision_cfg.layers, vision_cfg.layers + select_layer + 1)\n    else:\n        layers = min(vision_cfg.layers, select_layer)\n\n    model = VisionTransformer(\n        img_size=image_size,\n        patch_size=vision_cfg.patch_size,\n        embed_dim=vision_cfg.width,\n        depth=layers,\n        num_heads=vision_cfg.heads,\n        mlp_ratio=vision_cfg.mlp_ratio,\n        class_token=vision_cfg.class_token,\n        global_pool=vision_cfg.global_pool,\n        ignore_head=kwargs.get(\"ignore_head\", True),\n        weight_init=kwargs.get(\"weight_init\", \"skip\"),\n        num_classes=0,\n    )\n\n    if ckpt_path:\n        state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n\n        incompatible_keys = model.load_state_dict(state_dict, strict=False)\n        print(\n            f\"SigLIP-ViT restores from {ckpt_path},\\n\"\n            f\"\\tincompatible_keys:', {incompatible_keys}.\"\n        )\n\n    return model\n"}
{"type": "source_file", "path": "three_party/Janus/janus/models/image_processing_vlm.py", "content": "# Copyright (c) 2023-2024 DeepSeek.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the \"Software\"), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n# the Software, and to permit persons to whom the Software is furnished to do so,\n# subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom typing import List, Tuple, Union\n\nimport numpy as np\nimport torch\nimport torchvision\nimport torchvision.transforms.functional\nfrom PIL import Image\nfrom transformers import AutoImageProcessor, PretrainedConfig\nfrom transformers.image_processing_utils import BaseImageProcessor, BatchFeature\nfrom transformers.image_utils import to_numpy_array\nfrom transformers.utils import logging\n\nlogger = logging.get_logger(__name__)\n\nImageType = Union[np.ndarray, torch.Tensor, Image.Image]\nIMAGENET_MEAN = (0.48145466, 0.4578275, 0.40821073)\nIMAGENET_STD = (0.26862954, 0.26130258, 0.27577711)\nIMAGENET_INCEPTION_MEAN = (0.5, 0.5, 0.5)\nIMAGENET_INCEPTION_STD = (0.5, 0.5, 0.5)\n\n\ndef expand2square(pil_img, background_color):\n    width, height = pil_img.size\n    if width == height:\n        return pil_img\n    elif width > height:\n        result = Image.new(pil_img.mode, (width, width), background_color)\n        result.paste(pil_img, (0, (width - height) // 2))\n        return result\n    else:\n        result = Image.new(pil_img.mode, (height, height), background_color)\n        result.paste(pil_img, ((height - width) // 2, 0))\n        return result\n\n\nclass VLMImageProcessorConfig(PretrainedConfig):\n    model_type = \"deepseek_vlm\"\n    image_size: int\n    min_size: int\n    image_mean: Union[Tuple[float, float, float], List[float]]\n    image_std: Union[Tuple[float, float, float], List[float]]\n    rescale_factor: float\n    do_normalize: bool\n\n    def __init__(\n        self,\n        image_size: int,\n        min_size: int = 14,\n        image_mean: Union[Tuple[float, float, float], List[float]] = (\n            0.48145466,\n            0.4578275,\n            0.40821073,\n        ),\n        image_std: Union[Tuple[float, float, float], List[float]] = (\n            0.26862954,\n            0.26130258,\n            0.27577711,\n        ),\n        rescale_factor: float = 1.0 / 255.0,\n        do_normalize: bool = True,\n        **kwargs,\n    ):\n        self.image_size = image_size\n        self.min_size = min_size\n        self.image_mean = image_mean\n        self.image_std = image_std\n        self.rescale_factor = rescale_factor\n        self.do_normalize = do_normalize\n\n        super().__init__(**kwargs)\n\n\nclass VLMImageProcessor(BaseImageProcessor):\n    model_input_names = [\"pixel_values\"]\n\n    def __init__(\n        self,\n        image_size: int,\n        min_size: int = 14,\n        image_mean: Union[Tuple[float, float, float], List[float]] = (\n            0.48145466,\n            0.4578275,\n            0.40821073,\n        ),\n        image_std: Union[Tuple[float, float, float], List[float]] = (\n            0.26862954,\n            0.26130258,\n            0.27577711,\n        ),\n        rescale_factor: float = 1.0 / 255.0,\n        do_normalize: bool = True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n\n        self.image_size = image_size\n        self.rescale_factor = rescale_factor\n        self.image_mean = image_mean\n        self.image_std = image_std\n        self.min_size = min_size\n        self.do_normalize = do_normalize\n\n        if image_mean is None:\n            self.background_color = (127, 127, 127)\n        else:\n            self.background_color = tuple([int(x * 255) for x in image_mean])\n\n    def resize(self, pil_img: Image) -> np.ndarray:\n        \"\"\"\n\n        Args:\n            pil_img (PIL.Image): [H, W, 3] in PIL.Image in RGB\n\n        Returns:\n            x (np.ndarray): [3, self.image_size, self.image_size]\n        \"\"\"\n\n        width, height = pil_img.size\n        max_size = max(width, height)\n\n        size = [\n            max(int(height / max_size * self.image_size), self.min_size),\n            max(int(width / max_size * self.image_size), self.min_size),\n        ]\n\n        if width <= 0 or height <= 0 or size[0] <= 0 or size[1] <= 0:\n            print(f\"orig size = {pil_img.size}, new size = {size}\")\n            raise ValueError(\"Invalid size!\")\n\n        pil_img = torchvision.transforms.functional.resize(\n            pil_img,\n            size,\n            interpolation=torchvision.transforms.functional.InterpolationMode.BICUBIC,\n            antialias=True,\n        )\n\n        pil_img = expand2square(pil_img, self.background_color)\n        x = to_numpy_array(pil_img)\n\n        # [H, W, 3] -> [3, H, W]\n        x = np.transpose(x, (2, 0, 1))\n\n        return x\n\n    def preprocess(self, images, return_tensors: str = \"pt\", **kwargs) -> BatchFeature:\n        # resize and pad to [self.image_size, self.image_size]\n        # then convert from [H, W, 3] to [3, H, W]\n        images: List[np.ndarray] = [self.resize(image) for image in images]\n\n        # resacle from [0, 255] -> [0, 1]\n        images = [\n            self.rescale(\n                image=image,\n                scale=self.rescale_factor,\n                input_data_format=\"channels_first\",\n            )\n            for image in images\n        ]\n\n        # normalize\n        if self.do_normalize:\n            images = [\n                self.normalize(\n                    image=image,\n                    mean=self.image_mean,\n                    std=self.image_std,\n                    input_data_format=\"channels_first\",\n                )\n                for image in images\n            ]\n\n        data = {\"pixel_values\": images}\n        return BatchFeature(data=data, tensor_type=return_tensors)\n\n    @property\n    def default_shape(self):\n        return [3, self.image_size, self.image_size]\n\n\nAutoImageProcessor.register(VLMImageProcessorConfig, VLMImageProcessor)\n\n\nif __name__ == \"__main__\":\n    image_processor = VLMImageProcessor(\n        image_size=1024,\n        image_mean=IMAGENET_INCEPTION_MEAN,\n        image_std=IMAGENET_INCEPTION_STD,\n        do_normalize=True,\n    )\n"}
{"type": "source_file", "path": "three_party/Janus/janus/models/clip_encoder.py", "content": "# Copyright (c) 2023-2024 DeepSeek.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the \"Software\"), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n# the Software, and to permit persons to whom the Software is furnished to do so,\n# subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom typing import Dict, List, Literal, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms\nfrom einops import rearrange\n\nfrom janus.models.siglip_vit import create_siglip_vit\n\n\nclass CLIPVisionTower(nn.Module):\n    def __init__(\n        self,\n        model_name: str = \"siglip_large_patch16_384\",\n        image_size: Union[Tuple[int, int], int] = 336,\n        select_feature: str = \"patch\",\n        select_layer: int = -2,\n        select_layers: list = None,\n        ckpt_path: str = \"\",\n        pixel_mean: Optional[List[float]] = None,\n        pixel_std: Optional[List[float]] = None,\n        **kwargs,\n    ):\n        super().__init__()\n\n        self.model_name = model_name\n        self.select_feature = select_feature\n        self.select_layer = select_layer\n        self.select_layers = select_layers\n\n        vision_tower_params = {\n            \"model_name\": model_name,\n            \"image_size\": image_size,\n            \"ckpt_path\": ckpt_path,\n            \"select_layer\": select_layer,\n        }\n        vision_tower_params.update(kwargs)\n        self.vision_tower, self.forward_kwargs = self.build_vision_tower(\n            vision_tower_params\n        )\n\n        if pixel_mean is not None and pixel_std is not None:\n            image_norm = torchvision.transforms.Normalize(\n                mean=pixel_mean, std=pixel_std\n            )\n        else:\n            image_norm = None\n\n        self.image_norm = image_norm\n\n    def build_vision_tower(self, vision_tower_params):\n        if self.model_name.startswith(\"siglip\"):\n            self.select_feature = \"same\"\n            vision_tower = create_siglip_vit(**vision_tower_params)\n            forward_kwargs = dict()\n\n        elif self.model_name.startswith(\"sam\"):\n            vision_tower = create_sam_vit(**vision_tower_params)\n            forward_kwargs = dict()\n\n        else:  # huggingface\n            from transformers import CLIPVisionModel\n\n            vision_tower = CLIPVisionModel.from_pretrained(**vision_tower_params)\n            forward_kwargs = dict(output_hidden_states=True)\n\n        return vision_tower, forward_kwargs\n\n    def feature_select(self, image_forward_outs):\n        if isinstance(image_forward_outs, torch.Tensor):\n            # the output has been the self.select_layer\"s features\n            image_features = image_forward_outs\n        else:\n            image_features = image_forward_outs.hidden_states[self.select_layer]\n\n        if self.select_feature == \"patch\":\n            # if the output has cls_token\n            image_features = image_features[:, 1:]\n        elif self.select_feature == \"cls_patch\":\n            image_features = image_features\n        elif self.select_feature == \"same\":\n            image_features = image_features\n\n        else:\n            raise ValueError(f\"Unexpected select feature: {self.select_feature}\")\n        return image_features\n\n    def forward(self, images):\n        \"\"\"\n\n        Args:\n            images (torch.Tensor): [b, 3, H, W]\n\n        Returns:\n            image_features (torch.Tensor): [b, n_patch, d]\n        \"\"\"\n\n        if self.image_norm is not None:\n            images = self.image_norm(images)\n\n        image_forward_outs = self.vision_tower(images, **self.forward_kwargs)\n        image_features = self.feature_select(image_forward_outs)\n        return image_features\n"}
{"type": "source_file", "path": "three_party/Janus/janus/janusflow/models/uvit.py", "content": "# Copyright (c) 2023-2024 DeepSeek.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the \"Software\"), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n# the Software, and to permit persons to whom the Software is furnished to do so,\n# subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n# modified from: https://github.com/lucidrains/denoising-diffusion-pytorch/blob/main/denoising_diffusion_pytorch/simple_diffusion.py\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.distributed as dist\nimport torch.nn.functional as F\nfrom typing import Optional, Tuple, Union\n\nimport numpy as np\nimport torchvision\nimport torchvision.utils\nfrom diffusers.models.embeddings import Timesteps, TimestepEmbedding\nfrom transformers.models.llama.modeling_llama import LlamaRMSNorm as RMSNorm\n\n\nclass ImageHead(nn.Module):\n\n    def __init__(self, decoder_cfg, gpt_cfg, layer_id=None):\n        super().__init__()\n        self.layer_id = layer_id\n        cfg = (\n            AttrDict(\n                norm_type=\"layernorm\",\n                is_exp_norm=False,\n                sequence_parallel=False,\n                use_userbuffer=False,\n                norm_eps=1e-5,\n                norm_bias=True,\n                gradient_accumulation_fusion=True,\n                use_fp32_head_weight=False,\n            )\n            + gpt_cfg\n        )\n        group = PG.tensor_parallel_group()\n        assert cfg.norm_type in [\n            \"layernorm\",\n            \"rmsnorm\",\n        ], f\"Norm type:{cfg.norm_type} not supported\"\n        if cfg.norm_type == \"rmsnorm\":\n            self.norm = DropoutAddRMSNorm(\n                cfg.n_embed,\n                prenorm=False,\n                eps=cfg.norm_eps,\n                is_exp_norm=cfg.is_exp_norm,\n                sequence_parallel=cfg.sequence_parallel,\n            )\n        else:\n            self.norm = DropoutAddLayerNorm(\n                cfg.n_embed,\n                prenorm=False,\n                eps=cfg.norm_eps,\n                is_exp_norm=cfg.is_exp_norm,\n                sequence_parallel=cfg.sequence_parallel,\n                bias=cfg.norm_bias,\n            )\n\n        multiple_of = 256\n        if decoder_cfg.in_channels % multiple_of != 0:\n            warnings.warn(\n                f\"建议把 vocab_size 设置为 {multiple_of} 的倍数, 否则会影响矩阵乘法的性能\"\n            )\n\n        dtype = default_dtype = torch.get_default_dtype()\n        if cfg.use_fp32_head_weight:\n            dtype = torch.float32\n            print(\n                \"使用 fp32 head weight!!!! 与原来的 bf16 head weight 不兼容\\n\",\n                end=\"\",\n                flush=True,\n            )\n        torch.set_default_dtype(dtype)\n        self.head = ColumnParallelLinear(\n            cfg.n_embed,\n            decoder_cfg.in_channels,\n            bias=True,\n            group=group,\n            sequence_parallel=cfg.sequence_parallel,\n            use_userbuffer=cfg.use_userbuffer,\n            gradient_accumulation_fusion=cfg.gradient_accumulation_fusion,\n            use_fp32_output=False,\n        )\n        torch.set_default_dtype(default_dtype)\n\n        self.use_fp32_head_weight = cfg.use_fp32_head_weight\n\n    def forward(\n        self, input_args, images_split_mask: Optional[torch.BoolTensor] = None, **kwargs\n    ):\n        residual = None\n        if isinstance(input_args, tuple):\n            x, residual = input_args\n        else:\n            x = input_args\n\n        x = self.norm(x, residual)\n\n        if self.use_fp32_head_weight:\n            assert (\n                self.head.weight.dtype == torch.float32\n            ), f\"head.weight is {self.head.weight.dtype}\"\n            x = x.float()\n\n        if images_split_mask is None:\n            logits = self.head(x)\n        else:\n            bs, n_images = images_split_mask.shape[:2]\n            n_embed = x.shape[-1]\n\n            images_embed = torch.masked_select(\n                x.unsqueeze(1), images_split_mask.unsqueeze(-1)\n            )\n            images_embed = images_embed.view((bs * n_images, -1, n_embed))\n            logits = self.head(images_embed)\n\n        return logits\n\n\nclass GlobalResponseNorm(nn.Module):\n    # Taken from https://github.com/facebookresearch/ConvNeXt-V2/blob/3608f67cc1dae164790c5d0aead7bf2d73d9719b/models/utils.py#L105\n    def __init__(self, dim):\n        super().__init__()\n        self.weight = nn.Parameter(torch.zeros(1, 1, 1, dim))\n        self.bias = nn.Parameter(torch.zeros(1, 1, 1, dim))\n\n    def forward(self, x):\n        gx = torch.norm(x, p=2, dim=(1, 2), keepdim=True)\n        nx = gx / (gx.mean(dim=-1, keepdim=True) + 1e-6)\n\n        return torch.addcmul(self.bias, (self.weight * nx + 1), x, value=1)\n\n\nclass Downsample2D(nn.Module):\n    \"\"\"A 2D downsampling layer with an optional convolution.\n\n    Parameters:\n        channels (`int`):\n            number of channels in the inputs and outputs.\n        use_conv (`bool`, default `False`):\n            option to use a convolution.\n        out_channels (`int`, optional):\n            number of output channels. Defaults to `channels`.\n        padding (`int`, default `1`):\n            padding for the convolution.\n        name (`str`, default `conv`):\n            name of the downsampling 2D layer.\n    \"\"\"\n\n    def __init__(\n        self,\n        channels: int,\n        use_conv: bool = False,\n        out_channels: Optional[int] = None,\n        padding: int = 1,\n        name: str = \"conv\",\n        kernel_size=3,\n        stride=2,\n        norm_type=None,\n        eps=None,\n        elementwise_affine=None,\n        bias=True,\n    ):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.padding = padding\n        self.name = name\n\n        if norm_type == \"ln_norm\":\n            self.norm = nn.LayerNorm(channels, eps, elementwise_affine)\n        elif norm_type == \"rms_norm\":\n            self.norm = RMSNorm(channels, eps)\n        elif norm_type is None:\n            self.norm = None\n        else:\n            raise ValueError(f\"unknown norm_type: {norm_type}\")\n\n        if use_conv:\n            conv = nn.Conv2d(\n                self.channels,\n                self.out_channels,\n                kernel_size=kernel_size,\n                stride=stride,\n                padding=padding,\n                bias=bias,\n            )\n        else:\n            assert self.channels == self.out_channels\n            conv = nn.AvgPool2d(kernel_size=stride, stride=stride)\n\n        # TODO(Suraj, Patrick) - clean up after weight dicts are correctly renamed\n        if name == \"conv\":\n            self.Conv2d_0 = conv\n            self.conv = conv\n        elif name == \"Conv2d_0\":\n            self.conv = conv\n        else:\n            self.conv = conv\n\n    def forward(self, hidden_states: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n\n        assert hidden_states.shape[1] == self.channels\n\n        if self.norm is not None:\n            hidden_states = self.norm(hidden_states.permute(0, 2, 3, 1)).permute(\n                0, 3, 1, 2\n            )\n\n        if self.use_conv and self.padding == 0:\n            pad = (0, 1, 0, 1)\n            hidden_states = F.pad(hidden_states, pad, mode=\"constant\", value=0)\n\n        assert hidden_states.shape[1] == self.channels\n\n        hidden_states = self.conv(hidden_states)\n\n        return hidden_states\n\n\nclass Upsample2D(nn.Module):\n    \"\"\"A 2D upsampling layer with an optional convolution.\n\n    Parameters:\n        channels (`int`):\n            number of channels in the inputs and outputs.\n        use_conv (`bool`, default `False`):\n            option to use a convolution.\n        use_conv_transpose (`bool`, default `False`):\n            option to use a convolution transpose.\n        out_channels (`int`, optional):\n            number of output channels. Defaults to `channels`.\n        name (`str`, default `conv`):\n            name of the upsampling 2D layer.\n    \"\"\"\n\n    def __init__(\n        self,\n        channels: int,\n        use_conv: bool = False,\n        use_conv_transpose: bool = False,\n        out_channels: Optional[int] = None,\n        name: str = \"conv\",\n        kernel_size: Optional[int] = None,\n        padding=1,\n        stride=2,\n        norm_type=None,\n        eps=None,\n        elementwise_affine=None,\n        bias=True,\n        interpolate=True,\n    ):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.use_conv_transpose = use_conv_transpose\n        self.name = name\n        self.interpolate = interpolate\n        self.stride = stride\n\n        if norm_type == \"ln_norm\":\n            self.norm = nn.LayerNorm(channels, eps, elementwise_affine)\n        elif norm_type == \"rms_norm\":\n            self.norm = RMSNorm(channels, eps)\n        elif norm_type is None:\n            self.norm = None\n        else:\n            raise ValueError(f\"unknown norm_type: {norm_type}\")\n\n        conv = None\n        if use_conv_transpose:\n            if kernel_size is None:\n                kernel_size = 4\n            conv = nn.ConvTranspose2d(\n                channels,\n                self.out_channels,\n                kernel_size=kernel_size,\n                stride=stride,\n                padding=padding,\n                bias=bias,\n            )\n        elif use_conv:\n            if kernel_size is None:\n                kernel_size = 3\n            conv = nn.Conv2d(\n                self.channels,\n                self.out_channels,\n                kernel_size=kernel_size,\n                padding=padding,\n                bias=bias,\n            )\n\n        # TODO(Suraj, Patrick) - clean up after weight dicts are correctly renamed\n        if name == \"conv\":\n            self.conv = conv\n        else:\n            self.Conv2d_0 = conv\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        output_size: Optional[int] = None,\n        *args,\n        **kwargs,\n    ) -> torch.Tensor:\n\n        assert hidden_states.shape[1] == self.channels\n\n        if self.norm is not None:\n            hidden_states = self.norm(hidden_states.permute(0, 2, 3, 1)).permute(\n                0, 3, 1, 2\n            )\n\n        if self.use_conv_transpose:\n            return self.conv(hidden_states)\n\n        # Cast to float32 to as 'upsample_nearest2d_out_frame' op does not support bfloat16\n        # TODO(Suraj): Remove this cast once the issue is fixed in PyTorch\n        # https://github.com/pytorch/pytorch/issues/86679\n        dtype = hidden_states.dtype\n        if dtype == torch.bfloat16:\n            hidden_states = hidden_states.to(torch.float32)\n\n        # upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984\n        if hidden_states.shape[0] >= 64:\n            hidden_states = hidden_states.contiguous()\n\n        # if `output_size` is passed we force the interpolation output\n        # size and do not make use of `scale_factor=2`\n        if self.interpolate:\n            if output_size is None:\n                hidden_states = F.interpolate(\n                    hidden_states, scale_factor=self.stride, mode=\"nearest\"\n                )\n            else:\n                hidden_states = F.interpolate(\n                    hidden_states, size=output_size, mode=\"nearest\"\n                )\n\n        # If the input is bfloat16, we cast back to bfloat16\n        if dtype == torch.bfloat16:\n            hidden_states = hidden_states.to(dtype)\n\n        # TODO(Suraj, Patrick) - clean up after weight dicts are correctly renamed\n        if self.use_conv:\n            if self.name == \"conv\":\n                hidden_states = self.conv(hidden_states)\n            else:\n                hidden_states = self.Conv2d_0(hidden_states)\n\n        return hidden_states\n\n\nclass ConvNextBlock(nn.Module):\n    def __init__(\n        self,\n        channels,\n        norm_eps,\n        elementwise_affine,\n        use_bias,\n        hidden_dropout,\n        hidden_size,\n        res_ffn_factor: int = 4,\n    ):\n        super().__init__()\n        self.depthwise = nn.Conv2d(\n            channels,\n            channels,\n            kernel_size=7,\n            padding=3,\n            groups=channels,\n            bias=use_bias,\n        )\n        self.norm = RMSNorm(channels, norm_eps)\n        self.channelwise_linear_1 = nn.Linear(\n            channels, int(channels * res_ffn_factor), bias=use_bias\n        )\n        self.channelwise_act = nn.GELU()\n        self.channelwise_norm = GlobalResponseNorm(int(channels * res_ffn_factor))\n        self.channelwise_linear_2 = nn.Linear(\n            int(channels * res_ffn_factor), channels, bias=use_bias\n        )\n        self.channelwise_dropout = nn.Dropout(hidden_dropout)\n        self.cond_embeds_mapper = nn.Linear(hidden_size, channels * 2, use_bias)\n\n    def forward(self, x, cond_embeds):\n        x_res = x\n\n        x = self.depthwise(x)\n\n        x = x.permute(0, 2, 3, 1)\n        x = self.norm(x)\n        x = self.channelwise_linear_1(x)\n        x = self.channelwise_act(x)\n        x = self.channelwise_norm(x)\n        x = self.channelwise_linear_2(x)\n        x = self.channelwise_dropout(x)\n        x = x.permute(0, 3, 1, 2)\n\n        x = x + x_res\n\n        scale, shift = self.cond_embeds_mapper(F.silu(cond_embeds)).chunk(2, dim=1)\n        # x = x * (1 + scale[:, :, None, None]) + shift[:, :, None, None]\n        x = torch.addcmul(\n            shift[:, :, None, None], x, (1 + scale)[:, :, None, None], value=1\n        )\n\n        return x\n\n\nclass Patchify(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        block_out_channels,\n        patch_size,\n        bias,\n        elementwise_affine,\n        eps,\n        kernel_size=None,\n    ):\n        super().__init__()\n        if kernel_size is None:\n            kernel_size = patch_size\n        self.patch_conv = nn.Conv2d(\n            in_channels,\n            block_out_channels,\n            kernel_size=kernel_size,\n            stride=patch_size,\n            bias=bias,\n        )\n        self.norm = RMSNorm(block_out_channels, eps)\n\n    def forward(self, x):\n        embeddings = self.patch_conv(x)\n        embeddings = embeddings.permute(0, 2, 3, 1)\n        embeddings = self.norm(embeddings)\n        embeddings = embeddings.permute(0, 3, 1, 2)\n        return embeddings\n\n\nclass Unpatchify(nn.Module):\n    def __init__(\n        self, in_channels, out_channels, patch_size, bias, elementwise_affine, eps\n    ):\n        super().__init__()\n        self.norm = RMSNorm(in_channels, eps)\n        self.unpatch_conv = nn.Conv2d(\n            in_channels,\n            out_channels * patch_size * patch_size,\n            kernel_size=1,\n            bias=bias,\n        )\n        self.pixel_shuffle = nn.PixelShuffle(patch_size)\n        self.patch_size = patch_size\n\n    def forward(self, x):\n        # [b, c, h, w]\n        x = x.permute(0, 2, 3, 1)\n        x = self.norm(x)\n        x = x.permute(0, 3, 1, 2)\n        x = self.unpatch_conv(x)\n        x = self.pixel_shuffle(x)\n        return x\n\n\nclass UVitBlock(nn.Module):\n    def __init__(\n        self,\n        channels,\n        out_channels,\n        num_res_blocks,\n        stride,\n        hidden_size,\n        hidden_dropout,\n        elementwise_affine,\n        norm_eps,\n        use_bias,\n        downsample: bool,\n        upsample: bool,\n        res_ffn_factor: int = 4,\n        seq_len=None,\n        concat_input=False,\n        original_input_channels=None,\n        use_zero=True,\n        norm_type=\"RMS\",\n    ):\n        super().__init__()\n\n        self.res_blocks = nn.ModuleList()\n        for i in range(num_res_blocks):\n            conv_block = ConvNextBlock(\n                channels,\n                norm_eps,\n                elementwise_affine,\n                use_bias,\n                hidden_dropout,\n                hidden_size,\n                res_ffn_factor=res_ffn_factor,\n            )\n\n            self.res_blocks.append(conv_block)\n\n        if downsample:\n            self.downsample = Downsample2D(\n                channels=channels,\n                out_channels=out_channels,\n                use_conv=True,\n                name=\"Conv2d_0\",\n                kernel_size=3,\n                padding=1,\n                stride=stride,\n                norm_type=\"rms_norm\",\n                eps=norm_eps,\n                elementwise_affine=elementwise_affine,\n                bias=use_bias,\n            )\n        else:\n            self.downsample = None\n\n        if upsample:\n            self.upsample = Upsample2D(\n                channels=channels,\n                out_channels=out_channels,\n                use_conv_transpose=False,\n                use_conv=True,\n                kernel_size=3,\n                padding=1,\n                stride=stride,\n                name=\"conv\",\n                norm_type=\"rms_norm\",\n                eps=norm_eps,\n                elementwise_affine=elementwise_affine,\n                bias=use_bias,\n                interpolate=True,\n            )\n        else:\n            self.upsample = None\n\n    def forward(self, x, emb, recompute=False):\n        for res_block in self.res_blocks:\n            x = res_block(x, emb)\n\n        if self.downsample is not None:\n            x = self.downsample(x)\n\n        if self.upsample is not None:\n            x = self.upsample(x)\n\n        return x\n\n\nclass ShallowUViTEncoder(nn.Module):\n    def __init__(\n        self,\n        input_channels=3,\n        stride=4,\n        kernel_size=7,\n        padding=None,\n        block_out_channels=(768,),\n        layers_in_middle=2,\n        hidden_size=2048,\n        elementwise_affine=True,\n        use_bias=True,\n        norm_eps=1e-6,\n        dropout=0.0,\n        use_mid_block=True,\n        **kwargs,\n    ):\n        super().__init__()\n\n        self.time_proj = Timesteps(\n            block_out_channels[0], flip_sin_to_cos=True, downscale_freq_shift=0\n        )\n        self.time_embed = TimestepEmbedding(\n            block_out_channels[0], hidden_size, sample_proj_bias=use_bias\n        )\n\n        if padding is None:\n            padding = math.ceil(kernel_size - stride)\n        self.in_conv = nn.Conv2d(\n            in_channels=input_channels,\n            out_channels=block_out_channels[0],\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n        )\n        if use_mid_block:\n            self.mid_block = UVitBlock(\n                block_out_channels[-1],\n                block_out_channels[-1],\n                num_res_blocks=layers_in_middle,\n                hidden_size=hidden_size,\n                hidden_dropout=dropout,\n                elementwise_affine=elementwise_affine,\n                norm_eps=norm_eps,\n                use_bias=use_bias,\n                downsample=False,\n                upsample=False,\n                stride=1,\n                res_ffn_factor=4,\n            )\n        else:\n            self.mid_block = None\n\n    def get_num_extra_tensors(self):\n        return 2\n\n    def forward(self, x, timesteps):\n\n        bs = x.shape[0]\n        dtype = x.dtype\n\n        t_emb = self.time_proj(timesteps.flatten()).view(bs, -1).to(dtype)\n        t_emb = self.time_embed(t_emb)\n        x_emb = self.in_conv(x)\n\n        if self.mid_block is not None:\n            x_emb = self.mid_block(x_emb, t_emb)\n\n        hs = [x_emb]\n        return x_emb, t_emb, hs\n\n\nclass ShallowUViTDecoder(nn.Module):\n    def __init__(\n        self,\n        in_channels=768,\n        out_channels=3,\n        block_out_channels: Tuple[int] = (768,),\n        upsamples=2,\n        layers_in_middle=2,\n        hidden_size=2048,\n        elementwise_affine=True,\n        norm_eps=1e-6,\n        use_bias=True,\n        dropout=0.0,\n        use_mid_block=True,\n        **kwargs,\n    ):\n        super().__init__()\n        if use_mid_block:\n            self.mid_block = UVitBlock(\n                in_channels + block_out_channels[-1],\n                block_out_channels[\n                    -1\n                ],  # In fact, the parameter is not used because it has no effect when both downsample and upsample are set to false.\n                num_res_blocks=layers_in_middle,\n                hidden_size=hidden_size,\n                hidden_dropout=dropout,\n                elementwise_affine=elementwise_affine,\n                norm_eps=norm_eps,\n                use_bias=use_bias,\n                downsample=False,\n                upsample=False,\n                stride=1,\n                res_ffn_factor=4,\n            )\n        else:\n            self.mid_block = None\n        self.out_convs = nn.ModuleList()\n        for rank in range(upsamples):\n            if rank == upsamples - 1:\n                curr_out_channels = out_channels\n            else:\n                curr_out_channels = block_out_channels[-1]\n            if rank == 0:\n                curr_in_channels = block_out_channels[-1] + in_channels\n            else:\n                curr_in_channels = block_out_channels[-1]\n            self.out_convs.append(\n                Unpatchify(\n                    curr_in_channels,\n                    curr_out_channels,\n                    patch_size=2,\n                    bias=use_bias,\n                    elementwise_affine=elementwise_affine,\n                    eps=norm_eps,\n                )\n            )\n        self.input_norm = RMSNorm(in_channels, norm_eps)\n\n    def forward(self, x, hs, t_emb):\n\n        x = x.permute(0, 2, 3, 1)\n        x = self.input_norm(x)\n        x = x.permute(0, 3, 1, 2)\n\n        x = torch.cat([x, hs.pop()], dim=1)\n        if self.mid_block is not None:\n            x = self.mid_block(x, t_emb)\n        for out_conv in self.out_convs:\n            x = out_conv(x)\n        assert len(hs) == 0\n        return x\n"}
{"type": "source_file", "path": "three_party/Janus/janus/models/__init__.py", "content": "# Copyright (c) 2023-2024 DeepSeek.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of\n# this software and associated documentation files (the \"Software\"), to deal in\n# the Software without restriction, including without limitation the rights to\n# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n# the Software, and to permit persons to whom the Software is furnished to do so,\n# subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom .image_processing_vlm import VLMImageProcessor\nfrom .modeling_vlm import MultiModalityCausalLM\nfrom .processing_vlm import VLChatProcessor\n\n__all__ = [\n    \"VLMImageProcessor\",\n    \"VLChatProcessor\",\n    \"MultiModalityCausalLM\",\n]\n"}
