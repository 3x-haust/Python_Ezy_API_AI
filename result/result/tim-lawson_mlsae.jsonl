{"repo_info": {"repo_name": "mlsae", "repo_owner": "tim-lawson", "repo_url": "https://github.com/tim-lawson/mlsae"}}
{"type": "test_file", "path": "mlsae/metrics/tests/test_layerwise_mse.py", "content": "import pytest\nimport torch\nfrom jaxtyping import Float\n\nfrom mlsae.metrics import LayerwiseMSE\n\nn_layers = 6\nshape = (n_layers, 1, 2048, 512)\n\n\n@pytest.mark.parametrize(\n    (\"n_layers\", \"inputs\", \"recons\", \"expected\"),\n    [\n        pytest.param(\n            n_layers,\n            torch.zeros(*shape),\n            torch.zeros(*shape),\n            torch.zeros(n_layers),\n            id=\"both 0\",\n        ),\n        pytest.param(\n            n_layers,\n            torch.ones(*shape),\n            torch.ones(*shape),\n            torch.zeros(n_layers),\n            id=\"both 1\",\n        ),\n        pytest.param(\n            n_layers,\n            torch.zeros(*shape),\n            torch.ones(*shape),\n            torch.ones(n_layers),\n            id=\"0 and 1\",\n        ),\n        pytest.param(\n            n_layers,\n            torch.ones(*shape),\n            torch.zeros(*shape),\n            torch.ones(n_layers),\n            id=\"1 and 0\",\n        ),\n        pytest.param(\n            1,\n            torch.ones((1, *shape[1:])),\n            torch.zeros((1, *shape[1:])),\n            torch.ones(1),\n            id=\"single layer\",\n        ),\n    ],\n)\ndef test_layerwise_mse(\n    n_layers: int,\n    inputs: Float[torch.Tensor, \"n_layers batch pos n_inputs\"],\n    recons: Float[torch.Tensor, \"n_layers batch pos n_inputs\"],\n    expected: Float[torch.Tensor, \"n_layers\"],\n) -> None:\n    metric = LayerwiseMSE(n_layers)\n\n    metric.update(inputs=inputs, recons=recons)\n    assert torch.allclose(metric.compute(), expected)\n\n    metric.update(inputs=inputs, recons=recons)\n    metric.update(inputs=inputs, recons=recons)\n    assert torch.allclose(metric.compute(), expected)\n"}
{"type": "test_file", "path": "mlsae/model/autoencoders/tests/test_autoencoders.py", "content": "import torch\n\nfrom mlsae.model.autoencoders import SAE, TopKSAE\nfrom mlsae.model.decoder import scatter_topk\n\n\n@torch.no_grad()\ndef test_autoencoders() -> None:\n    n_inputs = 512\n    n_latents = 64 * n_inputs\n    dead_steps_threshold = 10_000_000\n    k = n_latents\n\n    sae: SAE = SAE(n_inputs, n_latents, dead_steps_threshold)  # type: ignore\n\n    topk_sae: TopKSAE = TopKSAE(n_inputs, n_latents, k, dead_steps_threshold, auxk=None)  # type: ignore\n    topk_sae.encoder.weight.data = sae.encoder.weight.data\n    topk_sae.decoder.weight.data = sae.decoder.weight.data\n    topk_sae.pre_encoder_bias.data = sae.pre_encoder_bias.data\n\n    inputs = torch.rand(1, n_inputs)\n\n    sae_latents, sae_recons, sae_dead = sae.forward(inputs)\n    topk_sae_topk, topk_sae_recons, _, _, topk_sae_dead = topk_sae.forward(inputs)\n    topk_sae_latents = scatter_topk(topk_sae_topk, n_latents)\n\n    assert torch.allclose(sae_latents, topk_sae_latents, atol=1e-3)\n    assert torch.allclose(sae_recons, topk_sae_recons, atol=1e-3)\n    assert torch.allclose(sae_dead, topk_sae_dead, atol=1e-3)\n"}
{"type": "test_file", "path": "mlsae/metrics/tests/__init__.py", "content": ""}
{"type": "test_file", "path": "mlsae/metrics/tests/test_dead_latents.py", "content": "import torch\n\nfrom mlsae.metrics import DeadLatents\n\n\ndef test_dead_latents() -> None:\n    metric = DeadLatents(4, 4)\n\n    metric.update(indices=torch.tensor([[[[0], [0]]], [[[0], [0]]]]))\n    assert metric.tokens == 2\n    assert torch.allclose(metric.latent_tokens, torch.tensor([4.0, 0.0, 0.0, 0.0]))\n    assert torch.allclose(metric.compute(), torch.tensor(0.75))\n\n    metric.update(indices=torch.tensor([[[[0], [0]]], [[[1], [1]]]]))\n    assert metric.tokens == 4\n    assert torch.allclose(metric.latent_tokens, torch.tensor([6.0, 2.0, 0.0, 0.0]))\n    assert torch.allclose(metric.compute(), torch.tensor(0.5))\n\n    metric.update(indices=torch.tensor([[[[1], [1]]], [[[2], [2]]]]))\n    assert metric.tokens == 6\n    assert torch.allclose(metric.latent_tokens, torch.tensor([6.0, 4.0, 2.0, 0.0]))\n    assert torch.allclose(metric.compute(), torch.tensor(0.25))\n\n    metric.update(indices=torch.tensor([[[[2], [2]]], [[[3], [3]]]]))\n    assert metric.tokens == 8\n    assert torch.allclose(metric.latent_tokens, torch.tensor([6.0, 4.0, 4.0, 2.0]))\n    assert torch.allclose(metric.compute(), torch.tensor(0.0))\n\n    metric.update(indices=torch.tensor([[[[3], [3]]], [[[0], [0]]]]))\n    assert metric.tokens == 10\n    assert torch.allclose(metric.latent_tokens, torch.tensor([8.0, 4.0, 4.0, 4.0]))\n    assert torch.allclose(metric.compute(), torch.tensor(0.0))\n\n    metric.update(indices=torch.tensor([[[[0], [0]]], [[[0], [0]]]]))\n    assert metric.tokens == 12\n    assert torch.allclose(metric.latent_tokens, torch.tensor([12.0, 4.0, 4.0, 4.0]))\n    assert torch.allclose(metric.compute(), torch.tensor(0.0))\n\n    metric.update(indices=torch.tensor([[[[0], [0]]], [[[0], [0]]]]))\n    assert metric.tokens == 14\n    assert torch.allclose(metric.latent_tokens, torch.tensor([16.0, 4.0, 4.0, 4.0]))\n    assert torch.allclose(metric.compute(), torch.tensor(0.0))\n\n    metric.update(indices=torch.tensor([[[[0], [0]]], [[[0], [0]]]]))\n    assert metric.tokens == 16\n    assert torch.allclose(metric.latent_tokens, torch.tensor([20.0, 4.0, 4.0, 4.0]))\n    assert torch.allclose(metric.compute(), torch.tensor(0.0))\n\n    metric.update(indices=torch.tensor([[[[0], [0]]], [[[0], [0]]]]))\n    assert metric.tokens == 18\n    assert torch.allclose(metric.latent_tokens, torch.tensor([24.0, 4.0, 4.0, 4.0]))\n    assert torch.allclose(metric.compute(), torch.tensor(0.75))\n"}
{"type": "test_file", "path": "mlsae/metrics/tests/test_layerwise_fvu.py", "content": "import pytest\nimport torch\nfrom jaxtyping import Float\n\nfrom mlsae.metrics import LayerwiseFVU\n\nn_layers = 6\nshape = (n_layers, 1, 2048, 512)\n\ngenerator = torch.Generator()\ngenerator.manual_seed(42)\n\nnormal_zeros = torch.normal(torch.ones(*shape), std=1, generator=generator)\nnormal_ones = torch.normal(torch.zeros(*shape), std=1, generator=generator)\n\n\n@pytest.mark.parametrize(\n    (\"n_layers\", \"inputs\", \"recons\", \"expected\"),\n    [\n        pytest.param(\n            n_layers,\n            normal_zeros,\n            normal_zeros,\n            torch.zeros(n_layers),\n            id=\"both 1\",\n        ),\n        pytest.param(\n            n_layers,\n            normal_ones,\n            normal_ones,\n            torch.zeros(n_layers),\n            id=\"both 0\",\n        ),\n        pytest.param(\n            n_layers,\n            normal_zeros,\n            torch.zeros(*shape),\n            torch.ones(n_layers) * 2,\n            id=\"1 and 0\",\n        ),\n        pytest.param(\n            n_layers,\n            normal_ones,\n            torch.ones(*shape),\n            torch.ones(n_layers) * 2,\n            id=\"0 and 1\",\n        ),\n        pytest.param(\n            1,\n            normal_zeros[0, ...].unsqueeze(0),\n            normal_zeros[0, ...].unsqueeze(0),\n            torch.zeros(1),\n            id=\"single layer\",\n        ),\n    ],\n)\ndef test_layerwise_fvu(\n    n_layers: int,\n    inputs: Float[torch.Tensor, \"n_layers batch pos n_inputs\"],\n    recons: Float[torch.Tensor, \"n_layers batch pos n_inputs\"],\n    expected: Float[torch.Tensor, \"n_layers\"],\n) -> None:\n    metric = LayerwiseFVU(n_layers)\n\n    metric.update(inputs=inputs, recons=recons)\n    assert torch.allclose(metric.compute(), expected, atol=1e-2)\n\n    metric.update(inputs=inputs, recons=recons)\n    metric.update(inputs=inputs, recons=recons)\n    assert torch.allclose(metric.compute(), expected, atol=1e-2)\n"}
{"type": "test_file", "path": "mlsae/metrics/tests/test_layerwise_l0_norm.py", "content": "import pytest\nimport torch\nfrom jaxtyping import Float\n\nfrom mlsae.metrics import LayerwiseL0Norm\n\nn_layers = 6\nshape = (n_layers, 1, 2048, 32)\n\n\n@pytest.mark.parametrize(\n    (\"n_layers\", \"dead_threshold\", \"values\", \"expected\"),\n    [\n        pytest.param(\n            n_layers,\n            1e-3,\n            torch.zeros(*shape),\n            torch.zeros(n_layers),\n            id=\"all zero\",\n        ),\n        pytest.param(\n            n_layers,\n            1e-3,\n            torch.ones(*shape) * 1e-4,\n            torch.zeros(n_layers),\n            id=\"below threshold\",\n        ),\n        pytest.param(\n            n_layers,\n            1e-3,\n            torch.ones(*shape),\n            torch.ones(n_layers) * 32.0,\n            id=\"above threshold\",\n        ),\n        pytest.param(\n            1,\n            1e-3,\n            torch.ones((1, *shape[1:])),\n            torch.ones(1) * 32.0,\n            id=\"single layer\",\n        ),\n    ],\n)\ndef test_layerwise_l0_norm(\n    n_layers: int,\n    dead_threshold: float,\n    values: Float[torch.Tensor, \"n_layers batch pos k\"],\n    expected: Float[torch.Tensor, \"n_layers\"],\n) -> None:\n    metric = LayerwiseL0Norm(n_layers, dead_threshold)\n\n    metric.update(values=values)\n    assert torch.allclose(metric.compute(), expected)\n\n    metric.update(values=values)\n    metric.update(values=values)\n    assert torch.allclose(metric.compute(), expected)\n"}
{"type": "test_file", "path": "mlsae/metrics/tests/test_loss_mse.py", "content": "import pytest\nimport torch\nfrom jaxtyping import Float\n\nfrom mlsae.metrics import MSELoss\n\nn_layers = 6\nshape = (n_layers, 1, 2048, 512)\n\ngenerator = torch.Generator()\ngenerator.manual_seed(42)\n\nnormal_zeros = torch.normal(torch.ones(*shape), std=1, generator=generator)\nnormal_ones = torch.normal(torch.zeros(*shape), std=1, generator=generator)\n\n\n@pytest.mark.parametrize(\n    (\"n_layers\", \"inputs\", \"recons\", \"expected\"),\n    [\n        pytest.param(\n            n_layers,\n            normal_zeros,\n            normal_zeros,\n            torch.tensor(0.0),\n            id=\"both 0\",\n        ),\n        pytest.param(\n            n_layers,\n            normal_ones,\n            normal_ones,\n            torch.tensor(0.0),\n            id=\"both 1\",\n        ),\n        pytest.param(\n            n_layers,\n            normal_zeros,\n            torch.ones(*shape),\n            torch.tensor(1.0),\n            id=\"0 and 1\",\n        ),\n        pytest.param(\n            n_layers,\n            normal_ones,\n            torch.zeros(*shape),\n            torch.tensor(1.0),\n            id=\"1 and 0\",\n        ),\n        pytest.param(\n            1,\n            normal_ones[0, ...].unsqueeze(0),\n            torch.zeros((1, *shape[1:])),\n            torch.tensor(1.0),\n            id=\"single layer\",\n        ),\n    ],\n)\ndef test_loss_mse(\n    n_layers: int,\n    inputs: Float[torch.Tensor, \"n_layers batch pos n_inputs\"],\n    recons: Float[torch.Tensor, \"n_layers batch pos n_inputs\"],\n    expected: Float[torch.Tensor, \"n_layers\"],\n) -> None:\n    metric = MSELoss(n_layers)\n\n    metric.update(inputs=inputs, recons=recons)\n    assert torch.allclose(metric.compute(), expected, atol=1e-2)\n\n    metric.update(inputs=inputs, recons=recons)\n    metric.update(inputs=inputs, recons=recons)\n    assert torch.allclose(metric.compute(), expected, atol=1e-2)\n"}
{"type": "test_file", "path": "mlsae/metrics/tests/test_layerwise_l1_norm.py", "content": "import pytest\nimport torch\nfrom jaxtyping import Float\n\nfrom mlsae.metrics import LayerwiseL1Norm\n\nn_layers = 6\nshape = (n_layers, 1, 2048, 32)\n\n\n@pytest.mark.parametrize(\n    (\"n_layers\", \"values\", \"expected\"),\n    [\n        pytest.param(\n            n_layers,\n            torch.zeros(*shape),\n            torch.zeros(n_layers),\n            id=\"all zero\",\n        ),\n        pytest.param(\n            n_layers,\n            torch.ones(*shape),\n            torch.ones(n_layers) * 32.0,\n            id=\"all +1\",\n        ),\n        pytest.param(\n            n_layers,\n            torch.ones(*shape) * -1,\n            torch.ones(n_layers) * 32.0,\n            id=\"all -1\",\n        ),\n        pytest.param(\n            1,\n            torch.ones((1, *shape[1:])) * -1,\n            torch.ones(1) * 32.0,\n            id=\"single layer\",\n        ),\n    ],\n)\ndef test_layerwise_l1_norm(\n    n_layers: int,\n    values: Float[torch.Tensor, \"n_layers batch pos k\"],\n    expected: Float[torch.Tensor, \"n_layers\"],\n) -> None:\n    metric = LayerwiseL1Norm(n_layers)\n\n    metric.update(values=values)\n    assert torch.allclose(metric.compute(), expected)\n\n    metric.update(values=values)\n    metric.update(values=values)\n    assert torch.allclose(metric.compute(), expected)\n"}
{"type": "test_file", "path": "mlsae/model/transformers/tests/test_gpt2.py", "content": "import torch\nfrom transformers import AutoTokenizer\n\nfrom mlsae.model.transformers.gpt2 import GPT2Transformer\nfrom mlsae.model.transformers.models.gpt2.modeling_gpt2 import (\n    GPT2Config,\n    GPT2LMHeadModel,\n    GPT2Model,\n)\nfrom mlsae.utils import get_input_ids\n\natol = 1e-2\n\n\n@torch.no_grad()\ndef test_hidden_states() -> None:\n    model_name = \"openai-community/gpt2\"\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    input_ids = get_input_ids(tokenizer, \"The quick brown fox jumps over the lazy dog.\")\n\n    gpt2: GPT2Model = GPT2Model.from_pretrained(model_name)  # type: ignore\n    config: GPT2Config = gpt2.config  # type: ignore\n\n    # Skip the final layer norm when collecting hidden states\n    hidden_states = torch.stack(\n        gpt2.forward(\n            input_ids, output_hidden_states=True, skip_final_layer_norm=True\n        ).hidden_states[1:]  # type: ignore\n    )\n\n    # We usually skip special tokens, but we may as well compare them\n    my_gpt2 = GPT2Transformer(\n        model_name, config.n_positions, batch_size=1, skip_special_tokens=False\n    )\n    my_hidden_states = my_gpt2.hidden_states(input_ids)\n\n    for layer in range(len(hidden_states)):\n        assert torch.allclose(\n            hidden_states[layer],\n            my_hidden_states[layer],\n            atol=atol,\n        )\n\n\n@torch.no_grad()\ndef test_forward_at_layer() -> None:\n    model_name = \"openai-community/gpt2\"\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    input_ids = get_input_ids(tokenizer, \"The quick brown fox jumps over the lazy dog.\")\n\n    gpt2: GPT2LMHeadModel = GPT2LMHeadModel.from_pretrained(model_name)  # type: ignore\n    config: GPT2Config = gpt2.config  # type: ignore\n\n    # Skip the final layer norm when collecting hidden states\n    hidden_states = torch.stack(\n        gpt2.forward(\n            input_ids,\n            output_hidden_states=True,\n            skip_final_layer_norm=True,\n        ).hidden_states[1:]  # type: ignore\n    )\n\n    # Don't skip the final layer norm when computing the loss/logits\n    output = gpt2.forward(input_ids, labels=input_ids)\n    loss: torch.Tensor = output.loss  # type: ignore\n    logits = output.logits  # type: ignore\n\n    # We usually skip special tokens, but we may as well compare them\n    my_gpt2 = GPT2Transformer(\n        model_name, config.n_positions, batch_size=1, skip_special_tokens=False\n    )\n\n    for layer in range(config.n_layer):\n        my_loss, my_logits = my_gpt2.forward_at_layer(\n            inputs_embeds=hidden_states,\n            start_at_layer=layer,\n            return_type=\"both\",\n            tokens=input_ids,\n        )\n        assert torch.allclose(my_loss, loss, atol=atol)\n        assert torch.allclose(my_logits, logits, atol=atol)\n"}
{"type": "test_file", "path": "mlsae/model/transformers/tests/test_llama.py", "content": "import pytest\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n)\nfrom transformers.models.llama.configuration_llama import LlamaConfig\n\nfrom mlsae.model.transformers.llama import LlamaTransformer\nfrom mlsae.model.transformers.models.llama.modeling_llama import (\n    LlamaForCausalLM,\n    LlamaModel,\n)\nfrom mlsae.utils import get_input_ids\n\natol = 1e-2\n\n\n@pytest.mark.slow()\n@torch.no_grad()\ndef test_hidden_states() -> None:\n    model_name = \"meta-llama/Llama-3.2-1B\"\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    input_ids = get_input_ids(tokenizer, \"The quick brown fox jumps over the lazy dog.\")\n\n    llama: LlamaModel = LlamaModel.from_pretrained(model_name)  # type: ignore\n    config: LlamaConfig = llama.config  # type: ignore\n\n    # Skip the final layer norm when collecting hidden states\n    hidden_states = torch.stack(\n        llama.forward(\n            input_ids, output_hidden_states=True, skip_final_layer_norm=True\n        ).hidden_states[1:]  # type: ignore\n    )\n\n    # We usually skip special tokens, but we may as well compare them\n    my_llama = LlamaTransformer(\n        model_name,\n        config.max_position_embeddings,\n        batch_size=1,\n        skip_special_tokens=False,\n    )\n    my_hidden_states = my_llama.hidden_states(input_ids)\n\n    for layer in range(len(hidden_states)):\n        assert torch.allclose(\n            hidden_states[layer],\n            my_hidden_states[layer],\n            atol=atol,\n        )\n\n\n@pytest.mark.slow()\n@torch.no_grad()\ndef test_forward_at_layer() -> None:\n    model_name = \"meta-llama/Llama-3.2-1B\"\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    input_ids = get_input_ids(tokenizer, \"The quick brown fox jumps over the lazy dog.\")\n\n    llama: LlamaForCausalLM = LlamaForCausalLM.from_pretrained(model_name)  # type: ignore\n    config: LlamaConfig = llama.config  # type: ignore\n\n    # Skip the final layer norm when collecting hidden states\n    hidden_states = torch.stack(\n        llama.forward(\n            input_ids,\n            output_hidden_states=True,\n            skip_final_layer_norm=True,\n        ).hidden_states[1:]  # type: ignore\n    )\n\n    # Don't skip the final layer norm when computing the loss/logits\n    output = llama.forward(input_ids, labels=input_ids)\n    loss: torch.Tensor = output.loss  # type: ignore\n    logits = output.logits  # type: ignore\n\n    # We usually skip special tokens, but we may as well compare them\n    my_llama = LlamaTransformer(\n        model_name, config.n_positions, batch_size=1, skip_special_tokens=False\n    )\n\n    for layer in range(config.n_layer):\n        my_loss, my_logits = my_llama.forward_at_layer(\n            inputs_embeds=hidden_states,\n            start_at_layer=layer,\n            return_type=\"both\",\n            tokens=input_ids,\n        )\n        assert torch.allclose(my_loss, loss, atol=atol)\n        assert torch.allclose(my_logits, logits, atol=atol)\n"}
{"type": "source_file", "path": "figures/heatmap.py", "content": "import torch\nfrom matplotlib import pyplot as plt\nfrom matplotlib.colors import Colormap, Normalize\n\n\ndef save_heatmap(\n    data: torch.Tensor,\n    filename: str,\n    figsize: tuple[float, float] = (5.5, 1.25),\n    dpi: int = 1200,\n    cmap: str | Colormap | None = \"magma_r\",\n    norm: str | Normalize | None = None,\n) -> None:\n    # Exclude latents with only NaN values\n    data = data[:, ~torch.all(data.isnan(), dim=0)]\n\n    n_layers, n_latents = data.shape\n    extent = (0, n_latents, 0, n_layers)\n\n    plt.rcParams.update({\"axes.linewidth\": 0})\n    fig, ax = plt.subplots(1, 1, figsize=figsize, dpi=dpi)\n\n    ax.imshow(\n        data,\n        cmap=cmap,\n        norm=norm,\n        aspect=\"auto\",\n        extent=extent,\n        interpolation=\"nearest\",\n    )\n    ax.set_axis_off()\n\n    fig.savefig(filename, format=\"pdf\", bbox_inches=\"tight\", pad_inches=0)\n    plt.close(fig)\n"}
{"type": "source_file", "path": "figures/layer_hist.py", "content": "import os\nfrom dataclasses import dataclass\n\nimport numpy\nimport pandas as pd\nimport torch\nfrom simple_parsing import parse\n\nfrom mlsae.analysis.dists import Dists\nfrom mlsae.trainer import SweepConfig\nfrom mlsae.utils import get_device\n\n\n@dataclass\nclass Config(SweepConfig):\n    noninteger: bool = False\n    \"\"\"Whether to plot the non-integer component of the center of mass.\"\"\"\n\n\ndef main(\n    config: Config, device: torch.device, out: str | os.PathLike[str] = \".out\"\n) -> None:\n    os.makedirs(out, exist_ok=True)\n    for repo_id in config.repo_ids(transformer=True, tuned_lens=config.tuned_lens):\n        dists = Dists.load(repo_id, device)\n        values = dists.layer_mean[~torch.isnan(dists.layer_mean)].cpu().numpy()\n\n        repo_id = repo_id.split(\"/\")[-1]\n        bins = 16 * dists.n_layers\n\n        if config.noninteger:\n            values = numpy.abs(values - numpy.round(values))\n            range = (0, 0.5)\n            filename = f\"layer_hist_nonint_{repo_id}.csv\"\n        else:\n            range = (0, dists.n_layers - 1)\n            filename = f\"layer_hist_{repo_id}.csv\"\n\n        hist, bins = numpy.histogram(values, bins=bins, range=range, density=True)\n        hist = numpy.append(hist, 0)  # bins has one more element\n        pd.DataFrame({\"layer\": bins, \"density\": hist}).to_csv(\n            os.path.join(out, filename), index=False\n        )\n\n\nif __name__ == \"__main__\":\n    main(parse(Config), get_device())\n"}
{"type": "source_file", "path": "figures/embed_sim.py", "content": "import os\nfrom dataclasses import dataclass\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom simple_parsing import field, parse\nfrom transformers import AutoTokenizer, GPTNeoXForCausalLM\n\nfrom mlsae.model import MLSAETransformer\nfrom mlsae.trainer.config import SweepConfig\nfrom mlsae.utils import get_device, get_repo_id, normalize\n\n\n@dataclass\nclass Config(SweepConfig):\n    filename: str = \"embed_sim.csv\"\n    \"\"\"The name of the file to save the results to.\"\"\"\n\n    latents: list[int] = field(default_factory=lambda: [])\n    \"\"\"The latent indices to find the most similar embeddings to.\"\"\"\n\n    n_embeds: int = 8\n    \"\"\"The number of most similar embeddings to save.\"\"\"\n\n    seed: int = 42\n    \"\"\"The seed for global random state.\"\"\"\n\n\n@torch.no_grad()\ndef get_similar_embeds(\n    config: Config, repo_id: str, model_name: str, device: torch.device\n) -> tuple[torch.Tensor, torch.Tensor]:\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    mlsae = MLSAETransformer.from_pretrained(repo_id).to(device).autoencoder\n    W_dec = normalize(mlsae.decoder.weight)\n    if len(config.latents) > 0:\n        W_dec = W_dec[:, config.latents]\n    latents = (\n        config.latents if len(config.latents) > 0 else list(range(mlsae.n_latents))\n    )\n\n    def save_csv(topk: torch.return_types.topk, path: Path | str):\n        rows = [\n            {\n                \"latent\": latent,\n                \"token\": tokenizer.decode(topk.indices[embed_index, latent_index]),\n                \"sim\": topk.values[embed_index, latent_index].detach().item(),\n            }\n            for latent_index, latent in enumerate(latents)\n            for embed_index in range(config.n_embeds)\n        ]\n        pd.DataFrame(rows).to_csv(path, index=False)\n\n    model: GPTNeoXForCausalLM = GPTNeoXForCausalLM.from_pretrained(model_name)  # type: ignore\n    embed_in = normalize(model.get_input_embeddings().weight.to(device), dim=1)\n    embed_out = normalize(model.get_output_embeddings().weight.to(device), dim=1)\n\n    topk_in = torch.topk(embed_in @ W_dec, k=config.n_embeds, dim=0)\n    topk_out = torch.topk(embed_out @ W_dec, k=config.n_embeds, dim=0)\n\n    repo_id = repo_id.split(\"/\")[-1]\n    save_csv(topk_in, os.path.join(\"out\", f\"embed_in_cos_sim_{repo_id}.csv\"))\n    save_csv(topk_out, os.path.join(\"out\", f\"embed_out_cos_sim_{repo_id}.csv\"))\n\n    return topk_in.values[0, :], topk_out.values[0, :]\n\n\ndef main(\n    config: Config, device: torch.device, out: str | os.PathLike[str] = \".out\"\n) -> None:\n    os.makedirs(out, exist_ok=True)\n    rows: list[dict[str, str | int | float]] = []\n    for model_name, expansion_factor, k in config:\n        repo_id = get_repo_id(\n            model_name=model_name,\n            expansion_factor=expansion_factor,\n            k=k,\n            tuned_lens=config.tuned_lens,\n            transformer=False,\n        )\n        topk_in, topk_out = get_similar_embeds(config, repo_id, model_name, device)\n        n_latents = topk_in.shape[0]\n        rows.append(\n            {\n                \"model_name\": model_name,\n                \"n_latents\": n_latents,\n                \"expansion_factor\": expansion_factor,\n                \"k\": k,\n                \"tuned_lens\": config.tuned_lens,\n                \"in_mean\": topk_in.mean().item(),\n                \"in_var\": topk_in.var().item(),\n                \"in_std\": topk_in.std().item(),\n                \"in_sem\": topk_in.std().item() / np.sqrt(n_latents),\n                \"out_mean\": topk_out.mean().item(),\n                \"out_var\": topk_out.var().item(),\n                \"out_std\": topk_out.std().item(),\n                \"out_sem\": topk_out.std().item() / np.sqrt(n_latents),\n            }\n        )\n    pd.DataFrame(rows).to_csv(os.path.join(out, config.filename), index=False)\n\n\nif __name__ == \"__main__\":\n    main(parse(Config), get_device())\n"}
{"type": "source_file", "path": "figures/__init__.py", "content": ""}
{"type": "source_file", "path": "figures/entropy.py", "content": "import math\nimport os\nfrom dataclasses import dataclass\n\nimport numpy\nimport pandas as pd\nimport torch\nfrom simple_parsing import parse\n\nfrom figures.test import parse_mlsae_repo_id\nfrom mlsae.analysis.dists import Dists\nfrom mlsae.trainer import SweepConfig\nfrom mlsae.utils import get_device\n\n\n@dataclass\nclass Config(SweepConfig):\n    filename: str = \"entropy.csv\"\n    \"\"\"The filename to save the results to.\"\"\"\n\n\ndef main(\n    config: Config, device: torch.device, out: str | os.PathLike[str] = \".out\"\n) -> None:\n    os.makedirs(out, exist_ok=True)\n    rows: list[dict[str, str | int | float]] = []\n    for repo_id in config.repo_ids(transformer=True, tuned_lens=config.tuned_lens):\n        dists = Dists.load(repo_id, device)\n        values = dists.entropies\n        values = values[~torch.isnan(values)]\n\n        repo_id = repo_id.split(\"/\")[-1]\n        model_name, expansion_factor, k, tuned_lens = parse_mlsae_repo_id(repo_id)\n\n        rows.append(\n            {\n                \"model_name\": model_name,\n                \"n_layers\": dists.n_layers,\n                \"n_latents\": dists.n_latents,\n                \"expansion_factor\": expansion_factor,\n                \"k\": k,\n                \"tuned_lens\": tuned_lens,\n                \"mean\": values.mean().item(),\n                \"var\": values.var().item(),\n                \"std\": values.std().item(),\n                \"sem\": values.std().item() / values.size(0) ** 0.5,\n                \"rel\": values.mean().item() / math.log(dists.n_layers),\n            }\n        )\n\n        hist, bins = numpy.histogram(\n            values.cpu().numpy(),\n            bins=dists.n_latents // expansion_factor,\n            range=(0, math.log(dists.n_layers)),\n        )\n        hist = numpy.append(hist, 0)\n        pd.DataFrame({\"bins\": bins, \"hist\": hist}).to_csv(\n            os.path.join(out, f\"entropy_{repo_id.split(\"/\")[-1]}.csv\"), index=False\n        )\n\n    pd.DataFrame(rows).to_csv(os.path.join(out, config.filename), index=False)\n\n\nif __name__ == \"__main__\":\n    main(parse(Config), get_device())\n"}
{"type": "source_file", "path": "figures/layer_sim.py", "content": "import os\n\nimport torch\nfrom matplotlib import pyplot as plt\nfrom matplotlib.colors import Colormap\nfrom simple_parsing import parse\n\nfrom mlsae.analysis.dists import Dists\nfrom mlsae.model import MLSAETransformer\nfrom mlsae.trainer.config import SweepConfig\nfrom mlsae.utils import get_device, normalize\n\n\n@torch.no_grad()\ndef get_heatmap_data(\n    repo_id: str, device: torch.device\n) -> tuple[torch.Tensor, torch.Tensor]:\n    mlsae = MLSAETransformer.from_pretrained(repo_id).to(device).autoencoder\n    W_dec = mlsae.decoder.weight.detach()\n    W_dec = normalize(W_dec)\n\n    # Sort latents in descending order of mean layer\n    dists = Dists.load(repo_id, device)\n    _, indices = dists.layer_mean.sort(descending=True)\n    W_dec = W_dec[:, indices]\n\n    # Pairwise differences between mean layers\n    layer_mean = dists.layer_mean.view(-1, 1) - dists.layer_mean.view(1, -1)\n\n    # Pairwise cosine similarities between decoder weight vectors\n    cos_sim = torch.mm(W_dec.T, W_dec)\n\n    # Remove duplicates and self-similarities\n    triu_indices = torch.triu_indices(*cos_sim.shape, offset=1)\n    x = layer_mean[*triu_indices].cpu()\n    y = cos_sim[*triu_indices].cpu()\n\n    return x, y\n\n\ndef save_heatmap(\n    x: torch.Tensor,\n    y: torch.Tensor,\n    filename: str,\n    figsize: tuple[float, float] = (2, 2),\n    dpi: int = 300,\n    cmap: str | Colormap | None = \"magma_r\",\n) -> None:\n    plt.rcParams.update({\"axes.linewidth\": 0})\n    fig, ax = plt.subplots(1, 1, figsize=figsize, dpi=dpi)\n    ax.hist2d(x, y, bins=[64, 512], range=[(0, 5), (-0.25, 0.25)], cmap=cmap)\n    ax.set_axis_off()\n    fig.savefig(filename, format=\"pdf\", bbox_inches=\"tight\", pad_inches=0)\n    plt.close(fig)\n\n\ndef main(\n    config: SweepConfig, device: torch.device, out: str | os.PathLike[str] = \".out\"\n) -> None:\n    os.makedirs(out, exist_ok=True)\n    for repo_id in config.repo_ids(transformer=False, tuned_lens=config.tuned_lens):\n        filename = f\"layer_sim_{repo_id.split('/')[-1]}.pdf\"\n        x, y = get_heatmap_data(repo_id, device)\n        save_heatmap(x, y, os.path.join(out, filename))\n\n\nif __name__ == \"__main__\":\n    main(parse(SweepConfig), get_device())\n"}
{"type": "source_file", "path": "figures/heatmap_aggregate.py", "content": "import os\nfrom dataclasses import dataclass\n\nimport torch\nfrom matplotlib.colors import PowerNorm\nfrom simple_parsing import parse\n\nfrom figures.heatmap import save_heatmap\nfrom mlsae.analysis.dists import Dists\nfrom mlsae.trainer import SweepConfig\nfrom mlsae.utils import get_device\n\n\n@dataclass\nclass Config(SweepConfig):\n    mode: str = \"probs\"\n    \"\"\"Whether to plot counts, totals, or probabilities.\"\"\"\n\n    gamma: float = 0.5\n    \"\"\"Gamma value for PowerNorm. Only applies to counts and totals.\"\"\"\n\n\ndef get_heatmap_data(dists: Dists, mode: str) -> torch.Tensor:\n    if mode == \"counts\":\n        return dists.counts\n    if mode == \"totals\":\n        return dists.totals\n    if mode == \"probs\":\n        return dists.probs\n    raise ValueError(f\"Invalid mode: {mode}\")\n\n\ndef get_heatmap_filename(repo_id: str, mode: str) -> str:\n    return f\"heatmap_aggregate_{mode}_{repo_id.split('/')[-1]}.pdf\"\n\n\ndef main(\n    repo_id: str,\n    config: Config,\n    device: torch.device,\n    out: str | os.PathLike[str] = \".out\",\n):\n    os.makedirs(out, exist_ok=True)\n    norm = None if config.mode == \"probs\" else PowerNorm(config.gamma)\n    dists = Dists.load(repo_id, device)\n    _, indices = dists.layer_mean.sort(descending=True)\n    save_heatmap(\n        get_heatmap_data(dists, config.mode)[:, indices].cpu(),\n        os.path.join(out, get_heatmap_filename(repo_id, config.mode)),\n        norm=norm,\n    )\n\n\ndef sweep(\n    config: Config, device: torch.device, out: str | os.PathLike[str] = \".out\"\n) -> None:\n    os.makedirs(out, exist_ok=True)\n    for repo_id in config.repo_ids(transformer=True, tuned_lens=config.tuned_lens):\n        main(repo_id, config, device, out)\n\n\nif __name__ == \"__main__\":\n    device = get_device()\n    sweep(parse(Config), device)\n"}
{"type": "source_file", "path": "figures.py", "content": "import os\nfrom dataclasses import dataclass\n\nfrom simple_parsing import Serializable, parse\n\nfrom figures import (\n    embed_sim,\n    entropy,\n    heatmap_aggregate,\n    heatmap_prompt,\n    layer_hist,\n    layer_sim,\n    layer_std,\n    mmcs,\n    num_layers,\n    scatter_freq,\n    wdec_sim,\n)\nfrom mlsae.trainer import SweepConfig\nfrom mlsae.utils import get_device\n\npythia_70m = \"EleutherAI/pythia-70m-deduped\"\npythia_160m = \"EleutherAI/pythia-160m-deduped\"\npythia_410m = \"EleutherAI/pythia-410m-deduped\"\npythia_1b = \"EleutherAI/pythia-1b-deduped\"\npythia_1_4b = \"EleutherAI/pythia-1.4b-deduped\"\ngpt2_small = \"openai-community/gpt2\"\nllama_3b = \"meta-llama/Llama-3.2-3B\"\ngemma_2b = \"google/gemma-2-2b\"\n\nexpansion_factors = [1, 2, 4, 8, 16, 32, 64, 128, 256]\nks = [16, 32, 64, 128, 256, 512]\n\n\n@dataclass\nclass FigureSweep(SweepConfig):\n    id: str | None = None\n    \"\"\"The identifier to use for filenames.\"\"\"\n\n    enabled: bool = True\n    \"\"\"Whether to enable this sweep.\"\"\"\n\n\n@dataclass\nclass FigureConfig(Serializable):\n    out: str = \".out\"\n    \"\"\"The directory to save the results to.\"\"\"\n\n    # in the paper\n    heatmap_aggregate: bool = False\n    heatmap_prompt: bool = False\n    mmcs: bool = False\n    wdec_sim: bool = False\n    num_layers: bool = False\n    entropy: bool = False\n\n    # not in the paper\n    embed_sim: bool = False\n    layer_std: bool = False\n    layer_hist: bool = False\n    layer_sim: bool = False\n    scatter_freq: bool = False\n\n\ndef main(config: FigureConfig, sweeps: list[FigureSweep]) -> None:\n    device = get_device()\n    os.makedirs(config.out, exist_ok=True)\n\n    for sweep in sweeps:\n        id = sweep.__dict__.pop(\"id\")\n        print(id)\n        enabled = sweep.__dict__.pop(\"enabled\")\n        if not enabled:\n            continue\n        sweep_dict = sweep.__dict__\n\n        for mode in [\"probs\", \"counts\", \"totals\"]:\n            gamma = 0.25\n\n            if config.heatmap_aggregate:\n                print(f\"> heatmap_aggregate ({mode})\")\n                heatmap_aggregate_config = heatmap_aggregate.Config(\n                    **sweep_dict, mode=mode, gamma=gamma\n                )\n                heatmap_aggregate.sweep(\n                    heatmap_aggregate_config,\n                    device,\n                    os.path.join(config.out, f\"heatmap_aggregate_{mode}\"),\n                )\n\n            if config.heatmap_prompt:\n                print(f\"> heatmap_prompt ({mode})\")\n                heatmap_prompt_config = heatmap_prompt.Config(\n                    **sweep_dict, mode=mode, gamma=gamma\n                )\n                heatmap_prompt.sweep(\n                    heatmap_prompt_config,\n                    device,\n                    os.path.join(config.out, f\"heatmap_prompt_{mode}\"),\n                )\n\n        if config.mmcs:\n            print(\"> mmcs\")\n            mmcs_config = mmcs.Config(**sweep_dict, filename=f\"mmcs_{id}.csv\")\n            mmcs.main(mmcs_config, device, os.path.join(config.out, \"mmcs\"))\n\n        if config.wdec_sim:\n            print(\"> wdec_sim\")\n            wdec_sim.main(sweep, device, os.path.join(config.out, \"wdec_sim\"))\n\n        if config.num_layers:\n            print(\"> num_layers\")\n            for threshold in [1, 10, 100, 1000, 10000, 100000, 1000000]:\n                num_layers_config = num_layers.Config(\n                    **sweep_dict,\n                    filename=f\"num_layers_{id}_{threshold}.csv\",\n                    threshold=threshold,\n                )\n                num_layers.main(\n                    num_layers_config, device, os.path.join(config.out, \"num_layers\")\n                )\n\n        if config.entropy:\n            print(\"> entropy\")\n            entropy_config = entropy.Config(**sweep_dict, filename=f\"entropy_{id}.csv\")\n            entropy.main(entropy_config, device, os.path.join(config.out, \"entropy\"))\n\n        if config.embed_sim:\n            print(\"> embed_sim\")\n            embed_sim_config = embed_sim.Config(\n                **sweep_dict, filename=f\"embed_sim_{id}.csv\"\n            )\n            embed_sim.main(\n                embed_sim_config, device, os.path.join(config.out, \"embed_sim\")\n            )\n\n        if config.layer_std:\n            print(\"> layer_std\")\n            layer_std_config = layer_std.Config(\n                **sweep_dict, filename=f\"layer_std_{id}.csv\"\n            )\n            layer_std.main(\n                layer_std_config, device, os.path.join(config.out, \"layer_std\")\n            )\n\n        if config.layer_hist:\n            print(\"> layer_hist\")\n            layer_hist_config = layer_hist.Config(**sweep_dict)\n            layer_hist.main(\n                layer_hist_config, device, os.path.join(config.out, \"layer_hist\")\n            )\n\n        if config.layer_sim:\n            print(\"> layer_sim\")\n            layer_sim.main(sweep, device, os.path.join(config.out, \"layer_sim\"))\n\n        if config.scatter_freq:\n            print(\"> scatter_freq\")\n            scatter_freq.main(sweep, device, os.path.join(config.out, \"scatter_freq\"))\n\n\nsweeps: list[FigureSweep] = [\n    # Non-Pythia models for R = 64 and k = 32\n    FigureSweep(\n        id=\"other\",\n        enabled=True,\n        model_name=[gpt2_small, llama_3b, gemma_2b],\n        expansion_factor=[64],\n        k=[32],\n        tuned_lens=False,\n    ),\n    # Varying model for R = 64 and k = 32\n    FigureSweep(\n        id=\"model_name\",\n        enabled=False,\n        model_name=[pythia_70m, pythia_160m, pythia_410m, pythia_1b, pythia_1_4b],\n        expansion_factor=[64],\n        k=[32],\n        tuned_lens=False,\n    ),\n    # Varying model with tuned lens for R = 64 and k = 32\n    FigureSweep(\n        id=\"lens_model_name\",\n        enabled=False,\n        model_name=[pythia_70m, pythia_160m, pythia_410m],\n        expansion_factor=[64],\n        k=[32],\n        tuned_lens=True,\n    ),\n    # Varying R for Pythia-70m and k = 32\n    FigureSweep(\n        id=\"pythia-70m-deduped_expansion_factor\",\n        enabled=False,\n        model_name=[pythia_70m],\n        expansion_factor=expansion_factors,\n        k=[32],\n        tuned_lens=False,\n    ),\n    # Varying k for Pythia-70m and R = 64\n    FigureSweep(\n        id=\"pythia-70m-deduped_k\",\n        enabled=False,\n        model_name=[pythia_70m],\n        expansion_factor=[64],\n        k=ks,\n        tuned_lens=False,\n    ),\n    # Varying R for Pythia-160m and k = 32\n    FigureSweep(\n        id=\"pythia-160m-deduped_expansion_factor\",\n        enabled=False,\n        model_name=[pythia_160m],\n        expansion_factor=expansion_factors,\n        k=[32],\n        tuned_lens=False,\n    ),\n    # Varying k for Pythia-160m and R = 64\n    FigureSweep(\n        id=\"pythia-160m-deduped_k\",\n        enabled=False,\n        model_name=[pythia_160m],\n        expansion_factor=[64],\n        k=ks,\n        tuned_lens=False,\n    ),\n    # Varying R for Pythia-70m with tuned lens and k = 32\n    FigureSweep(\n        id=\"pythia-70m-deduped_lens_expansion_factor\",\n        enabled=False,\n        model_name=[pythia_70m],\n        expansion_factor=expansion_factors,\n        k=[32],\n        tuned_lens=True,\n    ),\n    # Varying k for Pythia-70m with tuned lens and R = 64\n    FigureSweep(\n        id=\"pythia-70m-deduped_lens_k\",\n        enabled=False,\n        model_name=[pythia_70m],\n        expansion_factor=[64],\n        k=ks,\n        tuned_lens=True,\n    ),\n]\n\nif __name__ == \"__main__\":\n    main(parse(FigureConfig), sweeps)\n"}
{"type": "source_file", "path": "figures/heatmap_prompt.py", "content": "import os\nfrom dataclasses import dataclass\n\nimport torch\nfrom matplotlib.colors import PowerNorm\nfrom simple_parsing import parse\n\nfrom figures.heatmap import save_heatmap\nfrom mlsae.model import MLSAETransformer\nfrom mlsae.model.decoder import scatter_topk\nfrom mlsae.trainer import SweepConfig\nfrom mlsae.utils import get_device\n\n\n@dataclass\nclass Config(SweepConfig):\n    prompt: str = \"When Mary and John went to the store, John gave a drink to\"\n    \"\"\"The prompt to generate heatmaps for.\"\"\"\n\n    dead_threshold: float = 1e-3\n    \"\"\"The threshold activation to exclude latents.\"\"\"\n\n    mode: str = \"probs\"\n    \"\"\"Whether to plot counts, totals, or probabilities.\"\"\"\n\n    gamma: float = 0.5\n    \"\"\"Gamma value for PowerNorm. Only applies to counts and totals.\"\"\"\n\n\n@torch.no_grad()\ndef get_heatmap_data(\n    config: Config, repo_id: str, device: torch.device | str\n) -> torch.Tensor:\n    model = MLSAETransformer.from_pretrained(repo_id).to(device)\n    model.transformer.tokenizer.pad_token = model.transformer.tokenizer.eos_token\n    assert model.transformer.tokenizer.pad_token_id is not None\n\n    tokens = torch.tensor(\n        model.transformer.tokenizer.encode(\n            config.prompt,\n            padding=\"max_length\",\n            max_length=model.max_length,\n        )\n    )\n\n    inputs = model.transformer.forward(tokens.unsqueeze(0).to(device))\n    inputs = inputs[:, :, tokens.ne(model.transformer.tokenizer.pad_token_id), :]\n\n    topk = model.autoencoder.forward(inputs).topk\n\n    latents = scatter_topk(topk, model.n_latents).squeeze()\n\n    probs = latents.sum(dim=1) / latents.sum(dim=1).sum(dim=0, keepdim=True)\n\n    if config.mode == \"counts\":\n        data = latents.where(latents.gt(config.dead_threshold), 0).float().sum(dim=1)\n    elif config.mode == \"totals\":\n        data = latents.sum(dim=1)\n    elif config.mode == \"probs\":\n        latents = latents.sum(dim=1)\n        data = latents / latents.sum(dim=0, keepdim=True)\n    else:\n        raise ValueError(f\"Invalid mode: {config.mode}\")\n\n    # Exclude latents that never activate\n    mask = torch.any(data.gt(0), dim=0)\n    data = data[:, mask]\n\n    layers = torch.arange(0, model.n_layers, device=device).unsqueeze(-1)\n\n    _, indices = (probs[:, mask] * layers).sum(0).sort(descending=True)\n\n    return data[:, indices]\n\n\ndef get_heatmap_filename(repo_id: str, mode: str) -> str:\n    return f\"heatmap_prompt_{mode}_{repo_id.split('/')[-1]}.pdf\"\n\n\ndef sweep(\n    config: Config, device: torch.device | str, out: str | os.PathLike[str] = \".out\"\n) -> None:\n    os.makedirs(out, exist_ok=True)\n    norm = None if config.mode == \"probs\" else PowerNorm(config.gamma)\n    for repo_id in config.repo_ids(transformer=True, tuned_lens=config.tuned_lens):\n        data = get_heatmap_data(config, repo_id, device)\n        save_heatmap(\n            data.cpu(),\n            os.path.join(out, get_heatmap_filename(repo_id, config.mode)),\n            norm=norm,\n        )\n\n\nif __name__ == \"__main__\":\n    sweep(parse(Config), get_device())\n"}
{"type": "source_file", "path": "figures/num_layers.py", "content": "import os\nfrom dataclasses import dataclass\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom simple_parsing import parse\n\nfrom figures.test import parse_mlsae_repo_id\nfrom mlsae.analysis.dists import Dists\nfrom mlsae.trainer import SweepConfig\nfrom mlsae.utils import get_device\n\n\n@dataclass\nclass Config(SweepConfig):\n    filename: str = \"num_layers.csv\"\n    \"\"\"The filename to save the results to.\"\"\"\n\n    threshold: int = 10_000\n    \"\"\"The minimum non-zero activations to be considered 'active' at a layer.\"\"\"\n\n\ndef main(\n    config: Config, device: torch.device, out: str | os.PathLike[str] = \".out\"\n) -> None:\n    os.makedirs(out, exist_ok=True)\n    rows: list[dict[str, str | int | float]] = []\n    for repo_id in config.repo_ids(transformer=True, tuned_lens=config.tuned_lens):\n        dists = Dists.load(repo_id, device)\n        values = torch.where(dists.counts >= config.threshold, 1, 0).sum(0).float()\n\n        repo_id = repo_id.split(\"/\")[-1]\n        model_name, expansion_factor, k, tuned_lens = parse_mlsae_repo_id(repo_id)\n\n        rows.append(\n            {\n                \"model_name\": model_name,\n                \"n_layers\": dists.n_layers,\n                \"n_latents\": dists.n_latents,\n                \"expansion_factor\": expansion_factor,\n                \"k\": k,\n                \"tuned_lens\": tuned_lens,\n                \"mean\": values.mean().item(),\n                \"var\": values.var().item(),\n                \"std\": values.std().item(),\n                \"sem\": values.std().item() / values.size(0) ** 0.5,\n                \"rel\": values.mean().item() / dists.n_layers,\n            }\n        )\n\n        values = values.cpu().numpy()\n        hist, bins = np.histogram(\n            values, bins=dists.n_layers, range=(0, dists.n_layers)\n        )\n        hist = np.append(hist, 0)\n        pd.DataFrame({\"bins\": bins, \"hist\": hist}).to_csv(\n            os.path.join(out, f\"num_layers_{repo_id}_{config.threshold}.csv\"),\n            index=False,\n        )\n\n    pd.DataFrame(rows).to_csv(os.path.join(out, config.filename), index=False)\n\n\nif __name__ == \"__main__\":\n    main(parse(Config), get_device())\n"}
{"type": "source_file", "path": "mlsae/api/analyser.py", "content": "\"\"\"\nA helper class to analyse a pretrained MLSAE.\nBased on https://github.com/callummcdougall/sae_vis.\n\"\"\"\n\nfrom typing import NamedTuple\n\nimport einops\nimport torch\nimport torch.nn.functional as F\nfrom jaxtyping import Float, Int\nfrom loguru import logger\nfrom pydantic import BaseModel\nfrom torch import Tensor\n\nfrom mlsae.analysis.examples import Examples\nfrom mlsae.api.models import (\n    Example,\n    LatentActivations,\n    LayerHistograms,\n    Logit,\n    LogitChanges,\n    MaxLogits,\n    Token,\n)\nfrom mlsae.model import MLSAETransformer\nfrom mlsae.model.decoder import scatter_topk\nfrom mlsae.utils import cache_method\n\n\nclass DefaultParams(BaseModel):\n    bins: int = 64\n    \"\"\"The number of equal-width bins for histograms.\"\"\"\n\n    num_tokens: int = 16\n    \"\"\"The number of logits to return for each token position.\"\"\"\n\n\nclass Output(NamedTuple):\n    \"\"\"A thin wrapper around the outputs of the autoencoder forward pass.\"\"\"\n\n    tokens: Int[Tensor, \"pos\"]\n    inputs: Float[Tensor, \"n_layers pos n_inputs\"]\n    latents: Float[Tensor, \"n_layers pos n_latents\"]\n    recons: Float[Tensor, \"n_layers pos n_inputs\"]\n    metrics: dict[str, Float[Tensor, \"\"]]\n\n\nclass LogitsProbs(NamedTuple):\n    logits: Float[Tensor, \"pos d_vocab\"]\n    probs: Float[Tensor, \"pos d_vocab\"]\n\n\nclass Analyser:\n    def __init__(\n        self,\n        repo_id: str,\n        device: torch.device | str = \"cpu\",\n        default_params: DefaultParams | None = None,\n    ) -> None:\n        logger.info(f\"repo_id: {repo_id}\")\n        self.model = MLSAETransformer.from_pretrained(repo_id).to(device)\n        self.model.requires_grad_(False)\n\n        self.autoencoder = self.model.autoencoder\n        self.transformer = self.model.transformer\n        self.transformer.skip_special_tokens = False\n        self.tokenizer = self.model.transformer.tokenizer\n\n        self.default_params = default_params or DefaultParams()\n\n        self.examples = None\n        try:\n            self.examples = Examples(repo_id)\n        except Exception as exception:\n            logger.warning(f\"no examples found: {exception}\")\n\n    def params(self) -> dict:\n        return self.model.hparams_initial\n\n    def _convert_text_to_ids(self, text: str) -> list[int]:\n        return self.tokenizer.encode(text)\n\n    def _convert_ids_to_tokens(self, ids: list[int]) -> list[str]:\n        tokens: list[str] = self.tokenizer.convert_ids_to_tokens(\n            ids, skip_special_tokens=False\n        )  # type: ignore\n        return [replace_special(token) for token in tokens]\n\n    def _convert_text_to_batch(self, text: str) -> Int[Tensor, \"batch pos\"]:\n        return torch.tensor(self._convert_text_to_ids(text)).unsqueeze(0)\n\n    @cache_method()\n    def _forward(self, text: str) -> Output:\n        \"\"\"Forward pass through the transformer and autoencoder.\"\"\"\n\n        tokens = self._convert_text_to_batch(text).to(self.transformer.model.device)\n        inputs = self.transformer.forward(tokens)\n        topk, recons, auxk, auxk_recons, dead = self.autoencoder.forward(inputs)\n        latents = scatter_topk(topk, self.model.n_latents)\n\n        metrics = self.model.train_metrics.forward(\n            inputs=inputs,\n            indices=topk.indices,\n            values=topk.values,\n            recons=recons,\n        )\n\n        return Output(\n            tokens=tokens.squeeze(),\n            inputs=inputs.squeeze(),\n            latents=latents.squeeze(),\n            recons=recons.squeeze(),\n            metrics=metrics,\n        )\n\n    def latent_examples(self, layer: int, latent: int) -> list[Example]:\n        \"\"\"Find the maximally activating examples for the specified latent and layer.\"\"\"\n\n        if self.examples is None:\n            return []\n        return [\n            Example(\n                latent=example.latent,\n                layer=example.layer,\n                token_id=example.token_id,\n                token=example.token,\n                act=example.act,\n                token_ids=example.token_ids,\n                tokens=[replace_special(token) for token in example.tokens],\n                acts=example.acts,\n            )\n            for example in self.examples.get(layer, latent)\n        ]\n\n    def prompt_tokens(self, prompt: str) -> list[Token]:\n        \"\"\"Tokenize the specified prompt.\"\"\"\n\n        ids = self._convert_text_to_ids(prompt)\n        return [\n            Token(id=id, token=token, pos=pos)\n            for pos, (id, token) in enumerate(\n                zip(ids, self._convert_ids_to_tokens(ids), strict=False)\n            )\n        ]\n\n    def prompt_metrics(self, prompt: str) -> dict[str, float]:\n        \"\"\"Find the metric values for the specified prompt.\"\"\"\n\n        return {k: v.item() for k, v in self._forward(prompt).metrics.items()}\n\n    def prompt_latent_activations(self, prompt: str) -> LatentActivations:\n        \"\"\"Find the latent activations for the specified prompt.\"\"\"\n\n        latents = self._forward(prompt).latents\n        max = einops.reduce(\n            latents, \"n_layers pos n_latents -> n_layers pos\", reduction=\"max\"\n        )\n        return LatentActivations(values=latents.tolist(), max=max.tolist())\n\n    def prompt_layer_histograms(\n        self, prompt: str, bins: int | None = None\n    ) -> LayerHistograms:\n        \"\"\"\n        Find layer-wise histograms of the latent activations for the specified prompt.\n        \"\"\"\n\n        bins = bins or self.default_params.bins\n\n        latents = self._forward(prompt).latents\n        max = latents.max().item()\n\n        values: list[list[int]] = []\n        edges = [round(edge, 3) for edge in torch.linspace(0, max, bins + 1).tolist()]\n        for layer in latents:\n            values.append(torch.histc(layer, bins=bins, min=0, max=max).tolist())\n            layer = layer.cpu().detach()\n\n        return LayerHistograms(values=values, edges=edges)\n\n    def prompt_logits_input(\n        self, prompt: str, num_tokens: int | None = None\n    ) -> MaxLogits:\n        \"\"\"Find the maximum logits of the transformer for the specified prompt.\"\"\"\n\n        num_tokens = num_tokens or self.default_params.num_tokens\n\n        return self._prompt_max_logits(\n            self._logit_probs(self._forward(prompt).inputs), num_tokens\n        )\n\n    def prompt_logits_recon(\n        self, prompt: str, layer: int, num_logits: int | None = None\n    ) -> tuple[MaxLogits, LogitChanges]:\n        \"\"\"\n        Find the maximum logits and changes in logits when the activations of the\n        transformer at the specified layer are reconstructed from the autoencoder\n        latents for the specified prompt.\n        \"\"\"\n\n        num_logits = num_logits or self.default_params.num_tokens\n\n        tokens, inputs, latents, recons, metrics = self._forward(prompt)\n\n        before = self._logit_probs(inputs, layer)\n        after = self._logit_probs(recons, layer)\n\n        return (\n            self._prompt_max_logits(after, num_logits),\n            self._prompt_logit_changes(before, after, num_logits),\n        )\n\n    def prompt_logits_steer(\n        self,\n        prompt: str,\n        latent: int,\n        layer: int,\n        factor: float = -1,\n        num_logits: int | None = None,\n    ) -> tuple[MaxLogits, LogitChanges]:\n        \"\"\"\n        Find the maximum logits and changes in logits when the activations of the\n        transformer at the specified layer are steered by the specified autoencoder\n        latent for the specified prompt.\n        \"\"\"\n\n        num_logits = num_logits or self.default_params.num_tokens\n\n        inputs = self._forward(prompt).inputs\n        steered = inputs + factor * self.autoencoder.decoder.weight[:, latent]\n\n        before = self._logit_probs(inputs, layer)\n        after = self._logit_probs(steered, layer)\n\n        return (\n            self._prompt_max_logits(after, num_logits),\n            self._prompt_logit_changes(before, after, num_logits),\n        )\n\n    def _logit_probs(\n        self, x: Float[Tensor, \"n_layers pos n_inputs\"], layer: int | None = None\n    ) -> LogitsProbs:\n        \"\"\"\n        Find the logits and softmax-normalized probabilities when the specified\n        activations are passed through the transformer at the specified layer.\n        \"\"\"\n\n        start_at_layer = layer or self.transformer.model.config.num_hidden_layers - 1\n\n        logits = self.transformer.forward_at_layer(\n            x.unsqueeze(1), start_at_layer, return_type=\"logits\"\n        ).squeeze()\n\n        return LogitsProbs(logits=logits, probs=F.softmax(logits, dim=-1))\n\n    def _prompt_max_logits(self, after: LogitsProbs, k: int) -> MaxLogits:\n        \"\"\"Find the k largest logits and softmax-normalized probabilities.\"\"\"\n\n        def pos_max_logits(pos: int) -> list[Logit]:\n            topk = torch.topk(after.logits[pos], k)\n\n            ids = topk.indices.tolist()\n            tokens = self._convert_ids_to_tokens(ids)\n            logits = topk.values.tolist()\n            probs = torch.gather(after.probs[pos], 0, topk.indices).tolist()\n\n            return [\n                Logit(id=id, token=token or \"\", logit=logit, prob=prob)\n                for id, token, logit, prob in zip(\n                    ids, tokens, logits, probs, strict=False\n                )\n            ]\n\n        return MaxLogits(\n            max=[pos_max_logits(pos) for pos in range(after.logits.size(0))]\n        )\n\n    def _prompt_logit_changes(\n        self, before: LogitsProbs, after: LogitsProbs, k: int\n    ) -> LogitChanges:\n        \"\"\"Find the k largest positive and negative changes in the specified logits.\"\"\"\n\n        change = after.logits - before.logits\n\n        def pos_logit_changes(pos: int, largest: bool) -> list[Logit]:\n            topk = torch.topk(change[pos], k, largest=largest)\n            ids = topk.indices.tolist()\n            tokens = self._convert_ids_to_tokens(ids)\n            logits = topk.values.tolist()\n            return [\n                Logit(id=id, token=token or \"\", logit=logit)\n                for id, token, logit in zip(ids, tokens, logits, strict=False)\n            ]\n\n        max, min = [], []\n        for pos in range(after.logits.size(0)):\n            max.append(pos_logit_changes(pos, True))\n            min.append(pos_logit_changes(pos, False))\n        return LogitChanges(max=max, min=min)\n\n\n# Based on https://github.com/callummcdougall/sae_vis/blob/eee2ac65737a63f1442416d1206f9ad23ffb9e07/sae_vis/utils_fns.py#L199-L271.\ndef replace_special(token: str) -> str:\n    \"\"\"Replace special tokenization characters with human-readable equivalents.\"\"\"\n\n    for k, v in {\n        \"âĢĶ\": \"—\",\n        \"âĢĵ\": \"–\",\n        \"âĢĭ\": \"\",\n        \"âĢľ\": '\"',\n        \"âĢĿ\": '\"',\n        \"âĢĺ\": \"'\",\n        \"âĢĻ\": \"'\",\n        \"Ġ\": \" \",\n        \"Ċ\": \"\\n\",\n        \"ĉ\": \"\\t\",\n    }.items():\n        token = token.replace(k, v) if token is not None else \"\"\n    return token\n"}
{"type": "source_file", "path": "figures/layer_std.py", "content": "import os\nfrom dataclasses import dataclass\n\nimport pandas as pd\nimport torch\nfrom simple_parsing import parse\n\nfrom mlsae.analysis.dists import Dists, get_stats\nfrom mlsae.trainer import SweepConfig\nfrom mlsae.utils import get_device, get_repo_id\n\n\n@dataclass\nclass Config(SweepConfig):\n    filename: str = \"layer_std.csv\"\n    \"\"\"The name of the file to save the results to.\"\"\"\n\n\ndef main(\n    config: Config, device: torch.device, out: str | os.PathLike[str] = \".out\"\n) -> None:\n    os.makedirs(out, exist_ok=True)\n    rows: list[dict[str, str | int | float]] = []\n    for model_name, expansion_factor, k in config:\n        repo_id = get_repo_id(\n            model_name=model_name,\n            expansion_factor=expansion_factor,\n            k=k,\n            tuned_lens=config.tuned_lens,\n            transformer=True,\n        )\n        dists = Dists.load(repo_id, device)\n        stats = get_stats(dists.layer_std)\n        rows.append(\n            {\n                \"model_name\": model_name,\n                \"n_layers\": dists.n_layers,\n                \"n_latents\": dists.n_latents,\n                \"expansion_factor\": expansion_factor,\n                \"k\": k,\n                \"tuned_lens\": config.tuned_lens,\n                **stats,\n                **{f\"{k}_rel\": v / dists.n_layers for k, v in stats.items()},\n            }\n        )\n    pd.DataFrame(rows).to_csv(os.path.join(out, config.filename), index=False)\n\n\nif __name__ == \"__main__\":\n    main(parse(Config), get_device())\n"}
{"type": "source_file", "path": "mlsae/analysis/__init__.py", "content": ""}
{"type": "source_file", "path": "figures/mmcs.py", "content": "import os\nfrom dataclasses import dataclass\n\nimport pandas as pd\nimport torch\nfrom simple_parsing import parse\nfrom tqdm import tqdm\n\nfrom mlsae.model import MLSAETransformer\nfrom mlsae.trainer import SweepConfig\nfrom mlsae.utils import get_device, get_repo_id, normalize\n\n\n@dataclass\nclass Config(SweepConfig):\n    filename: str = \"mmcs.csv\"\n    \"\"\"The name of the file to save the results to.\"\"\"\n\n\n@torch.no_grad()\ndef get_max_cos_sim(\n    model_name: str,\n    expansion_factor: int,\n    k: int,\n    tuned_lens: bool,\n    max_latents: int = 16384,\n    chunk_size: int = 1024,\n    device: torch.device | str = \"cpu\",\n) -> tuple[torch.Tensor, int]:\n    repo_id = get_repo_id(\n        model_name=model_name,\n        expansion_factor=expansion_factor,\n        k=k,\n        tuned_lens=tuned_lens,\n        transformer=True,\n    )\n    mlsae = MLSAETransformer.from_pretrained(repo_id).to(device).autoencoder\n    W_dec = normalize(mlsae.decoder.weight.detach())\n\n    _, n_latents = W_dec.shape\n    if n_latents < max_latents:\n        # Compute the full cosine similarity matrix\n        cos_sim = torch.triu(torch.mm(W_dec.T, W_dec), diagonal=1)\n        max_cos_sim = cos_sim.max(dim=0).values\n    else:\n        # Compute the maximum cosine similarities in chunks\n        max_cos_sim = torch.zeros(n_latents, device=device)\n        for i in tqdm(range(0, n_latents, chunk_size), total=n_latents // chunk_size):\n            chunk_W_dec = W_dec[:, i : i + chunk_size]\n            chunk_cos_sim = torch.mm(W_dec.T, chunk_W_dec)\n            mask = torch.ones_like(chunk_cos_sim, dtype=torch.bool, device=device)\n            mask[: i + chunk_size, :] = torch.triu(\n                mask[: i + chunk_size, :], diagonal=1\n            )\n            chunk_cos_sim = chunk_cos_sim.masked_fill(~mask, float(\"-inf\"))\n            chunk_max_cos_sim = torch.max(chunk_cos_sim, dim=0).values\n            max_cos_sim[i : i + chunk_size] = torch.max(\n                max_cos_sim[i : i + chunk_size], chunk_max_cos_sim\n            )\n    return max_cos_sim.cpu(), mlsae.n_latents\n\n\ndef main(\n    config: Config, device: torch.device, out: str | os.PathLike[str] = \".out\"\n) -> None:\n    os.makedirs(out, exist_ok=True)\n    rows: list[dict[str, str | int | float]] = []\n    for model_name, expansion_factor, k in config:\n        max_cos_sim, n_latents = get_max_cos_sim(\n            model_name=model_name,\n            expansion_factor=expansion_factor,\n            k=k,\n            tuned_lens=config.tuned_lens,\n            device=device,\n        )\n        rows.append(\n            {\n                \"model_name\": model_name,\n                \"n_latents\": n_latents,\n                \"expansion_factor\": expansion_factor,\n                \"k\": k,\n                \"tuned_lens\": config.tuned_lens,\n                \"mean\": max_cos_sim.mean().item(),\n                \"var\": max_cos_sim.var().item(),\n                \"std\": max_cos_sim.std().item(),\n                \"sem\": max_cos_sim.std().item() / max_cos_sim.size(0) ** 0.5,\n            }\n        )\n    pd.DataFrame(rows).to_csv(os.path.join(out, config.filename), index=False)\n\n\nif __name__ == \"__main__\":\n    main(parse(Config), get_device())\n"}
{"type": "source_file", "path": "figures/wdec_sim.py", "content": "import math\nimport os\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom simple_parsing import parse\n\nfrom mlsae.model import MLSAETransformer\nfrom mlsae.trainer import SweepConfig\nfrom mlsae.utils import get_device, normalize\n\n\ndef get_filename(repo_id: str, mode: str) -> str:\n    return f\"wdec_sim_{mode}_{repo_id.split('/')[-1]}.csv\"\n\n\ndef get_positive(\n    shape: torch.Size, n_repeats: int, std: float, device: torch.device\n) -> torch.Tensor:\n    positive = torch.normal(\n        0,\n        1,\n        (shape[0], math.ceil(shape[1] / n_repeats)),\n        device=device,\n    ).repeat(1, n_repeats)[:, : shape[1]]\n    positive += torch.normal(0, std, positive.shape, device=device)\n    return normalize(positive)\n\n\ndef get_pairwise_sims(x: torch.Tensor, chunk_size: int = 1024) -> torch.Tensor:\n    _, n_elements = x.shape\n    cos_sim = torch.empty((n_elements * (n_elements - 1)) // 2, device=x.device)\n    idx = 0\n    for i in range(0, n_elements, chunk_size):\n        chunk_i_end = min(i + chunk_size, n_elements)\n        chunk_i = x[:, i:chunk_i_end]\n        for j in range(i, n_elements, chunk_size):\n            if j < i:\n                continue\n            chunk_j_end = min(j + chunk_size, n_elements)\n            chunk_j = x[:, j:chunk_j_end]\n            chunk_cos_sim = torch.mm(chunk_i.T, chunk_j)\n            if i == j:\n                triu_indices = torch.triu_indices(\n                    chunk_i_end - i, chunk_j_end - j, offset=1\n                )\n                chunk_cos_sim = chunk_cos_sim[triu_indices[0], triu_indices[1]]\n            else:\n                chunk_cos_sim = chunk_cos_sim.view(-1)\n            next_idx = idx + chunk_cos_sim.shape[0]\n            cos_sim[idx:next_idx] = chunk_cos_sim\n            idx = next_idx\n    return cos_sim\n\n\ndef get_hist(x: torch.Tensor) -> tuple[np.ndarray, np.ndarray]:\n    hist, bins = np.histogram(\n        get_pairwise_sims(normalize(x)).cpu().numpy(), bins=200, range=(-1, 1)\n    )\n    hist = np.append(hist, 0)\n    return bins, hist\n\n\ndef main(\n    config: SweepConfig, device: torch.device, out: str | os.PathLike[str] = \".out\"\n) -> None:\n    os.makedirs(out, exist_ok=True)\n    for repo_id in config.repo_ids(transformer=True, tuned_lens=config.tuned_lens):\n        model = MLSAETransformer.from_pretrained(repo_id).to(device)\n        autoencoder = model.autoencoder\n        shape = autoencoder.decoder.weight.shape\n\n        bins, actual = get_hist(autoencoder.decoder.weight.detach())\n\n        # Negative control: n_latents IID Gaussian vectors\n        _, negative = get_hist(torch.normal(0, 1, shape, device=device))\n\n        # Positive control: n_latents // n_layers IID Gaussian vectors, repeated\n        # n_layers times with a small amount of noise\n        _, positive1 = get_hist(get_positive(shape, model.n_layers, 0.1, device))\n        _, positive2 = get_hist(get_positive(shape, model.n_layers, 0.2, device))\n        _, positive3 = get_hist(get_positive(shape, model.n_layers, 0.3, device))\n        _, positive4 = get_hist(get_positive(shape, model.n_layers, 0.4, device))\n        _, positive5 = get_hist(get_positive(shape, model.n_layers, 0.5, device))\n        _, positive6 = get_hist(get_positive(shape, model.n_layers, 0.6, device))\n        _, positive7 = get_hist(get_positive(shape, model.n_layers, 0.7, device))\n        _, positive8 = get_hist(get_positive(shape, model.n_layers, 0.8, device))\n        _, positive9 = get_hist(get_positive(shape, model.n_layers, 0.9, device))\n        _, positive10 = get_hist(get_positive(shape, model.n_layers, 1.0, device))\n        _, positive11 = get_hist(get_positive(shape, model.n_layers, 1.1, device))\n        _, positive12 = get_hist(get_positive(shape, model.n_layers, 1.2, device))\n        _, positive13 = get_hist(get_positive(shape, model.n_layers, 1.3, device))\n        _, positive14 = get_hist(get_positive(shape, model.n_layers, 1.4, device))\n        _, positive15 = get_hist(get_positive(shape, model.n_layers, 1.5, device))\n        _, positive16 = get_hist(get_positive(shape, model.n_layers, 1.6, device))\n        _, positive17 = get_hist(get_positive(shape, model.n_layers, 1.7, device))\n        _, positive18 = get_hist(get_positive(shape, model.n_layers, 1.8, device))\n        _, positive19 = get_hist(get_positive(shape, model.n_layers, 1.9, device))\n        _, positive20 = get_hist(get_positive(shape, model.n_layers, 2.0, device))\n\n        pd.DataFrame(\n            {\n                \"bins\": bins,\n                \"actual\": actual,\n                \"negative\": negative,\n                \"positive1\": positive1,\n                \"positive2\": positive2,\n                \"positive3\": positive3,\n                \"positive4\": positive4,\n                \"positive5\": positive5,\n                \"positive6\": positive6,\n                \"positive7\": positive7,\n                \"positive8\": positive8,\n                \"positive9\": positive9,\n                \"positive10\": positive10,\n                \"positive11\": positive11,\n                \"positive12\": positive12,\n                \"positive13\": positive13,\n                \"positive14\": positive14,\n                \"positive15\": positive15,\n                \"positive16\": positive16,\n                \"positive17\": positive17,\n                \"positive18\": positive18,\n                \"positive19\": positive19,\n                \"positive20\": positive20,\n            }\n        ).to_csv(\n            os.path.join(out, f\"wdec_sim_{repo_id.split('/')[-1]}.csv\"), index=False\n        )\n\n\nif __name__ == \"__main__\":\n    main(parse(SweepConfig), get_device())\n"}
{"type": "source_file", "path": "mlsae/metrics/auxiliary_loss.py", "content": "import torch\nfrom jaxtyping import Float\nfrom torchmetrics import Metric\n\n\nclass AuxiliaryLoss(Metric):\n    \"\"\"\n    The auxiliary loss (AuxK) models the reconstruction error using the top-`k_aux` dead\n    latents (typically `d_model // 2`) [Gao et al., 2024].\n    Latents are flagged as dead during training if they have not activated for\n    some predetermined number of tokens (typically 10 million).\n\n    Then, given the reconstruction error of the main model `e = inputs - recons`, we\n    define the auxiliary loss as the MSE between `e` and the reconstruction using the\n    top `k_aux` dead latents.\n    We compute the MSE normalization per token, because the scale of the error changes\n    throughout training.\n    \"\"\"\n\n    is_differentiable = True\n    full_state_update = False\n\n    auxk_coef: float\n    \"\"\"Coefficient of the auxiliary loss.\"\"\"\n\n    auxk_mse: Float[torch.Tensor, \"\"]\n    \"\"\"Sum of MSEs between reconstruction errors and top-`k_aux` reconstructions.\"\"\"\n\n    def __init__(self, auxk_coef: float) -> None:\n        super().__init__()\n        self.auxk_coef = auxk_coef\n        self.add_state(\n            \"auxk_mse\",\n            torch.zeros(1, dtype=torch.float),\n            dist_reduce_fx=\"sum\",\n        )\n\n    def update(\n        self,\n        inputs: Float[torch.Tensor, \"n_layers batch pos n_inputs\"],\n        recons: Float[torch.Tensor, \"n_layers batch pos n_inputs\"],\n        auxk_recons: Float[torch.Tensor, \"n_layers batch pos n_inputs\"] | None,\n        **kwargs,\n    ) -> None:\n        if auxk_recons is not None:\n            error = inputs - recons\n            self.auxk_mse.add_(\n                (error - auxk_recons).pow(2).mean()\n                / (error - torch.mean(error, dim=3, keepdim=True)).pow(2).mean()\n            )\n\n    def compute(self) -> Float[torch.Tensor, \"\"]:\n        return self.auxk_coef * self.auxk_mse.nan_to_num(0)\n"}
{"type": "source_file", "path": "layer_dists.py", "content": "import os\nfrom dataclasses import dataclass, field\n\nimport pandas as pd\nimport torch\nfrom datasets import Dataset\nfrom safetensors.torch import save_file\nfrom simple_parsing import Serializable, parse\nfrom tqdm import tqdm\n\nfrom mlsae.analysis.dists import Dists, Metric, get_stats\nfrom mlsae.model import get_test_dataloader\nfrom mlsae.model.data import DataConfig\nfrom mlsae.trainer import initialize\nfrom mlsae.utils import forward_single_layer, get_device, get_repo_id, load_single_layer\n\n\n@dataclass\nclass Config(Serializable):\n    model_name: str\n    layer: int\n    expansion_factor: int = 64\n    k: int = 32\n    tuned_lens: bool = False\n\n    data: DataConfig = field(default_factory=DataConfig)\n    \"\"\"The data configuration. Remember to set max_tokens to a reasonable value!\"\"\"\n\n    seed: int = 42\n    \"\"\"The seed for global random state.\"\"\"\n\n    log_every_n_steps: int | None = 8\n    \"\"\"The number of steps between logging statistics.\"\"\"\n\n    push_to_hub: bool = True\n    \"\"\"Whether to push the dataset to HuggingFace.\"\"\"\n\n\n@torch.no_grad()\ndef get_tensors(config: Config, device: torch.device) -> dict[str, torch.Tensor]:\n    model = load_single_layer(\n        config.model_name,\n        config.layer,\n        device,\n        expansion_factor=config.expansion_factor,\n        k=config.k,\n        tuned_lens=config.tuned_lens,\n    )\n\n    dataloader = get_test_dataloader(\n        model.model_name,\n        config.data.max_length,\n        config.data.batch_size,\n    )\n\n    tokens_per_step = config.data.batch_size * config.data.max_length\n\n    metric = Metric(model.n_layers, model.n_latents, device)\n    rows: list[dict[str, str | int | float]] = []\n\n    for i, batch in enumerate(tqdm(dataloader, total=config.data.max_steps)):\n        inputs, recons, topk = forward_single_layer(\n            model, batch[\"input_ids\"].to(device)\n        )\n        metric.update(topk)\n\n        if config.log_every_n_steps is not None and i % config.log_every_n_steps == 0:\n            dists = Dists.from_tensors(metric.compute(), metric.device)\n            rows.append(\n                {\n                    \"model_name\": model.model_name,\n                    \"n_layers\": model.n_layers,\n                    \"n_latents\": model.n_latents,\n                    \"expansion_factor\": model.expansion_factor,\n                    \"k\": model.k,\n                    \"step\": i,\n                    \"tokens\": (i + 1) * tokens_per_step,\n                    **get_stats(dists.layer_std),\n                }\n            )\n\n        if i > config.data.max_steps:\n            break\n\n    if len(rows) > 0:\n        repo_id = get_repo_id(\n            config.model_name,\n            config.expansion_factor,\n            config.k,\n            config.tuned_lens,\n            True,\n            [config.layer],\n        ).split(\"/\")[-1]\n        pd.DataFrame(rows).to_csv(\n            os.path.join(\"out\", f\"dists_layer_std_step_{repo_id}.csv\"), index=False\n        )\n\n    return metric.compute()\n\n\ndef main(config: Config, device: torch.device) -> None:\n    initialize(config.seed)\n\n    tensors = get_tensors(config, device)\n    repo_id = get_repo_id(\n        config.model_name,\n        config.expansion_factor,\n        config.k,\n        config.tuned_lens,\n        True,\n        [config.layer],\n    )\n    repo_id = Dists.repo_id(repo_id)\n    filename = Dists.filename(repo_id)\n\n    save_file(tensors, filename)\n    _test = Dists.from_tensors(tensors, device)\n    _test = Dists.from_file(filename, device)\n\n    if config.push_to_hub:\n        dataset = Dataset.from_generator(Dists(tensors).__iter__)\n        assert isinstance(dataset, Dataset)\n        dataset.push_to_hub(repo_id, commit_description=config.dumps_json())\n        _test = Dists.from_dataset(dataset, device)\n        _test = Dists.from_hub(repo_id, device)\n\n\nif __name__ == \"__main__\":\n    main(parse(Config), get_device())\n"}
{"type": "source_file", "path": "mlsae/api/models.py", "content": "from pydantic import BaseModel\n\n\nclass Token(BaseModel):\n    id: int\n    \"\"\"The token id.\"\"\"\n    token: str\n    \"\"\"The token string.\"\"\"\n    pos: int\n    \"\"\"The token position.\"\"\"\n\n\nclass Logit(BaseModel):\n    id: int\n    \"\"\"The token id.\"\"\"\n    token: str\n    \"\"\"The token string.\"\"\"\n    logit: float\n    \"\"\"The logit value.\"\"\"\n    prob: float | None = None\n    \"\"\"The softmax-normalized logit value.\"\"\"\n\n\nclass MaxLogits(BaseModel):\n    max: list[list[Logit]]\n    \"\"\"The maximum logit values for each token position.\"\"\"\n\n\nclass LogitChanges(BaseModel):\n    max: list[list[Logit]]\n    \"\"\"The maximum changes in logit values for each token position.\"\"\"\n    min: list[list[Logit]]\n    \"\"\"The minimum changes in logit values for each token position.\"\"\"\n\n\nclass LatentActivations(BaseModel):\n    values: list[list[list[float]]]\n    \"\"\"The latent activations for each layer, position, and latent dimension.\"\"\"\n    max: list[list[float]]\n    \"\"\"The maximum latent activations for each layer and token position.\"\"\"\n\n\nclass LayerHistograms(BaseModel):\n    values: list[list[int]]\n    \"\"\"The histogram values for each layer.\"\"\"\n    edges: list[float]\n    \"\"\"The histogram edges across all layers.\"\"\"\n\n\nclass Example(BaseModel):\n    latent: int\n    \"The latent index.\"\n    layer: int\n    \"The layer index.\"\n    token_id: int\n    \"\"\"The token id for the maximum activation.\"\"\"\n    token: str\n    \"\"\"The token string for the maximum activation.\"\"\"\n    act: float\n    \"\"\"The maximum activation value.\"\"\"\n    token_ids: list[int]\n    \"\"\"The token ids around the maximum.\"\"\"\n    tokens: list[str]\n    \"\"\"The token strings around the maximum.\"\"\"\n    acts: list[float]\n    \"\"\"The activation values around the maximum.\"\"\"\n"}
{"type": "source_file", "path": "figures/scatter_freq.py", "content": "import os\n\nimport numpy as np\nimport torch\nfrom matplotlib import pyplot as plt\nfrom simple_parsing import parse\n\nfrom mlsae.analysis.dists import Dists\nfrom mlsae.trainer import SweepConfig\nfrom mlsae.utils import get_device\n\n\ndef main(\n    config: SweepConfig, device: torch.device, out: str | os.PathLike[str] = \".out\"\n) -> None:\n    os.makedirs(out, exist_ok=True)\n    figsize, dpi = (6, 6), 300\n\n    for repo_id in config.repo_ids(tuned_lens=config.tuned_lens):\n        model_name = repo_id.split(\"/\")[-1]\n        dists = Dists.load(repo_id, device)\n\n        fig, ax = plt.subplots(1, 1, figsize=figsize, dpi=dpi)\n        ax.set_xlim(0, 1e7)\n        cmap = plt.colormaps[\"viridis\"]\n        colors = cmap(np.linspace(0, 1), dists.n_layers)\n\n        for layer, color in zip(range(dists.n_layers), colors, strict=False):\n            ax.scatter(\n                dists.counts[layer],\n                dists.totals[layer],\n                s=2,\n                alpha=0.5,\n                color=color,\n            )\n        ax.legend([f\"Layer {i}\" for i in range(dists.n_layers)], loc=\"upper left\")\n\n        fig.savefig(os.path.join(out, f\"scatter_freq_{model_name}.png\"), format=\"png\")\n        plt.close(fig)\n\n\nif __name__ == \"__main__\":\n    main(parse(SweepConfig), get_device())\n"}
{"type": "source_file", "path": "figures/resid_sim.py", "content": "import math\nimport os\n\nimport einops\nimport pandas as pd\nimport torch\nfrom simple_parsing import parse\nfrom tqdm import tqdm\nfrom tuned_lens import TunedLens\n\nfrom mlsae.model import PythiaTransformer, get_test_dataloader\nfrom mlsae.trainer import RunConfig, initialize\nfrom mlsae.utils import get_device, normalize\n\n\nclass VarianceMetric:\n    def __init__(\n        self, size: tuple[int, ...] = (1,), device: torch.device | str = \"cpu\"\n    ) -> None:\n        self.count = 0\n        self.mean = torch.zeros(size, device=device)\n        self.squared = torch.zeros(size, device=device)\n\n    def update(self, x: torch.Tensor) -> None:\n        self.count += x.shape[0]\n        delta1 = x - self.mean\n        self.mean += torch.sum(delta1, dim=0) / self.count\n        delta2 = x - self.mean\n        self.squared += torch.sum(delta1 * delta2, dim=0)\n\n    def compute(self) -> dict[str, torch.Tensor]:\n        var = self.squared / (self.count - 1)\n        std = var.sqrt()\n        sem = std / math.sqrt(self.count)\n        return dict(mean=self.mean, var=var, std=std, sem=sem)\n\n\n@torch.no_grad()\ndef main(\n    config: RunConfig, device: torch.device, out: str | os.PathLike[str] = \".out\"\n) -> None:\n    os.makedirs(out, exist_ok=True)\n    initialize(config.seed)\n\n    transformer = PythiaTransformer(\n        config.model_name,\n        config.data.max_length,\n        config.data.batch_size,\n        config.autoencoder.skip_special_tokens,\n        layers=config.layers,\n        device=torch.device(device),\n    )\n    transformer.model.to(device)  # type: ignore\n\n    lens = (\n        TunedLens.from_model_and_pretrained(\n            transformer.model,\n            transformer.model_name,\n            map_location=device,\n        )\n        if config.autoencoder.tuned_lens\n        else None\n    )\n    lens_name = \"lens_\" if lens is not None else \"\"\n\n    def forward_lens(inputs: torch.Tensor) -> torch.Tensor:\n        if lens is None:\n            return inputs\n        lens.to(inputs.device)\n        for layer in range(transformer.n_layers):\n            inputs[layer, ...] = lens.transform_hidden(inputs[layer, ...], layer)\n        return inputs\n\n    dataloader = get_test_dataloader(\n        config.model_name,\n        config.data.max_length,\n        config.data.batch_size,\n    )\n\n    model_name = config.model_name.split(\"/\")[-1]\n\n    means = [\n        VarianceMetric(size=(transformer.config.hidden_size,), device=device)\n        for _ in range(transformer.n_layers)\n    ]\n    l2_norms = [\n        VarianceMetric(size=(1,), device=device) for _ in range(transformer.n_layers)\n    ]\n    cos_sims = [\n        VarianceMetric(size=(1,), device=device)\n        for _ in range(transformer.n_layers - 1)\n    ]\n\n    # First, compute the mean residual stream activation vectors over the dataset\n    # https://www.lesswrong.com/s/6njwz6XdSYwNhtsCJ/p/eLNo7b56kQQerCzp2\n    for i, batch in tqdm(enumerate(dataloader), total=config.data.max_steps):\n        x = forward_lens(transformer.forward(batch[\"input_ids\"].to(device)))\n        x = einops.rearrange(x, \"l b p i -> l (b p) i\")\n        for layer in range(transformer.n_layers):\n            means[layer].update(x[layer, ...])\n            l2_norms[layer].update(x[layer, ...].norm(dim=-1))\n        if i > config.data.max_steps:\n            break\n\n    l2_norms = [metric.compute() for metric in l2_norms]\n    df = pd.DataFrame([{k: v.item() for k, v in layer.items()} for layer in l2_norms])\n    df.index.name = \"layer\"\n    df.to_csv(os.path.join(\"out\", f\"resid_l2_norm_{lens_name}{model_name}.csv\"))\n\n    means = [metric.compute() for metric in means]\n    means = torch.stack([metric[\"mean\"] for metric in means])  # l i\n    assert means.shape == (transformer.n_layers, transformer.config.hidden_size)\n\n    # Then, compute the mean cosine similarities between centered residual stream\n    # activation vectors at adjacent layers\n    for i, batch in tqdm(enumerate(dataloader), total=config.data.max_steps):\n        x = forward_lens(transformer.forward(batch[\"input_ids\"].to(device)))\n        x = einops.rearrange(x, \"l b p i -> l (b p) i\")\n        x = x - means.unsqueeze(1)\n        x = normalize(x, -1)\n        for layer in range(transformer.n_layers - 1):\n            cos_sim = einops.einsum(x[layer], x[layer + 1], \"bp i, bp i -> bp\")\n            cos_sims[layer].update(cos_sim.flatten())\n        if i > config.data.max_steps:\n            break\n\n    data = [metric.compute() for metric in cos_sims]\n    data = [{k: v.item() for k, v in layer.items()} for layer in data]\n\n    df = pd.DataFrame(data)\n    df.index.name = \"start_at_layer\"\n    df.to_csv(os.path.join(out, f\"resid_cos_sim_{lens_name}{model_name}.csv\"))\n\n\nif __name__ == \"__main__\":\n    main(parse(RunConfig), get_device())\n"}
{"type": "source_file", "path": "mlsae/analysis/dists.py", "content": "import os\nfrom collections.abc import Generator\nfrom dataclasses import dataclass\nfrom functools import cached_property\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom datasets import Dataset, DatasetDict, load_dataset\nfrom safetensors.torch import load_file, save_file\nfrom simple_parsing import Serializable, field, parse\nfrom tqdm import tqdm\n\nfrom mlsae.model import DataConfig, MLSAETransformer, TopK, get_test_dataloader\nfrom mlsae.trainer import initialize\nfrom mlsae.utils import get_device\n\n\n@dataclass\nclass Config(Serializable):\n    repo_id: str\n    \"\"\"\n    The name of a pretrained autoencoder and transformer from HuggingFace, or the path\n    to a directory that contains them.\n    \"\"\"\n\n    data: DataConfig = field(default_factory=DataConfig)\n    \"\"\"The data configuration. Remember to set max_tokens to a reasonable value!\"\"\"\n\n    seed: int = 42\n    \"\"\"The seed for global random state.\"\"\"\n\n    log_every_n_steps: int | None = 8\n    \"\"\"The number of steps between logging statistics.\"\"\"\n\n    push_to_hub: bool = True\n    \"\"\"Whether to push the dataset to HuggingFace.\"\"\"\n\n\nclass Metric:\n    def __init__(\n        self, n_layers: int, n_latents: int, device: torch.device | str\n    ) -> None:\n        self.n_layers = n_layers\n        self.n_latents = n_latents\n        self.counts = torch.zeros((n_layers, n_latents), device=device)\n        self.totals = torch.zeros((n_layers, n_latents), device=device)\n        self.layers = torch.arange(n_layers, device=device).unsqueeze(1)\n        self.device = device\n\n    def update(self, x: TopK) -> None:\n        for layer in range(self.n_layers):\n            indices = x.indices[layer].squeeze().view(-1)\n            values = x.values[layer].squeeze().view(-1)\n            ones = torch.ones_like(indices, dtype=torch.float)\n            self.counts[layer].put_(indices, ones, accumulate=True)\n            self.totals[layer].put_(indices, values, accumulate=True)\n\n    def compute(self) -> dict[str, torch.Tensor]:\n        return dict(counts=self.counts, totals=self.totals)\n\n\ndef get_stats(layer_std: torch.Tensor) -> dict[str, float]:\n    values = layer_std.cpu().numpy()\n    std = np.nanstd(values).item()\n    return {\n        \"mean\": np.nanmean(values).item(),\n        \"var\": np.nanvar(values).item(),\n        \"std\": std,\n        \"sem\": std / np.sqrt(len(values)),\n    }\n\n\n@torch.no_grad()\ndef get_tensors(config: Config, device: torch.device | str) -> dict[str, torch.Tensor]:\n    model = MLSAETransformer.from_pretrained(config.repo_id).to(device)\n\n    dataloader = get_test_dataloader(\n        model.model_name,\n        config.data.max_length,\n        config.data.batch_size,\n    )\n\n    tokens_per_step = config.data.batch_size * config.data.max_length\n\n    metric = Metric(model.n_layers, model.n_latents, device)\n    rows: list[dict[str, str | int | float]] = []\n\n    for i, batch in enumerate(tqdm(dataloader, total=config.data.max_steps)):\n        inputs = model.transformer.forward(batch[\"input_ids\"].to(device))\n        topk, auxk, stats, dead = model.autoencoder.encode(inputs)\n        metric.update(topk)\n\n        if config.log_every_n_steps is not None and i % config.log_every_n_steps == 0:\n            dists = Dists.from_tensors(metric.compute(), metric.device)\n            rows.append(\n                {\n                    \"model_name\": model.model_name,\n                    \"n_layers\": model.n_layers,\n                    \"n_latents\": model.n_latents,\n                    \"expansion_factor\": model.expansion_factor,\n                    \"k\": model.k,\n                    \"step\": i,\n                    \"tokens\": (i + 1) * tokens_per_step,\n                    **get_stats(dists.layer_std),\n                }\n            )\n\n        if i > config.data.max_steps:\n            break\n\n    if len(rows) > 0:\n        repo_id = config.repo_id.split(\"/\")[-1]\n        pd.DataFrame(rows).to_csv(\n            os.path.join(\"out\", f\"dists_layer_std_step_{repo_id}.csv\"), index=False\n        )\n\n    return metric.compute()\n\n\nclass Dists:\n    def __init__(\n        self,\n        tensors: dict[str, torch.Tensor] | None = None,\n        filename: str | os.PathLike[str] | None = None,\n        device: torch.device | str | int = \"cpu\",\n    ):\n        device = str(device) if isinstance(device, torch.device) else device\n        if tensors is not None:\n            self.tensors = {k: v.to(device) for k, v in tensors.items()}\n        elif filename is not None:\n            self.tensors = load_file(filename, device)\n        else:\n            raise ValueError(\"either tensors or filename must be provided\")\n        self.counts = self.tensors[\"counts\"]  # n_layers n_latents\n        self.totals = self.tensors[\"totals\"]  # n_layers n_latents\n        self.n_layers, self.n_latents = self.counts.shape\n        self.layers = torch.arange(self.n_layers, device=device).unsqueeze(1)\n\n    @cached_property\n    def count(self) -> torch.Tensor:\n        return self.counts.sum(0)  # n_latents\n\n    @cached_property\n    def total(self) -> torch.Tensor:\n        return self.totals.sum(0)  # n_latents\n\n    @cached_property\n    def mean(self) -> torch.Tensor:\n        return self.total / (self.count + 1e-8)  # n_latents\n\n    @cached_property\n    def means(self) -> torch.Tensor:\n        return self.totals / (self.counts + 1e-8)  # n_layers n_latents\n\n    @cached_property\n    def probs(self) -> torch.Tensor:\n        return self.totals / self.totals.sum(0)  # n_layers n_latents\n\n    @cached_property\n    def entropies(self) -> torch.Tensor:\n        return -(self.probs * self.probs.log()).sum(0)  # n_latents\n\n    @cached_property\n    def layer_mean(self) -> torch.Tensor:\n        return (self.probs * self.layers).sum(0)  # n_latents\n\n    @cached_property\n    def layer_var(self) -> torch.Tensor:\n        return (self.probs * self.layers**2).sum(0) - self.layer_mean**2  # n_latents\n\n    @cached_property\n    def layer_std(self) -> torch.Tensor:\n        return self.layer_var.sqrt()  # n_latents\n\n    def __iter__(self) -> Generator[dict[str, list[float] | float], None, None]:\n        for latent in range(self.n_latents):\n            yield {\n                \"latent\": latent,\n                \"count\": self.count[latent].item(),\n                \"total\": self.total[latent].item(),\n                \"mean\": self.mean[latent].item(),\n                \"layer_mean\": self.layer_mean[latent].item(),\n                \"layer_var\": self.layer_var[latent].item(),\n                \"layer_std\": self.layer_std[latent].item(),\n                \"counts\": self.counts[:, latent].tolist(),\n                \"totals\": self.totals[:, latent].tolist(),\n                \"means\": self.means[:, latent].tolist(),\n                \"probs\": self.probs[:, latent].tolist(),\n            }\n\n    @staticmethod\n    def load(repo_id: str, device: torch.device | str | int) -> \"Dists\":\n        repo_id = Dists.repo_id(repo_id)\n        try:\n            filename = Dists.filename(repo_id)\n            return Dists.from_file(filename, str(device))\n        except Exception:\n            return Dists.from_hub(repo_id, str(device))\n\n    @staticmethod\n    def from_tensors(\n        tensors: dict[str, torch.Tensor], device: torch.device | str | int\n    ) -> \"Dists\":\n        return Dists(tensors=tensors, device=device)\n\n    @staticmethod\n    def from_file(\n        filename: str | os.PathLike[str], device: torch.device | str | int\n    ) -> \"Dists\":\n        return Dists(filename=filename, device=device)\n\n    @staticmethod\n    def from_dataset(dataset: Dataset, device: torch.device | str | int) -> \"Dists\":\n        n_layers, n_latents = len(dataset[\"counts\"][0]), len(dataset[\"counts\"])\n        tensors = {\n            \"counts\": torch.zeros((n_layers, n_latents), device=device),\n            \"totals\": torch.zeros((n_layers, n_latents), device=device),\n        }\n        for i, item in enumerate(dataset):\n            assert isinstance(item, dict)\n            tensors[\"counts\"][:, i] = torch.tensor(item[\"counts\"], device=device)\n            tensors[\"totals\"][:, i] = torch.tensor(item[\"totals\"], device=device)\n        return Dists(tensors=tensors, device=device)\n\n    @staticmethod\n    def from_hub(repo_id: str, device: torch.device | str | int) -> \"Dists\":\n        dataset = load_dataset(Dists.repo_id(repo_id))\n        assert isinstance(dataset, DatasetDict)\n        return Dists.from_dataset(dataset[\"train\"], device)\n\n    @staticmethod\n    def repo_id(repo_id: str) -> str:\n        if repo_id.endswith(\"-dists\"):\n            return repo_id\n        if repo_id.endswith(\"-tfm\"):\n            return repo_id.replace(\"-tfm\", \"-dists\")\n        return repo_id + \"-dists\"\n\n    @staticmethod\n    def filename(repo_id: str) -> str:\n        os.makedirs(\"out\", exist_ok=True)\n        return os.path.join(\n            \"out\", f\"{Dists.repo_id(repo_id).replace('/', '-')}.safetensors\"\n        )\n\n\ndef main(config: Config, device: torch.device | str) -> None:\n    initialize(config.seed)\n\n    tensors = get_tensors(config, device)\n    repo_id = Dists.repo_id(config.repo_id)\n    filename = Dists.filename(repo_id)\n\n    save_file(tensors, filename)\n    _test = Dists.from_tensors(tensors, device)\n    _test = Dists.from_file(filename, device)\n\n    if config.push_to_hub:\n        dataset = Dataset.from_generator(Dists(tensors, filename, device).__iter__)\n        assert isinstance(dataset, Dataset)\n        dataset.push_to_hub(repo_id, commit_description=config.dumps_json())\n        _test = Dists.from_dataset(dataset, device)\n        _test = Dists.from_hub(repo_id, device)\n\n\nif __name__ == \"__main__\":\n    main(parse(Config), get_device())\n"}
{"type": "source_file", "path": "mlsae/metrics/layerwise_l1_norm.py", "content": "import torch\nfrom jaxtyping import Float, Int64\nfrom torchmetrics import Metric\n\n\nclass LayerwiseL1Norm(Metric):\n    \"\"\"L1 norm. Average sum of absolute latent activations.\"\"\"\n\n    is_differentiable = False\n    full_state_update = False\n\n    layer_abs: Float[torch.Tensor, \"n_layers\"]\n    \"\"\"Layerwise sum of absolute latent activations.\"\"\"\n\n    tokens: Int64[torch.Tensor, \"\"]\n    \"\"\"Layerwise count of tokens.\"\"\"\n\n    def __init__(self, n_layers: int) -> None:\n        super().__init__()\n        self.add_state(\n            \"layer_abs\", torch.zeros(n_layers, dtype=torch.float), dist_reduce_fx=\"sum\"\n        )\n        self.add_state(\n            \"tokens\", torch.zeros(1, dtype=torch.int64), dist_reduce_fx=\"sum\"\n        )\n\n    @torch.no_grad()\n    def update(\n        self, values: Float[torch.Tensor, \"n_layers batch pos k\"], **kwargs\n    ) -> None:\n        self.layer_abs.add_(torch.sum(torch.abs(values), dim=(1, 2, 3)))\n        self.tokens.add_(values.shape[1] * values.shape[2])\n\n    @torch.no_grad()\n    def compute(self) -> Float[torch.Tensor, \"n_layers\"]:\n        return self.layer_abs / self.tokens\n"}
{"type": "source_file", "path": "mlsae/api/__main__.py", "content": "from dataclasses import dataclass\n\nimport orjson\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse\nfrom pydantic import BaseModel\nfrom simple_parsing import Serializable, parse\n\nfrom mlsae.trainer import initialize\nfrom mlsae.utils import get_device\n\nfrom .analyser import Analyser\nfrom .models import (\n    Example,\n    LatentActivations,\n    LayerHistograms,\n    LogitChanges,\n    MaxLogits,\n    Token,\n)\n\n\n@dataclass\nclass Config(Serializable):\n    repo_id: str\n    \"\"\"\n    The name of a pretrained autoencoder and transformer from HuggingFace, or the path\n    to a directory that contains them.\n    \"\"\"\n\n\nconfig = parse(Config)\nanalyser = Analyser(repo_id=config.repo_id, device=get_device())\n\n\nclass ORJSONResponse(JSONResponse):\n    media_type = \"application/json\"\n\n    def render(self, content) -> bytes:\n        return orjson.dumps(content)\n\n\napp = FastAPI(\n    docs_url=\"/api/py/docs\",\n    openapi_url=\"/api/py/openapi.json\",\n    default_response_class=ORJSONResponse,\n)\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n\n@app.get(\"/api/py/params\")\nasync def params() -> dict:\n    return analyser.params()\n\n\nclass ExamplesRequest(BaseModel):\n    layer: int\n    latent: int\n\n\n@app.post(\"/api/py/examples\")\nasync def examples(body: ExamplesRequest) -> list[Example]:\n    return analyser.latent_examples(body.layer, body.latent)\n\n\nclass PromptRequest(BaseModel):\n    prompt: str\n\n\n@app.post(\"/api/py/prompt/tokens\")\nasync def prompt_tokens(body: PromptRequest) -> list[Token]:\n    return analyser.prompt_tokens(body.prompt)\n\n\n@app.post(\"/api/py/prompt/metrics\")\nasync def prompt_metrics(body: PromptRequest) -> dict[str, float]:\n    return analyser.prompt_metrics(body.prompt)\n\n\n@app.post(\"/api/py/prompt/latent-activations\")\nasync def prompt_latent_activations(body: PromptRequest) -> LatentActivations:\n    return analyser.prompt_latent_activations(body.prompt)\n\n\n@app.post(\"/api/py/prompt/layer-histograms\")\nasync def prompt_layer_histograms(body: PromptRequest) -> LayerHistograms:\n    return analyser.prompt_layer_histograms(body.prompt)\n\n\n@app.post(\"/api/py/prompt/logits-input\")\nasync def prompt_logits_input(body: PromptRequest) -> MaxLogits:\n    return analyser.prompt_logits_input(body.prompt)\n\n\nclass PromptLogitsReconRequest(BaseModel):\n    prompt: str\n    layer: int\n\n\n@app.post(\"/api/py/prompt/logits-recon\")\nasync def prompt_logits_recon(\n    body: PromptLogitsReconRequest,\n) -> tuple[MaxLogits, LogitChanges]:\n    return analyser.prompt_logits_recon(body.prompt, body.layer)\n\n\nclass PromptLogitsSteerRequest(BaseModel):\n    prompt: str\n    latent: int\n    layer: int\n    factor: float\n\n\n@app.post(\"/api/py/prompt/logits-steer\")\nasync def prompt_logits_steer(\n    body: PromptLogitsSteerRequest,\n) -> tuple[MaxLogits, LogitChanges]:\n    return analyser.prompt_logits_steer(\n        body.prompt, body.latent, body.layer, body.factor\n    )\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    initialize(42)\n    uvicorn.run(app, port=8001)\n"}
{"type": "source_file", "path": "mlsae/metrics/layerwise_loss_delta.py", "content": "import torch\nfrom jaxtyping import Float\nfrom torchmetrics import Metric\n\n\nclass LayerwiseLossDelta(Metric):\n    \"\"\"\n    Downstream loss (replace the inputs by the reconstruction during the forward pass).\n\n    The average delta between the cross-entropy loss for the inputs and reconstructions.\n    \"\"\"\n\n    is_differentiable = False\n    full_state_update = False\n\n    layer_delta_loss: Float[torch.Tensor, \"n_layers\"]\n    \"\"\"Layerwise sum of deltas between cross-entropy losses.\"\"\"\n\n    def __init__(self, n_layers: int) -> None:\n        super().__init__()\n        self.add_state(\n            \"layer_delta_loss\", default=torch.zeros(n_layers), dist_reduce_fx=\"mean\"\n        )\n\n    @torch.no_grad()\n    def update(\n        self,\n        loss_true: Float[torch.Tensor, \"n_layers\"],\n        loss_pred: Float[torch.Tensor, \"n_layers\"],\n        **kwargs,\n    ) -> None:\n        self.layer_delta_loss.add_(loss_pred - loss_true)\n\n    @torch.no_grad()\n    def compute(self) -> Float[torch.Tensor, \"n_layers\"]:\n        return self.layer_delta_loss\n"}
{"type": "source_file", "path": "mlsae/analysis/variances.py", "content": "import os\nfrom dataclasses import dataclass\n\nimport einops\nimport pandas as pd\nimport torch\nfrom simple_parsing import field, parse\nfrom tqdm import tqdm\n\nfrom mlsae.model import DataConfig, MLSAETransformer, get_test_dataloader\nfrom mlsae.model.decoder import scatter_topk\nfrom mlsae.trainer.config import SweepConfig, initialize\nfrom mlsae.utils import get_device\n\n\n@dataclass\nclass Config(SweepConfig):\n    data: DataConfig = field(default_factory=DataConfig)\n    \"\"\"The data configuration. Remember to set max_tokens to a reasonable value!\"\"\"\n\n    seed: int = 42\n    \"\"\"The seed for global random state.\"\"\"\n\n    filename: str = \"variances.csv\"\n    \"\"\"The name of the file to save the results to.\"\"\"\n\n\nclass Metric:\n    def __init__(\n        self,\n        n_layers: int,\n        n_tokens: int,\n        n_latents: int,\n        device: torch.device | str = \"cpu\",\n    ) -> None:\n        self.n_layers = n_layers\n        self.n_tokens = n_tokens\n        self.n_latents = n_latents\n        self.layers = torch.arange(self.n_layers, device=device)\n\n        self.exp_var_l_f = []\n        self.exp_var_l_tf = []\n        self.var_l = []\n        self.rel_var_f = []\n        self.rel_var_t = []\n\n    def var(self, x: torch.Tensor):\n        layers = self.layers.view((self.n_layers, *([1] * (len(x.shape) - 1))))\n\n        ell = (layers * x).sum(dim=0)\n        ell_sq = ((layers**2) * x).sum(dim=0)\n        return ell_sq - ell**2\n\n    def update(self, latents: torch.Tensor):\n        assert latents.shape == (self.n_layers, self.n_tokens, self.n_latents)\n\n        probs = latents / latents.sum(dim=0)\n        probs = probs.nan_to_num_(0.0)\n\n        e_var_l_f = self.var(probs.mean(1)).mean()\n        e_var_l_tf = self.var(probs).mean()\n        var_l = self.var(probs.mean((1, 2)))\n\n        self.exp_var_l_f.append(e_var_l_f)\n        self.exp_var_l_tf.append(e_var_l_tf)\n        self.var_l.append(var_l)\n        self.rel_var_f.append(e_var_l_f / var_l)\n        self.rel_var_t.append(e_var_l_tf / e_var_l_f)\n\n    def compute(self) -> dict[str, float]:\n        return dict(\n            exp_var_l_f=torch.stack(self.exp_var_l_f).mean().item(),\n            exp_var_l_tf=torch.stack(self.exp_var_l_tf).mean().item(),\n            var_l=torch.stack(self.var_l).mean().item(),\n            rel_var_f=torch.stack(self.rel_var_f).mean().item(),\n            rel_var_t=torch.stack(self.rel_var_t).mean().item(),\n        )\n\n\n@torch.no_grad()\ndef main(\n    repo_id: str,\n    data: DataConfig,\n    device: torch.device | str = \"cpu\",\n    out: str | os.PathLike[str] = \".out\",\n) -> dict:\n    model = MLSAETransformer.from_pretrained(repo_id).to(device)\n\n    dataloader = get_test_dataloader(model.model_name, data.max_length, data.batch_size)\n\n    tokens_per_step = data.batch_size * data.max_length\n\n    metric = Metric(model.n_layers, tokens_per_step, model.n_latents, device)\n\n    i = 0\n    for i, batch in enumerate(tqdm(dataloader, total=data.max_steps)):\n        inputs = model.transformer.forward(batch[\"input_ids\"].to(device))\n        topk, auxk, stats, dead = model.autoencoder.encode(inputs)\n\n        latents = scatter_topk(topk, model.n_latents)\n        latents = einops.rearrange(latents, \"l b t f -> l (b t) f\")\n\n        metric.update(latents)\n\n        if i > data.max_steps:\n            break\n\n    row = {\n        \"model_name\": model.model_name,\n        \"n_layers\": model.n_layers,\n        \"n_latents\": model.n_latents,\n        \"expansion_factor\": model.expansion_factor,\n        \"k\": model.k,\n        \"step\": i,\n        \"tokens\": (i + 1) * tokens_per_step,\n        **metric.compute(),\n    }\n    pd.DataFrame({k: [v] for k, v in row.items()}).to_csv(\n        os.path.join(out, f\"variances_{repo_id.split(\"/\")[-1]}.csv\"), index=False\n    )\n    return row\n\n\ndef sweep(\n    config: Config, device: torch.device, out: str | os.PathLike[str] = \".out\"\n) -> None:\n    initialize(config.seed)\n    rows: list[dict] = []\n    for repo_id in config.repo_ids(transformer=True):\n        rows.append(main(repo_id, config.data, device=device))\n    pd.DataFrame(rows).to_csv(os.path.join(out, config.filename), index=False)\n\n\nif __name__ == \"__main__\":\n    sweep(parse(Config), get_device())\n"}
{"type": "source_file", "path": "mlsae/__init__.py", "content": ""}
{"type": "source_file", "path": "mlsae/api/__init__.py", "content": ""}
{"type": "source_file", "path": "mlsae/metrics/dead_latents.py", "content": "import torch\nfrom jaxtyping import Float, Int\nfrom torchmetrics import Metric\n\n\nclass DeadLatents(Metric):\n    \"\"\"\n    Estimate the fraction of dead latents from the number of tokens activated by each\n    latent and the number of tokens elapsed in a training step.\n\n    Note that we consider a latent live if it is activated *at any layer*.\n    \"\"\"\n\n    is_differentiable = False\n    full_state_update = False\n\n    latent_tokens: Float[torch.Tensor, \"n_latents\"]\n    \"\"\"Count of tokens activated by each latent.\"\"\"\n\n    tokens: Int[torch.Tensor, \"\"]\n    \"\"\"Count of tokens.\"\"\"\n\n    def __init__(self, n_latents: int, dead_tokens_threshold: float) -> None:\n        super().__init__()\n        self.n_latents = n_latents\n        self.dead_tokens_threshold = dead_tokens_threshold\n        self.add_state(\n            \"latent_tokens\",\n            torch.zeros(n_latents, dtype=torch.float),\n            dist_reduce_fx=\"sum\",\n        )\n        self.add_state(\n            \"tokens\", default=torch.tensor(0, dtype=torch.int64), dist_reduce_fx=\"sum\"\n        )\n\n    @torch.no_grad()\n    def update(\n        self, indices: Int[torch.Tensor, \"n_layers batch pos k\"], **kwargs\n    ) -> None:\n        self.latent_tokens.add_(\n            torch.bincount(indices.int().reshape(-1), minlength=self.n_latents)\n        )\n        self.tokens += indices.shape[1] * indices.shape[2]\n\n    @torch.no_grad()\n    def compute(self) -> torch.Tensor:\n        return (\n            torch.sum(\n                self.latent_tokens < self.tokens / self.dead_tokens_threshold,\n                dtype=torch.float,\n            )\n            / self.n_latents\n        )\n"}
{"type": "source_file", "path": "mlsae/metrics/layerwise_fvu.py", "content": "import torch\nfrom jaxtyping import Float\nfrom torchmetrics import Metric\n\n\nclass LayerwiseFVU(Metric):\n    \"\"\"\n    Fraction of variance unexplained (FVU). MSE divided by the input variance.\n\n    Equivalent to normalized MSE in Gao et al. [2024], except we compute the variance\n    per batch instead of once at the beginning of training.\n    \"\"\"\n\n    is_differentiable = True\n    full_state_update = False\n\n    layer_mse: Float[torch.Tensor, \"n_layers\"]\n    \"\"\"Layerwise sum of MSEs between the inputs and reconstructions.\"\"\"\n\n    layer_var: Float[torch.Tensor, \"n_layers\"]\n    \"\"\"Layerwise sum of variances of the inputs.\"\"\"\n\n    def __init__(self, n_layers: int) -> None:\n        super().__init__()\n        self.add_state(\n            \"layer_mse\", torch.zeros(n_layers, dtype=torch.float), dist_reduce_fx=\"sum\"\n        )\n        self.add_state(\n            \"layer_var\", torch.zeros(n_layers, dtype=torch.float), dist_reduce_fx=\"sum\"\n        )\n\n    @torch.no_grad()\n    def update(\n        self,\n        inputs: Float[torch.Tensor, \"n_layers batch pos n_inputs\"],\n        recons: Float[torch.Tensor, \"n_layers batch pos n_inputs\"],\n        **kwargs,\n    ) -> None:\n        self.layer_mse.add_(torch.mean((recons - inputs).pow(2), dim=(1, 2, 3)))\n        self.layer_var.add_(torch.var(inputs, dim=(1, 2, 3)))\n\n    def compute(self) -> Float[torch.Tensor, \"n_layers\"]:\n        return self.layer_mse / self.layer_var\n"}
{"type": "source_file", "path": "mlsae/metrics/layerwise_logit_kl_div.py", "content": "import torch\nimport torch.nn.functional as F\nfrom jaxtyping import Float\nfrom torchmetrics import Metric\n\n\nclass LayerwiseLogitKLDiv(Metric):\n    \"\"\"\n    Downstream loss (replace the inputs by the reconstruction during the forward pass).\n\n    The mean KL divergence between the logits for the inputs and reconstructions.\n    \"\"\"\n\n    is_differentiable = False\n    full_state_update = False\n\n    layer_logit_kl_div: Float[torch.Tensor, \"n_layers\"]\n    \"\"\"Layerwise sum of KL divergences between logits.\"\"\"\n\n    def __init__(self, n_layers: int) -> None:\n        super().__init__()\n        self.n_layers = n_layers\n        self.add_state(\n            \"layer_logit_kl_div\",\n            default=torch.zeros(n_layers),\n            dist_reduce_fx=\"mean\",\n        )\n\n    @torch.no_grad()\n    def update(\n        self,\n        logits_true: Float[torch.Tensor, \"n_layers batch pos d_vocab\"],\n        logits_pred: Float[torch.Tensor, \"n_layers batch pos d_vocab\"],\n        **kwargs,\n    ) -> None:\n        # NOTE: Iterate over layers to reduce memory usage.\n        for layer in range(self.n_layers):\n            self.layer_logit_kl_div[layer].add_(\n                F.kl_div(\n                    F.log_softmax(logits_true[layer], dim=-1),\n                    F.log_softmax(logits_pred[layer], dim=-1),\n                    log_target=True,\n                    reduction=\"batchmean\",\n                )\n            )\n\n    @torch.no_grad()\n    def compute(self) -> Float[torch.Tensor, \"n_layers\"]:\n        return self.layer_logit_kl_div\n"}
{"type": "source_file", "path": "mlsae/metrics/layerwise.py", "content": "from functools import partial\n\nimport torch\nfrom jaxtyping import Float\nfrom torchmetrics import ClasswiseWrapper, Metric\n\n\n# Based on https://github.com/ai-safety-foundation/sparse_autoencoder/blob/b6ba6cb7c90372cb5462855c21e5f52fc9130557/sparse_autoencoder/metrics/wrappers/classwise.py\nclass LayerwiseWrapper(ClasswiseWrapper):\n    def __init__(self, metric: Metric, labels: list[str], prefix: str) -> None:\n        super().__init__(metric, labels=labels, prefix=prefix)\n\n    def _convert_output(self, x: Float[torch.Tensor, \"layer\"]) -> dict:\n        if x.ndim == 0:\n            x = x.unsqueeze(0)\n        metrics = super()._convert_output(x)\n        return {**metrics, f\"{self._prefix}avg\": x.mean(dim=0, dtype=torch.float)}\n\n\ndef layerwise(n_layers: int) -> partial[LayerwiseWrapper]:\n    return partial(\n        LayerwiseWrapper,\n        labels=[f\"layer_{layer}\" for layer in range(n_layers)],\n    )\n"}
{"type": "source_file", "path": "mlsae/metrics/__init__.py", "content": "from .auxiliary_loss import AuxiliaryLoss\nfrom .dead_latents import DeadLatents\nfrom .layerwise import LayerwiseWrapper, layerwise\nfrom .layerwise_fvu import LayerwiseFVU\nfrom .layerwise_l0_norm import LayerwiseL0Norm\nfrom .layerwise_l1_norm import LayerwiseL1Norm\nfrom .layerwise_logit_kl_div import LayerwiseLogitKLDiv\nfrom .layerwise_logit_mse import LayerwiseLogitMSE\nfrom .layerwise_loss_delta import LayerwiseLossDelta\nfrom .layerwise_mse import LayerwiseMSE\nfrom .mse_loss import MSELoss\n\n__all__ = [\n    \"AuxiliaryLoss\",\n    \"DeadLatents\",\n    \"layerwise\",\n    \"LayerwiseFVU\",\n    \"LayerwiseL0Norm\",\n    \"LayerwiseL1Norm\",\n    \"LayerwiseLogitKLDiv\",\n    \"LayerwiseLogitMSE\",\n    \"LayerwiseLossDelta\",\n    \"LayerwiseMSE\",\n    \"LayerwiseWrapper\",\n    \"MSELoss\",\n]\n"}
{"type": "source_file", "path": "mlsae/analysis/examples.py", "content": "import json\nimport os\nimport sqlite3\nfrom collections.abc import Generator\nfrom dataclasses import dataclass\nfrom os import PathLike\nfrom pathlib import Path\nfrom typing import NamedTuple\n\nimport einops\nimport torch\nfrom datasets import Dataset, DatasetDict, load_dataset\nfrom loguru import logger\nfrom simple_parsing import Serializable, field, parse\nfrom tqdm import tqdm\n\nfrom mlsae.model import DataConfig, MLSAETransformer, get_train_dataloader\nfrom mlsae.trainer import initialize\nfrom mlsae.utils import get_device\n\n\n@dataclass\nclass Config(Serializable):\n    repo_id: str\n    \"\"\"\n    The name of a pretrained autoencoder and transformer from HuggingFace, or the path\n    to a directory that contains them.\n    \"\"\"\n\n    data: DataConfig = field(default_factory=DataConfig)\n    \"\"\"The data configuration. Remember to set max_tokens to a reasonable value!\"\"\"\n\n    seed: int = 42\n    \"\"\"The seed for global random state.\"\"\"\n\n    n_examples: int = 32\n    \"\"\"The number of examples to keep for each latent and layer.\"\"\"\n\n    n_tokens: int = 4\n    \"\"\"The number of tokens to include either side of the maximum activation.\"\"\"\n\n    delete_every_n_steps: int = 10\n    \"\"\"The number of steps between deleting examples not in the top n_examples.\"\"\"\n\n    push_to_hub: bool = True\n    \"\"\"Whether to push the dataset to HuggingFace.\"\"\"\n\n\nclass Example(NamedTuple):\n    latent: int\n    layer: int\n    token_id: int\n    token: str\n    act: float\n    token_ids: list[int]\n    tokens: list[str]\n    acts: list[float]\n\n    def serialize(self) -> tuple[int | str | float, ...]:\n        return (\n            self.latent,\n            self.layer,\n            self.token_id,\n            self.token,\n            self.act,\n            json.dumps(self.token_ids),\n            json.dumps(self.tokens),\n            json.dumps(self.acts),\n        )\n\n    @staticmethod\n    def from_row(row: tuple) -> \"Example\":\n        return Example(\n            row[0],\n            row[1],\n            row[2],\n            row[3],\n            row[4],\n            json.loads(row[5]),\n            json.loads(row[6]),\n            json.loads(row[7]),\n        )\n\n    @staticmethod\n    def from_dict(data: dict) -> \"Example\":\n        return Example(\n            data[\"latent\"],\n            data[\"layer\"],\n            data[\"token_id\"],\n            data[\"token\"],\n            data[\"act\"],\n            json.loads(data[\"token_ids\"]),\n            json.loads(data[\"tokens\"]),\n            json.loads(data[\"acts\"]),\n        )\n\n\nclass Examples:\n    conn: sqlite3.Connection\n\n    def __init__(self, repo_id: str) -> None:\n        repo_id = Examples.repo_id(repo_id)\n        filename = Examples.filename(repo_id)\n\n        if Path(filename).exists():\n            logger.info(f\"connecting to database: {filename}\")\n            self.conn = sqlite3.connect(filename)\n\n        else:\n            logger.info(f\"loading dataset: {repo_id}\")\n            dataset = load_dataset(repo_id)\n            assert isinstance(dataset, DatasetDict)\n            dataset = dataset[\"train\"]\n\n            logger.info(f\"creating database: {filename}\")\n            self.conn, cursor = create_db(filename)\n            batch = []\n            for i, example in tqdm(enumerate(dataset.to_list())):\n                batch.append(Example.from_dict(example))\n                if i % 1000 == 0:\n                    insert_examples(cursor, batch)\n                    self.conn.commit()\n                    batch = []\n            insert_examples(cursor, batch)\n            self.conn.commit()\n\n    def get(self, layer: int, latent: int) -> list[Example]:\n        return select_examples(self.conn.cursor(), latent, layer)\n\n    @staticmethod\n    def repo_id(repo_id: str) -> str:\n        if repo_id.endswith(\"-examples\"):\n            return repo_id\n        if repo_id.endswith(\"-tfm\"):\n            return repo_id.replace(\"-tfm\", \"-examples\")\n        return repo_id + \"-examples\"\n\n    @staticmethod\n    def filename(repo_id: str) -> str:\n        os.makedirs(\"out\", exist_ok=True)\n        return os.path.join(\"out\", f\"{Examples.repo_id(repo_id).replace('/', '-')}.db\")\n\n\ndef get_examples(\n    model: MLSAETransformer,\n    batch: dict[str, torch.Tensor],\n    n_tokens: int,\n    dead_threshold: float = 1e-3,\n    device: torch.device | str = \"cpu\",\n) -> Generator[Example, None, None]:\n    batch_tokens = batch[\"input_ids\"].to(device)\n    inputs = model.transformer.forward(batch_tokens)\n    topk, auxk, stats, dead = model.autoencoder.encode(inputs)\n\n    batch_tokens = einops.rearrange(batch_tokens, \"b s -> (b s)\")\n    batch_acts = einops.rearrange(topk.values, \"l b s k -> l (b s) k\").half()\n    batch_latents = einops.rearrange(topk.indices, \"l b s k -> l (b s) k\")\n\n    layers, positions, indices = torch.where(batch_acts > dead_threshold)\n    for layer, pos, k, latent in torch.stack(\n        [layers, positions, indices, batch_latents[layers, positions, indices]], dim=1\n    ).tolist():\n        token_id = int(batch_tokens[pos].item())\n        token = model.transformer.tokenizer.decode(token_id)\n        act = batch_acts[layer, pos, k].item()\n\n        token_ids = batch_tokens[pos - n_tokens : pos + n_tokens].tolist()\n        tokens: list[str] = model.transformer.tokenizer.convert_ids_to_tokens(token_ids)  # type: ignore\n        acts = (\n            torch.where(\n                batch_latents[layer, pos - n_tokens : pos + n_tokens] == latent,\n                batch_acts[layer, pos - n_tokens : pos + n_tokens],\n                0.0,\n            )\n            .sum(dim=-1)\n            .tolist()\n        )\n\n        yield Example(latent, layer, token_id, token, act, token_ids, tokens, acts)\n\n\ndef create_db(database: str | PathLike) -> tuple[sqlite3.Connection, sqlite3.Cursor]:\n    conn = sqlite3.connect(database)\n    cursor = conn.cursor()\n    cursor.execute(\n        \"\"\"\n        CREATE TABLE IF NOT EXISTS examples (\n            id INTEGER PRIMARY KEY,\n            latent INTEGER,\n            layer INTEGER,\n            token_id INTEGER,\n            token TEXT,\n            act REAL,\n            token_ids JSON,\n            tokens JSON,\n            acts JSON\n        )\n        \"\"\",\n    )\n    conn.commit()\n    return conn, cursor\n\n\ndef insert_examples(cursor: sqlite3.Cursor, examples: list[Example]) -> None:\n    cursor.executemany(\n        \"\"\"\n        INSERT INTO examples (\n            latent,\n            layer,\n            token_id,\n            token,\n            act,\n            token_ids,\n            tokens,\n            acts\n        )\n        VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n        \"\"\",\n        [example.serialize() for example in examples],\n    )\n\n\ndef delete_examples(cursor: sqlite3.Cursor, n_examples: int) -> None:\n    cursor.execute(\n        \"\"\"\n        DELETE FROM examples\n        WHERE id IN (\n            SELECT id\n            FROM (\n                SELECT id,\n                    ROW_NUMBER() OVER (\n                        PARTITION BY latent, layer\n                        ORDER BY act DESC\n                    ) as rank\n                FROM examples\n            )\n            WHERE rank > ?\n        );\n        \"\"\",\n        (n_examples,),\n    )\n\n\ndef select_examples(cursor: sqlite3.Cursor, latent: int, layer: int) -> list[Example]:\n    cursor.execute(\n        \"\"\"\n        SELECT latent,\n        layer,\n        token_id,\n        token,\n        act,\n        token_ids,\n        tokens,\n        acts\n        FROM examples\n        WHERE layer = ?\n        AND latent = ?\n        ORDER BY act DESC\n        \"\"\",\n        (layer, latent),\n    )\n    return [Example.from_row(row) for row in cursor.fetchall()]\n\n\n@torch.no_grad()\ndef save_examples(config: Config, device: torch.device | str = \"cpu\") -> None:\n    model = MLSAETransformer.from_pretrained(config.repo_id).to(device)\n\n    dataloader = get_train_dataloader(\n        config.data.path,\n        model.model_name,\n        config.data.max_length,\n        config.data.batch_size,\n    )\n\n    conn, cursor = create_db(Examples.filename(config.repo_id))\n    batch: dict[str, torch.Tensor]\n    for i, batch in tqdm(enumerate(dataloader), total=config.data.max_steps):\n        examples = list(get_examples(model, batch, config.n_tokens, device=device))\n        insert_examples(cursor, examples)\n        if i > config.data.max_steps:\n            break\n        if i % config.delete_every_n_steps == 0:\n            delete_examples(cursor, config.n_examples)\n            conn.commit()\n    delete_examples(cursor, config.n_examples)\n    conn.commit()\n\n    if config.push_to_hub:\n        repo_id = Examples.repo_id(config.repo_id)\n        dataset = Dataset.from_sql(\"SELECT * FROM examples\", conn)\n        assert isinstance(dataset, Dataset)\n        dataset.push_to_hub(repo_id, commit_description=config.dumps_json())\n\n\nif __name__ == \"__main__\":\n    device = get_device()\n    config = parse(Config)\n    initialize(config.seed)\n    save_examples(config, device)\n"}
{"type": "source_file", "path": "mlsae/metrics/layerwise_logit_mse.py", "content": "import torch\nimport torch.nn.functional as F\nfrom jaxtyping import Float\nfrom torchmetrics import Metric\n\n\nclass LayerwiseLogitMSE(Metric):\n    \"\"\"\n    Downstream loss (replace the inputs by the reconstruction during the forward pass).\n\n    The MSE between the logits for the inputs and reconstructions.\n    \"\"\"\n\n    is_differentiable = False\n    full_state_update = False\n\n    layer_logit_mse: Float[torch.Tensor, \"n_layers\"]\n    \"\"\"Layerwise sum of MSEs between logits.\"\"\"\n\n    def __init__(self, n_layers: int) -> None:\n        super().__init__()\n        self.n_layers = n_layers\n        self.add_state(\n            \"layer_logit_mse\",\n            default=torch.zeros(n_layers),\n            dist_reduce_fx=\"mean\",\n        )\n\n    @torch.no_grad()\n    def update(\n        self,\n        logits_true: Float[torch.Tensor, \"n_layers batch pos d_vocab\"],\n        logits_pred: Float[torch.Tensor, \"n_layers batch pos d_vocab\"],\n        **kwargs,\n    ) -> None:\n        # NOTE: Iterate over layers to reduce memory usage.\n        for layer in range(self.n_layers):\n            self.layer_logit_mse[layer].add_(\n                F.mse_loss(logits_true[layer], logits_pred[layer])\n            )\n\n    @torch.no_grad()\n    def compute(self) -> Float[torch.Tensor, \"n_layers\"]:\n        return self.layer_logit_mse\n"}
{"type": "source_file", "path": "mlsae/metrics/layerwise_l0_norm.py", "content": "import torch\nfrom jaxtyping import Float, Int64\nfrom torchmetrics import Metric\n\n\nclass LayerwiseL0Norm(Metric):\n    \"\"\"\n    L0 norm (sparsity). Average count of nonzero latent activations.\n\n    Fixed at k (the number of largest latents to keep) during training.\n    \"\"\"\n\n    is_differentiable = False\n    full_state_update = False\n\n    layer_nonzero: Float[torch.Tensor, \"n_layers\"]\n    \"\"\"Layerwise count of nonzero latent activations.\"\"\"\n\n    tokens: Int64[torch.Tensor, \"\"]\n    \"\"\"Count of tokens.\"\"\"\n\n    def __init__(self, n_layers: int, dead_threshold: float) -> None:\n        super().__init__()\n        self.add_state(\n            \"layer_nonzero\",\n            torch.zeros(n_layers, dtype=torch.float),\n            dist_reduce_fx=\"sum\",\n        )\n        self.add_state(\n            \"tokens\", torch.zeros(1, dtype=torch.int64), dist_reduce_fx=\"sum\"\n        )\n        self.dead_threshold = dead_threshold\n\n    @torch.no_grad()\n    def update(\n        self, values: Float[torch.Tensor, \"n_layers batch pos k\"], **kwargs\n    ) -> None:\n        self.layer_nonzero.add_(torch.sum(values > self.dead_threshold, dim=(1, 2, 3)))\n        self.tokens.add_(values.shape[1] * values.shape[2])\n\n    @torch.no_grad()\n    def compute(self) -> Float[torch.Tensor, \"n_layers\"]:\n        return self.layer_nonzero / self.tokens\n"}
{"type": "source_file", "path": "mlsae/model/autoencoders/__init__.py", "content": "from .standard import SAE, SAEOut\nfrom .topk import TopKSAE, TopKSAEOut\nfrom .utils import standardize, unit_norm_decoder, unit_norm_decoder_gradient\n\n__all__ = [\n    \"SAE\",\n    \"SAEOut\",\n    \"TopKSAE\",\n    \"TopKSAEOut\",\n    \"standardize\",\n    \"unit_norm_decoder\",\n    \"unit_norm_decoder_gradient\",\n]\n"}
{"type": "source_file", "path": "mlsae/model/autoencoders/utils.py", "content": "import einops\nimport torch\nfrom jaxtyping import Float\nfrom torch.nn import Linear\n\nfrom mlsae.model.types import Stats\n\n\ndef unit_norm_decoder(decoder: Linear) -> None:\n    \"\"\"Unit-normalize the decoder weight vectors.\"\"\"\n\n    decoder.weight.data /= decoder.weight.data.norm(dim=0)\n\n\n# TODO: Use kernels.triton_add_mul_ if it's available\n@torch.no_grad()\ndef unit_norm_decoder_gradient(decoder: Linear) -> None:\n    \"\"\"\n    Remove the component of the gradient parallel to the decoder weight vectors.\n    Assumes that the decoder weight vectors are unit-normalized.\n    NOTE: Without `@torch.no_grad()`, this causes a memory leak!\n    \"\"\"\n\n    assert decoder.weight.grad is not None\n    scalar = einops.einsum(\n        decoder.weight.grad,\n        decoder.weight,\n        \"... n_latents n_inputs, ... n_latents n_inputs -> ... n_inputs\",\n    )\n    vector = einops.einsum(\n        scalar,\n        decoder.weight,\n        \"... n_inputs, ... n_latents n_inputs -> ... n_latents n_inputs\",\n    )\n    decoder.weight.grad -= vector\n\n\ndef standardize(\n    x: Float[torch.Tensor, \"... n_inputs\"], eps: float = 1e-5\n) -> tuple[Float[torch.Tensor, \"... n_inputs\"], Stats]:\n    \"\"\"Standardize the inputs to zero mean and unit variance.\"\"\"\n\n    mu = x.mean(dim=-1, keepdim=True)\n    x = x - mu\n    std = x.std(dim=-1, keepdim=True)\n    x = x / (std + eps)\n    return x, Stats(mu, std)\n"}
{"type": "source_file", "path": "mlsae/model/decoder.py", "content": "import os\n\nimport torch\nfrom jaxtyping import Float\nfrom loguru import logger\nfrom torch import Tensor\n\nfrom mlsae.model.types import TopK\n\n\n# NOTE: Avoid this where possible to save memory!\ndef scatter_topk(topk: TopK, n_latents: int) -> Float[Tensor, \"... n_latents\"]:\n    \"\"\"\n    Scatter the k largest latents into a new tensor of shape (..., n_latents).\n\n    Args:\n        topk (TopK): The k largest latents.\n\n        n_latents (int): The number of latents.\n\n    Returns:\n        out (Float[Tensor, \"... n_latents\"]): The k largest latents.\n    \"\"\"\n\n    # ... n_latents\n    buffer = topk.values.new_zeros((*topk.indices.shape[:-1], n_latents))\n    # ... k -> ... n_latents\n    return buffer.scatter_(dim=-1, index=topk.indices, src=topk.values)\n\n\n# Based on https://github.com/EleutherAI/sae/blob/19d95a401e9d17dbf7d6fb0fa7a91081f1b0d01f/sae/utils.py\ndef decode_triton(topk: TopK, weight: Tensor) -> Tensor:\n    shape = topk.indices.shape[:-1]\n    k = topk.indices.shape[-1]\n    n_inputs, n_latents = weight.shape\n\n    indices_flat = topk.indices.view(-1, k)\n    values_flat = topk.values.view(-1, k)\n\n    output: Tensor = TritonDecoderAutograd.apply(indices_flat, values_flat, weight)  # type: ignore\n\n    return output.view(*shape, n_inputs)\n\n\ndef decode_cuda(topk: TopK, weight: Tensor, chunk_size: int = 1024) -> Tensor:\n    shape = topk.indices.shape[:-1]\n    k = topk.indices.shape[-1]\n    n_inputs, n_latents = weight.shape\n\n    indices_flat = topk.indices.view(-1, k)\n    values_flat = topk.values.view(-1, k)\n\n    batch_size = indices_flat.shape[0]\n\n    output = torch.zeros(\n        batch_size, n_inputs, device=topk.values.device, dtype=topk.values.dtype\n    )\n\n    for i in range(0, batch_size, chunk_size):\n        indices_chunk = indices_flat[i : i + chunk_size]\n        values_chunk = values_flat[i : i + chunk_size]\n\n        chunk_sparse = torch.sparse_coo_tensor(\n            indices=torch.cat(\n                [\n                    torch.arange(\n                        indices_chunk.shape[0], device=indices_chunk.device\n                    ).repeat_interleave(k),\n                    indices_chunk.flatten(),\n                ]\n            ).view(2, -1),\n            values=values_chunk.flatten(),\n            size=(indices_chunk.shape[0], n_latents),\n        )\n\n        chunk_output = torch.sparse.mm(chunk_sparse, weight.t())\n\n        output[i : i + chunk_size] = chunk_output\n\n    return output.view(*shape, n_inputs)\n\n\n# NOTE: 'sparse_coo_tensor' isn't supported yet for the MPS backend\ndef decode_mps(topk: TopK, weight: Tensor, chunk_size: int = 1024) -> Tensor:\n    shape = topk.indices.shape[:-1]\n    k = topk.indices.shape[-1]\n    n_inputs, n_latents = weight.shape\n\n    indices_flat = topk.indices.view(-1, k)\n    values_flat = topk.values.view(-1, k)\n\n    batch_size = indices_flat.shape[0]\n\n    output = torch.zeros(\n        batch_size, n_inputs, device=topk.values.device, dtype=topk.values.dtype\n    )\n\n    for i in range(0, batch_size, chunk_size):\n        indices_chunk = indices_flat[i : i + chunk_size]\n        values_chunk = values_flat[i : i + chunk_size]\n\n        weight_mask = weight[:, indices_chunk.view(-1)].view(\n            n_inputs, indices_chunk.shape[0], k\n        )\n\n        output_chunk = torch.bmm(\n            values_chunk.unsqueeze(1), weight_mask.permute(1, 2, 0)\n        ).squeeze(1)\n\n        output[i : i + chunk_size] = output_chunk\n\n    return output.view(*shape, n_inputs)\n\n\ndef decode(topk: TopK, weight: Tensor) -> Tensor:\n    \"\"\"\n    Sparse decoder implementation.\n\n    Args:\n        topk (TopK): The k largest latents.\n\n        weight (Float[Tensor, \"n_inputs n_latents\"]): The decoder weight matrix.\n\n    Returns:\n        out (Float[Tensor, \"... n_inputs\"]): The reconstructions.\n    \"\"\"\n    ...\n\n\ntry:\n    from .kernels import TritonDecoderAutograd\nexcept ImportError:\n    logger.info(\"Triton not found\")\n    if torch.backends.mps.is_available():\n        logger.info(\"MPS backend, using 'bmm' decoder\")\n        decode = decode_mps\n    else:\n        logger.info(\"CPU/CUDA backend, using 'sparse_coo_tensor' decoder\")\n        decode = decode_cuda\nelse:\n    logger.info(\"Triton found\")\n    if os.environ.get(\"USE_TRITON\", \"1\") == \"1\":\n        logger.info(\"Triton enabled, using Triton decoder\")\n        decode = decode_triton\n    else:\n        logger.info(\"Triton disabled, using 'sparse_coo_tensor' decoder\")\n        decode = decode_cuda\n"}
{"type": "source_file", "path": "mlsae/model/data.py", "content": "import math\nfrom dataclasses import dataclass\n\nimport torch\nfrom datasets import IterableDataset, load_dataset\nfrom datasets.formatting.formatting import LazyBatch\nfrom jaxtyping import Int\nfrom simple_parsing import Serializable\nfrom torch import Tensor\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, PreTrainedTokenizerBase\n\n\n@dataclass\nclass DataConfig(Serializable):\n    \"\"\"The data configuration.\"\"\"\n\n    path: str = \"monology/pile-uncopyrighted\"\n    \"\"\"The path to a HuggingFace text dataset.\"\"\"\n\n    max_length: int = 2048\n    \"\"\"The maximum length of a tokenized input sequence.\"\"\"\n\n    batch_size: int = 1\n    \"\"\"The number of sequences in a batch.\"\"\"\n\n    max_tokens: float = 1_000_000_000\n    \"\"\"The maximum number of tokens to train on.\"\"\"\n\n    num_workers: int | None = None\n    \"\"\"The number of workers to use for data loading.\"\"\"\n\n    @property\n    def max_steps(self) -> int:\n        \"\"\"The maximum number of batches to train on.\"\"\"\n\n        return math.ceil(self.max_tokens / (self.batch_size * self.max_length))\n\n\ndef concat_and_tokenize(\n    dataset: IterableDataset,\n    tokenizer: PreTrainedTokenizerBase,\n    max_length: int,\n) -> IterableDataset:\n    return dataset.map(\n        _concat_and_tokenize,\n        batched=True,\n        # Large batch size minimizes the number of tokens dropped\n        batch_size=1024,\n        # TODO: Column names are not always available\n        remove_columns=dataset.column_names or [\"text\", \"meta\"],\n        fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length},\n    ).with_format(\"torch\")\n\n\n# Based on https://github.com/EleutherAI/sae/blob/19d95a401e9d17dbf7d6fb0fa7a91081f1b0d01f/sae/data.py\ndef _concat_and_tokenize(\n    batch: LazyBatch, tokenizer: PreTrainedTokenizerBase, max_length: int\n) -> dict:\n    output = tokenizer(\n        # Concatenate the batch of text with the EOS token\n        tokenizer.eos_token.join([\"\"] + batch[\"text\"]),  # type: ignore\n        truncation=True,\n        max_length=max_length,\n        return_attention_mask=False,\n        return_overflowing_tokens=True,\n    )\n\n    overflowing_tokens = output.pop(\"overflowing_tokens\", None)\n    _ = output.pop(\"overflow_to_sample_mapping\", None)\n\n    # Split the overflowing tokens into sequences of the maximum length\n    if overflowing_tokens is not None:\n        output[\"input_ids\"] += [\n            overflowing_tokens[i * max_length : (i + 1) * max_length]\n            for i in range(math.ceil(len(overflowing_tokens) / max_length))\n        ]  # type: ignore\n\n    # Drop the last batch, which is probably incomplete\n    return {k: v[:-1] for k, v in output.items()}\n\n\ndef get_dataloader(\n    dataset: IterableDataset,\n    model_name: str,\n    max_length: int,\n    batch_size: int,\n    num_workers: int = 1,\n) -> DataLoader[Int[Tensor, \"batch pos\"]]:\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    # Constrain the maximum length of a tokenized input sequence\n    max_length = min(tokenizer.model_max_length, max_length)\n\n    return DataLoader(\n        concat_and_tokenize(dataset, tokenizer, max_length),  # type: ignore\n        batch_size=batch_size,\n        num_workers=num_workers,\n    )\n\n\ndef get_train_dataloader(\n    path: str, model_name: str, max_length: int, batch_size: int, num_workers: int = 1\n) -> DataLoader[torch.Tensor]:\n    return get_dataloader(\n        load_dataset(path, split=\"train\", streaming=True),  # type: ignore\n        model_name,\n        max_length,\n        batch_size,\n        num_workers,\n    )\n\n\ndef get_test_dataloader(\n    model_name: str, max_length: int, batch_size: int, num_workers: int = 1\n) -> DataLoader[torch.Tensor]:\n    return get_dataloader(\n        load_dataset(\n            \"json\",\n            data_files=\"./data/test.jsonl.zst\",\n            split=\"train\",\n            streaming=True,\n        ),  # type: ignore\n        model_name,\n        max_length,\n        batch_size,\n        num_workers,\n    )\n"}
{"type": "source_file", "path": "mlsae/model/__init__.py", "content": "from .autoencoders import SAE, SAEOut, TopKSAE, TopKSAEOut\nfrom .data import DataConfig, get_test_dataloader, get_train_dataloader\nfrom .lightning import MLSAEConfig, MLSAETransformer\nfrom .transformers import GPT2Transformer, PythiaTransformer\nfrom .types import Stats, TopK\n\n__all__ = [\n    \"DataConfig\",\n    \"get_test_dataloader\",\n    \"get_train_dataloader\",\n    \"SAE\",\n    \"SAEOut\",\n    \"TopKSAE\",\n    \"TopKSAEOut\",\n    \"MLSAEConfig\",\n    \"MLSAETransformer\",\n    \"GPT2Transformer\",\n    \"PythiaTransformer\",\n    \"Stats\",\n    \"TopK\",\n]\n"}
{"type": "source_file", "path": "mlsae/model/geom_median.py", "content": "# Based on https://github.com/EleutherAI/sae/blob/19d95a401e9d17dbf7d6fb0fa7a91081f1b0d01f/sae/utils.py\n\nimport einops\nimport torch\nfrom jaxtyping import Float\n\n\n@torch.no_grad()\ndef geometric_median(\n    points: Float[torch.Tensor, \"layer batch pos n_inputs\"],\n    max_iter: int = 100,\n    tol: float = 1e-5,\n) -> Float[torch.Tensor, \"n_inputs\"]:\n    \"\"\"\n    Compute the geometric median of the points along the last axis.\n\n    Used to initialize the pre-encoder bias.\n\n    Args:\n        points (Float[torch.Tensor, \"layer batch pos n_inputs\"]): The points from\n            which to compute the geometric median.\n\n        max_iter (int): The maximum number of iterations. Defaults to 100.\n\n        tol (float): The tolerance for early stopping. Defaults to 1e-5.\n\n    Returns:\n        out (Float[torch.Tensor, \"n_inputs\"]): The geometric median of the points along\n            the last axis.\n    \"\"\"\n\n    points = einops.rearrange(\n        points, \"layer batch pos n_inputs -> (layer batch pos) n_inputs\"\n    )\n    curr = points.mean(dim=0)\n    prev = torch.zeros_like(curr)\n    weights = torch.ones(len(points), device=points.device)\n    for _ in range(max_iter):\n        prev = curr\n        weights = 1 / torch.norm(points - curr, dim=1)\n        weights /= weights.sum()\n        curr = (weights.unsqueeze(1) * points).sum(dim=0)\n        if torch.norm(curr - prev) < tol:\n            break\n    return curr\n"}
{"type": "source_file", "path": "mlsae/model/autoencoders/topk.py", "content": "# Based on https://github.com/openai/sparse_autoencoder/blob/4965b941e9eb590b00b253a2c406db1e1b193942/sparse_autoencoder/train.py\n\nfrom typing import NamedTuple\n\nimport torch\nfrom huggingface_hub import PyTorchModelHubMixin\nfrom torch.nn import Linear, Module, Parameter\n\nfrom mlsae.model.decoder import decode\nfrom mlsae.model.types import Stats, TopK\nfrom mlsae.model_card import model_card_template\n\nfrom .utils import standardize, unit_norm_decoder\n\n\nclass TopKSAEOut(NamedTuple):\n    \"\"\"The output of the autoencoder forward pass.\"\"\"\n\n    topk: TopK\n    \"\"\"The k largest latents.\"\"\"\n\n    recons: torch.Tensor\n    \"\"\"The reconstructions from the k largest latents.\"\"\"\n\n    auxk: TopK | None\n    \"\"\"If auxk is not None, the auxk largest dead latents.\"\"\"\n\n    auxk_recons: torch.Tensor | None\n    \"\"\"If auxk is not None, the reconstructions from the auxk largest dead latents.\"\"\"\n\n    dead: torch.Tensor\n    \"\"\"The fraction of dead latents.\"\"\"\n\n\nclass TopKSAE(\n    Module,\n    PyTorchModelHubMixin,\n    model_card_template=model_card_template(False),\n    license=\"mit\",\n    language=\"en\",\n    library_name=\"mlsae\",\n    repo_url=\"https://github.com/tim-lawson/mlsae\",\n    tags=[\"arxiv:2409.04185\"],\n):\n    last_nonzero: torch.Tensor\n    \"\"\"The number of steps since the latents have activated.\"\"\"\n\n    def __init__(\n        self,\n        n_inputs: int,\n        n_latents: int,\n        k: int,\n        dead_steps_threshold: int,\n        dead_threshold: float = 1e-3,\n        # TODO: Make this optional and default to a power of 2 close to d_model / 2.\n        auxk: int | None = 256,\n        standardize: bool = True,\n    ) -> None:\n        \"\"\"\n        Args:\n            n_inputs (int): The number of inputs.\n\n            n_latents (int): The number of latents.\n\n            k (int): The number of largest latents to keep.\n\n            dead_steps_threshold (int): The number of steps after which a latent is\n                flagged as dead during training.\n\n            dead_threshold (float): The threshold for a latent to be considered\n                activated. Defaults to 1e-3.\n\n            auxk (int | None): The number of dead latents with which to model the\n                reconstruction error. Defaults to 256.\n\n            standardize (bool): Whether to standardize the inputs. Defaults to True.\n        \"\"\"\n\n        super().__init__()\n\n        self.n_inputs = n_inputs\n        self.n_latents = n_latents\n        self.k = k\n        self.auxk = auxk\n        self.dead_steps_threshold = dead_steps_threshold\n        self.dead_threshold = dead_threshold\n        self.standardize = standardize\n\n        self.encoder = Linear(n_inputs, n_latents, bias=False)\n        self.decoder = Linear(n_latents, n_inputs, bias=False)\n        self.pre_encoder_bias = Parameter(torch.zeros(n_inputs))\n\n        self.register_buffer(\"last_nonzero\", torch.zeros(n_latents, dtype=torch.long))\n\n        self.decoder.weight.data = self.encoder.weight.data.T.clone()\n        self.decoder.weight.data = self.decoder.weight.data.T.contiguous().T\n        unit_norm_decoder(self.decoder)\n\n    def encode(\n        self, inputs: torch.Tensor\n    ) -> tuple[TopK, TopK | None, Stats | None, torch.Tensor]:\n        stats = None\n        if self.standardize:\n            inputs, stats = standardize(inputs)\n\n        # Keep a reference to the latents before the TopK activation function\n        latents = self.encoder.forward(inputs - self.pre_encoder_bias)\n\n        # Find the k largest latents\n        topk = TopK(*torch.topk(latents, k=self.k, sorted=False))\n\n        # Update the number of steps since the latents have activated\n        last_nonzero = torch.zeros_like(self.last_nonzero, device=inputs.device)\n        last_nonzero.scatter_add_(\n            dim=0,\n            index=topk.indices.reshape(-1),\n            src=(topk.values > self.dead_threshold).to(last_nonzero.dtype).reshape(-1),\n        )\n        self.last_nonzero *= 1 - last_nonzero.clamp(max=1)\n        self.last_nonzero += 1\n\n        # Mask the latents flagged as dead during training\n        dead_mask = self.last_nonzero >= self.dead_steps_threshold\n        latents.data *= dead_mask  # in-place to save memory\n\n        # Compute the fraction of dead latents\n        dead = torch.sum(dead_mask, dtype=torch.float32).detach() / self.n_latents\n\n        # If auxk is not None, find the auxk largest dead latents\n        auxk = None\n        if self.auxk is not None:\n            auxk = TopK(*torch.topk(latents, k=self.auxk, sorted=False))\n\n        return topk, auxk, stats, dead\n\n    def decode(self, topk: TopK, stats: Stats | None = None) -> torch.Tensor:\n        recons = decode(topk, self.decoder.weight) + self.pre_encoder_bias\n        if stats is not None:\n            recons = recons * stats.std + stats.mean\n        return recons\n\n    def forward(self, inputs: torch.Tensor) -> TopKSAEOut:\n        topk, auxk, stats, dead = self.encode(inputs)\n\n        # Apply ReLU to ensure the k largest latents are non-negative\n        values = torch.relu(topk.values)\n        topk = TopK(values, topk.indices)\n        recons = self.decode(topk, stats)\n\n        auxk_recons = None\n        if auxk is not None:\n            auxk_values = torch.relu(auxk.values)\n            auxk = TopK(auxk_values, auxk.indices)\n            auxk_recons = self.decode(auxk)\n\n        return TopKSAEOut(topk, recons, auxk, auxk_recons, dead)\n"}
{"type": "source_file", "path": "mlsae/metrics/mse_loss.py", "content": "import torch\nfrom jaxtyping import Float\nfrom torchmetrics import Metric\n\n\nclass MSELoss(Metric):\n    \"\"\"\n    The average FVU of the main model `e = inputs - recons`, where `recons` is the\n    reconstruction using the top-k latents.\n\n    Equivalent to normalized MSE in Gao et al. [2024], except we compute the variance\n    per batch instead of once at the beginning of training.\n    \"\"\"\n\n    is_differentiable = True\n    full_state_update = False\n\n    layer_mse: Float[torch.Tensor, \"n_layers\"]\n    \"\"\"Layerwise sum of MSEs between the inputs and reconstructions.\"\"\"\n\n    layer_var: Float[torch.Tensor, \"n_layers\"]\n    \"\"\"Layerwise sum of variances of the inputs.\"\"\"\n\n    def __init__(self, n_layers: int) -> None:\n        super().__init__()\n        self.add_state(\n            \"layer_mse\", torch.zeros(n_layers, dtype=torch.float), dist_reduce_fx=\"sum\"\n        )\n        self.add_state(\n            \"layer_var\", torch.zeros(n_layers, dtype=torch.float), dist_reduce_fx=\"sum\"\n        )\n\n    def update(\n        self,\n        inputs: Float[torch.Tensor, \"n_layers batch pos n_inputs\"],\n        recons: Float[torch.Tensor, \"n_layers batch pos n_inputs\"],\n        **kwargs,\n    ) -> None:\n        self.layer_mse.add_(torch.mean((recons - inputs).pow(2), dim=(1, 2, 3)))\n        self.layer_var.add_(torch.var(inputs, dim=(1, 2, 3)))\n\n    def compute(self) -> Float[torch.Tensor, \"\"]:\n        return (self.layer_mse / self.layer_var).mean()\n"}
{"type": "source_file", "path": "mlsae/metrics/layerwise_mse.py", "content": "import torch\nfrom jaxtyping import Float\nfrom torchmetrics import Metric\n\n\nclass LayerwiseMSE(Metric):\n    \"\"\"Mean squared error (MSE) or L2 reconstruction loss.\"\"\"\n\n    is_differentiable = True\n    full_state_update = False\n\n    layer_mse: Float[torch.Tensor, \"n_layers\"]\n    \"\"\"Layerwise mean of MSEs between inputs and reconstructions.\"\"\"\n\n    def __init__(self, n_layers: int) -> None:\n        super().__init__()\n        self.add_state(\n            \"layer_mse\", torch.zeros(n_layers, dtype=torch.float), dist_reduce_fx=\"mean\"\n        )\n\n    @torch.no_grad()\n    def update(\n        self,\n        inputs: Float[torch.Tensor, \"n_layers batch pos n_inputs\"],\n        recons: Float[torch.Tensor, \"n_layers batch pos n_inputs\"],\n        **kwargs,\n    ) -> None:\n        self.layer_mse.add_(torch.mean((recons - inputs).pow(2), dim=(1, 2, 3)))\n\n    @torch.no_grad()\n    def compute(self) -> Float[torch.Tensor, \"n_layers\"]:\n        return self.layer_mse / self.update_count\n"}
{"type": "source_file", "path": "mlsae/model/autoencoders/standard.py", "content": "from typing import NamedTuple\n\nimport torch\nfrom huggingface_hub import PyTorchModelHubMixin\nfrom torch.nn import Linear, Module, Parameter\n\nfrom mlsae.model.types import Stats\n\nfrom .utils import standardize, unit_norm_decoder\n\n\nclass SAEOut(NamedTuple):\n    \"\"\"The output of the autoencoder forward pass.\"\"\"\n\n    latents: torch.Tensor\n    \"\"\"The latents.\"\"\"\n\n    recons: torch.Tensor\n    \"\"\"The reconstructions.\"\"\"\n\n    dead: torch.Tensor\n    \"\"\"The fraction of dead latents.\"\"\"\n\n\n# TODO: This is equivalent to TopK SAE with k = n_latents and auxk = None.\nclass SAE(Module, PyTorchModelHubMixin):\n    last_nonzero: torch.Tensor\n    \"\"\"The number of steps since the latents have activated.\"\"\"\n\n    def __init__(\n        self,\n        n_inputs: int,\n        n_latents: int,\n        dead_steps_threshold: int,\n        dead_threshold: float = 1e-3,\n        standardize: bool = True,\n    ) -> None:\n        \"\"\"\n        Args:\n            n_inputs (int): The number of inputs.\n\n            n_latents(int): The number of latents.\n\n            dead_steps_threshold (int): The number of steps after which a latent is\n                flagged as dead during training.\n\n            dead_threshold (float): The threshold for a latent to be considered\n                activated. Defaults to 1e-3.\n\n            standardize (bool): Whether to standardize the inputs. Defaults to True.\n        \"\"\"\n        super().__init__()\n\n        self.n_inputs = n_inputs\n        self.n_latents = n_latents\n        self.dead_steps_threshold = dead_steps_threshold\n        self.dead_threshold = dead_threshold\n        self.standardize = standardize\n\n        self.encoder = Linear(n_inputs, n_latents, bias=False)\n        self.decoder = Linear(n_latents, n_inputs, bias=False)\n        self.pre_encoder_bias = Parameter(torch.zeros(n_inputs))\n\n        self.register_buffer(\"last_nonzero\", torch.zeros(n_latents, dtype=torch.long))\n\n        self.decoder.weight.data = self.encoder.weight.data.T.clone()\n        self.decoder.weight.data = self.decoder.weight.data.T.contiguous().T\n        unit_norm_decoder(self.decoder)\n\n    def encode(\n        self, inputs: torch.Tensor\n    ) -> tuple[torch.Tensor, Stats | None, torch.Tensor]:\n        stats = None\n        if self.standardize:\n            inputs, stats = standardize(inputs)\n\n        latents = self.encoder.forward(inputs - self.pre_encoder_bias)\n\n        # Find the k largest latents (purely to maximize consistency with TopKSAE)\n        topk = torch.topk(latents, self.n_latents, sorted=False)\n\n        # Update the number of steps since the latents have activated\n        last_nonzero = torch.zeros_like(self.last_nonzero, device=inputs.device)\n        last_nonzero.scatter_add_(\n            dim=0,\n            index=topk.indices.reshape(-1),\n            src=(topk.values > self.dead_threshold).to(last_nonzero.dtype).reshape(-1),\n        )\n        self.last_nonzero *= 1 - last_nonzero.clamp(max=1)\n        self.last_nonzero += 1\n\n        # Mask the latents flagged as dead during training\n        dead_mask = self.last_nonzero >= self.dead_steps_threshold\n\n        # Compute the fraction of dead latents\n        dead = torch.sum(dead_mask, dtype=torch.float32).detach() / self.n_latents\n\n        return latents, stats, dead\n\n    def decode(self, latents: torch.Tensor, stats: Stats | None = None) -> torch.Tensor:\n        recons = (latents @ self.decoder.weight.T) + self.pre_encoder_bias\n        if stats is not None:\n            recons = recons * stats.std + stats.mean\n        return recons\n\n    def forward(self, inputs: torch.Tensor) -> SAEOut:\n        latents, stats, dead = self.encode(inputs)\n        latents = torch.relu(latents)\n        recons = self.decode(latents, stats)\n        return SAEOut(latents, recons, dead)\n"}
{"type": "source_file", "path": "mlsae/model/lightning.py", "content": "from dataclasses import dataclass\nfrom functools import partial\n\nimport einops\nimport torch\nimport wandb\nimport wandb.plot\nfrom huggingface_hub import PyTorchModelHubMixin\nfrom jaxtyping import Float, Int\nfrom lightning.pytorch import LightningModule\nfrom simple_parsing import Serializable\nfrom torchmetrics import MetricCollection\nfrom tuned_lens import TunedLens\n\nfrom mlsae.metrics import (\n    AuxiliaryLoss,\n    DeadLatents,\n    LayerwiseFVU,\n    LayerwiseL1Norm,\n    # LayerwiseLogitKLDiv,\n    # LayerwiseLogitMSE,\n    LayerwiseLossDelta,\n    LayerwiseMSE,\n    LayerwiseWrapper,\n    MSELoss,\n)\nfrom mlsae.model.autoencoders import (\n    TopKSAE,\n    TopKSAEOut,\n    unit_norm_decoder,\n    unit_norm_decoder_gradient,\n)\nfrom mlsae.model.geom_median import geometric_median\nfrom mlsae.model.transformers import GPT2Transformer, PythiaTransformer\nfrom mlsae.model.transformers.gemma2 import GemmaTransformer\nfrom mlsae.model.transformers.llama import LlamaTransformer\nfrom mlsae.model_card import model_card_template\n\n\n@dataclass\nclass MLSAEConfig(Serializable):\n    \"\"\"The autoencoder configuration.\"\"\"\n\n    dead_tokens_threshold: int = 10_000_000\n    \"\"\"The number of tokens after which a latent is flagged as dead during training.\"\"\"\n\n    expansion_factor: int = 64\n    \"\"\"The ratio of the number of latents to the number of inputs.\"\"\"\n\n    k: int = 32\n    \"\"\"The number of largest latents to keep.\"\"\"\n\n    # TODO: Make this optional and default to a power of 2 close to d_model / 2.\n    auxk: int | None = 256\n    \"\"\"The number of dead latents with which to model the reconstruction error.\"\"\"\n\n    auxk_coef: float | None = 1 / 32\n    \"\"\"The coefficient of the auxiliary loss.\"\"\"\n\n    dead_threshold: float = 1e-3\n    \"\"\"The threshold activation for a latent to be considered activated.\"\"\"\n\n    # TODO: Make this optional and default to the scaling law from Gao et al [2024].\n    lr: float = 1e-4\n    \"\"\"The learning rate.\"\"\"\n\n    standardize: bool = True\n    \"\"\"Whether to standardize the inputs.\"\"\"\n\n    skip_special_tokens: bool = True\n    \"\"\"Whether to ignore special tokens.\"\"\"\n\n    tuned_lens: bool = False\n    \"\"\"Whether to apply a pretrained tuned lens before the encoder.\"\"\"\n\n\ndef create_untransform_hidden(tuned_lens: TunedLens):\n    invs = []\n    lens: torch.nn.Linear\n    for lens in tuned_lens.layer_translators:  # type: ignore\n        invs.append(\n            torch.linalg.inv(\n                lens.weight + torch.eye(lens.weight.shape[0], device=lens.weight.device)\n            )\n        )\n\n    def untransform_hidden(h: torch.Tensor, idx: int) -> torch.Tensor:\n        lens: torch.nn.Linear = tuned_lens.layer_translators[idx]  # type: ignore\n        inv: torch.Tensor = invs[idx]\n        return einops.einsum(inv.to(h.device), h - lens.bias, \"n n, b p n -> b p n\")\n\n    return untransform_hidden\n\n\nclass MLSAETransformer(\n    LightningModule,\n    PyTorchModelHubMixin,\n    model_card_template=model_card_template(True),\n    license=\"mit\",\n    language=\"en\",\n    library_name=\"mlsae\",\n    repo_url=\"https://github.com/tim-lawson/mlsae\",\n    tags=[\"arxiv:2409.04185\"],\n):\n    loss_true: Float[torch.Tensor, \"n_layers\"]\n    loss_pred: Float[torch.Tensor, \"n_layers\"]\n    logits_true: Float[torch.Tensor, \"n_layers pos d_vocab\"]\n    logits_pred: Float[torch.Tensor, \"n_layers pos d_vocab\"]\n\n    def __init__(\n        self,\n        model_name: str = \"EleutherAI/pythia-70m-deduped\",\n        layers: list[int] | None = None,\n        expansion_factor: int = 16,\n        k: int = 32,\n        auxk: int | None = 256,\n        auxk_coef: float | None = 1 / 32,\n        dead_tokens_threshold: int = 10_000_000,\n        dead_threshold: float = 1e-3,\n        lr: float = 1e-4,\n        standardize: bool = True,\n        skip_special_tokens: bool = True,\n        max_length: int = 2048,\n        batch_size: int = 1,\n        accumulate_grad_batches: int = 64,\n        tuned_lens: bool = False,\n        # NOTE: These are only used for loading pretrained models\n        dead_steps_threshold: int | None = None,\n    ) -> None:\n        \"\"\"\n        Multi-Layer Sparse Autoencoder (MLSAE) PyTorch Lightning module.\n        Includes the underlying transformer.\n\n        References:\n\n        - [Gao et al., 2024. Scaling and evaluating sparse autoencoders.](https://cdn.openai.com/papers/sparse-autoencoders.pdf)\n        - [Bricken et al., 2023. Towards Monosemanticity.](https://transformer-circuits.pub/2023/monosemantic-features)\n\n        Args:\n            model_name (str): The name of a pretrained model.\n\n            layers (list[int] | None): The layers to train on.\n                If None, all layers are trained on. Defaults to None.\n\n            expansion_factor (int): The ratio of the number of latents to the number of\n                inputs. Defaults to 16.\n\n            k (int): The number of largest latents to keep. Defaults to 32.\n\n            auxk (int | None): The number of dead latents with which to model the\n                reconstruction error. Defaults to 256.\n\n            auxk_coef (float | None): The coefficient of the auxiliary loss.\n                Defaults to 1 / 32.\n\n            dead_tokens_threshold (int): The number of tokens after which a latent is\n                flagged as dead during training. Defaults to 10 million.\n\n            dead_threshold (float): The threshold for a latent to be considered\n                activated. Defaults to 1e-3.\n\n            lr (float): The learning rate. Defaults to 1e-4.\n\n            standardize (bool): Whether to standardize the inputs. Defaults to True.\n\n            skip_special_tokens (bool): Whether to ignore special tokens.\n                Defaults to True.\n\n            max_length (int): The maximum length of a tokenized input sequence.\n                Defaults to 2048.\n\n            batch_size (int): The number of sequences in a batch. Defaults to 1.\n\n            accumulate_grad_batches (int): The number of batches over which to\n                accumulate gradients. Defaults to 64.\n        \"\"\"\n\n        super().__init__()\n\n        self.model_name = model_name\n        self.expansion_factor = expansion_factor\n        self.k = k\n        self.auxk = auxk\n        self.auxk_coef = auxk_coef\n        self.dead_tokens_threshold = dead_tokens_threshold\n        self.dead_threshold = dead_threshold\n        self.lr = lr\n        self.standardize = standardize\n        self.skip_special_tokens = skip_special_tokens\n        self.max_length = max_length\n        self.batch_size = batch_size\n        self.accumulate_grad_batches = accumulate_grad_batches\n        self.tuned_lens = tuned_lens\n\n        # Set the number of steps after which a latent is flagged as dead from the\n        # number of tokens per batch and the number of batches per step.\n        self.dead_steps_threshold = (\n            dead_steps_threshold\n            or self.dead_tokens_threshold\n            // (self.batch_size * self.max_length * self.accumulate_grad_batches)\n        )\n\n        transformer_kwargs = {\n            \"model_name\": self.model_name,\n            \"max_length\": self.max_length,\n            \"batch_size\": self.batch_size,\n            \"skip_special_tokens\": self.skip_special_tokens,\n            \"layers\": layers,\n            \"device\": self.device,\n        }\n        # TODO: Improve this...\n        if \"pythia\" in model_name:\n            self.transformer = PythiaTransformer(**transformer_kwargs)\n        elif \"gpt2\" in model_name:\n            self.transformer = GPT2Transformer(**transformer_kwargs)\n        elif \"llama\" in model_name:\n            self.transformer = LlamaTransformer(**transformer_kwargs)\n        elif \"gemma\" in model_name:\n            self.transformer = GemmaTransformer(**transformer_kwargs)\n        else:\n            raise ValueError(f\"Unknown model name: {model_name}\")\n        self.transformer.eval()\n        self.transformer.requires_grad_(False)\n\n        self.layers = self.transformer.layers\n        self.n_layers = self.transformer.n_layers\n        self.n_inputs = self.transformer.config.hidden_size\n        self.n_latents = self.n_inputs * self.expansion_factor\n\n        self.save_hyperparameters(ignore=[\"autoencoder\", \"transformer\"])\n\n        self.autoencoder: TopKSAE = TopKSAE(\n            self.n_inputs,\n            self.n_latents,\n            self.k,\n            self.dead_steps_threshold,\n            self.dead_threshold,\n            self.auxk,\n            self.standardize,\n        )  # type: ignore\n\n        if self.tuned_lens:\n            self.lens = TunedLens.from_model_and_pretrained(\n                self.transformer.model,\n                self.transformer.model_name,\n                map_location=self.device,\n            )\n            self.lens.eval()\n            self.lens.requires_grad_(False)\n            self.untransform_hidden = create_untransform_hidden(self.lens)\n\n        self.mse_loss = MSELoss(self.n_layers)\n        self.aux_loss = AuxiliaryLoss(self.auxk_coef or 0.0)\n\n        wrap = partial(\n            LayerwiseWrapper,\n            labels=[f\"layer_{layer}\" for layer in self.transformer.layers],\n        )\n\n        self.train_metrics = MetricCollection(\n            {\n                \"dead/rel\": DeadLatents(self.n_latents, self.dead_tokens_threshold),\n                \"l1\": wrap(LayerwiseL1Norm(self.n_layers), prefix=\"l1/\"),\n                \"mse\": wrap(LayerwiseMSE(self.n_layers), prefix=\"mse/\"),\n                \"fvu\": wrap(LayerwiseFVU(self.n_layers), prefix=\"fvu/\"),\n            },\n            prefix=\"train/\",\n        )\n\n        self.val_metrics = MetricCollection(\n            {\n                \"loss/delta\": wrap(\n                    LayerwiseLossDelta(self.n_layers), prefix=\"loss/delta/\"\n                ),\n                # \"logit/mse\": wrap(\n                #     LayerwiseLogitMSE(self.n_layers), prefix=\"logit/mse/\"\n                # ),\n                # \"logit/kldiv\": wrap(\n                #     LayerwiseLogitKLDiv(self.n_layers), prefix=\"logit/kldiv/\"\n                # ),\n            },\n            prefix=\"val/\",\n        )\n\n        # logits = (\n        #     self.n_layers,\n        #     self.transformer.batch_size,\n        #     self.transformer.max_length,\n        #     self.transformer.config.vocab_size,\n        # )\n        self.register_buffer(\"loss_true\", torch.zeros(self.n_layers))\n        self.register_buffer(\"loss_pred\", torch.zeros(self.n_layers))\n        # self.register_buffer(\"logits_true\", torch.zeros(logits))\n        # self.register_buffer(\"logits_pred\", torch.zeros(logits))\n\n    def forward(self, tokens: Int[torch.Tensor, \"batch pos\"]) -> TopKSAEOut:\n        inputs = self.forward_lens(self.transformer.forward(tokens))\n        topk, recons, auxk, auxk_recons, dead = self.autoencoder.forward(inputs)\n        recons = self.inverse_lens(recons)\n        return TopKSAEOut(topk, recons, auxk, auxk_recons, dead)\n\n    def forward_lens(\n        self, inputs: Float[torch.Tensor, \"layer batch pos n_inputs\"]\n    ) -> Float[torch.Tensor, \"layer batch pos n_latents\"]:\n        if not self.tuned_lens:\n            return inputs\n        self.lens.to(inputs.device)\n        for layer in range(self.n_layers):\n            inputs[layer, ...] = self.lens.transform_hidden(inputs[layer, ...], layer)\n        return inputs\n\n    def inverse_lens(\n        self, recons: Float[torch.Tensor, \"layer batch pos n_latents\"]\n    ) -> Float[torch.Tensor, \"layer batch pos n_inputs\"]:\n        if not self.tuned_lens:\n            return recons\n        self.lens.to(recons.device)\n        for layer in range(self.n_layers):\n            recons[layer, ...] = self.untransform_hidden(recons[layer, ...], layer)\n        return recons\n\n    def training_step(\n        self, batch: dict[str, Int[torch.Tensor, \"batch pos\"]], batch_idx: int\n    ) -> Float[torch.Tensor, \"\"]:\n        inputs = self.forward_lens(self.transformer.forward(batch[\"input_ids\"]))\n\n        if batch_idx == 0:\n            self.autoencoder.pre_encoder_bias.data = geometric_median(inputs)\n\n        topk, recons, auxk, auxk_recons, dead = self.autoencoder.forward(inputs)\n\n        train_metrics = self.train_metrics.forward(\n            inputs=inputs,\n            indices=topk.indices,\n            values=topk.values,\n            recons=recons,\n        )\n\n        mse_loss = self.mse_loss.forward(inputs=inputs, recons=recons)\n        aux_loss = self.aux_loss.forward(\n            inputs=inputs, recons=recons, auxk_recons=auxk_recons\n        )\n        loss = mse_loss + aux_loss\n\n        self.log_dict(\n            {\n                **train_metrics,\n                \"loss/total\": loss,\n                \"loss/mse\": mse_loss,\n                \"loss/auxk\": aux_loss,\n                \"train/dead/abs\": dead,\n            }\n        )\n\n        return loss\n\n    def forward_at_layer(\n        self,\n        inputs: Float[torch.Tensor, \"n_layers batch pos n_inputs\"],\n        recons: Float[torch.Tensor, \"n_layers batch pos n_inputs\"],\n        tokens: Int[torch.Tensor, \"batch pos\"],\n    ) -> None:\n        for layer in range(self.n_layers):\n            loss = self.transformer.forward_at_layer(\n                inputs, layer, return_type=\"loss\", tokens=tokens\n            )\n            self.loss_true[layer] = loss\n            # self.logits_true[layer] = logits\n\n            loss = self.transformer.forward_at_layer(\n                recons, layer, return_type=\"loss\", tokens=tokens\n            )\n            self.loss_pred[layer] = loss\n            # self.logits_pred[layer] = logits\n\n    @torch.no_grad()\n    def validation_step(self, batch: dict[str, Int[torch.Tensor, \"batch pos\"]]) -> None:\n        tokens = batch[\"input_ids\"]\n        inputs = self.forward_lens(self.transformer.forward(tokens))\n        topk, recons, auxk, auxk_recons, dead = self.autoencoder.forward(inputs)\n        recons = self.inverse_lens(recons)\n\n        self.forward_at_layer(inputs, recons, tokens)\n        val_metrics = self.val_metrics.forward(\n            loss_true=self.loss_true,\n            loss_pred=self.loss_pred,\n            # logits_true=self.logits_true,\n            # logits_pred=self.logits_pred,\n        )\n\n        self.log_dict(val_metrics)\n\n    @torch.no_grad()\n    def test_step(self, batch: dict[str, Int[torch.Tensor, \"batch pos\"]]) -> None:\n        tokens = batch[\"input_ids\"]\n        inputs = self.forward_lens(self.transformer.forward(tokens))\n        topk, recons, auxk, auxk_recons, dead = self.autoencoder.forward(inputs)\n\n        # NOTE: We compute the reconstruction error *before* the inverse lens\n        train_metrics = self.train_metrics.forward(\n            inputs=inputs,\n            indices=topk.indices,\n            values=topk.values,\n            recons=recons,\n        )\n\n        recons = self.inverse_lens(recons)\n\n        self.forward_at_layer(inputs, recons, tokens)\n        val_metrics = self.val_metrics.forward(\n            loss_true=self.loss_true,\n            loss_pred=self.loss_pred,\n            # logits_true=self.logits_true,\n            # logits_pred=self.logits_pred,\n        )\n\n        mse_loss = self.mse_loss.forward(inputs=inputs, recons=recons)\n        aux_loss = self.aux_loss.forward(\n            inputs=inputs, recons=recons, auxk_recons=auxk_recons\n        )\n        loss = mse_loss + aux_loss\n\n        self.log_dict(\n            {\n                **train_metrics,\n                **val_metrics,\n                \"loss/total\": loss,\n                \"loss/mse\": mse_loss,\n                \"loss/auxk\": aux_loss,\n            }\n        )\n\n    def on_after_backward(self) -> None:\n        unit_norm_decoder(self.autoencoder.decoder)\n        unit_norm_decoder_gradient(self.autoencoder.decoder)\n\n    def on_train_end(self) -> None:\n        del self.loss_true\n        del self.loss_pred\n        # del self.logits_true\n        # del self.logits_pred\n        del self.autoencoder.last_nonzero\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(  # type: ignore\n            self.autoencoder.parameters(), lr=self.lr, eps=6.25e-10\n        )\n\n    def _log_latent_histograms(\n        self, values: Float[torch.Tensor, \"layer batch pos k\"]\n    ) -> None:\n        for layer in range(self.n_layers):\n            title = f\"latent/layer_{layer}\"\n            table = wandb.Table(\n                # Convert 3-d tensor (batch, pos, n_latents) to 2-d array [[x], ...]\n                data=values[layer].detach().cpu().numpy().reshape(-1, 1),\n                columns=[\"latent\"],\n            )\n            wandb.log({title: wandb.plot.histogram(table, \"latent\", title=title)})\n"}
{"type": "source_file", "path": "mlsae/model/transformers/gemma2.py", "content": "# TODO: Share code between transformers.\n\nfrom typing import Literal, overload\n\nimport torch\nfrom jaxtyping import Bool, Float, Int\nfrom torch import Tensor\nfrom torch.nn import CrossEntropyLoss, Module\nfrom transformers import (\n    AutoTokenizer,\n    PreTrainedTokenizer,\n    PreTrainedTokenizerFast,\n)\nfrom transformers.modeling_attn_mask_utils import (\n    _prepare_4d_causal_attention_mask_for_sdpa,\n)\nfrom transformers.models.gemma2.configuration_gemma2 import Gemma2Config\n\nfrom .models.gemma2.modeling_gemma2 import (\n    Gemma2DecoderLayer,\n    Gemma2ForCausalLM,\n)\n\n\nclass GemmaTransformer(Module):\n    def __init__(\n        self,\n        model_name: str,\n        max_length: int,\n        batch_size: int,\n        skip_special_tokens: bool = True,\n        layers: list[int] | None = None,\n        device: torch.device | None = None,\n    ) -> None:\n        \"\"\"\n        Args:\n            model_name (str): The name of a pretrained GemmaForCausalLM model.\n\n            max_length (int): The maximum length of a tokenized input sequence.\n\n            batch_size (int): The number of sequences in a batch.\n\n            skip_special_tokens (bool): Whether to ignore special tokens.\n                Defaults to True.\n\n            layers (list[int] | None): The layers to train on.\n                If None, all layers are trained on.\n                Defaults to None.\n\n            device (torch.device | str): The device to use.\n                Defaults to \"cpu\".\n        \"\"\"\n\n        super().__init__()\n\n        device = device or torch.device(\"cpu\")\n\n        self.model_name = model_name\n        self.model: Gemma2ForCausalLM = Gemma2ForCausalLM.from_pretrained(model_name)  # type: ignore\n        self.model.eval()\n\n        self.batch_size = batch_size\n        self.max_length = max_length\n\n        self.config: Gemma2Config = self.model.config  # type: ignore\n        self.tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast = (\n            AutoTokenizer.from_pretrained(model_name)\n        )\n\n        if layers is not None:\n            assert all(0 <= i < self.config.num_hidden_layers for i in layers)\n            self.layers = layers\n        else:\n            self.layers = list(range(self.config.num_hidden_layers))\n\n        self.n_layers = len(self.layers)\n        self.skip_special_tokens = skip_special_tokens\n\n        self.loss = CrossEntropyLoss()\n\n    @torch.no_grad()\n    def forward(\n        self, tokens: Int[Tensor, \"batch pos\"]\n    ) -> Float[Tensor, \"layer batch pos d_model\"]:\n        \"\"\"\n        Returns the residual stream activation vectors from the specified layers.\n\n        Args:\n            tokens (Int[Tensor, \"batch pos\"]): The input tokens.\n\n        Returns:\n            out (Float[Tensor, \"layer batch pos d_model\"]): The residual stream\n                activation vectors from the specified layers.\n        \"\"\"\n\n        hidden_states = self.hidden_states(tokens)\n        out = torch.stack([hidden_states[i] for i in self.layers])\n        if self.skip_special_tokens:\n            out[:, ~self._mask_special_tokens(tokens), :] = 0.0\n        return out\n\n    @torch.no_grad()\n    def hidden_states(\n        self, tokens: Int[Tensor, \"batch pos\"]\n    ) -> list[Float[Tensor, \"batch pos d_model\"]]:\n        \"\"\"\n        Return the hidden states.\n\n        Similar to the `run_with_hooks` method of `HookedTransformer` in the\n        TransformerLens library.\n\n        Args:\n            tokens (Int[Tensor, \"batch pos\"]): The input tokens.\n\n        Returns:\n            out (list[Float[Tensor, \"batch pos d_model\"]]): The hidden states.\n        \"\"\"\n\n        output = self.model.model.forward(\n            input_ids=tokens,  # type: ignore\n            output_hidden_states=True,\n            past_key_values=None,\n            use_cache=False,\n            skip_final_layer_norm=True,\n        )\n        hidden_states: tuple[torch.Tensor, ...] = output.hidden_states  # type: ignore\n\n        # We don't include the input embeddings in hidden_states.\n        return list(hidden_states[1:])\n\n    @overload\n    def forward_at_layer(\n        self,\n        inputs_embeds: Float[Tensor, \"layer batch pos d_model\"],\n        start_at_layer: int,\n        return_type: Literal[\"loss\"],\n        tokens: Int[Tensor, \"batch pos\"] | None = None,\n    ) -> Float[Tensor, \"\"]: ...\n\n    @overload\n    def forward_at_layer(\n        self,\n        inputs_embeds: Float[Tensor, \"layer batch pos d_model\"],\n        start_at_layer: int,\n        return_type: Literal[\"logits\"],\n        tokens: Int[Tensor, \"batch pos\"] | None = None,\n    ) -> Float[Tensor, \"batch pos d_vocab\"]: ...\n\n    @overload\n    def forward_at_layer(\n        self,\n        inputs_embeds: Float[Tensor, \"layer batch pos d_model\"],\n        start_at_layer: int,\n        return_type: Literal[\"both\"],\n        tokens: Int[Tensor, \"batch pos\"] | None = None,\n    ) -> tuple[Float[Tensor, \"\"], Float[Tensor, \"batch pos d_vocab\"]]: ...\n\n    @torch.no_grad()\n    def forward_at_layer(\n        self,\n        inputs_embeds: Float[Tensor, \"layer batch pos d_model\"],\n        start_at_layer: int,\n        return_type: Literal[\"loss\", \"logits\", \"both\"] = \"both\",\n        tokens: Int[Tensor, \"batch pos\"] | None = None,\n    ) -> (\n        Float[Tensor, \"\"]\n        | Float[Tensor, \"batch pos d_vocab\"]\n        | tuple[Float[Tensor, \"\"], Float[Tensor, \"batch pos d_vocab\"]]\n    ):\n        \"\"\"\n        Return the cross-entropy loss and/or logits, starting from the specified layer.\n\n        The input tokens are needed to compute the loss.\n\n        Also similar to the TransformerLens API.\n\n        Args:\n            inputs (Float[torch.Tensor, \"layer batch pos d_model\"]): The residual\n                stream activations at the specified layer.\n\n            start_at_layer (int): The layer at which to start the forward pass.\n\n            return_type (Literal[\"loss\", \"logits\", \"both\"]): Whether to return the\n                cross-entropy loss and/or logits.\n\n            tokens (Int[torch.Tensor, \"batch pos\"] | None): If the return_type is\n                \"loss\" or \"both\", the input tokens, otherwise None.\n\n        Returns:\n            The cross-entropy loss and/or logits.\n        \"\"\"\n\n        if return_type in [\"loss\", \"both\"] and tokens is None:\n            raise ValueError(\"The input tokens are needed to compute the loss.\")\n\n        input_shape = inputs_embeds.size()[:-1]\n        batch_size = inputs_embeds.shape[0]\n\n        position_ids = torch.arange(\n            0, input_shape[-1], dtype=torch.long, device=inputs_embeds.device\n        ).unsqueeze(0)\n\n        attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n            attention_mask=None,\n            input_shape=(batch_size, input_shape[-1]),\n            inputs_embeds=inputs_embeds,\n            past_key_values_length=0,\n        )\n\n        # Get the hidden states at the specified layer\n        hidden_states = inputs_embeds[start_at_layer, ...]\n\n        layer: Gemma2DecoderLayer\n        for i, layer in enumerate(self.model.model.layers):  # type: ignore\n            # Skip layers before the specified layer\n            if start_at_layer >= i:\n                continue\n\n            outputs = layer.forward(\n                hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,  # type: ignore\n            )\n            hidden_states = outputs[0]  # type: ignore\n\n        hidden_states = self.model.model.norm.forward(hidden_states)\n        logits: torch.Tensor = self.model.lm_head.forward(hidden_states)\n\n        if return_type == \"logits\":\n            return logits\n\n        # Shift to evaluate next-token predictions\n        shifted = logits[..., :-1, :].contiguous()\n\n        labels = tokens.to(logits.device)[..., 1:].contiguous()  # type: ignore\n\n        loss = self.loss(shifted.view(-1, shifted.size(-1)), labels.view(-1))\n\n        if return_type == \"loss\":\n            return loss\n\n        return loss, logits\n\n    # TODO: Implement this properly\n    @torch.no_grad()\n    def _mask_special_tokens(\n        self, tokens: Int[Tensor, \"batch pos\"]\n    ) -> Bool[Tensor, \"batch pos\"]:\n        \"\"\"Mask out special tokens (zero the activations).\"\"\"\n\n        mask = torch.ones_like(tokens, dtype=torch.bool, device=tokens.device)\n\n        if not self.skip_special_tokens or self.tokenizer is None:\n            return mask\n\n        if self.tokenizer.eos_token_id is not None:\n            mask = mask & torch.ne(tokens, self.tokenizer.eos_token_id)\n        if self.tokenizer.pad_token_id is not None:\n            mask = mask & torch.ne(tokens, self.tokenizer.pad_token_id)\n        if self.tokenizer.bos_token_id is not None:\n            mask = mask & torch.ne(tokens, self.tokenizer.bos_token_id)\n\n        return mask\n"}
{"type": "source_file", "path": "mlsae/model/transformers/__init__.py", "content": "from .gpt2 import GPT2Transformer\nfrom .pythia import PythiaTransformer\n\n__all__ = [\n    \"GPT2Transformer\",\n    \"PythiaTransformer\",\n]\n"}
{"type": "source_file", "path": "mlsae/model/kernels.py", "content": "# Copied from https://github.com/openai/sparse_autoencoder/blob/4965b941e9eb590b00b253a2c406db1e1b193942/sparse_autoencoder/kernels.py\n\n# fmt: off\n# ruff: noqa\n# type: ignore\n\nimport torch\n\nimport triton\nimport triton.language as tl\n\n\n## kernels\n\n\ndef triton_sparse_transpose_dense_matmul(\n    sparse_indices: torch.Tensor,\n    sparse_values: torch.Tensor,\n    dense: torch.Tensor,\n    N: int,\n    BLOCK_SIZE_AK=128,\n) -> torch.Tensor:\n    \"\"\"\n    calculates sparse.T @ dense (i.e reducing along the collated dimension of sparse)\n    dense must be contiguous along dim 0 (in other words, dense.T is contiguous)\n\n    sparse_indices is shape (A, k)\n    sparse_values is shape (A, k)\n    dense is shape (A, B)\n\n    output is shape (N, B)\n    \"\"\"\n\n    assert sparse_indices.shape == sparse_values.shape\n    assert sparse_indices.is_contiguous()\n    assert sparse_values.is_contiguous()\n    assert dense.is_contiguous()  # contiguous along B\n\n    K = sparse_indices.shape[1]\n    A = dense.shape[0]\n    B = dense.shape[1]\n    assert sparse_indices.shape[0] == A\n\n    # COO-format and sorted\n    sorted_indices = sparse_indices.view(-1).sort()\n    coo_indices = torch.stack(\n        [\n            torch.arange(A, device=sparse_indices.device).repeat_interleave(K)[\n                sorted_indices.indices\n            ],\n            sorted_indices.values,\n        ]\n    )  # shape (2, A * K)\n    coo_values = sparse_values.view(-1)[sorted_indices.indices]  # shape (A * K,)\n    return triton_coo_sparse_dense_matmul(coo_indices, coo_values, dense, N, BLOCK_SIZE_AK)\n\n\ndef triton_coo_sparse_dense_matmul(\n    coo_indices: torch.Tensor,\n    coo_values: torch.Tensor,\n    dense: torch.Tensor,\n    N: int,\n    BLOCK_SIZE_AK=128,\n) -> torch.Tensor:\n    AK = coo_indices.shape[1]\n    B = dense.shape[1]\n\n    out = torch.zeros(N, B, device=dense.device, dtype=coo_values.dtype)\n\n    grid = lambda META: (\n        triton.cdiv(AK, META[\"BLOCK_SIZE_AK\"]),\n        1,\n    )\n    triton_sparse_transpose_dense_matmul_kernel[grid](\n        coo_indices,\n        coo_values,\n        dense,\n        out,\n        stride_da=dense.stride(0),\n        stride_db=dense.stride(1),\n        B=B,\n        N=N,\n        AK=AK,\n        BLOCK_SIZE_AK=BLOCK_SIZE_AK,\n        BLOCK_SIZE_B=triton.next_power_of_2(B),\n    )\n    return out\n\n\n@triton.jit\ndef triton_sparse_transpose_dense_matmul_kernel(\n    coo_indices_ptr,\n    coo_values_ptr,\n    dense_ptr,\n    out_ptr,\n    stride_da,\n    stride_db,\n    B,\n    N,\n    AK,\n    BLOCK_SIZE_AK: tl.constexpr,\n    BLOCK_SIZE_B: tl.constexpr,\n):\n    \"\"\"\n    coo_indices is shape (2, AK)\n    coo_values is shape (AK,)\n    dense is shape (A, B), contiguous along B\n    out is shape (N, B)\n    \"\"\"\n\n    pid_ak = tl.program_id(0)\n    pid_b = tl.program_id(1)\n\n    coo_offsets = tl.arange(0, BLOCK_SIZE_AK)\n    b_offsets = tl.arange(0, BLOCK_SIZE_B)\n\n    A_coords = tl.load(\n        coo_indices_ptr + pid_ak * BLOCK_SIZE_AK + coo_offsets,\n        mask=pid_ak * BLOCK_SIZE_AK + coo_offsets < AK,\n    )\n    K_coords = tl.load(\n        coo_indices_ptr + pid_ak * BLOCK_SIZE_AK + coo_offsets + AK,\n        mask=pid_ak * BLOCK_SIZE_AK + coo_offsets < AK,\n    )\n    values = tl.load(\n        coo_values_ptr + pid_ak * BLOCK_SIZE_AK + coo_offsets,\n        mask=pid_ak * BLOCK_SIZE_AK + coo_offsets < AK,\n    )\n\n    last_k = tl.min(K_coords)\n    accum = tl.zeros((BLOCK_SIZE_B,), dtype=tl.float32)\n\n    for ind in range(BLOCK_SIZE_AK):\n        if ind + pid_ak * BLOCK_SIZE_AK < AK:\n            # workaround to do A_coords[ind]\n            a = tl.sum(\n                tl.where(\n                    tl.arange(0, BLOCK_SIZE_AK) == ind,\n                    A_coords,\n                    tl.zeros((BLOCK_SIZE_AK,), dtype=tl.int64),\n                )\n            )\n\n            k = tl.sum(\n                tl.where(\n                    tl.arange(0, BLOCK_SIZE_AK) == ind,\n                    K_coords,\n                    tl.zeros((BLOCK_SIZE_AK,), dtype=tl.int64),\n                )\n            )\n\n            v = tl.sum(\n                tl.where(\n                    tl.arange(0, BLOCK_SIZE_AK) == ind,\n                    values,\n                    tl.zeros((BLOCK_SIZE_AK,), dtype=tl.float32),\n                )\n            )\n\n            tl.device_assert(k < N)\n\n            if k != last_k:\n                tl.atomic_add(\n                    out_ptr + last_k * B + BLOCK_SIZE_B * pid_b + b_offsets,\n                    accum,\n                    mask=BLOCK_SIZE_B * pid_b + b_offsets < B,\n                )\n                accum *= 0\n                last_k = k\n\n            if v != 0:\n                accum += v * tl.load(dense_ptr + a * stride_da + b_offsets, mask=b_offsets < B)\n\n    tl.atomic_add(\n        out_ptr + last_k * B + BLOCK_SIZE_B * pid_b + b_offsets,\n        accum,\n        mask=BLOCK_SIZE_B * pid_b + b_offsets < B,\n    )\n\n\ndef triton_sparse_dense_matmul(\n    sparse_indices: torch.Tensor,\n    sparse_values: torch.Tensor,\n    dense: torch.Tensor,\n) -> torch.Tensor:\n    \"\"\"\n    calculates sparse @ dense (i.e reducing along the uncollated dimension of sparse)\n    dense must be contiguous along dim 0 (in other words, dense.T is contiguous)\n\n    sparse_indices is shape (A, k)\n    sparse_values is shape (A, k)\n    dense is shape (N, B)\n\n    output is shape (A, B)\n    \"\"\"\n    N = dense.shape[0]\n    assert sparse_indices.shape == sparse_values.shape\n    assert sparse_indices.is_contiguous()\n    assert sparse_values.is_contiguous()\n    assert dense.is_contiguous()  # contiguous along B\n\n    A = sparse_indices.shape[0]\n    K = sparse_indices.shape[1]\n    B = dense.shape[1]\n\n    out = torch.zeros(A, B, device=dense.device, dtype=sparse_values.dtype)\n\n    triton_sparse_dense_matmul_kernel[(A,)](\n        sparse_indices,\n        sparse_values,\n        dense,\n        out,\n        stride_dn=dense.stride(0),\n        stride_db=dense.stride(1),\n        A=A,\n        B=B,\n        N=N,\n        K=K,\n        BLOCK_SIZE_K=triton.next_power_of_2(K),\n        BLOCK_SIZE_B=triton.next_power_of_2(B),\n    )\n    return out\n\n\n@triton.jit\ndef triton_sparse_dense_matmul_kernel(\n    sparse_indices_ptr,\n    sparse_values_ptr,\n    dense_ptr,\n    out_ptr,\n    stride_dn,\n    stride_db,\n    A,\n    B,\n    N,\n    K,\n    BLOCK_SIZE_K: tl.constexpr,\n    BLOCK_SIZE_B: tl.constexpr,\n):\n    \"\"\"\n    sparse_indices is shape (A, K)\n    sparse_values is shape (A, K)\n    dense is shape (N, B), contiguous along B\n    out is shape (A, B)\n    \"\"\"\n\n    pid = tl.program_id(0)\n\n    offsets_k = tl.arange(0, BLOCK_SIZE_K)\n    sparse_indices = tl.load(\n        sparse_indices_ptr + pid * K + offsets_k, mask=offsets_k < K\n    )  # shape (K,)\n    sparse_values = tl.load(\n        sparse_values_ptr + pid * K + offsets_k, mask=offsets_k < K\n    )  # shape (K,)\n\n    accum = tl.zeros((BLOCK_SIZE_B,), dtype=tl.float32)\n\n    offsets_b = tl.arange(0, BLOCK_SIZE_B)\n\n    for k in range(K):\n        # workaround to do sparse_indices[k]\n        i = tl.sum(\n            tl.where(\n                tl.arange(0, BLOCK_SIZE_K) == k,\n                sparse_indices,\n                tl.zeros((BLOCK_SIZE_K,), dtype=tl.int64),\n            )\n        )\n        # workaround to do sparse_values[k]\n        v = tl.sum(\n            tl.where(\n                tl.arange(0, BLOCK_SIZE_K) == k,\n                sparse_values,\n                tl.zeros((BLOCK_SIZE_K,), dtype=tl.float32),\n            )\n        )\n\n        tl.device_assert(i < N)\n        if v != 0:\n            accum += v * tl.load(\n                dense_ptr + i * stride_dn + offsets_b * stride_db, mask=offsets_b < B\n            )\n\n    tl.store(out_ptr + pid * B + offsets_b, accum.to(sparse_values.dtype), mask=offsets_b < B)\n\n\ndef triton_dense_dense_sparseout_matmul(\n    dense1: torch.Tensor,\n    dense2: torch.Tensor,\n    at_indices: torch.Tensor,\n) -> torch.Tensor:\n    \"\"\"\n    dense1: shape (A, B)\n    dense2: shape (B, N)\n    at_indices: shape (A, K)\n    out values: shape (A, K)\n    calculates dense1 @ dense2 only for the indices in at_indices\n\n    equivalent to (dense1 @ dense2).gather(1, at_indices)\n    \"\"\"\n    A, B = dense1.shape\n    N = dense2.shape[1]\n    assert dense2.shape[0] == B\n    assert at_indices.shape[0] == A\n    K = at_indices.shape[1]\n    assert at_indices.is_contiguous()\n\n    assert dense1.stride(1) == 1, \"dense1 must be contiguous along B\"\n    assert dense2.stride(0) == 1, \"dense2 must be contiguous along B\"\n\n    if K > 512:\n        # print(\"WARN - using naive matmul for large K\")\n        # naive is more efficient for large K\n        return (dense1 @ dense2).gather(1, at_indices)\n\n    out = torch.zeros(A, K, device=dense1.device, dtype=dense1.dtype)\n\n    # grid = lambda META: (triton.cdiv(A, META['BLOCK_SIZE_A']),)\n\n    triton_dense_dense_sparseout_matmul_kernel[(A,)](\n        dense1,\n        dense2,\n        at_indices,\n        out,\n        stride_d1a=dense1.stride(0),\n        stride_d1b=dense1.stride(1),\n        stride_d2b=dense2.stride(0),\n        stride_d2n=dense2.stride(1),\n        A=A,\n        B=B,\n        N=N,\n        K=K,\n        BLOCK_SIZE_B=triton.next_power_of_2(B),\n        BLOCK_SIZE_N=triton.next_power_of_2(N),\n        BLOCK_SIZE_K=triton.next_power_of_2(K),\n    )\n\n    return out\n\n\n@triton.jit\ndef triton_dense_dense_sparseout_matmul_kernel(\n    dense1_ptr,\n    dense2_ptr,\n    at_indices_ptr,\n    out_ptr,\n    stride_d1a,\n    stride_d1b,\n    stride_d2b,\n    stride_d2n,\n    A,\n    B,\n    N,\n    K,\n    BLOCK_SIZE_B: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    \"\"\"\n    dense1: shape (A, B)\n    dense2: shape (B, N)\n    at_indices: shape (A, K)\n    out values: shape (A, K)\n    \"\"\"\n\n    pid = tl.program_id(0)\n\n    offsets_k = tl.arange(0, BLOCK_SIZE_K)\n    at_indices = tl.load(at_indices_ptr + pid * K + offsets_k, mask=offsets_k < K)  # shape (K,)\n\n    offsets_b = tl.arange(0, BLOCK_SIZE_B)\n    dense1 = tl.load(\n        dense1_ptr + pid * stride_d1a + offsets_b * stride_d1b, mask=offsets_b < B\n    )  # shape (B,)\n\n    accum = tl.zeros((BLOCK_SIZE_K,), dtype=tl.float32)\n\n    for k in range(K):\n        # workaround to do at_indices[b]\n        i = tl.sum(\n            tl.where(\n                tl.arange(0, BLOCK_SIZE_K) == k,\n                at_indices,\n                tl.zeros((BLOCK_SIZE_K,), dtype=tl.int64),\n            )\n        )\n        tl.device_assert(i < N)\n\n        dense2col = tl.load(\n            dense2_ptr + offsets_b * stride_d2b + i * stride_d2n, mask=offsets_b < B\n        )  # shape (B,)\n        accum += tl.where(\n            tl.arange(0, BLOCK_SIZE_K) == k,\n            tl.sum(dense1 * dense2col),\n            tl.zeros((BLOCK_SIZE_K,), dtype=tl.int64),\n        )\n\n    tl.store(out_ptr + pid * K + offsets_k, accum, mask=offsets_k < K)\n\n\nclass TritonDecoderAutograd(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, sparse_indices, sparse_values, decoder_weight):\n        ctx.save_for_backward(sparse_indices, sparse_values, decoder_weight)\n        return triton_sparse_dense_matmul(sparse_indices, sparse_values, decoder_weight.T)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        sparse_indices, sparse_values, decoder_weight = ctx.saved_tensors\n\n        assert grad_output.is_contiguous(), \"grad_output must be contiguous; this is probably because the subsequent op was a .sum() or something like that, which returns a non contiguous gradient\"\n\n        decoder_grad = triton_sparse_transpose_dense_matmul(\n            sparse_indices, sparse_values, grad_output, N=decoder_weight.shape[1]\n        ).T\n\n        return (\n            None,\n            triton_dense_dense_sparseout_matmul(grad_output, decoder_weight, sparse_indices),\n            # decoder is contiguous when transposed so this is a matching layout\n            decoder_grad,\n            None,\n        )\n\n\ndef triton_add_mul_(\n    x: torch.Tensor,\n    a: torch.Tensor,\n    b: torch.Tensor,\n    c: float,\n):\n    \"\"\"\n    does\n    x += a * b * c\n\n    x : [m, n]\n    a : [m, n]\n    b : [m, n]\n    c : float\n    \"\"\"\n\n    if len(a.shape) == 1:\n        a = a[None, :].broadcast_to(x.shape)\n\n    if len(b.shape) == 1:\n        b = b[None, :].broadcast_to(x.shape)\n\n    assert x.shape == a.shape == b.shape\n\n    BLOCK_SIZE_M = 64\n    BLOCK_SIZE_N = 64\n    grid = lambda META: (\n        triton.cdiv(x.shape[0], META[\"BLOCK_SIZE_M\"]),\n        triton.cdiv(x.shape[1], META[\"BLOCK_SIZE_N\"]),\n    )\n    triton_add_mul_kernel[grid](\n        x,\n        a,\n        b,\n        c,\n        x.stride(0),\n        x.stride(1),\n        a.stride(0),\n        a.stride(1),\n        b.stride(0),\n        b.stride(1),\n        BLOCK_SIZE_M,\n        BLOCK_SIZE_N,\n        x.shape[0],\n        x.shape[1],\n    )\n\n\n@triton.jit\ndef triton_add_mul_kernel(\n    x_ptr,\n    a_ptr,\n    b_ptr,\n    c,\n    stride_x0,\n    stride_x1,\n    stride_a0,\n    stride_a1,\n    stride_b0,\n    stride_b1,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    M: tl.constexpr,\n    N: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    offsets_m = tl.arange(0, BLOCK_SIZE_M) + pid_m * BLOCK_SIZE_M\n    offsets_n = tl.arange(0, BLOCK_SIZE_N) + pid_n * BLOCK_SIZE_N\n\n    x = tl.load(\n        x_ptr + offsets_m[:, None] * stride_x0 + offsets_n[None, :] * stride_x1,\n        mask=(offsets_m[:, None] < M) & (offsets_n[None, :] < N),\n    )\n    a = tl.load(\n        a_ptr + offsets_m[:, None] * stride_a0 + offsets_n[None, :] * stride_a1,\n        mask=(offsets_m[:, None] < M) & (offsets_n[None, :] < N),\n    )\n    b = tl.load(\n        b_ptr + offsets_m[:, None] * stride_b0 + offsets_n[None, :] * stride_b1,\n        mask=(offsets_m[:, None] < M) & (offsets_n[None, :] < N),\n    )\n\n    x_dtype = x.dtype\n    x = (x.to(tl.float32) + a.to(tl.float32) * b.to(tl.float32) * c).to(x_dtype)\n\n    tl.store(\n        x_ptr + offsets_m[:, None] * stride_x0 + offsets_n[None, :] * stride_x1,\n        x,\n        mask=(offsets_m[:, None] < M) & (offsets_n[None, :] < N),\n    )\n\n\n\ndef triton_sum_dim0_in_fp32(xs):\n    a, b = xs.shape\n\n    assert xs.is_contiguous()\n    assert xs.dtype == torch.float16\n\n    BLOCK_SIZE_A = min(triton.next_power_of_2(a), 512)\n    BLOCK_SIZE_B = 64  # cache line is 128 bytes\n\n    out = torch.zeros(b, dtype=torch.float32, device=xs.device)\n\n    grid = lambda META: (triton.cdiv(b, META[\"BLOCK_SIZE_B\"]),)\n\n    triton_sum_dim0_in_fp32_kernel[grid](\n        xs,\n        out,\n        stride_a=xs.stride(0),\n        a=a,\n        b=b,\n        BLOCK_SIZE_A=BLOCK_SIZE_A,\n        BLOCK_SIZE_B=BLOCK_SIZE_B,\n    )\n\n    return out\n\n\n@triton.jit\ndef triton_sum_dim0_in_fp32_kernel(\n    xs_ptr,\n    out_ptr,\n    stride_a,\n    a,\n    b,\n    BLOCK_SIZE_A: tl.constexpr,\n    BLOCK_SIZE_B: tl.constexpr,\n):\n    # each program handles 64 columns of xs\n    pid = tl.program_id(0)\n    offsets_b = tl.arange(0, BLOCK_SIZE_B) + pid * BLOCK_SIZE_B\n\n    all_out = tl.zeros((BLOCK_SIZE_B,), dtype=tl.float32)\n\n    for i in range(0, a, BLOCK_SIZE_A):\n        offsets_a = tl.arange(0, BLOCK_SIZE_A) + i\n        xs = tl.load(\n            xs_ptr + offsets_a[:, None] * stride_a + offsets_b[None, :],\n            mask=(offsets_a < a)[:, None] & (offsets_b < b)[None, :],\n            other=0,\n        )\n        xs = xs.to(tl.float32)\n        out = tl.sum(xs, axis=0)\n        all_out += out\n\n    tl.store(out_ptr + offsets_b, all_out, mask=offsets_b < b)\n\n\ndef mse(\n    output,\n    target,\n):  # fusing fp32 cast and MSE to save memory\n    assert output.shape == target.shape\n    assert len(output.shape) == 2\n    assert output.stride(1) == 1\n    assert target.stride(1) == 1\n\n    a, b = output.shape\n\n    BLOCK_SIZE_B = triton.next_power_of_2(b)\n\n    class _MSE(torch.autograd.Function):\n        @staticmethod\n        def forward(ctx, output, target):\n            ctx.save_for_backward(output, target)\n            out = torch.zeros(a, dtype=torch.float32, device=output.device)\n\n            triton_mse_loss_fp16_kernel[(a,)](\n                output,\n                target,\n                out,\n                stride_a_output=output.stride(0),\n                stride_a_target=target.stride(0),\n                a=a,\n                b=b,\n                BLOCK_SIZE_B=BLOCK_SIZE_B,\n            )\n\n            return out\n\n        @staticmethod\n        def backward(ctx, grad_output):\n            output, target = ctx.saved_tensors\n            res = (output - target).float()\n            res *= grad_output[:, None] * 2 / b\n            return res, None\n\n    return _MSE.apply(output, target).mean()\n\n\ndef normalized_mse(recon: torch.Tensor, xs: torch.Tensor) -> torch.Tensor:\n    # only used for auxk\n    xs_mu = (\n        triton_sum_dim0_in_fp32(xs) / xs.shape[0]\n        if xs.dtype == torch.float16\n        else xs.mean(dim=0)\n    )\n\n    loss = mse(recon, xs) / mse(\n        xs_mu[None, :].broadcast_to(xs.shape), xs\n    )\n\n    return loss\n\n\n@triton.jit\ndef triton_mse_loss_fp16_kernel(\n    output_ptr,\n    target_ptr,\n    out_ptr,\n    stride_a_output,\n    stride_a_target,\n    a,\n    b,\n    BLOCK_SIZE_B: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    offsets_b = tl.arange(0, BLOCK_SIZE_B)\n\n    output = tl.load(\n        output_ptr + pid * stride_a_output + offsets_b,\n        mask=offsets_b < b,\n    )\n    target = tl.load(\n        target_ptr + pid * stride_a_target + offsets_b,\n        mask=offsets_b < b,\n    )\n\n    output = output.to(tl.float32)\n    target = target.to(tl.float32)\n\n    mse = tl.sum((output - target) * (output - target)) / b\n\n    tl.store(out_ptr + pid, mse)\n\n\ndef triton_add_mul_(\n    x: torch.Tensor,\n    a: torch.Tensor,\n    b: torch.Tensor,\n    c: float,\n):\n    \"\"\"\n    does\n    x += a * b * c\n\n    x : [m, n]\n    a : [m, n]\n    b : [m, n]\n    c : float\n    \"\"\"\n\n    if len(a.shape) == 1:\n        a = a[None, :].broadcast_to(x.shape)\n\n    if len(b.shape) == 1:\n        b = b[None, :].broadcast_to(x.shape)\n\n    assert x.shape == a.shape == b.shape\n\n    BLOCK_SIZE_M = 64\n    BLOCK_SIZE_N = 64\n    grid = lambda META: (\n        triton.cdiv(x.shape[0], META[\"BLOCK_SIZE_M\"]),\n        triton.cdiv(x.shape[1], META[\"BLOCK_SIZE_N\"]),\n    )\n    triton_add_mul_kernel[grid](\n        x,\n        a,\n        b,\n        c,\n        x.stride(0),\n        x.stride(1),\n        a.stride(0),\n        a.stride(1),\n        b.stride(0),\n        b.stride(1),\n        BLOCK_SIZE_M,\n        BLOCK_SIZE_N,\n        x.shape[0],\n        x.shape[1],\n    )\n"}
{"type": "source_file", "path": "mlsae/model/transformers/gpt2.py", "content": "# TODO: Share code between transformers.\n\nfrom typing import Literal, overload\n\nimport torch\nfrom jaxtyping import Bool, Float, Int\nfrom torch import Tensor\nfrom torch.nn import CrossEntropyLoss, Module\nfrom transformers import (\n    AutoTokenizer,\n    PreTrainedTokenizer,\n    PreTrainedTokenizerFast,\n)\nfrom transformers.modeling_attn_mask_utils import (\n    _prepare_4d_causal_attention_mask_for_sdpa,\n)\n\nfrom .models.gpt2.modeling_gpt2 import (\n    GPT2Block,\n    GPT2Config,\n    GPT2LMHeadModel,\n)\n\n\nclass GPT2Transformer(Module):\n    def __init__(\n        self,\n        model_name: str,\n        max_length: int,\n        batch_size: int,\n        skip_special_tokens: bool = True,\n        layers: list[int] | None = None,\n        device: torch.device | None = None,\n    ) -> None:\n        \"\"\"\n        Args:\n            model_name (str): The name of a pretrained GPT2LMHeadModel model.\n\n            max_length (int): The maximum length of a tokenized input sequence.\n\n            batch_size (int): The number of sequences in a batch.\n\n            skip_special_tokens (bool): Whether to ignore special tokens.\n                Defaults to True.\n\n            layers (list[int] | None): The layers to train on.\n                If None, all layers are trained on.\n                Defaults to None.\n\n            device (torch.device | str): The device to use.\n                Defaults to \"cpu\".\n        \"\"\"\n\n        super().__init__()\n\n        device = device or torch.device(\"cpu\")\n\n        self.model_name = model_name\n        self.model: GPT2LMHeadModel = GPT2LMHeadModel.from_pretrained(model_name)  # type: ignore\n        self.model.eval()\n\n        self.batch_size = batch_size\n        self.max_length = max_length\n\n        self.config: GPT2Config = self.model.config  # type: ignore\n        self.tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast = (\n            AutoTokenizer.from_pretrained(model_name)\n        )\n\n        if layers is not None:\n            assert all(0 <= i < self.config.n_layer for i in layers)\n            self.layers = layers\n        else:\n            self.layers = list(range(self.config.n_layer))\n\n        self.n_layers = len(self.layers)\n        self.skip_special_tokens = skip_special_tokens\n\n        self.loss = CrossEntropyLoss()\n\n    @torch.no_grad()\n    def forward(\n        self, tokens: Int[Tensor, \"batch pos\"]\n    ) -> Float[Tensor, \"layer batch pos d_model\"]:\n        \"\"\"\n        Returns the residual stream activation vectors from the specified layers.\n\n        Args:\n            tokens (Int[Tensor, \"batch pos\"]): The input tokens.\n\n        Returns:\n            out (Float[Tensor, \"layer batch pos d_model\"]): The residual stream\n                activation vectors from the specified layers.\n        \"\"\"\n\n        hidden_states = self.hidden_states(tokens)\n        out = torch.stack([hidden_states[i] for i in self.layers])\n        if self.skip_special_tokens:\n            out[:, ~self._mask_special_tokens(tokens), :] = 0.0\n        return out\n\n    @torch.no_grad()\n    def hidden_states(\n        self, tokens: Int[Tensor, \"batch pos\"]\n    ) -> list[Float[Tensor, \"batch pos d_model\"]]:\n        \"\"\"\n        Return the hidden states.\n\n        Similar to the `run_with_hooks` method of `HookedTransformer` in the\n        TransformerLens library.\n\n        Args:\n            tokens (Int[Tensor, \"batch pos\"]): The input tokens.\n\n        Returns:\n            out (list[Float[Tensor, \"batch pos d_model\"]]): The hidden states.\n        \"\"\"\n\n        output = self.model.transformer.forward(\n            input_ids=tokens,  # type: ignore\n            output_hidden_states=True,\n            skip_final_layer_norm=True,\n        )\n        hidden_states: tuple[torch.Tensor, ...] = output.hidden_states  # type: ignore\n\n        # We don't include the input embeddings in hidden_states.\n        return list(hidden_states[1:])\n\n    @overload\n    def forward_at_layer(\n        self,\n        inputs_embeds: Float[Tensor, \"layer batch pos d_model\"],\n        start_at_layer: int,\n        return_type: Literal[\"loss\"],\n        tokens: Int[Tensor, \"batch pos\"] | None = None,\n    ) -> Float[Tensor, \"\"]: ...\n\n    @overload\n    def forward_at_layer(\n        self,\n        inputs_embeds: Float[Tensor, \"layer batch pos d_model\"],\n        start_at_layer: int,\n        return_type: Literal[\"logits\"],\n        tokens: Int[Tensor, \"batch pos\"] | None = None,\n    ) -> Float[Tensor, \"batch pos d_vocab\"]: ...\n\n    @overload\n    def forward_at_layer(\n        self,\n        inputs_embeds: Float[Tensor, \"layer batch pos d_model\"],\n        start_at_layer: int,\n        return_type: Literal[\"both\"],\n        tokens: Int[Tensor, \"batch pos\"] | None = None,\n    ) -> tuple[Float[Tensor, \"\"], Float[Tensor, \"batch pos d_vocab\"]]: ...\n\n    @torch.no_grad()\n    def forward_at_layer(\n        self,\n        inputs_embeds: Float[Tensor, \"layer batch pos d_model\"],\n        start_at_layer: int,\n        return_type: Literal[\"loss\", \"logits\", \"both\"] = \"both\",\n        tokens: Int[Tensor, \"batch pos\"] | None = None,\n    ) -> (\n        Float[Tensor, \"\"]\n        | Float[Tensor, \"batch pos d_vocab\"]\n        | tuple[Float[Tensor, \"\"], Float[Tensor, \"batch pos d_vocab\"]]\n    ):\n        \"\"\"\n        Return the cross-entropy loss and/or logits, starting from the specified layer.\n\n        The input tokens are needed to compute the loss.\n\n        Also similar to the TransformerLens API.\n\n        Args:\n            inputs (Float[torch.Tensor, \"layer batch pos d_model\"]): The residual\n                stream activations at the specified layer.\n\n            start_at_layer (int): The layer at which to start the forward pass.\n\n            return_type (Literal[\"loss\", \"logits\", \"both\"]): Whether to return the\n                cross-entropy loss and/or logits.\n\n            tokens (Int[torch.Tensor, \"batch pos\"] | None): If the return_type is\n                \"loss\" or \"both\", the input tokens, otherwise None.\n\n        Returns:\n            The cross-entropy loss and/or logits.\n        \"\"\"\n\n        if return_type in [\"loss\", \"both\"] and tokens is None:\n            raise ValueError(\"The input tokens are needed to compute the loss.\")\n\n        input_shape = inputs_embeds.size()[:-1]\n        batch_size = inputs_embeds.shape[0]\n\n        attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n            attention_mask=None,\n            input_shape=(batch_size, input_shape[-1]),\n            inputs_embeds=inputs_embeds,\n            past_key_values_length=0,\n        )\n\n        # Get the hidden states at the specified layer\n        hidden_states = inputs_embeds[start_at_layer, ...]\n\n        layer: GPT2Block\n        for i, layer in enumerate(self.model.transformer.h):  # type: ignore\n            # Skip layers before the specified layer\n            if start_at_layer >= i:\n                continue\n\n            outputs = layer.forward(hidden_states, attention_mask=attention_mask)  # type: ignore\n            hidden_states = outputs[0]  # type: ignore\n\n        hidden_states = self.model.transformer.ln_f.forward(hidden_states)\n        logits: torch.Tensor = self.model.lm_head.forward(hidden_states)\n\n        if return_type == \"logits\":\n            return logits\n\n        # Shift to evaluate next-token predictions\n        shifted = logits[..., :-1, :].contiguous()\n\n        labels = tokens.to(logits.device)[..., 1:].contiguous()  # type: ignore\n\n        loss = self.loss(shifted.view(-1, shifted.size(-1)), labels.view(-1))\n\n        if return_type == \"loss\":\n            return loss\n\n        return loss, logits\n\n    # TODO: Implement this properly\n    @torch.no_grad()\n    def _mask_special_tokens(\n        self, tokens: Int[Tensor, \"batch pos\"]\n    ) -> Bool[Tensor, \"batch pos\"]:\n        \"\"\"Mask out special tokens (zero the activations).\"\"\"\n\n        mask = torch.ones_like(tokens, dtype=torch.bool, device=tokens.device)\n\n        if not self.skip_special_tokens or self.tokenizer is None:\n            return mask\n\n        if self.tokenizer.eos_token_id is not None:\n            mask = mask & torch.ne(tokens, self.tokenizer.eos_token_id)\n        if self.tokenizer.pad_token_id is not None:\n            mask = mask & torch.ne(tokens, self.tokenizer.pad_token_id)\n        if self.tokenizer.bos_token_id is not None:\n            mask = mask & torch.ne(tokens, self.tokenizer.bos_token_id)\n\n        return mask\n"}
