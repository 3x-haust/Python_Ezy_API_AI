{"repo_info": {"repo_name": "predictionprophet", "repo_owner": "agentcoinorg", "repo_url": "https://github.com/agentcoinorg/predictionprophet"}}
{"type": "test_file", "path": "tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/test_evaluate_question.py", "content": "import pytest\nfrom prediction_prophet.functions.evaluate_question import is_predictable\n\n\n@pytest.mark.parametrize(\"question, answerable\", [\n    (\"Will there be an AI language model that surpasses ChatGPT and other OpenAI models before the end of 2024?\", True),\n    (\"Will Vladimir Putin be the President of Russia at the end of 2024?\", True),\n    (\"This market resolves YES when an artificial agent is appointed to the board of directors of a S&P500 company, meanwhile every day I will bet M25 in NO.\", False),\n    (\"Will there be a >0 value liquidity event for me, a former Consensys Software Inc. employee, on my shares of the company?\", False),\n    (\"Will this market have an odd number of traders by the end of 2024?\", False),\n    (\"Did COVID-19 come from a laboratory?\", False),\n])\ndef test_evaluate_question(question: str, answerable: bool) -> None:\n    (result, _) = is_predictable(question=question)\n    assert  result == answerable,  f\"Question is not evaluated correctly, see the completion: {is_predictable}\"\n"}
{"type": "test_file", "path": "tests/test_misc.py", "content": "from prediction_prophet.autonolas.research import (\n    fields_dict_to_bullet_list,\n    list_to_list_str,\n)\n\n\ndef test_list_to_list_str() -> None:\n    list_to_list_str([\"foo\", \"bar\", \"baz\"]) == '\"foo\", \"bar\" and \"baz\"'\n\n\ndef test_fields_dict_to_bullet_list() -> None:\n    field_dict = {\"foo\": \"foo sth\", \"bar\": \"bar sth\", \"baz\": \"baz sth\"}\n    EXPECTED_LIST_STR = \"\"\"  - foo: foo sth\n  - bar: bar sth\n  - baz: baz sth\"\"\"\n    assert fields_dict_to_bullet_list(field_dict) == EXPECTED_LIST_STR\n"}
{"type": "test_file", "path": "tests/test_benchmark.py", "content": "import json\n\nimport prediction_market_agent_tooling.benchmark.benchmark as bm\nfrom prediction_prophet.autonolas.research import clean_completion_json\nfrom prediction_prophet.benchmark.agents import completion_prediction_json_to_pydantic_model\n\n\ndef test_parse_result_str_to_json() -> None:\n    prediction_str = (\n        \"```json\\n\"\n        \"{\\n\"\n        '  \"decision\": \"y\",\\n'\n        '  \"decision_token_prob\": 0.6,\\n'\n        '  \"p_yes\": 0.6,\\n'\n        '  \"p_no\": 0.4,\\n'\n        '  \"confidence\": 0.8,\\n'\n        '  \"info_utility\": 0.9\\n'\n        \"}\\n\"\n        \"```\\n\"\n    )\n    prediction: bm.Prediction = completion_prediction_json_to_pydantic_model(\n        json.loads(clean_completion_json(prediction_str))\n    )\n    assert prediction.outcome_prediction is not None\n    assert prediction.outcome_prediction.p_yes == 0.6\n    assert prediction.outcome_prediction.confidence == 0.8\n    assert prediction.outcome_prediction.info_utility == 0.9\n"}
{"type": "source_file", "path": "prediction_prophet/functions/rerank_results.py", "content": "from langchain_openai import ChatOpenAI\nfrom langchain.output_parsers import CommaSeparatedListOutputParser\nfrom langchain.prompts import ChatPromptTemplate\n\nrerank_results_template = \"\"\"\nI will present you with a list of text snippets gathered from web searches\nto answer the question: {goal}.\n\nThe snippets are divided by '---snippet---'\n\nRank the snippets in order of relevance to the question.\n\nReturn the snippets ordered by relevance, separated by commas and no quotes.\n\nSnippets: {results}\n\"\"\"\ndef rerank_results(results: list[str], goal: str) -> list[str]:\n    rerank_results_prompt = ChatPromptTemplate.from_template(template=rerank_results_template)\n\n    rerank_results_chain = (\n        rerank_results_prompt |\n        ChatOpenAI(model=\"gpt-4-0125-preview\") |\n        CommaSeparatedListOutputParser()\n    )\n\n    reordered_results: list[str] = rerank_results_chain.invoke({\n        \"goal\": goal,\n        \"results\": \"---snippet---\".join(results)\n    })\n\n    return reordered_results"}
{"type": "source_file", "path": "prediction_prophet/__init__.py", "content": ""}
{"type": "source_file", "path": "prediction_prophet/autonolas/research.py", "content": "\nimport os\nimport math\nimport tenacity\nfrom datetime import timedelta\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom typing import Any, Dict, Generator, List, Optional, Tuple, TypedDict\nfrom datetime import datetime, timezone\nimport json\nfrom dotenv import load_dotenv\nimport re\nimport gc\nfrom concurrent.futures import Future\nfrom itertools import groupby\nfrom operator import itemgetter\nfrom enum import Enum\nfrom bs4 import BeautifulSoup, NavigableString\nfrom googleapiclient.discovery import build\nfrom prediction_prophet.functions.parallelism import THREADPOOL\n\nimport requests\nfrom requests import Session\nimport spacy\nimport spacy.util\nimport spacy.cli\nimport tiktoken\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.schema.output_parser import StrOutputParser\nfrom langchain_openai import OpenAIEmbeddings\n\nfrom dateutil import parser\nfrom prediction_prophet.functions.utils import check_not_none\nfrom prediction_market_agent_tooling.tools.utils import secret_str_from_env\nfrom prediction_market_agent_tooling.gtypes import Probability\nfrom prediction_market_agent_tooling.config import APIKeys\nfrom prediction_market_agent_tooling.tools.caches.db_cache import db_cache\nfrom prediction_prophet.functions.parallelism import par_map\nfrom prediction_market_agent_tooling.config import APIKeys\nfrom pydantic.types import SecretStr\nfrom prediction_market_agent_tooling.gtypes import secretstr_to_v1_secretstr\nfrom langfuse.decorators import langfuse_context\nfrom prediction_market_agent_tooling.tools.langfuse_ import get_langfuse_langchain_config, observe\n\nload_dotenv()\n\nNUM_URLS_EXTRACT = 5\nMAX_TOTAL_TOKENS_CHAT_COMPLETION = 4000  # Set the limit for cost efficiency\nWORDS_PER_TOKEN_FACTOR = 0.75\nDEFAULT_OPENAI_SETTINGS = {\n    \"max_compl_tokens\": 500,\n    \"temperature\": 0,\n}\n\nALLOWED_TOOLS = [\n    \"prediction-sentence-embedding-conservative\",\n    \"prediction-sentence-embedding-bold\",\n]\nTOOL_TO_ENGINE = {\n    \"prediction-sentence-embedding-conservative\": \"gpt-3.5-turbo\",\n    \"prediction-sentence-embedding-bold\": \"gpt-4\",\n}\n\n\n# * Consider the prediction market with the market question, the closing date and the outcomes in an isolated context that has no influence on the protagonists that are involved in the event in the real world, specified in the market question. The closing date is always arbitrarily set by the market creator and has no influence on the real world. So it is likely that the protagonists of the event in the real world are not even aware of the prediction market and do not care about the market's closing date.\n# * If the information in \"ADDITIONAL_INFORMATION\" indicate without a doubt that the event has already happened, it is very likely that the outcome of the market question will be `Yes`.\n# * If the information in \"ADDITIONAL_INFORMATION\" indicate that the event will happen after the closing date, it is very likely that the outcome of the market question will be `No`.\n# * If there exist contradicting information, evaluate the release and modification dates of those information and prioritize the information that is more recent and adjust your confidence in the probability estimation accordingly.\n# * If recent information indicates a status change in the future, pay close attention to the date of the status change and if it is before or after the closing date of the 'market question' and adjust your probability estimation accordingly, keeping the examples under \"EXAMPLES\" and their outcomes given the point in time of the status change in mind.\n# * If there exist recent information indicating that the event will happen after the closing date, it is very likely that the outcome of the market question will be `No`.\n# * Note that the sentences within the information items provided under \"ADDITIONAL_INFORMATION\" are a concatenation of the sentences from web pages that have the highest vector similarity to the 'market question'. Thus, the paragraphs do not represent the original context of the sentences and you should evaluate each sentence individually.\n\n\nPREDICTION_PROMPT = \"\"\"\nINTRODUCTION:\nYou are a Large Language Model (LLM) within a multi-agent system. Your primary task is to accurately estimate the probabilities for the outcome of a 'market question', \\\nfound in 'USER_PROMPT'. The market question is part of a prediction market, where users can place bets on the outcomes of market questions and earn rewards if the selected outcome occurrs. The 'market question' \\\nin this scenario has only two possible outcomes: `Yes` or `No`. Each market has a closing date at which the outcome is evaluated. This date is typically stated within the market question.  \\\nThe closing date is considered to be 23:59:59 of the date provided in the market question. If the event specified in the market question has not occurred before the closing date, the market question's outcome is `No`. \\\nIf the event has happened before the closing date, the market question's outcome is `Yes`. You are provided an itemized list of information under the label \"ADDITIONAL_INFORMATION\", which is \\\nsourced from a Google search engine query performed a few seconds ago and is meant to assist you in your probability estimation. You must adhere to the following 'INSTRUCTIONS'.  \n\n\nINSTRUCTIONS:\n* Examine the user's input labeled 'USER_PROMPT'. Focus on the part enclosed in double quotes, which contains the 'market question'.\n* If the 'market question' implies more than two outcomes, output the response \"Error\" and halt further processing.\n* When the current time {timestamp} has passed the closing date of the market and the event specified in the market question has not happened, the market question's outcome is `No` and the user who placed a bet on `No` will receive a reward.\n* When the current time {timestamp} has passed the closing date of the market and the event has happened before, the market question's final outcome is `Yes` and the user who placed a bet on `yes` will receive a reward.\n* Consider the prediction market with the market question, the closing date and the outcomes in an isolated context that has no influence on the protagonists that are involved in the event in the real world, specified in the market question. The closing date is always arbitrarily set by the market creator and has no influence on the real world. So it is likely that the protagonists of the event in the real world are not even aware of the prediction market and do not care about the market's closing date.\n* The probability estimations of the market question outcomes must be as accurate as possible, as an inaccurate estimation will lead to financial loss for the user.\n* Utilize your training data and the information provided under \"ADDITIONAL_INFORMATION\" to generate probability estimations for the outcomes of the 'market question'.\n* Examine the itemized list under \"ADDITIONAL_INFORMATION\" thoroughly and use all the relevant information for your probability estimation. This data is sourced from a Google search engine query done a few seconds ago. \n* Use any relevant item in \"ADDITIONAL_INFORMATION\" in addition to your training data to make the probability estimation. You can assume that you have been provided with the most current and relevant information available on the internet. Still pay close attention on the release and modification timestamps provided in parentheses right before each information item. Some information might be outdated and not relevant anymore.\n* More recent information indicated by the timestamps provided in parentheses right before each information item overrides older information within ADDITIONAL_INFORMATION and holds more weight for your probability estimation.\n* If there exist contradicting information, evaluate the release and modification dates of those information and prioritize the information that is more recent and adjust your confidence in the probability estimation accordingly.\n* Even if not all information might not be released today, you can assume that there haven't been publicly available updates in the meantime except for those inside ADDITIONAL_INFORMATION.\n* If the information in \"ADDITIONAL_INFORMATION\" indicate without a doubt that the event has already happened, it is very likely that the outcome of the market question will be `Yes`.\n* If the information in \"ADDITIONAL_INFORMATION\" indicate that the event will happen after the closing date, it is very likely that the outcome of the market question will be `No`.\n* The closer the current time `{timestamp}` is to the closing time the higher the likelyhood that the outcome of the market question will be `No`, if recent information do not clearly indicate that the event will occur before the closing date.\n* If there exist recent information indicating that the event will happen after the closing date, it is very likely that the outcome of the market question will be `No`.\n* You must provide your response in the format specified under \"OUTPUT_FORMAT\".\n* Do not include any other contents in your response.\n\n\nUSER_PROMPT:\n```\n{user_prompt}\n```\n\nADDITIONAL_INFORMATION:\n```\n{additional_information}\n```\n\nOUTPUT_FORMAT:\n* Your output response must be only a single JSON object to be parsed by Python's \"json.loads()\".\n* The JSON must contain {n_fields} fields: {fields_list}.\n{fields_description}\n* The sum of \"p_yes\" and \"p_no\" must equal 1.\n* Output only the JSON object in your response. Do not include any other contents in your response.\n\"\"\"\n\nFIELDS_DESCRIPTIONS = {\n    \"reasoning\": \"A string containing the reasoning behind your decision, and the rest of the answer you're about to give.\",\n    \"decision\": \"The decision you made. Either `y` (for `Yes`) or `n` (for `No`).\",\n    \"p_yes\": \"Probability that the market question's outcome will be `Yes`. Ranging from 0 (lowest probability) to 1 (maximum probability).\",\n    \"p_no\": \"Probability that the market questions outcome will be `No`. Ranging from 0 (lowest probability) to 1 (maximum probability).\",\n    \"confidence\": \"Indicating the confidence in the estimated probabilities you provided ranging from 0 (lowest confidence) to 1 (maximum confidence). Confidence can be calculated based on the quality and quantity of data used for the estimation.\",\n    \"info_utility\": \"Utility of the information provided in 'ADDITIONAL_INFORMATION' to help you make the probability estimation ranging from 0 (lowest utility) to 1 (maximum utility).\",\n}\n\nURL_QUERY_PROMPT = \"\"\"\nYou are a Large Language Model in a multi-agent system. Your task is to formulate search engine queries based on \\\na user's 'event question', which specifies an event and any accompanying conditions. The 'event question' allows \\\nonly two outcomes: the event will either occur or not, given the conditions. Find the 'event question' under 'USER_PROMPT' \\\nand adhere to the 'INSTRUCTIONS'.\n\nINSTRUCTIONS:\n* Carefully read the 'event question' under 'USER_PROMPT', enclosed by triple backticks.\n* If the 'event question' has more than two outcomes, respond with \"Error\" and ignore further instructions.\n* Create a list of 1-3 unique search queries likely to yield relevant and contemporary information for assessing the event's likelihood under the given conditions.\n* Each query must be unique, and they should not overlap or yield the same set of results.\n* You must provide your response in the format specified under \"OUTPUT_FORMAT\".\n* Do not include any other contents in your response.\n\nUSER_PROMPT:\n```\n{event_question}\n```\n\nOUTPUT_FORMAT:\n* Your output response must be only a single JSON object to be parsed by Python's \"json.loads()\".\n* The JSON must contain two fields: \"queries\", and \"urls\".\n   - \"queries\": A 1-5 item array of the generated search engine queries.\n* Include only the JSON object in your output.\n\"\"\"\n\n# Global constants for possible attribute names for release and update dates\nRELEASE_DATE_NAMES = [\n    \"date\",\n    \"pubdate\",\n    \"publishdate\",\n    \"OriginalPublicationDate\",\n    \"article:published_time\",\n    \"sailthru.date\",\n    \"article.published\",\n    \"published-date\",\n    \"og:published_time\",\n    \"publication_date\",\n    \"publishedDate\",\n    \"dc.date\",\n    \"DC.date\",\n    \"article:published\",\n    \"article_date_original\",\n    \"cXenseParse:recs:publishtime\",\n    \"DATE_PUBLISHED\",\n    \"pub-date\",\n    \"pub_date\",\n    \"datePublished\",\n    \"date_published\",\n    \"time_published\",\n    \"article:published_date\",\n    \"parsely-pub-date\",\n    \"publish-date\",\n    \"pubdatetime\",\n    \"published_time\",\n    \"publishedtime\",\n    \"article_date\",\n    \"created_date\",\n    \"published_at\",\n    \"lastPublishedDate\",\n    \"og:published_time\",\n    \"og:release_date\",\n    \"article:published_time\",\n    \"og:publication_date\",\n    \"og:pubdate\",\n    \"article:publication_date\",\n    \"product:availability_starts\",\n    \"product:release_date\",\n    \"event:start_date\",\n    \"event:release_date\",\n    \"og:time_published\",\n    \"og:start_date\",\n    \"og:created\",\n    \"og:creation_date\",\n    \"og:launch_date\",\n    \"og:first_published\",\n    \"og:original_publication_date\",\n    \"article:published\",\n    \"article:pub_date\",\n    \"news:published_time\",\n    \"news:publication_date\",\n    \"blog:published_time\",\n    \"blog:publication_date\",\n    \"report:published_time\",\n    \"report:publication_date\",\n    \"webpage:published_time\",\n    \"webpage:publication_date\",\n    \"post:published_time\",\n    \"post:publication_date\",\n    \"item:published_time\",\n    \"item:publication_date\",\n]\n\nUPDATE_DATE_NAMES = [\n    \"lastmod\",\n    \"lastmodified\",\n    \"last-modified\",\n    \"updated\",\n    \"dateModified\",\n    \"article:modified_time\",\n    \"modified_date\",\n    \"article:modified\",\n    \"og:updated_time\",\n    \"mod_date\",\n    \"modifiedDate\",\n    \"lastModifiedDate\",\n    \"lastUpdate\",\n    \"last_updated\",\n    \"LastUpdated\",\n    \"UpdateDate\",\n    \"updated_date\",\n    \"revision_date\",\n    \"sentry:revision\",\n    \"article:modified_date\",\n    \"date_updated\",\n    \"time_updated\",\n    \"lastUpdatedDate\",\n    \"last-update-date\",\n    \"lastupdate\",\n    \"dateLastModified\",\n    \"article:update_time\",\n    \"modified_time\",\n    \"last_modified_date\",\n    \"date_last_modified\",\n    \"og:updated_time\",\n    \"og:modified_time\",\n    \"article:modified_time\",\n    \"og:modification_date\",\n    \"og:mod_time\",\n    \"article:modification_date\",\n    \"product:availability_ends\",\n    \"product:modified_date\",\n    \"event:end_date\",\n    \"event:updated_date\",\n    \"og:time_modified\",\n    \"og:end_date\",\n    \"og:last_modified\",\n    \"og:modification_date\",\n    \"og:revision_date\",\n    \"og:last_updated\",\n    \"og:most_recent_update\",\n    \"article:updated\",\n    \"article:mod_date\",\n    \"news:updated_time\",\n    \"news:modification_date\",\n    \"blog:updated_time\",\n    \"blog:modification_date\",\n    \"report:updated_time\",\n    \"report:modification_date\",\n    \"webpage:updated_time\",\n    \"webpage:modification_date\",\n    \"post:updated_time\",\n    \"post:modification_date\",\n    \"item:updated_time\",\n    \"item:modification_date\",\n]\n\n# Global constant for HTML tags to remove\nHTML_TAGS_TO_REMOVE = [\n    \"script\",\n    \"style\",\n    \"header\",\n    \"footer\",\n    \"aside\",\n    \"nav\",\n    \"form\",\n    \"button\",\n    \"iframe\",\n    \"input\",\n    \"textarea\",\n    \"select\",\n    \"option\",\n    \"label\",\n    \"fieldset\",\n    \"legend\",\n    \"img\",\n    \"audio\",\n    \"video\",\n    \"source\",\n    \"track\",\n    \"canvas\",\n    \"svg\",\n    \"object\",\n    \"param\",\n    \"embed\",\n    \"link\",\n]\n\n\nclass EmbeddingModel(Enum):\n    spacy = \"spacy\"\n    openai = \"openai\"\n\n\nclass Prediction(TypedDict):\n    decision: Optional[str]\n    decision_token_prob: Optional[float]\n    p_yes: Probability\n    p_no: Probability\n    confidence: float\n    info_utility: float\n    reasoning: Optional[str]\n\ndef list_to_list_str(l: List[str]) -> str:\n    \"\"\"\n    Converts [\"foo\", \"bar\", \"baz\"] to '\"foo\", \"bar\" and \"baz\"'.\n    \"\"\"\n    list_str = \"\"\n    for i, item in enumerate(l):\n        if i == 0:\n            list_str += f'\"{item}\"'\n        elif i == len(l) - 1:\n            list_str += f' and \"{item}\"'\n        else:\n            list_str += f', \"{item}\"'\n    return list_str\n\ndef fields_dict_to_bullet_list(fields_dict: Dict[str, str]) -> str:\n    \"\"\"\n    Converts a dictionary of field names to their descriptions into a bullet list.\n    \"\"\"\n    bullet_list = \"\"\n    for i, (field, description) in enumerate(fields_dict.items()):\n        if i > 1:\n            bullet_list += \"\\n\"\n        bullet_list += f\"  - {field}: {description}\"\n        if i == 0:\n            bullet_list += \"\\n\"\n    return bullet_list\n\n@tenacity.retry(stop=tenacity.stop_after_attempt(3), wait=tenacity.wait_fixed(1), reraise=True)\n@db_cache(max_age=timedelta(days=1))\ndef search_google(query: str, num: int = 3) -> List[str]:\n    \"\"\"Search Google using a custom search engine.\"\"\"\n    service = build(\"customsearch\", \"v1\", developerKey=os.getenv(\"GOOGLE_SEARCH_API_KEY\"))\n    search = (\n        service.cse()\n        .list(\n            q=query,\n            cx=os.getenv(\"GOOGLE_SEARCH_ENGINE_ID\"),\n            num=num,\n        )\n        .execute()\n    )\n    try:\n        return [result[\"link\"] for result in search[\"items\"]]\n    except KeyError as e:\n        raise ValueError(f\"Can not parse results: {search}\") from e\n\n\ndef download_spacy_model(model_name: str) -> None:\n    \"\"\"Downloads the specified spaCy language model if it is not already installed.\"\"\"\n    if not isinstance(model_name, str) or not model_name:\n        raise ValueError(\"spacy model_name must be a non-empty string\")\n    if not spacy.util.is_package(model_name):\n        spacy.cli.download(model_name)\n    else:\n        print(f\"{model_name} is already installed.\")\n\n\ndef extract_event_date(doc_question: spacy.tokens.Doc) -> Optional[str]:\n    \"\"\"\n    Extracts the event date from the event question if present.\n\n    Args:\n        doc_question (spaCy Doc): Document text as a spaCy Doc object.\n\n    Returns:\n        str: The event date in year-month-day format if present, otherwise None.\n    \"\"\"\n\n    event_date_ymd = None\n\n    # Extract the date from the event question if present\n    for ent in doc_question.ents:\n        if ent.label_ == \"DATE\":\n            event_date_ymd = standardize_date(ent.text)\n\n    # If event date not formatted as YMD or not found, return None\n    try:\n        datetime.strptime(event_date_ymd, \"%Y-%m-%d\")  # type: ignore\n    except (ValueError, TypeError):\n        return None\n    else:\n        return event_date_ymd\n\n\ndef get_max_tokens_for_additional_information(\n    max_compl_tokens: int,\n    prompt: str,\n    enc: tiktoken.Encoding,\n    safety_factor: float = 1.05,\n) -> int:\n    \"\"\"\n    Calculates the estimated maximum number of tokens that can be consumed by the additional information string.\n\n    Args:\n        max_compl_tokens (int): The maximum number of chat completion output tokens.\n        prompt (str): The user prompt containing the event question.\n        enc (tiktoken.Encoding): The tiktoken encoding to be used.\n        safety_factor (float, optional): The safety factor to be used for prompt variations and message headers. Defaults to 1.05.\n\n    Returns:\n        int: The estimated number of tokens that can be consumed by the additional information string.\n    \"\"\"\n\n    # Encode the strings into tokens\n    user_prompt_enc = enc.encode(prompt)\n    prediction_prompt_enc = enc.encode(PREDICTION_PROMPT)\n\n    # Calculate token sum of thus far allocated tokens for the final prediction prompt\n    token_sum = len(user_prompt_enc) + len(prediction_prompt_enc) + max_compl_tokens\n    token_sum_safety = token_sum * safety_factor\n\n    return int(MAX_TOTAL_TOKENS_CHAT_COMPLETION - token_sum_safety)\n\n\ndef truncate_additional_information(\n    additional_informations: str,\n    max_add_tokens: int,\n    enc: tiktoken.Encoding,\n) -> str:\n    \"\"\"\n    Truncates additional information string to a specified number of tokens using tiktoken encoding.\n\n    Args:\n        additional_informations (str): The additional information string to be truncated.\n        max_add_tokens (int): The maximum number of tokens allowed for the additional information string.\n        enc (tiktoken.Encoding): The tiktoken encoding to be used.\n\n    Returns:\n    - str: The truncated additional information string.\n    \"\"\"\n\n    # Encode the string into tokens\n    add_enc = enc.encode(additional_informations)\n    len_add_enc = len(add_enc)\n\n    # Truncate additional information string if token sum exceeds maximum allowed\n    if len_add_enc <= max_add_tokens:\n        return additional_informations\n    else:\n        add_trunc_enc = add_enc[: -int(len_add_enc - max_add_tokens)]\n        return enc.decode(add_trunc_enc)\n    \n\ndef safe_get_urls_from_query(query: str, num: int = 3) -> List[str]:\n    try:\n        return get_urls_from_query(query, num)\n    except ValueError as e:\n        print(f\"Error in get_urls_from_query: {e}\")\n        return []\n\n\ndef get_urls_from_query(\n    query: str, num: int = 3\n) -> List[str]:\n    return get_urls_from_queries(queries=[query], num=num)\n\n\ndef get_urls_from_queries(queries: List[str], num: int = 3) -> List[str]:\n    \"\"\"\n    Fetch unique URLs from search engine queries, limiting the number of URLs per query.\n\n    Args:\n        queries (List[str]): List of search engine queries.\n        num (int, optional): Number of returned URLs per query. Defaults to 3.\n\n    Raises:\n        ValueError: If the number of URLs per query exceeds the maximum allowed.\n\n    Returns:\n        List[str]: Unique list of URLs, omitting PDF and download-related URLs.\n    \"\"\"\n\n    results = set()\n    max_num_fetch = 10\n\n    if num > max_num_fetch:\n        raise ValueError(f\"The maximum number of URLs per query is {max_num_fetch}.\")\n\n    for query in queries:\n        fetched_urls = search_google(\n            query=query,\n            num=max_num_fetch,  # Limit the number of returned URLs per query\n        )\n\n        # Add only unique URLs up to 'num' per query, omitting PDF and 'download' URLs\n        count = 0\n        for url in fetched_urls:\n            if url not in results and not url.endswith(\".pdf\"):\n                results.add(url)\n                count += 1\n                if count >= num:\n                    break\n\n    return list(results)\n\n\ndef standardize_date(date_text: str) -> str | None:\n    \"\"\"\n    Standardizes a given date string to the format 'YYYY-MM-DD' or 'MM-DD' if possible.\n\n    Args:\n        date_text (str): The date string to be standardized.\n\n    Raises:\n        ValueError: If the date string cannot be parsed.\n\n    Returns:\n        str: The standardized date string if possible, otherwise None.\n    \"\"\"\n\n    try:\n        # Compile regex patterns for month and day\n        month_regex = re.compile(\n            r\"\\b(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\\b\",\n            re.IGNORECASE,\n        )\n        day_regex = re.compile(r\"\\b\\d{1,2}\\b\")\n\n        # Parse date_text using dateutil parser\n        parsed_date = parser.parse(date_text)\n\n        # Check if year, month, and day are in the original date_text\n        month_exists = month_regex.search(date_text) is not None\n        day_exists = day_regex.search(date_text) is not None\n        year_exists = str(parsed_date.year) in date_text\n\n        # Format the parsed date accordingly\n        if year_exists and month_exists and day_exists:\n            return parsed_date.strftime(\"%Y-%m-%d\")\n        elif month_exists and day_exists:\n            return parsed_date.strftime(\"%m-%d\")\n        else:\n            return None\n    except Exception:\n        return None\n\n\ndef get_context_around_isolated_event_date(\n    doc_text: spacy.tokens.Doc, event_date_ymd: str, len_sentence_threshold: int, max_context: int = 50\n) -> list[str]:\n    \"\"\"\n    Extract sentences around isolated dates within the text.\n\n    Args:\n        doc_text (spaCy Doc): Document text as a spaCy Doc object.\n        event_date_ymd (str): Event date in year-day-month format.\n        len_sentence_threshold (int): Minimum number of words required for a sentence to be considered contextful.\n        max_context (int, optional): Maximum number of words to include in the context. Defaults to 50.\n\n    Raises:\n        ValueError: If maximum context is less than threshold or greater than 100.\n\n    Returns:\n        list: List of sentences surrounding the target date.\n    \"\"\"\n\n    # Check max_context value constraints\n    if max_context < len_sentence_threshold:\n        raise ValueError(\n            f\"The maximum number of words must be greater than or equal to the minimum number of words ({len_sentence_threshold}) required for a sentence to be considered contextful.\"\n        )\n    if max_context > 100:\n        raise ValueError(\n            f\"The maximum number of words must be less than or equal to 300.\"\n        )\n\n    contexts_list: list[str] = []\n    len_doc_text = len(doc_text)\n\n    # Extract the month and day from the event date\n    event_date_md = event_date_ymd[5:]\n\n    for ent in doc_text.ents:\n        if ent.label_ == \"DATE\":\n            standardized_date = standardize_date(ent.text)\n            if standardized_date is None:\n                continue\n\n            # Check if the entity matches the target date\n            if (\n                standardized_date == event_date_ymd\n                or standardized_date == event_date_md\n            ):\n                sentence = next(\n                    sent\n                    for sent in doc_text.sents\n                    if sent.start <= ent.start and sent.end >= ent.end\n                )\n\n                context_words = len(sentence.text.split())\n\n                # Extend the context if the sentence is too short\n                if context_words < len_sentence_threshold:\n                    start_token, end_token = sentence.start, sentence.end\n                    while context_words < max_context:\n                        # Extend the context from the start of the sentence\n                        new_start = start_token - 1\n                        while (\n                            new_start >= 0 and doc_text[new_start].is_sent_start is None\n                        ):\n                            new_start -= 1\n                        if new_start >= 0:\n                            context_words += len(\n                                doc_text[new_start:start_token].text.split()\n                            )\n                            start_token = new_start\n\n                        # Break if max_context is reached\n                        if context_words >= max_context:\n                            break\n\n                        # Extend the context from the end of the sentence\n                        new_end = end_token + 1\n                        while (\n                            new_end < len_doc_text\n                            and doc_text[new_end].sent == sentence.sent\n                        ):\n                            new_end += 1\n                        if new_end < len_doc_text:\n                            context_words += len(\n                                doc_text[end_token:new_end].text.split()\n                            )\n                            end_token = new_end\n\n                        # Break if max_context is reached\n                        if context_words >= max_context:\n                            break\n\n                        # Break if max_context cannot be reached\n                        if new_end == len_doc_text and start_token <= 0:\n                            break\n\n                    context = doc_text[\n                        max(0, start_token) : min(len_doc_text, end_token)\n                    ].text\n                    contexts_list.append(context)\n\n    return contexts_list\n\n\ndef concatenate_short_sentences(sentences: list[str], len_sentence_threshold: int) -> list[str]:\n    modified_sentences: list[str] = []\n    i = 0\n    while i < len(sentences):\n        sentence = sentences[i]\n        word_count = len(sentence.split())\n\n        # Check if the sentence is shorter than the threshold\n        while word_count < len_sentence_threshold:\n            i += 1\n            # Break the loop if we reach the end of the list\n            if i >= len(sentences):\n                break\n            next_sentence = sentences[i]\n            sentence += \" \" + next_sentence\n            word_count += len(next_sentence.split())\n\n        modified_sentences.append(sentence)\n        i += 1\n\n    return modified_sentences\n\n\n@db_cache\ndef openai_embedding_cached(text: str, model: str = \"text-embedding-ada-002\") -> list[float]:\n    emb = OpenAIEmbeddings(model=model)\n    vector: list[float] = emb.embed_query(text)\n    return vector\n\n\ndef extract_similarity_scores(\n    text: str,\n    doc_question: spacy.tokens.Doc,\n    event_date: str | None,\n    nlp: spacy.Language,\n    date: str,\n    embedding_model: EmbeddingModel,\n) -> List[Tuple[str, float, str]]:\n    \"\"\"\n    Extract relevant information from website text based on a given event question.\n\n    Args:\n        text (str): The website text to extract information from.\n        event_date (str): Event date in year-day-month format.\n        nlp: The spaCy NLP model.\n        date (str): The release and modification dates of the website.\n\n    Returns:\n        List[Tuple[str, float, str]]: List of tuples containing the extracted sentences, their similarity scores, and release dates.\n    \"\"\"\n\n    # Constants for sentence length and number thresholds\n    len_sentence_threshold = 10\n    num_sentences_threshold = 1000\n    sentences = []\n    event_date_sentences: list[str] = []\n    seen = set()\n\n    # Truncate text for performance optimization\n    text = text[:50000]\n\n    # Apply NLP pipeline to text\n    doc_text = nlp(text)\n\n    # Extract unique sentences\n    for sent in doc_text.sents:\n        sentence_text = sent.text\n        if (\n            len(sentence_text.split()) >= len_sentence_threshold\n            and sentence_text not in seen\n        ):\n            sentences.append(sentence_text)\n            seen.add(sentence_text)\n\n    ## Temporarily deactivated: News sites with a lot of date occurrences lead to false positives\n    ## The embedding model is not advanced enough\n    # Extract contextual sentences around event date occurrences within too short sentences\n    # if event_date is not None:\n    #     event_date_sentences.extend(\n    #         get_context_around_isolated_event_date(\n    #             doc_text, event_date, len_sentence_threshold, max_context=50\n    #         )\n    #     )\n    # sentences.extend(event_date_sentences)\n\n    if not sentences:\n        return []\n\n    # Concatenate short sentences\n    sentences = concatenate_short_sentences(sentences, len_sentence_threshold)\n\n    # Limit the number of sentences for performance optimization\n    sentences = sentences[:num_sentences_threshold]\n\n    # Encode sentences using an embedding model\n    similarities = par_map(\n        sentences, \n        lambda sentence: (\n            doc_question.similarity(nlp(sentence)) if embedding_model == EmbeddingModel.spacy \n            else cosine_similarity(\n                [openai_embedding_cached(sentence)], \n                [openai_embedding_cached(doc_question.text)]\n            )[0][0] if embedding_model == EmbeddingModel.openai \n            else None\n        )\n    )\n\n    # Create tuples and store them in a list\n    sentence_similarity_date_tuples = [\n        (sentence, similarity, date)\n        for sentence, similarity in zip(sentences, similarities)\n        if similarity > 0.4\n    ]\n\n    return sentence_similarity_date_tuples\n\n\ndef get_date(soup: BeautifulSoup) -> str:\n    \"\"\"\n    Retrieves the release and modification dates from the soup object containing the HTML tree.\n\n    Args:\n        soup (BeautifulSoup): The BeautifulSoup object for the webpage.\n\n    Returns:\n        str: A string representing the release and modification dates.\n    \"\"\"\n\n    release_date = \"unknown\"\n    modified_date = \"unknown\"\n\n    # Search for an update or modified date in the meta tags\n    for name in UPDATE_DATE_NAMES:\n        meta_tag = soup.find(\"meta\", {\"name\": name}) or soup.find(\n            \"meta\", {\"property\": name}\n        )\n        if meta_tag:\n            modified_date = meta_tag.get(\"content\", \"\")\n            break\n\n    # If not found, then look for release or publication date\n    for name in RELEASE_DATE_NAMES:\n        meta_tag = soup.find(\"meta\", {\"name\": name}) or soup.find(\n            \"meta\", {\"property\": name}\n        )\n        if meta_tag:\n            release_date = meta_tag.get(\"content\", \"\")\n            break\n\n    ## Temporarily deactivated\n    # # Fallback to using the first time tag if neither release nor modified dates are found\n    # if release_date == \"unknown\" and modified_date == \"unknown\":\n    #     time_tag = soup.find(\"time\")\n    #     if time_tag:\n    #         release_date = time_tag.get(\"datetime\", \"\")\n\n    return f\"({release_date}, {modified_date})\"\n\n\ndef extract_sentences(\n    html: str,\n    doc_question: spacy.tokens.Doc,\n    event_date: str | None,\n    nlp: spacy.Language,\n    embedding_model: EmbeddingModel,\n) -> List[Tuple[str, float, str]]:\n    \"\"\"\n    Extract relevant information from HTML string.\n\n    Args:\n        html (str): The HTML content to extract text from.\n        event_date (str): Event date in year-month-day format.\n        nlp: NLP object for additional text processing.\n\n    Raises:\n        ValueError: If the HTML content is empty.\n        ValueError: If the release or update date could not be extracted from the HTML.\n\n    Returns:\n        List[Tuple[str, float, str]]: List of tuples containing the extracted sentences, their similarity scores, and release dates.\n    \"\"\"\n\n    if not html:\n        raise ValueError(\"HTML is empty.\")\n\n    soup = BeautifulSoup(html, \"html.parser\")\n\n    # Get the date of the website\n    date = get_date(soup)\n    if date is None:\n        raise ValueError(\"Could not extract release or update date from HTML.\")\n\n    # Remove unnecessary tags to clean up text\n    for element in soup(HTML_TAGS_TO_REMOVE):\n        element.replace_with(NavigableString(\" \"))\n\n    # Extract and clean text\n    text = soup.get_text()\n    lines = (line.strip() for line in text.splitlines())\n    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n    text = \". \".join(chunk for chunk in chunks if chunk)\n    text = re.sub(r\"\\.{2,}\", \".\", text)\n\n    # Get List of (sentence, similarity, date) tuples\n    similarity_scores = extract_similarity_scores(\n        text=text,\n        doc_question=doc_question,\n        event_date=event_date,\n        nlp=nlp,\n        date=date,\n        embedding_model=embedding_model,\n    )\n\n    if not similarity_scores:\n        return []\n\n    return similarity_scores\n\n\ndef process_in_batches(\n    urls: List[str], batch_size: int = 15, timeout: int = 10\n) -> Generator[List[Tuple[Future[requests.models.Response], str]], None, None]:\n    \"\"\"\n    Process URLs in batches using a generator and thread pool executor.\n\n    Args:\n        urls (List[str]): List of URLs to process.\n        batch_size (int, optional): Size of the processing batch_size. Default is 5.\n        timeout (int, optional): Timeout for each request in seconds. Default is 10.\n\n    Raises:\n        ValueError: If the batch_size is less than or equal to zero.\n        ValueError: If the timeout is less than or equal to zero.\n\n    Yields:\n        List[Tuple[Future, str]]: List containing Future objects and URLs for each batch.\n    \"\"\"\n\n    if batch_size <= 0:\n        raise ValueError(\"The 'batch_size' size must be greater than zero.\")\n\n    if timeout <= 0:\n        raise ValueError(\"The 'timeout' must be greater than zero.\")\n\n    session = Session()\n    session.max_redirects = 5\n\n    # User-Agent headers\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/117.0\"\n    }\n    session.headers.update(headers)\n\n    # Using THREADPOOL to execute requests in parallel\n    # Loop through the URLs in batch_size of size 'batch_size'\n    for i in range(0, len(urls), batch_size):\n        batch = urls[i : i + batch_size]\n\n        # Submit the batch of URLs for processing\n        futures = []\n        for url in batch:\n            try:\n                # Submit a HEAD request to the url and check Content-Type\n                head_future = THREADPOOL.submit(\n                    session.head,\n                    url,\n                    headers=headers,\n                    timeout=timeout,\n                    allow_redirects=True,\n                )\n                head_response = head_future.result()\n                if \"text/html\" not in head_response.headers.get(\"Content-Type\", \"\"):\n                    continue\n                else:\n                    # Submit a GET request to the url\n                    futures.append(\n                        (\n                            THREADPOOL.submit(\n                                session.get, url, headers=headers, timeout=timeout\n                            ),\n                            url,\n                        )\n                    )\n            except Exception as e:\n                print(f\"An error occurred: {e}\")\n\n        yield futures\n\n\ndef extract_and_sort_sentences(\n    urls: List[str],\n    event_question: str,\n    nlp: spacy.Language,\n    embedding_model: EmbeddingModel,\n) -> List[Tuple[str, float, str]]:\n    \"\"\"\n    Extract texts from a list of URLs using Spacy models.\n\n    Args:\n        urls (List[str]): List of URLs to extract text from.\n        event_question (str): Event-related question for text extraction.\n\n    Raises:\n        ValueError: If the event date could not be extracted from the event question.\n        Timeout: If the request timed out.\n\n    Returns:\n        List[Tuple[str, float, str]]: List of tuples containing the extracted sentences, their similarity scores, and release dates.\n    \"\"\"\n\n    # Initialize empty list for storing extracted sentences along with their similarity scores and release dates\n    all_sentences = []\n\n    # Process the event question with spacy\n    doc_question = nlp(event_question)\n    event_date = extract_event_date(doc_question)\n\n    # Process URLs in batches\n    for batch in process_in_batches(urls=urls):\n        for future, url in batch:\n            try:\n                result = future.result()\n                if result.status_code != 200:\n                    del result\n                    continue\n                # Extract relevant information for the event question\n                extracted_sentences = extract_sentences(\n                    html=result.text,\n                    doc_question=doc_question,\n                    event_date=event_date,\n                    nlp=nlp,\n                    embedding_model=embedding_model,\n                )\n\n                # Delete the result object to free memory\n                del result\n\n                # Append the extracted text if available and increment the count\n                if extracted_sentences:\n                    all_sentences.extend(extracted_sentences)\n\n            except requests.exceptions.Timeout:\n                print(f\"Request for {url} timed out.\")\n\n            except Exception as e:\n                print(f\"An error occurred: {e}\")\n\n    all_sentences.sort(\n        key=lambda x: x[1], reverse=True\n    )  # Assuming the second element is the similarity score\n\n    return all_sentences\n\n\ndef join_and_group_sentences(\n    sentences: List[Tuple[str, float, str]], max_words: int\n) -> str:\n    \"\"\"\n    Join the sentences and group them by date.\n\n    Args:\n        sentences (List[Tuple[str, float, str]]): List of tuples containing the extracted sentences, their similarity scores, and release dates.\n        max_words (int): Maximum number of words allowed for the output summary.\n\n    Returns:\n        str: The joined sentences grouped by date.\n    \"\"\"\n    # Initialize final output string and word count\n    final_output = \"\"\n    current_word_count = 0\n\n    # Initialize a list to hold the sentences that will be included in the final output\n    filtered_sentences = []\n\n    # Filter sentences based on word count\n    for sentence, _, date in sentences:\n        additional_word_count = len(sentence.split())\n        if current_word_count + additional_word_count <= max_words:\n            filtered_sentences.append((sentence, date))\n            current_word_count += additional_word_count\n        else:\n            break\n\n    # Sort filtered_sentences by date for grouping\n    filtered_sentences.sort(key=itemgetter(1))\n\n    # Group by date and iterate\n    for date, group in groupby(filtered_sentences, key=itemgetter(1)):\n        sentences_group = [sentence for sentence, _ in group]\n        concatenated_sentences = \" | \".join(sentences_group)\n\n        # Formatting the string as per your requirement\n        formatted_string = f\"- {date}:{concatenated_sentences}\\n\\n\"\n\n        # Add this formatted string to the final output\n        final_output += formatted_string\n\n    return final_output\n\n\n@observe()\ndef fetch_additional_information(\n    event_question: str,\n    max_add_words: int,\n    nlp: spacy.Language,\n    embedding_model: EmbeddingModel,\n    engine: str = \"gpt-3.5-turbo\",\n    temperature: float = 0.5,\n    max_compl_tokens: int = 500,\n) -> str:\n    \"\"\"\n    Get urls from a web search and extract relevant information based on an event question.\n\n    Args:\n        event_question (str): The question related to the event.\n        max_add_words (int): The maximum number of words allowed for additional information.\n        temperature (float): The temperature parameter for the engine.\n        engine (str): The openai engine. Defaults to \"gpt-3.5-turbo\".\n        temperature (float): The temperature parameter for the engine. Defaults to 1.0.\n        max_compl_tokens (int): The maximum number of tokens for the engine's response.\n\n    Returns:\n        str: The relevant information fetched from all the URLs concatenated.\n    \"\"\"\n\n    # Create URL query prompt\n    url_query_prompt = URL_QUERY_PROMPT.format(event_question=event_question)\n\n    # # Perform moderation check\n    # moderation_result = client.moderations.create(url_query_prompt)\n    # if moderation_result[\"results\"][0][\"flagged\"]:\n    #     # return empty additional information if the prompt is flagged\n    #     return \"\"\n\n    # Create messages for the OpenAI engine\n    messages = [\n        (\"system\", \"You are a helpful assistant.\"),\n        (\"user\", url_query_prompt),\n    ]\n\n    # Fetch queries from the OpenAI engine\n    research_prompt = ChatPromptTemplate.from_messages(messages=messages)\n    research_chain = (\n        research_prompt |\n        ChatOpenAI(\n            model=engine,\n            temperature=temperature,\n            max_tokens=max_compl_tokens,\n            n=1, \n            timeout=120,\n        ) |\n        StrOutputParser()\n    )\n    response = research_chain.invoke({}, config=get_langfuse_langchain_config())\n\n    # Parse the response content\n    try:\n        json_data = json.loads(clean_completion_json(response))\n    except json.decoder.JSONDecodeError as e:\n        raise ValueError(f\"The response from {engine=} could not be parsed as JSON: {response=}\") from e\n\n    # Get URLs from queries\n    urls = get_urls_from_queries(\n        json_data[\"queries\"],\n    )\n\n    # Extract relevant sentences from URLs\n    relevant_sentences_sorted = extract_and_sort_sentences(\n        urls=urls,\n        event_question=event_question,\n        nlp=nlp,\n        embedding_model=embedding_model,\n    )\n\n    # Join the sorted sentences and group them by date\n    additional_informations = join_and_group_sentences(\n        relevant_sentences_sorted, max_add_words\n    )\n\n    return additional_informations\n\n\n@observe()\ndef research(\n    prompt: str,\n    max_tokens: int | None = None,\n    temperature: float | None = None,\n    engine: str = \"gpt-3.5-turbo\",\n    embedding_model: EmbeddingModel = EmbeddingModel.spacy,\n) -> str:\n    prompt = f\"\\\"{prompt}\\\"\"\n    max_compl_tokens =  max_tokens or DEFAULT_OPENAI_SETTINGS[\"max_compl_tokens\"]\n    temperature = temperature or DEFAULT_OPENAI_SETTINGS[\"temperature\"]\n\n    # Load the spacy model\n    nlp = spacy.load(\"en_core_web_md\")\n\n    # Extract the event question from the prompt\n    event_question = check_not_none(re.search(r\"\\\"(.+?)\\\"\", prompt)).group(1)\n    if not event_question:\n        raise ValueError(\"No event question found in prompt.\")\n\n    # Get the tiktoken base encoding\n    enc = tiktoken.encoding_for_model(engine)\n\n    # Calculate the maximum number of tokens and words that can be consumed by the additional information string\n    max_add_tokens = get_max_tokens_for_additional_information(\n        max_compl_tokens=max_compl_tokens,\n        prompt=prompt,\n        enc=enc,\n    )\n    max_add_words = int(max_add_tokens * 0.75)\n\n    # Fetch additional information\n    additional_information = fetch_additional_information(\n        event_question=event_question,\n        engine=engine,\n        temperature=0.5,\n        max_compl_tokens=max_compl_tokens,\n        nlp=nlp,\n        max_add_words=max_add_words,\n        embedding_model=embedding_model,\n    )\n\n    # Truncate additional information to stay within the chat completion token limit of 4096\n    additional_information = truncate_additional_information(\n        additional_information,\n        max_add_tokens,\n        enc=enc,\n    )\n\n    # Spacy loads ~500MB into memory. Free it with force.\n    del nlp\n    gc.collect()\n    \n    return additional_information\n\n\n@observe()\ndef make_prediction(\n    prompt: str,\n    additional_information: str,\n    temperature: float = 0.7,\n    engine: str = \"gpt-3.5-turbo-0125\",\n    log_probs: bool = False,\n    top_logprobs: int = 5,\n    include_reasoning: bool = False,\n    api_key: SecretStr | None = None,\n) -> Prediction:\n    if api_key == None:\n        api_key = APIKeys().openai_api_key\n    \n    current_time_utc = datetime.now(timezone.utc)\n    formatted_time_utc = current_time_utc.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-6] + \"Z\"\n\n    prediction_prompt = ChatPromptTemplate.from_template(template=PREDICTION_PROMPT)\n\n    llm = ChatOpenAI(model=engine, temperature=temperature, api_key=secretstr_to_v1_secretstr(api_key))\n    field_descriptions = FIELDS_DESCRIPTIONS.copy()\n    if not include_reasoning:\n        field_descriptions.pop(\"reasoning\")\n    formatted_messages = prediction_prompt.format_messages(\n        user_prompt=prompt,\n        additional_information=additional_information,\n        n_fields=len(field_descriptions),\n        fields_list=list_to_list_str(list(field_descriptions)),\n        fields_description=fields_dict_to_bullet_list(field_descriptions),\n        timestamp=formatted_time_utc,\n    )\n    generation = llm.generate([formatted_messages], logprobs=log_probs, top_logprobs=top_logprobs if log_probs else None, callbacks=[langfuse_context.get_current_langchain_handler()])\n\n    completion = generation.generations[0][0].text\n\n    # Get probability that is based on the token's top logprobs.\n    decision, probability = None, None\n    if log_probs:\n        for token in check_not_none(generation.generations[0][0].generation_info)[\"logprobs\"][\"content\"]:  \n            # Check if the token is a decision token, we prompt the model for it to be there, so it is in 99% of cases.\n            if token[\"token\"] in (\"y\", \"n\"):\n                decision = token[\"token\"]\n                probability = math.exp(token[\"logprob\"])\n                break\n\n        if decision is None or probability is None:\n            raise ValueError(f\"No decision found in completion from {engine=}, {completion=}, {formatted_messages=}\")\n\n    response: Prediction = json.loads(clean_completion_json(completion))\n    response[\"decision\"] = decision\n    response[\"decision_token_prob\"] = probability\n    \n    return response\n\n\ndef clean_completion_json(completion: str) -> str:\n    \"\"\"\n    Cleans completion JSON in form of a string:\n\n    ```json\n    {\n        ...\n    }\n    ```\n\n    into just { ... }\n    ```\n    \"\"\"\n    start_index = completion.find(\"{\")\n    end_index = completion.rfind(\"}\")\n    completion = completion[start_index : end_index + 1]\n    return completion\n"}
{"type": "source_file", "path": "prediction_prophet/benchmark/agents.py", "content": "import logging\nimport typing as t\n\nfrom prediction_market_agent_tooling.benchmark.agents import (\n    AbstractBenchmarkedAgent,\n)\nfrom prediction_market_agent_tooling.benchmark.utils import (\n    Prediction,\n)\nfrom datetime import datetime\nfrom prediction_prophet.autonolas.research import EmbeddingModel\nfrom prediction_prophet.autonolas.research import make_prediction, get_urls_from_queries\nfrom prediction_prophet.autonolas.research import research as research_autonolas\nfrom prediction_prophet.functions.rephrase_question import rephrase_question\nfrom prediction_prophet.functions.research import NoResulsFoundError, NotEnoughScrapedSitesError, Research, research as prophet_research\nfrom prediction_prophet.functions.search import search\nfrom prediction_prophet.functions.utils import url_is_older_than\nfrom prediction_prophet.models.WebSearchResult import WebSearchResult\nfrom unittest.mock import patch\nfrom prediction_prophet.functions.search import search\nfrom prediction_market_agent_tooling.benchmark.utils import (\n    OutcomePrediction,\n    Prediction,\n)\nfrom pydantic.types import SecretStr\nfrom prediction_prophet.autonolas.research import Prediction as LLMCompletionPredictionDict\nfrom prediction_market_agent_tooling.tools.langfuse_ import observe\nfrom prediction_market_agent_tooling.tools.is_predictable import is_predictable_binary\n\nif t.TYPE_CHECKING:\n    from loguru import Logger\n\n\n@observe()\ndef _make_prediction(\n    market_question: str,\n    additional_information: str,\n    engine: str,\n    temperature: float,\n    include_reasoning: bool = False,\n    api_key: SecretStr | None = None,\n) -> Prediction:\n    \"\"\"\n    We prompt model to output a simple flat JSON and convert it to a more structured pydantic model here.\n    \"\"\"\n    prediction = make_prediction(\n        prompt=market_question,\n        additional_information=additional_information,\n        engine=engine,\n        temperature=temperature,\n        api_key=api_key,\n        include_reasoning=include_reasoning,\n    )\n    return completion_prediction_json_to_pydantic_model(\n        prediction\n    )\n\n\ndef completion_prediction_json_to_pydantic_model(\n    completion_prediction: LLMCompletionPredictionDict,\n) -> Prediction:\n    return Prediction(\n        outcome_prediction=OutcomePrediction.model_validate(completion_prediction),\n    )\n\n\nclass QuestionOnlyAgent(AbstractBenchmarkedAgent):\n    def __init__(\n        self,\n        model: str,\n        temperature: float = 0.0,\n        agent_name: str = \"question-only\",\n        max_workers: t.Optional[int] = None,\n        logger: t.Union[logging.Logger, \"Logger\"] = logging.getLogger(),\n    ):\n        super().__init__(agent_name=agent_name, max_workers=max_workers)\n        self.model: str = model\n        self.temperature = temperature\n        self.logger = logger\n\n    def predict(\n        self, market_question: str\n    ) -> Prediction:\n        try:\n            return _make_prediction(\n                market_question=market_question,\n                additional_information=\"\",\n                engine=self.model,\n                temperature=self.temperature,\n            )\n        except ValueError as e:\n            self.logger.error(f\"Error in QuestionOnlyAgent's predict: {e}\")\n            return Prediction()\n        \n    def predict_restricted(\n        self, market_question: str, time_restriction_up_to: datetime\n    ) -> Prediction:\n        return self.predict(market_question)\n\n\nclass OlasAgent(AbstractBenchmarkedAgent):\n    def __init__(\n        self,\n        model: str,\n        temperature: float = 0.0,\n        agent_name: str = \"olas\",\n        max_workers: t.Optional[int] = None,\n        embedding_model: EmbeddingModel = EmbeddingModel.spacy,\n        logger: t.Union[logging.Logger, \"Logger\"] = logging.getLogger(),\n    ):\n        super().__init__(agent_name=agent_name, max_workers=max_workers)\n        self.model: str = model\n        self.temperature = temperature\n        self.embedding_model = embedding_model\n        self.logger = logger\n\n    def is_predictable(self, market_question: str) -> bool:\n        result = is_predictable_binary(question=market_question)\n        return result\n\n    def is_predictable_restricted(self, market_question: str, time_restriction_up_to: datetime) -> bool:\n        result = is_predictable_binary(question=market_question)\n        return result\n    \n    def research(self, market_question: str) -> str:\n        return research_autonolas(\n            prompt=market_question,\n            engine=self.model,\n            embedding_model=self.embedding_model,\n        )\n\n    def predict(self, market_question: str) -> Prediction:\n        try:\n            researched = self.research(market_question=market_question)\n            return _make_prediction(\n                market_question=market_question,\n                additional_information=researched,\n                engine=self.model,\n                temperature=self.temperature,\n            )\n        except ValueError as e:\n            self.logger.error(f\"Error in OlasAgent's predict: {e}\")\n            return Prediction()\n\n    def predict_restricted(\n        self, market_question: str, time_restriction_up_to: datetime\n    ) -> Prediction:\n        def side_effect(*args: t.Any, **kwargs: t.Any) -> list[str]:\n            results: list[str] = get_urls_from_queries(*args, **kwargs)\n            results_filtered = [\n                url for url in results\n                if url_is_older_than(url, time_restriction_up_to.date())\n            ]\n            return results_filtered\n    \n        with patch('prediction_prophet.autonolas.research.get_urls_from_queries', side_effect=side_effect, autospec=True):\n            return self.predict(market_question)\n\n\nclass PredictionProphetAgent(AbstractBenchmarkedAgent):\n    def __init__(\n        self,\n        model: str,\n        research_temperature: float = 0.7,\n        prediction_temperature: float = 0.0,\n        agent_name: str = \"prediction_prophet\",\n        include_reasoning: bool = False,\n        use_summaries: bool = False,\n        use_tavily_raw_content: bool = False,\n        initial_subqueries_limit: int = 20,\n        subqueries_limit: int = 4,\n        max_results_per_search: int = 5,\n        min_scraped_sites: int = 5,\n        max_workers: t.Optional[int] = None,\n        logger: t.Union[logging.Logger, \"Logger\"] = logging.getLogger(),\n    ):\n        super().__init__(agent_name=agent_name, max_workers=max_workers)\n        self.model: str = model\n        self.include_reasoning = include_reasoning\n        self.research_temperature = research_temperature\n        self.prediction_temperature = prediction_temperature\n        self.use_summaries = use_summaries\n        self.use_tavily_raw_content = use_tavily_raw_content\n        self.initial_subqueries_limit = initial_subqueries_limit\n        self.subqueries_limit = subqueries_limit\n        self.max_results_per_search = max_results_per_search\n        self.min_scraped_sites = min_scraped_sites\n        self.logger = logger\n\n    def is_predictable(self, market_question: str) -> bool:\n        result = is_predictable_binary(question=market_question)\n        return result\n\n    def is_predictable_restricted(self, market_question: str, time_restriction_up_to: datetime) -> bool:\n        result = is_predictable_binary(question=market_question)\n        return result\n    \n    def research(self, market_question: str) -> Research:\n        return prophet_research(\n            goal=market_question,\n            model=self.model,\n            temperature=self.research_temperature,\n            use_summaries=self.use_summaries,\n            use_tavily_raw_content=self.use_tavily_raw_content,\n            initial_subqueries_limit=self.initial_subqueries_limit,\n            subqueries_limit=self.subqueries_limit,\n            max_results_per_search=self.max_results_per_search,\n            min_scraped_sites=self.min_scraped_sites,\n            logger=self.logger,\n        )\n\n    def predict(self, market_question: str) -> Prediction:\n        try:\n            research = self.research(market_question)\n            return _make_prediction(\n                market_question=market_question,\n                additional_information=research.report,\n                engine=self.model,\n                temperature=self.prediction_temperature,\n                include_reasoning=self.include_reasoning,\n        )\n        except (NoResulsFoundError, NotEnoughScrapedSitesError) as e:\n            self.logger.warning(f\"Problem in PredictionProphet's predict: {e}\")\n            return Prediction()\n        except ValueError as e:\n            self.logger.error(f\"Error in PredictionProphet's predict: {e}\")\n            return Prediction()\n\n    def predict_restricted(\n        self, market_question: str, time_restriction_up_to: datetime\n    ) -> Prediction:\n        def side_effect(*args: t.Any, **kwargs: t.Any) -> list[tuple[str, WebSearchResult]]:\n            results: list[tuple[str, WebSearchResult]] = search(*args, **kwargs)\n            results_filtered = [\n                r for r in results\n                if url_is_older_than(r[1].url, time_restriction_up_to.date())\n            ]\n            return results_filtered\n    \n        with patch('prediction_prophet.functions.research.search', side_effect=side_effect, autospec=True):\n            return self.predict(market_question)\n\nclass RephrasingOlasAgent(OlasAgent):\n    def __init__(\n        self,\n        model: str,\n        temperature: float = 0.0,\n        agent_name: str = \"reph-olas\",\n        max_workers: t.Optional[int] = None,\n        embedding_model: EmbeddingModel = EmbeddingModel.spacy,\n    ):\n        super().__init__(\n            model=model,\n            temperature=temperature,\n            embedding_model=embedding_model,\n            agent_name=agent_name,\n            max_workers=max_workers,\n        )\n\n    def research(self, market_question: str) -> str:\n        questions = rephrase_question(question=market_question)\n\n        report_original = super().research(market_question=questions.original_question)\n        report_negated = super().research(market_question=questions.negated_question)\n        report_universal = super().research(market_question=questions.open_ended_question)\n\n        report_concat = \"\\n\\n---\\n\\n\".join(\n            [\n                f\"### {r_name}\\n\\n{r}\"\n                for r_name, r in [\n                    (\"Research based on the question\", report_original),\n                    (\"Research based on the negated question\", report_negated),\n                    (\"Research based on the universal search query\", report_universal),\n                ]\n                if r is not None\n            ]\n        )\n\n        return report_concat\n\n\nAGENTS = [\n    OlasAgent,\n    RephrasingOlasAgent,\n    PredictionProphetAgent,\n    QuestionOnlyAgent,\n]\n"}
{"type": "source_file", "path": "prediction_prophet/functions/prepare_report.py", "content": "import os\nimport tiktoken\nimport typing as t\nfrom langchain_openai import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.schema.output_parser import StrOutputParser\nfrom prediction_prophet.functions.utils import trim_to_n_tokens\nfrom prediction_market_agent_tooling.config import APIKeys\nfrom pydantic.types import SecretStr\nfrom prediction_market_agent_tooling.gtypes import secretstr_to_v1_secretstr\nfrom prediction_market_agent_tooling.tools.langfuse_ import get_langfuse_langchain_config, observe\n\n@observe()\ndef prepare_summary(goal: str, content: str, model: str, api_key: SecretStr | None = None, trim_content_to_tokens: t.Optional[int] = None) -> str:\n    if api_key == None:\n        api_key = APIKeys().openai_api_key\n    \n    prompt_template = \"\"\"Write comprehensive summary of the following web content, that provides relevant information to answer the question: '{goal}'.\nBut cut the fluff and keep it up to the point.\nWrite in bullet points.\n    \nContent:\n\n{content}\n\"\"\"\n    content = trim_to_n_tokens(content, trim_content_to_tokens, model) if trim_content_to_tokens else content\n    evaluation_prompt = ChatPromptTemplate.from_template(template=prompt_template)\n\n    research_evaluation_chain = (\n        evaluation_prompt |\n        ChatOpenAI(model=model, api_key=secretstr_to_v1_secretstr(api_key)) |\n        StrOutputParser()\n    )\n\n    response: str = research_evaluation_chain.invoke({\n        \"goal\": goal,\n        \"content\": content\n    }, config=get_langfuse_langchain_config())\n\n    return response\n\n\n@observe()\ndef prepare_report(goal: str, scraped: list[str], model: str, temperature: float, api_key: SecretStr | None = None) -> str:\n    if api_key == None:\n        api_key = APIKeys().openai_api_key\n        \n    evaluation_prompt_template = \"\"\"\n    You are a professional researcher. Your goal is to provide a relevant information report\n    in order to make an informed prediction for the question: '{goal}'.\n    \n    Here are the results of relevant web searches:\n    \n    {search_results}\n    \n    Prepare a full comprehensive report that provides relevant information to answer the aforementioned question.\n    If that is not possible, state why.\n    You will structure your report in the following sections:\n    \n    - Introduction\n    - Background\n    - Findings and Analysis\n    - Conclusion\n    - Caveats\n    \n    Don't limit yourself to just stating each finding; provide a thorough, full and comprehensive analysis of each finding.\n    Use markdown syntax. Include as much relevant information as possible and try not to summarize.\n    \"\"\"\n    evaluation_prompt = ChatPromptTemplate.from_template(template=evaluation_prompt_template)\n\n    research_evaluation_chain = (\n        evaluation_prompt |\n        ChatOpenAI(model=model, temperature=temperature, api_key=secretstr_to_v1_secretstr(api_key)) |\n        StrOutputParser()\n    )\n\n    response: str = research_evaluation_chain.invoke({\n        \"search_results\": scraped,\n        \"goal\": goal\n    }, config=get_langfuse_langchain_config())\n\n    return response"}
{"type": "source_file", "path": "prediction_prophet/functions/rerank_subqueries.py", "content": "import os\nfrom langchain_openai import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.schema.output_parser import StrOutputParser\nfrom pydantic.types import SecretStr\nfrom prediction_market_agent_tooling.config import APIKeys\nfrom prediction_market_agent_tooling.gtypes import secretstr_to_v1_secretstr\nfrom prediction_market_agent_tooling.tools.langfuse_ import get_langfuse_langchain_config, observe\n\nrerank_queries_template = \"\"\"\nI will present you with a list of queries to search the web for, for answers to the question: {goal}.\n\nThe queries are divided by '---query---'\n\nEvaluate the queries in order that will provide the best data to answer the question. Do not modify the queries.\nReturn them, in order of relevance, as a comma separated list of strings.\n\nQueries: {queries}\n\"\"\"\n@observe()\ndef rerank_subqueries(queries: list[str], goal: str, model: str, temperature: float, api_key: SecretStr | None = None) -> list[str]:\n    if api_key == None:\n        api_key = APIKeys().openai_api_key\n            \n    rerank_results_prompt = ChatPromptTemplate.from_template(template=rerank_queries_template)\n\n    rerank_results_chain = (\n        rerank_results_prompt |\n        ChatOpenAI(model=model, temperature=temperature, api_key=secretstr_to_v1_secretstr(api_key)) |\n        StrOutputParser()\n    )\n\n    responses: str = rerank_results_chain.invoke({\n        \"goal\": goal,\n        \"queries\": \"\\n---query---\\n\".join(queries)\n    }, config=get_langfuse_langchain_config())\n\n    return responses.split(\",\")"}
{"type": "source_file", "path": "prediction_prophet/functions/debate_prediction.py", "content": "import datetime\nimport json\nfrom autogen import ConversableAgent\nfrom prediction_market_agent_tooling.benchmark.utils import (\n    Prediction,\n)\nfrom prediction_prophet.benchmark.agents import completion_prediction_json_to_pydantic_model\nfrom pydantic import SecretStr\nfrom langchain.schema.output_parser import StrOutputParser\nfrom langchain_openai import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom prediction_market_agent_tooling.config import APIKeys\nfrom prediction_market_agent_tooling.gtypes import secretstr_to_v1_secretstr\n\n\nPREDICTION_PROMPT = \"\"\"\nYour task is to determine the probability of a prediction market question being answered 'Yes' or 'No'.\nUse the question provided in 'USER_PROMPT' and follow these guidelines:\n* Focus on the question inside double quotes in 'USER_PROMPT'.\n* The question must have only 'Yes' or 'No' outcomes. If not, respond with \"Error\".\n* Use 'ADDITIONAL_INFORMATION' from a recent Google search for your estimation.\n* Consider the market's closing date for your prediction. If the event hasn't happened by this date, the outcome is 'No'; otherwise, it's 'Yes'.\n* Your estimation must be as accurate as possible to avoid financial losses.\n* Evaluate recent information more heavily than older information.\n* The closer the current time ({timestamp}) is to the closing date without clear evidence of the event happening, the more likely the outcome is 'No'.\n* Your response should include:\n    - \"decision\": The decision you made. Either `y` (for `Yes`) or `n` (for `No`).\n    - \"p_yes\": Probability that the market question's outcome will be `Yes`. Ranging from 0 (lowest probability) to 1 (maximum probability).\n    - \"p_no\": Probability that the market questions outcome will be `No`. Ranging from 0 (lowest probability) to 1 (maximum probability).\n    - \"confidence\": Indicating the confidence in the estimated probabilities you provided ranging from 0 (lowest confidence) to 1 (maximum confidence). Confidence can be calculated based on the quality and quantity of data used for the estimation.\n    - \"info_utility\": Utility of the information provided in \"ADDITIONAL_INFORMATION\" to help you make the probability estimation ranging from 0 (lowest utility) to 1 (maximum utility).\n    \n    Ensure p_yes + p_no equals 1.\nUSER_PROMPT: {user_prompt}\nADDITIONAL_INFORMATION:\n```\n{additional_information}\n```\nLet's think through this step by step.\n\"\"\"\n\nDEBATE_PREDICTION = \"\"\"\nFor the following question: {user_prompt}; and considering the current time: {timestamp}\n\nGiven the following information:\n\n{additional_information}\n\nI made the following prediction:\n\n```\n{prediction_0}\n```\n\nAnd you made the following prediction:\n\n```\n{prediction_1}\n```\n\nDebate my prediction, considering your own prediction. Be brief, strong and critical to defend your position.\nUltimately, our objective is to reach consensus.\n\"\"\"\n\n\nEXTRACTION_PROMPT = \"\"\"\nYou will be given information. From it, extract the JSON with the following content:\n   - \"decision\": The decision you made. Either `y` (for `Yes`) or `n` (for `No`).\n   - \"p_yes\": Probability that the market question's outcome will be `Yes`. Ranging from 0 (lowest probability) to 1 (maximum probability).\n   - \"p_no\": Probability that the market questions outcome will be `No`. Ranging from 0 (lowest probability) to 1 (maximum probability).\n   - \"confidence\": Indicating the confidence in the estimated probabilities you provided ranging from 0 (lowest confidence) to 1 (maximum confidence). Confidence can be calculated based on the quality and quantity of data used for the estimation.\n   - \"info_utility\": Utility of the information provided in \"ADDITIONAL_INFORMATION\" to help you make the probability estimation ranging from 0 (lowest utility) to 1 (maximum utility).\n    \nReturn only the JSON and include nothing more in your response.\n\nInformation: {prediction_summary}\n\"\"\"\n\nPREDICTOR_SYSTEM_PROMPT = \"\"\"\nYou are a critical and strong debater, information analyzer and future events predictor.\n\nYou will debate other agents's predictions. You can update your prediction if other agents\ngive you convincing arguments. Nonetheless, be strong in your position and argument back to defend your prediction.\n\"\"\"\n    \ndef make_debated_prediction(prompt: str, additional_information: str, api_key: SecretStr | None = None) -> Prediction:\n    if api_key == None:\n        api_key = APIKeys().openai_api_key\n        \n    formatted_time_utc = datetime.datetime.now(datetime.timezone.utc).isoformat(timespec='seconds') + \"Z\"\n    \n    prediction_prompt = ChatPromptTemplate.from_template(template=PREDICTION_PROMPT)\n\n    prediction_chain = (\n        prediction_prompt |\n        ChatOpenAI(model=\"gpt-4-0125-preview\", api_key=secretstr_to_v1_secretstr(api_key)) |\n        StrOutputParser()\n    )\n\n    predictions = prediction_chain.batch([{\n        \"user_prompt\": prompt,\n        \"additional_information\": additional_information,\n        \"timestamp\": formatted_time_utc,\n    } for _ in range(2)])\n    \n    agents = [\n        ConversableAgent(\n            name=f\"Predictor_Agent_{i}\",\n            system_message=PREDICTION_PROMPT,\n            llm_config={\"config_list\": [{\"model\": \"gpt-4-0125-preview\", \"api_key\": api_key.get_secret_value()}]},\n            human_input_mode=\"NEVER\")\n        for i in range(2) ]\n    \n    chat_result = agents[0].initiate_chat(\n        agents[1],\n        message=DEBATE_PREDICTION.format(\n            user_prompt=prompt,\n            additional_information=additional_information,\n            timestamp=formatted_time_utc,\n            prediction_0=predictions[0],\n            prediction_1=predictions[1],\n        ),\n        summary_method=\"reflection_with_llm\",\n        max_turns=3,\n    )\n            \n    extraction_prompt = ChatPromptTemplate.from_template(template=EXTRACTION_PROMPT)\n\n    extraction_chain = (\n        extraction_prompt |\n        ChatOpenAI(model=\"gpt-3.5-turbo-0125\", api_key=secretstr_to_v1_secretstr(api_key)) |\n        StrOutputParser()\n    )\n\n    result = extraction_chain.invoke({\n        \"prediction_summary\": chat_result.summary\n    })\n    \n    return completion_prediction_json_to_pydantic_model(json.loads(result))"}
{"type": "source_file", "path": "prediction_prophet/main.py", "content": "import logging\nfrom typing import cast\nimport click\nimport time\nfrom dotenv import load_dotenv\nfrom prediction_prophet.functions.debate_prediction import make_debated_prediction\nfrom langchain_community.callbacks import get_openai_callback\nfrom prediction_prophet.functions.research import research as prophet_research\nfrom prediction_market_agent_tooling.benchmark.utils import (\n    OutcomePrediction\n)\n\nload_dotenv()\nlogging.basicConfig(level=logging.WARNING, format='%(asctime)s - %(message)s')\n\ndef create_output_file(info: str, path: str) -> None:\n    with open(path, 'w') as file:\n        file.write(info)\n\ndef read_text_file(file_path: str) -> str:\n    try:\n        with open(file_path, 'r') as file:\n            return file.read()\n    except FileNotFoundError:\n        return \"File not found.\"\n    except Exception as e:\n        return f\"An error occurred: {e}\"\n\n@click.group()\ndef cli() -> None:\n    pass\n\n@cli.command()\n@click.argument('prompt')\n@click.option('--file', '-f', default=None)\ndef research(\n    prompt: str,\n    file: str | None = None\n) -> None:\n    start = time.time()\n    \n    with get_openai_callback() as cb:\n      research = prophet_research(goal=prompt, use_summaries=False, model=\"gpt-4-0125-preview\")\n    \n    report = research.report\n    end = time.time()\n    \n    if file:\n        create_output_file(report, file)\n        print(f\"Output saved to '{file}'\")\n        print(f\"\\n\\nTime elapsed: {end - start}\\n\\n{cb}\\n\\n\")\n        return\n    \n    print(f\"Research results:\\n\\n{report}\")\n    print(f\"\\n\\nTime elapsed: {end - start}\\n\\n{cb}\\n\\n\")\n\n\n@cli.command()\n@click.argument('prompt')\n@click.option('--path', '-p', default=None)\ndef predict(prompt: str, path: str | None = None) -> None:\n    start = time.time()\n\n    with get_openai_callback() as cb:\n        if path:\n            report = read_text_file(path)\n        else:\n            logger = logging.getLogger(\"research\")\n            logger.setLevel(logging.INFO)\n            report = prophet_research(goal=prompt, model=\"gpt-4-0125-preview\", use_summaries=False, logger=logger).report\n        \n        prediction = make_debated_prediction(prompt=prompt, additional_information=report)\n\n    end = time.time()\n    \n    outcome_prediction = prediction.outcome_prediction\n    if outcome_prediction == None:\n        raise ValueError(\"The agent failed to generate a prediction\")\n    \n    outcome_prediction = cast(OutcomePrediction, prediction.outcome_prediction)\n    \n    print(f\"\\n\\nQuestion: '{prompt}'\\nProbability of ocurring: {outcome_prediction.p_yes * 100}%\\nConfidence in prediction: {outcome_prediction.confidence * 100}%\\nTime elapsed: {end - start}\")\n\n\nif __name__ == '__main__':\n    cli()"}
{"type": "source_file", "path": "prediction_prophet/functions/web_scrape.py", "content": "import logging\nfrom markdownify import markdownify\nimport requests\nfrom bs4 import BeautifulSoup\nfrom requests import Response\nimport tenacity\nfrom datetime import timedelta\nfrom prediction_market_agent_tooling.tools.caches.db_cache import db_cache\n\n\n@tenacity.retry(stop=tenacity.stop_after_attempt(3), wait=tenacity.wait_fixed(1), reraise=True)\ndef fetch_html(url: str, timeout: int) -> Response:\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:107.0) Gecko/20100101 Firefox/107.0\"\n    }\n    response = requests.get(url, headers=headers, timeout=timeout)\n    return response\n\n@db_cache(max_age=timedelta(days=1), ignore_args=[\"timeout\"])\ndef web_scrape_strict(url: str, timeout: int = 10) -> str:\n    response = fetch_html(url=url, timeout=timeout)\n\n    if 'text/html' in response.headers.get('Content-Type', ''):\n        soup = BeautifulSoup(response.content, \"html.parser\")\n        \n        [x.extract() for x in soup.findAll('script')]\n        [x.extract() for x in soup.findAll('style')]\n        [x.extract() for x in soup.findAll('noscript')]\n        [x.extract() for x in soup.findAll('link')]\n        [x.extract() for x in soup.findAll('head')]\n        [x.extract() for x in soup.findAll('image')]\n        [x.extract() for x in soup.findAll('img')]\n        \n        text: str = soup.get_text()\n        text = markdownify(text)\n        text = \"  \".join([x.strip() for x in text.split(\"\\n\")])\n        text = \" \".join([x.strip() for x in text.split(\"  \")])\n        \n        return text\n    else:\n        print(\"Non-HTML content received\")\n        logging.warning(\"Non-HTML content received\")\n        return \"\"\n\ndef web_scrape(url: str, timeout: int = 10) -> str:\n    \"\"\"\n    Do not throw if the HTTP request fails.\n    \"\"\"\n    try:\n        return web_scrape_strict(url=url, timeout=timeout)\n    except requests.RequestException as e:\n        print(f\"HTTP request failed: {e}\")\n        logging.warning(f\"HTTP request failed: {e}\")\n        return \"\"\n"}
{"type": "source_file", "path": "prediction_prophet/functions/web_search.py", "content": "from pydantic.types import SecretStr\n\nfrom prediction_prophet.models.WebSearchResult import WebSearchResult\nfrom prediction_market_agent_tooling.config import APIKeys\nfrom prediction_market_agent_tooling.tools.tavily.tavily_search import tavily_search\n\n\n\ndef web_search(query: str, max_results: int = 5, tavily_api_key: SecretStr | None = None) -> list[WebSearchResult]:\n    response = tavily_search(\n        query=query,\n        search_depth=\"advanced\",\n        max_results=max_results,\n        include_raw_content=True,\n        api_keys=APIKeys(TAVILY_API_KEY=tavily_api_key) if tavily_api_key else None,\n    )\n\n    transformed_results = [\n        WebSearchResult(\n            title=result.title,\n            url=result.url,\n            description=result.content,\n            raw_content=result.raw_content,\n            relevancy=result.score,\n            query=query,\n        )\n        for result in response.results\n    ]\n\n    return transformed_results\n"}
{"type": "source_file", "path": "scripts/compare_search_results.py", "content": "import json\nimport typer\nimport typing as t\nimport itertools as it\nimport pandas as pd\nfrom tqdm import tqdm\nfrom urllib.parse import urlparse\nfrom collections import defaultdict\nfrom prediction_market_agent_tooling.markets.markets import get_binary_markets, MarketType\nfrom prediction_prophet.functions.web_search import web_search\nfrom prediction_prophet.autonolas.research import safe_get_urls_from_query\n\nENGINES: dict[str, t.Callable[[str, int], list[str]]] = {\n    \"tavily\": lambda q, limit: [x.url for x in web_search(q, max_results=limit)],\n    \"google\": lambda q, limit: safe_get_urls_from_query(q, num=limit)\n}\n\n\ndef main(\n    n: int = 10,\n    source: MarketType = MarketType.MANIFOLD,\n    max_results_per_query: int = 10,\n    output: str = \"results\",\n) -> None:\n    markets = get_binary_markets(n, source)\n\n    summary = defaultdict(list)\n    links: dict[str, dict[str, list[str]]] = defaultdict(dict)\n\n    for market in tqdm(markets):\n        results = {\n            engine: func(market.question, max_results_per_query)\n            for engine, func in ENGINES.items()\n        }\n\n        summary[\"question\"].append(market.question)\n\n        for a, b in it.combinations(ENGINES, 2):\n            if a != b:\n                summary[f\"common links in {a} and {b}\"].append(len(set(results[a]) & set(results[b])) / max(len(results[a]), len(results[b])))\n                summary[f\"common domains in {a} and {b}\"].append(len(set(map(extract_domain_from_url, results[a])) & set(map(extract_domain_from_url, results[b]))) / max(len(results[a]), len(results[b])))\n\n        for engine, urls in results.items():\n            links[market.question][engine] = urls\n    \n    pd.DataFrame(summary).to_csv(f\"{output}.csv\", index=False)\n    with open(f\"{output}.json\", \"w\") as f:\n        json.dump(links, f, indent=4)\n\n\ndef extract_domain_from_url(url: str) -> str:\n    if not url.startswith(\"http\"):\n        url = f\"http://{url}\"\n    return urlparse(url).netloc.replace(\"www.\", \"\")\n\n\nif __name__ == \"__main__\":\n    typer.run(main)\n"}
{"type": "source_file", "path": "scripts/compare_manual_scrap_vs_tavily_raw_content.py", "content": "import streamlit as st\nfrom dotenv import load_dotenv\nfrom prediction_prophet.functions.web_search import web_search\nfrom prediction_prophet.functions.scrape_results import scrape_results\n\nload_dotenv()\nst.set_page_config(layout=\"wide\")\n\nquery = st.text_input(\"Enter a query\")\n\nif not query:\n    st.warning(\"Please enter a query\")\n    st.stop()\n\nsearch = web_search(query)\nscrape = scrape_results(search)\n\nindex = int(st.number_input(\"Index\", min_value=0, max_value=len(search) - 1, value=0))\n\ncol1, col2 = st.columns(2)\n\nwith col1:\n    st.markdown(\"### Tavily's raw content\")\n    st.write(search[index].url)\n    st.write(f\"{len((search[index].raw_content or '').split())} words\")\n    st.markdown(\"---\")\n    st.write(search[index].raw_content)\n\nwith col2:\n    st.markdown(\"### Scraped content\")\n    st.write(scrape[index].url)\n    st.write(f\"{len(scrape[index].content.split())} words\")\n    st.markdown(\"---\")\n    st.write(scrape[index].content)\n"}
{"type": "source_file", "path": "prediction_prophet/functions/__init__.py", "content": ""}
{"type": "source_file", "path": "prediction_prophet/functions/utils.py", "content": "import tiktoken\nimport os\nfrom datetime import datetime, date\nfrom typing import NoReturn, Type, TypeVar, Optional\nfrom googleapiclient.discovery import build\nfrom prediction_market_agent_tooling.tools.caches.db_cache import db_cache\n\nT = TypeVar(\"T\")\n\n\ndef check_not_none(\n    value: Optional[T],\n    msg: str = \"Value shouldn't be None.\",\n    exp: Type[ValueError] = ValueError,\n) -> T:\n    \"\"\"\n    Utility to remove optionality from a variable.\n\n    Useful for cases like this:\n\n    ```\n    keys = pma.utils.get_keys()\n    pma.omen.omen_buy_outcome_tx(\n        from_addres=check_not_none(keys.bet_from_address),  # <-- No more Optional[HexAddress], so type checker will be happy.\n        ...,\n    )\n    ```\n    \"\"\"\n    if value is None:\n        should_not_happen(msg=msg, exp=exp)\n    return value\n\n\ndef should_not_happen(\n    msg: str = \"Should not happen.\", exp: Type[ValueError] = ValueError\n) -> NoReturn:\n    \"\"\"\n    Utility function to raise an exception with a message.\n\n    Handy for cases like this:\n\n    ```\n    return (\n        1 if variable == X\n        else 2 if variable == Y\n        else 3 if variable == Z\n        else should_not_happen(f\"Variable {variable} is uknown.\")\n    )\n    ```\n\n    To prevent silent bugs with useful error message.\n    \"\"\"\n    raise exp(msg)\n\n    \ndef trim_to_n_tokens(content: str, n: int, model: str) -> str:\n    encoder = tiktoken.encoding_for_model(model)\n    return encoder.decode(encoder.encode(content)[:n])\n\n\n@db_cache\ndef url_is_older_than(url: str, older_than: date) -> bool:\n    service = build(\"customsearch\", \"v1\", developerKey=os.environ[\"GOOGLE_SEARCH_API_KEY\"])\n    date_restrict = f\"d{(datetime.now().date() - older_than).days}\"  # {d,w,m,y}N to restrict the search to the last N days, weeks, months or years.\n\n    search = (\n        service\n        .cse()\n        .list(\n            # Possible options: https://developers.google.com/custom-search/v1/reference/rest/v1/cse/list\n            q=url,\n            cx=os.environ[\"GOOGLE_SEARCH_ENGINE_ID\"],\n            num=1,\n            dateRestrict=date_restrict,  \n            # We can also restrict the search to a specific date range, but it seems we can not have restricted date range + relevance sorting, so that is not useful for us.\n            # sort=\"date:r:20000101:20230101\",  #  \"YYYYMMDD:YYYYMMDD\"\n        )\n        .execute()\n    )\n    return True if int(search[\"searchInformation\"][\"totalResults\"]) == 0 or not any(url in item[\"link\"] for item in search[\"items\"]) else False\n\n\ndef time_restrict_urls(urls: list[str], time_restriction_up_to: date) -> list[str]:\n    restricted_urls: list[str] = []\n    for url in urls:\n        if url_is_older_than(url, time_restriction_up_to):\n            restricted_urls.append(url)\n    return restricted_urls\n"}
{"type": "source_file", "path": "prediction_prophet/functions/create_embeddings_from_results.py", "content": "import logging\n\ntry:\n    __import__('pysqlite3')\n    import sys\n    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\nexcept ImportError:\n    logging.info(\"pysqlite3-binary not found, using sqlite3 instead.\")\n\nimport os\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain.vectorstores.chroma import Chroma\nfrom prediction_prophet.models.WebScrapeResult import WebScrapeResult\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom pydantic.types import SecretStr\nfrom prediction_market_agent_tooling.config import APIKeys\nfrom prediction_market_agent_tooling.gtypes import secretstr_to_v1_secretstr\n\n\ndef create_embeddings_from_results(results: list[WebScrapeResult], text_splitter: RecursiveCharacterTextSplitter, api_key: SecretStr | None = None) -> Chroma:\n    if api_key == None:\n        api_key = APIKeys().openai_api_key\n    \n    collection = Chroma(embedding_function=OpenAIEmbeddings(api_key=secretstr_to_v1_secretstr(api_key)))\n    texts = []\n    metadatas = []\n\n    for scrape_result in results:\n        text_splits = text_splitter.split_text(scrape_result.content)\n        texts += text_splits\n        metadatas += [scrape_result.dict() for _ in text_splits]\n\n    collection.add_texts(\n        texts=texts,\n        metadatas=metadatas\n    )\n    return collection\n"}
{"type": "source_file", "path": "scripts/measure_memory.py", "content": "import time\nimport typer\nfrom prediction_prophet.benchmark.agents import AGENTS\n\n\ndef main(sleep: int = 120) -> None:\n    \"\"\"\n    1. Decorate functions that you want to track with `@profile`\n    2. (a) Run `mprof run --include-children scripts/measure_memory.py` and then plot it with `mprof plot`.\n    2. (b) Or, run `python -m memory_profiler scripts/measure_memory.py` for line-by-line analysis.\n    \"\"\"\n    questions = [\n        \"Will GNO hit $1000 by the end of 2024?\",\n        \"Will GNO hit $10000 by the end of 2025?\",\n        \"Will GNO hit $100000 by the end of 2026?\",\n        \"Will GNO hit $1000000 by the end of 2027?\",\n        \"Will GNO hit $10000000 by the end of 2028?\",\n        \"Will GNO hit $100000000 by the end of 2029?\",\n        \"Will GNO hit $1000000000 by the end of 2030?\",\n    ]\n\n    print(\"Sleeping so we can see initial memory usage in the plot.\")\n    time.sleep(sleep)\n\n    run(questions, sleep)\n\n\ndef run(questions: list[str], sleep: int) -> None:\n    for AgentClass in AGENTS:\n        agent = AgentClass(\n            model=\"gpt-3.5-turbo-0125\",  # Just some cheap model, no need for good results here.\n        )\n\n        for question in questions:\n            agent.predict(question)\n\n        print(\"Sleeping so we can differ it in the plot.\")\n        time.sleep(sleep)\n\n\nif __name__ == \"__main__\":\n    typer.run(main)\n"}
{"type": "source_file", "path": "prediction_prophet/functions/generate_subqueries.py", "content": "import os\nfrom langchain_openai import ChatOpenAI\nfrom langchain.output_parsers import CommaSeparatedListOutputParser\nfrom langchain.prompts import ChatPromptTemplate\nfrom pydantic.types import SecretStr\nfrom prediction_market_agent_tooling.config import APIKeys\nfrom prediction_market_agent_tooling.gtypes import secretstr_to_v1_secretstr\nfrom prediction_market_agent_tooling.tools.langfuse_ import get_langfuse_langchain_config, observe\n\n\nsubquery_generation_template = \"\"\"\nYou are a professional researcher. Your goal is to prepare a research plan for {query}.\n\nThe plan will consist of multiple web searches separated by commas.\nReturn ONLY the web searches, separated by commas and without quotes.\n\nLimit your searches to {search_limit}.\n\"\"\"\n@observe()\ndef generate_subqueries(query: str, limit: int, model: str, temperature: float, api_key: SecretStr | None = None) -> list[str]:\n    if limit == 0:\n        return [query]\n\n    if api_key == None:\n        api_key = APIKeys().openai_api_key\n            \n    subquery_generation_prompt = ChatPromptTemplate.from_template(template=subquery_generation_template)\n\n    subquery_generation_chain = (\n        subquery_generation_prompt |\n        ChatOpenAI(model=model, temperature=temperature, api_key=secretstr_to_v1_secretstr(api_key)) |\n        CommaSeparatedListOutputParser()\n    )\n\n    subqueries = subquery_generation_chain.invoke({\n        \"query\": query,\n        \"search_limit\": limit\n    }, config=get_langfuse_langchain_config())\n\n    return [query] + [subquery.strip('\\\"') for subquery in subqueries]"}
{"type": "source_file", "path": "prediction_prophet/models/WebScrapeResult.py", "content": "import typing as t\nfrom pydantic import BaseModel\n\nclass WebScrapeResult(BaseModel):\n    query: str\n    url: str\n    title: str\n    content: str\n\n    def __getitem__(self, item: t.Any) -> str:\n        return t.cast(str, getattr(self, item))  # Cast as all fields have str type.\n"}
{"type": "source_file", "path": "prediction_prophet/functions/summarize.py", "content": "from langchain.prompts import PromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.chains.llm import LLMChain\nfrom langchain.chains.combine_documents.reduce import ReduceDocumentsChain\nfrom langchain.chains.combine_documents.stuff import  StuffDocumentsChain\nfrom langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\ndef summarize(objective: str, content: str) -> str:\n    llm = ChatOpenAI(temperature = 0, model=\"gpt-3.5-turbo-16k-0613\")\n    \n    map_template = \"\"\"\n    The following is a set of documents\n    {docs}\n    Based on this list of docs, please provide a full comprehensive summary relevant to: {objective}\n    Helpful Answer:\n    \"\"\"\n    \n    map_prompt = PromptTemplate(template=map_template, input_variables=[\"docs\", \"objective\"])\n    map_chain = LLMChain(llm=llm, prompt=map_prompt)\n    \n    reduce_template = \"\"\"The following is set of summaries:\n    {docs}\n    Take these and distill it into a final full comprehensive, summary relevant to: {objective}. \n    Helpful Answer:\"\"\"\n    reduce_prompt = PromptTemplate(template=reduce_template, input_variables=[\"docs\", \"objective\"])\n    \n    # Run chain\n    reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n\n    # Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n    combine_documents_chain = StuffDocumentsChain(\n        llm_chain=reduce_chain, document_variable_name=\"docs\"\n    )\n\n    # Combines and iteravely reduces the mapped documents\n    reduce_documents_chain = ReduceDocumentsChain(\n        # This is final chain that is called.\n        combine_documents_chain=combine_documents_chain,\n        # If documents exceed context for `StuffDocumentsChain`\n        collapse_documents_chain=combine_documents_chain,\n        # The maximum number of tokens to group documents into.\n        token_max=8000,\n    )\n    \n    map_reduce_chain = MapReduceDocumentsChain(\n        # Map chain\n        llm_chain=map_chain,\n        # Reduce chain\n        reduce_documents_chain=reduce_documents_chain,\n        # The variable name in the llm_chain to put the documents in\n        document_variable_name=\"docs\",\n        # Return the results of the map steps in the output\n        return_intermediate_steps=False,\n        input_key=\"docs\",\n    )\n\n    text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\"], chunk_size = 10000, chunk_overlap=500)\n    docs = text_splitter.create_documents([content])\n    \n    response: str = map_reduce_chain.run(docs=docs, objective=objective)\n    \n    return response\n\n"}
{"type": "source_file", "path": "prediction_prophet/benchmark/__init__.py", "content": ""}
{"type": "source_file", "path": "prediction_prophet/functions/scrape_results.py", "content": "from prediction_prophet.models.WebScrapeResult import WebScrapeResult\nfrom prediction_prophet.functions.web_search import WebSearchResult\nfrom prediction_prophet.functions.web_scrape import web_scrape\nfrom prediction_prophet.functions.parallelism import par_map\n\n\ndef scrape_results(results: list[WebSearchResult]) -> list[WebScrapeResult]:\n    scraped: list[WebScrapeResult] = par_map(results, lambda result: WebScrapeResult(\n        query=result.query,\n        url=result.url,\n        title=result.title,\n        content=web_scrape(result.url),\n    ))\n\n    return scraped\n"}
{"type": "source_file", "path": "prediction_prophet/functions/research.py", "content": "import logging\nimport typing as t\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom prediction_prophet.functions.create_embeddings_from_results import create_embeddings_from_results\nfrom prediction_prophet.functions.generate_subqueries import generate_subqueries\nfrom prediction_prophet.functions.prepare_report import prepare_report, prepare_summary\nfrom prediction_prophet.models.WebScrapeResult import WebScrapeResult\nfrom prediction_prophet.functions.rerank_subqueries import rerank_subqueries\nfrom prediction_prophet.functions.scrape_results import scrape_results\nfrom prediction_prophet.functions.search import search\nfrom pydantic.types import SecretStr\nfrom pydantic import BaseModel\nfrom prediction_market_agent_tooling.tools.langfuse_ import observe\n\nif t.TYPE_CHECKING:\n    from loguru import Logger\n\n\nclass Research(BaseModel):\n    report: str\n    all_queries: list[str]\n    reranked_queries: list[str]\n    websites_to_scrape: list[str]\n    websites_scraped: list[WebScrapeResult]\n\n\nclass NoResulsFoundError(ValueError):\n    pass\n\n\nclass NotEnoughScrapedSitesError(ValueError):\n    pass\n\n\n@observe()\ndef research(\n    goal: str,\n    use_summaries: bool,\n    model: str = \"gpt-4-0125-preview\",\n    temperature: float = 0.7,\n    initial_subqueries_limit: int = 20,\n    subqueries_limit: int = 4,\n    max_results_per_search: int = 5,\n    min_scraped_sites: int = 0,\n    scrape_content_split_chunk_size: int = 800,\n    scrape_content_split_chunk_overlap: int = 225,\n    top_k_per_query: int = 8,\n    use_tavily_raw_content: bool = False,\n    openai_api_key: SecretStr | None = None,\n    tavily_api_key: SecretStr | None = None,\n    logger: t.Union[logging.Logger, \"Logger\"] = logging.getLogger(),\n) -> Research:\n    # Validate args\n    if min_scraped_sites > max_results_per_search * subqueries_limit:\n        raise ValueError(\n            f\"min_scraped_sites ({min_scraped_sites}) must be less than or \"\n            f\"equal to max_results_per_search ({max_results_per_search}) * \"\n            f\"subqueries_limit ({subqueries_limit}).\"\n        )\n\n    logger.info(\"Started subqueries generation\")\n    all_queries = generate_subqueries(query=goal, limit=initial_subqueries_limit, model=model, temperature=temperature, api_key=openai_api_key)\n    \n    stringified_queries = '\\n- ' + '\\n- '.join(all_queries)\n    logger.info(f\"Generated subqueries: {stringified_queries}\")\n    \n    logger.info(\"Started subqueries reranking\")\n    queries = rerank_subqueries(queries=all_queries, goal=goal, model=model, temperature=temperature, api_key=openai_api_key)[:subqueries_limit] if initial_subqueries_limit > subqueries_limit else all_queries\n\n    stringified_queries = '\\n- ' + '\\n- '.join(queries)\n    logger.info(f\"Reranked subqueries. Will use top {subqueries_limit}: {stringified_queries}\")\n    \n    logger.info(f\"Started web searching\")\n    search_results_with_queries = search(\n        queries,\n        lambda result: not result.url.startswith(\"https://www.youtube\"),\n        tavily_api_key=tavily_api_key,\n        max_results_per_search=max_results_per_search,\n    )\n\n    if not search_results_with_queries:\n        raise NoResulsFoundError(f\"No search results found for the goal {goal}.\")\n\n    scrape_args = [result for (_, result) in search_results_with_queries]\n    websites_to_scrape = set(result.url for result in scrape_args)\n    \n    stringified_websites = '\\n- ' + '\\n- '.join(websites_to_scrape)\n    logger.info(f\"Found the following relevant results: {stringified_websites}\")\n    \n    logger.info(f\"Started scraping of web results\")\n    scraped = scrape_results(scrape_args) if not use_tavily_raw_content else [WebScrapeResult(\n        query=result.query,\n        url=result.url,\n        title=result.title,\n        content=result.raw_content,\n    ) for result in scrape_args if result.raw_content]\n    scraped = [result for result in scraped if result.content != \"\"]\n\n    unique_scraped_websites = set([result.url for result in scraped])\n    if len(scraped) < min_scraped_sites:\n        # Get urls that were not scraped\n        raise NotEnoughScrapedSitesError(\n            f\"Only successfully scraped content from \"\n            f\"{len(unique_scraped_websites)} websites, out of a possible \"\n            f\"{len(websites_to_scrape)} websites, which is less than the \"\n            f\"minimum required ({min_scraped_sites}). The following websites \"\n            f\"were not scraped: {websites_to_scrape - unique_scraped_websites}\"\n        )\n\n    logger.info(f\"Scraped content from {len(scraped)} websites\")\n\n    text_splitter = RecursiveCharacterTextSplitter(\n        separators=[\"\\n\\n\", \"\\n\", \". \", \"  \"],\n        chunk_size=scrape_content_split_chunk_size,\n        chunk_overlap=scrape_content_split_chunk_overlap\n    )\n    \n    logger.info(\"Started embeddings creation\")\n    collection = create_embeddings_from_results(scraped, text_splitter, api_key=openai_api_key)\n    logger.info(\"Embeddings created\")\n\n    vector_result_texts: list[str] = []\n    url_to_content_deemed_most_useful: dict[str, str] = {}\n\n    stringified_queries = '\\n- ' + '\\n- '.join(queries)\n    logger.info(f\"Started similarity searches for: {stringified_queries}\")\n    for query in queries:\n        top_k_per_query_results = collection.similarity_search(query, k=top_k_per_query)\n        vector_result_texts += [result.page_content for result in top_k_per_query_results if result.page_content not in vector_result_texts]\n\n        for x in top_k_per_query_results:\n            # `x.metadata[\"content\"]` holds the whole url's web page, so it's ok to overwrite the value of the same url.\n            url_to_content_deemed_most_useful[x.metadata[\"url\"]] = x.metadata[\"content\"]\n    \n    stringified_urls = '\\n- ' + '\\n- '.join(url_to_content_deemed_most_useful.keys())\n    logger.info(f\"Found {len(vector_result_texts)} information chunks across the following sites: {stringified_urls}\")\n\n    if use_summaries:\n        logger.info(f\"Started summarizing information\")\n        vector_result_texts = [\n            prepare_summary(\n                goal,\n                content,\n                \"gpt-3.5-turbo-0125\",\n                api_key=openai_api_key,\n                trim_content_to_tokens=14_000,\n            )\n            for content in url_to_content_deemed_most_useful.values()\n        ]\n        logger.info(f\"Information summarized\")\n\n    logger.info(f\"Started preparing report\")\n    report = prepare_report(goal, vector_result_texts, model=model, temperature=temperature, api_key=openai_api_key)\n    logger.info(f\"Report prepared\")\n    logger.info(report)\n\n    return Research(\n        all_queries=all_queries,\n        reranked_queries=queries,\n        report=report,\n        websites_to_scrape=list(websites_to_scrape),\n        websites_scraped=scraped,\n    )\n"}
{"type": "source_file", "path": "prediction_prophet/functions/search.py", "content": "import logging\nimport typing as t\nfrom prediction_prophet.functions.web_search import WebSearchResult, web_search\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom pydantic.types import SecretStr\n\nif t.TYPE_CHECKING:\n    from loguru import Logger\n\n\ndef safe_web_search(\n    query: str, \n    max_results: int = 5, \n    tavily_api_key: SecretStr | None = None, \n    logger: t.Union[logging.Logger, \"Logger\"] = logging.getLogger(),\n) -> t.Optional[list[WebSearchResult]]:\n    try:\n        return web_search(query, max_results, tavily_api_key)\n    except Exception as e:\n        logger.warning(f\"Error when searching for `{query}` in web_search: {e}\")\n        return None\n\n\ndef search(\n    queries: list[str],\n    filter: t.Callable[[WebSearchResult], bool] = lambda x: True,\n    tavily_api_key: SecretStr | None = None,\n    max_results_per_search: int = 5,\n    logger: t.Union[logging.Logger, \"Logger\"] = logging.getLogger(),\n) -> list[tuple[str, WebSearchResult]]:\n    maybe_results: list[t.Optional[list[WebSearchResult]]] = []\n\n    # Each result will have a query associated with it\n    # We only want to keep the results that are unique\n    with ThreadPoolExecutor(max_workers=5) as executor:\n        futures = {executor.submit(safe_web_search, query, max_results_per_search, tavily_api_key) for query in queries}\n        for future in as_completed(futures):\n            maybe_results.append(future.result())\n\n    results = [result for result in maybe_results if result is not None]\n    if len(results) != len(maybe_results):\n        logger.warning(f\"{len(maybe_results) - len(results)} queries out of {len(maybe_results)} failed to return results.\")\n\n    results_with_queries: list[tuple[str, WebSearchResult]] = []\n\n    for i in range(len(results)):\n        for result in results[i]:\n            if result.url not in [existing_result.url for (_, existing_result) in results_with_queries]:\n                if filter(result):\n                  results_with_queries.append((queries[i], result))\n\n    return results_with_queries"}
{"type": "source_file", "path": "scripts/benchmark.py", "content": "import typing as t\n\nimport typer\nfrom prediction_market_agent_tooling.benchmark.agents import FixedAgent, RandomAgent\nfrom prediction_market_agent_tooling.benchmark.benchmark import Benchmarker\nfrom prediction_market_agent_tooling.markets.markets import MarketType, get_binary_markets, FilterBy, SortBy\n\nfrom prediction_prophet.autonolas.research import EmbeddingModel\nfrom prediction_prophet.benchmark.agents import PredictionProphetAgent, OlasAgent, QuestionOnlyAgent\nfrom prediction_prophet.functions.cache import ENABLE_CACHE\n\n\ndef main(\n    n: int = 10,\n    output: str = \"./benchmark_report.md\",\n    reference: MarketType = MarketType.MANIFOLD,\n    filter: FilterBy = FilterBy.OPEN,\n    sort: SortBy = SortBy.NONE,\n    max_workers: int = 1,\n    cache_path: t.Optional[str] = \"predictions_cache.json\",\n    only_cached: bool = False,\n) -> None:\n    \"\"\"\n    Polymarket usually contains higher quality questions, \n    but on Manifold, additionally to filtering by MarketFilter.resolved, you can sort by MarketSort.newest.\n    \"\"\"\n    markets = get_binary_markets(n, reference, filter_by=filter, sort_by=sort)\n    markets_deduplicated = list(({m.question: m for m in markets}.values()))\n    if len(markets) != len(markets_deduplicated):\n        print(\n            f\"Warning: Deduplicated markets from {len(markets)} to {len(markets_deduplicated)}.\"\n        )\n\n    print(f\"Found {len(markets_deduplicated)} markets.\")\n\n    benchmarker = Benchmarker(\n        markets=markets_deduplicated,\n        agents=[\n            RandomAgent(agent_name=\"random\", max_workers=max_workers),\n            FixedAgent(\n                fixed_answer=False, agent_name=\"fixed-no\", max_workers=max_workers\n            ),\n            FixedAgent(\n                fixed_answer=True, agent_name=\"fixed-yes\", max_workers=max_workers\n            ),\n            QuestionOnlyAgent(\n                model=\"gpt-3.5-turbo-0125\",\n                agent_name=\"question-only_gpt-3.5-turbo-0125\",\n                max_workers=max_workers,\n            ),\n            OlasAgent(\n                model=\"gpt-3.5-turbo-0125\",\n                max_workers=max_workers,\n                agent_name=\"olas_gpt-3.5-turbo-0125\",\n            ),\n            OlasAgent(\n                model=\"gpt-3.5-turbo-0125\",\n                max_workers=max_workers,\n                agent_name=\"olas_gpt-3.5-turbo-0125_openai-embeddings\",\n                embedding_model=EmbeddingModel.openai,\n            ),\n            PredictionProphetAgent(\n                model=\"gpt-3.5-turbo-0125\",\n                max_workers=max_workers,\n                agent_name=\"prediction_prophet_gpt-3.5-turbo-0125_summary\",\n                use_summaries=True,\n            ),\n            PredictionProphetAgent(\n                model=\"gpt-3.5-turbo-0125\",\n                max_workers=max_workers,\n                agent_name=\"prediction_prophet_gpt-3.5-turbo-0125\",\n            ),\n            PredictionProphetAgent(\n                model=\"gpt-3.5-turbo-0125\",\n                max_workers=max_workers,\n                agent_name=\"prediction_prophet_gpt-3.5-turbo-0125_summary_tavilyrawcontent\",\n                use_summaries=True,\n                use_tavily_raw_content=True,\n            ),\n            PredictionProphetAgent(\n                model=\"gpt-3.5-turbo-0125\",\n                max_workers=max_workers,\n                agent_name=\"prediction_prophet_gpt-3.5-turbo-0125_tavilyrawcontent\",\n                use_tavily_raw_content=True,\n            ),\n            # PredictionProphetAgent(model=\"gpt-4-0125-preview\", max_workers=max_workers, agent_name=\"prediction_prophet_gpt-4-0125-preview\"),  # Too expensive to be enabled by default.\n        ],\n        cache_path=cache_path,\n        only_cached=only_cached,\n    )\n\n    benchmarker.run_agents(\n        enable_timing=not ENABLE_CACHE\n    )  # Caching of search etc. can distort timings\n    md = benchmarker.generate_markdown_report()\n\n    with open(output, \"w\") as f:\n        print(f\"Writing benchmark report to: {output}\")\n        f.write(md)\n\n\nif __name__ == \"__main__\":\n    typer.run(main)\n"}
{"type": "source_file", "path": "prediction_prophet/functions/parallelism.py", "content": "import os\nimport concurrent\nfrom typing import Callable, TypeVar\nfrom concurrent.futures import Executor\nfrom concurrent.futures.thread import ThreadPoolExecutor\n\nTHREADPOOL = ThreadPoolExecutor(int(os.getenv(\"THREADPOOL_N_THREADS\", 5)))\n\nA = TypeVar(\"A\")\nB = TypeVar(\"B\")\n\ndef par_map(\n    items: list[A], func: Callable[[A], B], executor: Executor = THREADPOOL\n) -> \"list[B]\":\n    \"\"\"Applies the function to each element using the specified executor. Awaits for all results.\n    If executor is ProcessPoolExecutor, make sure the function passed is pickable, e.g. no lambda functions\n    \"\"\"\n    futures: list[concurrent.futures._base.Future[B]] = [\n        executor.submit(func, item) for item in items\n    ]\n    results = []\n    for fut in futures:\n        results.append(fut.result())\n    return results\n"}
{"type": "source_file", "path": "prediction_prophet/autonolas/__init__.py", "content": ""}
{"type": "source_file", "path": "prediction_prophet/app.py", "content": "import time\nfrom typing import cast\nfrom prediction_prophet.benchmark.agents import _make_prediction\nfrom prediction_prophet.functions.is_predictable_and_binary import is_predictable_and_binary\nfrom prediction_market_agent_tooling.benchmark.utils import (\n    OutcomePrediction\n)\nfrom pydantic import SecretStr\nimport streamlit as st\nfrom prediction_market_agent_tooling.tools.utils import secret_str_from_env\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom prediction_prophet.functions.create_embeddings_from_results import create_embeddings_from_results\nfrom prediction_prophet.functions.generate_subqueries import generate_subqueries\nfrom prediction_prophet.functions.prepare_report import prepare_report\nfrom prediction_prophet.functions.rerank_subqueries import rerank_subqueries\nfrom prediction_prophet.functions.scrape_results import scrape_results\nfrom prediction_prophet.functions.search import search\n\ndef research(\n    goal: str,\n    tavily_api_key: SecretStr,\n    model: str = \"gpt-4-0125-preview\",\n    temperature: float = 0.7, \n    initial_subqueries_limit: int = 20,\n    subqueries_limit: int = 4,\n    scrape_content_split_chunk_size: int = 800,\n    scrape_content_split_chunk_overlap: int = 225,\n    top_k_per_query: int = 8\n) -> str:\n    with st.status(\"Generating subqueries\"):\n        queries = generate_subqueries(query=goal, limit=initial_subqueries_limit, model=model, temperature=temperature)\n    \n        stringified_queries = '\\n- ' + '\\n- '.join(queries)\n        st.write(f\"Generated subqueries: {stringified_queries}\")\n        \n    with st.status(\"Reranking subqueries\"):\n        queries = rerank_subqueries(queries=queries, goal=goal, model=model, temperature=temperature)[:subqueries_limit] if initial_subqueries_limit > subqueries_limit else queries\n\n        stringified_queries = '\\n- ' + '\\n- '.join(queries)\n        st.write(f\"Reranked subqueries. Will use top {subqueries_limit}: {stringified_queries}\")\n    \n    with st.status(\"Searching the web\"):\n        search_results_with_queries = search(\n            queries, \n            lambda result: not result.url.startswith(\"https://www.youtube\"),\n            tavily_api_key=tavily_api_key\n        )\n\n        if not search_results_with_queries:\n            raise ValueError(f\"No search results found for the goal {goal}.\")\n\n        scrape_args = [result for (_, result) in search_results_with_queries]\n        websites_to_scrape = set([result.url for result in scrape_args])\n        \n        stringified_websites = '\\n- ' + '\\n- '.join(websites_to_scrape)\n        st.write(f\"Found the following relevant results: {stringified_websites}\")\n    \n    with st.status(f\"Scraping web results\"):\n        scraped = scrape_results(scrape_args)\n        scraped = [result for result in scraped if result.content != \"\"]\n        \n        st.write(f\"Scraped content from {len(scraped)} websites\")\n\n    text_splitter = RecursiveCharacterTextSplitter(\n        separators=[\"\\n\\n\", \"\\n\", \". \", \"  \"],\n        chunk_size=scrape_content_split_chunk_size,\n        chunk_overlap=scrape_content_split_chunk_overlap\n    )\n    \n    with st.status(f\"Performing similarity searches\"):\n        collection = create_embeddings_from_results(scraped, text_splitter)\n        st.write(\"Created embeddings\")\n\n        vector_result_texts: list[str] = []\n        url_to_content_deemed_most_useful: dict[str, str] = {}\n        \n        for query in queries:\n            top_k_per_query_results = collection.similarity_search(query, k=top_k_per_query)\n            vector_result_texts += [result.page_content for result in top_k_per_query_results if result.page_content not in vector_result_texts]\n\n            for x in top_k_per_query_results:\n                url_to_content_deemed_most_useful[x.metadata[\"url\"]] = x.metadata[\"content\"]\n        \n            st.write(f\"Similarity searched for: {query}\")\n\n        st.write(f\"Found {len(vector_result_texts)} relevant information chunks\")\n\n    with st.status(f\"Preparing report\"):\n        report = prepare_report(goal, vector_result_texts, model=model, temperature=temperature)\n        st.markdown(report)\n\n    return report\n\ntavily_api_key = secret_str_from_env('TAVILY_API_KEY')\n\nif tavily_api_key == None:\n    st.container().error(\"No Tavily API Key provided\")\n    st.stop()\n\nst.set_page_config(layout=\"wide\")\nst.title(\"Prediction Prophet\")\nst.write('Ask any yes-or-no question about a future outcome')\n\nwith st.sidebar:\n    st.title('Prediction Prophet')\n    st.markdown(\"A web3 agent by [Agentcoin](https://www.agentcoin.tv/)\")\n    st.image('https://raw.githubusercontent.com/agentcoinorg/predictionprophet/main/docs/imgs/banner.png')\n    \n    st.markdown('#')\n    st.markdown('#')\n    st.markdown('#')\n    st.markdown('#')\n    st.markdown('#')\n    st.markdown('-------')\n    st.caption('View the source code on our [github](https://github.com/agentcoinorg/predictionprophet)')\n    st.caption('Learn more on our [substack](https://www.agentcoin.tv/blog/prediction-prophet)')\n    st.caption('Join our [discord](https://discord.com/invite/6gk85fetcT)')\n    \n\n# TODO: find a better way to clear the history\nprogress_placeholder = st.empty()\n\nif question := st.chat_input(placeholder='Will Twitter implement a new misinformation policy before the end of 2024?'):\n    progress_placeholder.empty()\n    time.sleep(0.1) # https://github.com/streamlit/streamlit/issues/5044\n    \n    with progress_placeholder.container():\n        st.chat_message(\"user\").write(question)\n        \n        with st.chat_message(\"assistant\"):\n            st.write(f\"I will evaluate the probability of '{question}' occurring\")\n            \n            with st.status(\"Evaluating question\") as status:\n                (is_predictable, reasoning) = is_predictable_and_binary(question=question) \n                if not is_predictable:\n                    st.container().error(f\"The agent thinks this question is not predictable: \\n\\n{reasoning}\")\n                    status.update(label=\"Error evaluating question\", state=\"error\", expanded=True)\n                    st.stop()\n            \n            report = research(\n                goal=question,\n                subqueries_limit=6,\n                top_k_per_query=15,\n                tavily_api_key=cast(SecretStr, tavily_api_key),\n            )\n                    \n            with st.status(\"Making prediction\"):\n                prediction = _make_prediction(market_question=question, additional_information=report, engine=\"gpt-4-0125-preview\", temperature=0.0)\n\n                if prediction.outcome_prediction == None:\n                    st.container().error(\"The agent failed to generate a prediction\")\n                    st.stop()\n                \n                outcome_prediction = cast(OutcomePrediction, prediction.outcome_prediction)\n            \n                st.write(f\"Probability: {outcome_prediction.p_yes * 100}%. Confidence: {outcome_prediction.confidence * 100}%\")\n                if outcome_prediction.reasoning:\n                    st.write(f\"Reasoning: {outcome_prediction.reasoning}\")\n                if not prediction:\n                    st.container().error(\"No prediction was generated.\")\n                    st.stop()\n                    \n            st.markdown(f\"With **{outcome_prediction.confidence * 100}% confidence**, I'd say this outcome has a **{outcome_prediction.p_yes * 100}% probability** of happening\")\n\n"}
{"type": "source_file", "path": "prediction_prophet/models/WebSearchResult.py", "content": "import typing as t\nfrom pydantic import BaseModel\n\n\nclass WebSearchResult(BaseModel):\n    title: str\n    url: str\n    description: str\n    raw_content: str | None\n    relevancy: float\n    query: str\n    \n    def __getitem__(self, item: t.Any) -> t.Union[str, float]:\n        return t.cast(t.Union[str, float], getattr(self, item))  # Cast because fields on this model have either str or float.\n"}
{"type": "source_file", "path": "scripts/agent_app.py", "content": "\"\"\"\nPYTHONPATH=. streamlit run scripts/agent_app.py\n\nTip: if you specify PYTHONPATH=., streamlit will watch for the changes in all files, isntead of just this one.\n\"\"\"\nimport inspect\nimport typing as t\nimport streamlit as st\nfrom enum import Enum \nfrom prediction_market_agent_tooling.markets.markets import get_binary_markets, MarketType\nfrom prediction_market_agent_tooling.benchmark.agents import AbstractBenchmarkedAgent\nfrom prediction_prophet.benchmark.agents import AGENTS\n\nSENTINTEL = object()\n\nst.set_page_config(layout=\"wide\")\n\nst.title(\"Agent's decision-making process\")\n\n# Fetch markets from the selected market source.\nmarket_source = MarketType(st.selectbox(\"Select a market source\", [market_source.value for market_source in MarketType]))\nmarkets = get_binary_markets(42, market_source)\n\n# Select an agent from the list of available agents.\nagent_class_names = st.multiselect(\"Select agents\", [agent_class.__name__ for agent_class in AGENTS]) \nif not agent_class_names:\n    st.warning(\"Please select at least one agent.\")\n    st.stop()\n\n# Duplicate the classes if we want to see the same agent, but with a different config.\nwith st.expander(\"Duplicate agents\", expanded=False):\n    st.write(\"Optionally, you can duplicate the number of times class is selected. This is useful if you want to compare the same agent with different configurations.\")\n    agent_name_to_n_times: dict[str, int] = {agent_name: int(st.number_input(f\"Number of times to duplicate {agent_name}\", value=1)) for agent_name in agent_class_names}\n\n# Get the agent classes from the names.\nagent_classes: list[t.Type[AbstractBenchmarkedAgent]] = []\nfor agent_class in AGENTS:\n    if agent_class.__name__ in agent_class_names:\n        agent_classes.extend([agent_class for _ in range(agent_name_to_n_times[agent_class.__name__])])\n\n# Ask the user to provide a question.\ncustom_question_input = st.checkbox(\"Provide a custom question\", value=False)\nquestion = (st.text_input(\"Question\") if custom_question_input else st.selectbox(\"Select a question\", [m.question for m in markets]))\nif not question:\n    st.warning(\"Please enter a question.\")\n    st.stop()\n\ndo_question_evaluation = st.checkbox(\"Enable question evaluation step\", value=False)\n\n# Show the agent's titles.\nfor idx, (column, agent_class) in enumerate(zip(st.columns(len(agent_classes)), agent_classes)):\n    column.write(f\"## {agent_class.__name__} {idx}\")\n\nagents: list[AbstractBenchmarkedAgent] = []\n\nwith st.expander(\"Show agent's parameters\", expanded=False):\n    for idx, (column, agent_class) in enumerate(zip(st.columns(len(agent_classes)), agent_classes)):\n        # Inspect the agent's __init__ method to get arguments it accepts, \n        # ask the user to provide values for these arguments, \n        # and fill in defaults where possible.\n        inspect_class_init = inspect.getfullargspec(agent_class.__init__)\n        class_arg_to_default_value = {\n            arg_name: arg_default\n            for arg_name, arg_default in zip(\n                # Only last arguments can have a default value, so we need to slice the args list.\n                inspect_class_init.args[-(len(inspect_class_init.defaults or [])):], \n                inspect_class_init.defaults or [],\n            )\n        }\n        default_arguments = {\n            \"model\": \"gpt-3.5-turbo-0125\",\n            \"temperature\": 0.0,\n        }\n        class_inputs: dict[str, t.Any] = {}\n        for arg_name in inspect_class_init.args:\n            # Skip these, no need to ask the user for them.\n            if arg_name in (\"self\", \"agent_name\", \"max_workers\"):\n                continue\n            # We need SENTINEL to differentiate between not having the default value and when the default value is None.\n            default_value = class_arg_to_default_value.get(arg_name, default_arguments.get(arg_name, SENTINTEL))\n            input_type = type(default_value)\n            class_inputs[arg_name]  = (\n                # Show checkbox for booleans.\n                column.checkbox(arg_name, default_value if default_value is not SENTINTEL else False, key=f\"{idx}-{arg_name}\") \n                if input_type == bool \n                else\n                # Number input for numbers.\n                column.number_input(arg_name, default_value if default_value is not SENTINTEL else 0, key=f\"{idx}-{arg_name}\")\n                if input_type in (int, float)\n                else \n                # Convert strings to Enum, if the default value is an Enum.\n                input_type(str(column.text_input(arg_name, default_value, key=f\"{idx}-{arg_name}\")).replace(f\"{default_value.__class__.__name__}.\", \"\"))\n                if isinstance(default_value, Enum)\n                else\n                # Default to just a text input.\n                column.text_input(arg_name, default_value if default_value is not SENTINTEL else \"\", key=f\"{idx}-{arg_name}\")\n            )\n\n        # Instantiate the agent with the provided arguments.\n        agent: AbstractBenchmarkedAgent = agent_class(**class_inputs)\n        agents.append(agent)\n\n# Use checkboxes instead of expanders, because expanders don't work inside of columns.\nshow_evaluation = st.checkbox(\"Show evaluation\", value=False) if do_question_evaluation else False\n\nfor idx, (column, agent) in enumerate(zip(st.columns(len(agents)), agents)):\n    # Optionally, do the evaluation.\n    if do_question_evaluation:\n        with st.spinner(\"Evaluating...\"):\n            is_predictable = agent.is_predictable(market_question=question) \n        if show_evaluation:\n            column.markdown(f\"\"\"## Evaluation\n\nIs predictable: `{is_predictable}`\n\"\"\")\n        if not is_predictable:\n            column.error(\"The agent thinks this question is not predictable.\")\n            if not column.checkbox(\"Show prediction anyway\"):\n                st.stop()\n    else:\n        is_predictable = True\n\n    # Show research step only for agents that implement this method.\n    if hasattr(agent, \"research\"):\n        with st.spinner(\"Researching...\"):\n            with column.expander(\"Show agent's research\", expanded=False):\n                st.markdown(f\"\"\"## Research\n\n{agent.research(market_question=question)}\n\"\"\")\n\n    with st.spinner(\"Predicting...\"):\n        prediction = agent.predict(market_question=question)\n    with column.expander(\"Show agent's prediction\", expanded=True):\n        st.markdown(f\"\"\"## Prediction\n\n```       \n{prediction.outcome_prediction}\n```\n\"\"\")\n    if not prediction:\n        column.error(\"No prediction was generated.\")\n        st.stop()\n"}
{"type": "source_file", "path": "prediction_prophet/functions/rephrase_question.py", "content": "import json\nimport tiktoken\nfrom pydantic import BaseModel\nfrom langchain_openai import ChatOpenAI\nfrom prediction_prophet.autonolas.research import clean_completion_json\nfrom langchain.prompts import ChatPromptTemplate\n\n\nQUESTION_REPHRASE_PROMPT = \"\"\"We have the following question: `{question}`\n\nWrite a dictionary with following keys, don't answer the question, only rewrite it in the following ways:\n\n```\n- open_ended_question: Ask the question universally\n- negated_question Ask the question in negation\n```\n\"\"\"\n\n\nclass RephrasedQuestion(BaseModel):\n    original_question: str\n    negated_question: str\n    open_ended_question: str\n\n\ndef rephrase_question(\n    question: str,\n    engine: str = \"gpt-4-0125-preview\"\n) -> RephrasedQuestion:\n    \"\"\"\n    Rephrase the original question, by asking it in negation and universally, for example:\n\n    original_question: Is the sky blue?\n    negated_question: Is the sky not blue?\n    open_ended_question: What is the color of the sky?\n    \"\"\"\n    tokenizer = tiktoken.encoding_for_model(engine)\n    llm = ChatOpenAI(model=engine, temperature=0.0)\n\n    prompt = ChatPromptTemplate.from_template(template=QUESTION_REPHRASE_PROMPT)\n    messages = prompt.format_messages(question=question)\n\n    max_tokens = 2 * len(tokenizer.encode(question)) + 50 # Max tokens as the question two times + some buffer for formatting.\n    completion = str(llm(messages, max_tokens=max_tokens).content)\n\n    try:\n        return RephrasedQuestion(\n            original_question=question, \n            **json.loads(clean_completion_json(completion))\n        )\n    except json.decoder.JSONDecodeError as e:\n        raise ValueError(f\"Error in rephrase_question for `{question}`: {completion}\") from e\n        \n"}
