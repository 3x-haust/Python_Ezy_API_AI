{"repo_info": {"repo_name": "doku", "repo_owner": "amerkurev", "repo_url": "https://github.com/amerkurev/doku"}}
{"type": "test_file", "path": "app/contrib/tests/__init__.py", "content": ""}
{"type": "test_file", "path": "app/scan/tests/test_scanner.py", "content": "from pathlib import Path\nfrom unittest.mock import MagicMock, patch, call, ANY\n\nimport pytest\nfrom docker.models.images import Image\nfrom docker.models.containers import Container\n\nfrom contrib.types import (\n    DockerMount,\n    DockerImageList,\n    DockerContainerList,\n    DockerVolumeList,\n    DockerBuildCacheList,\n    DockerContainerLog,\n    DockerBindMounts,\n    DockerOverlay2Layer,\n)\nfrom scan.scanner import BaseScanner, SystemDFScanner, LogfilesScanner, BindMountsScanner, Overlay2Scanner\n\n\n@pytest.fixture\ndef mock_docker_client():\n    return MagicMock()\n\n\n@pytest.fixture\ndef mock_is_stop():\n    return MagicMock(return_value=False)\n\n\n@pytest.fixture\ndef docker_mount():\n    return DockerMount.model_validate({\n        'Source': '/host/path',\n        'Destination': '/container/path',\n        'Mode': 'ro',\n        'Propagation': 'rprivate',\n        'RW': False,\n        'Type': 'bind',\n        'Root': True,\n    })\n\n\ndef test_base_scanner(mock_docker_client):\n    with patch('scan.scanner.docker_from_env', return_value=mock_docker_client):\n        scanner = BaseScanner()\n        assert scanner.logger is not None\n\n        with pytest.raises(NotImplementedError):\n            scanner.scan()\n\n        with pytest.raises(NotImplementedError):\n            _ = scanner.database_name\n\n        with pytest.raises(NotImplementedError):\n            _ = scanner.table_name\n\n\ndef test_system_df_scanner(mock_docker_client, docker_mount):\n    # mock DF data\n    df_data = {\n        'Images': [\n            {\n                'Id': 'sha256:123456789abcdef',\n                'Created': '2023-01-01T12:00:00Z',\n                'ParentId': 'sha256:987654321fedcba',\n                'RepoTags': ['image:latest', 'image:v1'],\n                'SharedSize': 1000,\n                'Size': 5000,\n            }\n        ],\n        'Containers': [\n            {\n                'Id': '123456789abcdef',\n                'Names': ['/container_name'],\n                'Image': 'nginx:latest',\n                'ImageID': 'sha256:123456789abcdef',\n                'Created': '2023-01-01T12:00:00Z',\n                'SizeRw': 50000,\n                'SizeRootFs': 200000,\n                'State': 'running',\n            }\n        ],\n        'Volumes': [\n            {\n                'Name': 'volume1',\n                'Driver': 'local',\n                'CreatedAt': '2023-01-01T12:00:00Z',\n                'Mountpoint': '/var/lib/docker/volumes/volume1',\n                'Scope': 'local',\n                'UsageData': {'Size': 1000, 'RefCount': 2},\n            }\n        ],\n        'BuildCache': [\n            {\n                'ID': 'abcdef123456',\n                'Type': 'layer',\n                'Description': 'mount / from exec /bin/sh -c apk update && apk add ' * 10,\n                'InUse': True,\n                'Shared': False,\n                'Size': 1024,\n                'CreatedAt': '2023-01-01T12:00:00Z',\n                'LastUsedAt': '2023-02-01T12:00:00Z',\n                'UsageCount': 5,\n            }\n        ],\n    }\n\n    # container with image and volume\n    mock_container = MagicMock(spec=Container)\n    mock_container.name = 'container1'\n    mock_container.attrs = {'Image': 'sha256:123456789abcdef', 'Mounts': [{'Type': 'volume', 'Name': 'volume1'}]}\n\n    mock_docker_client.df.return_value = df_data\n    mock_docker_client.containers.list.return_value = [mock_container]\n\n    with (\n        patch('scan.scanner.docker_from_env', return_value=mock_docker_client),\n        patch('scan.scanner.doku_mounts', return_value=[docker_mount]),\n        patch('scan.scanner.kvstore.set') as mock_kvstore_set,\n    ):\n        scanner = SystemDFScanner()\n        scanner.client = mock_docker_client\n        scanner.scan()\n\n        mock_docker_client.df.assert_called_once()\n        mock_docker_client.containers.list.assert_called_once_with(all=True, sparse=False)\n\n        images = DockerImageList.model_validate(df_data['Images'])\n        images.root[0].containers = ['container1']\n\n        volumes = DockerVolumeList.model_validate(df_data['Volumes'])\n        volumes.root[0].containers = ['container1']\n\n        # check what was stored in the kvstore\n        mock_kvstore_set.assert_has_calls(\n            [\n                call('image', images, ANY),\n                call('container', DockerContainerList.model_validate(df_data['Containers']), ANY),\n                call('volume', volumes, ANY),\n                call('build_cache', DockerBuildCacheList.model_validate(df_data['BuildCache']), ANY),\n                call('root_mount', docker_mount, ANY),\n            ],\n            any_order=True,\n        )\n\n\ndef test_logfiles_scanner(mock_docker_client, mock_is_stop, docker_mount):\n    short_id = '7d2de847bebae847b'\n    name = 'container1'\n    log_path = '/var/lib/docker/containers/7d2de847bebae847b-json.log'\n    image = 'nginx:latest'\n    st_size = 1024 * 10\n\n    # mock container data\n    mock_container = MagicMock(spec=Container)\n    mock_container.short_id = short_id\n    mock_container.name = name\n    mock_container.attrs = {'LogPath': log_path}\n\n    mock_image = MagicMock(spec=Image)\n    mock_image.tags = [image]\n    mock_container.image = mock_image\n\n    mock_docker_client.containers.list.return_value = [mock_container]\n\n    with (\n        patch('scan.scanner.docker_from_env', return_value=mock_docker_client),\n        patch('scan.scanner.doku_mounts', return_value=[docker_mount]),\n        patch('scan.scanner.map_host_path_to_container') as mock_map_path,\n        patch('scan.scanner.kvstore.set') as mock_kvstore_set,\n    ):\n        mock_path = MagicMock(spec=Path)\n        mock_path.stat.return_value.st_size = st_size\n        mock_map_path.return_value = mock_path\n\n        scanner = LogfilesScanner(mock_is_stop)\n        scanner.client = mock_docker_client\n        scanner.scan()\n\n        mock_docker_client.containers.list.assert_called_once_with(all=True)\n\n        # check what was stored in the kvstore\n        obj = DockerContainerLog.model_validate({\n            'id': short_id,\n            'name': name,\n            'image': image,\n            'path': log_path,\n            'size': st_size,\n            'last_scan': '2023-01-01T12:00:00Z',\n        })\n        obj.last_scan = ANY\n        mock_kvstore_set.assert_called_once_with(mock_container.short_id, obj, ANY)\n\n        mock_map_path.return_value = None\n        scanner.scan()\n\n\ndef test_bind_mounts_scanner(mock_docker_client, mock_is_stop, docker_mount):\n    # mock Doku container\n    doku_container = MagicMock(spec=Container)\n    doku_container.short_id = 'doku1234'\n    doku_container.name = 'doku_container'\n    doku_container.attrs = {\n        'Mounts': [\n            {'Type': 'bind', 'Source': '/doku/path', 'Destination': '/doku/container/path', 'Mode': 'rw', 'RW': True}\n        ]\n    }\n    doku_image = MagicMock(spec=Image)\n    doku_image.tags = ['doku:latest']\n    doku_container.image = doku_image\n\n    # mock regular container\n    regular_container = MagicMock(spec=Container)\n    regular_container.short_id = 'cont1234'\n    regular_container.name = 'container1'\n    regular_container.attrs = {\n        'Mounts': [{'Type': 'bind', 'Source': '/host/path', 'Destination': '/container/path', 'Mode': 'rw', 'RW': True}]\n    }\n    regular_image = MagicMock(spec=Image)\n    regular_image.tags = ['image:latest']\n    regular_container.image = regular_image\n\n    # mock list containers\n    mock_docker_client.containers.list.side_effect = lambda **kwargs: (\n        [doku_container]\n        if kwargs.get('filters') == {'label': 'github.repo=amerkurev/doku'}\n        else [doku_container, regular_container, regular_container]\n    )\n\n    with (\n        patch('scan.scanner.docker_from_env', return_value=mock_docker_client),\n        patch('scan.scanner.doku_mounts', return_value=[docker_mount]),\n        patch('scan.scanner.map_host_path_to_container') as mock_map_path,\n        patch('scan.scanner.kvstore.set') as mock_kvstore_set,\n    ):\n        mock_path = MagicMock(spec=Path)\n        mock_path.stat.return_value.st_size = 2048\n        mock_map_path.return_value = mock_path\n\n        scanner = BindMountsScanner(mock_is_stop)\n        scanner.client = mock_docker_client\n        scanner.scan()\n\n        assert mock_docker_client.containers.list.call_count == 2\n        mock_docker_client.containers.list.assert_any_call(filters={'label': 'github.repo=amerkurev/doku'})\n        mock_docker_client.containers.list.assert_any_call(all=True)\n\n        # check what was stored in the kvstore\n        obj = DockerBindMounts(\n            path='/host/path',\n            err=False,\n            size=0,\n            scan_in_progress=False,\n            last_scan='2023-01-01T12:00:00Z',\n            containers=['container1', 'container1'],\n        )\n        obj.last_scan = ANY\n\n        mock_kvstore_set.assert_has_calls(\n            [\n                call('/host/path', obj, ANY),\n                call('/host/path', obj, ANY),\n                call('/host/path', obj, ANY),\n            ],\n            any_order=True,\n        )\n\n        mock_map_path.return_value = None\n        scanner.scan()\n\n\n@pytest.fixture\ndef mock_diff_subdirs():\n    counter = 0\n\n    def _mock_func(path: Path) -> list[Path]:\n        nonlocal counter\n        counter += 1\n\n        if counter == 1:  # len(subdirs) == 1\n            m = MagicMock(spec=Path)\n            m.name = 'root'\n            return [m]\n        elif counter == 3:  # len(subdirs) > 1\n            m1 = MagicMock(spec=Path)\n            m1.name = 'mnt'\n            m2 = MagicMock(spec=Path)\n            m2.name = 'root'\n            return [m1, m2]\n        return []  # len(subdirs) == 0\n\n    return _mock_func\n\n\ndef test_overlay2_scanner(mock_docker_client, mock_is_stop, docker_mount, mock_diff_subdirs):\n    # mock image data\n    mock_image = MagicMock(spec=Image)\n    mock_image.id = 'sha256:abc123'\n    mock_image.tags = ['image:latest']\n    mock_image.attrs = {\n        'GraphDriver': {\n            'Data': {\n                'MergedDir': '/var/lib/docker/overlay2/img123/merged',\n                'UpperDir': '/var/lib/docker/overlay2/img123/diff',\n                'WorkDir': '/var/lib/docker/overlay2/img123/work',\n                'LowerDir': '/var/lib/docker/overlay2/img456/diff:/var/lib/docker/overlay2/img789/diff',\n            },\n            'Name': 'overlay2',\n        }\n    }\n\n    # mock container data\n    mock_container = MagicMock(spec=Container)\n    mock_container.short_id = 'cont1234'\n    mock_container.name = 'container1'\n    mock_container.attrs = {\n        'GraphDriver': {\n            'Data': {\n                'MergedDir': '/var/lib/docker/overlay2/abc123/merged',\n                'UpperDir': '/var/lib/docker/overlay2/abc123/diff',\n                'WorkDir': '/var/lib/docker/overlay2/abc123/work',\n                'LowerDir': '/var/lib/docker/overlay2/def456/diff:/var/lib/docker/overlay2/ghi789/diff',\n            },\n            'Name': 'overlay2',\n        }\n    }\n\n    mock_container.image = mock_image\n\n    mock_docker_client.containers.list.return_value = [mock_container]\n    mock_docker_client.images.list.return_value = [mock_image]\n\n    with (\n        patch('scan.scanner.docker_from_env', return_value=mock_docker_client),\n        patch('scan.scanner.doku_mounts', return_value=[docker_mount]),\n        patch('scan.scanner.map_host_path_to_container') as mock_map_path,\n        patch('pathlib.Path.exists', return_value=True),\n        patch('scan.scanner.diff_subdirs', side_effect=mock_diff_subdirs),\n        patch('scan.scanner.get_size') as mock_get_size,\n        patch('scan.scanner.kvstore.set') as mock_kvstore_set,\n    ):\n        # mock for map_host_path_to_container\n        mock_path = MagicMock(spec=Path)\n        mock_path.stat.return_value.st_size = 1024\n        mock_map_path.return_value = mock_path\n\n        scanner = Overlay2Scanner(mock_is_stop)\n        scanner.client = mock_docker_client\n\n        scanner.overlay2_dir = MagicMock(spec=Path)\n\n        overlay2_iterdir_0 = MagicMock(spec=Path)\n        overlay2_iterdir_0.name = '/var/lib/docker/overlay2/img123'\n        overlay2_iterdir_0.stat.return_value.st_ctime = 1234567890\n\n        overlay2_iterdir_1 = MagicMock(spec=Path)\n        overlay2_iterdir_1.name = '/var/lib/docker/overlay2/img456'\n        overlay2_iterdir_1.stat.return_value.st_ctime = 1234567890\n\n        scanner.overlay2_dir.iterdir.return_value = [overlay2_iterdir_0, overlay2_iterdir_1]\n\n        mock_get_size.side_effect = [1024, Exception('Failed to get size')]\n        scanner.scan()\n\n        # verify method calls\n        mock_docker_client.containers.list.assert_called_once_with(all=True)\n        mock_docker_client.images.list.assert_called_once()\n\n        overlay2_1 = DockerOverlay2Layer(\n            id='/var/lib/docker/overlay2/img123',\n            created='2023-01-01T12:00:00Z',\n            diff_root='/root',\n            err=False,\n            size=1024,\n            scan_in_progress=False,\n            last_scan='2023-01-01T12:00:00Z',\n            in_use=False,\n        )\n        overlay2_1.last_scan = ANY\n        overlay2_1.created = ANY\n\n        overlay2_2 = DockerOverlay2Layer(\n            id='/var/lib/docker/overlay2/img456',\n            created='2023-01-01T12:00:00Z',\n            diff_root='/mnt, /root',\n            err=True,\n            size=0,\n            scan_in_progress=False,\n            last_scan='2023-01-01T12:00:00Z',\n            in_use=False,\n        )\n        overlay2_2.last_scan = ANY\n        overlay2_2.created = ANY\n\n        # check what was stored in the kvstore\n        mock_kvstore_set.assert_has_calls(\n            [\n                call('/var/lib/docker/overlay2/img123', overlay2_1, ANY),\n                call('/var/lib/docker/overlay2/img123', overlay2_1, ANY),\n                call('/var/lib/docker/overlay2/img456', overlay2_2, ANY),\n                call('/var/lib/docker/overlay2/img456', overlay2_2, ANY),\n            ],\n            any_order=True,\n        )\n\n        # test when path mapping returns None\n        mock_map_path.return_value = None\n        scanner.scan()\n"}
{"type": "test_file", "path": "app/scan/tests/test_df.py", "content": "from unittest.mock import patch, MagicMock\n\nimport pytest\n\nfrom scan.df import main\n\n\n@pytest.fixture\ndef mock_stop_signal():\n    \"\"\"Creates a mock that returns False 10 times, then True\"\"\"\n    mock = MagicMock()\n    mock.is_stop = MagicMock(side_effect=[False] * 10 + [True])\n    return mock\n\n\n@patch('scan.df.SignalHandler')\n@patch('scan.df.setup_logger')\n@patch('scan.df.SystemDFScanner')\n@patch('scan.df.LogfilesScanner')\n@patch('scan.df.time.sleep')\n@patch('scan.df.schedule.every')\ndef test_main(\n    mock_schedule,\n    mock_sleep,\n    mock_logfiles_scanner,\n    mock_system_scanner,\n    mock_logger,\n    mock_signal_handler,\n    mock_stop_signal,\n):\n    # Setup mocks\n    mock_signal_handler.return_value = mock_stop_signal\n    mock_logger.return_value.info = MagicMock()\n\n    # Run main function\n    main()\n\n    mock_signal_handler.assert_called_once()\n    mock_logger.assert_called_once()\n\n    mock_logger.return_value.info.assert_any_call('DF scanner started (system df + logfiles).')\n    mock_logger.return_value.info.assert_any_call('DF scanner stopped.')\n\n    mock_system_scanner.assert_called_once()\n    mock_system_scanner.return_value.scan.assert_called_once()\n\n    mock_logfiles_scanner.assert_called_once_with(is_stop=mock_stop_signal.is_stop)\n    mock_logfiles_scanner.return_value.scan.assert_called_once()\n\n    assert mock_schedule.call_count == 2\n\n    # Verify sleep was called 10 times (matches our mock signal setup)\n    assert mock_sleep.call_count == 10\n    mock_sleep.assert_called_with(1)\n"}
{"type": "test_file", "path": "app/server/tests/test_auth.py", "content": "from unittest.mock import MagicMock\n\nimport bcrypt\nimport pytest\nfrom fastapi import HTTPException, Request\nfrom fastapi.security import HTTPBasicCredentials\n\nfrom server.auth import check_password, basic_auth\n\n\ndef test_check_password_valid():\n    password = 'testpassword'\n    hashed = bcrypt.hashpw(password.encode(), bcrypt.gensalt()).decode()\n    assert check_password(password, hashed) is True\n\n\ndef test_check_password_invalid():\n    password = 'testpassword'\n    wrong_password = 'wrongpassword'\n    hashed = bcrypt.hashpw(password.encode(), bcrypt.gensalt()).decode()\n    assert check_password(wrong_password, hashed) is False\n\n\ndef test_basic_auth_no_credentials():\n    # Test when no credentials are configured in the app\n    mock_request = MagicMock(spec=Request)\n    mock_request.state.basic_auth_credentials = None\n    credentials = HTTPBasicCredentials(username='someuser', password='somepass')\n\n    # Should return username without checking password\n    result = basic_auth(mock_request, credentials)\n    assert result == 'someuser'\n\n\ndef test_basic_auth_valid_credentials():\n    # Test with valid credentials\n    password = 'testpassword'\n    hashed = bcrypt.hashpw(password.encode(), bcrypt.gensalt()).decode()\n\n    mock_request = MagicMock(spec=Request)\n    mock_request.state.basic_auth_credentials = {'testuser': hashed}\n    credentials = HTTPBasicCredentials(username='testuser', password='testpassword')\n\n    result = basic_auth(mock_request, credentials)\n    assert result == 'testuser'\n\n\ndef test_basic_auth_invalid_username():\n    # Test with invalid username\n    password = 'testpassword'\n    hashed = bcrypt.hashpw(password.encode(), bcrypt.gensalt()).decode()\n\n    mock_request = MagicMock(spec=Request)\n    mock_request.state.basic_auth_credentials = {'testuser': hashed}\n    credentials = HTTPBasicCredentials(username='wronguser', password='testpassword')\n\n    with pytest.raises(HTTPException) as excinfo:\n        basic_auth(mock_request, credentials)\n\n    assert excinfo.value.status_code == 401\n    assert excinfo.value.detail == 'Incorrect username or password'\n    assert excinfo.value.headers == {'WWW-Authenticate': 'Basic'}\n\n\ndef test_basic_auth_invalid_password():\n    # Test with invalid password\n    password = 'testpassword'\n    hashed = bcrypt.hashpw(password.encode(), bcrypt.gensalt()).decode()\n\n    mock_request = MagicMock(spec=Request)\n    mock_request.state.basic_auth_credentials = {'testuser': hashed}\n    credentials = HTTPBasicCredentials(username='testuser', password='wrongpassword')\n\n    with pytest.raises(HTTPException) as excinfo:\n        basic_auth(mock_request, credentials)\n\n    assert excinfo.value.status_code == 401\n    assert excinfo.value.detail == 'Incorrect username or password'\n    assert excinfo.value.headers == {'WWW-Authenticate': 'Basic'}\n"}
{"type": "test_file", "path": "app/scan/tests/test_du.py", "content": "from unittest.mock import patch, MagicMock\n\nimport pytest\n\nfrom scan.du import main\n\n\n@pytest.fixture\ndef mock_stop_signal():\n    \"\"\"Creates a mock that returns False 10 times, then True\"\"\"\n    mock = MagicMock()\n    mock.is_stop = MagicMock(side_effect=[False] * 10 + [True])\n    return mock\n\n\n@patch('scan.du.SignalHandler')\n@patch('scan.du.setup_logger')\n@patch('scan.du.BindMountsScanner')\n@patch('scan.du.Overlay2Scanner')\n@patch('scan.du.time.sleep')\n@patch('scan.du.schedule.every')\ndef test_main(\n    mock_schedule,\n    mock_sleep,\n    mock_overlay2_scanner,\n    mock_bindmounts_scanner,\n    mock_logger,\n    mock_signal_handler,\n    mock_stop_signal,\n):\n    # Setup mocks\n    mock_signal_handler.return_value = mock_stop_signal\n    mock_logger.return_value.info = MagicMock()\n\n    # Run main function\n    main()\n\n    mock_signal_handler.assert_called_once()\n    mock_logger.assert_called_once()\n\n    mock_logger.return_value.info.assert_any_call('DU scanner started (bind mounts + overlay2).')\n    mock_logger.return_value.info.assert_any_call('DU scanner stopped.')\n\n    mock_overlay2_scanner.assert_called_once()\n    mock_overlay2_scanner.return_value.scan.assert_called_once()\n\n    mock_bindmounts_scanner.assert_called_once_with(is_stop=mock_stop_signal.is_stop)\n    mock_bindmounts_scanner.return_value.scan.assert_called_once()\n\n    assert mock_schedule.call_count == 2\n\n    # Verify sleep was called 10 times (matches our mock signal setup)\n    assert mock_sleep.call_count == 10\n    mock_sleep.assert_called_with(1)\n"}
{"type": "test_file", "path": "app/test_main.py", "content": "from unittest.mock import patch\n\nfrom settings import Settings, to_string as settings_to_string\nfrom main import main\n\n\n@patch('uvicorn.run')\ndef test_main_with_default_settings(mock_run):\n    with patch('settings.SSL_KEYFILE', ''), patch('settings.SSL_CERTFILE', ''):\n        main()\n        mock_run.assert_called_once()\n        args, kwargs = mock_run.call_args\n        assert args[0] == 'main:app'\n        assert 'ssl_keyfile' not in kwargs\n        assert 'ssl_certfile' not in kwargs\n\n\n@patch('uvicorn.run')\ndef test_main_with_ssl_files_not_exist(mock_run):\n    with (\n        patch('settings.SSL_KEYFILE', '/not/exist/key.pem'),\n        patch('settings.SSL_CERTFILE', '/not/exist/cert.pem'),\n        patch('pathlib.Path.is_file', return_value=False),\n    ):\n        main()\n        mock_run.assert_called_once()\n        args, kwargs = mock_run.call_args\n        assert args[0] == 'main:app'\n        assert 'ssl_keyfile' not in kwargs\n        assert 'ssl_certfile' not in kwargs\n\n\n@patch('uvicorn.run')\ndef test_main_with_ssl_files_exist(mock_run):\n    with (\n        patch('settings.SSL_KEYFILE', '/path/to/key.pem'),\n        patch('settings.SSL_CERTFILE', '/path/to/cert.pem'),\n        patch('settings.SSL_KEYFILE_PASSWORD', ''),\n        patch('pathlib.Path.is_file', return_value=True),\n    ):\n        main()\n        mock_run.assert_called_once()\n        args, kwargs = mock_run.call_args\n        assert args[0] == 'main:app'\n        assert kwargs['ssl_keyfile'] == '/path/to/key.pem'\n        assert kwargs['ssl_certfile'] == '/path/to/cert.pem'\n        assert 'ssl_keyfile_password' not in kwargs\n\n\n@patch('uvicorn.run')\ndef test_main_with_ssl_password(mock_run):\n    with (\n        patch('settings.SSL_KEYFILE', '/path/to/key.pem'),\n        patch('settings.SSL_CERTFILE', '/path/to/cert.pem'),\n        patch('settings.SSL_KEYFILE_PASSWORD', 'password'),\n        patch('pathlib.Path.is_file', return_value=True),\n    ):\n        main()\n        mock_run.assert_called_once()\n        args, kwargs = mock_run.call_args\n        assert args[0] == 'main:app'\n        assert kwargs['ssl_keyfile'] == '/path/to/key.pem'\n        assert kwargs['ssl_certfile'] == '/path/to/cert.pem'\n        assert kwargs['ssl_keyfile_password'] == 'password'\n\n\ndef test_settings_to_string():\n    with patch('settings._settings') as mock_settings:\n        # Configure the mock settings object\n        mock_settings.model_dump.return_value = {\n            'host': '0.0.0.0',\n            'port': 9090,\n            'log_level': 'info',\n            'root_path': '/doku',\n            'ssl_keyfile': '/.ssl/key.pem',\n            'ssl_keyfile_password': 'secret',\n            'ssl_certfile': '/.ssl/cert.pem',\n            'scan_interval': 60,\n            'bindmount_ignore_patterns': '/home/*;/tmp/*;*/.git/*',\n            'disable_overlay2_scan': False,\n            'workers': 4,\n            'docker_host': 'unix:///var/run/docker.sock',\n            'git_tag': 'v1.2.3',\n            'git_sha': 'abcdef1234567890',\n        }\n\n        # Set needed attributes on the mock\n        mock_settings.ssl_keyfile_password = 'secret'\n\n        # Call the function\n        result = settings_to_string()\n\n        # Verify the result contains expected information\n        assert 'Doku settings:' in result\n\n        # Verify each category exists\n        assert 'General settings:' in result\n        assert 'SSL settings:' in result\n        assert 'Scan settings:' in result\n        assert 'Uvicorn settings:' in result\n        assert 'Docker settings:' in result\n        assert 'Version info:' in result\n\n        # Verify specific settings are included\n        assert 'host: 0.0.0.0' in result\n        assert 'port: 9090' in result\n        assert 'log_level: info' in result\n        assert 'root_path: /doku' in result\n\n        # Scan settings\n        assert 'scan_interval: 60' in result\n        assert 'bindmount_ignore_patterns: /home/*;/tmp/*;*/.git/*' in result\n        assert 'disable_overlay2_scan: False' in result\n\n        # Verify password is masked\n        assert 'ssl_keyfile_password: ********' in result\n        assert 'ssl_keyfile_password: secret' not in result\n\n        # Verify SSL settings\n        assert 'ssl_keyfile: /.ssl/key.pem' in result\n        assert 'ssl_certfile: /.ssl/cert.pem' in result\n\n        # Verify Docker settings\n        assert 'docker_host: unix:///var/run/docker.sock' in result\n\n        # Verify version info\n        assert 'git_tag: v1.2.3' in result\n        assert 'git_sha: abcdef1234567890' in result\n\n\ndef patterns_assert(patterns: list[str]):\n    assert len(patterns) == 3\n    assert '/home/*' in patterns\n    assert '/tmp/*' in patterns\n    assert '*/.git/*' in patterns\n\n\ndef test_bindmount_ignore_patterns_list():\n    with patch('os.environ', {'BINDMOUNT_IGNORE_PATTERNS': '/home/*;/tmp/*;*/.git/*'}):\n        s = Settings()\n        patterns_assert(s.bindmount_ignore_patterns_list)\n\n    # Test with entire string in double quotes\n    with patch('os.environ', {'BINDMOUNT_IGNORE_PATTERNS': '\"/home/*;/tmp/*;*/.git/*\"'}):\n        s = Settings()\n        patterns_assert(s.bindmount_ignore_patterns_list)\n\n    # Test with entire string in single quotes\n    with patch('os.environ', {'BINDMOUNT_IGNORE_PATTERNS': \"';/home/*;/tmp/*;*/.git/*;'\"}):\n        s = Settings()\n        patterns_assert(s.bindmount_ignore_patterns_list)\n\n    # Test with a single pattern\n    with patch('os.environ', {'BINDMOUNT_IGNORE_PATTERNS': '/var/log/*;'}):\n        s = Settings()\n        patterns = s.bindmount_ignore_patterns_list\n        assert len(patterns) == 1\n        assert '/var/log/*' in patterns\n\n    # Test with empty string\n    with patch('os.environ', {'BINDMOUNT_IGNORE_PATTERNS': ';;'}):\n        s = Settings()\n        patterns = s.bindmount_ignore_patterns_list\n        assert len(patterns) == 0\n\n    # Test with whitespace\n    with patch('os.environ', {'BINDMOUNT_IGNORE_PATTERNS': '  /path1/*;  /path2/*  '}):\n        s = Settings()\n        patterns = s.bindmount_ignore_patterns_list\n        assert len(patterns) == 2\n        assert '/path1/*' in patterns\n        assert '/path2/*' in patterns\n"}
{"type": "test_file", "path": "app/contrib/tests/test_signal.py", "content": "import signal\n\nfrom contrib.signal import SignalHandler\n\n\ndef test_signal_handler():\n    handler = SignalHandler()\n    handler.is_stop() is False\n    handler.handler(signal.SIGINT, None)\n    handler.is_stop() is True\n"}
{"type": "test_file", "path": "app/contrib/tests/test_kvstore.py", "content": "import datetime\n\nimport pytest\nfrom peewee import SqliteDatabase\nfrom playhouse.kv import KeyValue\n\nfrom contrib import kvstore\nfrom contrib.types import DockerImage\n\n\n@pytest.fixture\ndef test_model():\n    return DockerImage(\n        Id='test_id',\n        Created=datetime.datetime.now(),\n        RepoTags=['test_repo:tag', 'test_repo:latest'],\n        SharedSize=0,\n        Size=100,\n        containers=['test_container'],\n    )\n\n\ndef test_set_get(test_model):\n    db = SqliteDatabase(':memory:')\n    kv = KeyValue(database=db, table_name='test_kvstore')\n\n    with db:\n        # Test setting and retrieving a single item\n        key = 'test_key'\n        kvstore.set(key, test_model, kv)\n\n        ret = kvstore.get(key, kv, DockerImage)\n\n        assert ret.id == test_model.id\n        assert ret.created == test_model.created\n        assert ret.repo_tags == test_model.repo_tags\n        assert ret.shared_size == test_model.shared_size\n        assert ret.size == test_model.size\n        assert ret.containers == test_model.containers\n        assert isinstance(ret, DockerImage)\n\n\ndef test_get_all(test_model):\n    db = SqliteDatabase(':memory:')\n    kv = KeyValue(database=db, table_name='test_kvstore')\n\n    with db:\n        # Test getting all items\n        test_model.id = 'key1'\n        kvstore.set('key1', test_model, kv)\n        test_model.id = 'key2'\n        kvstore.set('key2', test_model, kv)\n\n        ret = kvstore.get_all(kv, DockerImage)\n\n        assert len(ret) == 2\n        for n, item in enumerate(ret, 1):\n            assert item.id == 'key' + str(n)\n            assert item.created == test_model.created\n            assert item.repo_tags == test_model.repo_tags\n            assert item.shared_size == test_model.shared_size\n            assert item.size == test_model.size\n            assert item.containers == test_model.containers\n            assert isinstance(item, DockerImage)\n"}
{"type": "test_file", "path": "app/server/tests/test_state.py", "content": "from unittest import mock\n\nimport pytest\nfrom fastapi import FastAPI\n\nfrom server import state\nfrom server.state import lifespan\n\n\n@pytest.fixture\ndef mock_settings():\n    with mock.patch('server.state.settings') as mock_settings:\n        mock_settings.VERSION = 'test_version'\n        mock_settings.BASIC_HTPASSWD = None\n        yield mock_settings\n\n\n@pytest.mark.asyncio\nasync def test_lifespan_no_credentials(mock_settings):\n    app = FastAPI()\n    async with lifespan(app) as state_instance:\n        assert state_instance['basic_auth_credentials'] is None\n        assert state_instance['version'] == 'test_version'\n\n\n@pytest.mark.asyncio\nasync def test_lifespan_with_credentials_file_not_found(mock_settings, tmp_path):\n    mock_settings.BASIC_HTPASSWD = str(tmp_path / 'non_existent.htpasswd')\n\n    app = FastAPI()\n    async with lifespan(app) as state_instance:\n        assert state_instance['basic_auth_credentials'] is None\n        assert state_instance['version'] == 'test_version'\n\n\n@pytest.mark.asyncio\nasync def test_lifespan_with_credentials_file(mock_settings, tmp_path):\n    htpasswd_file = tmp_path / 'test.htpasswd'\n    htpasswd_content = 'user1:hash1\\nuser2:hash2'\n    htpasswd_file.write_text(htpasswd_content)\n    mock_settings.BASIC_HTPASSWD = str(htpasswd_file)\n\n    app = FastAPI()\n    async with lifespan(app) as state_instance:\n        assert state_instance['basic_auth_credentials'] == {'user1': 'hash1', 'user2': 'hash2'}\n        assert state_instance['version'] == 'test_version'\n\n\ndef test_state_typed_dict():\n    # Test that State is a properly defined TypedDict\n    state_dict = state.State(basic_auth_credentials={'user': 'hash'}, version='1.0.0')\n    assert state_dict['basic_auth_credentials'] == {'user': 'hash'}\n    assert state_dict['version'] == '1.0.0'\n"}
{"type": "test_file", "path": "app/server/tests/test_site.py", "content": "import time\nfrom pathlib import Path\nfrom unittest.mock import patch, MagicMock\n\nimport pytest\nfrom fastapi.testclient import TestClient\n\nimport settings\nfrom main import app\nfrom contrib.types import (\n    DockerImage,\n    DockerImageList,\n    DockerContainer,\n    DockerContainerList,\n    DockerVolume,\n    DockerVolumeList,\n    DockerBuildCache,\n    DockerBuildCacheList,\n    DockerMount,\n    DockerContainerLog,\n    DockerBindMounts,\n    DockerOverlay2Layer,\n)\n\n\nclient = TestClient(app)\n\n\n@pytest.fixture\ndef log_start_time():\n    def _log_start_time(database: Path, table: str) -> int:\n        ts = int(time.time())\n        filename = database / f'{table}.timestamp'\n        with filename.open('w') as fd:\n            fd.write(str(ts))\n        return ts\n\n    return _log_start_time\n\n\n@patch('server.router.context.kvstore')\ndef test_images(mock_kvstore, log_start_time):\n    settings.DB_DF.touch()\n    log_start_time(settings.DB_DIR, settings.TABLE_SYSTEM_DF)\n\n    images = [\n        DockerImage(\n            Id='sha256:123456789abcdef',\n            Created='2023-01-01T12:00:00Z',\n            ParentID='sha256:987654321fedcba',\n            RepoTags=['image:latest', 'image:v1'],\n            SharedSize=1000,\n            Size=4000,\n            containers=['test_container1', 'test_container2'],\n        ),\n    ]\n    mock_kvstore.get.return_value = MagicMock(root=images)\n\n    response = client.get('/site/images/')\n    assert response.status_code == 200\n    assert response.headers['content-type'] == 'text/html; charset=utf-8'\n\n\n@patch('server.router.context.kvstore')\ndef test_containers(mock_kvstore, log_start_time):\n    settings.DB_DF.touch()\n    log_start_time(settings.DB_DIR, settings.TABLE_SYSTEM_DF)\n\n    containers = [\n        DockerContainer(\n            Id='sha256:123456789abcdef',\n            Names=['/test_container1', '/test_container2'],\n            Image='image:latest',\n            ImageID='sha256:987654321fedcba',\n            Created='2023-01-01T12:00:00Z',\n            SizeRw=50000,\n            SizeRootFs=200000,\n            State='running',\n        ),\n    ]\n    mock_kvstore.get.return_value = MagicMock(root=containers)\n\n    response = client.get('/site/containers/')\n    assert response.status_code == 200\n    assert response.headers['content-type'] == 'text/html; charset=utf-8'\n\n\n@patch('server.router.context.kvstore')\ndef test_volumes(mock_kvstore, log_start_time):\n    settings.DB_DF.touch()\n    log_start_time(settings.DB_DIR, settings.TABLE_SYSTEM_DF)\n\n    volumes = [\n        DockerVolume(\n            Name='test_volume1',\n            Driver='local',\n            CreatedAt='2023-01-01T12:00:00Z',\n            Mountpoint='/var/lib/docker/volumes/test_volume1/_data',\n            Scope='local',\n            UsageData={'Size': 1000, 'RefCount': 2},\n        ),\n    ]\n    mock_kvstore.get.return_value = MagicMock(root=volumes)\n\n    response = client.get('/site/volumes/')\n    assert response.status_code == 200\n    assert response.headers['content-type'] == 'text/html; charset=utf-8'\n\n\n@patch('server.router.context.kvstore')\ndef test_build_cache(mock_kvstore, log_start_time):\n    settings.DB_DF.touch()\n    log_start_time(settings.DB_DIR, settings.TABLE_SYSTEM_DF)\n\n    build_cache = [\n        DockerBuildCache(\n            ID='sha256:123456789abcdef',\n            Type='layer',\n            Description='mount / from exec /bin/sh -c apk update && apk add postgresql-dev',\n            InUse=True,\n            Shared=True,\n            Size=2048,\n            CreatedAt='2023-01-01T12:00:00Z',\n            LastUsedAt='2023-01-01T12:00:00Z',\n            UsageCount=2,\n        ),\n    ]\n    mock_kvstore.get.return_value = MagicMock(root=build_cache)\n\n    response = client.get('/site/build-cache/')\n    assert response.status_code == 200\n    assert response.headers['content-type'] == 'text/html; charset=utf-8'\n\n\n@patch('server.router.context.kvstore')\ndef test_bind_mounts(mock_kvstore, log_start_time):\n    settings.DB_DU.touch()\n    log_start_time(settings.DB_DIR, settings.TABLE_BINDMOUNTS)\n\n    bind_mounts = [\n        DockerBindMounts(\n            path='/mnt/bind',\n            err=False,\n            size=100000,\n            scan_in_progress=True,\n            last_scan='2023-01-01T12:00:00Z',\n            containers=['container1', 'container2'],\n        ),\n    ]\n    mock_kvstore.get_all.return_value = bind_mounts\n\n    response = client.get('/site/bind-mounts/')\n    assert response.status_code == 200\n    assert response.headers['content-type'] == 'text/html; charset=utf-8'\n\n\n@patch('server.router.context.kvstore')\ndef test_logs(mock_kvstore, log_start_time):\n    settings.DB_DF.touch()\n    log_start_time(settings.DB_DIR, settings.TABLE_LOGFILES)\n\n    logs = [\n        DockerContainerLog(\n            id='123456789abc',\n            name='nginx',\n            image='nginx:latest',\n            path='/var/lib/docker/containers/7d2de847bebae847b-json.log',\n            size=50000,\n            last_scan='2023-01-01T12:00:00Z',\n        ),\n    ]\n    mock_kvstore.get_all.return_value = logs\n\n    response = client.get('/site/logs/')\n    assert response.status_code == 200\n    assert response.headers['content-type'] == 'text/html; charset=utf-8'\n\n\n@patch('server.router.context.kvstore')\ndef test_overlay2(mock_kvstore, log_start_time):\n    settings.DB_DU.touch()\n    log_start_time(settings.DB_DIR, settings.TABLE_OVERLAY2)\n\n    overlay2 = [\n        DockerOverlay2Layer(\n            id='jq1br13jcumomv9j6u8rce531485cee0f83624769a2d',\n            created='2023-01-01T12:00:00Z',\n            diff_root='/usr, /etc, /opt, /var, /entrypoint.sh, /sys, /sbin, /mnt, /bitnami, /run, /lib64, /boot, /proc, /dev, /media, /lib, /srv, /tmp, /home, /bin, /root, /run.sh',\n            err=False,\n            size=5000,\n            scan_in_progress=False,\n            last_scan='2023-02-01T12:00:00Z',\n            in_use=True,\n        ),\n    ]\n    mock_kvstore.get_all.return_value = overlay2\n\n    response = client.get('/site/overlay2/')\n    assert response.status_code == 200\n    assert response.headers['content-type'] == 'text/html; charset=utf-8'\n\n\n@patch('docker.api.client.APIClient.version')\n@patch('server.router.context.kvstore')\n@patch('contrib.docker.docker_from_env')\ndef test_dashboard(\n    mock_docker_from_env,\n    mock_kvstore,\n    mock_version,\n):\n    settings.DB_DF.touch()\n    settings.DB_DU.touch()\n\n    version_data = {\n        'Platform': {'Name': 'Docker Engine - Community'},\n        'Version': '20.10.7',\n        'ApiVersion': '1.41',\n        'MinAPIVersion': '1.12',\n        'Os': 'linux',\n        'Arch': 'amd64',\n        'KernelVersion': '5.10.25-linuxkit',\n    }\n\n    mock_version.return_value = version_data\n\n    mock_client = MagicMock()\n    mock_client.version.return_value = version_data\n    mock_docker_from_env.return_value = mock_client\n\n    # Define mock data for different keys\n    mock_data = {\n        DockerImageList: [\n            DockerImage(\n                Id='sha256:123456789abcdef',\n                Created='2023-01-01T12:00:00Z',\n                ParentID='sha256:987654321fedcba',\n                RepoTags=['image:latest'],\n                SharedSize=1000,\n                Size=4000,\n                containers=[],\n            )\n        ],\n        DockerContainerList: [\n            DockerContainer(\n                Id='sha256:123456789abcdef',\n                Names=['/test_container'],\n                Image='image:latest',\n                ImageID='sha256:987654321fedcba',\n                Created='2023-01-01T12:00:00Z',\n                SizeRw=50000,\n                SizeRootFs=200000,\n                State='running',\n            )\n        ],\n        DockerVolumeList: [\n            DockerVolume(\n                Name='test_volume',\n                Driver='local',\n                CreatedAt='2023-01-01T12:00:00Z',\n                Mountpoint='/var/lib/docker/volumes/test_volume/_data',\n                Scope='local',\n                UsageData={'Size': 1000, 'RefCount': 2},\n            )\n        ],\n        DockerBuildCacheList: [\n            DockerBuildCache(\n                ID='sha256:123456789abcdef',\n                Type='layer',\n                Description='mount / from exec /bin/sh -c apk update',\n                InUse=True,\n                Shared=True,\n                Size=2048,\n                CreatedAt='2023-01-01T12:00:00Z',\n                LastUsedAt='2023-01-01T12:00:00Z',\n                UsageCount=2,\n            )\n        ],\n        DockerBindMounts: [\n            DockerBindMounts(\n                path='/mnt/bind',\n                err=False,\n                size=100000,\n                scan_in_progress=False,\n                last_scan='2023-01-01T12:00:00Z',\n                containers=['container1'],\n            )\n        ],\n        DockerContainerLog: [\n            DockerContainerLog(\n                id='123456789abc',\n                name='nginx',\n                image='nginx:latest',\n                path='/var/lib/docker/containers/123456789abc-json.log',\n                size=50000,\n                last_scan='2023-01-01T12:00:00Z',\n            )\n        ],\n        DockerOverlay2Layer: [\n            DockerOverlay2Layer(\n                id='jq1br13jcumomv9j6u8rce531',\n                created='2023-01-01T12:00:00Z',\n                diff_root='/var/lib/docker/overlay2/diff/jq1br13jcumomv9j6u8rce531',\n                err=False,\n                size=5000,\n                scan_in_progress=False,\n                last_scan='2023-01-01T12:00:00Z',\n                in_use=True,\n            )\n        ],\n        DockerMount: [\n            DockerMount(\n                Source='/host/path',\n                Destination='/container/path',\n                Mode='ro',\n                Propagation='rprivate',\n                RW=False,\n                Type='bind',\n                Root=True,\n            )\n        ],\n    }\n\n    def get_side_effect(key, kv, model):\n        return MagicMock(root=mock_data[model])\n\n    def get_all_side_effect(kv, model):\n        return mock_data[model]\n\n    mock_kvstore.get.side_effect = get_side_effect\n    mock_kvstore.get_all.side_effect = get_all_side_effect\n\n    response = client.get('/site/')\n    assert response.status_code == 200\n    assert response.headers['content-type'] == 'text/html; charset=utf-8'\n\n    assert mock_version.called\n"}
{"type": "test_file", "path": "app/contrib/tests/test_docker.py", "content": "from pathlib import Path\nfrom unittest.mock import Mock, patch\n\nimport docker\nimport pytest\n\nfrom contrib.docker import (\n    docker_from_env,\n    doku_container,\n    doku_mounts,\n    _get_mounts,\n    map_host_path_to_container,\n)\n\n\n@pytest.fixture\ndef docker_client_mock():\n    return Mock(spec=docker.DockerClient)\n\n\n@pytest.fixture\ndef container_mock():\n    container = Mock()\n    container.attrs = {\n        'Config': {'Hostname': 'test-hostname'},\n        'Mounts': [\n            {\n                'Type': 'bind',\n                'Source': '/host/path',\n                'Destination': '/container/path',\n                'Mode': 'rw',\n                'RW': True,\n                'Propagation': 'rprivate',\n            },\n            {\n                'Type': 'bind',\n                'Source': '/var/run/docker.sock',\n                'Destination': '/var/run/docker.sock',\n                'Mode': 'rw',\n                'RW': True,\n                'Propagation': 'rprivate',\n            },\n        ],\n        'LogPath': '/host/path/logs',\n    }\n    return container\n\n\n@patch('contrib.docker.docker.from_env')\n@patch('contrib.docker.settings')\ndef test_docker_from_env(settings_mock, docker_from_env_mock):\n    settings_mock.DOCKER_VERSION = '1.41'\n    settings_mock.DOCKER_TIMEOUT = 30\n    settings_mock.DOCKER_MAX_POOL_SIZE = 10\n    settings_mock.DOCKER_ENV = {'foo': 'bar'}\n    settings_mock.DOCKER_USE_SSH_CLIENT = False\n\n    docker_from_env()\n    docker_from_env_mock.assert_called_once_with(\n        version='1.41',\n        timeout=30,\n        max_pool_size=10,\n        environment={'foo': 'bar'},\n        use_ssh_client=False,\n    )\n\n\n@patch('contrib.docker.settings')\ndef test_doku_container_not_in_docker(settings_mock, docker_client_mock):\n    settings_mock.IN_DOCKER = False\n    assert doku_container(docker_client_mock) is None\n\n\n@patch('contrib.docker.settings')\ndef test_doku_container_match_hostname(settings_mock, docker_client_mock, container_mock):\n    settings_mock.IN_DOCKER = True\n    settings_mock.GITHUB_REPO = 'test/repo'\n    settings_mock.MY_HOSTNAME = 'test-hostname'\n\n    container2 = Mock()\n    container2.attrs = {'Config': {'Hostname': 'other-hostname'}}\n\n    docker_client_mock.containers.list.return_value = [container2, container_mock]\n    assert doku_container(docker_client_mock) == container_mock\n\n\ndef test_doku_mounts(docker_client_mock, container_mock):\n    docker_client_mock.containers.list.return_value = [container_mock]\n    mounts = doku_mounts(docker_client_mock)\n    assert len(mounts) == 1\n    assert mounts[0].src == '/host/path'\n    assert mounts[0].dst == '/container/path'\n    assert mounts[0].mode == 'rw'\n\n\ndef test_get_mounts(container_mock):\n    mounts = _get_mounts(container_mock)\n    assert len(mounts) == 1\n    assert mounts[0].dst == '/container/path'\n    assert mounts[0].root is False\n\n\n@patch('pathlib.Path.exists')\ndef test_map_host_path_to_container(mock_exists):\n    mock_exists.return_value = True\n    result = map_host_path_to_container('/source', '/dest', '/source/subdir/file.txt')\n    assert result == Path('/dest/subdir/file.txt')\n\n\n@patch('pathlib.Path.exists')\ndef test_map_host_path_host_mnt_prefix(mock_exists):\n    mock_exists.return_value = True\n    result = map_host_path_to_container('/host_mnt/source', '/dest', '/source/file.txt')\n    assert result == Path('/dest/file.txt')\n"}
{"type": "test_file", "path": "app/scan/tests/test_utils.py", "content": "import os\nfrom pathlib import Path\nfrom unittest.mock import patch, MagicMock\n\nimport settings\nfrom scan.utils import du_available, cpu_throttling, run_du, get_size, pretty_size\n\n\ndef test_cpu_throttling():\n    with patch('time.sleep') as mock_sleep:\n        for _ in range(100):\n            cpu_throttling(0.1)\n        mock_sleep.assert_called_once_with(0.1)\n\n\ndef test_du_available():\n    assert du_available() is True\n\n\ndef test_run_du():\n    p = Path(__file__).resolve()\n    s1 = run_du(Path(p))\n    s2 = os.path.getsize(p)\n    assert s1 == s2\n\n    with patch('subprocess.run') as mock_run:\n        mock_run.return_value = MagicMock(returncode=0, stdout='not a number')\n        assert run_du(Path(p)) == 0\n\n\ndef test_get_size():\n    p = settings.BASE_DIR\n    assert get_size(Path(p), 0.1, lambda: True, use_du=True) == 0\n    s1 = get_size(Path(p), 0.1, lambda: False, use_du=True)\n    s2 = get_size(Path(p), 0.1, lambda: False, use_du=False)\n    assert round(s1, -6) == round(s2, -6)\n\n\ndef test_pretty_size():\n    settings.SI = False\n    assert pretty_size(0) == '0'\n    assert pretty_size(1024) == '1.0 KiB'\n    assert pretty_size(1048576) == '1.0 MiB'\n    assert pretty_size(1073741824) == '1.0 GiB'\n\n    settings.SI = True\n    assert pretty_size(0) == '0'\n    assert pretty_size(1000) == '1.0 kB'\n    assert pretty_size(1000000) == '1.0 MB'\n    assert pretty_size(1000000000) == '1.0 GB'\n"}
{"type": "test_file", "path": "app/server/tests/test_context.py", "content": "from dataclasses import dataclass\n\nfrom server.router.context import total_size\n\n\n@dataclass\nclass Item:\n    size: int\n\n\n@dataclass\nclass ItemShared:\n    shared_size: int\n\n\ndef test_total_size():\n    # Test total_size function\n    items = [\n        Item(size=1),\n        Item(size=2),\n        Item(size=3),\n    ]\n    assert total_size(items) == '6 Bytes'\n\n    items = [\n        ItemShared(shared_size=1024),\n        ItemShared(shared_size=1024 * 1024),\n        ItemShared(shared_size=1024 * 1024 * 1024),\n    ]\n    assert total_size(items, field_name='shared_size') == '1.1 GB'\n\n    assert total_size([]) == '0'\n\n    assert total_size(None) == '0'\n"}
{"type": "test_file", "path": "app/scan/tests/__init__.py", "content": ""}
{"type": "test_file", "path": "app/contrib/tests/test_logger.py", "content": "import logging\n\nimport pytest\nfrom unittest.mock import patch, MagicMock\n\nfrom contrib.logger import get_logger, setup_logger, LOGGER_NAME\n\n\ndef test_get_logger_returns_logger():\n    logger = get_logger()\n    assert isinstance(logger, logging.Logger)\n    assert logger.name == LOGGER_NAME\n\n\n@pytest.mark.parametrize('in_docker', [True, False])\n@patch('contrib.logger.logging')\n@patch('contrib.logger.settings')\n@patch('contrib.logger.DefaultFormatter')\ndef test_setup_logger_configuration(\n    mock_formatter_class,\n    mock_settings,\n    mock_logging,\n    in_docker,\n):\n    mock_settings.IN_DOCKER = in_docker\n    mock_settings.LOG_LEVEL = logging.INFO\n\n    mock_handler = MagicMock()\n    mock_logging.StreamHandler.return_value = mock_handler\n\n    mock_formatter = MagicMock()\n    mock_formatter_class.return_value = mock_formatter\n\n    mock_logger = MagicMock()\n    mock_logging.getLogger.return_value = mock_logger\n\n    # Call the function\n    result = setup_logger()\n\n    # Assert the logger was configured correctly\n    mock_logging.getLogger.assert_called_once_with(LOGGER_NAME)\n    mock_logging.StreamHandler.assert_called_once()\n    mock_formatter_class.assert_called_once_with(\n        fmt='%(levelprefix)s %(message)s',\n        use_colors=not in_docker,\n    )\n    mock_handler.setFormatter.assert_called_once_with(mock_formatter)\n    mock_logger.setLevel.assert_called_once_with(mock_settings.LOG_LEVEL)\n    assert mock_logger.handlers == []\n    mock_logger.addHandler.assert_called_once_with(mock_handler)\n    assert result == mock_logger\n"}
{"type": "test_file", "path": "app/contrib/tests/test_types.py", "content": "from datetime import datetime, timezone\n\nfrom contrib.types import (\n    DockerVersion,\n    DockerImage,\n    DockerImageList,\n    DockerContainer,\n    DockerContainerList,\n    DockerVolume,\n    DockerVolumeList,\n    DockerBuildCache,\n    DockerBuildCacheList,\n    DockerSystemDF,\n    DockerMount,\n    DockerContainerLog,\n    DockerBindMounts,\n    DockerOverlay2Layer,\n    DiskUsage,\n)\n\n\ndef test_docker_version():\n    x = DockerVersion.model_validate({\n        'Platform': {'Name': 'Docker Engine - Community'},\n        'Version': '20.10.7',\n        'ApiVersion': '1.41',\n        'MinAPIVersion': '1.12',\n        'Os': 'linux',\n        'Arch': 'amd64',\n        'KernelVersion': '5.10.25-linuxkit',\n    })\n\n    assert x.platform_name == 'Docker Engine - Community'\n    assert x.version == '20.10.7'\n    assert x.api_version == '1.41'\n    assert x.min_api_version == '1.12'\n    assert x.os == 'linux'\n    assert x.arch == 'amd64'\n    assert x.kernel_version == '5.10.25-linuxkit'\n\n\ndef test_docker_image():\n    x = DockerImage.model_validate({\n        'Id': 'sha256:123456789abcdef',\n        'Created': '2023-01-01T12:00:00Z',\n        'ParentId': 'sha256:987654321fedcba',\n        'RepoTags': ['image:latest', 'image:v1'],\n        'SharedSize': 1000,\n        'Size': 5000,\n    })\n\n    assert x.id == 'sha256:123456789abcdef'\n    assert x.created == datetime(2023, 1, 1, 12, 0, tzinfo=timezone.utc)\n    assert x.parent_id == 'sha256:987654321fedcba'\n    assert x.repo_tags == ['image:latest', 'image:v1']\n    assert x.safe_repo_tags == x.repo_tags\n    assert x.shared_size == 1000\n    assert x.size == 5000\n    assert x.short_id == x.id[7:19]\n    assert isinstance(x.created_delta, str)\n    assert x.containers == []\n\n    # repo_tags = None\n    x = DockerImage.model_validate({\n        'Id': 'sha256:123456789abcdef',\n        'Created': '2023-01-01T12:00:00Z',\n        'ParentId': 'sha256:987654321fedcba',\n        'RepoTags': None,\n        'SharedSize': 1000,\n        'Size': 5000,\n    })\n\n    assert x.repo_tags is None\n    assert x.safe_repo_tags == ['<none>:<none>']\n\n    # repo_tags = []\n    x = DockerImage.model_validate({\n        'Id': 'sha256:123456789abcdef',\n        'Created': '2023-01-01T12:00:00Z',\n        'ParentId': 'sha256:987654321fedcba',\n        'RepoTags': [],\n        'SharedSize': 1000,\n        'Size': 5000,\n    })\n\n    assert x.repo_tags == []\n    assert x.safe_repo_tags == ['<none>:<none>']\n\n\ndef test_docker_image_list():\n    images = [\n        {'Id': 'sha256:123', 'Created': '2023-01-01T12:00:00Z', 'Size': 1000},\n        {'Id': 'sha256:456', 'Created': '2023-01-02T12:00:00Z', 'Size': 2000},\n    ]\n    x = DockerImageList.model_validate(images)\n\n    assert len(x.root) == 2\n    assert x[0].id == 'sha256:123'\n    assert x[1].id == 'sha256:456'\n    assert list(x) == x.root\n\n\ndef test_docker_container():\n    x = DockerContainer.model_validate({\n        'Id': '123456789abcdef',\n        'Names': ['/container_name'],\n        'Image': 'nginx:latest',\n        'ImageID': 'sha256:abcdef123456',\n        'Created': '2023-01-01T12:00:00Z',\n        'SizeRw': 50000,\n        'SizeRootFs': 200000,\n        'State': 'running',\n    })\n\n    assert x.id == '123456789abcdef'\n    assert x.names == ['/container_name']\n    assert x.image == 'nginx:latest'\n    assert x.short_image == x.image\n    assert x.image_id == 'sha256:abcdef123456'\n    assert x.created == datetime(2023, 1, 1, 12, 0, tzinfo=timezone.utc)\n    assert x.size_rw == 50000\n    assert x.size_root_fs == 200000\n    assert x.state == 'running'\n    assert x.short_id == x.id[:12]\n    assert x.clean_names == ['container_name']\n    assert isinstance(x.created_delta, str)\n\n    x = DockerContainer.model_validate({\n        'Id': '123456789abcdef',\n        'Image': 'nginx:latest',\n        'ImageID': 'sha256:abcdef123456',\n        'Created': '2023-01-01T12:00:00Z',\n        'SizeRw': 50000,\n        'SizeRootFs': 200000,\n        'State': 'running',\n    })\n    assert x.clean_names == []\n\n    x.image = 'sha256:f31294ac0a9255aa810e2cd9639485260ff9b3c19ef3263a56a091fad37e23b5'\n    assert x.short_image == 'f31294ac0a92'\n\n\ndef test_docker_container_list():\n    containers = [\n        {\n            'Id': '123',\n            'Image': 'nginx:latest',\n            'ImageID': 'sha256:abcdef123456abcde',\n            'Created': '2023-01-01T12:00:00Z',\n        },\n        {\n            'Id': '456',\n            'Image': 'redis:latest',\n            'ImageID': 'sha256:123456abcdef123456',\n            'Created': '2023-01-02T12:00:00Z',\n        },\n    ]\n    x = DockerContainerList.model_validate(containers)\n\n    assert len(x.root) == 2\n    assert x[0].id == '123'\n    assert x[1].id == '456'\n    assert list(x) == x.root\n\n\ndef test_docker_volume():\n    x = DockerVolume.model_validate({\n        'Name': 'volume1',\n        'Driver': 'local',\n        'CreatedAt': '2023-01-01T12:00:00Z',\n        'Mountpoint': '/var/lib/docker/volumes/volume1',\n        'Scope': 'local',\n        'UsageData': {'Size': 1000, 'RefCount': 2},\n    })\n\n    assert x.name == 'volume1'\n    assert x.short_name == x.name\n    assert x.driver == 'local'\n    assert x.created_at == datetime(2023, 1, 1, 12, 0, tzinfo=timezone.utc)\n    assert x.mountpoint == '/var/lib/docker/volumes/volume1'\n    assert x.scope == 'local'\n    assert x.size == 1000\n    assert x.ref_count == 2\n    assert x.containers == []\n\n    x.name = '26fb9bbf928c9a4dae3198438fb02f7b286894069dadf2827f8d3f98bbb2aa64'\n    assert x.short_name == x.name[:39] + '...'\n\n\ndef test_docker_volume_list():\n    volumes = [\n        {'Name': 'vol1', 'Driver': 'local', 'CreatedAt': '2023-01-01T12:00:00Z', 'UsageData': {}},\n        {'Name': 'vol2', 'Driver': 'local', 'CreatedAt': '2023-01-02T12:00:00Z', 'UsageData': {}},\n    ]\n    x = DockerVolumeList.model_validate(volumes)\n\n    assert len(x.root) == 2\n    assert x[0].name == 'vol1'\n    assert x[0].ref_count == 0\n    assert x[1].name == 'vol2'\n    assert x[1].size == 0\n    assert list(x) == x.root\n\n\ndef test_docker_build_cache():\n    x = DockerBuildCache.model_validate({\n        'ID': 'abcdef123456',\n        'Type': 'layer',\n        'Description': 'mount / from exec /bin/sh -c apk update && apk add ' * 10,\n        'InUse': True,\n        'Shared': False,\n        'Size': 1024,\n        'CreatedAt': '2023-01-01T12:00:00Z',\n        'LastUsedAt': '2023-02-01T12:00:00Z',\n        'UsageCount': 5,\n    })\n\n    assert x.id == 'abcdef123456'\n    assert x.type == 'layer'\n    assert x.description\n    assert x.in_use is True\n    assert x.shared is False\n    assert x.size == 1024\n    assert x.usage_count == 5\n    assert isinstance(x.last_used_delta, str)\n    assert x.short_desc == x.description[:180] + '...'\n\n\ndef test_docker_build_cache_list():\n    caches = [\n        {\n            'ID': 'cache1',\n            'Type': 'layer',\n            'Description': 'Layer cache 1',\n            'InUse': True,\n            'Shared': False,\n            'Size': 2048,\n            'CreatedAt': '2023-01-01T12:00:00Z',\n            'LastUsedAt': '2023-02-01T12:00:00Z',\n            'UsageCount': 10,\n        },\n        {\n            'ID': 'cache2',\n            'Type': 'layer',\n            'Description': 'Layer cache 2',\n            'InUse': False,\n            'Shared': True,\n            'Size': 1024,\n            'CreatedAt': '2023-01-02T12:00:00Z',\n            'LastUsedAt': '2023-02-02T12:00:00Z',\n            'UsageCount': 5,\n        },\n    ]\n\n    x = DockerBuildCacheList.model_validate(caches)\n\n    assert len(x.root) == 2\n    assert x[0].id == 'cache1'\n    assert x[1].id == 'cache2'\n    assert x[0].in_use is True\n    assert x[1].in_use is False\n    assert x[0].size == 2048\n    assert x[1].size == 1024\n    assert x[0].usage_count == 10\n    assert x[1].usage_count == 5\n    assert isinstance(x[0].last_used_delta, str)\n    assert isinstance(x[1].last_used_delta, str)\n    assert list(x) == x.root\n\n\ndef test_docker_mount():\n    x = DockerMount.model_validate({\n        'Source': '/host/path',\n        'Destination': '/container/path',\n        'Mode': 'ro',\n        'Propagation': 'rprivate',\n        'RW': False,\n        'Type': 'bind',\n        'Root': True,\n    })\n\n    assert x.source == '/host/path'\n    assert x.destination == '/container/path'\n    assert x.mode == 'ro'\n    assert x.propagation == 'rprivate'\n    assert x.rw is False\n    assert x.type == 'bind'\n    assert x.root is True\n    assert x.src == '/host/path'\n    assert x.dst == '/container/path'\n    assert x.info() == '/host/path -> /container/path:ro (root)'\n\n\ndef test_docker_container_log():\n    x = DockerContainerLog.model_validate({\n        'id': '123456789abc',\n        'name': 'nginx',\n        'image': 'nginx:latest',\n        'path': '/var/lib/docker/containers/7d2de847bebae847b-json.log',\n        'size': 50000,\n        'last_scan': '2023-01-01T12:00:00Z',\n    })\n\n    assert x.id == '123456789abc'\n    assert x.name == 'nginx'\n    assert x.image == 'nginx:latest'\n    assert x.path == '/var/lib/docker/containers/7d2de847bebae847b-json.log'\n    assert x.size == 50000\n    assert x.last_scan == datetime(2023, 1, 1, 12, 0, tzinfo=timezone.utc)\n    assert isinstance(x.short_path, str)\n    assert len(x.short_path) < len(x.path)\n\n\ndef test_docker_bind_mounts():\n    x = DockerBindMounts.model_validate({\n        'path': '/mnt/bind',\n        'err': False,\n        'size': 100000,\n        'scan_in_progress': True,\n        'last_scan': '2023-01-01T12:00:00Z',\n        'containers': ['container1', 'container2'],\n    })\n\n    assert x.path == '/mnt/bind'\n    assert x.err is False\n    assert x.size == 100000\n    assert x.scan_in_progress is True\n    assert x.containers == ['container1', 'container2']\n    assert isinstance(x.last_scan_delta, str)\n\n\ndef test_docker_overlay2_layer():\n    x = DockerOverlay2Layer.model_validate({\n        'id': 'jq1br13jcumomv9j6u8rce531485cee0f83624769a2d',\n        'created': '2023-01-01T12:00:00Z',\n        'diff_root': '/var/lib/docker/overlay2/diff/jq1br13jcumomv9j6u8rce531485cee0f83624769a2d',\n        'err': False,\n        'size': 5000,\n        'scan_in_progress': False,\n        'last_scan': '2023-02-01T12:00:00Z',\n        'in_use': True,\n    })\n\n    assert x.id == 'jq1br13jcumomv9j6u8rce531485cee0f83624769a2d'\n    assert x.diff_root == '/var/lib/docker/overlay2/diff/jq1br13jcumomv9j6u8rce531485cee0f83624769a2d'\n    assert x.err is False\n    assert x.size == 5000\n    assert x.scan_in_progress is False\n    assert x.in_use is True\n    assert x.short_id == x.id[:22] + '...'\n    assert isinstance(x.created_delta, str)\n    assert isinstance(x.last_scan_delta, str)\n\n\ndef test_disk_usage():\n    x = DiskUsage.model_validate({\n        'total': 1000000,\n        'used': 500000,\n        'free': 500000,\n        'percent': 50.0,\n    })\n\n    assert x.total == 1000000\n    assert x.used == 500000\n    assert x.free == 500000\n    assert x.percent == 50.0\n    assert isinstance(x.pretty_total, str)\n    assert isinstance(x.pretty_used, str)\n    assert isinstance(x.pretty_free, str)\n\n\ndef test_docker_system_df():\n    data = {\n        'Images': [\n            {'Id': 'sha256:img1', 'Created': '2023-01-01T12:00:00Z', 'Size': 1500},\n            {'Id': 'sha256:img2', 'Created': '2023-01-02T12:00:00Z', 'Size': 2500},\n        ],\n        'Containers': [\n            {\n                'Id': 'cont1',\n                'Image': 'nginx:latest',\n                'ImageID': 'sha256:abcdef123456abcde',\n                'Created': '2023-01-01T12:00:00Z',\n            },\n            {\n                'Id': 'cont2',\n                'Image': 'redis:latest',\n                'ImageID': 'sha256:123456abcdef123456',\n                'Created': '2023-01-02T12:00:00Z',\n            },\n        ],\n        'Volumes': [\n            {'Name': 'vol1', 'Driver': 'local', 'CreatedAt': '2023-01-01T12:00:00Z'},\n            {'Name': 'vol2', 'Driver': 'local', 'CreatedAt': '2023-01-02T12:00:00Z'},\n        ],\n        'BuildCache': [\n            {\n                'ID': 'cache1',\n                'Type': 'layer',\n                'Description': 'Layer cache 1',\n                'InUse': True,\n                'Shared': False,\n                'Size': 2048,\n                'CreatedAt': '2023-01-01T12:00:00Z',\n                'LastUsedAt': '2023-02-01T12:00:00Z',\n                'UsageCount': 10,\n            }\n        ],\n    }\n\n    x = DockerSystemDF.model_validate(data)\n\n    assert len(x.images.root) == 2\n    assert x.images[0].id == 'sha256:img1'\n    assert x.images[1].id == 'sha256:img2'\n\n    assert len(x.containers.root) == 2\n    assert x.containers[0].id == 'cont1'\n    assert x.containers[1].id == 'cont2'\n\n    assert len(x.volumes.root) == 2\n    assert x.volumes[0].name == 'vol1'\n    assert x.volumes[1].name == 'vol2'\n\n    assert len(x.build_cache.root) == 1\n    assert x.build_cache[0].id == 'cache1'\n    assert x.build_cache[0].size == 2048\n    assert x.build_cache[0].usage_count == 10\n"}
{"type": "test_file", "path": "app/server/tests/__init__.py", "content": ""}
{"type": "source_file", "path": "app/contrib/__init__.py", "content": ""}
{"type": "source_file", "path": "app/contrib/docker.py", "content": "from pathlib import Path\n\nimport docker\nfrom docker.models.containers import Container\nfrom pydantic import ValidationError\n\nimport settings\nfrom contrib.types import DockerMount\n\n\ndef docker_from_env() -> docker.DockerClient:\n    \"\"\"\n    Create a Docker client with settings from environment.\n    \"\"\"\n    client = docker.from_env(\n        version=settings.DOCKER_VERSION,\n        timeout=settings.DOCKER_TIMEOUT,\n        max_pool_size=settings.DOCKER_MAX_POOL_SIZE,\n        environment=settings.DOCKER_ENV,\n        use_ssh_client=settings.DOCKER_USE_SSH_CLIENT,\n    )\n    return client\n\n\ndef doku_container(client: docker.DockerClient) -> Container | None:\n    \"\"\"\n    Get the Doku container (current container).\n    \"\"\"\n    if not settings.IN_DOCKER:\n        return None\n\n    # filter by label github.repo=settings.GITHUB_REPO\n    filters = {'label': 'github.repo=' + settings.GITHUB_REPO}\n    c = client.containers.list(filters=filters)\n    if not c:\n        return None\n\n    # if only one container is found, return it\n    if len(c) == 1:\n        return c[0]\n\n    # if multiple containers are found, return the container with the same hostname\n    if not settings.MY_HOSTNAME:\n        return None\n\n    for cont in c:\n        attrs = cont.attrs\n        if 'Config' in attrs and 'Hostname' in attrs['Config']:\n            hostname = attrs['Config']['Hostname']\n            if hostname == settings.MY_HOSTNAME:\n                return cont\n    return None\n\n\ndef doku_mounts(client: docker.DockerClient) -> list[DockerMount]:\n    \"\"\"\n    Get all mounts of the Doku container.\n    \"\"\"\n    cont = doku_container(client)\n    return _get_mounts(cont) if cont else []\n\n\ndef _get_mounts(cont: Container) -> list[DockerMount]:\n    if 'Mounts' not in cont.attrs:\n        return []\n\n    ret = []\n    mounts = cont.attrs['Mounts']\n    log_path = cont.attrs.get('LogPath')\n\n    for mount in mounts:\n        try:\n            mnt = DockerMount.model_validate(mount)\n        except ValidationError:\n            continue\n\n        # skip mounts to docker socket\n        if mnt.dst == '/var/run/docker.sock':\n            continue\n\n        # check if the mount is a root mount\n        if log_path and map_host_path_to_container(mnt.src, mnt.dst, log_path):\n            mnt.root = True\n\n        ret.append(mnt)\n\n    return ret\n\n\ndef map_host_path_to_container(source: str, destination: str, host_path: str) -> Path | None:\n    \"\"\"\n    Map host file path to container path based on mount configuration.\n\n    Args:\n        source: Mount source path on host\n        destination: Mount destination path in container\n        host_path: Path on host to map\n    \"\"\"\n    # remove trailing slashes for consistency\n    source = source.rstrip('/')\n    destination = destination.rstrip('/')\n    host_path = host_path.rstrip('/')\n\n    # check if host path is under mount source\n    if not host_path.startswith(source):\n        # if source is /host_mnt, try to remove it and check again\n        if source.startswith('/host_mnt'):\n            source = source.removeprefix('/host_mnt')\n            return map_host_path_to_container(source, destination, host_path)\n\n        return None\n\n    # remove source prefix and join with destination\n    relative_path = host_path[len(source) :]\n    path = Path(destination + relative_path)\n    return path if path.exists() else None\n"}
{"type": "source_file", "path": "app/main.py", "content": "import ssl\nimport logging\nfrom pathlib import Path\n\nfrom fastapi import FastAPI, status\nfrom fastapi.requests import Request\nfrom fastapi.responses import HTMLResponse, RedirectResponse\nfrom fastapi.staticfiles import StaticFiles\n\nimport settings\nfrom contrib.logger import setup_logger\nfrom server.router import site\nfrom server.state import lifespan\n\n\napp = FastAPI(lifespan=lifespan, root_path=settings.ROOT_PATH)\napp.mount('/static', StaticFiles(directory=settings.STATIC_DIR), name='static')\napp.include_router(site.router)\n\n\n@app.get('/', response_class=HTMLResponse, include_in_schema=False)\nasync def index(request: Request):\n    url = request.url_for('dashboard')\n    return RedirectResponse(url=url, status_code=status.HTTP_303_SEE_OTHER)\n\n\ndef main():\n    logger = setup_logger()\n    logger.info(f'Revision: {settings.REVISION}')\n\n    if settings.LOG_LEVEL == logging.DEBUG:\n        logger.debug(settings.to_string())\n\n    kwargs = {\n        'host': settings.HOST,\n        'port': settings.PORT,\n        'workers': settings.WORKERS,\n        'reload': settings.DEBUG,\n        'log_level': settings.LOG_LEVEL,\n        'access_log': settings.DEBUG,\n        'server_header': False,\n        'ssl_cert_reqs': ssl.CERT_NONE,\n        'ssl_ca_certs': None,  # TODO: add settings.SSL_CA_CERTS\n        'ssl_ciphers': settings.SSL_CIPHERS,\n        'proxy_headers': True,\n        'forwarded_allow_ips': '*',\n    }\n\n    # enable SSL if key and cert files are provided\n    if settings.SSL_KEYFILE and settings.SSL_CERTFILE:\n        if Path(settings.SSL_KEYFILE).is_file() and Path(settings.SSL_CERTFILE).is_file():\n            kwargs.update({\n                'ssl_keyfile': settings.SSL_KEYFILE,\n                'ssl_certfile': settings.SSL_CERTFILE,\n            })\n            if settings.SSL_KEYFILE_PASSWORD:\n                kwargs['ssl_keyfile_password'] = settings.SSL_KEYFILE_PASSWORD\n\n    import uvicorn\n\n    uvicorn.run('main:app', **kwargs)\n\n\nif __name__ == '__main__':\n    main()  # pragma: no cover\n"}
{"type": "source_file", "path": "app/contrib/types.py", "content": "from datetime import datetime\n\nfrom humanize import naturaltime\nfrom pydantic import BaseModel, RootModel, Field\n\nfrom scan.utils import pretty_size\n\n\ndef truncate(name: str, limit: int) -> str:\n    if len(name) <= limit:\n        return name\n    return name[:limit] + '...'\n\n\nclass DockerVersion(BaseModel):\n    platform_name: str = Field(alias=('Platform', 'Name'), default='')\n    version: str = Field(alias='Version')\n    api_version: str = Field(alias='ApiVersion')\n    min_api_version: str = Field(alias='MinAPIVersion')\n    os: str = Field(alias='Os')\n    arch: str = Field(alias='Arch')\n    kernel_version: str = Field(alias='KernelVersion')\n\n\nclass DockerImage(BaseModel):\n    id: str = Field(alias='Id')\n    created: datetime = Field(alias='Created')\n    parent_id: str = Field(alias='ParentId', default='')\n    repo_tags: list[str] | None = Field(alias='RepoTags', default_factory=list)\n    shared_size: int = Field(alias='SharedSize', default=0)\n    size: int = Field(alias='Size', default=0)\n    containers: list[str] | None = Field(default_factory=list)\n\n    @property\n    def short_id(self) -> str:\n        s = self.id.removeprefix('sha256:')\n        return s[:12]\n\n    @property\n    def created_delta(self) -> str:\n        return naturaltime(self.created)\n\n    @property\n    def safe_repo_tags(self) -> list[str]:\n        if not self.repo_tags:  # empty list or None\n            return ['<none>:<none>']\n        return self.repo_tags\n\n\nclass DockerImageList(RootModel):\n    root: list[DockerImage]\n\n    def __iter__(self):\n        return iter(self.root)\n\n    def __getitem__(self, item):\n        return self.root[item]\n\n\nclass DockerContainer(BaseModel):\n    id: str = Field(alias='Id')\n    names: list[str] | None = Field(alias='Names', default_factory=list)\n    image: str = Field(alias='Image')\n    image_id: str = Field(alias='ImageID')\n    created: datetime = Field(alias='Created')\n    size_rw: int = Field(alias='SizeRw', default=0)\n    size_root_fs: int = Field(alias='SizeRootFs', default=0)\n    state: str = Field(alias='State', default='')\n\n    @property\n    def short_id(self) -> str:\n        return self.id[:12]\n\n    @property\n    def short_image(self) -> str:\n        if self.image.startswith('sha256:'):\n            return self.image[7:19]\n        return self.image\n\n    @property\n    def clean_names(self) -> list[str]:\n        if not self.names:\n            return []\n        return [name.lstrip('/') for name in self.names]\n\n    @property\n    def created_delta(self) -> str:\n        return naturaltime(self.created)\n\n\nclass DockerContainerList(RootModel):\n    root: list[DockerContainer]\n\n    def __iter__(self):\n        return iter(self.root)\n\n    def __getitem__(self, item):\n        return self.root[item]\n\n\nclass DockerVolume(BaseModel):\n    name: str = Field(alias='Name')\n    driver: str = Field(alias='Driver')\n    created_at: datetime = Field(alias='CreatedAt')\n    mountpoint: str = Field(alias='Mountpoint', default='')\n    scope: str = Field(alias='Scope', default='local')\n    usage_data: dict | None = Field(alias='UsageData', default_factory=dict)\n    containers: list[str] | None = Field(default_factory=list)\n\n    @property\n    def short_name(self) -> str:\n        return truncate(self.name, 39)\n\n    @property\n    def size(self) -> int:\n        if 'Size' not in self.usage_data:\n            return 0\n        return self.usage_data['Size']\n\n    @property\n    def ref_count(self) -> int:\n        if 'RefCount' not in self.usage_data:\n            return 0\n        return self.usage_data['RefCount']\n\n\nclass DockerVolumeList(RootModel):\n    root: list[DockerVolume]\n\n    def __iter__(self):\n        return iter(self.root)\n\n    def __getitem__(self, item):\n        return self.root[item]\n\n\nclass DockerBuildCache(BaseModel):\n    id: str = Field(alias='ID')\n    type: str = Field(alias='Type')\n    description: str = Field(alias='Description', default='')\n    in_use: bool = Field(alias='InUse', default=False)\n    shared: bool = Field(alias='Shared', default=False)\n    size: int = Field(alias='Size', default=0)\n    created_at: datetime = Field(alias='CreatedAt')\n    last_used: datetime = Field(alias='LastUsedAt')\n    usage_count: int = Field(alias='UsageCount', default=0)\n\n    @property\n    def last_used_delta(self) -> str:\n        return naturaltime(self.last_used)\n\n    @property\n    def short_desc(self) -> str:\n        return truncate(self.description, 180)\n\n\nclass DockerBuildCacheList(RootModel):\n    root: list[DockerBuildCache]\n\n    def __iter__(self):\n        return iter(self.root)\n\n    def __getitem__(self, item):\n        return self.root[item]\n\n\nclass DockerSystemDF(BaseModel):\n    images: DockerImageList = Field(alias='Images', default_factory=list)\n    containers: DockerContainerList = Field(alias='Containers', default_factory=list)\n    volumes: DockerVolumeList = Field(alias='Volumes', default_factory=list)\n    build_cache: DockerBuildCacheList = Field(alias='BuildCache', default_factory=list)\n\n\nclass DockerMount(BaseModel):\n    source: str = Field(alias='Source')\n    destination: str = Field(alias='Destination')\n    mode: str = Field(alias='Mode', default='')\n    propagation: str = Field(alias='Propagation', default='')\n    rw: bool = Field(alias='RW', default=False)\n    type: str = Field(alias='Type', default='')\n    root: bool = Field(alias='Root', default=False)\n\n    @property\n    def src(self) -> str:\n        return self.source\n\n    @property\n    def dst(self) -> str:\n        return self.destination\n\n    def info(self) -> str:\n        s = f'{self.src} -> {self.dst}'\n        if self.mode:\n            s += f':{self.mode}'\n        if self.root:\n            s += ' (root)'\n        return s\n\n\nclass DockerContainerLog(BaseModel):\n    id: str  # short ID of the container\n    name: str  # name of the container\n    image: str  # image of the container\n    path: str  # path to the log file\n    size: int  # size of the log file in bytes\n    last_scan: datetime  # timestamp of the last scan\n\n    @property\n    def short_path(self) -> str:\n        return self.path[:39] + '...' + self.path[-9:]\n\n\nclass DockerBindMounts(BaseModel):\n    path: str  # path to the bind mount directory\n    err: bool  # flag to indicate an error during scanning\n    size: int  # size of the bind mount directory in bytes\n    scan_in_progress: bool  # flag to indicate that the scan is in progress\n    last_scan: datetime  # timestamp of the last scan\n    containers: list[str]  # list of containers using the bind mount\n\n    @property\n    def last_scan_delta(self) -> str:\n        return naturaltime(self.last_scan)\n\n\nclass DockerOverlay2Layer(BaseModel):\n    id: str  # ID of the overlay2 layer\n    created: datetime  # timestamp of the layer creation\n    diff_root: str  # diff directory content\n    err: bool  # flag to indicate an error during scanning\n    size: int  # size of the overlay2 layer in bytes (only diff directory scanned)\n    scan_in_progress: bool  # flag to indicate that the scan is in progress\n    last_scan: datetime  # timestamp of the last scan\n    in_use: bool  # flag to indicate that the layer is in use\n\n    @property\n    def short_id(self) -> str:\n        return truncate(self.id, 22)\n\n    @property\n    def created_delta(self) -> str:\n        return naturaltime(self.created)\n\n    @property\n    def last_scan_delta(self) -> str:\n        return naturaltime(self.last_scan)\n\n\nclass DiskUsage(BaseModel):\n    total: int\n    used: int\n    free: int\n    percent: float\n\n    @property\n    def pretty_total(self) -> str:\n        return pretty_size(self.total)\n\n    @property\n    def pretty_used(self) -> str:\n        return pretty_size(self.used)\n\n    @property\n    def pretty_free(self) -> str:\n        return pretty_size(self.free)\n"}
{"type": "source_file", "path": "app/contrib/signal.py", "content": "import signal\n\n\nclass SignalHandler:\n    \"\"\"\n    Signal handler for graceful shutdown of a scanner.\n    \"\"\"\n\n    def __init__(self):\n        self.running = True\n        signal.signal(signal.SIGINT, self.handler)\n        signal.signal(signal.SIGTERM, self.handler)\n\n    def handler(self, sig, frame) -> None:\n        self.running = False\n\n    def is_stop(self) -> bool:\n        return not self.running\n"}
{"type": "source_file", "path": "app/scan/df.py", "content": "import time\n\nimport schedule\n\nimport settings\nfrom scan.scanner import SystemDFScanner, LogfilesScanner\nfrom contrib.signal import SignalHandler\nfrom contrib.logger import setup_logger\n\n\ndef main():\n    \"\"\"\n    DF scanner monitors disk space usage for Docker containers and log files.\n    \"\"\"\n    signal_ = SignalHandler()\n    logger = setup_logger()\n    logger.info('DF scanner started (system df + logfiles).')\n\n    # make sure the database file exists\n    settings.DB_DF.parent.mkdir(parents=True, exist_ok=True)\n\n    ### Docker Disk Usage Scanner ###\n    scanner = SystemDFScanner()\n    scanner.scan()  # run once immediately\n    schedule.every(settings.SCAN_INTERVAL).seconds.do(scanner.scan)\n\n    ### Logfiles Scanner ###\n    scanner = LogfilesScanner(is_stop=signal_.is_stop)\n    scanner.scan()  # run once immediately\n    schedule.every(settings.SCAN_LOGFILE_INTERVAL).seconds.do(scanner.scan)\n\n    # main loop\n    while not signal_.is_stop():\n        schedule.run_pending()\n        time.sleep(1)\n\n    logger.info('DF scanner stopped.')\n\n\nif __name__ == '__main__':\n    main()  # pragma: no cover\n"}
{"type": "source_file", "path": "app/contrib/logger.py", "content": "import logging\n\nimport settings\nfrom uvicorn.logging import DefaultFormatter\n\n\nLOGGER_NAME = 'doku'\n\n\ndef get_logger() -> logging.Logger:\n    logger = logging.getLogger(LOGGER_NAME)\n    return logger\n\n\ndef setup_logger() -> logging.Logger:\n    \"\"\"\n    Configure a logger with settings from environment.\n    \"\"\"\n    handler = logging.StreamHandler()\n\n    # get the formatter from uvicorn for similar log output\n    formatter = DefaultFormatter(\n        fmt='%(levelprefix)s %(message)s',\n        use_colors=not settings.IN_DOCKER,\n    )\n    handler.setFormatter(formatter)\n\n    logger = logging.getLogger(LOGGER_NAME)\n    logger.setLevel(settings.LOG_LEVEL)\n    logger.handlers = []  # remove default handler\n    logger.addHandler(handler)\n    return logger\n"}
{"type": "source_file", "path": "app/scan/scanner.py", "content": "import time\nimport fnmatch\nfrom collections.abc import Callable\nfrom pathlib import Path\n\nfrom docker.models.containers import Container\nfrom peewee import SqliteDatabase\nfrom playhouse.kv import KeyValue\nfrom pydantic import ValidationError\n\nimport settings\nfrom scan.utils import get_size, du_available, pretty_size\nfrom contrib import kvstore\nfrom contrib.logger import get_logger\nfrom contrib.types import (\n    DockerSystemDF,\n    DockerMount,\n    DockerContainerLog,\n    DockerBindMounts,\n    DockerOverlay2Layer,\n)\nfrom contrib.docker import (\n    docker_from_env,\n    doku_container,\n    doku_mounts,\n    map_host_path_to_container,\n)\n\n\nclass BaseScanner:\n    def __init__(self):\n        self.logger = get_logger()\n        self.client = docker_from_env()\n\n    def scan(self):\n        raise NotImplementedError\n\n    @property\n    def database_name(self):\n        raise NotImplementedError\n\n    @property\n    def table_name(self):\n        raise NotImplementedError\n\n    def log_start_time(self) -> Path:\n        filename = settings.DB_DIR / f'{self.table_name}.timestamp'\n        with filename.open('w') as fd:\n            fd.write(str(int(time.time())))\n        return filename\n\n\nclass SystemDFScanner(BaseScanner):\n    \"\"\"\n    Scans the disk usage of the Docker system. E.g. images, containers, volumes.\n    It's the equivalent of running `docker system df`.\n    \"\"\"\n\n    @property\n    def database_name(self):\n        return settings.DB_DF\n\n    @property\n    def table_name(self):\n        return settings.TABLE_SYSTEM_DF\n\n    def scan(self):\n        self.log_start_time()\n        db = SqliteDatabase(self.database_name)\n        kv = KeyValue(database=db, table_name=self.table_name)\n\n        with db:\n            start = time.perf_counter()\n            self.logger.debug('Scanning Docker disk usage (df)...')\n\n            data = self.client.df()\n            df = DockerSystemDF.model_validate(data)\n\n            # create image id -> image object mapping\n            image_map = {item.id: item for item in df.images}\n\n            # create volume name -> volume object mapping\n            volume_map = {item.name: item for item in df.volumes}\n\n            # add container name to each referenced image\n            for cont in self.client.containers.list(all=True, sparse=False):\n                image_id = cont.attrs['Image']\n                mounts = cont.attrs.get('Mounts', [])\n\n                # add container name to each referenced image\n                if image_id in image_map:\n                    image_map[image_id].containers.append(cont.name)\n\n                # filter volume mounts and update volume objects\n                volume_mounts = (\n                    mount['Name'] for mount in mounts if mount['Type'] == 'volume' and mount['Name'] in volume_map\n                )\n\n                # add container name to each referenced volume\n                for name in volume_mounts:\n                    volume_map[name].containers.append(cont.name)\n\n            kvstore.set(settings.IMAGE_KEY, df.images, kv)\n            kvstore.set(settings.CONTAINER_KEY, df.containers, kv)\n            kvstore.set(settings.VOLUME_KEY, df.volumes, kv)\n            kvstore.set(settings.BUILD_CACHE_KEY, df.build_cache, kv)\n\n            for mnt in doku_mounts(self.client):\n                if mnt.root:\n                    kvstore.set(settings.ROOT_MOUNT_KEY, mnt, kv)\n                    break\n\n            elapsed = time.perf_counter() - start\n            self.logger.info(f'Docker disk usage (df) has been analyzed. Elapsed time: {elapsed:.2f} seconds.')\n\n\nclass LogfilesScanner(BaseScanner):\n    \"\"\"\n    Scans the disk usage of log files.\n    Docker stores log files in `/var/lib/docker/containers/<container-id>/`.\n    \"\"\"\n\n    def __init__(self, is_stop: Callable[[], bool]):\n        super().__init__()\n        self.is_stop = is_stop\n        self.root_mount = self._root_mount()\n\n    def _root_mount(self) -> DockerMount | None:\n        mounts = doku_mounts(self.client)\n        root_mounts = [mnt for mnt in mounts if mnt.root]\n        if not root_mounts:\n            self.logger.error('No root mount found. Logfiles will not be scanned.')\n\n        root_mount = root_mounts[0] if root_mounts else None\n        return root_mount\n\n    @property\n    def database_name(self):\n        return settings.DB_DF\n\n    @property\n    def table_name(self):\n        return settings.TABLE_LOGFILES\n\n    def scan(self):\n        if not self.root_mount:\n            return\n\n        self.log_start_time()\n        db = SqliteDatabase(self.database_name)\n        kv = KeyValue(database=db, table_name=self.table_name)\n\n        with db:\n            total = 0\n            num = 0\n            start = time.perf_counter()\n            self.logger.debug('Scanning logfiles...')\n\n            kv.clear()  # clear previous calculations\n\n            for cont in self.client.containers.list(all=True):\n                if self.is_stop():\n                    break\n\n                cont: Container\n                if 'LogPath' not in cont.attrs:\n                    continue\n\n                id_ = cont.short_id\n                name = cont.name\n                image_id = cont.image.short_id\n                image = cont.image.tags[0] if cont.image.tags else image_id.removeprefix('sha256:')\n\n                log_path = cont.attrs['LogPath']\n\n                # map host path to doku container path (used only for size calculation)\n                path: Path | None = map_host_path_to_container(\n                    source=self.root_mount.src,\n                    destination=self.root_mount.dst,\n                    host_path=log_path,\n                )\n\n                if not path:\n                    self.logger.error(f'Logfile {log_path} of container {name} not found or not accessible.')\n                    continue\n\n                # timestamp of the last scan in seconds\n                last_scan = round(time.time())\n\n                size = path.stat().st_size  # log file size in bytes\n                total += size\n                num += 1\n\n                obj = DockerContainerLog(id=id_, name=name, image=image, path=log_path, size=size, last_scan=last_scan)\n                kvstore.set(id_, obj, kv)\n                self.logger.debug(f'Logfile of container {name} scanned. Size: {pretty_size(size)}.')\n\n            elapsed = time.perf_counter() - start\n            self.logger.info(\n                f'{num} logfiles scanned. Total size: {pretty_size(total)}. Elapsed time: {elapsed:.2f} seconds.'\n            )\n\n\nclass BindMountsScanner(BaseScanner):\n    \"\"\"\n    Scans the disk usage of bind mounts.\n    Bind mounts are used to share files between the host and a container.\n    So we need to calculate the size of the files on the host.\n    \"\"\"\n\n    def __init__(self, is_stop: Callable[[], bool]):\n        super().__init__()\n        self.is_stop = is_stop\n        self.doku_mounts = self._doku_mounts()\n\n    def _doku_mounts(self) -> list[DockerMount]:\n        mounts = doku_mounts(self.client)\n\n        if not mounts:\n            self.logger.error('Doku container mounts not found. Bind mounts will not be scanned.')\n        else:\n            self.logger.info('Doku container mounts:')\n            for mnt in mounts:\n                self.logger.info(f'  {mnt.info()}')\n        return mounts\n\n    @property\n    def database_name(self):\n        return settings.DB_DU\n\n    @property\n    def table_name(self):\n        return settings.TABLE_BINDMOUNTS\n\n    def should_ignore_path(self, path: str) -> bool:\n        \"\"\"Check if the path matches any ignore pattern.\"\"\"\n        for pattern in settings.BINDMOUNT_IGNORE_PATTERNS:\n            if fnmatch.fnmatch(path, pattern):\n                return True\n        return False\n\n    def scan(self):\n        if not self.doku_mounts:\n            return\n\n        self.log_start_time()\n        db = SqliteDatabase(self.database_name)\n        kv = KeyValue(database=db, table_name=self.table_name)\n\n        with db:\n            total = 0\n            num = 0\n            start = time.perf_counter()\n            self.logger.info('Scanning bind mounts...')\n\n            kv.clear()  # clear previous calculations\n\n            already_scanned: dict[str, DockerBindMounts] = {}  # set of processed bindmounts\n            myself = doku_container(self.client)\n\n            # loop through all containers\n            for cont in self.client.containers.list(all=True):\n                cont: Container\n\n                # skip containers without mounts\n                if 'Mounts' not in cont.attrs:\n                    continue\n\n                # skip the current container\n                if cont.id == myself.id:\n                    continue\n\n                name = cont.name\n                mounts = cont.attrs['Mounts']\n\n                # loop through all mounts of the container\n                for mount in mounts:\n                    if self.is_stop():\n                        break\n\n                    try:\n                        mnt: DockerMount = DockerMount.model_validate(mount)\n                    except ValidationError:\n                        continue\n\n                    # skip non-bind mounts\n                    if mnt.type != 'bind':\n                        continue\n\n                    # skip mounts to docker socket and secrets\n                    if mnt.dst == '/var/run/docker.sock' or mnt.dst.startswith('/run/secrets/'):\n                        continue\n\n                    # Skip paths matching ignore patterns\n                    if self.should_ignore_path(mnt.src):\n                        self.logger.debug(f'Skipping bind mount {mnt.src} as it matches an ignore pattern')\n                        continue\n\n                    if mnt.src in already_scanned:\n                        # skip already scanned bind mounts, but update the list of containers\n                        obj = already_scanned[mnt.src]\n                        obj.containers.append(name)\n                        kvstore.set(mnt.src, obj, kv)\n                        continue\n\n                    # timestamp of the last scan in seconds\n                    last_scan = round(time.time())\n\n                    obj = DockerBindMounts(\n                        containers=[name],\n                        path=mnt.src,\n                        err=False,\n                        size=0,\n                        scan_in_progress=True,  # flag to indicate that the scan is in progress\n                        last_scan=last_scan,\n                    )\n                    kvstore.set(mnt.src, obj, kv)  # for early access from the web interface\n                    already_scanned[mnt.src] = obj\n\n                    # looking for a suitable bind mount in doku_mounts\n                    for doku_mnt in self.doku_mounts:\n                        # map host path to doku container path (used only for size calculation)\n                        path: Path | None = map_host_path_to_container(\n                            source=doku_mnt.src,\n                            destination=doku_mnt.dst,\n                            host_path=mnt.src,\n                        )\n\n                        if not path:\n                            continue\n\n                        self.logger.debug(f'Start scanning bind mount {mnt.src} of container {name}...')\n                        size = get_size(\n                            path,\n                            sleep_duration=settings.SCAN_SLEEP_DURATION,\n                            is_stop=self.is_stop,\n                            use_du=settings.SCAN_USE_DU and du_available(),\n                        )\n                        total += size\n                        num += 1\n\n                        obj.size = size\n                        obj.scan_in_progress = False\n                        kvstore.set(mnt.src, obj, kv)  # update the key-value store with the final size\n                        already_scanned[mnt.src] = obj\n\n                        self.logger.debug(f'Bind mount {mnt.src} scanned. Size: {pretty_size(size)}.')\n                        # this mount is processed, no need to check other Doku mounts\n                        break\n                    else:\n                        obj.err = True\n                        obj.scan_in_progress = False\n                        kvstore.set(mnt.src, obj, kv)  # update the key-value store with the error status\n                        already_scanned[mnt.src] = obj\n                        self.logger.error(f'Bind mount {mnt.src} of container {name} not found or not accessible.')\n\n            elapsed = time.perf_counter() - start\n            self.logger.info(\n                f'{num} bind mounts scanned. Total size: {pretty_size(total)}. Elapsed time: {elapsed:.2f} seconds.'\n            )\n\n\nclass Overlay2Scanner(BaseScanner):\n    \"\"\"\n    Scans the disk usage of overlay2 storage driver.\n    Docker stores overlay2 data in `/var/lib/docker/overlay2/`.\n    \"\"\"\n\n    OVERLAY2_DIR = '/var/lib/docker/overlay2/'\n\n    def __init__(self, is_stop: Callable[[], bool]):\n        super().__init__()\n        self.is_stop = is_stop\n        self.overlay2_dir = self._overlay2_dir()\n\n    def _overlay2_dir(self) -> Path | None:\n        mounts = doku_mounts(self.client)\n        root_mounts = [mnt for mnt in mounts if mnt.root]\n        if not root_mounts:\n            self.logger.error('No root mount found. Overlay2 storage driver will not be scanned.')\n\n        root_mount = root_mounts[0] if root_mounts else None\n        if not root_mount:\n            return None\n\n        return map_host_path_to_container(\n            source=root_mount.src,\n            destination=root_mount.dst,\n            host_path=self.OVERLAY2_DIR,\n        )\n\n    @property\n    def database_name(self):\n        return settings.DB_DU\n\n    @property\n    def table_name(self):\n        return settings.TABLE_OVERLAY2\n\n    def collect_overlay2_layers(self) -> set[str]:\n        layers = []\n        graph = []\n\n        # analyze graph driver of each container and image\n        self.logger.debug('Collecting overlay2 layers from containers and images...')\n\n        for cont in self.client.containers.list(all=True):\n            if 'GraphDriver' in cont.attrs:\n                graph.append(cont.attrs['GraphDriver'])\n\n        for img in self.client.images.list(all=True):\n            if 'GraphDriver' in img.attrs:\n                graph.append(img.attrs['GraphDriver'])\n\n        for g in graph:\n            if 'Name' in g and g['Name'] == 'overlay2' and 'Data' in g:\n                for key, item in g['Data'].items():\n                    for path in item.split(':'):\n                        if path.endswith('/diff') and path.startswith(self.OVERLAY2_DIR):\n                            layers.append(Path(path).parent.name)\n\n        layers = set(layers)\n        self.logger.debug(f'Overlay2 layers: {len(layers)} collected.')\n        return layers\n\n    def scan(self):\n        if not self.overlay2_dir:\n            return\n\n        self.log_start_time()\n        db = SqliteDatabase(self.database_name)\n        kv = KeyValue(database=db, table_name=self.table_name)\n        layers = self.collect_overlay2_layers()\n\n        with db:\n            total = 0\n            num = 0\n            start = time.perf_counter()\n            self.logger.info('Scanning overlay2 storage driver...')\n\n            kv.clear()  # clear previous calculations\n\n            for path in self.overlay2_dir.iterdir():\n                if self.is_stop():\n                    break\n\n                diff_dir = path / 'diff'\n                if not diff_dir.is_dir():\n                    continue\n\n                id_ = path.name\n                short_id = id_[:12]\n                created = path.stat().st_ctime\n\n                # diff directories contain the actual data of the overlay2 layer.\n                diff_root = Path('/')\n\n                subdirs = diff_subdirs(diff_dir)\n                if len(subdirs) == 0:\n                    # empty diff directory, skip it\n                    continue\n\n                if len(subdirs) > 1:\n                    # the root has many subdirectories, list them\n                    diff_root = ', '.join('/' + x.name for x in subdirs)\n\n                while len(subdirs) == 1:\n                    # traverse the subdirectories while there is only one subdirectory on each level\n                    diff_root /= subdirs[0].name\n                    if subdirs[0].is_dir():\n                        subdirs = diff_subdirs(subdirs[0])\n                    else:\n                        break  # the last element is a file\n\n                # timestamp of the last scan in seconds\n                last_scan = round(time.time())\n\n                obj = DockerOverlay2Layer(\n                    id=id_,\n                    created=created,\n                    diff_root=str(diff_root),\n                    err=False,\n                    size=0,\n                    scan_in_progress=True,  # flag to indicate that the scan is in progress\n                    last_scan=last_scan,\n                    in_use=id_ in layers,\n                )\n                kvstore.set(id_, obj, kv)  # for early access from the web interface\n\n                try:\n                    self.logger.debug(f'Start scanning overlay2 layer {short_id}...')\n                    # only diff directories are scanned\n                    size = get_size(\n                        diff_dir,\n                        sleep_duration=settings.SCAN_SLEEP_DURATION,\n                        is_stop=self.is_stop,\n                        use_du=settings.SCAN_USE_DU and du_available(),\n                    )\n                    total += size\n                    num += 1\n\n                    obj.size = size\n                    obj.scan_in_progress = False\n                    kvstore.set(id_, obj, kv)  # update the key-value store with the final size\n                    self.logger.debug(f'Overlay2 layer {short_id} scanned. Size: {pretty_size(size)}.')\n\n                except Exception:\n                    obj.err = True\n                    obj.scan_in_progress = False\n                    kvstore.set(id_, obj, kv)\n\n            elapsed = time.perf_counter() - start\n            self.logger.info(\n                f'{num} overlay2 layers scanned. Total size: {pretty_size(total)}. Elapsed time: {elapsed:.2f} seconds.'\n            )\n\n\ndef diff_subdirs(diff_dir: Path) -> list[Path]:\n    return list(diff_dir.iterdir())\n"}
{"type": "source_file", "path": "app/scan/du.py", "content": "import time\n\nimport schedule\n\nimport settings\nfrom scan.scanner import BindMountsScanner, Overlay2Scanner\nfrom contrib.signal import SignalHandler\nfrom contrib.logger import setup_logger\n\n\ndef main():\n    \"\"\"\n    DU scanner monitors disk space usage for Docker bind mounts and Docker overlay2 directory.\n    \"\"\"\n    signal_ = SignalHandler()\n    logger = setup_logger()\n    logger.info('DU scanner started (bind mounts + overlay2).')\n\n    # make sure the database file exists\n    settings.DB_DU.parent.mkdir(parents=True, exist_ok=True)\n\n    ### Bindmounts Scanner ###\n    scanner = BindMountsScanner(is_stop=signal_.is_stop)\n    scanner.scan()  # run once immediately\n    schedule.every(settings.SCAN_BINDMOUNTS_INTERVAL).seconds.do(scanner.scan)\n\n    ### Docker Overlay2 Scanner ###\n    if settings.DISABLE_OVERLAY2_SCAN:\n        logger.warning('Overlay2 scanner disabled.')\n    else:\n        scanner = Overlay2Scanner(is_stop=signal_.is_stop)\n        scanner.scan()  # run once immediately\n        schedule.every(settings.SCAN_OVERLAY2_INTERVAL).seconds.do(scanner.scan)\n\n    # main loop\n    while not signal_.is_stop():\n        schedule.run_pending()\n        time.sleep(1)\n\n    logger.info('DU scanner stopped.')\n\n\nif __name__ == '__main__':\n    main()  # pragma: no cover\n"}
{"type": "source_file", "path": "app/contrib/kvstore.py", "content": "from pydantic import BaseModel\nfrom playhouse.kv import KeyValue\n\n\ndef set(key: str, obj: BaseModel, kv: KeyValue) -> None:\n    \"\"\"\n    Set a key in the KeyValue store\n    \"\"\"\n    kv[key] = obj.model_dump_json(by_alias=True)\n\n\ndef get[T](key: str, kv: KeyValue, model: type[T]) -> T:\n    \"\"\"\n    Get a value from the KeyValue store\n    \"\"\"\n    return model.model_validate_json(kv[key])\n\n\ndef get_all[T](kv: KeyValue, model: type[T]) -> list[T]:\n    \"\"\"\n    Get all values from the KeyValue store\n    \"\"\"\n    return [model.model_validate_json(val) for val in kv.values()]\n"}
{"type": "source_file", "path": "app/scan/__init__.py", "content": ""}
{"type": "source_file", "path": "app/scan/utils.py", "content": "import shutil\nimport subprocess\nimport time\nfrom collections.abc import Callable\nfrom pathlib import Path\nfrom subprocess import CompletedProcess\n\nfrom humanize import naturalsize\n\nimport settings\nfrom contrib.logger import get_logger\n\n\n_files_processed = 0\n\n\ndef cpu_throttling(sleep_duration: float):\n    global _files_processed\n    _files_processed += 1\n    if _files_processed % 100 == 0:  # every 100 files we sleep for a while\n        time.sleep(sleep_duration)\n\n\ndef du_available() -> bool:\n    \"\"\"\n    Check if the `du` command is available in the system.\n    \"\"\"\n    return shutil.which('du') is not None\n\n\ndef run_du(path: Path) -> int:\n    \"\"\"\n    Run the `du` command on a path and return the disk usage in bytes.\n    \"\"\"\n    res: CompletedProcess = subprocess.run(['du', '-sb', path], capture_output=True, text=True)\n    if res.returncode == 0:\n        try:\n            # output is in the format 'size path' (see -s option)\n            return int(res.stdout.split()[0])\n        except ValueError:\n            pass\n\n    # error branch\n    logger = get_logger()\n    output = repr(res.stderr or res.stdout).strip()\n    logger.debug(f\"Error running 'du' on {path}: {output}\")\n    return 0\n\n\ndef get_size(path: Path, /, sleep_duration: float, is_stop: Callable[[], bool], use_du=True) -> int:\n    \"\"\"\n    Calculate disk usage of a path in bytes (recursively).\n    Path can be a file or a directory.\n\n    Args:\n        path: Path to calculate size for\n        sleep_duration: Duration to sleep for every 100 files processed\n        is_stop: Callable to check if the process should stop\n        use_du: Whether to use 'du' command\n    \"\"\"\n    total = 0\n    if is_stop():\n        return total\n\n    match path:\n        case _ if path.is_dir(follow_symlinks=False):\n            if use_du:\n                total += run_du(path)\n                cpu_throttling(sleep_duration)\n            else:\n                for item in path.iterdir():\n                    total += get_size(\n                        item,\n                        sleep_duration=sleep_duration,\n                        is_stop=is_stop,\n                        use_du=use_du,\n                    )\n\n        case _ if path.is_file(follow_symlinks=False):\n            total += path.stat(follow_symlinks=False).st_size\n            cpu_throttling(sleep_duration)\n\n    return total\n\n\ndef pretty_size(size: int) -> str:\n    \"\"\"\n    Convert a size in bytes to a human-readable format.\n    \"\"\"\n    if size == 0:\n        return '0'\n    binary = not settings.SI\n    return naturalsize(size, binary=binary)\n"}
{"type": "source_file", "path": "app/server/__init__.py", "content": ""}
{"type": "source_file", "path": "app/server/router/site.py", "content": "from fastapi import APIRouter, Request\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.templating import Jinja2Templates\n\nimport settings\nfrom server.auth import AuthRequired, NoOpAuth\nfrom server.router import context\n\n\nif not settings.AUTH_ENABLED:\n    AuthRequired = NoOpAuth\n\n\nrouter = APIRouter(prefix='/site')\ntemplates = Jinja2Templates(directory=settings.TEMPLATES_DIR)\n\n\n@router.get('/', name='dashboard', response_class=HTMLResponse, include_in_schema=False)\ndef dashboard(request: Request, _: AuthRequired):\n    ctx = context.dashboard()\n    return templates.TemplateResponse(request=request, name='pages/dashboard.html', context=ctx)\n\n\n@router.get('/images/', name='images', response_class=HTMLResponse, include_in_schema=False)\ndef images(request: Request, _: AuthRequired):\n    ctx = context.images()\n    return templates.TemplateResponse(request=request, name='pages/images.html', context=ctx)\n\n\n@router.get('/containers/', name='containers', response_class=HTMLResponse, include_in_schema=False)\ndef containers(request: Request, _: AuthRequired):\n    ctx = context.containers()\n    return templates.TemplateResponse(request=request, name='pages/containers.html', context=ctx)\n\n\n@router.get('/volumes/', name='volumes', response_class=HTMLResponse, include_in_schema=False)\ndef volumes(request: Request, _: AuthRequired):\n    ctx = context.volumes()\n    return templates.TemplateResponse(request=request, name='pages/volumes.html', context=ctx)\n\n\n@router.get('/bind-mounts/', name='bind_mounts', response_class=HTMLResponse, include_in_schema=False)\ndef bind_mounts(request: Request, _: AuthRequired):\n    ctx = context.bind_mounts()\n    return templates.TemplateResponse(request=request, name='pages/bind_mounts.html', context=ctx)\n\n\n@router.get('/logs/', name='logs', response_class=HTMLResponse, include_in_schema=False)\ndef logs(request: Request, _: AuthRequired):\n    ctx = context.logs()\n    return templates.TemplateResponse(request=request, name='pages/logs.html', context=ctx)\n\n\n@router.get('/build-cache/', name='build_cache', response_class=HTMLResponse, include_in_schema=False)\ndef build_cache(request: Request, _: AuthRequired):\n    ctx = context.build_cache()\n    return templates.TemplateResponse(request=request, name='pages/build_cache.html', context=ctx)\n\n\n@router.get('/overlay2/', name='overlay2', response_class=HTMLResponse, include_in_schema=False)\ndef overlay2(request: Request, _: AuthRequired):\n    ctx = context.overlay2()\n    return templates.TemplateResponse(request=request, name='pages/overlay2.html', context=ctx)\n"}
{"type": "source_file", "path": "app/server/state.py", "content": "import contextlib\nfrom pathlib import Path\nfrom typing import TypedDict\n\nfrom fastapi import FastAPI\n\nimport settings\n\n\nclass State(TypedDict):\n    basic_auth_credentials: dict[str, str] | None  # username: bcrypt hash of password\n    version: str\n\n\n@contextlib.asynccontextmanager\nasync def lifespan(_: FastAPI):\n    creds = None\n\n    # load basic auth credentials from htpasswd file if provided\n    if settings.BASIC_HTPASSWD:\n        path = Path(settings.BASIC_HTPASSWD)\n        if path.is_file():\n            with path.open('r') as fd:\n                creds = dict(line.strip().split(':', 1) for line in fd)\n\n    yield State(\n        basic_auth_credentials=creds,\n        version=settings.VERSION,\n    )\n"}
{"type": "source_file", "path": "app/server/router/__init__.py", "content": ""}
{"type": "source_file", "path": "app/server/auth.py", "content": "from typing import Annotated\n\nimport bcrypt\nfrom fastapi import Request, Depends, HTTPException, status\nfrom fastapi.security import HTTPBasic, HTTPBasicCredentials\n\n\nsecurity = HTTPBasic()\n\n\ndef check_password(password: str, hashed: str) -> bool:\n    return bcrypt.checkpw(password.encode(), hashed.encode())\n\n\ndef basic_auth(request: Request, credentials: Annotated[HTTPBasicCredentials, Depends(security)]) -> str:\n    username = credentials.username\n    password = credentials.password\n\n    creds = request.state.basic_auth_credentials\n    if creds is not None:\n        if username in creds and check_password(password, creds[username]):\n            return username\n\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail='Incorrect username or password',\n            headers={'WWW-Authenticate': 'Basic'},\n        )\n\n    return credentials.username\n\n\ndef no_op_auth(request: Request) -> str:\n    return 'anonymous'\n\n\nAuthRequired = Annotated[str, Depends(basic_auth)]\nNoOpAuth = Annotated[str, Depends(no_op_auth)]\n"}
{"type": "source_file", "path": "app/server/router/context.py", "content": "from collections.abc import Sequence\nfrom datetime import datetime, UTC\nfrom operator import attrgetter\n\nimport psutil\nfrom humanize import naturaltime\nfrom peewee import SqliteDatabase\nfrom playhouse.kv import KeyValue\nfrom pydantic import BaseModel\n\nimport settings\nfrom contrib import kvstore\nfrom contrib.docker import docker_from_env\nfrom contrib.types import (\n    DockerVersion,\n    DockerImageList,\n    DockerContainerList,\n    DockerVolumeList,\n    DockerBuildCacheList,\n    DockerMount,\n    DockerContainerLog,\n    DockerBindMounts,\n    DockerOverlay2Layer,\n    DiskUsage,\n)\nfrom scan.utils import pretty_size\n\n\ndef total_size(items: Sequence | None, field_name='size') -> str:\n    if items is None:\n        return '0'\n    total = sum(map(attrgetter(field_name), items))\n    return pretty_size(total)\n\n\ndef last_scan_time(table_name: str) -> tuple[datetime, str] | None:\n    filename = settings.DB_DIR / f'{table_name}.timestamp'\n    if filename.is_file():\n        with filename.open('r') as fd:\n            ts = int(fd.read().strip())\n            dt = datetime.fromtimestamp(ts, UTC)\n            return dt, naturaltime(dt)\n    return None\n\n\ndef last_df_scan_time() -> tuple[datetime, str] | None:\n    return last_scan_time(settings.TABLE_SYSTEM_DF)\n\n\ndef images() -> dict:\n    items = None\n\n    if settings.DB_DF.exists():\n        db = SqliteDatabase(settings.DB_DF)\n        with db:\n            kv = KeyValue(database=db, table_name=settings.TABLE_SYSTEM_DF)\n            if settings.IMAGE_KEY in kv:\n                items = kvstore.get(settings.IMAGE_KEY, kv, DockerImageList).root\n\n    context = {\n        'name': 'images',\n        'items': items,\n        'total': total_size(items, field_name='shared_size'),\n        'si': settings.SI,\n        'last_scan_at': last_df_scan_time(),\n    }\n    return context\n\n\ndef containers() -> dict:\n    items = None\n\n    if settings.DB_DF.exists():\n        db = SqliteDatabase(settings.DB_DF)\n        with db:\n            kv = KeyValue(database=db, table_name=settings.TABLE_SYSTEM_DF)\n            if settings.CONTAINER_KEY in kv:\n                items = kvstore.get(settings.CONTAINER_KEY, kv, DockerContainerList).root\n\n    context = {\n        'name': 'containers',\n        'items': items,\n        'total': total_size(items, field_name='size_rw'),\n        'si': settings.SI,\n        'last_scan_at': last_df_scan_time(),\n    }\n    return context\n\n\ndef volumes() -> dict:\n    items = None\n\n    if settings.DB_DF.exists():\n        db = SqliteDatabase(settings.DB_DF)\n        with db:\n            kv = KeyValue(database=db, table_name=settings.TABLE_SYSTEM_DF)\n            if settings.VOLUME_KEY in kv:\n                items = kvstore.get(settings.VOLUME_KEY, kv, DockerVolumeList).root\n\n    context = {\n        'name': 'volumes',\n        'items': items,\n        'total': total_size(items),\n        'si': settings.SI,\n        'last_scan_at': last_df_scan_time(),\n    }\n    return context\n\n\ndef build_cache() -> dict:\n    items = None\n\n    if settings.DB_DF.exists():\n        db = SqliteDatabase(settings.DB_DF)\n        with db:\n            kv = KeyValue(database=db, table_name=settings.TABLE_SYSTEM_DF)\n            if settings.BUILD_CACHE_KEY in kv:\n                items = kvstore.get(settings.BUILD_CACHE_KEY, kv, DockerBuildCacheList).root\n\n    if items:\n        for item in items:\n            item.last_used = item.last_used.replace(microsecond=0)\n\n    context = {\n        'name': 'build cache',\n        'items': items,\n        'total': total_size(items),\n        'si': settings.SI,\n        'last_scan_at': last_df_scan_time(),\n    }\n    return context\n\n\ndef bind_mounts() -> dict:\n    items = None\n\n    if settings.DB_DU.exists():\n        db = SqliteDatabase(settings.DB_DU)\n        with db:\n            kv = KeyValue(database=db, table_name=settings.TABLE_BINDMOUNTS)\n            items = kvstore.get_all(kv, DockerBindMounts)\n\n    context = {\n        'name': 'bind mounts',\n        'items': items,\n        'total': total_size(items),\n        'si': settings.SI,\n        'last_scan_at': last_scan_time(settings.TABLE_BINDMOUNTS),\n    }\n    return context\n\n\ndef logs() -> dict:\n    items = None\n\n    if settings.DB_DF.exists():\n        db = SqliteDatabase(settings.DB_DF)\n        with db:\n            kv = KeyValue(database=db, table_name=settings.TABLE_LOGFILES)\n            items = kvstore.get_all(kv, DockerContainerLog)\n\n    context = {\n        'name': 'logs',\n        'items': items,\n        'total': total_size(items),\n        'si': settings.SI,\n        'last_scan_at': last_scan_time(settings.TABLE_LOGFILES),\n    }\n    return context\n\n\ndef overlay2() -> dict:\n    items = None\n\n    if settings.DB_DU.exists():\n        db = SqliteDatabase(settings.DB_DU)\n        with db:\n            kv = KeyValue(database=db, table_name=settings.TABLE_OVERLAY2)\n            items = kvstore.get_all(kv, DockerOverlay2Layer)\n\n    context = {\n        'name': 'overlay2',\n        'items': items,\n        'total': total_size(items),\n        'si': settings.SI,\n        'last_scan_at': last_scan_time(settings.TABLE_OVERLAY2),\n    }\n    return context\n\n\nclass Summary(BaseModel):\n    num: int = 0\n    total_size: int\n    pretty_total_size: str\n\n\ndef summary(db: SqliteDatabase) -> dict[str, Summary]:\n    r = {}\n\n    # retrieve images, containers, volumes, and build cache\n    kv = KeyValue(database=db, table_name=settings.TABLE_SYSTEM_DF)\n    types = {\n        settings.IMAGE_KEY: DockerImageList,\n        settings.CONTAINER_KEY: DockerContainerList,\n        settings.VOLUME_KEY: DockerVolumeList,\n        settings.BUILD_CACHE_KEY: DockerBuildCacheList,\n    }\n    for key, value in types.items():\n        if key in kv:\n            items = kvstore.get(key, kv, value).root\n            if key == settings.IMAGE_KEY:\n                field_name = 'shared_size'\n            elif key == settings.CONTAINER_KEY:\n                field_name = 'size_rw'\n            else:\n                field_name = 'size'\n\n            total_size = sum(map(attrgetter(field_name), items))\n            r[key] = Summary(\n                num=len(items),\n                total_size=total_size,\n                pretty_total_size=pretty_size(total_size),\n            )\n\n    # retrieve logs\n    kv = KeyValue(database=db, table_name=settings.TABLE_LOGFILES)\n    key = settings.TABLE_LOGFILES\n    items = kvstore.get_all(kv, DockerContainerLog)\n    total_size = sum(map(attrgetter('size'), items))\n    r[key] = Summary(\n        num=len(items),\n        total_size=total_size,\n        pretty_total_size=pretty_size(total_size),\n    )\n    return r\n\n\ndef overlay2_summary(db: SqliteDatabase) -> dict[str, Summary]:\n    r = {}\n\n    kv = KeyValue(database=db, table_name=settings.TABLE_OVERLAY2)\n    layers = kvstore.get_all(kv, DockerOverlay2Layer)\n\n    in_use = [layer for layer in layers if layer.in_use]\n    not_in_use = [layer for layer in layers if not layer.in_use]\n\n    in_use_size = sum(map(attrgetter('size'), in_use))\n    not_in_use_size = sum(map(attrgetter('size'), not_in_use))\n\n    r['overlay2_in_use'] = Summary(\n        num=len(in_use),\n        total_size=in_use_size,\n        pretty_total_size=pretty_size(in_use_size),\n    )\n    r['overlay2_not_in_use'] = Summary(\n        num=len(not_in_use),\n        total_size=not_in_use_size,\n        pretty_total_size=pretty_size(not_in_use_size),\n    )\n    r['overlay2'] = Summary(\n        num=len(layers),\n        total_size=in_use_size + not_in_use_size,\n        pretty_total_size=pretty_size(in_use_size + not_in_use_size),\n    )\n    return r\n\n\ndef disk_usage() -> DiskUsage:\n    du = psutil.disk_usage('/')\n\n    if settings.DB_DF.exists():\n        db = SqliteDatabase(settings.DB_DF)\n        with db:\n            # retrieve the root mount point\n            kv = KeyValue(database=db, table_name=settings.TABLE_SYSTEM_DF)\n            if settings.ROOT_MOUNT_KEY in kv:\n                root_mount = kvstore.get(settings.ROOT_MOUNT_KEY, kv, DockerMount)\n                _du = psutil.disk_usage(root_mount.destination)\n                if _du.total > du.total:\n                    du = _du\n\n    return DiskUsage(\n        total=du.total,\n        used=du.used,\n        free=du.free,\n        percent=du.percent,\n    )\n\n\ndef dashboard() -> dict:\n    items = {}\n    client = docker_from_env()\n    version = DockerVersion.model_validate(client.version())\n\n    if settings.DB_DF.exists():\n        db = SqliteDatabase(settings.DB_DF)\n        with db:\n            # retrieve images, containers, volumes, build cache and logs\n            items = summary(db)\n\n    # pretty print the total size of each type\n    total_size = sum(map(attrgetter('total_size'), items.values()))\n    items['total'] = Summary(\n        total_size=total_size,\n        pretty_total_size=pretty_size(total_size),\n    )\n\n    if settings.DB_DU.exists():\n        db = SqliteDatabase(settings.DB_DU)\n        with db:\n            # retrieve bind mounts and overlay2 layers\n            items |= overlay2_summary(db)\n\n    context = {\n        'name': 'dashboard',\n        'si': settings.SI,\n        'version': version,\n        'disk_usage': disk_usage(),\n    } | items\n    return context\n"}
{"type": "source_file", "path": "app/settings.py", "content": "import logging\nfrom enum import Enum\nfrom functools import cached_property\nfrom pathlib import Path\n\nfrom docker import constants as docker\nfrom dotenv import load_dotenv\nfrom pydantic import Field, PositiveInt, ValidationError, field_validator\nfrom pydantic_settings import BaseSettings\n\n\nload_dotenv()  # take environment variables from .env.\n\n\nclass LogLevel(str, Enum):\n    DEBUG = 'debug'\n    INFO = 'info'\n    WARNING = 'warning'\n    ERROR = 'error'\n    CRITICAL = 'critical'\n\n\nclass ScanIntensity(str, Enum):\n    AGGRESSIVE = 'aggressive'\n    NORMAL = 'normal'\n    LIGHT = 'light'\n\n\nclass Settings(BaseSettings):\n    # general settings\n    host: str = Field(alias='HOST', default='0.0.0.0', description='Interface address to bind the server to')\n    port: PositiveInt = Field(alias='PORT', default=9090, description='Web interface port number')\n    in_docker: bool = Field(alias='IN_DOCKER', default=False)\n    log_level: LogLevel = Field(alias='LOG_LEVEL', default=LogLevel.INFO, description='Logging detail level')\n    github_repo: str = Field(alias='GITHUB_REPO', default='amerkurev/doku')\n    my_hostname: str = Field(alias='HOSTNAME', default='')  # it is set by the container automatically\n    si: bool = Field(\n        alias='SI', default=True, description='Use SI units (base 1000) instead of binary units (base 1024)'\n    )\n    basic_htpasswd: str = Field(\n        alias='BASIC_HTPASSWD', default='/.htpasswd', description='Path to the htpasswd file for basic authentication'\n    )\n    root_path: str = Field(\n        alias='ROOT_PATH', default='', description='URL prefix when served behind a proxy (e.g., \"/doku\")'\n    )\n\n    # ssl settings\n    ssl_keyfile: str = Field(alias='SSL_KEYFILE', default='/.ssl/key.pem')\n    ssl_keyfile_password: str | None = Field(alias='SSL_KEYFILE_PASSWORD', default=None)\n    ssl_certfile: str = Field(alias='SSL_CERTFILE', default='/.ssl/cert.pem')\n    ssl_ciphers: str = Field(alias='SSL_CIPHERS', default='TLSv1')\n\n    # scan settings\n    scan_interval: PositiveInt = Field(\n        alias='SCAN_INTERVAL', default=60, description='How often to collect basic Docker usage data (in seconds)'\n    )\n    scan_logfile_interval: PositiveInt = Field(\n        alias='SCAN_LOGFILE_INTERVAL',\n        default=60 * 5,\n        description='How frequently to check container log sizes (in seconds)',\n    )\n    scan_bindmounts_interval: PositiveInt = Field(\n        alias='SCAN_BINDMOUNTS_INTERVAL',\n        default=60 * 60,\n        description='Time between bind mount scanning operations (in seconds)',\n    )\n    bindmount_ignore_patterns: str = Field(\n        alias='BINDMOUNT_IGNORE_PATTERNS',\n        default='',\n        examples=['/home/*;/tmp/*;*/.git/*'],\n        description='Paths matching these patterns will be excluded from bind mount scanning (semicolon-separated)',\n    )\n    scan_overlay2_interval: PositiveInt = Field(\n        alias='SCAN_OVERLAY2_INTERVAL',\n        default=60 * 60 * 24,\n        description='How often to analyze Overlay2 storage (in seconds)',\n    )\n    disable_overlay2_scan: bool = Field(\n        alias='DISABLE_OVERLAY2_SCAN',\n        default=False,\n        description='Disable Overlay2 storage scanning',\n    )\n    scan_intensity: ScanIntensity = Field(\n        alias='SCAN_INTENSITY',\n        default=ScanIntensity.NORMAL,\n        description='Performance impact level: \"aggressive\" (highest CPU usage), \"normal\" (balanced), or \"light\" (lowest impact)',\n    )\n    scan_use_du: bool = Field(\n        alias='SCAN_USE_DU',\n        default=True,\n        description='Use the faster system `du` command for disk calculations instead of slower built-in methods',\n    )\n\n    # uvicorn settings\n    workers: PositiveInt = Field(\n        alias='UVICORN_WORKERS', default=1, description='Number of web server worker processes'\n    )\n    debug: bool = Field(alias='DEBUG', default=False, description='Enable debug mode')\n\n    # docker daemon settings\n    docker_host: str = Field(\n        alias='DOCKER_HOST',\n        default='unix:///var/run/docker.sock',\n        description='Connection string for the Docker daemon',\n    )\n    docker_tls_verify: bool = Field(\n        alias='DOCKER_TLS_VERIFY', default=False, description='Enable TLS verification for Docker daemon connection'\n    )\n    docker_cert_path: str | None = Field(\n        alias='DOCKER_CERT_PATH', default=None, description='Directory containing Docker TLS certificates'\n    )\n    docker_version: str = Field(alias='DOCKER_VERSION', default='auto', description='Docker API version to use')\n    docker_timeout: PositiveInt = Field(\n        alias='DOCKER_TIMEOUT',\n        default=docker.DEFAULT_TIMEOUT_SECONDS,\n        description='Timeout in seconds for Docker API requests',\n    )\n    docker_max_pool_size: PositiveInt = Field(\n        alias='DOCKER_MAX_POOL_SIZE',\n        default=docker.DEFAULT_MAX_POOL_SIZE,\n        description='Maximum number of connections in the Docker API connection pool',\n    )\n    docker_use_ssh_client: bool = Field(\n        alias='DOCKER_USE_SSH_CLIENT',\n        default=False,\n        description='Use SSH for Docker daemon connection instead of HTTP/HTTPS',\n    )\n\n    # version settings\n    git_tag: str = Field(alias='GIT_TAG', default='v0.0.0')\n    git_sha: str = Field(alias='GIT_SHA', default='')\n\n    @field_validator('log_level', mode='before')\n    def lowercase_log_level(cls, v):\n        if isinstance(v, str):\n            return v.lower()\n        return v\n\n    @cached_property\n    def log_level_num(self) -> int:\n        level_map = {\n            'debug': logging.DEBUG,\n            'info': logging.INFO,\n            'warning': logging.WARNING,\n            'error': logging.ERROR,\n            'critical': logging.CRITICAL,\n        }\n        return level_map[self.log_level]\n\n    @cached_property\n    def bindmount_ignore_patterns_list(self) -> list[str]:\n        patterns = self.bindmount_ignore_patterns\n\n        # Remove surrounding quotes from the entire string if present\n        if (patterns.startswith('\"') and patterns.endswith('\"')) or (\n            patterns.startswith(\"'\") and patterns.endswith(\"'\")\n        ):\n            patterns = patterns[1:-1]\n\n        # Split and filter empty values\n        result = list(filter(None, map(str.strip, patterns.split(';'))))\n        return result\n\n\ntry:\n    _settings = Settings()\nexcept ValidationError as err:\n    raise SystemExit(err) from err\n\n\nVERSION = _settings.git_tag\nREVISION = f'{_settings.git_tag}-{_settings.git_sha[:7]}'\n\n# general settings\nHOST = _settings.host\nPORT = _settings.port\nIN_DOCKER = _settings.in_docker\nLOG_LEVEL = _settings.log_level_num\nGITHUB_REPO = _settings.github_repo\nMY_HOSTNAME = _settings.my_hostname\nSI = _settings.si\nBASIC_HTPASSWD = _settings.basic_htpasswd\nAUTH_ENABLED = Path(BASIC_HTPASSWD).exists()\nROOT_PATH = _settings.root_path\n\n# ssl settings\nSSL_KEYFILE = _settings.ssl_keyfile\nSSL_KEYFILE_PASSWORD = _settings.ssl_keyfile_password\nSSL_CERTFILE = _settings.ssl_certfile\nSSL_CIPHERS = _settings.ssl_ciphers\n\n# scan settings\nSCAN_INTERVAL = _settings.scan_interval\nSCAN_LOGFILE_INTERVAL = _settings.scan_logfile_interval\nSCAN_BINDMOUNTS_INTERVAL = _settings.scan_bindmounts_interval\nBINDMOUNT_IGNORE_PATTERNS = _settings.bindmount_ignore_patterns_list\nSCAN_OVERLAY2_INTERVAL = _settings.scan_overlay2_interval\nDISABLE_OVERLAY2_SCAN = _settings.disable_overlay2_scan\nSCAN_INTENSITY = _settings.scan_intensity\nSCAN_SLEEP_DURATION = {\n    ScanIntensity.AGGRESSIVE: 0,  # no sleep, but CPU throttling\n    ScanIntensity.NORMAL: 0.001,  # 1ms\n    ScanIntensity.LIGHT: 0.01,  # 10ms\n}[ScanIntensity(_settings.scan_intensity)]\nSCAN_USE_DU = _settings.scan_use_du\n\n# uvicorn settings\nWORKERS = _settings.workers\nDEBUG = _settings.debug\n\n# docker daemon settings\nDOCKER_HOST = _settings.docker_host\nDOCKER_TLS_VERIFY = _settings.docker_tls_verify\nDOCKER_CERT_PATH = _settings.docker_cert_path\nDOCKER_VERSION = _settings.docker_version\nDOCKER_TIMEOUT = _settings.docker_timeout\nDOCKER_MAX_POOL_SIZE = _settings.docker_max_pool_size\nDOCKER_USE_SSH_CLIENT = _settings.docker_use_ssh_client\nDOCKER_ENV = {\n    'DOCKER_HOST': DOCKER_HOST,\n    'DOCKER_TLS_VERIFY': DOCKER_TLS_VERIFY or '',  # see kwargs_from_env in docker.from_env\n    'DOCKER_CERT_PATH': DOCKER_CERT_PATH,\n}\n\n# paths\nBASE_DIR = Path(__file__).resolve().parent\nROOT_DIR = BASE_DIR.parent\nTEMPLATES_DIR = BASE_DIR / 'templates'\nSTATIC_DIR = BASE_DIR / 'static'\nDB_DIR = BASE_DIR / 'db'\nDB_DU = DB_DIR / 'du.sqlite3'\nDB_DF = DB_DIR / 'df.sqlite3'\nTABLE_LOGFILES = 'logfiles'\nTABLE_BINDMOUNTS = 'bindmounts'\nTABLE_SYSTEM_DF = 'system_df'\nTABLE_OVERLAY2 = 'overlay2'\nIMAGE_KEY = 'image'\nCONTAINER_KEY = 'container'\nVOLUME_KEY = 'volume'\nBUILD_CACHE_KEY = 'build_cache'\nROOT_MOUNT_KEY = 'root_mount'\n\n\ndef to_string() -> str:\n    \"\"\"Return a formatted string with all settings for logging purposes.\"\"\"\n    settings_dict = _settings.model_dump()\n    # Hide sensitive information\n    if _settings.ssl_keyfile_password:\n        settings_dict['ssl_keyfile_password'] = '********'\n\n    lines = []\n    lines.append('Doku settings:')\n\n    # Group settings by categories for better readability\n    categories = {\n        'General settings': [\n            'host',\n            'port',\n            'in_docker',\n            'log_level',\n            'github_repo',\n            'my_hostname',\n            'si',\n            'basic_htpasswd',\n            'root_path',\n        ],\n        'SSL settings': ['ssl_keyfile', 'ssl_keyfile_password', 'ssl_certfile', 'ssl_ciphers'],\n        'Scan settings': [\n            'scan_interval',\n            'scan_logfile_interval',\n            'scan_bindmounts_interval',\n            'bindmount_ignore_patterns',\n            'scan_overlay2_interval',\n            'disable_overlay2_scan',\n            'scan_intensity',\n            'scan_use_du',\n        ],\n        'Uvicorn settings': ['workers', 'debug'],\n        'Docker settings': [\n            'docker_host',\n            'docker_tls_verify',\n            'docker_cert_path',\n            'docker_version',\n            'docker_timeout',\n            'docker_max_pool_size',\n            'docker_use_ssh_client',\n        ],\n        'Version info': ['git_tag', 'git_sha'],\n    }\n\n    for category, keys in categories.items():\n        lines.append(f'\\n{category}:')\n        for key in keys:\n            if key in settings_dict:\n                lines.append(f'  {key}: {settings_dict[key]}')\n\n    return '\\n'.join(lines)\n"}
