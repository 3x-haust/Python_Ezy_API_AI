{"repo_info": {"repo_name": "odoo-expert", "repo_owner": "MFYDev", "repo_url": "https://github.com/MFYDev/odoo-expert"}}
{"type": "source_file", "path": "__init__.py", "content": ""}
{"type": "source_file", "path": "src/api/app.py", "content": "from contextlib import asynccontextmanager\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom src.config.settings import settings\nfrom src.core.services.db_service import DatabaseService\nfrom .routes import chat_router\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup: Create and verify database connection\n    db_service = DatabaseService()\n    if not await db_service.check_health():\n        raise RuntimeError(\"Failed to connect to database\")\n    \n    yield  # Server is running and handling requests\n    \n    # Shutdown: Cleanup\n    await db_service.close()\n\ndef create_app() -> FastAPI:\n    \"\"\"Create and configure the FastAPI application.\"\"\"\n    app = FastAPI(\n        title=settings.API_TITLE,\n        description=settings.API_DESCRIPTION,\n        version=settings.API_VERSION,\n        lifespan=lifespan\n    )\n    \n    # Configure CORS\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=settings.cors_origins_list,\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n    \n    # Register routers\n    app.include_router(chat_router, prefix=\"/api\")\n    \n    return app\n\napp = create_app()"}
{"type": "source_file", "path": "docker/healthcheck.py", "content": "#!/usr/bin/env python3\nimport sys\nimport urllib.request\nimport urllib.error\nimport subprocess\nimport os\nimport asyncio\nfrom src.core.services.db_service import DatabaseService\nfrom src.utils.logging import logger\n\ndef check_database():\n    \"\"\"Check database connectivity.\"\"\"\n    try:\n        db = DatabaseService()\n        return asyncio.run(db.check_health())\n    except Exception as e:\n        logger.error(f\"Database healthcheck failed: {e}\")\n        return False\n\ndef check_service(port: int, path: str = None) -> bool:\n    \"\"\"Check if a service is running on the specified port.\"\"\"\n    try:\n        # For API service (port 8000), check the OpenAPI docs endpoint\n        if port == 8000:\n            url = f\"http://localhost:{port}/docs\"\n        # For Streamlit (port 8501), check the root path\n        elif port == 8501:\n            url = f\"http://localhost:{port}\"\n        else:\n            url = f\"http://localhost:{port}\"\n            if path:\n                url = f\"{url}/{path.lstrip('/')}\"\n\n        with urllib.request.urlopen(url, timeout=5) as response:\n            return response.getcode() == 200\n    except Exception as e:\n        logger.error(f\"Service healthcheck failed for port {port}: {e}\")\n        return False\n\ndef check_supervisor():\n    \"\"\"Check if supervisor processes are running.\"\"\"\n    try:\n        result = subprocess.run(\n            [\"supervisorctl\", \"status\"], \n            capture_output=True, \n            text=True,\n            check=True\n        )\n        return all(\"RUNNING\" in line for line in result.stdout.splitlines())\n    except Exception as e:\n        logger.error(f\"Supervisor healthcheck failed: {e}\")\n        return False\n\ndef main():\n    \"\"\"Run all health checks.\"\"\"\n    try:\n        # Check all services\n        checks = {\n            \"UI\": check_service(8501),\n            \"API\": check_service(8000),\n            \"Database\": check_database(),\n            \"Supervisor\": check_supervisor()\n        }\n        \n        # Log results\n        for service, status in checks.items():\n            logger.info(f\"{service} health check: {'PASSED' if status else 'FAILED'}\")\n        \n        # Exit with appropriate status\n        if all(checks.values()):\n            sys.exit(0)\n        else:\n            failed_services = [svc for svc, status in checks.items() if not status]\n            logger.error(f\"Health check failed for services: {', '.join(failed_services)}\")\n            sys.exit(1)\n            \n    except Exception as e:\n        logger.error(f\"Health check failed with error: {e}\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "src/api/dependencies/__init__.py", "content": "from .auth import verify_token\n\n__all__ = ['verify_token']"}
{"type": "source_file", "path": "main.py", "content": "import uvicorn\nimport argparse\nimport asyncio\nimport subprocess\nfrom pathlib import Path\nfrom src.api.app import app\nfrom src.processing.document_processor import DocumentProcessor\nfrom src.processing.markdown_converter import MarkdownConverter\nfrom src.processing.file_update_handler import FileUpdateHandler\nfrom src.core.services.embedding import EmbeddingService\nfrom src.core.services.db_service import DatabaseService\nfrom src.config.settings import settings\nfrom openai import AsyncOpenAI\nfrom src.utils.logging import logger\n\nasync def process_documents(base_dir: str):\n    \"\"\"Process markdown documents to embeddings\"\"\"\n    openai_client = AsyncOpenAI(\n        api_key=settings.OPENAI_API_KEY,\n        base_url=settings.OPENAI_API_BASE\n    )\n    db_service = DatabaseService()\n    embedding_service = EmbeddingService(openai_client)\n    processor = DocumentProcessor(db_service, embedding_service)\n    await processor.process_directory(base_dir)\n\nasync def process_raw_data(raw_dir: str, output_dir: str, process_docs: bool = False):\n    \"\"\"Process raw RST files to markdown and optionally process documents\n    \n    Args:\n        raw_dir (str): Directory containing raw RST files\n        output_dir (str): Output directory for markdown files\n        process_docs (bool): Whether to process documents after conversion\n    \"\"\"\n    # Step 1: Convert RST to Markdown\n    converter = MarkdownConverter()\n    converter.process_directory(raw_dir, output_dir)\n    \n    # Step 2: Process markdown files to documents (optional)\n    if process_docs:\n        await process_documents(output_dir)\n\nasync def check_updates(raw_dir: str, markdown_dir: str):\n    \"\"\"Check for updates in raw data and process changed files.\n\n    Args:\n        raw_dir (str): Raw data directory\n        markdown_dir (str): Markdown data directory\n\n    Returns:\n        Added, modified, removed files\n    \"\"\"\n    openai_client = AsyncOpenAI(\n        api_key=settings.OPENAI_API_KEY,\n        base_url=settings.OPENAI_API_BASE\n    )\n    \n    db_service = DatabaseService()\n    embedding_service = EmbeddingService(openai_client)\n    document_processor = DocumentProcessor(db_service, embedding_service)\n    markdown_converter = MarkdownConverter()\n    \n    update_handler = FileUpdateHandler(\n        document_processor=document_processor,\n        markdown_converter=markdown_converter\n    )\n    \n    added, modified, removed = await update_handler.check_and_process_updates(\n        raw_dir=raw_dir,\n        markdown_dir=markdown_dir\n    )\n    \n    logger.info(f\"Added files: {len(added)}\")\n    logger.info(f\"Modified files: {len(modified)}\")\n    logger.info(f\"Removed files: {len(removed)}\")\n    \n    return added, modified, removed\n\nif __name__ == \"__main__\":\n\n    parser = argparse.ArgumentParser(description='Odoo Documentation Assistant')\n    subparsers = parser.add_subparsers(dest='command', help='Commands')\n    \n    # Server command\n    server_parser = subparsers.add_parser('serve', help='Run the server')\n    server_parser.add_argument('--mode', choices=['api', 'ui'], required=True,\n                             help='Run mode: api for FastAPI server or ui for Streamlit interface')\n    server_parser.add_argument('--host', default='0.0.0.0', help='Host to run the server on')\n    server_parser.add_argument('--port', type=int, default=8000, help='Port to run the server on')\n    \n    # Process commands\n    process_raw_parser = subparsers.add_parser('process-raw', help='Process raw RST files')\n    process_raw_parser.add_argument('--process-docs', action='store_true',\n                                  help='Process documents after conversion')\n    \n    process_docs_parser = subparsers.add_parser('process-docs', help='Process markdown documents')\n\n    # Add check-updates command\n    check_updates_parser = subparsers.add_parser('check-updates', \n                                                help='Check and process updated files')\n    \n    args = parser.parse_args()\n    \n    if args.command == 'serve':\n        if args.mode == 'api':\n            uvicorn.run(app, host=args.host, port=args.port)\n        elif args.mode == 'ui':\n            subprocess.run([\"streamlit\", \"run\", \"src/ui/streamlit_app.py\"])\n    elif args.command == 'process-raw':\n        asyncio.run(process_raw_data(settings.RAW_DATA_DIR, settings.MARKDOWN_DATA_DIR, args.process_docs))\n    elif args.command == 'process-docs':\n        async def run_sequential():\n            # First run check-updates to generate file cache\n            await check_updates(settings.RAW_DATA_DIR, settings.MARKDOWN_DATA_DIR)\n            # Then process the documents\n            await process_documents(settings.MARKDOWN_DATA_DIR)\n        \n        asyncio.run(run_sequential())\n    elif args.command == 'check-updates':\n        asyncio.run(check_updates(settings.RAW_DATA_DIR, settings.MARKDOWN_DATA_DIR))\n    else:\n        parser.print_help()\n"}
{"type": "source_file", "path": "src/api/models/__init__.py", "content": "from .chat import ChatRequest, ChatResponse\n\n__all__ = ['ChatRequest', 'ChatResponse']"}
{"type": "source_file", "path": "src/api/dependencies/auth.py", "content": "# src/api/dependencies/auth.py\nfrom fastapi import Security, HTTPException\nfrom fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\nfrom src.config.settings import settings\n\nsecurity = HTTPBearer()\n\ndef verify_token(credentials: HTTPAuthorizationCredentials = Security(security)) -> bool:\n    \"\"\"Verify the API token.\"\"\"\n    if credentials.credentials not in settings.bearer_tokens_list:\n        raise HTTPException(\n            status_code=401,\n            detail=\"Invalid API token\"\n        )\n    return True"}
{"type": "source_file", "path": "src/api/__init__.py", "content": "from .routes.chat import router as chat_router\n\n__all__ = ['chat_router']"}
{"type": "source_file", "path": "src/api/routes/__init__.py", "content": "from .chat import router as chat_router\n\n__all__ = ['chat_router']"}
{"type": "source_file", "path": "src/processing/document_processor.py", "content": "import asyncio\nimport json\nimport re\nfrom pathlib import Path\nfrom typing import Dict, Any, Set\nfrom datetime import datetime, timezone\nfrom src.core.services.embedding import EmbeddingService\nfrom src.utils.logging import logger\nfrom src.core.services.db_service import DatabaseService\nfrom .markdown_converter import MarkdownConverter\nfrom src.config.settings import settings\n\n\nclass DocumentProcessor:\n    def __init__(\n        self,\n        db_service: DatabaseService,\n        embedding_service: EmbeddingService\n    ):\n        self.db_service = db_service\n        self.embedding_service = embedding_service\n        self.markdown_converter = MarkdownConverter()\n        self.progress_file = Path(\"processing_progress.json\")\n    \n    def _load_progress(self) -> Dict[str, Set[str]]:\n        \"\"\"Load processing progress from file.\"\"\"\n        if self.progress_file.exists():\n            with open(self.progress_file, 'r') as f:\n                progress = json.load(f)\n                # Convert lists back to sets\n                return {k: set(v) for k, v in progress.items()}\n        return {}\n\n    def _save_progress(self, progress: Dict[str, Set[str]]):\n        \"\"\"Save processing progress to file.\"\"\"\n        # Convert sets to lists for JSON serialization\n        progress_json = {k: list(v) for k, v in progress.items()}\n        with open(self.progress_file, 'w') as f:\n            json.dump(progress_json, f)\n\n    async def process_chunk(\n        self,\n        chunk: Dict[str, Any],\n        chunk_number: int,\n        file_path: str,\n        version: int\n    ):\n        try:\n            # Get the header path from metadata\n            header_path = chunk[\"metadata\"].get(\"header_path\", \"\")\n            \n            # Get document URL - only use the URL part, not the version\n            documentation_url, _ = self.markdown_converter.convert_path_to_url(\n                file_path,\n                header_path\n            )\n            \n            # Extract title\n            title = self.extract_title_from_chunk(chunk)\n            \n            # Get embedding\n            embedding = await self.embedding_service.get_embedding(chunk[\"content\"])\n            \n            # Prepare metadata\n            metadata = {\n                \"source\": \"markdown_file\",\n                \"chunk_size\": len(chunk[\"content\"]),\n                \"processed_at\": datetime.now(timezone.utc).isoformat(),\n                \"filename\": Path(file_path).name,\n                \"version_str\": f\"{version/10:.1f}\",\n                **chunk[\"metadata\"]\n            }\n            \n            # Insert into database\n            return await self._insert_chunk({\n                \"url\": documentation_url,  # Now only contains the URL string\n                \"chunk_number\": chunk_number,\n                \"title\": title,\n                \"content\": chunk[\"content\"],\n                \"metadata\": metadata,\n                \"embedding\": embedding,\n                \"version\": version\n            })\n            \n        except Exception as e:\n            logger.error(f\"Error processing chunk: {e}\")\n            raise\n\n    async def process_file(self, file_path: str, version: int):\n        \"\"\"Process individual file with chunk tracking.\"\"\"\n        try:\n            logger.info(f\"Processing file: {file_path}\")\n            \n            # Read and chunk the markdown file\n            chunks = self.markdown_converter.chunk_markdown(file_path)\n            logger.info(f\"Split into {len(chunks)} chunks\")\n            \n            # Process chunks with retries\n            for i, chunk in enumerate(chunks):\n                max_retries = 3\n                retry_delay = 1\n                \n                for attempt in range(max_retries):\n                    try:\n                        await self.process_chunk(chunk, i, file_path, version)\n                        break\n                    except Exception as e:\n                        if attempt == max_retries - 1:\n                            raise\n                        logger.warning(f\"Retry {attempt + 1}/{max_retries} for chunk {i} due to: {e}\")\n                        await asyncio.sleep(retry_delay * (attempt + 1))\n            \n            logger.info(f\"Successfully processed {file_path}\")\n            \n        except Exception as e:\n            logger.error(f\"Error processing file {file_path}: {e}\")\n            raise\n\n    async def process_directory(self, base_directory: str):\n        \"\"\"Process directory with progress tracking.\"\"\"\n        progress = self._load_progress()\n        \n        try:\n            version_dirs = settings.odoo_versions_list\n            for version_str in version_dirs:\n                version = int(float(version_str) * 10)\n                version_path = Path(base_directory) / \"versions\" / version_str\n                \n                if not version_path.exists():\n                    logger.warning(f\"Version directory {version_path} does not exist\")\n                    continue\n                \n                # Initialize progress tracking for this version if not exists\n                if version_str not in progress:\n                    progress[version_str] = set()\n                \n                logger.info(f\"Processing version {version_str}\")\n                \n                # Get all markdown files\n                markdown_files = list(version_path.rglob(\"*.md\"))\n                logger.info(f\"Found {len(markdown_files)} markdown files\")\n                \n                # Process unprocessed files\n                for file_path in markdown_files:\n                    file_str = str(file_path)\n                    if file_str in progress[version_str]:\n                        logger.info(f\"Skipping already processed file: {file_str}\")\n                        continue\n                    \n                    try:\n                        await self.process_file(file_str, version)\n                        progress[version_str].add(file_str)\n                        self._save_progress(progress)\n                        logger.info(f\"Successfully processed and saved progress for {file_str}\")\n                    except Exception as e:\n                        logger.error(f\"Error processing file {file_str}: {e}\")\n                        # Don't save progress for failed file\n                        raise\n                        \n        except Exception as e:\n            logger.error(f\"Error processing directory {base_directory}: {e}\")\n            raise\n        finally:\n            # Ensure progress is saved even if there's an error\n            self._save_progress(progress)\n\n    async def _insert_chunk(self, chunk_data: Dict[str, Any]):\n        try:\n            result = await self.db_service.insert_document(chunk_data)\n            logger.info(\n                f\"Inserted chunk {chunk_data['chunk_number']} \"\n                f\"(version {chunk_data['metadata']['version_str']}): \"\n                f\"{chunk_data['title']}\"\n            )\n            return result\n        except Exception as e:\n            logger.error(f\"Error inserting chunk: {e}\")\n            raise\n    \n    def extract_title_from_chunk(self, chunk: Dict[str, Any]) -> str:\n        \"\"\"Extract a title from a chunk of text.\n\n        Args:\n            chunk (Dict[str, Any]): Dictionary containing content and metadata for a chunk\n\n        Returns:\n            str: Extracted title from the chunk\n        \"\"\"\n        # First try to use the header path if available\n        if \"header_path\" in chunk[\"metadata\"] and chunk[\"metadata\"][\"header_path\"]:\n            return chunk[\"metadata\"][\"header_path\"]\n        \n        # Then try individual headers from metadata\n        metadata = chunk[\"metadata\"]\n        for header_level in range(1, 5):\n            header_key = f\"Header {header_level}\"\n            if header_key in metadata and metadata[header_key]:\n                return metadata[header_key]\n        \n        # Remove header path from content if present\n        content = chunk[\"content\"]\n        content_lines = content.split(\"\\n\")\n        if len(content_lines) > 0 and \"[#\" in content_lines[0] and \" > \" in content_lines[0]:\n            content = \"\\n\".join(content_lines[1:])\n        \n        # Try to find headers in remaining content\n        header_match = re.search(r'^#+\\s+(.+)$', content, re.MULTILINE)\n        if header_match:\n            return header_match.group(1)\n        \n        # Final fallback to first line of actual content\n        first_line = content.split('\\n')[0].strip()\n        if len(first_line) > 100:\n            return first_line[:97] + \"...\"\n        return first_line\n    \n    async def process_chunk_with_update(\n        self,\n        chunk: Dict[str, Any],\n        chunk_number: int,\n        file_path: str,\n        version: int\n    ):\n        \"\"\"Process a chunk and update if it exists, otherwise insert.\"\"\"\n        try:\n            # Get document URL - only use the URL part, not the version\n            documentation_url, _ = self.markdown_converter.convert_path_to_url(\n                file_path, \n                chunk[\"metadata\"].get(\"header_path\", \"\")\n            )\n            \n            # Extract filename for matching\n            filename = Path(file_path).name\n            version_str = f\"{version/10:.1f}\"\n            \n            # Extract title\n            title = self.extract_title_from_chunk(chunk)\n            \n            # Get embedding\n            embedding = await self.embedding_service.get_embedding(chunk[\"content\"])\n            \n            # Prepare metadata\n            metadata = {\n                \"source\": \"markdown_file\",\n                \"chunk_size\": len(chunk[\"content\"]),\n                \"processed_at\": datetime.now(timezone.utc).isoformat(),\n                \"filename\": filename,\n                \"version_str\": version_str,\n                **chunk[\"metadata\"]\n            }\n            \n            try:\n                # Delete existing records based on metadata\n                await self.db_service.delete_document_by_metadata(filename, version_str)\n                \n                # Prepare record data\n                document = {\n                    \"url\": documentation_url,\n                    \"chunk_number\": chunk_number,\n                    \"title\": title,\n                    \"content\": chunk[\"content\"],\n                    \"metadata\": metadata,\n                    \"embedding\": embedding,\n                    \"version\": version\n                }\n                \n                # Insert new record\n                result = await self.db_service.insert_document(document)\n                \n                logger.info(\n                    f\"Processed chunk {chunk_number} \"\n                    f\"(version {metadata['version_str']}): \"\n                    f\"{title}\"\n                )\n                \n                return result\n                \n            except Exception as e:\n                logger.warning(f\"Operation failed: {e}\")\n                raise\n                \n        except Exception as e:\n            logger.error(f\"Error processing chunk: {e}\")\n            raise\n\n    async def _delete_existing_record(\n        self,\n        url: str,\n        chunk_number: int,\n        version: int\n    ) -> None:\n        \"\"\"Delete an existing record if it exists.\"\"\"\n        try:\n            await self.db_service.delete_document(url, chunk_number, version)\n            await asyncio.sleep(0.5)  # Keep the delay for safety\n            logger.debug(f\"Deleted existing record for URL: {url}, chunk: {chunk_number}, version: {version}\")\n        except Exception as e:\n            raise Exception(f\"Error in delete operation: {e}\")\n\n    async def process_file_with_update(self, file_path: str, version: int):\n        \"\"\"Process a markdown file and update existing records if they exist.\"\"\"\n        try:\n            logger.info(f\"Processing file with update: {file_path}\")\n            \n            # Read and chunk the markdown file\n            chunks = self.markdown_converter.chunk_markdown(file_path)\n            logger.info(f\"Split into {len(chunks)} chunks\")\n            \n            # Process chunks sequentially to avoid race conditions\n            for i, chunk in enumerate(chunks):\n                await self.process_chunk_with_update(chunk, i, file_path, version)\n            \n            logger.info(f\"Successfully processed {file_path}\")\n            \n        except Exception as e:\n            logger.error(f\"Error processing file {file_path}: {e}\")\n            raise"}
{"type": "source_file", "path": "src/api/models/chat.py", "content": "from pydantic import BaseModel, Field\nfrom typing import List, Dict, Optional\n\nclass Source(BaseModel):\n    url: str = Field(..., description=\"URL of the source document\")\n    title: str = Field(..., description=\"Title of the source document\")\n\nclass ChatRequest(BaseModel):\n    query: str = Field(..., description=\"The user's question\")\n    version: int = Field(..., description=\"Odoo version number (e.g., 160 for 16.0)\")\n    conversation_history: Optional[List[Dict[str, str]]] = Field(\n        default=[],\n        description=\"Previous conversation turns\"\n    )\n\nclass ChatResponse(BaseModel):\n    answer: str = Field(..., description=\"Generated response\")\n    sources: List[Source] = Field(..., description=\"Source documents used for the response\")"}
{"type": "source_file", "path": "src/core/services/chat_service.py", "content": "from typing import List, Dict, Optional, Tuple\nfrom openai import AsyncOpenAI\nfrom src.core.services.embedding import EmbeddingService\nfrom src.core.services.db_service import DatabaseService\nfrom src.config.settings import settings\nfrom src.utils.logging import logger\n\nclass ChatService:\n    def __init__(\n        self,\n        openai_client: AsyncOpenAI,\n        db_service: DatabaseService,\n        embedding_service: EmbeddingService\n    ):\n        self.openai_client = openai_client\n        self.db_service = db_service\n        self.embedding_service = embedding_service\n\n    async def retrieve_relevant_chunks(\n        self,\n        query: str,\n        version: int,\n        limit: int = 6\n    ) -> List[Dict]:\n        try:\n            query_embedding = await self.embedding_service.get_embedding(query)\n            chunks = await self.db_service.search_documents(\n                query_embedding,\n                version,\n                limit\n            )\n            return chunks\n        except Exception as e:\n            logger.error(f\"Error retrieving chunks: {e}\")\n            raise\n\n    def prepare_context(self, chunks: List[Dict]) -> Tuple[str, List[Dict[str, str]]]:\n        \"\"\"Prepare context and sources from retrieved chunks.\"\"\"\n        context_parts = []\n        sources = []\n        \n        for i, chunk in enumerate(chunks, 1):\n            source_info = (\n                f\"Context:\\n\"\n                f\"Document: {chunk['url']}\\n\"\n                f\"Title: {chunk['title']}\\n\"\n                f\"Content: {chunk['content']}\"\n            )\n            context_parts.append(source_info)\n            sources.append({\n                \"url\": chunk[\"url\"],\n                \"title\": chunk[\"title\"]\n            })\n        \n        return \"\\n\\n---\\n\\n\".join(context_parts), sources\n\n    async def generate_response(\n        self,\n        query: str,\n        context: str,\n        conversation_history: Optional[List[Dict]] = None,\n        stream: bool = False\n    ):\n        \"\"\"Generate AI response based on query and context.\"\"\"\n        try:\n            messages = [\n                {\n                    \"role\": \"system\",\n                    \"content\": settings.SYSTEM_PROMPT\n                }\n            ]\n            \n            if conversation_history:\n                history_text = \"\\n\".join([\n                    f\"User: {msg['user']}\\nAssistant: {msg['assistant']}\"\n                    for msg in conversation_history[-3:]\n                ])\n                messages.append({\n                    \"role\": \"user\",\n                    \"content\": f\"Previous conversation:\\n{history_text}\"\n                })\n            \n            messages.append({\n                \"role\": \"user\",\n                \"content\": f\"Question: {query}\\n\\nRelevant documentation:\\n{context}\"\n            })\n            \n            response = await self.openai_client.chat.completions.create(\n                model=settings.LLM_MODEL,\n                messages=messages,\n                stream=stream\n            )\n            \n            if stream:\n                return response\n            return response.choices[0].message.content\n            \n        except Exception as e:\n            logger.error(f\"Error generating response: {e}\")\n            raise"}
{"type": "source_file", "path": "src/core/services/__init__.py", "content": "from .chat_service import ChatService\nfrom .embedding import EmbeddingService\n\n__all__ = ['ChatService', 'EmbeddingService']"}
{"type": "source_file", "path": "src/core/models/chat.py", "content": "from pydantic import BaseModel, Field\nfrom typing import Dict, Any, List\n\nclass DocumentChunk(BaseModel):\n    url: str\n    title: str\n    content: str\n    embedding: List[float]\n    metadata: Dict[str, Any]\n    version: int\n\nclass ConversationTurn(BaseModel):\n    user: str\n    assistant: str\n    timestamp: str"}
{"type": "source_file", "path": "src/core/services/embedding.py", "content": "from typing import List\nfrom openai import AsyncOpenAI\nfrom src.utils.logging import logger\n\nclass EmbeddingService:\n    def __init__(self, client: AsyncOpenAI):\n        self.client = client\n\n    async def get_embedding(self, text: str) -> List[float]:\n        try:\n            text = text.replace(\"\\n\", \" \")\n            if len(text) > 8000:\n                text = text[:8000] + \"...\"\n                \n            response = await self.client.embeddings.create(\n                model=\"text-embedding-3-small\",\n                input=text\n            )\n            return response.data[0].embedding\n        except Exception as e:\n            logger.error(f\"Error getting embedding: {e}\")\n            raise"}
{"type": "source_file", "path": "src/config/__init__.py", "content": "from .settings import settings\n\n__all__ = ['settings']"}
{"type": "source_file", "path": "src/config/settings.py", "content": "from pathlib import Path\nfrom typing import List\nfrom pydantic_settings import BaseSettings\n\nclass Settings(BaseSettings):\n    # API Settings\n    API_VERSION: str = \"0.0.1\"\n    API_TITLE: str = \"Odoo Expert API\"\n    API_DESCRIPTION: str = \"API for querying Odoo documentation with RAG-powered responses\"\n    \n    # OpenAI Settings\n    OPENAI_API_KEY: str\n    OPENAI_API_BASE: str\n    LLM_MODEL: str = \"gpt-4o\"\n\n    # PostgreSQL Settings\n    POSTGRES_USER: str = \"postgres\"\n    POSTGRES_PASSWORD: str = \"postgres\"\n    POSTGRES_DB: str = \"odoo_expert\"\n    POSTGRES_HOST: str = \"localhost\"\n    POSTGRES_PORT: int = 5432\n    \n    # Security\n    BEARER_TOKEN: str = \"\"\n    CORS_ORIGINS: str = \"*\"\n    \n    # Odoo Settings\n    ODOO_VERSIONS: str = \"16.0,17.0,18.0\"\n    \n    # Chat Settings\n    SYSTEM_PROMPT: str\n    \n    # Paths\n    PROJECT_ROOT: Path = Path(__file__).parent.parent.parent\n    LOGS_DIR: Path = PROJECT_ROOT / \"logs\"\n    RAW_DATA_DIR: str = \"raw_data\"\n    MARKDOWN_DATA_DIR: str = \"markdown\"\n    \n    @property\n    def bearer_tokens_list(self) -> List[str]:\n        if not self.BEARER_TOKEN:\n            return []\n        return [x.strip() for x in self.BEARER_TOKEN.split(',') if x.strip()]\n    \n    @property\n    def cors_origins_list(self) -> List[str]:\n        if self.CORS_ORIGINS == \"*\":\n            return [\"*\"]\n        return [x.strip() for x in self.CORS_ORIGINS.split(',') if x.strip()]\n    \n    @property\n    def odoo_versions_list(self) -> List[str]:\n        return [x.strip() for x in self.ODOO_VERSIONS.split(',') if x.strip()]\n    \n    class Config:\n        env_file = \".env\"\n\nsettings = Settings()\n"}
{"type": "source_file", "path": "src/processing/__init__.py", "content": "from .document_processor import DocumentProcessor\nfrom .markdown_converter import MarkdownConverter\nfrom .file_update_handler import FileUpdateHandler\n\n__all__ = ['DocumentProcessor', 'MarkdownConverter', 'FileUpdateHandler']"}
{"type": "source_file", "path": "src/core/__init__.py", "content": "from .services.chat_service import ChatService\nfrom .services.embedding import EmbeddingService\n\n__all__ = ['ChatService', 'EmbeddingService']"}
{"type": "source_file", "path": "src/core/models/__init__.py", "content": "from .chat import DocumentChunk, ConversationTurn\n\n__all__ = ['DocumentChunk', 'ConversationTurn']"}
{"type": "source_file", "path": "src/core/services/db_service.py", "content": "from typing import Dict, List, Any, Optional\nimport json\nimport psycopg\nfrom psycopg_pool import ConnectionPool\nfrom tenacity import retry, stop_after_attempt, wait_exponential\nfrom src.config.settings import settings\nfrom tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\nfrom src.utils.logging import logger\n\n_db_service: Optional['DatabaseService'] = None\n\ndef get_db_service() -> 'DatabaseService':\n    \"\"\"Get or create singleton DatabaseService instance.\"\"\"\n    global _db_service\n    if _db_service is None:\n        _db_service = DatabaseService()\n    return _db_service\n\nclass DatabaseService:\n    def __init__(self):\n        self.pool = None\n        self.init_pool()\n\n    def init_pool(self):\n        \"\"\"Initialize the connection pool with retry logic.\"\"\"\n        try:\n            conn_params = {\n                \"dbname\": settings.POSTGRES_DB,\n                \"user\": settings.POSTGRES_USER,\n                \"password\": settings.POSTGRES_PASSWORD,\n                \"host\": settings.POSTGRES_HOST,\n                \"port\": settings.POSTGRES_PORT,\n            }\n            \n            logger.info(\"Connection parameters:\")\n            debug_params = conn_params.copy()\n            debug_params[\"password\"] = \"****\"\n            logger.info(f\"Parameters: {debug_params}\")\n\n            self.pool = ConnectionPool(\n                conninfo=\" \".join([f\"{k}={v}\" for k, v in conn_params.items()]),\n                min_size=1,\n                max_size=10,\n                timeout=30\n            )\n        except Exception as e:\n            logger.error(f\"Failed to initialize connection pool: {e}\")\n            raise\n\n    async def close(self):\n        \"\"\"Close the connection pool.\"\"\"\n        if self.pool:\n            self.pool.close()\n\n    @retry(\n        stop=stop_after_attempt(3),\n        wait=wait_exponential(multiplier=1, min=4, max=10),\n        retry=retry_if_exception_type((psycopg.OperationalError, psycopg.InterfaceError))\n    )\n    async def check_health(self) -> bool:\n        \"\"\"Check database connectivity.\"\"\"\n        try:\n            with self.pool.connection() as conn:\n                with conn.cursor() as cur:\n                    cur.execute(\"SELECT 1\")\n                    return True\n        except Exception as e:\n            logger.error(f\"Database health check failed: {e}\")\n            return False\n\n    @retry(\n        stop=stop_after_attempt(3),\n        wait=wait_exponential(multiplier=1, min=4, max=10),\n        retry=retry_if_exception_type((psycopg.OperationalError, psycopg.InterfaceError))\n    )\n    async def search_documents(\n        self,\n        query_embedding: List[float],\n        version: int,\n        limit: int = 6\n    ) -> List[Dict[str, Any]]:\n        try:\n            with self.pool.connection() as conn:\n                with conn.cursor() as cur:\n                    query = \"\"\"\n                    WITH ranked_docs AS (\n                        SELECT \n                            url,\n                            title,\n                            content,\n                            1 - (embedding <=> %s::vector) as similarity\n                        FROM odoo_docs\n                        WHERE version = %s\n                        ORDER BY similarity DESC\n                        LIMIT %s\n                    )\n                    SELECT \n                        url,\n                        title,\n                        content,\n                        similarity\n                    FROM ranked_docs;\n                    \"\"\"\n                    \n                    # Log the search parameters\n                    logger.info(f\"Searching documents for version {version} with limit {limit}\")\n                    \n                    cur.execute(query, (query_embedding, version, limit))\n                    results = cur.fetchall()\n                    columns = [desc[0] for desc in cur.description]\n                    return [dict(zip(columns, row)) for row in results]\n                    \n        except Exception as e:\n            logger.error(f\"Error searching documents: {e}\")\n            raise\n\n    async def insert_document(self, document: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Insert a document into the database.\"\"\"\n        try:\n            with self.pool.connection() as conn:\n                with conn.cursor() as cur:\n                    logger.info(f\"Inserting document with URL: {document['url']}\")\n                    \n                    # Convert metadata to JSON string\n                    metadata_json = json.dumps(document['metadata'])\n                    \n                    query = \"\"\"\n                        INSERT INTO odoo_docs (\n                            url, chunk_number, version, title,\n                            content, metadata, embedding\n                        ) VALUES (\n                            %s, %s, %s, %s, %s, %s::jsonb, %s\n                        )\n                        RETURNING *\n                    \"\"\"\n                    \n                    # Pass parameters as a tuple\n                    params = (\n                        document['url'],\n                        document['chunk_number'],\n                        document['version'],\n                        document['title'],\n                        document['content'],\n                        metadata_json,\n                        document['embedding']\n                    )\n                    \n                    cur.execute(query, params)\n                    conn.commit()\n                    \n                    result = cur.fetchone()\n                    columns = [desc[0] for desc in cur.description]\n                    return dict(zip(columns, result))\n                    \n        except Exception as e:\n            logger.error(f\"Error inserting document: {e}\")\n            raise\n\n    async def update_document(self, document: Dict[str, Any]) -> Dict[str, Any]:\n        try:\n            with self.pool.connection() as conn:\n                with conn.cursor() as cur:\n                    cur.execute(\n                        \"\"\"\n                        UPDATE odoo_docs\n                        SET title = $1, content = $2, metadata = $3, embedding = $4\n                        WHERE url = $5 AND chunk_number = $6 AND version = $7\n                        RETURNING *\n                        \"\"\",\n                        (\n                            document[\"title\"],\n                            document[\"content\"],\n                            document[\"metadata\"],\n                            document[\"embedding\"],\n                            document[\"url\"],\n                            document[\"chunk_number\"],\n                            document[\"version\"]\n                        )\n                    )\n                    conn.commit()\n                    result = cur.fetchone()\n                    columns = [desc[0] for desc in cur.description]\n                    return dict(zip(columns, result))\n        except Exception as e:\n            logger.error(f\"Error updating document: {e}\")\n            raise\n\n    async def delete_document(self, url: str, chunk_number: int, version: int):\n        try:\n            with self.pool.connection() as conn:\n                with conn.cursor() as cur:\n                    cur.execute(\n                        \"\"\"\n                        DELETE FROM odoo_docs\n                        WHERE url = $1 AND chunk_number = $2 AND version = $3\n                        \"\"\",\n                        (url, chunk_number, version)\n                    )\n                    conn.commit()\n        except Exception as e:\n            logger.error(f\"Error deleting document: {e}\")\n            raise\n    \n    async def delete_document_by_metadata(self, filename: str, version_str: str):\n        \"\"\"Delete documents matching metadata criteria.\"\"\"\n        try:\n            with self.pool.connection() as conn:\n                with conn.cursor() as cur:\n                    cur.execute(\n                        \"\"\"\n                        DELETE FROM odoo_docs\n                        WHERE metadata->>'filename' = %s\n                        AND metadata->>'version_str' = %s\n                        \"\"\",\n                        (filename, version_str)\n                    )\n                    conn.commit()\n        except Exception as e:\n            logger.error(f\"Error deleting documents by metadata: {e}\")\n            raise\n\n"}
{"type": "source_file", "path": "src/processing/file_update_handler.py", "content": "import os\nimport hashlib\nimport asyncio\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, Set, Tuple\nimport json\nfrom src.utils.logging import logger\nfrom src.processing.markdown_converter import MarkdownConverter\nfrom src.processing.document_processor import DocumentProcessor\nfrom src.config.settings import settings\n\nclass FileUpdateHandler:\n    def __init__(\n        self,\n        document_processor: DocumentProcessor,\n        markdown_converter: MarkdownConverter,\n        cache_file: str = None\n    ):\n        # Use a persistent location for the cache file\n        if cache_file is None:\n            # Store in the project root directory\n            project_root = Path(__file__).parent.parent.parent\n            self.cache_file = str(project_root / '.file_cache.json')\n        else:\n            self.cache_file = cache_file\n            \n        self.document_processor = document_processor\n        self.markdown_converter = markdown_converter\n        self.file_cache = self._load_cache()\n        logger.info(f\"Using cache file: {self.cache_file}\")\n        logger.info(f\"Current cache has {len(self.file_cache)} files\")\n\n    def _load_cache(self) -> Dict[str, str]:\n        \"\"\"Load the file cache from disk.\"\"\"\n        try:\n            if os.path.exists(self.cache_file):\n                with open(self.cache_file, 'r') as f:\n                    cache = json.load(f)\n                    logger.info(f\"Loaded existing cache with {len(cache)} entries\")\n                    return cache\n            logger.info(\"No existing cache found\")\n            return {}\n        except Exception as e:\n            logger.error(f\"Error loading cache: {e}\")\n            return {}\n\n    def _save_cache(self):\n        \"\"\"Save the file cache to disk.\"\"\"\n        try:\n            # Ensure directory exists\n            os.makedirs(os.path.dirname(self.cache_file), exist_ok=True)\n            with open(self.cache_file, 'w') as f:\n                json.dump(self.file_cache, f)\n            logger.info(f\"Saved cache with {len(self.file_cache)} entries\")\n        except Exception as e:\n            logger.error(f\"Error saving cache: {e}\")\n\n    def _get_file_hash(self, filepath: str) -> str:\n        \"\"\"Calculate MD5 hash of a file.\"\"\"\n        try:\n            with open(filepath, 'rb') as f:\n                return hashlib.md5(f.read()).hexdigest()\n        except Exception as e:\n            logger.error(f\"Error calculating hash for {filepath}: {e}\")\n            return \"\"\n\n    def _get_version_from_path(self, filepath: str) -> int:\n        \"\"\"Extract version number from file path.\"\"\"\n        path = Path(filepath)\n        version_str = path.parts[path.parts.index('versions') + 1]\n        return int(float(version_str) * 10)\n\n    async def check_and_process_updates(\n        self,\n        raw_dir: str,\n        markdown_dir: str\n    ) -> Tuple[Set[str], Set[str], Set[str]]:\n        \"\"\"Check for file updates and process changed files.\"\"\"\n        current_files = {}\n        added_files = set()\n        modified_files = set()\n        removed_files = set()\n        total_files = 0\n        unchanged_files = 0\n        processed_successfully = True  # Track if all processing succeeded\n\n        # Scan current files\n        logger.info(\"Starting file scan...\")\n        for version in settings.odoo_versions_list:\n            version_path = Path(raw_dir) / 'versions' / version / 'content'\n            if not version_path.exists():\n                continue\n\n            for rst_file in version_path.rglob('*.rst'):\n                total_files += 1\n                file_path = str(rst_file)\n                current_hash = self._get_file_hash(file_path)\n                current_files[file_path] = current_hash\n\n                # Only track changes if we have an existing cache\n                if self.file_cache:\n                    if file_path not in self.file_cache:\n                        logger.info(f\"New file detected: {file_path}\")\n                        added_files.add(file_path)\n                    elif self.file_cache[file_path] != current_hash:\n                        logger.info(f\"Modified file detected: {file_path}\")\n                        modified_files.add(file_path)\n                    else:\n                        unchanged_files += 1\n\n        # Only check for removed files if we have an existing cache\n        if self.file_cache:\n            removed_files = set(self.file_cache.keys()) - set(current_files.keys())\n            for file_path in removed_files:\n                logger.info(f\"Removed file detected: {file_path}\")\n\n        # Log summary\n        logger.info(f\"Scan complete:\")\n        logger.info(f\"Total files scanned: {total_files}\")\n\n        if not self.file_cache:\n            logger.info(\"Creating initial cache without processing files\")\n            self.file_cache = current_files\n            self._save_cache()\n            return set(), set(), set()\n\n        logger.info(f\"Files unchanged: {unchanged_files}\")\n        logger.info(f\"New files: {len(added_files)}\")\n        logger.info(f\"Modified files: {len(modified_files)}\")\n        logger.info(f\"Removed files: {len(removed_files)}\")\n\n        # Store the original cache in case we need to rollback\n        original_cache = self.file_cache.copy()\n\n        # Process changes only if there are any\n        files_to_process = added_files | modified_files\n        if not files_to_process:\n            logger.info(\"No files need to be updated\")\n        else:\n            logger.info(f\"Processing {len(files_to_process)} files...\")\n            for idx, file_path in enumerate(files_to_process, 1):\n                try:\n                    logger.info(f\"Processing file {idx}/{len(files_to_process)}: {file_path}\")\n                    \n                    # Convert RST to markdown\n                    version = self._get_version_from_path(file_path)\n                    rel_path = Path(file_path).relative_to(Path(raw_dir) / 'versions' / f\"{version/10:.1f}\" / 'content')\n                    md_path = Path(markdown_dir) / 'versions' / f\"{version/10:.1f}\" / 'content' / rel_path.with_suffix('.md')\n                    \n                    # Ensure directory exists\n                    md_path.parent.mkdir(parents=True, exist_ok=True)\n                    \n                    # Convert content\n                    with open(file_path, 'r', encoding='utf-8') as f:\n                        content = f.read()\n                    md_content = self.markdown_converter.convert_rst_to_markdown(content)\n                    \n                    # Write markdown file\n                    with open(md_path, 'w', encoding='utf-8') as f:\n                        f.write(md_content)\n                    \n                    # Process markdown for database\n                    await self.document_processor.process_file_with_update(str(md_path), version)\n                    \n                except Exception as e:\n                    logger.error(f\"Error processing {file_path}: {e}\")\n                    processed_successfully = False\n                    # Restore original cache\n                    self.file_cache = original_cache\n                    self._save_cache()\n                    logger.info(\"Restored original cache due to processing error\")\n                    break\n\n        # Only update cache if all processing was successful\n        if processed_successfully:\n            self.file_cache = current_files\n            self._save_cache()\n            logger.info(\"Cache updated successfully\")\n        else:\n            logger.warning(\"Cache not updated due to processing errors\")\n\n        return added_files, modified_files, removed_files"}
{"type": "source_file", "path": "src/processing/markdown_converter.py", "content": "# src/processing/markdown.py\nimport re\nimport os\nimport subprocess\nfrom pathlib import Path\nfrom tempfile import NamedTemporaryFile\nfrom typing import List, Dict, Any\nfrom langchain_text_splitters import (\n    MarkdownHeaderTextSplitter,\n    RecursiveCharacterTextSplitter\n)\nfrom src.config.settings import settings\nfrom src.utils.logging import logger\n\nclass MarkdownConverter:\n    def __init__(self):\n        self.headers_to_split_on = [\n            (\"#\", \"Header 1\"),\n            (\"##\", \"Header 2\"),\n            (\"###\", \"Header 3\"),\n            (\"####\", \"Header 4\"),\n        ]\n\n    def process_directory(self, base_dir: str, output_dir: str = None):\n        \"\"\"Process all RST files in the given directory and its subdirectories.\n        \n        Args:\n            base_dir (str): Source directory containing RST files\n            output_dir (str, optional): Target directory for markdown files.\n                If not provided, defaults to base_dir/markdown\n        \"\"\"\n        base_path = Path(base_dir)\n        # If output_dir is not provided, use the default path\n        output_path = Path(output_dir if output_dir is not None else base_path / 'markdown')\n        versions = settings.odoo_versions_list\n        \n        for version in versions:\n            source_dir = base_path / 'versions' / version / 'content'\n            target_dir = output_path / 'versions' / version / 'content'\n            \n            if not source_dir.exists():\n                logger.warning(f\"Source directory {source_dir} does not exist\")\n                continue\n                \n            # Walk through all files in the source directory\n            for rst_file in source_dir.rglob('*.rst'):\n                # Calculate the relative path from the source_dir\n                rel_path = rst_file.relative_to(source_dir)\n                \n                # Create the corresponding markdown file path\n                md_file = target_dir / rel_path.with_suffix('.md')\n                \n                # Create target directory if it doesn't exist\n                md_file.parent.mkdir(parents=True, exist_ok=True)\n                \n                logger.info(f\"Processing: {rst_file} -> {md_file}\")\n                try:\n                    # Read RST content\n                    with open(rst_file, 'r', encoding='utf-8') as f:\n                        content = f.read()\n                        \n                    # Convert the content\n                    md_content = self.convert_rst_to_markdown(content)\n                    \n                    # Write to markdown file\n                    with open(md_file, 'w', encoding='utf-8') as f:\n                        f.write(md_content)\n                        \n                except Exception as e:\n                    logger.error(f\"Error processing file {rst_file}: {e}\")\n\n    def convert_rst_to_markdown(self, content: str) -> str:\n        \"\"\"Convert RST content to markdown.\"\"\"\n        try:\n            # Create a temporary file for the RST content\n            with NamedTemporaryFile(mode='w', suffix='.rst', encoding='utf-8', delete=False) as temp_rst:\n                temp_rst.write(content)\n                temp_rst_path = temp_rst.name\n                \n            # Create a temporary file for the intermediate markdown\n            with NamedTemporaryFile(mode='w', suffix='.md', encoding='utf-8', delete=False) as temp_md:\n                temp_md_path = temp_md.name\n                \n            try:\n                # Run pandoc conversion\n                subprocess.run(\n                    ['pandoc', temp_rst_path, '-f', 'rst', '-t', 'markdown', '-o', temp_md_path], \n                    check=True,\n                    capture_output=True\n                )\n                \n                # Read the converted content\n                with open(temp_md_path, 'r', encoding='utf-8') as f:\n                    md_content = f.read()\n                    \n                # Clean up the markdown content\n                return self.clean_markdown(md_content)\n                \n            finally:\n                # Clean up temporary files\n                os.unlink(temp_rst_path)\n                os.unlink(temp_md_path)\n                    \n        except subprocess.CalledProcessError as e:\n            logger.error(f\"Pandoc conversion failed: {e.stderr.decode()}\")\n            raise\n        except Exception as e:\n            logger.error(f\"Conversion failed: {e}\")\n            raise\n    \n    def clean_markdown(self, content: str) -> str:\n        \"\"\"Clean up the markdown content.\n        \n        Args:\n            content (str): Raw markdown content to clean\n            \n        Returns:\n            str: Cleaned markdown content\n        \"\"\"\n        # Remove initial metadata before first heading while preserving structure\n        lines = content.split('\\n')\n        first_content_line = 0\n        in_metadata = True\n        \n        for i, line in enumerate(lines):\n            stripped = line.strip()\n            # Stop looking for metadata if we hit a heading, table, or other structured content\n            if (stripped.startswith('#') or\n                stripped.startswith('+--') or\n                stripped.startswith('|') or\n                (stripped and not stripped == ':' and \n                not any(marker in stripped.lower() for marker in \n                        ['show-content', 'hide-page-toc', 'show-toc', 'nosearch', 'orphan']))):\n                in_metadata = False\n                first_content_line = i\n                break\n                \n        # Keep content from first non-metadata line onwards\n        content = '\\n'.join(lines[first_content_line:])\n        \n        # First fix line breaks (but preserve tables and other formatted content)\n        content = self.fix_line_breaks(content)\n        \n        # Clean up directive blocks\n        content = re.sub(r'::: seealso\\n(.*?)\\n:::', r'::: seealso\\n\\1\\n:::', content, flags=re.DOTALL)\n        content = re.sub(r':::: tip\\n::: title\\nTip\\n:::\\n\\n(.*?)\\n::::', r'Tip: \\1', content, flags=re.DOTALL)\n        content = re.sub(r':::: note\\n::: title\\nNote\\n:::\\n\\n(.*?)\\n::::', r'Note: \\1', content, flags=re.DOTALL)\n        content = re.sub(r':::: important\\n::: title\\nImportant\\n:::\\n\\n(.*?)\\n::::', r'Important: \\1', content, flags=re.DOTALL)\n        \n        # Clean up all RST-style roles\n        content = re.sub(r'\\{\\.interpreted-text\\s+role=\"[^\"]+\"\\}', '', content, flags=re.DOTALL)\n        \n        # Convert related content block to a list\n        def format_related_content(match):\n            items = match.group(1).split()\n            formatted_items = \"\\n\".join(f\"- {item.strip()}\" for item in items if item.strip())\n            return f\"## Related content:\\n\\n{formatted_items}\"\n        \n        content = re.sub(\n            r'::: \\{\\.toctree titlesonly=\"\"\\}\\n(.*?)\\n:::',\n            format_related_content,\n            content,\n            flags=re.DOTALL,\n        )\n        \n        # Remove extra blank lines\n        content = re.sub(r'\\n{3,}', '\\n\\n', content)\n        \n        return content.strip()\n\n    def fix_line_breaks(self, content: str) -> str:\n        \"\"\"Fix unnecessary line breaks while preserving formatting.\n        \n        Args:\n            content (str): Content to fix line breaks in\n            \n        Returns:\n            str: Content with fixed line breaks\n        \"\"\"\n        lines = content.split('\\n')\n        result = []\n        current_line = ''\n        in_code_block = False\n        in_table = False\n        \n        def should_preserve_line_break(line):\n            return (line.strip().startswith('#') or\n                    line.strip().startswith(':::') or\n                    line.strip().startswith('- ') or\n                    line.strip().startswith('* ') or\n                    line.strip().startswith('[') or\n                    line.strip().startswith('+') or  # Table markers\n                    line.strip().startswith('|') or  # Table content\n                    not line.strip())  # Empty lines\n\n        for line in lines:\n            stripped_line = line.strip()\n            \n            # Check for table markers\n            if stripped_line.startswith('+') and '-' in stripped_line:\n                in_table = True\n                result.append(line)\n                continue\n                \n            # If in table, preserve formatting\n            if in_table:\n                if stripped_line.startswith('+'):  # End of table section\n                    in_table = False\n                result.append(line)\n                continue\n            \n            # Handle code blocks\n            if stripped_line.startswith('```'):\n                if current_line:\n                    result.append(current_line)\n                    current_line = ''\n                result.append(line)\n                in_code_block = not in_code_block\n                continue\n            \n            # Preserve code block content\n            if in_code_block:\n                result.append(line)\n                continue\n            \n            # Handle preserved lines\n            if should_preserve_line_break(line):\n                if current_line:\n                    result.append(current_line)\n                    current_line = ''\n                result.append(line)\n                continue\n            \n            # Handle regular content\n            if current_line:\n                current_line += ' ' + stripped_line\n            else:\n                current_line = stripped_line\n        \n        # Add any remaining content\n        if current_line:\n            result.append(current_line)\n        \n        return '\\n'.join(result)\n    \n    def chunk_markdown(self, file_path: str, chunk_size: int = 5000, chunk_overlap: int = 500) -> List[Dict[str, Any]]:\n        \"\"\"Split a markdown file into chunks based on headers and size.\n        \n        Args:\n            file_path (str): Path to the markdown file\n            chunk_size (int): Maximum chunk size in characters\n            chunk_overlap (int): Overlap between chunks in characters\n            \n        Returns:\n            List[Dict[str, Any]]: List of chunks with content and metadata\n        \"\"\"\n        try:\n            # Read the markdown file\n            with open(file_path, 'r', encoding='utf-8') as f:\n                text = f.read()\n            \n            # Split by headers first\n            markdown_splitter = MarkdownHeaderTextSplitter(\n                headers_to_split_on=self.headers_to_split_on,\n                strip_headers=False\n            )\n            md_header_splits = markdown_splitter.split_text(text)\n            \n            # Then split by size if needed\n            text_splitter = RecursiveCharacterTextSplitter(\n                chunk_size=chunk_size,\n                chunk_overlap=chunk_overlap,\n                length_function=len,\n                separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n            )\n            \n            final_splits = text_splitter.split_documents(md_header_splits)\n            \n            # Convert to list of dicts with content and metadata\n            chunks = []\n            for split in final_splits:\n                # Create header path\n                header_path = self.create_header_path(split.metadata)\n                \n                # Combine header path with content\n                full_content = f\"{header_path}\\n{split.page_content}\" if header_path else split.page_content\n                \n                chunks.append({\n                    \"content\": full_content,\n                    \"metadata\": {\n                        **split.metadata,\n                        \"header_path\": header_path\n                    }\n                })\n            \n            return chunks\n        except Exception as e:\n            logger.error(f\"Error chunking markdown file {file_path}: {e}\")\n            raise\n\n    def create_header_path(self, metadata: Dict[str, str]) -> str:\n        \"\"\"Create a hierarchical header path from metadata.\n        \n        Args:\n            metadata (Dict[str, str]): Metadata dictionary containing headers\n            \n        Returns:\n            str: String representing the header hierarchy\n        \"\"\"\n        headers = []\n        for i in range(1, 5):\n            key = f\"Header {i}\"\n            if key in metadata and metadata[key]:\n                header_level = \"#\" * i\n                headers.append(f\"[{header_level}] {metadata[key]}\")\n        \n        return \" > \".join(headers) if headers else \"\"\n    \n    def convert_path_to_url(self, file_path: str, header_path: str = \"\") -> tuple[str, int]:\n        \"\"\"Convert a local file path to a full URL for the Odoo documentation and extract version.\n\n        Args:\n            file_path (str): Local file path to convert\n            header_path (str, optional): Header path for section anchors. Defaults to \"\".\n\n        Returns:\n            tuple[str, int]: Full URL for the documentation page and version number\n        \"\"\"\n        # Extract version from path\n        version_match = re.search(r'/versions/(\\d+\\.\\d+)/', file_path)\n        if not version_match:\n            raise ValueError(f\"Could not extract version from path: {file_path}\")\n        \n        version_str = version_match.group(1)\n        version = int(float(version_str) * 10)  # Convert \"16.0\" to 160, \"17.0\" to 170, etc.\n        \n        # Extract the path after the version number\n        path_match = re.search(r'/versions/\\d+\\.\\d+/(.+?)\\.md$', file_path)\n        if not path_match:\n            raise ValueError(f\"Could not extract content path from: {file_path}\")\n        \n        content_path = path_match.group(1)\n        # Remove 'content/' from the path if it exists\n        content_path = re.sub(r'^content/', '', content_path)\n        \n        base_url = f\"https://www.odoo.com/documentation/{version_str}\"\n        url = f\"{base_url}/{content_path}.html\"\n        \n        # Add section anchor if header path is provided\n        section_anchor = self.extract_section_anchor(header_path)\n        if section_anchor:\n            url = f\"{url}#{section_anchor}\"\n        \n        return url, version\n    \n    def extract_section_anchor(self, header_path: str) -> str:\n        \"\"\"Extract the last section from a header path to create an anchor.\n        \n        Args:\n            header_path (str): Full header path (e.g., \"[#] Database management > [##] Installation\")\n            \n        Returns:\n            str: Section anchor or empty string if no valid section found\n        \"\"\"\n        if not header_path:\n            return \"\"\n            \n        # Get the last section from the header path\n        sections = header_path.split(\" > \")\n        if sections:\n            last_section = sections[-1]\n            # Remove the header level indicator (e.g., \"[##]\")\n            last_section = re.sub(r'\\[#+\\]\\s*', '', last_section)\n            # Clean the section title to create the anchor\n            return self.clean_section_name(last_section)\n        return \"\"\n    \n    def clean_section_name(self, title: str) -> str:\n        \"\"\"Convert a section title to a URL-friendly anchor.\n        \n        Args:\n            title (str): The section title to convert\n            \n        Returns:\n            str: URL-friendly anchor name\n            \n        Examples:\n            \"Installation\" -> \"installation\"\n            \"Invite / remove users\" -> \"invite-remove-users\"\n            \"Database Management\" -> \"database-management\"\n        \"\"\"\n        # Remove markdown header markers and any {#...} custom anchors\n        title = re.sub(r'\\[#+\\]\\s*', '', title)\n        title = re.sub(r'\\{#.*?\\}', '', title)\n        \n        # Remove special characters and extra spaces\n        title = re.sub(r'[^a-zA-Z0-9\\s-]', '', title)\n        \n        # Convert to lowercase and replace spaces with dashes\n        title = title.lower().strip()\n        title = re.sub(r'\\s+', '-', title)\n        \n        return title\n    "}
{"type": "source_file", "path": "src/utils/__init__.py", "content": "from .errors import AppError\nfrom .logging import logger\n\n__all__ = ['AppError', 'logger']"}
{"type": "source_file", "path": "src/ui/streamlit_app.py", "content": "import sys\nfrom pathlib import Path\n\n# Add project root to Python path\nproject_root = Path(__file__).parent.parent.parent\nsys.path.append(str(project_root))\n\nimport asyncio\nimport streamlit as st\nfrom datetime import datetime\nfrom src.core.services.chat_service import ChatService\nfrom src.core.services.embedding import EmbeddingService\nfrom src.config.settings import settings\nfrom src.utils.logging import logger\nfrom openai import AsyncOpenAI\nfrom src.core.services.db_service import DatabaseService\n\nclass StreamlitUI:\n    def __init__(self):\n        self.openai_client = AsyncOpenAI(\n            api_key=settings.OPENAI_API_KEY,\n            base_url=settings.OPENAI_API_BASE\n        )\n        self.db_service = DatabaseService()\n        self.embedding_service = EmbeddingService(self.openai_client)\n        self.chat_service = ChatService(\n            self.openai_client,\n            self.db_service,\n            self.embedding_service\n        )\n    \n    async def cleanup(self):\n        \"\"\"Cleanup resources.\"\"\"\n        if hasattr(self, 'db_service'):\n            await self.db_service.close()\n\n    def setup_page(self):\n        st.title(\"Odoo Expert\")\n        st.write(\"Ask me anything about Odoo and I'll provide you with the best answers with references and citations!\")\n\n    def setup_sidebar(self):\n        version_options = {\n            \"16.0\": 160,\n            \"17.0\": 170,\n            \"18.0\": 180\n        }\n        selected_version = st.sidebar.selectbox(\n            \"Select Odoo Version\",\n            options=list(version_options.keys()),\n            format_func=lambda x: f\"Version {x}\",\n            index=2  # Default to 18.0\n        )\n        return version_options[selected_version]\n\n    @staticmethod\n    def display_chat_message(role: str, content: str):\n        with st.chat_message(role):\n            st.markdown(content)\n\n    async def process_query(self, query: str, version: int):\n        \"\"\"Process a query and display the response.\"\"\"\n        try:\n            # Show a loading message\n            with st.chat_message(\"assistant\"):\n                response_placeholder = st.empty()\n                response_placeholder.markdown(\"Searching documentation...\")\n\n            # Get relevant chunks\n            chunks = await self.chat_service.retrieve_relevant_chunks(query, version)\n            \n            if not chunks:\n                with st.chat_message(\"assistant\"):\n                    st.error(\"No relevant documentation found for your query. Try rephrasing your question or choosing a different Odoo version.\")\n                return\n            \n            # Show processing message\n            response_placeholder.markdown(\"Generating response...\")\n            \n            # Prepare context and generate response\n            context, sources = self.chat_service.prepare_context(chunks)\n            \n            full_response = \"\"\n            try:\n                response = await self.chat_service.generate_response(\n                    query=query,\n                    context=context,\n                    conversation_history=st.session_state.conversation_history,\n                    stream=True\n                )\n                \n                async for chunk in response:\n                    # Add more robust error checking\n                    if chunk and hasattr(chunk, 'choices') and chunk.choices:\n                        delta = chunk.choices[0].delta\n                        if hasattr(delta, 'content') and delta.content:\n                            full_response += delta.content\n                            response_placeholder.markdown(full_response)\n                    \n                if full_response:\n                    # Add to conversation history only if we got a valid response\n                    st.session_state.conversation_history.append({\n                        \"user\": query,\n                        \"assistant\": full_response,\n                        \"timestamp\": datetime.now().isoformat()\n                    })\n                else:\n                    response_placeholder.markdown(\"I couldn't generate a response. Please try rephrasing your question.\")\n                    \n            except Exception as e:\n                logger.error(f\"Error generating response: {e}\")\n                import traceback\n                logger.error(traceback.format_exc())  # This will give you a full stack trace\n                response_placeholder.markdown(f\"Sorry, I encountered an error: {str(e)}\")\n                \n        except Exception as e:\n            logger.error(f\"Error processing query: {e}\")\n            import traceback\n            logger.error(traceback.format_exc())  # This will give you a full stack trace\n            with st.chat_message(\"assistant\"):\n                st.error(f\"An error occurred while processing your query: {str(e)}\")\n\n    async def main(self):\n        try:\n            self.setup_page()\n            version = self.setup_sidebar()\n\n            if 'conversation_history' not in st.session_state:\n                st.session_state.conversation_history = []\n\n            for message in st.session_state.conversation_history:\n                self.display_chat_message(\"user\", message[\"user\"])\n                self.display_chat_message(\"assistant\", message[\"assistant\"])\n\n            user_input = st.chat_input(\"Ask a question about Odoo...\")\n\n            if user_input:\n                self.display_chat_message(\"user\", user_input)\n                await self.process_query(user_input, version)\n\n            if st.button(\"Clear Conversation\"):\n                st.session_state.conversation_history = []\n                st.rerun()\n        finally:\n            await self.cleanup()\n\ndef run_app():\n    ui = StreamlitUI()\n    asyncio.run(ui.main())\n\nif __name__ == \"__main__\":\n    run_app()"}
{"type": "source_file", "path": "src/api/routes/chat.py", "content": "from fastapi import APIRouter, Depends, HTTPException\nfrom fastapi.responses import StreamingResponse\nfrom openai import AsyncOpenAI\nfrom src.core.services.db_service import DatabaseService\nfrom src.api.models.chat import ChatRequest, ChatResponse\nfrom src.api.dependencies.auth import verify_token\nfrom src.core.services.chat_service import ChatService\nfrom src.core.services.embedding import EmbeddingService\nfrom src.config.settings import settings\nfrom src.utils.logging import logger\n\nrouter = APIRouter()\n\n# Create dependency for services\nasync def get_services():\n    openai_client = AsyncOpenAI(\n        api_key=settings.OPENAI_API_KEY,\n        base_url=settings.OPENAI_API_BASE\n    )\n    \n    db_service = DatabaseService()\n    embedding_service = EmbeddingService(openai_client)\n    chat_service = ChatService(openai_client, db_service, embedding_service)\n    \n    return chat_service\n\n@router.post(\"/chat\", response_model=ChatResponse)\nasync def chat_endpoint(\n    request: ChatRequest,\n    authenticated: bool = Depends(verify_token),\n    chat_service: ChatService = Depends(get_services)\n):\n    try:\n        chunks = await chat_service.retrieve_relevant_chunks(\n            request.query, \n            request.version\n        )\n        \n        if not chunks:\n            raise HTTPException(\n                status_code=404,\n                detail=\"No relevant documentation found\"\n            )\n        \n        context, sources = chat_service.prepare_context(chunks)\n        \n        response = await chat_service.generate_response(\n            query=request.query,\n            context=context,\n            conversation_history=request.conversation_history,\n            stream=False\n        )\n        \n        if not response:\n            raise HTTPException(\n                status_code=500,\n                detail=\"Failed to generate response\"\n            )\n        \n        return ChatResponse(\n            answer=response,\n            sources=sources\n        )\n        \n    except Exception as e:\n        logger.error(f\"Error in chat endpoint: {e}\")\n        raise HTTPException(\n            status_code=500,\n            detail=str(e)\n        )\n\n@router.post(\"/stream\", response_class=StreamingResponse)\nasync def stream_endpoint(\n    request: ChatRequest,\n    authenticated: bool = Depends(verify_token),\n    chat_service: ChatService = Depends(get_services)\n):\n    try:\n        chunks = await chat_service.retrieve_relevant_chunks(\n            request.query, \n            request.version\n        )\n        \n        if not chunks:\n            raise HTTPException(\n                status_code=404,\n                detail=\"No relevant documentation found\"\n            )\n        \n        context, sources = chat_service.prepare_context(chunks)\n        \n        stream = await chat_service.generate_response(\n            query=request.query,\n            context=context,\n            conversation_history=request.conversation_history,\n            stream=True\n        )\n        \n        async def generate():\n            try:\n                async for chunk in stream:\n                    if (hasattr(chunk, 'choices') and \n                        chunk.choices and \n                        hasattr(chunk.choices[0], 'delta') and \n                        hasattr(chunk.choices[0].delta, 'content') and \n                        chunk.choices[0].delta.content):\n                        yield chunk.choices[0].delta.content\n            except Exception as e:\n                logger.error(f\"Error in stream generation: {e}\")\n                raise\n        \n        return StreamingResponse(\n            generate(),\n            media_type=\"text/event-stream\"\n        )\n        \n    except Exception as e:\n        logger.error(f\"Error in stream endpoint: {e}\")\n        raise HTTPException(\n            status_code=500,\n            detail=str(e)\n        )\n"}
{"type": "source_file", "path": "src/ui/__init__.py", "content": "from .streamlit_app import run_app\n\n__all__ = ['run_app']"}
{"type": "source_file", "path": "src/utils/logging.py", "content": "import logging\nimport sys\nfrom pathlib import Path\nfrom src.config.settings import settings\n\ndef setup_logger():\n    \"\"\"Configure and return a logger instance.\"\"\"\n    logger = logging.getLogger(\"odoo_expert\")\n    \n    # Only add handlers if they haven't been added already\n    if not logger.handlers:\n        logger.setLevel(logging.INFO)\n        \n        # Create formatter\n        formatter = logging.Formatter(\n            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n        )\n        \n        # Console handler\n        console_handler = logging.StreamHandler(sys.stdout)\n        console_handler.setLevel(logging.INFO)\n        console_handler.setFormatter(formatter)\n        logger.addHandler(console_handler)\n        \n        # File handler\n        try:\n            # Create logs directory if it doesn't exist\n            settings.LOGS_DIR.mkdir(parents=True, exist_ok=True)\n            \n            # Setup file handler\n            file_handler = logging.FileHandler(settings.LOGS_DIR / \"app.log\")\n            file_handler.setLevel(logging.INFO)\n            file_handler.setFormatter(formatter)\n            logger.addHandler(file_handler)\n        except Exception as e:\n            print(f\"Warning: Could not setup file logging: {e}\")\n    \n    return logger\n\n# Create a singleton logger instance\nlogger = setup_logger()"}
{"type": "source_file", "path": "src/utils/errors.py", "content": "# src/utils/errors.py\nclass AppError(Exception):\n    \"\"\"Base error class for application exceptions.\"\"\"\n    def __init__(self, message: str, status_code: int = 500):\n        super().__init__(message)\n        self.status_code = status_code"}
