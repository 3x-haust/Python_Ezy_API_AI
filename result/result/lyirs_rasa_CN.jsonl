{"repo_info": {"repo_name": "rasa_CN", "repo_owner": "lyirs", "repo_url": "https://github.com/lyirs/rasa_CN"}}
{"type": "source_file", "path": "actions/__init__.py", "content": "import yaml\nimport os\n\n\nclass ConfigLoader:\n    _instance = None\n    _config = None\n\n    def __new__(cls, path='config.yml'):\n        if cls._instance is None:\n            cls._instance = super(ConfigLoader, cls).__new__(cls)\n            cls._config = cls._load_config(path)\n        return cls._instance\n\n    @classmethod\n    def _load_config(cls, path):\n        if not os.path.exists(path):\n            raise FileNotFoundError(f\"Config file not found at: {path}\")\n        try:\n            with open(path, 'r') as file:\n                config = yaml.safe_load(file)\n                if config is None:\n                    raise ValueError(\"Config file is empty or invalid.\")\n                return config\n        except yaml.YAMLError as e:\n            raise ValueError(f\"Failed to parse config file: {e}\")\n\n    @classmethod\n    def get_config(cls):\n        if cls._instance is None:\n            cls._instance = cls() \n        return cls._config\n"}
{"type": "source_file", "path": "actions/keyword.py", "content": "# actions.py\nimport random\nfrom typing import Any, Text, Dict, List\nimport yaml\n\nfrom rasa_sdk import Action, Tracker\nfrom rasa_sdk.executor import CollectingDispatcher\n\n\nclass ActionKeywordResponse(Action):\n    def name(self) -> Text:\n        return \"action_keyword_response\"\n\n    @staticmethod\n    def load_keywords(file_path: Text) -> List[Dict[Text, Any]]:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            data = yaml.safe_load(f)\n        return data.get('keywords', [])\n\n    def run(self, dispatcher: CollectingDispatcher,\n            tracker: Tracker,\n            domain: Dict[Text, Any]) -> List[Dict[Text, Any]]:\n\n        keyword = next(tracker.get_latest_entity_values(\"keyword\"), None)\n        if keyword:\n            keywords = self.load_keywords('data/keywords/keywords.yml')\n            for item in keywords:\n                if item['keyword'] == keyword:\n                    responses = item.get('responses', [])\n                    if responses:\n                        response = random.choice(responses)\n                        dispatcher.utter_message(text=response)\n                        return []\n            return []\n"}
{"type": "source_file", "path": "actions/exchange.py", "content": "# This files contains your custom actions which can be used to run\n# custom Python code.\n#\n# See this guide on how to implement these action:\n# https://rasa.com/docs/rasa/custom-actions\n\nimport os\nfrom typing import Any, Text, Dict, List\n\nfrom rasa_sdk import Action, Tracker\nfrom rasa_sdk.executor import CollectingDispatcher\nimport requests\nfrom typing import Any, Dict, List, Text, Optional\n\nfrom actions import ConfigLoader\nconfig = ConfigLoader.get_config()\n\nclass ActionExchangeRate(Action):\n\n    def name(self) -> Text:\n        return \"action_exchange_rate\"\n\n    def run(self, dispatcher: CollectingDispatcher,\n            tracker: Tracker,\n            domain: Dict[Text, Any]) -> List[Dict[Text, Any]]:\n        \n        currency_map = {\n            \"美元\":\"USD\",\n            \"日元\":\"JPY\",\n            \"欧元\":\"EUR\",\n            \"英镑\":\"GBP\",\n            \"韩元\":\"KER\",\n            \"港币\":\"HKD\",\n            \"卢布\":\"RUB\",\n        }\n\n        # number = next(tracker.get_latest_entity_values(\"number\"), None)\n        number = tracker.get_slot(\"number\")\n        currency = tracker.get_slot(\"currency\")\n        \n        _config = next(\n            item for item in config['apis'] if item['name'] == 'Exchange')\n        url = _config['url']\n        api_key = _config['key']\n\n        try:\n            params = {\n                'key': api_key,\n                'from': currency_map[currency],\n                'to': 'CNY',\n            }\n        except Exception as e:\n            dispatcher.utter_message(text=\"暂时不清楚这个货币的汇率哦~\")\n            return\n        \n        headers = {\n            'Content-Type': 'application/x-www-form-urlencoded',\n        }\n        response = requests.get(url=url, params=params, headers=headers)\n\n        if response.status_code != 200:\n            dispatcher.utter_message(text=\"我无法从服务器获取数据。\")\n            return []\n\n        data = response.json()\n        result = float(data['result'][0]['result'])\n        result = float(number) * result  # type: ignore\n        result = round(result, 2)\n\n        info = f\"当前汇率\\n{currency_map[currency]}->CNY\\n{data['result'][0]['exchange']}\\n(结果：{result})\"\n\n        dispatcher.utter_message(text=info)\n\n        return []\n"}
{"type": "source_file", "path": "actions/express.py", "content": "# This files contains your custom actions which can be used to run\n# custom Python code.\n#\n# See this guide on how to implement these action:\n# https://rasa.com/docs/rasa/custom-actions\n\n\nfrom typing import Any, Text, Dict, List\nfrom rasa_sdk.events import SlotSet\nfrom rasa_sdk import Action, Tracker\nfrom rasa_sdk.executor import CollectingDispatcher\nimport time\nimport json\nimport requests\n\n\nfrom actions import ConfigLoader\nconfig = ConfigLoader.get_config()\n\nexpress_list = {\n    \"圆通\": \"yuantong\",\n    \"顺丰\": \"shunfeng\",\n    \"中通\": \"zhongtong\"\n}\n\n\ndef get_url(code, id):\n    time_new = str(int(time.time() * 1000))\n    _config = next(\n        item for item in config['apis'] if item['name'] == 'Exchange')\n    url = _config['url']\n    url = url + \\\n        id + \"-\" + code + \"-UUCAO\" + time_new + \".html\"\n    return url\n\n\nclass ActionSearchExpress(Action):\n\n    def name(self) -> Text:\n        return \"action_search_express\"\n\n    def run(self, dispatcher: CollectingDispatcher,\n            tracker: Tracker,\n            domain: Dict[Text, Any]) -> List[Dict[Text, Any]]:\n        user_id = tracker.sender_id\n        express = tracker.slots.get(\"express\")\n        number = tracker.slots.get(\"number\")\n        headers = {\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36',\n            \"Referer\": \"http://www.kuaidi.com/cominterface2345.html\"\n        }\n        url = get_url(express_list[express], number)\n        response = requests.get(url, headers=headers)\n        datas = json.loads(response.text)['data']\n        text = \"\"\n        for item in datas:\n            time_ = item['time']\n            info = item['context']\n            text += f'时间：{time_}'\n            text += f'物流状态：{info}' + '\\n'\n\n        dispatcher.utter_message(text=text)\n        return [SlotSet(\"number\", None),SlotSet(\"express\", None)]\n"}
{"type": "source_file", "path": "components/custom_number_extractor.py", "content": "import re\nfrom typing import Any, Dict, List, Text, Type\n\nfrom rasa.engine.graph import ExecutionContext, GraphComponent\nfrom rasa.engine.recipes.default_recipe import DefaultV1Recipe\nfrom rasa.engine.storage.resource import Resource\nfrom rasa.engine.storage.storage import ModelStorage\nfrom rasa.shared.nlu.constants import ENTITIES, TEXT\nfrom rasa.nlu.extractors.extractor import EntityExtractorMixin\nfrom rasa.shared.nlu.training_data.message import Message\n\n\n@DefaultV1Recipe.register(\n    DefaultV1Recipe.ComponentType.ENTITY_EXTRACTOR,\n    is_trainable=False\n)\nclass CustomNumberExtractor(GraphComponent, EntityExtractorMixin):\n    \"\"\"Entity extractor which uses regular expressions to find numbers.\"\"\"\n\n    @staticmethod\n    def get_default_config() -> Dict[Text, Any]:\n        \"\"\"The component's default config.\"\"\"\n        return {\n            \"number_pattern\": r'\\b\\d+'\n        }\n\n    def __init__(self, config: Dict[Text, Any]) -> None:\n        \"\"\"Initialize CustomNumberExtractor.\"\"\"\n        self._config = config\n\n    @classmethod\n    def create(\n        cls,\n        config: Dict[Text, Any],\n        model_storage: ModelStorage,\n        resource: Resource,\n        execution_context: ExecutionContext,\n    ) -> GraphComponent:\n        \"\"\"Creates a new component.\"\"\"\n        return cls(config)\n\n    def process(self, messages: List[Message]) -> List[Message]:\n        \"\"\"Extract numbers using regular expressions.\n\n        Args:\n            messages: List of messages to process.\n\n        Returns: The processed messages.\n        \"\"\"\n        number_pattern = re.compile(self._config[\"number_pattern\"])\n\n        for message in messages:\n            text = message.get(TEXT)\n            matches = number_pattern.finditer(text)\n\n            extracted_entities = []\n            for match in matches:\n                start, end = match.span()\n                value = match.group()\n\n                entity = {\n                    \"entity\": \"number\",\n                    \"value\": value,\n                    \"start\": start,\n                    \"confidence\": 75,\n                    \"end\": end,\n                    \"extractor\": \"CustomNumberExtractor\",\n\n                }\n                extracted_entities.append(entity)\n\n            message.set(\n                ENTITIES, message.get(ENTITIES, []) + extracted_entities, add_to_output=True\n            )\n\n        return messages\n"}
{"type": "source_file", "path": "components/jieba_tokenizer.py", "content": "from __future__ import annotations\nimport glob\nimport logging\nimport os\nimport shutil\nfrom typing import Any, Dict, List, Optional, Text\nimport re\n\nfrom rasa.engine.graph import ExecutionContext\nfrom rasa.engine.recipes.default_recipe import DefaultV1Recipe\nfrom rasa.engine.storage.resource import Resource\nfrom rasa.engine.storage.storage import ModelStorage\n\nfrom rasa.nlu.tokenizers.tokenizer import Token, Tokenizer\nfrom rasa.shared.nlu.training_data.message import Message\nfrom rasa.shared.nlu.constants import TEXT_TOKENS\nfrom rasa.shared.nlu.training_data.training_data import TrainingData\nfrom rasa.shared.nlu.constants import (\n    INTENT,\n    INTENT_RESPONSE_KEY,\n    RESPONSE_IDENTIFIER_DELIMITER,\n    ACTION_NAME,\n)\nfrom rasa.nlu.constants import TOKENS_NAMES, MESSAGE_ATTRIBUTES\n\nlogger = logging.getLogger(__name__)\n\n\n@DefaultV1Recipe.register(\n    DefaultV1Recipe.ComponentType.MESSAGE_TOKENIZER, is_trainable=True\n)\nclass JiebaTokenizer(Tokenizer):\n    \"\"\"This tokenizer is a wrapper for Jieba (https://github.com/fxsjy/jieba).\"\"\"\n\n    # 返回支持的语言列表，这里仅支持中文（简体）\n    @staticmethod\n    def supported_languages() -> Optional[List[Text]]:\n        \"\"\"Supported languages (see parent class for full docstring).\"\"\"\n        return [\"zh\"]\n\n    # 返回默认配置，包括自定义词典路径、意图分词标志、意图分割符号和用于检测词汇的正则表达式。\n    @staticmethod\n    def get_default_config() -> Dict[Text, Any]:\n        \"\"\"Returns default config (see parent class for full docstring).\"\"\"\n        return {\n            # default don't load custom dictionary\n            \"dictionary_path\": None,\n            # Flag to check whether to split intents\n            \"intent_tokenization_flag\": False,\n            # Symbol on which intent should be split\n            \"intent_split_symbol\": \"_\",\n            # Regular expression to detect tokens\n            \"token_pattern\": None,\n            # Symbol on which prefix should be split\n            \"prefix_separator_symbol\": None,\n        }\n\n    # 初始化函数，主要用于设置模型存储和资源对象\n    def __init__(\n        self, config: Dict[Text, Any], model_storage: ModelStorage, resource: Resource\n    ) -> None:\n        \"\"\"Initialize the tokenizer.\"\"\"\n        super().__init__(config)\n        self._model_storage = model_storage\n        self._resource = resource\n\n    # 用于创建新的 Jieba 分词器实例。如果配置中提供了自定义词典路径，它会加载自定义词典。\n    @classmethod\n    def create(\n        cls,\n        config: Dict[Text, Any],\n        model_storage: ModelStorage,\n        resource: Resource,\n        execution_context: ExecutionContext,\n    ) -> JiebaTokenizer:\n        \"\"\"Creates a new component (see parent class for full docstring).\"\"\"\n        # Path to the dictionaries on the local filesystem.\n        dictionary_path = config[\"dictionary_path\"]\n        print(dictionary_path)\n        if dictionary_path is not None:\n            cls._load_custom_dictionary(dictionary_path)\n        return cls(config, model_storage, resource)\n\n    # 列出运行此组件所需的第三方 Python 依赖包\n    @staticmethod\n    def required_packages() -> List[Text]:\n        \"\"\"Any extra python dependencies required for this component to run.\"\"\"\n        return [\"jieba\"]\n\n    # 静态方法，用于加载自定义词典。\n    @staticmethod\n    def _load_custom_dictionary(path: Text) -> None:\n        \"\"\"Load all the custom dictionaries stored in the path.\n\n        More information about the dictionaries file format can\n        be found in the documentation of jieba.\n        https://github.com/fxsjy/jieba#load-dictionary\n        \"\"\"\n        import jieba\n\n        jieba_userdicts = glob.glob(f\"{path}/*\")\n        for jieba_userdict in jieba_userdicts:\n            logger.info(f\"Loading Jieba User Dictionary at {jieba_userdict}\")\n            jieba.load_userdict(jieba_userdict)\n\n    def train(self, training_data: TrainingData) -> Resource:\n        \"\"\"Copies the dictionary to the model storage.\"\"\"\n        self.persist()\n        return self._resource\n\n    # 重写_apply_token_pattern方法，使之接收ExtendedToken\n    def _apply_token_pattern(self, tokens: List[ExtendedToken]) -> List[ExtendedToken]:\n        if not self._config[\"token_pattern\"]:\n            return tokens\n\n        compiled_pattern = re.compile(self._config[\"token_pattern\"])\n\n        final_tokens = []\n        for token in tokens:\n            new_tokens = compiled_pattern.findall(token.text)\n            new_tokens = [t for t in new_tokens if t]\n\n            if not new_tokens:\n                final_tokens.append(token)\n\n            running_offset = 0\n            for new_token in new_tokens:\n                word_offset = token.text.index(new_token, running_offset)\n                word_len = len(new_token)\n                running_offset = word_offset + word_len\n                final_tokens.append(\n                    ExtendedToken(\n                        new_token,\n                        token.start + word_offset,\n                        data=token.data,\n                        lemma=token.lemma,\n                        pos=token.pos\n                    )\n                )\n\n        return final_tokens\n\n    # 对给定的消息属性进行分词，并返回分词后的 Token 列表。\n    def tokenize(self, message: Message, attribute: Text) -> List[ExtendedToken]:\n        \"\"\"Tokenizes the text of the provided attribute of the incoming message.\"\"\"\n        import jieba.posseg as pseg\n        text = message.get(attribute)\n\n        tokenized = pseg.cut(text)\n        tokens = []\n        current_position = 0\n        # breakpoint()\n        for word, flag in tokenized:\n            if word.strip() == \"\":\n                continue\n            word_start = text.find(word, current_position)\n            word_end = word_start + len(word)\n            tokens.append(ExtendedToken(word, word_start, word_end, pos=flag))\n            current_position = word_end\n\n        # for token in tokens:\n        #     print(f\"Word: {token.text}, POS: {token.pos}\")\n\n        return self._apply_token_pattern(tokens)\n\n    def process(self, messages: List[Message]) -> List[Message]:\n        \"\"\"Tokenize the incoming messages.\"\"\"\n        for message in messages:\n            for attribute in MESSAGE_ATTRIBUTES:\n                if isinstance(message.get(attribute), str):\n                    if attribute in [\n                        INTENT,\n                        ACTION_NAME,\n                        RESPONSE_IDENTIFIER_DELIMITER,\n                    ]:\n                        tokens = self._split_name(message, attribute)\n                    else:\n                        tokens = self.tokenize(message, attribute)\n\n                    # Store the original text_tokens without POS information\n                    message.set(TOKENS_NAMES[attribute], tokens)\n\n                    # Store the text_tokens with POS information in a new field\n                    text_tokens_with_pos = [\n                        {\"text\": t.text, \"start\": t.start,\n                            \"end\": t.end, \"pos\": t.pos}\n                        for t in tokens if isinstance(t, ExtendedToken)\n                    ]\n                    message.set(\"text_tokens_with_pos\",\n                                text_tokens_with_pos, True)\n        return messages\n\n    # 类方法，用于从模型存储中加载自定义词典。\n    @classmethod\n    def load(\n        cls,\n        config: Dict[Text, Any],\n        model_storage: ModelStorage,\n        resource: Resource,\n        execution_context: ExecutionContext,\n        **kwargs: Any,\n    ) -> JiebaTokenizer:\n        \"\"\"Loads a custom dictionary from model storage.\"\"\"\n        dictionary_path = config[\"dictionary_path\"]\n\n        # If a custom dictionary path is in the config we know that it should have\n        # been saved to the model storage.\n        if dictionary_path is not None:\n            try:\n                with model_storage.read_from(resource) as resource_directory:\n                    cls._load_custom_dictionary(str(resource_directory))\n            except ValueError:\n                logger.debug(\n                    f\"Failed to load {cls.__name__} from model storage. \"\n                    f\"Resource '{resource.name}' doesn't exist.\"\n                )\n        return cls(config, model_storage, resource)\n\n    # 用于将一个目录中的文件复制到另一个目录\n    @staticmethod\n    def _copy_files_dir_to_dir(input_dir: Text, output_dir: Text) -> None:\n        # make sure target path exists\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        target_file_list = glob.glob(f\"{input_dir}/*\")\n        for target_file in target_file_list:\n            shutil.copy2(target_file, output_dir)\n\n    # 将自定义词典持久化到模型存储中\n\n    def persist(self) -> None:\n        \"\"\"Persist the custom dictionaries.\"\"\"\n        dictionary_path = self._config[\"dictionary_path\"]\n        if dictionary_path is not None:\n            with self._model_storage.write_to(self._resource) as resource_directory:\n                self._copy_files_dir_to_dir(\n                    dictionary_path, str(resource_directory))\n\n\n# 扩展 rasa.nlu.tokenizers.tokenizer.Token 类以添加一个新属性来存储词性信息。\n# 用于在输出中观察到词性标注信息\nclass ExtendedToken(Token):\n    def __init__(\n        self,\n        text: Text,\n        start: int,\n        end: Optional[int] = None,\n        data: Optional[Dict[Text, Any]] = None,\n        lemma: Optional[Text] = None,\n        pos: Optional[Text] = None,\n    ) -> None:\n        super().__init__(text, start, end, data, lemma)\n        self.pos = pos\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, ExtendedToken):\n            return NotImplemented\n\n        return (\n            self.text == other.text\n            and self.start == other.start\n            and self.end == other.end\n            and self.data == other.data\n            and self.lemma == other.lemma\n            and self.pos == other.pos\n        )\n\n    def __repr__(self) -> Text:\n        return f\"ExtendedToken(text={self.text!r}, start={self.start}, end={self.end}, pos={self.pos!r})\"\n"}
{"type": "source_file", "path": "actions/weather.py", "content": "from typing import Any, Dict, List, Text, Optional\nfrom rasa_sdk import Action, Tracker\nfrom rasa_sdk.executor import CollectingDispatcher\nimport datetime\nimport os\nimport requests\nimport json\nfrom requests import ConnectionError, HTTPError, TooManyRedirects, Timeout\n\nfrom actions import ConfigLoader\nconfig = ConfigLoader.get_config()\n\n_config = next(\n    item for item in config['apis'] if item['name'] == 'Weather')\n\nAPI = _config['url']\nKEY = _config['key']\nUNIT = \"c\"  # 温度单位\nLANGUAGE = \"zh-Hans\"  # 查询结果的返回语言\none_day_timedelta = datetime.timedelta(days=1)\n\n\ndef fetch_weather(location: str, start=0, days=15) -> dict:\n    result = requests.get(\n        API,\n        params={\n            \"key\": KEY,\n            \"location\": location,\n            \"language\": LANGUAGE,\n            \"unit\": UNIT,\n            \"start\": start,\n            \"days\": days,\n        },\n        timeout=2,\n    )\n    return result.json()\n\n\ndef get_weather_by_date(location: str, date: datetime.date) -> dict:\n    day_timedelta = date - datetime.datetime.today().date()\n    day = day_timedelta // one_day_timedelta\n\n    return get_weather_by_day(location, day)\n\n\ndef get_weather_by_day(location: str, day=1) -> dict:\n    result = fetch_weather(location)\n    print(result)\n    normal_result = {\n        \"location\": result[\"results\"][0][\"location\"],\n        \"result\": result[\"results\"][0][\"daily\"][day],\n    }\n\n    return normal_result\n\n\ndef get_text_weather_date(\n    address: str, date_time: datetime.date, raw_date_time: str\n) -> str:\n    try:\n        result = get_weather_by_date(address, date_time)\n    except (ConnectionError, HTTPError, TooManyRedirects, Timeout) as e:\n        text_message = \"{}\".format(e)\n    else:\n        text_message_tpl = \"{} {} ({}) 的天气情况为：白天：{}；夜晚：{}；气温：{}-{} 度\"\n        text_message = text_message_tpl.format(\n            result[\"location\"][\"name\"],\n            raw_date_time,\n            result[\"result\"][\"date\"],\n            result[\"result\"][\"text_day\"],\n            result[\"result\"][\"text_night\"],\n            result[\"result\"][\"high\"],\n            result[\"result\"][\"low\"],\n        )\n\n    return text_message\n\n\ndef text_to_date(text_date: str) -> Optional[datetime.date]:\n    \"\"\"convert text based Chinese date info into datatime object\n\n    if the convert is not supprted will return None\n    \"\"\"\n\n    today = datetime.datetime.now()\n    one_more_day = datetime.timedelta(days=1)\n\n    if text_date == \"今天\":\n        return today.date()\n    if text_date == \"明天\":\n        return (today + one_more_day).date()\n    if text_date == \"后天\":\n        return (today + one_more_day * 2).date()\n\n    # Not supported by weather API provider freely\n    if text_date == \"大后天\":\n        # return 3\n        return (today + one_more_day * 3).date()\n\n    if text_date.startswith(\"星期\"):\n        # not supported yet\n        return None\n\n    if text_date.startswith(\"下星期\"):\n        # not supported yet\n        return None\n\n    # follow APIs are not supported by weather API provider freely\n    if text_date == \"昨天\":\n        return None\n    if text_date == \"前天\":\n        return None\n    if text_date == \"大前天\":\n        return None\n\n\nclass ActionWeatherForm(Action):\n    def name(self) -> Text:\n        return \"action_weather_form_submit\"\n\n    def run(\n        self, dispatch: CollectingDispatcher, tracker: Tracker, domain: Dict[Text, Any]\n    ) -> List[Dict]:\n        city = tracker.get_slot(\"address\")\n        date_text = tracker.get_slot(\"date-time\")\n\n        date_object = text_to_date(date_text)\n\n        if not date_object:  # parse date_time failed\n            msg = \"暂不支持查询 {} 的天气\".format([city, date_text])\n            dispatch.utter_message(msg)\n        else:\n            try:\n                weather_data = get_text_weather_date(\n                    city, date_object, date_text)\n            except Exception as e:\n                exec_msg = str(e)\n                dispatch.utter_message(exec_msg)\n            else:\n                dispatch.utter_message(weather_data)\n\n        return []\n"}
{"type": "source_file", "path": "actions/food.py", "content": "# This files contains your custom actions which can be used to run\n# custom Python code.\n#\n# See this guide on how to implement these action:\n# https://rasa.com/docs/rasa/custom-actions\n\n\nimport os\nfrom typing import Any, Text, Dict, List\n\nfrom rasa_sdk import Action, Tracker\nfrom rasa_sdk.executor import CollectingDispatcher\nimport requests\nfrom typing import Any, Dict, List, Text, Optional\n\nfrom actions import ConfigLoader\nconfig = ConfigLoader.get_config()\n\nclass ActionNutrient(Action):\n\n    def name(self) -> Text:\n        return \"action_get_nutrient\"\n\n    def run(self, dispatcher: CollectingDispatcher,\n            tracker: Tracker,\n            domain: Dict[Text, Any]) -> List[Dict[Text, Any]]:\n\n        food = tracker.get_slot(\"food\")\n\n        _config = next(\n            item for item in config['apis'] if item['name'] == 'Food')\n        url = _config['url']\n        api_key = _config['key']\n\n        params = {\n            'key': api_key,\n            'word': food,\n            'mode': 0\n        }\n        headers = {\n            'Content-Type': 'application/x-www-form-urlencoded',\n        }\n        response = requests.get(url=url, params=params, headers=headers)\n\n        if response.status_code != 200:\n            dispatcher.utter_message(text=\"我无法从服务器获取数据。\")\n            return []\n\n        data = response.json()\n\n        info_list = []  #\n        for new_data in data['result']['list'][:5]:\n            name = new_data['name']\n            rl = new_data['rl']\n            gai = new_data['gai']\n            tei = new_data['tei']\n            xin = new_data['xin']\n            xi = new_data['xi']\n            dbz = new_data['dbz']\n            zf = new_data['zf']\n            shhf = new_data['shhf']\n            wssa = new_data['wssa']\n            wsfc = new_data['wsfc']\n            wsse = new_data['wsse']\n            ssxw = new_data['ssxw']\n            dgc = new_data['dgc']\n            info_list.append(\n                f\"{name}[100mg]\\n热量(大卡):{rl} 钙:{gai} 铁:{tei} 锌:{xin} 硒:{xi} 蛋白质:{dbz} 脂肪:{zf} 碳水化合物:{shhf} 维生素A:{wssa} 维生素C:{wsfc} 维生素E:{wsse} 膳食纤维:{ssxw} 胆固醇:{dgc}\")\n\n        dispatcher.utter_message(\n            text='\\n'.join(info_list))\n\n        return []\n"}
{"type": "source_file", "path": "actions/news.py", "content": "# This files contains your custom actions which can be used to run\n# custom Python code.\n#\n# See this guide on how to implement these action:\n# https://rasa.com/docs/rasa/custom-actions\n\n\nimport os\nfrom typing import Any, Text, Dict, List\nimport yaml\nfrom rasa_sdk import Action, Tracker\nfrom rasa_sdk.executor import CollectingDispatcher\nimport requests\nfrom typing import Any, Dict, List, Text, Optional\n\n\nfrom actions import ConfigLoader\nconfig = ConfigLoader.get_config()\nclass ActionNews(Action):\n\n    def name(self) -> Text:\n        return \"action_get_news\"\n\n    def run(self, dispatcher: CollectingDispatcher,\n            tracker: Tracker,\n            domain: Dict[Text, Any]) -> List[Dict[Text, Any]]:\n        \n        _config = next(\n            item for item in config['apis'] if item['name'] == 'News')\n        url = _config['url']\n        api_key = _config['key']\n\n        params = {\n            'key': api_key,\n            'type': 'top',\n            'page': 1,\n            'page_size': 10,\n            'is_filter': 1\n        }\n        headers = {\n            'Content-Type': 'application/x-www-form-urlencoded',\n        }\n        response = requests.get(url=url, params=params, headers=headers)\n\n        if response.status_code != 200:\n            dispatcher.utter_message(text=\"我无法从服务器获取数据。\")\n            return []\n\n        data = response.json()\n\n        info_list = []  #\n        for new_data in data['result']['data']:\n            title = new_data['title']\n            author = new_data['author_name']\n            date = new_data['date']\n            info_list.append(\n                f\"{title}[{date}]\")\n\n        dispatcher.utter_message(\n            text='\\n'.join(info_list))\n\n        return []\n\n\nclass ActionWeiboHot(Action):\n\n    def name(self) -> Text:\n        return \"action_get_weibohot\"\n\n    def run(self, dispatcher: CollectingDispatcher,\n            tracker: Tracker,\n            domain: Dict[Text, Any]) -> List[Dict[Text, Any]]:\n        \n        _config = next(\n            item for item in config['apis'] if item['name'] == 'WeiboHot')\n        url = _config['url']\n        api_key = _config['key']\n\n        params = {\n            'key': api_key,\n        }\n        headers = {\n            'Content-Type': 'application/x-www-form-urlencoded',\n        }\n        response = requests.get(url=url, params=params, headers=headers)\n\n        if response.status_code != 200:\n            dispatcher.utter_message(text=\"我无法从服务器获取数据。\")\n            return []\n\n        data = response.json()\n\n        info_list = []  #\n        for new_data in data['result']['list'][:10]:\n            title = new_data['hotword']\n            hotwordnum = new_data['hotwordnum']\n            info_list.append(\n                f\"{title}(热度：{hotwordnum})\")\n\n        dispatcher.utter_message(\n            text='\\n'.join(info_list))\n\n        return []\n\n\nclass ActionToutiaoHot(Action):\n\n    def name(self) -> Text:\n        return \"action_get_toutiaohot\"\n\n    def run(self, dispatcher: CollectingDispatcher,\n            tracker: Tracker,\n            domain: Dict[Text, Any]) -> List[Dict[Text, Any]]:\n        \n        _config = next(\n            item for item in config['apis'] if item['name'] == 'ToutiaoHot')\n        url = _config['url']\n        api_key = _config['key']\n\n        params = {\n            'key': api_key,\n        }\n        headers = {\n            'Content-Type': 'application/x-www-form-urlencoded',\n        }\n        response = requests.get(url=url, params=params, headers=headers)\n\n        if response.status_code != 200:\n            dispatcher.utter_message(text=\"我无法从服务器获取数据。\")\n            return []\n\n        data = response.json()\n\n        info_list = []  #\n        for new_data in data['result']['list'][:10]:\n            title = new_data['word']\n            hotwordnum = new_data['hotindex']\n            info_list.append(\n                f\"{title}(热度：{hotwordnum})\")\n\n        dispatcher.utter_message(\n            text='\\n'.join(info_list))\n\n        return []\n"}
{"type": "source_file", "path": "components/custom_time_extractor.py", "content": "# HH:mm\n# hh:mm A/AM/PM\n# hh:mm A/AM/PM (with optional seconds)\n# ISO 8601 time format with an optional timezone (e.g., \"12:34:56+08:00\")\n\n\nimport re\nimport typing\nfrom typing import Any, Dict, List, Text, Type\n\nfrom rasa.engine.graph import ExecutionContext, GraphComponent\nfrom rasa.engine.recipes.default_recipe import DefaultV1Recipe\nfrom rasa.engine.storage.resource import Resource\nfrom rasa.engine.storage.storage import ModelStorage\nfrom rasa.shared.nlu.constants import ENTITIES, TEXT\nfrom rasa.nlu.extractors.extractor import EntityExtractorMixin\nfrom rasa.shared.nlu.training_data.message import Message\n\n@DefaultV1Recipe.register(\n    DefaultV1Recipe.ComponentType.ENTITY_EXTRACTOR,\n    is_trainable=False\n)\nclass CustomTimeExtractor(GraphComponent, EntityExtractorMixin):\n    \"\"\"Entity extractor which uses regular expressions to find time expressions.\"\"\"\n\n    @staticmethod\n    def get_default_config() -> Dict[Text, Any]:\n        \"\"\"The component's default config.\"\"\"\n        return {\n            \"time_pattern\": r'\\b(?:(?:2[0-3]|[01]?[0-9]):(?:[0-5]?[0-9])(?::(?:[0-5]?[0-9]))?(?:\\s?(?:A|P)M)?(?:[+-](?:2[0-3]|[01]?[0-9]):(?:[0-5]?[0-9]))?)\\b'\n        }\n\n    def __init__(self, config: Dict[Text, Any]) -> None:\n        \"\"\"Initialize CustomTimeExtractor.\"\"\"\n        self._config = config\n\n    @classmethod\n    def create(\n        cls,\n        config: Dict[Text, Any],\n        model_storage: ModelStorage,\n        resource: Resource,\n        execution_context: ExecutionContext,\n    ) -> GraphComponent:\n        \"\"\"Creates a new component.\"\"\"\n        return cls(config)\n\n    def process(self, messages: List[Message]) -> List[Message]:\n        \"\"\"Extract time expressions using regular expressions.\n\n        Args:\n            messages: List of messages to process.\n\n        Returns: The processed messages.\n        \"\"\"\n        time_pattern = re.compile(self._config[\"time_pattern\"])\n\n        for message in messages:\n            text = message.get(TEXT)\n            matches = time_pattern.finditer(text)\n\n            extracted_entities = []\n            for match in matches:\n                start, end = match.span()\n                value = match.group()\n\n                entity = {\n                    \"entity\": \"time\",\n                    \"value\": value,\n                    \"start\": start,\n                    \"confidence\": None,\n                    \"end\": end,\n                    \"extractor\": \"CustoTimeExtractor\",\n                }\n                extracted_entities.append(entity)\n\n            message.set(\n                ENTITIES, message.get(ENTITIES, []) + extracted_entities, add_to_output=True\n            )\n\n        return messages\n"}
{"type": "source_file", "path": "actions/llm.py", "content": "import os\nfrom typing import Any, Dict, List, Text, Optional\nfrom rasa_sdk import Action, Tracker\nfrom rasa_sdk.executor import CollectingDispatcher\nimport requests\nimport json\nfrom langchain_openai import ChatOpenAI\nfrom langchain.chains import RetrievalQA\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_community.document_loaders import WebBaseLoader, DirectoryLoader\nfrom langchain.document_loaders.csv_loader import CSVLoader\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_community.vectorstores import FAISS\n\nfrom actions import ConfigLoader\nconfig = ConfigLoader.get_config()\n\n\nclass ActionOpenai(Action):\n    def name(self) -> Text:\n        return \"action_openai_fallback\"\n\n    def run(\n        self, dispatcher: CollectingDispatcher, tracker: Tracker, domain: Dict[Text, Any]\n    ) -> List[Dict]:\n\n        _config = next(\n            item for item in config['apis'] if item['name'] == 'OpenAI')\n        url = _config['url']\n        api_key = _config['key']\n\n        llm = ChatOpenAI(\n            openai_api_base=url,\n            openai_api_key=api_key,\n        )\n\n        # 初始化 openai 的 embeddings 对象\n        embeddings = OpenAIEmbeddings(\n            openai_api_base=url,\n            openai_api_key=api_key,\n        )\n\n        # res = llm.invoke(\"hi\")\n        # print(res)\n\n        output_parser = StrOutputParser()\n\n        loader = DirectoryLoader(\n            'document', glob='**/*.txt')\n        documents = loader.load()\n        # 初始化加载器\n        text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n        # 切割加载的 document\n        split_docs = text_splitter.split_documents(documents)\n\n        db = Chroma.from_documents(split_docs, embeddings)\n        retriever = db.as_retriever()\n\n        qa = RetrievalQA.from_chain_type(\n            llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n\n        result = qa({\"query\": tracker.latest_message['text']})\n\n        res = result['result']\n        dispatcher.utter_message(text=res)\n\n        return []\n"}
{"type": "source_file", "path": "actions/actionContextSensitiveError.py", "content": "from typing import Any, Text, Dict, List\nfrom rasa_sdk import Action, Tracker\nfrom rasa_sdk.events import UserUtteranceReverted\nfrom rasa_sdk.executor import CollectingDispatcher\n\nclass ActionContextSensitiveError(Action):\n\n    def name(self) -> Text:\n        return \"action_context_sensitive_error\"\n\n    def run(self, dispatcher: CollectingDispatcher, tracker: Tracker, domain: Dict[Text, Any]) -> List[Dict[Text, Any]]:\n        # 获取最近的意图\n        last_intent = tracker.latest_message['intent'].get('name')\n\n        # 根据最近的意图选择上下文相关的错误回复\n        if last_intent == \"intent1\":\n            response = \"抱歉，我没有理解您关于intent1的问题。请尝试重新提问。\"\n        elif last_intent == \"intent2\":\n            response = \"对不起，我无法回答您关于intent2的问题。请重新描述您的疑问。\"\n        else:\n            dispatcher.utter_message(template=\"utter_default\")\n            return []\n\n        # 发送上下文相关的错误回复\n        dispatcher.utter_message(text=response)\n\n        return []\n"}
{"type": "source_file", "path": "actions/actions.py", "content": "# This files contains your custom actions which can be used to run\n# custom Python code.\n#\n# See this guide on how to implement these action:\n# https://rasa.com/docs/rasa/custom-actions\n\n\nfrom typing import Any, Text, Dict, List\n\nfrom rasa_sdk import Action, Tracker\nfrom rasa_sdk.executor import CollectingDispatcher\nclass ActionFallback(Action):\n\n    def name(self) -> Text:\n        return \"action_fallback\"\n\n    def run(self, dispatcher: CollectingDispatcher,\n            tracker: Tracker,\n            domain: Dict[Text, Any]) -> List[Dict[Text, Any]]:\n\n        dispatcher.utter_message(text=\"我不知道你在说什么_(:з」∠)_\")\n\n        return []\n"}
{"type": "source_file", "path": "actions/train.py", "content": "# This files contains your custom actions which can be used to run\n# custom Python code.\n#\n# See this guide on how to implement these action:\n# https://rasa.com/docs/rasa/custom-actions\n\n\nfrom typing import Any, Text, Dict, List\n\nfrom rasa_sdk import Action, Tracker\nfrom rasa_sdk.executor import CollectingDispatcher\nimport time\nimport json\nimport requests\nfrom typing import Any, Dict, List, Text, Optional\nimport datetime\n\nfrom actions import ConfigLoader\nconfig = ConfigLoader.get_config()\n\nwith open('dataset/station_map.json', 'r', encoding='utf-8') as json_file:\n    station_map = json.load(json_file)\n\n\ndef text_to_date(text_date: str) -> Optional[datetime.date]:\n    today = datetime.datetime.now()\n    one_more_day = datetime.timedelta(days=1)\n    if text_date == \"今天\":\n        return today.date()\n    if text_date == \"明天\":\n        return (today + one_more_day).date()\n    if text_date == \"后天\":\n        return (today + one_more_day * 2).date()\n    if text_date == \"大后天\":\n        return (today + one_more_day * 3).date()\n\n\nclass ActionQueryTrain(Action):\n\n    def name(self) -> Text:\n        return \"action_query_train\"\n\n    def run(self, dispatcher: CollectingDispatcher,\n            tracker: Tracker,\n            domain: Dict[Text, Any]) -> List[Dict[Text, Any]]:\n\n        # 提取地点实体\n        departure = tracker.get_slot(\"departure\")\n        destination = tracker.get_slot(\"destination\")\n        date_text = tracker.get_slot(\"date-time\")\n\n        date_object = text_to_date(date_text)\n\n        departure_code = station_map.get(departure)\n        destination_code = station_map.get(destination)\n\n        if not departure_code or not destination_code:\n            dispatcher.utter_message(text=\"我不知道这些地方的代码。\")\n            return []\n        _config = next(\n            item for item in config['apis'] if item['name'] == 'Train')\n        url = _config['url']\n\n        params = {\n            'leftTicketDTO.train_date': date_object,\n            'leftTicketDTO.from_station': departure_code,\n            'leftTicketDTO.to_station': destination_code,\n            'purpose_codes': 'ADULT',\n        }\n        headers = {\n            'Cookie': '_uab_collina=163108019860709243490927; JSESSIONID=3A879F34238B594124705B10D7C0B0E6; BIGipServerotn=3956736266.64545.0000; guidesStatus=off; highContrastMode=defaltMode; cursorStatus=off; BIGipServerpassport=870842634.50215.0000; RAIL_EXPIRATION=1631354049020; RAIL_DEVICEID=jY49UGp1PWZZ0cY6CWj2wmKFDH60qsPXbu7L4D2DjNDJSM4sbqZmmlUm62-6L3k9SNtBAUgBPn7Rh1-FAxka97-nHNpT3QIh5YIXtw3mGao0mjLNkIv2ayvwqxWyFhdbos5_ziUA3XVil7awDZ0EjzKBAWdl22Hu; route=495c805987d0f5c8c84b14f60212447d; _jc_save_fromStation=%u957F%u6C99%2CCSQ; _jc_save_fromDate=2021-09-08; _jc_save_toDate=2021-09-08; _jc_save_wfdc_flag=dc; _jc_save_toStation=%u5CB3%u9633%u4E1C%2CYIQ',\n            # 'Host': 'kyfw.12306.cn',\n            # 'Referer': 'https://kyfw.12306.cn/otn/leftTicket/init?linktypeid=dc&fs=%E9%95%BF%E6%B2%99,CSQ&ts=%E5%B2%B3%E9%98%B3%E4%B8%9C,YIQ&date=2021-09-08&flag=N,N,Y',\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n        }\n        response = requests.get(url=url, params=params, headers=headers)\n\n        if response.status_code != 200:\n            dispatcher.utter_message(text=\"我无法从服务器获取数据。\")\n            return []\n\n        data = response.json()\n\n        if 'data' not in data or 'result' not in data['data'] or not data['data']['result']:\n            dispatcher.utter_message(text=\"我没能找到火车信息。\")\n            return []\n\n        # 提取火车信息\n        train_info_list = []  # 创建一个空列表，用于存储所有的火车信息\n        for train_data in data['data']['result']:\n            result = train_data.split('|')\n\n            train_no = result[3]  # 火车编号\n            departure_city = departure  # 出发城市\n            destination_city = destination  # 到达城市\n            departure_time = result[8]  # 出发时间\n            arrival_time = result[9]  # 到达时间\n\n            # 将火车信息添加到列表中\n            train_info_list.append(\n                f\"{train_no} {departure_city}-{destination_city} {departure_time}-{arrival_time}\")\n\n        # 将所有的火车信息合并成一条消息，并发送给用户\n        dispatcher.utter_message(\n            text='\\n'.join(train_info_list))\n\n        return []\n"}
{"type": "source_file", "path": "server/app.py", "content": "import os\r\nfrom flask import Flask, render_template, request, jsonify\r\nfrom flask_cors import CORS\r\nimport sqlite3\r\nimport requests\r\n\r\napp = Flask(__name__)\r\nCORS(app)\r\n\r\n# 简易前端服务\r\nRASA_SERVER_URL = \"http://localhost:5005\"  # 根据您的 Rasa 服务器配置进行修改\r\n\r\n@app.route('/')\r\ndef home():\r\n    return render_template(\"index.html\")\r\n\r\n@app.route('/send_message', methods=['POST'])\r\ndef send_message():\r\n    message = request.form['message']\r\n    response = requests.post(f\"{RASA_SERVER_URL}/webhooks/rest/webhook\", json={\"message\": message})\r\n    return jsonify(response.json())\r\n\r\n\r\n# 获取用户对应session\r\ndef get_sessions_by_user(user_id):\r\n    connection = sqlite3.connect(\"user_sessions.db\")\r\n    cursor = connection.cursor()\r\n\r\n    cursor.execute(\"SELECT session_id FROM user_sessions WHERE user_id=?\", (user_id,))\r\n    sessions = cursor.fetchall()\r\n\r\n    if not sessions:\r\n        connection.close()\r\n        return []\r\n\r\n    sessions = [row[0] for row in sessions]\r\n    connection.close()\r\n    return sessions\r\n\r\n\r\n@app.route(\"/user_sessions/<user_id>\", methods=[\"GET\"])\r\ndef get_user_sessions(user_id):\r\n    sessions = get_sessions_by_user(user_id)\r\n    return jsonify({\"sessions\": sessions})\r\n\r\n\r\n# 创建新对话\r\n@app.route('/new_conversation/<user_id>/<session_id>', methods=['GET'])\r\ndef new_conversation(user_id, session_id):\r\n    conn = sqlite3.connect('user_sessions.db')\r\n    with conn:\r\n        cur = conn.cursor()\r\n        # 检查是否存在相同的 session_id\r\n        cur.execute(\"SELECT session_id FROM user_sessions WHERE user_id=? AND session_id=?\", (user_id, session_id))\r\n        existing_session_id = cur.fetchone()\r\n\r\n        # 如果不存在重复的 session_id，则插入新会话\r\n        if not existing_session_id:\r\n            cur.execute(\"INSERT INTO user_sessions (user_id, session_id) VALUES (?, ?)\", (user_id, session_id))\r\n            conn.commit()\r\n\r\n    return \"OK\", 200\r\n\r\n\r\n\r\n# 删除对话（tracker_store.db）\r\n@app.route(\"/delete_conversation/<session_id>\", methods=[\"DELETE\"])\r\ndef delete_conversation(session_id):\r\n    try:\r\n        # Connect to the SQLite database\r\n        conn = sqlite3.connect(\"tracker_store.db\")\r\n        cursor = conn.cursor()\r\n\r\n        # Delete the conversation from the database\r\n        cursor.execute(\"DELETE FROM events WHERE sender_id = ?\", (session_id,))\r\n        conn.commit()\r\n\r\n        # Close the database connection\r\n        conn.close()\r\n        return jsonify({\"success\": True}), 200\r\n    except Exception as e:\r\n        print(f\"Error deleting conversation: {e}\")\r\n        return jsonify({\"error\": \"Failed to delete conversation\"}), 500\r\n\r\n# 删除对话（user_sessions.db）\r\n@app.route(\"/delete_session/<session_id>\", methods=[\"DELETE\"])\r\ndef delete_session(session_id):\r\n    try:\r\n        conn = sqlite3.connect('user_sessions.db')\r\n        cur = conn.cursor()\r\n        cur.execute(\"DELETE FROM user_sessions WHERE session_id = ?\", (session_id,))\r\n        conn.commit()\r\n        conn.close()\r\n\r\n        return jsonify({\"success\": True}), 200\r\n    except Exception as e:\r\n        print(f\"Error deleting conversation: {e}\")\r\n        return jsonify({\"error\": \"Failed to delete session\"}), 500\r\n\r\n# 获取可用的模型列表\r\ncurrent_script_path = os.path.abspath(__file__)\r\nroot_dir = os.path.dirname(current_script_path)\r\nMODELS_DIR = os.path.join(root_dir, \"../models\")\r\nMODELS_DIR = os.path.abspath(MODELS_DIR)\r\ndef get_models_list():\r\n    models = []\r\n    for entry in os.listdir(MODELS_DIR):\r\n        if os.path.isfile(os.path.join(MODELS_DIR, entry)):\r\n            models.append(entry)\r\n    return models\r\n@app.route(\"/models\", methods=[\"GET\"])\r\ndef get_models():\r\n    models = get_models_list()\r\n    return jsonify({\"models\": models})\r\n\r\nif __name__ == '__main__':\r\n    app.run(port= 5001 ,debug=True)\r\n"}
{"type": "source_file", "path": "components/keyword.py", "content": "# custom_components.py\n\nfrom typing import Any, Dict, Optional, Text, List\nimport yaml\nfrom rasa.shared.nlu.constants import (INTENT)\nfrom rasa.engine.graph import GraphComponent, ExecutionContext\nfrom rasa.engine.recipes.default_recipe import DefaultV1Recipe\nfrom rasa.shared.nlu.constants import TEXT\nfrom rasa.shared.nlu.training_data.message import Message\n\n\n@DefaultV1Recipe.register(\n    [DefaultV1Recipe.ComponentType.INTENT_CLASSIFIER,\n        DefaultV1Recipe.ComponentType.ENTITY_EXTRACTOR], is_trainable=False\n)\nclass KeywordComponent(GraphComponent):\n    name = \"KeywordComponent\"\n\n    def __init__(self, config: Dict[Text, Any], keywords: List[Dict[Text, Any]]) -> None:\n        self.config = config\n        self.keywords = keywords\n\n    @staticmethod\n    def load_keywords(file_path: Text) -> List[Dict[Text, Any]]:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            data = yaml.safe_load(f)\n        return data.get('keywords', [])\n\n    @staticmethod\n    def get_default_config() -> Dict[Text, Any]:\n        return {\"keywords_file\": \"keywords.yml\"}\n\n    @classmethod\n    def create(\n        cls,\n        config: Dict[Text, Any],\n        model_storage: Any,\n        resource: Any,\n        execution_context: ExecutionContext,\n    ) -> \"KeywordComponent\":\n        keywords_file = config.get(\"keywords_file\", \"keywords.yml\")\n        keywords = cls.load_keywords(keywords_file)\n        return cls(config, keywords)\n\n    def process(self, messages: List[Message]) -> List[Message]:\n        for message in messages:\n            text = message.get(TEXT)\n            for item in self.keywords:\n                if item['keyword'] in text:\n                    intent = {\"name\": \"keyword_intent\", \"confidence\": 1.0}\n                    message.set(\n                        INTENT, intent, add_to_output=True)\n                    message.set(\"intent_ranking\", [intent], add_to_output=True)\n                    message.set(\n                        \"entities\", [{\"entity\": \"keyword\", \"value\": item['keyword']}], add_to_output=True)\n                    break\n        return messages\n"}
{"type": "source_file", "path": "server/start_services.py", "content": "import subprocess\r\nimport webbrowser\r\nimport os\r\nimport signal\r\nimport time\r\nimport sqlite3\r\nimport requests\r\nfrom sqlite3 import Error\r\n\r\n\r\nclass RasaServicesManager:\r\n    def __init__(self):\r\n        self.processes = []\r\n\r\n    def start_service(self, command, name, check_url=None):\r\n        try:\r\n            process = subprocess.Popen(\r\n                command, stdout=None, stderr=None)\r\n            self.processes.append((name, process))\r\n            print(f\"Starting {name}...\")\r\n\r\n            if check_url:\r\n                # 等待服务可用\r\n                for _ in range(60):  # 最长等待 60 秒\r\n                    try:\r\n                        response = requests.get(check_url)\r\n                        if response.status_code == 200:\r\n                            print(f\"{name} started successfully.\")\r\n                            break\r\n                    except requests.exceptions.ConnectionError:\r\n                        pass\r\n                    time.sleep(1)\r\n                else:\r\n                    print(\r\n                        f\"Warning: {name} may not have started successfully within the expected time.\")\r\n            else:\r\n                print(f\"{name} started successfully.\")\r\n        except Exception as e:\r\n            print(f\"Failed to start {name}: {e}\")\r\n\r\n    def stop_all_services(self):\r\n        for name, process in self.processes:\r\n            try:\r\n                os.kill(process.pid, signal.SIGTERM)\r\n                print(f\"{name} stopped successfully.\")\r\n            except Exception as e:\r\n                print(f\"Failed to stop {name}: {e}\")\r\n\r\n    def wait_for_services(self):\r\n        try:\r\n            while True:\r\n                time.sleep(1)\r\n        except KeyboardInterrupt:\r\n            print(\"\\nStopping all services...\")\r\n            self.stop_all_services()\r\n\r\n\r\ndef create_user_sessions_table():\r\n    try:\r\n        conn = sqlite3.connect(\"user_sessions.db\")\r\n        with conn:\r\n            cur = conn.cursor()\r\n            cur.execute('''CREATE TABLE IF NOT EXISTS user_sessions (\r\n                            user_id TEXT NOT NULL,\r\n                            session_id TEXT NOT NULL\r\n                           );''')\r\n        print(\"Database checked/created successfully.\")\r\n    except Error as e:\r\n        print(f\"Error creating database: {e}\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    manager = RasaServicesManager()\r\n\r\n    os.chdir(os.path.dirname(os.path.abspath(__file__ + \"/../\")))\r\n\r\n    # Check and create the user_sessions database if it doesn't exist\r\n    create_user_sessions_table()\r\n\r\n    # Start Rasa action server\r\n    manager.start_service([\"rasa\", \"run\", \"actions\"], \"Rasa Action Server\")\r\n\r\n    # Start Rasa server, wait until it's available\r\n    manager.start_service(\r\n        [\"rasa\", \"run\", \"--enable-api\", \"--cors\", \"*\"],\r\n        \"Rasa Server\",\r\n        check_url=\"http://localhost:5005/status\"\r\n    )\r\n\r\n    # Start your backend server (e.g., Flask)\r\n    manager.start_service([\"python\", \"server/app.py\"],\r\n                          \"Backend Server\", check_url=\"http://localhost:5001\")\r\n\r\n    # Open the web page in a new browser tab\r\n    webbrowser.open(\"http://localhost:5001\")\r\n\r\n    # Wait for the services to finish\r\n    manager.wait_for_services()\r\n"}
{"type": "source_file", "path": "server/createe_user_seession_db.py", "content": "import sqlite3\nfrom sqlite3 import Error\n\ndef create_connection(db_file):\n    conn = None\n    try:\n        conn = sqlite3.connect(db_file)\n    except Error as e:\n        print(e)\n\n    if conn is not None:\n        return conn\n\ndef create_user_sessions_table():\n    conn = create_connection(\"user_sessions.db\")\n    with conn:\n        cur = conn.cursor()\n        cur.execute('''CREATE TABLE IF NOT EXISTS user_sessions (\n                        user_id TEXT NOT NULL,\n                        session_id TEXT NOT NULL\n                       );''')\n\ncreate_user_sessions_table()\n"}
{"type": "source_file", "path": "server/visualize_story.py", "content": "import os\nimport subprocess\nimport sys\n\ndef visualize_story():\n    try:\n        # 使用 'rasa visualize' 命令生成故事的 HTML 可视化文件\n        subprocess.run([\"rasa\", \"visualize\", \"--domain\", \"domain\"], check=True)\n\n        current_dir = os.path.dirname(os.path.abspath(__file__))\n\n        graph_html_path = os.path.join(current_dir, '..', 'graph.html')\n        # 读取生成的 HTML 文件内容\n        with open(graph_html_path, \"r\", encoding=\"utf-8\") as file:\n            html_content = file.read()\n\n        return html_content\n    except Exception as e:\n        print(\"Error: \", e)\n        return None\n"}
{"type": "source_file", "path": "utils/csv_to_rasa.py", "content": "import pandas as pd\n\ncsv_file = \"csv_file.csv\"\noutput_nlu = \"nlu.yml\"\noutput_domain = \"domain.yml\"\noutput_stories = \"stories.yml\"\n\ndef main():\n    data = pd.read_csv(csv_file)\n\n    unique_intents = set()\n    unique_responses = {}\n    stories = {}\n    generated_stories = set()\n\n    for index, row in data.iterrows():\n        story_id = row[\"story_id\"] if not pd.isna(row[\"story_id\"]) else f\"default_story_{index}\"\n        user_message, reply, intent = row[\"user_message\"], row[\"reply\"], row[\"intent\"]\n        unique_intents.add(intent)\n\n        if intent not in unique_responses:\n            unique_responses[intent] = reply\n\n        if story_id not in stories:\n            stories[story_id] = []\n\n        stories[story_id].append((intent, user_message))\n\n    with open(output_nlu, \"w\", encoding=\"utf-8\") as nlu_out:\n        nlu_out.write(\"version: \\\"3.1\\\"\\n\")\n        nlu_out.write(\"nlu:\\n\")\n\n        for intent in unique_intents:\n            nlu_examples = \"\\n\".join([f\"    - {example}\" for story_id in stories for intent_name, example in stories[story_id] if intent_name == intent])\n            nlu_out.write(f\"- intent: {intent}\\n\")\n            nlu_out.write(\"  examples: |\\n\")\n            nlu_out.write(nlu_examples + \"\\n\\n\")\n\n    with open(output_domain, \"w\", encoding=\"utf-8\") as domain_out:\n        domain_out.write(\"version: \\\"3.1\\\"\\n\")\n        domain_out.write(\"intents:\\n\")\n        domain_out.write(\"\\n\".join([f\"  - {intent}\" for intent in unique_intents]) + \"\\n\")\n\n        domain_out.write(\"responses:\\n\")\n        for intent, reply in unique_responses.items():\n            domain_out.write(f\"  utter_{intent}:\\n\")\n            domain_out.write(f\"  - text: \\\"{reply}\\\"\\n\")\n\n    with open(output_stories, \"w\", encoding=\"utf-8\") as stories_out:\n        stories_out.write(\"version: \\\"3.1\\\"\\n\")\n        stories_out.write(\"stories:\\n\")\n\n        for story_id, story in stories.items():\n            story_str = \"\\n\".join([f\"  - intent: {intent}\\n  - action: utter_{intent}\" for intent, _ in story])\n\n            if story_str not in generated_stories:\n                stories_out.write(f\"- story: conversation {story_id}\\n\")\n                stories_out.write(\"  steps:\\n\")\n                stories_out.write(story_str + \"\\n\\n\")\n                generated_stories.add(story_str)\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "source/diet_classifier.py", "content": "from __future__ import annotations\nimport copy\nimport logging\nfrom collections import defaultdict\nfrom pathlib import Path\n\nfrom rasa.exceptions import ModelNotFound\nfrom rasa.nlu.featurizers.featurizer import Featurizer\n\nimport numpy as np\nimport scipy.sparse\nimport tensorflow as tf\n\nfrom typing import Any, Dict, List, Optional, Text, Tuple, Union, TypeVar, Type\n\nfrom rasa.engine.graph import ExecutionContext, GraphComponent\nfrom rasa.engine.recipes.default_recipe import DefaultV1Recipe\nfrom rasa.engine.storage.resource import Resource\nfrom rasa.engine.storage.storage import ModelStorage\nfrom rasa.nlu.extractors.extractor import EntityExtractorMixin\nfrom rasa.nlu.classifiers.classifier import IntentClassifier\nimport rasa.shared.utils.io\nimport rasa.utils.io as io_utils\nimport rasa.nlu.utils.bilou_utils as bilou_utils\nfrom rasa.shared.constants import DIAGNOSTIC_DATA\nfrom rasa.nlu.extractors.extractor import EntityTagSpec\nfrom rasa.nlu.classifiers import LABEL_RANKING_LENGTH\nfrom rasa.utils import train_utils\nfrom rasa.utils.tensorflow import rasa_layers\nfrom rasa.utils.tensorflow.models import RasaModel, TransformerRasaModel\nfrom rasa.utils.tensorflow.model_data import (\n    RasaModelData,\n    FeatureSignature,\n    FeatureArray,\n)\nfrom rasa.nlu.constants import TOKENS_NAMES, DEFAULT_TRANSFORMER_SIZE\nfrom rasa.shared.nlu.constants import (\n    SPLIT_ENTITIES_BY_COMMA_DEFAULT_VALUE,\n    TEXT,\n    INTENT,\n    INTENT_RESPONSE_KEY,\n    ENTITIES,\n    ENTITY_ATTRIBUTE_TYPE,\n    ENTITY_ATTRIBUTE_GROUP,\n    ENTITY_ATTRIBUTE_ROLE,\n    NO_ENTITY_TAG,\n    SPLIT_ENTITIES_BY_COMMA,\n)\nfrom rasa.shared.exceptions import InvalidConfigException\nfrom rasa.shared.nlu.training_data.training_data import TrainingData\nfrom rasa.shared.nlu.training_data.message import Message\nfrom rasa.utils.tensorflow.constants import (\n    LABEL,\n    IDS,\n    HIDDEN_LAYERS_SIZES,\n    RENORMALIZE_CONFIDENCES,\n    SHARE_HIDDEN_LAYERS,\n    TRANSFORMER_SIZE,\n    NUM_TRANSFORMER_LAYERS,\n    NUM_HEADS,\n    BATCH_SIZES,\n    BATCH_STRATEGY,\n    EPOCHS,\n    RANDOM_SEED,\n    LEARNING_RATE,\n    RANKING_LENGTH,\n    LOSS_TYPE,\n    SIMILARITY_TYPE,\n    NUM_NEG,\n    SPARSE_INPUT_DROPOUT,\n    DENSE_INPUT_DROPOUT,\n    MASKED_LM,\n    ENTITY_RECOGNITION,\n    TENSORBOARD_LOG_DIR,\n    INTENT_CLASSIFICATION,\n    EVAL_NUM_EXAMPLES,\n    EVAL_NUM_EPOCHS,\n    UNIDIRECTIONAL_ENCODER,\n    DROP_RATE,\n    DROP_RATE_ATTENTION,\n    CONNECTION_DENSITY,\n    NEGATIVE_MARGIN_SCALE,\n    REGULARIZATION_CONSTANT,\n    SCALE_LOSS,\n    USE_MAX_NEG_SIM,\n    MAX_NEG_SIM,\n    MAX_POS_SIM,\n    EMBEDDING_DIMENSION,\n    BILOU_FLAG,\n    KEY_RELATIVE_ATTENTION,\n    VALUE_RELATIVE_ATTENTION,\n    MAX_RELATIVE_POSITION,\n    AUTO,\n    BALANCED,\n    CROSS_ENTROPY,\n    TENSORBOARD_LOG_LEVEL,\n    CONCAT_DIMENSION,\n    FEATURIZERS,\n    CHECKPOINT_MODEL,\n    SEQUENCE,\n    SENTENCE,\n    SEQUENCE_LENGTH,\n    DENSE_DIMENSION,\n    MASK,\n    CONSTRAIN_SIMILARITIES,\n    MODEL_CONFIDENCE,\n    SOFTMAX,\n    RUN_EAGERLY,\n)\n\nlogger = logging.getLogger(__name__)\n\nSPARSE = \"sparse\"\nDENSE = \"dense\"\nLABEL_KEY = LABEL\nLABEL_SUB_KEY = IDS\n\nPOSSIBLE_TAGS = [ENTITY_ATTRIBUTE_TYPE, ENTITY_ATTRIBUTE_ROLE, ENTITY_ATTRIBUTE_GROUP]\n\n\nDIETClassifierT = TypeVar(\"DIETClassifierT\", bound=\"DIETClassifier\")\n\n\n@DefaultV1Recipe.register(\n    [\n        DefaultV1Recipe.ComponentType.INTENT_CLASSIFIER,\n        DefaultV1Recipe.ComponentType.ENTITY_EXTRACTOR,\n    ],\n    is_trainable=True,\n)\nclass DIETClassifier(GraphComponent, IntentClassifier, EntityExtractorMixin):\n    \"\"\"A multi-task model for intent classification and entity extraction.\n\n    DIET is Dual Intent and Entity Transformer.\n    The architecture is based on a transformer which is shared for both tasks.\n    A sequence of entity labels is predicted through a Conditional Random Field (CRF)\n    tagging layer on top of the transformer output sequence corresponding to the\n    input sequence of tokens. The transformer output for the ``__CLS__`` token and\n    intent labels are embedded into a single semantic vector space. We use the\n    dot-product loss to maximize the similarity with the target label and minimize\n    similarities with negative samples.\n    \"\"\"\n\n    # 必须的组件：Featurizer\n    @classmethod\n    def required_components(cls) -> List[Type]:\n        \"\"\"Components that should be included in the pipeline before this component.\"\"\"\n        return [Featurizer]\n\n    # 默认配置\n    @staticmethod\n    def get_default_config() -> Dict[Text, Any]:\n        \"\"\"The component's default config (see parent class for full docstring).\"\"\"\n        # please make sure to update the docs when changing a default parameter\n        return {\n            # ## Architecture of the used neural network\n            # Hidden layer sizes for layers before the embedding layers for user message\n            # and labels.\n            # The number of hidden layers is equal to the length of the corresponding\n            # list.\n            # 此参数允许您为用户消息和意图定义前馈层的数量及其输出维度（默认值：文本：[]，标签：[]）。\n            # 列表中的每个条目都对应一个前馈层。例如，如果设置text:[256，128]，\n            # 我们将在转换器前面添加两个前馈层。输入token的向量（来自用户消息）将被传递到这些层。\n            # 第一层的输出维度为256，第二层的输出维度为128。\n            # 如果使用空列表（默认行为），则不会添加前馈层。\n            # 确保只使用正整数值。\n            # 通常使用二次幂的数字，第二个值小于或等于前一个值。\n            HIDDEN_LAYERS_SIZES: {TEXT: [], LABEL: []},\n            # Whether to share the hidden layer weights between user message and labels.\n            # 用户消息与标签之间是否共享隐藏层权重。\n            SHARE_HIDDEN_LAYERS: False,\n            # Number of units in transformer\n            # Transformer编码器中的隐藏层大小，默认值256。\n            TRANSFORMER_SIZE: DEFAULT_TRANSFORMER_SIZE,\n            # Number of transformer layers\n            # Transformer编码器的层数，默认值2。\n            NUM_TRANSFORMER_LAYERS: 2,\n            # Number of attention heads in transformer\n            # transformer中的注意力头数量\n            NUM_HEADS: 4,\n            # If 'True' use key relative embeddings in attention\n            KEY_RELATIVE_ATTENTION: False,\n            # If 'True' use value relative embeddings in attention\n            VALUE_RELATIVE_ATTENTION: False,\n            # Max position for relative embeddings. Only in effect if key- or value\n            # relative attention are turned on\n            MAX_RELATIVE_POSITION: 5,\n            # Use a unidirectional or bidirectional encoder.\n            UNIDIRECTIONAL_ENCODER: False,\n            # ## Training parameters\n            # Initial and final batch sizes:\n            # Batch size will be linearly increased for each epoch.\n            BATCH_SIZES: [64, 256],\n            # Strategy used when creating batches.\n            # Can be either 'sequence' or 'balanced'.\n            BATCH_STRATEGY: BALANCED,\n            # Number of epochs to train\n            EPOCHS: 300,\n            # Set random seed to any 'int' to get reproducible results\n            RANDOM_SEED: None,\n            # Initial learning rate for the optimizer\n            LEARNING_RATE: 0.001,\n            # ## Parameters for embeddings\n            # Dimension size of embedding vectors\n            # 输入表示（词嵌入）的维度，默认20。\n            EMBEDDING_DIMENSION: 20,\n            # Dense dimension to use for sparse features.\n            DENSE_DIMENSION: {TEXT: 128, LABEL: 20},\n            # Default dimension to use for concatenating sequence and sentence features.\n            CONCAT_DIMENSION: {TEXT: 128, LABEL: 20},\n            # The number of incorrect labels. The algorithm will minimize\n            # their similarity to the user input during training.\n            NUM_NEG: 20,\n            # Type of similarity measure to use, either 'auto' or 'cosine' or 'inner'.\n            SIMILARITY_TYPE: AUTO,\n            # The type of the loss function, either 'cross_entropy' or 'margin'.\n            LOSS_TYPE: CROSS_ENTROPY,\n            # Number of top intents for which confidences should be reported.\n            # Set to 0 if confidences for all intents should be reported.\n            RANKING_LENGTH: LABEL_RANKING_LENGTH,\n            # Indicates how similar the algorithm should try to make embedding vectors\n            # for correct labels.\n            # Should be 0.0 < ... < 1.0 for 'cosine' similarity type.\n            MAX_POS_SIM: 0.8,\n            # Maximum negative similarity for incorrect labels.\n            # Should be -1.0 < ... < 1.0 for 'cosine' similarity type.\n            MAX_NEG_SIM: -0.4,\n            # If 'True' the algorithm only minimizes maximum similarity over\n            # incorrect intent labels, used only if 'loss_type' is set to 'margin'.\n            USE_MAX_NEG_SIM: True,\n            # If 'True' scale loss inverse proportionally to the confidence\n            # of the correct prediction\n            SCALE_LOSS: False,\n            # ## Regularization parameters\n            # The scale of regularization\n            REGULARIZATION_CONSTANT: 0.002,\n            # The scale of how important is to minimize the maximum similarity\n            # between embeddings of different labels,\n            # used only if 'loss_type' is set to 'margin'.\n            NEGATIVE_MARGIN_SCALE: 0.8,\n            # Dropout rate for encoder\n            # dropout比率，用于正则化以防止过拟合。\n            DROP_RATE: 0.2,\n            # Dropout rate for attention\n            DROP_RATE_ATTENTION: 0,\n            # Fraction of trainable weights in internal layers.\n            # 模型中所有前馈层设置为非零值的内核权重的分数，默认值0.2。\n            CONNECTION_DENSITY: 0.2,\n            # If 'True' apply dropout to sparse input tensors\n            SPARSE_INPUT_DROPOUT: True,\n            # If 'True' apply dropout to dense input tensors\n            DENSE_INPUT_DROPOUT: True,\n            # ## Evaluation parameters\n            # How often calculate validation accuracy.\n            # Small values may hurt performance.\n            EVAL_NUM_EPOCHS: 20,\n            # How many examples to use for hold out validation set\n            # Large values may hurt performance, e.g. model accuracy.\n            # Set to 0 for no validation.\n            EVAL_NUM_EXAMPLES: 0,\n            # ## Model config\n            # If 'True' intent classification is trained and intent predicted.\n            INTENT_CLASSIFICATION: True,\n            # If 'True' named entity recognition is trained and entities predicted.\n            ENTITY_RECOGNITION: True,\n            # If 'True' random tokens of the input message will be masked and the model\n            # should predict those tokens.\n            MASKED_LM: False,\n            # 'BILOU_flag' determines whether to use BILOU tagging or not.\n            # If set to 'True' labelling is more rigorous, however more\n            # examples per entity are required.\n            # Rule of thumb: you should have more than 100 examples per entity.\n            BILOU_FLAG: True,\n            # If you want to use tensorboard to visualize training and validation\n            # metrics, set this option to a valid output directory.\n            TENSORBOARD_LOG_DIR: None,\n            # Define when training metrics for tensorboard should be logged.\n            # Either after every epoch or for every training step.\n            # Valid values: 'epoch' and 'batch'\n            TENSORBOARD_LOG_LEVEL: \"epoch\",\n            # Perform model checkpointing\n            CHECKPOINT_MODEL: False,\n            # Specify what features to use as sequence and sentence features\n            # By default all features in the pipeline are used.\n            FEATURIZERS: [],\n            # Split entities by comma, this makes sense e.g. for a list of ingredients\n            # in a recipie, but it doesn't make sense for the parts of an address\n            SPLIT_ENTITIES_BY_COMMA: True,\n            # If 'True' applies sigmoid on all similarity terms and adds\n            # it to the loss function to ensure that similarity values are\n            # approximately bounded. Used inside cross-entropy loss only.\n            # 当设置为 True 时，此参数对所有相似项应用sigmoid交叉熵损失。\n            # 这有助于将输入标签和负标签之间的相似性保持在较小的值。\n            # 这应该有助于更好地将模型推广到现实世界的测试集，默认值False。\n            CONSTRAIN_SIMILARITIES: True,\n            # Model confidence to be returned during inference. Currently, the only\n            # possible value is `softmax`.\n            MODEL_CONFIDENCE: SOFTMAX,\n            # Determines whether the confidences of the chosen top intents should be\n            # renormalized so that they sum up to 1. By default, we do not renormalize\n            # and return the confidences for the top intents as is.\n            # Note that renormalization only makes sense if confidences are generated\n            # via `softmax`.\n            RENORMALIZE_CONFIDENCES: False,\n            # Determines whether to construct the model graph or not.\n            # This is advantageous when the model is only trained or inferred for\n            # a few steps, as the compilation of the graph tends to take more time than\n            # running it. It is recommended to not adjust the optimization parameter.\n            RUN_EAGERLY: False,\n        }\n\n\n    # 初始化\n    def __init__(\n        self,\n        config: Dict[Text, Any],\n        model_storage: ModelStorage,\n        resource: Resource,\n        execution_context: ExecutionContext,\n        index_label_id_mapping: Optional[Dict[int, Text]] = None,\n        entity_tag_specs: Optional[List[EntityTagSpec]] = None,\n        model: Optional[RasaModel] = None,\n        sparse_feature_sizes: Optional[Dict[Text, Dict[Text, List[int]]]] = None,\n    ) -> None:\n        \"\"\"Declare instance variables with default values.\"\"\"\n        if EPOCHS not in config:\n            rasa.shared.utils.io.raise_warning(\n                f\"请在配置文件中设置'{EPOCHS}' 的值.\"\n                f\" 我们将在未来将 '{EPOCHS}' 的默认值设置为1. \"\n            )\n\n        self.component_config = config\n        self._model_storage = model_storage\n        self._resource = resource\n        self._execution_context = execution_context\n\n        self._check_config_parameters()\n\n        # transform numbers to labels\n        self.index_label_id_mapping = index_label_id_mapping or {}\n\n        self._entity_tag_specs = entity_tag_specs\n\n        self.model = model\n\n        self.tmp_checkpoint_dir = None\n        if self.component_config[CHECKPOINT_MODEL]:\n            self.tmp_checkpoint_dir = Path(rasa.utils.io.create_temporary_directory())\n\n        self._label_data: Optional[RasaModelData] = None\n        self._data_example: Optional[Dict[Text, Dict[Text, List[FeatureArray]]]] = None\n\n        self.split_entities_config = rasa.utils.train_utils.init_split_entities(\n            self.component_config[SPLIT_ENTITIES_BY_COMMA],\n            SPLIT_ENTITIES_BY_COMMA_DEFAULT_VALUE,\n        )\n\n        self.finetune_mode = self._execution_context.is_finetuning\n        self._sparse_feature_sizes = sparse_feature_sizes\n\n    # init helpers\n    def _check_masked_lm(self) -> None:\n        if (\n            self.component_config[MASKED_LM]\n            and self.component_config[NUM_TRANSFORMER_LAYERS] == 0\n        ):\n            raise ValueError(\n                f\"If number of transformer layers is 0, \"\n                f\"'{MASKED_LM}' option should be 'False'.\"\n            )\n\n    def _check_share_hidden_layers_sizes(self) -> None:\n        if self.component_config.get(SHARE_HIDDEN_LAYERS):\n            first_hidden_layer_sizes = next(\n                iter(self.component_config[HIDDEN_LAYERS_SIZES].values())\n            )\n            # 检查所有隐藏层的大小是否相同\n            identical_hidden_layer_sizes = all(\n                current_hidden_layer_sizes == first_hidden_layer_sizes\n                for current_hidden_layer_sizes in self.component_config[\n                    HIDDEN_LAYERS_SIZES\n                ].values()\n            )\n            if not identical_hidden_layer_sizes:\n                raise ValueError(\n                    f\"如果隐藏层权重是共享的, \"\n                    f\"{HIDDEN_LAYERS_SIZES} 必须一致.\"\n                )\n\n    def _check_config_parameters(self) -> None:\n        self.component_config = train_utils.check_deprecated_options(\n            self.component_config\n        )\n\n        self._check_masked_lm()\n        self._check_share_hidden_layers_sizes()\n\n        self.component_config = train_utils.update_confidence_type(\n            self.component_config\n        )\n\n        train_utils.validate_configuration_settings(self.component_config)\n\n        self.component_config = train_utils.update_similarity_type(\n            self.component_config\n        )\n        self.component_config = train_utils.update_evaluation_parameters(\n            self.component_config\n        )\n\n    # 创建一个新的组件\n    @classmethod\n    def create(\n        cls,\n        config: Dict[Text, Any],\n        model_storage: ModelStorage,\n        resource: Resource,\n        execution_context: ExecutionContext,\n    ) -> DIETClassifier:\n        \"\"\"创建一个新的未经训练的组件 (see parent class for full docstring).\"\"\"\n        return cls(config, model_storage, resource, execution_context)\n\n    @property\n    def label_key(self) -> Optional[Text]:\n        \"\"\"如果意图分类被激活，返回key.\"\"\"\n        return LABEL_KEY if self.component_config[INTENT_CLASSIFICATION] else None\n\n    @property\n    def label_sub_key(self) -> Optional[Text]:\n        \"\"\"如果意图分类被激活，返回sub key.\"\"\"\n        return LABEL_SUB_KEY if self.component_config[INTENT_CLASSIFICATION] else None\n\n    # 创建一个DIET模型\n    @staticmethod\n    def model_class() -> Type[RasaModel]:\n        return DIET\n\n    # training data helpers:\n    @staticmethod\n    def _label_id_index_mapping(\n        training_data: TrainingData, attribute: Text\n    ) -> Dict[Text, int]:\n        \"\"\"Create label_id dictionary.\"\"\"\n\n        distinct_label_ids = {\n            example.get(attribute) for example in training_data.intent_examples\n        } - {None}\n        return {\n            label_id: idx for idx, label_id in enumerate(sorted(distinct_label_ids))\n        }\n\n    @staticmethod\n    def _invert_mapping(mapping: Dict) -> Dict:\n        return {value: key for key, value in mapping.items()}\n\n    def _create_entity_tag_specs(\n        self, training_data: TrainingData\n    ) -> List[EntityTagSpec]:\n        \"\"\"Create entity tag specifications with their respective tag id mappings.\n        使用各自的标签id映射创建实体标签规范。\"\"\"\n\n        _tag_specs = []\n\n        for tag_name in POSSIBLE_TAGS:\n            if self.component_config[BILOU_FLAG]:\n                tag_id_index_mapping = bilou_utils.build_tag_id_dict(\n                    training_data, tag_name\n                )\n            else:\n                tag_id_index_mapping = self._tag_id_index_mapping_for(\n                    tag_name, training_data\n                )\n\n            if tag_id_index_mapping:\n                _tag_specs.append(\n                    EntityTagSpec(\n                        tag_name=tag_name,\n                        tags_to_ids=tag_id_index_mapping,\n                        ids_to_tags=self._invert_mapping(tag_id_index_mapping),\n                        num_tags=len(tag_id_index_mapping),\n                    )\n                )\n\n        return _tag_specs\n\n    @staticmethod\n    def _tag_id_index_mapping_for(\n        tag_name: Text, training_data: TrainingData\n    ) -> Optional[Dict[Text, int]]:\n        \"\"\"Create mapping from tag name to id.\"\"\"\n        if tag_name == ENTITY_ATTRIBUTE_ROLE:\n            distinct_tags = training_data.entity_roles\n        elif tag_name == ENTITY_ATTRIBUTE_GROUP:\n            distinct_tags = training_data.entity_groups\n        else:\n            distinct_tags = training_data.entities\n\n        distinct_tags = distinct_tags - {NO_ENTITY_TAG} - {None}\n\n        if not distinct_tags:\n            return None\n\n        tag_id_dict = {\n            tag_id: idx for idx, tag_id in enumerate(sorted(distinct_tags), 1)\n        }\n        # NO_ENTITY_TAG corresponds to non-entity which should correspond to 0 index\n        # needed for correct prediction for padding\n        tag_id_dict[NO_ENTITY_TAG] = 0\n\n        return tag_id_dict\n\n    @staticmethod\n    def _find_example_for_label(\n        label: Text, examples: List[Message], attribute: Text\n    ) -> Optional[Message]:\n        for ex in examples:\n            if ex.get(attribute) == label:\n                return ex\n        return None\n\n    def _check_labels_features_exist(\n        self, labels_example: List[Message], attribute: Text\n    ) -> bool:\n        \"\"\"Checks if all labels have features set.\"\"\"\n\n        return all(\n            label_example.features_present(\n                attribute, self.component_config[FEATURIZERS]\n            )\n            for label_example in labels_example\n        )\n\n    # 对获取的sparse与dense的sequence_features与sentence_features分别进行相应的处理\n    def _extract_features(\n        self, message: Message, attribute: Text\n    ) -> Dict[Text, Union[scipy.sparse.spmatrix, np.ndarray]]:\n\n        (\n            sparse_sequence_features,\n            sparse_sentence_features,\n        ) = message.get_sparse_features(attribute, self.component_config[FEATURIZERS])\n        dense_sequence_features, dense_sentence_features = message.get_dense_features(\n            attribute, self.component_config[FEATURIZERS]\n        )\n\n        if dense_sequence_features is not None and sparse_sequence_features is not None:\n            if (\n                dense_sequence_features.features.shape[0]\n                != sparse_sequence_features.features.shape[0]\n            ):\n                raise ValueError(\n                    f\"稀疏和稠密序列特征的序列维度 \"\n                    f\"don't coincide in '{message.get(TEXT)}'\"\n                    f\"for attribute '{attribute}'.\"\n                )\n        if dense_sentence_features is not None and sparse_sentence_features is not None:\n            if (\n                dense_sentence_features.features.shape[0]\n                != sparse_sentence_features.features.shape[0]\n            ):\n                raise ValueError(\n                    f\"稀疏和稠密句子特征的序列维度 \"\n                    f\"don't coincide in '{message.get(TEXT)}'\"\n                    f\"for attribute '{attribute}'.\"\n                )\n\n        # If we don't use the transformer and we don't want to do entity recognition,\n        # to speed up training take only the sentence features as feature vector.\n        # We would not make use of the sequence anyway in this setup. Carrying over\n        # those features to the actual training process takes quite some time.\n        # 如果我们不使用transformer，也不想进行实体识别，为了加快训练速度，只将句子特征作为特征向量。\n        # 在这种设置中，我们无论如何都不会使用该序列。\n        # 将这些特征带到实际的训练过程需要相当多的时间。\n        if (\n            self.component_config[NUM_TRANSFORMER_LAYERS] == 0\n            and not self.component_config[ENTITY_RECOGNITION]\n            and attribute not in [INTENT, INTENT_RESPONSE_KEY]\n        ):\n            sparse_sequence_features = None\n            dense_sequence_features = None\n\n        out = {}\n\n        if sparse_sentence_features is not None:\n            out[f\"{SPARSE}_{SENTENCE}\"] = sparse_sentence_features.features\n        if sparse_sequence_features is not None:\n            out[f\"{SPARSE}_{SEQUENCE}\"] = sparse_sequence_features.features\n        if dense_sentence_features is not None:\n            out[f\"{DENSE}_{SENTENCE}\"] = dense_sentence_features.features\n        if dense_sequence_features is not None:\n            out[f\"{DENSE}_{SEQUENCE}\"] = dense_sequence_features.features\n\n        return out\n\n    def _check_input_dimension_consistency(self, model_data: RasaModelData) -> None:\n        \"\"\"Checks if features have same dimensionality if hidden layers are shared.\n        如果隐藏层是共享的，则检查特征是否具有相同的维度\"\"\"\n        if self.component_config.get(SHARE_HIDDEN_LAYERS):\n            num_text_sentence_features = model_data.number_of_units(TEXT, SENTENCE)\n            num_label_sentence_features = model_data.number_of_units(LABEL, SENTENCE)\n            num_text_sequence_features = model_data.number_of_units(TEXT, SEQUENCE)\n            num_label_sequence_features = model_data.number_of_units(LABEL, SEQUENCE)\n\n            if (0 < num_text_sentence_features != num_label_sentence_features > 0) or (\n                0 < num_text_sequence_features != num_label_sequence_features > 0\n            ):\n                raise ValueError(\n                    \"If embeddings are shared text features and label features \"\n                    \"must coincide. Check the output dimensions of previous components.\"\n                    \"如果嵌入是共享的，则文本特征和标签特征必须一致。检查之前组件的输出维度\"\n                )\n\n\n    # 这个函数用于从输入的 label_examples中提取预先计算好的特征。\n    def _extract_labels_precomputed_features(\n        self, label_examples: List[Message], attribute: Text = INTENT\n    ) -> Tuple[List[FeatureArray], List[FeatureArray]]:\n        \"\"\"Collects precomputed encodings.\"\"\"\n        features = defaultdict(list)\n\n        # 提取特征：遍历 label_examples 中的每个 Message 对象 e，使用 _extract_features() 方法提取特征，然后将提取到的特征添加到 features 字典中。\n        for e in label_examples:\n            label_features = self._extract_features(e, attribute)\n            for feature_key, feature_value in label_features.items():\n                features[feature_key].append(feature_value)\n        sequence_features = []\n        sentence_features = []\n        # 区分序列特征和句子特征\n        for feature_name, feature_value in features.items():\n            if SEQUENCE in feature_name:\n                sequence_features.append(\n                    FeatureArray(np.array(feature_value), number_of_dimensions=3)\n                )\n            else:\n                sentence_features.append(\n                    FeatureArray(np.array(feature_value), number_of_dimensions=3)\n                )\n        return sequence_features, sentence_features\n\n\n    # 这个函数用于计算标签的 one-hot 编码表示。简要地说，它是将分类问题中的类别标签表示为一个向量，其中只有一个元素是 1，其他元素都是 0。\n    @staticmethod\n    def _compute_default_label_features(\n        labels_example: List[Message],\n    ) -> List[FeatureArray]:\n        \"\"\"Computes one-hot representation for the labels.\"\"\"\n        logger.debug(\"没有找到标签特征，将计算默认的标签特征。\")\n        # 使用 NumPy 创建一个单位矩阵 \n        eye_matrix = np.eye(len(labels_example), dtype=np.float32)\n        # add sequence dimension to one-hot labels\n        # 计算 one-hot 编码：接下来，函数遍历单位矩阵的每一行，将其扩展为一个新的轴，然后将其包装为一个 FeatureArray 对象。\n        # 这样做的目的是将每个 one-hot 编码表示的标签添加一个序列维度。\n        return [\n            FeatureArray(\n                np.array([np.expand_dims(a, 0) for a in eye_matrix]),\n                number_of_dimensions=3,\n            )\n        ]\n\n    # 这个函数用于从训练数据中创建一个包含标签数据的矩阵，其中每一行都包含一个用词袋表示的标签 ID。\n    # 这个矩阵将用于训练 DIET 分类器。\n    def _create_label_data(\n        self,\n        training_data: TrainingData,\n        label_id_dict: Dict[Text, int],\n        attribute: Text,\n    ) -> RasaModelData:\n        \"\"\"Create matrix with label_ids encoded in rows as bag of words.\n\n        Find a training example for each label and get the encoded features\n        from the corresponding Message object.\n        If the features are already computed, fetch them from the message object\n        else compute a one hot encoding for the label as the feature vector.\n        \"\"\"\n        # 收集每个标签的一个示例\n        labels_idx_examples = []\n        for label_name, idx in label_id_dict.items():\n            label_example = self._find_example_for_label(\n                label_name, training_data.intent_examples, attribute\n            )\n            labels_idx_examples.append((idx, label_example))\n\n        # 根据标签 ID 排序，然后，从排序后的列表中提取标签示例并将其存储到 labels_example 中。\n        labels_idx_examples = sorted(labels_idx_examples, key=lambda x: x[0])\n        labels_example = [example for (_, example) in labels_idx_examples]\n        # 收集特征：首先检查 labels_example 中是否已存在预先计算的特征。\n        # 如果存在，提取序列特征（sequence_features）和句子特征（sentence_features）。\n        # 如果不存在预先计算的特征，则计算默认的标签特征，这里只计算句子特征，序列特征设置为 None。\n        if self._check_labels_features_exist(labels_example, attribute):\n            (\n                sequence_features,\n                sentence_features,\n            ) = self._extract_labels_precomputed_features(labels_example, attribute)\n        else:\n            sequence_features = None\n            sentence_features = self._compute_default_label_features(labels_example)\n\n        label_data = RasaModelData()\n        label_data.add_features(LABEL, SEQUENCE, sequence_features)\n        label_data.add_features(LABEL, SENTENCE, sentence_features)\n        if label_data.does_feature_not_exist(\n            LABEL, SENTENCE\n        ) and label_data.does_feature_not_exist(LABEL, SEQUENCE):\n            raise ValueError(\n                \"No label features are present. Please check your configuration file.\"\n            )\n\n        label_ids = np.array([idx for (idx, _) in labels_idx_examples])\n        # explicitly add last dimension to label_ids\n        # to track correctly dynamic sequences\n        label_data.add_features(\n            LABEL_KEY,\n            LABEL_SUB_KEY,\n            [\n                FeatureArray(\n                    np.expand_dims(label_ids, -1),\n                    number_of_dimensions=2,\n                )\n            ],\n        )\n\n        label_data.add_lengths(LABEL, SEQUENCE_LENGTH, LABEL, SEQUENCE)\n\n        return label_data\n\n    def _use_default_label_features(self, label_ids: np.ndarray) -> List[FeatureArray]:\n        if self._label_data is None:\n            return []\n\n        feature_arrays = self._label_data.get(LABEL, SENTENCE)\n        all_label_features = feature_arrays[0]\n        return [\n            FeatureArray(\n                np.array([all_label_features[label_id] for label_id in label_ids]),\n                number_of_dimensions=all_label_features.number_of_dimensions,\n            )\n        ]\n\n    # 用于创建训练DIET模型所需要的数据\n    def _create_model_data(\n        self,\n        training_data: List[Message],\n        label_id_dict: Optional[Dict[Text, int]] = None,\n        label_attribute: Optional[Text] = None,\n        training: bool = True,\n    ) -> RasaModelData:\n        \"\"\"Prepare data for training and create a RasaModelData object.\"\"\"\n        from rasa.utils.tensorflow import model_data_utils\n\n        attributes_to_consider = [TEXT]\n        if training and self.component_config[INTENT_CLASSIFICATION]:\n            # we don't have any intent labels during prediction, just add them during\n            # training\n            attributes_to_consider.append(label_attribute)\n        if (\n            training\n            and self.component_config[ENTITY_RECOGNITION]\n            and self._entity_tag_specs\n        ):\n            # Add entities as labels only during training and only if there was\n            # training data added for entities with DIET configured to predict entities.\n            attributes_to_consider.append(ENTITIES)\n\n        if training and label_attribute is not None:\n            # only use those training examples that have the label_attribute set\n            # during training\n            training_data = [\n                example for example in training_data if label_attribute in example.data\n            ]\n\n        training_data = [\n            message\n            for message in training_data\n            if message.features_present(\n                attribute=TEXT, featurizers=self.component_config.get(FEATURIZERS)\n            )\n        ]\n\n        if not training_data:\n            # no training data are present to train\n            return RasaModelData()\n\n        (\n            features_for_examples,\n            sparse_feature_sizes,\n        ) = model_data_utils.featurize_training_examples(\n            training_data,\n            attributes_to_consider,\n            entity_tag_specs=self._entity_tag_specs,\n            featurizers=self.component_config[FEATURIZERS],\n            bilou_tagging=self.component_config[BILOU_FLAG],\n        )\n        attribute_data, _ = model_data_utils.convert_to_data_format(\n            features_for_examples, consider_dialogue_dimension=False\n        )\n\n        model_data = RasaModelData(\n            label_key=self.label_key, label_sub_key=self.label_sub_key\n        )\n        model_data.add_data(attribute_data)\n        model_data.add_lengths(TEXT, SEQUENCE_LENGTH, TEXT, SEQUENCE)\n        # Current implementation doesn't yet account for updating sparse\n        # feature sizes of label attributes. That's why we remove them.\n        sparse_feature_sizes = self._remove_label_sparse_feature_sizes(\n            sparse_feature_sizes=sparse_feature_sizes, label_attribute=label_attribute\n        )\n        model_data.add_sparse_feature_sizes(sparse_feature_sizes)\n\n        self._add_label_features(\n            model_data, training_data, label_attribute, label_id_dict, training\n        )\n\n        # make sure all keys are in the same order during training and prediction\n        # as we rely on the order of key and sub-key when constructing the actual\n        # tensors from the model data\n        model_data.sort()\n\n        return model_data\n\n    @staticmethod\n    def _remove_label_sparse_feature_sizes(\n        sparse_feature_sizes: Dict[Text, Dict[Text, List[int]]],\n        label_attribute: Optional[Text] = None,\n    ) -> Dict[Text, Dict[Text, List[int]]]:\n\n        if label_attribute in sparse_feature_sizes:\n            del sparse_feature_sizes[label_attribute]\n        return sparse_feature_sizes\n\n    def _add_label_features(\n        self,\n        model_data: RasaModelData,\n        training_data: List[Message],\n        label_attribute: Text,\n        label_id_dict: Dict[Text, int],\n        training: bool = True,\n    ) -> None:\n        label_ids = []\n        if training and self.component_config[INTENT_CLASSIFICATION]:\n            for example in training_data:\n                if example.get(label_attribute):\n                    label_ids.append(label_id_dict[example.get(label_attribute)])\n            # explicitly add last dimension to label_ids\n            # to track correctly dynamic sequences\n            model_data.add_features(\n                LABEL_KEY,\n                LABEL_SUB_KEY,\n                [\n                    FeatureArray(\n                        np.expand_dims(label_ids, -1),\n                        number_of_dimensions=2,\n                    )\n                ],\n            )\n\n        if (\n            label_attribute\n            and model_data.does_feature_not_exist(label_attribute, SENTENCE)\n            and model_data.does_feature_not_exist(label_attribute, SEQUENCE)\n        ):\n            # no label features are present, get default features from _label_data\n            model_data.add_features(\n                LABEL, SENTENCE, self._use_default_label_features(np.array(label_ids))\n            )\n\n        # as label_attribute can have different values, e.g. INTENT or RESPONSE,\n        # copy over the features to the LABEL key to make\n        # it easier to access the label features inside the model itself\n        model_data.update_key(label_attribute, SENTENCE, LABEL, SENTENCE)\n        model_data.update_key(label_attribute, SEQUENCE, LABEL, SEQUENCE)\n        model_data.update_key(label_attribute, MASK, LABEL, MASK)\n\n        model_data.add_lengths(LABEL, SEQUENCE_LENGTH, LABEL, SEQUENCE)\n\n    # train helpers\n    # 对训练数据做一些预处理，譬如提取labels的encoding\n    def preprocess_train_data(self, training_data: TrainingData) -> RasaModelData:\n        \"\"\"Prepares data for training.\n\n        Performs sanity checks on training data, extracts encodings for labels.\n        \"\"\"\n        if (\n            self.component_config[BILOU_FLAG]\n            and self.component_config[ENTITY_RECOGNITION]\n        ):\n            bilou_utils.apply_bilou_schema(training_data)\n\n        label_id_index_mapping = self._label_id_index_mapping(\n            training_data, attribute=INTENT\n        )\n\n        if not label_id_index_mapping:\n            # no labels are present to train\n            return RasaModelData()\n\n        self.index_label_id_mapping = self._invert_mapping(label_id_index_mapping)\n\n        self._label_data = self._create_label_data(\n            training_data, label_id_index_mapping, attribute=INTENT\n        )\n\n        self._entity_tag_specs = self._create_entity_tag_specs(training_data)\n\n        label_attribute = (\n            INTENT if self.component_config[INTENT_CLASSIFICATION] else None\n        )\n        model_data = self._create_model_data(\n            training_data.nlu_examples,\n            label_id_index_mapping,\n            label_attribute=label_attribute,\n        )\n\n        self._check_input_dimension_consistency(model_data)\n\n        return model_data\n\n    @staticmethod\n    def _check_enough_labels(model_data: RasaModelData) -> bool:\n        return len(np.unique(model_data.get(LABEL_KEY, LABEL_SUB_KEY))) >= 2\n\n    # 训练DIET模型\n    def train(self, training_data: TrainingData) -> Resource:\n        \"\"\"Train the embedding intent classifier on a data set.\"\"\"\n        model_data = self.preprocess_train_data(training_data)\n        if model_data.is_empty():\n            logger.debug(\n                f\"Cannot train '{self.__class__.__name__}'. No data was provided. \"\n                f\"Skipping training of the classifier.\"\n            )\n            return self._resource\n\n        if not self.model and self.finetune_mode:\n            raise rasa.shared.exceptions.InvalidParameterException(\n                f\"{self.__class__.__name__} was instantiated \"\n                f\"with `model=None` and `finetune_mode=True`. \"\n                f\"This is not a valid combination as the component \"\n                f\"needs an already instantiated and trained model \"\n                f\"to continue training in finetune mode.\"\n            )\n\n        if self.component_config.get(INTENT_CLASSIFICATION):\n            if not self._check_enough_labels(model_data):\n                logger.error(\n                    f\"Cannot train '{self.__class__.__name__}'. \"\n                    f\"Need at least 2 different intent classes. \"\n                    f\"Skipping training of classifier.\"\n                )\n                return self._resource\n        if self.component_config.get(ENTITY_RECOGNITION):\n            self.check_correct_entity_annotations(training_data)\n\n        # keep one example for persisting and loading\n        self._data_example = model_data.first_data_example()\n\n        if not self.finetune_mode:\n            # 判断是否需要load之前已训练的模型，还是需要新创建一个模型\n            # No pre-trained model to load from. Create a new instance of the model.\n            self.model = self._instantiate_model_class(model_data)\n            self.model.compile(\n                optimizer=tf.keras.optimizers.Adam(\n                    self.component_config[LEARNING_RATE]\n                ),\n                run_eagerly=self.component_config[RUN_EAGERLY],\n            )\n        else:\n            if self.model is None:\n                raise ModelNotFound(\"Model could not be found. \")\n\n            self.model.adjust_for_incremental_training(\n                data_example=self._data_example,\n                new_sparse_feature_sizes=model_data.get_sparse_feature_sizes(),\n                old_sparse_feature_sizes=self._sparse_feature_sizes,\n            )\n        self._sparse_feature_sizes = model_data.get_sparse_feature_sizes()\n\n        data_generator, validation_data_generator = train_utils.create_data_generators(\n            model_data,\n            self.component_config[BATCH_SIZES],\n            self.component_config[EPOCHS],\n            self.component_config[BATCH_STRATEGY],\n            self.component_config[EVAL_NUM_EXAMPLES],\n            self.component_config[RANDOM_SEED],\n        )\n        callbacks = train_utils.create_common_callbacks(\n            self.component_config[EPOCHS],\n            self.component_config[TENSORBOARD_LOG_DIR],\n            self.component_config[TENSORBOARD_LOG_LEVEL],\n            self.tmp_checkpoint_dir,\n        )\n\n        # 调用Transformer对输入数据进行处理，并对训练后的模型进行持久化操作\n        self.model.fit(\n            data_generator,\n            epochs=self.component_config[EPOCHS],\n            validation_data=validation_data_generator,\n            validation_freq=self.component_config[EVAL_NUM_EPOCHS],\n            callbacks=callbacks,\n            verbose=False,\n            shuffle=False,  # we use custom shuffle inside data generator\n        )\n\n        self.persist()\n\n        return self._resource\n\n    # process helpers\n    def _predict(\n        self, message: Message\n    ) -> Optional[Dict[Text, Union[tf.Tensor, Dict[Text, tf.Tensor]]]]:\n        if self.model is None:\n            logger.debug(\n                f\"There is no trained model for '{self.__class__.__name__}': The \"\n                f\"component is either not trained or didn't receive enough training \"\n                f\"data.\"\n            )\n            return None\n\n        # create session data from message and convert it into a batch of 1\n        model_data = self._create_model_data([message], training=False)\n        if model_data.is_empty():\n            return None\n        return self.model.run_inference(model_data)\n\n    def _predict_label(\n        self, predict_out: Optional[Dict[Text, tf.Tensor]]\n    ) -> Tuple[Dict[Text, Any], List[Dict[Text, Any]]]:\n        \"\"\"Predicts the intent of the provided message.\"\"\"\n        label: Dict[Text, Any] = {\"name\": None, \"confidence\": 0.0}\n        label_ranking: List[Dict[Text, Any]] = []\n\n        if predict_out is None:\n            return label, label_ranking\n\n        message_sim = predict_out[\"i_scores\"]\n        message_sim = message_sim.flatten()  # sim is a matrix\n\n        # if X contains all zeros do not predict some label\n        if message_sim.size == 0:\n            return label, label_ranking\n\n        # rank the confidences\n        ranking_length = self.component_config[RANKING_LENGTH]\n        renormalize = (\n            self.component_config[RENORMALIZE_CONFIDENCES]\n            and self.component_config[MODEL_CONFIDENCE] == SOFTMAX\n        )\n        ranked_label_indices, message_sim = train_utils.rank_and_mask(\n            message_sim, ranking_length=ranking_length, renormalize=renormalize\n        )\n\n        # construct the label and ranking\n        casted_message_sim: List[float] = message_sim.tolist()  # np.float to float\n        top_label_idx = ranked_label_indices[0]\n        label = {\n            \"name\": self.index_label_id_mapping[top_label_idx],\n            \"confidence\": casted_message_sim[top_label_idx],\n        }\n\n        ranking = [(idx, casted_message_sim[idx]) for idx in ranked_label_indices]\n        label_ranking = [\n            {\"name\": self.index_label_id_mapping[label_idx], \"confidence\": score}\n            for label_idx, score in ranking\n        ]\n\n        return label, label_ranking\n\n    def _predict_entities(\n        self, predict_out: Optional[Dict[Text, tf.Tensor]], message: Message\n    ) -> List[Dict]:\n        if predict_out is None:\n            return []\n\n        predicted_tags, confidence_values = train_utils.entity_label_to_tags(\n            predict_out, self._entity_tag_specs, self.component_config[BILOU_FLAG]\n        )\n\n        entities = self.convert_predictions_into_entities(\n            message.get(TEXT),\n            message.get(TOKENS_NAMES[TEXT], []),\n            predicted_tags,\n            self.split_entities_config,\n            confidence_values,\n        )\n\n        entities = self.add_extractor_name(entities)\n        entities = message.get(ENTITIES, []) + entities\n\n        return entities\n\n    def process(self, messages: List[Message]) -> List[Message]:\n        \"\"\"Augments the message with intents, entities, and diagnostic data.\"\"\"\n        for message in messages:\n            out = self._predict(message)\n\n            if self.component_config[INTENT_CLASSIFICATION]:\n                label, label_ranking = self._predict_label(out)\n\n                message.set(INTENT, label, add_to_output=True)\n                message.set(\"intent_ranking\", label_ranking, add_to_output=True)\n\n            if self.component_config[ENTITY_RECOGNITION]:\n                entities = self._predict_entities(out, message)\n\n                message.set(ENTITIES, entities, add_to_output=True)\n\n            if out and self._execution_context.should_add_diagnostic_data:\n                message.add_diagnostic_data(\n                    self._execution_context.node_name, out.get(DIAGNOSTIC_DATA)\n                )\n\n        return messages\n\n    def persist(self) -> None:\n        \"\"\"Persist this model into the passed directory.\"\"\"\n        if self.model is None:\n            return None\n\n        with self._model_storage.write_to(self._resource) as model_path:\n            file_name = self.__class__.__name__\n            tf_model_file = model_path / f\"{file_name}.tf_model\"\n\n            rasa.shared.utils.io.create_directory_for_file(tf_model_file)\n\n            if self.component_config[CHECKPOINT_MODEL] and self.tmp_checkpoint_dir:\n                self.model.load_weights(self.tmp_checkpoint_dir / \"checkpoint.tf_model\")\n                # Save an empty file to flag that this model has been\n                # produced using checkpointing\n                checkpoint_marker = model_path / f\"{file_name}.from_checkpoint.pkl\"\n                checkpoint_marker.touch()\n\n            self.model.save(str(tf_model_file))\n\n            io_utils.pickle_dump(\n                model_path / f\"{file_name}.data_example.pkl\", self._data_example\n            )\n            io_utils.pickle_dump(\n                model_path / f\"{file_name}.sparse_feature_sizes.pkl\",\n                self._sparse_feature_sizes,\n            )\n            io_utils.pickle_dump(\n                model_path / f\"{file_name}.label_data.pkl\",\n                dict(self._label_data.data) if self._label_data is not None else {},\n            )\n            io_utils.json_pickle(\n                model_path / f\"{file_name}.index_label_id_mapping.json\",\n                self.index_label_id_mapping,\n            )\n\n            entity_tag_specs = (\n                [tag_spec._asdict() for tag_spec in self._entity_tag_specs]\n                if self._entity_tag_specs\n                else []\n            )\n            rasa.shared.utils.io.dump_obj_as_json_to_file(\n                model_path / f\"{file_name}.entity_tag_specs.json\", entity_tag_specs\n            )\n\n    @classmethod\n    def load(\n        cls: Type[DIETClassifierT],\n        config: Dict[Text, Any],\n        model_storage: ModelStorage,\n        resource: Resource,\n        execution_context: ExecutionContext,\n        **kwargs: Any,\n    ) -> DIETClassifierT:\n        \"\"\"Loads a policy from the storage (see parent class for full docstring).\"\"\"\n        try:\n            with model_storage.read_from(resource) as model_path:\n                return cls._load(\n                    model_path, config, model_storage, resource, execution_context\n                )\n        except ValueError:\n            logger.debug(\n                f\"Failed to load {cls.__class__.__name__} from model storage. Resource \"\n                f\"'{resource.name}' doesn't exist.\"\n            )\n            return cls(config, model_storage, resource, execution_context)\n\n    @classmethod\n    def _load(\n        cls: Type[DIETClassifierT],\n        model_path: Path,\n        config: Dict[Text, Any],\n        model_storage: ModelStorage,\n        resource: Resource,\n        execution_context: ExecutionContext,\n    ) -> DIETClassifierT:\n        \"\"\"Loads the trained model from the provided directory.\"\"\"\n        (\n            index_label_id_mapping,\n            entity_tag_specs,\n            label_data,\n            data_example,\n            sparse_feature_sizes,\n        ) = cls._load_from_files(model_path)\n\n        config = train_utils.update_confidence_type(config)\n        config = train_utils.update_similarity_type(config)\n\n        model = cls._load_model(\n            entity_tag_specs,\n            label_data,\n            config,\n            data_example,\n            model_path,\n            finetune_mode=execution_context.is_finetuning,\n        )\n\n        return cls(\n            config=config,\n            model_storage=model_storage,\n            resource=resource,\n            execution_context=execution_context,\n            index_label_id_mapping=index_label_id_mapping,\n            entity_tag_specs=entity_tag_specs,\n            model=model,\n            sparse_feature_sizes=sparse_feature_sizes,\n        )\n\n    @classmethod\n    def _load_from_files(\n        cls, model_path: Path\n    ) -> Tuple[\n        Dict[int, Text],\n        List[EntityTagSpec],\n        RasaModelData,\n        Dict[Text, Dict[Text, List[FeatureArray]]],\n        Dict[Text, Dict[Text, List[int]]],\n    ]:\n        file_name = cls.__name__\n\n        data_example = io_utils.pickle_load(\n            model_path / f\"{file_name}.data_example.pkl\"\n        )\n        label_data = io_utils.pickle_load(model_path / f\"{file_name}.label_data.pkl\")\n        label_data = RasaModelData(data=label_data)\n        sparse_feature_sizes = io_utils.pickle_load(\n            model_path / f\"{file_name}.sparse_feature_sizes.pkl\"\n        )\n        index_label_id_mapping = io_utils.json_unpickle(\n            model_path / f\"{file_name}.index_label_id_mapping.json\"\n        )\n        entity_tag_specs = rasa.shared.utils.io.read_json_file(\n            model_path / f\"{file_name}.entity_tag_specs.json\"\n        )\n        entity_tag_specs = [\n            EntityTagSpec(\n                tag_name=tag_spec[\"tag_name\"],\n                ids_to_tags={\n                    int(key): value for key, value in tag_spec[\"ids_to_tags\"].items()\n                },\n                tags_to_ids={\n                    key: int(value) for key, value in tag_spec[\"tags_to_ids\"].items()\n                },\n                num_tags=tag_spec[\"num_tags\"],\n            )\n            for tag_spec in entity_tag_specs\n        ]\n\n        # jsonpickle converts dictionary keys to strings\n        index_label_id_mapping = {\n            int(key): value for key, value in index_label_id_mapping.items()\n        }\n\n        return (\n            index_label_id_mapping,\n            entity_tag_specs,\n            label_data,\n            data_example,\n            sparse_feature_sizes,\n        )\n\n    @classmethod\n    def _load_model(\n        cls,\n        entity_tag_specs: List[EntityTagSpec],\n        label_data: RasaModelData,\n        config: Dict[Text, Any],\n        data_example: Dict[Text, Dict[Text, List[FeatureArray]]],\n        model_path: Path,\n        finetune_mode: bool = False,\n    ) -> \"RasaModel\":\n        file_name = cls.__name__\n        tf_model_file = model_path / f\"{file_name}.tf_model\"\n\n        label_key = LABEL_KEY if config[INTENT_CLASSIFICATION] else None\n        label_sub_key = LABEL_SUB_KEY if config[INTENT_CLASSIFICATION] else None\n\n        model_data_example = RasaModelData(\n            label_key=label_key, label_sub_key=label_sub_key, data=data_example\n        )\n\n        model = cls._load_model_class(\n            tf_model_file,\n            model_data_example,\n            label_data,\n            entity_tag_specs,\n            config,\n            finetune_mode=finetune_mode,\n        )\n\n        return model\n\n    @classmethod\n    def _load_model_class(\n        cls,\n        tf_model_file: Text,\n        model_data_example: RasaModelData,\n        label_data: RasaModelData,\n        entity_tag_specs: List[EntityTagSpec],\n        config: Dict[Text, Any],\n        finetune_mode: bool,\n    ) -> \"RasaModel\":\n\n        predict_data_example = RasaModelData(\n            label_key=model_data_example.label_key,\n            data={\n                feature_name: features\n                for feature_name, features in model_data_example.items()\n                if TEXT in feature_name\n            },\n        )\n\n        return cls.model_class().load(\n            tf_model_file,\n            model_data_example,\n            predict_data_example,\n            data_signature=model_data_example.get_signature(),\n            label_data=label_data,\n            entity_tag_specs=entity_tag_specs,\n            config=copy.deepcopy(config),\n            finetune_mode=finetune_mode,\n        )\n\n    def _instantiate_model_class(self, model_data: RasaModelData) -> \"RasaModel\":\n        return self.model_class()(\n            data_signature=model_data.get_signature(),\n            label_data=self._label_data,\n            entity_tag_specs=self._entity_tag_specs,\n            config=self.component_config,\n        )\n\n\nclass DIET(TransformerRasaModel):\n    def __init__(\n        self,\n        data_signature: Dict[Text, Dict[Text, List[FeatureSignature]]],\n        label_data: RasaModelData,\n        entity_tag_specs: Optional[List[EntityTagSpec]],\n        config: Dict[Text, Any],\n    ) -> None:\n        # create entity tag spec before calling super otherwise building the model\n        # will fail\n        super().__init__(\"DIET\", config, data_signature, label_data)\n        self._entity_tag_specs = self._ordered_tag_specs(entity_tag_specs)\n\n        self.predict_data_signature = {\n            feature_name: features\n            for feature_name, features in data_signature.items()\n            if TEXT in feature_name\n        }\n\n        # tf training\n        self._create_metrics()\n        self._update_metrics_to_log()\n\n        # needed for efficient prediction\n        self.all_labels_embed: Optional[tf.Tensor] = None\n\n        self._prepare_layers()\n\n    @staticmethod\n    def _ordered_tag_specs(\n        entity_tag_specs: Optional[List[EntityTagSpec]],\n    ) -> List[EntityTagSpec]:\n        \"\"\"Ensure that order of entity tag specs matches CRF layer order.\"\"\"\n        if entity_tag_specs is None:\n            return []\n\n        crf_order = [\n            ENTITY_ATTRIBUTE_TYPE,\n            ENTITY_ATTRIBUTE_ROLE,\n            ENTITY_ATTRIBUTE_GROUP,\n        ]\n\n        ordered_tag_spec = []\n\n        for tag_name in crf_order:\n            for tag_spec in entity_tag_specs:\n                if tag_name == tag_spec.tag_name:\n                    ordered_tag_spec.append(tag_spec)\n\n        return ordered_tag_spec\n\n    def _check_data(self) -> None:\n        if TEXT not in self.data_signature:\n            raise InvalidConfigException(\n                f\"No text features specified. \"\n                f\"Cannot train '{self.__class__.__name__}' model.\"\n            )\n        if self.config[INTENT_CLASSIFICATION]:\n            if LABEL not in self.data_signature:\n                raise InvalidConfigException(\n                    f\"No label features specified. \"\n                    f\"Cannot train '{self.__class__.__name__}' model.\"\n                )\n\n            if self.config[SHARE_HIDDEN_LAYERS]:\n                different_sentence_signatures = False\n                different_sequence_signatures = False\n                if (\n                    SENTENCE in self.data_signature[TEXT]\n                    and SENTENCE in self.data_signature[LABEL]\n                ):\n                    different_sentence_signatures = (\n                        self.data_signature[TEXT][SENTENCE]\n                        != self.data_signature[LABEL][SENTENCE]\n                    )\n                if (\n                    SEQUENCE in self.data_signature[TEXT]\n                    and SEQUENCE in self.data_signature[LABEL]\n                ):\n                    different_sequence_signatures = (\n                        self.data_signature[TEXT][SEQUENCE]\n                        != self.data_signature[LABEL][SEQUENCE]\n                    )\n\n                if different_sentence_signatures or different_sequence_signatures:\n                    raise ValueError(\n                        \"If hidden layer weights are shared, data signatures \"\n                        \"for text_features and label_features must coincide.\"\n                    )\n\n        if self.config[ENTITY_RECOGNITION] and (\n            ENTITIES not in self.data_signature\n            or ENTITY_ATTRIBUTE_TYPE not in self.data_signature[ENTITIES]\n        ):\n            logger.debug(\n                f\"You specified '{self.__class__.__name__}' to train entities, but \"\n                f\"no entities are present in the training data. Skipping training of \"\n                f\"entities.\"\n            )\n            self.config[ENTITY_RECOGNITION] = False\n\n    def _create_metrics(self) -> None:\n        # self.metrics will have the same order as they are created\n        # so create loss metrics first to output losses first\n        self.mask_loss = tf.keras.metrics.Mean(name=\"m_loss\")\n        self.intent_loss = tf.keras.metrics.Mean(name=\"i_loss\")\n        self.entity_loss = tf.keras.metrics.Mean(name=\"e_loss\")\n        self.entity_group_loss = tf.keras.metrics.Mean(name=\"g_loss\")\n        self.entity_role_loss = tf.keras.metrics.Mean(name=\"r_loss\")\n        # create accuracy metrics second to output accuracies second\n        self.mask_acc = tf.keras.metrics.Mean(name=\"m_acc\")\n        self.intent_acc = tf.keras.metrics.Mean(name=\"i_acc\")\n        self.entity_f1 = tf.keras.metrics.Mean(name=\"e_f1\")\n        self.entity_group_f1 = tf.keras.metrics.Mean(name=\"g_f1\")\n        self.entity_role_f1 = tf.keras.metrics.Mean(name=\"r_f1\")\n\n    def _update_metrics_to_log(self) -> None:\n        debug_log_level = logging.getLogger(\"rasa\").level == logging.DEBUG\n\n        if self.config[MASKED_LM]:\n            self.metrics_to_log.append(\"m_acc\")\n            if debug_log_level:\n                self.metrics_to_log.append(\"m_loss\")\n        if self.config[INTENT_CLASSIFICATION]:\n            self.metrics_to_log.append(\"i_acc\")\n            if debug_log_level:\n                self.metrics_to_log.append(\"i_loss\")\n        if self.config[ENTITY_RECOGNITION]:\n            for tag_spec in self._entity_tag_specs:\n                if tag_spec.num_tags != 0:\n                    name = tag_spec.tag_name\n                    self.metrics_to_log.append(f\"{name[0]}_f1\")\n                    if debug_log_level:\n                        self.metrics_to_log.append(f\"{name[0]}_loss\")\n\n        self._log_metric_info()\n\n    def _log_metric_info(self) -> None:\n        metric_name = {\n            \"t\": \"total\",\n            \"i\": \"intent\",\n            \"e\": \"entity\",\n            \"m\": \"mask\",\n            \"r\": \"role\",\n            \"g\": \"group\",\n        }\n        logger.debug(\"Following metrics will be logged during training: \")\n        for metric in self.metrics_to_log:\n            parts = metric.split(\"_\")\n            name = f\"{metric_name[parts[0]]} {parts[1]}\"\n            logger.debug(f\"  {metric} ({name})\")\n\n    def _prepare_layers(self) -> None:\n        # For user text, prepare layers that combine different feature types, embed\n        # everything using a transformer and optionally also do masked language\n        # modeling.\n        self.text_name = TEXT\n        self._tf_layers[\n            f\"sequence_layer.{self.text_name}\"\n        ] = rasa_layers.RasaSequenceLayer(\n            self.text_name, self.data_signature[self.text_name], self.config\n        )\n        if self.config[MASKED_LM]:\n            self._prepare_mask_lm_loss(self.text_name)\n\n        # Intent labels are treated similarly to user text but without the transformer,\n        # without masked language modelling, and with no dropout applied to the\n        # individual features, only to the overall label embedding after all label\n        # features have been combined.\n        if self.config[INTENT_CLASSIFICATION]:\n            self.label_name = TEXT if self.config[SHARE_HIDDEN_LAYERS] else LABEL\n\n            # disable input dropout applied to sparse and dense label features\n            label_config = self.config.copy()\n            label_config.update(\n                {SPARSE_INPUT_DROPOUT: False, DENSE_INPUT_DROPOUT: False}\n            )\n\n            self._tf_layers[\n                f\"feature_combining_layer.{self.label_name}\"\n            ] = rasa_layers.RasaFeatureCombiningLayer(\n                self.label_name, self.label_signature[self.label_name], label_config\n            )\n\n            self._prepare_ffnn_layer(\n                self.label_name,\n                self.config[HIDDEN_LAYERS_SIZES][self.label_name],\n                self.config[DROP_RATE],\n            )\n\n            self._prepare_label_classification_layers(predictor_attribute=TEXT)\n\n        if self.config[ENTITY_RECOGNITION]:\n            self._prepare_entity_recognition_layers()\n\n    # 计算遮罩损失的嵌入层和损失层。\n    def _prepare_mask_lm_loss(self, name: Text) -> None:\n        # for embedding predicted tokens at masked positions\n        self._prepare_embed_layers(f\"{name}_lm_mask\")\n\n        # for embedding the true tokens that got masked\n        self._prepare_embed_layers(f\"{name}_golden_token\")\n\n        # mask loss is additional loss\n        # set scaling to False, so that it doesn't overpower other losses\n        self._prepare_dot_product_loss(f\"{name}_mask\", scale_loss=False)\n\n    def _create_bow(\n        self,\n        sequence_features: List[Union[tf.Tensor, tf.SparseTensor]],\n        sentence_features: List[Union[tf.Tensor, tf.SparseTensor]],\n        sequence_feature_lengths: tf.Tensor,\n        name: Text,\n    ) -> tf.Tensor:\n\n        x, _ = self._tf_layers[f\"feature_combining_layer.{name}\"](\n            (sequence_features, sentence_features, sequence_feature_lengths),\n            training=self._training,\n        )\n\n        # convert to bag-of-words by summing along the sequence dimension\n        x = tf.reduce_sum(x, axis=1)\n\n        return self._tf_layers[f\"ffnn.{name}\"](x, self._training)\n\n    # 获取所有标签的嵌入表示。\n    def _create_all_labels(self) -> Tuple[tf.Tensor, tf.Tensor]:\n        all_label_ids = self.tf_label_data[LABEL_KEY][LABEL_SUB_KEY][0]\n\n        sequence_feature_lengths = self._get_sequence_feature_lengths(\n            self.tf_label_data, LABEL\n        )\n\n        x = self._create_bow(\n            self.tf_label_data[LABEL][SEQUENCE],\n            self.tf_label_data[LABEL][SENTENCE],\n            sequence_feature_lengths,\n            self.label_name,\n        )\n        all_labels_embed = self._tf_layers[f\"embed.{LABEL}\"](x)\n\n        return all_label_ids, all_labels_embed\n\n    def _mask_loss(\n        self,\n        outputs: tf.Tensor,\n        inputs: tf.Tensor,\n        seq_ids: tf.Tensor,\n        mlm_mask_boolean: tf.Tensor,\n        name: Text,\n    ) -> tf.Tensor:\n        # make sure there is at least one element in the mask\n        mlm_mask_boolean = tf.cond(\n            tf.reduce_any(mlm_mask_boolean),\n            lambda: mlm_mask_boolean,\n            lambda: tf.scatter_nd([[0, 0, 0]], [True], tf.shape(mlm_mask_boolean)),\n        )\n\n        mlm_mask_boolean = tf.squeeze(mlm_mask_boolean, -1)\n\n        # Pick elements that were masked, throwing away the batch & sequence dimension\n        # and effectively switching from shape (batch_size, sequence_length, units) to\n        # (num_masked_elements, units).\n        outputs = tf.boolean_mask(outputs, mlm_mask_boolean)\n        inputs = tf.boolean_mask(inputs, mlm_mask_boolean)\n        ids = tf.boolean_mask(seq_ids, mlm_mask_boolean)\n\n        tokens_predicted_embed = self._tf_layers[f\"embed.{name}_lm_mask\"](outputs)\n        tokens_true_embed = self._tf_layers[f\"embed.{name}_golden_token\"](inputs)\n\n        # To limit the otherwise computationally expensive loss calculation, we\n        # constrain the label space in MLM (i.e. token space) to only those tokens that\n        # were masked in this batch. Hence the reduced list of token embeddings\n        # (tokens_true_embed) and the reduced list of labels (ids) are passed as\n        # all_labels_embed and all_labels, respectively. In the future, we could be less\n        # restrictive and construct a slightly bigger label space which could include\n        # tokens not masked in the current batch too.\n        return self._tf_layers[f\"loss.{name}_mask\"](\n            inputs_embed=tokens_predicted_embed,\n            labels_embed=tokens_true_embed,\n            labels=ids,\n            all_labels_embed=tokens_true_embed,\n            all_labels=ids,\n        )\n\n    def _calculate_label_loss(\n        self, text_features: tf.Tensor, label_features: tf.Tensor, label_ids: tf.Tensor\n    ) -> tf.Tensor:\n        all_label_ids, all_labels_embed = self._create_all_labels()\n\n        text_embed = self._tf_layers[f\"embed.{TEXT}\"](text_features)\n        label_embed = self._tf_layers[f\"embed.{LABEL}\"](label_features)\n\n        return self._tf_layers[f\"loss.{LABEL}\"](\n            text_embed, label_embed, label_ids, all_labels_embed, all_label_ids\n        )\n\n    # 计算给定批次的总损失，包括遮罩语言模型损失、意图分类损失和实体识别损失。\n    def batch_loss(\n        self, batch_in: Union[Tuple[tf.Tensor, ...], Tuple[np.ndarray, ...]]\n    ) -> tf.Tensor:\n        \"\"\"Calculates the loss for the given batch.\n\n        Args:\n            batch_in: The batch.\n\n        Returns:\n            The loss of the given batch.\n        \"\"\"\n        tf_batch_data = self.batch_to_model_data_format(batch_in, self.data_signature)\n\n        sequence_feature_lengths = self._get_sequence_feature_lengths(\n            tf_batch_data, TEXT\n        )\n\n        (\n            text_transformed,\n            text_in,\n            mask_combined_sequence_sentence,\n            text_seq_ids,\n            mlm_mask_boolean_text,\n            _,\n        ) = self._tf_layers[f\"sequence_layer.{self.text_name}\"](\n            (\n                tf_batch_data[TEXT][SEQUENCE],\n                tf_batch_data[TEXT][SENTENCE],\n                sequence_feature_lengths,\n            ),\n            training=self._training,\n        )\n\n        losses = []\n\n        # Lengths of sequences in case of sentence-level features are always 1, but they\n        # can effectively be 0 if sentence-level features aren't present.\n        sentence_feature_lengths = self._get_sentence_feature_lengths(\n            tf_batch_data, TEXT\n        )\n\n        combined_sequence_sentence_feature_lengths = (\n            sequence_feature_lengths + sentence_feature_lengths\n        )\n\n        if self.config[MASKED_LM] and self._training:\n            loss, acc = self._mask_loss(\n                text_transformed, text_in, text_seq_ids, mlm_mask_boolean_text, TEXT\n            )\n            self.mask_loss.update_state(loss)\n            self.mask_acc.update_state(acc)\n            losses.append(loss)\n\n        if self.config[INTENT_CLASSIFICATION]:\n            loss = self._batch_loss_intent(\n                combined_sequence_sentence_feature_lengths,\n                text_transformed,\n                tf_batch_data,\n            )\n            losses.append(loss)\n\n        if self.config[ENTITY_RECOGNITION]:\n            losses += self._batch_loss_entities(\n                mask_combined_sequence_sentence,\n                sequence_feature_lengths,\n                text_transformed,\n                tf_batch_data,\n            )\n\n        return tf.math.add_n(losses)\n\n    #  计算意图分类任务的损失\n    def _batch_loss_intent(\n        self,\n        combined_sequence_sentence_feature_lengths_text: tf.Tensor,\n        text_transformed: tf.Tensor,\n        tf_batch_data: Dict[Text, Dict[Text, List[tf.Tensor]]],\n    ) -> tf.Tensor:\n        # get sentence features vector for intent classification\n        sentence_vector = self._last_token(\n            text_transformed, combined_sequence_sentence_feature_lengths_text\n        )\n\n        sequence_feature_lengths_label = self._get_sequence_feature_lengths(\n            tf_batch_data, LABEL\n        )\n\n        label_ids = tf_batch_data[LABEL_KEY][LABEL_SUB_KEY][0]\n        label = self._create_bow(\n            tf_batch_data[LABEL][SEQUENCE],\n            tf_batch_data[LABEL][SENTENCE],\n            sequence_feature_lengths_label,\n            self.label_name,\n        )\n        loss, acc = self._calculate_label_loss(sentence_vector, label, label_ids)\n\n        self._update_label_metrics(loss, acc)\n\n        return loss\n\n    # 更新意图分类任务的评估指标。\n    def _update_label_metrics(self, loss: tf.Tensor, acc: tf.Tensor) -> None:\n\n        self.intent_loss.update_state(loss)\n        self.intent_acc.update_state(acc)\n\n    # 计算实体识别任务的损失。\n    def _batch_loss_entities(\n        self,\n        mask_combined_sequence_sentence: tf.Tensor,\n        sequence_feature_lengths: tf.Tensor,\n        text_transformed: tf.Tensor,\n        tf_batch_data: Dict[Text, Dict[Text, List[tf.Tensor]]],\n    ) -> List[tf.Tensor]:\n        losses = []\n\n        entity_tags = None\n\n        for tag_spec in self._entity_tag_specs:\n            if tag_spec.num_tags == 0:\n                continue\n\n            tag_ids = tf_batch_data[ENTITIES][tag_spec.tag_name][0]\n            # add a zero (no entity) for the sentence features to match the shape of\n            # inputs\n            tag_ids = tf.pad(tag_ids, [[0, 0], [0, 1], [0, 0]])\n\n            loss, f1, _logits = self._calculate_entity_loss(\n                text_transformed,\n                tag_ids,\n                mask_combined_sequence_sentence,\n                sequence_feature_lengths,\n                tag_spec.tag_name,\n                entity_tags,\n            )\n\n            if tag_spec.tag_name == ENTITY_ATTRIBUTE_TYPE:\n                # use the entity tags as additional input for the role\n                # and group CRF\n                entity_tags = tf.one_hot(\n                    tf.cast(tag_ids[:, :, 0], tf.int32), depth=tag_spec.num_tags\n                )\n\n            self._update_entity_metrics(loss, f1, tag_spec.tag_name)\n\n            losses.append(loss)\n\n        return losses\n\n    # 更新实体识别任务的评估指标。\n    def _update_entity_metrics(\n        self, loss: tf.Tensor, f1: tf.Tensor, tag_name: Text\n    ) -> None:\n        if tag_name == ENTITY_ATTRIBUTE_TYPE:\n            self.entity_loss.update_state(loss)\n            self.entity_f1.update_state(f1)\n        elif tag_name == ENTITY_ATTRIBUTE_GROUP:\n            self.entity_group_loss.update_state(loss)\n            self.entity_group_f1.update_state(f1)\n        elif tag_name == ENTITY_ATTRIBUTE_ROLE:\n            self.entity_role_loss.update_state(loss)\n            self.entity_role_f1.update_state(f1)\n\n    # 为预测准备模型。\n    def prepare_for_predict(self) -> None:\n        \"\"\"Prepares the model for prediction.\"\"\"\n        if self.config[INTENT_CLASSIFICATION]:\n            _, self.all_labels_embed = self._create_all_labels()\n\n    # 对给定批次进行预测，包括意图分类和实体识别。\n    def batch_predict(\n        self, batch_in: Union[Tuple[tf.Tensor, ...], Tuple[np.ndarray, ...]]\n    ) -> Dict[Text, tf.Tensor]:\n        \"\"\"Predicts the output of the given batch.\n\n        Args:\n            batch_in: The batch.\n\n        Returns:\n            The output to predict.\n        \"\"\"\n        tf_batch_data = self.batch_to_model_data_format(\n            batch_in, self.predict_data_signature\n        )\n\n        sequence_feature_lengths = self._get_sequence_feature_lengths(\n            tf_batch_data, TEXT\n        )\n        sentence_feature_lengths = self._get_sentence_feature_lengths(\n            tf_batch_data, TEXT\n        )\n\n        text_transformed, _, _, _, _, attention_weights = self._tf_layers[\n            f\"sequence_layer.{self.text_name}\"\n        ](\n            (\n                tf_batch_data[TEXT][SEQUENCE],\n                tf_batch_data[TEXT][SENTENCE],\n                sequence_feature_lengths,\n            ),\n            training=self._training,\n        )\n        predictions = {\n            DIAGNOSTIC_DATA: {\n                \"attention_weights\": attention_weights,\n                \"text_transformed\": text_transformed,\n            }\n        }\n\n        if self.config[INTENT_CLASSIFICATION]:\n            predictions.update(\n                self._batch_predict_intents(\n                    sequence_feature_lengths + sentence_feature_lengths,\n                    text_transformed,\n                )\n            )\n\n        if self.config[ENTITY_RECOGNITION]:\n            predictions.update(\n                self._batch_predict_entities(sequence_feature_lengths, text_transformed)\n            )\n\n        return predictions\n\n    # 预测实体识别任务的输出。\n    def _batch_predict_entities(\n        self, sequence_feature_lengths: tf.Tensor, text_transformed: tf.Tensor\n    ) -> Dict[Text, tf.Tensor]:\n        predictions: Dict[Text, tf.Tensor] = {}\n\n        entity_tags = None\n\n        for tag_spec in self._entity_tag_specs:\n            # skip crf layer if it was not trained\n            if tag_spec.num_tags == 0:\n                continue\n\n            name = tag_spec.tag_name\n            _input = text_transformed\n\n            if entity_tags is not None:\n                _tags = self._tf_layers[f\"embed.{name}.tags\"](entity_tags)\n                _input = tf.concat([_input, _tags], axis=-1)\n\n            _logits = self._tf_layers[f\"embed.{name}.logits\"](_input)\n            pred_ids, confidences = self._tf_layers[f\"crf.{name}\"](\n                _logits, sequence_feature_lengths\n            )\n\n            predictions[f\"e_{name}_ids\"] = pred_ids\n            predictions[f\"e_{name}_scores\"] = confidences\n\n            if name == ENTITY_ATTRIBUTE_TYPE:\n                # use the entity tags as additional input for the role\n                # and group CRF\n                entity_tags = tf.one_hot(\n                    tf.cast(pred_ids, tf.int32), depth=tag_spec.num_tags\n                )\n\n        return predictions\n\n    # 预测意图分类任务的输出。\n    def _batch_predict_intents(\n        self,\n        combined_sequence_sentence_feature_lengths: tf.Tensor,\n        text_transformed: tf.Tensor,\n    ) -> Dict[Text, tf.Tensor]:\n\n        if self.all_labels_embed is None:\n            raise ValueError(\n                \"The model was not prepared for prediction. \"\n                \"Call `prepare_for_predict` first.\"\n            )\n\n        # get sentence feature vector for intent classification\n        sentence_vector = self._last_token(\n            text_transformed, combined_sequence_sentence_feature_lengths\n        )\n        sentence_vector_embed = self._tf_layers[f\"embed.{TEXT}\"](sentence_vector)\n\n        _, scores = self._tf_layers[\n            f\"loss.{LABEL}\"\n        ].get_similarities_and_confidences_from_embeddings(\n            sentence_vector_embed[:, tf.newaxis, :],\n            self.all_labels_embed[tf.newaxis, :, :],\n        )\n\n        return {\"i_scores\": scores}\n"}
