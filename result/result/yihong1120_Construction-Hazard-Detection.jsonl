{"repo_info": {"repo_name": "Construction-Hazard-Detection", "repo_owner": "yihong1120", "repo_url": "https://github.com/yihong1120/Construction-Hazard-Detection"}}
{"type": "test_file", "path": "tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/examples/YOLO_server_api/backend/models_test.py", "content": "from __future__ import annotations\n\nimport asyncio\nimport unittest\nfrom pathlib import Path\nfrom unittest.mock import create_autospec\nfrom unittest.mock import MagicMock\nfrom unittest.mock import Mock\nfrom unittest.mock import patch\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy.orm import sessionmaker\n\nfrom examples.YOLO_server_api.backend.models import Base\nfrom examples.YOLO_server_api.backend.models import DetectionModelManager\nfrom examples.YOLO_server_api.backend.models import get_db\nfrom examples.YOLO_server_api.backend.models import ModelFileChangeHandler\nfrom examples.YOLO_server_api.backend.models import User\n\n# Define the in-memory database URI for testing\nDATABASE_URL = 'sqlite:///:memory:'\n\n# Configure the testing database and session\nengine = create_engine(DATABASE_URL)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\nBase.metadata.create_all(bind=engine)\n\n\nclass TestUserModel(unittest.TestCase):\n    \"\"\"\n    Test cases for the User model.\n    \"\"\"\n\n    def setUp(self) -> None:\n        \"\"\"\n        Set up a test database session.\n        \"\"\"\n        self.session: Session = SessionLocal()\n\n    def tearDown(self) -> None:\n        \"\"\"\n        Clean up the database after each test.\n        \"\"\"\n        self.session.close()\n\n    def test_set_password(self) -> None:\n        \"\"\"\n        Test password hashing in the User model.\n        \"\"\"\n        user = User(username='testuser')\n        user.set_password('secure_password')\n        # Ensure password hash does not store the actual password\n        self.assertNotEqual(user.password_hash, 'secure_password')\n        # Check password validation\n        self.assertTrue(asyncio.run(user.check_password('secure_password')))\n\n    def test_check_password(self) -> None:\n        \"\"\"\n        Test password verification in the User model.\n        \"\"\"\n        user = User(username='testuser')\n        user.set_password('secure_password')\n        # Confirm correct and incorrect passwords\n        self.assertTrue(asyncio.run(user.check_password('secure_password')))\n        self.assertFalse(asyncio.run(user.check_password('wrong_password')))\n\n    def test_to_dict(self) -> None:\n        \"\"\"\n        Test the to_dict method of the User model.\n        \"\"\"\n        user = User(username='testuser', role='admin', is_active=True)\n        user.set_password('secure_password')\n        self.session.add(user)\n        self.session.commit()\n\n        # Convert user instance to dictionary\n        user_dict = user.to_dict()\n\n        # Validate dictionary contents\n        self.assertEqual(user_dict['username'], 'testuser')\n        self.assertEqual(user_dict['role'], 'admin')\n        self.assertTrue(user_dict['is_active'])\n        self.assertIn('created_at', user_dict)\n        self.assertIn('updated_at', user_dict)\n\n\nclass TestDetectionModelManager(unittest.TestCase):\n    \"\"\"\n    Test cases for the DetectionModelManager.\n    \"\"\"\n\n    def setUp(self) -> None:\n        \"\"\"\n        Set up the model manager for testing.\n        \"\"\"\n        # Patch AutoDetectionModel to avoid actual model loading\n        self.patcher = patch(\n            'examples.YOLO_server_api.backend.models.AutoDetectionModel',\n        )\n        self.mock_model = self.patcher.start()\n        self.model_manager = DetectionModelManager()\n\n    def tearDown(self) -> None:\n        \"\"\"\n        Stop patching the model manager.\n        \"\"\"\n        self.patcher.stop()\n\n    def test_load_single_model(self) -> None:\n        \"\"\"\n        Test loading a single model into the manager.\n        \"\"\"\n        # Confirm model is loaded with correct parameters\n        self.assertEqual(self.mock_model.from_pretrained.call_count, 5)\n        self.mock_model.from_pretrained.assert_any_call(\n            'yolo11', model_path=str(Path('models/pt/best_yolo11n.pt')),\n            device='cuda:0',\n        )\n\n    def test_get_model(self) -> None:\n        \"\"\"\n        Test retrieving a model by its key from the manager.\n        \"\"\"\n        # Mock a loaded model for testing retrieval\n        self.model_manager.models['yolo11n'] = self.mock_model\n        model = self.model_manager.get_model('yolo11n')\n\n        # Validate that the retrieved model matches the mock model\n        self.assertEqual(model, self.mock_model)\n\n        # Test retrieval of non-existent model\n        model_none = self.model_manager.get_model('nonexistent')\n        self.assertIsNone(model_none)\n\n    @patch.object(DetectionModelManager, 'observer', create=True)\n    def test_cleanup_on_delete(self, mock_observer: Mock) -> None:\n        \"\"\"\n        Test that the observer is stopped and joined upon deletion.\n        \"\"\"\n        manager = DetectionModelManager()\n        manager.observer = mock_observer\n\n        # Explicitly call __del__ to test cleanup\n        manager.__del__()\n        mock_observer.stop.assert_called_once()\n        mock_observer.join.assert_called_once()\n\n\nclass TestModelFileChangeHandler(unittest.TestCase):\n    \"\"\"\n    Test cases for the ModelFileChangeHandler.\n    \"\"\"\n\n    def setUp(self) -> None:\n        \"\"\"\n        Set up the model manager and file change handler.\n        \"\"\"\n        # Mock DetectionModelManager with necessary attributes and methods\n        self.model_manager = create_autospec(DetectionModelManager)\n        self.model_manager.model_names = ['yolo11n']\n        self.model_manager.models = {}\n        self.model_manager.load_single_model = Mock(return_value='dummy_model')\n\n        # Initialise the handler with the mocked model manager\n        self.handler = ModelFileChangeHandler(self.model_manager)\n\n    def test_on_modified_with_directory(self) -> None:\n        \"\"\"\n        Test handling of a directory modification event (should ignore).\n        \"\"\"\n        event = MagicMock()\n        event.is_directory = True\n        # Ensure directory modifications do not trigger model loading\n        self.handler.on_modified(event)\n        self.model_manager.load_single_model.assert_not_called()\n\n    def test_on_modified_with_model_file(self) -> None:\n        \"\"\"\n        Test handling of a model file modification event.\n        \"\"\"\n        event = MagicMock()\n        event.is_directory = False\n        event.src_path = 'models/pt/best_yolo11n.pt'\n\n        # Trigger the file modification event\n        self.handler.on_modified(event)\n\n        # Verify the model was reloaded\n        self.model_manager.load_single_model.assert_called_once_with('yolo11n')\n        self.assertEqual(self.model_manager.models['yolo11n'], 'dummy_model')\n\n\nclass TestDatabase(unittest.TestCase):\n    \"\"\"\n    Test cases for database connection and session generator.\n    \"\"\"\n\n    @staticmethod\n    async def async_get_db() -> AsyncSession:\n        \"\"\"\n        Yield an asynchronous database session for testing.\n        \"\"\"\n        async for session in get_db():\n            return session\n        raise RuntimeError('Failed to create AsyncSession.')\n\n    def test_get_db(self) -> None:\n        \"\"\"\n        Test the database session generator.\n        \"\"\"\n        session = asyncio.run(self.async_get_db())\n\n        # Confirm the session is an AsyncSession instance\n        self.assertIsInstance(session, AsyncSession)\n\n        # Close session to avoid resource leak\n        asyncio.run(session.close())\n\n\n# Run the tests with pytest if this script is called directly\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/examples/YOLO_data_augmentation/data_augmentation_test.py", "content": "from __future__ import annotations\n\nimport sys\nimport unittest\nfrom pathlib import Path\nfrom unittest.mock import MagicMock\nfrom unittest.mock import mock_open\nfrom unittest.mock import patch\n\nimport numpy as np\n\nfrom examples.YOLO_data_augmentation.data_augmentation import (\n    DataAugmentation,\n)\nfrom examples.YOLO_data_augmentation.data_augmentation import main\n\n\nclass TestDataAugmentation(unittest.TestCase):\n    def setUp(self) -> None:\n        \"\"\"\n        Set up the test environment before each test.\n        \"\"\"\n        self.train_path: str = 'tests/cv_dataset'\n        self.num_augmentations: int = 5\n        self.augmenter: DataAugmentation = DataAugmentation(\n            self.train_path, self.num_augmentations,\n        )\n\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.imageio.imread',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.imageio.imwrite',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.Path.glob',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'iaa.Sequential.__call__',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.Path.open',\n        new_callable=mock_open,\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.Path.write_text',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.Path.write_bytes',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.Path.rename',\n    )\n    def test_augment_image(\n        self,\n        mock_rename: MagicMock,\n        mock_write_bytes: MagicMock,\n        mock_write_text: MagicMock,\n        mock_open_file: MagicMock,\n        mock_seq_call: MagicMock,\n        mock_glob: MagicMock,\n        mock_imwrite: MagicMock,\n        mock_imread: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test the augment_image method.\n        \"\"\"\n        # Mock the image and label data with alpha channel\n        mock_image = np.random.randint(0, 256, (100, 100, 4), dtype=np.uint8)\n        mock_imread.return_value = mock_image\n        mock_augmented_image = np.random.randint(\n            0, 256, (100, 100, 3), dtype=np.uint8,\n        )\n        mock_seq_call.return_value = (mock_augmented_image, MagicMock())\n\n        # Mock the label file reading\n        mock_glob.return_value = [\n            Path('tests/cv_dataset/images/mock_image.jpg'),\n        ]\n\n        # Test augment_image\n        self.augmenter.augment_image(\n            Path('tests/cv_dataset/images/mock_image.jpg'),\n        )\n\n        mock_imread.assert_called_once_with(\n            Path('tests/cv_dataset/images/mock_image.jpg'),\n        )\n        mock_imwrite.assert_called()\n        mock_seq_call.assert_called()\n        mock_open_file.assert_called()\n        mock_write_text.assert_not_called()\n        mock_write_bytes.assert_not_called()\n        mock_rename.assert_not_called()\n\n        # Check if the alpha channel was removed\n        self.assertEqual(mock_augmented_image.shape[2], 3)\n\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'Path.glob',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'gc.collect',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'DataAugmentation.augment_image',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'Path.write_text',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'Path.write_bytes',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'Path.rename',\n    )\n    def test_augment_data(\n        self,\n        mock_rename: MagicMock,\n        mock_write_bytes: MagicMock,\n        mock_write_text: MagicMock,\n        mock_augment_image: MagicMock,\n        mock_gc_collect: MagicMock,\n        mock_glob: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test the augment_data method.\n        \"\"\"\n        mock_glob.return_value = [\n            Path(f'tests/dataset/images/mock_image_{i:02d}.jpg')\n            for i in range(10)\n        ]\n\n        self.augmenter.augment_data(batch_size=2)\n\n        self.assertEqual(mock_augment_image.call_count, 10)\n        self.assertEqual(mock_gc_collect.call_count, 5)\n        mock_write_text.assert_not_called()\n        mock_write_bytes.assert_not_called()\n        mock_rename.assert_not_called()\n\n    @patch(\n        'builtins.open',\n        new_callable=mock_open,\n        read_data='5 0.5 0.5 0.75 0.75\\n',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'Path.exists',\n        return_value=True,\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'Path.open',\n        new_callable=mock_open,\n    )\n    def test_read_label_file(\n        self,\n        mock_open_path: MagicMock,\n        mock_exists: MagicMock,\n        mock_file: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test the read_label_file method.\n        \"\"\"\n        mock_label_path = Path('tests/cv_dataset/labels/mock_label.txt')\n\n        # Test reading the label file\n        image_shape = (100, 100, 3)\n\n        bbs = self.augmenter.read_label_file(mock_label_path, image_shape)\n        self.assertEqual(len(bbs), 1)\n        self.assertEqual(bbs[0].label, 5)\n        self.assertAlmostEqual(bbs[0].x1, 12.5)\n        self.assertAlmostEqual(bbs[0].y1, 12.5)\n        self.assertAlmostEqual(bbs[0].x2, 87.5)\n        self.assertAlmostEqual(bbs[0].y2, 87.5)\n\n    @patch('builtins.open', new_callable=mock_open)\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'Path.exists',\n        return_value=True,\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'Path.mkdir',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'Path.parent',\n        new_callable=MagicMock,\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'Path.write_text',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'Path.write_bytes',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'Path.rename',\n    )\n    def test_write_label_file(\n        self,\n        mock_rename: MagicMock,\n        mock_write_bytes: MagicMock,\n        mock_write_text: MagicMock,\n        mock_parent: MagicMock,\n        mock_mkdir: MagicMock,\n        mock_exists: MagicMock,\n        mock_file: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test the write_label_file method.\n        \"\"\"\n        # Mock bounding box data\n        bbs = MagicMock()\n        bbs.bounding_boxes = [MagicMock(x1=10, y1=10, x2=50, y2=50, label=0)]\n\n        # Mock the open method\n        m = mock_open()\n\n        # Revise the mock_open_path to return the mock_open object\n        mock_label_path = MagicMock(spec=Path)\n        mock_label_path.open = m\n\n        # Utilise the write_label_file method\n        image_shape = (100, 100, 3)\n        self.augmenter.write_label_file(\n            bbs, mock_label_path, image_shape[1], image_shape[0],\n        )\n\n        # Assert that the write method was called with the correct content\n        m().write.assert_called_once_with('0 0.3 0.3 0.4 0.4\\n')\n        mock_write_text.assert_not_called()\n        mock_write_bytes.assert_not_called()\n        mock_rename.assert_not_called()\n\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'Path.glob',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'random.shuffle',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'Path.rename',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'Path.write_text',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'Path.write_bytes',\n    )\n    def test_shuffle_data(\n        self,\n        mock_write_bytes: MagicMock,\n        mock_write_text: MagicMock,\n        mock_rename: MagicMock,\n        mock_shuffle: MagicMock,\n        mock_glob: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test the shuffle_data method.\n        \"\"\"\n        mock_glob.side_effect = [\n            [\n                Path(\n                    f'tests/dataset/images/mock_image_{i:02d}.jpg',\n                ) for i in range(5)\n            ],\n            [\n                Path(\n                    f'tests/dataset/labels/mock_label_{i:02d}.txt',\n                ) for i in range(5)\n            ],\n        ]\n\n        with patch(\n            'examples.YOLO_data_augmentation.data_augmentation.'\n            'uuid.uuid4',\n            side_effect=[f'uuid_{i:02d}' for i in range(5)],\n        ):\n            self.augmenter.shuffle_data()\n\n        mock_shuffle.assert_called_once()\n        self.assertEqual(mock_rename.call_count, 10)\n        mock_write_text.assert_not_called()\n        mock_write_bytes.assert_not_called()\n\n    @patch('time.sleep', return_value=None)  # Mock time.sleep to skip delay\n    @patch.object(\n        sys,\n        'argv',\n        [\n            'main', '--train_path=tests/dataset',\n            '--num_augmentations=2', '--batch_size=2',\n        ],\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'DataAugmentation.shuffle_data',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'DataAugmentation.augment_data',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'Path.write_text',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'Path.write_bytes',\n    )\n    def test_main(\n        self,\n        mock_write_bytes: MagicMock,\n        mock_write_text: MagicMock,\n        mock_augment_data: MagicMock,\n        mock_shuffle_data: MagicMock,\n        mock_sleep: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test the main function with command line arguments.\n        \"\"\"\n        main()\n        mock_augment_data.assert_called_once_with(batch_size=2)\n        mock_shuffle_data.assert_called_once()\n        mock_sleep.assert_called_once()\n        mock_write_text.assert_not_called()\n        mock_write_bytes.assert_not_called()\n\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'imageio.imread',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'imageio.imwrite',\n    )\n    @patch('examples.YOLO_data_augmentation.data_augmentation.Path.glob')\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'iaa.Sequential.__call__',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.Path.open',\n        new_callable=unittest.mock.mock_open,\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.Path.'\n        'write_text',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.Path.'\n        'write_bytes',\n    )\n    @patch('examples.YOLO_data_augmentation.data_augmentation.Path.rename')\n    def test_augment_image_resize_small(\n        self,\n        mock_rename: MagicMock,\n        mock_write_bytes: MagicMock,\n        mock_write_text: MagicMock,\n        mock_open_file: MagicMock,\n        mock_seq_call: MagicMock,\n        mock_glob: MagicMock,\n        mock_imwrite: MagicMock,\n        mock_imread: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test the augment_image method for small images.\n        \"\"\"\n        # Mock a small image\n        mock_image = np.random.randint(0, 256, (20, 20, 3), dtype=np.uint8)\n        mock_imread.return_value = mock_image\n        mock_augmented_image = np.random.randint(\n            0, 256, (100, 100, 3), dtype=np.uint8,\n        )\n        mock_seq_call.return_value = (mock_augmented_image, MagicMock())\n\n        # Mock the label file reading\n        mock_glob.return_value = [\n            Path('tests/cv_dataset/images/mock_image.jpg'),\n        ]\n\n        with patch('builtins.print') as mock_print:\n            self.augmenter.augment_image(\n                Path('tests/cv_dataset/images/mock_image.jpg'),\n            )\n            mock_print.assert_any_call(\n                f\"Resize tests/cv_dataset/images/mock_image.jpg \"\n                f\"due to small size: {mock_image.shape}\",\n            )\n\n        mock_imread.assert_called_once_with(\n            Path('tests/cv_dataset/images/mock_image.jpg'),\n        )\n        mock_imwrite.assert_called()\n        mock_seq_call.assert_called()\n        mock_open_file.assert_called()\n        mock_write_text.assert_not_called()\n        mock_write_bytes.assert_not_called()\n        mock_rename.assert_not_called()\n\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'imageio.imread',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'imageio.imwrite',\n    )\n    @patch('examples.YOLO_data_augmentation.data_augmentation.Path.glob')\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'iaa.Sequential.__call__',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.Path.open',\n        new_callable=unittest.mock.mock_open,\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'Path.write_text',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'Path.write_bytes',\n    )\n    @patch('examples.YOLO_data_augmentation.data_augmentation.Path.rename')\n    def test_augment_image_resize_large(\n        self,\n        mock_rename: MagicMock,\n        mock_write_bytes: MagicMock,\n        mock_write_text: MagicMock,\n        mock_open_file: MagicMock,\n        mock_seq_call: MagicMock,\n        mock_glob: MagicMock,\n        mock_imwrite: MagicMock,\n        mock_imread: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test the augment_image method for large images.\n        \"\"\"\n        # Mock a large image\n        mock_image = np.random.randint(0, 256, (2000, 2000, 3), dtype=np.uint8)\n        mock_imread.return_value = mock_image\n        mock_augmented_image = np.random.randint(\n            0, 256, (1000, 1000, 3), dtype=np.uint8,\n        )\n        mock_seq_call.return_value = (mock_augmented_image, MagicMock())\n\n        # Mock the label file reading\n        mock_glob.return_value = [\n            Path('tests/cv_dataset/images/mock_image.jpg'),\n        ]\n\n        with patch('builtins.print') as mock_print:\n            self.augmenter.augment_image(\n                Path('tests/cv_dataset/images/mock_image.jpg'),\n            )\n            mock_print.assert_any_call(\n                f'Resize tests/cv_dataset/images/mock_image.jpg '\n                f'due to large size: {mock_image.shape}',\n            )\n\n        mock_imread.assert_called_once_with(\n            Path('tests/cv_dataset/images/mock_image.jpg'),\n        )\n        mock_imwrite.assert_called()\n        mock_seq_call.assert_called()\n        mock_open_file.assert_called()\n        mock_write_text.assert_not_called()\n        mock_write_bytes.assert_not_called()\n        mock_rename.assert_not_called()\n\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'imageio.imread',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'imageio.imwrite',\n    )\n    @patch('examples.YOLO_data_augmentation.data_augmentation.Path.glob')\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'iaa.Sequential.__call__',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.Path.open',\n        new_callable=mock_open,\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'Path.write_text',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'Path.write_bytes',\n    )\n    @patch('examples.YOLO_data_augmentation.data_augmentation.Path.rename')\n    def test_augment_image_none(\n        self,\n        mock_rename: MagicMock,\n        mock_write_bytes: MagicMock,\n        mock_write_text: MagicMock,\n        mock_open_file: MagicMock,\n        mock_seq_call: MagicMock,\n        mock_glob: MagicMock,\n        mock_imwrite: MagicMock,\n        mock_imread: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test the augment_image method when image is None.\n        \"\"\"\n        # Mock image as None\n        mock_imread.return_value = None\n\n        # Mock the label file reading\n        mock_glob.return_value = [\n            Path('tests/cv_dataset/images/mock_image.jpg'),\n        ]\n\n        with patch('builtins.print') as mock_print:\n            self.augmenter.augment_image(\n                Path('tests/cv_dataset/images/mock_image.jpg'),\n            )\n            mock_print.assert_any_call(\n                'Image is None or has no shape: '\n                'tests/cv_dataset/images/mock_image.jpg',\n            )\n\n        mock_imread.assert_called_once_with(\n            Path('tests/cv_dataset/images/mock_image.jpg'),\n        )\n        mock_imwrite.assert_not_called()\n        mock_seq_call.assert_not_called()\n        mock_open_file.assert_not_called()\n        mock_write_text.assert_not_called()\n        mock_write_bytes.assert_not_called()\n        mock_rename.assert_not_called()\n\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'imageio.imread',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'imageio.imwrite',\n    )\n    @patch('examples.YOLO_data_augmentation.data_augmentation.Path.glob')\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.iaa.'\n        'Sequential.__call__',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.Path.open',\n        new_callable=mock_open,\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'Path.write_text',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'Path.write_bytes',\n    )\n    @patch('examples.YOLO_data_augmentation.data_augmentation.Path.rename')\n    def test_augment_image_no_shape(\n        self,\n        mock_rename: MagicMock,\n        mock_write_bytes: MagicMock,\n        mock_write_text: MagicMock,\n        mock_open_file: MagicMock,\n        mock_seq_call: MagicMock,\n        mock_glob: MagicMock,\n        mock_imwrite: MagicMock,\n        mock_imread: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test the augment_image method when image has no shape.\n        \"\"\"\n        # Mock image with no shape\n        mock_image = MagicMock()\n        mock_image.shape = None\n        mock_imread.return_value = mock_image\n\n        # Mock the label file reading\n        mock_glob.return_value = [\n            Path('tests/cv_dataset/images/mock_image.jpg'),\n        ]\n\n        with patch('builtins.print') as mock_print:\n            self.augmenter.augment_image(\n                Path('tests/cv_dataset/images/mock_image.jpg'),\n            )\n            mock_print.assert_any_call(\n                'Image is None or has no shape: '\n                'tests/cv_dataset/images/mock_image.jpg',\n            )\n\n        mock_imread.assert_called_once_with(\n            Path('tests/cv_dataset/images/mock_image.jpg'),\n        )\n        mock_imwrite.assert_not_called()\n        mock_seq_call.assert_not_called()\n        mock_open_file.assert_not_called()\n        mock_write_text.assert_not_called()\n        mock_write_bytes.assert_not_called()\n        mock_rename.assert_not_called()\n\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation.'\n        'imageio.imread',\n    )\n    @patch('builtins.print')\n    def test_augment_image_exception(\n        self,\n        mock_print: MagicMock,\n        mock_imread: MagicMock,\n    ) -> None:\n        # Mock image reading exception\n        mock_imread.side_effect = Exception('Mocked exception')\n\n        # Test augment_image\n        self.augmenter.augment_image(\n            Path('tests/cv_dataset/images/mock_image.jpg'),\n        )\n\n        # Check if the print method was called with the correct output\n        mock_print.assert_any_call(\n            'Error processing image: tests/cv_dataset/images/mock_image.jpg:',\n        )\n        # Check if the exception message is in the print output\n        self.assertTrue(\n            any(\n                'Mocked exception' in str(call)\n                for call in mock_print.call_args_list\n            ),\n        )\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/examples/YOLO_server_api/backend/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/examples/YOLO_server_api/backend/routers_test.py", "content": "from __future__ import annotations\n\nimport base64\nimport datetime\nimport unittest\nfrom collections.abc import AsyncGenerator\nfrom contextlib import contextmanager\nfrom unittest.mock import AsyncMock\nfrom unittest.mock import MagicMock\nfrom unittest.mock import patch\n\nfrom fastapi import FastAPI\nfrom fastapi.testclient import TestClient\nfrom fastapi_jwt import JwtAuthorizationCredentials\n\nfrom examples.YOLO_server_api.backend.routers import auth_router\nfrom examples.YOLO_server_api.backend.routers import custom_rate_limiter\nfrom examples.YOLO_server_api.backend.routers import detection_router\nfrom examples.YOLO_server_api.backend.routers import get_db\nfrom examples.YOLO_server_api.backend.routers import jwt_access\nfrom examples.YOLO_server_api.backend.routers import model_management_router\nfrom examples.YOLO_server_api.backend.routers import user_management_router\n# from fastapi import UploadFile\n\n\nclass TestRouters(unittest.TestCase):\n    \"\"\"\n    Tests for the routers of the YOLO server API.\n    \"\"\"\n    app: FastAPI  # Type hint for the app attribute\n    client: TestClient  # Type hint for the client attribute\n\n    @classmethod\n    def setUpClass(cls) -> None:\n        \"\"\"\n        Set up the FastAPI application and override dependencies.\n        \"\"\"\n        app = FastAPI()\n        app.include_router(auth_router)\n        app.include_router(detection_router)\n        app.include_router(user_management_router)\n        app.include_router(model_management_router)\n\n        # Mock RedisClient\n        mock_redis_client = MagicMock()\n        mock_redis_client.client = AsyncMock()\n\n        # Set redis_client to app state\n        app.state.redis_client = mock_redis_client\n\n        def override_jwt_access() -> JwtAuthorizationCredentials:\n            \"\"\"\n            Override JWT access for testing with admin credentials.\n            \"\"\"\n            return JwtAuthorizationCredentials(\n                subject={'role': 'admin', 'id': 1},\n            )\n\n        def override_custom_rate_limiter() -> int:\n            \"\"\"\n            Override the rate limiter for testing purposes.\n            \"\"\"\n            return 10  # Or any other value you wish\n\n        async def override_get_db() -> AsyncGenerator[str]:\n            \"\"\"\n            Simulate a database session for testing.\n            \"\"\"\n            yield 'mock_db_session'\n\n        app.dependency_overrides[jwt_access] = override_jwt_access\n        app.dependency_overrides[custom_rate_limiter] = (\n            override_custom_rate_limiter\n        )\n        app.dependency_overrides[get_db] = override_get_db\n\n        cls.app = app\n        cls.client = TestClient(app)\n\n    @contextmanager\n    def override_jwt_credentials(self, subject: dict):\n        \"\"\"\n        Context manager to temporarily override JWT credentials\n        during tests.\n        \"\"\"\n        original_jwt_access = self.app.dependency_overrides.get(jwt_access)\n        self.app.dependency_overrides[jwt_access] = (\n            lambda: JwtAuthorizationCredentials(\n                subject=subject,\n            )\n        )\n        try:\n            yield\n        finally:\n            if original_jwt_access is not None:\n                self.app.dependency_overrides[jwt_access] = original_jwt_access\n            else:\n                del self.app.dependency_overrides[jwt_access]\n\n    @patch(\n        'examples.YOLO_server_api.backend.routers.create_token_logic',\n        new_callable=AsyncMock,\n    )\n    def test_create_token_endpoint(self, mock_create_token_logic):\n        \"\"\"\n        Test the create_token_endpoint with mocked create_token_logic.\n        \"\"\"\n        # Mock the return value of create_token_logic\n        mock_create_token_logic.return_value = {'access_token': 'mock_token'}\n\n        # Define input data\n        user_data = {'username': 'testuser', 'password': 'testpassword'}\n\n        # Simulate a POST request to the /api/token endpoint\n        response = self.client.post('/api/token', json=user_data)\n\n        # Validate response\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.json(), {'access_token': 'mock_token'})\n\n    @patch(\n        'examples.YOLO_server_api.backend.routers.process_labels',\n        new_callable=AsyncMock,\n    )\n    @patch('examples.YOLO_server_api.backend.routers.compile_detection_data')\n    @patch(\n        'examples.YOLO_server_api.backend.routers.get_prediction_result',\n        new_callable=AsyncMock,\n    )\n    @patch(\n        'examples.YOLO_server_api.backend.routers.convert_to_image',\n        new_callable=AsyncMock,\n    )\n    @patch('examples.YOLO_server_api.backend.routers.model_loader.get_model')\n    def test_detect_endpoint(\n        self,\n        mock_get_model,\n        mock_convert_to_image,\n        mock_get_prediction_result,\n        mock_compile_detection_data,\n        mock_process_labels,\n    ):\n        \"\"\"\n        Test the detection endpoint with mocked dependencies.\n        \"\"\"\n        # Mock dependencies\n        mock_get_model.return_value = 'mock_model_instance'\n        mock_convert_to_image.return_value = 'mock_image'\n        mock_get_prediction_result.return_value = 'mock_result'\n        mock_compile_detection_data.return_value = [\n            [1.0, 2.0, 3.0, 4.0],\n        ]  # Return a list of the expected type\n        # Return a list of the expected type\n        mock_process_labels.return_value = [[1.0, 2.0, 3.0, 4.0]]\n\n        # Create a test image file\n        image_content = b'test_image_data'\n        files = {'image': ('test_image.jpg', image_content, 'image/jpeg')}\n        data = {'model': 'yolo11n'}\n\n        # Successful case\n        response = self.client.post('/api/detect', data=data, files=files)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.json(), [[1.0, 2.0, 3.0, 4.0]])\n\n        # Model not found case\n        mock_get_model.return_value = None\n        response = self.client.post('/api/detect', data=data, files=files)\n        self.assertEqual(response.status_code, 404)\n\n    @patch(\n        'examples.YOLO_server_api.backend.routers.add_user',\n        new_callable=AsyncMock,\n    )\n    @patch('examples.YOLO_server_api.backend.routers.logger')\n    def test_add_user(self, mock_logger, mock_add_user):\n        \"\"\"\n        Test adding a user with mocked add_user function.\n        \"\"\"\n        mock_add_user.return_value = {'success': True, 'message': 'User added'}\n\n        user_data = {\n            'username': 'testuser',\n            'password': 'testpassword',\n            'role': 'user',\n        }\n\n        response = self.client.post('/api/add_user', json=user_data)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(\n            response.json(), {\n                'message': 'User added successfully.',\n            },\n        )\n        mock_logger.info.assert_called_with('User added')\n\n        # Non-admin role\n        with self.override_jwt_credentials({'role': 'user', 'id': 1}):\n            response = self.client.post('/api/add_user', json=user_data)\n            self.assertEqual(response.status_code, 400)\n\n        # IntegrityError case\n        mock_add_user.return_value = {\n            'success': False,\n            'error': 'IntegrityError',\n            'message': 'Duplicate',\n        }\n        with self.override_jwt_credentials({'role': 'admin', 'id': 1}):\n            response = self.client.post('/api/add_user', json=user_data)\n            self.assertEqual(response.status_code, 400)\n\n    @patch(\n        'examples.YOLO_server_api.backend.routers.delete_user',\n        new_callable=AsyncMock,\n    )\n    @patch('examples.YOLO_server_api.backend.routers.logger')\n    def test_delete_user(self, mock_logger, mock_delete_user):\n        \"\"\"\n        Test deleting a user with mocked delete_user function.\n        \"\"\"\n        mock_delete_user.return_value = {\n            'success': True, 'message': 'User deleted',\n        }\n\n        user_data = {'username': 'testuser'}\n\n        response = self.client.post('/api/delete_user', json=user_data)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(\n            response.json(), {\n                'message': 'User deleted successfully.',\n            },\n        )\n        mock_logger.info.assert_called_with('User deleted')\n\n        # NotFound case\n        mock_delete_user.return_value = {\n            'success': False, 'error': 'NotFound', 'message': 'No user',\n        }\n        response = self.client.post('/api/delete_user', json=user_data)\n        self.assertEqual(response.status_code, 404)\n\n        # Other error case\n        mock_delete_user.return_value = {\n            'success': False, 'error': 'Other', 'message': 'Error',\n        }\n        response = self.client.post('/api/delete_user', json=user_data)\n        self.assertEqual(response.status_code, 500)\n\n        # Non-admin role\n        self.app.dependency_overrides[jwt_access] = (\n            lambda: JwtAuthorizationCredentials(\n                subject={'role': 'user', 'id': 1},\n            )\n        )\n        response = self.client.post('/api/delete_user', json=user_data)\n        self.assertEqual(response.status_code, 403)\n\n    @patch(\n        'examples.YOLO_server_api.backend.routers.update_username',\n        new_callable=AsyncMock,\n    )\n    @patch('examples.YOLO_server_api.backend.routers.logger')\n    def test_update_username(self, mock_logger, mock_update_username):\n        \"\"\"\n        Test updating a username with mocked update_username function.\n        \"\"\"\n        async def success_func(old_username, new_username, db):\n            return {'success': True, 'message': 'Username updated'}\n        mock_update_username.side_effect = success_func\n\n        user_data = {'old_username': 'olduser', 'new_username': 'newuser'}\n        with self.override_jwt_credentials({'role': 'admin', 'id': 1}):\n            response = self.client.put('/api/update_username', json=user_data)\n            self.assertEqual(response.status_code, 200)\n            self.assertEqual(\n                response.json(), {\n                    'message': 'Username updated successfully.',\n                },\n            )\n            mock_logger.info.assert_called_with('Username updated')\n\n        # IntegrityError\n        async def integrity_error_func(old_username, new_username, db):\n            return {\n                'success': False,\n                'error': 'IntegrityError',\n                'message': 'Duplicate',\n            }\n        mock_update_username.side_effect = integrity_error_func\n        with self.override_jwt_credentials({'role': 'admin', 'id': 1}):\n            response = self.client.put('/api/update_username', json=user_data)\n            self.assertEqual(response.status_code, 400)\n\n        # NotFound\n        async def not_found_func(old_username, new_username, db):\n            return {\n                'success': False,\n                'error': 'NotFound',\n                'message': 'Not found',\n            }\n        mock_update_username.side_effect = not_found_func\n        with self.override_jwt_credentials({'role': 'admin', 'id': 1}):\n            response = self.client.put('/api/update_username', json=user_data)\n            self.assertEqual(response.status_code, 404)\n\n        # Non-admin role\n        with self.override_jwt_credentials({'role': 'user', 'id': 1}):\n            response = self.client.put('/api/update_username', json=user_data)\n            self.assertEqual(response.status_code, 400)\n\n    @patch(\n        'examples.YOLO_server_api.backend.routers.update_password',\n        new_callable=AsyncMock,\n    )\n    @patch('examples.YOLO_server_api.backend.routers.logger')\n    def test_update_password(self, mock_logger, mock_update_password):\n        \"\"\"\n        Test updating a password with mocked update_password function.\n        \"\"\"\n        async def success_func(username, new_password, db):\n            return {'success': True, 'message': 'Password updated'}\n        mock_update_password.side_effect = success_func\n\n        user_data = {'username': 'testuser', 'new_password': 'newpassword'}\n        with self.override_jwt_credentials({'role': 'admin', 'id': 1}):\n            response = self.client.put('/api/update_password', json=user_data)\n            self.assertEqual(response.status_code, 200)\n            self.assertEqual(\n                response.json(), {\n                    'message': 'Password updated successfully.',\n                },\n            )\n            mock_logger.info.assert_called_with('Password updated')\n\n        # NotFound\n        async def not_found_func(username, new_password, db):\n            return {\n                'success': False,\n                'error': 'NotFound',\n                'message': 'No user',\n            }\n        mock_update_password.side_effect = not_found_func\n        with self.override_jwt_credentials({'role': 'admin', 'id': 1}):\n            response = self.client.put('/api/update_password', json=user_data)\n            self.assertEqual(response.status_code, 404)\n\n        # Other error\n        async def other_error_func(username, new_password, db):\n            return {'success': False, 'error': 'Other', 'message': 'Error'}\n        mock_update_password.side_effect = other_error_func\n        with self.override_jwt_credentials({'role': 'admin', 'id': 1}):\n            response = self.client.put('/api/update_password', json=user_data)\n            self.assertEqual(response.status_code, 500)\n\n        # Non-admin role\n        with self.override_jwt_credentials({'role': 'user', 'id': 1}):\n            response = self.client.put('/api/update_password', json=user_data)\n            self.assertEqual(response.status_code, 400)\n\n    @patch(\n        'examples.YOLO_server_api.backend.routers.set_user_active_status',\n        new_callable=AsyncMock,\n    )\n    @patch('examples.YOLO_server_api.backend.routers.logger')\n    def test_set_user_active_status(\n        self,\n        mock_logger,\n        mock_set_user_active_status,\n    ):\n        \"\"\"\n        Test setting a user's active status\n        with mocked set_user_active_status function.\n        \"\"\"\n        async def success_func(username, is_active, db):\n            return {'success': True, 'message': 'Status updated'}\n        mock_set_user_active_status.side_effect = success_func\n\n        user_data = {'username': 'testuser', 'is_active': True}\n        with self.override_jwt_credentials({'role': 'admin', 'id': 1}):\n            response = self.client.put(\n                '/api/set_user_active_status', json=user_data,\n            )\n            self.assertEqual(response.status_code, 200)\n            self.assertEqual(\n                response.json(), {\n                    'message': 'User active status updated successfully.',\n                },\n            )\n            mock_logger.info.assert_called_with('Status updated')\n\n        # NotFound\n        async def not_found_func(username, is_active, db):\n            return {\n                'success': False,\n                'error': 'NotFound',\n                'message': 'No user',\n            }\n        mock_set_user_active_status.side_effect = not_found_func\n        with self.override_jwt_credentials({'role': 'admin', 'id': 1}):\n            response = self.client.put(\n                '/api/set_user_active_status', json=user_data,\n            )\n            self.assertEqual(response.status_code, 404)\n\n        # Other error\n        async def other_error_func(username, is_active, db):\n            return {'success': False, 'error': 'Other', 'message': 'Error'}\n        mock_set_user_active_status.side_effect = other_error_func\n        with self.override_jwt_credentials({'role': 'admin', 'id': 1}):\n            response = self.client.put(\n                '/api/set_user_active_status', json=user_data,\n            )\n            self.assertEqual(response.status_code, 500)\n\n        # Non-admin role\n        with self.override_jwt_credentials({'role': 'user', 'id': 1}):\n            response = self.client.put(\n                '/api/set_user_active_status', json=user_data,\n            )\n            self.assertEqual(response.status_code, 403)\n\n    @patch(\n        'examples.YOLO_server_api.backend.routers.update_model_file',\n        new_callable=AsyncMock,\n    )\n    @patch('examples.YOLO_server_api.backend.routers.logger')\n    def test_model_file_update(self, mock_logger, mock_update_model_file):\n        \"\"\"\n        Test updating a model file with mocked update_model_file function.\n        \"\"\"\n        # Simulate successful execution\n        async def mock_update_model_file_func(model, temp_path):\n            pass\n        mock_update_model_file.side_effect = mock_update_model_file_func\n\n        model_file_content = b'model file content'\n        files = {\n            'file': (\n                'model.pt', model_file_content,\n                'application/octet-stream',\n            ),\n        }\n        data = {'model': 'yolo11n'}\n\n        # Admin role success\n        with self.override_jwt_credentials({'role': 'admin', 'id': 1}):\n            response = self.client.post(\n                '/api/model_file_update', data=data, files=files,\n            )\n            self.assertEqual(response.status_code, 200)\n            self.assertEqual(\n                response.json(), {\n                    'message': 'Model yolo11n updated successfully.',\n                },\n            )\n            mock_logger.info.assert_called_with(\n                'Model yolo11n updated successfully.',\n            )\n\n        # Non-admin or model_manage role\n        with self.override_jwt_credentials({'role': 'user', 'id': 1}):\n            response = self.client.post(\n                '/api/model_file_update', data=data, files=files,\n            )\n            self.assertEqual(response.status_code, 403)\n\n        # Invalid file path scenario (ValueError)\n        with patch(\n            'examples.YOLO_server_api.backend.routers.secure_filename',\n            return_value='../model.pt',\n        ):\n            with self.override_jwt_credentials({'role': 'admin', 'id': 1}):\n                response = self.client.post(\n                    '/api/model_file_update', data=data, files=files,\n                )\n                self.assertEqual(response.status_code, 400)\n                mock_logger.error.assert_called()\n\n        # Simulate ValueError in update_model_file\n        async def mock_update_model_file_raise_value_error(model, temp_path):\n            raise ValueError('Invalid model')\n        mock_update_model_file.side_effect = (\n            mock_update_model_file_raise_value_error\n        )\n\n        with self.override_jwt_credentials({'role': 'admin', 'id': 1}):\n            response = self.client.post(\n                '/api/model_file_update', data=data, files=files,\n            )\n            self.assertEqual(response.status_code, 400)\n            self.assertEqual(response.json()['detail'], 'Invalid model')\n            mock_logger.error.assert_called_with(\n                'Model update validation error: Invalid model',\n            )\n\n        # Simulate OSError during file operation\n        with patch('pathlib.Path.open', side_effect=OSError('Disk error')):\n            mock_update_model_file.side_effect = mock_update_model_file_func\n            with self.override_jwt_credentials({'role': 'admin', 'id': 1}):\n                response = self.client.post(\n                    '/api/model_file_update', data=data, files=files,\n                )\n                self.assertEqual(response.status_code, 500)\n                self.assertEqual(response.json()['detail'], 'Disk error')\n                mock_logger.error.assert_called_with(\n                    'Model update I/O error: Disk error',\n                )\n\n    @patch('examples.YOLO_server_api.backend.routers.get_new_model_file')\n    @patch('examples.YOLO_server_api.backend.routers.logger')\n    def test_get_new_model(self, mock_logger, mock_get_new_model_file):\n        \"\"\"\n        Test retrieving a new model file\n        with mocked get_new_model_file function.\n        \"\"\"\n        async def mock_get_new_model_file_func(model, user_last_update):\n            return b'new model content'  # Assume a new model is available\n        mock_get_new_model_file.side_effect = mock_get_new_model_file_func\n\n        request_data = {\n            'model': 'yolo11n',\n            'last_update_time': datetime.datetime.now().isoformat(),\n        }\n\n        response = self.client.post('/api/get_new_model', json=request_data)\n        self.assertEqual(response.status_code, 200)\n        expected_response = {\n            'message': 'Model yolo11n is updated.',\n            'model_file': base64.b64encode(b'new model content').decode(),\n        }\n        self.assertEqual(response.json(), expected_response)\n        mock_logger.info.assert_called_with(\n            'Newer model file for yolo11n retrieved.',\n        )\n\n        # No new model available\n        async def mock_no_update(model, user_last_update):\n            return None\n        mock_get_new_model_file.side_effect = mock_no_update\n        response = self.client.post('/api/get_new_model', json=request_data)\n        self.assertEqual(response.status_code, 200)\n        expected_response = {'message': 'Model yolo11n is up to date.'}\n        self.assertEqual(response.json(), expected_response)\n        mock_logger.info.assert_called_with(\n            'No update required for model yolo11n.',\n        )\n\n        # Invalid datetime format\n        invalid_request_data = {\n            'model': 'yolo11n',\n            'last_update_time': 'invalid_datetime',\n        }\n        response = self.client.post(\n            '/api/get_new_model', json=invalid_request_data,\n        )\n        self.assertEqual(response.status_code, 400)\n\n        # Exception scenario\n        async def mock_exception(model, user_last_update):\n            raise Exception('Some error')\n        mock_get_new_model_file.side_effect = mock_exception\n        response = self.client.post('/api/get_new_model', json=request_data)\n        self.assertEqual(response.status_code, 500)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/examples/YOLO_evaluation/evaluate_sahi_yolo_test.py", "content": "from __future__ import annotations\n\nimport argparse\nimport unittest\nfrom unittest.mock import MagicMock\nfrom unittest.mock import patch\n\nimport numpy as np\n\nfrom examples.YOLO_evaluation.evaluate_sahi_yolo import COCOEvaluator\nfrom examples.YOLO_evaluation.evaluate_sahi_yolo import main\n\n\nclass TestCOCOEvaluator(unittest.TestCase):\n    def setUp(self) -> None:\n        self.model_path: str = 'models/pt/best_yolo11n.pt'\n        self.coco_json: str = 'tests/dataset/coco_annotations.json'\n        self.image_dir: str = 'tests/dataset/val/images'\n        self.evaluator = COCOEvaluator(\n            model_path=self.model_path,\n            coco_json=self.coco_json,\n            image_dir=self.image_dir,\n        )\n\n    def tearDown(self) -> None:\n        \"\"\"\n        Clean up after each test.\n        \"\"\"\n        del self.evaluator\n\n    @patch(\n        'examples.YOLO_evaluation.evaluate_sahi_yolo.COCOeval',\n    )\n    @patch(\n        'examples.YOLO_evaluation.evaluate_sahi_yolo.COCO',\n    )\n    @patch(\n        'examples.YOLO_evaluation.evaluate_sahi_yolo.'\n        'Coco.from_coco_dict_or_path',\n    )\n    @patch(\n        'examples.YOLO_evaluation.evaluate_sahi_yolo.get_sliced_prediction',\n    )\n    @patch(\n        'examples.YOLO_evaluation.evaluate_sahi_yolo.'\n        'AutoDetectionModel.from_pretrained',\n    )\n    def test_evaluate(\n        self,\n        mock_auto_model: MagicMock,\n        mock_get_sliced_prediction: MagicMock,\n        mock_coco_from_path: MagicMock,\n        mock_coco: MagicMock,\n        mock_cocoeval: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test the evaluate method for computing COCO metrics.\n        \"\"\"\n        # Mock the model\n        mock_model: MagicMock = MagicMock()\n        mock_auto_model.return_value = mock_model\n\n        # Mock COCO annotations and evaluation\n        mock_coco_instance: MagicMock = MagicMock()\n\n        # Define mock categories with specific names and ids\n        mock_category1 = MagicMock()\n        mock_category1.name = 'Hardhat'\n        mock_category1.id = 1\n\n        mock_category2 = MagicMock()\n        mock_category2.name = 'Helmet'\n        mock_category2.id = 2\n\n        mock_coco_instance.categories = [mock_category1, mock_category2]\n\n        # Define mock images with specific ids and file names\n        mock_image1 = MagicMock()\n        mock_image1.id = 101\n        mock_image1.file_name = 'image1.jpg'\n\n        mock_image2 = MagicMock()\n        mock_image2.id = 102\n        mock_image2.file_name = 'image2.jpg'\n\n        mock_coco_instance.images = [mock_image1, mock_image2]\n\n        mock_coco.return_value = mock_coco_instance\n\n        # Mock the Coco.from_coco_dict_or_path to return the mock_coco_instance\n        mock_coco_from_path.return_value = mock_coco_instance\n\n        mock_eval_instance: MagicMock = MagicMock()\n        # Simulate the precision and recall values\n        mock_eval_instance.eval = {\n            'precision': np.random.rand(10, 10, 10, 10, 10),\n            'recall': np.random.rand(10, 10, 10, 10),\n        }\n        mock_cocoeval.return_value = mock_eval_instance\n\n        # Mock the predictions with specific category names\n        mock_pred1 = MagicMock()\n        mock_pred1.category.name = 'Hardhat'\n        mock_pred1.bbox.minx = 10\n        mock_pred1.bbox.miny = 20\n        mock_pred1.bbox.maxx = 110\n        mock_pred1.bbox.maxy = 220\n        mock_pred1.score.value = 0.9\n\n        mock_pred2 = MagicMock()\n        mock_pred2.category.name = 'Helmet'\n        mock_pred2.bbox.minx = 15\n        mock_pred2.bbox.miny = 25\n        mock_pred2.bbox.maxx = 115\n        mock_pred2.bbox.maxy = 225\n        mock_pred2.score.value = 0.85\n\n        mock_get_sliced_prediction.return_value.object_prediction_list = [\n            mock_pred1,\n            mock_pred2,\n        ]\n\n        # Run the evaluation\n        metrics: dict[str, float] = self.evaluator.evaluate()\n\n        # Verify that the metrics returned\n        # by the evaluate method are as expected\n        expected_metrics: dict[str, float] = {\n            'Average Precision': np.mean(\n                mock_eval_instance.eval['precision'][:, :, :, 0, -1],\n            ),\n            'Average Recall': np.mean(\n                mock_eval_instance.eval['recall'][:, :, 0, -1],\n            ),\n            'mAP at IoU=50': np.mean(\n                mock_eval_instance.eval['precision'][0, :, :, 0, 2],\n            ),\n            'mAP at IoU=50-95': np.mean(\n                mock_eval_instance.eval['precision'][0, :, :, 0, :],\n            ),\n        }\n\n        self.assertEqual(metrics, expected_metrics)\n\n    @patch(\n        'examples.YOLO_evaluation.evaluate_sahi_yolo.COCOEvaluator.evaluate',\n    )\n    @patch(\n        'argparse.ArgumentParser.parse_args', return_value=argparse.Namespace(\n            model_path='models/pt/best_yolo11n.pt',\n            coco_json='tests/dataset/coco_annotations.json',\n            image_dir='tests/dataset/val/images',\n        ),\n    )\n    def test_main(\n        self,\n        mock_parse_args: MagicMock,\n        mock_evaluate: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test the main function.\n        \"\"\"\n        mock_evaluate.return_value = {\n            'Average Precision': 0.5,\n            'Average Recall': 0.6,\n            'mAP at IoU=50': 0.7,\n            'mAP at IoU=50-95': 0.8,\n        }\n\n        with patch('builtins.print') as mock_print:\n            main()\n            mock_print.assert_any_call(\n                'Evaluation metrics:', {\n                    'Average Precision': 0.5,\n                    'Average Recall': 0.6,\n                    'mAP at IoU=50': 0.7,\n                    'mAP at IoU=50-95': 0.8,\n                },\n            )\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/src/danger_detector_test.py", "content": "from __future__ import annotations\n\nimport unittest\nfrom unittest.mock import MagicMock\nfrom unittest.mock import patch\n\nfrom shapely.geometry import Polygon\n\nfrom src.danger_detector import DangerDetector\nfrom src.danger_detector import main\nfrom src.utils import Utils\n\n\nclass TestDangerDetector(unittest.TestCase):\n    \"\"\"\n    Unit tests for the DangerDetector class methods.\n    \"\"\"\n\n    def setUp(self) -> None:\n        \"\"\"\n        Set up method to create an instance of DangerDetector for each test.\n        \"\"\"\n        self.detector: DangerDetector = DangerDetector()\n\n    def test_initialization(self) -> None:\n        \"\"\"\n        Test case for checking if a person is driving based on bounding boxes.\n        \"\"\"\n        # Valid detection items\n        valid_detection_items = {\n            'detect_no_safety_vest_or_helmet': True,\n            'detect_near_machinery_or_vehicle': True,\n            'detect_in_restricted_area': True,\n        }\n        detector = DangerDetector(valid_detection_items)\n        self.assertEqual(detector.detection_items, valid_detection_items)\n\n        # Invalid detection items (not a dictionary)\n        invalid_items_not_dict = {'invalid', 'items'}\n        detector = DangerDetector(invalid_items_not_dict)  # type: ignore\n        self.assertEqual(detector.detection_items, {})\n\n        # Invalid detection items (keys not strings or values not booleans)\n        invalid_items_wrong_value = {\n            'detect_no_safety_vest_or_helmet': 'yes',  # Invalid value\n            'detect_near_machinery_or_vehicle': True,\n            'detect_in_restricted_area': True,\n        }\n        detector = DangerDetector(invalid_items_wrong_value)  # type: ignore\n        self.assertEqual(detector.detection_items, {})\n\n    def test_detect_danger(self) -> None:\n        \"\"\"\n        Test case for detecting danger based on a list of data points.\n        \"\"\"\n        data: list[list[float]] = [\n            [50, 50, 150, 150, 0.95, 0],    # Hardhat\n            [200, 200, 300, 300, 0.85, 5],  # Person\n            [400, 400, 500, 500, 0.75, 2],  # No-Safety Vest\n            [0, 0, 10, 10, 0.88, 6],        # Safety cone\n            [0, 1000, 10, 1010, 0.87, 6],   # Safety cone\n            [1000, 0, 1010, 10, 0.89, 6],   # Safety cone\n            [100, 100, 120, 120, 0.9, 6],   # Safety cone\n            [150, 150, 170, 170, 0.85, 6],  # Safety cone\n            [200, 200, 220, 220, 0.89, 6],  # Safety cone\n            [250, 250, 270, 270, 0.85, 6],  # Safety cone\n            [450, 450, 470, 470, 0.92, 6],  # Safety cone\n            [500, 500, 520, 520, 0.88, 6],  # Safety cone\n            [550, 550, 570, 570, 0.86, 6],  # Safety cone\n            [600, 600, 620, 620, 0.84, 6],  # Safety cone\n            [650, 650, 670, 670, 0.82, 6],  # Safety cone\n            [700, 700, 720, 720, 0.80, 6],  # Safety cone\n            [750, 750, 770, 770, 0.78, 6],  # Safety cone\n            [800, 800, 820, 820, 0.76, 6],  # Safety cone\n            [850, 850, 870, 870, 0.74, 6],  # Safety cone\n        ]\n        data = Utils.normalise_data(data)\n        warnings, polygons = self.detector.detect_danger(data)\n        self.assertIn(\n            'Warning: 1 people have entered the controlled area!', warnings,\n        )\n        self.assertIn('Warning: Someone is not wearing a hardhat!', warnings)\n\n        data = [\n            [706.87, 445.07, 976.32, 1073.6, 0.91, 5.0],  # Person\n            [0.45513, 471.77, 662.03, 1071.4, 0.75853, 5.0],  # Person\n            [1042.7, 638.5, 1077.5, 731.98, 0.56060, 4.0],  # No safety vest\n            [500, 500, 700, 700, 0.95, 8],  # Machinery\n            [50, 50, 70, 70, 0.89, 6],\n            [250, 250, 270, 270, 0.85, 6],\n            [450, 450, 470, 470, 0.92, 6],\n        ]\n        data = Utils.normalise_data(data)\n        warnings, polygons = self.detector.detect_danger(data)\n        self.assertIn(\n            'Warning: Someone is not wearing a safety vest!', warnings,\n        )\n\n        data = [\n            [706.87, 445.07, 976.32, 1073.6, 0.91, 2.0],  # No hardhat\n            [0.45513, 471.77, 662.03, 1071.4, 0.75853, 4.0],  # No safety vest\n            [500, 500, 700, 700, 0.95, 8],  # Machinery\n            [50, 50, 70, 70, 0.89, 6],\n            [250, 250, 270, 270, 0.85, 6],\n            [450, 450, 470, 470, 0.92, 6],\n        ]\n        data = Utils.normalise_data(data)\n        warnings, polygons = self.detector.detect_danger(data)\n        self.assertIn('Warning: Someone is not wearing a hardhat!', warnings)\n        self.assertIn(\n            'Warning: Someone is not wearing a safety vest!', warnings,\n        )\n        self.assertNotIn(\n            'Warning: Someone is too close to machinery!', warnings,\n        )\n\n    def test_no_data(self) -> None:\n        \"\"\"\n        Test case for checking behavior when no data is provided.\n        \"\"\"\n        data: list[list[float]] = []\n        warnings, polygons = self.detector.detect_danger(data)\n        self.assertEqual(len(warnings), 0)\n        self.assertEqual(len(polygons), 0)\n\n    def test_vehicle_too_close(self) -> None:\n        \"\"\"\n        Test case for checking behaviour\n        when a person is too close to a vehicle.\n        \"\"\"\n        data: list[list[float]] = [\n            [100, 100, 120, 120, 0.95, 5],  # Person\n            [110, 110, 200, 200, 0.85, 8],  # Machinery\n        ]\n        normalised_data = Utils.normalise_data(data)\n        print(f\"Normalized data: {normalised_data}\")\n        warnings, polygons = self.detector.detect_danger(normalised_data)\n        self.assertIn('Warning: Someone is too close to machinery!', warnings)\n\n    @patch('builtins.print')\n    def test_main(self, mock_print: MagicMock) -> None:\n        \"\"\"\n        Test case for the main function.\n        \"\"\"\n        with patch.object(\n            DangerDetector, 'detect_danger', return_value=(\n                [\n                    'Warning: Someone is not wearing a hardhat!',\n                    'Warning: 1 people have entered the controlled area!',\n                ],\n                [Polygon([(0, 0), (10, 0), (10, 10), (0, 10)])],\n            ),\n        ) as mock_detect_danger:\n            main()\n            mock_detect_danger.assert_called_once()\n            mock_print.assert_any_call(\n                \"Warnings: ['Warning: Someone is not wearing a hardhat!', \"\n                \"'Warning: 1 people have entered the controlled area!']\",\n            )\n            mock_print.assert_any_call(\n                'Polygons: [<POLYGON ((0 0, 10 0, 10 10, 0 10, 0 0))>]',\n            )\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/examples/YOLO_evaluation/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/examples/YOLO_data_augmentation/data_augmentation_albumentations_test.py", "content": "from __future__ import annotations\n\nimport argparse\nimport unittest\nfrom pathlib import Path\nfrom unittest.mock import MagicMock\nfrom unittest.mock import patch\n\nimport numpy as np\n\nfrom examples.YOLO_data_augmentation.data_augmentation_albumentations import (\n    DataAugmentation,\n)\nfrom examples.YOLO_data_augmentation.data_augmentation_albumentations import (\n    main,\n)\n\n\nclass TestDataAugmentation(unittest.TestCase):\n    \"\"\"\n    Unit tests for the DataAugmentation class.\n    \"\"\"\n\n    def setUp(self) -> None:\n        \"\"\"\n        Set up the test environment.\n        \"\"\"\n        self.train_path = 'tests/cv_dataset'\n        self.num_augmentations = 2\n        self.augmenter = DataAugmentation(\n            self.train_path, self.num_augmentations,\n        )\n\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation_albumentations.'\n        'cv2.imread',\n    )\n    @patch('builtins.print')\n    def test_augment_image_exception(\n        self, mock_print: MagicMock, mock_imread: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test augment_image when an exception occurs during image reading.\n\n        Args:\n            mock_print (MagicMock): Mocked print function.\n            mock_imread (MagicMock): Mocked cv2.imread function.\n        \"\"\"\n        # Mock image reading exception\n        mock_imread.side_effect = Exception('Mocked exception')\n\n        # Test augment_image\n        self.augmenter.augment_image(\n            Path('tests/cv_dataset/images/mock_image.jpg'),\n        )\n\n        # Check if the print method was called with the correct output\n        mock_print.assert_any_call(\n            'Error processing image: '\n            'tests/cv_dataset/images/mock_image.jpg: Mocked exception',\n        )\n\n    @patch('builtins.print')\n    def test_augment_image_none(self, mock_print: MagicMock) -> None:\n        \"\"\"\n        Test augment_image method when the image is None.\n\n        Args:\n            mock_print (MagicMock): Mocked print function.\n        \"\"\"\n        # Test when image is None\n        self.augmenter.augment_image(None)\n        mock_print.assert_any_call('Error processing image: None')\n\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation_albumentations.'\n        'cv2.imread',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.'\n        'data_augmentation_albumentations.cv2.cvtColor',\n    )\n    def test_augment_image(\n        self, mock_cvtColor: MagicMock, mock_imread: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test augment_image method with valid image data.\n\n        Args:\n            mock_cvtColor (MagicMock): Mocked cv2.cvtColor function.\n            mock_imread (MagicMock): Mocked cv2.imread function.\n        \"\"\"\n        # Mock image and label data\n        mock_image = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)\n        mock_bboxes = [[0.5, 0.5, 0.2, 0.2]]\n        mock_class_labels = [1]\n\n        mock_imread.return_value = mock_image\n        mock_cvtColor.return_value = mock_image\n\n        with patch.object(\n            self.augmenter,\n            'read_label_file',\n            return_value=(mock_class_labels, mock_bboxes),\n        ):\n            with patch.object(\n                self.augmenter, 'write_label_file',\n            ) as mock_write_label_file:\n                with patch(\n                    'examples.YOLO_data_augmentation.'\n                    'data_augmentation_albumentations.cv2.imwrite',\n                ) as mock_imwrite:\n                    self.augmenter.augment_image(Path('image.jpg'))\n                    self.assertTrue(mock_imread.called)\n                    self.assertTrue(mock_cvtColor.called)\n                    self.assertTrue(mock_imwrite.called)\n                    self.assertTrue(mock_write_label_file.called)\n\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation_albumentations.'\n        'cv2.imread',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation_albumentations.'\n        'cv2.cvtColor',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation_albumentations.'\n        'cv2.imwrite',\n    )\n    def test_augment_image_with_alpha_channel(\n        self,\n        mock_imwrite: MagicMock,\n        mock_cvtColor: MagicMock,\n        mock_imread: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test augment_image method with an image that has an alpha channel.\n\n        Args:\n            mock_imwrite (MagicMock): Mocked cv2.imwrite function.\n            mock_cvtColor (MagicMock): Mocked cv2.cvtColor function.\n            mock_imread (MagicMock): Mocked cv2.imread function.\n        \"\"\"\n        # Mock image with alpha channel\n        mock_image = np.random.randint(0, 255, (100, 100, 4), dtype=np.uint8)\n        mock_imread.return_value = mock_image\n\n        mock_bboxes = [[0.5, 0.5, 0.2, 0.2]]\n        mock_class_labels = [1]\n\n        with patch.object(\n            self.augmenter,\n            'read_label_file',\n            return_value=(mock_class_labels, mock_bboxes),\n        ):\n            # Simulate cv2.cvtColor behaviour to remove alpha channel\n            mock_cvtColor.side_effect = lambda img, code: img[\n                :,\n                :, :3,\n            ] if img.shape[2] == 4 else img\n\n            # Mock the mask to be used in MaskDropout\n            mock_mask = np.random.randint(0, 2, (100, 100), dtype=np.uint8)\n\n            with patch(\n                'examples.YOLO_data_augmentation.'\n                'data_augmentation_albumentations.A.MaskDropout',\n                return_value={'mask': mock_mask},\n            ):\n                self.augmenter.augment_image(Path('image_with_alpha.jpg'))\n                self.assertTrue(mock_imread.called)\n                self.assertTrue(mock_cvtColor.called)\n\n                # Ensure the image has 3 channels\n                # after removing the alpha channel\n                processed_image = mock_cvtColor(mock_image, None)\n                self.assertEqual(processed_image.shape[2], 3)\n\n                self.assertTrue(mock_imwrite.called)\n\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation_albumentations.'\n        'A.Compose',\n    )\n    def test_resize_small_image(self, mock_compose: MagicMock) -> None:\n        \"\"\"\n        Test resize_image_and_bboxes method with a small image.\n        \"\"\"\n        mock_image = np.random.randint(0, 255, (20, 20, 3), dtype=np.uint8)\n        mock_bboxes = [[0.5, 0.5, 0.2, 0.2]]\n        class_labels = [1]\n        image_path = Path('small_image.jpg')\n\n        # Mock the transformation result\n        mock_transformed = {\n            'image': np.random.randint(0, 255, (64, 64, 3), dtype=np.uint8),\n            'bboxes': [[0.5, 0.5, 0.2, 0.2]],\n        }\n        mock_transform = MagicMock()\n        mock_transform.return_value = mock_transformed\n        mock_compose.return_value = mock_transform\n\n        resized_image, resized_bboxes = self.augmenter.resize_image_and_bboxes(\n            mock_image, mock_bboxes, class_labels, image_path,\n        )\n\n        # Verify that the Compose and BboxParams were called correctly\n        mock_compose.assert_called()\n        mock_transform.assert_called_once_with(\n            image=mock_image, bboxes=mock_bboxes, class_labels=class_labels,\n        )\n\n        # Verify the transformation result\n        self.assertEqual(resized_image.shape, (64, 64, 3))\n        self.assertEqual(resized_bboxes, [[0.5, 0.5, 0.2, 0.2]])\n\n    def test_resize_large_image(self) -> None:\n        \"\"\"\n        Test resize_image_and_bboxes method with a large image.\n        \"\"\"\n        mock_image = np.random.randint(0, 255, (2000, 2000, 3), dtype=np.uint8)\n        mock_bboxes = [[0.5, 0.5, 0.2, 0.2]]\n        mock_class_labels = [1]\n        image_path = Path('large_image.jpg')\n\n        with patch('builtins.print') as mock_print:\n            resized_image, resized_bboxes = (\n                self.augmenter.resize_image_and_bboxes(\n                    mock_image,\n                    mock_bboxes,\n                    mock_class_labels,\n                    image_path,\n                )\n            )\n            self.assertEqual(resized_image.shape, (1920, 1920, 3))\n            mock_print.assert_called_with(\n                f\"Resize {image_path} due to large size: {mock_image.shape}\",\n            )\n\n    def test_random_crop_with_random_size(self) -> None:\n        \"\"\"\n        Test random_crop_with_random_size method.\n        \"\"\"\n        image = np.random.randint(0, 255, (1000, 1000, 3), dtype=np.uint8)\n        cropped_image = self.augmenter.random_crop_with_random_size(image)\n        self.assertTrue(400 <= cropped_image.shape[0] <= 800)\n        self.assertTrue(400 <= cropped_image.shape[1] <= 800)\n\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation_albumentations.'\n        'ProcessPoolExecutor',\n    )\n    def test_augment_data(self, mock_executor: MagicMock) -> None:\n        \"\"\"\n        Test augment_data method.\n\n        Args:\n            mock_executor (MagicMock): Mocked ProcessPoolExecutor.\n        \"\"\"\n        mock_executor.return_value.__enter__.return_value.map = MagicMock()\n        self.augmenter.augment_data(batch_size=2)\n        self.assertTrue(mock_executor.called)\n\n    def test_read_label_file(self) -> None:\n        \"\"\"\n        Test read_label_file method.\n        \"\"\"\n        label_content = '0 0.5 0.5 0.2 0.2\\n'\n        label_path = Path('label.txt')\n        with patch(\n            'builtins.open',\n            unittest.mock.mock_open(read_data=label_content),\n        ):\n            class_labels, bboxes = self.augmenter.read_label_file(label_path)\n            self.assertEqual(class_labels, [0])\n            self.assertEqual(bboxes, [[0.5, 0.5, 0.2, 0.2]])\n\n    def test_write_label_file(self) -> None:\n        \"\"\"\n        Test write_label_file method.\n        \"\"\"\n        bboxes_aug = [(0.5, 0.5, 0.2, 0.2)]\n        class_labels_aug = [0]\n        label_path = Path('label_aug.txt')\n        with patch('builtins.open', unittest.mock.mock_open()) as mock_file:\n            self.augmenter.write_label_file(\n                bboxes_aug, class_labels_aug, label_path,\n            )\n            mock_file().write.assert_called_with('0 0.5 0.5 0.2 0.2\\n')\n\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation_albumentations.'\n        'random.shuffle',\n    )\n    def test_shuffle_data(self, mock_shuffle: MagicMock) -> None:\n        \"\"\"\n        Test shuffle_data method.\n\n        Args:\n            mock_shuffle (MagicMock): Mocked random.shuffle function.\n        \"\"\"\n        image_dir = Path(self.train_path) / 'images'\n        label_dir = Path(self.train_path) / 'labels'\n        image_paths = [image_dir / f'image_{i}.jpg' for i in range(5)]\n        label_paths = [label_dir / f'label_{i}.txt' for i in range(5)]\n\n        with patch.object(\n            Path, 'glob',\n            side_effect=[image_paths, label_paths],\n        ):\n            with patch.object(Path, 'rename') as mock_rename:\n                self.augmenter.shuffle_data()\n                self.assertTrue(mock_shuffle.called)\n                self.assertTrue(mock_rename.called)\n\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation_albumentations.'\n        'DataAugmentation',\n    )\n    @patch('argparse.ArgumentParser.parse_args')\n    def test_main(\n        self, mock_parse_args: MagicMock, MockDataAugmentation: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test main function.\n\n        Args:\n            mock_parse_args (MagicMock):\n                Mocked argparse.ArgumentParser.parse_args function.\n            MockDataAugmentation (MagicMock): Mocked DataAugmentation class.\n        \"\"\"\n        # Mock command line arguments\n        mock_parse_args.return_value = argparse.Namespace(\n            train_path='./dataset_aug/train',\n            num_augmentations=10,\n            batch_size=5,\n        )\n\n        # Mock DataAugmentation class\n        mock_augmenter = MockDataAugmentation.return_value\n        mock_augmenter.augment_data = MagicMock()\n        mock_augmenter.shuffle_data = MagicMock()\n\n        # Execute main function\n        main()\n\n        # Verify DataAugmentation class was correctly initialised\n        MockDataAugmentation.assert_called_once_with('./dataset_aug/train', 10)\n\n        # Verify augment_data and shuffle_data methods were called\n        mock_augmenter.augment_data.assert_called_once_with(batch_size=5)\n        mock_augmenter.shuffle_data.assert_called_once()\n\n    @patch(\n        'examples.YOLO_data_augmentation.data_augmentation_albumentations.'\n        'DataAugmentation',\n    )\n    @patch('argparse.ArgumentParser.parse_args')\n    def test_main_exception(\n        self, mock_parse_args: MagicMock, MockDataAugmentation: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test main function when an exception occurs.\n\n        Args:\n            mock_parse_args (MagicMock):\n                Mocked argparse.ArgumentParser.parse_args function.\n            MockDataAugmentation (MagicMock): Mocked DataAugmentation class.\n        \"\"\"\n        # Mock command line arguments\n        mock_parse_args.return_value = argparse.Namespace(\n            train_path='./dataset_aug/train',\n            num_augmentations=10,\n            batch_size=5,\n        )\n\n        # Mock DataAugmentation class to raise an exception\n        mock_augmenter = MockDataAugmentation.return_value\n        mock_augmenter.augment_data.side_effect = Exception('Test exception')\n\n        with patch('builtins.print') as mock_print:\n            # Execute main function\n            main()\n\n            # Verify print was called with the correct error message\n            mock_print.assert_called_with('Error: Test exception')\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/src/lang_config_test.py", "content": "from __future__ import annotations\n\nimport os\nimport subprocess\nimport unittest\nfrom unittest.mock import patch\n\nfrom src.lang_config import LANGUAGES\nfrom src.lang_config import main\nfrom src.lang_config import Translator\n\n\nclass TestLangConfig(unittest.TestCase):\n    def setUp(self):\n        # Define the expected keys for all languages\n        self.expected_keys = {\n            'warning_people_in_controlled_area',\n            'warning_no_hardhat',\n            'warning_no_safety_vest',\n            'warning_close_to_machinery',\n            'no_warning',\n            'machinery',\n            'vehicle',\n            'helmet',\n            'person',\n            'no_helmet',\n            'vest',\n            'no_vest',\n            'mask',\n            'no_mask',\n            'cone',\n        }\n\n        # Define all supported languages\n        self.supported_languages = {\n            'zh-TW', 'zh-CN', 'en', 'fr', 'vi', 'id', 'th',\n        }\n        # Backup original translations for restoration in tearDown\n        self.original_translations = LANGUAGES.copy()\n\n    def tearDown(self):\n        # Restore original LANGUAGES dictionary\n        LANGUAGES.clear()\n        LANGUAGES.update(self.original_translations)\n        # Clear the cache to avoid side effects\n        Translator.translate_warning.cache_clear()\n\n    def test_all_languages_exist(self):\n        \"\"\"\n        Test that all the expected languages exist in the LANGUAGES dictionary.\n        \"\"\"\n        for lang in self.supported_languages:\n            self.assertIn(\n                lang, LANGUAGES,\n                f\"Language '{lang}' is missing in LANGUAGES.\",\n            )\n\n    def test_all_keys_exist_for_each_language(self):\n        \"\"\"\n        Test that all the expected keys exist for each language\n        in the LANGUAGES dictionary.\n        \"\"\"\n        for lang, translations in LANGUAGES.items():\n            for key in self.expected_keys:\n                self.assertIn(\n                    key, translations,\n                    f\"Key '{key}' is missing in language '{lang}'.\",\n                )\n\n    def test_no_extra_keys_for_each_language(self):\n        \"\"\"\n        Test that there are no extra keys in any language's translation\n        dictionary.\n        \"\"\"\n        for lang, translations in LANGUAGES.items():\n            extra_keys = set(translations.keys()) - self.expected_keys\n            self.assertFalse(\n                extra_keys,\n                f\"Language '{lang}' has unexpected keys: {extra_keys}\",\n            )\n\n    def test_translate_warning_with_unsupported_language(self):\n        \"\"\"\n        Test the translate_warning method with an unsupported language,\n        ensuring it defaults to English.\n        \"\"\"\n        warnings = [\n            'Warning: Someone is not wearing a hardhat!',\n            'Warning: 2 people have entered the controlled area!',\n            'Warning: Someone is too close to machinery!',\n        ]\n        unsupported_language = 'xx'  # This language code is not supported\n\n        translated_warnings = Translator.translate_warning(\n            tuple(warnings), unsupported_language,\n        )\n\n        # Since the language is unsupported, it should default to English\n        expected_warnings = [\n            'Warning: Someone is not wearing a hardhat!',\n            'Warning: 2 people have entered the controlled area!',\n            'Warning: Someone is too close to machinery!',\n        ]\n        self.assertEqual(translated_warnings, expected_warnings)\n\n    def test_translation_values_are_not_empty(self):\n        \"\"\"\n        Test that translation values for all keys\n        in all languages are not empty.\n        \"\"\"\n        for lang, translations in LANGUAGES.items():\n            for key in self.expected_keys:\n                value = translations.get(key, '')\n                self.assertTrue(\n                    value,\n                    f\"Value for key '{key}' in language '{lang}' is empty.\",\n                )\n\n    def test_translation_format(self):\n        \"\"\"\n        Test that all translations containing placeholders have correct format.\n        \"\"\"\n        # Placeholders expected in specific translations\n        placeholder_keys = {\n            'warning_people_in_controlled_area': '{count}',\n            'warning_close_to_machinery': '{label}',\n        }\n\n        for lang, translations in LANGUAGES.items():\n            for key, placeholder in placeholder_keys.items():\n                if key in translations:\n                    self.assertIn(\n                        placeholder,\n                        translations[key],\n                        (\n                            f\"Expected placeholder '{placeholder}' not in \"\n                            f\"key '{key}' for language '{lang}'.\"\n                        ),\n                    )\n\n    def test_placeholder_replacement_correctness(self):\n        \"\"\"\n        Test that placeholders in translations\n        can be correctly replaced without errors.\n        \"\"\"\n        test_values = {\n            'warning_people_in_controlled_area': {'count': '5'},\n            'warning_close_to_machinery': {'label': 'machine'},\n        }\n\n        for lang, translations in LANGUAGES.items():\n            for key, replacements in test_values.items():\n                if key in translations:\n                    try:\n                        translated = translations[key].format(**replacements)\n                        # Simple check to ensure placeholders are replaced\n                        for placeholder, value in replacements.items():\n                            self.assertIn(\n                                value, translated,\n                                f\"Placeholder '{placeholder}' not replaced \"\n                                f\"correctly in language '{lang}'.\",\n                            )\n\n                    except KeyError as e:\n                        self.fail(\n                            f\"Missing placeholder '{e.args[0]}' in language \"\n                            f\"'{lang}' for key '{key}'.\",\n                        )\n                    except Exception as e:\n                        self.fail(\n                            f\"Error formatting key '{key}' in language \"\n                            f\"'{lang}': {e}\",\n                        )\n\n    def test_translate_warning_no_safety_vest(self):\n        \"\"\"\n        Test the translation of the warning\n        for 'Someone is not wearing a safety vest!'.\n        \"\"\"\n        warnings = [\n            'Warning: Someone is not wearing a safety vest!!',\n        ]\n        language = 'zh-TW'  # Example of a non-English language\n\n        translated_warnings = Translator.translate_warning(\n            tuple(warnings), language,\n        )\n\n        expected_warnings = [\n            ': !',\n        ]\n        self.assertEqual(translated_warnings, expected_warnings)\n\n    def test_language_code_case_insensitivity(self):\n        \"\"\"\n        Test that language codes are case-insensitive.\n        \"\"\"\n        warnings = [\n            'Warning: Someone is not wearing a hardhat!',\n            'Warning: 2 people have entered the controlled area!',\n            'Warning: Someone is too close to machinery!',\n        ]\n        language = 'EN'  # Uppercase\n\n        try:\n            translated_warnings = Translator.translate_warning(\n                tuple(warnings), language.lower(),\n            )\n            expected_warnings = [\n                'Warning: Someone is not wearing a hardhat!',\n                'Warning: 2 people have entered the controlled area!',\n                'Warning: Someone is too close to machinery!',\n            ]\n            self.assertEqual(translated_warnings, expected_warnings)\n        except Exception as e:\n            self.fail(f\"Translation failed for uppercase language code: {e}\")\n\n    def test_specific_language_translation_correctness(self):\n        \"\"\"\n        Test that specific language translations are correct.\n        \"\"\"\n        test_cases = {\n            'zh-TW': {\n                'Warning: Someone is not wearing a hardhat!': (\n                    ': !'\n                ),\n                'Warning: 2 people have entered the controlled area!': (\n                    ': 2!'\n                ),\n                'Warning: Someone is too close to machinery!': (\n                    ': !'\n                ),\n            },\n            'fr': {\n                'Warning: Someone is not wearing a hardhat!': (\n                    \"Avertissement: Quelqu'un ne porte pas de casque!\"\n                ),\n                'Warning: 2 people have entered the controlled area!': (\n                    'Avertissement: 2 personnes sont entres '\n                    'dans la zone contrle!'\n                ),\n                'Warning: Someone is too close to machinery!': (\n                    'Avertissement: Quelquun est trop proche de machinerie!'\n                ),\n            },\n            # Add more languages and their expected translations if necessary\n        }\n\n        for lang, expected_translations in test_cases.items():\n            translated_warnings = Translator.translate_warning(\n                tuple(expected_translations.keys()), lang,\n            )\n            for original, expected in expected_translations.items():\n                self.assertIn(\n                    expected, translated_warnings,\n                    (\n                        f\"Expected translation '{expected}' for \"\n                        f\"'{original}' in language '{lang}' not found.\"\n                    ),\n                )\n\n    def test_translation_with_missing_placeholders(self):\n        \"\"\"\n        Test that translations with missing placeholders\n        are handled gracefully.\n        \"\"\"\n        warnings = [\n            'Warning: Someone is not wearing a hardhat!',\n            'Warning: 2 people have entered the controlled area!',\n            'Warning: Someone is too close to machinery!',\n        ]\n        language = 'en'\n\n        # Manually remove a placeholder to\n        # simulate a missing placeholder scenario\n        LANGUAGES[language][\n            'warning_people_in_controlled_area'\n        ] = 'Warning: {count} people have entered the controlled area!'\n\n        try:\n            translated_warnings = Translator.translate_warning(\n                tuple(warnings), language,\n            )\n            expected_warnings = [\n                'Warning: Someone is not wearing a hardhat!',\n                'Warning: 2 people have entered the controlled area!',\n                'Warning: Someone is too close to machinery!',\n            ]\n            self.assertEqual(translated_warnings, expected_warnings)\n        except Exception as e:\n            self.fail(f\"Translation failed with all placeholders present: {e}\")\n\n        # Clear the cache to ensure changes to LANGUAGES are picked up\n        Translator.translate_warning.cache_clear()\n\n        # Now, remove a placeholder and expect it to handle gracefully\n        # Missing {count}\n        LANGUAGES[language]['warning_people_in_controlled_area'] = (\n            'Warning: people have entered the controlled area!'\n        )\n\n        try:\n            translated_warnings = Translator.translate_warning(\n                tuple(warnings), language,\n            )\n            expected_warnings = [\n                'Warning: Someone is not wearing a hardhat!',\n                'Warning: people have entered the controlled area!',\n                'Warning: Someone is too close to machinery!',\n            ]\n            self.assertEqual(translated_warnings, expected_warnings)\n        except Exception as e:\n            self.fail(f\"Translation failed with missing placeholders: {e}\")\n\n    def test_translate_warning_no_match(self):\n        \"\"\"\n        Test that if a warning message does not match any predefined warning,\n        the original message is returned.\n        \"\"\"\n        # Unmatched warning message\n        warnings = [\n            'Warning: This is an unmatched warning!',\n        ]\n        language = 'zh-TW'  # Example of a non-English language\n\n        translated_warnings = Translator.translate_warning(\n            tuple(warnings), language,\n        )\n\n        # Since the warning doesn't match any known pattern,\n        # it should be kept as-is\n        expected_warnings = [\n            'Warning: This is an unmatched warning!',\n        ]\n        self.assertEqual(translated_warnings, expected_warnings)\n\n    @patch('builtins.print')\n    def test_main_function(self, mock_print):\n        \"\"\"\n        Test the main function for proper translation and output.\n        \"\"\"\n        # Execute main function\n        main()\n\n        # Check that the output was printed\n        mock_print.assert_any_call(\n            'Original Warnings:', [\n                'Warning: Someone is not wearing a hardhat!',\n                'Warning: 2 people have entered the controlled area!',\n                'Warning: Someone is too close to machinery!',\n            ],\n        )\n        mock_print.assert_any_call(\n            'Translated Warnings:', [\n                ': !',\n                ': 2!',\n                ': !',\n            ],\n        )\n\n    def test_main_as_script(self):\n        \"\"\"\n        Test executing the script directly with `if __name__ == '__main__'`.\n        This will cover the main() function when invoked as standalone script.\n        \"\"\"\n        script_path = os.path.join(\n            os.path.dirname(\n                __file__,\n            ), '../../src/lang_config.py',\n        )\n\n        result = subprocess.run(\n            ['python', script_path],\n            capture_output=True, text=True,\n        )\n\n        # Assert that the script runs without errors\n        self.assertEqual(\n            result.returncode, 0,\n            'Script exited with a non-zero status.',\n        )\n\n        # You can also verify the expected output here if needed\n        self.assertIn('Original Warnings:', result.stdout)\n        self.assertIn('Translated Warnings:', result.stdout)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/src/drawing_manager_test.py", "content": "from __future__ import annotations\n\nimport shutil\nimport unittest\nfrom pathlib import Path\nfrom unittest.mock import patch\n\nimport numpy as np\nfrom PIL import ImageFont\nfrom shapely.geometry import Polygon\n\nfrom src.drawing_manager import DrawingManager\nfrom src.drawing_manager import main\n\n\nclass TestDrawingManager(unittest.TestCase):\n    \"\"\"\n    Unit tests for the DrawingManager class.\n    \"\"\"\n\n    def setUp(self) -> None:\n        \"\"\"\n        Set up the initial state needed for tests.\n        \"\"\"\n        self.drawer: DrawingManager = DrawingManager()\n        self.frame: np.ndarray = np.zeros((480, 640, 3), dtype=np.uint8)\n        self.datas: list[list[float]] = [\n            [50, 50, 150, 150, 0.95, 0],    # Hardhat\n            [200, 200, 300, 300, 0.85, 5],  # Person\n            [400, 400, 500, 500, 0.75, 9],  # Vehicle\n            [100, 100, 120, 120, 0.9, 6],   # Safety Cone\n            [250, 250, 270, 270, 0.8, 6],   # Safety Cone\n            [450, 450, 470, 470, 0.7, 6],   # Safety Cone\n            [500, 200, 520, 220, 0.7, 6],   # Safety Cone\n            [150, 400, 170, 420, 0.7, 6],   # Safety Cone\n        ]\n        self.polygons: list[Polygon] = [\n            Polygon([\n                (100, 100), (250, 250), (450, 450),\n                (500, 200), (150, 400),\n            ]).convex_hull,\n        ]\n\n    def tearDown(self) -> None:\n        \"\"\"\n        Clean up after each test.\n        \"\"\"\n        del self.drawer\n        del self.frame\n        del self.datas\n        del self.polygons\n\n        # Remove the output directory\n        output_dir: Path = Path('detected_frames/test_output')\n        if output_dir.exists() and output_dir.is_dir():\n            shutil.rmtree(output_dir)\n\n        root_dir: Path = Path('detected_frames')\n        if root_dir.exists() and root_dir.is_dir():\n            shutil.rmtree(root_dir)\n\n    def test_get_font_cache(self) -> None:\n        \"\"\"\n        Test the font caching mechanism in the get_font method.\n        \"\"\"\n        # First call to get_font should load and cache the font\n        font_first_call = self.drawer.get_font('en')\n\n        # Second call to get_font with the same language\n        # should use the cached font\n        font_second_call = self.drawer.get_font('en')\n\n        # Assert that the returned font objects are the same,\n        # indicating caching\n        self.assertIs(\n            font_first_call, font_second_call,\n            'Font should be retrieved from cache',\n        )\n\n        # Check if the font is indeed cached\n        font_path = 'assets/fonts/NotoSansTC-VariableFont_wght.ttf'\n        self.assertIn(\n            font_path, self.drawer.font_cache,\n            'Font should be cached after first retrieval',\n        )\n\n    def test_get_font_fallback_to_default(self) -> None:\n        \"\"\"\n        Test the fallback behaviour when loading a font fails.\n\n        Raises:\n            AssertionError: If the fallback behaviour is incorrect.\n        \"\"\"\n        # Force reset to ensure the fallback logic is triggered\n        DrawingManager.default_font = None\n\n        # Save the reference to the original `truetype` function\n        # from PIL.ImageFont\n        original_truetype = ImageFont.truetype\n\n        # Define a custom side effect\n        # to simulate a loading failure for specific paths\n        def my_side_effect(font_path: str, size: int, *args, **kwargs):\n            \"\"\"\n            Simulates a failure when attempting to load a specific custom font.\n\n            Args:\n                font_path (str): The path to the font file.\n                size (int): The font size to be loaded.\n\n            Returns:\n                Any: The font object if loading succeeds.\n\n            Raises:\n                OSError: If the font path matches a specific failing condition.\n            \"\"\"\n            # Simulate an error for a specific font path\n            if font_path == 'assets/fonts/NotoSansTC-VariableFont_wght.ttf':\n                raise OSError('Simulated font load error')\n\n            # For all other paths (e.g., those used by `load_default`),\n            # call the original `truetype`\n            return original_truetype(font_path, size, *args, **kwargs)\n\n        # Patch `PIL.ImageFont.truetype` with the custom side effect\n        with patch('PIL.ImageFont.truetype', side_effect=my_side_effect):\n            # Attempt to retrieve the font,\n            # expecting a fallback to the default font\n            fallback_font = self.drawer.get_font('en')\n\n            # Ensure that the default font has been set\n            self.assertIsNotNone(\n                DrawingManager.default_font,\n                'Default font should be initialised after fallback',\n            )\n\n            # Verify that the fallback font is the same as the default font\n            self.assertIs(\n                fallback_font,\n                DrawingManager.default_font,\n                'Should fall back to the default font when loading fails',\n            )\n\n    def test_draw_detections_on_frame_with_thai_language(self) -> None:\n        \"\"\"\n        Test drawing detections on a frame with Thai language labels.\n        \"\"\"\n        # Example detection data\n        datas = [\n            [50, 50, 150, 150, 0.95, 0],  # Hardhat\n            [200, 200, 300, 300, 0.85, 5],  # Person\n            [400, 400, 500, 500, 0.75, 9],  # Vehicle\n        ]\n\n        # Example polygon for safety cones\n        polygons = [\n            Polygon(\n                [(100, 100), (250, 250), (450, 450), (500, 200), (150, 400)],\n            ).convex_hull,\n        ]\n\n        # Draw detections on frame in Thai language\n        frame_with_detections = self.drawer.draw_detections_on_frame(\n            self.frame.copy(), polygons, datas, language='th',\n        )\n\n        # Check if the frame returned is a numpy array\n        self.assertIsInstance(frame_with_detections, np.ndarray)\n\n        # Check if the frame dimensions are the same\n        self.assertEqual(frame_with_detections.shape, self.frame.shape)\n\n    def test_draw_detections_on_frame(self) -> None:\n        \"\"\"\n        Test drawing detections on a frame.\n        \"\"\"\n        frame_with_detections = self.drawer.draw_detections_on_frame(\n            self.frame.copy(), self.polygons, self.datas,\n        )\n\n        # Check if the frame returned is a numpy array\n        self.assertIsInstance(frame_with_detections, np.ndarray)\n\n        # Check if the frame dimensions are the same\n        self.assertEqual(frame_with_detections.shape, self.frame.shape)\n\n    def test_draw_detections_on_frame_with_invalid_label(self) -> None:\n        \"\"\"\n        Test drawing on a frame with an invalid label.\n        \"\"\"\n        invalid_data = [\n            [10, 10, 50, 50, 0.99, 999],  # Invalid label ID\n        ]\n        frame_with_detections = self.drawer.draw_detections_on_frame(\n            self.frame.copy(), [], invalid_data,\n        )\n\n        # Check if the frame returned is a numpy array\n        self.assertIsInstance(frame_with_detections, np.ndarray)\n\n        # Check if the frame dimensions are the same\n        self.assertEqual(frame_with_detections.shape, self.frame.shape)\n\n    def test_draw_detections_on_frame_with_no_detections(self) -> None:\n        \"\"\"\n        Test drawing on a frame with no detections.\n        \"\"\"\n        frame_with_detections = self.drawer.draw_detections_on_frame(\n            self.frame.copy(), [], [],\n        )\n\n        # Check if the frame returned is a numpy array\n        self.assertIsInstance(frame_with_detections, np.ndarray)\n\n        # Check if the frame dimensions are the same\n        self.assertEqual(frame_with_detections.shape, self.frame.shape)\n\n        # Check if no objects are drawn (frame should be unchanged)\n        np.testing.assert_array_equal(\n            frame_with_detections, self.frame,\n            'Frame should not be changed with no detections',\n        )\n\n    def test_draw_polygons(self) -> None:\n        \"\"\"\n        Test drawing polygons on the frame.\n        \"\"\"\n        frame_with_polygons = self.drawer.draw_polygons(\n            self.frame.copy(), self.polygons,\n        )\n\n        # Check if the frame returned is a numpy array\n        self.assertIsInstance(frame_with_polygons, np.ndarray)\n\n        # Check if the frame dimensions are the same\n        self.assertEqual(frame_with_polygons.shape, self.frame.shape)\n\n    def test_save_frame(self) -> None:\n        \"\"\"\n        Test saving a frame to disk.\n        \"\"\"\n        frame_bytes: bytearray = bytearray(\n            np.zeros((480, 640, 3), dtype=np.uint8).tobytes(),\n        )\n        output_filename: str = 'test_frame'\n\n        with patch('builtins.open', unittest.mock.mock_open()) as mock_file:\n            self.drawer.save_frame(frame_bytes, output_filename)\n\n            # Construct the expected file path\n            expected_file: Path = Path('detected_frames/test_frame.png')\n\n            # Assert calls to open and write\n            mock_file.assert_called_once_with(expected_file, 'wb')\n            mock_file().write.assert_called_once_with(frame_bytes)\n\n    def test_main(self) -> None:\n        \"\"\"\n        Test the main function to ensure the complete process is covered.\n        \"\"\"\n        with patch('pathlib.Path.mkdir') as mock_mkdir, \\\n                patch('builtins.open', unittest.mock.mock_open()) as mock_file:\n\n            main()\n\n            # Assert the directory was created\n            mock_mkdir.assert_called_once_with(parents=True, exist_ok=True)\n\n            # Construct the expected file path\n            expected_file: Path = Path('detected_frames/frame_001.png')\n\n            # Assert calls to open and write\n            mock_file.assert_called_once_with(expected_file, 'wb')\n            mock_file().write.assert_called_once()\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/examples/YOLO_server_api/backend/detection_test.py", "content": "from __future__ import annotations\n\nimport unittest\nfrom io import BytesIO\nfrom unittest.mock import AsyncMock\nfrom unittest.mock import MagicMock\nfrom unittest.mock import patch\n\nimport numpy as np\nfrom PIL import Image\n\nfrom examples.YOLO_server_api.backend.detection import calculate_area\nfrom examples.YOLO_server_api.backend.detection import calculate_intersection\nfrom examples.YOLO_server_api.backend.detection import calculate_overlap\nfrom examples.YOLO_server_api.backend.detection import check_containment\nfrom examples.YOLO_server_api.backend.detection import compile_detection_data\nfrom examples.YOLO_server_api.backend.detection import convert_to_image\nfrom examples.YOLO_server_api.backend.detection import find_contained_indices\nfrom examples.YOLO_server_api.backend.detection import find_contained_labels\nfrom examples.YOLO_server_api.backend.detection import find_overlapping_indices\nfrom examples.YOLO_server_api.backend.detection import find_overlaps\nfrom examples.YOLO_server_api.backend.detection import get_category_indices\nfrom examples.YOLO_server_api.backend.detection import get_prediction_result\nfrom examples.YOLO_server_api.backend.detection import is_contained\nfrom examples.YOLO_server_api.backend.detection import process_labels\nfrom examples.YOLO_server_api.backend.detection import (\n    remove_completely_contained_labels,\n)\nfrom examples.YOLO_server_api.backend.detection import (\n    remove_overlapping_labels,\n)\n\n\nclass TestDetection(unittest.IsolatedAsyncioTestCase):\n    \"\"\"\n    Test cases for the detection functionalities.\n    \"\"\"\n\n    def setUp(self) -> None:\n        \"\"\"\n        Sets up test environment by creating image data.\n        \"\"\"\n        # Create a simple JPEG image\n        img = Image.new('RGB', (100, 100), color='white')\n        buf = BytesIO()\n        img.save(buf, format='JPEG')\n        self.image_data = buf.getvalue()\n\n    @patch('examples.YOLO_server_api.backend.detection.np.frombuffer')\n    @patch('examples.YOLO_server_api.backend.detection.cv2.imdecode')\n    async def test_convert_to_image(\n        self,\n        mock_imdecode: MagicMock,\n        mock_frombuffer: MagicMock,\n    ) -> None:\n        \"\"\"\n        Tests the conversion of byte data to an image using mocked numpy\n        and OpenCV methods.\n\n        Args:\n            mock_imdecode (MagicMock): Mocked image decoding method.\n            mock_frombuffer (MagicMock): Mocked buffer conversion method.\n        \"\"\"\n        mock_frombuffer.return_value = MagicMock()\n        mock_imdecode.return_value = MagicMock()\n\n        img = await convert_to_image(self.image_data)\n\n        mock_frombuffer.assert_called_once_with(self.image_data, np.uint8)\n        mock_imdecode.assert_called_once()\n        self.assertIsNotNone(img)\n\n    @patch('examples.YOLO_server_api.backend.detection.get_sliced_prediction')\n    async def test_get_prediction_result(\n        self, mock_get_sliced_prediction: MagicMock,\n    ) -> None:\n        \"\"\"\n        Tests obtaining a prediction result by mocking the prediction method.\n\n        Args:\n            mock_get_sliced_prediction (MagicMock): Mocked sliced\n                prediction method.\n        \"\"\"\n        mock_get_sliced_prediction.return_value = MagicMock(\n            object_prediction_list=[],\n        )\n        img = MagicMock()\n        model = MagicMock()\n\n        result = await get_prediction_result(img, model)\n\n        mock_get_sliced_prediction.assert_called_once_with(\n            img, model, slice_height=370, slice_width=370,\n            overlap_height_ratio=0.3, overlap_width_ratio=0.3,\n        )\n        self.assertIsNotNone(result)\n\n    def test_compile_detection_data(self) -> None:\n        \"\"\"\n        Tests compiling prediction data into structured format.\n        \"\"\"\n        mock_object_prediction = MagicMock()\n        mock_object_prediction.category.id = 1\n        mock_object_prediction.bbox.to_voc_bbox.return_value = [10, 20, 30, 40]\n        mock_object_prediction.score.value = 0.9\n\n        mock_result = MagicMock()\n        mock_result.object_prediction_list = [mock_object_prediction]\n\n        result = compile_detection_data(mock_result)\n        self.assertEqual(len(result), 1)\n        self.assertEqual(result[0], [10, 20, 30, 40, 0.9, 1])\n\n    @patch(\n        'examples.YOLO_server_api.backend.detection.remove_overlapping_labels',\n        new_callable=AsyncMock,\n    )\n    @patch(\n        'examples.YOLO_server_api.backend.detection.'\n        'remove_completely_contained_labels',\n        new_callable=AsyncMock,\n    )\n    async def test_process_labels(\n        self,\n        mock_remove_completely_contained_labels: AsyncMock,\n        mock_remove_overlapping_labels: AsyncMock,\n    ) -> None:\n        \"\"\"\n        Tests label processing by applying overlapping and containment checks.\n\n        Args:\n            mock_remove_completely_contained_labels (AsyncMock): Mocked\n                containment removal method.\n            mock_remove_overlapping_labels (AsyncMock): Mocked overlapping\n                removal method.\n        \"\"\"\n        mock_remove_overlapping_labels.side_effect = lambda datas: datas\n        mock_remove_completely_contained_labels.side_effect = (\n            lambda datas: datas\n        )\n\n        datas = [[10, 20, 30, 40, 0.9, 1]]\n        result = await process_labels(datas)\n\n        self.assertEqual(result, datas)\n        mock_remove_overlapping_labels.assert_called()\n        mock_remove_completely_contained_labels.assert_called()\n\n    async def test_remove_overlapping_labels(self) -> None:\n        \"\"\"\n        Tests removal of overlapping labels.\n        \"\"\"\n        datas = [[0, 0, 50, 50, 0.9, 1], [10, 10, 60, 60, 0.85, 2]]\n        result = await remove_overlapping_labels(datas)\n        self.assertIsInstance(result, list)\n\n    async def test_remove_completely_contained_labels(self) -> None:\n        \"\"\"\n        Tests removal of completely contained labels.\n        \"\"\"\n        datas = [[0, 0, 50, 50, 0.9, 1], [10, 10, 40, 40, 0.85, 2]]\n        result = await remove_completely_contained_labels(datas)\n        self.assertIsInstance(result, list)\n\n    def test_get_category_indices(self) -> None:\n        \"\"\"\n        Tests retrieval of category indices for different labels.\n        \"\"\"\n        datas = [[10, 20, 30, 40, 0.9, 0], [50, 60, 70, 80, 0.85, 7]]\n        indices = get_category_indices(datas)\n        self.assertIsInstance(indices, dict)\n\n    def test_calculate_overlap(self) -> None:\n        \"\"\"\n        Tests calculation of overlapping area between two bounding boxes.\n        \"\"\"\n        bbox1 = [0, 0, 50, 50]\n        bbox2 = [25, 25, 75, 75]\n        overlap = calculate_overlap(bbox1, bbox2)\n        self.assertGreater(overlap, 0)\n\n    def test_calculate_intersection(self) -> None:\n        \"\"\"\n        Tests calculation of intersection area between two bounding boxes.\n        \"\"\"\n        bbox1 = [0, 0, 50, 50]\n        bbox2 = [25, 25, 75, 75]\n        intersection = calculate_intersection(bbox1, bbox2)\n        self.assertEqual(intersection, (25, 25, 50, 50))\n\n    def test_calculate_area(self) -> None:\n        \"\"\"\n        Tests calculation of area for a bounding box.\n        \"\"\"\n        x1, y1, x2, y2 = 0, 0, 50, 50\n        area = calculate_area(x1, y1, x2, y2)\n        self.assertEqual(area, 2601)\n\n    def test_is_contained(self) -> None:\n        \"\"\"\n        Tests if one bounding box is contained within another.\n        \"\"\"\n        inner_bbox = [10, 10, 30, 30]\n        outer_bbox = [0, 0, 50, 50]\n        result = is_contained(inner_bbox, outer_bbox)\n        self.assertTrue(result)\n\n    async def test_find_overlaps(self) -> None:\n        \"\"\"\n        Tests identification of overlapping bounding boxes.\n        \"\"\"\n        indices1 = [0]\n        indices2 = [1]\n        datas = [[0, 0, 50, 50, 0.9, 1], [25, 25, 75, 75, 0.85, 2]]\n        result = await find_overlaps(indices1, indices2, datas, 0.5)\n        self.assertIsInstance(result, set)\n\n    async def test_find_contained_labels(self) -> None:\n        \"\"\"\n        Tests identification of labels that are contained within others.\n        \"\"\"\n        indices1 = [0]\n        indices2 = [1]\n        datas = [[0, 0, 50, 50, 0.9, 1], [10, 10, 40, 40, 0.85, 2]]\n        result = await find_contained_labels(indices1, indices2, datas)\n        self.assertIsInstance(result, set)\n\n    async def test_find_overlapping_indices(self) -> None:\n        \"\"\"\n        Tests finding indices of overlapping bounding boxes.\n        \"\"\"\n        index1 = 0\n        indices2 = [1]\n        datas = [[0, 0, 50, 50, 0.9, 1], [25, 25, 75, 75, 0.85, 2]]\n        result = await find_overlapping_indices(index1, indices2, datas, 0.5)\n        self.assertIsInstance(result, set)\n\n    async def test_find_contained_indices(self) -> None:\n        \"\"\"\n        Tests finding indices of contained bounding boxes.\n        \"\"\"\n        index1 = 0\n        indices2 = [1]\n        datas = [[0, 0, 50, 50, 0.9, 1], [10, 10, 40, 40, 0.85, 2]]\n        result = await find_contained_indices(index1, indices2, datas)\n        self.assertIsInstance(result, set)\n\n    async def test_check_containment(self) -> None:\n        \"\"\"\n        Tests checking if one bounding box is contained within another.\n        \"\"\"\n        index1 = 0\n        index2 = 1\n        datas = [[0, 0, 50, 50, 0.9, 1], [10, 10, 40, 40, 0.85, 2]]\n        result = await check_containment(index1, index2, datas)\n        self.assertIsInstance(result, set)\n\n    async def test_remove_completely_contained_labels_line_340(self) -> None:\n        \"\"\"\n        Test the `remove_completely_contained_labels` function to ensure it\n        removes labels that are completely contained within another label.\n        \"\"\"\n        # Test data representing bounding boxes and their corresponding labels\n        datas = [\n            [50, 50, 150, 150, 0.9, 0],   # 'hardhat' label\n            # 'no_hardhat' label, fully contained within 'hardhat'\n            [70, 70, 130, 130, 0.8, 2],\n            [200, 200, 300, 300, 0.85, 7],  # 'safety_vest' label\n            # 'no_safety_vest' label, fully contained within 'safety_vest'\n            [220, 220, 280, 280, 0.7, 4],\n        ]\n\n        # Call the function and pass a copy of the data\n        result = await remove_completely_contained_labels(datas.copy())\n\n        # Assert the length of the resulting list is as expected\n        self.assertEqual(len(result), 2)\n\n        # Extract remaining labels and verify they match the expected values\n        remaining_labels = [d[5] for d in result]\n        self.assertListEqual(remaining_labels, [0, 7])\n\n    async def test_check_containment_elif_condition(self) -> None:\n        \"\"\"\n        Test the `check_containment` function's `elif` condition to ensure\n        that the appropriate lines of code are covered.\n\n        This test simulates a scenario where one bounding box\n        (`index1`) is completely contained within another bounding box\n        (`index2`). The function should correctly identify that\n        `index1` is contained and return the set containing `index1`.\n\n        Args:\n            None\n\n        Returns:\n            None\n        \"\"\"\n        # Define the indices of the bounding boxes to test\n        index1: int = 0\n        index2: int = 1\n\n        # Define the test data representing bounding boxes\n        datas: list[list[float | int]] = [\n            [70, 70, 130, 130, 0.9, 1],  # Bounding box for index1\n            [50, 50, 150, 150, 0.85, 2],  # Bounding box for index2\n        ]\n\n        # Call the `check_containment` function with the test data\n        result: set[int] = await check_containment(index1, index2, datas)\n\n        # Assert that the result matches the expected output\n        # In this case, `index1` is contained within `index2`\n        self.assertSetEqual(\n            result, {0},\n            'The function did not correctly identify containment.',\n        )\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/examples/YOLO_data_augmentation/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/src/live_stream_detection_test.py", "content": "from __future__ import annotations\n\nimport os\nimport sys\nimport unittest\nfrom typing import Any\nfrom unittest.mock import AsyncMock\nfrom unittest.mock import MagicMock\nfrom unittest.mock import patch\n\nimport aiohttp\nimport cv2\nimport numpy as np\nimport yarl\nfrom multidict import CIMultiDict\nfrom multidict import CIMultiDictProxy\n\nfrom src.live_stream_detection import LiveStreamDetector\nfrom src.live_stream_detection import main\n\n\nclass TestLiveStreamDetector(unittest.IsolatedAsyncioTestCase):\n    \"\"\"\n    Unit tests for the LiveStreamDetector class methods.\n    \"\"\"\n\n    def setUp(self) -> None:\n        \"\"\"\n        Set up the LiveStreamDetector instance for tests.\n        \"\"\"\n        # Mock environment variables to avoid missing credentials\n        patcher_env = patch.dict(\n            os.environ,\n            {'API_USERNAME': 'test_user', 'API_PASSWORD': 'test_pass'},\n        )\n        patcher_env.start()\n        self.addCleanup(patcher_env.stop)\n\n        # Initialise default detector parameters for testing\n        self.api_url: str = 'http://mocked-api.com'\n        self.model_key: str = 'yolo11n'\n        self.output_folder: str = 'test_output'\n        self.detect_with_server: bool = False\n\n        # Create an instance of LiveStreamDetector for use in tests\n        self.detector: LiveStreamDetector = LiveStreamDetector(\n            api_url=self.api_url,\n            model_key=self.model_key,\n            output_folder=self.output_folder,\n            detect_with_server=self.detect_with_server,\n        )\n\n    ########################################################################\n    # Initialisation tests\n    ########################################################################\n\n    def test_initialisation(self) -> None:\n        \"\"\"\n        Test the initialisation of the LiveStreamDetector instance.\n        \"\"\"\n        detector = LiveStreamDetector(\n            api_url=self.api_url,\n            model_key=self.model_key,\n            output_folder=self.output_folder,\n            detect_with_server=self.detect_with_server,\n        )\n\n        # Assert initialisation values\n        self.assertEqual(detector.api_url, self.api_url)\n        self.assertEqual(detector.model_key, self.model_key)\n        self.assertEqual(detector.output_folder, self.output_folder)\n        self.assertEqual(detector.detect_with_server, self.detect_with_server)\n        self.assertEqual(detector.shared_token, {'access_token': ''})\n\n    def test_initialisation_with_shared_token(self) -> None:\n        \"\"\"\n        Test initialisation when shared_token is provided.\n        \"\"\"\n        shared_token = {\n            'access_token': 'test_token',\n        }\n\n        # Initialise with a shared token\n        detector = LiveStreamDetector(\n            api_url=self.api_url,\n            model_key=self.model_key,\n            output_folder=self.output_folder,\n            detect_with_server=self.detect_with_server,\n            shared_token=shared_token,\n        )\n\n        # Validate that the shared token is set correctly\n        self.assertEqual(detector.shared_token, shared_token)\n\n    def test_shared_lock(self):\n        \"\"\"\n        Test shared lock acquire and release methods.\n        \"\"\"\n        # Mock a shared lock for testing\n        shared_lock = MagicMock()\n        shared_lock.acquire = MagicMock()\n        shared_lock.release = MagicMock()\n\n        # Initialise the detector with a shared lock\n        detector = LiveStreamDetector(\n            api_url=self.api_url,\n            model_key=self.model_key,\n            shared_lock=shared_lock,\n        )\n\n        # Test acquiring the lock\n        detector.acquire_shared_lock()\n        shared_lock.acquire.assert_called_once()\n\n        # Test releasing the lock\n        detector.release_shared_lock()\n        shared_lock.release.assert_called_once()\n\n    ########################################################################\n    # Authentication tests\n    ########################################################################\n\n    @patch('aiohttp.ClientSession.post')\n    async def test_authenticate_skip_if_token_exists(\n        self,\n        mock_post: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test authenticate skips re-authentication if token exists.\n        \"\"\"\n        # Set an existing token to bypass authentication\n        self.detector.shared_token['access_token'] = 'existing_token'\n\n        # Call authenticate and ensure no network requests are made\n        await self.detector.authenticate()\n        mock_post.assert_not_called()\n\n    @patch('aiohttp.ClientSession.post')\n    async def test_authenticate(\n        self,\n        mock_post: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test successful authentication flow.\n        \"\"\"\n        mock_response = MagicMock()\n        mock_response.raise_for_status = MagicMock()\n        mock_response.json = AsyncMock(\n            return_value={'access_token': 'fake_token'},\n        )\n        mock_post.return_value.__aenter__.return_value = mock_response\n\n        await self.detector.authenticate()\n        self.assertEqual(\n            self.detector.shared_token['access_token'], 'fake_token',\n        )\n\n    @patch('aiohttp.ClientSession.post')\n    async def test_authenticate_missing_credentials(\n        self,\n        mock_post: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test the authenticate method\n        when credentials are missing in environment variables.\n        \"\"\"\n        with patch.dict(os.environ, {}, clear=True):\n            # API_USERNAME / API_PASSWORD not exist\n            with self.assertRaises(ValueError) as ctx:\n                await self.detector.authenticate()\n            self.assertIn(\n                'Missing API_USERNAME or API_PASSWORD',\n                str(ctx.exception),\n            )\n\n    @patch('aiohttp.ClientSession.post')\n    async def test_authenticate_raises_for_status(\n        self,\n        mock_post: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test the authenticate method raising ClientResponseError\n        if response.raise_for_status fails.\n        \"\"\"\n        # Create typed headers and mock RequestInfo\n        headers: CIMultiDict[str] = CIMultiDict()\n        mock_request_info = aiohttp.RequestInfo(\n            url=yarl.URL('http://mock.com/auth'),\n            method='POST',\n            headers=CIMultiDictProxy(headers),\n            real_url=yarl.URL('http://mock.com/auth'),\n        )\n\n        # Mock a response with status 401\n        mock_response = MagicMock()\n        mock_response.raise_for_status.side_effect = (\n            aiohttp.ClientResponseError(\n                request_info=mock_request_info,\n                history=(),\n                status=401,\n                message='Unauthorized',\n            )\n        )\n        mock_post.return_value.__aenter__.return_value = mock_response\n\n        # Ensure a ClientResponseError is raised by the authenticate call\n        with self.assertRaises(aiohttp.ClientResponseError):\n            await self.detector.authenticate()\n\n    @patch('aiohttp.ClientSession.post')\n    async def test_authenticate_raises_key_error_if_no_access_token(\n        self,\n        mock_post: MagicMock,\n    ) -> None:\n        \"\"\"\n        If the server returns a JSON response without 'access_token',\n        a KeyError should occur.\n        \"\"\"\n        mock_response: MagicMock = MagicMock()\n        mock_response.raise_for_status = MagicMock()\n\n        # Return an empty JSON response\n        mock_response.json = AsyncMock(return_value={})\n        mock_post.return_value.__aenter__.return_value = mock_response\n\n        # Expect a KeyError to be raised\n        with self.assertRaises(KeyError):\n            await self.detector.authenticate()\n\n    ########################################################################\n    # Detection tests\n    ########################################################################\n\n    @patch('cv2.imencode', return_value=(False, None))\n    async def test_generate_detections_cloud_encode_fail(\n        self,\n        mock_imencode: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test generate_detections_cloud raises ValueError\n        if frame encoding fails.\n        \"\"\"\n        # Simulate a frame encoding failure\n        frame = np.zeros((480, 640, 3), dtype=np.uint8)\n        with self.assertRaises(ValueError) as ctx:\n            await self.detector.generate_detections_cloud(frame)\n        self.assertIn(\n            'Failed to encode frame as PNG bytes.',\n            str(ctx.exception),\n        )\n\n    @patch('aiohttp.ClientSession.post')\n    async def test_generate_detections_cloud(\n        self,\n        mock_post: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test cloud detection generation.\n        \"\"\"\n        frame: np.ndarray = np.zeros((480, 640, 3), dtype=np.uint8)\n\n        # Simulate the token response\n        mock_token_response = MagicMock()\n        mock_token_response.raise_for_status = MagicMock()\n        mock_token_response.json = AsyncMock(\n            return_value={'access_token': 'fake_token'},\n        )\n\n        # Simulate the detection response\n        mock_detection_response = MagicMock()\n        mock_detection_response.raise_for_status = MagicMock()\n        mock_detection_response.json = AsyncMock(\n            return_value=[\n                [10, 10, 50, 50, 0.9, 0],\n                [20, 20, 60, 60, 0.8, 1],\n            ],\n        )\n\n        # Simulate the two responses from the server\n        mock_post.return_value.__aenter__.side_effect = [\n            mock_token_response,       # First call: /token\n            mock_detection_response,   # Second call: /detect\n        ]\n\n        datas: list[list[Any]] = (\n            await self.detector.generate_detections_cloud(frame)\n        )\n\n        # Validate the response from the server\n        self.assertIsInstance(datas, list)\n        self.assertEqual(len(datas), 2)\n\n        # Validate the structure and types of the detection data\n        for data in datas:\n            self.assertIsInstance(data, list)\n            self.assertEqual(len(data), 6)\n            self.assertIsInstance(data[0], int)\n            self.assertIsInstance(data[1], int)\n            self.assertIsInstance(data[2], int)\n            self.assertIsInstance(data[3], int)\n            self.assertIsInstance(data[4], float)\n            self.assertIsInstance(data[5], int)\n\n    @patch('aiohttp.ClientSession.post')\n    async def test_generate_detections_cloud_retry_on_token_expiry(\n        self,\n        mock_post: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test generate_detections_cloud retries on token expiry.\n        \"\"\"\n        # Assume the token already exists, so authenticate() is skipped\n        self.detector.shared_token['access_token'] = 'old_token'\n\n        frame = np.zeros((480, 640, 3), dtype=np.uint8)\n\n        # Simulate the first detection response with 401 Unauthorised\n        mock_unauthorized_detection = MagicMock()\n        mock_unauthorized_detection.status = 401\n        headers: CIMultiDict[str] = CIMultiDict()\n        mock_unauthorized_detection.raise_for_status.side_effect = (\n            aiohttp.ClientResponseError(\n                request_info=aiohttp.RequestInfo(\n                    url=yarl.URL('http://mock.com/detect'),\n                    method='POST',\n                    headers=CIMultiDictProxy(headers),\n                ),\n                history=(),\n                status=401,\n                message='Unauthorized',\n            )\n        )\n\n        # Simulate the token response with the new token\n        mock_token_response = MagicMock()\n        mock_token_response.status = 200\n        mock_token_response.raise_for_status = MagicMock()\n        mock_token_response.json = AsyncMock(\n            return_value={'access_token': 'new_token'},\n        )\n\n        # Simulate the successful detection response\n        mock_success_detection = MagicMock()\n        mock_success_detection.status = 200\n        mock_success_detection.raise_for_status = MagicMock()\n        mock_success_detection.json = AsyncMock(\n            return_value=[\n                [10, 10, 50, 50, 0.9, 0],\n                [20, 20, 60, 60, 0.8, 1],\n            ],\n        )\n\n        mock_post.return_value.__aenter__.side_effect = [\n            # First detection attempt: 401 Unauthorised\n            mock_unauthorized_detection,\n            # Token refresh response\n            mock_token_response,\n            # Second detection attempt: Success\n            mock_success_detection,\n        ]\n\n        datas = await self.detector.generate_detections_cloud(frame)\n        self.assertEqual(len(datas), 2)\n        self.assertEqual(\n            self.detector.shared_token['access_token'], 'new_token',\n        )\n\n        # Validate the structure and types of the detection data\n        for data in datas:\n            self.assertIsInstance(data, list)\n            self.assertEqual(len(data), 6)\n            self.assertIsInstance(data[0], int)\n            self.assertIsInstance(data[1], int)\n            self.assertIsInstance(data[2], int)\n            self.assertIsInstance(data[3], int)\n            self.assertIsInstance(data[4], float)\n            self.assertIsInstance(data[5], int)\n\n    @patch('aiohttp.ClientSession.post')\n    async def test_generate_detections_cloud_request_error(\n        self,\n        mock_post: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test generate_detections_cloud handles request errors properly.\n        \"\"\"\n        # Assume the token already exists, so authenticate() is skipped\n        self.detector.shared_token['access_token'] = 'fake_token'\n\n        frame = np.zeros((480, 640, 3), dtype=np.uint8)\n\n        # Simulate the detection response with 500 Internal Server Error\n        headers: CIMultiDict[str] = CIMultiDict()\n        request_info = aiohttp.RequestInfo(\n            url=yarl.URL('http://mock.com/detect'),\n            method='POST',\n            headers=CIMultiDictProxy(headers),\n            real_url=yarl.URL('http://mock.com/detect'),\n        )\n\n        # Simulate the detection response with 500 Internal Server Error\n        mock_response = MagicMock()\n        mock_response.raise_for_status.side_effect = (\n            aiohttp.ClientResponseError(\n                request_info=request_info,\n                history=(),\n                status=500,\n                message='Internal Server Error',\n            )\n        )\n        mock_post.return_value.__aenter__.return_value = mock_response\n\n        with self.assertLogs(self.detector.logger, level='ERROR') as captured:\n            with self.assertRaises(aiohttp.ClientResponseError):\n                await self.detector.generate_detections_cloud(frame)\n\n        # Validate the error message in the logs\n        combined_logs = '\\n'.join(captured.output)\n        self.assertIn('Failed to send detection request:', combined_logs)\n\n    @patch('src.live_stream_detection.get_sliced_prediction')\n    @patch('src.live_stream_detection.AutoDetectionModel.from_pretrained')\n    async def test_generate_detections_local_with_predictions(\n        self,\n        mock_from_pretrained: MagicMock,\n        mock_get_sliced_prediction: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test the generate_detections_local method with predictions.\n        \"\"\"\n        # Mock the model\n        mock_model: MagicMock = MagicMock()\n        mock_from_pretrained.return_value = mock_model\n\n        # Mock the object predictions\n        mock_result: MagicMock = MagicMock()\n        mock_result.object_prediction_list = [\n            MagicMock(\n                category=MagicMock(id=0),  # Set category ID is 0\n                bbox=MagicMock(\n                    # Set the bounding box\n                    to_voc_bbox=lambda: [10.5, 20.3, 50.8, 60.1],\n                ),\n                score=MagicMock(value=0.85),  # Set the score\n            ),\n            MagicMock(\n                category=MagicMock(id=1),  # Set category ID to 1\n                bbox=MagicMock(\n                    to_voc_bbox=lambda: [30, 40, 70, 80],\n                ),\n                score=MagicMock(value=0.9),\n            ),\n        ]\n        mock_get_sliced_prediction.return_value = mock_result\n\n        # Set up the input frame\n        frame: np.ndarray = np.zeros((480, 640, 3), dtype=np.uint8)\n\n        # Generate the detections\n        datas: list[list[Any]] = await self.detector.generate_detections_local(\n            frame,\n        )\n\n        # Validate the structure and types of the detection data\n        self.assertIsInstance(datas, list)\n\n        # Ensure two detections are returned\n        self.assertEqual(len(datas), 2)\n\n        # Validate the structure and types of the detection data\n        for data in datas:\n            self.assertIsInstance(data, list)\n            self.assertEqual(len(data), 6)\n            self.assertIsInstance(data[0], int)\n            self.assertIsInstance(data[1], int)\n            self.assertIsInstance(data[2], int)\n            self.assertIsInstance(data[3], int)\n            self.assertIsInstance(data[4], float)\n            self.assertIsInstance(data[5], int)\n\n        # Ensure the first detection is correct\n        self.assertEqual(datas[0], [10, 20, 50, 60, 0.85, 0])\n        # Ensure the second detection is correct\n        self.assertEqual(datas[1], [30, 40, 70, 80, 0.9, 1])\n\n        # Validate the calls to the model\n        mock_get_sliced_prediction.assert_called_once_with(\n            frame,\n            mock_model,\n            slice_height=376,\n            slice_width=376,\n            overlap_height_ratio=0.3,\n            overlap_width_ratio=0.3,\n        )\n\n    async def test_generate_detections(self) -> None:\n        \"\"\"\n        Test the generate_detections method.\n        \"\"\"\n        frame = np.zeros((480, 640, 3), dtype=np.uint8)\n        mat_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n        self.detector.detect_with_server = False\n        with patch.object(\n            self.detector, 'generate_detections_local',\n            return_value=[[10, 10, 50, 50, 0.9, 0]],\n        ) as mock_local:\n            datas, _ = await self.detector.generate_detections(mat_frame)\n            self.assertEqual(len(datas), 1)\n            self.assertEqual(datas[0][5], 0)\n            mock_local.assert_called_once_with(mat_frame)\n\n        self.detector.detect_with_server = True\n        with patch.object(\n            self.detector, 'generate_detections_cloud',\n            return_value=[[20, 20, 60, 60, 0.8, 1]],\n        ) as mock_cloud:\n            datas, _ = await self.detector.generate_detections(mat_frame)\n            self.assertEqual(len(datas), 1)\n            self.assertEqual(datas[0][5], 1)\n            mock_cloud.assert_called_once_with(mat_frame)\n\n    ########################################################################\n    # run_detection method tests\n    ########################################################################\n\n    @patch('src.live_stream_detection.cv2.VideoCapture')\n    async def test_run_detection_stream_not_opened(\n        self,\n        mock_vcap: MagicMock,\n    ) -> None:\n        \"\"\"\n        If the stream cannot be opened, ValueError is raised.\n        \"\"\"\n        cap_mock = MagicMock()\n        cap_mock.isOpened.return_value = False\n        mock_vcap.return_value = cap_mock\n\n        with self.assertRaises(ValueError) as ctx:\n            await self.detector.run_detection('fake_stream')\n\n        # Validate the error message\n        self.assertIn('Failed to open stream', str(ctx.exception))\n\n    async def test_run_detection(self) -> None:\n        \"\"\"\n        Test the run_detection method.\n        \"\"\"\n        stream_url: str = 'http://example.com/virtual_stream'\n        cap_mock: MagicMock = MagicMock()\n        cap_mock.read.side_effect = [\n            (True, np.zeros((480, 640, 3), dtype=np.uint8)),\n            (True, np.zeros((480, 640, 3), dtype=np.uint8)),\n            (False, None),\n        ]\n        cap_mock.isOpened.return_value = True\n\n        with patch(\n            'src.live_stream_detection.cv2.VideoCapture',\n            return_value=cap_mock,\n        ):\n            with patch('src.live_stream_detection.cv2.imshow'):\n                with patch(\n                    'src.live_stream_detection.cv2.waitKey',\n                    side_effect=[-1, ord('q')],\n                ):\n                    await self.detector.run_detection(stream_url)\n\n        cap_mock.read.assert_called()\n        cap_mock.release.assert_called_once()\n\n    @patch(\n        'src.live_stream_detection.cv2.waitKey',\n        side_effect=[-1, -1, ord('q')],\n    )\n    @patch('src.live_stream_detection.cv2.imshow')\n    @patch('src.live_stream_detection.cv2.VideoCapture')\n    async def test_run_detection_loop(\n        self,\n        mock_vcap: MagicMock,\n        mock_imshow: MagicMock,\n        mock_waitKey: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test run_detection loop with valid frames then user presses 'q'.\n        \"\"\"\n        cap_mock = MagicMock()\n        cap_mock.isOpened.return_value = True\n\n        # Mock the frames read from the stream\n        frames_side_effect = [\n            (True, np.zeros((480, 640, 3), dtype=np.uint8)),\n            (False, None),\n            (True, np.zeros((480, 640, 3), dtype=np.uint8)),\n            (True, np.zeros((480, 640, 3), dtype=np.uint8)),\n        ]\n        cap_mock.read.side_effect = frames_side_effect\n        mock_vcap.return_value = cap_mock\n\n        # Execute run_detection\n        await self.detector.run_detection('fake_stream')\n\n        # Validate the calls\n        self.assertGreaterEqual(cap_mock.read.call_count, 4, 'read()4')\n        cap_mock.release.assert_called_once()\n        mock_imshow.assert_called()\n        mock_waitKey.assert_called()\n\n    ########################################################################\n    # Post-processing function tests\n    ########################################################################\n\n    def test_remove_overlapping_labels(self) -> None:\n        \"\"\"\n        Test the remove_overlapping_labels method.\n        \"\"\"\n        datas = [\n            [10, 10, 50, 50, 0.9, 0],  # Hardhat\n            [10, 10, 50, 45, 0.8, 2],  # NO-Hardhat (overlap > 0.8)\n            [20, 20, 60, 60, 0.85, 7],  # Safety Vest\n            [20, 20, 60, 55, 0.75, 4],  # NO-Safety Vest (overlap > 0.8)\n        ]\n        expected_datas = [\n            [10, 10, 50, 50, 0.9, 0],  # Hardhat\n            [20, 20, 60, 60, 0.85, 7],  # Safety Vest\n        ]\n        filtered_datas = self.detector.remove_overlapping_labels(datas)\n        self.assertEqual(len(filtered_datas), len(expected_datas))\n        # for fd, ed in zip(filtered_datas, expected_datas):\n        self.assertEqual(filtered_datas, expected_datas)\n\n        # Add more test cases to cover different overlap scenarios\n        datas = [\n            [10, 10, 50, 50, 0.9, 0],  # Hardhat\n            [30, 30, 70, 70, 0.8, 2],  # NO-Hardhat (no overlap)\n            [10, 10, 50, 50, 0.85, 7],  # Safety Vest (same as Hardhat)\n            [60, 60, 100, 100, 0.75, 4],  # NO-Safety Vest (no overlap)\n        ]\n        expected_datas = [\n            [10, 10, 50, 50, 0.9, 0],  # Hardhat\n            [10, 10, 50, 50, 0.85, 7],  # Safety Vest\n            [30, 30, 70, 70, 0.8, 2],  # NO-Hardhat\n            [60, 60, 100, 100, 0.75, 4],  # NO-Safety Vest\n        ]\n        filtered_datas = self.detector.remove_overlapping_labels(datas)\n        self.assertEqual(len(filtered_datas), len(expected_datas))\n\n        # Sorting both lists before comparing\n        filter_datas_sorted = sorted(filtered_datas)\n        expected_datas_sorted = sorted(expected_datas)\n\n        for fd, ed in zip(filter_datas_sorted, expected_datas_sorted):\n            self.assertEqual(fd, ed)\n\n    def test_overlap_percentage(self) -> None:\n        \"\"\"\n        Test the overlap_percentage method.\n        \"\"\"\n        bbox1 = [10, 10, 50, 50]\n        bbox2 = [20, 20, 40, 40]\n        overlap = self.detector.overlap_percentage(bbox1, bbox2)\n        self.assertAlmostEqual(overlap, 0.262344, places=6)\n\n    def test_is_contained(self) -> None:\n        \"\"\"\n        Test the is_contained method.\n        \"\"\"\n        outer_bbox = [10, 10, 50, 50]\n        inner_bbox = [20, 20, 40, 40]\n        self.assertTrue(self.detector.is_contained(inner_bbox, outer_bbox))\n\n        inner_bbox = [5, 5, 40, 40]\n        self.assertFalse(self.detector.is_contained(inner_bbox, outer_bbox))\n\n        inner_bbox = [10, 10, 50, 50]\n        self.assertTrue(self.detector.is_contained(inner_bbox, outer_bbox))\n\n        inner_bbox = [0, 0, 60, 60]\n        self.assertFalse(self.detector.is_contained(inner_bbox, outer_bbox))\n\n    def test_remove_completely_contained_labels(self) -> None:\n        \"\"\"\n        Test the remove_completely_contained_labels method.\n        \"\"\"\n        datas = [\n            [10, 10, 50, 50, 0.9, 0],   # Hardhat\n            [20, 20, 40, 40, 0.8, 2],   # NO-Hardhat (contained within Hardhat)\n            [20, 20, 60, 60, 0.85, 7],  # Safety Vest\n            # NO-Safety Vest (contained within Safety Vest)\n            [25, 25, 35, 35, 0.75, 4],\n        ]\n        expected_datas = [\n            [10, 10, 50, 50, 0.9, 0],   # Hardhat\n            [20, 20, 60, 60, 0.85, 7],  # Safety Vest\n        ]\n        filtered_datas = self.detector.remove_completely_contained_labels(\n            datas,\n        )\n        self.assertEqual(filtered_datas, expected_datas)\n\n        # Add more test cases to cover different containment scenarios\n        datas = [\n            [10, 10, 50, 50, 0.9, 0],   # Hardhat\n            [30, 30, 70, 70, 0.8, 2],   # NO-Hardhat (not contained)\n            [10, 10, 50, 50, 0.85, 7],  # Safety Vest (same as Hardhat)\n            [60, 60, 100, 100, 0.75, 4],  # NO-Safety Vest (not contained)\n        ]\n        expected_datas = [\n            [10, 10, 50, 50, 0.9, 0],   # Hardhat\n            [30, 30, 70, 70, 0.8, 2],   # NO-Hardhat\n            [10, 10, 50, 50, 0.85, 7],  # Safety Vest\n            [60, 60, 100, 100, 0.75, 4],  # NO-Safety Vest\n        ]\n        filtered_datas = self.detector.remove_completely_contained_labels(\n            datas,\n        )\n        self.assertEqual(filtered_datas, expected_datas)\n\n        # Sorting both lists before comparing\n        filter_datas_sorted = sorted(filtered_datas)\n        expected_datas_sorted = sorted(expected_datas)\n\n        for fd, ed in zip(filter_datas_sorted, expected_datas_sorted):\n            self.assertEqual(fd, ed)\n\n    def test_remove_hardhat_in_no_hardhat(self) -> None:\n        \"\"\"\n        Test the remove_completely_contained_labels method\n        with a Hardhat bounding box.\n        \"\"\"\n        # Input data with bounding boxes\n        datas = [\n            [10, 10, 50, 50, 0.8, 2],  # No-Hardhat\n            [20, 20, 30, 30, 0.9, 0],  # Hardhat\n        ]\n\n        # Expected result: Only the No-Hardhat bounding box remains\n        expected_datas = [\n            [10, 10, 50, 50, 0.8, 2],\n        ]\n\n        # Call the method being tested\n        filtered_datas = self.detector.remove_completely_contained_labels(\n            datas,\n        )\n\n        # Assert that the filtered data matches the expected output\n        self.assertEqual(\n            filtered_datas,\n            expected_datas,\n            'Hardhat should be removed '\n            \"when it's completely contained within No-Hardhat\",\n        )\n\n    def test_remove_safety_vest_in_no_vest(self) -> None:\n        \"\"\"\n        Test the remove_completely_contained_labels method\n        with a Safety Vest bounding box.\n        \"\"\"\n        # Input data with bounding boxes\n        datas: list[list[float]] = [\n            # No-Safety Vest bounding box (large box)\n            [10, 10, 50, 50, 0.85, 4],\n            # Safety Vest bounding box (contained within the first)\n            [20, 20, 30, 30, 0.9, 7],\n        ]\n\n        # Expected filtered output: Safety Vest box is removed\n        expected_datas: list[list[float]] = [\n            [10, 10, 50, 50, 0.85, 4],\n        ]\n\n        # Perform filtering using the method under test\n        filtered_datas = self.detector.remove_completely_contained_labels(\n            datas,\n        )\n\n        # Assert the output matches the expected result\n        self.assertEqual(\n            filtered_datas,\n            expected_datas,\n            'Safety Vest should be removed '\n            \"when it's contained by No-Safety Vest\",\n        )\n\n    ########################################################################\n    # Test main()\n    ########################################################################\n\n    @patch.object(\n        sys, 'argv', [\n            'python',  #  Python \n            '--url', 'http://example.com/virtual_stream',\n            '--api_url', 'http://mocked-api.com',\n            '--detect_with_server',\n        ],\n    )\n    @patch(\n        'src.live_stream_detection.LiveStreamDetector.run_detection',\n        new_callable=AsyncMock,\n    )\n    @patch(\n        'src.live_stream_detection.LiveStreamDetector.__init__',\n        return_value=None,\n    )\n    async def test_main(\n        self,\n        mock_init: MagicMock,\n        mock_run_detection: AsyncMock,\n    ) -> None:\n        \"\"\"\n        Test the main function with valid arguments.\n        \"\"\"\n        # Execute the main function\n        await main()\n\n        # Ensure the detector was initialised with the correct arguments\n        mock_init.assert_called_once_with(\n            api_url='http://mocked-api.com',\n            model_key='yolo11n',\n            output_folder=None,\n            detect_with_server=True,\n            shared_token={'access_token': ''},\n        )\n\n        # Ensure the run_detection method was called with the expected URL\n        mock_run_detection.assert_called_once_with(\n            'http://example.com/virtual_stream',\n        )\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/src/notifiers/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/examples/YOLO_server_api/backend/security_test.py", "content": "from __future__ import annotations\n\nimport unittest\nfrom unittest.mock import patch\n\nfrom fastapi import FastAPI\n\nfrom examples.YOLO_server_api.backend.security import update_secret_key\n\n\nclass TestUpdateSecretKey(unittest.TestCase):\n    \"\"\"\n    Unit tests for the `update_secret_key` function in FastAPI applications.\n    \"\"\"\n\n    def setUp(self) -> None:\n        \"\"\"\n        Initialises the FastAPI application instance before each test.\n        \"\"\"\n        self.app: FastAPI = FastAPI()\n\n    @patch('examples.YOLO_server_api.backend.security.secrets.token_urlsafe')\n    def test_update_secret_key(\n        self, mock_token_urlsafe: unittest.mock.MagicMock,\n    ) -> None:\n        \"\"\"\n        Tests that `update_secret_key` updates the secret key correctly.\n\n        Args:\n            mock_token_urlsafe (unittest.mock.MagicMock): Mocked\n                `token_urlsafe` function.\n        \"\"\"\n        # Mock the return value of `token_urlsafe` to a specific key\n        mock_token_urlsafe.return_value = 'mocked_secret_key'\n\n        # Invoke `update_secret_key` function\n        update_secret_key(self.app)\n\n        # Verify if the `jwt_secret_key` has been set to the mocked key\n        self.assertEqual(self.app.state.jwt_secret_key, 'mocked_secret_key')\n\n        # Confirm that `token_urlsafe` was called exactly once with\n        # an argument of 16\n        mock_token_urlsafe.assert_called_once_with(16)\n\n    def test_update_secret_key_different_keys(self) -> None:\n        \"\"\"Tests that `update_secret_key` generates unique keys each time.\n\n        This test verifies that each invocation of `update_secret_key`\n        generates a new and unique secret key.\n        \"\"\"\n        # Generate the first secret key\n        update_secret_key(self.app)\n        first_key: str = self.app.state.jwt_secret_key\n\n        # Generate the second secret key\n        update_secret_key(self.app)\n        second_key: str = self.app.state.jwt_secret_key\n\n        # Assert that the two keys are different, are strings,\n        # and the second key is non-empty\n        self.assertNotEqual(first_key, second_key)\n        self.assertIsInstance(second_key, str)\n        self.assertGreater(len(second_key), 0)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/examples/YOLO_server_api/backend/auth_test.py", "content": "from __future__ import annotations\n\nimport unittest\nfrom unittest.mock import AsyncMock\nfrom unittest.mock import MagicMock\nfrom unittest.mock import patch\n\nfrom redis.asyncio import Redis\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom examples.YOLO_server_api.backend.auth import create_token_logic\nfrom examples.YOLO_server_api.backend.auth import UserLogin\nfrom examples.YOLO_server_api.backend.models import User\n\n\nclass TestCreateTokenLogic(unittest.IsolatedAsyncioTestCase):\n    '''\n    Class to test the create_token_logic function.\n    '''\n\n    async def asyncSetUp(self):\n        \"\"\"\n        Set up mocks before each test.\n        \"\"\"\n        # Build Mock DB and Redis\n        self.mock_db = AsyncMock(spec=AsyncSession)\n        self.mock_redis = AsyncMock(spec=Redis)\n        # Build mock jwt_access\n        self.mock_jwt_access = MagicMock()\n\n        # Build user_login object\n        self.username = 'testuser'\n        self.password = 'testpassword'\n        self.user_login = UserLogin(\n            username=self.username, password=self.password,\n        )\n\n        # Build mock user object with the same username and password\n        self.mock_user = MagicMock(spec=User)\n        self.mock_user.id = 123\n        self.mock_user.username = self.username\n        self.mock_user.role = 'user'\n        self.mock_user.is_active = True\n        # Set the check_password method to return True\n        self.mock_user.check_password = AsyncMock(return_value=True)\n\n    @patch(\n        'examples.YOLO_server_api.backend.auth.get_user_data',\n        new_callable=AsyncMock,\n    )\n    @patch(\n        'examples.YOLO_server_api.backend.auth.set_user_data',\n        new_callable=AsyncMock,\n    )\n    async def test_user_not_in_cache(\n        self,\n        mock_set_user_data: MagicMock,\n        mock_get_user_data: MagicMock,\n    ):\n        \"\"\"\n        Test token creation when the user is not in the cache.\n\n        Args:\n            mock_set_user_data (MagicMock): Mock for set_user_data function.\n            mock_get_user_data (MagicMock): Mock for get_user_data function.\n\n        \"\"\"\n        mock_get_user_data.return_value = None\n\n        # If the user is not in the cache, the database should be queried\n        mock_result = MagicMock()\n        mock_result.scalar.return_value = self.mock_user\n        self.mock_db.execute.return_value = mock_result\n\n        # Mock the JWT access token creation\n        self.mock_jwt_access.create_access_token.return_value = 'fake_token'\n\n        # Call the create_token_logic function\n        resp = await create_token_logic(\n            user=self.user_login,\n            db=self.mock_db,\n            redis_pool=self.mock_redis,\n            jwt_access=self.mock_jwt_access,\n        )\n\n        self.assertIn('access_token', resp)\n        self.assertEqual(resp['access_token'], 'fake_token')\n        self.assertEqual(resp['role'], 'user')\n        self.assertEqual(resp['username'], 'testuser')\n\n        # Verify that the user data was set in the cache\n        mock_set_user_data.assert_awaited_once()\n\n    @patch(\n        'examples.YOLO_server_api.backend.auth.get_user_data',\n        new_callable=AsyncMock,\n    )\n    @patch(\n        'examples.YOLO_server_api.backend.auth.set_user_data',\n        new_callable=AsyncMock,\n    )\n    async def test_user_in_cache(\n        self,\n        mock_set_user_data: MagicMock,\n        mock_get_user_data: MagicMock,\n    ):\n        \"\"\"\n        Test token creation when the user is in the cache.\n\n        Args:\n            mock_set_user_data (MagicMock): Mock for set_user_data function.\n            mock_get_user_data (MagicMock): Mock for get_user_data function.\n        \"\"\"\n        # If the user is in the cache, the database should not be queried\n        mock_get_user_data.return_value = {\n            'db_user': {\n                'id': 123,\n                'username': self.username,\n                'role': 'user',\n                'is_active': True,\n            },\n            'jti_list': [],\n        }\n\n        # Call the create_token_logic function\n        mock_result = MagicMock()\n        mock_result.scalar.return_value = self.mock_user\n        self.mock_db.execute.return_value = mock_result\n        self.mock_jwt_access.create_access_token.return_value = 'fake_token'\n\n        # Call the create_token_logic function\n        resp = await create_token_logic(\n            user=self.user_login,\n            db=self.mock_db,\n            redis_pool=self.mock_redis,\n            jwt_access=self.mock_jwt_access,\n        )\n        self.assertEqual(resp['access_token'], 'fake_token')\n        self.assertEqual(resp['username'], 'testuser')\n        self.assertEqual(resp['role'], 'user')\n\n        # Verify that the user data was not set in the cache\n        mock_set_user_data.assert_awaited_once()\n\n    @patch(\n        'examples.YOLO_server_api.backend.auth.get_user_data',\n        new_callable=AsyncMock,\n    )\n    async def test_user_not_found_in_db(\n        self,\n        mock_get_user_data: MagicMock,\n    ):\n        \"\"\"\n        Test token creation failure when the user is not found in the database\n\n        Args:\n            mock_get_user_data (MagicMock): Mock for get_user_data function.\n        \"\"\"\n        # Mock the get_user_data function to return None\n        mock_get_user_data.return_value = None\n        mock_result = MagicMock()\n        mock_result.scalar.return_value = None\n        self.mock_db.execute.return_value = mock_result\n\n        # Verify that an exception is raised\n        # when the user is not found in the database\n        with self.assertRaisesRegex(Exception, '401'):\n            await create_token_logic(\n                user=self.user_login,\n                db=self.mock_db,\n                redis_pool=self.mock_redis,\n                jwt_access=self.mock_jwt_access,\n            )\n\n    @patch(\n        'examples.YOLO_server_api.backend.auth.get_user_data',\n        new_callable=AsyncMock,\n    )\n    async def test_wrong_password(\n        self,\n        mock_get_user_data: MagicMock,\n    ):\n        \"\"\"\n        Test token creation failure when the user password is incorrect.\n\n        Args:\n            mock_get_user_data (MagicMock): Mock for get_user_data function.\n        \"\"\"\n        # Set the mock user data to None\n        mock_get_user_data.return_value = None\n        # Set the check_password method to return False\n        self.mock_user.check_password.return_value = False\n\n        # Mock the database execute result to return the user\n        mock_result = MagicMock()\n        mock_result.scalar.return_value = self.mock_user\n        self.mock_db.execute.return_value = mock_result\n\n        # Verify that an exception is raised\n        # when the user password is incorrect\n        with self.assertRaisesRegex(Exception, '401'):\n            await create_token_logic(\n                user=self.user_login,\n                db=self.mock_db,\n                redis_pool=self.mock_redis,\n                jwt_access=self.mock_jwt_access,\n            )\n\n    @patch(\n        'examples.YOLO_server_api.backend.auth.get_user_data',\n        new_callable=AsyncMock,\n    )\n    async def test_user_inactive(\n        self,\n        mock_get_user_data: MagicMock,\n    ):\n        \"\"\"\n        Test token creation failure when the user account is inactive.\n\n        Args:\n            mock_get_user_data (MagicMock): Mock for get_user_data function.\n        \"\"\"\n        # Set the mock user to inactive\n        mock_get_user_data.return_value = None\n        self.mock_user.is_active = False\n\n        # Mock the database execute result to return the user\n        mock_result = MagicMock()\n        mock_result.scalar.return_value = self.mock_user\n        self.mock_db.execute.return_value = mock_result\n\n        # Verify that an exception is raised when the user account is inactive\n        with self.assertRaisesRegex(Exception, '403'):\n            await create_token_logic(\n                user=self.user_login,\n                db=self.mock_db,\n                redis_pool=self.mock_redis,\n                jwt_access=self.mock_jwt_access,\n            )\n\n    @patch(\n        'examples.YOLO_server_api.backend.auth.get_user_data',\n        new_callable=AsyncMock,\n    )\n    async def test_invalid_role(\n        self,\n        mock_get_user_data: MagicMock,\n    ):\n        \"\"\"\n        Test token creation failure when the user role is invalid.\n\n        Args:\n            mock_get_user_data (MagicMock): Mock for get_user_data function\n        \"\"\"\n        # Set the mock user to have an invalid role\n        mock_get_user_data.return_value = None\n        self.mock_user.role = 'invalid_role'\n\n        # Mock the database execute result to return the user\n        mock_result = MagicMock()\n        mock_result.scalar.return_value = self.mock_user\n        self.mock_db.execute.return_value = mock_result\n\n        # Verify that an exception is raised when the user role is invalid\n        with self.assertRaisesRegex(Exception, '403'):\n            await create_token_logic(\n                user=self.user_login,\n                db=self.mock_db,\n                redis_pool=self.mock_redis,\n                jwt_access=self.mock_jwt_access,\n            )\n\n    @patch(\n        'examples.YOLO_server_api.backend.auth.get_user_data',\n        new_callable=AsyncMock,\n    )\n    @patch(\n        'examples.YOLO_server_api.backend.auth.set_user_data',\n        new_callable=AsyncMock,\n    )\n    async def test_jti_list_full(\n        self,\n        mock_set_user_data: MagicMock,\n        mock_get_user_data: MagicMock,\n    ):\n        \"\"\"\n        Test token creation when the JTI list is full.\n\n        Args:\n            mock_set_user_data (MagicMock): Mock for set_user_data function.\n            mock_get_user_data (MagicMock): Mock for get_user_data function.\n        \"\"\"\n        # Mock the user data with a full JTI list\n        mock_get_user_data.return_value = {\n            'db_user': {\n                'id': 123,\n                'username': self.username,\n                'role': 'user',\n                'is_active': True,\n            },\n            'jti_list': ['jti1', 'jti2'],  # Assume max_jti=2\n        }\n\n        # Mock the database execute result to return the user\n        mock_result = MagicMock()\n        mock_result.scalar.return_value = self.mock_user\n        self.mock_db.execute.return_value = mock_result\n\n        # Mock the JWT access token creation\n        self.mock_jwt_access.create_access_token.return_value = 'fake_token'\n\n        # Call the create_token_logic function with max_jti=2\n        resp = await create_token_logic(\n            user=self.user_login,\n            db=self.mock_db,\n            redis_pool=self.mock_redis,\n            jwt_access=self.mock_jwt_access,\n            max_jti=2,\n        )\n\n        # Validate the response\n        self.assertEqual(resp['access_token'], 'fake_token')\n        self.assertEqual(resp['role'], 'user')\n        self.assertEqual(resp['username'], self.username)\n\n        # Verify that the JTI list was updated correctly\n        # Extract user data\n        updated_user_data = mock_set_user_data.call_args[0][2]\n        self.assertEqual(len(updated_user_data['jti_list']), 2)  # Max length\n        # Oldest JTI removed\n        self.assertNotIn('jti1', updated_user_data['jti_list'])\n        # Second JTI remains\n        self.assertIn('jti2', updated_user_data['jti_list'])\n        self.assertIn(\n            updated_user_data['jti_list'][-1],  # New JTI added\n            updated_user_data['jti_list'],\n        )\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/examples/streaming_web/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/examples/YOLO_evaluation/convert_yolo_to_coco_test.py", "content": "from __future__ import annotations\n\nimport argparse\nimport json\nimport unittest\nfrom unittest.mock import MagicMock\nfrom unittest.mock import mock_open\nfrom unittest.mock import patch\n\nfrom examples.YOLO_evaluation.convert_yolo_to_coco import COCOConverter\nfrom examples.YOLO_evaluation.convert_yolo_to_coco import main\n\n\nclass TestCOCOConverter(unittest.TestCase):\n    def setUp(self):\n        self.categories = [\n            'Hardhat', 'Mask', 'NO-Hardhat', 'NO-Mask', 'NO-Safety Vest',\n            'Person', 'Safety Cone', 'Safety Vest', 'machinery', 'vehicle',\n        ]\n        self.converter = COCOConverter(self.categories)\n\n    def tearDown(self):\n        \"\"\"\n        Clean up after each test.\n        \"\"\"\n        # Clear the COCO format data to avoid state leakage between tests\n        self.converter.coco_format.clear()\n\n    @patch('examples.YOLO_evaluation.convert_yolo_to_coco.os.listdir')\n    @patch('examples.YOLO_evaluation.convert_yolo_to_coco.os.path.exists')\n    @patch('examples.YOLO_evaluation.convert_yolo_to_coco.Image.open')\n    @patch(\n        'builtins.open',\n        new_callable=mock_open,\n        read_data='0 0.5 0.5 0.5 0.5\\n',\n    )\n    @patch('builtins.print')  # Mock print to check warning messages\n    def test_convert_annotations(\n        self,\n        mock_print,\n        mock_file,\n        mock_image_open,\n        mock_exists,\n        mock_listdir,\n    ):\n        \"\"\"\n        Test the conversion of YOLO annotations to COCO format,\n        including the handling of non-existing images.\n        \"\"\"\n        # Setup the mocks\n        mock_listdir.return_value = ['image1.txt', 'image2.txt']\n        # Simulate image1 exists, image2 does not exist\n        mock_exists.side_effect = lambda path: path.endswith('image1.jpg')\n        mock_image = MagicMock()\n        mock_image.size = (800, 600)\n        mock_image_open.return_value = mock_image\n\n        # Run the conversion\n        self.converter.convert_annotations('labels_dir', 'images_dir')\n\n        # Check that the image metadata is\n        # correctly added only for the existing image\n        self.assertEqual(len(self.converter.coco_format['images']), 1)\n        self.assertEqual(\n            self.converter.coco_format['images'][0]['file_name'], 'image1.jpg',\n        )\n\n        # Check that the annotation is\n        # correctly added only for the existing image\n        self.assertEqual(len(self.converter.coco_format['annotations']), 1)\n        annotation = self.converter.coco_format['annotations'][0]\n        self.assertEqual(annotation['image_id'], 1)\n        self.assertEqual(annotation['category_id'], 1)\n        self.assertEqual(annotation['bbox'], [200.0, 150.0, 400.0, 300.0])\n\n        # Check that a warning was printed for the non-existing image\n        mock_print.assert_called_with(\n            'Warning: images_dir/image2.jpg does not exist.',\n        )\n\n    @patch('builtins.open', new_callable=mock_open)\n    def test_save_to_json(self, mock_file):\n        \"\"\"\n        Test saving the COCO format data to a JSON file.\n        \"\"\"\n        # Add some dummy data to the COCO format\n        self.converter.coco_format['images'].append({\n            'id': 1,\n            'width': 800,\n            'height': 600,\n            'file_name': 'image1.jpg',\n        })\n        self.converter.coco_format['annotations'].append({\n            'id': 1,\n            'image_id': 1,\n            'category_id': 1,\n            'bbox': [200.0, 150.0, 400.0, 300.0],\n            'area': 120000.0,\n            'segmentation': [],\n            'iscrowd': 0,\n        })\n\n        # Run the save to JSON method\n        self.converter.save_to_json('output.json')\n\n        # Check that the file was written with the correct data\n        mock_file.assert_called_once_with('output.json', 'w')\n        written_data = json.loads(\n            ''.join(call.args[0] for call in mock_file().write.mock_calls),\n        )\n        self.assertIn('images', written_data)\n        self.assertIn('annotations', written_data)\n        self.assertEqual(len(written_data['images']), 1)\n        self.assertEqual(len(written_data['annotations']), 1)\n\n    @patch(\n        'builtins.open',\n        new_callable=mock_open,\n        read_data='0 0.5 0.5 0.5 0.5\\n',\n    )\n    def test_initialise_categories(self, mock_file):\n        \"\"\"\n        Test the initialisation of categories in COCO format.\n        \"\"\"\n        # Reset categories to avoid duplication\n        self.converter.coco_format['categories'] = []\n        self.converter.initialise_categories(self.categories)\n        categories = self.converter.coco_format['categories']\n        self.assertEqual(len(categories), len(self.categories))\n        for i, category in enumerate(self.categories):\n            self.assertEqual(categories[i]['name'], category)\n            self.assertEqual(categories[i]['id'], i + 1)\n\n    @patch(\n        'examples.YOLO_evaluation.convert_yolo_to_coco.'\n        'COCOConverter.convert_annotations',\n    )\n    @patch(\n        'examples.YOLO_evaluation.convert_yolo_to_coco.'\n        'COCOConverter.save_to_json',\n    )\n    @patch('argparse.ArgumentParser.parse_args')\n    def test_main(\n            self,\n            mock_parse_args,\n            mock_save_to_json,\n            mock_convert_annotations,\n    ):\n        \"\"\"\n        Test the main function.\n        \"\"\"\n        # Setup the mock arguments\n        mock_parse_args.return_value = argparse.Namespace(\n            labels_dir='dataset/valid/labels',\n            images_dir='dataset/valid/images',\n            output='dataset/coco_annotations.json',\n        )\n\n        # Mock open to avoid creating a real file\n        with patch('builtins.open', mock_open()):\n            # Run the main function\n            main()\n\n            # Check that convert_annotations and save_to_json were called\n            mock_convert_annotations.assert_called_once_with(\n                'dataset/valid/labels', 'dataset/valid/images',\n            )\n            mock_save_to_json.assert_called_once_with(\n                'dataset/coco_annotations.json',\n            )\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/examples/YOLO_evaluation/evaluate_yolo_test.py", "content": "from __future__ import annotations\n\nimport sys\nimport unittest\nfrom io import StringIO\nfrom unittest.mock import MagicMock\nfrom unittest.mock import patch\n\nfrom examples.YOLO_evaluation.evaluate_yolo import main\nfrom examples.YOLO_evaluation.evaluate_yolo import ModelEvaluator\n\n\nclass TestModelEvaluator(unittest.TestCase):\n    def setUp(self) -> None:\n        self.model_path: str = 'models/pt/best_yolov8n.pt'\n        self.data_path: str = 'tests/dataset/data.yaml'\n        self.evaluator: ModelEvaluator | None = None\n\n    def tearDown(self) -> None:\n        \"\"\"\n        Clean up after each test.\n        \"\"\"\n        self.evaluator = None\n\n    @patch('examples.YOLO_evaluation.evaluate_yolo.YOLO')\n    def test_evaluate(self, mock_yolo: MagicMock) -> None:\n        \"\"\"\n        Test the evaluate method to ensure it returns the expected results.\n        \"\"\"\n        # Create a mock model and mock return value for the val method\n        mock_model: MagicMock = MagicMock()\n        mock_yolo.return_value = mock_model\n        expected_results: dict[str, str] = {\n            'metrics': 'some_evaluation_results',\n        }\n        mock_model.val.return_value = expected_results\n\n        # Initialise the ModelEvaluator after mocking YOLO\n        self.evaluator = ModelEvaluator(\n            model_path=self.model_path,\n            data_path=self.data_path,\n        )\n\n        # Call the evaluate method\n        results: dict[str, str] = self.evaluator.evaluate()\n\n        # Verify that the YOLO class was instantiated\n        # with the correct model path\n        mock_yolo.assert_called_once_with(self.model_path)\n        self.assertEqual(results, expected_results)\n\n    @patch('examples.YOLO_evaluation.evaluate_yolo.ModelEvaluator')\n    @patch('argparse.ArgumentParser.parse_args')\n    def test_main(\n        self,\n        mock_parse_args: MagicMock,\n        mock_model_evaluator: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test the main function.\n        \"\"\"\n        # Mock the command line arguments\n        mock_parse_args.return_value = MagicMock(\n            model_path=self.model_path,\n            data_path=self.data_path,\n        )\n\n        # Mock the ModelEvaluator and its evaluate method\n        mock_evaluator_instance: MagicMock = MagicMock()\n        mock_model_evaluator.return_value = mock_evaluator_instance\n        mock_evaluator_instance.evaluate.return_value = {\n            'metrics': 'some_evaluation_results',\n        }\n\n        # Capture the output\n        captured_output: StringIO = StringIO()\n        sys.stdout = captured_output\n\n        # Call the main function\n        main()\n\n        # Verify the output\n        self.assertIn('some_evaluation_results', captured_output.getvalue())\n\n        # Reset stdout\n        sys.stdout = sys.__stdout__\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/examples/streaming_web/backend/app_test.py", "content": "from __future__ import annotations\n\nimport unittest\nfrom unittest.mock import AsyncMock\nfrom unittest.mock import MagicMock\nfrom unittest.mock import patch\n\nfrom fastapi.testclient import TestClient\nfrom fastapi_limiter import FastAPILimiter\n\nimport examples.streaming_web.backend.app as app_module\n\n\nclass TestStreamingWebApp(unittest.IsolatedAsyncioTestCase):\n    \"\"\"\n    Test suite for the streaming_web FastAPI app.\n    \"\"\"\n\n    def setUp(self) -> None:\n        \"\"\"\n        Set up the test environment before each test.\n        \"\"\"\n        self.app = app_module.app\n        self.client = TestClient(self.app)\n\n    @patch(\n        'examples.streaming_web.backend.app.redis_manager.client',\n        new_callable=AsyncMock,\n    )\n    async def test_redis_connection(\n        self, mock_redis_client: AsyncMock,\n    ) -> None:\n        \"\"\"\n        Test that the Redis connection is properly established using\n        a mocked Redis client with virtual parameters.\n        \"\"\"\n        # Simulate an AsyncMock instance as a Redis client with\n        # virtual parameters\n        mock_redis_client.return_value = AsyncMock()\n\n        # Mock connection using arbitrary (virtual) Redis parameters\n        await mock_redis_client(\n            host='virtualhost', port=1234,\n            password='virtualpass', decode_responses=True,\n        )\n\n        # Verify that the mock Redis client was called with\n        # the virtual parameters\n        self.assertIsInstance(mock_redis_client, AsyncMock)\n        mock_redis_client.assert_awaited_once_with(\n            host='virtualhost', port=1234,\n            password='virtualpass', decode_responses=True,\n        )\n\n    @patch('examples.streaming_web.backend.app.CORSMiddleware')\n    def test_cors_initialization(self, mock_cors: MagicMock) -> None:\n        \"\"\"\n        Test that CORS is properly initialized for the FastAPI app.\n        \"\"\"\n        cors = mock_cors(\n            self.app, allow_origins=['*'],\n            allow_credentials=True, allow_methods=['*'], allow_headers=['*'],\n        )\n        self.assertIsInstance(cors, MagicMock)\n        mock_cors.assert_called_once_with(\n            self.app, allow_origins=['*'],\n            allow_credentials=True, allow_methods=['*'], allow_headers=['*'],\n        )\n\n    @patch(\n        'examples.streaming_web.backend.app.FastAPILimiter.init',\n        new_callable=AsyncMock,\n    )\n    @patch(\n        'examples.streaming_web.backend.app.redis_manager.client',\n        new_callable=AsyncMock,\n    )\n    async def test_rate_limiter_initialization(\n        self,\n        mock_redis_client: AsyncMock,\n        mock_limiter_init: AsyncMock,\n    ) -> None:\n        \"\"\"\n        Test that the rate limiter is properly initialized with\n        a mocked Redis client.\n        \"\"\"\n        await FastAPILimiter.init(mock_redis_client)\n        mock_limiter_init.assert_awaited_once_with(mock_redis_client)\n\n    @patch('uvicorn.run')\n    def test_app_running_configuration(\n        self, mock_uvicorn_run: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test that the application runs with the expected configurations.\n        \"\"\"\n        app_module.run_server()\n        mock_uvicorn_run.assert_called_once_with(\n            'examples.streaming_web.backend.app:sio_app',\n            host='127.0.0.1', port=8000, log_level='info',\n        )\n\n    @patch(\n        'examples.streaming_web.backend.app.redis_manager.client',\n        new_callable=AsyncMock,\n    )\n    @patch(\n        'examples.streaming_web.backend.app.FastAPILimiter.init',\n        new_callable=AsyncMock,\n    )\n    def test_lifespan_events(\n        self,\n        mock_limiter_init: AsyncMock,\n        mock_redis_client: AsyncMock,\n    ) -> None:\n        \"\"\"\n        Test that the lifespan events are properly handled.\n        \"\"\"\n        with TestClient(app_module.app) as client:\n            response = client.get('/')\n            #  404\n            self.assertEqual(response.status_code, 404)\n        mock_limiter_init.assert_called_once_with(mock_redis_client)\n        mock_redis_client.close.assert_called_once()\n\n    def tearDown(self) -> None:\n        del self.client\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/src/notifiers/broadcast_notifier_test.py", "content": "from __future__ import annotations\n\nimport os\nimport subprocess\nimport unittest\nfrom unittest.mock import MagicMock\nfrom unittest.mock import Mock\nfrom unittest.mock import patch\n\nimport requests\n\nfrom src.notifiers.broadcast_notifier import BroadcastNotifier\nfrom src.notifiers.broadcast_notifier import main\n\n\nclass TestBroadcastNotifier(unittest.TestCase):\n\n    def setUp(self):\n        \"\"\"\n        Set up the BroadcastNotifier instance for each test.\n        \"\"\"\n        self.notifier = BroadcastNotifier('http://localhost:8080/broadcast')\n\n    @patch('src.notifiers.broadcast_notifier.requests.post')\n    def test_broadcast_message_success(self, mock_post):\n        \"\"\"\n        Test that a message is successfully broadcasted.\n        \"\"\"\n        mock_post.return_value = Mock(status_code=200)\n        message = 'Test broadcast message'\n        result = self.notifier.broadcast_message(message)\n\n        self.assertTrue(result)\n        mock_post.assert_called_once_with(\n            self.notifier.broadcast_url, json={'message': message},\n        )\n\n    @patch('src.notifiers.broadcast_notifier.requests.post')\n    def test_broadcast_message_failure(self, mock_post):\n        \"\"\"\n        Test that a failure in broadcasting the message is handled correctly.\n        \"\"\"\n        mock_post.return_value = Mock(status_code=500)\n        message = 'Test broadcast message'\n        result = self.notifier.broadcast_message(message)\n\n        self.assertFalse(result)\n        mock_post.assert_called_once_with(\n            self.notifier.broadcast_url, json={'message': message},\n        )\n\n    @patch('src.notifiers.broadcast_notifier.requests.post')\n    def test_broadcast_message_exception(self, mock_post):\n        \"\"\"\n        Test that an exception during the broadcast is handled correctly.\n        \"\"\"\n        mock_post.side_effect = requests.exceptions.RequestException(\n            'Network error',\n        )\n\n        message = 'Test broadcast message'\n        result = self.notifier.broadcast_message(message)\n\n        self.assertFalse(result)\n        mock_post.assert_called_once_with(\n            self.notifier.broadcast_url, json={'message': message},\n        )\n\n    @patch(\n        'src.notifiers.broadcast_notifier.BroadcastNotifier.broadcast_message',\n    )\n    @patch('src.notifiers.broadcast_notifier.print')\n    def test_main(\n        self,\n        mock_print,\n        mock_broadcast_message,\n    ):\n        \"\"\"\n        Ensure broadcasts a message and prints the status.\n        \"\"\"\n        mock_broadcast_message.return_value = True\n\n        main()\n\n        # Assert broadcast_message was called once\n        mock_broadcast_message.assert_called_once_with(\n            'Test broadcast message',\n        )\n\n        # Assert the print function was called with the expected output\n        mock_print.assert_called_once_with('Broadcast status: True')\n\n    @patch('requests.post')\n    def test_main_as_script(self, mock_post: MagicMock) -> None:\n        \"\"\"\n        Test running the broadcast_notifier.py script as the main program.\n        \"\"\"\n        mock_response: MagicMock = MagicMock()\n        mock_response.status_code = 200\n        mock_post.return_value = mock_response\n\n        # Get the absolute path to the broadcast_notifier.py script\n        script_path = os.path.abspath(\n            os.path.join(\n                os.path.dirname(__file__),\n                '../../../src/notifiers/broadcast_notifier.py',\n            ),\n        )\n\n        # Run the script using subprocess\n        result = subprocess.run(\n            ['python', script_path],\n            capture_output=True, text=True,\n        )\n\n        # Print stdout and stderr for debugging\n        print('STDOUT:', result.stdout)\n        print('STDERR:', result.stderr)\n\n        # Assert that the script runs without errors\n        self.assertEqual(\n            result.returncode, 0,\n            'Script exited with a non-zero status.',\n        )\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/examples/YOLO_server_api/backend/user_operation_test.py", "content": "from __future__ import annotations\n\nimport unittest\nfrom unittest.mock import AsyncMock\nfrom unittest.mock import patch\n\nfrom sqlalchemy import select\nfrom sqlalchemy.ext.asyncio import async_sessionmaker\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.ext.asyncio import create_async_engine\n\nfrom examples.YOLO_server_api.backend.models import Base\nfrom examples.YOLO_server_api.backend.models import User\nfrom examples.YOLO_server_api.backend.user_operation import add_user\nfrom examples.YOLO_server_api.backend.user_operation import delete_user\nfrom examples.YOLO_server_api.backend.user_operation import (\n    set_user_active_status,\n)\nfrom examples.YOLO_server_api.backend.user_operation import update_password\nfrom examples.YOLO_server_api.backend.user_operation import update_username\n\n\nclass UserOperationTestCase(unittest.IsolatedAsyncioTestCase):\n    \"\"\"\n    Unit tests for user operations, including adding, deleting,\n    updating usernames, updating passwords, and setting user active status.\n\n    Attributes:\n        TEST_DATABASE_URL (str): The URL for the test database.\n        test_engine (AsyncEngine): The test database engine.\n        test_sessionmaker (sessionmaker): Session maker for\n            creating database sessions.\n        session (AsyncSession): The current database session.\n    \"\"\"\n\n    async def asyncSetUp(self) -> None:\n        \"\"\"\n        Set up the test environment before each test by configuring an\n        in-memory SQLite database and creating the required tables.\n        \"\"\"\n        self.TEST_DATABASE_URL: str = 'sqlite+aiosqlite:///:memory:'\n        self.test_engine = create_async_engine(\n            self.TEST_DATABASE_URL, echo=False,\n        )\n        self.test_sessionmaker = async_sessionmaker(\n            bind=self.test_engine,\n            class_=AsyncSession,\n            expire_on_commit=False,\n        )\n        self.session: AsyncSession = self.test_sessionmaker()\n\n        # Create the database tables\n        async with self.test_engine.begin() as conn:\n            await conn.run_sync(Base.metadata.create_all)\n\n    async def asyncTearDown(self) -> None:\n        \"\"\"\n        Clean up the test environment after each test by dropping all tables\n        and closing the session and engine.\n        \"\"\"\n        async with self.test_engine.begin() as conn:\n            await conn.run_sync(Base.metadata.drop_all)\n        await self.session.close()\n        await self.test_engine.dispose()\n\n    async def test_add_user_success(self) -> None:\n        \"\"\"\n        Test that a user can be added successfully.\n        \"\"\"\n        result = await add_user(\n            'testuser', 'password123', 'user', self.session,\n        )\n        self.assertTrue(result['success'])\n        self.assertEqual(\n            result['message'],\n            \"User 'testuser' added successfully.\",\n        )\n\n        stmt = select(User).where(User.username == 'testuser')\n        execution_result = await self.session.execute(stmt)\n        user = execution_result.scalars().first()\n        self.assertIsNotNone(user)\n        if user:\n            self.assertTrue(await user.check_password('password123'))\n\n    async def test_add_user_duplicate_username(self) -> None:\n        \"\"\"\n        Test that adding a user with a duplicate username returns an error.\n        \"\"\"\n        await add_user('testuser', 'password123', 'user', self.session)\n        result = await add_user(\n            'testuser', 'password456', 'user', self.session,\n        )\n        self.assertFalse(result['success'])\n        self.assertEqual(result['error'], 'IntegrityError')\n        self.assertEqual(\n            result['message'],\n            \"Username 'testuser' already exists.\",\n        )\n\n    async def test_delete_user_success(self) -> None:\n        \"\"\"\n        Test that a user can be deleted successfully.\n        \"\"\"\n        await add_user('testuser', 'password123', 'user', self.session)\n        result = await delete_user('testuser', self.session)\n        self.assertTrue(result['success'])\n        self.assertEqual(\n            result['message'],\n            \"User 'testuser' deleted successfully.\",\n        )\n\n        stmt = select(User).where(User.username == 'testuser')\n        execution_result = await self.session.execute(stmt)\n        user = execution_result.scalars().first()\n        self.assertIsNone(user)\n\n    async def test_delete_user_not_found(self) -> None:\n        \"\"\"\n        Test that deleting a non-existent user returns an error.\n        \"\"\"\n        result = await delete_user('nonexistentuser', self.session)\n        self.assertFalse(result['success'])\n        self.assertEqual(result['error'], 'NotFound')\n        self.assertEqual(\n            result['message'],\n            \"User 'nonexistentuser' not found.\",\n        )\n\n    async def test_update_username_success(self) -> None:\n        \"\"\"\n        Test that a user's username can be updated successfully.\n        \"\"\"\n        await add_user('oldusername', 'password123', 'user', self.session)\n        result = await update_username(\n            'oldusername', 'newusername', self.session,\n        )\n        self.assertTrue(result['success'])\n        self.assertEqual(\n            result['message'],\n            \"Username updated from 'oldusername' to 'newusername'.\",\n        )\n\n        stmt = select(User).where(User.username == 'newusername')\n        execution_result = await self.session.execute(stmt)\n        user = execution_result.scalars().first()\n        self.assertIsNotNone(user)\n\n    async def test_update_username_not_found(self) -> None:\n        \"\"\"\n        Test that updating a non-existent user's username returns an error.\n        \"\"\"\n        result = await update_username(\n            'nonexistentuser', 'newusername', self.session,\n        )\n        self.assertFalse(result['success'])\n        self.assertEqual(result['error'], 'NotFound')\n        self.assertEqual(\n            result['message'],\n            \"User 'nonexistentuser' not found.\",\n        )\n\n    async def test_update_password_success(self) -> None:\n        \"\"\"\n        Test that a user's password can be updated successfully.\n        \"\"\"\n        await add_user('testuser', 'password123', 'user', self.session)\n        result = await update_password(\n            'testuser', 'newpassword123', self.session,\n        )\n        self.assertTrue(result['success'])\n        self.assertEqual(\n            result['message'],\n            \"Password updated successfully for user 'testuser'.\",\n        )\n\n        stmt = select(User).where(User.username == 'testuser')\n        execution_result = await self.session.execute(stmt)\n        user = execution_result.scalars().first()\n        if user:\n            self.assertTrue(await user.check_password('newpassword123'))\n\n    async def test_update_password_user_not_found(self) -> None:\n        \"\"\"\n        Test that updating the password of a non-existent user\n        returns an error.\n        \"\"\"\n        result = await update_password(\n            'nonexistentuser', 'newpassword123', self.session,\n        )\n        self.assertFalse(result['success'])\n        self.assertEqual(result['error'], 'NotFound')\n        self.assertEqual(\n            result['message'],\n            \"User 'nonexistentuser' not found.\",\n        )\n\n    async def test_set_user_active_status_success(self) -> None:\n        \"\"\"\n        Test that a user's active status can be updated successfully.\n        \"\"\"\n        await add_user('testuser', 'password123', 'user', self.session)\n        result = await set_user_active_status(\n            'testuser', is_active=False, db=self.session,\n        )\n        self.assertTrue(result['success'])\n        self.assertEqual(\n            result['message'],\n            \"User 'testuser' is now inactive.\",\n        )\n\n        stmt = select(User).where(User.username == 'testuser')\n        execution_result = await self.session.execute(stmt)\n        user = execution_result.scalars().first()\n        if user:\n            self.assertFalse(user.is_active)\n\n    async def test_set_user_active_status_user_not_found(self) -> None:\n        \"\"\"\n        Test that updating the active status of a non-existent user\n        returns an error.\n        \"\"\"\n        result = await set_user_active_status(\n            'nonexistentuser', is_active=True, db=self.session,\n        )\n        self.assertFalse(result['success'])\n        self.assertEqual(result['error'], 'NotFound')\n        self.assertEqual(\n            result['message'],\n            \"User 'nonexistentuser' not found.\",\n        )\n\n    async def test_add_user_unknown_error(self) -> None:\n        \"\"\"\n        Test that add_user returns the correct error message\n        when an unknown exception occurs.\n        \"\"\"\n        with patch.object(\n            self.session,\n            'commit',\n            new_callable=AsyncMock,\n        ) as mock_commit:\n            mock_commit.side_effect = Exception('Unknown error')\n            result = await add_user(\n                'testuser',\n                'password123',\n                'user',\n                self.session,\n            )\n            self.assertFalse(result['success'])\n            self.assertEqual(result['error'], 'UnknownError')\n            self.assertIn('Failed to add user', result['message'])\n\n    async def test_delete_user_unknown_error(self) -> None:\n        \"\"\"\n        Test that delete_user returns the correct error message\n        when an unknown exception occurs.\n        \"\"\"\n        await add_user('testuser', 'password123', 'user', self.session)\n        with patch.object(\n            self.session,\n            'commit',\n            new_callable=AsyncMock,\n        ) as mock_commit:\n            mock_commit.side_effect = Exception('Unknown error')\n            result = await delete_user('testuser', self.session)\n            self.assertFalse(result['success'])\n            self.assertEqual(result['error'], 'UnknownError')\n            self.assertIn('Failed to delete user', result['message'])\n\n    async def test_update_username_unknown_error(self) -> None:\n        \"\"\"\n        Test that update_username returns the correct error message\n        when an unknown exception occurs\n        \"\"\"\n        await add_user('testuser', 'password123', 'user', self.session)\n        with patch.object(\n            self.session,\n            'commit',\n            new_callable=AsyncMock,\n        ) as mock_commit:\n            mock_commit.side_effect = Exception('Unknown error')\n            result = await update_username(\n                'testuser',\n                'newusername',\n                self.session,\n            )\n            self.assertFalse(result['success'])\n            self.assertEqual(result['error'], 'UnknownError')\n            self.assertIn('Failed to update username', result['message'])\n\n    async def test_update_password_unknown_error(self) -> None:\n        \"\"\"\n        Test that update_password returns the correct error message\n        when an unknown exception occurs.\n        \"\"\"\n        await add_user('testuser', 'password123', 'user', self.session)\n        with patch.object(\n            self.session,\n            'commit',\n            new_callable=AsyncMock,\n        ) as mock_commit:\n            mock_commit.side_effect = Exception('Unknown error')\n            result = await update_password(\n                'testuser',\n                'newpassword123',\n                self.session,\n            )\n            self.assertFalse(result['success'])\n            self.assertEqual(result['error'], 'UnknownError')\n            self.assertIn('Failed to update password', result['message'])\n\n    async def test_set_user_active_status_unknown_error(self) -> None:\n        \"\"\"\n        Test that set_user_active_status returns the correct error message\n        when an unknown exception occurs.\n        \"\"\"\n        await add_user('testuser', 'password123', 'user', self.session)\n        with patch.object(\n            self.session,\n            'commit',\n            new_callable=AsyncMock,\n        ) as mock_commit:\n            mock_commit.side_effect = Exception('Unknown error')\n            result = await set_user_active_status(\n                'testuser',\n                is_active=True,\n                db=self.session,\n            )\n            self.assertFalse(result['success'])\n            self.assertEqual(result['error'], 'UnknownError')\n            self.assertIn('Failed to update active status', result['message'])\n\n    async def test_update_username_integrity_error(self) -> None:\n        \"\"\"\n        Test to return an IntegrityError\n        when updating username to an existing username.\n        \"\"\"\n        # Add two users\n        await add_user('user1', 'password123', 'user', self.session)\n        await add_user('user2', 'password123', 'user', self.session)\n\n        # Attempt to update username 'user1' to 'user2'\n        result = await update_username('user1', 'user2', self.session)\n        self.assertFalse(result['success'])\n        self.assertEqual(result['error'], 'IntegrityError')\n        self.assertEqual(result['message'], \"Username 'user2' already exists.\")\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/examples/streaming_web/backend/routes_test.py", "content": "from __future__ import annotations\n\nimport asyncio\nimport unittest\nfrom unittest.mock import AsyncMock\nfrom unittest.mock import MagicMock\nfrom unittest.mock import patch\n\nfrom fastapi import FastAPI\nfrom fastapi import WebSocketDisconnect\nfrom fastapi.testclient import TestClient\nfrom fastapi_limiter import FastAPILimiter\n\nfrom examples.streaming_web.backend.routes import rate_limiter_index\nfrom examples.streaming_web.backend.routes import rate_limiter_label\nfrom examples.streaming_web.backend.routes import register_routes\nfrom examples.streaming_web.backend.utils import RedisManager\n\n\nclass TestRoutes(unittest.IsolatedAsyncioTestCase):\n    def setUp(self) -> None:\n        \"\"\"\n        Set up the test environment before each test.\n        \"\"\"\n        self.app = FastAPI()\n\n        # Mock Redis instance with AsyncMock\n        self.redis_manager = RedisManager()\n        RedisManager.client = AsyncMock()  # Mock Redis client\n\n        # Register routes\n        register_routes(self.app)\n\n        # Initialise rate limiter with the Redis client\n        asyncio.run(FastAPILimiter.init(RedisManager.client))\n\n        # Mock rate limiter dependencies\n        async def mock_rate_limiter():\n            pass\n\n        self.app.dependency_overrides[rate_limiter_index] = mock_rate_limiter\n        self.app.dependency_overrides[rate_limiter_label] = mock_rate_limiter\n\n        # Create a test client for the FastAPI app\n        self.client = TestClient(self.app)\n\n    def tearDown(self) -> None:\n        \"\"\"\n        Clean up after each test.\n        \"\"\"\n        self.app.dependency_overrides.clear()\n        patch.stopall()\n\n    @patch(\n        'examples.streaming_web.backend.utils.'\n        'RedisManager.get_labels',\n        new_callable=AsyncMock,\n    )\n    def test_get_labels_success(self, mock_get_labels: AsyncMock):\n        \"\"\"\n        Test the /api/labels route to ensure it returns\n        correct labels on success.\n        \"\"\"\n        mock_get_labels.return_value = ['label1', 'label2']\n        response = self.client.get('/api/labels')\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.json(), {'labels': ['label1', 'label2']})\n\n    @patch(\n        'examples.streaming_web.backend.utils.RedisManager.get_labels',\n        new_callable=AsyncMock,\n    )\n    def test_get_labels_value_error(self, mock_get_labels: AsyncMock):\n        \"\"\"\n        Test the /api/labels route to ensure it handles ValueError.\n        \"\"\"\n        mock_get_labels.side_effect = ValueError('Invalid data')\n        response = self.client.get('/api/labels')\n        self.assertEqual(response.status_code, 400)\n        self.assertIn('Invalid data encountered', response.text)\n\n    @patch(\n        'examples.streaming_web.backend.utils.RedisManager.get_labels',\n        new_callable=AsyncMock,\n    )\n    def test_get_labels_key_error(self, mock_get_labels: AsyncMock):\n        \"\"\"\n        Test the /api/labels route to ensure it handles KeyError.\n        \"\"\"\n        mock_get_labels.side_effect = KeyError('missing_key')\n        response = self.client.get('/api/labels')\n        self.assertEqual(response.status_code, 404)\n        self.assertIn('Missing key encountered', response.text)\n\n    @patch(\n        'examples.streaming_web.backend.utils.RedisManager.get_labels',\n        new_callable=AsyncMock,\n    )\n    def test_get_labels_connection_error(self, mock_get_labels: AsyncMock):\n        \"\"\"\n        Test the /api/labels route to ensure it handles ConnectionError.\n        \"\"\"\n        mock_get_labels.side_effect = ConnectionError('DB connection failed')\n        response = self.client.get('/api/labels')\n        self.assertEqual(response.status_code, 503)\n        self.assertIn('Failed to connect to the database', response.text)\n\n    @patch(\n        'examples.streaming_web.backend.utils.RedisManager.get_labels',\n        new_callable=AsyncMock,\n    )\n    def test_get_labels_timeout_error(self, mock_get_labels: AsyncMock):\n        \"\"\"\n        Test the /api/labels route to ensure it handles TimeoutError.\n        \"\"\"\n        mock_get_labels.side_effect = TimeoutError('Timeout!')\n        response = self.client.get('/api/labels')\n        self.assertEqual(response.status_code, 504)\n        self.assertIn('Request timed out', response.text)\n\n    @patch(\n        'examples.streaming_web.backend.utils.RedisManager.get_labels',\n        new_callable=AsyncMock,\n    )\n    def test_get_labels_generic_error(self, mock_get_labels: AsyncMock):\n        \"\"\"\n        Test the /api/labels route to ensure it handles generic exceptions.\n        \"\"\"\n        mock_get_labels.side_effect = Exception('Unknown error')\n        response = self.client.get('/api/labels')\n        self.assertEqual(response.status_code, 500)\n        self.assertIn('Failed to fetch labels', response.text)\n\n    @patch(\n        'examples.streaming_web.backend.utils.RedisManager.get_labels',\n        new_callable=AsyncMock,\n    )\n    @patch(\n        'examples.streaming_web.backend.utils.RedisManager.get_keys_for_label',\n        new_callable=AsyncMock,\n    )\n    def test_label_page_found(\n        self, mock_get_keys: AsyncMock, mock_get_labels: AsyncMock,\n    ):\n        \"\"\"\n        Test the WebSocket route for an existing label.\n        \"\"\"\n        mock_get_labels.return_value = ['label1', 'label2']\n        mock_get_keys.return_value = ['stream_frame:label1_Cam0']\n\n        # Call the API endpoint\n        response = self.client.get('/api/labels')\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.json(), {'labels': ['label1', 'label2']})\n\n    @patch(\n        'examples.streaming_web.backend.utils.'\n        'WebhookHandler.process_webhook_events',\n        new_callable=AsyncMock,\n    )\n    def test_webhook_all_skipped(\n        self,\n        mock_process_webhook_events: AsyncMock,\n    ):\n        \"\"\"\n        Test the webhook route when all events are skipped.\n\n        Args:\n            mock_process_webhook_events: Mock of\n                the process_webhook_events method\n        \"\"\"\n        # Mock process_webhook_events to return all skipped responses\n        mock_process_webhook_events.return_value = [\n            {'status': 'skipped'},\n            {'status': 'skipped'},\n        ]\n\n        body = {'events': ['event1', 'event2']}\n        response = self.client.post('/api/webhook', json=body)\n\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(\n            response.json(),\n            {'status': 'skipped', 'message': 'All events skipped.'},\n        )\n\n    @patch(\n        'examples.streaming_web.backend.utils.'\n        'WebhookHandler.process_webhook_events',\n        new_callable=AsyncMock,\n    )\n    def test_webhook_partial_error(\n        self,\n        mock_process_webhook_events: AsyncMock,\n    ):\n        \"\"\"\n        Test the webhook route when some events fail.\n\n        Args:\n            mock_process_webhook_events: Mock of\n                the process_webhook_events method\n        \"\"\"\n        # Mock process_webhook_events to return mixed responses\n        mock_process_webhook_events.return_value = [\n            {'status': 'ok'},\n            {'status': 'error'},\n        ]\n\n        body = {'events': ['event1', 'event2']}\n        response = self.client.post('/api/webhook', json=body)\n\n        self.assertEqual(response.status_code, 207)\n        self.assertEqual(\n            response.json(),\n            {\n                'status': 'partial_error',\n                'responses': [\n                    {'status': 'ok'},\n                    {'status': 'error'},\n                ],\n            },\n        )\n\n    @patch(\n        'examples.streaming_web.backend.utils.'\n        'WebhookHandler.process_webhook_events',\n        new_callable=AsyncMock,\n    )\n    def test_webhook_success(\n        self,\n        mock_process_webhook_events: AsyncMock,\n    ):\n        \"\"\"\n        Test the webhook route when all events are successfully processed.\n\n        Args:\n            mock_process_webhook_events: Mock of\n                the process_webhook_events method\n        \"\"\"\n        # Mock process_webhook_events to return all successful responses\n        mock_process_webhook_events.return_value = [\n            {'status': 'ok'},\n            {'status': 'ok'},\n        ]\n\n        body = {'events': ['event1', 'event2']}\n        response = self.client.post('/api/webhook', json=body)\n\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(\n            response.json(),\n            {\n                'status': 'ok', 'responses': [\n                    {'status': 'ok'},\n                    {'status': 'ok'},\n                ],\n            },\n        )\n\n    @patch(\n        'examples.streaming_web.backend.utils.'\n        'WebhookHandler.process_webhook_events',\n        new_callable=AsyncMock,\n    )\n    def test_webhook_internal_error(\n        self,\n        mock_process_webhook_events: AsyncMock,\n    ):\n        \"\"\"\n        Test the webhook route when an exception occurs during processing.\n\n        Args:\n            mock_process_webhook_events: Mock of\n                the process_webhook_events method.\n        \"\"\"\n        # Mock process_webhook_events to raise an exception\n        mock_process_webhook_events.side_effect = Exception('Unexpected error')\n\n        body = {'events': ['event1', 'event2']}\n        response = self.client.post('/api/webhook', json=body)\n\n        self.assertEqual(response.status_code, 500)\n        self.assertIn('Webhook processing failed', response.text)\n\n    @patch('examples.streaming_web.backend.utils.Utils.verify_localhost')\n    @patch('examples.streaming_web.backend.utils.Utils.load_configuration')\n    def test_get_config(\n        self,\n        mock_load_config: MagicMock,\n        mock_verify: MagicMock,\n    ):\n        \"\"\"\n        Test the GET /api/config endpoint.\n        \"\"\"\n        mock_verify.return_value = True\n        mock_load_config.return_value = {'setting': 'value'}\n        response = self.client.get('/api/config')\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.json(), {'config': {'setting': 'value'}})\n        mock_verify.assert_called_once()\n\n    @patch('examples.streaming_web.backend.utils.Utils.verify_localhost')\n    @patch('examples.streaming_web.backend.utils.Utils.update_configuration')\n    async def test_update_config(\n        self,\n        mock_update_config: AsyncMock,\n        mock_verify: MagicMock,\n    ):\n        \"\"\"\n        Test the POST /api/config endpoint.\n        \"\"\"\n        mock_verify.return_value = True\n        mock_update_config.return_value = {'setting': 'new_value'}\n\n        response = self.client.post(\n            '/api/config', json={'config': {'setting': 'new_value'}},\n        )\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(\n            response.json(), {\n                'status': 'Configuration updated successfully.',\n                'config': {'setting': 'new_value'},\n            },\n        )\n        mock_verify.assert_called_once()\n\n    @patch('examples.streaming_web.backend.utils.Utils.verify_localhost')\n    @patch('examples.streaming_web.backend.utils.Utils.update_configuration')\n    def test_update_config_failure(\n        self,\n        mock_update_config: MagicMock,\n        mock_verify: MagicMock,\n    ):\n        \"\"\"\n        Test the POST /api/config endpoint when update fails.\n        \"\"\"\n        mock_verify.return_value = True\n        mock_update_config.side_effect = Exception('Update failed')\n\n        response = self.client.post(\n            '/api/config', json={'config': {'setting': 'fail'}},\n        )\n        self.assertEqual(response.status_code, 500)\n        self.assertIn('Failed to update configuration', response.text)\n\n    @patch(\n        'examples.streaming_web.backend.utils.RedisManager.get_keys_for_label',\n        new_callable=AsyncMock,\n    )\n    @patch(\n        'examples.streaming_web.backend.utils.'\n        'RedisManager.fetch_latest_frames',\n        new_callable=AsyncMock,\n    )\n    def test_websocket_label_stream_no_keys(\n        self,\n        mock_fetch_frames: AsyncMock,\n        mock_get_keys: AsyncMock,\n    ):\n        \"\"\"\n        Test the /api/ws/labels/{label} websocket when no keys are found.\n        \"\"\"\n        mock_get_keys.return_value = []\n\n        with self.client.websocket_connect(\n            '/api/ws/labels/mylabel',\n        ) as websocket:\n            data = websocket.receive_json()\n            self.assertIn('error', data)\n            self.assertIn('No keys found for label', data['error'])\n\n    @patch(\n        'examples.streaming_web.backend.utils.RedisManager.get_keys_for_label',\n        new_callable=AsyncMock,\n    )\n    @patch(\n        'examples.streaming_web.backend.utils.'\n        'RedisManager.fetch_latest_frames',\n        new_callable=AsyncMock,\n    )\n    @patch(\n        'examples.streaming_web.backend.utils.Utils.send_frames',\n        new_callable=AsyncMock,\n    )\n    def test_websocket_label_stream_with_data(\n        self,\n        mock_send_frames: AsyncMock,\n        mock_fetch_frames: AsyncMock,\n        mock_get_keys: AsyncMock,\n    ):\n        \"\"\"\n        Test the /api/ws/labels/{label} websocket with some data.\n        \"\"\"\n        mock_get_keys.return_value = ['stream_frame:label1_Cam0']\n        mock_fetch_frames.side_effect = [\n            [{'id': '1', 'data': 'frame1'}],\n            WebSocketDisconnect(),\n        ]\n\n        # Test that we can connect and receive data until disconnect\n        try:\n            with self.client.websocket_connect('/api/ws/labels/label1'):\n                # Mock the WebSocket connection\n                pass\n\n        except WebSocketDisconnect:\n            pass\n\n        mock_send_frames.assert_called()  # Ensure frames were sent\n\n    @patch(\n        'examples.streaming_web.backend.utils.Utils.encode',\n        side_effect=lambda x: x,\n    )\n    @patch(\n        (\n            'examples.streaming_web.backend.utils.'\n            'RedisManager.fetch_latest_frame_for_key'\n        ),\n        new_callable=AsyncMock,\n    )\n    def test_websocket_stream_with_data(\n        self,\n        mock_fetch_frame: AsyncMock,\n        mock_encode: MagicMock,\n    ):\n        \"\"\"\n        Test the /api/ws/stream/{label}/{key} websocket with data.\n        \"\"\"\n        # Mock the fetch_latest_frame_for_key method to return a frame\n        # and then return None to simulate no new data available\n        mock_fetch_frame.side_effect = [\n            {'id': '1', 'frame': 'data'},\n            None,\n        ]\n\n        with self.client.websocket_connect(\n            '/api/ws/stream/label1/key1',\n        ) as websocket:\n            # First receive: should get the data frame\n            data = websocket.receive_json()\n            self.assertIn('id', data)\n            self.assertEqual(data['id'], '1')\n            self.assertEqual(data['frame'], 'data')\n\n            # Second receive: should get an error message\n            data = websocket.receive_json()\n            self.assertIn('error', data)\n            self.assertEqual(data['error'], 'No new data available')\n            # Do not need to close the connection, the server will close it\n\n    @patch(\n        'examples.streaming_web.backend.utils.Utils.encode',\n        side_effect=lambda x: x,\n    )\n    @patch(\n        'examples.streaming_web.backend.utils.'\n        'RedisManager.fetch_latest_frame_for_key',\n        new_callable=AsyncMock,\n    )\n    def test_websocket_stream_error_handling(\n        self,\n        mock_fetch_frame: AsyncMock,\n        mock_encode: MagicMock,\n    ):\n        \"\"\"\n        Test that /api/ws/stream/{label}/{key} handles unexpected exceptions.\n        \"\"\"\n        mock_fetch_frame.side_effect = Exception('Some error')\n\n        with self.assertRaises(WebSocketDisconnect):\n            with self.client.websocket_connect(\n                '/api/ws/stream/label1/key1',\n            ) as websocket:\n                # Once the connection is established, the server will close it\n                websocket.receive_json()\n\n    # def test_upload_file_successful(self):\n    #     \"\"\"\n    #     Test the upload route to ensure it returns a successful response.\n    #     \"\"\"\n    #     file_content = b'fake image data'\n    #     files = {'file': ('test_image.png', file_content, 'image/png')}\n    #     response = self.client.post('/api/upload', files=files)\n    #     self.assertEqual(response.status_code, 200)\n    #     self.assertIn('url', response.json())\n\n    # def test_upload_file_missing_filename(self):\n    #     \"\"\"\n    #     Test the upload route to ensure it returns a 422 error\n    #     when the filename is missing.\n    #     \"\"\"\n    #     # FastAPI automatically raises a 422 for missing file validation\n    #     files = {'file': ('', b'data', 'image/png')}\n    #     response = self.client.post('/api/upload', files=files)\n    #     self.assertEqual(response.status_code, 422)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/examples/YOLO_data_augmentation/visualise_bounding_boxes_test.py", "content": "from __future__ import annotations\n\nimport unittest\nfrom typing import Any\nfrom unittest.mock import mock_open\nfrom unittest.mock import patch\n\nimport numpy as np\n\nfrom examples.YOLO_data_augmentation.visualise_bounding_boxes import (\n    BoundingBoxVisualiser,\n)\nfrom examples.YOLO_data_augmentation.visualise_bounding_boxes import main\n\n\nclass TestBoundingBoxVisualiser(unittest.TestCase):\n    def setUp(self) -> None:\n        self.image_path: str = (\n            'tests/cv_dataset/images/'\n            '-_jpeg.rf.3e98d2f5b90e0b1459e15f570a433459.jpg'\n        )\n        self.label_path: str = (\n            'tests/cv_dataset/labels/'\n            '-_jpeg.rf.3e98d2f5b90e0b1459e15f570a433459.txt'\n        )\n        self.class_names: list[str] = [\n            'Hardhat', 'Mask', 'NO-Hardhat', 'NO-Mask', 'NO-Safety Vest',\n            'Person', 'Safety Cone', 'Safety Vest', 'machinery', 'vehicle',\n        ]\n        self.visualiser: BoundingBoxVisualiser | None = None\n\n    def tearDown(self) -> None:\n        \"\"\"\n        Clean up resources after each test case.\n        \"\"\"\n        if self.visualiser is not None:\n            del self.visualiser\n        # Reset the visualiser to None\n        self.visualiser = None\n\n    @patch(\n        'examples.YOLO_data_augmentation.'\n        'visualise_bounding_boxes.cv2.imread',\n        return_value=None,\n    )\n    def test_image_loading_failure(self, mock_imread: Any) -> None:\n        \"\"\"\n        Test if ValueError is raised when image could not be loaded.\n        \"\"\"\n        with self.assertRaises(ValueError) as context:\n            _ = BoundingBoxVisualiser(\n                self.image_path, self.label_path, self.class_names,\n            )\n\n        self.assertEqual(\n            str(context.exception),\n            'Image could not be loaded. Please check the image path.',\n        )\n\n    @patch(\n        'pathlib.Path.open',\n        new_callable=mock_open,\n        read_data='0 0.5 0.5 0.5 0.5\\n',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.'\n        'visualise_bounding_boxes.cv2.imread',\n    )\n    def test_draw_bounding_boxes(\n        self, mock_imread: Any, mock_file: Any,\n    ) -> None:\n        \"\"\"\n        Test drawing bounding boxes on an image.\n        \"\"\"\n        # Mock the image as a numpy array with shape (100, 100, 3)\n        mock_image: np.ndarray = np.zeros((100, 100, 3), dtype=np.uint8)\n        mock_imread.return_value = mock_image\n\n        with patch('pathlib.Path.open', mock_file):\n            self.visualiser = BoundingBoxVisualiser(\n                self.image_path, self.label_path, self.class_names,\n            )\n\n            self.visualiser.draw_bounding_boxes()\n\n        # Assert that rectangle was called correctly\n        mock_imread.assert_called_once_with(self.image_path)\n\n        # Ensure 'r' mode is passed correctly\n        mock_file.assert_called_once_with('r')\n\n    @patch(\n        'examples.YOLO_data_augmentation.'\n        'visualise_bounding_boxes.cv2.imwrite',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.'\n        'visualise_bounding_boxes.cv2.imread',\n    )\n    def test_save_image(\n        self, mock_imread: Any, mock_imwrite: Any,\n    ) -> None:\n        \"\"\"\n        Test saving the image with drawn bounding boxes.\n        \"\"\"\n        # Mock the image as a numpy array with shape (100, 100, 3)\n        mock_image: np.ndarray = np.zeros((100, 100, 3), dtype=np.uint8)\n        mock_imread.return_value = mock_image\n\n        self.visualiser = BoundingBoxVisualiser(\n            self.image_path, self.label_path, self.class_names,\n        )\n\n        self.visualiser.draw_bounding_boxes()\n        self.visualiser.save_or_display_image(\n            output_path='output.jpg', save=True,\n        )\n\n        mock_imwrite.assert_called_once_with('output.jpg', mock_image)\n\n    @patch(\n        'examples.YOLO_data_augmentation.'\n        'visualise_bounding_boxes.plt.show',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.'\n        'visualise_bounding_boxes.plt.imshow',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.'\n        'visualise_bounding_boxes.cv2.imread',\n    )\n    def test_display_image(\n        self, mock_imread: Any, mock_imshow: Any, mock_show: Any,\n    ) -> None:\n        \"\"\"\n        Test displaying the image with drawn bounding boxes.\n        \"\"\"\n        # Mock the image as a numpy array with shape (100, 100, 3)\n        mock_image: np.ndarray = np.zeros((100, 100, 3), dtype=np.uint8)\n        mock_imread.return_value = mock_image\n\n        self.visualiser = BoundingBoxVisualiser(\n            self.image_path, self.label_path, self.class_names,\n        )\n\n        self.visualiser.draw_bounding_boxes()\n        self.visualiser.save_or_display_image(\n            output_path='output.jpg', save=False,\n        )\n\n        mock_imshow.assert_called_once()\n        mock_show.assert_called_once()\n\n    @patch(\n        'sys.argv',\n        [\n            'visualise_bounding_boxes.py', '--image',\n            'image.jpg', '--label', 'label.txt', '--save',\n        ],\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.visualise_bounding_boxes.'\n        'BoundingBoxVisualiser.__init__',\n        return_value=None,\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.visualise_bounding_boxes.'\n        'BoundingBoxVisualiser.draw_bounding_boxes',\n    )\n    @patch(\n        'examples.YOLO_data_augmentation.visualise_bounding_boxes.'\n        'BoundingBoxVisualiser.save_or_display_image',\n    )\n    def test_main(\n        self, mock_save: Any, mock_draw: Any, mock_init: Any,\n    ) -> None:\n        \"\"\"\n        Test the main function by simulating command-line arguments.\n        \"\"\"\n        main()\n\n        mock_init.assert_called_once_with(\n            'image.jpg', 'label.txt', [\n                'Hardhat', 'Mask', 'NO-Hardhat', 'NO-Mask', 'NO-Safety Vest',\n                'Person', 'Safety Cone', 'Safety Vest', 'machinery', 'vehicle',\n            ],\n        )\n        mock_draw.assert_called_once()\n        mock_save.assert_called_once_with('visualised_image.jpg', True)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/examples/streaming_web/backend/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/examples/line_chatbot/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/src/stream_capture_test.py", "content": "from __future__ import annotations\n\nimport argparse\nimport sys\nimport time\nimport unittest\nfrom typing import cast\nfrom unittest import IsolatedAsyncioTestCase\nfrom unittest.mock import AsyncMock\nfrom unittest.mock import MagicMock\nfrom unittest.mock import patch\n\nimport numpy as np\n\nfrom src.stream_capture import main as stream_capture_main\nfrom src.stream_capture import StreamCapture\n\n\nclass TestStreamCapture(IsolatedAsyncioTestCase):\n    \"\"\"\n    Tests for the StreamCapture class.\n    \"\"\"\n\n    def setUp(self) -> None:\n        \"\"\"Set up a StreamCapture instance for use in tests.\"\"\"\n        # Initialise StreamCapture instance with a presumed stream URL\n        self.stream_capture: StreamCapture = StreamCapture(\n            'http://example.com/stream',\n        )\n\n    @patch('cv2.VideoCapture')\n    async def test_initialise_stream_success(\n        self,\n        mock_video_capture: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test that the stream is successfully initialised.\n\n        Args:\n            mock_video_capture (MagicMock): Mock for cv2.VideoCapture.\n        \"\"\"\n        # Mock VideoCapture object's isOpened method to\n        # return True, indicating the stream opened successfully\n        mock_video_capture.return_value.isOpened.return_value = True\n\n        # Call initialise_stream method to initialise the stream\n        await self.stream_capture.initialise_stream(\n            self.stream_capture.stream_url,\n        )\n\n        # Assert that the cap object is successfully initialised\n        self.assertIsNotNone(self.stream_capture.cap)\n\n        # Verify that VideoCapture was called correctly\n        mock_video_capture.assert_called_once_with(\n            self.stream_capture.stream_url,\n        )\n\n        # Release resources\n        await self.stream_capture.release_resources()\n\n    @patch('cv2.VideoCapture')\n    async def test_execute_capture_cap_is_none(\n        self,\n        mock_video_capture: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test that the generator reinitialises `self.cap` if it is `None`.\n\n        Args:\n            mock_video_capture (MagicMock): Mock object for `cv2.VideoCapture`.\n\n        Raises:\n            AssertionError: If the test conditions are not met.\n        \"\"\"\n        # Mock `isOpened()` to always return True\n        mock_video_capture.return_value.isOpened.return_value = True\n\n        # Set `read()` to return two frames successfully\n        mock_frame = MagicMock(name='MockFrame')\n        mock_video_capture.return_value.read.side_effect = [\n            (True, mock_frame),  # First iteration\n            (True, mock_frame),  # Second iteration\n        ]\n\n        # Set `capture_interval` to 0\n        # to allow immediate yielding during each iteration\n        self.stream_capture.capture_interval = 0\n\n        # Start the asynchronous generator for `execute_capture`\n        generator = self.stream_capture.execute_capture()\n\n        # First call to `__anext__`: `self.cap` should not be `None`\n        frame1, ts1 = await generator.__anext__()\n        self.assertIsNotNone(frame1, 'First frame should not be None')\n\n        # Manually set `self.cap` to `None`\n        # to trigger the `if self.cap is None` branch\n        self.stream_capture.cap = None\n\n        # Second call to `__anext__`: The branch `if self.cap is None`\n        # should execute and reinitialise `self.cap`\n        frame2, ts2 = await generator.__anext__()\n        self.assertIsNotNone(\n            frame2, 'Second frame should not be None after reinitialisation',\n        )\n\n        # Close the generator to avoid warnings\n        await generator.aclose()\n\n        # Verify that `cv2.VideoCapture` was called twice\n        mock_video_capture.assert_called_with(self.stream_capture.stream_url)\n\n    @patch('cv2.VideoCapture')\n    @patch.object(StreamCapture, 'capture_generic_frames')\n    async def test_execute_capture_switch_to_generic(\n        self,\n        mock_capture_generic: MagicMock,\n        mock_video_capture: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test that the generator switches to `capture_generic_frames`\n        after 5 consecutive failures.\n\n        Args:\n            mock_capture_generic (MagicMock): Mock for\n                the `capture_generic_frames` method.\n            mock_video_capture (MagicMock): Mock for\n                the `cv2.VideoCapture` object.\n\n        Returns:\n            None\n        \"\"\"\n        # Mock VideoCapture object's read method to return False 5 times\n        mock_video_capture.return_value.read.side_effect = [(False, None)] * 6\n        mock_video_capture.return_value.isOpened.return_value = True\n\n        # Set capture interval to 0 to avoid delays during the test.\n        self.stream_capture.successfully_captured = False\n\n        # Start the coroutine generator.\n        async def mock_generic():\n            for i in range(2):\n                frame_mock = MagicMock()\n                # Explicitly set frame name\n                frame_mock.name = f\"GenericFrame{i}\"\n                yield (frame_mock, float(i))\n        mock_capture_generic.side_effect = mock_generic\n\n        # Set capture interval to 0 to avoid delays during the test.\n        self.stream_capture.capture_interval = 0\n\n        # Start the coroutine generator.\n        gen = self.stream_capture.execute_capture()\n\n        # First frame: Validate first frame\n        # from `capture_generic_frames`.\n        generic_frame_0, ts_0 = await gen.__anext__()\n        generic_frame_0 = cast(MagicMock, generic_frame_0)\n        self.assertEqual(generic_frame_0.name, 'GenericFrame0')\n\n        # Second frame: Validate subsequent\n        # frame from `generic_frames`.\n        generic_frame_1, ts_1 = await gen.__anext__()\n        generic_frame_1 = cast(MagicMock, generic_frame_1)\n        self.assertEqual(generic_frame_1.name, 'GenericFrame1')\n\n        # After `generic_frames` iteration completes,\n        # the generator should raise `StopAsyncIteration`.\n        with self.assertRaises(StopAsyncIteration):\n            await gen.__anext__()\n\n    @patch('cv2.VideoCapture')\n    @patch('asyncio.sleep', new_callable=AsyncMock)\n    async def test_initialise_stream_retry(\n        self,\n        mock_sleep: AsyncMock,\n        mock_video_capture: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test that the stream initialisation retries if it fails initially.\n\n        Args:\n            mock_sleep (AsyncMock): Mock for asyncio.sleep.\n            mock_video_capture (MagicMock): Mock for cv2.VideoCapture.\n        \"\"\"\n        # Mock VideoCapture object's isOpened method to\n        # return False on the first call and True on the second\n        instance = mock_video_capture.return_value\n        instance.isOpened.side_effect = [False, True]\n\n        # Call initialise_stream method to simulate retry mechanism\n        await self.stream_capture.initialise_stream(\n            self.stream_capture.stream_url,\n        )\n\n        # Assert that the cap object is eventually successfully initialised\n        self.assertIsNotNone(self.stream_capture.cap)\n\n        # Verify that sleep method was called once to wait before retrying\n        mock_sleep.assert_called_once_with(5)\n\n    async def test_release_resources(self) -> None:\n        \"\"\"\n        Test that resources are released correctly.\n        \"\"\"\n        # Initialise StreamCapture instance and mock cap object\n        stream_capture: StreamCapture = StreamCapture('test_stream_url')\n        stream_capture.cap = MagicMock()\n\n        # Call release_resources method to release resources\n        await stream_capture.release_resources()\n\n        # Assert that cap object is set to None\n        self.assertIsNone(stream_capture.cap)\n\n    @patch('cv2.VideoCapture')\n    @patch('cv2.Mat')\n    @patch('time.sleep', return_value=None)\n    async def test_execute_capture(\n        self,\n        mock_sleep: MagicMock,\n        mock_mat: MagicMock,\n        mock_video_capture: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test that frames are captured and returned with a timestamp.\n\n        Args:\n            mock_sleep (MagicMock): Mock for time.sleep.\n            mock_video_capture (MagicMock): Mock for cv2.VideoCapture.\n        \"\"\"\n        # Mock VideoCapture object's read method to\n        # return a frame and True indicating successful read\n        mock_video_capture.return_value.read.return_value = (True, mock_mat)\n        mock_video_capture.return_value.isOpened.return_value = True\n\n        # Execute capture frame generator and get the first frame and timestamp\n        generator = self.stream_capture.execute_capture()\n        frame, timestamp = await generator.__anext__()\n\n        # Assert that the captured frame is not None\n        # and the timestamp is a float\n        self.assertIsNotNone(frame)\n        self.assertIsInstance(timestamp, float)\n\n        # Release resources\n        await self.stream_capture.release_resources()\n\n    @patch('speedtest.Speedtest')\n    def test_check_internet_speed(self, mock_speedtest: MagicMock) -> None:\n        \"\"\"\n        Test that internet speed is correctly checked and returned.\n\n        Args:\n            mock_speedtest (MagicMock): Mock for speedtest.Speedtest.\n        \"\"\"\n        # Mock Speedtest object's download and upload methods\n        # to return download and upload speeds\n        mock_speedtest.return_value.download.return_value = 50_000_000\n        mock_speedtest.return_value.upload.return_value = 10_000_000\n\n        # Check internet speed and assert that\n        # the returned speeds are correct\n        download_speed, upload_speed = (\n            self.stream_capture.check_internet_speed()\n        )\n        self.assertEqual(download_speed, 50.0)\n        self.assertEqual(upload_speed, 10.0)\n\n    @patch('streamlink.streams')\n    def test_select_quality_based_on_speed_high_speed(\n        self,\n        mock_streams: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test that the highest quality stream is selected\n        for high internet speed.\n\n        Args:\n            mock_streams (MagicMock): Mock for streamlink.streams.\n        \"\"\"\n        # Mock streamlink to return different quality streams\n        mock_streams.return_value = {\n            'best': MagicMock(url='http://best.stream'),\n            '1080p': MagicMock(url='http://1080p.stream'),\n            '720p': MagicMock(url='http://720p.stream'),\n            '480p': MagicMock(url='http://480p.stream'),\n        }\n\n        # Mock internet speed check result\n        with patch.object(\n            self.stream_capture,\n            'check_internet_speed',\n            return_value=(20, 5),\n        ):\n            # Select the best stream quality based on internet speed\n            selected_quality = (\n                self.stream_capture.select_quality_based_on_speed()\n            )\n            self.assertEqual(selected_quality, 'http://best.stream')\n\n    @patch('streamlink.streams')\n    def test_select_quality_based_on_speed_medium_speed(\n        self,\n        mock_streams: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test that an appropriate quality stream is selected\n        for medium internet speed.\n\n        Args:\n            mock_streams (MagicMock): Mock for streamlink.streams.\n        \"\"\"\n        # Mock streamlink to return medium quality streams\n        mock_streams.return_value = {\n            '720p': MagicMock(url='http://720p.stream'),\n            '480p': MagicMock(url='http://480p.stream'),\n            '360p': MagicMock(url='http://360p.stream'),\n        }\n\n        # Mock internet speed check result\n        with patch.object(\n            self.stream_capture,\n            'check_internet_speed',\n            return_value=(7, 5),\n        ):\n            # Select the appropriate stream quality based on internet speed\n            selected_quality = (\n                self.stream_capture.select_quality_based_on_speed()\n            )\n            self.assertEqual(selected_quality, 'http://720p.stream')\n\n    @patch('streamlink.streams')\n    def test_select_quality_based_on_speed_low_speed(\n        self,\n        mock_streams: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test that a lower quality stream is selected for low internet speed.\n\n        Args:\n            mock_streams (MagicMock): Mock for streamlink.streams.\n        \"\"\"\n        # Mock streamlink to return low quality streams\n        mock_streams.return_value = {\n            '480p': MagicMock(url='http://480p.stream'),\n            '360p': MagicMock(url='http://360p.stream'),\n            '240p': MagicMock(url='http://240p.stream'),\n        }\n\n        # Mock internet speed check result\n        with patch.object(\n            self.stream_capture,\n            'check_internet_speed',\n            return_value=(3, 5),\n        ):\n            # Select the lower quality stream based on internet speed\n            selected_quality = (\n                self.stream_capture.select_quality_based_on_speed()\n            )\n            self.assertEqual(selected_quality, 'http://480p.stream')\n\n    @patch('streamlink.streams', return_value={})\n    @patch.object(StreamCapture, 'check_internet_speed', return_value=(20, 5))\n    def test_select_quality_based_on_speed_no_quality(\n        self,\n        mock_check_speed: MagicMock,\n        mock_streams: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test that None is returned if no suitable stream quality is available.\n\n        Args:\n            mock_check_speed (MagicMock): Mock for check_internet_speed method.\n            mock_streams (MagicMock): Mock for streamlink.streams.\n        \"\"\"\n        # Mock internet speed and stream quality check result to be empty\n        selected_quality = self.stream_capture.select_quality_based_on_speed()\n        self.assertIsNone(selected_quality)\n\n    @patch(\n        'streamlink.streams', return_value={\n            'best': MagicMock(url='http://best.stream'),\n            '720p': MagicMock(url='http://720p.stream'),\n            '480p': MagicMock(url='http://480p.stream'),\n        },\n    )\n    @patch.object(StreamCapture, 'check_internet_speed', return_value=(20, 5))\n    @patch('cv2.VideoCapture')\n    @patch('time.sleep', return_value=None)\n    async def test_capture_generic_frames(\n        self,\n        mock_sleep: MagicMock,\n        mock_video_capture: MagicMock,\n        mock_check_speed: MagicMock,\n        mock_streams: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test that generic frames are captured and returned with a timestamp.\n\n        Args:\n            mock_sleep (MagicMock): Mock for time.sleep.\n            mock_video_capture (MagicMock): Mock for cv2.VideoCapture.\n            mock_check_speed (MagicMock): Mock for check_internet_speed method.\n            mock_streams (MagicMock): Mock for streamlink.streams.\n        \"\"\"\n        # Mock VideoCapture object's behaviour\n        mock_video_capture.return_value.read.return_value = (True, MagicMock())\n        mock_video_capture.return_value.isOpened.return_value = True\n\n        # Execute capture frame generator\n        generator = self.stream_capture.capture_generic_frames()\n        frame, timestamp = await generator.__anext__()\n\n        # Verify the returned frame and timestamp\n        self.assertIsNotNone(frame)\n        self.assertIsInstance(timestamp, float)\n\n        # Release resources\n        await self.stream_capture.release_resources()\n\n    def test_update_capture_interval(self) -> None:\n        \"\"\"\n        Test that the capture interval is updated correctly.\n        \"\"\"\n        # Update capture interval and verify\n        self.stream_capture.update_capture_interval(20)\n        self.assertEqual(self.stream_capture.capture_interval, 20)\n\n    @patch('argparse.ArgumentParser.parse_args')\n    @patch('builtins.print')\n    @patch('gc.collect')\n    async def test_main_function(\n        self,\n        mock_gc_collect: MagicMock,\n        mock_print: MagicMock,\n        mock_parse_args: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test that the main function correctly initialises\n        and executes StreamCapture.\n\n        Args:\n            mock_gc_collect (MagicMock): Mock for gc.collect function.\n            mock_print (MagicMock): Mock for print function.\n            mock_parse_args (MagicMock): Mock for\n                argparse.ArgumentParser.parse_args.\n        \"\"\"\n        # Mock parse_args method to return a stream URL\n        mock_parse_args.return_value = argparse.Namespace(\n            url='test_stream_url',\n        )\n\n        # Mock frame and timestamp\n        mock_frame = np.zeros((480, 640, 3), dtype=np.uint8)\n        mock_timestamp = 1234567890.0\n\n        async def mock_execute_capture(self):\n            yield mock_frame, mock_timestamp\n\n        # Execute main function and verify print and gc.collect calls\n        with patch.object(\n            StreamCapture, 'execute_capture', mock_execute_capture,\n        ):\n            with patch.object(\n                sys, 'argv', ['stream_capture.py', '--url', 'test_stream_url'],\n            ):\n                await stream_capture_main()\n\n            # Verify print and gc.collect calls\n            mock_print.assert_any_call(f\"Frame at {mock_timestamp} displayed\")\n            mock_gc_collect.assert_called()\n\n    @patch('cv2.VideoCapture')\n    @patch('time.sleep', return_value=None)\n    async def test_execute_capture_failures(\n        self,\n        mock_sleep: MagicMock,\n        mock_video_capture: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test that execute_capture handles multiple failures before success.\n\n        Args:\n            mock_sleep (MagicMock): Mock for time.sleep.\n            mock_video_capture (MagicMock): Mock for cv2.VideoCapture.\n        \"\"\"\n        # Mock VideoCapture object's multiple failures and one success read\n        instance: MagicMock = mock_video_capture.return_value\n        instance.read.side_effect = [(False, None)] * 5 + [(True, MagicMock())]\n        instance.isOpened.return_value = True\n\n        # Mock capture_generic_frames method and execute\n        async def mock_generic_frames():\n            for _ in range(3):\n                yield np.zeros((480, 640, 3), dtype=np.uint8), time.time()\n\n        with patch.object(\n            self.stream_capture,\n            'capture_generic_frames',\n            side_effect=mock_generic_frames,\n        ):\n            generator = self.stream_capture.execute_capture()\n            frame, timestamp = await generator.__anext__()\n\n            # Assert that a frame and timestamp were returned\n            self.assertIsInstance(frame, np.ndarray)\n            self.assertIsInstance(timestamp, float)\n\n    @patch.object(\n        StreamCapture,\n        'select_quality_based_on_speed',\n        return_value=None,\n    )\n    async def test_capture_generic_frames_no_quality(\n        self, mock_quality: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test that capture_generic_frames handles no suitable quality.\n\n        Args:\n            mock_quality (MagicMock): Mock for\n                select_quality_based_on_speed method.\n        \"\"\"\n        async for _ in self.stream_capture.capture_generic_frames():\n            self.fail('No frame should be yielded when quality is None.')\n\n    @patch('speedtest.Speedtest')\n    @patch('streamlink.streams', side_effect=Exception('Streamlink error'))\n    def test_select_quality_based_on_speed_exception(\n        self,\n        mock_streams: MagicMock,\n        mock_speedtest: MagicMock,\n    ) -> None:\n        # Mock Speedtest object's download and upload methods\n        mock_speedtest.return_value.get_best_server.return_value = {}\n        mock_speedtest.return_value.download.return_value = 20_000_000\n        mock_speedtest.return_value.upload.return_value = 5_000_000\n\n        selected_quality = self.stream_capture.select_quality_based_on_speed()\n        self.assertIsNone(selected_quality)\n\n    @patch('cv2.VideoCapture')\n    @patch.object(\n        StreamCapture,\n        'select_quality_based_on_speed',\n        return_value='http://example.com/stream',\n    )\n    async def test_generic_frame_reinitialisation_logic(\n        self,\n        mock_quality: MagicMock,\n        mock_video_capture: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test that the generic frame capture handles reinitialisation correctly\n        after multiple consecutive failures.\n        \"\"\"\n        # Set up the StreamCapture instance with a capture interval of 0\n        self.stream_capture = StreamCapture(\n            'http://example.com/stream', capture_interval=0,\n        )\n\n        # Mock VideoCapture object's read method to\n        # return False 5 times and then True\n        mock_video_capture.return_value.read.side_effect = (\n            [(False, None)] * 5 + [(True, MagicMock())] * 5\n        )\n        mock_video_capture.return_value.isOpened.return_value = True\n\n        # Use the generic frame capture method to\n        # get the first frame and timestamp\n        generator = self.stream_capture.capture_generic_frames()\n        frame, timestamp = await generator.__anext__()\n\n        self.assertIsNotNone(\n            frame, 'Frame should not be None after reinitialisation',\n        )\n        self.assertIsInstance(timestamp, float, 'Timestamp should be a float')\n\n    @patch('cv2.VideoCapture')\n    @patch.object(\n        StreamCapture,\n        'select_quality_based_on_speed',\n        return_value=None,\n    )\n    async def test_generic_frame_no_quality(\n        self,\n        mock_quality: MagicMock,\n        mock_video_capture: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test that capture_generic_frames skips iterations\n        when no quality is available.\n\n        Args:\n            mock_quality (MagicMock): Mock for select_quality_based_on_speed.\n            mock_video_capture (MagicMock): Mock for cv2.VideoCapture.\n        \"\"\"\n        # Iterate over the generator and ensure no frames are yielded\n        async for _ in self.stream_capture.capture_generic_frames():\n            self.fail('No frames should be yielded when quality is None')\n\n        # Verify that VideoCapture was not called\n        mock_video_capture.assert_not_called()\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/examples/YOLO_train/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/src/model_fetcher_test.py", "content": "from __future__ import annotations\n\nimport datetime\nimport unittest\nfrom pathlib import Path\nfrom unittest.mock import MagicMock\nfrom unittest.mock import mock_open\nfrom unittest.mock import patch\n\nimport requests\n\nfrom src.model_fetcher import ModelFetcher\nfrom src.model_fetcher import run_scheduler_loop\nfrom src.model_fetcher import schedule_task\n\n\nclass TestModelFetcher(unittest.TestCase):\n    \"\"\"\n    Test suite for the ModelFetcher class and related functions.\n    \"\"\"\n\n    def setUp(self) -> None:\n        \"\"\"\n        Set up the test environment.\n        \"\"\"\n        self.api_url: str = 'http://test-server/get_new_model'\n        self.local_dir: str = 'test_models/pt'\n        self.models: list[str] = ['test_model1', 'test_model2']\n        self.fetcher: ModelFetcher = ModelFetcher(\n            api_url=self.api_url,\n            models=self.models,\n            local_dir=self.local_dir,\n        )\n\n        # Ensure the local directory exists before running tests.\n        Path(self.local_dir).mkdir(parents=True, exist_ok=True)\n\n    def tearDown(self) -> None:\n        \"\"\"\n        Clean up the test environment.\n        \"\"\"\n        test_dir = Path(self.local_dir)\n        if test_dir.exists():\n            for file in test_dir.glob('*'):\n                file.unlink()  # Delete all files in the directory\n            test_dir.rmdir()  # Delete the directory\n\n    @patch('src.model_fetcher.Path.stat')\n    def test_get_last_update_time_existing_file(\n        self, mock_stat: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test fetching the last update time of an existing model file.\n\n        Args:\n            mock_stat: Mocked version of Path.stat to control return values.\n        \"\"\"\n        mock_stat.return_value.st_mtime = datetime.datetime(\n            2022, 1, 1,\n        ).timestamp()\n        file_path = Path(self.local_dir) / 'best_test_model1.pt'\n        file_path.touch()\n\n        last_update_time = self.fetcher.get_last_update_time('test_model1')\n        self.assertEqual(last_update_time, '2022-01-01T00:00:00')\n\n    def test_get_last_update_time_nonexistent_file(self) -> None:\n        \"\"\"\n        Test fetching the last update time of a nonexistent file.\n        \"\"\"\n        last_update_time = self.fetcher.get_last_update_time('test_model1')\n        self.assertEqual(last_update_time, '1970-01-01T00:00:00')\n\n    @patch('builtins.open', new_callable=mock_open)\n    @patch('src.model_fetcher.Path.mkdir')\n    def test_download_and_save_model(\n        self, mock_mkdir: MagicMock, mock_open_file: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test downloading and saving a model file.\n\n        Args:\n            mock_mkdir: Mocked Path.mkdir method to\n                avoid actual directory creation.\n            mock_open_file: Mocked open function to\n                avoid writing a real file.\n        \"\"\"\n        model_content = b'test content'\n        self.fetcher.download_and_save_model('test_model1', model_content)\n\n        file_path = Path(self.local_dir) / 'best_test_model1.pt'\n        mock_mkdir.assert_called_once_with(parents=True, exist_ok=True)\n        mock_open_file.assert_called_once_with(file_path, 'wb')\n        mock_open_file().write.assert_called_once_with(model_content)\n\n    @patch('src.model_fetcher.requests.get')\n    @patch('src.model_fetcher.ModelFetcher.download_and_save_model')\n    def test_request_new_model_success(\n        self,\n        mock_download_and_save: MagicMock,\n        mock_requests_get: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test requesting and saving a new model from the server.\n\n        Args:\n            mock_download_and_save: Mocked method to save the model locally.\n            mock_requests_get: Mocked requests.get function to\n                simulate server response.\n        \"\"\"\n        mock_response = MagicMock()\n        mock_response.status_code = 200\n        mock_response.json.return_value = {\n            'model_file': b'testcontent'.hex(),\n        }\n        mock_requests_get.return_value = mock_response\n\n        self.fetcher.request_new_model('test_model1', '2022-01-01T00:00:00')\n        mock_requests_get.assert_called_once_with(\n            self.api_url,\n            params={\n                'model': 'test_model1',\n                'last_update_time': '2022-01-01T00:00:00',\n            },\n            timeout=10,\n        )\n        mock_download_and_save.assert_called_once_with(\n            'test_model1', b'testcontent',\n        )\n\n    @patch('src.model_fetcher.requests.get')\n    def test_request_new_model_no_update_needed(\n        self, mock_requests_get: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test requesting a model when no update is needed.\n\n        Args:\n            mock_requests_get: Mocked requests.get function to\n                simulate server response.\n        \"\"\"\n        mock_response = MagicMock()\n        mock_response.status_code = 200\n        mock_response.json.return_value = {}\n        mock_requests_get.return_value = mock_response\n\n        with self.assertLogs(level='INFO') as log:\n            self.fetcher.request_new_model(\n                'test_model1', '2022-01-01T00:00:00',\n            )\n            self.assertIn(\n                'Model test_model1 is already up to date.',\n                log.output[0],\n            )\n\n        mock_requests_get.assert_called_once()\n        self.assertFalse(\n            Path(self.local_dir).joinpath('best_test_model1.pt').exists(),\n        )\n\n    @patch('src.model_fetcher.requests.get')\n    def test_request_new_model_server_error(\n        self, mock_requests_get: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test handling a server error during model request.\n\n        Args:\n            mock_requests_get: Mocked requests.get function to\n                simulate server response.\n        \"\"\"\n        mock_response = MagicMock()\n        mock_response.status_code = 500\n        mock_requests_get.return_value = mock_response\n\n        with self.assertLogs(level='ERROR') as log:\n            self.fetcher.request_new_model(\n                'test_model1', '2022-01-01T00:00:00',\n            )\n            self.assertIn(\n                'Failed to fetch model test_model1. '\n                'Server returned status code: 500',\n                log.output[0],\n            )\n\n    @patch('src.model_fetcher.requests.get')\n    def test_request_new_model_timeout(\n        self, mock_requests_get: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test handling a request timeout.\n\n        Args:\n            mock_requests_get: Mocked requests.get function to\n                simulate a timeout.\n        \"\"\"\n        mock_requests_get.side_effect = requests.exceptions.Timeout\n\n        with self.assertLogs(level='ERROR') as log:\n            self.fetcher.request_new_model(\n                'test_model1', '2022-01-01T00:00:00',\n            )\n            self.assertIn(\n                'Error requesting model test_model1:',\n                log.output[0],\n            )\n\n    @patch('src.model_fetcher.requests.get')\n    def test_request_new_model_connection_error(\n        self, mock_requests_get: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test handling a connection error.\n\n        Args:\n            mock_requests_get: Mocked requests.get function to\n                simulate a connection error.\n        \"\"\"\n        mock_requests_get.side_effect = requests.exceptions.ConnectionError\n\n        with self.assertLogs(level='ERROR') as log:\n            self.fetcher.request_new_model(\n                'test_model1', '2022-01-01T00:00:00',\n            )\n            self.assertIn(\n                'Error requesting model test_model1:',\n                log.output[0],\n            )\n\n    @patch('src.model_fetcher.ModelFetcher.request_new_model')\n    def test_update_all_models(\n        self, mock_request_new_model: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test updating all models.\n\n        Args:\n            mock_request_new_model: Mocked method\n                to simulate requesting new models.\n        \"\"\"\n        self.fetcher.update_all_models()\n        self.assertEqual(mock_request_new_model.call_count, len(self.models))\n        mock_request_new_model.assert_any_call(\n            'test_model1', '1970-01-01T00:00:00',\n        )\n        mock_request_new_model.assert_any_call(\n            'test_model2', '1970-01-01T00:00:00',\n        )\n\n    @patch('src.model_fetcher.ModelFetcher.request_new_model')\n    def test_update_all_models_with_exception(\n        self, mock_request_new_model: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test handling exceptions during update_all_models.\n\n        Args:\n            mock_request_new_model: Mocked method that raises an exception.\n        \"\"\"\n        # Mock request_new_model to raise an exception\n        mock_request_new_model.side_effect = Exception('Simulated exception')\n\n        with self.assertLogs(level='ERROR') as log:\n            self.fetcher.update_all_models()\n            self.assertEqual(\n                mock_request_new_model.call_count, len(self.models),\n            )\n            for model in self.models:\n                self.assertTrue(\n                    any(\n                        f\"Failed to update model {model}: Simulated exception\"\n                        in message\n                        for message in log.output\n                    ),\n                    f\"Expected log for model {model} not found.\",\n                )\n\n    @patch('src.model_fetcher.ModelFetcher.update_all_models')\n    def test_schedule_task(self, mock_update_all_models: MagicMock) -> None:\n        \"\"\"\n        Test scheduling the update task.\n\n        Args:\n            mock_update_all_models: Mocked update_all_models method\n                to verify it is called.\n        \"\"\"\n        schedule_task()\n        mock_update_all_models.assert_called_once()\n\n    @patch('src.model_fetcher.schedule.run_pending')\n    @patch(\n        'src.model_fetcher.time.sleep',\n        side_effect=[None, Exception('Stop loop')],\n    )\n    def test_schedule_task_loop(\n        self, mock_sleep: MagicMock, mock_run_pending: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test the scheduled task loop.\n\n        Args:\n            mock_sleep: Mocked time.sleep to control loop iterations.\n            mock_run_pending: Mocked schedule.run_pending\n                to simulate task execution.\n        \"\"\"\n        # Mock run_pending executes normally on the first call\n        # and triggers Exception on the second call to end the loop test.\n        mock_run_pending.side_effect = [None, Exception('Stop loop')]\n\n        with self.assertLogs(level='INFO') as log:\n            try:\n                run_scheduler_loop()  # Run the loop\n            except Exception as e:\n                self.assertEqual(str(e), 'Stop loop')\n\n            # Verify that the expected log message is present\n            self.assertTrue(\n                any(\n                    'Starting scheduled tasks. Press Ctrl+C to exit.'\n                    in message\n                    for message in log.output\n                ),\n                'Expected log message not found.',\n            )\n\n        mock_run_pending.assert_called()\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/examples/streaming_web/backend/sockets_test.py", "content": "from __future__ import annotations\n\nimport unittest\nfrom typing import Any\nfrom unittest.mock import AsyncMock\nfrom unittest.mock import Mock\nfrom unittest.mock import patch\n\nimport socketio\n\nfrom examples.streaming_web.backend.sockets import register_sockets\nfrom examples.streaming_web.backend.sockets import update_images\n\n\nclass TestSockets(unittest.IsolatedAsyncioTestCase):\n    \"\"\"\n    Test suite for the streaming_web sockets.\n    \"\"\"\n\n    def setUp(self) -> None:\n        \"\"\"\n        Sets up the test environment by initialising Socket.IO server\n        and mocking RedisManager.\n        \"\"\"\n        self.sio = socketio.AsyncServer()\n        self.redis_manager = AsyncMock()\n        register_sockets(self.sio, self.redis_manager)\n\n        # Extracting the decorated event handlers from sio\n        self.connect_handler = self.sio.handlers['/']['connect']\n        self.disconnect_handler = self.sio.handlers['/']['disconnect']\n        self.error_handler = self.sio.handlers['/']['error']\n\n    async def test_connect(self) -> None:\n        \"\"\"\n        Tests if a client can successfully connect to the server\n        and receive a welcome message.\n        \"\"\"\n        sid = 'test_sid'\n        environ: dict[str, Any] = {}\n\n        with patch.object(\n            self.sio, 'emit', new_callable=AsyncMock,\n        ) as mock_emit, patch.object(\n            self.sio, 'start_background_task',\n        ) as mock_background_task:\n            await self.connect_handler(sid, environ)\n\n            # Emit is awaited with the correct arguments\n            mock_emit.assert_awaited_with(\n                'message', {'data': 'Connected'}, room=sid,\n            )\n            # Ensure the background task is started\n            mock_background_task.assert_called_once_with(\n                update_images, self.sio, self.redis_manager,\n            )\n\n    async def test_disconnect(self) -> None:\n        \"\"\"\n        Tests if a client disconnect event is logged successfully.\n        \"\"\"\n        sid = 'test_sid'\n\n        # Use Mock instead of AsyncMock to avoid warnings\n        with patch('builtins.print', new_callable=Mock) as mock_print:\n            await self.disconnect_handler(sid)\n\n            # Check if print was called with the disconnect message\n            mock_print.assert_called_once_with('Client disconnected')\n\n    async def test_error(self) -> None:\n        \"\"\"\n        Tests if errors are logged correctly during events.\n        \"\"\"\n        sid = 'test_sid'\n        error = Exception('Test error')\n\n        # Use Mock instead of AsyncMock to avoid warnings\n        with patch('builtins.print', new_callable=Mock) as mock_print:\n            await self.error_handler(sid, error)\n\n            # Check if print was called with the error message\n            mock_print.assert_called_once_with(f\"Error: {str(error)}\")\n\n    async def test_update_images(self) -> None:\n        \"\"\"\n        Tests the update_images background task to ensure\n        that images are emitted correctly to all clients.\n        \"\"\"\n        with (\n            patch.object(\n                self.sio, 'sleep', new_callable=AsyncMock,\n            ) as mock_sleep,\n            patch.object(\n                self.sio, 'emit', new_callable=AsyncMock,\n            ) as mock_emit,\n        ):\n            mock_sleep.side_effect = [None, None, Exception('Stop Loop')]\n\n            self.redis_manager.get_labels.return_value = ['label1', 'label2']\n            self.redis_manager.fetch_latest_frames.side_effect = (\n                lambda label: [\n                    (f\"image_data_{label}_1\", f\"image_name_{label}_1\"),\n                    (f\"image_data_{label}_2\", f\"image_name_{label}_2\"),\n                ]\n            )\n\n            try:\n                await update_images(self.sio, self.redis_manager)\n            except Exception as e:\n                print(f\"Error updating images: {str(e)}\")\n\n            self.redis_manager.get_labels.assert_awaited()\n\n            # Verifying that emit is called with the correct data for\n            # each label\n            mock_emit.assert_any_await(\n                'update',\n                {\n                    'label': 'label1',\n                    'images': [\n                        'image_data_label1_1',\n                        'image_data_label1_2',\n                    ],\n                    'image_names': [\n                        'image_name_label1_1',\n                        'image_name_label1_2',\n                    ],\n                },\n            )\n            mock_emit.assert_any_await(\n                'update',\n                {\n                    'label': 'label2',\n                    'images': [\n                        'image_data_label2_1',\n                        'image_data_label2_2',\n                    ],\n                    'image_names': [\n                        'image_name_label2_1',\n                        'image_name_label2_2',\n                    ],\n                },\n            )\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/examples/YOLO_server_api/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/examples/YOLO_server_api/backend/app_test.py", "content": "from __future__ import annotations\n\nimport unittest\nfrom unittest.mock import AsyncMock\nfrom unittest.mock import MagicMock\nfrom unittest.mock import patch\n\nimport socketio\nfrom fastapi.testclient import TestClient\n\nfrom examples.YOLO_server_api.backend.app import app\nfrom examples.YOLO_server_api.backend.app import lifespan\nfrom examples.YOLO_server_api.backend.app import run_uvicorn_app\nfrom examples.YOLO_server_api.backend.app import sio\nfrom examples.YOLO_server_api.backend.app import sio_app\n\n\nclass TestApp(unittest.IsolatedAsyncioTestCase):\n    \"\"\"\n    Unit tests for FastAPI app functionality and dependencies.\n    \"\"\"\n\n    def setUp(self) -> None:\n        \"\"\"\n        Sets up the TestClient instance for testing FastAPI app routes.\n        \"\"\"\n        self.client = TestClient(app)\n\n    @patch(\n        'examples.YOLO_server_api.backend.app.RedisClient.connect',\n        new_callable=AsyncMock,\n    )\n    @patch(\n        'examples.YOLO_server_api.backend.app.scheduler.shutdown',\n        new_callable=MagicMock,\n    )\n    @patch('examples.YOLO_server_api.backend.app.engine', autospec=True)\n    async def test_redis_initialization(\n        self,\n        mock_engine: MagicMock,\n        mock_scheduler_shutdown: MagicMock,\n        mock_redis_connect: AsyncMock,\n    ) -> None:\n        \"\"\"\n        Tests Redis and SQLAlchemy engine initialisation within app lifespan.\n\n        Args:\n            mock_engine (MagicMock): Mocked SQLAlchemy engine instance.\n            mock_scheduler_shutdown (MagicMock): Mock for scheduler shutdown.\n            mock_redis_connect (AsyncMock): Mock for Redis connection.\n        \"\"\"\n        # Mock SQLAlchemy engine connection and transaction\n        mock_conn = AsyncMock()\n        mock_engine.begin.return_value.__aenter__.return_value = mock_conn\n\n        # Start the lifespan context to test initialisation behaviour\n        async with lifespan(app):\n            mock_redis_connect.assert_called_once()\n            mock_engine.begin.assert_called_once()\n            mock_scheduler_shutdown.assert_not_called()\n\n    @patch(\n        'examples.YOLO_server_api.backend.app.FastAPILimiter.init',\n        new_callable=AsyncMock,\n    )\n    @patch(\n        'examples.YOLO_server_api.backend.app.RedisClient.connect',\n        new_callable=AsyncMock,\n    )\n    @patch('examples.YOLO_server_api.backend.app.engine', autospec=True)\n    async def test_lifespan_context(\n        self,\n        mock_engine: MagicMock,\n        mock_redis_connect: AsyncMock,\n        mock_limiter_init: AsyncMock,\n    ) -> None:\n        \"\"\"\n        Tests the lifespan context to verify resource initialisation.\n\n        Args:\n            mock_engine (MagicMock): Mocked SQLAlchemy engine instance.\n            mock_redis_connect (AsyncMock): Mock for Redis connection.\n            mock_limiter_init (AsyncMock): Mock for rate limiter\n                initialisation.\n        \"\"\"\n        # Mock SQLAlchemy engine connection and transaction\n        mock_conn = AsyncMock()\n        mock_engine.begin.return_value.__aenter__.return_value = mock_conn\n\n        # Start the lifespan context to test the initialisation behaviour\n        async with lifespan(app):\n            mock_redis_connect.assert_called_once()\n            mock_engine.begin.assert_called_once()\n            mock_limiter_init.assert_called_once()\n\n    def test_routes_exist(self) -> None:\n        \"\"\"\n        Verifies the existence and behaviour of primary application routes.\n        \"\"\"\n        # Test the /api/token endpoint; it should\n        # return 405 (method not allowed for GET).\n        response = self.client.get('/api/token')\n        self.assertEqual(response.status_code, 405)\n\n        # Test the /api/detect endpoint; it should\n        # return 405 (method not allowed for GET).\n        response = self.client.get('/api/detect')\n        self.assertEqual(response.status_code, 405)\n\n        # Test the /api/model_file_update endpoint; it should\n        # return 405 (method not allowed for GET).\n        response = self.client.get('/api/model_file_update')\n        self.assertEqual(response.status_code, 405)\n\n        # Test the /api/get_new_model endpoint; it should\n        # return 405 (method not allowed for GET).\n        response = self.client.get('/api/get_new_model')\n        self.assertEqual(response.status_code, 405)\n\n        # Test the /api/add_user endpoint; it should\n        # return 405 (method not allowed for GET).\n        response = self.client.get('/api/add_user')\n        self.assertEqual(response.status_code, 405)\n\n    @patch('builtins.print')\n    @patch.object(socketio.AsyncClient, 'disconnect', new_callable=AsyncMock)\n    @patch.object(socketio.AsyncClient, 'connect', new_callable=AsyncMock)\n    async def test_socketio_connect_disconnect(\n        self,\n        mock_connect: AsyncMock,\n        mock_disconnect: AsyncMock,\n        mock_print: MagicMock,\n    ) -> None:\n        \"\"\"\n        Tests the Socket.IO client connection and disconnection.\n\n        Args:\n            mock_connect (AsyncMock): Mocked connect() method of the client.\n            mock_disconnect (AsyncMock): Mocked disconnect() method of\n                the client.\n            mock_print (MagicMock): Mocked print function for logging.\n        \"\"\"\n        # 1) Mock the connect() / disconnect() so no real network calls occur\n        mock_connect.return_value = None\n        mock_disconnect.return_value = None\n\n        # 2) Create a client and \"connect\" / \"disconnect\" (all mock)\n        client = socketio.AsyncClient()\n        await client.connect('http://fake-host:9999', wait=True)\n        await client.disconnect()\n\n        # 3) Manually trigger the server's 'connect' event\n        sid = '123'\n        environ = {'REMOTE_ADDR': '127.0.0.1'}\n        await sio._trigger_event('connect', '/', sid, environ)\n\n        # 4) Manually trigger the server's 'disconnect' event (without reason)\n        await sio._trigger_event('disconnect', '/', sid)\n\n        # 5) Check the print calls for server side\n        mock_print.assert_any_call('Client connected:', sid)\n        mock_print.assert_any_call('Client disconnected:', sid)\n\n    @patch('examples.YOLO_server_api.backend.app.uvicorn.run')\n    def test_main_entry_uvicorn_run(self, mock_uvicorn_run: MagicMock) -> None:\n        \"\"\"\n        Test the run_uvicorn_app function to ensure\n        uvicorn.run is called with correct parameters.\n\n        Args:\n            mock_uvicorn_run (MagicMock): Mocked uvicorn.run function.\n        \"\"\"\n        run_uvicorn_app()\n        mock_uvicorn_run.assert_called_once_with(\n            sio_app,\n            host='0.0.0.0',\n            port=8000,\n            workers=2,\n        )\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/src/notifiers/messenger_notifier_test.py", "content": "from __future__ import annotations\n\nimport unittest\nfrom typing import Any\nfrom unittest.mock import MagicMock\nfrom unittest.mock import patch\n\nimport numpy as np\n\nfrom src.notifiers.messenger_notifier import main\nfrom src.notifiers.messenger_notifier import MessengerNotifier\n\n\nclass TestMessengerNotifier(unittest.TestCase):\n    \"\"\"\n    Unit tests for the MessengerNotifier class methods.\n    \"\"\"\n\n    messenger_notifier: MessengerNotifier\n\n    @classmethod\n    @patch(\n        'src.notifiers.messenger_notifier.os.getenv',\n        return_value='test_page_access_token',\n    )\n    def setUpClass(cls, mock_getenv) -> None:\n        \"\"\"\n        Set up class method to initialise the MessengerNotifier instance.\n        \"\"\"\n        cls.messenger_notifier = MessengerNotifier()\n\n    def test_init(self) -> None:\n        \"\"\"\n        Test if the MessengerNotifier instance is initialised correctly.\n        \"\"\"\n        self.assertIsInstance(self.messenger_notifier, MessengerNotifier)\n\n    @patch('requests.post')\n    @patch(\n        'src.notifiers.messenger_notifier.os.getenv',\n        return_value='test_page_access_token',\n    )\n    def test_send_notification_no_image(\n        self,\n        mock_getenv: Any,\n        mock_post: Any,\n    ) -> None:\n        \"\"\"\n        Test sending a notification without an image.\n\n        Args:\n            mock_post (Any): Mock object for the requests.post method.\n        \"\"\"\n        mock_post.return_value.status_code = 200\n        recipient_id: str = 'test_recipient_id'\n        message: str = 'Hello, Messenger!'\n        response_code: int = self.messenger_notifier.send_notification(\n            recipient_id, message,\n        )\n        self.assertEqual(response_code, 200)\n        self.assertTrue(mock_post.called)\n        args, kwargs = mock_post.call_args\n        self.assertEqual(\n            kwargs['json'], {\n                'message': {\n                    'text': message,\n                }, 'recipient': {'id': recipient_id},\n            },\n        )\n\n    @patch('requests.post')\n    @patch(\n        'src.notifiers.messenger_notifier.os.getenv',\n        return_value='test_page_access_token',\n    )\n    def test_send_notification_with_image(\n        self,\n        mock_getenv: Any,\n        mock_post: Any,\n    ) -> None:\n        \"\"\"\n        Test sending a notification with an image.\n\n        Args:\n            mock_post (Any): Mock object for the requests.post method.\n        \"\"\"\n        mock_post.return_value.status_code = 200\n        recipient_id: str = 'test_recipient_id'\n        message: str = 'Hello, Messenger!'\n        image: np.ndarray = np.zeros((100, 100, 3), dtype=np.uint8)\n        response_code: int = self.messenger_notifier.send_notification(\n            recipient_id, message, image=image,\n        )\n        self.assertEqual(response_code, 200)\n        self.assertTrue(mock_post.called)\n        args, kwargs = mock_post.call_args\n        files = kwargs['files']\n        self.assertIn('filedata', files)\n        self.assertEqual(files['filedata'][0], 'image.png')\n        self.assertEqual(files['filedata'][2], 'image/png')\n\n    @patch(\n        'src.notifiers.messenger_notifier.MessengerNotifier.send_notification',\n        return_value=200,\n    )\n    @patch(\n        'src.notifiers.messenger_notifier.os.getenv',\n        return_value='test_page_access_token',\n    )\n    def test_main(\n        self,\n        mock_getenv: MagicMock,\n        mock_send_notification: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test the main function.\n        \"\"\"\n        with patch('builtins.print') as mock_print:\n            main()\n            mock_send_notification.assert_called_once()\n            mock_print.assert_called_with('Response code: 200')\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/src/notifiers/telegram_notifier_test.py", "content": "from __future__ import annotations\n\nimport unittest\nfrom datetime import datetime\nfrom io import BytesIO\nfrom unittest.mock import AsyncMock\nfrom unittest.mock import patch\n\nimport numpy as np\nfrom telegram import Chat\nfrom telegram import Message\n\nfrom src.notifiers.telegram_notifier import main\nfrom src.notifiers.telegram_notifier import TelegramNotifier\n\n\nclass TestTelegramNotifier(unittest.IsolatedAsyncioTestCase):\n    \"\"\"\n    Unit tests for the TelegramNotifier class methods.\n    \"\"\"\n\n    telegram_notifier: TelegramNotifier\n\n    @classmethod\n    @patch(\n        'src.notifiers.telegram_notifier.os.getenv',\n        return_value='test_bot_token',\n    )\n    def setUpClass(cls, mock_getenv) -> None:\n        \"\"\"\n        Set up the TelegramNotifier instance for tests.\n        \"\"\"\n        cls.telegram_notifier = TelegramNotifier()\n\n    def test_init(self) -> None:\n        \"\"\"\n        Test if the TelegramNotifier instance is initialised correctly.\n        \"\"\"\n        self.assertIsInstance(self.telegram_notifier, TelegramNotifier)\n\n    @patch('telegram.Bot.send_message', new_callable=AsyncMock)\n    @patch(\n        'src.notifiers.telegram_notifier.os.getenv',\n        return_value='test_bot_token',\n    )\n    async def test_send_notification_no_image(\n        self,\n        mock_getenv,\n        mock_send_message: AsyncMock,\n    ) -> None:\n        \"\"\"\n        Test sending a notification without an image.\n        \"\"\"\n        chat = Chat(id=1, type='private')\n        mock_send_message.return_value = Message(\n            message_id=1, date=datetime.now(), chat=chat,\n        )\n        chat_id: str = 'test_chat_id'\n        message: str = 'Hello, Telegram!'\n        response: Message = await self.telegram_notifier.send_notification(\n            chat_id,\n            message,\n        )\n        self.assertIsInstance(response, Message)\n        mock_send_message.assert_called_once_with(\n            chat_id=chat_id, text=message,\n        )\n\n    @patch('telegram.Bot.send_photo', new_callable=AsyncMock)\n    @patch(\n        'src.notifiers.telegram_notifier.os.getenv',\n        return_value='test_bot_token',\n    )\n    async def test_send_notification_with_image(\n        self,\n        mock_getenv,\n        mock_send_photo: AsyncMock,\n    ) -> None:\n        \"\"\"\n        Test sending a notification with an image.\n        \"\"\"\n        chat = Chat(id=1, type='private')\n        mock_send_photo.return_value = Message(\n            message_id=1, date=datetime.now(), chat=chat,\n        )\n        chat_id: str = 'test_chat_id'\n        message: str = 'Hello, Telegram!'\n        image: np.ndarray = np.zeros((100, 100, 3), dtype=np.uint8)\n        response: Message = await self.telegram_notifier.send_notification(\n            chat_id, message, image=image,\n        )\n        self.assertIsInstance(response, Message)\n        mock_send_photo.assert_called_once()\n        args, kwargs = mock_send_photo.call_args\n        self.assertEqual(kwargs['chat_id'], chat_id)\n        self.assertEqual(kwargs['caption'], message)\n        self.assertIsInstance(kwargs['photo'], BytesIO)\n\n    @patch(\n        'src.notifiers.telegram_notifier.TelegramNotifier.send_notification',\n        new_callable=AsyncMock,\n    )\n    @patch('src.notifiers.telegram_notifier.os.getenv')\n    async def test_main(\n        self,\n        mock_getenv: AsyncMock,\n        mock_send_notification: AsyncMock,\n    ) -> None:\n        \"\"\"\n        Test the main function.\n        \"\"\"\n        mock_getenv.side_effect = (\n            lambda key: 'test_bot_token'\n            if key == 'TELEGRAM_BOT_TOKEN'\n            else None\n        )\n        chat = Chat(id=1, type='private')\n        mock_send_notification.return_value = Message(\n            message_id=1, date=datetime.now(), chat=chat,\n        )\n\n        with patch('builtins.print') as mock_print:\n            await main()\n            mock_send_notification.assert_called_once()\n            args, kwargs = mock_send_notification.call_args\n            self.assertEqual(args[0], 'your_chat_id_here')\n            self.assertEqual(args[1], 'Hello, Telegram!')\n            if len(args) > 2:\n                self.assertIsInstance(args[2], np.ndarray)\n                self.assertEqual(args[2].shape, (100, 100, 3))\n            mock_print.assert_called_once_with(\n                mock_send_notification.return_value,\n            )\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/src/monitor_logger_test.py", "content": "from __future__ import annotations\n\nimport logging\nimport shutil\nimport subprocess\nimport sys\nimport unittest\nfrom pathlib import Path\nfrom unittest.mock import MagicMock\nfrom unittest.mock import patch\n\nfrom src.monitor_logger import LoggerConfig\nfrom src.monitor_logger import main\n\n\nclass TestLoggerConfig(unittest.TestCase):\n    \"\"\"\n    Unit tests for the LoggerConfig class methods.\n    \"\"\"\n\n    def setUp(self) -> None:\n        \"\"\"\n        Initialise test variables and ensure the log directory exists.\n        \"\"\"\n        self.log_file: str = 'test.log'\n        self.log_dir: str = 'test_logs'\n        self.level: int = logging.DEBUG\n        self.formatter: logging.Formatter = logging.Formatter('%(message)s')\n\n        # Remove the log directory if it exists\n        if Path(self.log_dir).exists():\n            shutil.rmtree(self.log_dir)\n\n        # Create the log directory\n        Path(self.log_dir).mkdir(parents=True, exist_ok=True)\n\n        self.logger_config: LoggerConfig = LoggerConfig(\n            log_file=self.log_file,\n            log_dir=self.log_dir,\n            level=self.level,\n            formatter=self.formatter,\n        )\n\n    @patch('src.monitor_logger.Path.mkdir')\n    @patch('src.monitor_logger.RotatingFileHandler')\n    @patch('src.monitor_logger.logging.StreamHandler')\n    def test_setup_logger(\n        self,\n        mock_stream_handler: MagicMock,\n        mock_rotating_file_handler: MagicMock,\n        mock_mkdir: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test setting up the logger with file and console handlers.\n        \"\"\"\n        # Create mock handlers\n        mock_file_handler = MagicMock()\n        mock_console_handler = MagicMock()\n        mock_rotating_file_handler.return_value = mock_file_handler\n        mock_stream_handler.return_value = mock_console_handler\n\n        # Set level attribute for the handlers\n        mock_file_handler.level = self.level\n        mock_console_handler.level = self.level\n\n        # Run setup_logger\n        self.logger_config.setup_logger()\n\n        # Verify handlers were added to the logger\n        logger = self.logger_config.get_logger()\n        handlers = logger.handlers\n\n        self.assertIn(mock_file_handler, handlers)\n        self.assertIn(mock_console_handler, handlers)\n\n        # Verify the log directory was created\n        mock_mkdir.assert_called_once_with(parents=True, exist_ok=True)\n\n        # Verify file handler configuration\n        mock_rotating_file_handler.assert_called_once_with(\n            filename=Path(self.log_dir) / self.log_file,\n            maxBytes=1_000_000,\n            backupCount=5,\n        )\n        mock_file_handler.setLevel.assert_called_once_with(self.level)\n        mock_file_handler.setFormatter.assert_called_once_with(self.formatter)\n\n        # Verify console handler configuration\n        mock_console_handler.setLevel.assert_called_once_with(self.level)\n        mock_console_handler.setFormatter.assert_called_once_with(\n            self.formatter,\n        )\n\n    def test_get_file_handler(self) -> None:\n        \"\"\"\n        Test retrieving the file handler from LoggerConfig.\n        \"\"\"\n        file_handler = self.logger_config.get_file_handler()\n        self.assertIsInstance(file_handler, logging.Handler)\n        self.assertEqual(file_handler.level, self.level)\n        self.assertEqual(file_handler.formatter, self.formatter)\n\n    def test_get_console_handler(self) -> None:\n        \"\"\"\n        Test retrieving the console handler from LoggerConfig.\n        \"\"\"\n        console_handler = self.logger_config.get_console_handler()\n        self.assertIsInstance(console_handler, logging.Handler)\n        self.assertEqual(console_handler.level, self.level)\n        self.assertEqual(console_handler.formatter, self.formatter)\n\n    @patch('src.monitor_logger.Path.mkdir')\n    @patch('src.monitor_logger.RotatingFileHandler')\n    @patch('src.monitor_logger.logging.StreamHandler')\n    def test_logger_output(\n        self,\n        mock_stream_handler: MagicMock,\n        mock_rotating_file_handler: MagicMock,\n        mock_mkdir: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test the logger output to ensure it logs messages correctly.\n        \"\"\"\n        mock_file_handler = MagicMock()\n        mock_console_handler = MagicMock()\n        mock_rotating_file_handler.return_value = mock_file_handler\n        mock_stream_handler.return_value = mock_console_handler\n\n        # Initialise logger configuration\n        from src.monitor_logger import LoggerConfig\n        self.logger_config = LoggerConfig(\n            log_file='test.log', log_dir='logs_test',\n        )\n\n        # Mock the mkdir function to avoid actual directory creation\n        mock_mkdir.return_value = None\n\n        # Test logger setup\n        self.logger_config.setup_logger()\n        logger = self.logger_config.get_logger()\n\n        # Set level attribute for the handlers in case they are checked\n        mock_file_handler.level = logging.DEBUG\n        mock_console_handler.level = logging.DEBUG\n\n        # Test logging output\n        with self.assertLogs(logger, level='INFO') as log:\n            logger.info('Test log message')\n            expected_message = (\n                'INFO:SiteSafetyMonitor_test.log:Test log message'.upper()\n            )\n            log_messages = [msg.upper() for msg in log.output]\n            self.assertIn(expected_message, log_messages)\n\n    @patch('src.monitor_logger.LoggerConfig')\n    def test_main_function(self, mock_logger_config: MagicMock) -> None:\n        \"\"\"\n        Test the main function to ensure the logging setup is complete.\n        \"\"\"\n        # Create a mock logger instance\n        mock_logger_instance = MagicMock()\n        mock_logger_config.return_value.get_logger.return_value = (\n            mock_logger_instance\n        )\n\n        # Call the main function\n        main()\n\n        # Verify that the LoggerConfig was initialized\n        mock_logger_config.assert_called_once()\n\n        # Verify that the logging setup complete message was logged\n        mock_logger_instance.info.assert_called_once_with(\n            'Logging setup complete.',\n        )\n\n    def test_if_main_called(self) -> None:\n        \"\"\"\n        Test case to ensure `main` is called when the script is executed.\n        \"\"\"\n        result = subprocess.run(\n            [sys.executable, 'src/monitor_logger.py'],\n            capture_output=True,\n            text=True,\n        )\n        print(f\"STDOUT: {result.stdout}\")\n        print(f\"STDERR: {result.stderr}\")\n        self.assertIn('Logging setup complete.', result.stderr)\n\n    def tearDown(self) -> None:\n        \"\"\"\n        Clean up any files or directories created during tests.\n        \"\"\"\n        if Path(self.log_dir).exists():\n            try:\n                # Remove the log directory\n                shutil.rmtree(self.log_dir)\n                print(f\"Successfully removed directory: {self.log_dir}\")\n            except Exception as e:\n                print(f\"Failed to remove directory {self.log_dir}: {e}\")\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/src/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/examples/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/examples/YOLO_server_api/backend/model_files_test.py", "content": "from __future__ import annotations\n\nimport datetime\nimport unittest\nfrom pathlib import Path\nfrom unittest.mock import MagicMock\nfrom unittest.mock import mock_open\nfrom unittest.mock import patch\n\nfrom examples.YOLO_server_api.backend.model_files import get_new_model_file\nfrom examples.YOLO_server_api.backend.model_files import update_model_file\n\n\nclass TestModelFilesWithMock(unittest.IsolatedAsyncioTestCase):\n    \"\"\"\n    Unit tests for model file operations using virtual files.\n    \"\"\"\n\n    def setUp(self) -> None:\n        \"\"\"\n        Set up variables for testing.\n        \"\"\"\n        self.valid_model = 'yolo11n'\n        self.invalid_model = 'yolo_invalid'\n        self.model_file = Path('test_model.pt')\n        self.updated_time = datetime.datetime(2023, 1, 1, 0, 0, 0)\n        self.destination_path = Path(f'models/pt/best_{self.valid_model}.pt')\n\n    @patch('torch.jit.load')\n    @patch('pathlib.Path.rename')\n    @patch('pathlib.Path.is_file', return_value=True)\n    @patch('pathlib.Path.suffix', new_callable=MagicMock(return_value='.pt'))\n    async def test_update_model_file_valid(\n        self,\n        mock_suffix: MagicMock,\n        mock_is_file: MagicMock,\n        mock_rename: MagicMock,\n        mock_torch_jit_load: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test updating a valid model file with virtual files.\n        \"\"\"\n        mock_torch_jit_load.return_value = True\n\n        await update_model_file(self.valid_model, self.model_file)\n        mock_torch_jit_load.assert_called_once_with(str(self.model_file))\n\n        expected_destination_path = self.destination_path.resolve()\n        mock_rename.assert_called_once_with(expected_destination_path)\n\n    async def test_update_model_file_invalid_model(self) -> None:\n        \"\"\"\n        Test updating with an invalid model key.\n        \"\"\"\n        with self.assertRaises(ValueError) as context:\n            await update_model_file(self.invalid_model, self.model_file)\n        self.assertIn('Invalid model key', str(context.exception))\n\n    @patch('pathlib.Path.is_file', return_value=False)\n    async def test_update_model_file_invalid_file(\n        self,\n        mock_is_file: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test updating with an invalid file path.\n        \"\"\"\n        with self.assertRaises(ValueError) as context:\n            await update_model_file(self.valid_model, self.model_file)\n        self.assertIn('Invalid file', str(context.exception))\n\n    @patch('torch.load', side_effect=Exception('Invalid model format'))\n    @patch('pathlib.Path.is_file', return_value=True)\n    @patch('pathlib.Path.suffix', new_callable=MagicMock(return_value='.pt'))\n    async def test_update_model_file_invalid_torch_file(\n        self,\n        mock_suffix: MagicMock,\n        mock_is_file: MagicMock,\n        mock_torch_load: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test updating with an invalid `.pt` file.\n        \"\"\"\n        with self.assertRaises(ValueError) as context:\n            await update_model_file(self.valid_model, self.model_file)\n        self.assertIn('Invalid PyTorch model file', str(context.exception))\n\n    @patch('pathlib.Path.is_file', return_value=False)\n    async def test_get_new_model_file_no_file(\n        self, mock_is_file: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test retrieving a model file when it doesn't exist.\n        \"\"\"\n        result = await get_new_model_file(self.valid_model, self.updated_time)\n        self.assertIsNone(result)\n\n    @patch('pathlib.Path.stat')\n    @patch('pathlib.Path.is_file', return_value=True)\n    @patch(\n        'pathlib.Path.open',\n        new_callable=mock_open,\n        read_data=b'model_data',\n    )\n    async def test_get_new_model_file_updated(\n        self,\n        mock_open: MagicMock,\n        mock_is_file: MagicMock,\n        mock_stat: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test retrieving an updated model file.\n        \"\"\"\n        mock_stat.return_value.st_mtime = (\n            self.updated_time + datetime.timedelta(days=1)\n        ).timestamp()\n\n        result = await get_new_model_file(self.valid_model, self.updated_time)\n        self.assertIsInstance(result, bytes)\n        self.assertEqual(result, b'model_data')\n        mock_open.assert_called_once_with('rb')  # \n\n    @patch('pathlib.Path.stat')\n    @patch('pathlib.Path.is_file', return_value=True)\n    async def test_get_new_model_file_not_updated(\n        self, mock_is_file: MagicMock, mock_stat: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test retrieving a model file that has not been updated.\n        \"\"\"\n        mock_stat.return_value.st_mtime = self.updated_time.timestamp()\n\n        result = await get_new_model_file(self.valid_model, self.updated_time)\n        self.assertIsNone(result)\n\n    @patch(\n        'pathlib.Path.open',\n        side_effect=FileNotFoundError('[Errno 2] No such file or directory'),\n    )\n    @patch('pathlib.Path.is_file', return_value=True)\n    async def test_get_new_model_file_read_error(\n        self, mock_is_file: MagicMock, mock_open: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test error while reading the model file.\n        \"\"\"\n        with self.assertRaises(OSError) as context:\n            await get_new_model_file(self.valid_model, self.updated_time)\n        self.assertIn('No such file or directory', str(context.exception))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/examples/YOLO_server_api/backend/redis_pool_test.py", "content": "from __future__ import annotations\n\nimport unittest\nfrom unittest.mock import AsyncMock\nfrom unittest.mock import MagicMock\nfrom unittest.mock import patch\n\nfrom fastapi import Request\n\nfrom examples.YOLO_server_api.backend.redis_pool import get_redis_pool\nfrom examples.YOLO_server_api.backend.redis_pool import RedisClient\n\n\nclass TestRedisClient(unittest.IsolatedAsyncioTestCase):\n    \"\"\"\n    Unit tests for the RedisClient class.\n    \"\"\"\n\n    def setUp(self) -> None:\n        \"\"\"\n        Set up the test environment before each test.\n        \"\"\"\n        self.redis_url = 'redis://localhost:6379/0'\n        self.redis_client = RedisClient(self.redis_url)\n\n    async def test_initialisation(self) -> None:\n        \"\"\"\n        Test that the RedisClient is initialised correctly.\n        \"\"\"\n        self.assertEqual(self.redis_client.url, self.redis_url)\n        self.assertIsNone(self.redis_client.client)\n\n    @patch('redis.asyncio.from_url', new_callable=AsyncMock)\n    async def test_connect(self, mock_from_url: AsyncMock) -> None:\n        \"\"\"\n        Test the connect method of RedisClient.\n        \"\"\"\n        mock_redis_instance = AsyncMock()\n        mock_from_url.return_value = mock_redis_instance\n\n        # Call the connect method\n        client = await self.redis_client.connect()\n\n        # Verify that redis.from_url was called with the correct arguments\n        mock_from_url.assert_called_once_with(\n            self.redis_url,\n            encoding='utf-8',\n            decode_responses=True,\n        )\n\n        # Verify the returned client is the mocked instance\n        self.assertEqual(client, mock_redis_instance)\n        self.assertEqual(self.redis_client.client, mock_redis_instance)\n\n    @patch('redis.asyncio.from_url', new_callable=AsyncMock)\n    async def test_connect_existing_client(\n        self,\n        mock_from_url: AsyncMock,\n    ) -> None:\n        \"\"\"\n        Test that connect does not reinitialise an existing client.\n        \"\"\"\n        mock_redis_instance = AsyncMock()\n        self.redis_client.client = mock_redis_instance\n\n        # Call the connect method\n        client = await self.redis_client.connect()\n\n        # Ensure redis.from_url was not called again\n        mock_from_url.assert_not_called()\n\n        # Verify the client is still the existing instance\n        self.assertEqual(client, mock_redis_instance)\n\n    async def test_close(self) -> None:\n        \"\"\"\n        Test the close method of RedisClient.\n        \"\"\"\n        mock_redis_instance = AsyncMock()\n        self.redis_client.client = mock_redis_instance\n\n        # Call the close method\n        await self.redis_client.close()\n\n        # Verify that the close method of the Redis client was called\n        mock_redis_instance.close.assert_awaited_once()\n\n        # Verify that the client is set to None\n        self.assertIsNone(self.redis_client.client)\n\n    async def test_close_no_client(self) -> None:\n        \"\"\"\n        Test the close method when no client is connected.\n        \"\"\"\n        # Ensure no client is set\n        self.redis_client.client = None\n\n        # Call the close method\n        await self.redis_client.close()\n\n        # Verify that no exceptions are raised and client remains None\n        self.assertIsNone(self.redis_client.client)\n\n\nclass TestGetRedisPool(unittest.IsolatedAsyncioTestCase):\n    \"\"\"\n    Unit tests for the get_redis_pool function.\n    \"\"\"\n\n    @patch('redis.asyncio.from_url', new_callable=AsyncMock)\n    async def test_get_redis_pool(self, mock_from_url: AsyncMock) -> None:\n        # Build a mock Redis instance\n        mock_redis_instance = AsyncMock()\n        # Mock the from_url function to return the mock instance\n        mock_from_url.return_value = mock_redis_instance\n\n        # Build a mock RedisClient instance\n        mock_redis_client = RedisClient(url='redis://localhost')\n\n        # Ensure the client is not connected\n        self.assertIsNone(mock_redis_client.client)\n\n        # Build a mock request with the RedisClient instance\n        mock_request = MagicMock(spec=Request)\n        mock_request.app.state.redis_client = mock_redis_client\n\n        # Call the get_redis_pool function\n        client = await get_redis_pool(mock_request)\n\n        # Verify that the connect method was called\n        mock_from_url.assert_awaited_once()\n\n        # Verify the returned client is the mocked instance\n        self.assertIsNotNone(client)\n        self.assertIs(client, mock_redis_instance)\n        self.assertIs(mock_redis_client.client, mock_redis_instance)\n\n    async def test_get_redis_pool_existing_client(self) -> None:\n        \"\"\"\n        Test that get_redis_pool does not reconnect if a client exists.\n        \"\"\"\n        # Mock the Redis client and request\n        mock_redis_instance = AsyncMock()\n\n        mock_redis_client = MagicMock(spec=RedisClient)\n        mock_redis_client.client = mock_redis_instance\n\n        mock_request = MagicMock(spec=Request)\n        mock_request.app.state.redis_client = mock_redis_client\n\n        # Call the get_redis_pool function\n        client = await get_redis_pool(mock_request)\n\n        # Verify that the connect method was not called\n        mock_redis_client.connect.assert_not_called()\n\n        # Verify the returned client is the mocked instance\n        self.assertEqual(client, mock_redis_instance)\n\n    @patch('redis.asyncio.from_url', new_callable=AsyncMock)\n    async def test_get_redis_pool_raises_runtime_error(\n        self,\n        mock_from_url: AsyncMock,\n    ) -> None:\n        \"\"\"\n        Test that get_redis_pool raises RuntimeError\n        when the client is not connected.\n\n        Args:\n            mock_from_url (AsyncMock): Mock for the from_url function.\n        \"\"\"\n        # Build a mock RedisClient instance\n        mock_redis_client = RedisClient(url='redis://localhost')\n\n        # Simulate that client is not created\n        mock_from_url.return_value = None\n\n        # Build a mock request with the RedisClient instance\n        mock_request = MagicMock(spec=Request)\n        mock_request.app.state.redis_client = mock_redis_client\n\n        # Call the get_redis_pool function and verify the exception\n        with self.assertRaises(RuntimeError) as context:\n            await get_redis_pool(mock_request)\n\n        self.assertEqual(\n            str(context.exception),\n            'Redis client is not connected.',\n        )\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/examples/YOLO_train/train_test.py", "content": "from __future__ import annotations\n\nimport argparse\nimport unittest\nfrom unittest.mock import MagicMock\nfrom unittest.mock import patch\n\nfrom examples.YOLO_train.train import main\nfrom examples.YOLO_train.train import YOLOModelHandler\n\n\nclass TestYOLOModelHandler(unittest.TestCase):\n    def setUp(self) -> None:\n        \"\"\"\n        Set up the test environment before each test.\n        \"\"\"\n        self.model_name: str = 'models/pt/best_yolo11x.pt'\n        self.handler: YOLOModelHandler = YOLOModelHandler(self.model_name)\n\n    def tearDown(self) -> None:\n        \"\"\"\n        Clean up after each test.\n        \"\"\"\n        # Delete the handler object\n        if hasattr(self, 'handler'):\n            del self.handler\n\n    @patch('examples.YOLO_train.train.YOLO')\n    def test_load_model_with_yaml(\n        self,\n        mock_yolo: unittest.mock.MagicMock,\n    ) -> None:\n        \"\"\"\n        Test loading a model from a YAML file.\n        \"\"\"\n        mock_yolo.return_value = MagicMock()\n        handler = YOLOModelHandler('models/config.yaml')\n        mock_yolo.assert_called_with('models/config.yaml')\n        self.assertIsNotNone(handler.model)\n\n    @patch('examples.YOLO_train.train.YOLO')\n    @patch('torch.cuda.is_available')\n    @patch('torch.backends.mps.is_available')\n    def test_load_model_with_pt_and_device_selection(\n        self,\n        mock_mps_available: unittest.mock.MagicMock,\n        mock_cuda_available: unittest.mock.MagicMock,\n        mock_yolo: unittest.mock.MagicMock,\n    ) -> None:\n        \"\"\"\n        Test loading a model from a .pt file and selecting the device.\n        \"\"\"\n        mock_yolo.return_value = MagicMock()\n\n        # Case 1: MPS available\n        mock_mps_available.return_value = True\n        mock_cuda_available.return_value = False\n        handler = YOLOModelHandler('models/pt/best_yolo11x.pt')\n        mock_yolo.assert_called_with('models/pt/best_yolo11x.pt')\n        self.assertEqual(handler.device.type, 'mps')\n\n        # Case 2: CUDA available but MPS is not\n        mock_mps_available.return_value = False\n        mock_cuda_available.return_value = True\n        handler = YOLOModelHandler('models/pt/best_yolo11x.pt')\n        mock_yolo.assert_called_with('models/pt/best_yolo11x.pt')\n        self.assertEqual(handler.device.type, 'cuda')\n\n        # Case 3: Neither MPS nor CUDA is available, should fall back to CPU\n        mock_mps_available.return_value = False\n        mock_cuda_available.return_value = False\n        handler = YOLOModelHandler('models/pt/best_yolo11x.pt')\n        mock_yolo.assert_called_with('models/pt/best_yolo11x.pt')\n        self.assertEqual(handler.device.type, 'cpu')\n\n    @patch('examples.YOLO_train.train.YOLO')\n    def test_load_model(self, mock_yolo: unittest.mock.MagicMock) -> None:\n        \"\"\"\n        Test loading a model.\n        \"\"\"\n        mock_yolo.return_value = MagicMock()\n        handler = YOLOModelHandler(self.model_name)\n        mock_yolo.assert_called_with(self.model_name)\n        self.assertIsNotNone(handler.model)\n\n    def test_load_model_with_unsupported_format(self) -> None:\n        \"\"\"\n        Test that loading a model with an unsupported format raises ValueError.\n        \"\"\"\n        with self.assertRaises(ValueError) as context:\n            _ = YOLOModelHandler('unsupported_format.txt')\n        self.assertEqual(\n            str(context.exception),\n            \"Unsupported model format. Use '.yaml' or '.pt'\",\n        )\n\n    @patch('examples.YOLO_train.train.YOLO')\n    def test_train_model(self, mock_yolo: unittest.mock.MagicMock) -> None:\n        \"\"\"\n        Test training the model.\n        \"\"\"\n        mock_model = MagicMock()\n        mock_yolo.return_value = mock_model\n        handler = YOLOModelHandler(self.model_name)\n        handler.train_model(\n            data_config='tests/cv_dataset/data.yaml',\n            epochs=10,\n            optimizer='auto',\n        )\n        mock_model.train.assert_called_with(\n            data='tests/cv_dataset/data.yaml',\n            epochs=10,\n            batch=handler.batch_size,\n            optimizer='auto',\n        )\n\n    @patch('examples.YOLO_train.train.YOLO')\n    def test_train_model_without_loading(\n        self,\n        mock_yolo: unittest.mock.MagicMock,\n    ) -> None:\n        \"\"\"\n        Test that training without loading a model raises RuntimeError.\n        \"\"\"\n        handler = YOLOModelHandler('models/pt/best_yolo11x.pt')\n        handler.model = None  # Simulate that model is not loaded properly\n        with self.assertRaises(RuntimeError) as context:\n            handler.train_model('dataset/data.yaml', 10, 'auto')\n        self.assertEqual(\n            str(context.exception),\n            'The model is not loaded properly.',\n        )\n\n    @patch('examples.YOLO_train.train.YOLO')\n    def test_validate_model_without_loading(\n        self,\n        mock_yolo: unittest.mock.MagicMock,\n    ) -> None:\n        \"\"\"\n        Test that validating without loading a model raises RuntimeError.\n        \"\"\"\n        handler = YOLOModelHandler('models/pt/best_yolo11x.pt')\n        handler.model = None  # Simulate that model is not loaded properly\n        with self.assertRaises(RuntimeError) as context:\n            handler.validate_model()\n        self.assertEqual(\n            str(context.exception),\n            'The model is not loaded properly.',\n        )\n\n    @patch('examples.YOLO_train.train.YOLO')\n    def test_predict_image_without_loading(\n        self,\n        mock_yolo: unittest.mock.MagicMock,\n    ) -> None:\n        \"\"\"\n        Test that predicting without loading a model raises RuntimeError.\n        \"\"\"\n        handler = YOLOModelHandler('models/pt/best_yolo11x.pt')\n        handler.model = None  # Simulate that model is not loaded properly\n        with self.assertRaises(RuntimeError) as context:\n            handler.predict_image('path/to/image.jpg')\n        self.assertEqual(\n            str(context.exception),\n            'The model is not loaded properly.',\n        )\n\n    @patch('examples.YOLO_train.train.YOLO')\n    def test_export_model_without_loading(\n        self,\n        mock_yolo: unittest.mock.MagicMock,\n    ) -> None:\n        \"\"\"\n        Test that exporting without loading a model raises RuntimeError.\n        \"\"\"\n        handler = YOLOModelHandler('models/pt/best_yolo11x.pt')\n        handler.model = None  # Simulate that model is not loaded properly\n        with self.assertRaises(RuntimeError) as context:\n            handler.export_model('onnx')\n        self.assertEqual(\n            str(context.exception),\n            'The model is not loaded properly.',\n        )\n\n    @patch('examples.YOLO_train.train.YOLO')\n    def test_save_model_without_loading(\n        self,\n        mock_yolo: unittest.mock.MagicMock,\n    ) -> None:\n        \"\"\"\n        Test that saving without loading a model raises RuntimeError.\n        \"\"\"\n        handler = YOLOModelHandler('models/pt/best_yolo11x.pt')\n        handler.model = None  # Simulate that model is not loaded properly\n        with self.assertRaises(RuntimeError) as context:\n            handler.save_model('path/to/save/model.pt')\n        self.assertEqual(\n            str(context.exception),\n            'The model is not loaded properly.',\n        )\n\n    @patch('examples.YOLO_train.train.YOLO')\n    def test_cross_validate_model_without_loading(\n        self, mock_yolo: unittest.mock.MagicMock,\n    ) -> None:\n        \"\"\"\n        Test that cross-validation without loading a model raises RuntimeError.\n        \"\"\"\n        handler = YOLOModelHandler('models/pt/best_yolo11x.pt')\n        handler.model = None  # Simulate that model is not loaded properly\n        with self.assertRaises(RuntimeError) as context:\n            handler.cross_validate_model('dataset/data.yaml', 10, 'auto')\n        self.assertEqual(\n            str(context.exception),\n            'The model is not loaded properly.',\n        )\n\n    @patch('examples.YOLO_train.train.YOLO')\n    def test_validate_model(self, mock_yolo: unittest.mock.MagicMock) -> None:\n        \"\"\"\n        Test validating the model.\n        \"\"\"\n        mock_model = MagicMock()\n        mock_yolo.return_value = mock_model\n        handler = YOLOModelHandler(self.model_name)\n        handler.validate_model()\n        mock_model.val.assert_called_with(batch=handler.batch_size)\n\n    @patch('examples.YOLO_train.train.YOLO')\n    def test_predict_image(self, mock_yolo: unittest.mock.MagicMock) -> None:\n        \"\"\"\n        Test predicting an image.\n        \"\"\"\n        mock_model = MagicMock()\n        mock_yolo.return_value = mock_model\n        handler = YOLOModelHandler(self.model_name)\n        handler.predict_image('path/to/image.jpg')\n        mock_model.assert_called_with('path/to/image.jpg')\n\n    @patch('examples.YOLO_train.train.YOLO')\n    def test_export_model(self, mock_yolo: unittest.mock.MagicMock) -> None:\n        \"\"\"\n        Test exporting the model.\n        \"\"\"\n        mock_model = MagicMock()\n        mock_yolo.return_value = mock_model\n        handler = YOLOModelHandler(self.model_name)\n        handler.export_model('onnx')\n        mock_model.export.assert_called_with(format='onnx')\n\n    @patch('examples.YOLO_train.train.torch.save')\n    @patch('examples.YOLO_train.train.YOLO')\n    def test_save_model(\n        self,\n        mock_yolo: unittest.mock.MagicMock,\n        mock_save: unittest.mock.MagicMock,\n    ) -> None:\n        \"\"\"\n        Test saving the model.\n        \"\"\"\n        mock_model = MagicMock()\n        mock_yolo.return_value = mock_model\n        handler = YOLOModelHandler(self.model_name)\n        handler.save_model('path/to/save/model.pt')\n        mock_save.assert_called_with(\n            mock_model.state_dict(), 'path/to/save/model.pt',\n        )\n\n    @patch('examples.YOLO_train.train.KFold.split')\n    @patch('examples.YOLO_train.train.YOLO')\n    def test_cross_validate_model(\n        self,\n        mock_yolo: unittest.mock.MagicMock,\n        mock_kfold_split: unittest.mock.MagicMock,\n    ) -> None:\n        \"\"\"\n        Test cross-validation of the model.\n        \"\"\"\n        mock_model = MagicMock()\n        mock_yolo.return_value = mock_model\n        mock_kfold_split.return_value = [([0, 1], [2]), ([2], [0, 1])]\n        handler = YOLOModelHandler(self.model_name)\n        handler.cross_validate_model(\n            data_config='tests/cv_dataset/data.yaml',\n            epochs=10, optimizer='auto',\n            n_splits=2,\n        )\n\n        self.assertEqual(mock_model.train.call_count, 2)\n        self.assertEqual(mock_model.val.call_count, 2)\n\n    @patch('examples.YOLO_train.train.AutoDetectionModel.from_pretrained')\n    @patch('examples.YOLO_train.train.get_sliced_prediction')\n    def test_predict_image_sahi(\n        self,\n        mock_sahi_predict: unittest.mock.MagicMock,\n        mock_auto_detection_model: unittest.mock.MagicMock,\n    ) -> None:\n        \"\"\"\n        Test predicting an image using SAHI.\n        \"\"\"\n        mock_model = MagicMock()\n        mock_auto_detection_model.return_value = mock_model\n        mock_sahi_predict.return_value = MagicMock(\n            object_prediction_list=[{'label': 'test', 'score': 0.9}],\n        )\n\n        result = YOLOModelHandler.predict_image_sahi(\n            'models/pt/best_yolo11x.pt', 'path/to/image.jpg',\n        )\n        mock_auto_detection_model.assert_called_with(\n            model_type='yolov8',\n            model_path='models/pt/best_yolo11x.pt',\n            confidence_threshold=0.3,\n        )\n        mock_sahi_predict.assert_called()\n        self.assertIsNotNone(result)\n\n    def test_predict_image_sahi_without_model_path(self) -> None:\n        \"\"\"\n        Test that calling predict_image_sahi\n        without a valid model path raises RuntimeError.\n        \"\"\"\n        with self.assertRaises(RuntimeError) as context:\n            YOLOModelHandler.predict_image_sahi('', 'path/to/image.jpg')\n\n        self.assertEqual(\n            str(context.exception),\n            'The model path is not provided.',\n        )\n\n    @patch('argparse.ArgumentParser.parse_args')\n    @patch('examples.YOLO_train.train.YOLOModelHandler')\n    def test_main(\n        self,\n        mock_handler_class: unittest.mock.MagicMock,\n        mock_parse_args: unittest.mock.MagicMock,\n    ) -> None:\n        \"\"\"\n        Test the main function.\n        \"\"\"\n        mock_handler = MagicMock()\n        mock_handler_class.return_value = mock_handler\n        mock_parse_args.return_value = argparse.Namespace(\n            data_config='dataset/data.yaml',\n            epochs=100,\n            model_name='models/pt/best_yolo11x.pt',\n            export_format='onnx',\n            onnx_path=None,\n            pt_path='model.pt',\n            sahi_image_path='../../assets/IMG_1091.PNG',\n            batch_size=16,\n            optimizer='auto',\n            cross_validate=False,\n            n_splits=5,\n        )\n\n        main()\n\n        mock_handler.train_model.assert_called()\n        mock_handler.validate_model.assert_called()\n        mock_handler.export_model.assert_called_with(export_format='onnx')\n        mock_handler.save_model.assert_called_with('model.pt')\n\n    @patch('argparse.ArgumentParser.parse_args')\n    @patch('examples.YOLO_train.train.YOLOModelHandler')\n    def test_main_with_cross_validate(\n        self,\n        mock_handler_class: unittest.mock.MagicMock,\n        mock_parse_args: unittest.mock.MagicMock,\n    ) -> None:\n        \"\"\"\n        Test the main function when cross-validation is enabled.\n        \"\"\"\n        mock_handler = MagicMock()\n        mock_handler_class.return_value = mock_handler\n        mock_parse_args.return_value = argparse.Namespace(\n            data_config='dataset/data.yaml',\n            epochs=100,\n            model_name='models/pt/best_yolo11x.pt',\n            export_format='onnx',\n            onnx_path=None,\n            pt_path='model.pt',\n            sahi_image_path='../../assets/IMG_1091.PNG',\n            batch_size=16,\n            optimizer='auto',\n            cross_validate=True,  # Enable cross-validation\n            n_splits=5,\n        )\n\n        main()\n\n        # Check that cross-validation was called\n        mock_handler.cross_validate_model.assert_called_with(\n            data_config='dataset/data.yaml',\n            epochs=100,\n            optimizer='auto',\n            n_splits=5,\n        )\n\n        # Since cross-validation was used, we do not expect train_model\n        # or validate_model to be called separately\n        mock_handler.train_model.assert_not_called()\n        mock_handler.validate_model.assert_not_called()\n\n        # Export and save should still be called\n        mock_handler.export_model.assert_called_with(export_format='onnx')\n        mock_handler.save_model.assert_called_with('model.pt')\n\n    @patch('examples.YOLO_train.train.YOLOModelHandler.train_model')\n    @patch('argparse.ArgumentParser.parse_args')\n    def test_main_exception_handling(\n        self,\n        mock_parse_args: unittest.mock.MagicMock,\n        mock_train_model: unittest.mock.MagicMock,\n    ) -> None:\n        \"\"\"\n        Test the main function's exception handling.\n        \"\"\"\n        mock_parse_args.return_value = argparse.Namespace(\n            data_config='dataset/data.yaml',\n            epochs=100,\n            model_name='models/pt/best_yolo11x.pt',\n            export_format='onnx',\n            onnx_path=None,\n            pt_path='model.pt',\n            sahi_image_path='../../assets/IMG_1091.PNG',\n            batch_size=16,\n            optimizer='auto',\n            cross_validate=False,\n            n_splits=5,\n        )\n\n        # Simulate an exception during training\n        mock_train_model.side_effect = Exception('Mocked training error')\n\n        with (\n            patch('builtins.print') as mock_print,\n            self.assertRaises(SystemExit),\n        ):\n            main()\n            mock_print.assert_any_call('Error occurred: Mocked training error')\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/src/notifiers/wechat_notifier_test.py", "content": "from __future__ import annotations\n\nimport os\nimport subprocess\nimport unittest\nfrom io import BytesIO\nfrom unittest.mock import MagicMock\nfrom unittest.mock import patch\n\nimport numpy as np\nfrom PIL import Image\n\nfrom src.notifiers.wechat_notifier import main\nfrom src.notifiers.wechat_notifier import WeChatNotifier\n\n\nclass TestWeChatNotifier(unittest.TestCase):\n    \"\"\"\n    Unit tests for the WeChatNotifier class methods.\n    \"\"\"\n\n    wechat_notifier: WeChatNotifier\n\n    @classmethod\n    def setUpClass(cls) -> None:\n        \"\"\"\n        Set up the WeChatNotifier instance for tests.\n        \"\"\"\n        cls.wechat_notifier = WeChatNotifier(\n            corp_id='test_corp_id',\n            corp_secret='test_corp_secret',\n            agent_id=1000002,\n        )\n\n    @patch('requests.get')\n    def test_get_access_token(self, mock_get: MagicMock) -> None:\n        \"\"\"\n        Test the get_access_token method.\n        \"\"\"\n        mock_response = MagicMock()\n        mock_response.json.return_value = {'access_token': 'test_access_token'}\n        mock_get.return_value = mock_response\n        access_token: str = self.wechat_notifier.get_access_token()\n        self.assertEqual(access_token, 'test_access_token')\n        mock_get.assert_called_once_with(\n            'https://qyapi.weixin.qq.com/cgi-bin/gettoken?'\n            'corpid=test_corp_id&corpsecret=test_corp_secret',\n        )\n\n    @patch('requests.post')\n    def test_send_notification_no_image(self, mock_post: MagicMock) -> None:\n        \"\"\"\n        Test sending a notification without an image.\n        \"\"\"\n        mock_response = MagicMock()\n        mock_response.json.return_value = {'errcode': 0, 'errmsg': 'ok'}\n        mock_post.return_value = mock_response\n        user_id: str = 'test_user_id'\n        message: str = 'Hello, WeChat!'\n        response = self.wechat_notifier.send_notification(user_id, message)\n        self.assertEqual(response, {'errcode': 0, 'errmsg': 'ok'})\n        url: str = (\n            f\"https://qyapi.weixin.qq.com/cgi-bin/message/send?\"\n            f\"access_token={self.wechat_notifier.access_token}\"\n        )\n        payload = {\n            'touser': user_id,\n            'msgtype': 'text',\n            'agentid': self.wechat_notifier.agent_id,\n            'text': {\n                'content': message,\n            },\n            'safe': 0,\n        }\n\n        mock_post.assert_called_once_with(url, json=payload)\n\n    @patch('requests.post')\n    @patch.object(WeChatNotifier, 'upload_media')\n    def test_send_notification_with_image(\n        self,\n        mock_upload_media: MagicMock,\n        mock_post: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test sending a notification with an image.\n        \"\"\"\n        mock_post.return_value.json.return_value = {\n            'errcode': 0, 'errmsg': 'ok',\n        }\n        mock_upload_media.return_value = 'test_media_id'\n        user_id: str = 'test_user_id'\n        message: str = 'Hello, WeChat!'\n        image: np.ndarray = np.zeros((100, 100, 3), dtype=np.uint8)\n        response = self.wechat_notifier.send_notification(\n            user_id, message, image=image,\n        )\n        self.assertEqual(response, {'errcode': 0, 'errmsg': 'ok'})\n        url: str = (\n            f\"https://qyapi.weixin.qq.com/cgi-bin/message/send?\"\n            f\"access_token={self.wechat_notifier.access_token}\"\n        )\n        payload = {\n            'touser': user_id,\n            'msgtype': 'image',\n            'agentid': self.wechat_notifier.agent_id,\n            'image': {\n                'media_id': 'test_media_id',\n            },\n            'safe': 0,\n        }\n\n        mock_post.assert_called_once_with(url, json=payload)\n\n    @patch('requests.post')\n    def test_upload_media(self, mock_post: MagicMock) -> None:\n        \"\"\"\n        Test the upload_media method.\n        \"\"\"\n        mock_response = MagicMock()\n        mock_response.json.return_value = {'media_id': 'test_media_id'}\n        mock_post.return_value = mock_response\n        image: np.ndarray = np.zeros((100, 100, 3), dtype=np.uint8)\n        buffer: BytesIO = BytesIO()\n        image_pil: Image.Image = Image.fromarray(image)\n        image_pil.save(buffer, format='PNG')\n        buffer.seek(0)\n        media_id: str = self.wechat_notifier.upload_media(image)\n        self.assertEqual(media_id, 'test_media_id')\n\n        args, kwargs = mock_post.call_args\n        self.assertEqual(kwargs['files']['media'][0], 'image.png')\n        self.assertIsInstance(kwargs['files']['media'][1], BytesIO)\n        self.assertEqual(kwargs['files']['media'][2], 'image/png')\n\n    @patch(\n        'src.notifiers.wechat_notifier.WeChatNotifier.send_notification',\n        return_value={'errcode': 0, 'errmsg': 'ok'},\n    )\n    @patch(\n        'src.notifiers.wechat_notifier.WeChatNotifier.get_access_token',\n        return_value='test_access_token',\n    )\n    @patch('src.notifiers.wechat_notifier.os.getenv')\n    def test_main(\n        self,\n        mock_getenv: MagicMock,\n        mock_get_access_token: MagicMock,\n        mock_send_notification: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test the main function.\n        \"\"\"\n        mock_getenv.side_effect = lambda key: {\n            'WECHAT_CORP_ID': 'test_corp_id',\n            'WECHAT_CORP_SECRET': 'test_corp_secret',\n            'WECHAT_AGENT_ID': '1000002',\n        }.get(key, '')\n\n        with patch('builtins.print') as mock_print:\n            main()\n            mock_send_notification.assert_called_once()\n            args, kwargs = mock_send_notification.call_args\n            self.assertEqual(args[0], 'your_user_id_here')\n            self.assertEqual(args[1], 'Hello, WeChat!')\n            if len(args) > 2:\n                self.assertIsInstance(args[2], np.ndarray)\n                self.assertEqual(args[2].shape, (100, 100, 3))\n            mock_print.assert_called()\n            print_args, print_kwargs = mock_print.call_args\n            self.assertIn('errcode', print_args[0])\n            self.assertIn('errmsg', print_args[0])\n\n    @patch('requests.post')\n    @patch.dict(\n        os.environ, {\n            'WECHAT_CORP_ID': 'test_corp_id',\n            'WECHAT_CORP_SECRET': 'test_corp_secret',\n            'WECHAT_AGENT_ID': '1000002',\n        },\n    )\n    def test_main_as_script(self, mock_post: MagicMock) -> None:\n        \"\"\"\n        Test running the wechat_notifier.py script as the main program.\n        \"\"\"\n        mock_response: MagicMock = MagicMock()\n        mock_response.status_code = 200\n        mock_post.return_value = mock_response\n\n        # Get the absolute path to the wechat_notifier.py script\n        script_path = os.path.abspath(\n            os.path.join(\n                os.path.dirname(__file__),\n                '../../../src/notifiers/wechat_notifier.py',\n            ),\n        )\n\n        # Run the script using subprocess\n        result = subprocess.run(\n            ['python', script_path],\n            capture_output=True, text=True,\n        )\n\n        # Print stdout and stderr for debugging\n        print('STDOUT:', result.stdout)\n        print('STDERR:', result.stderr)\n\n        # Assert that the script runs without errors\n        self.assertEqual(\n            result.returncode, 0,\n            'Script exited with a non-zero status.',\n        )\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/examples/YOLO_server_api/backend/cache_test.py", "content": "from __future__ import annotations\n\nimport unittest\nfrom unittest.mock import AsyncMock\nfrom unittest.mock import MagicMock\nfrom unittest.mock import patch\n\nfrom fastapi import HTTPException\n\nfrom examples.YOLO_server_api.backend.cache import custom_rate_limiter\nfrom examples.YOLO_server_api.backend.cache import get_user_data\nfrom examples.YOLO_server_api.backend.cache import set_user_data\n\n\nclass CacheTestCase(unittest.IsolatedAsyncioTestCase):\n    \"\"\"\n    Test cases for cache functionalities and rate limiter.\n    \"\"\"\n\n    async def test_get_user_data(self):\n        \"\"\"\n        Test retrieving user data from the Redis cache.\n        \"\"\"\n        redis_pool = AsyncMock()\n        redis_pool.get.return_value = (\n            '{\"username\": \"test_user\", \"role\": \"user\"}'\n        )\n\n        user_data = await get_user_data(redis_pool, 'test_user')\n\n        # Validate the user data retrieved from Redis\n        self.assertIsInstance(user_data, dict)\n        self.assertEqual(user_data['username'], 'test_user')\n        self.assertEqual(user_data['role'], 'user')\n\n        redis_pool.get.assert_called_once_with('user_cache:test_user')\n\n    async def test_get_user_data_not_found(self):\n        \"\"\"\n        Test retrieving user data when it does not exist in the Redis cache.\n        \"\"\"\n        redis_pool = AsyncMock()\n        redis_pool.get.return_value = None  # \n\n        user_data = await get_user_data(redis_pool, 'nonexistent_user')\n\n        #  None\n        self.assertIsNone(user_data)\n        redis_pool.get.assert_called_once_with('user_cache:nonexistent_user')\n\n    async def test_set_user_data(self):\n        \"\"\"\n        Test storing user data in the Redis cache.\n        \"\"\"\n        redis_pool = AsyncMock()\n\n        user_data = {'username': 'test_user', 'role': 'user'}\n        await set_user_data(redis_pool, 'test_user', user_data)\n\n        # Validate the user data stored in Redis\n        redis_pool.set.assert_called_once_with(\n            'user_cache:test_user',\n            '{\"username\": \"test_user\", \"role\": \"user\"}',\n        )\n\n    async def test_rate_limiter_guest_role(self):\n        \"\"\"\n        Test rate limiter functionality for a guest role that exceeds\n        the limit.\n        \"\"\"\n        # Mock Redis and request\n        redis_pool = AsyncMock()\n        redis_pool.incr.return_value = 25  # Exceeding limit\n        redis_pool.ttl.return_value = -1\n\n        mock_request = MagicMock()\n        mock_request.app.state.redis_client.client = redis_pool\n        mock_request.url.path = '/rate_limit_test'\n\n        # Mock JWT credentials\n        mock_jwt_access = MagicMock(\n            subject={\n                'role': 'guest',\n                'username': 'test_user',\n                'jti': 'test_jti',\n            },\n        )\n\n        # Mock Redis user data\n        with patch(\n            'examples.YOLO_server_api.backend.cache.get_user_data',\n            return_value={'jti_list': ['test_jti']},\n        ):\n            with self.assertRaises(HTTPException) as exc:\n                await custom_rate_limiter(\n                    mock_request,\n                    mock_jwt_access,\n                )\n            self.assertEqual(exc.exception.status_code, 429)\n            self.assertEqual(exc.exception.detail, 'Rate limit exceeded')\n\n    async def test_rate_limiter_with_ttl_expiry(self):\n        \"\"\"\n        Test rate limiter functionality where TTL is set because it was -1.\n        \"\"\"\n        # Mock Redis and request\n        redis_pool = AsyncMock()\n        redis_pool.incr.return_value = 10  # Within limit\n        redis_pool.ttl.return_value = -1  # TTL not set\n\n        mock_request = MagicMock()\n        mock_request.app.state.redis_client.client = redis_pool\n        mock_request.url.path = '/rate_limit_test'\n\n        # Mock JWT credentials\n        mock_jwt_access = MagicMock(\n            subject={\n                'role': 'guest',\n                'username': 'test_user',\n                'jti': 'test_jti',\n            },\n        )\n\n        # Mock Redis user data with jti\n        with patch(\n            'examples.YOLO_server_api.backend.cache.get_user_data',\n            return_value={'jti_list': ['test_jti']},\n        ):\n            remaining_requests = await custom_rate_limiter(\n                mock_request,\n                mock_jwt_access,\n            )\n\n        # Assert remaining requests are calculated correctly\n        self.assertEqual(remaining_requests, 24 - 10)\n\n        # Verify Redis interactions\n        redis_pool.incr.assert_called_once_with(\n            'rate_limit:guest:test_user:/rate_limit_test',\n        )\n        redis_pool.ttl.assert_called_once_with(\n            'rate_limit:guest:test_user:/rate_limit_test',\n        )\n        redis_pool.expire.assert_called_once_with(\n            'rate_limit:guest:test_user:/rate_limit_test', 86400,\n        )\n\n    async def test_rate_limiter_invalid_jti(self):\n        \"\"\"\n        Test rate limiter with an invalid token jti.\n        \"\"\"\n        redis_pool = AsyncMock()\n\n        mock_request = MagicMock()\n        mock_request.app.state.redis_client.client = redis_pool\n        mock_request.url.path = '/rate_limit_test'\n\n        mock_jwt_access = MagicMock(\n            subject={\n                'role': 'user', 'username': 'test_user',\n                'jti': 'invalid_jti',\n            },\n        )\n\n        # Mock Redis user data with invalid jti\n        with patch(\n            'examples.YOLO_server_api.backend.cache.get_user_data',\n            return_value={'jti_list': ['valid_jti']},\n        ):\n            with self.assertRaises(HTTPException) as exc:\n                await custom_rate_limiter(\n                    mock_request,\n                    mock_jwt_access,\n                )\n            self.assertEqual(exc.exception.status_code, 401)\n            self.assertEqual(\n                exc.exception.detail,\n                'Token jti is invalid or replaced',\n            )\n\n    async def test_rate_limiter_missing_or_invalid_fields(self):\n        \"\"\"\n        Test rate limiter when the token has missing or invalid fields.\n        \"\"\"\n        redis_pool = AsyncMock()\n\n        mock_request = MagicMock()\n        mock_request.app.state.redis_client.client = redis_pool\n        mock_request.url.path = '/rate_limit_test'\n\n        # Mock missing username field\n        mock_jwt_access = MagicMock(\n            subject={\n                'role': 'guest',\n                'jti': 'test_jti',  # Lack of username\n            },\n        )\n\n        with self.assertRaises(HTTPException) as exc:\n            await custom_rate_limiter(mock_request, mock_jwt_access)\n        self.assertEqual(exc.exception.status_code, 401)\n        self.assertEqual(\n            exc.exception.detail,\n            'Token is missing or invalid fields',\n        )\n\n        # Mock missing jti field\n        mock_jwt_access = MagicMock(\n            subject={\n                'role': 'guest',\n                'username': 'test_user',  # Lack of jti\n            },\n        )\n\n        with self.assertRaises(HTTPException) as exc:\n            await custom_rate_limiter(mock_request, mock_jwt_access)\n        self.assertEqual(exc.exception.status_code, 401)\n        self.assertEqual(\n            exc.exception.detail,\n            'Token is missing or invalid fields',\n        )\n\n        # Mock invalid data types for username and jti\n        mock_jwt_access = MagicMock(\n            subject={\n                'role': 'guest',\n                'username': 123,  # Non-string\n                'jti': ['invalid_jti'],  # Non-string\n            },\n        )\n\n        with self.assertRaises(HTTPException) as exc:\n            await custom_rate_limiter(mock_request, mock_jwt_access)\n        self.assertEqual(exc.exception.status_code, 401)\n        self.assertEqual(\n            exc.exception.detail,\n            'Token is missing or invalid fields',\n        )\n\n    async def test_rate_limiter_no_user_in_redis(self):\n        \"\"\"\n        Test rate limiter when no user data is found in Redis.\n        \"\"\"\n        redis_pool = AsyncMock()\n\n        mock_request = MagicMock()\n        mock_request.app.state.redis_client.client = redis_pool\n        mock_request.url.path = '/rate_limit_test'\n\n        mock_jwt_access = MagicMock(\n            subject={\n                'role': 'guest',\n                'username': 'test_user',\n                'jti': 'test_jti',\n            },\n        )\n\n        # Mock no user data in Redis\n        with patch(\n            'examples.YOLO_server_api.backend.cache.get_user_data',\n            return_value=None,  # No such user in Redis\n        ):\n            with self.assertRaises(HTTPException) as exc:\n                await custom_rate_limiter(mock_request, mock_jwt_access)\n            self.assertEqual(exc.exception.status_code, 401)\n            self.assertEqual(exc.exception.detail, 'No such user in Redis')\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/examples/YOLO_server_api/backend/config_test.py", "content": "from __future__ import annotations\n\nimport os\nimport unittest\nfrom unittest.mock import patch\n\nfrom examples.YOLO_server_api.backend.config import Settings\n\n\nclass TestSettings(unittest.TestCase):\n    \"\"\"\n    Unit tests for the FastAPI app settings configuration.\n    \"\"\"\n\n    def setUp(self):\n        \"\"\"\n        Backup environment variables\n        \"\"\"\n        self.original_jwt_secret_key = os.getenv('JWT_SECRET_KEY')\n        self.original_database_url = os.getenv('DATABASE_URL')\n\n    def tearDown(self):\n        \"\"\"\n        Restore environment variables after each test.\n        \"\"\"\n        if self.original_jwt_secret_key is not None:\n            os.environ['JWT_SECRET_KEY'] = self.original_jwt_secret_key\n        if self.original_database_url is not None:\n            os.environ['DATABASE_URL'] = self.original_database_url\n\n    @patch.dict(\n        os.environ,\n        {\n            'JWT_SECRET_KEY': 'your_fallback_secret_key',\n            'DATABASE_URL': (\n                'mysql+asyncmy://test_user:test_password@localhost/'\n                'test_db'\n            ),\n        },\n    )\n    def test_settings_with_env_variables(self):\n        \"\"\"\n        Test the settings configuration with environment variables.\n        \"\"\"\n        # Instantiate the Settings class with environment variables\n        settings = Settings()\n\n        # Assert that the settings are correctly loaded from\n        # environment variables\n        self.assertEqual(\n            settings.authjwt_secret_key,\n            'your_fallback_secret_key',\n        )\n\n        # Assert that the database URL is correctly loaded from\n        # environment variables\n        self.assertEqual(\n            settings.sqlalchemy_database_uri,\n            'mysql+asyncmy://username:password@mysql/'\n            'construction_hazard_detection',\n        )\n\n        # Assert that the SQLAlchemy track modifications setting is correctly\n        # loaded from environment variables\n        self.assertFalse(settings.sqlalchemy_track_modifications)\n\n    @patch.dict(os.environ, {}, clear=True)\n    def test_settings_with_default_values(self):\n        \"\"\"\n        Test the settings configuration with default values.\n        \"\"\"\n        # Instantiate the Settings class with default values\n        settings = Settings()\n\n        # Assert that the settings are correctly loaded with default values\n        self.assertEqual(\n            settings.authjwt_secret_key,\n            'your_fallback_secret_key',\n        )\n\n        # Assert that the database URL is correctly loaded with default values\n        self.assertEqual(\n            settings.sqlalchemy_database_uri,\n            'mysql+asyncmy://username:password@mysql/'\n            'construction_hazard_detection',\n        )\n\n        # Assert that the SQLAlchemy track modifications setting is correctly\n        # loaded with default\n        self.assertFalse(settings.sqlalchemy_track_modifications)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/src/stream_viewer_test.py", "content": "from __future__ import annotations\n\nimport unittest\nfrom unittest.mock import MagicMock\nfrom unittest.mock import patch\n\nfrom src.stream_viewer import main\nfrom src.stream_viewer import StreamViewer\n\n\nclass TestStreamViewer(unittest.TestCase):\n    \"\"\"\n    Unit tests for the StreamViewer class methods.\n    \"\"\"\n\n    @patch('src.stream_viewer.cv2.VideoCapture')\n    def test_initialisation(self, mock_video_capture: MagicMock) -> None:\n        \"\"\"\n        Test the initialisation of the StreamViewer instance.\n        \"\"\"\n        # Initialise StreamViewer with a test URL\n        stream_url: str = 'tests/videos/test.mp4'\n        viewer: StreamViewer = StreamViewer(stream_url)\n\n        # Check if the URL is set correctly\n        self.assertEqual(viewer.stream_url, stream_url)\n\n        # Check if the window name is set correctly\n        self.assertEqual(viewer.window_name, 'Stream Viewer')\n\n        # Check if VideoCapture was called with the correct URL\n        mock_video_capture.assert_called_once_with(stream_url)\n\n    @patch('src.stream_viewer.cv2.destroyAllWindows')\n    @patch('src.stream_viewer.cv2.VideoCapture')\n    @patch('src.stream_viewer.cv2.waitKey')\n    @patch('src.stream_viewer.cv2.imshow')\n    def test_display_stream(\n        self,\n        mock_imshow: MagicMock,\n        mock_wait_key: MagicMock,\n        mock_video_capture: MagicMock,\n        mock_destroy_all_windows: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test the display_stream method for streaming video.\n        \"\"\"\n        # Mock VideoCapture instance\n        mock_cap_instance = MagicMock()\n        mock_video_capture.return_value = mock_cap_instance\n\n        # Simulate read() returning True with a dummy frame\n        mock_cap_instance.read.side_effect = [\n            (True, MagicMock()), (True, MagicMock()), (False, None),\n        ]\n\n        # Simulate waitKey() returning 'q' to break the loop\n        mock_wait_key.side_effect = [ord('a'), ord('b'), ord('q')]\n\n        # Initialise StreamViewer and call display_stream\n        viewer: StreamViewer = StreamViewer('https://example.com/stream')\n        viewer.display_stream()\n\n        # Check if imshow was called correctly\n        self.assertEqual(mock_imshow.call_count, 2)\n\n        # Check if waitKey was called at least twice\n        self.assertGreaterEqual(mock_wait_key.call_count, 2)\n\n        # Check if read was called at least twice\n        self.assertGreaterEqual(mock_cap_instance.read.call_count, 2)\n\n        # Check if destroyAllWindows was called\n        mock_destroy_all_windows.assert_called_once()\n\n    @patch('src.stream_viewer.cv2.VideoCapture')\n    @patch('src.stream_viewer.cv2.destroyAllWindows')\n    def test_release_resources(\n        self,\n        mock_destroy_all_windows: MagicMock,\n        mock_video_capture: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test the release_resources method.\n        \"\"\"\n        # Mock VideoCapture instance\n        mock_cap_instance = MagicMock()\n        mock_video_capture.return_value = mock_cap_instance\n\n        # Initialise StreamViewer and call release_resources\n        viewer: StreamViewer = StreamViewer('https://example.com/stream')\n        viewer.release_resources()\n\n        # Check if release was called on VideoCapture instance\n        mock_cap_instance.release.assert_called_once()\n\n        # Check if destroyAllWindows was called\n        mock_destroy_all_windows.assert_called_once()\n\n    @patch('src.stream_viewer.StreamViewer.display_stream')\n    @patch('src.stream_viewer.StreamViewer.__init__', return_value=None)\n    def test_main(\n        self,\n        mock_init: MagicMock,\n        mock_display_stream: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test the main function.\n        \"\"\"\n        main()\n\n        # Check if StreamViewer was initialised with the correct URL\n        mock_init.assert_called_once_with(\n            'https://cctv4.kctmc.nat.gov.tw/50204bfc/',\n        )\n\n        # Check if display_stream was called\n        mock_display_stream.assert_called_once()\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/examples/streaming_web/backend/utils_test.py", "content": "from __future__ import annotations\n\nimport base64\nimport json\nimport unittest\nfrom unittest.mock import AsyncMock\nfrom unittest.mock import MagicMock\nfrom unittest.mock import mock_open\nfrom unittest.mock import patch\n\nimport redis\nfrom fastapi import HTTPException\nfrom fastapi import WebSocket\nfrom linebot import LineBotApi\nfrom linebot.models import TextSendMessage\n\nfrom examples.streaming_web.backend.utils import RedisManager\nfrom examples.streaming_web.backend.utils import Utils\nfrom examples.streaming_web.backend.utils import WebhookHandler\n\n\nclass TestRedisManager(unittest.IsolatedAsyncioTestCase):\n    \"\"\"\n    Test suite for RedisManager methods.\n    \"\"\"\n\n    def setUp(self) -> None:\n        \"\"\"\n        Set up the test environment before each test.\n        \"\"\"\n        self.redis_mock = MagicMock(spec=redis.Redis)\n        self.redis_mock.xrevrange = AsyncMock()\n        self.redis_mock.scan = AsyncMock()\n        self.redis_mock.get = AsyncMock()\n        self.redis_mock.set = AsyncMock()\n        self.redis_mock.delete = AsyncMock()\n        self.redis_mock.close = AsyncMock()\n\n    async def test_fetch_latest_frame_for_key_with_data(self) -> None:\n        \"\"\"\n        Test fetch_latest_frame_for_key method when Redis contains valid data.\n        \"\"\"\n        redis_key = 'stream_frame:test_label_image1'\n        last_id = '0-0'\n\n        # Mock Redis response\n        message_id = '1234-0'\n        frame_data = b'sample_frame_data'\n        warnings_data = b'Sample warning'\n        self.redis_mock.xrevrange.return_value = [\n            (\n                message_id.encode('utf-8'),\n                {b'frame': frame_data, b'warnings': warnings_data},\n            ),\n        ]\n\n        redis_manager = RedisManager('localhost', 6379, 'password')\n        redis_manager.client = self.redis_mock\n        result = await redis_manager.fetch_latest_frame_for_key(\n            redis_key,\n            last_id,\n        )\n\n        expected_result = {\n            'id': message_id,\n            'image': base64.b64encode(frame_data).decode('utf-8'),\n            'warnings': warnings_data.decode('utf-8'),\n        }\n        self.assertEqual(result, expected_result)\n\n    async def test_fetch_latest_frame_for_key_no_data(self) -> None:\n        \"\"\"\n        Test fetch_latest_frame_for_key method when Redis contains no new data.\n        \"\"\"\n        redis_key = 'stream_frame:test_label_image1'\n        last_id = '0-0'\n\n        # Mock Redis response\n        self.redis_mock.xrevrange.return_value = []\n\n        redis_manager = RedisManager('localhost', 6379, 'password')\n        redis_manager.client = self.redis_mock\n        result = await redis_manager.fetch_latest_frame_for_key(\n            redis_key,\n            last_id,\n        )\n\n        self.assertIsNone(result)\n\n    async def test_fetch_latest_frame_for_key_no_frame_data(self) -> None:\n        \"\"\"\n        Test fetch_latest_frame_for_key method when Redis contains no new data.\n        \"\"\"\n        redis_key = 'stream_frame:test_label_image1'\n        last_id = '0-0'\n        self.redis_mock.xrevrange.return_value = [\n            (b'1234-0', {b'warnings': b'No frame here'}),\n        ]\n\n        redis_manager = RedisManager('localhost', 6379, 'password')\n        redis_manager.client = self.redis_mock\n        result = await redis_manager.fetch_latest_frame_for_key(\n            redis_key,\n            last_id,\n        )\n\n        self.assertIsNone(result)\n\n    async def test_fetch_latest_frames(self) -> None:\n        \"\"\"\n        Test the fetch_latest_frames method for multiple streams.\n        \"\"\"\n        last_ids = {\n            'stream_frame:test_label|image1': '0-0',\n            'stream_frame:test_label|image2': '0-0',\n        }\n\n        # Mock Redis response\n        self.redis_mock.xrevrange.side_effect = [\n            [('1234-0', {b'frame': b'image1_frame'})],\n            [('5678-0', {b'frame': b'image2_frame'})],\n        ]\n\n        redis_manager = RedisManager('localhost', 6379, 'password')\n        redis_manager.client = self.redis_mock\n        result = await redis_manager.fetch_latest_frames(last_ids)\n\n        expected_result = [\n            {\n                'key': 'image1', 'image': base64.b64encode(\n                    b'image1_frame',\n                ).decode('utf-8'),\n            },\n            {\n                'key': 'image2', 'image': base64.b64encode(\n                    b'image2_frame',\n                ).decode('utf-8'),\n            },\n        ]\n        self.assertEqual(result, expected_result)\n\n    async def test_fetch_latest_frames_empty(self) -> None:\n        \"\"\"\n        Test fetch_latest_frames method when Redis contains no new data.\n        \"\"\"\n        last_ids = {\n            'stream_frame:test_label|image1': '0-0',\n            'stream_frame:test_label|image2': '0-0',\n        }\n\n        self.redis_mock.xrevrange.side_effect = [\n            [],  # key\n            [('5678-0', {b'frame': b'image2_frame'})],\n        ]\n\n        redis_manager = RedisManager('localhost', 6379, 'password')\n        redis_manager.client = self.redis_mock\n        result = await redis_manager.fetch_latest_frames(last_ids)\n\n        expected_result = [\n            {\n                'key': 'image2', 'image': base64.b64encode(\n                    b'image2_frame',\n                ).decode('utf-8'),\n            },\n        ]\n        self.assertEqual(result, expected_result)\n\n    async def test_get_labels(self) -> None:\n        \"\"\"\n        Test the get_labels function to ensure it returns expected labels.\n        \"\"\"\n        # Mock the Redis scan method to return some keys\n        self.redis_mock.scan.return_value = (\n            0, [\n                b'stream_frame:bGFiZWwx|image1',\n                b'stream_frame:bGFiZWwx|image2',\n                b'stream_frame:bGFiZWwy|image1',\n                b'stream_frame:bGFiZWwz|image1',\n                b'stream_frame:dGVzdA==|image',\n                b'__invalid_key',\n                b'_another_invalid_key',\n            ],\n        )\n\n        # Call the function\n        redis_manager = RedisManager('localhost', 6379, 'password')\n        redis_manager.client = self.redis_mock\n        result = await redis_manager.get_labels()\n\n        # Check the expected result\n        expected_result = ['label1', 'label2', 'label3']\n        self.assertEqual(result, expected_result)\n\n    async def test_get_keys_for_label(self) -> None:\n        \"\"\"\n        Test the get_keys_for_label function to ensure it returns correct keys.\n        \"\"\"\n        label = 'label1'\n        encoded_label = Utils.encode(label)\n\n        # Mock the Redis scan method to return keys matching the label\n        self.redis_mock.scan.return_value = (\n            0, [\n                f'stream_frame:{encoded_label}|image1'.encode(),\n                f'stream_frame:{encoded_label}|image2'.encode(),\n            ],\n        )\n\n        # Call the function\n        redis_manager = RedisManager('localhost', 6379, 'password')\n        redis_manager.client = self.redis_mock\n        result = await redis_manager.get_keys_for_label(label)\n\n        # Check the expected result\n        expected_result = [\n            f'stream_frame:{encoded_label}|image1',\n            f'stream_frame:{encoded_label}|image2',\n        ]\n        self.assertEqual(result, expected_result)\n\n    async def test_update_partial_config(self) -> None:\n        \"\"\"\n        Test the update_partial_config function to ensure it updates correctly.\n        \"\"\"\n        key = 'new_key'\n        value = 'new_value'\n        cached_config = {'existing_key': 'existing_value'}\n\n        # Mock the Redis get and set methods\n        self.redis_mock.get.return_value = json.dumps(\n            cached_config,\n        ).encode('utf-8')\n\n        redis_manager = RedisManager('localhost', 6379, 'password')\n        redis_manager.client = self.redis_mock\n        await redis_manager.update_partial_config(key, value)\n\n        # Check if the set method was called with the updated config\n        cached_config[key] = value\n        self.redis_mock.set.assert_called_once_with(\n            'config_cache', json.dumps(cached_config), ex=3600,\n        )\n\n    async def test_get_partial_config(self) -> None:\n        \"\"\"\n        Test the get_partial_config function to ensure it retrieves correctly.\n        \"\"\"\n        key = 'existing_key'\n        cached_config = {'existing_key': 'existing_value'}\n\n        # Mock the Redis get method\n        self.redis_mock.get.return_value = json.dumps(\n            cached_config,\n        ).encode('utf-8')\n\n        redis_manager = RedisManager('localhost', 6379, 'password')\n        redis_manager.client = self.redis_mock\n        result = await redis_manager.get_partial_config(key)\n\n        # Check the expected result\n        self.assertEqual(result, cached_config[key])\n\n    async def test_delete_config_cache(self) -> None:\n        \"\"\"\n        Test the delete_config_cache function to ensure it deletes correctly.\n        \"\"\"\n        redis_manager = RedisManager('localhost', 6379, 'password')\n        redis_manager.client = self.redis_mock\n        await redis_manager.delete_config_cache()\n\n        # Check if the delete method was called with the correct key\n        self.redis_mock.delete.assert_called_once_with('config_cache')\n\n    async def test_get_config_cache(self) -> None:\n        \"\"\"\n        Test the get_config_cache function to ensure it retrieves correctly.\n        \"\"\"\n        cached_config = {'existing_key': 'existing_value'}\n\n        # Mock the Redis get method\n        self.redis_mock.get.return_value = json.dumps(\n            cached_config,\n        ).encode('utf-8')\n\n        redis_manager = RedisManager('localhost', 6379, 'password')\n        redis_manager.client = self.redis_mock\n        result = await redis_manager.get_config_cache()\n\n        # Check the expected result\n        self.assertEqual(result, cached_config)\n\n    async def test_set_config_cache(self) -> None:\n        \"\"\"\n        Test the set_config_cache function to ensure it sets correctly.\n        \"\"\"\n        config = {'new_key': 'new_value'}\n\n        redis_manager = RedisManager('localhost', 6379, 'password')\n        redis_manager.client = self.redis_mock\n        await redis_manager.set_config_cache(config)\n\n        # Check if the set method was called with the correct config\n        self.redis_mock.set.assert_called_once_with(\n            'config_cache', json.dumps(config), ex=3600,\n        )\n\n    async def test_close(self) -> None:\n        \"\"\"\n        Test the close method to ensure the Redis connection is closed.\n        \"\"\"\n        redis_manager = RedisManager('localhost', 6379, 'password')\n        redis_manager.client = self.redis_mock\n        await redis_manager.close()\n        self.redis_mock.close.assert_awaited_once()\n\n\nclass TestUtils(unittest.IsolatedAsyncioTestCase):\n    \"\"\"\n    Test suite for utility functions in the streaming_web module.\n    \"\"\"\n\n    def setUp(self) -> None:\n        \"\"\"\n        Set up the test environment before each test.\n        \"\"\"\n        self.redis_mock = MagicMock(spec=redis.Redis)\n        self.redis_mock.scan = AsyncMock()\n        self.redis_mock.mget = AsyncMock()\n        self.redis_mock.get = AsyncMock()\n\n    def tearDown(self) -> None:\n        \"\"\"\n        Clean up after each test.\n        \"\"\"\n        self.redis_mock.reset_mock()\n\n    async def test_send_frames(self) -> None:\n        \"\"\"\n        Test the send_frames function to ensure it sends data to\n        WebSocket client.\n        \"\"\"\n        # Mock the WebSocket send_json method\n        websocket_mock = MagicMock(spec=WebSocket)\n        websocket_mock.send_json = AsyncMock()\n\n        # Prepare data\n        label = 'label1'\n        updated_data = [\n            {'key': 'image1', 'image': 'encoded_image_data_1'},\n            {'key': 'image2', 'image': 'encoded_image_data_2'},\n        ]\n\n        # Call the function\n        await Utils.send_frames(websocket_mock, label, updated_data)\n\n        # Check if send_json was called with correct data\n        expected_data = {\n            'label': label,\n            'images': updated_data,\n        }\n        websocket_mock.send_json.assert_called_once_with(expected_data)\n\n    async def test_is_base64(self) -> None:\n        \"\"\"\n        Test the is_base64 function for different cases.\n        \"\"\"\n        valid_base64 = 'QmFzZTY0U3RyaW5n'\n        invalid_base64 = 'NotBase64@#%'\n        empty_string = ''\n\n        self.assertTrue(Utils.is_base64(valid_base64))\n        self.assertFalse(Utils.is_base64(invalid_base64))\n        self.assertFalse(Utils.is_base64(empty_string))\n\n    async def test_encode(self) -> None:\n        \"\"\"\n        Test the encode function to ensure it encodes correctly.\n        \"\"\"\n        input_string = 'test_label'\n        encoded_string = Utils.encode(input_string)\n\n        # Check if encoding and underscore replacement work as expected\n        expected_encoded = base64.urlsafe_b64encode(\n            input_string.encode('utf-8'),\n        ).decode('utf-8')\n        self.assertEqual(encoded_string, expected_encoded)\n\n    async def test_decode_valid_base64(self) -> None:\n        \"\"\"\n        Test the decode function with valid Base64 input.\n        \"\"\"\n        input_string = base64.urlsafe_b64encode(\n            b'test_label',\n        ).decode('utf-8')\n        decoded_string = Utils.decode(input_string)\n        self.assertEqual(decoded_string, 'test_label')\n\n    async def test_decode_invalid_base64(self) -> None:\n        \"\"\"\n        Test the decode function with invalid Base64 input.\n        \"\"\"\n        input_string = 'Invalid_String!'\n        decoded_string = Utils.decode(input_string)\n        self.assertEqual(decoded_string, input_string)\n\n    async def test_load_configuration(self) -> None:\n        \"\"\"\n        Test the load_configuration function to ensure it loads correctly.\n        \"\"\"\n        config_path = 'test_config.json'\n        config_data = [{'key': 'value'}]\n\n        # Mock the open function to return the config data\n        with patch(\n            'builtins.open',\n            mock_open(read_data=json.dumps(config_data)),\n        ):\n            result = Utils.load_configuration(config_path)\n\n        self.assertEqual(result, config_data)\n\n    async def test_load_configuration_exception(self) -> None:\n        \"\"\"\n        Test load_configuration when an error occurs while reading the file.\n        \"\"\"\n        config_path = 'non_existent.json'\n        with patch(\n            'builtins.open',\n            side_effect=FileNotFoundError('File not found'),\n        ):\n            result = Utils.load_configuration(config_path)\n        self.assertEqual(result, [])\n\n    async def test_save_configuration(self) -> None:\n        \"\"\"\n        Test the save_configuration function to ensure it saves correctly.\n        \"\"\"\n        config_path = 'test_config.json'\n        config_data = [{'key': 'value'}]\n\n        # Mock the open function\n        with patch('builtins.open', mock_open()) as mock_file:\n            Utils.save_configuration(config_path, config_data)\n\n            # Check if the file was written with the correct data\n            mock_file().write.assert_called_once_with(\n                json.dumps(config_data, indent=4, ensure_ascii=False),\n            )\n\n    async def test_save_configuration_exception(self) -> None:\n        \"\"\"\n        Test save_configuration when an error occurs while writing to the file.\n        \"\"\"\n        config_path = 'test_config.json'\n        config_data = [{'key': 'value'}]\n\n        with patch('builtins.open', side_effect=OSError('Write error')):\n            # This function should not raise an exception\n            # even if an error occurs while writing to the file\n            Utils.save_configuration(config_path, config_data)\n\n    async def test_verify_localhost(self) -> None:\n        \"\"\"\n        Test the verify_localhost function to ensure it verifies correctly.\n        \"\"\"\n        request_mock = MagicMock()\n        request_mock.client.host = '127.0.0.1'\n\n        # Should not raise an exception\n        Utils.verify_localhost(request_mock)\n\n        request_mock.client.host = '192.168.1.1'\n        with self.assertRaises(HTTPException):\n            Utils.verify_localhost(request_mock)\n\n    async def test_update_configuration(self) -> None:\n        \"\"\"\n        Test the update_configuration function to ensure it updates correctly.\n        \"\"\"\n        config_path = 'test_config.json'\n        current_config = [{'video_url': 'url1', 'key': 'value1'}]\n        new_config = [\n            {'video_url': 'url1', 'key': 'new_value1'},\n            {'video_url': 'url2', 'key': 'value2'},\n        ]\n\n        # Mock the load_configuration and save_configuration functions\n        with patch(\n            'examples.streaming_web.backend.utils.Utils.load_configuration',\n            return_value=current_config,\n        ):\n            with patch(\n                'examples.streaming_web.backend.utils.'\n                'Utils.save_configuration',\n            ) as mock_save:\n                result = Utils.update_configuration(config_path, new_config)\n\n                # Check the expected result\n                expected_result = [\n                    {'video_url': 'url1', 'key': 'new_value1'},\n                    {'video_url': 'url2', 'key': 'value2'},\n                ]\n                self.assertEqual(result, expected_result)\n\n                # Check if the save_configuration function\n                # was called with the correct data\n                mock_save.assert_called_once_with(config_path, expected_result)\n\n    async def test_update_configuration_not_list(self) -> None:\n        \"\"\"\n        Test update_configuration when the configuration is not a list.\n        \"\"\"\n        config_path = 'test_config.json'\n        not_a_list = {'video_url': 'url1', 'key': 'value1'}\n        new_config = [{'video_url': 'url2', 'key': 'value2'}]\n\n        with patch(\n            'examples.streaming_web.backend.utils.Utils.load_configuration',\n            return_value=not_a_list,\n        ):\n            with self.assertRaises(ValueError) as cm:\n                Utils.update_configuration(config_path, new_config)\n            self.assertIn('Invalid configuration format', str(cm.exception))\n\n    async def test_get_config_cache_empty(self):\n        \"\"\"\n        Test get_config_cache when Redis contains no configuration data.\n        \"\"\"\n        # Mock Redis response\n        self.redis_mock.get.return_value = None\n\n        redis_manager = RedisManager('localhost', 6379, 'password')\n        redis_manager.client = self.redis_mock\n        result = await redis_manager.get_config_cache()\n\n        # Check the expected result\n        self.assertEqual(result, {})\n\n\nclass TestWebhookHandler(unittest.IsolatedAsyncioTestCase):\n    \"\"\"\n    Test suite for the WebhookHandler class.\n    \"\"\"\n\n    def setUp(self):\n        \"\"\"\n        Set up the test environment before each test.\n        \"\"\"\n        # Mock LineBotApi instance\n        self.line_bot_api_mock = MagicMock(spec=LineBotApi)\n        self.webhook_handler = WebhookHandler(\n            line_bot_api=self.line_bot_api_mock,\n        )\n\n    async def test_process_webhook_events_success_group(self):\n        \"\"\"\n        Test process_webhook_events with a group message containing 'token'.\n        \"\"\"\n        body = {\n            'events': [\n                {\n                    'type': 'message',\n                    'message': {'type': 'text', 'text': 'token'},\n                    'replyToken': 'test_reply_token',\n                    'source': {\n                        'type': 'group',\n                        'groupId': 'test_group_id',\n                        'userId': 'test_user_id',\n                    },\n                },\n            ],\n        }\n\n        # Call the method under test\n        responses = await self.webhook_handler.process_webhook_events(body)\n\n        # Verify the push_message call\n        self.line_bot_api_mock.push_message.assert_called_once_with(\n            'test_group_id',\n            TextSendMessage(\n                text='group ID: test_group_id\\nuser ID: test_user_id',\n            ),\n        )\n\n        # Verify the response\n        expected_responses = [\n            {'status': 'success', 'target_id': 'test_group_id'},\n        ]\n        self.assertEqual(responses, expected_responses)\n\n    async def test_process_webhook_events_success_user(self):\n        \"\"\"\n        Test process_webhook_events with a user message containing 'token'.\n        \"\"\"\n        body = {\n            'events': [\n                {\n                    'type': 'message',\n                    'message': {'type': 'text', 'text': 'token'},\n                    'replyToken': 'test_reply_token',\n                    'source': {'type': 'user', 'userId': 'test_user_id'},\n                },\n            ],\n        }\n\n        # Call the method under test\n        responses = await self.webhook_handler.process_webhook_events(body)\n\n        # Verify the push_message call\n        self.line_bot_api_mock.push_message.assert_called_once_with(\n            'test_user_id',\n            TextSendMessage(\n                text='group ID: not provided\\nuser ID: test_user_id',\n            ),\n        )\n\n        # Verify the response\n        expected_responses = [\n            {'status': 'success', 'target_id': 'test_user_id'},\n        ]\n        self.assertEqual(responses, expected_responses)\n\n    async def test_process_webhook_events_redelivery(self):\n        \"\"\"\n        Test process_webhook_events skips redelivery events.\n        \"\"\"\n        body = {\n            'events': [\n                {\n                    'type': 'message',\n                    'deliveryContext': {'isRedelivery': True},\n                    'message': {'type': 'text', 'text': 'token'},\n                },\n            ],\n        }\n\n        # Call the method under test\n        responses = await self.webhook_handler.process_webhook_events(body)\n\n        # Verify no push_message call\n        self.line_bot_api_mock.push_message.assert_not_called()\n\n        # Verify the response\n        expected_responses = [{'status': 'skipped', 'reason': 'Redelivery'}]\n        self.assertEqual(responses, expected_responses)\n\n    async def test_process_webhook_events_unexpected_error(self):\n        \"\"\"\n        Test process_webhook_events handles unexpected exceptions.\n        \"\"\"\n        body = {\n            'events': [\n                {\n                    'type': 'message',\n                    'message': {'type': 'text', 'text': 'token'},\n                    'replyToken': 'test_reply_token',\n                    'source': {'type': 'user', 'userId': 'test_user_id'},\n                },\n            ],\n        }\n\n        # Simulate an unexpected error\n        self.line_bot_api_mock.push_message.side_effect = Exception(\n            'Unexpected error',\n        )\n\n        # Call the method under test\n        responses = await self.webhook_handler.process_webhook_events(body)\n\n        # Verify the response\n        expected_responses = [\n            {'status': 'error', 'message': 'Unexpected error'},\n        ]\n        self.assertEqual(responses, expected_responses)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/src/notifiers/line_notifier_test.py", "content": "from __future__ import annotations\n\nimport os\nimport subprocess\nimport unittest\nfrom io import BytesIO\nfrom unittest.mock import MagicMock\nfrom unittest.mock import patch\n\nimport numpy as np\nfrom PIL import Image\n\nfrom src.notifiers.line_notifier import LineNotifier\nfrom src.notifiers.line_notifier import main\n\n\nclass TestLineNotifier(unittest.IsolatedAsyncioTestCase):\n    \"\"\"\n    Unit tests for the LineNotifier class methods.\n    \"\"\"\n\n    def setUp(self) -> None:\n        \"\"\"\n        Sets up the test environment by initialising test variables\n        and a LineNotifier instance.\n        \"\"\"\n        # Mock LINE Notify token for testing\n        self.line_token: str = 'test_token'\n\n        # Mock message to send\n        self.message: str = 'Test Message'\n        self.image: np.ndarray = np.zeros(\n            (100, 100, 3), dtype=np.uint8,\n        )  # Mock image\n        self.notifier: LineNotifier = LineNotifier()\n\n    @patch.dict('os.environ', {'LINE_NOTIFY_TOKEN': 'test_env_token'})\n    @patch('aiohttp.ClientSession.post')\n    async def test_init_with_env_token(self, mock_post: MagicMock) -> None:\n        \"\"\"\n        Tests the notification sending functionality\n        using an environment variable token.\n\n        Args:\n            mock_post (MagicMock): Mock object for aiohttp.ClientSession.post.\n        \"\"\"\n        mock_response: MagicMock = MagicMock()\n        mock_response.status = 200  # Simulate a successful response\n        mock_post.return_value.__aenter__.return_value = mock_response\n\n        notifier: LineNotifier = LineNotifier()\n        status_code: int = await notifier.send_notification(self.message)\n\n        # Assert that the HTTP response status is 200\n        self.assertEqual(status_code, 200)\n        # Ensure that aiohttp's post method was called exactly once\n        mock_post.assert_called_once()\n\n    async def test_init_without_token(self) -> None:\n        \"\"\"\n        Tests the behaviour when attempting to send a notification\n        without providing a token.\n\n        Raises:\n            ValueError: Expected exception when no token is provided.\n        \"\"\"\n        with self.assertRaises(ValueError):\n            await self.notifier.send_notification(\n                self.message, line_token=None,\n            )\n\n    @patch('aiohttp.ClientSession.post')\n    async def test_send_notification_without_image(\n        self, mock_post: MagicMock,\n    ) -> None:\n        \"\"\"\n        Tests sending a notification without an image.\n\n        Args:\n            mock_post (MagicMock): Mock object for aiohttp.ClientSession.post.\n        \"\"\"\n        mock_response: MagicMock = MagicMock()\n        mock_response.status = 200  # Simulate a successful response\n        mock_post.return_value.__aenter__.return_value = mock_response\n\n        status_code: int = await self.notifier.send_notification(\n            self.message, line_token=self.line_token,\n        )\n        self.assertEqual(status_code, 200)\n        mock_post.assert_called_once()\n        args, kwargs = mock_post.call_args\n        self.assertIn('data', kwargs)\n        self._assert_field_exists(\n            kwargs['data']._fields, 'message', self.message,\n        )\n\n    @patch('aiohttp.ClientSession.post')\n    async def test_send_notification_with_image(\n        self, mock_post: MagicMock,\n    ) -> None:\n        \"\"\"\n        Tests sending a notification with an image provided as a NumPy array.\n\n        Args:\n            mock_post (MagicMock): Mock object for aiohttp.ClientSession.post.\n        \"\"\"\n        mock_response: MagicMock = MagicMock()\n        mock_response.status = 200  # Simulate a successful response\n        mock_post.return_value.__aenter__.return_value = mock_response\n\n        status_code: int = await self.notifier.send_notification(\n            self.message, self.image, line_token=self.line_token,\n        )\n        self.assertEqual(status_code, 200)\n        mock_post.assert_called_once()\n        args, kwargs = mock_post.call_args\n        self.assertIn('data', kwargs)\n        self._assert_field_exists(\n            kwargs['data']._fields, 'message', self.message,\n        )\n        self._assert_field_exists(kwargs['data']._fields, 'imageFile', None)\n\n    @patch('aiohttp.ClientSession.post')\n    async def test_send_notification_with_bytes_image(\n        self, mock_post: MagicMock,\n    ) -> None:\n        \"\"\"\n        Tests sending a notification with an image provided as bytes.\n\n        Args:\n            mock_post (MagicMock): Mock object for aiohttp.ClientSession.post.\n        \"\"\"\n        mock_response: MagicMock = MagicMock()\n        mock_response.status = 200  # Simulate a successful response\n        mock_post.return_value.__aenter__.return_value = mock_response\n\n        # Convert the NumPy array image to bytes\n        buffer: BytesIO = BytesIO()\n        Image.fromarray(self.image).save(buffer, format='PNG')\n        buffer.seek(0)\n        image_bytes: bytes = buffer.read()\n\n        status_code: int = await self.notifier.send_notification(\n            self.message, image_bytes, line_token=self.line_token,\n        )\n        self.assertEqual(status_code, 200)\n        mock_post.assert_called_once()\n        args, kwargs = mock_post.call_args\n        self.assertIn('data', kwargs)\n        self._assert_field_exists(\n            kwargs['data']._fields, 'message', self.message,\n        )\n        self._assert_field_exists(kwargs['data']._fields, 'imageFile', None)\n\n    @patch('aiohttp.ClientSession.post')\n    async def test_main(self, mock_post: MagicMock) -> None:\n        \"\"\"\n        Tests the main function to ensure the entire process works as expected.\n\n        Args:\n            mock_post (MagicMock): Mock object for aiohttp.ClientSession.post.\n        \"\"\"\n        mock_response: MagicMock = MagicMock()\n        mock_response.status = 200  # Simulate a successful response\n        mock_post.return_value.__aenter__.return_value = mock_response\n\n        with patch('builtins.print') as mock_print:\n            await main()\n            mock_print.assert_called_once_with('Response code: 200')\n\n    def _assert_field_exists(\n        self, fields: list, field_name: str, expected_value: str | None = None,\n    ) -> None:\n        \"\"\"\n        Helper method to assert that a field exists in FormData fields.\n\n        Args:\n            fields (list): List of FormData fields.\n            field_name (str): The name of the field to check for.\n            expected_value (str | None):\n                The expected value of the field (optional).\n\n        Raises:\n            AssertionError: If the field is not found\n            or does not match the expected value.\n        \"\"\"\n        found = False\n        for field, _, value in fields:\n            if field.get('name') == field_name:\n                found = True\n                if expected_value is not None:\n                    self.assertEqual(value, expected_value)\n                break\n        self.assertTrue(\n            found, f\"Field '{field_name}' not found in FormData fields.\",\n        )\n\n    @patch.dict(os.environ, {'LINE_NOTIFY_TOKEN': 'test_token'})\n    @patch('aiohttp.ClientSession.post')\n    async def test_main_as_script(self, mock_post: MagicMock) -> None:\n        \"\"\"\n        Tests running the main function as a standalone script.\n\n        Args:\n            mock_post (MagicMock): Mock object for aiohttp.ClientSession.post.\n        \"\"\"\n        mock_response: MagicMock = MagicMock()\n        mock_response.status = 200  # Simulate a successful response\n        mock_post.return_value.__aenter__.return_value = mock_response\n\n        # Get the absolute path to the script\n        script_path = os.path.abspath(\n            os.path.join(\n                os.path.dirname(\n                    __file__,\n                ), '../../../src/notifiers/line_notifier.py',\n            ),\n        )\n\n        # Run the script using subprocess\n        result = subprocess.run(\n            ['python', script_path], capture_output=True, text=True,\n        )\n\n        self.assertEqual(result.returncode, 0)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/src/live_stream_tracker_test.py", "content": "from __future__ import annotations\n\nimport datetime\nimport unittest\nfrom unittest.mock import MagicMock\nfrom unittest.mock import patch\n\nimport numpy as np\n\nfrom src.live_stream_tracker import LiveStreamDetector\nfrom src.live_stream_tracker import main\n\n\nclass TestLiveStreamDetector(unittest.TestCase):\n    \"\"\"\n    Unit tests for the LiveStreamDetector class methods.\n    \"\"\"\n\n    def setUp(self) -> None:\n        \"\"\"\n        Initialise test variables and LiveStreamDetector.\n        \"\"\"\n        self.stream_url: str = 'tests/videos/test.mp4'\n        self.model_path: str = 'models/pt/best_yolo11n.pt'\n        self.detector: LiveStreamDetector = LiveStreamDetector(\n            self.stream_url, self.model_path,\n        )\n\n    @patch('src.live_stream_tracker.YOLO')\n    @patch('src.live_stream_tracker.cv2.VideoCapture')\n    def test_initialisation(\n        self,\n        mock_video_capture: MagicMock,\n        mock_yolo: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test case for initialising LiveStreamDetector.\n        \"\"\"\n        mock_cap_instance = MagicMock()\n        mock_video_capture.return_value = mock_cap_instance\n\n        detector: LiveStreamDetector = LiveStreamDetector(\n            self.stream_url, self.model_path,\n        )\n\n        self.assertEqual(detector.stream_url, self.stream_url)\n        self.assertEqual(detector.model_path, self.model_path)\n        self.assertEqual(detector.model, mock_yolo.return_value)\n        self.assertEqual(detector.cap, mock_cap_instance)\n\n    @patch('src.live_stream_tracker.YOLO')\n    @patch('src.live_stream_tracker.cv2.VideoCapture')\n    @patch('src.live_stream_tracker.datetime')\n    @patch('src.live_stream_tracker.cv2.waitKey', return_value=0xFF & ord('q'))\n    def test_generate_detections(\n        self,\n        mock_wait_key: MagicMock,\n        mock_datetime: MagicMock,\n        mock_video_capture: MagicMock,\n        mock_yolo: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test case for generating detections from a video stream.\n        \"\"\"\n        mock_cap_instance = MagicMock()\n        mock_video_capture.return_value = mock_cap_instance\n        mock_cap_instance.isOpened.side_effect = [True, True, False]\n        mock_cap_instance.read.side_effect = [\n            (True, np.zeros((480, 640, 3), dtype=np.uint8)),\n            (True, np.zeros((480, 640, 3), dtype=np.uint8)),\n            (False, None),\n        ]\n\n        mock_yolo_instance = MagicMock()\n        mock_yolo.return_value = mock_yolo_instance\n        mock_results = MagicMock()\n        mock_boxes = MagicMock()\n        mock_boxes.id = MagicMock()\n        mock_boxes.data = MagicMock()\n        mock_boxes.id.cpu.return_value = mock_boxes.id\n        mock_boxes.data.cpu.return_value = mock_boxes.data\n        mock_boxes.id.numpy.return_value = [1, 2, 3]\n        mock_boxes.data.numpy.return_value = [\n            [0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8],\n        ]\n        mock_results[0].boxes = mock_boxes\n        mock_yolo_instance.track.return_value = mock_results\n\n        mock_now = datetime.datetime(\n            2023, 1, 1, 0, 0, 0, tzinfo=datetime.timezone.utc,\n        )\n        mock_datetime.datetime.now.side_effect = [mock_now, mock_now]\n\n        frame_generator = self.detector.generate_detections()\n\n        ids, datas, frame, timestamp = next(frame_generator)\n        self.assertIsInstance(ids, list)\n        self.assertIsInstance(datas, list)\n        self.assertIsInstance(frame, np.ndarray)\n        self.assertIsInstance(timestamp, float)\n\n        try:\n            ids, datas, frame, timestamp = next(frame_generator)\n            self.assertEqual(ids, [])\n            self.assertEqual(datas, [])\n            self.assertIsInstance(frame, np.ndarray)\n            self.assertIsInstance(timestamp, float)\n        except StopIteration:\n            # Allow StopIteration to pass without failing the test\n            pass\n\n    @patch('src.live_stream_tracker.cv2.VideoCapture')\n    @patch('src.live_stream_tracker.cv2.destroyAllWindows')\n    def test_release_resources(\n        self,\n        mock_destroy_all_windows: MagicMock,\n        mock_video_capture: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test case for releasing video capture and window resources.\n        \"\"\"\n        mock_cap_instance = MagicMock()\n        mock_video_capture.return_value = mock_cap_instance\n        self.detector.cap = mock_cap_instance\n\n        self.detector.release_resources()\n\n        mock_cap_instance.release.assert_called_once()\n        mock_destroy_all_windows.assert_called_once()\n\n    @patch('src.live_stream_tracker.LiveStreamDetector.generate_detections')\n    def test_run_detection(self, mock_generate_detections: MagicMock) -> None:\n        \"\"\"\n        Test case for running detection on a video stream.\n        \"\"\"\n        mock_generate_detections.return_value = iter(\n            [(\n                [1, 2, 3], [[0.1, 0.2, 0.3, 0.4]], np.zeros(\n                    (480, 640, 3), dtype=np.uint8,\n                ), 1234567890.0,\n            )],\n        )\n\n        with patch('builtins.print') as mock_print:\n            self.detector.run_detection()\n            expected_datetime = datetime.datetime.fromtimestamp(\n                1234567890.0, tz=datetime.timezone.utc,\n            ).strftime('%Y-%m-%d %H:%M:%S')\n            self.assertTrue(\n                any(\n                    'Timestamp:' in str(\n                        call,\n                    ) and expected_datetime in str(call)\n                    for call in mock_print.call_args_list\n                ),\n            )\n            self.assertTrue(\n                any(\n                    'IDs:' in str(call)\n                    for call in mock_print.call_args_list\n                ),\n            )\n            self.assertTrue(\n                any(\n                    'Data (xyxy format):' in str(call)\n                    for call in mock_print.call_args_list\n                ),\n            )\n\n    @patch('argparse.ArgumentParser.parse_args')\n    @patch('src.live_stream_tracker.LiveStreamDetector')\n    def test_main(\n        self,\n        mock_live_stream_detector: MagicMock,\n        mock_parse_args: MagicMock,\n    ) -> None:\n        \"\"\"\n        Test case for the main function.\n        \"\"\"\n        mock_args = MagicMock()\n        mock_args.url = 'test_url'\n        mock_args.model = 'test_model'\n        mock_parse_args.return_value = mock_args\n\n        mock_detector_instance = MagicMock()\n        mock_live_stream_detector.return_value = mock_detector_instance\n\n        main()\n\n        mock_parse_args.assert_called_once()\n        mock_live_stream_detector.assert_called_once_with(\n            'test_url', 'test_model',\n        )\n        mock_detector_instance.run_detection.assert_called_once()\n        mock_detector_instance.release_resources.assert_called_once()\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/examples/line_chatbot/line_bot_test.py", "content": "from __future__ import annotations\n\nimport json\nimport unittest\nfrom unittest.mock import patch\n\nfrom fastapi.testclient import TestClient\n\nfrom examples.line_chatbot.line_bot import app\nfrom examples.line_chatbot.line_bot import handler\nfrom examples.line_chatbot.line_bot import line_bot_api\n\n\nclass TestLineBot(unittest.TestCase):\n    \"\"\"\n    Unit tests for the LINE Bot API using FastAPI.\n    This class validates webhook handling and message processing behaviours.\n    \"\"\"\n\n    def setUp(self) -> None:\n        \"\"\"\n        Set up the test environment before each test.\n        Initialise a TestClient for the FastAPI app and mock necessary objects.\n        \"\"\"\n        # FastAPI test client\n        self.client: TestClient = TestClient(app)\n\n        # Mock the LINE Bot API reply_message method\n        self.mock_line_bot_api = patch.object(\n            line_bot_api, 'reply_message',\n        ).start()\n\n        # Mock the LINE Bot WebhookHandler handle method\n        self.mock_handler_handle = patch.object(handler, 'handle').start()\n\n    def tearDown(self) -> None:\n        \"\"\"\n        Clean up resources and stop all patches after each test.\n        \"\"\"\n        patch.stopall()\n\n    def test_callback_ok(self) -> None:\n        \"\"\"\n        Test that the webhook endpoint returns 'OK' when provided with\n        a valid signature and body payload.\n        \"\"\"\n        # Fake signature and body\n        fake_signature: str = 'fake_signature'\n        fake_body: dict = {\n            'events': [\n                {\n                    'replyToken': 'reply_token',\n                    'type': 'message',\n                    'message': {\n                        'type': 'text',\n                        'text': 'Hello',\n                    },\n                },\n            ],\n        }\n\n        # Mock handler.handle to simulate no error\n        self.mock_handler_handle.return_value = None\n\n        # Send a POST request to the webhook endpoint\n        response = self.client.post(\n            '/webhook',\n            json=fake_body,\n            headers={\n                'X-Line-Signature': fake_signature,\n                'Content-Type': 'application/json',\n            },\n        )\n\n        # Validate response\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.text, 'OK')\n\n        # Ensure the handler.handle method\n        # was called once with correct arguments\n        expected_body = json.dumps(fake_body, separators=(',', ':'))\n        self.mock_handler_handle.assert_called_once_with(\n            expected_body, fake_signature,\n        )\n\n    def test_callback_missing_signature(self) -> None:\n        \"\"\"\n        Test that the webhook endpoint returns 400 if no signature is provided.\n        \"\"\"\n        # Fake body without signature\n        fake_body: dict = {'events': []}\n\n        # Send a POST request without a signature header\n        response = self.client.post(\n            '/webhook',\n            json=fake_body,\n            headers={},  # Missing X-Line-Signature\n        )\n\n        # Validate response\n        self.assertEqual(response.status_code, 400)\n\n    def test_callback_invalid_signature(self) -> None:\n        \"\"\"\n        Test that the webhook endpoint returns 400 if the signature is invalid.\n        \"\"\"\n        from linebot.exceptions import InvalidSignatureError\n\n        fake_signature: str = 'fake_signature'\n        fake_body: dict = {'events': []}\n\n        # Simulate an InvalidSignatureError being raised\n        self.mock_handler_handle.side_effect = InvalidSignatureError\n\n        # Send a POST request with an invalid signature\n        response = self.client.post(\n            '/webhook',\n            json=fake_body,\n            headers={'X-Line-Signature': fake_signature},\n        )\n\n        # Validate response\n        self.assertEqual(response.status_code, 400)\n\n    def test_callback_internal_server_error(self) -> None:\n        \"\"\"\n        Test that the webhook endpoint returns 500\n        for unexpected server errors.\n        \"\"\"\n        fake_signature: str = 'fake_signature'\n        fake_body: dict = {'events': []}\n\n        # Simulate a generic exception being raised\n        self.mock_handler_handle.side_effect = Exception('Some error')\n\n        # Send a POST request\n        response = self.client.post(\n            '/webhook',\n            json=fake_body,\n            headers={'X-Line-Signature': fake_signature},\n        )\n\n        # Validate response\n        self.assertEqual(response.status_code, 500)\n\n    def test_handle_text_message(self) -> None:\n        \"\"\"\n        Test that the handle_text_message function processes a text message\n        event correctly and sends an appropriate reply.\n        \"\"\"\n        # Create a fake TextMessage event\n        from linebot.models import MessageEvent, TextMessage\n        event: MessageEvent = MessageEvent(\n            reply_token='dummy_token',\n            message=TextMessage(text='Hello, World!'),\n        )\n\n        # Import the function to test\n        from examples.line_chatbot.line_bot import handle_text_message\n\n        # Call the function with the fake event\n        handle_text_message(event)\n\n        # Validate that line_bot_api.reply_message was called once\n        self.mock_line_bot_api.assert_called_once()\n\n        # Extract the arguments passed to reply_message\n        args, kwargs = self.mock_line_bot_api.call_args\n\n        # Verify the reply token\n        self.assertEqual(args[0], 'dummy_token')\n\n        # Verify the reply message content\n        text_send_message = args[1]\n        self.assertEqual(text_send_message.text, ': Hello, World!')\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/src/notifiers/line_notifier_message_api_test.py", "content": "from __future__ import annotations\n\nimport logging\nimport os\nimport unittest\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom unittest.mock import mock_open\nfrom unittest.mock import patch\n\nimport cv2\nimport numpy as np\n\nfrom src.notifiers.line_notifier_message_api import LineMessenger\nfrom src.notifiers.line_notifier_message_api import main\n\n\nclass TestLineMessenger(unittest.TestCase):\n    \"\"\"\n    Test cases for the LineMessenger class.\n    \"\"\"\n\n    def setUp(self):\n        \"\"\"\n        Set up mock data and environment for each test case.\n        \"\"\"\n        logging.basicConfig(level=logging.ERROR)\n\n        self.channel_access_token = 'test_channel_access_token'\n        self.messenger = LineMessenger(\n            channel_access_token=self.channel_access_token,\n        )\n        self.message = 'Test message'\n        self.recipient_id = 'test_recipient_id'\n        self.image_bytes = b'test_image_bytes'\n        self.headers = {\n            'Content-Type': 'application/json',\n            'Authorization': f\"Bearer {self.channel_access_token}\",\n        }\n\n    @patch.dict(os.environ, {'LINE_CHANNEL_ACCESS_TOKEN': ''})\n    def test_init_without_channel_access_token(self):\n        \"\"\"\n        Test initializing LineMessenger without a channel access token.\n        \"\"\"\n        with self.assertRaises(ValueError) as context:\n            LineMessenger()\n        self.assertEqual(\n            str(context.exception),\n            'LINE_CHANNEL_ACCESS_TOKEN not provided '\n            'or in environment variables.',\n        )\n\n    @patch('src.notifiers.line_notifier_message_api.requests.post')\n    @patch(\n        'src.notifiers.line_notifier_message_api.LineMessenger.'\n        'upload_image_to_cloudinary',\n    )\n    def test_push_message_with_image(\n        self,\n        mock_upload_image_to_cloudinary: unittest.mock.MagicMock,\n        mock_post: unittest.mock.MagicMock,\n    ):\n        \"\"\"\n        Test sending a message with an image via LINE Messaging API.\n        \"\"\"\n        mock_upload_image_to_cloudinary.return_value = (\n            'http://mock_image_url', 'mock_public_id',\n        )\n        mock_post.return_value.status_code = 200\n\n        response_code = self.messenger.push_message(\n            recipient_id=self.recipient_id,\n            message=self.message,\n            image_bytes=self.image_bytes,\n        )\n\n        # Verify that the upload_image_to_cloudinary method was called\n        mock_upload_image_to_cloudinary.assert_called_once_with(\n            self.image_bytes,\n        )\n\n        # Verify that the LINE API was called with the correct data\n        expected_data = {\n            'to': self.recipient_id,\n            'messages': [\n                {\n                    'type': 'text',\n                    'text': self.message,\n                },\n                {\n                    'type': 'image',\n                    'originalContentUrl': 'http://mock_image_url',\n                    'previewImageUrl': 'http://mock_image_url',\n                },\n            ],\n        }\n        mock_post.assert_called_once_with(\n            'https://api.line.me/v2/bot/message/push',\n            headers=self.headers,\n            json=expected_data,\n        )\n\n        # Verify the response code is 200\n        self.assertEqual(response_code, 200)\n\n    @patch(\n        'src.notifiers.line_notifier_message_api.LineMessenger.'\n        'delete_old_images',\n    )\n    @patch(\n        'src.notifiers.line_notifier_message_api.LineMessenger.'\n        'save_image_records',\n    )\n    def test_delete_old_images_with_interval_no_last_checked(\n        self,\n        mock_save_image_records: unittest.mock.MagicMock,\n        mock_delete_old_images: unittest.mock.MagicMock,\n    ):\n        \"\"\"\n        Test the delete_old_images_with_interval method\n        when last_checked is None.\n        \"\"\"\n        # Ensure last_checked is None\n        self.messenger.image_records['last_checked'] = None\n\n        # Call the method\n        self.messenger.delete_old_images_with_interval()\n\n        # Verify that delete_old_images\n        # was called since last_checked is None\n        mock_delete_old_images.assert_called_once()\n\n        # Verify that the save_image_records\n        # was called to save the new check time\n        mock_save_image_records.assert_called_once()\n\n        # Check that last_checked is now set to the current time\n        self.assertIn('last_checked', self.messenger.image_records)\n\n    @patch(\n        'src.notifiers.line_notifier_message_api.LineMessenger.'\n        'delete_old_images',\n    )\n    @patch(\n        'src.notifiers.line_notifier_message_api.LineMessenger.'\n        'save_image_records',\n    )\n    def test_delete_old_images_with_invalid_last_checked(\n        self,\n        mock_save_image_records: unittest.mock.MagicMock,\n        mock_delete_old_images: unittest.mock.MagicMock,\n    ):\n        \"\"\"\n        Test the delete_old_images_with_interval method\n        when last_checked is invalid.\n        \"\"\"\n        # Set an invalid last_checked value to\n        # simulate ValueError during parsing\n        self.messenger.image_records['last_checked'] = (\n            'invalid_datetime_format'\n        )\n\n        # Call the method\n        self.messenger.delete_old_images_with_interval()\n\n        # Verify that delete_old_images was called\n        # since last_checked is invalid\n        mock_delete_old_images.assert_called_once()\n\n        # Verify that save_image_records was called to save the new check time\n        mock_save_image_records.assert_called_once()\n\n        # Check that last_checked is now set to the current time\n        self.assertIn('last_checked', self.messenger.image_records)\n\n    @patch('src.notifiers.line_notifier_message_api.requests.post')\n    def test_push_message_without_image(self, mock_post):\n        \"\"\"\n        Test sending a message without an image via LINE Messaging API.\n        \"\"\"\n        mock_post.return_value.status_code = 200\n\n        response_code = self.messenger.push_message(\n            recipient_id=self.recipient_id,\n            message=self.message,\n        )\n\n        # Verify that no image upload is attempted\n        expected_data = {\n            'to': self.recipient_id,\n            'messages': [\n                {\n                    'type': 'text',\n                    'text': self.message,\n                },\n            ],\n        }\n        mock_post.assert_called_once_with(\n            'https://api.line.me/v2/bot/message/push',\n            headers=self.headers,\n            json=expected_data,\n        )\n\n        # Verify the response code is 200\n        self.assertEqual(response_code, 200)\n\n    @patch('cloudinary.uploader.upload')\n    def test_upload_image_to_cloudinary_success(self, mock_upload):\n        \"\"\"\n        Test successful image upload to Cloudinary.\n        \"\"\"\n        mock_upload.return_value = {\n            'secure_url': 'http://mock_image_url',\n            'public_id': 'mock_public_id',\n        }\n\n        image_url, public_id = self.messenger.upload_image_to_cloudinary(\n            self.image_bytes,\n        )\n\n        # Verify that the Cloudinary uploader\n        # was called with the correct arguments\n        mock_upload.assert_called_once_with(\n            self.image_bytes, resource_type='image',\n        )\n\n        # Verify the return values\n        self.assertEqual(image_url, 'http://mock_image_url')\n        self.assertEqual(public_id, 'mock_public_id')\n\n    @patch('cloudinary.uploader.upload')\n    def test_upload_image_to_cloudinary_failure(self, mock_upload):\n        \"\"\"\n        Test failure when uploading an image to Cloudinary.\n        \"\"\"\n        mock_upload.side_effect = Exception('Cloudinary upload failed')\n\n        image_url, public_id = self.messenger.upload_image_to_cloudinary(\n            self.image_bytes,\n        )\n\n        # Verify the return values are empty strings on failure\n        self.assertEqual(image_url, '')\n        self.assertEqual(public_id, '')\n\n    @patch('cloudinary.uploader.destroy')\n    def test_delete_image_from_cloudinary_success(self, mock_destroy):\n        \"\"\"\n        Test successful image deletion from Cloudinary.\n        \"\"\"\n        mock_destroy.return_value = {'result': 'ok'}\n\n        self.messenger.delete_image_from_cloudinary('mock_public_id')\n\n        # Verify that Cloudinary's destroy method\n        # was called with the correct public_id\n        mock_destroy.assert_called_once_with('mock_public_id')\n\n    @patch('cloudinary.uploader.destroy')\n    def test_delete_image_from_cloudinary_failure(self, mock_destroy):\n        \"\"\"\n        Test failure when deleting an image from Cloudinary.\n        \"\"\"\n        mock_destroy.return_value = {'result': 'error'}\n\n        self.messenger.delete_image_from_cloudinary('mock_public_id')\n\n        # Verify that Cloudinary's destroy method\n        # was called with the correct public_id\n        mock_destroy.assert_called_once_with('mock_public_id')\n\n    @patch('cloudinary.uploader.destroy')\n    def test_delete_image_from_cloudinary_exception(self, mock_destroy):\n        \"\"\"\n        Test exception handling when deleting an image from Cloudinary.\n        \"\"\"\n        mock_destroy.side_effect = Exception('Mocked exception')\n\n        with patch('builtins.print') as mock_print:\n            self.messenger.delete_image_from_cloudinary('mock_public_id')\n            mock_print.assert_called_once_with(\n                'Error deleting image from Cloudinary: Mocked exception',\n            )\n\n    @patch(\n        'src.notifiers.line_notifier_message_api.LineMessenger.'\n        'delete_image_from_cloudinary',\n    )\n    @patch(\n        'src.notifiers.line_notifier_message_api.LineMessenger.'\n        'save_image_records',\n    )\n    def test_delete_old_images(\n        self,\n        mock_save_image_records: unittest.mock.MagicMock,\n        mock_delete_image_from_cloudinary: unittest.mock.MagicMock,\n    ):\n        \"\"\"\n        Test the delete_old_images method.\n        \"\"\"\n        now = datetime.now()\n        old_time = (now - timedelta(days=8)).isoformat()\n        recent_time = (now - timedelta(days=1)).isoformat()\n\n        self.messenger.image_records = {\n            'old_image_id': old_time,\n            'recent_image_id': recent_time,\n            'last_checked': now.isoformat(),\n        }\n\n        self.messenger.delete_old_images()\n\n        # Verify that the old image was deleted\n        mock_delete_image_from_cloudinary.assert_called_once_with(\n            'old_image_id',\n        )\n\n        # Verify that the old image was removed from records\n        self.assertNotIn('old_image_id', self.messenger.image_records)\n\n        # Verify that the recent image was not deleted\n        self.assertIn('recent_image_id', self.messenger.image_records)\n\n        # Verify that the records were saved\n        mock_save_image_records.assert_called_once()\n\n    @patch(\n        'src.notifiers.line_notifier_message_api.LineMessenger.'\n        'delete_old_images',\n    )\n    @patch(\n        'src.notifiers.line_notifier_message_api.LineMessenger.'\n        'save_image_records',\n    )\n    @patch(\n        'src.notifiers.line_notifier_message_api.datetime', wraps=datetime,\n    )\n    def test_delete_old_images_with_interval(\n        self,\n        mock_datetime: unittest.mock.MagicMock,\n        mock_save_image_records: unittest.mock.MagicMock,\n        mock_delete_old_images: unittest.mock.MagicMock,\n    ):\n        \"\"\"\n        Test the delete_old_images_with_interval method.\n        \"\"\"\n        now = datetime(2023, 10, 1)\n        mock_datetime.now.return_value = now\n\n        # Set last_checked to 2 days ago\n        self.messenger.image_records['last_checked'] = (\n            now - timedelta(days=2)\n        ).isoformat()\n\n        self.messenger.delete_old_images_with_interval()\n\n        # Verify that the check time was updated\n        self.assertEqual(\n            self.messenger.image_records['last_checked'], now.isoformat(),\n        )\n\n        # Verify that delete_old_images was called\n        mock_delete_old_images.assert_called_once()\n\n        # Verify that save_image_records was called\n        mock_save_image_records.assert_called_once()\n\n    @patch(\n        'src.notifiers.line_notifier_message_api.'\n        'LineMessenger.push_message',\n    )\n    @patch(\n        'src.notifiers.line_notifier_message_api.LineMessenger.'\n        'upload_image_to_cloudinary',\n    )\n    @patch(\n        'src.notifiers.line_notifier_message_api.LineMessenger.__init__',\n        return_value=None,\n    )\n    def test_main(\n        self,\n        mock_init: unittest.mock.MagicMock,\n        mock_upload_image_to_cloudinary: unittest.mock.MagicMock,\n        mock_push_message: unittest.mock.MagicMock,\n    ):\n        \"\"\"\n        Test the main function.\n        \"\"\"\n        mock_push_message.return_value = 200\n        mock_upload_image_to_cloudinary.return_value = (\n            'http://mock_image_url', 'mock_public_id',\n        )\n\n        # Create a black image for testing\n        height, width = 480, 640\n        frame_with_detections = np.zeros((height, width, 3), dtype=np.uint8)\n        _, buffer = cv2.imencode('.png', frame_with_detections)\n        frame_bytes = buffer.tobytes()\n\n        with patch('builtins.print') as mock_print:\n            main()\n            mock_print.assert_any_call('Response code: 200')\n\n        # Verify that the LineMessenger\n        # was initialised with the correct arguments\n        mock_init.assert_called_once_with(\n            channel_access_token='YOUR_LINE_CHANNEL',\n            recipient_id='YOUR_RECIPIENT_ID',\n        )\n\n        # Verify that the push_message method\n        # was called with the correct arguments\n        mock_push_message.assert_called_once_with(\n            'Hello, LINE Messaging API!', image_bytes=frame_bytes,\n        )\n\n    # @patch('builtins.open', new_callable=mock_open)\n    # @patch('logging.error')\n    # def test_load_image_records_failure(\n    #     self,\n    #     mock_logging_error: MagicMock,\n    #     mock_file: MagicMock,\n    # ) -> None:\n    #     \"\"\"\n    #     Test failure when loading image records from JSON file.\n    #     \"\"\"\n    #     mock_file.side_effect = Exception('Mocked exception')\n\n    #     records = self.messenger.load_image_records()\n    #     self.assertEqual(records, {})\n    #     mock_logging_error.assert_called_once_with(\n    #         'Failed to load image records: Mocked exception',\n    #     )\n\n    @patch('builtins.open', new_callable=mock_open)\n    def test_save_image_records_failure(self, mock_file):\n        \"\"\"\n        Test failure when saving image records to JSON file.\n        \"\"\"\n        mock_file.side_effect = Exception('Mocked exception')\n\n        with patch('builtins.print') as mock_print:\n            self.messenger.save_image_records()\n            mock_print.assert_called_once_with(\n                'Failed to save image records: Mocked exception',\n            )\n\n    @patch(\n        'src.notifiers.line_notifier_message_api.LineMessenger.'\n        'upload_image_to_cloudinary',\n    )\n    def test_push_message_image_upload_failure(\n        self,\n        mock_upload_image_to_cloudinary: unittest.mock.MagicMock,\n    ):\n        \"\"\"\n        Test push_message method when image upload to Cloudinary fails.\n        \"\"\"\n        mock_upload_image_to_cloudinary.return_value = ('', '')\n\n        with self.assertRaises(ValueError) as context:\n            self.messenger.push_message(\n                recipient_id=self.recipient_id,\n                message=self.message,\n                image_bytes=self.image_bytes,\n            )\n        self.assertEqual(\n            str(context.exception),\n            'Failed to upload image to Cloudinary',\n        )\n\n    @patch('src.notifiers.line_notifier_message_api.requests.post')\n    def test_push_message_api_error(self, mock_post):\n        \"\"\"\n        Test push_message method when LINE Messaging API returns an error.\n        \"\"\"\n        mock_post.return_value.status_code = 400\n        mock_post.return_value.text = 'Bad Request'\n\n        with patch('builtins.print') as mock_print:\n            response_code = self.messenger.push_message(\n                recipient_id=self.recipient_id,\n                message=self.message,\n            )\n            mock_print.assert_called_once_with('Error: 400, Bad Request')\n            self.assertEqual(response_code, 400)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "source_file", "path": "examples/YOLO_evaluation/__init__.py", "content": ""}
{"type": "source_file", "path": "examples/YOLO_evaluation/evaluate_yolo.py", "content": "from __future__ import annotations\n\nimport argparse\nfrom typing import Any\n\nfrom ultralytics import YOLO  # Import the YOLO class\n\n\nclass ModelEvaluator:\n    \"\"\"\n    A class to evaluate YOLO models using Ultralytics framework.\n    \"\"\"\n\n    def __init__(self, model_path: str, data_path: str):\n        \"\"\"\n        Initialises the model evaluator with the path to the model and dataset.\n\n        Args:\n            model_path (str): The path to the trained model file.\n            data_path (str): The path to the dataset configuration file.\n        \"\"\"\n        self.model_path = model_path\n        self.data_path = data_path\n        self.model = YOLO(self.model_path)\n\n    def evaluate(self) -> dict[str, Any]:\n        \"\"\"\n        Evaluates the model using the provided dataset.\n\n        Returns:\n            Dict[str, Any]: The results from the model evaluation.\n        \"\"\"\n        print(f\"Evaluating model with data path: {self.data_path}\")\n        # The 'val' method is for evaluation; 'test' can be used if needed.\n        return self.model.val(data=self.data_path)\n\n\ndef main():\n    parser = argparse.ArgumentParser(description='Evaluates a YOLO model.')\n    parser.add_argument(\n        '--model_path',\n        type=str,\n        required=True,\n        help='Path to the trained model file.',\n    )\n    parser.add_argument(\n        '--data_path',\n        type=str,\n        required=True,\n        help='Path to the dataset configuration file.',\n    )\n\n    args = parser.parse_args()\n\n    evaluator = ModelEvaluator(\n        model_path=args.model_path,\n        data_path=args.data_path,\n    )\n    results = evaluator.evaluate()\n    print(results)\n\n\nif __name__ == '__main__':\n    main()\n\n\"\"\"example usage\npython evaluate_yolo.py \\\n    --model_path \"../../models/pt/best_yolov8x.pt\" \\\n    --data_path \"dataset/data.yaml\"\n\"\"\"\n"}
{"type": "source_file", "path": "examples/YOLO_server_api/backend/config.py", "content": "from __future__ import annotations\n\nimport os\n\nfrom dotenv import load_dotenv\nfrom pydantic_settings import BaseSettings\n\n# Load environment variables from a .env file\nload_dotenv()\n\n\nclass Settings(BaseSettings):\n    \"\"\"\n    A class to represent the application settings.\n\n    Attributes:\n        authjwt_secret_key (str): The secret key for JWT authentication.\n        sqlalchemy_database_uri (str): The URI for the SQLAlchemy database\n            connection.\n        sqlalchemy_track_modifications (bool): Flag to track modifications in\n            SQLAlchemy.\n    \"\"\"\n\n    authjwt_secret_key: str = os.getenv(\n        'JWT_SECRET_KEY',\n        'your_fallback_secret_key',\n    )\n    sqlalchemy_database_uri: str = os.getenv(\n        'DATABASE_URL',\n        'mysql+asyncmy://user:password@localhost/dbname',\n    )\n    sqlalchemy_track_modifications: bool = False\n\n    def __init__(self) -> None:\n        \"\"\"\n        Initialise the Settings instance with environment variables.\n\n        If the environment variables are not set, fallback values will be used.\n        \"\"\"\n        super().__init__()  # Ensure the BaseSettings initialisation is called\n"}
{"type": "source_file", "path": "examples/line_chatbot/__init__.py", "content": ""}
{"type": "source_file", "path": "examples/line_chatbot/line_bot.py", "content": "from __future__ import annotations\n\nimport logging\n\nfrom fastapi import FastAPI\nfrom fastapi import HTTPException\nfrom fastapi import Request\nfrom fastapi.responses import PlainTextResponse\nfrom linebot import LineBotApi\nfrom linebot import WebhookHandler\nfrom linebot.exceptions import InvalidSignatureError\nfrom linebot.exceptions import LineBotApiError\nfrom linebot.models import MessageEvent\nfrom linebot.models import TextMessage\nfrom linebot.models import TextSendMessage\n\n# Create a FastAPI application instance\napp = FastAPI()\n\n# Initialise the LINE Bot API and WebhookHandler with access token and secret\nline_bot_api: LineBotApi = LineBotApi('YOUR_LINE_CHANNEL_ACCESS_TOKEN')\nhandler: WebhookHandler = WebhookHandler('YOUR_LINE_CHANNEL_SECRET')\n\n\n@app.post('/webhook', response_class=PlainTextResponse)\nasync def callback(request: Request) -> str:\n    \"\"\"\n    Handle incoming webhook requests from LINE.\n\n    Args:\n        request (Request): The HTTP request object\n            containing the webhook data.\n\n    Returns:\n        str: A plain text response\n            indicating successful handling of the request.\n\n    Raises:\n        HTTPException: If the request is invalid\n            or an error occurs during processing.\n    \"\"\"\n    # Extract the signature and body from the request\n    signature: str = request.headers.get('X-Line-Signature', '')\n    body: bytes = await request.body()\n    body_text: str = body.decode('utf-8') if body else ''\n\n    # Check if signature or body is missing\n    if not signature or not body_text:\n        logging.warning(\n            'Received invalid request: signature or body is missing',\n        )\n        raise HTTPException(status_code=400, detail='Bad Request')\n\n    try:\n        # Process the incoming message using the LINE handler\n        handler.handle(body_text, signature)\n    except InvalidSignatureError:\n        logging.error('Invalid signature error')\n        raise HTTPException(status_code=400, detail='Invalid Signature Error')\n    except Exception as e:\n        logging.error(f\"Unexpected error: {e}\")\n        raise HTTPException(status_code=500, detail='Internal Server Error')\n\n    # Return success response\n    return 'OK'\n\n\n@handler.add(MessageEvent, message=TextMessage)\ndef handle_text_message(event: MessageEvent) -> None:\n    \"\"\"\n    Handle incoming text messages from users.\n\n    Args:\n        event (MessageEvent): The LINE message event\n            containing the user's message.\n\n    Raises:\n        LineBotApiError: If an error occurs\n            while sending a reply message.\n        Exception: If an unexpected error occurs during processing.\n\n    Returns:\n        None\n    \"\"\"\n    # Extract the user's message\n    user_message: str = event.message.text\n\n    # Check if the message is empty or contains only whitespace\n    if not user_message.strip():\n        logging.warning('Received empty user message')\n        return\n\n    try:\n        # Generate a response based on the user's message\n        assistant_response: str = f\": {user_message}\"\n\n        # Send the response back to the user via LINE\n        line_bot_api.reply_message(\n            event.reply_token,\n            TextSendMessage(text=assistant_response),\n        )\n    except LineBotApiError as e:\n        logging.error(f\"Error responding to message: {e}\")\n    except Exception as e:\n        logging.error(f\"Unexpected error: {e}\")\n\n\nif __name__ == '__main__':\n    import uvicorn\n\n    # Start the FastAPI application using uvicorn\n    uvicorn.run(app, host='127.0.0.1', port=8000)\n"}
{"type": "source_file", "path": "examples/YOLO_evaluation/evaluate_sahi_yolo.py", "content": "from __future__ import annotations\n\nimport argparse\nimport json\nimport os\n\nimport numpy as np\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\nfrom sahi import AutoDetectionModel\nfrom sahi.predict import get_sliced_prediction\nfrom sahi.utils.coco import Coco\n\n\nclass COCOEvaluator:\n    \"\"\"\n    Evaluates object detection models using COCO metrics.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: str,\n        coco_json: str,\n        image_dir: str,\n        confidence_threshold: float = 0.3,\n        slice_height: int = 370,\n        slice_width: int = 370,\n        overlap_height_ratio: float = 0.3,\n        overlap_width_ratio: float = 0.3,\n    ):\n        \"\"\"\n        Initialises the evaluator with model and dataset parameters.\n\n        Args:\n            model_path (str): Path to the trained model file.\n            coco_json (str): Path to the COCO format annotations JSON file.\n            image_dir (str): Directory containing the evaluation image set.\n            confidence_threshold (float, optional): Threshold for preds.\n                Defaults to 0.3.\n            slice_height (int, optional): Height of the slices for pred.\n                Defaults to 370.\n            slice_width (int, optional): Width of the slices for pred.\n                Defaults to 370.\n            overlap_height_ratio (float, optional): Height slice overlap ratio.\n                Defaults to 0.3.\n            overlap_width_ratio (float, optional): Width slice overlap ratio.\n                Defaults to 0.3.\n        \"\"\"\n        self.model = AutoDetectionModel.from_pretrained(\n            model_type='yolov8',\n            model_path=model_path,\n            confidence_threshold=confidence_threshold,\n            # device=\"cpu\",  # Uncomment this to force CPU usage\n        )\n        self.coco_json = coco_json\n        self.image_dir = image_dir\n        self.slice_height = slice_height\n        self.slice_width = slice_width\n        self.overlap_height_ratio = overlap_height_ratio\n        self.overlap_width_ratio = overlap_width_ratio\n\n    def evaluate(self) -> dict[str, float]:\n        \"\"\"\n        Evaluates the model on the dataset and computes COCO metrics.\n\n        Returns:\n            Dict[str, float]: A dictionary containing computed metrics.\n        \"\"\"\n        print(f\"Evaluating model with data path: {self.coco_json}\")\n        coco = Coco.from_coco_dict_or_path(self.coco_json)\n        pycoco = COCO(self.coco_json)\n        predictions = []\n        category_to_id = {\n            category.name: category.id for category in coco.categories\n        }\n\n        for image_info in coco.images:\n            image_path = os.path.join(self.image_dir, image_info.file_name)\n            print(f\"Processing image: {image_path}\")\n            prediction_result = get_sliced_prediction(\n                image_path,\n                self.model,\n                slice_height=self.slice_height,\n                slice_width=self.slice_width,\n                overlap_height_ratio=self.overlap_height_ratio,\n                overlap_width_ratio=self.overlap_width_ratio,\n            )\n            for pred in prediction_result.object_prediction_list:\n                predictions.append(\n                    {\n                        'image_id': image_info.id,\n                        'category_id': category_to_id[pred.category.name],\n                        'bbox': [\n                            pred.bbox.minx,\n                            pred.bbox.miny,\n                            pred.bbox.maxx - pred.bbox.minx,\n                            pred.bbox.maxy - pred.bbox.miny,\n                        ],\n                        'score': pred.score.value,\n                    },\n                )\n\n        # Save the predictions to a JSON file\n        predictions_path = 'predictions.json'\n        with open(predictions_path, 'w') as f:\n            json.dump(predictions, f)\n\n        # Load the predictions and evaluate\n        pycoco_pred = pycoco.loadRes(predictions_path)\n        coco_eval = COCOeval(pycoco, pycoco_pred, 'bbox')\n        coco_eval.evaluate()\n        coco_eval.accumulate()\n        coco_eval.summarize()\n\n        metrics = {\n            'Average Precision': np.mean(\n                coco_eval.eval['precision'][:, :, :, 0, -1],\n            ),\n            'Average Recall': np.mean(\n                coco_eval.eval['recall'][:, :, 0, -1],\n            ),\n            'mAP at IoU=50': np.mean(\n                coco_eval.eval['precision'][0, :, :, 0, 2],\n            ),\n            'mAP at IoU=50-95': np.mean(\n                coco_eval.eval['precision'][0, :, :, 0, :],\n            ),\n        }\n        return metrics\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description='Evaluates a YOLO model using COCO metrics.',\n    )\n    parser.add_argument(\n        '--model_path',\n        type=str,\n        required=True,\n        help='Path to the trained model file.',\n    )\n    parser.add_argument(\n        '--coco_json',\n        type=str,\n        required=True,\n        help='Path to the COCO format annotations JSON file.',\n    )\n    parser.add_argument(\n        '--image_dir',\n        type=str,\n        required=True,\n        help='Directory containing the evaluation image set.',\n    )\n    args = parser.parse_args()\n    evaluator = COCOEvaluator(\n        model_path=args.model_path,\n        coco_json=args.coco_json,\n        image_dir=args.image_dir,\n    )\n    metrics = evaluator.evaluate()\n    print('Evaluation metrics:', metrics)\n\n\nif __name__ == '__main__':\n    main()\n\n\"\"\"example usage\npython evaluate_sahi_yolo.py \\\n    --model_path \"../../models/pt/best_yolov8x.pt\" \\\n    --coco_json \"dataset/coco_annotations.json\" \\\n    --image_dir \"dataset/valid/images\"\n\"\"\"\n"}
{"type": "source_file", "path": "examples/YOLO_data_augmentation/__init__.py", "content": ""}
{"type": "source_file", "path": "examples/YOLO_server_api/backend/models.py", "content": "from __future__ import annotations\n\nimport asyncio\nimport threading\nfrom collections.abc import AsyncGenerator\nfrom datetime import datetime\nfrom datetime import timezone\nfrom pathlib import Path\n\nfrom sahi.predict import AutoDetectionModel\nfrom sqlalchemy import Boolean\nfrom sqlalchemy import DateTime\nfrom sqlalchemy import Integer\nfrom sqlalchemy import String\nfrom sqlalchemy.ext.asyncio import async_sessionmaker\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.ext.asyncio import create_async_engine\nfrom sqlalchemy.orm import DeclarativeBase\nfrom sqlalchemy.orm import Mapped\nfrom sqlalchemy.orm import mapped_column\nfrom watchdog.events import FileSystemEventHandler\nfrom watchdog.observers import Observer\nfrom werkzeug.security import check_password_hash\nfrom werkzeug.security import generate_password_hash\n\nfrom .config import Settings\n\n# Load settings\nsettings = Settings()\n\n# Set up SQLAlchemy async engine and session\nengine = create_async_engine(\n    settings.sqlalchemy_database_uri.replace('mysql://', 'mysql+asyncmy://'),\n)\nAsyncSessionLocal = async_sessionmaker(\n    engine, class_=AsyncSession, expire_on_commit=False,\n)\n\n# Base for SQLAlchemy ORM models\n\n\nclass Base(DeclarativeBase):\n    pass\n\n\nasync def get_db() -> AsyncGenerator[AsyncSession]:\n    \"\"\"\n    Provides a database session generator, ensuring that the session is\n    closed after use.\n    \"\"\"\n    async with AsyncSessionLocal() as session:\n        yield session\n\n\nclass User(Base):\n    \"\"\"\n    Represents a user entity in the database\n    with password hashing for security.\n    \"\"\"\n\n    __tablename__ = 'users'\n\n    id: Mapped[int] = mapped_column(Integer, primary_key=True, index=True)\n    username: Mapped[str] = mapped_column(\n        String(80), unique=True, nullable=False,\n    )\n    password_hash: Mapped[str] = mapped_column(String(255), nullable=False)\n    role: Mapped[str] = mapped_column(\n        String(20), default='user', nullable=False,\n    )\n    is_active: Mapped[bool] = mapped_column(\n        Boolean, default=True, nullable=False,\n    )\n    created_at: Mapped[datetime] = mapped_column(\n        DateTime, default=datetime.now(timezone.utc), nullable=False,\n    )\n    updated_at: Mapped[datetime] = mapped_column(\n        DateTime,\n        default=datetime.now(timezone.utc),\n        onupdate=datetime.now(timezone.utc),\n        nullable=False,\n    )\n\n    def set_password(self, password: str) -> None:\n        self.password_hash = generate_password_hash(password)\n\n    async def check_password(self, password: str) -> bool:\n        return await asyncio.to_thread(\n            check_password_hash, str(self.password_hash), password,\n        )\n\n    def to_dict(self):\n        return {\n            'id': self.id,\n            'username': self.username,\n            'role': self.role,\n            'is_active': self.is_active,\n            'created_at': self.created_at,\n            'updated_at': self.updated_at,\n        }\n\n\nclass ModelFileChangeHandler(FileSystemEventHandler):\n    \"\"\"\n    Handles file system events for model files, triggering reloads on\n    modification.\n    \"\"\"\n\n    def __init__(self, model_manager: DetectionModelManager) -> None:\n        \"\"\"\n        Initialises the file change handler with a model manager.\n\n        Args:\n            model_manager (DetectionModelManager): The manager responsible for\n            loading models.\n        \"\"\"\n        self.model_manager = model_manager\n\n    def on_modified(self, event) -> None:\n        \"\"\"\n        Handles the modification event, reloading models\n        if relevant files are updated.\n\n        Args:\n            event: The file system event.\n        \"\"\"\n        # Ignore directories\n        if event.is_directory:\n            return\n\n        # Reload model if it is a .pt file\n        if event.src_path.endswith('.pt'):\n            model_name = Path(event.src_path).stem.split('best_')[-1]\n            if model_name in self.model_manager.model_names:\n                # Reload the model in the manager\n                self.model_manager.models[model_name] = (\n                    self.model_manager.load_single_model(model_name)\n                )\n                print(f\"Model {model_name} reloaded due to file modification.\")\n\n\nclass DetectionModelManager:\n    \"\"\"\n    Manages the loading and access of object detection models\n    with file system monitoring.\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"\n        Initialises the model manager, loading models\n        and setting up a file monitor.\n        \"\"\"\n        self.base_model_path: Path = Path('models/pt/')\n        self.model_names: list[str] = [\n            'yolo11x',\n            'yolo11l', 'yolo11m', 'yolo11s', 'yolo11n',\n        ]\n\n        # Load each model\n        self.models: dict[str, AutoDetectionModel] = {\n            name: self.load_single_model(name) for name in self.model_names\n        }\n\n        # Set up a watchdog observer for monitoring model file changes\n        self.event_handler = ModelFileChangeHandler(self)\n        self.observer = Observer()\n        self.observer.schedule(\n            self.event_handler, str(\n                self.base_model_path,\n            ), recursive=False,\n        )\n\n        # Run the observer in a separate thread\n        self.observer_thread = threading.Thread(target=self.observer.start)\n        self.observer_thread.start()\n\n    def load_single_model(self, model_name: str) -> AutoDetectionModel:\n        \"\"\"\n        Loads a specified model from a file and returns it\n        as an AutoDetectionModel.\n\n        Args:\n            model_name (str): The name of the model to load.\n\n        Returns:\n            AutoDetectionModel: The loaded model ready for predictions.\n        \"\"\"\n        return AutoDetectionModel.from_pretrained(\n            'yolo11',\n            model_path=str(self.base_model_path / f\"best_{model_name}.pt\"),\n            device='cuda:0',\n        )\n\n    def get_model(self, model_key: str) -> AutoDetectionModel | None:\n        \"\"\"\n        Retrieves a model by its key if it exists within the loaded models.\n\n        Args:\n            model_key (str): The key name of the model to retrieve.\n\n        Returns:\n            AutoDetectionModel | None: The requested model\n            or None if it does not exist.\n        \"\"\"\n        return self.models.get(model_key)\n\n    def __del__(self) -> None:\n        \"\"\"\n        Cleans up by stopping the file observer thread if it exists.\n        \"\"\"\n        if hasattr(self, 'observer') and self.observer is not None:\n            self.observer.stop()\n            self.observer.join()\n"}
{"type": "source_file", "path": "examples/YOLO_train/train.py", "content": "from __future__ import annotations\n\nimport argparse\nimport os\nimport shutil\nfrom typing import Any\n\nimport torch\nfrom sahi import AutoDetectionModel\nfrom sahi.predict import get_sliced_prediction\nfrom sklearn.model_selection import KFold\nfrom ultralytics import YOLO\n\n\nclass YOLOModelHandler:\n    \"\"\"Handles loading, training, validating, and predicting with YOLO models.\n\n    Attributes:\n        model_name (str): The name of the model file to be loaded.\n        model (YOLO, Optional): The loaded YOLO model object.\n    \"\"\"\n\n    def __init__(self, model_name: str, batch_size: int = -1):\n        \"\"\"\n        Initialises the YOLOModelHandler with a specified model.\n\n        Args:\n            model_name (str): The name of the model file (either .yaml or .pt).\n            batch_size (int): The batch size for training and validation.\n\n        Raises:\n            ValueError: If the model format is not supported.\n        \"\"\"\n        self.model_name: str = model_name\n        self.model: YOLO | None = None\n        self.batch_size: int = batch_size\n        self.load_model()\n\n    def load_model(self) -> None:\n        \"\"\"Loads the YOLO model specified by the model name.\"\"\"\n        if self.model_name.endswith('.yaml'):\n            # Build a new model from scratch\n            self.model = YOLO(self.model_name)\n            # Set device to CPU by default for YAML models\n            self.device = torch.device('cpu')\n        elif self.model_name.endswith('.pt'):\n            # Load a pre-trained model (recommended for training)\n            self.model = YOLO(self.model_name)\n\n            # Check and set the device\n            if torch.backends.mps.is_available():\n                # Use MPS if available\n                self.device = torch.device('mps')\n            elif torch.cuda.is_available():\n                # Use CUDA if MPS is unavailable but CUDA is\n                self.device = torch.device('cuda')\n            else:\n                # Use CPU if neither MPS nor CUDA is available\n                self.device = torch.device('cpu')\n\n        # Load the model onto the specified device\n        if self.model:\n            self.model.to(self.device)\n        else:\n            raise ValueError(\"Unsupported model format. Use '.yaml' or '.pt'\")\n\n    def train_model(\n        self, data_config: str, epochs: int, optimizer: str,\n    ) -> None:\n        \"\"\"\n        Trains the YOLO model with the given data config and number of epochs.\n\n        Args:\n            data_config (str): The path to the data configuration file.\n            epochs (int): The number of training epochs.\n            batch_size (int): The batch size for training and validation.\n            optimizer (str): The type of optimizer to use.\n\n        Raises:\n            RuntimeError: If the model is not loaded properly before training.\n        \"\"\"\n        if self.model is None:\n            raise RuntimeError('The model is not loaded properly.')\n        # Train the model\n        self.model.train(\n            data=data_config,\n            epochs=epochs,\n            batch=self.batch_size,\n            optimizer=optimizer,\n        )\n\n    def validate_model(self) -> Any:\n        \"\"\"\n        Validates the YOLO model on the validation dataset.\n\n        Args:\n            batch_size (int): The batch size for training and validation.\n\n        Returns:\n            The validation results.\n\n        Raises:\n            RuntimeError: If model is not loaded properly before validation.\n        \"\"\"\n        if self.model is None:\n            raise RuntimeError('The model is not loaded properly.')\n        # Evaluate model performance on the validation set\n        return self.model.val(batch=self.batch_size)\n\n    def predict_image(self, image_path: str) -> Any:\n        \"\"\"\n        Makes a prediction using the YOLO model on the specified image.\n\n        Args:\n            image_path (str): The path to the image file for prediction.\n\n        Returns:\n            The prediction results.\n\n        Raises:\n            RuntimeError: If  model is not loaded properly before prediction.\n        \"\"\"\n        if self.model is None:\n            raise RuntimeError('The model is not loaded properly.')\n        # Predict on an image\n        return self.model(image_path)\n\n    @staticmethod\n    def predict_image_sahi(yolo_model_path: str, image_path: str) -> Any:\n        \"\"\"\n        Makes a prediction using the YOLO model on the specified image\n        with SAHI post-processing.\n\n        Args:\n            yolo_model_path (str): The path to the YOLO model file.\n            image_path (str): The path to the image file for prediction.\n\n        Returns:\n            The prediction results with SAHI post-processing.\n\n        Raises:\n            RuntimeError: If model is not loaded properly before prediction.\n        \"\"\"\n        if not yolo_model_path:\n            raise RuntimeError('The model path is not provided.')\n\n        # Convert YOLO model to SAHI format; adjust for your YOLO version\n        sahi_model = AutoDetectionModel.from_pretrained(\n            model_type='yolov8',\n            model_path=yolo_model_path,\n            confidence_threshold=0.3,\n            # device=\"cpu\", or 'cuda:0'\n        )\n\n        # With an image path, get the sliced prediction\n        result = get_sliced_prediction(\n            image_path,\n            sahi_model,\n            slice_height=640,\n            slice_width=640,\n            overlap_height_ratio=0.2,\n            overlap_width_ratio=0.2,\n        )\n\n        # Visualise the prediction results\n        result.export_visuals(export_dir='./')\n\n        # Access the object prediction list\n        object_prediction_list = result.object_prediction_list\n\n        # Return the SAHI formatted results\n        return object_prediction_list\n\n    def export_model(self, export_format: str = 'onnx') -> str:\n        \"\"\"\n        Exports the YOLO model to the specified format.\n\n        Args:\n            export_format (str): The format to export the model to.\n\n        Returns:\n            The path to the exported model file.\n\n        Raises:\n            RuntimeError: If the model is not loaded properly before exporting.\n        \"\"\"\n        if self.model is None:\n            raise RuntimeError('The model is not loaded properly.')\n        # Export the model to the desired format\n        return self.model.export(format=export_format)\n\n    def save_model(self, save_path: str) -> None:\n        \"\"\"\n        Saves the YOLO model to a .pt file.\n\n        Args:\n            save_path (str): The path to save the .pt model file.\n        \"\"\"\n        if self.model is None:\n            raise RuntimeError('The model is not loaded properly.')\n        # Save the model to the specified path\n        torch.save(self.model.state_dict(), save_path)\n\n    def cross_validate_model(\n        self,\n        data_config: str,\n        epochs: int,\n        optimizer: str,\n        n_splits: int = 5,\n    ) -> None:\n        \"\"\"\n        Performs k-fold cross-validation on the YOLO model.\n\n        Args:\n            data_config (str): The path to the data configuration file.\n            epochs (int): The number of training epochs.\n            optimizer (str): The type of optimizer to use.\n            n_splits (int): Number of folds for cross-validation.\n\n        Raises:\n            RuntimeError: If the model is not loaded properly before training.\n        \"\"\"\n        if self.model is None:\n            raise RuntimeError('The model is not loaded properly.')\n\n        # Load the data\n        dataset_path = os.path.join(os.path.dirname(data_config))\n        images_path = os.path.join(dataset_path, 'images')\n        labels_path = os.path.join(dataset_path, 'labels')\n\n        # List all image files\n        image_files = [\n            f\n            for f in os.listdir(\n                images_path,\n            )\n            if os.path.isfile(os.path.join(images_path, f))\n        ]\n        kf = KFold(n_splits=n_splits)\n\n        fold = 1\n        for train_index, val_index in kf.split(image_files):\n            train_images = [image_files[i] for i in train_index]\n            val_images = [image_files[i] for i in val_index]\n\n            # Create temporary directories for training and validation sets\n            temp_train_dir = os.path.join(dataset_path, 'train')\n            temp_val_dir = os.path.join(dataset_path, 'val')\n\n            os.makedirs(temp_train_dir, exist_ok=True)\n            os.makedirs(temp_val_dir, exist_ok=True)\n\n            os.makedirs(os.path.join(temp_train_dir, 'images'), exist_ok=True)\n            os.makedirs(os.path.join(temp_train_dir, 'labels'), exist_ok=True)\n            os.makedirs(os.path.join(temp_val_dir, 'images'), exist_ok=True)\n            os.makedirs(os.path.join(temp_val_dir, 'labels'), exist_ok=True)\n\n            # Copy files to the temporary directories\n            for image in train_images:\n                shutil.copy(\n                    os.path.join(images_path, image),\n                    os.path.join(temp_train_dir, 'images', image),\n                )\n                shutil.copy(\n                    os.path.join(\n                        labels_path,\n                        image.replace(\n                            '.jpg',\n                            '.txt',\n                        ).replace('.png', '.txt'),\n                    ),\n                    os.path.join(\n                        temp_train_dir,\n                        'labels',\n                        image.replace(\n                            '.jpg',\n                            '.txt',\n                        ).replace('.png', '.txt'),\n                    ),\n                )\n\n            for image in val_images:\n                shutil.copy(\n                    os.path.join(images_path, image),\n                    os.path.join(temp_val_dir, 'images', image),\n                )\n                shutil.copy(\n                    os.path.join(\n                        labels_path,\n                        image.replace(\n                            '.jpg',\n                            '.txt',\n                        ).replace('.png', '.txt'),\n                    ),\n                    os.path.join(\n                        temp_val_dir,\n                        'labels',\n                        image.replace(\n                            '.jpg',\n                            '.txt',\n                        ).replace('.png', '.txt'),\n                    ),\n                )\n\n            # Update data_config file for this fold\n            with open(data_config) as file:\n                data_yaml = file.read()\n\n            data_yaml = data_yaml.replace(\n                'dataset/train/images',\n                temp_train_dir + '/images',\n            )\n            data_yaml = data_yaml.replace(\n                'dataset/valid/images',\n                temp_val_dir + '/images',\n            )\n\n            temp_data_config = os.path.join(\n                dataset_path,\n                f\"data_fold{fold}.yaml\",\n            )\n            with open(temp_data_config, 'w') as file:\n                file.write(data_yaml)\n\n            print(f\"Training fold {fold}/{n_splits}\")\n            self.train_model(\n                data_config=temp_data_config,\n                epochs=epochs,\n                optimizer=optimizer,\n            )\n            metrics = self.validate_model()\n            print(f\"Validation metrics for fold {fold}:\", metrics)\n\n            # Clean up temporary directories\n            shutil.rmtree(temp_train_dir)\n            shutil.rmtree(temp_val_dir)\n            os.remove(temp_data_config)\n\n            fold += 1\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description='YOLO training, validation, prediction, and export.',\n    )\n\n    parser.add_argument(\n        '--data_config',\n        type=str,\n        default='dataset/data.yaml',\n        help='Path to the data configuration file',\n    )\n    parser.add_argument(\n        '--epochs',\n        type=int,\n        default=100,\n        help='Number of training epochs',\n    )\n    parser.add_argument(\n        '--model_name',\n        type=str,\n        default='./../../models/pt/best_yolo11x.pt',\n        help='Name or path of the YOLO model file',\n    )\n    parser.add_argument(\n        '--export_format',\n        type=str,\n        default='onnx',\n        help='Format to export the model to',\n    )\n    parser.add_argument(\n        '--onnx_path',\n        type=str,\n        default=None,\n        help='Path to save the exported ONNX model',\n    )\n    parser.add_argument(\n        '--pt_path',\n        type=str,\n        default='model.pt',\n        help='Path to save the trained model in .pt format',\n    )\n    parser.add_argument(\n        '--sahi_image_path',\n        type=str,\n        default='../../assets/IMG_1091.PNG',\n        help='Path to the image file for SAHI prediction',\n    )\n\n    parser.add_argument(\n        '--batch_size',\n        type=int,\n        default=-1,\n        help='Batch size for training and validation',\n    )\n\n    parser.add_argument(\n        '--optimizer',\n        type=str,\n        default='auto',\n        help='Type of optimizer to use',\n    )\n\n    parser.add_argument(\n        '--cross_validate',\n        action='store_true',\n        help='Perform cross-validation',\n    )\n\n    parser.add_argument(\n        '--n_splits',\n        type=int,\n        default=5,\n        help='Number of folds for cross-validation',\n    )\n\n    args = parser.parse_args()\n\n    handler = YOLOModelHandler(args.model_name, args.batch_size)\n\n    try:\n        if args.cross_validate:\n            handler.cross_validate_model(\n                data_config=args.data_config,\n                epochs=args.epochs,\n                optimizer=args.optimizer,\n                n_splits=args.n_splits,\n            )\n        else:\n            handler.train_model(\n                data_config=args.data_config,\n                epochs=args.epochs,\n                optimizer=args.optimizer,\n            )\n            metrics = handler.validate_model()\n            print('Validation metrics:', metrics)\n\n        export_path = (\n            handler.export_model(export_format=args.export_format)\n            if args.onnx_path is None\n            else args.onnx_path\n        )\n        handler.save_model(args.pt_path)\n    except Exception as e:\n        print(f\"Error occurred: {e}\")\n        exit(1)\n\n    print(f\"{args.export_format.upper()} model exported to:\", export_path)\n    print(f\"Model saved to: {args.pt_path}\")\n\n\nif __name__ == '__main__':\n    main()\n\n    # Predict on an image\n    # results = handler.predict_image(\"https://ultralytics.com/images/bus.jpg\")\n\n    # SAHI Prediction\n    # sahi_result = handler.predict_image_sahi(\n    #     args.model_name, args.sahi_image_path\n    # )\n    # print(\"SAHI Prediction Results:\", sahi_result)\n\n    # Example command to run the script\n    # python train.py \\\n    #     --data_config=cv_dataset/data.yaml \\\n    #     --epochs=100 \\\n    #     --model_name=../../models/pt/best_yolo11x.pt \\\n    #     --batch_size=16 \\\n    #     --optimizer=auto \\\n    #     --cross_validate \\\n    #     --n_splits=5\n"}
{"type": "source_file", "path": "examples/YOLO_data_augmentation/data_augmentation.py", "content": "from __future__ import annotations\n\nimport argparse\nimport gc\nimport random\nimport time\nimport uuid\nfrom pathlib import Path\n\nimport imageio.v3 as imageio\nimport imgaug.augmenters as iaa\nfrom imgaug.augmentables.bbs import BoundingBox\nfrom imgaug.augmentables.bbs import BoundingBoxesOnImage\nfrom tqdm import tqdm\n\n\nclass DataAugmentation:\n    \"\"\"\n    A class to perform data augmentation for image datasets, especially useful\n    for training machine learning models.\n    \"\"\"\n\n    def __init__(self, train_path: str, num_augmentations: int = 1):\n        \"\"\"\n        Initialise the DataAugmentation class.\n\n        Args:\n            train_path (str): The path to the training data.\n            num_augmentations (int): Number of augmentations per image.\n            seq (iaa.Sequential): The sequence of augmentations to apply.\n        \"\"\"\n        self.train_path = Path(train_path)\n        self.num_augmentations = num_augmentations\n        self.seq = self._get_augmentation_sequence()\n\n    def _get_augmentation_sequence(self) -> iaa.Sequential:\n        \"\"\"\n        Define a sequence of augmentations with different probabilities\n        for each augmentation.\n\n        Returns:\n            iaa.Sequential: The sequence of augmentations to apply.\n        \"\"\"\n        augmentations = [\n            # 50% probability to flip upside down\n            iaa.Sometimes(0.5, iaa.Flipud()),\n            # 50% probability to flip left to right\n            iaa.Sometimes(0.5, iaa.Fliplr()),\n            # 50% probability to rotate\n            iaa.Sometimes(0.5, iaa.Affine(rotate=(-45, 45))),\n            # 50% probability to resize\n            iaa.Sometimes(0.5, iaa.Resize((0.7, 1.3))),\n            # 40% probability to change brightness\n            iaa.Sometimes(0.4, iaa.Multiply((0.8, 1.2))),\n            # 40% probability to change contrast\n            iaa.Sometimes(0.4, iaa.LinearContrast((0.8, 1.2))),\n            # 30% probability to blur\n            iaa.Sometimes(0.3, iaa.GaussianBlur(sigma=(0, 0.5))),\n            # 40% probability to crop\n            iaa.Sometimes(0.4, iaa.Crop(percent=(0, 0.3))),\n            # 30% probability for salt and pepper noise\n            iaa.Sometimes(0.3, iaa.SaltAndPepper(0.02)),\n            # 30% probability for elastic transformation\n            iaa.Sometimes(\n                0.3,\n                iaa.ElasticTransformation(\n                    alpha=(0, 30),\n                    sigma=10,\n                ),\n            ),\n            # 20% probability to add motion blur to simulate water flow\n            iaa.Sometimes(0.2, iaa.MotionBlur(k=15, angle=[-45, 45])),\n            # 40% probability to shear on X axis\n            iaa.Sometimes(0.4, iaa.ShearX((-40, 40))),\n            # 40% probability to shear on Y axis\n            iaa.Sometimes(0.4, iaa.ShearY((-40, 40))),\n            # 30% probability to sharpen\n            iaa.Sometimes(\n                0.3,\n                iaa.Sharpen(\n                    alpha=(0, 0.5),\n                    lightness=(0.8, 1.2),\n                ),\n            ),\n            # 20% probability for piecewise affine\n            iaa.Sometimes(0.2, iaa.PiecewiseAffine(scale=(0.01, 0.03))),\n            # 30% probability to grayscale\n            iaa.Sometimes(0.3, iaa.Grayscale(alpha=(0.0, 1.0))),\n            # 30% probability to change hue and saturation\n            iaa.Sometimes(0.3, iaa.AddToHueAndSaturation((-30, 30))),\n            # 30% probability to change gamma contrast\n            iaa.Sometimes(0.3, iaa.GammaContrast((0.5, 1.5))),\n            # 30% probability to change color temperature\n            iaa.Sometimes(0.3, iaa.ChangeColorTemperature((3300, 6500))),\n            # 20% probability for perspective transform\n            iaa.Sometimes(0.2, iaa.PerspectiveTransform(scale=(0.01, 0.1))),\n            # 20% probability for coarse dropout\n            iaa.Sometimes(\n                0.2,\n                iaa.CoarseDropout(\n                    (0.0, 0.05),\n                    size_percent=(0.02, 0.25),\n                ),\n            ),\n            # 20% probability to invert colors\n            iaa.Sometimes(0.2, iaa.Invert(0.3)),\n            # 20% probability for Gaussian noise\n            iaa.Sometimes(\n                0.2,\n                iaa.AdditiveGaussianNoise(\n                    scale=(0, 0.05 * 255),\n                ),\n            ),\n            # 20% probability for Poisson noise\n            iaa.Sometimes(0.2, iaa.AdditivePoissonNoise(lam=(0, 30))),\n            # 30% probability for Dropout2d\n            iaa.Sometimes(0.3, iaa.Dropout2d(p=(0.1, 0.3))),\n            # 20% probability for edge detection\n            iaa.Sometimes(0.2, iaa.EdgeDetect(alpha=(0.2, 0.5))),\n            iaa.Sometimes(\n                0.2,\n                iaa.WithColorspace(\n                    to_colorspace='HSV',\n                    from_colorspace='RGB',\n                    # 20% probability to change HSV\n                    children=iaa.WithChannels(0, iaa.Add((10, 50))),\n                ),\n            ),\n            # 40% probability to change brightness\n            iaa.Sometimes(0.4, iaa.AddToBrightness((-30, 30))),\n            # 40% probability to add watermarks\n            iaa.Sometimes(0.3, iaa.imgcorruptlike.Spatter(severity=1)),\n            # 10% probability to add Snow\n            iaa.Sometimes(0.1, iaa.imgcorruptlike.Snow(severity=2)),\n            # 10% probability to add Frost\n            iaa.Sometimes(0.1, iaa.imgcorruptlike.Frost(severity=2)),\n            # 20% probability for speckle noise\n            iaa.Sometimes(0.3, iaa.ImpulseNoise(0.02)),\n            # 20% probability for superpixels\n            iaa.Sometimes(0.2, iaa.Superpixels(p_replace=(0.1, 0.5))),\n            # 30% probability for Fog\n            iaa.Sometimes(0.3, iaa.imgcorruptlike.Fog(severity=2)),\n            # 30% probability for brightness augmentation\n            iaa.Sometimes(0.5, iaa.MultiplyBrightness((0.5, 1.5))),\n            # 20% probability for cutout\n            iaa.Sometimes(\n                0.2, iaa.Cutout(\n                    nb_iterations=2, size=0.2, squared=False,\n                ),\n            ),\n        ]\n        return iaa.Sequential(augmentations, random_order=True)\n\n    def augment_image(self, image_path: Path):\n        \"\"\"\n        Processes and augments a single image.\n\n        Args:\n            image_path (Path): The path to the image file.\n        \"\"\"\n        image = None\n        bbs = None\n        try:\n            print(f\"Processing image: {image_path}\")\n            image = imageio.imread(image_path)\n\n            # Check if the image is None or has no shape, and return early\n            if image is None or image.shape is None:\n                print(f\"Image is None or has no shape: {image_path}\")\n                return\n\n            # Remove alpha channel if present\n            if image.shape[2] == 4:\n                image = image[:, :, :3]\n\n            # Ensure original_shape is a tuple of three integers\n            original_shape: tuple[int, int, int] = (\n                image.shape[0], image.shape[1], image.shape[2],\n            )\n            label_path = (\n                self.train_path / 'labels' /\n                image_path.with_suffix('.txt').name\n            )\n            bbs = BoundingBoxesOnImage(\n                self.read_label_file(label_path, original_shape),\n                shape=original_shape,\n            )\n\n            # Check and resize small images\n            if image.shape[0] < 32 or image.shape[1] < 32:\n                print(f\"Resize {image_path} due to small size: {image.shape}\")\n                image = iaa.Resize(\n                    {'shorter-side': 32, 'longer-side': 'keep-aspect-ratio'},\n                )(image=image)\n\n            # Check and resize large images\n            if image.shape[0] > 1920 or image.shape[1] > 1920:\n                print(f\"Resize {image_path} due to large size: {image.shape}\")\n                image = iaa.Resize(\n                    {'longer-side': 1920, 'shorter-side': 'keep-aspect-ratio'},\n                )(image=image)\n\n            resized_shape = image.shape\n            # Adjust bounding boxes to the new image shape\n            bbs = bbs.on(resized_shape)\n\n            for i in range(self.num_augmentations):\n                # Apply augmentations to the image and bounding boxes\n                image_aug, bbs_aug = self.seq(image=image, bounding_boxes=bbs)\n                bbs_aug = bbs_aug.clip_out_of_image()\n\n                # Save the augmented image and label file\n                aug_image_filename = (\n                    f\"{image_path.stem}_aug_{i}{image_path.suffix}\"\n                )\n                aug_label_filename = (\n                    f\"{image_path.stem}_aug_{i}.txt\"\n                )\n\n                image_aug_path = (\n                    self.train_path / 'images' / aug_image_filename\n                )\n                label_aug_path = (\n                    self.train_path / 'labels' / aug_label_filename\n                )\n\n                print(f\"Saving augmented image to {image_aug_path}\")\n                imageio.imwrite(image_aug_path, image_aug, pilmode='RGB')\n                self.write_label_file(\n                    bbs_aug,\n                    label_aug_path,\n                    image_aug.shape[1],\n                    image_aug.shape[0],\n                )\n\n                # Explicitly delete to free memory\n                del image_aug, bbs_aug\n        except Exception as e:\n            print(f\"Error processing image: {image_path}:\")\n            print(e)\n        finally:\n            if image is not None:\n                del image\n            if bbs is not None:\n                del bbs\n\n    def augment_data(self, batch_size=10):\n        \"\"\"\n        Processes images in batches to save memory.\n\n        Args:\n            batch_size (int): The number of images to process in each batch.\n        \"\"\"\n        image_paths = list(self.train_path.glob('images/*.jpg'))\n        batches = [\n            image_paths[i: i + batch_size]\n            for i in range(0, len(image_paths), batch_size)\n        ]\n\n        for batch in tqdm(batches):\n            for image_path in batch:\n                self.augment_image(image_path)\n            gc.collect()  # Collect garbage after each batch\n\n    @staticmethod\n    def read_label_file(\n        label_path: Path,\n        image_shape: tuple[int, int, int],\n    ) -> list[BoundingBox]:\n        \"\"\"\n        Reads a label file and converts it into a list of bounding boxes.\n\n        Args:\n            label_path (Path): The path to the label file.\n            image_shape (tuple): The shape of the image.\n\n        Returns:\n            list[BoundingBox]: The list of bounding boxes.\n        \"\"\"\n        bounding_boxes: list[BoundingBox] = []\n        if not label_path.exists():\n            return bounding_boxes\n\n        with open(label_path, encoding='utf-8') as file:\n            lines = file.readlines()\n\n        for line in lines:\n            values = list(map(float, line.split()))\n            if len(values) == 5:\n                class_id, x_center, y_center, width, height = values\n                x1 = (x_center - width / 2) * image_shape[1]\n                y1 = (y_center - height / 2) * image_shape[0]\n                x2 = (x_center + width / 2) * image_shape[1]\n                y2 = (y_center + height / 2) * image_shape[0]\n                bounding_boxes.append(\n                    BoundingBox(\n                        x1=x1,\n                        y1=y1,\n                        x2=x2,\n                        y2=y2,\n                        label=int(class_id),\n                    ),\n                )\n        return bounding_boxes\n\n    @staticmethod\n    def write_label_file(\n        bounding_boxes: BoundingBoxesOnImage,\n        label_path: Path,\n        image_width: int,\n        image_height: int,\n    ):\n        \"\"\"\n        Writes bounding boxes to a label file.\n\n        Args:\n            bounding_boxes (BoundingBoxesOnImage): The bounding boxes to write.\n            label_path (Path): The path to the label file.\n            image_width (int): The width of the image.\n            image_height (int): The height of the image.\n        \"\"\"\n        with label_path.open('w') as f:\n            # Iterate through actual bounding boxes\n            for (bb) in bounding_boxes.bounding_boxes:\n                print(f\"Writing bounding box: {bb}\")\n                x_center = (bb.x1 + bb.x2) / 2 / image_width\n                y_center = (bb.y1 + bb.y2) / 2 / image_height\n                width = (bb.x2 - bb.x1) / image_width\n                height = (bb.y2 - bb.y1) / image_height\n                f.write(f\"{bb.label} {x_center} {y_center} {width} {height}\\n\")\n\n    def shuffle_data(self) -> None:\n        \"\"\"\n        Shuffles the augmented dataset to ensure randomness.\n\n        This method pairs each image file with its corresponding label file,\n        assigns a unique UUID to each pair, and then saves them with the new\n        names into the original directories.\n        \"\"\"\n        image_dir = self.train_path / 'images'\n        label_dir = self.train_path / 'labels'\n\n        # Retrieve paths for all image and label files\n        image_paths = list(image_dir.glob('*'))\n        label_paths = list(label_dir.glob('*'))\n\n        # Ensure the count of images and labels matches\n        assert len(image_paths) == len(\n            label_paths,\n        ), 'The counts of image and label files do not match!'\n\n        # Sort paths to ensure matching between images and labels\n        image_paths.sort()\n        label_paths.sort()\n\n        # Shuffle image and label paths together to maintain correspondence\n        combined = list(zip(image_paths, label_paths))\n        random.shuffle(combined)\n\n        # Rename files with a new UUID\n        for image_path, label_path in combined:\n            # Generate a unique identifier\n            unique_id = str(uuid.uuid4())\n            new_image_name = unique_id + image_path.suffix\n            new_label_name = unique_id + label_path.suffix\n\n            new_image_path = image_dir / new_image_name\n            new_label_path = label_dir / new_label_name\n\n            image_path.rename(new_image_path)\n            label_path.rename(new_label_path)\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description='Perform data augmentation on image datasets.',\n    )\n    parser.add_argument(\n        '--train_path',\n        type=str,\n        default='./dataset_aug/train',\n        help='Path to the training data directory.',\n    )\n    parser.add_argument(\n        '--num_augmentations',\n        type=int,\n        default=40,\n        help='Number of augmentations per image.',\n    )\n    parser.add_argument(\n        '--batch_size',\n        type=int,\n        default=5,\n        help='Number of images to process in each batch.',\n    )\n    args = parser.parse_args()\n\n    augmenter = DataAugmentation(args.train_path, args.num_augmentations)\n    augmenter.augment_data(batch_size=args.batch_size)\n\n    # Pause for 5 seconds before shuffling to allow for user inspection.\n    print('Pausing for 5 seconds before shuffling data...')\n    time.sleep(5)\n\n    augmenter.shuffle_data()\n    print('Data augmentation and shuffling complete.')\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "examples/YOLO_server_api/backend/security.py", "content": "from __future__ import annotations\n\nimport secrets\n\nfrom fastapi import FastAPI\n\n\ndef update_secret_key(app: FastAPI) -> None:\n    \"\"\"\n    Updates the JWT secret key for the FastAPI application.\n\n    This function generates a new, secure JWT secret key and assigns it to\n    the application's state. The new key is a URL-safe token of 16 bytes.\n\n    Args:\n        app (FastAPI): The FastAPI application instance to update.\n    \"\"\"\n    # Generate a new URL-safe token of 16 bytes\n    app.state.jwt_secret_key = secrets.token_urlsafe(16)\n"}
{"type": "source_file", "path": "examples/YOLO_train/__init__.py", "content": ""}
{"type": "source_file", "path": "examples/YOLO_server_api/backend/detection.py", "content": "from __future__ import annotations\n\nimport gc\nfrom typing import Any\n\nimport cv2\nimport numpy as np\nfrom sahi.predict import get_sliced_prediction\n\nfrom .models import DetectionModelManager\n\nmodel_loader = DetectionModelManager()\n\n\nasync def convert_to_image(data: bytes) -> np.ndarray:\n    \"\"\"\n    Converts raw image bytes into an OpenCV image format.\n\n    Args:\n        data (bytes): The image data in bytes.\n\n    Returns:\n        np.ndarray: The decoded image in OpenCV format.\n    \"\"\"\n    # Convert image bytes to a NumPy array\n    npimg = np.frombuffer(data, np.uint8)\n    # Decode the NumPy array into an OpenCV image\n    img = cv2.imdecode(npimg, cv2.IMREAD_COLOR)\n    return img\n\n\nasync def get_prediction_result(\n    img: np.ndarray,\n    model: DetectionModelManager,\n) -> Any:\n    \"\"\"\n    Generates sliced predictions for an image using the specified model.\n\n    Args:\n        img (np.ndarray): The image in OpenCV format.\n        model (DetectionModelManager): The object detection model instance.\n\n    Returns:\n        Any: The prediction result from the model.\n    \"\"\"\n    # Use the SAHI library's get_sliced_prediction function for detection\n    return get_sliced_prediction(\n        img,\n        model,\n        slice_height=370,\n        slice_width=370,\n        overlap_height_ratio=0.3,\n        overlap_width_ratio=0.3,\n    )\n\n\ndef compile_detection_data(result: Any) -> list[list[float | int]]:\n    \"\"\"\n    Compiles detection data from the model's prediction results.\n\n    Args:\n        result (Any): The result of the model prediction.\n\n    Returns:\n        list[list[float | int]]: Detection data including bounding boxes,\n        confidence, and label IDs.\n    \"\"\"\n    datas = []\n    # Extract bounding box, confidence, and label ID for each prediction\n    for object_prediction in result.object_prediction_list:\n        label = int(object_prediction.category.id)\n        x1, y1, x2, y2 = (int(x) for x in object_prediction.bbox.to_voc_bbox())\n        confidence = float(object_prediction.score.value)\n        datas.append([x1, y1, x2, y2, confidence, label])\n    return datas\n\n\nasync def process_labels(\n    datas: list[list[float | int]],\n) -> list[list[float | int]]:\n    \"\"\"\n    Processes detection data to remove overlapping and contained labels.\n\n    Args:\n        datas (list[list[float | int]]): The detection data to process.\n\n    Returns:\n        list[list[float | int]]: The processed detection data.\n    \"\"\"\n    # Remove overlapping and completely contained labels\n    datas = await remove_overlapping_labels(datas)\n    datas = await remove_completely_contained_labels(datas)\n    datas = await remove_overlapping_labels(datas)\n    return datas\n\n\nasync def remove_overlapping_labels(\n    datas: list[list[float | int]],\n) -> list[list[float | int]]:\n    \"\"\"\n    Removes overlapping labels based on predefined thresholds.\n\n    Args:\n        datas (list[list[float | int]]): The detection data to process.\n\n    Returns:\n        list[list[float | int]]: The detection data\n        with overlapping labels removed.\n    \"\"\"\n    # Organise data by category indices for efficient processing\n    category_indices = get_category_indices(datas)\n\n    # Identify overlapping labels to remove\n    to_remove = set()\n    to_remove.update(\n        await find_overlaps(\n            category_indices['hardhat'],\n            category_indices['no_hardhat'],\n            datas,\n            0.5,\n        ),\n    )\n    to_remove.update(\n        await find_overlaps(\n            category_indices['safety_vest'],\n            category_indices['no_safety_vest'],\n            datas,\n            0.5,\n        ),\n    )\n\n    # Remove overlapping entries from the data list\n    for index in sorted(to_remove, reverse=True):\n        datas.pop(index)\n\n    # Run garbage collection to free memory\n    gc.collect()\n    return datas\n\n\ndef get_category_indices(\n    datas: list[list[float | int]],\n) -> dict[str, list[int]]:\n    \"\"\"\n    Organises detection data by category indices for quicker access.\n\n    Args:\n        datas (list[list[float | int]]): The detection data.\n\n    Returns:\n        dict[str, list[int]]:\n            A dictionary mapping category names to lists of indices.\n    \"\"\"\n    # Create a dictionary with lists of indices for each category\n    return {\n        'hardhat': [i for i, d in enumerate(datas) if d[5] == 0],\n        'no_hardhat': [i for i, d in enumerate(datas) if d[5] == 2],\n        'safety_vest': [i for i, d in enumerate(datas) if d[5] == 7],\n        'no_safety_vest': [i for i, d in enumerate(datas) if d[5] == 4],\n    }\n\n\nasync def find_overlaps(\n    indices1: list[int],\n    indices2: list[int],\n    datas: list[list[float | int]],\n    threshold: float,\n) -> set[int]:\n    \"\"\"\n    Finds overlaps between two sets of category indices.\n\n    Args:\n        indices1 (list[int]): The first set of indices.\n        indices2 (list[int]): The second set of indices.\n        datas (list[list[float | int]]): The detection data.\n        threshold (float): The overlap threshold.\n\n    Returns:\n        set[int]: Indices of overlapping labels to be removed.\n    \"\"\"\n    to_remove = set()\n    # Check for overlaps between two sets of indices\n    for index1 in indices1:\n        to_remove.update(\n            await find_overlapping_indices(\n                index1, indices2, datas, threshold,\n            ),\n        )\n    return to_remove\n\n\nasync def find_overlapping_indices(\n    index1: int,\n    indices2: list[int],\n    datas: list[list[float | int]], threshold: float,\n) -> set[int]:\n    \"\"\"\n    Finds overlapping indices for a single detection index.\n\n    Args:\n        index1 (int): The index of the first detection.\n        indices2 (list[int]): The indices of potential overlapping detections.\n        datas (list[list[float | int]]): The detection data.\n        threshold (float): The overlap threshold.\n\n    Returns:\n        set[int]: Indices of overlapping labels.\n    \"\"\"\n    # Calculate overlaps for a specific detection\n    # and return those exceeding the threshold\n    return {\n        index2 for index2 in indices2\n        if calculate_overlap(\n            [int(x) for x in datas[index1][:4]],\n            [int(x) for x in datas[index2][:4]],\n        ) > threshold\n    }\n\n\ndef calculate_overlap(bbox1: list[int], bbox2: list[int]) -> float:\n    \"\"\"\n    Calculates the overlap between two bounding boxes.\n\n    Args:\n        bbox1 (list[int]): The first bounding box.\n        bbox2 (list[int]): The second bounding box.\n\n    Returns:\n        float: The overlap percentage.\n    \"\"\"\n    # Calculate intersection coordinates and area\n    x1, y1, x2, y2 = calculate_intersection(bbox1, bbox2)\n    intersection_area = calculate_area(x1, y1, x2, y2)\n    # Calculate area of each bounding box\n    bbox1_area = calculate_area(*bbox1)\n    bbox2_area = calculate_area(*bbox2)\n\n    # Calculate the overlap percentage\n    overlap_percentage = intersection_area / \\\n        float(bbox1_area + bbox2_area - intersection_area)\n    gc.collect()\n    return overlap_percentage\n\n\ndef calculate_intersection(\n    bbox1: list[int],\n    bbox2: list[int],\n) -> tuple[int, int, int, int]:\n    \"\"\"\n    Calculates the intersection coordinates of two bounding boxes.\n\n    Args:\n        bbox1 (list[int]): The first bounding box.\n        bbox2 (list[int]): The second bounding box.\n\n    Returns:\n        tuple[int, int, int, int]: The intersection coordinates.\n    \"\"\"\n    # Determine the coordinates for the intersection area\n    x1 = max(bbox1[0], bbox2[0])\n    y1 = max(bbox1[1], bbox2[1])\n    x2 = min(bbox1[2], bbox2[2])\n    y2 = min(bbox1[3], bbox2[3])\n    return x1, y1, x2, y2\n\n\ndef calculate_area(x1: int, y1: int, x2: int, y2: int) -> int:\n    \"\"\"\n    Calculates the area of a bounding box.\n\n    Args:\n        x1 (int): The x-coordinate of the top-left corner.\n        y1 (int): The y-coordinate of the top-left corner.\n        x2 (int): The x-coordinate of the bottom-right corner.\n        y2 (int): The y-coordinate of the bottom-right corner.\n\n    Returns:\n        int: The area of the bounding box.\n    \"\"\"\n    # Calculate area, ensuring no negative dimensions\n    return max(0, x2 - x1 + 1) * max(0, y2 - y1 + 1)\n\n\ndef is_contained(inner_bbox: list[int], outer_bbox: list[int]) -> bool:\n    \"\"\"\n    Checks if one bounding box is fully contained within another.\n\n    Args:\n        inner_bbox (list[int]): The inner bounding box.\n        outer_bbox (list[int]): The outer bounding box.\n\n    Returns:\n        bool: True if the inner bounding box is fully contained in\n        the outer bounding box.\n    \"\"\"\n    # Verify if each coordinate of inner_bbox is within outer_bbox\n    return (\n        inner_bbox[0] >= outer_bbox[0]\n        and inner_bbox[2] <= outer_bbox[2]\n        and inner_bbox[1] >= outer_bbox[1]\n        and inner_bbox[3] <= outer_bbox[3]\n    )\n\n\nasync def remove_completely_contained_labels(\n    datas: list[list[float | int]],\n) -> list[list[float | int]]:\n    \"\"\"\n    Removes labels that are fully contained within other labels.\n\n    Args:\n        datas (list[list[float | int]]): The detection data.\n\n    Returns:\n        list[list[float | int]]: The detection data\n        with contained labels removed.\n    \"\"\"\n    # Get indices of each category for comparison\n    category_indices = get_category_indices(datas)\n\n    # Identify labels that are contained within others\n    to_remove = set()\n    to_remove.update(\n        await find_contained_labels(\n            category_indices['hardhat'],\n            category_indices['no_hardhat'],\n            datas,\n        ),\n    )\n    to_remove.update(\n        await find_contained_labels(\n            category_indices['safety_vest'],\n            category_indices['no_safety_vest'],\n            datas,\n        ),\n    )\n\n    # Remove fully contained labels\n    for index in sorted(to_remove, reverse=True):\n        datas.pop(index)\n\n    return datas\n\n\nasync def find_contained_labels(\n    indices1: list[int],\n    indices2: list[int],\n    datas: list[list[float | int]],\n) -> set[int]:\n    \"\"\"\n    Finds labels that are fully contained within other labels.\n\n    Args:\n        indices1 (list[int]): The indices of the first category.\n        indices2 (list[int]): The indices of the second category.\n        datas (list[list[float | int]]): The detection data.\n\n    Returns:\n        set[int]: Indices of contained labels to be removed.\n    \"\"\"\n    to_remove = set()\n    # Check for containment of labels across two sets of indices\n    for index1 in indices1:\n        to_remove.update(await find_contained_indices(index1, indices2, datas))\n    return to_remove\n\n\nasync def find_contained_indices(\n    index1: int,\n    indices2: list[int],\n    datas: list[list[float | int]],\n) -> set[int]:\n    \"\"\"\n    Finds indices of detections that are fully contained within others.\n\n    Args:\n        index1 (int): The index of the first detection.\n        indices2 (list[int]): The indices of potential containing detections.\n        datas (list[list[float | int]]): The detection data.\n\n    Returns:\n        set[int]: Indices of contained labels.\n    \"\"\"\n    to_remove = set()\n    # Determine if one detection is fully contained within another\n    for index2 in indices2:\n        to_remove.update(await check_containment(index1, index2, datas))\n    return to_remove\n\n\nasync def check_containment(\n    index1: int,\n    index2: int,\n    datas: list[list[float | int]],\n) -> set[int]:\n    \"\"\"\n    Checks if one detection is fully contained within another.\n\n    Args:\n        index1 (int): The index of the first detection.\n        index2 (int): The index of the second detection.\n        datas (list[list[float | int]]): The detection data.\n\n    Returns:\n        set[int]: Indices of contained labels.\n    \"\"\"\n    to_remove = set()\n    # Verify if either index1 or index2 is contained within the other\n    if is_contained(\n        [int(x) for x in datas[index2][:4]],\n        [int(x) for x in datas[index1][:4]],\n    ):\n        to_remove.add(index2)\n    elif is_contained(\n        [int(x) for x in datas[index1][:4]],\n        [int(x) for x in datas[index2][:4]],\n    ):\n        to_remove.add(index1)\n    return to_remove\n"}
{"type": "source_file", "path": "examples/YOLO_data_augmentation/visualise_bounding_boxes.py", "content": "from __future__ import annotations\n\nimport argparse\nfrom pathlib import Path\n\nimport cv2\nfrom matplotlib import pyplot as plt\n\n\nclass BoundingBoxVisualiser:\n    \"\"\"\n    Class for visualising bounding boxes on images.\n    \"\"\"\n\n    def __init__(\n        self,\n        image_path: str | Path,\n        label_path: str | Path,\n        class_names: list,\n    ):\n        \"\"\"\n        Initialises the BoundingBoxVisualiser with the specified image,\n        label paths, and class names.\n\n        Args:\n            image_path: The path to the image file.\n            label_path: The path to the label file.\n            class_names: A list of class names.\n        \"\"\"\n        self.image_path = Path(image_path)\n        self.label_path = Path(label_path)\n        self.class_names = class_names\n        self.image = cv2.imread(str(self.image_path))\n        if self.image is None:\n            raise ValueError(\n                'Image could not be loaded. Please check the image path.',\n            )\n\n    def draw_bounding_boxes(self) -> None:\n        \"\"\"\n        Draws bounding boxes on the image based on the label file.\n        \"\"\"\n        height, width, _ = self.image.shape\n\n        # Explicitly specify the mode 'r' when opening the file\n        with self.label_path.open('r') as f:\n            lines = f.readlines()\n\n        for line in lines:\n            class_id, x_centre, y_centre, bbox_width, bbox_height = map(\n                float,\n                line.split(),\n            )\n\n            # Convert from relative to absolute coordinates\n            x_centre, bbox_width = x_centre * width, bbox_width * width\n            y_centre, bbox_height = y_centre * height, bbox_height * height\n\n            # Calculate the top left corner\n            x1, y1 = int(\n                x_centre - bbox_width / 2,\n            ), int(y_centre - bbox_height / 2)\n            x2, y2 = int(\n                x_centre + bbox_width / 2,\n            ), int(y_centre + bbox_height / 2)\n\n            # Draw the rectangle and label\n            cv2.rectangle(self.image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n            label = self.class_names[int(class_id)]\n            cv2.putText(\n                self.image,\n                label,\n                (x1, y1 - 10),\n                cv2.FONT_HERSHEY_SIMPLEX,\n                0.9,\n                (255, 0, 0),\n                2,\n            )\n\n    def save_or_display_image(\n        self, output_path: str | Path, save: bool,\n    ) -> None:\n        \"\"\"\n        Saves or displays the image based on the user's preference.\n\n        Args:\n            output_path: Path to save the image with drawn bounding boxes.\n            save: A boolean indicating whether to save the image or display it.\n        \"\"\"\n        if save:\n            cv2.imwrite(str(output_path), self.image)\n            print(f\"Image saved to {output_path}\")\n        else:\n            plt.imshow(cv2.cvtColor(self.image, cv2.COLOR_BGR2RGB))\n            plt.axis('off')\n            plt.show()\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description='Visualise bounding boxes on images.',\n    )\n    parser.add_argument(\n        '--image',\n        help='The path to the image file.',\n        required=True,\n    )\n    parser.add_argument(\n        '--label',\n        help='The path to the label file.',\n        required=True,\n    )\n    parser.add_argument(\n        '--output',\n        help='The path where the image should be saved.',\n        default='visualised_image.jpg',\n    )\n    parser.add_argument(\n        '--save',\n        action='store_true',\n        help='Flag whether to save the image instead of displaying it.',\n    )\n\n    args = parser.parse_args()\n\n    # List of class names as specified in your data.yaml file\n    class_names = [\n        'Hardhat',\n        'Mask',\n        'NO-Hardhat',\n        'NO-Mask',\n        'NO-Safety Vest',\n        'Person',\n        'Safety Cone',\n        'Safety Vest',\n        'machinery',\n        'vehicle',\n    ]\n\n    visualiser = BoundingBoxVisualiser(args.image, args.label, class_names)\n    visualiser.draw_bounding_boxes()\n    visualiser.save_or_display_image(args.output, args.save)\n\n\nif __name__ == '__main__':\n    main()\n\n\n\"\"\"example\npython visualise_bounding_boxes.py \\\n    --image './aug_4.jpg' \\\n    --label './aug_4.txt'\n\"\"\"\n"}
{"type": "source_file", "path": "examples/YOLO_data_augmentation/data_augmentation_albumentations.py", "content": "from __future__ import annotations\n\nimport argparse\nimport gc\nimport os\nimport random\nimport time\nimport uuid\nfrom collections.abc import Sequence\nfrom concurrent.futures import ProcessPoolExecutor\nfrom pathlib import Path\n\nimport albumentations as A\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\n\n\nclass DataAugmentation:\n    \"\"\"\n    A class to perform data augmentation for image datasets, especially useful\n    for training machine learning models.\n    \"\"\"\n\n    def __init__(self, train_path: str, num_augmentations: int = 1):\n        \"\"\"\n        Initialise the DataAugmentation class.\n\n        Args:\n            train_path (str): The path to the training data.\n            num_augmentations (int): Number of augmentations per image.\n        \"\"\"\n        self.train_path = Path(train_path)\n        self.num_augmentations = num_augmentations\n\n    def resize_image_and_bboxes(\n        self,\n        image: np.ndarray,\n        bboxes: list[list[float]],\n        class_labels: list[int],\n        image_path: Path,\n    ) -> tuple[np.ndarray, list]:\n        \"\"\"\n        Resize images and bounding boxes if they are too small or too large.\n\n        Args:\n            image (np.ndarray): The input image.\n            bboxes (list): The bounding boxes.\n            class_labels (list[int]): The class labels.\n            image_path (Path): The path to the image file.\n\n        Returns:\n            tuple[np.ndarray, list]: The resized image and bounding boxes.\n        \"\"\"\n        # Check and resize small images\n        if image.shape[0] < 32 or image.shape[1] < 32:\n            print(f\"Resize {image_path} due to small size: {image.shape}\")\n            transform = A.Compose(\n                [\n                    A.SmallestMaxSize(\n                        max_size=64, interpolation=cv2.INTER_LINEAR,\n                    ),\n                    A.LongestMaxSize(\n                        max_size=64, interpolation=cv2.INTER_LINEAR,\n                    ),\n                ],\n                bbox_params=A.BboxParams(\n                    format='yolo',\n                    label_fields=['class_labels'],\n                    clip=True,\n                ),\n            )\n            transformed = transform(\n                image=image, bboxes=bboxes, class_labels=class_labels,\n            )\n            image, bboxes = transformed['image'], transformed['bboxes']\n\n        # Check and resize large images\n        if image.shape[0] > 1920 or image.shape[1] > 1920:\n            print(f\"Resize {image_path} due to large size: {image.shape}\")\n            transform = A.Compose(\n                [\n                    A.LongestMaxSize(\n                        max_size=1920, interpolation=cv2.INTER_LINEAR,\n                    ),\n                    A.SmallestMaxSize(\n                        max_size=1920, interpolation=cv2.INTER_LINEAR,\n                    ),\n                ],\n                bbox_params=A.BboxParams(\n                    format='yolo',\n                    label_fields=['class_labels'],\n                    clip=True,\n                ),\n            )\n            transformed = transform(\n                image=image, bboxes=bboxes, class_labels=class_labels,\n            )\n            image, bboxes = transformed['image'], transformed['bboxes']\n\n        return image, bboxes\n\n    def random_crop_with_random_size(self, image, **kwargs) -> np.ndarray:\n        \"\"\"\n        Randomly crop the image at a random position with a random size.\n\n        Args:\n            image (np.ndarray): Input image.\n\n        Returns:\n            np.ndarray: Cropped image.\n        \"\"\"\n        height, width = image.shape[:2]\n\n        # Randomly generate the height and width of the crop box\n        crop_height = random.randint(400, 800)\n        crop_width = random.randint(400, 800)\n\n        # Ensure the crop box does not exceed the original image size\n        max_x = max(0, width - crop_width)\n        max_y = max(0, height - crop_height)\n\n        # Randomly generate the top-left coordinates of the crop box\n        start_x = random.randint(0, max_x)\n        start_y = random.randint(0, max_y)\n\n        # Crop the image\n        cropped_image = image[\n            start_y:start_y +\n            crop_height, start_x:start_x + crop_width,\n        ]\n        return cropped_image\n\n    def get_random_target_image(self) -> np.ndarray:\n        \"\"\"\n        Get a random target image for the Fisher Discriminant Analysis (FDA).\n\n        Returns:\n            np.ndarray: The target image.\n        \"\"\"\n        image_paths = [\n            p for p in self.train_path.glob(\n                'images/*.jpg',\n            ) if '_aug_' not in p.stem\n        ]\n        random_image_path = random.choice(image_paths)\n        image = cv2.imread(str(random_image_path))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        return image\n\n    def random_transform(self) -> A.Compose:\n        \"\"\"\n        Generate a random augmentation pipeline.\n\n        Returns:\n            A.Compose: The augmentation pipeline.\n        \"\"\"\n        # Augmentations that affect bounding boxes\n        bbox_augmentations: Sequence[A.BasicTransform | A.BaseCompose] = [\n            A.HorizontalFlip(p=1),\n            A.VerticalFlip(p=1),\n            A.RandomRotate90(p=1),\n            A.Rotate(limit=(-45, 45), p=1),\n            A.Affine(\n                scale=(0.9, 1.1),  # Scaling range\n                translate_percent=(0.05, 0.1),  # Translation percentage range\n                shear=(-15, 15),  # Shear angle range\n                rotate=45,  # Rotation angle range\n                p=1,  # Probability\n            ),\n            A.Transpose(p=1),\n            A.Perspective(scale=(0.05, 0.1), p=1),\n            A.ElasticTransform(alpha=1, sigma=50, p=1),\n            A.Lambda(image=self.random_crop_with_random_size, p=1),\n            A.RandomResizedCrop(size=(640, 640), scale=(0.3, 1.0), p=1),\n            A.RandomGridShuffle(grid=(3, 3), p=1),\n            A.GridDistortion(p=1),\n        ]\n\n        # Augmentations that do not affect bounding boxes\n        non_bbox_augmentations = [\n            # Colour and brightness adjustments\n            A.RandomBrightnessContrast(\n                brightness_limit=(0.0, 0.03),\n                contrast_limit=(0.0, 0.03),\n                p=1.0,\n            ),\n            A.RGBShift(\n                r_shift_limit=3, g_shift_limit=3,\n                b_shift_limit=3, p=1.0,\n            ),\n            A.HueSaturationValue(\n                hue_shift_limit=3, sat_shift_limit=5, val_shift_limit=3, p=1.0,\n            ),\n            A.ColorJitter(\n                brightness=0.05, contrast=0.05,\n                saturation=0.05, hue=0.02, p=1.0,\n            ),\n            A.PlanckianJitter(p=1),\n            # Blur and noise\n            A.MotionBlur(blur_limit=(3, 7), p=1.0),\n            A.GaussianBlur(blur_limit=(1, 3), p=1.0),\n            A.GaussNoise(std_range=(0.1, 0.5), p=1.0),\n            A.ISONoise(color_shift=(0.01, 0.05), intensity=(0.05, 0.2), p=1.0),\n            A.MedianBlur(blur_limit=3, p=1.0),\n            # Colour space and contrast adjustments\n            A.CLAHE(clip_limit=2, p=1.0),\n            A.Sharpen(alpha=(0.1, 0.3), lightness=(0.7, 1.0), p=1.0),\n            A.CoarseDropout(\n                num_holes_range=(1, 6),\n                hole_height_range=(1, 6),\n                hole_width_range=(1, 6), p=1.0,\n            ),\n            A.ToGray(p=1),\n            A.Equalize(p=1),\n            A.Posterize(num_bits=4, p=1.0),\n            A.InvertImg(p=1),\n            # Special effects\n            A.RandomShadow(\n                shadow_roi=(0, 0.5, 1, 1),\n                num_shadows_limit=(1, 1), shadow_dimension=5, p=1.0,\n            ),\n            # A.RandomFog(fog_coef_lower=0.05, fog_coef_upper=0.1, p=1.0),\n            A.RandomSnow(\n                snow_point_range=(0.05, 0.15),\n                brightness_coeff=1.2, p=1.0,\n            ),\n            A.RandomRain(\n                slant_range=(-5, 5), drop_length=5,\n                drop_width=1, blur_value=1, p=1.0,\n            ),\n            A.ChannelShuffle(p=1),\n            A.RandomSunFlare(\n                flare_roi=(0.02, 0.04, 0.08, 0.08),\n                angle_range=(0, 1), p=1,\n            ),\n            A.Defocus(radius=(3, 5), p=1),\n            # Other augmentations\n            A.RandomToneCurve(scale=0.05, p=1.0),\n            A.RandomGamma(gamma_limit=(90, 110), p=1.0),\n            A.Superpixels(p_replace=0.1, n_segments=100, p=1),\n            A.ImageCompression(quality_range=(70, 90), p=1.0),\n            A.GlassBlur(sigma=0.5, max_delta=1, p=1.0),\n            A.PixelDropout(dropout_prob=0.05, p=1),\n            A.OpticalDistortion(\n                distort_limit=(0.05, 0.1),\n                shift_limit=(0.05, 0.1), p=1.0,\n            ),\n            A.Emboss(p=1),\n            A.MaskDropout(\n                max_objects=3,\n                mask_fill_value=0,\n                p=1.0,\n            ),\n            A.Spatter(p=1),\n            A.ToSepia(p=1),\n            A.FancyPCA(alpha=0.1, p=1),\n            A.FDA(\n                [self.get_random_target_image()], beta_limit=0.5,\n                p=1, read_fn=lambda x: x,\n            ),  #  FDA\n        ]\n\n        # Randomly select 1 to 2 augmentations that affect bounding boxes\n        num_bbox_transforms = random.randint(1, 2)\n        chosen_bbox_transforms = random.sample(\n            bbox_augmentations, k=num_bbox_transforms,\n        )\n\n        # Randomly select 2 to 3 augmentations\n        # that do not affect bounding boxes\n        num_non_bbox_transforms = random.randint(2, 3)\n        chosen_non_bbox_transforms = random.sample(\n            non_bbox_augmentations, k=num_non_bbox_transforms,\n        )\n\n        chosen_transforms = chosen_bbox_transforms + chosen_non_bbox_transforms\n\n        # Return the augmentation pipeline\n        return A.Compose(\n            chosen_transforms + [\n                A.Normalize(\n                    mean=(0.485, 0.456, 0.406), std=(\n                        0.229, 0.224, 0.225,\n                    ), max_pixel_value=255.0,\n                ),\n            ],\n            bbox_params=A.BboxParams(\n                format='yolo',\n                label_fields=['class_labels'],\n                clip=True,\n            ),\n        )\n\n    def process_image(\n        self,\n        image: np.ndarray,\n        bboxes: list[list[float]],\n        class_labels: list[int],\n    ) -> dict:\n        \"\"\"\n        Apply augmentations to the image and bounding boxes.\n\n        Args:\n            image (np.ndarray): The input image.\n            bboxes (list): The bounding boxes.\n            class_labels (list[int]): The class labels.\n\n        Returns:\n            dict: The transformed image and bounding boxes.\n        \"\"\"\n        # Generate a random augmentation pipeline\n        aug_transform = self.random_transform()\n\n        # Apply the augmentation pipeline to the image and bounding boxes\n        transformed = aug_transform(\n            image=image, bboxes=bboxes, class_labels=class_labels,\n        )\n\n        return transformed\n\n    def augment_image(self, image_path: Path) -> None:\n        \"\"\"\n        Processes and augments a single image.\n\n        Args:\n            image_path (Path): The path to the image file.\n        \"\"\"\n        if image_path is None:\n            print('Error processing image: None')\n            return\n\n        # Initialise image and bboxes to None or an empty array\n        image = None\n        bboxes: list[list[float]] = []\n\n        try:\n            # Read the image using OpenCV\n            image = cv2.imread(str(image_path))\n\n            if image is None:\n                print('Error processing image: None')\n                return\n\n            # Remove the alpha channel if the image has 4 channels\n            if image.shape[2] == 4:\n                image = image[:, :, :3]\n\n            # Convert the BGR image to RGB\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            image = np.clip(image, 0, 255).astype(np.uint8)\n\n            # Read the label file\n            label_path = self.train_path / 'labels' / \\\n                image_path.with_suffix('.txt').name\n            class_labels, bboxes = self.read_label_file(label_path)\n\n            # Resize the image and bounding boxes\n            image, bboxes = self.resize_image_and_bboxes(\n                image, bboxes, class_labels, image_path,\n            )\n\n            # Ensure the coordinates are between 0 and 1\n            bboxes = np.clip(bboxes, 0, 1).tolist()\n\n            if bboxes is None or class_labels is None:\n                print(\n                    f\"Skipping augmentation for {image_path} \"\n                    f\"due to missing labels.\",\n                )\n                return\n\n            for i in range(self.num_augmentations):\n                # Apply augmentations to the image and bounding boxes\n                transformed = self.process_image(\n                    image=image, bboxes=bboxes, class_labels=class_labels,\n                )\n                image_aug, bboxes_aug, class_labels_aug = transformed[\n                    'image'\n                ], transformed['bboxes'], transformed['class_labels']\n\n                # Ensure the coordinates are between 0 and 1\n                bboxes_aug = np.clip(bboxes_aug, 0, 1)\n\n                # Ensure the image data type is uint8\n                image_aug = np.clip(image_aug * 255, 0, 255).astype(np.uint8)\n\n                # Save the augmented image and label file\n                aug_image_filename = (\n                    f\"{image_path.stem}_aug_{i}{image_path.suffix}\"\n                )\n                aug_label_filename = (\n                    f\"{image_path.stem}_aug_{i}.txt\"\n                )\n\n                image_aug_path = (\n                    self.train_path / 'images' / aug_image_filename\n                )\n                label_aug_path = (\n                    self.train_path / 'labels' / aug_label_filename\n                )\n\n                # Convert the image back to uint8\n                image_aug = (image_aug * 255).clip(0, 255).astype(np.uint8)\n\n                # Save the image using OpenCV\n                cv2.imwrite(\n                    str(image_aug_path), cv2.cvtColor(\n                        image_aug * 255, cv2.COLOR_RGB2BGR,\n                    ),\n                )\n                self.write_label_file(\n                    bboxes_aug, class_labels_aug, Path(label_aug_path),\n                )\n\n        except Exception as e:\n            print(f\"Error processing image: {image_path}: {e}\")\n        finally:\n            if image is not None:\n                del image\n            if bboxes is not None:\n                del bboxes\n            gc.collect()\n\n    def augment_data(self, batch_size=10) -> None:\n        \"\"\"\n        Processes images in parallel to save time.\n\n        Args:\n            batch_size (int): The number of images to process in each batch.\n        \"\"\"\n        image_paths = list(self.train_path.glob('images/*.jpg'))\n        cpu_count = os.cpu_count() or 1\n        num_workers = min(batch_size, cpu_count - 1)\n\n        print(f\"Using {num_workers} parallel workers for data augmentation.\")\n\n        with ProcessPoolExecutor(max_workers=num_workers) as executor:\n            list(\n                tqdm(\n                    executor.map(\n                        self.augment_image,\n                        image_paths,\n                    ), total=len(image_paths),\n                ),\n            )\n\n    @staticmethod\n    def read_label_file(\n        label_path: Path,\n    ) -> tuple[list[int], list[list[float]]]:\n        \"\"\"\n        Reads a label file and converts it into a list of bounding boxes\n        and class labels.\n\n        Args:\n            label_path (Path): The path to the label file.\n\n        Returns:\n            tuple[list[list[float]], list[int]]:\n                The list of bounding boxes and class labels.\n        \"\"\"\n        annotations = []\n        with open(label_path) as f:\n            for line in f:\n                class_id, x_center, y_center, width, height = map(\n                    float, line.strip().split(),\n                )\n                annotations.append(\n                    [class_id, x_center, y_center, width, height],\n                )\n\n        # Prepare the bounding boxes and class labels\n        bboxes = [ann[1:] for ann in annotations]\n        class_labels = [int(ann[0]) for ann in annotations]\n\n        return class_labels, bboxes\n\n    @staticmethod\n    def write_label_file(\n        bboxes_aug: list[tuple],\n        class_labels_aug: list[int],\n        label_path: Path,\n    ) -> None:\n        \"\"\"\n        Writes bounding boxes and class labels to a label file.\n\n        Args:\n            bboxes (list[tuple]): The bounding boxes to write.\n            class_labels (list[int]): The class labels to write.\n            label_path (Path): The path to the label file.\n        \"\"\"\n        # Combine class labels with the transformed bounding boxes\n        annotations = [[class_labels_aug[i]] +\n                       list(bboxes_aug[i]) for i in range(len(bboxes_aug))]\n\n        with open(label_path, 'w') as f:\n            for ann in annotations:\n                class_id, x_center, y_center, width, height = ann\n                f.write(\n                    f'{int(class_id)} {x_center:.6} {y_center:.6} '\n                    f'{width:.6} {height:.6}\\n',\n                )\n\n    def shuffle_data(self) -> None:\n        \"\"\"\n        Shuffles the augmented dataset to ensure randomness.\n\n        This method pairs each image file with its corresponding label file,\n        assigns a unique UUID to each pair, and then saves them with the new\n        names into the original directories.\n        \"\"\"\n        image_dir = self.train_path / 'images'\n        label_dir = self.train_path / 'labels'\n\n        # Retrieve paths for all image and label files\n        image_paths = list(image_dir.glob('*'))\n        label_paths = list(label_dir.glob('*'))\n\n        # Ensure the count of images and labels matches\n        assert len(image_paths) == len(\n            label_paths,\n        ), 'The counts of image and label files do not match!'\n\n        # Sort paths to ensure matching between images and labels\n        image_paths.sort()\n        label_paths.sort()\n\n        # Shuffle image and label paths together to maintain correspondence\n        combined = list(zip(image_paths, label_paths))\n        random.shuffle(combined)\n\n        # Rename files with a new UUID\n        for image_path, label_path in combined:\n            unique_id = str(uuid.uuid4())\n            new_image_name = unique_id + image_path.suffix\n            new_label_name = unique_id + label_path.suffix\n\n            new_image_path = image_dir / new_image_name\n            new_label_path = label_dir / new_label_name\n\n            image_path.rename(new_image_path)\n            label_path.rename(new_label_path)\n\n\ndef main():\n    \"\"\"\n    Main function to perform data augmentation on image datasets.\n    \"\"\"\n    try:\n        parser = argparse.ArgumentParser(\n            description='Perform data augmentation on image datasets.',\n        )\n        parser.add_argument(\n            '--train_path',\n            type=str,\n            default='./dataset_aug/train',\n            help='Path to the training data directory.',\n        )\n        parser.add_argument(\n            '--num_augmentations',\n            type=int,\n            default=10,\n            help='Number of augmentations per image.',\n        )\n        parser.add_argument(\n            '--batch_size',\n            type=int,\n            default=5,\n            help='Number of images to process in each batch.',\n        )\n        args = parser.parse_args()\n\n        augmenter = DataAugmentation(args.train_path, args.num_augmentations)\n        augmenter.augment_data(batch_size=args.batch_size)\n\n        print('Pausing for 5 seconds before shuffling data...')\n        time.sleep(5)\n\n        augmenter.shuffle_data()\n        print('Data augmentation and shuffling complete.')\n\n    except Exception as e:\n        print(f\"Error: {e}\")\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "examples/YOLO_server_api/backend/auth.py", "content": "# examples/YOLO_server_api/backend/auth.py\nfrom __future__ import annotations\n\nfrom typing import Any\nfrom uuid import uuid4\n\nfrom fastapi import HTTPException\nfrom pydantic import BaseModel\nfrom redis.asyncio import Redis\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.future import select\n\nfrom .cache import get_user_data\nfrom .cache import set_user_data\nfrom .models import User\n\n\nclass UserLogin(BaseModel):\n    username: str\n    password: str\n\n\nasync def create_token_logic(\n    user: UserLogin,\n    db: AsyncSession,\n    redis_pool: Redis,\n    jwt_access,\n    max_jti: int = 2,\n) -> dict[str, Any]:\n    \"\"\"\n    Authenticates a user and generates an access token for them.\n\n    Args:\n        user (UserLogin): The user login details.\n        db (AsyncSession): The database session.\n        redis_pool (Redis): The Redis connection pool.\n        jwt_access: The JWT access token generator.\n        max_jti (int): The maximum number of JTI values to store.\n\n    Returns:\n        dict[str, Any]: {'access_token': str, 'role': str, 'username': str}\n    \"\"\"\n    print(f\"db_user.__dict__ = {user.__dict__}\")\n\n    # Check if the user is in the cache\n    user_data = await get_user_data(redis_pool, user.username)\n\n    # If the user is not in the cache, check the database\n    if not user_data:\n        result = await db.execute(\n            select(User).where(User.username == user.username),\n        )\n        db_user = result.scalar()\n        if not db_user:\n            raise HTTPException(401, detail='Wrong username or password')\n\n        user_data = {\n            'db_user': {\n                'id': db_user.id,\n                'username': db_user.username,\n                'role': db_user.role,\n                'is_active': db_user.is_active,\n            },\n            'jti_list': [],\n        }\n    else:\n        db_user = user_data['db_user']\n\n    # Check if the user is active and has a valid role\n    result = await db.execute(\n        select(User).where(User.username == user.username),\n    )\n    real_db_user = result.scalar()\n    if (\n        not real_db_user\n        or not await real_db_user.check_password(user.password)\n    ):\n        raise HTTPException(401, detail='Wrong username or password')\n    if not real_db_user.is_active:\n        raise HTTPException(403, detail='User account is inactive')\n    if real_db_user.role not in ['admin', 'model_manager', 'user', 'guest']:\n        raise HTTPException(403, detail='User does not have the required role')\n\n    # Generate a new JTI value and update the JTI list\n    new_jti = str(uuid4())\n    jti_list = user_data['jti_list']\n    if len(jti_list) >= max_jti:\n        jti_list.pop(0)\n    jti_list.append(new_jti)\n\n    # Update the user data with the new JTI value\n    user_data['db_user']['role'] = real_db_user.role\n    user_data['db_user']['is_active'] = real_db_user.is_active\n\n    # Store the updated user data in the cache\n    await set_user_data(redis_pool, user.username, user_data)\n\n    # Generate a new access token for the user\n    access_token = jwt_access.create_access_token(\n        subject={\n            'username': real_db_user.username,\n            'role': real_db_user.role,\n            'jti': new_jti,\n        },\n    )\n\n    return {\n        'access_token': access_token,\n        'role': real_db_user.role,\n        'username': real_db_user.username,\n    }\n"}
{"type": "source_file", "path": "src/danger_detector.py", "content": "from __future__ import annotations\n\nfrom shapely.geometry import Polygon\nfrom sklearn.cluster import HDBSCAN\n\nfrom .utils import Utils\n\n\nclass DangerDetector:\n    \"\"\"\n    A class to detect potential safety hazards based on the detection data.\n    \"\"\"\n\n    def __init__(self, detection_items: dict[str, bool] = {}):\n        \"\"\"\n        Initialises the danger detector.\n\n        Args:\n            detection_items (Dict[str, bool]): A dictionary of detection items\n                to enable/disable specific safety checks. The keys are:\n                - 'detect_no_safety_vest_or_helmet': Detect if workers are not\n                  wearing hardhats or safety vests.\n                - 'detect_near_machinery_or_vehicle': Detect if workers are\n                  dangerously close to machinery or vehicles.\n                - 'detect_in_restricted_area': Detect if workers are entering\n                  restricted areas.\n\n        Raises:\n            ValueError: If the detection_items is not a dictionary or if any\n                of the keys are not strings or values are not booleans.\n\n        Examples:\n            >>> detector = DangerDetector({\n            ...     'detect_no_safety_vest_or_helmet': True,\n            ...     'detect_near_machinery_or_vehicle': True,\n            ...     'detect_in_restricted_area': True,\n            ... })\n        \"\"\"\n        # Initialise the HDBSCAN clusterer\n        self.clusterer = HDBSCAN(min_samples=3, min_cluster_size=2)\n\n        # Define required keys\n        required_keys = {\n            'detect_no_safety_vest_or_helmet',\n            'detect_near_machinery_or_vehicle',\n            'detect_in_restricted_area',\n        }\n\n        # Validate detection_items type and content\n        if isinstance(detection_items, dict) and all(\n            isinstance(k, str) and isinstance(v, bool)\n            for k, v in detection_items.items()\n        ) and required_keys.issubset(detection_items.keys()):\n            self.detection_items = detection_items\n        else:\n            self.detection_items = {}\n\n    def detect_danger(\n        self,\n        datas: list[list[float]],\n    ) -> tuple[list[str], list[Polygon]]:\n        \"\"\"\n        Detects potential safety violations in a construction site.\n\n        This function checks for two types of safety violations:\n        1. Workers entering the controlled area.\n        2. Workers not wearing hardhats or safety vests.\n        3. Workers dangerously close to machinery or vehicles.\n\n        Args:\n            datas (List[List[float]]): A list of detections which includes\n                bounding box coordinates, confidence score, and class label.\n\n        Returns:\n            Tuple[Set[str], List[Polygon]]: Warnings and polygons list.\n        \"\"\"\n        # Initialise the list to store warning messages\n        warnings: set[str] = set()\n\n        # Normalise data\n        datas = Utils.normalise_data(datas)\n\n        polygons: list[Polygon] = []\n\n        ############################################################\n        # Check if detection is enabled or no specific detection items are set\n        ############################################################\n        if (\n            not self.detection_items or\n            self.detection_items.get('detect_in_restricted_area', False)\n        ):\n            self.check_restricted_area(datas, warnings, polygons)\n\n        ############################################################\n        # Classify detected objects into different categories\n        ############################################################\n\n        # Persons\n        persons = [d for d in datas if d[5] == 5]\n\n        # No hardhat\n        hardhat_violations = [d for d in datas if d[5] == 2]\n\n        # No safety vest\n        safety_vest_violations = [\n            d for d in datas if d[5] == 4\n        ]\n\n        # Machinery and vehicles\n        machinery_vehicles = [\n            d for d in datas if d[5]\n            in [8, 9]\n        ]\n\n        # Filter out persons who are likely drivers\n        if machinery_vehicles:\n            non_drivers = [\n                p for p in persons if not any(\n                    Utils.is_driver(p[:4], mv[:4]) for mv in machinery_vehicles\n                )\n            ]\n            persons = non_drivers\n\n        ############################################################\n        # Check if people are not wearing hardhats or safety vests\n        ############################################################\n        if (\n            not self.detection_items or\n                self.detection_items.get(\n                    'detect_no_safety_vest_or_helmet', False,\n                )\n        ):\n            self.check_safety_violations(\n                persons, hardhat_violations,\n                safety_vest_violations, warnings,\n            )\n\n        ############################################################\n        # Check if people are dangerously close to machinery or vehicles\n        ############################################################\n        if (\n            not self.detection_items or\n            self.detection_items.get('detect_near_machinery_or_vehicle', False)\n        ):\n            self.check_proximity_violations(\n                persons, machinery_vehicles, warnings,\n            )\n\n        return list(warnings), polygons\n\n    def check_restricted_area(\n        self,\n        datas: list[list[float]],\n        warnings: set[str],\n        polygons: list[Polygon],\n    ) -> None:\n        \"\"\"\n        Check if people are entering the controlled area.\n\n        Args:\n            datas (List[List[float]]): A list of detections.\n            warnings (set[str]): A set to store warning messages.\n            polygons (list[Polygon]): A list to store detected polygons.\n        \"\"\"\n        polygons.extend(Utils.detect_polygon_from_cones(datas, self.clusterer))\n        people_count = Utils.calculate_people_in_controlled_area(\n            polygons, datas,\n        )\n        if people_count > 0:\n            warnings.add(\n                f\"Warning: {people_count} people have \"\n                'entered the controlled area!',\n            )\n\n    def check_safety_violations(\n        self, persons: list[list[float]],\n        hardhat_violations: list[list[float]],\n        safety_vest_violations: list[list[float]],\n        warnings: set[str],\n    ) -> None:\n        \"\"\"\n        Check for hardhat and safety vest violations.\n\n        Args:\n            persons (List[List[float]]): A list of person detections.\n            hardhat_violations (List[List[float]]):\n                A list of hardhat violations.\n            safety_vest_violations (List[List[float]]):\n                A list of safety vest violations.\n            warnings (set[str]): A set to store warning messages.\n        \"\"\"\n        for violation in hardhat_violations + safety_vest_violations:\n            label = 'NO-Hardhat' if violation[5] == 2 else 'NO-Safety Vest'\n            if not any(\n                Utils.overlap_percentage(violation[:4], p[:4]) > 0.5\n                for p in persons\n            ):\n                warning_msg = (\n                    'Warning: Someone is not wearing a hardhat!'\n                    if label == 'NO-Hardhat'\n                    else 'Warning: Someone is not wearing a safety vest!'\n                )\n                warnings.add(warning_msg)\n\n    def check_proximity_violations(\n        self, persons: list[list[float]],\n        machinery_vehicles: list[list[float]],\n        warnings: set[str],\n    ) -> None:\n        \"\"\"\n        Check if anyone is dangerously close to machinery or vehicles.\n\n        Args:\n            persons (List[List[float]]): A list of person detections.\n            machinery_vehicles (List[List[float]]): A list of machinery\n                and vehicle detections.\n            warnings (set[str]): A set to store warning messages.\n        \"\"\"\n        for person in persons:\n            for mv in machinery_vehicles:\n                label = 'machinery' if mv[5] == 8 else 'vehicle'\n                if Utils.is_dangerously_close(person[:4], mv[:4], label):\n                    warning_msg = (\n                        f\"Warning: Someone is too close to {label}!\"\n                    )\n                    warnings.add(warning_msg)\n                    break\n\n\ndef main() -> None:\n    \"\"\"\n    Main function to demonstrate the usage of the DangerDetector class.\n    \"\"\"\n    detector = DangerDetector()\n\n    data: list[list[float]] = [\n        [50, 50, 150, 150, 0.95, 0],    # Hardhat\n        [200, 200, 300, 300, 0.85, 5],  # Person\n        [400, 400, 500, 500, 0.75, 2],  # NO-Hardhat\n        [0, 0, 10, 10, 0.88, 6],  # Safety cone\n        [0, 1000, 10, 1010, 0.87, 6],  # Safety cone\n        [1000, 0, 1010, 10, 0.89, 6],  # Safety cone\n        [100, 100, 120, 120, 0.9, 6],  # Safety cone\n        [150, 150, 170, 170, 0.85, 6],  # Safety cone\n        [200, 200, 220, 220, 0.89, 6],  # Safety cone\n        [250, 250, 270, 270, 0.85, 6],  # Safety cone\n        [450, 450, 470, 470, 0.92, 6],  # Safety cone\n        [500, 500, 520, 520, 0.88, 6],  # Safety cone\n        [550, 550, 570, 570, 0.86, 6],  # Safety cone\n        [600, 600, 620, 620, 0.84, 6],  # Safety cone\n        [650, 650, 670, 670, 0.82, 6],  # Safety cone\n        [700, 700, 720, 720, 0.80, 6],  # Safety cone\n        [750, 750, 770, 770, 0.78, 6],  # Safety cone\n        [800, 800, 820, 820, 0.76, 6],  # Safety cone\n        [850, 850, 870, 870, 0.74, 6],  # Safety cone\n    ]\n\n    warnings, polygons = detector.detect_danger(data)\n    print(f\"Warnings: {warnings}\")\n    print(f\"Polygons: {polygons}\")\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "src/monitor_logger.py", "content": "from __future__ import annotations\n\nimport logging\nfrom logging.handlers import RotatingFileHandler\nfrom pathlib import Path\n\n\nclass LoggerConfig:\n    \"\"\"\n    Sets up app logger with console and file handlers.\n    \"\"\"\n\n    def __init__(\n        self,\n        log_file='monitor.log',\n        log_dir='logs',\n        level=logging.INFO,\n        formatter=None,\n    ):\n        \"\"\"\n        Initialise logger with file name, level, and formatter.\n\n        Args:\n            log_file (str): Log file name, defaults to 'monitor.log'.\n            log_dir (str): Log storage directory, defaults to 'logs'.\n            level (logging.Level): The logging level. Defaults to logging.INFO.\n            formatter (logging.Formatter): Log formatter, defaults to standard.\n        \"\"\"\n        self.log_file = log_file\n        self.log_dir = log_dir\n        self.level = level\n        self.formatter = formatter or logging.Formatter(\n            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n        )\n\n        # Ensure that we get a unique logger instance by using a unique name\n        self.logger = logging.getLogger(f\"SiteSafetyMonitor_{log_file}\")\n        self.setup_logger()\n\n    def setup_logger(self):\n        \"\"\"\n        Configures the logger with rotating file handler and console handler.\n        \"\"\"\n        # Create log directory if it doesn't exist\n        Path(self.log_dir).mkdir(parents=True, exist_ok=True)\n\n        # Prevent adding handlers multiple times\n        if self.logger.hasHandlers():\n            self.logger.handlers.clear()\n\n        # Configure the file handler\n        file_handler = self.get_file_handler()\n        # Configure the console handler\n        console_handler = self.get_console_handler()\n\n        # Add the handlers to the logger\n        self.logger.addHandler(file_handler)\n        self.logger.addHandler(console_handler)\n        self.logger.setLevel(self.level)\n\n        # Prevent log messages from propagating to the parent logger\n        self.logger.propagate = False\n\n        # Debug: Log to verify the handlers have been added\n        self.logger.debug('Logger handlers set up complete.')\n\n    def get_file_handler(self):\n        \"\"\"\n        Creates and returns a rotating file handler.\n\n        Returns:\n            logging.Handler: A configured rotating file handler.\n        \"\"\"\n        file_handler = RotatingFileHandler(\n            filename=Path(self.log_dir) / self.log_file,\n            maxBytes=1_000_000,\n            backupCount=5,\n        )\n        file_handler.setLevel(self.level)\n        file_handler.setFormatter(self.formatter)\n        return file_handler\n\n    def get_console_handler(self):\n        \"\"\"\n        Creates and returns a console handler.\n\n        Returns:\n            logging.Handler: A configured console handler.\n        \"\"\"\n        console_handler = logging.StreamHandler()\n        console_handler.setLevel(self.level)\n        console_handler.setFormatter(self.formatter)\n        return console_handler\n\n    def get_logger(self):\n        \"\"\"\n        Returns the configured logger instance.\n\n        Returns:\n            logging.Logger: A configured logger instance.\n        \"\"\"\n        return self.logger\n\n\ndef main():\n    \"\"\"\n    Main function to initialise logger and log a message.\n    \"\"\"\n    # Initialise the logger configuration\n    logger_config = LoggerConfig()\n    logger = logger_config.get_logger()\n\n    # Log a message indicating that the logging setup is complete\n    logger.info('Logging setup complete.')\n\n\n# This block is executed when the script is run directly, not when imported\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "main.py", "content": "from __future__ import annotations\n\nimport argparse\nimport asyncio\nimport gc\nimport json\nimport logging\nimport os\nimport time\nfrom datetime import datetime\nfrom multiprocessing import Manager\nfrom multiprocessing import Process\nfrom typing import TypedDict\n\nimport cv2\nfrom dotenv import load_dotenv\nfrom watchdog.observers import Observer\n\nfrom src.danger_detector import DangerDetector\nfrom src.drawing_manager import DrawingManager\nfrom src.live_stream_detection import LiveStreamDetector\nfrom src.monitor_logger import LoggerConfig\nfrom src.notifiers.line_notifier import LineNotifier\nfrom src.stream_capture import StreamCapture\nfrom src.utils import FileEventHandler\nfrom src.utils import RedisManager\nfrom src.utils import Utils\n\n# Load environment variables\nload_dotenv()\n\n\nclass AppConfig(TypedDict, total=False):\n    \"\"\"\n    Typed dictionary for the configuration of a video stream.\n    \"\"\"\n    video_url: str\n    model_key: str\n    site: str | None\n    stream_name: str\n    notifications: dict[str, str] | None\n    detect_with_server: bool\n    expire_date: str | None\n    line_token: str | None\n    language: str | None\n    detection_items: dict[str, bool] | None\n    work_start_hour: int | None\n    work_end_hour: int | None\n    store_in_redis: bool\n\n\nclass MainApp:\n    \"\"\"\n    Main application class for managing multiple video streams.\n    \"\"\"\n\n    def __init__(self, config_file: str):\n        \"\"\"\n        Initialise the MainApp class.\n\n        Args:\n            config_file (str): The path to the JSON configuration file.\n        \"\"\"\n        self.config_file = config_file\n        self.running_processes: dict[str, dict] = {}\n        self.current_config_hashes: dict[str, str] = {}\n        self.lock = asyncio.Lock()\n        self.logger = LoggerConfig().get_logger()\n\n        manager = Manager()\n        # Build shared token for API access\n        self.shared_token = manager.dict()\n        self.shared_token['access_token'] = None\n        self.shared_token['token_expiry'] = 0\n\n        # Build shared lock for API access\n        self.shared_lock = manager.Lock()\n\n    def compute_config_hash(self, config: dict) -> str:\n        \"\"\"\n        Compute a hash based on relevant configuration parameters.\n\n        Args:\n            config (dict): The configuration dictionary for a video stream.\n\n        Returns:\n            str: A hash representing the configuration.\n        \"\"\"\n        relevant_config = {\n            'video_url': config['video_url'],\n            'model_key': config['model_key'],\n            'site': config['site'],\n            'stream_name': config.get('stream_name', 'prediction_visual'),\n            'notifications': config['notifications'],\n            'detect_with_server': config['detect_with_server'],\n            'expire_date': config.get('expire_date'),\n            'language': config.get('language'),\n            'detection_items': config.get('detection_items'),\n            'work_start_hour': config.get('work_start_hour'),\n            'work_end_hour': config.get('work_end_hour'),\n            'store_in_redis': config.get('store_in_redis', False),\n        }\n        return str(relevant_config)  # Convert to string for hashing\n\n    async def reload_configurations(self):\n        async with self.lock:\n            redis_manager = RedisManager()\n\n            self.logger.info('Reloading configurations...')\n            with open(self.config_file, encoding='utf-8') as file:\n                configurations = json.load(file)\n\n            current_configs = {\n                config['video_url']: config for config in configurations\n            }\n\n            # Track keys that exist in the current config\n            current_keys = {\n                (\n                    f\"{config['site']}_\"\n                    f\"{config.get('stream_name', 'prediction_visual')}\"\n                )\n                for config in configurations\n            }\n\n            # Stop processes for removed or updated configurations\n            for video_url in list(self.running_processes.keys()):\n                config_data = self.running_processes[video_url]\n                config = current_configs.get(video_url)\n\n                # Get the key to be deleted\n                # if config has been removed or modified\n                site = config_data['config']['site']\n                stream_name = config_data['config'].get(\n                    'stream_name', 'prediction_visual',\n                )\n\n                site = Utils.encode(site)\n                stream_name = Utils.encode(stream_name)\n\n                key_to_delete = f\"stream_frame:{site}|{stream_name}\"\n\n                # Stop the process if the configuration is removed\n                if not config or Utils.is_expired(config.get('expire_date')):\n                    self.logger.info(f\"Stop workflow: {video_url}\")\n                    self.stop_process(config_data['process'])\n                    del self.running_processes[video_url]\n                    del self.current_config_hashes[video_url]\n\n                    # Delete old key in Redis\n                    # if it no longer exists in the config\n                    if key_to_delete not in current_keys:\n                        await redis_manager.delete(key_to_delete)\n                        self.logger.info(f\"Deleted Redis key: {key_to_delete}\")\n\n                # Restart the process if the configuration is updated\n                elif self.compute_config_hash(config) != (\n                    self.current_config_hashes.get(\n                        video_url,\n                    )\n                ):\n                    self.logger.info(\n                        f\"Config changed for {video_url}. \"\n                        'Restarting workflow.',\n                    )\n                    self.stop_process(config_data['process'])\n\n                    # Delete old key in Redis\n                    # if it no longer exists in the config\n                    if key_to_delete not in current_keys:\n                        await redis_manager.delete(key_to_delete)\n                        self.logger.info(f\"Deleted Redis key: {key_to_delete}\")\n\n                    # Start the new process\n                    self.running_processes[video_url] = {\n                        'process': self.start_process(config),\n                        'config': config,\n                    }\n                    self.current_config_hashes[video_url] = (\n                        self.compute_config_hash(\n                            config,\n                        )\n                    )\n\n            # Start processes for new configurations\n            for video_url, config in current_configs.items():\n                if Utils.is_expired(config.get('expire_date')):\n                    self.logger.info(\n                        f\"Skip expired configuration: {video_url}\",\n                    )\n                    continue\n\n                if video_url not in self.running_processes:\n                    self.logger.info(f\"Launch new workflow: {video_url}\")\n                    self.running_processes[video_url] = {\n                        'process': self.start_process(config),\n                        'config': config,\n                    }\n                    self.current_config_hashes[video_url] = (\n                        self.compute_config_hash(\n                            config,\n                        )\n                    )\n\n            # Close Redis connection\n            await redis_manager.close_connection()\n\n    async def run_multiple_streams(self) -> None:\n        \"\"\"\n        Manage multiple video streams based on a config file.\n        \"\"\"\n        # Initial load of configurations\n        await self.reload_configurations()\n\n        # Get the absolute path of the configuration file\n        config_file_path = os.path.abspath(self.config_file)\n        # Get the directory of the configuration file\n        config_dir = os.path.dirname(config_file_path)\n        # Get the current event loop\n        loop = asyncio.get_running_loop()\n\n        # Set up watchdog observer\n        event_handler = FileEventHandler(\n            config_file_path, self.reload_configurations, loop,\n        )\n        observer = Observer()\n        observer.schedule(event_handler, path=config_dir, recursive=False)\n        observer.start()\n\n        try:\n            while True:\n                await asyncio.sleep(1)\n        except KeyboardInterrupt:\n            self.logger.info(\n                '\\n[INFO] Received KeyboardInterrupt. Stopping observer...',\n            )\n            observer.stop()\n        finally:\n            observer.join()  # Wait for the observer to finish\n            self.logger.info('[INFO] Observer stopped.')\n            # Stop all running processes\n            for video_url, data in self.running_processes.items():\n                self.stop_process(data['process'])\n            self.running_processes.clear()\n            self.logger.info('[INFO] All processes stopped.')\n\n    async def process_single_stream(\n        self,\n        logger: logging.Logger,\n        video_url: str,\n        model_key: str = 'yolo11n',\n        site: str | None = None,\n        stream_name: str = 'prediction_visual',\n        notifications: dict[str, str] | None = None,\n        detect_with_server: bool = False,\n        detection_items: dict[str, bool] | None = {},\n        work_start_hour: int = 7,\n        work_end_hour: int = 18,\n        store_in_redis: bool = False,\n    ) -> None:\n        \"\"\"\n        Process a single video stream with hazard detection, notifications,\n        and Redis storage.\n\n        Args:\n            logger (logging.Logger): Logger instance for logging.\n            video_url (str): Video stream URL.\n            model_key (str): Detection model key.\n            site (str): Site name for the stream.\n            stream_name (str): Stream name for notifications.\n            notifications (dict): Line tokens with their languages.\n            detect_with_server (bool): Whether to use server for detection.\n            detection_items (dict): Items to detect.\n            work_start_hour (int): Start hour for notifications.\n            work_end_hour (int): End hour for notifications.\n            store_in_redis (bool): Whether to store frames in Redis.\n        \"\"\"\n        if store_in_redis:\n            redis_manager = RedisManager()\n\n        # Initialise the stream capture object\n        streaming_capture = StreamCapture(stream_url=video_url)\n\n        # Initialise the live stream detector\n        live_stream_detector = LiveStreamDetector(\n            api_url=os.getenv('API_URL', 'http://localhost:5000'),\n            model_key=model_key,\n            output_folder=site,\n            detect_with_server=detect_with_server,\n            # Pass shared token and lock for API access\n            shared_token=self.shared_token,\n            # Pass shared lock for API access\n            shared_lock=self.shared_lock,\n        )\n\n        # Initialise the drawing manager\n        drawing_manager = DrawingManager()\n\n        # Initialise the LINE notifier\n        line_notifier = LineNotifier()\n\n        # Initialise the DangerDetector\n        danger_detector = DangerDetector(detection_items or {})\n\n        # Notifications setup\n        last_notification_times = {\n            token: int(\n                time.time(),\n            ) - 300 for token in (notifications or {})\n        }\n\n        # Get the language for Redis storage\n        redis_storage_language = (\n            list(notifications.values())[-1] if notifications else 'en'\n        )\n\n        # Use the generator function to process detections\n        async for frame, timestamp in streaming_capture.execute_capture():\n            timestamp = int(timestamp)\n            start_time = time.time()\n\n            # Convert UNIX timestamp to datetime object and format it as string\n            detection_time = datetime.fromtimestamp(timestamp)\n\n            # Check if the current time is within working hours\n            is_working_hour = (\n                work_start_hour <= detection_time.hour < work_end_hour\n            )\n\n            # Detection step\n            datas, _ = await live_stream_detector.generate_detections(frame)\n            warnings, controlled_zone_polygon = danger_detector.detect_danger(\n                datas,\n            )\n            controlled_zone_warning = [\n                w for w in warnings if 'controlled area' in w\n            ]\n\n            # Notification step\n            for token, lang in (notifications or {}).items():\n                # Check if it is time to send a notification\n                if not Utils.should_notify(\n                    timestamp,\n                    last_notification_times[token],\n                ):\n                    continue\n\n                # Generate the notification message\n                message = Utils.generate_message(\n                    stream_name,\n                    detection_time,\n                    warnings,\n                    controlled_zone_warning,\n                    lang,\n                    is_working_hour,\n                )\n                if not message:\n                    continue\n\n                # Draw detections for LINE notification\n                frame_with_detections = (\n                    drawing_manager.draw_detections_on_frame(\n                        frame, controlled_zone_polygon, datas, language=lang,\n                    )\n                )\n                frame_bytes = Utils.encode_frame(frame_with_detections)\n\n                # Send the notification\n                status = await line_notifier.send_notification(\n                    message,\n                    image=frame_bytes\n                    if frame_bytes is not None\n                    else None,\n                    line_token=token,\n                )\n                if status == 200:\n                    logger.info(f\"Notification sent: {message}\")\n                    last_notification_times[token] = timestamp\n\n            # Draw detections for Redis storage\n            frame_with_detections = drawing_manager.draw_detections_on_frame(\n                frame,\n                controlled_zone_polygon,\n                datas,\n                language=redis_storage_language or 'en',\n            )\n            frame_bytes = Utils.encode_frame(frame_with_detections)\n\n            # Store the frame bytes to Redis\n            if store_in_redis:\n                await redis_manager.store_to_redis(\n                    site=site or 'default',\n                    stream_name=stream_name,\n                    frame_bytes=frame_bytes,\n                    warnings=warnings,\n                    language=redis_storage_language,\n                )\n\n            # Update the capture interval based on processing time\n            processing_time = time.time() - start_time\n            streaming_capture.update_capture_interval(\n                max(1, int(processing_time) + 1),\n            )\n\n            # Log the detection results\n            logger.info(\n                f\"Processed {site}-{stream_name} in {processing_time:.2f}s\",\n            )\n\n        # Release resources after processing\n        await streaming_capture.release_resources()\n\n        # Close the Redis connection\n        if store_in_redis:\n            await redis_manager.close_connection()\n\n        gc.collect()\n\n    async def process_streams(self, config: AppConfig) -> None:\n        \"\"\"\n        Process a video stream based on the given configuration.\n\n        Args:\n            config (AppConfig): The configuration for the stream processing.\n        \"\"\"\n        try:\n            # Check if 'notifications' field exists (new format)\n            if (\n                'notifications' in config and\n                config['notifications'] is not None\n            ):\n                notifications = config['notifications']\n            # Otherwise, handle the old format\n            elif 'line_token' in config and 'language' in config:\n                line_token = config.get('line_token')\n                language = config.get('language')\n                if line_token is not None and language is not None:\n                    notifications = {line_token: language}\n                else:\n                    notifications = None\n            else:\n                notifications = None\n\n            # Continue processing the remaining configuration\n            video_url = config.get('video_url', '')\n            model_key = config.get('model_key', 'yolo11n')\n            site = config.get('site')\n            stream_name = config.get('stream_name', 'prediction_visual')\n            detect_with_server = config.get('detect_with_server', False)\n            detection_items = config.get('detection_items', None)\n            store_in_redis = config.get('store_in_redis', False)\n            work_start_hour = config.get('work_start_hour', 7)\n            work_end_hour = config.get('work_end_hour', 18)\n\n            # Run hazard detection on a single video stream\n            await self.process_single_stream(\n                self.logger,\n                video_url=video_url,\n                model_key=model_key,\n                site=site,\n                stream_name=stream_name,\n                notifications=notifications,\n                detect_with_server=detect_with_server,\n                detection_items=detection_items,\n                work_start_hour=work_start_hour or 7,\n                work_end_hour=work_end_hour or 18,\n                store_in_redis=store_in_redis,\n            )\n        finally:\n            # Clean up Redis storage if needed\n            if config.get('store_in_redis', False):\n                redis_manager = RedisManager()\n                site = config.get('site') or 'default site'\n                stream_name = config.get(\n                    'stream_name',\n                ) or 'default stream name'\n\n                site = Utils.encode(site)\n                stream_name = Utils.encode(stream_name)\n\n                key = f\"stream_frame:{site}|{stream_name}\"\n                await redis_manager.delete(key)\n                self.logger.info(f\"Deleted Redis key: {key}\")\n\n                # Close the Redis connection\n                await redis_manager.close_connection()\n\n    def start_process(self, config: AppConfig) -> Process:\n        \"\"\"\n        Start a new process for processing a video stream.\n\n        Args:\n            config (AppConfig): The configuration for the stream processing.\n\n        Returns:\n            Process: The newly started process.\n        \"\"\"\n        p = Process(\n            target=lambda: asyncio.run(self.process_streams(config)),\n        )\n        p.start()\n        return p\n\n    def stop_process(self, process: Process) -> None:\n        \"\"\"\n        Stop a running process.\n\n        Args:\n            process (Process): The process to be terminated.\n        \"\"\"\n        process.terminate()\n        process.join()\n\n\nasync def process_single_image(\n    image_path: str,\n    model_key: str = 'yolo11n',\n    output_folder: str = 'output_images',\n    stream_name: str | None = None,\n    language: str = 'en',\n) -> None:\n    \"\"\"\n    Process a single image for hazard detection and save the result.\n\n    Args:\n        image_path (str): The path to the image file.\n        model_key (str): The model key to use for detection.\n        output_folder (str): The folder to save the output image.\n        stream_name (str): The name of the output image file.\n        language (str): The language for labels on the output image.\n\n    Returns:\n        None\n    \"\"\"\n    try:\n        # Check if the image path exists\n        if not os.path.exists(image_path):\n            print(f\"Error: Image path {image_path} does not exist.\")\n            return\n\n        # Load the image using OpenCV\n        image = cv2.imread(image_path)\n\n        if image is None:\n            print(f\"Error: Failed to load image {image_path}\")\n            return\n\n        # Initialise the live stream detector,\n        # but here used for a single image\n        live_stream_detector = LiveStreamDetector(\n            api_url=os.getenv('API_URL', 'http://localhost:5000'),\n            model_key=model_key,\n            output_folder=output_folder,\n            # Shared token not needed for single image processing\n            shared_token={},\n        )\n\n        # Initialise the drawing manager\n        drawing_manager = DrawingManager()\n\n        # Detect hazards in the image\n        detections, _ = await live_stream_detector.generate_detections(image)\n\n        # For this example, no polygons are needed, so pass an empty list\n        frame_with_detections = drawing_manager.draw_detections_on_frame(\n            image, [], detections,\n            language=language,\n        )\n\n        # Ensure the output folder exists\n        if not os.path.exists(output_folder):\n            os.makedirs(output_folder)\n\n        # Generate the output file name if not provided\n        if not stream_name:\n            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n            stream_name = f\"detection_{timestamp}.png\"\n        output_path = os.path.join(output_folder, stream_name)\n\n        # Save the image with detections\n        cv2.imwrite(output_path, frame_with_detections)\n        print(f\"Processed image saved to {output_path}\")\n\n    except Exception as e:\n        print(f\"Error processing the image: {str(e)}\")\n\n\nasync def main():\n    parser = argparse.ArgumentParser(\n        description=(\n            'Run hazard detection on multiple video streams or a single image.'\n        ),\n    )\n    parser.add_argument(\n        '--config',\n        type=str,\n        default='config/configuration.json',\n        help='Configuration file path for stream processing',\n    )\n    parser.add_argument(\n        '--image',\n        type=str,\n        help='Path to a single image for detection',\n    )\n    parser.add_argument(\n        '--model_key',\n        type=str,\n        default='yolo11n',\n        help='Model key to use for detection',\n    )\n    parser.add_argument(\n        '--output_folder',\n        type=str,\n        default='output_images',\n        help='Folder to save the output image',\n    )\n    parser.add_argument(\n        '--language',\n        type=str,\n        default='en',\n        help='Language for labels on the output image',\n    )\n    args = parser.parse_args()\n\n    try:\n        if args.image:\n            # If an image path is provided, process the single image\n            await process_single_image(\n                image_path=args.image,\n                model_key=args.model_key,\n                output_folder=args.output_folder,\n                language=args.language,\n            )\n        else:\n            # Otherwise, run hazard detection on multiple video streams\n            app = MainApp(args.config)\n            await app.run_multiple_streams()\n    except KeyboardInterrupt:\n        print('\\n[INFO] Received KeyboardInterrupt. Shutting down...')\n    finally:\n        # Perform necessary cleanup if needed\n        # if store_in_redis:\n        #     await redis_manager.close_connection()\n        #     print('[INFO] Redis connection closed.')\n        print('[INFO] Application stopped.')\n        # Clear the asyncio event loop\n        await asyncio.sleep(0)\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "src/drawing_manager.py", "content": "from __future__ import annotations\n\nfrom pathlib import Path\n\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom PIL import ImageDraw\nfrom PIL import ImageFont\nfrom shapely.geometry import Polygon\n\nfrom src.lang_config import LANGUAGES\n\n\nclass DrawingManager:\n    \"\"\"\n    A class for drawing detections on frames and saving them to disk.\n    \"\"\"\n\n    # Class variable for caching default font\n    default_font: ImageFont.FreeTypeFont | ImageFont.ImageFont | None = None\n\n    def __init__(self) -> None:\n        \"\"\"\n        Initialise the DrawingManager class.\n        \"\"\"\n        # Font cache to avoid repeated loading\n        self.font_cache: dict[\n            str, ImageFont.FreeTypeFont |\n            ImageFont.ImageFont,\n        ] = {}\n\n        # Load default font if not already loaded\n        if DrawingManager.default_font is None:\n            DrawingManager.default_font = ImageFont.load_default()\n\n    def get_font(\n        self, language: str,\n    ) -> ImageFont.FreeTypeFont | ImageFont.ImageFont:\n        \"\"\"\n        Load the appropriate font based on the language input, with caching.\n\n        Args:\n            language (str): The language to use for the font.\n\n        Returns:\n            ImageFont.FreeTypeFont | ImageFont.ImageFont:\n            The loaded font object.\n        \"\"\"\n        # Select font path based on language\n        if language == 'th':\n            font_path = 'assets/fonts/NotoSansThai-VariableFont_wdth.ttf'\n        else:\n            font_path = 'assets/fonts/NotoSansTC-VariableFont_wght.ttf'\n\n        # Check if the font is already in the cache\n        if font_path in self.font_cache:\n            return self.font_cache[font_path]\n\n        # Load and cache the font\n        try:\n            font = ImageFont.truetype(font_path, 20)\n        except OSError:\n            print(f\"Error loading font from {font_path}. Using default font.\")\n            if DrawingManager.default_font is None:\n                # Load default font only once\n                DrawingManager.default_font = ImageFont.load_default()\n            return DrawingManager.default_font\n\n        # Cache the font using the path as the key\n        self.font_cache[font_path] = font\n        return font\n\n    def draw_polygons(\n        self,\n        frame: np.ndarray,\n        polygons: list[Polygon],\n    ) -> np.ndarray:\n        \"\"\"\n        Draws polygons on the given frame.\n\n        Args:\n            frame (np.ndarray): The frame on which to draw polygons.\n            polygons (List[Polygon]): list of polygons containing safety cones.\n\n        Returns:\n            np.ndarray: The frame with polygons drawn.\n        \"\"\"\n        # Convert frame to PIL image\n        frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n\n        # Create a transparent layer\n        overlay = Image.new('RGBA', frame_pil.size, (255, 0, 0, 0))\n        overlay_draw = ImageDraw.Draw(overlay)\n\n        # Draw the polygons\n        for polygon in polygons:\n            # Get polygon points\n            points = polygon.exterior.coords if isinstance(\n                polygon, Polygon,\n            ) else polygon.coords\n            points = [\n                tuple(point)\n                for point in points\n            ]  # Convert points to tuples\n\n            # Draw the polygon or line\n            overlay_draw.polygon(points, fill=(255, 105, 180, 128))\n\n            # Draw the polygon border\n            overlay_draw.line(\n                points + [points[0]],\n                fill=(255, 0, 255), width=2,\n            )\n\n        # Composite the overlay with the original image\n        frame_pil = Image.alpha_composite(frame_pil.convert('RGBA'), overlay)\n\n        # Convert back to OpenCV image\n        frame = cv2.cvtColor(\n            np.array(frame_pil.convert('RGB')), cv2.COLOR_RGB2BGR,\n        )\n\n        return frame\n\n    def draw_detections_on_frame(\n        self,\n        frame: np.ndarray,\n        polygons: list[Polygon],\n        datas: list[list[float]],\n        language: str = 'en',  # Accept language as input\n    ) -> np.ndarray:\n        \"\"\"\n        Draws detections on the given frame\n        and supports dynamic language selection.\n\n        Args:\n            frame (np.ndarray): The frame on which to draw detections.\n            datas (List[List[float]]): The detection data.\n            language (str): The language to use for labels.\n\n        Returns:\n            np.ndarray: The frame with detections drawn.\n        \"\"\"\n        # Load language configuration\n        lang_config = LANGUAGES.get(language, LANGUAGES['en'])\n\n        # Define category names based on language\n        category_id_to_name: dict[int, str] = {\n            0: lang_config['helmet'],\n            1: lang_config['mask'],\n            2: lang_config['no_helmet'],\n            3: lang_config['no_mask'],\n            4: lang_config['no_vest'],\n            5: lang_config['person'],\n            6: lang_config['cone'],\n            7: lang_config['vest'],\n            8: lang_config['machinery'],\n            9: lang_config['vehicle'],\n        }\n\n        # Define colours for each category\n        colours: dict[str, tuple[int, int, int]] = {\n            lang_config['helmet']: (0, 255, 0),\n            lang_config['vest']: (0, 255, 0),\n            lang_config['machinery']: (255, 225, 0),\n            lang_config['vehicle']: (255, 255, 0),\n            lang_config['no_helmet']: (255, 0, 0),\n            lang_config['no_vest']: (255, 0, 0),\n            lang_config['person']: (255, 165, 0),\n        }\n\n        # Load the font based on the input language\n        font = self.get_font(language)\n\n        # Draw polygons first\n        if polygons:\n            frame = self.draw_polygons(frame, polygons)\n\n        # Convert the frame to RGB and create a PIL image\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        pil_image = Image.fromarray(frame_rgb)\n        draw = ImageDraw.Draw(pil_image)\n\n        # Draw the detections on the frame\n        for data in datas:\n            x1, y1, x2, y2, _, label_id = data\n            label_id = int(label_id)\n            if label_id in category_id_to_name:\n                label = category_id_to_name[label_id]\n            else:\n                continue\n\n            # Draw the bounding box\n            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n            if label in colours:\n                colour = colours.get(label, (255, 255, 255))\n                draw.rectangle((x1, y1, x2, y2), outline=colour, width=2)\n                text = f\"{label}\"\n                text_bbox = draw.textbbox((x1, y1), text, font=font)\n                text_width, text_height = text_bbox[2] - \\\n                    text_bbox[0], text_bbox[3] - text_bbox[1]\n                text_background = (\n                    x1, y1 - text_height -\n                    5, x1 + text_width, y1,\n                )\n                draw.rectangle(text_background, fill=colour)\n                draw.text(\n                    (x1, y1 - text_height - 5), text,\n                    fill=(0, 0, 0), font=font,\n                )\n\n        # Convert the PIL image back to OpenCV format\n        frame_with_detections = cv2.cvtColor(\n            np.array(pil_image), cv2.COLOR_RGB2BGR,\n        )\n\n        return frame_with_detections\n\n    def save_frame(self, frame_bytes: bytearray, output_filename: str) -> None:\n        \"\"\"\n        Saves detected frame to given output folder and filename.\n\n        Args:\n            frame_bytes (bytearray): The byte stream of the frame.\n            output_filename (str): The output filename.\n        \"\"\"\n        # Create the output directory if it does not exist\n        output_dir = Path('detected_frames')\n        output_dir.mkdir(parents=True, exist_ok=True)\n\n        # Define the output path\n        output_path = output_dir / f\"{output_filename}.png\"\n\n        # Save the byte stream to the output path\n        with open(output_path, 'wb') as f:\n            f.write(frame_bytes)\n\n\ndef main() -> None:\n    \"\"\"\n    Main function to process and save the frame with detections.\n    \"\"\"\n    drawer_saver = DrawingManager()\n\n    # Load frame and detection data (example)\n    frame = np.zeros((480, 640, 3), dtype=np.uint8)\n\n    # Example data including objects and safety cones\n    datas = [\n        # Example objects (Hardhat, Person, Vehicle)\n        [50, 50, 150, 150, 0.95, 0],   # Hardhat\n        [200, 200, 300, 300, 0.85, 5],  # Person\n        [400, 400, 500, 500, 0.75, 9],  # Vehicle\n\n        # Example safety cones (Safety Cone)\n        [100, 100, 120, 120, 0.9, 6],\n        [250, 250, 270, 270, 0.8, 6],\n        [450, 450, 470, 470, 0.7, 6],\n        [500, 200, 520, 220, 0.7, 6],\n        [150, 400, 170, 420, 0.7, 6],\n    ]\n\n    # Define the points directly\n    points = [(100, 100), (250, 250), (450, 450), (500, 200), (150, 400)]\n    polygon = Polygon(points).convex_hull\n\n    # Draw detections on frame (including safety cones)\n    frame_with_detections = drawer_saver.draw_detections_on_frame(\n        frame, [polygon], datas, language='en',\n    )\n\n    # Save the frame with detections\n    output_filename = 'frame_001'\n    frame_bytes = cv2.imencode('.png', frame_with_detections)[1].tobytes()\n    drawer_saver.save_frame(bytearray(frame_bytes), output_filename)\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "examples/streaming_web/backend/__init__.py", "content": ""}
{"type": "source_file", "path": "src/live_stream_tracker.py", "content": "from __future__ import annotations\n\nimport argparse\nimport datetime\nfrom collections.abc import Generator\nfrom typing import TypedDict\n\nimport cv2\nimport numpy as np\nfrom ultralytics import YOLO\n\n\nclass DetectionResult(TypedDict):\n    ids: list[int]\n    data: list[list[float]]\n    frame: cv2.Mat | np.ndarray\n    timestamp: float\n\n\nclass LiveStreamDetector:\n    \"\"\"\n    A class to perform live stream detection and tracking using YOLO11.\n    \"\"\"\n\n    def __init__(\n        self,\n        stream_url: str,\n        model_path: str = '../models/pt/best_yolo11n.pt',\n    ):\n        \"\"\"\n        Initialise live stream detector with video URL, YOLO model path.\n\n        Args:\n            stream_url (str): The full URL to the live video stream.\n            model_path (str): The path to the YOLO11 model file.\n        \"\"\"\n        self.stream_url = stream_url\n        self.model_path = model_path\n        self.model = YOLO(self.model_path)\n        self.cap = cv2.VideoCapture(self.stream_url)\n\n    def generate_detections(\n        self,\n    ) -> Generator[\n        tuple[list[int], list[list[float]], cv2.Mat | np.ndarray, float],\n    ]:\n        \"\"\"\n        Yields detection results, timestamp per frame from video capture.\n\n        Yields:\n            Generator[Tuple]: Tuple of detection ids, detection data, frame,\n            and the current timestamp for each video frame.\n        \"\"\"\n        while self.cap.isOpened():\n            success, frame = self.cap.read()\n            if not success:\n                break  # Exit if there is an issue with video capture\n\n            # Get the current timestamp\n            now = datetime.datetime.now()\n            timestamp = now.timestamp()  # Unix timestamp\n\n            # Run YOLO11 tracking, maintain tracks across frames\n            results = self.model.track(source=frame, persist=True)\n\n            # If detections exist, extract IDs and data\n            if results[0].boxes is not None and len(results[0].boxes) > 0:\n                # Check emptiness, not converting to list\n                ids = results[0].boxes.id\n                datas = results[0].boxes.data  # Same as above\n\n                # Convert ids and datas to lists if they are not empty\n                ids_list = (\n                    ids.cpu().numpy().tolist()\n                    if ids is not None and len(ids) > 0\n                    else []\n                )\n                datas_list = (\n                    datas.cpu().numpy().tolist()\n                    if datas is not None and len(datas) > 0\n                    else []\n                )\n\n                # Yield the results\n                yield ids_list, datas_list, frame, timestamp\n            else:\n                yield [], [], frame, timestamp\n\n            # Exit loop if 'q' is pressed\n            if cv2.waitKey(1) & 0xFF == ord('q'):\n                break\n\n    def release_resources(self):\n        \"\"\"\n        Releases the video capture object and closes all OpenCV windows.\n        \"\"\"\n        self.cap.release()\n        cv2.destroyAllWindows()\n\n    def run_detection(self):\n        \"\"\"\n        Runs the live stream detection and prints out detection results.\n        \"\"\"\n        for ids, datas, frame, timestamp in self.generate_detections():\n            print(\n                'Timestamp:', datetime.datetime.fromtimestamp(\n                    timestamp, tz=datetime.timezone.utc,\n                ).strftime('%Y-%m-%d %H:%M:%S'),\n            )\n            print('IDs:', ids)\n            print('Data (xyxy format):')\n            print(datas)\n\n\ndef main():\n    \"\"\"\n    Main function to run the live stream detection.\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description='Perform live stream detection and tracking using YOLO11.',\n    )\n    parser.add_argument(\n        '--url',\n        type=str,\n        help='Live stream URL',\n        required=True,\n    )\n    parser.add_argument(\n        '--model',\n        type=str,\n        default='../models/yolo11n.pt',\n        help='Path to the YOLO model',\n    )\n    args = parser.parse_args()\n\n    # Initialize the detector with the stream URL and model path\n    detector = LiveStreamDetector(args.url, args.model)\n\n    # Run the detection and print the results\n    detector.run_detection()\n\n    # Release resources after detection is complete\n    detector.release_resources()\n\n\nif __name__ == '__main__':\n    main()\n\n\"\"\"example\npython live_stream_tracker.py --url https://cctv6.kctmc.nat.gov.tw/ea05668e/\n\"\"\"\n"}
{"type": "source_file", "path": "examples/streaming_web/backend/sockets.py", "content": "from __future__ import annotations\n\nfrom typing import Any\n\nimport socketio\n\n\ndef register_sockets(sio: socketio.AsyncServer, redis_manager: Any) -> None:\n    \"\"\"\n    Registers Socket.IO event handlers\n    for connection, disconnection, and errors.\n\n    Args:\n        sio (socketio.AsyncServer): The Socket.IO server instance.\n        redis_manager (Any): The RedisManager instance for Redis operations.\n    \"\"\"\n\n    @sio.event\n    async def connect(sid: str, environ: dict) -> None:\n        \"\"\"\n        Handles new client connections by emitting a welcome message\n        and starting the background task for image updates.\n\n        Args:\n            sid (str): The session ID of the connected client.\n            environ (dict): The environment dictionary for the connection.\n        \"\"\"\n        await sio.emit('message', {'data': 'Connected'}, room=sid)\n        sio.start_background_task(update_images, sio, redis_manager)\n\n    @sio.event\n    async def disconnect(sid: str) -> None:\n        \"\"\"\n        Handles client disconnections by logging the event.\n\n        Args:\n            sid (str): The session ID of the disconnected client.\n        \"\"\"\n        print('Client disconnected')\n\n    @sio.event\n    async def error(sid: str, e: Exception) -> None:\n        \"\"\"\n        Handles errors by logging the exception details.\n\n        Args:\n            sid (str): The session ID of the client experiencing the error.\n            e (Exception): The exception raised during the event.\n        \"\"\"\n        print(f\"Error: {str(e)}\")\n\n\nasync def update_images(sio: socketio.AsyncServer, redis_manager: Any) -> None:\n    \"\"\"\n    Background task to update images for all labels at a set interval. Emits\n    updated images to connected clients for each label.\n\n    Args:\n        sio (socketio.AsyncServer): The Socket.IO server instance.\n        redis_manager (Any): The RedisManager instance to interact with Redis.\n    \"\"\"\n    while True:\n        try:\n            # Waits for 1 seconds before updating images\n            await sio.sleep(1)\n\n            # Fetches all labels from Redis\n            labels = await redis_manager.get_labels()\n\n            for label in labels:\n                # Fetches image data for each label\n                image_data = await redis_manager.fetch_latest_frames(label)\n\n                # Emits updated image data to all clients\n                await sio.emit(\n                    'update',\n                    {\n                        'label': label,\n                        'images': [img for img, _ in image_data],\n                        'image_names': [name for _, name in image_data],\n                    },\n                )\n        except Exception as e:\n            # Logs any exceptions that occur during the update process\n            print(f\"Error updating images: {str(e)}\")\n            break\n"}
{"type": "source_file", "path": "examples/YOLO_server_api/backend/model_files.py", "content": "from __future__ import annotations\n\nimport datetime\nfrom pathlib import Path\n\nimport torch\n\n\nasync def update_model_file(model: str, model_file: Path) -> None:\n    \"\"\"\n    Update the model file for a specified model.\n\n    Args:\n        model (str): The model key (e.g., 'yolo11n', 'yolo11s').\n        model_file (Path): The path to the new `.pt` model file.\n    \"\"\"\n    # Define valid models and their corresponding filenames\n    valid_models = {\n        'yolo11n': 'best_yolo11n.pt',\n        'yolo11s': 'best_yolo11s.pt',\n        'yolo11m': 'best_yolo11m.pt',\n        'yolo11l': 'best_yolo11l.pt',\n        'yolo11x': 'best_yolo11x.pt',\n    }\n    if model not in valid_models:\n        raise ValueError(\n            f\"Invalid model key: {model}. \"\n            f\"Must be one of {list(valid_models.keys())}.\",\n        )\n\n    if not model_file.is_file() or model_file.suffix != '.pt':\n        raise ValueError(\n            f\"Invalid file: {model_file}. Must be a valid `.pt` file.\",\n        )\n\n    try:\n        torch.jit.load(str(model_file))\n    except Exception as e:\n        raise ValueError(f\"Invalid PyTorch model file: {e}\")\n\n    # Use a base directory and construct the destination path safely\n    base_dir = Path('models/pt').resolve()\n    destination_filename = valid_models[model]\n    destination_path = base_dir / destination_filename\n\n    # Ensure the destination path is within the base directory\n    destination_path = destination_path.resolve()\n    if not str(destination_path).startswith(str(base_dir)):\n        raise ValueError('Attempted path traversal in destination path.')\n\n    destination_path.parent.mkdir(parents=True, exist_ok=True)\n\n    try:\n        # Move the model file to the destination path\n        model_file.rename(destination_path)\n    except Exception as e:\n        raise OSError(f\"Failed to update model file: {e}\")\n\n\nasync def get_new_model_file(\n    model: str, last_update_time: datetime.datetime,\n) -> bytes | None:\n    \"\"\"\n    Retrieve the new model file if updated since the provided time.\n\n    Args:\n        model (str): The model key (e.g., 'yolo11n', 'yolo11s').\n        last_update_time (datetime.datetime): The last update time\n            provided by the user.\n\n    Returns:\n        bytes | None: Model file content if updated, else None.\n    \"\"\"\n    # Define valid models and their corresponding filenames\n    valid_models = {\n        'yolo11n': 'best_yolo11n.pt',\n        'yolo11s': 'best_yolo11s.pt',\n        'yolo11m': 'best_yolo11m.pt',\n        'yolo11l': 'best_yolo11l.pt',\n        'yolo11x': 'best_yolo11x.pt',\n    }\n\n    if model not in valid_models:\n        raise ValueError(\n            f\"Invalid model key: {model}. \"\n            f\"Must be one of {list(valid_models.keys())}.\",\n        )\n\n    # Use a base directory and construct the destination path safely\n    base_dir = Path('models/pt').resolve()\n    destination_filename = valid_models[model]\n    destination_path = base_dir / destination_filename\n\n    # Ensure the destination path is within the base directory\n    destination_path = destination_path.resolve()\n    if not str(destination_path).startswith(str(base_dir)):\n        raise ValueError('Attempted path traversal in destination path.')\n\n    if not destination_path.is_file():\n        return None\n\n    file_mod_time = datetime.datetime.fromtimestamp(\n        destination_path.stat().st_mtime,\n    )\n    if file_mod_time > last_update_time:\n        try:\n            with destination_path.open('rb') as f:\n                return f.read()\n        except Exception as e:\n            raise OSError(f\"Failed to read model file: {e}\")\n    return None\n"}
{"type": "source_file", "path": "examples/YOLO_server_api/backend/redis_pool.py", "content": "from __future__ import annotations\n\nimport redis.asyncio as redis\nfrom fastapi import Request\n\n\nclass RedisClient:\n    \"\"\"\n    A class to represent a Redis client.\n    \"\"\"\n\n    def __init__(self, url: str) -> None:\n        \"\"\"\n        Initialises the RedisClient with the provided Redis server URL.\n\n        Args:\n            url (str): The URL of the Redis server.\n        \"\"\"\n        self.url = url\n        self.client: redis.Redis | None = None\n\n    async def connect(self) -> redis.Redis:\n        \"\"\"\n        Initialises (or retrieves) the asynchronous Redis client.\n\n        Returns:\n            redis.Redis: An asynchronous Redis client instance.\n\n        Raises:\n            redis.exceptions.ConnectionError:\n                If the connection to the Redis server fails.\n        \"\"\"\n        if not self.client:\n            # Establish a connection to the Redis server\n            #  with encoding and response decoding.\n            self.client = await redis.from_url(\n                self.url,\n                encoding='utf-8',\n                decode_responses=True,\n            )\n        return self.client\n\n    async def close(self) -> None:\n        \"\"\"\n        Closes the Redis connection if it is active.\n\n        This method ensures proper cleanup by releasing resources and\n        setting the client to `None`.\n        \"\"\"\n        if self.client:\n            await self.client.close()\n            self.client = None\n\n\nasync def get_redis_pool(request: Request) -> redis.Redis:\n    \"\"\"\n    Retrieves the Redis client from the application state.\n\n    Args:\n        request (Request): The FastAPI request object,\n            which contains the application state.\n\n    Returns:\n        redis.Redis: An asynchronous Redis client instance.\n\n    Raises:\n        RuntimeError: If the RedisClient\n            is not properly initialised in the application state.\n    \"\"\"\n    redis_client: RedisClient = request.app.state.redis_client\n    if not redis_client.client:\n        # Establish a connection if the client is not yet connected.\n        await redis_client.connect()\n\n    # Raise an error if the client is still not connected.\n    if redis_client.client is None:\n        raise RuntimeError('Redis client is not connected.')\n\n    return redis_client.client\n"}
{"type": "source_file", "path": "examples/YOLO_evaluation/convert_yolo_to_coco.py", "content": "from __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom typing import Any\n\nfrom PIL import Image\n\n\nclass COCOConverter:\n    \"\"\"\n    Converts YOLO format annotations to COCO format.\n    \"\"\"\n\n    def __init__(self, categories: list[str]):\n        \"\"\"\n        Initialises COCO data structure and category mappings.\n\n        Args:\n            categories (List[str]): A list of category names.\n        \"\"\"\n        self.coco_format: dict[str, list[Any]] = {\n            'images': [],\n            'annotations': [],\n            'categories': [],\n        }\n        self.initialise_categories(categories)\n        self.image_id = 1  # Unique ID for each image\n        self.annotation_id = 1  # Unique ID for each annotation\n\n    def initialise_categories(self, categories: list[str]):\n        \"\"\"Initialises categories for COCO format.\n\n        Args:\n            categories (List[str]): A list of category names.\n        \"\"\"\n        for i, category in enumerate(categories):\n            self.coco_format['categories'].append(\n                {\n                    'id': i + 1,\n                    'name': category,\n                    'supercategory': 'none',\n                },\n            )\n\n    def convert_annotations(self, labels_dir: str, images_dir: str):\n        \"\"\"Reads YOLO formatted annotations and converts them to COCO format.\n\n        Args:\n            labels_dir (str): Directory containing YOLO labels.\n            images_dir (str): Directory containing image files.\n        \"\"\"\n        for filename in os.listdir(labels_dir):\n            if filename.endswith('.txt'):\n                image_name = filename.replace('.txt', '.jpg')\n                image_path = os.path.join(images_dir, image_name)\n\n                if not os.path.exists(image_path):\n                    print(f\"Warning: {image_path} does not exist.\")\n                    continue\n\n                image = Image.open(image_path)\n                width, height = image.size\n\n                self.coco_format['images'].append(\n                    {\n                        'id': self.image_id,\n                        'width': width,\n                        'height': height,\n                        'file_name': image_name,\n                    },\n                )\n\n                with open(os.path.join(labels_dir, filename)) as file:\n                    for line in file:\n                        cls_id, x_center, y_center, bbox_width, bbox_height = (\n                            (\n                                float(x) if float(x) != int(\n                                    float(x),\n                                ) else int(float(x))\n                            )\n                            for x in line.strip().split()\n                        )\n\n                        x_min = (x_center - bbox_width / 2) * width\n                        y_min = (y_center - bbox_height / 2) * height\n                        bbox_width *= width\n                        bbox_height *= height\n\n                        self.coco_format['annotations'].append(\n                            {\n                                'id': self.annotation_id,\n                                'image_id': self.image_id,\n                                'category_id': int(cls_id) + 1,\n                                'bbox': [\n                                    x_min, y_min, bbox_width, bbox_height,\n                                ],\n                                'area': bbox_width * bbox_height,\n                                'segmentation': [],\n                                'iscrowd': 0,\n                            },\n                        )\n                        self.annotation_id += 1\n                self.image_id += 1\n\n    def save_to_json(self, output_path: str):\n        \"\"\"Saves the COCO formatted data to a JSON file.\n\n        Args:\n            output_path (str): Path to save the JSON output.\n        \"\"\"\n        with open(output_path, 'w') as json_file:\n            json.dump(self.coco_format, json_file, indent=4)\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description='Convert YOLO format annotations to COCO format.',\n    )\n    parser.add_argument(\n        '--labels_dir',\n        type=str,\n        required=True,\n        help='Directory containing YOLO labels.',\n    )\n    parser.add_argument(\n        '--images_dir',\n        type=str,\n        required=True,\n        help='Directory containing image files.',\n    )\n    parser.add_argument(\n        '--output',\n        type=str,\n        required=True,\n        help='Output JSON file path for COCO formatted annotations.',\n    )\n\n    args = parser.parse_args()\n\n    categories = [\n        'Hardhat',\n        'Mask',\n        'NO-Hardhat',\n        'NO-Mask',\n        'NO-Safety Vest',\n        'Person',\n        'Safety Cone',\n        'Safety Vest',\n        'machinery',\n        'vehicle',\n    ]\n\n    converter = COCOConverter(categories)\n    converter.convert_annotations(args.labels_dir, args.images_dir)\n    converter.save_to_json(args.output)\n    print(f\"COCO format annotations have been saved to {args.output}\")\n\n\nif __name__ == '__main__':\n    main()\n\n\"\"\"example usage\npython convert_yolo_to_coco.py \\\n    --labels_dir tests/dataset/val/labels \\\n    --images_dir tests/dataset/val/images \\\n    --output tests/dataset/coco_annotations.json\n\"\"\"\n"}
{"type": "source_file", "path": "examples/YOLO_server_api/backend/app.py", "content": "from __future__ import annotations\n\nimport os\nfrom collections.abc import AsyncGenerator\nfrom contextlib import asynccontextmanager\n\nimport socketio\nimport uvicorn\nfrom apscheduler.schedulers.background import BackgroundScheduler\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi_jwt import JwtAccessBearer\nfrom fastapi_limiter import FastAPILimiter\n\nfrom .config import Settings\nfrom .models import Base\nfrom .models import engine\nfrom .redis_pool import RedisClient\nfrom .routers import auth_router\nfrom .routers import detection_router\nfrom .routers import model_management_router\nfrom .routers import user_management_router\nfrom .security import update_secret_key\n\n# Instantiate the FastAPI app\napp = FastAPI()\n\n# Add CORS middleware\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=['*'],\n    allow_credentials=True,\n    allow_methods=['*'],\n    allow_headers=['*'],\n)\n\n# Register API routers for different functionalities\napp.include_router(auth_router)\napp.include_router(detection_router)\napp.include_router(model_management_router)\napp.include_router(user_management_router)\n\n# Set up JWT authentication\njwt_access = JwtAccessBearer(secret_key=Settings().authjwt_secret_key)\n\n# Configure background scheduler to refresh secret keys every 30 days\nscheduler = BackgroundScheduler()\nscheduler.add_job(\n    func=lambda: update_secret_key(app),\n    trigger='interval',\n    days=30,\n)\nscheduler.start()\n\n# Socket.IO server (AsyncServer) + ASGI app\nsio = socketio.AsyncServer(async_mode='asgi')\nsio_app = socketio.ASGIApp(sio, app)\n\n# Define Socket.IO events for real-time communication\n\n\n@sio.event\nasync def connect(sid: str, environ: dict) -> None:\n    \"\"\"\n    Handles client connection event to the Socket.IO server.\n\n    Args:\n        sid (str): The session ID of the connected client.\n        environ (dict): The environment dictionary for the connection.\n    \"\"\"\n    print('Client connected:', sid)\n\n\n# Define Socket.IO event for client disconnection from the server\n@sio.event\nasync def disconnect(sid: str) -> None:\n    \"\"\"\n    Handles client disconnection from the Socket.IO server.\n\n    Args:\n        sid (str): The session ID of the disconnected client.\n    \"\"\"\n    print('Client disconnected:', sid)\n\n# Define lifespan event to manage the application startup and shutdown\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI) -> AsyncGenerator[None]:\n    \"\"\"\n    Context manager to handle application startup and shutdown tasks.\n\n    Args:\n        app (FastAPI): The FastAPI application instance.\n    \"\"\"\n    # Initialise Redis connection\n    redis_host = os.getenv('REDIS_HOST', '127.0.0.1')\n    redis_port = os.getenv('REDIS_PORT', '6379')\n    redis_password = os.getenv('REDIS_PASSWORD', '')\n\n    redis_url = f\"redis://:{redis_password}@{redis_host}:{redis_port}/0\"\n\n    app.state.redis_client = RedisClient(redis_url)\n    redis_conn = await app.state.redis_client.connect()\n    await FastAPILimiter.init(redis_conn)\n\n    # Create database tables\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n\n    # Yield control to allow application operation\n    yield\n\n    # Shutdown the scheduler and Redis connection pool\n    # upon application termination\n    scheduler.shutdown()\n    await app.state.redis_client.close()\n\n# Assign lifespan context to the FastAPI app\napp.router.lifespan_context = lifespan\n\n# Run the FastAPI app with Uvicorn\n\n\ndef run_uvicorn_app():\n    \"\"\"\n    Wraps uvicorn.run(...) for easier patching in tests.\n    \"\"\"\n    uvicorn.run(\n        sio_app,\n        host='0.0.0.0',\n        port=8000,\n        workers=2,\n    )\n\n\nif __name__ == '__main__':\n    run_uvicorn_app()\n"}
{"type": "source_file", "path": "src/__init__.py", "content": ""}
{"type": "source_file", "path": "src/live_stream_detection.py", "content": "from __future__ import annotations\n\nimport argparse\nimport asyncio\nimport gc\nimport logging\nimport os\nfrom pathlib import Path\nfrom typing import MutableMapping\nfrom typing import TypedDict\n\nimport aiohttp\nimport cv2\nimport numpy as np\nfrom sahi import AutoDetectionModel\nfrom sahi.predict import get_sliced_prediction\n\n\nclass InputData(TypedDict):\n    frame: np.ndarray\n    model_key: str\n    detect_with_server: bool\n\n\nclass DetectionData(TypedDict):\n    x1: float\n    y1: float\n    x2: float\n    y2: float\n    confidence: float\n    label: int\n\n\nclass LiveStreamDetector:\n    \"\"\"\n    A class to perform live stream detection and tracking\n    using YOLO with SAHI.\n    \"\"\"\n\n    def __init__(\n        self,\n        api_url: str = 'http://localhost:5000',\n        model_key: str = 'yolo11n',\n        output_folder: str | None = None,\n        detect_with_server: bool = False,\n        shared_token: MutableMapping[str, str] | None = None,\n        shared_lock=None,\n    ):\n        \"\"\"\n        Initialises the LiveStreamDetector.\n\n        Args:\n            api_url (str): The URL of the API for detection.\n            model_key (str): The model key for detection.\n            output_folder (Optional[str]): Folder for detected frames.\n            detect_with_server (bool): Whether to use server-based detection.\n            shared_token (Optional[dict]): A shared dictionary for\n                token storage.\n            shared_lock: A shared multiprocessing Lock to\n                avoid multiple logins simultaneously.\n        \"\"\"\n        self.api_url: str = (\n            api_url if api_url.startswith('http') else f\"http://{api_url}\"\n        )\n        self.model_key: str = model_key\n        self.output_folder: str | None = output_folder\n        self.detect_with_server: bool = detect_with_server\n        self.shared_token: MutableMapping[str, str] = shared_token or dict(\n            access_token='',\n        )\n        self.shared_lock = shared_lock\n        self.model: AutoDetectionModel | None = None\n        self.logger = logging.getLogger(__name__)\n\n    #######################################################################\n    # Authentication functions\n    #######################################################################\n\n    def acquire_shared_lock(self):\n        \"\"\"\n        Acquires the shared lock for authentication.\n        \"\"\"\n        if self.shared_lock:\n            self.shared_lock.acquire()\n\n    def release_shared_lock(self):\n        \"\"\"\n        Releases the shared lock for authentication.\n        \"\"\"\n        if self.shared_lock:\n            self.shared_lock.release()\n\n    async def authenticate(self, force: bool = False) -> None:\n        \"\"\"\n        Ensures that the user is authenticated, re-authenticates if needed.\n\n        Args:\n            force (bool): Whether to force re-authentication.\n\n        Raises:\n            aiohttp.ClientResponseError: If the authentication fails.\n            ValueError: If credentials are missing.\n        \"\"\"\n        username = os.getenv('API_USERNAME')\n        password = os.getenv('API_PASSWORD')\n\n        if not username or not password:\n            raise ValueError(\n                'Missing API_USERNAME '\n                'or API_PASSWORD in environment variables',\n            )\n\n        # Check if re-authentication is needed\n        if not force and self.shared_token.get('access_token'):\n            return\n\n        self.acquire_shared_lock()\n        try:\n            async with aiohttp.ClientSession() as session:\n                async with session.post(\n                    f\"{self.api_url}/api/token\",\n                    json={\n                        'username': username,\n                        'password': password,\n                    },\n                ) as response:\n                    response.raise_for_status()\n                    token_data = await response.json()\n                    self.shared_token['access_token'] = (\n                        token_data['access_token']\n                    )\n                    self.logger.info(\n                        'Successfully authenticated and retrieved token.',\n                    )\n        finally:\n            self.release_shared_lock()\n\n    #######################################################################\n    # Detection functions\n    #######################################################################\n\n    async def generate_detections_cloud(\n        self,\n        frame: np.ndarray,\n    ) -> list[list[float]]:\n        \"\"\"\n        Sends the frame to the API for detection and retrieves the detections.\n\n        Args:\n            frame (np.ndarray): The frame to send for detection.\n\n        Returns:\n            list[list[float]]: The detection data.\n        \"\"\"\n        # Encode frame as PNG bytes\n        success, frame_encoded = cv2.imencode('.png', frame)\n        if not success:\n            raise ValueError('Failed to encode frame as PNG bytes.')\n        frame_bytes = frame_encoded.tobytes()\n\n        # Ensure authenticated\n        await self.authenticate()\n\n        # Send detection request\n        try:\n            headers = {\n                'Authorization': f\"Bearer {self.shared_token['access_token']}\",\n            }\n            data = aiohttp.FormData()\n            data.add_field(\n                'image',\n                frame_bytes,\n                filename='frame.png',\n                content_type='image/png',\n            )\n\n            async with aiohttp.ClientSession() as session:\n                async with session.post(\n                    f\"{self.api_url}/api/detect\",\n                    data=data,\n                    params={'model': self.model_key},\n                    headers=headers,\n                ) as response:\n                    # Token expired or invalid\n                    if response.status in (401, 403):\n                        self.logger.warning(\n                            'Token expired or invalid. Re-authenticating...',\n                        )\n                        # Re-authenticate and retry detection request\n                        await self.authenticate(force=True)\n\n                        # Retry detection request\n                        return await self.generate_detections_cloud(frame)\n                    response.raise_for_status()\n                    return await response.json()\n        except aiohttp.ClientResponseError as exc:\n            self.logger.error(f\"Failed to send detection request: {exc}\")\n            raise\n\n    async def generate_detections_local(\n        self,\n        frame: np.ndarray,\n    ) -> list[list[float]]:\n        \"\"\"\n        Generates detections locally using YOLO.\n\n        Args:\n            frame (np.ndarray): The frame to send for detection.\n\n        Returns:\n            list[list[float]]: The detection data.\n        \"\"\"\n        if self.model is None:\n            model_path = Path('models/pt/') / f\"best_{self.model_key}.pt\"\n            self.model = AutoDetectionModel.from_pretrained(\n                'yolo11',\n                model_path=model_path,\n                device='cuda:0',\n            )\n\n        result = get_sliced_prediction(\n            frame,\n            self.model,\n            slice_height=376,\n            slice_width=376,\n            overlap_height_ratio=0.3,\n            overlap_width_ratio=0.3,\n        )\n\n        # Compile detection data in YOLO format\n        datas = []\n        for obj in result.object_prediction_list:\n            label = int(obj.category.id)\n            x1, y1, x2, y2 = (int(x) for x in obj.bbox.to_voc_bbox())\n            confidence = float(obj.score.value)\n            datas.append([x1, y1, x2, y2, confidence, label])\n\n        # Remove overlapping labels for Hardhat and Safety Vest categories\n        datas = self.remove_overlapping_labels(datas)\n\n        # Remove fully contained Hardhat and Safety Vest labels\n        datas = self.remove_completely_contained_labels(datas)\n\n        return datas\n\n    async def generate_detections(\n        self, frame: np.ndarray,\n    ) -> tuple[list[list[float]], np.ndarray]:\n        \"\"\"\n        Generates detections with local model or cloud API as configured.\n\n        Args:\n            frame (np.ndarray): The frame to send for detection.\n\n        Returns:\n            Tuple[List[List[float]], np.ndarray]:\n                Detections and original frame.\n        \"\"\"\n        if self.detect_with_server:\n            datas = await self.generate_detections_cloud(frame)\n        else:\n            datas = await self.generate_detections_local(frame)\n        return datas, frame\n\n    async def run_detection(self, stream_url: str) -> None:\n        \"\"\"\n        Runs detection on the live stream.\n\n        Args:\n            stream_url (str): The URL of the live stream.\n        \"\"\"\n        cap = cv2.VideoCapture(stream_url)\n        if not cap.isOpened():\n            raise ValueError('Failed to open stream.')\n\n        try:\n            while True:\n                ret, frame = cap.read()\n                if not ret:\n                    print('Failed to read frame from the stream. Retrying...')\n                    continue\n\n                # Perform detection\n                datas, frame = await self.generate_detections(frame)\n                print(datas)  # You can replace this with actual processing\n\n                cv2.imshow('Frame', frame)\n                if cv2.waitKey(1) & 0xFF == ord('q'):\n                    break\n        finally:\n            cap.release()\n            cv2.destroyAllWindows()\n\n    #######################################################################\n    # Post-processing functions\n    #######################################################################\n\n    def remove_overlapping_labels(self, datas):\n        \"\"\"\n        Removes overlapping labels for Hardhat and Safety Vest categories.\n\n        Args:\n            datas (list): A list of detection data in YOLO format.\n\n        Returns:\n            list: A list of detection data with overlapping labels removed.\n        \"\"\"\n        # Indices of Hardhat detections\n        hardhat_indices = [\n            i for i, d in enumerate(\n                datas,\n            ) if d[5] == 0\n        ]\n        # Indices of NO-Hardhat detections\n        no_hardhat_indices = [i for i, d in enumerate(datas) if d[5] == 2]\n        # Indices of Safety Vest detections\n        safety_vest_indices = [i for i, d in enumerate(datas) if d[5] == 7]\n        # Indices of NO-Safety Vest detections\n        no_safety_vest_indices = [i for i, d in enumerate(datas) if d[5] == 4]\n\n        to_remove = set()\n        for hardhat_index in hardhat_indices:\n            for no_hardhat_index in no_hardhat_indices:\n                overlap = self.overlap_percentage(\n                    datas[hardhat_index][:4], datas[no_hardhat_index][:4],\n                )\n                if overlap > 0.8:\n                    to_remove.add(no_hardhat_index)\n\n        for safety_vest_index in safety_vest_indices:\n            for no_safety_vest_index in no_safety_vest_indices:\n                overlap = self.overlap_percentage(\n                    datas[safety_vest_index][:4],\n                    datas[no_safety_vest_index][:4],\n                )\n                if overlap > 0.8:\n                    to_remove.add(no_safety_vest_index)\n\n        for index in sorted(to_remove, reverse=True):\n            datas.pop(index)\n\n        gc.collect()\n        return datas\n\n    def overlap_percentage(self, bbox1, bbox2):\n        \"\"\"\n        Calculates the percentage of overlap between two bounding boxes.\n\n        Args:\n            bbox1 (list): The first bounding box [x1, y1, x2, y2].\n            bbox2 (list): The second bounding box [x1, y1, x2, y2].\n\n        Returns:\n            float: The percentage of overlap between the two bounding boxes.\n        \"\"\"\n        x1 = max(bbox1[0], bbox2[0])\n        y1 = max(bbox1[1], bbox2[1])\n        x2 = min(bbox1[2], bbox2[2])\n        y2 = min(bbox1[3], bbox2[3])\n\n        intersection_area = max(0, x2 - x1 + 1) * max(0, y2 - y1 + 1)\n        bbox1_area = (bbox1[2] - bbox1[0] + 1) * (bbox1[3] - bbox1[1] + 1)\n        bbox2_area = (bbox2[2] - bbox2[0] + 1) * (bbox2[3] - bbox2[1] + 1)\n\n        overlap_percentage = intersection_area / float(\n            bbox1_area + bbox2_area - intersection_area,\n        )\n        gc.collect()\n\n        return overlap_percentage\n\n    def is_contained(self, inner_bbox, outer_bbox):\n        \"\"\"\n        Determines if one bounding box is completely contained within another.\n\n        Args:\n            inner_bbox (list): The inner bounding box [x1, y1, x2, y2].\n            outer_bbox (list): The outer bounding box [x1, y1, x2, y2].\n\n        Returns:\n            bool: Checks if inner box is fully within outer bounding box.\n        \"\"\"\n        return (\n            inner_bbox[0] >= outer_bbox[0]\n            and inner_bbox[2] <= outer_bbox[2]\n            and inner_bbox[1] >= outer_bbox[1]\n            and inner_bbox[3] <= outer_bbox[3]\n        )\n\n    def remove_completely_contained_labels(self, datas):\n        \"\"\"\n        Removes labels fully contained in Hardhat/Safety Vest categories.\n\n        Args:\n            datas (list): A list of detection data in YOLO format.\n\n        Returns:\n            list: Detection data with fully contained labels removed.\n        \"\"\"\n        # Indices of Hardhat detections\n        hardhat_indices = [\n            i\n            for i, d in enumerate(\n                datas,\n            )\n            if d[5] == 0\n        ]\n\n        # Indices of NO-Hardhat detections\n        no_hardhat_indices = [i for i, d in enumerate(datas) if d[5] == 2]\n\n        # Indices of Safety Vest detections\n        safety_vest_indices = [i for i, d in enumerate(datas) if d[5] == 7]\n\n        # Indices of NO-Safety Vest detections\n        no_safety_vest_indices = [i for i, d in enumerate(datas) if d[5] == 4]\n\n        to_remove = set()\n        # Check hardhats\n        for hardhat_index in hardhat_indices:\n            for no_hardhat_index in no_hardhat_indices:\n                if self.is_contained(\n                    datas[no_hardhat_index][:4],\n                    datas[hardhat_index][:4],\n                ):\n                    to_remove.add(no_hardhat_index)\n                elif self.is_contained(\n                    datas[hardhat_index][:4],\n                    datas[no_hardhat_index][:4],\n                ):\n                    to_remove.add(hardhat_index)\n\n        # Check safety vests\n        for safety_vest_index in safety_vest_indices:\n            for no_safety_vest_index in no_safety_vest_indices:\n                if self.is_contained(\n                    datas[no_safety_vest_index][:4],\n                    datas[safety_vest_index][:4],\n                ):\n                    to_remove.add(no_safety_vest_index)\n                elif self.is_contained(\n                    datas[safety_vest_index][:4],\n                    datas[no_safety_vest_index][:4],\n                ):\n                    to_remove.add(safety_vest_index)\n\n        for index in sorted(to_remove, reverse=True):\n            datas.pop(index)\n\n        return datas\n\n\nasync def main():\n    parser = argparse.ArgumentParser(\n        description='Perform live stream detection and tracking using YOLO.',\n    )\n    parser.add_argument(\n        '--url',\n        type=str,\n        help='Live stream URL',\n        required=True,\n    )\n    parser.add_argument(\n        '--api_url',\n        type=str,\n        default='http://localhost:5000',\n        help='API URL for detection',\n    )\n    parser.add_argument(\n        '--model_key',\n        type=str,\n        default='yolo11n',\n        help='Model key for detection',\n    )\n    parser.add_argument(\n        '--output_folder',\n        type=str,\n        help='Output folder for detected frames',\n    )\n    parser.add_argument(\n        '--detect_with_server',\n        action='store_true',\n        help='Run detection using server api',\n    )\n    args = parser.parse_args()\n\n    # Shared token for authentication\n    shared_token = {\n        'access_token': '',\n    }\n\n    detector = LiveStreamDetector(\n        api_url=args.api_url,\n        model_key=args.model_key,\n        output_folder=args.output_folder,\n        detect_with_server=args.detect_with_server,\n        # If you want to share token across threads, use Manager()\n        shared_token=shared_token,\n    )\n    await detector.run_detection(args.url)\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "examples/YOLO_server_api/backend/user_operation.py", "content": "from __future__ import annotations\n\nfrom sqlalchemy.exc import IntegrityError\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.future import select\n\nfrom examples.YOLO_server_api.backend.models import User\n\n\nasync def add_user(\n    username: str, password: str, role: str, db: AsyncSession,\n) -> dict:\n    \"\"\"\n    Add a new user to the database.\n\n    Args:\n        username (str): The username for the new user.\n        password (str): The password for the new user.\n        role (str): The role assigned to the user.\n        db (AsyncSession): The database session.\n\n    Returns:\n        dict: A dictionary containing the result of the operation.\n    \"\"\"\n    new_user = User(username=username, role=role)\n    new_user.set_password(password)\n    db.add(new_user)\n\n    try:\n        await db.commit()\n        return {\n            'success': True,\n            'message': f\"User '{username}' added successfully.\",\n        }\n    except IntegrityError:\n        await db.rollback()\n        return {\n            'success': False,\n            'error': 'IntegrityError',\n            'message': f\"Username '{username}' already exists.\",\n        }\n    except Exception as e:\n        await db.rollback()\n        return {\n            'success': False,\n            'error': 'UnknownError',\n            'message': f\"Failed to add user: {str(e)}\",\n        }\n\n\nasync def delete_user(username: str, db: AsyncSession) -> dict:\n    \"\"\"\n    Delete a user from the database.\n\n    Args:\n        username (str): The username of the user to delete.\n        db (AsyncSession): The database session.\n\n    Returns:\n        dict: A dictionary containing the result of the operation.\n    \"\"\"\n    stmt = select(User).where(User.username == username)\n    result = await db.execute(stmt)\n    user = result.scalars().first()\n\n    if not user:\n        return {\n            'success': False,\n            'error': 'NotFound',\n            'message': f\"User '{username}' not found.\",\n        }\n\n    await db.delete(user)\n    try:\n        await db.commit()\n        return {\n            'success': True,\n            'message': f\"User '{username}' deleted successfully.\",\n        }\n    except Exception as e:\n        await db.rollback()\n        return {\n            'success': False,\n            'error': 'UnknownError',\n            'message': f\"Failed to delete user: {str(e)}\",\n        }\n\n\nasync def update_username(\n    old_username: str, new_username: str, db: AsyncSession,\n) -> dict:\n    \"\"\"\n    Update a user's username.\n\n    Args:\n        old_username (str): The current username of the user.\n        new_username (str): The new username to assign to the user.\n        db (AsyncSession): The database session.\n\n    Returns:\n        dict: A dictionary containing the result of the operation.\n    \"\"\"\n    stmt = select(User).where(User.username == old_username)\n    result = await db.execute(stmt)\n    user = result.scalars().first()\n\n    if not user:\n        return {\n            'success': False,\n            'error': 'NotFound',\n            'message': f\"User '{old_username}' not found.\",\n        }\n\n    user.username = new_username\n    try:\n        await db.commit()\n        return {\n            'success': True,\n            'message': (\n                f\"Username updated from '{old_username}' \"\n                f\"to '{new_username}'.\"\n            ),\n        }\n    except IntegrityError:\n        await db.rollback()\n        return {\n            'success': False,\n            'error': 'IntegrityError',\n            'message': f\"Username '{new_username}' already exists.\",\n        }\n    except Exception as e:\n        await db.rollback()\n        return {\n            'success': False,\n            'error': 'UnknownError',\n            'message': f\"Failed to update username: {str(e)}\",\n        }\n\n\nasync def update_password(\n    username: str, new_password: str, db: AsyncSession,\n) -> dict:\n    \"\"\"\n    Update a user's password.\n\n    Args:\n        username (str): The username of the user.\n        new_password (str): The new password.\n        db (AsyncSession): The database session.\n\n    Returns:\n        dict: A dictionary containing the result of the operation.\n    \"\"\"\n    stmt = select(User).where(User.username == username)\n    result = await db.execute(stmt)\n    user = result.scalars().first()\n\n    if not user:\n        return {\n            'success': False,\n            'error': 'NotFound',\n            'message': f\"User '{username}' not found.\",\n        }\n\n    user.set_password(new_password)\n    try:\n        await db.commit()\n        return {\n            'success': True,\n            'message': f\"Password updated successfully for user '{username}'.\",\n        }\n    except Exception as e:\n        await db.rollback()\n        return {\n            'success': False,\n            'error': 'UnknownError',\n            'message': f\"Failed to update password: {str(e)}\",\n        }\n\n\nasync def set_user_active_status(\n    username: str, is_active: bool, db: AsyncSession,\n) -> dict:\n    \"\"\"\n    Update a user's active status.\n\n    Args:\n        username (str): The username of the user.\n        is_active (bool): The new active status.\n        db (AsyncSession): The database session.\n\n    Returns:\n        dict: A dictionary containing the result of the operation.\n    \"\"\"\n    stmt = select(User).where(User.username == username)\n    result = await db.execute(stmt)\n    user = result.scalars().first()\n\n    if not user:\n        return {\n            'success': False,\n            'error': 'NotFound',\n            'message': f\"User '{username}' not found.\",\n        }\n\n    user.is_active = is_active\n    try:\n        await db.commit()\n        return {\n            'success': True,\n            'message': (\n                f\"User '{username}' is now \"\n                f\"{'active' if is_active else 'inactive'}.\"\n            ),\n        }\n    except Exception as e:\n        await db.rollback()\n        return {\n            'success': False,\n            'error': 'UnknownError',\n            'message': f\"Failed to update active status: {str(e)}\",\n        }\n"}
{"type": "source_file", "path": "examples/streaming_web/backend/routes.py", "content": "from __future__ import annotations\n\nimport os\nfrom typing import Any\n\nfrom fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom fastapi import Request\nfrom fastapi import WebSocket\nfrom fastapi import WebSocketDisconnect\nfrom fastapi.responses import JSONResponse\nfrom fastapi_limiter.depends import RateLimiter\nfrom linebot import LineBotApi\n\nfrom .utils import RedisManager\nfrom .utils import Utils\nfrom .utils import WebhookHandler\n# from pathlib import Path\n# from fastapi import UploadFile\n\nredis_manager = RedisManager()\n\n# Create an API router for defining routes\nrouter = APIRouter()\nline_bot_api = LineBotApi(\n    os.getenv('LINE_CHANNEL_ACCESS_TOKEN'),\n)\nwebhook_handler = WebhookHandler(line_bot_api)\n\nCONFIG_PATH = 'config/configuration.json'  # Path to the configuration file\n\n\ndef register_routes(app: Any) -> None:\n    \"\"\"\n    Registers the API router with the FastAPI application.\n\n    Args:\n        app (Any): The FastAPI application instance.\n    \"\"\"\n    app.include_router(router)\n\n\n# Create rate limiters for the API routes\nrate_limiter_index = RateLimiter(times=60, seconds=60)\nrate_limiter_label = RateLimiter(times=6000, seconds=6000)\n\n\n@router.get('/api/labels', dependencies=[Depends(rate_limiter_index)])\nasync def get_labels() -> JSONResponse:\n    \"\"\"\n    Renders the page for a specific label with available labels from Redis.\n\n    Args:\n        None\n\n    Returns:\n        JSONResponse: A JSON response containing the labels.\n    \"\"\"\n    try:\n        # Retrieve available labels from Redis\n        labels = await redis_manager.get_labels()\n\n    except ValueError as ve:\n        print(f\"ValueError while fetching labels: {str(ve)}\")\n        raise HTTPException(\n            status_code=400,\n            detail=f\"Invalid data encountered: {str(ve)}\",\n        )\n    except KeyError as ke:\n        print(f\"KeyError while fetching labels: {str(ke)}\")\n        raise HTTPException(\n            status_code=404,\n            detail=f\"Missing key encountered: {str(ke)}\",\n        )\n    except ConnectionError as ce:\n        print(f\"ConnectionError while fetching labels: {str(ce)}\")\n        raise HTTPException(\n            status_code=503,\n            detail=f\"Failed to connect to the database: {str(ce)}\",\n        )\n    except TimeoutError as te:\n        print(f\"TimeoutError while fetching labels: {str(te)}\")\n        raise HTTPException(\n            status_code=504,\n            detail=f\"Request timed out: {str(te)}\",\n        )\n    except Exception as e:\n        print(f\"Unexpected error while fetching labels: {str(e)}\")\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Failed to fetch labels: {str(e)}\",\n        )\n\n    return JSONResponse(content={'labels': labels})\n\n\n@router.websocket('/api/ws/labels/{label}')\nasync def websocket_label_stream(websocket: WebSocket, label: str) -> None:\n    \"\"\"\n    Establishes a WebSocket connection to stream updated frames\n    for a specific label.\n\n    Args:\n        websocket (WebSocket): The WebSocket connection object.\n        label (str): The label identifier for the frame stream.\n    \"\"\"\n    await websocket.accept()\n    try:\n        # Fetch Redis keys associated with the label\n        keys = await redis_manager.get_keys_for_label(label)\n        # Check if there are any keys associated with the label\n        if not keys:\n            await websocket.send_json({\n                'error': f\"No keys found for label '{label}'\",\n            })\n            await websocket.close()\n            return\n\n        # Initialise last message IDs for each key in the stream\n        last_ids: dict[str, str] = {key: '0' for key in keys}\n\n        while True:\n            # Fetch the latest frames for each key in the specified label\n            updated_data = await redis_manager.fetch_latest_frames(last_ids)\n            if updated_data:\n                # Send the updated frames to the WebSocket client\n                await Utils.send_frames(websocket, label, updated_data)\n    except WebSocketDisconnect:\n        # Handle WebSocket disconnection gracefully\n        print('WebSocket disconnected')\n    except Exception as e:\n        # Log unexpected errors\n        print(f\"Unexpected error: {e}\")\n    finally:\n        # Final message on WebSocket closure\n        print('WebSocket connection closed')\n\n\n@router.websocket('/api/ws/stream/{label}/{key}')\nasync def websocket_stream(websocket: WebSocket, label: str, key: str) -> None:\n    \"\"\"\n    Establishes a WebSocket connection to stream data for a single camera.\n\n    Args:\n        websocket (WebSocket): The WebSocket connection object.\n        label (str): The label associated with the stream.\n        key (str): The key identifying the specific camera stream.\n    \"\"\"\n    await websocket.accept()\n    try:\n        # Encode the label and key for Redis lookup\n        encoded_label = Utils.encode(label)\n        encoded_key = Utils.encode(key)\n        redis_key = f\"stream_frame:{encoded_label}|{encoded_key}\"\n\n        # Initialize last message ID for the stream\n        last_id = '0'\n\n        while True:\n            # Fetch the latest frame for the specific stream\n            message = await redis_manager.fetch_latest_frame_for_key(\n                redis_key, last_id,\n            )\n            if message:\n                # Update the last ID\n                last_id = message['id']\n                # Send the latest frame and warnings to the client\n                await websocket.send_json(message)\n            else:\n                # If no new data is available, close the connection\n                await websocket.send_json({'error': 'No new data available'})\n                await websocket.close()\n                break\n\n    except WebSocketDisconnect:\n        print('WebSocket disconnected')\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        # Close the WebSocket connection on error\n        await websocket.close()\n    finally:\n        print('WebSocket connection closed')\n\n\n@router.post('/api/webhook')\nasync def webhook(request: Request) -> JSONResponse:\n    \"\"\"\n    Processes incoming webhook requests by logging the request body.\n\n    Args:\n        request (Request): The HTTP request containing the webhook payload.\n\n    Returns:\n        JSONResponse: A JSON response indicating the status of the request.\n    \"\"\"\n    try:\n        # Retrieve the request body\n        body = await request.json()\n        responses = await webhook_handler.process_webhook_events(body)\n\n        # If all events were skipped, return a success response\n        if all(resp.get('status') == 'skipped' for resp in responses):\n            return JSONResponse(\n                content={\n                    'status': 'skipped',\n                    'message': 'All events skipped.',\n                },\n                status_code=200,\n            )\n\n        # If any event failed, return a partial error response\n        if any(resp['status'] == 'error' for resp in responses):\n            return JSONResponse(\n                content={'status': 'partial_error', 'responses': responses},\n                status_code=207,  # 207 Multi-Status\n            )\n\n        return JSONResponse(content={'status': 'ok', 'responses': responses})\n\n    except Exception as e:\n        print(f\"Unexpected error while processing webhook: {e}\")\n        raise HTTPException(\n            status_code=500, detail='Webhook processing failed',\n        )\n\n\n@router.get('/api/config')\nasync def get_config(request: Request) -> JSONResponse:\n    \"\"\"\n    Retrieve the current configuration.\n\n    Args:\n        request (Request): The HTTP request.\n\n    Returns:\n        JSONResponse: A JSON response containing the current configuration.\n    \"\"\"\n    Utils.verify_localhost(request)\n    config = Utils.load_configuration(CONFIG_PATH)\n    return JSONResponse(content={'config': config})\n\n\n@router.post('/api/config')\nasync def update_config(request: Request) -> JSONResponse:\n    \"\"\"\n    Update the configuration with the provided data.\n\n    Args:\n        request (Request): The HTTP request containing the new configuration.\n\n    Returns:\n        JSONResponse: A JSON response indicating the status of the request.\n    \"\"\"\n    Utils.verify_localhost(request)\n\n    try:\n        # Retrieve the new configuration data\n        data = await request.json()\n        new_config = data.get('config', [])\n\n        # Update the configuration file\n        updated_config = Utils.update_configuration(CONFIG_PATH, new_config)\n\n        return JSONResponse(\n            content={\n                'status': 'Configuration updated successfully.',\n                'config': updated_config,\n            },\n        )\n\n    except Exception as e:\n        raise HTTPException(\n            status_code=500, detail=f\"Failed to update configuration: {e}\",\n        )\n\n\n# Uncomment and use the following endpoint for file uploads if needed\n# @router.post('/api/upload')\n# async def upload_file(file: UploadFile) -> JSONResponse:\n#     \"\"\"\n#     Saves an uploaded file to the designated upload folder\n#     and returns its accessible URL.\n\n#     Args:\n#         file (UploadFile): The file to upload.\n\n#     Returns:\n#         JSONResponse: A JSON response containing the URL of\n#             the uploaded file.\n#     \"\"\"\n#     UPLOAD_FOLDER = Path('uploads')\n#     UPLOAD_FOLDER.mkdir(parents=True, exist_ok=True)\n\n#     # Check if the file has a filename\n#     if not file.filename:\n#         raise HTTPException(status_code=400, detail='Filename is missing')\n\n#     file_path = UPLOAD_FOLDER / file.filename\n#     try:\n#         with open(file_path, 'wb') as buffer:\n#             buffer.write(await file.read())\n#     except PermissionError as e:\n#         raise HTTPException(\n#             status_code=500, detail=f\"Failed to save file: {str(e)}\",\n#         )\n\n#     url = f\"/uploads/{file.filename}\"\n#     return JSONResponse(content={'url': url})\n"}
{"type": "source_file", "path": "examples/streaming_web/backend/utils.py", "content": "from __future__ import annotations\n\nimport base64\nimport json\nimport os\nimport re\nfrom typing import Any\n\nimport redis.asyncio as redis\nfrom dotenv import load_dotenv\nfrom fastapi import HTTPException\nfrom fastapi import Request\nfrom fastapi import WebSocket\nfrom linebot import LineBotApi\nfrom linebot.exceptions import LineBotApiError\nfrom linebot.models import TextSendMessage\n\n# Load environment variables\nload_dotenv()\n\n\nclass RedisManager:\n    \"\"\"\n    Manages asynchronous Redis operations for fetching labels, keys,\n    and image data.\n    \"\"\"\n\n    def __init__(\n        self,\n        redis_host: str = '127.0.0.1',\n        redis_port: int = 6379,\n        redis_password: str = '',\n        max_connections: int = 10,\n    ) -> None:\n        \"\"\"\n        Initialises RedisManager with Redis configuration details.\n\n        Args:\n            redis_host (str): The Redis server hostname.\n            redis_port (int): The Redis server port.\n            redis_password (str): The Redis password for authentication.\n        \"\"\"\n        self.redis_host: str = os.getenv('REDIS_HOST') or redis_host\n        self.redis_port: int = int(os.getenv('REDIS_PORT') or redis_port)\n        self.redis_password: str = os.getenv(\n            'REDIS_PASSWORD',\n        ) or redis_password\n\n        # Set up Redis connection pool\n        self.pool = redis.ConnectionPool(\n            host=self.redis_host,\n            port=self.redis_port,\n            password=self.redis_password,\n            decode_responses=False,\n            max_connections=max_connections,\n        )\n        self.client = redis.Redis(connection_pool=self.pool)\n\n    async def get_labels(self) -> list[str]:\n        \"\"\"\n        Fetches unique label and stream_name combinations from Redis keys.\n\n        Returns:\n            list[str]: A sorted list of unique label-stream_name combinations.\n        \"\"\"\n        cursor = 0\n        labels: set[str] = set()\n\n        while True:\n            # Scan Redis for keys and decode them to UTF-8\n            cursor, keys = await self.client.scan(cursor=cursor)\n            decoded_keys = [\n                key.decode('utf-8', errors='ignore')\n                for key in keys\n            ]\n\n            # Match and extract unique label and stream_name\n            for key in decoded_keys:\n                match = re.match(\n                    r'stream_frame:([\\w\\x80-\\xFF]+)\\|([\\w\\x80-\\xFF]+)', key,\n                )\n                if not match:\n                    continue\n\n                encoded_label, encoded_stream_name = match.groups()\n                label = Utils.decode(encoded_label)\n                if 'test' in label:\n                    continue\n\n                labels.add(label)\n\n            if cursor == 0:  # Exit loop if scan cursor has reached the end\n                break\n\n        return sorted(labels)\n\n    async def get_keys_for_label(self, label: str) -> list[str]:\n        \"\"\"\n        Retrieves Redis keys that match a given label-stream_name pattern.\n\n        Args:\n            label (str): The label-stream_name combination to search for.\n\n        Returns:\n            list[str]: A list of Redis keys associated with the given label.\n        \"\"\"\n        cursor = 0\n        matching_keys: list[str] = []\n\n        # Encode the label to ensure compatibility\n        encoded_label = Utils.encode(label)\n\n        while True:\n            cursor, keys = await self.client.scan(\n                cursor=cursor, match=f\"stream_frame:{encoded_label}|*\",\n            )\n            matching_keys.extend(\n                key.decode('utf-8')\n                for key in keys\n                if key.decode('utf-8').startswith('stream_frame:')\n            )\n\n            if cursor == 0:  # Exit loop if scan cursor has reached the end\n                break\n\n        return sorted(matching_keys)\n\n    async def fetch_latest_frames(\n        self, last_ids: dict[str, str],\n    ) -> list[dict[str, str]]:\n        \"\"\"\n        Fetches only the latest frame for each Redis stream.\n\n        Args:\n            last_ids (dict[str, str]): A dictionary mapping stream names to\n                their last read message ID.\n\n        Returns:\n            list[dict[str, str]]: A list of dictionaries\n                with updated frame data for each stream.\n        \"\"\"\n        updated_data = []\n\n        for key, last_id in last_ids.items():\n            messages = await self.client.xrevrange(key, count=1)\n            if not messages:\n                continue\n\n            message_id, data = messages[0]\n            last_ids[key] = message_id  # Update to the latest message ID\n            frame_data = data.get(b'frame')\n            if frame_data:\n                stream_name = key.split('|')[-1]\n                decoded_stream_name = Utils.decode(stream_name)\n                image = base64.b64encode(frame_data).decode('utf-8')\n                updated_data.append(\n                    {'key': decoded_stream_name, 'image': image},\n                )\n\n        return updated_data\n\n    async def fetch_latest_frame_for_key(\n        self,\n        redis_key: str,\n        last_id: str,\n    ) -> dict[str, str] | None:\n        \"\"\"\n        Fetches the latest frame and warnings for a specific Redis key.\n\n        Args:\n            redis_key (str): The Redis key to fetch data for.\n            last_id (str): The last read message ID.\n\n        Returns:\n            dict[str, str] | None: A dictionary with frame and warnings data,\n            or None if no new data is found.\n        \"\"\"\n        # Fetch the latest message for the specific Redis key\n        messages = await self.client.xrevrange(redis_key, min=last_id, count=1)\n\n        if not messages:\n            return None\n\n        # Extract the message ID and data\n        message_id, data = messages[0]\n\n        frame_data = data.get(b'frame')\n        warnings_data = data.get(b'warnings')\n\n        if frame_data:\n            # Decode frame and warnings data\n            image = base64.b64encode(frame_data).decode('utf-8')\n            warnings = warnings_data.decode('utf-8') if warnings_data else ''\n\n            return {\n                'id': message_id.decode('utf-8'),\n                'image': image,\n                'warnings': warnings,\n            }\n\n        return None\n\n    async def update_partial_config(self, key: str, value: Any) -> None:\n        \"\"\"\n        Update a single key in the Redis configuration cache.\n        \"\"\"\n        cached_config = await self.get_config_cache()\n        cached_config[key] = value\n        await self.set_config_cache(cached_config)\n\n    async def get_partial_config(self, key: str) -> Any:\n        \"\"\"\n        Retrieve a single key from the Redis configuration cache.\n        \"\"\"\n        cached_config = await self.get_config_cache()\n        return cached_config.get(key)\n\n    async def delete_config_cache(self) -> None:\n        \"\"\"\n        Clear the Redis configuration cache.\n        \"\"\"\n        await self.client.delete('config_cache')\n\n    async def get_config_cache(self) -> dict:\n        \"\"\"\n        Retrieve the full configuration from Redis cache.\n        \"\"\"\n        cached_config = await self.client.get('config_cache')\n        if cached_config:\n            return json.loads(cached_config)\n        return {}\n\n    async def set_config_cache(self, config: dict, ttl: int = 3600) -> None:\n        \"\"\"\n        Save the full configuration to Redis cache.\n        \"\"\"\n        await self.client.set('config_cache', json.dumps(config), ex=ttl)\n\n    async def close(self):\n        \"\"\"\n        Close the Redis connection pool.\n        \"\"\"\n        await self.client.close()\n\n\nclass Utils:\n    \"\"\"\n    Contains utility methods for processing and sending frame data.\n    \"\"\"\n\n    @staticmethod\n    def encode(value: str) -> str:\n        \"\"\"\n        Encode a value into a URL-safe Base64 string.\n\n        Args:\n            value (str): The value to encode.\n\n        Returns:\n            str: The encoded string.\n        \"\"\"\n        return base64.urlsafe_b64encode(\n            value.encode('utf-8'),\n        ).decode('utf-8')\n\n    @staticmethod\n    def is_base64(value: str) -> bool:\n        \"\"\"\n        Check if the string is a valid Base64 encoded string.\n\n        Args:\n            value (str): The string to check.\n\n        Returns:\n            bool: True if the string is Base64, False otherwise.\n        \"\"\"\n        if not value or not isinstance(value, str):\n            return False\n        # Base64 strings must be a multiple of 4 in length\n        if len(value) % 4 != 0:\n            return False\n        # Check if the string matches the Base64 regex pattern\n        return re.fullmatch(r'^[A-Za-z0-9\\-_]+={0,2}$', value) is not None\n\n    @staticmethod\n    def decode(value: str) -> str:\n        \"\"\"\n        Decode a URL-safe Base64 string.\n\n        Args:\n            value (str): The encoded string to decode.\n\n        Returns:\n            str: The decoded value.\n        \"\"\"\n        if not Utils.is_base64(value):\n            return value  # Return the original value if it's not Base64\n        return base64.urlsafe_b64decode(value.encode('utf-8')).decode('utf-8')\n\n    @staticmethod\n    async def send_frames(\n        websocket: WebSocket,\n        label: str,\n        updated_data: list[dict[str, str]],\n    ) -> None:\n        \"\"\"\n        Sends the latest frames to the WebSocket client.\n\n        Args:\n            websocket (WebSocket): The WebSocket connection object.\n            label (str): The label associated with the frames.\n            updated_data (list[dict[str, str]]): The latest frames to be sent.\n        \"\"\"\n        await websocket.send_json({\n            'label': label,\n            'images': updated_data,\n        })\n\n    @staticmethod\n    def load_configuration(config_path: str) -> list[dict]:\n        \"\"\"\n        Load a JSON configuration file and return its content as a list.\n\n        Args:\n            config_path (str): The path to the JSON configuration file.\n\n        Returns:\n            list[dict]: The configuration data as a list of dictionaries.\n        \"\"\"\n        try:\n            with open(config_path, encoding='utf-8') as file:\n                content = file.read()\n                return json.loads(content)\n        except Exception as e:\n            print(f\"Error loading configuration: {e}\")\n            return []\n\n    @staticmethod\n    def save_configuration(config_path: str, data: list[dict]) -> None:\n        \"\"\"\n        Save a list of dictionaries as JSON to a configuration file.\n\n        Args:\n            config_path (str): The path to the JSON configuration file.\n            data (list[dict]): The data to save to the file\n\n        Raises:\n            Exception: If an error occurs while saving the configuration.\n        \"\"\"\n        try:\n            with open(config_path, mode='w', encoding='utf-8') as file:\n                content = json.dumps(data, indent=4, ensure_ascii=False)\n                file.write(content)\n        except Exception as e:\n            print(f\"Error saving configuration: {e}\")\n\n    @staticmethod\n    def verify_localhost(request: Request) -> None:\n        \"\"\"\n        Verify that the request is made from localhost.\n\n        Args:\n            request (Request): The incoming HTTP request.\n\n        Raises:\n            HTTPException: If the request is not made from localhost.\n        \"\"\"\n        client = request.client\n        if client is None or client.host not in ['127.0.0.1', '::1']:\n            raise HTTPException(\n                status_code=403,\n                detail='Access is restricted to localhost only.',\n            )\n\n    @staticmethod\n    def update_configuration(\n        config_path: str, new_config: list[dict],\n    ) -> list[dict]:\n        \"\"\"\n        Update the configuration file with new data.\n\n        Args:\n            config_path (str): The path to the JSON configuration file.\n            new_config (list[dict]): The new configuration data to update.\n\n        Returns:\n            list[dict]: The updated configuration data.\n\n        Raises:\n            ValueError: If the configuration file\n                is not in the expected format.\n        \"\"\"\n        current_config = Utils.load_configuration(config_path)\n\n        if not isinstance(current_config, list):\n            raise ValueError(\n                'Invalid configuration format. Expected a list in JSON file.',\n            )\n\n        # Find items that should be removed\n        current_urls = {item['video_url'] for item in current_config}\n        new_urls = {item['video_url'] for item in new_config}\n        urls_to_remove = current_urls - new_urls\n\n        # Remove items that are not in the new configuration\n        updated_config = [\n            item\n            for item in current_config\n            if item['video_url'] not in urls_to_remove\n        ]\n\n        # Update or add new items\n        for new_item in new_config:\n            # Check if the item already exists in the configuration\n            existing_item = next(\n                (\n                    item for item in updated_config if item['video_url']\n                    == new_item['video_url']\n                ),\n                None,\n            )\n            if existing_item:\n                # If the item exists, update it\n                existing_item.update(new_item)\n            else:\n                # If the item does not exist, add it to the configuration\n                updated_config.append(new_item)\n\n        # Save the updated configuration to the file\n        Utils.save_configuration(config_path, updated_config)\n        return updated_config\n\n\nclass WebhookHandler:\n    \"\"\"\n    Handles LINE webhook events and sends responses to users.\n    \"\"\"\n\n    def __init__(self, line_bot_api: LineBotApi):\n        \"\"\"\n        Initialises the WebhookHandler with a LineBotApi instance.\n\n        Args:\n            line_bot_api (LineBotApi): The LineBotApi instance to use.\n        \"\"\"\n        self.line_bot_api = line_bot_api\n\n    async def process_webhook_events(self, body: dict) -> list[dict]:\n        \"\"\"\n        Process LINE webhook events and send a response.\n\n        Args:\n            body (dict): The request body containing LINE events.\n\n        Returns:\n            list[dict]: A list of responses for each event.\n        \"\"\"\n        responses = []\n\n        for event in body.get('events', []):\n            # Ignore redelivery events\n            if event.get('deliveryContext', {}).get('isRedelivery', False):\n                print(f\"Skipped redelivery event: {event}\")\n                responses.append({'status': 'skipped', 'reason': 'Redelivery'})\n                continue\n\n            try:\n                # Process text messages and extract user/group IDs\n                if (\n                    event['type'] == 'message'\n                    and event['message']['type'] == 'text'\n                ):\n                    text = event['message']['text']\n                    source = event['source']\n                    user_id = source.get('userId', 'not provided')\n                    group_id = source.get('groupId', 'not provided')\n\n                    if 'token' in text.lower():\n                        response_message = (\n                            f\"group ID: {group_id}\\n\"\n                            f\"user ID: {user_id}\"\n                        )\n\n                        # Send a response message to the user or group\n                        target_id = (\n                            group_id\n                            if group_id != 'not provided'\n                            else user_id\n                        )\n                        self.line_bot_api.push_message(\n                            target_id, TextSendMessage(text=response_message),\n                        )\n                        responses.append(\n                            {'status': 'success', 'target_id': target_id},\n                        )\n\n            except LineBotApiError as e:\n                # Fetch LINE API errors\n                print(f\"LineBotApiError: {e.message}\")\n                details_data = getattr(e.error, 'details', [])\n                if isinstance(details_data, list):\n                    details = json.dumps(details_data)\n                else:\n                    details = str(details_data)\n                responses.append(\n                    {\n                        'status': 'error',\n                        'message': e.message,\n                        'details': details,\n                    },\n                )\n            except Exception as e:\n                # Catch and log unexpected errors\n                print(f\"Unexpected error: {e}\")\n                responses.append({'status': 'error', 'message': str(e)})\n\n        return responses\n"}
{"type": "source_file", "path": "src/lang_config.py", "content": "from __future__ import annotations\n\nfrom functools import lru_cache\n\nLANGUAGES = {\n    'zh-TW': {\n        'warning_people_in_controlled_area': ': {count}!',\n        'warning_no_hardhat': ': !',\n        'warning_no_safety_vest': ': !',\n        'warning_close_to_machinery': ': {label}!',\n        'no_warning': '',\n        'machinery': '',\n        'vehicle': '',\n        'helmet': '',\n        'person': '',\n        'no_helmet': '',\n        'vest': '',\n        'no_vest': '',\n        'mask': '',\n        'no_mask': '',\n        'cone': '',\n    },\n    'zh-CN': {\n        'warning_people_in_controlled_area': ': {count}!',\n        'warning_no_hardhat': ': !',\n        'warning_no_safety_vest': ': !',\n        'warning_close_to_machinery': ': {label}!',\n        'no_warning': '',\n        'machinery': '',\n        'vehicle': '',\n        'helmet': '',\n        'person': '',\n        'no_helmet': '',\n        'vest': '',\n        'no_vest': '',\n        'mask': '',\n        'no_mask': '',\n        'cone': '',\n    },\n    'en': {\n        'warning_people_in_controlled_area': (\n            'Warning: {count} people have entered the controlled area!'\n        ),\n        'warning_no_hardhat': 'Warning: Someone is not wearing a hardhat!',\n        'warning_no_safety_vest': (\n            'Warning: Someone is not wearing a safety vest!'\n        ),\n        'warning_close_to_machinery': (\n            'Warning: Someone is too close to {label}!'\n        ),\n        'no_warning': 'No warning',\n        'machinery': 'machinery',\n        'vehicle': 'vehicle',\n        'helmet': 'helmet',\n        'person': 'person',\n        'no_helmet': 'no helmet',\n        'vest': 'safety vest',\n        'no_vest': 'no safety vest',\n        'mask': 'mask',\n        'no_mask': 'no mask',\n        'cone': 'cone',\n    },\n    'fr': {\n        'warning_people_in_controlled_area': (\n            'Avertissement: {count} personnes sont entres '\n            'dans la zone contrle!'\n        ),\n        'warning_no_hardhat': (\n            \"Avertissement: Quelqu'un ne porte pas de casque!\"\n        ),\n        'warning_no_safety_vest': (\n            \"Avertissement: Quelqu'un ne porte pas de gilet de scurit!\"\n        ),\n        'warning_close_to_machinery': (\n            'Avertissement: Quelquun est trop proche de {label}!'\n        ),\n        'no_warning': 'Pas d\\'avertissement',\n        'machinery': 'machinerie',\n        'vehicle': 'vhicule',\n        'helmet': 'casque',\n        'person': 'personne',\n        'no_helmet': 'pas de casque',\n        'vest': 'gilet de scurit',\n        'no_vest': 'pas de gilet de scurit',\n        'mask': 'masque',\n        'no_mask': 'pas de masque',\n        'cone': 'cne',\n    },\n    'vi': {\n        'warning_people_in_controlled_area': (\n            'Cnh bo: C {count} ngi  vo khu vc kim sot!'\n        ),\n        'warning_no_hardhat': 'Cnh bo: C ngi khng i m bo h!',\n        'warning_no_safety_vest': (\n            'Cnh bo: C ngi khng mc o gi-l an ton!'\n        ),\n        'warning_close_to_machinery': (\n            'Cnh bo: C ngi qu gn vi {label}!'\n        ),\n        'no_warning': 'Khng c cnh bo',\n        'machinery': 'my mc',\n        'vehicle': 'phng tin',\n        'helmet': 'm bo h',\n        'person': 'ngi',\n        'no_helmet': 'khng m bo h',\n        'vest': 'o gi-l an ton',\n        'no_vest': 'khng o gi-l an ton',\n        'mask': 'khu trang',\n        'no_mask': 'khng khu trang',\n        'cone': 'cc tiu',\n    },\n    'id': {\n        'warning_people_in_controlled_area': (\n            'Peringatan: {count} orang telah memasuki area terkontrol!'\n        ),\n        'warning_no_hardhat': (\n            'Peringatan: Seseorang tidak mengenakan helm!'\n        ),\n        'warning_no_safety_vest': (\n            'Peringatan: Seseorang tidak mengenakan rompi keselamatan!'\n        ),\n        'warning_close_to_machinery': (\n            'Peringatan: Seseorang terlalu dekat dengan {label}!'\n        ),\n        'no_warning': 'Tidak ada peringatan',\n        'machinery': 'mesin',\n        'vehicle': 'kendaraan',\n        'helmet': 'helm',\n        'person': 'orang',\n        'no_helmet': 'tanpa helm',\n        'vest': 'rompi keselamatan',\n        'no_vest': 'tanpa rompi keselamatan',\n        'mask': 'masker',\n        'no_mask': 'tanpa masker',\n        'cone': 'kerucut pengaman',\n    },\n    'th': {\n        'warning_people_in_controlled_area': (\n            ':  {count} !'\n        ),\n        'warning_no_hardhat': ': !',\n        'warning_no_safety_vest': ': !',\n        'warning_close_to_machinery': (\n            ':  {label} !'\n        ),\n        'no_warning': '',\n        'machinery': '',\n        'vehicle': '',\n        'helmet': '',\n        'person': '',\n        'no_helmet': '',\n        'vest': '',\n        'no_vest': '',\n        'mask': '',\n        'no_mask': '',\n        'cone': '',\n    },\n}\n\n\nclass Translator:\n    \"\"\"\n    A class to handle translations based on the provided language.\n    \"\"\"\n\n    @staticmethod\n    @lru_cache(maxsize=128)\n    def translate_warning(\n        warnings: tuple[str, ...],\n        language: str,\n    ) -> list[str]:\n        \"\"\"\n        Translate warnings from English to the specified language.\n\n        Args:\n            warnings (tuple[str, ...]): A tuple of warnings in English.\n            language (str): The target language code (e.g., 'zh-TW', 'en').\n\n        Returns:\n            list[str]: Translated warnings.\n        \"\"\"\n        translated_warnings = []\n\n        # Use English as the default language\n        # if the specified language is not supported\n        if language not in LANGUAGES:\n            language = 'en'\n\n        # Loop through all warnings and translate them\n        for warning in warnings:\n            if warning == 'No warning':\n                translated_warning = LANGUAGES[language].get(\n                    'no_warning', warning,\n                )\n            elif 'Someone is not wearing a hardhat!' in warning:\n                translated_warning = LANGUAGES[language].get(\n                    'warning_no_hardhat', warning,\n                )\n            elif 'Someone is not wearing a safety vest!' in warning:\n                translated_warning = LANGUAGES[language].get(\n                    'warning_no_safety_vest', warning,\n                )\n            elif 'Someone is too close to' in warning:\n                label_key = (\n                    'machinery' if 'machinery' in warning else 'vehicle'\n                )\n                translated_warning = LANGUAGES[language].get(\n                    'warning_close_to_machinery', warning,\n                )\n                translated_warning = translated_warning.replace(\n                    '{label}', LANGUAGES[language].get(label_key, label_key),\n                )\n            elif 'people have entered the controlled area!' in warning:\n                count = warning.split(' ')[1]  # Extract count of people\n                translated_warning = LANGUAGES[language].get(\n                    'warning_people_in_controlled_area', warning,\n                )\n                translated_warning = translated_warning.replace(\n                    '{count}', count,\n                )\n            else:\n                # Keep the original warning if no match\n                translated_warning = warning\n            translated_warnings.append(translated_warning)\n\n        return translated_warnings\n\n\ndef main():\n    # Example warnings in English\n    warnings = [\n        'Warning: Someone is not wearing a hardhat!',\n        'Warning: 2 people have entered the controlled area!',\n        'Warning: Someone is too close to machinery!',\n    ]\n\n    # Specify the language to translate to\n    # (e.g., 'zh-TW' for Traditional Chinese)\n    language = 'zh-TW'\n    translated_warnings = Translator.translate_warning(\n        tuple(warnings), language,\n    )\n    print('Original Warnings:', warnings)\n    print('Translated Warnings:', translated_warnings)\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "examples/__init__.py", "content": ""}
{"type": "source_file", "path": "examples/YOLO_server_api/backend/cache.py", "content": "from __future__ import annotations\n\nimport json\nfrom typing import Any\n\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom fastapi import Request\nfrom fastapi_jwt import JwtAccessBearer\nfrom fastapi_jwt import JwtAuthorizationCredentials\nfrom redis.asyncio import Redis\n\nfrom .config import Settings\n\njwt_access = JwtAccessBearer(secret_key=Settings().authjwt_secret_key)\n\n\nasync def get_user_data(\n    redis_pool: Redis,\n    username: str,\n) -> dict[str, Any] | None:\n    \"\"\"\n    Retrieve user data from the Redis cache.\n\n    Args:\n        redis_pool (Redis): The Redis connection pool\n            used to interact with the cache.\n        username (str): The username to retrieve the cached data for.\n\n    Returns:\n        Optional[dict[str, Any]]: A dictionary containing user data if found,\n        otherwise `None`.\n\n    Example:\n        >>> data = await get_user_data(redis_pool, \"john_doe\")\n        >>> print(data)\n        {\"id\": 123, \"username\": \"john_doe\", \"role\": \"user\"}\n    \"\"\"\n    # Construct the Redis key for the user.\n    key = f\"user_cache:{username}\"\n    # Attempt to retrieve data from Redis.\n    raw_data = await redis_pool.get(key)\n\n    if raw_data is None:\n        return None  # Return None if no data is found.\n\n    # Parse and return the JSON data as a dictionary.\n    return json.loads(raw_data)\n\n\nasync def set_user_data(\n    redis_pool: Redis,\n    username: str,\n    data: dict[str, Any],\n) -> None:\n    \"\"\"\n    Store user data in the Redis cache.\n\n    Args:\n        redis_pool (Redis): The Redis connection pool\n            used to interact with the cache.\n        username (str): The username for which the data is being stored.\n        data (dict[str, Any]): The user data to be cached.\n    \"\"\"\n    # Construct the Redis key for the user.\n    key = f\"user_cache:{username}\"\n    # Serialise the data dictionary to JSON and store it in Redis.\n    await redis_pool.set(key, json.dumps(data))\n\n\nasync def custom_rate_limiter(\n    request: Request,\n    credentials: JwtAuthorizationCredentials = Depends(jwt_access),\n) -> int:\n    \"\"\"\n    Custom rate limiter for different user roles.\n\n    Args:\n        request (Request): The incoming request.\n        credentials (JwtAuthorizationCredentials): The JWT credentials.\n\n    Returns:\n        int: The remaining number of requests allowed.\n    \"\"\"\n    # Extract the username and token jti from the JWT payload\n    payload = credentials.subject\n    username = payload.get('username')\n    token_jti = payload.get('jti')\n\n    # Ensure the username and token jti are valid strings\n    if not isinstance(username, str) or not isinstance(token_jti, str):\n        raise HTTPException(\n            status_code=401, detail='Token is missing or invalid fields',\n        )\n\n    # Get the Redis connection pool from the request state\n    redis_pool: Redis = request.app.state.redis_client.client\n\n    user_data = await get_user_data(redis_pool, username)\n    if not user_data:\n        raise HTTPException(status_code=401, detail='No such user in Redis')\n\n    # Check if the token jti is valid\n    jti_list = user_data.get('jti_list', [])\n    if token_jti not in jti_list:\n        raise HTTPException(\n            status_code=401, detail='Token jti is invalid or replaced',\n        )\n\n    # Extract the user role and construct the Redis key prefix\n    role: str = payload.get('role', 'user')\n    key_prefix: str = f\"rate_limit:{role}:{username}:{request.url.path}\"\n\n    max_requests = 24 if role == 'guest' else 3000\n    window_seconds = 86400 if role == 'guest' else 60\n\n    # Increment the request count\n    current_requests = await redis_pool.incr(key_prefix)\n    ttl = await redis_pool.ttl(key_prefix)\n\n    # Set the expiration only if not already set\n    if ttl == -1:\n        await redis_pool.expire(key_prefix, window_seconds)\n\n    if current_requests > max_requests:\n        raise HTTPException(status_code=429, detail='Rate limit exceeded')\n\n    return max_requests - current_requests\n"}
{"type": "source_file", "path": "src/model_fetcher.py", "content": "from __future__ import annotations\n\nimport datetime\nimport logging\nimport time\nfrom pathlib import Path\n\nimport requests\nimport schedule\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass ModelFetcher:\n    \"\"\"\n    A class to fetch and update model files from a server.\n    \"\"\"\n\n    def __init__(\n        self,\n        api_url: str = 'http://your-server-address/get_new_model',\n        models: list[str] | None = None,\n        local_dir: str = 'models/pt',\n    ):\n        \"\"\"\n        Initialises the ModelFetcher instance with default values.\n\n        Args:\n            api_url (str): The API URL for fetching updated models.\n            models (Optional[list[str]]): A list of model names to update.\n                Defaults to common YOLO models.\n            local_dir (str): The directory to store model files.\n        \"\"\"\n        self.api_url = api_url\n        self.models = models or [\n            'yolo11n', 'yolo11s', 'yolo11m', 'yolo11l', 'yolo11x',\n        ]\n        self.local_dir = Path(local_dir)\n\n    def get_last_update_time(self, model: str) -> str:\n        \"\"\"\n        Get the last update time of a local model file.\n        If the file does not exist, return Unix epoch timestamp.\n\n        Args:\n            model (str): The name of the model.\n\n        Returns:\n            str: The last modification time in ISO format.\n        \"\"\"\n        local_file_path = self.local_dir / f'best_{model}.pt'\n        if local_file_path.exists():\n            last_mod_time = datetime.datetime.fromtimestamp(\n                local_file_path.stat().st_mtime,\n            )\n        else:\n            last_mod_time = datetime.datetime(1970, 1, 1)\n        return last_mod_time.isoformat()\n\n    def download_and_save_model(\n        self,\n        model: str,\n        model_file_content: bytes,\n    ) -> None:\n        \"\"\"\n        Download and save the model file to the local directory.\n\n        Args:\n            model (str): The name of the model.\n            model_file_content (bytes): The content of the model file.\n\n        Returns:\n            None\n        \"\"\"\n        local_file_path = self.local_dir / f'best_{model}.pt'\n        self.local_dir.mkdir(parents=True, exist_ok=True)\n        with open(local_file_path, 'wb') as f:\n            f.write(model_file_content)\n        logger.info(f\"Model {model} successfully updated at {local_file_path}\")\n\n    def request_new_model(self, model: str, last_update_time: str) -> None:\n        \"\"\"\n        Request a new model file from the server.\n\n        Args:\n            model (str): The name of the model.\n            last_update_time (str): The last modification time of local model.\n\n        Returns:\n            None\n        \"\"\"\n        try:\n            response = requests.get(\n                self.api_url,\n                params={'model': model, 'last_update_time': last_update_time},\n                timeout=10,  # Set timeout for the request\n            )\n\n            if response.status_code == 200:\n                data = response.json()\n                if 'model_file' in data:\n                    model_file_content = bytes.fromhex(data['model_file'])\n                    self.download_and_save_model(model, model_file_content)\n                else:\n                    logger.info(f\"Model {model} is already up to date.\")\n            else:\n                logger.error(\n                    f\"Failed to fetch model {model}. \"\n                    f\"Server returned status code: {response.status_code}\",\n                )\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Error requesting model {model}: {e}\")\n\n    def update_all_models(self):\n        \"\"\"\n        Attempt to update all model files.\n\n        Returns:\n            None\n        \"\"\"\n        for model in self.models:\n            try:\n                logger.info(f\"Checking for updates for model {model}...\")\n                last_update_time = self.get_last_update_time(model)\n                self.request_new_model(model, last_update_time)\n            except Exception as e:\n                logger.error(f\"Failed to update model {model}: {e}\")\n\n\n# Schedule the task to run every hour\ndef schedule_task():\n    updater = ModelFetcher()\n    updater.update_all_models()\n\n\ndef run_scheduler_loop():\n    logger.info('Starting scheduled tasks. Press Ctrl+C to exit.')\n    while True:\n        schedule.run_pending()\n        time.sleep(1)\n\n\nif __name__ == '__main__':\n    # Execute the scheduled task every hour\n    schedule.every(1).hour.do(schedule_task)\n    run_scheduler_loop()\n"}
{"type": "source_file", "path": "examples/streaming_web/backend/app.py", "content": "from __future__ import annotations\n\nfrom collections.abc import AsyncGenerator\nfrom contextlib import asynccontextmanager\n\nimport socketio\nimport uvicorn\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi_limiter import FastAPILimiter\n\nfrom .routes import register_routes\nfrom .sockets import register_sockets\nfrom .utils import RedisManager\n# from fastapi.staticfiles import StaticFiles\n\nredis_manager = RedisManager()\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI) -> AsyncGenerator[None]:\n    \"\"\"\n    Initialises resources at startup and performs cleanup on shutdown.\n\n    Args:\n        app (FastAPI): The FastAPI application instance.\n\n    Yields:\n        None\n    \"\"\"\n    # Initialises rate limiter with the Redis client\n    await FastAPILimiter.init(redis_manager.client)\n    try:\n        yield\n    finally:\n        # Cleanup code: Closes the Redis connection after application shutdown\n        await redis_manager.client.close()\n        print('Redis connection closed.')\n\n\n# Create the FastAPI application with a lifespan manager for setup and cleanup\napp = FastAPI(lifespan=lifespan)\n\n# Add CORS middleware to allow cross-origin requests\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=['*'],\n    allow_credentials=True,\n    allow_methods=['*'],\n    allow_headers=['*'],\n)\n\n# Uncomment and use the following endpoint for file uploads if needed\n# Mount the static files directory to serve static assets\n# app.mount(\n#     '/static',\n#     StaticFiles(directory='examples/streaming_web/backend/static'),\n#     name='static',\n# )\n\n# Initialise Socket.IO server with ASGI support\nsio = socketio.AsyncServer(async_mode='asgi')\nsio_app = socketio.ASGIApp(sio, app)\n\n# Register application routes and Socket.IO events\nregister_routes(app)\nregister_sockets(sio, redis_manager)\n\n\ndef run_server():\n    \"\"\"\n    Run the application using Uvicorn ASGI server.\n    \"\"\"\n    uvicorn.run(\n        'examples.streaming_web.backend.app:sio_app',\n        host='127.0.0.1',\n        port=8000,\n        log_level='info',\n    )\n\n\nif __name__ == '__main__':\n    run_server()\n"}
{"type": "source_file", "path": "examples/YOLO_server_api/backend/routers.py", "content": "from __future__ import annotations\n\nimport base64\nimport datetime\nfrom asyncio.log import logger\nfrom pathlib import Path\n\nfrom fastapi import APIRouter\nfrom fastapi import Body\nfrom fastapi import Depends\nfrom fastapi import File\nfrom fastapi import Form\nfrom fastapi import HTTPException\nfrom fastapi import UploadFile\nfrom fastapi_jwt import JwtAuthorizationCredentials\nfrom pydantic import BaseModel\nfrom redis.asyncio import Redis\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom werkzeug.utils import secure_filename\n\nfrom .auth import create_token_logic\nfrom .auth import UserLogin\nfrom .cache import custom_rate_limiter\nfrom .cache import jwt_access\nfrom .detection import compile_detection_data\nfrom .detection import convert_to_image\nfrom .detection import get_prediction_result\nfrom .detection import process_labels\nfrom .model_files import get_new_model_file\nfrom .model_files import update_model_file\nfrom .models import DetectionModelManager\nfrom .models import get_db\nfrom .redis_pool import get_redis_pool\nfrom .user_operation import add_user\nfrom .user_operation import delete_user\nfrom .user_operation import set_user_active_status\nfrom .user_operation import update_password\nfrom .user_operation import update_username\n\n# Define routers for different functionalities\nauth_router = APIRouter()\ndetection_router = APIRouter()\nuser_management_router = APIRouter()\nmodel_management_router = APIRouter()\n\n# Load detection models\nmodel_loader = DetectionModelManager()\n\n\n# Authentication APIs\n@auth_router.post('/api/token')\nasync def create_token_endpoint(\n    user: UserLogin,\n    db: AsyncSession = Depends(get_db),\n    redis_pool: Redis = Depends(get_redis_pool),\n):\n    \"\"\"\n    Endpoint to authenticate a user and generate an access token.\n    \"\"\"\n    # Call the create_token_logic function to generate the token\n    return await create_token_logic(\n        user=user,\n        db=db,\n        redis_pool=redis_pool,\n        jwt_access=jwt_access,  # Use the jwt_access dependency\n        max_jti=2,  # Set the maximum number of JTI values\n    )\n\n# Detection APIs\n\n\nclass DetectionRequest(BaseModel):\n    \"\"\"\n    Represents the input format for the object detection endpoint.\n    \"\"\"\n    image: UploadFile\n    model: str\n\n\n@detection_router.post('/api/detect')\nasync def detect(\n    image: UploadFile = File(...),\n    model: str = 'yolo11n',\n    credentials: JwtAuthorizationCredentials = Depends(jwt_access),\n    remaining_requests: int = Depends(custom_rate_limiter),\n) -> list[list[float | int]]:\n    \"\"\"\n    Processes the uploaded image to detect objects based on\n    the specified model.\n\n    Args:\n        image (UploadFile): The uploaded image file for object detection.\n        model (str): The model name to be used for detection\n             (default is 'yolo11n').\n        credentials (JwtAuthorizationCredentials): The JWT credentials for\n            authorisation.\n        remaining_requests (int): The remaining number of allowed requests.\n\n    Returns:\n        List[List[float | int]]: A list containing detection data including\n        bounding boxes, confidence scores, and labels.\n    \"\"\"\n    print(f\"Authenticated user: {credentials.subject}\")\n    print(f\"Remaining requests: {remaining_requests}\")\n\n    # Retrieve image data and convert to OpenCV format\n    data: bytes = await image.read()\n    img = await convert_to_image(data)\n\n    # Load the specified model for detection\n    model_instance = model_loader.get_model(model)\n\n    if model_instance is None:\n        raise HTTPException(status_code=404, detail='Model not found')\n\n    # Perform object detection on the uploaded image\n    result = await get_prediction_result(img, model_instance)\n\n    # Compile and process the detection results\n    datas = compile_detection_data(result)\n    datas = await process_labels(datas)\n    return datas\n\n\n# User Management APIs\nclass UserCreate(BaseModel):\n    \"\"\"\n    Represents the data required to create a user.\n    \"\"\"\n    username: str\n    password: str\n    role: str = 'user'\n\n\n@user_management_router.post('/api/add_user')\nasync def add_user_route(\n    user: UserCreate,\n    db: AsyncSession = Depends(get_db),\n    credentials: JwtAuthorizationCredentials = Depends(jwt_access),\n) -> dict:\n    \"\"\"\n    Endpoint to add a new user to the system.\n\n    Args:\n        user (UserCreate): The data required to create the user.\n        db (AsyncSession): The database session dependency.\n\n    Returns:\n        dict: A success message if the operation is successful.\n\n    Raises:\n        HTTPException: If user creation fails.\n    \"\"\"\n    print(f\"UserCreate: {user}\")\n\n    if credentials.subject['role'] not in ['admin']:\n        raise HTTPException(\n            status_code=400,\n            detail=(\n                'Invalid role. Must be one of: '\n                'admin, model_manage, user, guest.'\n            ),\n        )\n    result = await add_user(user.username, user.password, user.role, db)\n    if result['success']:\n        logger.info(result['message'])\n        return {'message': 'User added successfully.'}\n    logger.error(f\"Add User Error: {result['message']}\")\n    raise HTTPException(\n        status_code=400 if result['error'] == 'IntegrityError' else 500,\n        detail='Failed to add user.',\n    )\n\n\nclass DeleteUser(BaseModel):\n    \"\"\"\n    Represents the data required to delete a user.\n    \"\"\"\n    username: str\n\n\n@user_management_router.post('/api/delete_user')\nasync def delete_user_route(\n    user: DeleteUser,\n    db: AsyncSession = Depends(get_db),\n    credentials: JwtAuthorizationCredentials = Depends(jwt_access),\n) -> dict:\n    \"\"\"\n    Endpoint to delete a user.\n\n    Args:\n        user (DeleteUser): The data required to delete the user.\n        db (AsyncSession): The database session dependency.\n        credentials (JwtAuthorizationCredentials): The JWT credentials\n        for authorisation.\n\n    Returns:\n        dict: A success message if the operation is successful.\n\n    Raises:\n        HTTPException: If user deletion fails.\n    \"\"\"\n    if credentials.subject['role'] not in ['admin']:\n        raise HTTPException(\n            status_code=403,\n            detail='Permission denied. Admin role required.',\n        )\n\n    result = await delete_user(user.username, db)\n    if result['success']:\n        logger.info(result['message'])\n        return {'message': 'User deleted successfully.'}\n\n    logger.error(f\"Delete User Error: {result['message']}\")\n    raise HTTPException(\n        status_code=404 if result['error'] == 'NotFound' else 500,\n        detail='Failed to delete user.',\n    )\n\n\nclass UpdateUsername(BaseModel):\n    \"\"\"\n    Represents the data required to update a user's username.\n    \"\"\"\n    old_username: str\n    new_username: str\n\n\n@user_management_router.put('/api/update_username')\nasync def update_username_route(\n    update_data: UpdateUsername,\n    db: AsyncSession = Depends(get_db),\n    credentials: JwtAuthorizationCredentials = Depends(jwt_access),\n) -> dict:\n    \"\"\"\n    Endpoint to update a user's username.\n\n    Args:\n        update_data (UpdateUsername): The data required for the update.\n        db (AsyncSession): The database session dependency.\n\n    Returns:\n        dict: A success message if the operation is successful.\n\n    Raises:\n        HTTPException: If the username update fails.\n    \"\"\"\n    if credentials.subject['role'] not in ['admin']:\n        raise HTTPException(\n            status_code=400,\n            detail=(\n                'Invalid role. Must be one of: '\n                'admin, model_manage, user, guest.'\n            ),\n        )\n\n    result = await update_username(\n        update_data.old_username, update_data.new_username, db,\n    )\n    if result['success']:\n        logger.info(result['message'])\n        return {'message': 'Username updated successfully.'}\n    logger.error(f\"Update Username Error: {result['message']}\")\n    raise HTTPException(\n        status_code=400 if result['error'] == 'IntegrityError' else 404,\n        detail='Failed to update username.',\n\n    )\n\n\nclass UpdatePassword(BaseModel):\n    \"\"\"\n    Represents the data required to update a user's password.\n    \"\"\"\n    username: str\n    new_password: str\n    role: str = 'user'\n\n\n@user_management_router.put('/api/update_password')\nasync def update_password_route(\n    update_data: UpdatePassword,\n    db: AsyncSession = Depends(get_db),\n    credentials: JwtAuthorizationCredentials = Depends(jwt_access),\n) -> dict:\n    \"\"\"\n    Endpoint to update a user's password.\n\n    Args:\n        update_data (UpdatePassword): The data required for the update.\n        db (AsyncSession): The database session dependency.\n\n    Returns:\n        dict: A success message if the operation is successful.\n\n    Raises:\n        HTTPException: If the password update fails.\n    \"\"\"\n    if credentials.subject['role'] not in ['admin']:\n        raise HTTPException(\n            status_code=400,\n            detail=(\n                'Invalid role. Must be one of: '\n                'admin, model_manage, user, guest.'\n            ),\n        )\n\n    result = await update_password(\n        update_data.username, update_data.new_password, db,\n    )\n    if result['success']:\n        logger.info(result['message'])\n        return {'message': 'Password updated successfully.'}\n    logger.error(f\"Update Password Error: {result['message']}\")\n    raise HTTPException(\n        status_code=404 if result['error'] == 'NotFound' else 500,\n        detail='Failed to update password.',\n    )\n\n\nclass SetUserActiveStatus(BaseModel):\n    \"\"\"\n    Represents the data required to update a user's active status.\n\n    Attributes:\n        username (str): The username of the user.\n        is_active (bool): The new active status to set.\n    \"\"\"\n    username: str\n    is_active: bool\n\n\n@user_management_router.put('/api/set_user_active_status')\nasync def set_user_active_status_route(\n    user_status: SetUserActiveStatus,  # \n    db: AsyncSession = Depends(get_db),\n    credentials: JwtAuthorizationCredentials = Depends(jwt_access),\n) -> dict:\n    \"\"\"\n    Endpoint to update a user's active status.\n\n    Args:\n        user_status (SetUserActiveStatus): The data containing username\n            and active status to update.\n        db (AsyncSession): The database session dependency.\n        credentials (JwtAuthorizationCredentials): The JWT credentials\n        for authorisation.\n\n    Returns:\n        dict: A success message if the operation is successful.\n\n    Raises:\n        HTTPException: If the status update fails or permissions are invalid.\n    \"\"\"\n    if credentials.subject['role'] != 'admin':\n        raise HTTPException(\n            status_code=403,\n            detail='Admin privileges are required to update user status.',\n        )\n\n    result = await set_user_active_status(\n        username=user_status.username,\n        is_active=user_status.is_active,\n        db=db,\n    )\n    if result['success']:\n        logger.info(result['message'])\n        return {'message': 'User active status updated successfully.'}\n    logger.error(f\"Set Active Status Error: {result['message']}\")\n    raise HTTPException(\n        status_code=404 if result['error'] == 'NotFound' else 500,\n        detail='Failed to update active status.',\n    )\n\n\n# Model Management APIs\nclass ModelFileUpdate(BaseModel):\n    \"\"\"\n    Represents the data required to update a model file.\n    \"\"\"\n    model: str\n    file: UploadFile\n\n    @classmethod\n    def as_form(\n        cls,\n        model: str = Form(...),\n        file: UploadFile = File(...),\n    ) -> ModelFileUpdate:\n        \"\"\"\n        Enables the use of ModelFileUpdate as a FastAPI dependency\n        with form inputs.\n\n        Args:\n            model (str): The name of the model.\n            file (UploadFile): The file to upload.\n\n        Returns:\n            ModelFileUpdate: An instance of ModelFileUpdate populated\n            with the inputs.\n        \"\"\"\n        return cls(model=model, file=file)\n\n\n@model_management_router.post('/api/model_file_update')\nasync def model_file_update(\n    data: ModelFileUpdate = Depends(ModelFileUpdate.as_form),\n    credentials: JwtAuthorizationCredentials = Depends(jwt_access),\n) -> dict:\n    \"\"\"\n    Endpoint to update a model file.\n\n    Args:\n        data (ModelFileUpdate): The data required to update the model file.\n        credentials (JwtAuthorizationCredentials): The JWT credentials\n        for authorisation.\n\n    Returns:\n        dict: A success message if the operation is successful.\n    \"\"\"\n    if credentials.subject['role'] not in ['admin', 'model_manage']:\n        raise HTTPException(\n            status_code=403,\n            detail=(\n                \"Permission denied. Role must be 'admin' \"\n                \"or 'model_manage'.\"\n            ),\n        )\n\n    try:\n        # Ensure the filename is secure\n        filename = data.file.filename or 'default_model_name'\n        secure_file_name = secure_filename(filename)\n        temp_dir = Path('/tmp')\n        temp_path = temp_dir / secure_file_name\n\n        # Check if the path is within the intended directory\n        if not temp_path.resolve().parent == temp_dir:\n            logger.error(f\"Invalid file path detected: {temp_path}\")\n            raise HTTPException(status_code=400, detail='Invalid file path.')\n\n        # Write file to disk\n        with temp_path.open('wb') as temp_file:\n            temp_file.write(await data.file.read())\n\n        # Update the model file\n        await update_model_file(data.model, temp_path)\n\n        # Log and return success\n        logger.info(f\"Model {data.model} updated successfully.\")\n        return {'message': f'Model {data.model} updated successfully.'}\n    except ValueError as e:\n        logger.error(f\"Model update validation error: {e}\")\n        raise HTTPException(status_code=400, detail=str(e))\n    except OSError as e:\n        logger.error(f\"Model update I/O error: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n    finally:\n        # Clean up temporary file\n        if temp_path.exists():\n            temp_path.unlink()\n\n\nclass UpdateModelRequest(BaseModel):\n    \"\"\"\n    Represents the data required to retrieve a new model file.\n\n    Attributes:\n        model (str): The name of the model.\n        last_update_time (str): The last update time of the model file\n        in ISO format.\n    \"\"\"\n    model: str\n    last_update_time: str\n\n\n@model_management_router.post('/api/get_new_model')\nasync def get_new_model(\n    update_request: UpdateModelRequest = Body(...),\n) -> dict:\n    \"\"\"\n    Endpoint to retrieve the new model file for a specific model.\n\n    Args:\n        update_request (UpdateModelRequest): The request data containing\n        model name and last update time.\n\n    Returns:\n        dict: The new model file if available\n        or a message indicating no update.\n    \"\"\"\n    try:\n        # Extract data from the request\n        model = update_request.model\n        last_update_time = update_request.last_update_time\n\n        # Parse the last update time provided by the user\n        user_last_update = datetime.datetime.fromisoformat(last_update_time)\n\n        # Check for a new model file\n        model_file_content = await get_new_model_file(model, user_last_update)\n        if model_file_content:\n            logger.info(f\"Newer model file for {model} retrieved.\")\n            return {\n                'message': f\"Model {model} is updated.\",\n                'model_file': base64.b64encode(model_file_content).decode(),\n            }\n\n        # No update required\n        logger.info(f\"No update required for model {model}.\")\n        return {'message': f\"Model {model} is up to date.\"}\n    except ValueError as e:\n        logger.error(f\"Validation error: {e}\")\n        raise HTTPException(status_code=400, detail=str(e))\n    except Exception as e:\n        logger.error(f\"Error retrieving model: {e}\")\n        raise HTTPException(\n            status_code=500, detail='Failed to retrieve model.',\n        )\n"}
{"type": "source_file", "path": "src/notifiers/line_notifier_message_api.py", "content": "from __future__ import annotations\n\nimport json\nimport logging\nimport os\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom typing import TypedDict\n\nimport cloudinary.api\nimport cloudinary.uploader\nimport cv2\nimport numpy as np\nimport requests\nfrom dotenv import load_dotenv\n\n\nclass InputData(TypedDict):\n    message: str\n    image: bytes | None\n\n\nclass ResultData(TypedDict):\n    response_code: int\n\n\nclass LineMessenger:\n    \"\"\"\n    A class for managing notifications sent\n    via the LINE Messaging API and Cloudinary image upload.\n    \"\"\"\n\n    def __init__(\n        self,\n        channel_access_token: str | None = None,\n        image_record_file: str = 'config/image_records.json',\n        check_interval_days: int = 1,\n    ) -> None:\n        \"\"\"\n        Initialises the LineMessenger instance.\n\n        Args:\n            channel_access_token (Optional[str]): The LINE Messaging API\n                channel access token.\n            image_record_file (str): Path to JSON file to store image records.\n            check_interval_days (int): Number of days to check for old images.\n        \"\"\"\n        load_dotenv()\n        self.channel_access_token = channel_access_token or os.getenv(\n            'LINE_CHANNEL_ACCESS_TOKEN',\n        )\n\n        if not self.channel_access_token:\n            raise ValueError(\n                'LINE_CHANNEL_ACCESS_TOKEN not provided '\n                'or in environment variables.',\n            )\n\n        # Configure Cloudinary\n        cloudinary.config(\n            cloud_name=os.getenv('CLOUDINARY_CLOUD_NAME'),\n            api_key=os.getenv('CLOUDINARY_API_KEY'),\n            api_secret=os.getenv('CLOUDINARY_API_SECRET'),\n        )\n\n        # Set image record file and check interval\n        self.image_record_file = image_record_file\n        self.check_interval_days = check_interval_days\n\n        # Load upload records\n        self.image_records = self.load_image_records()\n\n    def load_image_records(self) -> dict:\n        \"\"\"\n        Loads image records from the JSON file.\n        \"\"\"\n        try:\n            if os.path.exists(self.image_record_file):\n                with open(self.image_record_file) as file:\n                    return json.load(file)\n        except Exception as e:\n            logging.error(f\"Failed to load image records: {e}\")\n        return {}\n\n    def save_image_records(self) -> None:\n        \"\"\"\n        Saves image records to the JSON file.\n        \"\"\"\n        try:\n            with open(self.image_record_file, 'w') as file:\n                json.dump(self.image_records, file)\n        except Exception as e:\n            print(f\"Failed to save image records: {e}\")\n\n    def push_message(\n        self,\n        recipient_id: str,\n        message: str,\n        image_bytes: bytes | None = None,\n    ) -> int:\n        \"\"\"\n        Sends a message via LINE Messaging API, optionally including an image.\n\n        Args:\n            recipient_id (str): The recipient ID.\n            message (str): The message to send.\n            image_bytes (Optional[bytes]): The image bytes\n                to upload to Cloudinary. Defaults to None.\n\n        Returns:\n            response.status_code (int): The status code of the response.\n        \"\"\"\n        headers = {\n            'Content-Type': 'application/json',\n            'Authorization': f\"Bearer {self.channel_access_token}\",\n        }\n\n        message_data = {\n            'to': recipient_id,\n            'messages': [\n                {\n                    'type': 'text',\n                    'text': message,\n                },\n            ],\n        }\n\n        public_id = None\n\n        if image_bytes is not None:\n            # Upload image to Cloudinary\n            image_url, public_id = self.upload_image_to_cloudinary(image_bytes)\n            if not image_url:\n                raise ValueError('Failed to upload image to Cloudinary')\n\n            print(f\"Image URL: {image_url}\")\n            message_data['messages'].append({\n                'type': 'image',\n                'originalContentUrl': image_url,\n                'previewImageUrl': image_url,\n            })\n\n        # Send message via LINE API\n        response = requests.post(\n            'https://api.line.me/v2/bot/message/push',\n            headers=headers,\n            json=message_data,\n        )\n\n        # Print response for debugging\n        if response.status_code == 200:\n            print('Message sent successfully.')\n        else:\n            print(f\"Error: {response.status_code}, {response.text}\")\n\n        if public_id:\n            # Record image upload time\n            self.record_image_upload(public_id)\n            # Delete images older than 7 days, checked once a day\n            self.delete_old_images_with_interval()\n\n        return response.status_code\n\n    def upload_image_to_cloudinary(self, image_data: bytes) -> tuple[str, str]:\n        \"\"\"\n        Uploads an image to Cloudinary and returns the URL and public_id.\n\n        Args:\n            image_data (bytes): The image data to upload.\n\n        Returns:\n            tuple: The URL of the uploaded image and its public_id.\n        \"\"\"\n        try:\n            response = cloudinary.uploader.upload(\n                image_data, resource_type='image',\n            )\n            return response['secure_url'], response['public_id']\n        except Exception as e:\n            print(f\"Failed to upload image to Cloudinary: {e}\")\n            return '', ''\n\n    def record_image_upload(self, public_id: str) -> None:\n        \"\"\"\n        Records the image upload time in the JSON file.\n        \"\"\"\n        self.image_records[public_id] = datetime.now().isoformat()\n        self.save_image_records()\n\n    def delete_old_images_with_interval(self) -> None:\n        \"\"\"\n        Deletes images that have been uploaded for more than 7 days,\n        and only checks every day.\n        \"\"\"\n        last_checked = self.image_records.get('last_checked')\n        if last_checked is None:\n            # If last_checked is not recorded, check immediately\n            should_check = True\n        else:\n            try:\n                last_checked_time = datetime.fromisoformat(last_checked)\n                now = datetime.now()\n                interval = timedelta(days=self.check_interval_days)\n                should_check = now - last_checked_time > interval\n            except ValueError:\n                # If last_checked is not a valid datetime, check immediately\n                should_check = True\n\n        if should_check:\n            self.image_records['last_checked'] = datetime.now().isoformat()\n            self.delete_old_images()\n            self.save_image_records()\n\n    def delete_old_images(self) -> None:\n        \"\"\"\n        Deletes images that have been uploaded for more than 7 days.\n        \"\"\"\n        now = datetime.now()\n        expired_public_ids = [\n            public_id for public_id, upload_time in self.image_records.items()\n            if public_id != 'last_checked'\n            and now - datetime.fromisoformat(upload_time) > timedelta(days=7)\n        ]\n\n        for public_id in expired_public_ids:\n            self.delete_image_from_cloudinary(public_id)\n            # Remove expired image from records\n            del self.image_records[public_id]\n\n        # Save records after deletion\n        self.save_image_records()\n\n    def delete_image_from_cloudinary(self, public_id: str) -> None:\n        \"\"\"\n        Deletes an image from Cloudinary using its public_id.\n\n        Args:\n            public_id (str): The public_id of the image to delete.\n        \"\"\"\n        try:\n            response = cloudinary.uploader.destroy(public_id)\n            if response.get('result') == 'ok':\n                print(\n                    f\"Image with public_id {public_id} successfully \"\n                    'deleted from Cloudinary.',\n                )\n            else:\n                print(\n                    f\"Failed to delete image with public_id {public_id}. \"\n                    f\"Response: {response}\",\n                )\n        except Exception as e:\n            print(f\"Error deleting image from Cloudinary: {e}\")\n\n\n# Example usage\ndef main():\n    channel_access_token = os.getenv(\n        'LINE_CHANNEL_ACCESS_TOKEN', 'YOUR_LINE_CHANNEL',\n    )\n    recipient_id = os.getenv('LINE_RECIPIENT_ID', 'YOUR_RECIPIENT_ID')\n\n    messenger = LineMessenger(\n        channel_access_token=channel_access_token,\n        recipient_id=recipient_id,\n    )\n\n    message = 'Hello, LINE Messaging API!'\n\n    # Create a black image for testing\n    height, width = 480, 640\n    frame_with_detections = np.zeros((height, width, 3), dtype=np.uint8)\n    _, buffer = cv2.imencode('.png', frame_with_detections)\n    frame_bytes = buffer.tobytes()\n\n    response_code = messenger.push_message(message, image_bytes=frame_bytes)\n    print(f\"Response code: {response_code}\")\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "src/notifiers/messenger_notifier.py", "content": "from __future__ import annotations\n\nimport os\nfrom io import BytesIO\n\nimport numpy as np\nimport requests\nfrom dotenv import load_dotenv\nfrom PIL import Image\n\n\nclass MessengerNotifier:\n    \"\"\"\n    A class to handle sending notifications through Facebook Messenger\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialises the MessengerNotifier.\n        \"\"\"\n        load_dotenv()\n\n    def send_notification(\n        self,\n        recipient_id: str,\n        message: str,\n        image: np.ndarray | None = None,\n        page_access_token: str | None = None,\n    ) -> int:\n        \"\"\"\n        Sends a notification to a specified recipient via Facebook Messenger.\n\n        Args:\n            recipient_id (str): The recipient's ID.\n            message (str): The text message to send.\n            image (np.ndarray): Optional image as a NumPy array (RGB format).\n            page_access_token (str, optional): The token for the Facebook page.\n                Defaults to environment variable 'FACEBOOK_PAGE_ACCESS_TOKEN'.\n\n        Returns:\n            int: The HTTP status code of the response.\n\n        Raises:\n            ValueError: If 'FACEBOOK_PAGE_ACCESS_TOKEN' is missing.\n\n        Notes:\n            - If image is provided, it sends a message with image attachment.\n            - Otherwise, sends a text message.\n        \"\"\"\n        page_access_token = page_access_token or os.getenv(\n            'FACEBOOK_PAGE_ACCESS_TOKEN',\n        )\n        if not page_access_token:\n            raise ValueError('FACEBOOK_PAGE_ACCESS_TOKEN missing.')\n\n        headers = {'Authorization': f\"Bearer {page_access_token}\"}\n        url = (\n            f\"https://graph.facebook.com/v11.0/me/messages?\"\n            f\"access_token={page_access_token}\"\n        )\n\n        if image is not None:\n            # Prepare image data\n            image_pil = Image.fromarray(image)\n            buffer = BytesIO()\n            image_pil.save(buffer, format='PNG')\n            buffer.seek(0)\n            files = {'filedata': ('image.png', buffer, 'image/png')}\n\n            # Send message with image attachment\n            response = requests.post(\n                url=url,\n                headers=headers,\n                files=files,\n                data={\n                    'recipient': f'{{\"id\":\"{recipient_id}\"}}',\n                    'message': '{\"attachment\":{\"type\":\"image\",\"payload\":{}}}',\n                },\n            )\n        else:\n            # Send plain text message\n            payload = {\n                'message': {'text': message},\n                'recipient': {'id': recipient_id},\n            }\n            response = requests.post(\n                url=url,\n                headers=headers,\n                json=payload,\n            )\n\n        return response.status_code\n\n\n# Example usage\ndef main():\n    notifier = MessengerNotifier()\n    recipient_id = 'your_recipient_id_here'\n    message = 'Hello, Messenger!'\n    image = np.zeros((100, 100, 3), dtype=np.uint8)  # Example image (black)\n    page_access_token = 'your_page_access_token_here'\n    response_code = notifier.send_notification(\n        recipient_id,\n        message,\n        image=image,\n        page_access_token=page_access_token,\n    )\n    print(f\"Response code: {response_code}\")\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "src/notifiers/line_notifier.py", "content": "from __future__ import annotations\n\nimport os\nfrom io import BytesIO\nfrom typing import TypedDict\n\nimport aiohttp\nimport numpy as np\nfrom dotenv import load_dotenv\nfrom PIL import Image\n\n\nclass InputData(TypedDict):\n    \"\"\"\n    A type definition for input data to the notifier.\n\n    Attributes:\n        message (str): The notification message.\n        image (np.ndarray | None): An optional image in NumPy array format.\n    \"\"\"\n    message: str\n    image: np.ndarray | None\n\n\nclass ResultData(TypedDict):\n    \"\"\"\n    A type definition for the result of a notification request.\n\n    Attributes:\n        response_code (int): The HTTP response status code.\n    \"\"\"\n    response_code: int\n\n\nclass LineNotifier:\n    \"\"\"\n    A class for managing notifications sent via the LINE Notify API.\n\n    This class facilitates sending messages and optional images to LINE Notify,\n    handling token retrieval and image preparation.\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"\n        Initialises the LineNotifier instance.\n        \"\"\"\n        load_dotenv()\n\n    async def send_notification(\n        self,\n        message: str,\n        image: np.ndarray | bytes | None = None,\n        line_token: str | None = None,\n    ) -> int:\n        \"\"\"\n        Sends a notification via LINE Notify.\n\n        Args:\n            message (str): The message to be sent.\n            image (np.ndarray | bytes | None): An optional image, provided as\n                a NumPy array or bytes (e.g., encoded image data).\n            line_token (str | None): The LINE Notify token. If not provided,\n                attempts to load from environment variables.\n\n        Returns:\n            int: The HTTP response status code from LINE Notify.\n\n        Raises:\n            ValueError: If the LINE Notify token is not provided or not found\n                in environment variables.\n        \"\"\"\n        if not line_token:\n            line_token = os.getenv('LINE_NOTIFY_TOKEN')\n        if not line_token:\n            raise ValueError(\n                'LINE_NOTIFY_TOKEN not provided '\n                'or found in environment variables.',\n            )\n\n        # Prepare the payload and headers for the request.\n        payload = {'message': message}\n        headers = {'Authorization': f\"Bearer {line_token}\"}\n\n        # Use FormData to handle file attachment and form submission.\n        form = aiohttp.FormData()\n        # Add message to form data.\n        for key, value in payload.items():\n            form.add_field(key, value)\n\n        if image is not None:\n            # Prepare the image for upload.\n            image_buffer = self._prepare_image_file(image)\n            form.add_field(\n                'imageFile', image_buffer,\n                filename='image.png', content_type='image/png',\n            )\n\n        # Send the request asynchronously using aiohttp.\n        async with aiohttp.ClientSession() as session:\n            async with session.post(\n                'https://notify-api.line.me/api/notify',\n                headers=headers,\n                data=form,\n            ) as response:\n                return response.status\n\n    def _prepare_image_file(self, image: np.ndarray | bytes) -> BytesIO:\n        \"\"\"Prepares an image file for sending.\n\n        Converts a NumPy array or raw bytes into a PNG image suitable for\n        upload.\n\n        Args:\n            image (np.ndarray | bytes): The image to prepare.\n\n        Returns:\n            BytesIO: A binary stream containing the PNG image.\n        \"\"\"\n        if isinstance(image, bytes):\n            # If the image is in bytes, decode it into a NumPy array.\n            image = np.array(Image.open(BytesIO(image)))\n        # Convert the NumPy array into a PIL Image.\n        image_pil = Image.fromarray(image)\n        # Save the image into a binary buffer in PNG format.\n        buffer = BytesIO()\n        image_pil.save(buffer, format='PNG')\n        buffer.seek(0)  # Reset the buffer position for reading.\n        return buffer\n\n\nasync def main() -> None:\n    \"\"\"\n    Demonstrates usage of the LineNotifier class.\n\n    Sends a test message with an optional dummy image to LINE Notify.\n    \"\"\"\n    notifier = LineNotifier()\n    message = 'Hello, LINE Notify!'\n    # Create a dummy image (100x100 black square) for testing.\n    image = np.zeros((100, 100, 3), dtype=np.uint8)\n    response_code = await notifier.send_notification(\n        message, image=image, line_token='YOUR_LINE_TOKEN',\n    )\n    print(f\"Response code: {response_code}\")\n\n\nif __name__ == '__main__':\n    import asyncio\n    # Run the example usage function in an asynchronous event loop.\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "src/notifiers/broadcast_notifier.py", "content": "from __future__ import annotations\n\nimport logging\n\nimport requests\n\n\nclass BroadcastNotifier:\n    \"\"\"\n    A class to connect to a broadcast system and send messages.\n    \"\"\"\n\n    def __init__(self, broadcast_url: str):\n        \"\"\"\n        Initialises the BroadcastNotifier with the broadcast system's URL.\n\n        Args:\n            broadcast_url (str): The URL of the broadcast API or service.\n        \"\"\"\n        self.broadcast_url = broadcast_url\n        self.logger = logging.getLogger(__name__)\n\n    def broadcast_message(self, message: str) -> bool:\n        \"\"\"\n        Sends a message to the broadcast system.\n\n        Args:\n            message (str): The message to broadcast.\n\n        Returns:\n            bool: True if the message was successfully sent, False otherwise.\n        \"\"\"\n        try:\n            # Example of sending a POST request to the broadcast system's API\n            response = requests.post(\n                self.broadcast_url, json={'message': message},\n            )\n\n            if response.status_code == 200:\n                self.logger.info(f\"Message broadcast successfully: {message}\")\n                return True\n            else:\n                self.logger.error(\n                    f\"Failed to broadcast message: {message}. \"\n                    f\"Status code: {response.status_code}\",\n                )\n                return False\n\n        except requests.exceptions.RequestException as e:\n            self.logger.error(f\"Error broadcasting message: {e}\")\n            return False\n\n\ndef main():\n    # Configure logging\n    logging.basicConfig(level=logging.INFO)\n\n    # Initialise the BroadcastNotifier with the broadcast system URL\n    notifier = BroadcastNotifier('http://localhost:8080/broadcast')\n\n    # Send a test message\n    status = notifier.broadcast_message('Test broadcast message')\n    print(f\"Broadcast status: {status}\")\n\n\n# Example usage:\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "src/notifiers/__init__.py", "content": "from __future__ import annotations\n\nfrom .broadcast_notifier import BroadcastNotifier\nfrom .line_notifier import LineNotifier\nfrom .line_notifier_message_api import LineMessenger\nfrom .messenger_notifier import MessengerNotifier\nfrom .telegram_notifier import TelegramNotifier\nfrom .wechat_notifier import WeChatNotifier\n\n__all__ = [\n    'BroadcastNotifier',\n    'MessengerNotifier',\n    'TelegramNotifier',\n    'LineNotifier',\n    'WeChatNotifier',\n    'LineMessenger',\n]\n"}
{"type": "source_file", "path": "src/utils.py", "content": "from __future__ import annotations\n\nimport asyncio\nimport base64\nimport logging\nimport os\nfrom datetime import datetime\nfrom typing import Any\n\nimport cv2\nimport numpy as np\nimport redis.asyncio as redis\nfrom shapely.geometry import MultiPoint\nfrom shapely.geometry import Point\nfrom shapely.geometry import Polygon\nfrom sklearn.cluster import HDBSCAN\nfrom watchdog.events import FileSystemEventHandler\n\nfrom src.lang_config import Translator\n\n\nclass Utils:\n    \"\"\"\n    A class to provide utility functions.\n    \"\"\"\n\n    @staticmethod\n    def is_expired(expire_date_str: str | None) -> bool:\n        \"\"\"\n        Check if the given expire date string is expired.\n\n        Args:\n            expire_date_str (str | None): The expire date string\n                in ISO 8601 format.\n\n        Returns:\n            bool: True if expired, False otherwise.\n        \"\"\"\n        if expire_date_str:\n            try:\n                expire_date = datetime.fromisoformat(expire_date_str)\n                return datetime.now() > expire_date\n            except ValueError:\n                # If the string cannot be parsed as a valid ISO 8601 date\n                return False\n        return False\n\n    @staticmethod\n    def encode(value: str) -> str:\n        \"\"\"\n        Encode a value into a URL-safe Base64 string.\n\n        Args:\n            value (str): The value to encode.\n\n        Returns:\n            str: The encoded string.\n        \"\"\"\n        return base64.urlsafe_b64encode(\n            value.encode('utf-8'),\n        ).decode('utf-8')\n\n    @staticmethod\n    def encode_frame(frame: Any) -> bytes | None:\n        try:\n            _, buffer = cv2.imencode('.png', frame)\n            return buffer.tobytes()\n        except Exception as e:\n            logging.error(f\"Error encoding frame: {e}\")\n            return None\n\n    @staticmethod\n    def generate_message(\n        stream_name: str,\n        detection_time: datetime,\n        warnings: list[str],\n        controlled_zone_warning: list[str],\n        language: str,\n        is_working_hour: bool,\n    ) -> str | None:\n        \"\"\"\n        Generate a message to send to the notification service.\n\n        Args:\n            stream_name (str): The name of the stream.\n            detection_time (datetime): The time of detection.\n            warnings (list[str]): The list of warnings.\n            controlled_zone_warning (list[str]):\n                The list of controlled zone warnings.\n            language (str): The language for the warnings.\n            is_working_hour (bool): Whether it is working hours.\n\n        Returns:\n            str | None: The message to send, or None if no message to send.\n        \"\"\"\n        if is_working_hour and warnings:\n            translated_warnings = Translator.translate_warning(\n                tuple(warnings), language,\n            )\n            return (\n                f\"{stream_name}\\n[{detection_time}]\\n\"\n                + '\\n'.join(translated_warnings)\n            )\n\n        if not is_working_hour and controlled_zone_warning:\n            translated_controlled_zone_warning = (\n                Translator.translate_warning(\n                    tuple(controlled_zone_warning), language,\n                )\n            )\n            return (\n                f\"{stream_name}\\n[{detection_time}]\\n\"\n                + '\\n'.join(translated_controlled_zone_warning)\n            )\n\n        return None\n\n    @staticmethod\n    def should_notify(\n        timestamp: int,\n        last_notification_time: int,\n        cooldown_period: int = 300,\n    ) -> bool:\n        \"\"\"\n        Check if a notification should be sent based on the cooldown period.\n\n        Args:\n            timestamp (int): The current timestamp.\n            last_notification_time (int):\n                The timestamp of the last notification.\n            cooldown_period (int): The cooldown period in seconds.\n\n        Returns:\n            bool: True if a notification should be sent, False otherwise.\n        \"\"\"\n        return (timestamp - last_notification_time) >= cooldown_period\n\n    @staticmethod\n    def normalise_bbox(bbox: list[float]) -> list[float]:\n        \"\"\"\n        Normalises the bounding box coordinates.\n\n        Args:\n            bbox (List[float]): The bounding box coordinates.\n\n        Returns:\n            List[float]: Normalised coordinates.\n        \"\"\"\n        left_x = min(bbox[0], bbox[2])\n        right_x = max(bbox[0], bbox[2])\n        top_y = min(bbox[1], bbox[3])\n        bottom_y = max(bbox[1], bbox[3])\n        if len(bbox) > 4:\n            return [left_x, top_y, right_x, bottom_y, bbox[4], bbox[5]]\n        return [left_x, top_y, right_x, bottom_y]\n\n    @staticmethod\n    def normalise_data(datas: list[list[float]]) -> list[list[float]]:\n        \"\"\"\n        Normalises a list of bounding box data.\n\n        Args:\n            datas (List[List[float]]): List of bounding box data.\n\n        Returns:\n            List[List[float]]: Normalised data.\n        \"\"\"\n        return [Utils.normalise_bbox(data[:4] + data[4:]) for data in datas]\n\n    @staticmethod\n    def overlap_percentage(bbox1: list[float], bbox2: list[float]) -> float:\n        \"\"\"\n        Calculate the overlap percentage between two bounding boxes.\n\n        Args:\n            bbox1 (List[float]): The first bounding box.\n            bbox2 (List[float]): The second bounding box.\n\n        Returns:\n            float: The overlap percentage.\n        \"\"\"\n        # Calculate the coordinates of the intersection rectangle\n        x1 = max(bbox1[0], bbox2[0])\n        y1 = max(bbox1[1], bbox2[1])\n        x2 = min(bbox1[2], bbox2[2])\n        y2 = min(bbox1[3], bbox2[3])\n\n        # Calculate the area of the intersection rectangle\n        overlap_area = max(0, x2 - x1) * max(0, y2 - y1)\n\n        # Calculate the area of both bounding boxes\n        area1 = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])\n        area2 = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])\n\n        # Calculate the overlap percentage\n        return overlap_area / float(area1 + area2 - overlap_area)\n\n    @staticmethod\n    def is_driver(person_bbox: list[float], vehicle_bbox: list[float]) -> bool:\n        \"\"\"\n        Check if a person is a driver based on position near a vehicle.\n\n        Args:\n            person_bbox (List[float]): Bounding box of person.\n            vehicle_bbox (List[float]): Bounding box of vehicle.\n\n        Returns:\n            bool: True if the person is likely the driver, False otherwise.\n        \"\"\"\n        # Extract coordinates and dimensions of person and vehicle boxes\n        person_bottom_y = person_bbox[3]\n        person_top_y = person_bbox[1]\n        person_left_x = person_bbox[0]\n        person_right_x = person_bbox[2]\n        person_width = person_bbox[2] - person_bbox[0]\n        person_height = person_bbox[3] - person_bbox[1]\n\n        vehicle_top_y = vehicle_bbox[1]\n        vehicle_bottom_y = vehicle_bbox[3]\n        vehicle_left_x = vehicle_bbox[0]\n        vehicle_right_x = vehicle_bbox[2]\n        vehicle_height = vehicle_bbox[3] - vehicle_bbox[1]\n\n        # 1. Check vertical bottom position: person's bottom should be above\n        #    the vehicle's bottom by at least half the person's height\n        if not (\n            person_bottom_y < vehicle_bottom_y\n            and vehicle_bottom_y - person_bottom_y >= person_height / 2\n        ):\n            return False\n\n        # 2. Check horizontal position: person's edges should not extend\n        #    beyond half the width of the person from the vehicle's edges\n        if not (\n            person_left_x >= vehicle_left_x - person_width / 2\n            and person_right_x <= vehicle_right_x + person_width / 2\n        ):\n            return False\n\n        # 3. The person's top must be below the vehicle's top\n        if not (person_top_y > vehicle_top_y):\n            return False\n\n        # 4. Person's height is less than or equal to half the vehicle's height\n        if not (person_height <= vehicle_height / 2):\n            return False\n\n        return True\n\n    @staticmethod\n    def is_dangerously_close(\n        person_bbox: list[float],\n        vehicle_bbox: list[float],\n        label: str,\n    ) -> bool:\n        \"\"\"\n        Determine if a person is dangerously close to machinery or vehicles.\n\n        Args:\n            person_bbox (list[float]): Bounding box of person.\n            vehicle_bbox (list[float]): Machine/vehicle box.\n            label (str): Type of the second object ('machinery' or 'vehicle').\n\n        Returns:\n            bool: True if the person is dangerously close, False otherwise.\n        \"\"\"\n        # Calculate dimensions of the person bounding box\n        person_width = person_bbox[2] - person_bbox[0]\n        person_height = person_bbox[3] - person_bbox[1]\n        person_area = person_width * person_height\n\n        # Calculate the area of the vehicle bounding box\n        vehicle_area = (vehicle_bbox[2] - vehicle_bbox[0]) * \\\n            (vehicle_bbox[3] - vehicle_bbox[1])\n        acceptable_ratio = 0.1 if label == 'vehicle' else 0.05\n\n        # Check if person area ratio is acceptable compared to vehicle area\n        if person_area / vehicle_area > acceptable_ratio:\n            return False\n\n        # Define danger distances\n        danger_distance_horizontal = 5 * person_width\n        danger_distance_vertical = 1.5 * person_height\n\n        # Calculate min horizontal/vertical distance between person and vehicle\n        horizontal_distance = min(\n            abs(person_bbox[2] - vehicle_bbox[0]),\n            abs(person_bbox[0] - vehicle_bbox[2]),\n        )\n        vertical_distance = min(\n            abs(person_bbox[3] - vehicle_bbox[1]),\n            abs(person_bbox[1] - vehicle_bbox[3]),\n        )\n\n        # Determine if the person is dangerously close\n        return (\n            horizontal_distance <= danger_distance_horizontal\n            and vertical_distance <= danger_distance_vertical\n        )\n\n    @staticmethod\n    def detect_polygon_from_cones(\n        datas: list[list[float]],\n        clusterer: HDBSCAN,\n    ) -> list[Polygon]:\n        \"\"\"\n        Detects polygons from the safety cones in the detection data.\n\n        Args:\n            datas (list[list[float]]): The detection data.\n\n        Returns:\n            list[Polygon]: A list of polygons formed by the safety cones.\n        \"\"\"\n        if not datas:\n            return []\n\n        # Get positions of safety cones\n        cone_positions = np.array([\n            (\n                (float(data[0]) + float(data[2])) / 2,\n                (float(data[1]) + float(data[3])) / 2,\n            )\n            for data in datas if data[5] == 6\n        ])\n\n        # Check if there are at least three safety cones to form a polygon\n        if len(cone_positions) < 3:\n            return []\n\n        # Cluster the safety cones\n        labels = clusterer.fit_predict(cone_positions)\n\n        # Extract clusters\n        clusters: dict[int, list[np.ndarray]] = {}\n        for point, label in zip(cone_positions, labels):\n            if label == -1:\n                continue  # Skip noise points\n            if label not in clusters:\n                clusters[label] = []\n            clusters[label].append(point)\n\n        # Create polygons from clusters\n        polygons = []\n        for cluster_points in clusters.values():\n            if len(cluster_points) >= 3:\n                polygon = MultiPoint(cluster_points).convex_hull\n                polygons.append(polygon)\n\n        return polygons\n\n    @staticmethod\n    def calculate_people_in_controlled_area(\n        polygons: list[Polygon],\n        datas: list[list[float]],\n    ) -> int:\n        \"\"\"\n        Calculates the number of people within the safety cone area.\n\n        Args:\n            polygons (list[Polygon]): Polygons representing controlled areas.\n            datas (list[list[float]]): The detection data.\n\n        Returns:\n            int: The number of people within the controlled area.\n        \"\"\"\n        # Check if there are any detections\n        if not datas:\n            return 0\n\n        # Check if there are valid polygons\n        if not polygons:\n            return 0\n\n        # Use a set to track unique people\n        unique_people = set()\n\n        # Count the number of people within the controlled area\n        for data in datas:\n            if data[5] == 5:  # Check if it's a person\n                x_center = (data[0] + data[2]) / 2\n                y_center = (data[1] + data[3]) / 2\n                point = Point(x_center, y_center)\n                for polygon in polygons:\n                    if polygon.contains(point):\n                        # Update the set of unique people\n                        unique_people.add((x_center, y_center))\n                        break  # No need to check other polygons\n\n        return len(unique_people)\n\n\nclass FileEventHandler(FileSystemEventHandler):\n    \"\"\"\n    A class to handle file events.\n    \"\"\"\n\n    def __init__(self, file_path: str, callback, loop):\n        \"\"\"\n        Initialises the FileEventHandler instance.\n\n        Args:\n            file_path (str): The path of the file to watch.\n            callback (Callable): The function to call when file is modified.\n            loop (asyncio.AbstractEventLoop): The asyncio event loop.\n        \"\"\"\n        self.file_path = os.path.abspath(file_path)\n        self.callback = callback\n        self.loop = loop\n\n    def on_modified(self, event):\n        \"\"\"\n        Called when a file is modified.\n\n        Args:\n            event (FileSystemEvent): The event object.\n        \"\"\"\n        event_path = os.path.abspath(event.src_path)\n        if event_path == self.file_path:\n            print(f\"[DEBUG] Configuration file modified: {event_path}\")\n            asyncio.run_coroutine_threadsafe(\n                # Ensure the callback is run in the loop\n                self.callback(), self.loop,\n            )\n\n\nclass RedisManager:\n    \"\"\"\n    A class to manage Redis operations.\n    \"\"\"\n\n    def __init__(\n        self,\n        redis_host: str = '127.0.0.1',\n        redis_port: int = 6379,\n        redis_password: str = '',\n    ) -> None:\n        \"\"\"\n        Initialises RedisManager with Redis configuration details.\n\n        Args:\n            redis_host (str): The Redis server hostname.\n            redis_port (int): The Redis server port.\n            redis_password (str): The Redis password for authentication.\n        \"\"\"\n        self.redis_host: str = os.getenv('REDIS_HOST') or redis_host\n        self.redis_port: int = int(os.getenv('REDIS_PORT') or redis_port)\n        self.redis_password: str = os.getenv(\n            'REDIS_PASSWORD',\n        ) or redis_password\n\n        # Create Redis connection\n        self.redis = redis.Redis(\n            host=self.redis_host,\n            port=self.redis_port,\n            password=self.redis_password,\n            decode_responses=False,\n        )\n\n    async def set(self, key: str, value: bytes) -> None:\n        \"\"\"\n        Set a key-value pair in Redis.\n\n        Args:\n            key (str): The key under which to store the value.\n            value (bytes): The value to store (in bytes).\n        \"\"\"\n        try:\n            await self.redis.set(key, value)\n        except Exception as e:\n            logging.error(f\"Error setting Redis key {key}: {str(e)}\")\n\n    async def get(self, key: str) -> bytes | None:\n        \"\"\"\n        Retrieve a value from Redis based on the key.\n\n        Args:\n            key (str): The key whose value needs to be retrieved.\n\n        Returns:\n            bytes | None: The value if found, None otherwise.\n        \"\"\"\n        try:\n            return await self.redis.get(key)\n        except Exception as e:\n            logging.error(f\"Error retrieving Redis key {key}: {str(e)}\")\n            return None\n\n    async def delete(self, key: str) -> None:\n        \"\"\"\n        Delete a key from Redis.\n\n        Args:\n            key (str): The key to delete from Redis.\n        \"\"\"\n        try:\n            await self.redis.delete(key)\n        except Exception as e:\n            logging.error(f\"Error deleting Redis key {key}: {str(e)}\")\n\n    async def add_to_stream(\n        self,\n        stream_name: str,\n        data: dict,\n        maxlen: int = 10,\n    ) -> None:\n        \"\"\"\n        Add data to a Redis stream with a maximum length.\n\n        Args:\n            stream_name (str): The name of the Redis stream.\n            data (dict): The data to add to the stream.\n            maxlen (int): The maximum length of the stream.\n        \"\"\"\n        try:\n            await self.redis.xadd(stream_name, data, maxlen=maxlen)\n        except Exception as e:\n            logging.error(\n                f\"Error adding to Redis stream {stream_name}: {str(e)}\",\n            )\n\n    async def read_from_stream(\n        self,\n        stream_name: str,\n        last_id: str = '0',\n    ) -> list:\n        \"\"\"\n        Read data from a Redis stream.\n\n        Args:\n            stream_name (str): The name of the Redis stream.\n            last_id (str): The ID of the last read message.\n\n        Returns:\n            list: A list of messages from the stream.\n        \"\"\"\n        try:\n            return await self.redis.xread({stream_name: last_id})\n        except Exception as e:\n            logging.error(\n                f\"Error reading from Redis stream {stream_name}: {str(e)}\",\n            )\n            return []\n\n    async def delete_stream(self, stream_name: str) -> None:\n        \"\"\"\n        Delete a Redis stream.\n\n        Args:\n            stream_name (str): The name of the Redis stream to delete.\n        \"\"\"\n        try:\n            await self.redis.delete(stream_name)\n            logging.info(f\"Deleted Redis stream: {stream_name}\")\n        except Exception as e:\n            logging.error(\n                f\"Error deleting Redis stream {stream_name}: {str(e)}\",\n            )\n\n    async def close_connection(self) -> None:\n        \"\"\"\n        Close the Redis connection.\n        \"\"\"\n        try:\n            await self.redis.close()\n            logging.info('[INFO] Redis connection successfully closed.')\n        except Exception as e:\n            logging.error(f\"[ERROR] Failed to close Redis connection: {e}\")\n\n    async def store_to_redis(\n        self,\n        site: str,\n        stream_name: str,\n        frame_bytes: bytes | None,\n        warnings: list[str],\n        language: str = 'en',\n    ) -> None:\n        \"\"\"\n        Store frame and warnings to a Redis stream.\n\n        Args:\n            site (str): Site name.\n            stream_name (str): Stream name.\n            frame_bytes (optional[bytes]): Encoded frame bytes.\n            warnings (list[str]): List of warnings.\n            language (str): Language for the warnings.\n        \"\"\"\n        # Check if frame is None\n        if not frame_bytes:\n            return\n\n        # Generate the Redis key\n        key = f\"stream_frame:{Utils.encode(site)}|{Utils.encode(stream_name)}\"\n\n        # Translate warnings to the specified language\n        warnings_to_translate = warnings if warnings else ['No warning']\n        translated_warnings = Translator.translate_warning(\n            tuple(warnings_to_translate), language,\n        )\n        warnings_str = '\\n'.join(translated_warnings)\n\n        try:\n            await self.add_to_stream(\n                key,\n                {'frame': frame_bytes, 'warnings': warnings_str},\n                maxlen=10,\n            )\n        except Exception as e:\n            logging.error(f\"Error storing data to Redis: {e}\")\n"}
{"type": "source_file", "path": "src/stream_viewer.py", "content": "from __future__ import annotations\n\nimport cv2\n\n\nclass StreamViewer:\n    \"\"\"\n    A class to handle the viewing of video streams (RTSP, HTTP, etc.).\n    \"\"\"\n\n    def __init__(self, stream_url: str, window_name: str = 'Stream Viewer'):\n        \"\"\"\n        Initialises the StreamViewer instance with a stream URL\n        and a window name.\n\n        Args:\n            stream_url (str): The URL of the video stream.\n            window_name (str): The name of the window where the stream\n                               will be displayed.\n        \"\"\"\n        self.stream_url = stream_url\n        self.window_name = window_name\n        self.cap = cv2.VideoCapture(self.stream_url)\n\n    def display_stream(self):\n        \"\"\"\n        Displays the video stream in a window.\n\n        Continuously captures frames from the video stream and displays them.\n        The loop breaks when 'q' is pressed or if the stream cannot be\n        retrieved.\n        \"\"\"\n        while True:\n            # Capture the next frame from the stream.\n            ret, frame = self.cap.read()\n\n            # If the frame was successfully retrieved.\n            if ret:\n                # Display the video frame.\n                cv2.imshow(self.window_name, frame)\n\n                # Break the loop if 'q' is pressed.\n                if cv2.waitKey(1) & 0xFF == ord('q'):\n                    break\n            else:\n                print('Failed to retrieve frame.')\n                break\n\n        # Release the video capture object and close all OpenCV windows.\n        self.release_resources()\n\n    def release_resources(self):\n        \"\"\"\n        Releases resources used by the StreamViewer.\n        \"\"\"\n        self.cap.release()\n        cv2.destroyAllWindows()\n\n\ndef main():\n    \"\"\"\n    Main function to run the StreamViewer.\n    \"\"\"\n    # Replace 'vide0_url' with your stream URL.\n    video_url = (\n        'https://cctv4.kctmc.nat.gov.tw/50204bfc/'\n    )\n    viewer = StreamViewer(video_url)\n    viewer.display_stream()\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "src/notifiers/wechat_notifier.py", "content": "from __future__ import annotations\n\nimport os\nfrom io import BytesIO\n\nimport numpy as np\nimport requests\nfrom dotenv import load_dotenv\nfrom PIL import Image\n\n\nclass WeChatNotifier:\n    \"\"\"\n    A class to handle sending notifications through WeChat Work.\n    \"\"\"\n\n    def __init__(\n        self, corp_id: str | None = None,\n        corp_secret: str | None = None,\n        agent_id: int | None = None,\n    ):\n        \"\"\"\n        Initialises the WeChatNotifier with authentication details.\n\n        Args:\n            corp_id (str, optional): The corporation ID.\n                Defaults to environment variable 'WECHAT_CORP_ID'.\n            corp_secret (str, optional): The corporation secret.\n                Defaults to environment variable 'WECHAT_CORP_SECRET'.\n            agent_id (int, optional): The agent ID.\n                Defaults to env variable 'WECHAT_AGENT_ID' or 0 if not set.\n        \"\"\"\n        load_dotenv()\n        self.corp_id = corp_id or os.getenv('WECHAT_CORP_ID')\n        self.corp_secret = corp_secret or os.getenv('WECHAT_CORP_SECRET')\n        self.agent_id = agent_id or int(os.getenv('WECHAT_AGENT_ID') or 0)\n        self.access_token = self.get_access_token()\n\n    def get_access_token(self) -> str:\n        \"\"\"\n        Retrieves the access token from WeChat Work API.\n\n        Returns:\n            str: The access token string.\n        \"\"\"\n        url = (\n            f\"https://qyapi.weixin.qq.com/cgi-bin/gettoken?\"\n            f\"corpid={self.corp_id}&corpsecret={self.corp_secret}\"\n        )\n        response = requests.get(url)\n        return response.json().get('access_token')\n\n    def send_notification(\n        self,\n        user_id: str,\n        message: str,\n        image: np.ndarray | None = None,\n    ) -> dict:\n        \"\"\"\n        Sends a notification to a specified user in WeChat Work.\n\n        Args:\n            user_id (str): The user ID to send the notification to.\n            message (str): The text message to send.\n            image (np.ndarray): Optional image as a NumPy array (RGB format).\n\n        Returns:\n            dict: The response JSON from the WeChat API.\n        \"\"\"\n        url = (\n            f\"https://qyapi.weixin.qq.com/cgi-bin/message/send?\"\n            f\"access_token={self.access_token}\"\n        )\n        payload = {\n            'touser': user_id,\n            'msgtype': 'text',\n            'agentid': self.agent_id,\n            'text': {\n                'content': message,\n            },\n            'safe': 0,\n        }\n\n        if image is not None:\n            media_id = self.upload_media(image)\n            payload = {\n                'touser': user_id,\n                'msgtype': 'image',\n                'agentid': self.agent_id,\n                'image': {\n                    'media_id': media_id,\n                },\n                'safe': 0,\n            }\n\n        response = requests.post(url, json=payload)\n        return response.json()\n\n    def upload_media(self, image: np.ndarray) -> str:\n        \"\"\"\n        Uploads an image media to WeChat Work.\n\n        Args:\n            image (np.ndarray): The image as a NumPy array (RGB format).\n\n        Returns:\n            str: The media ID of the uploaded image.\n        \"\"\"\n        image_pil = Image.fromarray(image)\n        buffer = BytesIO()\n        image_pil.save(buffer, format='PNG')\n        buffer.seek(0)\n        url = (\n            f\"https://qyapi.weixin.qq.com/cgi-bin/media/upload?\"\n            f\"access_token={self.access_token}&type=image\"\n        )\n        files = {'media': ('image.png', buffer, 'image/png')}\n        response = requests.post(url, files=files)\n        return response.json().get('media_id')\n\n\n# Example usage\ndef main():\n    notifier = WeChatNotifier()\n    user_id = 'your_user_id_here'\n    message = 'Hello, WeChat!'\n    image = np.zeros((100, 100, 3), dtype=np.uint8)  # Example image (black)\n    response = notifier.send_notification(user_id, message, image=image)\n\n    # Remove sensitive data from response before logging\n    sanitized_response = {\n        k: v for k, v in response.items() if k not in [\n            'access_token', 'corp_secret',\n        ]\n    }\n    print(sanitized_response)\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "src/notifiers/telegram_notifier.py", "content": "from __future__ import annotations\n\nimport asyncio\nimport os\nfrom io import BytesIO\nfrom typing import TypedDict\n\nimport numpy as np\nfrom dotenv import load_dotenv\nfrom PIL import Image\nfrom telegram import Bot\nfrom telegram import Message\n\n\nclass InputData(TypedDict):\n    \"\"\"\n    Structure of input data for sending a Telegram notification.\n    \"\"\"\n    chat_id: str\n    message: str\n    image: np.ndarray\n\n\nclass TelegramResponse(TypedDict):\n    \"\"\"\n    Structure of the response from Telegram API.\n    \"\"\"\n    message_id: int\n    chat_id: int\n    date: int\n    text: str\n\n\nclass TelegramNotifier:\n    \"\"\"\n    A class to handle sending notifications through Telegram.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialises the TelegramNotifier.\n        \"\"\"\n        load_dotenv()\n\n    async def send_notification(\n        self,\n        chat_id: str,\n        message: str,\n        image: np.ndarray | None = None,\n        bot_token: str | None = None,\n    ) -> Message:\n        \"\"\"\n        Sends a notification to a specified Telegram chat.\n\n        Args:\n            chat_id (str): The chat ID where the notification will be sent.\n            message (str): The text message to send.\n            image (np.ndarray): An optional image in NumPy array (RGB format).\n            bot_token (str, optional): The Telegram bot token. If not provided,\n                attempts to read from environment variables.\n\n        Returns:\n            Message: The response object from Telegram API.\n\n        Raises:\n            ValueError: If 'TELEGRAM_BOT_TOKEN' is missing.\n        \"\"\"\n        bot_token = bot_token or os.getenv('TELEGRAM_BOT_TOKEN')\n        if not bot_token:\n            raise ValueError('Telegram bot token must be provided')\n        bot = Bot(token=bot_token)\n\n        if image is not None:\n            # Convert NumPy array to PIL Image\n            image_pil = Image.fromarray(image)\n            buffer = BytesIO()\n            # Save image to BytesIO buffer as PNG\n            image_pil.save(buffer, format='PNG')\n            buffer.seek(0)\n            # Send photo with caption\n            response = await bot.send_photo(\n                chat_id=chat_id,\n                photo=buffer,\n                caption=message,\n            )\n        else:\n            # Send text message\n            response = await bot.send_message(\n                chat_id=chat_id,\n                text=message,\n            )\n        return response\n\n\n# Example usage\nasync def main():\n    notifier = TelegramNotifier()\n    chat_id = 'your_chat_id_here'\n    message = 'Hello, Telegram!'\n    image = np.zeros((100, 100, 3), dtype=np.uint8)  # Example image (black)\n    bot_token = 'your_bot_token_here'\n    response = await notifier.send_notification(\n        chat_id, message, image=image, bot_token=bot_token,\n    )\n    print(response)\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "src/stream_capture.py", "content": "from __future__ import annotations\n\nimport argparse\nimport asyncio\nimport datetime\nimport gc\nfrom collections.abc import AsyncGenerator\nfrom typing import TypedDict\n\nimport cv2\nimport numpy as np\nimport speedtest\nimport streamlink\n\n\nclass InputData(TypedDict):\n    stream_url: str\n    capture_interval: int\n\n\nclass ResultData(TypedDict):\n    frame: np.ndarray\n    timestamp: float\n\n\nclass StreamCapture:\n    \"\"\"\n    A class to capture frames from a video stream.\n    \"\"\"\n\n    def __init__(self, stream_url: str, capture_interval: int = 15):\n        \"\"\"\n        Initialises the StreamCapture with the given stream URL.\n\n        Args:\n            stream_url (str): The URL of the video stream.\n            capture_interval (int, optional): The interval at which frames\n                should be captured. Defaults to 15.\n        \"\"\"\n        # Video stream URL\n        self.stream_url = stream_url\n        # Video capture object\n        self.cap: cv2.VideoCapture | None = None\n        # Frame capture interval in seconds\n        self.capture_interval = capture_interval\n        # Flag to indicate successful capture\n        self.successfully_captured = False\n\n    async def initialise_stream(self, stream_url: str) -> None:\n        \"\"\"\n        Initialises the video stream.\n\n        Args:\n            stream_url (str): The URL of the stream to initialise.\n        \"\"\"\n        self.cap = cv2.VideoCapture(stream_url)\n        self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n        # self.cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*'H264'))\n\n        if not self.cap.isOpened():\n            await asyncio.sleep(5)\n            self.cap.open(stream_url)\n\n    async def release_resources(self) -> None:\n        \"\"\"\n        Releases resources like the capture object.\n        \"\"\"\n        if self.cap:\n            self.cap.release()\n            self.cap = None\n        gc.collect()\n\n    async def execute_capture(\n        self,\n    ) -> AsyncGenerator[tuple[np.ndarray, float]]:\n        \"\"\"\n        Captures frames from the stream and yields them with timestamps.\n\n        Yields:\n            Tuple[np.ndarray, float]: The captured frame and the timestamp.\n        \"\"\"\n        await self.initialise_stream(self.stream_url)\n        last_process_time = datetime.datetime.now() - datetime.timedelta(\n            seconds=self.capture_interval,\n        )\n        fail_count = 0  # Counter for consecutive failures\n\n        while True:\n            if self.cap is None:\n                await self.initialise_stream(self.stream_url)\n\n            ret, frame = (\n                self.cap.read() if self.cap is not None else (False, None)\n            )\n\n            if not ret or frame is None:\n                fail_count += 1\n                print(\n                    'Failed to read frame, trying to reinitialise stream. '\n                    f\"Fail count: {fail_count}\",\n                )\n                await self.release_resources()\n                await self.initialise_stream(self.stream_url)\n                # Switch to generic frame capture after 5 consecutive failures\n                if fail_count >= 5 and not self.successfully_captured:\n                    print('Switching to generic frame capture method.')\n                    async for generic_frame, timestamp in (\n                        self.capture_generic_frames()\n                    ):\n                        yield generic_frame, timestamp\n                    return\n                continue\n            else:\n                # Reset fail count on successful read\n                fail_count = 0\n\n                # Mark as successfully captured\n                self.successfully_captured = True\n\n            # Process the frame if the capture interval has elapsed\n            current_time = datetime.datetime.now()\n            elapsed_time = (current_time - last_process_time).total_seconds()\n\n            # If the capture interval has elapsed, yield the frame\n            if elapsed_time >= self.capture_interval:\n                last_process_time = current_time\n                timestamp = current_time.timestamp()\n                yield frame, timestamp\n\n                # Clear memory\n                del frame, timestamp\n                gc.collect()\n\n            await asyncio.sleep(0.01)  # Adjust the sleep time as needed\n\n        await self.release_resources()\n\n    def check_internet_speed(self) -> tuple[float, float]:\n        \"\"\"\n        Checks internet speed using the Speedtest library.\n\n        Returns:\n            Tuple[float, float]: Download and upload speeds (Mbps).\n        \"\"\"\n        st = speedtest.Speedtest()\n        st.get_best_server()\n        download_speed = st.download() / 1_000_000  # Turn into Mbps\n        upload_speed = st.upload() / 1_000_000\n        return download_speed, upload_speed\n\n    def select_quality_based_on_speed(self) -> str | None:\n        \"\"\"\n        Selects stream quality based on internet speed.\n\n        Returns:\n            str: The URL of the selected stream quality.\n\n        Raises:\n            Exception: If compatible stream quality is not available.\n        \"\"\"\n        download_speed, _ = self.check_internet_speed()\n        try:\n            streams = streamlink.streams(self.stream_url)\n            available_qualities = list(streams.keys())\n            print(f\"Available qualities: {available_qualities}\")\n\n            if download_speed > 10:\n                preferred_qualities = [\n                    'best',\n                    '1080p',\n                    '720p',\n                    '480p',\n                    '360p',\n                    '240p',\n                    'worst',\n                ]\n            elif 5 < download_speed <= 10:\n                preferred_qualities = ['720p', '480p', '360p', '240p', 'worst']\n            else:\n                preferred_qualities = ['480p', '360p', '240p', 'worst']\n\n            for quality in preferred_qualities:\n                if quality in available_qualities:\n                    selected_stream = streams[quality]\n                    print(f\"Selected quality based on speed: {quality}\")\n                    return selected_stream.url\n\n            raise Exception('No compatible stream quality is available.')\n        except Exception as e:\n            print(f\"Error selecting quality based on speed: {e}\")\n            return None\n\n    async def capture_generic_frames(\n        self,\n    ) -> AsyncGenerator[tuple[np.ndarray, float]]:\n        \"\"\"\n        Captures frames from a generic stream.\n\n        Yields:\n            Tuple[np.ndarray, float]: The captured frame and the timestamp.\n        \"\"\"\n        # Select the stream quality based on internet speed\n        stream_url = self.select_quality_based_on_speed()\n        if not stream_url:\n            print('Failed to get suitable stream quality.')\n            return\n\n        # Initialise the stream with the selected URL\n        await self.initialise_stream(stream_url)\n\n        last_process_time = datetime.datetime.now()\n        fail_count = 0  # Counter for consecutive failures\n\n        while True:\n            # Read the frame from the stream\n            ret, frame = (\n                self.cap.read() if self.cap is not None else (False, None)\n            )\n\n            # Handle failed frame reads\n            if not ret or frame is None:\n                fail_count += 1\n                print(\n                    'Failed to read frame from generic stream. '\n                    f\"Fail count: {fail_count}\",\n                )\n\n                # Reinitialise the stream after 5 consecutive failures\n                if fail_count >= 5 and not self.successfully_captured:\n                    print('Reinitialising the generic stream.')\n                    await self.release_resources()\n                    await asyncio.sleep(5)\n                    stream_url = self.select_quality_based_on_speed()\n\n                    # Exit if no suitable stream quality is available\n                    if not stream_url:\n                        print('Failed to get suitable stream quality.')\n                        continue\n\n                    # Reinitialise the stream with the new URL\n                    await self.initialise_stream(stream_url)\n                    fail_count = 0\n                continue\n            else:\n                # Reset fail count on successful read\n                fail_count = 0\n\n                # Mark as successfully captured\n                self.successfully_captured = True\n\n            current_time = datetime.datetime.now()\n            elapsed_time = (current_time - last_process_time).total_seconds()\n\n            if elapsed_time >= self.capture_interval:\n                last_process_time = current_time\n                timestamp = current_time.timestamp()\n                yield frame, timestamp\n\n                # Clear memory\n                del frame, timestamp\n                gc.collect()\n\n            await asyncio.sleep(0.01)  # Adjust the sleep time as needed\n\n    def update_capture_interval(self, new_interval: int) -> None:\n        \"\"\"\n        Updates the capture interval.\n\n        Args:\n            new_interval (int): Frame capture interval in seconds.\n        \"\"\"\n        self.capture_interval = new_interval\n\n\nasync def main():\n    parser = argparse.ArgumentParser(\n        description='Capture video stream frames asynchronously.',\n    )\n    parser.add_argument(\n        '--url',\n        type=str,\n        help='Live stream URL',\n        required=True,\n    )\n    args = parser.parse_args()\n\n    stream_capture = StreamCapture(args.url)\n    async for frame, timestamp in stream_capture.execute_capture():\n        # Process the frame here\n        print(f\"Frame at {timestamp} displayed\")\n        # Release the frame resources\n        del frame\n        gc.collect()\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n"}
