{"repo_info": {"repo_name": "Pothole-Detection-Pothole-Detection-using-python-and-deep-learning", "repo_owner": "noorkhokhar99", "repo_url": "https://github.com/noorkhokhar99/Pothole-Detection-Pothole-Detection-using-python-and-deep-learning"}}
{"type": "source_file", "path": "predict.py", "content": "# Ultralytics YOLO ðŸš€, GPL-3.0 license\n\nimport hydra\nimport torch\n\nfrom ultralytics.yolo.engine.predictor import BasePredictor\nfrom ultralytics.yolo.utils import DEFAULT_CONFIG, ROOT, ops\nfrom ultralytics.yolo.utils.checks import check_imgsz\nfrom ultralytics.yolo.utils.plotting import Annotator, colors, save_one_box\n\n\nclass DetectionPredictor(BasePredictor):\n\n    def get_annotator(self, img):\n        return Annotator(img, line_width=self.args.line_thickness, example=str(self.model.names))\n\n    def preprocess(self, img):\n        img = torch.from_numpy(img).to(self.model.device)\n        img = img.half() if self.model.fp16 else img.float()  # uint8 to fp16/32\n        img /= 255  # 0 - 255 to 0.0 - 1.0\n        return img\n\n    def postprocess(self, preds, img, orig_img):\n        preds = ops.non_max_suppression(preds,\n                                        self.args.conf,\n                                        self.args.iou,\n                                        agnostic=self.args.agnostic_nms,\n                                        max_det=self.args.max_det)\n\n        for i, pred in enumerate(preds):\n            shape = orig_img[i].shape if self.webcam else orig_img.shape\n            pred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], shape).round()\n\n        return preds\n\n    def write_results(self, idx, preds, batch):\n        p, im, im0 = batch\n        log_string = \"\"\n        if len(im.shape) == 3:\n            im = im[None]  # expand for batch dim\n        self.seen += 1\n        im0 = im0.copy()\n        if self.webcam:  # batch_size >= 1\n            log_string += f'{idx}: '\n            frame = self.dataset.count\n        else:\n            frame = getattr(self.dataset, 'frame', 0)\n\n        self.data_path = p\n        # save_path = str(self.save_dir / p.name)  # im.jpg\n        self.txt_path = str(self.save_dir / 'labels' / p.stem) + ('' if self.dataset.mode == 'image' else f'_{frame}')\n        log_string += '%gx%g ' % im.shape[2:]  # print string\n        self.annotator = self.get_annotator(im0)\n\n        det = preds[idx]\n        self.all_outputs.append(det)\n        if len(det) == 0:\n            return log_string\n        for c in det[:, 5].unique():\n            n = (det[:, 5] == c).sum()  # detections per class\n            log_string += f\"{n} {self.model.names[int(c)]}{'s' * (n > 1)}, \"\n        # write\n        gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n        for *xyxy, conf, cls in reversed(det):\n            if self.args.save_txt:  # Write to file\n                xywh = (ops.xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n                line = (cls, *xywh, conf) if self.args.save_conf else (cls, *xywh)  # label format\n                with open(f'{self.txt_path}.txt', 'a') as f:\n                    f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n\n            if self.args.save or self.args.save_crop or self.args.show:  # Add bbox to image\n                c = int(cls)  # integer class\n                label = None if self.args.hide_labels else (\n                    self.model.names[c] if self.args.hide_conf else f'{self.model.names[c]} {conf:.2f}')\n                self.annotator.box_label(xyxy, label, color=colors(c, True))\n            if self.args.save_crop:\n                imc = im0.copy()\n                save_one_box(xyxy,\n                             imc,\n                             file=self.save_dir / 'crops' / self.model.model.names[c] / f'{self.data_path.stem}.jpg',\n                             BGR=True)\n\n        return log_string\n\n\n@hydra.main(version_base=None, config_path=str(DEFAULT_CONFIG.parent), config_name=DEFAULT_CONFIG.name)\ndef predict(cfg):\n    cfg.model = cfg.model or \"yolov8n.pt\"\n    cfg.imgsz = check_imgsz(cfg.imgsz, min_dim=2)  # check image size\n    cfg.source = cfg.source if cfg.source is not None else ROOT / \"assets\"\n    predictor = DetectionPredictor(cfg)\n    predictor()\n\n\nif __name__ == \"__main__\":\n    predict()\n"}
{"type": "source_file", "path": "train.py", "content": "# Ultralytics YOLO ðŸš€, GPL-3.0 license\n\nfrom copy import copy\n\nimport hydra\nimport torch\nimport torch.nn as nn\n\nfrom ultralytics.nn.tasks import DetectionModel\nfrom ultralytics.yolo import v8\nfrom ultralytics.yolo.data import build_dataloader\nfrom ultralytics.yolo.data.dataloaders.v5loader import create_dataloader\nfrom ultralytics.yolo.engine.trainer import BaseTrainer\nfrom ultralytics.yolo.utils import DEFAULT_CONFIG, colorstr\nfrom ultralytics.yolo.utils.loss import BboxLoss\nfrom ultralytics.yolo.utils.ops import xywh2xyxy\nfrom ultralytics.yolo.utils.plotting import plot_images, plot_results\nfrom ultralytics.yolo.utils.tal import TaskAlignedAssigner, dist2bbox, make_anchors\nfrom ultralytics.yolo.utils.torch_utils import de_parallel\n\n\n# BaseTrainer python usage\nclass DetectionTrainer(BaseTrainer):\n\n    def get_dataloader(self, dataset_path, batch_size, mode=\"train\", rank=0):\n        # TODO: manage splits differently\n        # calculate stride - check if model is initialized\n        gs = max(int(de_parallel(self.model).stride.max() if self.model else 0), 32)\n        return create_dataloader(path=dataset_path,\n                                 imgsz=self.args.imgsz,\n                                 batch_size=batch_size,\n                                 stride=gs,\n                                 hyp=dict(self.args),\n                                 augment=mode == \"train\",\n                                 cache=self.args.cache,\n                                 pad=0 if mode == \"train\" else 0.5,\n                                 rect=self.args.rect,\n                                 rank=rank,\n                                 workers=self.args.workers,\n                                 close_mosaic=self.args.close_mosaic != 0,\n                                 prefix=colorstr(f'{mode}: '),\n                                 shuffle=mode == \"train\",\n                                 seed=self.args.seed)[0] if self.args.v5loader else \\\n            build_dataloader(self.args, batch_size, img_path=dataset_path, stride=gs, rank=rank, mode=mode)[0]\n\n    def preprocess_batch(self, batch):\n        batch[\"img\"] = batch[\"img\"].to(self.device, non_blocking=True).float() / 255\n        return batch\n\n    def set_model_attributes(self):\n        nl = de_parallel(self.model).model[-1].nl  # number of detection layers (to scale hyps)\n        self.args.box *= 3 / nl  # scale to layers\n        # self.args.cls *= self.data[\"nc\"] / 80 * 3 / nl  # scale to classes and layers\n        self.args.cls *= (self.args.imgsz / 640) ** 2 * 3 / nl  # scale to image size and layers\n        self.model.nc = self.data[\"nc\"]  # attach number of classes to model\n        self.model.args = self.args  # attach hyperparameters to model\n        # TODO: self.model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device) * nc\n        self.model.names = self.data[\"names\"]\n\n    def get_model(self, cfg=None, weights=None, verbose=True):\n        model = DetectionModel(cfg, ch=3, nc=self.data[\"nc\"], verbose=verbose)\n        if weights:\n            model.load(weights)\n\n        return model\n\n    def get_validator(self):\n        self.loss_names = 'box_loss', 'cls_loss', 'dfl_loss'\n        return v8.detect.DetectionValidator(self.test_loader,\n                                            save_dir=self.save_dir,\n                                            logger=self.console,\n                                            args=copy(self.args))\n\n    def criterion(self, preds, batch):\n        if not hasattr(self, 'compute_loss'):\n            self.compute_loss = Loss(de_parallel(self.model))\n        return self.compute_loss(preds, batch)\n\n    def label_loss_items(self, loss_items=None, prefix=\"train\"):\n        \"\"\"\n        Returns a loss dict with labelled training loss items tensor\n        \"\"\"\n        # Not needed for classification but necessary for segmentation & detection\n        keys = [f\"{prefix}/{x}\" for x in self.loss_names]\n        if loss_items is not None:\n            loss_items = [round(float(x), 5) for x in loss_items]  # convert tensors to 5 decimal place floats\n            return dict(zip(keys, loss_items))\n        else:\n            return keys\n\n    def progress_string(self):\n        return ('\\n' + '%11s' *\n                (4 + len(self.loss_names))) % ('Epoch', 'GPU_mem', *self.loss_names, 'Instances', 'Size')\n\n    def plot_training_samples(self, batch, ni):\n        plot_images(images=batch[\"img\"],\n                    batch_idx=batch[\"batch_idx\"],\n                    cls=batch[\"cls\"].squeeze(-1),\n                    bboxes=batch[\"bboxes\"],\n                    paths=batch[\"im_file\"],\n                    fname=self.save_dir / f\"train_batch{ni}.jpg\")\n\n    def plot_metrics(self):\n        plot_results(file=self.csv)  # save results.png\n\n\n# Criterion class for computing training losses\nclass Loss:\n\n    def __init__(self, model):  # model must be de-paralleled\n\n        device = next(model.parameters()).device  # get model device\n        h = model.args  # hyperparameters\n\n        m = model.model[-1]  # Detect() module\n        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n        self.hyp = h\n        self.stride = m.stride  # model strides\n        self.nc = m.nc  # number of classes\n        self.no = m.no\n        self.reg_max = m.reg_max\n        self.device = device\n\n        self.use_dfl = m.reg_max > 1\n        self.assigner = TaskAlignedAssigner(topk=10, num_classes=self.nc, alpha=0.5, beta=6.0)\n        self.bbox_loss = BboxLoss(m.reg_max - 1, use_dfl=self.use_dfl).to(device)\n        self.proj = torch.arange(m.reg_max, dtype=torch.float, device=device)\n\n    def preprocess(self, targets, batch_size, scale_tensor):\n        if targets.shape[0] == 0:\n            out = torch.zeros(batch_size, 0, 5, device=self.device)\n        else:\n            i = targets[:, 0]  # image index\n            _, counts = i.unique(return_counts=True)\n            out = torch.zeros(batch_size, counts.max(), 5, device=self.device)\n            for j in range(batch_size):\n                matches = i == j\n                n = matches.sum()\n                if n:\n                    out[j, :n] = targets[matches, 1:]\n            out[..., 1:5] = xywh2xyxy(out[..., 1:5].mul_(scale_tensor))\n        return out\n\n    def bbox_decode(self, anchor_points, pred_dist):\n        if self.use_dfl:\n            b, a, c = pred_dist.shape  # batch, anchors, channels\n            pred_dist = pred_dist.view(b, a, 4, c // 4).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n            # pred_dist = pred_dist.view(b, a, c // 4, 4).transpose(2,3).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n            # pred_dist = (pred_dist.view(b, a, c // 4, 4).softmax(2) * self.proj.type(pred_dist.dtype).view(1, 1, -1, 1)).sum(2)\n        return dist2bbox(pred_dist, anchor_points, xywh=False)\n\n    def __call__(self, preds, batch):\n        loss = torch.zeros(3, device=self.device)  # box, cls, dfl\n        feats = preds[1] if isinstance(preds, tuple) else preds\n        pred_distri, pred_scores = torch.cat([xi.view(feats[0].shape[0], self.no, -1) for xi in feats], 2).split(\n            (self.reg_max * 4, self.nc), 1)\n\n        pred_scores = pred_scores.permute(0, 2, 1).contiguous()\n        pred_distri = pred_distri.permute(0, 2, 1).contiguous()\n\n        dtype = pred_scores.dtype\n        batch_size = pred_scores.shape[0]\n        imgsz = torch.tensor(feats[0].shape[2:], device=self.device, dtype=dtype) * self.stride[0]  # image size (h,w)\n        anchor_points, stride_tensor = make_anchors(feats, self.stride, 0.5)\n\n        # targets\n        targets = torch.cat((batch[\"batch_idx\"].view(-1, 1), batch[\"cls\"].view(-1, 1), batch[\"bboxes\"]), 1)\n        targets = self.preprocess(targets.to(self.device), batch_size, scale_tensor=imgsz[[1, 0, 1, 0]])\n        gt_labels, gt_bboxes = targets.split((1, 4), 2)  # cls, xyxy\n        mask_gt = gt_bboxes.sum(2, keepdim=True).gt_(0)\n\n        # pboxes\n        pred_bboxes = self.bbox_decode(anchor_points, pred_distri)  # xyxy, (b, h*w, 4)\n\n        _, target_bboxes, target_scores, fg_mask, _ = self.assigner(\n            pred_scores.detach().sigmoid(), (pred_bboxes.detach() * stride_tensor).type(gt_bboxes.dtype),\n            anchor_points * stride_tensor, gt_labels, gt_bboxes, mask_gt)\n\n        target_bboxes /= stride_tensor\n        target_scores_sum = target_scores.sum()\n\n        # cls loss\n        # loss[1] = self.varifocal_loss(pred_scores, target_scores, target_labels) / target_scores_sum  # VFL way\n        loss[1] = self.bce(pred_scores, target_scores.to(dtype)).sum() / target_scores_sum  # BCE\n\n        # bbox loss\n        if fg_mask.sum():\n            loss[0], loss[2] = self.bbox_loss(pred_distri, pred_bboxes, anchor_points, target_bboxes, target_scores,\n                                              target_scores_sum, fg_mask)\n\n        loss[0] *= self.hyp.box  # box gain\n        loss[1] *= self.hyp.cls  # cls gain\n        loss[2] *= self.hyp.dfl  # dfl gain\n\n        return loss.sum() * batch_size, loss.detach()  # loss(box, cls, dfl)\n\n\n@hydra.main(version_base=None, config_path=str(DEFAULT_CONFIG.parent), config_name=DEFAULT_CONFIG.name)\ndef train(cfg):\n    cfg.model = cfg.model or \"yolov8n.yaml\"\n    cfg.data = cfg.data or \"coco128.yaml\"  # or yolo.ClassificationDataset(\"mnist\")\n    # trainer = DetectionTrainer(cfg)\n    # trainer.train()\n    from ultralytics import YOLO\n    model = YOLO(cfg.model)\n    model.train(**cfg)\n\n\nif __name__ == \"__main__\":\n    \"\"\"\n    CLI usage:\n    python ultralytics/yolo/v8/detect/train.py model=yolov8n.yaml data=coco128 epochs=100 imgsz=640\n\n    TODO:\n    yolo task=detect mode=train model=yolov8n.yaml data=coco128.yaml epochs=100\n    \"\"\"\n    train()\n"}
{"type": "source_file", "path": "val.py", "content": "# Ultralytics YOLO ðŸš€, GPL-3.0 license\n\nimport os\nfrom pathlib import Path\n\nimport hydra\nimport numpy as np\nimport torch\n\nfrom ultralytics.yolo.data import build_dataloader\nfrom ultralytics.yolo.data.dataloaders.v5loader import create_dataloader\nfrom ultralytics.yolo.engine.validator import BaseValidator\nfrom ultralytics.yolo.utils import DEFAULT_CONFIG, colorstr, ops, yaml_load\nfrom ultralytics.yolo.utils.checks import check_file, check_requirements\nfrom ultralytics.yolo.utils.metrics import ConfusionMatrix, DetMetrics, box_iou\nfrom ultralytics.yolo.utils.plotting import output_to_target, plot_images\nfrom ultralytics.yolo.utils.torch_utils import de_parallel\n\n\nclass DetectionValidator(BaseValidator):\n\n    def __init__(self, dataloader=None, save_dir=None, pbar=None, logger=None, args=None):\n        super().__init__(dataloader, save_dir, pbar, logger, args)\n        self.data_dict = yaml_load(check_file(self.args.data), append_filename=True) if self.args.data else None\n        self.is_coco = False\n        self.class_map = None\n        self.metrics = DetMetrics(save_dir=self.save_dir, plot=self.args.plots)\n        self.iouv = torch.linspace(0.5, 0.95, 10)  # iou vector for mAP@0.5:0.95\n        self.niou = self.iouv.numel()\n\n    def preprocess(self, batch):\n        batch[\"img\"] = batch[\"img\"].to(self.device, non_blocking=True)\n        batch[\"img\"] = (batch[\"img\"].half() if self.args.half else batch[\"img\"].float()) / 255\n        for k in [\"batch_idx\", \"cls\", \"bboxes\"]:\n            batch[k] = batch[k].to(self.device)\n\n        nb, _, height, width = batch[\"img\"].shape\n        batch[\"bboxes\"] *= torch.tensor((width, height, width, height), device=self.device)  # to pixels\n        self.lb = [torch.cat([batch[\"cls\"], batch[\"bboxes\"]], dim=-1)[batch[\"batch_idx\"] == i]\n                   for i in range(nb)] if self.args.save_hybrid else []  # for autolabelling\n\n        return batch\n\n    def init_metrics(self, model):\n        head = model.model[-1] if self.training else model.model.model[-1]\n        val = self.data.get('val', '')  # validation path\n        self.is_coco = isinstance(val, str) and val.endswith(f'coco{os.sep}val2017.txt')  # is COCO dataset\n        self.class_map = ops.coco80_to_coco91_class() if self.is_coco else list(range(1000))\n        self.args.save_json |= self.is_coco and not self.training  # run on final val if training COCO\n        self.nc = head.nc\n        self.names = model.names\n        self.metrics.names = self.names\n        self.confusion_matrix = ConfusionMatrix(nc=self.nc)\n        self.seen = 0\n        self.jdict = []\n        self.stats = []\n\n    def get_desc(self):\n        return ('%22s' + '%11s' * 6) % ('Class', 'Images', 'Instances', 'Box(P', \"R\", \"mAP50\", \"mAP50-95)\")\n\n    def postprocess(self, preds):\n        preds = ops.non_max_suppression(preds,\n                                        self.args.conf,\n                                        self.args.iou,\n                                        labels=self.lb,\n                                        multi_label=True,\n                                        agnostic=self.args.single_cls,\n                                        max_det=self.args.max_det)\n        return preds\n\n    def update_metrics(self, preds, batch):\n        # Metrics\n        for si, pred in enumerate(preds):\n            idx = batch[\"batch_idx\"] == si\n            cls = batch[\"cls\"][idx]\n            bbox = batch[\"bboxes\"][idx]\n            nl, npr = cls.shape[0], pred.shape[0]  # number of labels, predictions\n            shape = batch[\"ori_shape\"][si]\n            correct_bboxes = torch.zeros(npr, self.niou, dtype=torch.bool, device=self.device)  # init\n            self.seen += 1\n\n            if npr == 0:\n                if nl:\n                    self.stats.append((correct_bboxes, *torch.zeros((2, 0), device=self.device), cls.squeeze(-1)))\n                    if self.args.plots:\n                        self.confusion_matrix.process_batch(detections=None, labels=cls.squeeze(-1))\n                continue\n\n            # Predictions\n            if self.args.single_cls:\n                pred[:, 5] = 0\n            predn = pred.clone()\n            ops.scale_boxes(batch[\"img\"][si].shape[1:], predn[:, :4], shape,\n                            ratio_pad=batch[\"ratio_pad\"][si])  # native-space pred\n\n            # Evaluate\n            if nl:\n                tbox = ops.xywh2xyxy(bbox)  # target boxes\n                ops.scale_boxes(batch[\"img\"][si].shape[1:], tbox, shape,\n                                ratio_pad=batch[\"ratio_pad\"][si])  # native-space labels\n                labelsn = torch.cat((cls, tbox), 1)  # native-space labels\n                correct_bboxes = self._process_batch(predn, labelsn)\n                # TODO: maybe remove these `self.` arguments as they already are member variable\n                if self.args.plots:\n                    self.confusion_matrix.process_batch(predn, labelsn)\n            self.stats.append((correct_bboxes, pred[:, 4], pred[:, 5], cls.squeeze(-1)))  # (conf, pcls, tcls)\n\n            # Save\n            if self.args.save_json:\n                self.pred_to_json(predn, batch[\"im_file\"][si])\n            # if self.args.save_txt:\n            #    save_one_txt(predn, save_conf, shape, file=save_dir / 'labels' / f'{path.stem}.txt')\n\n    def get_stats(self):\n        stats = [torch.cat(x, 0).cpu().numpy() for x in zip(*self.stats)]  # to numpy\n        if len(stats) and stats[0].any():\n            self.metrics.process(*stats)\n        self.nt_per_class = np.bincount(stats[-1].astype(int), minlength=self.nc)  # number of targets per class\n        return self.metrics.results_dict\n\n    def print_results(self):\n        pf = '%22s' + '%11i' * 2 + '%11.3g' * len(self.metrics.keys)  # print format\n        self.logger.info(pf % (\"all\", self.seen, self.nt_per_class.sum(), *self.metrics.mean_results()))\n        if self.nt_per_class.sum() == 0:\n            self.logger.warning(\n                f'WARNING âš ï¸ no labels found in {self.args.task} set, can not compute metrics without labels')\n\n        # Print results per class\n        if (self.args.verbose or not self.training) and self.nc > 1 and len(self.stats):\n            for i, c in enumerate(self.metrics.ap_class_index):\n                self.logger.info(pf % (self.names[c], self.seen, self.nt_per_class[c], *self.metrics.class_result(i)))\n\n        if self.args.plots:\n            self.confusion_matrix.plot(save_dir=self.save_dir, names=list(self.names.values()))\n\n    def _process_batch(self, detections, labels):\n        \"\"\"\n        Return correct prediction matrix\n        Arguments:\n            detections (array[N, 6]), x1, y1, x2, y2, conf, class\n            labels (array[M, 5]), class, x1, y1, x2, y2\n        Returns:\n            correct (array[N, 10]), for 10 IoU levels\n        \"\"\"\n        iou = box_iou(labels[:, 1:], detections[:, :4])\n        correct = np.zeros((detections.shape[0], self.iouv.shape[0])).astype(bool)\n        correct_class = labels[:, 0:1] == detections[:, 5]\n        for i in range(len(self.iouv)):\n            x = torch.where((iou >= self.iouv[i]) & correct_class)  # IoU > threshold and classes match\n            if x[0].shape[0]:\n                matches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]),\n                                    1).cpu().numpy()  # [label, detect, iou]\n                if x[0].shape[0] > 1:\n                    matches = matches[matches[:, 2].argsort()[::-1]]\n                    matches = matches[np.unique(matches[:, 1], return_index=True)[1]]\n                    # matches = matches[matches[:, 2].argsort()[::-1]]\n                    matches = matches[np.unique(matches[:, 0], return_index=True)[1]]\n                correct[matches[:, 1].astype(int), i] = True\n        return torch.tensor(correct, dtype=torch.bool, device=detections.device)\n\n    def get_dataloader(self, dataset_path, batch_size):\n        # TODO: manage splits differently\n        # calculate stride - check if model is initialized\n        gs = max(int(de_parallel(self.model).stride if self.model else 0), 32)\n        return create_dataloader(path=dataset_path,\n                                 imgsz=self.args.imgsz,\n                                 batch_size=batch_size,\n                                 stride=gs,\n                                 hyp=dict(self.args),\n                                 cache=False,\n                                 pad=0.5,\n                                 rect=True,\n                                 workers=self.args.workers,\n                                 prefix=colorstr(f'{self.args.mode}: '),\n                                 shuffle=False,\n                                 seed=self.args.seed)[0] if self.args.v5loader else \\\n            build_dataloader(self.args, batch_size, img_path=dataset_path, stride=gs, mode=\"val\")[0]\n\n    def plot_val_samples(self, batch, ni):\n        plot_images(batch[\"img\"],\n                    batch[\"batch_idx\"],\n                    batch[\"cls\"].squeeze(-1),\n                    batch[\"bboxes\"],\n                    paths=batch[\"im_file\"],\n                    fname=self.save_dir / f\"val_batch{ni}_labels.jpg\",\n                    names=self.names)\n\n    def plot_predictions(self, batch, preds, ni):\n        plot_images(batch[\"img\"],\n                    *output_to_target(preds, max_det=15),\n                    paths=batch[\"im_file\"],\n                    fname=self.save_dir / f'val_batch{ni}_pred.jpg',\n                    names=self.names)  # pred\n\n    def pred_to_json(self, predn, filename):\n        stem = Path(filename).stem\n        image_id = int(stem) if stem.isnumeric() else stem\n        box = ops.xyxy2xywh(predn[:, :4])  # xywh\n        box[:, :2] -= box[:, 2:] / 2  # xy center to top-left corner\n        for p, b in zip(predn.tolist(), box.tolist()):\n            self.jdict.append({\n                'image_id': image_id,\n                'category_id': self.class_map[int(p[5])],\n                'bbox': [round(x, 3) for x in b],\n                'score': round(p[4], 5)})\n\n    def eval_json(self, stats):\n        if self.args.save_json and self.is_coco and len(self.jdict):\n            anno_json = self.data['path'] / \"annotations/instances_val2017.json\"  # annotations\n            pred_json = self.save_dir / \"predictions.json\"  # predictions\n            self.logger.info(f'\\nEvaluating pycocotools mAP using {pred_json} and {anno_json}...')\n            try:  # https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoEvalDemo.ipynb\n                check_requirements('pycocotools>=2.0.6')\n                from pycocotools.coco import COCO  # noqa\n                from pycocotools.cocoeval import COCOeval  # noqa\n\n                for x in anno_json, pred_json:\n                    assert x.is_file(), f\"{x} file not found\"\n                anno = COCO(str(anno_json))  # init annotations api\n                pred = anno.loadRes(str(pred_json))  # init predictions api (must pass string, not Path)\n                eval = COCOeval(anno, pred, 'bbox')\n                if self.is_coco:\n                    eval.params.imgIds = [int(Path(x).stem) for x in self.dataloader.dataset.im_files]  # images to eval\n                eval.evaluate()\n                eval.accumulate()\n                eval.summarize()\n                stats[self.metrics.keys[-1]], stats[self.metrics.keys[-2]] = eval.stats[:2]  # update mAP50-95 and mAP50\n            except Exception as e:\n                self.logger.warning(f'pycocotools unable to run: {e}')\n        return stats\n\n\n@hydra.main(version_base=None, config_path=str(DEFAULT_CONFIG.parent), config_name=DEFAULT_CONFIG.name)\ndef val(cfg):\n    cfg.data = cfg.data or \"coco128.yaml\"\n    validator = DetectionValidator(args=cfg)\n    validator(model=cfg.model)\n\n\nif __name__ == \"__main__\":\n    val()\n"}
