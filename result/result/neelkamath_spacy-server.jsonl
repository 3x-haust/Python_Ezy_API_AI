{"repo_info": {"repo_name": "spacy-server", "repo_owner": "neelkamath", "repo_url": "https://github.com/neelkamath/spacy-server"}}
{"type": "test_file", "path": "src/test_main.py", "content": "import json\nimport re\nfrom typing import List \n\nimport pytest\nfrom fastapi import HTTPException\nfrom main import NERRequest, PhraseInSentence, TextModel, app, compute_phrases, enforce_components, nlp\nfrom pydantic import BaseModel\nfrom requests import Response\nfrom starlette.testclient import TestClient\n\nclient: TestClient = TestClient(\n    app\n)\n\nner_sections: List[str] = [\n    'Net income was $9.4 million compared to the prior year of $2.7 million. '\n    + 'Google is a big company.',\n    'Revenue exceeded twelve billion dollars, with a loss of $1b.'\n]\n\n\ndef test_ner_sense2vec_enabled() -> None:\n    response: Response = client.post(\n        '/ner',\n        json=dict(NERRequest(sections=ner_sections, sense2vec=True))\n    )\n    assert response.status_code == 200\n    with open('src/outputs/ner/sense2vec_enabled.json') as f:\n        assert response.json() == json.load(f)\n\n\ndef test_ner_sense2vec_disabled() -> None:\n    response: Response = client.post(\n        '/ner',\n        json=dict(NERRequest(sections=ner_sections))\n    )\n    with open('src/outputs/ner/sense2vec_disabled.json') as f:\n        assert response.json() == json.load(f)\n\n\ndef test_ner_spacy_fail() -> None:\n    fail('/ner', NERRequest(sections=ner_sections), pipe='ner')\n\n\ndef test_ner_sense2vec_fail() -> None:\n    fail(\n        '/ner',\n        NERRequest(sections=ner_sections, sense2vec=True),\n        pipe='sense2vec'\n    )\n\n\ndef test_sense2vec_success() -> None:\n    body: PhraseInSentence = PhraseInSentence(\n        sentence='Bill Gates founded Microsoft in April 4, 1975.',\n        phrase='Bill Gates'\n    )\n    response: Response = client.post('/sense2vec', json=dict(body))\n    assert response.status_code == 200\n    with open('src/outputs/sense2vec.json') as f:\n        assert response.json() == json.load(f)\n\n\npos_body: TextModel = TextModel(\n    text='Apple is looking at buying U.K. startup for $1 billion'\n)\n\n\ndef test_pos() -> None:\n    response: Response = client.post('/pos', json=dict(pos_body))\n    assert response.status_code == 200\n    with open('src/outputs/pos.json') as f:\n        assert response.json() == json.load(f)\n\n\ndef test_pos_fail() -> None:\n    fail('/pos', pos_body, pipe='parser')\n\n\ndef test_tokenizer() -> None:\n    text: TextModel = TextModel(\n        text='Apple is looking at buying U.K. startup for $1 billion'\n    )\n    response: Response = client.post('/tokenizer', json=dict(text))\n    assert response.status_code == 200\n    with open('src/outputs/tokenizer.json') as f:\n        assert response.json() == json.load(f)\n\n\nsentencizer_body: TextModel = TextModel(\n    text='Apple is looking at buying U.K. startup for $1 billion. Another '\n         + 'sentence.'\n)\n\n\ndef test_sentencizer() -> None:\n    response: Response = client.post(\n        '/sentencizer',\n        json=dict(sentencizer_body)\n    )\n    assert response.status_code == 200\n    with open('src/outputs/sentencizer.json') as f:\n        assert response.json() == json.load(f)\n\n\ndef test_sentencizer_fail() -> None:\n    fail('/sentencizer', sentencizer_body, pipe='parser')\n\n\ndef test_health_check() -> None:\n    assert client.get('/health_check').status_code == 204\n\n\ndef fail(endpoint: str, body: BaseModel, pipe: str) -> None:\n    with nlp.disable_pipes(pipe):\n        response: Response = client.post(endpoint, json=dict(body))\n        assert re.match(r'4\\d\\d', str(response.status_code))\n        assert 'detail' in response.json()\n\n\ndef test_enforce_components() -> None:\n    with pytest.raises(HTTPException):\n        component: str = 'nonexistent_component'\n        enforce_components([component], component)\n\n\ndef test_compute_phrases() -> None:\n    sentence: str = 'Bill Gates founded Microsoft in April 4, 1975.'\n    doc: nlp = nlp(sentence, disable=['tagger'])\n    for ent in list(doc.sents)[0].ents:\n        if ent.text == 'Bill Gates':\n            with open('src/outputs/compute_phrases.json') as f:\n                assert compute_phrases(ent) == json.load(f)\n"}
{"type": "source_file", "path": "src/main.py", "content": "\"\"\"Provides NLP via spaCy and sense2vec over an HTTP API.\"\"\"\n\nimport os\nfrom typing import List\n\nimport spacy\nfrom dataclasses import dataclass\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel, root_validator\nfrom sense2vec import Sense2VecComponent\nfrom starlette.responses import Response\nfrom starlette.status import HTTP_204_NO_CONTENT\n\napp: FastAPI = FastAPI()\nmodel: str = os.getenv('SPACY_MODEL')\npipeline_error: str = f\"The model ({model}) doesn't support \" + '{}.'\nnlp: spacy = spacy.load(model)\nif os.getenv('SENSE2VEC') == '1':\n    s2v = nlp.add_pipe(\"sense2vec\")\n    s2v.from_disk(\"src/s2v_old\")\n\n\ndef enforce_components(components: List[str], message: str) -> None:\n    \"\"\"Throws the <message> if the model doesn't have the <components>.\"\"\"\n    for component in components:\n        if not nlp.has_pipe(component):\n            raise HTTPException(\n                status_code=400,\n                detail=pipeline_error.format(message)\n            )\n\n\nclass NERRequest(BaseModel):\n    sections: List[str]\n    sense2vec: bool = False\n\n\n@dataclass\nclass BuiltEntity:\n    text: str\n    label: str\n    start_char: int\n    end_char: int\n    lemma: str\n    start: int\n    end: int\n    text_with_ws: str\n    sense2vec: List[str]\n\n\n@dataclass\nclass SentenceWithEntities:\n    text: str\n    entities: List[BuiltEntity]\n\n\n@dataclass\nclass NERResponse:\n    data: List[SentenceWithEntities]\n\n\n@app.post('/ner')\nasync def recognize_named_entities(request: NERRequest) -> NERResponse:\n    enforce_components(['ner', 'parser'], 'named entity recognition')\n    if request.sense2vec:\n        enforce_components(\n            ['sense2vec'],\n            'There is no sense2vec model bundled with this service.'\n        )\n    response: NERResponse = NERResponse([])\n    for doc in nlp.pipe(request.sections, disable=['tagger']):\n        for sent in doc.sents:\n            entities: List[BuiltEntity] = [\n                build_entity(ent, request.sense2vec) for ent in sent.ents\n            ]\n            data: SentenceWithEntities = SentenceWithEntities(\n                text=sent.text,\n                entities=entities\n            )\n            response.data.append(data)\n    return response\n\n\nclass SimilarPhrase(BaseModel):\n    \"\"\"Similar phrases computed by sense2vec.\"\"\"\n\n    \"\"\"The similar phrase.\"\"\"\n    phrase: str\n    \"\"\"The phrase's similarity in the range of 0-1.\"\"\"\n    similarity: float\n\n\ndef compute_phrases(ent) -> List[SimilarPhrase]:\n    \"\"\"Computes similar phrases for the entity (<ent>).\n\n    The entity must have already been processed by the ner, parser, and\n    sense2vec pipeline components.\n    \"\"\"\n    similar: List[SimilarPhrase] = []\n    if ent._.in_s2v:\n        for data in ent._.s2v_most_similar():\n            similar.append(\n                SimilarPhrase(phrase=data[0][0], similarity=float(data[1]))\n            )\n    return similar\n\n\ndef build_entity(ent: spacy, use_sense2vec: bool) -> BuiltEntity:\n    return BuiltEntity(\n        text=ent.text,\n        label=ent.label_,\n        start_char=ent.start_char,\n        end_char=ent.end_char,\n        lemma=ent.lemma_,\n        start=ent.start,\n        end=ent.end,\n        text_with_ws=ent.text_with_ws,\n        sense2vec=compute_phrases(ent) if use_sense2vec else [],\n    )\n\n\nclass PhraseInSentence(BaseModel):\n    \"\"\"A <phrase> in a <sentence>.\"\"\"\n\n    sentence: str\n    phrase: str\n\n    @root_validator\n    def phrase_must_be_in_sentence(cls, values):\n        if values.get('phrase') not in values.get('sentence'):\n            raise ValueError('phrase must be in sentence')\n        return values\n\n\n@dataclass\nclass Sense2vecResponse:\n    sense2vec: List[SimilarPhrase]\n\n\n@app.post('/sense2vec')\nasync def sense2vec(request: PhraseInSentence) -> Sense2vecResponse:\n    enforce_components(['ner', 'parser', 'sense2vec'], 'sense2vec')\n    doc: nlp = nlp(request.sentence, disable=['tagger'])\n    phrases: List[SimilarPhrase] = []\n    for ent in list(doc.sents)[0].ents:\n        if ent.text == request.phrase:\n            phrases: List[SimilarPhrase] = compute_phrases(ent)\n    return Sense2vecResponse(phrases)\n\n\nclass TextModel(BaseModel):\n    text: str\n\n\n@dataclass\nclass Token:\n    text: str\n    text_with_ws: str\n    whitespace: str\n    head: str\n    head_index: int\n    left_edge: str\n    right_edge: str\n    index: int\n    ent_type: str\n    ent_iob: str\n    lemma: str\n    normalized: str\n    shape: str\n    prefix: str\n    suffix: str\n    is_alpha: bool\n    is_ascii: bool\n    is_digit: bool\n    is_title: bool\n    is_punct: bool\n    is_left_punct: bool\n    is_right_punct: bool\n    is_space: bool\n    is_bracket: bool\n    is_quote: bool\n    is_currency: bool\n    like_url: bool\n    like_num: bool\n    like_email: bool\n    is_oov: bool\n    is_stop: bool\n    pos: str\n    tag: str\n    dep: str\n    lang: str\n    prob: int\n    char_offset: int\n\n\n@dataclass\nclass TaggedText:\n    text: str\n    tags: List[Token]\n\n\n@dataclass\nclass POSResponse:\n    data: List[TaggedText]\n\n\n@dataclass\nclass TokenWithSentence:\n    token: Token\n    sent: str\n\n\n@app.post('/pos')\nasync def tag_parts_of_speech(request: TextModel) -> POSResponse:\n    enforce_components(['ner', 'parser', 'tagger'], 'part-of-speech tagging')\n    data: List[TaggedText] = []\n    doc: nlp = nlp(request.text, disable=['sense2vec'])\n    for token_with_sent in [build_token_with_sent(token) for token in doc]:\n        if token_with_sent.sent in [obj.text for obj in data]:\n            for obj in data:\n                if obj.text == token_with_sent.sent:\n                    obj.tags.append(token_with_sent.token)\n                    break\n        else:\n            data.append(\n                TaggedText(token_with_sent.sent, [token_with_sent.token])\n            )\n    return POSResponse(data)\n\n\ndef build_token_with_sent(token) -> TokenWithSentence:\n    return TokenWithSentence(\n        sent=token.sent.text,\n        token=Token(\n            text=token.text,\n            text_with_ws=token.text_with_ws,\n            whitespace=token.whitespace_,\n            head=token.head.text,\n            head_index=token.head.i,\n            left_edge=token.left_edge.text,\n            right_edge=token.right_edge.text,\n            index=token.i,\n            ent_type=token.ent_type_,\n            ent_iob=token.ent_iob_,\n            lemma=token.lemma_,\n            normalized=token.norm_,\n            shape=token.shape_,\n            prefix=token.prefix_,\n            suffix=token.suffix_,\n            is_alpha=token.is_alpha,\n            is_ascii=token.is_ascii,\n            is_digit=token.is_digit,\n            is_title=token.is_title,\n            is_punct=token.is_punct,\n            is_left_punct=token.is_left_punct,\n            is_right_punct=token.is_right_punct,\n            is_space=token.is_space,\n            is_bracket=token.is_bracket,\n            is_quote=token.is_quote,\n            is_currency=token.is_currency,\n            like_url=token.like_url,\n            like_num=token.like_num,\n            like_email=token.like_email,\n            is_oov=token.is_oov,\n            is_stop=token.is_stop,\n            pos=token.pos_,\n            tag=token.tag_,\n            dep=token.dep_,\n            lang=token.lang_,\n            prob=token.prob,\n            char_offset=token.idx,\n        )\n    )\n\n\n@dataclass\nclass TokenizerResponse:\n    tokens: List[str]\n\n\n@app.post('/tokenizer')\nasync def tokenize(request: TextModel) -> TokenizerResponse:\n    doc: nlp = nlp(\n        request.text,\n        disable=['tagger', 'parser', 'ner', 'sense2vec']\n    )\n    return TokenizerResponse([token.text for token in doc])\n\n\n@dataclass\nclass SentencizerResponse:\n    sentences: List[str]\n\n\n@app.post('/sentencizer')\nasync def sentencize(request: TextModel) -> SentencizerResponse:\n    enforce_components(['parser'], 'sentence segmentation')\n    doc: nlp = nlp(request.text, disable=['tagger', 'ner', 'sense2vec'])\n    return SentencizerResponse([sent.text for sent in doc.sents])\n\n\n@app.get('/health_check')\nasync def check_health() -> Response:\n    return Response(\n        status_code=HTTP_204_NO_CONTENT\n    )\n"}
