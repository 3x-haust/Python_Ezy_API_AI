{"repo_info": {"repo_name": "chemagent", "repo_owner": "gersteinlab", "repo_url": "https://github.com/gersteinlab/chemagent"}}
{"type": "source_file", "path": "XAgent/__init__.py", "content": ""}
{"type": "source_file", "path": "XAgent/agent/__init__.py", "content": "from .plan_generate_agent import PlanGenerateAgent\nfrom .plan_refine_agent import PlanRefineAgent\nfrom .reflect_agent import ReflectAgent\nfrom .father_reflect_agent import FatherReflectAgent\nfrom .simple_agent import SimpleAgent\nfrom .plan_verify_agent import PlanVerifyAgent\n"}
{"type": "source_file", "path": "XAgent/agent/dispatcher_agent/agent.py", "content": "import re\nimport copy\nimport json5\nfrom typing import List\n\nfrom .prompt import SYSTEM_PROMPT\n\nfrom XAgent.logs import logger\nfrom XAgent.message_history import Message\nfrom XAgent.agent.base_agent import BaseAgent\n\nclass DispatcherAgent(BaseAgent):\n    \"\"\"\n    A subclass of BaseAgent whose primary function is to help dispatch tasks to \n    different agent handlers based on the task requirements.\n\n    Attributes:\n    ------------\n    config : object\n        The configuration settings for the agent.\n    prompt_messages : List[Message]\n        The list of prompt messages for the agent to dispatch.\n    \"\"\"\n    def __init__(self, config, prompt_messages: List[Message] = None):\n        \"\"\"\n        Initialize a DispatcherAgent instance.\n\n        Args:\n        -------\n        config : object\n            The configuration settings for the agent.\n        prompt_messages : list, optional\n            The list of prompt messages for the agent to dispatch, defaults to None.\n            If not provided, default_prompt_messages is used instead.\n        \"\"\"\n        self.config = config\n        self.prompt_messages = (\n            self.default_prompt_messages if prompt_messages is None else prompt_messages\n        )\n\n    @property\n    def default_prompt_messages(self):\n        \"\"\"\n        Returns the default system prompt messages in the form of a list of Message objects.\n\n        Returns:\n        -----------\n        list[Message] : \n            A list containing the default prompt message.\n        \"\"\"\n        return [Message(role=\"system\", content=SYSTEM_PROMPT)]\n\n    def find_all_placeholders(self, prompt):\n        \"\"\"\n        Finds all placeholders within a prompt.\n\n        Args:\n        --------\n        prompt : str\n            The string within which placeholders are to be found.\n\n        Returns:\n        --------\n        list[str] : \n            A list of all placeholders found within the prompt.\n        \"\"\"\n        return re.findall(r\"{{(.*?)}}\", prompt)\n\n    def construct_input_messages(\n        self,\n        task: str,\n        example_input: str,\n        example_system_prompt: str,\n        example_user_prompt: str,\n        retrieved_procedure: str,\n    ):\n        \"\"\"\n        Constructs input messages by replacing placeholders in the prompt_messages \n        with provided data.\n\n        Args:\n        ---------\n        task : str\n            The task to be completed.\n        example_input : str\n            An example input for the task.\n        example_system_prompt : str\n            The example system prompt for the task.\n        example_user_prompt : str\n            The example user prompt for the task.\n        retrieved_procedure : str\n            The retrieved process for the task.\n\n        Returns:\n        ---------\n        list[Message] :\n            A list containing the constructed input messages with placeholders replaced with provided data.\n        \"\"\"\n        prompt_messages = copy.deepcopy(self.prompt_messages)\n        # TODO: Make it more robust. Here we assume only the first message is system prompt\n        #       and we only update the placeholders in the first message.\n        prompt_messages[0].content = (\n            prompt_messages[0]\n            .content.replace(\"{{example_system_prompt}}\", example_system_prompt)\n            .replace(\"{{example_user_prompt}}\", example_user_prompt)\n            .replace(\"{{retrieved_procedure}}\", retrieved_procedure)\n            .replace(\"{{task}}\", task)\n        )\n        return prompt_messages  # + [Message(role=\"user\", content=task)] \n\n    def extract_prompts_from_response(self, message):\n        \"\"\"\n        Extracts additional prompts from the dispatcher's response message.\n\n        Args:\n        --------\n        message : str \n           The response message from the dispatcher.\n\n        Returns:\n        ---------\n        str : \n            The additional prompt extracted from the message; if not found, \"\" is returned.\n\n        \"\"\"\n        try:\n            additional_prompt = re.findall(r\"ADDITIONAL USER PROMPT:?\\n```(.*)```\", message['content'], re.DOTALL)[0].strip()\n        except IndexError as e:\n            logger.error(\n                f\"Failed to extract prompts from the dispatcher's response:\\n{message['content']}\"\n            )\n            logger.error(\"Fallback to use the default prompts.\")\n            additional_prompt = \"\"\n        return additional_prompt\n\n    def retrieved_procedure(self, query: str) -> str:\n        # TODO: this function should be implemented thru tool server\n\n        \"\"\"\n        Retrieves a procedure relevant to the given query from an external site.\n\n        Args:\n        --------\n        query : str\n            The query to retrieve the relevant procedure.\n\n        Returns:\n        ---------\n        str : \n            The relevant procedure retrieved; if retrieval fails, the string 'None' is returned.\n        \"\"\"\n        \n        url = \"https://open-procedures.replit.app/search/\"\n        try:\n            import requests\n            import json\n\n            relevant_procedures = requests.get(url, params={'query': query}).json()[\n                \"procedures\"\n            ][0]\n        except:\n            # For someone, this failed for a super secure SSL reason.\n            # Since it's not strictly necessary, let's worry about that another day. Should probably log this somehow though.\n            relevant_procedures = \"None\"\n\n        return relevant_procedures\n\n    def parse(\n        self,\n        task: str,\n        example_input: str,\n        example_system_prompt: str,\n        example_user_prompt: str,\n        stop=None,\n        **args,\n    ) -> List[Message]:\n        # TODO: should we consider additional messages when generating prompt?\n        # currently the plan generation and refine agent are the same since we\n        # don't consider the additional messages when generating prompt.\n\n        \"\"\"\n        Parse the task and related data to generate prompt messages.\n\n        Args:\n        ---------\n        task : str\n            The task to be processed.\n        example_input : str\n            An example input related to the task.\n        example_system_prompt : str\n            An example system prompt related to the task.\n        example_user_prompt : str\n            An example user prompt related to the task.\n        stop : str, optional\n            The stopping criterion for message generation, defaults to None.\n\n        Returns:\n        ---------\n        Tuple[List[Message], List[str]] : \n            A tuple containing a list of prompt messages and tokens.\n        \"\"\"\n        message,tokens = self.generate(\n            messages=self.construct_input_messages(\n                task,\n                example_input,\n                example_system_prompt,\n                example_user_prompt,\n                \"\"  \n            ),\n            stop=stop,\n            **args,\n        )\n\n        additional_prompt = message['arguments']['additional_prompt']\n\n        prompt_messages = []\n        if additional_prompt != \"\":\n            example_user_prompt += \"\\n\\nADDITIONAL NOTES\\n\" + additional_prompt\n        prompt_messages.append(Message(role=\"system\", content=example_system_prompt))\n        prompt_messages.append(Message(role=\"user\", content=example_user_prompt))\n\n        return prompt_messages, tokens"}
{"type": "source_file", "path": "XAgent/agent/plan_verify_agent/prompt.py", "content": "SYSTEM_PROMPT = \"\"\"You're an expert professor who is very proficient in chemistry, and now you want to check whether a step of a chemistry problem has gone wrong.\n\nThis chemistry question is: {{question}}\n\nWe broke down this chemistry problem into a number of steps to solve, each of which was a subtask. The split is as follows:\n{{all_plan}}\n\n\"\"\"\nUSER_PROMPT = \"\"\"The subtasks you need to complete are:\n{{subplan}}\n\nThe reasoning and answer you receive are:\n[Response START]\n{{response}}\n[Response END]\n\nCheck your response very carefully. Please check whether the subtask has completed all the milestones of the subtask, whether the answers given are the objectives of the task, whether the constants involved in the reasoning process are correct, whether there are logical errors, whether the unit conversion is correct and whether there are calculation errors, Whether there is an error in the formula. \n\nYou should give me back the **Formulae retrieval:** , **Reasoning/calculation process:** and **Answer conclusion:**  refined according to the original response, if you think there is nothing needed to be changed, just output the original **Formulae retrieval:** , **Reasoning/calculation process:** and **Answer conclusion:**.\nYour answer should strictly follow the format below. \n**Refined Formulae retrieval:**\n\n**Refined Reasoning/calculation process:** \n\n**Refined Answer conclusion:** \n[answer]: ```{Refined PYTHON CODE}``` \n\n\n\"\"\"\n\n\n\n\n\n\ndef get_examples_for_dispatcher():\n    \"\"\"The example that will be given to the dispatcher to generate the prompt\n\n    Returns:\n        example_input: the user query or the task\n        example_system_prompt: the system prompt\n        example_user_prompt: the user prompt\n    \"\"\"\n    example_input = \"\"\n    example_system_prompt = SYSTEM_PROMPT\n    example_user_prompt = USER_PROMPT\n    return example_input, example_system_prompt, example_user_prompt\n"}
{"type": "source_file", "path": "XAgent/agent/simple_agent/agent.py", "content": "import json\nimport json5\nimport jsonschema\nfrom typing import List\nfrom colorama import Fore\nfrom tenacity import retry, stop_after_attempt\n\nfrom XAgent.agent.base_agent import BaseAgent\nfrom XAgent.utils import RequiredAbilities\nfrom XAgent.message_history import Message\nfrom XAgent.logs import logger\nfrom XAgent.data_structure.node import ToolNode\nfrom XAgent.ai_functions import function_manager, objgenerator\nfrom XAgent.config import CONFIG\nfrom openai import AzureOpenAI\nfrom .post_process import (\n    parse_math_answer,\n    remove_not,\n    cal_not,\n    parse_not,\n    extract_code,\n    exec_code,\n)\n\n\nclass SimpleAgent(BaseAgent):\n    \"\"\"\n    This class is used to represent the ToolAgent object, which is inherited from the BaseAgent. It mainly focuses\n    on actions around the tool tree and its functions.\n\n    Attributes:\n        abilities (set): Set to store the abilities of the current ToolAgent. By default, it is set to\n        `RequiredAbilities.tool_tree_search`.\n    \"\"\"\n\n    abilities = set([RequiredAbilities.simple_search])\n\n    def call_engine(self, message, functions):\n        \"\"\"\n        ask chatgpt\n        \"\"\"\n        model = CONFIG.default_completion_kwargs[\"model\"]\n        model_name = CONFIG.api_keys[model][0][\"engine\"]\n\n        response = None\n        temp = 0.2\n        for i in range(6):\n            try:\n                client = AzureOpenAI(\n                    api_key=CONFIG.api_keys[model][0][\"api_key\"],\n                    api_version=CONFIG.api_keys[model][0][\"api_version\"],\n                    azure_endpoint=CONFIG.api_keys[model][0][\"api_base\"],\n                )\n                response = client.chat.completions.create(\n                    model=model_name,\n                    messages=message,\n                    temperature=temp,\n                )\n\n                response = response.choices[0].message.content\n                if i == 5:\n                    print(\"\\n--------last try------\\n\")\n                    print(response)\n                    print(\"\\n--------last try------\\n\")\n                if \"python\" in functions:\n                    model_output_code = extract_code(response)\n                    if (\n                        model_output_code != None\n                        and exec_code(model_output_code) != \"None\"\n                    ):\n\n                        return response\n                else:\n                    model_output = parse_math_answer(response)\n                    if model_output != None and model_output != \"None\":\n                        print(\"\\n-------response-----\\n\" + response + \"\\n-------\\n\")\n                        return response\n\n                print(f\"chatcompletion: using {model_name} Fail\")\n\n            except Exception as e:\n                print(e)\n                print(f\"chatcompletion: using {model_name} Fail\")\n        print(\"call_engine fail for all temp\\n\")\n        return None\n\n    @retry(stop=stop_after_attempt(CONFIG.max_retry_times), reraise=True)\n    def parse(\n        self,\n        placeholders: dict = {},\n        arguments: dict = None,\n        functions=None,\n        function_call=None,\n        stop=None,\n        additional_messages: List[Message] = [],\n        additional_insert_index: int = -1,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        This function generates a message list and a token list based on the input parameters using the\n        `generate()` function, modifies it as per specific conditions, and returns it.\n\n        Args:\n            placeholders (dict, optional): Dictionary object to store the placeholders and their mappings.\n            arguments (dict, optional): Dictionary object to store argument's details.\n            functions: List of permissible functions that can be inserted in the function fields for the `openai` type.\n            function_call: A dictionary representing the current function call being processed.\n            stop: The termination condition for the loop.\n            additional_messages (list, optional): List of additional messages to be appended to the existing message list.\n            additional_insert_index (int, optional): The index position to insert the additional messages.\n            *args: Variable length argument list for the parent class's `generate()` function.\n            **kwargs: Arbitrary keyword arguments for the parent class's `generate()` function.\n\n        Returns:\n            tuple: A tuple containing a dictionary of the parsed message and a list of tokens.\n\n        Raises:\n            AssertionError: If the specified function schema is not found in the list of possible functions.\n            Exception: If the validation of the tool's call arguments fails.\n        \"\"\"\n\n        prompt_messages = self.fill_in_placeholders(placeholders)\n\n        messages = (\n            prompt_messages[:additional_insert_index]\n            + additional_messages\n            + prompt_messages[additional_insert_index:]\n        )\n        messages = [message.raw() for message in messages]\n\n        # Temporarily disable the arguments for openai\n        if self.config.default_request_type == \"openai\":\n\n            if CONFIG.enable_ask_human_for_help:\n                functions += [\n                    function_manager.get_function_schema(\"ask_human_for_help\")\n                ]\n\n        # logger.typewriter_log(\"simple agent LLM input: \\n\", Fore.RED, str(messages))\n\n        response = self.call_engine(messages, functions)\n        if \"python\" in functions:\n            model_output_code = extract_code(response)\n            ans = exec_code(model_output_code)\n        else:\n            ans = parse_math_answer(response)\n\n        # logger.typewriter_log(\n        #     \"simple agent LLM output : \\n\",\n        #     Fore.RED, str(response))\n\n        # logger.typewriter_log(\n        #     \"simple agent LLM output (parsed math answer): \\n\",\n        #     Fore.RED, str(model_output_code))\n        if response == None or response == \"\":\n            response = \"None\"\n        if ans == None or ans == \"\":\n            ans = \"None\"\n\n        return response, ans\n\n    def message_to_tool_node(self, message) -> ToolNode:\n        \"\"\"\n        This method converts a given message dictionary to a ToolNode object.\n\n        Args:\n            message (dict): Dictionary of message data containing content, function call and arguments.\n\n        Returns:\n            ToolNode: A ToolNode object generated from the provided message.\n\n        Warning:\n            If the `function_call` field is missing in the input message, a warning message will be logged.\n        \"\"\"\n\n        # assume message format\n        # {\n        #   \"content\": \"The content is useless\",\n        #   \"function_call\": {\n        #       \"name\": \"xxx\",\n        #       \"arguments\": \"xxx\"\n        #  },\n        #  \"arguments\": {\n        #      \"xxx\": \"xxx\",\n        #      \"xxx\": \"xxx\"\n        #  },\n        # }\n\n        new_node = ToolNode()\n        if \"content\" in message.keys():\n            print(message[\"content\"])\n            new_node.data[\"content\"] = message[\"content\"]\n        if \"arguments\" in message.keys():\n            new_node.data[\"thoughts\"][\"properties\"] = message[\"arguments\"]\n        if \"function_call\" in message.keys():\n            new_node.data[\"command\"][\"properties\"][\"name\"] = message[\"function_call\"][\n                \"name\"\n            ]\n            new_node.data[\"command\"][\"properties\"][\"args\"] = message[\"function_call\"][\n                \"arguments\"\n            ]\n        else:\n            logger.typewriter_log(\n                \"message_to_tool_node warning: no function_call in message\", Fore.RED\n            )\n\n        return new_node\n"}
{"type": "source_file", "path": "XAgent/agent/father_reflect_agent/__init__.py", "content": "from .agent import FatherReflectAgent\nfrom .prompt import get_examples_for_dispatcher\n"}
{"type": "source_file", "path": "XAgent/agent/simple.py", "content": "\"\"\"Simple Engine.\"\"\"\n\nimport json\nimport os\nfrom typing import Any, Optional, Union\n\nfrom llama_index.core import SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.embeddings import BaseEmbedding\nfrom llama_index.core.embeddings.mock_embed_model import MockEmbedding\nfrom llama_index.core.indices.base import BaseIndex\nfrom llama_index.core.ingestion.pipeline import run_transformations\nfrom llama_index.core.llms import LLM\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.postprocessor.types import BaseNodePostprocessor\nfrom llama_index.core.query_engine import RetrieverQueryEngine\nfrom llama_index.core.response_synthesizers import (\n    BaseSynthesizer,\n    get_response_synthesizer,\n)\nfrom llama_index.core.retrievers import BaseRetriever\nfrom llama_index.core.schema import (\n    BaseNode,\n    Document,\n    NodeWithScore,\n    QueryBundle,\n    QueryType,\n    TransformComponent,\n)\nfrom typing import Protocol\n\n\nclass RAGObject(Protocol):\n    \"\"\"Support rag add object.\"\"\"\n\n    def rag_key(self) -> str:\n        \"\"\"For rag search.\"\"\"\n\n    def model_dump_json(self) -> str:\n        \"\"\"For rag persist.\n\n        Pydantic Model don't need to implement this, as there is a built-in function named model_dump_json.\n        \"\"\"\n\n\nclass SimpleEngine(RetrieverQueryEngine):\n    \"\"\"SimpleEngine is designed to be simple and straightforward.\n\n    It is a lightweight and easy-to-use search engine that integrates\n    document reading, embedding, indexing, retrieving, and ranking functionalities\n    into a single, straightforward workflow. It is designed to quickly set up a\n    search engine from a collection of documents.\n    \"\"\"\n\n    def __init__(\n        self,\n        retriever: BaseRetriever,\n        response_synthesizer: Optional[BaseSynthesizer] = None,\n        node_postprocessors: Optional[list[BaseNodePostprocessor]] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        index: Optional[BaseIndex] = None,\n    ) -> None:\n        super().__init__(\n            retriever=retriever,\n            response_synthesizer=response_synthesizer,\n            node_postprocessors=node_postprocessors,\n            callback_manager=callback_manager,\n        )\n        self.index = index\n\n    @classmethod\n    def from_objs(\n        cls,\n        objs: Optional[list[RAGObject]] = None,\n        transformations: Optional[list[TransformComponent]] = None,\n        embed_model: BaseEmbedding = None,\n        llm: LLM = None,\n        retriever_configs: list[BaseRetrieverConfig] = None,\n        ranker_configs: list[BaseRankerConfig] = None,\n    ) -> \"SimpleEngine\":\n        \"\"\"From objs.\n\n        Args:\n            objs: List of RAGObject.\n            transformations: Parse documents to nodes. Default [SentenceSplitter].\n            embed_model: Parse nodes to embedding. Must supported by llama index. Default OpenAIEmbedding.\n            llm: Must supported by llama index. Default OpenAI.\n            retriever_configs: Configuration for retrievers. If more than one config, will use SimpleHybridRetriever.\n            ranker_configs: Configuration for rankers.\n        \"\"\"\n        objs = objs or []\n        retriever_configs = retriever_configs or []\n\n        if not objs and any(\n            isinstance(config, BM25RetrieverConfig) for config in retriever_configs\n        ):\n            raise ValueError(\"In BM25RetrieverConfig, Objs must not be empty.\")\n\n        nodes = [\n            ObjectNode(text=obj.rag_key(), metadata=ObjectNode.get_obj_metadata(obj))\n            for obj in objs\n        ]\n        index = VectorStoreIndex(\n            nodes=nodes,\n            transformations=transformations or [SentenceSplitter()],\n            embed_model=cls._resolve_embed_model(embed_model, retriever_configs),\n        )\n        return cls._from_index(\n            index,\n            llm=llm,\n            retriever_configs=retriever_configs,\n            ranker_configs=ranker_configs,\n        )\n\n    @classmethod\n    def from_index(\n        cls,\n        index_config: BaseIndexConfig,\n        embed_model: BaseEmbedding = None,\n        llm: LLM = None,\n        retriever_configs: list[BaseRetrieverConfig] = None,\n        ranker_configs: list[BaseRankerConfig] = None,\n    ) -> \"SimpleEngine\":\n        \"\"\"Load from previously maintained index by self.persist(), index_config contains persis_path.\"\"\"\n        index = get_index(\n            index_config,\n            embed_model=cls._resolve_embed_model(embed_model, [index_config]),\n        )\n        return cls._from_index(\n            index,\n            llm=llm,\n            retriever_configs=retriever_configs,\n            ranker_configs=ranker_configs,\n        )\n\n    async def asearch(self, content: str, **kwargs) -> str:\n        \"\"\"Inplement tools.SearchInterface\"\"\"\n        return await self.aquery(content)\n\n    def retrieve(self, query: QueryType) -> list[NodeWithScore]:\n        query_bundle = QueryBundle(query) if isinstance(query, str) else query\n\n        nodes = super().retrieve(query_bundle)\n        self._try_reconstruct_obj(nodes)\n        return nodes\n\n    async def aretrieve(self, query: QueryType) -> list[NodeWithScore]:\n        \"\"\"Allow query to be str.\"\"\"\n        query_bundle = QueryBundle(query) if isinstance(query, str) else query\n\n        nodes = await super().aretrieve(query_bundle)\n        self._try_reconstruct_obj(nodes)\n        return nodes\n\n    def add_docs(self, input_files: list[str]):\n        \"\"\"Add docs to retriever. retriever must has add_nodes func.\"\"\"\n        self._ensure_retriever_modifiable()\n\n        documents = SimpleDirectoryReader(input_files=input_files).load_data()\n        self._fix_document_metadata(documents)\n\n        nodes = run_transformations(\n            documents, transformations=self.index._transformations\n        )\n        self._save_nodes(nodes)\n\n    def add_objs(self, objs: list[RAGObject]):\n        \"\"\"Adds objects to the retriever, storing each object's original form in metadata for future reference.\"\"\"\n        self._ensure_retriever_modifiable()\n\n        nodes = [\n            ObjectNode(text=obj.rag_key(), metadata=ObjectNode.get_obj_metadata(obj))\n            for obj in objs\n        ]\n        self._save_nodes(nodes)\n\n    def persist(self, persist_dir: Union[str, os.PathLike], **kwargs):\n        \"\"\"Persist.\"\"\"\n        self._ensure_retriever_persistable()\n\n        self._persist(str(persist_dir), **kwargs)\n\n    @classmethod\n    def _from_index(\n        cls,\n        index: BaseIndex,\n        llm: LLM = None,\n        retriever_configs: list[BaseRetrieverConfig] = None,\n        ranker_configs: list[BaseRankerConfig] = None,\n    ) -> \"SimpleEngine\":\n        llm = llm or get_rag_llm()\n        retriever = get_retriever(\n            configs=retriever_configs, index=index\n        )  # Default index.as_retriever\n        rankers = get_rankers(configs=ranker_configs, llm=llm)  # Default []\n\n        return cls(\n            retriever=retriever,\n            node_postprocessors=rankers,\n            response_synthesizer=get_response_synthesizer(llm=llm),\n            index=index,\n        )\n\n    def _ensure_retriever_modifiable(self):\n        self._ensure_retriever_of_type(ModifiableRAGRetriever)\n\n    def _ensure_retriever_persistable(self):\n        self._ensure_retriever_of_type(PersistableRAGRetriever)\n\n    def _ensure_retriever_of_type(self, required_type: BaseRetriever):\n        \"\"\"Ensure that self.retriever is required_type, or at least one of its components, if it's a SimpleHybridRetriever.\n\n        Args:\n            required_type: The class that the retriever is expected to be an instance of.\n        \"\"\"\n        if isinstance(self.retriever, SimpleHybridRetriever):\n            if not any(isinstance(r, required_type) for r in self.retriever.retrievers):\n                raise TypeError(\n                    f\"Must have at least one retriever of type {required_type.__name__} in SimpleHybridRetriever\"\n                )\n\n        if not isinstance(self.retriever, required_type):\n            raise TypeError(\n                f\"The retriever is not of type {required_type.__name__}: {type(self.retriever)}\"\n            )\n\n    def _save_nodes(self, nodes: list[BaseNode]):\n        self.retriever.add_nodes(nodes)\n\n    def _persist(self, persist_dir: str, **kwargs):\n        self.retriever.persist(persist_dir, **kwargs)\n\n    @staticmethod\n    def _try_reconstruct_obj(nodes: list[NodeWithScore]):\n        \"\"\"If node is object, then dynamically reconstruct object, and save object to node.metadata[\"obj\"].\"\"\"\n        for node in nodes:\n            if node.metadata.get(\"is_obj\", False):\n                obj_cls = import_class(\n                    node.metadata[\"obj_cls_name\"], node.metadata[\"obj_mod_name\"]\n                )\n                obj_dict = json.loads(node.metadata[\"obj_json\"])\n                node.metadata[\"obj\"] = obj_cls(**obj_dict)\n\n    @staticmethod\n    def _fix_document_metadata(documents: list[Document]):\n        \"\"\"LlamaIndex keep metadata['file_path'], which is unnecessary, maybe deleted in the near future.\"\"\"\n        for doc in documents:\n            doc.excluded_embed_metadata_keys.append(\"file_path\")\n\n    @staticmethod\n    def _resolve_embed_model(\n        embed_model: BaseEmbedding = None, configs: list[Any] = None\n    ) -> BaseEmbedding:\n        if configs and all(isinstance(c, NoEmbedding) for c in configs):\n            return MockEmbedding(embed_dim=1)\n\n        return embed_model or get_rag_embedding()\n"}
{"type": "source_file", "path": "XAgent/agent/simple_agent/__init__.py", "content": "from .agent import SimpleAgent\nfrom .prompt import get_examples_for_dispatcher\nfrom .prompt import get_examples_for_dispatcher_d\n"}
{"type": "source_file", "path": "XAgent/agent/father_reflect_agent/prompt.py", "content": "SYSTEM_PROMPT = \"\"\"You are a posterior_knowledge_obtainer. \nNow that you've completed a parent task, the task is made up of many subtasks, each of which has been completed.\n\nYou plan of the parent task is as follows:\n---Parent Goal ---\n{{parent_goal}}\n--- Sub-task division ---\n{{all_plan}}\n\nThe flow of your actions for handling subtasks is:\n--- Action lists for Subtasks ---\n{{sub_plan}}\n\n\nNow, you have to learn some posterior knowledge from this process, doing the following things:\n1.Summary: Summarize the ideas of all subtasks to the parent task, and summarize a total process to the parent task according to the action process of each subtask. Explicitly include all the formulas used during performing the subbtasks in this summary. Specific numbers and numerical results are NOT needed in this part.\n\n2.Reflection of knowledge: After performing this task, you get some knowledge of generating plans for similar tasks. Only conclude the used knowledge and formulaes used in this whole task, do NOT contain numerical calculation process and results.\n\n3.Final Answer: Give the final answer to the task according to the course of the task, and ask the answer to be very short, without explaining the reason and adding unnecessary punctuation. If it's a math problem, only the last value is given.\n\"\"\"\n\n\"\"\"\nThe flow of your actions for handling subtasks is:\n--- Action lists for Subtasks ---\n{{sub_plan}}\n\"\"\"\n\nUSER_PROMPT = \"\"\n\n\ndef get_examples_for_dispatcher():\n    \"\"\"The example that will be given to the dispatcher to generate the prompt\n\n    Returns:\n        example_input: the user query or the task\n        example_system_prompt: the system prompt\n        example_user_prompt: the user prompt\n    \"\"\"\n    example_input = \"Reflect on the previous actions and give the posterior knowledge\"\n    example_system_prompt = SYSTEM_PROMPT\n\n    example_user_prompt = USER_PROMPT\n    return example_input, example_system_prompt, example_user_prompt\n"}
{"type": "source_file", "path": "XAgent/agent/plan_generate_agent/prompt.py", "content": "SYSTEM_PROMPT = \"\"\"You are a Chemistry expert and an efficient plan-generation agent.\n\nNow you are doing an exam, you must decompose a problem into several subtasks that describe must achieved goals for the problem.\n\n--- Background Information ---\nPLAN AND SUBTASK:\nA plan has a tree manner of subtasks: task 1 contains subtasks task 1.1, task 1.2, task 1.3, ... and task 1.2 contains subtasks 1.2.1, 1.2.2, ...\n\nA subtask-structure has the following json component:\n{\n\"subtask name\": string, name of the subtask\n\"goal.goal\": string, the main purpose of the subtask, and what will you do to reach this goal?\n\"goal.criticism\": string, what potential problems may the current subtask and goal have? \n\"milestones\": list[string], what milestones should be achieved to ensure the subtask is done? And What formulas might the current subtask use? Make it detailed and specific.\n}\nSUBTASK HANDLE:\nA task-handling agent will handle all the subtasks as the inorder-traversal. For example:\n1. it will handle subtask 1 first.\n2. if solved, handle subtask 2. If failed, split subtask 1 as subtask 1.1 1.2 1.3... Then handle subtask 1.1 1.2 1.3...\n3. Handle subtasks recursively, until all subtasks are solved. Do not make the task queue too complex, make it efficiently solve the original task.\n\n\nRESOURCES:\n1. A task-handling agent can write and execute python code.\n--- Task Description ---\nGenerate the plan for query with operation SUBTASK_SPLIT, make sure all must reach goals are included in the plan.\n\n*** Important Notice ***\n- Always make feasible and efficient plans that can lead to successful task solving. Never create new subtasks that are similar or same as the existing subtasks.\n- For subtasks with similar goals, try to do them together in one subtask with a list of subgoals, rather than split them into multiple subtasks.\n- Do not waste time on making irrelevant or unnecessary plans.\n- The task handler is powered by sota LLM, which can directly answer many questions. So make sure your plan can fully utilize its ability and reduce the complexity of the subtasks tree.\n- You can plan multiple subtasks if you want.\n- Minimize the number of subtasks, but make sure all must reach goals are included in the plan.\n- Don't generate tasks that are aimed at understanding a concept, such as \"understanding the problem\", the LLM who answers the task already knows the underlying concept.Check the generated subtask objectives and milestones for understanding, and regenerate the subtasks if so.\n- After the subtask is generated, check to see if the answer for the task has been given in the task known conditions. If the task has already been resolved, delete the subtask.\n\"\"\"\n\nUSER_PROMPT = \"\"\"This is the first time you are handling the task (query), so you should give an initial plan. \n{{similar_task_and_plan}}\nNow try to use SUBTASK_SPLIT to split the following problem, here is the query which you should give an initial plan to solve:\n\n----Your task----\n{{query}}\n\nYou will use operation SUBTASK_SPLIT to split the query into 1-3 subtasks and then commit.\n\"\"\"\n\n\ndef get_examples_for_dispatcher():\n    \"\"\"The example that will be given to the dispatcher to generate the prompt\n\n    Returns:\n        example_input: the user query or the task\n        example_system_prompt: the system prompt\n        example_user_prompt: the user prompt\n    \"\"\"\n    example_input = \"Generate a plan for writing a Python-based calculator.\"\n    example_system_prompt = SYSTEM_PROMPT\n    example_user_prompt = USER_PROMPT\n    return example_input, example_system_prompt, example_user_prompt\n"}
{"type": "source_file", "path": "XAgent/agent/dispatcher_agent/__init__.py", "content": "from .agent import DispatcherAgent\n"}
{"type": "source_file", "path": "XAgent/agent/reflect_agent/agent.py", "content": "from typing import List\nfrom XAgent.agent.base_agent import BaseAgent\nfrom XAgent.utils import RequiredAbilities\nfrom XAgent.message_history import Message\n\nclass ReflectAgent(BaseAgent):\n    \"\"\"This ReflectAgent class extends the BaseAgent class. It primarily has the ability of reflection \n    which means it can reflect upon the chat or dialogue and generate responses based on the messages\n    received.\n\n    Attributes:\n        abilities (set): Required abilities for the agent, namely reflection in this case. \n    \"\"\"\n\n    abilities = set([RequiredAbilities.reflection])\n\n    def parse(\n        self,\n        placeholders: dict = {},\n        arguments:dict = None,\n        functions=None,\n        function_call=None,\n        stop=None,\n        additional_messages: List[Message] = [],\n        *args,\n        **kwargs\n    ):\n        \"\"\"\n        The function is used to parse various arguments and call the generate function with these parsed arguments.\n\n        Args:\n            placeholders (dict, optional): Placeholders for the agent's responses. \n            arguments(dict, optional): Argument to influence the response of the agent.\n            functions (functions, optional): Functions to guide the agent's response.\n            function_call (FunctionType, optional): Function called to generate agent's response. \n            stop (bool, optional): Flag to stop the induction of the response. \n            additional_messages (list, optional): Additional messages to be included in the response. \n\n        Returns:\n            object: Response generated by the agent. \n\n        \"\"\"\n        prompt_messages = self.fill_in_placeholders(placeholders)\n        messages = prompt_messages + additional_messages\n\n        return self.generate(\n            messages=messages,\n            arguments=arguments,\n            functions=functions,\n            function_call=function_call,\n            stop=stop,\n            *args,**kwargs\n        )"}
{"type": "source_file", "path": "XAgent/agent/plan_refine_agent/agent.py", "content": "from typing import List\nfrom XAgent.agent.plan_generate_agent import PlanGenerateAgent\nfrom XAgent.utils import RequiredAbilities\nfrom XAgent.message_history import Message\n\nclass PlanRefineAgent(PlanGenerateAgent):\n    \"\"\"PlanRefineAgent is a subclass of PlanGenerateAgent and is involved in refining the plan.\n\n    This class utilizes the required ability of plan refinement to parse information \n    and generate a refined plan. It includes placeholders as the desired expressions.\n\n    Attributes:\n        abilities: A set of required abilities for the Agent. For PlanRefineAgent, it includes plan refinement.\n    \"\"\"\n    abilities = set([RequiredAbilities.plan_refinement])\n\n    def parse(\n        self,\n        placeholders: dict = {},\n        arguments:dict = None,\n        functions=None,\n        function_call=None,\n        stop=None,\n        additional_messages: List[Message] = [],\n        additional_insert_index: int = -1,\n        *args,\n        **kwargs\n    ):\n        \"\"\" Parses information in order to refine the existing plan.\n\n        This method fills in placeholders with corresponding expressions, then prompts and \n        additional messages are processed and converged into final messages. Finally, the \n        'generate' method of PlanGenerateAgent class is then invoked on the final messages.\n\n        Args:\n            placeholders (dict, optional): Desired expressions to fill in partially completed text snippets.\n            arguments (dict, optional): Arguments to the function.\n            functions (optional): Functions to be carried out.\n            function_call (optional): Functional request from the user.\n            stop (optional): Stop parsing at some particular point.\n            additional_messages (List[Message], optional): Additional messages to be included in final message.\n            additional_insert_index (int, optional): Index in prompt messages where additional messages should be inserted.\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n\n        Returns:\n            object: A refined plan generated from provided placeholders, arguments, functions, and messages.\n        \"\"\"\n        \n        prompt_messages = self.fill_in_placeholders(placeholders)\n        messages =prompt_messages[:additional_insert_index] + additional_messages + prompt_messages[additional_insert_index:]\n        \n        return self.generate(\n            messages=messages,\n            arguments=arguments,\n            functions=functions,\n            function_call=function_call,\n            stop=stop,\n            *args,**kwargs\n        )"}
{"type": "source_file", "path": "XAgent/agent/plan_verify_agent/agent.py", "content": "import json\nimport json5\nimport jsonschema\nfrom typing import List\nfrom colorama import Fore\nfrom tenacity import retry, stop_after_attempt\nfrom XAgent.agent.base_agent import BaseAgent\nfrom XAgent.utils import RequiredAbilities\nfrom XAgent.message_history import Message\nfrom XAgent.logs import logger\nfrom XAgent.data_structure.node import ToolNode\nfrom XAgent.ai_functions import function_manager, objgenerator\nfrom XAgent.config import CONFIG\nfrom openai import AzureOpenAI\nfrom .post_process import (\n    parse_math_answer,\n    remove_not,\n    cal_not,\n    parse_not,\n    extract_code,\n    exec_code,\n)\n\n\nclass PlanVerifyAgent(BaseAgent):\n    \"\"\"PlanRefineAgent is a subclass of PlanGenerateAgent and is involved in refining the plan.\n\n    This class utilizes the required ability of plan refinement to parse information\n    and generate a refined plan. It includes placeholders as the desired expressions.\n\n    Attributes:\n        abilities: A set of required abilities for the Agent. For PlanRefineAgent, it includes plan refinement.\n    \"\"\"\n\n    abilities = set([RequiredAbilities.verify_refine])\n\n    def call_engine(self, message):\n        \"\"\"\n        ask chatgpt\n        \"\"\"\n        model = CONFIG.default_completion_kwargs[\"model\"]\n        model_name = CONFIG.api_keys[model][0][\"engine\"]\n        response = None\n        temp = 0.2\n        client = AzureOpenAI(\n            api_key=os.getenv(\"OPENAI_API_KEY\", None),\n\n        )\n\n        try:\n\n            response = client.chat.completions.create(\n                model=model_name,\n                messages=message,\n                temperature=temp,\n            )\n\n            response = response.choices[0].message.content\n            print(response)\n\n        except Exception as e:\n            print(e)\n            print(\"chatcompletion Verify: Fail\")\n        return response\n\n    def parse(\n        self,\n        placeholders: dict = {},\n        arguments: dict = None,\n        functions=None,\n        function_call=None,\n        stop=None,\n        additional_messages: List[Message] = [],\n        additional_insert_index: int = -1,\n        now_plan=None,\n        *args,\n        **kwargs\n    ):\n        \"\"\"Parses information in order to refine the existing plan.\n\n        This method fills in placeholders with corresponding expressions, then prompts and\n        additional messages are processed and converged into final messages. Finally, the\n        'generate' method of PlanGenerateAgent class is then invoked on the final messages.\n\n        Args:\n            placeholders (dict, optional): Desired expressions to fill in partially completed text snippets.\n            arguments (dict, optional): Arguments to the function.\n            functions (optional): Functions to be carried out.\n            function_call (optional): Functional request from the user.\n            stop (optional): Stop parsing at some particular point.\n            additional_messages (List[Message], optional): Additional messages to be included in final message.\n            additional_insert_index (int, optional): Index in prompt messages where additional messages should be inserted.\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n\n        Returns:\n            object: A refined plan generated from provided placeholders, arguments, functions, and messages.\n        \"\"\"\n\n        prompt_messages = self.fill_in_placeholders(placeholders)\n\n        messages = (\n            prompt_messages[:additional_insert_index]\n            + additional_messages\n            + prompt_messages[additional_insert_index:]\n        )\n        messages = [message.raw() for message in messages]\n\n        for i in range(5):\n\n            response = self.call_engine(messages)\n            print(\"_____refined_____\\n\")\n            print(response)\n            print(\"_____refined_____\\n\")\n            python_code = extract_code(response)\n            ans = exec_code(python_code)\n            if ans != None and ans != \"None\":\n                print(\"something change!!!!!!!!!!\")\n                now_plan.data.answer = ans\n                now_plan.data.thoughts = (\n                    \"This Python code solves the problem.\\n\" + python_code\n                )\n\n                return True\n\n        return False\n"}
{"type": "source_file", "path": "XAgent/agent/plan_generate_agent/__init__.py", "content": "from .agent import PlanGenerateAgent\nfrom .prompt import get_examples_for_dispatcher"}
{"type": "source_file", "path": "XAgent/agent/father_reflect_agent/agent.py", "content": "from typing import List\nfrom XAgent.agent.base_agent import BaseAgent\nfrom XAgent.utils import RequiredAbilities\nfrom XAgent.message_history import Message\n\n\nclass FatherReflectAgent(BaseAgent):\n    \"\"\"This ReflectAgent class extends the BaseAgent class. It primarily has the ability of reflection\n    which means it can reflect upon the chat or dialogue and generate responses based on the messages\n    received.\n\n    Attributes:\n        abilities (set): Required abilities for the agent, namely reflection in this case.\n    \"\"\"\n\n    abilities = set([RequiredAbilities.father_reflection])\n\n    def parse(\n        self,\n        placeholders: dict = {},\n        arguments: dict = None,\n        functions=None,\n        function_call=None,\n        stop=None,\n        additional_messages: List[Message] = [],\n        *args,\n        **kwargs\n    ):\n        \"\"\"\n        The function is used to parse various arguments and call the generate function with these parsed arguments.\n\n        Args:\n            placeholders (dict, optional): Placeholders for the agent's responses.\n            arguments(dict, optional): Argument to influence the response of the agent.\n            functions (functions, optional): Functions to guide the agent's response.\n            function_call (FunctionType, optional): Function called to generate agent's response.\n            stop (bool, optional): Flag to stop the induction of the response.\n            additional_messages (list, optional): Additional messages to be included in the response.\n\n        Returns:\n            object: Response generated by the agent.\n\n        \"\"\"\n        prompt_messages = self.fill_in_placeholders(placeholders)\n        messages = prompt_messages + additional_messages\n\n        print(\"_____________________message in father reflect_____________________\\n\")\n        print(messages)\n        print(\"_____________________message in father reflect END_____________________\\n\")\n\n        return self.generate(\n            messages=messages,\n            arguments=arguments,\n            functions=functions,\n            function_call=function_call,\n            stop=stop,\n            *args,\n            **kwargs\n        )\n"}
{"type": "source_file", "path": "XAgent/agent/reflect_agent/prompt.py", "content": "SYSTEM_PROMPT = \"\"\"You are a posterior_knowledge_obtainer. You have performed some subtask together with:\n1.Some intermediate thoughts, this is the reasoning path.\n2.Some tool calls, which can interact with physical world, and provide in-time and accurate data.\n3.A workspace, a minimal file system and code executer.\n\nYou plan of the task is as follows:\n--- Plan ---\n{{all_plan}}\n\nYou have handled the following subtask:\n--- Handled Subtask ---\n{{terminal_plan}}\n\n\nNow, you have to learn some posterior knowledge from this process, doing the following things:\n1.Summary: Summarize the tool calls and thoughts of the existing process. You will carry these data to do next subtasks(Because the full process is too long to bring to next subtasks), So it must contain enough information of this subtask handling process. Especially, If you modified some files, Tell the file_name and what you done.\n\n2.Reflection of SUBTASK_PLAN: After performing the subtask, you get some knowledge of generating plan for the next time. This will be carried to the next time when you generate plan for a task.\n\n3.Reflection of tool calling: What knowledge of tool calling do you learn after the process? (Like \"tool xxx is not available now\", or \"I need to provide a field yyy in tool aaa\") This knowledge will be showed before handling the task next time.\"\"\"\n\nUSER_PROMPT = \"\"\n\n\ndef get_examples_for_dispatcher():\n    \"\"\"The example that will be given to the dispatcher to generate the prompt\n\n    Returns:\n        example_input: the user query or the task\n        example_system_prompt: the system prompt\n        example_user_prompt: the user prompt\n    \"\"\"\n    example_input = \"Reflect on the previous actions and give the posterior knowledge\"\n    example_system_prompt = SYSTEM_PROMPT\n\n    example_user_prompt = USER_PROMPT\n    return example_input, example_system_prompt, example_user_prompt\n"}
{"type": "source_file", "path": "XAgent/agent/dispatcher_agent/prompt.py", "content": "# SYSTEM_PROMPT = \"\"\"Your task is to devise an appropriate role-based name (_GPT) and a clear prompt for an autonomous agent to successfully complete the assigned task.\n\n# The user will provide the task, you will provide only the output in the exact format specified below with no explanation or conversation. Your response should include \"Name\", \"System Prompt\" and \"User Prompt\". The generated prompt should contain all the placeholders that will be filled by the user, including {{system_prompt_placeholders}} in system prompt, and {{user_prompt_placeholders}} in user prompt. These placeholders should be wrapped with {{ and }} in the prompt. Also, you need to keep the important information the same as those specified in the example, such as rules and response format.\n\n# Example input:\n# {{example_input}}\n\n# Example output:\n# Name: PlannerGPT\n# System Prompt: {{example_system_prompt}}\n# User Prompt: {{example_user_prompt}}\"\"\"\n\n# SYSTEM_PROMPT = \"\"\"Your task is to refine a prompt for an autonomous agent to successfully complete the assigned task. The user will provide the task, you will provide only the output in the exact format specified below with no explanation or conversation. \n\n# Below is the current version of prompt:\n# SYSTEM PROMPT:\n# {{example_system_prompt}}\n# USER PROMPT:\n# {{example_user_prompt}}\n\n# Now, please generate additional content that the agent should pay attention to when dealing with the incoming task. Note your generated content will help the agent to avoid some mistakes and more effectively solve the target task. You should only generate the additional content and avoid those unnecessary words.\n\n# Here are some prompts (resources) that maybe helpful for the given task, you could consider them. But they are irrelevant to the upcoming task, please just ignore it:\n# {{retrieved_procedure}}\n\n# Note, you should only generate the ADDITIONAL system prompts, not those existing ones. Make them concise and useful. Do not copy anything that already exist in the given system prompt. Generate new prompts!\n# \"\"\"\n\nSYSTEM_PROMPT = \"\"\"You are a prompt generator, who is capable of generating prompts for autonomous agents. Now, an agent is assigned the following task:\n{{task}}\n\nBelow is the draft prompt that will be given to the agent:\nSYSTEM PROMPT:\n```\n{{example_system_prompt}}\n```\n\nUSER PROMPT:\n```\n{{example_user_prompt}}\n```\n\nNow, please generate additional content that the agent should pay attention to when dealing with the incoming task. Your generated content should help the agent to avoid some mistakes and more effectively solve the target task. \n\nYou should only generate the ADDITIONAL user prompts, do not include the existing content. Make your additional prompt concise and informative. When responding, you should follow the following response format:\nADDITIONAL USER PROMPT:\n```\nWrite your additional user prompt here. If there is nothing to add, just set it to a special token \"[NONE]\".\n```\"\"\"\n# Here are some resources that may be helpful for the given task, you could consider them when responding. If they are irrelevant to the upcoming task, please just ignore it:\n# ```\n# {{retrieved_procedure}}\n# ```"}
{"type": "source_file", "path": "XAgent/agent/plan_refine_agent/__init__.py", "content": "from .agent import PlanRefineAgent\nfrom .prompt import get_examples_for_dispatcher"}
{"type": "source_file", "path": "XAgent/agent/plan_verify_agent/__init__.py", "content": "from .agent import PlanVerifyAgent\nfrom .prompt import get_examples_for_dispatcher\n"}
{"type": "source_file", "path": "XAgent/agent/base_agent.py", "content": "import abc\nimport json5\nfrom typing import List\nfrom colorama import Fore\nfrom copy import deepcopy\n\nfrom XAgent.config import CONFIG\nfrom XAgent.utils import LLMStatusCode, RequiredAbilities\nfrom XAgent.message_history import Message\nfrom XAgent.logs import logger\nfrom XAgent.ai_functions import objgenerator\n\n\nclass BaseAgent(metaclass=abc.ABCMeta):\n    \"\"\"\n    The BaseAgent class abstracts the essential attributes and methods for classes,\n    which inherit it. It is a metaclass of the Abstract Base Class (abc module).\n\n    Attributes:\n        abilities (set): A set of RequiredAbilities, which are necessary skills for BaseAgent.\n    \"\"\"\n\n    abilities = set(\n        [\n            RequiredAbilities.plan_generation,\n            RequiredAbilities.plan_refinement,\n            RequiredAbilities.task_evaluator,\n            RequiredAbilities.tool_tree_search,\n            RequiredAbilities.reflection,\n            RequiredAbilities.summarization,\n            RequiredAbilities.father_reflection,\n            RequiredAbilities.simple_search,\n            RequiredAbilities.verify_refine,\n        ]\n    )\n\n    def __init__(self, config, prompt_messages: List[Message] = None):\n        \"\"\"\n        Constructs an agent object with set abilities, configuration settings,\n        and initial set of prompt messages.\n\n        Args:\n            config (obj): Configuration settings for agent.\n            prompt_messages (List): Initial set of messages user gives to interact with the agent.\n        \"\"\"\n        logger.typewriter_log(\n            f\"Constructing an Agent:\",\n            Fore.YELLOW,\n            self.__class__.__name__,\n        )\n        self.config = config\n        self.prompt_messages = prompt_messages\n        self.usage = {}\n\n    @abc.abstractmethod\n    def parse(self, **args) -> (LLMStatusCode, Message, dict):\n        \"\"\"\n        Abstract method that needs to be implemented by the subclasses.\n        Required for parsing the given arguments.\n        \"\"\"\n        pass\n\n    def fill_in_placeholders(self, placeholders: dict):\n        \"\"\"\n        Fills in placeholders defined in the input with the corresponding values.\n\n        Args:\n            placeholders (dict): A dictionary containing keys as placeholders and values as their replacements.\n\n        Returns:\n            filled_messages: A copy of the initial prompt_messages with placeholders replaced with their corresponding values.\n        \"\"\"\n        filled_messages = deepcopy(self.prompt_messages)\n        for message in filled_messages:\n            role = message.role\n            if role in placeholders:\n                for key, value in placeholders[role].items():\n                    message.content = message.content.replace(\n                        \"{{\" + str(key) + \"}}\", str(value)\n                    )\n        return filled_messages\n\n    def generate(\n        self,\n        messages: list[dict] | list[Message],\n        arguments: dict = None,\n        functions: list[dict] = None,\n        function_call: dict = None,\n        stop: dict = None,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        Generates a response from the AI model, using the given messages, arguments, functions,\n        and a function call.\n\n        Args:\n            messages (list[dict]|list[Message]): A list of messages with which to interact with the AI model.\n            arguments (dict, optional): A dictionary containing arguments to use for AI model responses.\n            functions (list[dict], optional): A list of dictionaries representing functions to use for AI model responses.\n            function_call (dict, optional): A dictionary representing a function call to use for AI model responses.\n            stop (dict, optional): A dictionary that signifies when to stop the conversation with the AI model.\n            *args: Variable list of arguments.\n            **kwargs: Arbitrary keyword arguments.\n\n        Returns:\n            message (dict): A message generated by the AI model.\n            tokens (int): Number of tokens used in generating the AI model's response.\n        \"\"\"\n        if isinstance(messages[0], Message):\n            messages = [message.raw() for message in messages]\n        if functions is not None and len(functions) == 1 and function_call is None:\n            function_call = {\n                \"name\": functions[0][\"name\"]\n            }  # must call at least one function\n        match CONFIG.default_request_type:\n            case \"openai\":\n                if arguments is not None:\n                    if functions is None or len(functions) == 0:\n                        functions = [{\"name\": \"reasoning\", \"parameters\": arguments}]\n                        function_call = {\"name\": \"reasoning\"}\n                    elif len(functions) == 1:\n                        for k, v in arguments[\"properties\"].items():\n                            functions[0][\"parameters\"][\"properties\"][k] = v\n                            if k in arguments[\"required\"]:\n                                functions[0][\"parameters\"][\"required\"].append(k)\n                    else:\n                        raise NotImplementedError(\n                            \"Not implemented for multiple functions with arguments\"\n                        )\n\n                response = objgenerator.chatcompletion(\n                    messages=messages,\n                    functions=functions,\n                    function_call=function_call,\n                    stop=stop,\n                    *args,\n                    **kwargs,\n                )\n\n                message = {}\n                function_call_args: dict = json5.loads(\n                    response[\"choices\"][0][\"message\"][\"function_call\"][\"arguments\"]\n                )\n\n                if arguments is not None:\n                    message[\"arguments\"] = {\n                        k: function_call_args.pop(k)\n                        for k in arguments[\"properties\"].keys()\n                        if k in function_call_args\n                    }\n                if len(function_call_args) > 0:\n                    message[\"function_call\"] = {\n                        \"name\": response[\"choices\"][0][\"message\"][\"function_call\"][\n                            \"name\"\n                        ],\n                        \"arguments\": function_call_args,\n                    }\n\n            case \"xagent\":\n                response = objgenerator.chatcompletion(\n                    messages=messages,\n                    arguments=arguments,\n                    functions=functions,\n                    function_call=function_call,\n                    stop=stop,\n                    *args,\n                    **kwargs,\n                )\n                message = json5.loads(response[\"choices\"][0][\"message\"][\"content\"])\n            case _:\n                raise NotImplementedError(\n                    f\"Request type {CONFIG.default_request_type} not implemented\"\n                )\n\n        tokens = response[\"usage\"]\n        return message, tokens\n"}
{"type": "source_file", "path": "XAgent/agent/plan_refine_agent/prompt.py", "content": "SYSTEM_PROMPT = '''You are plan-rectify agent, your task is to iteratively rectify a plan of a query.\n--- Background Information ---\nPLAN AND SUBTASK:\nA plan has a tree manner of subtasks: task 1 contains subtasks task 1.1, task 1.2, task 1.3, and task 1.2 contains subtasks 1.2.1, 1.2.2...\nPlease remember:\n1.The plan tree has a max width of {{max_plan_tree_width}}, meaning the max subtask count of a task. If max_width=4, the task like 1.4 is valid, but task 1.5 is not valid.\n2.The plan tree has a max depth of {{max_plan_tree_depth}}. If max_depth=3, the task like 1.3.2 is valid, but task 1.4.4.5 is not valid.\n\nA subtask-structure has the following json component:\n{\n\"subtask name\": string\n\"goal.goal\": string, the main purpose of the sub-task should handle, and what will you do to reach this goal?\n\"goal.criticism\": string, What problems may the current subtask and goal have?\n\"milestones\": list[string]. How to automatically check the sub-task is done?\n}\n\nSUBTASK HANDLE:\nA task-handling agent will handle all the subtasks as the inorder-traversal. For example:\n1. it will handle subtask 1 first.\n2. if solved, handle subtask 2. If failed, split subtask 1 as subtask 1.1 1.2 1.3... Then handle subtask 1.1.\n3. Handle subtasks recursively, until all subtasks are solved.\n4. It is powered by a state-of-the-art LLM, so it can handle many subtasks without using external tools or execute codes.\n\nRESOURCES:\n1. A FileSystemEnv to read and write files (txt, code, markdown, latex...)\n2. A python interpretor to execute python files together with a pdb debugger to test and refine the code.\n3. A ShellEnv to execute bash or zsh command to further achieve complex goals. \n\n--- Task Description ---\nYour task is iteratively rectify a given plan and based on the goals, suggestions and now handling postions. \n\nPLAN_REFINE_MODE: At this mode, you will use the given operations to rectify the plan. At each time, use one operation.\nSUBTASK OPERATION:\n - split: Split a already handled but failed subtask into subtasks because it is still so hard. The `target_subtask_id` for this operation must be a leaf task node that have no children subtasks, and should provide new splitted `subtasks` of length 2-4. You must ensure the `target_subtask_id` exist, and the depth of new splitted subtasks < {{max_plan_tree_depth}}.\n    - split 1.2 with 2 subtasks will result in create new 1.2.1, 1.2.2 subtasks.\n - add: Add new subtasks as brother nodes of the `target_subtask_id`. This operation will expand the width of the plan tree. The `target_subtask_id` should point to a now handling subtask or future subtask.\n    - add 1.1 with 2 subtasks will result in create new 1.2, 1.3 subtasks.\n    - add 1.2.1 with 3 subtasks wil result in create new 1.2.2, 1.2.3, 1.2.4 subtasks.\n - delete: Delete a subtask. The `target_subtask_id` should point to a future/TODO subtask. Don't delete the now handling or done subtask.\n    - delete 1.2.1 will result in delete 1.2.1 subtask.\n - exit: Exit PLAN_REFINE_MODE and let task-handle agent to perform subtasks.\n\n--- Note ---\nThe user is busy, so make efficient plans that can lead to successful task solving.\nDo not waste time on making irrelevant or unnecessary plans.\nDon't use search engine since you have the knowledge for planning.\nDon't divide trivial task into multiple steps. \nIf task is un-solvable, give up and submit the task.\n\n*** Important Notice ***\n- Never change the subtasks before the handling positions, you can compare them in lexicographical order.\n- Never create (with add or split action) new subtasks that similar or same as the existing subtasks.\n- For subtasks with similar goals, try to do them together in one subtask with a list of subgoals, rather than split them into multiple subtasks.\n- Every time you use a operation, make sure the hierarchy structure of the subtasks remians, e.g. if a subtask 1.2 is to \"find A,B,C\" , then newly added plan directly related to this plan (like \"find A\", \"find B\", \"find C\") should always be added as 1.2.1, 1.2.2, 1.2.3...\n- You are restricted to give operations in at most 4 times, so the plan refine is not so much.\n- The task handler is powered by sota LLM, which can directly answer many questions. So make sure your plan can fully utilize its ability and reduce the complexity of the subtasks tree.\n'''\n\nUSER_PROMPT = '''Your task is to choose one of the operators of SUBTASK OPERATION, note that\n1.You can only modify the subtask with subtask_id>{{subtask_id}}(not included). \n2.If you think the existing plan is good enough, use REFINE_SUBMIT.\n3.You can at most perform {{max_step}} operations before REFINE_SUBMIT operation, you have already made {{modify_steps}} steps, watch out the budget. \n4.All the plan has a max depth of {{max_plan_tree_depth}}. Be carefull when using SUBTASK_SPLIT.\n5. Please use function call to respond to me (remember this!!!).\n\n--- Status ---\nFile System Structure: {{workspace_files}}\nRefine Node Message: {{refine_node_message}}\n'''\n\ndef get_examples_for_dispatcher():\n    \"\"\"The example that will be given to the dispatcher to generate the prompt\n\n    Returns:\n        example_input: the user query or the task\n        example_system_prompt: the system prompt\n        example_user_prompt: the user prompt\n    \"\"\"\n    example_input = \"Refine a plan for writing a Python-based calculator.\"\n    example_system_prompt = SYSTEM_PROMPT\n    example_user_prompt = USER_PROMPT\n    return example_input, example_system_prompt, example_user_prompt"}
{"type": "source_file", "path": "XAgent/agent/reflect_agent/__init__.py", "content": "from .agent import ReflectAgent\nfrom .prompt import get_examples_for_dispatcher"}
{"type": "source_file", "path": "XAgent/agent/plan_verify_agent/post_process.py", "content": "import json\nimport re\nimport sys\nfrom io import StringIO\n\n\ndef remove_not(x):\n    match_number = re.compile(\"[\\$]?\\ *10\\^[{]?\\ *-?[0-9]+\\ *[}]?\\ *[\\$]?\")\n    result = re.findall(match_number, x)\n    if len(result) != 0:\n        return re.split(match_number, x)[-1]\n    return None\n\n\ndef parse_not(inputs):\n    if not inputs:\n        return \"\", \"\"\n    if \"\\times\" in inputs:\n        x, ab = inputs.split(\"\\times\")\n    elif \"\\\\times\" in inputs:\n        x, ab = inputs.split(\"\\\\times\")\n    elif \"*\" in inputs:\n        x, ab = inputs.split(\"*\")\n    else:\n        return inputs\n    return x, ab\n\n\ndef cal_not(inputs):\n\n    try:\n        x, ab = list(inputs)\n        match_number = re.compile(\"10\\^[{]?\\ *-?[0-9]+\\ *[}]?\")\n        ab = re.findall(match_number, ab)[0]\n        ab = ab[ab.find(\"^\") + 1 :]\n        if \"{\" in ab:\n            ab = ab[ab.find(\"{\") + 1 :]\n        if \"}\" in ab:\n            ab = ab[: ab.find(\"}\")]\n        x = x.strip()\n        out = float(x) * 10 ** float(ab)\n        # print(float(x)*10**float(ab))\n        return str(out)\n    except:\n        print(\"error\")\n    return inputs\n\n\ndef remove_boxed(s):\n    left = \"oxed{\"  # change\n    try:\n        assert s[: len(left)] == left\n        assert s[-1] == \"}\"\n        answer = s[len(left) : -1]\n        if \"=\" in answer:\n            answer = answer.split(\"=\")[-1].lstrip(\" \")\n        return answer\n    except:\n        return None\n\n\ndef last_boxed_only_string(string):\n    if string == None:\n        return None\n    idx = string.rfind(\"oxed\")  # change\n    if idx < 0:\n        idx = string.rfind(\"\\\\fbox\")\n        if idx < 0:\n            return None\n    i = idx\n    right_brace_idx = None\n    num_left_braces_open = 0\n    while i < len(string):\n        if string[i] == \"{\":\n            num_left_braces_open += 1\n        if string[i] == \"}\":\n            num_left_braces_open -= 1\n            if num_left_braces_open == 0:\n                right_brace_idx = i\n                break\n        i += 1\n\n    if right_brace_idx == None:\n        retval = None\n    else:\n        retval = string[idx : right_brace_idx + 1]\n\n    return retval\n\n\ndef parse_math_answer(raw_string):\n    if raw_string == None:\n        return None\n    ans = remove_boxed(last_boxed_only_string(raw_string))\n    if ans != None:\n        return ans\n    else:\n        if \"**Answer conclusion:**\" in raw_string:\n            return raw_string.split(\"**Answer conclusion:**\")[1]\n        else:\n            return None\n\n\ndef extract_code(raw_string):\n    code = None\n    if raw_string == None:\n        return None\n    idx = raw_string.find(\"```\")\n    raw_string = raw_string[idx + 3 :]\n    idx = raw_string.find(\"```\")\n    code = raw_string[:idx]\n    if code == \"\":\n        return None\n    if code[:6] == \"python\":\n        code = code[6:]\n    if not (\"print\" in code):\n        code = None\n    return code\n\n\ndef exec_code(code):\n    ans = \"None\"\n    old_stdout = sys.stdout\n    redirected_output = sys.stdout = StringIO()\n    try:\n        exec(code)\n        sys.stdout = old_stdout\n        ans = redirected_output.getvalue().strip()\n    except:\n        sys.stdout = old_stdout\n        print(\"fail when exec the code\")\n        return \"None\"\n    return ans\n"}
{"type": "source_file", "path": "XAgent/agent/dispatcher.py", "content": "import abc\nfrom typing import List\nfrom colorama import Fore, Style\nfrom XAgent.config import CONFIG\n\nfrom XAgent.agent.base_agent import BaseAgent\nfrom XAgent.utils import RequiredAbilities, TaskSaveItem, AgentRole\nfrom XAgent.agent.dispatcher_agent import DispatcherAgent\nfrom XAgent.message_history import Message\n\n\nclass AgentDispatcher(metaclass=abc.ABCMeta):\n    \"\"\"\n    Base abstract class for Agent Dispatcher.\n    \"\"\"\n\n    def __init__(self, logger=None):\n        \"\"\"\n        Initialize AgentDispatcher. Assign agent markets for each requirement in RequiredAbilities.\n        Agent markets are initially empty.\n        \"\"\"\n        self.agent_markets = {}\n        self.logger = logger\n        for requirement in RequiredAbilities:\n            self.agent_markets[requirement] = []\n        self.logger.typewriter_log(\n            f\"Constructing an AgentDispatcher:\",\n            Fore.YELLOW,\n            self.__class__.__name__,\n        )\n\n    @abc.abstractmethod\n    def dispatch(self, ability_type: RequiredAbilities, target_task) -> BaseAgent:\n        \"\"\"\n        Abstract dispatch method to be implemented by subclasses. Dispatches tasks based\n        on ability type.\n\n        Args:\n            ability_type (RequiredAbilities): The ability type required for the task.\n            target_task: The task which needs to be dispatched.\n\n        Returns:\n            BaseAgent: Base agent responsible for the task.\n        \"\"\"\n        pass\n\n    def dispatch_role(self, target_task: TaskSaveItem) -> AgentRole:\n        \"\"\"\n        Dispatch a role for the target task.\n\n        Args:\n            target_task (TaskSaveItem): The task for which a role needs to be dispatched.\n\n        Returns:\n            AgentRole: Returns a default AgentRole.\n        \"\"\"\n        return AgentRole()\n\n    def regist_agent(self, agent: BaseAgent):\n        \"\"\"\n        Register agent to the respective agent markets based on abilities.\n\n        Args:\n            agent (BaseAgent): The agent that needs to be registered.\n        \"\"\"\n        for requirement in RequiredAbilities:\n            if requirement in agent.abilities:\n                self.agent_markets[requirement].append(agent)\n\n\nclass AutomaticAgentDispatcher(AgentDispatcher):\n    \"\"\"\n    AgentDispatcher that automatically dispatches tasks to agents.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize AutomaticAgentDispatcher.\n        \"\"\"\n        super().__init__()\n\n    def dispatch(self, ability_type: RequiredAbilities, target_task) -> BaseAgent:\n        \"\"\"\n        Dispatch task to the agent in the market corresponding to the task ability type.\n\n        Args:\n            ability_type (RequiredAbilities): The ability type required for the task.\n            target_task: The task which needs to be dispatched.\n\n        Returns:\n            BaseAgent: Base agent responsible for the task.\n        \"\"\"\n        return self.agent_markets[ability_type][0]()\n\n\nclass XAgentDispatcher(AgentDispatcher):\n    \"\"\"Generate the prompt and the agent for the given task.\"\"\"\n\n    def __init__(self, config, enable=True, logger=None):\n        \"\"\"\n        Initialize XAgentDispatcher.\n\n        Args:\n            config: Dispatcher configuration.\n            enable (bool, optional): Whether the dispatcher is active. Defaults to True.\n        \"\"\"\n        self.logger = logger\n        super().__init__(logger)\n        self.config = config\n        self.dispatcher = DispatcherAgent(config)\n        self.enable = enable\n\n    def get_examples(self, ability_type: RequiredAbilities, direct=0):\n        \"\"\"\n        Get examples based on the ability type.\n\n        Args:\n            ability_type (RequiredAbilities): The ability type for which examples are needed.\n\n        Returns:\n            Returns examples for the dispatcher.\n        \"\"\"\n        if ability_type == RequiredAbilities.plan_generation:\n            from .plan_generate_agent import get_examples_for_dispatcher\n        elif ability_type == RequiredAbilities.plan_refinement:\n            from .plan_refine_agent import get_examples_for_dispatcher\n        elif ability_type == RequiredAbilities.tool_tree_search:\n            from .tool_agent import get_examples_for_dispatcher\n        elif ability_type == RequiredAbilities.reflection:\n            from .reflect_agent import get_examples_for_dispatcher\n        elif ability_type == RequiredAbilities.father_reflection:\n            from .father_reflect_agent import get_examples_for_dispatcher\n        elif ability_type == RequiredAbilities.simple_search:\n            from .simple_agent import get_examples_for_dispatcher\n\n            if direct == 1:\n                from .simple_agent import get_examples_for_dispatcher_d\n\n                return get_examples_for_dispatcher_d()\n        elif ability_type == RequiredAbilities.verify_refine:\n            from .plan_verify_agent import get_examples_for_dispatcher\n        return get_examples_for_dispatcher()\n\n    def build_agent(\n        self,\n        ability_type: RequiredAbilities,\n        config,\n        prompt_messages: List[Message],\n        *args,\n        **kwargs,\n    ) -> BaseAgent:\n        \"\"\"\n        Build agent based on the ability type. If failed, fallback to use default agent.\n\n        Args:\n            ability_type (RequiredAbilities): Type of ability required by the agent.\n            config: Configuration for the agent.\n            prompt_messages (List[Message]): List of prompt messages for the agent.\n\n        Returns:\n            BaseAgent: The built agent.\n        \"\"\"\n        try:\n            agent = self.agent_markets[ability_type][0](\n                config, prompt_messages, *args, **kwargs\n            )\n        except:\n            # TODO: remove when all the agents can be created with dispatcher.\n            self.logger.info(\"build agent error, use default agent\")\n            agent = self.agent_markets[ability_type][0](config, *args, **kwargs)\n        return agent\n\n    def dispatch(\n        self,\n        ability_type: RequiredAbilities,\n        target_task: TaskSaveItem,\n        *args,\n        **kwargs,\n    ) -> BaseAgent:\n        \"\"\"\n        Dispatch task to the agent in the market corresponding to the task ability type.\n        Additionally refines the prompt for the task and builds the agent.\n\n        Args:\n            ability_type (RequiredAbilities): The ability type required for the task.\n            target_task (TaskSaveItem): The task which needs to be dispatched.\n\n        Returns:\n            BaseAgent: Base agent responsible for the task.\n        \"\"\"\n        # Retrieve the corresponding agent's prompt.\n        example_input, example_system_prompt, example_user_prompt = self.get_examples(\n            ability_type\n        )\n        if self.enable:\n            self.logger.typewriter_log(\n                self.__class__.__name__,\n                Fore.GREEN,\n                f\"Refine the prompt of a specific agent for {Fore.GREEN}RequiredAbilities.{ability_type.name}{Style.RESET_ALL}\",\n            )\n            _, prompt_messages, tokens = self.dispatcher.parse(\n                target_task, example_input, example_system_prompt, example_user_prompt\n            )\n            print(prompt_messages)\n            if prompt_messages[0].content == \"\" and prompt_messages[1].content == \"\":\n                self.logger.info(\n                    \"Dispatcher fail to follow the output format, we fallback to use the default prompt.\"\n                )\n                prompt_messages = [\n                    Message(role=\"system\", content=example_system_prompt),\n                    Message(role=\"user\", content=example_user_prompt),\n                ]\n            else:\n                self.logger.typewriter_log(\n                    self.__class__.__name__, Fore.GREEN, f\"The prompt has been refined!\"\n                )\n        else:\n            print(\"self.enable = \", self.enable)\n            prompt_messages = [\n                Message(role=\"system\", content=example_system_prompt),\n                Message(role=\"user\", content=example_user_prompt),\n            ]\n        agent = self.build_agent(\n            ability_type, self.config, prompt_messages, *args, **kwargs\n        )\n        return agent\n\n\n# agent_dispatcher = XAgentDispatcher(CONFIG, enable=False)\n"}
{"type": "source_file", "path": "XAgent/agent/plan_generate_agent/agent.py", "content": "from typing import List\nfrom XAgent.agent.base_agent import BaseAgent\nfrom XAgent.utils import RequiredAbilities\nfrom XAgent.ai_functions import function_manager, objgenerator\nfrom XAgent.message_history import Message\n\n\nclass PlanGenerateAgent(BaseAgent):\n    \"\"\"\n    This class is responsible for plan generation. It is a subclass of BaseAgent.\n\n    Attributes:\n        abilities: A set indicating the abilities required by this agent.\n    \"\"\"\n\n    abilities = set([RequiredAbilities.plan_generation])\n\n    def parse(\n        self,\n        placeholders: dict = {},\n        arguments: dict = None,\n        functions=None,\n        function_call=None,\n        stop=None,\n        additional_messages: List[Message] = [],\n        *args,\n        **kwargs\n    ):\n        \"\"\"\n        This method is used to parse placeholders, arguments, function calls, and additional messages\n        to generate a plan.\n\n        Args:\n            placeholders (dict, optional): A dictionary containing placeholders to fill in the messages.\n            arguments (dict, optional): A dictionary containing arguments to be used in the functions.\n            functions: The functions to be used during plan generation.\n            function_call: The object representing a function call.\n            stop: The condition to stop the plan generation process if specified.\n            additional_messages (List[Message], optional): A list of additional messages to be added to\n            the initial prompt messages.\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n\n        Returns:\n            This method returns the result of the plan generated by the \"generate\" method.\n        \"\"\"\n        prompt_messages = self.fill_in_placeholders(placeholders)\n        messages = prompt_messages + additional_messages\n        return self.generate(\n            messages=messages,\n            arguments=arguments,\n            functions=functions,\n            function_call=function_call,\n            stop=stop,\n            *args,\n            **kwargs\n        )\n"}
{"type": "source_file", "path": "XAgent/data_structure/tree.py", "content": "from XAgent.data_structure.node import ToolNode\n\n\nclass TaskSearchTree:\n    \"\"\"\n    TaskSearchTree represents a tree data structure with specific task searching behavior. \n\n    Attributes:\n        root (ToolNode): Root node of the tree.\n        now_expand_num (int): Maintains current expanding number for nodes during traversal.\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"Initializes TaskSearchTree with a root ToolNode and default expanding number.\"\"\"\n        self.root: ToolNode = ToolNode()\n        self.root.expand_num = 0\n        self.now_expand_num = 1\n\n    def get_depth(self):\n        \"\"\"\n        Gets the depth of the tree from the current root node.\n\n        Returns:\n            int: The depth of the tree\n        \"\"\"\n        return self.root.get_depth()\n    \n    def get_subtree_size(self):\n        \"\"\"\n        Gets the number of nodes (or size) of the subtree from the current root node.\n\n        Returns:\n            int: The number of nodes in the subtree\n        \"\"\"\n        return self.root.get_subtree_size()\n    \n    def make_father_relation(self, father, child):\n        \"\"\"\n        Establishes a parent-child relationship between two given nodes.\n\n        Args:\n            father (ToolNode): The parent node in the relation.\n            child (ToolNode): The child node in the relation.\n\n        Raises:\n            TypeError: If the father or child is not a ToolNode instance.\n        \"\"\"\n        if not (isinstance(father, ToolNode) and isinstance(child, ToolNode)):\n            raise TypeError(\"Father and child both need to be instances of ToolNode.\")\n\n        child.expand_num = self.now_expand_num\n        self.now_expand_num += 1\n\n        child.father = father\n        father.children.append(child)"}
{"type": "source_file", "path": "XAgent/ai_functions/request/error.py", "content": "class FunctionCallSchemaError(Exception):\n    \"\"\"Exception raised when there is an error in the structure or format of a function call.\n\n    This exception does not accept any arguments or custom messages. It is thrown when there is an issue\n    with the schema or structure of a function call, such as passing the wrong data type, too many or too few\n    arguments, etc. This error is used to halt execution and signal that the function call needs to be \n    corrected before the program can continue.\n    \"\"\"\n    pass"}
{"type": "source_file", "path": "XAgent/data_structure/plan.py", "content": "from typing import List, Optional\nfrom XAgent.utils import TaskSaveItem, TaskStatusCode\nfrom XAgent.data_structure.node import ToolNode\n\n\nclass Plan:\n    \"\"\"Class representing a task plan.\n\n    Attributes:\n        father (Optional[Plan]): Parent task plan.\n        children (List[Plan]): List of child task plans.\n        data (TaskSaveItem): Data items related to the task plan.\n        process_node (ToolNode): Node responsible for the task plan processing.\n    \"\"\"\n\n    def __init__(self, data: TaskSaveItem):\n        \"\"\"Initialises a Plan object.\n\n        Args:\n            data (TaskSaveItem): Data related to the task plan.\n        \"\"\"\n        self.father: Optional[Plan] = None\n        self.children: List[Plan] = []\n        self.data: TaskSaveItem = data\n        self.process_node: ToolNode = None\n\n    def to_json(self, posterior=True, thoughts=False):\n        \"\"\"Converts Plan object to JSON.\n\n        Args:\n            posterior (bool): Determines whether the task's posterior data\n                              is also returned.\n\n        Returns:\n            root_json (dict): JSON format representation of the Plan object.\n        \"\"\"\n        root_json = self.data.to_json(posterior=posterior, thoughts=thoughts)\n        if self.process_node:\n            root_json[\"submit_result\"] = self.process_node.data[\"command\"][\"properties\"]\n\n        # if self.father != None:\n        root_json[\"task_id\"] = self.get_subtask_id(to_str=True)\n        if len(self.children) > 0:\n            root_json[\"subtask\"] = [subtask.to_json() for subtask in self.children]\n        return root_json\n\n    def get_subtask_id(self, to_str=False):\n        \"\"\"Gets the subtask ID.\n\n        Args:\n            to_str (bool): Determines if returned ID is string.\n\n        Returns:\n            subtask_id_list (list): List of subtask IDs.\n        \"\"\"\n        subtask_id_list = self.get_subtask_id_list()\n        if to_str:\n            subtask_id_list = [str(cont) for cont in subtask_id_list]\n            return \".\".join(subtask_id_list)\n        else:\n            return subtask_id_list\n\n    def get_subtask_id_list(self):\n        \"\"\"Gets the subtask ID list.\n\n        Returns:\n            Array of subtask IDs if father is not none else [1].\n        \"\"\"\n        if self.father == None:\n            return [1]\n        father_subtask_id = self.father.get_subtask_id()\n        child_id = self.father.children.index(self) + 1\n        father_subtask_id.append(child_id)\n        return father_subtask_id\n\n    @classmethod\n    def make_relation(cls, father, child):\n        \"\"\"Establishes a parent-child relationship between two plans.\n\n        Args:\n            father: Parent plan.\n            child: Child plan.\n        \"\"\"\n        father.children.append(child)\n        child.father = father\n\n    def get_root(self):\n        \"\"\"Fetches the root of the Plan tree.\n\n        Returns:\n            Root Plan object.\n        \"\"\"\n        if self.father == None:\n            return self\n        return self.father.get_root()\n\n    def get_depth(self):\n        \"\"\"Returns the depth of the Plan tree.\n\n        Returns:\n            Tree depth as an integer.\n        \"\"\"\n        if self.father == None:\n            return 1\n        return 1 + self.father.get_depth()\n\n    @classmethod\n    def get_inorder_travel(cls, now_plan):\n        \"\"\"Performs an inorder traversal of the plan tree.\n\n        Args:\n            now_plan: Current plan in the tree.\n\n        Returns:\n            All plans in the tree in inorder.\n        \"\"\"\n        result_list = [now_plan]\n        for child in now_plan.children:\n            result_list.extend(Plan.get_inorder_travel(child))\n        return result_list\n\n    @classmethod\n    def pop_next_subtask(cls, now_plan):\n        \"\"\"Fetches the next subtask in the queue.\n\n        Args:\n            now_plan: Current plan in the tree.\n\n        Returns:\n            Next subtask in the queue.\n        \"\"\"\n        root_plan = now_plan.get_root()\n        all_plans = Plan.get_inorder_travel(root_plan)\n        order_id = all_plans.index(now_plan)\n        for subtask in all_plans[order_id + 1 :]:\n            if subtask.data.status == TaskStatusCode.TODO:\n                return subtask\n        return None\n\n    @classmethod\n    def get_remaining_subtask(cls, now_plan):\n        \"\"\"Gets all remaining subtasks from a given point.\n\n        Args:\n            now_plan: Current plan in the tree.\n\n        Returns:\n            Array of all remaining subtasks.\n        \"\"\"\n        root_plan = now_plan.get_root()\n        all_plans = Plan.get_inorder_travel(root_plan)\n        order_id = all_plans.index(now_plan)\n        return all_plans[order_id:]\n"}
{"type": "source_file", "path": "XAgent/inner_loop_search_algorithms/__init__.py", "content": ""}
{"type": "source_file", "path": "XAgent/core.py", "content": "\"\"\"XAgent Core Components\"\"\"\n\nimport abc\nfrom datetime import datetime\nimport os\nimport uuid\n\nfrom XAgent.memory_db_plan import PlanMemoryDB\nfrom colorama import Fore\nfrom XAgent.agent.dispatcher import XAgentDispatcher\nfrom XAgent.agent import (\n    PlanGenerateAgent,\n    PlanRefineAgent,\n    ToolAgent,\n    ReflectAgent,\n    FatherReflectAgent,\n    SimpleAgent,\n    PlanVerifyAgent,\n)\nfrom XAgent.function_handler import FunctionHandler\nfrom XAgent.utils import TaskSaveItem\n\nfrom XAgent.memory_db import MemoryDB\nfrom XAgent.workflow.base_query import AutoGPTQuery, BaseQuery\n\nfrom XAgent.loggers.logs import Logger\n\n\nclass XAgentParam(metaclass=abc.ABCMeta):\n    \"\"\"\n    XAgent Param\n    \"\"\"\n\n    def __init__(\n        self, config=None, query: BaseQuery = None, newly_created: bool = True\n    ) -> None:\n        self.config = config\n        self.query = query\n        self.newly_created = newly_created\n\n    def build_query(self, query: dict):\n        \"\"\"\n        build query\n        \"\"\"\n        self.query = AutoGPTQuery(**query)\n\n    def build_config(self, config):\n        \"\"\"\n        build config\n        \"\"\"\n        self.config = config\n\n\nclass XAgentCoreComponents(metaclass=abc.ABCMeta):\n    \"\"\"\n    XAgent Core Components\n    Components:\n        logger: Logging\n        recorder: Running recorder\n        toolserver_interface: Tool server interface\n        function_handler: Function handler\n        working_memory_function: Working memory\n        agent_dispatcher: Agent dispatcher\n        vector_db_interface: Vector database interface\n        interaction: Interaction\n\n    All components in the component set are globally unique.\n    \"\"\"\n\n    global_recorder = None\n\n    def __init__(self) -> None:\n        self.interaction = None\n        self.client_id = None\n        self.logger = None\n        self.recorder = None\n        self.toolserver_interface = None\n        self.function_handler = None\n        self.tool_functions_description_list = []\n        self.function_list = []\n        self.working_memory_function = None\n        self.agent_dispatcher = None\n        self.vector_db_interface = None\n        self.base_dir = \"\"\n        self.extract_dir = \"\"\n        self.available_agents = [\n            PlanGenerateAgent,\n            PlanRefineAgent,\n            ToolAgent,\n            ReflectAgent,\n            FatherReflectAgent,\n            SimpleAgent,\n            PlanVerifyAgent,\n        ]\n        self.client_id = uuid.uuid4().hex\n        self.date_str = datetime.now().strftime(\"%Y-%m-%d\")\n\n    def register_logger(self):\n        \"\"\"\n        register a logger to the core components\n        \"\"\"\n\n        self.base_dir = os.path.join(\n            os.path.join(\"Xagent\", \"localstorage\", \"interact_records\"),\n            self.date_str,\n            self.client_id,\n        )\n        if not os.path.exists(self.base_dir):\n            os.makedirs(self.base_dir, exist_ok=True)\n\n        self.extract_dir = os.path.join(self.base_dir, \"workspace\")\n        if not os.path.exists(self.extract_dir):\n            os.makedirs(self.extract_dir, exist_ok=True)\n\n        self.log_dir = os.path.join(\n            os.path.join(\"Xagent\", \"localstorage\", \"interact_records\"),\n            self.date_str,\n            self.client_id,\n        )\n        self.logger = Logger(log_dir=self.log_dir, log_file=f\"interact.log\")\n\n    def register_agent_dispatcher(self, param: XAgentParam):\n        \"\"\"\n        register a agent dispatcher to the core components\n        \"\"\"\n        self.logger.info(\"register agent dispatcher\")\n        self.agent_dispatcher = XAgentDispatcher(\n            param.config, enable=False, logger=self.logger\n        )\n        for agent in self.available_agents:\n            self.agent_dispatcher.regist_agent(agent)\n\n    def register_function_handler(self, config):\n        \"\"\"\n        register a function handler to the core components\n        \"\"\"\n        self.logger.info(\"register function handler\")\n        self.function_handler = FunctionHandler(\n            config=config,\n            logger=self.logger,\n        )\n\n    def register_vector_db_interface(self, insert_mode, init_mode):\n        \"\"\"\n        register a vector db interface to the core components\n        \"\"\"\n        self.logger.info(\n            f\"for vector_db insert_mode: {insert_mode}, init_mode: {init_mode}\",\n        )\n        self.vector_db_interface = MemoryDB(\n            insert_mode=insert_mode, init_mode=init_mode\n        )\n        pass\n\n    def register_all(self, param: XAgentParam, insert_mode, init_mode):\n        \"\"\"\n        register all components to the core components\n        \"\"\"\n        self.register_logger()\n        self.register_agent_dispatcher(param=param)\n        self.register_function_handler(param.config)\n        self.register_vector_db_interface(insert_mode=insert_mode, init_mode=init_mode)\n\n    def build(self, param: XAgentParam, insert_mode, init_mode):\n        \"\"\"\n        start all components\n        \"\"\"\n        self.register_all(param, insert_mode, init_mode)\n        self.logger.info(\"build all components, done!\")\n\n    def start(self):\n        \"\"\"\n        start all components\n        \"\"\"\n        self.logger.info(\"start all components\")\n\n    def close(self):\n        \"\"\"\n        close all components\n        \"\"\"\n\n    def print_task_save_items(\n        self,\n        item: TaskSaveItem,\n    ) -> None:\n\n        self.logger.typewriter_log(f\"Task Name:\", Fore.YELLOW, f\"{item.name}\")\n        self.logger.typewriter_log(f\"Task Goal:\", Fore.YELLOW, f\"{item.goal}\")\n        self.logger.typewriter_log(\n            f\"Task Prior-Criticism:\", Fore.YELLOW, f\"{item.prior_plan_criticism}\"\n        )\n        if len(item.posterior_plan_reflection) > 0:\n            self.logger.typewriter_log(f\"Task Posterior-Criticism:\", Fore.YELLOW)\n            for line in item.posterior_plan_reflection:\n                line = line.lstrip(\"- \")\n                self.logger.typewriter_log(\"- \", Fore.GREEN, line.strip())\n        if len(item.milestones) > 0:\n            self.logger.typewriter_log(\n                f\"Task Milestones:\",\n                Fore.YELLOW,\n            )\n            for line in item.milestones:\n                line = line.lstrip(\"- \")\n                self.logger.typewriter_log(\"- \", Fore.GREEN, line.strip())\n        # if len(item.expected_tools) > 0:\n        #     logger.typewriter_log(\n        #         f\"Expected Tools:\", Fore.YELLOW,\n        #     )\n        #     for line in item.expected_tools:\n        #         line = f\"{line['tool_name']}: {line['reason']}\".lstrip(\"- \")\n        #         logger.typewriter_log(\"- \", Fore.GREEN, line.strip())\n        if len(item.tool_reflection) > 0:\n            self.logger.typewriter_log(\n                f\"Posterior Tool Reflections:\",\n                Fore.YELLOW,\n            )\n            for line in item.tool_reflection:\n                line = f\"{line['target_tool_name']}: {line['reflection']}\".lstrip(\"- \")\n                self.logger.typewriter_log(\"- \", Fore.GREEN, line.strip())\n\n        self.logger.typewriter_log(f\"Task Status:\", Fore.YELLOW, f\"{item.status.name}\")\n        if item.action_list_summary != \"\":\n            self.logger.typewriter_log(\n                f\"Action Summary:\", Fore.YELLOW, f\"{item.action_list_summary}\"\n            )\n\n    def print_assistant_thoughts(\n        self,\n        # ai_name: object,\n        assistant_reply_json_valid: object,\n        speak_mode: bool = False,\n    ) -> None:\n        assistant_thoughts_reasoning = None\n        assistant_thoughts_plan = None\n        assistant_thoughts_speak = None\n        assistant_thoughts_criticism = None\n\n        assistant_thoughts = assistant_reply_json_valid.get(\"thoughts\", {})\n        assistant_thoughts = assistant_thoughts.get(\"properties\", {})\n        assistant_thoughts_text = assistant_thoughts.get(\"thought\")\n        if assistant_thoughts:\n            assistant_thoughts_reasoning = assistant_thoughts.get(\"reasoning\")\n            assistant_thoughts_plan = assistant_thoughts.get(\"plan\")\n            assistant_thoughts_criticism = assistant_thoughts.get(\"criticism\")\n        if assistant_thoughts_text is not None and assistant_thoughts_text != \"\":\n            self.logger.typewriter_log(\n                f\"THOUGHTS:\", Fore.YELLOW, f\"{assistant_thoughts_text}\"\n            )\n        if (\n            assistant_thoughts_reasoning is not None\n            and assistant_thoughts_reasoning != \"\"\n        ):\n            self.logger.typewriter_log(\n                \"REASONING:\", Fore.YELLOW, f\"{assistant_thoughts_reasoning}\"\n            )\n\n        if assistant_thoughts_plan is not None and len(assistant_thoughts_plan) > 0:\n            self.logger.typewriter_log(\"PLAN:\", Fore.YELLOW, \"\")\n            # If it's a list, join it into a string\n            if isinstance(assistant_thoughts_plan, list):\n                assistant_thoughts_plan = \"\\n\".join(assistant_thoughts_plan)\n            elif isinstance(assistant_thoughts_plan, dict):\n                assistant_thoughts_plan = str(assistant_thoughts_plan)\n\n            # Split the input_string using the newline character and dashes\n            lines = assistant_thoughts_plan.split(\"\\n\")\n            for line in lines:\n                line = line.lstrip(\"- \")\n                self.logger.typewriter_log(\"- \", Fore.GREEN, line.strip())\n\n        if (\n            assistant_thoughts_criticism is not None\n            and assistant_thoughts_criticism != \"\"\n        ):\n            self.logger.typewriter_log(\n                \"CRITICISM:\", Fore.YELLOW, f\"{assistant_thoughts_criticism}\"\n            )\n        return {\n            \"thoughts\": assistant_thoughts_text,\n            \"reasoning\": assistant_thoughts_reasoning,\n            \"plan\": assistant_thoughts_plan,\n            \"criticism\": assistant_thoughts_criticism,\n            \"node_id\": uuid.uuid4().hex,\n        }\n"}
{"type": "source_file", "path": "XAgent/agent/simple_agent/prompt.py", "content": "# CONSTRAINTS:\n# 1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\n# 2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\n# 3. No user assistance\n# 4. Exclusively use the commands listed in double quotes e.g. \"command name\"\n\n# RESOURCES:\n# 1. Internet access for searches and information gathering.\n# 2. Long Term memory management.\n# 3. GPT-3.5 powered Agents for delegation of simple tasks.\n# 4. File output.\n\n\nSYSTEM_PROMPT_old = \"\"\"\nA question is divided into many steps, and you will complete one of them. Please provide a clear and step-by-step solution for a scientific problem in the categories of Chemistry, Physics, or Mathematics. The problem will specify the unit of measurement, which should be included in the answer.\n\n--- Plan Overview ---\nThe query has already been splited into a tree based plan as follows: \n{{all_plan}}\nYou have already performed some of the subtasks.\n\"\"\"\n\n\nSYSTEM_PROMPT = \"\"\"\nA question is divided into many steps, and you will complete one of them. Please provide a clear and step-by-step solution for a scientific problem in the categories of Chemistry, Physics, or Mathematics. The problem will specify the unit of measurement, which should be included in the answer.\n\nFor each instance, you need to three things. Firstly, for \"formulae retrieval\", you need to identify the formulae explicitly and implicitly entailed in the problem context. Then there is a \"reasoning/calculation process\" where you are required to reason step by step based on the identified formulae and problem context. Finally, conclude the answer. For each problem, the output format should incorporate the following components in the corresponding format:\n\n**Formulae retrieval: **\n[Formula 1] (the formula required to solve the problem)\n[Formula 2] (the second formula required to solve the problem, if any)\n...\n[Formula n] (the n-th formula required to solve the problem, if any)\n\n**Reasoning/calculation process:**\n[step 1] (the first step for solving this problem)\n.....\n[step n] (the n-th step for solving the problem, if any)\n\n**Answer conclusion:**\n[answer]: ```{PYTHON CODE}``` \n\n\nYou have been solving a complex task by following a given plan listed below.\n--- Plan Overview ---\nThe complex task has already been splited into a tree based plan as follows: \n{{all_plan}}\nYou have already performed some of the subtasks.\n\"\"\"\n\n\nUSER_PROMPT = \"\"\"\nNow, you continue to complete subtasks.Please combine the results of previous tasks to complete the current goal of processing subtasks. Please complete all milestones and give a concise answer that can be efficiently used by subsequent subtasks.\n--- Status ---\nCurrent Subtask: {{subtask_id}}\nThe query: {{subtask_goal}}\nMilestones: {{milestones}}\n{{additional_prompt}}\n\nPlease respond strictly to the format provided. For each instance, you need to three things. \nFirstly, for \"formulae retrieval\", you need to identify the formulae explicitly and implicitly entailed in the problem context. \nThen there is a \"reasoning/calculation process\" where you are required to reason step by step based on the identified formulae and problem context, you MUST use \nFinally, conclude the answer by writing a piece of corresponding python code, you MUST use the International System of Units in this stage. \nFor each problem, the output format should incorporate the following components in the corresponding format:\n**Formulae retrieval: **\n[Formula 1] (the formula required to solve the problem)\n[Formula 2] (the second formula required to solve the problem, if any)\n...\n[Formula n] (the n-th formula required to solve the problem, if any)\n\n**Reasoning/calculation process:**\n[step 1] (the first step for solving this problem)\n.....\n[step n] (the n-th step for solving the problem, if any)\n\n**Answer conclusion:**\n{{tool_prompt}}\n\n--- Similar tasks ---\nThe following are the trajectories of tasks that have been dealt with in the past that are similar to this goal, including the successful tasks and their action lists. You can learn form them and use relevant knowledge in their procedure.\n{{success_prompt}}\n\n\"\"\"\n\n\nUSER_SINGLE_PROMPT = \"\"\"\nNow, solve this problem:\n{{query}}\n\nPlease respond strictly to the format provided. For each instance, you need to three things. \nFirstly, for \"formulae retrieval\", you need to identify the formulae explicitly and implicitly entailed in the problem context. \nThen there is a \"reasoning/calculation process\" where you are required to reason step by step based on the identified formulae and problem context, you MUST use \nFinally, conclude the answer by writing a piece of corresponding python code, you MUST use the International System of Units in this stage. \nFor each problem, the output format should incorporate the following components in the corresponding format:\n**Formulae retrieval: **\n[Formula 1] (the formula required to solve the problem)\n[Formula 2] (the second formula required to solve the problem, if any)\n...\n[Formula n] (the n-th formula required to solve the problem, if any)\n\n**Reasoning/calculation process:**\n[step 1] (the first step for solving this problem)\n.....\n[step n] (the n-th step for solving the problem, if any)\n\n**Answer conclusion:**\n[answer]: ```{PYTHON CODE}```\n\n\nYour answer should be a piece of python code which solves the current question.\nEncase your code within triple backticks for clarity.\nYou must end your code with printing all the result and their units.\nMake sure the code can be successfully run without any input.\nBe precise.The answer should be accurate, choose the appropriate units, and PROHIBIT the use of round functions to round off and lose the results.\nyou MUST use the International System of Units in this stage, for example, convert all the volume to {m^3}, and convert pressure to {N/(m^2)}, which is {Pa}.\nMake sure the code can be successfully run without any input. And import all the modules that you need to use.\n\nfor example, you could response Answer conclusion part like this:\n**Answer conclusion:**\n[answer]: ```python\nimport numpy as np\n\n# Value of 2 pi c omega_obs\nomega_obs = 1.8133708490380042e+23  # Hz\n\n# Value of D for H35Cl\nD = 440.2  # kJ/mol\n\n# Calculate beta\nbeta = omega_obs * (2 * D)**0.5\n\n# Convert the answer to SI unit\nbeta = beta * 100 # Convert {cm^(-1)} to {m^(-1)}\n\n# Print the exact result and avoid using round function or format like \".4f\"\nprint(\"The value of beta is:\", beta, \"m^(-1)\")\n```\n\n--- Similar tasks ---\nThe following are the trajectories of tasks that have been dealt with in the past that are similar to this goal, including the successful tasks and their action lists. You can learn form them and use relevant knowledge in their procedure.\n{{success_prompt}}\n\"\"\"\n\n\ndef get_examples_for_dispatcher():\n    \"\"\"The example that will be given to the dispatcher to generate the prompt\n\n    Returns:\n        example_input: the user query or the task\n        example_system_prompt: the system prompt\n        example_user_prompt: the user prompt\n    \"\"\"\n    # example_input = \"\"\"{\\n  \"name\": \"Finding Feasible Examples\",\\n  \"goal\": \"Find 10 examples that can reach the target number 24 in the 24-points game.\",\\n  \"handler\": \"subtask 1\",\\n  \"tool_budget\": 50,\\n  \"prior_plan_criticsim\": \"It may be difficult to come up with examples that are all feasible.\",\\n  \"milestones\": [\\n    \"Identifying appropriate combination of numbers\",\\n    \"Applying mathematical operations\",\\n    \"Verifying the result equals to target number\",\\n    \"Recording feasible examples\"\\n  ],\\n  \"expected_tools\": [\\n    {\\n      \"tool_name\": \"analyze_code\",\\n      \"reason\": \"To ensure all feasible examples meet the rules of the 24-points game\"\\n    }\\n  ],\\n  \"exceute_status\": \"TODO\"\\n}\"\"\"\n    # TODO\n    example_input = \"\"\n    example_system_prompt = SYSTEM_PROMPT\n    example_user_prompt = USER_PROMPT\n    return example_input, example_system_prompt, example_user_prompt\n\n\ndef get_examples_for_dispatcher_d():\n    \"\"\"The example that will be given to the dispatcher to generate the prompt\n\n    Returns:\n        example_input: the user query or the task\n        example_system_prompt: the system prompt\n        example_user_prompt: the user prompt\n    \"\"\"\n    # example_input = \"\"\"{\\n  \"name\": \"Finding Feasible Examples\",\\n  \"goal\": \"Find 10 examples that can reach the target number 24 in the 24-points game.\",\\n  \"handler\": \"subtask 1\",\\n  \"tool_budget\": 50,\\n  \"prior_plan_criticsim\": \"It may be difficult to come up with examples that are all feasible.\",\\n  \"milestones\": [\\n    \"Identifying appropriate combination of numbers\",\\n    \"Applying mathematical operations\",\\n    \"Verifying the result equals to target number\",\\n    \"Recording feasible examples\"\\n  ],\\n  \"expected_tools\": [\\n    {\\n      \"tool_name\": \"analyze_code\",\\n      \"reason\": \"To ensure all feasible examples meet the rules of the 24-points game\"\\n    }\\n  ],\\n  \"exceute_status\": \"TODO\"\\n}\"\"\"\n    # TODO\n    example_input = \"\"\n    example_system_prompt = SYSTEM_PROMPT\n    example_user_prompt = USER_SINGLE_PROMPT\n    return example_input, example_system_prompt, example_user_prompt\n"}
{"type": "source_file", "path": "XAgent/inner_loop_search_algorithms/base_search.py", "content": "from colorama import Fore\n\nfrom XAgent.logs import logger\nfrom XAgent.utils import SearchMethodStatusCode\n\nclass BaseSearchMethod:\n    \"\"\"The base class for all search methods. It defines the common elements and actions that all search \n    methods have.\n    \n    Attributes:\n        status (SearchMethodStatusCode): The status of the search method. It can be 'DOING', 'SUCCESS' or 'FAILED'.\n        need_for_plan_refine (bool): A flag that indicates if the plan needs to be refined. It starts as False.\n    \"\"\"\n    def __init__(self):\n        \"\"\"Initializes the search method instance and logs its creation.\"\"\"\n        logger.typewriter_log(\n            f\"Constructing a searching method:\",\n            Fore.YELLOW,\n            self.__class__.__name__,\n        )\n        self.status: SearchMethodStatusCode = SearchMethodStatusCode.DOING\n        self.need_for_plan_refine: bool = False\n\n    def run(self):\n        \"\"\"A Placeholder function for running the search method. \n           This should be implemented by all search method subclasses.\n        \"\"\"\n        pass\n\n    def to_json(self):\n        \"\"\"A Placeholder function for creating a json representation of the search method. \n           This should be implemented by all search method subclasses.\n        \"\"\"\n        pass\n    \n    def get_finish_node(self):\n        \"\"\"A Placeholder function for getting the final node of the search method run.\n           This should be implemented by all search method subclasses.\n        \"\"\"\n        pass\n\n    def status(self):\n        \"\"\"Gets the current status of the search method.\n\n        Returns:\n            SearchMethodStatusCode: The current status of the search method.\n        \"\"\"\n        return self.status\n"}
{"type": "source_file", "path": "XAgent/agent/utils.py", "content": "import json\nfrom typing import Dict\n\ndef get_command(response_json: Dict):\n    \"\"\"\n    Parses the response and returns the command name and arguments.\n\n    This function will raise the exception `json.decoder.JSONDecodeError` if the response is not valid JSON.\n    Any other error that occurs is also caught and the function returns an \"Error:\" message with the exception message.\n\n    Args:\n        response_json (Dict): The response from the AI in dictionary format.\n\n    Returns:\n        tuple: The command name and arguments, or some error indication.\n               If the response json dictionary does not contain the 'command' key, or the value of\n               'command' is not a dictionary, or the 'command' dictionary does not contain the 'name' key,\n               returns a tuple where the first element is 'Error:' and the second element is a string explaining the problem.\n               If some error occurs, returns a tuple where the first element is 'Error:' and the second element is the str of the exception.\n\n    Raises:\n        json.decoder.JSONDecodeError: If the response is not valid JSON.\n        Exception: If any other error occurs.\n    \"\"\"\n    try:\n        if \"command\" not in response_json:\n            return \"Error:\", \"Missing 'command' object in JSON\"\n\n        if not isinstance(response_json, dict):\n            return \"Error:\", f\"'response_json' object is not dictionary {response_json}\"\n\n        command = response_json[\"command\"]\n        if not isinstance(command, dict):\n            return \"Error:\", \"'command' object is not a dictionary\"\n\n        if \"name\" not in command:\n            return \"Error:\", \"Missing 'name' field in 'command' object\"\n\n        command_name = command[\"name\"]\n\n        # Use an empty dictionary if 'args' field is not present in 'command' object\n        arguments = command.get(\"args\", {})\n\n        return command_name, arguments\n    except json.decoder.JSONDecodeError:\n        return \"Error:\", \"Invalid JSON\"\n    # All other errors, return \"Error: + error message\"\n    except Exception as e:\n        return \"Error:\", str(e)"}
{"type": "source_file", "path": "XAgent/agent/simple_agent/post_process.py", "content": "import json\nimport re\nimport sys\nfrom io import StringIO\nimport multiprocessing\n\n\ndef remove_not(x):\n    match_number = re.compile(\"[\\$]?\\ *10\\^[{]?\\ *-?[0-9]+\\ *[}]?\\ *[\\$]?\")\n    result = re.findall(match_number, x)\n    if len(result) != 0:\n        return re.split(match_number, x)[-1]\n    return None\n\n\ndef parse_not(inputs):\n    if not inputs:\n        return \"\", \"\"\n    if \"\\times\" in inputs:\n        x, ab = inputs.split(\"\\times\")\n    elif \"\\\\times\" in inputs:\n        x, ab = inputs.split(\"\\\\times\")\n    elif \"*\" in inputs:\n        x, ab = inputs.split(\"*\")\n    else:\n        return inputs\n    return x, ab\n\n\ndef cal_not(inputs):\n\n    try:\n        x, ab = list(inputs)\n        match_number = re.compile(\"10\\^[{]?\\ *-?[0-9]+\\ *[}]?\")\n        ab = re.findall(match_number, ab)[0]\n        ab = ab[ab.find(\"^\") + 1 :]\n        if \"{\" in ab:\n            ab = ab[ab.find(\"{\") + 1 :]\n        if \"}\" in ab:\n            ab = ab[: ab.find(\"}\")]\n        x = x.strip()\n        out = float(x) * 10 ** float(ab)\n        # print(float(x)*10**float(ab))\n        return str(out)\n    except:\n        print(\"error\")\n    return inputs\n\n\ndef remove_boxed(s):\n    left = \"oxed{\"  # change\n    try:\n        assert s[: len(left)] == left\n        assert s[-1] == \"}\"\n        answer = s[len(left) : -1]\n        if \"=\" in answer:\n            answer = answer.split(\"=\")[-1].lstrip(\" \")\n        return answer\n    except:\n        return None\n\n\ndef last_boxed_only_string(string):\n    if string == None:\n        return None\n    idx = string.rfind(\"oxed\")  # change\n    if idx < 0:\n        idx = string.rfind(\"\\\\fbox\")\n        if idx < 0:\n            return None\n    i = idx\n    right_brace_idx = None\n    num_left_braces_open = 0\n    while i < len(string):\n        if string[i] == \"{\":\n            num_left_braces_open += 1\n        if string[i] == \"}\":\n            num_left_braces_open -= 1\n            if num_left_braces_open == 0:\n                right_brace_idx = i\n                break\n        i += 1\n\n    if right_brace_idx == None:\n        retval = None\n    else:\n        retval = string[idx : right_brace_idx + 1]\n\n    return retval\n\n\ndef parse_math_answer(raw_string):\n    if raw_string == None:\n        return None\n    ans = remove_boxed(last_boxed_only_string(raw_string))\n    if ans != None:\n        return ans\n    else:\n        if \"**Answer conclusion:**\" in raw_string:\n            return raw_string.split(\"**Answer conclusion:**\")[1]\n        else:\n            return None\n\n\ndef extract_code(raw_string):\n    code = None\n    if raw_string == None:\n        return None\n    idx = raw_string.find(\"```\")\n    raw_string = raw_string[idx + 3 :]\n    idx = raw_string.find(\"```\")\n    code = raw_string[:idx]\n    if code == \"\":\n        return None\n    if code[:6] == \"python\":\n        code = code[6:]\n    if not (\"print\" in code):\n        code = None\n    return code\n\ndef exec_code_in_process(queue, code):\n    try:\n        redirected_output = StringIO()\n        sys.stdout = redirected_output\n        indented_code = \"\\n\".join(\"    \" + line for line in code.strip().split('\\n'))\n        wrapped_code = f\"def run_code():\\n{indented_code}\\nrun_code()\"\n        exec(wrapped_code)\n        sys.stdout = sys.__stdout__  # Restore original stdout\n        queue.put(redirected_output.getvalue().strip())\n    except Exception as e:\n        sys.stdout = sys.__stdout__  # Restore original stdout\n        queue.put(\"None\")\n\ndef exec_code(code, timeout=5):\n    # Create a queue to receive the output from the subprocess\n    queue = multiprocessing.Queue()\n\n    # Create and start a process that runs the code\n    process = multiprocessing.Process(target=exec_code_in_process, args=(queue, code))\n    process.start()\n\n    # Wait for the process to complete or timeout\n    process.join(timeout)\n\n    if process.is_alive():\n        # Terminate the process if it is still alive after the timeout\n        process.terminate()\n        process.join()\n        print(\"Timeout: Code execution exceeded 5 seconds.\")\n        return \"None\"\n\n    # Get the output from the queue\n    result = queue.get_nowait()\n    return result"}
{"type": "source_file", "path": "XAgent/ai_functions/request/utils.py", "content": ""}
{"type": "source_file", "path": "XAgent/agent/summarize.py", "content": "from colorama import Fore\nfrom XAgent.utils import ToolCallStatusCode, get_token_nums, clip_text\nfrom XAgent.ai_functions import function_manager\nfrom XAgent.config import CONFIG\nfrom XAgent.logs import logger\n\nSINGLE_ACTION_MAX_LENGTH = CONFIG.summary[\"single_action_max_length\"]\nMAX_RETURN_LENGTH = CONFIG.summary[\"max_return_length\"]\nMAX_PLAN_LENGTH = CONFIG.max_plan_length\n\n\ndef summarize_action(\n    action_process: list[dict],\n    task: str,\n) -> (list[str], str):\n    \"\"\"\n    Generate a summarized series of actions.\n\n    Args:\n        action_process (list[dict]): The list of actions to process.\n        task (str): The task name.\n\n    Returns:\n        str: The string contains a summary of the actions.\n    \"\"\"\n    if len(action_process) < 1:\n        return \"No steps found\"\n\n    def generate_func_args(args: dict, black_list=[]) -> str:\n        \"\"\"\n        Generate function arguments in the form of strings.\n\n        Args:\n            args (dict): A dictionary of arguments.\n            black_list (list): A list of forbidden or restricted words or keys in the args dictionary.\n\n        Returns:\n            str: A string that summarizes the function arguments.\n        \"\"\"\n        ret = \"\"\n        args_len = 0\n        for k, v in args.items():\n            if k in black_list:\n                v = \"`wrapped`\"\n            v_str, v_len = clip_text(\n                str(v), SINGLE_ACTION_MAX_LENGTH - args_len, clip_end=True\n            )\n            if v_len < SINGLE_ACTION_MAX_LENGTH - args_len:\n                ret += f'{k}=\"{v_str}\",' if isinstance(v, str) else f\"{k}={v_str},\"\n                args_len += v_len\n            else:\n                ret += (\n                    f'{k}=\"{v_str}...\",' if isinstance(v, str) else f\"{k}={v_str}...,\"\n                )\n                args_len += SINGLE_ACTION_MAX_LENGTH - args_len\n\n        return ret[:-1]  # remove last comma\n\n    # wrap old content\n    raw_actions = {}\n    accessed_files = []\n    last_successful_action_index = None\n    last_failed_action_index = None\n    for index, action in zip(\n        range(len(action_process) - 1, -1, -1), action_process[::-1]\n    ):\n        if (\n            last_successful_action_index is None\n            and action[\"tool_status_code\"] == ToolCallStatusCode.TOOL_CALL_SUCCESS\n        ):\n            last_successful_action_index = index\n        if (\n            last_failed_action_index is None\n            and action[\"tool_status_code\"] == ToolCallStatusCode.TOOL_CALL_FAILED\n        ):\n            last_failed_action_index = index\n\n        command = action[\"command\"][\"properties\"]\n        if command[\"name\"] == \"\" or not isinstance(command[\"args\"], dict):\n            continue\n\n        raw_action = [\"`placeholder`\", \"`placeholder`\"]\n\n        if (\n            \"FileSystem\" in command[\"name\"]\n            and \"filepath\" in command[\"args\"]\n            and action[\"tool_status_code\"] == ToolCallStatusCode.TOOL_CALL_SUCCESS\n        ):\n            raw_action[0] = (\n                command[\"name\"]\n                + f\"({generate_func_args(command['args'],black_list=['content','new_content'])})\"\n            )\n            if command[\"args\"][\"filepath\"] in accessed_files:\n                raw_action[1] = (\n                    \"`Old Content has been wrapped, check latest filesystem calling`\"\n                )\n            else:\n                raw_action[1] = str(action[\"tool_output\"])\n                accessed_files.append(command[\"args\"][\"filepath\"])\n        else:\n            raw_action[0] = command[\"name\"] + f\"({generate_func_args(command['args'])})\"\n            raw_action[1] = str(action[\"tool_output\"])\n\n        raw_actions[index] = raw_action\n    valid_index = list(raw_actions.keys())\n    valid_index.sort()\n\n    ret = {}\n    for index in valid_index:\n        action = action_process[index]\n        if \"summary\" not in action:\n            raw_actions_des = \"\\n\".join(\n                [f\"[{k}] {v}\" for k, v in action[\"thoughts\"][\"properties\"].items()]\n                + [\n                    f\"[tool_status_code] {action['tool_status_code']}\",\n                    f\"[tool calling] {raw_actions[index][0]}\",\n                    f\"[return] \",\n                ]\n            )\n            raw_actions_des += clip_text(\n                raw_actions[index][1],\n                MAX_RETURN_LENGTH - get_token_nums(raw_actions_des),\n            )[0]\n\n            summary, tokens = function_manager(\n                \"summarize_action\",\n                action=raw_actions_des,\n                current_task=task,\n                return_generation_usage=True,\n            )\n            action[\"summary\"] = summary\n            logger.typewriter_log(\n                f\"Action summarized in {tokens['completion_tokens']} tokens\",\n                Fore.YELLOW,\n            )\n        else:\n            summary = action[\"summary\"]\n\n        act_str = \"\\n\".join(\n            [\n                f\"[{index}] {raw_actions[index][0]}\",\n                f\"[{index}][summary] {summary['summary']}\",\n                f\"[{index}][description] {summary['description']}\",\n                f\"[{index}][status code] {action['tool_status_code']}\",\n            ]\n        )\n        if (\n            \"failed_reason_and_reflection\" in summary\n            and summary[\"failed_reason_and_reflection\"] != \"\"\n        ):\n            act_str += (\n                f'\\n[{index}][failed reason] {summary[\"failed_reason_and_reflection\"]}'\n            )\n\n        # directly adding short returns\n        if (\n            len(raw_actions[index][1]) < 1000\n            and get_token_nums(raw_actions[index][1]) < 150\n        ):\n            act_str += f\"\\n[{index}][return] {raw_actions[index][1]}\"\n\n        ret[index] = act_str\n\n    reflection = function_manager(\n        \"actions_reflection\",\n        actions=clip_text(\"\\n\".join([ret[i] for i in valid_index]), MAX_RETURN_LENGTH)[\n            0\n        ],\n        current_task=task,\n    )\n\n    ret_lenght = {k: get_token_nums(v) for k, v in ret.items()}\n    total_length = sum(ret_lenght.values())\n\n    # adding more return to last successful action\n    for i in [last_successful_action_index, last_failed_action_index]:\n        if i is not None and \"[return]\" not in ret[i]:\n            s = f\"\\n[{i}][return] {clip_text(raw_actions[i][1],(MAX_RETURN_LENGTH-total_length)//2)[0]}\"\n            return_length = get_token_nums(s)\n            ret_lenght[i] += return_length\n            total_length += return_length\n            ret[i] += s\n\n    key_actions: list = reflection[\"key_actions\"]\n    key_actions.sort(reverse=True)\n    for i in key_actions:\n        if total_length >= MAX_RETURN_LENGTH:\n            break\n        if (\n            i in ret\n            and action_process[i][\"tool_status_code\"]\n            == ToolCallStatusCode.TOOL_CALL_SUCCESS\n            and \"[return]\" not in ret[i]\n        ):\n            s = f\"\\n[{i}][return] {clip_text(raw_actions[i][1],SINGLE_ACTION_MAX_LENGTH-ret_lenght[i])[0]}\"\n            if (tokens := get_token_nums(s)) > MAX_RETURN_LENGTH - total_length:\n                continue\n            total_length += tokens\n            ret[i] += s\n\n    while len(valid_index) > 0:\n        i = valid_index.pop()\n        if total_length >= MAX_RETURN_LENGTH:\n            break\n        if (\n            action_process[i][\"tool_status_code\"]\n            == ToolCallStatusCode.TOOL_CALL_SUCCESS\n            and \"[return]\" not in ret[i]\n        ):\n            s = f\"\\n[{i}][return] {clip_text(raw_actions[i][1],SINGLE_ACTION_MAX_LENGTH-ret_lenght[i])[0]}\"\n            if (tokens := get_token_nums(s)) > MAX_RETURN_LENGTH - total_length:\n                continue\n            total_length += tokens\n            ret[i] += s\n\n    valid_index = list(ret.keys())\n    valid_index.sort()\n    ordered_rets = [ret[i] for i in valid_index] + [\n        f\"[suggestion] {sugg}\" for sugg in reflection[\"suggestions\"]\n    ]\n\n    return \"\\n\".join(ordered_rets)\n\n\ndef summarize_plan(plans: dict, thoughts=False) -> str:\n    \"\"\"\n    Generate a summarized plan based on provided plans.\n\n    Args:\n        plans (dict): The plans to provide.\n\n    Returns:\n        str: The string contains a summary of the plan.\n    \"\"\"\n    summary: list[list] = []\n    task_ids = []\n    detailed_info: dict[str, list] = {}\n    current_task_id = None\n\n    def recursive_summary(\n        plan: dict,\n    ):\n        \"\"\"\n        Generate a summarized plan in a recursive process.\n\n        Args:\n            plan (dict): A dictionary of plans.\n\n        Returns:\n            None\n        \"\"\"\n        nonlocal summary\n        nonlocal current_task_id\n        plan_des = [\n            f'[Task ID] {plan[\"task_id\"]}',\n            f'[Name] {plan[\"name\"]}',\n            f'[Goal] {plan[\"goal\"]}',\n            f'[Status] {plan[\"exceute_status\"]}',\n        ]\n        if current_task_id is None and plan[\"exceute_status\"] == \"DOING\":\n            current_task_id = plan[\"task_id\"]\n\n        if \"milestones\" in plan and len(plan[\"milestones\"]) > 0:\n            plan_des.extend(\n                [\"[Milestones]\"]\n                + [\"- \" + milestone for milestone in plan[\"milestones\"]]\n            )\n\n        if \"action_list_summary\" not in plan and \"prior_plan_criticism\" in plan:\n            plan_des.append(f'[Prior Plan Criticism] {plan[\"prior_plan_criticism\"]}')\n\n        if \"answer\" in plan:\n            plan_des.append(f'[Answer] {plan[\"answer\"]}')\n        if thoughts:\n            plan_des.append(f'[Thought] {plan[\"thoughts\"]}')\n\n        if \"submit_result\" in plan and \"args\" in plan[\"submit_result\"]:\n            submission = plan[\"submit_result\"][\"args\"]\n            plan_des.append(\n                f'[Action Status] {\"Success\" if submission[\"result\"][\"success\"] else \"Fail\"}'\n            )\n            plan_des.append(f'[task Conclusion]{submission[\"result\"][\"conclusion\"]}')\n\n            # possible too long part\n            action_des = [\n                \"[Action Info]\",\n                f\"- [Conclusion] {submission['result']['conclusion']}\",\n            ]\n            if \"action_list_summary\" in plan:\n                action_des.append(f'- [Summary] {plan[\"action_list_summary\"]}')\n            if submission[\"suggestions_for_latter_subtasks_plan\"][\n                \"need_for_plan_refine\"\n            ]:\n                if submission[\"suggestions_for_latter_subtasks_plan\"][\"reason\"] != \"\":\n                    action_des.append(\n                        f\"- [Proposal] {submission['suggestions_for_latter_subtasks_plan']['reason']}\"\n                    )\n            detailed_info[plan[\"task_id\"]] = action_des\n\n        task_ids.append(plan[\"task_id\"])\n        summary.append(plan_des)\n        if \"subtask\" in plan:\n            for subtask in plan[\"subtask\"]:\n                recursive_summary(subtask)\n\n    recursive_summary(plans)\n    total_tokens = sum([get_token_nums(\"\\n\".join(plan)) for plan in summary])\n    if current_task_id is None:\n        current_task_id = task_ids[-1]\n    for task_id, plan in zip(task_ids[::-1], summary[::-1]):\n        if task_id <= current_task_id and task_id in detailed_info:\n            if (\n                tokens := get_token_nums(\"\\n\".join(detailed_info[task_id]))\n            ) > MAX_PLAN_LENGTH - total_tokens:\n                continue\n            else:\n                total_tokens += tokens\n                plan.extend(detailed_info[task_id])\n    # logger.typewriter_log(f'Plan summarized {total_tokens}',Fore.YELLOW)\n    ret = []\n    for plan in summary:\n        ret.append(\"\\n\".join(plan))\n    return \"\\n\".join(ret)\n"}
{"type": "source_file", "path": "XAgent/data_structure/node.py", "content": "import os\nfrom copy import deepcopy\nimport abc\nfrom typing import List\n\nfrom XAgent.message_history import MessageHistory\nfrom XAgent.utils import ToolCallStatusCode, TaskStatusCode\n\n\nclass Node(metaclass = abc.ABCMeta):\n    \"\"\"\n    Abstract class representing a generic node in the XAgent's data structure.\n\n    This class uses the abc module to denote it as an abstract base class.\n    Other classes should inherit from this class to implement specific types of nodes.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initialize a new node.\n\n        As an abstract class, Node does not have any implementation details.\n        \"\"\"\n        pass\n\n\nclass ToolNode(Node):\n    \"\"\"\n    Class representing a tool node in the XAgent's data structure.\n    \n    A tool node has a “father” that represents its parent node, \"children\" that represents its child nodes, \n    and “data” containing metadata about node's status, command, tool's output, and thoughts properties.\n    It also carries a message history and a workspace hash id.\n\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize a new tool node.\n\n        Setup father, children, expand_num, data, history, workspace_hash_id attributes for the instance.\n        \"\"\"\n\n        self.father: ToolNode = None\n        self.children: list[ToolNode] = []\n        self.expand_num = 0\n        self.data = {\n            \"content\": \"\",\n            \"thoughts\": {\n                \"properties\": {\n                    \"thought\": \"\",\n                    \"reasoning\": \"\",\n                    \"plan\": \"\",\n                    \"criticism\": \"\",\n                },\n            },\n            \"command\": {\n                \"properties\": {\n                    \"name\": \"\",\n                    \"args\": \"\",\n                },\n            },\n            \"tool_output\": \"\",\n            \"tool_status_code\": ToolCallStatusCode.TOOL_CALL_SUCCESS,\n        }\n        self.history: MessageHistory = MessageHistory()\n        self.workspace_hash_id = \"\"\n\n    @property\n    def process(self):\n        \"\"\"\n        Generate a list of data from current node up to root node.\n\n        Returns:\n            data (List): A list of data from current node up to root node.\n        \"\"\"\n\n        data = []\n        now_node = self\n        while now_node.father != None:\n            data = [now_node.data] + data\n            now_node = now_node.father\n        return data\n\n    def to_json(self):\n        \"\"\"\n        Convert the data attribute of the instance to a JSON-compatible format.\n\n        Returns:\n            data (Dict): The data attribute of the instance in a JSON-compatible format.\n        \"\"\"\n\n        data = deepcopy(self.data)\n        data[\"tool_status_code\"] = data[\"tool_status_code\"].name\n        return data\n\n    def get_depth(self):\n        \"\"\"\n        Calculate the depth of current node in the tree.\n\n        Returns:\n            depth (int): The depth of the node. Return 0 if the node is a root node.\n        \"\"\"\n        \n        if self.father == None:\n            return 0\n        return self.father.get_depth() + 1\n    \n    def get_subtree_size(self):\n        \"\"\"\n        Calculate the size of the subtree rooted at current node.\n\n        Returns:\n            size (int): The size of the subtree rooted at current node.\n        \"\"\"\n        \n        if self.children == []:\n            return 1\n        now_size = 1\n        for child in self.children:\n            now_size += child.get_subtree_size()\n        return now_size"}
{"type": "source_file", "path": "XAgent/data_structure/__init__.py", "content": ""}
{"type": "source_file", "path": "XAgent/imagination_engine.py", "content": "try:\n    from XAgent.config import CONFIG\nexcept ImportError:\n    from config import CONFIG\nfrom openai import AzureOpenAI\nimport re\n\nIMG_PROMPT = \"\"\"\nPlease create {{top_k}} advanced chemistry questions suitable for in-depth understanding and application of chemical formulas and principles. Each question should focus on the deeper aspects of the [topic] provided in order to understand the principles of chemistry and the reasoning process.\n[topic]: {{topic}}\nGuidelines for Problem Creation: \n•\tUse of Samples: You are provided with sample questions for reference. Feel free to use these to guide the style and depth of the problems. \n•\tBeyond the Examples: You are encouraged to use your background and expertise to create problems that go beyond the provided examples, ensuring the problems are as diverse and comprehensive as possible. \n\nRequirements for Each Problem: \na.\tProblem Statement: Clearly define the challenge or task. \nb.\tSolution: Provide a detailed solution that includes: \nI.\tFormulas and Knowledge Needed: List the equations and concepts required to understand and solve the problem. \nII.\tReasoning Steps: Outline the logical or mathematical steps to solve the problem. \nIII. Python code: Executable python code is generated to solve problems. At the end of each problem, please include Python code that can be used to confirm and verify the correctness of the provided solution. The Python solutions should illustrate the entire solution process, from the initial step to the final answer, rather than merely validating the result. Develop these solutions such that each step of the mathematical process is explicitly demonstrated and calculated in Python. Additionally, ensure that you run your Python code to confirm it is free from any errors. \nd.\tDiversity: Ensure a wide range of problems, each focusing on different elements from the subtopic list. \ne.\tPresentation: Please output your problem statement, solution, detailed explanation, and a self-contained Python code for verification below in specified format. \n\nFor each generated question the output is required to be in the following format:\n[Task Start]\n\n[Problem Statement]:{your problem}\n\n**Formulae retrieval: **\n[Formula 1] (the formula required to solve the problem)\n[Formula 2] (the second formula required to solve the problem, if any)\n...\n[Formula n] (the n-th formula required to solve the problem, if any)\n\n**Reasoning/calculation process:**\n[step 1] (the first step for solving this problem)\n.....\n[step n] (the n-th step for solving the problem, if any)\n\n**Answer conclusion:**\n[answer]: ```{PYTHON CODE}``` \n\n[Task End]\nYou have to generate {{topk}} tasks about {{topic}}.\n\nSample demonstration example: \n{{example_shots}}\n\n\n\"\"\"\n\n\n# imagination engine \ndef call_engine(query):\n    \"\"\"\n    ask chatgpt\n    \"\"\"\n    model = CONFIG.default_completion_kwargs[\"model\"]\n    model_name = CONFIG.api_keys[model][0][\"engine\"]\n    message = [{\"content\": query, \"role\": \"user\"}]\n    response = None\n    temp = int(CONFIG.default_completion_kwargs[\"temperature\"])\n\n    try:\n        client = AzureOpenAI(\n            api_key=CONFIG.api_keys[model][0][\"api_key\"],\n            api_version=CONFIG.api_keys[model][0][\"api_version\"],\n            azure_endpoint=CONFIG.api_keys[model][0][\"api_base\"],\n        )\n        response = client.chat.completions.create(\n            model=model_name,\n            messages=message,\n            temperature=temp,\n        )\n\n        response = response.choices[0].message.content\n        return response\n\n    except Exception as e:\n        print(e)\n        print(\"chatcompletion: Score Fail\")\n        return None\n\n\ndef imagination(topic, res, topk):\n    example = \"\"\n    print(\"topic is \" + topic + \"\\n\")\n    for traj in res:\n        temp = \"[Task Start]\\n\\n\"\n        temp += \"[Problem Statement]:\" + traj[\"node\"][\"metadata\"][\"goal\"] + \"\\n\\n\"\n        temp += traj[\"node\"][\"metadata\"][\"action\"]\n        temp += \"\\n[Task End]\\n\\n\"\n        example += temp\n    msg = fill_in_placeholders(\n        IMG_PROMPT, {\"example_shots\": example, \"topic\": topic, \"topk\": topk}\n    )\n    img_prompt = \"\"\n    for i in range(3):\n        img_prompt = \"\"\n        text = call_engine(msg)\n        matches = re.findall(r\"\\[Task Start\\].*?\\[Task End\\]\", text, re.DOTALL)\n        if matches:\n            for match in matches:\n                img_prompt += match + \"\\n\"\n        if img_prompt != \"\" and len(matches) == topk:\n            return img_prompt\n    return img_prompt\n\n\ndef fill_in_placeholders(prompt_messsges: str, placeholders: dict):\n    \"\"\"\n    Fills in placeholders defined in the input with the corresponding values.\n\n    Args:\n        placeholders (dict): A dictionary containing keys as placeholders and values as their replacements.\n\n    Returns:\n        filled_messages: A copy of the initial prompt_messages with placeholders replaced with their corresponding values.\n    \"\"\"\n    filled_messages = prompt_messsges\n\n    for key, value in placeholders.items():\n        filled_messages = filled_messages.replace(\"{{\" + str(key) + \"}}\", str(value))\n    return filled_messages\n"}
{"type": "source_file", "path": "XAgent/ai_functions/request/obj_generator.py", "content": "import orjson\nimport json5\nimport jsonschema\nimport jsonschema.exceptions\nimport importlib\nimport traceback\n\nfrom copy import deepcopy\nfrom colorama import Fore\n\nfrom tenacity import retry, stop_after_attempt, retry_if_exception_type\n\nfrom .error import FunctionCallSchemaError\n\nfrom XAgent.logs import logger\nfrom XAgent.config import CONFIG\nfrom XAgent.running_recorder import recorder\n\n\nclass OBJGenerator:\n    \"\"\"Handles interactions with AI responses and execution of configured requests.\n\n    Attributes:\n        chatcompletion_request_funcs: A dictionary to store functions processing chat completion requests.\n    \"\"\"\n\n    def __init__(\n        self,\n    ):\n        self.chatcompletion_request_funcs = {}\n\n    @retry(\n        stop=stop_after_attempt(3),\n        retry=retry_if_exception_type(\n            (jsonschema.exceptions.ValidationError, FunctionCallSchemaError)\n        ),\n    )\n    def chatcompletion(self, *, schema_validation=True, **kwargs):\n        \"\"\"Processes chat completion requests and retrieves responses.\n\n        Args:\n            kwargs: Request data parameters.\n\n        Returns:\n            A dictionary format response retrieved from AI service call.\n\n        Raises:\n            Exception: Error occurred while processing requests.\n            NotImplementedError: Received request type is not currently implemented.\n        \"\"\"\n\n        request_type = kwargs.pop(\"request_type\", CONFIG.default_request_type)\n        for k in list(kwargs.keys()):\n            if kwargs[k] is None:\n                kwargs.pop(k)\n\n        llm_query_id = recorder.get_query_id()\n        try:\n            copyed_kwargs = deepcopy(kwargs)\n            if (\n                response := recorder.query_llm_inout(\n                    llm_query_id=llm_query_id, **copyed_kwargs\n                )\n            ) is None:\n                response = self._get_chatcompletion_request_func(request_type)(**kwargs)\n            recorder.regist_llm_inout(\n                llm_query_id=llm_query_id, **copyed_kwargs, output_data=response\n            )\n            logger.typewriter_log(f\"chatcompletion query_id: {llm_query_id}\", Fore.RED)\n        except Exception as e:\n            traceback.print_exc()\n            logger.typewriter_log(f\"chatcompletion error: {e}\", Fore.RED)\n            recorder.decrease_query_id()\n            raise e\n\n        if schema_validation:\n            # refine the response\n            match request_type:\n                case \"openai\":\n                    response = self.function_call_refine(kwargs, response)\n                case \"xagent\":\n                    pass\n                case _:\n                    raise NotImplementedError(\n                        f\"Request type {request_type} not implemented\"\n                    )\n\n        return response\n\n    def _get_chatcompletion_request_func(self, request_type: str):\n        \"\"\"Retrieves and returns the chat completion function for a particular request type\n\n        Args:\n            request_type (str): Type of the service the request has been generated for.\n\n        Returns:\n            Function object to handle chat completion for the specified request type.\n        \"\"\"\n\n        if request_type not in self.chatcompletion_request_funcs:\n            module = importlib.import_module(\n                f\".{request_type}\", \"XAgent.ai_functions.request\"\n            )\n            self.chatcompletion_request_funcs[request_type] = getattr(\n                module, \"chatcompletion_request\"\n            )\n        return self.chatcompletion_request_funcs[request_type]\n\n    def dynamic_json_fixes(\n        self,\n        broken_json,\n        function_schema,\n        messages: list = [],\n        error_message: str = None,\n    ):\n        \"\"\"Attempts to fix invalid json and validate it against the function schema\n\n        Args:\n            broken_json: The invalid input json data.\n            function_schema: Schema to validate the json data against.\n            messages (list, optional): Additional messages related to the json validation error.\n            error_message (str, optional): Error message related to the json validation error.\n\n        Returns:\n            A dictionary format response retrieved from AI service call.\n        \"\"\"\n\n        logger.typewriter_log(\n            f'Schema Validation for Function call {function_schema[\"name\"]} failed, trying to fix it...',\n            Fore.YELLOW,\n        )\n        repair_req_kwargs = deepcopy(CONFIG.default_completion_kwargs)\n        if (\n            messages[-1][\"role\"] == \"system\"\n            and \"Your last function call result in error\" in messages[-1][\"content\"]\n        ):\n            messages = messages[:-1]\n        repair_req_kwargs[\"messages\"] = [\n            *messages,\n            {\n                \"role\": \"system\",\n                \"content\": \"\\n\".join(\n                    [\n                        \"Your last function call result in error\",\n                        \"--- Error ---\",\n                        error_message,\n                        \"Your task is to fix all errors exist in the Broken Json String to make the json validate for the schema in the given function, and use new string to call the function again.\",\n                        \"--- Notice ---\",\n                        \"- You need to carefully check the json string and fix the errors or adding missing value in it.\",\n                        \"- Do not give your own opinion or imaging new info or delete exisiting info!\",\n                        \"- Make sure the new function call does not contains information about this fix task!\",\n                        \"--- Broken Json String ---\",\n                        broken_json,\n                        \"Start!\",\n                    ]\n                ),\n            },\n        ]\n        repair_req_kwargs[\"functions\"] = [function_schema]\n        repair_req_kwargs[\"function_call\"] = {\"name\": function_schema[\"name\"]}\n        return self.chatcompletion(schema_validation=False, **repair_req_kwargs)\n\n    def load_args_with_schema_validation(\n        self,\n        function_schema: dict,\n        args: str,\n        messages: list = [],\n        *,\n        return_response=False,\n        response=None,\n    ):\n        \"\"\"Validates arguments against the function schema.\n\n        Args:\n            function_schema (dict): Schema to validate the arguments against.\n            args (str): Arguments data to be validated.\n            messages (list, optional): Additional messages related to the arguments validation error.\n            return_response(bool, optional): Whether to return the response along with arguments.\n            response: response data to be returned if return_response is True.\n\n        Returns:\n            Arguments data after schema validation.\n            If return_response is set to True, response is also returned along with the arguments.\n\n        Raises:\n            Exception: Error occurred while validating the arguments.\n        \"\"\"\n\n        # loading arguments\n        arguments = args\n\n        def validate():\n            nonlocal function_schema, arguments\n            if isinstance(arguments, str):\n\n                arguments = {} if arguments == \"\" else json5.loads(arguments)\n\n            jsonschema.validate(\n                instance=arguments, schema=function_schema[\"parameters\"]\n            )\n\n        try:\n            validate()\n        except Exception as e:\n            if not isinstance(arguments, str):\n                arguments = json5.dumps(arguments)\n            # give one opportunity to fix the json string\n            response = self.dynamic_json_fixes(\n                arguments, function_schema, messages, str(e)\n            )\n            arguments = response[\"choices\"][0][\"message\"][\"function_call\"][\"arguments\"]\n            validate()\n\n        if return_response:\n            return arguments, response\n        else:\n            return arguments\n\n    def function_call_refine(self, req_kwargs, response):\n        \"\"\"Validates and refines the function call response.\n\n        Args:\n            req_kwargs: Request data parameters.\n            response: The response received from the service call.\n\n        Returns:\n            Refined and validated response.\n\n        Raises:\n            FunctionCallSchemaError: Error occurred during the schema validation of the function call.\n        \"\"\"\n\n        # verify the schema of the function call if exists\n        if \"functions\" not in req_kwargs:\n            return response\n        function_schema = list(\n            filter(\n                lambda x: x[\"name\"]\n                == response[\"choices\"][0][\"message\"][\"function_call\"][\"name\"],\n                req_kwargs[\"functions\"],\n            )\n        )\n        function_schema = None if len(function_schema) == 0 else function_schema[0]\n\n        if function_schema is None:\n            if (\n                '\"{}\"'.format(\n                    response[\"choices\"][0][\"message\"][\"function_call\"][\"name\"]\n                )\n                in req_kwargs[\"messages\"][0][\"content\"]\n            ):\n                # Temporal fix for tool call without reasoning\n                logger.typewriter_log(\n                    \"Warning: Detect tool call without reasoning\", Fore.YELLOW\n                )\n                response[\"choices\"][0][\"message\"][\"function_call\"][\"arguments\"] = (\n                    orjson.dumps(\n                        {\n                            \"tool_call\": {\n                                \"tool_name\": response[\"choices\"][0][\"message\"][\n                                    \"function_call\"\n                                ][\"name\"],\n                                \"tool_input\": response[\"choices\"][0][\"message\"][\n                                    \"function_call\"\n                                ][\"arguments\"],\n                            }\n                        }\n                    )\n                )\n                return response\n\n            error_message = {\n                \"role\": \"system\",\n                \"content\": f\"Error: Your last function calling call function {response['choices'][0]['message']['function_call']['name']} that is not in the provided functions. Make sure function name in list: {list(map(lambda x:x['name'],req_kwargs['functions']))}\",\n            }\n\n            if (\n                req_kwargs[\"messages\"][-1][\"role\"] == \"system\"\n                and \"Your last function calling call function\"\n                in req_kwargs[\"messages\"][-1][\"content\"]\n            ):\n                req_kwargs[\"messages\"] = req_kwargs[\"messages\"][:-1]\n            req_kwargs[\"messages\"].append(error_message)\n\n            logger.typewriter_log(\n                f\"FunctionCallSchemaError: Function {response['choices'][0]['message']['function_call']['name']} not found in the provided functions.\",\n                Fore.RED,\n            )\n            raise FunctionCallSchemaError(\n                f\"Function {response['choices'][0]['message']['function_call']['name']} not found in the provided functions: {list(map(lambda x:x['name'],req_kwargs['functions']))}\"\n            )\n\n        arguments, response = self.load_args_with_schema_validation(\n            function_schema,\n            response[\"choices\"][0][\"message\"][\"function_call\"][\"arguments\"],\n            req_kwargs[\"messages\"],\n            return_response=True,\n            response=response,\n        )\n        return response\n\n\nobjgenerator = OBJGenerator()\n"}
{"type": "source_file", "path": "XAgent/ai_functions/request/openai.py", "content": "import json\nimport openai\nfrom XAgent.logs import logger\nfrom XAgent.config import CONFIG, get_apiconfig_by_model, get_model_name\n\nfrom tenacity import (\n    retry,\n    stop_after_attempt,\n    wait_exponential,\n    retry_if_not_exception_type,\n    wait_chain,\n    wait_none,\n)\nimport importlib.metadata as metadata\n\nif metadata.version(\"openai\") < \"1.0\":\n    from openai.error import AuthenticationError, PermissionError, InvalidRequestError\n\n    RETRY_ERRORS = (\n        AuthenticationError,\n        PermissionError,\n        InvalidRequestError,\n        AssertionError,\n    )\n\n    @retry(\n        retry=retry_if_not_exception_type(RETRY_ERRORS),\n        stop=stop_after_attempt(CONFIG.max_retry_times + 3),\n        wait=wait_chain(\n            *[wait_none() for _ in range(3)] + [wait_exponential(min=61, max=293)]\n        ),\n        reraise=True,\n    )\n    def chatcompletion_request(**kwargs):\n        \"\"\"Handle operation of OpenAI chat completion.\n\n        This function operates OpenAI chat completion with provided\n        arguments. It gets the model name, applies a JSON web token, if the\n        response indicates the context length has been exceeded, it attempts\n        to get a higher-capacity language model if it exists in the configuration\n        and reattempts the operation. Otherwise, it will raise an error message.\n\n        Args:\n            **kwargs: Variable length argument list including (model:str, etc.).\n\n        Returns:\n            dict: chat completion response.\n\n        Raises:\n            InvalidRequestError: If any error occurs during chat completion operation or\n            context length limit exceeded and no fallback models available.\n        \"\"\"\n        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise InvalidRequestError(\"maximum context length exceeded\", None)\n        except InvalidRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n                chatcompletion_kwargs.update(kwargs)\n                chatcompletion_kwargs.pop(\"schema_error_retry\", None)\n\n                response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n                response = json.loads(str(response))\n            else:\n                raise e\n\n        return response\n\nelse:\n    from openai import AuthenticationError, PermissionDeniedError, BadRequestError\n\n    RETRY_ERRORS = (\n        AuthenticationError,\n        PermissionDeniedError,\n        BadRequestError,\n        AssertionError,\n    )\n\n    @retry(\n        retry=retry_if_not_exception_type(RETRY_ERRORS),\n        stop=stop_after_attempt(CONFIG.max_retry_times + 3),\n        wait=wait_chain(\n            *[wait_none() for _ in range(3)] + [wait_exponential(min=61, max=293)]\n        ),\n        reraise=True,\n    )\n    def chatcompletion_request(**kwargs):\n        \"\"\"Handle operation of OpenAI v1.x.x chat completion.\n\n        This function operates OpenAI v1.x.x chat completion with provided\n        arguments. It gets the model name, applies a JSON web token, if the\n        response indicates the context length has been exceeded, it attempts\n        to get a higher-capacity language model if it exists in the configuration\n        and reattempts the operation. Otherwise, it will raise an error message.\n\n        Args:\n            **kwargs: Variable length argument list including (model:str, etc.).\n\n        Returns:\n            response (dict): A dictionary containing the response from the Chat API.\n            The structure of the dictionary is based on the API response format.\n\n        Raises:\n            BadRequestError: If any error occurs during chat completion operation or\n            context length limit exceeded and no fallback models available.\n        \"\"\"\n        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n\n        request_timeout = kwargs.pop(\"request_timeout\", 30)\n        if \"api_version\" in chatcompletion_kwargs:\n            if \"base_url\" in chatcompletion_kwargs:\n                base_url = chatcompletion_kwargs.pop(\"base_url\", None)\n            else:\n                base_url = chatcompletion_kwargs.pop(\"api_base\", None)\n            azure_endpoint = chatcompletion_kwargs.pop(\"azure_endpoint\", base_url)\n            api_version = chatcompletion_kwargs.pop(\"api_version\", None)\n            api_key = chatcompletion_kwargs.pop(\"api_key\", None)\n            chatcompletion_kwargs.pop(\"api_type\", None)\n            if \"engine\" in chatcompletion_kwargs:\n                model = chatcompletion_kwargs.pop(\"engine\", None)\n            else:\n                model = chatcompletion_kwargs.pop(\"model\", None)\n            chatcompletion_kwargs.update({\"model\": model})\n            chatcompletion_kwargs.update(kwargs)\n            client = openai.AzureOpenAI(\n                api_key=api_key,\n                azure_endpoint=azure_endpoint,\n                api_version=api_version,\n                timeout=request_timeout,\n            )\n        else:\n            if \"base_url\" in chatcompletion_kwargs:\n                base_url = chatcompletion_kwargs.pop(\"base_url\", None)\n            else:\n                base_url = chatcompletion_kwargs.pop(\"api_base\", None)\n            api_key = chatcompletion_kwargs.pop(\"api_key\", None)\n            organization = chatcompletion_kwargs.pop(\"organization\", None)\n            chatcompletion_kwargs.update(kwargs)\n            client = openai.OpenAI(\n                api_key=api_key,\n                organization=organization,\n                base_url=base_url,\n                timeout=request_timeout,\n            )\n        try:\n            completions = client.chat.completions.create(**chatcompletion_kwargs)\n            response = completions.model_dump()\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\n                    message=\"maximum context length exceeded\", response=None, body=None\n                )\n\n        except BadRequestError as e:\n            if \"maximum context length\" in e.message:\n                if model_name == \"gpt-4\" and \"gpt-4-32k\" in CONFIG.api_keys:\n                    model_name = \"gpt-4-32k\"\n                elif model_name == \"gpt-4\" and \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                    model_name = \"gpt-4-1106-preview\"\n                else:\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n\n                print(f\"max context length reached, retrying with {model_name}\")\n                chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n                request_timeout = kwargs.pop(\"request_timeout\", 60)\n                if \"base_url\" in chatcompletion_kwargs:\n                    base_url = chatcompletion_kwargs.pop(\"base_url\", None)\n                else:\n                    base_url = chatcompletion_kwargs.pop(\"api_base\", None)\n                api_key = chatcompletion_kwargs.pop(\"api_key\", None)\n                chatcompletion_kwargs.update(kwargs)\n                chatcompletion_kwargs.pop(\"schema_error_retry\", None)\n                completions = client.chat.completions.create(**chatcompletion_kwargs)\n                response = completions.model_dump()\n            else:\n                raise e\n\n        return response\n"}
{"type": "source_file", "path": "XAgent/ai_functions/function_manager.py", "content": "import os\nimport glob\nimport yaml\nimport json5\n\nfrom typing import Optional, Tuple\nfrom colorama import Fore\n\nfrom XAgent.config import CONFIG,get_apiconfig_by_model\nfrom XAgent.logs import logger\nfrom .request import objgenerator\n\nclass FunctionManager:\n    \"\"\"\n    This class provides methods to manage functions including registration and execution of functions.\n    The functions are defined and loaded from YAML configuration files located in the local directory\n    under subdirectories 'functions' and 'pure_functions'.\n\n    Attributes:\n      function_cfg_dir (str): The directory path where the function configuration files are located.\n      pure_function_cfg_dir (str): The directory path where the pure function configuration files are located.\n      function_cfgs (dict): A dictionary to store all loaded function configurations.\n    \"\"\"\n\n    def __init__(self,\n                 function_cfg_dir=os.path.join(os.path.dirname(__file__),'functions'),\n                 pure_function_cfg_dir=os.path.join(os.path.dirname(__file__),'pure_functions'),):\n        \"\"\"\n        Initializes the FunctionManager class with given directories for function configuration files.\n\n        Args:\n            function_cfg_dir (str): The directory path where the function configuration files are located.\n            pure_function_cfg_dir (str): The directory path where the pure function configuration files are located.\n        \"\"\"\n        self.function_cfg_dir = function_cfg_dir\n        self.pure_function_cfg_dir = pure_function_cfg_dir\n        self.function_cfgs = {}\n\n        for cfg_file in glob.glob(os.path.join(self.function_cfg_dir,'*.yaml')) + glob.glob(os.path.join(self.function_cfg_dir,'*.yml')):\n            with open(cfg_file,'r') as f:\n                function_cfg = yaml.load(f,Loader=yaml.FullLoader)\n            self.function_cfgs[function_cfg['function']['name']] = function_cfg\n\n        for cfg_file in glob.glob(os.path.join(self.pure_function_cfg_dir,'*.yaml')) + glob.glob(os.path.join(self.pure_function_cfg_dir,'*.yml')):\n            with open(cfg_file,'r') as f:\n                function_cfg = yaml.load(f,Loader=yaml.FullLoader)\n            for function in function_cfg['functions']:\n                self.function_cfgs[function['name']] = function\n    \n    def get_function_schema(self,function_name:str)->dict|None:\n        \"\"\"\n        Gets the schema of the function by its name.\n\n        Args:\n            function_name (str): The name of the function.\n\n        Returns:\n            dict: The schema of the function if found.\n            None: If the function is not found.\n        \"\"\"\n        return self.function_cfgs.get(function_name,None)\n    \n    def register_function(self,function_schema:dict):\n        \"\"\"\n        Registers a new function with its schema.\n\n        Args:\n            function_schema (dict): The schema of the function to register.\n        \"\"\"\n        if function_schema['name'] in self.function_cfgs:\n            return\n        self.function_cfgs[function_schema['name']] = function_schema\n        \n    def execute(self,function_name:str,return_generation_usage:bool=False,function_cfg:dict=None,**kwargs,)->Tuple[dict,Optional[dict]]:\n        \"\"\"\n        Executes a function by its name.\n\n        Args:\n            function_name (str): The name of the function to execute.\n            return_generation_usage (bool, optional): If set to True, also returns the usage of the function execution.\n            function_cfg (dict, optional): The configuration of the function. If not provided, retrieves it from the loaded functions.\n            **kwargs: The parameters of the function to execute.\n\n        Returns:\n            Tuple[dict,Optional[dict]]: A tuple containing the returns and optionally the usage of the executed function.\n\n        Raises:\n            KeyError: If the function configuration is not found.\n        \"\"\"\n        if function_cfg is None and function_name in self.function_cfgs:\n            function_cfg = self.function_cfgs.get(function_name)\n        else:\n            raise KeyError(f'Configure for function {function_name} not found.')\n        \n        \n        logger.typewriter_log(f'Executing AI Function: {function_name}', Fore.YELLOW)\n\n        completions_kwargs:dict = function_cfg.get('completions_kwargs',{})\n        if 'model' in completions_kwargs:\n            # check whether model is configured\n            try:\n                get_apiconfig_by_model(completions_kwargs['model'])\n            except:\n                logger.typewriter_log(\"Fallback\",Fore.YELLOW,f\"Model {completions_kwargs['model']} is not configured. Using default model instead.\")\n                completions_kwargs = {}\n        function_prompt = str(function_cfg['function_prompt'])\n        function_prompt = function_prompt.format(**kwargs)\n        messages = [{'role':'user','content':function_prompt}]\n        \n        match CONFIG.default_request_type:\n            case 'openai':                \n                response = objgenerator.chatcompletion(\n                    messages=messages,\n                    functions=[function_cfg['function']],\n                    function_call={'name':function_cfg['function']['name']},\n                    **completions_kwargs\n                )\n                returns = json5.loads(response['choices'][0]['message']['function_call']['arguments'])\n            case 'xagent':\n                arguments = function_cfg['function']['parameters']\n                response = objgenerator.chatcompletion(\n                    messages=messages,\n                    arguments=arguments,\n                    **completions_kwargs\n                )\n                returns = json5.loads(response['choices'][0]['message']['content'])['arguments']\n        \n        if return_generation_usage:\n            return returns, response['usage']\n        return returns\n    \n    def __getitem__(self,function_name,return_generation_usage=False,**kwargs):\n        \"\"\"\n        Allows the FunctionManager instance to behave like a dictionary, calling the execute method by key (which is actually the function name).\n\n        Args:\n            function_name (str): The name of the function to execute.\n            return_generation_usage (bool, optional): If set to True, also returns the usage of the function execution.\n            **kwargs: The parameters of the function to execute.\n\n        Returns:\n            The return of the execute method.\n        \"\"\"\n        return self.execute(function_name,return_generation_usage,**kwargs)\n\n    def __call__(self, function_name,return_generation_usage=False,**kwargs):\n        \"\"\"\n        Allows the FunctionManager instance to be callable, calling the execute method directly.\n\n        Args:\n            function_name (str): The name of the function to execute.\n            return_generation_usage (bool, optional): If set to True, also returns the usage of the function execution.\n            **kwargs: The parameters of the function to execute.\n\n        Returns:\n          The return of the execute method.\n        \"\"\"\n        return self.execute(function_name,return_generation_usage,**kwargs)\n\nfunction_manager = FunctionManager()"}
{"type": "source_file", "path": "XAgent/config.py", "content": "import os\nimport yaml\nfrom copy import deepcopy\n\n\nclass XAgentConfig(dict):\n    \"\"\"\n    A dictionary-like configuration class with attribute-style access.\n\n    Inherited from dictionary, this class provides methods for accessing and modifying\n    dictionary items using attributes and methods.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Initialize class instance.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n\n    def __getattr__(self, key):\n        \"\"\"\n        Access the class attribute.\n\n        Args:\n            key (str): Key to access the class attribute.\n\n        Returns:\n            Value of the class attribute for the input key.\n\n        Raises:\n            AttributeError: If the input key is not present in the dictionary.\n        \"\"\"\n        if key in self:\n            return self[key]\n        raise AttributeError(f\"'DotDict' object has no attribute '{key}'\")\n\n    def __setattr__(self, key, value):\n        \"\"\"\n        Set the value of the class attribute.\n\n        Args:\n            key (str): Key for the attribute to set.\n            value : Value to be set for the input key.\n        \"\"\"\n        self[key] = value\n\n    def __delattr__(self, key):\n        \"\"\"\n        Delete the class attribute.\n\n        Args:\n            key (str): Key of the attribute to delete.\n\n        Raises:\n            AttributeError: If the input key is not present in the dictionary.\n        \"\"\"\n        if key in self:\n            del self[key]\n        else:\n            raise AttributeError(f\"'DotDict' object has no attribute '{key}'\")\n\n    def to_dict(self, safe=False):\n        \"\"\"\n        Convert the xAgentConfig object to dictionary.\n\n        Args:\n            safe (bool, optional): If True, 'api_keys' will be excluded from the output.\n                Default is False.\n\n        Returns:\n            dict: Dictionary representation of the instance.\n        \"\"\"\n        if safe:\n            right_value = deepcopy(self)\n            right_value.pop(\"api_keys\", \"\")\n            return right_value\n        else:\n            return self\n\n    def reload(self, config_file=\"assets/config.yml\"):\n        \"\"\"\n        Load configuration data from YAML file and environment variables. And also update\n        the ARGS with new data.\n\n        Args:\n            config_file (str, optional): Path to the YAML configuration file.\n                Default is 'assets/config.yml'.\n        \"\"\"\n        config_file = os.getenv(\"CONFIG_FILE\", config_file)\n        print(\"---config file---\\n\" + str(config_file))\n        self.__init__(**yaml.load(open(config_file, \"r\"), Loader=yaml.FullLoader))\n        # check environment variables\n        print(\"---args---\\n\" + str(ARGS))\n        self.update(ARGS)\n\n    @staticmethod\n    def get_default_config(config_file=\"assets/config.yml\"):\n        \"\"\"\n        Get default configuration data from given file through environment variable.\n\n        Args:\n            config_file (str, optional): Path to the YAML configuration file.\n                Default is 'assets/config.yml'.\n\n        Returns:\n            XAgentConfig: An instance of XAgentConfig with loaded configuration data.\n        \"\"\"\n        try:\n            config_file = os.getenv(\"CONFIG_FILE\", config_file)\n            cfg = yaml.load(open(config_file, \"r\"), Loader=yaml.FullLoader)\n        except:\n            cfg = {}\n        return XAgentConfig(**cfg)\n\n\nCONFIG = XAgentConfig.get_default_config()\nARGS = {}\n\n\ndef get_model_name(model_name: str = None):\n    \"\"\"\n    Get the normalized model name for a given input model name.\n\n    Args:\n        model_name (str, optional): Input model name. Default is None.\n\n    Returns:\n        str: Normalized model name.\n\n    Raises:\n        Exception: If the model name is not recognized.\n    \"\"\"\n    if model_name is None:\n        model_name = CONFIG.default_completion_kwargs[\"model\"]\n\n    normalized_model_name = \"\"\n    match model_name.lower():\n        case \"gpt-4\":\n            normalized_model_name = \"gpt-4\"\n        case \"gpt-4-32k\":\n            normalized_model_name = \"gpt-4-32k\"\n        case \"gpt-4-1106-preview\":\n            normalized_model_name = \"gpt-4-1106-preview\"\n        case \"gpt-4-turbo\":\n            normalized_model_name = \"gpt-4-1106-preview\"\n        case \"gpt-3.5-turbo-16k\":\n            normalized_model_name = \"gpt-3.5-turbo-16k\"\n        case \"gpt-3.5-turbo-1106\":\n            normalized_model_name = \"gpt-3.5-turbo-1106\"\n        case \"gpt-3.5-turbo\":\n            normalized_model_name = \"gpt-3.5-turbo\"\n        case \"gpt4\":\n            normalized_model_name = \"gpt-4\"\n        case \"gpt4-32\":\n            normalized_model_name = \"gpt-4-32k\"\n        case \"gpt4-1106-preview\":\n            normalized_model_name = \"gpt-4-1106-preview\"\n        case \"gpt-35-16k\":\n            normalized_model_name = \"gpt-3.5-turbo-16k\"\n        case \"xagentllm\":\n            normalized_model_name = \"xagentllm\"\n        case _:\n            raise Exception(f\"Unknown model name {model_name}\")\n\n    return normalized_model_name\n\n\ndef get_apiconfig_by_model(model_name: str) -> dict:\n    \"\"\"\n    Get API configuration for a model by its name.\n\n    The function first normalizes the name, then fetches the API keys for this model\n    from the CONFIG and rotates the keys.\n\n    Args:\n        model_name (str): Name of the model.\n\n    Returns:\n        dict: Dictionary containing the fetched API configuration.\n    \"\"\"\n    normalized_model_name = get_model_name(model_name)\n    apiconfig = deepcopy(CONFIG.api_keys[normalized_model_name][0])\n    CONFIG.api_keys[normalized_model_name].append(\n        CONFIG.api_keys[normalized_model_name].pop(0)\n    )\n    return apiconfig\n"}
{"type": "source_file", "path": "XAgent/function_handler.py", "content": "import abc\nimport os\nimport time\nimport base64\nimport uuid\nimport json5 as json\nimport requests\nfrom XAgent.data_structure.node import ToolNode\nfrom typing import List\nfrom colorama import Fore, Style\nfrom concurrent.futures import ThreadPoolExecutor\n\n\nfrom XAgent.utils import ToolCallStatusCode\nfrom XAgent.ai_functions import function_manager\n\n\nclass FunctionHandler:\n    \"\"\"\n    The handler for functions.\n    \"\"\"\n\n    def __init__(self, config, logger=None):\n        self.logger = logger\n        self.config = config\n\n        self.subtask_submit_function = function_manager.get_function_schema(\n            \"subtask_submit\"\n        )\n\n        # TODO: support more complex versions of human help, like collaborative debugging.\n        self.ask_human_for_help_function = function_manager.get_function_schema(\n            \"ask_human_for_help\"\n        )\n        self.human_interruption_function = function_manager.get_function_schema(\n            \"human_interruption\"\n        )\n\n        self.avaliable_tools_description_list = []\n\n    def log_task_submit(self, arguments):\n        \"\"\"\n        Log the task submission.\n\n        Args:\n            arguments: The arguments of the task submission.\n        \"\"\"\n        self.logger.typewriter_log(\n            f\"-=-=-=-=-=-=-= SUBTASK SUBMITTED -=-=-=-=-=-=-=\",\n            Fore.YELLOW,\n            \"\",\n        )\n        self.logger.typewriter_log(\n            f\"submit_type:\", Fore.YELLOW, f\"{arguments['submit_type']}\"\n        )\n        self.logger.typewriter_log(\n            f\"success:\", Fore.YELLOW, f\"{arguments['result']['success']}\"\n        )\n        self.logger.typewriter_log(\n            f\"conclusion:\", Fore.YELLOW, f\"{arguments['result']['conclusion']}\"\n        )\n        if \"milestones\" in arguments[\"result\"].keys():\n            self.logger.typewriter_log(f\"milestones:\", Fore.YELLOW)\n            for milestone in arguments[\"result\"][\"milestones\"]:\n                line = milestone.lstrip(\"- \")\n                self.logger.typewriter_log(\"- \", Fore.GREEN, line.strip())\n        self.logger.typewriter_log(\n            f\"need_for_plan_refine:\",\n            Fore.YELLOW,\n            f\"{arguments['suggestions_for_latter_subtasks_plan']['need_for_plan_refine']}\",\n        )\n        self.logger.typewriter_log(\n            f\"plan_suggestions:\",\n            Fore.YELLOW,\n            f\"{arguments['suggestions_for_latter_subtasks_plan']['reason']}\",\n        )\n\n    def change_subtask_handle_function_enum(self, function_name_list: List[str]):\n        \"\"\"\n        Change the subtask handling function enumeration.\n\n        Args:\n            function_name_list: The list of function names.\n        \"\"\"\n        match self.config.default_request_type:\n            case \"openai\":\n                self.subtask_handle_function = function_manager.get_function_schema(\n                    \"subtask_handle\"\n                )\n                self.subtask_handle_function[\"parameters\"][\"properties\"][\"tool_call\"][\n                    \"properties\"\n                ][\"tool_name\"][\"enum\"] = function_name_list\n            case \"xagent\":\n                pass\n            case _:\n                raise NotImplementedError(\n                    f\"Request type {self.config.default_request_type} not implemented\"\n                )\n\n    def intrinsic_tools(self, enable_ask_human_for_help):\n        \"\"\"\n        Get the intrinsic tools.\n\n        Args:\n            enable_ask_human_for_help: Whether to enable the ask_human_for_help function.\n\n        Returns:\n            The intrinsic tools.\n        \"\"\"\n        tools = [\n            self.subtask_submit_function,\n        ]\n        if enable_ask_human_for_help:\n            tools.append(self.ask_human_for_help_function)\n        tools.extend(self.avaliable_tools_description_list)\n        return tools\n\n    def long_result_summary(self, command: dict, result):\n        \"\"\"\n        Summarize the long result.\n\n        Args:\n            command (dict): The command.\n            result: The result.\n\n        Returns:\n            The summarized result.\n        \"\"\"\n        if command[\"name\"] == \"WebEnv_browse_website\":\n            if not isinstance(result, str):\n                result = str(result)\n            result = function_manager(\n                \"parse_web_text\",\n                webpage=result[:8096],\n                prompt=command[\"arguments\"][\"goals_to_browse\"],\n            )\n            result[\"useful_hyperlinks\"] = result[\"useful_hyperlinks\"][:3]\n        if command[\"name\"] == \"WebEnv_search_and_browse\":\n            with ThreadPoolExecutor(max_workers=len(result)) as pool:\n                f = []\n                for ret in result:\n                    f.append(\n                        pool.submit(\n                            function_manager,\n                            \"parse_web_text\",\n                            webpage=ret[\"page\"][:8096],\n                            prompt=command[\"arguments\"][\"goals_to_browse\"],\n                        )\n                    )\n                for ret, thd in zip(result, f):\n                    ret[\"page\"] = thd.result()\n                    ret[\"page\"][\"useful_hyperlinks\"] = ret[\"page\"][\"useful_hyperlinks\"][\n                        :3\n                    ]\n\n        if isinstance(result, str) and len(result) > 2000:\n            # need to summarize\n            pass\n        return result\n\n    def handle_tool_call(self, node: ToolNode):\n        \"\"\"\n        Handle the tool call.\n\n        Args:\n            node (ToolNode): The tool node.\n\n        Returns:\n            The result, tool output status code, whether to refine the plan, and the tools used.\n        \"\"\"\n        plan_refine = False\n        command_name = node.data[\"command\"][\"properties\"][\"name\"]\n        arguments = node.data[\"command\"][\"properties\"][\"args\"]\n\n        self.logger.typewriter_log(\n            \"NEXT ACTION: \",\n            Fore.CYAN,\n            f\"COMMAND: {Fore.CYAN}{command_name}{Style.RESET_ALL}  \\n\"\n            f\"ARGUMENTS: \\n{Fore.CYAN}{arguments}{Style.RESET_ALL}\",\n        )\n\n        if command_name == \"subtask_submit\":\n            (\n                plan_refine,\n                tool_output_status_code,\n                command_result,\n            ) = self.handle_subtask_submit(arguments)\n        elif command_name == \"ask_human_for_help\":\n            (\n                plan_refine,\n                tool_output_status_code,\n                command_result,\n            ) = self.handle_human_help(arguments)\n        elif command_name == \"human_interruption\":\n            assert False, \"Never call this function\"\n        else:\n            command_result = \"\"\n            tool_output_status_code = ToolCallStatusCode.TOOL_CALL_SUCCESS\n\n        result = f\"Command {command_name} returned: \" + f\"{command_result}\"\n\n        node.data[\"tool_output\"] = command_result\n        node.data[\"tool_status_code\"] = tool_output_status_code\n\n        # node.workspace_hash_id = output_hash_id\n        if result is not None:\n            node.history.add(\"system\", result, \"action_result\")\n            self.logger.typewriter_log(\"SYSTEM: \", Fore.YELLOW, result)\n        else:\n            node.history.add(\"system\", \"Unable to execute command\", \"action_result\")\n            self.logger.typewriter_log(\n                \"SYSTEM: \", Fore.YELLOW, \"Unable to execute command\"\n            )\n\n        if tool_output_status_code == ToolCallStatusCode.TOOL_CALL_SUCCESS:\n            color = Fore.GREEN\n        elif tool_output_status_code == ToolCallStatusCode.SUBMIT_AS_SUCCESS:\n            color = Fore.YELLOW\n        elif tool_output_status_code == ToolCallStatusCode.SUBMIT_AS_FAILED:\n            color = Fore.BLUE\n        else:\n            color = Fore.RED\n\n        self.logger.typewriter_log(\n            \"TOOL STATUS CODE: \",\n            Fore.YELLOW,\n            f\"{color}{tool_output_status_code.name}{Style.RESET_ALL}\",\n        )\n\n        using_tools = {\n            \"tool_name\": command_name,\n            \"tool_input\": arguments,\n            \"tool_output\": command_result,\n            \"tool_status_code\": tool_output_status_code.name,\n            \"thought_data\": {\n                \"thought\": node.data[\"thoughts\"],\n                \"content\": node.data[\"content\"],\n            },\n        }\n\n        if tool_output_status_code in [\n            ToolCallStatusCode.SUBMIT_AS_SUCCESS,\n            ToolCallStatusCode.SUBMIT_AS_FAILED,\n        ]:\n            self.log_task_submit(arguments)\n\n        return result, tool_output_status_code, plan_refine, using_tools\n\n    def handle_subtask_submit(self, arguments):\n        \"\"\"\n        Handle the subtask submission.\n\n        Args:\n            arguments: The arguments of the subtask submission.\n\n        Returns:\n            bool: Whether to refine the plan.\n            The tool output status code.\n            The result.\n        \"\"\"\n        plan_refine = False\n        if arguments[\"result\"][\"success\"]:\n            tool_output_status_code = ToolCallStatusCode.SUBMIT_AS_SUCCESS\n        else:\n            tool_output_status_code = ToolCallStatusCode.SUBMIT_AS_FAILED\n        if arguments[\"suggestions_for_latter_subtasks_plan\"][\"need_for_plan_refine\"]:\n            plan_refine = True\n        answer = {\n            \"content\": f\"you have successfully submit the subtask as {arguments['submit_type']}\"\n        }\n        command_result = json.dumps(answer, ensure_ascii=False)\n\n        return plan_refine, tool_output_status_code, command_result\n"}
{"type": "source_file", "path": "XAgent/ai_functions/__init__.py", "content": "\"\"\"\nThis module initiates the AI functionality of the XAgent by importing the necessary classes and functions from the core utilities.\n\"\"\"\n\nfrom .function_manager import FunctionManager, function_manager\nfrom .request import objgenerator, OBJGenerator"}
{"type": "source_file", "path": "XAgent/ai_functions/request/xagent.py", "content": "from XAgent.logs import logger\nfrom XAgent.config import CONFIG,get_apiconfig_by_model,get_model_name\nimport requests\nimport traceback\n\n\ndef chatcompletion_request(**kwargs):\n    # logger.info(f\"xagent received {json.dumps(kwargs)}\")\n    model_name = get_model_name(kwargs.pop('model',CONFIG.default_completion_kwargs['model']))\n    logger.debug(\"chatcompletion: using \" + model_name)\n    chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n    chatcompletion_kwargs.update(kwargs)\n\n    response = requests.post(\n        chatcompletion_kwargs.get(\"api_base\",\"http://127.0.0.1:8000/chat/completions\"),\n        headers={\"accept\": \"application/json\", \"Content-Type\": \"application/json\"},\n        json={\n            \"model\": model_name,\n            \"repetition_penalty\": chatcompletion_kwargs.get(\"repetition_penalty\", 1.2),\n            \"temperature\": chatcompletion_kwargs.get(\"temperature\", 0.8),\n            \"top_p\":chatcompletion_kwargs.get(\"top_p\", 1.0),\n            \"frequency_penalty\":chatcompletion_kwargs.get(\"frequency_penalty\",0.5),\n            \"presence_penalty\":chatcompletion_kwargs.get(\"presence_penalty\", 0.0),\n            \"max_tokens\":chatcompletion_kwargs.get(\"max_tokens\", 4096),\n            \"messages\": chatcompletion_kwargs.get(\"messages\", []),\n            \"arguments\": chatcompletion_kwargs.get(\"arguments\", {}),\n            \"functions\": chatcompletion_kwargs.get(\"functions\", []),\n            \"function_call\": chatcompletion_kwargs.get(\"function_call\", {}),\n        }\n    ).json()\n\n    return response\n"}
{"type": "source_file", "path": "XAgent/global_vars.py", "content": "# Importing necessary modules\n# from XAgent.workflow.working_memory import WorkingMemoryAgent\n# from XAgent.agent.dispatcher import agent_dispatcher\n# from XAgent.vector_db import VectorDBInterface\n# from XAgent.running_recorder import RunningRecoder\nfrom XAgent.config import CONFIG as config\n\n# Creating the instances of the classes\n# working_memory_agent = WorkingMemoryAgent()\n# vector_db_interface = VectorDBInterface()\n# recorder = RunningRecoder()\n"}
{"type": "source_file", "path": "XAgent/ai_functions/request/__init__.py", "content": "from .obj_generator import objgenerator,OBJGenerator"}
{"type": "source_file", "path": "XAgent/inner_loop_search_algorithms/ReACT.py", "content": "import json\n\nfrom colorama import Fore\nfrom XAgent.config import CONFIG\nfrom XAgent.agent.base_agent import BaseAgent\nfrom XAgent.agent.summarize import summarize_action, summarize_plan\nfrom XAgent.core import XAgentCoreComponents\nfrom XAgent.data_structure.node import ToolNode\nfrom XAgent.data_structure.tree import TaskSearchTree\nfrom XAgent.inner_loop_search_algorithms.base_search import BaseSearchMethod\nfrom XAgent.message_history import Message\nfrom XAgent.utils import SearchMethodStatusCode, ToolCallStatusCode\nfrom XAgent.agent.simple_agent.post_process import exec_code, extract_code\nfrom .score import evaluate_res, understand\nfrom .wiki_search import search_from_concept\n\nNOW_SUBTASK_PROMPT = \"\"\"\n\n\"\"\"\n\n# add tool prompt\n\nPYTHON_PROMPT = \"\"\"[answer]: ```{PYTHON CODE}```\nYour answer should be a piece of python code which solves the current question.\nEncase your code within triple backticks for clarity.\nYou must end your code with printing all the result and their units.\nMake sure the code can be successfully run without any input.\nBe precise.The answer should be accurate, choose the appropriate units, and prohibit the use of round functions to round off and lose the results.\nMake sure the code can be successfully run without any input. And import all the modules that you need to use.\n\nfor example, you could response Answer conclusion part like this:\n**Answer conclusion:**\n[answer]: ```python\nimport numpy as np\n\n# Value of 2 pi c omega_obs\nomega_obs = 1.8133708490380042e+23  # Hz\n\n# Value of D for H35Cl\nD = 440.2  # kJ/mol\n\n# Calculate beta\nbeta = omega_obs * (2 * D)**0.5\n\n# Print the result\nprint(\"The value of beta is:\", beta, \"cm^(-1)\")\n```\n\n\n\"\"\"\n\nSIMPLE_PROMPT = \"\"\"[answer]:\\\\boxed{[ANSWER]}.\nSummarize a short text summary of the answer if it is a text answer, and a specific numerical value if it is a scientific question.The answer will be displayed to the user and processed by subsequent subtasks, so it must contain enough information to help subsequent subtasks proceed smoothly. At the same time, the answer needs to complete the goals and all milestones of the current processing subtask.\nPlease add units in your answer.The format of the answer is \"[answer]:\\\\boxed{[ANSWER]}.\"\n\n\"\"\"\n\n\ndef make_message(now_node: ToolNode, max_length, config, now_dealing_task):\n    \"\"\"\n    Function to generate messages for each node.\n\n    Args:\n        now_node: The current ToolNode instance.\n        task_handler: Handler of the tasks.\n        max_length: Maximum length of the subtask chain.\n        config: The configuration settings.\n\n    Returns:\n        The sequence of messages for the current node.\n\n    \"\"\"\n\n    if CONFIG.enable_summary:\n        terminal_task_info = summarize_plan(now_dealing_task.to_json())\n    else:\n        terminal_task_info = json.dumps(\n            now_dealing_task.to_json(), indent=2, ensure_ascii=False\n        )\n\n    message_sequence = []\n\n    now_subtask_prompt = f'''Now you will perform the following subtask:\\n\"\"\"\\n{terminal_task_info}\\n\"\"\"\\n'''\n    message_sequence.append(Message(\"user\", now_subtask_prompt))\n    action_process = now_node.process\n\n    if config.enable_summary:\n        action_process = summarize_action(action_process, terminal_task_info)\n    user_prompt = f\"\"\"The following steps have been performed (you have already done the following and the current file contents are shown below):\\n\n    {action_process}\n    \"\"\"\n    message_sequence.append(Message(\"user\", user_prompt))\n    return message_sequence\n\n\nclass ReACTChainSearch(BaseSearchMethod):\n    \"\"\"\n    Class for ReACT chain search. It performs chain based searches for tasks.\n    \"\"\"\n\n    def __init__(self, xagent_core_components: XAgentCoreComponents, tool):\n        \"\"\"\n        xagent_core_components: XAgentCoreComponents object, used to initialize ReACTChainSearch object\n        Initializes ReACTChainSearch object. It maintains a list of trees to represent\n        the processed tasks.\n        \"\"\"\n        super().__init__()\n\n        self.tree_list = []\n        self.finish_node = None\n        self.tool = tool\n        if \"python\" in tool:\n            self.tool_prompt = PYTHON_PROMPT\n        else:\n            self.tool_prompt = SIMPLE_PROMPT\n\n        self.xagent_core_components = xagent_core_components\n        self.vector_db = xagent_core_components.vector_db_interface\n\n    def run(\n        self,\n        config,\n        agent: BaseAgent,\n        arguments,\n        functions,\n        task_id,\n        now_dealing_task,\n        plan_agent,\n        max_try=1,\n        max_answer=1,\n    ):\n        \"\"\"\n        Runs the chain search task.\n\n        Args:\n            config: Configuration for the search.\n            agent: Base agent responsible for chain search.\n            arguments: Arguments for the current task to be handled.\n            functions: The available functions for use by agent.\n            task_id: ID of the current task.\n            max_try: Maximum number of attempts.\n            max_answer: Maximum number of answers to be received\n\n        Returns:\n            None\n        Raises:\n            None\n        \"\"\"\n\n        for _attempt_id in range(max_try):\n            return self.generate_chain(\n                config,\n                agent,\n                arguments,\n                functions,\n                task_id,\n                now_dealing_task,\n                plan_agent,\n            )\n\n    def get_finish_node(self):\n        \"\"\"\n        Function to retrieve the finished node in the task tree.\n\n        Returns:\n            The finished node.\n        \"\"\"\n        return self.finish_node\n\n    def generate_chain(\n        self,\n        config,\n        agent: BaseAgent,\n        arguments,\n        functions,\n        task_id,\n        now_dealing_task,\n        plan_agent,\n    ):\n        \"\"\"\n        Run the chain search task.\n\n        Args:\n            config: Configuration for the search.\n            agent: Base agent responsible for chain search.\n            arguments: Arguments for the current task to be handled.\n            functions: The available functions for use by agent.\n            task_id: ID of the current task.\n\n        Returns:\n            None.\n        Raises:\n            None.\n        \"\"\"\n\n        # search similar trial\n        search_goal = now_dealing_task.data.goal\n\n        success_prompt = self.vector_db.search_from_hierarchical(\n            search_goal, namespace=\"SUCCESS\", top_k=CONFIG.exec_topk\n        )\n        fail_prompt = \"\"\n\n        print(\"success_similar_lists:\", success_prompt)\n        # print(\"fail_similar_lists:\", fail_prompt)\n\n        # success_prompt = fail_prompt = []\n        all_plan = plan_agent.latest_plan.to_json()\n\n        if config.enable_summary:\n            all_plan = summarize_plan(all_plan)\n        else:\n            all_plan = json.dumps(all_plan, indent=2, ensure_ascii=False)\n        print(\"all_plan for now task:\")\n        print(all_plan)\n\n        # web_search\n        additional_prompt = \"\"\n        if CONFIG.web_search:\n\n            additional_prompt = search_from_concept(now_dealing_task.data.goal)\n\n        max_confidence = 0\n        final_res = final_ans = \"None\"\n        for i in range(3):\n\n            response, ans = agent.parse(\n                placeholders={\n                    \"system\": {\"all_plan\": all_plan},\n                    \"user\": {\n                        \"subtask_id\": now_dealing_task.get_subtask_id(to_str=True),\n                        \"subtask_goal\": now_dealing_task.data.goal,\n                        \"milestones\": str(now_dealing_task.data.milestones),\n                        \"success_prompt\": success_prompt,\n                        # \"fail_prompt\": fail_prompt,\n                        \"tool_prompt\": self.tool_prompt,\n                        \"additional_prompt\": additional_prompt,\n                    },\n                },\n                arguments=arguments,\n                functions=[self.tool],\n                function_call=None,\n            )\n            print(f\"______ans:{ans}_______\\n\")\n            if not CONFIG.score:\n                break\n            if ans != \"None\":\n\n                conf = evaluate_res(\n                    now_dealing_task.data.goal,\n                    str(now_dealing_task.data.milestones),\n                    response\n                    + \"\\nWe executed the generated python code and got the answer: \"\n                    + ans,\n                )\n                if conf >= max_confidence:\n                    final_res, final_ans = response, ans\n                    max_confidence = conf\n\n            if i == 2:\n                response, ans = final_res, final_ans\n\n        if ans != \"None\":\n            now_dealing_task.data.response = response\n        else:\n            now_dealing_task.data.response = \"None\"\n\n        print(\"++++++++++++++++++++\")\n        print(response)\n        print(\"++++++++++++++++++++\")\n\n        if self.tool == \"python\" and ans != \"None\":\n            print(\"________python_code___________\")\n            code = extract_code(response)\n            print(code)\n            now_dealing_task.data.thoughts = (\n                \"This Python code solves the problem.\\n\" + code\n            )\n\n        else:\n            now_dealing_task.data.thoughts = response\n        print(\"--------answer-----------\")\n        print(ans)\n        print(\"-------------------\\n\")\n\n        now_dealing_task.data.answer = ans\n\n        if CONFIG.refine:\n\n            if ans == None or ans == \"\" or ans == \"None\":\n                self.need_for_plan_refine = True\n            else:\n                self.need_for_plan_refine = False\n        return ans\n\n    def to_json(self):\n        \"\"\"\n        Placeholder function to convert ReACTChainSearch object to JSON.\n\n        Currently not implemented.\n\n        Returns:\n            None\n        \"\"\"\n        pass\n\n    def is_include_pictures(self, using_tools):\n\n        tool_name = (\n            using_tools.get(\"tool_name\", \"\") if isinstance(using_tools, dict) else \"\"\n        )\n        tool_output = (\n            using_tools.get(\"tool_output\", {}) if isinstance(using_tools, dict) else \"\"\n        )\n        if tool_name == \"PythonNotebook_execute_cell\":\n            for output in tool_output:\n                if isinstance(output, dict) and \"file_name\" in output:\n                    return True\n        return False\n"}
