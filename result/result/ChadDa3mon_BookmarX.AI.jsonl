{"repo_info": {"repo_name": "BookmarX.AI", "repo_owner": "ChadDa3mon", "repo_url": "https://github.com/ChadDa3mon/BookmarX.AI"}}
{"type": "test_file", "path": "app/tests/test_exceptions.py", "content": "\n# Since exceptions.py simply defines two custom exceptions, it might not be necessary to write tests for it.\n# Instead, you could write tests in other modules that check whether the correct exception is raised when expected.\n"}
{"type": "test_file", "path": "app/tests/test_gui.py", "content": "# This is a test file for the corresponding module in the app directory.\n"}
{"type": "test_file", "path": "app/tests/test_utils.py", "content": "\nimport sys\nimport os\n# Run 'pytest' from within the 'apps' directory\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nfrom unittest.mock import Mock, patch\nfrom utils import search_db, get_all_bookmarx, get_bookmarx_by_id, add_bookmark\n\ndef test_search_db():\n    # Create a mock for the cursor\n    cursor_mock = Mock()\n    cursor_mock.fetchall.return_value = [(1, 'http://example.com', 'Example')]\n\n    # Call the function with a test argument\n    result = search_db('test', cursor=cursor_mock)\n\n    # Check that the mock was used correctly\n    cursor_mock.execute.assert_called_once_with(\n        'select id,url,summary from webpages where match(raw_text) against (%s in natural language mode);',\n        ('test',)\n    )\n    cursor_mock.fetchall.assert_called_once()\n\n    # Check that the result is as expected\n    assert result == [(1, 'http://example.com', 'Example')]\n\n\n\n\ndef test_get_all_bookmarx():\n    # Placeholder for test\n    pass\n\ndef test_get_bookmarx_by_id():\n    # Placeholder for test\n    pass\n\ndef test_add_bookmark():\n    # Placeholder for test\n    pass\n"}
{"type": "test_file", "path": "app/tests/test_models.py", "content": "\nimport sys\nimport os\n# This helps me to run 'pytest' from within the 'apps' directory\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nfrom urllib.parse import urlparse, parse_qs, urlencode\nfrom models import Bookmarx,Bookmark\n\ndef test_bookmark():\n    # Create a Bookmark with a test input\n    bookmark = Bookmark(url='http://example.com?utm_source=test')\n\n    # Check that the Bookmark's attributes are as expected\n    assert str(bookmark.url) == 'http://example.com/'\n"}
{"type": "test_file", "path": "app/tests/test_logstuff.py", "content": "# This is a test file for the corresponding module in the app directory.\n"}
{"type": "source_file", "path": "app/exceptions.py", "content": "class URLAlreadyExistsError(Exception):\n    pass\n\nclass WriteArticleToDBError(Exception):\n    pass"}
{"type": "source_file", "path": "app/main.py", "content": "from fastapi import FastAPI, Path\nfrom pydantic import BaseModel\nfrom models import Bookmark,BookmarxResponse,BookmarxListResponse, URLAlreadyExistsError\nfrom typing import List\nfrom utils import add_bookmark,get_all_bookmarx,get_bookmarx_by_id\nfrom exceptions import URLAlreadyExistsError#, WriteArticleToDBError\n\n\napp = FastAPI()\n\n@app.get(\"/bookmarx\",response_model=BookmarxListResponse)\n#@app.get(\"/bookmarx\")\nasync def get_bookmarks():\n    bookmark_list = get_all_bookmarx()\n    for bookmarx in bookmark_list:\n        print(f\"Bookmark: {bookmarx}\")\n        print(f\" ID Type: {type(bookmarx['id'])}\")\n        print(f\" URL Type: {type(bookmarx['url'])}\")\n        print(f\" Summary Type: {type(bookmarx['summary'])}\")\n    return BookmarxListResponse(bookmarx = bookmark_list)\n\n@app.get(\"/bookmarx/{id}\", response_model=BookmarxResponse)\nasync def get_bookmarx(id: int = Path(..., title=\"Bookmark ID\")):\n    bookmark = get_bookmarx_by_id(id)\n    if bookmark is None:\n        raise HTTPException(status_code=404, detail=\"Bookmark not found\")\n    return bookmark\n\n\n@app.post(\"/bookmarks/add\")\nasync def add_bookmark_route(payload: Bookmark):\n    url = str(payload.url)\n    # print(f\"Received URL: {url}\")\n    \n    try:\n        await add_bookmark(url)\n    except URLAlreadyExistsError as e:\n        return {\"message\": str(e)}\n    # except WriteArticleToDBError as e:\n    #     return {\"message\": str(e)}\n    except Exception:\n        return {\"message\": \"An unknown error occurred\"}\n    \n    return {\"message\": \"Bookmark added successfully\"}\n\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"}
{"type": "source_file", "path": "app/gui.py", "content": "import gradio as gr\nfrom utils import search_db,get_markdown\n\ndef on_select(evt: gr.SelectData):  # SelectData is a subclass of EventData\n    print(f\"You selected {evt.value} at {evt.index} from {evt.target}\")\n    markdown = get_markdown(evt.value)\n    return markdown\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# BookmarX.AI\\n\\n[GitHub](https://github.com/ChadDa3mon/BookmarX.AI)\")\n    with gr.Tab(\"Add Bookmark\"):\n        with gr.Row():\n            add_url = gr.Textbox(label=\"URL To Add\",info=\"Enter the URL you wish to add\",scale=5)\n    with gr.Tab(\"Search Bookmarks\"):\n        with gr.Row():\n            search_term = gr.Textbox(label=\"Search Term\")\n        with gr.Row():\n            search_results = gr.Dataframe(label=\"Search Results\",headers=['ID','URL','Summary'],wrap=True)\n        with gr.Row():\n            webpage_markdown = gr.Markdown()\n        search_term.submit(search_db,search_term,search_results)\n        #search_results.select(get_markdown,search_results,webpage_markdown)\n        #search_results.select(on_select, None, webpage_markdown, scroll_to_output=True)\n        search_results.select(on_select, None, webpage_markdown)\n\n\ndemo.launch(inbrowser=False,server_name=\"0.0.0.0\")\n"}
{"type": "source_file", "path": "app/logstuff.py", "content": "from datetime import date\nimport logging\nfrom pythonjsonlogger import jsonlogger\n\ndef setup_logging(log_level=logging.INFO, log_file=None):\n    log_file = \"request.log\"\n    # Create a JSON logger\n    logger = logging.getLogger()\n\n    # Clear any existing handlers to avoid duplicate logs\n    logger.handlers.clear()\n\n    # Create a console handler with the JSON formatter\n    console_handler = logging.StreamHandler()\n    formatter = jsonlogger.JsonFormatter(fmt='%(asctime)s %(levelname)s %(message)s')\n    console_handler.setFormatter(formatter)\n    logger.addHandler(console_handler)\n\n    # Add a file handler if log_file path is provided\n    if log_file:\n        file_handler = logging.FileHandler(log_file)\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n\n    logger.setLevel(log_level)\n\n\n    return logger\n\nif __name__ == \"__main__\":\n    pass"}
{"type": "source_file", "path": "app/models.py", "content": "from pydantic import BaseModel,validator, HttpUrl,root_validator,Field\nimport urllib.parse\nfrom typing import ClassVar,List\nfrom logstuff import setup_logging\nfrom urllib.parse import urlparse, parse_qs, urlencode\n\n\nlogger = setup_logging()\n\nclass Bookmark(BaseModel):\n    url: HttpUrl\n\n    #tracking_params_to_remove = ['utm_source', 'utm_medium', 'utm_name', 'utm_content', 'utm_term']\n    tracking_params_to_remove: ClassVar[list] = ['utm_source', 'utm_medium', 'utm_name', 'utm_content', 'utm_term']\n\n    @validator('url', pre=True)\n    def strip_utm(cls, v):\n        # Parse the URL and the query string\n        parsed_url = urlparse(v)\n        query_params = parse_qs(parsed_url.query)\n\n        # Remove the tracking parameters\n        query_params.pop('utm_source', None)\n        query_params.pop('utm_medium', None)\n        query_params.pop('utm_campaign', None)\n\n        # Construct a new query string without the tracking parameters\n        new_query = urlencode(query_params, doseq=True)\n\n        # Replace the old query string with the new one in the URL\n        stripped_url = parsed_url._replace(query=new_query).geturl()\n        logger.debug(f\"DEBUG: Original URL: {v}\")\n        logger.debug(f\"DEBUG: Stripped URL: {stripped_url}\")\n        return stripped_url\n\n\n    # @validator('url', pre=True) \n    # def strip_tracking_info(cls, value):\n    #     logger.info(f\"Received URL {value}, checking for tracking parameters\")\n    #     # Parse the URL to get its components\n    #     parsed_url = urllib.parse.urlparse(value)\n\n    #     # Check if the query component of the URL contains tracking parameters\n    #     query_params = urllib.parse.parse_qs(parsed_url.query)\n\n    #     # Remove the specified tracking parameters from the query string\n    #     for param in cls.tracking_params_to_remove:\n    #         logger.debug(f\"Removed tracking parameter '{param}'\")\n    #         query_params.pop(param, None)\n\n    #     # Rebuild the query string without the removed tracking parameters\n    #     new_query_string = urllib.parse.urlencode(query_params, doseq=True)\n\n    #     # Reassemble the URL with the modified query string\n    #     new_url = parsed_url._replace(query=new_query_string).geturl()\n    #     # logger.debug(f\"URL after stripping out tracking: {new_url}\")\n    #     if new_url == value:\n    #         logger.debug(f\"No tracking found\")\n    #     else:\n    #         logger.debug(f\"Tracking found, new URL: {new_url}\")\n    #     return new_url \n\nclass BookmarxResponse(BaseModel):\n    url: str\n    summary: str\n    raw_text: str\n    markdown: str\n\nclass Bookmarx(BaseModel):\n    \"\"\"Bookmarx represents a single entry\"\"\"\n    id: int = Field(..., description=\"Some description of the int\", ge=0)\n    url: str\n    summary: str\n\nclass BookmarxListResponse(BaseModel):\n    bookmarx: List[Bookmarx]\n\ndef URLAlreadyExistsError(Exception):\n    pass\n\ndef WriteArticleToDBError(Exception):\n    pass"}
{"type": "source_file", "path": "app/utils.py", "content": "import openai\nimport os\nfrom dotenv import load_dotenv\nimport mysql.connector\nimport requests\nfrom newspaper import Article,fulltext\nimport json\nfrom datetime import date\nimport asyncio\nfrom logstuff import setup_logging\nfrom exceptions import URLAlreadyExistsError, WriteArticleToDBError\n# import pymysql\n# from pymysqlpool import ConnectionPool\nimport mysql.connector.pooling\n\nlogger = setup_logging()\n\n\nload_dotenv()\n\nopenai_api_key = os.getenv(\"OPENAI_API_KEY\",\"Error\")\ndb_host = os.getenv(\"DB_HOST\")\ndb_table = os.getenv(\"DB_TABLE\")\ndb_user = os.getenv(\"DB_USER\")\ndb_pass = os.getenv(\"DB_PASS\")\ndb_name = os.getenv(\"DB_NAME\")\n\n# cnx = mysql.connector.connect(\n#     host=db_host,\n#     user=db_user,\n#     password=db_pass,\n#     database=db_name\n#     )\npool = mysql.connector.pooling.MySQLConnectionPool(\n    host=db_host,\n    user=db_user,\n    password=db_pass,\n    database=db_name,\n    pool_name=\"mypool\",\n    pool_size=5  # Set the desired pool size\n)\n\n# def search_db(search_term):\n#     cnx = mysql.connector.connect(\n#     host=db_host,\n#     user=db_user,\n#     password=db_pass,\n#     database=db_name\n#     )\n\n#     logger.info(f\"Searching for '{search_term}\")\n#     cursor = cnx.cursor()\n#     query = \"select id,url,summary from webpages where match(raw_text) against (%s in natural language mode);\"\n#     cursor.execute(query, (search_term,))\n#     rows = cursor.fetchall()\n#     cursor.close()\n#     return rows\n\ndef search_db(search_term, cursor=None):\n    # If no cursor is provided, create one using the connection\n    if cursor is None:\n        connection = pool.get_connection()\n        cursor = connection.cursor()\n\n    cursor.execute(\n        'select id,url,summary from webpages where match(raw_text) against (%s in natural language mode);',\n        (search_term,)\n    )\n    result = cursor.fetchall()\n    cursor.close()\n    connection.close()\n\n    # If a connection was created, close it\n    # if cursor is None:\n    #     cnx.close()\n\n    return result\n\n\ndef get_all_bookmarx():\n    connection = pool.get_connection()\n    cursor = connection.cursor()\n    query = \"select id,url,summary from webpages\"\n    cursor.execute(query)\n    rows = cursor.fetchall()\n    cursor.close()\n    connection.close()\n\n    # Convert the rows tuple into a list of dictionaries\n    bookmarx_list = [{\"id\": int(row[0]), \"url\": row[1], \"summary\": row[2]} for row in rows]\n    return bookmarx_list\n\ndef get_bookmarx_by_id(id):\n    logger.info(f\"Retrieving results for ID: {id}\")\n    connection = pool.get_connection()\n    cursor = connection.cursor()\n    cursor = cnx.cursor()\n    query = \"select url,summary,raw_text,markdown from webpages where id = %s\"\n    cursor.execute(query,(id,))\n    row = cursor.fetchone()\n    cursor.close()\n    connection.close()\n\n    if row is None: # Make sure I got something back\n        logger.error(f\"No results found for ID: {id}\")\n        return f\"No match for ID: {id}\"\n\n    # row comes back as a tuple, I want to convert it to a dict so I can send it back via FastAPI as JSON\n    if not isinstance(row,str): \n        logger.debug(f\"Retrieved Results for ID {id}\")\n        bookmark_dict = {\n            \"url\": row[0],\n            \"summary\": row[1],\n            \"raw_text\": row[2],\n            \"markdown\": row[3]\n        }\n        return bookmark_dict\n    else:\n        logger.error(f\"Error retrieving results for ID: {id}: {row}\")\n        return {\"error\": row}\n\n    \n    \n\ndef get_markdown(url):\n    #my_url = url['URL'].tolist()[0]\n    my_url_list = [url]\n    #logger.info(f\"Retrieving markdown for {url['URL'].tolist()[0]}\")\n    logger.info(f\"Retrieving markdown for {url}\")\n    connection = pool.get_connection()\n    cursor = connection.cursor()\n    query = \"select markdown from webpages where url = %s\"\n    #cursor.execute(query, (url['URL'].tolist()))\n    cursor.execute(query, (my_url_list))\n    row = cursor.fetchone()\n    cursor.close()\n    connection.close()\n\n    if row is not None:\n        # Extract the value from the tuple (assuming 'markdown' is the first column)\n        markdown_text = row[0]\n        return markdown_text\n    else:\n        logger.error(f\"No markdown returned for {url}\")\n        # Handle the case when the query returns no results\n        return False\n    \n\n\nasync def get_tags():\n    connection = pool.get_connection()\n    cursor = connection.cursor()\n    query = \"SELECT * from tags\"\n    cursor.execute(query)\n    rows = cursor.fetchall()\n    tags_list = [row[1] for row in rows]\n    cursor.close()\n    connection.close()\n\n    return tags_list\n\nasync def get_url_from_db(url):\n    logger.debug(f\"Checking DB for URL: {url}\")\n    try:\n        connection = pool.get_connection()\n        cursor = connection.cursor()\n        query = f\"SELECT * from webpages where url = '{url}'\"\n        cursor.execute(query)\n        rows = cursor.fetchall()\n        cursor.close()\n        connection.close()\n\n        if not rows: #No results found\n            logger.debug(f\"URL {url} was not found in the database\")\n            return False\n        else:\n            logger.debug(f\"Found URL {url} in database\")\n            return rows\n    except Exception as e:\n        logger.error(f\"Error looking for URL: {url}: {e}\")\n\nasync def write_article_to_db(webpage_title,webpage_summary,webpage_text,webpage_markdown,url,tags):\n    logger.info(f\"Writing URL: {url} to database\")\n    current_date = date.today().isoformat()\n    tags_str = \",\".join(tags)\n    connection = pool.get_connection()\n    cursor = connection.cursor()\n    query = f\"INSERT INTO {db_table} (url, title, date_added, summary, raw_text, tags, markdown) VALUES (%s, %s, %s, %s, %s, %s, %s)\"\n    values = (url, webpage_title, current_date, webpage_summary, webpage_text, tags_str, webpage_markdown)\n    try:\n        cursor.execute(query, values)\n        connection.commit()\n        cursor.close()\n        connection.close()\n        return True\n    except Exception as e:\n        logger.error(f\"Error: {e} \")\n        cursor.close()\n        connection.close()\n        return False\n\n\n\n\nasync def get_webpage_text(url):\n    logger.info(f\"Retrieving webpage text for {url}\")\n    article = Article(url)\n    article.download()\n    try:\n        article.parse()\n        article_title = article.title\n        article_text = article.text\n        logger.info(f\" - parsed title: {article_title}\")\n    except:\n        article_title = \"No Title Found\"\n        html = requests.get(url).text\n        article_text = fulltext(html)\n    return article_title, article_text\n        \n\n\nasync def query_gpt_summary(body):\n    tags = await get_tags()\n\n    response = openai.ChatCompletion.create(\n    model=\"gpt-4\",\n    messages = [\n        {\n            \"role\":\"system\",\n            \"content\":f\"\"\"\nYou are a helpful assistant who will summarize the body of a website for me. \nFor the following article, you will do two things: Create a 1 paragraph summary, \nand assign any tags from my list you think are applicable. \nThe list of tags I want you to consider is {tags}. Your response should be in JSON, for example:\n\n    \"tags\":list_of_tags,\n    \"summary\":webpage_summary\n\n\"\"\"\n        },\n        {\n            \"role\":\"user\",\n            \"content\":body\n        }\n    ],\n    temperature=.4,\n    max_tokens=1024,\n    api_key=openai_api_key \n    ) \n\n    return response.choices[0].message.content\n\nasync def query_gpt_markdown(body):\n\n    response = openai.ChatCompletion.create(\n    model=\"gpt-4\",\n    messages = [\n        {\n            \"role\":\"system\",\n            \"content\":f\"\"\"\nYou are an AI assistant. I am going to give you the raw text from a website\nand you will transform it into markdown syntax, then send it back to me.\"\"\"\n        },\n        {\n            \"role\":\"user\",\n            \"content\":body\n        }\n    ],\n    temperature=.4,\n    max_tokens=1024,\n    api_key=openai_api_key\n    ) \n\n    return response.choices[0].message.content\n\nasync def add_bookmark(url):\n    logger.info(f\"Received request to download {url}\")\n    logger.info(f\"Checking to see if we already have {url} in our database\")\n\n    already_exists = await get_url_from_db(url)\n    if already_exists:\n        msg = f\"URL {url} already exists in database\"\n        logger.info(msg)\n        raise URLAlreadyExistsError(msg)\n        return msg\n    \n    logger.info(f\"{url} not found in database, proceeding to add\")\n    \n    webpage_title, webpage_text = await get_webpage_text(url)\n    webpage_summary_payload = await query_gpt_summary(webpage_text)\n    webpage_dict = json.loads(webpage_summary_payload)\n    tags = webpage_dict['tags']\n    webpage_summary = webpage_dict['summary']\n    webpage_markdown = await query_gpt_markdown(webpage_text)\n    db_update_reult = await write_article_to_db(webpage_title,webpage_summary,webpage_text,webpage_markdown,url,tags)\n\n    if not db_update_reult:\n        msg = f\"Failed to add URL {url} the database\"\n        logger.error(msg)\n        raise WriteArticleToDBError(msg)\n    \n    logger.info(f\"URL {url} successfully added to database\")\n  \n\nif __name__ == \"__main__\":\n    url = \"https://www.linkedin.com/pulse/3-ways-vector-databases-take-your-llm-use-cases-next-level-mishra\"\n    #add_bookmark(url)\n    #results = get_bookmarx_by_id(\"1\")\n    results = get_all_bookmarx()\n    print(f\"Results is type {type(results)}\\n{results} \")"}
