{"repo_info": {"repo_name": "multimodal_rag_for_industry", "repo_owner": "riedlerm", "repo_url": "https://github.com/riedlerm/multimodal_rag_for_industry"}}
{"type": "source_file", "path": "src/data_extraction/context_reduction.py", "content": "from transformers import OpenAIGPTTokenizer\n\n# Initialize the (static) tokenizer\ntokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\n\n\ndef get_token_count(text: str) -> int:\n    \"\"\"\n    Get the length of a text in (GPT) tokens\n    :param text: The text to count the tokens\n    :return: The number of tokens\n    \"\"\"\n    token_count = len(tokenizer.tokenize(text))\n    return token_count\n\n\ndef approximate_truncate_to_token_count(text: str, token_count: int) -> str:\n    \"\"\"\n    Get text approximately in the token count length\n    :param token_count: Length of text in tokens. The returned text will have this size +- 10%\n    :param text: The text to count the tokens\n    :return: The number of tokens\n    \"\"\"\n    # First approximation\n    token_count = int(token_count * 0.95)\n    char_length = int(token_count * 10)\n    # maximum iterations to 10 to avoid endless loop\n    for i in range(0, 10):\n        truncated_text = text[:char_length]\n        current_token_count = get_token_count(truncated_text)\n        # Break if variance < 5%\n        if token_count * 1.05 > current_token_count > token_count * 0.95:\n            break\n        # Calculate correction factor; avoid division by zero\n        correction_factor = token_count / max(current_token_count, 1)\n        char_length = int(char_length * correction_factor)\n\n    return truncated_text\n"}
{"type": "source_file", "path": "src/evaluation/evaluators/base_evaluator.py", "content": "import os\nfrom abc import abstractmethod\nfrom langchain.chains.transform import TransformChain\nfrom langchain_core.output_parsers import JsonOutputParser\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_core.runnables import RunnableLambda\nfrom langchain_openai import AzureChatOpenAI\nfrom langchain.output_parsers import BooleanOutputParser, OutputFixingParser\nfrom transformers import LlavaNextForConditionalGeneration\nfrom evaluation.evaluators.evaluator_interface import EvaluatorInterface\nfrom utils.azure_config import get_azure_config\nfrom utils.model_loading_and_prompting.llava import llava_call\n\n\nclass EvaluationResult(BaseModel):\n    \"\"\"The result of an evaluation for a given metric\"\"\"\n\n    grade: str = Field(description=\"the grade after evaluating the metric (YES or NO)\")\n    reason: str = Field(description=\"The reasoning behind the grading decision\")\n\n\nclass BaseEvaluator(EvaluatorInterface):\n    \"\"\"  \n    A base class for an LLM evaluator.\n  \n    Attributes: \n        model (str): The model to be used for evaluation.\n        tokenizer (LlavaNextProcessor or PreTrainedTokenizerFast): Tokenizer used for tokenization. Can be None.\n        model_type (AzureChatOpenAI or LlavaNextForConditionalGeneration): Type of the model to use for evaluation.\n        json_parser (JsonOutputParser): Parser used to parse evaluation results to a json object.\n        boolean_parser (BooleanOutputParser): Parser used to parse the assigned grade to a boolean value.\n        check_grade_chain (TransformChain): Applies the transformation from the LLM output for the grade to a boolean value.\n        fix_format_parser (OutputFixingParser): Parser used to fix misformatted json output of an LLM.\n    \"\"\"\n    def __init__(self, model, tokenizer=None, **kwargs):\n        \"\"\"  \n        Initializes the BaseEvaluator object.\n  \n        :param model: The model to be used for evaluation.\n        :param tokenizer: The tokenizer to be used for tokenization. Can be None.\n        \n        Keyword Args:\n            user_query (str): The user query\n            generated_answer (str): The answer produced by the model\n            reference_answer (str): The ground truth answer\n            context (str): The texts retrieved by the retrieval system\n            image (str): The image retrieved by the retrieval system\n        \"\"\"\n        self.model = model\n        self.model_type = type(self.model)\n        self.json_parser = JsonOutputParser(pydantic_object=EvaluationResult)\n        self.boolean_parser = BooleanOutputParser()\n        self.kwargs = kwargs\n        self.check_grade_chain = TransformChain(\n            input_variables=[\"grade\", \"reason\"],\n            output_variables=[\"grade\", \"reason\"],\n            transform=self.get_numeric_score\n        )\n        \n        # if a tokenizer is specified, the Evaluator is initialized for evaluation with LLaVA, otherwise GPT4v\n        if tokenizer:\n            self.tokenizer = tokenizer\n            self.config = get_azure_config()\n            gpt4v_config = self.config['gpt4']\n            fixing_llm = AzureChatOpenAI(\n                openai_api_version=gpt4v_config[\"openai_api_version\"],\n                azure_endpoint=gpt4v_config[\"openai_endpoint\"],\n                azure_deployment=gpt4v_config[\"deployment_name\"],\n                model=gpt4v_config[\"model_version\"],\n                api_key=os.environ.get(\"GPT4V_API_KEY\"),\n                max_tokens=500\n            )\n        \n            self.fix_format_parser = OutputFixingParser.from_llm(parser=self.json_parser, llm=fixing_llm)\n        \n        else:\n            self.tokenizer = None\n            \n            \n    def call_llava(self, inputs: dict) -> str:\n        \n        prompt = inputs['prompt']\n        image = inputs.get('image', None)\n        ans = llava_call(prompt, self.model, self.tokenizer, device=\"cuda\", image=image)\n        return ans\n        \n\n    def get_numeric_score(self, inputs: str) -> dict:\n        \"\"\"\n        Checks that the obtained grade (YES or NO) can be parsed to a boolean and sets the grade to its integer value (0 ur 1)\n        \"\"\"\n        inputs[\"grade\"] = int(self.boolean_parser.parse(inputs[\"grade\"]))\n        return inputs\n\n    def run_evaluation(self) -> dict:\n        \"\"\"  \n        Performs evaluation for one output of a RAG system.\n        Creates an evaluation chain that constructs the prompt, calls the model, fixes possible \n        json formatting errors and checks the validity of the assigned grade.\n\n        :return: A json object with a grade (0 or 1) and a reason for the grading as string.\n        \"\"\" \n        if self.tokenizer:\n            chain = RunnableLambda(self.get_prompt) | RunnableLambda(self.call_llava) | self.fix_format_parser | self.check_grade_chain\n        else:\n            # GPT4v chain\n            chain = RunnableLambda(self.get_prompt) | self.model | self.json_parser | self.check_grade_chain\n            # chain = RunnableLambda(self.get_prompt) | self.model | self.fix_format_parser | self.check_grade_chain\n        result = chain.invoke(self.kwargs)\n\n        return result\n\n    @abstractmethod\n    def get_prompt(self, inputs: dict):\n        \"\"\"\n        Construct the prompt for evaluation based on a dictionary containing required input arguments.\n        \"\"\"\n        pass\n"}
{"type": "source_file", "path": "src/question_answering/baseline/qa_chain.py", "content": "from langchain_core.messages import HumanMessage\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnableLambda\nfrom langchain_openai import AzureChatOpenAI\nfrom utils.model_loading_and_prompting.llava import load_llava_model, llava_call\nfrom utils.azure_config import get_azure_config\n\n\nclass QAChain:\n    def __init__(self, model_type):\n        \"\"\"\n        Question Answering Chain: Baseline without RAG (direct model prompting without additional context)\n        The steps are as follows:\n        1. Create a QA prompt using the question and an instruction to answer the question.\n        2. Call the LLM and prompt it with the created prompt\n        3. Obtain the generated answer from the model\n        \n        :param model: The type of model used for answer generation.\n        \"\"\"\n        config = get_azure_config()\n\n        if model_type in config:\n            print(\"Using Azure model\")\n            azure_llm_config = config[model_type]\n            self.model = AzureChatOpenAI(\n                openai_api_version=azure_llm_config[\"openai_api_version\"],\n                azure_endpoint=azure_llm_config[\"openai_endpoint\"],\n                azure_deployment=azure_llm_config[\"deployment_name\"],\n                model=azure_llm_config[\"model_version\"],\n                api_key=azure_llm_config[\"openai_api_key\"],\n                max_tokens=400)\n            self.tokenizer = None\n            \n        else:\n            print(\"Using LLaVA model\")\n            self.model, self.tokenizer = load_llava_model(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n        \n        # here we don't have a retriever since we prompt the model directly, without any additional context\n        self.chain = (self.prompt_func | RunnableLambda(self.call_model) | StrOutputParser())\n        \n        \n    def run(self, inputs):\n        return self.chain.invoke(inputs)\n        \n        \n    def call_model(self, prompt: str) -> RunnableLambda:\n        \"\"\"\n        Calls the model based on the model type.\n        \n        :return: A Langchain abstraction (RunnableLambda) to turn the model into a pipe-compatible function for the RAG chain.\n        \"\"\"\n        if self.tokenizer:\n            return RunnableLambda(self.call_llava)\n        else:\n            return RunnableLambda(self.model)\n\n        \n    def call_llava(self, inputs):\n        \n        prompt = inputs['prompt']\n        ans = llava_call(prompt, self.model, self.tokenizer, device=\"cuda\")\n        return ans\n\n\n    def prompt_func(self, data_dict):\n        \n        qa_prompt = \"\"\"You are an expert AI assistant that answers questions about manuals from the industrial domain.\\n\"\"\"\n        \n        if type(self.model) == AzureChatOpenAI:\n            prompt = self.azure_qa(data_dict, qa_prompt)\n        else:\n            prompt = self.llava_qa(data_dict, qa_prompt)\n            \n        return prompt\n    \n    \n    def llava_qa(self, inputs, qa_prompt):\n        \"\"\"\n        Constructs a prompt for question answering using the formatting required by LLaVA.\n        \"\"\"\n    \n        prompt = f\"[INST]\\n{qa_prompt}\\nUser-provided question: {inputs['question']}\\n\\n[/INST]\"\n        image = None\n        \n        return {\"prompt\": prompt, \"image\": image}\n\n        \n        \n    def azure_qa(self, data_dict, qa_prompt):\n        \"\"\"\n        Constructs a prompt for question answering using the formatting required by AzureOpenAI.\n        \"\"\"\n        \n        messages = []\n\n        # Adding the text for answer generation\n        text_message = {\n            \"type\": \"text\",\n            \"text\": (\n                f\"{qa_prompt}\\n\"\n                f\"User-provided question: {data_dict['question']}\\n\\n\"\n            ),\n        }\n        messages.append(text_message)\n        print(\"Calling model...\")\n        return [HumanMessage(content=messages)]"}
{"type": "source_file", "path": "src/data_summarization/context_summarization.py", "content": "import pandas as pd\nimport torch\nfrom langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\nfrom langchain_core.messages import HumanMessage\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_openai import AzureChatOpenAI\nfrom langchain.schema.runnable import RunnableLambda\nfrom transformers import pipeline, LlavaNextForConditionalGeneration, LlavaNextProcessor\nfrom tqdm.auto import tqdm\nfrom typing import List, Tuple\nfrom utils.azure_config import get_azure_config\nfrom utils.base64_utils.base64_utils import *\nfrom utils.model_loading_and_prompting.llava import format_prompt_with_image, llava_call\nfrom rag_env import INPUT_DATA, IMG_SUMMARIES_CACHE_DIR, TEXT_SUMMARIES_CACHE_DIR\n\n\n\nclass TextSummarizer:\n    \"\"\"  \n    A class to summarize texts using either AzureOpenAI's model or the LLama 3 8B model.\n  \n    Attributes:\n        model_type (str): Type of the model to use for summarization.\n        cache_path (str): Path to the directory where summaries will be cached as a CSV file.\n        model (AzureChatOpenAI or HuggingFacePipeline): The summarization model loaded based on `model_type`.\n        cache_file (str): The complete path to the cached CSV file containing text summaries.\n        df (pd.DataFrame): DataFrame to store and manage texts and their corresponding summaries.\n    \"\"\"    \n    \n    def __init__(self, model_type: str, cache_path: str):\n        \"\"\"  \n        Initializes the TextSummarizer object.\n  \n        :param model_type: The type of model to be used for summarization.\n        :param cache_path: The directory path where the summaries will be cached.\n        \"\"\" \n        self.model_type = model_type\n        config = get_azure_config()\n        \n        if model_type in config:\n            azure_llm_config = config[model_type]\n            self.model = AzureChatOpenAI(\n                openai_api_version=azure_llm_config[\"openai_api_version\"],\n                azure_endpoint=azure_llm_config[\"openai_endpoint\"],\n                azure_deployment=azure_llm_config[\"deployment_name\"],\n                model=azure_llm_config[\"model_version\"],\n                api_key=os.environ.get(\"GPT4V_API_KEY\"),\n                max_tokens=400)\n        else:\n            pipe = pipeline(\"text-generation\",\n                        model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n                        model_kwargs={\"torch_dtype\": torch.bfloat16},\n                        device_map=\"auto\")\n\n            self.model = HuggingFacePipeline(pipeline=pipe)\n            \n        # Load cached DataFrame if it exists\n        self.cache_file = os.path.join(cache_path, f'text_summaries_{self.model_type}.csv')\n        if os.path.exists(self.cache_file):\n            self.df = pd.read_csv(self.cache_file)\n        else:\n            # Initialize DataFrame if it doesn't exist\n            self.df = pd.DataFrame(columns=['text', 'text_summary'])\n            self.df.to_csv(self.cache_file, index=False)\n    \n    \n    def summarize(self, texts: List[str]) -> List[str]:\n        \"\"\"  \n        Generates summaries for a list of texts.\n        This method determines which model to use based on the `model_type` attribute,\n        and then calls the appropriate summarization method.\n  \n        :param texts: A list of texts to be summarized.\n        :return: A list of summarized texts.\n        \"\"\"  \n        if type(self.model) == AzureChatOpenAI:\n            text_summaries = self.summarize_azure(texts)\n        else:\n            text_summaries = self.summarize_llama(texts)\n        return text_summaries\n    \n    \n    def summarize_llama(self, texts: List[str]) -> List[str]:\n        \"\"\"  \n        Summarizes texts using the LLama 3 8B model.\n        Iterates over the list of texts, checks if a summary already exists in the cache,\n        and if not, generates a new summary using the LLama 3 8B model. Updates the cache\n        with new summaries.\n  \n        :param texts: A list of texts to be summarized.\n        :return: A list of summaries generated by the LLama model.\n        \"\"\"\n  \n        # Iterate over texts and generate summaries with a progress bar\n        with tqdm(total=len(texts), desc=\"Summarizing texts\") as pbar:\n            for i, text in enumerate(texts):\n                self.df.at[i, 'text'] = text\n\n                pbar.update(1)\n  \n                # Skip if summary already exists\n                if i < len(self.df) and pd.notna(self.df.at[i, 'text_summary']):\n                    print(f\"Summary for text {i + 1} already exists. Skipping...\")\n                    continue\n                \n                template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system_msg}\n                        <|start_header_id|>user<|end_header_id|>Text: {text}\\nSummary:\\n<|eot_id|>\n                        <|start_header_id|>assistant<|end_header_id|>\"\"\"\n\n                system = \"\"\"You are an assistant tasked with summarizing text for retrieval.\n                    These summaries will be embedded and used to retrieve the raw text elements.\n                    Give a concise summary of the text that is well optimized for retrieval.\n                    Only output the summary, no additional explanation.\\n\"\"\"\n\n                prompt = PromptTemplate.from_template(template)\n                prompt = prompt.partial(system_msg=system)\n\n                print(\"Summarizing texts...\")\n                \n                # Remove the text given to the model as input from the generation\n                strip_output = RunnableLambda(lambda output: output.split(\"<|start_header_id|>assistant<|end_header_id|>\",1)[1].strip())\n\n                # Text summary chain\n                summarize_chain = {\"text\": lambda text: text} | prompt | self.model | strip_output | StrOutputParser()\n                summary = summarize_chain.invoke({\"text\": text})\n\n                # Update DataFrame with new summary\n                self.df.at[i, 'text_summary'] = summary\n                # Cache the DataFrame after each generation\n                self.df.to_csv(self.cache_file, index=False)\n  \n        return self.df['text_summary'].tolist()\n    \n    \n    def summarize_azure(self, texts: List[str]) -> List[str]:\n        \"\"\"\n        Summarizes texts using the Azure model.\n        Iterates over the list of texts, checks if a summary already exists in the cache,\n        and if not, generates a new summary using the Azure model. Updates the cache\n        with new summaries.\n  \n        :param texts: A list of texts to be summarized.\n  \n        :return: A list of summaries generated by the Azure model.\n        \"\"\"\n        print(\"Summarizing texts with Azure\")\n  \n        # Iterate over texts and generate summaries with a progress bar\n        with tqdm(total=len(texts), desc=\"Summarizing texts\") as pbar:\n            for i, text in enumerate(texts):\n                self.df.at[i, 'text'] = text\n\n                pbar.update(1)\n  \n                # Skip if summary already exists\n                if i < len(self.df) and pd.notna(self.df.at[i, 'text_summary']):\n                    print(f\"Summary for text {i + 1} already exists. Skipping...\")\n                    continue\n  \n                # Prompt template\n                prompt = f\"\"\"You are an assistant tasked with summarizing text for retrieval.\n                These summaries will be embedded and used to retrieve the raw text element.\n                Give a concise summary of the text that is well optimized for retrieval.\n                Text: {text}\\n\"\"\"\n  \n                try:\n                    print(f\"Summarizing text {i + 1} of {len(texts)}\")\n                    if self.model_type == \"gpt4\":\n                        summary = self.model.invoke(\n                            [\n                                HumanMessage(\n                                    content=prompt\n                                )\n                            ]\n                        )\n                        print(summary.content)\n                except Exception as e:\n                    print(f\"Failed to summarize text {i}: {e}\")\n                    with open('summarization_fails.txt', 'a') as f:\n                        f.write(f\"Failed to summarize text {i}: {e}\\n\")\n                    continue\n  \n                # Update DataFrame with new summary\n                self.df.at[i, 'text_summary'] = summary.content\n                # Cache the DataFrame after each generation\n                self.df.to_csv(self.cache_file, index=False)\n  \n        return self.df['text_summary'].tolist()\n    \n\n\nclass ImageSummarizer:\n    \"\"\"\n    A class to summarize images using different models. It can encode images in base64,\n    generate textual summaries, and cache these summaries for quick retrieval.\n    \"\"\"\n    def __init__(self, model, tokenizer=None):\n        \"\"\"\n        Initializes the ImageSummarizer with a specific model and an optional tokenizer.\n          \n        :param model: The model to be used for generating image summaries. This can be an instance of either\n                      AzureChatOpenAI, LlavaNextForConditionalGeneration, or any model that supports image summarization.\n        :param tokenizer: The tokenizer to be used with the model, if necessary. This is model-dependent and optional.\n        \"\"\"\n        self.model = model\n        self.tokenizer=tokenizer\n        \n        \n    def summarize(self, image_bytes_list: List[bytes], cache_path: str) -> Tuple[List[str], List[str]]:\n        \"\"\"  \n        Generate summaries and base64 encoded strings for images. This function also checks for cached summaries\n        to avoid re-processing images. If a summary does not exist, it will generate a new one, update the cache,\n        and return the summaries along with their base64 encoded strings.\n          \n        :param image_bytes_list: A list of image bytes. Each entry in the list should be the binary content of an image file.\n        :param cache_path: The file system path where cached summaries are stored. This path is used to store summaries\n                           in a CSV file to avoid re-processing images.\n        :return: A tuple containing two lists - the first list contains the base64 encoded strings of the images,\n                 and the second list contains the textual summaries of the images.\n        \"\"\"\n        # Initialize base64 list\n        img_base64_list = []\n\n        if type(self.model) == AzureChatOpenAI:\n            model_type = \"gpt4v\"\n        else:\n            model_type = \"llava\"\n        # Load cached DataFrame if it exists\n        cache_file = os.path.join(cache_path, f'image_summaries_{model_type}.csv')\n        if os.path.exists(cache_file):\n            df = pd.read_csv(cache_file)\n        else:\n            # Initialize DataFrame if it doesn't exist\n            df = pd.DataFrame(columns=['image_summary'])\n            df.to_csv(cache_file, index=False)\n\n        # Prompt template\n        prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n                    These summaries will be embedded and used to retrieve the raw image. \\\n                    Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n\n        # Iterate over image bytes and generate base64 encoded string\n        for i, image_bytes in enumerate(image_bytes_list):\n            # Convert image bytes to base64\n            try:\n                img_base64 = encode_image_from_bytes(image_bytes)\n                img_base64_list.append(img_base64)\n            except:\n                print(f\"Failed to encode img {i}...\")\n                continue\n\n            # Skip if summary already exists\n            if i < len(df):\n                print(f\"Summary for image {i + 1} already exists. Skipping...\")\n                continue\n            try:\n                print(f\"Summarizing image {i + 1} of {len(image_bytes_list)}\")\n                if model_type == \"gpt4v\":  \n                    summary_content = self.summarize_image_azure(img_base64, prompt)\n                else:\n                    summary_content = self.summarize_image_llava(img_base64, prompt)\n            except:\n                print(f\"Failed to summarize img {i}\")\n                with open('summarization_fails.txt', 'a') as f:\n                    f.write(f\"Failed to summarize img {i}\" + '\\n')\n                continue\n\n            # Update DataFrame with new summary\n            df.at[i, 'image_summary'] = summary_content\n\n            # Cache the DataFrame after each generation\n            df.to_csv(cache_file, index=False)\n\n        return img_base64_list, df['image_summary'].tolist()\n        \n        \n    def summarize_image_llava(self, img_base64: str, prompt: str) -> str:\n        \n        llava_prompt = format_prompt_with_image(prompt)\n        image = decode_image_to_bytes(img_base64)\n        image = Image.open(io.BytesIO(image))\n        img_summary = llava_call(llava_prompt, self.model, self.tokenizer, device=\"cuda\", image=image)\n\n        return img_summary\n\n\n    def summarize_image_azure(self, img_base64: str, prompt: str) -> str:\n\n        msg = self.model.invoke(\n            [\n                HumanMessage(\n                    content=[\n                        {\"type\": \"text\", \"text\": prompt},\n                        {\n                            \"type\": \"image_url\",\n                            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n                        },\n                    ]\n                )\n            ]\n        )\n        print(msg.content)\n\n        return msg.content\n\n\nif __name__ == \"__main__\":\n\n    # text summarization\n    text_summarizer = TextSummarizer(model_type='llama3', cache_path=TEXT_SUMMARIES_CACHE_DIR)\n    df = pd.read_parquet(INPUT_DATA)\n    texts = list(df.drop_duplicates(subset='text')['text'])\n    text_summarizer.summarize(texts)\n\n    # image summarization\n    df = pd.read_parquet(INPUT_DATA)\n    filtered_df = df[df['has_image'] == True]\n    images = list(filtered_df[\"image_bytes\"])\n    \n    model_id = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n    processor = LlavaNextProcessor.from_pretrained(model_id)\n    model = LlavaNextForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\")\n    \n    image_summarizer = ImageSummarizer(model, processor)\n    image_summarizer.summarize(images, cache_path=IMG_SUMMARIES_CACHE_DIR)\n"}
{"type": "source_file", "path": "src/question_answering/correct_context_prompting/run_image_only_correct_context_qa.py", "content": "import os\nimport pandas as pd\nfrom utils.base64_utils.base64_utils import encode_image_from_bytes\nfrom correct_context_qa_chain import CorrectContextQAChain\nfrom rag_env import INPUT_DATA, MODEL_TYPE, RAG_OUTPUT_DIR, REFERENCE_QA\n\ndef write_to_df(df, user_query, reference_answer, generated_answer, context, image, output_file):\n    df.loc[len(df)] = [user_query, reference_answer, generated_answer, context, image]\n    df.to_json(output_file, orient=\"records\", indent=2)\n    \n    \ndef process_dataframe(input_df, qa_df, pipeline, output_file, output_df=None):\n    if not output_df:\n        columns = ['user_query', 'reference_answer', 'generated_answer', 'context', 'image']\n        output_df= pd.DataFrame(columns=columns)\n    for index, row in input_df.iterrows():\n        print(f\"Processing query no. {index+1}...\")\n        question = qa_df[\"question\"][index]\n        print(\"QUESTION:\", question)\n        reference_answer = qa_df[\"reference_answer\"][index]\n        print(\"REFERENCE:\", reference_answer)\n        context = input_df[\"text\"][index]\n        img_bytes = input_df[\"image_bytes\"][index]\n        image = encode_image_from_bytes(img_bytes)\n        inputs = dict()\n        inputs[\"context\"] = dict()\n        inputs[\"context\"][\"images\"] = [image]\n        inputs[\"context\"][\"texts\"] = []\n        inputs[\"question\"] = question\n        generated_answer = pipeline.run(inputs)\n        print(\"GENERATED ANSWER:\", generated_answer)\n        write_to_df(output_df, question, reference_answer, generated_answer, context, image, output_file)\n    return output_df\n\n\nif __name__ == \"__main__\":\n\n    chain = CorrectContextQAChain(model_type=MODEL_TYPE)\n\n    output_file = os.path.join(RAG_OUTPUT_DIR, f\"rag_output_{MODEL_TYPE}_image_only_correct_context.json\")\n    # dataframe containing the questions and reference answers\n    qa_df = pd.read_excel(REFERENCE_QA)\n    # dataframe containing the extracted texts and images for each qa pair\n    input_df = pd.read_parquet(INPUT_DATA)\n    process_dataframe(input_df, qa_df, chain, output_file)"}
{"type": "source_file", "path": "src/evaluation/evaluators/evaluator_interface.py", "content": "from abc import ABC, abstractmethod\n\n\nclass EvaluatorInterface(ABC):\n    \"\"\"\n    Abstract base class for an LLM Evaluator.\n    \"\"\"\n\n    @abstractmethod\n    def run_evaluation(self) -> dict:\n        \"\"\"\n        Perform evaluation and return results as a dictionary.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_prompt(self, inputs: dict):\n        \"\"\"\n        Construct the prompt for evaluation.\n        \"\"\"\n        pass\n"}
{"type": "source_file", "path": "src/question_answering/correct_context_prompting/run_multimodal_correct_context_qa.py", "content": "import os\nimport pandas as pd\nfrom utils.base64_utils.base64_utils import encode_image_from_bytes\nfrom correct_context_qa_chain import CorrectContextQAChain\nfrom rag_env import INPUT_DATA, MODEL_TYPE, RAG_OUTPUT_DIR, REFERENCE_QA\n\ndef write_to_df(df, user_query, reference_answer, generated_answer, context, image, output_file):\n    df.loc[len(df)] = [user_query, reference_answer, generated_answer, context, image]\n    df.to_json(output_file, orient=\"records\", indent=2)\n    \n    \ndef process_dataframe(input_df, qa_df, pipeline, output_file, output_df=None):\n    if not output_df:\n        columns = ['user_query', 'reference_answer', 'generated_answer', 'context', 'image']\n        output_df= pd.DataFrame(columns=columns)\n    for index, row in input_df.iterrows():\n        print(f\"Processing query no. {index+1}...\")\n        question = qa_df[\"question\"][index]\n        print(\"QUESTION:\", question)\n        reference_answer = qa_df[\"reference_answer\"][index]\n        print(\"REFERENCE:\", reference_answer)\n        context = input_df[\"text\"][index]\n        img_bytes = input_df[\"image_bytes\"][index]\n        image = encode_image_from_bytes(img_bytes)\n        inputs = dict()\n        inputs[\"context\"] = dict()\n        inputs[\"context\"][\"images\"] = [image]\n        inputs[\"context\"][\"texts\"] = [context]\n        inputs[\"question\"] = question\n        generated_answer = pipeline.run(inputs)\n        print(\"GENERATED ANSWER:\", generated_answer)\n        write_to_df(output_df, question, reference_answer, generated_answer, context, image, output_file)\n    return output_df\n\n\nif __name__ == \"__main__\":\n\n    chain = CorrectContextQAChain(model_type=MODEL_TYPE)\n\n    output_file = os.path.join(RAG_OUTPUT_DIR, f\"rag_output_{MODEL_TYPE}_multimodal_correct_context.json\")\n    # dataframe containing the questions and reference answers\n    qa_df = pd.read_excel(REFERENCE_QA)\n    # dataframe containing the extracted texts and images for each qa pair\n    input_df = pd.read_parquet(INPUT_DATA)\n    process_dataframe(input_df, qa_df, chain, output_file)"}
{"type": "source_file", "path": "src/question_answering/correct_context_prompting/run_text_only_correct_context_qa.py", "content": "import os\nimport pandas as pd\nfrom correct_context_qa_chain import CorrectContextQAChain\nfrom rag_env import INPUT_DATA, MODEL_TYPE, RAG_OUTPUT_DIR, REFERENCE_QA\n\n\ndef write_to_df(df, user_query, reference_answer, generated_answer, context, image, output_file):\n    df.loc[len(df)] = [user_query, reference_answer, generated_answer, context, image]\n    df.to_json(output_file, orient=\"records\", indent=2)\n\n\ndef process_dataframe(input_df, qa_df, pipeline, output_file, output_df=None):\n    if not output_df:\n        columns = ['user_query', 'reference_answer', 'generated_answer', 'context', 'image']\n        output_df= pd.DataFrame(columns=columns)\n    for index, row in input_df.iterrows():\n        print(f\"Processing query no. {index+1}...\")\n        question = qa_df[\"question\"][index]\n        print(\"QUESTION:\", question)\n        reference_answer = qa_df[\"reference_answer\"][index]\n        print(\"REFERENCE:\", reference_answer)\n        context = input_df[\"text\"][index]\n        inputs = dict()\n        inputs[\"context\"] = dict()\n        inputs[\"context\"][\"images\"] = []\n        inputs[\"context\"][\"texts\"] = [context]\n        inputs[\"question\"] = question\n        generated_answer = pipeline.run(inputs)\n        print(\"GENERATED ANSWER:\", generated_answer)\n        write_to_df(output_df, question, reference_answer, generated_answer, context, None, output_file)\n    return output_df\n\n\nif __name__ == \"__main__\":\n\n    chain = CorrectContextQAChain(model_type=MODEL_TYPE)\n\n    output_file = os.path.join(RAG_OUTPUT_DIR, f\"rag_output_{MODEL_TYPE}_text_only_correct_context.json\")\n    # dataframe containing the questions and reference answers\n    qa_df = pd.read_excel(REFERENCE_QA)\n    # dataframe containing the extracted texts and images for each qa pair\n    input_df = pd.read_parquet(INPUT_DATA)\n    process_dataframe(input_df, qa_df, chain, output_file)"}
{"type": "source_file", "path": "src/question_answering/baseline/run_baseline.py", "content": "import os\nimport pandas as pd\nfrom qa_chain import QAChain\nfrom rag_env import MODEL_TYPE, RAG_OUTPUT_DIR, REFERENCE_QA\n    \n    \ndef write_to_df(df, user_query, reference_answer, generated_answer, context, image, output_file):\n    df.loc[len(df)] = [user_query, reference_answer, generated_answer, context, image]\n    df.to_json(output_file, orient=\"records\", indent=2)\n    \n    \ndef process_dataframe(input_df, pipeline, output_file, output_df=None):\n    if not output_df:\n        columns = ['user_query', 'reference_answer', 'generated_answer', 'context', 'image']\n        output_df= pd.DataFrame(columns=columns)\n    for index, row in input_df.iterrows():\n        print(f\"Processing query no. {index+1}...\")\n        user_query = input_df[\"question\"][index]\n        print(\"USER QUERY:\\n\", user_query)\n        reference_answer = input_df[\"reference_answer\"][index]\n        print(\"REFERENCE ANSWER:\", reference_answer)\n        context = None\n        image = None\n        inputs = dict()\n        inputs[\"question\"] = user_query\n        generated_answer = pipeline.run(inputs)       \n        print(\"GENERATED ANSWER:\\n\", generated_answer)\n        write_to_df(output_df, user_query, reference_answer, generated_answer, context, image, output_file)\n    return output_df\n\n\nif __name__ == \"__main__\":\n\n    chain = QAChain(model_type=MODEL_TYPE)\n\n    output_file = os.path.join(RAG_OUTPUT_DIR, f\"rag_output_{MODEL_TYPE}_baseline.json\")\n    df = pd.read_excel(REFERENCE_QA)\n    process_dataframe(df, chain, MODEL_TYPE)\n"}
{"type": "source_file", "path": "src/evaluation/evaluate_rag_pipeline.py", "content": "import os\nimport pandas as pd\nfrom typing import List\nfrom evaluation_module import EvaluationModule\n\n\nAZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n\nMETRICS = ['Answer Correctness', 'Answer Relevancy']\nIMAGE_METRICS = ['Image Faithfulness', 'Image Context Relevancy']\nTEXT_METRICS = ['Text Faithfulness', 'Text Context Relevancy']\nAGGREGATED_METRICS = ['Faithfulness', 'Context Relevancy']\n\n\n\ndef evaluate_row(metrics: List[str], index: int, context: str, image: str, user_query: str, generated_answer: str,\n                 reference_answer: str, evaluator: str, scores_df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Evaluates the results for one output of a RAG system.\n\n    :param metrics: A list of metrics to be evaluated.\n    :param index: Index of the current row in the dataframe.\n    :param context: Textual context retrieved by the retrieval system.\n    :param image: Base64 encoded string of an image retrieved by the retrieval system.\n    :param user_query: Question to be answered by the RAG system.\n    :param generated_answer: Answer generated by the RAG system.\n    :param reference_answer: Gold standard answer to the user query.\n    :param evaluator: Model to be used as evaluator (should be gpt4_vision or llava).\n    :param scores_df: A dataframe containing the evaluation results.\n    \n    :return: A dataframe containing the evaluation results.\n    \"\"\"\n    results = evaluator.evaluate(metrics=metrics,\n                                query=user_query,\n                                context=context,\n                                image=image,\n                                generated_answer=generated_answer,\n                                reference_answer=reference_answer)\n    print(results)\n    for k, v in results.items():\n        scores_df.at[index, f\"{k} grade\"] = v[\"grade\"]\n        scores_df.at[index, f\"{k} reason\"] = v[\"reason\"]\n\n    return scores_df\n\n\ndef handle_no_data(index, data_type: str, scores_df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Defines behaviour for cases where no text or image is provided as context.\n    In this case the grade will be None and the reason is, that no data has been provided.\n\n    :param metrics: A string indicating the missing data type (text or image).\n    :param scores_df: A dataframe containing the evaluation results.\n    \n    :return: A dataframe containing the evaluation results.\n    \"\"\"  \n    scores_df.at[index, f\"{data_type} Faithfulness grade\"] = None\n    scores_df.at[index, f\"{data_type} Faithfulness reason\"] = f\"No {data_type} provided\"\n    scores_df.at[index, f\"{data_type} Context Relevancy grade\"] = None\n    scores_df.at[index, f\"{data_type} Context Relevancy reason\"] = f\"No {data_type} provided\"\n    return scores_df\n\n\ndef evaluate_dataframe(input_df: pd.DataFrame, evaluator: str, output_file: str) -> pd.DataFrame:\n    \"\"\"\n    Evaluates the outputs of a RAG system.\n\n    :param input_df: Dataframe containing the outputs of a RAG system.\n    :param evaluator: Model to be used as evaluator (should be gpt4_vision, or llava).\n    :param output_file: json file where the evaluation results are stored.\n    \n    :return: A dataframe containing the evaluation results.\n    \"\"\"\n    scores_df = pd.DataFrame()\n    for index, row in input_df.iterrows():\n        print(f\"Evaluating query no. {index+1}...\")\n        user_query = input_df[\"user_query\"][index]\n        reference_answer = input_df[\"reference_answer\"][index]\n        generated_answer = input_df[\"generated_answer\"][index]\n        context = input_df[\"context\"][index]\n        image = input_df[\"image\"][index][0] if input_df[\"image\"][index] else []\n        metrics = METRICS.copy()\n        if not image:\n            scores_df = handle_no_data(index, \"Image\", scores_df)\n        else:\n            metrics.extend(IMAGE_METRICS)\n        if not context:\n            scores_df = handle_no_data(index, \"Text\", scores_df)\n        else:\n            metrics.extend(TEXT_METRICS)\n        scores_df = evaluate_row(metrics, index, context, image, user_query, generated_answer, reference_answer, evaluator, scores_df)\n                \n        for metric in AGGREGATED_METRICS:\n            # calculating overall faithfulness and context relevancy for image and text combined\n            img_metric = scores_df.at[index, f\"Image {metric} grade\"]\n            text_metric = scores_df.at[index, f\"Text {metric} grade\"]\n            \n            df_tmp = pd.DataFrame()\n            df_tmp[metric] = [img_metric, text_metric]\n            \n            grade = df_tmp[metric].mean()\n            scores_df.at[index, f\"{metric} grade\"] = grade\n        \n        scores_df.to_json(output_file, orient=\"records\", indent=2)\n    return scores_df\n  \n  \ndef calculate_and_print_averages(scores_df: pd.DataFrame):\n    \"\"\"\n    Averages the scores for each metric over the entire dataset and prints them.\n    :param scores_df: Dataframe containing the evaluation results.\n    \"\"\"\n    average_dict = {}\n    for grade in METRICS + IMAGE_METRICS + TEXT_METRICS + AGGREGATED_METRICS:\n        average_dict[grade] = scores_df[f'{grade} grade'].mean()\n        print(f\"{grade.capitalize()}: {average_dict[grade]}\")\n        \n\n\nif __name__ == \"__main__\":\n    generator_model = \"llava\"   # model that was used as generator in the rag pipeline to be evaluated\n    evaluator_model = \"llava\"   # choose among llava and gpt4_vision\n\n    # json file containing the results of a rag pipeline\n    rag_output_file = rf\"../../sample_data/rag_outputs/rag_output_img_summaries_{generator_model}.json\"\n    # file for saving evaluaton results\n    evaluation_output_file = rf\"../../sample_data/rag_evaluation_results/evaluation_{generator_model}_generator_{evaluator_model}_evaluator.json\"\n\n    evaluator = EvaluationModule(evaluator_model)\n\n    input_df = pd.read_json(rag_output_file)\n    scores_df = evaluate_dataframe(input_df, evaluator, evaluation_output_file)\n    calculate_and_print_averages(scores_df)"}
{"type": "source_file", "path": "src/question_answering/correct_context_prompting/correct_context_qa_chain.py", "content": "\nfrom collections import defaultdict\nfrom langchain_core.messages import HumanMessage\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnableLambda\nfrom langchain_openai import AzureChatOpenAI\nfrom utils.base64_utils.base64_utils import *\nfrom utils.model_loading_and_prompting.llava import llava_call, load_llava_model\nfrom utils.azure_config import get_azure_config\n\n\nclass CorrectContextQAChain:\n    def __init__(self, model_type):\n        \"\"\"\n        Multi-modal RAG chain\n        \"\"\"\n        \n        config = get_azure_config()\n\n        if model_type in config:\n            print(\"Using Azure model\")\n            azure_llm_config = config[model_type]\n            self.model = AzureChatOpenAI(\n                openai_api_version=azure_llm_config[\"openai_api_version\"],\n                azure_endpoint=azure_llm_config[\"openai_endpoint\"],\n                azure_deployment=azure_llm_config[\"deployment_name\"],\n                model=azure_llm_config[\"model_version\"],\n                api_key=azure_llm_config[\"openai_api_key\"],\n                max_tokens=400)\n            self.tokenizer = None\n            \n        else:\n            print(\"Using LLaVA model\")\n            self.model, self.tokenizer = load_llava_model(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n        \n        # here we don't have a retriever since we already know which are the relevant documents and pass them directly when invoking the chain\n        self.chain = (\n                self.img_prompt_func | RunnableLambda(self.call_model) | StrOutputParser()\n        )\n\n        \n    def run(self, inputs):\n        return self.chain.invoke(inputs)\n        \n\n    def call_model(self, prompt: str) -> RunnableLambda:\n        \"\"\"\n        Calls the model based on the model type.\n        \n        :return: A Langchain abstraction (RunnableLambda) to turn the model into a pipe-compatible function for the RAG chain.\n        \"\"\"\n        if self.tokenizer:\n            return RunnableLambda(self.call_llava)\n        else:\n            return RunnableLambda(self.model)\n\n        \n    def call_llava(self, inputs: dict) -> str:\n        \n        prompt = inputs['prompt']\n        image = inputs.get('image', None)\n        ans = llava_call(prompt, self.model, self.tokenizer, device=\"cuda\", image=image)\n        return ans\n    \n\n    def split_image_text_types(self, docs):\n        \"\"\"\n        Split base64-encoded images and texts.\n        \n        :return: A dictionary with separate entries for texts and base64-encoded images.\n        \"\"\"\n        b64_images = []\n        texts = []\n        \n        for doc in docs[\"context\"][\"images\"]:\n            if looks_like_base64(doc) and is_image_data(doc):\n                doc = resize_base64_image(doc, size=(1300, 600))\n                b64_images.append(doc)\n            else:\n                texts.append(doc)\n                \n        self.retrieved_docs = defaultdict(list)\n        self.retrieved_docs['images'] = b64_images\n        self.retrieved_docs['texts'] = docs[\"context\"][\"texts\"]\n\n        return self.retrieved_docs\n\n\n    def img_prompt_func(self, data_dict: dict) -> dict:\n        \"\"\"\n        Constructs a dictionary containing the model-specific prompt and an image.\n        \"\"\"\n        \n        qa_prompt = \"\"\"You are an expert AI assistant that answers questions about manuals from the industrial domain.\\n\n        You will be given some context consisiting of text and/or image(s) that can be photos, screenshots, graphs, charts and other.\\n\n        Use this information from both text and image (if present) to provide an answer to the user question.\\n\n        Avoid expressions like: 'according to the text/image provided' and similar, and just answer the question directly.\"\"\"\n        \n        if type(self.model) == AzureChatOpenAI:\n            prompt_dict = self.azure_qa(data_dict, qa_prompt)\n        else:\n            prompt_dict = self.llava_qa(data_dict, qa_prompt)\n            \n        return (prompt_dict)\n    \n    \n    def llava_qa(self, inputs: dict, qa_prompt: str) -> dict:\n        \"\"\"\n        Constructs a prompt for question answering using the formatting required by LLaVA.\n        \"\"\"\n        \n        formatted_texts = \"\\n\".join(inputs[\"context\"][\"texts\"])\n    \n        prompt = f\"[INST]{'<image>' if inputs['context']['images'] else ' '}\\n{qa_prompt}\\nUser-provided question: {inputs['question']}\\n\\nText:\\n{formatted_texts}[/INST]\"\n       \n        if inputs['context']['images']:\n             # pass always only the first image as llava cannot handle multiple images\n            image = inputs['context']['images'][0]\n            image = decode_image_to_bytes(image)\n            image = Image.open(io.BytesIO(image))\n        else:\n            image = None  \n        \n        return {\"prompt\": prompt, \"image\": image}\n    \n        \n    def azure_qa(self, data_dict, qa_prompt):\n        \n        formatted_texts = data_dict[\"context\"][\"texts\"]\n        messages = []\n\n        # Adding image(s) to the messages if present\n        if data_dict[\"context\"][\"images\"]:\n            # if multiple images should be added\n            # for image in data_dict[\"context\"][\"images\"]:\n            # add only the first image since llava only supports 1 image\n            image = data_dict[\"context\"][\"images\"][0]\n            image_message = {\n                \"type\": \"image_url\",\n                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n            }\n            messages.append(image_message)\n\n        # Adding the text for answer generation\n        text_message = {\n            \"type\": \"text\",\n            \"text\": (\n                f\"{qa_prompt}\\n\"\n                f\"User-provided question: {data_dict['question']}\\n\\n\"\n                \"Text:\\n\"\n                f\"{formatted_texts}\"\n            ),\n        }\n        messages.append(text_message)\n        return [HumanMessage(content=messages)]\n"}
{"type": "source_file", "path": "src/evaluation/evaluators/evaluators_llava.py", "content": "import io\nfrom evaluation.evaluators.base_evaluator import BaseEvaluator\nfrom PIL import Image\nfrom utils.base64_utils.base64_utils import decode_image_to_bytes\n\n\"\"\"\nContains evaluator classes for specific metrics with LLaVA as evaluator model.\nThe required input arguments vary depending on the metric to be evaluated.\nEach Evaluator inherits from the BaseEvaluator and implements the get_prompt method for prompt construction.\nEach prompt instructs the model to evaluate the desired metric, it provides a description of the metric,\nit provides the required input arguments to the model, and it describes the output format required.\n\"\"\"\n    \n\nclass ImageContextRelevancyEvaluator(BaseEvaluator):\n    def __init__(self, user_query: str, image: str, model, tokenizer) -> dict:\n        super().__init__(model=model, user_query=user_query, image=image, tokenizer=tokenizer)\n        \n    def get_prompt(self, inputs: dict):\n        \n        if not inputs[\"image\"]:\n            return None\n        \n        json_format = \"{grade: '', 'reason': ''}\"\n        \n        prompt = f\"\"\"\n            [INST] {'<image>' if inputs['image'] else ' '}\\n\n            Evaluate the following metric by comparing the user query with the provided image:\\n\n            image_context_relevancy: Is the content of the image relevant to the user\\'s query \"{inputs[\"user_query\"]}\", i.e. can it contribute to answer the query? (YES or NO)\\n\n            Write out in a step by step manner your reasoning to be sure that your conclusion is correct by filling out the following JSON format with the grade and a concise reason behind the grade:\n            {json_format}\n            Output the reason as a string, not as a list.\n            The only allowed grades are YES or NO. [/INST]\n            \"\"\"\n        \n        image = decode_image_to_bytes(inputs['image'])\n        image = Image.open(io.BytesIO(image))\n        \n        return {\"prompt\": prompt, \"image\": image}\n\n\nclass ImageFaithfulnessEvaluator(BaseEvaluator):\n    def __init__(self, user_query: str, generated_answer: str, image: str, model, tokenizer) -> dict:\n        super().__init__(user_query=user_query, generated_answer=generated_answer, image=image, model=model, tokenizer=tokenizer)\n\n    def get_prompt(self, inputs: dict):\n        \n        if not inputs[\"image\"]:\n            return None\n        \n        json_format = \"\\{grade: '', 'reason': ''}\"\n        \n        prompt = f\"\"\"\n            [INST] {'<image>' if inputs['image'] else ' '}\\n\n            Evaluate the following metric by comparing the answer with the provided image:\\n\n            image_faithfulness: Is the answer faithful to the content of the image, i.e. does it factually align with the image? (YES or NO)\\n\n            ANSWER: \"{inputs[\"generated_answer\"]}\"\\n\\\n            Write out in a step by step manner your reasoning to be sure that your conclusion is correct by filling out the following JSON format with the grade and a concise reason behind the grade:\n            {json_format}\n            Output the reason as a string, not as a list.\n            The only allowed grades are YES or NO. [/INST]\n            \"\"\"\n\n        image = decode_image_to_bytes(inputs['image'])\n        image = Image.open(io.BytesIO(image))\n        \n        return {\"prompt\": prompt, \"image\": image}\n    \n    \nclass ContextRelevancyEvaluator(BaseEvaluator):\n    def __init__(self, user_query: str, context: str, image: str, model, tokenizer) -> dict:\n        super().__init__(model=model, user_query=user_query, context=context, image=image, tokenizer=tokenizer)\n        \n    def get_prompt(self, inputs: dict):\n        \n        json_format = \"\\{grade: '', 'reason': ''}\"\n        \n        prompt = f\"\"\"\n            [INST] Evaluate the following metric by comparing the user query with the provided image and text:\\n\\n\n            context_relevancy: Is the context provided (as text and/or image) relevant to the user\\'s query? (YES or NO)\\n\n            USER QUERY: {inputs[\"user_query\"]}\\n\n            {\"TEXT: \" + inputs[\"context\"] if inputs[\"context\"] else \"\"}\n            Write out in a step by step manner your reasoning to be sure that your conclusion is correct by filling out the following JSON format with the grade and a concise reason behind the grade:\n            {json_format}\n            Output the reason as a string, not as a list.\n            The only allowed grades are YES or NO. [/INST]\n            \"\"\"\n        \n        return {\"prompt\": prompt}\n\n\nclass TextContextRelevancyEvaluator(BaseEvaluator):\n    def __init__(self, user_query: str, context: str, model, tokenizer) -> dict:\n        super().__init__(model=model, user_query=user_query, context=context, tokenizer=tokenizer)\n        \n        \n    def get_prompt(self, inputs: dict):\n        \n        json_format = \"\\{grade: '', 'reason': ''}\"\n        \n        prompt = f\"\"\"\n            [INST] Evaluate the following metric:\\n\n            text_context_relevancy: Is the text \"{inputs[\"context\"]}\" relevant to the user\\'s query \"{inputs[\"user_query\"]}\"? (YES or NO)\\n\n            Write out in a step by step manner your reasoning to be sure that your conclusion is correct by filling out the following JSON format with the grade and a concise reason behind the grade:\n            {json_format}\n            Output the reason as a string, not as a list.\n            The only allowed grades are YES or NO. [/INST]\n            \"\"\"\n        \n        return {\"prompt\": prompt}\n\n\nclass AnswerRelevancyEvaluator(BaseEvaluator):\n    def __init__(self, user_query: str, generated_answer: str, model, tokenizer) -> dict:\n        super().__init__(model=model, user_query=user_query, generated_answer=generated_answer, tokenizer=tokenizer)\n        \n    \n    def get_prompt(self, inputs: dict):\n        \n        json_format = \"\\{grade: '', 'reason': ''}\"\n        \n        prompt = f\"\"\"\n            [INST] Evaluate the following metric:\\n\n            answer_relevancy: Is the answer \"{inputs[\"generated_answer\"]}\" relevant to the user\\'s query \"{inputs[\"user_query\"]}\"? (YES or NO)\\n\n            Write out in a step by step manner your reasoning to be sure that your conclusion is correct by filling out the following JSON format with the grade and a concise reason behind the grade:\n            {json_format}\n            Output the reason as a string, not as a list.\n            The only allowed grades are YES or NO. [/INST]\n            \"\"\"\n        \n        return {\"prompt\": prompt}\n\n\nclass AnswerCorrectnessEvaluator(BaseEvaluator):\n    def __init__(self, user_query: str, generated_answer: str, reference_answer: str, model, tokenizer) -> dict:\n        super().__init__(model=model, user_query=user_query, reference_answer=reference_answer,\n                         generated_answer=generated_answer, tokenizer=tokenizer)\n         \n    def get_prompt(self, inputs: dict):\n        \n        json_format = \"\\{grade: '', 'reason': ''}\"\n        \n        prompt = f\"\"\"\n            [INST] You are given a question, the correct reference answer, and the student\\'s answer. \\\n            You are asked to grade the student\\'s answer as either correct or incorrect, based on the reference answer. \\\n            Ignore differences in punctuation and phrasing between the student answer and true answer. \\\n            It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements.\\\n            USER QUERY: \"{inputs[\"user_query\"]}\"\\n\\\n            REFERENCE ANSWER: \"{inputs[\"reference_answer\"]}\"\\n\\\n            STUDENT ANSWER: \"{inputs[\"generated_answer\"]}\"\\n\\\n            answer_correctness: Is the student's answer correct? (YES or NO)\\n\n            Write out in a step by step manner your reasoning to be sure that your conclusion is correct by filling out the following JSON format with the grade and a concise reason behind the grade:\n            {json_format}\n            Output the reason as a string, not as a list.\n            The only allowed grades are YES or NO. [/INST]\n            \"\"\"\n        \n        return {\"prompt\": prompt}\n\n\nclass TextFaithfulnessEvaluator(BaseEvaluator):\n    def __init__(self, user_query: str, generated_answer: str, context: str, model, tokenizer) -> dict:\n        super().__init__(user_query=user_query, generated_answer=generated_answer, context=context, model=model, tokenizer=tokenizer)\n        \n    def get_prompt(self, inputs: dict):\n        \n        json_format = \"\\{grade: '', 'reason': ''}\"\n        \n        prompt = f\"\"\"\n            [INST] Evaluate the following metric:\\n\n            text_faithfulness: Is the answer faithful to the context provided by the text, i.e. does it factually align with the context? (YES or NO)\\n\n            ANSWER: \"{inputs[\"generated_answer\"]}\"\\n\\\n            TEXT: \"{inputs[\"context\"]}\"\\n\\\n            Write out in a step by step manner your reasoning to be sure that your conclusion is correct by filling out the following JSON format with the grade and a concise reason behind the grade:\n            {json_format}\n            Output the reason as a string, not as a list.\n            The only allowed grades are YES or NO. [/INST]\n            \"\"\"\n        \n        return {\"prompt\": prompt}\n"}
{"type": "source_file", "path": "src/evaluation/evaluation_module.py", "content": "import base64\r\nimport importlib\r\nimport os\r\nfrom typing import Dict, List\r\nfrom langchain_openai import AzureChatOpenAI\r\nfrom utils.azure_config import get_azure_config\r\nfrom utils.model_loading_and_prompting.llava import load_llava_model\r\n\r\n  \r\nclass EvaluationModule:\r\n    \"\"\"\r\n    A class to evaluate the performance of a RAG pipeline based on different LLM-based metrics.\r\n    The models that can be used as evaluators are GPT-4V, LLaVA or CogVLM2.\r\n    The metrics used are Answer Correctness, Answer Relevancy, Text Context Relevancy, Image Context Relevancy,\r\n    Text Faithfulness and Image Faithfulness.\r\n    For each metric, an evaluator with the desired model is created to create the prompt and evaluate the corresponding metric.        \r\n    \"\"\"\r\n    \r\n    def __init__(self, model_type: str):\r\n        \"\"\"\r\n        Initializes the EvaluationModule with selected model as evaluator.\r\n  \r\n        :param model_type: model_type (str): Model to use for evaluation (either gpt4_vision or llava).\r\n        \"\"\"\r\n        self.model_type = model_type\r\n          \r\n        # Dynamically load the model and tokenizer based on model_type\r\n        if model_type == \"gpt4_vision\":\r\n            self.config = get_azure_config()\r\n            gpt4v_config = self.config['gpt4_vision']\r\n            self.model = AzureChatOpenAI(\r\n                openai_api_version=gpt4v_config[\"openai_api_version\"],\r\n                azure_endpoint=gpt4v_config[\"openai_endpoint\"],\r\n                azure_deployment=gpt4v_config[\"deployment_name\"],\r\n                model=gpt4v_config[\"model_version\"],\r\n                api_key=os.environ.get(\"GPT4V_API_KEY\"),\r\n                max_tokens=500\r\n            )\r\n            self.tokenizer=None\r\n            evaluator_module_name = \"openai\"\r\n        \r\n        else:\r\n            self.model, self.tokenizer = load_llava_model(\"llava-hf/llava-v1.6-mistral-7b-hf\")\r\n            evaluator_module_name = 'llava'\r\n          \r\n        # Dynamically import evaluator module based on model_type  \r\n        self.evaluator_module = self._import_evaluator_module(evaluator_module_name)\r\n\r\n        # Define supported metrics and required input arguments\r\n        self._metrics = {\r\n            'Answer Relevancy': {\r\n                'eval_method': self._evaluate_answer_relevancy,\r\n                'required_args': ['query', 'generated_answer']\r\n            },\r\n            'Answer Correctness': {\r\n                'eval_method': self._evaluate_answer_correctness,\r\n                'required_args': ['query', 'generated_answer', 'reference_answer']\r\n            },\r\n            'Image Faithfulness': {\r\n                'eval_method': self._evaluate_image_faithfulness,\r\n                'required_args': ['query', 'generated_answer', 'image']\r\n            },\r\n            'Text Faithfulness': {\r\n                'eval_method': self._evaluate_text_faithfulness,\r\n                'required_args': ['query', 'generated_answer', 'context']\r\n            },\r\n            'Image Context Relevancy': {\r\n                'eval_method': self._evaluate_image_context_relevancy,\r\n                'required_args': ['query', 'image']\r\n            },\r\n            'Text Context Relevancy': {\r\n                'eval_method': self._evaluate_text_context_relevancy,\r\n                'required_args': ['query', 'context']\r\n            }\r\n        }\r\n        \r\n    def _import_evaluator_module(self, model_type: str):  \r\n        # Construct module name and import it  \r\n        module_name = f'evaluation.evaluators.evaluators_{model_type}'   \r\n        evaluator_module = importlib.import_module(module_name)  \r\n        return evaluator_module  \r\n  \r\n    # Method to dynamically create an evaluator instance  \r\n    def create_evaluator_instance(self, evaluator_class_name: str, **kwargs):  \r\n        # Access the evaluator class from the dynamically imported module  \r\n        evaluator_class = getattr(self.evaluator_module, evaluator_class_name)  \r\n        # Instantiate the evaluator with arguments it needs  \r\n        if self.tokenizer:\r\n            evaluator_instance = evaluator_class(model=self.model, tokenizer=self.tokenizer, **kwargs)\r\n        else:\r\n            evaluator_instance = evaluator_class(model=self.model, **kwargs)\r\n        return evaluator_instance  \r\n  \r\n\r\n    # create evaluators for each metric\r\n    def _evaluate_answer_relevancy(self, query: str, generated_answer: str) -> dict:  \r\n        evaluator = self.create_evaluator_instance('AnswerRelevancyEvaluator', user_query=query, generated_answer=generated_answer)  \r\n        return {'Answer Relevancy': evaluator.run_evaluation()}\r\n\r\n    def _evaluate_answer_correctness(self, query: str, generated_answer: str, reference_answer: str) -> dict:\r\n        evaluator = self.create_evaluator_instance('AnswerCorrectnessEvaluator', user_query=query, generated_answer=generated_answer, reference_answer=reference_answer)\r\n        return {'Answer Correctness': evaluator.run_evaluation()}\r\n    \r\n    def _evaluate_image_faithfulness(self, query: str, generated_answer: str, image: str) -> dict:\r\n        evaluator = self.create_evaluator_instance('ImageFaithfulnessEvaluator', user_query=query, generated_answer=generated_answer, image=image)\r\n        return {'Image Faithfulness': evaluator.run_evaluation()}\r\n    \r\n    def _evaluate_text_faithfulness(self, query: str, generated_answer: str, context: str) -> dict:\r\n        evaluator = self.create_evaluator_instance('TextFaithfulnessEvaluator', user_query=query, generated_answer=generated_answer, context=context)\r\n        return {'Text Faithfulness': evaluator.run_evaluation()}\r\n    \r\n    def _evaluate_image_context_relevancy(self, query: str, image: str) -> dict:\r\n        evaluator = self.create_evaluator_instance('ImageContextRelevancyEvaluator', user_query=query, image=image)\r\n        return {'Image Context Relevancy': evaluator.run_evaluation()}\r\n\r\n    def _evaluate_text_context_relevancy(self, query: str, context: str) -> dict:\r\n        evaluator = self.create_evaluator_instance('TextContextRelevancyEvaluator', user_query=query, context=context)\r\n        return {'Text Context Relevancy': evaluator.run_evaluation()}\r\n\r\n\r\n    def evaluate(self, metrics: List[str], **kwargs) -> Dict[str, dict]:\r\n        \"\"\"\r\n        Evaluates the specified metrics and returns the results as a dictionary.\r\n\r\n        Args:\r\n            metrics (List): List of metrics to be calculated.\r\n\r\n        Keyword Args:\r\n            query (str): The user query\r\n            generated_answer (str): The answer produced by the model\r\n            reference_answer (str): The ground truth answer\r\n            context (str): The texts retrieved by the retrieval system\r\n            image (str): The image retrieved by the retrieval system\r\n\r\n        Returns:\r\n            Dict[str, dict]: A dictionary with metric names as keys\r\n            and a dictionary with grade and reason as values.\r\n\r\n        Raises:\r\n            ValueError: If a required argument for a metric is missing or if an invalid metric is specified.\r\n        \"\"\"\r\n        results = {}\r\n        for metric in metrics:\r\n            if metric in self._metrics:\r\n                required_args = self._metrics[metric]['required_args']\r\n                if self._check_required_arguments(required_args, metric, list(kwargs.keys())):\r\n                    metric_kwargs = {arg: kwargs[arg] for arg in required_args}\r\n                    results.update(self._metrics[metric]['eval_method'](**metric_kwargs))\r\n            else:\r\n                raise ValueError(f\"Invalid metric '{metric}'\\n\"\r\n                                 f\"Valid metrics: {list(self._metrics.keys())}\")\r\n        return results\r\n\r\n    def _check_required_arguments(self, required_args: List[str], metric: str, kwargs: List[str]) -> bool:\r\n        \"\"\"\r\n           Check if all required arguments for a specific metric are present in the provided arguments.\r\n\r\n           Args:\r\n               required_args (List[str]): A list of arguments required for the metric.\r\n               metric (str): The name of the metric being checked.\r\n               kwargs (List[str]): A list of provided arguments.\r\n\r\n           Returns:\r\n               bool: True if all required arguments are present, False otherwise.\r\n\r\n           Raises:\r\n               ValueError: If one or more required arguments are missing.\r\n           \"\"\"\r\n        missing_args = [arg for arg in required_args if arg not in kwargs]\r\n        if missing_args:\r\n            raise ValueError(f\"Missing required arguments for metric '{metric}':\\n\"\r\n                             f\"Required arguments: {self._metrics[metric]['required_args']}\\n\"\r\n                             f\"Missing: {', '.join(missing_args)}\")\r\n        return True\r\n\r\n\r\nif __name__ == '__main__':\r\n    evaluator_model = \"llava\"\r\n    evaluation_module = EvaluationModule(evaluator_model)\r\n\r\n    user_query = \"What is the transformer architecture?\"\r\n    context = \"\"\"The dominant sequence transduction models are based on complex recurrent or convolutional neural\r\n            networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder\r\n            through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely\r\n            on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine\r\n            translation tasks show these models to be superior in quality while being more parallelizable and requiring\r\n            significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation\r\n            task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014\r\n            English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of\r\n            41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from\r\n            the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to\r\n            English constituency parsing both with large and limited training data.\"\"\"\r\n\r\n    img_path = \"./img/transformer.PNG\"\r\n    with open(img_path, \"rb\") as image_file:\r\n        image = base64.b64encode(image_file.read()).decode(\"utf-8\")\r\n\r\n    reference_answer = \"\"\"The transformer architecture is a popular deep learning model used in natural language\r\n    processing tasks. It replaces recurrent neural networks with a self-attention mechanism, allowing the model to\r\n    capture long-range dependencies more effectively. It consists of an encoder and decoder, each with multiple layers.\r\n    The transformer has achieved state-of-the-art performance in NLP and serves as the basis for models like BERT, GPT,\r\n    and T5.\"\"\"\r\n    generated_answer = \"I love cats\"\r\n    \r\n    METRICS = ['Answer Correctness', 'Answer Relevancy','Image Faithfulness',\r\n               'Image Context Relevancy','Text Faithfulness', 'Text Context Relevancy']\r\n    \r\n    results = evaluation_module.evaluate(metrics=METRICS,\r\n                                         query=user_query,\r\n                                         context=context,\r\n                                         image=image,\r\n                                         generated_answer=generated_answer,\r\n                                         reference_answer=reference_answer)\r\n    \r\n    print(results)\r\n"}
{"type": "source_file", "path": "src/evaluation/evaluators/evaluators_openai.py", "content": "from langchain_core.messages import HumanMessage\nfrom evaluation.evaluators.base_evaluator import BaseEvaluator\n\n\"\"\"\nContains evaluator classes for specific metrics with GPT-4V(ision) as evaluator model.\nThe required input arguments vary depending on the metric to be evaluated.\nEach Evaluator inherits from the BaseEvaluator and implements the get_prompt method for prompt construction.\nEach prompt instructs the model to evaluate the desired metric, it provides a description of the metric,\nit provides the required input arguments to the model, and it describes the output format required.\n\"\"\"\n\n\nclass TextContextRelevancyEvaluator(BaseEvaluator):\n    def __init__(self, user_query: str, context: str, model):\n        super().__init__(model=model, user_query=user_query, context=context)\n\n    def get_prompt(self, inputs: dict):\n        message = {\n            \"type\": \"text\",\n            \"text\": (\n                f\"\"\"\n            Evaluate the following metric:\\n\n            text_context_relevancy: Is the context provided by the text \"{inputs[\"context\"]}\" relevant to the user\\'s query \"{inputs[\"user_query\"]}\"? (YES or NO)\\n\n            Write out in a step by step manner your reasoning to be sure that your conclusion is correct.\n            Give the reason as a string, not a list.\n            {self.json_parser.get_format_instructions()}\n            \"\"\"\n            ),\n        }\n\n        return [HumanMessage(content=[message])]\n\n\nclass ImageContextRelevancyEvaluator(BaseEvaluator):\n    def __init__(self, user_query: str, image: str, model):\n        super().__init__(model=model, user_query=user_query, image=[image])\n\n    def get_prompt(self, inputs: dict):\n        messages = []\n        for image in inputs['image']:\n            image_message = {\n                \"type\": \"image_url\",\n                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n            }\n            messages.append(image_message)\n\n        text_message = {\n            \"type\": \"text\",\n            \"text\": (\n                f\"\"\"\n            Evaluate the following metric:\\n\n            image_context_relevancy: Is the context provided by the image(s) relevant to the user\\'s query \"{inputs[\"user_query\"]}\"? (YES or NO)\\n\n            Write out in a step by step manner your reasoning to be sure that your conclusion is correct.\n            Give the reason as a string, not a list.\n            {self.json_parser.get_format_instructions()}\n            \"\"\"\n            ),\n        }\n\n        messages.append(text_message)\n        return [HumanMessage(content=messages)]\n\n\nclass AnswerRelevancyEvaluator(BaseEvaluator):\n    def __init__(self, user_query: str, generated_answer: str, model):\n        super().__init__(model=model, user_query=user_query, generated_answer=generated_answer)\n\n    def get_prompt(self, inputs: dict):\n        message = {\n            \"type\": \"text\",\n            \"text\": (\n                f\"\"\"\n            Evaluate the following metric:\\n\n            answer_relevancy: Is the answer \"{inputs[\"generated_answer\"]}\" relevant to the user\\'s query \"{inputs[\"user_query\"]}\"? (YES or NO)\\n\n            Write out in a step by step manner your reasoning to be sure that your conclusion is correct.\n            Give the reason as a string, not a list.\n            {self.json_parser.get_format_instructions()}\n            \"\"\"\n            ),\n        }\n\n        return [HumanMessage(content=[message])]\n\n\nclass AnswerCorrectnessEvaluator(BaseEvaluator):\n    def __init__(self, user_query: str, generated_answer: str, reference_answer: str, model):\n        super().__init__(model=model, user_query=user_query, reference_answer=reference_answer,\n                         generated_answer=generated_answer)\n\n    def get_prompt(self, inputs: dict):\n        message = {\n            \"type\": \"text\",\n            \"text\": (\n                f\"\"\"\n                You are given a question, the correct reference answer, and the student\\'s answer. \\\n                You are asked to grade the student\\'s answer as either correct or incorrect, based on the reference answer. \\\n                Ignore differences in punctuation and phrasing between the student answer and true answer. \\\n                It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements.\\\n                USER QUERY: \"{inputs[\"user_query\"]}\"\\n\\\n                REFERENCE ANSWER: \"{inputs[\"reference_answer\"]}\"\\n\\\n                STUDENT ANSWER: \"{inputs[\"generated_answer\"]}\"\\n\\\n                answer_correctness: Is the student's answer correct? (YES or NO)\\n\n                Write out in a step by step manner your reasoning to be sure that your conclusion is correct.\n                Give the reason as a string, not a list.\n                {self.json_parser.get_format_instructions()}\n                \"\"\"\n            ),\n        }\n\n        return [HumanMessage(content=[message])]\n\n\nclass ImageFaithfulnessEvaluator(BaseEvaluator):\n    def __init__(self, user_query: str, generated_answer: str, image: str, model):\n        super().__init__(user_query=user_query, generated_answer=generated_answer, image=[image], model=model)\n\n    def get_prompt(self, inputs: dict):\n        \n        if not inputs[\"image\"]:\n            return None\n\n            \n        \n        messages = []\n        for image in inputs['image']:\n            image_message = {\n                \"type\": \"image_url\",\n                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n            }\n            messages.append(image_message)\n\n        text_message = {\n            \"type\": \"text\",\n            \"text\": (\n                f\"\"\"\n                Evaluate the following metric:\\n\n                image_faithfulness: Is the answer faithful to the context provided by the image(s), i.e. does it factually align with the context? (YES or NO)\\n\n                ANSWER: \"{inputs[\"generated_answer\"]}\"\\n\\\n                Write out in a step by step manner your reasoning to be sure that your conclusion is correct.\n                Give the reason as a string, not a list.\n                {self.json_parser.get_format_instructions()}\n                \"\"\"\n            ),\n        }\n\n        messages.append(text_message)\n        return [HumanMessage(content=messages)]\n\n\nclass TextFaithfulnessEvaluator(BaseEvaluator):\n    def __init__(self, user_query: str, generated_answer: str, context: str, model):\n        super().__init__(user_query=user_query, generated_answer=generated_answer, context=context, model=model)\n\n    def get_prompt(self, inputs: dict):\n        \n        if not inputs[\"context\"]:\n            return None\n        \n        message = {\n            \"type\": \"text\",\n            \"text\": (\n                f\"\"\"\n                Evaluate the following metric:\\n\n                text_faithfulness: Is the answer faithful to the context provided by the text, i.e. does it factually align with the context? (YES or NO)\\n\n                ANSWER: \"{inputs[\"generated_answer\"]}\"\\n\\\n                TEXT: \"{inputs[\"context\"]}\"\\n\\\n                Write out in a step by step manner your reasoning to be sure that your conclusion is correct.\n                Give the reason as a string, not a list.\n                {self.json_parser.get_format_instructions()}\n                \"\"\"\n            ),\n        }\n\n        return [HumanMessage(content=[message])]\n"}
{"type": "source_file", "path": "src/data_extraction/pdf_content_extractor.py", "content": "import fitz\nimport glob\nimport os\nimport pandas as pd\nimport sys\nimport time\nfrom typing import List, Any\nfrom data_extraction.context_reduction import get_token_count\nfrom rag_env import IMAGES_DIR, INPUT_DATA, MANUALS_DIR\nfrom utils.utils_logging import utils_logger\n\n\nsys.setrecursionlimit(5000)\n\n\ndef get_pdf_chunks(pdf_reader: Any, max_tokens: int) -> List[List[Any]]:\n    \"\"\"\n    Breaks a PDF document into chunks of pages.\n    Each chunk contains pages that together have no more than max_tokens tokens.\n\n    :param pdf_reader: The PDF document to be chunked.\n    :param max_tokens: The maximum number of tokens that each chunk of pages can contain.\n    :return: A list of chunks. Each chunk is a list of pages.\n    \"\"\"\n    chunks = []\n    pages = []\n    n_tokens = 0\n    for page in pdf_reader:\n        page_tokens = get_token_count(page.get_text())\n        if page_tokens + n_tokens > max_tokens:\n            if len(pages):\n                chunks.append(pages)\n                pages = []\n                n_tokens = 0\n        n_tokens += page_tokens\n        pages.append(page)\n    if pages:\n        chunks.append(pages)\n    return chunks\n\n\ndef recoverpix(doc, item):\n    xref = item[0]  # xref of PDF image\n    smask = item[1]  # xref of its /SMask\n\n    # special case: /SMask or /Mask exists\n    if smask > 0:\n        pix0 = fitz.Pixmap(doc.extract_image(xref)[\"image\"])\n        if pix0.alpha:  # catch irregular situation\n            pix0 = fitz.Pixmap(pix0, 0)  # remove alpha channel\n        mask = fitz.Pixmap(doc.extract_image(smask)[\"image\"])\n\n        try:\n            pix = fitz.Pixmap(pix0, mask)\n        except:  # fallback to original base image in case of problems\n            pix = fitz.Pixmap(doc.extract_image(xref)[\"image\"])\n\n        if pix0.n > 3:\n            ext = \"pam\"\n        else:\n            ext = \"png\"\n\n        return {  # create dictionary expected by caller\n            \"ext\": ext,\n            \"colorspace\": pix.colorspace.n,\n            \"image\": pix.tobytes(ext),\n        }\n\n    # special case: /ColorSpace definition exists\n    # to be sure, we convert these cases to RGB PNG images\n    if \"/ColorSpace\" in doc.xref_object(xref, compressed=True):\n        pix = fitz.Pixmap(doc, xref)\n        pix = fitz.Pixmap(fitz.csRGB, pix)\n        return {  # create dictionary expected by caller\n            \"ext\": \"png\",\n            \"colorspace\": 3,\n            \"image\": pix.tobytes(\"png\"),\n        }\n    return doc.extract_image(xref)\n\n\ndef extract_images_from_pdf(pdf_reader, imgdir, chunk_number, store_to_folder=False, dimlimit=50, abssize=1024, relsize=0.0):\n    \"\"\"\n    Extract images from a PDF document based on dimension, absolute size, and relative size limits.\n\n    :param pdf_reader: The PDF document to extract images from.\n    :param chunk_number: The number of the chunk from which to extract images.\n    :param dimlimit: The minimum dimension an image must have to be extracted.\n    :param abssize: The minimum absolute size an image must have to be extracted.\n    :param relsize: The minimum relative size an image must have to be extracted.\n    :return: A list of dictionaries, each containing the bytes of an image.\n    \"\"\"\n\n    images = []\n    xreflist = []\n\n    img_list = pdf_reader.get_page_images(chunk_number - 1)\n\n    for img in img_list:\n        img_dict = {}\n        xref = img[0]\n        if xref in xreflist:\n            continue\n        width = img[2]\n        height = img[3]\n        if min(width, height) <= dimlimit:\n            continue\n        image = recoverpix(pdf_reader, img)\n        colorspace = image[\"colorspace\"]\n\n        imgdata = image[\"image\"]\n        imgfile = os.path.join(imgdir, f\"img%05i_p{chunk_number}.%s\" % (xref, image[\"ext\"]))\n\n        if len(imgdata) <= abssize:\n            continue\n        if len(imgdata) / (width * height * colorspace) <= relsize:\n            continue\n\n        img_dict[\"image_bytes\"] = imgdata\n        img_dict[\"image_id\"] = os.path.basename(imgfile)\n\n        if store_to_folder:\n            if not os.path.exists(imgdir):  # make subfolder if necessary\n                os.mkdir(imgdir)\n            fout = open(imgfile, \"wb\")\n            fout.write(imgdata)\n            fout.close()\n\n        images.append(img_dict)\n\n    return images\n\n\ndef create_dataframe_from_pdf(input_file: str, use_pages: bool, max_tokens: int, imgdir: str, pandas_df: pd.DataFrame = None) -> pd.DataFrame:\n    \"\"\"\n    Create a dataframe from a PDF document. Each row in the dataframe corresponds to an image in the document.\n\n    :param input_file: The path of the PDF file.\n    :param use_pages: If True, the PDF will be chunked into individual pages. Otherwise, it will be chunked into groups\n    of pages with no more than max_tokens tokens.\n    :param max_tokens: The maximum number of tokens each chunk can contain. Ignored if use_pages is True.\n    :param pandas_df: Optional Dataframe from which to read the relevant pages that match the current document.\n    :return: A dataframe where each row corresponds to an image from the PDF document.\n    \"\"\"\n    docs = []\n    imgdir = os.path.join(imgdir, os.path.basename(input_file).split('.')[0])\n    with fitz.open(input_file) as pdf_reader:\n        try:\n            doc_title = \"\"\n            if pdf_reader.metadata:\n                doc_title = pdf_reader.metadata.get(\"title\", \"\")\n                if pdf_reader.metadata.get(\"subject\", \"\"):\n                    doc_title += \" - \" + pdf_reader.metadata.get(\"subject\", \"\")\n        except Exception as ex:\n            utils_logger.warning(\n                f\"Could not parse pdf document (PDFReader error) {input_file}: {ex}\"\n            )\n\n        try:\n            # Get the pages from the pandas dataframe that match the current document\n            if use_pages:\n                if pandas_df is not None:\n                    pages_to_extract = pandas_df[pandas_df['doc_id'] == os.path.basename(input_file)]['page_number'].tolist()\n                    chunks = [[page] for i, page in enumerate(pdf_reader) if i + 1 in pages_to_extract]\n                else:\n                    chunks = [[page] for page in pdf_reader]\n            else:\n                chunks = get_pdf_chunks(pdf_reader, max_tokens)\n        except Exception as ex:\n            utils_logger.warning(f\"Could not extract pages from pdf document (PDFReader error) {input_file}: {ex}\")\n\n        for chunk_number, chunk in enumerate(chunks, start=1):\n            text_parts = [page.get_text() for page in chunk]\n            text = \" \".join(text_parts)\n            first_page_number = chunk[0].number + 1\n            last_page_number = chunk[-1].number + 1\n            if first_page_number != last_page_number:\n                page_number = f\"{first_page_number}-{last_page_number}\"\n            else:\n                page_number = str(first_page_number)\n\n            images = extract_images_from_pdf(pdf_reader, imgdir, chunk[0].number+1, store_to_folder=True)\n\n            if len(images) > 0:\n\n                for image in images:\n                    doc = {\"doc_id\": os.path.basename(input_file),\n                           \"doc_title\": doc_title,\n                           \"page_number\": page_number,\n                           \"text\": text,\n                           \"url\": f\"{os.path.basename(input_file)}#page={first_page_number}\",\n                           \"has_image\": True,\n                           \"image_id\": f\"{os.path.basename(input_file).split('.')[0]}\\\\{image['image_id']}\",\n                           \"image_bytes\": image[\"image_bytes\"]}\n\n                    docs.append(doc)\n\n            else:\n                doc = {\"doc_id\": os.path.basename(input_file),\n                       \"doc_title\": doc_title,\n                       \"page_number\": page_number,\n                       \"text\": text,\n                       \"url\": f\"{os.path.basename(input_file)}#page={first_page_number}\",\n                       \"has_image\": False,\n                       \"image_id\": None,\n                       \"image_bytes\": None}\n\n                docs.append(doc)\n\n    df = pd.DataFrame(docs)\n    df.insert(loc=0, column='index', value=df.index)\n\n    return df\n\n\nif __name__ == \"__main__\":\n    start_time = time.time()\n\n    combined_df = pd.DataFrame()\n\n    input_files = glob.glob(os.path.join(MANUALS_DIR, \"*.pdf\"))\n    if not os.path.exists(IMAGES_DIR):\n        os.mkdir(IMAGES_DIR)\n\n    for input_file in input_files:\n        df = create_dataframe_from_pdf(\n            input_file=input_file,\n            use_pages=True,\n            max_tokens=0,\n            imgdir=IMAGES_DIR,\n            pandas_df=None\n        )\n        combined_df = pd.concat([combined_df, df], ignore_index=True)\n\n    combined_df.to_parquet(INPUT_DATA, engine='pyarrow')\n\n    end_time = time.time()\n    print(\"total time %g sec\" % (end_time - start_time))\n"}
{"type": "source_file", "path": "src/question_answering/rag/separate_vector_stores/dual_retrieval.py", "content": "import os\nimport uuid\nfrom typing import List, Tuple\nfrom langchain.retrievers import MultiVectorRetriever\nfrom langchain_community.embeddings import HuggingFaceBgeEmbeddings\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_core.documents import Document\nfrom langchain_experimental.open_clip import OpenCLIPEmbeddings\nfrom langchain_openai import AzureOpenAIEmbeddings\nfrom langchain.storage import LocalFileStore\nfrom utils.azure_config import get_azure_config\nfrom rag_env import IMAGES_DIR\n\n\nclass DualSummaryStoreAndRetriever:\n    \"\"\"\n    A class providing two separate document stores and vector stores\n    to contain texts and images respectively, together with their emebeddings.\n    The class also provides a text retriever and an image retriever\n    to find documents that are relevant for a query.\n    Retrieval is performed using the embeddings in the vector store, but the documents contained in the\n    document store are returned. This allows image retrieval via the image summaries, while still ensuring\n    that the original images associated with the summaries are returned.\n  \n    Attributes:\n        embedding_model (str): Model used to embed the texts and image summaries.\n        store_path (str): Path where the vector and document stores should be saved.\n        img_docstore (LocalFileStore): Document store containing images.\n        text_docstore (LocalFileStore): Document store containing texts.\n        img_vectorstore (Chroma): Vector store containing embedded image summaries.\n        text_vectorstore (Chroma): Vector store containing embedded texts.\n        img_retriever (MultiVectorRetriever): Retriever that encommpasses both a vector store and a document store for image retrieval.\n        text_retriever (MultiVectorRetriever): Retriever that encommpasses both a vector store and a document store for text retrieval.\n    \"\"\"\n    def __init__(self, embedding_model, store_path=None, model_id=None):\n        if embedding_model == 'openai':\n            print(\"Using text-embedding-3-small\")\n            azure_embedding_config = get_azure_config()['text_embedding_3']\n            self.embeddings = AzureOpenAIEmbeddings(model=azure_embedding_config[\"model_version\"],\n                                                    azure_endpoint=azure_embedding_config[\"openai_endpoint\"],\n                                                    openai_api_version=azure_embedding_config[\"openai_api_version\"],\n                                                    openai_api_key=os.getenv(\"AZURE_OPENAI_API_KEY_EMBEDDING\"),\n                                                    chunk_size=64,\n                                                    show_progress_bar=True\n                                                    )\n        else:\n            print(\"Using BGE embeddings\")\n            model_name = \"BAAI/bge-m3\"\n            model_kwargs = {\"device\": \"cuda\"}\n            encode_kwargs = {\"normalize_embeddings\": True, \"batch_size\": 1, \"show_progress_bar\":True}\n            self.embeddings = HuggingFaceBgeEmbeddings(\n                model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n                )\n        \n        self.store_path = store_path\n        img_vectorstore_dir = os.path.join(self.store_path, rf\"image_only_{model_id}_vectorstore_{embedding_model}\")\n        img_docstore_dir = os.path.join(self.store_path, rf\"image_only_{model_id}_docstore_{embedding_model}\")\n        text_vectorstore_dir = os.path.join(self.store_path, rf\"text_only_{model_id}_vectorstore_{embedding_model}\")\n        text_docstore_dir = os.path.join(self.store_path, rf\"text_only_{model_id}_docstore_{embedding_model}\")\n        \n        self.img_docstore = LocalFileStore(img_docstore_dir)\n        self.text_docstore = LocalFileStore(text_docstore_dir)\n        self.id_key = \"doc_id\"\n        self.doc_ids = []\n\n        # Initialize the image vectorstore with the embedding function\n        self.img_vectorstore = Chroma(\n            persist_directory=img_vectorstore_dir,\n            embedding_function=self.embeddings,\n            collection_name=f\"mm_rag_with_image_summaries_{embedding_model}_embeddings\"\n        )\n\n        # Initialize the image vectorstore with the embedding function\n        self.text_vectorstore = Chroma(\n            persist_directory=text_vectorstore_dir,\n            embedding_function=self.embeddings,\n            collection_name=f\"mm_rag_with_image_summaries_{embedding_model}_embeddings\"\n        )\n        \n        results_img = self.img_vectorstore.get(include=[\"embeddings\", \"documents\", \"metadatas\"])\n        results_text = self.text_vectorstore.get(include=[\"embeddings\", \"documents\", \"metadatas\"])\n        \n        # A vector store is considered as already existing if it contains embeddings for both modalities\n        self.is_new_vectorstore = bool(results_img[\"embeddings\"]) and bool(results_text[\"embeddings\"])\n\n        if self.is_new_vectorstore:\n            print(f\"Vectorstore at path {img_vectorstore_dir} already exists\")\n\n        else:\n            print(f\"Creating new vectorstore and docstore at path {img_vectorstore_dir}\")\n\n        self.text_retriever = MultiVectorRetriever(\n            vectorstore=self.text_vectorstore,\n            docstore=self.text_docstore,\n            id_key=self.id_key,\n            search_kwargs={\"k\": 2}\n        )\n        self.img_retriever = MultiVectorRetriever(\n            vectorstore=self.img_vectorstore,\n            docstore=self.img_docstore,\n            id_key=self.id_key,\n            search_kwargs={\"k\": 2}\n        )\n        \n        self.retrieved_docs = []\n        self.retrieved_imgs = []\n        self.retrieved_texts =[]\n        \n\n    def add_docs(self, doc_summaries: List[str], doc_contents: List[str], doc_filenames: List[str], modality: str):\n        \"\"\"\n        Add documents to the vector store and document store of the selected modality.\n        \n        :param doc_summaries: Either text or image summaries to be stored in the vector store.\n        :param doc_contents: The original texts or images to be stored in the document store.\n        :param doc_filenames: File names associated with the respective documents to be stored as additional metadata.\n        \"\"\"\n        if not self.is_new_vectorstore:\n            print(\"Adding documents...\")\n            doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n            summary_docs = [\n                Document(page_content=s, metadata={self.id_key: doc_ids[i], \"filename\": doc_filenames[i]})\n                for i, s in enumerate(doc_summaries)\n            ]\n            \n            if modality == \"text\":\n                self.text_vectorstore.add_documents(summary_docs)\n                self.text_docstore.mset(list(zip(doc_ids, map(lambda x: str.encode(x), doc_contents))))\n            else:\n                self.img_vectorstore.add_documents(summary_docs)\n                self.img_docstore.mset(list(zip(doc_ids, map(lambda x: str.encode(x), doc_contents))))\n        else:\n            print(\"Documents have already been added before, skipping...\")\n            \n\n    def retrieve(self, query: str, limit: int, retriever) -> Tuple[List[Document], List[Document]]:\n        \"\"\"\n        Retrieve the most relevant documents based on the query.\n        \"\"\"\n        if retriever == \"image\":\n            self.retrieved_imgs = self.img_retriever.invoke(query, limit=limit)\n        else:\n            self.retrieved_texts = self.text_retriever.invoke(query, limit=limit)\n\n        return self.retrieved_imgs, self.retrieved_texts\n\n\n\n\nclass DualClipRetriever:\n    \"\"\"  \n    A class providing a vector store to contain multimodal CLIP embeddings of images\n    and a vector store and a vector store and associated document store for texts.\n    \n    The class also provides a text retriever and an image retriever\n    to find documents that are relevant for a query.\n  \n    Attributes:  \n        store_path (str): Path where the vector stores and document store should be saved.\n        text_model_id:\n        text_embedding_model (str): Model used to embed the texts.\n        images_dir (str): Directory containing the images to be embedded.\n        img_vectorstore (Chroma): Vector store containing images embedded with CLIP.\n        text_vectorstore (Chroma): Vector store containing texts embedded with the desired text_embedding_model.\n        text_docstore (LocalFileStore): Document store containing texts.\n        img_retriever (MultiVectorRetriever): Retriever that used CLIP embeddings for image retrieval.\n        text_retriever (MultiVectorRetriever): Retriever that encommpasses both a vector store and a document store for text retrieval.\n    \"\"\"  \n    def __init__(self, store_path, text_model_id, text_embedding_model,\n                 images_dir=IMAGES_DIR):\n        self.images_dir = images_dir\n\n        if text_embedding_model == 'openai':\n            print(\"Using openai embeddings\")\n            azure_embedding_config = get_azure_config()['text_embedding_3']\n            self.embeddings = AzureOpenAIEmbeddings(model=azure_embedding_config[\"model_version\"],\n                                                    azure_endpoint=azure_embedding_config[\"openai_endpoint\"],\n                                                    openai_api_version=azure_embedding_config[\"openai_api_version\"],\n                                                    openai_api_key=os.getenv(\"AZURE_OPENAI_API_KEY_EMBEDDING\"),\n                                                    chunk_size=64,\n                                                    show_progress_bar=True\n                                                    )\n        else:\n            print(\"Using BGE embeddings\")\n            model_name = \"BAAI/bge-m3\"\n            model_kwargs = {\"device\": \"cuda\"}\n            encode_kwargs = {\"normalize_embeddings\": True, \"batch_size\": 1, \"show_progress_bar\":True}\n            self.embeddings = HuggingFaceBgeEmbeddings(\n                model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n                )\n\n        self.store_path = store_path\n        img_vectorstore_dir = os.path.join(self.store_path, rf\"image_only_clip/image_only_vectorstore_clip\")\n        text_vectorstore_dir = os.path.join(self.store_path, rf\"text_only_{text_model_id}/text_only_{text_model_id}_vectorstore_{text_embedding_model}\")\n        text_docstore_dir = os.path.join(self.store_path, rf\"text_only_{text_model_id}/text_only_{text_model_id}_docstore_{text_embedding_model}\")\n\n        self.text_docstore = LocalFileStore(text_docstore_dir)\n        self.id_key = \"doc_id\"\n        self.doc_ids = []\n\n        # Create chroma vectorstore\n        self.img_vectorstore = Chroma(\n            collection_name=\"mm_rag_clip_photos\",\n            embedding_function=OpenCLIPEmbeddings(),\n            persist_directory=img_vectorstore_dir\n        )\n\n\n        self.text_vectorstore = Chroma(\n            persist_directory=text_vectorstore_dir,\n            embedding_function=self.embeddings,\n            collection_name=f\"mm_rag_with_image_summaries_{text_embedding_model}_embeddings\"\n        )\n\n        results_img = self.img_vectorstore.get(include=[\"embeddings\", \"documents\", \"metadatas\"])\n        results_text = self.text_vectorstore.get(include=[\"embeddings\", \"documents\", \"metadatas\"])\n\n        self.is_new_vectorstore = bool(results_img[\"embeddings\"]) and bool(results_text[\"embeddings\"])\n\n        if self.is_new_vectorstore:\n            print(f\"Vectorstore at path {img_vectorstore_dir} already exists\")\n\n        else:\n            print(f\"Creating new vectorstore and docstore at path {img_vectorstore_dir}\")\n\n        self.img_retriever = self.img_vectorstore.as_retriever(search_kwargs={\"k\": 2})\n\n        self.text_retriever = MultiVectorRetriever(\n            vectorstore=self.text_vectorstore,\n            docstore=self.text_docstore,\n            id_key=self.id_key,\n            search_kwargs={\"k\": 2}\n        )\n\n        self.retrieved_docs = []\n        self.retrieved_imgs = []\n        self.retrieved_texts =[]\n\n\n    def add_images(self, images_dir: str=None):\n        \"\"\"\n        Add images to the image vector store.\n        \n        :param images_dir: Directory containing the images to be embedded.\n        \"\"\"\n        if not self.is_new_vectorstore:\n            if images_dir:\n                image_uris = self.extract_image_uris(images_dir)\n                print(f\"Found {len(image_uris)} images\")\n                # Convert the list of URIs to a list of dictionaries\n                image_metadatas = [{'filename': self.extract_manual_name(uri)} for uri in image_uris]\n\n                print(\"Adding images to vectorstore...\")\n                self.img_vectorstore.add_images(uris=image_uris, metadatas=image_metadatas)\n        else:\n            print(\"Documents have already been added before, skipping...\")\n\n\n\n    def extract_image_uris(self, root_path: str, image_extension: str=\".png\") -> List[str]:\n        image_uris = []\n        for subdir, dirs, files in os.walk(root_path):\n            for file in files:\n                if file.endswith(image_extension):\n                    image_uris.append(os.path.join(subdir, file))\n        return sorted(image_uris)\n\n\n\n    def extract_manual_name(self, uri: str) -> str:  \n        # Split the URI into parts  \n        parts = uri.split('/')\n        # The directory name is the second to last element  \n        directory_name = parts[-2]\n        # Append \".pdf\" to the directory name  \n        return f\"{directory_name}.pdf\"\n\n\n    def add_texts(self, doc_summaries: List[str], doc_contents: List[str], doc_filenames: List[str]):\n        \"\"\"\n        Add texts to the text vector store and document store.\n        \n        :param doc_summaries: Text summaries to be stored in the vector store.\n        :param doc_contents: The original texts to be stored in the document store.\n        :param doc_filenames: File names associated with the texts to be stored as additional metadata.\n        \"\"\"\n        if not self.is_new_vectorstore:\n            print(\"Adding documents...\")\n            doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n            summary_docs = [\n                Document(page_content=s, metadata={self.id_key: doc_ids[i], \"filename\": doc_filenames[i]})\n                for i, s in enumerate(doc_summaries)\n            ]\n            self.text_vectorstore.add_documents(summary_docs)\n            self.text_docstore.mset(list(zip(doc_ids, map(lambda x: str.encode(x), doc_contents))))\n        else:\n            print(\"Documents have already been added before, skipping...\")\n\n\n    def retrieve(self, query: str, limit: int, retriever) -> Tuple[List[Document], List[Document]]:\n        \"\"\"\n        Retrieve the most relevant documents based on the query.\n        \"\"\"\n        if retriever == \"image\":\n            self.retrieved_imgs = self.img_retriever.invoke(query, limit=limit)\n        else:\n            self.retrieved_texts = self.text_retriever.invoke(query, limit=limit)\n\n        return self.retrieved_imgs, self.retrieved_texts\n\n"}
{"type": "source_file", "path": "src/utils/azure_config.py", "content": "def get_azure_config():\n    return {\n        'gpt4': {\n            'openai_endpoint': 'YOUR_ENDPOINT_HERE',\n            'deployment_name': 'gpt-4',\n            'openai_api_version': '2024-02-15-preview',\n            'model_version': 'gpt-4',\n        },\n        'gpt4_vision': {\n            'openai_endpoint': 'YOUR_ENDPOINT_HERE',\n            'deployment_name': 'gpt-4-vision-preview',\n            'openai_api_version': '2024-02-15-preview',\n            'model_version': 'gpt-4-vision-preview',\n        },\n        'text_embedding_3': {\n            'openai_endpoint': 'YOUR_ENDPOINT_HERE',\n            'deployment_name': 'text-embedding-3-small',\n            'openai_api_version': '2024-02-15-preview',\n            'model_version': 'text-embedding-3-small',\n        },\n        'gpt3.5': {\n            'openai_endpoint': 'YOUR_ENDPOINT_HERE',\n            'deployment_name': 'gpt-35-turbo',\n            'openai_api_version': '2024-02-15-preview',\n            'model_version': 'gpt-35-turbo',\n        },\n    }"}
{"type": "source_file", "path": "src/question_answering/rag/run_image_only_rag.py", "content": "import os\nimport pandas as pd\nfrom single_vector_store.rag_pipeline_clip import MultimodalRAGPipelineClip\nfrom single_vector_store.rag_pipeline_summaries import MultimodalRAGPipelineSummaries\nfrom rag_env import EMBEDDING_MODEL_TYPE, IMAGES_DIR, IMG_SUMMARIES_CACHE_DIR, INPUT_DATA, MODEL_TYPE, RAG_OUTPUT_DIR, REFERENCE_QA, VECTORSTORE_PATH_IMAGE_ONLY\n\nAZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n\ndef write_to_df(df, user_query, reference_answer, generated_answer, context, image, output_file):\n    df.loc[len(df)] = [user_query, reference_answer, generated_answer, context, image]\n    df.to_json(output_file, orient=\"records\", indent=2)\n\n\ndef process_dataframe(input_df, pipeline, output_file, output_df=None):\n    if not output_df:\n        columns = ['user_query', 'reference_answer', 'generated_answer', 'context', 'image']\n        output_df= pd.DataFrame(columns=columns)\n    for index, row in input_df.iterrows():\n        print(f\"Processing query no. {index+1}...\")\n        user_query = input_df[\"question\"][index]\n        print(\"USER QUERY:\\n\", user_query)\n        reference_answer = input_df[\"reference_answer\"][index]\n        print(\"REFERENCE ANSWER:\", reference_answer)\n        generated_answer = pipeline.answer_question(user_query)\n        print(\"GENERATED ANSWER:\\n\", generated_answer)\n        relevant_images = pipeline.rag_chain.retrieved_images\n        print(\"Retrieved images:\", len(relevant_images))\n        image = relevant_images[0] if len(relevant_images) > 0 else []\n        write_to_df(output_df, user_query, reference_answer, generated_answer, [], image, output_file)\n    return output_df\n\n\ndef run_pipeline_with_clip(model, vectorstore_path, images_dir, reference_qa, output_dir):\n    pipeline = MultimodalRAGPipelineClip(model_type=model, store_path=vectorstore_path)\n    # the pipeline is indexed only with images, no texts are added\n    pipeline.index_data(images_dir=images_dir, texts_df=None)\n    \n    df = pd.read_excel(reference_qa)\n \n    output_file = os.path.join(output_dir, f\"rag_output_{model}_image_only_clip.json\")\n    output_df = process_dataframe(df, pipeline, output_file)\n    return output_df\n\n    \n    \ndef run_pipeline_with_summaries(qa_model, embedding_model, vectorstore_path, input_df, reference_qa, output_dir, img_summaries_dir):\n    summaries_pipeline = MultimodalRAGPipelineSummaries(model_type=qa_model,\n                                                        store_path=vectorstore_path,\n                                                        embedding_model=embedding_model)\n    \n    _, images_df = summaries_pipeline.load_data(input_df)\n    images, image_filenames = images_df[[\"image_bytes\"]][\"image_bytes\"].tolist(), images_df[[\"doc_id\"]][\"doc_id\"].tolist()\n    img_base64_list, image_summaries = summaries_pipeline.image_summarizer.summarize(images, img_summaries_dir)\n    # the pipeline is indexed only with images, no texts are added\n    summaries_pipeline.index_data(image_summaries=image_summaries,\n                                  images_base64=img_base64_list, image_filenames=image_filenames,\n                                  texts = None, text_summaries=None)\n    \n    df = pd.read_excel(reference_qa)\n    \n    output_file = os.path.join(output_dir, f\"rag_output_{qa_model}_image_only_summaries.json\")\n    output_df = process_dataframe(df, summaries_pipeline, output_file)\n    return output_df\n    \n  \nif __name__ == \"__main__\":  \n    # uncomment one of the following two options to run image-only RAG either with CLIP embedings or with image summaries\n    rag_results_clip = run_pipeline_with_clip(model=MODEL_TYPE, vectorstore_path=VECTORSTORE_PATH_IMAGE_ONLY,\n                                         images_dir=IMAGES_DIR, reference_qa=REFERENCE_QA, output_dir=RAG_OUTPUT_DIR)\n    \n    rag_results_summaries = run_pipeline_with_summaries(qa_model=MODEL_TYPE, vectorstore_path=VECTORSTORE_PATH_IMAGE_ONLY,\n                                              embedding_model=EMBEDDING_MODEL_TYPE, input_df=INPUT_DATA,\n                                              reference_qa=REFERENCE_QA, output_dir=RAG_OUTPUT_DIR,\n                                              img_summaries_dir=IMG_SUMMARIES_CACHE_DIR)"}
{"type": "source_file", "path": "src/question_answering/rag/single_vector_store/rag_pipeline_summaries.py", "content": "import logging\nimport os\nimport pandas as pd\nfrom typing import List, Tuple\nfrom data_summarization.context_summarization import ImageSummarizer, TextSummarizer\nfrom langchain_openai import AzureChatOpenAI\nfrom question_answering.rag.single_vector_store.rag_chain import MultimodalRAGChain\nfrom question_answering.rag.single_vector_store.retrieval import SummaryStoreAndRetriever\nfrom utils.azure_config import get_azure_config\nfrom utils.model_loading_and_prompting.llava import load_llava_model\nfrom rag_env import EMBEDDING_MODEL_TYPE, IMG_SUMMARIES_CACHE_DIR, INPUT_DATA, MODEL_TYPE, TEXT_SUMMARIES_CACHE_DIR, VECTORSTORE_PATH_SUMMARIES_SINGLE\n\n\nclass MultimodalRAGPipelineSummaries:\n    \"\"\"\n    Initializes the Multimodal RAG pipeline.\n    Answers a user query retrieving additional context from texts and images within a document collection.\n    Can be used with different models for answer generation (AzureOpenAI and LLaVA).\n    Transforms images into textual summaries and embeds the image summaries, the texts, and the query into a single vector store.\n    \n    Attributes:\n        model_type (str): Type of the model to use for answer synthesis.\n        store_path (str): Path to the directory where the vector database is stored.\n        embedding_model (str): Text embedding model used to embed texts and image summaries.\n        model: The model used for answer synthesis loaded based on `model_type`.\n        tokenizer: The tokenizer used for tokenization. Can be None.\n        text_summarizer (TextSummarizer): Can be used to summarize texts before retrieving them.\n        image_summarizer: (ImageSummarizer): Used to generate textual summaries from images.\n        sotre_and_retriever (SummaryStoreAndRetriever): Retrieval using CLIP embeddings for images.\n        rag_chain (MultimodalRAGChain): RAG chain performing the QA task.\n    \"\"\"\n    def __init__(self, model_type, store_path, embedding_model):\n        \n        config = get_azure_config()\n        \n        if model_type in config:\n            print(\"Using Azure model for answer generation\")\n            azure_llm_config = config[model_type]\n            self.model = AzureChatOpenAI(\n                openai_api_version=azure_llm_config[\"openai_api_version\"],\n                azure_endpoint=azure_llm_config[\"openai_endpoint\"],\n                azure_deployment=azure_llm_config[\"deployment_name\"],\n                model=azure_llm_config[\"model_version\"],\n                api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n                max_tokens=400)\n            self.tokenizer = None\n            \n        else:\n            print(\"Using LLaVA model for answer generation\")\n            self.model, self.tokenizer = load_llava_model(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n\n        self.text_summarizer = TextSummarizer(model_type=\"gpt4\", cache_path=TEXT_SUMMARIES_CACHE_DIR)\n        self.image_summarizer = ImageSummarizer(self.model, self.tokenizer)\n        self.store_and_retriever = SummaryStoreAndRetriever(embedding_model=embedding_model,\n                                                            store_path=f\"{store_path}_{model_type}\")\n        self.rag_chain = MultimodalRAGChain(self.model, self.tokenizer, self.store_and_retriever.retriever)\n\n\n    def load_data(self, path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n        df = pd.read_parquet(path)\n        # drop duplicate entries in the texts column of the dataframe\n        texts = df.drop_duplicates(subset='text')[[\"text\", \"doc_id\"]]\n        img = df[[\"doc_id\", \"image_bytes\"]]\n        images = img.dropna(subset=['image_bytes'])  \n        return texts, images\n\n\n    def summarize_data(self, texts: List[str], images: List[bytes], cache_file:str) -> Tuple[List[str], List[str], List[str]]:\n        # Summarize images\n        img_base64_list, image_summaries = self.image_summarizer.summarize(images, cache_file)\n        # Summarize texts\n        if texts:\n            text_summaries = self.text_summarizer.summarize(texts)\n            return text_summaries, image_summaries, img_base64_list\n        return texts, image_summaries, img_base64_list\n\n\n    def index_data(self, texts: List[str]=None, text_summaries: List[str]=None,\n                   image_summaries: List[str]=None, images_base64: List[str]=None,\n                   text_filenames: List[str]=None, image_filenames: List[str]=None):\n        if text_summaries:\n            print(\"Adding text summaries to store\")\n            self.store_and_retriever.add_docs(text_summaries, texts, text_filenames)\n        if texts:\n            print(\"Adding texts to store\")\n            self.store_and_retriever.add_docs(texts, texts, text_filenames)\n        if image_summaries:\n            print(\"Adding image summaries to store\")\n            self.store_and_retriever.add_docs(image_summaries, images_base64, image_filenames)\n\n\n    def answer_question(self, question: str) -> str:\n        return self.rag_chain.run(question)\n\n\ndef main():\n    pipeline = MultimodalRAGPipelineSummaries(model_type=MODEL_TYPE,\n                                     store_path=VECTORSTORE_PATH_SUMMARIES_SINGLE,\n                                     embedding_model=EMBEDDING_MODEL_TYPE)\n    \n    texts_df, images_df = pipeline.load_data(INPUT_DATA)\n    texts, texts_filenames = texts_df[[\"text\"]][\"text\"].tolist(), texts_df[[\"doc_id\"]][\"doc_id\"].tolist()\n    images, image_filenames = images_df[[\"image_bytes\"]][\"image_bytes\"].tolist(), images_df[[\"doc_id\"]][\"doc_id\"].tolist()\n    img_base64_list, image_summaries = pipeline.image_summarizer.summarize(images, IMG_SUMMARIES_CACHE_DIR)\n\n    pipeline.index_data(texts, image_summaries=image_summaries, \n                        images_base64=img_base64_list, \n                        image_filenames=image_filenames, \n                        text_filenames=texts_filenames)\n\n    question = \"I want to change the behaviour of the stations to continue, if a moderate error occurrs. How can I do this?\"\n    answer = pipeline.answer_question(question)\n    relevant_docs = pipeline.rag_chain.retrieved_docs  \n    print(\"Retrieved images:\", len(relevant_docs[\"images\"]), \", Retrieved texts:\", len(relevant_docs[\"texts\"]))  \n    print(answer)\n\n\nif __name__ == \"__main__\":\n    try:\n        main()\n    except Exception as e:\n        logging.error(\"An error occurred during execution\", exc_info=True)\n"}
{"type": "source_file", "path": "src/question_answering/rag/run_text_only_rag.py", "content": "import os\nimport pandas as pd  \nfrom single_vector_store.rag_pipeline_summaries import MultimodalRAGPipelineSummaries\nfrom rag_env import EMBEDDING_MODEL_TYPE, INPUT_DATA, MODEL_TYPE, RAG_OUTPUT_DIR, REFERENCE_QA, VECTORSTORE_PATH_TEXT_ONLY\n\n\nAZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n\ndef write_to_df(df, user_query, reference_answer, generated_answer, context, image, output_file):\n    df.loc[len(df)] = [user_query, reference_answer, generated_answer, context, image]\n    df.to_json(output_file, orient=\"records\", indent=2)\n\n\ndef process_dataframe(input_df, pipeline, output_file, output_df=None):\n    if not output_df:\n        columns = ['user_query', 'reference_answer', 'generated_answer', 'context', 'image']\n        output_df= pd.DataFrame(columns=columns)\n    for index, row in input_df.iterrows():\n        print(f\"Processing query no. {index+1}...\")\n        user_query = input_df[\"question\"][index]\n        print(\"USER QUERY:\\n\", user_query)\n        reference_answer = input_df[\"reference_answer\"][index]\n        print(\"REFERENCE ANSWER:\", reference_answer)\n        generated_answer = pipeline.answer_question(user_query)\n        print(\"GENERATED ANSWER:\\n\", generated_answer)\n        relevant_texts = pipeline.rag_chain.retrieved_texts\n        print(\"Retrieved texts:\", len(relevant_texts))\n        context = \"\\n\".join(relevant_texts) if len(relevant_texts) > 0 else []\n        write_to_df(output_df, user_query, reference_answer, generated_answer, context, [], output_file)\n    return output_df\n\n\n  \nif __name__ == \"__main__\":\n    \n    pipeline = MultimodalRAGPipelineSummaries(MODEL_TYPE, VECTORSTORE_PATH_TEXT_ONLY, EMBEDDING_MODEL_TYPE)\n    texts_df, _ = pipeline.load_data(INPUT_DATA)\n    texts, texts_filenames = texts_df[[\"text\"]][\"text\"].tolist(), texts_df[[\"doc_id\"]][\"doc_id\"].tolist()\n    # the pipeline is indexed only with text, no images are added\n    pipeline.index_data(texts=texts, text_filenames=texts_filenames)\n    df = pd.read_excel(REFERENCE_QA)\n\n    output_file = os.path.join(RAG_OUTPUT_DIR, f\"rag_output_{MODEL_TYPE}_text_only.json\")\n    output_df = process_dataframe(df, pipeline, output_file)"}
{"type": "source_file", "path": "src/question_answering/rag/separate_vector_stores/dual_rag_pipeline_clip.py", "content": "import logging\nimport os\nimport pandas as pd\nfrom typing import List\nfrom data_summarization.context_summarization import TextSummarizer\nfrom langchain_openai import AzureChatOpenAI\nfrom question_answering.rag.separate_vector_stores.dual_rag_chain import DualMultimodalRAGChain\nfrom question_answering.rag.separate_vector_stores.dual_retrieval import DualClipRetriever\nfrom utils.azure_config import get_azure_config\nfrom utils.model_loading_and_prompting.llava import load_llava_model\nfrom rag_env import EMBEDDING_MODEL_TYPE, IMAGES_DIR, INPUT_DATA, MODEL_TYPE, TEXT_SUMMARIES_CACHE_DIR, VECTORSTORE_PATH_CLIP_SEPARATE\n\n\nclass DualMultimodalRAGPipelineClip:\n    \"\"\"\n    Initializes the Multimodal RAG pipeline with separate vector stores and retrievers for texts and images.\n    Answers a user query retrieving additional context from texts and images within a document collection.\n    Can be used with different models for answer generation (AzureOpenAI and LLaVA).\n    Uses CLIP as multimodal embedding model to embed the images.\n    Uses a text embedding model to embed the text.\n    The query is embedded both with CLIP and with the text embedding model to allow retrieval for each modality.\n    \n    Attributes:  \n        model_type (str): Type of the model to use for answer synthesis.  \n        store_path (str): Path to the directory where the vector databases for texts and images are stored.  \n        model: The model used for answer synthesis loaded based on `model_type`.  \n        tokenizer: The tokenizer used for tokenization. Can be None.\n        text_summarizer (TextSummarizer): Can be used to summarize texts before retrieving them.\n        dual_retriever (DualClipRetriever): Retrieval using CLIP embeddings for images and text embeddings for texts.\n        rag_chain (DualMultimodalRAGChain): RAG chain performing the QA task.\n    \"\"\"\n    def __init__(self, model_type, store_path, text_embedding_model):\n        \n        config = get_azure_config()\n        \n        if model_type in config:\n            print(\"Using Azure model for answer generation\")\n            azure_llm_config = config[model_type]\n            self.model = AzureChatOpenAI(\n                openai_api_version=azure_llm_config[\"openai_api_version\"],\n                azure_endpoint=azure_llm_config[\"openai_endpoint\"],\n                azure_deployment=azure_llm_config[\"deployment_name\"],\n                model=azure_llm_config[\"model_version\"],\n                api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n                max_tokens=400)\n            self.tokenizer = None\n            \n        else:\n            print(\"Using LLaVA model for answer generation\")\n            self.model, self.tokenizer = load_llava_model(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n\n        self.text_summarizer = TextSummarizer(model_type=\"gpt4\", cache_path=TEXT_SUMMARIES_CACHE_DIR)\n        self.dual_retriever = DualClipRetriever(store_path=store_path,\n                                            text_model_id=model_type,\n                                            text_embedding_model=text_embedding_model)\n\n        self.rag_chain = DualMultimodalRAGChain(self.model,\n                                            self.tokenizer,\n                                            self.dual_retriever.text_retriever,\n                                            self.dual_retriever.img_retriever)\n\n    def load_data(self, path: str) -> pd.DataFrame:\n        df = pd.read_parquet(path)\n        # drop duplicate entries in the texts column of the dataframe\n        texts = df.drop_duplicates(subset='text')[[\"text\", \"doc_id\"]]\n        return texts\n\n    def summarize_data(self, texts: List[str]) -> List[str]:\n        text_summaries = self.text_summarizer.summarize(texts)\n        return text_summaries\n    \n    def index_data(self, images_dir: str, texts: List[str], text_filenames: List[str], text_summaries: List[str]=None):\n        self.dual_retriever.add_images(images_dir)\n        if text_summaries:\n            self.dual_retriever.add_texts(text_summaries, texts, text_filenames)\n        else:\n            self.dual_retriever.add_texts(texts, texts, text_filenames)\n\n    def answer_question(self, question: str) -> str:\n        return self.rag_chain.run(question)\n\n\ndef main():\n    pipeline = DualMultimodalRAGPipelineClip(model_type=MODEL_TYPE,\n                                     store_path=VECTORSTORE_PATH_CLIP_SEPARATE,\n                                     text_embedding_model=EMBEDDING_MODEL_TYPE)\n    texts_df = pipeline.load_data(INPUT_DATA)\n    texts, texts_filenames = texts_df[[\"text\"]][\"text\"].tolist(), texts_df[[\"doc_id\"]][\"doc_id\"].tolist()\n    \n    pipeline.index_data(images_dir=IMAGES_DIR, texts=texts, text_summaries=texts, text_filenames=texts_filenames)\n\n    question = \"I want to change the behaviour of the stations to continue, if a moderate error occurrs. How can I do this?\"\n    answer = pipeline.answer_question(question)\n    relevant_images = pipeline.rag_chain.retrieved_images\n    relevant_texts = pipeline.rag_chain.retrieved_texts\n    print(\"Retrieved images:\", len(relevant_images), \", Retrieved texts:\", len(relevant_texts))  \n    print(answer)\n\n\nif __name__ == \"__main__\":\n    try:\n        main()\n    except Exception as e:\n        logging.error(\"An error occurred during execution\", exc_info=True)\n"}
{"type": "source_file", "path": "src/question_answering/rag/separate_vector_stores/dual_rag_pipeline_summaries.py", "content": "import os\nimport logging\nimport pandas as pd\nfrom typing import List, Tuple\nfrom data_summarization.context_summarization import ImageSummarizer, TextSummarizer\nfrom langchain_openai import AzureChatOpenAI\nfrom question_answering.rag.separate_vector_stores.dual_rag_chain import DualMultimodalRAGChain\nfrom question_answering.rag.separate_vector_stores.dual_retrieval import DualSummaryStoreAndRetriever\nfrom utils.azure_config import get_azure_config\nfrom utils.model_loading_and_prompting.llava import load_llava_model\nfrom rag_env import EMBEDDING_MODEL_TYPE, IMG_SUMMARIES_CACHE_DIR, INPUT_DATA, MODEL_TYPE, TEXT_SUMMARIES_CACHE_DIR, VECTORSTORE_PATH_SUMMARIES_SEPARATE\n\n\nclass DualMultimodalRAGPipelineSummaries:\n    \"\"\"\n    Initializes the Multimodal RAG pipeline with separate vector stores and retrievers for texts and images.\n    Answers a user query retrieving additional context from texts and images within a document collection.\n    Can be used with different models for answer generation (AzureOpenAI and LLaVA).\n    Transforms images into textual summaries and embeds the image summaries.\n    Image summaries and texts are stored in separate vector stores.\n    \n    Attributes:\n        model_type (str): Type of the model to use for answer synthesis.\n        store_path (str): Path to the directory where the vector database is stored.\n        embedding_model (str): Text embedding model used to embed texts and image summaries.\n        model: The model used for answer synthesis loaded based on `model_type`.\n        tokenizer: The tokenizer used for tokenization. Can be None.\n        text_summarizer (TextSummarizer): Can be used to summarize texts before retrieving them.\n        image_summarizer: (ImageSummarizer): Used to generate textual summaries from images.\n        sotre_and_retriever (SummaryStoreAndRetriever): Retrieval using textual summaries from images.\n        rag_chain (MultimodalRAGChain): RAG chain performing the QA task.\n    \"\"\"\n    def __init__(self, model_type, store_path, embedding_model):\n        \n        config = get_azure_config()\n        \n        if model_type in config:\n            print(\"Using Azure model for answer generation\")\n            azure_llm_config = config[model_type]\n            self.model = AzureChatOpenAI(\n                openai_api_version=azure_llm_config[\"openai_api_version\"],\n                azure_endpoint=azure_llm_config[\"openai_endpoint\"],\n                azure_deployment=azure_llm_config[\"deployment_name\"],\n                model=azure_llm_config[\"model_version\"],\n                api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n                max_tokens=400)\n            self.tokenizer = None\n            \n        else:\n            print(\"Using LLaVA model for answer generation\")\n            self.model, self.tokenizer = load_llava_model(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n\n        self.text_summarizer = TextSummarizer(model_type=\"gpt4\", cache_path=TEXT_SUMMARIES_CACHE_DIR)\n        self.image_summarizer = ImageSummarizer(self.model, self.tokenizer)\n        self.store_and_retriever = DualSummaryStoreAndRetriever(embedding_model=embedding_model,\n                                                            store_path=f\"{store_path}_{model_type}\",\n                                                            model_id=model_type)\n        self.rag_chain = DualMultimodalRAGChain(self.model,\n                                            self.tokenizer,\n                                            self.store_and_retriever.text_retriever,\n                                            self.store_and_retriever.img_retriever)\n\n    def load_data(self, path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n        df = pd.read_parquet(path)\n        # drop duplicate entries in the texts column of the dataframe\n        texts = df.drop_duplicates(subset='text')[[\"text\", \"doc_id\"]]\n        img = df[[\"doc_id\", \"image_bytes\"]]\n        images = img.dropna(subset=['image_bytes'])  \n        return texts, images\n    \n\n    def summarize_data(self, texts: List[str], images: List[bytes], cache_file:str) -> Tuple[List[str], List[str], List[str]]:\n        # Summarize images\n        img_base64_list, image_summaries = self.image_summarizer.summarize(images, cache_file)\n        # Summarize texts\n        if texts:\n            text_summaries = self.text_summarizer.summarize(texts)\n            return text_summaries, image_summaries, img_base64_list\n        return texts, image_summaries, img_base64_list\n\n\n    def index_data(self, texts: List[str]=None, text_summaries: List[str]=None,\n                   image_summaries: List[str]=None, images_base64: List[str]=None,\n                   text_filenames: List[str]=None, image_filenames: List[str]=None):\n        if text_summaries:\n            print(\"Adding text summaries to store\")\n            self.store_and_retriever.add_docs(text_summaries, texts, text_filenames, \"text\")\n        else:\n            print(\"Adding texts to store\")\n            self.store_and_retriever.add_docs(texts, texts, text_filenames, \"text\")\n        if image_summaries:\n            print(\"Adding image summaries to store\")\n            self.store_and_retriever.add_docs(image_summaries, images_base64, image_filenames, \"image\")\n\n\n    def answer_question(self, question: str) -> str:\n        return self.rag_chain.run(question)\n\n\ndef main():\n    pipeline = DualMultimodalRAGPipelineSummaries(model_type=MODEL_TYPE,\n                                     store_path=VECTORSTORE_PATH_SUMMARIES_SEPARATE,\n                                     embedding_model=EMBEDDING_MODEL_TYPE)\n\n    texts_df, images_df = pipeline.load_data(INPUT_DATA)\n    texts, texts_filenames = texts_df[[\"text\"]][\"text\"].tolist(), texts_df[[\"doc_id\"]][\"doc_id\"].tolist()\n    images, image_filenames = images_df[[\"image_bytes\"]][\"image_bytes\"].tolist(), images_df[[\"doc_id\"]][\"doc_id\"].tolist()\n    img_base64_list, image_summaries = pipeline.image_summarizer.summarize(images, IMG_SUMMARIES_CACHE_DIR)\n\n    pipeline.index_data(texts,\n                        image_summaries=image_summaries,\n                        images_base64=img_base64_list,\n                        image_filenames=image_filenames,\n                        text_filenames=texts_filenames)\n\n    question = \"I want to change the behaviour of the stations to continue, if a moderate error occurrs. How can I do this?\"\n    answer = pipeline.answer_question(question)\n    relevant_images = pipeline.rag_chain.retrieved_images\n    relevant_texts = pipeline.rag_chain.retrieved_texts\n    print(\"Retrieved images:\", len(relevant_images), \", Retrieved texts:\", len(relevant_texts))  \n    print(answer)\n\n\nif __name__ == \"__main__\":\n    try:\n        main()\n    except Exception as e:\n        logging.error(\"An error occurred during execution\", exc_info=True)\n"}
{"type": "source_file", "path": "src/question_answering/rag/single_vector_store/rag_pipeline_clip.py", "content": "import pandas as pd\nimport logging\nimport os\nfrom question_answering.rag.single_vector_store.rag_chain import MultimodalRAGChain\nfrom question_answering.rag.single_vector_store.retrieval import ClipRetriever\nfrom langchain_openai import AzureChatOpenAI\nfrom typing import List\nfrom utils.azure_config import get_azure_config\nfrom utils.model_loading_and_prompting.llava import load_llava_model\nfrom rag_env import IMAGES_DIR, INPUT_DATA, MODEL_TYPE, VECTORSTORE_PATH_CLIP_SINGLE\n\n\nclass MultimodalRAGPipelineClip:\n    \"\"\"\n    Initializes the Multimodal RAG pipeline.\n    Answers a user query retrieving additional context from texts and images within a document collection.\n    Can be used with different models for answer generation (AzureOpenAI and LLaVA).\n    Uses CLIP as multimodal embedding model to embed the query, the images, and the texts into a single vector store.\n    \n    Attributes:\n        model_type (str): Type of the model to use for answer synthesis.\n        store_path (str): Path to the directory where the vector database is stored.\n        model: The model used for answer synthesis loaded based on `model_type`.\n        tokenizer: The tokenizer used for tokenization. Can be None.\n        text_summarizer (TextSummarizer): Can be used to summarize texts before retrieving them.\n        clip_retriever (ClipRetriever): Retrieval using CLIP embeddings for images.\n        rag_chain (MultimodalRAGChain): RAG chain performing the QA task.\n    \"\"\"\n    def __init__(self, model_type, store_path):\n        \n        config = get_azure_config()\n        \n        if model_type in config:\n            print(\"Using Azure model\")\n            azure_llm_config = config[model_type]\n            self.model = AzureChatOpenAI(\n                openai_api_version=azure_llm_config[\"openai_api_version\"],\n                azure_endpoint=azure_llm_config[\"openai_endpoint\"],\n                azure_deployment=azure_llm_config[\"deployment_name\"],\n                model=azure_llm_config[\"model_version\"],\n                api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n                max_tokens=400)\n            self.tokenizer = None\n            \n        else:\n            print(\"Using LLaVA model\")\n            self.model, self.tokenizer = load_llava_model(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n\n        # self.text_summarizer = TextSummarizer(model_type=\"gpt4\", cache_path=TEXT_SUMMARIES_CACHE_DIR)\n        self.clip_retriever = ClipRetriever(vectorstore_dir=store_path)\n        self.rag_chain = MultimodalRAGChain(self.model, self.tokenizer, self.clip_retriever.retriever)\n    \n    \n    def load_data(self, path: str) -> pd.DataFrame:\n        df = pd.read_parquet(path)\n        # drop duplicate entries in the texts column of the dataframe\n        texts = df.drop_duplicates(subset='text')[[\"text\", \"doc_id\"]]\n        return texts\n    \n    def summarize_data(self, texts: List[str]) -> List[str]:\n        text_summaries = self.text_summarizer.summarize(texts)\n        return text_summaries   \n    \n    def index_data(self, texts_df: pd.DataFrame, images_dir: str):\n        self.clip_retriever.add_documents(images_dir=images_dir, texts_df=texts_df)\n\n    def answer_question(self, question: str) -> str:\n        return self.rag_chain.run(question)\n\n\ndef main():\n    pipeline = MultimodalRAGPipelineClip(model_type=MODEL_TYPE, store_path=VECTORSTORE_PATH_CLIP_SINGLE)\n    texts_df = pipeline.load_data(INPUT_DATA)\n    pipeline.index_data(texts_df=texts_df, images_dir=IMAGES_DIR)\n\n    question = \"I want to change the behaviour of the stations to continue, if a moderate error occurrs. How can I do this?\"\n    answer = pipeline.answer_question(question)\n    relevant_docs = pipeline.rag_chain.retrieved_docs  \n    print(\"Retrieved images:\", len(relevant_docs[\"images\"]), \", Retrieved texts:\", len(relevant_docs[\"texts\"]))  \n    print(answer)\n\n\nif __name__ == \"__main__\":\n    try:\n        main()\n    except Exception as e:\n        logging.error(\"An error occurred during execution\", exc_info=True)\n"}
{"type": "source_file", "path": "src/utils/utils_logging.py", "content": "import logging\nimport sys\n\nutils_logger: logging.Logger = logging.getLogger(\"utils_logger\")\n\nif not utils_logger.hasHandlers():\n    stdout_handler_name = \"stdout_handler\"\n    utils_logger.setLevel(logging.DEBUG)\n    stdout_handler = logging.StreamHandler(sys.stdout)\n    stdout_formatter = logging.Formatter(\"%(asctime)-16s %(levelname)-10s %(message)s\")\n    stdout_handler.setFormatter(stdout_formatter)\n    stdout_handler.name = stdout_handler_name\n    utils_logger.addHandler(stdout_handler)\n\n\ndef add_file_logger(logging_path: str):\n    # Check whether stdout handler was already added\n    utils_logger.setLevel(logging.DEBUG)\n    handler = logging.FileHandler(logging_path, encoding=\"utf-8\", mode=\"a\")\n    formatter = logging.Formatter(\"%(asctime)-16s %(levelname)-10s %(message)s\")\n    handler.setFormatter(formatter)\n    utils_logger.addHandler(handler)\n"}
{"type": "source_file", "path": "src/question_answering/rag/single_vector_store/rag_chain.py", "content": "\nfrom collections import defaultdict\nfrom langchain_core.documents import Document\nfrom langchain_core.messages import HumanMessage\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnableLambda, RunnablePassthrough\nfrom langchain_openai import AzureChatOpenAI\nfrom utils.base64_utils.base64_utils import *\nfrom utils.model_loading_and_prompting.llava import llava_call\nfrom rag_env import REFERENCE_QA\nimport pandas as pd\nfrom typing import List\n\n\nclass MultimodalRAGChain:\n    def __init__(self, model, tokenizer, retriever):\n        \"\"\"\n        Multi-modal RAG chain\n        The steps are as follows: \n        1. Call the retriever to find relevant documents\n        2. Split the retrieved documents into images and texts\n        3. Create a QA prompt using the question and retrieved image/text context\n        4. Call the LLM and prompt it with the created prompt\n        5. Obtain the generated answer from the model\n        \n        :param model: The model used for answer generation.\n        :param tokenizer: The tokenizer used for tokenization.\n        :param retriever: The retriever used for text and image retrieval.\n        :param df: The Dataframe containing the user questions. The names of the files associated with the\n        user questions can be used to filter the document collection for retrieval.\n        \"\"\"\n        self.model = model\n        self.retriever = retriever\n        self.tokenizer = tokenizer\n        self.df = pd.read_excel(REFERENCE_QA)\n        \n        self.chain = (\n                {\n                    \"context\": self.retriever | RunnableLambda(self.split_image_text_types),\n                    \"question\": RunnablePassthrough(),\n                }\n                | RunnableLambda(self.img_prompt_func)\n                | RunnableLambda(self.call_model)\n                | StrOutputParser()\n        )\n        \n        \n    def run(self, question: str) -> str:\n        return self.chain.invoke(question)\n        \n\n    def call_model(self, prompt: str) -> RunnableLambda:\n        \"\"\"\n        Calls the model based on the model type.\n        \n        :return: A Langchain abstraction (RunnableLambda) to turn the model into a pipe-compatible function for the RAG chain.\n        \"\"\"\n        if self.tokenizer:\n            return RunnableLambda(self.call_llava)\n        else:\n            return RunnableLambda(self.model)\n\n        \n    def call_llava(self, inputs: dict) -> str:\n        \n        prompt = inputs['prompt']\n        image = inputs.get('image', None)\n        ans = llava_call(prompt, self.model, self.tokenizer, device=\"cuda\", image=image)\n        return ans\n    \n\n    def split_image_text_types(self, docs):\n        \"\"\"\n        Split base64-encoded images and texts.\n        \n        :return: A dictionary with separate entries for texts and base64-encoded images.\n        \"\"\"\n        b64_images = []\n        texts = []\n        for doc in docs:\n            # Check if the document is of type Document and extract page_content if so\n            if isinstance(doc, Document):\n                doc = doc.page_content\n            else:\n                doc = doc.decode('utf-8')\n            if looks_like_base64(doc) and is_image_data(doc):\n                doc = resize_base64_image(doc, size=(1300, 600))\n                b64_images.append(doc)\n            else:\n                texts.append(doc)\n                \n        self.retrieved_docs = defaultdict(list)\n        self.retrieved_images = b64_images\n        self.retrieved_texts = texts\n\n        return self.retrieved_docs\n\n\n    def img_prompt_func(self, data_dict: dict) -> dict:\n        \"\"\"\n        Constructs a dictionary containing the model-specific prompt and an image.\n        \"\"\"\n        \n        qa_prompt = \"\"\"You are an expert AI assistant that answers questions about manuals from the industrial domain.\\n\n        You will be given some context consisiting of text and/or image(s) that can be photos, screenshots, graphs, charts and other.\\n\n        Use this information from both text and image (if present) to provide an answer to the user question.\\n\n        Avoid expressions like: 'according to the text/image provided' and similar, and just answer the question directly.\"\"\"\n        \n        if type(self.model) == AzureChatOpenAI:\n            prompt_dict = self.azure_qa(data_dict, qa_prompt)\n        else:\n            prompt_dict = self.llava_qa(data_dict, qa_prompt)\n            \n        return prompt_dict\n    \n    \n    def llava_qa(self, inputs: dict, qa_prompt: str) -> dict:\n        \"\"\"\n        Constructs a prompt for question answering using the formatting required by LLaVA.\n        \"\"\"\n        \n        formatted_texts = \"\\n\".join(inputs[\"context\"][\"texts\"])\n    \n        prompt = f\"[INST]{'<image>' if inputs['context']['images'] else ' '}\\n{qa_prompt}\\nUser-provided question: {inputs['question']}\\n\\nText:\\n{formatted_texts}[/INST]\"\n       \n        if inputs['context']['images']:\n             # pass always only the first image as llava cannot handle multiple images\n            image = inputs['context']['images'][0]\n            image = decode_image_to_bytes(image)\n            image = Image.open(io.BytesIO(image))\n        else:\n            image = None  \n        \n        return {\"prompt\": prompt, \"image\": image}\n        \n        \n    def azure_qa(self, data_dict: dict, qa_prompt: str) -> List[HumanMessage]:\n        \"\"\"\n        Constructs a prompt for question answering using the formatting required by AzureOpenAI.\n        \"\"\"\n        \n        formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n        messages = []\n\n        # Adding image(s) to the messages if present\n        if data_dict[\"context\"][\"images\"]:\n            # if multiple images should be added\n            # for image in data_dict[\"context\"][\"images\"]:\n            # add only the first image since llava only supports 1 image\n            image = data_dict[\"context\"][\"images\"][0]\n            image_message = {\n                \"type\": \"image_url\",\n                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n            }\n            messages.append(image_message)\n\n        # Adding the text for answer generation\n        text_message = {\n            \"type\": \"text\",\n            \"text\": (\n                f\"{qa_prompt}\\n\"\n                f\"User-provided question: {data_dict['question']}\\n\\n\"\n                \"Text:\\n\"\n                f\"{formatted_texts}\"\n            ),\n        }\n        messages.append(text_message)\n        return [HumanMessage(content=messages)]\n"}
{"type": "source_file", "path": "src/utils/model_loading_and_prompting/llama3.py", "content": "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\nfrom transformers import pipeline\nfrom langchain_core.prompts import PromptTemplate\nimport torch\n\n# demo code for summarizing text using LLama3 8B Instruct from HuggingFace\n\ntext = \"\"\"Language model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the relationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce fine-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\nThere are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-specific architectures that\ninclude the pre-trained representations as additional features. The fine-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-specific parameters, and is trained on the\ndownstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\n\"\"\"\n\n\ntemplate = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system_msg}\n                <|start_header_id|>user<|end_header_id|>Text: {text}\\nSummary:\\n<|eot_id|>\n                <|start_header_id|>assistant<|end_header_id|>\"\"\"\n\nsystem_msg = \"\"\"You are an assistant tasked with summarizing text for retrieval.\n            These summaries will be embedded and used to retrieve the raw text elements.\n            Give a concise summary of the text that is well optimized for retrieval.\n            Only output the summary, no additional explanation.\\n\"\"\"\n            \n\nprompt = PromptTemplate.from_template(template)\n\ngeneration_params = {\n    \"do_sample\": True,\n    \"temperature\": 0.7,\n    \"top_p\": 0.95,\n    \"top_k\": 40,\n    \"max_new_tokens\": 512,\n    \"repetition_penalty\": 1.1\n}\n\n\npipe = pipeline(\n\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\", **generation_params\n)\n\nmodel = HuggingFacePipeline(pipeline=pipe)\n\n\nsummarize_chain = prompt | model\nres = summarize_chain.invoke({\"system_msg\": system_msg, \"text\": text})\noutput = res.split(\"<|start_header_id|>assistant<|end_header_id|>\",1)[1].strip()\nprint(\"Summary: \", output)"}
{"type": "source_file", "path": "src/question_answering/rag/separate_vector_stores/dual_rag_chain.py", "content": "import pandas as pd\nfrom collections import defaultdict\nfrom langchain_core.documents import Document\nfrom langchain_core.messages import HumanMessage\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnableLambda, RunnablePassthrough\nfrom langchain_openai import AzureChatOpenAI\nfrom utils.base64_utils.base64_utils import *\nfrom utils.model_loading_and_prompting.llava import llava_call\nfrom rag_env import REFERENCE_QA\n\n\nclass DualMultimodalRAGChain:\n    def __init__(self, model, tokenizer, text_retriever, image_retriever):\n        \"\"\"\n        Multi-modal RAG chain with separate vector stores and retrievers for texts and images\n        The steps are as follows:\n        1. Call the retrievers to find relevant texts and images\n        2. Create dictionary with retrieved texts or images\n        3. Create a QA prompt using the question and retrieved image/text context\n        4. Call the LLM and prompt it with the created prompt\n        5. Obtain the generated answer from the model\n        \n        :param model: The model used for answer generation.\n        :param tokenizer: The tokenizer used for tokenization.\n        :param text_retriever: The retriever used for text retrieval.\n        :param image_retriever: The retriever used for image retrieval.\n        :param df: The Dataframe containing the user questions. The names of the files associated with the\n        user questions can be used to filter the document collection for retrieval.\n        \"\"\"\n        self.model = model\n        self.tokenizer = tokenizer\n        self.text_retriever = text_retriever\n        self.image_retriever = image_retriever\n        self.df = pd.read_excel(REFERENCE_QA)\n        \n        self.chain = (\n            {\n                \"text_context\": self.text_retriever | RunnableLambda(self.split_image_text_types),\n                \"image_context\": self.image_retriever | RunnableLambda(self.split_image_text_types),\n                \"question\": RunnablePassthrough(),\n            }\n            | RunnableLambda(self.img_prompt_func)\n            | RunnableLambda(self.call_model)\n            | StrOutputParser()\n        )\n\n        self.retrieved_docs = defaultdict(list)\n        self.retrieved_images = []\n        self.retrieved_texts = []\n\n        \n    def run(self, question):\n        return self.chain.invoke(question)\n        \n\n    def call_model(self, prompt):\n        \"\"\"  \n        Calls the model based on the model type.\n        \n        :return: A Langchain abstraction (RunnableLambda) to turn the model into a pipe-compatible function for the RAG chain.  \n        \"\"\" \n        if self.tokenizer:\n            return RunnableLambda(self.call_llava)\n        else:\n            return RunnableLambda(self.model)\n\n        \n    def call_llava(self, inputs):\n        \n        prompt = inputs['prompt']\n        image = inputs.get('image', None)\n        ans = llava_call(prompt, self.model, self.tokenizer, device=\"cuda\", image=image)\n        return ans\n\n\n    def split_image_text_types(self, docs):\n        \"\"\"\n        Split base64-encoded images and texts.\n        \n        :return: A dictionary with separate entries for texts and base64-encoded images.\n        \"\"\"\n        b64_images = []\n        texts = []\n        is_image = False\n        for doc in docs:\n            # Check if the document is of type Document and extract page_content if so\n            if isinstance(doc, Document):\n                doc = doc.page_content\n            else:\n                doc = doc.decode('utf-8')\n            if looks_like_base64(doc) and is_image_data(doc):\n                is_image = True\n                doc = resize_base64_image(doc, size=(1300, 600))\n                b64_images.append(doc)\n            else:\n                texts.append(doc)\n                \n        if is_image:\n            self.retrieved_images = b64_images\n            return {\"images\": b64_images, \"texts\": []}\n        else:\n            self.retrieved_texts = texts\n            return {\"images\": [], \"texts\": texts}\n\n\n    def img_prompt_func(self, data_dict):\n        \"\"\"\n        Constructs a dictionary containing the model-specific prompt and an image.\n        \"\"\"\n        \n        qa_prompt = \"\"\"You are an expert AI assistant that answers questions about manuals from the industrial domain.\\n\n        You will be given some context consisiting of text and/or image(s) that can be photos, screenshots, graphs, charts and other.\\n\n        Use this information from both text and image (if present) to provide an answer to the user question.\\n\n        Avoid expressions like: 'according to the text/image provided' and similar, and just answer the question directly.\"\"\"\n        \n        if type(self.model) == AzureChatOpenAI:\n            prompt = self.azure_qa(data_dict, qa_prompt)\n        else:\n            prompt = self.llava_qa(data_dict, qa_prompt)\n            \n        return prompt\n    \n    \n    def llava_qa(self, inputs, qa_prompt):\n        \"\"\"\n        Constructs a prompt for question answering using the formatting required by LLaVA.\n        \"\"\"\n        \n        formatted_texts = \"\\n\".join(inputs[\"text_context\"][\"texts\"])\n    \n        prompt = f\"[INST]{'<image>' if inputs['image_context']['images'] else ' '}\\n{qa_prompt}\\nUser-provided question: {inputs['question']}\\n\\nText:\\n{formatted_texts}[/INST]\"\n       \n        if inputs['image_context']['images']:\n             # pass always only the first image as llava cannot handle multiple images\n            image = inputs['image_context']['images'][0]\n            image = decode_image_to_bytes(image)\n            image = Image.open(io.BytesIO(image))\n        else:\n            image = None  \n        \n        return {\"prompt\": prompt, \"image\": image}\n\n        \n        \n    def azure_qa(self, data_dict, qa_prompt):\n        \"\"\"\n        Constructs a prompt for question answering using the formatting required by AzureOpenAI.\n        \"\"\"\n        if data_dict[\"text_context\"][\"texts\"]:\n            formatted_texts = \"Text:\\n\\n\".join(data_dict[\"text_context\"][\"texts\"])\n        else:\n            formatted_texts = \"\"\n        messages = []\n\n        # Adding image(s) to the messages if present\n        if data_dict[\"image_context\"][\"images\"]:\n            # if multiple images should be added\n            # for image in data_dict[\"image_context\"][\"images\"]:\n            # add only the first image since llava only supports 1 image\n            image = data_dict[\"image_context\"][\"images\"][0]\n            image_message = {\n                \"type\": \"image_url\",\n                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n            }\n            messages.append(image_message)\n\n        # Adding the text for answer generation\n        text_message = {\n            \"type\": \"text\",\n            \"text\": (\n                f\"{qa_prompt}\\n\"\n                f\"User-provided question: {data_dict['question']}\\n\\n\"\n                f\"{formatted_texts}\"\n            ),\n        }\n        messages.append(text_message)\n        print(\"Calling model...\")\n        return [HumanMessage(content=messages)]\n"}
{"type": "source_file", "path": "src/utils/base64_utils/base64_utils.py", "content": "import base64\nimport io\nimport os\nimport re\nfrom IPython.display import HTML, display\nfrom PIL import Image\n\n\ndef encode_image_from_bytes(image_bytes):\n    return base64.b64encode(image_bytes).decode(\"utf-8\")\n\n\ndef decode_image_to_bytes(b64_string):\n    return base64.b64decode(b64_string.encode(\"utf-8\"))\n\n\ndef encode_image_from_path(image_path):\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n    \n    \ndef load_image(inputs: dict) -> dict:\n    \"\"\"Load image from file or image bytes and encode it as base64.\"\"\"\n    image = inputs[\"image\"]\n    if os.path.exists(image):   # image is a file path\n        image_base64 = encode_image_from_path(image)\n    else:   # image is provided as bytes\n        image_base64 = encode_image_from_bytes(image)\n    return {\"image\": image_base64}\n\n\ndef looks_like_base64(sb):\n    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n\n\ndef is_image_data(b64data):\n    \"\"\"\n    Check if the base64 data is an image by looking at the start of the data\n    \"\"\"\n    image_signatures = {\n        b\"\\xFF\\xD8\\xFF\": \"jpg\",\n        b\"\\x89\\x50\\x4E\\x47\\x0D\\x0A\\x1A\\x0A\": \"png\",\n        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n    }\n    try:\n        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n        for sig, format in image_signatures.items():\n            if header.startswith(sig):\n                return True\n        return False\n    except Exception:\n        return False\n\n\ndef resize_base64_image(base64_string, size=(128, 128)):\n    \"\"\"\n    Resize an image encoded as a Base64 string\n    \"\"\"\n    # Decode the Base64 string\n    img_data = base64.b64decode(base64_string)\n    img = Image.open(io.BytesIO(img_data))\n\n    # Resize the image\n    resized_img = img.resize(size, Image.LANCZOS)\n\n    # Save the resized image to a bytes buffer\n    buffered = io.BytesIO()\n    resized_img.save(buffered, format=img.format)\n\n    # Encode the resized image to Base64\n    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n\n\ndef plt_img_base64(img_base64):\n    \"\"\"Display base64 encoded string as image\"\"\"\n    # Create an HTML img tag with the base64 string as the source\n    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n    # Display the image by rendering the HTML\n    display(HTML(image_html))\n"}
{"type": "source_file", "path": "src/rag_env.py", "content": "\"\"\"\nConfiguration for the RAG pipeline. Replace the paths with your own actual paths.\nThe data used here is small sample data to show the expected format, not the real data.\nThis toy data will not lead to good results.\n\"\"\"\n\n# model to use for answer synthesis and image summarization. ('gpt4_vision', otherwise LLaVA will be used)\nMODEL_TYPE  = 'gpt4_vision'\n# text embedding model, set to 'openai' to use text-embeding-3-small, otherwise bge-m3 will be used\nEMBEDDING_MODEL_TYPE = \"openai\"\n\n# excel file containing questions and reference answers\nREFERENCE_QA = r\"../../sample_data/reference_qa.xlsx\"\n\n# directory containing the pdf files from which to extract texts and images\nMANUALS_DIR = \"YOUR_PATH_HERE\"\n\n# parquet file where extracted texts and image bytes are stored\nINPUT_DATA = r'../../sample_data/extracted_texts_and_imgs.parquet'\n\n# directory where extracted images are stored\nIMAGES_DIR = r\"../../sample_data/images\"\n\n# directories containing csv files with text summaries or image summaries\nIMG_SUMMARIES_CACHE_DIR = r\"../../sample_data/image_summaries_test\"\nTEXT_SUMMARIES_CACHE_DIR = r\"../../sample_data/text_summaries\"\n\n# directories where vector stores are saved\nVECTORSTORE_PATH_CLIP_SINGLE = r\"../../sample_data/vec_and_doc_stores/clip\"\nVECTORSTORE_PATH_CLIP_SEPARATE = r\"../../sample_data/vec_and_doc_stores/clip_dual\"\nVECTORSTORE_PATH_SUMMARIES_SINGLE = r\"../../sample_data/vec_and_doc_stores/image_summaries\"\nVECTORSTORE_PATH_SUMMARIES_SEPARATE = r\"../../sample_data/vec_and_doc_stores/image_summaries_dual\"\nVECTORSTORE_PATH_IMAGE_ONLY = r\".../../sample_data/vec_and_doc_stores/image_only\"\nVECTORSTORE_PATH_TEXT_ONLY = r\".../../sample_data/vec_and_doc_stores/text_only\"\n\n# directory where the output of a RAG pipeline is stored\nRAG_OUTPUT_DIR = r\".../../sample_data/rag_outputs\"\n\n# directory where the evaluation results for a RAG pipeline are stored\nEVAL_RESULTS_PATH = r\"../../sample_data/rag_evaluation_results\"\n"}
{"type": "source_file", "path": "src/question_answering/rag/run_multimodal_rag.py", "content": "import os  \nimport pandas as pd\nfrom single_vector_store.rag_pipeline_clip import MultimodalRAGPipelineClip\nfrom separate_vector_stores.dual_rag_pipeline_clip import DualMultimodalRAGPipelineClip\nfrom single_vector_store.rag_pipeline_summaries import MultimodalRAGPipelineSummaries\nfrom separate_vector_stores.dual_rag_pipeline_summaries import DualMultimodalRAGPipelineSummaries\nfrom rag_env import *\n\n\nAZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n\n\ndef write_to_df(df, user_query, reference_answer, generated_answer, context, image, output_file):\n    df.loc[len(df)] = [user_query, reference_answer, generated_answer, context, image]\n    df.to_json(output_file, orient=\"records\", indent=2)\n\n\ndef process_dataframe(input_df, pipeline, output_file, output_df=None):\n    if not output_df:\n        columns = ['user_query', 'reference_answer', 'generated_answer', 'context', 'image']\n        output_df= pd.DataFrame(columns=columns)\n    for index, row in input_df.iterrows():\n        print(f\"Processing query no. {index+1}...\")\n        user_query = input_df[\"question\"][index]\n        print(\"USER QUERY:\\n\", user_query)\n        reference_answer = input_df[\"reference_answer\"][index]\n        print(\"REFERENCE ANSWER:\", reference_answer)\n        generated_answer = pipeline.answer_question(user_query)\n        print(\"GENERATED ANSWER:\\n\", generated_answer)\n        relevant_images = pipeline.rag_chain.retrieved_images\n        relevant_texts = pipeline.rag_chain.retrieved_texts\n        print(\"Retrieved images:\", len(relevant_images), \", Retrieved texts:\", len(relevant_texts))\n        context = \"\\n\".join(relevant_texts) if len(relevant_texts) > 0 else []\n        image = relevant_images[0] if len(relevant_images) > 0 else []\n        write_to_df(output_df, user_query, reference_answer, generated_answer, context, image, output_file)\n    return output_df\n\n\n\ndef run_pipeline_with_clip_single(model, input_df, vectorstore_path, images_dir, reference_qa, output_dir):\n    pipeline = MultimodalRAGPipelineClip(model_type=model, store_path=vectorstore_path)\n    texts_df = pipeline.load_data(input_df)\n    pipeline.index_data(texts_df=texts_df, images_dir=images_dir)\n\n    df = pd.read_excel(reference_qa)\n \n    output_file = os.path.join(output_dir, f\"rag_output_{model}_multimodal_clip_single.json\")\n    output_df = process_dataframe(df, pipeline, output_file)\n    return output_df\n\n\n\ndef run_pipeline_with_clip_dual(model, input_df, vectorstore_path, images_dir, reference_qa, output_dir, text_embedding_model):\n    pipeline = DualMultimodalRAGPipelineClip(model_type=model,\n                                             store_path=vectorstore_path,\n                                             text_embedding_model=text_embedding_model)\n    texts_df = pipeline.load_data(input_df)\n    texts, texts_filenames = texts_df[[\"text\"]][\"text\"].tolist(), texts_df[[\"doc_id\"]][\"doc_id\"].tolist()\n    \n    pipeline.index_data(images_dir=images_dir, texts=texts, text_summaries=texts, text_filenames=texts_filenames)\n\n    df = pd.read_excel(reference_qa)\n \n    output_file = os.path.join(output_dir, f\"rag_output_{model}_multimodal_clip_dual.json\")\n    output_df = process_dataframe(df, pipeline, output_file)\n    return output_df\n\n    \n    \ndef run_pipeline_with_summaries_single(qa_model, embedding_model, vectorstore_path, input_df, reference_qa, output_dir, img_summaries_dir):\n    summaries_pipeline = MultimodalRAGPipelineSummaries(model_type=qa_model,\n                                                        store_path=vectorstore_path,\n                                                        embedding_model=embedding_model)\n    \n    texts_df, images_df = summaries_pipeline.load_data(input_df)\n    texts, texts_filenames = texts_df[[\"text\"]][\"text\"].tolist(), texts_df[[\"doc_id\"]][\"doc_id\"].tolist()\n    images, image_filenames = images_df[[\"image_bytes\"]][\"image_bytes\"].tolist(), images_df[[\"doc_id\"]][\"doc_id\"].tolist()\n    img_base64_list, image_summaries = summaries_pipeline.image_summarizer.summarize(images, img_summaries_dir)\n    summaries_pipeline.index_data(image_summaries=image_summaries, \n                                  images_base64=img_base64_list, image_filenames=image_filenames, \n                                  texts = texts, text_filenames=texts_filenames)\n    \n    df = pd.read_excel(reference_qa)\n    \n    output_file = os.path.join(output_dir, f\"rag_output_{qa_model}_multimodal_summaries_single.json\")\n    output_df = process_dataframe(df, summaries_pipeline, output_file)\n    return output_df  \n\n\n\ndef run_pipeline_with_summaries_dual(qa_model, embedding_model, vectorstore_path, input_df, reference_qa, output_dir, img_summaries_dir):\n    summaries_pipeline = DualMultimodalRAGPipelineSummaries(model_type=qa_model,\n                                                        store_path=vectorstore_path, \n                                                        embedding_model=embedding_model)\n    \n    texts_df, images_df = summaries_pipeline.load_data(input_df)\n    texts, texts_filenames = texts_df[[\"text\"]][\"text\"].tolist(), texts_df[[\"doc_id\"]][\"doc_id\"].tolist()\n    images, image_filenames = images_df[[\"image_bytes\"]][\"image_bytes\"].tolist(), images_df[[\"doc_id\"]][\"doc_id\"].tolist()\n    img_base64_list, image_summaries = summaries_pipeline.image_summarizer.summarize(images, img_summaries_dir)\n    summaries_pipeline.index_data(image_summaries=image_summaries, \n                                  images_base64=img_base64_list, image_filenames=image_filenames, \n                                  texts = texts, text_filenames=texts_filenames)\n    \n    df = pd.read_excel(reference_qa)\n    \n    output_file = os.path.join(output_dir, f\"rag_output_{qa_model}_multimodal_summaries_dual.json\")\n    output_df = process_dataframe(df, summaries_pipeline, output_file)\n    return output_df  \n\n\n  \nif __name__ == \"__main__\":  \n    # uncomment one of the following options to run multimodal RAG either with CLIP embedings or with image summaries\n    # and either with a single vector store for both modalities or a dedicated one for each modality.\n    rag_results_clip_single = run_pipeline_with_clip_single(model=MODEL_TYPE, input_df=INPUT_DATA,\n                                                            vectorstore_path=VECTORSTORE_PATH_IMAGE_ONLY,\n                                                            images_dir=IMAGES_DIR, reference_qa=REFERENCE_QA,\n                                                            output_dir=RAG_OUTPUT_DIR)\n    \n    rag_results_clip_dual = run_pipeline_with_clip_dual(model=MODEL_TYPE, input_df=INPUT_DATA,\n                                                        vectorstore_path=VECTORSTORE_PATH_IMAGE_ONLY,\n                                                        images_dir=IMAGES_DIR, reference_qa=REFERENCE_QA, \n                                                        output_dir=RAG_OUTPUT_DIR, text_embedding_model=EMBEDDING_MODEL_TYPE)\n    \n    rag_results_summaries_single = run_pipeline_with_summaries_single(qa_model=MODEL_TYPE,\n                                                                      vectorstore_path=VECTORSTORE_PATH_IMAGE_ONLY,\n                                                                      embedding_model=EMBEDDING_MODEL_TYPE, input_df=INPUT_DATA,\n                                                                      reference_qa=REFERENCE_QA, output_dir=RAG_OUTPUT_DIR,\n                                                                      img_summaries_dir=IMG_SUMMARIES_CACHE_DIR)\n    \n    rag_results_summaries_single = run_pipeline_with_summaries_dual(qa_model=MODEL_TYPE, vectorstore_path=VECTORSTORE_PATH_IMAGE_ONLY,\n                                                                    embedding_model=EMBEDDING_MODEL_TYPE, input_df=INPUT_DATA,\n                                                                    reference_qa=REFERENCE_QA, output_dir=RAG_OUTPUT_DIR,\n                                                                    img_summaries_dir=IMG_SUMMARIES_CACHE_DIR)"}
{"type": "source_file", "path": "src/utils/model_loading_and_prompting/gpt4.py", "content": "import os\nimport requests\n\n# Configuration\nGPT4V_KEY = os.environ.get(\"GPT4V_API_KEY\")\nGPT4V_ENDPOINT = os.environ.get(\"GPT4V_ENDPOINT\")\n\n\ndef gpt4v_qa_prompt_template(system_prompt, question, context, image=None, temperature=0.7, top_p=0.95, max_tokens=300):\n    payload = {\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": f\"{system_prompt}\"\n                    }\n                ]\n            },\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": f\"{context}\\n\\nQuestion:\\n{question}\\n\"\n                    }\n                ]\n            }\n        ],\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"max_tokens\": max_tokens\n    }\n\n    if image:\n        img_dict = {\n            \"type\": \"image_url\",\n            \"image_url\": {\n                \"url\": f\"data:image/jpeg;base64,{image}\"\n            }\n        }\n        payload[\"messages\"][1][\"content\"].append(img_dict)\n\n    return payload\n\n\ndef gpt4v_dataset_generation_prompt_template(system_prompt, text, image=None, temperature=0.7, top_p=0.95,\n                                             max_tokens=300):\n    payload = {\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": f\"{system_prompt}\"\n                    }\n                ]\n            },\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": f\"{text}\\n\\nImage:\\n\"\n                    }\n                ]\n            }\n        ],\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"max_tokens\": max_tokens\n    }\n\n    if image:\n        img_dict = {\n            \"type\": \"image_url\",\n            \"image_url\": {\n                \"url\": f\"data:image/jpeg;base64,{image}\"\n            }\n        }\n        payload[\"messages\"][1][\"content\"].append(img_dict)\n\n    return payload\n\n\ndef gpt4v_call(system_prompt, text, task, question=None, image=None, temperature=0.7, top_p=0.95, max_tokens=300):\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"api-key\": GPT4V_KEY,\n    }\n\n    if task == \"qa\":\n        json_prompt = gpt4v_qa_prompt_template(system_prompt, question, text, image, temperature, top_p, max_tokens)\n    else:   # synthetic dataset generation\n        json_prompt = gpt4v_dataset_generation_prompt_template(system_prompt, text, image, temperature, top_p, max_tokens)\n\n    # Send request\n    try:\n        response = requests.post(GPT4V_ENDPOINT, headers=headers, json=json_prompt)\n        response.raise_for_status()  # Will raise an HTTPError if the HTTP request returned an unsuccessful status code\n    except requests.RequestException as e:\n        raise SystemExit(f\"Failed to make the request. Error: {e}\")\n\n    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n"}
{"type": "source_file", "path": "src/utils/model_loading_and_prompting/llava.py", "content": "from transformers import BitsAndBytesConfig, LlavaNextProcessor, LlavaNextForConditionalGeneration\nfrom PIL import Image\nimport io\nimport pandas as pd\nfrom typing import Tuple\nfrom rag_env import INPUT_DATA\n\n\nquantization_config = BitsAndBytesConfig(\n    load_in_8bit=True,\n    low_cpu_mem_usage=True,\n    use_flash_attention_2=True\n)\n\ndef format_prompt_with_image(prompt: str) -> str:\n    return f\"[INST] <image>\\n{prompt} [/INST]\"\n\n\ndef get_qa_prompt(model_id:str, system_prompt:str, question: str, context: str, image: Image=None) -> str:\n    if \"vicuna\" in model_id:\n        prompt = f\"USER:{'<image>' if image else ' '}\\n{system_prompt}\\n{context}\\n\\nQuestion:\\n{question}\\n\\nASSISTANT:\"\n    else:   # mistral\n        prompt = f\"[INST]{'<image>' if image else ' '}\\n{system_prompt}\\n{context}\\n\\nQuestion:\\n{question}\\n\\n[/INST]\"\n\n    return prompt\n\n\ndef get_dataset_generation_prompt(model_id: str, system_prompt: str, context: str, image: Image=None):\n    if \"vicuna\" in model_id:\n        prompt = f\"USER:{'<image>' if image else ' '}\\n{system_prompt}\\n{context}\\n\\nASSISTANT:\"\n    else:   # mistral\n        prompt = f\"[INST]{'<image>' if image else ' '}\\n{system_prompt}\\n{context}\\n\\n[/INST]\"\n\n    return prompt\n\n\ndef format_output(raw_output, processor: LlavaNextProcessor, prompt: str) -> str:\n    out = processor.decode(raw_output[0], skip_special_tokens=True)\n    out_prompt = prompt.replace(\"<image>\", \" \").strip()\n    formatted_output = out.replace(out_prompt, \"\").strip()\n\n    return formatted_output\n\n\ndef get_prompt(task: str, model_id: str, system_prompt: str, text: str, image: Image, question: str) -> str:\n    if task == \"qa\":\n        prompt = get_qa_prompt(model_id, system_prompt, question, text, image)\n    else:\n        prompt = get_dataset_generation_prompt(model_id, system_prompt, text, image)\n    return prompt\n\n\ndef llava_call(prompt: str, model: LlavaNextForConditionalGeneration, processor: LlavaNextProcessor, device: str, image: Image=None) -> str:\n\n    inputs = processor(prompt, image, return_tensors=\"pt\").to(device)\n    raw_output = model.generate(**inputs, max_new_tokens=300)\n    formatted_output = format_output(raw_output, processor, prompt)\n\n    return formatted_output\n\n\ndef load_llava_model(model_id: str) -> Tuple[LlavaNextForConditionalGeneration, LlavaNextProcessor]:\n    processor = LlavaNextProcessor.from_pretrained(model_id)\n    # uncomment to use quantized version of the model\n    # model = LlavaNextForConditionalGeneration.from_pretrained(model_id, quantization_config=quantization_config, device_map=\"auto\")\n    model = LlavaNextForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\")\n\n    return model, processor\n\n\nif __name__ == \"__main__\":\n    index = 56\n    device = \"cuda\"\n    model, processor = load_llava_model(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n    model = model.eval()\n    \n    df = pd.read_parquet(INPUT_DATA)\n\n    qa_system_prompt = \"You are an AI assistant that answers question from the industrial domain based on a given context. Use the information from the context to answer the question. \\nContext:\\n\"\n    qa_system_prompt_img = \"You are an AI assistant that answers question from the industrial domain based on a given context as text and image. Use both the information from text and image to answer the question. \\nContext:\\n\"\n\n    question = \"What are the possible positions of the manual operator and what colors are associated with each position?\"\n    context = df[\"text\"][index]\n    img_bytes = df[\"image_bytes\"][index]\n    image = Image.open(io.BytesIO(img_bytes))\n    \n    img_prompt = get_qa_prompt(\"llava-hf/llava-v1.6-mistral-7b-hf\", qa_system_prompt_img, question, context, image)\n    no_img_prompt = get_qa_prompt(\"llava-hf/llava-v1.6-mistral-7b-hf\", qa_system_prompt, question, context)\n\n    # get response from image and text context\n    print(\"============== Answer with image:\")\n    llava_response_img = llava_call(img_prompt, model, processor, device, image)\n    print(llava_response_img)\n    \n    # get response using text only\n    print(\"============== Answer without image:\")\n    llava_response_no_img = llava_call(no_img_prompt, model, processor, device)\n    print(llava_response_no_img)\n"}
{"type": "source_file", "path": "src/question_answering/rag/single_vector_store/retrieval.py", "content": "import os\nimport uuid\nimport pandas as pd\nfrom langchain.retrievers import MultiVectorRetriever\nfrom langchain_community.embeddings import HuggingFaceBgeEmbeddings\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_core.documents import Document\nfrom langchain_experimental.open_clip import OpenCLIPEmbeddings\nfrom langchain_openai import AzureOpenAIEmbeddings\nfrom langchain.storage import LocalFileStore\nfrom utils.azure_config import get_azure_config\nfrom rag_env import IMAGES_DIR\nfrom typing import List\n\n\nclass SummaryStoreAndRetriever:\n    \"\"\"  \n    A class providing a document store and a vector store to contain texts and images and their embeddings.\n    The class also provides a retriever to find documents that are relevant for a query.\n    Retrieval is performed using the embeddings in the vector store, but the documents contained in the\n    document store are returned. This allows image retrieval via the image summaries, while still ensuring\n    that the original images associated with the summaries are returned.\n  \n    Attributes:\n        embedding_model (str): Model used to embed the texts and image summaries.\n        store_path (str): Path where the vector and document stores should be saved.\n        docstore (LocalFileStore): Document store containing texts and images.\n        vectorstore (Chroma): Vector store containing embedded texts and image summaries.\n        retriever (MultiVectorRetriever): Retriever that encommpasses both a vector store and a document store.\n    \"\"\"   \n    def __init__(self, embedding_model, store_path=None):\n        \n        # embed using openai embedding model\n        if embedding_model == 'openai':\n            print(\"Using text-embedding-3-small\")\n            azure_embedding_config = get_azure_config()['text_embedding_3']\n            self.embeddings = AzureOpenAIEmbeddings(model=azure_embedding_config[\"model_version\"],\n                                                    azure_endpoint=azure_embedding_config[\"openai_endpoint\"],\n                                                    openai_api_version=azure_embedding_config[\"openai_api_version\"],\n                                                    openai_api_key=os.getenv(\"AZURE_OPENAI_API_KEY_EMBEDDING\"),\n                                                    chunk_size=64,\n                                                    show_progress_bar=True\n                                                    )\n        # embed with BGE embeddings\n        else:\n            print(\"Using BGE embeddings\")\n            model_name = \"BAAI/bge-m3\"\n            model_kwargs = {\"device\": \"cuda\"}\n            encode_kwargs = {\"normalize_embeddings\": True, \"batch_size\": 1, \"show_progress_bar\":True}\n            self.embeddings = HuggingFaceBgeEmbeddings(\n                model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n                )\n\n        self.store_path = store_path\n        vectorstore_dir = os.path.join(self.store_path, f\"{os.path.basename(self.store_path)}_vectorstore_{embedding_model}\")\n        docstore_dir = os.path.join(self.store_path, f\"{os.path.basename(self.store_path)}_docstore_{embedding_model}\")\n        \n        # Initialize the document store\n        self.docstore = LocalFileStore(docstore_dir)\n        self.id_key = \"doc_id\"\n        self.doc_ids = []\n\n        # Initialize the vector store with the embedding function\n        self.vectorstore = Chroma(\n            persist_directory=vectorstore_dir,\n            embedding_function=self.embeddings,\n            collection_name=f\"mm_rag_with_image_summaries_{embedding_model}_embeddings\"\n        )\n        results = self.vectorstore.get(include=[\"embeddings\", \"documents\", \"metadatas\"])\n        self.is_new_vectorstore = bool(results[\"embeddings\"])\n\n        if self.is_new_vectorstore:\n            print(f\"Vectorstore at path {vectorstore_dir} already exists\")\n\n        else:\n            print(f\"Creating new vectorstore and docstore at path {self.store_path}\")\n\n        self.retriever = MultiVectorRetriever(\n            vectorstore=self.vectorstore,\n            docstore=self.docstore,\n            id_key=self.id_key\n        )\n        self.retrieved_docs = []\n\n\n    def add_docs(self, doc_summaries: List[str], doc_contents: List[str], doc_filenames: List[str]):\n        \"\"\"\n        Add documents to the vector store and document store.\n        \n        :param doc_summaries: Either text or image summaries to be stored in the vector store.\n        :param doc_contents: The original texts or images to be stored in the document store.\n        :param doc_filenames: File names associated with the respective documents to be stored as additional metadata.\n        \"\"\"\n        if not self.is_new_vectorstore:\n            print(\"Adding documents...\")\n            doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n            summary_docs = [\n                Document(page_content=s, metadata={self.id_key: doc_ids[i], \"filename\": doc_filenames[i]})\n                for i, s in enumerate(doc_summaries)\n            ]\n            self.vectorstore.add_documents(summary_docs)\n            self.docstore.mset(list(zip(doc_ids, map(lambda x: str.encode(x), doc_contents))))\n        else:\n            print(\"Documents have already been added before, skipping...\")\n\n\n    def retrieve(self, query: str, limit: int) -> List[Document]:\n        \"\"\"\n        Retrieve the most relevant documents based on the query.\n        \"\"\"\n        self.retrieved_docs = self.retriever.invoke(query, limit=limit)\n\n        return self.retrieved_docs\n    \n    \n    \n    \nclass ClipRetriever:\n    \"\"\"  \n    A class providing a vector store to contain texts and images embedded with the multimodal embedding model CLIP.\n    The vector store can also be used as retriever to find documents that are relevant for a query.\n  \n    Attributes:\n        vectorstore_dir (str): Path where the vector store should be saved.\n        images_dir (str): Directory containing the images to be embedded.\n        vectorstore (Chroma): Vector store containing embedded texts and images.\n    \"\"\"\n    def __init__(self, vectorstore_dir, images_dir=IMAGES_DIR):\n        self.images_dir = images_dir\n\n        # Create chroma vectorstore\n        self.vectorstore = Chroma(\n            collection_name=\"mm_rag_clip_photos\", \n            embedding_function=OpenCLIPEmbeddings(),\n            persist_directory=vectorstore_dir\n        )\n\n        results = self.vectorstore.get(include=[\"embeddings\", \"documents\", \"metadatas\"])\n        self.is_new_vectorstore = bool(results[\"embeddings\"])\n\n        if self.is_new_vectorstore:\n            print(f\"Vectorstore at path {vectorstore_dir} already exists\")\n\n        else:\n            print(f\"Creating new vectorstore at path {vectorstore_dir}\")\n\n        # Make retriever\n        self.retriever = self.vectorstore.as_retriever()\n        self.retrieved_docs = []\n        \n        \n        \n    def add_documents(self, images_dir: str=None, texts_df: pd.DataFrame=None):\n        \"\"\"\n        Add images and texts to the vector store.\n        \n        :param images_dir: Directory containing the images to be embedded.\n        :param texts_df: Dataframe containing the texts to be embedded.\n        \"\"\"\n        if not self.is_new_vectorstore:\n            if images_dir:\n                image_uris = self.extract_image_uris(images_dir)  \n                print(f\"Found {len(image_uris)} images\")\n                \n                # Convert the list of URIs to a list of dictionaries  \n                image_metadatas = [{'filename': self.extract_manual_name(uri)} for uri in image_uris]  \n                \n                print(\"Adding images to vectorstore...\")\n                self.vectorstore.add_images(uris=image_uris, metadatas=image_metadatas)\n                \n            if texts_df is not None:\n                texts = texts_df[\"text\"].to_list()\n                text_metadatas = [{'filename': doc_id} for doc_id in texts_df['doc_id']] \n\n                print(\"Adding texts to vectorstore...\")\n                self.vectorstore.add_texts(texts=texts, metadatas=text_metadatas)\n        else:\n            print(\"Documents have already been added before, skipping...\")\n\n\n        \n    def extract_image_uris(self, root_path: str, image_extension: str=\".png\") -> List[str]:\n        image_uris = []\n        for subdir, dirs, files in os.walk(root_path):\n            for file in files:\n                if file.endswith(image_extension):\n                    image_uris.append(os.path.join(subdir, file))\n        return sorted(image_uris)\n\n\n\n    def extract_manual_name(self, uri: str) -> str:\n        # Split the URI into parts\n        parts = uri.split('/')\n        # The directory name is the second to last element\n        directory_name = parts[-2]\n        # Append \".pdf\" to the directory name\n        return f\"{directory_name}.pdf\"\n    \n    \n    def retrieve(self, query: str, limit: int) -> List[Document]:\n        \"\"\"\n        Retrieve the most relevant documents based on the query.\n        \"\"\"\n        self.retrieved_docs = self.retriever.invoke(query, limit=limit)\n\n        return self.retrieved_docs\n"}
