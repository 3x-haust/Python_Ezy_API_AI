{"repo_info": {"repo_name": "omron-subnet", "repo_owner": "inference-labs-inc", "repo_url": "https://github.com/inference-labs-inc/omron-subnet"}}
{"type": "source_file", "path": "__init__.py", "content": "\n"}
{"type": "source_file", "path": "neurons/_miner/circuit_manager.py", "content": "from __future__ import annotations\nimport time\nimport hashlib\nimport boto3\nimport traceback\nimport threading\nfrom typing import Optional, Dict\nfrom substrateinterface import Keypair\nfrom pathlib import Path\nfrom botocore.config import Config\nimport bittensor as bt\nfrom pydantic import BaseModel\nimport cli_parser\n\n\nclass CircuitCommitment(BaseModel):\n    \"\"\"\n    Represents a circuit commitment with signed URLs for validator access.\n\n    Attributes:\n        vk_hash (str): SHA256 hash of vk.key - this is committed on-chain\n        file_urls (Dict[str, str]): Map of filenames to signed URLs for validator download\n        expiry (int): Unix timestamp when URLs expire\n        signature (str): Hotkey signature of commitment data\n        last_modified (int): Unix timestamp of when circuit files were last modified\n    \"\"\"\n\n    vk_hash: str\n    file_urls: Dict[str, str]\n    expiry: int\n    signature: str\n    last_modified: int\n\n\nclass CircuitManager:\n    \"\"\"\n    Manages circuit file monitoring, cloud storage uploads, and chain commitments.\n\n    This class ensures synchronization between:\n    1. Local circuit files\n    2. Cloud storage (R2/S3)\n    3. On-chain commitments\n\n    It periodically checks for changes in circuit files and automatically:\n    - Uploads modified files to cloud storage\n    - Updates on-chain commitments\n    - Provides signed URLs to validators\n\n    Security:\n    - Ensures hash commitment matches VK before providing URLs\n    - Maintains lockstep between chain state and file state\n    - Prevents race conditions in updates\n    \"\"\"\n\n    def __init__(\n        self,\n        wallet: Keypair,\n        netuid: int,\n        circuit_dir: str,\n        storage_config: dict,\n        check_interval: int = 60,\n        existing_vk_hash: Optional[str] = None,\n    ):\n        \"\"\"\n        Initialize the CircuitManager.\n\n        Args:\n            wallet: Bittensor wallet for signing\n            netuid: Network UID\n            circuit_dir: Directory containing circuit files\n            storage_config: Storage configuration dict containing:\n                - provider: 'r2' or 's3'\n                - bucket: bucket name\n                - account_id: account ID (required for R2)\n                - access_key: access key ID\n                - secret_key: secret key\n                - region: region (required for S3)\n            check_interval: How often to check for changes (seconds)\n        \"\"\"\n        self.wallet = wallet\n        self.netuid = netuid\n        self.circuit_dir = Path(circuit_dir)\n        self.check_interval = check_interval\n        self.storage_config = storage_config\n        self.bucket = storage_config[\"bucket\"]\n\n        self.subtensor = bt.subtensor(config=cli_parser.config)\n        bt.logging.debug(\n            \"Created dedicated subtensor instance for circuit manager using cli config\"\n        )\n\n        if not storage_config or not storage_config[\"provider\"]:\n            raise ValueError(\n                \"Storage configuration is required to initialize CircuitManager.\"\n            )\n\n        if storage_config[\"provider\"] == \"r2\":\n            self.storage = boto3.client(\n                \"s3\",\n                endpoint_url=f\"https://{storage_config['account_id']}.r2.cloudflarestorage.com\",\n                aws_access_key_id=storage_config[\"access_key\"],\n                aws_secret_access_key=storage_config[\"secret_key\"],\n                config=Config(\n                    retries={\"max_attempts\": 3},\n                    connect_timeout=5,\n                    read_timeout=30,\n                    region_name=\"auto\",\n                ),\n            )\n        else:\n            self.storage = boto3.client(\n                \"s3\",\n                aws_access_key_id=storage_config[\"access_key\"],\n                aws_secret_access_key=storage_config[\"secret_key\"],\n                region_name=storage_config[\"region\"],\n                config=Config(\n                    retries={\"max_attempts\": 3}, connect_timeout=5, read_timeout=30\n                ),\n            )\n\n        self.current_vk_hash = existing_vk_hash\n        self.last_upload_time: Optional[int] = None\n        self._current_object_keys: Optional[Dict[str, str]] = None\n        self._lock = threading.Lock()\n        self._stop_event = threading.Event()\n\n        if existing_vk_hash and self._calculate_vk_hash() == existing_vk_hash:\n            try:\n                required_files = [\"vk.key\", \"pk.key\", \"model.compiled\", \"settings.json\"]\n                self._current_object_keys = {\n                    fname: f\"{existing_vk_hash}/{fname}\" for fname in required_files\n                }\n                self.last_upload_time = int(time.time())\n                bt.logging.info(\n                    f\"Initialized with existing VK hash: {existing_vk_hash[:8]}...\"\n                )\n            except Exception as e:\n                bt.logging.error(f\"Failed to initialize with existing hash: {str(e)}\")\n\n        self._monitor_thread = threading.Thread(\n            target=self._monitor_circuit_files, daemon=True\n        )\n        self._monitor_thread.start()\n\n    def stop(self):\n        \"\"\"Stop the monitoring thread.\"\"\"\n        self._stop_event.set()\n        self._monitor_thread.join()\n\n    def _calculate_vk_hash(self) -> Optional[str]:\n        \"\"\"\n        Calculate SHA256 hash of vk.key.\n\n        Returns:\n            str: Hex digest of hash, or None if file not found\n        \"\"\"\n        vk_path = self.circuit_dir / \"vk.key\"\n        if not vk_path.exists():\n            return None\n\n        with open(vk_path, \"rb\") as f:\n            return hashlib.sha256(f.read()).hexdigest()\n\n    def _upload_circuit_files(self) -> Dict[str, str]:\n        \"\"\"\n        Upload all circuit files to storage (R2 or S3).\n\n        Returns:\n            Dict[str, str]: Map of filenames to object keys\n        \"\"\"\n        required_files = [\n            \"vk.key\",\n            \"pk.key\",\n            \"model.compiled\",\n            \"settings.json\",\n        ]\n\n        uploaded = {}\n        for fname in required_files:\n            fpath = self.circuit_dir / fname\n            if not fpath.exists():\n                bt.logging.warning(f\"Missing required file: {fname}\")\n                continue\n\n            key = f\"{self._calculate_vk_hash()}/{fname}\"\n            self.storage.upload_file(str(fpath), self.bucket, key)\n            uploaded[fname] = key\n\n        return uploaded\n\n    def _get_signed_urls(self, object_keys: Dict[str, str]) -> Dict[str, str]:\n        \"\"\"\n        Generate signed URLs for all uploaded files.\n\n        Args:\n            object_keys: Map of filenames to storage object keys\n\n        Returns:\n            Dict[str, str]: Map of filenames to signed URLs\n        \"\"\"\n        urls = {}\n        for fname, key in object_keys.items():\n            url = self.storage.generate_presigned_url(\n                \"get_object\",\n                Params={\"Bucket\": self.bucket, \"Key\": key},\n                ExpiresIn=3600,\n            )\n            urls[fname] = url\n        return urls\n\n    def _commit_to_chain(self, vk_hash: str, max_retries: int = 3) -> bool:\n        \"\"\"\n        Commit circuit hash to chain with retries.\n\n        Args:\n            vk_hash: Hash to commit\n            max_retries: Maximum number of retry attempts\n\n        Returns:\n            bool: True if commit succeeded, False otherwise\n        \"\"\"\n        for attempt in range(max_retries):\n            try:\n                bt.logging.debug(\n                    f\"Committing hash {vk_hash[:8]}... (attempt {attempt + 1}/{max_retries})\"\n                )\n                self.subtensor.commit(\n                    wallet=self.wallet,\n                    netuid=self.netuid,\n                    data=vk_hash,\n                )\n                bt.logging.success(f\"Successfully committed hash {vk_hash[:8]}...\")\n                return True\n            except Exception as e:\n                if attempt < max_retries - 1:\n                    bt.logging.warning(f\"Commit attempt {attempt + 1} failed: {str(e)}\")\n                    time.sleep(2**attempt)\n                else:\n                    bt.logging.error(\n                        f\"All commit attempts failed for hash {vk_hash[:8]}\"\n                    )\n                    bt.logging.debug(f\"Final error: {str(e)}\")\n                    return False\n        return False\n\n    def _monitor_circuit_files(self):\n        \"\"\"\n        Continuously monitor circuit files for changes.\n\n        This method:\n        1. Checks VK hash periodically\n        2. If changed, uploads all files\n        3. Updates chain commitment\n        \"\"\"\n        while not self._stop_event.is_set():\n            try:\n                with self._lock:\n                    new_vk_hash = self._calculate_vk_hash()\n                    if not new_vk_hash:\n                        bt.logging.warning(\"No verification key found\")\n                        time.sleep(self.check_interval)\n                        continue\n\n                    if new_vk_hash != self.current_vk_hash:\n                        bt.logging.info(\"Circuit files changed, uploading...\")\n\n                        object_keys = self._upload_circuit_files()\n                        upload_time = int(time.time())\n\n                        if not self._commit_to_chain(new_vk_hash):\n                            bt.logging.error(\n                                \"Failed to commit to chain, will retry next cycle\"\n                            )\n                            time.sleep(self.check_interval)\n                            continue\n\n                        self.current_vk_hash = new_vk_hash\n                        self.last_upload_time = upload_time\n                        self._current_object_keys = object_keys\n\n                        bt.logging.success(\n                            f\"Upload complete. Updated circuit commitment: {new_vk_hash[:8]}...\"\n                        )\n\n            except Exception as e:\n                bt.logging.error(f\"Error in circuit monitor: {str(e)}\")\n                bt.logging.error(traceback.format_exc())\n            time.sleep(self.check_interval)\n\n    def get_current_commitment(self) -> Optional[CircuitCommitment]:\n        \"\"\"\n        Get current circuit commitment with signed URLs.\n\n        This is called by the Competition synapse handler.\n\n        Returns:\n            CircuitCommitment: Current commitment info with signed URLs,\n                             or None if not ready\n        \"\"\"\n        with self._lock:\n\n            if not (\n                self.current_vk_hash\n                and self.last_upload_time\n                and self._current_object_keys\n            ):\n                return None\n\n            urls = self._get_signed_urls(self._current_object_keys)\n            expiry = int(time.time()) + 3600\n\n            commitment = CircuitCommitment(\n                vk_hash=self.current_vk_hash,\n                file_urls=urls,\n                expiry=expiry,\n                last_modified=self.last_upload_time,\n                signature=self.wallet.hotkey.sign(\n                    f\"{self.current_vk_hash}:{expiry}\"\n                ).hex(),\n            )\n\n            return commitment\n"}
{"type": "source_file", "path": "neurons/_validator/competitions/services/data_source.py", "content": "from abc import ABC, abstractmethod\nfrom typing import Optional\nimport torch\nimport os\nimport json\nimport numpy as np\nimport bittensor as bt\nfrom urllib.parse import urlparse\nimport requests\nimport random\nimport zipfile\nimport cv2\nfrom tqdm import tqdm\n\n\nclass ImageProcessor:\n    @staticmethod\n    def normalize(\n        img: torch.Tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n    ) -> torch.Tensor:\n        mean = torch.tensor(mean).view(-1, 1, 1)\n        std = torch.tensor(std).view(-1, 1, 1)\n        return (img - mean) / std\n\n    @staticmethod\n    def to_tensor(img: np.ndarray) -> torch.Tensor:\n        img = img.transpose((2, 0, 1))  # HWC to CHW\n        img = torch.from_numpy(img).float()\n        return img / 255.0\n\n\nclass CompetitionDataProcessor(ABC):\n    @abstractmethod\n    def process(self, data: torch.Tensor) -> torch.Tensor:\n        pass\n\n\nclass DefaultDataProcessor(CompetitionDataProcessor):\n    def process(self, data: torch.Tensor) -> torch.Tensor:\n        return data\n\n\nclass CompetitionDataSource(ABC):\n    def __init__(\n        self,\n        competition_directory: str,\n        processor: Optional[CompetitionDataProcessor] = None,\n    ):\n        self.competition_directory = competition_directory\n        self.processor = processor or DefaultDataProcessor()\n        self._load_config()\n\n    def _load_config(self):\n        config_path = os.path.join(\n            self.competition_directory, \"competition_config.json\"\n        )\n        try:\n            with open(config_path) as f:\n                self.config = json.load(f)\n        except Exception as e:\n            bt.logging.error(f\"Error loading competition config: {e}\")\n            self.config = {}\n\n    @abstractmethod\n    def get_benchmark_data(self) -> Optional[torch.Tensor]:\n        pass\n\n    @abstractmethod\n    def sync_data(self) -> bool:\n        pass\n\n\nclass RandomDataSource(CompetitionDataSource):\n    def get_benchmark_data(self) -> Optional[torch.Tensor]:\n        try:\n            input_shape = self.config[\"circuit_settings\"][\"input_shape\"]\n            data = torch.randn(*input_shape)\n            return self.processor.process(data)\n        except Exception as e:\n            bt.logging.error(f\"Error generating random data: {e}\")\n            return None\n\n    def sync_data(self) -> bool:\n        return True\n\n\nclass RemoteDataSource(CompetitionDataSource):\n    def __init__(\n        self, competition_directory: str, processor: CompetitionDataProcessor = None\n    ):\n        super().__init__(competition_directory, processor)\n        self.processed_path = os.path.join(self.competition_directory, \"processed_64\")\n        self.data_config = self.config.get(\"data_source\", {})\n\n    def sync_data(self) -> bool:\n        try:\n            if os.path.exists(self.processed_path) and os.listdir(self.processed_path):\n                return True\n\n            os.makedirs(self.processed_path, exist_ok=True)\n            extracted_path = os.path.join(self.competition_directory, \"extracted\")\n            os.makedirs(extracted_path, exist_ok=True)\n            zip_path = os.path.join(self.competition_directory, \"age.zip\")\n\n            url = self.data_config.get(\"url\", \"https://storage.omron.ai/age.zip\")\n            if not self._validate_url(url):\n                bt.logging.error(f\"Invalid URL: {url}\")\n                return False\n\n            bt.logging.info(\"Downloading dataset...\")\n            response = requests.get(url, stream=True)\n            total_size = int(response.headers.get(\"content-length\", 0))\n\n            with (\n                open(zip_path, \"wb\") as f,\n                tqdm(\n                    desc=\"Downloading\", total=total_size, unit=\"iB\", unit_scale=True\n                ) as pbar,\n            ):\n                for data in response.iter_content(chunk_size=1024):\n                    size = f.write(data)\n                    pbar.update(size)\n\n            bt.logging.info(\"Extracting zip...\")\n            with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n                zip_ref.extractall(extracted_path)\n\n            bt.logging.info(\"Processing images to 64x64...\")\n            for img_name in tqdm(os.listdir(extracted_path)):\n                if img_name.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n                    img_path = os.path.join(extracted_path, img_name)\n                    try:\n                        img = cv2.imread(img_path)\n                        if img is not None:\n                            img = cv2.resize(img, (64, 64))\n                            cv2.imwrite(\n                                os.path.join(self.processed_path, img_name), img\n                            )\n                    except Exception as e:\n                        bt.logging.warning(f\"Failed to process {img_name}: {e}\")\n\n            os.remove(zip_path)\n            import shutil\n\n            shutil.rmtree(extracted_path)\n\n            bt.logging.info(f\"Dataset processed and saved to {self.processed_path}\")\n            return True\n\n        except Exception as e:\n            bt.logging.error(f\"Failed to download/process dataset: {e}\")\n            return False\n\n    def _validate_url(self, url: str) -> bool:\n        try:\n            parsed = urlparse(url)\n            return any(\n                domain in parsed.netloc\n                for domain in [\n                    \"r2.cloudflarestorage.com\",\n                    \"s3.amazonaws.com\",\n                    \".r2.dev\",\n                    \"storage.omron.ai\",\n                ]\n            )\n        except Exception:\n            return False\n\n    def get_benchmark_data(self) -> Optional[torch.Tensor]:\n        try:\n            if not os.path.exists(self.processed_path):\n                if not self.sync_data():\n                    return None\n\n            image_files = [\n                f\n                for f in os.listdir(self.processed_path)\n                if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n            ]\n\n            if not image_files:\n                bt.logging.error(\"No processed images found\")\n                return None\n\n            # Randomly select an image\n            img_path = os.path.join(self.processed_path, random.choice(image_files))\n            img = cv2.imread(img_path)\n            if img is None:\n                bt.logging.error(f\"Failed to load image: {img_path}\")\n                return None\n\n            # Convert BGR to RGB\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n            # Convert to tensor and normalize\n            tensor = ImageProcessor.to_tensor(img)\n            tensor = ImageProcessor.normalize(tensor)\n\n            # Add batch dimension\n            tensor = tensor.unsqueeze(0)\n\n            return self.processor.process(tensor)\n\n        except Exception as e:\n            bt.logging.error(f\"Error getting benchmark data: {e}\")\n            return None\n"}
{"type": "source_file", "path": "neurons/_validator/api/certificate_manager.py", "content": "import os\nimport time\nfrom OpenSSL import crypto\nimport bittensor as bt\nfrom constants import ONE_YEAR\n\n\nclass CertificateManager:\n    def __init__(self, cert_path: str):\n        self.cert_path = cert_path\n        self.key_path = os.path.join(cert_path, \"key.pem\")\n        self.cert_file = os.path.join(cert_path, \"cert.pem\")\n\n    def ensure_valid_certificate(self, external_ip: str) -> None:\n        if not os.path.exists(self.cert_file):\n            bt.logging.warning(\n                \"Certificate not found. Generating new self-signed certificate.\"\n            )\n            os.makedirs(self.cert_path, exist_ok=True)\n            self._generate_certificate(external_ip)\n\n    def _generate_certificate(self, cn: str) -> None:\n        key = crypto.PKey()\n        key.generate_key(crypto.TYPE_RSA, 4096)\n\n        cert = crypto.X509()\n        cert.get_subject().CN = cn\n        cert.set_serial_number(int(time.time()))\n        cert.gmtime_adj_notBefore(0)\n        cert.gmtime_adj_notAfter(2 * ONE_YEAR)\n        cert.set_issuer(cert.get_subject())\n        cert.set_pubkey(key)\n        cert.sign(key, \"sha256\")\n\n        with open(self.cert_file, \"wb\") as f:\n            f.write(crypto.dump_certificate(crypto.FILETYPE_PEM, cert))\n\n        with open(self.key_path, \"wb\") as f:\n            f.write(crypto.dump_privatekey(crypto.FILETYPE_PEM, key))\n"}
{"type": "source_file", "path": "neurons/_validator/api/__init__.py", "content": "from __future__ import annotations\nimport os\nimport traceback\nfrom fastapi import (\n    FastAPI,\n    WebSocket,\n    WebSocketDisconnect,\n    WebSocketException,\n    Request,\n    Response,\n)\n\nfrom fastapi.responses import JSONResponse\n\nfrom jsonrpcserver import (\n    async_dispatch,\n    Success,\n    Error,\n    InvalidParams,\n)\n\nfrom fastapi.routing import APIRoute, APIWebSocketRoute\nimport bittensor as bt\nfrom _validator.models.poc_rpc_request import ProofOfComputationRPCRequest\nfrom _validator.models.pow_rpc_request import ProofOfWeightsRPCRequest\nimport hashlib\nfrom constants import (\n    MAX_SIGNATURE_LIFESPAN,\n    MAINNET_TESTNET_UIDS,\n    EXTERNAL_REQUEST_QUEUE_TIME_SECONDS,\n)\nfrom _validator.config import ValidatorConfig\nimport base64\nimport substrateinterface\nimport time\nfrom _validator.api.cache import ValidatorKeysCache\nimport threading\nimport uvicorn\nfrom _validator.api.certificate_manager import CertificateManager\nfrom _validator.api.websocket_manager import WebSocketManager\nimport asyncio\nfrom OpenSSL import crypto\nfrom deployment_layer.circuit_store import circuit_store\n\n\napp = FastAPI()\n\nrecent_requests: dict[str, int] = {}\n\n\n@app.middleware(\"http\")\nasync def rate_limiter(request: Request, call_next):\n    if request.url.path == \"/rpc\":\n        return await call_next(request)\n\n    ip = request.client.host\n    if _should_rate_limit(ip):\n        return Response(status_code=429)\n    return await call_next(request)\n\n\ndef _should_rate_limit(ip: str):\n    if ip in recent_requests.keys():\n        return max(0, time.time() - recent_requests[ip]) < 1\n    recent_requests[ip] = time.time()\n    return False\n\n\nclass ValidatorAPI:\n    def __init__(self, config: ValidatorConfig):\n        self.config = config\n        self.external_requests_queue: list[\n            ProofOfWeightsRPCRequest | ProofOfComputationRPCRequest\n        ] = []\n        self.ws_manager = WebSocketManager()\n        self.recent_requests: dict[str, int] = {}\n        self.validator_keys_cache = ValidatorKeysCache(config)\n        self.server_thread: threading.Thread | None = None\n        self.pending_requests: dict[str, asyncio.Event] = {}\n        self.request_results: dict[str, dict[str, any]] = {}\n        self.is_testnet = config.bt_config.subtensor.network == \"test\"\n        self._setup_api()\n\n    def _setup_api(self) -> None:\n        if not self.config.api.enabled:\n            bt.logging.info(\"API Disabled: --ignore-external-requests flag present\")\n            return\n\n        bt.logging.debug(\"Starting WebSocket API server...\")\n\n        for route in self._get_routes():\n            app.routes.append(route)\n\n        if self.config.api.certificate_path:\n            cert_manager = CertificateManager(self.config.api.certificate_path)\n            cert_manager.ensure_valid_certificate(\n                bt.axon(self.config.wallet).external_ip\n            )\n            self.commit_cert_hash()\n\n        self.start_server()\n        bt.logging.success(\"Ready to serve external requests\")\n\n    async def handle_ws(self, websocket: WebSocket):\n        if (\n            self.config.api.verify_external_signatures\n            and not await self.validate_connection(websocket.headers)\n        ):\n            raise WebSocketException(code=3000, reason=\"Connection validation failed\")\n\n        try:\n            await self.ws_manager.connect(websocket)\n            async for data in websocket.iter_text():\n                response = await async_dispatch(\n                    data,\n                    context=websocket,\n                    methods={\n                        \"omron.proof_of_weights\": self.handle_proof_of_weights,\n                        \"omron.proof_of_computation\": self.handle_proof_of_computation,\n                    },\n                )\n                await websocket.send_text(str(response))\n        except WebSocketDisconnect:\n            bt.logging.debug(\"Client disconnected normally\")\n        except Exception as e:\n            bt.logging.error(f\"WebSocket error: {str(e)}\")\n        finally:\n            await self.ws_manager.disconnect(websocket)\n\n    def get_circuits(self, request: Request) -> None:\n\n        try:\n            return JSONResponse(circuit_store.list_circuit_metadata())\n        except Exception:\n            bt.logging.error(\"Failed to fetch circuit metadata from circuit store.\")\n            traceback.print_exc()\n        return Response(status_code=500)\n\n    def _get_routes(self) -> list[APIWebSocketRoute | APIRoute]:\n        rpc_endpoint = APIWebSocketRoute(\"/rpc\", self.handle_ws)\n        get_circuits_endpoint = APIRoute(\"/circuits\", self.get_circuits)\n        return [rpc_endpoint, get_circuits_endpoint]\n\n    async def handle_proof_of_weights(\n        self, websocket: WebSocket, **params: dict[str, object]\n    ) -> dict[str, object]:\n        if not websocket.headers.get(\"x-netuid\"):\n            return InvalidParams(\n                \"Missing x-netuid header (required for proof of weights requests)\"\n            )\n\n        evaluation_data = params.get(\"evaluation_data\")\n        weights_version = params.get(\"weights_version\")\n\n        if not evaluation_data:\n            return InvalidParams(\"Missing evaluation data\")\n\n        try:\n            netuid = websocket.headers.get(\"x-netuid\")\n            if netuid is None:\n                return InvalidParams(\"Missing x-netuid header\")\n\n            if self.is_testnet:\n                testnet_uids = [\n                    uid[0] for uid in MAINNET_TESTNET_UIDS if uid[1] == int(netuid)\n                ]\n                if not testnet_uids:\n                    return InvalidParams(\n                        f\"No testnet UID mapping found for mainnet UID {netuid}\"\n                    )\n                netuid = testnet_uids[0]\n\n            netuid = int(netuid)\n            try:\n                external_request = ProofOfWeightsRPCRequest(\n                    evaluation_data=evaluation_data,\n                    netuid=netuid,\n                    weights_version=weights_version,\n                )\n            except ValueError as e:\n                return InvalidParams(str(e))\n\n            self.pending_requests[external_request.hash] = asyncio.Event()\n            self.external_requests_queue.insert(0, external_request)\n            bt.logging.success(\n                f\"External request with hash {external_request.hash} added to queue\"\n            )\n            try:\n                await asyncio.wait_for(\n                    self.pending_requests[external_request.hash].wait(),\n                    timeout=external_request.circuit.timeout\n                    + EXTERNAL_REQUEST_QUEUE_TIME_SECONDS,\n                )\n                result = self.request_results.pop(external_request.hash, None)\n\n                if result[\"success\"]:\n                    bt.logging.success(\n                        f\"External request with hash {external_request.hash} processed successfully\"\n                    )\n                    return Success(result)\n                bt.logging.error(\n                    f\"External request with hash {external_request.hash} failed to process\"\n                )\n                return Error(9, \"Request processing failed\")\n            except asyncio.TimeoutError:\n                bt.logging.error(\n                    f\"External request with hash {external_request.hash} timed out\"\n                )\n                return Error(9, \"Request processing failed\", \"Request timed out\")\n            finally:\n                self.pending_requests.pop(external_request.hash, None)\n\n        except Exception as e:\n            bt.logging.error(f\"Error processing request: {str(e)}\")\n            traceback.print_exc()\n            return Error(9, \"Request processing failed\", str(e))\n\n    async def handle_proof_of_computation(\n        self, websocket: WebSocket, **params: dict[str, object]\n    ) -> dict[str, object]:\n        input_json = params.get(\"input\")\n        circuit_id = params.get(\"circuit\")\n\n        if not input_json:\n            return InvalidParams(\"Missing input to the circuit\")\n\n        if not circuit_id:\n            return InvalidParams(\"Missing circuit id\")\n\n        try:\n            try:\n                external_request = ProofOfComputationRPCRequest(\n                    circuit_id=circuit_id,\n                    inputs=input_json,\n                )\n            except ValueError as e:\n                bt.logging.error(\n                    f\"Error creating proof of computation request: {str(e)}\"\n                )\n                return InvalidParams(str(e))\n\n            self.pending_requests[external_request.hash] = asyncio.Event()\n            self.external_requests_queue.insert(0, external_request)\n            bt.logging.success(\n                f\"External request with hash {external_request.hash} added to queue\"\n            )\n            try:\n                await asyncio.wait_for(\n                    self.pending_requests[external_request.hash].wait(),\n                    timeout=external_request.circuit.timeout\n                    + EXTERNAL_REQUEST_QUEUE_TIME_SECONDS,\n                )\n                result = self.request_results.pop(external_request.hash, None)\n\n                if result[\"success\"]:\n                    bt.logging.success(\n                        f\"External request with hash {external_request.hash} processed successfully\"\n                    )\n                    return Success(result)\n                bt.logging.error(\n                    f\"External request with hash {external_request.hash} failed to process\"\n                )\n                return Error(9, \"Request processing failed\")\n            except asyncio.TimeoutError:\n                bt.logging.error(\n                    f\"External request with hash {external_request.hash} timed out\"\n                )\n                return Error(9, \"Request processing failed\", \"Request timed out\")\n            finally:\n                self.pending_requests.pop(external_request.hash, None)\n\n        except Exception as e:\n            bt.logging.error(f\"Error processing request: {str(e)}\")\n            traceback.print_exc()\n            return Error(9, \"Request processing failed\", str(e))\n\n    def start_server(self):\n        \"\"\"Start the uvicorn server in a separate thread\"\"\"\n        self.server_thread = threading.Thread(\n            target=uvicorn.run,\n            args=(app,),\n            kwargs={\n                \"host\": \"0.0.0.0\",\n                \"port\": self.config.api.port,\n                \"ssl_keyfile\": os.path.join(\n                    self.config.api.certificate_path, \"key.pem\"\n                ),\n                \"ssl_certfile\": os.path.join(\n                    self.config.api.certificate_path, \"cert.pem\"\n                ),\n            },\n            daemon=True,\n        )\n        self.server_thread.start()\n        if not self.config.api.serve_axon:\n            return\n        try:\n            bt.logging.info(f\"Serving axon on port {self.config.api.port}\")\n            axon = bt.axon(\n                wallet=self.config.wallet, external_port=self.config.api.port\n            )\n            existing_axon = self.config.metagraph.axons[self.config.user_uid]\n            if (\n                existing_axon\n                and existing_axon.port == axon.external_port\n                and existing_axon.ip == axon.external_ip\n            ):\n                bt.logging.debug(\n                    f\"Axon already serving on ip {axon.external_ip} and port {axon.external_port}\"\n                )\n                return\n            axon.serve(self.config.bt_config.netuid, self.config.subtensor)\n            bt.logging.success(\"Axon served\")\n        except Exception as e:\n            bt.logging.error(f\"Error serving axon: {e}\")\n\n    async def stop(self):\n        \"\"\"Gracefully shutdown the WebSocket server\"\"\"\n        for connection in self.ws_manager.active_connections:\n            await connection.close()\n        self.ws_manager.active_connections.clear()\n\n    async def validate_connection(self, headers) -> bool:\n        required_headers = [\"x-timestamp\", \"x-origin-ss58\", \"x-signature\"]\n        if not all(header in headers for header in required_headers):\n            bt.logging.warning(\n                f\"Incoming request is missing required headers: {required_headers}\"\n            )\n            return False\n\n        try:\n            timestamp = int(headers[\"x-timestamp\"])\n            current_time = time.time()\n            if current_time - timestamp > MAX_SIGNATURE_LIFESPAN:\n                bt.logging.warning(\n                    f\"Incoming request signature timestamp {timestamp} is too old. Current time: {current_time}\"\n                )\n                return False\n\n            ss58_address = headers[\"x-origin-ss58\"]\n            signature = base64.b64decode(headers[\"x-signature\"])\n\n            public_key = substrateinterface.Keypair(ss58_address=ss58_address)\n            if not public_key.verify(str(timestamp).encode(), signature):\n                bt.logging.warning(\n                    f\"Incoming request signature verification failed for address {ss58_address}\"\n                )\n                return False\n\n            if \"x-netuid\" in headers:\n                netuid = int(headers[\"x-netuid\"])\n                return await self.validator_keys_cache.check_validator_key(\n                    ss58_address, netuid\n                )\n            else:\n                return await self.validator_keys_cache.check_whitelisted_key(\n                    ss58_address\n                )\n\n        except Exception as e:\n            bt.logging.error(f\"Validation error: {str(e)}\")\n            traceback.print_exc()\n            return False\n\n    def commit_cert_hash(self):\n        \"\"\"Commit the cert hash to the chain. Clients will use this for certificate pinning.\"\"\"\n\n        existing_commitment = None\n        try:\n            existing_commitment = self.config.subtensor.get_commitment(\n                self.config.subnet_uid, self.config.user_uid\n            )\n        except Exception:\n            bt.logging.warning(\n                \"Error getting existing commitment. Assuming no commitment exists.\"\n            )\n            traceback.print_exc()\n\n        if not self.config.api.certificate_path:\n            return\n\n        cert_path = os.path.join(self.config.api.certificate_path, \"cert.pem\")\n        if not os.path.exists(cert_path):\n            return\n\n        with open(cert_path, \"rb\") as f:\n            cert_data = f.read()\n            cert = crypto.load_certificate(crypto.FILETYPE_PEM, cert_data)\n            cert_der = crypto.dump_certificate(crypto.FILETYPE_ASN1, cert)\n            cert_hash = hashlib.sha256(cert_der).hexdigest()\n            if cert_hash != existing_commitment:\n                try:\n                    self.config.subtensor.commit(\n                        self.config.wallet, self.config.subnet_uid, cert_hash\n                    )\n                    bt.logging.success(\"Certificate hash committed to chain.\")\n                except Exception as e:\n                    bt.logging.error(f\"Error committing certificate hash: {str(e)}\")\n                    traceback.print_exc()\n            else:\n                bt.logging.debug(\"Certificate hash already committed to chain.\")\n\n    def set_request_result(self, request_hash: str, result: dict[str, any]):\n        \"\"\"Set the result for a pending request and signal its completion.\"\"\"\n        if request_hash in self.pending_requests:\n            self.request_results[request_hash] = result\n            self.pending_requests[request_hash].set()\n"}
{"type": "source_file", "path": "neurons/__init__.py", "content": "\"\"\"\nThis version number is used to trigger automatic updates.\n\"\"\"\n\n__version__ = \"7.4.6\"\n"}
{"type": "source_file", "path": "neurons/_validator/competitions/1/data_processor.py", "content": "import torch\nimport torch.nn.functional as F\n\nfrom _validator.competitions.services.data_source import (\n    CompetitionDataProcessor,\n)\n\n\nclass DatasetProcessor(CompetitionDataProcessor):\n    def __init__(self, noise_scale: float = 0.01, jitter_scale: float = 0.1):\n        self.noise_scale = noise_scale\n        self.jitter_scale = jitter_scale\n\n    def process(self, inputs: torch.Tensor) -> torch.Tensor:\n        batch_size = inputs.shape[0]\n\n        if torch.rand(1) > 0.5:\n            inputs = torch.flip(inputs, dims=[3])\n\n        jitter = (\n            1.0\n            + (torch.rand(batch_size, 1, 1, 1, device=inputs.device) * 2 - 1)\n            * self.jitter_scale\n        )\n        inputs = inputs * jitter\n\n        angle = (torch.rand(1) * 20 - 10) * (3.14159 / 180)\n        cos_theta = torch.cos(angle)\n        sin_theta = torch.sin(angle)\n        rotation_matrix = torch.tensor(\n            [[cos_theta, -sin_theta, 0], [sin_theta, cos_theta, 0]],\n            device=inputs.device,\n        )\n        grid = F.affine_grid(\n            rotation_matrix.unsqueeze(0).expand(batch_size, -1, -1),\n            inputs.size(),\n            align_corners=True,\n        )\n        inputs = F.grid_sample(inputs, grid, align_corners=True)\n\n        noise = torch.randn_like(inputs) * self.noise_scale\n        perturbed = inputs + noise\n\n        return torch.clamp(perturbed, -1, 1)\n"}
{"type": "source_file", "path": "neurons/_validator/competitions/services/circuit_manager.py", "content": "import os\nimport shutil\nimport json\nimport aiohttp\nimport bittensor as bt\nfrom urllib.parse import urlparse\nimport traceback\nfrom constants import ONE_HOUR\nfrom protocol import Competition\nimport asyncio\n\n\nclass CircuitManager:\n    def __init__(self, temp_dir: str, competition_id: int, dendrite: bt.dendrite):\n        self.temp_dir = temp_dir\n        self.competition_id = competition_id\n        self.dendrite = dendrite\n        os.makedirs(temp_dir, exist_ok=True)\n\n    def cleanup_temp_files(self, circuit_dir: str):\n        try:\n            if os.path.exists(circuit_dir):\n                shutil.rmtree(circuit_dir)\n            temp_files = [\n                os.path.join(\"/tmp/omron\", f)\n                for f in [\"temp_proof.json\", \"temp_witness.json\"]\n            ]\n            for f in temp_files:\n                if os.path.exists(f):\n                    os.remove(f)\n        except Exception as e:\n            bt.logging.warning(f\"Error cleaning up temp files: {e}\")\n\n    def _validate_url(self, url: str) -> bool:\n        \"\"\"Validate that URL is from R2 or S3\"\"\"\n        try:\n            parsed = urlparse(url)\n            return any(\n                domain in parsed.netloc\n                for domain in [\n                    \"r2.cloudflarestorage.com\",\n                    \"s3.amazonaws.com\",\n                    \".r2.dev\",\n                ]\n            )\n        except Exception:\n            return False\n\n    async def _download_large_file(\n        self, session: aiohttp.ClientSession, url: str, file_path: str\n    ) -> bool:\n        try:\n            async with session.get(url, timeout=ONE_HOUR) as response:\n                response.raise_for_status()\n                total_size = int(response.headers.get(\"content-length\", 0))\n\n                with open(file_path, \"wb\") as f:\n                    downloaded = 0\n                    chunk_size = 1024 * 1024\n                    last_log_time = 0\n\n                    async for chunk in response.content.iter_chunked(chunk_size):\n                        if chunk:\n                            f.write(chunk)\n                            downloaded += len(chunk)\n\n                            current_time = asyncio.get_event_loop().time()\n                            if current_time - last_log_time >= 5:\n                                progress = (\n                                    (downloaded / total_size * 100)\n                                    if total_size > 0\n                                    else 0\n                                )\n                                bt.logging.debug(\n                                    f\"Download progress: {progress:.2f}% ({downloaded}/{total_size} bytes)\"\n                                )\n                                last_log_time = current_time\n\n                return True\n        except Exception as e:\n            bt.logging.error(f\"Error downloading file: {str(e)}\")\n            if os.path.exists(file_path):\n                os.remove(file_path)\n            return False\n\n    async def download_files(self, axon: bt.axon, hash: str, circuit_dir: str) -> bool:\n        try:\n            bt.logging.info(\n                f\"Starting download of circuit files from miner {axon.ip}:{axon.port}\"\n            )\n\n            if not self.dendrite:\n                bt.logging.error(\"Dendrite not initialized\")\n                return False\n\n            synapse = Competition(\n                id=self.competition_id,\n                hash=hash,\n                file_name=\"commitment\",\n            )\n\n            response = await self.dendrite.forward(\n                axons=[axon],\n                synapse=synapse,\n                timeout=60,\n                deserialize=True,\n            )\n\n            if not response or not response[0]:\n                bt.logging.error(\"No response from axon\")\n                return False\n\n            response_data = response[0]\n            if isinstance(response_data, dict):\n                response_synapse = Competition(**response_data)\n            else:\n                response_synapse = response_data\n\n            if not response_synapse.commitment:\n                bt.logging.warning(\"No commitment data in response\")\n                return False\n\n            try:\n                commitment = json.loads(response_synapse.commitment)\n            except json.JSONDecodeError:\n                bt.logging.error(\"Invalid commitment data\")\n                return False\n\n            if \"signed_urls\" not in commitment:\n                bt.logging.error(\"No signed URLs in commitment data\")\n                return False\n\n            signed_urls = commitment[\"signed_urls\"]\n            required_files = [\"vk.key\", \"pk.key\", \"settings.json\", \"model.compiled\"]\n            all_files_downloaded = True\n\n            timeout = aiohttp.ClientTimeout(total=ONE_HOUR, connect=60)\n            conn = aiohttp.TCPConnector(force_close=True, limit=1)\n\n            async with aiohttp.ClientSession(\n                connector=conn, timeout=timeout\n            ) as session:\n                for file_name in required_files:\n                    if file_name not in signed_urls:\n                        bt.logging.error(f\"Missing signed URL for {file_name}\")\n                        all_files_downloaded = False\n                        break\n\n                    url = signed_urls[file_name]\n                    if not self._validate_url(url):\n                        bt.logging.error(f\"Invalid URL for {file_name}: {url}\")\n                        all_files_downloaded = False\n                        break\n\n                    file_path = os.path.join(circuit_dir, file_name)\n                    bt.logging.debug(f\"Starting download of {file_name} from {url}\")\n\n                    if not await self._download_large_file(session, url, file_path):\n                        all_files_downloaded = False\n                        break\n\n                    bt.logging.info(\n                        f\"Successfully downloaded {file_name} ({os.path.getsize(file_path)} bytes)\"\n                    )\n\n            if all_files_downloaded:\n                bt.logging.success(f\"Successfully downloaded all files for {hash[:8]}\")\n                return True\n            else:\n                bt.logging.error(f\"Failed to download all files for {hash[:8]}\")\n                return False\n\n        except Exception as e:\n            bt.logging.error(f\"Error downloading circuit files: {e}\")\n            bt.logging.error(f\"Stack trace: {traceback.format_exc()}\")\n            return False\n"}
{"type": "source_file", "path": "neurons/_validator/competitions/competition_manager.py", "content": "from __future__ import annotations\nimport time\nfrom typing import Optional, Dict, Any\nimport bittensor as bt\nfrom pydantic import BaseModel\nfrom datetime import datetime\nimport json\nimport os\nimport enum\nimport cli_parser\nfrom utils.gc_logging import gc_log_competition_metrics\n\n\nclass CompetitionStatus(enum.Enum):\n    PENDING = \"Pending\"\n    ACTIVE = \"Active\"\n    COMPLETED = \"Completed\"\n    INACTIVE = \"Inactive\"\n\n\nclass CompetitionMetrics(BaseModel):\n    \"\"\"Competition metrics for logging\"\"\"\n\n    competition_id: int\n    name: str\n    status: CompetitionStatus\n    accuracy_weight: float\n    active_participants: int\n    total_circuits_evaluated: int\n    avg_accuracy: float\n    avg_proof_size: float\n    avg_response_time: float\n    sota_score: float\n    sota_hotkey: Optional[str]\n    sota_proof_size: float\n    sota_response_time: float\n    timestamp: int\n\n\nclass DataSourceConfig(BaseModel):\n    type: str = \"random\"\n    url: Optional[str] = None\n    format: str = \"npz\"  # npz, zip, tar\n    input_key: str = \"inputs\"  # For npz, or subdir name for zip/tar\n    input_pattern: Optional[str] = None  # For matching files in zip/tar\n    input_transform: Optional[str] = None  # resize, normalize, etc\n    transform_params: Dict[str, Any] = {}\n\n\nclass CompetitionConfig(BaseModel):\n    \"\"\"Configuration for a competition\"\"\"\n\n    id: int\n    name: str\n    description: str\n    start_timestamp: int\n    end_timestamp: int\n    baseline_model_path: str\n    max_accuracy_weight: float = 1.0\n    min_accuracy_weight: float = 0.0\n    data_source: Optional[DataSourceConfig] = None\n    circuit_settings: Dict[str, Any] = {}\n\n\nclass CompetitionState(BaseModel):\n    \"\"\"Current state of a competition\"\"\"\n\n    is_active: bool = False\n    current_accuracy_weight: float = 0.0\n    last_sync_timestamp: int = 0\n    total_circuits_evaluated: int = 0\n    active_participants: int = 0\n    current_metrics: Optional[CompetitionMetrics] = None\n\n\nclass CompetitionManager:\n    \"\"\"Manages competition lifecycle and state\"\"\"\n\n    def __init__(self, config_dir: str):\n        self.config_dir = config_dir\n        self.state_file = os.path.join(config_dir, \"competition_state.json\")\n        self.config_file = os.path.join(config_dir, \"competition_config.json\")\n        self.current_competition: Optional[CompetitionConfig] = None\n        self.state = CompetitionState()\n        self.wallet = bt.wallet(config=cli_parser.config)\n\n        self._load_state()\n        self._load_config()\n\n    def _load_state(self):\n        \"\"\"Load competition state from disk\"\"\"\n        try:\n            if os.path.exists(self.state_file):\n                with open(self.state_file, \"r\") as f:\n                    data = json.load(f)\n                    self.state = CompetitionState(**data)\n        except Exception as e:\n            bt.logging.error(f\"Error loading competition state: {e}\")\n            self.state = CompetitionState()\n\n    def _save_state(self):\n        \"\"\"Save current competition state to disk\"\"\"\n        try:\n            with open(self.state_file, \"w\") as f:\n                json.dump(self.state.dict(), f, indent=4)\n        except Exception as e:\n            bt.logging.error(f\"Error saving competition state: {e}\")\n\n    def _load_config(self):\n        \"\"\"Load competition config from disk\"\"\"\n        try:\n            if os.path.exists(self.config_file):\n                with open(self.config_file, \"r\") as f:\n                    data = json.load(f)\n                    self.current_competition = CompetitionConfig(**data)\n        except Exception as e:\n            bt.logging.error(f\"Error loading competition config: {e}\")\n            self.current_competition = None\n\n    def update_competition_state(self) -> None:\n        \"\"\"Update competition state based on current time\"\"\"\n        if not self.current_competition:\n            return\n\n        current_time = int(time.time())\n\n        is_active = (\n            self.current_competition.start_timestamp\n            <= current_time\n            <= self.current_competition.end_timestamp\n        )\n\n        if is_active:\n            accuracy_weight = self.current_competition.max_accuracy_weight\n        else:\n            accuracy_weight = self.current_competition.min_accuracy_weight\n\n        self.state.is_active = is_active\n        self.state.current_accuracy_weight = accuracy_weight\n        self.state.last_sync_timestamp = current_time\n\n        self._save_state()\n\n    def get_accuracy_weight(self) -> float:\n        \"\"\"Get current accuracy weight for scoring\"\"\"\n        return self.state.current_accuracy_weight\n\n    def is_competition_active(self) -> bool:\n        \"\"\"Check if competition is currently active\"\"\"\n        return self.state.is_active\n\n    def get_competition_status(self) -> dict:\n        \"\"\"Get current competition status\"\"\"\n        if not self.current_competition:\n            return {\"status\": CompetitionStatus.INACTIVE}\n\n        current_time = int(time.time())\n\n        if current_time < self.current_competition.start_timestamp:\n            time_to_start = self.current_competition.start_timestamp - current_time\n            return {\n                \"status\": CompetitionStatus.PENDING,\n                \"competition_id\": self.current_competition.id,\n                \"name\": self.current_competition.name,\n                \"starts_in\": f\"{time_to_start // 3600} hours\",\n                \"accuracy_weight\": self.state.current_accuracy_weight,\n            }\n        elif current_time > self.current_competition.end_timestamp:\n            return {\n                \"status\": CompetitionStatus.COMPLETED,\n                \"competition_id\": self.current_competition.id,\n                \"name\": self.current_competition.name,\n                \"ended\": datetime.fromtimestamp(\n                    self.current_competition.end_timestamp\n                ).isoformat(),\n                \"accuracy_weight\": self.state.current_accuracy_weight,\n            }\n        else:\n            time_remaining = self.current_competition.end_timestamp - current_time\n            return {\n                \"status\": CompetitionStatus.ACTIVE,\n                \"competition_id\": self.current_competition.id,\n                \"name\": self.current_competition.name,\n                \"time_remaining\": f\"{time_remaining // 3600} hours\",\n                \"accuracy_weight\": self.state.current_accuracy_weight,\n            }\n\n    def log_metrics(self, metrics: dict):\n        \"\"\"Log competition metrics.\"\"\"\n        if not self.current_competition:\n            return\n\n        try:\n\n            comp_metrics = {\n                \"competition_id\": self.current_competition.id,\n                \"name\": self.current_competition.name,\n                \"status\": self.get_competition_status()[\"status\"].value,\n                \"accuracy_weight\": self.state.current_accuracy_weight,\n                \"total_circuits_evaluated\": self.state.total_circuits_evaluated,\n                \"timestamp\": int(time.time()),\n                **metrics,\n            }\n\n            gc_log_competition_metrics(comp_metrics, self.wallet.hotkey)\n        except Exception as e:\n            bt.logging.error(f\"Error logging metrics: {e}\")\n\n    def increment_circuits_evaluated(self):\n        \"\"\"Increment the total circuits evaluated counter\"\"\"\n        self.state.total_circuits_evaluated += 1\n        self._save_state()\n\n    def update_active_participants(self, count: int):\n        \"\"\"Update the count of active participants\"\"\"\n        self.state.active_participants = count\n        self._save_state()\n"}
{"type": "source_file", "path": "neurons/_validator/competitions/services/onnx_runner.py", "content": "import sys\nimport numpy as np\nimport onnxruntime as ort\nimport traceback\n\n\ndef run_inference(model_path: str, input_path: str, output_path: str) -> None:\n    try:\n        session = ort.InferenceSession(model_path)\n        input_name = session.get_inputs()[0].name\n        input_data = np.load(input_path)\n\n        options = ort.RunOptions()\n        options.log_severity_level = 3\n\n        output_names = [output.name for output in session.get_outputs()]\n        outputs = session.run(output_names, {input_name: input_data}, options)\n\n        flattened = []\n        for out in outputs:\n            flattened.extend(out.flatten())\n        final_output = np.array(flattened)\n        np.save(output_path, final_output)\n    except Exception as e:\n        print(f\"Error running inference: {str(e)}\")\n        print(f\"Traceback:\\n{traceback.format_exc()}\")\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 4:\n        print(\"Usage: python onnx_runner.py <model_path> <input_path> <output_path>\")\n        sys.exit(1)\n    run_inference(sys.argv[1], sys.argv[2], sys.argv[3])\n"}
{"type": "source_file", "path": "neurons/_validator/competitions/__init__.py", "content": "from enum import Enum\n\n\nclass Competition(Enum):\n    \"\"\"\n    Enum for competitions.\n    \"\"\"\n\n    FIRST = 1\n"}
{"type": "source_file", "path": "neurons/_validator/competitions/services/circuit_validator.py", "content": "import os\nimport json\nimport bittensor as bt\nfrom constants import MAX_CIRCUIT_SIZE_GB\n\n\nclass CircuitValidator:\n    REQUIRED_FILES = [\n        \"vk.key\",\n        \"pk.key\",\n        \"settings.json\",\n        \"model.compiled\",\n    ]\n\n    REQUIRED_SETTINGS = {\n        \"run_args\": {\n            \"input_visibility\": \"Private\",\n            \"output_visibility\": \"Public\",\n            \"param_visibility\": \"Private\",\n            \"commitment\": \"KZG\",\n        }\n    }\n\n    @classmethod\n    def validate_files(cls, circuit_dir: str) -> bool:\n        try:\n            if not cls._validate_size(circuit_dir):\n                return False\n\n            if not cls._validate_required_files(circuit_dir):\n                return False\n\n            if not cls._validate_settings(circuit_dir):\n                return False\n\n            return True\n\n        except Exception as e:\n            bt.logging.error(f\"Error validating circuit files: {e}\")\n            return False\n\n    @classmethod\n    def _validate_size(cls, circuit_dir: str) -> bool:\n        total_size = sum(\n            os.path.getsize(os.path.join(circuit_dir, f))\n            for f in os.listdir(circuit_dir)\n            if os.path.isfile(os.path.join(circuit_dir, f))\n        )\n        if total_size > MAX_CIRCUIT_SIZE_GB * 1024 * 1024 * 1024:\n            bt.logging.error(\n                f\"Circuit files too large: {total_size / (1024 * 1024 * 1024):.2f} GB\"\n            )\n            return False\n        return True\n\n    @classmethod\n    def _validate_required_files(cls, circuit_dir: str) -> bool:\n        for f in cls.REQUIRED_FILES:\n            if not os.path.exists(os.path.join(circuit_dir, f)):\n                bt.logging.error(f\"Missing required file: {f}\")\n                return False\n        return True\n\n    @classmethod\n    def _validate_settings(cls, circuit_dir: str) -> bool:\n        try:\n            with open(os.path.join(circuit_dir, \"settings.json\")) as f:\n                settings = json.load(f)\n                if \"run_args\" not in settings:\n                    bt.logging.error(\"Missing run_args in settings.json\")\n                    return False\n\n                run_args = settings[\"run_args\"]\n                required_args = cls.REQUIRED_SETTINGS[\"run_args\"]\n\n                for key, value in required_args.items():\n                    if key not in run_args:\n                        bt.logging.error(f\"Missing required run_args setting: {key}\")\n                        return False\n                    if run_args[key] != value:\n                        bt.logging.error(\n                            f\"Invalid value for {key}: expected {value}, got {run_args[key]}\"\n                        )\n                        return False\n\n            return True\n        except json.JSONDecodeError:\n            bt.logging.error(\"Invalid JSON in settings.json\")\n            return False\n"}
{"type": "source_file", "path": "neurons/_validator/competitions/models/circuit.py", "content": "from pydantic import BaseModel\n\n\nclass CircuitFiles(BaseModel):\n    verification_key: str\n    proving_key: str\n    settings: str\n    circuit: str\n    hash: str\n"}
{"type": "source_file", "path": "neurons/_validator/core/validator_loop.py", "content": "from __future__ import annotations\n\nimport asyncio\nimport sys\nimport traceback\nimport time\nfrom typing import NoReturn\nimport concurrent.futures\n\nimport bittensor as bt\n\nfrom _validator.config import ValidatorConfig\nfrom _validator.api import ValidatorAPI\nfrom _validator.core.prometheus import (\n    start_prometheus_logging,\n    stop_prometheus_logging,\n    log_system_metrics,\n    log_queue_metrics,\n    log_weight_update,\n    log_score_change,\n    log_error,\n)\nfrom _validator.core.request import Request\nfrom _validator.core.request_pipeline import RequestPipeline\nfrom _validator.core.response_processor import ResponseProcessor\nfrom _validator.models.miner_response import MinerResponse\nfrom _validator.scoring.score_manager import ScoreManager\nfrom _validator.scoring.weights import WeightsManager\nfrom _validator.utils.axon import query_single_axon\nfrom _validator.models.request_type import RequestType, ValidatorMessage\nfrom _validator.utils.proof_of_weights import save_proof_of_weights\nfrom _validator.utils.uid import get_queryable_uids\nfrom utils import AutoUpdate, clean_temp_files, with_rate_limit\nfrom utils.gc_logging import log_responses as gc_log_responses\nfrom _validator.utils.logging import log_responses as console_log_responses\nfrom constants import (\n    LOOP_DELAY_SECONDS,\n    EXCEPTION_DELAY_SECONDS,\n    MAX_CONCURRENT_REQUESTS,\n    ONE_MINUTE,\n    FIVE_MINUTES,\n    ONE_HOUR,\n)\nfrom _validator.competitions.competition import Competition\nfrom multiprocessing import Queue as MPQueue\n\n\nclass ValidatorLoop:\n    \"\"\"\n    Main loop for the validator node.\n\n    The main loop for the validator. Handles everything from score updates to weight updates.\n    \"\"\"\n\n    def __init__(self, config: ValidatorConfig):\n        \"\"\"\n        Initialize the ValidatorLoop based on provided configuration.\n\n        Args:\n            config (bt.config): Bittensor configuration object.\n        \"\"\"\n        self.config = config\n        self.config.check_register()\n        self.auto_update = AutoUpdate()\n\n        self.validator_to_competition_queue = MPQueue()  # Messages TO competition\n        self.competition_to_validator_queue = MPQueue()  # Messages FROM competition\n        self.current_concurrency = MAX_CONCURRENT_REQUESTS\n\n        try:\n            competition_id = 1\n            bt.logging.info(\"Initializing competition module...\")\n            self.competition = Competition(\n                competition_id,\n                self.config.metagraph,\n                self.config.wallet,\n                self.config.bt_config,\n            )\n            self.competition.set_validator_message_queues(\n                self.validator_to_competition_queue, self.competition_to_validator_queue\n            )\n            bt.logging.success(\"Competition module initialized successfully\")\n        except Exception as e:\n            bt.logging.warning(\n                f\"Failed to initialize competition, continuing without competition support: {e}\"\n            )\n            traceback.print_exc()\n            self.competition = None\n\n        self.score_manager = ScoreManager(\n            self.config.metagraph,\n            self.config.user_uid,\n            self.config.full_path_score,\n            self.competition,\n        )\n        self.response_processor = ResponseProcessor(\n            self.config.metagraph,\n            self.score_manager,\n            self.config.user_uid,\n            self.config.wallet.hotkey,\n        )\n        self.weights_manager = WeightsManager(\n            self.config.subtensor,\n            self.config.metagraph,\n            self.config.wallet,\n            self.config.user_uid,\n        )\n        self.last_pow_commit_block = 0\n        self.api = ValidatorAPI(self.config)\n        self.request_pipeline = RequestPipeline(\n            self.config, self.score_manager, self.api\n        )\n\n        self.request_queue = asyncio.Queue()\n        self.active_tasks: dict[int, asyncio.Task] = {}\n        self.processed_uids: set[int] = set()\n        self.queryable_uids: list[int] = []\n        self.last_response_time = time.time()\n\n        self._should_run = True\n\n        self.thread_pool = concurrent.futures.ThreadPoolExecutor(max_workers=16)\n        self.response_thread_pool = concurrent.futures.ThreadPoolExecutor(\n            max_workers=16\n        )\n        self.last_competition_sync = 0\n        self.is_syncing_competition = False\n        self.competition_commitments = []\n\n        self.recent_responses: list[MinerResponse] = []\n\n        if self.config.bt_config.prometheus_monitoring:\n            start_prometheus_logging(self.config.bt_config.prometheus_port)\n\n    @with_rate_limit(period=FIVE_MINUTES)\n    def update_weights(self):\n        start_time = time.time()\n        try:\n            self.weights_manager.update_weights(self.score_manager.scores)\n            duration = time.time() - start_time\n            log_weight_update(duration, success=True)\n        except Exception as e:\n            log_weight_update(0.0, success=False, failure_reason=str(e))\n            log_error(\"weight_update\", \"weights_manager\", str(e))\n            raise\n\n    @with_rate_limit(period=ONE_HOUR)\n    def sync_scores_uids(self):\n        self.score_manager.sync_scores_uids(self.config.metagraph.uids.tolist())\n\n    @with_rate_limit(period=ONE_HOUR)\n    def sync_metagraph(self):\n        self.config.metagraph.sync(subtensor=self.config.subtensor)\n\n    @with_rate_limit(period=FIVE_MINUTES)\n    def check_auto_update(self):\n        self._handle_auto_update()\n\n    @with_rate_limit(period=FIVE_MINUTES)\n    def update_queryable_uids(self):\n        self.queryable_uids = list(get_queryable_uids(self.config.metagraph))\n\n    @with_rate_limit(period=ONE_MINUTE / 4)\n    def log_health(self):\n        bt.logging.info(\n            f\"In-flight requests: {len(self.active_tasks)} / {MAX_CONCURRENT_REQUESTS}\"\n        )\n        bt.logging.debug(f\"Processed UIDs: {len(self.processed_uids)}\")\n        bt.logging.debug(f\"Queryable UIDs: {len(self.queryable_uids)}\")\n\n        log_system_metrics()\n        queue_size = self.request_queue.qsize()\n        est_latency = (\n            queue_size * (LOOP_DELAY_SECONDS / MAX_CONCURRENT_REQUESTS)\n            if queue_size > 0\n            else 0\n        )\n        log_queue_metrics(queue_size, est_latency)\n\n    def update_processed_uids(self):\n        if len(self.processed_uids) >= len(self.queryable_uids):\n            self.processed_uids.clear()\n\n    @with_rate_limit(period=ONE_HOUR)\n    async def sync_competition(self):\n        if not self.competition:\n            bt.logging.debug(\"Competition module not initialized, skipping sync\")\n            return\n        if self.is_syncing_competition:\n            bt.logging.debug(\"Competition sync already in progress, skipping\")\n            return\n        try:\n            self.is_syncing_competition = True\n            bt.logging.info(\"Starting competition sync...\")\n            bt.logging.debug(\"Fetching commitments...\")\n            commitments = self.competition.fetch_commitments()\n            bt.logging.debug(f\"Found {len(commitments)} commitments\")\n            if commitments:\n                bt.logging.success(f\"Found {len(commitments)} new circuits to evaluate\")\n                for uid, hotkey, hash in commitments:\n                    if hash not in {\n                        state.hash for state in self.competition.miner_states.values()\n                    }:\n                        bt.logging.debug(\n                            f\"Queueing download for circuit {hash[:8]}... from {hotkey[:8]}...\"\n                        )\n                        self.competition.queue_download(uid, hotkey, hash)\n                bt.logging.debug(\n                    f\"Queue size after adding: {len(self.competition.download_queue)}\"\n                )\n            else:\n                bt.logging.debug(\"No new circuits found during sync\")\n        except Exception as e:\n            bt.logging.error(f\"Error in competition sync: {e}\")\n            traceback.print_exc()\n        finally:\n            self.is_syncing_competition = False\n            bt.logging.debug(\"Competition sync complete\")\n\n    @with_rate_limit(period=ONE_MINUTE)\n    async def log_responses(self):\n        if self.recent_responses:\n            console_log_responses(self.recent_responses)\n\n            try:\n                block = (\n                    self.config.metagraph.block.item()\n                    if self.config.metagraph.block is not None\n                    else 0\n                )\n                _ = await asyncio.get_event_loop().run_in_executor(\n                    self.thread_pool,\n                    lambda: gc_log_responses(\n                        self.config.metagraph,\n                        self.config.wallet.hotkey,\n                        self.config.user_uid,\n                        self.recent_responses,\n                        (\n                            time.time() - self.last_response_time\n                            if hasattr(self, \"last_response_time\")\n                            else 0\n                        ),\n                        block,\n                        self.score_manager.scores,\n                    ),\n                )\n            except Exception as e:\n                bt.logging.error(f\"Error in GC logging: {e}\")\n\n            self.last_response_time = time.time()\n            self.recent_responses = []\n\n    async def maintain_request_pool(self):\n        while self._should_run:\n            try:\n                try:\n                    message = await asyncio.get_event_loop().run_in_executor(\n                        self.thread_pool,\n                        lambda: self.competition_to_validator_queue.get(timeout=0.1),\n                    )\n                    if message == ValidatorMessage.WINDDOWN:\n                        bt.logging.info(\n                            \"Received winddown message, reducing concurrency to zero\"\n                        )\n                        self.current_concurrency = 0\n                    elif message == ValidatorMessage.COMPETITION_COMPLETE:\n                        bt.logging.info(\n                            \"Received competition complete message, restoring concurrency\"\n                        )\n                        self.current_concurrency = MAX_CONCURRENT_REQUESTS\n                except Exception:\n                    pass\n                slots_available = self.current_concurrency - len(self.active_tasks)\n\n                if slots_available > 0:\n                    available_uids = [\n                        uid\n                        for uid in self.queryable_uids\n                        if uid not in self.processed_uids\n                        and uid not in self.active_tasks\n                    ]\n\n                    for uid in available_uids[:slots_available]:\n                        request = self.request_pipeline.prepare_single_request(uid)\n                        if request:\n                            task = asyncio.create_task(\n                                self._process_single_request(request)\n                            )\n                            self.active_tasks[uid] = task\n                            task.add_done_callback(\n                                lambda t, uid=uid: self._handle_completed_task(t, uid)\n                            )\n\n                await asyncio.sleep(0)\n            except Exception as e:\n                bt.logging.error(f\"Error maintaining request pool: {e}\")\n                traceback.print_exc()\n                await asyncio.sleep(EXCEPTION_DELAY_SECONDS)\n\n    def _handle_completed_task(self, task: asyncio.Task, uid: int):\n        try:\n            self.processed_uids.add(uid)\n        except Exception as e:\n            bt.logging.error(f\"Error in task for UID {uid}: {e}\")\n            traceback.print_exc()\n        finally:\n            if uid in self.active_tasks:\n                del self.active_tasks[uid]\n                if (\n                    self.current_concurrency == 0\n                    and not self.active_tasks\n                    and self.competition\n                ):\n                    bt.logging.info(\n                        \"All tasks completed during winddown, sending winddown complete message\"\n                    )\n                    self.validator_to_competition_queue.put(\n                        ValidatorMessage.WINDDOWN_COMPLETE\n                    )\n\n    async def run_periodic_tasks(self):\n        while self._should_run:\n            try:\n                self.check_auto_update()\n                self.sync_metagraph()\n                self.sync_scores_uids()\n                self.update_weights()\n                self.update_queryable_uids()\n                self.update_processed_uids()\n                self.log_health()\n                await self.log_responses()\n                if self.current_concurrency:\n                    await self.sync_competition()\n                await asyncio.sleep(LOOP_DELAY_SECONDS)\n            except Exception as e:\n                bt.logging.error(f\"Error in periodic tasks: {e}\")\n                traceback.print_exc()\n                await asyncio.sleep(EXCEPTION_DELAY_SECONDS)\n\n    async def run(self) -> NoReturn:\n        \"\"\"\n        Run the main validator loop indefinitely.\n        \"\"\"\n        bt.logging.success(\n            f\"Validator started on subnet {self.config.subnet_uid} using UID {self.config.user_uid}\"\n        )\n\n        try:\n            await asyncio.gather(\n                self.maintain_request_pool(),\n                self.run_periodic_tasks(),\n            )\n        except KeyboardInterrupt:\n            self._should_run = False\n            self._handle_keyboard_interrupt()\n        except Exception as e:\n            bt.logging.error(f\"Fatal error in validator loop: {e}\")\n            raise\n\n    async def _process_single_request(self, request: Request) -> Request:\n        \"\"\"\n        Process a single request and return the response.\n        \"\"\"\n        try:\n            response = await asyncio.get_event_loop().run_in_executor(\n                self.thread_pool,\n                lambda: query_single_axon(self.config.dendrite, request),\n            )\n            response = await response\n            processed_response = await asyncio.get_event_loop().run_in_executor(\n                self.response_thread_pool,\n                self.response_processor.process_single_response,\n                response,\n            )\n            if processed_response:\n                await self._handle_response(processed_response)\n        except Exception as e:\n            bt.logging.error(f\"Error processing request for UID {request.uid}: {e}\")\n            traceback.print_exc()\n            log_error(\"request_processing\", \"axon_query\", str(e))\n        return request\n\n    async def _handle_response(self, response: MinerResponse) -> None:\n        \"\"\"\n        Handle a processed response, updating scores and weights as needed.\n\n        Args:\n            response (MinerResponse): The processed response to handle.\n        \"\"\"\n        try:\n            request_hash = response.input_hash\n            self.recent_responses.append(response)\n            if response.request_type == RequestType.RWR:\n                if response.verification_result:\n                    self.api.set_request_result(\n                        request_hash,\n                        {\n                            \"hash\": request_hash,\n                            \"public_signals\": response.public_json,\n                            \"proof\": response.proof_content,\n                            \"success\": True,\n                        },\n                    )\n                else:\n                    self.api.set_request_result(\n                        request_hash,\n                        {\n                            \"success\": False,\n                        },\n                    )\n\n            if response.verification_result and response.save:\n                save_proof_of_weights(\n                    public_signals=[response.public_json],\n                    proof=[response.proof_content],\n                    metadata={\n                        \"circuit\": str(response.circuit),\n                        \"request_hash\": response.input_hash,\n                        \"miner_uid\": response.uid,\n                    },\n                    hotkey=self.config.wallet.hotkey,\n                    is_testnet=self.config.subnet_uid == 118,\n                    proof_filename=request_hash,\n                )\n\n            old_score = (\n                float(self.score_manager.scores[response.uid])\n                if response.uid < len(self.score_manager.scores)\n                else 0.0\n            )\n            self.score_manager.update_single_score(response, self.queryable_uids)\n            new_score = (\n                float(self.score_manager.scores[response.uid])\n                if response.uid < len(self.score_manager.scores)\n                else 0.0\n            )\n            log_score_change(old_score, new_score)\n\n        except Exception as e:\n            bt.logging.error(f\"Error handling response: {e}\")\n            traceback.print_exc()\n            log_error(\"response_handling\", \"response_processor\", str(e))\n\n    def _handle_auto_update(self):\n        \"\"\"Handle automatic updates if enabled.\"\"\"\n        if not self.config.bt_config.no_auto_update:\n            self.auto_update.try_update()\n        else:\n            bt.logging.debug(\"Automatic updates are disabled, skipping version check\")\n\n    def _handle_keyboard_interrupt(self):\n        \"\"\"Handle keyboard interrupt by cleaning up and exiting.\"\"\"\n        bt.logging.success(\"Keyboard interrupt detected. Exiting validator.\")\n        loop = asyncio.get_event_loop()\n        loop.run_until_complete(self.api.stop())\n        stop_prometheus_logging()\n        clean_temp_files()\n        if self.competition:\n            self.competition.competition_thread.stop()\n            if hasattr(self.competition.circuit_manager, \"close\"):\n                loop.run_until_complete(self.competition.circuit_manager.close())\n        sys.exit(0)\n"}
{"type": "source_file", "path": "neurons/_validator/config/__init__.py", "content": "import sys\nimport bittensor as bt\nfrom constants import DEFAULT_NETUID, COMPETITION_SYNC_INTERVAL\n\nfrom utils import wandb_logger\nfrom _validator.config.api import ApiConfig\n\n\nclass ValidatorConfig:\n    \"\"\"\n    Configuration class for the Validator.\n\n    This class initializes and manages the configuration settings for the Omron validator.\n\n    Attributes:\n        config (bt.config): The Bittensor configuration object.\n        subnet_uid (int): The unique identifier for the subnet.\n        wallet (bt.wallet): The Bittensor wallet object.\n        subtensor (bt.subtensor): The Bittensor subtensor object.\n        dendrite (bt.dendrite): The Bittensor dendrite object.\n        metagraph (bt.metagraph): The Bittensor metagraph object.\n        user_uid (int): The unique identifier for the validator within the subnet's metagraph.\n        api_enabled (bool): Whether the API is enabled.\n    \"\"\"\n\n    def __init__(self, config: bt.config):\n        \"\"\"\n        Initialize the ValidatorConfig object.\n\n        Args:\n            config (bt.config): The Bittensor configuration object.\n        \"\"\"\n        for key, value in vars(config).items():\n            setattr(self, key, value)\n\n        self.bt_config: bt.Config = config\n        self.subnet_uid = int(\n            self.bt_config.netuid if self.bt_config.netuid else DEFAULT_NETUID\n        )\n        self.wallet = bt.wallet(config=self.bt_config)\n        self.dendrite = bt.dendrite(wallet=self.wallet)\n        self.subtensor = bt.subtensor(config=self.bt_config)\n        try:\n            self.metagraph = self.subtensor.metagraph(self.subnet_uid)\n        except Exception as e:\n            bt.logging.error(f\"Error getting metagraph: {e}\")\n            self.metagraph = None\n        self.user_uid = int(\n            self.metagraph.hotkeys.index(self.wallet.hotkey.ss58_address)\n        )\n        self.localnet = self.bt_config.localnet\n        self.api = ApiConfig(self.bt_config)\n        self.competition_sync_interval = (\n            COMPETITION_SYNC_INTERVAL\n            if self.bt_config.competition_sync_interval is None\n            else self.bt_config.competition_sync_interval\n        )\n\n        # Initialize wandb logger\n        wandb_logger.safe_init(\n            \"Validator\",\n            self.wallet,\n            self.metagraph,\n            self.bt_config,\n        )\n\n    def check_register(self):\n        \"\"\"\n        Check if the validator is registered on the subnet.\n\n        This method verifies if the validator's hotkey is registered in the metagraph.\n        If not registered, it logs an error and exits.\n        If registered, it sets the user_uid and logs it.\n\n        Raises:\n            SystemExit: If the validator is not registered on the network.\n        \"\"\"\n        if self.wallet.hotkey.ss58_address not in self.metagraph.hotkeys:\n            bt.logging.error(\n                f\"\\nYour validator: {self.wallet} is not registered to the chain: \"\n                f\"{self.subtensor} \\nRun btcli register and try again.\"\n            )\n            sys.exit(1)\n        else:\n            uid = self.metagraph.hotkeys.index(self.wallet.hotkey.ss58_address)\n            bt.logging.info(f\"Running validator on uid: {uid}\")\n            self.user_uid = uid\n"}
{"type": "source_file", "path": "neurons/_miner/miner_session.py", "content": "# from __future__ import annotations\nimport json\nimport os\nimport time\nimport traceback\nfrom typing import Tuple, Union\n\nimport bittensor as bt\nimport websocket\nfrom rich.console import Console\nfrom rich.table import Table\n\nimport cli_parser\nfrom _validator.models.request_type import RequestType\nfrom constants import (\n    SINGLE_PROOF_OF_WEIGHTS_MODEL_ID,\n    STEAK,\n    VALIDATOR_STAKE_THRESHOLD,\n    ONE_HOUR,\n    CRICUIT_TIMEOUT_SECONDS,\n)\nfrom deployment_layer.circuit_store import circuit_store\nfrom execution_layer.generic_input import GenericInput\nfrom execution_layer.verified_model_session import VerifiedModelSession\nfrom protocol import (\n    QueryForProofAggregation,\n    QueryZkProof,\n    ProofOfWeightsSynapse,\n    Competition,\n)\nfrom utils import AutoUpdate, clean_temp_files, wandb_logger\nfrom utils.rate_limiter import with_rate_limit\nfrom .circuit_manager import CircuitManager\n\nCOMPETITION_DIR = os.path.join(\n    os.path.dirname(__file__), \"..\", \"..\", \"competition_circuit\"\n)\n\n\nclass MinerSession:\n\n    axon: Union[bt.axon, None] = None\n\n    def __init__(self):\n        self.configure()\n        self.check_register(should_exit=True)\n        self.auto_update = AutoUpdate()\n        self.log_batch = []\n        if cli_parser.config.disable_blacklist:\n            bt.logging.warning(\n                \"Blacklist disabled, allowing all requests. Consider enabling to filter requests.\"\n            )\n        websocket.setdefaulttimeout(30)\n\n    def start_axon(self):\n        bt.logging.info(\n            \"Starting axon. Custom arguments include the following.\\n\"\n            \"Note that any null values will fallback to defaults, \"\n            f\"which are usually sufficient. {cli_parser.config.axon}\"\n        )\n\n        axon = bt.axon(wallet=self.wallet, config=cli_parser.config)\n        bt.logging.info(f\"Axon created: {axon.info()}\")\n\n        bt.logging.info(\"Attaching forward functions to axon...\")\n        axon.attach(forward_fn=self.queryZkProof, blacklist_fn=self.proof_blacklist)\n        axon.attach(\n            forward_fn=self.handle_pow_request,\n            blacklist_fn=self.pow_blacklist,\n        )\n        axon.attach(\n            forward_fn=self.handleCompetitionRequest,\n            blacklist_fn=self.competition_blacklist,\n        )\n\n        bt.logging.info(\"Attached forward functions to axon\")\n\n        bt.logging.info(f\"Starting axon server: {axon.info()}\")\n        axon.start()\n        bt.logging.info(f\"Started axon server: {axon.info()}\")\n\n        existing_axon = self.metagraph.axons[self.subnet_uid]\n\n        if (\n            existing_axon\n            and existing_axon.port == axon.external_port\n            and existing_axon.ip == axon.external_ip\n        ):\n            bt.logging.debug(\n                f\"Axon already serving on ip {axon.external_ip} and port {axon.external_port}\"\n            )\n            return\n        bt.logging.info(\n            f\"Serving axon on network: {self.subtensor.chain_endpoint} with netuid: {cli_parser.config.netuid}\"\n        )\n\n        axon.serve(netuid=cli_parser.config.netuid, subtensor=self.subtensor)\n        bt.logging.info(\n            f\"Served axon on network: {self.subtensor.chain_endpoint} with netuid: {cli_parser.config.netuid}\"\n        )\n\n        self.axon = axon\n\n    def run(self):\n        \"\"\"\n        Keep the miner alive.\n        This loop maintains the miner's operations until intentionally stopped.\n        \"\"\"\n        bt.logging.info(\"Starting miner...\")\n        self.start_axon()\n\n        step = 0\n\n        while True:\n            step += 1\n            try:\n                if step % 10 == 0:\n                    if not cli_parser.config.no_auto_update:\n                        self.auto_update.try_update()\n                    else:\n                        bt.logging.info(\n                            \"Automatic updates are disabled, skipping version check\"\n                        )\n\n                if step % 20 == 0:\n                    if len(self.log_batch) > 0:\n                        bt.logging.debug(\n                            f\"Logging batch to WandB of size {len(self.log_batch)}\"\n                        )\n                        for log in self.log_batch:\n                            wandb_logger.safe_log(log)\n                        self.log_batch = []\n                    else:\n                        bt.logging.debug(\"No logs to log to WandB\")\n                if step % 600 == 0:\n                    self.check_register()\n\n                if step % 24 == 0 and self.subnet_uid is not None:\n                    table = Table(title=f\"Miner Status (UID: {self.subnet_uid})\")\n                    table.add_column(\"Block\", justify=\"center\", style=\"cyan\")\n                    table.add_column(\"Stake\", justify=\"center\", style=\"cyan\")\n                    table.add_column(\"Rank\", justify=\"center\", style=\"cyan\")\n                    table.add_column(\"Trust\", justify=\"center\", style=\"cyan\")\n                    table.add_column(\"Consensus\", justify=\"center\", style=\"cyan\")\n                    table.add_column(\"Incentive\", justify=\"center\", style=\"cyan\")\n                    table.add_column(\"Emission\", justify=\"center\", style=\"cyan\")\n                    table.add_row(\n                        str(self.metagraph.block.item()),\n                        str(self.metagraph.S[self.subnet_uid]),\n                        str(self.metagraph.R[self.subnet_uid]),\n                        str(self.metagraph.T[self.subnet_uid]),\n                        str(self.metagraph.C[self.subnet_uid]),\n                        str(self.metagraph.I[self.subnet_uid]),\n                        str(self.metagraph.E[self.subnet_uid]),\n                    )\n                    console = Console()\n                    console.print(table)\n                self.sync_metagraph()\n\n                time.sleep(1)\n\n            except KeyboardInterrupt:\n                bt.logging.success(\"Miner killed via keyboard interrupt.\")\n                clean_temp_files()\n                break\n            except Exception:\n                bt.logging.error(traceback.format_exc())\n                continue\n\n    def check_register(self, should_exit=False):\n        if self.wallet.hotkey.ss58_address not in self.metagraph.hotkeys:\n            bt.logging.error(\n                f\"\\nYour miner: {self.wallet} is not registered to the network: {self.subtensor} \\n\"\n                \"Run btcli register and try again.\"\n            )\n            if should_exit:\n                exit()\n            self.subnet_uid = None\n        else:\n            subnet_uid = self.metagraph.hotkeys.index(self.wallet.hotkey.ss58_address)\n            self.subnet_uid = subnet_uid\n\n    def configure(self):\n        self.wallet = bt.wallet(config=cli_parser.config)\n        self.subtensor = bt.subtensor(config=cli_parser.config)\n        self.metagraph = self.subtensor.metagraph(cli_parser.config.netuid)\n        wandb_logger.safe_init(\"Miner\", self.wallet, self.metagraph, cli_parser.config)\n\n        if cli_parser.config.storage:\n            storage_config = {\n                \"provider\": cli_parser.config.storage.provider,\n                \"bucket\": cli_parser.config.storage.bucket,\n                \"account_id\": cli_parser.config.storage.account_id,\n                \"access_key\": cli_parser.config.storage.access_key,\n                \"secret_key\": cli_parser.config.storage.secret_key,\n                \"region\": cli_parser.config.storage.region,\n            }\n        else:\n            bt.logging.warning(\n                \"No storage config provided, circuit manager will not be initialized.\"\n            )\n            storage_config = None\n\n        try:\n            current_commitment = self.subtensor.get_commitment(\n                cli_parser.config.netuid,\n                self.metagraph.hotkeys.index(self.wallet.hotkey.ss58_address),\n            )\n\n            self.circuit_manager = CircuitManager(\n                wallet=self.wallet,\n                netuid=cli_parser.config.netuid,\n                circuit_dir=COMPETITION_DIR,\n                storage_config=storage_config,\n                existing_vk_hash=current_commitment,\n            )\n        except Exception as e:\n            bt.logging.error(f\"Error initializing circuit manager: {e}\")\n            self.circuit_manager = None\n\n    @with_rate_limit(period=ONE_HOUR)\n    def sync_metagraph(self):\n        try:\n            self.metagraph.sync(subtensor=self.subtensor)\n            return True\n        except Exception as e:\n            bt.logging.warning(f\"Failed to sync metagraph: {e}\")\n            return False\n\n    def proof_blacklist(self, synapse: QueryZkProof) -> Tuple[bool, str]:\n        \"\"\"\n        Blacklist method for the proof generation endpoint\n        \"\"\"\n        return self._blacklist(synapse)\n\n    def aggregation_blacklist(\n        self, synapse: QueryForProofAggregation\n    ) -> Tuple[bool, str]:\n        \"\"\"\n        Blacklist method for the aggregation endpoint\n        \"\"\"\n        return self._blacklist(synapse)\n\n    def pow_blacklist(self, synapse: ProofOfWeightsSynapse) -> Tuple[bool, str]:\n        \"\"\"\n        Blacklist method for the proof generation endpoint\n        \"\"\"\n        return self._blacklist(synapse)\n\n    def competition_blacklist(self, synapse: Competition) -> Tuple[bool, str]:\n        \"\"\"\n        Blacklist method for the competition endpoint\n        \"\"\"\n        return self._blacklist(synapse)\n\n    def _blacklist(\n        self,\n        synapse: Union[QueryZkProof, QueryForProofAggregation, ProofOfWeightsSynapse],\n    ) -> Tuple[bool, str]:\n        \"\"\"\n        Filters requests if any of the following conditions are met:\n        - Requesting hotkey is not registered\n        - Requesting UID's stake is below 1k\n        - Requesting UID does not have a validator permit\n\n        Does not filter if the --disable-blacklist flag has been set.\n\n        synapse: The request synapse object\n        returns: (is_blacklisted, reason)\n        \"\"\"\n        try:\n            if cli_parser.config.disable_blacklist:\n                bt.logging.trace(\"Blacklist disabled, allowing request.\")\n                return False, \"Allowed\"\n\n            if synapse.dendrite.hotkey not in self.metagraph.hotkeys:  # type: ignore\n                return True, \"Hotkey is not registered\"\n\n            requesting_uid = self.metagraph.hotkeys.index(synapse.dendrite.hotkey)  # type: ignore\n            stake = self.metagraph.S[requesting_uid].item()\n\n            try:\n                bt.logging.info(\n                    f\"Request by: {synapse.dendrite.hotkey} | UID: {requesting_uid} \"  # type: ignore\n                    f\"| Stake: {stake} {STEAK}\"\n                )\n            except UnicodeEncodeError:\n                bt.logging.info(\n                    f\"Request by: {synapse.dendrite.hotkey} | UID: {requesting_uid} | Stake: {stake}\"  # type: ignore\n                )\n\n            if stake < VALIDATOR_STAKE_THRESHOLD:\n                return True, \"Stake below minimum\"\n\n            validator_permit = self.metagraph.validator_permit[requesting_uid].item()\n            if not validator_permit:\n                return True, \"Requesting UID has no validator permit\"\n\n            bt.logging.trace(f\"Allowing request from UID: {requesting_uid}\")\n            return False, \"Allowed\"\n\n        except Exception as e:\n            bt.logging.error(f\"Error during blacklist {e}\")\n            return True, \"An error occurred while filtering the request\"\n\n    def handleCompetitionRequest(self, synapse: Competition) -> Competition:\n        \"\"\"\n        Handle competition circuit requests from validators.\n\n        This endpoint provides signed URLs for validators to download circuit files.\n        The process ensures:\n        1. Files are uploaded to R2/S3\n        2. VK hash matches chain commitment\n        3. URLs are signed and time-limited\n        4. All operations are thread-safe\n        \"\"\"\n        bt.logging.info(\n            f\"Handling competition request for id={synapse.id} hash={synapse.hash}\"\n        )\n        try:\n            if not self.circuit_manager:\n                bt.logging.critical(\n                    \"Circuit manager not initialized, unable to respond to validator.\"\n                )\n                return Competition(\n                    id=synapse.id,\n                    hash=synapse.hash,\n                    file_name=synapse.file_name,\n                    error=\"Circuit manager not initialized\",\n                )\n\n            bt.logging.info(\"Getting current commitment from circuit manager\")\n            commitment = self.circuit_manager.get_current_commitment()\n            if not commitment:\n                bt.logging.critical(\n                    \"No valid circuit commitment available. Unable to respond to validator.\"\n                )\n                return Competition(\n                    id=synapse.id,\n                    hash=synapse.hash,\n                    file_name=synapse.file_name,\n                    error=\"No valid circuit commitment available\",\n                )\n\n            bt.logging.info(\"Getting chain commitment from subtensor\")\n            chain_commitment = self.subtensor.get_commitment(\n                cli_parser.config.netuid,\n                self.metagraph.hotkeys.index(self.wallet.hotkey.ss58_address),\n            )\n            if commitment.vk_hash != chain_commitment:\n                bt.logging.critical(\n                    f\"Hash mismatch - local: {commitment.vk_hash[:8]} \"\n                    f\"chain: {chain_commitment[:8]}\"\n                )\n                return Competition(\n                    id=synapse.id,\n                    hash=synapse.hash,\n                    file_name=synapse.file_name,\n                    error=\"Hash mismatch between local and chain commitment\",\n                )\n\n            bt.logging.info(\"Generating signed URLs for required files\")\n            required_files = [\"vk.key\", \"pk.key\", \"settings.json\", \"model.compiled\"]\n            object_keys = {}\n            for file_name in required_files:\n                object_keys[file_name] = f\"{commitment.vk_hash}/{file_name}\"\n            signed_urls = self.circuit_manager._get_signed_urls(object_keys)\n            if not signed_urls:\n                bt.logging.error(\"Failed to get signed URLs\")\n                return Competition(\n                    id=synapse.id,\n                    hash=synapse.hash,\n                    file_name=synapse.file_name,\n                    error=\"Failed to get signed URLs\",\n                )\n\n            bt.logging.info(\"Preparing commitment data response\")\n            commitment_data = commitment.model_dump()\n            commitment_data[\"signed_urls\"] = signed_urls\n\n            response = Competition(\n                id=synapse.id,\n                hash=synapse.hash,\n                file_name=synapse.file_name,\n                commitment=json.dumps(commitment_data),\n            )\n            bt.logging.info(\"Successfully prepared competition response\")\n            return response\n\n        except Exception as e:\n            bt.logging.error(f\"Error handling competition request: {str(e)}\")\n            traceback.print_exc()\n            return Competition(\n                id=synapse.id,\n                hash=synapse.hash,\n                file_name=synapse.file_name,\n                error=str(e),\n            )\n\n    def queryZkProof(self, synapse: QueryZkProof) -> QueryZkProof:\n        \"\"\"\n        This function run proof generation of the model (with its output as well)\n        \"\"\"\n        if cli_parser.config.competition_only:\n            bt.logging.info(\"Competition only mode enabled. Skipping proof generation.\")\n            synapse.query_output = \"Competition only mode enabled\"\n            return synapse\n\n        time_in = time.time()\n        bt.logging.debug(\"Received request from validator\")\n        bt.logging.debug(f\"Input data: {synapse.query_input} \\n\")\n\n        if not synapse.query_input or not synapse.query_input.get(\n            \"public_inputs\", None\n        ):\n            bt.logging.error(\"Received empty query input\")\n            synapse.query_output = \"Empty query input\"\n            return synapse\n\n        model_id = synapse.query_input.get(\"model_id\", SINGLE_PROOF_OF_WEIGHTS_MODEL_ID)\n        public_inputs = synapse.query_input[\"public_inputs\"]\n\n        circuit_timeout = CRICUIT_TIMEOUT_SECONDS\n        try:\n            circuit = circuit_store.get_circuit(str(model_id))\n            if not circuit:\n                raise ValueError(\n                    f\"Circuit {model_id} not found. This indicates a missing deployment layer folder or invalid request\"\n                )\n            circuit_timeout = circuit.timeout\n            bt.logging.info(f\"Running proof generation for {circuit}\")\n            model_session = VerifiedModelSession(\n                GenericInput(RequestType.RWR, public_inputs), circuit\n            )\n            bt.logging.debug(\"Model session created successfully\")\n            proof, public, proof_time = model_session.gen_proof()\n            if isinstance(proof, bytes):\n                proof = proof.hex()\n\n            synapse.query_output = json.dumps(\n                {\n                    \"proof\": proof,\n                    \"public_signals\": public,\n                }\n            )\n            bt.logging.trace(f\"Proof: {synapse.query_output}, Time: {proof_time}\")\n            model_session.end()\n            try:\n                bt.logging.info(f\"✅ Proof completed for {circuit}\\n\")\n            except UnicodeEncodeError:\n                bt.logging.info(f\"Proof completed for {circuit}\\n\")\n        except Exception as e:\n            synapse.query_output = \"An error occurred\"\n            bt.logging.error(f\"An error occurred while generating proven output\\n{e}\")\n            traceback.print_exc()\n            proof_time = time.time() - time_in\n\n        time_out = time.time()\n        delta_t = time_out - time_in\n        bt.logging.info(\n            f\"Total response time {delta_t}s. Proof time: {proof_time}s. \"\n            f\"Overhead time: {delta_t - proof_time}s.\"\n        )\n        self.log_batch.append(\n            {\n                str(model_id): {\n                    \"proof_time\": proof_time,\n                    \"overhead_time\": delta_t - proof_time,\n                    \"total_response_time\": delta_t,\n                }\n            }\n        )\n\n        if delta_t > circuit_timeout:\n            bt.logging.error(\n                \"Response time is greater than circuit timeout. \"\n                \"This indicates your hardware is not processing the requests in time.\"\n            )\n        return synapse\n\n    def handle_pow_request(\n        self, synapse: ProofOfWeightsSynapse\n    ) -> ProofOfWeightsSynapse:\n        \"\"\"\n        Handles a proof of weights request\n        \"\"\"\n        if cli_parser.config.competition_only:\n            bt.logging.info(\"Competition only mode enabled. Skipping proof generation.\")\n            synapse.query_output = \"Competition only mode enabled\"\n            return synapse\n\n        time_in = time.time()\n        bt.logging.debug(\"Received proof of weights request from validator\")\n        bt.logging.debug(f\"Input data: {synapse.inputs} \\n\")\n\n        if not synapse.inputs:\n            bt.logging.error(\"Received empty input for proof of weights\")\n            return synapse\n\n        circuit_timeout = CRICUIT_TIMEOUT_SECONDS\n        try:\n            circuit = circuit_store.get_circuit(str(synapse.verification_key_hash))\n            if not circuit:\n                raise ValueError(\n                    f\"Circuit {synapse.verification_key_hash} not found. \"\n                    \"This indicates a missing deployment layer folder or invalid request\"\n                )\n            circuit_timeout = circuit.timeout\n            bt.logging.info(f\"Running proof generation for {circuit}\")\n            model_session = VerifiedModelSession(\n                GenericInput(RequestType.RWR, synapse.inputs), circuit\n            )\n\n            bt.logging.debug(\"Model session created successfully\")\n            proof, public, proof_time = model_session.gen_proof()\n            model_session.end()\n\n            synapse.proof = proof.hex() if isinstance(proof, bytes) else proof\n            synapse.public_signals = public\n            bt.logging.info(f\"✅ Proof of weights completed for {circuit}\\n\")\n        except Exception as e:\n            bt.logging.error(\n                f\"An error occurred while generating proof of weights\\n{e}\"\n            )\n            traceback.print_exc()\n            proof_time = time.time() - time_in\n\n        time_out = time.time()\n        delta_t = time_out - time_in\n        bt.logging.info(\n            f\"Total response time {delta_t}s. Proof time: {proof_time}s. \"\n            f\"Overhead time: {delta_t - proof_time}s.\"\n        )\n        self.log_batch.append(\n            {\n                str(synapse.verification_key_hash): {\n                    \"proof_time\": proof_time,\n                    \"overhead_time\": delta_t - proof_time,\n                    \"total_response_time\": delta_t,\n                }\n            }\n        )\n\n        if delta_t > circuit_timeout:\n            bt.logging.error(\n                \"Response time is greater than circuit timeout. \"\n                \"This indicates your hardware is not processing the requests in time.\"\n            )\n        return synapse\n\n    def aggregateProof(\n        self, synapse: QueryForProofAggregation\n    ) -> QueryForProofAggregation:\n        \"\"\"\n        Generates an aggregate proof for the provided proofs.\n        \"\"\"\n        raise NotImplementedError(\"Proof aggregation not supported at this time.\")\n"}
{"type": "source_file", "path": "neurons/_validator/competitions/services/circuit_evaluator.py", "content": "import os\nimport json\nimport time\nimport tempfile\nimport subprocess\nimport torch\nimport numpy as np\nimport bittensor as bt\nfrom typing import Tuple, List, Optional\nfrom constants import LOCAL_EZKL_PATH\nfrom _validator.competitions.services.sota_manager import SotaManager\nfrom _validator.competitions.services.data_source import (\n    CompetitionDataSource,\n    RandomDataSource,\n    RemoteDataSource,\n)\nfrom utils.wandb_logger import safe_log\nimport shutil\nimport traceback\nfrom _validator.competitions.services.data_source import (\n    CompetitionDataProcessor,\n)\nimport logging\nfrom utils.system import get_temp_folder\n\nlogging.getLogger(\"onnxruntime\").setLevel(logging.ERROR)\nos.environ[\"ONNXRUNTIME_LOGGING_LEVEL\"] = \"3\"\n\n\nclass CircuitEvaluator:\n    def __init__(\n        self,\n        config: dict,\n        competition_directory: str,\n        sota_manager: SotaManager,\n    ):\n\n        self.competition_directory = competition_directory\n        self.sota_manager = sota_manager\n\n        self.baseline_model = os.path.join(\n            self.competition_directory, config[\"baseline_model_path\"]\n        )\n        bt.logging.debug(f\"Using baseline model at: {self.baseline_model}\")\n\n        self.is_onnx = not isinstance(self.baseline_model, torch.nn.Module)\n\n        self.onnx_venv = os.path.abspath(\n            os.path.join(competition_directory, \"onnx_venv\")\n        )\n        self.onnx_runner = os.path.join(\n            os.path.dirname(os.path.abspath(__file__)), \"onnx_runner.py\"\n        )\n\n        if self.is_onnx:\n            if not os.path.exists(self.onnx_venv):\n                if not self._setup_onnx_env():\n                    raise RuntimeError(\"Failed to set up ONNX environment\")\n            else:\n                bt.logging.debug(f\"Using existing ONNX environment at {self.onnx_venv}\")\n\n        self.data_source = self._setup_data_source()\n\n    def _get_python_path(self):\n        python_path = os.path.join(self.onnx_venv, \"bin\", \"python\")\n        if os.path.exists(python_path):\n            return python_path\n\n        python_path = os.path.join(self.onnx_venv, \"bin\", \"python3\")\n        if os.path.exists(python_path):\n            return python_path\n\n        for root, dirs, files in os.walk(self.onnx_venv):\n            for file in files:\n                if file == \"python\" or file == \"python3\":\n                    return os.path.join(root, file)\n\n        return None\n\n    def _get_pip_path(self):\n        pip_path = os.path.join(self.onnx_venv, \"bin\", \"pip\")\n        if os.path.exists(pip_path):\n            return pip_path\n\n        pip_path = os.path.join(self.onnx_venv, \"bin\", \"pip3\")\n        if os.path.exists(pip_path):\n            return pip_path\n\n        for root, dirs, files in os.walk(self.onnx_venv):\n            for file in files:\n                if file == \"pip\" or file == \"pip3\":\n                    return os.path.join(root, file)\n\n        return None\n\n    def _find_site_packages(self):\n        site_packages = None\n        for root, dirs, files in os.walk(self.onnx_venv):\n            if os.path.basename(root) == \"site-packages\":\n                site_packages = root\n                break\n\n        if not site_packages:\n            python_path = self._get_python_path()\n            if python_path:\n                try:\n                    python_version_result = subprocess.run(\n                        [python_path, \"--version\"],\n                        capture_output=True,\n                        text=True,\n                    )\n                    python_version_output = python_version_result.stdout.strip()\n                    if not python_version_output:\n                        python_version_output = python_version_result.stderr.strip()\n                    python_version = python_version_output.split()[1][:3]\n                    site_packages = os.path.join(\n                        self.onnx_venv,\n                        \"lib\",\n                        f\"python{python_version}\",\n                        \"site-packages\",\n                    )\n                except Exception:\n                    site_packages = os.path.join(\n                        self.onnx_venv, \"lib\", \"python3.10\", \"site-packages\"\n                    )\n            else:\n                site_packages = os.path.join(\n                    self.onnx_venv, \"lib\", \"python3.10\", \"site-packages\"\n                )\n\n        if not os.path.exists(site_packages):\n            os.makedirs(site_packages, exist_ok=True)\n\n        return site_packages\n\n    def _get_runner_path(self):\n        runner_path = None\n        for root, dirs, files in os.walk(self.onnx_venv):\n            if \"onnx_runner.py\" in files:\n                runner_path = os.path.join(root, \"onnx_runner.py\")\n                break\n\n        if not runner_path or not os.path.exists(runner_path):\n            site_packages = self._find_site_packages()\n            runner_path = os.path.join(site_packages, \"onnx_runner.py\")\n            if not os.path.exists(runner_path):\n                shutil.copy2(self.onnx_runner, runner_path)\n\n        if not os.path.exists(runner_path):\n            runner_path = self.onnx_runner\n\n        return runner_path\n\n    def _setup_onnx_env(self):\n        try:\n            parent_dir = os.path.dirname(self.onnx_venv)\n            if not os.path.exists(parent_dir):\n                bt.logging.error(f\"Parent directory {parent_dir} does not exist\")\n                return False\n\n            if not os.access(parent_dir, os.W_OK):\n                bt.logging.error(f\"No write permission for directory {parent_dir}\")\n                return False\n\n            os.makedirs(self.onnx_venv, exist_ok=True)\n            bt.logging.debug(f\"Creating ONNX venv at {self.onnx_venv}\")\n\n            result = subprocess.run(\n                [\"python\", \"-m\", \"venv\", self.onnx_venv], capture_output=True, text=True\n            )\n\n            if result.returncode != 0:\n                bt.logging.error(f\"Failed to create venv: {result.stderr}\")\n\n                result = subprocess.run(\n                    [\"python3\", \"-m\", \"venv\", self.onnx_venv],\n                    capture_output=True,\n                    text=True,\n                )\n                if result.returncode != 0:\n                    bt.logging.error(\n                        f\"Failed to create venv with python3: {result.stderr}\"\n                    )\n                    return False\n\n            python_path = self._get_python_path()\n            if not python_path:\n                bt.logging.error(\n                    f\"Python not found in venv at {self.onnx_venv} after creation\"\n                )\n                return False\n\n            bt.logging.debug(f\"Found Python at {python_path}\")\n\n            python_version_result = subprocess.run(\n                [python_path, \"--version\"],\n                capture_output=True,\n                text=True,\n            )\n\n            if python_version_result.returncode != 0:\n                bt.logging.error(\n                    f\"Failed to get Python version: {python_version_result.stderr}\"\n                )\n                return False\n\n            python_version_output = python_version_result.stdout.strip()\n            if not python_version_output:\n                python_version_output = python_version_result.stderr.strip()\n\n            python_version = python_version_output.split()[1][:3]\n            bt.logging.debug(f\"Python version: {python_version}\")\n\n            pip_path = self._get_pip_path()\n            if not pip_path:\n                bt.logging.error(f\"Pip not found in venv at {self.onnx_venv}\")\n                return False\n\n            bt.logging.debug(f\"Found pip at {pip_path}\")\n\n            pip_result = subprocess.run(\n                [pip_path, \"install\", \"numpy\", \"onnxruntime==1.20.1\"],\n                capture_output=True,\n                text=True,\n            )\n\n            if pip_result.returncode != 0:\n                bt.logging.error(f\"Failed to install dependencies: {pip_result.stderr}\")\n                return False\n\n            site_packages = self._find_site_packages()\n            bt.logging.debug(f\"Using site-packages at {site_packages}\")\n\n            runner_path = os.path.join(site_packages, \"onnx_runner.py\")\n            shutil.copy2(self.onnx_runner, runner_path)\n            bt.logging.debug(f\"Copied onnx_runner.py to {runner_path}\")\n\n            bt.logging.success(\n                f\"ONNX environment set up at {self.onnx_venv} with Python {python_version}\"\n            )\n            return True\n        except Exception as e:\n            bt.logging.error(f\"Error setting up ONNX environment: {str(e)}\")\n            bt.logging.error(f\"Stack trace: {traceback.format_exc()}\")\n            return False\n\n    def _setup_data_source(self) -> CompetitionDataSource:\n        try:\n            config_path = os.path.join(\n                self.competition_directory, \"competition_config.json\"\n            )\n            with open(config_path) as f:\n                config = json.load(f)\n                data_config = config.get(\"data_source\", {})\n\n                processor = None\n                processor_path = os.path.join(\n                    self.competition_directory, \"data_processor.py\"\n                )\n                if os.path.exists(processor_path):\n                    import importlib.util\n\n                    spec = importlib.util.spec_from_file_location(\n                        \"data_processor\", processor_path\n                    )\n                    module = importlib.util.module_from_spec(spec)\n                    spec.loader.exec_module(module)\n\n                    for attr_name in dir(module):\n                        attr = getattr(module, attr_name)\n                        if (\n                            isinstance(attr, type)\n                            and issubclass(attr, CompetitionDataProcessor)\n                            and attr != CompetitionDataProcessor\n                        ):\n                            processor = attr()\n                            break\n\n                if data_config.get(\"type\") == \"remote\":\n                    data_source = RemoteDataSource(\n                        self.competition_directory, processor\n                    )\n                    if not data_source.sync_data():\n                        bt.logging.error(\"Failed to sync remote data source\")\n                        return RandomDataSource(self.competition_directory, processor)\n                    return data_source\n                return RandomDataSource(self.competition_directory, processor)\n        except Exception as e:\n            bt.logging.error(f\"Error setting up data source: {e}\")\n            traceback.print_exc()\n            return RandomDataSource(self.competition_directory)\n\n    def _calculate_relative_score(\n        self, raw_accuracy: float, proof_size: float, response_time: float\n    ) -> float:\n        if raw_accuracy == 0:\n            return 0.0\n\n        sota_state = self.sota_manager.current_state\n\n        try:\n            with open(\n                os.path.join(self.competition_directory, \"competition_config.json\")\n            ) as f:\n                config = json.load(f)\n                weights = config[\"evaluation\"][\"scoring_weights\"]\n        except Exception as e:\n            bt.logging.error(f\"Error loading scoring weights, using defaults: {e}\")\n            weights = {\"accuracy\": 0.4, \"proof_size\": 0.3, \"response_time\": 0.3}\n\n        accuracy_diff = max(0, sota_state.raw_accuracy - raw_accuracy)\n\n        if sota_state.proof_size > 0:\n            proof_size_diff = max(\n                0, (proof_size - sota_state.proof_size) / sota_state.proof_size\n            )\n        else:\n            proof_size_diff = 0 if proof_size == 0 else 1\n\n        if sota_state.response_time > 0:\n            response_time_diff = max(\n                0, (response_time - sota_state.response_time) / sota_state.response_time\n            )\n        else:\n            response_time_diff = 0 if response_time == 0 else 1\n\n        total_diff = torch.tensor(\n            accuracy_diff * weights[\"accuracy\"]\n            + proof_size_diff * weights[\"proof_size\"]\n            + response_time_diff * weights[\"response_time\"]\n        )\n\n        return torch.exp(-total_diff).item()\n\n    def _calculate_improvements(\n        self, raw_accuracy: float, proof_size: float, response_time: float\n    ) -> dict:\n        sota_state = self.sota_manager.current_state\n\n        if sota_state.sota_relative_score == 0:\n            return {\n                \"raw\": {\"accuracy\": 0, \"proof_size\": 0, \"response_time\": 0},\n                \"weighted\": {\"accuracy\": 0, \"proof_size\": 0, \"response_time\": 0},\n            }\n\n        try:\n            with open(\n                os.path.join(self.competition_directory, \"competition_config.json\")\n            ) as f:\n                config = json.load(f)\n                weights = config[\"evaluation\"][\"scoring_weights\"]\n        except Exception as e:\n            bt.logging.error(f\"Error loading scoring weights, using defaults: {e}\")\n            weights = {\"accuracy\": 0.4, \"proof_size\": 0.3, \"response_time\": 0.3}\n\n        raw_improvements = {\n            \"accuracy\": sota_state.raw_accuracy - raw_accuracy,\n            \"proof_size\": (\n                (proof_size - sota_state.proof_size) / sota_state.proof_size\n                if sota_state.proof_size > 0\n                else 0\n            ),\n            \"response_time\": (\n                (response_time - sota_state.response_time) / sota_state.response_time\n                if sota_state.response_time > 0\n                else 0\n            ),\n        }\n\n        return {\n            \"raw\": raw_improvements,\n            \"weighted\": {\n                \"accuracy\": raw_improvements[\"accuracy\"] * weights[\"accuracy\"],\n                \"proof_size\": raw_improvements[\"proof_size\"] * weights[\"proof_size\"],\n                \"response_time\": raw_improvements[\"response_time\"]\n                * weights[\"response_time\"],\n            },\n        }\n\n    def evaluate(self, circuit_dir: str) -> Tuple[float, float, float, bool, dict]:\n        raw_accuracy_scores, proof_sizes, response_times, verification_results = (\n            [],\n            [],\n            [],\n            [],\n        )\n        input_shape = self._get_input_shape(circuit_dir)\n        bt.logging.debug(f\"Got input shape: {input_shape}\")\n\n        if not input_shape:\n            bt.logging.error(\"Failed to get input shape\")\n            return 0.0, float(\"inf\"), float(\"inf\"), False, {}\n\n        try:\n            with open(\n                os.path.join(self.competition_directory, \"competition_config.json\")\n            ) as f:\n                config = json.load(f)\n                num_iterations = config[\"evaluation\"][\"num_iterations\"]\n        except Exception as e:\n            bt.logging.error(f\"Error loading num_iterations, using default: {e}\")\n            num_iterations = 10\n\n        for i in range(num_iterations):\n            bt.logging.debug(f\"Running evaluation {i + 1}/{num_iterations}\")\n\n            try:\n                test_inputs = self.data_source.get_benchmark_data()\n                if test_inputs is None:\n                    bt.logging.error(\"Failed to get benchmark data\")\n                    raw_accuracy_scores.append(0.0)\n                    continue\n\n                bt.logging.debug(f\"Got benchmark data with shape: {test_inputs.shape}\")\n\n                baseline_output = self._run_baseline_model(test_inputs)\n                if baseline_output is None:\n                    bt.logging.error(\"Baseline model run failed\")\n                    raw_accuracy_scores.append(0.0)\n                    continue\n\n                bt.logging.debug(\n                    f\"Got baseline output with shape: {np.array(baseline_output).shape}\"\n                )\n\n                proof_result = self._generate_proof(circuit_dir, test_inputs)\n                if not proof_result:\n                    bt.logging.error(\"Proof generation failed\")\n\n                    raw_accuracy_scores.append(0.0)\n                    verification_results.append(False)\n                    proof_sizes.append(float(\"inf\"))\n                    response_times.append(float(\"inf\"))\n                    continue\n\n                proof_path, proof_data, response_time = proof_result\n                bt.logging.debug(\n                    f\"Generated proof with size: {len(proof_data['proof'])}\"\n                )\n                response_times.append(response_time)\n\n                proof = proof_data.get(\"proof\", [])\n                public_signals = [\n                    float(x)\n                    for sublist in proof_data.get(\"pretty_public_inputs\", {}).get(\n                        \"rescaled_outputs\", []\n                    )\n                    for x in sublist\n                ]\n                proof_sizes.append(len(proof))\n\n                verify_result = self._verify_proof(circuit_dir, proof_path)\n                bt.logging.debug(f\"Proof verification result: {verify_result}\")\n                verification_results.append(verify_result)\n\n                if verify_result:\n                    raw_accuracy = self._compare_outputs(\n                        baseline_output, public_signals\n                    )\n                    bt.logging.debug(f\"Raw accuracy: {raw_accuracy}\")\n                    raw_accuracy_scores.append(raw_accuracy)\n\n                else:\n                    bt.logging.error(\"Proof verification failed\")\n                    raw_accuracy_scores.append(0.0)\n\n            except Exception as e:\n                bt.logging.error(f\"Error in evaluation iteration: {str(e)}\")\n                bt.logging.error(f\"Stack trace: {traceback.format_exc()}\")\n\n                raw_accuracy_scores.append(0.0)\n                verification_results.append(False)\n                proof_sizes.append(float(\"inf\"))\n                response_times.append(float(\"inf\"))\n\n        if not all(verification_results):\n            bt.logging.error(\n                \"One or more verifications failed - setting all scores to 0\"\n            )\n            return 0.0, float(\"inf\"), float(\"inf\"), False, {}\n\n        avg_raw_accuracy = (\n            sum(raw_accuracy_scores) / len(raw_accuracy_scores)\n            if raw_accuracy_scores\n            else 0\n        )\n        avg_proof_size = (\n            sum(proof_sizes) / len(proof_sizes) if proof_sizes else float(\"inf\")\n        )\n        avg_response_time = (\n            sum(response_times) / len(response_times)\n            if response_times\n            else float(\"inf\")\n        )\n\n        sota_relative_score = self._calculate_relative_score(\n            avg_raw_accuracy, avg_proof_size, avg_response_time\n        )\n\n        improvements = self._calculate_improvements(\n            avg_raw_accuracy, avg_proof_size, avg_response_time\n        )\n\n        safe_log(\n            {\n                \"sota_relative_score\": float(sota_relative_score),\n                \"avg_raw_accuracy\": float(avg_raw_accuracy),\n                \"avg_proof_size\": (\n                    float(avg_proof_size) if avg_proof_size != float(\"inf\") else -1\n                ),\n                \"avg_response_time\": (\n                    float(avg_response_time)\n                    if avg_response_time != float(\"inf\")\n                    else -1\n                ),\n                \"improvements\": improvements,\n                \"total_iterations\": num_iterations,\n                \"successful_iterations\": len([s for s in raw_accuracy_scores if s > 0]),\n                \"verification_success_rate\": sum(verification_results)\n                / max(len(verification_results), 1),\n                \"raw_accuracy_distribution\": raw_accuracy_scores,\n                \"proof_sizes_distribution\": [\n                    float(x) if x != float(\"inf\") else -1 for x in proof_sizes\n                ],\n                \"response_times_distribution\": [\n                    float(x) if x != float(\"inf\") else -1 for x in response_times\n                ],\n            }\n        )\n\n        bt.logging.info(\n            f\"Circuit evaluation complete - SOTA Score: {sota_relative_score:.4f}, \"\n            f\"Raw Accuracy: {avg_raw_accuracy:.4f}, \"\n            f\"Proof Size: {avg_proof_size:.0f}, Response Time: {avg_response_time:.2f}s\"\n        )\n\n        return (\n            sota_relative_score,\n            avg_proof_size,\n            avg_response_time,\n            True,\n            improvements,\n            avg_raw_accuracy,\n        )\n\n    def _get_input_shape(self, circuit_dir: str) -> Tuple[int, int] | None:\n        try:\n            config_path = os.path.join(\n                self.competition_directory, \"competition_config.json\"\n            )\n            bt.logging.debug(f\"Reading config from: {config_path}\")\n            with open(config_path) as f:\n                config = json.load(f)\n                if (\n                    \"circuit_settings\" in config\n                    and \"input_shape\" in config[\"circuit_settings\"]\n                ):\n                    return tuple(config[\"circuit_settings\"][\"input_shape\"])\n            return None\n        except Exception as e:\n            bt.logging.error(f\"Error reading input shape: {e}\")\n            return None\n\n    def _run_baseline_model(self, test_inputs: torch.Tensor) -> List | None:\n        try:\n            if not self.is_onnx:\n                return self.baseline_model(test_inputs).tolist()\n\n            python_path = self._get_python_path()\n            if not python_path:\n                bt.logging.warning(\n                    f\"Python not found in ONNX venv at {self.onnx_venv}, attempting to recreate\"\n                )\n                if not self._setup_onnx_env():\n                    bt.logging.error(\"Failed to recreate ONNX environment\")\n                    return None\n\n                python_path = self._get_python_path()\n                if not python_path:\n                    bt.logging.error(\n                        \"Still couldn't find Python after recreating environment\"\n                    )\n                    return None\n\n            with (\n                tempfile.NamedTemporaryFile(suffix=\".npy\") as input_file,\n                tempfile.NamedTemporaryFile(suffix=\".npy\") as output_file,\n            ):\n                np.save(input_file.name, test_inputs.numpy())\n                model_path = os.path.abspath(self.baseline_model)\n\n                bt.logging.debug(f\"ONNX model path: {model_path}\")\n                bt.logging.debug(f\"Input shape: {test_inputs.shape}\")\n                bt.logging.debug(f\"Input file: {input_file.name}\")\n                bt.logging.debug(f\"Output file: {output_file.name}\")\n                bt.logging.debug(f\"Using Python at: {python_path}\")\n\n                if not os.path.exists(model_path):\n                    bt.logging.error(f\"ONNX model not found at {model_path}\")\n                    return None\n\n                runner_path = self._get_runner_path()\n                bt.logging.debug(f\"Using runner at: {runner_path}\")\n\n                process = subprocess.Popen(\n                    [\n                        python_path,\n                        runner_path,\n                        model_path,\n                        input_file.name,\n                        output_file.name,\n                    ],\n                    stdout=subprocess.PIPE,\n                    stderr=subprocess.PIPE,\n                    text=True,\n                    bufsize=1,\n                    universal_newlines=True,\n                )\n\n                stdout, stderr = process.communicate()\n\n                if process.returncode != 0:\n                    bt.logging.error(\n                        f\"ONNX runner failed with code {process.returncode}\"\n                    )\n                    bt.logging.error(f\"STDOUT: {stdout}\")\n                    bt.logging.error(f\"STDERR: {stderr}\")\n                    return None\n\n                output = np.load(output_file.name)\n                output_list = output.flatten().tolist()\n                return output_list\n        except Exception as e:\n            bt.logging.error(f\"Error running baseline model: {e}\")\n            bt.logging.error(f\"Stack trace: {traceback.format_exc()}\")\n            return None\n\n    def _generate_proof(\n        self, circuit_dir: str, test_inputs: torch.Tensor\n    ) -> Tuple[str, dict] | None:\n        try:\n            input_data = {\n                \"input_data\": [[float(x) for x in test_inputs.flatten().tolist()]]\n            }\n\n            with tempfile.NamedTemporaryFile(\n                mode=\"w+\", suffix=\".json\", dir=get_temp_folder(), delete=False\n            ) as temp_input:\n                json.dump(input_data, temp_input, indent=2)\n                temp_input_path = temp_input.name\n\n            with tempfile.NamedTemporaryFile(\n                mode=\"w+\", suffix=\".json\", dir=get_temp_folder(), delete=False\n            ) as temp_witness:\n                witness_path = temp_witness.name\n\n            with tempfile.NamedTemporaryFile(\n                mode=\"w+\", suffix=\".json\", dir=get_temp_folder(), delete=False\n            ) as temp_proof:\n                temp_proof_path = temp_proof.name\n\n            model_path = os.path.join(circuit_dir, \"model.compiled\")\n            if not os.path.exists(model_path):\n                bt.logging.error(f\"model.compiled not found at {model_path}\")\n                return None\n\n            bt.logging.debug(f\"Input data: {json.dumps(input_data, indent=2)}\")\n            witness_result = subprocess.run(\n                [\n                    LOCAL_EZKL_PATH,\n                    \"gen-witness\",\n                    \"--data\",\n                    temp_input_path,\n                    \"--compiled-circuit\",\n                    model_path,\n                    \"--output\",\n                    witness_path,\n                ],\n                capture_output=True,\n                text=True,\n                timeout=300,\n            )\n\n            if witness_result.returncode != 0:\n                bt.logging.error(\n                    f\"Witness generation failed with code {witness_result.returncode}\"\n                )\n                bt.logging.error(f\"STDOUT: {witness_result.stdout}\")\n                bt.logging.error(f\"STDERR: {witness_result.stderr}\")\n                return None\n\n            bt.logging.debug(\"Witness generation successful, starting proof generation\")\n            proof_start = time.perf_counter()\n            prove_result = subprocess.run(\n                [\n                    LOCAL_EZKL_PATH,\n                    \"prove\",\n                    \"--compiled-circuit\",\n                    model_path,\n                    \"--witness\",\n                    witness_path,\n                    \"--pk-path\",\n                    os.path.join(circuit_dir, \"pk.key\"),\n                    \"--proof-path\",\n                    temp_proof_path,\n                ],\n                capture_output=True,\n                text=True,\n                timeout=300,\n            )\n            proof_time = time.perf_counter() - proof_start\n\n            os.unlink(temp_input_path)\n            os.unlink(witness_path)\n\n            if prove_result.returncode != 0:\n                bt.logging.error(\n                    f\"Proof generation failed with code {prove_result.returncode}\"\n                )\n                bt.logging.error(f\"STDOUT: {prove_result.stdout}\")\n                bt.logging.error(f\"STDERR: {prove_result.stderr}\")\n                return None\n\n            with open(temp_proof_path) as f:\n                proof_data = json.load(f)\n                bt.logging.debug(f\"Proof timing - Proof: {proof_time:.3f}s\")\n                return temp_proof_path, proof_data, proof_time\n        except Exception as e:\n            bt.logging.error(f\"Error generating proof: {e}\")\n            return None\n\n    def _verify_proof(self, circuit_dir: str, proof_path: str) -> bool:\n        try:\n            verify_result = subprocess.run(\n                [\n                    LOCAL_EZKL_PATH,\n                    \"verify\",\n                    \"--proof-path\",\n                    proof_path,\n                    \"--settings-path\",\n                    os.path.join(circuit_dir, \"settings.json\"),\n                    \"--vk-path\",\n                    os.path.join(circuit_dir, \"vk.key\"),\n                ],\n                capture_output=True,\n                text=True,\n                timeout=300,\n            )\n            return verify_result.returncode == 0\n        except Exception as e:\n            bt.logging.error(f\"Error verifying proof: {e}\")\n            return False\n        finally:\n            if os.path.exists(proof_path):\n                os.unlink(proof_path)\n\n    def _compare_outputs(self, expected: list[float], actual: list[float]) -> float:\n        try:\n            with open(\n                os.path.join(self.competition_directory, \"competition_config.json\")\n            ) as f:\n                config = json.load(f)\n                output_shapes = config[\"circuit_settings\"][\"output_shapes\"]\n                total_size = sum(np.prod(shape) for shape in output_shapes.values())\n\n            if isinstance(actual, dict) and \"pretty_public_inputs\" in actual:\n                rescaled = actual[\"pretty_public_inputs\"].get(\"rescaled_outputs\", [])\n                actual = [float(x) for sublist in rescaled for x in sublist]\n            elif isinstance(actual, list):\n                if len(actual) > 0 and isinstance(actual[0], list):\n                    actual = [float(x) for sublist in actual for x in sublist]\n\n            if len(actual) != total_size:\n                bt.logging.error(\n                    f\"Output size mismatch: expected {total_size}, got {len(actual)}\"\n                )\n                return 0.0\n\n            expected = expected[:total_size]\n            expected_tensor = torch.tensor(expected)\n            actual_tensor = torch.tensor(actual)\n\n            mae = torch.nn.functional.l1_loss(actual_tensor, expected_tensor)\n            raw_accuracy = torch.exp(-mae).item()\n            return raw_accuracy\n        except Exception as e:\n            bt.logging.error(f\"Error comparing outputs: {e}\")\n            bt.logging.error(f\"Stack trace: {traceback.format_exc()}\")\n            return 0.0\n\n    def _run_onnx_model(\n        self, model_path: str, inputs: np.ndarray, output_path: str\n    ) -> Optional[np.ndarray]:\n        try:\n            python_path = self._get_python_path()\n            if not python_path:\n                bt.logging.error(f\"Python not found in ONNX venv at {self.onnx_venv}\")\n                return None\n\n            runner_path = self._get_runner_path()\n            input_path = output_path + \".input.npy\"\n            np.save(input_path, inputs)\n\n            bt.logging.debug(f\"Running ONNX model with Python at: {python_path}\")\n            bt.logging.debug(f\"Using runner at: {runner_path}\")\n            bt.logging.debug(f\"Model path: {model_path}\")\n            bt.logging.debug(f\"Input path: {input_path}\")\n            bt.logging.debug(f\"Output path: {output_path}\")\n\n            process = subprocess.Popen(\n                [python_path, runner_path, model_path, input_path, output_path],\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                text=True,\n            )\n\n            stdout, stderr = process.communicate()\n\n            if process.returncode != 0:\n                bt.logging.error(f\"ONNX runner failed with code {process.returncode}\")\n                bt.logging.error(f\"STDOUT: {stdout}\")\n                bt.logging.error(f\"STDERR: {stderr}\")\n                return None\n\n            try:\n                return np.load(output_path)\n            except Exception as e:\n                bt.logging.error(f\"Failed to load ONNX output: {e}\")\n                return None\n\n        except Exception as e:\n            bt.logging.error(f\"Error running ONNX model: {e}\")\n            bt.logging.error(f\"Stack trace: {traceback.format_exc()}\")\n            return None\n"}
{"type": "source_file", "path": "neurons/_validator/competitions/competition.py", "content": "import os\nimport shutil\nfrom typing import Dict, Tuple, List, Optional\nimport bittensor as bt\nimport platform\nimport asyncio\nimport json\nimport traceback\nimport threading\nfrom multiprocessing import Queue as MPQueue\nfrom queue import Empty\nimport time\n\ntry:\n    from async_substrate_interface.types import ScaleObj\nexcept ImportError:\n    ScaleObj = None\nfrom substrateinterface.utils.ss58 import ss58_encode\n\nfrom .models.neuron import NeuronState\nfrom .services.circuit_validator import CircuitValidator\nfrom .services.circuit_manager import CircuitManager\nfrom bittensor.core.chain_data import decode_account_id\nfrom .services.circuit_evaluator import CircuitEvaluator\nfrom .services.sota_manager import SotaManager\n\nfrom .utils.cleanup import cleanup_temp_dir\nfrom constants import VALIDATOR_REQUEST_TIMEOUT_SECONDS\nfrom _validator.models.request_type import ValidatorMessage\nfrom utils.system import get_temp_folder\nfrom utils.wandb_logger import safe_log\n\n\nclass CompetitionThread(threading.Thread):\n    def __init__(\n        self,\n        competition: \"Competition\",\n        config: bt.config,\n    ):\n        super().__init__()\n        bt.logging.info(\"=== Competition Thread Constructor Start ===\")\n        self.competition = competition\n        self._should_run = threading.Event()\n        self._should_run.set()\n        self.task_queue = MPQueue()\n        self.daemon = True\n        self.validator_to_competition_queue = None  # Messages FROM validator\n        self.competition_to_validator_queue = None  # Messages TO validator\n        self._loop = None\n        self._loop_lock = threading.Lock()\n\n        self.subtensor = bt.subtensor(config=config)\n\n        bt.logging.info(\"Competition thread initialized with:\")\n        bt.logging.info(f\"- Competition ID: {self.competition.competition_id}\")\n        bt.logging.info(f\"- Competition Dir: {self.competition.competition_directory}\")\n        bt.logging.info(f\"- Thread Daemon: {self.daemon}\")\n        bt.logging.info(\"=== Competition Thread Constructor End ===\")\n\n    def _get_loop(self):\n        with self._loop_lock:\n            if self._loop is None or self._loop.is_closed():\n                self._loop = asyncio.new_event_loop()\n            return self._loop\n\n    def set_validator_message_queues(\n        self,\n        validator_to_competition_queue: MPQueue,\n        competition_to_validator_queue: MPQueue,\n    ):\n        self.validator_to_competition_queue = validator_to_competition_queue\n        self.competition_to_validator_queue = competition_to_validator_queue\n\n    def wait_for_message(\n        self,\n        expected_message: ValidatorMessage,\n        timeout: float = VALIDATOR_REQUEST_TIMEOUT_SECONDS,\n    ) -> bool:\n        start_time = time.time()\n        while time.time() - start_time < timeout:\n            try:\n                message = self.validator_to_competition_queue.get(timeout=0.1)\n                if message == expected_message:\n                    return True\n            except Empty:\n                continue\n        bt.logging.error(f\"Timeout waiting for {expected_message} message\")\n        return False\n\n    def _process_next_download(self):\n        if not self.competition.current_download and self.competition.download_queue:\n            uid, hotkey, hash = self.competition.download_queue.pop(0)\n            self.competition.current_download = (uid, hotkey, hash)\n            bt.logging.info(\n                f\"Processing download for circuit {hash[:8]} from {hotkey[:8]}\"\n            )\n\n            try:\n                circuit_dir = os.path.join(self.competition.temp_directory, hash)\n                os.makedirs(circuit_dir, exist_ok=True)\n\n                axon = self.competition.metagraph.axons[uid]\n\n                loop = self._get_loop()\n                try:\n                    download_success = loop.run_until_complete(\n                        self.competition.circuit_manager.download_files(\n                            axon, hash, circuit_dir\n                        )\n                    )\n                except Exception as e:\n                    bt.logging.error(f\"Error during download: {str(e)}\")\n                    bt.logging.error(f\"Stack trace: {traceback.format_exc()}\")\n                    download_success = False\n\n                if download_success:\n                    bt.logging.info(\"Pausing request processing for evaluation...\")\n                    if self.competition_to_validator_queue:\n                        self.competition_to_validator_queue.put(\n                            ValidatorMessage.WINDDOWN\n                        )\n                        bt.logging.info(\"Waiting for validator winddown to complete...\")\n\n                        if not self.wait_for_message(\n                            ValidatorMessage.WINDDOWN_COMPLETE\n                        ):\n                            bt.logging.error(\n                                \"Failed to wait for validator winddown, proceeding anyway\"\n                            )\n\n                    bt.logging.info(\"Starting circuit evaluation...\")\n                    try:\n                        uid, hotkey, hash = self.competition.current_download\n                        self.competition._evaluate_circuit(\n                            circuit_dir=circuit_dir,\n                            circuit_uid=str(uid),\n                            circuit_owner=hotkey,\n                        )\n                        self.competition.cleanup_circuit_dir(circuit_dir)\n                    except Exception as e:\n                        bt.logging.error(f\"Error during circuit evaluation: {str(e)}\")\n                        bt.logging.error(f\"Stack trace: {traceback.format_exc()}\")\n                    finally:\n                        bt.logging.info(\"Resuming request processing...\")\n                        if self.competition_to_validator_queue:\n                            self.competition_to_validator_queue.put(\n                                ValidatorMessage.COMPETITION_COMPLETE\n                            )\n                else:\n                    bt.logging.error(\n                        f\"Circuit download or evaluation failed for {hash[:8]} from {hotkey[:8]}\"\n                    )\n            finally:\n                self.competition.clear_current_download()\n\n    def run(self):\n        bt.logging.info(\"Competition thread starting...\")\n\n        while self._should_run.is_set():\n            try:\n                if (\n                    not self.competition.download_queue\n                    and not self.competition.current_download\n                ):\n                    time.sleep(1)\n                    continue\n\n                self._process_next_download()\n\n            except Exception as e:\n                bt.logging.error(f\"Error in competition thread cycle: {str(e)}\")\n                bt.logging.error(f\"Stack trace: {traceback.format_exc()}\")\n                time.sleep(5)\n\n        bt.logging.info(\"Competition thread stopped\")\n\n    def stop(self):\n        bt.logging.info(\"=== Competition Thread Stop Start ===\")\n        self._should_run.clear()\n        with self._loop_lock:\n            if self._loop and not self._loop.is_closed():\n                self._loop.stop()\n                self._loop.close()\n        bt.logging.info(\"Competition thread stop signal sent\")\n        bt.logging.info(\"=== Competition Thread Stop End ===\")\n\n\nclass Competition:\n    def __init__(\n        self,\n        competition_id: int,\n        metagraph: bt.metagraph,\n        wallet: bt.wallet,\n        config: bt.config,\n    ):\n        bt.logging.info(\"=== Competition Module Initialization Start ===\")\n        bt.logging.info(f\"Initializing competition module with ID {competition_id}...\")\n        self.competition_id = competition_id\n        self.metagraph = metagraph\n        self.wallet = wallet\n        self.dendrite = bt.dendrite(wallet=wallet)\n        self.competition_directory = os.path.join(\n            os.path.dirname(__file__), str(competition_id)\n        )\n        self.temp_directory = os.path.join(get_temp_folder(), str(competition_id))\n        self.sota_directory = os.path.join(self.competition_directory, \"sota\")\n\n        if not os.path.exists(self.temp_directory):\n            os.makedirs(self.temp_directory)\n\n        if not os.path.exists(self.sota_directory):\n            os.makedirs(self.sota_directory)\n\n        bt.logging.info(\"Loading config and initializing subtensor...\")\n        config_path = os.path.join(\n            self.competition_directory, \"competition_config.json\"\n        )\n        with open(config_path) as f:\n            self.config = json.load(f)\n            self.accuracy_weight = (\n                self.config.get(\"evaluation\", {})\n                .get(\"scoring_weights\", {})\n                .get(\"accuracy\", 0.4)\n            )\n            bt.logging.debug(f\"Loaded accuracy weight: {self.accuracy_weight}\")\n\n        self.sota_manager = SotaManager(self.sota_directory)\n        self.circuit_manager = CircuitManager(\n            self.temp_directory, self.competition_id, self.dendrite\n        )\n        self.circuit_validator = CircuitValidator()\n\n        self.circuit_evaluator = CircuitEvaluator(\n            self.config, self.competition_directory, self.sota_manager\n        )\n\n        self.miner_states: Dict[str, NeuronState] = {}\n\n        self.download_queue: List[Tuple[int, str, str]] = []\n        self.current_download: Optional[Tuple[int, str, str]] = None\n        self.download_complete = asyncio.Event()\n        self.download_task: Optional[asyncio.Task] = None\n        self.download_lock = asyncio.Lock()\n\n        bt.logging.info(\"=== Starting Competition Thread ===\")\n        self.competition_thread = CompetitionThread(self, config)\n        bt.logging.info(\"Competition thread instance created\")\n        self.competition_thread.start()\n        bt.logging.info(\"=== Competition Thread Started ===\")\n\n    def fetch_commitments(self) -> List[Tuple[int, str, str]]:\n        if platform.system() != \"Darwin\" and platform.machine() != \"arm64\":\n            bt.logging.critical(\n                \"Competitions are only supported on macOS arm64 architecture.\"\n            )\n\n        queryable_uids = self.metagraph.uids\n        hotkey_to_uid = {self.metagraph.hotkeys[uid]: uid for uid in queryable_uids}\n        self.miner_states = {\n            k: v for k, v in self.miner_states.items() if k in self.metagraph.hotkeys\n        }\n        commitments = []\n\n        try:\n            commitment_map = self.competition_thread.subtensor.substrate.query_map(\n                module=\"Commitments\",\n                storage_function=\"CommitmentOf\",\n                params=[self.metagraph.netuid],\n            )\n            if not commitment_map or not hasattr(commitment_map, \"__iter__\"):\n                return []\n\n            for result in commitment_map:\n                if not result or len(result) != 2:\n                    continue\n\n                acc, info = result\n                if not acc:\n                    continue\n\n                try:\n                    if not isinstance(acc, (list, tuple)) or not acc:\n                        continue\n                    acc_bytes = bytes(\n                        acc[0] if isinstance(acc[0], (list, tuple)) else acc\n                    )\n                    hotkey = decode_account_id(acc_bytes)\n                    if ss58_encode(acc_bytes) != hotkey:\n                        continue\n\n                    if hotkey not in hotkey_to_uid:\n                        continue\n\n                    uid = hotkey_to_uid[hotkey]\n                    hash = self._extract_hash(info)\n\n                    if not hash:\n                        continue\n\n                    if self._is_new_valid_circuit(hotkey, hash):\n                        commitments.append((uid, hotkey, hash))\n                        bt.logging.success(\n                            f\"New circuit detected for {hotkey} with hash {hash}\"\n                        )\n\n                except Exception as e:\n                    bt.logging.debug(f\"Failed to process commitment: {str(e)}\")\n                    continue\n\n        except Exception as e:\n            bt.logging.error(f\"Error fetching commitments: {str(e)}\")\n            bt.logging.debug(traceback.format_exc())\n\n        return commitments\n\n    def _extract_hash(self, info) -> Optional[str]:\n        \"\"\"Extract hash from commitment info based on network type.\"\"\"\n        try:\n            if not isinstance(info, (dict, ScaleObj)):\n                return None\n\n            info_val = info.value if isinstance(info, ScaleObj) else info\n            fields = info_val.get(\"info\", {}).get(\"fields\", [])\n\n            if not fields or not fields[0]:\n                return None\n\n            commitment = fields[0][0]\n            if not commitment or not isinstance(commitment, dict):\n                return None\n\n            commitment_type = next(iter(commitment.keys()))\n            bytes_tuple = commitment[commitment_type][0]\n\n            if not isinstance(bytes_tuple, (list, tuple)):\n                return None\n\n            return bytes(bytes_tuple).decode()\n\n        except Exception:\n            return None\n\n    def _is_new_valid_circuit(self, hotkey: str, hash: str) -> bool:\n        \"\"\"Check if circuit is new and valid.\"\"\"\n        if hotkey not in self.miner_states or self.miner_states[hotkey].hash != hash:\n            if hash in {state.hash for state in self.miner_states.values()}:\n                bt.logging.warning(\n                    f\"Circuit with hash {hash} already exists for another miner\"\n                )\n                return False\n            return True\n        return False\n\n    async def process_downloads(self) -> bool:\n        try:\n\n            async with self.download_lock:\n                if not self.download_queue and not self.current_download:\n                    return False\n\n                if not self.current_download and self.download_queue:\n                    self.current_download = self.download_queue.pop(0)\n\n            if self.current_download:\n                uid, hotkey, hash = self.current_download\n\n                axon = self.metagraph.axons[uid]\n                circuit_dir = os.path.join(self.temp_directory, hash)\n                os.makedirs(circuit_dir, exist_ok=True)\n\n                if await self.circuit_manager.download_files(axon, hash, circuit_dir):\n                    if self.circuit_validator.validate_files(circuit_dir):\n                        self.miner_states[hotkey] = NeuronState(\n                            hotkey=hotkey,\n                            uid=uid,\n                            sota_relative_score=0.0,\n                            proof_size=0,\n                            response_time=0.0,\n                            verification_result=False,\n                            raw_accuracy=0.0,\n                            hash=hash,\n                        )\n                        return True\n                if os.path.exists(circuit_dir):\n                    shutil.rmtree(circuit_dir)\n\n                self.current_download = None\n                return False\n\n        except Exception as e:\n            bt.logging.error(f\"Error in download processing: {e}\")\n            traceback.print_exc()\n            if self.current_download:\n                uid, hotkey, hash = self.current_download\n                circuit_dir = os.path.join(self.temp_directory, hash)\n                if os.path.exists(circuit_dir):\n                    shutil.rmtree(circuit_dir)\n                self.current_download = None\n            return False\n\n    def queue_download(self, uid: int, hotkey: str, hash: str) -> None:\n        self.download_queue.append((uid, hotkey, hash))\n        self.download_complete.set()\n\n    def get_current_download(self) -> Optional[Tuple[int, str, str]]:\n        return self.current_download\n\n    def clear_current_download(self) -> None:\n        \"\"\"Clear the current download without cleaning up the directory.\"\"\"\n        self.current_download = None\n\n    def cleanup_circuit_dir(self, circuit_dir: str) -> None:\n        \"\"\"Clean up a circuit directory if it exists.\"\"\"\n        if os.path.exists(circuit_dir):\n            bt.logging.info(f\"Cleaning up circuit directory: {circuit_dir}\")\n            specific_dir = os.path.basename(circuit_dir)\n            cleanup_temp_dir(specific_dir=specific_dir)\n\n    def _evaluate_circuit(\n        self,\n        circuit_dir: str,\n        circuit_uid: str,\n        circuit_owner: str,\n    ) -> bool:\n        try:\n            (\n                sota_relative_score,\n                proof_size,\n                response_time,\n                verification_result,\n                improvements,\n                accuracy,\n            ) = self.circuit_evaluator.evaluate(circuit_dir)\n\n            if circuit_owner not in self.miner_states:\n                self.miner_states[circuit_owner] = NeuronState(\n                    hotkey=circuit_owner,\n                    uid=int(circuit_uid),\n                    sota_relative_score=0.0,\n                    proof_size=0,\n                    response_time=0.0,\n                    verification_result=False,\n                    raw_accuracy=0.0,\n                    hash=os.path.basename(circuit_dir),\n                )\n\n            neuron_state = self.miner_states[circuit_owner]\n            neuron_state.sota_relative_score = sota_relative_score\n            neuron_state.proof_size = proof_size\n            neuron_state.response_time = response_time\n            neuron_state.verification_result = verification_result\n            neuron_state.raw_accuracy = accuracy\n            if verification_result and self.sota_manager.check_if_sota(\n                sota_relative_score, proof_size, response_time, improvements\n            ):\n                self.sota_manager.preserve_circuit(\n                    circuit_dir, neuron_state, self.miner_states\n                )\n\n            self._update_competition_metrics()\n            return verification_result\n        except Exception as e:\n            bt.logging.error(f\"Error in competition thread cycle: {str(e)}\")\n            bt.logging.error(f\"Stack trace: {traceback.format_exc()}\")\n            return False\n\n    def _update_competition_metrics(self):\n        try:\n            active_miners = [\n                (k, v)\n                for k, v in self.miner_states.items()\n                if v.verification_result and v.sota_relative_score > 0\n            ]\n            active_participants = len(active_miners)\n\n            sota_state = self.sota_manager.current_state\n\n            if active_miners:\n                sorted_by_accuracy = sorted(\n                    active_miners, key=lambda x: x[1].raw_accuracy, reverse=True\n                )\n                sorted_by_proof_size = sorted(\n                    active_miners, key=lambda x: x[1].proof_size\n                )\n                sorted_by_response_time = sorted(\n                    active_miners, key=lambda x: x[1].response_time\n                )\n\n                metrics = {}\n\n                for miner_hotkey, miner_state in active_miners:\n                    accuracy_rank = (\n                        next(\n                            i\n                            for i, (h, _) in enumerate(sorted_by_accuracy)\n                            if h == miner_hotkey\n                        )\n                        + 1\n                    )\n                    proof_size_rank = (\n                        next(\n                            i\n                            for i, (h, _) in enumerate(sorted_by_proof_size)\n                            if h == miner_hotkey\n                        )\n                        + 1\n                    )\n                    response_time_rank = (\n                        next(\n                            i\n                            for i, (h, _) in enumerate(sorted_by_response_time)\n                            if h == miner_hotkey\n                        )\n                        + 1\n                    )\n                    overall_rank = (\n                        accuracy_rank + proof_size_rank + response_time_rank\n                    ) / 3\n\n                    metrics.update(\n                        {\n                            f\"hotkey.{miner_hotkey}.uid\": miner_state.uid,\n                            f\"hotkey.{miner_hotkey}.sota_score\": miner_state.sota_relative_score,\n                            f\"hotkey.{miner_hotkey}.raw_accuracy\": miner_state.raw_accuracy,\n                            f\"hotkey.{miner_hotkey}.proof_size\": miner_state.proof_size,\n                            f\"hotkey.{miner_hotkey}.response_time\": miner_state.response_time,\n                            f\"hotkey.{miner_hotkey}.relative_to_sota.accuracy\": (\n                                miner_state.raw_accuracy / sota_state.raw_accuracy\n                                if sota_state.raw_accuracy > 0\n                                else 0\n                            ),\n                            f\"hotkey.{miner_hotkey}.relative_to_sota.proof_size\": (\n                                sota_state.proof_size / miner_state.proof_size\n                                if miner_state.proof_size > 0\n                                else 0\n                            ),\n                            f\"hotkey.{miner_hotkey}.relative_to_sota.response_time\": (\n                                sota_state.response_time / miner_state.response_time\n                                if miner_state.response_time > 0\n                                else 0\n                            ),\n                            f\"hotkey.{miner_hotkey}.rank.overall\": overall_rank,\n                            f\"hotkey.{miner_hotkey}.rank.accuracy\": accuracy_rank,\n                            f\"hotkey.{miner_hotkey}.rank.proof_size\": proof_size_rank,\n                            f\"hotkey.{miner_hotkey}.rank.response_time\": response_time_rank,\n                        }\n                    )\n\n                    if hasattr(miner_state, \"last_score\"):\n                        time_delta = (\n                            time.time() - miner_state.last_score_time\n                            if hasattr(miner_state, \"last_score_time\")\n                            else 1\n                        )\n                        improvement_rate = (\n                            miner_state.sota_relative_score - miner_state.last_score\n                        ) / time_delta\n                        metrics.update(\n                            {\n                                f\"hotkey.{miner_hotkey}.historical.improvement_rate\": improvement_rate,\n                                f\"hotkey.{miner_hotkey}.historical.best_sota_score\": max(\n                                    miner_state.sota_relative_score,\n                                    getattr(miner_state, \"best_sota_score\", 0),\n                                ),\n                            }\n                        )\n\n                    miner_state.last_score = miner_state.sota_relative_score\n                    miner_state.last_score_time = time.time()\n                    miner_state.best_sota_score = max(\n                        miner_state.sota_relative_score,\n                        getattr(miner_state, \"best_sota_score\", 0),\n                    )\n\n                metrics.update(\n                    {\n                        \"active_participants\": active_participants,\n                        \"avg_raw_accuracy\": sum(\n                            m[1].raw_accuracy for m in active_miners\n                        )\n                        / active_participants,\n                        \"avg_proof_size\": sum(m[1].proof_size for m in active_miners)\n                        / active_participants,\n                        \"avg_response_time\": sum(\n                            m[1].response_time for m in active_miners\n                        )\n                        / active_participants,\n                        \"sota_relative_score\": sota_state.sota_relative_score,\n                        \"sota_hotkey\": sota_state.hotkey,\n                        \"sota_proof_size\": sota_state.proof_size,\n                        \"sota_response_time\": sota_state.response_time,\n                        \"sota_timestamp\": sota_state.timestamp,\n                        \"sota_raw_accuracy\": sota_state.raw_accuracy,\n                        \"sota_uid\": sota_state.uid,\n                    }\n                )\n\n                safe_log(metrics)\n        except Exception as e:\n            bt.logging.warning(f\"Error updating competition metrics: {e}\")\n            bt.logging.debug(f\"Stack trace: {traceback.format_exc()}\")\n\n    def set_validator_message_queues(\n        self,\n        validator_to_competition_queue: MPQueue,\n        competition_to_validator_queue: MPQueue,\n    ):\n        self.competition_thread.set_validator_message_queues(\n            validator_to_competition_queue, competition_to_validator_queue\n        )\n"}
{"type": "source_file", "path": "neurons/_validator/api/websocket_manager.py", "content": "from __future__ import annotations\nfrom fastapi import WebSocket\n\n\nclass WebSocketManager:\n    def __init__(self):\n        self.active_connections: set[WebSocket] = set()\n\n    async def connect(self, websocket: WebSocket) -> None:\n        await websocket.accept()\n        self.active_connections.add(websocket)\n\n    async def disconnect(self, websocket: WebSocket) -> None:\n        self.active_connections.remove(websocket)\n        await websocket.close()\n\n    async def close_all(self) -> None:\n        for connection in self.active_connections.copy():\n            await self.disconnect(connection)\n"}
{"type": "source_file", "path": "neurons/_validator/models/miner_response.py", "content": "from __future__ import annotations\nfrom dataclasses import dataclass\n\nimport bittensor as bt\nimport json\n\nfrom constants import (\n    DEFAULT_PROOF_SIZE,\n    SINGLE_PROOF_OF_WEIGHTS_MODEL_ID,\n    CRICUIT_TIMEOUT_SECONDS,\n)\nfrom deployment_layer.circuit_store import circuit_store\nfrom _validator.core.request import Request\nfrom execution_layer.circuit import ProofSystem, Circuit\nfrom _validator.models.request_type import RequestType\n\n\n@dataclass\nclass MinerResponse:\n    \"\"\"\n    Represents a response from a miner.\n\n    Attributes:\n        uid (int): Unique identifier of the miner.\n        verification_result (bool): Whether the miner's response was verified.\n        response_time (float): Time taken by the miner to respond.\n        verification_time (float): Time taken to verify the proof.\n        proof_size (int): Size of the proof provided by the miner.\n        circuit (Circuit): Circuit used.\n        proof_content (Any): Content of the proof - either a string or a dict.\n        raw (str): Deserialized form of the response.\n        error (str): Error message, if any occurred during processing.\n    \"\"\"\n\n    uid: int\n    verification_result: bool\n    input_hash: str\n    response_time: float\n    proof_size: int\n    circuit: Circuit\n    verification_time: float | None = None\n    proof_content: dict | str | None = None\n    public_json: list[str] | None = None\n    request_type: RequestType | None = None\n    raw: dict | None = None\n    error: str | None = None\n    save: bool = False\n\n    @classmethod\n    def from_raw_response(cls, response: Request) -> \"MinerResponse\":\n        \"\"\"\n        Creates a MinerResponse object from a raw response dictionary.\n\n        Args:\n            response (dict): Raw response from a miner.\n\n        Returns:\n            MinerResponse: Processed miner response object.\n        \"\"\"\n        try:\n            deserialized_response = response.deserialized\n\n            proof_content = None\n            public_json = None\n            if isinstance(deserialized_response, str):\n                try:\n                    deserialized_response = json.loads(deserialized_response)\n                except json.JSONDecodeError as e:\n                    bt.logging.debug(f\"JSON decoding error: {e}\")\n                    return cls.empty(uid=response.uid, circuit=response.circuit)\n\n            if isinstance(deserialized_response, dict):\n                proof = deserialized_response.get(\"proof\", \"{}\")\n                public_signals = deserialized_response.get(\"public_signals\", \"[]\")\n\n                if isinstance(proof, str):\n                    if all(c in \"0123456789ABCDEFabcdef\" for c in proof):\n                        proof_content = proof\n                    else:\n                        proof_content = json.loads(proof)\n                else:\n                    proof_content = proof\n                if public_signals:\n                    public_json = (\n                        json.loads(public_signals)\n                        if isinstance(public_signals, str)\n                        else public_signals\n                    )\n\n            if isinstance(proof_content, str):\n                proof_size = len(proof_content)\n            else:\n                if response.circuit.proof_system == ProofSystem.CIRCOM:\n                    proof_size = (\n                        sum(\n                            len(str(value))\n                            for key in (\"pi_a\", \"pi_b\", \"pi_c\")\n                            for element in proof_content.get(key, [])\n                            for value in (\n                                element if isinstance(element, list) else [element]\n                            )\n                        )\n                        if proof_content\n                        else DEFAULT_PROOF_SIZE\n                    )\n                elif response.circuit.proof_system == ProofSystem.EZKL:\n                    proof_size = len(proof_content[\"proof\"])\n                else:\n                    proof_size = DEFAULT_PROOF_SIZE\n\n            return cls(\n                uid=response.uid,\n                verification_result=False,\n                response_time=response.response_time,\n                proof_size=proof_size or DEFAULT_PROOF_SIZE,\n                circuit=response.circuit,\n                proof_content=proof_content,\n                request_type=response.request_type,\n                input_hash=response.request_hash,\n                public_json=public_json,\n                raw=deserialized_response,\n                save=response.save,\n            )\n        except json.JSONDecodeError as e:\n            bt.logging.error(f\"JSON decoding error: {e}\")\n            return cls.empty(uid=response.uid, circuit=response.circuit)\n        except Exception as e:\n            bt.logging.error(f\"Error processing miner response: {e}\")\n            return cls.empty(uid=response.uid, circuit=response.circuit)\n\n    @classmethod\n    def empty(cls, uid: int = 0, circuit: Circuit | None = None) -> \"MinerResponse\":\n        \"\"\"\n        Creates an empty MinerResponse object.\n\n        Returns:\n            MinerResponse: An empty MinerResponse object.\n        \"\"\"\n        if circuit is None:\n            circuit = circuit_store.get_circuit(SINGLE_PROOF_OF_WEIGHTS_MODEL_ID)\n        timeout = circuit.timeout if circuit and circuit.timeout else CRICUIT_TIMEOUT_SECONDS\n        return cls(\n            uid=uid,\n            verification_result=False,\n            response_time=timeout,\n            verification_time=None,\n            proof_size=DEFAULT_PROOF_SIZE,\n            circuit=circuit,\n            proof_content=None,\n            public_json=None,\n            request_type=RequestType.BENCHMARK,\n            input_hash=None,\n            raw=None,\n            error=\"Empty response\",\n            save=False,\n        )\n\n    def to_log_dict(self, metagraph: bt.metagraph) -> dict:  # type: ignore\n        \"\"\"\n        Parse a MinerResponse object into a dictionary.\n        \"\"\"\n        return {\n            \"miner_key\": metagraph.hotkeys[self.uid],\n            \"miner_uid\": self.uid,\n            \"proof_model\": (\n                self.circuit.metadata.name\n                if self.circuit is not None\n                else str(self.circuit.id)\n            ),\n            \"proof_system\": (\n                self.circuit.metadata.proof_system\n                if self.circuit is not None\n                else \"Unknown\"\n            ),\n            \"proof_size\": self.proof_size,\n            \"response_duration\": self.response_time,\n            \"is_verified\": self.verification_result,\n            \"input_hash\": self.input_hash,\n            \"request_type\": self.request_type.value,\n            \"error\": self.error,\n            \"save\": self.save,\n        }\n\n    def set_verification_result(self, result: bool):\n        \"\"\"\n        Sets the verification result for the miner's response.\n\n        Args:\n            result (bool): The verification result to set.\n        \"\"\"\n        self.verification_result = result\n\n    def __iter__(self):\n        return iter(self.__dict__.items())\n"}
{"type": "source_file", "path": "neurons/_validator/competitions/utils/cleanup.py", "content": "import os\nimport shutil\nfrom utils.system import get_temp_folder\nimport bittensor as bt\n\n\ndef cleanup_temp_dir(signum=None, frame=None, specific_dir=None):\n    temp_folder = get_temp_folder()\n    if not os.path.exists(temp_folder):\n        return\n\n    if specific_dir:\n        dir_path = os.path.join(temp_folder, specific_dir)\n        if os.path.exists(dir_path):\n            try:\n                if os.path.isfile(dir_path) or os.path.islink(dir_path):\n                    os.unlink(dir_path)\n                elif os.path.isdir(dir_path):\n                    shutil.rmtree(dir_path)\n            except Exception as e:\n                bt.logging.error(f\"Error cleaning up directory {dir_path}: {e}\")\n    else:\n        bt.logging.debug(\"No specific directory provided for cleanup, skipping...\")\n"}
{"type": "source_file", "path": "neurons/_validator/models/pow_rpc_request.py", "content": "from __future__ import annotations\nfrom _validator.models.base_rpc_request import RealWorldRequest\nfrom pydantic import Field\nfrom deployment_layer.circuit_store import circuit_store\n\n\nclass ProofOfWeightsRPCRequest(RealWorldRequest):\n    \"\"\"\n    Request for the Proof of Weights RPC method.\n    \"\"\"\n\n    weights_version: int | None = Field(\n        None, description=\"The version of weights in use by the origin subnet\"\n    )\n    netuid: int = Field(..., description=\"The origin subnet UID\")\n    evaluation_data: dict = Field(default_factory=dict)\n\n    class Config:\n        arbitrary_types_allowed = True\n        extra = \"allow\"\n\n    def __init__(self, **data):\n        netuid = data.get(\"netuid\")\n        weights_version = data.get(\"weights_version\")\n        evaluation_data = data.get(\"evaluation_data\")\n\n        circuit = None\n        if weights_version is None:\n            circuit = circuit_store.get_latest_circuit_for_netuid(netuid)\n            weights_version = circuit.metadata.weights_version\n        else:\n            circuit = circuit_store.get_circuit_for_netuid_and_version(\n                netuid=netuid, version=weights_version\n            )\n        if circuit is None:\n            raise ValueError(\n                f\"No circuit found for netuid {netuid} and weights version {weights_version}\"\n            )\n\n        super().__init__(\n            circuit=circuit,\n            inputs=evaluation_data,\n            evaluation_data=evaluation_data,\n            netuid=netuid,\n            weights_version=weights_version,\n        )\n"}
{"type": "source_file", "path": "neurons/_validator/config/api.py", "content": "import bittensor as bt\n\n\nclass ApiConfig:\n    \"\"\"\n    Configuration class for the API.\n\n    Attributes:\n        enabled (bool): Whether the API is enabled.\n        host (str): The host for the API.\n        port (int): The port for the API.\n        workers (int): The number of workers for the API.\n        verify_external_signatures (bool): Whether to verify external signatures.\n        certificate_path (str): The path to the certificate directory.\n        serve_axon (bool): Whether to serve the axon displaying your API information.\n    \"\"\"\n\n    def __init__(self, config: bt.config):\n        self.enabled = not config.ignore_external_requests\n        self.host = config.external_api_host\n        self.port = config.external_api_port\n        self.workers = config.external_api_workers\n        self.verify_external_signatures = not config.do_not_verify_external_signatures\n        self.certificate_path = config.certificate_path\n        self.whitelisted_public_keys = config.whitelisted_public_keys\n        self.serve_axon = config.serve_axon\n"}
{"type": "source_file", "path": "neurons/_validator/competitions/models/sota.py", "content": "from dataclasses import dataclass\n\n\n@dataclass\nclass SotaState:\n    sota_relative_score: float = 0.0\n    hash: str | None = None\n    hotkey: str | None = None\n    uid: int | None = None\n    proof_size: float = float(\"inf\")\n    response_time: float = float(\"inf\")\n    timestamp: int = 0\n    raw_accuracy: float = 0.0\n"}
{"type": "source_file", "path": "neurons/_validator/models/base_rpc_request.py", "content": "from pydantic import BaseModel\nfrom execution_layer.circuit import Circuit\nfrom _validator.utils.api import hash_inputs\n\n\nclass RealWorldRequest(BaseModel):\n    circuit: Circuit\n    inputs: dict\n\n    model_config = {\"arbitrary_types_allowed\": True}\n\n    @property\n    def hash(self) -> str:\n        return hash_inputs(self.inputs)\n"}
{"type": "source_file", "path": "neurons/_validator/core/prometheus.py", "content": "import statistics\nimport threading\nimport psutil\nfrom typing import Optional\nfrom wsgiref.simple_server import WSGIServer\n\nfrom prometheus_client import Histogram, Gauge, Counter, start_http_server\n\n\n_server: Optional[WSGIServer] = None\n_thread: Optional[threading.Thread] = None\n\n# Performance Metrics\n_validation_times: Optional[Histogram] = None\n_response_times: Optional[Histogram] = None\n_proof_sizes: Optional[Histogram] = None\n\n# Success/Failure Metrics\n_verification_ratio: Optional[Histogram] = None\n_verification_failures: Optional[Counter] = None\n_timeout_counter: Optional[Counter] = None\n_network_errors: Optional[Counter] = None\n\n# Resource Usage\n_active_requests: Optional[Gauge] = None\n_processed_uids: Optional[Gauge] = None\n_memory_usage: Optional[Gauge] = None\n_cpu_usage: Optional[Gauge] = None\n_disk_usage: Optional[Gauge] = None\n_network_io: Optional[Gauge] = None\n\n# Queue Metrics\n_request_queue_size: Optional[Gauge] = None\n_request_queue_latency: Optional[Histogram] = None\n\n# Weight Update Metrics\n_weight_update_duration: Optional[Histogram] = None\n_weight_update_failures: Optional[Counter] = None\n_last_weight_update: Optional[Gauge] = None\n\n# Score Metrics\n_score_changes: Optional[Histogram] = None\n_score_distribution: Optional[Histogram] = None\n\n# Business Metrics\n_total_proofs_verified: Optional[Counter] = None\n_total_requests_processed: Optional[Counter] = None\n_avg_response_time: Optional[Gauge] = None\n\n# Error Tracking\n_error_counter: Optional[Counter] = None\n_last_error_timestamp: Optional[Gauge] = None\n\n\ndef start_prometheus_logging(port: int) -> None:\n    global _server\n    global _thread\n    global _validation_times\n    global _response_times\n    global _proof_sizes\n    global _verification_ratio\n    global _verification_failures\n    global _timeout_counter\n    global _network_errors\n    global _active_requests\n    global _processed_uids\n    global _memory_usage\n    global _cpu_usage\n    global _disk_usage\n    global _network_io\n    global _request_queue_size\n    global _request_queue_latency\n    global _weight_update_duration\n    global _weight_update_failures\n    global _last_weight_update\n    global _score_changes\n    global _score_distribution\n    global _total_proofs_verified\n    global _total_requests_processed\n    global _avg_response_time\n    global _error_counter\n    global _last_error_timestamp\n\n    _server, _thread = start_http_server(port)\n\n    # Performance Metrics\n    _validation_times = Histogram(\n        \"validating_seconds\",\n        \"Time spent validating responses\",\n        buckets=(\n            0.005,\n            0.01,\n            0.025,\n            0.05,\n            0.075,\n            0.1,\n            0.25,\n            0.5,\n            0.75,\n            1.0,\n            2.5,\n            5.0,\n            7.5,\n            10.0,\n        ),\n    )\n    _response_times = Histogram(\n        \"requests_seconds\",\n        \"Time spent processing requests\",\n        [\"aggregation_type\", \"model\"],\n        buckets=(0.1, 0.5, 1.0, 2.5, 5.0, 7.5, 10.0, 15.0, 20.0, 30.0),\n    )\n    _proof_sizes = Histogram(\n        \"proof_sizes\",\n        \"Size of proofs in bytes\",\n        [\"aggregation_type\", \"model\"],\n        buckets=(1000, 2500, 5000, 7500, 10000, 25000, 50000, 75000, 100000),\n    )\n\n    # Success/Failure Metrics\n    _verification_ratio = Histogram(\n        \"verified_proofs_ratio\", \"Ratio of successfully verified proofs\", [\"model\"]\n    )\n    _verification_failures = Counter(\n        \"verification_failures_total\",\n        \"Total number of proof verification failures\",\n        [\"model\", \"failure_type\"],\n    )\n    _timeout_counter = Counter(\n        \"timeouts_total\", \"Total number of request timeouts\", [\"model\"]\n    )\n    _network_errors = Counter(\n        \"network_errors_total\", \"Total number of network errors\", [\"error_type\"]\n    )\n\n    # Resource Usage\n    _active_requests = Gauge(\"active_requests\", \"Number of currently active requests\")\n    _processed_uids = Gauge(\"processed_uids\", \"Number of processed UIDs\")\n    _memory_usage = Gauge(\"memory_usage_bytes\", \"Current memory usage in bytes\")\n    _cpu_usage = Gauge(\"cpu_usage_percent\", \"Current CPU usage percentage\")\n    _disk_usage = Gauge(\n        \"disk_usage_bytes\", \"Current disk usage in bytes\", [\"mount_point\"]\n    )\n    _network_io = Gauge(\"network_io_bytes\", \"Network IO statistics\", [\"direction\"])\n\n    # Queue Metrics\n    _request_queue_size = Gauge(\n        \"request_queue_size\", \"Current size of the request queue\"\n    )\n    _request_queue_latency = Histogram(\n        \"request_queue_latency_seconds\",\n        \"Time requests spend in queue\",\n        buckets=(0.1, 0.5, 1.0, 2.5, 5.0, 7.5, 10.0, 15.0, 20.0, 30.0),\n    )\n\n    # Weight Update Metrics\n    _weight_update_duration = Histogram(\n        \"weight_update_duration_seconds\",\n        \"Time spent updating weights\",\n        buckets=(0.1, 0.5, 1.0, 2.5, 5.0, 7.5, 10.0, 15.0, 20.0, 30.0),\n    )\n    _weight_update_failures = Counter(\n        \"weight_update_failures_total\",\n        \"Total number of weight update failures\",\n        [\"reason\"],\n    )\n    _last_weight_update = Gauge(\n        \"last_weight_update_timestamp\",\n        \"Timestamp of last successful weight update\",\n    )\n\n    # Score Metrics\n    _score_changes = Histogram(\n        \"score_changes\",\n        \"Changes in miner scores\",\n        [\"direction\"],\n        buckets=(-1.0, -0.5, -0.1, -0.01, 0.0, 0.01, 0.1, 0.5, 1.0),\n    )\n    _score_distribution = Histogram(\n        \"score_distribution\",\n        \"Distribution of miner scores\",\n        buckets=(0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0),\n    )\n\n    # Business Metrics\n    _total_proofs_verified = Counter(\n        \"total_proofs_verified\",\n        \"Total number of proofs successfully verified\",\n        [\"model\"],\n    )\n    _total_requests_processed = Counter(\n        \"total_requests_processed\",\n        \"Total number of requests processed\",\n        [\"model\", \"status\"],\n    )\n    _avg_response_time = Gauge(\n        \"avg_response_time_seconds\", \"Moving average of response times\", [\"model\"]\n    )\n\n    # Error Tracking\n    _error_counter = Counter(\n        \"errors_total\",\n        \"Total number of errors by type\",\n        [\"error_type\", \"component\"],\n    )\n    _last_error_timestamp = Gauge(\n        \"last_error_timestamp\",\n        \"Timestamp of last error by type\",\n        [\"error_type\", \"component\"],\n    )\n\n\ndef stop_prometheus_logging() -> None:\n    global _server\n    global _thread\n    global _validation_times\n    global _response_times\n    global _proof_sizes\n    global _verification_ratio\n    global _verification_failures\n    global _timeout_counter\n    global _network_errors\n    global _active_requests\n    global _processed_uids\n    global _memory_usage\n    global _cpu_usage\n    global _disk_usage\n    global _network_io\n    global _request_queue_size\n    global _request_queue_latency\n    global _weight_update_duration\n    global _weight_update_failures\n    global _last_weight_update\n    global _score_changes\n    global _score_distribution\n    global _total_proofs_verified\n    global _total_requests_processed\n    global _avg_response_time\n    global _error_counter\n    global _last_error_timestamp\n\n    if _server:\n        _server.shutdown()\n        _server = None\n        _thread = None\n        _validation_times = None\n        _response_times = None\n        _proof_sizes = None\n        _verification_ratio = None\n        _verification_failures = None\n        _timeout_counter = None\n        _network_errors = None\n        _active_requests = None\n        _processed_uids = None\n        _memory_usage = None\n        _cpu_usage = None\n        _disk_usage = None\n        _network_io = None\n        _request_queue_size = None\n        _request_queue_latency = None\n        _weight_update_duration = None\n        _weight_update_failures = None\n        _last_weight_update = None\n        _score_changes = None\n        _score_distribution = None\n        _total_proofs_verified = None\n        _total_requests_processed = None\n        _avg_response_time = None\n        _error_counter = None\n        _last_error_timestamp = None\n\n\ndef log_validation_time(time: float) -> None:\n    if _validation_times:\n        _validation_times.observe(time)\n\n\ndef log_response_times(response_times: list[float], model_name: str) -> None:\n    if _response_times and response_times:\n        _response_times.labels(\"max\", model_name).observe(max(response_times))\n        _response_times.labels(\"min\", model_name).observe(min(response_times))\n        mean = statistics.mean(response_times)\n        _response_times.labels(\"mean\", model_name).observe(mean)\n        _response_times.labels(\"median\", model_name).observe(\n            statistics.median(response_times)\n        )\n\n        if _avg_response_time:\n            _avg_response_time.labels(model_name).set(mean)\n\n        if _total_requests_processed:\n            _total_requests_processed.labels(model_name, \"success\").inc(\n                len(response_times)\n            )\n\n\ndef log_proof_sizes(proof_sizes: list[int], model_name: str) -> None:\n    if _proof_sizes and proof_sizes:\n        _proof_sizes.labels(\"max\", model_name).observe(max(proof_sizes))\n        _proof_sizes.labels(\"min\", model_name).observe(min(proof_sizes))\n        _proof_sizes.labels(\"mean\", model_name).observe(statistics.mean(proof_sizes))\n        _proof_sizes.labels(\"median\", model_name).observe(\n            statistics.median(proof_sizes)\n        )\n\n\ndef log_verification_ratio(value: float, model_name: str) -> None:\n    if _verification_ratio:\n        _verification_ratio.labels(model_name).observe(value)\n\n    if _total_proofs_verified and value > 0:\n        _total_proofs_verified.labels(model_name).inc()\n\n\ndef log_verification_failure(model_name: str, failure_type: str) -> None:\n    if _verification_failures:\n        _verification_failures.labels(model_name, failure_type).inc()\n\n    if _total_requests_processed:\n        _total_requests_processed.labels(model_name, \"failed\").inc()\n\n\ndef log_timeout(model_name: str) -> None:\n    if _timeout_counter:\n        _timeout_counter.labels(model_name).inc()\n\n    if _total_requests_processed:\n        _total_requests_processed.labels(model_name, \"timeout\").inc()\n\n\ndef log_network_error(error_type: str) -> None:\n    if _network_errors:\n        _network_errors.labels(error_type).inc()\n\n\ndef log_request_metrics(\n    active_requests: int,\n    processed_uids: int,\n    memory_bytes: Optional[int] = None,\n) -> None:\n    if _active_requests:\n        _active_requests.set(active_requests)\n    if _processed_uids:\n        _processed_uids.set(processed_uids)\n    if _memory_usage and memory_bytes:\n        _memory_usage.set(memory_bytes)\n\n\ndef log_system_metrics() -> None:\n    if _cpu_usage:\n        _cpu_usage.set(psutil.cpu_percent())\n\n    if _memory_usage:\n        memory = psutil.Process().memory_info()\n        _memory_usage.set(memory.rss)\n\n    if _disk_usage:\n        for partition in psutil.disk_partitions():\n            usage = psutil.disk_usage(partition.mountpoint)\n            _disk_usage.labels(partition.mountpoint).set(usage.used)\n\n    if _network_io:\n        net_io = psutil.net_io_counters()\n        _network_io.labels(\"bytes_sent\").set(net_io.bytes_sent)\n        _network_io.labels(\"bytes_recv\").set(net_io.bytes_recv)\n\n\ndef log_queue_metrics(queue_size: int, latency: float) -> None:\n    if _request_queue_size:\n        _request_queue_size.set(queue_size)\n    if _request_queue_latency:\n        _request_queue_latency.observe(latency)\n\n\ndef log_weight_update(\n    duration: float, success: bool = True, failure_reason: str = \"\"\n) -> None:\n    if _weight_update_duration and success:\n        _weight_update_duration.observe(duration)\n        _last_weight_update.set_to_current_time()\n    elif _weight_update_failures and not success:\n        _weight_update_failures.labels(failure_reason).inc()\n\n\ndef log_score_change(old_score: float, new_score: float) -> None:\n    if _score_changes:\n        change = new_score - old_score\n        direction = \"increase\" if change > 0 else \"decrease\"\n        _score_changes.labels(direction).observe(abs(change))\n\n    if _score_distribution:\n        _score_distribution.observe(new_score)\n\n\ndef log_error(error_type: str, component: str, error_msg: str) -> None:\n    if _error_counter:\n        _error_counter.labels(error_type, component).inc()\n    if _last_error_timestamp:\n        _last_error_timestamp.labels(error_type, component).set_to_current_time()\n"}
{"type": "source_file", "path": "neurons/_validator/api/cache.py", "content": "import datetime\nimport asyncio\nfrom _validator.config import ValidatorConfig\nimport bittensor as bt\n\n\nclass ValidatorKeysCache:\n    \"\"\"\n    A thread-safe cache for validator keys to reduce the number of requests to the metagraph.\n    \"\"\"\n\n    def __init__(self, config: ValidatorConfig) -> None:\n        self.cached_keys: dict[int, list[str]] = {}\n        self.cached_timestamps: dict[int, datetime.datetime] = {}\n        self.config: ValidatorConfig = config\n        self._lock = asyncio.Lock()\n\n    async def fetch_validator_keys(self, netuid: int) -> None:\n        \"\"\"\n        Fetch the validator keys for a given netuid and cache them.\n        Thread-safe implementation using a lock.\n        \"\"\"\n        subtensor = bt.subtensor(config=self.config.bt_config)\n        self.cached_keys[netuid] = [\n            neuron.hotkey\n            for neuron in subtensor.neurons_lite(netuid)\n            if neuron.validator_permit\n        ]\n        self.cached_timestamps[netuid] = datetime.datetime.now() + datetime.timedelta(\n            hours=12\n        )\n\n    async def check_validator_key(self, ss58_address: str, netuid: int) -> bool:\n        \"\"\"\n        Thread-safe check if a given key is a validator key for a given netuid.\n        \"\"\"\n        if (\n            self.config.api.whitelisted_public_keys\n            and ss58_address in self.config.api.whitelisted_public_keys\n        ):\n            # If the sender is whitelisted, we don't need to check the key\n            return True\n\n        cache_timestamp = self.cached_timestamps.get(netuid, None)\n        if cache_timestamp is None or cache_timestamp < datetime.datetime.now():\n            await self.fetch_validator_keys(netuid)\n        return ss58_address in self.cached_keys.get(netuid, [])\n\n    async def check_whitelisted_key(self, ss58_address: str) -> bool:\n        if not self.config.api.whitelisted_public_keys:\n            return False\n        return ss58_address in self.config.api.whitelisted_public_keys\n"}
{"type": "source_file", "path": "neurons/_validator/competitions/models/neuron.py", "content": "from dataclasses import dataclass\n\n\n@dataclass\nclass NeuronState:\n    hotkey: str\n    uid: int\n    sota_relative_score: float\n    proof_size: float\n    response_time: float\n    verification_result: bool\n    raw_accuracy: float\n    hash: str\n"}
{"type": "source_file", "path": "neurons/_validator/core/request.py", "content": "from dataclasses import dataclass\n\nfrom execution_layer.circuit import Circuit\nfrom _validator.models.request_type import RequestType\nfrom protocol import QueryZkProof, ProofOfWeightsSynapse\nfrom execution_layer.generic_input import GenericInput\nimport bittensor as bt\n\n\n@dataclass\nclass Request:\n    \"\"\"\n    A request to be sent to a miner.\n    \"\"\"\n\n    uid: int\n    axon: bt.axon\n    synapse: QueryZkProof | ProofOfWeightsSynapse\n    circuit: Circuit\n    request_type: RequestType\n    inputs: GenericInput | None = None\n    request_hash: str | None = None\n    response_time: float | None = None\n    deserialized: dict[str, object] | None = None\n    result: bt.Synapse | None = None\n    save: bool = False\n"}
{"type": "source_file", "path": "neurons/_validator/models/request_type.py", "content": "from enum import Enum\n\n\nclass RequestType(Enum):\n    BENCHMARK = \"benchmark_request\"\n    RWR = \"real_world_request\"\n\n    def __str__(self) -> str:\n        if self == RequestType.BENCHMARK:\n            return \"Benchmark\"\n        elif self == RequestType.RWR:\n            return \"Real World Request\"\n        else:\n            raise ValueError(f\"Unknown request type: {self}\")\n\n\nclass ValidatorMessage(Enum):\n    WINDDOWN = \"winddown\"\n    WINDDOWN_COMPLETE = \"winddown_complete\"\n    COMPETITION_COMPLETE = \"competition_complete\"\n\n    def __str__(self) -> str:\n        return self.value\n"}
{"type": "source_file", "path": "neurons/_validator/utils/axon.py", "content": "import traceback\nimport bittensor as bt\nfrom aiohttp.client_exceptions import InvalidUrlClientError\nfrom _validator.core.request import Request\n\n\nasync def query_single_axon(dendrite: bt.dendrite, request: Request) -> Request | None:\n    \"\"\"\n    Query a single axon with a request. Per Circuit query.\n\n    Args:\n        dendrite (bt.dendrite): The dendrite to use for querying.\n        request (Request): The request to send.\n\n    Returns:\n        Request | None: The request with results populated, or None if the request failed.\n    \"\"\"\n\n    try:\n        result = await dendrite.call(\n            target_axon=request.axon,\n            synapse=request.synapse,\n            timeout=request.circuit.timeout,\n            deserialize=False,\n        )\n\n        if not result:\n            return None\n        request.result = result\n        request.response_time = (\n            result.dendrite.process_time\n            if result.dendrite.process_time is not None\n            else request.circuit.timeout\n        )\n\n        request.deserialized = result.deserialize()\n        return request\n\n    except InvalidUrlClientError:\n        bt.logging.warning(\n            f\"Ignoring UID as axon is not a valid URL: {request.uid}. {request.axon.ip}:{request.axon.port}\"\n        )\n        return None\n\n    except Exception as e:\n        bt.logging.warning(f\"Failed to query axon for UID: {request.uid}. Error: {e}\")\n        traceback.print_exc()\n        return None\n"}
{"type": "source_file", "path": "neurons/_validator/scoring/score_manager.py", "content": "from __future__ import annotations\nimport torch\nimport bittensor as bt\n\nfrom _validator.models.miner_response import MinerResponse\nfrom _validator.utils.logging import log_scores\nfrom _validator.utils.proof_of_weights import ProofOfWeightsItem\nfrom _validator.utils.uid import get_queryable_uids\nfrom constants import SINGLE_PROOF_OF_WEIGHTS_MODEL_ID, ONE_MINUTE\nfrom execution_layer.verified_model_session import VerifiedModelSession\nfrom deployment_layer.circuit_store import circuit_store\nfrom _validator.models.request_type import RequestType\nfrom execution_layer.circuit import CircuitEvaluationItem\nfrom utils.rate_limiter import with_rate_limit\nfrom _validator.competitions.competition import Competition\n\n\nclass ScoreManager:\n    \"\"\"Manages the scoring of miners.\"\"\"\n\n    def __init__(\n        self,\n        metagraph: bt.metagraph,\n        user_uid: int,\n        score_path: str,\n        competition: Competition | None = None,\n    ):\n        \"\"\"\n        Initialize the ScoreManager.\n\n        Args:\n            metagraph: The metagraph of the subnet.\n            user_uid: The UID of the current user.\n        \"\"\"\n        self.metagraph = metagraph\n        self.user_uid = user_uid\n        self.score_path = score_path\n        self.scores = self.init_scores()\n        self.last_processed_queue_step = -1\n        self.proof_of_weights_queue = []\n        self.competition = competition\n\n    def init_scores(self) -> torch.Tensor:\n        \"\"\"Initialize or load existing scores.\"\"\"\n        bt.logging.info(\"Initializing validation weights\")\n        try:\n            scores = torch.load(self.score_path, weights_only=True)\n        except FileNotFoundError:\n            scores = self._create_initial_scores()\n        except Exception as e:\n            bt.logging.error(f\"Error loading scores: {e}\")\n            scores = self._create_initial_scores()\n\n        bt.logging.success(\"Successfully initialized scores\")\n        log_scores(scores)\n        return scores\n\n    def _create_initial_scores(self) -> torch.Tensor:\n        \"\"\"Create initial scores based on metagraph data.\"\"\"\n        return torch.zeros(len(self.metagraph.uids), dtype=torch.float32)\n\n    def sync_scores_uids(self, uids: list[int]):\n        \"\"\"\n        If there are more uids than scores, add more weights.\n        \"\"\"\n        if len(uids) > len(self.scores):\n            bt.logging.trace(\n                f\"Scores length: {len(self.scores)}, UIDs length: {len(uids)}. Adding more weights\"\n            )\n            size_difference = len(uids) - len(self.scores)\n            new_scores = torch.zeros(size_difference, dtype=torch.float32)\n            queryable_uids = set(get_queryable_uids(self.metagraph))\n            new_scores = new_scores * torch.Tensor(\n                [\n                    uid in queryable_uids\n                    for uid in self.metagraph.uids[len(self.scores) :]\n                ]\n            )\n            self.scores = torch.cat((self.scores, new_scores))\n\n    def _create_pow_items(\n        self,\n        responses: list[MinerResponse],\n        max_score: float,\n        median_max_response_time: float,\n        min_response_time: float,\n        model_id: str,\n    ) -> list[ProofOfWeightsItem]:\n        \"\"\"Create ProofOfWeightsItems from responses.\"\"\"\n        return [\n            ProofOfWeightsItem.from_miner_response(\n                response,\n                max_score,\n                self.scores[response.uid],\n                median_max_response_time,\n                min_response_time,\n                self.metagraph.block.item(),\n                self.user_uid,\n            )\n            for response in responses\n        ]\n\n    @with_rate_limit(period=ONE_MINUTE)\n    def log_pow_queue_status(self):\n        bt.logging.info(f\"PoW Queue Status: {len(self.proof_of_weights_queue)} items\")\n\n    def _update_scores_from_witness(\n        self, proof_of_weights_items: list[ProofOfWeightsItem], model_id: str\n    ):\n        pow_circuit = circuit_store.get_circuit(model_id)\n        bt.logging.info(\n            f\"Processing PoW witness generation for {len(proof_of_weights_items)} items using {str(pow_circuit)}\"\n        )\n        if not pow_circuit:\n            raise ValueError(\n                f\"Proof of weights circuit not found for model ID: {model_id}\"\n            )\n\n        inputs = pow_circuit.input_handler(\n            RequestType.RWR, ProofOfWeightsItem.to_dict_list(proof_of_weights_items)\n        )\n        session = VerifiedModelSession(inputs, pow_circuit)\n        try:\n            witness = session.generate_witness(return_content=True)\n            bt.logging.success(\n                f\"Witness for {str(pow_circuit)} generated successfully.\"\n            )\n        except Exception as e:\n            bt.logging.error(f\"Error generating witness: {e}\")\n            return\n\n        witness_list = witness if isinstance(witness, list) else list(witness.values())\n        self._process_witness_results(witness_list, pow_circuit.settings[\"scaling\"])\n        session.end()\n\n    def _process_witness_results(self, witness: list, scaling: int):\n        \"\"\"Process the results from the witness.\"\"\"\n        scores = torch.div(\n            torch.tensor([float(w) for w in witness[1:257]]), scaling\n        ).tolist()\n        miner_uids = [int(float(w)) for w in witness[513:769]]\n\n        bt.logging.debug(\n            f\"Proof of weights scores: {scores} for miner UIDs: {miner_uids}, existing scores: {self.scores}\"\n        )\n\n        for uid, score in zip(miner_uids, scores):\n            if uid < 0 or uid >= len(self.scores):\n                continue\n            self.scores[uid] = float(score)\n            bt.logging.debug(f\"Updated score for UID {uid}: {score}\")\n\n        log_scores(self.scores)\n        self._try_store_scores()\n\n    def _update_pow_queue(self, new_items: list[ProofOfWeightsItem]):\n        if not new_items:\n            return\n\n        self.proof_of_weights_queue.extend(new_items)\n        self.log_pow_queue_status()\n\n    def process_pow_queue(self, model_id: str) -> bool:\n        \"\"\"Process items in the proof of weights queue for a specific model.\"\"\"\n        if (\n            len(self.proof_of_weights_queue) < 256\n            or len(self.proof_of_weights_queue) % 256 != 0\n        ):\n            return False\n\n        current_step = len(self.proof_of_weights_queue) >> 8\n        if current_step == self.last_processed_queue_step:\n            return False\n\n        pow_circuit = circuit_store.get_circuit(model_id)\n        if not pow_circuit:\n            bt.logging.error(f\"Circuit not found for model ID: {model_id}\")\n            return False\n\n        items_to_process = self.proof_of_weights_queue[-256:]\n        self._update_scores_from_witness(items_to_process, model_id)\n        self.last_processed_queue_step = current_step\n\n        return True\n\n    def _try_store_scores(self):\n        \"\"\"Attempt to store scores to disk.\"\"\"\n        try:\n            torch.save(self.scores, self.score_path)\n        except Exception as e:\n            bt.logging.error(f\"Error storing scores: {e}\")\n\n    def clear_proof_of_weights_queue(self):\n        \"\"\"Clear the proof of weights queue.\"\"\"\n        self.proof_of_weights_queue = []\n\n    @with_rate_limit(period=ONE_MINUTE * 5)\n    def process_non_queryable_scores(self, queryable_uids: set[int], max_score: float):\n        \"\"\"\n        Decay scores for non-queryable UIDs.\n        \"\"\"\n        for uid in range(len(self.scores)):\n            if uid not in queryable_uids:\n                hotkey = self.metagraph.hotkeys[uid]\n                if not (self.competition and hotkey in self.competition.miner_states):\n                    self.scores[uid] = 0\n                elif self.competition and hotkey in self.competition.miner_states:\n                    pow_item = ProofOfWeightsItem.for_competition(\n                        uid=uid,\n                        maximum_score=max_score,\n                        competition_score=self.competition.miner_states[\n                            hotkey\n                        ].sota_relative_score,\n                        block_number=self.metagraph.block.item(),\n                        validator_uid=self.user_uid,\n                    )\n                    self._update_pow_queue([pow_item])\n\n    def update_single_score(\n        self,\n        response: MinerResponse,\n        queryable_uids: set[int] | None = None,\n    ) -> None:\n        \"\"\"\n        Update the score for a single miner based on their response.\n\n        Args:\n            response (MinerResponse): The processed response from a miner.\n            queryable_uids: Optional pre-computed set of queryable UIDs.\n        \"\"\"\n        if queryable_uids is None:\n            queryable_uids = set(get_queryable_uids(self.metagraph))\n\n        circuit = response.circuit\n        hotkey = self.metagraph.hotkeys[response.uid]\n\n        competition_score = None\n        if self.competition and hotkey in self.competition.miner_states:\n            competition_score = self.competition.miner_states[\n                hotkey\n            ].sota_relative_score\n\n        if not response.verification_result and competition_score:\n            # If the miner is not responding to requests, but is in the competition, consider it verified\n            # Note that default values are set earlier up, therefore they receive a very poor response score.\n            response.verification_result = True\n\n        evaluation_data = CircuitEvaluationItem(\n            circuit=circuit,\n            uid=response.uid,\n            minimum_response_time=circuit.evaluation_data.minimum_response_time,\n            proof_size=response.proof_size,\n            response_time=response.response_time,\n            score=self.scores[response.uid],\n            verification_result=response.verification_result,\n        )\n        circuit.evaluation_data.update(evaluation_data)\n\n        max_score = 1 / len(self.scores)\n        self.process_non_queryable_scores(queryable_uids, max_score)\n\n        pow_item = ProofOfWeightsItem.from_miner_response(\n            response,\n            max_score,\n            self.scores[response.uid],\n            circuit.evaluation_data.maximum_response_time,\n            circuit.evaluation_data.minimum_response_time,\n            self.metagraph.block.item(),\n            self.user_uid,\n            competition_score if competition_score is not None else 0,\n        )\n\n        self._update_pow_queue([pow_item])\n\n        if (\n            len(self.proof_of_weights_queue) >= 256\n            and len(self.proof_of_weights_queue) % 256 == 0\n        ):\n            self.process_pow_queue(SINGLE_PROOF_OF_WEIGHTS_MODEL_ID)\n\n    def get_pow_queue(self) -> list[ProofOfWeightsItem]:\n        \"\"\"Get the current proof of weights queue.\"\"\"\n        return self.proof_of_weights_queue\n\n    def remove_processed_items(self, count: int):\n        if count <= 0:\n            return\n        self.proof_of_weights_queue = self.proof_of_weights_queue[count:]\n"}
{"type": "source_file", "path": "neurons/_validator/scoring/reward.py", "content": "\"\"\"\nThis module contains the reward function for the validator.\n\"\"\"\n\nimport torch\nfrom torch import nn\n\nRATE_OF_DECAY = 0.4\nRATE_OF_RECOVERY = 0.1\nFLATTENING_COEFFICIENT = 0.9\nPROOF_SIZE_THRESHOLD = 3648\nPROOF_SIZE_WEIGHT = 0\nRESPONSE_TIME_WEIGHT = 1\nMAXIMUM_RESPONSE_TIME_DECIMAL = 0.99\n\n\nclass Reward(nn.Module):\n    \"\"\"\n    This module is responsible for calculating the reward for a miner based on the provided score, verification_result,\n    response_time, and proof_size in it's forward pass.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.RATE_OF_DECAY = torch.tensor(RATE_OF_DECAY)\n        self.RATE_OF_RECOVERY = torch.tensor(RATE_OF_RECOVERY)\n        self.FLATTENING_COEFFICIENT = torch.tensor(FLATTENING_COEFFICIENT)\n        self.PROOF_SIZE_THRESHOLD = torch.tensor(PROOF_SIZE_THRESHOLD)\n        self.PROOF_SIZE_WEIGHT = torch.tensor(PROOF_SIZE_WEIGHT)\n        self.RESPONSE_TIME_WEIGHT = torch.tensor(RESPONSE_TIME_WEIGHT)\n        self.MAXIMUM_RESPONSE_TIME_DECIMAL = torch.tensor(MAXIMUM_RESPONSE_TIME_DECIMAL)\n\n    def shifted_tan(self, x: torch.FloatTensor) -> torch.FloatTensor:\n        \"\"\"\n        Shifted tangent curve\n        \"\"\"\n        return torch.tan(\n            torch.mul(\n                torch.mul(torch.sub(x, torch.tensor(0.5)), torch.pi),\n                self.FLATTENING_COEFFICIENT,\n            )\n        )\n\n    def tan_shift_difference(self, x: torch.FloatTensor) -> torch.FloatTensor:\n        \"\"\"\n        Difference\n        \"\"\"\n        return torch.sub(self.shifted_tan(x), self.shifted_tan(torch.tensor(0.0)))\n\n    def normalized_tangent_curve(self, x: torch.FloatTensor) -> torch.FloatTensor:\n        return torch.div(\n            self.tan_shift_difference(x), self.tan_shift_difference(torch.tensor(1.0))\n        )\n\n    def forward(\n        self,\n        maximum_score: torch.FloatTensor,\n        previous_score: torch.FloatTensor,\n        verified: torch.BoolTensor,\n        proof_size: torch.IntTensor,\n        response_time: torch.FloatTensor,\n        maximum_response_time: torch.FloatTensor,\n        minimum_response_time: torch.FloatTensor,\n        block_number: torch.IntTensor,\n        validator_uid: torch.IntTensor,\n        miner_uid: torch.IntTensor,\n    ):\n        \"\"\"\n        This method calculates the reward for a miner based on the provided score, verification_result,\n        response_time, and proof_size using a neural network module.\n        Positional Arguments:\n            max_score (FloatTensor): The maximum score for the miner.\n            score (FloatTensor): The current score for the miner.\n            verified (BoolTensor): Whether the response that the miner submitted was valid.\n            proof_size (FloatTensor): The size of the proof.\n            response_time (FloatTensor): The time taken to respond to the query.\n            maximum_response_time (FloatTensor): The maximum response time received from validator queries\n            minimum_response_time (FloatTensor): The minimum response time received from validator queries\n            validator_hotkey (FloatTensor[]): Ascii representation of the validator's hotkey\n            block_number (FloatTensor): The block number of the block that the response was submitted in.\n        Returns:\n            [new_score, validator_hotkey, block_number]\n        \"\"\"\n        # Determine rate of scoring change based on whether the response was verified\n        rate_of_change = torch.where(\n            verified, self.RATE_OF_RECOVERY, self.RATE_OF_DECAY\n        )\n\n        # Normalize the response time into a decimal between zero and the maximum response time decimal\n        # Maximum is capped at maximum response time decimal here to limit degree of score reduction\n        # in cases of very poor performance\n        response_time_normalized = torch.clamp(\n            torch.div(\n                torch.sub(response_time, minimum_response_time),\n                torch.sub(maximum_response_time, minimum_response_time),\n            ),\n            0,\n            self.MAXIMUM_RESPONSE_TIME_DECIMAL,\n        )\n\n        # Calculate reward metrics from both response time and proof size\n        response_time_reward_metric = torch.mul(\n            self.RESPONSE_TIME_WEIGHT,\n            torch.sub(\n                torch.tensor(1), self.normalized_tangent_curve(response_time_normalized)\n            ),\n        )\n        proof_size_reward_metric = torch.mul(\n            self.PROOF_SIZE_WEIGHT,\n            torch.clamp(\n                proof_size / self.PROOF_SIZE_THRESHOLD, torch.tensor(0), torch.tensor(1)\n            ),\n        )\n\n        # Combine reward metrics to provide a final score based on provided inputs\n        calculated_score_fraction = torch.clamp(\n            torch.sub(response_time_reward_metric, proof_size_reward_metric),\n            torch.tensor(0),\n            torch.tensor(1),\n        )\n\n        # Adjust the maximum score for the miner based on calculated metrics\n        maximum_score = torch.mul(maximum_score, calculated_score_fraction)\n\n        # Get the distance of the previous score from the new maximum or zero, depending on verification status\n        distance_from_score = torch.where(\n            verified, torch.sub(maximum_score, previous_score), previous_score\n        )\n\n        # Calculate the difference in scoring that will be applied based on the rate and distance from target score\n        change_in_score = torch.mul(rate_of_change, distance_from_score)\n\n        # Provide a new score based on their previous score and change in score. In cases where verified is false,\n        # scores are always decreased.\n        new_score = torch.where(\n            verified,\n            previous_score + change_in_score,\n            previous_score - change_in_score,\n        )\n\n        # Technically, new score is the only output that matters since we verify all inputs.\n        # These metadata fields are included to force torch.jit to leave them in when converting to ONNX\n        return [new_score, block_number, miner_uid, validator_uid]\n"}
{"type": "source_file", "path": "neurons/_validator/competitions/services/sota_manager.py", "content": "import os\nimport json\nimport time\nimport shutil\nimport bittensor as bt\nfrom ..models.sota import SotaState\nfrom ..models.neuron import NeuronState\nimport torch\n\n\nclass SotaManager:\n    def __init__(self, sota_directory: str):\n        self.sota_directory = sota_directory\n        self.sota_state_path = os.path.join(sota_directory, \"sota_state.json\")\n        self.sota_state = self._load_state()\n\n    def _load_state(self) -> SotaState:\n        try:\n            if os.path.exists(self.sota_state_path):\n                with open(self.sota_state_path, \"r\") as f:\n                    data = json.load(f)\n                    return SotaState(**data)\n            return SotaState()\n        except Exception as e:\n            bt.logging.error(f\"Error loading SOTA state: {e}\")\n            return SotaState()\n\n    def _save_state(self):\n        try:\n            with open(self.sota_state_path, \"w\") as f:\n                json.dump(self.sota_state.__dict__, f, indent=4)\n        except Exception as e:\n            bt.logging.error(f\"Error saving SOTA state: {e}\")\n\n    def preserve_circuit(\n        self,\n        circuit_dir: str,\n        neuron_state: NeuronState,\n        miner_states: dict[str, NeuronState],\n    ):\n        try:\n            sota_circuit_dir = os.path.join(self.sota_directory, \"circuit\")\n            if os.path.exists(sota_circuit_dir):\n                shutil.rmtree(sota_circuit_dir)\n            shutil.copytree(circuit_dir, sota_circuit_dir)\n\n            self.sota_state = SotaState(\n                sota_relative_score=neuron_state.sota_relative_score,\n                hash=neuron_state.hash,\n                hotkey=neuron_state.hotkey,\n                uid=neuron_state.uid,\n                proof_size=neuron_state.proof_size,\n                response_time=neuron_state.response_time,\n                timestamp=int(time.time()),\n                raw_accuracy=neuron_state.raw_accuracy,\n            )\n            self._save_state()\n\n            bt.logging.success(\n                f\"New SOTA achieved by {neuron_state.hotkey}! \"\n                f\"Score: {neuron_state.sota_relative_score:.4f}, \"\n                f\"Raw Accuracy: {neuron_state.raw_accuracy:.4f}, \"\n                f\"Proof Size: {neuron_state.proof_size:.2f} bytes, \"\n                f\"Response Time: {neuron_state.response_time:.4f}s\"\n            )\n\n            self.recalculate_miner_scores(miner_states)\n        except Exception as e:\n            bt.logging.error(f\"Error preserving SOTA circuit: {e}\")\n\n    def recalculate_miner_scores(self, miner_states: dict[str, NeuronState]) -> None:\n        \"\"\"Recalculate all miner scores relative to the new SOTA.\"\"\"\n        try:\n            with open(\n                os.path.join(\n                    os.path.dirname(self.sota_directory), \"competition_config.json\"\n                )\n            ) as f:\n                config = json.load(f)\n                weights = config[\"evaluation\"][\"scoring_weights\"]\n        except Exception as e:\n            bt.logging.error(\n                f\"Error loading scoring weights for recalculation, using defaults: {e}\"\n            )\n            weights = {\"accuracy\": 0.4, \"proof_size\": 0.3, \"response_time\": 0.3}\n\n        try:\n            for hotkey, state in miner_states.items():\n                if not state.verification_result or state.raw_accuracy == 0:\n                    state.sota_relative_score = 0.0\n                    continue\n\n                accuracy_diff = max(\n                    0, self.sota_state.raw_accuracy - state.raw_accuracy\n                )\n\n                if self.sota_state.proof_size > 0:\n                    proof_size_diff = max(\n                        0,\n                        (state.proof_size - self.sota_state.proof_size)\n                        / self.sota_state.proof_size,\n                    )\n                else:\n                    proof_size_diff = 0 if state.proof_size == 0 else 1\n\n                if self.sota_state.response_time > 0:\n                    response_time_diff = max(\n                        0,\n                        (state.response_time - self.sota_state.response_time)\n                        / self.sota_state.response_time,\n                    )\n                else:\n                    response_time_diff = 0 if state.response_time == 0 else 1\n\n                total_diff = torch.tensor(\n                    accuracy_diff * weights[\"accuracy\"]\n                    + proof_size_diff * weights[\"proof_size\"]\n                    + response_time_diff * weights[\"response_time\"]\n                )\n\n                state.sota_relative_score = torch.exp(-total_diff).item()\n        except Exception as e:\n            bt.logging.error(f\"Error recalculating miner scores: {e}\")\n\n    def check_if_sota(\n        self,\n        sota_relative_score: float,\n        proof_size: float,\n        response_time: float,\n        improvements: dict = None,\n    ) -> bool:\n        EPSILON = 1e-6\n        MAX_DEGRADATION = 0.1\n\n        if self.sota_state.sota_relative_score == 0:\n            return True\n\n        if sota_relative_score < self.sota_state.sota_relative_score - MAX_DEGRADATION:\n            return False\n\n        if improvements and \"raw\" in improvements and \"weighted\" in improvements:\n            if (\n                improvements[\"raw\"][\"accuracy\"] > MAX_DEGRADATION\n                or improvements[\"raw\"][\"proof_size\"] > MAX_DEGRADATION\n                or improvements[\"raw\"][\"response_time\"] > MAX_DEGRADATION\n            ):\n                return False\n\n            weighted_improvement = sum(-v for v in improvements[\"weighted\"].values())\n            return weighted_improvement > EPSILON\n\n        proof_size_degradation = (\n            proof_size - self.sota_state.proof_size\n        ) / self.sota_state.proof_size\n        response_time_degradation = (\n            response_time - self.sota_state.response_time\n        ) / self.sota_state.response_time\n\n        if (\n            proof_size_degradation > MAX_DEGRADATION\n            or response_time_degradation > MAX_DEGRADATION\n        ):\n            return False\n\n        return False\n\n    @property\n    def current_state(self) -> SotaState:\n        return self.sota_state\n"}
{"type": "source_file", "path": "neurons/_validator/pow/proof_of_weights_handler.py", "content": "from bittensor import logging\nfrom _validator.utils.proof_of_weights import ProofOfWeightsItem\nfrom execution_layer.circuit import Circuit, CircuitType\nfrom constants import (\n    BATCHED_PROOF_OF_WEIGHTS_MODEL_ID,\n)\nfrom protocol import ProofOfWeightsSynapse, QueryZkProof\nfrom _validator.models.request_type import RequestType\n\n\nclass ProofOfWeightsHandler:\n    \"\"\"\n    Handles internal proof of weights\n    This covers the case where the origin validator is a validator on Omron;\n    no external requests are needed as this internal mechanism is used to generate the proof of weights.\n    \"\"\"\n\n    @staticmethod\n    def prepare_pow_request(\n        circuit: Circuit, score_manager\n    ) -> ProofOfWeightsSynapse | QueryZkProof:\n        queue = score_manager.get_pow_queue()\n        batch_size = 1024\n\n        if circuit.id != BATCHED_PROOF_OF_WEIGHTS_MODEL_ID:\n            logging.debug(\"Not a batched PoW model. Defaulting to benchmark.\")\n            return None, False\n\n        if len(queue) < batch_size:\n            logging.debug(\n                f\"Queue is less than {batch_size} items. Defaulting to benchmark.\"\n            )\n            return None, False\n\n        pow_items = ProofOfWeightsItem.pad_items(\n            queue[:batch_size], target_item_count=batch_size\n        )\n\n        logging.info(f\"Preparing PoW request for {str(circuit)}\")\n        score_manager.remove_processed_items(batch_size)\n        return (\n            ProofOfWeightsHandler._create_request_from_items(circuit, pow_items),\n            True,\n        )\n\n    @staticmethod\n    def _create_request_from_items(\n        circuit: Circuit, pow_items: list[ProofOfWeightsItem]\n    ) -> ProofOfWeightsSynapse | QueryZkProof:\n        inputs = circuit.input_handler(\n            RequestType.RWR, ProofOfWeightsItem.to_dict_list(pow_items)\n        ).to_json()\n\n        if circuit.metadata.type == CircuitType.PROOF_OF_WEIGHTS:\n            return ProofOfWeightsSynapse(\n                subnet_uid=circuit.metadata.netuid,\n                verification_key_hash=circuit.id,\n                proof_system=circuit.proof_system,\n                inputs=inputs,\n                proof=\"\",\n                public_signals=\"\",\n            )\n        return QueryZkProof(\n            query_input={\"public_inputs\": inputs, \"model_id\": circuit.id},\n            query_output=\"\",\n        )\n"}
{"type": "source_file", "path": "neurons/_validator/core/response_processor.py", "content": "from __future__ import annotations\nimport traceback\nimport time\nimport bittensor as bt\nfrom _validator.core.request import Request\nfrom _validator.models.completed_proof_of_weights import CompletedProofOfWeightsItem\nfrom _validator.models.miner_response import MinerResponse\nfrom _validator.models.request_type import RequestType\nfrom _validator.scoring.score_manager import ScoreManager\nfrom execution_layer.generic_input import GenericInput\nfrom execution_layer.verified_model_session import VerifiedModelSession\n\n\nclass ResponseProcessor:\n    def __init__(\n        self,\n        metagraph,\n        score_manager: ScoreManager,\n        user_uid,\n        hotkey: substrateinterface.Keypair,\n    ):\n        self.metagraph = metagraph\n        self.score_manager = score_manager\n        self.user_uid = user_uid\n        self.hotkey = hotkey\n        self.proof_batches_queue = []\n        self.completed_proof_of_weights_queue: list[CompletedProofOfWeightsItem] = []\n\n    def process_single_response(self, response: Request) -> MinerResponse:\n        miner_response = MinerResponse.from_raw_response(response)\n        if miner_response.proof_content is None:\n            bt.logging.debug(\n                f\"Miner at UID: {miner_response.uid} failed to provide a valid proof for \"\n                f\"{str(miner_response.circuit)}.\"\n                f\"Response from miner: {miner_response.raw}\"\n            )\n        elif miner_response.proof_content:\n            bt.logging.debug(\n                f\"Attempting to verify proof for UID: {miner_response.uid} \"\n                f\"using {str(miner_response.circuit)}.\"\n            )\n            try:\n                start_time = time.time()\n                verification_result = self.verify_proof_string(\n                    miner_response, response.inputs\n                )\n                miner_response.verification_time = time.time() - start_time\n                miner_response.set_verification_result(verification_result)\n                if not verification_result:\n                    bt.logging.warning(\n                        f\"Miner at UID: {miner_response.uid} provided a proof\"\n                        f\" for {str(miner_response.circuit)}\"\n                        \", but verification failed.\"\n                    )\n            except Exception as e:\n                bt.logging.warning(\n                    f\"Unable to verify proof for UID: {miner_response.uid}. Error: {e}\"\n                )\n                traceback.print_exc()\n\n            if miner_response.verification_result:\n                bt.logging.success(\n                    f\"Miner at UID: {miner_response.uid} provided a valid proof \"\n                    f\"for {str(miner_response.circuit)} \"\n                    f\"in {miner_response.response_time} seconds.\"\n                )\n        return miner_response\n\n    def verify_proof_string(\n        self, response: MinerResponse, validator_inputs: GenericInput\n    ) -> bool:\n        if not response.proof_content or not response.public_json:\n            bt.logging.error(f\"Proof or public json not found for UID: {response.uid}\")\n            return False\n        try:\n            inference_session = VerifiedModelSession(\n                GenericInput(RequestType.RWR, response.public_json),\n                response.circuit,\n            )\n            res: bool = inference_session.verify_proof(\n                validator_inputs, response.proof_content\n            )\n            inference_session.end()\n            return res\n        except Exception as e:\n            raise e\n"}
{"type": "source_file", "path": "neurons/_validator/models/poc_rpc_request.py", "content": "from _validator.models.base_rpc_request import RealWorldRequest\nfrom pydantic import Field\nfrom deployment_layer.circuit_store import circuit_store\nfrom execution_layer.circuit import CircuitType\n\n\nclass ProofOfComputationRPCRequest(RealWorldRequest):\n    \"\"\"\n    Request for the Proof of Computation RPC method.\n    \"\"\"\n\n    circuit_id: str = Field(..., description=\"The ID of the circuit to use\")\n\n    class Config:\n        arbitrary_types_allowed = True\n        extra = \"allow\"\n\n    def __init__(self, **data):\n        circuit_id = data.get(\"circuit_id\")\n        if not circuit_id:\n            raise ValueError(\"circuit_id is required\")\n\n        circuit = circuit_store.get_circuit(circuit_id)\n        if circuit is None:\n            raise ValueError(f\"No circuit found for ID {circuit_id}\")\n\n        if circuit.metadata.type != CircuitType.PROOF_OF_COMPUTATION:\n            raise ValueError(\n                f\"Circuit {circuit_id} is not a proof of computation circuit\"\n            )\n\n        super().__init__(\n            circuit=circuit, inputs=data.get(\"inputs\"), circuit_id=circuit_id\n        )\n"}
{"type": "source_file", "path": "neurons/_validator/models/completed_proof_of_weights.py", "content": "from __future__ import annotations\nfrom dataclasses import dataclass\n\nfrom attrs import field\n\n\n@dataclass\nclass CompletedProofOfWeightsItem:\n    \"\"\"\n    A completed proof of weights item, to be logged to the chain.\n    \"\"\"\n\n    signals: list[str] | None = field(default=None)\n    proof: dict | str | None = field(default=None)\n    model_id: str | None = field(default=None)\n    netuid: int | None = field(default=None)\n\n    def __post_init__(self):\n        self.signals = self.signals\n        self.proof = self.proof\n\n    def to_remark(self) -> dict:\n        return {\n            \"type\": \"proof_of_weights\",\n            \"signals\": self.signals,\n            \"proof\": self.proof,\n            \"verification_key\": self.model_id,\n            \"netuid\": self.netuid,\n        }\n"}
{"type": "source_file", "path": "neurons/_validator/utils/api.py", "content": "import hashlib\nfrom execution_layer.generic_input import GenericInput\n\n\ndef hash_inputs(inputs: GenericInput | dict) -> str:\n    \"\"\"\n    Hashes inputs to proof of weights, excluding dynamic fields.\n\n    Args:\n        inputs (dict): The inputs to hash.\n\n    Returns:\n        str: The hashed inputs.\n    \"\"\"\n    if isinstance(inputs, GenericInput):\n        inputs = inputs.to_json()\n    filtered_inputs = {\n        k: v\n        for k, v in inputs.items()\n        if k not in [\"validator_uid\", \"nonce\", \"uid_responsible_for_proof\"]\n    }\n    return hashlib.sha256(str(filtered_inputs).encode()).hexdigest()\n"}
{"type": "source_file", "path": "neurons/_validator/utils/hash_guard.py", "content": "from execution_layer.base_input import BaseInput\nimport bittensor as bt\nimport json\nimport hashlib\nfrom collections import deque\n\n\nclass HashGuard:\n    \"\"\"\n    A safety checker to ensure input data is never repeated.\n    Uses SHA-256 for consistent hashing across sessions and sorted keys for deterministic JSON.\n    Uses a set for O(1) lookups and a deque for FIFO order.\n    \"\"\"\n\n    MAX_HASHES = 32768\n\n    def __init__(self):\n        self.hash_set = set()\n        self.hash_queue = deque(maxlen=self.MAX_HASHES)\n\n    def check_hash(self, input: BaseInput) -> None:\n\n        if isinstance(input, BaseInput):\n            input = input.to_json()\n\n        def sort_dict(d):\n            if isinstance(d, dict):\n                return {k: sort_dict(v) for k, v in sorted(d.items())}\n            if isinstance(d, list):\n                return [sort_dict(x) for x in d]\n            return d\n\n        sorted_input = sort_dict(input)\n        json_str = json.dumps(sorted_input, sort_keys=True)\n        hash_value = hashlib.sha256(json_str.encode()).hexdigest()\n\n        if hash_value in self.hash_set:\n            bt.logging.error(f\"Hash already exists: {hash_value}. Inputs: {input}\")\n            raise ValueError(\"Hash already exists\")\n\n        if len(self.hash_queue) == self.MAX_HASHES:\n            old_hash = self.hash_queue.popleft()\n            self.hash_set.remove(old_hash)\n\n        self.hash_set.add(hash_value)\n        self.hash_queue.append(hash_value)\n"}
{"type": "source_file", "path": "neurons/_validator/scoring/weights.py", "content": "from __future__ import annotations\nfrom dataclasses import dataclass, field\nimport torch\nimport bittensor as bt\nfrom constants import WEIGHT_RATE_LIMIT, WEIGHTS_VERSION, ONE_MINUTE\nfrom _validator.utils.logging import log_weights\nfrom _validator.utils.proof_of_weights import ProofOfWeightsItem\n\n\n@dataclass\nclass WeightsManager:\n    \"\"\"\n    Manages weight setting for the Omron validator.\n\n    Attributes:\n        subtensor (bt.subtensor): The Bittensor subtensor instance.\n        metagraph (bt.metagraph): The Bittensor metagraph instance.\n        wallet (bt.wallet): The Bittensor wallet instance.\n        user_uid (int): The unique identifier of the validator.\n        weights (Optional[torch.Tensor]): The current weights tensor.\n        last_update_weights_block (int): The last block number when weights were updated.\n        proof_of_weights_queue (List[ProofOfWeightsItem]): Queue for proof of weights items.\n    \"\"\"\n\n    subtensor: bt.subtensor\n    metagraph: bt.metagraph\n    wallet: bt.wallet\n    user_uid: int\n    last_update_weights_block: int = 0\n    proof_of_weights_queue: list[ProofOfWeightsItem] = field(default_factory=list)\n\n    def set_weights(self, netuid, wallet, uids, weights, version_key):\n        return self.subtensor.set_weights(\n            netuid=netuid,\n            wallet=wallet,\n            uids=uids,\n            weights=weights,\n            wait_for_inclusion=True,\n            version_key=version_key,\n        )\n\n    def should_update_weights(self) -> tuple[bool, str]:\n        \"\"\"Check if weights should be updated based on rate limiting.\"\"\"\n        blocks_since_last_update = self.subtensor.blocks_since_last_update(\n            self.metagraph.netuid, self.user_uid\n        )\n        if blocks_since_last_update < WEIGHT_RATE_LIMIT:\n            blocks_until_update = WEIGHT_RATE_LIMIT - blocks_since_last_update\n            minutes_until_update = round((blocks_until_update * 12) / ONE_MINUTE, 1)\n            return (\n                False,\n                f\"Next weight update in {blocks_until_update} blocks \"\n                f\"(approximately {minutes_until_update:.1f} minutes)\",\n            )\n        return True, \"\"\n\n    def update_weights(self, scores: torch.Tensor) -> bool:\n        \"\"\"Updates the weights based on the given scores and sets them on the chain.\"\"\"\n        should_update, message = self.should_update_weights()\n        if not should_update:\n            bt.logging.info(message)\n            return True\n\n        bt.logging.info(\"Updating weights\")\n        weights = torch.zeros(self.metagraph.n)\n        nonzero_indices = scores.nonzero()\n        bt.logging.debug(\n            f\"Weights: {weights}, Nonzero indices: {nonzero_indices}, Scores: {scores}\"\n        )\n        if nonzero_indices.sum() > 0:\n            weights[nonzero_indices] = scores[nonzero_indices]\n\n        try:\n            success, message = self.set_weights(\n                netuid=self.metagraph.netuid,\n                wallet=self.wallet,\n                uids=self.metagraph.uids.tolist(),\n                weights=weights.tolist(),\n                version_key=WEIGHTS_VERSION,\n            )\n\n            if message:\n                bt.logging.info(f\"Set weights message: {message}\")\n\n            if success:\n                bt.logging.success(\"Weights were set successfully\")\n                log_weights(weights)\n                self.last_update_weights_block = int(self.metagraph.block.item())\n                return True\n            return False\n\n        except Exception as e:\n            bt.logging.error(f\"Failed to set weights on chain with exception: {e}\")\n            return False\n"}
{"type": "source_file", "path": "neurons/_validator/utils/pps.py", "content": "import time\nimport bittensor as bt\nimport requests\nfrom substrateinterface import Keypair\n\n\nclass ProofPublishingService:\n    def __init__(self, url: str):\n        self.url = url\n\n    def publish_proof(self, proof_json: dict, hotkey: Keypair):\n        \"\"\"\n        Publishes a proof to the proof publishing service.\n\n        Args:\n            proof_json (dict): The proof data as a JSON object\n            hotkey (Keypair): The hotkey used to sign the proof\n        \"\"\"\n        try:\n            timestamp = str(int(time.time()))\n            message = timestamp.encode(\"utf-8\")\n            signature = hotkey.sign(message)\n\n            response = requests.post(\n                f\"{self.url}/proof\",\n                json={\"proof\": proof_json},\n                headers={\n                    \"x-timestamp\": timestamp,\n                    \"x-origin-ss58\": hotkey.ss58_address,\n                    \"x-signature\": signature.hex(),\n                    \"Content-Type\": \"application/json\",\n                },\n            )\n\n            if response.status_code == 200:\n                response_json = response.json()\n                bt.logging.success(f\"Proof of weights uploaded to {self.url}\")\n                bt.logging.info(f\"Response: {response_json}\")\n                return response_json\n            else:\n                bt.logging.warning(\n                    f\"Failed to upload proof of weights to {self.url}. Status code: {response.status_code}\"\n                )\n                return None\n        except Exception as e:\n            bt.logging.warning(f\"Error uploading proof of weights: {e}\")\n            return None\n"}
{"type": "source_file", "path": "neurons/_validator/utils/uid.py", "content": "from collections.abc import Generator, Iterable\nimport bittensor as bt\nimport torch\nimport ipaddress\n\nfrom constants import VALIDATOR_STAKE_THRESHOLD, MAINNET_TESTNET_UIDS, DEFAULT_NETUID\n\n\ndef is_valid_ip(ip: str) -> bool:\n    try:\n        address = ipaddress.IPv4Address(ip)\n        return address.is_global and not address.is_multicast\n    except ValueError:\n        return False\n\n\ndef get_queryable_uids(metagraph: bt.metagraph) -> Generator[int, None, None]:\n    \"\"\"\n    Returns the uids of the miners that are queryable\n    \"\"\"\n    uids = metagraph.uids.tolist()\n    stake_threshold = VALIDATOR_STAKE_THRESHOLD\n    if metagraph.netuid in [\n        i[1] for i in MAINNET_TESTNET_UIDS if i[0] == DEFAULT_NETUID\n    ]:\n        stake_threshold = 1e19\n    total_stake = (\n        torch.tensor(metagraph.total_stake, dtype=torch.float32)\n        if not isinstance(metagraph.total_stake, torch.Tensor)\n        else metagraph.total_stake\n    )\n    total_stake = total_stake[uids]\n    queryable_flags: Iterable[bool] = (\n        (total_stake < stake_threshold)\n        & torch.tensor([is_valid_ip(metagraph.axons[i].ip) for i in uids])\n    ).tolist()\n    for uid, is_queryable in zip(uids, queryable_flags):\n        if is_queryable:\n            yield uid\n"}
{"type": "source_file", "path": "neurons/_validator/utils/proof_of_weights.py", "content": "import json\nimport os\nimport traceback\nfrom dataclasses import dataclass\nimport bittensor as bt\nimport torch\nimport time\nfrom typing import Optional\nfrom _validator.utils.pps import ProofPublishingService\n\nfrom substrateinterface import ExtrinsicReceipt, Keypair\nfrom _validator.models.miner_response import (\n    MinerResponse,\n)\nfrom constants import (\n    DEFAULT_MAX_SCORE,\n    DEFAULT_PROOF_SIZE,\n    VALIDATOR_REQUEST_TIMEOUT_SECONDS,\n    PPS_URL,\n    TESTNET_PPS_URL,\n)\n\n# Constants\n\nPOW_DIRECTORY = os.path.join(\n    os.path.dirname(os.path.realpath(__file__)), \"..\", \"proof_of_weights\"\n)\nif not os.path.exists(POW_DIRECTORY):\n    os.makedirs(POW_DIRECTORY)\n\n\nPOW_RECEIPT_DIRECTORY = os.path.join(POW_DIRECTORY, \"receipts\")\nif not os.path.exists(POW_RECEIPT_DIRECTORY):\n    os.makedirs(POW_RECEIPT_DIRECTORY)\n\n\ndef to_tensor(value, dtype):\n    if isinstance(value, torch.Tensor):\n        return value.clone().detach().to(dtype)\n    return torch.tensor(value, dtype=dtype)\n\n\ndummy_miner_response = MinerResponse.empty()\n\n\n@dataclass\nclass ProofOfWeightsItem:\n    maximum_score: torch.Tensor\n    previous_score: torch.Tensor\n    verified: torch.Tensor\n    proof_size: torch.Tensor\n    response_time: torch.Tensor\n    competition: torch.Tensor\n    maximum_response_time: torch.Tensor\n    minimum_response_time: torch.Tensor\n    block_number: torch.Tensor\n    validator_uid: torch.Tensor\n    miner_uid: torch.Tensor\n\n    def __post_init__(self):\n        self.maximum_score = to_tensor(self.maximum_score, torch.float32)\n        self.previous_score = to_tensor(self.previous_score, torch.float32)\n        self.verified = to_tensor(self.verified, torch.bool)\n        self.proof_size = to_tensor(self.proof_size, torch.int64)\n        self.response_time = to_tensor(self.response_time, torch.float32)\n        self.maximum_response_time = to_tensor(\n            self.maximum_response_time, torch.float32\n        )\n        self.competition = to_tensor(self.competition, torch.float32)\n        self.minimum_response_time = to_tensor(\n            self.minimum_response_time, torch.float32\n        )\n        self.block_number = to_tensor(self.block_number, torch.int64)\n        self.validator_uid = to_tensor(self.validator_uid, torch.int64)\n        self.miner_uid = to_tensor(self.miner_uid, torch.int64)\n\n    @staticmethod\n    def for_competition(\n        uid: int,\n        maximum_score: float,\n        competition_score: float,\n        block_number: int,\n        validator_uid: int,\n    ):\n        return ProofOfWeightsItem(\n            maximum_score=maximum_score,\n            previous_score=0,\n            verified=torch.tensor(True),\n            proof_size=torch.tensor(1),\n            response_time=torch.tensor(1),\n            competition=torch.tensor(competition_score),\n            maximum_response_time=torch.tensor(1),\n            minimum_response_time=torch.tensor(0),\n            block_number=torch.tensor(block_number),\n            validator_uid=torch.tensor(validator_uid),\n            miner_uid=torch.tensor(uid, dtype=torch.int64),\n        )\n\n    @staticmethod\n    def from_miner_response(\n        response: MinerResponse,\n        maximum_score,\n        previous_score,\n        maximum_response_time,\n        minimum_response_time,\n        block_number,\n        validator_uid,\n        competition,\n    ):\n        return ProofOfWeightsItem(\n            maximum_score=maximum_score,\n            previous_score=previous_score,\n            verified=torch.tensor(response.verification_result, dtype=torch.bool),\n            proof_size=torch.tensor(response.proof_size, dtype=torch.int64),\n            response_time=(\n                torch.tensor(response.response_time, dtype=torch.float32)\n                if response.verification_result\n                else maximum_response_time\n            ),\n            competition=competition,\n            maximum_response_time=maximum_response_time,\n            minimum_response_time=minimum_response_time,\n            block_number=block_number,\n            validator_uid=validator_uid,\n            miner_uid=torch.tensor(response.uid, dtype=torch.int64),\n        )\n\n    @staticmethod\n    def pad_items(\n        items: list[\"ProofOfWeightsItem\"], target_item_count: int = 256\n    ) -> list[\"ProofOfWeightsItem\"]:\n        if len(items) == 0:\n            return [ProofOfWeightsItem.empty()] * target_item_count\n\n        # Pad or truncate the input list to exactly target_item_count\n        if len(items) < target_item_count:\n            items.extend(\n                [ProofOfWeightsItem.empty()] * (target_item_count - len(items))\n            )\n        elif len(items) > target_item_count:\n            items = items[-target_item_count:]\n\n        return items\n\n    @staticmethod\n    def empty():\n        return ProofOfWeightsItem(\n            maximum_score=torch.tensor(DEFAULT_MAX_SCORE, dtype=torch.float32),\n            previous_score=torch.tensor(0, dtype=torch.float32),\n            verified=torch.tensor(0, dtype=torch.int64),\n            proof_size=torch.tensor(DEFAULT_PROOF_SIZE, dtype=torch.int64),\n            response_time=torch.tensor(\n                VALIDATOR_REQUEST_TIMEOUT_SECONDS, dtype=torch.float32\n            ),\n            competition=torch.tensor(0, dtype=torch.float32),\n            maximum_response_time=torch.tensor(\n                VALIDATOR_REQUEST_TIMEOUT_SECONDS, dtype=torch.float32\n            ),\n            minimum_response_time=torch.tensor(0, dtype=torch.float32),\n            block_number=torch.tensor(0, dtype=torch.int64),\n            validator_uid=torch.tensor(0, dtype=torch.int64),\n            miner_uid=torch.tensor(-1, dtype=torch.int64),\n        )\n\n    @staticmethod\n    def to_dict_list(items: list[\"ProofOfWeightsItem\"]):\n        result = {\n            \"maximum_score\": [],\n            \"previous_score\": [],\n            \"verified\": [],\n            \"proof_size\": [],\n            \"response_time\": [],\n            \"competition\": [],\n            \"maximum_response_time\": [],\n            \"minimum_response_time\": [],\n            \"block_number\": [],\n            \"validator_uid\": [],\n            \"miner_uid\": [],\n        }\n        for item in items:\n            result[\"maximum_score\"].append(item.maximum_score.item())\n            result[\"previous_score\"].append(item.previous_score.item())\n            result[\"verified\"].append(item.verified.item())\n            result[\"proof_size\"].append(item.proof_size.item())\n            result[\"response_time\"].append(item.response_time.item())\n            result[\"competition\"].append(item.competition.item())\n            result[\"maximum_response_time\"].append(item.maximum_response_time.item())\n            result[\"minimum_response_time\"].append(item.minimum_response_time.item())\n            result[\"block_number\"].append(item.block_number.item())\n            result[\"validator_uid\"].append(item.validator_uid.item())\n            result[\"miner_uid\"].append(item.miner_uid.item())\n        return result\n\n\ndef save_proof_of_weights(\n    public_signals: list,\n    proof: str,\n    metadata: dict,\n    hotkey: Keypair,\n    is_testnet: bool = False,\n    proof_filename: Optional[str] = None,\n):\n    \"\"\"\n    Save the proof of weights to a JSON file.\n\n    Args:\n        public_signals (list): The public signals data as a JSON array.\n        proof (str): The proof.\n        metadata (dict): Additional metadata for the proof.\n        hotkey (Keypair): The hotkey used to sign the proof.\n        proof_filename (Optional[str]): Custom filename for the proof file.\n    \"\"\"\n    try:\n        if proof_filename is None:\n            proof_filename = str(int(time.time()))\n\n        file_path = os.path.join(POW_DIRECTORY, f\"{proof_filename}.json\")\n\n        proof_json = {\n            \"public_signals\": public_signals,\n            \"proof\": proof,\n            \"metadata\": metadata,\n        }\n\n        pps = ProofPublishingService(PPS_URL if not is_testnet else TESTNET_PPS_URL)\n        response = pps.publish_proof(proof_json, hotkey)\n\n        if response is None:\n            bt.logging.error(\"Failed to publish proof of weights, saving to disk.\")\n            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(proof_json, f)\n            return\n        else:\n            bt.logging.success(f\"Proof of weights receipt saved to {file_path}\")\n            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(response, f)\n    except Exception as e:\n        bt.logging.error(f\"Error saving proof of weights to file: {e}\")\n        traceback.print_exc()\n\n\ndef save_receipt(receipt: ExtrinsicReceipt):\n    with open(\n        os.path.join(POW_RECEIPT_DIRECTORY, f\"{receipt.extrinsic_hash}.json\"),\n        \"w\",\n        encoding=\"utf-8\",\n    ) as f:\n        json.dump({k: str(v) for k, v in receipt.__dict__.items()}, f, indent=4)\n"}
{"type": "source_file", "path": "neurons/_validator/core/request_pipeline.py", "content": "from __future__ import annotations\n\nimport copy\nimport traceback\nimport random\n\nimport bittensor as bt\n\nfrom _validator.api import ValidatorAPI\nfrom _validator.config import ValidatorConfig\nfrom _validator.core.request import Request\nfrom _validator.models.request_type import RequestType\nfrom _validator.pow.proof_of_weights_handler import ProofOfWeightsHandler\nfrom _validator.scoring.score_manager import ScoreManager\nfrom _validator.utils.hash_guard import HashGuard\nfrom constants import (\n    BATCHED_PROOF_OF_WEIGHTS_MODEL_ID,\n    SINGLE_PROOF_OF_WEIGHTS_MODEL_ID,\n)\nfrom deployment_layer.circuit_store import circuit_store\nfrom execution_layer.circuit import Circuit, CircuitType\nfrom execution_layer.generic_input import GenericInput\nfrom protocol import ProofOfWeightsSynapse, QueryZkProof\nfrom utils.wandb_logger import safe_log\nfrom execution_layer.base_input import BaseInput\n\n\nclass RequestPipeline:\n    def __init__(\n        self, config: ValidatorConfig, score_manager: ScoreManager, api: ValidatorAPI\n    ):\n        self.config = config\n        self.score_manager = score_manager\n        self.api = api\n        self.hash_guard = HashGuard()\n\n    def prepare_requests(self, filtered_uids) -> list[Request]:\n        \"\"\"\n        Prepare requests for the current validation step.\n        This includes both regular benchmark requests and any external requests.\n\n        Args:\n            filtered_uids (list): List of UIDs to send requests to.\n\n        Returns:\n            list[Request]: List of prepared requests.\n        \"\"\"\n        if len(filtered_uids) == 0:\n            bt.logging.error(\"No UIDs to query\")\n            return []\n\n        if self.api.external_requests_queue:\n            return self._prepare_real_world_requests(filtered_uids)\n        return self._prepare_benchmark_requests(filtered_uids)\n\n    def _check_and_create_request(\n        self,\n        uid: int,\n        synapse: ProofOfWeightsSynapse | QueryZkProof,\n        circuit: Circuit,\n        request_type: RequestType,\n        request_hash: str | None = None,\n        save: bool = False,\n    ) -> Request | None:\n        \"\"\"Check hash and create request if valid.\"\"\"\n        try:\n            if isinstance(synapse, ProofOfWeightsSynapse):\n                input_data = synapse.inputs\n            else:\n                input_data = synapse.query_input[\"public_inputs\"]\n\n            self.hash_guard.check_hash(input_data)\n        except Exception as e:\n            bt.logging.error(f\"Hash already exists: {e}\")\n            safe_log({\"hash_guard_error\": 1})\n            if request_type == RequestType.RWR:\n                self.api.set_request_result(\n                    request_hash, {\"success\": False, \"error\": \"Hash already exists\"}\n                )\n            return None\n\n        return Request(\n            uid=uid,\n            axon=self.config.metagraph.axons[uid],\n            synapse=synapse,\n            circuit=circuit,\n            request_type=request_type,\n            inputs=GenericInput(RequestType.RWR, input_data),\n            request_hash=request_hash,\n            save=save,\n        )\n\n    def _prepare_real_world_requests(self, filtered_uids: list[int]) -> list[Request]:\n        external_request = self.api.external_requests_queue.pop()\n        requests = []\n\n        for uid in filtered_uids:\n            try:\n                synapse, save = self.get_synapse_request(\n                    RequestType.RWR, external_request.circuit, external_request\n                )\n                request = self._check_and_create_request(\n                    uid=uid,\n                    synapse=synapse,\n                    circuit=external_request.circuit,\n                    request_type=RequestType.RWR,\n                    request_hash=external_request.hash,\n                    save=save,\n                )\n                if request:\n                    requests.append(request)\n            except Exception as e:\n                bt.logging.error(f\"Error preparing request for UID {uid}: {e}\")\n                traceback.print_exc()\n                self.api.set_request_result(\n                    external_request.hash,\n                    {\"success\": False, \"error\": \"Error preparing request\"},\n                )\n                continue\n        return requests\n\n    def _prepare_benchmark_requests(self, filtered_uids: list[int]) -> list[Request]:\n        circuit = self.select_circuit_for_benchmark()\n        if circuit is None:\n            bt.logging.error(\"No circuit selected\")\n            return []\n\n        requests = []\n        for uid in filtered_uids:\n            synapse, save = self.get_synapse_request(RequestType.BENCHMARK, circuit)\n            request = self._check_and_create_request(\n                uid=uid,\n                synapse=synapse,\n                circuit=circuit,\n                request_type=RequestType.BENCHMARK,\n                save=save,\n            )\n            if request:\n                requests.append(request)\n        return requests\n\n    def select_circuit_for_benchmark(self) -> Circuit:\n        \"\"\"\n        Select a circuit for benchmarking using weighted random selection.\n        \"\"\"\n        circuits = list(circuit_store.circuits.values())\n\n        return random.choices(\n            circuits,\n            weights=[\n                (circuit.metadata.benchmark_choice_weight or 0) for circuit in circuits\n            ],\n            k=1,\n        )[0]\n\n    def format_for_query(\n        self, inputs: dict[str, object] | BaseInput, circuit: Circuit\n    ) -> dict[str, object]:\n        if hasattr(inputs, \"to_json\"):\n            inputs = inputs.to_json()\n        return {\"public_inputs\": inputs, \"model_id\": circuit.id}\n\n    def get_synapse_request(\n        self,\n        request_type: RequestType,\n        circuit: Circuit,\n        request: any | None = None,\n    ) -> tuple[ProofOfWeightsSynapse | QueryZkProof, bool]:\n        inputs = (\n            circuit.input_handler(request_type)\n            if request_type == RequestType.BENCHMARK\n            else circuit.input_handler(\n                RequestType.RWR,\n                copy.deepcopy(request.inputs),\n            )\n        )\n\n        if request_type == RequestType.RWR:\n            if circuit.metadata.type == CircuitType.PROOF_OF_WEIGHTS:\n                return (\n                    ProofOfWeightsSynapse(\n                        subnet_uid=circuit.metadata.netuid,\n                        verification_key_hash=circuit.id,\n                        proof_system=circuit.proof_system,\n                        inputs=inputs.to_json(),\n                        proof=\"\",\n                        public_signals=\"\",\n                    ),\n                    True,\n                )\n            return (\n                QueryZkProof(\n                    model_id=circuit.id,\n                    query_input=self.format_for_query(inputs, circuit),\n                    query_output=\"\",\n                ),\n                True,\n            )\n\n        if circuit.id in [\n            SINGLE_PROOF_OF_WEIGHTS_MODEL_ID,\n            BATCHED_PROOF_OF_WEIGHTS_MODEL_ID,\n        ]:\n            synapse_request, save = ProofOfWeightsHandler.prepare_pow_request(\n                circuit, self.score_manager\n            )\n            if synapse_request:\n                return synapse_request, save\n\n        if circuit.metadata.type == CircuitType.PROOF_OF_COMPUTATION:\n            return (\n                QueryZkProof(\n                    model_id=circuit.id,\n                    query_input=self.format_for_query(inputs, circuit),\n                    query_output=\"\",\n                ),\n                False,\n            )\n\n        return (\n            ProofOfWeightsSynapse(\n                subnet_uid=circuit.metadata.netuid,\n                verification_key_hash=circuit.id,\n                proof_system=circuit.proof_system,\n                inputs=inputs.to_json(),\n                proof=\"\",\n                public_signals=\"\",\n            ),\n            False,\n        )\n\n    def prepare_single_request(self, uid: int) -> Request | None:\n        \"\"\"\n        Prepare a single request for a specific UID.\n\n        Args:\n            uid (int): The UID to prepare a request for.\n\n        Returns:\n            Request | None: The prepared request, or None if preparation failed.\n        \"\"\"\n        if self.api.external_requests_queue:\n            requests = self._prepare_real_world_requests([uid])\n        else:\n            requests = self._prepare_benchmark_requests([uid])\n\n        return requests[0] if requests else None\n"}
{"type": "source_file", "path": "neurons/_validator/validator_session.py", "content": "from __future__ import annotations\n\nimport sys\n\nimport bittensor as bt\n\nimport cli_parser\nfrom _validator.config import ValidatorConfig\nfrom _validator.core.validator_loop import ValidatorLoop\nfrom utils import clean_temp_files\nimport asyncio\n\n\nclass ValidatorSession:\n    def __init__(self):\n        self.config = ValidatorConfig(cli_parser.config)\n        self.validator_loop = ValidatorLoop(self.config)\n\n    def run(self):\n        \"\"\"\n        Start the validator session and run the main loop\n        \"\"\"\n        bt.logging.debug(\"Validator session started\")\n\n        try:\n            asyncio.run(self.validator_loop.run())\n        except KeyboardInterrupt:\n            bt.logging.info(\"KeyboardInterrupt caught. Exiting validator.\")\n            clean_temp_files()\n            sys.exit(0)\n"}
{"type": "source_file", "path": "neurons/_validator/utils/logging.py", "content": "from __future__ import annotations\n\nimport torch\nfrom rich.console import Console, JustifyMethod\nfrom rich.table import Table\n\nfrom utils import wandb_logger\nfrom _validator.models.miner_response import MinerResponse\n\n\ndef create_and_print_table(\n    title: str, columns: list[tuple[str, JustifyMethod, str]], rows: list[list[str]]\n):\n    \"\"\"\n    Create and print a table.\n\n    Args:\n        title (str): The title of the table.\n        columns (list[tuple[str, JustifyMethod, str]]): A list of tuples containing column information.\n            Each tuple should contain (column_name, justification, style).\n        rows (list[list[str]]): A list of rows, where each row is a list of string values.\n\n    \"\"\"\n    table = Table(title=title)\n    for col_name, justify, style in columns:\n        table.add_column(col_name, justify=justify, style=style, no_wrap=True)\n    for row in rows:\n        table.add_row(*row)\n    console = Console(color_system=\"truecolor\")\n    console.width = 120\n    console.print(table)\n\n\ndef log_tensor_data(title: str, data: torch.Tensor, log_key: str):\n    \"\"\"\n    Log tensor data to a table and Weights & Biases.\n\n    Args:\n        title (str): The title of the table.\n        data (torch.Tensor): The tensor data to be logged.\n        log_key (str): The key used for logging in Weights & Biases.\n    \"\"\"\n    rows = [[str(uid), f\"{value.item():.6f}\"] for uid, value in enumerate(data)]\n    create_and_print_table(\n        title, [(\"uid\", \"right\", \"cyan\"), (log_key, \"right\", \"yellow\")], rows\n    )\n    wandb_logger.safe_log(\n        {log_key: {uid: value.item() for uid, value in enumerate(data)}}\n    )\n\n\ndef log_scores(scores: torch.Tensor):\n    \"\"\"\n    Log scores to a table and Weights & Biases.\n\n    Args:\n        scores (torch.Tensor): The scores tensor to be logged.\n\n    \"\"\"\n    log_tensor_data(\"scores\", scores, \"scores\")\n\n\ndef log_weights(weights: torch.Tensor):\n    \"\"\"\n    Log weights to a table and Weights & Biases.\n\n    Args:\n        weights (torch.Tensor): The weights tensor to be logged.\n    \"\"\"\n    log_tensor_data(\"weights\", weights, \"weights\")\n\n\ndef log_verify_result(results: list[tuple[int, bool]]):\n    \"\"\"\n    Log verification results to a table and Weights & Biases.\n\n    Args:\n        results (list[tuple[int, bool]]): A list of tuples containing (uid, verification_result).\n\n    \"\"\"\n    rows = [[str(uid), str(result)] for uid, result in results]\n    create_and_print_table(\n        \"proof verification result\",\n        [(\"uid\", \"right\", \"cyan\"), (\"Verified?\", \"right\", \"green\")],\n        rows,\n    )\n    wandb_logger.safe_log(\n        {\"verification_results\": {uid: int(result) for uid, result in results}}\n    )\n\n\ndef log_responses(responses: list[MinerResponse]):\n    \"\"\"\n    Log miner responses to a table and Weights & Biases.\n\n    Args:\n        responses (list[MinerResponse]): A list of MinerResponse objects to be logged.\n    \"\"\"\n    columns = [\n        (\"UID\", \"right\", \"cyan\"),\n        (\"Verification Result\", \"right\", \"green\"),\n        (\"Response Time\", \"right\", \"yellow\"),\n        (\"Proof Size\", \"right\", \"blue\"),\n        (\"Circuit Name\", \"left\", \"magenta\"),\n        (\"Proof System\", \"left\", \"red\"),\n    ]\n\n    sorted_responses = sorted(responses, key=lambda x: x.uid)\n    rows = [\n        [\n            str(response.uid),\n            str(response.verification_result),\n            str(response.response_time),\n            str(response.proof_size),\n            (response.circuit.metadata.name if response.circuit else \"Unknown\"),\n            (response.circuit.metadata.proof_system if response.circuit else \"Unknown\"),\n        ]\n        for response in sorted_responses\n    ]\n    create_and_print_table(\"Responses\", columns, rows)\n\n    wandb_log = {\n        \"responses\": {\n            response.uid: {\n                str(response.circuit): {\n                    \"verification_result\": int(response.verification_result),\n                    \"response_time\": response.response_time,\n                    \"proof_size\": response.proof_size,\n                }\n            }\n            for response in sorted_responses\n            if response.verification_result\n        }\n    }\n    wandb_logger.safe_log(wandb_log)\n"}
{"type": "source_file", "path": "neurons/cli_parser.py", "content": "import argparse\nimport os\nimport sys\nfrom typing import Optional\n\nfrom constants import (\n    ONCHAIN_PROOF_OF_WEIGHTS_ENABLED,\n    PROOF_OF_WEIGHTS_INTERVAL,\n    TEMP_FOLDER,\n    Roles,\n    COMPETITION_SYNC_INTERVAL,\n)\n\nSHOW_HELP = False\n\n# Intercept --help/-h flags before importing bittensor since it overrides help behavior\n# This allows showing our custom help message instead of bittensor's default one\nif \"--help\" in sys.argv:\n    SHOW_HELP = True\n    sys.argv.remove(\"--help\")\nelif \"-h\" in sys.argv:\n    SHOW_HELP = True\n    sys.argv.remove(\"-h\")\n\n# flake8: noqa\nimport bittensor as bt\n\nparser: Optional[argparse.ArgumentParser]\nconfig: Optional[bt.config]\n\n\nDESCRIPTION = {\n    Roles.MINER: \"Omron Miner. Starts a Bittensor node that mines on the Omron subnet.\",\n    Roles.VALIDATOR: \"Omron Validator. Starts a Bittensor node that validates on the Omron subnet.\",\n}\n\n\ndef init_config(role: Optional[str] = None):\n    \"\"\"\n    Initialize the configuration for the node.\n    Config is based on CLI arguments, some of which are common to both miner and validator,\n    and some of which are specific to each.\n    The configuration itself is stored in the global variable `config`. Kinda singleton pattern.\n    \"\"\"\n    from utils import wandb_logger\n\n    global parser\n    global config\n\n    if not os.path.exists(TEMP_FOLDER):\n        os.makedirs(TEMP_FOLDER)\n\n    parser = argparse.ArgumentParser(\n        description=DESCRIPTION.get(role, \"\"),\n        epilog=\"For more information, visit https://omron.ai/\",\n        allow_abbrev=False,\n    )\n\n    # init common CLI arguments for both miner and validator:\n    parser.add_argument(\"--netuid\", type=int, default=1, help=\"The UID of the subnet.\")\n    parser.add_argument(\n        \"--no-auto-update\",\n        default=bool(os.getenv(\"OMRON_NO_AUTO_UPDATE\", False)),\n        help=\"Whether this miner should NOT automatically update upon new release.\",\n        action=\"store_true\",\n    )\n    parser.add_argument(\n        \"--wandb-key\", type=str, default=\"\", help=\"A https://wandb.ai API key\"\n    )\n    parser.add_argument(\n        \"--disable-wandb\",\n        default=False,\n        help=\"Whether to disable WandB logging.\",\n        action=\"store_true\",\n    )\n    parser.add_argument(\n        \"--dev\",\n        default=False,\n        help=\"Whether to run in development mode for internal testing.\",\n        action=\"store_true\",\n    )\n    parser.add_argument(\n        \"--localnet\",\n        action=\"store_true\",\n        default=False,\n        help=\"Whether to run the miner in localnet mode.\",\n    )\n    parser.add_argument(\n        \"--timeout\",\n        default=120,\n        type=int,\n        help=\"Timeout for requests in seconds (default: 120)\",\n    )\n    parser.add_argument(\n        \"--external-model-dir\",\n        default=None,\n        help=\"Custom location for storing models data (optional)\",\n    )\n    if role == Roles.VALIDATOR:\n        # CLI arguments specific to the validator\n        _validator_config()\n    elif role == Roles.MINER:\n        # CLI arguments specific to the miner\n        _miner_config()\n    else:\n        bt.subtensor.add_args(parser)\n        bt.logging.add_args(parser)\n        bt.wallet.add_args(parser)\n        config = bt.config(parser, strict=True)\n\n    if SHOW_HELP:\n        # --help or -h flag was passed, show the help message and exit\n        parser.print_help()\n        sys.exit(0)\n\n    if config.localnet:\n        # quick localnet configuration set up for testing (common params for both miner and validator)\n        if (\n            config.subtensor.chain_endpoint\n            == \"wss://entrypoint-finney.opentensor.ai:443\"\n        ):\n            # in case of default value, change to localnet\n            config.subtensor.chain_endpoint = \"ws://127.0.0.1:9944\"\n        if config.subtensor.network == \"finney\":\n            config.subtensor.network = \"local\"\n        config.eth_wallet = (\n            config.eth_wallet if config.eth_wallet is not None else \"0x002\"\n        )\n        config.disable_wandb = True\n        config.verbose = config.verbose if config.verbose is None else True\n\n    config.full_path = os.path.expanduser(\"~/.bittensor/omron\")  # type: ignore\n    config.full_path_score = os.path.join(config.full_path, \"scores\", \"scores.pt\")\n    if not config.certificate_path:\n        config.certificate_path = os.path.join(config.full_path, \"cert\")\n\n    if config.external_model_dir:\n        # user might have specified a custom location for storing models data\n        # if not, we use the default location\n        config.full_path_models = config.external_model_dir\n    else:\n        config.full_path_models = os.path.join(config.full_path, \"models\")\n\n    if config.whitelisted_public_keys:\n        config.whitelisted_public_keys = config.whitelisted_public_keys.split(\",\")\n\n    os.makedirs(config.full_path, exist_ok=True)\n    os.makedirs(config.full_path_models, exist_ok=True)\n    os.makedirs(config.certificate_path, exist_ok=True)\n    os.makedirs(os.path.dirname(config.full_path_score), exist_ok=True)\n    bt.logging(config=config, logging_dir=config.logging.logging_dir)\n    bt.logging.enable_info()\n\n    # Make sure we have access to the models directory\n    if not os.access(config.full_path, os.W_OK):\n        bt.logging.error(\n            f\"Cannot write to {config.full_path}. Please make sure you have the correct permissions.\"\n        )\n\n    if config.wandb_key:\n        wandb_logger.safe_login(api_key=config.wandb_key)\n        bt.logging.success(\"Logged into WandB\")\n\n\ndef _miner_config():\n    \"\"\"\n    Add CLI arguments specific to the miner.\n    \"\"\"\n    global parser\n    global config\n\n    parser.add_argument(\n        \"--disable-blacklist\",\n        default=None,\n        action=\"store_true\",\n        help=\"Disables request filtering and allows all incoming requests.\",\n    )\n\n    parser.add_argument(\n        \"--storage.provider\",\n        type=str,\n        choices=[\"r2\", \"s3\"],\n        help=\"Storage provider (r2 or s3)\",\n        default=os.getenv(\"STORAGE_PROVIDER\", \"r2\"),\n    )\n\n    parser.add_argument(\n        \"--storage.bucket\",\n        type=str,\n        help=\"Storage bucket name for competition files\",\n        default=os.getenv(\"STORAGE_BUCKET\") or os.getenv(\"R2_BUCKET\"),\n    )\n\n    parser.add_argument(\n        \"--storage.account_id\",\n        type=str,\n        help=\"Storage account ID (required for R2)\",\n        default=os.getenv(\"STORAGE_ACCOUNT_ID\") or os.getenv(\"R2_ACCOUNT_ID\"),\n    )\n\n    parser.add_argument(\n        \"--storage.access_key\",\n        type=str,\n        help=\"Storage access key ID\",\n        default=os.getenv(\"STORAGE_ACCESS_KEY\") or os.getenv(\"R2_ACCESS_KEY\"),\n    )\n\n    parser.add_argument(\n        \"--storage.secret_key\",\n        type=str,\n        help=\"Storage secret key\",\n        default=os.getenv(\"STORAGE_SECRET_KEY\") or os.getenv(\"R2_SECRET_KEY\"),\n    )\n\n    parser.add_argument(\n        \"--storage.region\",\n        type=str,\n        help=\"Storage region (required for S3)\",\n        default=os.getenv(\"STORAGE_REGION\", \"us-east-1\"),\n    )\n\n    parser.add_argument(\n        \"--competition-only\",\n        action=\"store_true\",\n        help=\"Whether to only run the competition. Disables regular mining when set.\",\n    )\n\n    bt.subtensor.add_args(parser)\n    bt.logging.add_args(parser)\n    bt.wallet.add_args(parser)\n    bt.axon.add_args(parser)\n\n    config = bt.config(parser, strict=True)\n\n    if config.localnet:\n        # quick localnet configuration set up for testing (specific params for miner)\n        if config.wallet.name == \"default\":\n            config.wallet.name = \"miner\"\n        if not config.axon:\n            config.axon = bt.config()\n            config.axon.ip = \"127.0.0.1\"\n            config.axon.external_ip = \"127.0.0.1\"\n        config.disable_blacklist = (\n            config.disable_blacklist if config.disable_blacklist is not None else True\n        )\n\n\ndef _validator_config():\n    \"\"\"\n    Add CLI arguments specific to the validator.\n    \"\"\"\n    global parser\n    global config\n\n    parser.add_argument(\n        \"--blocks_per_epoch\",\n        type=int,\n        default=100,\n        help=\"Number of blocks to wait before setting weights\",\n    )\n\n    parser.add_argument(\n        \"--disable-statistic-logging\",\n        default=False,\n        help=\"Whether to disable statistic logging.\",\n        action=\"store_true\",\n    )\n\n    parser.add_argument(\n        \"--enable-pow\",\n        default=ONCHAIN_PROOF_OF_WEIGHTS_ENABLED,\n        action=\"store_true\",\n        help=\"Whether proof of weights is enabled\",\n    )\n\n    parser.add_argument(\n        \"--pow-target-interval\",\n        type=int,\n        default=PROOF_OF_WEIGHTS_INTERVAL,\n        help=\"The target interval for committing proof of weights to the chain\",\n    )\n\n    parser.add_argument(\n        \"--ignore-external-requests\",\n        type=lambda x: x.lower() == \"true\",\n        default=True,\n        help=\"Whether to ignore external requests.\",\n    )\n\n    parser.add_argument(\n        \"--competition-sync-interval\",\n        type=int,\n        default=COMPETITION_SYNC_INTERVAL,\n        help=\"The interval for syncing the competition in seconds. Defaults to 86400 (1 day).\",\n    )\n\n    parser.add_argument(\n        \"--external-api-host\",\n        type=str,\n        default=\"0.0.0.0\",\n        help=\"The host for the external API.\",\n    )\n\n    parser.add_argument(\n        \"--external-api-port\",\n        type=int,\n        default=8443,\n        help=\"The port for the external API.\",\n    )\n\n    parser.add_argument(\n        \"--external-api-workers\",\n        type=int,\n        default=1,\n        help=\"The number of workers for the external API.\",\n    )\n\n    parser.add_argument(\n        \"--serve-axon\",\n        type=bool,\n        default=False,\n        help=\"Whether to serve the axon displaying your API information.\",\n    )\n\n    parser.add_argument(\n        \"--do-not-verify-external-signatures\",\n        default=False,\n        action=\"store_true\",\n        help=(\n            \"External PoW requests are signed by validator's (sender's) wallet. \"\n            \"By default we verify is the wallet legitimate. \"\n            \"You can disable this check with the flag.\"\n        ),\n    )\n\n    parser.add_argument(\n        \"--whitelisted-public-keys\",\n        type=str,\n        default=None,\n        help=\"A comma-separated list of public keys to whitelist for external requests.\",\n    )\n\n    parser.add_argument(\n        \"--certificate-path\",\n        type=str,\n        default=None,\n        help=\"A custom path to a directory containing a public and private SSL certificate. \"\n        \"(cert.pem and key.pem) \"\n        \"Please note that this should not be used unless you have issued your own certificate. \"\n        \"Omron will issue a certificate for you by default.\",\n    )\n\n    parser.add_argument(\n        \"--prometheus-monitoring\",\n        action=\"store_true\",\n        default=False,\n        help=\"Whether to enable prometheus monitoring.\",\n    )\n\n    parser.add_argument(\n        \"--prometheus-port\",\n        type=int,\n        default=9090,\n        help=\"The port for the prometheus monitoring.\",\n    )\n\n    bt.subtensor.add_args(parser)\n    bt.logging.add_args(parser)\n    bt.wallet.add_args(parser)\n\n    config = bt.config(parser, strict=True)\n\n    if config.localnet:\n        # quick localnet configuration set up for testing (specific params for validator)\n        if config.wallet.name == \"default\":\n            config.wallet.name = \"validator\"\n        config.external_api_workers = config.external_api_workers or 1\n        config.external_api_port = config.external_api_port or 8443\n        config.do_not_verify_external_signatures = True\n        config.disable_statistic_logging = True\n"}
{"type": "source_file", "path": "neurons/constants.py", "content": "import os\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass Roles:\n    VALIDATOR = \"validator\"\n    MINER = \"miner\"\n\n\n# The model ID for a batched proof of weights model\nBATCHED_PROOF_OF_WEIGHTS_MODEL_ID = (\n    \"1e6fcdaea58741e7248b631718dda90398a17b294480beb12ce8232e27ca3bff\"\n)\n# The model ID for a single proof of weights model\nSINGLE_PROOF_OF_WEIGHTS_MODEL_ID = (\n    \"fa0d509d52abe2d1e809124f8aba46258a02f7253582f7b7f5a22e1e0bca0dfb\"\n)\n\nIGNORED_MODEL_HASHES = [\n    \"0\",\n    \"0a92bc32ea02abe54159da70aeb541d52c3cba27c8708669eda634e096a86f8b\",\n    \"b7d33e7c19360c042d94c5a7360d7dc68c36dd56c449f7c49164a0098769c01f\",\n    \"55de10a6bcf638af4bc79901d63204a9e5b1c6534670aa03010bae6045e3d0e8\",\n    \"9998a12b8194d3e57d332b484ede57c3d871d42a176456c4e10da2995791d181\",\n    \"ed8ba401d709ee31f6b9272163c71451da171c7d71800313fe5db58d0f6c483a\",\n    \"1d60d545b7c5123fd60524dcbaf57081ca7dc4a9ec36c892927a3153328d17c0\",\n    \"37320fc74fec80805eedc8e92baf3c58842a2cb2a4ae127ad6e930f0c8441c7a\",\n    \"1d60d545b7c5123fd60524dcbaf57081ca7dc4a9ec36c892927a3153328d17c0\",\n    \"33b92394b18412622adad75733a6fc659b4e202b01ee8a5465958a6bad8ded62\",\n    \"37320fc74fec80805eedc8e92baf3c58842a2cb2a4ae127ad6e930f0c8441c7a\",\n    \"8dcff627a782525ea86196941a694ffbead179905f0cd4550ddc3df9e2b90924\",\n    \"a4bcecaf699fd9212600a1f2fcaa40c444e1aeaab409ea240a38c33ed356f4e2\",\n    \"e84b2e5f223621fa20078eb9f920d8d4d3a4ff95fa6e2357646fdbb43a2557c9\",\n    \"a849500803abdbb86a9460e18684a6411dc7ae0b75f1f6330e3028081a497dea\",\n]\n\n# The maximum timespan allowed for miners to respond to a query\nVALIDATOR_REQUEST_TIMEOUT_SECONDS = 120\n# The maximum timespan allowed for miners to process through a circuit\nCRICUIT_TIMEOUT_SECONDS = 60\n# An additional queueing time for external requests\nEXTERNAL_REQUEST_QUEUE_TIME_SECONDS = 10\n# Maximum number of concurrent requests that the validator will handle\nMAX_CONCURRENT_REQUESTS = 16\n# Default proof size when we're unable to determine the actual size\nDEFAULT_PROOF_SIZE = 5000\n# Size in percent of the sample to be used for the maximum score median\nMAXIMUM_SCORE_MEDIAN_SAMPLE = 0.05\n# Shift in seconds to apply to the minimum response time for vertical asymptote adjustment\nMINIMUM_SCORE_SHIFT = 0.0\n# Weights version hyperparameter\nWEIGHTS_VERSION = 1730\n# Rate limit for weight updates\nWEIGHT_RATE_LIMIT: int = 100\n# Delay between loop iterations\nLOOP_DELAY_SECONDS = 0.1\n# Exception delay for loop\nEXCEPTION_DELAY_SECONDS = 10\n# Default maximum score\nDEFAULT_MAX_SCORE = 1 / 235\n# Default subnet UID\nDEFAULT_NETUID = 2\n# Validator stake threshold\nVALIDATOR_STAKE_THRESHOLD = 1024\n# 🥩🥩🥩🥩🥩🥩🥩🥩🥩🥩🥩🥩🥩🥩🥩🥩🥩🥩🥩🥩🥩🥩🥩🥩🥩🥩🥩🥩🥩🥩🥩🥩🥩🥩🥩🥩🥩🥩\nSTEAK = \"🥩\"\n# Field modulus\nFIELD_MODULUS = (\n    21888242871839275222246405745257275088548364400416034343698204186575808495617\n)\n# Whether on-chain proof of weights is enabled by default\nONCHAIN_PROOF_OF_WEIGHTS_ENABLED = False\n# Frequency in terms of blocks at which proof of weights are posted\nPROOF_OF_WEIGHTS_INTERVAL = 1000\n# Maximum number of proofs to log at once\nMAX_PROOFS_TO_LOG = 0\n# Era period for proof of weights (mortality of the pow log)\nPROOF_OF_WEIGHTS_LIFESPAN = 2\n# Active competition\nACTIVE_COMPETITION = 0\n# Frequency in terms of seconds at which the competition is synced and evaluated\nCOMPETITION_SYNC_INTERVAL = 60 * 60 * 24\n# Maximum signature lifespan for WebSocket requests\nMAX_SIGNATURE_LIFESPAN = 300\n# Whitelisted public keys (ss58 addresses) we accept external requests from by default\n# (even if an address is not in the metagraph)\nWHITELISTED_PUBLIC_KEYS = []\n# Mainnet <> Testnet UID mapping\nMAINNET_TESTNET_UIDS = [\n    (1, 61),  # apex\n    (2, 118),  # omron\n    (3, 223),  # templar\n    (4, 40),  # targon\n    (5, 88),  # kaito\n    (6, 155),  # infinite\n    (7, 92),  # subvortex\n    (8, 3),  # ptn\n    (8, 116),  # ptn (PTN)\n    (10, 104),  # sturdy\n    (11, 135),  # dippy\n    (12, 174),  # horde\n    (13, 254),  # dataverse\n    (14, 203),  # palaidn\n    (15, 202),  # deval\n    (16, 120),  # bitads\n    (17, 89),  # 3gen\n    (18, 24),  # cortex\n    (19, 176),  # inference\n    (20, 76),  # bitagent\n    (21, 157),  # any-any\n    (23, 119),  # social\n    (24, 96),  # omega\n    (25, 141),  # protein\n    (26, 25),  # alchemy\n    (27, 15),  # compute\n    (28, 93),  # oracle\n    (31, 123),  # naschain\n    (32, 87),  # itsai\n    (33, 138),  # ready\n    (34, 168),  # mind\n    (35, 78),  # logic\n    (39, 159),  # edge\n    (40, 166),  # chunk\n    (41, 172),  # sportstensor\n    (42, 165),  # masa\n    (43, 65),  # graphite\n    (44, 180),  # score\n    (45, 171),  # gen42\n    (46, 182),  # neural\n    (48, 208),  # nextplace\n    (49, 100),  # automl\n    (50, 31),  # audio\n    (52, 98),  # dojo\n    (53, 232),  # efficient-frontier\n    (54, 236),  # docs-insights\n    (57, 237),  # gaia\n    (59, 249),  # agent-arena\n]\n# Proof publishing service URL\nPPS_URL = os.getenv(\n    \"OMRON_PPS_URL\",\n    \"https://pps.omron.ai/\",\n)\n# Testnet PPS URL\nTESTNET_PPS_URL = os.getenv(\n    \"OMRON_PPS_URL\",\n    \"https://cllswjfpzmg67rwythmiiufvtm0gsthd.lambda-url.us-east-1.on.aws/\",\n)\n# EZKL path\nLOCAL_EZKL_PATH = os.path.join(os.path.expanduser(\"~\"), \".ezkl\", \"ezkl\")\n# GitHub repository URL\nREPO_URL = \"https://github.com/inference-labs-inc/omron-subnet\"\n# Various time constants in seconds\nONE_SECOND = 1\nONE_MINUTE = 60\nFIVE_MINUTES = ONE_MINUTE * 5\nONE_HOUR = ONE_MINUTE * 60\nONE_DAY = ONE_HOUR * 24\nONE_YEAR = ONE_DAY * 365\n# Temporary folder for storing proof files\nTEMP_FOLDER = \"/tmp/omron\"\n\n# Queue size limits\nMAX_POW_QUEUE_SIZE = 1024\nMAX_EVALUATION_ITEMS = 1024\n\n# Maximum circuit size in GB for competitions\nMAX_CIRCUIT_SIZE_GB = 50\n"}
{"type": "source_file", "path": "neurons/deployment_layer/circuit_store.py", "content": "from __future__ import annotations\n\nimport os\nimport traceback\nfrom typing import Optional\n\nimport bittensor as bt\nfrom packaging import version\n\nfrom constants import IGNORED_MODEL_HASHES, MAINNET_TESTNET_UIDS\nfrom execution_layer.circuit import Circuit\n\n\nclass CircuitStore:\n    \"\"\"\n    A Singleton class to manage and store Circuit objects.\n\n    This class is responsible for loading, storing, and retrieving Circuit objects.\n    \"\"\"\n\n    _instance = None\n\n    def __new__(cls, *args, **kwargs):\n        \"\"\"\n        Override the __new__ method to implement the Singleton pattern.\n        \"\"\"\n        if not cls._instance:\n            cls._instance = super(CircuitStore, cls).__new__(cls, *args, **kwargs)\n        return cls._instance\n\n    def __init__(self):\n        \"\"\"\n        Initialize the CircuitStore.\n\n        Creates an empty dictionary to store Circuit objects and loads circuits.\n        \"\"\"\n        self.circuits: dict[str, Circuit] = {}\n\n    def load_circuits(self, deployment_layer_path: Optional[str] = None):\n        \"\"\"\n        Load circuits from the file system.\n\n        Searches for directories starting with 'model_' in the deployment layer path,\n        attempts to create Circuit objects from these directories, and stores them\n        in the circuits dictionary.\n        \"\"\"\n        deployment_layer_path = os.path.dirname(__file__)\n        bt.logging.info(f\"Loading circuits from {deployment_layer_path}\")\n\n        for folder_name in os.listdir(deployment_layer_path):\n            folder_path = os.path.join(deployment_layer_path, folder_name)\n\n            if os.path.isdir(folder_path) and folder_name.startswith(\"model_\"):\n                circuit_id = folder_name.split(\"_\")[1]\n\n                if circuit_id in IGNORED_MODEL_HASHES:\n                    bt.logging.info(f\"Ignoring circuit {circuit_id}\")\n                    continue\n\n                try:\n                    bt.logging.debug(f\"Attempting to load circuit {circuit_id}\")\n                    circuit = Circuit(circuit_id)\n                    self.circuits[circuit_id] = circuit\n                    bt.logging.info(f\"Successfully loaded circuit {circuit_id}\")\n                except Exception as e:\n                    bt.logging.error(f\"Error loading circuit {circuit_id}: {e}\")\n                    traceback.print_exc()\n                    continue\n\n        bt.logging.info(f\"Loaded {len(self.circuits)} circuits\")\n\n    def get_circuit(self, circuit_id: str) -> Circuit | None:\n        \"\"\"\n        Retrieve a Circuit object by its ID.\n\n        Args:\n            circuit_id (str): The ID of the circuit to retrieve.\n\n        Returns:\n            Circuit | None: The Circuit object if found, None otherwise.\n        \"\"\"\n        circuit = self.circuits.get(circuit_id)\n        if circuit:\n            bt.logging.debug(f\"Retrieved circuit {circuit}\")\n        else:\n            bt.logging.warning(f\"Circuit {circuit_id} not found\")\n        return circuit\n\n    def get_latest_circuit_for_netuid(self, netuid: int):\n        \"\"\"\n        Get the latest circuit for a given netuid by comparing semver version strings.\n\n        Args:\n            netuid (int): The subnet ID to find the latest circuit for\n\n        Returns:\n            Circuit | None: The circuit with the highest semver version for the given netuid,\n            or None if no circuits found\n        \"\"\"\n\n        matching_circuits = [\n            c for c in self.circuits.values() if c.metadata.netuid == netuid\n        ]\n        if not matching_circuits:\n            return None\n\n        return max(matching_circuits, key=lambda c: version.parse(c.metadata.version))\n\n    def get_circuit_for_netuid_and_version(\n        self, netuid: int, version: int\n    ) -> Circuit | None:\n        \"\"\"\n        Get the circuit for a given netuid and version.\n        \"\"\"\n        matching_circuits = [\n            c\n            for c in self.circuits.values()\n            if c.metadata.netuid == netuid and c.metadata.weights_version == version\n        ]\n        if not matching_circuits:\n            bt.logging.warning(\n                f\"No circuit found for netuid {netuid} and weights version {version}\"\n            )\n            return None\n        return matching_circuits[0]\n\n    def get_latest_circuit_by_name(self, circuit_name: str) -> Circuit | None:\n        \"\"\"\n        Get the latest circuit by name.\n        \"\"\"\n        matching_circuits = [\n            c for c in self.circuits.values() if c.metadata.name == circuit_name\n        ]\n        return max(matching_circuits, key=lambda c: version.parse(c.metadata.version))\n\n    def get_circuit_by_name_and_version(\n        self, circuit_name: str, version: int\n    ) -> Circuit | None:\n        \"\"\"\n        Get the circuit by name and version.\n        \"\"\"\n        matching_circuits = [\n            c\n            for c in self.circuits.values()\n            if c.metadata.name == circuit_name and c.metadata.version == version\n        ]\n        return matching_circuits[0] if matching_circuits else None\n\n    def list_circuits(self) -> list[str]:\n        \"\"\"\n        Get a list of all circuit IDs.\n\n        Returns:\n            list[str]: A list of circuit IDs.\n        \"\"\"\n        circuit_list = list(self.circuits.keys())\n        bt.logging.debug(f\"Listed {len(circuit_list)} circuits\")\n        return circuit_list\n\n    def list_circuit_metadata(self) -> list[dict]:\n        \"\"\"\n        JSON safe circuit metadata for use in API serving.\n        \"\"\"\n        data: list[dict] = []\n        for circuit in self.circuits.values():\n            data.append(\n                {\n                    \"id\": circuit.id,\n                    \"name\": circuit.metadata.name,\n                    \"description\": circuit.metadata.description,\n                    \"author\": circuit.metadata.author,\n                    \"version\": circuit.metadata.version,\n                    \"type\": circuit.metadata.type,\n                    \"proof_system\": circuit.metadata.proof_system,\n                    \"netuid\": circuit.metadata.netuid,\n                    \"testnet_netuids\": (\n                        [\n                            uid[1]\n                            for uid in MAINNET_TESTNET_UIDS\n                            if uid[0] == int(circuit.metadata.netuid)\n                        ]\n                        if circuit.metadata.netuid\n                        else None\n                    ),\n                    \"weights_version\": circuit.metadata.weights_version,\n                    \"input_schema\": circuit.input_handler.schema.model_json_schema(),\n                }\n            )\n        return data\n\n\ncircuit_store = CircuitStore()\nbt.logging.info(\"CircuitStore initialized\")\n"}
