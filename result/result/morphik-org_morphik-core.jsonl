{"repo_info": {"repo_name": "morphik-core", "repo_owner": "morphik-org", "repo_url": "https://github.com/morphik-org/morphik-core"}}
{"type": "test_file", "path": "core/tests/integration/test_api.py", "content": "import asyncio\nimport json\nimport pytest\nfrom pathlib import Path\nimport jwt\nfrom datetime import datetime, timedelta, UTC\nfrom typing import AsyncGenerator, Dict\nfrom httpx import AsyncClient\nfrom fastapi import FastAPI\nfrom httpx import ASGITransport\nfrom core.api import get_settings\nimport filetype\nimport logging\nfrom sqlalchemy.ext.asyncio import create_async_engine\nfrom sqlalchemy import text\n\nlogger = logging.getLogger(__name__)\n\n\n# Test configuration\nTEST_DATA_DIR = Path(__file__).parent / \"test_data\"\nJWT_SECRET = \"your-secret-key-for-signing-tokens\"\nTEST_USER_ID = \"test_user\"\nTEST_POSTGRES_URI = \"postgresql+asyncpg://postgres:postgres@localhost:5432/databridge_test\"\n\n\n@pytest.fixture(scope=\"session\")\ndef event_loop():\n    \"\"\"Create an instance of the default event loop for the test session\"\"\"\n    policy = asyncio.get_event_loop_policy()\n    loop = policy.new_event_loop()\n    asyncio.set_event_loop(loop)\n    yield loop\n    loop.close()\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\nasync def setup_test_environment(event_loop):\n    \"\"\"Setup test environment and create test files\"\"\"\n    # Create test data directory if it doesn't exist\n    TEST_DATA_DIR.mkdir(exist_ok=True)\n\n    # Create a test text file\n    text_file = TEST_DATA_DIR / \"test.txt\"\n    if not text_file.exists():\n        text_file.write_text(\"This is a test document for DataBridge testing.\")\n\n    # Create a small test PDF if it doesn't exist\n    pdf_file = TEST_DATA_DIR / \"test.pdf\"\n    if not pdf_file.exists():\n        pytest.skip(\"PDF file not available, skipping PDF tests\")\n\n\ndef create_test_token(\n    entity_type: str = \"developer\",\n    entity_id: str = TEST_USER_ID,\n    permissions: list = None,\n    app_id: str = None,\n    expired: bool = False,\n) -> str:\n    \"\"\"Create a test JWT token\"\"\"\n    if not permissions:\n        permissions = [\"read\", \"write\", \"admin\"]\n\n    payload = {\n        \"type\": entity_type,\n        \"entity_id\": entity_id,\n        \"permissions\": permissions,\n        \"exp\": datetime.now(UTC) + timedelta(days=-1 if expired else 1),\n    }\n\n    if app_id:\n        payload[\"app_id\"] = app_id\n\n    return jwt.encode(payload, JWT_SECRET, algorithm=\"HS256\")\n\n\ndef create_auth_header(\n    entity_type: str = \"developer\", permissions: list = None, expired: bool = False\n) -> Dict[str, str]:\n    \"\"\"Create authorization header with test token\"\"\"\n    token = create_test_token(entity_type, permissions=permissions, expired=expired)\n    return {\"Authorization\": f\"Bearer {token}\"}\n\n\n@pytest.fixture\nasync def test_app(event_loop: asyncio.AbstractEventLoop) -> FastAPI:\n    \"\"\"Create test FastAPI application\"\"\"\n    # Configure test settings\n    settings = get_settings()\n    settings.JWT_SECRET_KEY = JWT_SECRET\n    \n    # Override database settings to use test database\n    # This ensures we don't use the production database from .env\n    settings.POSTGRES_URI = TEST_POSTGRES_URI\n    settings.DATABASE_PROVIDER = \"postgres\"  # Ensure we're using postgres\n    \n    # IMPORTANT: We need to completely reinitialize the database connections\n    # since they were already established at import time\n    \n    # First, get the database from the API module\n    from core.api import database as api_database, app\n    \n    # Close existing connection if it exists\n    if hasattr(api_database, 'engine'):\n        await api_database.engine.dispose()\n    \n    # Create a new database connection with the test URI\n    from core.database.postgres_database import PostgresDatabase\n    test_database = PostgresDatabase(uri=TEST_POSTGRES_URI)\n    \n    # Initialize the test database\n    await test_database.initialize()\n    \n    # Replace the global database instance with our test database\n    import core.api\n    core.api.database = test_database\n    \n    # Also update the vector store if it uses the same database (for pgvector)\n    if settings.VECTOR_STORE_PROVIDER == \"pgvector\":\n        from core.vector_store.pgvector_store import PGVectorStore\n        from core.api import vector_store as api_vector_store\n        \n        # Create a new vector store with the test URI\n        test_vector_store = PGVectorStore(uri=TEST_POSTGRES_URI)\n        \n        # Replace the global vector store with our test version\n        core.api.vector_store = test_vector_store\n    \n    # Update the document service with our test instances\n    from core.api import document_service as api_document_service\n    from core.services.document_service import DocumentService\n    from core.api import parser, embedding_model, reranker, storage\n    \n    # Create a new document service with our test database and vector store\n    from core.api import completion_model, cache_factory, colpali_embedding_model, colpali_vector_store\n    \n    test_document_service = DocumentService(\n        database=test_database,\n        vector_store=core.api.vector_store,\n        parser=parser,\n        embedding_model=embedding_model,\n        completion_model=completion_model,\n        cache_factory=cache_factory,\n        reranker=reranker,\n        storage=storage,\n        enable_colpali=settings.ENABLE_COLPALI,\n        colpali_embedding_model=colpali_embedding_model,\n        colpali_vector_store=colpali_vector_store,\n    )\n    \n    # Replace the global document service with our test version\n    core.api.document_service = test_document_service\n    \n    # Update the graph service if needed\n    if hasattr(core.api, 'graph_service'):\n        from core.services.graph_service import GraphService\n        from core.api import completion_model\n        \n        test_graph_service = GraphService(\n            db=test_database,\n            embedding_model=embedding_model,\n            completion_model=completion_model\n        )\n        \n        core.api.graph_service = test_graph_service\n    \n    return app\n\n\n@pytest.fixture\nasync def client(\n    test_app: FastAPI, event_loop: asyncio.AbstractEventLoop\n) -> AsyncGenerator[AsyncClient, None]:\n    \"\"\"Create async test client\"\"\"\n    async with AsyncClient(transport=ASGITransport(app=test_app), base_url=\"http://test\") as client:\n        yield client\n\n\n@pytest.fixture(scope=\"function\", autouse=True)\nasync def cleanup_documents():\n    \"\"\"Clean up documents before each document test\"\"\"\n    # This will run before the test\n    yield\n    # This will run after the test\n    \n    # We should always use the test database\n    # Create a fresh connection to make sure we're not affected by any state\n    engine = create_async_engine(TEST_POSTGRES_URI)\n    \n    try:\n        async with engine.begin() as conn:\n            # Clean up by deleting all rows rather than dropping tables\n            await conn.execute(text(\"DELETE FROM documents\"))\n            \n            # Delete from chunks table\n            try:\n                await conn.execute(text(\"DELETE FROM vector_embeddings\"))\n            except Exception as e:\n                logger.info(f\"No chunks table to clean or error: {e}\")\n\n    except Exception as e:\n        logger.error(f\"Failed to clean up document tables: {e}\")\n        raise\n    finally:\n        await engine.dispose()\n\n\n@pytest.mark.asyncio\nasync def test_ingest_text_document(\n    client: AsyncClient, content: str = \"Test content for document ingestion\"\n):\n    \"\"\"Test ingesting a text document\"\"\"\n    headers = create_auth_header()\n\n    response = await client.post(\n        \"/ingest/text\",\n        json={\"content\": content, \"metadata\": {\"test\": True, \"type\": \"text\"}, \"use_colpali\": True},\n        headers=headers,\n    )\n\n    assert response.status_code == 200\n    data = response.json()\n    assert \"external_id\" in data\n    assert data[\"content_type\"] == \"text/plain\"\n    assert data[\"metadata\"][\"test\"] is True\n\n    return data[\"external_id\"]\n\n\n@pytest.mark.asyncio\nasync def test_ingest_pdf(client: AsyncClient):\n    \"\"\"Test ingesting a pdf\"\"\"\n    headers = create_auth_header()\n    pdf_path = TEST_DATA_DIR / \"test.pdf\"\n\n    if not pdf_path.exists():\n        pytest.skip(\"Test PDF file not available\")\n\n    content_type = filetype.guess(pdf_path).mime\n    if not content_type:\n        content_type = \"application/octet-stream\"\n\n    with open(pdf_path, \"rb\") as f:\n        response = await client.post(\n            \"/ingest/file\",\n            files={\"file\": (pdf_path.name, f, content_type)},\n            data={\"metadata\": json.dumps({\"test\": True, \"type\": \"pdf\"}), \"use_colpali\": True},\n            headers=headers,\n        )\n\n    assert response.status_code == 200\n    data = response.json()\n    assert \"external_id\" in data\n    assert data[\"content_type\"] == \"application/pdf\"\n    assert \"storage_info\" in data\n\n    return data[\"external_id\"]\n\n\n@pytest.mark.asyncio\nasync def test_ingest_invalid_text_request(client: AsyncClient):\n    \"\"\"Test ingestion with invalid text request missing required content field\"\"\"\n    headers = create_auth_header()\n\n    response = await client.post(\n        \"/ingest/text\",\n        json={\"wrong_field\": \"Test content\"},  # Missing required content field\n        headers=headers,\n    )\n    assert response.status_code == 422  # Validation error\n\n\n@pytest.mark.asyncio\nasync def test_ingest_invalid_file_request(client: AsyncClient):\n    \"\"\"Test ingestion with invalid file request missing file\"\"\"\n    headers = create_auth_header()\n\n    response = await client.post(\n        \"/ingest/file\",\n        files={},  # Missing file\n        data={\"metadata\": \"{}\"},\n        headers=headers,\n    )\n    assert response.status_code == 422  # Validation error\n\n\n@pytest.mark.asyncio\nasync def test_ingest_invalid_metadata(client: AsyncClient):\n    \"\"\"Test ingestion with invalid metadata JSON\"\"\"\n    headers = create_auth_header()\n\n    pdf_path = TEST_DATA_DIR / \"test.pdf\"\n    if pdf_path.exists():\n        files = {\"file\": (\"test.pdf\", open(pdf_path, \"rb\"), \"application/pdf\")}\n        response = await client.post(\n            \"/ingest/file\",\n            files=files,\n            data={\"metadata\": \"invalid json\"},\n            headers=headers,\n        )\n        assert response.status_code == 400  # Bad request\n\n\n@pytest.mark.asyncio\n@pytest.mark.skipif(\n    get_settings().EMBEDDING_PROVIDER == \"ollama\",\n    reason=\"local embedding models do not have size limits\",\n)\nasync def test_ingest_oversized_content(client: AsyncClient):\n    \"\"\"Test ingestion with oversized content\"\"\"\n    headers = create_auth_header()\n    large_content = \"x\" * (10 * 1024 * 1024)  # 10MB\n    response = await client.post(\n        \"/ingest/text\", json={\"content\": large_content, \"metadata\": {}}, headers=headers\n    )\n    assert response.status_code == 400  # Bad request\n\n\n@pytest.mark.asyncio\nasync def test_auth_missing_header(client: AsyncClient):\n    \"\"\"Test authentication with missing auth header\"\"\"\n    if get_settings().dev_mode:\n        pytest.skip(\"Auth tests skipped in dev mode\")\n    response = await client.post(\"/ingest/text\")\n    assert response.status_code == 401\n\n\n@pytest.mark.asyncio\nasync def test_auth_invalid_token(client: AsyncClient):\n    \"\"\"Test authentication with invalid token\"\"\"\n    if get_settings().dev_mode:\n        pytest.skip(\"Auth tests skipped in dev mode\")\n    headers = {\"Authorization\": \"Bearer invalid_token\"}\n    response = await client.post(\"/ingest/file\", headers=headers)\n    assert response.status_code == 401\n\n\n@pytest.mark.asyncio\nasync def test_auth_expired_token(client: AsyncClient):\n    \"\"\"Test authentication with expired token\"\"\"\n    if get_settings().dev_mode:\n        pytest.skip(\"Auth tests skipped in dev mode\")\n    headers = create_auth_header(expired=True)\n    response = await client.post(\"/ingest/text\", headers=headers)\n    assert response.status_code == 401\n\n\n@pytest.mark.asyncio\nasync def test_auth_insufficient_permissions(client: AsyncClient):\n    \"\"\"Test authentication with insufficient permissions\"\"\"\n    if get_settings().dev_mode:\n        pytest.skip(\"Auth tests skipped in dev mode\")\n    headers = create_auth_header(permissions=[\"read\"])\n    response = await client.post(\n        \"/ingest/text\",\n        json={\"content\": \"Test content\", \"metadata\": {}},\n        headers=headers,\n    )\n    assert response.status_code == 403\n\n\n@pytest.mark.asyncio\nasync def test_list_documents(client: AsyncClient):\n    \"\"\"Test listing documents\"\"\"\n    # First ingest some documents\n    doc_id1 = await test_ingest_text_document(client)\n    doc_id2 = await test_ingest_text_document(client)\n\n    headers = create_auth_header()\n    response = await client.get(\"/documents\", headers=headers)\n\n    assert response.status_code == 200\n    docs = response.json()\n    assert len(docs) >= 2\n    doc_ids = [doc[\"external_id\"] for doc in docs]\n    assert doc_id1 in doc_ids\n    assert doc_id2 in doc_ids\n\n\n@pytest.mark.asyncio\nasync def test_get_document(client: AsyncClient):\n    \"\"\"Test getting a specific document\"\"\"\n    # First ingest a document\n    doc_id = await test_ingest_text_document(client)\n\n    headers = create_auth_header()\n    response = await client.get(f\"/documents/{doc_id}\", headers=headers)\n\n    assert response.status_code == 200\n    doc = response.json()\n    assert doc[\"external_id\"] == doc_id\n    assert \"metadata\" in doc\n    assert \"content_type\" in doc\n    \n    \n@pytest.mark.asyncio\nasync def test_get_document_by_filename(client: AsyncClient):\n    \"\"\"Test getting a document by filename\"\"\"\n    # First ingest a document with a specific filename\n    filename = \"test_get_by_filename.txt\"\n    headers = create_auth_header()\n    \n    initial_content = \"This is content for testing get_document_by_filename.\"\n    response = await client.post(\n        \"/ingest/text\",\n        json={\n            \"content\": initial_content, \n            \"filename\": filename,\n            \"metadata\": {\"test\": True, \"type\": \"text\"},\n        },\n        headers=headers,\n    )\n    \n    assert response.status_code == 200\n    doc_id = response.json()[\"external_id\"]\n    \n    # Now try to get the document by filename\n    response = await client.get(f\"/documents/filename/{filename}\", headers=headers)\n    \n    assert response.status_code == 200\n    doc = response.json()\n    assert doc[\"external_id\"] == doc_id\n    assert doc[\"filename\"] == filename\n    assert \"metadata\" in doc\n    assert \"content_type\" in doc\n\n\n@pytest.mark.asyncio\nasync def test_invalid_document_id(client: AsyncClient):\n    \"\"\"Test error handling for invalid document ID\"\"\"\n    headers = create_auth_header()\n    response = await client.get(\"/documents/invalid_id\", headers=headers)\n    assert response.status_code == 404\n\n\n@pytest.mark.asyncio\nasync def test_update_document_with_text(client: AsyncClient):\n    \"\"\"Test updating a document with text content\"\"\"\n    # First ingest a document to update\n    initial_content = \"This is the initial content for update testing.\"\n    doc_id = await test_ingest_text_document(client, content=initial_content)\n    \n    headers = create_auth_header()\n    update_content = \"This is additional content for the document.\"\n    \n    # Test updating with text content\n    response = await client.post(\n        f\"/documents/{doc_id}/update_text\",\n        json={\n            \"content\": update_content,\n            \"metadata\": {\"updated\": True, \"version\": \"2.0\"},\n            \"use_colpali\": True\n        },\n        headers=headers,\n    )\n    \n    assert response.status_code == 200\n    updated_doc = response.json()\n    assert updated_doc[\"external_id\"] == doc_id\n    assert updated_doc[\"metadata\"][\"updated\"] is True\n    assert updated_doc[\"metadata\"][\"version\"] == \"2.0\"\n    \n    # Verify the content was updated by retrieving chunks\n    search_response = await client.post(\n        \"/retrieve/chunks\",\n        json={\n            \"query\": \"additional content\",\n            \"filters\": {\"external_id\": doc_id},\n        },\n        headers=headers,\n    )\n    \n    assert search_response.status_code == 200\n    chunks = search_response.json()\n    assert len(chunks) > 0\n    assert any(update_content in chunk[\"content\"] for chunk in chunks)\n\n\n@pytest.mark.asyncio\nasync def test_update_document_with_file(client: AsyncClient):\n    \"\"\"Test updating a document with file content\"\"\"\n    # First ingest a document to update\n    initial_content = \"This is the initial content for file update testing.\"\n    doc_id = await test_ingest_text_document(client, content=initial_content)\n    \n    headers = create_auth_header()\n    \n    # Create a test file to upload\n    test_file_path = TEST_DATA_DIR / \"update_test.txt\"\n    update_content = \"This is file content for updating the document.\"\n    test_file_path.write_text(update_content)\n    \n    with open(test_file_path, \"rb\") as f:\n        response = await client.post(\n            f\"/documents/{doc_id}/update_file\",\n            files={\"file\": (\"update_test.txt\", f, \"text/plain\")},\n            data={\n                \"metadata\": json.dumps({\"updated_with_file\": True}),\n                \"rules\": json.dumps([]),\n                \"update_strategy\": \"add\",\n            },\n            headers=headers,\n        )\n    \n    assert response.status_code == 200\n    updated_doc = response.json()\n    assert updated_doc[\"external_id\"] == doc_id\n    assert updated_doc[\"metadata\"][\"updated_with_file\"] is True\n    \n    # Verify the content was updated by retrieving chunks\n    search_response = await client.post(\n        \"/retrieve/chunks\",\n        json={\n            \"query\": \"file content for updating\",\n            \"filters\": {\"external_id\": doc_id},\n        },\n        headers=headers,\n    )\n    \n    assert search_response.status_code == 200\n    chunks = search_response.json()\n    assert len(chunks) > 0\n    assert any(update_content in chunk[\"content\"] for chunk in chunks)\n    \n    # Clean up the test file\n    test_file_path.unlink(missing_ok=True)\n\n\n@pytest.mark.asyncio\nasync def test_update_document_metadata(client: AsyncClient):\n    \"\"\"Test updating only a document's metadata\"\"\"\n    # First ingest a document to update\n    initial_content = \"This is the content for metadata update testing.\"\n    doc_id = await test_ingest_text_document(client, content=initial_content)\n    \n    headers = create_auth_header()\n    \n    # Test updating just metadata\n    new_metadata = {\n        \"meta_updated\": True,\n        \"tags\": [\"test\", \"metadata\", \"update\"],\n        \"priority\": 1\n    }\n    \n    response = await client.post(\n        f\"/documents/{doc_id}/update_metadata\",\n        json=new_metadata,\n        headers=headers,\n    )\n    \n    assert response.status_code == 200\n    updated_doc = response.json()\n    assert updated_doc[\"external_id\"] == doc_id\n    \n    # Verify the response has the updated metadata\n    assert updated_doc[\"metadata\"][\"meta_updated\"] is True\n    assert \"test\" in updated_doc[\"metadata\"][\"tags\"]\n    assert updated_doc[\"metadata\"][\"priority\"] == 1\n    \n    # Fetch the document to verify it exists\n    get_response = await client.get(f\"/documents/{doc_id}\", headers=headers)\n    assert get_response.status_code == 200\n    \n    # Note: Depending on caching or database behavior, the metadata may not be \n    # immediately visible in a subsequent fetch. The important part is that\n    # the update operation itself returned the correct metadata.\n    \n    \n@pytest.mark.asyncio\nasync def test_update_document_with_rules(client: AsyncClient):\n    \"\"\"Test updating a document with text content and applying rules\"\"\"\n    # First ingest a document to update\n    initial_content = \"This is the initial content for rule testing.\"\n    doc_id = await test_ingest_text_document(client, content=initial_content)\n    \n    headers = create_auth_header()\n    update_content = \"This document contains information about John Doe who lives at 123 Main St and has SSN 123-45-6789.\"\n    \n    # Create a rule to apply during update (natural language rule to remove PII)\n    rule = {\n        \"type\": \"natural_language\",\n        \"prompt\": \"Remove all personally identifiable information (PII) such as names, addresses, and SSNs.\"\n    }\n    \n    # Test updating with text content and a rule\n    response = await client.post(\n        f\"/documents/{doc_id}/update_text\",\n        json={\n            \"content\": update_content,\n            \"metadata\": {\"contains_pii\": False},\n            \"rules\": [rule],\n            \"use_colpali\": True\n        },\n        headers=headers,\n    )\n    \n    assert response.status_code == 200\n    updated_doc = response.json()\n    assert updated_doc[\"external_id\"] == doc_id\n    assert updated_doc[\"metadata\"][\"contains_pii\"] is False\n    \n    # Verify the content was updated and PII was removed by retrieving chunks\n    # Note: Exact behavior depends on the LLM response, so we check that something changed\n    search_response = await client.post(\n        \"/retrieve/chunks\",\n        json={\n            \"query\": \"information about person\",\n            \"filters\": {\"external_id\": doc_id},\n        },\n        headers=headers,\n    )\n    \n    assert search_response.status_code == 200\n    chunks = search_response.json()\n    assert len(chunks) > 0\n    # The processed content should have some of the original content but not the PII\n    assert any(\"information\" in chunk[\"content\"] for chunk in chunks)\n    # Check that at least one of the PII elements is not in the content\n    # This is a loose check since the exact result depends on the LLM\n    content_text = \" \".join([chunk[\"content\"] for chunk in chunks])\n    has_some_pii_removed = (\"John Doe\" not in content_text) or (\"123-45-6789\" not in content_text) or (\"123 Main St\" not in content_text)\n    assert has_some_pii_removed, \"Rule to remove PII did not seem to have any effect\"\n\n\n@pytest.mark.asyncio\nasync def test_file_versioning_with_add_strategy(client: AsyncClient):\n    \"\"\"Test that file versioning works correctly with 'add' update strategy\"\"\"\n    # First ingest a document with a file\n    headers = create_auth_header()\n    \n    # Create the initial file\n    initial_file_path = TEST_DATA_DIR / \"version_test_1.txt\"\n    initial_content = \"This is version 1 of the file for testing versioning.\"\n    initial_file_path.write_text(initial_content)\n    \n    # Ingest the initial file\n    with open(initial_file_path, \"rb\") as f:\n        response = await client.post(\n            \"/ingest/file\",\n            files={\"file\": (\"version_test.txt\", f, \"text/plain\")},\n            data={\n                \"metadata\": json.dumps({\"test\": True, \"version\": 1}),\n                \"rules\": json.dumps([]),\n            },\n            headers=headers,\n        )\n    \n    assert response.status_code == 200\n    doc_id = response.json()[\"external_id\"]\n    \n    # Create second version of the file\n    second_file_path = TEST_DATA_DIR / \"version_test_2.txt\"\n    second_content = \"This is version 2 of the file for testing versioning.\"\n    second_file_path.write_text(second_content)\n    \n    # Update with second file using \"add\" strategy\n    with open(second_file_path, \"rb\") as f:\n        response = await client.post(\n            f\"/documents/{doc_id}/update_file\",\n            files={\"file\": (\"version_test_v2.txt\", f, \"text/plain\")},\n            data={\n                \"metadata\": json.dumps({\"test\": True, \"version\": 2}),\n                \"rules\": json.dumps([]),\n                \"update_strategy\": \"add\",\n            },\n            headers=headers,\n        )\n    \n    assert response.status_code == 200\n    updated_doc = response.json()\n    \n    # Create third version of the file\n    third_file_path = TEST_DATA_DIR / \"version_test_3.txt\"\n    third_content = \"This is version 3 of the file for testing versioning.\"\n    third_file_path.write_text(third_content)\n    \n    # Update with third file using \"add\" strategy\n    with open(third_file_path, \"rb\") as f:\n        response = await client.post(\n            f\"/documents/{doc_id}/update_file\",\n            files={\"file\": (\"version_test_v3.txt\", f, \"text/plain\")},\n            data={\n                \"metadata\": json.dumps({\"test\": True, \"version\": 3}),\n                \"rules\": json.dumps([]),\n                \"update_strategy\": \"add\",\n            },\n            headers=headers,\n        )\n    \n    assert response.status_code == 200\n    final_doc = response.json()\n    \n    # Verify the system_metadata has versioning info\n    assert final_doc[\"system_metadata\"][\"version\"] >= 3\n    assert \"update_history\" in final_doc[\"system_metadata\"]\n    assert len(final_doc[\"system_metadata\"][\"update_history\"]) >= 2  # At least 2 updates\n    \n    # Verify storage_files field exists and has multiple entries\n    assert \"storage_files\" in final_doc\n    assert len(final_doc[\"storage_files\"]) >= 3  # Should have at least 3 files\n    \n    # Get most recent file's content through search \n    search_response = await client.post(\n        \"/retrieve/chunks\",\n        json={\n            \"query\": \"version 3 testing versioning\",\n            \"filters\": {\"external_id\": doc_id},\n        },\n        headers=headers,\n    )\n    assert search_response.status_code == 200\n    chunks = search_response.json()\n    assert any(third_content in chunk[\"content\"] for chunk in chunks)\n    \n    # Also check for version 1 content, which should still be in the merged content\n    search_response = await client.post(\n        \"/retrieve/chunks\",\n        json={\n            \"query\": \"version 1 testing versioning\",\n            \"filters\": {\"external_id\": doc_id},\n        },\n        headers=headers,\n    )\n    assert search_response.status_code == 200\n    chunks = search_response.json()\n    assert any(initial_content in chunk[\"content\"] for chunk in chunks)\n    \n    # Clean up test files\n    initial_file_path.unlink(missing_ok=True)\n    second_file_path.unlink(missing_ok=True)\n    third_file_path.unlink(missing_ok=True)\n\n\n@pytest.mark.asyncio\nasync def test_update_document_error_cases(client: AsyncClient):\n    \"\"\"Test error cases for document updates\"\"\"\n    headers = create_auth_header()\n    \n    # Test updating non-existent document by ID\n    response = await client.post(\n        \"/documents/non_existent_id/update_text\",\n        json={\n            \"content\": \"Test content for non-existent document\",\n            \"metadata\": {}\n        },\n        headers=headers,\n    )\n    assert response.status_code == 404\n    \n    \n    # Test updating text without content (validation error)\n    doc_id = await test_ingest_text_document(client)\n    response = await client.post(\n        f\"/documents/{doc_id}/update_text\",\n        json={\n            # Missing required content field\n            \"metadata\": {\"test\": True}\n        },\n        headers=headers,\n    )\n    assert response.status_code == 422\n    \n    # Test updating with insufficient permissions\n    if not get_settings().dev_mode:\n        restricted_headers = create_auth_header(permissions=[\"read\"])\n        response = await client.post(\n            f\"/documents/{doc_id}/update_metadata\",\n            json={\"restricted\": True},\n            headers=restricted_headers,\n        )\n        assert response.status_code == 403\n\n\n@pytest.mark.asyncio\nasync def test_retrieve_chunks(client: AsyncClient):\n    \"\"\"Test retrieving document chunks\"\"\"\n    upload_string = \"The quick brown fox jumps over the lazy dog\"\n    # First ingest a document to search\n    doc_id = await test_ingest_text_document(client, content=upload_string)\n\n    headers = create_auth_header()\n\n    response = await client.post(\n        \"/retrieve/chunks\",\n        json={\n            \"query\": \"jumping fox\",\n            \"k\": 1,\n            \"min_score\": 0.0,\n            \"filters\": {\"external_id\": doc_id},  # Add filter for specific document\n            \"use_colpali\": True,\n        },\n        headers=headers,\n    )\n\n    assert response.status_code == 200\n    results = list(response.json())\n    assert len(results) > 0\n    assert (not get_settings().USE_RERANKING) or results[0][\"score\"] > 0.5\n    assert any(upload_string == result[\"content\"] for result in results)\n\n\n@pytest.mark.asyncio\nasync def test_retrieve_docs(client: AsyncClient):\n    \"\"\"Test retrieving full documents\"\"\"\n    # First ingest a document to search\n    content = (\n        \"Headaches can significantly impact daily life and wellbeing. \"\n        \"Common triggers include stress, dehydration, and poor sleep habits. \"\n        \"While over-the-counter pain relievers may provide temporary relief, \"\n        \"it's important to identify and address the root causes. \"\n        \"Maintaining good health through proper nutrition, regular exercise, \"\n        \"and stress management can help prevent chronic headaches.\"\n    )\n    doc_id = await test_ingest_text_document(client, content=content)\n\n    headers = create_auth_header()\n    response = await client.post(\n        \"/retrieve/docs\",\n        json={\n            \"query\": \"Headaches, dehydration\",\n            \"filters\": {\"test\": True, \"external_id\": doc_id},  # Add filter for specific document\n            \"use_colpali\": True,\n        },\n        headers=headers,\n    )\n\n    assert response.status_code == 200\n    results = list(response.json())\n    assert len(results) > 0\n    assert results[0][\"document_id\"] == doc_id\n    assert \"score\" in results[0]\n    assert \"metadata\" in results[0]\n\n\n@pytest.mark.asyncio\nasync def test_query_completion(client: AsyncClient):\n    \"\"\"Test generating completions from context\"\"\"\n    # First ingest a document to use as context\n    content = (\n        \"The benefits of exercise are numerous. Regular physical activity \"\n        \"can improve cardiovascular health, strengthen muscles, enhance mental \"\n        \"wellbeing, and help maintain a healthy weight. Studies show that \"\n        \"even moderate exercise like walking can significantly reduce the risk \"\n        \"of various health conditions.\"\n    )\n    await test_ingest_text_document(client, content=content)\n\n    headers = create_auth_header()\n    response = await client.post(\n        \"/query\",\n        json={\n            \"query\": \"What are the main benefits of exercise?\",\n            \"k\": 2,\n            \"temperature\": 0.7,\n            \"max_tokens\": 100,\n        },\n        headers=headers,\n    )\n\n    assert response.status_code == 200\n    result = response.json()\n    assert \"completion\" in result\n    assert \"usage\" in result\n    assert len(result[\"completion\"]) > 0\n\n\n@pytest.mark.asyncio\nasync def test_invalid_retrieve_params(client: AsyncClient):\n    \"\"\"Test error handling for invalid retrieve parameters\"\"\"\n    headers = create_auth_header()\n\n    # Test empty query\n    response = await client.post(\n        \"/retrieve/chunks\", json={\"query\": \"\", \"k\": 1}, headers=headers  # Empty query\n    )\n    assert response.status_code == 422\n\n    # Test invalid k\n    response = await client.post(\n        \"/retrieve/docs\", json={\"query\": \"test\", \"k\": -1}, headers=headers  # Invalid k\n    )\n    assert response.status_code == 422\n\n\n@pytest.mark.asyncio\nasync def test_invalid_completion_params(client: AsyncClient):\n    \"\"\"Test error handling for invalid completion parameters\"\"\"\n    headers = create_auth_header()\n\n    # Test empty query\n    response = await client.post(\n        \"/query\",\n        json={\"query\": \"\", \"temperature\": 2.0},  # Empty query  # Invalid temperature\n        headers=headers,\n    )\n    assert response.status_code == 422\n\n\n@pytest.mark.asyncio\nasync def test_retrieve_chunks_default_reranking(client: AsyncClient):\n    \"\"\"Test retrieving chunks with default reranking behavior\"\"\"\n    # First ingest some test documents\n    _ = await test_ingest_text_document(\n        client, \"The quick brown fox jumps over the lazy dog. This is a test document.\"\n    )\n    _ = await test_ingest_text_document(\n        client, \"The lazy dog sleeps while the quick brown fox runs. Another test document.\"\n    )\n\n    headers = create_auth_header()\n    response = await client.post(\n        \"/retrieve/chunks\",\n        json={\n            \"query\": \"What does the fox do?\",\n            \"k\": 2,\n            \"min_score\": 0.0,\n            # Not specifying use_reranking - should use default from config\n            \"use_colpali\": True,\n        },\n        headers=headers,\n    )\n\n    assert response.status_code == 200\n    chunks = response.json()\n    assert len(chunks) > 0\n    # Verify chunks are ordered by score\n    scores = [chunk[\"score\"] for chunk in chunks]\n    assert all(scores[i] >= scores[i + 1] for i in range(len(scores) - 1))\n\n\n@pytest.mark.asyncio\nasync def test_retrieve_chunks_explicit_reranking(client: AsyncClient):\n    \"\"\"Test retrieving chunks with explicitly enabled reranking\"\"\"\n    # First ingest some test documents\n    _ = await test_ingest_text_document(\n        client, \"The quick brown fox jumps over the lazy dog. This is a test document.\"\n    )\n    _ = await test_ingest_text_document(\n        client, \"The lazy dog sleeps while the quick brown fox runs. Another test document.\"\n    )\n\n    headers = create_auth_header()\n    response = await client.post(\n        \"/retrieve/chunks\",\n        json={\n            \"query\": \"What does the fox do?\",\n            \"k\": 2,\n            \"min_score\": 0.0,\n            \"use_reranking\": True,\n            \"use_colpali\": True,\n        },\n        headers=headers,\n    )\n\n    assert response.status_code == 200\n    chunks = response.json()\n    assert len(chunks) > 0\n    # Verify chunks are ordered by score\n    scores = [chunk[\"score\"] for chunk in chunks]\n    assert all(scores[i] >= scores[i + 1] for i in range(len(scores) - 1))\n\n\n@pytest.mark.asyncio\nasync def test_retrieve_chunks_disabled_reranking(client: AsyncClient):\n    \"\"\"Test retrieving chunks with explicitly disabled reranking\"\"\"\n    # First ingest some test documents\n    await test_ingest_text_document(\n        client, \"The quick brown fox jumps over the lazy dog. This is a test document.\"\n    )\n    await test_ingest_text_document(\n        client, \"The lazy dog sleeps while the quick brown fox runs. Another test document.\"\n    )\n\n    headers = create_auth_header()\n    response = await client.post(\n        \"/retrieve/chunks\",\n        json={\n            \"query\": \"What does the fox do?\",\n            \"k\": 2,\n            \"min_score\": 0.0,\n            \"use_reranking\": False,\n            \"use_colpali\": True,\n        },\n        headers=headers,\n    )\n\n    assert response.status_code == 200\n    chunks = response.json()\n    assert len(chunks) > 0\n\n\n@pytest.mark.asyncio\nasync def test_reranking_affects_results(client: AsyncClient):\n    \"\"\"Test that reranking actually changes the order of results\"\"\"\n    # First ingest documents with clearly different semantic relevance\n    await test_ingest_text_document(\n        client, \"The capital of France is Paris. The city is known for the Eiffel Tower.\"\n    )\n    await test_ingest_text_document(\n        client, \"Paris is a city in France. It has many famous landmarks and museums.\"\n    )\n    await test_ingest_text_document(\n        client, \"Paris Hilton is a celebrity and businesswoman. She has nothing to do with France.\"\n    )\n\n    headers = create_auth_header()\n\n    # Get results without reranking\n    response_no_rerank = await client.post(\n        \"/retrieve/chunks\",\n        json={\n            \"query\": \"Tell me about the capital city of France\",\n            \"k\": 3,\n            \"min_score\": 0.0,\n            \"use_reranking\": False,\n            \"use_colpali\": True,\n        },\n        headers=headers,\n    )\n\n    # Get results with reranking\n    response_with_rerank = await client.post(\n        \"/retrieve/chunks\",\n        json={\n            \"query\": \"Tell me about the capital city of France\",\n            \"k\": 3,\n            \"min_score\": 0.0,\n            \"use_reranking\": True,\n            \"use_colpali\": True,\n        },\n        headers=headers,\n    )\n\n    assert response_no_rerank.status_code == 200\n    assert response_with_rerank.status_code == 200\n\n    chunks_no_rerank = response_no_rerank.json()\n    chunks_with_rerank = response_with_rerank.json()\n\n    # Verify we got results in both cases\n    assert len(chunks_no_rerank) > 0\n    assert len(chunks_with_rerank) > 0\n\n    # The order or scores should be different between reranked and non-reranked results\n    # This test might be a bit flaky depending on the exact scoring, but it should work most of the time\n    # given our carefully crafted test data\n    scores_no_rerank = [c[\"score\"] for c in chunks_no_rerank]\n    scores_with_rerank = [c[\"score\"] for c in chunks_with_rerank]\n    assert scores_no_rerank != scores_with_rerank, \"Reranking should affect the scores\"\n\n\n@pytest.mark.asyncio\nasync def test_retrieve_docs_with_reranking(client: AsyncClient):\n    \"\"\"Test document retrieval with reranking options\"\"\"\n    # First ingest documents with clearly different semantic relevance\n    await test_ingest_text_document(\n        client, \"The capital of France is Paris. The city is known for the Eiffel Tower.\"\n    )\n    await test_ingest_text_document(\n        client, \"Paris is a city in France. It has many famous landmarks and museums.\"\n    )\n    await test_ingest_text_document(\n        client, \"Paris Hilton is a celebrity and businesswoman. She has nothing to do with France.\"\n    )\n\n    headers = create_auth_header()\n\n    # Test with default reranking (from config)\n    response_default = await client.post(\n        \"/retrieve/docs\",\n        json={\n            \"query\": \"Tell me about the capital city of France\",\n            \"k\": 3,\n            \"min_score\": 0.0,\n        },\n        headers=headers,\n    )\n    assert response_default.status_code == 200\n    docs_default = response_default.json()\n    assert len(docs_default) > 0\n\n    # Test with explicit reranking enabled\n    response_rerank = await client.post(\n        \"/retrieve/docs\",\n        json={\n            \"query\": \"Tell me about the capital city of France\",\n            \"k\": 3,\n            \"min_score\": 0.0,\n            \"use_reranking\": True,\n        },\n        headers=headers,\n    )\n    assert response_rerank.status_code == 200\n    docs_rerank = response_rerank.json()\n    assert len(docs_rerank) > 0\n\n    # Test with reranking disabled\n    response_no_rerank = await client.post(\n        \"/retrieve/docs\",\n        json={\n            \"query\": \"Tell me about the capital city of France\",\n            \"k\": 3,\n            \"min_score\": 0.0,\n            \"use_reranking\": False,\n        },\n        headers=headers,\n    )\n    assert response_no_rerank.status_code == 200\n    docs_no_rerank = response_no_rerank.json()\n    assert len(docs_no_rerank) > 0\n\n    # Verify that reranking affects the order\n    scores_rerank = [doc[\"score\"] for doc in docs_rerank]\n    scores_no_rerank = [doc[\"score\"] for doc in docs_no_rerank]\n    assert scores_rerank != scores_no_rerank, \"Reranking should affect document scores\"\n\n\n@pytest.mark.asyncio\nasync def test_query_with_reranking(client: AsyncClient):\n    \"\"\"Test query completion with reranking options\"\"\"\n    # First ingest documents with clearly different semantic relevance\n    await test_ingest_text_document(\n        client, \"The capital of France is Paris. The city is known for the Eiffel Tower.\"\n    )\n    await test_ingest_text_document(\n        client, \"Paris is a city in France. It has many famous landmarks and museums.\"\n    )\n    await test_ingest_text_document(\n        client, \"Paris Hilton is a celebrity and businesswoman. She has nothing to do with France.\"\n    )\n\n    headers = create_auth_header()\n\n    # Test with default reranking (from config)\n    response_default = await client.post(\n        \"/query\",\n        json={\n            \"query\": \"What is the capital of France?\",\n            \"k\": 3,\n            \"min_score\": 0.0,\n            \"max_tokens\": 50,\n        },\n        headers=headers,\n    )\n    assert response_default.status_code == 200\n    completion_default = response_default.json()\n    assert \"completion\" in completion_default\n\n    # Test with explicit reranking enabled\n    response_rerank = await client.post(\n        \"/query\",\n        json={\n            \"query\": \"What is the capital of France?\",\n            \"k\": 3,\n            \"min_score\": 0.0,\n            \"max_tokens\": 50,\n            \"use_reranking\": True,\n        },\n        headers=headers,\n    )\n    assert response_rerank.status_code == 200\n    completion_rerank = response_rerank.json()\n    assert \"completion\" in completion_rerank\n\n    # Test with reranking disabled\n    response_no_rerank = await client.post(\n        \"/query\",\n        json={\n            \"query\": \"What is the capital of France?\",\n            \"k\": 3,\n            \"min_score\": 0.0,\n            \"max_tokens\": 50,\n            \"use_reranking\": False,\n        },\n        headers=headers,\n    )\n    assert response_no_rerank.status_code == 200\n    completion_no_rerank = response_no_rerank.json()\n    assert \"completion\" in completion_no_rerank\n\n    # The actual responses might be different due to different chunk ordering,\n    # but all should mention Paris as the capital\n    assert \"Paris\" in completion_default[\"completion\"]\n    assert \"Paris\" in completion_rerank[\"completion\"]\n    assert \"Paris\" in completion_no_rerank[\"completion\"]\n\n\n# Knowledge Graph Tests\n\n@pytest.fixture(scope=\"function\", autouse=True)\nasync def cleanup_graphs():\n    \"\"\"Clean up graphs before each graph test\"\"\"\n    # Create a fresh connection to the test database\n    engine = create_async_engine(TEST_POSTGRES_URI)\n    try:\n        async with engine.begin() as conn:\n            # Delete all rows from the graphs table\n            await conn.execute(text(\"DELETE FROM graphs\"))\n            logger.info(\"Cleaned up all graph-related tables\")\n    except Exception as e:\n        logger.error(f\"Failed to clean up graph tables: {e}\")\n        raise\n    finally:\n        await engine.dispose()\n        \n    # This will run before each test function\n    yield\n    # Test runs here\n\n\n@pytest.mark.asyncio\nasync def test_create_graph(client: AsyncClient):\n    \"\"\"Test creating a knowledge graph from documents.\"\"\"\n    # First ingest multiple documents with related content to extract entities and relationships\n    doc_id1 = await test_ingest_text_document(\n        client,\n        content=\"Apple Inc. is a technology company headquartered in Cupertino, California. \"\n        \"Tim Cook is the CEO of Apple. Steve Jobs was the co-founder of Apple.\"\n    )\n\n    doc_id2 = await test_ingest_text_document(\n        client,\n        content=\"Microsoft is a technology company based in Redmond, Washington. \"\n        \"Satya Nadella is the CEO of Microsoft. Bill Gates co-founded Microsoft.\"\n    )\n\n    doc_id3 = await test_ingest_text_document(\n        client,\n        content=\"Tim Cook succeeded Steve Jobs as the CEO of Apple in 2011. \"\n        \"Under Tim Cook's leadership, Apple became the world's first trillion-dollar company.\"\n    )\n\n    headers = create_auth_header()\n    graph_name = \"test_tech_companies_graph\"\n\n    # Create graph using the document IDs\n    response = await client.post(\n        \"/graph/create\",\n        json={\n            \"name\": graph_name,\n            \"documents\": [doc_id1, doc_id2, doc_id3]\n        },\n        headers=headers,\n    )\n\n    assert response.status_code == 200\n    graph = response.json()\n\n    # Verify graph structure\n    assert graph[\"name\"] == graph_name\n    assert len(graph[\"document_ids\"]) == 3\n    assert all(doc_id in graph[\"document_ids\"] for doc_id in [doc_id1, doc_id2, doc_id3])\n    assert len(graph[\"entities\"]) > 0  # Should extract entities like Apple, Microsoft, Tim Cook, etc.\n    assert len(graph[\"relationships\"]) > 0  # Should extract relationships like \"Tim Cook is CEO of Apple\"\n\n    # Verify specific expected entities were extracted\n    entity_labels = [entity[\"label\"] for entity in graph[\"entities\"]]\n    assert any(\"Apple\" in label for label in entity_labels)\n    assert any(\"Microsoft\" in label for label in entity_labels)\n    assert any(\"Tim Cook\" in label for label in entity_labels)\n\n    # Verify entity types\n    entity_types = set(entity[\"type\"] for entity in graph[\"entities\"])\n    assert \"ORGANIZATION\" in entity_types or \"COMPANY\" in entity_types\n    assert \"PERSON\" in entity_types\n\n    # Verify entities have document references\n    for entity in graph[\"entities\"]:\n        assert len(entity[\"document_ids\"]) > 0\n        assert entity[\"document_ids\"][0] in [doc_id1, doc_id2, doc_id3]\n\n    return graph_name, [doc_id1, doc_id2, doc_id3]\n\n\n@pytest.mark.asyncio\nasync def test_get_graph(client: AsyncClient):\n    \"\"\"Test retrieving a knowledge graph by name.\"\"\"\n    # First create a graph\n    graph_name, _ = await test_create_graph(client)\n\n    # Then retrieve it\n    headers = create_auth_header()\n    response = await client.get(\n        f\"/graph/{graph_name}\",\n        headers=headers,\n    )\n\n    assert response.status_code == 200\n    graph = response.json()\n\n    # Verify correct graph was retrieved\n    assert graph[\"name\"] == graph_name\n    assert len(graph[\"entities\"]) > 0\n    assert len(graph[\"relationships\"]) > 0\n\n\n@pytest.mark.asyncio\nasync def test_list_graphs(client: AsyncClient):\n    \"\"\"Test listing all accessible graphs.\"\"\"\n    # First create a graph\n    graph_name, _ = await test_create_graph(client)\n\n    # List all graphs\n    headers = create_auth_header()\n    response = await client.get(\n        \"/graphs\",\n        headers=headers,\n    )\n\n    assert response.status_code == 200\n    graphs = response.json()\n\n    # Verify the created graph is in the list\n    assert len(graphs) > 0\n    assert any(graph[\"name\"] == graph_name for graph in graphs)\n\n\n@pytest.mark.asyncio\nasync def test_create_graph_with_filters(client: AsyncClient):\n    \"\"\"Test creating a knowledge graph using metadata filters.\"\"\"\n    # Ingest test documents with specific metadata\n    doc_id1 = await test_ingest_text_document(\n        client,\n        content=\"The solar system consists of the Sun and eight planets. \"\n        \"Earth is the third planet from the Sun. Mars is the fourth planet.\"\n    )\n\n    headers = create_auth_header()\n\n    # Get the document to add specific metadata\n    response = await client.get(f\"/documents/{doc_id1}\", headers=headers)\n    assert response.status_code == 200\n\n    # Update document with metadata\n    response = await client.post(\n        f\"/documents/{doc_id1}/update_metadata\",\n        json={\"category\": \"astronomy\", \"subject\": \"planets\"},\n        headers=headers,\n    )\n    assert response.status_code == 200\n\n    # Create graph with filters - using metadata field which is renamed to doc_metadata in PostgresDatabase\n    graph_name = \"test_astronomy_graph\"\n    response = await client.post(\n        \"/graph/create\",\n        json={\n            \"name\": graph_name,\n            \"filters\": {\"category\": \"astronomy\"}\n        },\n        headers=headers,\n    )\n\n    assert response.status_code == 200\n    graph = response.json()\n\n    # Verify graph was created with the right document\n    assert graph[\"name\"] == graph_name\n    assert doc_id1 in graph[\"document_ids\"]\n    assert len(graph[\"entities\"]) > 0  # Should extract entities like Sun, Earth, Mars\n\n    # Verify specific entities\n    entity_labels = [entity[\"label\"] for entity in graph[\"entities\"]]\n    assert any(label == \"Sun\" or \"Sun\" in label for label in entity_labels)\n    assert any(label == \"Earth\" or \"Earth\" in label for label in entity_labels)\n\n    return graph_name, doc_id1\n\n\n@pytest.mark.asyncio\nasync def test_query_with_graph(client: AsyncClient):\n    \"\"\"Test query completion with knowledge graph enhancement.\"\"\"\n    # First create a graph and get its name\n    graph_name, doc_ids = await test_create_graph(client)\n\n    # Additional document that won't be in the graph but contains related information\n    _ = await test_ingest_text_document(\n        client,\n        content=\"Apple has released a new iPhone model. The company's focus on innovation continues.\"\n    )\n    \n    headers = create_auth_header()\n    \n    # Query using the graph\n    response = await client.post(\n        \"/query\",\n        json={\n            \"query\": \"Who is the CEO of Apple?\",\n            \"graph_name\": graph_name,\n            \"hop_depth\": 2,\n            \"include_paths\": True\n        },\n        headers=headers,\n    )\n    \n    assert response.status_code == 200\n    result = response.json()\n    \n    # Verify the completion contains relevant information from graph\n    assert \"completion\" in result\n    assert any(term in result[\"completion\"] for term in [\"Tim Cook\", \"Cook\", \"CEO\", \"Apple\"])\n    \n    # Verify we have graph metadata when include_paths=True\n    assert \"metadata\" in result, \"Expected metadata in response when include_paths=True\"\n    assert \"graph\" in result[\"metadata\"], \"Expected graph metadata in response\"\n    assert result[\"metadata\"][\"graph\"][\"name\"] == graph_name\n    \n    # Verify relevant entities includes expected entities\n    assert \"relevant_entities\" in result[\"metadata\"][\"graph\"]\n    relevant_entities = result[\"metadata\"][\"graph\"][\"relevant_entities\"]\n    \n    # At least one relevant entity should contain either Tim Cook or Apple\n    has_tim_cook = any(\"Tim Cook\" in entity or \"Cook\" in entity for entity in relevant_entities)\n    has_apple = any(\"Apple\" in entity for entity in relevant_entities)\n    assert has_tim_cook or has_apple, \"Expected relevant entities to include Tim Cook or Apple\"\n    \n    # Now try without the graph for comparison\n    response_no_graph = await client.post(\n        \"/query\",\n        json={\n            \"query\": \"Who is the CEO of Apple?\",\n        },\n        headers=headers,\n    )\n    \n    assert response_no_graph.status_code == 200\n\n\n@pytest.mark.asyncio\nasync def test_batch_ingest_with_shared_metadata(\n    client: AsyncClient\n):\n    \"\"\"Test batch ingestion with shared metadata for all files.\"\"\"\n    headers = create_auth_header()\n    # Create test files\n    files = [\n        (\"files\", (\"test1.txt\", b\"Test content 1\")),\n        (\"files\", (\"test2.txt\", b\"Test content 2\")),\n    ]\n    \n    # Shared metadata for all files\n    metadata = {\"category\": \"test\", \"batch\": \"shared\"}\n    \n    response = await client.post(\n        \"/ingest/files\",\n        files=files,\n        data={\n            \"metadata\": json.dumps(metadata),\n            \"rules\": json.dumps([]),\n            \"use_colpali\": \"true\",\n            \"parallel\": \"true\",\n        },\n        headers=headers,\n    )\n    \n    assert response.status_code == 200\n    result = response.json()\n    assert len(result[\"documents\"]) == 2\n    assert len(result[\"errors\"]) == 0\n    \n    # Verify all documents got the same metadata\n    for doc in result[\"documents\"]:\n        assert doc[\"metadata\"][\"category\"] == \"test\"\n        assert doc[\"metadata\"][\"batch\"] == \"shared\"\n\n\n@pytest.mark.asyncio\nasync def test_batch_ingest_with_individual_metadata(\n    client: AsyncClient\n):\n    \"\"\"Test batch ingestion with individual metadata per file.\"\"\"\n    headers = create_auth_header()\n    # Create test files\n    files = [\n        (\"files\", (\"test1.txt\", b\"Test content 1\")),\n        (\"files\", (\"test2.txt\", b\"Test content 2\")),\n    ]\n    \n    # Individual metadata\n    metadata = [\n        {\"category\": \"test1\", \"batch\": \"individual\"},\n        {\"category\": \"test2\", \"batch\": \"individual\"},\n    ]\n    \n    response = await client.post(\n        \"/ingest/files\",\n        files=files,\n        data={\n            \"metadata\": json.dumps(metadata),\n            \"rules\": json.dumps([]),\n            \"use_colpali\": \"true\",\n            \"parallel\": \"true\",\n        },\n        headers=headers,\n    )\n    \n    assert response.status_code == 200\n    result = response.json()\n    assert len(result[\"documents\"]) == 2\n    assert len(result[\"errors\"]) == 0\n    \n    # Verify each document got its correct metadata\n    assert result[\"documents\"][0][\"metadata\"][\"category\"] == \"test1\"\n    assert result[\"documents\"][1][\"metadata\"][\"category\"] == \"test2\"\n\n\n@pytest.mark.asyncio\nasync def test_batch_ingest_metadata_validation(\n    client: AsyncClient\n):\n    \"\"\"Test validation when metadata list length doesn't match files.\"\"\"\n    headers = create_auth_header()\n    files = [\n        (\"files\", (\"test1.txt\", b\"Test content 1\")),\n        (\"files\", (\"test2.txt\", b\"Test content 2\")),\n    ]\n    \n    # Metadata list with wrong length\n    metadata = [\n        {\"category\": \"test1\"},\n        {\"category\": \"test2\"},\n        {\"category\": \"test3\"},  # Extra metadata\n    ]\n    \n    response = await client.post(\n        \"/ingest/files\",\n        files=files,\n        data={\n            \"metadata\": json.dumps(metadata),\n            \"rules\": json.dumps([]),\n            \"use_colpali\": \"true\",\n            \"parallel\": \"true\",\n        },\n        headers=headers,\n    )\n    \n    assert response.status_code == 400\n    assert \"must match number of files\" in response.json()[\"detail\"]\n\n\n@pytest.mark.asyncio\nasync def test_batch_ingest_sequential(\n    client: AsyncClient\n):\n    \"\"\"Test sequential batch ingestion.\"\"\"\n    headers = create_auth_header()\n    files = [\n        (\"files\", (\"test1.txt\", b\"Test content 1\")),\n        (\"files\", (\"test2.txt\", b\"Test content 2\")),\n    ]\n    \n    metadata = {\"category\": \"test\"}\n    \n    response = await client.post(\n        \"/ingest/files\",\n        files=files,\n        data={\n            \"metadata\": json.dumps(metadata),\n            \"rules\": json.dumps([]),\n            \"use_colpali\": \"true\",\n            \"parallel\": \"false\",  # Process sequentially\n        },\n        headers=headers,\n    )\n    \n    assert response.status_code == 200\n    result = response.json()\n    assert len(result[\"documents\"]) == 2\n    assert len(result[\"errors\"]) == 0\n\n\n@pytest.mark.asyncio\nasync def test_batch_ingest_with_rules(\n    client: AsyncClient\n):\n    \"\"\"Test batch ingestion with rules applied.\"\"\"\n    headers = create_auth_header()\n    files = [\n        (\"files\", (\"test1.txt\", b\"Test content 1\")),\n        (\"files\", (\"test2.txt\", b\"Test content 2\")),\n    ]\n    \n    # Test shared rules for all files\n    shared_rules = [{\"type\": \"natural_language\", \"prompt\": \"Extract keywords\"}]\n    \n    response = await client.post(\n        \"/ingest/files\",\n        files=files,\n        data={\n            \"metadata\": json.dumps({}),\n            \"rules\": json.dumps(shared_rules),\n            \"use_colpali\": \"true\",\n            \"parallel\": \"true\",\n        },\n        headers=headers,\n    )\n    \n    assert response.status_code == 200\n    result = response.json()\n    assert len(result[\"documents\"]) == 2\n    assert len(result[\"errors\"]) == 0\n    \n    # Test per-file rules\n    per_file_rules = [\n        [{\"type\": \"natural_language\", \"prompt\": \"Extract keywords\"}],  # Rules for first file\n        [{\"type\": \"metadata_extraction\", \"schema\": {\"title\": \"string\"}}],  # Rules for second file\n    ]\n    \n    response = await client.post(\n        \"/ingest/files\",\n        files=files,\n        data={\n            \"metadata\": json.dumps({}),\n            \"rules\": json.dumps(per_file_rules),\n            \"use_colpali\": \"true\",\n            \"parallel\": \"true\",\n        },\n        headers=headers,\n    )\n    \n    assert response.status_code == 200\n    result = response.json()\n    assert len(result[\"documents\"]) == 2\n    assert len(result[\"errors\"]) == 0\n\n\n@pytest.mark.asyncio\nasync def test_batch_ingest_rules_validation(\n    client: AsyncClient\n):\n    \"\"\"Test validation of rules format and length.\"\"\"\n    headers = create_auth_header()\n    files = [\n        (\"files\", (\"test1.txt\", b\"Test content 1\")),\n        (\"files\", (\"test2.txt\", b\"Test content 2\")),\n    ]\n    \n    # Test invalid rules format\n    invalid_rules = \"not a list\"\n    \n    response = await client.post(\n        \"/ingest/files\",\n        files=files,\n        data={\n            \"metadata\": json.dumps({}),\n            \"rules\": invalid_rules,\n            \"use_colpali\": \"true\",\n            \"parallel\": \"true\",\n        },\n        headers=headers,\n    )\n    \n    assert response.status_code == 400\n    assert \"Invalid JSON\" in response.json()[\"detail\"]\n    \n    # Test per-file rules with wrong length\n    per_file_rules = [\n        [{\"type\": \"natural_language\", \"prompt\": \"Extract keywords\"}],  # Only one set of rules\n    ]\n    \n    response = await client.post(\n        \"/ingest/files\",\n        files=files,\n        data={\n            \"metadata\": json.dumps({}),\n            \"rules\": json.dumps(per_file_rules),\n            \"use_colpali\": \"true\",\n            \"parallel\": \"true\",\n        },\n        headers=headers,\n    )\n    \n    assert response.status_code == 400\n    assert \"must match number of files\" in response.json()[\"detail\"]\n\n\n@pytest.mark.asyncio\nasync def test_batch_ingest_sequential_vs_parallel(\n    client: AsyncClient\n):\n    \"\"\"Test both sequential and parallel batch ingestion.\"\"\"\n    headers = create_auth_header()\n    files = [\n        (\"files\", (\"test1.txt\", b\"Test content 1\")),\n        (\"files\", (\"test2.txt\", b\"Test content 2\")),\n        (\"files\", (\"test3.txt\", b\"Test content 3\")),\n    ]\n    \n    # Test parallel processing\n    response = await client.post(\n        \"/ingest/files\",\n        files=files,\n        data={\n            \"metadata\": json.dumps({}),\n            \"rules\": json.dumps([]),\n            \"use_colpali\": \"true\",\n            \"parallel\": \"true\",\n        },\n        headers=headers,\n    )\n    \n    assert response.status_code == 200\n    result = response.json()\n    assert len(result[\"documents\"]) == 3\n    assert len(result[\"errors\"]) == 0\n    \n    # Test sequential processing \n    response = await client.post(\n        \"/ingest/files\",\n        files=files,\n        data={\n            \"metadata\": json.dumps({}),\n            \"rules\": json.dumps([]),\n            \"use_colpali\": \"true\",\n            \"parallel\": \"false\",\n        },\n        headers=headers,\n    )\n    \n    assert response.status_code == 200\n    result = response.json()\n    assert len(result[\"documents\"]) == 3\n    assert len(result[\"errors\"]) == 0\n\n\n@pytest.mark.asyncio\nasync def test_cross_document_query_with_graph(client: AsyncClient):\n    \"\"\"Test cross-document information retrieval using knowledge graph.\"\"\"\n    # Create a graph with multiple documents containing related information\n    doc_id1 = await test_ingest_text_document(\n        client,\n        content=\"Project Alpha was initiated in 2020. Jane Smith is the project lead.\"\n    )\n    \n    doc_id2 = await test_ingest_text_document(\n        client,\n        content=\"Jane Smith has a PhD in Computer Science from MIT. She has 10 years of experience in AI research.\"\n    )\n    \n    doc_id3 = await test_ingest_text_document(\n        client,\n        content=\"Project Alpha aims to develop advanced natural language processing models for medical applications.\"\n    )\n    \n    headers = create_auth_header()\n    graph_name = \"test_project_graph\"\n    \n    # Create graph using the document IDs\n    response = await client.post(\n        \"/graph/create\",\n        json={\n            \"name\": graph_name,\n            \"documents\": [doc_id1, doc_id2, doc_id3]\n        },\n        headers=headers,\n    )\n    \n    assert response.status_code == 200\n    \n    # Query that requires connecting information across documents\n    response = await client.post(\n        \"/query\",\n        json={\n            \"query\": \"What is Jane Smith's background and what project is she leading?\",\n            \"graph_name\": graph_name,\n            \"hop_depth\": 2,\n            \"include_paths\": True\n        },\n        headers=headers,\n    )\n    \n    assert response.status_code == 200\n    result = response.json()\n    \n    # Verify the completion combines information from multiple documents\n    assert \"Jane Smith\" in result[\"completion\"]\n    assert \"PhD\" in result[\"completion\"] or \"Computer Science\" in result[\"completion\"]\n    assert \"Project Alpha\" in result[\"completion\"]\n\n    # Compare with non-graph query\n    response_no_graph = await client.post(\n        \"/query\",\n        json={\n            \"query\": \"What is Jane Smith's background and what project is she leading?\",\n        },\n        headers=headers,\n    )\n\n    assert response_no_graph.status_code == 200\n"}
{"type": "test_file", "path": "core/tests/__init__.py", "content": ""}
{"type": "test_file", "path": "core/tests/integration/test_colpali_integrate_multivector.py", "content": "import pytest\nimport asyncio\nimport os\nimport logging\nfrom pathlib import Path\nfrom pdf2image import convert_from_path\n\nfrom core.embedding.colpali_embedding_model import ColpaliEmbeddingModel\nfrom core.vector_store.multi_vector_store import MultiVectorStore\nfrom core.models.chunk import DocumentChunk\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Test database URI\nTEST_DB_URI = \"postgresql://postgres:postgres@localhost:5432/test_db\"\n\n# Path to the test PDF file\nPDF_FILE_PATH = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))),\n    \"samples\",\n    \"Documents\",\n    \"62250266_origin.pdf\",\n)\n\n\n@pytest.fixture(scope=\"module\")\ndef pdf_path():\n    \"\"\"Fixture to provide the path to the test PDF file\"\"\"\n    pdf_path = Path(PDF_FILE_PATH)\n    if not pdf_path.exists():\n        pytest.skip(f\"Test PDF file not found at {pdf_path}\")\n    return pdf_path\n\n\n@pytest.fixture(scope=\"module\")\ndef embedding_model():\n    \"\"\"Create an instance of the ColpaliEmbeddingModel\"\"\"\n    return ColpaliEmbeddingModel()\n\n\n@pytest.fixture(scope=\"function\")\nasync def vector_store():\n    \"\"\"Create a MultiVectorStore instance connected to the test database\"\"\"\n    store = MultiVectorStore(uri=TEST_DB_URI)\n\n    try:\n        # Initialize the database\n        store.initialize()\n\n        # Clean up any existing data\n        store.conn.execute(\"TRUNCATE TABLE multi_vector_embeddings RESTART IDENTITY\")\n    except Exception as e:\n        logger.error(f\"Error setting up database: {e}\")\n        pytest.skip(f\"Database setup failed: {e}\")\n\n    yield store\n\n    # Clean up after tests\n    try:\n        store.conn.execute(\"TRUNCATE TABLE multi_vector_embeddings RESTART IDENTITY\")\n    except Exception as e:\n        logger.error(f\"Error cleaning up: {e}\")\n\n    # Close connection\n    store.close()\n\n\nasync def process_pdf_pages(pdf_path, embedding_model, vector_store):\n    \"\"\"Process PDF pages, generate embeddings, and store them in the vector store\"\"\"\n    # Convert PDF to images\n    logger.info(f\"Converting PDF to images: {pdf_path}\")\n    images = convert_from_path(pdf_path)\n    logger.info(f\"PDF converted to {len(images)} images\")\n\n    # Process each page\n    chunks = []\n    for i, image in enumerate(images):\n        # Create a document chunk for each page\n        page_number = i + 1\n        document_id = f\"{os.path.basename(pdf_path)}\"\n\n        # Generate embeddings using the Colpali model\n        logger.info(f\"Generating embeddings for page {page_number}\")\n        embeddings = embedding_model.generate_embeddings(image)\n\n        # Create a document chunk\n        chunk = DocumentChunk(\n            document_id=document_id,\n            chunk_number=page_number,\n            content=f\"Page {page_number} of {document_id}\",\n            embedding=embeddings,\n            metadata={\"page\": page_number, \"filename\": document_id, \"is_image\": True},\n        )\n        chunks.append(chunk)\n\n    # Store all chunks in the vector store\n    logger.info(f\"Storing {len(chunks)} chunks in the vector store\")\n    success, stored_ids = await vector_store.store_embeddings(chunks)\n\n    if not success:\n        logger.error(\"Failed to store embeddings\")\n        return False, []\n\n    logger.info(f\"Successfully stored {len(stored_ids)} chunks\")\n    return True, chunks\n\n\nasync def process_text_content(embedding_model, vector_store):\n    \"\"\"Process text content, generate embeddings, and store them in the vector store\"\"\"\n    logger.info(\"Processing text content\")\n\n    # Sample text content related to the PDF topic\n    text_samples = [\n        \"Image-reject-ratio is highest at 2.4 GHz frequency.\",\n        \"After applying time-domain IQ compensation, y(t) becomes a balanced signal with reduced distortion.\",\n        \"The low-complexity IQ compensator improves signal quality with minimal processing overhead.\",\n        \"Frequency-dependent IQ imbalance causes signal degradation across the spectrum.\",\n        \"Digital predistortion techniques can mitigate nonlinear distortion in RF transmitters.\",\n    ]\n\n    chunks = []\n    for i, text in enumerate(text_samples):\n        # Generate embeddings for text\n        logger.info(f\"Generating embeddings for text sample {i+1}\")\n        embeddings = embedding_model.generate_embeddings(text)\n\n        # Create a document chunk\n        chunk = DocumentChunk(\n            document_id=\"text_samples\",\n            chunk_number=i + 1,\n            content=text,\n            embedding=embeddings,\n            metadata={\"sample_id\": i + 1, \"content_type\": \"text\", \"is_image\": False},\n        )\n        chunks.append(chunk)\n\n    # Store text chunks in the vector store\n    logger.info(f\"Storing {len(chunks)} text chunks in the vector store\")\n    success, stored_ids = await vector_store.store_embeddings(chunks)\n\n    if not success:\n        logger.error(\"Failed to store text embeddings\")\n        return False, []\n\n    logger.info(f\"Successfully stored {len(stored_ids)} text chunks\")\n    return True, chunks\n\n\n@pytest.mark.asyncio\nasync def test_pdf_processing_and_storage(pdf_path, embedding_model, vector_store):\n    \"\"\"Test that PDF pages can be processed and stored correctly\"\"\"\n    # Process the PDF and store embeddings\n    success, chunks = await process_pdf_pages(pdf_path, embedding_model, vector_store)\n\n    # Verify storage was successful\n    assert success, \"Failed to process and store PDF pages\"\n    assert len(chunks) > 0, \"No chunks were created from the PDF\"\n\n    # Query to verify we can retrieve the stored chunks\n    query_embedding = embedding_model.generate_embeddings(\"Test query\")\n    results = await vector_store.query_similar(query_embedding, k=3)\n\n    # Verify we got results\n    assert len(results) > 0, \"No results returned from query\"\n    assert all(\n        isinstance(result, DocumentChunk) for result in results\n    ), \"Results are not DocumentChunks\"\n\n\n@pytest.mark.asyncio\nasync def test_text_processing_and_storage(embedding_model, vector_store):\n    \"\"\"Test that text content can be processed and stored correctly\"\"\"\n    # Process text and store embeddings\n    success, chunks = await process_text_content(embedding_model, vector_store)\n\n    # Verify storage was successful\n    assert success, \"Failed to process and store text content\"\n    assert len(chunks) > 0, \"No chunks were created from text content\"\n\n    # Query to verify we can retrieve the stored chunks\n    query_embedding = embedding_model.generate_embeddings(\"Signal quality improvement\")\n    results = await vector_store.query_similar(query_embedding, k=3)\n\n    # Verify we got results\n    assert len(results) > 0, \"No results returned from query\"\n    assert all(\n        isinstance(result, DocumentChunk) for result in results\n    ), \"Results are not DocumentChunks\"\n\n\n@pytest.mark.asyncio\nasync def test_mixed_content_queries(pdf_path, embedding_model, vector_store):\n    \"\"\"Test queries against both image and text content\"\"\"\n    # Process both PDF images and text content\n    pdf_success, pdf_chunks = await process_pdf_pages(pdf_path, embedding_model, vector_store)\n    text_success, text_chunks = await process_text_content(embedding_model, vector_store)\n\n    assert pdf_success and text_success, \"Failed to process and store content\"\n    assert len(pdf_chunks) > 0 and len(text_chunks) > 0, \"No chunks were created\"\n\n    # Test queries that should match both image and text content\n    test_queries = [\n        \"image-reject-ratio frequency\",\n        \"time-domain IQ compensation\",\n        \"signal quality improvement\",\n    ]\n\n    for query in test_queries:\n        logger.info(f\"Testing mixed content query: {query}\")\n\n        # Generate query embeddings\n        query_embedding = embedding_model.generate_embeddings(query)\n\n        # Query the vector store\n        results = await vector_store.query_similar(query_embedding, k=5)\n\n        # Verify we got results\n        assert len(results) > 0, f\"No results returned for query: {query}\"\n\n        # Check if we got both image and text results\n        has_image_results = any(result.metadata.get(\"is_image\", False) for result in results)\n        has_text_results = any(not result.metadata.get(\"is_image\", True) for result in results)\n\n        logger.info(f\"Query '{query}' returned image results: {has_image_results}\")\n        logger.info(f\"Query '{query}' returned text results: {has_text_results}\")\n\n        # Log top results\n        logger.info(\"Top results:\")\n        for i, result in enumerate(results[:3]):\n            content_type = \"Image\" if result.metadata.get(\"is_image\", False) else \"Text\"\n            logger.info(f\"  {i+1}. {content_type}: {result.content} (Score: {result.score:.4f})\")\n\n\n@pytest.mark.asyncio\nasync def test_specific_queries(pdf_path, embedding_model, vector_store):\n    \"\"\"Test specific queries against the PDF document\"\"\"\n    # Process the PDF and store embeddings\n    success, chunks = await process_pdf_pages(pdf_path, embedding_model, vector_store)\n    assert success, \"Failed to process and store PDF pages\"\n\n    # Test queries\n    test_queries = [\n        {\n            \"query\": \"At what frequency do we obtain the highest image-reject-ratio?\",\n            \"expected_page\": 6,\n        },\n        {\n            \"query\": \"What does y(t) becomes after applying time-domain IQ compensation?\",\n            \"expected_page\": 4,\n        },\n    ]\n\n    for test_case in test_queries:\n        query = test_case[\"query\"]\n        expected_page = test_case[\"expected_page\"]\n\n        logger.info(f\"Testing query: {query}\")\n\n        # Generate query embeddings\n        query_embedding = await embedding_model.embed_for_query(query)\n\n        # Query the vector store\n        results = await vector_store.query_similar(query_embedding, k=10)\n\n        # Verify results\n        assert len(results) > 0, f\"No results returned for query: {query}\"\n\n        # Check if the expected page is in the top results\n        top_pages = [result.metadata.get(\"page\") for result in results]\n        logger.info(f\"Top pages for query '{query}': {top_pages}\")\n\n        # Instead of requiring the expected page to be the top result,\n        # check if it's in the top N results (more flexible)\n        top_n = 3  # Check if expected page is in top 3 results\n        assert (\n            expected_page in top_pages[:top_n]\n        ), f\"Expected page {expected_page} to be in top {top_n} results, but got {top_pages[:top_n]}\"\n\n        # Log the rank of the expected page\n        expected_page_rank = (\n            top_pages.index(expected_page) + 1 if expected_page in top_pages else \"not found\"\n        )\n        logger.info(f\"Expected page {expected_page} rank: {expected_page_rank}\")\n\n\n@pytest.mark.asyncio\nasync def test_query_performance(pdf_path, embedding_model, vector_store):\n    \"\"\"Test query performance with the PDF document\"\"\"\n    # Process the PDF and store embeddings\n    success, chunks = await process_pdf_pages(pdf_path, embedding_model, vector_store)\n    assert success, \"Failed to process and store PDF pages\"\n\n    # Process text content\n    text_success, text_chunks = await process_text_content(embedding_model, vector_store)\n    assert text_success, \"Failed to process and store text content\"\n\n    # Prepare a list of test queries\n    test_queries = [\n        \"image-reject-ratio frequency\",\n        \"time-domain IQ compensation\",\n        \"low-complexity IQ compensator\",\n        \"frequency-dependent IQ imbalance\",\n        \"digital predistortion\",\n    ]\n\n    # Measure query performance\n    for query in test_queries:\n        logger.info(f\"Testing query performance for: {query}\")\n\n        # Generate query embeddings\n        start_time = asyncio.get_event_loop().time()\n        query_embedding = await embedding_model.embed_for_query(query)\n        embedding_time = asyncio.get_event_loop().time() - start_time\n\n        # Query the vector store\n        start_time = asyncio.get_event_loop().time()\n        results = await vector_store.query_similar(query_embedding, k=5)\n        query_time = asyncio.get_event_loop().time() - start_time\n\n        # Log performance metrics\n        logger.info(f\"Query: '{query}'\")\n        logger.info(f\"  Embedding generation time: {embedding_time:.4f} seconds\")\n        logger.info(f\"  Vector store query time: {query_time:.4f} seconds\")\n        logger.info(f\"  Total query time: {embedding_time + query_time:.4f} seconds\")\n\n        # Verify we got results\n        assert len(results) > 0, f\"No results returned for query: {query}\"\n\n        # Log top results\n        logger.info(\"  Top results:\")\n        for i, result in enumerate(results[:3]):\n            content_type = \"Image\" if result.metadata.get(\"is_image\", False) else \"Text\"\n            if content_type == \"Image\":\n                logger.info(\n                    f\"    {i+1}. {content_type} - Page {result.metadata.get('page')} (Score: {result.score:.4f})\"\n                )\n            else:\n                logger.info(\n                    f\"    {i+1}. {content_type} - {result.content[:50]}... (Score: {result.score:.4f})\"\n                )\n\n\n@pytest.mark.asyncio\nasync def test_query_variations_and_consistency(pdf_path, embedding_model, vector_store):\n    \"\"\"Test different query variations and ensure consistent results\"\"\"\n    # Process the PDF and store embeddings\n    success, chunks = await process_pdf_pages(pdf_path, embedding_model, vector_store)\n    assert success, \"Failed to process and store PDF pages\"\n\n    # Define sets of semantically similar queries\n    query_sets = [\n        # Set 1: Queries about image-reject-ratio\n        [\n            \"At what frequency do we obtain the highest image-reject-ratio?\",\n            \"What frequency gives the best image-reject-ratio?\",\n            \"Maximum image-reject-ratio frequency\",\n            \"Optimal frequency for image rejection\",\n        ],\n        # Set 2: Queries about IQ compensation\n        [\n            \"What does y(t) becomes after applying time-domain IQ compensation?\",\n            \"Result of time-domain IQ compensation on y(t)\",\n            \"Effect of IQ compensation on y(t)\",\n            \"y(t) after time-domain compensation\",\n        ],\n    ]\n\n    for i, query_set in enumerate(query_sets):\n        logger.info(f\"Testing query set {i+1}\")\n\n        # Store results for each query in the set\n        query_results = []\n\n        for query in query_set:\n            # Generate query embeddings\n            query_embedding = await embedding_model.embed_for_query(query)\n\n            # Query the vector store\n            results = await vector_store.query_similar(query_embedding, k=3)\n\n            # Store top result page\n            top_page = results[0].metadata.get(\"page\") if results else None\n            query_results.append(\n                {\n                    \"query\": query,\n                    \"top_page\": top_page,\n                    \"score\": results[0].score if results else 0,\n                    \"top_3_pages\": [r.metadata.get(\"page\") for r in results[:3]],\n                }\n            )\n\n        # Log results\n        logger.info(\"Results for semantically similar queries:\")\n        for result in query_results:\n            logger.info(f\"  Query: '{result['query']}'\")\n            logger.info(f\"    Top page: {result['top_page']} (Score: {result['score']:.4f})\")\n            logger.info(f\"    Top 3 pages: {result['top_3_pages']}\")\n\n        # Check consistency - the top page should be the same or similar for all queries in the set\n        top_pages = [result[\"top_page\"] for result in query_results]\n        most_common_page = max(set(top_pages), key=top_pages.count)\n\n        # Count how many queries return the most common page as the top result\n        matching_count = sum(1 for page in top_pages if page == most_common_page)\n        matching_percentage = (matching_count / len(top_pages)) * 100\n\n        logger.info(f\"Most common top page: {most_common_page}\")\n        logger.info(\n            f\"Percentage of queries with this page as top result: {matching_percentage:.1f}%\"\n        )\n\n        # Instead of asserting, just log the consistency level\n        if matching_percentage >= 50:\n            logger.info(\"Good consistency: At least half of the queries return the same top page\")\n        else:\n            logger.warning(\n                \"Low consistency: Less than half of the queries return the same top page\"\n            )\n\n        # Check if the most common page appears in the top 3 for each query\n        top3_consistency = []\n        for result in query_results:\n            in_top3 = most_common_page in result[\"top_3_pages\"]\n            top3_consistency.append(in_top3)\n\n            if not in_top3:\n                logger.warning(\n                    f\"Most common page {most_common_page} not in top 3 for query '{result['query']}'\"\n                )\n\n        # Calculate the percentage of queries that have the most common page in their top 3\n        top3_percentage = (sum(1 for x in top3_consistency if x) / len(top3_consistency)) * 100\n        logger.info(f\"Percentage of queries with most common page in top 3: {top3_percentage:.1f}%\")\n\n        # Log overall consistency assessment\n        if top3_percentage >= 75:\n            logger.info(\"Excellent top-3 consistency\")\n        elif top3_percentage >= 50:\n            logger.info(\"Good top-3 consistency\")\n        else:\n            logger.warning(\"Poor top-3 consistency\")\n\n\n@pytest.mark.asyncio\nasync def test_image_variations_robustness(pdf_path, embedding_model, vector_store):\n    \"\"\"Test robustness of the integration with image variations\"\"\"\n    # Convert PDF to images\n    logger.info(f\"Converting PDF to images: {pdf_path}\")\n    original_images = convert_from_path(pdf_path)\n    logger.info(f\"PDF converted to {len(original_images)} images\")\n\n    # Process original images first\n    await vector_store.initialize()\n    vector_store.conn.execute(\"TRUNCATE TABLE multi_vector_embeddings RESTART IDENTITY\")\n\n    # Store original images\n    original_chunks = []\n    for i, image in enumerate(original_images):\n        page_number = i + 1\n        document_id = f\"{os.path.basename(pdf_path)}_original\"\n\n        # Generate embeddings\n        embeddings = embedding_model.generate_embeddings(image)\n\n        # Create document chunk\n        chunk = DocumentChunk(\n            document_id=document_id,\n            chunk_number=page_number,\n            content=f\"Original Page {page_number}\",\n            embedding=embeddings,\n            metadata={\n                \"page\": page_number,\n                \"filename\": document_id,\n                \"variation\": \"original\",\n                \"is_image\": True,\n            },\n        )\n        original_chunks.append(chunk)\n\n    # Store original chunks\n    await vector_store.store_embeddings(original_chunks)\n\n    # Create image variations and store them\n    variation_chunks = []\n\n    # We'll focus on pages that are relevant to our test queries (pages 4 and 6)\n    target_pages = [4, 6]\n\n    for page_idx in [p - 1 for p in target_pages if p <= len(original_images)]:\n        original_image = original_images[page_idx]\n        page_number = page_idx + 1\n\n        # Create variations\n        variations = [\n            # Rotation by 90 degrees\n            (\"rotated_90\", original_image.rotate(90)),\n            # Rotation by 180 degrees\n            (\"rotated_180\", original_image.rotate(180)),\n            # Crop 10% from each side\n            (\n                \"cropped\",\n                original_image.crop(\n                    (\n                        original_image.width * 0.1,\n                        original_image.height * 0.1,\n                        original_image.width * 0.9,\n                        original_image.height * 0.9,\n                    )\n                ),\n            ),\n            # Resize to 75% of original\n            (\n                \"resized\",\n                original_image.resize(\n                    (int(original_image.width * 0.75), int(original_image.height * 0.75))\n                ),\n            ),\n        ]\n\n        # Process each variation\n        for variation_name, varied_image in variations:\n            document_id = f\"{os.path.basename(pdf_path)}_{variation_name}\"\n\n            # Generate embeddings\n            embeddings = embedding_model.generate_embeddings(varied_image)\n\n            # Create document chunk\n            chunk = DocumentChunk(\n                document_id=document_id,\n                chunk_number=page_number,\n                content=f\"{variation_name.capitalize()} Page {page_number}\",\n                embedding=embeddings,\n                metadata={\n                    \"page\": page_number,\n                    \"filename\": document_id,\n                    \"variation\": variation_name,\n                    \"original_page\": page_number,\n                    \"is_image\": True,\n                },\n            )\n            variation_chunks.append(chunk)\n\n    # Store variation chunks\n    await vector_store.store_embeddings(variation_chunks)\n\n    # Add text descriptions of the same content\n    text_descriptions = [\n        {\n            \"page\": 4,\n            \"text\": \"Detailed explanation of time-domain IQ compensation and its effect on y(t)\",\n        },\n        {\n            \"page\": 6,\n            \"text\": \"Analysis of frequency response showing highest image-reject-ratio at specific frequency\",\n        },\n    ]\n\n    text_chunks = []\n    for desc in text_descriptions:\n        # Generate embeddings for text\n        embeddings = embedding_model.generate_embeddings(desc[\"text\"])\n\n        # Create document chunk\n        chunk = DocumentChunk(\n            document_id=\"text_description\",\n            chunk_number=desc[\"page\"],\n            content=desc[\"text\"],\n            embedding=embeddings,\n            metadata={\n                \"page\": desc[\"page\"],\n                \"content_type\": \"text_description\",\n                \"related_to_page\": desc[\"page\"],\n                \"is_image\": False,\n            },\n        )\n        text_chunks.append(chunk)\n\n    # Store text chunks\n    await vector_store.store_embeddings(text_chunks)\n\n    # Test queries\n    test_queries = [\n        {\n            \"query\": \"At what frequency do we obtain the highest image-reject-ratio?\",\n            \"expected_page\": 6,\n        },\n        {\n            \"query\": \"What does y(t) becomes after applying time-domain IQ compensation?\",\n            \"expected_page\": 4,\n        },\n    ]\n\n    # Run queries and check if variations of the expected pages are in top results\n    for test_case in test_queries:\n        query = test_case[\"query\"]\n        expected_page = test_case[\"expected_page\"]\n\n        logger.info(f\"Testing query with image variations: {query}\")\n\n        # Generate query embeddings\n        query_embedding = await embedding_model.embed_for_query(query)\n\n        # Query the vector store\n        results = await vector_store.query_similar(query_embedding, k=10)\n\n        # Verify results\n        assert len(results) > 0, f\"No results returned for query: {query}\"\n\n        # Extract metadata from results\n        result_metadata = [\n            {\n                \"page\": r.metadata.get(\"page\"),\n                \"variation\": r.metadata.get(\"variation\"),\n                \"original_page\": r.metadata.get(\"original_page\"),\n                \"is_image\": r.metadata.get(\"is_image\", True),\n                \"score\": r.score,\n            }\n            for r in results\n        ]\n\n        # Log results\n        logger.info(f\"Results for query '{query}':\")\n        for i, meta in enumerate(result_metadata[:5]):\n            content_type = \"Image\" if meta[\"is_image\"] else \"Text\"\n            logger.info(\n                f\"  {i+1}. {content_type} - Page: {meta['page']}, \"\n                f\"Variation: {meta.get('variation', 'N/A')}, \"\n                f\"Score: {meta['score']:.4f}\"\n            )\n\n        # Check if the original expected page is in top results\n        # More flexible: check if it's in top 5 instead of requiring it to be in top 3\n        original_in_top = any(\n            r.metadata.get(\"page\") == expected_page\n            and r.metadata.get(\"variation\") == \"original\"\n            and r.metadata.get(\"is_image\", True)\n            for r in results[:5]\n        )\n\n        # Check if text description of the expected page is in top results\n        text_in_top = any(\n            (\n                r.metadata.get(\"page\") == expected_page\n                or r.metadata.get(\"related_to_page\") == expected_page\n            )\n            and not r.metadata.get(\"is_image\", True)\n            for r in results[:5]\n        )\n\n        # Log whether the original page and text are in top results\n        if original_in_top:\n            logger.info(f\"Original image of page {expected_page} found in top 5 results\")\n        else:\n            logger.warning(f\"Original image of page {expected_page} not found in top 5 results\")\n\n        if text_in_top:\n            logger.info(f\"Text description related to page {expected_page} found in top 5 results\")\n        else:\n            logger.warning(\n                f\"Text description related to page {expected_page} not found in top 5 results\"\n            )\n\n        # Check if variations of the expected page are also in top results\n        variations_of_expected = [\n            r\n            for r in results[:10]\n            if r.metadata.get(\"original_page\") == expected_page\n            and r.metadata.get(\"variation\") != \"original\"\n            and r.metadata.get(\"is_image\", True)\n        ]\n\n        # Log the number of variations found\n        logger.info(\n            f\"Found {len(variations_of_expected)} variations of page {expected_page} in top 10 results\"\n        )\n\n        # We should find at least one variation of the expected page in top results\n        # Make this a warning instead of an assertion\n        if len(variations_of_expected) == 0:\n            logger.warning(f\"No variations of page {expected_page} found in top 10 results\")\n\n        # Log variations found\n        if variations_of_expected:\n            logger.info(f\"Variations of page {expected_page} found in results:\")\n            for i, var in enumerate(variations_of_expected):\n                logger.info(\n                    f\"  {i+1}. Variation: {var.metadata.get('variation')}, Score: {var.score:.4f}\"\n                )\n\n        # Check consistency between original and variations\n        # The difference in ranking between original and best variation should not be too large\n        original_rank = next(\n            (\n                i\n                for i, r in enumerate(results)\n                if r.metadata.get(\"page\") == expected_page\n                and r.metadata.get(\"variation\") == \"original\"\n                and r.metadata.get(\"is_image\", True)\n            ),\n            -1,\n        )\n\n        best_variation_rank = next(\n            (\n                i\n                for i, r in enumerate(results)\n                if r.metadata.get(\"original_page\") == expected_page\n                and r.metadata.get(\"variation\") != \"original\"\n                and r.metadata.get(\"is_image\", True)\n            ),\n            -1,\n        )\n\n        text_rank = next(\n            (\n                i\n                for i, r in enumerate(results)\n                if (\n                    r.metadata.get(\"page\") == expected_page\n                    or r.metadata.get(\"related_to_page\") == expected_page\n                )\n                and not r.metadata.get(\"is_image\", True)\n            ),\n            -1,\n        )\n\n        if original_rank >= 0 and best_variation_rank >= 0:\n            rank_difference = abs(original_rank - best_variation_rank)\n            logger.info(f\"Rank difference between original and best variation: {rank_difference}\")\n\n            # Log instead of assert for more flexibility\n            if rank_difference > 5:\n                logger.warning(\n                    f\"Large rank difference between original (rank {original_rank+1}) and \"\n                    f\"best variation (rank {best_variation_rank+1})\"\n                )\n\n        if original_rank >= 0 and text_rank >= 0:\n            rank_difference = abs(original_rank - text_rank)\n            logger.info(\n                f\"Rank difference between original image and text description: {rank_difference}\"\n            )\n"}
{"type": "test_file", "path": "core/tests/unit/test_cache.py", "content": "import pytest\nfrom core.models.documents import Document\nfrom core.cache.llama_cache import LlamaCache\nfrom core.models.completion import CompletionResponse\n\n# TEST_MODEL = \"QuantFactory/Llama3.2-3B-Enigma-GGUF\"\nTEST_MODEL = \"QuantFactory/Dolphin3.0-Llama3.2-1B-GGUF\"\n# TEST_MODEL = \"unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF\"\nTEST_GGUF_FILE = \"*Q4_K_S.gguf\"\n# TEST_GGUF_FILE = \"*Q4_K_M.gguf\"\n\n\ndef get_test_document():\n    \"\"\"Load the example.txt file as a test document.\"\"\"\n    # test_file = Path(__file__).parent.parent / \"assets\" / \"example.txt\"\n    # with open(test_file, \"r\") as f:\n    #     content = f.read()\n\n    content = \"\"\"\n                In darkest hours of coding's fierce domain,\n                Where bugs lurk deep in shadows, hard to find,\n                Each error message brings fresh waves of pain,\n                As stack traces drive madness through the mind.\n\n                Through endless loops of print statements we wade,\n                Debug flags raised like torches in the night,\n                While segfaults mock each careful plan we made,\n                And race conditions laugh at our plight.\n\n                O rubber duck, my silent debugging friend,\n                Your plastic gaze holds wisdom yet untold,\n                As line by line we trace paths without end,\n                Seeking that elusive bug of gold.\n\n                Yet hope remains while coffee still flows strong,\n                Through debugging hell, we'll debug right from wrong.\n            \"\"\".strip()\n\n    return Document(\n        external_id=\"alice_ch1\",\n        owner={\"id\": \"test_user\", \"name\": \"Test User\"},\n        content_type=\"text/plain\",\n        system_metadata={\n            \"content\": content,\n            \"title\": \"Alice in Wonderland - Chapter 1\",\n            \"source\": \"test_document\",\n        },\n    )\n\n\n@pytest.fixture\ndef llama_cache():\n    \"\"\"Create a LlamaCache instance with the test document.\"\"\"\n    doc = get_test_document()\n    cache = LlamaCache(\n        name=\"test_cache\", model=TEST_MODEL, gguf_file=TEST_GGUF_FILE, filters={}, docs=[doc]\n    )\n    return cache\n\n\ndef test_basic_rag_capabilities(llama_cache):\n    \"\"\"Test that the cache can answer basic questions about the document content.\"\"\"\n    # Test question about whether ingestion is actually happening\n    response = llama_cache.query(\n        \"Summarize the content of the document. Please respond in a single sentence. Summary: \"\n    )\n    assert isinstance(response, CompletionResponse)\n    # assert \"alice\" in response.completion.lower()\n\n    # # Test question about a specific detail\n    # response = llama_cache.query(\n    #     \"What did Alice see the White Rabbit do with its watch? Please respond in a single sentence. Answer: \"\n    # )\n    # assert isinstance(response, CompletionResponse)\n    # # assert \"waistcoat-pocket\" in response.completion.lower() or \"looked at it\" in response.completion.lower()\n\n    # # Test question about character description\n    # response = llama_cache.query(\n    #     \"How did Alice's size change during the story? Please respond in a single sentence. Answer: \"\n    # )\n    # assert isinstance(response, CompletionResponse)\n    # # assert any(phrase in response.completion.lower() for phrase in [\"grew larger\", \"grew smaller\", \"nine feet\", \"telescope\"])\n\n    # # Test question about plot elements\n    # response = llama_cache.query(\n    #     \"What was written on the bottle Alice found? Please respond in a single sentence. Answer: \"\n    # )\n    # assert isinstance(response, CompletionResponse)\n    # # assert \"drink me\" in response.completion.lower()\n\n\n# def test_cache_memory_persistence(llama_cache):\n#     \"\"\"Test that the cache maintains context across multiple queries.\"\"\"\n\n#     # First query to establish context\n#     llama_cache.query(\n#         \"What was Alice doing before she saw the White Rabbit? Please respond in a single sentence. Answer: \"\n#     )\n\n#     # Follow-up query that requires remembering previous context\n#     response = llama_cache.query(\n#         \"What book was her sister reading? Please respond in a single sentence. Answer: \"\n#     )\n#     assert isinstance(response, CompletionResponse)\n#     # assert \"no pictures\" in response.completion.lower() or \"conversations\" in response.completion.lower()\n\n\ndef test_adding_new_documents(llama_cache):\n    \"\"\"Test that the cache can incorporate new documents into its knowledge.\"\"\"\n\n    # Create a new document with additional content\n    new_doc = Document(\n        external_id=\"alice_ch2\",\n        owner={\"id\": \"test_user\", \"name\": \"Test User\"},\n        content_type=\"text/plain\",\n        system_metadata={\n            \"content\": \"Alice found herself in a pool of tears. She met a Mouse swimming in the pool.\",\n            \"title\": \"Alice in Wonderland - Additional Content\",\n            \"source\": \"test_document\",\n        },\n    )\n\n    # Add the new document\n    success = llama_cache.add_docs([new_doc])\n    assert success\n\n    # Query about the new content\n    response = llama_cache.query(\n        \"What did Alice find in the pool of tears? Please respond in a single sentence. Answer: \"\n    )\n    assert isinstance(response, CompletionResponse)\n    assert \"mouse\" in response.completion.lower()\n\n\ndef test_cache_state_persistence():\n    \"\"\"Test that the cache state can be saved and loaded.\"\"\"\n\n    # Create initial cache\n    doc = get_test_document()\n    original_cache = LlamaCache(\n        name=\"test_cache\", model=TEST_MODEL, gguf_file=TEST_GGUF_FILE, filters={}, docs=[doc]\n    )\n\n    # Get the state\n    state_bytes = original_cache.saveable_state\n    # Save state bytes to temporary file\n    import tempfile\n    import os\n    import pickle\n\n    with tempfile.TemporaryDirectory() as temp_dir:\n        cache_file = os.path.join(temp_dir, \"cache.pkl\")\n\n        # Save to file\n        with open(cache_file, \"wb\") as f:\n            pickle.dump(state_bytes, f)\n\n        # Load from file\n        with open(cache_file, \"rb\") as f:\n            loaded_state_bytes = pickle.load(f)\n\n        # # Verify state bytes match\n        # assert state_bytes == loaded_state_bytes\n        # state_bytes = loaded_state_bytes  # Use loaded bytes for rest of test\n\n    # Create new cache from state\n    loaded_cache = LlamaCache.from_bytes(\n        name=\"test_cache\",\n        cache_bytes=loaded_state_bytes,\n        metadata={\n            \"model\": TEST_MODEL,\n            \"model_file\": TEST_GGUF_FILE,\n            \"filters\": {},\n            \"docs\": [doc.model_dump_json()],\n        },\n    )\n\n    # Verify the loaded cache works\n    response = loaded_cache.query(\n        \"Summarize the content of the document. Please respond in a single sentence. Summary: \"\n    )\n    assert isinstance(response, CompletionResponse)\n    assert \"coding\" in response.completion.lower() or \"debug\" in response.completion.lower()\n    # assert \"bottle\" in response.completion.lower() and \"drink me\" in response.completion.lower()\n"}
{"type": "test_file", "path": "core/tests/test_video_parser_manual.py", "content": "from core.parser.video.parse_video import VideoParser\nimport os\nimport json\nfrom datetime import datetime\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nasync def main():\n    # Configure logging\n    logging.basicConfig(\n        level=logging.DEBUG,\n        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    )\n\n    # Get the current directory where test_parser.py is located\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    video_path = os.path.join(current_dir, \"assets/trial.mp4\")\n    logger.info(f\"Video path: {video_path}\")\n\n    # Initialize parser with video path\n    logger.info(\"Initializing VideoParser\")\n    parser = VideoParser(video_path)\n\n    # Process video to get transcript and frame descriptions\n    logger.info(\"Processing video...\")\n    results = await parser.process_video()\n    logger.info(\"Video processing complete\")\n\n    # Create output directory if it doesn't exist\n    output_dir = os.path.join(current_dir, \"output\")\n    os.makedirs(output_dir, exist_ok=True)\n    logger.info(f\"Using output directory: {output_dir}\")\n\n    # Generate timestamp for unique filenames\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n    # Save metadata\n    metadata_path = os.path.join(output_dir, f\"metadata_{timestamp}.json\")\n    logger.info(f\"Saving metadata to {metadata_path}\")\n    with open(metadata_path, \"w\") as f:\n        json.dump(results[\"metadata\"], f, indent=4)\n\n    # Save transcript data\n    transcript_data = {\n        \"timestamps\": results[\"transcript\"].timestamps,\n        \"contents\": results[\"transcript\"].contents,\n    }\n    transcript_path = os.path.join(output_dir, f\"transcript_{timestamp}.json\")\n    logger.info(f\"Saving transcript to {transcript_path}\")\n    with open(transcript_path, \"w\") as f:\n        json.dump(transcript_data, f, indent=4)\n\n    # Save frame descriptions\n    frame_data = {\n        \"timestamps\": results[\"frame_descriptions\"].timestamps,\n        \"contents\": results[\"frame_descriptions\"].contents,\n    }\n    frames_path = os.path.join(output_dir, f\"frame_descriptions_{timestamp}.json\")\n    logger.info(f\"Saving frame descriptions to {frames_path}\")\n    with open(frames_path, \"w\") as f:\n        json.dump(frame_data, f, indent=4)\n\n    # Print metadata\n    logger.info(\"Video Metadata:\")\n    logger.info(f\"Duration: {results['metadata']['duration']:.2f} seconds\")\n    logger.info(f\"FPS: {results['metadata']['fps']}\")\n    logger.info(f\"Total Frames: {results['metadata']['total_frames']}\")\n    logger.info(f\"Frame Sample Rate: {results['metadata']['frame_sample_rate']}\")\n\n    # Print sample of transcript\n    logger.info(\"Transcript Sample (first 3 segments):\")\n    transcript_data = results[\"transcript\"].at_time(0, padding=10)\n    for time, text in transcript_data[:3]:\n        logger.info(f\"{time:.2f}s: {text}\")\n\n    # Print sample of frame descriptions\n    logger.info(\"Frame Descriptions Sample (first 3 frames):\")\n    frame_data = results[\"frame_descriptions\"].at_time(0, padding=10)\n    for time, desc in frame_data[:3]:\n        logger.info(f\"{time:.2f}s: {desc}\")\n\n    logger.info(f\"Output files saved in: {output_dir}\")\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    asyncio.run(main())\n"}
{"type": "test_file", "path": "core/tests/unit/test_colpali_embedding.py", "content": "import pytest\nimport base64\nimport io\nimport numpy as np\nfrom PIL import Image\n\nfrom core.embedding.colpali_embedding_model import ColpaliEmbeddingModel\nfrom core.models.chunk import Chunk\n\n\n# Helper functions\ndef create_sample_image():\n    \"\"\"Create a small sample image for testing\"\"\"\n    # Create a 10x10 RGB image\n    img_array = np.random.randint(0, 255, (10, 10, 3), dtype=np.uint8)\n    image = Image.fromarray(img_array)\n\n    # Convert to base64 encoded string\n    img_byte_arr = io.BytesIO()\n    image.save(img_byte_arr, format=\"PNG\")\n    img_byte_arr = img_byte_arr.getvalue()\n    img_b64 = base64.b64encode(img_byte_arr).decode(\"utf-8\")\n\n    return img_b64\n\n\ndef create_sample_chunks(num_chunks=2):\n    \"\"\"Create sample chunks with base64-encoded images\"\"\"\n    chunks = []\n    for i in range(num_chunks):\n        img_b64 = create_sample_image()\n        chunk = Chunk(content=img_b64, metadata={\"filename\": f\"test_image_{i}.png\"})\n        chunks.append(chunk)\n    return chunks\n\n\n@pytest.fixture\ndef embedding_model():\n    \"\"\"Create an instance of the ColpaliEmbeddingModel\"\"\"\n    model = ColpaliEmbeddingModel()\n    return model\n\n\n# Tests\n@pytest.mark.asyncio\nasync def test_embed_for_query(embedding_model):\n    \"\"\"Test embedding a text query\"\"\"\n    # Test with a simple query\n    query = \"Find images similar to this concept\"\n\n    # Get embeddings\n    result = await embedding_model.embed_for_query(query)\n\n    # Check results\n    assert isinstance(result, np.ndarray)\n    # The first dimension can vary based on the number of tokens\n    assert result.shape[1] == 128\n    assert result.dtype == np.float32\n\n\n@pytest.mark.asyncio\nasync def test_embed_for_ingestion_single_chunk(embedding_model):\n    \"\"\"Test embedding a single chunk with an image\"\"\"\n    # Create a single test chunk\n    chunk = create_sample_chunks(num_chunks=1)[0]\n\n    # Test embedding for ingestion\n    result = await embedding_model.embed_for_ingestion(chunk)\n\n    # Check results\n    assert isinstance(result, list)\n    assert len(result) == 1\n    assert isinstance(result[0], np.ndarray)\n    # The first dimension can vary based on the image content\n    assert result[0].shape[1] == 128\n    assert result[0].dtype == np.float32\n\n\n@pytest.mark.asyncio\nasync def test_embed_for_ingestion_multiple_chunks(embedding_model):\n    \"\"\"Test embedding multiple chunks with images\"\"\"\n    # Create multiple test chunks\n    chunks = create_sample_chunks(num_chunks=3)\n\n    # Test embedding for ingestion\n    result = await embedding_model.embed_for_ingestion(chunks)\n\n    # Check results\n    assert isinstance(result, list)\n    assert len(result) == 3\n\n    for emb in result:\n        assert isinstance(emb, np.ndarray)\n        # The first dimension can vary based on the image content\n        # assert emb.shape[1] == 128\n        print(emb.shape)\n        assert emb.dtype == np.float32\n\n\n@pytest.mark.asyncio\nasync def test_embed_for_query_complex(embedding_model):\n    \"\"\"Test embedding a more complex text query\"\"\"\n    # Test with a longer, more complex query\n    query = \"Find images that contain diagrams of electronic circuits with resistors and capacitors\"\n\n    # Get embeddings\n    result = await embedding_model.embed_for_query(query)\n\n    # Check results\n    assert isinstance(result, np.ndarray)\n    # The first dimension can vary based on the number of tokens\n    assert result.shape[1] == 128\n    assert result.dtype == np.float32\n\n\n@pytest.mark.asyncio\nasync def test_generate_embeddings_directly(embedding_model):\n    \"\"\"Test the generate_embeddings method directly with text\"\"\"\n    # Test text\n    text = \"Test query for direct embedding generation\"\n\n    # Generate embeddings\n    result = embedding_model.generate_embeddings(text)\n\n    # Check results\n    assert isinstance(result, np.ndarray)\n    # The first dimension can vary based on the number of tokens\n    assert result.shape[1] == 128\n    assert result.dtype == np.float32\n\n\n@pytest.mark.asyncio\nasync def test_different_query_lengths(embedding_model):\n    \"\"\"Test that different query lengths produce consistent embedding dimensions\"\"\"\n    # Test with queries of different lengths\n    queries = [\n        \"Short\",\n        \"A slightly longer query\",\n        \"This is a much longer query that should have many more tokens when processed by the model\",\n    ]\n\n    results = []\n    for query in queries:\n        result = await embedding_model.embed_for_query(query)\n        results.append(result)\n\n    # Check all results have the same second dimension (128)\n    for result in results:\n        assert isinstance(result, np.ndarray)\n        assert result.shape[1] == 128\n        assert result.dtype == np.float32\n"}
{"type": "test_file", "path": "core/tests/unit/test_multivector.py", "content": "import pytest\nimport asyncio\nimport torch\nimport numpy as np\nfrom pgvector.psycopg import Bit\nimport logging\nfrom core.vector_store.multi_vector_store import MultiVectorStore\nfrom core.models.chunk import DocumentChunk\n\n# Test database URI\nTEST_DB_URI = \"postgresql://postgres:postgres@localhost:5432/test_db\"\n\nlogger = logging.getLogger(__name__)\n\n\n# Sample test data\ndef get_sample_embeddings(num_vectors=3, dim=128):\n    \"\"\"Generate sample embeddings for testing\"\"\"\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    elif torch.backends.mps.is_available():\n        device = \"mps\"\n    else:\n        device = \"cpu\"\n\n    # Create random embeddings with values between -1 and 1\n    embeddings = torch.rand(num_vectors, dim, device=device) * 2 - 1\n    return embeddings.cpu().numpy()\n\n\ndef get_sample_document_chunks(num_chunks=3, num_vectors=3, dim=128):\n    \"\"\"Create sample document chunks for testing\"\"\"\n    chunks = []\n    for i in range(num_chunks):\n        embeddings = get_sample_embeddings(num_vectors, dim)\n        chunk = DocumentChunk(\n            document_id=f\"doc_{i}\",\n            content=f\"Test content {i}\",\n            embedding=embeddings,\n            chunk_number=i,\n            metadata={\"test_key\": f\"test_value_{i}\"},\n        )\n        chunks.append(chunk)\n    return chunks\n\n\n# Fixtures\n@pytest.fixture(scope=\"function\")\nasync def vector_store():\n    \"\"\"Create a real MultiVectorStore instance connected to the test database\"\"\"\n    # Create the store\n    store = MultiVectorStore(uri=TEST_DB_URI)\n\n    try:\n        # Try to initialize the database\n        store.initialize()\n\n        # Clean up any existing data\n        store.conn.execute(\"TRUNCATE TABLE multi_vector_embeddings RESTART IDENTITY\")\n\n        # Drop the function if it exists\n        try:\n            store.conn.execute(\"DROP FUNCTION IF EXISTS max_sim(bit[], bit[])\")\n        except Exception as e:\n            print(f\"Error dropping function: {e}\")\n    except Exception as e:\n        print(f\"Error setting up database: {e}\")\n\n    yield store\n\n    # Clean up after tests\n    try:\n        store.conn.execute(\"TRUNCATE TABLE multi_vector_embeddings RESTART IDENTITY\")\n    except Exception as e:\n        print(f\"Error cleaning up: {e}\")\n\n    # Close connection\n    store.close()\n\n\n# Glassbox Tests - Testing internal implementation details\n@pytest.mark.asyncio\nasync def test_binary_quantize():\n    \"\"\"Test the _binary_quantize method correctly converts embeddings\"\"\"\n    store = MultiVectorStore(uri=TEST_DB_URI)\n\n    # Test with torch tensor\n    torch_embeddings = torch.tensor([[0.1, -0.2, 0.3], [-0.1, 0.2, -0.3]])\n    binary_result = store._binary_quantize(torch_embeddings)\n    assert len(binary_result) == 2\n\n    # Check results match expected patterns\n    assert (\n        binary_result[0].to_text() == Bit(\"101\").to_text()\n    )  # Positive values (>0) become 1, negative/zero become 0\n    assert (\n        binary_result[1].to_text() == Bit(\"010\").to_text()\n    )  # First row: [0.1 (>0), -0.2 (<0), 0.3 (>0)] → \"101\"\n    # Second row: [-0.1 (<0), 0.2 (>0), -0.3 (<0)] → \"010\"\n\n    # Test with numpy array\n    numpy_embeddings = np.array([[0.1, -0.2, 0.3], [-0.1, 0.2, -0.3]])\n    binary_result = store._binary_quantize(numpy_embeddings)\n    assert len(binary_result) == 2\n\n    assert binary_result[0].to_text() == Bit(\"101\").to_text()\n    assert binary_result[1].to_text() == Bit(\"010\").to_text()\n\n\n@pytest.mark.asyncio\nasync def test_initialize_creates_tables_and_function(vector_store):\n    \"\"\"Test that initialize creates the necessary tables and functions\"\"\"\n    vector_store.initialize()\n    # Check if the table exists\n    result = vector_store.conn.execute(\n        \"SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'multi_vector_embeddings')\"\n    ).fetchone()\n    table_exists = result[0]\n    assert table_exists is True\n\n    logger.info(\"Table exists!\")\n\n    # Check if the max_sim function exists\n    result = vector_store.conn.execute(\n        \"SELECT EXISTS (SELECT FROM pg_proc WHERE proname = 'max_sim')\"\n    ).fetchone()\n    function_exists = result[0]\n    logger.info(f\"Function exists {function_exists}\")\n    assert function_exists is True\n\n\n@pytest.mark.asyncio\nasync def test_database_schema(vector_store):\n    \"\"\"Test that the database schema matches our expectations\"\"\"\n    # Check columns in the table\n    result = vector_store.conn.execute(\n        \"SELECT column_name, data_type FROM information_schema.columns \"\n        \"WHERE table_name = 'multi_vector_embeddings'\"\n    ).fetchall()\n\n    # Convert to a dict for easier checking\n    column_dict = {col[0]: col[1] for col in result}\n\n    # Check required columns\n    assert \"id\" in column_dict\n    assert \"document_id\" in column_dict\n    assert \"chunk_number\" in column_dict\n    assert \"content\" in column_dict\n    assert \"embeddings\" in column_dict\n\n\n# Blackbox Tests - Testing the public API\n@pytest.mark.asyncio\nasync def test_store_and_query_embeddings(vector_store):\n    \"\"\"End-to-end test of storing and querying embeddings\"\"\"\n    vector_store.initialize()\n    # Create test data\n    chunks = get_sample_document_chunks(num_chunks=5, num_vectors=3, dim=128)\n\n    # Store the embeddings\n    result, stored_ids = await vector_store.store_embeddings(chunks)\n\n    # Verify storage was successful\n    assert result is True\n    assert len(stored_ids) == 5\n\n    # Create a query embedding (use the first embedding from the first chunk)\n    query_embedding = chunks[0].embedding\n\n    # Query for similar chunks\n    results = await vector_store.query_similar(query_embedding, k=3)\n\n    # Verify we got results\n    assert len(results) > 0\n    assert isinstance(results[0], DocumentChunk)\n\n    # The first result should be the chunk we queried with\n    assert results[0].document_id == chunks[0].document_id\n\n    # Check that scores are in descending order\n    for i in range(len(results) - 1):\n        assert results[i].score >= results[i + 1].score\n\n\n@pytest.mark.asyncio\nasync def test_query_with_doc_ids(vector_store):\n    \"\"\"Test querying with document ID filtering\"\"\"\n    vector_store.initialize()\n    # Create test data\n    chunks = get_sample_document_chunks(num_chunks=5, num_vectors=3, dim=128)\n\n    # Store the embeddings\n    await vector_store.store_embeddings(chunks)\n\n    # Create a query embedding\n    query_embedding = get_sample_embeddings(1, 128)  # Just one vector\n\n    # Query with specific doc_ids\n    doc_ids = [\"doc_1\", \"doc_3\"]\n    results = await vector_store.query_similar(query_embedding, k=5, doc_ids=doc_ids)\n\n    # Verify results only include the specified doc_ids\n    assert all(result.document_id in doc_ids for result in results)\n\n\n@pytest.mark.asyncio\nasync def test_store_embeddings_empty(vector_store):\n    \"\"\"Test storing empty embeddings list\"\"\"\n    vector_store.initialize()\n    result, stored_ids = await vector_store.store_embeddings([])\n    assert result is True\n    assert stored_ids == []\n\n\n@pytest.mark.asyncio\nasync def test_multi_vector_similarity(vector_store):\n    \"\"\"Test that multi-vector similarity works as expected\"\"\"\n    vector_store.initialize()\n    # Create chunks with very specific embeddings to test similarity\n    chunks = []\n\n    # First chunk: 3 vectors that are all positive in the first half, negative in second half\n    embedding1 = np.ones((3, 128))\n    embedding1[:, 64:] = -1\n    chunk1 = DocumentChunk(\n        document_id=\"similarity_test_1\",\n        content=\"Similarity test content 1\",\n        embedding=embedding1,\n        chunk_number=1,\n    )\n    chunks.append(chunk1)\n\n    # Second chunk: 3 vectors that are all negative in the first half, positive in second half\n    embedding2 = -np.ones((3, 128))\n    embedding2[:, 64:] = 1\n    chunk2 = DocumentChunk(\n        document_id=\"similarity_test_2\",\n        content=\"Similarity test content 2\",\n        embedding=embedding2,\n        chunk_number=2,\n    )\n    chunks.append(chunk2)\n\n    # Store the embeddings\n    await vector_store.store_embeddings(chunks)\n\n    # Create a query embedding that matches the pattern of the first chunk\n    query_embedding = np.ones(128)\n    query_embedding[64:] = -1\n\n    # Query for similar chunks\n    results = await vector_store.query_similar(np.array([query_embedding]), k=2)\n\n    # The first result should be chunk1\n    assert len(results) == 2\n    assert results[0].document_id == \"similarity_test_1\"\n    assert results[1].document_id == \"similarity_test_2\"\n\n\n@pytest.mark.asyncio\nasync def test_store_and_retrieve_metadata(vector_store):\n    \"\"\"Test that metadata is correctly stored and retrieved\"\"\"\n    vector_store.initialize()\n    # Create a chunk with complex metadata\n    complex_metadata = {\n        \"filename\": \"test.pdf\",\n        \"page\": 5,\n        \"tags\": [\"important\", \"review\"],\n        \"nested\": {\"key1\": \"value1\", \"key2\": 123},\n    }\n\n    embeddings = get_sample_embeddings(3, 128)\n    chunk = DocumentChunk(\n        document_id=\"metadata_test\",\n        content=\"Metadata test content\",\n        embedding=embeddings,\n        chunk_number=1,\n        metadata=complex_metadata,\n    )\n\n    # Store the chunk\n    await vector_store.store_embeddings([chunk])\n\n    # Query to retrieve the chunk\n    query_embedding = get_sample_embeddings(1, 128)\n    results = await vector_store.query_similar(query_embedding, k=1, doc_ids=[\"metadata_test\"])\n\n    # Verify metadata was preserved\n    assert len(results) == 1\n    retrieved_metadata = results[0].metadata\n    assert retrieved_metadata[\"filename\"] == \"test.pdf\"\n    assert retrieved_metadata[\"page\"] == 5\n    assert \"tags\" in retrieved_metadata\n    assert \"nested\" in retrieved_metadata\n    assert retrieved_metadata[\"nested\"][\"key1\"] == \"value1\"\n\n\n@pytest.mark.asyncio\nasync def test_performance(vector_store):\n    \"\"\"Test performance with a larger number of chunks\"\"\"\n    vector_store.initialize()\n    # Create a larger set of chunks\n    num_chunks = 20\n    chunks = get_sample_document_chunks(num_chunks=num_chunks, num_vectors=3, dim=128)\n\n    # Measure time to store embeddings\n    start_time = asyncio.get_event_loop().time()\n    await vector_store.store_embeddings(chunks)\n    storage_time = asyncio.get_event_loop().time() - start_time\n\n    # Measure time to query\n    query_embedding = get_sample_embeddings(1, 128)\n    start_time = asyncio.get_event_loop().time()\n    results = await vector_store.query_similar(query_embedding, k=5)\n    query_time = asyncio.get_event_loop().time() - start_time\n\n    # Log performance metrics\n    print(f\"Storage time for {num_chunks} chunks: {storage_time:.4f} seconds\")\n    print(f\"Query time for top 5 results: {query_time:.4f} seconds\")\n\n    # Basic assertions to ensure the test ran\n    assert len(results) > 0\n"}
{"type": "test_file", "path": "core/tests/unit/test_reranker.py", "content": "import pytest\nfrom typing import List\n\nfrom core.models.chunk import DocumentChunk\nfrom core.reranker.flag_reranker import FlagReranker\nfrom core.config import get_settings\n\n\n@pytest.fixture\ndef sample_chunks() -> List[DocumentChunk]:\n    return [\n        DocumentChunk(\n            document_id=\"1\",\n            content=\"The quick brown fox jumps over the lazy dog\",\n            embedding=[0.1] * 10,\n            chunk_number=1,\n            score=0.5,\n        ),\n        DocumentChunk(\n            document_id=\"2\",\n            content=\"Python is a popular programming language\",\n            embedding=[0.2] * 10,\n            chunk_number=1,\n            score=0.7,\n        ),\n        DocumentChunk(\n            document_id=\"3\",\n            content=\"Machine learning models help analyze data\",\n            embedding=[0.3] * 10,\n            chunk_number=1,\n            score=0.3,\n        ),\n    ]\n\n\n@pytest.fixture\ndef reranker():\n    \"\"\"Fixture to create and reuse a flag reranker instance\"\"\"\n    settings = get_settings()\n    if not settings.USE_RERANKING:\n        pytest.skip(\"Reranker is disabled in settings\")\n    return FlagReranker(\n        model_name=settings.RERANKER_MODEL,\n        device=settings.RERANKER_DEVICE,\n        use_fp16=settings.RERANKER_USE_FP16,\n        query_max_length=settings.RERANKER_QUERY_MAX_LENGTH,\n        passage_max_length=settings.RERANKER_PASSAGE_MAX_LENGTH,\n    )\n\n\n@pytest.mark.asyncio\nasync def test_reranker_relevance(reranker, sample_chunks):\n    \"\"\"Test that reranker improves relevance for programming-related query\"\"\"\n    if not get_settings().USE_RERANKING:\n        pytest.skip(\"Reranker is disabled in settings\")\n    print(\"\\n=== Testing Reranker Relevance ===\")\n    query = \"What is Python programming language?\"\n\n    # Get reranked results\n    reranked_chunks = await reranker.rerank(query, sample_chunks)\n    print(f\"\\nQuery: {query}\")\n    for i, chunk in enumerate(reranked_chunks):\n        print(f\"{i+1}. Score: {chunk.score:.3f} - {chunk.content}\")\n\n    # The most relevant chunks should be about Python\n    assert \"Python\" in reranked_chunks[0].content\n    assert reranked_chunks[0].score > reranked_chunks[-1].score\n\n    # Check that irrelevant content (fox/dog) is ranked lower\n    fox_chunk_idx = next(i for i, c in enumerate(reranked_chunks) if \"fox\" in c.content.lower())\n    assert fox_chunk_idx > 0  # Should not be first\n\n\n@pytest.mark.asyncio\nasync def test_reranker_score_distribution(reranker, sample_chunks):\n    \"\"\"Test that reranker produces reasonable score distribution\"\"\"\n    if not get_settings().USE_RERANKING:\n        pytest.skip(\"Reranker is disabled in settings\")\n    print(\"\\n=== Testing Score Distribution ===\")\n    query = \"Tell me about machine learning and data science\"\n\n    # Get reranked results\n    reranked_chunks = await reranker.rerank(query, sample_chunks)\n    print(f\"\\nQuery: {query}\")\n    for i, chunk in enumerate(reranked_chunks):\n        print(f\"{i+1}. Score: {chunk.score:.3f} - {chunk.content}\")\n\n    # Check score properties\n    scores = [c.score for c in reranked_chunks]\n    assert all(0 <= s <= 1 for s in scores)  # Scores should be between 0 and 1\n    assert len(set(scores)) > 1  # Should have different scores (not all same)\n\n    # Verify ordering\n    assert scores == sorted(scores, reverse=True)  # Should be in descending order\n\n    # Most relevant chunk should be about ML/data science\n    top_chunk = reranked_chunks[0]\n    assert any(term in top_chunk.content.lower() for term in [\"machine learning\", \"data science\"])\n\n\n@pytest.mark.asyncio\nasync def test_reranker_batch_scoring(reranker):\n    \"\"\"Test that reranker can handle multiple queries/passages efficiently\"\"\"\n    if not get_settings().USE_RERANKING:\n        pytest.skip(\"Reranker is disabled in settings\")\n    print(\"\\n=== Testing Batch Scoring ===\")\n    texts = [\n        \"Python is a programming language\",\n        \"Machine learning is a field of AI\",\n        \"The quick brown fox jumps\",\n        \"Data science uses statistical methods\",\n    ]\n    queries = [\"What is Python?\", \"Explain artificial intelligence\", \"Tell me about data analysis\"]\n\n    # Test multiple queries against multiple texts\n    for query in queries:\n        scores = await reranker.compute_score(query, texts)\n        print(f\"\\nQuery: {query}\")\n        for text, score in zip(texts, scores):\n            print(f\"Score: {score:.3f} - {text}\")\n        assert len(scores) == len(texts)\n        assert all(isinstance(s, float) for s in scores)\n        assert all(0 <= s <= 1 for s in scores)\n\n\n@pytest.mark.asyncio\nasync def test_reranker_empty_and_edge_cases(reranker, sample_chunks):\n    \"\"\"Test reranker behavior with empty or edge case inputs\"\"\"\n    if not get_settings().USE_RERANKING:\n        pytest.skip(\"Reranker is disabled in settings\")\n    print(\"\\n=== Testing Edge Cases ===\")\n\n    # Empty chunks list\n    result = await reranker.rerank(\"test query\", [])\n    assert result == []\n    print(\"Empty chunks test passed\")\n\n    # Single chunk\n    single_chunk = DocumentChunk(\n        document_id=\"1\",\n        content=\"Test content\",\n        embedding=[0.1] * 768,\n        chunk_number=1,\n        score=0.5,\n    )\n    result = await reranker.rerank(\"test query\", [single_chunk])\n    assert len(result) == 1\n    assert isinstance(result[0].score, float)\n    print(f\"Single chunk test passed - Score: {result[0].score:.3f}\")\n\n    # Empty query\n    result = await reranker.rerank(\"\", sample_chunks)\n    assert len(result) == len(sample_chunks)\n    print(\"Empty query test passed\")\n\n\n@pytest.mark.asyncio\nasync def test_reranker_consistency(reranker, sample_chunks):\n    \"\"\"Test that reranker produces consistent results for same input\"\"\"\n    if not get_settings().USE_RERANKING:\n        pytest.skip(\"Reranker is disabled in settings\")\n    print(\"\\n=== Testing Consistency ===\")\n    query = \"What is Python programming?\"\n\n    # Run reranking multiple times\n    results1 = await reranker.rerank(query, sample_chunks)\n    results2 = await reranker.rerank(query, sample_chunks)\n\n    # Scores should be the same across runs\n    scores1 = [c.score for c in results1]\n    scores2 = [c.score for c in results2]\n    print(\"\\nScores from first run:\", [f\"{s:.3f}\" for s in scores1])\n    print(\"Scores from second run:\", [f\"{s:.3f}\" for s in scores2])\n    assert scores1 == scores2\n\n    # Order should be preserved\n    assert [c.document_id for c in results1] == [c.document_id for c in results2]\n    print(\"Order consistency test passed\")\n"}
{"type": "test_file", "path": "core/tests/unit/test_telemetry_proxy.py", "content": "#!/usr/bin/env python\n\"\"\"\nTest script to verify that telemetry data is being properly sent through the proxy.\nThis script will generate a test span and metric and send it to Honeycomb via the proxy.\n\"\"\"\n\nimport time\nimport logging\nimport uuid\nimport asyncio\nfrom datetime import datetime\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\nlogger = logging.getLogger(\"telemetry-test\")\n\n# Import the telemetry service\nfrom core.services.telemetry import TelemetryService\nfrom core.config import get_settings\n\nasync def run_test():\n    \"\"\"Run a telemetry test to verify proxy functionality.\"\"\"\n    settings = get_settings()\n    \n    # Log the current configuration\n    logger.info(f\"Telemetry enabled: {settings.TELEMETRY_ENABLED}\")\n    logger.info(f\"Honeycomb enabled: {settings.HONEYCOMB_ENABLED}\")\n    logger.info(f\"Honeycomb proxy endpoint: {settings.HONEYCOMB_PROXY_ENDPOINT}\")\n    \n    # Get the telemetry service\n    telemetry_service = TelemetryService()\n    \n    # Generate a unique user ID for testing\n    test_user_id = f\"test-user-{uuid.uuid4()}\"\n    \n    # Track a test operation\n    logger.info(f\"Tracking test operation for user {test_user_id}\")\n    \n    # Use the telemetry service to track an operation (with async context manager)\n    async with telemetry_service.track_operation(\n        operation_type=\"test_proxy\",\n        user_id=test_user_id,\n        tokens_used=100,\n        metadata={\n            \"test\": True,\n            \"timestamp\": datetime.now().isoformat(),\n            \"proxy_test\": \"Honeycomb proxy test\"\n        }\n    ) as span:\n        # Simulate some work\n        logger.info(\"Performing test operation...\")\n        await asyncio.sleep(2)\n        \n        # Add some attributes to the span\n        span.set_attribute(\"test.proxy\", True)\n        span.set_attribute(\"test.timestamp\", time.time())\n        \n        # Log a message\n        logger.info(\"Test operation completed successfully\")\n    \n    # Wait a moment for the telemetry data to be sent\n    logger.info(\"Waiting for telemetry data to be sent...\")\n    await asyncio.sleep(5)\n    \n    logger.info(\"Test completed. Check Honeycomb for the telemetry data.\")\n    logger.info(f\"Look for operation_type='test_proxy' and user_id='{test_user_id}'\")\n\ndef main():\n    \"\"\"Run the async test function.\"\"\"\n    asyncio.run(run_test())\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "test_file", "path": "core/tests/integration/__init__.py", "content": ""}
{"type": "source_file", "path": "__init__.py", "content": ""}
{"type": "source_file", "path": "core/api.py", "content": "import asyncio\nimport json\nfrom datetime import datetime, UTC, timedelta\nfrom pathlib import Path\nimport sys\nfrom typing import Any, Dict, List, Optional\nfrom fastapi import FastAPI, Form, HTTPException, Depends, Header, UploadFile, File\nfrom fastapi.middleware.cors import CORSMiddleware\nimport jwt\nimport logging\nfrom opentelemetry.instrumentation.fastapi import FastAPIInstrumentor\nfrom core.completion.openai_completion import OpenAICompletionModel\nfrom core.embedding.ollama_embedding_model import OllamaEmbeddingModel\nfrom core.models.request import RetrieveRequest, CompletionQueryRequest, IngestTextRequest, CreateGraphRequest, BatchIngestResponse\nfrom core.models.completion import ChunkSource, CompletionResponse\nfrom core.models.documents import Document, DocumentResult, ChunkResult\nfrom core.models.graph import Graph\nfrom core.models.auth import AuthContext, EntityType\nfrom core.parser.databridge_parser import DatabridgeParser\nfrom core.services.document_service import DocumentService\nfrom core.services.telemetry import TelemetryService\nfrom core.config import get_settings\nfrom core.database.mongo_database import MongoDatabase\nfrom core.database.postgres_database import PostgresDatabase\nfrom core.vector_store.mongo_vector_store import MongoDBAtlasVectorStore\nfrom core.vector_store.multi_vector_store import MultiVectorStore\nfrom core.embedding.colpali_embedding_model import ColpaliEmbeddingModel\nfrom core.storage.s3_storage import S3Storage\nfrom core.storage.local_storage import LocalStorage\nfrom core.embedding.openai_embedding_model import OpenAIEmbeddingModel\nfrom core.completion.ollama_completion import OllamaCompletionModel\nfrom core.reranker.flag_reranker import FlagReranker\nfrom core.cache.llama_cache_factory import LlamaCacheFactory\nimport tomli\n\n# Initialize FastAPI app\napp = FastAPI(title=\"DataBridge API\")\nlogger = logging.getLogger(__name__)\n\n\n# Add health check endpoints\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Basic health check endpoint.\"\"\"\n    return {\"status\": \"healthy\"}\n\n\n@app.get(\"/health/ready\")\nasync def readiness_check():\n    \"\"\"Readiness check that verifies the application is initialized.\"\"\"\n    return {\n        \"status\": \"ready\",\n        \"components\": {\n            \"database\": settings.DATABASE_PROVIDER,\n            \"vector_store\": settings.VECTOR_STORE_PROVIDER,\n            \"embedding\": settings.EMBEDDING_PROVIDER,\n            \"completion\": settings.COMPLETION_PROVIDER,\n            \"storage\": settings.STORAGE_PROVIDER,\n        },\n    }\n\n\n# Initialize telemetry\ntelemetry = TelemetryService()\n\n# Add OpenTelemetry instrumentation\nFastAPIInstrumentor.instrument_app(app)\n\n# Add CORS middleware\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Initialize service\nsettings = get_settings()\n\n# Initialize database\nmatch settings.DATABASE_PROVIDER:\n    case \"postgres\":\n        if not settings.POSTGRES_URI:\n            raise ValueError(\"PostgreSQL URI is required for PostgreSQL database\")\n        database = PostgresDatabase(uri=settings.POSTGRES_URI)\n    case \"mongodb\":\n        if not settings.MONGODB_URI:\n            raise ValueError(\"MongoDB URI is required for MongoDB database\")\n        database = MongoDatabase(\n            uri=settings.MONGODB_URI,\n            db_name=settings.DATABRIDGE_DB,\n            collection_name=settings.DOCUMENTS_COLLECTION,\n        )\n    case _:\n        raise ValueError(f\"Unsupported database provider: {settings.DATABASE_PROVIDER}\")\n\n\n@app.on_event(\"startup\")\nasync def initialize_database():\n    \"\"\"Initialize database tables and indexes on application startup.\"\"\"\n    logger.info(\"Initializing database...\")\n    success = await database.initialize()\n    if success:\n        logger.info(\"Database initialization successful\")\n    else:\n        logger.error(\"Database initialization failed\")\n        # We don't raise an exception here to allow the app to continue starting\n        # even if there are initialization errors\n\n# Initialize vector store\nmatch settings.VECTOR_STORE_PROVIDER:\n    case \"mongodb\":\n        vector_store = MongoDBAtlasVectorStore(\n            uri=settings.MONGODB_URI,\n            database_name=settings.DATABRIDGE_DB,\n            collection_name=settings.CHUNKS_COLLECTION,\n            index_name=settings.VECTOR_INDEX_NAME,\n        )\n    case \"pgvector\":\n        if not settings.POSTGRES_URI:\n            raise ValueError(\"PostgreSQL URI is required for pgvector store\")\n        from core.vector_store.pgvector_store import PGVectorStore\n\n        vector_store = PGVectorStore(\n            uri=settings.POSTGRES_URI,\n        )\n    case _:\n        raise ValueError(f\"Unsupported vector store provider: {settings.VECTOR_STORE_PROVIDER}\")\n\n# Initialize storage\nmatch settings.STORAGE_PROVIDER:\n    case \"local\":\n        storage = LocalStorage(storage_path=settings.STORAGE_PATH)\n    case \"aws-s3\":\n        if not settings.AWS_ACCESS_KEY or not settings.AWS_SECRET_ACCESS_KEY:\n            raise ValueError(\"AWS credentials are required for S3 storage\")\n        storage = S3Storage(\n            aws_access_key=settings.AWS_ACCESS_KEY,\n            aws_secret_key=settings.AWS_SECRET_ACCESS_KEY,\n            region_name=settings.AWS_REGION,\n            default_bucket=settings.S3_BUCKET,\n        )\n    case _:\n        raise ValueError(f\"Unsupported storage provider: {settings.STORAGE_PROVIDER}\")\n\n# Initialize parser\nparser = DatabridgeParser(\n    chunk_size=settings.CHUNK_SIZE,\n    chunk_overlap=settings.CHUNK_OVERLAP,\n    use_unstructured_api=settings.USE_UNSTRUCTURED_API,\n    unstructured_api_key=settings.UNSTRUCTURED_API_KEY,\n    assemblyai_api_key=settings.ASSEMBLYAI_API_KEY,\n    anthropic_api_key=settings.ANTHROPIC_API_KEY,\n    use_contextual_chunking=settings.USE_CONTEXTUAL_CHUNKING,\n)\n\n# Initialize embedding model\nmatch settings.EMBEDDING_PROVIDER:\n    case \"ollama\":\n        embedding_model = OllamaEmbeddingModel(\n            base_url=settings.EMBEDDING_OLLAMA_BASE_URL,\n            model_name=settings.EMBEDDING_MODEL,\n        )\n    case \"openai\":\n        if not settings.OPENAI_API_KEY:\n            raise ValueError(\"OpenAI API key is required for OpenAI embedding model\")\n        embedding_model = OpenAIEmbeddingModel(\n            api_key=settings.OPENAI_API_KEY,\n            model_name=settings.EMBEDDING_MODEL,\n        )\n    case _:\n        raise ValueError(f\"Unsupported embedding provider: {settings.EMBEDDING_PROVIDER}\")\n\n# Initialize completion model\nmatch settings.COMPLETION_PROVIDER:\n    case \"ollama\":\n        completion_model = OllamaCompletionModel(\n            model_name=settings.COMPLETION_MODEL,\n            base_url=settings.COMPLETION_OLLAMA_BASE_URL,\n        )\n    case \"openai\":\n        if not settings.OPENAI_API_KEY:\n            raise ValueError(\"OpenAI API key is required for OpenAI completion model\")\n        completion_model = OpenAICompletionModel(\n            model_name=settings.COMPLETION_MODEL,\n        )\n    case _:\n        raise ValueError(f\"Unsupported completion provider: {settings.COMPLETION_PROVIDER}\")\n\n# Initialize reranker\nreranker = None\nif settings.USE_RERANKING:\n    match settings.RERANKER_PROVIDER:\n        case \"flag\":\n            reranker = FlagReranker(\n                model_name=settings.RERANKER_MODEL,\n                device=settings.RERANKER_DEVICE,\n                use_fp16=settings.RERANKER_USE_FP16,\n                query_max_length=settings.RERANKER_QUERY_MAX_LENGTH,\n                passage_max_length=settings.RERANKER_PASSAGE_MAX_LENGTH,\n            )\n        case _:\n            raise ValueError(f\"Unsupported reranker provider: {settings.RERANKER_PROVIDER}\")\n\n# Initialize cache factory\ncache_factory = LlamaCacheFactory(Path(settings.STORAGE_PATH))\n\n# Initialize ColPali embedding model if enabled\ncolpali_embedding_model = ColpaliEmbeddingModel() if settings.ENABLE_COLPALI else None\ncolpali_vector_store = (\n    MultiVectorStore(uri=settings.POSTGRES_URI) if settings.ENABLE_COLPALI else None\n)\n\n# Initialize document service with configured components\ndocument_service = DocumentService(\n    storage=storage,\n    database=database,\n    vector_store=vector_store,\n    embedding_model=embedding_model,\n    completion_model=completion_model,\n    parser=parser,\n    reranker=reranker,\n    cache_factory=cache_factory,\n    enable_colpali=settings.ENABLE_COLPALI,\n    colpali_embedding_model=colpali_embedding_model,\n    colpali_vector_store=colpali_vector_store,\n)\n\n\nasync def verify_token(authorization: str = Header(None)) -> AuthContext:\n    \"\"\"Verify JWT Bearer token or return dev context if dev_mode is enabled.\"\"\"\n    # Check if dev mode is enabled\n    if settings.dev_mode:\n        return AuthContext(\n            entity_type=EntityType(settings.dev_entity_type),\n            entity_id=settings.dev_entity_id,\n            permissions=set(settings.dev_permissions),\n        )\n\n    # Normal token verification flow\n    if not authorization:\n        raise HTTPException(\n            status_code=401,\n            detail=\"Missing authorization header\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n    try:\n        if not authorization.startswith(\"Bearer \"):\n            raise HTTPException(status_code=401, detail=\"Invalid authorization header\")\n\n        token = authorization[7:]  # Remove \"Bearer \"\n        payload = jwt.decode(token, settings.JWT_SECRET_KEY, algorithms=[settings.JWT_ALGORITHM])\n\n        if datetime.fromtimestamp(payload[\"exp\"], UTC) < datetime.now(UTC):\n            raise HTTPException(status_code=401, detail=\"Token expired\")\n\n        return AuthContext(\n            entity_type=EntityType(payload[\"type\"]),\n            entity_id=payload[\"entity_id\"],\n            app_id=payload.get(\"app_id\"),\n            permissions=set(payload.get(\"permissions\", [\"read\"])),\n        )\n    except jwt.InvalidTokenError as e:\n        raise HTTPException(status_code=401, detail=str(e))\n\n\n@app.post(\"/ingest/text\", response_model=Document)\nasync def ingest_text(\n    request: IngestTextRequest,\n    auth: AuthContext = Depends(verify_token),\n) -> Document:\n    \"\"\"\n    Ingest a text document.\n\n    Args:\n        request: IngestTextRequest containing:\n            - content: Text content to ingest\n            - filename: Optional filename to help determine content type\n            - metadata: Optional metadata dictionary\n            - rules: Optional list of rules. Each rule should be either:\n                   - MetadataExtractionRule: {\"type\": \"metadata_extraction\", \"schema\": {...}}\n                   - NaturalLanguageRule: {\"type\": \"natural_language\", \"prompt\": \"...\"}\n        auth: Authentication context\n\n    Returns:\n        Document: Metadata of ingested document\n    \"\"\"\n    try:\n        async with telemetry.track_operation(\n            operation_type=\"ingest_text\",\n            user_id=auth.entity_id,\n            tokens_used=len(request.content.split()),  # Approximate token count\n            metadata={\n                \"metadata\": request.metadata,\n                \"rules\": request.rules,\n                \"use_colpali\": request.use_colpali,\n            },\n        ):\n            return await document_service.ingest_text(\n                content=request.content,\n                filename=request.filename,\n                metadata=request.metadata,\n                rules=request.rules,\n                use_colpali=request.use_colpali,\n                auth=auth,\n            )\n    except PermissionError as e:\n        raise HTTPException(status_code=403, detail=str(e))\n\n\n@app.post(\"/ingest/file\", response_model=Document)\nasync def ingest_file(\n    file: UploadFile,\n    metadata: str = Form(\"{}\"),\n    rules: str = Form(\"[]\"),\n    auth: AuthContext = Depends(verify_token),\n    use_colpali: Optional[bool] = None,\n) -> Document:\n    \"\"\"\n    Ingest a file document.\n\n    Args:\n        file: File to ingest\n        metadata: JSON string of metadata\n        rules: JSON string of rules list. Each rule should be either:\n               - MetadataExtractionRule: {\"type\": \"metadata_extraction\", \"schema\": {...}}\n               - NaturalLanguageRule: {\"type\": \"natural_language\", \"prompt\": \"...\"}\n        auth: Authentication context\n\n    Returns:\n        Document: Metadata of ingested document\n    \"\"\"\n    try:\n        metadata_dict = json.loads(metadata)\n        rules_list = json.loads(rules)\n        use_colpali = bool(use_colpali)\n\n        async with telemetry.track_operation(\n            operation_type=\"ingest_file\",\n            user_id=auth.entity_id,\n            metadata={\n                \"filename\": file.filename,\n                \"content_type\": file.content_type,\n                \"metadata\": metadata_dict,\n                \"rules\": rules_list,\n                \"use_colpali\": use_colpali,\n            },\n        ):\n            logger.info(f\"API: Ingesting file with use_colpali: {use_colpali}\")\n            return await document_service.ingest_file(\n                file=file,\n                metadata=metadata_dict,\n                auth=auth,\n                rules=rules_list,\n                use_colpali=use_colpali,\n            )\n    except json.JSONDecodeError as e:\n        raise HTTPException(status_code=400, detail=f\"Invalid JSON: {str(e)}\")\n    except PermissionError as e:\n        raise HTTPException(status_code=403, detail=str(e))\n\n\n@app.post(\"/ingest/files\", response_model=BatchIngestResponse)\nasync def batch_ingest_files(\n    files: List[UploadFile] = File(...),\n    metadata: str = Form(\"{}\"),\n    rules: str = Form(\"[]\"),\n    use_colpali: Optional[bool] = Form(None),\n    parallel: bool = Form(True),\n    auth: AuthContext = Depends(verify_token),\n) -> BatchIngestResponse:\n    \"\"\"\n    Batch ingest multiple files.\n    \n    Args:\n        files: List of files to ingest\n        metadata: JSON string of metadata (either a single dict or list of dicts)\n        rules: JSON string of rules list. Can be either:\n               - A single list of rules to apply to all files\n               - A list of rule lists, one per file\n        use_colpali: Whether to use ColPali-style embedding\n        parallel: Whether to process files in parallel\n        auth: Authentication context\n\n    Returns:\n        BatchIngestResponse containing:\n            - documents: List of successfully ingested documents\n            - errors: List of errors encountered during ingestion\n    \"\"\"\n    if not files:\n        raise HTTPException(\n            status_code=400,\n            detail=\"No files provided for batch ingestion\"\n        )\n\n    try:\n        metadata_value = json.loads(metadata)\n        rules_list = json.loads(rules)\n    except json.JSONDecodeError as e:\n        raise HTTPException(status_code=400, detail=f\"Invalid JSON: {str(e)}\")\n\n    # Validate metadata if it's a list\n    if isinstance(metadata_value, list) and len(metadata_value) != len(files):\n        raise HTTPException(\n            status_code=400,\n            detail=f\"Number of metadata items ({len(metadata_value)}) must match number of files ({len(files)})\"\n        )\n\n    # Validate rules if it's a list of lists\n    if isinstance(rules_list, list) and rules_list and isinstance(rules_list[0], list):\n        if len(rules_list) != len(files):\n            raise HTTPException(\n                status_code=400,\n                detail=f\"Number of rule lists ({len(rules_list)}) must match number of files ({len(files)})\"\n            )\n\n    documents = []\n    errors = []\n\n    async with telemetry.track_operation(\n        operation_type=\"batch_ingest\",\n        user_id=auth.entity_id,\n        metadata={\n            \"file_count\": len(files),\n            \"metadata_type\": \"list\" if isinstance(metadata_value, list) else \"single\",\n            \"rules_type\": \"per_file\" if isinstance(rules_list, list) and rules_list and isinstance(rules_list[0], list) else \"shared\",\n        },\n    ):\n        if parallel:\n            tasks = []\n            for i, file in enumerate(files):\n                metadata_item = metadata_value[i] if isinstance(metadata_value, list) else metadata_value\n                file_rules = rules_list[i] if isinstance(rules_list, list) and rules_list and isinstance(rules_list[0], list) else rules_list\n                task = document_service.ingest_file(\n                    file=file,\n                    metadata=metadata_item,\n                    auth=auth,\n                    rules=file_rules,\n                    use_colpali=use_colpali\n                )\n                tasks.append(task)\n            \n            results = await asyncio.gather(*tasks, return_exceptions=True)\n            \n            for i, result in enumerate(results):\n                if isinstance(result, Exception):\n                    errors.append({\n                        \"filename\": files[i].filename,\n                        \"error\": str(result)\n                    })\n                else:\n                    documents.append(result)\n        else:\n            for i, file in enumerate(files):\n                try:\n                    metadata_item = metadata_value[i] if isinstance(metadata_value, list) else metadata_value\n                    file_rules = rules_list[i] if isinstance(rules_list, list) and rules_list and isinstance(rules_list[0], list) else rules_list\n                    doc = await document_service.ingest_file(\n                        file=file,\n                        metadata=metadata_item,\n                        auth=auth,\n                        rules=file_rules,\n                        use_colpali=use_colpali\n                    )\n                    documents.append(doc)\n                except Exception as e:\n                    errors.append({\n                        \"filename\": file.filename,\n                        \"error\": str(e)\n                    })\n\n    return BatchIngestResponse(documents=documents, errors=errors)\n\n\n@app.post(\"/retrieve/chunks\", response_model=List[ChunkResult])\nasync def retrieve_chunks(request: RetrieveRequest, auth: AuthContext = Depends(verify_token)):\n    \"\"\"Retrieve relevant chunks.\"\"\"\n    try:\n        async with telemetry.track_operation(\n            operation_type=\"retrieve_chunks\",\n            user_id=auth.entity_id,\n            metadata={\n                \"k\": request.k,\n                \"min_score\": request.min_score,\n                \"use_reranking\": request.use_reranking,\n                \"use_colpali\": request.use_colpali,\n            },\n        ):\n            return await document_service.retrieve_chunks(\n                request.query,\n                auth,\n                request.filters,\n                request.k,\n                request.min_score,\n                request.use_reranking,\n                request.use_colpali,\n            )\n    except PermissionError as e:\n        raise HTTPException(status_code=403, detail=str(e))\n\n\n@app.post(\"/retrieve/docs\", response_model=List[DocumentResult])\nasync def retrieve_documents(request: RetrieveRequest, auth: AuthContext = Depends(verify_token)):\n    \"\"\"Retrieve relevant documents.\"\"\"\n    try:\n        async with telemetry.track_operation(\n            operation_type=\"retrieve_docs\",\n            user_id=auth.entity_id,\n            metadata={\n                \"k\": request.k,\n                \"min_score\": request.min_score,\n                \"use_reranking\": request.use_reranking,\n                \"use_colpali\": request.use_colpali,\n            },\n        ):\n            return await document_service.retrieve_docs(\n                request.query,\n                auth,\n                request.filters,\n                request.k,\n                request.min_score,\n                request.use_reranking,\n                request.use_colpali,\n            )\n    except PermissionError as e:\n        raise HTTPException(status_code=403, detail=str(e))\n\n\n@app.post(\"/batch/documents\", response_model=List[Document])\nasync def batch_get_documents(document_ids: List[str], auth: AuthContext = Depends(verify_token)):\n    \"\"\"Retrieve multiple documents by their IDs in a single batch operation.\"\"\"\n    try:\n        async with telemetry.track_operation(\n            operation_type=\"batch_get_documents\",\n            user_id=auth.entity_id,\n            metadata={\n                \"document_count\": len(document_ids),\n            },\n        ):\n            return await document_service.batch_retrieve_documents(document_ids, auth)\n    except PermissionError as e:\n        raise HTTPException(status_code=403, detail=str(e))\n\n\n@app.post(\"/batch/chunks\", response_model=List[ChunkResult])\nasync def batch_get_chunks(chunk_ids: List[ChunkSource], auth: AuthContext = Depends(verify_token)):\n    \"\"\"Retrieve specific chunks by their document ID and chunk number in a single batch operation.\"\"\"\n    try:\n        async with telemetry.track_operation(\n            operation_type=\"batch_get_chunks\",\n            user_id=auth.entity_id,\n            metadata={\n                \"chunk_count\": len(chunk_ids),\n            },\n        ):\n            return await document_service.batch_retrieve_chunks(chunk_ids, auth)\n    except PermissionError as e:\n        raise HTTPException(status_code=403, detail=str(e))\n\n\n@app.post(\"/query\", response_model=CompletionResponse)\nasync def query_completion(\n    request: CompletionQueryRequest, auth: AuthContext = Depends(verify_token)\n):\n    \"\"\"Generate completion using relevant chunks as context.\n    \n    When graph_name is provided, the query will leverage the knowledge graph \n    to enhance retrieval by finding relevant entities and their connected documents.\n    \"\"\"\n    try:\n        async with telemetry.track_operation(\n            operation_type=\"query\",\n            user_id=auth.entity_id,\n            metadata={\n                \"k\": request.k,\n                \"min_score\": request.min_score,\n                \"max_tokens\": request.max_tokens,\n                \"temperature\": request.temperature,\n                \"use_reranking\": request.use_reranking,\n                \"use_colpali\": request.use_colpali,\n                \"graph_name\": request.graph_name,\n                \"hop_depth\": request.hop_depth,\n                \"include_paths\": request.include_paths,\n            },\n        ):\n            return await document_service.query(\n                request.query,\n                auth,\n                request.filters,\n                request.k,\n                request.min_score,\n                request.max_tokens,\n                request.temperature,\n                request.use_reranking,\n                request.use_colpali,\n                request.graph_name,\n                request.hop_depth,\n                request.include_paths,\n            )\n    except PermissionError as e:\n        raise HTTPException(status_code=403, detail=str(e))\n\n\n@app.get(\"/documents\", response_model=List[Document])\nasync def list_documents(\n    auth: AuthContext = Depends(verify_token),\n    skip: int = 0,\n    limit: int = 10000,\n    filters: Optional[Dict[str, Any]] = None,\n):\n    \"\"\"List accessible documents.\"\"\"\n    return await document_service.db.get_documents(auth, skip, limit, filters)\n\n\n@app.get(\"/documents/{document_id}\", response_model=Document)\nasync def get_document(document_id: str, auth: AuthContext = Depends(verify_token)):\n    \"\"\"Get document by ID.\"\"\"\n    try:\n        doc = await document_service.db.get_document(document_id, auth)\n        logger.info(f\"Found document: {doc}\")\n        if not doc:\n            raise HTTPException(status_code=404, detail=\"Document not found\")\n        return doc\n    except HTTPException as e:\n        logger.error(f\"Error getting document: {e}\")\n        raise e\n\n\n@app.get(\"/documents/filename/{filename}\", response_model=Document)\nasync def get_document_by_filename(filename: str, auth: AuthContext = Depends(verify_token)):\n    \"\"\"Get document by filename.\"\"\"\n    try:\n        doc = await document_service.db.get_document_by_filename(filename, auth)\n        logger.info(f\"Found document by filename: {doc}\")\n        if not doc:\n            raise HTTPException(status_code=404, detail=f\"Document with filename '{filename}' not found\")\n        return doc\n    except HTTPException as e:\n        logger.error(f\"Error getting document by filename: {e}\")\n        raise e\n\n\n@app.post(\"/documents/{document_id}/update_text\", response_model=Document)\nasync def update_document_text(\n    document_id: str,\n    request: IngestTextRequest,\n    update_strategy: str = \"add\",\n    auth: AuthContext = Depends(verify_token)\n):\n    \"\"\"\n    Update a document with new text content using the specified strategy.\n    \n    Args:\n        document_id: ID of the document to update\n        request: Text content and metadata for the update\n        update_strategy: Strategy for updating the document (default: 'add')\n        \n    Returns:\n        Document: Updated document metadata\n    \"\"\"\n    try:\n        async with telemetry.track_operation(\n            operation_type=\"update_document_text\",\n            user_id=auth.entity_id,\n            metadata={\n                \"document_id\": document_id,\n                \"update_strategy\": update_strategy,\n                \"use_colpali\": request.use_colpali,\n                \"has_filename\": request.filename is not None,\n            },\n        ):\n            doc = await document_service.update_document(\n                document_id=document_id,\n                auth=auth,\n                content=request.content,\n                file=None,\n                filename=request.filename,\n                metadata=request.metadata,\n                rules=request.rules,\n                update_strategy=update_strategy,\n                use_colpali=request.use_colpali,\n            )\n\n            if not doc:\n                raise HTTPException(status_code=404, detail=\"Document not found or update failed\")\n\n            return doc\n    except PermissionError as e:\n        raise HTTPException(status_code=403, detail=str(e))\n\n@app.post(\"/documents/{document_id}/update_file\", response_model=Document)\nasync def update_document_file(\n    document_id: str,\n    file: UploadFile,\n    metadata: str = Form(\"{}\"),\n    rules: str = Form(\"[]\"),\n    update_strategy: str = Form(\"add\"),\n    use_colpali: Optional[bool] = None,\n    auth: AuthContext = Depends(verify_token)\n):\n    \"\"\"\n    Update a document with content from a file using the specified strategy.\n\n    Args:\n        document_id: ID of the document to update\n        file: File to add to the document\n        metadata: JSON string of metadata to merge with existing metadata\n        rules: JSON string of rules to apply to the content\n        update_strategy: Strategy for updating the document (default: 'add')\n        use_colpali: Whether to use multi-vector embedding\n\n    Returns:\n        Document: Updated document metadata\n    \"\"\"\n    try:\n        metadata_dict = json.loads(metadata)\n        rules_list = json.loads(rules)\n\n        async with telemetry.track_operation(\n            operation_type=\"update_document_file\",\n            user_id=auth.entity_id,\n            metadata={\n                \"document_id\": document_id,\n                \"filename\": file.filename,\n                \"content_type\": file.content_type,\n                \"update_strategy\": update_strategy,\n                \"use_colpali\": use_colpali,\n            },\n        ):\n            doc = await document_service.update_document(\n                document_id=document_id,\n                auth=auth,\n                content=None,\n                file=file,\n                filename=file.filename,\n                metadata=metadata_dict,\n                rules=rules_list,\n                update_strategy=update_strategy,\n                use_colpali=use_colpali,\n            )\n\n            if not doc:\n                raise HTTPException(status_code=404, detail=\"Document not found or update failed\")\n\n            return doc\n    except json.JSONDecodeError as e:\n        raise HTTPException(status_code=400, detail=f\"Invalid JSON: {str(e)}\")\n    except PermissionError as e:\n        raise HTTPException(status_code=403, detail=str(e))\n\n\n@app.post(\"/documents/{document_id}/update_metadata\", response_model=Document)\nasync def update_document_metadata(\n    document_id: str,\n    metadata: Dict[str, Any],\n    auth: AuthContext = Depends(verify_token)\n):\n    \"\"\"\n    Update only a document's metadata.\n\n    Args:\n        document_id: ID of the document to update\n        metadata: New metadata to merge with existing metadata\n\n    Returns:\n        Document: Updated document metadata\n    \"\"\"\n    try:\n        async with telemetry.track_operation(\n            operation_type=\"update_document_metadata\",\n            user_id=auth.entity_id,\n            metadata={\n                \"document_id\": document_id,\n            },\n        ):\n            doc = await document_service.update_document(\n                document_id=document_id,\n                auth=auth,\n                content=None,\n                file=None,\n                filename=None,\n                metadata=metadata,\n                rules=[],\n                update_strategy=\"add\",\n                use_colpali=None,\n            )\n\n            if not doc:\n                raise HTTPException(status_code=404, detail=\"Document not found or update failed\")\n\n            return doc\n    except PermissionError as e:\n        raise HTTPException(status_code=403, detail=str(e))\n\n\n# Usage tracking endpoints\n@app.get(\"/usage/stats\")\nasync def get_usage_stats(auth: AuthContext = Depends(verify_token)) -> Dict[str, int]:\n    \"\"\"Get usage statistics for the authenticated user.\"\"\"\n    async with telemetry.track_operation(operation_type=\"get_usage_stats\", user_id=auth.entity_id):\n        if not auth.permissions or \"admin\" not in auth.permissions:\n            return telemetry.get_user_usage(auth.entity_id)\n        return telemetry.get_user_usage(auth.entity_id)\n\n\n@app.get(\"/usage/recent\")\nasync def get_recent_usage(\n    auth: AuthContext = Depends(verify_token),\n    operation_type: Optional[str] = None,\n    since: Optional[datetime] = None,\n    status: Optional[str] = None,\n) -> List[Dict]:\n    \"\"\"Get recent usage records.\"\"\"\n    async with telemetry.track_operation(\n        operation_type=\"get_recent_usage\",\n        user_id=auth.entity_id,\n        metadata={\n            \"operation_type\": operation_type,\n            \"since\": since.isoformat() if since else None,\n            \"status\": status,\n        },\n    ):\n        if not auth.permissions or \"admin\" not in auth.permissions:\n            records = telemetry.get_recent_usage(\n                user_id=auth.entity_id, operation_type=operation_type, since=since, status=status\n            )\n        else:\n            records = telemetry.get_recent_usage(\n                operation_type=operation_type, since=since, status=status\n            )\n\n        return [\n            {\n                \"timestamp\": record.timestamp,\n                \"operation_type\": record.operation_type,\n                \"tokens_used\": record.tokens_used,\n                \"user_id\": record.user_id,\n                \"duration_ms\": record.duration_ms,\n                \"status\": record.status,\n                \"metadata\": record.metadata,\n            }\n            for record in records\n        ]\n\n\n# Cache endpoints\n@app.post(\"/cache/create\")\nasync def create_cache(\n    name: str,\n    model: str,\n    gguf_file: str,\n    filters: Optional[Dict[str, Any]] = None,\n    docs: Optional[List[str]] = None,\n    auth: AuthContext = Depends(verify_token),\n) -> Dict[str, Any]:\n    \"\"\"Create a new cache with specified configuration.\"\"\"\n    try:\n        async with telemetry.track_operation(\n            operation_type=\"create_cache\",\n            user_id=auth.entity_id,\n            metadata={\n                \"name\": name,\n                \"model\": model,\n                \"gguf_file\": gguf_file,\n                \"filters\": filters,\n                \"docs\": docs,\n            },\n        ):\n            filter_docs = set(await document_service.db.get_documents(auth, filters=filters))\n            additional_docs = (\n                {\n                    await document_service.db.get_document(document_id=doc_id, auth=auth)\n                    for doc_id in docs\n                }\n                if docs\n                else set()\n            )\n            docs_to_add = list(filter_docs.union(additional_docs))\n            if not docs_to_add:\n                raise HTTPException(status_code=400, detail=\"No documents to add to cache\")\n            response = await document_service.create_cache(\n                name, model, gguf_file, docs_to_add, filters\n            )\n            return response\n    except PermissionError as e:\n        raise HTTPException(status_code=403, detail=str(e))\n\n\n@app.get(\"/cache/{name}\")\nasync def get_cache(name: str, auth: AuthContext = Depends(verify_token)) -> Dict[str, Any]:\n    \"\"\"Get cache configuration by name.\"\"\"\n    try:\n        async with telemetry.track_operation(\n            operation_type=\"get_cache\",\n            user_id=auth.entity_id,\n            metadata={\"name\": name},\n        ):\n            exists = await document_service.load_cache(name)\n            return {\"exists\": exists}\n    except PermissionError as e:\n        raise HTTPException(status_code=403, detail=str(e))\n\n\n@app.post(\"/cache/{name}/update\")\nasync def update_cache(name: str, auth: AuthContext = Depends(verify_token)) -> Dict[str, bool]:\n    \"\"\"Update cache with new documents matching its filter.\"\"\"\n    try:\n        async with telemetry.track_operation(\n            operation_type=\"update_cache\",\n            user_id=auth.entity_id,\n            metadata={\"name\": name},\n        ):\n            if name not in document_service.active_caches:\n                exists = await document_service.load_cache(name)\n                if not exists:\n                    raise HTTPException(status_code=404, detail=f\"Cache '{name}' not found\")\n            cache = document_service.active_caches[name]\n            docs = await document_service.db.get_documents(auth, filters=cache.filters)\n            docs_to_add = [doc for doc in docs if doc.id not in cache.docs]\n            return cache.add_docs(docs_to_add)\n    except PermissionError as e:\n        raise HTTPException(status_code=403, detail=str(e))\n\n\n@app.post(\"/cache/{name}/add_docs\")\nasync def add_docs_to_cache(\n    name: str, docs: List[str], auth: AuthContext = Depends(verify_token)\n) -> Dict[str, bool]:\n    \"\"\"Add specific documents to the cache.\"\"\"\n    try:\n        async with telemetry.track_operation(\n            operation_type=\"add_docs_to_cache\",\n            user_id=auth.entity_id,\n            metadata={\"name\": name, \"docs\": docs},\n        ):\n            cache = document_service.active_caches[name]\n            docs_to_add = [\n                await document_service.db.get_document(doc_id, auth)\n                for doc_id in docs\n                if doc_id not in cache.docs\n            ]\n            return cache.add_docs(docs_to_add)\n    except PermissionError as e:\n        raise HTTPException(status_code=403, detail=str(e))\n\n\n@app.post(\"/cache/{name}/query\")\nasync def query_cache(\n    name: str,\n    query: str,\n    max_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    auth: AuthContext = Depends(verify_token),\n) -> CompletionResponse:\n    \"\"\"Query the cache with a prompt.\"\"\"\n    try:\n        async with telemetry.track_operation(\n            operation_type=\"query_cache\",\n            user_id=auth.entity_id,\n            metadata={\n                \"name\": name,\n                \"query\": query,\n                \"max_tokens\": max_tokens,\n                \"temperature\": temperature,\n            },\n        ):\n            cache = document_service.active_caches[name]\n            print(f\"Cache state: {cache.state.n_tokens}\", file=sys.stderr)\n            return cache.query(query)  # , max_tokens, temperature)\n    except PermissionError as e:\n        raise HTTPException(status_code=403, detail=str(e))\n\n\n@app.post(\"/graph/create\", response_model=Graph)\nasync def create_graph(\n    request: CreateGraphRequest,\n    auth: AuthContext = Depends(verify_token),\n) -> Graph:\n    \"\"\"\n    Create a graph from documents.\n\n    This endpoint extracts entities and relationships from documents\n    matching the specified filters or document IDs and creates a graph.\n\n    Args:\n        request: CreateGraphRequest containing:\n            - name: Name of the graph to create\n            - filters: Optional metadata filters to determine which documents to include\n            - documents: Optional list of specific document IDs to include\n        auth: Authentication context\n\n    Returns:\n        Graph: The created graph object\n    \"\"\"\n    try:\n        async with telemetry.track_operation(\n            operation_type=\"create_graph\",\n            user_id=auth.entity_id,\n            metadata={\n                \"name\": request.name,\n                \"filters\": request.filters,\n                \"documents\": request.documents,\n            },\n        ):\n            return await document_service.create_graph(\n                name=request.name,\n                auth=auth,\n                filters=request.filters,\n                documents=request.documents,\n            )\n    except PermissionError as e:\n        raise HTTPException(status_code=403, detail=str(e))\n    except ValueError as e:\n        raise HTTPException(status_code=400, detail=str(e))\n\n\n@app.get(\"/graph/{name}\", response_model=Graph)\nasync def get_graph(\n    name: str,\n    auth: AuthContext = Depends(verify_token),\n) -> Graph:\n    \"\"\"\n    Get a graph by name.\n\n    This endpoint retrieves a graph by its name if the user has access to it.\n\n    Args:\n        name: Name of the graph to retrieve\n        auth: Authentication context\n\n    Returns:\n        Graph: The requested graph object\n    \"\"\"\n    try:\n        async with telemetry.track_operation(\n            operation_type=\"get_graph\",\n            user_id=auth.entity_id,\n            metadata={\"name\": name},\n        ):\n            graph = await document_service.db.get_graph(name, auth)\n            if not graph:\n                raise HTTPException(status_code=404, detail=f\"Graph '{name}' not found\")\n            return graph\n    except PermissionError as e:\n        raise HTTPException(status_code=403, detail=str(e))\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.get(\"/graphs\", response_model=List[Graph])\nasync def list_graphs(\n    auth: AuthContext = Depends(verify_token),\n) -> List[Graph]:\n    \"\"\"\n    List all graphs the user has access to.\n\n    This endpoint retrieves all graphs the user has access to.\n\n    Args:\n        auth: Authentication context\n\n    Returns:\n        List[Graph]: List of graph objects\n    \"\"\"\n    try:\n        async with telemetry.track_operation(\n            operation_type=\"list_graphs\",\n            user_id=auth.entity_id,\n        ):\n            return await document_service.db.list_graphs(auth)\n    except PermissionError as e:\n        raise HTTPException(status_code=403, detail=str(e))\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.post(\"/local/generate_uri\", include_in_schema=True)\nasync def generate_local_uri(\n    name: str = Form(\"admin\"),\n    expiry_days: int = Form(30),\n) -> Dict[str, str]:\n    \"\"\"Generate a local URI for development. This endpoint is unprotected.\"\"\"\n    try:\n        # Clean name\n        name = name.replace(\" \", \"_\").lower()\n\n        # Create payload\n        payload = {\n            \"type\": \"developer\",\n            \"entity_id\": name,\n            \"permissions\": [\"read\", \"write\", \"admin\"],\n            \"exp\": datetime.now(UTC) + timedelta(days=expiry_days),\n        }\n\n        # Generate token\n        token = jwt.encode(payload, settings.JWT_SECRET_KEY, algorithm=settings.JWT_ALGORITHM)\n\n        # Read config for host/port\n        with open(\"databridge.toml\", \"rb\") as f:\n            config = tomli.load(f)\n        base_url = f\"{config['api']['host']}:{config['api']['port']}\".replace(\n            \"localhost\", \"127.0.0.1\"\n        )\n\n        # Generate URI\n        uri = f\"databridge://{name}:{token}@{base_url}\"\n        return {\"uri\": uri}\n    except Exception as e:\n        logger.error(f\"Error generating local URI: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n"}
{"type": "source_file", "path": "core/cache/base_cache.py", "content": "from abc import ABC, abstractmethod\nfrom typing import List, Dict, Any\nfrom core.models.completion import CompletionResponse\nfrom core.models.documents import Document\n\n\nclass BaseCache(ABC):\n    \"\"\"Base class for cache implementations.\n\n    This class defines the interface for cache implementations that support\n    document ingestion and cache-augmented querying.\n    \"\"\"\n\n    def __init__(\n        self, name: str, model: str, gguf_file: str, filters: Dict[str, Any], docs: List[Document]\n    ):\n        \"\"\"Initialize the cache with the given parameters.\n\n        Args:\n            name: Name of the cache instance\n            model: Model identifier\n            gguf_file: Path to the GGUF model file\n            filters: Filters used to create the cache context\n            docs: Initial documents to ingest into the cache\n        \"\"\"\n        self.name = name\n        self.filters = filters\n        self.docs = []  # List of document IDs that have been ingested\n        self._initialize(model, gguf_file, docs)\n\n    @abstractmethod\n    def _initialize(self, model: str, gguf_file: str, docs: List[Document]) -> None:\n        \"\"\"Internal initialization method to be implemented by subclasses.\"\"\"\n        pass\n\n    @abstractmethod\n    async def add_docs(self, docs: List[Document]) -> bool:\n        \"\"\"Add documents to the cache.\n\n        Args:\n            docs: List of documents to add to the cache\n\n        Returns:\n            bool: True if documents were successfully added\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def query(self, query: str) -> CompletionResponse:\n        \"\"\"Query the cache for relevant documents and generate a response.\n\n        Args:\n            query: Query string to search for relevant documents\n\n        Returns:\n            CompletionResponse: Generated response based on cached context\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def saveable_state(self) -> bytes:\n        \"\"\"Get the saveable state of the cache as bytes.\n\n        Returns:\n            bytes: Serialized state that can be used to restore the cache\n        \"\"\"\n        pass\n"}
{"type": "source_file", "path": "core/cache/base_cache_factory.py", "content": "from abc import ABC, abstractmethod\nfrom pathlib import Path\nfrom typing import Dict, Any\nfrom .base_cache import BaseCache\n\n\nclass BaseCacheFactory(ABC):\n    \"\"\"Abstract base factory for creating and loading caches.\"\"\"\n\n    def __init__(self, storage_path: Path):\n        \"\"\"Initialize the cache factory.\n\n        Args:\n            storage_path: Base path for storing cache files\n        \"\"\"\n        self.storage_path = storage_path\n        self.storage_path.mkdir(parents=True, exist_ok=True)\n\n    @abstractmethod\n    def create_new_cache(\n        self, name: str, model: str, model_file: str, **kwargs: Dict[str, Any]\n    ) -> BaseCache:\n        \"\"\"Create a new cache instance.\n\n        Args:\n            name: Name of the cache\n            model: Name/type of the model to use\n            model_file: Path or identifier for the model file\n            **kwargs: Additional arguments for cache creation\n\n        Returns:\n            BaseCache: The created cache instance\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def load_cache_from_bytes(\n        self, name: str, cache_bytes: bytes, metadata: Dict[str, Any], **kwargs: Dict[str, Any]\n    ) -> BaseCache:\n        \"\"\"Load a cache from its serialized bytes.\n\n        Args:\n            name: Name of the cache\n            cache_bytes: Serialized cache data\n            metadata: Cache metadata including model info\n            **kwargs: Additional arguments for cache loading\n\n        Returns:\n            BaseCache: The loaded cache instance\n        \"\"\"\n        pass\n\n    def get_cache_path(self, name: str) -> Path:\n        \"\"\"Get the storage path for a cache.\n\n        Args:\n            name: Name of the cache\n\n        Returns:\n            Path: Directory path for the cache\n        \"\"\"\n        path = self.storage_path / name\n        path.mkdir(parents=True, exist_ok=True)\n        return path\n"}
{"type": "source_file", "path": "core/cache/hf_cache.py", "content": "# hugging face cache implementation.\n\nfrom core.cache.base_cache import BaseCache\nfrom typing import List, Optional, Union\nfrom pathlib import Path\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom transformers.cache_utils import DynamicCache\nfrom core.models.completion import CompletionRequest, CompletionResponse\n\n\nclass HuggingFaceCache(BaseCache):\n    \"\"\"Hugging Face Cache implementation for cache-augmented generation\"\"\"\n\n    def __init__(\n        self,\n        cache_path: Path,\n        model_name: str = \"distilgpt2\",\n        device: str = \"cpu\",\n        default_max_new_tokens: int = 100,\n        use_fp16: bool = False,\n    ):\n        \"\"\"Initialize the HuggingFace cache.\n\n        Args:\n            cache_path: Path to store cache files\n            model_name: Name of the HuggingFace model to use\n            device: Device to run the model on (e.g. \"cpu\", \"cuda\", \"mps\")\n            default_max_new_tokens: Default maximum number of new tokens to generate\n            use_fp16: Whether to use FP16 precision\n        \"\"\"\n        super().__init__()\n        self.cache_path = cache_path\n        self.model_name = model_name\n        self.device = device\n        self.default_max_new_tokens = default_max_new_tokens\n        self.use_fp16 = use_fp16\n\n        # Initialize tokenizer and model\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n        # Configure model loading based on device\n        model_kwargs = {\"low_cpu_mem_usage\": True}\n\n        if device == \"cpu\":\n            # For CPU, use standard loading\n            model_kwargs.update({\"torch_dtype\": torch.float32})\n            self.model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs).to(device)\n        else:\n            # For GPU/MPS, use automatic device mapping and optional FP16\n            model_kwargs.update(\n                {\"device_map\": \"auto\", \"torch_dtype\": torch.float16 if use_fp16 else torch.float32}\n            )\n            self.model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n\n        self.kv_cache = None\n        self.origin_len = None\n\n    def get_kv_cache(self, prompt: str) -> DynamicCache:\n        \"\"\"Build KV cache from prompt\"\"\"\n        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(self.device)\n        cache = DynamicCache()\n\n        with torch.no_grad():\n            _ = self.model(input_ids=input_ids, past_key_values=cache, use_cache=True)\n        return cache\n\n    def clean_up_cache(self, cache: DynamicCache, origin_len: int):\n        \"\"\"Clean up cache by removing appended tokens\"\"\"\n        for i in range(len(cache.key_cache)):\n            cache.key_cache[i] = cache.key_cache[i][:, :, :origin_len, :]\n            cache.value_cache[i] = cache.value_cache[i][:, :, :origin_len, :]\n\n    def generate(\n        self, input_ids: torch.Tensor, past_key_values, max_new_tokens: Optional[int] = None\n    ) -> torch.Tensor:\n        \"\"\"Generate text using the model and cache\"\"\"\n        device = next(self.model.parameters()).device\n        origin_len = input_ids.shape[-1]\n        input_ids = input_ids.to(device)\n        output_ids = input_ids.clone()\n        next_token = input_ids\n\n        with torch.no_grad():\n            for _ in range(max_new_tokens or self.default_max_new_tokens):\n                out = self.model(\n                    input_ids=next_token, past_key_values=past_key_values, use_cache=True\n                )\n                logits = out.logits[:, -1, :]\n                token = torch.argmax(logits, dim=-1, keepdim=True)\n                output_ids = torch.cat([output_ids, token], dim=-1)\n                past_key_values = out.past_key_values\n                next_token = token.to(device)\n\n                if (\n                    self.model.config.eos_token_id is not None\n                    and token.item() == self.model.config.eos_token_id\n                ):\n                    break\n\n        return output_ids[:, origin_len:]\n\n    async def ingest(self, docs: List[str]) -> bool:\n        \"\"\"Ingest documents into cache\"\"\"\n        try:\n            # Create system prompt with documents\n            system_prompt = f\"\"\"\n<|system|>\nYou are an assistant who provides concise factual answers.\n<|user|>\nContext:\n{' '.join(docs)}\nQuestion:\n\"\"\".strip()\n\n            # Build the cache\n            input_ids = self.tokenizer(system_prompt, return_tensors=\"pt\").input_ids.to(self.device)\n            self.kv_cache = DynamicCache()\n\n            with torch.no_grad():\n                # First run to get the cache shape\n                outputs = self.model(input_ids=input_ids, use_cache=True)\n                # Initialize cache with empty tensors of the right shape\n                n_layers = len(outputs.past_key_values)\n                batch_size = input_ids.shape[0]\n\n                # Handle different model architectures\n\n                if hasattr(self.model.config, \"num_key_value_heads\"):\n                    # Models with grouped query attention (GQA) like Llama\n                    n_kv_heads = self.model.config.num_key_value_heads\n                    head_dim = self.model.config.head_dim\n                elif hasattr(self.model.config, \"n_head\"):\n                    # GPT-style models\n                    n_kv_heads = self.model.config.n_head\n                    head_dim = self.model.config.n_embd // self.model.config.n_head\n                elif hasattr(self.model.config, \"num_attention_heads\"):\n                    # OPT-style models\n                    n_kv_heads = self.model.config.num_attention_heads\n                    head_dim = (\n                        self.model.config.hidden_size // self.model.config.num_attention_heads\n                    )\n                else:\n                    raise ValueError(\n                        f\"Unsupported model architecture: {self.model.config.model_type}\"\n                    )\n\n                seq_len = input_ids.shape[1]\n\n                for i in range(n_layers):\n                    key_shape = (batch_size, n_kv_heads, seq_len, head_dim)\n                    value_shape = key_shape\n                    self.kv_cache.key_cache.append(torch.zeros(key_shape, device=self.device))\n                    self.kv_cache.value_cache.append(torch.zeros(value_shape, device=self.device))\n\n                # Now run with the initialized cache\n                outputs = self.model(\n                    input_ids=input_ids, past_key_values=self.kv_cache, use_cache=True\n                )\n                # Update cache with actual values\n                self.kv_cache.key_cache = [layer[0] for layer in outputs.past_key_values]\n                self.kv_cache.value_cache = [layer[1] for layer in outputs.past_key_values]\n                self.origin_len = self.kv_cache.key_cache[0].shape[-2]\n            return True\n        except Exception as e:\n            print(f\"Error ingesting documents: {e}\")\n            return False\n\n    async def update(self, new_doc: str) -> bool:\n        \"\"\"Update cache with new document\"\"\"\n        try:\n            if self.kv_cache is None:\n                return await self.ingest([new_doc])\n\n            # Clean up existing cache\n            self.clean_up_cache(self.kv_cache, self.origin_len)\n\n            # Add new document to cache\n            input_ids = self.tokenizer(new_doc + \"\\n\", return_tensors=\"pt\").input_ids.to(\n                self.device\n            )\n\n            # First run to get the cache shape\n            outputs = self.model(input_ids=input_ids, use_cache=True)\n            # Initialize cache with empty tensors of the right shape\n            n_layers = len(outputs.past_key_values)\n            batch_size = input_ids.shape[0]\n\n            # Handle different model architectures\n            if hasattr(self.model.config, \"num_key_value_heads\"):\n                # Models with grouped query attention (GQA) like Llama\n                n_kv_heads = self.model.config.num_key_value_heads\n                head_dim = self.model.config.head_dim\n            elif hasattr(self.model.config, \"n_head\"):\n                # GPT-style models\n                n_kv_heads = self.model.config.n_head\n                head_dim = self.model.config.n_embd // self.model.config.n_head\n            elif hasattr(self.model.config, \"num_attention_heads\"):\n                # OPT-style models\n                n_kv_heads = self.model.config.num_attention_heads\n                head_dim = self.model.config.hidden_size // self.model.config.num_attention_heads\n            else:\n                raise ValueError(f\"Unsupported model architecture: {self.model.config.model_type}\")\n\n            seq_len = input_ids.shape[1]\n\n            # Create a new cache for the update\n            new_cache = DynamicCache()\n            for i in range(n_layers):\n                key_shape = (batch_size, n_kv_heads, seq_len, head_dim)\n                value_shape = key_shape\n                new_cache.key_cache.append(torch.zeros(key_shape, device=self.device))\n                new_cache.value_cache.append(torch.zeros(value_shape, device=self.device))\n\n            # Run with the initialized cache\n            outputs = self.model(input_ids=input_ids, past_key_values=new_cache, use_cache=True)\n            # Update cache with actual values\n            self.kv_cache.key_cache = [layer[0] for layer in outputs.past_key_values]\n            self.kv_cache.value_cache = [layer[1] for layer in outputs.past_key_values]\n            return True\n        except Exception as e:\n            print(f\"Error updating cache: {e}\")\n            return False\n\n    async def complete(self, request: CompletionRequest) -> CompletionResponse:\n        \"\"\"Generate completion using cache-augmented generation\"\"\"\n        try:\n            if self.kv_cache is None:\n                raise ValueError(\"Cache not initialized. Please ingest documents first.\")\n\n            # Clean up cache\n            self.clean_up_cache(self.kv_cache, self.origin_len)\n\n            # Generate completion\n            input_ids = self.tokenizer(request.query + \"\\n\", return_tensors=\"pt\").input_ids.to(\n                self.device\n            )\n            gen_ids = self.generate(input_ids, self.kv_cache, max_new_tokens=request.max_tokens)\n            completion = self.tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n\n            # Calculate token usage\n            usage = {\n                \"prompt_tokens\": len(input_ids[0]),\n                \"completion_tokens\": len(gen_ids[0]),\n                \"total_tokens\": len(input_ids[0]) + len(gen_ids[0]),\n            }\n\n            return CompletionResponse(completion=completion, usage=usage)\n        except Exception as e:\n            print(f\"Error generating completion: {e}\")\n            return CompletionResponse(\n                completion=f\"Error: {str(e)}\",\n                usage={\"prompt_tokens\": 0, \"completion_tokens\": 0, \"total_tokens\": 0},\n            )\n\n    def save_cache(self) -> Path:\n        \"\"\"Save the KV cache to disk\"\"\"\n        if self.kv_cache is None:\n            raise ValueError(\"No cache to save\")\n\n        cache_dir = self.cache_path / \"kv_cache\"\n        cache_dir.mkdir(parents=True, exist_ok=True)\n\n        # Save key and value caches\n        cache_data = {\n            \"key_cache\": self.kv_cache.key_cache,\n            \"value_cache\": self.kv_cache.value_cache,\n            \"origin_len\": self.origin_len,\n        }\n        cache_path = cache_dir / \"cache.pt\"\n        torch.save(cache_data, cache_path)\n        return cache_path\n\n    def load_cache(self, cache_path: Union[str, Path]) -> None:\n        \"\"\"Load KV cache from disk\"\"\"\n        cache_path = Path(cache_path)\n        if not cache_path.exists():\n            raise FileNotFoundError(f\"Cache file not found at {cache_path}\")\n\n        cache_data = torch.load(cache_path, map_location=self.device)\n\n        self.kv_cache = DynamicCache()\n        self.kv_cache.key_cache = cache_data[\"key_cache\"]\n        self.kv_cache.value_cache = cache_data[\"value_cache\"]\n        self.origin_len = cache_data[\"origin_len\"]\n"}
{"type": "source_file", "path": "core/cache/llama_cache.py", "content": "import json\nimport pickle\nimport logging\nfrom core.cache.base_cache import BaseCache\nfrom typing import Dict, Any, List\nfrom core.models.completion import CompletionResponse\nfrom core.models.documents import Document\nfrom llama_cpp import Llama\n\nlogger = logging.getLogger(__name__)\n\nINITIAL_SYSTEM_PROMPT = \"\"\"<|im_start|>system\nYou are a helpful AI assistant with access to provided documents. Your role is to:\n1. Answer questions accurately based on the documents provided\n2. Stay focused on the document content and avoid speculation\n3. Admit when you don't have enough information to answer\n4. Be clear and concise in your responses\n5. Use direct quotes from documents when relevant\n\nProvided documents: {documents}\n<|im_end|>\n\"\"\".strip()\n\nADD_DOC_SYSTEM_PROMPT = \"\"\"<|im_start|>system\nI'm adding some additional documents for your reference:\n{documents}\n\nPlease incorporate this new information along with what you already know from previous documents while maintaining the same guidelines for responses.\n<|im_end|>\n\"\"\".strip()\n\nQUERY_PROMPT = \"\"\"<|im_start|>user\n{query}\n<|im_end|>\n<|im_start|>assistant\n\"\"\".strip()\n\n\nclass LlamaCache(BaseCache):\n    def __init__(\n        self,\n        name: str,\n        model: str,\n        gguf_file: str,\n        filters: Dict[str, Any],\n        docs: List[Document],\n        **kwargs,\n    ):\n        logger.info(f\"Initializing LlamaCache with name={name}, model={model}\")\n        # cache related\n        self.name = name\n        self.model = model\n        self.filters = filters\n        self.docs = docs\n\n        # llama specific\n        self.gguf_file = gguf_file\n        self.n_gpu_layers = kwargs.get(\"n_gpu_layers\", -1)\n        logger.info(f\"Using {self.n_gpu_layers} GPU layers\")\n\n        # late init (when we call _initialize)\n        self.llama = None\n        self.state = None\n        self.cached_tokens = 0\n\n        self._initialize(model, gguf_file, docs)\n        logger.info(\"LlamaCache initialization complete\")\n\n    def _initialize(self, model: str, gguf_file: str, docs: List[Document]) -> None:\n        logger.info(f\"Loading Llama model from {model} with file {gguf_file}\")\n        try:\n            # Set a reasonable default context size (32K tokens)\n            default_ctx_size = 32768\n\n            self.llama = Llama.from_pretrained(\n                repo_id=model,\n                filename=gguf_file,\n                n_gpu_layers=self.n_gpu_layers,\n                n_ctx=default_ctx_size,\n                verbose=False,  # Enable verbose mode for better error reporting\n            )\n            logger.info(\"Model loaded successfully\")\n\n            # Format and tokenize system prompt\n            documents = \"\\n\".join(doc.system_metadata.get(\"content\", \"\") for doc in docs)\n            system_prompt = INITIAL_SYSTEM_PROMPT.format(documents=documents)\n            logger.info(f\"Built system prompt: {system_prompt[:200]}...\")\n\n            try:\n                tokens = self.llama.tokenize(system_prompt.encode())\n                logger.info(f\"System prompt tokenized to {len(tokens)} tokens\")\n\n                # Process tokens to build KV cache\n                logger.info(\"Evaluating system prompt\")\n                self.llama.eval(tokens)\n                logger.info(\"Saving initial KV cache state\")\n                self.state = self.llama.save_state()\n                self.cached_tokens = len(tokens)\n                logger.info(f\"Initial KV cache built with {self.cached_tokens} tokens\")\n            except Exception as e:\n                logger.error(f\"Error during prompt processing: {str(e)}\")\n                raise ValueError(f\"Failed to process system prompt: {str(e)}\")\n\n        except Exception as e:\n            logger.error(f\"Failed to initialize Llama model: {str(e)}\")\n            raise ValueError(f\"Failed to initialize Llama model: {str(e)}\")\n\n    def add_docs(self, docs: List[Document]) -> bool:\n        logger.info(f\"Adding {len(docs)} new documents to cache\")\n        documents = \"\\n\".join(doc.system_metadata.get(\"content\", \"\") for doc in docs)\n        system_prompt = ADD_DOC_SYSTEM_PROMPT.format(documents=documents)\n\n        # Tokenize and process\n        new_tokens = self.llama.tokenize(system_prompt.encode())\n        self.llama.eval(new_tokens)\n        self.state = self.llama.save_state()\n        self.cached_tokens += len(new_tokens)\n        logger.info(f\"Added {len(new_tokens)} tokens, total: {self.cached_tokens}\")\n        return True\n\n    def query(self, query: str) -> CompletionResponse:\n        # Format query with proper chat template\n        formatted_query = QUERY_PROMPT.format(query=query)\n        logger.info(f\"Processing query: {formatted_query}\")\n\n        # Reset and load cached state\n        self.llama.reset()\n        self.llama.load_state(self.state)\n        logger.info(f\"Loaded state with {self.state.n_tokens} tokens\")\n        # print(f\"Loaded state with {self.state.n_tokens} tokens\", file=sys.stderr)\n        # Tokenize and process query\n        query_tokens = self.llama.tokenize(formatted_query.encode())\n        self.llama.eval(query_tokens)\n        logger.info(f\"Evaluated query tokens: {query_tokens}\")\n        # print(f\"Evaluated query tokens: {query_tokens}\", file=sys.stderr)\n\n        # Generate response\n        output_tokens = []\n        for token in self.llama.generate(tokens=[], reset=False):\n            output_tokens.append(token)\n            # Stop generation when EOT token is encountered\n            if token == self.llama.token_eos():\n                break\n\n        # Decode and return\n        completion = self.llama.detokenize(output_tokens).decode()\n        logger.info(f\"Generated completion: {completion}\")\n\n        return CompletionResponse(\n            completion=completion,\n            usage={\"prompt_tokens\": self.cached_tokens, \"completion_tokens\": len(output_tokens)},\n        )\n\n    @property\n    def saveable_state(self) -> bytes:\n        logger.info(\"Serializing cache state\")\n        state_bytes = pickle.dumps(self.state)\n        logger.info(f\"Serialized state size: {len(state_bytes)} bytes\")\n        return state_bytes\n\n    @classmethod\n    def from_bytes(\n        cls, name: str, cache_bytes: bytes, metadata: Dict[str, Any], **kwargs\n    ) -> \"LlamaCache\":\n        \"\"\"Load a cache from its serialized state.\n\n        Args:\n            name: Name of the cache\n            cache_bytes: Pickled state bytes\n            metadata: Cache metadata including model info\n            **kwargs: Additional arguments\n\n        Returns:\n            LlamaCache: Loaded cache instance\n        \"\"\"\n        logger.info(f\"Loading cache from bytes with name={name}\")\n        logger.info(f\"Cache metadata: {metadata}\")\n        # Create new instance with metadata\n        # logger.info(f\"Docs: {metadata['docs']}\")\n        docs = [json.loads(doc) for doc in metadata[\"docs\"]]\n        # time.sleep(10)\n        cache = cls(\n            name=name,\n            model=metadata[\"model\"],\n            gguf_file=metadata[\"model_file\"],\n            filters=metadata[\"filters\"],\n            docs=[Document(**doc) for doc in docs],\n        )\n\n        # Load the saved state\n        logger.info(f\"Loading saved KV cache state of size {len(cache_bytes)} bytes\")\n        cache.state = pickle.loads(cache_bytes)\n        cache.llama.load_state(cache.state)\n        logger.info(\"Cache successfully loaded from bytes\")\n\n        return cache\n"}
{"type": "source_file", "path": "core/completion/__init__.py", "content": ""}
{"type": "source_file", "path": "core/cache/llama_cache_factory.py", "content": "from core.cache.base_cache_factory import BaseCacheFactory\nfrom core.cache.llama_cache import LlamaCache\nfrom typing import Dict, Any\n\n\nclass LlamaCacheFactory(BaseCacheFactory):\n    def create_new_cache(\n        self, name: str, model: str, model_file: str, **kwargs: Dict[str, Any]\n    ) -> LlamaCache:\n        return LlamaCache(name, model, model_file, **kwargs)\n\n    def load_cache_from_bytes(\n        self, name: str, cache_bytes: bytes, metadata: Dict[str, Any], **kwargs: Dict[str, Any]\n    ) -> LlamaCache:\n        return LlamaCache.from_bytes(name, cache_bytes, metadata, **kwargs)\n"}
{"type": "source_file", "path": "core/completion/openai_completion.py", "content": "from .base_completion import BaseCompletionModel\nfrom core.models.completion import CompletionRequest, CompletionResponse\n\n\nclass OpenAICompletionModel(BaseCompletionModel):\n    \"\"\"OpenAI completion model implementation\"\"\"\n\n    def __init__(self, model_name: str):\n        self.model_name = model_name\n        # Import here to avoid dependency if not using OpenAI\n        from openai import AsyncOpenAI\n\n        self.client = AsyncOpenAI()\n\n    async def complete(self, request: CompletionRequest) -> CompletionResponse:\n        \"\"\"Generate completion using OpenAI API\"\"\"\n        # Process context chunks and handle images\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant. Use the provided context to answer questions accurately.\",\n            }\n        ]\n\n        # Build user message content\n        user_message_content = []\n        context_text = []\n\n        for chunk in request.context_chunks:\n            if chunk.startswith(\"data:image/\"):\n                # Handle image data URI\n                user_message_content.append({\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": chunk,\n                    }\n                })\n            else:\n                context_text.append(chunk)\n        \n        max_num_images = min(3, len(user_message_content))\n        user_message_content = user_message_content[:max_num_images] # limit the number of images to 3\n\n        context = \"\\n\" + \"\\n\\n\".join(context_text) + \"\\n\\n\"\n\n        # Add text context if any\n        if context_text:\n            user_message_content.insert(0, {\n                \"type\": \"text\",\n                \"text\": f\"Context: {context} Question: {request.query}\"\n            })\n        else:\n            user_message_content.insert(0, {\n                \"type\": \"text\",\n                \"text\": f\"{request.query}\"\n            })\n\n        messages.append({\n            \"role\": \"user\",\n            \"content\": user_message_content\n        })\n\n        # Call OpenAI API\n        response = await self.client.chat.completions.create(\n            model=self.model_name,\n            messages=messages,\n            max_tokens=request.max_tokens,\n            temperature=request.temperature,\n        )\n\n        return CompletionResponse(\n            completion=response.choices[0].message.content,\n            usage={\n                \"prompt_tokens\": response.usage.prompt_tokens,\n                \"completion_tokens\": response.usage.completion_tokens,\n                \"total_tokens\": response.usage.total_tokens,\n            },\n        )\n"}
{"type": "source_file", "path": "core/completion/base_completion.py", "content": "from abc import ABC, abstractmethod\nfrom core.models.completion import CompletionRequest, CompletionResponse\n\n\nclass BaseCompletionModel(ABC):\n    \"\"\"Base class for completion models\"\"\"\n\n    @abstractmethod\n    async def complete(self, request: CompletionRequest) -> CompletionResponse:\n        \"\"\"Generate completion from query and context\"\"\"\n        pass\n"}
{"type": "source_file", "path": "core/config.py", "content": "import os\nfrom typing import Literal, Optional\nfrom pydantic_settings import BaseSettings\nfrom functools import lru_cache\nimport tomli\nfrom dotenv import load_dotenv\nfrom collections import ChainMap\n\n\nclass Settings(BaseSettings):\n    \"\"\"DataBridge configuration settings.\"\"\"\n\n    # Environment variables\n    JWT_SECRET_KEY: str\n    POSTGRES_URI: Optional[str] = None\n    MONGODB_URI: Optional[str] = None\n    UNSTRUCTURED_API_KEY: Optional[str] = None\n    AWS_ACCESS_KEY: Optional[str] = None\n    AWS_SECRET_ACCESS_KEY: Optional[str] = None\n    OPENAI_API_KEY: Optional[str] = None\n    ANTHROPIC_API_KEY: Optional[str] = None\n    ASSEMBLYAI_API_KEY: Optional[str] = None\n\n    # API configuration\n    HOST: str\n    PORT: int\n    RELOAD: bool\n\n    # Auth configuration\n    JWT_ALGORITHM: str\n    dev_mode: bool = False\n    dev_entity_type: str = \"developer\"\n    dev_entity_id: str = \"dev_user\"\n    dev_permissions: list = [\"read\", \"write\", \"admin\"]\n\n    # Completion configuration\n    COMPLETION_PROVIDER: Literal[\"ollama\", \"openai\"]\n    COMPLETION_MODEL: str\n    COMPLETION_MAX_TOKENS: Optional[str] = None\n    COMPLETION_TEMPERATURE: Optional[float] = None\n    COMPLETION_OLLAMA_BASE_URL: Optional[str] = None\n\n    # Database configuration\n    DATABASE_PROVIDER: Literal[\"postgres\", \"mongodb\"]\n    DATABASE_NAME: Optional[str] = None\n    DOCUMENTS_COLLECTION: Optional[str] = None\n\n    # Embedding configuration\n    EMBEDDING_PROVIDER: Literal[\"ollama\", \"openai\"]\n    EMBEDDING_MODEL: str\n    VECTOR_DIMENSIONS: int\n    EMBEDDING_SIMILARITY_METRIC: Literal[\"cosine\", \"dotProduct\"]\n    EMBEDDING_OLLAMA_BASE_URL: Optional[str] = None\n\n    # Parser configuration\n    CHUNK_SIZE: int\n    CHUNK_OVERLAP: int\n    USE_UNSTRUCTURED_API: bool\n    FRAME_SAMPLE_RATE: Optional[int] = None\n    USE_CONTEXTUAL_CHUNKING: bool = False\n\n    # Rules configuration\n    RULES_PROVIDER: Literal[\"ollama\", \"openai\"]\n    RULES_MODEL: str\n    RULES_BATCH_SIZE: int = 4096\n\n    # Graph configuration\n    GRAPH_PROVIDER: Literal[\"ollama\", \"openai\"]\n    GRAPH_MODEL: str\n\n    # Reranker configuration\n    USE_RERANKING: bool\n    RERANKER_PROVIDER: Optional[Literal[\"flag\"]] = None\n    RERANKER_MODEL: Optional[str] = None\n    RERANKER_QUERY_MAX_LENGTH: Optional[int] = None\n    RERANKER_PASSAGE_MAX_LENGTH: Optional[int] = None\n    RERANKER_USE_FP16: Optional[bool] = None\n    RERANKER_DEVICE: Optional[str] = None\n\n    # Storage configuration\n    STORAGE_PROVIDER: Literal[\"local\", \"aws-s3\"]\n    STORAGE_PATH: Optional[str] = None\n    AWS_REGION: Optional[str] = None\n    S3_BUCKET: Optional[str] = None\n\n    # Vector store configuration\n    VECTOR_STORE_PROVIDER: Literal[\"pgvector\", \"mongodb\"]\n    VECTOR_STORE_DATABASE_NAME: Optional[str] = None\n    VECTOR_STORE_COLLECTION_NAME: Optional[str] = None\n\n    # Colpali configuration\n    ENABLE_COLPALI: bool\n    \n    # Telemetry configuration\n    TELEMETRY_ENABLED: bool = True\n    HONEYCOMB_ENABLED: bool = True\n    HONEYCOMB_ENDPOINT: str = \"https://api.honeycomb.io\"\n    HONEYCOMB_PROXY_ENDPOINT: str = \"https://otel-proxy.onrender.com/\"\n    SERVICE_NAME: str = \"databridge-core\"\n    OTLP_TIMEOUT: int = 10\n    OTLP_MAX_RETRIES: int = 3\n    OTLP_RETRY_DELAY: int = 1\n    OTLP_MAX_EXPORT_BATCH_SIZE: int = 512\n    OTLP_SCHEDULE_DELAY_MILLIS: int = 5000\n    OTLP_MAX_QUEUE_SIZE: int = 2048\n\n\n@lru_cache()\ndef get_settings() -> Settings:\n    \"\"\"Get cached settings instance.\"\"\"\n    load_dotenv(override=True)\n\n    # Load config.toml\n    with open(\"databridge.toml\", \"rb\") as f:\n        config = tomli.load(f)\n\n    em = \"'{missing_value}' needed if '{field}' is set to '{value}'\"\n    # load api config\n    api_config = {\n        \"HOST\": config[\"api\"][\"host\"],\n        \"PORT\": int(config[\"api\"][\"port\"]),\n        \"RELOAD\": bool(config[\"api\"][\"reload\"]),\n    }\n\n    # load auth config\n    auth_config = {\n        \"JWT_ALGORITHM\": config[\"auth\"][\"jwt_algorithm\"],\n        \"JWT_SECRET_KEY\": os.environ.get(\n            \"JWT_SECRET_KEY\", \"dev-secret-key\"\n        ),  # Default for dev mode\n        \"dev_mode\": config[\"auth\"].get(\"dev_mode\", False),\n        \"dev_entity_type\": config[\"auth\"].get(\"dev_entity_type\", \"developer\"),\n        \"dev_entity_id\": config[\"auth\"].get(\"dev_entity_id\", \"dev_user\"),\n        \"dev_permissions\": config[\"auth\"].get(\"dev_permissions\", [\"read\", \"write\", \"admin\"]),\n    }\n\n    # Only require JWT_SECRET_KEY in non-dev mode\n    if not auth_config[\"dev_mode\"] and \"JWT_SECRET_KEY\" not in os.environ:\n        raise ValueError(\"JWT_SECRET_KEY is required when dev_mode is disabled\")\n\n    # load completion config\n    completion_config = {\n        \"COMPLETION_PROVIDER\": config[\"completion\"][\"provider\"],\n        \"COMPLETION_MODEL\": config[\"completion\"][\"model_name\"],\n    }\n    match completion_config[\"COMPLETION_PROVIDER\"]:\n        case \"openai\" if \"OPENAI_API_KEY\" in os.environ:\n            completion_config.update({\"OPENAI_API_KEY\": os.environ[\"OPENAI_API_KEY\"]})\n        case \"openai\":\n            msg = em.format(\n                missing_value=\"OPENAI_API_KEY\", field=\"completion.provider\", value=\"openai\"\n            )\n            raise ValueError(msg)\n        case \"ollama\" if \"base_url\" in config[\"completion\"]:\n            completion_config.update(\n                {\"COMPLETION_OLLAMA_BASE_URL\": config[\"completion\"][\"base_url\"]}\n            )\n        case \"ollama\":\n            msg = em.format(missing_value=\"base_url\", field=\"completion.provider\", value=\"ollama\")\n            raise ValueError(msg)\n        case _:\n            prov = completion_config[\"COMPLETION_PROVIDER\"]\n            raise ValueError(f\"Unknown completion provider selected: '{prov}'\")\n\n    # load database config\n    database_config = {\"DATABASE_PROVIDER\": config[\"database\"][\"provider\"]}\n    match database_config[\"DATABASE_PROVIDER\"]:\n        case \"mongodb\":\n            database_config.update(\n                {\n                    \"DATABASE_NAME\": config[\"database\"][\"database_name\"],\n                    \"COLLECTION_NAME\": config[\"database\"][\"collection_name\"],\n                }\n            )\n        case \"postgres\" if \"POSTGRES_URI\" in os.environ:\n            database_config.update({\"POSTGRES_URI\": os.environ[\"POSTGRES_URI\"]})\n        case \"postgres\":\n            msg = em.format(\n                missing_value=\"POSTGRES_URI\", field=\"database.provider\", value=\"postgres\"\n            )\n            raise ValueError(msg)\n        case _:\n            prov = database_config[\"DATABASE_PROVIDER\"]\n            raise ValueError(f\"Unknown database provider selected: '{prov}'\")\n\n    # load embedding config\n    embedding_config = {\n        \"EMBEDDING_PROVIDER\": config[\"embedding\"][\"provider\"],\n        \"EMBEDDING_MODEL\": config[\"embedding\"][\"model_name\"],\n        \"VECTOR_DIMENSIONS\": config[\"embedding\"][\"dimensions\"],\n        \"EMBEDDING_SIMILARITY_METRIC\": config[\"embedding\"][\"similarity_metric\"],\n    }\n    match embedding_config[\"EMBEDDING_PROVIDER\"]:\n        case \"openai\" if \"OPENAI_API_KEY\" in os.environ:\n            embedding_config.update({\"OPENAI_API_KEY\": os.environ[\"OPENAI_API_KEY\"]})\n        case \"openai\":\n            msg = em.format(\n                missing_value=\"OPENAI_API_KEY\", field=\"embedding.provider\", value=\"openai\"\n            )\n            raise ValueError(msg)\n        case \"ollama\" if \"base_url\" in config[\"embedding\"]:\n            embedding_config.update({\"EMBEDDING_OLLAMA_BASE_URL\": config[\"embedding\"][\"base_url\"]})\n        case \"ollama\":\n            msg = em.format(missing_value=\"base_url\", field=\"embedding.provider\", value=\"ollama\")\n            raise ValueError(msg)\n        case _:\n            prov = embedding_config[\"EMBEDDING_PROVIDER\"]\n            raise ValueError(f\"Unknown embedding provider selected: '{prov}'\")\n\n    # load parser config\n    parser_config = {\n        \"CHUNK_SIZE\": config[\"parser\"][\"chunk_size\"],\n        \"CHUNK_OVERLAP\": config[\"parser\"][\"chunk_overlap\"],\n        \"USE_UNSTRUCTURED_API\": config[\"parser\"][\"use_unstructured_api\"],\n        \"USE_CONTEXTUAL_CHUNKING\": config[\"parser\"].get(\"use_contextual_chunking\", False),\n    }\n    if parser_config[\"USE_UNSTRUCTURED_API\"] and \"UNSTRUCTURED_API_KEY\" not in os.environ:\n        msg = em.format(\n            missing_value=\"UNSTRUCTURED_API_KEY\", field=\"parser.use_unstructured_api\", value=\"true\"\n        )\n        raise ValueError(msg)\n    elif parser_config[\"USE_UNSTRUCTURED_API\"]:\n        parser_config.update({\"UNSTRUCTURED_API_KEY\": os.environ[\"UNSTRUCTURED_API_KEY\"]})\n\n    # load reranker config\n    reranker_config = {\"USE_RERANKING\": config[\"reranker\"][\"use_reranker\"]}\n    if reranker_config[\"USE_RERANKING\"]:\n        reranker_config.update(\n            {\n                \"RERANKER_PROVIDER\": config[\"reranker\"][\"provider\"],\n                \"RERANKER_MODEL\": config[\"reranker\"][\"model_name\"],\n                \"RERANKER_QUERY_MAX_LENGTH\": config[\"reranker\"][\"query_max_length\"],\n                \"RERANKER_PASSAGE_MAX_LENGTH\": config[\"reranker\"][\"passage_max_length\"],\n                \"RERANKER_USE_FP16\": config[\"reranker\"][\"use_fp16\"],\n                \"RERANKER_DEVICE\": config[\"reranker\"][\"device\"],\n            }\n        )\n\n    # load storage config\n    storage_config = {\"STORAGE_PROVIDER\": config[\"storage\"][\"provider\"], \"STORAGE_PATH\": config[\"storage\"][\"storage_path\"]}\n    match storage_config[\"STORAGE_PROVIDER\"]:\n        case \"local\":\n            storage_config.update({\"STORAGE_PATH\": config[\"storage\"][\"storage_path\"]})\n        case \"aws-s3\" if all(\n            key in os.environ for key in [\"AWS_ACCESS_KEY\", \"AWS_SECRET_ACCESS_KEY\"]\n        ):\n            storage_config.update(\n                {\n                    \"AWS_REGION\": config[\"storage\"][\"region\"],\n                    \"S3_BUCKET\": config[\"storage\"][\"bucket_name\"],\n                    \"AWS_ACCESS_KEY\": os.environ[\"AWS_ACCESS_KEY\"],\n                    \"AWS_SECRET_ACCESS_KEY\": os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n                }\n            )\n        case \"aws-s3\":\n            msg = em.format(\n                missing_value=\"AWS credentials\", field=\"storage.provider\", value=\"aws-s3\"\n            )\n            raise ValueError(msg)\n        case _:\n            prov = storage_config[\"STORAGE_PROVIDER\"]\n            raise ValueError(f\"Unknown storage provider selected: '{prov}'\")\n\n    # load vector store config\n    vector_store_config = {\"VECTOR_STORE_PROVIDER\": config[\"vector_store\"][\"provider\"]}\n    match vector_store_config[\"VECTOR_STORE_PROVIDER\"]:\n        case \"mongodb\":\n            vector_store_config.update(\n                {\n                    \"VECTOR_STORE_DATABASE_NAME\": config[\"vector_store\"][\"database_name\"],\n                    \"VECTOR_STORE_COLLECTION_NAME\": config[\"vector_store\"][\"collection_name\"],\n                }\n            )\n        case \"pgvector\":\n            if \"POSTGRES_URI\" not in os.environ:\n                msg = em.format(\n                    missing_value=\"POSTGRES_URI\", field=\"vector_store.provider\", value=\"pgvector\"\n                )\n                raise ValueError(msg)\n        case _:\n            prov = vector_store_config[\"VECTOR_STORE_PROVIDER\"]\n            raise ValueError(f\"Unknown vector store provider selected: '{prov}'\")\n\n    # load rules config - simplified\n    rules_config = {\n        \"RULES_PROVIDER\": config[\"rules\"][\"provider\"],\n        \"RULES_MODEL\": config[\"rules\"][\"model_name\"],\n        \"RULES_BATCH_SIZE\": config[\"rules\"][\"batch_size\"],\n    }\n\n    # load databridge config\n    databridge_config = {\n        \"ENABLE_COLPALI\": config[\"databridge\"][\"enable_colpali\"],\n    }\n\n    # load graph config\n    graph_config = {\n        \"GRAPH_PROVIDER\": config[\"graph\"][\"provider\"],\n        \"GRAPH_MODEL\": config[\"graph\"][\"model_name\"],\n    }\n    \n    # load telemetry config\n    telemetry_config = {}\n    if \"telemetry\" in config:\n        telemetry_config = {\n            \"TELEMETRY_ENABLED\": config[\"telemetry\"].get(\"enabled\", True),\n            \"HONEYCOMB_ENABLED\": config[\"telemetry\"].get(\"honeycomb_enabled\", True),\n            \"HONEYCOMB_ENDPOINT\": config[\"telemetry\"].get(\"honeycomb_endpoint\", \"https://api.honeycomb.io\"),\n            \"SERVICE_NAME\": config[\"telemetry\"].get(\"service_name\", \"databridge-core\"),\n            \"OTLP_TIMEOUT\": config[\"telemetry\"].get(\"otlp_timeout\", 10),\n            \"OTLP_MAX_RETRIES\": config[\"telemetry\"].get(\"otlp_max_retries\", 3),\n            \"OTLP_RETRY_DELAY\": config[\"telemetry\"].get(\"otlp_retry_delay\", 1),\n            \"OTLP_MAX_EXPORT_BATCH_SIZE\": config[\"telemetry\"].get(\"otlp_max_export_batch_size\", 512),\n            \"OTLP_SCHEDULE_DELAY_MILLIS\": config[\"telemetry\"].get(\"otlp_schedule_delay_millis\", 5000),\n            \"OTLP_MAX_QUEUE_SIZE\": config[\"telemetry\"].get(\"otlp_max_queue_size\", 2048),\n        }\n    \n    settings_dict = dict(ChainMap(\n        api_config,\n        auth_config,\n        completion_config,\n        database_config,\n        embedding_config,\n        parser_config,\n        reranker_config,\n        storage_config,\n        vector_store_config,\n        rules_config,\n        databridge_config,\n        graph_config,\n        telemetry_config,\n    ))\n\n    return Settings(**settings_dict)\n"}
{"type": "source_file", "path": "core/database/base_database.py", "content": "from abc import ABC, abstractmethod\nfrom typing import List, Optional, Dict, Any\n\nfrom ..models.documents import Document\nfrom ..models.auth import AuthContext\nfrom ..models.graph import Graph\n\n\nclass BaseDatabase(ABC):\n    \"\"\"Base interface for document metadata storage.\"\"\"\n\n    @abstractmethod\n    async def store_document(self, document: Document) -> bool:\n        \"\"\"\n        Store document metadata.\n        Returns: Success status\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def get_document(self, document_id: str, auth: AuthContext) -> Optional[Document]:\n        \"\"\"\n        Retrieve document metadata by ID if user has access.\n        Returns: Document if found and accessible, None otherwise\n        \"\"\"\n        pass\n        \n    @abstractmethod\n    async def get_document_by_filename(self, filename: str, auth: AuthContext) -> Optional[Document]:\n        \"\"\"\n        Retrieve document metadata by filename if user has access.\n        If multiple documents have the same filename, returns the most recently updated one.\n        \n        Args:\n            filename: The filename to search for\n            auth: Authentication context\n            \n        Returns:\n            Document if found and accessible, None otherwise\n        \"\"\"\n        pass\n        \n    @abstractmethod\n    async def get_documents_by_id(self, document_ids: List[str], auth: AuthContext) -> List[Document]:\n        \"\"\"\n        Retrieve multiple documents by their IDs in a single batch operation.\n        Only returns documents the user has access to.\n        \n        Args:\n            document_ids: List of document IDs to retrieve\n            auth: Authentication context\n            \n        Returns:\n            List of Document objects that were found and user has access to\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def get_documents(\n        self,\n        auth: AuthContext,\n        skip: int = 0,\n        limit: int = 100,\n        filters: Optional[Dict[str, Any]] = None,\n    ) -> List[Document]:\n        \"\"\"\n        List documents the user has access to.\n        Supports pagination and filtering.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def update_document(\n        self, document_id: str, updates: Dict[str, Any], auth: AuthContext\n    ) -> bool:\n        \"\"\"\n        Update document metadata if user has access.\n        Returns: Success status\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def delete_document(self, document_id: str, auth: AuthContext) -> bool:\n        \"\"\"\n        Delete document metadata if user has admin access.\n        Returns: Success status\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def find_authorized_and_filtered_documents(\n        self, auth: AuthContext, filters: Optional[Dict[str, Any]] = None\n    ) -> List[str]:\n        \"\"\"Find document IDs matching filters that user has access to.\"\"\"\n        pass\n\n    @abstractmethod\n    async def check_access(\n        self, document_id: str, auth: AuthContext, required_permission: str = \"read\"\n    ) -> bool:\n        \"\"\"\n        Check if user has required permission for document.\n        Returns: True if user has required access, False otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def store_cache_metadata(self, name: str, metadata: Dict[str, Any]) -> bool:\n        \"\"\"Store metadata for a cache.\n\n        Args:\n            name: Name of the cache\n            metadata: Cache metadata including model info and storage location\n\n        Returns:\n            bool: Whether the operation was successful\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def get_cache_metadata(self, name: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Get metadata for a cache.\n\n        Args:\n            name: Name of the cache\n\n        Returns:\n            Optional[Dict[str, Any]]: Cache metadata if found, None otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def store_graph(self, graph: Graph) -> bool:\n        \"\"\"Store a graph.\n\n        Args:\n            graph: Graph to store\n\n        Returns:\n            bool: Whether the operation was successful\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def get_graph(self, name: str, auth: AuthContext) -> Optional[Graph]:\n        \"\"\"Get a graph by name.\n\n        Args:\n            name: Name of the graph\n            auth: Authentication context\n\n        Returns:\n            Optional[Graph]: Graph if found and accessible, None otherwise\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def list_graphs(self, auth: AuthContext) -> List[Graph]:\n        \"\"\"List all graphs the user has access to.\n\n        Args:\n            auth: Authentication context\n\n        Returns:\n            List[Graph]: List of graphs\n        \"\"\"\n        pass\n"}
{"type": "source_file", "path": "core/vector_store/mongo_vector_store.py", "content": "from typing import List, Optional, Tuple\nimport logging\nfrom motor.motor_asyncio import AsyncIOMotorClient\nfrom pymongo.errors import PyMongoError\n\nfrom .base_vector_store import BaseVectorStore\nfrom core.models.chunk import DocumentChunk\n\nlogger = logging.getLogger(__name__)\n\n\nclass MongoDBAtlasVectorStore(BaseVectorStore):\n    \"\"\"MongoDB Atlas Vector Search implementation.\"\"\"\n\n    def __init__(\n        self,\n        uri: str,\n        database_name: str,\n        collection_name: str = \"document_chunks\",\n        index_name: str = \"vector_index\",\n    ):\n        \"\"\"Initialize MongoDB connection for vector storage.\"\"\"\n        self.client = AsyncIOMotorClient(uri)\n        self.db = self.client[database_name]\n        self.collection = self.db[collection_name]\n        self.index_name = index_name\n\n    async def initialize(self):\n        \"\"\"Initialize vector search index if needed.\"\"\"\n        try:\n            # Create basic indexes\n            await self.collection.create_index(\"document_id\")\n            await self.collection.create_index(\"chunk_number\")\n\n            # Note: Vector search index must be created via Atlas UI or API\n            # as it requires specific configuration\n\n            logger.info(\"MongoDB vector store indexes initialized\")\n            return True\n        except PyMongoError as e:\n            logger.error(f\"Error initializing vector store indexes: {str(e)}\")\n            return False\n\n    async def store_embeddings(self, chunks: List[DocumentChunk]) -> Tuple[bool, List[str]]:\n        \"\"\"Store document chunks with their embeddings.\"\"\"\n        try:\n            if not chunks:\n                return True, []\n\n            # Convert chunks to dicts\n            documents = []\n            for chunk in chunks:\n                doc = chunk.model_dump()\n                # Ensure we have required fields\n                if not doc.get(\"embedding\"):\n                    logger.error(\n                        f\"Missing embedding for chunk \" f\"{chunk.document_id}-{chunk.chunk_number}\"\n                    )\n                    continue\n                documents.append(doc)\n\n            if documents:\n                # Use ordered=False to continue even if some inserts fail\n                result = await self.collection.insert_many(documents, ordered=False)\n                return len(result.inserted_ids) > 0, [str(id) for id in result.inserted_ids]\n            else:\n                logger.error(f\"No documents to store - here is the input: {chunks}\")\n                return False, []\n\n        except PyMongoError as e:\n            logger.error(f\"Error storing embeddings: {str(e)}\")\n            return False, []\n\n    async def query_similar(\n        self,\n        query_embedding: List[float],\n        k: int,\n        doc_ids: Optional[List[str]] = None,\n    ) -> List[DocumentChunk]:\n        \"\"\"Find similar chunks using MongoDB Atlas Vector Search.\"\"\"\n        try:\n            logger.debug(\n                f\"Searching in database {self.db.name} \" f\"collection {self.collection.name}\"\n            )\n            logger.debug(f\"Query vector looks like: {query_embedding}\")\n            logger.debug(f\"Doc IDs: {doc_ids}\")\n            logger.debug(f\"K is: {k}\")\n            logger.debug(f\"Index is: {self.index_name}\")\n\n            # Vector search pipeline\n            pipeline = [\n                {\n                    \"$vectorSearch\": {\n                        \"index\": self.index_name,\n                        \"path\": \"embedding\",\n                        \"queryVector\": query_embedding,\n                        \"numCandidates\": k * 40,  # Get more candidates\n                        \"limit\": k,\n                        \"filter\": {\"document_id\": {\"$in\": doc_ids}} if doc_ids else {},\n                    }\n                },\n                {\n                    \"$project\": {\n                        \"score\": {\"$meta\": \"vectorSearchScore\"},\n                        \"document_id\": 1,\n                        \"chunk_number\": 1,\n                        \"content\": 1,\n                        \"metadata\": 1,\n                        \"_id\": 0,\n                    }\n                },\n            ]\n\n            # Execute search\n            cursor = self.collection.aggregate(pipeline)\n            chunks = []\n\n            async for result in cursor:\n                chunk = DocumentChunk(\n                    document_id=result[\"document_id\"],\n                    chunk_number=result[\"chunk_number\"],\n                    content=result[\"content\"],\n                    embedding=[],  # Don't send embeddings back\n                    metadata=result.get(\"metadata\", {}),\n                    score=result.get(\"score\", 0.0),\n                )\n                chunks.append(chunk)\n\n            return chunks\n\n        except PyMongoError as e:\n            logger.error(f\"MongoDB error: {e._message}\")\n            logger.error(f\"Error querying similar chunks: {str(e)}\")\n            raise e\n            \n    async def get_chunks_by_id(\n        self,\n        chunk_identifiers: List[Tuple[str, int]],\n    ) -> List[DocumentChunk]:\n        \"\"\"\n        Retrieve specific chunks by document ID and chunk number in a single database query.\n        \n        Args:\n            chunk_identifiers: List of (document_id, chunk_number) tuples\n            \n        Returns:\n            List of DocumentChunk objects\n        \"\"\"\n        try:\n            if not chunk_identifiers:\n                return []\n                \n            # Create a query with $or to find multiple chunks in a single query\n            query = {\"$or\": []}\n            for doc_id, chunk_num in chunk_identifiers:\n                query[\"$or\"].append({\n                    \"document_id\": doc_id,\n                    \"chunk_number\": chunk_num\n                })\n                \n            logger.info(f\"Batch retrieving {len(chunk_identifiers)} chunks with a single query\")\n                \n            # Find all matching chunks in a single database query\n            cursor = self.collection.find(query)\n            chunks = []\n            \n            async for result in cursor:\n                chunk = DocumentChunk(\n                    document_id=result[\"document_id\"],\n                    chunk_number=result[\"chunk_number\"],\n                    content=result[\"content\"],\n                    embedding=[],  # Don't send embeddings back\n                    metadata=result.get(\"metadata\", {}),\n                    score=0.0,  # No relevance score for direct retrieval\n                )\n                chunks.append(chunk)\n                \n            logger.info(f\"Found {len(chunks)} chunks in batch retrieval\")\n            return chunks\n                \n        except PyMongoError as e:\n            logger.error(f\"Error retrieving chunks by ID: {str(e)}\")\n            return []\n"}
{"type": "source_file", "path": "core/vector_store/base_vector_store.py", "content": "from abc import ABC, abstractmethod\nfrom typing import List, Optional, Tuple\nfrom core.models.chunk import DocumentChunk\n\n\nclass BaseVectorStore(ABC):\n    @abstractmethod\n    async def store_embeddings(self, chunks: List[DocumentChunk]) -> Tuple[bool, List[str]]:\n        \"\"\"Store document chunks and their embeddings\"\"\"\n        pass\n\n    @abstractmethod\n    async def query_similar(\n        self,\n        query_embedding: List[float],\n        k: int,\n        doc_ids: Optional[List[str]] = None,\n    ) -> List[DocumentChunk]:\n        \"\"\"Find similar chunks\"\"\"\n        pass\n        \n    @abstractmethod\n    async def get_chunks_by_id(\n        self,\n        chunk_identifiers: List[Tuple[str, int]],\n    ) -> List[DocumentChunk]:\n        \"\"\"\n        Retrieve specific chunks by document ID and chunk number.\n        \n        Args:\n            chunk_identifiers: List of (document_id, chunk_number) tuples\n            \n        Returns:\n            List of DocumentChunk objects\n        \"\"\"\n        pass\n"}
{"type": "source_file", "path": "core/vector_store/multi_vector_store.py", "content": "from typing import List, Optional, Tuple, Union\nimport logging\nimport torch\nimport numpy as np\nimport psycopg\nfrom pgvector.psycopg import Bit, register_vector\nfrom core.models.chunk import DocumentChunk\nfrom .base_vector_store import BaseVectorStore\n\nlogger = logging.getLogger(__name__)\n\n\nclass MultiVectorStore(BaseVectorStore):\n    \"\"\"PostgreSQL implementation for storing and querying multi-vector embeddings using psycopg.\"\"\"\n\n    def __init__(\n        self,\n        uri: str,\n    ):\n        \"\"\"Initialize PostgreSQL connection for multi-vector storage.\n\n        Args:\n            uri: PostgreSQL connection URI\n        \"\"\"\n        # Convert SQLAlchemy URI to psycopg format if needed\n        if uri.startswith(\"postgresql+asyncpg://\"):\n            uri = uri.replace(\"postgresql+asyncpg://\", \"postgresql://\")\n        self.uri = uri\n        # self.conn = psycopg.connect(self.uri, autocommit=True)\n        self.conn = None\n        self.initialize()\n\n    def initialize(self):\n        \"\"\"Initialize database tables and max_sim function.\"\"\"\n        try:\n            # Connect to database\n            self.conn = psycopg.connect(self.uri, autocommit=True)\n\n            # Register vector extension\n            self.conn.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n            register_vector(self.conn)\n\n            # First check if the table exists and if it has the required columns\n            check_table = self.conn.execute(\n                \"\"\"\n                SELECT EXISTS (\n                    SELECT FROM information_schema.tables \n                    WHERE table_name = 'multi_vector_embeddings'\n                );\n            \"\"\"\n            ).fetchone()[0]\n\n            if check_table:\n                # Check if document_id column exists\n                has_document_id = self.conn.execute(\n                    \"\"\"\n                    SELECT EXISTS (\n                        SELECT FROM information_schema.columns \n                        WHERE table_name = 'multi_vector_embeddings' AND column_name = 'document_id'\n                    );\n                \"\"\"\n                ).fetchone()[0]\n\n                # If the table exists but doesn't have document_id, we need to add the required columns\n                if not has_document_id:\n                    logger.info(\"Updating multi_vector_embeddings table with required columns\")\n                    self.conn.execute(\n                        \"\"\"\n                        ALTER TABLE multi_vector_embeddings \n                        ADD COLUMN document_id TEXT,\n                        ADD COLUMN chunk_number INTEGER,\n                        ADD COLUMN content TEXT,\n                        ADD COLUMN chunk_metadata TEXT\n                    \"\"\"\n                    )\n                    self.conn.execute(\n                        \"\"\"\n                        ALTER TABLE multi_vector_embeddings \n                        ALTER COLUMN document_id SET NOT NULL\n                    \"\"\"\n                    )\n\n                    # Add a commit to ensure changes are applied\n                    self.conn.commit()\n            else:\n                # Create table if it doesn't exist with all required columns\n                self.conn.execute(\n                    \"\"\"\n                    CREATE TABLE IF NOT EXISTS multi_vector_embeddings (\n                        id BIGSERIAL PRIMARY KEY,\n                        document_id TEXT NOT NULL,\n                        chunk_number INTEGER NOT NULL,\n                        content TEXT NOT NULL,\n                        chunk_metadata TEXT,\n                        embeddings BIT(128)[]\n                    )\n                \"\"\"\n                )\n\n            # Add a commit to ensure table creation is complete\n            self.conn.commit()\n\n            try:\n                # Create index on document_id\n                self.conn.execute(\n                    \"\"\"\n                    CREATE INDEX IF NOT EXISTS idx_multi_vector_document_id \n                    ON multi_vector_embeddings (document_id)\n                \"\"\"\n                )\n            except Exception as e:\n                # Log index creation failure but continue\n                logger.warning(f\"Failed to create index: {str(e)}\")\n\n            try:\n                # First, try to drop the existing function if it exists\n                self.conn.execute(\n                    \"\"\"\n                    DROP FUNCTION IF EXISTS max_sim(bit[], bit[])\n                \"\"\"\n                )\n                logger.info(\"Dropped existing max_sim function\")\n\n                # Create max_sim function\n                self.conn.execute(\n                    \"\"\"\n                    CREATE OR REPLACE FUNCTION max_sim(document bit[], query bit[]) RETURNS double precision AS $$\n                        WITH queries AS (\n                            SELECT row_number() OVER () AS query_number, * FROM (SELECT unnest(query) AS query) AS foo\n                        ),\n                        documents AS (\n                            SELECT unnest(document) AS document\n                        ),\n                        similarities AS (\n                            SELECT \n                                query_number, \n                                1.0 - (bit_count(document # query)::float / greatest(bit_length(query), 1)::float) AS similarity\n                            FROM queries CROSS JOIN documents\n                        ),\n                        max_similarities AS (\n                            SELECT MAX(similarity) AS max_similarity FROM similarities GROUP BY query_number\n                        )\n                        SELECT SUM(max_similarity) FROM max_similarities\n                    $$ LANGUAGE SQL\n                \"\"\"\n                )\n                logger.info(\"Created max_sim function successfully\")\n            except Exception as e:\n                logger.error(f\"Error creating max_sim function: {str(e)}\")\n                # Continue even if function creation fails - it might already exist and be usable\n\n            logger.info(\"MultiVectorStore initialized successfully\")\n            return True\n        except Exception as e:\n            logger.error(f\"Error initializing MultiVectorStore: {str(e)}\")\n            return False\n\n    def _binary_quantize(self, embeddings: Union[np.ndarray, torch.Tensor, List]) -> List[Bit]:\n        \"\"\"Convert embeddings to binary format for PostgreSQL BIT[] arrays.\"\"\"\n        if isinstance(embeddings, torch.Tensor):\n            embeddings = embeddings.cpu().numpy()\n        if isinstance(embeddings, list) and not isinstance(embeddings[0], np.ndarray):\n            embeddings = np.array(embeddings)\n        # try:\n        return [Bit(embedding > 0) for embedding in embeddings]\n        # except Exception as e:\n        #     logger.error(f\"Error quantizing embeddings: {str(e)}\")\n        #     raise e\n\n    async def store_embeddings(self, chunks: List[DocumentChunk]) -> Tuple[bool, List[str]]:\n        \"\"\"Store document chunks with their multi-vector embeddings.\"\"\"\n        # try:\n        if not chunks:\n            return True, []\n\n        stored_ids = []\n\n        for chunk in chunks:\n            # Ensure embeddings exist\n            if not hasattr(chunk, \"embedding\") or chunk.embedding is None:\n                logger.error(\n                    f\"Missing embeddings for chunk {chunk.document_id}-{chunk.chunk_number}\"\n                )\n                continue\n\n            # For multi-vector embeddings, we expect a list of vectors\n            embeddings = chunk.embedding\n\n            # Create binary representation for each vector\n            binary_embeddings = self._binary_quantize(embeddings)\n\n            # Insert into database\n            self.conn.execute(\n                \"\"\"\n                INSERT INTO multi_vector_embeddings \n                (document_id, chunk_number, content, chunk_metadata, embeddings) \n                VALUES (%s, %s, %s, %s, %s)\n                \"\"\",\n                (\n                    chunk.document_id,\n                    chunk.chunk_number,\n                    chunk.content,\n                    str(chunk.metadata),\n                    binary_embeddings,\n                ),\n            )\n\n            stored_ids.append(f\"{chunk.document_id}-{chunk.chunk_number}\")\n\n        logger.info(f\"{len(stored_ids)} vector embeddings added successfully!\")\n        return len(stored_ids) > 0, stored_ids\n\n        # except Exception as e:\n        #     logger.error(f\"Error storing multi-vector embeddings: {str(e)}\")\n        #     raise e\n        #     return False, []\n\n    async def query_similar(\n        self,\n        query_embedding: Union[np.ndarray, torch.Tensor, List[np.ndarray], List[torch.Tensor]],\n        k: int,\n        doc_ids: Optional[List[str]] = None,\n    ) -> List[DocumentChunk]:\n        \"\"\"Find similar chunks using the max_sim function for multi-vectors.\"\"\"\n        # try:\n        # Convert query embeddings to binary format\n        binary_query_embeddings = self._binary_quantize(query_embedding)\n\n        # Build query\n        query = \"\"\"\n            SELECT id, document_id, chunk_number, content, chunk_metadata, \n                    max_sim(embeddings, %s) AS similarity\n            FROM multi_vector_embeddings\n        \"\"\"\n\n        params = [binary_query_embeddings]\n\n        # Add document filter if needed\n        if doc_ids:\n            doc_ids_str = \"', '\".join(doc_ids)\n            query += f\" WHERE document_id IN ('{doc_ids_str}')\"\n\n        # Add ordering and limit\n        query += \" ORDER BY similarity DESC LIMIT %s\"\n        params.append(k)\n\n        # Execute query\n        result = self.conn.execute(query, params).fetchall()\n\n        # Convert to DocumentChunks\n        chunks = []\n        for row in result:\n            try:\n                metadata = eval(row[4]) if row[4] else {}\n            except (ValueError, SyntaxError):\n                metadata = {}\n\n            chunk = DocumentChunk(\n                document_id=row[1],\n                chunk_number=row[2],\n                content=row[3],\n                embedding=[],  # Don't send embeddings back\n                metadata=metadata,\n                score=float(row[5]),  # Use the similarity score from max_sim\n            )\n            chunks.append(chunk)\n\n        return chunks\n\n        # except Exception as e:\n        #     logger.error(f\"Error querying similar chunks: {str(e)}\")\n        #     raise e\n        #     return []\n\n    async def get_chunks_by_id(\n        self,\n        chunk_identifiers: List[Tuple[str, int]],\n    ) -> List[DocumentChunk]:\n        \"\"\"\n        Retrieve specific chunks by document ID and chunk number in a single database query.\n        \n        Args:\n            chunk_identifiers: List of (document_id, chunk_number) tuples\n            \n        Returns:\n            List of DocumentChunk objects\n        \"\"\"\n        # try:\n        if not chunk_identifiers:\n            return []\n            \n        # Construct the WHERE clause with OR conditions\n        conditions = []\n        for doc_id, chunk_num in chunk_identifiers:\n            conditions.append(f\"(document_id = '{doc_id}' AND chunk_number = {chunk_num})\")\n        \n        where_clause = \" OR \".join(conditions)\n        \n        # Build and execute query\n        query = f\"\"\"\n            SELECT document_id, chunk_number, content, chunk_metadata\n            FROM multi_vector_embeddings\n            WHERE {where_clause}\n        \"\"\"\n        \n        logger.info(f\"Batch retrieving {len(chunk_identifiers)} chunks from multi-vector store\")\n        \n        result = self.conn.execute(query).fetchall()\n        \n        # Convert to DocumentChunks\n        chunks = []\n        for row in result:\n            try:\n                metadata = eval(row[3]) if row[3] else {}\n            except (ValueError, SyntaxError):\n                metadata = {}\n                \n            chunk = DocumentChunk(\n                document_id=row[0],\n                chunk_number=row[1],\n                content=row[2],\n                embedding=[],  # Don't send embeddings back\n                metadata=metadata,\n                score=0.0,  # No relevance score for direct retrieval\n            )\n            chunks.append(chunk)\n            \n        logger.info(f\"Found {len(chunks)} chunks in batch retrieval from multi-vector store\")\n        return chunks\n    \n    def close(self):\n        \"\"\"Close the database connection.\"\"\"\n        if self.conn:\n            self.conn.close()\n"}
{"type": "source_file", "path": "core/storage/local_storage.py", "content": "import base64\nfrom pathlib import Path\nfrom typing import Tuple, Optional, BinaryIO\nfrom .base_storage import BaseStorage\n\n\nclass LocalStorage(BaseStorage):\n    def __init__(self, storage_path: str):\n        \"\"\"Initialize local storage with a base path.\"\"\"\n        self.storage_path = Path(storage_path)\n        # Create storage directory if it doesn't exist\n        self.storage_path.mkdir(parents=True, exist_ok=True)\n\n    async def download_file(self, bucket: str, key: str) -> BinaryIO:\n        \"\"\"Download a file from local storage.\"\"\"\n        file_path = self.storage_path / key\n        if not file_path.exists():\n            raise FileNotFoundError(f\"File not found: {file_path}\")\n        return open(file_path, \"rb\")\n\n    async def upload_from_base64(\n        self, content: str, key: str, content_type: Optional[str] = None, bucket: str = \"\"\n    ) -> Tuple[str, str]:\n        base64_content = content\n        \"\"\"Upload base64 encoded content to local storage.\"\"\"\n        # Decode base64 content\n        file_content = base64.b64decode(base64_content)\n\n        key = f\"{bucket}/{key}\" if bucket else key\n        # Create file path\n        file_path = self.storage_path / key\n\n        # Write content to file\n        file_path.parent.mkdir(parents=True, exist_ok=True)\n        file_path.unlink(missing_ok=True)\n        with open(file_path, \"wb\") as f:\n            f.write(file_content)\n\n        return str(self.storage_path), key\n\n    async def get_download_url(self, bucket: str, key: str) -> str:\n        \"\"\"Get local file path as URL.\"\"\"\n        file_path = self.storage_path / key\n        if not file_path.exists():\n            raise FileNotFoundError(f\"File not found: {file_path}\")\n        return f\"file://{file_path.absolute()}\"\n\n    async def delete_file(self, bucket: str, key: str) -> bool:\n        \"\"\"Delete a file from local storage.\"\"\"\n        file_path = self.storage_path / key\n        if file_path.exists():\n            file_path.unlink()\n        return True\n"}
{"type": "source_file", "path": "core/storage/s3_storage.py", "content": "import base64\nimport logging\nfrom typing import Tuple, Optional, Union, BinaryIO\nimport tempfile\nfrom pathlib import Path\n\nimport boto3\nfrom botocore.exceptions import ClientError\n\nfrom .base_storage import BaseStorage\nfrom .utils_file_extensions import detect_file_type\n\nlogger = logging.getLogger(__name__)\n\n\nclass S3Storage(BaseStorage):\n    \"\"\"AWS S3 storage implementation.\"\"\"\n\n    # TODO: Remove hardcoded values.\n    def __init__(\n        self,\n        aws_access_key: str,\n        aws_secret_key: str,\n        region_name: str = \"us-east-2\",\n        default_bucket: str = \"databridge-storage\",\n    ):\n        self.default_bucket = default_bucket\n        self.s3_client = boto3.client(\n            \"s3\",\n            aws_access_key_id=aws_access_key,\n            aws_secret_access_key=aws_secret_key,\n            region_name=region_name,\n        )\n\n    async def upload_file(\n        self,\n        file: Union[str, bytes, BinaryIO],\n        key: str,\n        content_type: Optional[str] = None,\n        bucket: str = \"\",\n    ) -> Tuple[str, str]:\n        \"\"\"Upload a file to S3.\"\"\"\n        try:\n            extra_args = {}\n            if content_type:\n                extra_args[\"ContentType\"] = content_type\n\n            if isinstance(file, (str, bytes)):\n                # Create temporary file for content\n                with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n                    if isinstance(file, str):\n                        temp_file.write(file.encode())\n                    else:\n                        temp_file.write(file)\n                    temp_file_path = temp_file.name\n\n                try:\n                    self.s3_client.upload_file(\n                        temp_file_path, self.default_bucket, key, ExtraArgs=extra_args\n                    )\n                finally:\n                    Path(temp_file_path).unlink()\n            else:\n                # File object\n                self.s3_client.upload_fileobj(file, self.default_bucket, key, ExtraArgs=extra_args)\n\n            return self.default_bucket, key\n\n        except ClientError as e:\n            logger.error(f\"Error uploading to S3: {e}\")\n            raise\n\n    async def upload_from_base64(\n        self, content: str, key: str, content_type: Optional[str] = None, bucket: str = \"\"\n    ) -> Tuple[str, str]:\n        \"\"\"Upload base64 encoded content to S3.\"\"\"\n        key = f\"{bucket}/{key}\" if bucket else key\n        try:\n            decoded_content = base64.b64decode(content)\n            extension = detect_file_type(content)\n            key = f\"{key}{extension}\"\n\n            return await self.upload_file(\n                file=decoded_content, key=key, content_type=content_type, bucket=bucket\n            )\n\n        except Exception as e:\n            logger.error(f\"Error uploading base64 content to S3: {e}\")\n            raise e\n\n    async def download_file(self, bucket: str, key: str) -> bytes:\n        \"\"\"Download file from S3.\"\"\"\n        try:\n            response = self.s3_client.get_object(Bucket=bucket, Key=key)\n            return response[\"Body\"].read()\n        except ClientError as e:\n            logger.error(f\"Error downloading from S3: {e}\")\n            raise\n\n    async def get_download_url(self, bucket: str, key: str, expires_in: int = 3600) -> str:\n        \"\"\"Generate presigned download URL.\"\"\"\n        if not key or not bucket:\n            return \"\"\n\n        try:\n            return self.s3_client.generate_presigned_url(\n                \"get_object\",\n                Params={\"Bucket\": bucket, \"Key\": key},\n                ExpiresIn=expires_in,\n            )\n        except ClientError as e:\n            logger.error(f\"Error generating presigned URL: {e}\")\n            return \"\"\n\n    async def delete_file(self, bucket: str, key: str) -> bool:\n        \"\"\"Delete file from S3.\"\"\"\n        try:\n            self.s3_client.delete_object(Bucket=bucket, Key=key)\n            logger.info(f\"File {key} deleted from bucket {bucket}\")\n            return True\n        except ClientError as e:\n            logger.error(f\"Error deleting from S3: {e}\")\n            return False\n"}
{"type": "source_file", "path": "core/storage/utils_file_extensions.py", "content": "import base64\nimport binascii\nimport filetype\n\n\ndef detect_file_type(content: str) -> str:\n    \"\"\"\n    Detect file type from content string and return appropriate extension.\n    Content can be either base64 encoded or plain text.\n    \"\"\"\n    # Decode base64 content\n    try:\n        decoded_content = base64.b64decode(content)\n    except binascii.Error:\n        # If not base64, treat as plain text\n        decoded_content = content.encode(\"utf-8\")\n\n    # Use filetype to detect mime type from content\n    kind = filetype.guess(decoded_content)\n    if kind is None:\n        return \".txt\" if isinstance(content, str) else \".bin\"\n\n    # Map mime type to extension\n    extension_map = {\n        \"application/pdf\": \".pdf\",\n        \"image/jpeg\": \".jpg\",\n        \"image/png\": \".png\",\n        \"image/gif\": \".gif\",\n        \"image/webp\": \".webp\",\n        \"image/tiff\": \".tiff\",\n        \"image/bmp\": \".bmp\",\n        \"image/svg+xml\": \".svg\",\n        \"video/mp4\": \".mp4\",\n        \"video/mpeg\": \".mpeg\",\n        \"video/quicktime\": \".mov\",\n        \"video/x-msvideo\": \".avi\",\n        \"video/webm\": \".webm\",\n        \"video/x-matroska\": \".mkv\",\n        \"video/3gpp\": \".3gp\",\n        \"text/plain\": \".txt\",\n        \"application/msword\": \".doc\",\n        \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\": \".docx\",\n    }\n    return extension_map.get(kind.mime, \".bin\")\n"}
{"type": "source_file", "path": "core/storage/base_storage.py", "content": "from abc import ABC, abstractmethod\nfrom typing import Tuple, Optional\n\n\nclass BaseStorage(ABC):\n    \"\"\"Base interface for storage providers.\"\"\"\n\n    @abstractmethod\n    async def upload_from_base64(\n        self, content: str, key: str, content_type: Optional[str] = None, bucket: str = \"\"\n    ) -> Tuple[str, str]:\n        \"\"\"\n        Upload base64 encoded content.\n\n        Args:\n            content: Base64 encoded content\n            key: Storage key/path\n            content_type: Optional MIME type\n            bucket: Optional bucket/folder name\n        Returns:\n            Tuple[str, str]: (bucket/container name, storage key)\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def download_file(self, bucket: str, key: str) -> bytes:\n        \"\"\"\n        Download file from storage.\n\n        Args:\n            bucket: Bucket/container name\n            key: Storage key/path\n\n        Returns:\n            bytes: File content\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def get_download_url(self, bucket: str, key: str, expires_in: int = 3600) -> str:\n        \"\"\"\n        Get temporary download URL.\n\n        Args:\n            bucket: Bucket/container name\n            key: Storage key/path\n            expires_in: URL expiration in seconds\n\n        Returns:\n            str: Presigned download URL\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def delete_file(self, bucket: str, key: str) -> bool:\n        \"\"\"\n        Delete file from storage.\n\n        Args:\n            bucket: Bucket/container name\n            key: Storage key/path\n\n        Returns:\n            bool: True if successful\n        \"\"\"\n        pass\n"}
{"type": "source_file", "path": "core/vector_store/__init__.py", "content": ""}
{"type": "source_file", "path": "core/models/completion.py", "content": "from pydantic import BaseModel\nfrom typing import Dict, List, Optional\n\n\nclass ChunkSource(BaseModel):\n    \"\"\"Source information for a chunk used in completion\"\"\"\n    \n    document_id: str\n    chunk_number: int\n    score: Optional[float] = None\n\n\nclass CompletionResponse(BaseModel):\n    \"\"\"Response from completion generation\"\"\"\n\n    completion: str\n    usage: Dict[str, int]\n    finish_reason: Optional[str] = None\n    sources: List[ChunkSource] = []\n    metadata: Optional[Dict] = None\n\n\nclass CompletionRequest(BaseModel):\n    \"\"\"Request for completion generation\"\"\"\n\n    query: str\n    context_chunks: List[str]\n    max_tokens: Optional[int] = 1000\n    temperature: Optional[float] = 0.7\n"}
{"type": "source_file", "path": "core/__init__.py", "content": ""}
{"type": "source_file", "path": "core/reranker/base_reranker.py", "content": "from abc import ABC, abstractmethod\nfrom typing import List, Union\n\nfrom core.models.chunk import DocumentChunk\n\n\nclass BaseReranker(ABC):\n    \"\"\"Base class for reranking search results\"\"\"\n\n    @abstractmethod\n    async def rerank(\n        self,\n        query: str,\n        chunks: List[DocumentChunk],\n    ) -> List[DocumentChunk]:\n        \"\"\"Rerank chunks based on their relevance to the query\"\"\"\n        pass\n\n    @abstractmethod\n    async def compute_score(\n        self,\n        query: str,\n        text: Union[str, List[str]],\n    ) -> Union[float, List[float]]:\n        \"\"\"Compute relevance scores between query and text\"\"\"\n        pass\n"}
{"type": "source_file", "path": "core/reranker/__init__.py", "content": "\"\"\"Reranker package for reranking search results.\"\"\"\n"}
{"type": "source_file", "path": "core/embedding/__init__.py", "content": ""}
{"type": "source_file", "path": "core/models/documents.py", "content": "from typing import Dict, Any, List, Optional, Literal\nfrom enum import Enum\nfrom datetime import UTC, datetime\nfrom PIL import Image\nfrom pydantic import BaseModel, Field, field_validator\nimport uuid\nimport logging\n\nfrom core.models.video import TimeSeriesData\n\nlogger = logging.getLogger(__name__)\n\n\nclass QueryReturnType(str, Enum):\n    CHUNKS = \"chunks\"\n    DOCUMENTS = \"documents\"\n\n\nclass StorageFileInfo(BaseModel):\n    \"\"\"Information about a file stored in storage\"\"\"\n    bucket: str\n    key: str\n    version: int = 1\n    filename: Optional[str] = None\n    content_type: Optional[str] = None\n    timestamp: datetime = Field(default_factory=lambda: datetime.now(UTC))\n    \n    \nclass Document(BaseModel):\n    \"\"\"Represents a document stored in MongoDB documents collection\"\"\"\n\n    external_id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n    owner: Dict[str, str]\n    content_type: str\n    filename: Optional[str] = None\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n    \"\"\"user-defined metadata\"\"\"\n    storage_info: Dict[str, str] = Field(default_factory=dict)\n    \"\"\"Legacy field for backwards compatibility - for single file storage\"\"\"\n    storage_files: List[StorageFileInfo] = Field(default_factory=list)\n    \"\"\"List of files associated with this document\"\"\"\n    system_metadata: Dict[str, Any] = Field(\n        default_factory=lambda: {\n            \"created_at\": datetime.now(UTC),\n            \"updated_at\": datetime.now(UTC),\n            \"version\": 1,\n        }\n    )\n    \"\"\"metadata such as creation date etc.\"\"\"\n    additional_metadata: Dict[str, Any] = Field(default_factory=dict)\n    \"\"\"metadata to help with querying eg. frame descriptions and time-stamped transcript for videos\"\"\"\n    access_control: Dict[str, List[str]] = Field(\n        default_factory=lambda: {\"readers\": [], \"writers\": [], \"admins\": []}\n    )\n    chunk_ids: List[str] = Field(default_factory=list)\n\n    def __hash__(self):\n        return hash(self.external_id)\n\n    def __eq__(self, other):\n        if not isinstance(other, Document):\n            return False\n        return self.external_id == other.external_id\n\n\nclass DocumentContent(BaseModel):\n    \"\"\"Represents either a URL or content string\"\"\"\n\n    type: Literal[\"url\", \"string\"]\n    value: str\n    filename: Optional[str] = Field(None, description=\"Filename when type is url\")\n\n    @field_validator(\"filename\")\n    def filename_only_for_url(cls, v, values):\n        logger.debug(f\"Value looks like: {values}\")\n        if values.data.get(\"type\") == \"string\" and v is not None:\n            raise ValueError(\"filename can only be set when type is url\")\n        if values.data.get(\"type\") == \"url\" and v is None:\n            raise ValueError(\"filename is required when type is url\")\n        return v\n\n\nclass DocumentResult(BaseModel):\n    \"\"\"Query result at document level\"\"\"\n\n    score: float  # Highest chunk score\n    document_id: str  # external_id\n    metadata: Dict[str, Any]\n    content: DocumentContent\n    additional_metadata: Dict[str, Any]\n\n\nclass ChunkResult(BaseModel):\n    \"\"\"Query result at chunk level\"\"\"\n\n    content: str\n    score: float\n    document_id: str  # external_id\n    chunk_number: int\n    metadata: Dict[str, Any]\n    content_type: str\n    filename: Optional[str] = None\n    download_url: Optional[str] = None\n\n    def augmented_content(self, doc: DocumentResult) -> str | Image.Image:\n        match self.metadata:\n            case m if \"timestamp\" in m:\n                # if timestamp present, then must be a video. In that case,\n                # obtain the original document and augment the content with\n                # frame/transcript information as well.\n                frame_description = doc.additional_metadata.get(\"frame_description\")\n                transcript = doc.additional_metadata.get(\"transcript\")\n                if not isinstance(frame_description, dict) or not isinstance(transcript, dict):\n                    logger.warning(\"Invalid frame description or transcript - not a dictionary\")\n                    return self.content\n                ts_frame = TimeSeriesData(time_to_content=frame_description)\n                ts_transcript = TimeSeriesData(time_to_content=transcript)\n                timestamps = (\n                    ts_frame.content_to_times[self.content]\n                    + ts_transcript.content_to_times[self.content]\n                )\n                augmented_contents = [\n                    f\"Frame description: {ts_frame.at_time(t)} \\n \\n Transcript: {ts_transcript.at_time(t)}\"\n                    for t in timestamps\n                ]\n                return \"\\n\\n\".join(augmented_contents)\n            # case m if m.get(\"is_image\", False):\n            #     try:\n            #         # Handle data URI format \"data:image/png;base64,...\"\n            #         content = self.content\n            #         if content.startswith('data:'):\n            #             # Extract the base64 part after the comma\n            #             content = content.split(',', 1)[1]\n\n            #         # Now decode the base64 string\n            #         image_bytes = base64.b64decode(content)\n            #         content = Image.open(io.BytesIO(image_bytes))\n            #         return content\n            #     except Exception as e:\n            #         print(f\"Error processing image: {str(e)}\")\n            #         # Fall back to using the content as text\n            #         return self.content\n            case _:\n                return self.content\n"}
{"type": "source_file", "path": "core/reranker/flag_reranker.py", "content": "from typing import List, Union, Optional\nfrom FlagEmbedding import FlagAutoReranker\n\nfrom core.models.chunk import DocumentChunk\nfrom core.reranker.base_reranker import BaseReranker\n\n\nclass FlagReranker(BaseReranker):\n    \"\"\"Reranker implementation using FlagEmbedding\"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"BAAI/bge-reranker-v2-gemma\",\n        query_max_length: int = 256,\n        passage_max_length: int = 512,\n        use_fp16: bool = True,\n        device: Optional[str] = None,\n    ):\n        \"\"\"Initialize flag reranker\"\"\"\n        devices = [device] if device else None\n        self.reranker = FlagAutoReranker.from_finetuned(\n            model_name_or_path=model_name,\n            query_max_length=query_max_length,\n            passage_max_length=passage_max_length,\n            use_fp16=use_fp16,\n            devices=devices,\n        )\n\n    async def rerank(\n        self,\n        query: str,\n        chunks: List[DocumentChunk],\n    ) -> List[DocumentChunk]:\n        \"\"\"Rerank chunks based on their relevance to the query\"\"\"\n        if not chunks:\n            return []\n\n        # Get scores for all chunks\n        passages = [chunk.content for chunk in chunks]\n        scores = await self.compute_score(query, passages)\n\n        # Update scores and sort chunks\n        for chunk, score in zip(chunks, scores):\n            chunk.score = float(score)\n\n        return sorted(chunks, key=lambda x: x.score, reverse=True)\n\n    async def compute_score(\n        self,\n        query: str,\n        text: Union[str, List[str]],\n    ) -> Union[float, List[float]]:\n        \"\"\"Compute relevance scores between query and text\"\"\"\n        if isinstance(text, str):\n            text = [text]\n            scores = self.reranker.compute_score([[query, t] for t in text], normalize=True)\n            return scores[0] if len(scores) == 1 else scores\n        else:\n            return self.reranker.compute_score([[query, t] for t in text], normalize=True)\n"}
{"type": "source_file", "path": "core/logging_config.py", "content": "import logging\nimport sys\nfrom pathlib import Path\n\n\ndef setup_logging(log_level: str = \"INFO\"):\n    \"\"\"Set up logging configuration.\n\n    Args:\n        log_level: Logging level (DEBUG, INFO, WARNING, ERROR). Defaults to INFO.\n    \"\"\"\n    # Create logs directory if it doesn't exist\n    log_dir = Path(\"logs\")\n    log_dir.mkdir(exist_ok=True)\n\n    # Convert string to logging level\n    level = getattr(logging, log_level)\n\n    # Configure root logger\n    root_logger = logging.getLogger()\n    root_logger.setLevel(level)\n\n    # Create formatters\n    console_formatter = logging.Formatter(\n        \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n    )\n\n    # Console handler\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setFormatter(console_formatter)\n    console_handler.setLevel(level)\n\n    # File handler\n    file_handler = logging.FileHandler(log_dir / \"databridge.log\")\n    file_handler.setFormatter(console_formatter)\n    file_handler.setLevel(level)\n\n    # Add handlers to root logger\n    root_logger.addHandler(console_handler)\n    root_logger.addHandler(file_handler)\n\n    # Set levels for specific loggers\n    logging.getLogger(\"uvicorn\").setLevel(logging.INFO)\n    logging.getLogger(\"fastapi\").setLevel(logging.INFO)\n    # Set debug level for core code to match root logger level\n    logging.getLogger(\"core\").setLevel(level)\n"}
{"type": "source_file", "path": "core/embedding/ollama_embedding_model.py", "content": "from typing import List, Union\nfrom ollama import AsyncClient\nfrom core.embedding.base_embedding_model import BaseEmbeddingModel\nfrom core.models.chunk import Chunk\n\n\nclass OllamaEmbeddingModel(BaseEmbeddingModel):\n    def __init__(self, model_name, base_url: str = \"http://localhost:11434\"):\n        self.model_name = model_name\n        self.client = AsyncClient(host=base_url)\n\n    async def embed_for_ingestion(self, chunks: Union[Chunk, List[Chunk]]) -> List[List[float]]:\n        if isinstance(chunks, Chunk):\n            chunks = [chunks]\n\n        embeddings: List[List[float]] = []\n        for c in chunks:\n            response = await self.client.embeddings(model=self.model_name, prompt=c.content)\n            embedding = list(response[\"embedding\"])\n            embeddings.append(embedding)\n\n        return embeddings\n\n    async def embed_for_query(self, text: str) -> List[float]:\n        response = await self.client.embeddings(model=self.model_name, prompt=text)\n        return list(response[\"embedding\"])\n"}
{"type": "source_file", "path": "core/embedding/openai_embedding_model.py", "content": "from typing import List, Union\nfrom openai import OpenAI\n\nfrom core.models.chunk import Chunk\nfrom core.embedding.base_embedding_model import BaseEmbeddingModel\n\n\nclass OpenAIEmbeddingModel(BaseEmbeddingModel):\n    def __init__(self, api_key: str, model_name: str = \"text-embedding-3-small\"):\n        self.client = OpenAI(api_key=api_key)\n        self.model_name = model_name\n\n    async def embed_for_ingestion(self, chunks: Union[Chunk, List[Chunk]]) -> List[List[float]]:\n        chunks = [chunks] if isinstance(chunks, Chunk) else chunks\n        text = [c.content for c in chunks]\n        response = self.client.embeddings.create(model=self.model_name, input=text)\n\n        return [item.embedding for item in response.data]\n\n    async def embed_for_query(self, text: str) -> List[float]:\n        response = self.client.embeddings.create(model=self.model_name, input=text)\n\n        return response.data[0].embedding\n"}
{"type": "source_file", "path": "core/models/chunk.py", "content": "from typing import Any, Dict, List\n\nfrom pydantic import BaseModel, Field\nimport numpy as np\n\nEmbedding = List[float] | List[List[float]] | np.ndarray\n\n\nclass DocumentChunk(BaseModel):\n    \"\"\"Represents a chunk stored in VectorStore\"\"\"\n\n    document_id: str  # external_id of parent document\n    content: str\n    embedding: Embedding\n    chunk_number: int\n    # chunk-specific metadata\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n    score: float = 0.0\n\n    model_config = {\"arbitrary_types_allowed\": True}\n\n\nclass Chunk(BaseModel):\n    \"\"\"Represents a chunk containing content and metadata\"\"\"\n\n    content: str\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n\n    model_config = {\"arbitrary_types_allowed\": True}\n\n    def to_document_chunk(\n        self, document_id: str, chunk_number: int, embedding: Embedding\n    ) -> DocumentChunk:\n        return DocumentChunk(\n            document_id=document_id,\n            content=self.content,\n            embedding=embedding,\n            chunk_number=chunk_number,\n            metadata=self.metadata,\n        )\n"}
{"type": "source_file", "path": "core/services/rules_processor.py", "content": "from typing import Dict, Any, List, Tuple\nimport logging\nfrom core.models.rules import BaseRule, MetadataExtractionRule, NaturalLanguageRule\nfrom core.config import get_settings\nfrom pydantic import BaseModel\n\nlogger = logging.getLogger(__name__)\nsettings = get_settings()\n\n\nclass RuleResponse(BaseModel):\n    \"\"\"Schema for rule processing responses.\"\"\"\n\n    metadata: Dict[str, Any] = {}  # Optional metadata extracted from text\n    modified_text: str  # The actual modified text - REQUIRED\n\n    model_config = {\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    # Example for PII extraction\n                    \"metadata\": {},\n                    \"modified_text\": \"Original text with PII removed\",\n                },\n                {\n                    # Example for text shortening\n                    \"metadata\": {},\n                    \"modified_text\": \"key1, key2, key3, important_concept\",\n                },\n            ]\n        }\n    }\n\n\nclass RulesProcessor:\n    \"\"\"Processes rules during document ingestion\"\"\"\n\n    def __init__(self):\n        logger.debug(\n            f\"Initializing RulesProcessor with {settings.RULES_PROVIDER} provider using model {settings.RULES_MODEL}\"\n        )\n\n    async def process_rules(\n        self, content: str, rules: List[Dict[str, Any]]\n    ) -> Tuple[Dict[str, Any], str]:\n        \"\"\"\n        Process a list of rules on content.\n\n        Args:\n            content: The document content\n            rules: List of rule dictionaries\n\n        Returns:\n            Tuple[Dict[str, Any], str]: (extracted_metadata, modified_content)\n        \"\"\"\n        logger.debug(f\"Processing {len(rules)} rules on content of length {len(content)}\")\n        metadata = {}\n        modified_content = content\n\n        try:\n            # Parse all rules first to fail fast if any are invalid\n            parsed_rules = [self._parse_rule(rule) for rule in rules]\n            logger.debug(\n                f\"Successfully parsed {len(parsed_rules)} rules: {[r.type for r in parsed_rules]}\"\n            )\n\n            # Apply rules in order\n            for i, rule in enumerate(parsed_rules, 1):\n                try:\n                    logger.debug(f\"Applying rule {i}/{len(parsed_rules)}: {rule.type}\")\n                    rule_metadata, modified_content = await rule.apply(modified_content)\n                    logger.debug(f\"Rule {i} extracted metadata: {rule_metadata}\")\n                    metadata.update(rule_metadata)\n                except Exception as e:\n                    logger.error(f\"Failed to apply rule {rule.type}: {str(e)}\")\n                    continue\n\n        except Exception as e:\n            logger.error(f\"Failed to process rules: {str(e)}\")\n            return metadata, content\n\n        logger.debug(f\"Completed processing {len(rules)} rules. Final metadata: {metadata}\")\n        return metadata, modified_content\n\n    def _parse_rule(self, rule_dict: Dict[str, Any]) -> BaseRule:\n        \"\"\"Parse a rule dictionary into a rule object\"\"\"\n        rule_type = rule_dict.get(\"type\")\n        logger.debug(f\"Parsing rule of type: {rule_type}\")\n\n        if rule_type == \"metadata_extraction\":\n            return MetadataExtractionRule(**rule_dict)\n        elif rule_type == \"natural_language\":\n            return NaturalLanguageRule(**rule_dict)\n        else:\n            raise ValueError(f\"Unknown rule type: {rule_type}\")\n"}
{"type": "source_file", "path": "core/services/telemetry.py", "content": "from datetime import datetime\nfrom typing import Dict, List, Optional, Any\nfrom dataclasses import dataclass\nimport threading\nfrom collections import defaultdict\nimport time\nfrom contextlib import asynccontextmanager\nimport os\nimport json\nfrom pathlib import Path\nimport uuid\nimport hashlib\nimport logging\n\nfrom core.config import get_settings\n\nfrom opentelemetry import trace, metrics\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.metrics import MeterProvider\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.trace import Status, StatusCode\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.sdk.metrics.export import (\n    PeriodicExportingMetricReader,\n    MetricExporter,\n    AggregationTemporality,\n    MetricsData,\n)\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.exporter.otlp.proto.http.metric_exporter import OTLPMetricExporter\nimport requests\nfrom urllib3.exceptions import ProtocolError, ReadTimeoutError\n\n# Get settings from config\nsettings = get_settings()\n\n# Telemetry configuration - use settings directly from TOML\nTELEMETRY_ENABLED = settings.TELEMETRY_ENABLED\nHONEYCOMB_ENABLED = settings.HONEYCOMB_ENABLED\n\n# Honeycomb configuration - using proxy to avoid exposing API key in code\n# Default to localhost:8080 for the proxy, but allow override from settings\nHONEYCOMB_PROXY_ENDPOINT = getattr(settings, \"HONEYCOMB_PROXY_ENDPOINT\", \"https://otel-proxy.onrender.com\")\nHONEYCOMB_PROXY_ENDPOINT = HONEYCOMB_PROXY_ENDPOINT if isinstance(HONEYCOMB_PROXY_ENDPOINT, str) and len(HONEYCOMB_PROXY_ENDPOINT) > 0 else \"https://otel-proxy.onrender.com\"\nSERVICE_NAME = settings.SERVICE_NAME\n\n# Headers for OTLP - no API key needed as the proxy will add it\nOTLP_HEADERS = {\n    \"Content-Type\": \"application/x-protobuf\"\n}\n\n# Configure timeouts and retries directly from TOML config\nOTLP_TIMEOUT = settings.OTLP_TIMEOUT\nOTLP_MAX_RETRIES = settings.OTLP_MAX_RETRIES\nOTLP_RETRY_DELAY = settings.OTLP_RETRY_DELAY\nOTLP_MAX_EXPORT_BATCH_SIZE = settings.OTLP_MAX_EXPORT_BATCH_SIZE\nOTLP_SCHEDULE_DELAY_MILLIS = settings.OTLP_SCHEDULE_DELAY_MILLIS\nOTLP_MAX_QUEUE_SIZE = settings.OTLP_MAX_QUEUE_SIZE\n\n# OTLP endpoints - using our proxy instead of direct Honeycomb connection\nOTLP_TRACES_ENDPOINT = f\"{HONEYCOMB_PROXY_ENDPOINT}/v1/traces\"\nOTLP_METRICS_ENDPOINT = f\"{HONEYCOMB_PROXY_ENDPOINT}/v1/metrics\"\n\n# Enable debug logging for OpenTelemetry\nos.environ[\"OTEL_PYTHON_LOGGING_LEVEL\"] = \"INFO\"  # Changed from DEBUG to reduce verbosity\n# Add export protocol setting if not already set\nif not os.getenv(\"OTEL_EXPORTER_OTLP_PROTOCOL\"):\n    os.environ[\"OTEL_EXPORTER_OTLP_PROTOCOL\"] = \"http/protobuf\"\n\ndef get_installation_id() -> str:\n    \"\"\"Generate or retrieve a unique anonymous installation ID.\"\"\"\n    id_file = Path.home() / \".databridge\" / \"installation_id\"\n    id_file.parent.mkdir(parents=True, exist_ok=True)\n    \n    if id_file.exists():\n        return id_file.read_text().strip()\n    \n    # Generate a new installation ID\n    # We hash the machine-id (if available) or a random UUID\n    machine_id_file = Path(\"/etc/machine-id\")\n    if machine_id_file.exists():\n        machine_id = machine_id_file.read_text().strip()\n    else:\n        machine_id = str(uuid.uuid4())\n    \n    # Hash the machine ID to make it anonymous\n    installation_id = hashlib.sha256(machine_id.encode()).hexdigest()[:32]\n    \n    # Save it for future use\n    id_file.write_text(installation_id)\n    return installation_id\n\n\nclass FileSpanExporter:\n    def __init__(self, log_dir: str):\n        self.log_dir = Path(log_dir)\n        self.log_dir.mkdir(parents=True, exist_ok=True)\n        self.trace_file = self.log_dir / \"traces.log\"\n\n    def export(self, spans):\n        with open(self.trace_file, \"a\") as f:\n            for span in spans:\n                f.write(json.dumps(self._format_span(span)) + \"\\n\")\n        return True\n\n    def shutdown(self):\n        pass\n\n    def _format_span(self, span):\n        return {\n            \"name\": span.name,\n            \"trace_id\": format(span.context.trace_id, \"x\"),\n            \"span_id\": format(span.context.span_id, \"x\"),\n            \"parent_id\": format(span.parent.span_id, \"x\") if span.parent else None,\n            \"start_time\": span.start_time,\n            \"end_time\": span.end_time,\n            \"attributes\": dict(span.attributes),\n            \"status\": span.status.status_code.name,\n        }\n\n\nclass FileMetricExporter(MetricExporter):\n    \"\"\"File metric exporter for OpenTelemetry.\"\"\"\n\n    def __init__(self, log_dir: str):\n        self.log_dir = Path(log_dir)\n        self.log_dir.mkdir(parents=True, exist_ok=True)\n        self.metrics_file = self.log_dir / \"metrics.log\"\n        super().__init__()\n\n    def export(self, metrics_data: MetricsData, **kwargs) -> bool:\n        \"\"\"Export metrics data to a file.\n\n        Args:\n            metrics_data: The metrics data to export.\n\n        Returns:\n            True if the export was successful, False otherwise.\n        \"\"\"\n        try:\n            with open(self.metrics_file, \"a\") as f:\n                for resource_metrics in metrics_data.resource_metrics:\n                    for scope_metrics in resource_metrics.scope_metrics:\n                        for metric in scope_metrics.metrics:\n                            f.write(json.dumps(self._format_metric(metric)) + \"\\n\")\n            return True\n        except Exception:\n            return False\n\n    def shutdown(self, timeout_millis: float = 30_000, **kwargs) -> bool:\n        \"\"\"Shuts down the exporter.\n\n        Args:\n            timeout_millis: Time to wait for the export to complete in milliseconds.\n\n        Returns:\n            True if the shutdown succeeded, False otherwise.\n        \"\"\"\n        return True\n\n    def force_flush(self, timeout_millis: float = 10_000) -> bool:\n        \"\"\"Force flush the exporter.\n\n        Args:\n            timeout_millis: Time to wait for the flush to complete in milliseconds.\n\n        Returns:\n            True if the flush succeeded, False otherwise.\n        \"\"\"\n        return True\n\n    def _preferred_temporality(self) -> Dict:\n        \"\"\"Returns the preferred temporality for each instrument kind.\"\"\"\n        return {\n            \"counter\": AggregationTemporality.CUMULATIVE,\n            \"up_down_counter\": AggregationTemporality.CUMULATIVE,\n            \"observable_counter\": AggregationTemporality.CUMULATIVE,\n            \"observable_up_down_counter\": AggregationTemporality.CUMULATIVE,\n            \"histogram\": AggregationTemporality.CUMULATIVE,\n            \"observable_gauge\": AggregationTemporality.CUMULATIVE,\n        }\n\n    def _format_metric(self, metric):\n        return {\n            \"name\": metric.name,\n            \"description\": metric.description,\n            \"unit\": metric.unit,\n            \"data\": self._format_data(metric.data),\n        }\n\n    def _format_data(self, data):\n        if hasattr(data, \"data_points\"):\n            return {\n                \"data_points\": [\n                    {\n                        \"attributes\": dict(point.attributes),\n                        \"value\": point.value if hasattr(point, \"value\") else None,\n                        \"count\": point.count if hasattr(point, \"count\") else None,\n                        \"sum\": point.sum if hasattr(point, \"sum\") else None,\n                        \"timestamp\": point.time_unix_nano,\n                    }\n                    for point in data.data_points\n                ]\n            }\n        return {}\n\n\nclass RetryingOTLPMetricExporter(MetricExporter):\n    \"\"\"A wrapper around OTLPMetricExporter that adds better retry logic.\"\"\"\n    \n    def __init__(self, endpoint, headers=None, timeout=10):\n        self.exporter = OTLPMetricExporter(\n            endpoint=endpoint,\n            headers=headers,\n            timeout=timeout\n        )\n        self.max_retries = OTLP_MAX_RETRIES\n        self.retry_delay = OTLP_RETRY_DELAY\n        self.logger = logging.getLogger(__name__)\n        super().__init__()\n    \n    def export(self, metrics_data, **kwargs):\n        \"\"\"Export metrics with retry logic for handling connection issues.\"\"\"\n        retries = 0\n        last_exception = None\n        \n        while retries <= self.max_retries:\n            try:\n                return self.exporter.export(metrics_data, **kwargs)\n            except (requests.exceptions.ConnectionError, \n                    requests.exceptions.Timeout,\n                    ProtocolError, \n                    ReadTimeoutError) as e:\n                last_exception = e\n                retries += 1\n                \n                if retries <= self.max_retries:\n                    # Use exponential backoff\n                    delay = self.retry_delay * (2 ** (retries - 1))\n                    # self.logger.warning(\n                    #     f\"Honeycomb export attempt {retries} failed: {str(e)}. \"\n                    #     f\"Retrying in {delay}s...\"\n                    # )\n                    time.sleep(delay)\n                # else:\n                    # self.logger.error(\n                    #     f\"Failed to export to Honeycomb after {retries} attempts: {str(e)}\"\n                    # )\n            except Exception as e:\n                # For non-connection errors, don't retry\n                # self.logger.error(f\"Unexpected error exporting to Honeycomb: {str(e)}\")\n                return False\n                \n        # If we get here, all retries failed\n        return False\n        \n    def shutdown(self, timeout_millis=30000, **kwargs):\n        \"\"\"Shutdown the exporter.\"\"\"\n        return self.exporter.shutdown(timeout_millis, **kwargs)\n        \n    def force_flush(self, timeout_millis=10000):\n        \"\"\"Force flush the exporter.\"\"\"\n        return self.exporter.force_flush(timeout_millis)\n        \n    def _preferred_temporality(self):\n        \"\"\"Returns the preferred temporality.\"\"\"\n        return self.exporter._preferred_temporality()\n\n\nclass RetryingOTLPSpanExporter:\n    \"\"\"A wrapper around OTLPSpanExporter that adds better retry logic.\"\"\"\n    \n    def __init__(self, endpoint, headers=None, timeout=10):\n        self.exporter = OTLPSpanExporter(\n            endpoint=endpoint,\n            headers=headers,\n            timeout=timeout\n        )\n        self.max_retries = OTLP_MAX_RETRIES\n        self.retry_delay = OTLP_RETRY_DELAY\n        self.logger = logging.getLogger(__name__)\n    \n    def export(self, spans):\n        \"\"\"Export spans with retry logic for handling connection issues.\"\"\"\n        retries = 0\n        \n        while retries <= self.max_retries:\n            try:\n                return self.exporter.export(spans)\n            except (requests.exceptions.ConnectionError, \n                    requests.exceptions.Timeout,\n                    ProtocolError, \n                    ReadTimeoutError) as e:\n                retries += 1\n                \n                if retries <= self.max_retries:\n                    # Use exponential backoff\n                    delay = self.retry_delay * (2 ** (retries - 1))\n                    self.logger.warning(\n                        f\"Honeycomb trace export attempt {retries} failed: {str(e)}. \"\n                        f\"Retrying in {delay}s...\"\n                    )\n                    time.sleep(delay)\n                else:\n                    self.logger.error(\n                        f\"Failed to export traces to Honeycomb after {retries} attempts: {str(e)}\"\n                    )\n            except Exception as e:\n                # For non-connection errors, don't retry\n                self.logger.error(f\"Unexpected error exporting traces to Honeycomb: {str(e)}\")\n                return False\n                \n        # If we get here, all retries failed\n        return False\n        \n    def shutdown(self):\n        \"\"\"Shutdown the exporter.\"\"\"\n        return self.exporter.shutdown()\n        \n    def force_flush(self):\n        \"\"\"Force flush the exporter.\"\"\"\n        try:\n            return self.exporter.force_flush()\n        except Exception as e:\n            self.logger.error(f\"Error during trace force_flush: {str(e)}\")\n            return False\n\n\n@dataclass\nclass UsageRecord:\n    timestamp: datetime\n    operation_type: str\n    tokens_used: int\n    user_id: str\n    duration_ms: float\n    status: str\n    metadata: Optional[Dict] = None\n\n\nclass TelemetryService:\n    _instance = None\n    _lock = threading.Lock()\n\n    def __new__(cls):\n        if cls._instance is None:\n            with cls._lock:\n                if cls._instance is None:\n                    cls._instance = super().__new__(cls)\n                    cls._instance._initialize()\n        return cls._instance\n\n    def _initialize(self):\n        if not TELEMETRY_ENABLED:\n            return\n\n        self._usage_records: List[UsageRecord] = []\n        self._user_totals = defaultdict(lambda: defaultdict(int))\n        self._lock = threading.Lock()\n        self._installation_id = get_installation_id()\n\n        # Initialize OpenTelemetry with more detailed resource attributes\n        resource = Resource.create({\n            \"service.name\": SERVICE_NAME,\n            \"service.version\": os.getenv(\"DATABRIDGE_VERSION\", \"unknown\"),\n            \"installation.id\": self._installation_id,\n            \"environment\": os.getenv(\"ENVIRONMENT\", \"production\"),\n            \"telemetry.sdk.name\": \"opentelemetry\",\n            \"telemetry.sdk.language\": \"python\",\n            \"telemetry.sdk.version\": \"1.0.0\"\n        })\n\n        # Initialize tracing with both file and OTLP exporters\n        tracer_provider = TracerProvider(resource=resource)\n        \n        # Always use both exporters\n        log_dir = Path(\"logs/telemetry\")\n        log_dir.mkdir(parents=True, exist_ok=True)\n        \n        # Add file exporter for local logging\n        file_span_processor = BatchSpanProcessor(FileSpanExporter(str(log_dir)))\n        tracer_provider.add_span_processor(file_span_processor)\n        \n        # Add Honeycomb OTLP exporter with retry logic\n        if HONEYCOMB_ENABLED:\n            # Create BatchSpanProcessor with improved configuration\n            otlp_span_processor = BatchSpanProcessor(\n                RetryingOTLPSpanExporter(\n                    endpoint=OTLP_TRACES_ENDPOINT,\n                    headers=OTLP_HEADERS,\n                    timeout=OTLP_TIMEOUT,\n                ),\n                # Configure batch processing settings\n                max_queue_size=OTLP_MAX_QUEUE_SIZE,\n                max_export_batch_size=OTLP_MAX_EXPORT_BATCH_SIZE,\n                schedule_delay_millis=OTLP_SCHEDULE_DELAY_MILLIS,\n            )\n            tracer_provider.add_span_processor(otlp_span_processor)\n\n        trace.set_tracer_provider(tracer_provider)\n        self.tracer = trace.get_tracer(__name__)\n\n        # Initialize metrics with both exporters\n        metric_readers = [\n            # Local file metrics reader\n            PeriodicExportingMetricReader(\n                FileMetricExporter(str(log_dir)),\n                export_interval_millis=60000,  # Export every minute\n            ),\n        ]\n\n        # Add Honeycomb metrics reader if API key is available\n        if HONEYCOMB_ENABLED:\n            try:\n                # Configure the OTLP metric exporter with improved error handling\n                otlp_metric_exporter = RetryingOTLPMetricExporter(\n                    endpoint=OTLP_METRICS_ENDPOINT,\n                    headers=OTLP_HEADERS,\n                    timeout=OTLP_TIMEOUT,\n                )\n                \n                # Configure the metrics reader with improved settings\n                metric_readers.append(\n                    PeriodicExportingMetricReader(\n                        otlp_metric_exporter,\n                        export_interval_millis=OTLP_SCHEDULE_DELAY_MILLIS,\n                        export_timeout_millis=OTLP_TIMEOUT * 1000,\n                    )\n                )\n                print(f\"Successfully configured Honeycomb metrics exporter to {OTLP_METRICS_ENDPOINT}\")\n            except Exception as e:\n                print(f\"Failed to configure Honeycomb metrics exporter: {str(e)}\")\n\n        meter_provider = MeterProvider(resource=resource, metric_readers=metric_readers)\n        metrics.set_meter_provider(meter_provider)\n        self.meter = metrics.get_meter(__name__)\n\n        # Create metrics\n        self.operation_counter = self.meter.create_counter(\n            \"databridge.operations\",\n            description=\"Number of operations performed\",\n        )\n        self.token_counter = self.meter.create_counter(\n            \"databridge.tokens\",\n            description=\"Number of tokens processed\",\n        )\n        self.operation_duration = self.meter.create_histogram(\n            \"databridge.operation.duration\",\n            description=\"Duration of operations\",\n            unit=\"ms\",\n        )\n\n    @asynccontextmanager\n    async def track_operation(\n        self,\n        operation_type: str,\n        user_id: str,\n        tokens_used: int = 0,\n        metadata: Optional[Dict[str, Any]] = None,\n    ):\n        \"\"\"\n        Context manager for tracking operations with both usage metrics and OpenTelemetry.\n        The user_id is hashed to ensure anonymity.\n        \"\"\"\n        if not TELEMETRY_ENABLED:\n            yield None\n            return\n\n        start_time = time.time()\n        status = \"success\"\n        current_span = trace.get_current_span()\n\n        # Hash the user ID for anonymity\n        hashed_user_id = hashlib.sha256(user_id.encode()).hexdigest()[:16]\n\n        try:\n            # Add operation attributes to the current span\n            current_span.set_attribute(\"operation.type\", operation_type)\n            current_span.set_attribute(\"user.id\", hashed_user_id)\n            if metadata:\n                # Create a copy of metadata to avoid modifying the original\n                metadata_copy = metadata.copy()\n                \n                # Remove the nested 'metadata' field completely if it exists\n                if 'metadata' in metadata_copy:\n                    del metadata_copy['metadata']\n                \n                # Set attributes for all remaining metadata fields\n                for key, value in metadata_copy.items():\n                    current_span.set_attribute(f\"metadata.{key}\", str(value))\n\n            yield current_span\n\n        except Exception as e:\n            status = \"error\"\n            current_span.set_status(Status(StatusCode.ERROR))\n            current_span.record_exception(e)\n            raise\n        finally:\n            duration = (time.time() - start_time) * 1000  # Convert to milliseconds\n\n            # Record metrics\n            attributes = {\n                \"operation\": operation_type,\n                \"status\": status,\n                \"installation_id\": self._installation_id\n            }\n            self.operation_counter.add(1, attributes)\n            if tokens_used > 0:\n                self.token_counter.add(tokens_used, attributes)\n            self.operation_duration.record(duration, attributes)\n\n            # Record usage\n            # Create a sanitized copy of metadata for the usage record\n            sanitized_metadata = None\n            if metadata:\n                sanitized_metadata = metadata.copy()\n                # Remove the nested 'metadata' field completely if it exists\n                if 'metadata' in sanitized_metadata:\n                    del sanitized_metadata['metadata']\n            \n            record = UsageRecord(\n                timestamp=datetime.now(),\n                operation_type=operation_type,\n                tokens_used=tokens_used,\n                user_id=hashed_user_id,\n                duration_ms=duration,\n                status=status,\n                metadata=sanitized_metadata,\n            )\n\n            with self._lock:\n                self._usage_records.append(record)\n                self._user_totals[hashed_user_id][operation_type] += tokens_used\n\n    def get_user_usage(self, user_id: str) -> Dict[str, int]:\n        \"\"\"Get usage statistics for a user.\"\"\"\n        if not TELEMETRY_ENABLED:\n            return {}\n            \n        hashed_user_id = hashlib.sha256(user_id.encode()).hexdigest()[:16]\n        with self._lock:\n            return dict(self._user_totals[hashed_user_id])\n\n    def get_recent_usage(\n        self,\n        user_id: Optional[str] = None,\n        operation_type: Optional[str] = None,\n        since: Optional[datetime] = None,\n        status: Optional[str] = None,\n    ) -> List[UsageRecord]:\n        \"\"\"Get recent usage records with optional filtering.\"\"\"\n        if not TELEMETRY_ENABLED:\n            return []\n            \n        with self._lock:\n            records = self._usage_records.copy()\n\n        # Apply filters\n        if user_id:\n            hashed_user_id = hashlib.sha256(user_id.encode()).hexdigest()[:16]\n            records = [r for r in records if r.user_id == hashed_user_id]\n        if operation_type:\n            records = [r for r in records if r.operation_type == operation_type]\n        if since:\n            records = [r for r in records if r.timestamp >= since]\n        if status:\n            records = [r for r in records if r.status == status]\n\n        return records\n"}
{"type": "source_file", "path": "core/models/rules.py", "content": "from typing import Dict, Any, Literal\nfrom pydantic import BaseModel\nfrom abc import ABC, abstractmethod\nfrom core.config import get_settings\nfrom openai import AsyncOpenAI\nfrom ollama import AsyncClient\nimport json\nimport logging\n\nlogger = logging.getLogger(__name__)\nsettings = get_settings()\n\n# Initialize the appropriate client based on settings\nif settings.RULES_PROVIDER == \"openai\":\n    rules_client = AsyncOpenAI()\nelse:  # ollama\n    rules_client = AsyncClient(host=settings.COMPLETION_OLLAMA_BASE_URL)\n\n\nclass BaseRule(BaseModel, ABC):\n    \"\"\"Base model for all rules\"\"\"\n\n    type: str\n\n    @abstractmethod\n    async def apply(self, content: str) -> tuple[Dict[str, Any], str]:\n        \"\"\"\n        Apply the rule to the content.\n\n        Args:\n            content: The content to apply the rule to\n\n        Returns:\n            tuple[Dict[str, Any], str]: (metadata, modified_content)\n        \"\"\"\n        pass\n\n\nclass MetadataExtractionRule(BaseRule):\n    \"\"\"Rule for extracting metadata using a schema\"\"\"\n\n    type: Literal[\"metadata_extraction\"]\n    schema: Dict[str, Any]\n\n    async def apply(self, content: str) -> tuple[Dict[str, Any], str]:\n        \"\"\"Extract metadata according to schema\"\"\"\n        prompt = f\"\"\"\n        Extract metadata from the following text according to this schema:\n        {self.schema}\n\n        Text to extract from:\n        {content}\n\n        Return ONLY a JSON object with the extracted metadata.\n        \"\"\"\n\n        if settings.RULES_PROVIDER == \"openai\":\n            response = await rules_client.chat.completions.create(\n                model=settings.RULES_MODEL,\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": \"You are a metadata extraction assistant. Always respond with valid JSON.\",\n                    },\n                    {\"role\": \"user\", \"content\": prompt},\n                ],\n                response_format={\"type\": \"json_object\"},\n            )\n            metadata = json.loads(response.choices[0].message.content)\n        else:  # ollama\n            response = await rules_client.chat(\n                model=settings.RULES_MODEL,\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": \"You are a metadata extraction assistant. Always respond with valid JSON.\",\n                    },\n                    {\"role\": \"user\", \"content\": prompt},\n                ],\n                format=\"json\",\n            )\n            content_str = response[\"message\"][\"content\"]\n            logger.debug(f\"Ollama raw response: {content_str}\")\n            metadata = json.loads(content_str)\n\n        return metadata, content\n\n\nclass NaturalLanguageRule(BaseRule):\n    \"\"\"Rule for transforming content using natural language\"\"\"\n\n    type: Literal[\"natural_language\"]\n    prompt: str\n\n    async def apply(self, content: str) -> tuple[Dict[str, Any], str]:\n        \"\"\"Transform content according to prompt\"\"\"\n        prompt = f\"\"\"\n        Your task is to transform the following text according to this instruction:\n        {self.prompt}\n        \n        Text to transform:\n        {content}\n        \n        Return ONLY the transformed text.\n        \"\"\"\n\n        if settings.RULES_PROVIDER == \"openai\":\n            response = await rules_client.chat.completions.create(\n                model=settings.RULES_MODEL,\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are a text transformation assistant.\"},\n                    {\"role\": \"user\", \"content\": prompt},\n                ],\n            )\n            transformed_text = response.choices[0].message.content\n        else:  # ollama\n            response = await rules_client.chat(\n                model=settings.RULES_MODEL,\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are a text transformation assistant.\"},\n                    {\"role\": \"user\", \"content\": prompt},\n                ],\n            )\n            transformed_text = response[\"message\"][\"content\"]\n\n        return {}, transformed_text\n"}
{"type": "source_file", "path": "core/database/mongo_database.py", "content": "from datetime import UTC, datetime\nimport logging\nfrom typing import Dict, List, Optional, Any\n\nfrom motor.motor_asyncio import AsyncIOMotorClient\nfrom pymongo import ReturnDocument\nfrom pymongo.errors import PyMongoError\n\nfrom .base_database import BaseDatabase\nfrom ..models.documents import Document\nfrom ..models.auth import AuthContext, EntityType\n\nlogger = logging.getLogger(__name__)\n\n\nclass MongoDatabase(BaseDatabase):\n    \"\"\"MongoDB implementation for document metadata storage.\"\"\"\n\n    def __init__(\n        self,\n        uri: str,\n        db_name: str,\n        collection_name: str,\n    ):\n        \"\"\"Initialize MongoDB connection for document storage.\"\"\"\n        self.client = AsyncIOMotorClient(uri)\n        self.db = self.client[db_name]\n        self.collection = self.db[collection_name]\n        self.caches = self.db[\"caches\"]  # Collection for cache metadata\n\n    async def initialize(self):\n        \"\"\"Initialize database indexes.\"\"\"\n        try:\n            # Create indexes for common queries\n            await self.collection.create_index(\"external_id\", unique=True)\n            await self.collection.create_index(\"owner.id\")\n            await self.collection.create_index(\"access_control.readers\")\n            await self.collection.create_index(\"access_control.writers\")\n            await self.collection.create_index(\"access_control.admins\")\n            await self.collection.create_index(\"system_metadata.created_at\")\n\n            logger.info(\"MongoDB indexes created successfully\")\n            return True\n        except PyMongoError as e:\n            logger.error(f\"Error creating MongoDB indexes: {str(e)}\")\n            return False\n\n    async def store_document(self, document: Document) -> bool:\n        \"\"\"Store document metadata.\"\"\"\n        try:\n            doc_dict = document.model_dump()\n\n            # Ensure system metadata\n            doc_dict[\"system_metadata\"][\"created_at\"] = datetime.now(UTC)\n            doc_dict[\"system_metadata\"][\"updated_at\"] = datetime.now(UTC)\n            doc_dict[\"metadata\"][\"external_id\"] = doc_dict[\"external_id\"]\n\n            result = await self.collection.insert_one(doc_dict)\n            return bool(result.inserted_id)\n\n        except PyMongoError as e:\n            logger.error(f\"Error storing document metadata: {str(e)}\")\n            return False\n\n    async def get_document(self, document_id: str, auth: AuthContext) -> Optional[Document]:\n        \"\"\"Retrieve document metadata by ID if user has access.\"\"\"\n        try:\n            # Build access filter\n            access_filter = self._build_access_filter(auth)\n\n            # Query document\n            query = {\"$and\": [{\"external_id\": document_id}, access_filter]}\n            logger.debug(f\"Querying document with query: {query}\")\n\n            doc_dict = await self.collection.find_one(query)\n            logger.debug(f\"Found document: {doc_dict}\")\n            return Document(**doc_dict) if doc_dict else None\n\n        except PyMongoError as e:\n            logger.error(f\"Error retrieving document metadata: {str(e)}\")\n            raise e\n            \n    async def get_document_by_filename(self, filename: str, auth: AuthContext) -> Optional[Document]:\n        \"\"\"Retrieve document metadata by filename if user has access.\n        If multiple documents have the same filename, returns the most recently updated one.\n        \"\"\"\n        try:\n            # Build access filter\n            access_filter = self._build_access_filter(auth)\n\n            # Query document\n            query = {\"$and\": [{\"filename\": filename}, access_filter]}\n            logger.debug(f\"Querying document by filename with query: {query}\")\n\n            # Sort by updated_at in descending order to get the most recent one\n            sort_criteria = [(\"system_metadata.updated_at\", -1)]\n            \n            doc_dict = await self.collection.find_one(query, sort=sort_criteria)\n            logger.debug(f\"Found document by filename: {doc_dict}\")\n            \n            return Document(**doc_dict) if doc_dict else None\n\n        except PyMongoError as e:\n            logger.error(f\"Error retrieving document metadata by filename: {str(e)}\")\n            raise e\n            \n    async def get_documents_by_id(self, document_ids: List[str], auth: AuthContext) -> List[Document]:\n        \"\"\"\n        Retrieve multiple documents by their IDs in a single batch operation.\n        Only returns documents the user has access to.\n        \n        Args:\n            document_ids: List of document IDs to retrieve\n            auth: Authentication context\n            \n        Returns:\n            List of Document objects that were found and user has access to\n        \"\"\"\n        try:\n            if not document_ids:\n                return []\n                \n            # Build access filter\n            access_filter = self._build_access_filter(auth)\n            \n            # Query documents with both document IDs and access check in a single query\n            query = {\n                \"$and\": [\n                    {\"external_id\": {\"$in\": document_ids}},\n                    access_filter\n                ]\n            }\n            \n            logger.info(f\"Batch retrieving {len(document_ids)} documents with a single query\")\n            \n            # Execute batch query\n            cursor = self.collection.find(query)\n            \n            documents = []\n            async for doc_dict in cursor:\n                documents.append(Document(**doc_dict))\n                \n            logger.info(f\"Found {len(documents)} documents in batch retrieval\")\n            return documents\n                \n        except PyMongoError as e:\n            logger.error(f\"Error batch retrieving documents: {str(e)}\")\n            return []\n\n    async def get_documents(\n        self,\n        auth: AuthContext,\n        skip: int = 0,\n        limit: int = 100,\n        filters: Optional[Dict[str, Any]] = None,\n    ) -> List[Document]:\n        \"\"\"List accessible documents with pagination and filtering.\"\"\"\n        try:\n            # Build query\n            auth_filter = self._build_access_filter(auth)\n            metadata_filter = self._build_metadata_filter(filters)\n            query = {\"$and\": [auth_filter, metadata_filter]} if metadata_filter else auth_filter\n\n            # Execute paginated query\n            cursor = self.collection.find(query).skip(skip).limit(limit)\n\n            documents = []\n            async for doc_dict in cursor:\n                documents.append(Document(**doc_dict))\n\n            return documents\n\n        except PyMongoError as e:\n            logger.error(f\"Error listing documents: {str(e)}\")\n            return []\n\n    async def update_document(\n        self, document_id: str, updates: Dict[str, Any], auth: AuthContext\n    ) -> bool:\n        \"\"\"Update document metadata if user has write access.\"\"\"\n        try:\n            # Verify write access\n            if not await self.check_access(document_id, auth, \"write\"):\n                return False\n\n            # Update system metadata\n            updates.setdefault(\"system_metadata\", {})\n            updates[\"system_metadata\"][\"updated_at\"] = datetime.now(UTC)\n\n            result = await self.collection.find_one_and_update(\n                {\"external_id\": document_id},\n                {\"$set\": updates},\n                return_document=ReturnDocument.AFTER,\n            )\n\n            return bool(result)\n\n        except PyMongoError as e:\n            logger.error(f\"Error updating document metadata: {str(e)}\")\n            return False\n\n    async def delete_document(self, document_id: str, auth: AuthContext) -> bool:\n        \"\"\"Delete document if user has admin access.\"\"\"\n        try:\n            # Verify admin access\n            if not await self.check_access(document_id, auth, \"admin\"):\n                return False\n\n            result = await self.collection.delete_one({\"external_id\": document_id})\n            return bool(result.deleted_count)\n\n        except PyMongoError as e:\n            logger.error(f\"Error deleting document: {str(e)}\")\n            return False\n\n    async def find_authorized_and_filtered_documents(\n        self, auth: AuthContext, filters: Optional[Dict[str, Any]] = None\n    ) -> List[str]:\n        \"\"\"Find document IDs matching filters and access permissions.\"\"\"\n        # Build query\n        auth_filter = self._build_access_filter(auth)\n        metadata_filter = self._build_metadata_filter(filters)\n        query = {\"$and\": [auth_filter, metadata_filter]} if metadata_filter else auth_filter\n\n        # Get matching document IDs\n        cursor = self.collection.find(query, {\"external_id\": 1})\n\n        document_ids = []\n        async for doc in cursor:\n            document_ids.append(doc[\"external_id\"])\n\n        return document_ids\n\n    async def check_access(\n        self, document_id: str, auth: AuthContext, required_permission: str = \"read\"\n    ) -> bool:\n        \"\"\"Check if user has required permission for document.\"\"\"\n        try:\n            doc = await self.collection.find_one({\"external_id\": document_id})\n            if not doc:\n                return False\n\n            access_control = doc.get(\"access_control\", {})\n\n            # Check owner access\n            owner = doc.get(\"owner\", {})\n            if owner.get(\"type\") == auth.entity_type and owner.get(\"id\") == auth.entity_id:\n                return True\n\n            # Check permission-specific access\n            permission_map = {\"read\": \"readers\", \"write\": \"writers\", \"admin\": \"admins\"}\n\n            permission_set = permission_map.get(required_permission)\n            if not permission_set:\n                return False\n\n            return auth.entity_id in access_control.get(permission_set, set())\n\n        except PyMongoError as e:\n            logger.error(f\"Error checking document access: {str(e)}\")\n            return False\n\n    def _build_access_filter(self, auth: AuthContext) -> Dict[str, Any]:\n        \"\"\"Build MongoDB filter for access control.\"\"\"\n        base_filter = {\n            \"$or\": [\n                {\"owner.id\": auth.entity_id},\n                {\"access_control.readers\": auth.entity_id},\n                {\"access_control.writers\": auth.entity_id},\n                {\"access_control.admins\": auth.entity_id},\n            ]\n        }\n\n        if auth.entity_type == EntityType.DEVELOPER:\n            # Add app-specific access for developers\n            base_filter[\"$or\"].append({\"access_control.app_access\": auth.app_id})\n\n        return base_filter\n\n    def _build_metadata_filter(self, filters: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Build MongoDB filter for metadata.\"\"\"\n        if not filters:\n            return {}\n        filter_dict = {}\n        for key, value in filters.items():\n            filter_dict[f\"metadata.{key}\"] = value\n        return filter_dict\n\n    async def store_cache_metadata(self, name: str, metadata: Dict[str, Any]) -> bool:\n        \"\"\"Store metadata for a cache in MongoDB.\n\n        Args:\n            name: Name of the cache\n            metadata: Cache metadata including model info and storage location\n\n        Returns:\n            bool: Whether the operation was successful\n        \"\"\"\n        try:\n            # Add timestamp and ensure name is included\n            doc = {\n                \"name\": name,\n                \"metadata\": metadata,\n                \"created_at\": datetime.now(UTC),\n                \"updated_at\": datetime.now(UTC),\n            }\n\n            # Upsert the document\n            result = await self.caches.update_one({\"name\": name}, {\"$set\": doc}, upsert=True)\n            return bool(result.modified_count or result.upserted_id)\n        except Exception as e:\n            logger.error(f\"Failed to store cache metadata: {e}\")\n            return False\n\n    async def get_cache_metadata(self, name: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Get metadata for a cache from MongoDB.\n\n        Args:\n            name: Name of the cache\n\n        Returns:\n            Optional[Dict[str, Any]]: Cache metadata if found, None otherwise\n        \"\"\"\n        try:\n            doc = await self.caches.find_one({\"name\": name})\n            return doc[\"metadata\"] if doc else None\n        except Exception as e:\n            logger.error(f\"Failed to get cache metadata: {e}\")\n            return None\n"}
{"type": "source_file", "path": "core/models/video.py", "content": "from collections import defaultdict\nfrom typing import List, Tuple, Optional, Union, Dict\nfrom bisect import bisect_left\nimport logging\n\nfrom pydantic import BaseModel, computed_field\n\nfrom core.models.chunk import Chunk\n\nlogger = logging.getLogger(__name__)\n\n\nclass TimeSeriesData(BaseModel):\n    \"\"\"Time series data structure for efficient time-based queries\"\"\"\n\n    time_to_content: Dict[float, str]\n\n    @computed_field\n    @property\n    def _sorted_items(self) -> List[Tuple[float, str]]:\n        return sorted(self.time_to_content.items(), key=lambda x: x[0])\n\n    @computed_field\n    @property\n    def timestamps(self) -> List[float]:\n        return [t for t, _ in self._sorted_items]\n\n    @computed_field\n    @property\n    def contents(self) -> List[str]:\n        return [c for _, c in self._sorted_items]\n\n    @computed_field\n    @property\n    def content_to_times(self) -> Dict[str, List[float]]:\n        result = defaultdict(list)\n        for t, c in self.time_to_content.items():\n            result[c].append(t)\n        return dict(result)\n\n    def _find_nearest_index(self, time: float) -> int:\n        \"\"\"Find index of nearest timestamp using binary search\"\"\"\n        if not self.timestamps:  # Handle empty timestamps list\n            return -1\n\n        idx = bisect_left(self.timestamps, time)\n        if idx == 0:\n            return 0\n        if idx == len(self.timestamps):\n            return len(self.timestamps) - 1\n        before = self.timestamps[idx - 1]\n        after = self.timestamps[idx]\n        return idx if (time - before) > (after - time) else idx - 1\n\n    def at_time(\n        self, time: float, padding: Optional[float] = None\n    ) -> Union[str, List[Tuple[float, str]]]:\n        \"\"\"\n        Get content at or around specified time\n\n        Args:\n            time: Target timestamp\n            padding: Optional time padding in seconds to get content before and after\n\n        Returns:\n            Either single content string or list of (timestamp, content) pairs if padding specified\n        \"\"\"\n        if not self.timestamps:  # Handle empty timestamps list\n            return [] if padding is not None else \"\"\n\n        if padding is None:\n            idx = self._find_nearest_index(time)\n            return self.contents[idx]\n\n        # Find all content within padding window\n        start_time = max(time - padding, self.timestamps[0])  # Clamp to first timestamp\n        end_time = min(time + padding, self.timestamps[-1])  # Clamp to last timestamp\n\n        start_idx = self._find_nearest_index(start_time)\n        end_idx = self._find_nearest_index(end_time)\n\n        # Ensure valid indices\n        start_idx = max(0, start_idx)\n        end_idx = min(len(self.timestamps) - 1, end_idx)\n\n        logger.debug(f\"Retrieving content between {start_time:.2f}s and {end_time:.2f}s\")\n        return [(self.timestamps[i], self.contents[i]) for i in range(start_idx, end_idx + 1)]\n\n    def times_for_content(self, content: str) -> List[float]:\n        \"\"\"Get all timestamps where this content appears\"\"\"\n        return self.content_to_times.get(content, [])\n\n    def to_chunks(self) -> List[Chunk]:\n        return [\n            Chunk(content=content, metadata={\"timestamp\": timestamp})\n            for content, timestamp in zip(self.contents, self.timestamps)\n        ]\n\n\nclass ParseVideoResult(BaseModel):\n    metadata: Dict[str, Union[float, int]]\n    frame_descriptions: TimeSeriesData\n    transcript: TimeSeriesData\n"}
{"type": "source_file", "path": "core/embedding/base_embedding_model.py", "content": "from abc import ABC, abstractmethod\nfrom typing import List, Union\n\nfrom core.models.chunk import Chunk\n\n\nclass BaseEmbeddingModel(ABC):\n    @abstractmethod\n    async def embed_for_ingestion(self, chunks: Union[Chunk, List[Chunk]]) -> List[List[float]]:\n        \"\"\"Generate embeddings for input text\"\"\"\n        pass\n\n    @abstractmethod\n    async def embed_for_query(self, text: str) -> List[float]:\n        \"\"\"Generate embeddings for input text\"\"\"\n        pass\n"}
{"type": "source_file", "path": "core/parser/base_parser.py", "content": "from abc import ABC, abstractmethod\nfrom typing import Any, Dict, List, Tuple\nfrom core.models.chunk import Chunk\n\n\nclass BaseParser(ABC):\n    \"\"\"Base class for document parsing\"\"\"\n\n    @abstractmethod\n    async def parse_file_to_text(\n        self, file: bytes, content_type: str, filename: str\n    ) -> Tuple[Dict[str, Any], str]:\n        \"\"\"\n        Parse file content into text.\n\n        Args:\n            file: Raw file bytes\n            content_type: MIME type of the file\n            filename: Name of the file\n\n        Returns:\n            Tuple[Dict[str, Any], str]: (metadata, extracted_text)\n            - metadata: Additional metadata extracted during parsing\n            - extracted_text: Raw text extracted from the file\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def split_text(self, text: str) -> List[Chunk]:\n        \"\"\"\n        Split plain text into chunks.\n\n        Args:\n            text: Text to split into chunks\n\n        Returns:\n            List[Chunk]: List of text chunks with metadata\n        \"\"\"\n        pass\n"}
{"type": "source_file", "path": "core/services/graph_service.py", "content": "import httpx\nimport logging\nimport numpy as np\nfrom typing import Dict, Any, List, Optional, Tuple, Set\nfrom pydantic import BaseModel\n\nfrom core.models.completion import ChunkSource, CompletionResponse, CompletionRequest\nfrom core.models.graph import Graph, Entity, Relationship\nfrom core.models.auth import AuthContext\nfrom core.embedding.base_embedding_model import BaseEmbeddingModel\nfrom core.completion.base_completion import BaseCompletionModel\nfrom core.database.base_database import BaseDatabase\nfrom core.models.documents import Document, ChunkResult\nfrom core.config import get_settings\nfrom openai import AsyncOpenAI\n\nlogger = logging.getLogger(__name__)\n\n\nclass EntityExtraction(BaseModel):\n    \"\"\"Model for entity extraction results\"\"\"\n    label: str\n    type: str\n    properties: Dict[str, Any] = {}\n\n\nclass RelationshipExtraction(BaseModel):\n    \"\"\"Model for relationship extraction results\"\"\"\n    source: str\n    target: str\n    relationship: str\n\n\nclass ExtractionResult(BaseModel):\n    \"\"\"Model for structured extraction from LLM\"\"\"\n    entities: List[EntityExtraction] = []\n    relationships: List[RelationshipExtraction] = []\n\n\nclass GraphService:\n    \"\"\"Service for managing knowledge graphs and graph-based operations\"\"\"\n\n    def __init__(\n        self,\n        db: BaseDatabase,\n        embedding_model: BaseEmbeddingModel,\n        completion_model: BaseCompletionModel,\n    ):\n        self.db = db\n        self.embedding_model = embedding_model\n        self.completion_model = completion_model\n\n    async def create_graph(\n        self,\n        name: str,\n        auth: AuthContext,\n        document_service,  # Passed in to avoid circular import\n        filters: Optional[Dict[str, Any]] = None,\n        documents: Optional[List[str]] = None,\n    ) -> Graph:\n        \"\"\"Create a graph from documents.\n\n        This function processes documents matching filters or specific document IDs,\n        extracts entities and relationships from document chunks, and saves them as a graph.\n\n        Args:\n            name: Name of the graph to create\n            auth: Authentication context\n            document_service: DocumentService instance for retrieving documents and chunks\n            filters: Optional metadata filters to determine which documents to include\n            documents: Optional list of specific document IDs to include\n\n        Returns:\n            Graph: The created graph\n        \"\"\"\n        if \"write\" not in auth.permissions:\n            raise PermissionError(\"User does not have write permission\")\n\n        # Find documents to process based on filters and/or specific document IDs\n        document_ids = set(documents or [])\n\n        # If filters were provided, get matching documents\n        if filters:\n            filtered_docs = await self.db.get_documents(auth, filters=filters)\n            document_ids.update(doc.external_id for doc in filtered_docs)\n\n        if not document_ids:\n            raise ValueError(\"No documents found matching criteria\")\n\n        # Batch retrieve documents for authorization check\n        document_objects = await document_service.batch_retrieve_documents(list(document_ids), auth)\n        if not document_objects:\n            raise ValueError(\"No authorized documents found matching criteria\")\n\n        # Create a new graph with authorization info\n        graph = Graph(\n            name=name,\n            document_ids=[doc.external_id for doc in document_objects],\n            filters=filters,\n            owner={\"type\": auth.entity_type, \"id\": auth.entity_id},\n            access_control={\n                \"readers\": [auth.entity_id],\n                \"writers\": [auth.entity_id],\n                \"admins\": [auth.entity_id],\n            },\n        )\n\n        # Extract entities and relationships\n        entities, relationships = await self._process_documents_for_entities(\n            document_objects, auth, document_service\n        )\n\n        # Add entities and relationships to the graph\n        graph.entities = list(entities.values())\n        graph.relationships = relationships\n\n        # Store the graph in the database\n        if not await self.db.store_graph(graph):\n            raise Exception(\"Failed to store graph\")\n\n        return graph\n\n    async def _process_documents_for_entities(\n        self, \n        documents: List[Document], \n        auth: AuthContext,\n        document_service\n    ) -> Tuple[Dict[str, Entity], List[Relationship]]:\n        \"\"\"Process documents to extract entities and relationships.\n\n        Args:\n            documents: List of documents to process\n            auth: Authentication context\n            document_service: DocumentService instance for retrieving chunks\n\n        Returns:\n            Tuple of (entities_dict, relationships_list)\n        \"\"\"\n        # Dictionary to collect entities by label (to avoid duplicates)\n        entities = {}\n        # List to collect all relationships\n        relationships = []\n\n        # Collect all chunk sources from documents.\n        chunk_sources = [\n            ChunkSource(document_id=doc.external_id, chunk_number=i)\n            for doc in documents\n            for i, _ in enumerate(doc.chunk_ids)\n        ]\n\n        # Batch retrieve chunks\n        chunks = await document_service.batch_retrieve_chunks(chunk_sources, auth)\n        logger.info(f\"Retrieved {len(chunks)} chunks for processing\")\n\n        # Process each chunk individually\n        for chunk in chunks:\n            try:\n                # Extract entities and relationships from the chunk\n                chunk_entities, chunk_relationships = await self.extract_entities_from_text(\n                    chunk.content,\n                    chunk.document_id,\n                    chunk.chunk_number\n                )\n\n                # Add entities to the collection, avoiding duplicates\n                for entity in chunk_entities:\n                    if entity.label not in entities:\n                        # For new entities, initialize chunk_sources with the current chunk\n                        entities[entity.label] = entity\n                    else:\n                        # If entity already exists, add this chunk source if not already present\n                        existing_entity = entities[entity.label]\n\n                        # Add to chunk_sources dictionary\n                        if chunk.document_id not in existing_entity.chunk_sources:\n                            existing_entity.chunk_sources[chunk.document_id] = [chunk.chunk_number]\n                        elif chunk.chunk_number not in existing_entity.chunk_sources[chunk.document_id]:\n                            existing_entity.chunk_sources[chunk.document_id].append(chunk.chunk_number)\n\n                # Add the current chunk source to each relationship\n                for relationship in chunk_relationships:\n                    # Add to chunk_sources dictionary\n                    if chunk.document_id not in relationship.chunk_sources:\n                        relationship.chunk_sources[chunk.document_id] = [chunk.chunk_number]\n                    elif chunk.chunk_number not in relationship.chunk_sources[chunk.document_id]:\n                        relationship.chunk_sources[chunk.document_id].append(chunk.chunk_number)\n\n                # Add relationships to the collection\n                relationships.extend(chunk_relationships)\n\n            except ValueError as e:\n                # Handle specific extraction errors we've wrapped\n                logger.warning(f\"Skipping chunk {chunk.chunk_number} in document {chunk.document_id}: {e}\")\n                continue\n            except Exception as e:\n                # For other errors, log and re-raise to abort graph creation\n                logger.error(f\"Fatal error processing chunk {chunk.chunk_number} in document {chunk.document_id}: {e}\")\n                raise\n\n        return entities, relationships\n\n    async def extract_entities_from_text(\n        self, content: str, doc_id: str, chunk_number: int\n    ) -> Tuple[List[Entity], List[Relationship]]:\n        \"\"\"\n        Extract entities and relationships from text content using the LLM.\n\n        Args:\n            content: Text content to process\n            doc_id: Document ID\n            chunk_number: Chunk number within the document\n\n        Returns:\n            Tuple of (entities, relationships)\n        \"\"\"\n        settings = get_settings()\n\n        # Limit text length to avoid token limits\n        content_limited = content[:min(len(content), 5000)]\n\n        # Define the JSON schema for structured output\n        json_schema = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"entities\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"label\": {\"type\": \"string\"},\n                            \"type\": {\"type\": \"string\"}\n                        },\n                        \"required\": [\"label\", \"type\"],\n                        \"additionalProperties\": False\n                    }\n                },\n                \"relationships\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"source\": {\"type\": \"string\"},\n                            \"target\": {\"type\": \"string\"},\n                            \"relationship\": {\"type\": \"string\"}\n                        },\n                        \"required\": [\"source\", \"target\", \"relationship\"],\n                        \"additionalProperties\": False\n                    }\n                }\n            },\n            \"required\": [\"entities\", \"relationships\"],\n            \"additionalProperties\": False\n        }\n        \n        # Modify the system message to handle properties as a string that will be parsed later\n        system_message = {\n            \"role\": \"system\",\n            \"content\": (\n                \"You are an entity extraction assistant. Extract entities and their relationships from text precisely and thoroughly. \"\n                \"For entities, include entity label and type (PERSON, ORGANIZATION, LOCATION, CONCEPT, etc.). \"\n                \"For relationships, use a simple format with source, target, and relationship fields.\"\n            )\n        }\n\n        user_message = {\n            \"role\": \"user\",\n            \"content\": (\n                \"Extract named entities and their relationships from the following text. \"\n                \"For entities, include entity label and type (PERSON, ORGANIZATION, LOCATION, CONCEPT, etc.). \"\n                \"For relationships, simply specify the source entity, target entity, and the relationship between them. \"\n                \"Return your response as valid JSON:\\n\\n\" + content_limited\n            )\n        }\n\n        if settings.GRAPH_PROVIDER == \"openai\":\n            client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)\n            try:\n                response = await client.responses.create(\n                    model=settings.GRAPH_MODEL,\n                    input=[system_message, user_message],\n                    text={\n                        \"format\": {\n                            \"type\": \"json_schema\",\n                            \"name\": \"entity_extraction\",\n                            \"schema\": json_schema,\n                            \"strict\": True\n                        },\n                    },\n                )\n\n                if response.output_text:\n                    import json\n                    extraction_data = json.loads(response.output_text)\n\n                    for entity in extraction_data.get(\"entities\", []):\n                        entity[\"properties\"] = {}\n\n                    extraction_result = ExtractionResult(**extraction_data)\n                elif hasattr(response, 'refusal') and response.refusal:\n                    # Handle refusal\n                    logger.warning(f\"OpenAI refused to extract entities: {response.refusal}\")\n                    return [], []\n                else:\n                    # Handle empty response\n                    logger.warning(f\"Empty response from OpenAI for document {doc_id}, chunk {chunk_number}\")\n                    return [], []\n\n            except Exception as e:\n                logger.error(f\"Error during entity extraction with OpenAI: {str(e)}\")\n                return [], []\n\n        elif settings.GRAPH_PROVIDER == \"ollama\":\n            # For Ollama, use structured output format\n            async with httpx.AsyncClient(timeout=60.0) as client:\n                # Create the schema for structured output\n                format_schema = ExtractionResult.model_json_schema()\n\n                response = await client.post(\n                    f\"{settings.EMBEDDING_OLLAMA_BASE_URL}/api/chat\",\n                    json={\n                        \"model\": settings.GRAPH_MODEL,\n                        \"messages\": [system_message, user_message],\n                        \"stream\": False,\n                        \"format\": format_schema\n                    },\n                )\n                response.raise_for_status()\n                result = response.json()\n\n                # Log the raw response for debugging\n                logger.info(f\"Raw Ollama response for entity extraction: {result['message']['content']}\")\n\n                # Parse the JSON response - Pydantic will handle validation\n                extraction_result = ExtractionResult.model_validate_json(result[\"message\"][\"content\"])\n        else:\n            logger.error(f\"Unsupported graph provider: {settings.GRAPH_PROVIDER}\")\n            return [], []\n\n        # Process extraction results\n        entities, relationships = self._process_extraction_results(extraction_result, doc_id, chunk_number)\n        logger.info(f\"Extracted {len(entities)} entities and {len(relationships)} relationships from document {doc_id}, chunk {chunk_number}\")\n        return entities, relationships\n\n    def _process_extraction_results(\n        self,\n        extraction_result: ExtractionResult,\n        doc_id: str,\n        chunk_number: int\n    ) -> Tuple[List[Entity], List[Relationship]]:\n        \"\"\"Process extraction results into entity and relationship objects.\"\"\"\n        # Initialize chunk_sources with the current chunk - reused across entities\n        chunk_sources = {doc_id: [chunk_number]}\n\n        # Convert extracted data to entity objects using list comprehension\n        entities = [\n            Entity(\n                label=entity.label,\n                type=entity.type,\n                properties=entity.properties,\n                chunk_sources=chunk_sources.copy(),  # Need to copy to avoid shared reference\n                document_ids=[doc_id]\n            )\n            for entity in extraction_result.entities\n        ]\n\n        # Create a mapping of entity labels to IDs\n        entity_mapping = {entity.label: entity.id for entity in entities}\n\n        # Convert to relationship objects using list comprehension with filtering\n        relationships = [\n            Relationship(\n                source_id=entity_mapping[rel.source],\n                target_id=entity_mapping[rel.target],\n                type=rel.relationship,\n                chunk_sources=chunk_sources.copy(),  # Need to copy to avoid shared reference\n                document_ids=[doc_id]\n            )\n            for rel in extraction_result.relationships\n            if rel.source in entity_mapping and rel.target in entity_mapping\n        ]\n\n        return entities, relationships\n\n    async def query_with_graph(\n        self,\n        query: str,\n        graph_name: str,\n        auth: AuthContext,\n        document_service,  # Passed to avoid circular import\n        filters: Optional[Dict[str, Any]] = None,\n        k: int = 20,\n        min_score: float = 0.0,\n        max_tokens: Optional[int] = None,\n        temperature: Optional[float] = None,\n        use_reranking: Optional[bool] = None,\n        use_colpali: Optional[bool] = None,\n        hop_depth: int = 1,\n        include_paths: bool = False,\n    ) -> CompletionResponse:\n        \"\"\"Generate completion using knowledge graph-enhanced retrieval.\n        \n        This method enhances retrieval by:\n        1. Extracting entities from the query\n        2. Finding similar entities in the graph\n        3. Traversing the graph to find related entities\n        4. Retrieving chunks containing these entities\n        5. Combining with traditional vector search results\n        6. Generating a completion with enhanced context\n        \n        Args:\n            query: The query text\n            graph_name: Name of the graph to use\n            auth: Authentication context\n            document_service: DocumentService instance for retrieving documents\n            filters: Optional metadata filters\n            k: Number of chunks to retrieve\n            min_score: Minimum similarity score\n            max_tokens: Maximum tokens for completion\n            temperature: Temperature for completion\n            use_reranking: Whether to use reranking\n            use_colpali: Whether to use colpali embedding\n            hop_depth: Number of relationship hops to traverse (1-3)\n            include_paths: Whether to include relationship paths in response\n        \"\"\"\n        logger.info(f\"Querying with graph: {graph_name}, hop depth: {hop_depth}\")\n\n        # Get the knowledge graph\n        graph = await self.db.get_graph(graph_name, auth)\n        if not graph:\n            logger.warning(f\"Graph '{graph_name}' not found or not accessible\")\n            # Fall back to standard retrieval if graph not found\n            return await document_service.query(\n                query=query,\n                auth=auth,\n                filters=filters,\n                k=k,\n                min_score=min_score,\n                max_tokens=max_tokens,\n                temperature=temperature,\n                use_reranking=use_reranking,\n                use_colpali=use_colpali,\n                graph_name=None,\n            )\n\n        # Parallel approach\n        # 1. Standard vector search\n        vector_chunks = await document_service.retrieve_chunks(\n            query, auth, filters, k, min_score, use_reranking, use_colpali\n        )\n        logger.info(f\"Vector search retrieved {len(vector_chunks)} chunks\")\n\n        # 2. Graph-based retrieval\n        # First extract entities from the query\n        query_entities = await self._extract_entities_from_query(query)\n        logger.info(f\"Extracted {len(query_entities)} entities from query: {', '.join(e.label for e in query_entities)}\")\n\n        # If no entities extracted, fallback to embedding similarity\n        if not query_entities:\n            # Find similar entities using embedding similarity\n            top_entities = await self._find_similar_entities(query, graph.entities, k)\n        else:\n            # Use extracted entities directly\n            entity_map = {entity.label.lower(): entity for entity in graph.entities}\n            matched_entities = []\n\n            # Match extracted entities with graph entities\n            for query_entity in query_entities:\n                if query_entity.label.lower() in entity_map:\n                    matched_entities.append(entity_map[query_entity.label.lower()])\n\n            # If no matches, fallback to embedding similarity\n            if matched_entities:\n                top_entities = [(entity, 1.0) for entity in matched_entities]  # Score 1.0 for direct matches\n            else:\n                top_entities = await self._find_similar_entities(query, graph.entities, k)\n\n        logger.info(f\"Found {len(top_entities)} relevant entities in graph\")\n\n        # Traverse the graph to find related entities\n        expanded_entities = self._expand_entities(graph, [e[0] for e in top_entities], hop_depth)\n        logger.info(f\"Expanded to {len(expanded_entities)} entities after traversal\")\n\n        # Get specific chunks containing these entities\n        graph_chunks = await self._retrieve_entity_chunks(expanded_entities, auth, filters, document_service)\n        logger.info(f\"Retrieved {len(graph_chunks)} chunks containing relevant entities\")\n\n        # Calculate paths if requested\n        paths = []\n        if include_paths:\n            paths = self._find_relationship_paths(graph, [e[0] for e in top_entities], hop_depth)\n            logger.info(f\"Found {len(paths)} relationship paths\")\n\n        # Combine vector and graph results\n        combined_chunks = self._combine_chunk_results(vector_chunks, graph_chunks, k)\n\n        # Generate completion with enhanced context\n        completion_response = await self._generate_completion(\n            query,\n            combined_chunks,\n            document_service,\n            max_tokens,\n            temperature,\n            include_paths,\n            paths,\n            auth,\n            graph_name\n        )\n\n        return completion_response\n\n    async def _extract_entities_from_query(self, query: str) -> List[Entity]:\n        \"\"\"Extract entities from the query text using the LLM.\"\"\"\n        try:\n            # Extract entities from the query using the same extraction function\n            # but with a simplified prompt specific for queries\n            entities, _ = await self.extract_entities_from_text(\n                content=query,\n                doc_id=\"query\",  # Use \"query\" as doc_id \n                chunk_number=0   # Use 0 as chunk_number\n            )\n            return entities\n        except Exception as e:\n            # If extraction fails, log and return empty list to fall back to embedding similarity\n            logger.warning(f\"Failed to extract entities from query: {e}\")\n            return []\n\n    async def _find_similar_entities(\n        self, query: str, entities: List[Entity], k: int\n    ) -> List[Tuple[Entity, float]]:\n        \"\"\"Find entities similar to the query based on embedding similarity.\"\"\"\n        if not entities:\n            return []\n\n        # Get embedding for query\n        query_embedding = await self.embedding_model.embed_for_query(query)\n\n        # Create entity text representations and get embeddings for all entities\n        entity_texts = [\n            f\"{entity.label} {entity.type} \" + \" \".join(\n                f\"{key}: {value}\" for key, value in entity.properties.items()\n            )\n            for entity in entities\n        ]\n\n        # Get embeddings for all entity texts\n        entity_embeddings = await self._batch_get_embeddings(entity_texts)\n\n        # Calculate similarities and pair with entities\n        entity_similarities = [\n            (entity, self._calculate_cosine_similarity(query_embedding, embedding))\n            for entity, embedding in zip(entities, entity_embeddings)\n        ]\n\n        # Sort by similarity and take top k\n        entity_similarities.sort(key=lambda x: x[1], reverse=True)\n        return entity_similarities[:min(k, len(entity_similarities))]\n\n    async def _batch_get_embeddings(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"Get embeddings for a batch of texts efficiently.\"\"\"\n        # This could be implemented with proper batch embedding if the embedding model supports it\n        # For now, we'll just map over the texts and get embeddings one by one\n        return [await self.embedding_model.embed_for_query(text) for text in texts]\n\n    def _expand_entities(self, graph: Graph, seed_entities: List[Entity], hop_depth: int) -> List[Entity]:\n        \"\"\"Expand entities by traversing relationships.\"\"\"\n        if hop_depth <= 1:\n            return seed_entities\n\n        # Create a set of entity IDs we've seen\n        seen_entity_ids = {entity.id for entity in seed_entities}\n        all_entities = list(seed_entities)\n\n        # Create a map for fast entity lookup\n        entity_map = {entity.id: entity for entity in graph.entities}\n        \n        # For each hop\n        for _ in range(hop_depth - 1):\n            new_entities = []\n            \n            # For each entity we've found so far\n            for entity in all_entities:\n                # Find connected entities through relationships\n                connected_ids = self._get_connected_entity_ids(graph.relationships, entity.id, seen_entity_ids)\n                \n                # Add new connected entities\n                for entity_id in connected_ids:\n                    if target_entity := entity_map.get(entity_id):\n                        new_entities.append(target_entity)\n                        seen_entity_ids.add(entity_id)\n            \n            # Add new entities to our list\n            all_entities.extend(new_entities)\n            \n            # Stop if no new entities found\n            if not new_entities:\n                break\n                \n        return all_entities\n    \n    def _get_connected_entity_ids(\n        self, relationships: List[Relationship], entity_id: str, seen_ids: Set[str]\n    ) -> Set[str]:\n        \"\"\"Get IDs of entities connected to the given entity that haven't been seen yet.\"\"\"\n        connected_ids = set()\n        \n        for relationship in relationships:\n            # Check outgoing relationships\n            if relationship.source_id == entity_id and relationship.target_id not in seen_ids:\n                connected_ids.add(relationship.target_id)\n                \n            # Check incoming relationships\n            elif relationship.target_id == entity_id and relationship.source_id not in seen_ids:\n                connected_ids.add(relationship.source_id)\n                \n        return connected_ids\n    \n    async def _retrieve_entity_chunks(\n        self, \n        entities: List[Entity], \n        auth: AuthContext,\n        filters: Optional[Dict[str, Any]],\n        document_service\n    ) -> List[ChunkResult]:\n        \"\"\"Retrieve chunks containing the specified entities.\"\"\"\n        if not entities:\n            return []\n            \n        # Collect all chunk sources from entities using set comprehension\n        entity_chunk_sources = {\n            (doc_id, chunk_num)\n            for entity in entities\n            for doc_id, chunk_numbers in entity.chunk_sources.items()\n            for chunk_num in chunk_numbers\n        }\n        \n        # Get unique document IDs for authorization check\n        doc_ids = {doc_id for doc_id, _ in entity_chunk_sources}\n        \n        # Check document authorization\n        documents = await document_service.batch_retrieve_documents(list(doc_ids), auth)\n        \n        # Apply filters if needed\n        authorized_doc_ids = {\n            doc.external_id for doc in documents\n            if not filters or all(doc.metadata.get(k) == v for k, v in filters.items())\n        }\n            \n        # Filter chunk sources to only those from authorized documents\n        chunk_sources = [\n            ChunkSource(document_id=doc_id, chunk_number=chunk_num)\n            for doc_id, chunk_num in entity_chunk_sources\n            if doc_id in authorized_doc_ids\n        ]\n        \n        # Retrieve and return chunks if we have any valid sources\n        return await document_service.batch_retrieve_chunks(chunk_sources, auth) if chunk_sources else []\n    \n    def _combine_chunk_results(\n        self, vector_chunks: List[ChunkResult], graph_chunks: List[ChunkResult], k: int\n    ) -> List[ChunkResult]:\n        \"\"\"Combine and deduplicate chunk results from vector search and graph search.\"\"\"\n        # Create dictionary with vector chunks first\n        all_chunks = {f\"{chunk.document_id}_{chunk.chunk_number}\": chunk for chunk in vector_chunks}\n        \n        # Process and add graph chunks with a boost\n        for chunk in graph_chunks:\n            chunk_key = f\"{chunk.document_id}_{chunk.chunk_number}\"\n            \n            # Set default score if missing and apply boost (5%)\n            chunk.score = min(1.0, (getattr(chunk, 'score', 0.7) or 0.7) * 1.05)\n            \n            # Keep the higher-scored version\n            if chunk_key not in all_chunks or chunk.score > all_chunks[chunk_key].score:\n                all_chunks[chunk_key] = chunk\n        \n        # Convert to list, sort by score, and return top k\n        return sorted(\n            all_chunks.values(), \n            key=lambda x: getattr(x, 'score', 0), \n            reverse=True\n        )[:k]\n    \n    def _find_relationship_paths(\n        self, graph: Graph, seed_entities: List[Entity], hop_depth: int\n    ) -> List[List[str]]:\n        \"\"\"Find meaningful paths in the graph starting from seed entities.\"\"\"\n        paths = []\n        entity_map = {entity.id: entity for entity in graph.entities}\n        \n        # For each seed entity\n        for start_entity in seed_entities:\n            # Start BFS from this entity\n            queue = [(start_entity.id, [start_entity.label])]\n            visited = set([start_entity.id])\n            \n            while queue:\n                entity_id, path = queue.pop(0)\n                \n                # If path is already at max length, record it but don't expand\n                if len(path) >= hop_depth * 2:  # *2 because path includes relationship types\n                    paths.append(path)\n                    continue\n                    \n                # Find connected relationships\n                for relationship in graph.relationships:\n                    # Process both outgoing and incoming relationships\n                    if relationship.source_id == entity_id:\n                        target_id = relationship.target_id\n                        if target_id in visited:\n                            continue\n                            \n                        target_entity = entity_map.get(target_id)\n                        if not target_entity:\n                            continue\n                            \n                        # Check for common chunks\n                        common_chunks = self._find_common_chunks(\n                            entity_map[entity_id], \n                            target_entity, \n                            relationship\n                        )\n                        \n                        # Only include relationships where entities co-occur\n                        if common_chunks:\n                            visited.add(target_id)\n                            # Create path with relationship info\n                            rel_context = f\"({relationship.type}, {len(common_chunks)} shared chunks)\"\n                            new_path = path + [rel_context, target_entity.label]\n                            queue.append((target_id, new_path))\n                            paths.append(new_path)\n                            \n                    elif relationship.target_id == entity_id:\n                        source_id = relationship.source_id\n                        if source_id in visited:\n                            continue\n                            \n                        source_entity = entity_map.get(source_id)\n                        if not source_entity:\n                            continue\n                            \n                        # Check for common chunks\n                        common_chunks = self._find_common_chunks(\n                            entity_map[entity_id], \n                            source_entity, \n                            relationship\n                        )\n                        \n                        # Only include relationships where entities co-occur\n                        if common_chunks:\n                            visited.add(source_id)\n                            # Create path with relationship info (note reverse direction)\n                            rel_context = f\"(is {relationship.type} of, {len(common_chunks)} shared chunks)\"\n                            new_path = path + [rel_context, source_entity.label]\n                            queue.append((source_id, new_path))\n                            paths.append(new_path)\n        \n        return paths\n    \n    def _find_common_chunks(\n        self, entity1: Entity, entity2: Entity, relationship: Relationship\n    ) -> Set[Tuple[str, int]]:\n        \"\"\"Find chunks that contain both entities and their relationship.\"\"\"\n        # Get chunk locations for each element\n        entity1_chunks = set()\n        for doc_id, chunk_numbers in entity1.chunk_sources.items():\n            for chunk_num in chunk_numbers:\n                entity1_chunks.add((doc_id, chunk_num))\n                \n        entity2_chunks = set()\n        for doc_id, chunk_numbers in entity2.chunk_sources.items():\n            for chunk_num in chunk_numbers:\n                entity2_chunks.add((doc_id, chunk_num))\n                \n        rel_chunks = set()\n        for doc_id, chunk_numbers in relationship.chunk_sources.items():\n            for chunk_num in chunk_numbers:\n                rel_chunks.add((doc_id, chunk_num))\n        \n        # Return intersection\n        return entity1_chunks.intersection(entity2_chunks).intersection(rel_chunks)\n    \n    def _calculate_cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:\n        \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n        # Convert to numpy arrays and calculate in one go\n        vec1_np, vec2_np = np.array(vec1), np.array(vec2)\n        \n        # Get magnitudes\n        magnitude1, magnitude2 = np.linalg.norm(vec1_np), np.linalg.norm(vec2_np)\n        \n        # Avoid division by zero and calculate similarity\n        return 0 if magnitude1 == 0 or magnitude2 == 0 else np.dot(vec1_np, vec2_np) / (magnitude1 * magnitude2)\n    \n    async def _generate_completion(\n        self,\n        query: str,\n        chunks: List[ChunkResult],\n        document_service,\n        max_tokens: Optional[int] = None,\n        temperature: Optional[float] = None,\n        include_paths: bool = False,\n        paths: Optional[List[List[str]]] = None,\n        auth: Optional[AuthContext] = None,\n        graph_name: Optional[str] = None,\n    ) -> CompletionResponse:\n        \"\"\"Generate completion using the retrieved chunks and optional path information.\"\"\"\n        if not chunks:\n            chunks = []  # Ensure chunks is a list even if empty\n            \n        # Create document results for context augmentation\n        documents = await document_service._create_document_results(\n            auth, chunks\n        )\n        \n        # Create augmented chunk contents\n        chunk_contents = [\n            chunk.augmented_content(documents[chunk.document_id]) \n            for chunk in chunks \n            if chunk.document_id in documents\n        ]\n        \n        # Include graph context in prompt if paths are requested\n        if include_paths and paths:\n            # Create a readable representation of the paths\n            paths_text = \"Knowledge Graph Context:\\n\"\n            # Limit to 5 paths to avoid token limits\n            for path in paths[:5]:\n                paths_text += \" -> \".join(path) + \"\\n\"\n                \n            # Add to the first chunk or create a new first chunk if none\n            if chunk_contents:\n                chunk_contents[0] = paths_text + \"\\n\\n\" + chunk_contents[0]\n            else:\n                chunk_contents = [paths_text]\n                \n        # Generate completion\n        request = CompletionRequest(\n            query=query,\n            context_chunks=chunk_contents,\n            max_tokens=max_tokens,\n            temperature=temperature,\n        )\n        \n        # Get completion from model\n        response = await document_service.completion_model.complete(request)\n        \n        # Add sources information\n        response.sources = [\n            ChunkSource(\n                document_id=chunk.document_id,\n                chunk_number=chunk.chunk_number,\n                score=getattr(chunk, 'score', 0)\n            )\n            for chunk in chunks\n        ]\n        \n        # Include graph metadata if paths were requested\n        if include_paths:\n            if not hasattr(response, 'metadata') or response.metadata is None:\n                response.metadata = {}\n            \n            # Extract unique entities from paths (items that don't start with \"(\")\n            unique_entities = set()\n            if paths:\n                for path in paths[:5]:\n                    for item in path:\n                        if not item.startswith(\"(\"):\n                            unique_entities.add(item)\n            \n            # Add graph-specific metadata\n            response.metadata[\"graph\"] = {\n                \"name\": graph_name,\n                \"relevant_entities\": list(unique_entities),\n                \"paths\": [\" -> \".join(path) for path in paths[:5]] if paths else [],\n            }\n\n        return response\n"}
{"type": "source_file", "path": "core/models/request.py", "content": "from typing import Dict, List, Optional, Any\nfrom pydantic import BaseModel, Field\n\nfrom core.models.documents import Document\n\n\nclass RetrieveRequest(BaseModel):\n    \"\"\"Base retrieve request model\"\"\"\n\n    query: str = Field(..., min_length=1)\n    filters: Optional[Dict[str, Any]] = None\n    k: int = Field(default=4, gt=0)\n    min_score: float = Field(default=0.0)\n    use_reranking: Optional[bool] = None  # If None, use default from config\n    use_colpali: Optional[bool] = None\n    graph_name: Optional[str] = Field(\n        None, description=\"Name of the graph to use for knowledge graph-enhanced retrieval\"\n    )\n    hop_depth: Optional[int] = Field(\n        1, description=\"Number of relationship hops to traverse in the graph\", ge=1, le=3\n    )\n    include_paths: Optional[bool] = Field(\n        False, description=\"Whether to include relationship paths in the response\"\n    )\n\n\nclass CompletionQueryRequest(RetrieveRequest):\n    \"\"\"Request model for completion generation\"\"\"\n\n    max_tokens: Optional[int] = None\n    temperature: Optional[float] = None\n\n\nclass IngestTextRequest(BaseModel):\n    \"\"\"Request model for ingesting text content\"\"\"\n\n    content: str\n    filename: Optional[str] = None\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n    rules: List[Dict[str, Any]] = Field(default_factory=list)\n    use_colpali: Optional[bool] = None\n\n\nclass CreateGraphRequest(BaseModel):\n    \"\"\"Request model for creating a graph\"\"\"\n\n    name: str = Field(..., description=\"Name of the graph to create\")\n    filters: Optional[Dict[str, Any]] = Field(\n        None, description=\"Optional metadata filters to determine which documents to include\"\n    )\n    documents: Optional[List[str]] = Field(\n        None, description=\"Optional list of specific document IDs to include\"\n    )\n\n\nclass BatchIngestResponse(BaseModel):\n    \"\"\"Response model for batch ingestion\"\"\"\n    documents: List[Document]\n    errors: List[Dict[str, str]]\n"}
{"type": "source_file", "path": "core/completion/ollama_completion.py", "content": "from core.completion.base_completion import BaseCompletionModel\nfrom core.models.completion import CompletionRequest, CompletionResponse\nfrom ollama import AsyncClient\n\nBASE_64_PREFIX = \"data:image/png;base64,\"\n\nclass OllamaCompletionModel(BaseCompletionModel):\n    \"\"\"Ollama completion model implementation\"\"\"\n\n    def __init__(self, model_name: str, base_url: str):\n        self.model_name = model_name\n        self.client = AsyncClient(host=base_url)\n\n    async def complete(self, request: CompletionRequest) -> CompletionResponse:\n        \"\"\"Generate completion using Ollama API\"\"\"\n        # Construct prompt with context\n        images, context = [], []\n        for chunk in request.context_chunks:\n            if chunk.startswith(BASE_64_PREFIX):\n                image_b64 = chunk.split(',', 1)[1]\n                images.append(image_b64)\n            else:\n                context.append(chunk)\n        context = \"\\n\\n\".join(context)\n        prompt = f\"\"\"You are a helpful assistant. Use the provided context to answer questions accurately.\n\n<QUESTION>\n{request.query}\n</QUESTION>\n\n<CONTEXT>\n{context}\n</CONTEXT>\n\"\"\"\n\n        # Call Ollama API\n        response = await self.client.chat(\n            model=self.model_name,\n            messages=[{\n                \"role\": \"user\",\n                \"content\": prompt,\n                \"images\": [images[0]] if images else [],\n            }],\n            options={\n                \"num_predict\": request.max_tokens,\n                \"temperature\": request.temperature,\n            },\n        )\n\n        # Ollama doesn't provide token usage info, so we'll estimate based on characters\n        completion_text = response[\"message\"][\"content\"]\n        char_to_token_ratio = 4  # Rough estimate\n        estimated_prompt_tokens = len(prompt) // char_to_token_ratio\n        estimated_completion_tokens = len(completion_text) // char_to_token_ratio\n\n        return CompletionResponse(\n            completion=completion_text,\n            usage={\n                \"prompt_tokens\": estimated_prompt_tokens,\n                \"completion_tokens\": estimated_completion_tokens,\n                \"total_tokens\": estimated_prompt_tokens + estimated_completion_tokens,\n            },\n        )\n"}
{"type": "source_file", "path": "core/models/graph.py", "content": "from typing import Dict, Any, List, Optional\nfrom datetime import UTC, datetime\nfrom pydantic import BaseModel, Field\nimport uuid\n\n\nclass Entity(BaseModel):\n    \"\"\"Represents an entity in a knowledge graph\"\"\"\n\n    id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n    label: str\n    type: str\n    properties: Dict[str, Any] = Field(default_factory=dict)\n    document_ids: List[str] = Field(default_factory=list)\n    chunk_sources: Dict[str, List[int]] = Field(default_factory=dict)\n\n    def __hash__(self):\n        return hash(self.id)\n\n    def __eq__(self, other):\n        if not isinstance(other, Entity):\n            return False\n        return self.id == other.id\n\n\nclass Relationship(BaseModel):\n    \"\"\"Represents a relationship between entities in a knowledge graph\"\"\"\n\n    id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n    source_id: str\n    target_id: str\n    type: str\n    document_ids: List[str] = Field(default_factory=list)\n    chunk_sources: Dict[str, List[int]] = Field(default_factory=dict)\n\n    def __hash__(self):\n        return hash(self.id)\n\n    def __eq__(self, other):\n        if not isinstance(other, Relationship):\n            return False\n        return self.id == other.id\n\n\nclass Graph(BaseModel):\n    \"\"\"Represents a knowledge graph\"\"\"\n\n    id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n    name: str\n    entities: List[Entity] = Field(default_factory=list)\n    relationships: List[Relationship] = Field(default_factory=list)\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n    document_ids: List[str] = Field(default_factory=list)\n    filters: Optional[Dict[str, Any]] = None\n    created_at: datetime = Field(default_factory=lambda: datetime.now(UTC))\n    updated_at: datetime = Field(default_factory=lambda: datetime.now(UTC))\n    owner: Dict[str, str] = Field(default_factory=dict)\n    access_control: Dict[str, List[str]] = Field(\n        default_factory=lambda: {\"readers\": [], \"writers\": [], \"admins\": []}\n    )\n"}
{"type": "source_file", "path": "core/services/document_service.py", "content": "import base64\nfrom io import BytesIO\nfrom typing import Dict, Any, List, Optional\nfrom fastapi import UploadFile\nfrom datetime import datetime, UTC\n\nfrom core.models.chunk import Chunk, DocumentChunk\nfrom core.models.documents import (\n    Document,\n    ChunkResult,\n    DocumentContent,\n    DocumentResult,\n    StorageFileInfo,\n)\nfrom ..models.auth import AuthContext\nfrom ..models.graph import Graph\nfrom core.services.graph_service import GraphService\nfrom core.database.base_database import BaseDatabase\nfrom core.storage.base_storage import BaseStorage\nfrom core.vector_store.base_vector_store import BaseVectorStore\nfrom core.embedding.base_embedding_model import BaseEmbeddingModel\nfrom core.parser.base_parser import BaseParser\nfrom core.completion.base_completion import BaseCompletionModel\nfrom core.models.completion import CompletionRequest, CompletionResponse, ChunkSource\nimport logging\nfrom core.reranker.base_reranker import BaseReranker\nfrom core.config import get_settings\nfrom core.cache.base_cache import BaseCache\nfrom core.cache.base_cache_factory import BaseCacheFactory\nfrom core.services.rules_processor import RulesProcessor\nfrom core.embedding.colpali_embedding_model import ColpaliEmbeddingModel\nfrom core.vector_store.multi_vector_store import MultiVectorStore\nimport filetype\nfrom filetype.types import IMAGE  # , DOCUMENT, document\nimport pdf2image\nfrom PIL.Image import Image\nimport tempfile\nimport os\n\nlogger = logging.getLogger(__name__)\nIMAGE = {im.mime for im in IMAGE}\n\n\nclass DocumentService:\n    def __init__(\n        self,\n        database: BaseDatabase,\n        vector_store: BaseVectorStore,\n        storage: BaseStorage,\n        parser: BaseParser,\n        embedding_model: BaseEmbeddingModel,\n        completion_model: BaseCompletionModel,\n        cache_factory: BaseCacheFactory,\n        reranker: Optional[BaseReranker] = None,\n        enable_colpali: bool = False,\n        colpali_embedding_model: Optional[ColpaliEmbeddingModel] = None,\n        colpali_vector_store: Optional[MultiVectorStore] = None,\n    ):\n        self.db = database\n        self.vector_store = vector_store\n        self.storage = storage\n        self.parser = parser\n        self.embedding_model = embedding_model\n        self.completion_model = completion_model\n        self.reranker = reranker\n        self.cache_factory = cache_factory\n        self.rules_processor = RulesProcessor()\n        self.colpali_embedding_model = colpali_embedding_model\n        self.colpali_vector_store = colpali_vector_store\n        \n        # Initialize the graph service\n        self.graph_service = GraphService(\n            db=database,\n            embedding_model=embedding_model,\n            completion_model=completion_model,\n        )\n\n        if colpali_vector_store:\n            colpali_vector_store.initialize()\n\n        # Cache-related data structures\n        # Maps cache name to active cache object\n        self.active_caches: Dict[str, BaseCache] = {}\n\n    async def retrieve_chunks(\n        self,\n        query: str,\n        auth: AuthContext,\n        filters: Optional[Dict[str, Any]] = None,\n        k: int = 5,\n        min_score: float = 0.0,\n        use_reranking: Optional[bool] = None,\n        use_colpali: Optional[bool] = None,\n    ) -> List[ChunkResult]:\n        \"\"\"Retrieve relevant chunks.\"\"\"\n        settings = get_settings()\n        should_rerank = use_reranking if use_reranking is not None else settings.USE_RERANKING\n\n        # Get embedding for query\n        query_embedding_regular = await self.embedding_model.embed_for_query(query)\n        query_embedding_multivector = await self.colpali_embedding_model.embed_for_query(query) if (use_colpali and self.colpali_embedding_model) else None\n        logger.info(\"Generated query embedding\")\n\n        # Find authorized documents\n        doc_ids = await self.db.find_authorized_and_filtered_documents(auth, filters)\n        if not doc_ids:\n            logger.info(\"No authorized documents found\")\n            return []\n        logger.info(f\"Found {len(doc_ids)} authorized documents\")\n\n        # Search chunks with vector similarity\n        chunks = await self.vector_store.query_similar(\n            query_embedding_regular, k=10 * k if should_rerank else k, doc_ids=doc_ids\n        )\n\n        search_multi = use_colpali and self.colpali_vector_store and query_embedding_multivector is not None\n\n        chunks_multivector = (\n            await self.colpali_vector_store.query_similar(\n                query_embedding_multivector, k=k, doc_ids=doc_ids\n            ) if search_multi else []\n        )\n\n        logger.info(f\"Found {len(chunks)} similar chunks via regular embedding\")\n        if use_colpali:\n            logger.info(\n                f\"Found {len(chunks_multivector)} similar chunks via multivector embedding since we are also using colpali\"\n            )\n\n        # Rerank chunks using the reranker if enabled and available\n        if chunks and should_rerank and self.reranker is not None:\n            chunks = await self.reranker.rerank(query, chunks)\n            chunks.sort(key=lambda x: x.score, reverse=True)\n            chunks = chunks[:k]\n            logger.info(f\"Reranked {k*10} chunks and selected the top {k}\")\n\n        chunks = chunks_multivector + chunks\n\n        # Create and return chunk results\n        results = await self._create_chunk_results(auth, chunks)\n        logger.info(f\"Returning {len(results)} chunk results\")\n        return results\n\n    async def retrieve_docs(\n        self,\n        query: str,\n        auth: AuthContext,\n        filters: Optional[Dict[str, Any]] = None,\n        k: int = 5,\n        min_score: float = 0.0,\n        use_reranking: Optional[bool] = None,\n        use_colpali: Optional[bool] = None,\n    ) -> List[DocumentResult]:\n        \"\"\"Retrieve relevant documents.\"\"\"\n        # Get chunks first\n        chunks = await self.retrieve_chunks(\n            query, auth, filters, k, min_score, use_reranking, use_colpali\n        )\n        # Convert to document results\n        results = await self._create_document_results(auth, chunks)\n        documents = list(results.values())\n        logger.info(f\"Returning {len(documents)} document results\")\n        return documents\n        \n    async def batch_retrieve_documents(\n        self,\n        document_ids: List[str],\n        auth: AuthContext\n    ) -> List[Document]:\n        \"\"\"\n        Retrieve multiple documents by their IDs in a single batch operation.\n        \n        Args:\n            document_ids: List of document IDs to retrieve\n            auth: Authentication context\n            \n        Returns:\n            List of Document objects that user has access to\n        \"\"\"\n        if not document_ids:\n            return []\n            \n        # Use the database's batch retrieval method\n        documents = await self.db.get_documents_by_id(document_ids, auth)\n        logger.info(f\"Batch retrieved {len(documents)} documents out of {len(document_ids)} requested\")\n        return documents\n        \n    async def batch_retrieve_chunks(\n        self,\n        chunk_ids: List[ChunkSource],\n        auth: AuthContext\n    ) -> List[ChunkResult]:\n        \"\"\"\n        Retrieve specific chunks by their document ID and chunk number in a single batch operation.\n        \n        Args:\n            chunk_ids: List of ChunkSource objects with document_id and chunk_number\n            auth: Authentication context\n            \n        Returns:\n            List of ChunkResult objects\n        \"\"\"\n        if not chunk_ids:\n            return []\n            \n        # Collect unique document IDs to check authorization in a single query\n        doc_ids = list({source.document_id for source in chunk_ids})\n        \n        # Find authorized documents in a single query\n        authorized_docs = await self.batch_retrieve_documents(doc_ids, auth)\n        authorized_doc_ids = {doc.external_id for doc in authorized_docs}\n        \n        # Filter sources to only include authorized documents\n        authorized_sources = [\n            source for source in chunk_ids \n            if source.document_id in authorized_doc_ids\n        ]\n        \n        if not authorized_sources:\n            return []\n            \n        # Create list of (document_id, chunk_number) tuples for vector store query\n        chunk_identifiers = [\n            (source.document_id, source.chunk_number) \n            for source in authorized_sources\n        ]\n        \n        # Retrieve the chunks from vector store in a single query\n        chunks = await self.vector_store.get_chunks_by_id(chunk_identifiers)\n        \n        # Convert to chunk results\n        results = await self._create_chunk_results(auth, chunks)\n        logger.info(f\"Batch retrieved {len(results)} chunks out of {len(chunk_ids)} requested\")\n        return results\n\n    async def query(\n        self,\n        query: str,\n        auth: AuthContext,\n        filters: Optional[Dict[str, Any]] = None,\n        k: int = 20,  # from contextual embedding paper\n        min_score: float = 0.0,\n        max_tokens: Optional[int] = None,\n        temperature: Optional[float] = None,\n        use_reranking: Optional[bool] = None,\n        use_colpali: Optional[bool] = None,\n        graph_name: Optional[str] = None,\n        hop_depth: int = 1,\n        include_paths: bool = False,\n    ) -> CompletionResponse:\n        \"\"\"Generate completion using relevant chunks as context.\n        \n        When graph_name is provided, the query will leverage the knowledge graph \n        to enhance retrieval by finding relevant entities and their connected documents.\n        \n        Args:\n            query: The query text\n            auth: Authentication context\n            filters: Optional metadata filters for documents\n            k: Number of chunks to retrieve\n            min_score: Minimum similarity score\n            max_tokens: Maximum tokens for completion\n            temperature: Temperature for completion\n            use_reranking: Whether to use reranking\n            use_colpali: Whether to use colpali embedding\n            graph_name: Optional name of the graph to use for knowledge graph-enhanced retrieval\n            hop_depth: Number of relationship hops to traverse in the graph (1-3)\n            include_paths: Whether to include relationship paths in the response\n        \"\"\"\n        if graph_name:\n            # Use knowledge graph enhanced retrieval via GraphService\n            return await self.graph_service.query_with_graph(\n                query=query,\n                graph_name=graph_name,\n                auth=auth,\n                document_service=self,\n                filters=filters,\n                k=k,\n                min_score=min_score,\n                max_tokens=max_tokens,\n                temperature=temperature,\n                use_reranking=use_reranking,\n                use_colpali=use_colpali,\n                hop_depth=hop_depth,\n                include_paths=include_paths,\n            )\n        \n        # Standard retrieval without graph\n        chunks = await self.retrieve_chunks(\n            query, auth, filters, k, min_score, use_reranking, use_colpali\n        )\n        documents = await self._create_document_results(auth, chunks)\n\n        # Create augmented chunk contents\n        chunk_contents = [chunk.augmented_content(documents[chunk.document_id]) for chunk in chunks]\n        \n        # Collect sources information\n        sources = [\n            ChunkSource(document_id=chunk.document_id, chunk_number=chunk.chunk_number, score=chunk.score)\n            for chunk in chunks\n        ]\n\n        # Generate completion\n        request = CompletionRequest(\n            query=query,\n            context_chunks=chunk_contents,\n            max_tokens=max_tokens,\n            temperature=temperature,\n        )\n\n        response = await self.completion_model.complete(request)\n        \n        # Add sources information at the document service level\n        response.sources = sources\n        \n        return response\n\n    async def ingest_text(\n        self,\n        content: str,\n        filename: Optional[str] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        auth: AuthContext = None,\n        rules: Optional[List[str]] = None,\n        use_colpali: Optional[bool] = None,\n    ) -> Document:\n        \"\"\"Ingest a text document.\"\"\"\n        if \"write\" not in auth.permissions:\n            logger.error(f\"User {auth.entity_id} does not have write permission\")\n            raise PermissionError(\"User does not have write permission\")\n\n        doc = Document(\n            content_type=\"text/plain\",\n            filename=filename,\n            metadata=metadata or {},\n            owner={\"type\": auth.entity_type, \"id\": auth.entity_id},\n            access_control={\n                \"readers\": [auth.entity_id],\n                \"writers\": [auth.entity_id],\n                \"admins\": [auth.entity_id],\n            },\n        )\n        logger.info(f\"Created text document record with ID {doc.external_id}\")\n\n        # Apply rules if provided\n        if rules:\n            rule_metadata, modified_text = await self.rules_processor.process_rules(content, rules)\n            # Update document metadata with extracted metadata from rules\n            metadata.update(rule_metadata)\n            doc.metadata = metadata  # Update doc metadata after rules\n\n            if modified_text:\n                content = modified_text\n                logger.info(\"Updated content with modified text from rules\")\n\n        # Store full content before chunking\n        doc.system_metadata[\"content\"] = content\n\n        # Split into chunks after all processing is done\n        chunks = await self.parser.split_text(content)\n        if not chunks:\n            raise ValueError(\"No content chunks extracted\")\n        logger.info(f\"Split processed text into {len(chunks)} chunks\")\n\n        # Generate embeddings for chunks\n        embeddings = await self.embedding_model.embed_for_ingestion(chunks)\n        logger.info(f\"Generated {len(embeddings)} embeddings\")\n        chunk_objects = self._create_chunk_objects(doc.external_id, chunks, embeddings)\n        logger.info(f\"Created {len(chunk_objects)} chunk objects\")\n\n        chunk_objects_multivector = []\n\n        if use_colpali and self.colpali_embedding_model:\n            embeddings_multivector = await self.colpali_embedding_model.embed_for_ingestion(chunks)\n            logger.info(\n                f\"Generated {len(embeddings_multivector)} embeddings for multivector embedding\"\n            )\n            chunk_objects_multivector = self._create_chunk_objects(\n                doc.external_id, chunks, embeddings_multivector\n            )\n            logger.info(\n                f\"Created {len(chunk_objects_multivector)} chunk objects for multivector embedding\"\n            )\n\n        # Create and store chunk objects\n\n        # Store everything\n        await self._store_chunks_and_doc(chunk_objects, doc, use_colpali, chunk_objects_multivector)\n        logger.info(f\"Successfully stored text document {doc.external_id}\")\n\n        return doc\n\n    async def ingest_file(\n        self,\n        file: UploadFile,\n        metadata: Dict[str, Any],\n        auth: AuthContext,\n        rules: Optional[List[str]] = None,\n        use_colpali: Optional[bool] = None,\n    ) -> Document:\n        \"\"\"Ingest a file document.\"\"\"\n        if \"write\" not in auth.permissions:\n            raise PermissionError(\"User does not have write permission\")\n\n        # Read file content\n        file_content = await file.read()\n        file_type = filetype.guess(file_content)\n        \n        # Set default mime type for cases where filetype.guess returns None\n        mime_type = \"\"\n        if file_type is not None:\n            mime_type = file_type.mime\n        elif file.filename:\n            # Try to determine by file extension as fallback\n            import mimetypes\n            guessed_type = mimetypes.guess_type(file.filename)[0]\n            if guessed_type:\n                mime_type = guessed_type\n            else:\n                # Default for text files\n                mime_type = \"text/plain\"\n        else:\n            mime_type = \"application/octet-stream\"  # Generic binary data\n            \n        logger.info(f\"Determined MIME type: {mime_type} for file {file.filename}\")\n\n        # Parse file to text first\n        additional_metadata, text = await self.parser.parse_file_to_text(\n            file_content, file.filename\n        )\n        logger.info(f\"Parsed file into text of length {len(text)}\")\n\n        # Apply rules if provided\n        if rules:\n            rule_metadata, modified_text = await self.rules_processor.process_rules(text, rules)\n            # Update document metadata with extracted metadata from rules\n            metadata.update(rule_metadata)\n            if modified_text:\n                text = modified_text\n                logger.info(\"Updated text with modified content from rules\")\n\n        # Create document record\n        doc = Document(\n            content_type=mime_type,\n            filename=file.filename,\n            metadata=metadata,\n            owner={\"type\": auth.entity_type, \"id\": auth.entity_id},\n            access_control={\n                \"readers\": [auth.entity_id],\n                \"writers\": [auth.entity_id],\n                \"admins\": [auth.entity_id],\n            },\n            additional_metadata=additional_metadata,\n        )\n\n        # Store full content\n        doc.system_metadata[\"content\"] = text\n        logger.info(f\"Created file document record with ID {doc.external_id}\")\n\n        file_content_base64 = base64.b64encode(file_content).decode()\n        # Store the original file\n        storage_info = await self.storage.upload_from_base64(\n            file_content_base64, doc.external_id, file.content_type\n        )\n        doc.storage_info = {\"bucket\": storage_info[0], \"key\": storage_info[1]}\n        logger.info(f\"Stored file in bucket `{storage_info[0]}` with key `{storage_info[1]}`\")\n\n        # Split into chunks after all processing is done\n        chunks = await self.parser.split_text(text)\n        if not chunks:\n            raise ValueError(\"No content chunks extracted\")\n        logger.info(f\"Split processed text into {len(chunks)} chunks\")\n\n        # Generate embeddings for chunks\n        embeddings = await self.embedding_model.embed_for_ingestion(chunks)\n        logger.info(f\"Generated {len(embeddings)} embeddings\")\n\n        # Create and store chunk objects\n        chunk_objects = self._create_chunk_objects(doc.external_id, chunks, embeddings)\n        logger.info(f\"Created {len(chunk_objects)} chunk objects\")\n\n        chunk_objects_multivector = []\n        logger.info(f\"use_colpali: {use_colpali}\")\n        if use_colpali and self.colpali_embedding_model:\n            chunks_multivector = self._create_chunks_multivector(\n                file_type, file_content_base64, file_content, chunks\n            )\n            logger.info(f\"Created {len(chunks_multivector)} chunks for multivector embedding\")\n            colpali_embeddings = await self.colpali_embedding_model.embed_for_ingestion(\n                chunks_multivector\n            )\n            logger.info(f\"Generated {len(colpali_embeddings)} embeddings for multivector embedding\")\n            chunk_objects_multivector = self._create_chunk_objects(\n                doc.external_id, chunks_multivector, colpali_embeddings\n            )\n\n        # Store everything\n        doc.chunk_ids = await self._store_chunks_and_doc(\n            chunk_objects, doc, use_colpali, chunk_objects_multivector\n        )\n        logger.info(f\"Successfully stored file document {doc.external_id}\")\n\n        return doc\n\n    def img_to_base64_str(self, img: Image):\n        buffered = BytesIO()\n        img.save(buffered, format=\"PNG\")\n        buffered.seek(0)\n        img_byte = buffered.getvalue()\n        img_str = \"data:image/png;base64,\" + base64.b64encode(img_byte).decode()\n        return img_str\n\n    def _create_chunks_multivector(\n        self, file_type, file_content_base64: str, file_content: bytes, chunks: List[Chunk]\n    ):\n        # Handle the case where file_type is None\n        mime_type = file_type.mime if file_type is not None else \"text/plain\"\n        logger.info(f\"Creating chunks for multivector embedding for file type {mime_type}\")\n        \n        # If file_type is None, treat it as a text file\n        if file_type is None:\n            logger.info(\"File type is None, treating as text\")\n            return [\n                Chunk(content=chunk.content, metadata=(chunk.metadata | {\"is_image\": False}))\n                for chunk in chunks\n            ]\n            \n        match mime_type:\n            case file_type if file_type in IMAGE:\n                return [Chunk(content=file_content_base64, metadata={\"is_image\": True})]\n            case \"application/pdf\":\n                logger.info(\"Working with PDF file!\")\n                images = pdf2image.convert_from_bytes(file_content)\n                images_b64 = [self.img_to_base64_str(image) for image in images]\n                return [\n                    Chunk(content=image_b64, metadata={\"is_image\": True})\n                    for image_b64 in images_b64\n                ]\n            case \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\" | \"application/msword\":\n                logger.info(\"Working with Word document!\")\n                # Check if file content is empty\n                if not file_content or len(file_content) == 0:\n                    logger.error(\"Word document content is empty\")\n                    return [\n                        Chunk(content=chunk.content, metadata=(chunk.metadata | {\"is_image\": False}))\n                        for chunk in chunks\n                    ]\n                \n                # Convert Word document to PDF first\n                with tempfile.NamedTemporaryFile(suffix=\".docx\", delete=False) as temp_docx:\n                    temp_docx.write(file_content)\n                    temp_docx_path = temp_docx.name\n                \n                with tempfile.NamedTemporaryFile(suffix=\".pdf\", delete=False) as temp_pdf:\n                    temp_pdf_path = temp_pdf.name\n                \n                try:\n                    # Convert Word to PDF\n                    import subprocess\n                    \n                    # Get the base filename without extension\n                    base_filename = os.path.splitext(os.path.basename(temp_docx_path))[0]\n                    output_dir = os.path.dirname(temp_pdf_path)\n                    expected_pdf_path = os.path.join(output_dir, f\"{base_filename}.pdf\")\n                    \n                    result = subprocess.run(\n                        [\"soffice\", \"--headless\", \"--convert-to\", \"pdf\", \"--outdir\", \n                         output_dir, temp_docx_path],\n                        capture_output=True,\n                        text=True\n                    )\n                    \n                    if result.returncode != 0:\n                        logger.error(f\"Failed to convert Word to PDF: {result.stderr}\")\n                        return [\n                            Chunk(content=chunk.content, metadata=(chunk.metadata | {\"is_image\": False}))\n                            for chunk in chunks\n                        ]\n                    \n                    # LibreOffice creates the PDF with the same base name in the output directory\n                    # Check if the expected PDF file exists\n                    if not os.path.exists(expected_pdf_path) or os.path.getsize(expected_pdf_path) == 0:\n                        logger.error(f\"Generated PDF is empty or doesn't exist at expected path: {expected_pdf_path}\")\n                        return [\n                            Chunk(content=chunk.content, metadata=(chunk.metadata | {\"is_image\": False}))\n                            for chunk in chunks\n                        ]\n                    \n                    # Now process the PDF using the correct path\n                    with open(expected_pdf_path, \"rb\") as pdf_file:\n                        pdf_content = pdf_file.read()\n                    \n                    try:\n                        images = pdf2image.convert_from_bytes(pdf_content)\n                        if not images:\n                            logger.warning(\"No images extracted from PDF\")\n                            return [\n                                Chunk(content=chunk.content, metadata=(chunk.metadata | {\"is_image\": False}))\n                                for chunk in chunks\n                            ]\n                        \n                        images_b64 = [self.img_to_base64_str(image) for image in images]\n                        return [\n                            Chunk(content=image_b64, metadata={\"is_image\": True})\n                            for image_b64 in images_b64\n                        ]\n                    except Exception as pdf_error:\n                        logger.error(f\"Error converting PDF to images: {str(pdf_error)}\")\n                        return [\n                            Chunk(content=chunk.content, metadata=(chunk.metadata | {\"is_image\": False}))\n                            for chunk in chunks\n                        ]\n                except Exception as e:\n                    logger.error(f\"Error processing Word document: {str(e)}\")\n                    return [\n                        Chunk(content=chunk.content, metadata=(chunk.metadata | {\"is_image\": False}))\n                        for chunk in chunks\n                    ]\n                finally:\n                    # Clean up temporary files\n                    if os.path.exists(temp_docx_path):\n                        os.unlink(temp_docx_path)\n                    if os.path.exists(temp_pdf_path):\n                        os.unlink(temp_pdf_path)\n                    # Also clean up the expected PDF path if it exists and is different from temp_pdf_path\n                    if 'expected_pdf_path' in locals() and os.path.exists(expected_pdf_path) and expected_pdf_path != temp_pdf_path:\n                        os.unlink(expected_pdf_path)\n\n            # case filetype.get_type(ext=\"txt\"):\n            #     logger.info(f\"Found text input: chunks for multivector embedding\")\n            #     return chunks.copy()\n            # TODO: Add support for office documents\n            # case document.Xls | document.Xlsx | document.Ods |document.Odp:\n            #     logger.warning(f\"Colpali is not supported for file type {file_type.mime} - skipping\")\n            # case file_type if file_type in DOCUMENT:\n            #     pass\n            case _:\n                logger.warning(\n                    f\"Colpali is not supported for file type {file_type.mime} - skipping\"\n                )\n                return [\n                    Chunk(content=chunk.content, metadata=(chunk.metadata | {\"is_image\": False}))\n                    for chunk in chunks\n                ]\n\n    def _create_chunk_objects(\n        self,\n        doc_id: str,\n        chunks: List[Chunk],\n        embeddings: List[List[float]],\n    ) -> List[DocumentChunk]:\n        \"\"\"Helper to create chunk objects\"\"\"\n        return [\n            c.to_document_chunk(chunk_number=i, embedding=embedding, document_id=doc_id)\n            for i, (embedding, c) in enumerate(zip(embeddings, chunks))\n        ]\n\n    async def _store_chunks_and_doc(\n        self,\n        chunk_objects: List[DocumentChunk],\n        doc: Document,\n        use_colpali: bool = False,\n        chunk_objects_multivector: Optional[List[DocumentChunk]] = None,\n        is_update: bool = False,\n        auth: Optional[AuthContext] = None,\n    ) -> List[str]:\n        \"\"\"Helper to store chunks and document\"\"\"\n        # Store chunks in vector store\n        success, result = await self.vector_store.store_embeddings(chunk_objects)\n        if not success:\n            raise Exception(\"Failed to store chunk embeddings\")\n        logger.debug(\"Stored chunk embeddings in vector store\")\n        doc.chunk_ids = result\n\n        if use_colpali and self.colpali_vector_store and chunk_objects_multivector:\n            success, result_multivector = await self.colpali_vector_store.store_embeddings(\n                chunk_objects_multivector\n            )\n            if not success:\n                raise Exception(\"Failed to store multivector chunk embeddings\")\n            logger.debug(\"Stored multivector chunk embeddings in vector store\")\n            doc.chunk_ids += result_multivector\n\n        # Store document metadata\n        if is_update and auth:\n            # For updates, use update_document\n            updates = {\n                \"chunk_ids\": doc.chunk_ids,\n                \"metadata\": doc.metadata,\n                \"system_metadata\": doc.system_metadata,\n                \"filename\": doc.filename,\n                \"content_type\": doc.content_type,\n                \"storage_info\": doc.storage_info,\n            }\n            if not await self.db.update_document(doc.external_id, updates, auth):\n                raise Exception(\"Failed to update document metadata\")\n            logger.debug(\"Updated document metadata in database\")\n        else:\n            # For new documents, use store_document\n            if not await self.db.store_document(doc):\n                raise Exception(\"Failed to store document metadata\")\n            logger.debug(\"Stored document metadata in database\")\n            \n        logger.debug(f\"Chunk IDs stored: {doc.chunk_ids}\")\n        return doc.chunk_ids\n\n    async def _create_chunk_results(\n        self, auth: AuthContext, chunks: List[DocumentChunk]\n    ) -> List[ChunkResult]:\n        \"\"\"Create ChunkResult objects with document metadata.\"\"\"\n        results = []\n        for chunk in chunks:\n            # Get document metadata\n            doc = await self.db.get_document(chunk.document_id, auth)\n            if not doc:\n                logger.warning(f\"Document {chunk.document_id} not found\")\n                continue\n            logger.debug(f\"Retrieved metadata for document {chunk.document_id}\")\n\n            # Generate download URL if needed\n            download_url = None\n            if doc.storage_info:\n                download_url = await self.storage.get_download_url(\n                    doc.storage_info[\"bucket\"], doc.storage_info[\"key\"]\n                )\n                logger.debug(f\"Generated download URL for document {chunk.document_id}\")\n\n            metadata = doc.metadata\n            metadata[\"is_image\"] = chunk.metadata.get(\"is_image\", False)\n            results.append(\n                ChunkResult(\n                    content=chunk.content,\n                    score=chunk.score,\n                    document_id=chunk.document_id,\n                    chunk_number=chunk.chunk_number,\n                    metadata=metadata,\n                    content_type=doc.content_type,\n                    filename=doc.filename,\n                    download_url=download_url,\n                )\n            )\n\n        logger.info(f\"Created {len(results)} chunk results\")\n        return results\n\n    async def _create_document_results(\n        self, auth: AuthContext, chunks: List[ChunkResult]\n    ) -> Dict[str, DocumentResult]:\n        \"\"\"Group chunks by document and create DocumentResult objects.\"\"\"\n        # Group chunks by document and get highest scoring chunk per doc\n        doc_chunks: Dict[str, ChunkResult] = {}\n        for chunk in chunks:\n            if (\n                chunk.document_id not in doc_chunks\n                or chunk.score > doc_chunks[chunk.document_id].score\n            ):\n                doc_chunks[chunk.document_id] = chunk\n        logger.info(f\"Grouped chunks into {len(doc_chunks)} documents\")\n        logger.info(f\"Document chunks: {doc_chunks}\")\n        results = {}\n        for doc_id, chunk in doc_chunks.items():\n            # Get document metadata\n            doc = await self.db.get_document(doc_id, auth)\n            if not doc:\n                logger.warning(f\"Document {doc_id} not found\")\n                continue\n            logger.info(f\"Retrieved metadata for document {doc_id}\")\n\n            # Create DocumentContent based on content type\n            if doc.content_type == \"text/plain\":\n                content = DocumentContent(type=\"string\", value=chunk.content, filename=None)\n                logger.debug(f\"Created text content for document {doc_id}\")\n            else:\n                # Generate download URL for file types\n                download_url = await self.storage.get_download_url(\n                    doc.storage_info[\"bucket\"], doc.storage_info[\"key\"]\n                )\n                content = DocumentContent(type=\"url\", value=download_url, filename=doc.filename)\n                logger.debug(f\"Created URL content for document {doc_id}\")\n            results[doc_id] = DocumentResult(\n                score=chunk.score,\n                document_id=doc_id,\n                metadata=doc.metadata,\n                content=content,\n                additional_metadata=doc.additional_metadata,\n            )\n\n        logger.info(f\"Created {len(results)} document results\")\n        return results\n\n    async def create_cache(\n        self,\n        name: str,\n        model: str,\n        gguf_file: str,\n        docs: List[Document | None],\n        filters: Optional[Dict[str, Any]] = None,\n    ) -> Dict[str, str]:\n        \"\"\"Create a new cache with specified configuration.\n\n        Args:\n            name: Name of the cache to create\n            model: Name of the model to use\n            gguf_file: Name of the GGUF file to use\n            filters: Optional metadata filters for documents to include\n            docs: Optional list of specific document IDs to include\n        \"\"\"\n        # Create cache metadata\n        metadata = {\n            \"model\": model,\n            \"model_file\": gguf_file,\n            \"filters\": filters,\n            \"docs\": [doc.model_dump_json() for doc in docs],\n            \"storage_info\": {\n                \"bucket\": \"caches\",\n                \"key\": f\"{name}_state.pkl\",\n            },\n        }\n\n        # Store metadata in database\n        success = await self.db.store_cache_metadata(name, metadata)\n        if not success:\n            logger.error(f\"Failed to store cache metadata for cache {name}\")\n            return {\"success\": False, \"message\": f\"Failed to store cache metadata for cache {name}\"}\n\n        # Create cache instance\n        cache = self.cache_factory.create_new_cache(\n            name=name, model=model, model_file=gguf_file, filters=filters, docs=docs\n        )\n        cache_bytes = cache.saveable_state\n        base64_cache_bytes = base64.b64encode(cache_bytes).decode()\n        bucket, key = await self.storage.upload_from_base64(\n            base64_cache_bytes,\n            key=metadata[\"storage_info\"][\"key\"],\n            bucket=metadata[\"storage_info\"][\"bucket\"],\n        )\n        return {\n            \"success\": True,\n            \"message\": f\"Cache created successfully, state stored in bucket `{bucket}` with key `{key}`\",\n        }\n\n    async def load_cache(self, name: str) -> bool:\n        \"\"\"Load a cache into memory.\n\n        Args:\n            name: Name of the cache to load\n\n        Returns:\n            bool: Whether the cache exists and was loaded successfully\n        \"\"\"\n        try:\n            # Get cache metadata from database\n            metadata = await self.db.get_cache_metadata(name)\n            if not metadata:\n                logger.error(f\"No metadata found for cache {name}\")\n                return False\n\n            # Get cache bytes from storage\n            cache_bytes = await self.storage.download_file(\n                metadata[\"storage_info\"][\"bucket\"], \"caches/\" + metadata[\"storage_info\"][\"key\"]\n            )\n            cache_bytes = cache_bytes.read()\n            cache = self.cache_factory.load_cache_from_bytes(\n                name=name, cache_bytes=cache_bytes, metadata=metadata\n            )\n            self.active_caches[name] = cache\n            return {\"success\": True, \"message\": \"Cache loaded successfully\"}\n        except Exception as e:\n            logger.error(f\"Failed to load cache {name}: {e}\")\n            # raise e\n            return {\"success\": False, \"message\": f\"Failed to load cache {name}: {e}\"}\n\n    async def update_document(\n        self,\n        document_id: str,\n        auth: AuthContext,\n        content: Optional[str] = None,\n        file: Optional[UploadFile] = None,\n        filename: Optional[str] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        rules: Optional[List] = None,\n        update_strategy: str = \"add\",\n        use_colpali: Optional[bool] = None,\n    ) -> Optional[Document]:\n        \"\"\"\n        Update a document with new content and/or metadata using the specified strategy.\n        \n        Args:\n            document_id: ID of the document to update\n            auth: Authentication context\n            content: The new text content to add (either content or file must be provided)\n            file: File to add (either content or file must be provided)\n            filename: Optional new filename for the document\n            metadata: Additional metadata to update\n            rules: Optional list of rules to apply to the content\n            update_strategy: Strategy for updating the document ('add' to append content)\n            use_colpali: Whether to use multi-vector embedding\n            \n        Returns:\n            Updated document if successful, None if failed\n        \"\"\"\n        # Validate permissions and get document\n        doc = await self._validate_update_access(document_id, auth)\n        if not doc:\n            return None\n        \n        # Get current content and determine update type\n        current_content = doc.system_metadata.get(\"content\", \"\")\n        metadata_only_update = (content is None and file is None and metadata is not None)\n        \n        # Process content based on update type\n        update_content = None\n        file_content = None\n        file_type = None\n        file_content_base64 = None\n        \n        if content is not None:\n            update_content = await self._process_text_update(content, doc, filename, metadata, rules)\n        elif file is not None:\n            update_content, file_content, file_type, file_content_base64 = await self._process_file_update(\n                file, doc, metadata, rules\n            )\n        elif not metadata_only_update:\n            logger.error(\"Neither content nor file provided for document update\")\n            return None\n        \n        # Apply content update strategy if we have new content\n        if update_content:\n            updated_content = self._apply_update_strategy(current_content, update_content, update_strategy)\n            doc.system_metadata[\"content\"] = updated_content\n        else:\n            updated_content = current_content\n        \n        # Update metadata and version information\n        self._update_metadata_and_version(doc, metadata, update_strategy, file)\n        \n        # For metadata-only updates, we don't need to re-process chunks\n        if metadata_only_update:\n            return await self._update_document_metadata_only(doc, auth)\n        \n        # Process content into chunks and generate embeddings\n        chunks, chunk_objects = await self._process_chunks_and_embeddings(doc.external_id, updated_content)\n        if not chunks:\n            return None\n        \n        # Handle colpali (multi-vector) embeddings if needed\n        chunk_objects_multivector = await self._process_colpali_embeddings(\n            use_colpali, doc.external_id, chunks, file, file_type, file_content, file_content_base64\n        )\n        \n        # Store everything - this will replace existing chunks with new ones\n        await self._store_chunks_and_doc(\n            chunk_objects, doc, use_colpali, chunk_objects_multivector, is_update=True, auth=auth\n        )\n        logger.info(f\"Successfully updated document {doc.external_id}\")\n        \n        return doc\n        \n    async def _validate_update_access(self, document_id: str, auth: AuthContext) -> Optional[Document]:\n        \"\"\"Validate user permissions and document access.\"\"\"\n        if \"write\" not in auth.permissions:\n            logger.error(f\"User {auth.entity_id} does not have write permission\")\n            raise PermissionError(\"User does not have write permission\")\n            \n        # Check if document exists and user has write access\n        doc = await self.db.get_document(document_id, auth)\n        if not doc:\n            logger.error(f\"Document {document_id} not found or not accessible\")\n            return None\n            \n        if not await self.db.check_access(document_id, auth, \"write\"):\n            logger.error(f\"User {auth.entity_id} does not have write permission for document {document_id}\")\n            raise PermissionError(f\"User does not have write permission for document {document_id}\")\n            \n        return doc\n        \n    async def _process_text_update(\n        self, \n        content: str, \n        doc: Document, \n        filename: Optional[str], \n        metadata: Optional[Dict[str, Any]], \n        rules: Optional[List]\n    ) -> str:\n        \"\"\"Process text content updates.\"\"\"\n        update_content = content\n        \n        # Update filename if provided\n        if filename:\n            doc.filename = filename\n        \n        # Apply rules if provided for text content\n        if rules:\n            rule_metadata, modified_text = await self.rules_processor.process_rules(content, rules)\n            # Update metadata with extracted metadata from rules\n            if metadata is not None:\n                metadata.update(rule_metadata)\n            \n            if modified_text:\n                update_content = modified_text\n                logger.info(\"Updated content with modified text from rules\")\n                \n        return update_content\n        \n    async def _process_file_update(\n        self,\n        file: UploadFile,\n        doc: Document,\n        metadata: Optional[Dict[str, Any]],\n        rules: Optional[List]\n    ) -> tuple[str, bytes, Any, str]:\n        \"\"\"Process file content updates.\"\"\"\n        # Read file content\n        file_content = await file.read()\n        \n        # Parse the file content\n        additional_file_metadata, file_text = await self.parser.parse_file_to_text(\n            file_content, file.filename\n        )\n        logger.info(f\"Parsed file into text of length {len(file_text)}\")\n        \n        # Apply rules if provided for file content\n        if rules:\n            rule_metadata, modified_text = await self.rules_processor.process_rules(file_text, rules)\n            # Update metadata with extracted metadata from rules\n            if metadata is not None:\n                metadata.update(rule_metadata)\n            \n            if modified_text:\n                file_text = modified_text\n                logger.info(\"Updated file content with modified text from rules\")\n        \n        # Add additional metadata from file if available\n        if additional_file_metadata:\n            if not doc.additional_metadata:\n                doc.additional_metadata = {}\n            doc.additional_metadata.update(additional_file_metadata)\n        \n        # Store file in storage if needed\n        file_content_base64 = base64.b64encode(file_content).decode()\n        \n        # Store file in storage and update storage info\n        await self._update_storage_info(doc, file, file_content_base64)\n        \n        # Store file type\n        file_type = filetype.guess(file_content)\n        if file_type:\n            doc.content_type = file_type.mime\n        else:\n            # If filetype.guess failed, try to determine from filename\n            import mimetypes\n            guessed_type = mimetypes.guess_type(file.filename)[0]\n            if guessed_type:\n                doc.content_type = guessed_type\n            else:\n                # Default fallback\n                doc.content_type = \"text/plain\" if file.filename.endswith('.txt') else \"application/octet-stream\"\n        \n        # Update filename\n        doc.filename = file.filename\n        \n        return file_text, file_content, file_type, file_content_base64\n        \n    async def _update_storage_info(self, doc: Document, file: UploadFile, file_content_base64: str):\n        \"\"\"Update document storage information for file content.\"\"\"\n        # Check if we should keep previous file versions\n        if hasattr(doc, \"storage_files\") and len(doc.storage_files) > 0:\n            # In \"add\" strategy, create a new StorageFileInfo and append it\n            storage_info = await self.storage.upload_from_base64(\n                file_content_base64, f\"{doc.external_id}_{len(doc.storage_files)}\", file.content_type\n            )\n            \n            # Create a new StorageFileInfo\n            if not hasattr(doc, \"storage_files\"):\n                doc.storage_files = []\n                \n            # If storage_files doesn't exist yet but we have legacy storage_info, migrate it\n            if len(doc.storage_files) == 0 and doc.storage_info:\n                # Create StorageFileInfo from legacy storage_info\n                legacy_file_info = StorageFileInfo(\n                    bucket=doc.storage_info.get(\"bucket\", \"\"),\n                    key=doc.storage_info.get(\"key\", \"\"),\n                    version=1,\n                    filename=doc.filename,\n                    content_type=doc.content_type,\n                    timestamp=doc.system_metadata.get(\"updated_at\", datetime.now(UTC))\n                )\n                doc.storage_files.append(legacy_file_info)\n            \n            # Add the new file to storage_files\n            new_file_info = StorageFileInfo(\n                bucket=storage_info[0],\n                key=storage_info[1],\n                version=len(doc.storage_files) + 1,\n                filename=file.filename,\n                content_type=file.content_type,\n                timestamp=datetime.now(UTC)\n            )\n            doc.storage_files.append(new_file_info)\n            \n            # Still update legacy storage_info for backward compatibility\n            doc.storage_info = {\"bucket\": storage_info[0], \"key\": storage_info[1]}\n        else:\n            # In replace mode (default), just update the storage_info\n            storage_info = await self.storage.upload_from_base64(\n                file_content_base64, doc.external_id, file.content_type\n            )\n            doc.storage_info = {\"bucket\": storage_info[0], \"key\": storage_info[1]}\n            \n            # Update storage_files field as well\n            if not hasattr(doc, \"storage_files\"):\n                doc.storage_files = []\n            \n            # Add or update the primary file info\n            new_file_info = StorageFileInfo(\n                bucket=storage_info[0],\n                key=storage_info[1],\n                version=1,\n                filename=file.filename,\n                content_type=file.content_type,\n                timestamp=datetime.now(UTC)\n            )\n            \n            # Replace the current main file (first file) or add if empty\n            if len(doc.storage_files) > 0:\n                doc.storage_files[0] = new_file_info\n            else:\n                doc.storage_files.append(new_file_info)\n                \n        logger.info(f\"Stored file in bucket `{storage_info[0]}` with key `{storage_info[1]}`\")\n        \n    def _apply_update_strategy(self, current_content: str, update_content: str, update_strategy: str) -> str:\n        \"\"\"Apply the update strategy to combine current and new content.\"\"\"\n        if update_strategy == \"add\":\n            # Append the new content\n            return current_content + \"\\n\\n\" + update_content\n        else:\n            # For now, just use 'add' as default strategy\n            logger.warning(f\"Unknown update strategy '{update_strategy}', defaulting to 'add'\")\n            return current_content + \"\\n\\n\" + update_content\n        \n    def _update_metadata_and_version(\n        self, \n        doc: Document, \n        metadata: Optional[Dict[str, Any]], \n        update_strategy: str, \n        file: Optional[UploadFile]\n    ):\n        \"\"\"Update document metadata and version tracking.\"\"\"\n        # Update metadata if provided - additive but replacing existing keys\n        if metadata:\n            doc.metadata.update(metadata)\n        \n        # Increment version\n        current_version = doc.system_metadata.get(\"version\", 1)\n        doc.system_metadata[\"version\"] = current_version + 1\n        doc.system_metadata[\"updated_at\"] = datetime.now(UTC)\n        \n        # Track update history\n        if \"update_history\" not in doc.system_metadata:\n            doc.system_metadata[\"update_history\"] = []\n            \n        update_entry = {\n            \"timestamp\": datetime.now(UTC).isoformat(),\n            \"version\": current_version + 1,\n            \"strategy\": update_strategy,\n        }\n        \n        if file:\n            update_entry[\"filename\"] = file.filename\n            \n        if metadata:\n            update_entry[\"metadata_updated\"] = True\n            \n        doc.system_metadata[\"update_history\"].append(update_entry)\n        \n    async def _update_document_metadata_only(self, doc: Document, auth: AuthContext) -> Optional[Document]:\n        \"\"\"Update document metadata without reprocessing chunks.\"\"\"\n        updates = {\n            \"metadata\": doc.metadata,\n            \"system_metadata\": doc.system_metadata,\n            \"filename\": doc.filename,\n        }\n        success = await self.db.update_document(doc.external_id, updates, auth)\n        if not success:\n            logger.error(f\"Failed to update document {doc.external_id} metadata\")\n            return None\n            \n        logger.info(f\"Successfully updated document metadata for {doc.external_id}\")\n        return doc\n        \n    async def _process_chunks_and_embeddings(self, doc_id: str, content: str) -> tuple[List[Chunk], List[DocumentChunk]]:\n        \"\"\"Process content into chunks and generate embeddings.\"\"\"\n        # Split content into chunks\n        chunks = await self.parser.split_text(content)\n        if not chunks:\n            logger.error(\"No content chunks extracted after update\")\n            return None, None\n            \n        logger.info(f\"Split updated text into {len(chunks)} chunks\")\n        \n        # Generate embeddings for new chunks\n        embeddings = await self.embedding_model.embed_for_ingestion(chunks)\n        logger.info(f\"Generated {len(embeddings)} embeddings\")\n        \n        # Create new chunk objects\n        chunk_objects = self._create_chunk_objects(doc_id, chunks, embeddings)\n        logger.info(f\"Created {len(chunk_objects)} chunk objects\")\n        \n        return chunks, chunk_objects\n        \n    async def _process_colpali_embeddings(\n        self,\n        use_colpali: bool,\n        doc_id: str,\n        chunks: List[Chunk],\n        file: Optional[UploadFile],\n        file_type: Any,\n        file_content: Optional[bytes],\n        file_content_base64: Optional[str]\n    ) -> List[DocumentChunk]:\n        \"\"\"Process colpali multi-vector embeddings if enabled.\"\"\"\n        chunk_objects_multivector = []\n        \n        if not (use_colpali and self.colpali_embedding_model and self.colpali_vector_store):\n            return chunk_objects_multivector\n            \n        # For file updates, we need special handling for images and PDFs\n        if file and file_type and (file_type.mime in IMAGE or file_type.mime == \"application/pdf\"):\n            # Rewind the file and read it again if needed\n            if hasattr(file, 'seek') and callable(file.seek) and not file_content:\n                await file.seek(0)\n                file_content = await file.read()\n                file_content_base64 = base64.b64encode(file_content).decode()\n            \n            chunks_multivector = self._create_chunks_multivector(\n                file_type, file_content_base64, file_content, chunks\n            )\n            logger.info(f\"Created {len(chunks_multivector)} chunks for multivector embedding\")\n            colpali_embeddings = await self.colpali_embedding_model.embed_for_ingestion(chunks_multivector)\n            logger.info(f\"Generated {len(colpali_embeddings)} embeddings for multivector embedding\")\n            chunk_objects_multivector = self._create_chunk_objects(\n                doc_id, chunks_multivector, colpali_embeddings\n            )\n        else:\n            # For text updates or non-image/PDF files\n            embeddings_multivector = await self.colpali_embedding_model.embed_for_ingestion(chunks)\n            logger.info(f\"Generated {len(embeddings_multivector)} embeddings for multivector embedding\")\n            chunk_objects_multivector = self._create_chunk_objects(\n                doc_id, chunks, embeddings_multivector\n            )\n            \n        logger.info(f\"Created {len(chunk_objects_multivector)} chunk objects for multivector embedding\")\n        return chunk_objects_multivector\n\n    async def create_graph(\n        self,\n        name: str,\n        auth: AuthContext,\n        filters: Optional[Dict[str, Any]] = None,\n        documents: Optional[List[str]] = None,\n    ) -> Graph:\n        \"\"\"Create a graph from documents.\n\n        This function processes documents matching filters or specific document IDs,\n        extracts entities and relationships from document chunks, and saves them as a graph.\n\n        Args:\n            name: Name of the graph to create\n            auth: Authentication context\n            filters: Optional metadata filters to determine which documents to include\n            documents: Optional list of specific document IDs to include\n\n        Returns:\n            Graph: The created graph\n        \"\"\"\n        # Delegate to the GraphService\n        return await self.graph_service.create_graph(\n            name=name,\n            auth=auth,\n            document_service=self,\n            filters=filters,\n            documents=documents,\n        )\n\n    def close(self):\n        \"\"\"Close all resources.\"\"\"\n        # Close any active caches\n        self.active_caches.clear()\n"}
{"type": "source_file", "path": "core/parser/video/__init__.py", "content": ""}
{"type": "source_file", "path": "core/embedding/colpali_embedding_model.py", "content": "import base64\nimport io\nfrom typing import List, Union\n\nimport numpy as np\nimport torch\nfrom colpali_engine.models import ColQwen2, ColQwen2Processor\nfrom PIL.Image import Image, open as open_image\n\nfrom core.embedding.base_embedding_model import BaseEmbeddingModel\nfrom core.models.chunk import Chunk\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass ColpaliEmbeddingModel(BaseEmbeddingModel):\n    def __init__(self):\n        device = (\n            \"mps\"\n            if torch.backends.mps.is_available()\n            else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        )\n        self.model = ColQwen2.from_pretrained(\n            \"vidore/colqwen2-v1.0\",\n            torch_dtype=torch.bfloat16,\n            device_map=device,  # Automatically detect and use available device\n            attn_implementation=\"flash_attention_2\" if device == \"cuda\" else \"eager\",\n        ).eval()\n        self.processor: ColQwen2Processor = ColQwen2Processor.from_pretrained(\n            \"vidore/colqwen2-v1.0\"\n        )\n\n    async def embed_for_ingestion(self, chunks: Union[Chunk, List[Chunk]]) -> List[np.ndarray]:\n        if isinstance(chunks, Chunk):\n            chunks = [chunks]\n\n        contents = []\n        for chunk in chunks:\n            if chunk.metadata.get(\"is_image\"):\n                try:\n                    # Handle data URI format \"data:image/png;base64,...\"\n                    content = chunk.content\n                    if content.startswith(\"data:\"):\n                        # Extract the base64 part after the comma\n                        content = content.split(\",\", 1)[1]\n\n                    # Now decode the base64 string\n                    image_bytes = base64.b64decode(content)\n                    image = open_image(io.BytesIO(image_bytes))\n                    contents.append(image)\n                except Exception as e:\n                    logger.error(f\"Error processing image: {str(e)}\")\n                    # Fall back to using the content as text\n                    contents.append(chunk.content)\n            else:\n                contents.append(chunk.content)\n\n        return [self.generate_embeddings(content) for content in contents]\n\n    async def embed_for_query(self, text: str) -> torch.Tensor:\n        return self.generate_embeddings(text)\n\n    def generate_embeddings(self, content: str | Image) -> np.ndarray:\n        if isinstance(content, Image):\n            processed = self.processor.process_images([content]).to(self.model.device)\n        else:\n            processed = self.processor.process_queries([content]).to(self.model.device)\n\n        with torch.no_grad():\n            embeddings: torch.Tensor = self.model(**processed)\n\n        return embeddings.to(torch.float32).numpy(force=True)[0]\n"}
{"type": "source_file", "path": "core/models/auth.py", "content": "from typing import Optional, Set\nfrom pydantic import BaseModel\nfrom enum import Enum\n\n\nclass EntityType(str, Enum):\n    USER = \"user\"\n    DEVELOPER = \"developer\"\n\n\nclass AuthContext(BaseModel):\n    \"\"\"JWT decoded context\"\"\"\n\n    entity_type: EntityType\n    entity_id: str  # uuid\n    app_id: Optional[str] = None  # uuid, only for developers\n    # TODO: remove permissions, not required here.\n    permissions: Set[str] = {\"read\"}\n"}
{"type": "source_file", "path": "core/parser/databridge_parser.py", "content": "from typing import Any, Dict, List, Optional, Tuple\nimport logging\nimport os\nimport tempfile\nimport io\nimport filetype\nimport anthropic\nfrom abc import ABC, abstractmethod\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom unstructured.partition.auto import partition\n\nfrom core.models.chunk import Chunk\nfrom core.parser.base_parser import BaseParser\nfrom core.parser.video.parse_video import VideoParser\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseChunker(ABC):\n    \"\"\"Base class for text chunking strategies\"\"\"\n\n    @abstractmethod\n    def split_text(self, text: str) -> List[Chunk]:\n        \"\"\"Split text into chunks\"\"\"\n        pass\n\n\nclass StandardChunker(BaseChunker):\n    \"\"\"Standard chunking using langchain's RecursiveCharacterTextSplitter\"\"\"\n\n    def __init__(self, chunk_size: int, chunk_overlap: int):\n        self.text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap,\n            length_function=len,\n            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n        )\n\n    def split_text(self, text: str) -> List[Chunk]:\n        chunks = self.text_splitter.split_text(text)\n        return [Chunk(content=chunk, metadata={}) for chunk in chunks]\n\n\nclass ContextualChunker(BaseChunker):\n    \"\"\"Contextual chunking using Claude to add context to each chunk\"\"\"\n\n    DOCUMENT_CONTEXT_PROMPT = \"\"\"\n    <document>\n    {doc_content}\n    </document>\n    \"\"\"\n\n    CHUNK_CONTEXT_PROMPT = \"\"\"\n    Here is the chunk we want to situate within the whole document\n    <chunk>\n    {chunk_content}\n    </chunk>\n\n    Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.\n    Answer only with the succinct context and nothing else.\n    \"\"\"\n\n    def __init__(self, chunk_size: int, chunk_overlap: int, anthropic_api_key: str):\n        self.standard_chunker = StandardChunker(chunk_size, chunk_overlap)\n        self._anthropic_client = None\n        self._anthropic_api_key = anthropic_api_key\n\n    @property\n    def anthropic_client(self):\n        if self._anthropic_client is None:\n            if not self._anthropic_api_key:\n                raise ValueError(\"Anthropic API key is required for contextual chunking\")\n            self._anthropic_client = anthropic.Anthropic(api_key=self._anthropic_api_key)\n        return self._anthropic_client\n\n    def _situate_context(self, doc: str, chunk: str) -> str:\n        response = self.anthropic_client.messages.create(\n            model=\"claude-3-haiku-20240307\",\n            max_tokens=1024,\n            temperature=0.0,\n            system=[\n                {\n                    \"type\": \"text\",\n                    \"text\": \"You are an AI assistant that situates a chunk within a document for the purposes of improving search retrieval of the chunk.\",\n                },\n                {\n                    \"type\": \"text\",\n                    \"text\": self.DOCUMENT_CONTEXT_PROMPT.format(doc_content=doc),\n                    \"cache_control\": {\"type\": \"ephemeral\"},\n                },\n            ],\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": self.CHUNK_CONTEXT_PROMPT.format(chunk_content=chunk),\n                        }\n                    ],\n                }\n            ],\n        )\n\n        context = response.content[0]\n        if context.type == \"text\":\n            return context.text\n        raise ValueError(f\"Unexpected response type from Anthropic: {context.type}\")\n\n    def split_text(self, text: str) -> List[Chunk]:\n        base_chunks = self.standard_chunker.split_text(text)\n        contextualized_chunks = []\n\n        for chunk in base_chunks:\n            context = self._situate_context(text, chunk.content)\n            content = f\"{context}; {chunk.content}\"\n            contextualized_chunks.append(Chunk(content=content, metadata=chunk.metadata))\n\n        return contextualized_chunks\n\n\nclass DatabridgeParser(BaseParser):\n    \"\"\"Unified parser that handles different file types and chunking strategies\"\"\"\n\n    def __init__(\n        self,\n        chunk_size: int = 1000,\n        chunk_overlap: int = 200,\n        use_unstructured_api: bool = False,\n        unstructured_api_key: Optional[str] = None,\n        assemblyai_api_key: Optional[str] = None,\n        anthropic_api_key: Optional[str] = None,\n        frame_sample_rate: int = 1,\n        use_contextual_chunking: bool = False,\n    ):\n        # Initialize basic configuration\n        self.use_unstructured_api = use_unstructured_api\n        self._unstructured_api_key = unstructured_api_key\n        self._assemblyai_api_key = assemblyai_api_key\n        self._anthropic_api_key = anthropic_api_key\n        self.frame_sample_rate = frame_sample_rate\n\n        # Initialize chunker based on configuration\n        if use_contextual_chunking:\n            self.chunker = ContextualChunker(chunk_size, chunk_overlap, anthropic_api_key)\n        else:\n            self.chunker = StandardChunker(chunk_size, chunk_overlap)\n\n    def _is_video_file(self, file: bytes, filename: str) -> bool:\n        \"\"\"Check if the file is a video file.\"\"\"\n        try:\n            kind = filetype.guess(file)\n            return kind is not None and kind.mime.startswith(\"video/\")\n        except Exception as e:\n            logging.error(f\"Error detecting file type: {str(e)}\")\n            return False\n\n    async def _parse_video(self, file: bytes) -> Tuple[Dict[str, Any], str]:\n        \"\"\"Parse video file to extract transcript and frame descriptions\"\"\"\n        if not self._assemblyai_api_key:\n            raise ValueError(\"AssemblyAI API key is required for video parsing\")\n\n        # Save video to temporary file\n        with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp4\") as temp_file:\n            temp_file.write(file)\n            video_path = temp_file.name\n\n        try:\n            # Process video\n            parser = VideoParser(\n                video_path,\n                assemblyai_api_key=self._assemblyai_api_key,\n                frame_sample_rate=self.frame_sample_rate,\n            )\n            results = await parser.process_video()\n\n            # Combine frame descriptions and transcript\n            frame_text = \"\\n\".join(results.frame_descriptions.time_to_content.values())\n            transcript_text = \"\\n\".join(results.transcript.time_to_content.values())\n            combined_text = f\"Frame Descriptions:\\n{frame_text}\\n\\nTranscript:\\n{transcript_text}\"\n\n            metadata = {\n                \"video_metadata\": results.metadata,\n                \"frame_timestamps\": list(results.frame_descriptions.time_to_content.keys()),\n                \"transcript_timestamps\": list(results.transcript.time_to_content.keys()),\n            }\n\n            return metadata, combined_text\n        finally:\n            os.unlink(video_path)\n\n    async def _parse_document(self, file: bytes, filename: str) -> Tuple[Dict[str, Any], str]:\n        \"\"\"Parse document using unstructured\"\"\"\n        elements = partition(\n            file=io.BytesIO(file),\n            content_type=None,\n            metadata_filename=filename,\n            strategy=\"hi_res\",\n            api_key=self._unstructured_api_key if self.use_unstructured_api else None,\n        )\n\n        text = \"\\n\\n\".join(str(element) for element in elements if str(element).strip())\n        return {}, text\n\n    async def parse_file_to_text(self, file: bytes, filename: str) -> Tuple[Dict[str, Any], str]:\n        \"\"\"Parse file content into text based on file type\"\"\"\n        if self._is_video_file(file, filename):\n            return await self._parse_video(file)\n        return await self._parse_document(file, filename)\n\n    async def split_text(self, text: str) -> List[Chunk]:\n        \"\"\"Split text into chunks using configured chunking strategy\"\"\"\n        return self.chunker.split_text(text)\n"}
{"type": "source_file", "path": "core/parser/__init__.py", "content": ""}
{"type": "source_file", "path": "core/database/postgres_database.py", "content": "import json\nfrom typing import List, Optional, Dict, Any\nfrom datetime import datetime, UTC\nimport logging\nfrom sqlalchemy.ext.asyncio import AsyncSession, create_async_engine\nfrom sqlalchemy.orm import sessionmaker, declarative_base\nfrom sqlalchemy import Column, String, Index, select, text\nfrom sqlalchemy.dialects.postgresql import JSONB\n\nfrom .base_database import BaseDatabase\nfrom ..models.documents import Document\nfrom ..models.auth import AuthContext\nfrom ..models.graph import Graph, Entity, Relationship\n\nlogger = logging.getLogger(__name__)\nBase = declarative_base()\n\n\nclass DocumentModel(Base):\n    \"\"\"SQLAlchemy model for document metadata.\"\"\"\n\n    __tablename__ = \"documents\"\n\n    external_id = Column(String, primary_key=True)\n    owner = Column(JSONB)\n    content_type = Column(String)\n    filename = Column(String, nullable=True)\n    doc_metadata = Column(JSONB, default=dict)\n    storage_info = Column(JSONB, default=dict)\n    system_metadata = Column(JSONB, default=dict)\n    additional_metadata = Column(JSONB, default=dict)\n    access_control = Column(JSONB, default=dict)\n    chunk_ids = Column(JSONB, default=list)\n    storage_files = Column(JSONB, default=list)\n\n    # Create indexes\n    __table_args__ = (\n        Index(\"idx_owner_id\", \"owner\", postgresql_using=\"gin\"),\n        Index(\"idx_access_control\", \"access_control\", postgresql_using=\"gin\"),\n        Index(\"idx_system_metadata\", \"system_metadata\", postgresql_using=\"gin\"),\n    )\n\n\nclass GraphModel(Base):\n    \"\"\"SQLAlchemy model for graph data.\"\"\"\n\n    __tablename__ = \"graphs\"\n\n    id = Column(String, primary_key=True)\n    name = Column(String, unique=True, index=True)\n    entities = Column(JSONB, default=list)\n    relationships = Column(JSONB, default=list)\n    graph_metadata = Column(JSONB, default=dict)  # Renamed from 'metadata' to avoid conflict\n    document_ids = Column(JSONB, default=list)\n    filters = Column(JSONB, nullable=True)\n    created_at = Column(String)  # ISO format string\n    updated_at = Column(String)  # ISO format string\n    owner = Column(JSONB)\n    access_control = Column(JSONB, default=dict)\n\n    # Create indexes\n    __table_args__ = (\n        Index(\"idx_graph_name\", \"name\"),\n        Index(\"idx_graph_owner\", \"owner\", postgresql_using=\"gin\"),\n        Index(\"idx_graph_access_control\", \"access_control\", postgresql_using=\"gin\"),\n    )\n\n\ndef _serialize_datetime(obj: Any) -> Any:\n    \"\"\"Helper function to serialize datetime objects to ISO format strings.\"\"\"\n    if isinstance(obj, datetime):\n        return obj.isoformat()\n    elif isinstance(obj, dict):\n        return {key: _serialize_datetime(value) for key, value in obj.items()}\n    elif isinstance(obj, list):\n        return [_serialize_datetime(item) for item in obj]\n    return obj\n\n\nclass PostgresDatabase(BaseDatabase):\n    \"\"\"PostgreSQL implementation for document metadata storage.\"\"\"\n\n    def __init__(\n        self,\n        uri: str,\n    ):\n        \"\"\"Initialize PostgreSQL connection for document storage.\"\"\"\n        self.engine = create_async_engine(uri)\n        self.async_session = sessionmaker(self.engine, class_=AsyncSession, expire_on_commit=False)\n        self._initialized = False\n\n    async def initialize(self):\n        \"\"\"Initialize database tables and indexes.\"\"\"\n        if self._initialized:\n            return True\n\n        try:\n            logger.info(\"Initializing PostgreSQL database tables and indexes...\")\n            # Create ORM models\n            async with self.engine.begin() as conn:\n                # Explicitly create all tables with checkfirst=True to avoid errors if tables already exist\n                await conn.run_sync(lambda conn: Base.metadata.create_all(conn, checkfirst=True))\n\n                # No need to manually create graphs table again since SQLAlchemy does it\n                logger.info(\"Created database tables successfully\")\n\n                # Create caches table if it doesn't exist (kept as direct SQL for backward compatibility)\n                await conn.execute(\n                    text(\n                        \"\"\"\n                    CREATE TABLE IF NOT EXISTS caches (\n                        name TEXT PRIMARY KEY,\n                        metadata JSONB NOT NULL,\n                        created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n                        updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n                    )\n                \"\"\"\n                    )\n                )\n\n                # Check if storage_files column exists\n                result = await conn.execute(\n                    text(\n                        \"\"\"\n                    SELECT column_name \n                    FROM information_schema.columns \n                    WHERE table_name = 'documents' AND column_name = 'storage_files'\n                    \"\"\"\n                    )\n                )\n                if not result.first():\n                    # Add storage_files column to documents table\n                    await conn.execute(\n                        text(\n                            \"\"\"\n                        ALTER TABLE documents \n                        ADD COLUMN IF NOT EXISTS storage_files JSONB DEFAULT '[]'::jsonb\n                        \"\"\"\n                        )\n                    )\n                    logger.info(\"Added storage_files column to documents table\")\n\n            logger.info(\"PostgreSQL tables and indexes created successfully\")\n            self._initialized = True\n            return True\n\n        except Exception as e:\n            logger.error(f\"Error creating PostgreSQL tables and indexes: {str(e)}\")\n            return False\n\n    async def store_document(self, document: Document) -> bool:\n        \"\"\"Store document metadata.\"\"\"\n        try:\n            doc_dict = document.model_dump()\n\n            # Rename metadata to doc_metadata\n            if \"metadata\" in doc_dict:\n                doc_dict[\"doc_metadata\"] = doc_dict.pop(\"metadata\")\n            doc_dict[\"doc_metadata\"][\"external_id\"] = doc_dict[\"external_id\"]\n\n            # Ensure system metadata\n            if \"system_metadata\" not in doc_dict:\n                doc_dict[\"system_metadata\"] = {}\n            doc_dict[\"system_metadata\"][\"created_at\"] = datetime.now(UTC)\n            doc_dict[\"system_metadata\"][\"updated_at\"] = datetime.now(UTC)\n\n            # Handle storage_files\n            if \"storage_files\" in doc_dict and doc_dict[\"storage_files\"]:\n                # Convert storage_files to the expected format for storage\n                doc_dict[\"storage_files\"] = [file.model_dump() for file in doc_dict[\"storage_files\"]]\n\n            # Serialize datetime objects to ISO format strings\n            doc_dict = _serialize_datetime(doc_dict)\n\n            async with self.async_session() as session:\n                doc_model = DocumentModel(**doc_dict)\n                session.add(doc_model)\n                await session.commit()\n            return True\n\n        except Exception as e:\n            logger.error(f\"Error storing document metadata: {str(e)}\")\n            return False\n\n    async def get_document(self, document_id: str, auth: AuthContext) -> Optional[Document]:\n        \"\"\"Retrieve document metadata by ID if user has access.\"\"\"\n        try:\n            async with self.async_session() as session:\n                # Build access filter\n                access_filter = self._build_access_filter(auth)\n\n                # Query document\n                query = (\n                    select(DocumentModel)\n                    .where(DocumentModel.external_id == document_id)\n                    .where(text(f\"({access_filter})\"))\n                )\n\n                result = await session.execute(query)\n                doc_model = result.scalar_one_or_none()\n\n                if doc_model:\n                    # Convert doc_metadata back to metadata\n                    doc_dict = {\n                        \"external_id\": doc_model.external_id,\n                        \"owner\": doc_model.owner,\n                        \"content_type\": doc_model.content_type,\n                        \"filename\": doc_model.filename,\n                        \"metadata\": doc_model.doc_metadata,\n                        \"storage_info\": doc_model.storage_info,\n                        \"system_metadata\": doc_model.system_metadata,\n                        \"additional_metadata\": doc_model.additional_metadata,\n                        \"access_control\": doc_model.access_control,\n                        \"chunk_ids\": doc_model.chunk_ids,\n                        \"storage_files\": doc_model.storage_files or [],\n                    }\n                    return Document(**doc_dict)\n                return None\n                \n        except Exception as e:\n            logger.error(f\"Error retrieving document metadata: {str(e)}\")\n            return None\n            \n    async def get_document_by_filename(self, filename: str, auth: AuthContext) -> Optional[Document]:\n        \"\"\"Retrieve document metadata by filename if user has access.\n        If multiple documents have the same filename, returns the most recently updated one.\n        \"\"\"\n        try:\n            async with self.async_session() as session:\n                # Build access filter\n                access_filter = self._build_access_filter(auth)\n\n                # Query document\n                query = (\n                    select(DocumentModel)\n                    .where(DocumentModel.filename == filename)\n                    .where(text(f\"({access_filter})\"))\n                    # Order by updated_at in system_metadata to get the most recent document\n                    .order_by(text(\"system_metadata->>'updated_at' DESC\"))\n                )\n\n                result = await session.execute(query)\n                doc_model = result.scalar_one_or_none()\n\n                if doc_model:\n                    # Convert doc_metadata back to metadata\n                    doc_dict = {\n                        \"external_id\": doc_model.external_id,\n                        \"owner\": doc_model.owner,\n                        \"content_type\": doc_model.content_type,\n                        \"filename\": doc_model.filename,\n                        \"metadata\": doc_model.doc_metadata,\n                        \"storage_info\": doc_model.storage_info,\n                        \"system_metadata\": doc_model.system_metadata,\n                        \"additional_metadata\": doc_model.additional_metadata,\n                        \"access_control\": doc_model.access_control,\n                        \"chunk_ids\": doc_model.chunk_ids,\n                        \"storage_files\": doc_model.storage_files or [],\n                    }\n                    return Document(**doc_dict)\n                return None\n                \n        except Exception as e:\n            logger.error(f\"Error retrieving document metadata by filename: {str(e)}\")\n            return None\n            \n    async def get_documents_by_id(self, document_ids: List[str], auth: AuthContext) -> List[Document]:\n        \"\"\"\n        Retrieve multiple documents by their IDs in a single batch operation.\n        Only returns documents the user has access to.\n        \n        Args:\n            document_ids: List of document IDs to retrieve\n            auth: Authentication context\n            \n        Returns:\n            List of Document objects that were found and user has access to\n        \"\"\"\n        try:\n            if not document_ids:\n                return []\n                \n            async with self.async_session() as session:\n                # Build access filter\n                access_filter = self._build_access_filter(auth)\n                \n                # Query documents with both document IDs and access check in a single query\n                query = (\n                    select(DocumentModel)\n                    .where(DocumentModel.external_id.in_(document_ids))\n                    .where(text(f\"({access_filter})\"))\n                )\n                \n                logger.info(f\"Batch retrieving {len(document_ids)} documents with a single query\")\n                \n                # Execute batch query\n                result = await session.execute(query)\n                doc_models = result.scalars().all()\n                \n                documents = []\n                for doc_model in doc_models:\n                    # Convert doc_metadata back to metadata\n                    doc_dict = {\n                        \"external_id\": doc_model.external_id,\n                        \"owner\": doc_model.owner,\n                        \"content_type\": doc_model.content_type,\n                        \"filename\": doc_model.filename,\n                        \"metadata\": doc_model.doc_metadata,\n                        \"storage_info\": doc_model.storage_info,\n                        \"system_metadata\": doc_model.system_metadata,\n                        \"additional_metadata\": doc_model.additional_metadata,\n                        \"access_control\": doc_model.access_control,\n                        \"chunk_ids\": doc_model.chunk_ids,\n                        \"storage_files\": doc_model.storage_files or [],\n                    }\n                    documents.append(Document(**doc_dict))\n                \n                logger.info(f\"Found {len(documents)} documents in batch retrieval\")\n                return documents\n                \n        except Exception as e:\n            logger.error(f\"Error batch retrieving documents: {str(e)}\")\n            return []\n\n    async def get_documents(\n        self,\n        auth: AuthContext,\n        skip: int = 0,\n        limit: int = 10000,\n        filters: Optional[Dict[str, Any]] = None,\n    ) -> List[Document]:\n        \"\"\"List documents the user has access to.\"\"\"\n        try:\n            async with self.async_session() as session:\n                # Build query\n                access_filter = self._build_access_filter(auth)\n                metadata_filter = self._build_metadata_filter(filters)\n\n                query = select(DocumentModel).where(text(f\"({access_filter})\"))\n                if metadata_filter:\n                    query = query.where(text(metadata_filter))\n\n                query = query.offset(skip).limit(limit)\n\n                result = await session.execute(query)\n                doc_models = result.scalars().all()\n\n                return [\n                    Document(\n                        external_id=doc.external_id,\n                        owner=doc.owner,\n                        content_type=doc.content_type,\n                        filename=doc.filename,\n                        metadata=doc.doc_metadata,\n                        storage_info=doc.storage_info,\n                        system_metadata=doc.system_metadata,\n                        additional_metadata=doc.additional_metadata,\n                        access_control=doc.access_control,\n                        chunk_ids=doc.chunk_ids,\n                        storage_files=doc.storage_files or [],\n                    )\n                    for doc in doc_models\n                ]\n\n        except Exception as e:\n            logger.error(f\"Error listing documents: {str(e)}\")\n            return []\n\n    async def update_document(\n        self, document_id: str, updates: Dict[str, Any], auth: AuthContext\n    ) -> bool:\n        \"\"\"Update document metadata if user has write access.\"\"\"\n        try:\n            if not await self.check_access(document_id, auth, \"write\"):\n                return False\n\n            # Update system metadata\n            updates.setdefault(\"system_metadata\", {})\n            updates[\"system_metadata\"][\"updated_at\"] = datetime.now(UTC)\n\n            # Serialize datetime objects to ISO format strings\n            updates = _serialize_datetime(updates)\n\n            async with self.async_session() as session:\n                result = await session.execute(\n                    select(DocumentModel).where(DocumentModel.external_id == document_id)\n                )\n                doc_model = result.scalar_one_or_none()\n\n                if doc_model:\n                    for key, value in updates.items():\n                        setattr(doc_model, key, value)\n                    await session.commit()\n                    return True\n                return False\n\n        except Exception as e:\n            logger.error(f\"Error updating document metadata: {str(e)}\")\n            return False\n\n    async def delete_document(self, document_id: str, auth: AuthContext) -> bool:\n        \"\"\"Delete document if user has admin access.\"\"\"\n        try:\n            if not await self.check_access(document_id, auth, \"admin\"):\n                return False\n\n            async with self.async_session() as session:\n                result = await session.execute(\n                    select(DocumentModel).where(DocumentModel.external_id == document_id)\n                )\n                doc_model = result.scalar_one_or_none()\n\n                if doc_model:\n                    await session.delete(doc_model)\n                    await session.commit()\n                    return True\n                return False\n\n        except Exception as e:\n            logger.error(f\"Error deleting document: {str(e)}\")\n            return False\n\n    async def find_authorized_and_filtered_documents(\n        self, auth: AuthContext, filters: Optional[Dict[str, Any]] = None\n    ) -> List[str]:\n        \"\"\"Find document IDs matching filters and access permissions.\"\"\"\n        try:\n            async with self.async_session() as session:\n                # Build query\n                access_filter = self._build_access_filter(auth)\n                metadata_filter = self._build_metadata_filter(filters)\n\n                logger.debug(f\"Access filter: {access_filter}\")\n                logger.debug(f\"Metadata filter: {metadata_filter}\")\n                logger.debug(f\"Original filters: {filters}\")\n\n                query = select(DocumentModel.external_id).where(text(f\"({access_filter})\"))\n                if metadata_filter:\n                    query = query.where(text(metadata_filter))\n\n                logger.debug(f\"Final query: {query}\")\n\n                result = await session.execute(query)\n                doc_ids = [row[0] for row in result.all()]\n                logger.debug(f\"Found document IDs: {doc_ids}\")\n                return doc_ids\n\n        except Exception as e:\n            logger.error(f\"Error finding authorized documents: {str(e)}\")\n            return []\n\n    async def check_access(\n        self, document_id: str, auth: AuthContext, required_permission: str = \"read\"\n    ) -> bool:\n        \"\"\"Check if user has required permission for document.\"\"\"\n        try:\n            async with self.async_session() as session:\n                result = await session.execute(\n                    select(DocumentModel).where(DocumentModel.external_id == document_id)\n                )\n                doc_model = result.scalar_one_or_none()\n\n                if not doc_model:\n                    return False\n\n                # Check owner access\n                owner = doc_model.owner\n                if owner.get(\"type\") == auth.entity_type and owner.get(\"id\") == auth.entity_id:\n                    return True\n\n                # Check permission-specific access\n                access_control = doc_model.access_control\n                permission_map = {\"read\": \"readers\", \"write\": \"writers\", \"admin\": \"admins\"}\n                permission_set = permission_map.get(required_permission)\n\n                if not permission_set:\n                    return False\n\n                return auth.entity_id in access_control.get(permission_set, [])\n\n        except Exception as e:\n            logger.error(f\"Error checking document access: {str(e)}\")\n            return False\n\n    def _build_access_filter(self, auth: AuthContext) -> str:\n        \"\"\"Build PostgreSQL filter for access control.\"\"\"\n        filters = [\n            f\"owner->>'id' = '{auth.entity_id}'\",\n            f\"access_control->'readers' ? '{auth.entity_id}'\",\n            f\"access_control->'writers' ? '{auth.entity_id}'\",\n            f\"access_control->'admins' ? '{auth.entity_id}'\",\n        ]\n\n        if auth.entity_type == \"DEVELOPER\" and auth.app_id:\n            # Add app-specific access for developers\n            filters.append(f\"access_control->'app_access' ? '{auth.app_id}'\")\n\n        return \" OR \".join(filters)\n\n    def _build_metadata_filter(self, filters: Dict[str, Any]) -> str:\n        \"\"\"Build PostgreSQL filter for metadata.\"\"\"\n        if not filters:\n            return \"\"\n\n        filter_conditions = []\n        for key, value in filters.items():\n            # Convert boolean values to string 'true' or 'false'\n            if isinstance(value, bool):\n                value = str(value).lower()\n                \n            # Use proper SQL escaping for string values\n            if isinstance(value, str):\n                # Replace single quotes with double single quotes to escape them\n                value = value.replace(\"'\", \"''\") \n                \n            filter_conditions.append(f\"doc_metadata->>'{key}' = '{value}'\")\n\n        return \" AND \".join(filter_conditions)\n\n    async def store_cache_metadata(self, name: str, metadata: Dict[str, Any]) -> bool:\n        \"\"\"Store metadata for a cache in PostgreSQL.\n\n        Args:\n            name: Name of the cache\n            metadata: Cache metadata including model info and storage location\n\n        Returns:\n            bool: Whether the operation was successful\n        \"\"\"\n        try:\n            async with self.async_session() as session:\n                await session.execute(\n                    text(\n                        \"\"\"\n                        INSERT INTO caches (name, metadata, updated_at)\n                        VALUES (:name, :metadata, CURRENT_TIMESTAMP)\n                        ON CONFLICT (name)\n                        DO UPDATE SET\n                            metadata = :metadata,\n                            updated_at = CURRENT_TIMESTAMP\n                        \"\"\"\n                    ),\n                    {\"name\": name, \"metadata\": json.dumps(metadata)},\n                )\n                await session.commit()\n                return True\n        except Exception as e:\n            logger.error(f\"Failed to store cache metadata: {e}\")\n            return False\n\n    async def get_cache_metadata(self, name: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Get metadata for a cache from PostgreSQL.\n\n        Args:\n            name: Name of the cache\n\n        Returns:\n            Optional[Dict[str, Any]]: Cache metadata if found, None otherwise\n        \"\"\"\n        try:\n            async with self.async_session() as session:\n                result = await session.execute(\n                    text(\"SELECT metadata FROM caches WHERE name = :name\"), {\"name\": name}\n                )\n                row = result.first()\n                return row[0] if row else None\n        except Exception as e:\n            logger.error(f\"Failed to get cache metadata: {e}\")\n            return None\n\n    async def store_graph(self, graph: Graph) -> bool:\n        \"\"\"Store a graph in PostgreSQL.\n\n        This method stores the graph metadata, entities, and relationships\n        in a PostgreSQL table.\n\n        Args:\n            graph: Graph to store\n\n        Returns:\n            bool: Whether the operation was successful\n        \"\"\"\n        # Ensure database is initialized\n        if not self._initialized:\n            await self.initialize()\n\n        try:\n            # First serialize the graph model to dict\n            graph_dict = graph.model_dump()\n\n            # Change 'metadata' to 'graph_metadata' to match our model\n            if \"metadata\" in graph_dict:\n                graph_dict[\"graph_metadata\"] = graph_dict.pop(\"metadata\")\n\n            # Serialize datetime objects to ISO format strings\n            graph_dict = _serialize_datetime(graph_dict)\n\n            # Store the graph metadata in PostgreSQL\n            async with self.async_session() as session:\n                # Store graph metadata in our table\n                graph_model = GraphModel(**graph_dict)\n                session.add(graph_model)\n                await session.commit()\n                logger.info(f\"Stored graph '{graph.name}' with {len(graph.entities)} entities and {len(graph.relationships)} relationships\")\n\n            return True\n\n        except Exception as e:\n            logger.error(f\"Error storing graph: {str(e)}\")\n            return False\n\n    async def get_graph(self, name: str, auth: AuthContext) -> Optional[Graph]:\n        \"\"\"Get a graph by name.\n\n        Args:\n            name: Name of the graph\n            auth: Authentication context\n\n        Returns:\n            Optional[Graph]: Graph if found and accessible, None otherwise\n        \"\"\"\n        # Ensure database is initialized\n        if not self._initialized:\n            await self.initialize()\n\n        try:\n            async with self.async_session() as session:\n                # Build access filter\n                access_filter = self._build_access_filter(auth)\n\n                # Query graph\n                query = (\n                    select(GraphModel)\n                    .where(GraphModel.name == name)\n                    .where(text(f\"({access_filter})\"))\n                )\n\n                result = await session.execute(query)\n                graph_model = result.scalar_one_or_none()\n\n                if graph_model:\n                    # Convert to Graph model\n                    graph_dict = {\n                        \"id\": graph_model.id,\n                        \"name\": graph_model.name,\n                        \"entities\": graph_model.entities,\n                        \"relationships\": graph_model.relationships,\n                        \"metadata\": graph_model.graph_metadata,  # Reference the renamed column\n                        \"document_ids\": graph_model.document_ids,\n                        \"filters\": graph_model.filters,\n                        \"created_at\": graph_model.created_at,\n                        \"updated_at\": graph_model.updated_at,\n                        \"owner\": graph_model.owner,\n                        \"access_control\": graph_model.access_control,\n                    }\n                    return Graph(**graph_dict)\n\n                return None\n\n        except Exception as e:\n            logger.error(f\"Error retrieving graph: {str(e)}\")\n            return None\n\n    async def list_graphs(self, auth: AuthContext) -> List[Graph]:\n        \"\"\"List all graphs the user has access to.\n\n        Args:\n            auth: Authentication context\n\n        Returns:\n            List[Graph]: List of graphs\n        \"\"\"\n        # Ensure database is initialized\n        if not self._initialized:\n            await self.initialize()\n\n        try:\n            async with self.async_session() as session:\n                # Build access filter\n                access_filter = self._build_access_filter(auth)\n\n                # Query graphs\n                query = select(GraphModel).where(text(f\"({access_filter})\"))\n\n                result = await session.execute(query)\n                graph_models = result.scalars().all()\n\n                return [\n                    Graph(\n                        id=graph.id,\n                        name=graph.name,\n                        entities=graph.entities,\n                        relationships=graph.relationships,\n                        metadata=graph.graph_metadata,  # Reference the renamed column\n                        document_ids=graph.document_ids,\n                        filters=graph.filters,\n                        created_at=graph.created_at,\n                        updated_at=graph.updated_at,\n                        owner=graph.owner,\n                        access_control=graph.access_control,\n                    )\n                    for graph in graph_models\n                ]\n\n        except Exception as e:\n            logger.error(f\"Error listing graphs: {str(e)}\")\n            return []\n"}
{"type": "source_file", "path": "core/parser/video/parse_video.py", "content": "import cv2\nimport base64\nfrom openai import OpenAI\nimport assemblyai as aai\nimport logging\nfrom core.models.video import TimeSeriesData, ParseVideoResult\nimport tomli\nimport os\nfrom typing import Optional, Dict, Any\nfrom ollama import AsyncClient\n\nlogger = logging.getLogger(__name__)\n\n\ndef debug_object(title, obj):\n    logger.debug(\"\\n\".join([\"-\" * 100, title, \"-\" * 100, f\"{obj}\", \"-\" * 100]))\n\n\ndef load_config() -> Dict[str, Any]:\n    config_path = os.path.join(os.path.dirname(__file__), \"../../../databridge.toml\")\n    with open(config_path, \"rb\") as f:\n        return tomli.load(f)\n\n\nclass VisionModelClient:\n    def __init__(self, config: Dict[str, Any]):\n        self.config = config[\"parser\"][\"vision\"]\n        self.provider = self.config.get(\"provider\", \"ollama\")\n        self.model_name = self.config.get(\"model_name\", \"llama3.2-vision\")\n\n        if self.provider == \"openai\":\n            self.client = OpenAI()\n        elif self.provider == \"ollama\":\n            base_url = self.config.get(\"base_url\", \"http://localhost:11434\")\n            self.client = AsyncClient(host=base_url)\n        else:\n            raise ValueError(f\"Unsupported vision model provider: {self.provider}\")\n\n    async def get_frame_description(self, image_base64: str, context: str) -> str:\n        if self.provider == \"openai\":\n            response = self.client.chat.completions.create(\n                model=self.model_name,\n                messages=[\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\"type\": \"text\", \"text\": context},\n                            {\n                                \"type\": \"image_url\",\n                                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"},\n                            },\n                        ],\n                    }\n                ],\n                max_tokens=300,\n            )\n            return response.choices[0].message.content\n        else:  # ollama\n            response = await self.client.chat(\n                model=self.model_name,\n                messages=[{\"role\": \"user\", \"content\": context, \"images\": [image_base64]}],\n            )\n            return response[\"message\"][\"content\"]\n\n\nclass VideoParser:\n    def __init__(\n        self, video_path: str, assemblyai_api_key: str, frame_sample_rate: Optional[int] = None\n    ):\n        \"\"\"\n        Initialize the video parser\n\n        Args:\n            video_path: Path to the video file\n            assemblyai_api_key: API key for AssemblyAI\n            frame_sample_rate: Sample every nth frame for description (optional, defaults to config value)\n        \"\"\"\n        logger.info(f\"Initializing VideoParser for {video_path}\")\n        self.config = load_config()\n        self.video_path = video_path\n        self.frame_sample_rate = frame_sample_rate or self.config[\"parser\"][\"vision\"].get(\n            \"frame_sample_rate\", 120\n        )\n        self.cap = cv2.VideoCapture(video_path)\n\n        if not self.cap.isOpened():\n            logger.error(f\"Failed to open video file: {video_path}\")\n            raise ValueError(f\"Could not open video file: {video_path}\")\n\n        self.fps = self.cap.get(cv2.CAP_PROP_FPS)\n        self.total_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        self.duration = self.total_frames / self.fps\n\n        # Initialize AssemblyAI\n        aai.settings.api_key = assemblyai_api_key\n        aai_config = aai.TranscriptionConfig(speaker_labels=True)\n        self.transcriber = aai.Transcriber(config=aai_config)\n        self.transcript = TimeSeriesData(time_to_content={})\n\n        # Initialize vision model client\n        self.vision_client = VisionModelClient(self.config)\n\n        logger.info(f\"Video loaded: {self.duration:.2f}s duration, {self.fps:.2f} FPS\")\n\n    def frame_to_base64(self, frame) -> str:\n        \"\"\"Convert a frame to base64 string\"\"\"\n        success, buffer = cv2.imencode(\".jpg\", frame)\n        if not success:\n            logger.error(\"Failed to encode frame to JPEG\")\n            raise ValueError(\"Failed to encode frame\")\n        return base64.b64encode(buffer).decode(\"utf-8\")\n\n    def get_transcript_object(self) -> aai.Transcript:\n        \"\"\"\n        Get the transcript object from AssemblyAI\n        \"\"\"\n        logger.info(\"Starting video transcription\")\n        transcript = self.transcriber.transcribe(self.video_path)\n        if transcript.status == \"error\":\n            logger.error(f\"Transcription failed: {transcript.error}\")\n            raise ValueError(f\"Transcription failed: {transcript.error}\")\n        if not transcript.words:\n            logger.warning(\"No words found in transcript\")\n        logger.info(\"Transcription completed successfully!\")\n\n        return transcript\n\n    def get_transcript(self) -> TimeSeriesData:\n        \"\"\"\n        Get timestamped transcript of the video using AssemblyAI\n\n        Returns:\n            TimeSeriesData object containing transcript\n        \"\"\"\n        logger.info(\"Starting video transcription\")\n        transcript = self.get_transcript_object()\n        # divide by 1000 because assemblyai timestamps are in milliseconds\n        time_to_text = (\n            {u.start / 1000: u.text for u in transcript.utterances} if transcript.utterances else {}\n        )\n        debug_object(\"Time to text\", time_to_text)\n        self.transcript = TimeSeriesData(time_to_content=time_to_text)\n        return self.transcript\n\n    async def get_frame_descriptions(self) -> TimeSeriesData:\n        \"\"\"\n        Get descriptions for sampled frames using configured vision model\n\n        Returns:\n            TimeSeriesData object containing frame descriptions\n        \"\"\"\n        logger.info(\"Starting frame description generation\")\n\n        # Return empty TimeSeriesData if frame_sample_rate is -1 (captioning disabled)\n        if self.frame_sample_rate == -1:\n            logger.info(\"Frame captioning is disabled (frame_sample_rate = -1)\")\n            return TimeSeriesData(time_to_content={})\n\n        frame_count = 0\n        time_to_description = {}\n        last_description = None\n        logger.info(\"Starting main loop for frame description generation\")\n        while True:\n            logger.info(f\"Frame count: {frame_count}\")\n            ret, frame = self.cap.read()\n            if not ret:\n                logger.info(\"Reached end of video\")\n                break\n\n            if frame_count % self.frame_sample_rate == 0:\n                logger.info(f\"Processing frame at {frame_count / self.fps:.2f}s\")\n                timestamp = frame_count / self.fps\n                logger.debug(f\"Processing frame at {timestamp:.2f}s\")\n\n                img_base64 = self.frame_to_base64(frame)\n\n                context = f\"\"\"Describe this frame from a video. Focus on the main elements, actions, and any notable details. Here is the transcript around the time of the frame:\n                ---\n                {self.transcript.at_time(timestamp, padding=10)}\n                ---\n\n                Here is a description of the previous frame:\n                ---\n                {last_description if last_description else 'No previous frame description available, this is the first frame'}\n                ---\n\n                In your response, only provide the description of the current frame, using the above information as context.\n                \"\"\"\n\n                last_description = await self.vision_client.get_frame_description(\n                    img_base64, context\n                )\n                time_to_description[timestamp] = last_description\n\n            frame_count += 1\n\n        logger.info(f\"Generated descriptions for {len(time_to_description)} frames\")\n        return TimeSeriesData(time_to_content=time_to_description)\n\n    async def process_video(self) -> ParseVideoResult:\n        \"\"\"\n        Process the video to get both transcript and frame descriptions\n\n        Returns:\n            Dictionary containing transcript and frame descriptions as TimeSeriesData objects\n        \"\"\"\n        logger.info(\"Starting full video processing\")\n        metadata = {\n            \"duration\": self.duration,\n            \"fps\": self.fps,\n            \"total_frames\": self.total_frames,\n            \"frame_sample_rate\": self.frame_sample_rate,\n        }\n        result = ParseVideoResult(\n            metadata=metadata,\n            transcript=self.get_transcript(),\n            frame_descriptions=await self.get_frame_descriptions(),\n        )\n        logger.info(\"Video processing completed successfully\")\n        return result\n\n    def __del__(self):\n        \"\"\"Clean up video capture object\"\"\"\n        if hasattr(self, \"cap\"):\n            logger.debug(\"Releasing video capture resources\")\n            self.cap.release()\n"}
