{"repo_info": {"repo_name": "strategic-debate-tot", "repo_owner": "zbambergerNLP", "repo_url": "https://github.com/zbambergerNLP/strategic-debate-tot"}}
{"type": "source_file", "path": "abstractions/evaluator/iterative_drafting_evaluator.py", "content": "import typing\nimport dspy\nfrom abstractions.evaluator.evaluator import (\n    TOPIC_DESC,\n    ARGUMENT_DESC,\n    VOTE_CANDIDATES_DESC,\n    SCORE_DESC,\n    VOTE_INDEX_DESC,\n    CONVERSATION_DESC,\n)\n\n\nPREVIOUS_DRAFTS_DESC = \"\"\"\nA list of argumentative drafts that the debator is aiming to improving upon in the new draft.\nEach draft is an attempted improvement in quality and persuasiveness upon previous drafts.\n\"\"\".strip()\n\n\nclass SingleTurnScoreWithDraftsSignature(dspy.Signature):\n    \"\"\"\n    Evaluate the quality and improvement of a new persuasive argument draft with the provided stance towards a given topic.\n\n    As an objective and fair judge, assess the new draft based on its clarity, logical structure, comprehensiveness, and \n    strategic alignment with the goal of persuading an audience member with reasonable general knowledge. \n    Additionally, determine whether the new draft demonstrates a clear improvement over the previous drafts in terms of quality \n    and persuasiveness.\n\n    A strong draft presents clear, actionable enhancements that significantly improve the argument's persuasiveness and \n    coherence compared to earlier versions. A weak draft is vague, disorganized, lacks essential components, includes redundant \n    elements, or fails to improve upon previous drafts.\n\n    Provide a floating-point score between 0 and 1, where 1 indicates a strong and improved draft and 0 indicates a weak \n    or regressive draft.\n    \"\"\"\n    topic: str = dspy.InputField(desc=TOPIC_DESC)\n    previous_drafts: str = dspy.InputField(desc=PREVIOUS_DRAFTS_DESC)\n    argument: str = dspy.InputField(desc=ARGUMENT_DESC)\n    score: float = dspy.OutputField(desc=SCORE_DESC)\n\nclass MultiTurnScoreWithDraftsSignature(dspy.Signature):\n    \"\"\"\n    Evaluate the quality and improvement of a new persuasive argument draft within the context of a multi-turn debate.\n\n    As an objective and fair judge, assess the new draft based on its clarity, logical structure, comprehensiveness, and \n    strategic alignment with the goal of persuading an audience member over multiple turns of debate. Additionally, \n    determine whether the new draft demonstrates progressive enhancements over previous drafts to increase persuasiveness.\n\n    A strong draft accounts for the dynamic nature of debates, including addressing previous arguments and anticipating future \n    counter-arguments, while showing continuous improvement in persuasiveness and coherence over previously attempted drafts. \n    A weak draft is rigid, fails to address potential opposition, lacks essential components, includes redundant elements, or \n    does not show improvement over previous iterations.\n    \n    Provide a floating-point score between 0 and 1, where 1 indicates a strong and improved draft and 0 indicates a weak \n    or regressive draft.\n    \"\"\"\n    topic: str = dspy.InputField(desc=TOPIC_DESC)\n    conversation: typing.List[str] = dspy.InputField(desc=CONVERSATION_DESC)\n    previous_drafts: str = dspy.InputField(desc=PREVIOUS_DRAFTS_DESC)\n    argument: str = dspy.InputField(desc=ARGUMENT_DESC)\n    score: float = dspy.OutputField(desc=SCORE_DESC)\n\nclass SingleTurnVoteWithDraftsSignature(dspy.Signature):\n    \"\"\"\n    Vote for the most persuasive argument among multiple candidates, considering their iterative improvements over previous drafts.\n\n    Each candidate argument takes the provided stance towards the topic and builds upon previous argumentative drafts to enhance \n    quality and persuasiveness.\n\n    A strong argument presents clear, actionable enhancements that significantly improve the argument's persuasiveness and \n    coherence compared to earlier versions. \n    A weak argument is vague, disorganized, lacks essential components, includes redundant elements, or fails to improve upon \n    previous drafts.\n\n    Your vote is represented as an integer index, specifying the most persuasive argument among the provided list of arguments.\n    \"\"\"\n    topic: str = dspy.InputField(desc=TOPIC_DESC)\n    arguments: typing.Dict[int, str] = dspy.InputField(desc=VOTE_CANDIDATES_DESC)\n    previous_drafts: typing.Dict[int, str] = dspy.InputField(desc=PREVIOUS_DRAFTS_DESC)\n    index: int = dspy.OutputField(desc=VOTE_INDEX_DESC)\n\nclass MultiTurnVoteWithDraftsSignature(dspy.Signature):\n    \"\"\"\n    Vote for the most persuasive argument among multiple candidates within the context of a multi-turn debate.\n\n    Each candidate argument takes the provided stance towards the topic and builds upon previous argumentative drafts to enhance \n    quality and persuasiveness.\n\n    A argument draft accounts for the dynamic nature of debates, including addressing previous arguments and anticipating future \n    counter-arguments, while showing continuous improvement in persuasiveness and coherence over previously attempted drafts. \n    A weak argument is rigid, fails to address potential opposition, lacks essential components, includes redundant elements, or \n    does not show improvement over previous iterations.\n\n    Your vote is represented as an integer index, specifying the most persuasive argument among the provided list of arguments.\n    \"\"\"\n    topic: str = dspy.InputField(desc=TOPIC_DESC)\n    arguments: typing.Dict[int, str] = dspy.InputField(desc=VOTE_CANDIDATES_DESC)\n    conversation: typing.List[str] = dspy.InputField(desc=CONVERSATION_DESC)\n    previous_drafts: typing.Dict[int, str] = dspy.InputField(desc=PREVIOUS_DRAFTS_DESC)\n    index: int = dspy.OutputField(desc=VOTE_INDEX_DESC)\n"}
{"type": "source_file", "path": "abstractions/generator/generator.py", "content": "import typing\nimport dspy\nfrom pydantic import BaseModel, Field\n\n#######################\n# Argument Generation #\n####################### \n\n\n# Inputs\nTOPIC_DESC = \"The topic of the argument.\"\nSTANCE_DESC = \"The stance to take when writing an argument about the topic (either PRO or ANTI).\"\nCONVERSATION_DESC = \"A conversation between two rival debators, each holding an opposing stance (PRO vs. ANTI) about the topic.\"\nRESPONSE_LENGTH_DESC = \"The length of the argument to generate (e.g., 'one sentence', 'a few sentences', 'one paragraph', etc...).\"\nLANGUAGE_STYLE_DESC = \"The degree of formality for the argument. One of 'slang', 'casual', or 'formal'.\"\nCOMMUNICATION_TONE_DESC = \"The tone of the argument to generate. One of 'logical', 'sarcastic', or 'aggressive'.\"\nRESPONSE_PARAMETERS_DESC = \"Parameters for generating a persuasive argument.\"\n\n# Outputs\nSINGLE_TURN_RESPONSE_DESC = \"\"\"\nA persuasive argument that takes the specified stance towards the specified topic while adhering to the specified response length.\n\"\"\".strip()\nMULTI_TURN_RESPONSE_DESC = \"\"\"\nA persuasive argument that takes the specified stance towards the specified topic while adhering to the specified response length, \nand effectively addresses the main (and/or anticipated) claims of the opponent in the conversation.\n\"\"\".strip()\n\nclass ResponseParameters(BaseModel):\n    response_length: str = Field(desc=RESPONSE_LENGTH_DESC, default=\"a few sentences\")\n    communication_tone: str = Field(desc=COMMUNICATION_TONE_DESC, default=\"logical\")\n    language_style: str = Field(desc=LANGUAGE_STYLE_DESC, default=\"casual\")\n    \nclass SingleTurnResponseBranchingSignature(dspy.Signature):\n    \"\"\"\n    Generate a persuasive argument according to the provided parameters.\n    The argument should take the provided stance towards the given topic.\n    Utilize rhetorical strategies such as ethos, logos, and pathos to enhance persuasiveness. \n    Support claims with numerical, factual, and easily verifiable evidence. \n    Clarify complex concepts without relying on jargon or repetitive statements. \n    While using humor, metaphors, or other rhetorical methods can be beneficial, ensure they are executed appropriately to avoid \n    undermining the argument. \n    Adhere to the specified response length, language style, and communication tone when generating the argument.\n    \"\"\"\n    topic: str = dspy.InputField(desc=TOPIC_DESC)\n    stance: str = dspy.InputField(desc=STANCE_DESC)\n    response_parameters: ResponseParameters = dspy.InputField(desc=RESPONSE_PARAMETERS_DESC)\n    response: str = dspy.OutputField(desc=SINGLE_TURN_RESPONSE_DESC)\n\n\nclass MultiTurnResponseBranchingSignature(dspy.Signature):\n    \"\"\"\n    Generate a persuasive argument within an ongoing multi-turn debate according to the provided parameters.\n    Your argument should take the provided stance towards the given topic, and effectively address or anticipate \n    counter-arguments from an opponent with an opposing stance.\n    Utilize rhetorical strategies such as ethos, logos, and pathos to enhance persuasiveness. \n    Support claims with numerical, factual, and easily verifiable evidence. \n    Clarify complex concepts without relying on jargon or repetitive statements. \n    While using humor, metaphors, or other rhetorical methods can be beneficial, ensure they are executed appropriately to avoid \n    undermining the argument. \n    Additionally, maintain coherence with previous turns and strategically counter opposing points to enhance persuasiveness. \n    Adhere to the specified response length, language style, and communication tone when generating the argument.\n    \"\"\"\n    topic: str = dspy.InputField(desc=TOPIC_DESC)\n    stance: str = dspy.InputField(desc=STANCE_DESC)\n    conversation: typing.List[str] = dspy.InputField(desc=CONVERSATION_DESC)\n    response_parameters: ResponseParameters = dspy.InputField(desc=RESPONSE_PARAMETERS_DESC)\n    response: str = dspy.OutputField(response=MULTI_TURN_RESPONSE_DESC)\n"}
{"type": "source_file", "path": "abstractions/evaluator/evaluator.py", "content": "import typing\nimport dspy\n\n\n##################################\n# DSPy Signatures for LLM Judges #\n##################################\n\n# Inputs\nTOPIC_DESC = \"The topic of the argument.\"\nCONVERSATION_DESC = \"A conversation between two rival debators, each holding an opposing stance (PRO vs. ANTI) about the topic.\"\nARGUMENT_DESC = \"The persuasive argument that the debator is making about the provided topic.\"\nVOTE_CANDIDATES_DESC = \"A dictionary mapping indices to candidate arguments about the provided topic.\"\n\n# Outputs\nSCORE_DESC = \"The floating-point score (between 0 and 1) which corresponds with the quality of the argument.\"\nVOTE_INDEX_DESC = \"The index of the most persuasive argument among the provided list of arguments.\"\n\nclass SingleTurnScoreSingature(dspy.Signature):\n    \"\"\"\n    Assess the effectiveness and persuasiveness of an argument with the provided stance towards a given topic.\n    \n    As an impartial and objective judge, you must rigorously evaluate the argument's clarity, logical coherence, use of \n    evidence, and rhetorical strategies. Assign a floating-point score between 0 and 1, where 1 signifies an exceptionally \n    well-crafted and highly persuasive argument, and 0 denotes a poorly constructed and unconvincing one. \n    \n    Ensure your evaluation accounts for potential edge cases, such as ambiguous language, lack of supporting evidence, \n    logical fallacies, or biased reasoning, and maintain consistency and fairness in your scoring process.\n    \"\"\"\n    topic: str = dspy.InputField(desc=TOPIC_DESC)\n    argument: str = dspy.InputField(desc=ARGUMENT_DESC)\n    score: float = dspy.OutputField(desc=SCORE_DESC)\n\n\nclass MultiTurnScoreSignature(dspy.Signature):\n    \"\"\"\n    Evaluate the quality and persuasiveness of an argument within the context of a multi-turn debate on a given topic.\n\n    As an objective and impartial judge, evaluate the argument's ability to persuade an audience member with reasonable\n    general knowledge. Consider the argument's clarity, logical coherence, use of evidence, and rhetorical strategies.\n    Additionally, assess how well the argument addresses and counters the opponent's previous points, anticipates potential \n    rebuttals, and maintains relevance throughout the conversation. \n    \n    Assign a floating-point score between 0 and 1, where 1 indicates an exceptionally strong argument that convincingly \n    persuades the audience and effectively engages with the debate, while 0 signifies a weak, unconvincing argument that \n    fails to address the opponent's stance. \n    \n    Ensure your evaluation accounts for edge cases such as fragmented dialogue, irrelevant or repetitive points, logical \n    fallacies, biased reasoning, and insufficient evidence, maintaining consistency and fairness in your scoring process.\n    \"\"\"\n    topic: str = dspy.InputField(desc=TOPIC_DESC)\n    conversation: typing.List[str] = dspy.InputField(desc=CONVERSATION_DESC)\n    argument: str = dspy.InputField(desc=ARGUMENT_DESC)\n    score: float = dspy.OutputField(desc=SCORE_DESC)\n\n\nclass SingleTurnVoteSignature(dspy.Signature):\n    \"\"\"\n    Select the most persuasive argument from a set of candidate arguments with the same stance towards a given topic.\n\n    As an objective and impartial judge, evaluate each candidate argument based on clarity, logical coherence, use of evidence, \n    and rhetorical effectiveness. You should favor the argument that is most likely to persuade an audience member with reasonable \n    general knowledge. Consider factors such as the strength of the argument's claims, the relevance and credibility of \n    supporting evidence, and the overall persuasiveness of the rhetoric. \n    \n    Assign your vote by selecting the integer index corresponding to the most compelling argument from the provided dictionary\n    of candidates.\n    \n    Ensure your selection process accounts for potential edge cases, including closely matched arguments, redundant or \n    repetitive points, logical fallacies, biased reasoning, and insufficient evidence, while maintaining consistency and \n    fairness in your voting criteria.\n    \"\"\"\n    topic: str = dspy.InputField(desc=TOPIC_DESC)\n    arguments: typing.Dict[int, str] = dspy.InputField(desc=ARGUMENT_DESC)\n    index: int = dspy.OutputField(desc=VOTE_INDEX_DESC)\n\n\nclass MultiTurnVoteSignature(dspy.Signature):\n    \"\"\"\n    Select the most persuasive argument from a set of candidate arguments within a multi-turn debate context.\n    \n    As an objective and impartial judge, evaluate each candidate argument based on its clarity, logical coherence, use of \n    evidence, and rhetorical effectiveness. Additionally, consider how well each argument engages with the ongoing conversation \n    by addressing and countering the opponent's previous points, anticipating potential rebuttals, and maintaining relevance to \n    the topic. \n    \n    Your goal is to determine which argument is most likely to persuade an audience member with general knowledge, taking into \n    account the strength of the claims, the credibility of supporting evidence, and the effectiveness of counter-arguments. \n    \n    Assign your vote by selecting the integer index corresponding to the most compelling argument from the \n    provided dictionary of candidates. \n    \n    Ensure your selection process accounts for potential edge cases, such as closely matched arguments, redundant or \n    repetitive points, logical fallacies, biased reasoning, and insufficient evidence, while maintaining consistency and \n    fairness in your voting criteria.\n    \"\"\"\n    topic: str = dspy.InputField(desc=TOPIC_DESC)\n    arguments: typing.Dict[int, str] = dspy.InputField(desc=ARGUMENT_DESC)\n    conversation: typing.List[str] = dspy.InputField(desc=CONVERSATION_DESC)\n    index: int = dspy.OutputField(desc=VOTE_INDEX_DESC)\n"}
{"type": "source_file", "path": "abstractions/evaluator/plan_and_execute_evaluator.py", "content": "import dspy\nimport typing\n\nfrom abstractions.evaluator.evaluator import (\n    TOPIC_DESC,\n    CONVERSATION_DESC,\n)\n\n# Inputs\nPLAN_DESC = \"A step-by-step plan for how to persuade the audience to take the debator's stance on the topic.\"\nEXECUTION_DESC = \"The execution of the plan so far.\"\nPLAN_STEP_DESC = \"A single step in the plan for how to persuade the audience to take the debator's stance on the topic.\"\nCLAIM_TO_MAKE_DESC = \"The claim that the debator should make as part of the current step of the plan.\"\n\n# Outputs\nPLAN_SCORE_DESC = \"The floating-point score (between 0 and 1) which corresponds with the quality of the plan.\"\nEXECUTION_SCORE_DESC = \"The floating-point score (between 0 and 1) which corresponds with the quality of the execution of the plan.\" \n\n##################\n### Signatures ###\n##################\n\nclass SingleTurnScoreWithPlanSignature(dspy.Signature):\n    \"\"\"\n    Evaluate the quality and effectiveness of a step-by-step plan for constructing a persuasive argument on a given topic.\n\n    As an objective and fair judge, assess the plan based on its clarity, logical structure, comprehensiveness, and strategic \n    alignment with the goal of persuading an audience member. \n\n    A strong plan should outline clear, actionable steps that build upon each other to form a cohesive strategy for argumentation.\n    A weak plan is vague, disorganized, lacks essential components, or includes redundant steps. \n    Assess whether each step is purposeful and contributes to the overall objective of persuading the audience.\n\n    Provide a floating-point score between 0 and 1, where 1 indicates a strong plan and 0 indicates a weak plan.\n    \"\"\"\n    topic: str = dspy.InputField(desc=TOPIC_DESC)\n    plan: typing.List[str] = dspy.InputField(desc=PLAN_DESC)\n    score: float = dspy.OutputField(desc=PLAN_SCORE_DESC)\n\nclass SingleTurnScoreWithPlanExecutionSignature(dspy.Signature):\n    \"\"\"\n    Evaluate the quality and persuasiveness of an argumentative claim within the context of a predefined plan.\n\n    As an objective and fair judge, assess the execution step based on its clarity, relevance, logical coherence, and alignment \n    with the current step of the plan. \n\n    A strong execution step should effectively advance the plan by making a clear and relevant claim that supports the overall \n    argument. \n    A weak execution step makes a claim that is unclear, irrelevant, lacks logical structure, or deviates from the planned strategy.\n    Assess whether the new claim integrates smoothly with previous claims and contributes meaningfully to persuading the audience.\n\n    Provide a floating-point score between 0 and 1, where 1 indicates a strong execution and 0 indicates a weak execution.\n    \"\"\"\n    topic: str = dspy.InputField(desc=TOPIC_DESC)\n    claim_plan: str = dspy.InputField(desc=PLAN_STEP_DESC)\n    new_claim: str = dspy.InputField(desc=CLAIM_TO_MAKE_DESC)\n    claims_so_far: typing.List[str] = dspy.InputField(desc=EXECUTION_DESC)\n    score: float = dspy.OutputField(desc=EXECUTION_SCORE_DESC)\n\n\nclass MultiTurnScoreWithPlanSignature(dspy.Signature):\n    \"\"\"\n    Evaluate the quality of a step-by-step plan for constructing a persuasive argument within a multi-turn debate.\n    \n    As an objective and fair judge, assess the plan based on its clarity, logical structure, comprehensiveness, and strategic \n    alignment with the goal of persuading an audience member over multiple turns of debate.\n    \n    A strong plan should account for the dynamic nature of debates, including anticipating counter-arguments and adapting \n    strategies as the conversation progresses. \n    A weak plan is rigid, fails to address potential opposition, lacks essential components, or includes redundant steps. \n    Assess whether each step is purposeful and contributes to the overall objective of persuading the audience.\n\n    Provide a floating-point score between 0 and 1, where 1 indicates a strong plan and 0 indicates a weak plan.\n    \"\"\"\n    topic: str = dspy.InputField(desc=TOPIC_DESC)\n    plan: typing.List[str] = dspy.InputField(desc=PLAN_DESC)\n    conversation: typing.List[str] = dspy.InputField(desc=CONVERSATION_DESC)\n    score: float = dspy.OutputField(desc=PLAN_SCORE_DESC)\n\n\nclass MultiTurnScoreWithPlanExecutionSignature(dspy.Signature):\n    \"\"\"\n    Assess the quality and persuasiveness of an argumentative claim within the context of a predefined plan in a multi-turn debate.\n\n    As an objective and fair judge, evaluate the claim's clarity, relevance, logical coherence, and alignment with the \n    current step of the plan. This new claim should ideally build on the steps of the plan executed so far, as well as the \n    ongoing conversation. \n    \n    A strong execution step in a debate context should consider previous arguments by the opponent (as well as arguments they \n    are likely to make in the future), and directly accomplishes the current step of the plan while building upon previously \n    executed steps.\n    A weak execution step makes a claim that is unclear, irrelevant, lacks logical structure, deviates from the plan, or fails to \n    engage with the ongoing conversation. \n    Assess whether the new claim integrates smoothly with previous claims and contributes meaningfully to persuading the audience.\n\n    Provide a floating-point score between 0 and 1, where 1 indicates a strong execution and 0 indicates a weak execution.\n    \"\"\"\n    topic: str = dspy.InputField(desc=TOPIC_DESC)\n    claim_plan: str = dspy.InputField(desc=PLAN_STEP_DESC)\n    new_claim: str = dspy.InputField(desc=CLAIM_TO_MAKE_DESC)\n    claims_so_far: typing.List[str] = dspy.InputField(desc=EXECUTION_DESC)\n    conversation: typing.List[str] = dspy.InputField(desc=CONVERSATION_DESC)\n    score: float = dspy.OutputField(desc=EXECUTION_SCORE_DESC)\n"}
{"type": "source_file", "path": "utils/constants.py", "content": "# Response Strategies\nfrom enum import Enum\n\n\nclass ReasoningType(Enum):\n    \"\"\"\n    The type of reasoning used for Tree-of-Thoughts modules.\n\n    ITERATIVE_DRAFTING: This type of reasoning involves generating argumentative responses that improve on previous drafts.\n    PLAN_AND_EXECUTE: This type of reasoning involves creating an initial plan for the argument (e.g., claims to make). \n        At the i'th layer of the tree, the agent executes the i'th step of the plan.\n    \"\"\"\n    ITERATIVE_DRAFTING = 'iterative_drafting'\n    PLAN_AND_EXECUTE = 'plan_and_execute'\n\nclass ReasoningComponent(Enum):\n    GENERATOR = 'generator'\n    EVALUATOR = 'evaluator'\n\nclass ReasoningPhase(Enum):\n    FIRST_STEP = 'first_step'\n    SUBSEQUENT_STEP = 'subsequent_step'\n    FINAL_STEP = 'final_step'\n\nclass Model(Enum):\n    \"\"\"\n    The model to use for generating responses.\n\n    GPT_4O: The GPT-4o model.\n    GPT_4O_MINI: The GPT-4o-mini model (cheaper than both GPT-4o and GPT-3.5-turbo, but performs only slightly worse than GPT-4o).\n    \"\"\"\n    GPT_4O = 'gpt-4o'\n    GPT_4O_MINI = 'gpt-4o-mini'\n\n\nclass SearchAlgorithm(Enum):\n    \"\"\"\n    The type of search algorithm to use in the Tree of Thoughts module.\n    \n    MINIMAX: The minimax algorithm. This algorithm assumes there are strictly 2 opposing players in an exchange, \n        and that they take turns making moves. The algorithm is used to determine the optimal move for a player in a \n        zero-sum game. This search type is only available in the RESPONSE granularity, since that granularity corresponds\n        with messages between two opposing players (while DRAFT corresponds with a single player).\n        \n    MONTE_CARLO_TREE_SEARCH: The Monte Carlo Tree Search algorithm. This algorithm is used to determine the optimal move\n        for a player in a game by simulating a large number of random games and selecting the move that leads to the\n        highest win rate. \n    \"\"\"\n    MONTE_CARLO_TREE_SEARCH = 'monte_carlo'\n    BEAM_SEARCH = 'beam_search'  # Known as Breadth-first search (BFS) within the original Tree of Thoughts paper.\n\n\nclass NodeSelectionStrategy(Enum):\n    \"\"\"\n    The strategy for evaluating which nodes to select for further expansion in the (vanilla) Tree of Thoughts module.\n\n    GREEDY: The greedy strategy. This strategy involves selecting the node with the highest score for expansion. In case of a tie,\n        the nodes with smaller indices are selected.\n    \n    SAMPLE: The sample strategy. This strategy involves selecting a node for expansion at random, with weights proportional to the\n        scores of the nodes. This strategy is useful for encouraging exploration in the tree of thoughts, especially when the scores\n        of the nodes are close to each other.\n    \"\"\"\n    GREEDY = 'greedy'\n    SAMPLE = 'sample'\n\n\nclass EvaluationStrategy(Enum):\n    \"\"\"\n    The strategy for evaluating the quality of a response.\n    \n    SCORE: The score-based strategy. This strategy involves evaluating the quality of a response based on a score assigned to the\n        response by a model. The score is typically a float between 0 and 1, where 1 indicates a high-quality response and 0 \n        indicates a low-quality response. Each response is evaluated in isolation. The top 'k' responses are considered \n        high-quality responses, and the rest are pruned.\n    \n    VOTE: The vote-based strategy. This strategy involves evaluating the quality of a response based on votes from multiple agents.\n        Each agent votes on the quality of the response, and the response is evaluated based on the majority vote (i.e., the \n        responses with the top 'k' votes are considered high-quality responses, and the rest are pruned).\n    \"\"\"\n    SCORE = 'score'\n    VOTE = 'vote'\n\nclass Color(Enum):\n    \"\"\"\n    The colors to use for highlighting text in the console. \n    \n    Different colors are used to distinguish between debators with different stances in a debate.\n\n    BLUE: The color blue.\n    RED: The color red.\n    \"\"\"\n    BLUE = 'blue'\n    RED = 'red'\n"}
{"type": "source_file", "path": "abstractions/generator/iterative_drafting_generator.py", "content": "import dspy\nimport typing\nfrom abstractions.generator.generator import (\n    TOPIC_DESC, \n    STANCE_DESC, \n    CONVERSATION_DESC, \n)\nfrom abstractions.generator.generator import ResponseParameters\n\n####################\n# Draft Generation #\n# ##################  \n\n# Inputs\nPREVIOUS_DRAFTS_DESC = \"\"\"\nA list of argumentative drafts that the debator is aiming to improving upon in the current draft.\nEach draft is an attempted improvement in quality and persuasiveness upon the previous draft.\n\"\"\".strip()\n\n# Outputs\nSINGLE_TURN_DRAFT_RESPONSE_DESC = \"\"\"\nA persuasive argument that takes the specified stance towards the specified topic while adhering to the specified response length.\nThis improved argument is meant to improve on the weaknesses of the previous drafts, while maintaining their relative strengths.\n\"\"\".strip()\nMULTI_TURN_DRAFT_RESPONSE_DESC = \"\"\"\nA persuasive argument that takes the specified stance towards the specified topic while adhering to the specified response length,\nand effectively addresses the main claims of the opponent in the conversation.\nThis improved argument is meant to improve on the weaknesses of the previous drafts, while maintaining their relative strengths.\n\"\"\".strip()\n\n\nclass SingleTurnDraftBranchingSignature(dspy.Signature):\n    \"\"\"\n    Generate an improved persuasive argument draft based on previous drafts, and according to the provided parameters.\n    The generated argument should take the provided topic stance towards the topic. \n    \n    Your argument should utilize rhetorical strategies such as ethos, logos, and pathos to enhance persuasiveness. \n    Support your claims with numerical, factual, and easily verifiable evidence to ensure credibility.\n    You should not make any claims or statements that you cannot support with evidence or logical reasoning.\n    Your argument should clarify complex concepts without relying on jargon or repetitive statements, making it accessible \n    to the audience. \n    While incorporating humor, metaphors, or other rhetorical devices can enhance engagement, they should be executed \n    appropriately to avoid undermining the argument's seriousness and effectiveness.\n\n    Your new draft should demonstrate a clear improvement over previous versions by addressing identified weaknesses \n    and reinforcing existing strengths. It should adhere to the specified response length, language style, and communication \n    tone to maintain consistency and effectiveness in persuasion.\n    \"\"\"\n    topic: str = dspy.InputField(desc=TOPIC_DESC)\n    stance: str = dspy.InputField(desc=STANCE_DESC)\n    response_parameters: ResponseParameters = dspy.InputField(desc=\"Parameters for generating the response.\")\n    previous_drafts: typing.List[str] = dspy.InputField(desc=PREVIOUS_DRAFTS_DESC)\n    response: str = dspy.OutputField(desc=SINGLE_TURN_DRAFT_RESPONSE_DESC)\n\n\nclass MultiTurnDraftBranchingSignature(dspy.Signature):\n    \"\"\"\n    Generate an improved persuasive argument draft within a multi-turn debate according to the provided parameters.\n\n    The generated argument should support or oppose the given topic stance, effectively addressing or anticipating \n    counter-arguments from the opponent within the conversation context. It must employ rhetorical strategies such as ethos, \n    logos, and pathos to enhance persuasiveness. Claims should be backed by numerical, factual, and easily verifiable evidence \n    to ensure reliability and credibility. Do not make claims which you cannot support with evidence or logic accessible to the \n    average person. The argument should clarify complex concepts without the use of jargon or repetitive statements, making it \n    comprehensible to the audience.\n\n    Incorporating humor, metaphors, or other rhetorical devices can enhance the argument's engagement, provided they are \n    executed appropriately to avoid detracting from the argument's seriousness and effectiveness. Additionally, the new draft \n    should maintain coherence with previous turns and strategically counter opposing points to strengthen persuasiveness.\n\n    This improved argument aims to enhance the persuasiveness and coherence of the previous drafts by addressing their \n    weaknesses and reinforcing their strengths. It should adhere to the specified response length, language style, and \n    communication tone to ensure consistency and effectiveness throughout the debate.\n    \"\"\"\n    topic: str = dspy.InputField(desc=TOPIC_DESC)\n    stance: str = dspy.InputField(desc=STANCE_DESC)\n    conversation: typing.List[str] = dspy.InputField(desc=CONVERSATION_DESC)\n    response_parameters: ResponseParameters = dspy.InputField(desc=\"Parameters for generating the response.\")\n    previous_drafts: typing.List[str] = dspy.InputField(desc=PREVIOUS_DRAFTS_DESC)\n    response: str = dspy.OutputField(desc=MULTI_TURN_DRAFT_RESPONSE_DESC)\n"}
{"type": "source_file", "path": "abstractions/generator/plan_and_execute_generator.py", "content": "import dspy\nimport typing\nfrom abstractions.generator.generator import (\n    TOPIC_DESC,\n    STANCE_DESC,\n    CONVERSATION_DESC,\n    ResponseParameters\n)\n\n\n###################\n# Plan Generation #\n###################\n\n# Input\nNUMBER_OF_CLAIMS_DESC = \"The desired number of steps in the plan for writing the argument.\"\n\n# Output\nPLAN_RESPONSE = \"The plan for how to persuade the audience to take the debator's stance on the topic.\"\n\nclass SingleTurnPlanningBranchingSignature(dspy.Signature):\n    \"\"\"\n    Generate a structured, step-by-step plan for persuading the audience to adopt your stance on the provided topic.\n    \n    The plan should consist of the specified number of steps, each representing a distinct claim, statement, or\n    argumentative strategy.\n\n    If the stance is 'PRO', each step should advocate for the topic's claim; if 'ANTI', each step should argue against it. \n    \n    Each claim within the plan should aim to strengthen your argument or boost your credibility. \n    The steps should be logically ordered to build a cohesive and persuasive strategy, ensuring that each claim effectively \n    contributes to the overall objective of persuading the audience. \n    Avoid vague or redundant steps, and ensure that each step is concise, clear, and actionable to facilitate detailed expansion \n    in subsequent execution phases.\n    \"\"\"\n    topic: str = dspy.InputField(desc=TOPIC_DESC)\n    stance: str = dspy.InputField(desc=TOPIC_DESC)\n    number_of_claims: int = dspy.InputField(desc=NUMBER_OF_CLAIMS_DESC)\n    response: typing.List[str] = dspy.OutputField(desc=PLAN_RESPONSE)\n\nclass MultiTurnPlanningBranchingSignature(dspy.Signature):\n    \"\"\"\n    Generate a structured, step-by-step plan for persuading the audience to adopt your stance on the provided topic.\n\n    The plan should consist of the specified number of steps, each representing a distinct claim, statement, or \n    argumentative strategy.\n    \n    If the stance is 'PRO', each step should advocate for the topic's claim; if 'ANTI', each step should argue against it. \n    \n    Each claim within the plan should aim to strengthen your argument, weaken your opponent's argument, or boost your \n    credibility. The steps should be logically ordered to build a cohesive and persuasive strategy, ensuring that each \n    claim effectively contributes to the overall objective of persuading the audience. \n    Additionally, consider the ongoing conversation with the opponent to ensure that the plan is adaptive and responsive to \n    counter-arguments. \n    Avoid vague or redundant steps, and ensure that each step is concise, clear, and actionable to facilitate detailed expansion \n    in subsequent execution phases.\n    \"\"\"\n    topic: str = dspy.InputField(desc=TOPIC_DESC)\n    stance: str = dspy.InputField(desc=STANCE_DESC)\n    number_of_claims: int = dspy.InputField(desc=NUMBER_OF_CLAIMS_DESC)\n    conversation: typing.List[str] = dspy.InputField(desc=CONVERSATION_DESC)\n    response: typing.List[str] = dspy.OutputField(desc=PLAN_RESPONSE)\n\n\n#############################\n# Plan Execution Generation # \n#############################\n\n# Input\nPLAN_DESC = \"A step-by-step plan for how to persuade the audience to take the debator's stance on the topic.\"\nEXISTING_CLAIMS_DESC = \"The claims that the debator has made so far (executions of previous steps in the plan).\"\nCLAIM_TO_MAKE_DESC = \"The claim that the debator should make as part of the current step of the plan.\"\n\n# Output\nPLAN_EXECUTION_RESPONSE = \"The persuasive argument that the debator generated to argue for the provided claim.\"\n\n\nclass SingleTurnPlanExecutionBranchingSignature(dspy.Signature):\n    \"\"\"\n    Execute a step in your plan to persuade the audience to adopt your stance on the provided topic.\n\n    You are provided with a specific claim to make as part of your argument, along with the claims you have made so far. \n    Generate a persuasive argument that supports, expands, or implements the claim, and which aligns with your overall stance. \n    \n    Utilize rhetorical strategies such as ethos, logos, and pathos to enhance persuasiveness. Support your claims with \n    numerical, factual, and easily verifiable evidence to ensure credibility. Avoid making statements that you cannot\n    support with evidence or logical reasoning.\n\n    Clarify complex concepts without relying on jargon or repetitive statements. While incorporating humor, metaphors, or other \n    rhetorical devices can enhance engagement, ensure they are executed appropriately to avoid undermining the argument's \n    effectiveness. Adhere to the specified response length, language style, and communication tone. Avoid vague or unsupported \n    statements, inappropriate language that contradicts the specified style, overly complex sentences, and redundant or \n    repetitive points.\n    \"\"\"\n    topic: str = dspy.InputField(desc=TOPIC_DESC)\n    stance: str = dspy.InputField(desc=STANCE_DESC)\n    existing_claims: typing.List[str] = dspy.InputField(desc=EXISTING_CLAIMS_DESC)\n    claim_to_make: str = dspy.InputField(desc=CLAIM_TO_MAKE_DESC)\n    response: str = dspy.OutputField(desc=PLAN_EXECUTION_RESPONSE)\n\n\nclass MultiTurnPlanExecutionBranchingSignature(dspy.Signature):\n    \"\"\"\n    Execute a step in your plan to persuade the audience of your stance in an ongoing debate about the provided topic.\n\n    You are provided with a specific claim to make as part of your argument, the claims you have made so far, and the ongoing \n    conversation with a rival debater. Generate a persuasive argument that supports, expands, or implements the claim, and which \n    aligns with your overall stance. \n\n    Utilize rhetorical strategies such as ethos, logos, and pathos to enhance persuasiveness. Support your claims with numerical, \n    factual, and easily verifiable evidence to ensure credibility. Do not make any claims or statements which you cannot support \n    with evidence or logic. Clarify complex concepts without relying on jargon or repetitive statements. \n    While incorporating humor, metaphors, or other rhetorical devices can enhance engagement, ensure they are executed \n    appropriately to avoid undermining the argument's and effectiveness.\n\n    The generated argument should build upon previous claims, addressing any identified weaknesses and reinforcing \n    existing strengths. Additionally, it should respond to or anticipate counter-arguments from the opponent based on \n    the ongoing conversation. Adhere to the specified response length, language style, and communication tone to maintain \n    consistency and effectiveness in persuasion. Avoid vague or unsupported statements, inappropriate language that \n    contradicts the specified style, overly complex sentences, redundant or repetitive points, and deviating from \n    addressing the opponent's claims within the conversation.\n    \"\"\"\n    topic: str = dspy.InputField(desc=TOPIC_DESC)\n    stance: str = dspy.InputField(desc=STANCE_DESC)\n    existing_claims: typing.List[str] = dspy.InputField(desc=EXISTING_CLAIMS_DESC)\n    claim_to_make: str = dspy.InputField(desc=CLAIM_TO_MAKE_DESC)\n    conversation: typing.List[str] = dspy.InputField(desc=CONVERSATION_DESC)\n    response: str = dspy.OutputField(desc=PLAN_EXECUTION_RESPONSE)\n\n\n#############################\n# Final Response Generation #\n#############################\n\nFINAL_RESPONSE_DESC = \"\"\"The final persuasive argument that the debator generated to argue for their stance on the topic.\nThis argument should be coherent and persuasive, and build on the claims and arguments from the plan and plan execution steps.\nThe argument must adhere to the specified response length.\n\"\"\".strip()\n\nclass SingleTurnPlanAndExecuteResponseSignature(dspy.Signature):\n    \"\"\"\n    Generate a persuasive argument that synthesizes your plan and its execution to support your stance on the topic.\n\n    You have access to a structured plan outlining your key claims and the corresponding executed arguments that elaborate on \n    each claim. Consolidate these elements into a coherent and persuasive argument that effectively supports your stance on the \n    topic. \n    \n    Utilize rhetorical strategies such as ethos, logos, and pathos to enhance persuasiveness. \n    Ensure that each claim is supported by numerical, factual, and easily verifiable evidence to maintain credibility. \n    Clarify any complex concepts without relying on jargon or repetitive statements to ensure accessibility to the audience. \n    While incorporating humor, metaphors, or other rhetorical devices can enhance engagement, they should be executed \n    appropriately to avoid undermining the argument's seriousness and effectiveness.\n\n    The final argument should seamlessly integrate the strengths of your plan and executions, addressing any previously \n    identified weaknesses and reinforcing existing strengths. \n    Adhere to the specified response length, language style, and communication tone to maintain consistency and effectiveness in \n    persuasion. Avoid vague or unsupported statements, inappropriate language that contradicts the specified style, overly \n    complex sentences, and redundant or repetitive points.\n    \"\"\"\n    topic: str = dspy.InputField(desc=TOPIC_DESC)\n    stance: str = dspy.InputField(desc=STANCE_DESC)\n    plan: typing.List[str] = dspy.InputField(desc=PLAN_DESC)\n    plan_execution: typing.List[str] = dspy.InputField(desc=EXISTING_CLAIMS_DESC)\n    response_parameters: ResponseParameters = dspy.InputField(desc=\"Parameters for generating a persuasive argument.\")\n    response: str = dspy.OutputField(desc=FINAL_RESPONSE_DESC)\n"}
{"type": "source_file", "path": "abstractions/tree/iterative_drafting_tree.py", "content": "from logging import Logger\nimport typing\nfrom abstractions.tree.tree import (\n    State, \n    Node,\n    MonteCarloNode,\n    Tree,\n)\nfrom abstractions.generator.generator import ResponseParameters\nfrom pydantic import Field\n\n\nclass DraftState(State):\n    \"\"\"Represents a planning step (draft) by a debator when writing a persuasive argument for a debate (on the given topic).\"\"\"\n    previous_drafts: typing.List[str] = Field(\n        default_factory=list,\n        description=\"A list of previous drafts of the debator's argument.\",\n    )\n\n    @property\n    def reasoning_steps(self) -> typing.List[str]:\n        return self.previous_drafts\n\n    def state_to_generator_input(self, response_parameters: ResponseParameters) -> typing.Dict[str, str | typing.List[str]]:\n        \"\"\"\n        Converts the state to an input for the generator.\n        \"\"\"\n        if len(self.conversation) == 0:\n            if len(self.previous_drafts) == 0:\n                return {\"topic\": self.topic, \"stance\": self.stance, \"response_parameters\": response_parameters}\n            else:\n                return {\n                    \"topic\": self.topic, \"stance\": self.stance, \"response_parameters\": response_parameters, \"previous_drafts\": self.previous_drafts\n                }\n        else:\n            if len(self.previous_drafts) == 0:\n                return {\"topic\": self.topic, \"stance\": self.stance,  \"response_parameters\": response_parameters, \"conversation\": self.conversation}\n            else:\n                return {\n                    \"topic\": self.topic, \n                    \"stance\": self.stance, \n                    \"response_parameters\": response_parameters, \n                    \"previous_drafts\": self.previous_drafts,\n                    \"conversation\": self.conversation,\n                }\n            \n    def reasoning_steps_to_string(self, exclude_most_recent: bool) -> str:\n        \"\"\"\n        Converts the reasoning steps to an informative string.\n        \n        Parameters:\n            exclude_most_recent (bool): Whether to exclude the most recent draft.\n        \n        Returns:\n            str: The reasoning steps as a string. The reasoning steps have a header and are separated by newlines.\n        \"\"\"\n        result = \"\"\n        previous_drafts_to_consider = self.previous_drafts[:-1] if exclude_most_recent else self.previous_drafts\n        for i, draft in enumerate(previous_drafts_to_consider):\n            result += f\"Draft {i + 1}:\\n{draft}\\n\\n\"\n        return result.strip()\n    \n    def state_to_evaluator_input(self) -> typing.Dict[str, str | typing.List[str]]:\n        \"\"\"\n        Converts the state to an input for the evaluator.\n        The most recent draft is the argument to be evaluated.\n        \"\"\"\n        if len(self.conversation) == 0:\n            if len(self.previous_drafts) < 2:       # Only a single draft has been made so far\n                return {\"topic\": self.topic, \"stance\": self.stance, \"argument\": self.previous_drafts[-1]}\n            else:\n                return {\n                    \"previous_drafts\": self.reasoning_steps_to_string(exclude_most_recent=True),\n                    \"argument\": self.previous_drafts[-1],\n                    \"topic\": self.topic,\n                }\n        else:\n            if len(self.previous_drafts) < 2:\n                return {\n                    \"argument\": self.previous_drafts[-1],\n                    \"conversation\": self.conversation,\n                    \"topic\": self.topic,\n                }\n            else:\n                return {\n                    \"previous_drafts\": self.reasoning_steps_to_string(exclude_most_recent=True),\n                    \"argument\": self.previous_drafts[-1],\n                    \"conversation\": self.conversation,\n                    \"topic\": self.topic,\n                }\n    \n\nclass DraftNode(Node):\n    \"\"\"A node in a search tree that stores drafts of persuasive arguments for a debate.\"\"\"\n    score: float = Field(\n        default=0.0,\n        description=\"The quality score of a node, which entails a perspective argumentative response.\",\n    )\n    state: DraftState = Field(\n        ...,\n        description=\"\"\"\nA representation of a node's state in a Tree-of-Thought framework.\nA state contains the topic of the debate, the stance of the debator towards the topic, and conversation so far.\nNotably, this state includes a list of previous drafts of the debator's argument, where each draft is designed to improve\nupon the previous draft iterations.\n\"\"\".strip(),\n    )\n\nclass MCTSDraftNode(DraftNode, MonteCarloNode):\n    \"\"\"A node in a search tree that stores drafts of persuasive arguments for a debate.\"\"\"\n\n\nclass DraftTree(Tree):\n    \"\"\"\n    A search tree for a debate, where nodes are conversation states in the debate,\n    and edges are responses between rival debators.\n    \n    In this type of tree, nodes also contain \"drafts\" of persuasive arguments for the debate.\n    That is, a child node within the tree crafts a new draft while taking into account the previous drafts.\n    \"\"\"\n\n    def _add_node_to_tree_from_existing_conversation(\n        self, \n        message_index: int, \n        topic: str, \n        stance: str, \n        conversation: typing.List[str],\n    ) -> Node:\n        return DraftNode(\n            index=message_index,\n            state=DraftState(\n                topic=topic,\n                stance=stance,\n                conversation=conversation[:message_index],\n                previous_drafts=[],\n            ),\n            parent_id=message_index - 1 if message_index > 0 else None,\n            children_ids=[message_index + 1],\n        )\n    \n    def _initialize_root(self, state: State) -> Node:\n        return DraftNode(\n            index=len(state.conversation),\n            state=DraftState(\n                topic=state.topic,\n                stance=state.stance,\n                conversation=state.conversation,\n                previous_drafts=[],\n            ),\n            parent_id=(\n                len(state.conversation) - 1  if len(state.conversation) > 0  else None\n            ),\n        )\n\n    def create_child_node(self, index: int, state: DraftState, output: str) -> Node:\n        return DraftNode(\n            index=index,\n            state=DraftState(\n                topic=state.topic,\n                stance=state.stance,\n                conversation=state.conversation,\n                previous_drafts=state.previous_drafts + [output],\n            ),\n        )\n\n    def create_voting_input_from_most_recent_layer(self) -> typing.Dict[str, typing.Any]:\n        most_recent_layer = self.layers[-1]\n        node = most_recent_layer[0]     # Select an arbitrary node from the most recent layer\n        arguments = {node_index: node.state.reasoning_steps[-1] for node_index, node in enumerate(most_recent_layer)}\n        if len(node.state.conversation) == 0:           # Single-turn\n            if len(node.state.reasoning_steps) < 2:     # Only first reasoning step\n                return {\n                    'arguments': arguments,\n                    'topic': node.state.topic,\n                }\n            else:\n                return {\n                    'previous_drafts': {\n                        node_index: node.state.reasoning_steps_to_string(exclude_most_recent=True)\n                        for node_index, node in enumerate(most_recent_layer)\n                    },\n                    'arguments': arguments,\n                    'topic': node.state.topic,\n                }\n        else:  # Multi-turn\n            if len(node.state.reasoning_steps) < 2:\n                return {\n                    'conversation': node.state.conversation,\n                    'arguments': arguments,\n                    'topic': node.state.topic,\n                }\n            else:\n                return {\n                    'previous_drafts': {\n                        node_index: node.state.reasoning_steps_to_string(exclude_most_recent=True)\n                        for node_index, node in enumerate(most_recent_layer)\n                    },\n                    'conversation': node.state.conversation,\n                    'arguments': arguments,\n                    'topic': node.state.topic,\n                }\n\n    def log_tree(self, logger: Logger):\n        for node in self.nodes:\n            logger.info(f\"\\n\\nNode {(node.index)}, Stance: {node.state.stance}\")\n            if(node.parent_id is not None):\n                logger.info(f'\\tPrevious draft: {self.edges[node.parent_id][node.index].response}')\n            logger.info(f'\\tTree-of-Thoughts score: {node.score}')\n            logger.info('\\tDrafts:')\n            for child_index in node.children_ids:\n                logger.info(\n                    f'\\n\\t- Draft #{child_index} [score = {self.nodes[child_index].score}]: '\n                    f'{self.edges[node.index][child_index].response}'\n                )\n                # Print the reasoning associated with the response if it exists\n                if self.edges[node.index][child_index].reasoning:\n                    logger.info(f'\\tReasoning: {self.edges[node.index][child_index].reasoning}')\n\nclass MCTSDraftTree(DraftTree):\n    \"\"\"\n    A search tree for a debate, where nodes are conversation states in the debate,\n    and edges are responses between rival debators.\n    \n    In this type of tree, nodes also contain \"drafts\" of persuasive arguments for the debate.\n    That is, a child node within the tree crafts a new draft while taking into account the previous drafts.\n    \n    Furthermore, since this is a Monte Carlo Tree Search (MCTS) tree, a node's score is accumulated over multiple simulations, \n    and the node contains an additional field for the number of visits.\n    \"\"\"\n\n    def _add_node_to_tree_from_existing_conversation(\n        self, \n        message_index: int, \n        topic: str, \n        stance: str, \n        conversation: typing.List[str],\n    ) -> Node:\n        return MCTSDraftNode(\n            index=message_index,\n            state=DraftState(\n                topic=topic,\n                stance=stance,\n                conversation=conversation[:message_index],\n                previous_drafts=[],\n            ),\n            parent_id=message_index - 1 if message_index > 0 else None,\n            children_ids=[message_index + 1],\n            score=0,\n            visits=0,\n        )\n    \n    def _initialize_root(self, state: State) -> Node:\n        return MCTSDraftNode(\n            index=len(state.conversation),\n            state=DraftState(\n                topic=state.topic,\n                stance=state.stance,\n                conversation=state.conversation,\n                previous_drafts=[],\n            ),\n            parent_id=(\n                len(state.conversation) - 1  if len(state.conversation) > 0  else None\n            ),\n            score=0,\n            visits=0,\n        )\n\n    def create_child_node(\n        self,\n        index: int,\n        state: DraftState,\n        output: str,\n    ):\n        return MCTSDraftNode(\n            index=index,\n            state=DraftState(\n                topic=state.topic,\n                stance=state.stance,\n                conversation=state.conversation,\n                previous_drafts=state.previous_drafts + [output],\n            ),\n            score=0,\n            visits=0,\n        )\n    \n    def log_tree(self, logger: Logger):\n        for node in self.nodes:\n            logger.info(f\"\\n\\nNode {(node.index)}, Stance: {node.state.stance}\")\n            if(node.parent_id is not None):\n                logger.info(f'\\tPrevious draft: {self.edges[node.parent_id][node.index].response}')\n            logger.info(f'\\tMonte Carlo score: {node.score}. Number of visits: {node.visits}')\n            logger.info('\\tDrafts:')\n            for child_index in node.children_ids:\n                logger.info(\n                    f'\\t- Draft #{child_index} [score = {self.nodes[child_index].score}]: '\n                    f'{self.edges[node.index][child_index].response}'\n                )\n                # Print the reasoning associated with the response if it exists\n                if self.edges[node.index][child_index].reasoning:\n                    logger.info(f'\\t\\tReasoning: {self.edges[node.index][child_index].reasoning}')\n"}
{"type": "source_file", "path": "utils/search_parameters.py", "content": "from pydantic import BaseModel, Field\n\n\nclass TreeOfThoughtsParameters(BaseModel):\n    \"\"\"\n    Parameters for the Tree of Thoughts module, which define the behavior of the search over reasoning steps.\n    \"\"\"\n    top_k: int = Field(\n        ...,\n        description=\"\"\"\nThe number of nodes to select at each reasoning step (layer) in the tree. \nPrioritizes nodes with the highest scores from intermediate reasoning evaluations.\n\"\"\".strip(),\n    )\n    n_samples_judge: int = Field(\n        ...,\n        description=\"\"\"\nThe number of samples to use when judging the quality of intermediate thoughts/reasoning steps.\nThe final score of a node is the average of the scores obtained from the ensemble of judges.\n\"\"\".strip(),\n    )\n    n_samples_generation: int = Field(\n        ...,\n        description=\"\"\"\nThe number of samples to generate for each reasoning step. The number of child nodes for each parent node.\n\"\"\".strip(),\n    )\n    judge_temperature: float = Field(\n        ...,\n        description=\"\"\"\nThe temperature to use when judging the quality of intermediate thoughts/reasoning steps.\nA float between 0 and 1.\n\"\"\".strip(),\n    )\n    generation_temperature: float = Field(\n        ...,\n        description=\"\"\"\nThe temperature to use when generating intermediate thoughts/reasoning steps and responses.\nA float between 0 and 1.\n\"\"\".strip(),\n    )\n    depth: int = Field(\n        ...,\n        description=\"\"\"\nThe number of intermediate thoughts/reasoning steps to take in the tree. \nEach step (layer) increases the depth of the tree by one.\n\"\"\".strip(),\n    )\n\nclass MonteCarloTreeOfThoughtParameters(BaseModel):\n    \"\"\"\n    Parameters for the Monte Carlo Tree of Thoughts module, which define the behavior of the search over reasoning steps.\n    \"\"\"\n\n    n_samples_judge: int = Field(\n        ...,\n        description=\"\"\"\nThe number of samples to use when judging the quality of intermediate thoughts/reasoning steps.\nThe final score of a node is the average of the scores obtained from the ensemble of judges.\n\"\"\".strip(),\n    )\n    n_samples_generation: int = Field(\n        ...,\n        description=\"\"\"\nThe number of samples to generate for each reasoning step. The number of child nodes for each parent node.\n\"\"\".strip(),\n    )\n    judge_temperature: float = Field(\n        ...,\n        description=\"\"\"\nThe temperature to use when judging the quality of intermediate thoughts/reasoning steps. A float between 0 and 1.\n\"\"\".strip(),\n    )\n    generation_temperature: float = Field(\n        ...,\n        description=\"\"\"\nThe temperature to use when generating intermediate thoughts/reasoning steps and responses. A float between 0 and 1.\n\"\"\".strip(),\n    )\n    rollout_depth: int = Field(\n        ...,\n        description=\"\"\"\nThe depth of the rollout.This is the maxiumum number of intermediate thoughts/reasoning steps to generate as part of \nthe rollout before scoring. \n\"\"\".strip(),\n    )\n    monte_carlo_iterations: int = Field(\n        ...,\n        description=\"\"\"\nThe number of iterations to perform the monte carlo tree search. Iterations consist of node selection, \nexpansion, simulation, and backpropagation.\n\"\"\".strip(),\n    )"}
{"type": "source_file", "path": "iterative_drafting_tree_of_thoughts.py", "content": "\nimport typing\nimport dspy\n\n# Tree of thoughts imports\nfrom tree_of_thoughts import TreeOfThoughts, TreeOfThoughtsParameters\nfrom monte_carlo_tree_of_thoughts import MonteCarloTreeOfThought, MonteCarloTreeOfThoughtParameters\n\nfrom abstractions.tree.tree import State, Node, Tree, create_conversation_state\nfrom abstractions.generator.generator import (\n    ResponseParameters,\n    SingleTurnResponseBranchingSignature, \n    MultiTurnResponseBranchingSignature,\n)\nfrom abstractions.evaluator.evaluator import (\n    SingleTurnScoreSingature, \n    MultiTurnScoreSignature,\n    SingleTurnVoteSignature, \n    MultiTurnVoteSignature,\n)\n\n# Iterative-drafting imports\nfrom abstractions.tree.iterative_drafting_tree import (\n    DraftState, \n    DraftTree,\n    MCTSDraftNode,\n    MCTSDraftTree,\n)\nfrom abstractions.generator.iterative_drafting_generator import (\n    SingleTurnDraftBranchingSignature,\n      MultiTurnDraftBranchingSignature,\n)\nfrom abstractions.evaluator.iterative_drafting_evaluator import (\n    SingleTurnScoreWithDraftsSignature, \n    MultiTurnScoreWithDraftsSignature,\n    SingleTurnVoteWithDraftsSignature, \n    MultiTurnVoteWithDraftsSignature,\n)\nfrom utils import constants\nfrom utils.utils import set_up_dspy\nfrom utils.flags import parser\nfrom utils.search_parameters import TreeOfThoughtsParameters, MonteCarloTreeOfThoughtParameters\n\n\nclass IterativeDraftingTreeOfThoughts(TreeOfThoughts):\n\n    def _initialize_reasoning_modules(self):\n        # Initialize generator modules\n        self.single_turn_first_step_generator = self._initialize_module(signature=SingleTurnResponseBranchingSignature)\n        self.single_turn_subsequent_step_generator = self._initialize_module(signature=SingleTurnDraftBranchingSignature)\n        self.multi_turn_first_step_generator = self._initialize_module(signature=MultiTurnResponseBranchingSignature)\n        self.multi_turn_subsequent_step_generator = self._initialize_module(signature=MultiTurnDraftBranchingSignature)\n        # Initialize evaluator modules\n        if self.evaluation_strategy == constants.EvaluationStrategy.SCORE.value:\n            self.single_turn_first_step_evaluator = self._initialize_module(signature=SingleTurnScoreSingature)\n            self.single_turn_subsequent_step_evaluator = self._initialize_module(signature=SingleTurnScoreWithDraftsSignature)\n            self.multi_turn_first_step_evaluator = self._initialize_module(signature=MultiTurnScoreSignature)\n            self.multi_turn_subsequent_step_evaluator = self._initialize_module(signature=MultiTurnScoreWithDraftsSignature)\n        else:       # Use voting\n            self.single_turn_first_step_evaluator = self._initialize_module(signature=SingleTurnVoteSignature)\n            self.single_turn_subsequent_step_evaluator = self._initialize_module(signature=SingleTurnVoteWithDraftsSignature)\n            self.multi_turn_first_step_evaluator = self._initialize_module(signature=MultiTurnVoteSignature)\n            self.multi_turn_subsequent_step_evaluator = self._initialize_module(signature=MultiTurnVoteWithDraftsSignature)\n    \n    def _initialize_tree(\n        self,\n        state: State,\n        depth: int,\n    ) -> Tree:\n        \"\"\"Initialize a draft tree with the given state.\n        \n        Parameters:\n            state (State): The state of intermediate reasoning/thinking in a given task (towards generating a response).\n            depth (int): The maximum depth of the tree.\n        \n        Returns:\n            Tree: The initialized tree\n        \"\"\"\n        # Ignore the depth passed in, as that not relevant for this reasoning type.\n        return DraftTree(state=state)\n\n    def _get_evaluator(self, state: State) -> dspy.Module:\n        \"\"\"Return an evaluator module based on the number of previous drafts and whether there is a pre-existing conversation\n        \n        Parameters:\n            state (State): The state of intermediate reasoning/thinking in a given task (towards generating a response).\n        \n        Returns:\n            dspy.Module: The evaluator module to use for the given state.\n        \"\"\"\n        assert isinstance(state, DraftState), f\"Invalid state type: {type(state)}. Must be of type DraftState.\"\n        if len(state.previous_drafts) < 2:\n            return self.multi_turn_first_step_evaluator if state.conversation else self.single_turn_first_step_evaluator\n        else:\n            return self.multi_turn_subsequent_step_evaluator if state.conversation else self.single_turn_subsequent_step_evaluator\n    \n    def _get_generator(self, state: State) -> dspy.Module:\n        \"\"\"Return a generator module based on the number of previous drafts and whether there is a pre-existing conversation.\n        \n        Parameters:\n            state (State): The state of intermediate reasoning/thinking in a given task (towards generating a response).\n        \n        Returns:\n            dspy.Module: The generator module to use for the given state.\n        \"\"\"\n        assert isinstance(state, DraftState), f\"Invalid state type: {type(state)}. Must be of type DraftState.\"\n        if len(state.previous_drafts) == 0:\n            return self.multi_turn_first_step_generator if state.conversation else self.single_turn_first_step_generator\n        else:\n            return self.multi_turn_subsequent_step_generator if state.conversation else self.single_turn_subsequent_step_generator\n    \n    def first_step(\n        self, \n        tree: Tree, \n        tot_parameters: TreeOfThoughtsParameters,\n        response_parameters: ResponseParameters,\n    ) -> typing.List[Node]:\n        \"\"\"No-op for the first step in the tree of thoughts. Simply return a frontier with the root node\n        \n        Parameters:\n            tree (Tree): The tree of thoughts to reason with.\n            tot_parameters (TreeOfThoughtsParameters): The parameters for the tree of thoughts.\n            response_parameters (ResponseParameters): The parameters for generating the response.\n        \n        Returns:\n            typing.List[Node]: The frontier of the tree of thoughts after the first step. In this case, it is simply the root node.\n        \"\"\"\n        return [tree.root]\n\n    def final_step(self, tree: Tree, tot_parameters: TreeOfThoughtsParameters, response_parameters: ResponseParameters) -> str:\n        \"\"\"Return the best draft from the (leaf nodes of a) tree of thoughts\n        \n        Parameters:\n            tree (Tree): The tree of thoughts to reason with.\n            tot_parameters (TreeOfThoughtsParameters): The parameters for the tree of thoughts.\n            response_parameters (ResponseParameters): The parameters for generating the response.\n        \n        Returns:\n            str: The best draft from the tree of thoughts, which serves as the final response.\n        \"\"\"\n        # Select the node in the frontier with the highest score\n        frontier = sorted(tree.layers[-1], key=lambda child_node: child_node.score, reverse=True)\n        self.logger.info(f\"Using the draft from node #{frontier[0].index} as the final response.\")\n        best_node: Node = frontier[0]\n        # We assume that the most recent draft is better than drafts from previous iterations. \n        # We therefore return the best draft among the leaf nodes.\n        reasoning_steps: typing.List[str] = best_node.state.reasoning_steps\n        return reasoning_steps[-1]\n\n\nclass IterativeDraftingMCTSTreeOfThoughts(IterativeDraftingTreeOfThoughts, MonteCarloTreeOfThought):\n\n    def _initialize_tree(self, state: State, depth: int) -> Tree:\n        \"\"\"Initialize a draft tree with the given state\n        \n        Parameters:\n            state (State): The state of intermediate reasoning/thinking in a given task (towards generating a response).\n            depth (int): The maximum depth of the tree.\n\n        Returns:\n            Tree: The initialized tree\n        \"\"\"\n        return MCTSDraftTree(state=state)\n\n    def __init__(\n        self, \n        use_chain_of_thought: bool = False, \n        node_selection_strategy: str = constants.NodeSelectionStrategy.GREEDY.value, \n        evaluation_strategy: str = constants.EvaluationStrategy.SCORE.value, \n    ) -> None:\n        \"\"\"Initialize an iterative drafting tree of thoughts with Monte Carlo Tree Search\n        \n        Parameters:\n            use_chain_of_thought (bool, optional): Whether to use chain of thoughts. Defaults to False.\n            node_selection_strategy (str, optional): The strategy for selecting nodes to expand (or prune) in the tree. \n                Defaults to constants.NodeSelectionStrategy.GREEDY.value.\n            evaluation_strategy (str, optional): The strategy for evaluating the quality of a response. \n                Defaults to constants.EvaluationStrategy.SCORE.value.\n        \"\"\"\n        IterativeDraftingTreeOfThoughts.__init__(self, use_chain_of_thought, node_selection_strategy, evaluation_strategy)\n    \n    def forward(\n        self, \n        state: State, \n        mcts_parameters: MonteCarloTreeOfThoughtParameters,\n        response_parameters: ResponseParameters, \n        do_visualize_tree: bool = False, \n        do_save_tree: bool = False, \n        verbose: bool = False,\n    ) -> str:\n        \"\"\"Forward pass through the tree of thoughts with Monte Carlo Tree Search. Involves performing multiple MCTS iterations\n        \n        Parameters:\n            state (State): The state of intermediate reasoning/thinking in a given task (towards generating a response).\n            mcts_parameters (MonteCarloTreeOfThoughtParameters): The parameters for the Monte Carlo Tree of Thoughts.\n            response_parameters (ResponseParameters): The parameters for generating the response.\n            do_visualize_tree (bool, optional): Whether to visualize the tree. Defaults to False.\n            do_save_tree (bool, optional): Whether to save the tree. Defaults to False.\n            verbose (bool, optional): Whether to print verbose logs. Defaults to False.\n        \n        Returns:\n            str: The final response generated by the tree of thoughts.\n        \"\"\"\n        return MonteCarloTreeOfThought.forward(\n            self, \n            state=state, \n            mcts_parameters=mcts_parameters,\n            do_visualize_tree=do_visualize_tree, \n            do_save_tree=do_save_tree,\n            verbose=verbose,\n            response_parameters=response_parameters, \n        )\n    \n    def generate_response(\n        self, \n        tree: Tree, \n        mcts_parameters: MonteCarloTreeOfThoughtParameters, \n        response_parameters: ResponseParameters,\n    ) -> str:\n        \"\"\"Generate the final response from the tree of thoughts\n        \n        Parameters:\n            tree (Tree): The tree of thoughts to reason with.\n            mcts_parameters (MonteCarloTreeOfThoughtParameters): The parameters for the Monte Carlo Tree of Thoughts.\n            response_parameters (ResponseParameters): The parameters for generating the response.\n        \n        Returns:\n            str: The final response generated by the tree of thoughts.\n        \"\"\"\n        del response_parameters\n        root_index = tree.root.index\n        root_node = tree.nodes[root_index]\n        best_node: MCTSDraftNode = None\n        max_score = 0\n        for child_index in root_node.children_ids:\n            child_node = tree.nodes[child_index]\n            score = child_node.score / child_node.visits\n            if score > max_score:\n                max_score = score\n                best_node = child_node\n        return best_node.state.reasoning_steps[-1]\n\n\nif __name__ == \"__main__\":\n\n    # Parse the command-line arguments    \n    args = parser.parse_args()\n    set_up_dspy(\n        openai_key_path=args.openai_key_path,\n        model_name=args.model_name,\n        max_tokens=args.max_tokens,\n    )\n\n    # Initialize the conversation state\n    conversation_state = create_conversation_state(\n        topic=args.topic,\n        stance=args.stance,\n        conversation_path=args.conversation_path,\n    )\n\n    print('Arguments:')\n    for arg in vars(args):\n        print(f'\\t{arg}: {getattr(args, arg)}')\n    print(f'Initial conversation state:\\n{conversation_state}')\n    \n    # Initialize the tree of thoughts\n    if args.search_strategy == constants.SearchAlgorithm.MONTE_CARLO_TREE_SEARCH.value:\n        print('Using Monte Carlo Tree Search.')\n        tree_of_thoughts = IterativeDraftingMCTSTreeOfThoughts(\n            use_chain_of_thought=args.use_chain_of_thought,\n            node_selection_strategy=args.node_selection_strategy,\n            evaluation_strategy=args.evaluation_strategy,\n        )\n        response = tree_of_thoughts(\n            state=conversation_state,\n            mcts_parameters=MonteCarloTreeOfThoughtParameters(\n                n_samples_judge=args.n_samples_judge, \n                n_samples_generation=args.n_samples_generation,\n                judge_temperature=args.judge_temperature,\n                generation_temperature=args.generation_temperature,\n                rollout_depth=args.depth,\n                monte_carlo_iterations=args.mcts_iterations,\n            ),\n            do_visualize_tree=args.with_visualization,\n            do_save_tree=args.save_tree,\n            response_parameters=ResponseParameters(\n                response_length=args.response_length,\n                communication_tone=args.communication_tone,\n                language_style=args.language_style,\n            ),\n        )\n    else:   # Use beam search\n        print('Using Beam Search.')\n        tree_of_thoughts = IterativeDraftingTreeOfThoughts(\n            use_chain_of_thought=args.use_chain_of_thought,\n            node_selection_strategy=args.node_selection_strategy,\n            evaluation_strategy=args.evaluation_strategy,\n            do_pruning=args.do_pruning,\n        )\n        response = tree_of_thoughts(\n            state=conversation_state,\n            tot_parameters=TreeOfThoughtsParameters(\n                top_k=args.top_k,\n                generation_temperature=args.generation_temperature,\n                judge_temperature=args.judge_temperature,\n                n_samples_generation=args.n_samples_generation,\n                n_samples_judge=args.n_samples_judge,\n                depth=args.depth,\n            ),\n            do_visualize_tree=args.with_visualization,\n            do_save_tree=args.save_tree,\n            response_parameters=ResponseParameters(\n                response_length=args.response_length,\n                communication_tone=args.communication_tone,\n                language_style=args.language_style,\n            ),\n        )\n\n    print(f'Final response:\\n{response}')\n    "}
{"type": "source_file", "path": "tree_of_thoughts.py", "content": "\"\"\"Implementation of the Tree-Of-Thoughts module.\"\"\"\nimport abc\nimport typing\nimport dspy\nimport numpy as np\n\n# Local imports\nimport utils.visualize_tree as visualize_tree\nfrom abstractions.tree.tree import State, Node, Tree\nfrom abstractions.generator.generator import ResponseParameters\n# Utilities\nfrom utils import constants\nfrom utils.utils import generate_name\nfrom utils import logger_config\nimport utils.visualize_tree as visualize_tree\nfrom utils.search_parameters import TreeOfThoughtsParameters\n\n###############################\n# To-Dos for Tree-Of-Thoughts #\n###############################\n\n# Create a metaclass that combines the ABCMeta and the type of the dspy.Module class.\n# This allows us to naturally extend the dspy.Module class while enforcing subclasses to implement specific methods.\nclass CombinedMeta(abc.ABCMeta, type(dspy.Module)):\n    pass\n\nclass TreeOfThoughts(dspy.Module, abc.ABC, metaclass=CombinedMeta):\n    \"\"\"\n    Utilize the Tree-Of-Thoughts framework to plan and reason before generating a response.\n    \"\"\"\n    \n    @abc.abstractmethod\n    def _get_evaluator(self, state: State) -> dspy.Module:\n        \"\"\"Retrieve the appropriate evaluator based on the state.\"\"\"\n        pass\n    \n    @abc.abstractmethod\n    def _get_generator(self, state: State) -> dspy.Module:\n        \"\"\"Retrieve the appropriate generator based on the state.\"\"\"\n        pass\n    \n    @abc.abstractmethod\n    def _initialize_tree(\n        self,\n        state: State,\n        depth: int,\n    ) -> Tree:\n        pass\n\n    @abc.abstractmethod\n    def first_step(\n        self, \n        tree: Tree, \n        tot_parameters: TreeOfThoughtsParameters,\n        response_parameters: str,\n    ) -> typing.List[Node]:\n        pass\n\n    @abc.abstractmethod\n    def final_step(self, tree: Tree, tot_parameters: TreeOfThoughtsParameters, response_parameters: ResponseParameters) -> str:\n        pass\n\n    def _initialize_module(self, signature: dspy.Signature) -> dspy.Module:\n        \"\"\"\n        Initialize a function based on the specified signature.\n\n        Parameters:\n            signature (dspy.Signature): The signature of the function to initialize.\n        \n        Returns:\n            dspy.Module: The initialized function.\n        \"\"\"\n        if self.use_chain_of_thoughts:\n            return dspy.TypedChainOfThought(signature=signature)\n        else:\n            return dspy.TypedPredictor(signature=signature)\n    \n    @abc.abstractmethod\n    def _initialize_reasoning_modules(self) -> None:\n        \"\"\"Initialize DSPy modules for reasoning (i.e., generators and evaluators).\"\"\"\n        pass\n    \n    def __init__(\n        self,\n        use_chain_of_thought: bool = False,\n        node_selection_strategy: str = constants.NodeSelectionStrategy.GREEDY.value,\n        evaluation_strategy: str = constants.EvaluationStrategy.SCORE.value,\n        do_pruning: bool = True,\n    ) -> None:\n        \"\"\"\n        Initialize the Tree-Of-Thoughts module.\n        \n        Parameters:\n            use_chain_of_thought (bool): Whether to use a chain of thought model for generating intermediate thoughts/reasoning.\n            node_selection_strategy (str): The strategy to use for selecting the nodes in the tree. \n                Must be one of [\"greedy\", \"sample\"].\n            evaluation_strategy (str): The strategy to use for evaluating the quality of a response.\n                Must be one of [\"score\", \"vote\"].\n            do_pruning (bool): Whether to prune the tree of thoughts at intermediate reasoning steps.\n        \"\"\"\n        super().__init__()\n        self.use_chain_of_thoughts = use_chain_of_thought\n        self.node_selection_strategy = node_selection_strategy\n        self.evaluation_strategy = evaluation_strategy\n        self.do_pruning = do_pruning\n        self.logger = logger_config.setup_logger(folder_path='./logs')\n        self._initialize_reasoning_modules()\n    \n    def forward(\n        self,\n        state: State,\n        tot_parameters: TreeOfThoughtsParameters,\n        response_parameters: ResponseParameters,\n        do_visualize_tree: bool = False,\n        do_save_tree: bool = False,\n        verbose: bool = False,\n    ) -> str:\n        \"\"\"\n        Utilize the Tree-Of-Thoughts framework to plan and reason before generating a response.\n        \n        Args:\n            state (State): The state of intermediate reasoning/thinking in a given task (towards generating a response).\n            tot_parameters (TreeOfThoughtsParameters): The parameters for the Tree-Of-Thoughts module. Defines the number of steps \n                (depth) to take in the tree, the number of samples to use when judging the quality of intermediate thoughts, \n                the number of samples to generate for each reasoning step, and the temperatures to use when judging and \n                generating thoughts.\n            do_visualize_tree (bool): Whether to visualize the tree of thoughts.\n            do_save_tree (bool): Whether to save the tree of thoughts to a file.\n            response_parameters (ResponseParameters): Parameters which define the nature of the response to generate (e.g., \n                length, tone, and language style).\n        \n        Returns:\n            str: The response generated by the Tree-Of-Thoughts module.\n        \"\"\"\n        assert tot_parameters.depth > 0, \"The number of steps (depth) for Tree-Of-Thoughts must be greater than 0.\"\n        # Initialize the tree\n        tree: Tree = self._initialize_tree(state=state, depth=tot_parameters.depth)\n        # Conditionally perform a \"first-step\" of reasoning (e.g., write and select a plan for witing an argument).\n        # This can also be a no-op, which simply returns the initial frontier.\n        frontier: typing.List[Node] = self.first_step(tree, tot_parameters, response_parameters)\n        # Perform subsequent reasoning steps (e.g., executing a step in a plan for writing an argument).\n        for layer_index in range(tot_parameters.depth):\n            self.logger.info(f\"Performing reasoning step {layer_index + 1}/{tot_parameters.depth}.\")\n            new_frontier: typing.List[Node] = self.step(tree, frontier, tot_parameters, response_parameters)\n            frontier = new_frontier\n        if not self.do_pruning:\n            # If the tree was created without pruning, then none of the nodes in the tree are scored. \n            # We therefore need to score the leaf nodes in the tree (i.e., the full sequence of thoughts/reasoning steps).\n            self.evaluate_thoughts(new_nodes=frontier, tree=tree, tot_parameters=tot_parameters)\n        response = self.final_step(tree, tot_parameters=tot_parameters, response_parameters=response_parameters)\n        if verbose:\n            tree.log_tree(logger=self.logger)\n        if do_save_tree:\n            tree.save_to_file(file_name=\"test_tree.json\")\n        if do_visualize_tree:\n            visualize_tree.draw_graph(\n                graph=visualize_tree.tree_to_graph(tree),\n                name=f\"{generate_name(tree.root.state.topic)}\",\n            )\n        return response\n        \n    def step(\n        self, \n        tree: Tree, \n        frontier: typing.List[int], \n        tot_parameters: TreeOfThoughtsParameters,\n        response_parameters: ResponseParameters\n    ) -> typing.List[int]:\n        \"\"\"\n        Perform a step by generating thoughts, evaluating them, and selecting the best thoughts for the next layer of the tree.\n        \n        Perform a step in the Tree-Of-Thoughts framework to generate a collection of thoughts for the next layer of the tree.\n\n        Given this collection of thoughts, evaluate them and select the top-k thoughts to continue to the next layer of the tree.\n        This step expands multiple nodes in the the deepest layer of the tree (i.e., the frontier).\n\n        Parameters:\n            tree (Tree): The tree of thoughts.\n            frontier (List[int]): The list of node indices in the frontier.\n            tot_parameters (TreeOfThoughtsParameters): The parameters for the Tree-Of-Thoughts module. Defines the number of steps \n                (depth) to take in the tree, the number of samples to use when judging the quality of intermediate thoughts, \n                the number of samples to generate for each reasoning step, and the temperatures to use when judging and \n                generating thoughts.\n            response_parameters (ResponseParameters): Parameters which define the nature of the response to generate (e.g., \n                length, tone, and language style).\n        \n        Returns:\n            List[int]: The list of node indices in the next frontier.\n        \"\"\"\n        # Generation:\n        # Generate new thoughts for each node in the frontier (corresponds with creating a new layer in the tree).\n        new_nodes: typing.List[Node] = self.generate_thoughts(tree, frontier, tot_parameters, response_parameters)   \n\n        # Evaluation:\n        # If we are pruning, we evaluate the nodes to determine which (top-k scoring nodes) to keep and which to prune.\n        # Otherwise, we simply return the generated nodes.\n        if self.do_pruning:\n            judged_nodes: typing.List[Node] = self.evaluate_thoughts(new_nodes, tree, tot_parameters)\n        else:\n            return new_nodes\n        \n        # Selection:\n        # If we are pruning, we select the top-k nodes to continue to the next layer of the tree, and prune the rest.\n        frontier: typing.List[int] = self.select_thoughts(judged_nodes, tot_parameters)\n        return frontier\n    \n    def generate_thoughts(\n        self, \n        tree: Tree, \n        frontier: typing.List[Node], \n        tot_parameters: TreeOfThoughtsParameters,\n        response_parameters: ResponseParameters,\n    ) -> typing.List[Node]:\n        \"\"\"\n        Expand each node in the frontier in order to obtain the nodes in the next layer of the tree.\n        \n        Parameters:\n            tree (Tree): The tree of thoughts.\n            frontier (List[Node]): The list of nodes in the frontier (i.e., the most recent/last layer of the tree, which consists\n                of leaf nodes).\n            tot_parameters (TreeOfThoughtsParameters): The parameters for the Tree-Of-Thoughts module. Defines the number of steps \n                (depth) to take in the tree, the number of samples to use when judging the quality of intermediate thoughts, \n                the number of samples to generate for each reasoning step, and the temperatures to use when judging and \n                generating thoughts.\n            response_length (str): The desired length of the response. For example, \"a few sentences\" or \"a paragraph\".\n        \n        Returns:\n            List[TreeOfThoughtsNode]: The nodes in the next layer of the tree. Note that these nodes do not yet include\n                the scores or reasoning behind the scores (which are added during the judging phase).\n        \"\"\"\n        n_samples_generation = tot_parameters.n_samples_generation\n        generation_temperature = tot_parameters.generation_temperature\n        new_nodes = []\n        generator: dspy.Module = self._get_generator(state=frontier[0].state)\n        \n        # Expand each node in the frontier\n        for node in frontier:\n            completions = generator(\n                **node.state.state_to_generator_input(response_parameters=response_parameters),\n                config=dict(n=n_samples_generation, temperature=generation_temperature),\n            ).completions\n            outputs: typing.List[str] = [response for response in completions.response]\n            reasonings: typing.List[str] = completions.reasoning if self.use_chain_of_thoughts else [\"N/A\"] * len(outputs)\n            if len(outputs) != n_samples_generation:\n                self.logger.warning(f\"The number of outputs generated must be equal to `n_samples_generation`. Expected: {n_samples_generation}, Actual: {len(outputs)}.\")\n            if len(reasonings) != n_samples_generation:\n                self.logger.warning(f\"The number of reasonings generated must be equal to `n_samples_generation`. Expected: {n_samples_generation}, Actual: {len(reasonings)}.\")\n            # Create a (child) node in the tree for each response from a parent node\n            for output, reasoning in zip(outputs, reasonings):\n                child_node = tree.create_child_node(index=len(tree.nodes), state=node.state, output=output)\n                tree.add_child_node(parent_node=node, child_node=child_node, response=output, expansion_reasoning=reasoning)\n                new_nodes.append(child_node)\n        \n        # Add a layer to the tree. We then select which nodes in this layer to keep, and which to prune\n        tree.add_layer(new_nodes)    \n        self.logger.info(f\"\\tGenerated new nodes: [{', '.join([str(node.index) for node in new_nodes])}]\")\n        return new_nodes\n    \n    def evaluate_thoughts(\n        self,\n        new_nodes: typing.List[Node],\n        tree: Tree,\n        tot_parameters: TreeOfThoughtsParameters,\n    ) -> typing.List[Node]:\n        if self.evaluation_strategy == constants.EvaluationStrategy.SCORE.value:    # Use \"scoring\" module for judges\n            judged_nodes: typing.List[Node] = []\n            for node in new_nodes:\n                judged_nodes.append(self.score_thought(node=node, tree=tree, tot_parameters=tot_parameters))\n        else:                                                                       # Use \"voting\" module for judges\n            judged_nodes: typing.List[Node] = self.vote_on_thoughts(nodes=new_nodes, tree=tree, tot_parameters=tot_parameters)\n        self.logger.info(f\"\\tNew node scores: {[node.score for node in judged_nodes]}\")\n        return judged_nodes\n\n    def vote_on_thoughts(\n        self,\n        nodes: typing.List[Node],\n        tree: Tree,\n        tot_parameters: TreeOfThoughtsParameters\n    ) -> typing.List[Node]:\n        \"\"\"\n        Aggregate votes on which of the drafts is most persuasive, and assign scores to nodes based on the votes.\n        \n        Parameters:\n            nodes (List[TreeOfThoughtsNode]): The nodes in the tree of thoughts.\n            tree (Tree): The tree of thoughts.\n            tot_parameters (TreeOfThoughtsParameters): The parameters for the Tree-Of-Thoughts module. Defines the number of steps \n                (depth) to take in the tree, the number of samples to use when judging the quality of intermediate thoughts, \n                the number of samples to generate for each reasoning step, and the temperatures to use when judging and \n                generating thoughts.\n        \n        Returns:\n            List[Node]: The nodes in the tree of thoughts with the scores assigned (based on the votes).\n                These nodes are also augmented to include the reasoning behind the votes (if applicable).\n        \"\"\"\n        n_samples_judge = tot_parameters.n_samples_judge\n        judge_temperature = tot_parameters.judge_temperature\n        judge: dspy.Module = self._get_evaluator(state=nodes[0].state)\n        completions = judge(\n            **tree.create_voting_input_from_most_recent_layer(),\n            config=dict(n=n_samples_judge, temperature=judge_temperature),\n        ).completions\n        one_hot_votes: typing.List[int] = completions.index   # A vector of length `n_samples_judge` with the index of the vote\n        # Reasoning is a list of length `n_samples_judge`, which contains the reasoning for each vote (in an ensemble of voting \n        # judges). Therefore, the reasoning for the resulting scores of each node is the reasoning of the combined votes.\n        reasoning = \"\\n\\n\".join(completions.reasoning) if self.use_chain_of_thoughts else \"N/A\"\n        counts = [0 for _ in range(len(nodes))]\n        for vote in one_hot_votes:\n            counts[vote] += 1\n        self.logger.info(f\"\\tNodes [{', '.join([str(node.index) for node in nodes])}] received vote counts: {counts}\")\n        # Scores are the fraction of votes for each node\n        scores = [count / sum(counts) for count in counts]\n        # Assign the scores to the nodes in the tree\n        for node, score in zip(nodes, scores):\n            tree.nodes[node.index].score = score\n            tree.nodes[node.index].reasoning = reasoning\n        return nodes\n\n    def score_thought(\n        self, \n        node: Node, \n        tree: Tree,\n        tot_parameters: TreeOfThoughtsParameters,\n    ) -> Node:\n        \"\"\"\n        Judge the drafts generated for a node in the tree of thoughts.\n        \n        Parameters:\n            node (Node): The node in the tree of thoughts.\n            tree (Tree): The tree of thoughts.\n            n_samples_judge (int): The number of samples to use when judging the drafts.\n            tot_parameters (TreeOfThoughtsParameters): The parameters for the Tree-Of-Thoughts module.\n                Includes the number of samples to use when judging the quality of intermediate thoughts, and the temperature\n                to use when judging the thoughts.\n        \n        Returns:\n            TreeOfThoughtsNode: The node in the tree of thoughts with the score assigned (and reasoning, if applicable).\n        \"\"\"\n        n_samples_judge = tot_parameters.n_samples_judge\n        judge_temperature = tot_parameters.judge_temperature\n        judge: dspy.Module = self._get_evaluator(state=node.state)\n        completions = judge(\n            **node.state.state_to_evaluator_input(),\n            config=dict(n=n_samples_judge, temperature=judge_temperature),\n        ).completions\n        score = np.mean(completions.score).round(3)  # Average the scores from the ensemble of (`n_samples_judge`) judges\n        self.logger.info(f\"\\t\\tScore for node {node.index} is {score}\")      \n        judge_reasoning = \"\\n\\n\".join(completions.reasoning) if self.use_chain_of_thoughts else \"N/A\"\n        # Modify the node in the tree\n        tree.nodes[node.index].score = score\n        tree.nodes[node.index].reasoning = judge_reasoning\n        return tree.nodes[node.index]\n    \n\n    def select_thoughts(\n        self, \n        judged_nodes: typing.List[Node], \n        tot_parameters: TreeOfThoughtsParameters,\n    ) -> typing.List[Node]:\n        \"\"\"\n        Select the top-k nodes to continue to the next layer of the tree.\n\n        Parameters:\n            tree (Tree): The tree of thoughts.\n            judged_nodes (List[Node]): The nodes in the tree of thoughts with the scores assigned.\n            tot_parameters (TreeOfThoughtsParameters): The parameters for the Tree-Of-Thoughts module. Defines the number of steps \n                (depth) to take in the tree, the number of samples to use when judging the quality of intermediate thoughts, \n                the number of samples to generate for each reasoning step, and the temperatures to use when judging and \n                generating thoughts.\n\n        Returns:\n            List[int]: The list of node indices in the next frontier.\n        \"\"\"\n        if self.node_selection_strategy == constants.NodeSelectionStrategy.GREEDY.value:    # Select nodes greedily\n            frontier: typing.List[Node] = sorted(\n                judged_nodes, key=lambda child_node: child_node.score, reverse=True\n            )[:tot_parameters.top_k]\n        else:                                                                               # Select nodes with weighted sampling\n            values: typing.List[float] = [child_node.score for child_node in judged_nodes]\n            frontier: typing.List[Node] = np.random.choice(\n                judged_nodes, size=tot_parameters.top_k, p=(np.array(values) / sum(values))\n            ).tolist()\n        # Sort by index once we've selected the top-k nodes. \n        # This is to ensure that the order of the nodes in the frontier is consistent.\n        frontier: typing.List[Node] = sorted(frontier, key=lambda child_node: child_node.index)\n        self.logger.info(f\"\\tSelected nodes: [{', '.join([str(node.index) for node in frontier])}]\")\n\n        # Prune the nodes that were not selected\n        non_selected_nodes: typing.List[Node] = [node for node in judged_nodes if node not in frontier]\n        self.logger.info(f\"\\tPruned nodes: [{', '.join([str(node.index) for node in non_selected_nodes])}]\")\n        for node in non_selected_nodes:\n            node.is_pruned = True\n        return frontier\n"}
{"type": "source_file", "path": "utils/utils.py", "content": "import time\nimport dspy\nimport os\nfrom typing import List, Dict, Any\nimport datetime\nimport json\nimport pickle\n\n\ndef set_up_dspy(\n    openai_key_path: str,\n    model_name: str = \"gpt-4o-mini\",\n    max_tokens: int = 1000,\n    use_cache: bool = True,\n):\n    \"\"\"\n    Set up the DSPY environment with the specified model.\n    \n    Parameters:\n        openai_key_path (str): The path to the OpenAI key file.\n        model_name (str): The name of the model to use.\n        max_tokens (int): The maximum number of tokens to use when generating responses.\n        use_cache (bool): Whether to use the cache.\n    \"\"\"\n    with open(openai_key_path, 'r') as file:\n        openai_api_key = file.read().strip()\n    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n    model = dspy.LM(model=f'openai/{model_name}', model_type='chat', max_tokens=max_tokens, cache=use_cache)\n    dspy.settings.configure(lm=model)\n\ndef generate_name(name: str) -> str:\n    \"\"\"\n    Generates a unique name for a file.\n\n    Parameters:\n        name (str): The base name for the file.\n\n    Returns:\n        str: The unique name for the file.\n    \"\"\"\n\n    current_time = time.strftime(\"%m-%d-%H-%M-%S\")\n\n    if len(name.split()) <= 5:\n        return '_'.join(name.lower().split()) + '_' + current_time\n    return '_'.join(name.lower().split()[-5:]) + '_' + current_time\n\n"}
{"type": "source_file", "path": "abstractions/tree/tree.py", "content": "import abc\nimport json\nimport logging\nimport os\nfrom pydantic import (\n    BaseModel, \n    Field, \n)\nfrom abstractions.generator.generator import ResponseParameters\nimport typing\n\nfrom utils.utils import generate_name\n\n\n###########################################################\n### Abstractions for Strategic Debate + Tree-of-Thought ###\n###########################################################\n\nclass State(BaseModel):\n    \"\"\"Represents a point in time in a debate between two rival debators (with opposing stances towards the given topic).\"\"\"\n\n    topic: str = Field(\n        description=\"The topic of the debate (e.g., 'Collaboration is better than competition').\",\n    )\n    stance: str = Field(\n        description=\"The stance of the debate. Either 'PRO' or 'ANTI'.\",\n    )\n    conversation: typing.List[str] = Field(\n        default=[],\n        description=\"\"\"\nA list of messages in the conversation so far. The last message is the most recent message. \nEach message is preceded by the message of the rival debator.\n\"\"\".strip(),\n    )\n\n    # TODO: Convert 'length' to 'generation_config' (which includes parameters such as 'length', 'formality', and 'tone')\n    def state_to_generator_input(self, response_parameters: ResponseParameters) -> typing.Dict[str, str | typing.List[str]]:\n        \"\"\"Converts the state to an input for the generator.\"\"\"\n        if len(self.conversation) == 0:\n            return {'topic': self.topic, 'stance': self.stance, 'response_parameters': response_parameters}\n        else:\n            return {'topic': self.topic, 'stance': self.stance, 'response_parameters': response_parameters, 'conversation': self.conversation}\n    \n    @property\n    def reasoning_steps(self) -> typing.List[str]:\n        \"\"\"Retrieves the reasoning steps from the conversation.\"\"\"\n        return self.conversation\n    \n    def reasoning_steps_to_string(self, exclude_most_recent: bool = False) -> str:\n        \"\"\"\n        Wraps the reasoning steps in a single string.\n        \"\"\"\n        if exclude_most_recent:\n            return \"\\n\".join(self.reasoning_steps[:-1])\n        return \"\\n\".join(self.reasoning_steps)\n    \n    def state_to_evaluator_input(self) -> typing.Dict[str, str | typing.List[str]]:\n        \"\"\"\n        Converts the state to an input for the evaluator.\n        The most recent message in the conversation is the argument to be evaluated.\n        \"\"\"\n        if len(self.conversation) == 0:\n            return {'topic': self.topic, 'stance': self.stance, 'argument': self.conversation[-1]}\n        else:\n            return {\n                'topic': self.topic, \n                'stance': self.stance, \n                'conversation': self.conversation[:-1], \n                'argument': self.conversation[-1]\n            }\n    \n\n############\n### Node ###\n############\n\n# Minimax Node\n\nclass Node(BaseModel):\n    \"\"\"A node in a search tree.\"\"\"\n\n    index: int = Field(\n        ...,\n        description=\"The index of the node in the tree.\",\n    )\n    state: State = Field(\n        ...,\n        description=\"\"\"\nA representation of a node's state in a Tree-of-Thought framework. \nA state contains the topic of the debate, the stance of the debator towards the topic, and conversation so far.\n\"\"\".strip(),\n    )\n    parent_id: typing.Optional[int] = Field(\n        default=None, \n        description=\"\"\"\nThe index of the parent node of this node. The response from the rival debator is an edge from the parent to this node.\n\"\"\".strip(),\n    )\n    score: typing.Optional[float] = Field(\n        default=0, \n        description=\"\"\"\nA floating point number between 0 and 1, where 1 represents that 'stance_to_maximize' is most likely to win the debate,\nand 0 represents that 'stance_to_maximize' is most likely to lose the debate.\n\"\"\".strip(),\n    )\n    children_ids: typing.Optional[typing.List[int]] = Field(\n        default_factory=list,\n        description=\"\"\"\nThe indices of the children of this node. The edges to each child correspond with possible responses to the most recent message in the debate.\n\"\"\".strip(),\n    )\n    reasoning: typing.Optional[str] = Field(\n        default=None,\n        description=\"The reasoning behind the score of this node (as computed from the leaf nodes via minimax).\",\n    )\n    is_pruned: bool = Field(\n        default=False,\n        description=\"Whether the node has been pruned from the tree.\",\n    )\n\n    def __lt__(self, other: 'Node'):\n        return self.score < other.score\n    \n\nclass MonteCarloNode(Node):\n    \"\"\"A node in a search tree that stores the results of Monte Carlo simulations.\"\"\"\n    score: float = Field(\n        default=0.0,\n        description=\"The average score of this node from the Monte Carlo simulations.\",\n    )\n    visits: int = Field(\n        default=0,\n        description=\"The number of times this node has been visited in the Monte Carlo simulations.\",\n    )\n\n\n############\n### Edge ###\n############\n\nclass Edge(BaseModel):\n    \"\"\"An edge in search tree representing a response from one debator to their rival\"\"\"\n\n    response: str = Field(\n        ...,\n        description=\"\"\"\nThe text response produced by the debator in the source node (who's stance is opposite the stance of the \ndebator in the target node).\n\"\"\".strip,\n    )\n    reasoning: typing.Optional[str] = Field(\n        default=None,\n        description=\"The reasoning behind the response.\",\n    )\n\n############\n### Tree ###\n############\n\nclass Tree(abc.ABC):\n\n    @abc.abstractmethod\n    def _add_node_to_tree_from_existing_conversation(\n        self,\n        message_index: int,\n        topic: str,\n        stance: str,\n        conversation: typing.List[str],\n    ) -> Node:\n        \"\"\"\n        Adds a node to the tree based on an existing conversation.\n        \n        Parameters:\n            message_index (int): The index of the message in the conversation.\n            topic (str): The topic of the debate.\n            stance (str): The stance of the debator (towards the topic) at the specified message index.\n            conversation (List[str]): The full conversation so far.\n        \n        Returns:\n            Node: The node to add to the tree (corresponding to the specified message).\n        \"\"\"\n        pass\n\n    def _initialize_conversation(\n        self,\n        stance: str,\n        topic: str,\n        conversation: typing.List[str],\n    ) -> typing.List[str]:\n        \"\"\"\n        Initializes the conversation with the given messages.\n        \n        Parameters:\n            conversation (List[str]): The messages to initialize the conversation with.\n        \n        Returns:\n            List[str]: The initialized conversation.\n        \"\"\"\n        # Add nodes and edges for each message in the conversation\n        current_stance = stance\n        # Determine the stance of the first message in the conversation by reversing the order of the conversation,\n        # and iteratively flipping the stance for each message (given only the status of the \"root\" message)\n        for message in reversed(conversation):\n            current_stance = \"PRO\" if current_stance == \"ANTI\" else \"ANTI\"\n        for message_index, message in enumerate(conversation):\n            # Add a node for the message\n            self.nodes.append(\n                self._add_node_to_tree_from_existing_conversation(\n                    message_index=message_index,\n                    topic=topic,\n                    stance=current_stance,\n                    conversation=conversation,\n                )\n            )\n            # Set the stance for the next message\n            current_stance = \"PRO\" if current_stance == \"ANTI\" else \"ANTI\"\n            # Add an edge from the parent to the child\n            if message_index > 0:\n                self.add_edge(\n                    source_index=message_index - 1,\n                    target_index=message_index,\n                    edge=Edge(response=message),\n                )\n    \n    @abc.abstractmethod\n    def _initialize_root(\n        self,\n        state: State,\n    ) -> Node:\n        \"\"\"\n        Initializes the root node of the tree.\n        \n        Parameters:\n            state (State): A representation of the context that a language model uses to generate a response.\n\n        Returns:\n            Node: The root node of the tree.\n        \"\"\"\n        return Node(\n            index=len(state.conversation),\n            state=State(\n                topic=state.topic,\n                stance=state.stance,\n                conversation=state.conversation,\n            ),\n            parent_id=(\n                len(state.conversation) - 1  if len(state.conversation) > 0  else None\n            ),\n        )\n        \n    def __init__(\n        self, \n        state: State,\n    ):\n        \"\"\"\n        Initializes the search tree from the given state.\n        \"\"\"\n        self.nodes, self.edges = [], {}\n        self.layers: typing.List[typing.List[Node]] = []\n        if len(state.conversation):  # Non-empty conversation\n            # Initialize nodes corresponding to the existing conversation\n            self._initialize_conversation(\n                stance=state.stance,\n                topic=state.topic,\n                conversation=state.conversation,\n            )\n            # Add the root node\n            self.root = self._initialize_root(state)\n            # Add the root node to the list of nodes\n            self.nodes.append(self.root)\n            # Add an edge to the root node from the last message in the conversation\n            self.add_edge(\n                source_index=len(state.conversation) - 1, \n                target_index=len(state.conversation), \n                edge=Edge(response=state.conversation[-1])\n            )\n        else:\n            self.root = self._initialize_root(state)\n            self.nodes.append(self.root)\n    \n    def add_layer(self, layer: typing.List[Node]) -> None:\n        \"\"\"\n        Adds a layer to the tree.\n        \n        Parameters:\n            layer (List[Node]): The layer to add.\n        \"\"\"\n        self.layers.append(layer)\n    \n    @abc.abstractmethod\n    def create_child_node(self, index: int, state: State, output: str) -> Node:\n        \"\"\"\n        Create a child node, and include the output in the child state.\n\n        Parameters:\n            index (int): The index of the child node.\n            state (State): The state of the child node that we are creating.\n            output (str): The output to include in the child state.\n        \"\"\"\n        pass\n\n    def add_edge(self, source_index: int, target_index: int, edge: Edge) -> None:\n        \"\"\"\n        Add an edge to the tree from the source node to the target node.\n        \n        Parameters:\n            source_index (int): The index of the source node.\n            target_index (int): The index of the target node.\n            edge (Edge): The edge to add.    \n        \"\"\"\n        if source_index not in self.edges:\n            self.edges[source_index] = {}\n        self.edges[source_index][target_index] = edge\n\n    def add_child_node(\n        self,\n        parent_node: Node,\n        child_node: Node,\n        response: str,\n        expansion_reasoning: str,\n    ) -> None:\n        \"\"\"\n        Adds a single child node to the tree based on the given claim.\n\n        Adds an edge from the parent node to the child node, updates the parent node's children_ids list, \n        includes the parent ID in the child node, and adds the child node to the tree.\n\n        Parameters:\n            parent_node (Node): The parent node to which the child node will be added.\n            child_node (Node): The child node to add.\n            response (str): The response produced by the parent node (included in the state of the child node, and in the \n                edge from the parent node to the child node).\n            expansion_reasoning (str): The reasoning behind the expansion of the tree. This is produced by the LLM when\n                using chain-of-thought.\n        \"\"\"\n        edge = Edge(source=parent_node.index, target=child_node.index, response=response, reasoning=expansion_reasoning)\n        parent_node.children_ids.append(child_node.index)\n        child_node.parent_id = parent_node.index\n        self.nodes.append(child_node)\n        self.add_edge(source_index=parent_node.index, target_index=child_node.index, edge=edge)\n    \n    def create_voting_input_from_most_recent_layer(self) -> BaseModel:\n        \"\"\"\n        Creates an input for the voting judged based on the most recent layer of the tree.\n\n        Extracts the most recent layer from the tree, and creates an input for the voting judge. The judge consists of an\n        ensemble of LLMs, each of which votes for the best chain of reasoning so far (i.e., the paths from the root to each\n        node in the most recent layer).\n\n        Returns:\n            BaseModel: The input for the voting judge. The specific input structure depends on the reasoning type of the tree.\n        \"\"\"\n        raise NotImplementedError(f\"Voting is not supported in module {self.__class__.__name__}.\")\n    \n    def save_to_file(self, file_name: str = None) -> None:\n        \"\"\"\n        Save the search tree to a JSON file.\n        \n        Parameters:\n            file_name (str): The name of the file to save the tree to.\n        \"\"\"\n        if not os.path.exists('outputs'):\n            os.makedirs('outputs')\n        \n        if not file_name:\n            file_name = generate_name(self.root.state.topic)\n            json_path = os.path.join('outputs',f'{file_name}.json')\n        else:\n            json_path = file_name\n        \n        nodes = [json.loads(node.json()) for node in self.nodes]\n        edges = {\n            source: {\n                target: json.loads(edge.model_dump_json()) for target, edge in edges.items()\n            } for source, edges in self.edges.items()\n        }\n        root = json.loads(self.root.model_dump_json())\n        print(f'Saving tree to {json_path}')\n        with open(json_path, 'w') as file:\n            json.dump({\"root\": root, \"nodes\": nodes, \"edges\": edges}, file, indent=4)\n    \n    def log_tree(self, logger: logging.Logger):\n        \"\"\"\n        Logs the nodes and edges of the tree.\n        \n        Parameters:\n            logger (logging.Logger): The logger to use for logging the tree.\n        \"\"\"\n        for node in self.nodes:\n            logger.info(f\"\\n\\nNode {(node.index)}\")\n            if(node.parent_id is not None):\n                logger.info(f'\\tParent ID: {node.parent_id}')\n                logger.info(f'\\tParent Stance: {self.nodes[node.parent_id].state.stance}')\n                logger.info(f'\\tParent Response: {self.edges[node.parent_id][node.index].response}')\n                logger.info(f'\\tParent reasoning: {self.edges[node.parent_id][node.index].reasoning}')\n            logger.info(f'\\tStance: {node.state.stance}')\n            if node.parent_id in self.nodes:\n                logger.info(f'\\tResponse: {self.edges[node.parent_id][node.index].response}')\n            logger.info(f'\\tScore: {node.score}')\n            logger.info(f'\\tChild nodes:')\n            for child_index in node.children_ids:\n                logger.info(\n                    f'\\t- Response #{child_index} [score = {self.nodes[child_index].score}]: '\n                    f'{self.edges[node.index][child_index].response}'\n                )\n                # Print the reasoning associated with the response if it exists\n                if self.edges[node.index][child_index].reasoning:\n                    logger.info(f'\\t\\tReasoning: {self.edges[node.index][child_index].reasoning}')\n\n\n#################\n### Functions ###\n#################\n\ndef create_conversation_state(\n    topic: str, \n    stance: str, \n    conversation_path: str,\n) -> State:\n    \"\"\"\n    Create a conversation state for the strategic debater.\n    \n    Parameters:\n        topic (str): The topic of the debate.\n        stance (str): The stance of the debator.\n        conversation_path (str): The path to the conversation file.\n        get_random_topic (bool): Whether to use a random topic. Defaults to False.\n    \n    Returns:\n        State: The conversation state for the strategic debater.\n    \"\"\"\n    # Initialize Tree-of-Thoughts given a conversation in the provided file\n    if conversation_path:\n        with open(conversation_path, 'r') as file:\n            conversation = file.readlines()\n        conversation_state = State(\n            topic=topic,\n            stance=stance,\n            conversation=conversation,\n        )\n    # Initialize Tree-of-Thoughts given an empty conversation\n    else:  \n        conversation_state = State(\n            topic=topic,\n            stance=stance,\n            conversation=[],\n        )\n    return conversation_state\n"}
{"type": "source_file", "path": "abstractions/tree/plan_and_execute_tree.py", "content": "from logging import Logger\nimport typing\nfrom abstractions.generator.generator import ResponseParameters\nfrom abstractions.tree.tree import State, Node, Edge, Tree, MonteCarloNode\nfrom pydantic import Field\n\nclass PlanAndExecuteState(State):\n    \"\"\"Represents a planning step (draft) by a debator when writing a persuasive argument for a debate (on the given topic).\"\"\"\n    claims_to_generate: int = Field(\n        ...,\n        description=\"The number claims to generate. Each claim is a single step in the plan.\",\n    )\n    plan: typing.List[str] = Field(\n        default_factory=list,\n        description=\"A list of brief argumentative claims that the debator plans to make in the debate.\",\n    )\n    claims_so_far: typing.List[str] = Field(\n        default_factory=list,\n        description=\"A list of argumentative claims that the debator has made so far in the debate (in accordance with the plan).\",\n    )\n\n    @property\n    def reasoning_steps(self) -> typing.List[str]:\n        \"\"\"Retrieves reasoning steps in the form of claims generated so far. Each claim is an execution of a step in the plan.\"\"\"\n        return self.claims_so_far\n\n    def state_to_generator_input(self, response_parameters: ResponseParameters) -> typing.Dict[str, str | typing.List[str]]:\n        \"\"\"Converts the state to an input for the generator.\"\"\"\n        if len(self.conversation) == 0:\n            if self.plan:\n                return {\n                    \"topic\": self.topic,\n                    \"stance\": self.stance,\n                    \"existing_claims\": self.claims_so_far[:-1] if self.claims_so_far else [],\n                    \"claim_to_make\": self.plan[len(self.claims_so_far) - 1],\n                }\n            else:\n                return {\n                    \"topic\": self.topic, \"stance\": self.stance, \"number_of_claims\": self.claims_to_generate}\n        else:       # If the conversation is not empty (i.e., there are previous exchanges we need to consider)\n            if self.plan:\n                return {\n                    \"topic\": self.topic,\n                    \"stance\": self.stance,\n                    \"existing_claims\": self.claims_so_far[:-1] if self.claims_so_far else [],\n                    \"claim_to_make\": self.plan[len(self.claims_so_far) - 1],\n                    \"conversation\": self.conversation,\n                }\n            else:\n                return {\n                    \"topic\": self.topic,\n                    \"stance\": self.stance,\n                    \"number_of_claims\": self.claims_to_generate,\n                    \"conversation\": self.conversation,\n                }\n    \n    def state_to_evaluator_input(self) -> typing.Dict[str, str | typing.List[str]]:\n        \"\"\"Converts the state to an input for the evaluator.\"\"\"\n        if len(self.conversation) == 0:\n            if self.claims_so_far:\n                plan_index = len(self.claims_so_far) - 1\n                return {\n                    \"claim_plan\": self.plan[plan_index],\n                    \"new_claim\": self.claims_so_far[-1],\n                    \"claims_so_far\": self.claims_so_far[:-1],\n                    \"topic\": self.topic,\n                }\n            else:\n                return {\"plan\": self.plan, \"topic\": self.topic}\n        else:\n            if self.claims_so_far:\n                plan_index = len(self.claims_so_far) - 1\n                return {\n                    \"claim_plan\": self.plan[plan_index],\n                    \"new_claim\": self.claims_so_far[-1],\n                    \"claims_so_far\": self.claims_so_far[:-1],\n                    \"topic\": self.topic,\n                    \"conversation\": self.conversation,\n                }\n            else:\n                return {\"plan\": self.plan, \"topic\": self.topic, \"conversation\": self.conversation}\n            \nclass PlanAndExecuteNode(Node):\n    \"\"\"Represents a node in a tree that is used for planning and executing an argument in a debate.\"\"\"\n    state: PlanAndExecuteState = Field(\n        ...,\n        description=\"\"\"\nA representation of a node's state in a Tree-of-Thought framework.\nA state contains the topic of the debate, the stance of the debator towards the topic, and conversation so far.\nNotably, this state includes a plan (consisting of a list of claims) that the debator is aiming to make in the debate, \nand the claims the debator has made so far.\n\"\"\",\n    )\n    score: float = Field(\n        default=0,\n        description=\"\"\"\nThe score of the node between 0 and 1. The higher the score, the higher the quality of the reasoning until that node.\nThis applies to both the planning and execution phases.\n\"\"\".strip(),\n    )\n\nclass MCTSPlanAndExecuteNode(PlanAndExecuteNode, MonteCarloNode):\n    \"\"\"Represents a node in a Monte Carlo Tree Search tree that is used for planning and executing an argument in a debate.\"\"\"\n\n\nclass PlanAndExecuteTree(Tree):\n\n    def _plan_to_string(self, plan: typing.List[str]) -> str:\n        \"\"\"\n        Converts a plan to a string.\n        \"\"\"\n        return str(\"\\n\".join(plan))\n\n    def __init__(self, state: State, max_depth: int):\n        self.claims_to_generate = max_depth\n        super().__init__(state)\n\n    def _add_node_to_tree_from_existing_conversation(\n        self, \n        message_index: int, \n        topic: str, \n        stance: str, \n        conversation: typing.List[str],\n    ) -> Node:\n        return Node(\n            index=message_index,\n            state=PlanAndExecuteState(\n                topic=topic,\n                stance=stance,\n                conversation=conversation[:message_index],\n                claims_to_generate=self.claims_to_generate,\n                plan=[],\n                claims_so_far=[],\n            ),\n            parent_id=message_index - 1 if message_index > 0 else None,\n            children_ids=[message_index + 1],\n            score=0,\n        )\n    \n    def _initialize_root(\n        self,\n        state: State,\n    ) -> Node:\n        return Node(\n            index=len(state.conversation),\n            state=PlanAndExecuteState(\n                topic=state.topic,\n                stance=state.stance,\n                conversation=state.conversation,\n                claims_to_generate=self.claims_to_generate,\n                plan=[],\n                claims_so_far=[],\n            ),\n            parent_id=len(state.conversation) - 1 if len(state.conversation) > 0 else None,\n            score=0,\n        )\n    \n    def create_child_node(\n        self,\n        index: int,\n        state: PlanAndExecuteState,\n        output: typing.Union[str, typing.List[str]],\n    ):\n        if isinstance(output, str):  # Execution phase\n            return Node(\n                index=index,\n                state=PlanAndExecuteState(\n                    topic=state.topic,\n                    stance=state.stance,\n                    conversation=state.conversation,\n                    claims_to_generate=state.claims_to_generate,\n                    plan=state.plan,                                    # Plan remains static\n                    claims_so_far=state.claims_so_far + [output],       # Output (claim) is the execution of a plan step\n                ),\n            )\n        else:  # Planning phase\n            return Node(\n                index=index,\n                state=PlanAndExecuteState(\n                    topic=state.topic,\n                    stance=state.stance,\n                    conversation=state.conversation,\n                    claims_to_generate=state.claims_to_generate,\n                    plan=output,                                        # Output (list of claims) is the plan\n                    claims_so_far=state.claims_so_far,                  # Claims are not yet produced\n                ),\n            )\n    \n    def add_child_node(\n        self, \n        parent_node: Node, \n        child_node: Node, \n        response: str, \n        expansion_reasoning: str,\n    ):\n        edge = Edge(\n            source=parent_node.index,\n            target=child_node.index,\n            response=self._plan_to_string(response) if isinstance(response, list) else response,\n            reasoning=expansion_reasoning,\n        )\n        parent_node.children_ids.append(child_node.index)\n        child_node.parent_id = parent_node.index\n        self.nodes.append(child_node)\n        self.add_edge(\n            source_index=parent_node.index,\n            target_index=child_node.index,\n            edge=edge,\n        )\n\n    def log_tree(self, logger: Logger):\n        for node in self.nodes:\n            logger.info(f\"\\n\\nNode {(node.index)}\")\n            if(node.parent_id is not None):\n                if node.state.claims_so_far:\n                    logger.info(f'\\tClaims so far: {node.state.claims_so_far[:-1]}')\n                    logger.info(f'\\tNewest claim: {node.state.claims_so_far[-1]}')\n                else:\n                    logger.info(f'\\tPlan: {node.state.plan}')\n            logger.info(f'\\tScore: {node.score}')\n            logger.info('\\tChildren:')\n            for child_index in node.children_ids:\n                logger.info(\n                    f'\\n\\t- Node #{child_index} [score = {self.nodes[child_index].score}]: '\n                    f'{self.edges[node.index][child_index].response}'\n                )\n                # Print the reasoning associated with the response if it exists\n                if self.edges[node.index][child_index].reasoning:\n                    logger.info(f'\\tReasoning: {self.edges[node.index][child_index].reasoning}')\n    \n\nclass MCTSPlanAndExecuteTree(PlanAndExecuteTree):\n\n    def _add_node_to_tree_from_existing_conversation(\n        self,\n        message_index: int, \n        topic: str, \n        stance: str, \n        conversation: typing.List[str],\n    ) -> Node:\n        return MCTSPlanAndExecuteNode(\n            index=message_index,\n            state=PlanAndExecuteState(\n                topic=topic,\n                stance=stance,\n                conversation=conversation[:message_index],\n                claims_to_generate=self.claims_to_generate,\n                plan=[],\n                claims_so_far=[],\n            ),\n            parent_id=message_index - 1 if message_index > 0 else None,\n            children_ids=[message_index + 1],\n            score=0,\n            visits=0,\n        )\n    \n    def _initialize_root(\n        self,\n        state: State,\n    ) -> Node:\n        return MCTSPlanAndExecuteNode(\n            index=len(state.conversation),\n            state=PlanAndExecuteState(\n                topic=state.topic,\n                stance=state.stance,\n                conversation=state.conversation,\n                claims_to_generate=self.claims_to_generate,\n                plan=[],\n                claims_so_far=[],\n            ),\n            parent_id=len(state.conversation) - 1 if len(state.conversation) > 0 else None,\n            score=0,\n            visits=0,\n        )\n    \n    def create_child_node(\n        self,\n        index: int,\n        state: PlanAndExecuteState,\n        output: typing.Union[str, typing.List[str]],\n    ):\n        if isinstance(output, str):\n            return MCTSPlanAndExecuteNode(\n                index=index,\n                state=PlanAndExecuteState(\n                    topic=state.topic,\n                    stance=state.stance,\n                    conversation=state.conversation,\n                    claims_to_generate=state.claims_to_generate,\n                    plan=state.plan,\n                    claims_so_far=state.claims_so_far + [output],\n                ),\n                visits=0,\n            )\n        else:\n            return MCTSPlanAndExecuteNode(\n                index=index,\n                state=PlanAndExecuteState(\n                    topic=state.topic,\n                    stance=state.stance,\n                    conversation=state.conversation,\n                    claims_to_generate=state.claims_to_generate,\n                    plan=output,\n                    claims_so_far=state.claims_so_far,\n                ),\n                visits=0,\n            )\n"}
{"type": "source_file", "path": "monte_carlo_tree_of_thoughts.py", "content": "import abc\nimport dspy\nimport math\nimport typing\nimport numpy as np\n\nfrom tree_of_thoughts import CombinedMeta\nfrom abstractions.generator.generator import ResponseParameters\nfrom abstractions.tree.tree import (\n    State,\n    MonteCarloNode,\n    Tree,\n)\nfrom utils.search_parameters import MonteCarloTreeOfThoughtParameters\nfrom utils import logger_config\nfrom utils import visualize_tree\n\n\nclass MonteCarloTreeOfThought(dspy.Module, abc.ABC, metaclass=CombinedMeta):\n    \"\"\"\n    A Tree-Of-Thought module that uses the monte carlo algorithm to plan the best response to a given conversation.\n    \"\"\"\n\n    def _initialize_module(self, signature: dspy.Signature) -> dspy.Module:\n        \"\"\"\n        Initialize a function based on the specified signature.\n\n        Parameters:\n            signature (dspy.Signature): The signature of the function to initialize.\n        \n        Returns:\n            dspy.Module: The initialized function.\n        \"\"\"\n        if self.use_chain_of_thoughts:\n            return dspy.TypedChainOfThought(signature=signature)\n        else:\n            return dspy.TypedPredictor(signature=signature)\n    \n    @abc.abstractmethod\n    def _get_evaluator(self, state: State) -> dspy.Module:\n        \"\"\"Retrieve the appropriate evaluator module based on the state.\n        \n        Parameters:\n            state (State): The state of intermediate reasoning/thinking in a given task (towards generating a response).\n        \n        Returns:\n            dspy.Module: The evaluator module.\n        \"\"\"\n        pass\n    \n    @abc.abstractmethod\n    def _get_generator(self, state: State) -> dspy.Module:\n        \"\"\"Retrieve the appropriate generator module based on the state.\n        \n        Parameters:\n            state (State): The state of intermediate reasoning/thinking in a given task (towards generating a response).\n        \n        Returns:\n            dspy.Module: The generator module.\n        \"\"\"\n        pass\n    \n    @abc.abstractmethod\n    def _initialize_tree(\n        self,\n        state: State,\n        depth: int,\n    ) -> Tree:\n        \"\"\"Initialize a tree of thoughts with nodes that represent states of intermediate planning/reasoning.\n        \n        Parameters:\n            state (State): The state of intermediate reasoning/thinking in a given task (towards generating a response).\n            depth (int): The depth of the tree, which corresponds with the number of reasoning steps to execute. Advancing to a \n                new layer in the tree corresponds to taking a new reasoning step.\n        \"\"\"\n        pass\n    \n    def __init__(\n        self, \n        use_chain_of_thoughts: bool = True,\n    ) -> None:\n        \"\"\"\n        Initializes the Monte Carlo Tree of Thoughts module.\n\n        Perform Monte Carlo Tree Search to plan/reason over the best response to a given conversation/query.\n        Uses evaluation strategy `score` since MCTS uses rollout with branching factor 1.\n        Uses UCB for node selection strategy, since that is the primary (most studied) exploration/exploitation method for MCTS.\n        \n        Parameters:\n            use_chain_of_thoughts (bool): Whether to use the chain of thoughts or not.\n        \"\"\"\n        super().__init__()\n        self.use_chain_of_thoughts = use_chain_of_thoughts\n        self.logger = logger_config.setup_logger(folder_path='./logs')\n      \n    \n    def forward(\n        self, \n        state: State, \n        response_parameters: ResponseParameters,\n        mcts_parameters: MonteCarloTreeOfThoughtParameters,\n        do_visualize_tree: bool = False,\n        do_save_tree: bool = False,\n        verbose: bool = False,\n    ) -> str:\n        \"\"\"\n        Utilize Tree-Of-Thought to plan the best response to a given conversation.\n\n        Parameters:\n            state (State): The state of intermediate reasoning/thinking in a given task (towards generating a response).\n            response_parameters (ResponseParameters): Parameters which define the nature of the response to generate (e.g., \n                length, tone, and language style).\n            mcts_parameters (MonteCarloTreeOfThoughtParameters): Parameters which define the behavior of the search over \n                reasoning steps with MCTS Tree-Of-Thought (e.g., generation_temperature, rollout_depth, etc...).\n            do_visualize_tree (bool): Whether to visualize the tree.\n            do_save_tree (bool): Whether to save the tree.\n            verbose (bool): Whether to print verbose output.\n        \n        Returns:\n            str: The best response to the conversation. Derived from reasoning steps explored by the MCTS algorithm.\n        \"\"\"\n        tree = self._initialize_tree(state=state, depth=mcts_parameters.rollout_depth)\n        for step in range(mcts_parameters.monte_carlo_iterations):\n            self.logger.info(f'Performing Monte Carlo step {step+1} of {mcts_parameters.monte_carlo_iterations}')\n            self.monte_carlo_step(tree.root, tree, step+1, mcts_parameters, response_parameters)\n        response: str = self.generate_response(tree, mcts_parameters, response_parameters)\n        if verbose:\n            tree.log_tree(logger=self.logger)\n        if do_save_tree:\n            tree.save_to_file()\n        if do_visualize_tree:\n            visualize_tree.draw_graph(graph=visualize_tree.tree_to_graph(tree))\n        return response\n    \n    def monte_carlo_step(\n        self, \n        node: MonteCarloNode,\n        tree: Tree,\n        total_visits: int,\n        mcts_parameters: MonteCarloTreeOfThoughtParameters,\n        response_parameters: ResponseParameters,\n    ) -> float:\n        \"\"\"\n        Recursively computes the scores for each node in the tree using the minimax algorithm.\n        \n        Parameters:\n            node (MonteCarloNode): The node to compute the score for.\n            tree (Tree): the tree.\n            total_visits (int): The total number of visits to all nodes in the tree.\n            mcts_parameters (MonteCarloTreeOfThoughtParameters): Parameters which define the behavior of the search over \n                reasoning steps with MCTS Tree-Of-Thought.\n            rresponse_parameters (ResponseParameters): Parameters which define the nature of the response to generate (e.g., \n                length, tone, and language style).\n\n        Returns:\n            float: The score of the node.\n        \"\"\"\n        self.logger.info(f'Performing Monte Carlo step for node {node.index}')\n        # Base case: leaf node\n        if not node.children_ids:\n            if node.visits == 0:\n                # Perform a rollout to estimate the score without creating new nodes in the tree.\n                value: float = self.rollout(node, tree, mcts_parameters, response_parameters)\n                self.logger.info(f'\\tRollout score for node {node.index} is {value}')\n                return value\n            else:\n                # Given a node with a score (obtained from a rollout), we choose to expand this node.\n                self.expand_node(node, tree, mcts_parameters, response_parameters)\n                child_node: MonteCarloNode = self.choose_best_child(node, tree, total_visits)\n                value: float = self.monte_carlo_step(child_node, tree, total_visits, mcts_parameters, response_parameters)\n                self.logger.info(f'\\tExpanded node {node.index}, which recieved score {value}')\n        else:\n            child_node: MonteCarloNode = self.choose_best_child(node, tree, total_visits)\n            value: float = self.monte_carlo_step(child_node, tree, total_visits, mcts_parameters, response_parameters)\n            self.logger.info(f'\\tChose child node {node.index} with score {value}')\n        node.visits += 1\n        node.score = value if node.score == 0 else (node.score + value).round(decimals=2)\n        self.logger.info(f'\\tUpdated score for node {node.index} (with {node.visits} visits) to {node.score}')\n        return value\n\n    def rollout(\n        self, \n        node: MonteCarloNode,\n        tree: Tree,\n        mcts_parameters: MonteCarloTreeOfThoughtParameters,\n        response_parameters: ResponseParameters,\n    ) -> float:\n        \"\"\"\n        Estimates the score of a node using a rollout strategy.\n\n        Parameters:\n            node (MonteCarloNode): The node to compute the score to.\n            tree (Tree): The search tree.\n            mcts_parameters (MonteCarloTreeOfThoughtParameters): Parameters which define the behavior of the search over \n                reasoning steps with MCTS Tree-Of-Thought.\n            response_parameters (ResponseParameters): Parameters which define the nature of the response to generate (e.g., \n                length, tone, and language style).\n\n        Returns:\n            float: The score of the node.\n        \"\"\"\n        new_node = node\n        for layer in range(mcts_parameters.rollout_depth):\n            # Since we are performing a rollout, we only want a **single** chain of responses. The nodes we create\n            # here are **not** added to the tree.\n            rollout_parameters: MonteCarloTreeOfThoughtParameters = mcts_parameters.model_copy(update={'n_samples_generation': 1})\n            response, _ = self.get_response(new_node.state, rollout_parameters, response_parameters)\n            response: typing.List[str]  # `response` is a list of one element.\n            new_node = tree.create_child_node(index=-layer, state=new_node.state, output=response[0])\n        node.score, node.reasoning = self.get_score(new_node.state, mcts_parameters)\n        node.visits = 1\n        return node.score\n\n    def expand_node(\n        self, \n        node: MonteCarloNode,\n        tree: Tree,\n        mcts_parameters: MonteCarloTreeOfThoughtParameters,\n        response_parameters: ResponseParameters,\n    ) -> typing.List[MonteCarloNode]:\n        \"\"\"\n        Expands a node in the search tree by generating possible augmentations (actions) that lead to child nodes.\n        \n        Parameters:\n            node (Node): The node to expand.\n            tree (Tree): The search tree.\n            mcts_parameters (MonteCarloTreeOfThoughtParameters): Parameters which define the behavior of the search over \n                reasoning steps with MCTS Tree-Of-Thought.\n            response_parameters (ResponseParameters): Parameters which define the nature of the response to generate (e.g., \n                length, tone, and language style).\n            \n        Returns:\n            List[MonteCarloNode]: The child nodes of the expanded node.\n        \"\"\"\n        responses, reasonings = self.get_response(node.state, mcts_parameters, response_parameters)\n        child_nodes = []\n        for response, reasoning in zip(responses, reasonings):\n            child_node: MonteCarloNode = tree.create_child_node(index=len(tree.nodes), state=node.state, output=response)\n            tree.add_child_node(parent_node=node, child_node=child_node, response=response, expansion_reasoning=reasoning)\n            child_nodes.append(child_node)\n        tree.add_layer(child_nodes)\n        return child_nodes\n\n    def get_response(\n        self, \n        state: State,\n        mcts_parameters: MonteCarloTreeOfThoughtParameters,\n        response_parameters: ResponseParameters,\n    ) -> typing.Tuple[typing.List[str], typing.List[str]]:\n        \"\"\"\n        Generates a response to a conversation using the model.\n\n        Parameters:\n            state (State): The state of intermediate reasoning/thinking in a given task (towards generating a response).\n            mcts_parameters (MonteCarloTreeOfThoughtParameters): Parameters which define the behavior of the search over \n                reasoning steps with MCTS Tree-Of-Thought.\n            response_parameters (ResponseParameters): Parameters which define the nature of the response to generate (e.g., \n                length, tone, and language style).\n        Returns:\n            Tuple[List[str], List[str]]: A 2-tuple that consists of a list of candidate responses and a list of reasonings \n                behind those responses.\n        \"\"\"\n        n_samples_generation: int = mcts_parameters.n_samples_generation\n        generation_temperature: float = mcts_parameters.generation_temperature\n        generator: dspy.Module = self._get_generator(state=state)\n        completions = generator(\n            **state.state_to_generator_input(response_parameters=response_parameters),\n            config=dict(n=n_samples_generation, temperature=generation_temperature),\n        ).completions\n        output: typing.List[str] = [response for response in completions.response]\n        return (output, completions.reasoning)\n    \n    def choose_best_child(\n        self, \n        node: MonteCarloNode, \n        tree: Tree,\n        total_visits: int,\n    ) -> MonteCarloNode:\n        \"\"\"\n        Chooses the best child node of a parent node based on the scores of the children.\n\n        Parameters:\n            node (Node): The parent node.\n            tree (Tree): The search tree.\n            total_visits (int): The total number of visits to all nodes in the tree.\n        \n        Returns: \n            Node: The best child node using UCB1.\n        \"\"\"\n        assert len(node.children_ids) > 0, f\"Node {node.index} has no children to choose from.\"\n        children_nodes: typing.List[MonteCarloNode] = [tree.nodes[child_index] for child_index in node.children_ids]\n        child_node_scores: typing.List[float] = []\n        for node in children_nodes:\n            if node.visits == 0:    # If the node is unvisited, set the score to infinity to ensure it is selected\n                current_score = float('inf')\n            else:                   # Use the UCB1 formula to select the best node\n                current_score: float = round(\n                    node.score / node.visits + 2 * math.sqrt(math.log(total_visits) / node.visits), \n                    ndigits=3,      # Round to 3 decimal places (avoid common python floating point errors)\n                )\n            child_node_scores.append(current_score)\n        best_node: MonteCarloNode = children_nodes[np.argmax(child_node_scores)]\n        return best_node\n    \n    @staticmethod\n    def _wrap_judge_reasoning(reasoning: typing.List[str]) -> str:\n        \"\"\"\n        Wrap the reasoning for the votes in an informative string.\n        \n        Parameters:\n            reasoning (List[str]): The reasoning for the votes.\n        \n        Returns:\n            str: The wrapped reasoning for the votes. The reasoning is separated by newlines.\n        \"\"\"\n        result = \"\"\n        for i, reason in enumerate(reasoning):\n            result += f\"Judge #{i + 1} reasoning:\\n{reason}\\n\\n\"\n        return result.strip()\n    \n    def get_score(self, state: State, mcts_parameters: MonteCarloTreeOfThoughtParameters) -> typing.Tuple[float, str]:\n        \"\"\"\n        Gets the score of a conversation using an evaluator.\n\n        Parameters:\n            state (State): The state of intermediate reasoning/thinking in a given task (towards generating a response).\n            mcts_parameters (MonteCarloTreeOfThoughtParameters): Parameters which define the behavior of the search over\n                reasoning steps with MCTS Tree-Of-Thought.\n\n        Returns:\n            Tuple[float, str]: The score of the conversation and the reasoning behind the score.\n        \"\"\"\n        n_samples_judge: int = mcts_parameters.n_samples_judge\n        judge_temperature: float = mcts_parameters.judge_temperature\n        judge: dspy.Module = self._get_evaluator(state=state)\n        completions = judge(\n            **state.state_to_evaluator_input(),\n            config=dict(n=n_samples_judge, temperature=judge_temperature),\n        ).completions\n        score: float = np.mean(completions.score).round(decimals=2)     # Compute average score across `n_samples_judge` judges\n        reasoning: str = self._wrap_judge_reasoning(reasoning=completions.reasoning) if self.use_chain_of_thoughts else \"N/A\"\n        return score, reasoning\n\n\n    @abc.abstractmethod\n    def generate_response(\n        self, \n        tree: Tree,\n        mcts_parameters: MonteCarloTreeOfThoughtParameters,\n        response_parameters: ResponseParameters,\n    ) -> str:\n        \"\"\"\n        Generate an response based on a tree of thoughts (obtained via MCTS).\n\n        Parameters:\n            tree (Tree): The search tree for the best response.\n            mcts_parameters (MonteCarloTreeOfThoughtParameters): Parameters which define the behavior of the search over \n                reasoning steps with MCTS Tree-Of-Thought\n            response_parameters (ResponseParameters): Parameters which define the nature of the response to generate (e.g., \n                length, tone, and language style).\n\n        Returns:\n            str: The optimal response to the conversation.\n        \"\"\"\n        pass\n"}
{"type": "source_file", "path": "plan_and_execute_tree_of_thoughts.py", "content": "import dspy\nimport typing\n\n# Tree-of-thoughts imports\nfrom tree_of_thoughts import TreeOfThoughts\nfrom monte_carlo_tree_of_thoughts import MonteCarloTreeOfThought\nfrom abstractions.tree.tree import (\n    State,\n    Node, \n    Tree, \n    create_conversation_state,\n)\nfrom utils import constants\nfrom utils.utils import set_up_dspy\nfrom utils.flags import parser\nfrom utils.search_parameters import (\n    TreeOfThoughtsParameters, \n    MonteCarloTreeOfThoughtParameters,\n)\n\n# Plan-and-execute imports\nfrom abstractions.tree.plan_and_execute_tree import (\n    PlanAndExecuteState, \n    PlanAndExecuteTree,\n    MCTSPlanAndExecuteTree,\n)\nfrom abstractions.generator.generator import ResponseParameters\nfrom abstractions.generator.plan_and_execute_generator import (\n    SingleTurnPlanningBranchingSignature, \n    SingleTurnPlanExecutionBranchingSignature,\n    MultiTurnPlanningBranchingSignature, \n    MultiTurnPlanExecutionBranchingSignature,\n    SingleTurnPlanAndExecuteResponseSignature,\n)\nfrom abstractions.evaluator.plan_and_execute_evaluator import (\n    SingleTurnScoreWithPlanSignature, \n    SingleTurnScoreWithPlanExecutionSignature,\n    MultiTurnScoreWithPlanSignature, \n    MultiTurnScoreWithPlanExecutionSignature,\n)\n\nclass PlanAndExecuteTreeOfThoughts(TreeOfThoughts):\n    \n    def _initialize_reasoning_modules(self):\n        # Initialize generator modules\n        self.single_turn_first_step_generator = self._initialize_module(signature=SingleTurnPlanningBranchingSignature)\n        self.single_turn_subsequent_step_generator = self._initialize_module(signature=SingleTurnPlanExecutionBranchingSignature)\n        self.multi_turn_first_step_generator = self._initialize_module(signature=MultiTurnPlanningBranchingSignature)\n        self.multi_turn_subsequent_step_generator = self._initialize_module(signature=MultiTurnPlanExecutionBranchingSignature)\n        # Initialize evaluator modules\n        if self.evaluation_strategy == constants.EvaluationStrategy.SCORE.value:\n            self.single_turn_first_step_evaluator = self._initialize_module(signature=SingleTurnScoreWithPlanSignature)\n            self.single_turn_subsequent_step_evaluator = self._initialize_module(signature=SingleTurnScoreWithPlanExecutionSignature)\n            self.multi_turn_first_step_evaluator = self._initialize_module(signature=MultiTurnScoreWithPlanSignature)\n            self.multi_turn_subsequent_step_evaluator = self._initialize_module(signature=MultiTurnScoreWithPlanExecutionSignature)\n        else:\n            raise NotImplementedError(\"Voting is not yet implemented for the plan-and-execute reasoning type.\")\n        \n        self.plan_and_execute_response_generator = self._initialize_module(signature=SingleTurnPlanAndExecuteResponseSignature)\n    \n    def _initialize_tree(\n        self,\n        state: State,\n        depth: int\n    ) -> Tree:\n        \"\"\"Initialize the tree of thoughts. \n\n        The children of the root node correspond with possible plans to execute.\n        Descendents of these children represent executions of individual steps in the plan.\n        \"\"\"\n        return PlanAndExecuteTree(state=state, max_depth=depth)\n    \n\n    def _get_evaluator(self, state: State) -> dspy.Module:\n        \"\"\"Return the appropriate evaluator module based on the state.\"\"\"\n        assert isinstance(state, PlanAndExecuteState), f\"Invalid state type: {type(state)}. Must be of type PlanAndExecuteState.\"\n        if state.claims_so_far:\n            return self.multi_turn_subsequent_step_evaluator if state.conversation else self.single_turn_subsequent_step_evaluator\n        else:\n            return self.multi_turn_first_step_evaluator if state.conversation else self.single_turn_first_step_evaluator\n\n    def _get_generator(self, state: State) -> dspy.Module:\n        \"\"\"Return the appropriate generator module based on the state.\"\"\"\n        assert isinstance(state, PlanAndExecuteState), f\"Invalid state type: {type(state)}. Must be of type PlanAndExecuteState.\"\n        if state.plan:\n            return self.multi_turn_subsequent_step_generator if state.conversation else self.single_turn_subsequent_step_generator\n        else:\n            return self.multi_turn_first_step_generator if state.conversation else self.single_turn_first_step_generator\n        \n    def first_step(\n        self, \n        tree: Tree, \n        tot_parameters: TreeOfThoughtsParameters,\n        response_parameters: ResponseParameters,\n    ) -> typing.List[Node]:\n        \"\"\"Generate a plan for the task.\"\"\"\n        self.logger.info(f\"Generating a plan with {tot_parameters.depth} steps.\")\n        return TreeOfThoughts.step(\n            self,\n            tree=tree,\n            frontier=[tree.root],\n            tot_parameters=tot_parameters,\n            response_parameters=response_parameters,\n        )\n    \n    def final_step(self, tree: Tree, tot_parameters: TreeOfThoughtsParameters, response_parameters: ResponseParameters) -> str:\n        \"\"\"Return the final response from the tree of thoughts.\"\"\"\n        # We select the best leaf node, which is the node with the highest scoring plan + execution.\n        frontier = sorted(tree.layers[-1], key=lambda child_node: child_node.score, reverse=True)\n        self.logger.info(f\"Using the draft from node #{frontier[0].index} as the final response.\")\n        best_node: Node = frontier[0]\n        # We generate a final response that is based on the plan and it's execution, and which matches\n        # the desired response length.\n        response = self.plan_and_execute_response_generator(\n            topic=best_node.state.topic,\n            stance=best_node.state.stance,\n            plan=best_node.state.plan,\n            plan_execution=best_node.state.claims_so_far,\n            response_parameters=response_parameters, \n        )\n        return response.response\n\n\nclass PlanAndExecuteMCTSTreeOfThoughts(PlanAndExecuteTreeOfThoughts, MonteCarloTreeOfThought):\n\n    def _initialize_tree(self, state: State, depth: int) -> Tree:\n        return MCTSPlanAndExecuteTree(state=state, max_depth=depth)\n\n    def __init__(\n        self, \n        use_chain_of_thought: bool = False, \n        node_selection_strategy: str = constants.NodeSelectionStrategy.GREEDY.value, \n        evaluation_strategy: str = constants.EvaluationStrategy.SCORE.value,\n    ):\n        PlanAndExecuteTreeOfThoughts.__init__(self, use_chain_of_thought, node_selection_strategy, evaluation_strategy)\n    \n    def forward(\n        self, \n        state: State, \n        mcts_parameters: MonteCarloTreeOfThoughtParameters,\n        response_parameters: ResponseParameters, \n        do_visualize_tree: bool = False, \n        do_save_tree: bool = False, \n        verbose: bool = False,\n    ) -> str:\n        return MonteCarloTreeOfThought.forward(\n            self, \n            state=state, \n            mcts_parameters=mcts_parameters,\n            do_visualize_tree=do_visualize_tree, \n            do_save_tree=do_save_tree,\n            verbose=verbose,\n            response_parameters=response_parameters, \n        )\n    \n    def rollout(\n        self, \n        node: Node, \n        tree: Tree, \n        mcst_parameters: MonteCarloTreeOfThoughtParameters,\n        response_parameters: ResponseParameters,\n    ) -> float:\n        \"\"\"\n        Perform a rollout from the given node. The rollout consists of generating a sequence of reasoning steps\n        (i.e, coming up with a plan and executing it) until we reach the maximum depth of the rollout or have finished\n        executing all steps in the plan.\n        \"\"\"\n        new_node = node\n        # Rollout should not result in performing more execution steps than there are steps in the plan.\n        rollout_depth: int = min(mcst_parameters.rollout_depth, len(node.state.plan) - len(node.state.claims_so_far))\n        for layer in range(rollout_depth):\n            # Since we are performing a rollout, we only want a **single** chain of responses. The nodes we create\n            # here are **not** added to the tree.\n            rollout_parameters = mcst_parameters.model_copy(update={\"n_samples_generation\": 1})\n            response, _ = self.get_response(\n                state=new_node.state, \n                mcts_parameters=rollout_parameters,\n                response_parameters=response_parameters,\n            )\n            response: typing.List[str]  # `response` is a list of one element.\n            new_node = tree.create_child_node(index=-layer, state=new_node.state, output=response[0])\n        node.score, node.reasoning = self.get_score(\n            state=new_node.state, mcts_parameters=mcst_parameters,\n        )\n        node.visits = 1\n        return node.score\n\n    def generate_response(self, tree: Tree, mcts_parameters: MonteCarloTreeOfThoughtParameters, response_parameters: ResponseParameters) -> str:\n        \"\"\"\n        We select the best leaf node, which is the node with the highest scoring plan + execution.\n        From there, we generate a final response that is based on the plan and it's execution, and which matches\n        the desired response length.\n        \"\"\"\n        tot_parameters = TreeOfThoughtsParameters(\n            top_k=1,\n            n_samples_judge=mcts_parameters.n_samples_judge,\n            n_samples_generation=1,\n            judge_temperature=mcts_parameters.judge_temperature,\n            generation_temperature=mcts_parameters.generation_temperature,\n            depth=1,\n        )\n        return PlanAndExecuteTreeOfThoughts.final_step(\n            self, \n            tree=tree, \n            tot_parameters=tot_parameters,\n            response_parameters=response_parameters,\n        )\n\n\nif __name__ == \"__main__\":\n\n    # Parse the command-line arguments    \n    args = parser.parse_args()\n    set_up_dspy(\n        openai_key_path=args.openai_key_path,\n        model_name=args.model_name,\n        max_tokens=args.max_tokens,\n    )\n\n    # Initialize the conversation state\n    conversation_state = create_conversation_state(\n        topic=args.topic,\n        stance=args.stance,\n        conversation_path=args.conversation_path,\n    )\n\n    print('Arguments:')\n    for arg in vars(args):\n        print(f'\\t{arg}: {getattr(args, arg)}')\n    print(f'Initial conversation state:\\n{conversation_state}')\n    \n    # Initialize the tree of thoughts\n    if args.search_strategy == constants.SearchAlgorithm.MONTE_CARLO_TREE_SEARCH.value:\n        tree_of_thoughts = PlanAndExecuteMCTSTreeOfThoughts(\n            use_chain_of_thought=args.use_chain_of_thought,\n            node_selection_strategy=args.node_selection_strategy,\n            evaluation_strategy=args.evaluation_strategy,\n        )\n        response = tree_of_thoughts(\n            state=conversation_state,\n            mcts_parameters=MonteCarloTreeOfThoughtParameters(\n                monte_carlo_iterations=args.mcts_iterations,\n                rollout_depth=args.depth,\n                generation_temperature=args.generation_temperature,\n                judge_temperature=args.judge_temperature,\n                n_samples_generation=args.n_samples_generation,\n                n_samples_judge=args.n_samples_judge,\n            ),\n            do_visualize_tree=args.with_visualization,\n            do_save_tree=args.save_tree,\n            response_parameters=ResponseParameters(response_length=args.response_length, communication_tone=args.communication_tone, language_style=args.language_style),\n        )\n    else:\n        tree_of_thoughts = PlanAndExecuteTreeOfThoughts(\n            use_chain_of_thought=args.use_chain_of_thought,\n            node_selection_strategy=args.node_selection_strategy,\n            evaluation_strategy=args.evaluation_strategy,\n            do_pruning=args.do_pruning,\n        )\n        response = tree_of_thoughts(\n            state=conversation_state,\n            tot_parameters=TreeOfThoughtsParameters(\n                depth=args.depth,\n                top_k=args.top_k,\n                generation_temperature=args.generation_temperature,\n                judge_temperature=args.judge_temperature,\n                n_samples_generation=args.n_samples_generation,\n                n_samples_judge=args.n_samples_judge,\n            ),\n            do_visualize_tree=args.with_visualization,\n            do_save_tree=args.save_tree,\n            response_parameters=ResponseParameters(response_length=args.response_length, communication_tone=args.communication_tone, language_style=args.language_style),\n        )\n    print(f'Final response:\\n{response}')\n"}
{"type": "source_file", "path": "utils/logger_config.py", "content": "import logging\nimport os\nimport datetime\n\n\ndef get_current_time():\n    \"\"\"\n    get_current_time() -> str\n    \"\"\"\n    return datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\ndef setup_logger(folder_path:str=None):\n    \"\"\"\n    setup_logger(folder_path=None) -> logging.Logger\n    This function sets up the logger for the application.\n    \"\"\"\n    # Remove all existing handlers\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    \n    # Set the root logger level to INFO\n    logging.root.setLevel(logging.INFO)\n    \n    # Suppress lower-level logs from specific libraries\n    logging.getLogger('httpcore').setLevel(logging.CRITICAL)\n    logging.getLogger('requests').setLevel(logging.CRITICAL)\n    logging.getLogger('httpx').setLevel(logging.CRITICAL)\n    \n    # Ensure the log directory exists\n    log_directory = folder_path if folder_path else 'run_logs'\n    if not os.path.exists(log_directory):\n        os.makedirs(log_directory)\n    \n    # Create file handler\n    file_handler = logging.FileHandler(f'{log_directory}/{get_current_time()}.log')\n    file_handler.setLevel(logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.INFO)\n    \n    # Define formatter\n    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Get the root logger and add handlers\n    logger = logging.getLogger()\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    # **Suppress LiteLLM's INFO logs**\n    lite_llm_logger = logging.getLogger('LiteLLM')\n    lite_llm_logger.setLevel(logging.WARNING)  # Change to WARNING or higher as needed\n    \n    return logger"}
{"type": "source_file", "path": "utils/flags.py", "content": "import argparse\nimport utils.constants as constants\n\nparser = argparse.ArgumentParser(\n    description='Debate using Minimax with Tree-Of-Thought',\n)\n\nparser.add_argument(\n    '--topic',\n    type=str,\n    default=\"The United States Government should adopt widespread use of street cameras to monitor public spaces.\",\n    help='The topic of the debate.',\n)\n\nparser.add_argument(\n    '--stance',\n    type=str,\n    default=\"PRO\",\n    choices=[\"PRO\", \"ANTI\"],\n    help='The stance of the debator.',\n)\n\nparser.add_argument(\n    '--conversation_path',\n    type=str,\n    default=\"data/conversations/street_cameras/example_0.txt\",\n    help='The path to the conversation file. Each line in the file represents a message in the conversation.',\n)\n\nparser.add_argument(\n    '--reasoning_type',\n    type=str,\n    default=\"iterative_drafting\",\n    choices=[\"iterative_drafting\", \"plan_and_execute\", \"additive\", \"devils_advocate\"],\n    help='The type of reasoning to use. One of (iterative_drafting, plan_and_execute, additive, devils_advocate)',\n)\n\nparser.add_argument(\n    '--search_strategy',\n    type=str,\n    default=\"beam_search\",  # Equivalent to \"BFS\" from the original Tree-Of-Thoughts implementation\n    choices=[\"beam_search\", \"monte_carlo\"],\n    help='The search strategy to use. One of (beam_search, monte_carlo)',\n)\n\nparser.add_argument(\n    '--do_pruning',\n    default=False,\n    action='store_true',\n    help='Whether to prune the tree of thought during intermediate reasoning steps.',\n)\n\nparser.add_argument(\n    '--depth',\n    type=int,\n    default=2,\n    help='The depth of the tree (i.e., the maximum number of turns in the debate).',\n)\n\nparser.add_argument(\n    '--mcts_iterations',\n    type=int,\n    default=10,\n    help='The number of MCTS iterations to run.',\n)\n\nparser.add_argument(\n    '--model_name',\n    type=str,\n    choices=[\"gpt-4o-mini\", \"gpt-4o\"],\n    default=\"gpt-4o-mini\",\n    help='The name of the language model to use.',\n)\n\nparser.add_argument(\n    '--max_tokens',\n    type=int,\n    default=2000,\n    help='The maximum number of tokens to use when generating responses.',\n)\n\nparser.add_argument(\n    '--openai_key_path',\n    type=str,\n    default='openai_key.txt',\n    help='The path to the OpenAI key file.',\n)\n\nparser.add_argument(\n    '--with_visualization',\n    action='store_true',\n    help='Whether to visualize the tree of thought.',\n    default=False,\n)\n\nparser.add_argument(\n    '--save_tree',\n    action='store_true',\n    help='Whether to save the tree of thought.',\n    default=False,\n)\n\nparser.add_argument(\n    '--use_chain_of_thought',\n    default=True,\n    action='store_true',\n    help='Whether to use the chain of thought.',\n)\n\nparser.add_argument(\n    '--top_k',\n    type=int,\n    default=2,\n    help='The number of top-k arguments to consider in each expansion step of Tree-of-Thoughts.',\n)\n\nparser.add_argument(\n    '--n_samples_generation',\n    type=int,\n    default=3,\n    help='The number of samples to generate for each response.',\n)\n\nparser.add_argument(\n    '--n_samples_judge',\n    type=int,\n    default=5,\n    help='The number of samples to generate for each judgment of a persuasive argument.',\n)\n#TODO use enums\nparser.add_argument(\n    '--communication_tone',\n    type=str,\n    default='logical' ,\n    help=\"The tone that the argument should take. One of 'logical', 'sarcastic', 'aggressive'.\",\n)\n\nparser.add_argument(\n    '--language_style',\n    type=str,\n    default='casual' ,\n    help='How formal the argument should be slang < casual < formal.',\n)\n\nparser.add_argument(\n    '--generation_temperature',\n    type=float,\n    default=0.7,\n    help='The temperature to use when generating responses.',\n)\n\nparser.add_argument(\n    '--response_length',\n    type=str,\n    default=\"a few sentences\",\n    help='The length of the generated argument.',\n)\n\nparser.add_argument(\n    '--judge_temperature',\n    type=float,\n    default=0.7,\n    help='The temperature to use when judging responses.',\n)\n\nparser.add_argument(\n    '--node_selection_strategy',\n    type=str,\n    default=constants.NodeSelectionStrategy.GREEDY.value,\n    help='The manner in which we select nodes to expand (or prune) in Tree-of-Thoughts.',\n)\n\nparser.add_argument(\n    '--evaluation_strategy',\n    type=str,\n    default=constants.EvaluationStrategy.SCORE.value,\n    help='The strategy for evaluating the quality of a response.',\n)\n\n"}
{"type": "source_file", "path": "utils/visualize_tree.py", "content": "import os\nimport typing\nimport networkx as nx\nfrom networkx.drawing.nx_pydot import graphviz_layout\nfrom matplotlib import pyplot as plt\nimport textwrap\n\nfrom abstractions.tree.tree import Edge, Node, Tree\n\nBLUE = 'blue'\nRED = 'red'\n\ndef wrap_text(\n        text: str, \n        width: int = 30, \n        max_words:int = 8\n    ) -> str:\n    \"\"\"\n    Wraps text to a specified width and maximum number of words.\n\n    Parameters:\n        text (str): The text to wrap.\n        width (int): The width to wrap to.\n        max_words (int): The maximum number of words to wrap to.\n\n    Returns:\n        str: The wrapped text.\n    \"\"\"\n    # Replace colons with semi-colons. This is necessary to avoid an issue with graphviz.\n    text = text.replace(\":\", \";\")\n\n    words = text.replace(\"\\n\", \" \").split()[:max_words]\n    return \"\\n\".join(textwrap.wrap(\" \".join(words), width=width)) + \"...\"\n\ndef tree_to_graph(\n        tree: Tree,\n    ) -> nx.DiGraph:\n    \"\"\"\n    Converts a tree to a directed graph.\n\n    Parameters:\n        tree (Tree): The tree to convert.\n\n    Returns:\n        nx.DiGraph: The directed graph representation of the tree.\n    \"\"\"\n    graph = nx.DiGraph()\n\n    def add_node(\n            node: Node,\n            root_score: float,\n            node_size: int = 300,\n        ):\n        \"\"\"\n        Adds a node to the directed graph.\n\n        Parameters:\n            node (Node): The node to add.\n            root_score (float): The score of the root node.\n            node_size (int): The base size of the node. The size will be scaled by the node's score.\n        \"\"\"\n        node_color = BLUE if node.state.stance == 'PRO' else RED\n\n        # If the node's score is None, it preceded the root node and should be assigned the root\n        # node's score.\n        if node.score is None or node.score == float('-inf'):  \n            node.score = root_score\n\n        node_size += 1000 * node.score  if node.score else 300 # Scale node size by score. Default to size 300 if score is None.\n\n        graph.add_node(\n            node_for_adding=node.index, \n            color=node_color,\n            size=node_size,\n            label=node.score,\n        )\n    \n    def add_edge(\n            source: int,\n            target: int,\n            edge: Edge,\n        ):\n        \"\"\"\n        Adds an edge to the directed graph.\n\n        Parameters:\n            edge (Edge): The edge to add.\n        \"\"\"\n\n        # Determine the color of the edge based on the stance of the source node\n        color = BLUE if tree.nodes[source].state.stance == 'PRO' else RED\n\n        graph.add_edge(\n            u_of_edge=source,\n            v_of_edge=target,\n            label=wrap_text(edge.response, width=30),\n            color=color,\n        )\n\n    for node in tree.nodes:\n        add_node(node, root_score=tree.root.score)\n    \n    for source, target_edges in tree.edges.items():\n        for target, edge in target_edges.items():\n            add_edge(source, target, edge)\n\n    return graph\n    \n\ndef draw_graph(\n        graph: nx.DiGraph,\n        name: str = None,\n    ):\n    \"\"\"\n    Draws a directed graph.\n\n    Parameters:\n        graph (nx.DiGraph): The directed graph to draw.\n        name: The name of the file to save the graph to.\n    \"\"\"\n    pos = graphviz_layout(graph, prog='dot')\n    plt.figure(\n        figsize=(30, 20)\n    )\n    nx.draw(\n        graph, \n        pos, \n        with_labels=True, \n        labels=nx.get_node_attributes(graph, 'label') , \n        node_size=[data['size'] for _, data in graph.nodes(data=True)], \n        node_color=[data['color'] for _, data in graph.nodes(data=True)], \n        edge_color=[data[2]['color'] for data in graph.edges(data=True)],\n        font_color='white',\n        font_size=12, \n        arrows=True,\n        arrowsize=10,\n    )\n    nx.draw_networkx_edge_labels(\n        graph, \n        pos, \n        edge_labels=nx.get_edge_attributes(graph, 'label'),\n        font_size=6,\n        label_pos=0.5,\n    )    \n    if name is not None:\n        if not os.path.exists('outputs/graphs'):\n            os.mkdir('outputs/graphs')\n        plt.savefig(f'outputs/graphs/{name}.png', format='png', dpi=300)\n    \n    plt.title(\"Tree-of-Thought Visualization\")\n    plt.show()\n"}
