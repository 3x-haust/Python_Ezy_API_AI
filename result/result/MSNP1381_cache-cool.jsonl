{"repo_info": {"repo_name": "cache-cool", "repo_owner": "MSNP1381", "repo_url": "https://github.com/MSNP1381/cache-cool"}}
{"type": "test_file", "path": "app/tests/__init__.py", "content": ""}
{"type": "test_file", "path": "app/tests/test_api.py", "content": ""}
{"type": "test_file", "path": "app/tests/test_services.py", "content": ""}
{"type": "source_file", "path": "app/__init__.py", "content": ""}
{"type": "source_file", "path": "app/api/__init__.py", "content": ""}
{"type": "source_file", "path": "app/core/__init__.py", "content": ""}
{"type": "source_file", "path": "app/core/config.py", "content": "import os\nfrom pydantic import BaseModel, Field\nfrom pydantic_settings import BaseSettings\nimport yaml\nfrom typing import Dict, List, Optional\n\n\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\nfrom pydantic import HttpUrl, Field\nfrom typing import List, Dict, Optional\n\nclass LLMSchema(BaseModel):\n    endpoint: HttpUrl\n    headers: List[str]\n    temperature_threshold: float\n\nclass MongoDBConfig(BaseModel):\n    uri: str\n    db_name: str\n    collection_name: str\n\nclass RedisConfig(BaseModel):\n    enabled: bool\n    host: str\n    port: int\n    db: int\n\nclass Configs(BaseModel):\n    llm_schemas: Dict[str, LLMSchema]\n    mongodb: MongoDBConfig\n    json_cache_file: str\n    redis: RedisConfig\n    current_llm_service: str\n    use_json_cache: bool\n    use_mongo_cache: bool\n\n    @classmethod\n    def load_from_yaml(cls, yaml_file: str):\n        with open(yaml_file, \"r\") as f:\n            config_data = yaml.safe_load(f)        \n        return cls(**config_data)\n    \n    # class Config(SettingsConfigDict):\n    #     env_file = \".env\"  # Specify an environment file if needed\n    #     env_file_encoding = 'utf-8'\n    #     case_sensitive = True\n    \n\n\nsettings = Configs.load_from_yaml(\"config.yaml\")\n"}
{"type": "source_file", "path": "app/api/routes.py", "content": "import json\nfrom typing import Dict\nfrom fastapi import APIRouter, HTTPException, Request\nfrom app.core.config import Configs, settings\nfrom app.services.llm_service import LLMService\nfrom app.services.cache_service import CacheService\nfrom app.api.models import ChatCompletionRequest, ConfigurationUpdate\n\nrouter = APIRouter()\n\n\n@router.post(\"/{schema_name}/chat/completions\")\nasync def chat_completions(schema_name: str,request: Request):\n    cache_service = CacheService()\n    llm_service = LLMService.check_type(schema_name)\n    request_body_json: Dict = await request.json()\n    \n    print(request_body_json)\n    chat_request=ChatCompletionRequest(**request_body_json)\n    # Access headers\n    headers = request.headers\n    api_key=headers.get(\"Authorization\")\n    if not api_key:\n        raise HTTPException(status_code=401, detail=\"no key!?\")\n\n    # Check cache if temperature is below threshold\n    if chat_request.temperature <= llm_service.get_temperature_threshold():\n        cached_response = await cache_service.get(json.dumps(request_body_json))\n        if cached_response:\n            return cached_response\n    # Make API call\n    response = await llm_service.generate_response(chat_request,api_key)\n\n    # Cache response if temperature is below threshold\n    if chat_request.temperature <= llm_service.get_temperature_threshold():\n        await cache_service.set(chat_request.model_dump_json(), response)\n\n    return response\n\n@router.get(\"/configure\")\nasync def get_configuration():\n    return settings\n\n\n@router.put(\"/configure\")\nasync def update_configuration(config: Configs):\n    global settings  # Declare settings as global to modify the global settings object\n    \n    try:\n        # Update settings using the `copy(update=...)` method\n        updated_settings = settings.copy(update=config.dict(exclude_unset=True))\n        \n        # Apply the updated settings\n        settings = updated_settings\n    except Exception as e:\n        # Return an error response if updating fails\n        raise HTTPException(status_code=400, detail=f'Error in input validation: {e}')\n    \n    # Reinitialize services if certain keys are updated\n    update_keys = config.dict(exclude_unset=True).keys()\n    \n    # Check if LLM service needs to be reinitialized\n    if 'current_llm_service' in update_keys:\n        LLMService().__init__()  # Reinitialize LLM service\n    \n    # Check if any cache-related settings were updated\n    cache_related_keys = {'use_json_cache', 'use_mongo_cache', 'redis'}\n    if cache_related_keys.intersection(update_keys):\n        CacheService().__init__()  # Reinitialize Cache service\n    \n    return {\"message\": \"Configuration updated successfully\"}"}
{"type": "source_file", "path": "app/api/models.py", "content": "from pydantic import BaseModel, HttpUrl, Field, model_validator\nfrom typing import Any, List, Optional, Dict\n\nclass Message(BaseModel):\n    role: Optional[str]\n    content: Optional[str]\n    message:Optional[ str]\n    \n    @model_validator(mode='before')\n    @classmethod\n    def validate_input(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n        # Check if the input is a simple string\n        if isinstance(values, str):\n            values = {\"role\": \"user\", \"content\": values, \"message\": values}\n        elif isinstance(values, dict):\n            # Ensure required fields are present\n            if 'content' not in values:\n                raise ValueError(\"Field 'content' is required.\")\n            if 'role' not in values:\n                values['role'] = \"user\"  # Default role if not provided\n            values['message'] = values.get('message', values['content'])  # Default message\n            \n        return values\n\nclass ChatCompletionRequest(BaseModel):\n    model: str\n    messages: List[Message]\n    temperature: float = Field(default=1.0, ge=0.0, le=2.0)\n    max_tokens: Optional[int] = None\n    n:Optional[int]=None \n    stop:Optional[str]=\"\"\n\nclass LLMSchemaUpdate(BaseModel):\n    endpoint: Optional[str] = None\n    headers: Optional[List[str]] = None\n    temperature_threshold: Optional[float] = None\n\n\nclass LLMSchema(BaseModel):\n    endpoint: HttpUrl\n    headers: List[str]\n    temperature_threshold: float\n\nclass MongoDBConfig(BaseModel):\n    uri: str\n    db_name: str\n    collection_name: str\n\nclass RedisConfig(BaseModel):\n    enabled: bool\n    host: str\n    port: int\n    db: int\n\nclass ConfigurationUpdate(BaseModel):\n    llm_schemas: Dict[str, LLMSchema]\n    mongodb: MongoDBConfig\n    json_cache_file: str\n    redis: RedisConfig\n    current_llm_service: str\n    use_json_cache: bool\n    use_mongo_cache: bool\n"}
{"type": "source_file", "path": "app/main.py", "content": "from fastapi import FastAPI\nfrom app.api.routes import router as api_router\nfrom app.core.config import settings\n\napp = FastAPI(title=\"cache-cool\", version=\"0.1.0\")\n\napp.include_router(api_router\n                #    , prefix=\"/v1\"\n                   )\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"ok\"}\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(\"app.main:app\", host=\"0.0.0.0\", port=8000, reload=True)"}
{"type": "source_file", "path": "app/core/security.py", "content": ""}
{"type": "source_file", "path": "app/services/cache_service.py", "content": "import json\nfrom pymongo import MongoClient\nimport redis\nfrom app.core.config import settings\n\nclass CacheService:\n    def __init__(self):\n        self.use_json_cache = settings.use_json_cache\n        self.use_mongo_cache = settings.use_mongo_cache\n        \n        if self.use_mongo_cache:\n            self.mongo_client = MongoClient(settings.mongodb.uri)\n            self.mongo_db = self.mongo_client[settings.mongodb.db_name]\n            self.mongo_collection = self.mongo_db[settings.mongodb.collection_name]\n        \n        if settings.redis.enabled:\n            self.redis_client = redis.Redis(\n                host=settings.redis.host,\n                port=settings.redis.port,\n                db=settings.redis.db\n            )\n\n    async def get(self, key: str):\n        if settings.redis.enabled:\n            redis_result = self.redis_client.get(key)\n            if redis_result:\n                return json.loads(redis_result)\n        \n        if self.use_mongo_cache:\n            mongo_result = self.mongo_collection.find_one({\"_id\": key})\n            if mongo_result:\n                return mongo_result[\"response\"]\n        \n        if self.use_json_cache:\n            try:\n                with open(settings.json_cache_file, 'r') as f:\n                    cache = json.load(f)\n                    return cache.get(key)\n            except (FileNotFoundError, json.JSONDecodeError):\n                pass\n        \n        return None\n\n    async def set(self, key: str, value: str, expire: int = 3600):\n        if settings.redis.enabled:\n            self.redis_client.setex(key, expire, json.dumps(value))\n        \n        if self.use_mongo_cache:\n            self.mongo_collection.update_one(\n                {\"_id\": key},\n                {\"$set\": {\"response\": value}},\n                upsert=True\n            )\n        \n        if self.use_json_cache:\n            try:\n                with open(settings.json_cache_file, 'r') as f:\n                    cache = json.load(f)\n            except (FileNotFoundError, json.JSONDecodeError):\n                cache = {}\n            \n            cache[key] = value\n            \n            with open(settings.json_cache_file, 'w') as f:\n                json.dump(cache, f)\n\n    async def delete(self, key: str):\n        if settings.redis.enabled:\n            self.redis_client.delete(key)\n        \n        if self.use_mongo_cache:\n            self.mongo_collection.delete_one({\"_id\": key})\n        \n        if self.use_json_cache:\n            try:\n                with open(settings.json_cache_file, 'r') as f:\n                    cache = json.load(f)\n                \n                if key in cache:\n                    del cache[key]\n                \n                with open(settings.json_cache_file, 'w') as f:\n                    json.dump(cache, f)\n            except (FileNotFoundError, json.JSONDecodeError):\n                pass"}
{"type": "source_file", "path": "app/services/llm_service.py", "content": "import requests\nfrom fastapi.datastructures import Headers\nimport httpx\nfrom app.core.config import settings\nfrom app.api.models import ChatCompletionRequest\n\nclass LLMService:\n    def __init__(self,service:str=None):\n        self.current_service = service or settings.current_llm_service\n        self.schema = settings.llm_schemas[self.current_service]\n\n    async def generate_response(self, request: ChatCompletionRequest,api_key:str):\n        request_json=request.model_dump_json()\n\n        headers = {header.split(': ')[0]: header.split(': ')[1].format(api_key=api_key)\n                for header in self.schema.headers}\n        print(headers)\n        async with httpx.AsyncClient() as client:\n            response = await client.post(\n                str(self.schema.endpoint),\n                headers=headers,\n                content=request_json\n            )\n        print([i for i in response.iter_text()])\n        response.raise_for_status()\n        return response.json()\n\n    def get_temperature_threshold(self):\n        return self.schema.temperature_threshold\n    @classmethod\n    def check_type(cls, schema_name):\n        if schema_name not in settings.llm_schemas:\n            return None\n        else: return cls(schema_name)"}
{"type": "source_file", "path": "app/services/__init__.py", "content": ""}
