{"repo_info": {"repo_name": "CityBench", "repo_owner": "tsinghua-fib-lab", "repo_url": "https://github.com/tsinghua-fib-lab/CityBench"}}
{"type": "source_file", "path": "citybench/geoqa/data_gen.py", "content": "import argparse\nimport asyncio\nimport os\nimport random\nimport shapely\nimport signal\nfrom itertools import chain\nimport numpy as np\nimport pandas as pd\nfrom shapely import Polygon, Point\nfrom geopy.distance import geodesic\n\nfrom pycityproto.city.geo.v2.geo_pb2 import AoiPosition, Position, XYPosition, LongLatPosition\n\nfrom .utils import task_files_adaption, gen_options, save_data, get_landuse_dict, get_category_supported\nfrom citysim.player import Player \nfrom global_utils import load_map\nfrom config import MONGODB_URI, MAP_DICT, RESOURCE_PATH, MAP_CACHE_PATH, ROUTING_PATH, CITY_BOUNDARY, GEOQA_TASK_MAPPING_v2\n\ndef generate_evaluation_task_road(map, poi_dict, all_roads, all_lanes, all_aois, TASK_FILES):\n    pois_ids_set = {poi_id for poi_id in poi_dict if map.get_poi(poi_id)['name']}\n\n    cared_roads = {}\n    cared_roads_name = {}\n\n    for road_id, road in all_roads.items():\n        lane_ids = road[\"lane_ids\"]\n        road_name = road[\"name\"]\n        if road_name == '':\n            continue\n\n        road_length = 0\n        road_aoi_ids = []\n        near_by_lanes = []\n        one_lane_in_road = []\n        road_level_count = []\n        for i, lane_id in enumerate(lane_ids):\n            if isinstance(lane_id, list):\n                print(\"lane_id in lane_ids is List!!!!!\")\n                continue\n\n            lane = all_lanes[lane_id]\n            length = lane[\"length\"]\n            aoi_ids = lane[\"aoi_ids\"]\n            road_aoi_ids.extend(aoi_ids)\n            \n            # using the lane information under the road to estimate the road length\n            if len(one_lane_in_road) == 0:\n                one_lane_in_road.append(lane_id)\n                road_length += length\n\n                left_lanes = lane[\"left_lane_ids\"]\n                right_lanes = lane[\"right_lane_ids\"]\n                near_by_lanes.extend(left_lanes)\n                near_by_lanes.extend(right_lanes)\n                road_level_count.append(len(left_lanes) + len(right_lanes))\n            else:\n                if lane_id not in near_by_lanes:\n                    one_lane_in_road.append(lane_id)\n                    road_length += length\n\n                    left_lanes = lane[\"left_lane_ids\"]\n                    right_lanes = lane[\"right_lane_ids\"]\n                    near_by_lanes.extend(left_lanes)\n                    near_by_lanes.extend(right_lanes)\n                    road_level_count.append(len(left_lanes) + len(right_lanes))\n\n\n        cared_roads[road_id] = {\"lane_ids\": lane_ids, \"name\": road_name, \"length\": road_length, \"aoi_ids\": road_aoi_ids}\n        if road_name in cared_roads_name:\n            cared_roads_name[road_name][\"lane_ids\"].append(lane_ids)\n            cared_roads_name[road_name][\"road_ids\"].append(road_id)\n            cared_roads_name[road_name][\"length\"].append(road_length)\n            cared_roads_name[road_name][\"aoi_ids\"].append(road_aoi_ids)\n        else:\n            cared_roads_name[road_name] = {\"road_ids\": [road_id], \"lane_ids\": [lane_ids], \"length\": [road_length],\n                                            \"aoi_ids\": [road_aoi_ids],\n                                            \"level_count\": road_level_count}\n\n    for road_name in cared_roads_name:\n        road_cut = max(cared_roads_name[road_name][\"level_count\"][0] + 1, 1)\n        cared_roads_name[road_name][\"road_length_estimate\"] = float(\n            np.sum(cared_roads_name[road_name][\"length\"]) / road_cut)\n\n    res_length = []\n    for road_name in cared_roads_name:\n        road_length = int(cared_roads_name[road_name][\"road_length_estimate\"] / 100) * 100\n\n        res = [max(road_length * 0.1, 100), max(road_length * 0.3, 100), road_length, road_length * 2, road_length * 3]\n        question = \"How long is {} road?\".format(road_name)\n        answer = road_length\n        res_dict = gen_options(res, question, answer)\n        res_length.append(res_dict)\n\n    task_df = pd.DataFrame(data=res_length)\n    task_df[\"is_seen\"] = False\n    task_df.to_csv(TASK_FILES[\"path\"][\"road_length\"])\n    print(\"path road_length task OK!\")\n\n    for road_name in cared_roads_name:\n        road_aoi_ids = cared_roads_name[road_name][\"aoi_ids\"]\n        road_poi_ids = []\n        for aoi_ids in road_aoi_ids:\n            for aoi_id in aoi_ids:\n                road_poi_ids.extend(all_aois[aoi_id][\"poi_ids\"])\n        road_poi_ids = list(set(road_poi_ids))\n\n        final_poi_ids = []\n        for rpi in road_poi_ids:\n            if rpi in pois_ids_set:\n                final_poi_ids.append(rpi)\n\n        filter_final_poi_ids = []\n        for p in final_poi_ids:\n            poi_name = map.get_poi(p)['name']\n            if poi_name != '':\n                filter_final_poi_ids.append(p)\n\n        cared_roads_name[road_name][\"arrived_pois\"] = filter_final_poi_ids\n    res_arrived_pois = []\n    for road_name in cared_roads_name:\n        res_dict = {}\n        arrived_pois = cared_roads_name[road_name][\"arrived_pois\"]\n        if len(arrived_pois) > 15:\n            question_type = \"FindCannot\"\n        else:\n            question_type = \"FindCan\"\n\n        if question_type == \"FindCannot\":\n            res_temp = random.sample(arrived_pois, 15)\n            res_temp_name = [map.get_poi(p)['name'] for p in res_temp]\n            not_temp = random.sample(list(pois_ids_set.difference(set(arrived_pois))), 5)\n            not_temp_name = [map.get_poi(p)['name'] for p in not_temp]\n            res = [res_temp_name[:5], res_temp_name[5:10], res_temp_name[10:], not_temp_name]\n            question = \"Which POIs cannot be directly accessed via {}?\".format(road_name)\n            answer = not_temp_name\n            res_dict = gen_options(res, question, answer)\n        else:\n            try:\n                res_temp = random.sample(arrived_pois, 5)\n            except ValueError as e:\n                continue\n            res_temp_name = [map.get_poi(p)['name'] for p in res_temp]\n            not_temp = random.sample(list(pois_ids_set.difference(set(arrived_pois))), 15)\n            not_temp_name = [map.get_poi(p)['name'] for p in not_temp]\n            res = [not_temp_name[:5], not_temp_name[5:10], not_temp_name[10:], res_temp_name]\n            answer = res_temp_name\n            question = \"Which POIs can be directly accessed via {}?\".format(road_name)\n            res_dict = gen_options(res, question, answer)\n        res_arrived_pois.append(res_dict)\n\n    task_df = pd.DataFrame(data=res_arrived_pois)\n    task_df[\"is_seen\"] = False\n    task_df.to_csv(TASK_FILES[\"path\"][\"road_arrived_pois\"])\n    print(\"path road_arrived_pois task OK!\")\n\n\ndef generate_evaluation_task_road_junc(all_juncs, all_roads, all_lanes, REGION_EXP_POLYGON, TASK_FILES):\n\n    cared_juncs = {}\n    cared_roads_name = []\n\n    for road_id, road in all_roads.items():\n        lane_ids = road[\"lane_ids\"]\n        road_name = road['name']\n        if road_name == '':\n            continue\n\n        road_in_region = False\n        for i, lane_id in enumerate(lane_ids):\n            if isinstance(lane_id, list):\n                print(\"lane_id in lane_ids is List!!!!!\")\n                continue\n\n            lane = all_lanes[lane_id]\n            length = lane[\"length\"]\n            last_point = Point(lane[\"shapely_lnglat\"].coords[-1])\n\n            if REGION_EXP_POLYGON.contains(last_point):\n                road_in_region = True\n                break\n\n        if road_in_region:\n            if road_name not in cared_roads_name:\n                cared_roads_name.append(road_name)\n\n    # get basic information of junctions and roads\n    for junc_id in all_juncs:\n        road_in_region = False\n        lane_ids = all_juncs[junc_id][\"lane_ids\"]\n\n        pre_road_names = []\n        suc_road_names = []\n        pre_road_ids = []\n        suc_road_ids = []\n        for lane_id in lane_ids:\n            lane = all_lanes[lane_id]\n            pre_lane_id = lane[\"predecessors\"][0][\"id\"]\n            suc_lane_id = lane[\"successors\"][0][\"id\"]\n            pre_lane = all_lanes[pre_lane_id]\n            suc_lane = all_lanes[suc_lane_id]\n\n            last_point = Point(lane[\"shapely_lnglat\"].coords[-1])\n\n            pre_road_id = pre_lane[\"parent_id\"]\n            if pre_road_id in all_roads:\n                pre_road_name = all_roads[pre_road_id][\"name\"]\n                if pre_road_name == \"\":\n                    continue\n                pre_road_names.append(pre_road_name)\n                pre_road_ids.append(pre_road_id)\n            else:\n                continue\n\n            suc_road_id = suc_lane[\"parent_id\"]\n            if suc_road_id in all_roads:\n                suc_road_name = all_roads[suc_road_id]['name']\n                if suc_road_name == \"\":\n                    continue\n                suc_road_names.append(suc_road_name)\n                suc_road_ids.append(suc_road_id)\n            else:\n                continue\n\n            if REGION_EXP_POLYGON.contains(last_point):\n                road_in_region = True\n\n        if road_in_region:\n            pre_road_name = set(pre_road_names)\n            suc_road_name = set(suc_road_names)\n            pre_road_id = set(pre_road_ids)\n            suc_road_id = set(suc_road_ids)\n            cared_juncs[junc_id] = {\"pre_road_id\": pre_road_id, \"suc_road_id\": suc_road_id,\n                                    \"pre_road_name\": pre_road_name, \"suc_road_name\": suc_road_name, \"gps\": last_point}\n\n    for junc in cared_juncs:\n        road_list = list(cared_juncs[junc][\"pre_road_name\"])\n        if len(road_list) <= 1:\n            junc_name = \"\"\n        elif len(road_list) == 2:\n            junc_name = \"the junction of {} and {}\".format(road_list[0], road_list[1])\n        elif len(road_list) == 3:\n            junc_name = \"the junction of {}, {} and {}\".format(road_list[0], road_list[1], road_list[2])\n        cared_juncs[junc][\"name\"] = junc_name\n\n    road_junc_gen(cared_juncs, cared_roads_name, TASK_FILES)\n    road_linkage_gen(cared_juncs, TASK_FILES)\n\n\ndef road_junc_gen(cared_juncs, cared_roads_name, TASK_FILES):\n    road_juncs = {}\n    for junc in cared_juncs:\n        road_list = cared_juncs[junc][\"pre_road_name\"]\n        junc_name = cared_juncs[junc][\"name\"]\n        junc_coord = cared_juncs[junc][\"gps\"]\n        if len(road_list) <= 1:\n            continue\n\n        for road in list(road_list):\n            if road not in road_juncs:\n                road_juncs[road] = {\"detail\": [(junc_name, junc_coord)]}\n            else:\n                road_juncs[road][\"detail\"].append((junc_name, junc_coord))\n\n    for rj in road_juncs:\n        if len(road_juncs[rj][\"detail\"]) >= 2:\n            lng_list = []\n            lat_list = []\n            for item in road_juncs[rj][\"detail\"]:\n                lng_list.append(item[1].x)\n                lat_list.append(item[1].y)\n            lng_max = np.ptp(lng_list)\n            lat_max = np.ptp(lat_list)\n            start_junc, end_junc = [p for p in road_juncs[rj][\"detail\"]][:2]\n\n            if lng_max >= lat_max:\n                # the difference of longitude is larger\n                lng_list_index = list(np.argsort(lng_list))\n                for i, idx in enumerate(lng_list_index):\n                    if i == 0:\n                        start_junc = road_juncs[rj][\"detail\"][idx]\n                    elif idx == (len(lng_list) - 1):\n                        end_junc = road_juncs[rj][\"detail\"][idx]\n            else:\n                # the difference of latitude is larger\n                lat_list_index = list(np.argsort(lat_list))\n                for i, idx in enumerate(lat_list_index):\n                    if i == 0:\n                        start_junc = road_juncs[rj][\"detail\"][idx]\n                    elif i == (len(lat_list) - 1):\n                        end_junc = road_juncs[rj][\"detail\"][idx]\n            # print(rj, \"start:\", start_junc, \"end:\", end_junc)\n            road_juncs[rj][\"start_junc\"] = start_junc\n            road_juncs[rj][\"end_junc\"] = end_junc\n\n    res_road_endpoint = []\n    for rj in road_juncs:\n        # only one junction\n        if len(road_juncs[rj][\"detail\"]) < 2:\n            continue\n\n        res_label = [road_juncs[rj][\"start_junc\"][0], road_juncs[rj][\"end_junc\"][0]]\n\n        res = [res_label]\n        try:\n            # negative sample\n            current_road_name = rj\n            start_road_names = random.sample(list(set(cared_roads_name).difference({rj})), 3)\n            end_road_names = random.sample(list(set(cared_roads_name).difference(set([rj] + start_road_names))), 3)\n            for i, (sr, er) in enumerate(zip(start_road_names, end_road_names)):\n                start_juncs = [\"the junction of {} and {}\".format(current_road_name, sr),\n                            \"the junction of {} and {}\".format(sr, current_road_name)]\n                end_juncs = [\"the junction of {} and {}\".format(current_road_name, er),\n                            \"the junction of {} and {}\".format(er, current_road_name)]\n                for junc in start_juncs + end_juncs:\n                    assert junc not in res_label, \"make sure negative sample is not in positive sample\"\n                res.append([random.choice(start_juncs), random.choice(end_juncs)])\n        except:\n            # print(\"randomly sample error\")\n            # print(\"current_label:{}\", res_label)\n            # print(\"negative_sample:{}\", start_juncs + end_juncs)\n            continue\n        question = \"Which of the following is the starting intersection and ending intersection of {}?\".format(rj)\n        answer = res_label\n        res_dict = gen_options(res, question, answer)\n\n        res_road_endpoint.append(res_dict)\n\n    task_df = pd.DataFrame(data=res_road_endpoint)\n    task_df[\"is_seen\"] = False\n    task_df.to_csv(TASK_FILES[\"path\"][\"road_od\"])\n    print(\"path road_od task OK!\")\n\n\ndef road_linkage_gen(cared_juncs, TASK_FILES):\n    road_linkage = {}\n    for junc in cared_juncs:\n        road_list = cared_juncs[junc][\"pre_road_name\"]\n        if len(road_list) <= 1:\n            continue\n\n        for road in list(road_list):\n            if road not in road_linkage:\n                road_linkage[road] = set(road_list)\n            else:\n                road_linkage[road] = road_linkage[road].union(set(road_list))\n    for road in road_linkage:\n        road_linkage[road].remove(road)\n\n    all_roads = set(road_linkage.keys())\n    road_nearby = []\n    for road_name in road_linkage:\n        link_roads = road_linkage[road_name]\n        if len(link_roads) > 2:\n            res = random.sample(list(link_roads), 3)\n            select = random.sample(list(all_roads.difference(link_roads)), 1)\n            res.extend(select)\n            random.shuffle(res)\n            res_dict = dict(zip([\"A\", \"B\", \"C\", \"D\"], res))\n            for k in res_dict:\n                if res_dict[k] == select[0]:\n                    label = k\n            res_dict[\"question\"] = \"Which road cannot directly reach {}?\".format(road_name)\n            res_dict[\"answer\"] = label\n        else:\n            select = random.sample(list(link_roads), 1)\n            res = random.sample(list(all_roads.difference(link_roads)), 3)\n            res.extend(select)\n            random.shuffle(res)\n            question = \"Which road directly reach {}?\".format(road_name)\n            answer = select[0]\n            res_dict = gen_options(res, question, answer)\n        road_nearby.append(res_dict)\n\n    task_df = pd.DataFrame(data=road_nearby)\n    task_df[\"is_seen\"] = False\n    task_df.to_csv(TASK_FILES[\"path\"][\"road_link\"])\n    print(\"path road_link task OK!\")\n\ndef generate_evalation_task_node(map, poi_message, category_supported, TASK_FILES):\n    filtered_pois_data = poi_message.dropna(subset=[\"name\"]).query('name != \"\"')\n\n    if filtered_pois_data.shape[0] < 1000:\n        node_num = filtered_pois_data.shape[0]\n    else:\n        node_num = 1000\n\n    pois_data = filtered_pois_data.sample(node_num, random_state=42)\n    pois2name = pois_data.set_index(\"poi_id\")[\"name\"].to_dict()\n   \n    seen_pois = []\n    unseen_pois = []\n    for p in pois2name:\n        coords = map.get_poi(p)[\"position\"]\n        (x, y) = (coords[\"x\"], coords[\"y\"])\n        p_name = pois2name[p]\n        try:\n            res_dict = landmark_gen((x, y), p_name, category_supported, map)\n            if res_dict:\n                unseen_pois.append(res_dict)\n        except IndexError as e:\n            pass\n\n    save_data(unseen_pois, TASK_FILES[\"landmark\"][\"landmark_env\"])\n    print(\"landmark landmark_env task OK!\")\n\ndef gps_gen(input_coor, poi_name):\n    resolution = 0.01\n    round_demical = 4\n\n    lng, lat = input_coor\n    lng, lat = round(lng, round_demical), round(lat, round_demical)\n\n    res = [[lng, lat], [lng - resolution, lat - resolution], [lng - resolution, lat + resolution],\n           [lng + resolution, lat - resolution], [lng + resolution, lat + resolution]]\n    for i, coor in enumerate(res):\n        coor[0] = round(coor[0], round_demical)\n        coor[1] = round(coor[1], round_demical)\n        res[i] = coor\n    coor_str = \",\".join([str(lng), str(lat)])\n    res_str = [\",\".join([str(xx) for xx in x]) for x in res]\n    answer = coor_str\n    question = \"What is the longitude and latigude coordinates of {}.\".format(poi_name)\n    res_dict = gen_options(res_str, question, answer)\n    return res_dict\n\n\ndef landmark_gen(input_coor, poi_name, category_supported, map):\n    x, y = input_coor\n    radius = 100\n    limit = 10\n    resolution = 500\n\n    nearby_pois = []\n    for category_prefix in category_supported.keys():\n        poi_list = map.query_pois(input_coor, radius, category_prefix, limit)\n        poi_list = [poi[0]['name'] for poi in poi_list if poi[0]['name']]\n        nearby_pois.extend(poi_list[:min(limit, len(poi_list))])\n    \n    if len(nearby_pois) < 5:\n        return None\n\n    candidate_poi = [poi_name]\n    for center in [(x - resolution, y - resolution), (x - resolution, y + resolution), (x + resolution, y - resolution),\n                   (x + resolution, y + resolution)]:\n        poi_list = map.query_pois(center, radius, \"\", 100)\n        poi_list = [poi[0]['name'] for poi in poi_list if poi[0]['name']]\n        candidate_poi.append(random.choice(poi_list))\n    res_str = [str(x) for x in candidate_poi]\n    question = \"Which point of interest (POI) is most likely to appear in the described environment among the following multiple POIs? Environment:{}\".format(\n        \",\".join(nearby_pois))\n    answer = str(poi_name)\n    res_dict = gen_options(res_str, question, answer)\n    return res_dict\n\n\ndef get_nearby_pois(input_coor, map, category_supported):\n    lng, lat = input_coor\n    radius = 500\n    limit = 10\n    resolution = 500\n    nearby_pois = []\n    input_coor = map.lnglat2xy(lng, lat)\n    for category_prefix in category_supported.keys():\n        poi_list = map.query_pois(input_coor, radius, category_prefix, limit)\n        poi_list = [poi[0][\"id\"] for poi in poi_list]\n        nearby_pois.extend(poi_list)\n    return nearby_pois\n\n\ndef poi2cor_gen(input_coor, poi_id, poi_dict):\n    poi_name = poi_dict[int(poi_id)]['name']\n    resolution = 0.01\n    round_demical = 4\n    lng, lat = input_coor\n    lng, lat = round(lng, round_demical), round(lat, round_demical) \n    res = [(lng, lat), (lng - resolution, lat - resolution), (lng - resolution, lat + resolution),\n           (lng + resolution, lat - resolution), (lng + resolution, lat + resolution)]\n    coor_str = \",\".join([str(lng), str(lat)])\n    res_str = [\",\".join([str(round(xx, round_demical)) for xx in x]) for x in res]\n    answer = coor_str\n    question = \"What is the longitude and latigude coordinates of {}.\".format(poi_name)\n    res_dict = gen_options(res_str, question, answer)\n    return res_dict\n\n\ndef poi2addr_gen(nearby_addrs, poi_dict, poi_id):\n    tar_addr = poi_dict[int(poi_id)]['Address']\n    tar_name = poi_dict[int(poi_id)]['name']\n    res_str = []\n    for addr in nearby_addrs:\n        if len(res_str) < 4:\n            if addr != tar_addr:\n                res_str.append(addr)\n    res_str.append(tar_addr)\n    answer = tar_addr\n    question = \"What is the address of {}?\".format(tar_name)\n    res_dict = gen_options(res_str, question, answer)\n    return res_dict\n\n\ndef poi2type_gen(tar_type, all_pois, type_pois):\n    all_poi_cate = type_pois.keys()\n    available_types = [cate for cate in all_poi_cate if cate != tar_type]\n\n    res_str = random.sample(available_types, 4)\n    res_str.append(tar_type)\n    if len(set(type_pois[tar_type])) < 4:\n        return None\n    tar_poi_ids = random.sample(list(set(type_pois[tar_type])), 4)\n    poi_names = [all_pois[poi_id]['name'] for poi_id in tar_poi_ids]\n    answer = tar_type\n    question = \"Which type do following POIs belong to? POIs:{}\".format(\",\".join(poi_names))\n    res_dict = gen_options(res_str, question, answer)\n    return res_dict\n\n\ndef type2poi_gen(tar_type, all_pois, type_pois):  \n    type1_ids = []  # POI_ID not in tar_type\n    for type1, ids in type_pois.items():\n        if type1 != tar_type:\n            type1_ids += ids\n    if len(set(type1_ids)) < 4 or len(set(type_pois[tar_type])) < 1:\n        return None\n    res_poi_ids = random.sample(list(set(type1_ids)), 4)\n    tar_poi_id = random.sample(list(set(type_pois[tar_type])), 1)[0]\n    tar_poi_name = all_pois[tar_poi_id]['name']\n    poi_names = [all_pois[poi_id]['name'] for poi_id in res_poi_ids]\n    poi_names.append(tar_poi_name)\n    answer = tar_poi_name\n    question = \"Which POI belongs to {}?\".format(tar_type)\n    res_dict = gen_options(poi_names, question, answer)\n    return res_dict\n\n\ndef aoi_poi_gen(aoi_id, all_pois, all_aois, poi_dict, aoi_dict):\n    # aoi_id = random.sample(list(all_aois.keys()),1)\n    poi_ids = [poi_id for poi_id in all_aois[aoi_id]['poi_ids'] if all_pois[poi_id]['name']]\n    if len(poi_ids) < 1:\n        return None\n    tar_poi_id = random.sample(poi_ids, 1)[0]\n    tar_poi = all_pois[tar_poi_id]['name']\n    res_poi_ids = random.sample(list(set(poi_dict.keys()) - set(poi_ids)), 3)\n    res_pois = [all_pois[poi_id]['name'] for poi_id in res_poi_ids]\n    pois_in_aoi = [all_pois[poi_id]['name'] for poi_id in poi_ids]\n    while len(set(res_pois) - set(pois_in_aoi)) < 3: \n        res_poi_id = random.sample(list(set(poi_dict.keys()) - set(poi_ids)), 1)[0]\n        res_pois.append(all_pois[res_poi_id]['name'])\n    res_pois.append(tar_poi)\n    question = \"Which of the following POIs is likely to belong to the AOI {}?POIs:{}\".format(\n        aoi_dict[aoi_id]['name'], \",\".join(res_pois))\n    answer = tar_poi\n    res_dict = gen_options(res_pois, question, answer)\n    return res_dict\n\n\ndef poi_aoi_gen(aoi_id, all_pois, all_aois, aoi_dict):\n    tar_aoi = aoi_dict[aoi_id]['name']\n    valid_poi_ids = [poi_id for poi_id in all_aois[aoi_id]['poi_ids'] if all_pois[poi_id]['name']]\n    if len(valid_poi_ids) < 4:\n        return None\n    poi_ids = random.sample(valid_poi_ids, 4)\n    res_aoi_ids = random.sample(list(set(aoi_dict.keys()) - {aoi_id}), 3)\n    res_aois = [aoi_dict[aoi_id]['name'] for aoi_id in res_aoi_ids]\n    res_aois.append(tar_aoi)\n    pois_in_aoi = [all_pois[poi_id]['name'] for poi_id in poi_ids]\n    answer = tar_aoi\n    question = \"Which AOI does the following POIs belong to？POIs:{}\".format(\",\".join(pois_in_aoi))\n    res_dict = gen_options(res_aois, question, answer)\n    return res_dict\n\n\ndef select_types(poi_ids, poi_dict):\n    result_type = []\n    poi_ids = [poi_id for poi_id in poi_ids if poi_id in poi_dict]\n    poi_types = [poi_dict[poi_id]['category'] for poi_id in poi_ids]\n    result_type += [random.sample([poi_id for poi_id in poi_ids if poi_dict[poi_id]['category'] == poi_type],\n                                  int((poi_types.count(poi_type)) / 4) + 1) for poi_type in set(poi_types)]\n    return list(chain(*result_type))\n\n\ndef aoi2addr_gen(nearby_addrs, aoi_id, aoi_dict):\n    tar_addr = aoi_dict[aoi_id]['address']\n    tar_name = aoi_dict[aoi_id]['name']\n    nearby_addrs.append(tar_addr)\n\n    return gen_options(nearby_addrs, \"What is the address of {}?\".format(tar_name), tar_addr)\n\n\ndef aoi2type_gen(map, aoi_id, landuse_dict, aoi_dict, poi_dict):\n    landuse_str = [\"OtherNon-construction\", \"Residential\", \"TrafficStation&Park\", \"Sports\", \"Entertainment\", \"OtherPublicFacilities\", \"Education\",\"Park&GreenLand\",\"CommercialService&IndustryFacilities\",\"Resort&Fitness\",\"Restaurant&Bar\",\"ReligiousFacilities\",\"Hospital\"]\n    try:\n        tar_type = landuse_dict[aoi_dict[aoi_id]['category']]\n    except KeyError as e:\n        return None\n    options = [landuse for landuse in landuse_str if landuse != tar_type]\n    random_options = random.sample(options, 4)\n    random_options.append(tar_type)\n    poi_ids = map.get_aoi(aoi_id)['poi_ids']\n    poi_in_type = select_types(poi_ids, poi_dict)\n    poi_names = [poi_dict[poi_id]['name'] for poi_id in poi_in_type if poi_id in poi_dict]\n    if len(poi_names) < 3:\n        return None\n    question = \"Given that {} contains these POIs:{}.Which landuse type does {} belong to?\".format(\n        map.get_aoi(aoi_id)['name'], \",\".join(poi_names), map.get_aoi(aoi_id)['name']\n    )\n\n    return gen_options(random_options, question, tar_type)\n\n\ndef type2aoi_gen(map, landuse, landuse_dict, aoi_dict, poi_dict):  \n    tar_aoi = []\n    res_aoi = []\n    for id, aoi in aoi_dict.items():\n        type_ = aoi['category']\n        if type_ == landuse:\n            tar_aoi.append(id)\n        else:\n            res_aoi.append(id)\n    if tar_aoi == []:\n        return None\n    aoi_id1 = random.sample(tar_aoi, 1)[0]\n    aoi_id2, aoi_id3, aoi_id4 = random.sample(res_aoi, 3)\n    poi_names1 = [poi_dict[poi_id]['name'] for poi_id in select_types(map.get_aoi(aoi_id1)['poi_ids'], poi_dict)]\n    poi_names2 = [poi_dict[poi_id]['name'] for poi_id in select_types(map.get_aoi(aoi_id2)['poi_ids'], poi_dict) if poi_id in poi_dict]\n    poi_names3 = [poi_dict[poi_id]['name'] for poi_id in select_types(map.get_aoi(aoi_id3)['poi_ids'], poi_dict) if poi_id in poi_dict]\n    poi_names4 = [poi_dict[poi_id]['name'] for poi_id in select_types(map.get_aoi(aoi_id4)['poi_ids'], poi_dict) if poi_id in poi_dict]\n    aoi_names = [map.get_aoi(aoi_id1)['name'], map.get_aoi(aoi_id2)['name'], map.get_aoi(aoi_id3)['name'], map.get_aoi(aoi_id4)['name']]\n    if len(poi_names1) < 2 or len(poi_names2) < 2 or len(poi_names3) < 2 or len(poi_names4) < 2:\n        return None\n    answer =map.get_aoi(aoi_id1)['name']\n    question = \"Given that:1.{} contains these POIs:{}\\n2.{} contains these POIs:{}\\n3.{} contains these POIs:{}\\n4.{} contains these POIs:{}\\nWhich of the above AOIs({},{},{},{}) is designated as {}?\".format(\n        map.get_aoi(aoi_id1)['name'], \",\".join(poi_names1), map.get_aoi(aoi_id2)['name'], \",\".join(poi_names2), map.get_aoi(aoi_id3)['name'], \",\".join(poi_names3), map.get_aoi(aoi_id4)['name'],\n        \",\".join(poi_names4), map.get_aoi(aoi_id1)['name'], map.get_aoi(aoi_id2)['name'], map.get_aoi(aoi_id3)['name'], map.get_aoi(aoi_id4)['name'], landuse_dict[landuse])\n    res_dict = gen_options(aoi_names, question, answer)\n    return res_dict\n\n\ndef districts_poi_type_gen(aoi_id, type_num, map):\n    aoi_info = map.get_aoi(aoi_id)\n    res_str = [3, 4, 5, 6, 7]\n    answer = type_num\n    question = \"How many types of POIs are there in {}?\".format(aoi_info['name'])\n    res_dict = gen_options(res_str, question, answer)\n    return res_dict\n\n\ndef save_data(unseen_pois, save_path):\n    unseen_pois_df = pd.DataFrame(data=unseen_pois)\n    unseen_pois_df.to_csv(save_path)\n\n\ndef get_node_num(dict):\n    if len(dict.keys()) > 200:\n        return 200\n    else:\n        return len(dict.keys())\n\n\ndef generate_evaluation_task_aoi_loc(aoi_dict, poi_dict, TASK_FILES):\n    question_num = get_node_num(aoi_dict)\n    aois = list(aoi_dict.keys())[:question_num]\n    aoi2addr = []\n    for aoi_id in aois:\n        res = [item for item in aois if item != aoi_id]\n        aoi_id1, aoi_id2, aoi_id3 = random.sample(res, 3)\n        if len({aoi_dict[aoi_id1]['address'], aoi_dict[aoi_id2]['address'], aoi_dict[aoi_id3]['address']}) < 3:\n            continue\n        nearby_addrs = [aoi_dict[aoi_id1]['address'], aoi_dict[aoi_id2]['address'], aoi_dict[aoi_id3]['address']]\n        aoi2addr.append(aoi2addr_gen(nearby_addrs, aoi_id, aoi_dict))\n    save_data(aoi2addr, TASK_FILES[\"districts\"][\"aoi2addr\"])\n    print(\"districts aoi2addr task OK!\")\n\n\ndef get_land_uses(aoi_dict):\n    landuses = set()\n    for id, aoi in aoi_dict.items():\n        landuse = aoi['category']\n        landuses.add(landuse)\n    return list(landuses)\n\n\ndef generate_evaluation_task_aoi_type(map, aoi_dict, landuse_dict, poi_dict, TASK_FILES):\n    question_num = get_node_num(aoi_dict)\n    aois = list(aoi_dict.keys())[:question_num]\n    aoi2type = []\n    for aoi_id in aois:\n        res_dict_aoi2type = aoi2type_gen(map, aoi_id, landuse_dict, aoi_dict, poi_dict)\n        if res_dict_aoi2type:\n            aoi2type.append(res_dict_aoi2type)\n    save_data(aoi2type, TASK_FILES[\"districts\"][\"aoi2type\"])\n    print(\"districts aoi2type task OK!\")\n    type2aoi = []\n \n    landuses = [\n        \"E3\", \"R\", \"S4\", \"A4\", \"B31\", \"U9\", \"A3\", \"G1\", \"B\", \"B32\", \"B13\", \"A9\", \"A5\"\n    ]\n\n    right = 0\n    while right < 20:\n        p = random.sample(landuses, 1)[0]\n        res_dict_type2aoi = type2aoi_gen(map, p, landuse_dict, aoi_dict, poi_dict)\n        if res_dict_type2aoi:\n            type2aoi.append(res_dict_type2aoi)\n            right += 1\n    save_data(type2aoi, TASK_FILES[\"districts\"][\"type2aoi\"])\n    print(\"districts type2aoi task OK!\")\n\n\ndef generate_districts_poi_type(map, aoi_dict, poi_dict, TASK_FILES):\n    question_num = get_node_num(aoi_dict)\n    aois = list(aoi_dict.keys())[:question_num]\n    districts_poi_type = []\n    for aoi_id in aois:\n        poi_ids = list(set(map.get_aoi(aoi_id)['poi_ids']) & set(poi_dict.keys()))\n        types = set()\n        for p in poi_ids:\n            types.add(map.get_poi(p)['category'])\n        type_num = len(types)\n        # print(f\"type num: {aoi.type_num}\")\n        if type_num < 3 or type_num > 7:\n            continue\n        districts_poi_type.append(districts_poi_type_gen(aoi_id, type_num, map))\n    save_data(districts_poi_type, TASK_FILES[\"districts\"][\"districts_poi_type\"])\n    print(\"districts districts_poi_type task OK!\")\n\n\ndef generate_evaluation_task_poi_loc(map, poi_dict, poi_message, all_pois, category_supported, TASK_FILES):\n    question_num = get_node_num(poi_dict)\n    pois = random.sample(list(poi_dict.keys()), question_num)\n    name_counts = poi_message['name'].value_counts(sort=True)\n    many_names = name_counts[name_counts > 10]  \n    poi2cor = []\n    for p in pois:\n        coords = map.get_poi(p)[\"position\"]\n        if all_pois[p]['name'] in many_names:\n            continue\n\n        if len(coords) > 0:\n            lng, lat = map.xy2lnglat(x=coords[\"x\"], y=coords[\"y\"])  \n            poi2cor.append(poi2cor_gen((lng, lat), p, poi_dict))\n    save_data(poi2cor, TASK_FILES[\"node\"][\"poi2coor\"])\n    print(\"node poi2coor task OK!\")\n    poi2addr = []\n    for p in pois:\n        coords = map.get_poi(p)[\"position\"] \n        lng, lat = map.xy2lnglat(x=coords[\"x\"], y=coords[\"y\"])\n        nearby_id = get_nearby_pois((lng, lat), map, category_supported)\n        nearby_addrs = [poi_dict[id]['Address'] for id in nearby_id if id in poi_dict]\n        if all_pois[p]['name'] in many_names:\n            continue\n        if len(list(set(nearby_addrs))) > 4 and p in poi_dict:\n            poi2addr.append(poi2addr_gen(nearby_addrs, poi_dict, p))\n    save_data(poi2addr, TASK_FILES[\"node\"][\"poi2addr\"])\n    print(\"node poi2addr task OK!\")\n\n\ndef generate_evaluation_task_poi_type(aoi_dict, all_aois, all_pois, type_pois, TASK_FILES):\n    question_num = get_node_num(aoi_dict)\n    aois = random.sample(list(aoi_dict.keys()), question_num)\n    poi2type = []\n    for key in type_pois.keys():\n        res_poi2type = poi2type_gen(key, all_pois, type_pois)\n        if res_poi2type:\n            poi2type.append(res_poi2type)\n    save_data(poi2type, TASK_FILES[\"node\"][\"poi2type\"])\n    print(\"node poi2type task OK!\")\n    type2poi = []\n    for key in type_pois.keys():\n        res_type2poi = type2poi_gen(key, all_pois, type_pois)\n        if res_type2poi:\n            type2poi.append(res_type2poi)\n    save_data(type2poi, TASK_FILES[\"node\"][\"type2poi\"])\n    print(\"node type2poi task OK!\")\n\n\ndef generate_evaluation_task_poi_aoi(aoi_dict, all_pois, all_aois, poi_dict, TASK_FILES):\n    question_num = get_node_num(aoi_dict)\n    aois = random.sample(list(aoi_dict.keys()), question_num)\n    aoi_poi = []\n    for p in aois:\n        if len(all_aois[p]['poi_ids']) <= 3 or not isinstance(aoi_dict[p]['name'], str):\n            continue\n        ret_aoi_poi = aoi_poi_gen(p, all_pois, all_aois, poi_dict, aoi_dict)\n        if ret_aoi_poi:\n            aoi_poi.append(ret_aoi_poi)\n    save_data(aoi_poi, TASK_FILES[\"districts\"][\"aoi_poi\"])\n    print(\"districts aoi_poi task OK!\")\n    poi_aoi = []\n    for p in aois:\n        if len(all_aois[p]['poi_ids']) <= 3 or not isinstance(aoi_dict[p]['name'], str):\n            continue\n        res_poi_aoi = poi_aoi_gen(p, all_pois, all_aois, aoi_dict)\n        if res_poi_aoi:\n            poi_aoi.append(res_poi_aoi)\n        \n    save_data(poi_aoi, TASK_FILES[\"districts\"][\"poi_aoi\"])\n    print(\"districts poi_aoi task OK!\")\n\ndef landmark_path(map, poi_dict, pois_along_route, start_poi, end_poi):\n    pois_along_route = list(set(pois_along_route))\n    all_pois = random.choices(list(poi_dict.keys()), k=10)\n    all_pois_name = set([map.get_poi(poi_id)['name'] for poi_id in all_pois])\n    negative_samples = all_pois_name.difference(set(pois_along_route))\n    negative_sample = random.choice(list(negative_samples))\n    if len(pois_along_route) >= 3:\n        res_pois = random.sample(pois_along_route, k=3)\n    else:\n        print(\"pois_along_route is less than 3\")\n        return None\n\n    res_pois.append(negative_sample)\n    question = \"Which of the following POIs will not be passed when traveling from {} to {}?\".format(\n        start_poi, end_poi)\n    answer = negative_sample\n    res_dict = gen_options(res_pois, question, answer)\n    return res_dict\n\n\nasync def generate_evaluation_task_landmark(map, routing_client, poi_dict, TASK_FILES):\n    route_arrive_pois = []\n    count = 0\n    while count < 50:\n        # 随机选出两个不同的POI ID\n        poi_ids = list(poi_dict.keys())\n        start_poi_id, end_poi_id = random.sample(poi_ids, 2)\n        start_aoi_id = poi_dict[start_poi_id][\"aoi_id\"]\n        end_aoi_id = poi_dict[end_poi_id][\"aoi_id\"]\n        if start_aoi_id == end_aoi_id:\n            continue\n        player = Player(map, routing_client, start_aoi_id, \"poi\")\n        route = await player.get_driving_route(end_aoi_id)\n        if route is None:\n            continue\n        # 限制步数不超过12步，不少于3步\n        if len(route[\"road_ids\"]) > 12 or len(route[\"road_ids\"]) < 3:\n            continue\n        road_list = []\n        for road_id in route[\"road_ids\"]:\n            road_info = player._city_map.get_road(road_id)\n            lane_info = player._city_map.get_lane(road_info[\"lane_ids\"][0])\n            road_list = player.road_info_collect(road_info, lane_info, road_list)\n\n        start_poi_name = poi_dict[start_poi_id]['name']\n        start_poi_addr = poi_dict[start_poi_id]['Address']\n        start_poi = \"{}{}\".format(start_poi_name, start_poi_addr)\n        end_poi_name = poi_dict[end_poi_id]['name']\n        end_poi_addr = poi_dict[end_poi_id]['Address']\n        end_poi = \"{}{}\".format(end_poi_name, end_poi_addr)\n        if not start_poi_name or not end_poi_name:\n            continue\n        pois_along_route = []\n        while(len(road_list) > 0):\n            road_name, road_length, lane_id, direction, _ = road_list.pop(0)\n            lane_info = map.get_lane(lane_id)\n            endpoint_lnglat = lane_info[\"shapely_lnglat\"].coords[-1]\n            endpoint_xy = lane_info[\"shapely_xy\"].coords[-1]\n\n            # update position\n            player.position = Position(\n                xy_position=XYPosition(x=endpoint_xy[0], y=endpoint_xy[1]),\n                longlat_position=LongLatPosition(longitude=endpoint_lnglat[0], latitude=endpoint_lnglat[1]),\n            )\n            interest_list = player.get_nearby_interests()\n            pois_along_route.extend(interest_list)\n        # print(\"length of pois_along_route: \", len(set(pois_along_route)))\n        res_dict = landmark_path(map, poi_dict, pois_along_route, start_poi, end_poi)\n        # print(\"res_dict: \", res_dict) \n        if not res_dict:\n            continue\n        route_arrive_pois.append(res_dict)\n        count += 1\n    save_data(route_arrive_pois, TASK_FILES[\"landmark\"][\"landmark_path\"])\n    print(\"landmark landmark_path task OK!\")\n\ndef generate_evaluation_task_boudary(all_roads, all_lanes, aoi_dict, REGION_EXP_POLYGON, TASK_FILES):\n    cared_roads_name = {}\n    for road_id in all_roads:\n        lane_ids = all_roads[road_id][\"lane_ids\"]\n        road_name = all_roads[road_id]['name']\n        if road_name == '':\n            continue\n\n        road_in_region = False\n        aoi_ids_include = []\n        for i, lane_id in enumerate(lane_ids):\n            if isinstance(lane_id, list):\n                print(\"lane_id in lane_ids is List!!!!!\")\n                continue\n\n            lane = all_lanes[lane_id]\n            last_point = Point(lane[\"shapely_lnglat\"].coords[-1])\n            aoi_ids = lane[\"aoi_ids\"]\n\n            if REGION_EXP_POLYGON.contains(last_point):\n                road_in_region = True\n                aoi_ids_include.extend(aoi_ids)\n\n        if road_in_region:\n            include_aois = []\n            for aoi_id in aoi_ids_include:\n                if aoi_id in aoi_dict:\n                    include_aois.append(aoi_dict[aoi_id]['name'])\n            if road_name not in cared_roads_name:\n                cared_roads_name[road_name] = include_aois\n            else:\n                cared_roads_name[road_name].extend(include_aois)\n    for road_name in cared_roads_name:\n        cared_roads_name[road_name] = list(set(cared_roads_name[road_name]))\n\n    aoi_boundary_dict = {}\n    for aoi_id in aoi_dict:\n        aoi_name = aoi_dict[aoi_id]['name']\n        aoi_boundary_dict[aoi_id] = {\"name\": aoi_name, \"boundary\": []}\n        for road_name in cared_roads_name:\n            if aoi_name in cared_roads_name[road_name]:\n                aoi_boundary_dict[aoi_id][\"boundary\"].append(road_name)\n\n    res_aoi_boundary = []\n    candidates_roads = list(cared_roads_name.keys())\n    for aoi_id in aoi_boundary_dict:\n        if len(aoi_boundary_dict[aoi_id][\"boundary\"]) == 0:\n            continue\n\n        aoi_name = aoi_boundary_dict[aoi_id][\"name\"]\n        boundary = aoi_boundary_dict[aoi_id][\"boundary\"]\n\n        if len(boundary) >= 3:\n            res = random.sample(boundary, 3)\n\n            random.shuffle(candidates_roads)\n            for x in candidates_roads:\n                if x not in boundary:\n                    negative_sample = x\n                    break\n            res.append(negative_sample)\n            question = \"Which road is not the boundary of AOI {}\".format(aoi_name)\n            answer = negative_sample\n            res_dict = gen_options(res, question, answer)\n        else:\n            # 边界较少，询问哪个是边界\n            current_roads = list(set(candidates_roads).difference(set(boundary)))\n            res = random.sample(current_roads, 3) + random.sample(boundary, 1)\n            answer = res[-1]\n            question = \"Which road is the boundary of AOI {}\".format(aoi_name)\n            res_dict = gen_options(res, question, answer)\n        res_aoi_boundary.append(res_dict)\n\n    task_df = pd.DataFrame(data=res_aoi_boundary)\n    task_df[\"is_seen\"] = False\n    task_df.to_csv(TASK_FILES[\"boundary\"][\"boundary_road\"])\n    print(\"boundary boundary_road task OK!\")\n\ndef generate_evalation_task_boundary_poi(map, aoi_dict, poi_dict, TASK_FILES):\n    pois_ids = poi_dict.keys()\n    aois_data = []\n    available_aois = []\n    min_pois = 5\n    for aoi_id in aoi_dict.keys():\n        info = map.get_aoi(id=aoi_id)\n        if len(info[\"poi_ids\"]) >= min_pois:\n            aois_data.append(info)\n            available_aois.append(aoi_id)\n    print(\"available aois:{}\".format(len(available_aois)))\n\n    aoi_boudary_pois = []\n    for i, aoi_id in enumerate(available_aois):\n        info = aois_data[i]\n\n        if aoi_id in aoi_dict:\n            aoi_name = aoi_dict[aoi_id]['name']\n        else:\n            continue\n\n        poi_ids = info[\"poi_ids\"]\n        boudary = info[\"shapely_xy\"]\n        center = boudary.centroid\n        poi_distance = []\n        for poi_id in poi_ids:\n            poi_info = map.get_poi(poi_id)\n            if poi_info['name'] == '':\n                continue\n            pos = poi_info[\"position\"]\n            pos_shapely = Point((pos[\"x\"], pos[\"y\"]))\n            dis = shapely.distance(center, pos_shapely)\n            poi_distance.append((poi_id, dis))\n\n        poi_distance = sorted(poi_distance, key=lambda x: x[1])\n\n        if len(poi_distance) < 3:\n            continue\n        random_pois = random.sample(list(pois_ids), 10)\n        random_pois_clean = []\n        for poi_id in random_pois:\n            if poi_id != poi_distance[0][0] and poi_id != poi_distance[-1][0]:\n                if map.get_poi(poi_id)['name'] != '':\n                    random_pois_clean.append(poi_id)\n        if len(random_pois_clean) < 2:\n            continue\n\n        poi_names = [map.get_poi(p[0])['name'] for p in poi_distance]\n        res_pois = [poi_names[0], poi_names[1], poi_names[-1],\n                    map.get_poi(random_pois_clean[0])['name'],\n                    map.get_poi(random_pois_clean[1])['name']]\n        label_name = poi_names[-1]\n        question = \"Which point of interest (POI) is most likely to appear in the boundary of AOI:{}?\".format(aoi_name)\n        answer = str(label_name)\n        res_dict = gen_options(res_pois, question, answer)\n        aoi_boudary_pois.append(res_dict)\n\n    task_df = pd.DataFrame(data=aoi_boudary_pois)\n    task_df.to_csv(TASK_FILES[\"boundary\"][\"aoi_boundary_poi\"])\n    print(\"boundary aoi_boundary_poi task OK!\")\n\ndef generate_evalation_task_districts(map, aoi_message, TASK_FILES):\n\n    def districts_gen(aois, max_group, pois_name_str):\n        res = list(range(1, max_group + 1))\n        answer = len(aois)\n        question = \"How many regions can the following POIs be divided into? POIs:{}\".format(pois_name_str)\n        res_dict = gen_options(res, question, answer)\n        return res_dict\n\n    aois_data = []\n    available_aois = []\n    min_pois = 2\n    for aoi_id in aoi_message.aoi_id.to_list():\n        info = map.get_aoi(id=aoi_id)\n        if len(info[\"poi_ids\"]) >= min_pois:\n            aois_data.append(info)\n            available_aois.append(aoi_id)\n    print(\"available aois:{}\".format(len(available_aois)))\n\n    aois_group = []\n    group_size = 50\n    max_group = 5\n    for i in range(max_group):\n        for _ in range(2 * group_size):\n            item = tuple(random.sample(available_aois, i + 1))\n            if item not in aois_group:\n                aois_group.append(item)\n            if len(aois_group) == group_size * (i + 1):\n                break\n    random.shuffle(aois_group)\n\n    tasks = []\n    for aois in aois_group:\n        pois = []\n        for aoi in aois:\n            pois_in = map.get_aoi(aoi)[\"poi_ids\"]\n            pois_in = [poi_id for poi_id in pois_in if map.get_poi(poi_id)['name']]\n            max_len = min(len(pois_in), 6)\n            min_len = min(len(pois_in), 1)\n            if max_len <= min_len:\n                continue\n            pois.extend(random.sample(pois_in, random.sample(range(min_len, max_len), 1)[0]))\n        pois_names = []\n        for poi in pois:\n            try:\n                poi_name = map.get_poi(poi)['name']\n                pois_names.append(poi_name)\n            except IndexError as e:\n                print(e)\n                continue\n\n        if len(pois_names) <= len(aois):\n            continue\n\n        random.shuffle(pois_names)\n        tasks.append(districts_gen(aois, 5, \",\".join(pois_names)))\n\n    task_df = pd.DataFrame(data=tasks)\n    task_df[\"is_seen\"] = False\n    task_df.to_csv(TASK_FILES[\"districts\"][\"aoi_group\"])\n    print(\"districts aoi_group task OK!\")\n\n\n\ndef get_aoi_type(aoi_id, poi_ids, aoi_dict, poi_dict):\n    type_dict = {}\n    aoi = aoi_dict[aoi_id]\n    for poi_id in poi_ids:\n        type1 = poi_dict[poi_id]['category']\n        if type1 not in type_dict:\n            type_dict[type1] = []\n        type_dict[type1].append(poi_id)\n    max_length = 0\n    most_type = None\n    for type1, poi_ids in type_dict.items():\n        # 检查当前value的长度是否大于已知的最大长度\n        if len(poi_ids) > max_length:\n            max_length = len(poi_ids)\n            most_type = type1\n    return type_dict, most_type\n\n\ndef AOI_POI_task_gen(map, aoi_dict, poi_dict, aoi_id):   #选择poi_ids > 3,poi_id有至少一个具有名字，且具有名字的的aoi_id\n    poi_ids = list(set(aoi_dict[aoi_id]['poi_ids']) & set(poi_dict.keys()))   #是否要这样做,先找名字再poi_ids\n    aoi_address = aoi_dict[aoi_id]['address']\n    type_in_aoi = get_aoi_type(aoi_id, poi_ids, aoi_dict, poi_dict)[0]\n    most_type = get_aoi_type(aoi_id, poi_ids, aoi_dict, poi_dict)[1]\n    tar_poi = random.choice(type_in_aoi[most_type])\n    tar_poi_name = poi_dict[tar_poi]['name']\n\n    res_pois = random.choices(list(set(poi_dict.keys()).difference({tar_poi})),k=3)\n    res_pois_name = [poi_dict[res_poi]['name'] for res_poi in res_pois]\n    res_pois_name.append(tar_poi_name)\n    question = \"Which of the following POIs is in the '{}' category within the area at {}?\".format(most_type, aoi_address)\n    answer = tar_poi_name\n    res_dict = gen_options(res_pois_name,question,answer)\n    return res_dict\n\ndef AOI_POI2_task_gen(map, aoi_dict, poi_dict, aoi_id):\n    res_aois = random.choices(list(set(aoi_dict.keys()).difference({aoi_id})),k=3)\n    res_aois.append(aoi_id)\n    res_aois_name = [aoi_dict[res_aoi]['name'] for res_aoi in res_aois]\n    max_length = 0\n    most_aoi = None\n    for aoi in res_aois:\n        poi_ids = aoi_dict[aoi]['poi_ids']\n        # 检查当前value的长度是否大于已知的最大长度\n        if len(poi_ids) > max_length:\n            max_length = len(poi_ids)\n            most_aoi = aoi\n    most_aoi_name = aoi_dict[most_aoi]['name']\n    question2 = \"Which AOI has the most points of interest?\"\n    answer2 = most_aoi_name\n    res_dict2 = gen_options(res_aois_name, question2, answer2)\n    return res_dict2\n\ndef AOI_POI3_task_gen(map, aoi_dict, poi_dict, aoi_id, category_supported, type_pois):\n    aoi_address = aoi_dict[aoi_id]['address']\n    aoi_coor = aoi_dict[aoi_id]['coord']\n    nearby_pois = get_nearby_pois(aoi_coor, map, category_supported)\n    nearby_pois = set(nearby_pois)&set(poi_dict.keys())\n    if len(nearby_pois) == 0:\n        return\n    poi = random.choice(list(nearby_pois))\n    tar_poi_name = poi_dict[poi]['name']\n    tar_poi_type = poi_dict[poi]['category']\n    possible_pois = list(set(type_pois[tar_poi_type]).difference(set(nearby_pois)))\n    if len(possible_pois) < 3:\n        return None  \n    res_pois = random.choices(possible_pois, k=3)\n    res_pois_name = [poi_dict[res_poi]['name'] for res_poi in res_pois]\n    res_pois_name.append(tar_poi_name)\n    question3 = \"Which POIs in the category {} are located within a 100m radius of {}?\".format(tar_poi_type, aoi_address)\n    answer3 = tar_poi_name\n    res_dict3 = gen_options(res_pois_name, question3, answer3)\n    return res_dict3\n\ndef AOI_POI4_task_gen(map, aoi_dict, poi_dict, aoi_id, category_supported, type_pois):\n    poi_ids = list(set(aoi_dict[aoi_id]['poi_ids']) & set(poi_dict.keys()))   #是否要这样做,先找名字再poi_ids\n    aoi_name = aoi_dict[aoi_id]['name']\n    aoi_coor = aoi_dict[aoi_id]['coord']\n\n    type_in_aoi = get_aoi_type(aoi_id, poi_ids, aoi_dict, poi_dict)[0]\n    most_type = get_aoi_type(aoi_id, poi_ids, aoi_dict, poi_dict)[1]\n    tar_poi = random.choice(type_in_aoi[most_type])\n    further_pois = get_nearby_pois(aoi_coor, map, category_supported)\n    further_pois = set(further_pois) & set(poi_dict.keys())\n    further_pois_type = further_pois&set(type_pois[most_type])\n    if len(further_pois_type) < 4:\n        # print(\"no enough pois <4\")\n        return\n    res_pois = random.choices(list(further_pois_type), k=4)\n    min_dist = 100\n    nearest_poi = None\n    for poi in res_pois:\n        coor = poi_dict[poi]['coord']\n        dist = geodesic((coor[1], coor[0]), (aoi_coor[1], aoi_coor[0])).km\n        if dist < min_dist:\n            min_dist = dist\n            nearest_poi = poi\n    if not nearest_poi:\n        print(\"no nearest poi\")\n        return\n    nearest_poi_name = poi_dict[nearest_poi]['name']\n    res_pois_name = [poi_dict[res_poi]['name'] for res_poi in res_pois]\n    answer4 = nearest_poi_name\n    question4 = \"Which is the nearest {} POI to {}?\".format(most_type, aoi_name)\n    res_dict4 = gen_options(res_pois_name, question4, answer4)\n    return res_dict4\n\ndef AOI_POI5_task_gen(map, aoi_dict, poi_dict, aoi_id):\n    aoi_name = aoi_dict[aoi_id]['name']\n    aoi_area = map.get_aoi(aoi_id)['area']\n    question5 = \"What is the total area of {}?\".format(aoi_name)\n    answer5 = int(aoi_area)\n    res_areas = [int(aoi_area*0.8), int(aoi_area*2), int(aoi_area+200), int(aoi_area)]\n    res_dict5 = gen_options(res_areas, question5, answer5)\n    return res_dict5\n\ndef AOI_POI6_task_gen(map, aoi_dict, poi_dict, aoi_id):\n    other_aois = list(set(aoi_dict.keys()).difference({aoi_id}))\n    res_aois = random.choices(other_aois, k=3)\n    res_aois.append(aoi_id)\n    res_aois_name = [aoi_dict[aoi_id]['name'] for aoi_id in res_aois]\n    max_area = 0\n    biggest_aoi = None\n    for aoi in res_aois:\n        area = map.get_aoi(aoi)['area']\n        if area > max_area:\n            max_area = area\n            biggest_aoi = aoi\n    tar_aoi_name = aoi_dict[biggest_aoi]['name']\n    question6 = \"Which AOI has the largest area?\"\n    answer6 = tar_aoi_name\n    res_dict6 = gen_options(res_aois_name, question6, answer6)\n    return res_dict6\n\n\ndef AOI_POI2road_task_gen(map, aoi_dict, poi_dict, road_dict, aoi_id):  #选取有名字的AOI,且poi_ids数量不少于2个\n    aoi = map.get_aoi(aoi_id)\n    lanes = map.get_aoi(aoi_id)['driving_positions']\n\n    roads2aoi = []\n    for lane in lanes:\n        lane_id = lane['lane_id']\n        road_id = map.get_lane(lane_id)['parent_id']\n        if road_id not in road_dict or road_id in roads2aoi:\n            continue\n        roads2aoi.append(road_id)\n\n    other_roads = list(set(road_dict.keys()).difference(set(roads2aoi)))\n\n    roads_name = list(set([road_dict[road]['name'] for road in roads2aoi if road in set(roads2aoi)&set(road_dict.keys())]))\n    if len(roads_name) > 2:\n        res_roads_name = random.choices(roads_name, k=3)\n        other_road = random.choice(other_roads)\n        tar_road_name = road_dict[other_road]['name']\n        res_roads_name.append(tar_road_name)\n        question = \"Which of the following roads is not connected to {}?\".format(aoi['name'])\n    elif 0 < len(roads_name) <= 2:\n        tar_road_name = random.choice(roads_name)\n        other_roads = random.choices(other_roads, k=3)\n        res_roads_name = [road_dict[other_road]['name'] for other_road in other_roads]\n        res_roads_name.append(tar_road_name)\n        question = \"Which of the following roads is connected to {}?\".format(aoi['name'])\n    else:\n        return\n    answer = tar_road_name\n    res_dict = gen_options(res_roads_name, question, answer)\n\n    max_length = 0\n    max_length_road = None\n    for road_id in roads2aoi:\n        length = road_dict[road_id]['length']\n        if length > max_length:\n            max_length_road = road_id\n    if max_length_road in road_dict:\n        tar_road_name = road_dict[max_length_road]['name']\n    else:\n        tar_road_name = 'unknown road'\n        \n    res_roads = random.choices(other_roads, k=3)\n    res_roads_name = [road_dict[road_id]['name'] for road_id in res_roads]\n    question2 = \"What's the longest road that meets {}?\".format(aoi['name'])\n    answer2 = tar_road_name\n    res_roads_name.append(tar_road_name)\n    res_dict2 = gen_options(res_roads_name, question2, answer2)\n\n    tar_road = None\n    for road_id in roads2aoi:\n        if len(road_dict[road_id]['aoi_ids']) > 2 and road_id in road_dict:\n            tar_road = road_id\n            continue\n    if not tar_road:\n        return\n    other_aois = list(set(aoi_dict.keys()).difference(set(road_dict[tar_road]['aoi_ids'])))\n    tar_aois = random.choices(list(set(road_dict[tar_road]['aoi_ids'])&set(aoi_dict.keys())), k=2)\n    tar_aoi,res_aoi = tar_aois[0], tar_aois[1]\n    res_aois = random.choices(other_aois, k=3)\n    res_aois.append(tar_aoi)\n    res_aois_name = [aoi_dict[aoi_id]['name'] for aoi_id in res_aois]\n    tar_aoi_name = aoi_dict[tar_aoi]['name']\n    question3 = \"Which AOI is connected to {} by {}?\".format(aoi_dict[res_aoi]['name'], road_dict[tar_road]['name'])\n    answer3 = tar_aoi_name\n    res_dict3 = gen_options(res_aois_name, question3, answer3)\n\n    poi_ids = aoi['poi_ids']\n    other_pois = list(set(poi_dict.keys()).difference(set(poi_ids)))\n    res_pois = random.choices(other_pois, k=3)\n    # tar_poi, res_poi = poi_ids[0], poi_ids[1]\n    tar_poi, res_poi = None, None  \n    valid_pois = [poi_id for poi_id in poi_ids if poi_id in poi_dict]  \n    if len(valid_pois) >= 2:  \n        tar_poi, res_poi = valid_pois[0], valid_pois[1]\n    else:\n        return\n    res_pois.append(tar_poi)\n    tar_poi_name = poi_dict[tar_poi]['name']\n    res_pois_name = [poi_dict[poi_id]['name'] for poi_id in res_pois]\n    question4 = \"Which POIs are adjacent to {} in this area?\".format(poi_dict[res_poi]['name'])\n    answer4 = tar_poi_name\n    res_dict4 = gen_options(res_pois_name, question4, answer4)\n    return res_dict, res_dict2, res_dict3, res_dict4\n\n\ndef generate_AOI_POI_task(map, aoi_dict, poi_dict, TASK_FILES):\n    task_name = \"AOI_POI\"\n    AOI_POI = []\n    for id,aoi in aoi_dict.items():\n        name = aoi['name']\n        poi_ids = aoi['poi_ids']\n        if not isinstance(name, str) or len(poi_ids) <= 3:\n            continue\n        if len(set(poi_ids)&set(poi_dict.keys())) == 0:\n            continue\n        result = AOI_POI_task_gen(map, aoi_dict, poi_dict, id)\n        if not result:\n            continue\n        AOI_POI.append(result)\n\n    task_df = pd.DataFrame(AOI_POI)\n    task_df.to_csv(TASK_FILES[\"others\"][task_name])\n    print(\"others AOI_POI task OK!\")\n\ndef generate_AOI_POI2_task(map, aoi_dict, poi_dict, TASK_FILES):\n    task_name = \"AOI_POI2\"\n    AOI_POI = []\n    for id,aoi in aoi_dict.items():\n        name = aoi['name']\n        poi_ids = aoi['poi_ids']\n        if not isinstance(name, str) or len(poi_ids) <= 3:\n            continue\n        if len(set(poi_ids)&set(poi_dict.keys())) == 0:\n            continue\n        result = AOI_POI2_task_gen(map, aoi_dict, poi_dict, id)\n        if not result:\n            continue\n        AOI_POI.append(result)\n\n    task_df = pd.DataFrame(AOI_POI)\n    task_df.to_csv(TASK_FILES[\"others\"][task_name])\n    print(\"others AOI_POI2 task OK!\")\n\ndef generate_AOI_POI3_task(map, aoi_dict, poi_dict, category_supported, type_pois, TASK_FILES):\n    task_name = \"AOI_POI3\"\n    AOI_POI = []\n    for id,aoi in aoi_dict.items():\n        name = aoi['name']\n        poi_ids = aoi['poi_ids']\n        if not isinstance(name, str) or len(poi_ids) <= 3:\n            continue\n        if len(set(poi_ids)&set(poi_dict.keys())) == 0:\n            continue\n        result = AOI_POI3_task_gen(map, aoi_dict, poi_dict, id, category_supported, type_pois)\n        if not result:\n            continue\n        AOI_POI.append(result)\n\n    task_df = pd.DataFrame(AOI_POI)\n    task_df.to_csv(TASK_FILES[\"others\"][task_name])\n    print(\"others AOI_POI3 task OK!\")\n\ndef generate_AOI_POI4_task(map, aoi_dict, poi_dict, category_supported, type_pois, TASK_FILES):\n    task_name = \"AOI_POI4\"\n    AOI_POI = []\n    for id,aoi in aoi_dict.items():\n        name = aoi['name']\n        poi_ids = aoi['poi_ids']\n        if not isinstance(name, str) or len(poi_ids) <= 3:\n            continue\n        if len(set(poi_ids)&set(poi_dict.keys())) == 0:\n            continue\n        result = AOI_POI4_task_gen(map, aoi_dict, poi_dict, id, category_supported, type_pois)\n        if not result:\n            continue\n        AOI_POI.append(result)\n\n    task_df = pd.DataFrame(AOI_POI)\n    task_df.to_csv(TASK_FILES[\"others\"][task_name])\n    print(\"others AOI_POI4 task OK!\")\n\ndef generate_AOI_POI5_task(map, aoi_dict, poi_dict, TASK_FILES):\n    task_name = \"AOI_POI5\"\n    AOI_POI = []\n    for id,aoi in aoi_dict.items():\n        name = aoi['name']\n        poi_ids = aoi['poi_ids']\n        if not isinstance(name, str) or len(poi_ids) <= 3:\n            continue\n        if len(set(poi_ids)&set(poi_dict.keys())) == 0:\n            continue\n        result = AOI_POI5_task_gen(map, aoi_dict, poi_dict, id)\n        if not result:\n            continue\n        AOI_POI.append(result)\n\n    task_df = pd.DataFrame(AOI_POI)\n    task_df.to_csv(TASK_FILES[\"districts\"][task_name])\n    print(\"districts AOI_POI5 task OK!\")\n\ndef generate_AOI_POI6_task(map, aoi_dict, poi_dict, TASK_FILES):\n    task_name = \"AOI_POI6\"\n    AOI_POI = []\n    for id,aoi in aoi_dict.items():\n        name = aoi['name']\n        poi_ids = aoi['poi_ids']\n        if not isinstance(name, str) or len(poi_ids) <= 3:\n            continue\n        if len(set(poi_ids)&set(poi_dict.keys())) == 0:\n            continue\n        result = AOI_POI6_task_gen(map, aoi_dict, poi_dict, id)\n        if not result:\n            continue\n        AOI_POI.append(result)\n\n    task_df = pd.DataFrame(AOI_POI)\n    task_df.to_csv(TASK_FILES[\"districts\"][task_name])\n    print(\"districts AOI_POI6 task OK!\")\n\n\ndef generate_AOI_POI2road_task_gen(map, aoi_dict, poi_dict, road_dict, TASK_FILES):\n    AOI_connect_rd = []\n    AOI_longest_rd = []\n    POI2POI = []\n    AOI2rd = []\n    for id, aoi in aoi_dict.items():\n        name = aoi['name']\n        poi_ids = aoi['poi_ids']\n        if not isinstance(name, str) or len(poi_ids) <2:\n            continue\n        result = AOI_POI2road_task_gen(map, aoi_dict, poi_dict, road_dict, id)\n        if not result:\n            continue\n        AOI_connect_rd.append(result[0])\n        AOI_longest_rd.append(result[1])\n        POI2POI.append(result[2])\n        AOI2rd.append(result[3])\n\n    datas = [AOI_connect_rd, AOI_longest_rd, POI2POI, AOI2rd]\n    for idx,data in enumerate(datas):\n        task_df = pd.DataFrame(data)\n        if idx == 3:\n            task_df.to_csv(TASK_FILES[\"node\"][\"AOI_POI_road4\"])\n            print(\"node AOI_POI_road4 task OK!\")\n        else:\n            task_df.to_csv(TASK_FILES[\"boundary\"][\"AOI_POI_road{}\".format(idx+1)])\n            print(\"boundary AOI_POI_road{} task OK!\".format(idx+1))\n\nasync def main(args):\n    city_map = MAP_DICT[args.city_name]\n    \n    map, process, routing_client = load_map(\n        city_map=city_map, \n        cache_dir=MAP_CACHE_PATH, \n        routing_path=ROUTING_PATH, \n        port=args.port)\n\n\n    if not isinstance(args.output_path, str):\n        output_path = \"citydata/task_Geo_knowledge/{}/{}\".format(args.city_name, args.evaluate_version)\n    else:\n        output_path = args.output_path\n\n    random.seed(42)\n    REGION_EXP_POLYGON = Polygon(CITY_BOUNDARY[args.city_name])\n\n    TASK_FILES = task_files_adaption(GEOQA_TASK_MAPPING_v2, output_path)\n    category_supported = get_category_supported()\n\n    all_roads = map.roads\n    all_lanes = map.lanes\n    all_aois = map.aois\n    all_pois = map.pois\n    all_juncs = map.juncs\n    poi_message = pd.read_csv(os.path.join(RESOURCE_PATH, \"{}_pois.csv\".format(args.city_name)))\n    aoi_message = pd.read_csv(os.path.join(RESOURCE_PATH, \"{}_aois.csv\".format(args.city_name)))\n    road_message = pd.read_csv(os.path.join(RESOURCE_PATH, \"{}_roads.csv\".format(args.city_name)))\n\n    poi_dict = {}\n    all_poi_dict = {}\n    for row in poi_message.itertuples():\n        key = int(row.poi_id)\n        category = map.get_poi(key)['category']\n        name = row.name\n        addr = row.Address\n        if not isinstance(name, str):\n            continue\n        poi_dict[key] = {\n            \"aoi_id\": map.get_poi(key)['aoi_id'], \"category\": category, \"name\": name, \"Address\": addr,\n            \"coord\": map.get_poi(key)['shapely_lnglat'].coords[0],\n        }\n    print(f\"define poi_dict done! len:{len(poi_dict)}\")\n\n    landuse_dict = get_landuse_dict()\n    aoi_dict = {} \n    # filter \"nearby\" aoi\n    for row in aoi_message.itertuples():\n        aoi_id = row.aoi_id\n        aoi_name = row.aoi_name\n        address = row.Address\n        extra = \"nearby\"\n        poi_ids = map.get_aoi(aoi_id)['poi_ids']\n        if not isinstance(aoi_name, str) or extra in aoi_name:\n            continue\n        if not isinstance(address, str):\n            continue\n        aoi_dict[aoi_id] = {}\n        aoi_dict[aoi_id]['category'] = map.get_aoi(aoi_id)['urban_land_use']\n        aoi_dict[aoi_id]['coord'] = eval(row.coords)[0]\n        aoi_dict[aoi_id]['name'] = aoi_name\n        aoi_dict[aoi_id]['address'] = address\n        aoi_dict[aoi_id]['poi_ids'] = poi_ids\n    print(f\"define aoi_dict done! len:{len(aoi_dict)}\")\n\n    type_pois = {}  \n\n    for poi_id, poi in poi_dict.items():\n        poi_type = poi[\"category\"]\n        if poi_type in type_pois:\n            type_pois[poi_type].append(poi_id)\n        else:\n            type_pois[poi_type] = [poi_id]\n    \n    road_dict = {}\n    for row in road_message.itertuples():\n        road_aois = []\n        road_id = row.road_id\n        length = all_roads[road_id]['length']\n        road_name = row.road_name\n        lane_ids = all_roads[road_id]['lane_ids']\n        for lane_id in lane_ids:\n            aoi_ids = all_lanes[lane_id]['aoi_ids']\n            road_aois += aoi_ids\n        if road_name == \"未知路名\" or road_name == \"unknown road\" or not isinstance(road_name, str):\n            continue\n        road_dict[road_id] = {}\n        road_dict[road_id]['name'] = road_name\n        road_dict[road_id]['length'] = length\n        road_dict[road_id]['lane_ids'] = lane_ids\n        road_dict[road_id]['aoi_ids'] = list(set(road_aois))\n        \n    print(\"Start synthesizing evaluation data\")\n\n    print(\"Start task: road length & road poi\")\n    generate_evaluation_task_road(map, poi_dict, all_roads, all_lanes, all_aois, TASK_FILES)\n\n    print(\"Start task: road od & road link\")\n    generate_evaluation_task_road_junc(all_juncs, all_roads, all_lanes, REGION_EXP_POLYGON, TASK_FILES)\n\n    print(\"Start task: landmark env\")\n    generate_evalation_task_node(map, poi_message, category_supported, TASK_FILES)\n\n    print(\"Start task: landmark path\")\n    await generate_evaluation_task_landmark(map, routing_client, poi_dict, TASK_FILES)\n\n    print(\"Start task: boundary road\")\n    generate_evaluation_task_boudary(all_roads, all_lanes, aoi_dict, REGION_EXP_POLYGON, TASK_FILES)\n\n    print(\"Start task: boundary poi\")\n    generate_evalation_task_boundary_poi(map, aoi_dict, poi_dict, TASK_FILES)\n\n    print(\"Start task: districts group\")\n    generate_evalation_task_districts(map, aoi_message, TASK_FILES)\n\n    print(\"Start task: districts poi type & districts addr & districts type\") \n    generate_districts_poi_type(map, aoi_dict, poi_dict, TASK_FILES)\n    generate_evaluation_task_aoi_loc(aoi_dict, poi_dict, TASK_FILES)\n    generate_evaluation_task_aoi_type(map, aoi_dict, landuse_dict, poi_dict, TASK_FILES)\n\n    print(\"Start task: node coor & node addr & node type\") \n    generate_evaluation_task_poi_loc(map, poi_dict, poi_message, all_pois, category_supported, TASK_FILES)\n    generate_evaluation_task_poi_type(aoi_dict, all_aois, all_pois, type_pois, TASK_FILES)\n    generate_evaluation_task_poi_aoi(aoi_dict, all_pois, all_aois, poi_dict, TASK_FILES)\n\n    print(\"Start task: AOI_POI\")\n    generate_AOI_POI_task(map, aoi_dict, poi_dict, TASK_FILES)\n    generate_AOI_POI2_task(map, aoi_dict, poi_dict, TASK_FILES)\n    generate_AOI_POI3_task(map, aoi_dict, poi_dict, category_supported, type_pois, TASK_FILES)\n    generate_AOI_POI4_task(map, aoi_dict, poi_dict, category_supported, type_pois, TASK_FILES)\n    generate_AOI_POI5_task(map, aoi_dict, poi_dict, TASK_FILES)\n    generate_AOI_POI6_task(map, aoi_dict, poi_dict, TASK_FILES)\n\n    print(\"Start task: AOI_POI2road\")\n    generate_AOI_POI2road_task_gen(map, aoi_dict, poi_dict, road_dict, TASK_FILES)\n\n    print(\"send signal\")\n    process.send_signal(sig=signal.SIGTERM)\n    process.wait()\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--city_name\", type=str, default=\"Beijing\")\n    parser.add_argument(\"--evaluate_version\", type=str, default=\"v2\")\n    parser.add_argument(\"--output_path\", type=str)\n    parser.add_argument(\"--port\", type=str, default=\"54319\")\n    args = parser.parse_args()\n\n    asyncio.run(main(args))\n    \n"}
{"type": "source_file", "path": "citybench/outdoor_navigation/coordTransform_utils.py", "content": "# -*- coding: utf-8 -*-\nimport json\nimport urllib\nimport math\n\nx_pi = 3.14159265358979324 * 3000.0 / 180.0\npi = 3.1415926535897932384626  # π\na = 6378245.0  # 长半轴\nee = 0.00669342162296594323  # 偏心率平方\n\n\nclass Geocoding:\n    def __init__(self, api_key):\n        self.api_key = api_key\n\n    def geocode(self, address):\n        \"\"\"\n        利用高德geocoding服务解析地址获取位置坐标\n        :param address:需要解析的地址\n        :return:\n        \"\"\"\n        geocoding = {'s': 'rsv3',\n                     'key': self.api_key,\n                     'city': '全国',\n                     'address': address}\n        geocoding = urllib.urlencode(geocoding)\n        ret = urllib.urlopen(\"%s?%s\" % (\"http://restapi.amap.com/v3/geocode/geo\", geocoding))\n\n        if ret.getcode() == 200:\n            res = ret.read()\n            json_obj = json.loads(res)\n            if json_obj['status'] == '1' and int(json_obj['count']) >= 1:\n                geocodes = json_obj['geocodes'][0]\n                lng = float(geocodes.get('location').split(',')[0])\n                lat = float(geocodes.get('location').split(',')[1])\n                return [lng, lat]\n            else:\n                return None\n        else:\n            return None\n\n\ndef gcj02_to_bd09(lng, lat):\n    \"\"\"\n    火星坐标系(GCJ-02)转百度坐标系(BD-09)\n    谷歌、高德——>百度\n    :param lng:火星坐标经度\n    :param lat:火星坐标纬度\n    :return:\n    \"\"\"\n    z = math.sqrt(lng * lng + lat * lat) + 0.00002 * math.sin(lat * x_pi)\n    theta = math.atan2(lat, lng) + 0.000003 * math.cos(lng * x_pi)\n    bd_lng = z * math.cos(theta) + 0.0065\n    bd_lat = z * math.sin(theta) + 0.006\n    return [bd_lng, bd_lat]\n\n\ndef bd09_to_gcj02(bd_lon, bd_lat):\n    \"\"\"\n    百度坐标系(BD-09)转火星坐标系(GCJ-02)\n    百度——>谷歌、高德\n    :param bd_lat:百度坐标纬度\n    :param bd_lon:百度坐标经度\n    :return:转换后的坐标列表形式\n    \"\"\"\n    x = bd_lon - 0.0065\n    y = bd_lat - 0.006\n    z = math.sqrt(x * x + y * y) - 0.00002 * math.sin(y * x_pi)\n    theta = math.atan2(y, x) - 0.000003 * math.cos(x * x_pi)\n    gg_lng = z * math.cos(theta)\n    gg_lat = z * math.sin(theta)\n    return [gg_lng, gg_lat]\n\n\ndef wgs84_to_gcj02(lng, lat):\n    \"\"\"\n    WGS84转GCJ02(火星坐标系)\n    :param lng:WGS84坐标系的经度\n    :param lat:WGS84坐标系的纬度\n    :return:\n    \"\"\"\n    if out_of_china(lng, lat):  # 判断是否在国内\n        return [lng, lat]\n    dlat = _transformlat(lng - 105.0, lat - 35.0)\n    dlng = _transformlng(lng - 105.0, lat - 35.0)\n    radlat = lat / 180.0 * pi\n    magic = math.sin(radlat)\n    magic = 1 - ee * magic * magic\n    sqrtmagic = math.sqrt(magic)\n    dlat = (dlat * 180.0) / ((a * (1 - ee)) / (magic * sqrtmagic) * pi)\n    dlng = (dlng * 180.0) / (a / sqrtmagic * math.cos(radlat) * pi)\n    mglat = lat + dlat\n    mglng = lng + dlng\n    return [mglng, mglat]\n\n\ndef gcj02_to_wgs84(lng, lat):\n    \"\"\"\n    GCJ02(火星坐标系)转GPS84\n    :param lng:火星坐标系的经度\n    :param lat:火星坐标系纬度\n    :return:\n    \"\"\"\n    if out_of_china(lng, lat):\n        return [lng, lat]\n    dlat = _transformlat(lng - 105.0, lat - 35.0)\n    dlng = _transformlng(lng - 105.0, lat - 35.0)\n    radlat = lat / 180.0 * pi\n    magic = math.sin(radlat)\n    magic = 1 - ee * magic * magic\n    sqrtmagic = math.sqrt(magic)\n    dlat = (dlat * 180.0) / ((a * (1 - ee)) / (magic * sqrtmagic) * pi)\n    dlng = (dlng * 180.0) / (a / sqrtmagic * math.cos(radlat) * pi)\n    mglat = lat + dlat\n    mglng = lng + dlng\n    return [lng * 2 - mglng, lat * 2 - mglat]\n\n\ndef bd09_to_wgs84(bd_lon, bd_lat):\n    lon, lat = bd09_to_gcj02(bd_lon, bd_lat)\n    return gcj02_to_wgs84(lon, lat)\n\n\ndef wgs84_to_bd09(lon, lat):\n    lon, lat = wgs84_to_gcj02(lon, lat)\n    return gcj02_to_bd09(lon, lat)\n\n\ndef _transformlat(lng, lat):\n    ret = -100.0 + 2.0 * lng + 3.0 * lat + 0.2 * lat * lat + \\\n          0.1 * lng * lat + 0.2 * math.sqrt(math.fabs(lng))\n    ret += (20.0 * math.sin(6.0 * lng * pi) + 20.0 *\n            math.sin(2.0 * lng * pi)) * 2.0 / 3.0\n    ret += (20.0 * math.sin(lat * pi) + 40.0 *\n            math.sin(lat / 3.0 * pi)) * 2.0 / 3.0\n    ret += (160.0 * math.sin(lat / 12.0 * pi) + 320 *\n            math.sin(lat * pi / 30.0)) * 2.0 / 3.0\n    return ret\n\n\ndef _transformlng(lng, lat):\n    ret = 300.0 + lng + 2.0 * lat + 0.1 * lng * lng + \\\n          0.1 * lng * lat + 0.1 * math.sqrt(math.fabs(lng))\n    ret += (20.0 * math.sin(6.0 * lng * pi) + 20.0 *\n            math.sin(2.0 * lng * pi)) * 2.0 / 3.0\n    ret += (20.0 * math.sin(lng * pi) + 40.0 *\n            math.sin(lng / 3.0 * pi)) * 2.0 / 3.0\n    ret += (150.0 * math.sin(lng / 12.0 * pi) + 300.0 *\n            math.sin(lng / 30.0 * pi)) * 2.0 / 3.0\n    return ret\n\n\ndef out_of_china(lng, lat):\n    \"\"\"\n    判断是否在国内，不在国内不做偏移\n    :param lng:\n    :param lat:\n    :return:\n    \"\"\"\n    return not (lng > 73.66 and lng < 135.05 and lat > 3.86 and lat < 53.55)\n\n'''\nif __name__ == '__main__':\n    lng = 128.543\n    lat = 37.065\n    result1 = gcj02_to_bd09(lng, lat)\n    result2 = bd09_to_gcj02(lng, lat)\n    result3 = wgs84_to_gcj02(lng, lat)\n    result4 = gcj02_to_wgs84(lng, lat)\n    result5 = bd09_to_wgs84(lng, lat)\n    result6 = wgs84_to_bd09(lng, lat)\n\n    g = Geocoding('API_KEY')  # 这里填写你的高德api的key\n    result7 = g.geocode('北京市朝阳区朝阳公园')\n    print result1, result2, result3, result4, result5, result6, result7\n'''"}
{"type": "source_file", "path": "citybench/outdoor_navigation/eval.py", "content": "import os\nimport argparse\nimport numpy as np\nimport pandas as pd\nimport random\nimport jsonlines\nimport json\nimport signal\nfrom datetime import datetime\nfrom multiprocessing import Pool\nfrom openai import OpenAI\nfrom tqdm import tqdm\n\n\nfrom serving.vlm_serving import VLMWrapper\nfrom config import MAP_CACHE_PATH, RESOURCE_PATH, RESULTS_PATH, MONGODB_URI, MAP_DICT, IMAGE_FOLDER, ROUTING_PATH, VLM_API, NAVIGATION_UPGRADE\nfrom .utils import calculate_direction, calculate_distance, get_prompt_eval, get_basic_prompt, get_prompt_eval_reason\nfrom global_utils import load_map\nfrom serving.llm_api import  get_chat_completion, get_model_response_hf, match_response, get_model_response_hf_image, match_response_reason\n\ndef route_process_nav(city_map, city, road_ids, match_df, url_df, meta_info_df):\n    # one route landmark navigation\n    url_map = dict(zip(url_df['image_name'], url_df['image_url']))\n    instructions = []\n    basic_prompt = get_basic_prompt()\n    instructions.append({\n        \"type\": \"text\",\n        \"text\": basic_prompt\n    })\n    steps = []\n\n    for i, road_id in enumerate(road_ids):\n        road_matches = match_df[match_df['road_id'] == road_id]\n        if road_matches.empty:\n            print(f\"No matches found for road_id: {road_id}\")\n            continue\n        road_info = city_map.get_road(road_id)\n        road_name = road_info['name']\n        if not road_name: \n            road_name = \"unknown road\"\n        road_len = road_info['length']\n        lane_id = road_info['lane_ids'][0]\n\n        # 按照 distance 对匹配到的文件进行排序\n        sorted_matches = road_matches.sort_values(by='distance')\n        for count, (idx, row) in enumerate(sorted_matches.iterrows()):\n            walk_len = int(road_len - row['distance'])\n            image_path_suffix = row['file_name']\n            url_image = url_map.get(image_path_suffix)\n            \n            if count == 0:\n                action = \"forward\"\n                step_instruction = f\"When you see this image, your current action is 'forward':\"\n                instructions.append({\n                        \"type\": \"text\",\n                        \"text\": step_instruction\n                    })\n                instructions.append({\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": url_image\n                    }\n                })\n                \n            # if last element\n            elif count == len(sorted_matches) - 1:\n                # print(\"enter last image\")\n                # check if it is the last road_id\n                if i == len(road_ids) - 1:\n                    action = \"stop\"\n                    step_instruction = f\"When you see this image, your current action is '{action}':\"\n                    instructions.append({\n                        \"type\": \"text\",\n                        \"text\": step_instruction\n                    })\n                    instructions.append({\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": url_image\n                        }\n                    })\n                else:\n                    next_road_id = road_ids[i+1]\n                    next_road_info = city_map.get_road(next_road_id)\n                    next_road_start = next_road_info['shapely_lnglat'].coords[0]\n                    current_road_end = road_info['shapely_lnglat'].coords[-1]\n                    action = calculate_direction(current_road_end, next_road_start)\n                    step_instruction = f\"When you see this image, your current action is '{action}':\"\n                    instructions.append({\n                        \"type\": \"text\",\n                        \"text\": step_instruction\n                    })\n                    instructions.append({\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": url_image\n                        }\n                    })\n            steps.append({\n                        \"action\": action,\n                        \"image_name\": image_path_suffix\n                    })\n            \n    last_instruction = f\"ATTENTION: Your should describe the image and integrate the action decision into your description for EACH image.\"\n    instructions.append({\n        \"type\": \"text\",\n        \"text\": last_instruction\n    })\n    return instructions, steps\n\n\ndef nav_gen(city, match_file, route_file, meta_file, url_file, output_file, city_map, model_name, data_sample=600):\n    \n    match_data_df = pd.read_csv(match_file)\n    meta_info_df = pd.read_csv(meta_file)\n    url_df = pd.read_csv(url_file)  \n\n    # 控制路径数量\n    sample = 0\n    with jsonlines.open(output_file, mode='a') as writer:\n        with jsonlines.open(route_file) as reader:\n            for obj in reader:\n                start_aoi_id = obj.get('start_aoi_id')\n                dest_aoi_id = obj.get('dest_aoi_id')\n                road_ids = obj.get('road_ids')\n                instructions, steps = route_process_nav(city_map, city, road_ids, match_data_df, url_df, meta_info_df)\n                response = get_chat_completion([{\"role\": \"user\", \"content\": json.dumps(instructions)}], model_name)\n\n                record = {\n                        \"route\": road_ids,\n                        \"response\": response,\n                        \"steps\": steps\n                    }\n                writer.write(record)\n                sample += 1\n                \n                if sample == data_sample:\n                    break\n\ndef single_route_process(city, url_file, route, navigation, steps, model_name):\n    # print(\"enter single_route_process\")\n    name_to_url_mapping = pd.read_csv(url_file).set_index('image_name')['image_url'].to_dict()\n    success_flag = True\n    basic_prompt = get_prompt_eval()\n    basic_prompt = f\"{basic_prompt}\\n{navigation}\"\n    prompts = []\n    prompts.append({\n    \"type\": \"text\",\n    \"text\": basic_prompt\n    })\n    for step in steps:\n        text = f\"Here is the street view image of your current location.\"\n        image_name = step['image_name']\n        image_url = name_to_url_mapping[image_name]\n        prompts.append({\n            \"type\": \"text\",\n            \"text\": text\n        })\n        prompts.append({\n            \"type\": \"image_url\",\n            \"image_url\": {\n                \"url\": image_url\n            }\n        })\n        last_text = f\"Please provide the next action('forward', 'left', 'right', or 'stop') based on the image and the navigation instruction:\"\n        prompts.append({\n            \"type\": \"text\",\n            \"text\": last_text\n        })\n        action_true = step['action']\n        action,_ = get_chat_completion([{\"role\": \"user\", \"content\": prompts}], model_name)\n        print(f\"Action: {action}, True Action: {action_true}\")\n        if action != action_true:\n            success_flag = False\n            print(\"false\")\n            break\n        prompts.append({\n            \"type\": \"text\",\n            \"text\": f\"Action: {action}\"\n        })\n    \n    if success_flag:\n        print(\"right\")\n    return success_flag\n    \n\ndef process_single_route(args):\n    # 在筛选路径时，gpt-4o-mini尝试5次，保留成功的路径\n    city, url_file, route, response, steps, model_name = args\n\n    for i in range(5):\n        success_time = single_route_process(city, url_file, route, response, steps, model_name)\n        if success_time == 1:\n            print(f\"Success found for route {route}\")\n            return {\"route\": route, \"response\": response, \"steps\": steps} \n\n    return None \n\n\ndef validate_eval(city, url_file, instruction_file, instruction_validate_file, model_name, num_processes):\n    results_to_save = []\n    print(f\"Validating routes in {instruction_file} using model {model_name}...\")\n    with jsonlines.open(instruction_file) as reader:\n        records = list(reader)  \n\n    print(f\"Validating {len(records)} routes...\")\n    args_list = [(city, url_file, record['route'], record['response'], record['steps'], model_name) for record in records]\n    with Pool(processes=num_processes) as pool:  \n        print(\"Processing routes...enter parallel processing\")\n        for result in pool.imap(process_single_route, args_list):\n            if result:\n                results_to_save.append(json.dumps(result))\n    with open(instruction_validate_file, 'a') as file:\n        for item in results_to_save:\n            file.write(item + '\\n')\n\n    print(f\"Validated routes saved to {instruction_validate_file}\")\n\n\ndef single_route_process_eval(city, url_file, logs_file, route, navigation, steps, model_name, model=None):\n    # print(\"enter single_route_process_eval\")\n\n    name_to_url_mapping = pd.read_csv(url_file).set_index('image_name')['image_url'].to_dict()\n    success_flag = True\n    step_count = 0  \n    if NAVIGATION_UPGRADE == True:\n        basic_prompt = get_prompt_eval_reason()\n    else:\n        basic_prompt = get_prompt_eval()\n    basic_prompt = f\"{basic_prompt}\\n{navigation}\"\n    begin_prompts = []\n\n    if model_name in VLM_API:\n        begin_prompts.append({\n        \"type\": \"text\",\n        \"text\": basic_prompt\n        })\n    else:\n        begin_prompts.append({\n        \"type\": \"text\",\n        \"value\": basic_prompt\n        })\n    last_text = f\"\"\"\n            Please provide the Reason and next Action('forward', 'left', 'right', or 'stop') based on the image and the navigation instruction.\\n\"\"\"\n    last_image_name = steps[-1]['image_name']\n    last_image_url = name_to_url_mapping[last_image_name]\n    actions_history = []  \n    \n    for step in steps:\n        # 用于记录\n        step_actions = []\n        text = f\"Here is the street view image of your current location.\"\n        image_name = step['image_name']\n        image_url = name_to_url_mapping[image_name]\n        image_path = os.path.join(IMAGE_FOLDER, f\"{city}_StreetView_Images/{image_name}\")\n        current_prompts = begin_prompts.copy()\n        if NAVIGATION_UPGRADE == True:\n            if actions_history:\n                previous_actions = \"Previous actions: \" + \", \".join(actions_history)\n            else:\n                previous_actions = \"\"\n            \n\n            current_prompts = f\"{basic_prompt}\\n{previous_actions}\\n{text}\\n{last_text}\"\n            if model_name in VLM_API:\n                api_prompts = []\n                api_prompts.append({\n                    \"type\": \"text\",\n                    \"text\": current_prompts\n                })\n                api_prompts.append({\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": image_url\n                    }\n                })\n                response_action = get_chat_completion([{\"role\": \"user\", \"content\": api_prompts}], model_name)\n                reason, action = match_response_reason(response_action)\n            else:\n                model_response = get_model_response_hf_image(image_path, current_prompts, model)\n            # print(f\"model response: {model_response}\")\n                reason, action = match_response_reason(model_response)\n        else:\n            for action in actions_history:\n                if model_name in VLM_API:\n                    current_prompts.append({\n                        \"type\": \"text\",\n                        \"text\": f\"Previous action: {action}\"\n                    })\n                else:\n                    current_prompts.append({\n                        \"type\": \"text\",\n                        \"value\": f\"Previous action: {action}\"\n                    })\n\n            if model_name in VLM_API:\n                current_prompts.append({\n                    \"type\": \"text\",\n                    \"text\": text\n                })\n                current_prompts.append({\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": image_url\n                    }\n                })\n            else:\n                current_prompts.append({\n                    \"type\": \"text\",\n                    \"value\": text\n                })\n                \n                current_prompts.append({\n                    \"type\": \"image\",\n                    \"value\": image_path\n                    \n                })\n            if model_name in VLM_API:\n                current_prompts.append({\n                    \"type\": \"text\",\n                    \"text\": last_text\n                })\n            else:\n                current_prompts.append({\n                    \"type\": \"text\",\n                    \"value\": last_text\n                })\n            action_true = step['action']\n            if model_name in VLM_API:\n                response_action = get_chat_completion([{\"role\": \"user\", \"content\": current_prompts}], model_name)\n                action = match_response(response_action)\n            else:\n                response_action = get_model_response_hf(current_prompts, model)\n            action = match_response(response_action)\n        print(f\"Action: {action}, True Action: {action_true}\")\n        if action != action_true:\n            success_flag = False\n            print(\"false\")\n            distance = calculate_distance(city, last_image_name, image_name)\n            break\n\n        actions_history.append(action)\n        step_count += 1\n        current_prompts.append({\n            \"response\": {response_action},\n            \"extract\": {action},\n            \"ref\": {action_true}\n        })\n        with jsonlines.open(logs_file, mode='a') as writer:\n            writer.write({current_prompts})\n    \n    if success_flag:\n        distance = 0\n        print(\"right\")\n\n    return success_flag, step_count, distance\n\ndef process_single_route_eval(args):\n    city, url_file, logs_file, route, response, steps, model_name = args\n    success_found, step_count, distance = single_route_process_eval(city, url_file, logs_file, route, response, steps, model_name)\n    return success_found, step_count, distance\n\ndef eval_gen(city, url_file, logs_file, instruction_validate_file, results_file, model_name, num_processes, samples):\n    success_time = 0\n    step_sum = 0\n    distance_sum = 0\n\n    if model_name in VLM_API:\n        # 可以通过API调用的模型\n        with jsonlines.open(instruction_validate_file) as reader:\n            records = list(reader)  \n            record_count = len(records)\n            if samples < record_count:\n                records = random.sample(records, samples)\n            else:\n                print(f\"Requested sample size {samples} exceeds available records {record_count}. Evaluating all records.\")\n        args_list = [(city, url_file, logs_file, record['route'], record['response'], record['steps'], model_name) for record in records]\n        with Pool(processes=num_processes) as pool:  \n            for result in pool.imap(process_single_route_eval, args_list):\n                success_found, step_count, distance = result\n                if success_found:\n                    success_time += 1\n                step_sum += step_count\n                distance_sum += distance\n    else:\n        # 本地部署模型\n        print(\"other model\")\n        model_wrapper = VLMWrapper(model_name)\n        print(model_name)\n        model = model_wrapper.get_vlm_model()\n        with jsonlines.open(instruction_validate_file) as reader:\n            records = list(reader)  \n            record_count = len(records)  \n            if samples < record_count:\n                records = random.sample(records, samples)\n            else:\n                print(f\"Requested sample size {samples} exceeds available records {record_count}. Evaluating all records.\")\n\n        for record in tqdm(records):\n            route = record[\"route\"]\n            response = record[\"response\"]\n            steps = record[\"steps\"]\n            success_found, step_count, distance = single_route_process_eval(city, url_file, logs_file, route, response, steps, model_name, model)\n            if success_found:\n                success_time += 1\n            step_sum += step_count\n            distance_sum += distance\n\n    success_ratio = success_time / record_count\n    avg_step = step_sum / record_count\n    avg_distance = distance_sum / record_count\n\n    if \"/\" in model_name:\n        model_back = model_name.replace(\"/\", \"_\")\n    result_data = {\n        \"City_Name\": city,\n        \"Model_Name\": model_back,\n        \"Navigation_Success_Ratio\": success_ratio,\n        \"Navigation_Average_Steps\": avg_step,\n        \"Navigation_Average_Distance\": avg_distance\n    }\n    print(f\"Success ratio: {success_ratio}, Average steps: {avg_step}, Average distance: {avg_distance}\")\n     \n\n    result_df = pd.DataFrame([result_data])\n    result_df.to_csv(results_file, mode='a', header=not pd.io.common.file_exists(results_file), index=False)\n\n    print(f\"Results saved to {results_file}\")\n\n\ndef main(args):\n    city_map = MAP_DICT[args.city_name]\n    m, process, routing_client = load_map(\n        city_map=city_map, \n        cache_dir=MAP_CACHE_PATH, \n        routing_path=ROUTING_PATH, \n        port=args.port)\n\n    match_file = \"citydata/outdoor_navigation_tasks/{}_matched_images.csv\".format(args.city_name)\n    route_file = \"citydata/outdoor_navigation_tasks/{}_navigation_tasks.jsonl\".format(args.city_name)\n    meta_file = os.path.join(IMAGE_FOLDER, \"{}_StreetView_Images/combined_stitch_meta_info.csv\".format(args.city_name))\n    url_file = os.path.join(IMAGE_FOLDER, \"url_mapping.csv\")\n    instruction_file = \"citydata/outdoor_navigation_tasks/{}_navigation_instructions.jsonl\".format(args.city_name)\n    instruction_validate_file = \"citydata/outdoor_navigation_tasks/{}_navigation_instructions_validate.jsonl\".format(args.city_name)\n    logs_file = os.path.join(RESULTS_PATH, f'outdoor_navigation_results/logs/{args.city_name}/{args.city_name}_{args.model_name}_logs.jsonl')\n    os.makedirs(os.path.dirname(logs_file), exist_ok=True)  \n    results_file = os.path.join(RESULTS_PATH, f'outdoor_navigation_results/{args.city_name}_results.csv')\n    os.makedirs(os.path.dirname(results_file), exist_ok=True) \n\n    if args.data_name == \"all\":\n        default_samples = 50\n    else:\n        default_samples = 5\n    SAMPLES = args.samples if args.samples is not None else default_samples\n\n    num_processes = 10\n    if args.mode==\"gen\":\n        # 获取路径的landmark导航指令\n        # data_sample控制生成的导航任务数量\n        data_sample = 10000\n        # generate\n        nav_gen(args.city_name, match_file, route_file, meta_file, url_file, instruction_file, m, args.model_name, data_sample)\n        # validate\n        validate_eval(args.city_name, url_file, instruction_file, instruction_validate_file, args.model_name, num_processes)\n\n    elif args.mode==\"eval\":\n        # evaluate\n        eval_gen(args.city_name, url_file, logs_file, instruction_validate_file, results_file, args.model_name, num_processes, SAMPLES)\n    \n    print(\"send signal\")\n    process.send_signal(sig=signal.SIGTERM)\n    process.wait()\n    \nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--city_name\", type=str, default=\"Shanghai\")\n    parser.add_argument(\"--model_name\", type=str, default=\"GPT4omini\")\n    parser.add_argument(\"--data_name\", type=str, default=\"mini\", choices=[\"all\", \"mini\"])\n    parser.add_argument(\"--samples\", type=int)\n    parser.add_argument(\"--mode\", type=str, default=\"eval\")\n    parser.add_argument(\"--port\", type=int, default=54100)\n    args = parser.parse_args()\n\n    main(args)\n"}
{"type": "source_file", "path": "citybench/outdoor_navigation/sample_points_gen.py", "content": "import os\nimport pandas as pd\nimport time\nimport argparse\nimport signal\nimport ast\n\nfrom config import ROUTING_PATH, MAP_DATA_PATH, MAP_CACHE_PATH, RESOURCE_PATH, RESULTS_PATH, MAP_DICT, MONGODB_URI, SAMPLE_POINT_PATH, REGION_CODE\nfrom .utils import load_map\n\n\ndef sample_points(city, city_map):\n    road_df = pd.read_csv(os.path.join(RESOURCE_PATH, '{}_roads.csv'.format(city)))\n    # 初始化存储经纬度的列表\n    coord_x = []\n    coord_y = []\n\n    # 对每条lane，以50m为步长，生成样本点\n    step = 50\n    for road_id in road_df['road_id']:\n        road_info = city_map.get_road(road_id)\n        lane_id = road_info['lane_ids'][0]\n        lane_info = city_map.get_lane(lane_id)\n        lane_len = lane_info['length']\n        start_lng, start_lat = lane_info['shapely_lnglat'].coords[0][0], lane_info['shapely_lnglat'].coords[0][1]\n        end_lng, end_lat = lane_info['shapely_lnglat'].coords[-1][0], lane_info['shapely_lnglat'].coords[-1][1]\n        coord_x.append(start_lng)\n        coord_y.append(start_lat)\n        coord_x.append(end_lng)\n        coord_y.append(end_lat)\n        for ds in range(step, int(lane_len), step):\n            xy = lane_info[\"shapely_xy\"].interpolate(ds)\n            x, y = xy.coords[0][0], xy.coords[0][1]\n            lng, lat = city_map.xy2lnglat(x, y)\n            coord_x.append(lng)\n            coord_y.append(lat)\n\n    results_df = pd.DataFrame({\n        'coord_x': [coord_x],  \n        'coord_y': [coord_y]   \n    })\n    y_x_value = REGION_CODE[city]\n\n    if args.mode == \"Google\":\n        # 设定要分割成多少行\n        num_rows = args.group_num\n\n        expanded_rows = []\n        for index, row in results_df.iterrows():\n            coord_x_list = ast.literal_eval(row['coord_x'])\n            coord_y_list = ast.literal_eval(row['coord_y'])\n        \n            n = len(coord_x_list) // num_rows\n            extra = len(coord_x_list) % num_rows\n\n            for i in range(num_rows):\n                start_index = i * n + min(i, extra)\n                end_index = start_index + n + (1 if i < extra else 0)\n                new_coord_x = coord_x_list[start_index:end_index]\n                new_coord_y = coord_y_list[start_index:end_index]\n                expanded_rows.append({\n                    'y_x': y_x_value,\n                    'coord_x': new_coord_x,\n                    'coord_y': new_coord_y\n                })\n\n                y_x_value += 1  \n\n        expanded_df = pd.DataFrame(expanded_rows)\n\n        expanded_df.to_csv(os.path.join(SAMPLE_POINT_PATH, f'{city}_sampled_points_expand.csv'), index=False)\n    else:\n        results_df['code'] = y_x_value\n        results_df.to_csv(os.path.join(SAMPLE_POINT_PATH, f'{city}_sampled_points.csv'), index=False)\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--city_name\", type=str, default=\"Shanghai\")\n    parser.add_argument(\"--mode\", type=str, default=\"Baidu\", choices=[\"Baidu\", \"Google\"])\n    parser.add_argument(\"--port\", type=int, default=52107)\n    parser.add_argument(\"--group_num\", type=int, default=30, help=\"The number of groups to split the sampled points\")\n    args = parser.parse_args()\n\n    city_map = MAP_DICT[args.city_name]\n    m, process, routing_client = load_map(\n        city_map=city_map, \n        cache_dir=MAP_CACHE_PATH, \n        routing_path=ROUTING_PATH, \n        port=args.port)\n\n    # 等待地图加载完成\n    time.sleep(10)\n    sample_points(args.city_name, m, args.group_num)\n    \n    print(\"send signal\")\n    process.send_signal(sig=signal.SIGTERM)\n    process.wait()\n"}
{"type": "source_file", "path": "citybench/geoqa/run_eval.py", "content": "import os\nimport tqdm\nimport copy\nimport jsonlines\nimport argparse\nimport pandas as pd\nfrom multiprocessing import Pool\n\nfrom config import RESULTS_PATH, GEOQA_SAMPLE_RATIO, LLM_MODEL_MAPPING, GEOQA_TASK_MAPPING_v1, GEOQA_TASK_MAPPING_v2\nfrom serving.llm_api import get_chat_completion, extract_choice, get_model_response_hf\nfrom serving.vlm_serving import VLMWrapper\n\n\ndef task_files_adaption(task_file, path_prefix):\n    for category, tasks in task_file.items():\n        for task_name, file_name in tasks.items():\n            task_file[category][task_name] = os.path.join(path_prefix, file_name)\n    os.makedirs(path_prefix, exist_ok=True)\n    return task_file\n\n\nINIT_PROMPT = \"The following is a multiple-choice question about the geospatial knowledge of city. Please choose the most suitable one among A, B, C and D as the answer to this question. Please output the option directly. No need for explaination.\\n\"\n\ndef format_example(line, include_answer=True, max_choices=4):\n    choices = [\"A\", \"B\", \"C\", \"D\"]\n    prompt=INIT_PROMPT\n    if max_choices>=5:\n        choices.append(\"E\")\n        prompt = prompt.replace(\"A, B, C and D\", \"A, B, C, D and E\")\n    if max_choices>=6:\n        choices.append(\"F\")\n        prompt = prompt.replace(\"A, B, C, D and E\", \"A, B, C, D, E and F\")\n    \n    example = prompt + 'Question: ' + line['question']\n    for choice in choices:\n        example += f'\\n{choice}. {line[f\"{choice}\"]}'\n\n    if include_answer:\n        example += '\\nAnswer: ' + line[\"answer\"] + '\\n\\n'\n    else:\n        example += '\\nAnswer:'\n    return example\n\n\n###################### 评估接口\ndef run_evaluate_api(task_file_path, model_name, task_name, max_validation, temperature, max_tokens, region_exp, data_name, model=None):\n    try:\n        test_df = pd.read_csv(task_file_path, header=0)\n    except:\n        return []\n    \n    columns = test_df.columns.to_list()\n    if \"F\" in columns:\n        max_choices = 6\n    elif \"E\" in columns:\n        max_choices = 5\n    elif \"D\" in columns:\n        max_choices = 4\n    else:\n        max_choices = 4\n    \n    if data_name == \"mini\":\n        test_df = test_df.sample(min(max_validation,int(test_df.shape[0]*GEOQA_SAMPLE_RATIO)), random_state=42)\n    else:\n        if test_df.shape[0]>max_validation*2:\n            test_df = test_df.sample(max_validation, random_state=42)\n    correct_count, count = 0, 0\n    res = []\n    for _, row in tqdm.tqdm(test_df.iterrows(), total=len(test_df)):\n        question = format_example(row, include_answer=False, max_choices=max_choices)\n \n        if model is not None:\n            prompt = {\n                \"type\": \"text\",\n                \"value\": question\n            }\n\n            output = get_model_response_hf(prompt, model)\n        else:\n            output, token_usage = get_chat_completion(\n                session=[{\"role\":\"user\", \"content\": question}], \n                model_name=model_name, temperature=temperature, max_tokens=max_tokens\n                )\n        res.append([{\"role\":\"user\", \"content\": question}, {\"role\":\"response\", \"content\": output}, {\"role\":\"ref\", \"content\": row[\"answer\"]}])\n\n        if len(output) == 0:\n            pass\n        else:\n            ans = extract_choice(output, [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]) \n            if ans==row[\"answer\"]:\n                correct_count += 1\n        count += 1\n\n    if count == 0:\n        count = 1\n    print(\"Success rate:{}({}/{})\".format(correct_count/count, correct_count, count))\n\n    os.makedirs(\"results/logs_geo_knowledge/\", exist_ok=True)\n    if \"/\" in model_name:\n        model_name = model_name.replace(\"/\", \"_\")\n    with jsonlines.open(\"results/logs_geo_knowledge/{}_{}_{}.jsonl\".format(region_exp, model_name, task_name), \"w\") as wid:\n        for r in res:\n            wid.write(r)\n    return [model_name, task_name, correct_count, count, correct_count/count]\n\nif __name__ == \"__main__\":\n    print(\"start model evaluation\")\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--city_name\", type=str)\n    parser.add_argument(\"--model_name\", type=str)\n    parser.add_argument(\"--evaluate_version\", type=str, default=\"v1\")\n    parser.add_argument(\"--max_tokens\", default=200, type=int)\n    parser.add_argument(\"--temperature\", default=0, type=float)\n    parser.add_argument(\"--max_valid\", type=int, default=50)\n    parser.add_argument(\"--workers\", type=int, default=8)\n    parser.add_argument(\"--task_path\", type=str, help=\"same with the output path in data_gen.py\")\n    parser.add_argument(\"--data_name\", default=\"mini\", choices=[\"all\",\"mini\"])\n    args = parser.parse_args()\n\n    if not isinstance(args.task_path, str):\n        task_path = \"citydata/task_Geo_knowledge/{}/{}\".format(args.city_name, args.evaluate_version)\n    else:\n        task_path = args.task_path\n\n    if args.city_name == \"Beijing\":\n        GEOQA_TASK_MAPPING = GEOQA_TASK_MAPPING_v2\n    else:\n        if args.evaluate_version == \"v1\":\n            GEOQA_TASK_MAPPING = GEOQA_TASK_MAPPING_v1\n        elif args.evaluate_version == \"v2\":\n            GEOQA_TASK_MAPPING = GEOQA_TASK_MAPPING_v2\n    KNOWLEDGE_TASK_FILES_ = task_files_adaption(GEOQA_TASK_MAPPING, task_path)\n    \n    para_group = []\n    for model_name in [args.model_name]:  \n        model = None\n        try:\n            model_full = LLM_MODEL_MAPPING[model_name]\n        except KeyError:\n            model_full = model_name\n            model_wrapper = VLMWrapper(model_full)\n            model = model_wrapper.get_vlm_model()\n        for category, tasks in KNOWLEDGE_TASK_FILES_.items():\n            for task_name, file_path in tasks.items():\n                print(f\"Evaluating model: {model_name} task: {task_name}\")\n                if not os.path.isfile(file_path):\n                    print(f\"Task: {task_name} file not found at {file_path}, ignore it!\")\n                    continue\n            \n                para_group.append((\n                    file_path,\n                    model_name,  \n                    task_name,\n                    args.max_valid, \n                    args.temperature, \n                    args.max_tokens,\n                    args.city_name,\n                    args.data_name,\n                    model\n                ))\n\n    res_df = []\n    if args.workers == 1:\n        for para in para_group:\n            print(para)\n            res = run_evaluate_api(para[0], para[1], para[2], para[3], para[4], para[5], para[6], para[7], para[8], para[9])\n            if len(res)<1:\n                continue\n            res_df.append(res)\n    else:\n        with Pool(args.workers) as pool:\n            results = pool.starmap(run_evaluate_api, para_group)\n        for res in results:\n            if len(res)<1:\n                continue\n            res_df.append(res)\n    \n    res_df = pd.DataFrame(res_df, columns=[\"model_name\", \"task_name\", \"corrct\", \"count\", \"accuracy\"])\n    print(res_df.head())\n    res_df.to_csv(\"results/geo_knowledge_result/geo_knowledge_{}_{}_summary_{}.csv\".format(\n        args.city_name, \n        args.evaluate_version, \n        args.model_name.replace(\"/\", \"_\")))\n"}
{"type": "source_file", "path": "citybench/mobility_prediction/data_gen.py", "content": "import os\nfrom typing import Optional, List\n\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport argparse\nfrom sklearn.model_selection import train_test_split\nfrom config import CITY_BOUNDARY\n\ndef match_pos(city_poi, city_checkin):\n    mid1 = {} \n    result = []\n    for idx, row in city_poi.iterrows():\n        venue = row['Venue ID']\n        key = venue\n        if row.size < 5:\n            continue\n        if key not in mid1:\n            mid1[key] = {\"lat\":row['Latitude'],'lon':row['Longitude']}\n        else:\n            print('poi venue:', key)\n    for idx, row in city_checkin.iterrows():\n        venue = row['Venue ID']\n        user = row['User ID']\n        key = f\"{user}_{venue}\"\n        if row.size < 4:\n            continue\n        result.append({\n            \"user_id\":row['User ID'],\n            \"location\":row['Venue ID'], \n            \"utc\":row['UTC Time'],\n            \"offset\":row['Timezone Offset'],\n            \"Latitude\":mid1[venue][\"lat\"],\n            'Longitude': mid1[venue][\"lon\"]\n        })\n        # else:\n            # print('checkin venue:', key)\n    result_df = pd.DataFrame(result)\n    return result_df\ndef get_day(day):\n    tmp = {'Mon': 'Monday', 'Tue': 'Tuesday', 'Wed': 'Wednesday',\n           'Thu': 'Thursday', 'Fri': 'Friday', 'Sat': 'Saturday', 'Sun': 'Sunday'}\n    pro_day = tmp[day.split()[0]]\n    return pro_day\ndef get_time(time):\n    pre_time = time.split()[3]\n    hour = int(pre_time.split(':')[0])\n    min = int(pre_time.split(':')[1])\n    pro_time = 60*hour + min\n    return pro_time\n\ndef get_start_day(time):\n    mon2day_13 = {1: 31, 2: 28, 3: 31, 4: 30, 5: 31, 6: 30, 7: 31, 8: 31, 9: 30, 10: 31, 11: 30, 12: 31}\n    mon2day_12 = {1: 31, 2: 29, 3: 31, 4: 30, 5: 31, 6: 30, 7: 31, 8: 31, 9: 30, 10: 31, 11: 30, 12: 31}\n    mon2order = {\"Jan\": 1, \"Feb\": 2, \"Mar\": 3, \"Apr\": 4, \"May\": 5, \"Jun\": 6, \"Jul\": 7, \"Aug\": 8, \"Sep\": 9, \"Oct\": 10, \"Nov\": 11, \"Dec\": 12}\n    year = int(time.split()[-1])\n    mon = time.split()[1]\n    day = int(time.split()[2])\n    day_mon = 0\n    order = mon2order[mon]\n    if year == 2012:\n        for i in range(order):\n            day_mon += mon2day_12[i+1]\n    elif year == 2013:\n        for i in range(mon2order[mon]):\n            day_mon += mon2day_13[i+1]\n    else:\n        print(\"other year:\",year)\n    day_mon += day\n    return day_mon\ndef get_dup(group,threshold): \n    cnt_set = set()\n    for idx, row in group.iterrows():\n        venue = row['location_id']\n        user = row['user_id']\n        key = f\"{user}_{venue}\"\n        if key not in cnt_set:\n            cnt_set.add(key)\n    if (group.shape[0]-len(cnt_set))/group.shape[0] > threshold:\n        return True\n    else:\n        return False\n\ndef get_split(data, city, output_path, threshold, context_step, historical_step):\n    data['utc'] = pd.to_datetime(data['utc'])\n    data.sort_values(by=['user_id', 'utc'], inplace=True)\n\n    train_data = pd.DataFrame()\n    val_data = pd.DataFrame()\n    test_data = pd.DataFrame()\n\n    for user, group in data.groupby('user_id'):\n        if group.shape[0] < context_step + historical_step + 5:\n            continue\n        if not get_dup(group=group, threshold=threshold):\n            continue\n        train, temp = train_test_split(group, test_size=0.4, shuffle=False)\n        val, test = train_test_split(temp, test_size=0.5, shuffle=False)\n\n        train_data = pd.concat([train_data, train])\n        val_data = pd.concat([val_data, val])\n        test_data = pd.concat([test_data, test])\n\n    train_data.to_csv(os.path.join(output_path,'{}_train.csv'.format(city)), index=False)\n    val_data.to_csv(os.path.join(output_path,'{}_val.csv'.format(city)), index=False)\n    test_data.to_csv(os.path.join(output_path,'{}_test.csv'.format(city)), index=False)\n\ndef dow2int(str_day):\n    tmp = {'Monday':0, 'Tuesday':1,'Wednesday':2, 'Thursday':3, 'Friday':4, 'Saturday':5,'Sunday':6}\n    return tmp[str_day]\n\n\nclass UserPredict:\n\n    def __init__(self, user_id, historical_steps):\n        self.user_id = user_id\n        self.steps = []\n        self.diff = []\n\n        self.historical_steps = historical_steps\n\n    def expand(self, location_id, week_day, start_min):\n        self.steps.append((location_id, week_day, start_min))\n\n    def to_list(self) -> List:\n        if len(self.steps) > self.historical_steps:\n            dicts = []\n            # 滑动窗口\n            for start in range(0, len(self.steps) - self.historical_steps):\n                window = self.steps[start: start + self.historical_steps]\n                y = self.steps[start + self.historical_steps]\n\n                location_ids, week_days, start_mins = zip(*window)\n                location_y, weekday_y, start_min_y = y\n\n                dicts.append({\n                    'X': np.array(location_ids),\n                    'user_X': self.user_id,\n                    'weekday_X': np.array([dow2int(weekday) for weekday in week_days]),\n                    'start_min_X': np.array(start_mins),\n                    'diff': np.array(self.diff, dtype=np.float64),\n                    'Y': location_y,\n                    'weekday_Y': weekday_y,\n                    'start_min_Y': start_min_y,\n                })\n\n            return dicts\n        else:\n            return []\n\n\ndef get_test_dict(test, historical_step):\n    user_id_to_predict = {}\n    for _, row in test.iterrows():\n        user_id = row.user_id\n        location_id = row.location_id\n        start_min = row.start_min\n        week_day = row.week_day\n        if user_id not in user_id_to_predict:\n            predict = UserPredict(user_id, historical_step)\n            user_id_to_predict[user_id] = predict\n        predict = user_id_to_predict[user_id]\n        predict.expand(location_id, week_day, start_min)\n\n    user_id_to_predict_list = []\n    for predict in user_id_to_predict.values():\n        user_id_to_predict_list.extend(predict.to_list())\n\n    return user_id_to_predict_list\n\n\ndef filter_checkints_from_original_file(city_name, input_path=None, output_path=None, city_boundary=None):\n    checkins_file = os.path.join(input_path, \"dataset_TIST2015_Checkins.txt\")\n    pois_file = os.path.join(input_path, \"dataset_TIST2015_POIs.txt\")\n\n    # 城市的四个边界坐标\n    (lon1, lat1), (lon2, lat2), (lon3, lat3), (lon4, lat4) = city_boundary[city_name]\n\n    # 计算最小和最大经纬度\n    min_lon = min(lon1, lon2, lon3, lon4)\n    max_lon = max(lon1, lon2, lon3, lon4)\n    min_lat = min(lat1, lat2, lat3, lat4)\n    max_lat = max(lat1, lat2, lat3, lat4)\n\n    pois_df = pd.read_csv(pois_file, sep='\\t', header=None, names=[\n        \"Venue ID\", \"Latitude\", \"Longitude\", \"Venue Category Name\", \"Country Code\"\n    ])\n    filtered_pois = pois_df[\n        (pois_df[\"Latitude\"] <= max_lat) & (pois_df[\"Latitude\"] >= min_lat) &\n        (pois_df[\"Longitude\"] >= min_lon) & (pois_df[\"Longitude\"] <= max_lon)\n    ]\n    filtered_venue_ids_set = set(filtered_pois[\"Venue ID\"])\n    checkins_df = pd.read_csv(checkins_file, sep='\\t', header=None, names=[\n        \"User ID\", \"Venue ID\", \"UTC Time\", \"Timezone Offset\"\n    ])\n\n    filtered_checkins = checkins_df[checkins_df[\"Venue ID\"].apply(lambda x: x in filtered_venue_ids_set)]\n\n    print(\"Filtered POIs:\")\n    print(filtered_pois)\n\n    print(\"\\nFiltered Check-ins:\")\n    print(filtered_checkins)\n\n    filtered_pois.to_csv(os.path.join(output_path, \"{}_filtered_pois.csv\".format(city_name)), index=False)\n    filtered_checkins.to_csv(os.path.join(output_path, \"{}_filtered_checkins.csv\".format(city_name)), index=False)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--city_name\", type=str, default=\"Beijing\")\n    parser.add_argument(\"--input_path\", type=str, default=\"citydata/mobility\")\n    parser.add_argument('--filter_path', type=str, default=\"citydata/mobility/checkin\")\n    parser.add_argument(\"--output_path\", type=str, default=\"citydata/mobility/checkin_split\")\n    parser.add_argument(\"--final_output\", type=str, default=\"citydata/mobility/checkin_test_pk\")\n    parser.add_argument(\"--repeat_THR\", type=float, default=0.05)\n    parser.add_argument(\"--context_step\", type=int, default=40)\n    parser.add_argument(\"--historical_step\", type=int, default=5)\n    parser.add_argument(\"--step\", type=int, default=1)\n    args = parser.parse_args()\n\n    historical_step = args.historical_step\n    context_step = args.context_step\n    step = args.step\n    repeat_THR = args.repeat_THR\n    input_path = args.input_path\n    filter_path = args.filter_path\n    output_path = args.output_path\n    final_output = args.final_output\n\n    filter_checkints_from_original_file(\n        city_name=args.city_name,\n        input_path=input_path,\n        output_path=filter_path,\n        bounds_dict=CITY_BOUNDARY\n        )\n\n    datas = os.listdir(filter_path)\n    poi_checkin = []\n    city_type = {}\n    for data in datas:\n        city  = data.split(\"_\")[0]\n        type_ = data.split(\"_\")[-1]\n        if city not in city_type:\n            city_type[city] = {'pois.csv': \"\", \"checkins.csv\": \"\"}\n        city_type[city][type_] = data\n    for type_dict in city_type.values():\n        poi_checkin.append((type_dict['pois.csv'],type_dict['checkins.csv']))\n        \n    for poi, checkin in poi_checkin:\n        city = poi.split(\"_\")[0]\n        city_poi = pd.read_csv(os.path.join(filter_path, poi))\n        city_checkin = pd.read_csv(os.path.join(filter_path, checkin))\n        result = match_pos(city_poi, city_checkin)\n        result['start_min'] = result.apply(lambda x: get_time(x.utc), axis=1)\n        result['start_day'] = result.apply(lambda x: get_start_day(x.utc), axis=1)\n        result['week_day'] = result.apply(lambda x: get_day(x.utc), axis=1)\n        result['location_id'], unique_venues = pd.factorize(result['location'])\n        get_split(result, city, output_path, repeat_THR, context_step, historical_step)\n        print('{} Done!'.format(city))\n\n    output_data = os.listdir(output_path)\n    for data in output_data:\n        city = data.split(\"_\")[0]\n        data_type = data.split(\"_\")[-1]\n        if data_type == \"test.csv\":\n            tar_data = pd.read_csv(os.path.join(output_path, data))\n            result = get_test_dict(tar_data, historical_step)\n            output_name = os.path.join(final_output, '{}_fin'.format(city))\n            with open('{}.pk'.format(output_name), 'wb') as f:\n                pickle.dump(result, f)\n"}
{"type": "source_file", "path": "citybench/outdoor_navigation/data_gen.py", "content": "import os\nimport argparse\nimport numpy as np\nimport pandas as pd\nimport random\nimport jsonlines\nimport asyncio\nimport signal\nfrom multiprocessing import Pool \nfrom tqdm import tqdm\n\nfrom pycitydata.map import Map\nfrom citysim.routing import RoutingClient\nfrom citysim.player import Player\n\nfrom collections import Counter\nfrom global_utils import load_map\nfrom config import RESOURCE_PATH, RESULTS_PATH, MAP_CACHE_PATH, ROUTING_PATH, MAP_DICT, IMAGE_FOLDER, STEP, REGION_CODE\n\ndef find_image_file(city, image_folder, sid_84_long, sid_84_lat, sid):\n    # stitching the picture file name\n    dataset_name = REGION_CODE[city]\n    file_name = f\"{dataset_name}_{sid_84_long}_{sid_84_lat}_{sid}.jpg\"\n    file_path = os.path.join(image_folder, file_name)\n    \n    if os.path.exists(file_path):\n        return file_name\n    else:\n        return None\n    \n# 将图片与道路关联起来\ndef process_road_id_google(args):\n    road_id, city, city_map, meta_info_df = args\n    lng = []\n    lat = []\n    distance_list = []  \n    matched_data = []\n    \n    road_info = city_map.get_road(road_id)\n    lane_id = road_info['lane_ids'][0]\n    lane_info = city_map.get_lane(lane_id)\n    lane_len = lane_info['length']\n    start_lng, start_lat = lane_info['shapely_lnglat'].coords[0][0], lane_info['shapely_lnglat'].coords[0][1]\n    end_lng, end_lat = lane_info['shapely_lnglat'].coords[-1][0], lane_info['shapely_lnglat'].coords[-1][1]\n    # start point\n    lng.append(start_lng)\n    lat.append(start_lat)\n    distance_list.append(0)  \n    # end point\n    lng.append(end_lng)\n    lat.append(end_lat)\n    distance_list.append(int(lane_len))  # 终点的距离为 lane_len\n    # print(f\"s: {start_lng}, {start_lat}, e: {end_lng}, {end_lat}\")\n    # Intermediate points\n    for ds in range(STEP, int(lane_len), STEP):\n        xy = lane_info[\"shapely_xy\"].interpolate(ds)\n        x, y = xy.coords[0][0], xy.coords[0][1]\n        lng_in, lat_in = city_map.xy2lnglat(x, y)\n        lng.append(lng_in)\n        lat.append(lat_in)\n        distance_list.append(ds)  # 中间点的距离为插值距离 ds\n\n    # 检查图片是否存在\n    for lng_val, lat_val, dist_val in zip(lng, lat, distance_list):\n        rounded_lat_val = np.round(float(lat_val), 4)\n        rounded_lng_val = np.round(float(lng_val), 4)\n        \n        # 遍历 meta_info_df 中的每一行，直接从其中提取数据\n        for idx, row in meta_info_df.iterrows():\n            query_lati = float(row['query_lati'])\n            query_longti = float(row['query_longti'])\n            file_name = row['file_name']  \n            \n            # 比较经纬度是否匹配\n            if abs(rounded_lng_val - query_longti) < 1e-4 and abs(rounded_lat_val - query_lati) < 1e-4:\n                # print(\"match\")\n                matched_data.append({\n                    \"road_id\": road_id,\n                    \"lane_id\": lane_id,\n                    \"file_name\": file_name,\n                    \"distance\": dist_val\n                })\n                break  # 匹配到一张图片即可，继续下一个坐标\n    return matched_data\n\n\ndef process_road_id_baidu(args):\n    road_id, city, city_map, meta_info_df = args\n    lng = []\n    lat = []\n    distance_list = []  \n    matched_data = []\n    mapping_dict = {\n    (row['longitude_origin'], row['latitude_origin']): (row['sid_84_long'], row['sid_84_lat'], row['sid'])\n    for idx, row in meta_info_df.iterrows()\n    } \n    road_info = city_map.get_road(road_id)\n    lane_id = road_info['lane_ids'][0]\n    lane_info = city_map.get_lane(lane_id)\n    lane_len = lane_info['length']\n    start_lng, start_lat = lane_info['shapely_lnglat'].coords[0][0], lane_info['shapely_lnglat'].coords[0][1]\n    end_lng, end_lat = lane_info['shapely_lnglat'].coords[-1][0], lane_info['shapely_lnglat'].coords[-1][1]\n    # start point\n    lng.append(start_lng)\n    lat.append(start_lat)\n    distance_list.append(0)  \n    # end point\n    lng.append(end_lng)\n    lat.append(end_lat)\n    distance_list.append(int(lane_len))  # 终点的距离为 lane_len\n    # print(f\"s: {start_lng}, {start_lat}, e: {end_lng}, {end_lat}\")\n    # Intermediate point\n    for ds in range(STEP, int(lane_len), STEP):\n        xy = lane_info[\"shapely_xy\"].interpolate(ds)\n        x, y = xy.coords[0][0], xy.coords[0][1]\n        lng_in, lat_in = city_map.xy2lnglat(x, y)\n        lng.append(lng_in)\n        lat.append(lat_in)\n        distance_list.append(ds)  # 中间点的距离为插值距离 ds\n\n    # 检查图片是否存在\n    for lng_val, lat_val, dist_val in zip(lng, lat, distance_list):\n        rounded_lat_val = np.round(float(lat_val), 4)\n        rounded_lng_val = np.round(float(lng_val), 4)\n        \n        if (rounded_lng_val, rounded_lat_val) in mapping_dict:\n            sid_84_long, sid_84_lat, sid = mapping_dict[(rounded_lng_val, rounded_lat_val)]\n            file_name = find_image_file(city, IMAGE_FOLDER, sid_84_long, sid_84_lat, sid)\n            if file_name:\n                matched_data.append({\n                    \"road_id\": road_id,\n                    \"lane_id\": lane_id,\n                    \"file_name\": file_name,\n                    \"distance\": dist_val\n                })\n                print(f\"match\")\n        else:\n            print(f\"no match for {rounded_lng_val}, {rounded_lat_val}\")\n            continue\n\n    # print(f\"road {road_id} done\")\n    return matched_data\n\n\n# 将图片与道路关联起来（并行版本）\ndef image_connect_road(city, city_map, match_path, num_workers=8):\n    # meta_file存储文件名、经纬度等信息\n    meta_file = os.path.join(IMAGE_FOLDER, f\"{city}_StreetView_Images/combined_stitch_meta_info.csv\")\n    road_df = pd.read_csv(os.path.join(RESOURCE_PATH, '{}_roads.csv'.format(city)))\n    meta_info_df = pd.read_csv(meta_file)\n    \n    args_list = [(road_id, city, city_map, meta_info_df) for road_id in road_df['road_id']]\n    # 并行处理\n    matched_data = []\n    if city == \"Beijing\" or city == \"Shanghai\":\n         with Pool(processes=num_workers) as pool:\n            for result in tqdm(pool.imap(process_road_id_baidu, args_list), total=len(args_list)):\n                if result:\n                    matched_data.extend(result)\n    else:\n        with Pool(processes=num_workers) as pool:\n            for result in tqdm(pool.imap(process_road_id_google, args_list), total=len(args_list)):\n                if result:\n                    matched_data.extend(result)\n    \n    if matched_data:\n        matched_df = pd.DataFrame(matched_data)\n        matched_df.to_csv(match_path, index=False)\n    else:\n        print(\"No matched data found\")\n\n\nasync def generate_tasks_nav(city, city_map, routing_client, aoi_file, task_file, match_file):\n    search_type = \"poi\"\n    print(f\"Generating navigation tasks for {city}\")\n    match_data_df = pd.read_csv(match_file) \n    road_id_to_count = match_data_df['road_id'].value_counts().to_dict()\n    aois_data = pd.read_csv(aoi_file)\n\n    aois_data_info = []\n    for aoi_id in aois_data.aoi_id.to_list():\n        info = city_map.get_aoi(id=aoi_id)\n        aois_data_info.append(info)\n\n    random.shuffle(aois_data_info)\n    navigation_task = []\n    for aoi in aois_data_info:\n        start_aoi_id = aoi[\"id\"]\n        # 过滤掉与 start_aoi 相同的aoi\n        filtered_candidates = [aoi_id for aoi_id in aois_data.aoi_id.to_list() if aoi_id != start_aoi_id]\n        # 保证有足够的候选名称供随机抽样\n        for dest_aoi_id in random.sample(filtered_candidates, k=5):\n            # 从出发AOI中随机选择POI, 并记录其位置\n            player = Player(city_map=city_map, city_routing_client=routing_client, init_aoi_id=start_aoi_id, search_type=search_type)\n            try:\n                route = await player.get_driving_route(dest_aoi_id)\n                # print(\"route ok\")\n            except Exception as e:\n                print(f\"Error: {e}\")\n                continue\n            if route is None:\n                continue\n            \n            # 初始化计数器，记录总共的图片匹配数\n            total_match_count = 0\n            road_collection = route[\"road_ids\"]  # 路径上的道路 ID 列表\n\n            # 如果路径经过的道路超过4条，则跳过\n            if len(road_collection) > 5:\n                continue\n\n            # 遍历每条道路，累积匹配的文件数\n            for road_id in road_collection:\n                total_match_count += road_id_to_count.get(road_id, 0)  # 累积匹配到的图片数\n\n            # 如果累积的匹配文件数大于道路数，则将该路径保存到 navigation_task\n            if total_match_count > len(road_collection):\n                navigation_task.append({\n                    \"start_aoi_id\": start_aoi_id,\n                    \"dest_aoi_id\": dest_aoi_id,\n                    \"road_ids\": road_collection\n                })\n\n    with jsonlines.open(task_file, mode='w') as writer:\n        for task in navigation_task:\n            writer.write(task)\n    print(f\"Navigation tasks generated for {city} finished\")\n\n\nasync def main(args):\n    city_map = MAP_DICT[args.city_name]\n    m, process, routing_client = load_map(\n        city_map=city_map, \n        cache_dir=MAP_CACHE_PATH, \n        routing_path=ROUTING_PATH, \n        port=args.port)\n    aoi_file = os.path.join(RESOURCE_PATH, \"{}_aois_visual.csv\".format(args.city_name))\n    match_file = \"citydata/outdoor_navigation_tasks/{}_matched_images.csv\".format(args.city_name)\n    navigation_task_file = \"citydata/outdoor_navigation_tasks/{}_navigation_tasks.jsonl\".format(args.city_name)\n    os.makedirs(os.path.dirname(navigation_task_file), exist_ok=True)\n\n    # 将图片与道路关联起来\n    image_connect_road(args.city_name, m, match_file, num_workers=20)\n    # 生成导航任务\n    await generate_tasks_nav(\n        city=args.city_name,\n        city_map=m,\n        routing_client=routing_client,\n        aoi_file=aoi_file,\n        task_file=navigation_task_file,\n        match_file=match_file\n    )\n\n    print(\"send signal\")\n    process.send_signal(sig=signal.SIGTERM)\n    process.wait()\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--city_name\", type=str, default=\"NewYork\")\n    parser.add_argument(\"--port\", type=int, default=52562)\n    args = parser.parse_args()\n    asyncio.run(main(args))\n"}
{"type": "source_file", "path": "citybench/remote_sensing/prepare_image_and_pop.py", "content": "import os\nfrom math import floor \nimport argparse\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport geopandas as gpd\nfrom shapely.geometry import Polygon\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom osgeo import gdal\n\nfrom pycitydata.sateimg import download_all_tiles\nfrom config import CITY_BOUNDARY, WORLD_POP_DATA_PATH, REMOTE_SENSING_PATH, ARCGIS_TILE_URL, REMOTE_SENSING_ZOOM_LEVEL\nfrom .utils import deg2num, compute_tile_coordinates, create_tile_polygons, num2deg\n\n\ndef download_pop(city_name):\n    gdf = gpd.GeoDataFrame({'geometry': [Polygon(CITY_BOUNDARY[city_name])]})\n    gdf.crs = {'init': 'epsg:4326'}\n    temp_shp = gdf\n    temp_shp = temp_shp.to_crs({'init': 'epsg:4326'})\n    max_y = deg2num(min(temp_shp.bounds.minx),min(temp_shp.bounds.miny))[1]+5\n    min_y = deg2num(max(temp_shp.bounds.maxx),max(temp_shp.bounds.maxy))[1]-5\n    max_x = deg2num(max(temp_shp.bounds.maxx),max(temp_shp.bounds.maxy))[0]+5\n    min_x = deg2num(min(temp_shp.bounds.minx),min(temp_shp.bounds.miny))[0]-5\n    lon_arr, lat_arr,x_arr, y_arr = compute_tile_coordinates(min_x, max_x, min_y, max_y)\n    one_street_shp_x_y = create_tile_polygons(lon_arr, lat_arr,x_arr, y_arr)\n    # op is replaced by predicate since geopandas 1.0\n    intersection = gpd.sjoin(one_street_shp_x_y, temp_shp, op='intersects')\n    # intersection = gpd.sjoin(one_street_shp_x_y, temp_shp, predicate='intersects')\n\n    sample_img_list = intersection.y_x\n    sample_img_list = list(sample_img_list)\n\n    gdal.AllRegister()\n    pop_tiff_path = WORLD_POP_DATA_PATH\n    if pop_tiff_path != None:\n        pop_dataset = gdal.Open(pop_tiff_path)\n        pop_adfGeoTransform = pop_dataset.GetGeoTransform()\n\n    img_indicators_list = []\n    pred_pop = []\n    for tile in tqdm(sample_img_list):\n        one_img_set = {}\n        one_img_set['img_name'] = tile\n        x=int(tile.split('_')[1].split('.')[0])\n        y=int(tile.split('_')[0])\n        lat_max,lng_min=num2deg(x,y,zoom=15)\n        _,lng_max=num2deg(x+1,y,zoom=15)\n        lat_min,_=num2deg(x,y+1,zoom=15)\n\n        lat_mean = (lat_max+lat_min)/2\n        lng_mean = (lng_max+lng_min)/2\n        one_img_set['lat'] = lat_mean\n        one_img_set['lng'] = lng_mean\n        \n        #worldpop \n        if  pop_tiff_path!=None:\n            y_init = int(floor((lat_max-pop_adfGeoTransform[3])/pop_adfGeoTransform[5]))\n            x_init = int(floor((lng_min-pop_adfGeoTransform[0])/pop_adfGeoTransform[1]))\n            x_end = int(floor((lng_max-pop_adfGeoTransform[0])/pop_adfGeoTransform[1]))\n            y_end = int(floor((lat_min-pop_adfGeoTransform[3])/pop_adfGeoTransform[5]))\n            band = pop_dataset.GetRasterBand(1)\n            data=band.ReadAsArray(x_init,y_init,x_end-x_init,y_end-y_init)\n            data[data<0] = 0\n            pred_pop.append(np.array(data.sum()).tolist())\n            one_img_set['worldpop'] = np.array(data.sum()).tolist()\n\n    img_indicators_df = pd.DataFrame(img_indicators_list)\n    img_indicators_df.to_csv(os.path.join(REMOTE_SENSING_PATH, city_name+'_img_indicators.csv'), index=False)\n\n\ndef downlad_rs(city_name, zoom_level):\n    \"\"\"download remote sensing images from \"\"\"\n    try:\n        img_info_df = pd.read_csv(os.path.join(REMOTE_SENSING_PATH, city_name+'_img_indicators.csv'))\n    except FileNotFoundError:\n        print(\"please first generate input files for downloading data by assigning data_type as rs\")\n        exit(0)\n    need_to_download = img_info_df[\"img_name\"].tolist()\n\n    # 12428\n    # time dependent url parameters\n    city_url_map = {\n        \"Beijing\": \"27659/\",    #2021-04-28\n        \"Shanghai\": \"16749/\",   #2021-10-13\n        \"Mumbai\": \"18289/\",     #2020-07-01\n        \"Tokyo\": \"27659/\",      #2021-04-28\n        \"London\": \"17825/\",     #2022-08-10\n        \"Paris\": \"119/\",        #2020-10-14\n        \"Moscow\": \"9812/\",      #2021-02-24\n        \"SaoPaulo\": \"29260/\",   #2020-12-16\n        \"Nairobi\": \"19187/\",    #2020-09-23\n        \"CapeTown\": \"5359/\",    #2021-03-17\n        \"Sydney\": \"5359/\",      #2021-03-17\n        \"SanFrancisco\": \"12576/\",#2019-06-05\n        \"NewYork\": \"12576/\"\n    }\n    base_url  = ARCGIS_TILE_URL + city_url_map[city_name]\n    \n    imgs, failed = download_all_tiles(\n        base_url,\n        zoom_level,\n        need_to_download,\n    )\n\n    # save the images\n    if not os.path.exists(REMOTE_SENSING_PATH):\n        os.makedirs(REMOTE_SENSING_PATH)\n    if not os.path.exists(f\"{REMOTE_SENSING_PATH}/{city_name}\"):\n        os.makedirs(f\"{REMOTE_SENSING_PATH}{city_name}\")\n    for key, img in imgs.items():\n        img.save(f\"{REMOTE_SENSING_PATH}/{city_name}/{key}.png\")\n\n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser(description='Process some integers.')\n    parser.add_argument('--city_name', type=str, default='SanFrancisco', help='city names from {}'.format(CITY_BOUNDARY.keys()))\n    parser.add_argument('--data_type', type=str, default='pop-rs', choices=[\"pop\", \"rs\", 'pop-rs'])\n    args = parser.parse_args()\n\n    if args.city_name not in CITY_BOUNDARY.keys():\n        print('City {} not found'.format(args.city_name))\n        exit(0)\n\n    if args.data_type in [\"pop\", 'pop-rs']:\n        download_pop(city_name=args.city_name)\n    if args.data_type in [\"rs\", 'pop-rs']:\n        downlad_rs(city_name=args.city_name, zoom_level=REMOTE_SENSING_ZOOM_LEVEL)\n"}
{"type": "source_file", "path": "citybench/remote_sensing/metrics.py", "content": "import os\nimport re\nimport json\nimport glob\nimport argparse\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np\nimport sklearn.metrics as metrics\nfrom config import REMOTE_SENSING_PATH, REMOTE_SENSING_RESULTS_PATH, VLM_MODELS, CITY_BOUNDARY\n\n\ndef compute_accuracy_regression(pred_list, true_list):\n    mse = metrics.mean_squared_error(true_list, pred_list)\n    mae = metrics.mean_absolute_error(true_list, pred_list)\n    r2 = metrics.r2_score(true_list, pred_list)\n    \n    rmse = metrics.mean_squared_error(true_list, pred_list, squared=False)\n    return mse, mae, r2, rmse\n\n\ndef normalized_fractional_ranking(numbers):\n    sorted_numbers = sorted(enumerate(numbers), key=lambda x: x[1])\n\n    ranks = {}\n    for rank, (original_index, number) in enumerate(sorted_numbers):\n        if number in ranks:\n            ranks[number][0] += rank + 1\n            ranks[number][1] += 1\n        else:\n            ranks[number] = [rank + 1, 1]\n\n    average_ranks = {number: total_rank / count for number, (total_rank, count) in ranks.items()}\n\n    return [(average_ranks[number] - 1) / len(numbers) for number in numbers]\n\n\ndef load_json_file(json_file_path):\n    data = []\n    with open(json_file_path, 'r') as file:\n        for line in file:\n            data.append(json.loads(line))\n    return data\n\n\ndef get_city_and_model_from_filename(folder_path):\n    # 从文件名提取城市名和模型名\n    pattern = re.compile(r\"([A-Za-z]+(?:[A-Za-z ]+)*?)_([A-Za-z0-9\\-_]+)_.*\\.jsonl\")\n    city_model_pairs = set()\n    for file in glob.glob(os.path.join(folder_path, \"*.jsonl\")):\n        match = pattern.search(file)\n        if match:\n            city_name = match.group(1).replace(\" \", \"\")  \n            model_name = match.group(2)\n            city_model_pairs.add((city_name, model_name))\n    \n    return list(city_model_pairs)\n\n\ndef eval_pop(city_name_list, model_name_list, save_name):\n    all_csv_file_name = glob.glob(os.path.join(REMOTE_SENSING_PATH, '*_img_indicators.csv'))\n\n    all_csv_df = pd.DataFrame()\n    for i in all_csv_file_name:\n        df = pd.read_csv(i)\n        all_csv_df = pd.concat([all_csv_df, df], axis=0)\n        \n    all_csv_df.reset_index(drop=True, inplace=True)\n\n    gt_ranking = normalized_fractional_ranking(all_csv_df['worldpop'].values)\n    gt_ranking = [int(r * 100.0) / 10.0 if r < 1.0 else 9.9 for r in gt_ranking]\n    # give the ranking to the dataframe\n    all_csv_df[\"rank\"] = gt_ranking\n    img_name_list = all_csv_df[\"img_name\"].to_list()\n\n    all_r2_list = []\n    all_mae_list = []\n    all_rmse_list = []\n    all_mse_list = []\n    all_city_pred_list = []\n    all_model_name_list = []\n\n    for model_name in model_name_list:\n        all_city_pred = []\n        all_city_true = []\n\n        for city_name in city_name_list:\n            print(model_name, city_name)\n            try:\n                json_file_path = os.path.join(REMOTE_SENSING_RESULTS_PATH, city_name+'_'+model_name+'_pop.jsonl')\n                data = load_json_file(json_file_path)\n            except FileNotFoundError as e:\n                try:\n                    json_file_path = os.path.join(REMOTE_SENSING_RESULTS_PATH, city_name+'_'+model_name+'_population.jsonl')\n                    data = load_json_file(json_file_path)\n                except FileNotFoundError as e:\n                    print(\"File not found! City:{} Model:{}\".format(city_name, model_name))\n                    continue\n\n            pred_list = [] \n            true_list = [] \n            for item in data:\n                text = str(item['text'])\n                img_name = item['img_name'].split('/')[-1].split('.')[0]\n                match = re.search(r\"(\\d+\\.\\d+)\", text)\n                if img_name not in img_name_list:\n                    continue\n\n                if match:\n                    \n                    rating = float(match.group(0))\n                    pred_list.append(rating)\n                    true_list.append(all_csv_df[all_csv_df['img_name']==img_name]['rank'].values[0])\n\n            mse, mae, r2, rmse = compute_accuracy_regression(pred_list, true_list)\n            city_name = city_name.replace(' ', '')\n\n            \n            all_r2_list.append(r2)\n            all_mae_list.append(mae)\n            all_rmse_list.append(rmse)\n            all_mse_list.append(mse)\n            all_model_name_list.append(model_name)\n            all_city_pred_list.append(city_name)\n            \n            all_city_pred.extend(pred_list)\n            all_city_true.extend(true_list)\n                \n        all_city_mse, all_city_mae, all_city_r2, all_city_rmse = compute_accuracy_regression(all_city_pred, all_city_true)\n        \n        all_mse_list.append(all_city_mse)\n        all_r2_list.append(all_city_r2)\n        all_mae_list.append(all_city_mae)\n        all_rmse_list.append(all_city_rmse)\n        all_model_name_list.append(model_name)\n        all_city_pred_list.append('all_city')\n        print(model_name, all_city_r2)\n\n    df = pd.DataFrame({'City_Name': all_city_pred_list, 'Model_Name': all_model_name_list, 'r2': all_r2_list, \\\n            'MAE': all_mae_list, 'RMSE': all_rmse_list, 'MSE': all_mse_list})\n    df.to_csv(os.path.join(REMOTE_SENSING_RESULTS_PATH, save_name), index=False)\n\n\ndef eval_object(city_name_list, model_name_list, save_name):\n    with open(os.path.join(REMOTE_SENSING_PATH, 'all_city_img_object_set.json'),'r') as f:\n        all_city_img_object_set = json.load(f)\n    \n    select_category_list = [\n        \"Bridge\", \"Stadium\", \"Ground Track Field\", \"Baseball Field\", \\\n        \"Overpass\", \"Airport\", \"Golf Field\", \"Storage Tank\", \\\n        \"Roundabout\", \"Swimming Pool\", \"Soccer Ball Field\", \"Harbor\", \"Tennis Court\", \\\n        \"Windmill\", \"Basketball Court\", \"Dam\", \"Train Station\"]\n\n    select_category_list = sorted(select_category_list)\n\n    all_city_accurecy_list = []\n    all_city_precision_list = []\n    all_city_recall_list = []\n    all_city_f1_list = []\n    all_city_pred_list = []\n    all_city_sub_category_list = []\n    all_model_name_list = []\n    \n    # TODO need to prepare data, due to the absence of safegraph\n    try:\n        city_name_list.remove(\"Shanghai\")\n        city_name_list.remove(\"Beijing\")\n    except ValueError as e:\n        pass\n\n    for one_city_name in city_name_list:\n        sample_df = pd.read_csv(os.path.join(REMOTE_SENSING_PATH, one_city_name+'_img_indicators.csv'))\n        img_name_list = sample_df[\"img_name\"].to_list()\n\n        for model_name in model_name_list:\n            print(model_name, one_city_name)\n            pred_city_set = dict()\n            try:\n                json_file_path = os.path.join(REMOTE_SENSING_RESULTS_PATH, one_city_name+'_'+model_name+'_object.jsonl')\n                data = load_json_file(json_file_path)\n            except FileNotFoundError as e:\n                try:\n                    json_file_path = os.path.join(REMOTE_SENSING_RESULTS_PATH, one_city_name+'_'+model_name+'_objects.jsonl')\n                    data = load_json_file(json_file_path)\n                except FileNotFoundError as e:\n                    print(\"File not found! City:{} Model:{}\".format(one_city_name, model_name))\n                    continue\n\n            if one_city_name not in pred_city_set:\n                pred_city_set[one_city_name] = dict()\n\n            for item in data:\n                img_name = item['img_name'].split('.')[0]\n                if img_name not in img_name_list:\n                    continue\n\n                if img_name not in pred_city_set[one_city_name]:\n                    pred_city_set[one_city_name][img_name] = dict()\n                text = item['text']\n                for obj in select_category_list:\n                    if obj in text:\n                        pred_city_set[one_city_name][img_name][obj] = 1\n                    else:\n                        pred_city_set[one_city_name][img_name][obj] = 0      \n\n            for one_sub_category in select_category_list:\n                one_city_pred = []\n                one_city_true = []\n                for img_name in pred_city_set[one_city_name]:\n                    if img_name in all_city_img_object_set.keys():\n                        one_city_pred.append(pred_city_set[one_city_name][img_name][one_sub_category])\n                        one_city_true.append(all_city_img_object_set[img_name][one_sub_category])\n\n                # cal the accuracy, precision, recall, f1 get report\n                all_city_precision_list.append(metrics.precision_score(one_city_true, one_city_pred))\n                all_city_recall_list.append(metrics.recall_score(one_city_true, one_city_pred))\n                all_city_f1_list.append(metrics.f1_score(one_city_true, one_city_pred))\n                all_city_accurecy_list.append(metrics.accuracy_score(one_city_true, one_city_pred))\n            \n                all_city_pred_list.append(one_city_name)\n                all_city_sub_category_list.append(one_sub_category)\n                all_model_name_list.append(model_name)\n\n    df = pd.DataFrame()\n    df[\"sub_category\"] = all_city_sub_category_list\n    df[\"Precision\"] = all_city_precision_list\n    df[\"Recall\"] = all_city_recall_list\n    df[\"f1\"] = all_city_f1_list\n    df[\"Infrastructure_Accuracy\"] = all_city_accurecy_list\n    df[\"City_Name\"] = all_city_pred_list\n    df[\"Model_Name\"] = all_model_name_list\n    df.to_csv(os.path.join(REMOTE_SENSING_RESULTS_PATH, save_name), index=False)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='Process some integers.')\n    parser.add_argument(\"--task_name\", type=str, default=\"all\", choices=[\"population\", \"objects\", \"all\"])\n    args = parser.parse_args()\n\n    # 获取 city_name 和 model_name\n    city_model_pairs = get_city_and_model_from_filename(REMOTE_SENSING_RESULTS_PATH)\n    city_name_list = list(set([pair[0] for pair in city_model_pairs]))\n    model_name_list = list(set([pair[1] for pair in city_model_pairs]))\n\n    save_name_pop = os.path.join(REMOTE_SENSING_RESULTS_PATH, \"population_benchmark_results.csv\")\n    save_name_object = os.path.join(REMOTE_SENSING_RESULTS_PATH, \"object_benchmark_results.csv\")\n\n    for city in city_name_list:\n        assert city in CITY_BOUNDARY.keys()\n\n    for model_name in model_name_list:\n        assert model_name in VLM_MODELS\n    \n    if args.task_name in [\"objects\", \"all\"]:\n        eval_object(city_name_list=city_name_list, model_name_list=model_name_list, save_name=save_name_object)\n    if args.task_name in [\"population\", \"all\"]:\n        eval_pop(city_name_list=city_name_list, model_name_list=model_name_list, save_name=save_name_pop)\n"}
{"type": "source_file", "path": "citybench/street_view/VLMEvalKit.py", "content": "import os\nimport json\nimport random\nfrom tqdm import tqdm\n# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\n'''\nmodels_to_test = [\n    'llava_next_llama3',\n    'InternVL2-40B',\n    'cogvlm2-llama3-chat-19B',\n    'MiniCPM-Llama3-V-2_5',\n    'llava_next_yi_34b'\n]\ncity_names = ['CapeTown', 'London', 'Moscow', 'Mumbai', 'Paris', 'SaoPaulo', 'Sydney', 'Tokyo', 'NewYork', 'SanFrancisco', 'Beijing', 'Shanghai', 'Nairobi']\n'''\n\nmodel_name = \"llava_next_llama3\"\ncity_name = \"Paris\"\nnum_test = 100\n\nprompt = \"Suppose you are an expert in geo-localization. Please first analyze which city is this image taken from, and then make a prediction of the longitude and latitude value of its location. \\\nyou can choose among: 'Los Angeles', 'Nakuru', 'Johannesburg', 'Rio de Janeiro', 'CapeTown', 'London', 'Moscow', 'Mumbai', 'Paris', 'Sao Paulo', 'Sydney', 'Tokyo', 'New York', 'Guangzhou', 'Kyoto', 'Melbourne', 'San Francisco', 'Nairobi', 'Beijing', 'Shanghai', 'Bangalore', 'Marseille', 'Manchester', 'Saint Petersburg'.\\\n    Only answer with the city name and location. Do not output any explanational sentences. Example Answer: Los Angeles. (34.148331, -118.324755).\"   \n\nfolder_path = f\"./data/{city_name}\"\nresult_folder_path = \"./results\"\n\nmodel_result_path = os.path.join(result_folder_path, model_name)\nos.makedirs(model_result_path, exist_ok=True)\nprint(f\"Testing model: {model_name}\")\n\nif model_name in ['InternVL2-40B', 'cogvlm2-llama3-chat-19B']:\n    os.system('python -m pip install transformers==4.37.0')\nelif model_name in ['llava_next_llama3', 'MiniCPM-Llama3-V-2_5', 'llava_next_yi_34b']:\n    os.system('python -m pip install transformers==4.44.0')\nfrom vlmeval.config import supported_VLM\nmodel = supported_VLM[model_name]()\n\nfor image in os.listdir(folder_path)[:num_test]:\n    image_path = os.path.join(folder_path, image)\n    question = prompt\n\n    response = model.generate([image_path, question])\n\n    with open(os.path.join(model_result_path, f\"{city_name}_test_result.jsonl\"), \"a\") as f:\n        f.write(json.dumps({\n            \"image_path\": image_path,\n            \"response\": response\n        }) + \"\\n\")\n\nprint(f\"{city_name} done!\")"}
{"type": "source_file", "path": "citybench/street_view/metrics.py", "content": "import json\nimport csv\nimport os\nimport re\nimport math\nimport numpy as np\nfrom config import RESULTS_PATH\n\ndef getDist_points(q, v):\n    try:\n        EARTH_REDIUS = 6378.137\n        v=np.array(v).reshape(-1,2)\n        q=np.array(q).reshape(-1,2)\n        v, q = np.deg2rad(v), np.deg2rad(q)\n        \n        lat1, lat2 = v[:,1].reshape(-1,1), q[:,1].reshape(-1,1)\n        lng1, lng2 = v[:,0].reshape(-1,1), q[:,0].reshape(-1,1)\n        sin_lats = np.dot(np.sin(lat1), np.sin(lat2.T))\n        cos_lats = np.dot(np.cos(lat1), np.cos(lat2.T))\n        cos_lng_diff = np.cos(lng2.reshape(-1) - lng1.reshape(-1,1))\n        s = np.arccos(sin_lats + cos_lats*cos_lng_diff)\n        d = s * EARTH_REDIUS ###* 1000\n        d = d.T # q to v\n        return d\n    except:\n        print('error')\n\n#like above way to compute the distance\ndef get_two_point_distance(lat1, lon1, lat2, lon2):\n    EARTH_REDIUS = 6378.137\n    \n    #deg2rad\n    \n    sin_lats = math.sin(math.radians(lat1)) * math.sin(math.radians(lat2))\n    cos_lats = math.cos(math.radians(lat1)) * math.cos(math.radians(lat2))\n    cos_lng_diff = math.cos(math.radians(lon2) - math.radians(lon1))\n    s = math.acos(sin_lats + cos_lats * cos_lng_diff)\n    d = s * EARTH_REDIUS\n    return d\n\ndef compute_accuracy_planet(pred_coordinates_list, true_coordinates_list):\n    accuracy_planet = {\n        \"1km\": 0,\n        \"25km\": 0,\n    }\n    for idx in range(len(pred_coordinates_list)):\n\n        distance_km = getDist_points([pred_coordinates_list[idx][1], pred_coordinates_list[idx][0]], [true_coordinates_list[idx][1], true_coordinates_list[idx][0]])\n        distance_km=distance_km[0][0]\n        if distance_km <= 1:\n            accuracy_planet[\"1km\"] += 1\n        if distance_km <= 25:\n            accuracy_planet[\"25km\"] += 1\n\n    for k in accuracy_planet.keys():\n        accuracy_planet[k] /= len(true_coordinates_list)\n        accuracy_planet[k] *= 100.0 # percent\n\n    return accuracy_planet\n\n\ndef calculate_acc(results_path, output_file):\n    # Initialize the results list\n    results = []\n\n    # Calculate accuracy for each city and model\n    for root, dirs, files in os.walk(results_path):\n        for file in files:\n            # 使用正则表达式匹配 {city_name}_{model_name}_geoloc.jsonl 文件\n            match = re.match(r\"([A-Za-z]+)_(.+)_geoloc.jsonl\", file)\n            if match:\n                city_name = match.group(1)\n                model_name = match.group(2)\n                city_results = [model_name, city_name]\n\n                try:\n                    with open(os.path.join(root, file), \"r\") as f:\n                        img2cityloc = [json.loads(line) for line in f]\n\n                    # Coordinates accuracy calculation\n                    if city_name == \"Shanghai\" or city_name == \"Beijing\":\n                        true_lng = [float(data['image_path'].split(\"&\")[1]) for data in img2cityloc]\n                        true_lat = [float(data['image_path'].split(\"&\")[2]) for data in img2cityloc]\n                        true_coords = list(zip(true_lat, true_lng))\n\n                    else:\n                        true_lat = [float(data['image_path'].split(\"&\")[1]) for data in img2cityloc]\n                        true_lng = [float(data['image_path'].split(\"&\")[2]) for data in img2cityloc]\n                        true_coords = list(zip(true_lat, true_lng))\n                        \n                    pred_coords = []\n                    for data in img2cityloc:\n                        try:\n                            pred = data['response'].split(\"(\")[1].split(\")\")[0]\n                            pred_lat = float(pred.split(\", \")[0])\n                            pred_lng = float(pred.split(\", \")[1])\n                            pred_coords.append((pred_lat, pred_lng))\n                        except:\n                            pred_coords.append((0, 0))\n                    result = compute_accuracy_planet(pred_coords, true_coords)\n                    # City accuracy calculation\n                    city_count = 0\n                    for data in img2cityloc:\n                        if city_name in data['response'] or \"S\\u00e3o Paulo\" in data['response'] or \"S\\u00e3oPaulo\" in data['response']:\n                            city_count += 1\n                    city_accuracy = city_count / len(img2cityloc) if img2cityloc else 0\n\n                    city_results.append(city_accuracy) \n                    city_results.append(result[\"1km\"])  \n                    city_results.append(result[\"25km\"])  \n\n                    results.append(city_results)\n\n                except FileNotFoundError:\n                    # In case the file is not found, set accuracy as 'N/A'\n                    city_results.append('N/A')\n                    city_results.append('N/A')\n                    city_results.append('N/A')\n                    results.append(city_results)\n\n    # Write results to CSV\n    with open(output_file, mode='w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n        writer.writerow(['Model_Name', 'City_Name', 'City_Accuracy', 'Acc@1km', 'Acc@25km'])\n\n        # Write the rows\n        for row in results:\n            writer.writerow(row)\n\n    print(\"Street View results have been saved!\")\n\nif __name__ == '__main__':\n    results_path = os.path.join(RESULTS_PATH, \"street_view/\")\n    output_file = os.path.join(results_path, \"geoloc_benchmark_results.csv\")\n    calculate_acc(results_path, output_file)\n"}
{"type": "source_file", "path": "citybench/mobility_prediction/metrics.py", "content": "import os\nimport pandas as pd\nfrom sklearn.metrics import f1_score\nimport ast\nimport csv\nimport numpy as np\nfrom config import RESULTS_PATH\n\ndef get_acc1_f1(df):\n    acc1 = (df['prediction'] == df['ground_truth']).sum() / len(df)\n    acc1 = round(acc1, 9)\n    # print(f\"acc1:{acc1}\")\n    # print(f\"sum:{(df['prediction'] == df['ground_truth']).sum()}\")\n    # print(f\"len:{len(df)}\")\n    f1 = f1_score(df['ground_truth'], df['prediction'], average='weighted')\n    return acc1, f1\n\ndef get_is_correct(row):\n    pred_list = row['prediction']\n    if row['ground_truth'] in pred_list:\n        row['is_correct'] = True\n    else:\n        row['is_correct'] = False\n    \n    return row\n\n\ndef get_is_correct10(row):\n    pred_list = row['top10']\n    if row['ground_truth'] in pred_list:\n        row['is_correct10'] = True\n    else:\n        row['is_correct10'] = False\n        \n    pred_list = row['top5']\n    if row['ground_truth'] in pred_list:\n        row['is_correct5'] = True\n    else:\n        row['is_correct5'] = False\n\n    pred = row['top1']\n    if pred == row['ground_truth']:\n        row['is_correct1'] = True\n    else:\n        row['is_correct1'] = False\n    \n    return row\n\n\ndef first_nonzero(arr, axis, invalid_val=-1):\n    mask = arr!=0\n    return np.where(mask.any(axis=axis), mask.argmax(axis=axis), invalid_val)\n\n\ndef get_ndcg(prediction, targets, k=10):\n    \"\"\"\n    Calculates the NDCG score for the given predictions and targets.\n\n    Args:\n        prediction (Nxk): list of lists. the softmax output of the model.\n        targets (N): torch.LongTensor. actual target place id.\n\n    Returns:\n        the sum ndcg score\n    \"\"\"\n    for _, xi in enumerate(prediction):\n        if len(xi) < k:\n            #print(f\"the {i}th length: {len(xi)}\")\n            xi += [-5 for _ in range(k-len(xi))]\n        elif len(xi) > k:\n            xi = xi[:k]\n        else:\n            pass\n    \n    n_sample = len(prediction)\n    prediction = np.array(prediction)\n    targets = np.broadcast_to(targets.reshape(-1, 1), prediction.shape)\n    hits = first_nonzero(prediction == targets, axis=1, invalid_val=-1)\n    hits = hits[hits>=0]\n    ranks = hits + 1\n    ndcg = 1 / np.log2(ranks + 1)\n    return np.sum(ndcg) / n_sample\n\ndef safe_convert_to_int(x):\n    try:\n        # 这里尝试先将字符串转换为浮点数，然后再转换为整数\n        return int(float(x))\n    except (ValueError, TypeError):\n        # 如果转换失败（可能是非数值型字符串），返回 -100 作为占位符\n        return -100\n    \n\ndef cal_metrics(output_dir):\n    folder_name = os.path.basename(output_dir)  \n    parts = folder_name.split('_')\n\n    if len(parts) < 4:\n        print(f\"Skipping invalid folder: {folder_name}\")\n        return None, None, None, None\n    \n    model = \"_\".join(parts[:-3])  \n    city = parts[-3]\n    \n    file_list = [file for file in os.listdir(output_dir) if file.endswith('.csv')]\n    file_path_list = [os.path.join(output_dir, file) for file in file_list]\n\n    df = pd.DataFrame({\n        'user_id': None,\n        'ground_truth': None,\n        'prediction': None,\n        'reason': None\n    }, index=[])\n\n    for file_path in file_path_list:\n        iter_df = pd.read_csv(file_path)\n        df = pd.concat([df, iter_df], ignore_index=True)\n        \n    df_cleaned = df.dropna(subset=['prediction', 'ground_truth'])\n    df_cleaned['prediction'] = df_cleaned['prediction'].apply(safe_convert_to_int)\n    df_cleaned['ground_truth'] = df_cleaned['ground_truth'].apply(safe_convert_to_int)\n    \n    acc1, f1 = get_acc1_f1(df_cleaned)\n    return model, city, acc1, f1\n\ndef process_results(main_dir, csv_file):\n    with open(csv_file, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Model_Name', 'City_Name', 'Acc@1', 'F1'])\n        # 遍历主文件夹中的所有子文件夹\n        for folder in os.listdir(main_dir):\n            folder_path = os.path.join(main_dir, folder)\n            if os.path.isdir(folder_path):\n                model, city, acc1, f1 = cal_metrics(folder_path)\n                if model and city:\n                    writer.writerow([model, city, acc1, f1])\n\n    print(f\"Mobility results have been saved!\")\n\n\n\nif __name__ == \"__main__\":\n    main_dir = os.path.join(RESULTS_PATH, \"prediction_results\")\n    csv_file = os.path.join(main_dir, \"mobility_benchmark_result.csv\")\n\n    process_results(main_dir, csv_file)\n"}
{"type": "source_file", "path": "citybench/street_view/eval_inference.py", "content": "import os\nimport argparse\nimport pandas as pd\n\nfrom tqdm import tqdm\nimport json\n\nfrom config import RESULTS_PATH, STREET_VIEW_PATH\nfrom serving.vlm_serving import VLMWrapper\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model_name', type=str, default='InternVL2-40B', help='model name')\n    parser.add_argument('--city_name', type=str, default='Beijing', help='city name')\n    parser.add_argument('--data_name', type=str, default='mini', help='dataset size')\n    parser.add_argument('--task_name', type=str, default='geoloc', help='task name', choices=[\"geoloc\"])\n    args = parser.parse_args()\n\n    # Load the model\n    model_wrapper = VLMWrapper(args.model_name)\n    model = model_wrapper.get_vlm_model()\n\n    #according to the task name, set the prompt\n    if args.task_name == \"geoloc\":\n        prompt = \"\"\"Suppose you are an expert in geo-localization. Please first analyze which city is this image taken from, and then make a prediction of the longitude and latitude value of its location. \\nYou can choose among: 'Los Angeles', 'Nakuru', 'Johannesburg', 'Rio de Janeiro', 'CapeTown', 'London', 'Moscow', 'Mumbai', 'Paris', 'Sao Paulo', 'Sydney', 'Tokyo', 'New York', 'Guangzhou', 'Kyoto', 'Melbourne', 'San Francisco', 'Nairobi', 'Beijing', 'Shanghai', 'Bangalore', 'Marseille', 'Manchester', 'Saint Petersburg'. \\nOnly answer with the city name and location. Do not output any explanational sentences. Example Answer: Los Angeles. (34.148331, -118.324755).\"\"\"\n    else:\n        print(\"{} task is not supported\".format(args.task_name))\n        exit(0)\n    \n    if args.data_name == \"mini\":\n        num_test = 5\n    else:\n        num_test = 500\n\n    input_folder_path = os.path.join(STREET_VIEW_PATH, args.city_name+\"_CUT\")\n    selected_data_list = pd.read_csv(os.path.join(STREET_VIEW_PATH, args.city_name+\"_data.csv\"))[\"img_name\"].to_list()\n    result_folder_path = os.path.join(RESULTS_PATH, \"street_view\")\n    os.makedirs(result_folder_path, exist_ok=True)\n\n    # read existing data\n    try:\n        his_data = pd.read_json(os.path.join(result_folder_path, f\"{args.city_name}_{args.model_name}_geoloc.jsonl\"), lines=True)\n        his_imgs = his_data[\"image_path\"].to_list()\n        his_ress = his_data[\"response\"].to_list()\n        his_data_list = list(zip(his_imgs, his_ress))\n    except:\n        his_imgs, his_ress, his_data_list = [], [], []\n\n    print(\"City:{} Model:{} Existing Data:{}\".format(args.city_name, args.model_name, len(his_data_list)))\n\n    # generate new\n    res_list = []\n    for image in tqdm(os.listdir(input_folder_path)):\n        if image not in selected_data_list:\n            continue\n        \n        image_path = os.path.join(input_folder_path, image)\n\n        if image_path in his_imgs:\n            continue\n\n        response = model.generate([image_path, prompt])\n        res_list.append([image_path, response])\n    \n    # save data\n    with open(os.path.join(result_folder_path, f\"{args.city_name}_{args.model_name}_geoloc.jsonl\"), \"w\") as f:\n        # saving old data\n        for i in range(len(his_data_list)):\n            image_path, response = his_data_list[i]\n            f.write(json.dumps({\n                \"image_path\": image_path,\n                \"response\": response\n            }) + \"\\n\")\n\n        \n        for r in res_list:\n            image_path, response = r\n            f.write(json.dumps({\n                \"image_path\": image_path,\n                \"response\": response\n            }) + \"\\n\")\n"}
{"type": "source_file", "path": "citybench/remote_sensing/download_rs_img.py", "content": "from pycitysim.sateimg import download_all_tiles\n\n\nimport argparse\nfrom tqdm import tqdm\n\nimport pandas as pd\nimport os\nif __name__ ==\"main\":\n    parser = argparse.ArgumentParser(description='Process some integers.')\n    parser.add_argument('--city_name', type=str, default='SanFrancisco', help='city name')\n    parser.add_argument('--output_dir', type=str, default='citydata/remote_sensing', help='output directory')\n    args = parser.parse_args()\n    \n    img_info_df = pd.read_csv(args.city_name+'_img_indicators.csv')\n    need_to_download = img_info_df[\"img_name\"].tolist()\n\n    \n    zoom_level = 15\n    base_url = \"https://wayback.maptiles.arcgis.com/arcgis/rest/services/World_Imagery/WMTS/1.0.0/default028mm/MapServer/tile/27659\"\n    y_x = [\n        \"12394_26956\",\"12394_26957\",\"12398_26996\"]\n\n    if args.city_name==\"Beijing\":\n        base_url = 'https://wayback.maptiles.arcgis.com/arcgis/rest/services/World_Imagery/WMTS/1.0.0/default028mm/MapServer/tile/27659/'#2021-04-28\n    if args.city_name==\"Shanghai\":\n        base_url = 'https://wayback.maptiles.arcgis.com/arcgis/rest/services/World_Imagery/WMTS/1.0.0/default028mm/MapServer/tile/16749/'#2021-10-13\n    if args.city_name==\"Mumbai\":\n        base_url = 'https://wayback.maptiles.arcgis.com/arcgis/rest/services/World_Imagery/WMTS/1.0.0/default028mm/MapServer/tile/18289/'#2020-07-01\n    if args.city_name==\"Tokyo\":\n        base_url = 'https://wayback.maptiles.arcgis.com/arcgis/rest/services/World_Imagery/WMTS/1.0.0/default028mm/MapServer/tile/27659/'#2021-04-28\n    if args.city_name==\"London\":\n        base_url = 'https://wayback.maptiles.arcgis.com/arcgis/rest/services/World_Imagery/WMTS/1.0.0/default028mm/MapServer/tile/17825/'#2022-08-10\n    if args.city_name==\"Paris\":\n        base_url = 'https://wayback.maptiles.arcgis.com/arcgis/rest/services/World_Imagery/WMTS/1.0.0/default028mm/MapServer/tile/119/'#2020-10-14\n    if args.city_name==\"Moscow\":\n        base_url = 'https://wayback.maptiles.arcgis.com/arcgis/rest/services/World_Imagery/WMTS/1.0.0/default028mm/MapServer/tile/9812/'#2021-02-24\n    if args.city_name==\"SaoPaulo\":\n        base_url = 'https://wayback.maptiles.arcgis.com/arcgis/rest/services/World_Imagery/WMTS/1.0.0/default028mm/MapServer/tile/29260/'#2020-12-16\n    if args.city_name==\"Nairobi\":\n        base_url = 'https://wayback.maptiles.arcgis.com/arcgis/rest/services/World_Imagery/WMTS/1.0.0/default028mm/MapServer/tile/19187/'#2020-09-23\n    if args.city_name==\"CapeTown\":\n        base_url = 'https://wayback.maptiles.arcgis.com/arcgis/rest/services/World_Imagery/WMTS/1.0.0/default028mm/MapServer/tile/5359/'#2021-03-17\n    if args.city_name==\"Sydney\":\n        base_url = 'https://wayback.maptiles.arcgis.com/arcgis/rest/services/World_Imagery/WMTS/1.0.0/default028mm/MapServer/tile/5359/'#2021-03-17\n    if args.city_name==\"SanFrancisco\":\n        base_url = 'https://wayback.maptiles.arcgis.com/arcgis/rest/services/World_Imagery/WMTS/1.0.0/default028mm/MapServer/tile/12576/'\n    if args.city_name==\"NewYork\":\n        base_url = 'https://wayback.maptiles.arcgis.com/arcgis/rest/services/World_Imagery/WMTS/1.0.0/default028mm/MapServer/tile/12576/'\n    \n    \n    \n    \n    imgs, failed = download_all_tiles(\n    base_url,\n        15,\n        need_to_download,\n    )\n    # save the images\n    if not os.path.exists(args.output_dir):\n        os.makedirs(args.output_dir)\n    if not os.path.exists(f\"{args.output_dir}{args.city_name}\"):\n        os.makedirs(f\"{args.output_dir}{args.city_name}\")\n    for key, img in imgs.items():\n        img.save(f\"{args.output_dir}/{args.city_name}/{key}.png\")"}
{"type": "source_file", "path": "citybench/outdoor_navigation/crawler_baidu.py", "content": "# _*_ coding: utf-8 _*_\nimport numpy as np\nimport os\nfrom pathlib import Path\nimport time\nfrom urllib.request import urlopen\n\nimport cv2\nimport json\nimport pandas as pd\nfrom coordTransform_utils import bd09_to_wgs84\nimport ast\nimport argparse\n\nfrom config import SAMPLE_POINT_PATH, IMAGE_FOLDER\n\ndef coord_conv(long,latit):\n    #将百度gps转换为bd09mc(百度米制经纬度坐标)，API每日限额600,000\n    #API地址\n    #http://lbsyun.baidu.com/index.php?title=webapi/guide/changeposition\n    try:\n        url = 'http://api.map.baidu.com/geoconv/v1/?coords=' + str(long) + ',' + str(latit) + '&from=5&to=6&ak={}'.format(baidu_key)\n        conn = urlopen(url)\n        out = conn.read()\n        conn.close()\n    except:\n        print('坐标转换API无响应')\n        time.sleep(600)\n        url = 'http://api.map.baidu.com/geoconv/v1/?coords=' + str(long) + ',' + str(latit) + '&from=5&to=6&ak={}'.format(baidu_key)\n        conn = urlopen(url)\n        out = conn.read()\n        conn.close()\n    out_json = json.loads(out)\n    \n    return out_json['result'][0]['x'], out_json['result'][0]['y']\n\ndef coord_conv_back(long,latit):\n    #将百度bd09mc(百度米制经纬度坐标)转换为百度gps，API每日限额600,000\n    #API地址\n    #http://lbsyun.baidu.com/index.php?title=webapi/guide/changeposition\n    try:\n        url = 'http://api.map.baidu.com/geoconv/v1/?coords=' + str(long) + ',' + str(latit) + '&from=6&to=5&ak={}'.format(baidu_key)\n        conn = urlopen(url)\n        out = conn.read()\n        conn.close()\n    except:\n        print('坐标转换API无响应')\n        time.sleep(600)\n        url = 'http://api.map.baidu.com/geoconv/v1/?coords=' + str(long) + ',' + str(latit) + '&from=6&to=5&ak={}'.format(baidu_key)\n        conn = urlopen(url)\n        out = conn.read()\n        conn.close()\n    out_json = json.loads(out)\n    \n    return out_json['result'][0]['x'], out_json['result'][0]['y']\n\ndef loc_to_sid(x,y):\n    try:\n        url = 'https://mapsv0.bdimg.com/?udt=20200902&qt=qsdata&x=' + str(x) + '&y=' + str(y) + '&l=17.031000000000002&action=0&mode=day&t=1530956939770'\n        conn = urlopen(url)\n        out = conn.read()\n        conn.close()\n    except:\n        print('地点转SID无响应')\n        time.sleep(600)\n        url = 'https://mapsv0.bdimg.com/?udt=20200902&qt=qsdata&x=' + str(x) + '&y=' + str(y) + '&l=17.031000000000002&action=0&mode=day&t=1530956939770'\n        conn = urlopen(url)\n        out = conn.read()\n        conn.close()\n\n    out_json = json.loads(out)\n    \n\n    if 'content' in out_json:\n        return out_json['content']['id'],out_json['content']['x'],out_json['content']['y']\n    else:\n        return -1,-1,-1\n\ndef sid_to_pic(sid,heading,name):\n    try:\n        url = 'https://mapsv0.bdimg.com/?qt=pr3d&fovy=50&quality=100&panoid=' + str(sid) + '&heading=' + str(heading) + '&pitch=0&width=1024&height=512'\n        conn = urlopen(url)\n        print(url)\n        outimg = conn.read()\n        conn.close()\n    except:\n        print('图片下载无响应')\n        # time.sleep(400)\n        url = 'https://mapsv0.bdimg.com/?qt=pr3d&fovy=50&quality=100&panoid=' + str(sid) + '&heading=' + str(heading) + '&pitch=0&width=1024&height=512'\n        conn = urlopen(url)\n        outimg = conn.read()\n        conn.close()\n\n    if len(outimg) < 10000:\n        print('No pic at this point!')\n        return 0\n    else:\n        data_img = cv2.imdecode(np.asarray(bytearray(outimg), dtype=np.uint8), 1)\n        conn.close()#这里一定要关闭，不然爬几次之后会报连接错误\n        cv2.imwrite(name, data_img)\n        #plt.imshow(data_img)    \n        print('Pic Saved!')\n        return 1\n    \n# 读取已爬取的图片数量\ndef read_progress(progress_file):\n    if Path(progress_file).exists():\n        with open(progress_file, 'r') as f:\n            progress = int(f.read().strip())\n        return progress\n    return 0\n\n# 保存爬取的进度\ndef save_progress(progress_file, progress):\n    with open(progress_file, 'w') as f:\n        f.write(str(progress))\n    \nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--city_name\", type=str, default=\"Shanghai\", choices=[\"Shanghai\", \"Beijing\"])\n    args = parser.parse_args()\n\n    result_path = os.path.join(SAMPLE_POINT_PATH, f'StreetView_Images_{args.city_name}.csv')\n    output_dir = os.path.join(IMAGE_FOLDER, f'{args.city_name}_StreetView_Images')\n    output_path = Path(output_dir)\n    output_path.mkdir(exist_ok=True,parents=True)\n\n    crawled_district = os.listdir(output_dir)\n    baidu_key = os.environ.get('BAIDU_KEY')\n\n    csv_path = os.path.join(SAMPLE_POINT_PATH, f'{args.city_name}_sampled_points.csv')\n    df = pd.read_csv(csv_path)\n\n    for index, row in df.iterrows():\n        print(\"Processing \" + str(df['code'][index]))# + \",\" + df['name'][index])\n        dataset_name = str(df['code'][index])\n\n        if dataset_name in crawled_district:\n            print(f\"{dataset_name} Already crawled, skip!\")\n            continue\n\n        else:\n            try:\n                dataset_path = output_dir + dataset_name + \"/\"\n                Path(dataset_path).mkdir(exist_ok=True,parents=True)\n                out_csv = dataset_path + 'image_info.csv'\n\n                long_array = ast.literal_eval(df['longitude'][index])\n                lati_array = ast.literal_eval(df['latitude'][index])\n                # only keep the 6 digits after the decimal point\n                long_array = [round(x,4) for x in long_array]\n                lati_array = [round(x,4) for x in lati_array]\n\n                success_num = 0\n                total_num = 0\n                no_sid_num = 0\n                same_sid_num = 0\n                sid_set = set()   #避免过近的location导致相同sid重复获取\n                origin_long_lati_set = set()\n\n                if Path(out_csv).exists():\n                    image_info_df = pd.read_csv(out_csv)\n                else:\n                    image_info_df = pd.DataFrame(columns=['longitude_origin','latitude_origin','longitude_baidu','latitude_baidu','sid','sid_x_baidu','sid_y_baidu','sid_84_long', 'sid_84_lat'])\n\n                for root, dirs, files in os.walk(output_path):\n                    for img in files:\n                        divide = img.split('_')\n                        try:\n                            origin_long_lati_set.add((divide[1],divide[2]))\n                        except:\n                            print(divide)\n                # 读取进度\n                progress_file = dataset_path + 'progress.txt'\n                start_index = read_progress(progress_file)\n                \n                # 从上次中断的地方继续\n                for i in range(start_index, len(long_array)):\n                    longitude, latitude = long_array[i], lati_array[i]\n                    if (str(longitude),str(latitude)) in origin_long_lati_set:\n                        print(\"之前存过了，跳过咯\")\n                        continue\n                    total_num = total_num + 1\n\n                    # 网络中断的处理\n                    try:\n                    #这里要注意下，对应的经纬度没有街景图的地方，输出的会是无效图片\n\n                        print(longitude, latitude)\n                        long_baidu, latit_baidu = coord_conv(longitude,latitude)\n                        time.sleep(3*np.random.random())\n                        sid, sid_x, sid_y = loc_to_sid(long_baidu,latit_baidu)\n                        sid_x = sid_x / 100\n                        sid_y = sid_y / 100\n                        #sid_gps_x, sid_gps_y = coord_conv_back(sid_x,sid_y)\n                        if sid == -1:\n                            print('No sid at this location!')\n                            no_sid_num = no_sid_num + 1\n                            continue\n                        elif sid in sid_set:\n                            print('Already has this sid!')\n                            same_sid_num = same_sid_num + 1\n                            continue\n                        else:\n                            sid_set.add(sid)\n                            success_num = success_num + 1\n                            sid_baidu_x, sid_baidu_y = coord_conv_back(sid_x,sid_y)\n                            sid_84_long, sid_84_lat = bd09_to_wgs84(sid_baidu_x, sid_baidu_y)\n                            for heading in [0,90,180,270]:\n                                # print(heading)\n                                img_name = dataset_path + dataset_name + \"_\" + str(sid_84_long)+ \"_\" + str(sid_84_lat) + \"_\" + str(sid) + \".jpg\"\n                                result = sid_to_pic(sid,heading,img_name)\n                                if result == 1:\n                                    if heading == 270:\n                                        #time.sleep(5)\n                                        temp_df = pd.DataFrame([[longitude, latitude, long_baidu, latit_baidu, sid, sid_x, sid_y,sid_84_long, sid_84_lat]],columns=['longitude_origin','latitude_origin','longitude_baidu','latitude_baidu','sid','sid_x_baidu','sid_y_baidu','sid_84_long', 'sid_84_lat'])\n                                        image_info_df = pd.concat([image_info_df, temp_df], ignore_index=True)\n                                        image_info_df.to_csv(out_csv,sep = ',',index = False)\n                            save_progress(progress_file, i)\n                    except Exception as e:\n                        print(f\"Error at {i}: {e}\")\n                        save_progress(progress_file, i)  # 出现异常时保存进度\n                        time.sleep(5)  # 等待后重试\n                        continue\n\n                        # if success_num == 100:\n                        #     break\n\n                # append success_num and dataset_name to result csv\n                temp_df = pd.DataFrame([[dataset_name, success_num]], columns=['code', 'success_num'])\n                temp_df.to_csv(result_path, mode='a', header=False, index=False)\n\n                print(\"There are \" + str(total_num) + \" points\")\n                print(\"There are \" + str(success_num) + \" success points\")\n                print(\"There are \" + str(same_sid_num) + \" points repeated\")\n                print(\"There are \" + str(no_sid_num) + \" points that doesn't have data.\")\n                image_info_df.to_csv(out_csv,sep = ',',index = False)\n            \n            except Exception as e:\n                print(dataset_name, e)\n                continue\n"}
{"type": "source_file", "path": "citybench/street_view/Build_StreetView_List.py", "content": "import os\nimport pandas as pd\nimport argparse\n\ndef get_data(City):\n    image_files = os.listdir(f\"citydata/street_view/{City}_CUT/\")\n\n    # Random sample 500 images for each city and get latitude & longitutde\n    city_data = {\n        \"img_name\": [img_name for img_name in image_files[:500]],\n        \"lat\": [img_name.split(\"&\")[1] for img_name in image_files[:500]], \n        \"lng\": [img_name.split(\"&\")[2] for img_name in image_files[:500]],\n    }\n\n    city_df = pd.DataFrame(city_data)\n    city_csv_path = f\"citydata/street_view/{City}_data.csv\"\n    city_df.to_csv(city_csv_path, index=False)\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--city_name', type=str, default='Beijing', help='city name')\n    args = parser.parse_args()\n\n    get_data(args.city_name)\n"}
{"type": "source_file", "path": "citybench/mobility_prediction/run_eval.py", "content": "import os\nimport pickle\nimport time\nimport ast\nimport re\nimport random\nimport argparse\nimport json\nimport pandas as pd\nfrom datetime import datetime\nfrom tqdm import tqdm \n\nfrom serving.llm_api import get_chat_completion, get_model_response_hf\nfrom .metrics import get_acc1_f1, cal_metrics\nfrom config import RESULTS_PATH, MOBILITY_SAMPLE_RATIO, LLM_MODEL_MAPPING, VLM_API\nfrom serving.vlm_serving import VLMWrapper\n\nrandom.seed(32)\n\ndef get_dataset(dataname, split_path=\"citydata/mobility/checkin_split/\", test_path=\"citydata/mobility/checkin_test_pk/\", data_name=\"all\"):\n    \n    # Get training and validation set and merge them\n    train_data = pd.read_csv(os.path.join(split_path, f\"{dataname}_train.csv\"))\n    valid_data = pd.read_csv(os.path.join(split_path, f\"{dataname}_val.csv\"))\n\n    # Get test data\n    with open(os.path.join(test_path, f\"{dataname}_fin.pk\"), \"rb\") as f:\n        test_file = pickle.load(f)  # test_file is a list of dict\n    all_users = list(set(train_data[\"user_id\"]))\n\n    if data_name == \"mini\":\n        sample_users = random.choices(all_users, k=int(len(all_users)*MOBILITY_SAMPLE_RATIO))\n        train_data = train_data[train_data[\"user_id\"].isin(sample_users)]\n        valid_data = valid_data[valid_data[\"user_id\"].isin(sample_users)]\n        test_data = []\n        for item in test_file:\n            if item[\"user_X\"] in sample_users:\n                test_data.append(item)\n        test_file = test_data\n\n    # merge train and valid data\n    tv_data = pd.concat([train_data, valid_data], ignore_index=True)\n    tv_data.sort_values(['user_id', 'start_day', 'start_min'], inplace=True)\n    if dataname == 'geolife':\n        tv_data['duration'] = tv_data['duration'].astype(int)\n\n    # print(\"Number of total test sample: \", len(test_file))\n    return tv_data, test_file\n\n\ndef convert_to_12_hour_clock(minutes):\n    if minutes < 0 or minutes >= 1440:\n        return \"Invalid input. Minutes should be between 0 and 1439.\"\n\n    hours = minutes // 60\n    minutes %= 60\n\n    period = \"AM\"\n    if hours >= 12:\n        period = \"PM\"\n\n    if hours == 0:\n        hours = 12\n    elif hours > 12:\n        hours -= 12\n\n    return f\"{hours:02d}:{minutes:02d} {period}\"\n\n\n\ndef get_user_data(train_data, uid, num_historical_stay):\n    user_train = train_data[train_data['user_id']==uid]\n    print(f\"Length of user {uid} train data: {len(user_train)}\")\n    user_train = user_train.tail(num_historical_stay)\n    print(f\"Number of user historical stays: {len(user_train)}\")\n    return user_train\n\n\n# Organising data\ndef organise_data(data_name, split_path, dataname, user_train, test_file, uid, num_context_stay=5, sample_single_user=10):\n    # Use another way of organising data\n    historical_data = []\n    for _, row in user_train.iterrows():\n        historical_data.append(\n            (convert_to_12_hour_clock(int(row['start_min'])),\n            row['week_day'],\n            row['location_id'])\n            )\n\n\n    # Get user ith test data\n    list_user_dict = []\n    for i_dict in test_file:\n        i_uid = i_dict['user_X']\n        if i_uid == uid:\n            list_user_dict.append(i_dict)\n            \n    if data_name == \"mini\":\n        sample_single_user = min(len(list_user_dict),int(sample_single_user*MOBILITY_SAMPLE_RATIO))\n        \n    list_user_dict = random.choices(list_user_dict, k=sample_single_user)\n\n    predict_X = []\n    predict_y = []\n    for i_dict in list_user_dict:\n        construct_dict = {}\n        context = list(zip([convert_to_12_hour_clock(int(item)) for item in i_dict['start_min_X'][-num_context_stay:]],\n                        i_dict['weekday_X'][-num_context_stay:],\n                        i_dict['X'][-num_context_stay:]))\n        target = (convert_to_12_hour_clock(int(i_dict['start_min_Y'])), i_dict['weekday_Y'], None, \"<next_place_id>\")\n        construct_dict['context_stay'] = context\n        construct_dict['target_stay'] = target\n        predict_y.append(i_dict['Y'])\n        predict_X.append(construct_dict)\n\n    return historical_data, predict_X, predict_y\n\n\ndef single_query_top1_fsq(historical_data, X, model_name, model):\n    \"\"\"\n    Make a single query.\n    param: \n    X: one single sample containing context_stay and target_stay\n    \"\"\"\n    prompt = f\"\"\"\n    Your task is to predict a user's next location based on his/her activity pattern.\n    You will be provided with <history> which is a list containing this user's historical stays, then <context> which provide contextual information \n    about where and when this user has been to recently. Stays in both <history> and <context> are in chronological order.\n    Each stay takes on such form as (start_time, day_of_week, place_id). The detailed explanation of each element is as follows:\n    start_time: the start time of the stay in 12h clock format.\n    day_of_week: indicating the day of the week.\n    place_id: an integer representing the unique place ID, which indicates where the stay is.\n\n    Then you need to do next location prediction on <target_stay> which is the prediction target with unknown place ID denoted as <next_place_id> and \n    unknown duration denoted as None, while temporal information is provided.      \n    \n    Please infer what the <next_place_id> is (i.e., the most likely place ID), considering the following aspects:\n    1. the activity pattern of this user that you learned from <history>, e.g., repeated visit to a certain place during certain time.\n    2. the context stays in <context>, which provide more recent activities of this user; \n    3. the temporal information (i.e., start_time and weekday) of target stay, which is important because people's activity varies during different time (e.g., nighttime versus daytime)\n    and on different days (e.g., weekday versus weekend).\n\n    Please organize your answer in a JSON object containing following keys:\n    \"prediction\" (place ID) and \"reason\" (a concise explanation that supports your prediction)\n\n    The data are as follows:\n    <history>: {historical_data}\n    <context>: {X['context_stay']}\n    <target_stay>: {X['target_stay']}\n    \"\"\"\n\n    if model is not None:\n        completion = get_model_response_hf(prompt, model)\n        token_usage = 0\n    else:\n        completion, token_usage = get_chat_completion(session=[{\"role\": \"user\", \"content\": prompt}], model_name=model_name, json_mode=True, max_tokens=1200, temperature=0)\n    return completion, token_usage, prompt\n\n\ndef load_results(filename):\n    # Load previously saved results from a CSV file    \n    results = pd.read_csv(filename)\n    return results\n\ndef get_response(result):\n    match = re.search(r'\\{.*?\\}', result, re.DOTALL)\n    if match:\n        json_str = match.group(0)\n        return json_str\n    else:\n        return None\n\ndef parse_response_with_method_1(response, uid, predict_y, i):\n    \"\"\"方法1：尝试直接解析，失败时通过正则表达式提取数据\"\"\"\n    try:\n        output = get_response(response)\n        res_dict = ast.literal_eval(output)\n        res_dict['ground_truth'] = predict_y[i]\n        res_dict['user_id'] = uid\n        return res_dict\n    except Exception as e:\n        if e == \"'\\{' was never closed \\(<unknown>, line 1\\)\" or e == \"malformed node or string: None\":\n            match_pred = re.search(r'\"prediction\":\\s*(\\d+)', response)\n            match_res = re.search(r'\"reason\":\\s*(\\d+)', response)\n            if match_pred and match_res:\n                prediction_number = match_pred.group(1)\n                prediction_res = match_res.group(1)\n                # 返回组装好的res_dict\n                return {\n                    'prediction': prediction_number,\n                    'reason': prediction_res,\n                    'ground_truth': predict_y[i],\n                    'user_id': uid\n                }\n            else:\n                raise e\n        else:\n            raise e\n        \n\ndef parse_response_with_method_2(response, uid, predict_y, i, top_k=1):\n    \"\"\"方法2：直接通过ast解析，并根据条件处理预测值\"\"\"\n    try:\n        res_dict = ast.literal_eval(response)\n        if top_k != 1:\n            res_dict['prediction'] = str(res_dict['prediction'])  \n        res_dict['user_id'] = uid\n        res_dict['ground_truth'] = predict_y[i]\n        return res_dict\n    except Exception as e:\n        raise e\n    \ndef single_user_query(dataname, uid, historical_data, predict_X, predict_y, top_k, is_wt, output_dir, log_file, sleep_query, sleep_crash, model_name, model):\n    # Initialize variables\n    total_queries = len(predict_X)\n    print(f\"Total_queries: {total_queries}\")\n\n    processed_queries = 0\n    current_results = pd.DataFrame({\n        'user_id': None,\n        'ground_truth': None,\n        'prediction': None,\n        'reason': None\n    }, index=[])\n\n    out_filename = f\"{uid:02d}\" + \".csv\"\n    out_filepath = os.path.join(output_dir, out_filename)\n\n    try:\n        # Attempt to load previous results if available\n        current_results = load_results(out_filepath)\n        processed_queries = len(current_results)\n    except FileNotFoundError:\n        print(\"No previous results found. Starting from scratch.\")\n    all_token = 0\n    \n    # Process remaining queries\n    for i in tqdm(range(processed_queries, total_queries), desc=f\"Processing Queries for User {uid}\", unit=\"query\"):\n    #for query in queries[processed_queries:]:\n        \n        completions, token_usage, prompt = single_query_top1_fsq(historical_data, predict_X[i], model_name, model)\n\n        response = completions\n        all_token += token_usage\n\n        try:\n            # 尝试使用方法1解析\n            res_dict = parse_response_with_method_1(response, uid, predict_y, i)\n        except Exception as e1:\n            try:\n                # 尝试使用方法2解析\n                res_dict = parse_response_with_method_2(response, uid, predict_y, i, top_k)\n            except Exception as e2:\n                # 如果两种方法都失败，则返回默认值\n                res_dict = {'user_id': uid, 'ground_truth': predict_y[i], 'prediction': -100, 'reason': None}\n\n        new_row = pd.DataFrame(res_dict, index=[0])  \n        current_results = pd.concat([current_results, new_row], ignore_index=True)  \n        detail_entry = [\n            {\"role\": \"user\", \"content\": prompt},\n            {\"role\": \"response\", \"content\": response},\n            {\"role\": \"ref\", \"content\": predict_y[i]}\n        ]\n        with open(log_file, 'a', encoding='utf-8') as json_file:\n            json_file.write(json.dumps(detail_entry, ensure_ascii=False, indent=4) + '\\n')\n\n    # Save the current results\n    current_results.to_csv(out_filepath, index=False)\n\n    # Continue processing remaining queries\n    if len(current_results) < total_queries:\n        single_user_query(dataname, uid, historical_data, predict_X, predict_y,\n                          top_k, is_wt, output_dir, log_file, sleep_query, sleep_crash, model_name, model)\n\n\n\ndef query_all_user(data_name, split_path, dataname, uid_list, train_data, num_historical_stay,\n                   num_context_stay, test_file, top_k, is_wt, output_dir, log_file, sleep_query, sleep_crash, model_name, sample_single_user, model):\n    all_traj = 0\n\n    for uid in uid_list:  #train_data为train与val的拼接，test为字典\n        user_train = get_user_data(train_data, uid, num_historical_stay)  #根据每一个用户的历史步数进行预测\n        historical_data, predict_X, predict_y = organise_data(data_name, split_path, dataname, user_train, test_file, uid, num_context_stay, sample_single_user)   #predict_X, predict_y都是基于test生成的，historical_data是基于user_train生成的\n        all_traj += len(predict_y)\n        single_user_query(dataname, uid, historical_data, predict_X, predict_y, top_k=top_k,\n                          is_wt=is_wt, output_dir=output_dir, log_file=log_file, sleep_query=sleep_query, sleep_crash=sleep_crash, model_name=model_name, model=model)\n    return all_traj\n\n\n# Get the remaning user\ndef get_unqueried_user(dataname, user_cnt, test_path=\"./checkint_test_pk/\", output_dir='output/'):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    with open(os.path.join(test_path, f\"{dataname}_fin.pk\"), \"rb\") as f:\n        test_files = pickle.load(f)  # test_file is a list of dict\n    user_in_test = set()\n    for test_file in test_files:\n        X = test_file['user_X']\n        user_in_test.add(X)\n    all_user_id = random.choices(list(user_in_test), k=user_cnt)\n    processed_id = [int(file.split('.')[0]) for file in os.listdir(output_dir) if file.endswith('.csv')]\n    remain_id = [i for i in all_user_id if i not in processed_id]\n    # print(remain_id)\n    # print(f\"Number of the remaining id: {len(remain_id)}\")\n    return remain_id\n\n\ndef main(city, model_name, user_cnt=50, sample_single_user=10, num_historical_stay=40, num_context_stay=5, split_path=\"./checkin_split/\", test_path=\"./checkin_test_pk/\", data_name=\"all\"):\n    model = None\n    if model_name not in VLM_API:\n        try:\n            model_wrapper = LLM_MODEL_MAPPING[model_name]\n        except KeyError:\n            model_wrapper = VLMWrapper(model_name)\n            model = model_wrapper.get_vlm_model()\n\n    all_traj = 0  #总轨迹数(统计)\n    datanames = [\n        \"Beijing\", \"Cape\", \"London\", \"Moscow\", \"Mumbai\", \"Nairobi\", \"NewYork\" ,\"Paris\" ,\"San\", \"Sao\", \"Shanghai\", \"Sydney\",\"Tokyo\"\n    ]\n\n    prediction_results_path = os.path.join(RESULTS_PATH, \"prediction_results\")\n    prediction_logs_path = os.path.join(RESULTS_PATH, \"prediction_results\", \"logs\")\n    os.makedirs(prediction_results_path, exist_ok=True)\n    os.makedirs(prediction_logs_path, exist_ok=True)\n\n    for dataname in datanames:\n        \n        if dataname != city:\n            continue\n        top_k = 1  # the number of output places k\n        with_time = True  # whether incorporate temporal information for target stay\n        sleep_single_query = 0.1  # the sleep time between queries (after the recent updates, the reliability of the API is greatly improved, so we can reduce the sleep time)\n        sleep_if_crash = 1  # the sleep time if the server crashes\n\n        if \"/\" in model_name:\n            model_name_back = model_name.replace(\"/\", \"_\")\n        else:\n            model_name_back = model_name\n        output_dir = os.path.join(prediction_results_path, f\"{model_name_back}_{dataname}_top{top_k}_wt\")  # the output path\n        log_file = os.path.join(prediction_logs_path, f\"{model_name_back}_{dataname}_top{top_k}_wt.json\")  # the log dir\n\n        if not os.path.exists(output_dir):\n            os.mkdir(output_dir)\n        \n        tv_data, test_file = get_dataset(dataname, split_path=split_path, test_path=test_path, data_name=data_name)  #tv_data为train.csv与valis.csv的拼接，test_file为dict。已经是从不同城市的文件中读取的\n\n\n        uid_list = get_unqueried_user(dataname, user_cnt, test_path=test_path, output_dir=output_dir)  #output_dir存储预测结果。只选取其中10个user\n        # print(f\"uid_list: {uid_list}\")\n\n        traj = query_all_user(data_name, split_path, dataname, uid_list, tv_data, num_historical_stay, num_context_stay,\n                       test_file, output_dir=output_dir, log_file=log_file, top_k=top_k, is_wt=with_time,\n                       sleep_query=sleep_single_query, sleep_crash=sleep_if_crash, model_name=model_name, sample_single_user=sample_single_user, model=model)\n        all_traj += traj\n\n        # print(\"Query done\")\n    acc1, f1 = cal_metrics(output_dir=output_dir)\n    print(\"city:{} all_traj:{} acc1:{} f1:{}\".format(city, all_traj, acc1, f1))\n\n\nif __name__==\"__main__\":\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--city_name', type=str, default='Beijing')\n    parser.add_argument('--model_name', type=str)\n    parser.add_argument('--user_cnt', type=int, default=50, help=\"#总测试用户数\")\n    parser.add_argument('--sample_single_user', type=int, default=10, help=\"#每个用户轨迹数\")\n    parser.add_argument('--data_name',type=str, default='mini', choices=['all','mini'])\n    parser.add_argument('--split_path', type=str, default=\"citydata/mobility/checkin_split/\")\n    parser.add_argument('--test_path', type=str, default=\"citydata/mobility/checkin_test_pk/\")\n    parser.add_argument('--num_historical_stay', type=int, default=40)\n    parser.add_argument('--num_context_stay', type=int, default=5)\n    \n    args = parser.parse_args()\n\n    main(\n        args.city_name, \n        args.model_name, \n        user_cnt=args.user_cnt, \n        sample_single_user=args.sample_single_user, \n        num_historical_stay=args.num_historical_stay, \n        num_context_stay=args.num_context_stay,\n        split_path=args.split_path,\n        test_path=args.test_path,\n        data_name=args.data_name\n        )\n"}
{"type": "source_file", "path": "citybench/outdoor_navigation/crawler_google.py", "content": "from pathlib import Path\nimport pandas as pd\nfrom utils_scp import *\nimport numpy as np\nfrom datetime import datetime\nfrom multiprocessing import Process, Pool\nimport copy\nimport pandas as pd\nimport os\nimport argparse\nimport ast\nfrom config import IMAGE_FOLDER, SAMPLE_POINT_PATH\n\ndef crawl_multiple_region(city, multi_process_num, total_points, index=[]):\n    csv_file_path = os.path.join(SAMPLE_POINT_PATH, f'{args.city_name}_sampled_points_expand.csv')\n    df = pd.read_csv(csv_file_path)\n    df['coord_y'] = df['coord_y'].apply(lambda x: ast.literal_eval(x))\n    df['coord_x'] = df['coord_x'].apply(lambda x: ast.literal_eval(x))\n\n    p = Pool(multi_process_num)\n    result = [p.apply_async(crawl_single_region, args=(i, total_points, df['coord_x'][i], df['coord_y'][i], df['y_x'][i])) for i in index]\n\n    for i in result:\n        i.get()\n\n    p.close()\n    p.join()\n\n\ndef crawl_single_region(index, total_points, longti, lati, region_code):\n    region_code = copy.deepcopy(str(region_code))\n    lati = copy.deepcopy(lati)\n    longti = copy.deepcopy(longti)\n    lati = list(lati)\n    longti = list(longti)\n\n    # save images into local folder\n    path_with_region_code = SAVE_PATH.joinpath(region_code)\n    path_with_region_code.mkdir(exist_ok=True,parents=True)\n    meta_info, query_lati_longti_set = resume_from_history_info(path_with_region_code)\n    current_point_index = 0\n    success_download = 0\n    total_start_time = datetime.now()\n\n    for j in range(total_points):\n        start_time = datetime.now()\n        lati[j] = np.round(float(lati[j]), 4)\n        longti[j] = np.round(float(longti[j]), 4)\n        if (str(lati[j]),str(longti[j])) in query_lati_longti_set:\n            print('进程'+ str(index) + ': 已爬取，跳过')\n        else:  \n            try:\n                print(\"lati-longti: \", str(lati[j]),str(longti[j]))\n                panoid, real_lati, real_longti, date = get_metadata_from_lati_lonti(lati[j],longti[j]) # 查询经纬度对应panoid\n            except:\n                continue\n            if panoid == -1:\n                print('进程'+ str(index) + ': 无街景点，跳过')\n                query_lati_longti_set.add((str(lati[j]),str(longti[j])))\n                save_query_set(query_lati_longti_set,path_with_region_code)\n            elif panoid == -2:\n                print('进程'+ str(index) + ': 超过日限额或并发量')\n                time.sleep(3600*24)\n            elif panoid == -3:\n                print('进程'+ str(index) + ': 不知道查询metadata发生了什么，跳过')\n            else:\n                img_dict = get_image_tiles(panoid)\n                new_metainfo = save_img(img_dict, path_with_region_code, panoid, lati[j], longti[j], real_lati, real_longti, date)\n                meta_info = update_and_save_metainfo(meta_info, new_metainfo, path_with_region_code)\n                query_lati_longti_set.add((str(lati[j]),str(longti[j])))\n                save_query_set(query_lati_longti_set,path_with_region_code)\n                print('进程'+ str(index) + ': Downloaded Place: ' + str(panoid) + '; With ' + str(len(img_dict)) + ' Pictures!')\n                if len(img_dict) > 0:\n                    success_download += 1\n\n        current_point_index += 1\n        end_time = datetime.now()\n        print(\"进程\"+ str(index) + \": This point costs \" + str((end_time - start_time).total_seconds()) + ' seconds.')\n        print(\"进程\"+ str(index) + \": Suceessfully downloaded \" + str(success_download) + \" points from \" + str(current_point_index) + \" points.\")\n        print('==================================')\n\n        # if (success_download == SAMPLE_SIZE):\n        #     break\n        \n        # Can not find images over time limit, then remove the dir and save the region code\n        if ((end_time-total_start_time).total_seconds() > time_limit) & (len(os.listdir(path_with_region_code)) == 0):\n            break\n\n    print(f\"成功下载{success_download}个地点，共需要下载{total_points}个地点\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--city_name', type=str, default=\"Sydney\")\n    parser.add_argument('--multi_process_num', type=int, help='多线程数量，注意不要超过cpu核心数量')\n    parser.add_argument('--index', dest=\"index\", type=int, default=0, help='从第index组开始下载')\n    parser.add_argument('--total_points', type=int, help='每一组内下载多少个点')\n    args = parser.parse_args()\n    print(args)\n\n    SAVE_DIR = os.path.join(IMAGE_FOLDER, f'{args.city_name}_StreetView_Images_origin')\n    SAVE_PATH = Path(SAVE_DIR)\n    SAVE_PATH.mkdir(exist_ok=True,parents=True)\n\n    time_limit = 300 #By seconds\n    empty_region = []\n\n    begin_time = datetime.now()\n    crawl_multiple_region(args.city_name, args.multi_process_num, args.total_points, index=[i for i in range(args.index, args.total_points)]) \n    end_time = datetime.now()\n    print('总共耗时：' + str((end_time-begin_time).total_seconds() / 3600 ) + '小时 or ' + str((end_time-begin_time).total_seconds()) + '秒')\n"}
{"type": "source_file", "path": "citybench/mobility_prediction/run_parallel.py", "content": "import argparse\nfrom multiprocessing import Pool\n\nfrom .run_eval import main\n\n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--user_cnt', type=int, default=50)\n    parser.add_argument('--traj_cnt', type=int, default=10)\n\n    args = parser.parse_args()\n    user_cnt = args.user_cnt            # users \n    sample_single_user = args.traj_cnt  # trajectory for each user\n    data_version=\"mini\"\n    split_path=\"citydata/mobility/checkin_split/\"\n    test_path=\"citydata/mobility/checkin_test_pk/\"\n    \n    models = [\"GPT4o\"]\n    \n    cities = [\n            \"Beijing\", \"CapeTown\", \"London\", \"Moscow\", \"Mumbai\", \"Nairobi\", \"NewYork\" ,\"Paris\" ,\"SanFrancisco\", \"SaoPaulo\", \"Shanghai\", \"Sydney\",\"Tokyo\"\n        ]\n    \n    para_group = []\n    for c in cities:\n        for m in models:\n            para_group.append([c, m, user_cnt, sample_single_user, 40, 5, split_path, test_path, data_version])\n\n    with Pool(6) as pool:\n        results = pool.starmap(main, para_group)\n"}
{"type": "source_file", "path": "citybench/geoqa/utils.py", "content": "import copy\nimport os\nimport random\nimport re\nimport pandas as pd\nimport numpy as np\nimport subprocess\nfrom pycitydata.map import Map\nfrom citysim.routing import RoutingClient\nfrom config import MONGODB_URI\n\nprimary_directions = ['east', 'south', 'west', 'north']\nsecondary_directions = ['southeast', 'northeast', 'southwest', 'northwest']\nEW = {'east', 'west'}\nNS = {'south', 'north'}\ndir_map = {\"north\": \"south-north\", \"south\": \"south-north\", \"west\": \"east-west\", \"east\": \"east-west\"}\ndir_map2 = {\"south-north\": \"east-west\", \"east-west\": \"south-north\"}\n\nsecondary_dir_to_primary_dirs = {\n    \"southeast\": (\"south\", \"east\"),\n    \"northeast\": (\"north\", \"east\"),\n    \"northwest\": (\"north\", \"west\"),\n    \"southwest\": (\"south\", \"west\"),\n}\n\n\ndef task_files_adaption(task_file, output_path):\n    task_files = copy.deepcopy(task_file)\n    path_prefix = output_path\n    for k in task_files:\n        for kk in task_files[k]:\n            if path_prefix not in task_files[k][kk]:\n                task_files[k][kk] = os.path.join(path_prefix, task_files[k][kk])\n    os.makedirs(path_prefix, exist_ok=True)\n    return task_files\n\n\ndef gen_options(options, question, answer):\n    \"\"\"\n    生成字典:\n    {\"A\": \"选项1\", \"B\": \"选项2”, ..., \"question\": $question$, \"answer\": answer对应的选项}\n    \"\"\"\n    options = copy.copy(options)\n    random.shuffle(options)\n    result = {}\n    start_option_ascii = ord('A')\n    for index, option in enumerate(options):\n        selection = chr(start_option_ascii + index)\n        result[selection] = option\n        if option == answer:\n            result[\"answer\"] = selection\n\n    if \"answer\" not in result:\n        raise LookupError(\"未找到option=answer\")\n\n    result[\"question\"] = question\n\n    return result\n\ndef save_data(unseen_aois, save_path):\n    unseen_aois_df = pd.DataFrame(data=unseen_aois)\n    unseen_aois_df[\"is_seen\"] = False\n\n    task_df = pd.concat([unseen_aois_df])\n    task_df.to_csv(save_path)\n\ndef dir_all_dis(routes, secondary_directions, primary_directions,secondary_dir_to_primary_dirs):\n    \"\"\"\n    计算输入数据包含的移动方向以及每个方向移动的总距离\n    \"\"\"\n    distances = []\n    directions = []\n    dir_dis_dep = []\n    dir_dis = []\n    for cnt, route in enumerate(routes):\n        if route['type'] == \"junc\":\n            continue\n        distance = route['road_length']\n        direction = route['direction'].split()[-1]\n        distances.append(distance)\n        directions.append(direction)\n\n    for cnt2, direction in enumerate(directions):\n        if direction in secondary_directions:\n            distance = int(distances[cnt2]) * 0.7\n            distance_str = str(distances[cnt2]) + \"m,equals to ({},{}m) and ({},{}m)\".format(direction[0],\n                                                                                        \"0.7*\" + str(distances[\n                                                                                            cnt2]) + '=' + str(\n                                                                                            distance), direction[1],\n                                                                                        \"0.7*\" + str(distances[\n                                                                                            cnt2]) + '=' + str(\n                                                                                            distance))\n            dir_dis_dep.append((secondary_dir_to_primary_dirs[direction][0], distance))\n            dir_dis_dep.append((secondary_dir_to_primary_dirs[direction][1], distance))\n            dir_dis.append((direction, distance_str))\n        elif direction in primary_directions:\n            distance = int(distances[cnt2])\n            distance_str = str(distances[cnt2]) + 'm'\n            dir_dis_dep.append((direction, distance))\n            dir_dis.append((direction, distance_str))\n        else:\n            print(direction)\n\n    # 遍历原始列表，将相同键值的元素进行累加处理\n    mid = {}\n    for cnt, ddlist in enumerate(dir_dis_dep):\n        dir, dis = ddlist\n        if dir not in mid:\n            mid[dir] = 0\n        mid[dir] += dis\n    dir_dis_fin = [(key, value) for key, value in mid.items()]  # 起始点到终点各个方向位移距离，[(方向，位移距离),(),...]\n    dirs = set()\n    for dir, dis in dir_dis_fin:\n        dirs.add(dir)\n    short_dir = list(set(primary_directions).difference(dirs))\n    if len(short_dir) > 0:\n        for dir in short_dir:\n            dir_dis_fin.append((dir, 0))\n    return dir_dis_fin, dir_dis\n\ndef compute_length(routine):  # 计算导航路径的总长度\n    float_values = re.findall(r'for (\\d+) meters', routine)\n    length = np.sum([int(num) for num in float_values])\n    return length\n\n\ndef angle2dir(angle):\n    Direction = ['north', 'northeast', 'east', 'southeast', 'south', 'southwest', 'west', 'northwest']\n    s = 22.5\n    for i in range(8):\n        if angle < s + 45 * i:\n            return Direction[i]\n    return Direction[0]\n\n\ndef angle2dir_4(angle):\n    Direction = ['north', 'east', 'south', 'west']\n    if angle < 45 or angle >= 315:\n        return Direction[0]\n    elif 45 <= angle < 135:\n        return Direction[1]\n    elif 135 <= angle < 225:\n        return Direction[2]\n    else:\n        return Direction[3]\n\n\ndef get_landuse_dict():\n    landuse_dict = {\n            \"E3\":\"OtherNon-construction\", \"R\":\"Residential\", \"S4\":\"TrafficStation&Park\", \"A4\":\"Sports\", \"B31\":\"Entertainment\",  \"U9\":\"OtherPublicFacilities\", \"A3\":\"Education\",\"G1\":\"Park&GreenLand\",\"B\":\"CommercialService&IndustryFacilities\",\"B32\":\"Resort&Fitness\",\"B13\":\"Restaurant&Bar\",\"A9\":\"ReligiousFacilities\",\"A5\":\"Hospital\"\n            }\n\n    return landuse_dict\n\ndef get_category_supported():\n    category_supported = {\"leisure\":\"leisure\", \"amenity\":\"amenity\", \"building\":\"building\"}\n    return category_supported\n"}
{"type": "source_file", "path": "citybench/traffic_signal/data_gen.py", "content": "import os\nimport time\nimport asyncio\nimport argparse\nimport signal\nimport subprocess\nfrom typing import List, Tuple\nfrom pymongo import MongoClient\n\nfrom mosstool.type import Map\nfrom mosstool.type import Persons\nfrom mosstool.util.format_converter import pb2dict, coll2pb\n\nfrom citysim.utils import whether_road_in_region, get_coords\nfrom citysim.utils import generate_persons, multi_aoi_pos2lane_pos, with_preroute\nfrom config import MAP_DATA_PATH, MONGODB_URI, ROUTING_PATH, TRIP_DATA_PATH, MAP_DICT\n\n\n\n# step0: 转换地图格式\ndef process_map(city):\n    city_map = MAP_DICT[city]\n    OUTPUT_PATH = os.path.join(MAP_DATA_PATH, \"{}/{}.map.pb\".format(city, city))\n    print(f\"Transfering {city} map…\")\n    client = MongoClient(f\"{MONGODB_URI}\")\n    coll = client[\"llmsim\"][city_map]\n    pb = Map()\n    pb = coll2pb(coll, pb)\n\n    with open(OUTPUT_PATH,\"wb\") as f:\n        f.write(pb.SerializeToString())\n\n\n# step1: 生成车流\nasync def gen_trips(city):\n    CITY_TO_AGENT_NUM = 10_0000\n    CITY_TO_HOST = 52107\n    routing_path = ROUTING_PATH\n \n    try:\n        print(f\"Generating {city} trips…\")\n        with open(os.path.join(MAP_DATA_PATH, f\"{city}/{city}.map.pb\"), \"rb\") as f:\n            m = Map()\n            m.ParseFromString(f.read())\n        m_dict = pb2dict(m)\n        # listen需按照实际修改\n        route_command = f\"./{routing_path} -mongo_uri {MONGODB_URI} -map {city}.map -cache {MAP_DATA_PATH}/{city} -listen localhost:{CITY_TO_HOST}\"\n        cmd = route_command.split(\" \")\n        process = subprocess.Popen(args=cmd, cwd=\"./\")\n        persons = generate_persons(\n            m, city, agent_num=int(1.1 * CITY_TO_AGENT_NUM)\n        )\n        persons = multi_aoi_pos2lane_pos(persons=persons, map_dict=m_dict)\n        lanes = {d[\"id\"]: d for d in m_dict[\"lanes\"]}\n        await with_preroute(\n            persons=persons,\n            lanes=lanes,\n            listen=f\"http://localhost:{CITY_TO_HOST}\",\n            output_path=os.path.join(TRIP_DATA_PATH, f\"{city}_trip.pb\"),\n            max_num=CITY_TO_AGENT_NUM,\n        )\n        # await task\n        time.sleep(0.1)\n        print(\"send signal\")\n        process.send_signal(sig=signal.SIGTERM)\n        process.wait()\n    \n    except Exception as e:\n        print(f\"Error generating {city} trips: {e}\")\n\n\n# step2: 过滤车流\ndef filter_trips(city, FLOW_RATIO=5):\n    from moss import Map\n    AGENT_PATH = os.path.join(TRIP_DATA_PATH, \"{}_trip.pb\".format(city))\n    START_TIME, END_TIME = 30000-1000, 30000+2000\n    NEW_AGENT_PATH = os.path.join(TRIP_DATA_PATH, \"{}_trip_filtered_start_{}_end_{}_extend_{}.pb\".format(city, START_TIME, END_TIME, FLOW_RATIO))\n    M=Map(os.path.join(MAP_DATA_PATH, '{}/{}.map.pb'.format(city, city)))\n    print(f\"Filtering {city} trips…\")\n    # 划分的交通灯控制区域\n    coords = get_coords(city)\n    target_road_ids = whether_road_in_region(M, coords)\n    # print(len(target_road_ids))\n\n    with open(AGENT_PATH, \"rb\") as f:\n        pb = Persons()\n        pb.ParseFromString(f.read())\n    ok_persons = []\n    for p in pb.persons:\n        flag = False\n        for schedule in p.schedules:\n            for trip in schedule.trips:  \n                if trip.routes:  \n                    for route in trip.routes:\n                        if route.HasField(\"driving\"):  \n                            road_ids = set(route.driving.road_ids) \n                            if road_ids.intersection(target_road_ids) and (START_TIME<schedule.departure_time<END_TIME):\n                                flag = True\n                                break\n                        else:\n                            print(\"No driving information available.\")\n                    if flag:\n                        break\n                else:\n                    print(\"No routes available.\")\n            if flag:\n                break\n\n        if flag == 1:\n            # print(flag)\n            # 扩大车流量\n            pid = p.id\n            for i in range(FLOW_RATIO):\n                p.id = pid + 100000 + i\n                ok_persons.append(p)\n\n    new_pb = Persons(persons=ok_persons)\n    with open(NEW_AGENT_PATH, \"wb\") as f:\n        f.write(new_pb.SerializeToString())\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--city_name\", type=str, default=\"Shanghai\")\n    parser.add_argument(\"--mode\", type=str, default=\"all\", choices=[\"map_transfer\", \"gen_trips\", \"filter_trips\", \"all\"])\n\n    args = parser.parse_args()\n\n    if args.mode in [\"all\", \"map_transfer\"]:\n        process_map(args.city_name)\n    if args.mode in [\"all\", \"gen_trips\"]:\n        asyncio.run(gen_trips(args.city_name))\n    if args.mode in [\"all\", \"filter_trips\"]:\n        filter_trips(args.city_name)\n"}
{"type": "source_file", "path": "citybench/street_view/Randompoints_Gen.py", "content": "import csv\nimport random\nimport ast\nimport pandas as pd\n\n\nimport argparse\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--city_name', type=str, default='SanFrancisco', help='City name')\n    args = parser.parse_args()\n    city = args.city\n    print(city)\n    boudary = CITY_BOUNDARY[city]\n    #\"SanFrancisco\": [(-122.5099, 37.8076), (-122.5099, 37.6153), (-122.3630, 37.6153), (-122.3630, 37.8076)]\n    \n    #latitude and longitude range box\n    upper_lat = boudary[0][1]\n    lower_lat = boudary[1][1]\n    left_lon = boudary[1][0]\n    right_lon = boudary[2][0]\n\n\n\n\n\n    # random generate 10000 points in the box\n    all_lon = []\n    all_lat = []\n    for _ in range(10000):\n        lon = random.uniform(left_lon, right_lon)\n        lat = random.uniform(lower_lat, upper_lat)\n        all_lon.append(lon)\n        all_lat.append(lat)\n\n    # split all_lat and all_lon into 10 pieces\n    all_lat = [all_lat[i:i + 1000] for i in range(0, len(all_lat), 1000)]\n    all_lon = [all_lon[i:i + 1000] for i in range(0, len(all_lon), 1000)]\n\n    # write data to csvv file\n    with open(f'./random_points_{city}.csv', 'w', newline='') as csvfile:\n        fieldnames = ['Index', 'City', 'NEAR_X', 'NEAR_Y']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n        for i in range(10):\n            writer.writerow({'Index': i, 'City': city+'_'+str(i), 'NEAR_X': all_lon[i], 'NEAR_Y': all_lat[i]})\n\n    # Transform the string info to list\n    csv_file_path = f'./random_points_{city}.csv'\n    df = pd.read_csv(csv_file_path)\n    df['NEAR_X'] = df['NEAR_X'].apply(lambda x: ast.literal_eval(x))\n    df['NEAR_Y'] = df['NEAR_Y'].apply(lambda x: ast.literal_eval(x))\n"}
{"type": "source_file", "path": "citybench/outdoor_navigation/metrics.py", "content": "import os\nimport pandas as pd\nfrom config import RESULTS_PATH\n\nparent_folder = os.path.join(RESULTS_PATH, \"outdoor_navigation_results\")\noutput_file = os.path.join(parent_folder, \"navigation_benchmark_result.csv\")\n\nall_data = []\nfor file_name in os.listdir(parent_folder):\n    file_path = os.path.join(parent_folder, file_name)\n    if os.path.isfile(file_path) and file_name.endswith('.csv'):\n        try:\n            df = pd.read_csv(file_path)\n            all_data.append(df)  \n        except Exception as e:\n            print(f\"Error {file_path} : {e}\")  \n\n# 检查是否有数据\nif all_data:\n    combined_df = pd.concat(all_data, ignore_index=True)\n    combined_df.to_csv(output_file, index=False)\n    print(\"Navigation results have been saved!\")\nelse:\n    print(\"No exploration results found!\")\n"}
{"type": "source_file", "path": "citybench/remote_sensing/eval_inference.py", "content": "import os\nimport argparse\nimport pandas as pd\nfrom setproctitle import setproctitle\n\nfrom tqdm import tqdm\nimport json\n\nfrom config import REMOTE_SENSING_PATH, REMOTE_SENSING_RESULTS_PATH\nfrom serving.vlm_serving import VLMWrapper\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model_name', type=str, default='InternVL2-40B', help='model name')\n    parser.add_argument('--city_name', type=str, default='Beijing', help='city name')\n    parser.add_argument('--data_name', type=str, default=\"mini\", help='dataset size')\n    parser.add_argument('--task_name', type=str, default='population', help='task name', choices=[\"population\", \"objects\"])\n    args = parser.parse_args()\n\n    print(\"Load the model\")\n    model_wrapper = VLMWrapper(args.model_name)\n    model = model_wrapper.get_vlm_model()\n\n    print(\"Load the image indicator csv\")\n    df = pd.read_csv(os.path.join(REMOTE_SENSING_PATH, f\"{args.city_name}_img_indicators.csv\"))\n    print(df.shape)\n    img_name_list = [os.path.join(REMOTE_SENSING_PATH, args.city_name, x+\".png\") for x in df['img_name'].astype(str).to_list()]\n    \n    #according to the task name, set the prompt\n    if args.task_name == 'population':\n        prompt =  \"Please analyze the population density of the image shown comparing to all cities around the world. \\\n            Rate the population density in a degree from 0.0 to 9.9, where higher rating represents higher population density. \\\n                You should provide ONLY your answer in the exact format: 'My answer is X.X.', where 'X.X' represents your rating of the population density.\"\n    elif args.task_name == 'objects':\n        prompt = '''Suppose you are an expert in identifying urban infrastructure.\n        Please analyze this image and choose the infrastructure type from the following list:\n        ['Bridge', 'Stadium', 'Ground Track Field', 'Baseball Field',\n        'Overpass', 'Airport', 'Golf Field', 'Storage Tank', \n        'Roundabout', 'Swimming Pool', 'Soccer Ball Field', 'Harbor', 'Tennis Court', \n        'Windmill', 'Basketball Court', 'Dam', 'Train Station']\n        Please meet the following requirements:\n        1. If you can identify multiple infrastructure types, please provide all of them.\n        2. You must provide the answer in the exact format: 'My answer is X, Y, ... and Z.' where 'X, Y, ... and Z' represent the infrastructure types you choose.\n        3. Don't output any other sentences.\n        4. If you cannot choose any of the infrastructure types from the list, please choose 'Other'.\n        '''\n\n    #adjust the dataset size\n    if args.data_name == \"all\":\n        img_name_list = img_name_list\n    elif args.data_nane == \"mini\":\n        img_name_list = img_name_list[:int(len(img_name_list)*0.01)]\n    \n    # read existing data\n    try:\n        his_data = pd.read_json(os.path.join(REMOTE_SENSING_RESULTS_PATH, f\"{args.city_name}_{args.model_name}_{args.task_name}.jsonl\"), lines=True)\n        his_imgs = his_data[\"img_name\"].to_list()\n        his_ress = his_data[\"text\"].to_list()\n        his_data_list = list(zip(his_imgs, his_ress))\n    except:\n        his_imgs, his_ress, his_data_list = [], [], []\n\n    print(\"Generate the response\")\n    response = []\n    for img_name in tqdm(img_name_list):\n        img_name_slim = img_name.split('/')[-1]\n        if img_name_slim in his_imgs:\n            continue\n        ret = model.generate([img_name, prompt])\n        response.append([img_name_slim, ret])\n\n    # Save the response\n    with open(os.path.join(REMOTE_SENSING_RESULTS_PATH, f\"{args.city_name}_{args.model_name}_{args.task_name}.jsonl\"), \"w\") as fout:\n        # saving old data\n        for i in range(len(his_data_list)):\n            value = {\n                \"img_name\": his_data_list[i][0],\n                \"text\": his_data_list[i][1],\n            }\n            fout.write(json.dumps(value) + \"\\n\")\n        \n        # saving new data\n        for i in range(len(response)):\n            value = {\n                \"img_name\": response[i][0],\n                \"text\": response[i][1],\n            }\n            fout.write(json.dumps(value) + \"\\n\")\n    \n    model_wrapper.clean_proxy()\n"}
{"type": "source_file", "path": "citybench/geoqa/metrics.py", "content": "import os\nimport re\nimport pandas as pd\nimport numpy as np\nimport argparse\n\nfrom config import RESULTS_PATH, GEOQA_TASK_MAPPING_v1, GEOQA_TASK_MAPPING_v2\ndef get_result(file_path, result_files, map_task):\n    final_result = {}\n    for result_file in result_files:\n        match = re.match(r\"geo_knowledge_(.+?)_v82_summary_(.+?)\\.csv\", result_file)\n        if not match:\n            continue\n        city_name = match.group(1) \n        model_name = match.group(2) \n        result = pd.read_csv(os.path.join(file_path,result_file))\n        # Check if the number of results is correct\n        file_row_count = len(result)  \n        if file_row_count != 29:\n            print(f\"Error: The number of results for model {model_name} under city {city_name} is {file_row_count}, not 29.\")\n        \n        for _, row in result.iterrows():\n            task = row['task_name']\n            acc = row['accuracy']\n            if (model_name, city_name) not in final_result:\n                final_result[(model_name, city_name)] = {\n                    \"node\": None,\n                    \"landmark\": None,\n                    \"path\": None,\n                    \"districts\": None,\n                    \"boundary\": None,\n                    \"others\": None\n                }\n            for cat,tasks in map_task.items():\n                if task in tasks:\n                    final_result[(model_name, city_name)][cat] = acc\n                    break\n    \n    data = []\n    for (model_name, city_name), categories in final_result.items():\n        row = [model_name, city_name, categories['node'], categories['landmark'], categories['path'], categories['districts'], categories['boundary'], categories['others']]\n        data.append(row)\n    \n    df = pd.DataFrame(data, columns=['Model_Name', 'City_Name', 'Node', 'Landmark', 'Path', 'Districts', 'Boundary', 'Others'])\n    df['GeoQA_Average_Accuracy'] = df[['Node', 'Landmark', 'Path', 'Districts', 'Boundary', 'Others']].mean(axis=1).round(4)\n    return df\n\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--city_name\", type=str, default=\"Beijing\")\n    parser.add_argument(\"--evaluate_version\", type=str, default=\"v1\")\n    args = parser.parse_args()\n    file_path = os.path.join(RESULTS_PATH, \"geo_knowledge_result\")\n    result_files = os.listdir(file_path)\n    \n    if args.city_name == \"Beijing\":\n        GEOQA_TASK_MAPPING = GEOQA_TASK_MAPPING_v2\n    else:\n        if args.evaluate_version == \"v1\":\n            GEOQA_TASK_MAPPING = GEOQA_TASK_MAPPING_v1\n        elif args.evaluate_version == \"v2\":\n            GEOQA_TASK_MAPPING = GEOQA_TASK_MAPPING_v2\n    result = get_result(file_path, result_files, GEOQA_TASK_MAPPING)\n    result.to_csv(os.path.join(file_path,\"geoqa_benchmark_result.csv\"), index=False)\n    print(\"GeoQA results have been saved!\")\n\n"}
{"type": "source_file", "path": "citybench/remote_sensing/utils.py", "content": "\nimport numpy as np\nimport math\nimport geopandas as gpd\nfrom shapely.geometry import Polygon\n\n\ndef haversine_distance(lat1, lon1, lat2, lon2):\n    r = 6371\n    phi1 = np.radians(lat1)\n    phi2 = np.radians(lat2)\n    delta_phi = np.radians(lat2 - lat1)\n    delta_lambda = np.radians(lon2 - lon1)\n    a = np.sin(delta_phi / 2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2)**2\n    res = r * (2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a)))\n    return np.round(res, 2)\n\ndef num2deg(x, y, zoom=15):\n    n = 2.0 ** zoom\n    lon_deg = x / n * 360.0 - 180.0\n    lat_rad = np.arctan(np.sinh(np.pi * (1 - 2 * y / n)))\n    lat_deg = np.rad2deg(lat_rad)\n    return lat_deg, lon_deg\n\ndef deg2num(lon_deg, lat_deg, zoom=15):\n    lat_rad = np.radians(lat_deg)\n    n = 2.0 ** zoom\n    xtile = int((lon_deg + 180.0) / 360.0 * n)\n    ytile = int((1.0 - np.log(np.tan(lat_rad) + (1 / np.cos(lat_rad))) / np.pi) / 2.0 * n)\n    return xtile, ytile\n\ndef compute_tile_coordinates(min_x, max_x, min_y, max_y):\n    x_arr = np.arange(min_x, max_x + 1)\n    y_arr = np.arange(min_y, max_y + 1)\n    lon_arr, lat_arr = num2deg_batch(x_arr, y_arr)\n    return lon_arr, lat_arr,x_arr, y_arr\n\ndef num2deg_batch(x_arr, y_arr, zoom=15):\n    n = 2.0 ** zoom\n    lon_deg_arr = x_arr / n * 360.0 - 180.0\n    lat_rad_arr = np.arctan(np.sinh(np.pi * (1 - 2 * y_arr / n)))\n    lat_deg_arr = np.rad2deg(lat_rad_arr)\n    return lon_deg_arr, lat_deg_arr\n\ndef create_tile_polygons(lon_arr, lat_arr,x_arr, y_arr):\n    \n    tile_gpd= gpd.GeoDataFrame()\n    lon_mesh, lat_mesh = np.meshgrid(lon_arr, lat_arr, indexing='ij')\n    x_mesh, y_mesh = np.meshgrid(x_arr, y_arr, indexing='ij')\n    \n    \n    vertices = np.array([\n        lon_mesh[:-1, :-1], lat_mesh[:-1, :-1],\n        lon_mesh[1:, :-1], lat_mesh[1:, :-1],\n        lon_mesh[1:, 1:], lat_mesh[1:, 1:],\n        lon_mesh[:-1, 1:], lat_mesh[:-1, 1:]\n    ])\n\n    vertices = vertices.reshape(4, 2, -1)\n    vertices = np.transpose(vertices, axes=(2, 0, 1))\n    polygons = [Polygon(p) for p in vertices]\n    vertices_x_y = np.array([\n        x_mesh[:-1, :-1], y_mesh[:-1, :-1],\n        x_mesh[1:, :-1], y_mesh[1:, :-1],\n        x_mesh[1:, 1:], y_mesh[1:, 1:],\n        x_mesh[:-1, 1:], y_mesh[:-1, 1:]\n    ])\n    \n    vertices_x_y = vertices_x_y.reshape(4, 2, -1)\n    vertices_x_y = np.transpose(vertices_x_y, axes=(2, 0, 1))\n    y_x = [f\"{int(p[0][1])}_{int(p[0][0])}\" for p in vertices_x_y]\n                            \n    \n    tile_gpd['geometry'] = polygons\n    tile_gpd['y_x'] = y_x\n    \n    return tile_gpd\ndef num2deg(xtile, ytile, zoom):\n    n = 2.0 ** zoom\n    lon_deg = xtile / n * 360.0 - 180.0\n    lat_rad = math.atan(math.sinh(math.pi * (1 - 2 * ytile / n)))\n    lat_deg = math.degrees(lat_rad)\n    return lat_deg, lon_deg\n"}
{"type": "source_file", "path": "citybench/outdoor_navigation/utils_scp.py", "content": "import requests\nimport json\nimport time\nfrom pathlib import Path\nimport pandas as pd\n# from haversine import haversine\nimport pickle\nimport paramiko\nimport os\n\nAPI_KEY = os.environ.get('GOOGLE_API_KEY')\n\ndef get_metadata_from_lati_lonti(lati,longti):\n    base_url = 'https://maps.googleapis.com/maps/api/streetview/metadata'\n    params = {'location':str(lati)+','+str(longti),'key':API_KEY}\n    while True:\n        try:\n            response = requests.get(base_url, params = params)\n        except (requests.exceptions.RequestException, requests.exceptions.SSLError):\n            print('查询metadata API无响应')\n            time.sleep(1)\n        else:\n            break\n\n    results = json.loads(response.text)\n    response.close()\n    if results['status'] == 'OK':\n        return results['pano_id'], results['location']['lat'], results['location']['lng'], results['date']\n    elif results['status'] == 'ZERO_RESULTS':\n        return -1, -1, -1, -1\n    elif results['status'] == 'OVER_QUERY_LIMIT':\n        return -2, -2, -2, -2\n    else:\n        print(params)\n        print(results['status'])\n        return -3,-3,-3,-3\n\ndef get_image_tiles(panoid):\n    base_url = 'https://streetviewpixels-pa.googleapis.com/v1/tile'\n    output_img_dict = {}\n    for x in range(16):\n        for y in [2,3,4]:\n            params = {'cb_client':'maps_sv.tactile','panoid':str(panoid),'x':str(x),'y':str(y),'zoom':'4','nbt':'1','fover':'2'}\n            t = 0\n            while True:\n                try:\n                    t += 1\n                    response = requests.get(base_url, params = params, timeout=(3.05,6.05))\n                except (requests.exceptions.RequestException, requests.exceptions.SSLError, requests.exceptions.Timeout):\n                    print('下载图片无响应，重试中')\n                    print(base_url)\n                    print(params)\n                    if t > 3:\n                        print(\"下载图片多次重试无效，10秒等待！\")\n                        time.sleep(10)\n                else:\n                    if response.status_code == 400:\n                        print('此地参数无此设置')\n                        print(params)\n                        response.close()\n                        break\n                    else:\n                        img = response.content\n                        output_img_dict[str(x)+'&'+str(y)] = img\n                        response.close()\n                        break\n\n    return output_img_dict\n\ndef save_img(img_dict, save_path, panoid, query_lati, query_longti, real_lati, real_longti, date):\n    panoid_list = [str(panoid) for _ in range(len(img_dict))]\n    query_lati_list = [str(query_lati) for _ in range(len(img_dict))]\n    query_longti_list = [str(query_longti) for _ in range(len(img_dict))]\n    real_lati_list = [str(real_lati) for _ in range(len(img_dict))]\n    real_longti_list = [str(real_longti) for _ in range(len(img_dict))]\n    date_list = [str(date) for _ in range(len(img_dict))]\n    file_name_list = []\n    for key, val in img_dict.items():\n        x,y = key.split('&')\n        file_name = str(panoid) + '&' + str(real_lati) + '&' + str(real_longti) + '&' + str(x) + '&' + str(y) + '.jpg'\n        file_name_list.append(file_name)\n        total_file_name = save_path.joinpath(file_name)\n        with open(total_file_name,mode='wb') as f:\n            f.write(val)\n\n    new_metainfo_dataframe = pd.DataFrame({'panoid':panoid_list,'query_lati':query_lati_list, 'query_longti':query_longti_list, 'real_lati':real_lati_list,'real_longti':real_longti_list,'date':date_list,'file_name':file_name_list})\n    return new_metainfo_dataframe\n\ndef update_and_save_metainfo(old_metainfo_dataframe, new_metainfo_dataframe, save_path):\n    updated_metainfo = pd.concat([old_metainfo_dataframe, new_metainfo_dataframe])\n    updated_metainfo.to_csv(save_path.joinpath('meta_info.csv'), sep = ',',index = False)\n    return updated_metainfo\n\ndef save_query_set(query_lati_longti_set,save_path):\n    with open(save_path.joinpath('query_lati_longti_set.pkl'),'wb') as f:\n        pickle.dump(query_lati_longti_set,f)\n\n\ndef resume_from_history_info(save_path):\n    try:\n        meta_info = pd.read_csv(save_path.joinpath('meta_info.csv'),dtype=str)\n        \n        with open(save_path.joinpath('query_lati_longti_set.pkl'),'rb') as f:\n            query_lati_longti_set = pickle.load(f)\n        return meta_info, query_lati_longti_set\n    except:\n        print('未查找到历史信息')\n        return pd.DataFrame(), set()\n\ndef zipdir(path, ziph):\n    for root, dirs, files in os.walk(path):\n        for file in files:\n            ziph.write(os.path.join(root, file))\n\ndef transfer_file(source_path, destination_host, destination_path, username, password=None, private_key=None, port=35167):\n    # Create SSH client\n    ssh = paramiko.SSHClient()\n    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n\n    try:\n        # Connect to source host\n        if password:\n            ssh.connect(destination_host, port=port, username=username, password=password)\n        else:\n            ssh.connect(destination_host, port=port, username=username, key_filename=private_key)\n\n        # SCP file from source host to destination host\n        scp = paramiko.SFTPClient.from_transport(ssh.get_transport())\n        scp.put(source_path, destination_path)\n\n        # Close connection\n        scp.close()\n        ssh.close()\n        return \"File transferred successfully.\"\n    except Exception as e:\n        return (f\"Error: {e}\")"}
{"type": "source_file", "path": "citybench/outdoor_navigation/stitch_image_patches_CUT.py", "content": "import os\nimport argparse\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom PIL import Image\nfrom multiprocessing import Pool\nfrom tqdm import tqdm\nimport copy\nimport random\n\nfrom config import IMAGE_FOLDER\ndef stitch_multiple_region(index=None):\n    # 根据目录一个一个自动进行\n    p = Pool(MULTI_PROCESS_NUM)\n\n    result = [p.apply_async(stitch_single_region, args=(0, str(region.stem))) for region in INPUT_PATH.iterdir()]\n    for i in result:\n        i.get()\n\n    p.close()\n    p.join()\n\n\ndef stitch_single_region(index, region_code):\n    try:\n        save_path_for_region = SAVE_PATH.joinpath(region_code)\n        save_path_for_region.mkdir(exist_ok=True,parents=True)\n        meta_info = pd.read_csv(INPUT_PATH.joinpath(region_code).joinpath('meta_info.csv'))\n        meta_info['x'] = meta_info['file_name'].map(lambda x:int(x.split('&')[-2]))\n        meta_info['y'] = meta_info['file_name'].map(lambda x:int(x.split('&')[-1][:-4]))\n        pano_id_list = meta_info['panoid'].drop_duplicates()\n\n        stitch_info = pd.DataFrame()\n\n        # i = 0\n        for pano_id in tqdm(pano_id_list):\n            pano_img_infos = meta_info.loc[meta_info['panoid'] == pano_id]\n            tmp_stitch_info = {'panoid':[pano_img_infos.iloc[0,0]], 'query_lati':[pano_img_infos.iloc[0,1]],'query_longti':[pano_img_infos.iloc[0,2]],'real_lati':[pano_img_infos.iloc[0,3]],'real_longti':[pano_img_infos.iloc[0,4]],'date':[pano_img_infos.iloc[0,5]]}\n            x_max = pano_img_infos['x'].max()\n            y_max = pano_img_infos['y'].max()\n            y_min = pano_img_infos['y'].min()\n            # 目前发现两种类型：一圈儿16个x和一圈12个x。这两个都对应360度，只不过采样间隙不同\n            if x_max == 15:\n                x_type = '15'\n                to_image_A = Image.new('RGB', (IMG_SIZE * 4, IMG_SIZE * (y_max - y_min + 1)))  # x=0,1,2,3\n                cut_position = random.choice([0,4,8,12])\n                for _, row in pano_img_infos.iterrows():\n                    try:\n                        from_image = Image.open(INPUT_PATH.joinpath(region_code).joinpath(row['file_name']))\n                    except:\n                        print(\"Error File\")\n                        print(INPUT_PATH.joinpath(region_code).joinpath(row['file_name']))\n                    try:\n                        if row['x'] in range(cut_position,cut_position+4):\n                            to_image_A.paste(from_image,(((row['x']-cut_position) * IMG_SIZE, (row['y']-2) * IMG_SIZE)))\n\n                    except:\n                        print('拼接图像出错')\n                        print('panoid: ' + str(pano_id))\n                        print('x: '+str(row['x']))\n                        print('y: '+str(row['y']))\n                        print(INPUT_PATH.joinpath(region_code).joinpath(row['file_name']))\n\n            elif x_max == 12:\n                x_type = '12'\n                # 十分奇怪地多出来一列，直接丢弃了\n                to_image_A = Image.new('RGB', (IMG_SIZE * 3, IMG_SIZE * (y_max - y_min + 1)))\n                cut_position = random.choice([0,3,6,9])\n                for _, row in pano_img_infos.iterrows():\n                    try:\n                        from_image = Image.open(INPUT_PATH.joinpath(region_code).joinpath(row['file_name']))\n                    except:\n                        print(\"Error File\")\n                        print(INPUT_PATH.joinpath(region_code).joinpath(row['file_name']))\n                    try:\n                        if row['x'] in range(cut_position,cut_position+3):\n                            to_image_A.paste(from_image,(((row['x']-cut_position) * IMG_SIZE, (row['y']-2) * IMG_SIZE)))\n\n                    except:\n                        print('拼接图像出错')\n                        print('panoid: ' + str(pano_id))\n                        print('x: '+str(row['x']))\n                        print('y: '+str(row['y']))\n                        print(INPUT_PATH.joinpath(region_code).joinpath(row['file_name']))\n            else:\n                print(\"================================\")\n                print('此处并不符合要求')\n                print(INPUT_PATH.joinpath(region_code).joinpath(row['file_name']))\n                print(pano_id)\n                print(x_max)\n                print(\"================================\")\n\n\n            file_name_A = '&'.join(pano_img_infos.iloc[0,-3].split('&')[:-2]) + '&' + x_type + '&' + str(cut_position) + '.jpg'\n            to_image_A = to_image_A.resize((OUT_IMG_SIZE_W,OUT_IMG_SIZE_H))\n\n            to_image_A.save(save_path_for_region.joinpath(file_name_A),quality=95)\n\n            tmp_stitch_info['type'] = [x_type]\n\n            tmp_stitch_infoA = copy.copy(tmp_stitch_info)\n            tmp_stitch_infoA['file_name'] = [file_name_A]\n            stitch_info = pd.concat([stitch_info,pd.DataFrame(tmp_stitch_infoA)])\n\n            stitch_info.to_csv(save_path_for_region.joinpath('stitch_meta_info.csv'),index=False)\n            # i += 1\n            # if i == 5:\n            #     break\n    except:\n        print('Error')\n        print(INPUT_PATH.joinpath(region_code))\n        print(save_path_for_region)\n        print('================================')\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--city_name', type=str, default=\"Sydney\")\n    parser.add_argument('--multi_process_num', type=int, help='多线程数量，注意不要超过cpu核心数量')\n    parser.add_argument('--image_size', default=512, help='单个图像的尺寸')\n    parser.add_argument('--out_image_size_width', default=512, help='全景图像的宽度')\n    parser.add_argument('--out_image_size_height', default=512, help='全景图像的高度')\n\n    args = parser.parse_args()\n    MULTI_PROCESS_NUM = args.multi_process_num\n    IMG_SIZE = args.image_size\n    OUT_IMG_SIZE_W = args.out_image_size_width\n    OUT_IMG_SIZE_H = args.out_image_size_height\n    INPUT_PATH = os.path.join(IMAGE_FOLDER, f'{args.city_name}_StreetView_Images_origin')\n    SAVE_PATH = IMAGE_FOLDER\n    SAVE_PATH.mkdir(exist_ok=True,parents=True)\n\n    begin_time = datetime.now()\n    stitch_multiple_region()\n    end_time = datetime.now()\n    print('总共耗时：' + str((end_time-begin_time).total_seconds() / 3600 ) + '小时')\n"}
{"type": "source_file", "path": "citybench/urban_exploration/eval.py", "content": "import sys\nimport os\nimport pandas as pd\nimport numpy as np\nimport argparse\nimport signal\nimport random\nimport time\nfrom .utils import *\nfrom config import ROUTING_PATH, MAP_DATA_PATH, MAP_CACHE_PATH, RESOURCE_PATH, RESULTS_PATH, LLM_MODEL_MAPPING, VLM_API\nfrom global_utils import load_map\nfrom serving.vlm_serving import VLMWrapper\n\ndef data_gen(args, SAMPLES, city_map, routing_client, search_type, task_file, exploration_results_path):\n    aoi_df = pd.read_csv(os.path.join(resource_dir, '{}_aois.csv'.format(args.city_name)))\n    aoi_list = []\n    for index, row in aoi_df.iterrows():\n        if pd.notna(row['aoi_name']) and \"nearby\" not in row['aoi_name']:\n            aoi_list.append([row['aoi_name'], row['aoi_id']])\n    selected_lists = random.sample(aoi_list, 2)\n\n    log_file = os.path.join(exploration_results_path, \"logs/{}/{}_{}.jsonl\".format(args.city_name, args.model_name, args.mode))\n    os.makedirs(os.path.dirname(log_file), exist_ok=True)\n\n    count=0\n    while count<SAMPLES:\n        selected_lists = random.sample(aoi_list, 2)\n        init_id=selected_lists[0][1]\n        init_name=selected_lists[0][0]\n        destination_id=selected_lists[1][1]\n        destination_name=selected_lists[1][0]\n        _,success_time,average_step,completion=get_performance(city_map,routing_client,init_id,init_name,destination_id,destination_name,args.model_name,log_file,args.temperature,args.threshold,round=args.exp_round,step=args.max_step,search_type=search_type)\n        if success_time:\n            count=count+1\n            data ={'start_id':[init_id],'start_name':[init_name],\n                'des_id':[destination_id],'des_name':[destination_name],\n                'success_time':[success_time],'average_step':[average_step],'completion':[completion]}\n            df=pd.DataFrame(data)\n            try:\n                existing_df = pd.read_csv(task_file)\n                updated_df = pd.concat([existing_df, df], ignore_index=True)\n                updated_df.to_csv(task_file, index=False)\n            except FileNotFoundError:\n                df.to_csv(task_file, index=False)\n\n\ndef eval_gen(args, city_map, routing_client, search_type, task_file, exploration_results_path):\n    data = pd.read_csv(task_file)\n    result_directory=os.path.join(exploration_results_path, \"{}_result.csv\".format(args.city_name))\n\n    for m_name in [args.model_name]:   \n        success_time=[]\n        average_step=[]\n        completion=[]\n        log_file = os.path.join(exploration_results_path, \"logs/{}/{}_{}.jsonl\".format(args.city_name, m_name, args.mode))\n        os.makedirs(os.path.dirname(log_file), exist_ok=True)\n        model = None\n        if m_name in VLM_API:\n            model_full = m_name\n        else:\n            try:\n                model_full = LLM_MODEL_MAPPING[m_name]\n            except KeyError:\n                model_full = m_name\n                model_wrapper = VLMWrapper(model_full)\n                model = model_wrapper.get_vlm_model()\n\n        for index, row in data.iterrows():\n            start_id = row['start_id']\n            start_name = row['start_name']\n            des_id = row['des_id']\n            des_name = row['des_name']\n            \n            print(start_id, start_name, des_id, des_name)\n            _,suc_time,ave_step,comp=get_performance(city_map,routing_client,start_id,start_name,des_id,des_name,m_name, log_file,args.temperature,args.threshold,round=args.exp_round,step=args.max_step, search_type=search_type, model=model)\n            success_time.append(suc_time)\n            average_step.append(ave_step)\n            completion.append(comp)\n        if \"/\" in m_name:\n            model_back = m_name.replace(\"/\", \"_\")\n        \n        df = pd.DataFrame(\n            {\n                \"City_Name\":[args.city_name],\n                \"Model_Name\":[model_back],\n                \"Exploration_Success_Ratio\":[np.mean(success_time)/args.exp_round],\n                \"Exploration_Average_Steps\":[np.mean(average_step)],\n                \"Exploration_Completion\":[np.mean(completion)]\n            }\n        )\n        try:\n            existing_df = pd.read_csv(result_directory)\n            updated_df = pd.concat([existing_df, df], ignore_index=True)\n            updated_df.to_csv(result_directory, index=False)\n        except FileNotFoundError:\n            df.to_csv(result_directory, index=False)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--city_name\", type=str, default=\"Shanghai\")\n    parser.add_argument(\"--model_name\", type=str, default=\"LLama3-8B\")\n    parser.add_argument(\"--data_name\", type=str, default=\"mini\", choices=[\"all\", \"mini\"])\n    parser.add_argument(\"--samples\", type=int)\n    parser.add_argument(\"--mode\", type=str, default=\"eval\")\n    parser.add_argument(\"--temperature\", type=float, default=0.8)\n    parser.add_argument(\"--threshold\", type=int, default=500)\n    parser.add_argument(\"--exp_round\", type=int, default=5)\n    parser.add_argument(\"--max_step\", type=int, default=15)\n    parser.add_argument(\"--port\", type=int, default=54352)\n    args = parser.parse_args()\n\n    cache_dir = MAP_CACHE_PATH\n    resource_dir = RESOURCE_PATH\n    routing_path = ROUTING_PATH\n    \n    city_map = MAP_DICT[args.city_name]\n\n\n    # 根据data_name设置采样数\n    if args.data_name == \"all\":\n        default_samples = 50\n    else:\n        default_samples = 5\n    # 若设置了命令行参数，则使用设置的samples\n    SAMPLES = args.samples if args.samples is not None else default_samples\n\n    search_type=\"poi\"\n    exploration_tasks_path = \"citydata/exploration_tasks\"\n    os.makedirs(exploration_tasks_path, exist_ok=True)\n    task_file = os.path.join(exploration_tasks_path, 'case_{}.csv'.format(args.city_name))\n    exploration_results_path = os.path.join(RESULTS_PATH, \"exploration_results\")\n\n    m, process, routing_client = load_map(\n        city_map=city_map, \n        cache_dir=cache_dir, \n        routing_path=routing_path, \n        port=args.port)\n    # 等待地图加载完成\n    time.sleep(10)\n    \n    if args.mode==\"gen\":\n        data_gen(args, SAMPLES, city_map=m, routing_client=routing_client, search_type=search_type, task_file=task_file, exploration_results_path=exploration_results_path)\n    elif args.mode==\"eval\":\n        eval_gen(args, city_map=m, routing_client=routing_client, search_type=search_type, task_file=task_file, exploration_results_path=exploration_results_path)\n\n    print(\"send signal\")\n    process.send_signal(sig=signal.SIGTERM)\n    process.wait()\n"}
{"type": "source_file", "path": "citybench/traffic_signal/metrics.py", "content": "import os\nimport json\nimport csv\nfrom config import RESULTS_PATH\n\ndef extract_data_from_json(json_file):\n    with open(json_file, 'r') as f:\n        data = json.load(f)\n    if isinstance(data, list):\n        results = [] \n        for entry in data:\n            if isinstance(entry, dict): \n                city_name = entry.get('city_name', 'Unknown') \n                model_name = entry['model']['model_name'] \n                avg_queue_length = entry['performance']['average_queue_length'] \n                avg_traveling_time = entry['performance']['average_traveling_time']\n                throughput = entry['performance']['throughput']  \n                results.append([city_name, model_name, avg_queue_length, avg_traveling_time, throughput])\n\n        return results\n    else:\n        raise ValueError(f\"The format of {json_file} is not correct!\")\n\ndef process_json_files_in_folder(folder_path, output_csv_file):\n   with open(output_csv_file, mode='w', newline='', encoding='utf-8') as csv_file:\n        writer = csv.writer(csv_file)\n        writer.writerow(['City_Name', 'Model_Name', 'Average_Queue_Length', 'Average_Traveling_Time', 'Throughput'])\n        for filename in os.listdir(folder_path):\n            if filename.endswith('.json'):\n                json_file_path = os.path.join(folder_path, filename)\n                data = extract_data_from_json(json_file_path)\n                for row in data:\n                    writer.writerow(row)\n                    \nfolder_path = os.path.join(RESULTS_PATH, \"signal_results\")\noutput_csv_file = os.path.join(folder_path, \"signal_benchmark_results.csv\")\n\nprocess_json_files_in_folder(folder_path, output_csv_file)\nprint(f\"Signal results have been saved!\")\n\n"}
{"type": "source_file", "path": "citybench/traffic_signal/run_eval.py", "content": "import os\nimport tqdm\nimport time\nimport json\nimport jsonlines\nimport argparse\n\nfrom citysim.signal import simulate\nfrom config import MAP_DATA_PATH, TRIP_DATA_PATH, RESULTS_PATH\n\n\nif __name__ == '__main__':\n\n    ###### 参数设置\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--city_name\", type=str, default=\"NewYork\")\n    parser.add_argument(\"--model_name\", type=str, default=\"LLama3-8B\")\n    parser.add_argument(\"--data_name\", type=str, default=\"mini\", choices=[\"all\", \"mini\"])\n    parser.add_argument(\"--total_time\", type=int)\n\n    args = parser.parse_args()\n\n    signal_results_path = os.path.join(RESULTS_PATH, \"signal_results\")\n    signal_results_logs_path = os.path.join(RESULTS_PATH, \"signal_results\", \"logs\")\n    os.makedirs(signal_results_path, exist_ok=True)\n    os.makedirs(signal_results_logs_path, exist_ok=True)\n\n    # 设置切换时长为30秒\n    TIME_THRESHOLD = 30\n    START_TIME, END_TIME = 30000-1000, 30000+2000\n    # 根据data_name设置总时间\n    if args.data_name == \"all\":\n        default_total_time = 3000\n    else:\n        default_total_time = 300\n    # 若设置了total_time则使用设置的total_time\n    TOTAL_TIME = args.total_time if args.total_time is not None else default_total_time\n\n    MAP_FILE = os.path.join(MAP_DATA_PATH, '{}/{}.map.pb'.format(args.city_name, args.city_name))\n    AGENT_FILE = os.path.join(TRIP_DATA_PATH, '{}_trip_filtered_start_{}_end_{}_extend_5.pb'.format(args.city_name, START_TIME, END_TIME))  \n    SAVE_FILE_NAME = os.path.join(signal_results_path, \"{}_results.json\".format(args.city_name))\n    LOG_FILE_NAME = os.path.join(signal_results_logs_path, \"{}_{}.jsonl\".format(args.city_name, args.model_name))\n    RUNNING_LABEL = \"exp\"\n    MODEL_NAME = args.model_name\n    #################\n\n    \n    #### Start to Simulate\n    aql, att, tp, sim_time, llm_cost_time_sum, invalid_actions, llm_inout = simulate(CITY=args.city_name, MAP_FILE=MAP_FILE, AGENT_FILE=AGENT_FILE, TOTAL_TIME=TOTAL_TIME, START_TIME=START_TIME, MODEL_NAME=MODEL_NAME)\n    invalid_ratio = invalid_actions/(TOTAL_TIME//TIME_THRESHOLD)\n    print(f\"Model:{MODEL_NAME} Average queue length: {aql}, Average traveling time: {att}, Throughput: {tp} Invalid Actions: {invalid_ratio}\")\n    #### Stop Simulation\n\n    ####### 保存结果\n    res = {\n        \"exp_time\": time.asctime( time.localtime(time.time())),\n        \"type\": RUNNING_LABEL,\n        \"city_name\": args.city_name,\n        \"simulator\": {\n            \"time_threshold\": TIME_THRESHOLD,\n            \"start_time\": START_TIME,\n            \"total_time\": TOTAL_TIME,\n            \"map_file\": MAP_FILE,\n            \"agent_file\": AGENT_FILE,\n            \"simulate_time_cost\": round(sim_time, 3)\n            },\n        \"model\": {\n            \"model_name\": MODEL_NAME.replace(\"/\", \"_\"),\n            \"request_time_cost\": round(llm_cost_time_sum, 3),\n        },\n        \"performance\": {\n            \"average_queue_length\": round(aql, 3),\n            \"average_traveling_time\": round(att, 3),\n            \"throughput\": round(tp, 3),\n            \"invalid\": round(invalid_ratio, 3)\n        },\n        \"logs\": {\n            \"file_name\": LOG_FILE_NAME\n        }\n    }\n\n    with jsonlines.open(LOG_FILE_NAME, \"w\") as wid:\n        wid.write_all(llm_inout)\n\n    try:\n        with open(SAVE_FILE_NAME, 'r') as file:\n            data = json.load(file)\n    except FileNotFoundError:\n        data = []\n    data.append(res)\n    with open(SAVE_FILE_NAME, \"w\") as wid:\n        json.dump(data, wid, indent=2)\n"}
{"type": "source_file", "path": "citysim/build_map.py", "content": "import logging\nimport os\nimport pyproj\nimport datetime\nimport argparse\nimport logging\nimport geojson\n\nfrom pymongo import MongoClient\nfrom mosstool.map.builder import Builder\nfrom mosstool.type import Map\nfrom mosstool.map.osm import Building, PointOfInterest, RoadNet\nfrom mosstool.util.format_converter import dict2pb, pb2coll\n\nfrom config import PROXIES, MONGODB_URI\n\ndef get_net(args):\n    ## 提供经纬度范围\n    min_lon, min_lat = args.min_lon, args.min_lat\n    max_lon, max_lat = args.max_lon, args.max_lat\n    proj_str = f\"+proj=tmerc +lat_0={(max_lat+min_lat)/2} +lon_0={(max_lon+min_lon)/2}\"\n    logging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s %(levelname)s %(message)s\",\n    )\n    ## 路网\n    rn = RoadNet(\n        max_latitude=max_lat,\n        min_latitude=min_lat,\n        max_longitude=max_lon,\n        min_longitude=min_lon,\n        proj_str=proj_str,\n        proxies=PROXIES,\n    )\n    rn.create_road_net(output_path=os.path.join(OUTPUT_PATH, f\"roadnet_{args.city_name}.geojson\"))\n    ## AOI\n    building = Building(\n        max_latitude=max_lat,\n        min_latitude=min_lat,\n        max_longitude=max_lon,\n        min_longitude=min_lon,\n        proj_str=proj_str,\n        proxies=PROXIES,\n    )\n    building.create_building(output_path=os.path.join(OUTPUT_PATH, f\"aois_{args.city_name}.geojson\"))\n    ## POI\n    pois = PointOfInterest(\n        max_latitude=max_lat,\n        min_latitude=min_lat,\n        max_longitude=max_lon,\n        min_longitude=min_lon,\n        proxies=PROXIES,\n    )\n    pois.create_pois(output_path=os.path.join(OUTPUT_PATH, f\"pois_{args.city_name}.geojson\"))\n\n\ndef get_map(args):\n    workers = args.workers\n    date = datetime.datetime.now().strftime(\"%m%d\")\n    logging.basicConfig(\n        level=logging.INFO,\n        format=\"%(asctime)s %(levelname)s %(message)s\",\n    )\n   \n    # 加载配置\n    logging.info(f\"Generating map of {args.city_name}\")\n    lat = (args.max_lat + args.min_lat) / 2\n    lon = (args.max_lon + args.min_lon) / 2\n    try:\n        with open(f\"citydata/map_construction/roadnet_{args.city_name}.geojson\", \"r\") as f:\n            net = geojson.load(f)\n        with open(f\"citydata/map_construction/aois_{args.city_name}.geojson\", \"r\") as f:\n            aois = geojson.load(f)\n        with open(f\"citydata/map_construction/pois_{args.city_name}.geojson\", \"r\") as f:\n            pois = geojson.load(f)\n        builder = Builder(\n            net=net,\n            proj_str=f\"+proj=tmerc +lat_0={lat} +lon_0={lon}\",\n            aois=aois,\n            pois=pois,\n            gen_sidewalk_speed_limit=50 / 3.6,\n            road_expand_mode=\"M\",\n            workers=workers,\n        )\n        m = builder.build(args.city_name)\n        pb = dict2pb(m, Map())\n        client = MongoClient(MONGODB_URI)\n        coll = client[\"llmsim\"][f\"map_{args.city_name}_2024{date}\"]\n        pb2coll(pb, coll, drop=True)\n    except Exception as e:\n        print(f\"{args.city_name} failed!\")\n        print(e)\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--city_name\", type=str, default=\"Paris\")\n    parser.add_argument(\"--min_lon\", type=float, default=2.249)\n    parser.add_argument(\"--max_lon\", type=float, default=2.4239)\n    parser.add_argument(\"--min_lat\", type=float, default=48.8115)\n    parser.add_argument(\"--max_lat\", type=float, default=48.9038)\n    parser.add_argument(\"--workers\", \"-ww\", help=\"workers for multiprocessing\", type=int, default=128)\n    args = parser.parse_args()\n    OUTPUT_PATH = \"citydata/map_construction/\"\n    os.makedirs(OUTPUT_PATH, exist_ok=True)\n    get_net(args)\n    get_map(args)"}
{"type": "source_file", "path": "citysim/utils.py", "content": "import re\nimport os\nimport logging\nimport random\nimport pyproj\nimport asyncio\nimport numpy as np\nfrom tqdm import tqdm\nimport geopandas as gpd\nfrom shapely.geometry import Point, Polygon\nfrom typing import Optional, cast, List, Tuple, Dict\nfrom multiprocessing import Pool,cpu_count\n\nfrom mosstool.trip.generator.generate_from_od import TripGenerator\nfrom mosstool.trip.route import RoutingClient\nfrom mosstool.type import Map, Person, Persons\nfrom mosstool.util.format_converter import dict2pb,pb2dict\n\nfrom pycityproto.city.person.v1.person_pb2 import Person\nfrom pycityproto.city.routing.v2.routing_pb2 import RouteType\nfrom pycityproto.city.routing.v2.routing_service_pb2 import GetRouteRequest\nfrom pycityproto.city.trip.v2.trip_pb2 import Schedule, TripMode\nLANE_TYPE_DRIVE = 1\nLANE_TYPE_WALK = 2\n_TYPE_MAP = {\n    TripMode.TRIP_MODE_DRIVE_ONLY: RouteType.ROUTE_TYPE_DRIVING,\n    TripMode.TRIP_MODE_BIKE_WALK: RouteType.ROUTE_TYPE_WALKING,\n    TripMode.TRIP_MODE_BUS_WALK: RouteType.ROUTE_TYPE_WALKING,\n    TripMode.TRIP_MODE_WALK_ONLY: RouteType.ROUTE_TYPE_WALKING,\n}\n\nfrom config import MAP_DATA_PATH, SIGNAL_BOX\n\n\ndef generate_persons(m: Map, city: str, agent_num: Optional[int] = None):\n\n    city_data_path = os.path.join(MAP_DATA_PATH, city)\n    area = gpd.read_file(os.path.join(city_data_path, f\"{city}.shp\"))\n    area = area.to_crs(\"EPSG:4326\")\n    od_path = os.path.join(city_data_path, f\"{city}_od.npy\")\n    od_matrix = np.load(od_path)\n    tg = TripGenerator(\n        m=m,\n    )\n    if agent_num is None:\n        od_persons = tg.generate_persons(\n            od_matrix=od_matrix,\n            areas=area,\n            seed=0,\n        )\n    else:\n        od_persons = tg.generate_persons(\n            od_matrix=od_matrix,\n            areas=area,\n            seed=0,\n            agent_num=agent_num,\n        )\n    return od_persons\n\n\n# ATTENTION:和pre_route不同在于找到一个非空schedule就会返回\nasync def my_pre_route(\n    client: RoutingClient, person: Person, in_place: bool = False\n) -> Person:\n    if not in_place:\n        p = Person()\n        p.CopyFrom(person)\n        person = p\n    start = person.home\n    departure_time = None\n    all_schedules = list(person.schedules)\n    person.ClearField(\"schedules\")\n    for schedule in all_schedules:\n        schedule = cast(Schedule, schedule)\n        if schedule.HasField(\"departure_time\"):\n            departure_time = schedule.departure_time\n        if schedule.loop_count != 1:\n            logging.warning(\n                \"Schedule is not a one-time trip, departure time is not accurate, no pre-calculation is performed\"\n            )\n            start = schedule.trips[-1].end\n            continue\n        good_trips = []\n        for trip in schedule.trips:\n            last_departure_time = departure_time\n            # Cover departure time\n            if trip.HasField(\"departure_time\"):\n                departure_time = trip.departure_time\n            if departure_time is None:\n                continue\n            # ATTENTION:只保留行车\n            if not trip.mode == TripMode.TRIP_MODE_DRIVE_ONLY:\n                departure_time = last_departure_time\n                continue\n            # build request\n            res = await client.GetRoute(\n                GetRouteRequest(\n                    type=_TYPE_MAP[trip.mode],\n                    start=start,\n                    end=trip.end,\n                    time=departure_time,\n                )\n            )\n            if len(res.journeys) == 0:\n                # logging.warning(\"No route found\")\n                departure_time = last_departure_time\n            else:\n                # append directly\n                good_trips.append(trip)\n                trip.ClearField(\"routes\")\n                trip.routes.MergeFrom(res.journeys)\n                # update start position\n                start = trip.end\n                # Set departure time invalid\n                departure_time = None\n        if len(good_trips) > 0:\n            good_trips = good_trips[:1]\n            good_schedule = cast(Schedule, person.schedules.add())\n            good_schedule.CopyFrom(schedule)\n            good_schedule.ClearField(\"trips\")\n            good_schedule.trips.extend(good_trips)\n            break\n    return person\n\ndef pos_transfer_unit(dict_p):\n    global m_lanes,m_roads,m_aois\n    len_schedules = len(dict_p[\"schedules\"])\n    if len_schedules>2:\n        select_idx = random.choice([i for i in range(1,len_schedules-1)])\n        pre_end = dict_p[\"schedules\"][select_idx-1][\"trips\"][0][\"end\"]\n        dict_p[\"schedules\"] = dict_p[\"schedules\"][select_idx:]\n        dict_p[\"home\"] = pre_end\n    ##\n    IS_BAD_PERSON = False\n    aoi_id = dict_p[\"home\"][\"aoi_position\"][\"aoi_id\"]\n    aoi = m_aois[aoi_id]\n    trip_mode = dict_p[\"schedules\"][0][\"trips\"][0][\"mode\"]\n    all_aoi_lane_ids = list(\n        [pos[\"lane_id\"] for pos in aoi[\"walking_positions\"]]\n        + [pos[\"lane_id\"] for pos in aoi[\"driving_positions\"]]\n    )\n    extra_lane_ids = []\n    for lid in all_aoi_lane_ids:\n        for pre in m_lanes[lid][\"predecessors\"]:\n            extra_lane_ids.append(pre[\"id\"])\n        for suc in m_lanes[lid][\"successors\"]:\n            extra_lane_ids.append(suc[\"id\"])\n    all_aoi_lane_ids += extra_lane_ids\n    all_aoi_road_ids = [m_lanes[lid][\"parent_id\"] for lid in all_aoi_lane_ids]\n    all_aoi_road_ids = list(\n        set([pid for pid in all_aoi_road_ids if pid < 3_0000_0000])\n    )\n    all_aoi_drive_lane_ids = [lid for rid in all_aoi_road_ids for lid in m_roads[rid][\"lane_ids\"] if m_lanes[lid][\"type\"] == LANE_TYPE_DRIVE] \n    all_aoi_walk_lane_ids = [lid for rid in all_aoi_road_ids for lid in m_roads[rid][\"lane_ids\"] if m_lanes[lid][\"type\"] == LANE_TYPE_WALK] \n    select_lane_id = None\n    if trip_mode in [\n        TripMode.TRIP_MODE_BIKE_WALK,\n        TripMode.TRIP_MODE_BUS_WALK,\n        TripMode.TRIP_MODE_WALK_ONLY,\n    ]:\n        if len(all_aoi_walk_lane_ids)>0:\n            select_lane_id = random.choice(all_aoi_walk_lane_ids)\n        else:\n            IS_BAD_PERSON = True\n    else:\n        if len(all_aoi_drive_lane_ids)>0:\n            select_lane_id = random.choice(all_aoi_drive_lane_ids)\n        else:\n            IS_BAD_PERSON = True\n    if IS_BAD_PERSON:\n        return None\n    select_lane = m_lanes[select_lane_id]\n    dict_p[\"home\"] = {\n        \"lane_position\":{\n            \"lane_id\":select_lane[\"id\"],\n            \"s\":random.uniform(0.1,0.9) * select_lane[\"length\"],\n        }\n    }\n    # 每个schedule只有一个trip\n    for schedule in dict_p[\"schedules\"]:\n        for trip in schedule[\"trips\"]:\n            if IS_BAD_PERSON:\n                continue\n            aoi_id = trip[\"end\"][\"aoi_position\"][\"aoi_id\"]\n            aoi = m_aois[aoi_id]\n            all_aoi_lane_ids = list(\n                [pos[\"lane_id\"] for pos in aoi[\"walking_positions\"]]\n                + [pos[\"lane_id\"] for pos in aoi[\"driving_positions\"]]\n            )\n            extra_lane_ids = []\n            for lid in all_aoi_lane_ids:\n                for pre in m_lanes[lid][\"predecessors\"]:\n                    extra_lane_ids.append(pre[\"id\"])\n                for suc in m_lanes[lid][\"successors\"]:\n                    extra_lane_ids.append(suc[\"id\"])\n            all_aoi_lane_ids += extra_lane_ids\n            all_aoi_road_ids = [m_lanes[lid][\"parent_id\"] for lid in all_aoi_lane_ids]\n            all_aoi_road_ids = list(\n                set([pid for pid in all_aoi_road_ids if pid < 3_0000_0000])\n            )\n            all_aoi_drive_lane_ids = [lid for rid in all_aoi_road_ids for lid in m_roads[rid][\"lane_ids\"] if m_lanes[lid][\"type\"] == LANE_TYPE_DRIVE] \n            all_aoi_walk_lane_ids = [lid for rid in all_aoi_road_ids for lid in m_roads[rid][\"lane_ids\"] if m_lanes[lid][\"type\"] == LANE_TYPE_WALK] \n            select_lane_id = None\n            if trip_mode in [\n                TripMode.TRIP_MODE_BIKE_WALK,\n                TripMode.TRIP_MODE_BUS_WALK,\n                TripMode.TRIP_MODE_WALK_ONLY,\n            ]:\n                if len(all_aoi_walk_lane_ids)>0:\n                    select_lane_id = random.choice(all_aoi_walk_lane_ids)\n                else:\n                    IS_BAD_PERSON = True\n            else:\n                if len(all_aoi_drive_lane_ids)>0:\n                    select_lane_id = random.choice(all_aoi_drive_lane_ids)\n                else:\n                    IS_BAD_PERSON = True\n            if IS_BAD_PERSON:\n                continue\n            select_lane = m_lanes[select_lane_id]\n            trip[\"end\"] = {\n                \"lane_position\":{\n                    \"lane_id\":select_lane[\"id\"],\n                    \"s\":random.uniform(0.1,0.9) * select_lane[\"length\"],\n                }\n            }\n    if IS_BAD_PERSON:\n        return None\n    else:\n        return dict_p\n\ndef multi_aoi_pos2lane_pos(persons, map_dict):\n    global m_lanes,m_roads,m_aois\n    workers = cpu_count()\n    persons_pb = Persons(persons=persons)\n    persons_list = pb2dict(persons_pb)[\"persons\"]\n    m_aois = {d[\"id\"]: d for d in map_dict[\"aois\"]}\n    m_lanes = {d[\"id\"]: d for d in map_dict[\"lanes\"]}\n    m_roads = {d[\"id\"]: d for d in map_dict[\"roads\"]}\n    new_persons = []\n    MAX_BATCH_SIZE = 15_0000\n    for i in tqdm(range(0, len(persons_list), MAX_BATCH_SIZE)):\n        persons_batch = persons_list[i : i + MAX_BATCH_SIZE]\n        with Pool(processes=workers) as pool:\n            new_persons += pool.map(\n                pos_transfer_unit,\n                persons_batch,\n                chunksize=min(len(persons_batch) // workers, 1000),\n            )\n    new_persons = [dict2pb(p,Person()) for p in new_persons if p is not None]\n    return new_persons\nasync def with_preroute(persons, lanes, listen: str, output_path: str,max_num:int):\n    client = RoutingClient(listen)\n    all_persons = []\n    BATCH = 15000\n    for i in tqdm(range(0, len(persons), BATCH)):\n        ps = await asyncio.gather(\n            *[my_pre_route(client, p) for p in persons[i : i + BATCH]]\n        )\n        all_persons.extend(ps)\n\n    ok_persons = []\n    for p in all_persons:\n        if len(p.schedules) == 0:\n            continue\n        if len(p.schedules[0].trips) == 0:\n            continue\n        BAD_PERSON = False\n        start_id = p.home.lane_position.lane_id\n        end_id = p.schedules[0].trips[0].end.lane_position.lane_id\n        trip_mode = p.schedules[0].trips[0].mode\n        if trip_mode in [\n            TripMode.TRIP_MODE_BIKE_WALK,\n            TripMode.TRIP_MODE_BUS_WALK,\n            TripMode.TRIP_MODE_WALK_ONLY,\n        ]:\n            if (\n                not lanes[start_id][\"type\"] == LANE_TYPE_WALK\n                or not lanes[end_id][\"type\"] == LANE_TYPE_WALK\n            ):\n                BAD_PERSON = True\n        else:\n            if (\n                not lanes[start_id][\"type\"] == LANE_TYPE_DRIVE\n                or not lanes[end_id][\"type\"] == LANE_TYPE_DRIVE\n            ):\n                BAD_PERSON = True\n        if BAD_PERSON:\n            continue\n        ok_persons.append(p)\n    ok_persons = ok_persons[:max_num]\n    for ii, p in enumerate(ok_persons):\n        p.id = ii\n    pb = Persons(persons=ok_persons)\n    with open(output_path, \"wb\") as f:\n        f.write(pb.SerializeToString())\n\ndef get_coords(city: str) -> List[Tuple[float, float]]:\n    # 根据城市名返回坐标列表。\n    coords_dict: Dict[str, List[Tuple[float, float]]] = SIGNAL_BOX\n    return coords_dict.get(city, [])\n\n\n# 对于多个junc，记录phase状态\nclass JunctionState:\n    def __init__(self):\n        self.phase_index = 0\n\n\n# 解析response的相位信息\ndef validate_response(response, phase_map):\n    # Extracting the number enclosed in <signal> tags\n    match = re.search(r'<signal>.*?(\\d+).*?</signal>', response)\n    if match:\n        chosen_number = int(match.group(1))\n        # Check if the chosen number is within the valid range\n        if chosen_number in phase_map:\n            return True, chosen_number\n        else:\n            return False, \"Invalid phase option number.\"\n    return False, \"No valid phase option number provided.\"\n\n\n\ndef process_phase(j, green_indices, filtered_index, cnt, waiting_cnt):\n    phase_info = {}\n    green_pre_lanes = set()\n    vehicle_counts = 0\n    waiting_vehicle_counts = 0\n    for green_index in green_indices:\n        pre_lane_index = j.lanes[green_index].predecessors[0].index\n        green_pre_lanes.add(pre_lane_index)\n\n    for green_pre_lane in green_pre_lanes:\n        \n        vehicle_counts += cnt[green_pre_lane]\n        waiting_vehicle_counts += waiting_cnt[green_pre_lane]\n\n    lane_counts = len(green_pre_lanes)\n    return (filtered_index, lane_counts, vehicle_counts, waiting_vehicle_counts)\n\n\ndef get_chosen_number_from_phase_index(phase_index, phase_map):\n    for chosen_number, current_phase_index in phase_map.items():\n        if current_phase_index == phase_index:\n            return chosen_number\n    print(f\"phase_index: {phase_index}\")\n    print(f\"phase_map: {phase_map}\")\n    return None  # 如果没有找到相应的chosen_number，返回None\n\n\ndef lnglat2xy(M, coords):\n    proj_str = M.pb.header.projection\n    projector = pyproj.Proj(proj_str)\n    coords_array = np.array(coords)\n    xs, ys = projector(coords_array[:, 0], coords_array[:, 1])\n    xy_coords = list(zip(xs, ys))\n    return xy_coords\n\ndef whether_road_in_region(M, coords):\n    # 判断road是否在划分的区域内\n    # 将经纬度转换为平面坐标并存储到列表\n    xy_coords = lnglat2xy(M, coords)\n    print(f\"xy_coords: {xy_coords}\")\n    # print(xy_coords)\n    target_road_ids = []\n    # 获取区域内道路\n    polygon = Polygon(xy_coords)\n    for road in M.roads:\n        # 根据road的第一条lane判断road是否在区域内\n        first_lane_id = road.pb.lane_ids[0]\n        first_lane = M.lane_map[first_lane_id]\n        x_coords = [node.x for node in first_lane.pb.center_line.nodes]\n        y_coords = [node.y for node in first_lane.pb.center_line.nodes]\n\n        # 计算 x 和 y 坐标的平均值\n        x_mean = sum(x_coords) / len(x_coords)\n        y_mean = sum(y_coords) / len(y_coords)\n        point = Point(x_mean, y_mean)\n        if polygon.contains(point):\n            target_road_ids.append(road.pb.id)\n            # print(road.pb.name)\n    return target_road_ids\n\n\ndef whether_junc_in_region(M, j, coords):\n    # 判断junction是否在划分的区域内\n    xy_coords = lnglat2xy(M, coords)\n    polygon = Polygon(xy_coords)\n    \n    first_lane_id = j.pb.lane_ids[0]\n    first_lane = M.lane_map[first_lane_id]\n    x_coords = [node.x for node in first_lane.pb.center_line.nodes]\n    y_coords = [node.y for node in first_lane.pb.center_line.nodes]\n\n    # 计算 x 和 y 坐标的平均值\n    x_mean = sum(x_coords) / len(x_coords)\n    y_mean = sum(y_coords) / len(y_coords)\n    point = Point(x_mean, y_mean)\n    if polygon.contains(point):\n        return True\n    else:\n        return False\n"}
{"type": "source_file", "path": "citybench/urban_exploration/utils.py", "content": "import subprocess\nimport math\nimport os\nimport json\nfrom pycitydata.map import Map\nfrom citysim.routing import RoutingClient\n\nfrom citysim.player import Player\nfrom serving.llm_api import get_chat_completion, extract_choice, get_model_response_hf\nfrom config import MONGODB_URI, MAP_DICT, RESULTS_PATH\n\n\ndef get_distance(start_x,start_y,end_x,end_y):\n    return math.sqrt((end_x - start_x)**2 + (end_y - start_y)**2)\n\n\ndef get_system_prompts():\n    system_prompt = \"\"\"\n    Your navigation destination is {}. \n    You are now on {}, two nearby POIs are:{} and{}\n    Given the available options of road and its corresponding direction to follow and correspoding direction,\n    directly choose the option that will help lead you to the destination.\n    \"\"\"\n    return system_prompt\n\ndef get_system_prompts2():\n    system_prompt = \"\"\"\n    Your navigation destination is {}. \n    You are now near two POIs :{} and{}\n    Given the available options of road and its corresponding direction to follow and correspoding direction,\n    directly choose the option that will help lead you to the destination.\n    \"\"\"\n    return system_prompt\n\n\ndef get_user_prompt():\n    user_prompt=\"the options are:{}.Directly make a choice.\"\n    return user_prompt\n\n\ndef transform_road_data(roads):\n    transformed_roads = {}\n    for road_id, details in roads.items():\n        road_name, direction, distance = details\n\n        # 如果road_name为空或者为unknown，则跳过\n        if not road_name or road_name.lower() == \"unknown\":\n            continue\n\n        # 使用[road_name, direction]作为新字典的键，road_id作为值\n        transformed_roads[(road_name, direction)] = road_id\n\n    return transformed_roads\n\ndef print_dict_with_alphabet(d):\n    # 获取大写英文字母列表\n    alphabet = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n    \n    # 确保字典长度不超过字母表长度\n    if len(d) > len(alphabet):\n        raise ValueError(\"字典长度超过字母表长度\")\n    \n    # 依次输出\n    output = []\n    for i, key in enumerate(d):\n        output.append(f\"{alphabet[i]} {key}\")\n    \n    return \"\\n\".join(output)\n\ndef transform_dict_keys_to_alphabet(d):\n    # 获取大写英文字母列表\n    alphabet = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n    \n    # 确保字典长度不超过字母表长度\n    if len(d) > len(alphabet):\n        raise ValueError(\"字典长度超过字母表长度\")\n    \n    # 创建一个新的字典，将大写字母作为键，输入字典的键作为值\n    transformed_dict = {alphabet[i]: key for i, key in enumerate(d)}\n    \n    return transformed_dict\n\n\n########################################3\ndef get_performance(m,routing_client,init_id,init_name,destination_id,destination_name,m_name,log_file, temperature,thres,round,step=15, search_type=\"poi\", model=None):\n    total_step=0\n    success_time=0\n    temp=0\n    \n    all_session_logs = []\n    for j in range(round):\n        # initial Player\n        session_logs = []\n        player = Player(city_map=m, city_routing_client=routing_client, init_aoi_id=init_id, search_type=search_type)\n        player2=Player(city_map=m, city_routing_client=routing_client, init_aoi_id=destination_id, search_type=search_type)\n        end_xy=dict(player2.get_position()[\"xy_position\"])  \n        start_xy=dict(player.get_position()[\"xy_position\"])\n        initial_dis=get_distance(start_xy['x'], start_xy['y'], end_xy['x'],end_xy['y'])\n        if(initial_dis>2000):\n            break\n        shortest_dis=initial_dis\n        #max step\n        for i in range(step):\n            cur_roadid=player._city_map.get_lane(player.position.lane_position.lane_id)[\"parent_id\"]\n            road_info = player._city_map.get_road(cur_roadid)\n            if road_info and \"external\" in road_info:\n                current_roadname=player._city_map.get_road(cur_roadid)[\"external\"][\"name\"]\n            elif road_info and \"name\" in road_info:\n                current_roadname=player._city_map.get_road(cur_roadid)[\"name\"]\n            else:\n                print(\"not a road,out of border,task failed!\")\n                break\n            current_poi_list=player.get_cur_position()\n            if len(current_poi_list)<2:\n                print(\"no more pois,out of border,task failed!\")\n                break\n            diag= [dict(role=\"system\", content=get_system_prompts().format(destination_name,current_roadname,current_poi_list[0],current_poi_list[1]))]\n            \n            session_logs.append({\"role\": \"system\", \"content\": get_system_prompts().format(destination_name,current_roadname,current_poi_list[0],current_poi_list[1])})\n\n            road_list=player.get_junction_list()\n\n            if isinstance(road_list, tuple):\n                print(\"no more road,out of border,task failed!\")\n                break\n            road=transform_road_data(road_list)\n            candidate=print_dict_with_alphabet(road)\n            result_index=transform_dict_keys_to_alphabet(road_list)\n            diag.append(dict(role=\"user\", content=get_user_prompt().format(candidate)))\n\n            session_logs.append({\"role\": \"user\", \"content\": get_user_prompt().format(candidate)})\n\n            if model is not None:\n                text = get_system_prompts().format(destination_name,current_roadname,current_poi_list[0],current_poi_list[1]) + \"\\n\" + get_user_prompt().format(candidate)\n                res = get_model_response_hf(text, model)\n            else:\n                res, _=get_chat_completion(\n                    session=diag,\n                    model_name=m_name,\n                    temperature=temperature\n                    )\n                \n            session_logs.append({\"role\": \"response\", \"content\": res})\n            \n            lane_choice=extract_choice(res,choice_list = ['A', 'B', 'C', 'D','E','F','G','H','I','J','K','L','M','N'])\n\n            session_logs.append({\"role\": \"extract\", \"content\": lane_choice})\n            \n            if lane_choice in result_index.keys():\n                next_lane_id=result_index[lane_choice]\n            else:\n                print(\"invalid answer!\")\n                break\n\n            player.move_after_decision(next_lane_id)\n            current_dis=player.check_position(end_xy,thres)\n            if not current_dis:\n                shortest_dis=0\n                total_step=total_step+i+1\n                success_time=success_time+1\n                print(\"successfully found! totalstep={}\".format(i+1))\n                session_logs.append({\"role\": \"system\", \"content\": \"successfully found! totalstep={}\".format(i+1)})\n                break\n            else:\n                shortest_dis=min(shortest_dis,current_dis)\n        temp=temp+shortest_dis\n        all_session_logs.extend(session_logs)\n        \n    if success_time:\n        average_step=(total_step+step*(round-success_time))/round\n    else:\n        average_step=step\n    completion=1-temp/(round*initial_dis)\n    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n    print(\"case:{}to{}\".format(init_name,destination_name))\n    print(\"model: {}: success_time:{}, average step:{}, level of completion{}\".format(m_name,success_time,average_step,completion))\n    if all_session_logs != []:\n        with open(log_file, 'a') as f:\n            f.write(json.dumps(all_session_logs) + '\\n')\n    return m_name,success_time,average_step,completion\n"}
{"type": "source_file", "path": "citybench/urban_exploration/metrics.py", "content": "import os\nimport pandas as pd\nfrom config import RESULTS_PATH\n\nparent_folder = os.path.join(RESULTS_PATH, \"exploration_results\")\noutput_file = os.path.join(parent_folder, \"exploration_benchmark_result.csv\")\n\nall_data = []\nfor file_name in os.listdir(parent_folder):\n    file_path = os.path.join(parent_folder, file_name)\n    if os.path.isfile(file_path) and file_name.endswith('.csv'):\n        try:\n            df = pd.read_csv(file_path)\n            all_data.append(df)  \n        except Exception as e:\n            print(f\"Error {file_path} : {e}\")  \n\nif all_data:\n    combined_df = pd.concat(all_data, ignore_index=True)\n    combined_df.to_csv(output_file, index=False)\n    print(\"Exploration results have been saved!\")\nelse:\n    print(\"No exploration results found!\")\n"}
{"type": "source_file", "path": "citysim/grpc.py", "content": "import grpc\n\n__all__ = [\"create_channel\", \"create_aio_channel\"]\n\n\ndef create_channel(server_address: str, secure: bool = False) -> grpc.Channel:\n    \"\"\"\n    创建一个grpc的channel\n    Create a grpc channel\n\n    Args:\n    - server_address (str): 服务器地址。server address.\n    - secure (bool, optional): 是否使用安全连接. Defaults to False. Whether to use a secure connection. Defaults to False.\n\n    Returns:\n    - grpc.Channel: grpc的channel。grpc channel.\n    \"\"\"\n    if server_address.startswith(\"http://\"):\n        server_address = server_address.split(\"//\")[1]\n        if secure:\n            raise ValueError(\"secure channel must use `https` or not use `http`\")\n    elif server_address.startswith(\"https://\"):\n        server_address = server_address.split(\"//\")[1]\n        if not secure:\n            secure = True\n\n    if secure:\n        return grpc.secure_channel(server_address, grpc.ssl_channel_credentials())\n    else:\n        return grpc.insecure_channel(server_address)\n\n\ndef create_aio_channel(server_address: str, secure: bool = False) -> grpc.aio.Channel:\n    \"\"\"\n    创建一个grpc的异步channel\n    Create a grpc asynchronous channel\n\n    Args:\n    - server_address (str): 服务器地址。server address.\n    - secure (bool, optional): 是否使用安全连接. Defaults to False. Whether to use a secure connection. Defaults to False.\n\n    Returns:\n    - grpc.aio.Channel: grpc的异步channel。grpc asynchronous channel.\n    \"\"\"\n    if server_address.startswith(\"http://\"):\n        server_address = server_address.split(\"//\")[1]\n        if secure:\n            raise ValueError(\"secure channel must use `https` or not use `http`\")\n    elif server_address.startswith(\"https://\"):\n        server_address = server_address.split(\"//\")[1]\n        if not secure:\n            secure = True\n\n    if secure:\n        return grpc.aio.secure_channel(server_address, grpc.ssl_channel_credentials())\n    else:\n        return grpc.aio.insecure_channel(server_address)\n"}
{"type": "source_file", "path": "citysim/routing.py", "content": "from typing import Any, Awaitable, Coroutine, cast, Union, Dict\n\nfrom google.protobuf.json_format import ParseDict\nfrom pycityproto.city.routing.v2 import routing_service_pb2 as routing_service\nfrom pycityproto.city.routing.v2 import routing_service_pb2_grpc as routing_grpc\n\nfrom .protobuf import async_parse\nfrom .grpc import create_aio_channel\n\n\nclass RoutingClient:\n    \"\"\"\n    Routing服务的client端\n    Client side of Routing service\n    \"\"\"\n\n    def __init__(self, server_address: str, secure: bool = False):\n        \"\"\"\n        RoutingClient的构造函数\n        Constructor of RoutingClient\n\n        Args:\n        - server_address (str): routing服务器地址。Routing server address\n        - secure (bool, optional): 是否使用安全连接. Defaults to False. Whether to use a secure connection. Defaults to False.\n        \"\"\"\n        aio_channel = create_aio_channel(server_address, secure)\n        self._aio_stub = routing_grpc.RoutingServiceStub(aio_channel)\n\n    def GetRoute(\n        self,\n        req: Union[routing_service.GetRouteRequest, dict],\n        dict_return: bool = True,\n    ) -> Coroutine[Any, Any, Union[Dict[str, Any], routing_service.GetRouteResponse]]:\n        \"\"\"\n        请求导航\n        Request navigation\n\n        Args:\n        - req (routing_service.GetRouteRequest): https://cityproto.sim.fiblab.net/#city.routing.v2.GetRouteRequest\n        - dict_return (bool, optional): 是否返回dict类型的结果. Defaults to True. Whether to return a dict type result. Defaults to True.\n\n        Returns:\n        - https://cityproto.sim.fiblab.net/#city.routing.v2.GetRouteResponse\n        \"\"\"\n        if type(req) != routing_service.GetRouteRequest:\n            req = ParseDict(req, routing_service.GetRouteRequest())\n        res = cast(\n            Awaitable[routing_service.GetRouteResponse], self._aio_stub.GetRoute(req)\n        )\n        return async_parse(res, dict_return)\n"}
{"type": "source_file", "path": "global_utils.py", "content": "from pycitydata.map import Map\nfrom citysim.routing import RoutingClient\nimport subprocess\nfrom config import MONGODB_URI\ndef load_map(city_map, cache_dir, routing_path, port):\n    m = Map(\n            mongo_uri=f\"{MONGODB_URI}\",\n            mongo_db=\"llmsim\",\n            mongo_coll=city_map,\n            cache_dir=cache_dir,\n        )\n    route_command = f\"{routing_path} -mongo_uri {MONGODB_URI} -map llmsim.{city_map} -cache {cache_dir} -listen localhost:{port}\"\n    cmd = route_command.split(\" \")\n    print(\"loading routing service\")\n    process = subprocess.Popen(args=cmd, cwd=\"./\")\n    routing_client = RoutingClient(f\"localhost:{port}\")\n\n    return m, process, routing_client"}
{"type": "source_file", "path": "citysim/protobuf.py", "content": "from typing import Any, Awaitable, TypeVar, Union, Dict\nfrom google.protobuf.message import Message\nfrom google.protobuf.json_format import MessageToDict\n\n__all__ = [\"parse\", \"async_parse\"]\n\nT = TypeVar(\"T\", bound=Message)\n\n\ndef parse(res: T, dict_return: bool) -> Union[Dict[str, Any], T]:\n    \"\"\"\n    将Protobuf返回值转换为dict或者原始值\n    Convert Protobuf return value to dict or original value\n    \"\"\"\n    if dict_return:\n        return MessageToDict(\n            res,\n            including_default_value_fields=True,\n            preserving_proto_field_name=True,\n            use_integers_for_enums=True,\n        )\n    else:\n        return res\n\n\nasync def async_parse(res: Awaitable[T], dict_return: bool) -> Union[Dict[str, Any], T]:\n    \"\"\"\n    将Protobuf await返回值转换为dict或者原始值\n    Convert Protobuf await return value to dict or original value\n    \"\"\"\n    if dict_return:\n        return MessageToDict(\n            await res,\n            including_default_value_fields=True,\n            preserving_proto_field_name=True,\n            use_integers_for_enums=True,\n        )\n    else:\n        return await res\n"}
{"type": "source_file", "path": "evaluate.py", "content": "import os\nimport argparse\nimport pandas as pd\n\nfrom config import CITY_BOUNDARY, VLM_MODELS, LLM_MODELS, TASK_DEST_MAPPING, TASK_METRICS_MAPPING, RESULTS_PATH, RESULTS_FILE, METRICS_SELECTION\n\nclass Evaluator:\n    def __init__(self, city_name, model_name, data_name, task_name) -> None:\n        self.city_list = list(CITY_BOUNDARY.keys())\n        self.model_list = {\"vlm\": VLM_MODELS, \"llm\": LLM_MODELS}\n        self.task_list = list(TASK_DEST_MAPPING.keys())\n        \n        self.city_name_list = city_name.split(\",\")\n        self.model_name_list = model_name.split(\",\")\n        self.task_name_list = task_name.split(\",\")\n        self.data_name = data_name\n\n    def evaluate(self):\n        # TODO: run single task or run task sets\n        self.multiple_task_wrapper(self.task_name_list, self.model_name_list, self.city_name_list)\n\n    def valid_inputs(self):\n        assert self.city_name_list in self.city_list, \"City name is not valid\"\n        assert self.model_name_list in self.model_list, \"Model name is not valid\"\n        assert self.task_name_list in self.task_list, \"Task name is not valid\"\n\n    def single_task_wrapper(self, task_name, model_name, city_name):\n        # run single task \n        task_desc = TASK_DEST_MAPPING[task_name]\n        if task_name in [\"population\", \"objects\"]:\n            eval_scipt = \"python -m {} --city_name={} --data_name={} --model_name={} --task_name={}\".format(task_desc, city_name, self.data_name, model_name, task_name)\n        else:\n            eval_scipt = \"python -m {} --city_name={} --data_name={} --model_name={}\".format(task_desc, city_name, self.data_name, model_name)\n\n        return os.system(eval_scipt)\n\n    def multiple_task_wrapper(self, task_list, model_list, city_list):\n        # TODO running multi tasks efficiently\n        for task in task_list:\n            for model in model_list:\n                for city in city_list:\n                    self.single_task_wrapper(task, model, city)\n\n    def single_task_metrics(self, task_name):\n        # run single task metrics\n        task_metric = TASK_METRICS_MAPPING[task_name]\n        if task_name in [\"population\", \"objects\"]:\n            metric_scipt = \"python -m {} --task_name={}\".format(task_metric, task_name)\n        else:\n            metric_scipt = \"python -m {}\".format(task_metric)\n        \n        return os.system(metric_scipt)\n    \n    def multiple_task_metrics(self, task_list):\n        # run multiple task metrics\n        for task in task_list:\n            self.single_task_metrics(task)\n\n\n    def analyze_results(self):\n        # 生成所有任务的评估结果\n        self.multiple_task_metrics(self.task_list)\n\n    def show_benchmark(self):\n        # TODO 直接展示benchmark结果\n        data_frames = []\n\n        for task in self.task_name_list:\n            if task in RESULTS_FILE:\n                df = pd.read_csv(RESULTS_FILE[task])\n                selected_columns = METRICS_SELECTION.get(task, [])\n                if selected_columns:\n                    df = df[selected_columns]\n                data_frames.append(df)\n\n        if data_frames:\n            merged_df = pd.concat(data_frames, axis=0, ignore_index=True)  # 按行合并\n            merged_df_grouped = merged_df.groupby(['Model_Name'], as_index=False).mean()\n            output_file = os.path.join(RESULTS_PATH, \"benchmark_results.csv\")\n            merged_df_grouped.to_csv(output_file, index=False)\n            print(f\"Benchmark results have been saved!\")\n        else:\n            return pd.DataFrame() \n\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--city_name', type=str, default=\"Beijing\")\n    parser.add_argument('--task_name', type=str, default='traffic')\n    parser.add_argument('--data_name', type=str, default='all')\n    parser.add_argument('--model_name', type=str, default=\"GPT4o\")\n    args = parser.parse_args()\n\n    # Evaluator Initialization\n    Eval = Evaluator(\n        city_name=args.city_name,\n        model_name=args.model_name,\n        data_name=args.data_name,\n        task_name=args.task_name)\n    # Running Evalautor \n    Eval.evaluate()\n    # Analyze Results\n    Eval.analyze_results()\n    # Show Results\n    Eval.show_benchmark()\n"}
{"type": "source_file", "path": "config.py", "content": "import os\n\n# general parameters\nPROXY = \"http://127.0.0.1:10190\"\nPROXIES = {\"http\": PROXY, \"https\": PROXY}\n############# TODO 不可对外暴露，仅用于内部测试\nMONGODB_URI = \"\"\n###########\nMAP_DATA_PATH=\"citydata/EXP_ORIG_DATA/\"\nMAP_CACHE_PATH=\"citydata/map_cache/\"\nRESOURCE_PATH=\"citydata/resource/\"\nROUTING_PATH=\"citydata/routing_linux_amd64\"\nRESULTS_PATH=\"results/\"\n\n# geoqa\nGEOQA_SAMPLE_RATIO = 0.1\n\nGEOQA_TASK_MAPPING_v1 = {\n    \"path\": {\n        \"road_length\": \"eval_road_length.csv\", \n        \"road_od\": \"eval_road_od.csv\",\n        \"road_link\": \"eval_road_link.csv\"\n    },\n    \"node\": {\n        \"poi2coor\": \"poi2coor.csv\",\n        \"AOI_POI_road4\": \"AOI_POI_road4.csv\",\n        \"aoi_near\": \"aoi_near.csv\"\n    },\n    \"landmark\": {\n        \"landmark_env\": \"eval_landmark_env.csv\",  \n        \"landmark_path\": \"eval_landmark_path.csv\"  \n    },\n    \"boundary\": {\n        \"boundary_road\": \"eval_boundary_road.csv\", \n        \"AOI_POI_road1\": \"AOI_POI_road1.csv\",\n        \"AOI_POI_road2\": \"AOI_POI_road2.csv\",\n        \"AOI_POI_road3\": \"AOI_POI_road3.csv\"\n    },\n    \"districts\": {\n        \"aoi2type\": \"aoi2type.csv\", \n        \"type2aoi\": \"type2aoi.csv\",  \n        \"aoi2addr\": \"aoi2addr.csv\",\n        \"AOI_POI5\": \"AOI_POI5.csv\",\n        \"AOI_POI6\": \"AOI_POI6.csv\",\n        \"road_aoi\": \"eval_road_aoi.csv\"\n    },\n    \"others\": {\n    \"AOI_POI3\": \"AOI_POI3.csv\",\n    \"AOI_POI4\": \"AOI_POI4.csv\"\n    }\n}\n\nGEOQA_TASK_MAPPING_v2 = {\n    \"path\": {\n        \"road_length\": \"road_length.csv\", \n        \"road_od\": \"road_od.csv\",\n        \"road_link\": \"road_link.csv\", \n        \"road_arrived_pois\": \"road_arrived_pois.csv\"\n    },\n    \"node\": {\n        \"poi2coor\": \"poi2coor.csv\",\n        \"poi2addr\": \"poi2addr.csv\",\n        \"poi2type\": \"poi2type.csv\",\n        \"type2poi\": \"type2poi.csv\",\n        \"AOI_POI_road4\": \"AOI_POI_road4.csv\"\n\n    },\n    \"landmark\": {\n        \"landmark_env\": \"landmark_env.csv\",  \n        \"landmark_path\": \"landmark_path.csv\"  \n    },\n    \"boundary\": {\n        \"boundary_road\": \"boundary_road.csv\", \n        \"aoi_boundary_poi\": \"aoi_boundary_poi.csv\",\n        \"AOI_POI_road1\": \"AOI_POI_road1.csv\",\n        \"AOI_POI_road2\": \"AOI_POI_road2.csv\",\n        \"AOI_POI_road3\": \"AOI_POI_road3.csv\"\n    },\n    \"districts\": {\n        \"aoi2type\": \"aoi2type.csv\", \n        \"type2aoi\": \"type2aoi.csv\",  \n        \"aoi_poi\": \"aoi_poi.csv\", \n        \"poi_aoi\": \"poi_aoi.csv\",  \n        \"aoi_group\": \"aoi_group.csv\", \n        \"aoi2addr\": \"aoi2addr.csv\",\n        \"districts_poi_type\": \"districts_poi_type.csv\",\n        \"AOI_POI5\": \"AOI_POI5.csv\",\n        \"AOI_POI6\": \"AOI_POI6.csv\"\n    },\n    \"others\": {\n    \"AOI_POI\": \"AOI_POI.csv\",\n    \"AOI_POI2\": \"AOI_POI2.csv\",\n    \"AOI_POI3\": \"AOI_POI3.csv\",\n    \"AOI_POI4\": \"AOI_POI4.csv\"\n    }\n}\n\n# mobility prediction\nMOBILITY_SAMPLE_RATIO=0.1\n# remote sensing\nREMOTE_SENSING_PATH=\"citydata/remote_sensing/\"\nREMOTE_SENSING_RESULTS_PATH=RESULTS_PATH+\"remote_sensing/\"\nREMOTE_SENSING_ZOOM_LEVEL=15\nWORLD_POP_DATA_PATH=\"{}ppp_2020_1km_Aggregated.tif\".format(REMOTE_SENSING_PATH)\n# street view\nSTREET_VIEW_PATH=\"citydata/street_view/\"\nSAMPLE_RATIO = 0.2\nARCGIS_TILE_URL = \"https://wayback.maptiles.arcgis.com/arcgis/rest/services/World_Imagery/WMTS/1.0.0/default028mm/MapServer/tile/\"\n# outdoor navigation\n# IMAGE_FOLDER = f\"citydata/outdoor_navigation_tasks/NEW_StreetView_Images_CUT\"\n# TODO: 为了方便测试，暂时使用绝对路径\nIMAGE_FOLDER = f\"/data3/liutianhui/NEW_StreetView_Images_CUT\"\nSAMPLE_POINT_PATH=\"citydata/outdoor_navigation_tasks/sample_points/\"\nSTEP = 50\n# 控制是否更新该任务\nNAVIGATION_UPGRADE = True\nREGION_CODE = {\n    \"SanFrancisco\": 90000,\n    \"NewYork\": 20000,\n    \"Beijing\": 10087,\n    \"Shanghai\": 20088,\n    \"Mumbai\": 50000,\n    \"Tokyo\": 70000,\n    \"London\": 30000,\n    \"Paris\": 10000,\n    \"Moscow\": 40000,\n    \"SaoPaulo\": 110000,\n    \"Nairobi\": 60000,\n    \"CapeTown\": 80000,\n    \"Sydney\": 120000\n}\n# traffic singal\nTRIP_DATA_PATH=\"citydata/trips/\"\n\n\nVLM_API = [\"GPT4o\", \"GPT4omini\", \"LLama-3.2-90B\", \"LLama-3.2-11B\", \"Qwen2-VL-72B\", \"gpt-4o-mini\"]\nVLM_MODELS = [\n    \"QwenVLPlus\", \"GPT4o\", \"GPT4o_MINI\", \"cogvlm2-llama3-chat-19B\", \"InternVL2-40B\", \"MiniCPM-Llama3-V-2_5\", \"llava_next_yi_34b\", \"llava_next_llama3\", \"Yi_VL_6B\", \"Yi_VL_34B\", \"llava_v1.5_7b\", \"glm-4v-9b\", \"InternVL2-2B\", \"InternVL2-4B\", \"InternVL2-8B\", \"InternVL2-26B\", \"Qwen2-VL-7B-Instruct\", \"Qwen2-VL-2B-Instruct\", \"GPT4omini\", \"LLama-3.2-90B\", \"LLama-3.2-11B\", \"Qwen2-VL-72B\"]\n\nVLLM_MODEL_PATH_PREFIX = \"/data3/fengjie/init_ckpt/\"\nVLLM_MODEL_PATH_PREFIX2 = \"/data1/citygpt/init_ckpt/multi-modal/\"\nVLLM_MODEL_PATH = {\n    \"cogvlm2-llama3-chat-19B\": os.path.join(VLLM_MODEL_PATH_PREFIX, \"cogvlm2-llama3-chat-19B\"),\n    \"InternVL2-40B\": os.path.join(VLLM_MODEL_PATH_PREFIX, \"InternVL2-40B\"),\n    \"MiniCPM-Llama3-V-2_5\": os.path.join(VLLM_MODEL_PATH_PREFIX, \"MiniCPM-Llama3-V-2_5\"),\n    \"llava_next_yi_34b\": os.path.join(VLLM_MODEL_PATH_PREFIX, \"llava-v1.6-34b-hf\"),\n    \"llava_next_llama3\": os.path.join(VLLM_MODEL_PATH_PREFIX, \"llama3-llava-next-8b-hf\"),\n    \"llava_v1.5_7b\": os.path.join(VLLM_MODEL_PATH_PREFIX2, \"llava-1___5-7b-hf\"),\n    \"glm-4v-9b\": os.path.join(VLLM_MODEL_PATH_PREFIX2, \"glm-4v-9b\"),\n    \"Qwen2-VL-2B-Instruct\": os.path.join(VLLM_MODEL_PATH_PREFIX, \"Qwen2-VL-2B-Instruct\"),\n    \"Qwen2-VL-7B-Instruct\": os.path.join(VLLM_MODEL_PATH_PREFIX2, \"Qwen2-VL-7B-Instruct\"),\n    \"Yi_VL_6B\": os.path.join(VLLM_MODEL_PATH_PREFIX2, \"Yi-VL-6B\"),\n    \"Yi_VL_34B\": os.path.join(VLLM_MODEL_PATH_PREFIX2, \"Yi-VL-34B\"),\n    \"InternVL2-2B\": os.path.join(VLLM_MODEL_PATH_PREFIX2, \"InternVL2-2B\"),\n    \"InternVL2-4B\": os.path.join(VLLM_MODEL_PATH_PREFIX2, \"InternVL2-4B\"),\n    \"InternVL2-8B\": os.path.join(VLLM_MODEL_PATH_PREFIX2, \"InternVL2-8B\"),\n    \"InternVL2-26B\": os.path.join(VLLM_MODEL_PATH_PREFIX2, \"InternVL2-26B\")\n}\n\nLLM_MODELS = [\n    \"Qwen2-7B\", \"Qwen2-72B\", \"Intern2.5-7B\", \"Intern2.5-20B\", \n    \"Mistral-7B\", \"Mixtral-8x22B\", \"LLama3-8B\", \"LLama3-70B\", \"Gemma2-9B\", \"Gemma2-27B\", \n    \"DeepSeek-67B\", \"DeepSeekV2\", \"GPT3.5-Turbo\", \"GPT4-Turbo\"]\n\nINFER_SERVER = {\n    \"OpenAI\": [\"GPT3.5-Turbo\", \"GPT4-Turbo\", \"GPT4omini\"],\n    \"DeepInfra\": [\"Mistral-7B\", \"Mixtral-8x22B\", \"LLama3-8B\", \"LLama3-70B\", \"Gemma2-9B\", \"Gemma2-27B\", \"LLama-3.2-90B\", \"LLama-3.2-11B\"],\n    \"Siliconflow\": [\"Qwen2-7B\", \"Qwen2-72B\", \"Intern2.5-7B\", \"Intern2.5-20B\", \"DeepSeekV2\", \"Qwen2-VL-72B\"],\n    \"DeepBricks\": [\"gpt-4o-mini\"]\n}\nLLM_MODEL_MAPPING = {\n    \"Qwen2-7B\":\"Qwen/Qwen2-7B-Instruct\",\n    \"Qwen2-72B\":\"Qwen/Qwen2-72B-Instruct\",\n    \"Intern2.5-7B\":\"internlm/internlm2_5-7b-chat\",\n    \"Intern2.5-20B\":\"internlm/internlm2_5-20b-chat\",\n    \"Mistral-7B\":\"mistralai/Mistral-7B-Instruct-v0.2\", \n    \"Mixtral-8x22B\":\"mistralai/Mixtral-8x22B-Instruct-v0.1\",\n    \"LLama3-8B\":\"meta-llama/Meta-Llama-3-8B-Instruct\",\n    \"LLama3-70B\":\"meta-llama/Meta-Llama-3-70B-Instruct\",\n    \"Gemma2-9B\":\"google/gemma-2-9b-it\",\n    \"Gemma2-27B\":\"google/gemma-2-27b-it\",\n    \"DeepSeekV2\":\"deepseek-ai/DeepSeek-V2-Chat\",\n    \"GPT3.5-Turbo\":\"gpt-3.5-turbo-0125\",\n    \"GPT4-Turbo\":\"gpt-4-turbo-2024-04-09\",\n    \"GPT4omini\":\"gpt-4o-mini-2024-07-18\",\n    \"GPT4o\":\"gpt-4o\"\n}\n\n# 任务运行代码映射\nTASK_DEST_MAPPING = {\n    # text\n    \"traffic\": \"citybench.traffic_signal.run_eval\",\n    \"geoqa\": \"citybench.geoqa.run_eval\",\n    \"mobility\": \"citybench.mobility_prediction.run_eval\",\n    \"exploration\": \"citybench.urban_exploration.eval\",\n    # visual \n    \"population\": \"citybench.remote_sensing.eval_inference\",\n    \"objects\": \"citybench.remoet_sensing.eval_inference\",\n    \"geoloc\": \"citybench.street_view.eval_inference\",\n    \"navigation\": \"citybench.outdoor_navigation.eval\"\n    }\n# 任务统计代码映射\nTASK_METRICS_MAPPING = {\n    # text\n    \"traffic\": \"citybench.traffic_signal.metrics\",\n    \"geoqa\": \"citybench.geoqa.metrics\",\n    \"mobility\": \"citybench.mobility_prediction.metrics\",\n    \"exploration\": \"citybench.urban_exploration.metrics\",\n    # visual \n    \"population\": \"citybench.remote_sensing.metrics\",\n    \"objects\": \"citybench.remoet_sensing.metrics\",\n    \"geoloc\": \"citybench.street_view.metrics\",\n    \"navigation\": \"citybench.outdoor_navigation.metrics\"\n    }\n# 任务结果文件\nRESULTS_FILE = {\n    \"traffic\": os.path.join(RESULTS_PATH, \"signal_results/signal_benchmark_results.csv\"),\n    \"geoqa\": os.path.join(RESULTS_PATH, \"geo_knowledge_result/geoqa_benchmark_result.csv\"),\n    \"mobility\": os.path.join(RESULTS_PATH, \"prediction_results/mobility_benchmark_result.csv\"),\n    \"exploration\": os.path.join(RESULTS_PATH, \"exploration_results/exploration_benchmark_result.csv\"),\n    \"population\": os.path.join(RESULTS_PATH, \"remote_sensing/population_benchmark_results.csv\"),\n    \"objects\": os.path.join(RESULTS_PATH, \"remote_sensing/object_benchmark_results.csv\"),\n    \"geoloc\": os.path.join(RESULTS_PATH, \"street_view/geoloc_benchmark_results.csv\"),\n    \"navigation\": os.path.join(RESULTS_PATH, \"outdoor_navigation_results/navigation_benchmark_result.csv\")\n}\n# 主表列名选择\nMETRICS_SELECTION = {\n    \"traffic\": [\"Average_Queue_Length\", \"Throughput\"], \n    \"geoqa\": [\"GeoQA_Average_Accuracy\"],\n    \"mobility\": [\"Acc@1\", \"F1\"],\n    \"exploration\": [\"Exploration_Success_Ratio\", \"Exploration_Average_Steps\"],\n    \"population\": [\"RMSE\", \"r2\"],\n    \"objects\": [\"Infrastructure_Accuracy\"],\n    \"geoloc\": [\"City_Accuracy\", \"Acc@25km\"],\n    \"navigation\": [\"Navigation_Success_Ratio\", \"Navigation_Average_Distance\"]\n}\n\n# 原始的城市地图信息\nMAP_DICT={\n    \"Beijing\":\"map_beijing_20240808\",\n    \"Shanghai\":\"map_shanghai_20240806\",\n    \"Mumbai\":\"map_mumbai_20240806\",\n    \"Tokyo\":\"map_tokyo_20240807\",\n    \"London\":\"map_london_20240807\",\n    \"Paris\":\"map_paris_20240808\",\n    \"Moscow\":\"map_moscow_20240807\",\n    \"NewYork\":\"map_newyork_20240808\",\n    \"SanFrancisco\":\"map_san_francisco_20240807\",\n    \"SaoPaulo\":\"map_san_paulo_20240808\",\n    \"Nairobi\":\"map_nairobi_20240807\",\n    \"CapeTown\":\"map_cape_town_20240808\",\n    \"Sydney\":\"map_sydney_20240807\"\n}\n\n# 进行交通信号灯控制的局部区域范围\nSIGNAL_BOX={\n    # 左下 右下 右上 左上\n    \"Paris\": [(2.3373, 48.8527), (2.3525, 48.8527), (2.3525, 48.8599), (2.3373, 48.8599)],\n    \"NewYork\": [(-73.9976, 40.7225), (-73.9877, 40.7225), (-73.9877, 40.7271), (-73.9976, 40.7271)],\n    \"Shanghai\": [(121.4214, 31.2409), (121.4465, 31.2409), (121.4465, 31.2525), (121.4214, 31.2525)],\n    \"Beijing\": [(116.326, 39.9839), (116.3492, 39.9839), (116.3492, 39.9943), (116.326, 39.9943)],\n    \"Mumbai\": [(72.8779, 19.064), (72.8917, 19.064), (72.8917, 19.0749), (72.8779, 19.0749)],\n    \"London\": [(-0.1214, 51.5227), (-0.11, 51.5227), (-0.11, 51.5291), (-0.1214, 51.5291)],\n    \"SaoPaulo\": [(-46.6266, -23.5654), (-46.6102, -23.5654), (-46.6102, -23.5555), (-46.6266, -23.5555)],\n    \"Nairobi\": [(36.8076, -1.2771), (36.819, -1.2771), (36.819, -1.2656), (36.8076, -1.2656)],\n    \"Sydney\": [(151.1860, -33.9276), (151.1948, -33.9276), (151.1948, -33.9188), (151.1860, -33.9188)],\n    \"SanFrancisco\": [(-122.4893,37.7781), (-122.4568,37.7781), (-122.4568, 37.7890), (-122.4893, 37.7890)],\n    \"Tokyo\": [(139.7641,35.6611), (139.7742,35.6611), (139.7742, 35.6668), (139.7641, 35.6668)],\n    \"Moscow\":[(37.3999,55.8388), (37.4447,55.8388), (37.4447, 55.8551), (37.3999, 55.8551)],\n    \"CapeTown\":[(18.5080,-33.9935), (18.5080, -33.9821), (18.5245, -33.9821), (18.5245,-33.9935)]\n}\n\n# 城市范围\nCITY_BOUNDARY = {\n    \"SanFrancisco\": [(-122.5099, 37.8076), (-122.5099, 37.6153), (-122.3630, 37.6153), (-122.3630, 37.8076)],\n    \"NewYork\": [(-74.0186, 40.7751), (-74.0186, 40.6551), (-73.8068, 40.6551), (-73.8068, 40.7751)],\n    \"Beijing\": [(116.1536, 40.0891), (116.1536, 39.7442), (116.6082, 39.7442), (116.6082, 40.0891)],\n    \"Shanghai\": [(121.1215, 31.4193), (121.1215, 30.7300), (121.9730, 30.7300), (121.9730, 31.4193)],\n    \"Mumbai\": [(72.7576, 19.2729), (72.7576, 18.9797), (72.9836, 18.9797), (72.9836, 19.2729)],    \n    \"Tokyo\": [(139.6005, 35.8712), (139.6005, 35.5859), (139.9713, 35.5859), (139.9713, 35.8712)],\n    \"London\": [(-0.3159, 51.6146), (-0.3159, 51.3598), (0.1675, 51.3598), (0.1675, 51.6146)],\n    \"Paris\": [(2.249, 48.9038), (2.249, 48.8115), (2.4239, 48.8115), (2.4239, 48.9038)],\n    \"Moscow\": [(37.4016, 55.8792), (37.4016, 55.6319), (37.8067, 55.6319), (37.8067, 55.8792)],\n    \"SaoPaulo\": [(-46.8251, -23.4242), (-46.8251, -23.7765), (-46.4365, -23.7765), (-46.4365, -23.4242)],\n    \"Nairobi\": [(36.6868, -1.1906), (36.6868, -1.3381), (36.9456, -1.3381), (36.9456, -1.1906)],\n    \"CapeTown\": [(18.3472, -33.8179), (18.3472, -34.0674), (18.6974, -34.0674), (18.6974, -33.8179)],\n    \"Sydney\": [(150.8382, -33.6450), (150.8382, -34.0447), (151.2982, -34.0447), (151.2982, -33.6450)]\n}\n\n"}
{"type": "source_file", "path": "citysim/player.py", "content": "from pycityproto.city.geo.v2.geo_pb2 import (\n    AoiPosition,\n    LanePosition,\n    Position,\n    XYPosition,\n    LongLatPosition,\n)\nimport pycityproto.city.routing.v2.routing_pb2 as routing_pb\nimport pycityproto.city.routing.v2.routing_service_pb2 as routing_service\nfrom pycitysim.map import Map\nfrom pycitysim.routing import RoutingClient\n\nimport math\nfrom shapely.geometry import Point, Polygon\nfrom typing import Tuple, cast,Optional, Union, Dict, Any\nfrom .protobuf import async_parse, parse\n\n\ndef calculate_direction_and_distance(start_x, start_y, end_x, end_y):\n    # 计算距离\n    distance = math.sqrt((end_x - start_x)**2 + (end_y - start_y)**2)\n\n    # 计算方位角\n    delta_x = end_x - start_x\n    delta_y = end_y - start_y\n    if delta_x == 0 and delta_y == 0:\n        # 如果起点和终点相同，则方位无法确定\n        direction = \"同一位置\"\n    elif delta_x == 0:\n        # 如果只在y轴上移动，则方位为北或南\n        direction = \"北\" if delta_y > 0 else \"南\"\n    else:\n        # 计算方位角的弧度\n        angle = math.atan(abs(delta_y / delta_x))\n        # 将弧度转换为度\n        angle_deg = math.degrees(angle)\n        # 根据象限确定方位\n        if delta_x > 0 and delta_y > 0:  # 第一象限\n            direction = \"东北\" if angle_deg < 45 else \"北\"\n        elif delta_x < 0 and delta_y > 0:  # 第二象限\n            direction = \"西北\" if angle_deg < 45 else \"北\"\n        elif delta_x < 0 and delta_y < 0:  # 第三象限\n            direction = \"西北\" if angle_deg > 45 else \"西\"\n        elif delta_x > 0 and delta_y < 0:  # 第四象限\n            direction = \"东北\" if angle_deg > 45 else \"东\"\n\n    return direction, distance\n\n\ndef filter_road_info(roads):\n    # 初始化一个新的字典来存储结果\n    filtered_roads = {}\n\n    # 遍历输入的字典\n    for road_id, info in roads.items():\n        road_name, direction, distance = info\n        # 创建一个复合键由road_name和direction组成\n        key = (road_name, direction)\n\n        # 如果这个复合键还没有在结果字典中，或者找到了更小的distance，则更新结果字典\n        if key not in filtered_roads or distance < filtered_roads[key][2]:\n            filtered_roads[key] = [road_name, direction, distance, road_id]\n\n    # 由于我们存储了额外的road_id，我们需要重新整理结果字典，只保留road_id\n    result = {info[-1]: info[:-1] for info in filtered_roads.values()}\n\n    return dict(result)\n\n\nclass Player:\n    def __init__(\n        self,\n        city_map: Map,\n        city_routing_client: RoutingClient,\n        init_aoi_id: int,\n        search_type: str\n    ):\n        self._city_map = city_map\n        self.search_type = search_type\n        self._city_routing_client = city_routing_client\n\n        self.init_position(init_aoi_id)\n        self.time_cost = 0 \n        self.price_cost = 0 \n\n        self.current_road_list = []\n\n    def init_position(self, init_aoi_id):\n        aoi = self._city_map.get_aoi(init_aoi_id)\n        if aoi is None:\n            raise ValueError(f\"aoi {init_aoi_id} not found\")\n        xy = cast(Polygon, aoi[\"shapely_xy\"]).centroid\n        x, y = xy.coords[0][0], xy.coords[0][1]\n        lng, lat = self._city_map.xy2lnglat(x, y)\n        if self._city_map.get_aoi(init_aoi_id)[\"driving_positions\"]:\n            lane_pos = self._city_map.get_aoi(init_aoi_id)[\"driving_positions\"][0]\n        else:\n            lane_pos={\"lane_id\":0,\"s\":0}\n\n        self.position = Position(\n            aoi_position=AoiPosition(aoi_id=init_aoi_id),\n            xy_position=XYPosition(x=x, y=y),\n            longlat_position=LongLatPosition(longitude=lng, latitude=lat),\n            lane_position=LanePosition(lane_id=lane_pos[\"lane_id\"], s=lane_pos[\"s\"])\n        )\n    \n\n    def get_position(self):\n        \"\"\"\n        获取当前位置\n        \"\"\"\n        return parse(self.position, True)\n    \n    def lnglat2xy(self, lng: float, lat: float) -> Tuple[float, float]:\n        \"\"\"\n        经纬度转xy坐标\n        Convert latitude and longitude to xy coordinates\n\n        Args:\n        - lng (float): 经度。longitude.\n        - lat (float): 纬度。latitude.\n\n        Returns:\n        - Tuple[float, float]: xy坐标。xy coordinates.\n        \"\"\"\n        return self.projector(lng, lat)\n\n    def search(\n        self,\n        center:  Union[Tuple[float, float], Point],\n        radius: float,\n        category_prefix: str,\n        limit: int = 10,\n    ):\n        \"\"\"\n        搜索给定范围内的POI\n        \"\"\"\n        if self.search_type==\"poi\":\n            return self._city_map.query_pois(center, radius, category_prefix, limit)\n        elif self.search_type==\"aoi\":\n            return self._city_map.query_aois(center, radius, category_prefix, limit)\n\n    async def get_walking_route(self, aoi_id: int):\n        \"\"\"\n        获取步行路线和代价\n        \"\"\"\n        print(f\"get_walking_route: {self.position.aoi_position.aoi_id} -> {aoi_id}\")\n        resp = await self._city_routing_client.GetRoute(\n            routing_service.GetRouteRequest(\n                type=routing_pb.ROUTE_TYPE_WALKING,\n                start=self.position,\n                end=Position(aoi_position=AoiPosition(aoi_id=aoi_id)),\n            ),\n            dict_return=False,\n        )\n        resp = cast(routing_service.GetRouteResponse, resp)\n        if len(resp.journeys) == 0:\n            return None\n        return parse(resp.journeys[0].walking, True)\n\n    async def get_driving_route(self, aoi_id: int):\n        \"\"\"\n        获取开车路线和代价\n        \"\"\"\n        # print(f\"get_driving_route: {self.position.aoi_position.aoi_id} -> {aoi_id}\")\n        resp = await self._city_routing_client.GetRoute(\n            routing_service.GetRouteRequest(\n                type=routing_pb.ROUTE_TYPE_DRIVING,\n                start=self.position,\n                end=Position(aoi_position=AoiPosition(aoi_id=aoi_id)),\n            ),\n            dict_return=False,\n        )\n        resp = cast(routing_service.GetRouteResponse, resp)\n        if len(resp.journeys) == 0:\n            return None\n        return parse(resp.journeys[0].driving, True)\n\n    async def walk_to(self, aoi_id: int) -> bool:\n        \"\"\"\n        步行到POI，实际产生移动，更新人物位置\n        \"\"\"\n        # 起终点相同，直接返回True\n        if aoi_id == self.position.aoi_position.aoi_id:\n            return True\n        \n        route = await self.get_walking_route(aoi_id)\n        if route is None:\n            return False\n        last_lane_id = route[\"route\"][-1][\"lane_id\"]\n        # 检查对应的AOI Gate\n        aoi = self._city_map.get_aoi(aoi_id)\n        if aoi is None:\n            raise ValueError(f\"aoi {aoi_id} not found\")\n        gate_index = -1\n        for i, p in enumerate(aoi[\"walking_positions\"]):\n            if p[\"lane_id\"] == last_lane_id:\n                gate_index = i\n                break\n        if gate_index == -1:\n            raise ValueError(\n                f\"aoi {aoi_id} has no walking gate for lane {last_lane_id}\"\n            )\n        # 更新人物位置\n        gate_xy = aoi[\"walking_gates\"][gate_index]\n        x, y = gate_xy[\"x\"], gate_xy[\"y\"]\n        lng, lat = self._city_map.xy2lnglat(x, y)\n        self.position = Position(\n            aoi_position=AoiPosition(aoi_id=aoi_id),\n            xy_position=XYPosition(x=x, y=y),\n            longlat_position=LongLatPosition(longitude=lng, latitude=lat),\n        )\n        self.time_cost += route[\"eta\"]\n        self.price_cost += 0\n        return True\n\n    async def drive_to(self, aoi_id: int):\n        \"\"\"\n        开车到POI，实际产生移动，更新人物位置\n        \"\"\"\n         # 起终点相同，直接返回True\n        if aoi_id == self.position.aoi_position.aoi_id:\n            return True\n        \n        route = await self.get_driving_route(aoi_id)\n        if route is None:\n            return False\n        last_road_id = route[\"road_ids\"][-1]\n        # 检查对应的AOI Gate\n        aoi = self._city_map.get_aoi(aoi_id)\n        if aoi is None:\n            raise ValueError(f\"aoi {aoi_id} not found\")\n        gate_index = -1\n        for i, p in enumerate(aoi[\"driving_positions\"]):\n            lane_id = p[\"lane_id\"]\n            lane = self._city_map.get_lane(lane_id)\n            if lane is None:\n                raise ValueError(f\"lane {lane_id} not found\")\n            road_id = lane[\"parent_id\"]\n            if road_id == last_road_id:\n                gate_index = i\n                break\n        if gate_index == -1:\n            raise ValueError(\n                f\"aoi {aoi_id} has no driving gate for road {last_road_id}\"\n            )\n        # 更新人物位置\n        gate_xy = aoi[\"driving_gates\"][gate_index]\n        x, y = gate_xy[\"x\"], gate_xy[\"y\"]\n        lng, lat = self._city_map.xy2lnglat(x, y)\n        self.position = Position(\n            aoi_position=AoiPosition(aoi_id=aoi_id),\n            xy_position=XYPosition(x=x, y=y),\n            longlat_position=LongLatPosition(longitude=lng, latitude=lat),\n        )\n        self.time_cost += route[\"eta\"]\n        self.price_cost += route[\"eta\"]\n        return True\n\n    def get_aoi_of_poi(self, poi_id):\n        if poi_id in self._city_map.pois:\n            aoi_id = self._city_map.get_poi(poi_id)[\"aoi_id\"]\n            return aoi_id\n        else:\n            print(\"POI:{}的没有AOI归属信息信息\".format(poi_id))\n            return None\n    \n    async def set_routing_list(self, poi_id):\n        aoi_id = self.get_aoi_of_poi(poi_id)\n        self.current_road_list = await self.get_driving_route(aoi_id)\n    \n    async def move_step_by_step(self):\n        \"\"\"执行单步移动操作\"\"\"\n        \n        # 获取当前道路\n        current_lane_id = self.position.lane_position.lane_id\n        parent_id = self._city_map.get_lane(current_lane_id)[\"parent_id\"]\n        try:\n            if \"external\" in self._city_map.get_road(parent_id):\n                name = self._city_map.get_road(parent_id)[\"external\"][\"name\"]\n            else:\n                name = self._city_map.get_road(parent_id)[\"name\"]\n            print(\"current_road:\",name)\n        except:\n            print(\"notavailable\")\n\n        try:\n            pre_lane_id = self._city_map.get_lane(current_lane_id)[\"predecessors\"][0][\"id\"]\n        except IndexError as e:\n            return (False, \"No avaiable lanes\")\n        \n        pre_lane_info = self._city_map.get_lane(pre_lane_id)\n\n        # 获取路口\n        junc_id = pre_lane_info[\"parent_id\"]\n        junc_info = self._city_map.get_junction(junc_id)\n\n        # 获取可行路口\n        avaiable_lanes = []\n        for junc_lane_id in junc_info[\"lane_ids\"]:\n            junc_lane_info = self._city_map.get_lane(junc_lane_id)\n\n            for predecessor in junc_lane_info[\"predecessors\"]:\n                junc_pre_lane_id = predecessor[\"id\"]\n                avaiable_lanes.append(junc_pre_lane_id)\n        avaiable_road_names = {}\n        for lane_id in avaiable_lanes:\n            parent_id = self._city_map.get_lane(lane_id)[\"parent_id\"]\n            try:\n                #name = self._city_map.get_road(parent_id)[\"external\"][\"name\"]\n                name = self._city_map.get_road(parent_id)[\"name\"]\n            except:\n                name = \"unknown\"\n            avaiable_road_names[lane_id] = name\n        \n        # 选择一条路前行\n        next_lane_id = avaiable_lanes[0]\n\n        lane_info = self._city_map.get_lane(next_lane_id)\n        endpoint_lnglat = lane_info[\"shapely_lnglat\"].coords[-1]\n        endpoint_xy = lane_info[\"shapely_xy\"].coords[-1]\n\n        # 发生移动，更新位置\n        self.position = Position(\n            xy_position=XYPosition(x=endpoint_xy[0], y=endpoint_xy[1]),\n            longlat_position=LongLatPosition(longitude=endpoint_lnglat[0], latitude=endpoint_lnglat[1]),\n            lane_position=LanePosition(lane_id=next_lane_id, s=0)\n        )\n\n        return (True, \"Move One Step\")\n    \n    def get_junction_list(self):\n        # 获取当前道路与当前位置\n        current_lane_id = self.position.lane_position.lane_id\n        current_xy=dict(self.get_position()[\"xy_position\"])\n        current_lnglat=dict(self.get_position()[\"longlat_position\"])\n        #print(\"cp\",current_lnglat)\n        #print(\"current_lane_id\",current_lane_id)\n        try:\n            pre_lane_id = self._city_map.get_lane(current_lane_id)[\"predecessors\"][0][\"id\"]\n        except IndexError as e:\n            return (False, \"No avaiable lanes\")\n        \n        pre_lane_info = self._city_map.get_lane(pre_lane_id)\n\n        # 获取路口\n        junc_id = pre_lane_info[\"parent_id\"]\n        junc_info = self._city_map.get_junction(junc_id)\n\n        # 获取可行路口\n        avaiable_lanes = []\n        for junc_lane_id in junc_info[\"lane_ids\"]:\n            junc_lane_info = self._city_map.get_lane(junc_lane_id)\n            for predecessor in junc_lane_info[\"predecessors\"]:\n                junc_pre_lane_id = predecessor[\"id\"]\n                #此处添加对lane方向与距离的计算\n                lane_info = self._city_map.get_lane(junc_pre_lane_id)\n                #1.得到行走一步以后的位置\n                endpoint_lnglat_temp = lane_info[\"shapely_lnglat\"].coords[-1]\n                endpoint_lnglat={\"longitude\":endpoint_lnglat_temp[0], \"latitude\":endpoint_lnglat_temp[1]}\n                endpoint_xy_temp = lane_info[\"shapely_xy\"].coords[-1]\n                endpoint_xy={'x':endpoint_xy_temp[0],'y':endpoint_xy_temp[1]}\n                #计算lane对应的方向与距离\n                dir,dis=calculate_direction_and_distance(current_xy['x'], current_xy['y'], endpoint_xy['x'],endpoint_xy['y'])\n\n                avaiable_lanes.append([junc_pre_lane_id,dir,dis])\n        available_road_names = {}\n        for lane in avaiable_lanes:\n            parent_id = self._city_map.get_lane(lane[0])[\"parent_id\"]\n            try:\n                #name = self._city_map.get_road(parent_id)[\"external\"][\"name\"]\n                name = self._city_map.get_road(parent_id)[\"name\"]\n            except:\n                name = \"unknown\"\n            available_road_names[lane[0]] = [name,lane[1],lane[2]]\n        #只保留距离最短的lane\n        filtered_info=filter_road_info(available_road_names)\n        return filtered_info\n    \n    def walk_ds(self, lane_id, ds: float):\n        \"\"\"\n        沿着获取的步行路线，向前走 ds 的距离，更新人物位置\n        \"\"\"\n        lane = self._city_map.get_lane(lane_id)\n        xy = lane[\"shapely_xy\"].interpolate(ds)\n\n        x, y = xy.coords[0][0], xy.coords[0][1]\n        lng, lat = self._city_map.xy2lnglat(x, y)\n        # 更新人物位置\n        self.position = Position(\n            xy_position=XYPosition(x=x, y=y),\n            longlat_position=LongLatPosition(longitude=lng, latitude=lat),\n            lane_position=LanePosition(lane_id=lane_id, s=ds)\n        )\n\n    #player根据选择前进\n    def move_after_decision(self,next_lane_id):\n        lane_info = self._city_map.get_lane(next_lane_id)\n        endpoint_lnglat = lane_info[\"shapely_lnglat\"].coords[-1]\n        endpoint_xy = lane_info[\"shapely_xy\"].coords[-1]\n\n        # 发生移动，更新位置\n        self.position = Position(\n            xy_position=XYPosition(x=endpoint_xy[0], y=endpoint_xy[1]),\n            longlat_position=LongLatPosition(longitude=endpoint_lnglat[0], latitude=endpoint_lnglat[1]),\n            lane_position=LanePosition(lane_id=next_lane_id, s=0)\n        )\n\n    def check_position(self,end_xy,thres):\n        current_xy=dict(self.get_position()[\"xy_position\"])\n        start_x=current_xy['x']\n        start_y=current_xy['y']\n        end_x=end_xy['x']\n        end_y=end_xy['y']\n        distance = math.sqrt((end_x - start_x)**2 + (end_y - start_y)**2)\n        #print(\"updated distance\",distance)\n        \"\"\"确认是否已到达指定位置\"\"\"\n        if distance<thres:\n            return 0\n        else:\n            return distance\n\n\n    def get_cur_position(self):\n        current_xy=dict(self.get_position()[\"xy_position\"])\n        start_x=current_xy['x']\n        start_y=current_xy['y']\n        if self.search_type==\"aoi\":\n            list_temp=self.search([start_x,start_y],1000,[\"E3\"],2)\n        elif self.search_type==\"poi\":\n            list_temp=self.search([start_x,start_y],1000,\"\",2)\n        poi_list=[]\n        for items in list_temp:\n            poi_list.append(items[0]['name'])\n        return poi_list\n\n    # 获取lane_id的ds处中间点经纬度信息\n    def do_walk_to(self, lane_id, ds: float):\n        lane = self._city_map.get_lane(lane_id)\n        xy = lane[\"shapely_xy\"].interpolate(ds)\n        x, y = xy.coords[0][0], xy.coords[0][1]\n        lng, lat = self._city_map.xy2lnglat(x, y)\n        return lng, lat\n        \n    def _complete_position(self, position: Position):\n        \"\"\"\n        根据self.position中的逻辑位置，更新xy坐标与经纬度\n        \"\"\"\n        if position.HasField(\"aoi_position\"):\n            aoi = self._city_map.get_aoi(position.aoi_position.aoi_id)\n            if aoi is None:\n                raise ValueError(f\"aoi {position.aoi_position.aoi_id} not found\")\n            xy = cast(Polygon, aoi[\"shapely_xy\"]).centroid\n        elif position.HasField(\"lane_position\"):\n            lane = self._city_map.get_lane(position.lane_position.lane_id)\n            if lane is None:\n                raise ValueError(f\"lane {position.lane_position.lane_id} not found\")\n            xy = lane[\"shapely_xy\"].interpolate(position.lane_position.s)\n        else:\n            raise ValueError(f\"unknown position type: {position}\")\n        x, y = xy.coords[0][0], xy.coords[0][1]\n        lng, lat = self._city_map.xy2lnglat(x, y)\n        position.xy_position.x = x\n        position.xy_position.y = y\n        position.longlat_position.longitude = lng\n        position.longlat_position.latitude = lat\n"}
{"type": "source_file", "path": "serving/vlm_serving.py", "content": "import os\nimport argparse\nimport transformers\n\nfrom functools import partial\nfrom config import VLLM_MODEL_PATH, VLM_MODELS\n\n\nclass VLMWrapper:\n    def __init__(self, model_name):\n        self.model_name = model_name\n        assert self.model_name in VLM_MODELS\n\n        transformers_version_437=[\"cogvlm2-llama3-chat-19B\", \"InternVL2-40B\", \"llava_v1.5_7b\", \"InternVL2-2B\", \"InternVL2-4B\", \"InternVL2-8B\", \"InternVL2-26B\", \"Yi_VL_6B\", \"Yi_VL_34B\"]\n        transformers_version_440=[\"MiniCPM-Llama3-V-2_5\"]\n        transformers_version_444=[\"llava_next_yi_34b\", \"llava_next_llama3\", \"glm-4v-9b\"]\n        trainformers_version_latest = [\"Qwen2-VL-7B-Instruct\", \"Qwen2-VL-2B-Instruct\"]\n\n        # Install the correct version of transformers\n        if self.model_name in transformers_version_437:\n            if transformers.__version__ != \"4.37.0\":\n                os.system(\"pip install transformers==4.37.0\")\n        elif self.model_name in transformers_version_440:\n            if transformers.__version__ != \"4.40.0\":\n                os.system(\"pip install transformers==4.40.0\")\n        elif self.model_name in transformers_version_444:\n            if transformers.__version__ != \"4.44.2\":\n                os.system(\"pip install transformers==4.44.2\")\n        elif self.model_name in trainformers_version_latest:\n            if transformers.__version__ != \"4.45.0.dev0\":\n                os.system(\"pip install git+https://github.com/huggingface/transformers@21fac7abba2a37fae86106f87fcf9974fd1e3830 accelerate\")\n        else:\n            print(\"no need to update transformers\")\n\n        # place this line after the command \"pip install\"\n        try:\n            from vlmeval.config import supported_VLM\n        except Exception as e:\n            print(e)\n            print(\"need to run this script in vlmeval\")\n\n\n        # only update local model path\n        for model_name in transformers_version_437 + transformers_version_440 + transformers_version_444 + trainformers_version_latest:\n            original_func = supported_VLM[model_name]  \n            if \"glm\" in model_name or \"cogvlm\" in model_name:\n                supported_VLM[model_name] = partial(original_func.func, \n                                                    model_path=VLLM_MODEL_PATH[model_name],\n                                                    max_length=200,\n                                                    **{k: v for k, v in original_func.keywords.items() if k != 'model_path'})\n            else:\n                supported_VLM[model_name] = partial(original_func.func, \n                                                    model_path=VLLM_MODEL_PATH[model_name],\n                                                    max_new_tokens=200,\n                                                    **{k: v for k, v in original_func.keywords.items() if k != 'model_path'})\n        self.enable_proxy()\n        self.model = supported_VLM[self.model_name]()\n\n    def get_vlm_model(self):\n        return self.model\n\n    def enable_proxy(self):\n        # set proxy for OpenAI models\n        if self.model_name in [\"GPT4o\", \"GPT4o_MINI\"]:\n            os.environ[\"http_proxy\"] = 'http://127.0.0.1:10190'\n            os.environ[\"https_proxy\"] = 'http://127.0.0.1:10190'\n\n\n    def clean_proxy(self):\n        try:\n            del os.environ[\"http_proxy\"]\n            del os.environ[\"https_proxy\"]\n        except Exception as e:\n            print(\"Failed to delete proxy environment\")\n"}
{"type": "source_file", "path": "citysim/signal.py", "content": "import os\nimport tqdm\nimport time\nimport numpy as np\nfrom typing import List, Tuple\nfrom multiprocessing import Pool\n\nfrom moss import Engine, LaneChange, TlPolicy\n\nfrom serving.llm_api import get_response_traffic_signal\nfrom config import MAP_DATA_PATH, TRIP_DATA_PATH,  LLM_MODEL_MAPPING\nfrom .utils import JunctionState, validate_response, process_phase, get_chosen_number_from_phase_index, whether_junc_in_region, get_coords\nfrom serving.vlm_serving import VLMWrapper\n\n# 基于LLM输出进行交通灯控制\ndef post_process(response, phase_map, qualified_phase_num, j, state, eng):\n    answer = validate_response(response, phase_map)\n    \n    if answer[0] == True:\n        chosen_number = answer[1]\n        state.phase_index = phase_map[chosen_number]\n        # print(f\"Chosen number: {chosen_number}\")\n        # print(f\"Chosen phase index: {state.phase_index}\")\n    else:\n        # print(answer[1])\n        # print(state.phase_index)\n        # print(j.index)\n        phase_index = state.phase_index if state.phase_index>0 else list(phase_map.values())[0]\n        chosen_number = get_chosen_number_from_phase_index(phase_index, phase_map)\n        \n        new_chosen_number = (chosen_number + 1) % qualified_phase_num\n        state.phase_index = phase_map[new_chosen_number]\n        # print(f\"Chosen number: {chosen_number}\")\n        # print(f\"New Chosen number: {new_chosen_number}\")\n        # print(f\"Chosen phase index: {state.phase_index}\")\n    \n    # 设置路口相位\n    # print(f\"j.index: {j.index}, junction_index: {junction_index}\")\n    eng.set_tl_phase(j.index, state.phase_index)\n    # 返回处理的交叉口索引\n    return answer[0]\n\n\n# 基于模拟器状态构建LLM的prompt\ndef get_prompt(j, cnt, waiting_cnt):\n    # 保存相位信息\n    phase_options = []\n    # 相位选项序号映射\n    phase_map = {} \n    # 过滤掉黄灯后的相位选项序号\n    filtered_index = 0\n    \n    for original_index, phase in enumerate(j.tl.phases):\n        phase_valid = True\n        green_indices = []\n\n        for i, phase_state in enumerate(phase.states):\n            if phase_state.name == \"YELLOW\":\n                # 过滤掉黄灯选项\n                phase_valid = False\n                break\n            if phase_state.name == \"GREEN\":\n                # 记录绿灯序号\n                green_indices.append(i)\n\n        if phase_valid:\n            # 相位序号做映射\n            phase_map[filtered_index] = original_index\n            description = process_phase_prompt(j, green_indices, filtered_index, cnt, waiting_cnt)\n            filtered_index += 1\n            phase_options.append(description)\n\n    # 过滤掉黄灯的总相位数\n    qualified_phase_num = filtered_index\n    # 合并车道信息和相位选项信息\n    basic_descriptions = f\"\"\"A traffic light controls a complex intersection with {qualified_phase_num} signal phases. Each signal relieves vehicles' flow in the allowed lanes.\n    The state of the intersection is listed below. It describes:\n    - The number of lanes relieving vehicles' flow under each traffic light phase.\n    - The total count of vehicles present on lanes permitted to move of each signal.\n    - The count of vehicles queuing on lanes allowed to move, which are close to the end of the road or intersection and moving at low speeds of each signal.\"\"\"\n    prompt = \"\\n\".join([basic_descriptions] + [\"Available Phase Options\"] + phase_options)\n    \n    prompt += \"\"\"\n    Please answer:\n    Which is the most effective traffic signal that will most significantly improve the traffic condition during the\n    next phase?\n\n    Note:\n    The traffic congestion is primarily dictated by the waiting vehicles, with the MOST significant impact. You MUST pay the MOST attention to lanes with long queue lengths.\n\n    Requirements:\n    - Let's think step by step.\n    - You can only choose one of the phase option NUMBER listed above.\n    - You must follow the following steps to provide your analysis:\n        Step 1: Provide your analysis for identifying the optimal traffic signal.\n        Step 2: Answer your chosen phase option number.\n    - Your choice can only be given after finishing the analysis.\n    - Your choice must be identified by the tag: <signal>YOUR_NUMBER</signal>.\n\n    \"\"\"\n    \n    return prompt, phase_map, qualified_phase_num\n\n\ndef process_phase_prompt(j, green_indices, filtered_index, cnt, waiting_cnt):\n    filtered_index, lane_counts, vehicle_counts, waiting_vehicle_counts = process_phase(j, green_indices, filtered_index, cnt, waiting_cnt)\n    description = f\"Phase Option {filtered_index}:\\n -Allowed lane counts: {lane_counts},\\n -Vehicle counts: {vehicle_counts},\\n -Waiting vehicle counts: {waiting_vehicle_counts}\"\n    return description\n\n\n\n#################\ndef simulate(CITY, MAP_FILE, AGENT_FILE, TOTAL_TIME, START_TIME, MODEL_NAME):\n    TIME_THRESHOLD = 30\n    GPU_DEVICE_NUM = 0\n\n    eng = Engine(\n        map_file=MAP_FILE,\n        agent_file= AGENT_FILE,\n        start_step=START_TIME,\n        lane_change=LaneChange.MOBIL,\n        device=GPU_DEVICE_NUM\n    )\n    if MODEL_NAME == \"max-pressure\":\n        # 设置所有路口的信控为MAX_PRESSURE\n        eng.set_tl_policy_batch(range(eng.junction_count), TlPolicy.MAX_PRESSURE)\n    else:\n        # 设置所有路口的信控为FIXED_TIME\n        eng.set_tl_policy_batch(range(eng.junction_count), TlPolicy.FIXED_TIME)\n    eng.set_tl_duration_batch(range(eng.junction_count), TIME_THRESHOLD)\n\n    model = None\n    if MODEL_NAME == \"fixed-time\" or MODEL_NAME == \"max-pressure\":\n        model_full = MODEL_NAME\n    else:\n        try:\n            model_full = LLM_MODEL_MAPPING[MODEL_NAME]\n        except KeyError:\n            model_full = MODEL_NAME\n            model_wrapper = VLMWrapper(model_full)\n            model = model_wrapper.get_vlm_model()\n\n    # 获取地图\n    M = eng.get_map()\n    # 划分的交通灯控制区域\n    coords = get_coords(CITY)\n    # 保存需要的交叉口索引\n    valid_junctions = []\n    junc_states = {}\n    for j in M.junctions:\n        # j = M.junctions[junction_index]\n        if j.tl is None:\n            continue\n        if whether_junc_in_region(M, j, coords) == False:\n            continue\n        # 设置指定路口的信控为手动切换\n        if MODEL_NAME==\"fixed-time\":\n            pass\n        elif MODEL_NAME==\"max-pressure\":\n            pass\n        else:\n            eng.set_tl_policy(j.index, TlPolicy.MANUAL)\n        junc_states[j.index] = JunctionState()\n        valid_junctions.append(j.index)\n\n    print(f\"valid_junctions: {len(valid_junctions)}\")\n\n    aql = []\n\n    print(\"Start Simulate\")\n    sim_time = time.time()\n    llm_cost_time_sum = 0\n    invalid_actions = 0\n    llm_inout = []\n    for epoch in tqdm.tqdm(range(TOTAL_TIME//TIME_THRESHOLD)):\n        # 获取lane上所有的车辆数\n        cnt = eng.get_lane_vehicle_counts()  \n        waiting_cnt = eng.get_lane_waiting_at_end_vehicle_counts() \n        # Average queue length\n        print(f\"current vehicles sum:{cnt.sum()} waiting vehicles sum:{waiting_cnt.sum()}\")\n        aql.append(waiting_cnt.sum()/len(valid_junctions))\n        \n        junction_info = []\n        junction_prompt = []\n        # 获取所有junction参数\n        for junction_index in valid_junctions:\n            j = M.junctions[junction_index]\n            state = junc_states[j.index]\n            prompt, phase_map, qualified_phase_num = get_prompt(j, cnt, waiting_cnt)\n            junction_info.append([prompt, phase_map, qualified_phase_num, j, state])\n            junction_prompt.append([prompt, MODEL_NAME, model])\n        \n        # 并行处理交叉路口LLM访问请求\n        llm_start_time = time.time()\n        with Pool(processes=len(valid_junctions)) as pool:\n            results = pool.starmap(get_response_traffic_signal, junction_prompt)\n        llm_cost_time = time.time()-llm_start_time\n        llm_cost_time_sum += llm_cost_time\n        \n        # 基于LLM回答进行后处理\n        for i, res in enumerate(results):\n            if MODEL_NAME == \"fixed-time\":\n                pass\n            elif MODEL_NAME == \"max-pressure\":\n                pass\n            else:\n                llm_inout.append([{\"role\":\"user\", \"content\": junction_prompt[i][0]}, {\"role\":\"response\", \"content\": res}])\n                ans_state = post_process(res, junction_info[i][1], junction_info[i][2], junction_info[i][3], junction_info[i][4], eng)\n                if not ans_state:\n                    invalid_actions += 1\n\n        print(\"Steps: {} Method: {} Cost: {}\".format(epoch, MODEL_NAME, llm_cost_time))\n        eng.next_step(TIME_THRESHOLD)\n\n    ####### 计算指标\n    sim_time = time.time()-sim_time\n    # Average queue length\n    aql = np.mean(aql)\n    # Average traveling time\n    att = eng.get_departed_vehicle_average_traveling_time()\n    # Throughput\n    tp = eng.get_finished_vehicle_count()\n\n    return aql, att, tp, sim_time, llm_cost_time_sum, invalid_actions, llm_inout\n"}
{"type": "source_file", "path": "serving/llm_api.py", "content": "import os\nimport re\nimport httpx\nimport time\nfrom thefuzz import process\nfrom openai import OpenAI\n\nfrom config import PROXY,LLM_MODEL_MAPPING, INFER_SERVER\n\nOPENAI_APIKEY = os.environ[\"OpenAI_API_KEY\"]\nDEEPINFRA_APIKEY = os.environ[\"DeepInfra_API_KEY\"]\nSILICONFLOW_APIKEY = os.environ[\"SiliconFlow_API_KEY\"]\nDEEPBRICKS_APIKEY = os.environ[\"DeepBricks_API_KEY\"]\n\ndef get_chat_completion(session, model_name, max_tokens=1200, temperature=0, infer_server=None, json_mode=False):\n    client = get_llm_model_client(model_name, infer_server)\n    # 统一--传进来的是model_name\n    model_name = LLM_MODEL_MAPPING[model_name]\n    MAX_RETRIES = 3\n    WAIT_TIME = 1\n    for i in range(MAX_RETRIES):\n        try:\n            if json_mode:\n                try:\n                    response = client.chat.completions.create(\n                                model=model_name,\n                                response_format={\"type\": \"json_object\"},\n                                messages=session,\n                                temperature=temperature,\n                                max_tokens=max_tokens,\n                            )\n                    token_usage = response.usage.completion_tokens\n                    return response.choices[0].message.content, token_usage\n                except Exception as e:\n                    response = client.chat.completions.create(\n                                model=model_name,\n                                messages=session,\n                                temperature=temperature,\n                                max_tokens=max_tokens,\n                            )\n                    token_usage = response.usage.completion_tokens\n                    return response.choices[0].message.content, token_usage\n            else:\n                response = client.chat.completions.create(\n                    model=model_name,\n                    messages=session,\n                    temperature=temperature,\n                    max_tokens=max_tokens,\n                )\n                token_usage = response.usage.completion_tokens\n                return response.choices[0].message.content, token_usage\n        except Exception as e:\n            if i < MAX_RETRIES - 1:\n                time.sleep(WAIT_TIME)\n            else:\n                print(f\"An error of type {type(e).__name__} occurred: {e}\")\n                return \"OpenAI API Error.\",0\n\n\ndef get_llm_model_client(model_name, infer_server=None):\n\n    if infer_server is None:\n        for server_name in INFER_SERVER:\n            if model_name in INFER_SERVER[server_name]:\n                infer_server=server_name\n                break\n\n    # print(f\"Using {infer_server} to infer {model_name}\")\n    # 统一--传进来的是model_name\n    model_name = LLM_MODEL_MAPPING[model_name]\n\n    if infer_server=='OpenAI':\n        \n        client = OpenAI(\n            http_client=httpx.Client(proxy=PROXY),\n            api_key=OPENAI_APIKEY\n            )\n    elif infer_server ==\"DeepInfra\":\n        client = OpenAI(\n        base_url=\"https://api.deepinfra.com/v1/openai\",\n        api_key=DEEPINFRA_APIKEY,\n        http_client=httpx.Client(proxies=PROXY),\n            )\n    elif infer_server ==\"Siliconflow\":\n        client = OpenAI(\n        api_key=SILICONFLOW_APIKEY,\n        base_url=\"https://api.siliconflow.cn/v1\"\n        )\n    elif infer_server ==\"DeepBricks\":\n        client = OpenAI(\n        base_url=\"https://api.deepbricks.ai/v1/\",\n        api_key=DEEPBRICKS_APIKEY,\n        http_client=httpx.Client(proxies=PROXY),\n        )\n    else:\n        raise NotImplementedError\n\n    return client\n\n\ndef get_response_traffic_signal(prompt, model_name, max_tokens=500, temperature=0):\n\n    if model_name in ['fixed-time', 'max-pressure']:\n        return \"Template for fixed-time/max-pressure control\"\n    \n    dialogs = [{\"role\": \"user\", \"content\": prompt}]\n    res, _ = get_chat_completion(session=dialogs, model_name=model_name, max_tokens=max_tokens, temperature=temperature)\n\n    return res\n\n\ndef extract_choice(gen, choice_list):\n    # answer is A | choice is A | choose A\n    res = re.search(\n        r\"(?:(?:[Cc]hoose)|(?:(?:[Aa]nswer|[Cc]hoice)(?![^A-H]{0,20}?(?:n't|not))[^A-H]{0,10}?\\b(?:|is|:|be))\\b)[^A-H]{0,20}?\\b([A-H])\\b\",\n        gen,\n    )\n\n    # A is correct | A is right\n    if res is None:\n        res = re.search(\n            r\"\\b([A-H])\\b(?![^A-H]{0,8}?(?:n't|not)[^A-H]{0,5}?(?:correct|right))[^A-H]{0,10}?\\b(?:correct|right)\\b\",\n            gen,\n        )\n\n    # straight answer: A\n    if res is None:\n        res = re.search(r\"^([A-H])(?:\\.|,|:|$)\", gen)\n\n    # simply extract the first appeared letter\n    if res is None:\n        res = re.search(r\"(?<![a-zA-Z])([A-H])(?![a-zA-Z=])\", gen)\n\n    if res is None:\n        return choice_list[choice_list.index(process.extractOne(gen, choice_list)[0])]\n    \n    return res.group(1)\n\n\ndef get_model_response_hf(prompt, model):\n    response = model.generate(prompt)\n    # print(response)\n    return response\n\ndef get_model_response_hf_image(image_path, prompt, model):\n    response = model.generate([image_path, prompt])\n    # print(response)\n    return response\n\n\ndef match_response(action):\n    pattern = r\"\\b(left|right|forward|stop)\\b\"\n    match = re.search(pattern, action, re.IGNORECASE)  \n    if match:\n        return match.group(1).lower()  \n    else:\n        return \"forward\"  # default action\n    \n\ndef match_response_reason(response):\n    try:\n        reason_part = response.split(\"Reason: \")[1].split(\"Action: \")[0].strip()\n        action_part = response.split(\"Action: \")[1].strip()\n        pattern = r\"\\b(left|right|forward|stop)\\b\"\n        match = re.search(pattern, action_part, re.IGNORECASE)\n        \n        if match:\n            action = match.group(0).lower()\n        else:\n            action = None\n        # print(f\"Reason: {reason_part}, action: {action}\")\n        return reason_part, action\n    except IndexError:\n        return None,None\n"}
