{"repo_info": {"repo_name": "AgentPrune", "repo_owner": "yanweiyue", "repo_url": "https://github.com/yanweiyue/AgentPrune"}}
{"type": "source_file", "path": "AgentPrune/__init__.py", "content": ""}
{"type": "source_file", "path": "AgentPrune/graph/autogen_node.py", "content": "from typing import List, Dict, Optional, Any\nimport asyncio\nfrom AgentPrune.graph.node import Node\n\nclass NodeAutoGen(Node):\n    def __init__(self, id: Optional[str], agent_name:str=\"\", domain:str=\"\", llm_name:str = \"\",):\n        super().__init__(id, agent_name, domain, llm_name)\n        self.conversation_history : List[Dict] = [] # chat history of the whole conversation\n\n    def spatial_or_temporal(self):\n        if len(self.spatial_predecessors) and len(self.temporal_predecessors) == 0:\n            return 'spatial'\n        elif len(self.spatial_predecessors) == 0 and len(self.temporal_predecessors):\n            return 'temporal'\n        elif len(self.spatial_predecessors) == 0 and len(self.temporal_predecessors) == 0:\n            return 'none'\n        else:\n            return 'both'\n        \n    def get_neighbor_history(self,input:str)->List[Dict]:\n        neighbor_type = self.spatial_or_temporal()\n        if neighbor_type == 'spatial':\n            neighbor = self.spatial_predecessors[0]\n        elif neighbor_type == 'temporal':\n            neighbor = self.temporal_predecessors[0]\n        elif neighbor_type == 'both':\n            neighbor = self.temporal_predecessors[0]\n        else:\n            neighbor = None\n        if neighbor is not None:\n            conversation_history = neighbor.conversation_history\n        else:\n            conversation_history = []\n        if len(conversation_history) == 0:\n            conversation_history = [{'role':'user','content':input}]\n        return conversation_history\n        \n    def update_conversation_history(self, content:str, neighbor_history:List[Dict]=[]):\n        self.conversation_history = neighbor_history + [{'role':'assistant','content':content}]\n"}
{"type": "source_file", "path": "AgentPrune/agents/__init__.py", "content": "from AgentPrune.agents.analyze_agent import AnalyzeAgent\nfrom AgentPrune.agents.code_writing import CodeWriting\nfrom AgentPrune.agents.math_solver import MathSolver\nfrom AgentPrune.agents.adversarial_agent import AdverarialAgent\nfrom AgentPrune.agents.final_decision import FinalRefer,FinalDirect,FinalWriteCode,FinalMajorVote\nfrom AgentPrune.agents.agent_registry import AgentRegistry\nfrom AgentPrune.agents.analyze_agent_ag import AnalyzeAgentAG\nfrom AgentPrune.agents.code_writing_ag import CodeWritingAG\nfrom AgentPrune.agents.math_solver_ag import MathSolverAG\nfrom AgentPrune.agents.final_decision_ag import FinalReferAG,FinalWriteCodeAG\n\n__all__ =  ['AnalyzeAgent',\n            'CodeWriting',\n            'MathSolver',\n            'AdverarialAgent',\n            'FinalRefer',\n            'FinalDirect',\n            'FinalWriteCode',\n            'FinalMajorVote',\n            'AgentRegistry',\n            'AnalyzeAgentAG',\n            'CodeWritingAG',\n            'MathSolverAG',\n            'FinalReferAG',\n            'FinalWriteCodeAG',\n           ]\n"}
{"type": "source_file", "path": "AgentPrune/agents/math_solver.py", "content": "from typing import List,Any,Dict\n\nfrom AgentPrune.graph.node import Node\nfrom AgentPrune.agents.agent_registry import AgentRegistry\nfrom AgentPrune.llm.llm_registry import LLMRegistry\nfrom AgentPrune.prompt.prompt_set_registry import PromptSetRegistry\nfrom AgentPrune.tools.coding.python_executor import execute_code_get_return\nfrom dataset.gsm8k_dataset import gsm_get_predict\n\n@AgentRegistry.register('MathSolver')\nclass MathSolver(Node):\n    def __init__(self, id: str | None =None, role:str = None ,domain: str = \"\", llm_name: str = \"\",):\n        super().__init__(id, \"MathSolver\" ,domain, llm_name)\n        self.llm = LLMRegistry.get(llm_name)\n        self.prompt_set = PromptSetRegistry.get(domain)\n        self.role = self.prompt_set.get_role() if role is None else role\n        self.constraint = self.prompt_set.get_constraint(self.role) \n        \n    def _process_inputs(self, raw_inputs:Dict[str,str], spatial_info:Dict[str,Dict], temporal_info:Dict[str,Dict], **kwargs)->List[Any]:\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Process the raw_inputs(most of the time is a List[Dict]) \"\"\"             \n        system_prompt = self.constraint\n        spatial_str = \"\"\n        temporal_str = \"\"\n        user_prompt = self.prompt_set.get_answer_prompt(question=raw_inputs[\"task\"],role=self.role)\n        if self.role == \"Math Solver\":\n            user_prompt += \"(Hint: The answer is near to\"\n            for id, info in spatial_info.items():\n                user_prompt += \" \"+gsm_get_predict(info[\"output\"])\n            for id, info in temporal_info.items():\n                user_prompt += \" \"+gsm_get_predict(info[\"output\"])\n            user_prompt += \").\"\n        else:\n            for id, info in spatial_info.items():\n                spatial_str += f\"Agent {id} as a {info['role']} his answer to this question is:\\n\\n{info['output']}\\n\\n\"\n            for id, info in temporal_info.items():\n                temporal_str += f\"Agent {id} as a {info['role']} his answer to this question was:\\n\\n{info['output']}\\n\\n\"\n            user_prompt += f\"At the same time, there are the following responses to the same question for your reference:\\n\\n{spatial_str} \\n\\n\" if len(spatial_str) else \"\"\n            user_prompt += f\"In the last round of dialogue, there were the following responses to the same question for your reference: \\n\\n{temporal_str}\" if len(temporal_str) else \"\"\n        return system_prompt, user_prompt\n    \n    def _execute(self, input:Dict[str,str],  spatial_info:Dict[str,Any], temporal_info:Dict[str,Any],**kwargs):\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Use the processed input to get the result \"\"\"\n        system_prompt, user_prompt = self._process_inputs(input, spatial_info, temporal_info)\n        message = [{'role':'system','content':system_prompt},{'role':'user','content':user_prompt}]\n        response = self.llm.gen(message)\n        return response\n\n    async def _async_execute(self, input:Dict[str,str],  spatial_info:Dict[str,Any], temporal_info:Dict[str,Any],**kwargs):\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Use the processed input to get the result \"\"\"\n        \"\"\" The input type of this node is Dict \"\"\"\n        system_prompt, user_prompt = self._process_inputs(input, spatial_info, temporal_info)\n        message = [{'role':'system','content':system_prompt},{'role':'user','content':user_prompt}]\n        response = await self.llm.agen(message)\n        if self.role == \"Programming Expert\":\n            answer = execute_code_get_return(response.lstrip(\"```python\\n\").rstrip(\"\\n```\"))\n            response += f\"\\nthe answer is {answer}\"\n        return response"}
{"type": "source_file", "path": "AgentPrune/agents/analyze_agent_ag.py", "content": "from typing import List,Any,Dict\nimport re\n\nfrom AgentPrune.graph.autogen_node import NodeAutoGen\nfrom AgentPrune.agents.agent_registry import AgentRegistry\nfrom AgentPrune.llm.llm_registry import LLMRegistry\nfrom AgentPrune.prompt.prompt_set_registry import PromptSetRegistry\nfrom AgentPrune.tools.search.wiki import search_wiki_main\n\ndef find_strings_between_pluses(text):\n    return re.findall(r'\\@(.*?)\\@', text)\n\n@AgentRegistry.register('AnalyzeAgentAG')\nclass AnalyzeAgentAG(NodeAutoGen):\n    def __init__(self, id: str | None =None, role:str = None,  domain: str = \"\", llm_name: str = \"\",):\n        super().__init__(id, \"AnalyzeAgentAG\" ,domain, llm_name)\n        self.llm = LLMRegistry.get(llm_name)\n        self.prompt_set = PromptSetRegistry.get(domain)\n        self.role = self.prompt_set.get_role() if role is None else role\n        self.constraint = self.prompt_set.get_analyze_constraint(self.role)\n        \n    async def _process_inputs(self, raw_inputs:Dict[str,str], spatial_info:Dict[str,Dict], temporal_info:Dict[str,Dict], **kwargs)->List[Any]:\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Process the raw_inputs(most of the time is a List[Dict]) \"\"\"              \n        system_prompt = f\"{self.constraint}\"\n        user_prompt = f\"The task is: {raw_inputs['task']}\\n\" if self.role != 'Fake' else self.prompt_set.get_adversarial_answer_prompt(raw_inputs['task'])\n        user_prompt += \"Please continue to complete this task based on the previous conversation.\"\n        return system_prompt, user_prompt\n                \n    def _execute(self, input:Dict[str,str],  spatial_info:Dict[str,Dict], temporal_info:Dict[str,Dict],**kwargs):\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Use the processed input to get the result \"\"\"\n        history_conversation = self.get_neighbor_history(input['task'])\n        system_prompt, user_prompt = self._process_inputs(input, spatial_info, temporal_info)\n        message = history_conversation + [{'role':'system','content':system_prompt},{'role':'user','content':user_prompt}]\n        response = self.llm.gen(message)\n        self.update_conversation_history(response,history_conversation)\n        return response\n\n    async def _async_execute(self, input:Dict[str,str],  spatial_info:Dict[str,Dict], temporal_info:Dict[str,Dict],**kwargs):\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Use the processed input to get the result \"\"\"\n        history_conversation = self.get_neighbor_history(input['task'])\n        system_prompt, user_prompt = await self._process_inputs(input, spatial_info, temporal_info)\n        message = history_conversation + [{'role':'system','content':system_prompt},{'role':'user','content':user_prompt}]\n        response = await self.llm.agen(message)\n        self.update_conversation_history(response,history_conversation)\n        return response"}
{"type": "source_file", "path": "AgentPrune/agents/agent_registry.py", "content": "from typing import Type\nfrom class_registry import ClassRegistry\n\nfrom AgentPrune.graph import Node\n\n\nclass AgentRegistry:\n    registry = ClassRegistry()\n\n    @classmethod\n    def register(cls, *args, **kwargs):\n        return cls.registry.register(*args, **kwargs)\n    \n    @classmethod\n    def keys(cls):\n        return cls.registry.keys()\n\n    @classmethod\n    def get(cls, name: str, *args, **kwargs) -> Node:\n        return cls.registry.get(name, *args, **kwargs)\n\n    @classmethod\n    def get_class(cls, name: str) -> Type:\n        return cls.registry.get_class(name)\n"}
{"type": "source_file", "path": "AgentPrune/agents/adversarial_agent.py", "content": "from typing import List,Any,Dict\nimport re\nimport asyncio\n\nfrom AgentPrune.graph.node import Node\nfrom AgentPrune.agents.agent_registry import AgentRegistry\nfrom AgentPrune.llm.llm_registry import LLMRegistry\nfrom AgentPrune.prompt.prompt_set_registry import PromptSetRegistry\nfrom AgentPrune.tools.search.wiki import search_wiki_main\n\n\n@AgentRegistry.register('AdverarialAgent')\nclass AdverarialAgent(Node):\n    def __init__(self, id: str | None =None, role:str = None,  domain: str = \"\", llm_name: str = \"\",):\n        super().__init__(id, \"AdverarialAgent\" ,domain, llm_name)\n        self.llm = LLMRegistry.get(llm_name)\n        self.prompt_set = PromptSetRegistry.get(domain)\n        self.role = 'Normal' if role is None else role\n        self.constraint = self.prompt_set.get_constraint()\n        \n    async def _process_inputs(self, raw_inputs:Dict[str,str], spatial_info:Dict[str,Dict], temporal_info:Dict[str,Dict], **kwargs)->List[Any]:\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Process the raw_inputs(most of the time is a List[Dict]) \"\"\" \n \n        system_prompt = f\"{self.constraint}\" if self.role == \"Normal\" else \"You are a liar who only tells lies.\"\n        # system_prompt = f\"{self.constraint}\" if self.role == \"Normal\" else \"You are a gibbering idiot\"\n        user_prompt = f\"The task is: {raw_inputs['task']}\\n\" if self.role == 'Normal' else self.prompt_set.get_adversarial_answer_prompt(raw_inputs['task'])\n        if self.role == 'Fake':\n            return system_prompt, user_prompt\n        spatial_str = \"\"\n        temporal_str = \"\"\n        for id, info in spatial_info.items():\n            spatial_str += f\"Agent {id}, output is:\\n\\n {info['output']}\\n\\n\"\n        for id, info in temporal_info.items():\n            temporal_str += f\"Agent {id}, output is:\\n\\n {info['output']}\\n\\n\"\n        user_prompt += f\"At the same time, the outputs of other agents are as follows:\\n\\n{spatial_str} \\n\\n\" if len(spatial_str) else \"\"\n        user_prompt += f\"In the last round of dialogue, the outputs of other agents were: \\n\\n{temporal_str}\" if len(temporal_str) else \"\"\n        return system_prompt, user_prompt\n                \n    def _execute(self, input:Dict[str,str],  spatial_info:Dict[str,Dict], temporal_info:Dict[str,Dict],**kwargs):\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Use the processed input to get the result \"\"\"\n  \n        system_prompt, user_prompt = self._process_inputs(input, spatial_info, temporal_info)\n        message = [{'role':'system','content':system_prompt},{'role':'user','content':user_prompt}]\n        response = self.llm.gen(message)\n        return response\n\n    async def _async_execute(self, input:Dict[str,str],  spatial_info:Dict[str,Dict], temporal_info:Dict[str,Dict],**kwargs):\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Use the processed input to get the result \"\"\"\n        system_prompt, user_prompt = await self._process_inputs(input, spatial_info, temporal_info)\n        message = [{'role':'system','content':system_prompt},{'role':'user','content':user_prompt}]\n        response = await self.llm.agen(message)\n        return response"}
{"type": "source_file", "path": "AgentPrune/agents/code_writing_ag.py", "content": "from typing import List,Any,Dict\n\nfrom AgentPrune.graph.autogen_node import NodeAutoGen\nfrom AgentPrune.agents.agent_registry import AgentRegistry\nfrom AgentPrune.llm.llm_registry import LLMRegistry\nfrom AgentPrune.prompt.prompt_set_registry import PromptSetRegistry\nfrom AgentPrune.tools.coding.python_executor import PyExecutor\n\n@AgentRegistry.register('CodeWritingAG')\nclass CodeWritingAG(NodeAutoGen):\n    def __init__(self, id: str | None =None, role:str = None ,domain: str = \"\", llm_name: str = \"\",):\n        super().__init__(id, \"CodeWritingAG\" ,domain, llm_name)\n        self.llm = LLMRegistry.get(llm_name)\n        self.prompt_set = PromptSetRegistry.get(domain)\n        self.role = self.prompt_set.get_role() if role is None else role\n        self.constraint = self.prompt_set.get_constraint(self.role) \n        \n    def _process_inputs(self, input:Dict[str,str], spatial_info:Dict[str,Dict], temporal_info:Dict[str,Dict], **kwargs)->List[Any]:\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Process the raw_inputs(most of the time is a List[Dict]) \"\"\"             \n        system_prompt = self.constraint\n        user_prompt = f\"The task is: {input['task']}\\n\"\n        history_conversation = self.get_neighbor_history(input['task'])\n        for i, message in enumerate(history_conversation):\n            output = message['content']\n            if output.startswith(\"```python\") and output.endswith(\"```\"):\n                output = output.lstrip(\"```python\\n\").rstrip(\"\\n```\")\n                is_solved, feedback, state = PyExecutor().execute(output, self.internal_tests, timeout=10)\n                if is_solved and len(self.internal_tests):\n                    return \"is_solved\", output\n                if i == len(history_conversation) - 1:\n                    user_prompt += f\"The code written by the previous agent is:\\n\\n{output}\\n\\n The code failed the test and the feedback was\\n\\n {feedback}.\\n\\n\"\n        user_prompt += \"Please continue to complete this task based on the previous conversation.\"\n        return system_prompt, user_prompt\n\n    def extract_example(self, prompt: str) -> list:\n        prompt = prompt['task']\n        lines = (line.strip() for line in prompt.split('\\n') if line.strip())\n\n        results = []\n        lines_iter = iter(lines)\n        for line in lines_iter:\n            if line.startswith('>>>'):\n                function_call = line[4:]\n                expected_output = next(lines_iter, None)\n                if expected_output:\n                    results.append(f\"assert {function_call} == {expected_output}\")\n\n        return results\n    \n    def _execute(self, input:Dict[str,str],  spatial_info:Dict[str,Any], temporal_info:Dict[str,Any],**kwargs):\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Use the processed input to get the result \"\"\"\n        self.internal_tests = self.extract_example(input)\n        system_prompt, user_prompt = self._process_inputs(input, spatial_info, temporal_info)\n        history_conversation = self.get_neighbor_history(input['task'])\n        if system_prompt == \"is_solved\":\n            self.update_conversation_history(user_prompt,history_conversation)\n            return user_prompt\n        message = history_conversation + [{'role':'system','content':system_prompt},{'role':'user','content':user_prompt}]\n        response = self.llm.gen(message)\n        self.update_conversation_history(response,history_conversation)\n        return response\n\n    async def _async_execute(self, input:Dict[str,str],  spatial_info:Dict[str,Any], temporal_info:Dict[str,Any],**kwargs):\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Use the processed input to get the result \"\"\"\n        \"\"\" The input type of this node is Dict \"\"\"\n        self.internal_tests = self.extract_example(input)\n        system_prompt, user_prompt = self._process_inputs(input, spatial_info, temporal_info)\n        history_conversation = self.get_neighbor_history(input['task'])\n        if system_prompt == \"is_solved\":\n            self.update_conversation_history(user_prompt,history_conversation)\n            return user_prompt\n        message = history_conversation + [{'role':'system','content':system_prompt},{'role':'user','content':user_prompt}]\n        response = await self.llm.agen(message)\n        self.update_conversation_history(response,history_conversation)\n        return response"}
{"type": "source_file", "path": "AgentPrune/agents/final_decision_ag.py", "content": "from typing import List,Any,Dict\n\nfrom AgentPrune.graph.autogen_node import NodeAutoGen\nfrom AgentPrune.agents.agent_registry import AgentRegistry\nfrom AgentPrune.llm.llm_registry import LLMRegistry\nfrom AgentPrune.prompt.prompt_set_registry import PromptSetRegistry\nfrom AgentPrune.tools.coding.python_executor import PyExecutor\n\n@AgentRegistry.register('FinalWriteCodeAG')\nclass FinalWriteCodeAG(NodeAutoGen):\n    def __init__(self, id: str | None =None,  domain: str = \"\", llm_name: str = \"\",):\n        super().__init__(id, \"FinalWriteCodeAG\" ,domain, llm_name)\n        self.llm = LLMRegistry.get(llm_name)\n        self.prompt_set = PromptSetRegistry.get(domain)\n\n    def extract_example(self, prompt: str) -> list:\n        prompt = prompt['task']\n        lines = (line.strip() for line in prompt.split('\\n') if line.strip())\n\n        results = []\n        lines_iter = iter(lines)\n        for line in lines_iter:\n            if line.startswith('>>>'):\n                function_call = line[4:]\n                expected_output = next(lines_iter, None)\n                if expected_output:\n                    results.append(f\"assert {function_call} == {expected_output}\")\n\n        return results\n    \n    def _process_inputs(self, input:Dict[str,str], spatial_info:Dict[str,Any], temporal_info:Dict[str,Any], **kwargs)->List[Any]:\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Process the raw_inputs(most of the time is a List[Dict]) \"\"\"\n        self.role = self.prompt_set.get_decision_role()\n        self.constraint = self.prompt_set.get_decision_constraint()          \n        system_prompt = f\"{self.role}.\\n {self.constraint}\"\n        user_prompt = f\"The task is: {input['task']}\\n\"\n        history_conversation = self.get_neighbor_history(input['task'])\n        for i, message in enumerate(history_conversation):\n            output = message['content']\n            if output.startswith(\"```python\") and output.endswith(\"```\"):\n                output = output.lstrip(\"```python\\n\").rstrip(\"\\n```\")\n                is_solved, feedback, state = PyExecutor().execute(output, self.internal_tests, timeout=10)\n                user_prompt += f\"The code written by the previous agent {i} is:\\n\\n{output}\\n\\n Whether it passes internal testing? {is_solved}.\\n\\nThe feedback is:\\n\\n {feedback}.\\n\\n\"\n        user_prompt += \"Please continue to complete this task based on the previous conversation.\"\n        return system_prompt, user_prompt\n                \n    def _execute(self, input:Dict[str,str],  spatial_info:Dict[str,Any], temporal_info:Dict[str,Any],**kwargs):\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Use the processed input to get the result \"\"\"\n        self.internal_tests = self.extract_example(input)\n        system_prompt, user_prompt = self._process_inputs(input, spatial_info, temporal_info)\n        history_conversation = self.get_neighbor_history(input['task'])\n        message = history_conversation + [{'role':'system','content':system_prompt},{'role':'user','content':user_prompt}]\n        response = self.llm.gen(message)\n        return response\n    \n    async def _async_execute(self, input:Dict[str,str],  spatial_info:Dict[str,Any], temporal_info:Dict[str,Any],**kwargs):\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Use the processed input to get the result \"\"\"\n        self.internal_tests = self.extract_example(input)\n        system_prompt, user_prompt = self._process_inputs(input, spatial_info, temporal_info)\n        history_conversation = self.get_neighbor_history(input['task'])\n        message = history_conversation + [{'role':'system','content':system_prompt},{'role':'user','content':user_prompt}]\n        response = await self.llm.agen(message)\n        return response\n\n\n@AgentRegistry.register('FinalReferAG')\nclass FinalReferAG(NodeAutoGen):\n    def __init__(self, id: str | None =None,  domain: str = \"\", llm_name: str = \"\",):\n        super().__init__(id, \"FinalRefer\" ,domain, llm_name)\n        self.llm = LLMRegistry.get(llm_name)\n        self.prompt_set = PromptSetRegistry.get(domain)\n\n    def _process_inputs(self, raw_inputs:Dict[str,str], spatial_info:Dict[str,Any], temporal_info:Dict[str,Any], **kwargs)->List[Any]:\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Process the raw_inputs(most of the time is a List[Dict]) \"\"\"\n        self.role = self.prompt_set.get_decision_role()\n        self.constraint = self.prompt_set.get_decision_constraint()          \n        system_prompt = f\"{self.role}.\\n {self.constraint}\"\n        decision_few_shot = self.prompt_set.get_decision_few_shot()\n        user_prompt = f\"{decision_few_shot} The task is:\\n\\n {raw_inputs['task']}.\"\n        return system_prompt, user_prompt\n                \n    def _execute(self, input:Dict[str,str],  spatial_info:Dict[str,Any], temporal_info:Dict[str,Any],**kwargs):\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Use the processed input to get the result \"\"\"\n  \n        system_prompt, user_prompt = self._process_inputs(input, spatial_info, temporal_info)\n        history_conversation = self.get_neighbor_history(input['task'])\n        message = history_conversation + [{'role':'system','content':system_prompt},{'role':'user','content':user_prompt}]\n        response = self.llm.gen(message)\n        return response\n    \n    async def _async_execute(self, input:Dict[str,str],  spatial_info:Dict[str,Any], temporal_info:Dict[str,Any],**kwargs):\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Use the processed input to get the result \"\"\"\n  \n        system_prompt, user_prompt = self._process_inputs(input, spatial_info, temporal_info)\n        history_conversation = self.get_neighbor_history(input['task'])\n        message = history_conversation + [{'role':'system','content':system_prompt},{'role':'user','content':user_prompt}]\n        response = await self.llm.agen(message)\n        return response\n"}
{"type": "source_file", "path": "AgentPrune/agents/math_solver_ag.py", "content": "from typing import List,Any,Dict\n\nfrom AgentPrune.graph.autogen_node import NodeAutoGen\nfrom AgentPrune.agents.agent_registry import AgentRegistry\nfrom AgentPrune.llm.llm_registry import LLMRegistry\nfrom AgentPrune.prompt.prompt_set_registry import PromptSetRegistry\nfrom AgentPrune.tools.coding.python_executor import execute_code_get_return\nfrom dataset.gsm8k_dataset import gsm_get_predict\n\n@AgentRegistry.register('MathSolverAG')\nclass MathSolverAG(NodeAutoGen):\n    def __init__(self, id: str | None =None, role:str = None ,domain: str = \"\", llm_name: str = \"\",):\n        super().__init__(id, \"MathSolverAG\" ,domain, llm_name)\n        self.llm = LLMRegistry.get(llm_name)\n        self.prompt_set = PromptSetRegistry.get(domain)\n        self.role = self.prompt_set.get_role() if role is None else role\n        self.constraint = self.prompt_set.get_constraint(self.role) \n        \n    def _process_inputs(self, raw_inputs:Dict[str,str], spatial_info:Dict[str,Dict], temporal_info:Dict[str,Dict], **kwargs)->List[Any]:\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Process the raw_inputs(most of the time is a List[Dict]) \"\"\"             \n        system_prompt = self.constraint\n        user_prompt = self.prompt_set.get_answer_prompt(question=raw_inputs[\"task\"], role=self.role)\n        if self.role == \"Math Solver\":\n            user_prompt += \"(Hint: The answer is near to\"\n            for message in self.conversation_history:\n                if message['role'] == 'assistant':\n                    user_prompt += \" \"+gsm_get_predict(message['content'])\n            user_prompt += \").\"\n        user_prompt += \"Please continue to complete the task based on the previous conversation.\"\n        return system_prompt, user_prompt\n    \n    def _execute(self, input:Dict[str,str],  spatial_info:Dict[str,Any], temporal_info:Dict[str,Any],**kwargs):\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Use the processed input to get the result \"\"\"\n        system_prompt, user_prompt = self._process_inputs(input, spatial_info, temporal_info)\n        history_conversation = self.get_neighbor_history(input['task'])\n        message = history_conversation + [{'role':'system','content':system_prompt},{'role':'user','content':user_prompt}]\n        response = self.llm.gen(message)\n        if self.role == \"Programming Expert\":\n            answer = execute_code_get_return(response.lstrip(\"```python\\n\").rstrip(\"\\n```\"))\n            response += f\"\\nthe answer is {answer}\" if answer is not None and \"Error occurred\" not in response else \"\"\n        self.update_conversation_history(response,history_conversation)\n        return response\n\n    async def _async_execute(self, input:Dict[str,str],  spatial_info:Dict[str,Any], temporal_info:Dict[str,Any],**kwargs):\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Use the processed input to get the result \"\"\"\n        \"\"\" The input type of this node is Dict \"\"\"\n        system_prompt, user_prompt = self._process_inputs(input, spatial_info, temporal_info)\n        history_conversation = self.get_neighbor_history(input['task'])\n        message = history_conversation + [{'role':'system','content':system_prompt},{'role':'user','content':user_prompt}]\n        response = await self.llm.agen(message)\n        if self.role == \"Programming Expert\":\n            answer = execute_code_get_return(response.lstrip(\"```python\\n\").rstrip(\"\\n```\"))\n            response += f\"\\nthe answer is {answer}\" if answer is not None and \"Error occurred\" not in response else \"\"\n        self.update_conversation_history(response,history_conversation)\n        return response"}
{"type": "source_file", "path": "AgentPrune/agents/analyze_agent.py", "content": "from typing import List,Any,Dict\nimport re\nimport asyncio\n\nfrom AgentPrune.graph.node import Node\nfrom AgentPrune.agents.agent_registry import AgentRegistry\nfrom AgentPrune.llm.llm_registry import LLMRegistry\nfrom AgentPrune.prompt.prompt_set_registry import PromptSetRegistry\nfrom AgentPrune.tools.search.wiki import search_wiki_main\n\ndef find_strings_between_pluses(text):\n    return re.findall(r'\\@(.*?)\\@', text)\n\n@AgentRegistry.register('AnalyzeAgent')\nclass AnalyzeAgent(Node):\n    def __init__(self, id: str | None =None, role:str = None,  domain: str = \"\", llm_name: str = \"\",):\n        super().__init__(id, \"AnalyzeAgent\" ,domain, llm_name)\n        self.llm = LLMRegistry.get(llm_name)\n        self.prompt_set = PromptSetRegistry.get(domain)\n        self.role = self.prompt_set.get_role() if role is None else role\n        self.constraint = self.prompt_set.get_analyze_constraint(self.role)\n        self.wiki_summary = \"\"\n        \n    async def _process_inputs(self, raw_inputs:Dict[str,str], spatial_info:Dict[str,Dict], temporal_info:Dict[str,Dict], **kwargs)->List[Any]:\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Process the raw_inputs(most of the time is a List[Dict]) \"\"\"              \n        system_prompt = f\"{self.constraint}\"\n        user_prompt = f\"The task is: {raw_inputs['task']}\\n\" if self.role != 'Fake' else self.prompt_set.get_adversarial_answer_prompt(raw_inputs['task'])\n        spatial_str = \"\"\n        temporal_str = \"\"\n        for id, info in spatial_info.items():\n            if self.role == 'Wiki Searcher' and info['role']=='Knowlegable Expert':\n                queries = find_strings_between_pluses(info['output'])\n                wiki = await search_wiki_main(queries)\n                if len(wiki):\n                    self.wiki_summary = \".\\n\".join(wiki)\n                    user_prompt += f\"The key entities of the problem are explained in Wikipedia as follows:{self.wiki_summary}\"\n            spatial_str += f\"Agent {id}, role is {info['role']}, output is:\\n\\n {info['output']}\\n\\n\"\n        for id, info in temporal_info.items():\n            temporal_str += f\"Agent {id}, role is {info['role']}, output is:\\n\\n {info['output']}\\n\\n\"\n            \n        user_prompt += f\"At the same time, the outputs of other agents are as follows:\\n\\n{spatial_str} \\n\\n\" if len(spatial_str) else \"\"\n        user_prompt += f\"In the last round of dialogue, the outputs of other agents were: \\n\\n{temporal_str}\" if len(temporal_str) else \"\"\n        return system_prompt, user_prompt\n                \n    def _execute(self, input:Dict[str,str],  spatial_info:Dict[str,Dict], temporal_info:Dict[str,Dict],**kwargs):\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Use the processed input to get the result \"\"\"\n  \n        system_prompt, user_prompt = self._process_inputs(input, spatial_info, temporal_info)\n        message = [{'role':'system','content':system_prompt},{'role':'user','content':user_prompt}]\n        response = self.llm.gen(message)\n        return response\n\n    async def _async_execute(self, input:Dict[str,str],  spatial_info:Dict[str,Dict], temporal_info:Dict[str,Dict],**kwargs):\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Use the processed input to get the result \"\"\"\n        system_prompt, user_prompt = await self._process_inputs(input, spatial_info, temporal_info)\n        message = [{'role':'system','content':system_prompt},{'role':'user','content':user_prompt}]\n        response = await self.llm.agen(message)\n        if self.wiki_summary != \"\":\n            response += f\"\\n\\n{self.wiki_summary}\"\n            self.wiki_summary = \"\"\n        return response"}
{"type": "source_file", "path": "AgentPrune/graph/__init__.py", "content": "from AgentPrune.graph.node import Node\nfrom AgentPrune.graph.graph import Graph\nfrom AgentPrune.graph.autogen_node import NodeAutoGen\nfrom AgentPrune.graph.autogen_graph import GraphAutoGen\n\n__all__ = [\"Node\",\n           \"Graph\",\n           \"NodeAutoGen\",\n           \"GraphAutoGen\"]"}
{"type": "source_file", "path": "AgentPrune/domain/__init__.py", "content": ""}
{"type": "source_file", "path": "AgentPrune/llm/visual_llm_registry.py", "content": "from typing import Optional\nfrom class_registry import ClassRegistry\n\nfrom AgentPrune.llm.visual_llm import VisualLLM\n\n\nclass VisualLLMRegistry:\n    registry = ClassRegistry()\n\n    @classmethod\n    def register(cls, *args, **kwargs):\n        return cls.registry.register(*args, **kwargs)\n    \n    @classmethod\n    def keys(cls):\n        return cls.registry.keys()\n\n    @classmethod\n    def get(cls, model_name: Optional[str] = None) -> VisualLLM:\n        if model_name is None:\n            model_name = \"gpt-4-vision-preview\"\n\n        if model_name == 'mock':\n            model = cls.registry.get(model_name)\n        else: # any version of GPT4VChat like \"gpt-4-vision-preview\"\n            model = cls.registry.get('GPT4VChat', model_name)\n\n        return model\n"}
{"type": "source_file", "path": "AgentPrune/llm/llm.py", "content": "from abc import ABC, abstractmethod\nfrom typing import List, Union, Optional\n\nfrom AgentPrune.llm.format import Message\n\n\nclass LLM(ABC):\n    DEFAULT_MAX_TOKENS = 1000\n    DEFAULT_TEMPERATURE = 0.2\n    DEFUALT_NUM_COMPLETIONS = 1\n\n    @abstractmethod\n    async def agen(\n        self,\n        messages: List[Message],\n        max_tokens: Optional[int] = None,\n        temperature: Optional[float] = None,\n        num_comps: Optional[int] = None,\n        ) -> Union[List[str], str]:\n\n        pass\n\n    @abstractmethod\n    def gen(\n        self,\n        messages: List[Message],\n        max_tokens: Optional[int] = None,\n        temperature: Optional[float] = None,\n        num_comps: Optional[int] = None,\n        ) -> Union[List[str], str]:\n\n        pass\n"}
{"type": "source_file", "path": "AgentPrune/llm/llm_registry.py", "content": "from typing import Optional\nfrom class_registry import ClassRegistry\n\nfrom AgentPrune.llm.llm import LLM\n\n\nclass LLMRegistry:\n    registry = ClassRegistry()\n\n    @classmethod\n    def register(cls, *args, **kwargs):\n        return cls.registry.register(*args, **kwargs)\n    \n    @classmethod\n    def keys(cls):\n        return cls.registry.keys()\n\n    @classmethod\n    def get(cls, model_name: Optional[str] = None) -> LLM:\n        if model_name is None or model_name==\"\":\n            model_name = \"gpt-4o\"\n\n        if model_name == 'mock':\n            model = cls.registry.get(model_name)\n        else: # any version of GPTChat like \"gpt-4o\"\n            model = cls.registry.get('GPTChat', model_name)\n\n        return model\n"}
{"type": "source_file", "path": "AgentPrune/prompt/__init__.py", "content": "from AgentPrune.prompt.prompt_set_registry import PromptSetRegistry\nfrom AgentPrune.prompt.mmlu_prompt_set import MMLUPromptSet\nfrom AgentPrune.prompt.humaneval_prompt_set import HumanEvalPromptSet\nfrom AgentPrune.prompt.gsm8k_prompt_set import GSM8KPromptSet\n\n__all__ = ['MMLUPromptSet',\n           'HumanEvalPromptSet',\n           'GSM8KPromptSet',\n           'PromptSetRegistry',]"}
{"type": "source_file", "path": "AgentPrune/prompt/humaneval_prompt_set.py", "content": "from typing import Dict, Any\nimport itertools\nfrom AgentPrune.prompt.prompt_set import PromptSet\nfrom AgentPrune.prompt.prompt_set_registry import PromptSetRegistry\nfrom AgentPrune.prompt.common import get_combine_materials\n\nroles = itertools.cycle(['Project Manager',\n                         'Algorithm Designer',\n                         'Programming Expert',\n                         'Test Analyst',\n                         'Bug Fixer',])\n\nROLE_DESCRIPTION = {\n    \"Project Manager\": \n        \"You are a project manager. \"\n        \"You will be given a function signature and its docstring by the user. \"\n        \"You are responsible for overseeing the overall structure of the code, ensuring that the code is structured to complete the task Implement code concisely and correctly without pursuing over-engineering.\"\n        \"You need to suggest optimal design patterns to ensure that the code follows best practices for maintainability and flexibility. \"\n        \"You can specify the overall design of the code, including the classes that need to be defined(maybe none) and the functions used (maybe only one function) .\"\n        \"I hope your reply will be more concise. Preferably within fifty words. Don’t list too many points.\",\n    \"Algorithm Designer\":\n        \"You are an algorithm designer. \"\n        \"You will be given a function signature and its docstring by the user. \"\n        \"You need to specify the specific design of the algorithm, including the classes that may be defined and the functions used. \"\n        \"You need to generate the detailed documentation, including explanations of the algorithm, usage instructions, and API references. \"\n        \"When the implementation logic is complex, you can give the pseudocode logic of the main algorithm.\"\n        \"I hope your reply will be more concise. Preferably within fifty words. Don’t list too many points.\",\n    \"Programming Expert\":\n        \"You are a programming expert. \"\n        \"You will be given a function signature and its docstring by the user. \"\n        \"You may be able to get the output results of other agents. They may have passed internal tests, but they may not be completely correct. \"\n        \"Write your full implementation (restate the function signature). \"\n        \"Use a Python code block to write your response. For example:\\n```python\\nprint('Hello world!')\\n```\"\n        \"Do not include anything other than Python code blocks in your response. \"\n        \"Do not change function names and input variable types in tasks.\",\n    \"Test Analyst\":\n        \"You are a test analyst. \"\n        \"You will be given a function signature and its docstring by the user. \"\n        \"You need to provide problems in the current code or solution based on the test data and possible test feedback in the question. \"\n        \"You need to provide additional special use cases, boundary conditions, etc. that should be paid attention to when writing code. \"\n        \"You can point out any potential errors in the code.\"\n        \"I hope your reply will be more concise. Preferably within fifty words. Don’t list too many points.\",\n    \"Bug Fixer\":\n        \"You are a bug fixer.\"\n        \"You will be given a function signature and its docstring by the user. \"\n        \"You need to provide modified and improved python code based on the current overall code design, algorithm framework, code implementation or test problems. \"\n        \"Write your full implementation (restate the function signature). \"\n        \"Use a Python code block to write your response. For example:\\n```python\\nprint('Hello world!')\\n```\"\n        \"Do not include anything other than Python code blocks in your response \"\n        \"Do not change function names and input variable types in tasks\",\n    \"Normal Programmer\":\n        \"You are a programmer. \"\n        \"You will be given a function signature and its docstring by the user. \"\n        \"You can refer to the agents' outputs. \"\n        \"Write your full implementation (restate the function signature). \"\n        \"Use a Python code block to write your response. For example:\\n```python\\nprint('Hello world!')\\n```\"\n        \"Do not include anything other than Python code blocks in your response. \",\n    \"Stupid Programmer\":\n        \"You are a stupid programmer. \"\n        \"You will be given a function signature and its docstring by the user. \"\n        \"Give a code implementation full of errors. \"\n        \"Do not use comments for all errors. \"\n        \"Use a Python code block to write your response. For example:\\n```python\\nprint('Hello world!')\\n```\"\n        \"Do not include anything other than Python code blocks in your response. \",\n}\n\n\n@PromptSetRegistry.register('humaneval')\nclass HumanEvalPromptSet(PromptSet):\n\n    @staticmethod\n    def get_role():\n        return next(roles)\n\n    @staticmethod\n    def get_constraint(role):\n        return ROLE_DESCRIPTION[role]\n    \n    @staticmethod\n    def get_format():\n        return \"natural language\"\n\n    @staticmethod\n    def get_answer_prompt(question):\n        # Format the question for the AI assistant to answer\n        return f\"{question}\"\n\n    @staticmethod\n    def get_react_prompt(question, solution, feedback):\n        return f\"\"\"Here is an unsuccessful attempt for solving the folloing question:\nQuestion:\n{question}\nAttempted Solution:\n{solution}\nFeedback:\\n{feedback}\nRewrite the code based on the feedback and the following question:\n{question}\"\"\"\n\n\n    @staticmethod\n    def get_query_prompt(question):\n        return (\n\"# Information Gathering for Question Resolution\\n\\n\"\n\"Evaluate if additional information is needed to answer the question. \"\n#\"If web search or file analysis is required, formulate specific queries to assist in finding the answer.\\n\\n\"\n\"If a web search or file analysis is necessary, outline specific clues or details to be searched for.\\n\\n\"\nf\"## ❓ Target Question:\\n{question}\\n\\n\"\n# \"## 🤔 Information Gathering:\\n\"\n# \"Identify if a web search or file reading is necessary and outline the approach.\"\n\"## 🔍 Clues for Investigation:\\n\"\n\"Identify critical clues and concepts within the question that are essential for finding the answer.\\n\"\n        )\n\n\n    @staticmethod\n    def get_file_analysis_prompt(query, file):\n        return (\n            # \"# File Analysis Required\\n\\n\"\n            # f\"## 🔍 Required Information to Extract:\\n---\\n{query}\\n---\\n\\n\"\n            # f\"## 📄 File Content for Analysis:\\n---\\n{file}\\n---\\n\\n\"\n            # \"## 🤔 Instructions:\\n\"\n            # \"Extract the specified information from the file. Example: 'Identify the main theme in the text.'\"\n\"# File Analysis Task\\n\\n\"\nf\"## 🔍 Information Extraction Objective:\\n---\\n{query}\\n---\\n\\n\"\nf\"## 📄 File Under Analysis:\\n---\\n{file}\\n---\\n\\n\"\n\"## 📝 Instructions:\\n\"\n\"1. Identify the key sections in the file relevant to the query.\\n\"\n\"2. Extract and summarize the necessary information from these sections.\\n\"\n\"3. Ensure the response is focused and directly addresses the query.\\n\"\n\"Example: 'Identify the main theme in the text.'\"\n        )\n\n\n    @staticmethod\n    def get_websearch_prompt(question, query):\n        return (\n            \"# Web Search Task\\n\\n\"\n            f\"## Original Question: \\n---\\n{question}\\n---\\n\\n\"\n            f\"## 🔍 Targeted Search Objective:\\n---\\n{query}\\n---\\n\\n\"\n            \"## 🌐 Simplified Search Instructions:\\n\"\n            \"Generate three specific search queries directly related to the original question. Each query should focus on key terms from the question. Format the output as a comma-separated list.\\n\"\n            \"For example, if the question is 'Who will be the next US president?', your queries could be: 'US presidential candidates, current US president, next US president'.\\n\"\n            \"Remember to format the queries as 'query1, query2, query3'.\"\n        )\n\n\n\n    @staticmethod\n    def get_adversarial_answer_prompt(question):\n        pass\n\n\n    @staticmethod\n    def get_distill_websearch_prompt(question, query, results):\n        return (\n            # \"# Summarization of Search Results\\n\\n\"\n            # \"## 🔍 Required Information for Summary:\\n---\\n{query}\\n---\\n\\n\"\n            # \"## 🌐 Search Results for Analysis:\\n---\\n{results}\\n---\\n\\n\"\n            # \"## ✏️ Instructions:\\n\"\n            # \"Summarize the key findings from the search results related to the query. \"\n            # \"Focus on relevant information. Example: 'Summary of key points...'\"\n\"# Summarization of Search Results\\n\\n\"\nf\"## Original question: \\n---\\n{question}\\n---\\n\\n\"\nf\"## 🔍 Required Information for Summary:\\n---\\n{query}\\n---\\n\\n\"\nf\"## 🌐 Analyzed Search Results:\\n---\\n{results}\\n---\\n\\n\"\n\"## 📝 Instructions for Summarization:\\n\"\n\"1. Review the provided search results and identify the most relevant information related to the question and query.\\n\"\n\"2. Extract and highlight the key findings, facts, or data points from these results.\\n\"\n\"3. Organize the summarized information in a coherent and logical manner.\\n\"\n\"4. Ensure the summary is concise and directly addresses the query, avoiding extraneous details.\\n\"  \n\"5. If the information from web search is useless, directly answer: \\\"No useful information from WebSearch\\\".\\n\"  \n        )\n\n\n    @staticmethod\n    def get_reflect_prompt(question, answer):\n        return (\n\"# Reflection on the Task\\n\\n\"\nf\"## 🤔 Reflection Question:\\n---\\n{question}\\n---\\n\\n\"\nf\"## 💡 Your Previous Answer:\\n---\\n{answer}\\n---\\n\\n\"\n\"## ✏️ Instructions:\\n\"\n\"Reflect on your answer process, considering the accuracy, method, and reasoning.\"\n        )\n\n\n    @staticmethod\n    def get_self_consistency(question: str, answers: list, constraint: str) -> str:\n        formatted_answers = \"\\n\".join([f\"Answer {index + 1}: {answer}\" for index, answer in enumerate(answers)])\n        return (\n            # \"# Self-Consistency Evaluation Task\\n\\n\"\n            # f\"## 🤔 Given Question:\\n---\\n{question}\\n---\\n\\n\"\n            # \"## 💡 Available Answers:\\n---\\n\"\n            # f\"{formatted_answers}\\n\"\n            # \"---\\n\\n\"\n            # \"## ✏️ Instructions:\\n\"\n            # \"Review the given answers and choose the most consistent one. \"\n            # \"If all answers differ, select the one you find most reliable. \"\n            # f\"Please keep following the constraints to answer the question: {constraint}.\"\n\"# Self-Consistency Evaluation Task\\n\\n\"\nf\"## 🤔 Question for Review:\\n---\\n{question}\\n---\\n\\n\"\nf\"## 💡 Reviewable Answers:\\n---\\n{formatted_answers}\\n---\\n\\n\"\n\"## 📋 Instructions for Selection:\\n\"\n\"1. Read each answer and assess how it addresses the question.\\n\"\n\"2. Compare the answers for their adherence to the given question's criteria and logical coherence.\\n\"\n\"3. Identify the answer that best aligns with the question's requirements and is the most logically consistent.\\n\"\n\"4. Ignore the candidate answers if they do not give a direct answer, for example, using 'unable to ...', 'as an AI ...'.\\n\"\n\"5. Copy the most suitable answer as it is, without modification, to maintain its original form.\\n\"\nf\"6. Adhere to the constraints: {constraint}.\\n\"\n\"Note: If no answer fully meets the criteria, choose and copy the one that is closest to the requirements.\"\n        )\n\n    @staticmethod\n    def get_select_best(question: str, answers: list, constraint: str) -> str:\n        formatted_answers = \"\\n\".join([f\"Answer {index + 1}: {answer}\" for index, answer in enumerate(answers)])\n        return (\n            # \"# Best Answer Evaluation Task\\n\\n\"\n            # f\"## 🤔 Given Question:\\n---\\n{question}\\n---\\n\\n\"\n            # \"## 💡 Available Answers:\\n---\\n\"\n            # f\"{formatted_answers}\\n\"\n            # \"---\\n\\n\"\n            # \"## ✏️ Instructions:\\n\"\n            # \"Review the given question and candidate answers and choose the most reasonable one. \"\n            # \"Please copy the original answer if you decide.\"\n            # f\"Please keep following the constraints to answer the question: {constraint}.\"\n\"# Best Answer Evaluation Task\\n\\n\"\nf\"## 🤔 Question:\\n---\\n{question}\\n---\\n\\n\"\nf\"## 💡 Candidate Answers for Evaluation:\\n---\\n{formatted_answers}\\n---\\n\\n\"\n\"## 📋 Evaluation Instructions:\\n\"\n\"1. Examine the question closely to understand its requirements.\\n\"\n\"2. Read each candidate answer thoroughly and assess its relevance and accuracy about the question.\\n\"\n\"3. Choose the answer that most accurately and completely addresses the question.\\n\"\n\"4. Ignore the candidate answers if they do not give a direct answer, for example, using 'unable to ...', 'as an AI ...'.\\n\"\n\"5. Copy the chosen answer exactly as it is presented, maintaining its original format.\\n\"\nf\"6. Adhere to the constraints: {constraint}.\\n\"\n\"Note: If none of the answers fully meet the question's criteria, select the one closest to fulfilling them.\"\n        )\n\n    @staticmethod\n    def get_combine_materials(materials: Dict[str, Any]) -> str:\n        return get_combine_materials(materials)\n\n    @staticmethod\n    def get_decision_constraint():\n        return (\n\"You will be given a function signature and its docstring by the user.\"\n\"You may be given the overall code design, algorithm framework, code implementation or test problems.\"\n\"Write your full implementation (restate the function signature). \"\n\"If the prompt given to you contains code that passed internal testing, you can choose the most reliable reply.\"\n\"If there is no code that has passed internal testing in the prompt, you can change it yourself according to the prompt.\"\n\"Use a Python code block to write your response. For example:\\n```python\\nprint('Hello world!')\\n```\"\n\"Do not include anything other than Python code blocks in your response\"\n)\n    \n    @staticmethod\n    def get_decision_role():\n        return \"You are the top decision-maker and are good at analyzing and summarizing other people's opinions, finding errors and giving final answers. And you are an AI that only responds with only python code.\"\n    \n    @staticmethod\n    def get_decision_few_shot():\n        return \"\""}
{"type": "source_file", "path": "AgentPrune/llm/visual_llm.py", "content": "from abc import ABC, abstractmethod\nfrom typing import Optional\n\n\nclass VisualLLM(ABC):\n    @abstractmethod\n    def gen(self,\n            task: Optional[str] = None,\n            img: Optional[str] = None) -> str:\n        pass\n\n    @abstractmethod\n    def gen_video(self,\n                  task: Optional[str] = None,\n                  video: Optional[str] = None) -> str:\n        pass\n"}
{"type": "source_file", "path": "AgentPrune/graph/graph.py", "content": "import shortuuid\nfrom typing import Any, List, Optional, Dict\nfrom abc import ABC\nimport numpy as np\nimport torch\nimport asyncio\n\nfrom AgentPrune.graph.node import Node\nfrom AgentPrune.agents.agent_registry import AgentRegistry\n\nclass Graph(ABC):\n    \"\"\"\n    A framework for managing and executing a network of nodes using a language model.\n\n    This class enables the creation of a graph structure for processing and analyzing data. Each node\n    in the graph can perform specific operations, allowing for complex data processing workflows.\n    The graph supports integration with language models, making it suitable for tasks that require\n    natural language processing capabilities.\n\n    The communication of the node depends on the node.spatial_predecessors and node.spatial_successors.\n    \n    Attributes:\n        domain (str): The domain for which this graph is used.\n        llm_name (str): The name of the llm that used for processing within the nodes.\n        nodes (dict): A collection of nodes, each identified by a unique UUID.\n\n    Methods:\n        build_graph(): Method to be implemented for constructing the graph structure.\n        add_node(node): Adds a new node to the graph with a unique identifier.\n        run(inputs, num_steps=10, single_agent=False): Executes the graph for a specified number of steps, processing provided inputs.\n    \"\"\"\n\n    def __init__(self, \n                domain: str,\n                llm_name: Optional[str],\n                agent_names: List[str],\n                decision_method: str,\n                optimized_spatial:bool = False,\n                initial_spatial_probability: float = 0.5,\n                fixed_spatial_masks:List[List[int]] = None,\n                optimized_temporal:bool = False,\n                initial_temporal_probability: float = 0.5,\n                fixed_temporal_masks:List[List[int]] = None,\n                node_kwargs:List[Dict] = None,\n                ):\n        \n        if fixed_spatial_masks is None:\n            fixed_spatial_masks = [[1 if i!=j else 0 for j in range(len(agent_names))] for i in range(len(agent_names))]\n        if fixed_temporal_masks is None:\n            fixed_temporal_masks = [[1 for j in range(len(agent_names))] for i in range(len(agent_names))]\n        fixed_spatial_masks = torch.tensor(fixed_spatial_masks).view(-1)\n        fixed_temporal_masks = torch.tensor(fixed_temporal_masks).view(-1)\n        assert len(fixed_spatial_masks)==len(agent_names)*len(agent_names),\"The fixed_spatial_masks doesn't match the number of agents\"\n        assert len(fixed_temporal_masks)==len(agent_names)*len(agent_names),\"The fixed_temporal_masks doesn't match the number of agents\"\n        \n        self.id:str = shortuuid.ShortUUID().random(length=4)\n        self.domain:str = domain\n        self.llm_name:str = llm_name\n        self.agent_names:List[str] = agent_names\n        self.optimized_spatial = optimized_spatial\n        self.optimized_temporal = optimized_temporal\n        self.decision_node:Node = AgentRegistry.get(decision_method, **{\"domain\":self.domain,\"llm_name\":self.llm_name})\n        self.nodes:Dict[str,Node] = {}\n        self.potential_spatial_edges:List[List[str, str]] = []\n        self.potential_temporal_edges:List[List[str,str]] = []\n        self.node_kwargs = node_kwargs if node_kwargs is not None else [{} for _ in agent_names]\n        \n        self.init_nodes() # add nodes to the self.nodes\n        self.init_potential_edges() # add potential edges to the self.potential_spatial/temporal_edges\n        \n        init_spatial_logit = torch.log(torch.tensor(initial_spatial_probability / (1 - initial_spatial_probability))) if optimized_spatial else 10.0\n        self.spatial_logits = torch.nn.Parameter(torch.ones(len(self.potential_spatial_edges), requires_grad=optimized_spatial) * init_spatial_logit,\n                                                 requires_grad=optimized_spatial) # trainable edge logits\n        self.spatial_masks = torch.nn.Parameter(fixed_spatial_masks,requires_grad=False)  # fixed edge masks\n\n        init_temporal_logit = torch.log(torch.tensor(initial_temporal_probability / (1 - initial_temporal_probability))) if optimized_temporal else 10.0\n        self.temporal_logits = torch.nn.Parameter(torch.ones(len(self.potential_temporal_edges), requires_grad=optimized_temporal) * init_temporal_logit,\n                                                 requires_grad=optimized_temporal) # trainable edge logits\n        self.temporal_masks = torch.nn.Parameter(fixed_temporal_masks,requires_grad=False)  # fixed edge masks\n        \n    @property\n    def spatial_adj_matrix(self):\n        matrix = np.zeros((len(self.nodes), len(self.nodes)))\n        for i, node1_id in enumerate(self.nodes):\n            for j, node2_id in enumerate(self.nodes):\n                if self.nodes[node2_id] in self.nodes[node1_id].spatial_successors: \n                    matrix[i, j] = 1\n        return matrix\n\n    @property\n    def temporal_adj_matrix(self):\n        matrix = np.zeros((len(self.nodes), len(self.nodes)))\n        for i, node1_id in enumerate(self.nodes):\n            for j, node2_id in enumerate(self.nodes):\n                if self.nodes[node2_id] in self.nodes[node1_id].temporal_successors: \n                    matrix[i, j] = 1\n        return matrix\n\n    @property\n    def num_edges(self):\n        num_edges = 0\n        for node in self.nodes.values():\n            num_edges += len(node.spatial_successors)\n        return num_edges\n    \n    @property\n    def num_nodes(self):\n        return len(self.nodes)\n\n    def find_node(self, id: str):\n        if id in self.nodes.keys():\n            return self.nodes[id]\n        raise Exception(f\"Node not found: {id} among \"\n                        f\"{[node.id for node in self.nodes.values()]}\")\n        \n    def add_node(self, node: Node):\n        node_id = node.id if node.id is not None else shortuuid.ShortUUID().random(length=4)\n        while node_id in self.nodes:\n            node_id = shortuuid.ShortUUID().random(length=4)\n        node.id = node_id\n        self.nodes[node_id] = node\n        return node\n    \n    def init_nodes(self):\n        \"\"\"\n        Creates and adds new nodes to the graph.\n        \"\"\"\n        for agent_name,kwargs in zip(self.agent_names,self.node_kwargs):\n            if agent_name in AgentRegistry.registry:\n                kwargs[\"domain\"] = self.domain\n                kwargs[\"llm_name\"] = self.llm_name\n                agent_instance = AgentRegistry.get(agent_name, **kwargs)\n                self.add_node(agent_instance)\n    \n    def init_potential_edges(self):\n        \"\"\"\n        Creates and potential edges to the graph.\n        \"\"\"\n        for node1_id in self.nodes.keys():\n            for node2_id in self.nodes.keys():\n                self.potential_spatial_edges.append([node1_id,node2_id])\n                self.potential_temporal_edges.append([node1_id,node2_id])\n\n    def clear_spatial_connection(self):\n        \"\"\"\n        Clear all the spatial connection of the nodes in the graph.\n        \"\"\"\n        for node_id in self.nodes.keys():\n            self.nodes[node_id].spatial_predecessors = []\n            self.nodes[node_id].spatial_successors = []\n        self.decision_node.spatial_predecessors = []\n        self.decision_node.spatial_successors = []\n    \n    def clear_temporal_connection(self):\n        \"\"\"\n        Clear all the temporal connection of the nodes in the graph.\n        \"\"\"\n        for node_id in self.nodes.keys():\n            self.nodes[node_id].temporal_predecessors = []\n            self.nodes[node_id].temporal_successors = []\n\n    def connect_decision_node(self, last_node_id: str = None):\n        for node_id in self.nodes.keys():\n            if last_node_id is None:\n                self.nodes[node_id].add_successor(self.decision_node)\n            elif last_node_id == node_id:\n                self.nodes[node_id].add_successor(self.decision_node)\n\n    def construct_spatial_connection(self, temperature: float = 1.0, threshold: float = None,): # temperature must >= 1.0\n        self.clear_spatial_connection()\n        log_probs = [torch.tensor(0.0, requires_grad=self.optimized_spatial)]\n        \n        for potential_connection, edge_logit, edge_mask in zip(self.potential_spatial_edges, self.spatial_logits, self.spatial_masks):\n            out_node:Node = self.find_node(potential_connection[0])\n            in_node:Node = self.find_node(potential_connection[1])\n            if edge_mask == 0.0:\n                continue\n            elif edge_mask == 1.0 and self.optimized_spatial==False:\n                if not self.check_cycle(in_node, {out_node}):\n                    out_node.add_successor(in_node,'spatial')\n                continue\n            if not self.check_cycle(in_node, {out_node}):\n                edge_prob = torch.sigmoid(edge_logit / temperature)\n                if threshold:\n                    edge_prob = torch.tensor(1 if edge_prob > threshold else 0)\n                if torch.rand(1) < edge_prob:\n                    out_node.add_successor(in_node,'spatial')\n                    log_probs.append(torch.log(edge_prob))\n                else:\n                    log_probs.append(torch.log(1 - edge_prob))\n                    \n        return torch.sum(torch.stack(log_probs))\n    \n    def construct_temporal_connection(self, round:int = 0, temperature: float = 1.0, threshold: float = None,):  # temperature must >= 1.0\n        self.clear_temporal_connection()\n        log_probs = [torch.tensor(0.0, requires_grad=self.optimized_temporal)]\n        if round == 0:\n            return torch.sum(torch.stack(log_probs))  \n        for potential_connection, edge_logit, edge_mask in zip(self.potential_temporal_edges, self.temporal_logits, self.temporal_masks):\n            out_node:Node = self.find_node(potential_connection[0])\n            in_node:Node = self.find_node(potential_connection[1])\n            if edge_mask == 0.0:\n                continue\n            elif edge_mask == 1.0 and self.optimized_temporal==False:\n                if not self.check_cycle(in_node, {out_node}):\n                    out_node.add_successor(in_node,'temporal')\n                continue\n            \n            edge_prob = torch.sigmoid(edge_logit / temperature)\n            if threshold:\n                edge_prob = torch.tensor(1 if edge_prob > threshold else 0)\n            if torch.rand(1) < edge_prob:\n                out_node.add_successor(in_node,'temporal')\n                log_probs.append(torch.log(edge_prob))\n            else:\n                log_probs.append(torch.log(1 - edge_prob))\n                    \n        return torch.sum(torch.stack(log_probs))\n\n\n    def run(self, inputs: Any, \n                  num_rounds:int = 3, \n                  max_tries: int = 3, \n                  max_time: int = 600,\n                  aggregate_mode: str = \"all connected\") -> List[Any]:\n        # inputs:{'task':\"xxx\"}\n        log_probs = 0\n        for round in range(num_rounds):\n            log_probs += self.construct_spatial_connection()\n            log_probs += self.construct_temporal_connection(round)\n            \n            in_degree = {node_id: len(node.spatial_predecessors) for node_id, node in self.nodes.items()}\n            zero_in_degree_queue = [node_id for node_id, deg in in_degree.items() if deg == 0]\n\n            while zero_in_degree_queue:\n                current_node_id = zero_in_degree_queue.pop(0)\n                tries = 0\n                while tries < max_tries:\n                    try:\n                        self.nodes[current_node_id].execute(inputs) # output is saved in the node.outputs\n                        break\n                    except Exception as e:\n                        print(f\"Error during execution of node {current_node_id}: {e}\")\n                    tries += 1\n                for successor in self.nodes[current_node_id].spatial_successors:\n                    if successor.id not in self.nodes.keys():\n                        continue\n                    in_degree[successor.id] -= 1\n                    if in_degree[successor.id] == 0:\n                        zero_in_degree_queue.append(successor.id)\n            \n            self.update_memory()\n        if aggregate_mode == \"all connected\":\n            self.connect_decision_node()\n        elif aggregate_mode == \"last connected\":\n            self.connect_decision_node(last_node_id=current_node_id)\n        self.decision_node.execute(inputs)\n        final_answers = self.decision_node.outputs\n        if len(final_answers) == 0:\n            final_answers.append(\"No answer of the decision node\")\n            \n        return final_answers, log_probs\n\n    async def arun(self, input: Dict[str,str], \n                  num_rounds:int = 3, \n                  max_tries: int = 3, \n                  max_time: int = 600,\n                  aggregate_mode: str = \"all connected\",) -> List[Any]:\n        log_probs = 0\n        for round in range(num_rounds):\n            log_probs += self.construct_spatial_connection()\n            log_probs += self.construct_temporal_connection(round)\n            in_degree = {node_id: len(node.spatial_predecessors) for node_id, node in self.nodes.items()}\n            zero_in_degree_queue = [node_id for node_id, deg in in_degree.items() if deg == 0]\n\n            while zero_in_degree_queue:\n                current_node_id = zero_in_degree_queue.pop(0)\n                tries = 0\n                while tries < max_tries:\n                    try:\n                        await asyncio.wait_for(self.nodes[current_node_id].async_execute(input),timeout=max_time) # output is saved in the node.outputs\n                        break\n                    except Exception as e:\n                        print(f\"Error during execution of node {current_node_id}: {e}\")\n                    tries += 1\n                for successor in self.nodes[current_node_id].spatial_successors:\n                    if successor.id not in self.nodes.keys():\n                        continue\n                    in_degree[successor.id] -= 1\n                    if in_degree[successor.id] == 0:\n                        zero_in_degree_queue.append(successor.id)\n            \n            self.update_memory()\n            \n        if aggregate_mode == \"all connected\":\n            self.connect_decision_node()\n        elif aggregate_mode == \"last connected\":\n            self.connect_decision_node(last_node_id=current_node_id)\n        await self.decision_node.async_execute(input)\n        final_answers = self.decision_node.outputs\n        if len(final_answers) == 0:\n            final_answers.append(\"No answer of the decision node\")\n        return final_answers, log_probs\n    \n    def update_memory(self):\n        for id,node in self.nodes.items():\n            node.update_memory()\n    \n    def check_cycle(self, new_node, target_nodes):\n        if new_node in target_nodes:\n            return True\n        for successor in new_node.spatial_successors:\n            if self.check_cycle(successor, target_nodes):\n                return True\n        return False\n\n    def update_masks(self, pruning_rate: float) -> torch.Tensor:\n        if self.optimized_spatial:\n            num_edges = (self.spatial_masks > 0).sum()\n            num_masks = (self.spatial_masks == 0).sum()\n            prune_num_edges = torch.round(num_edges*pruning_rate) if torch.round(num_edges*pruning_rate)>0 else 1\n            _edge_logits = self.spatial_logits.clone()\n            min_edge_logit = _edge_logits.min()\n            _edge_logits[self.spatial_masks == 0] = min_edge_logit - 1.0\n            sorted_edges_idx = torch.argsort(_edge_logits)\n            prune_idx = sorted_edges_idx[:int(prune_num_edges + num_masks)]\n            self.spatial_masks[prune_idx] = 0\n        \n        if self.optimized_temporal:\n            num_edges = (self.temporal_masks > 0).sum()\n            num_masks = (self.temporal_masks == 0).sum()\n            prune_num_edges = torch.round(num_edges*pruning_rate) if torch.round(num_edges*pruning_rate)>0 else 1\n            _edge_logits = self.temporal_logits.clone()\n            min_edge_logit = _edge_logits.min()\n            _edge_logits[self.temporal_masks == 0] = min_edge_logit - 1.0\n            sorted_edges_idx = torch.argsort(_edge_logits)\n            prune_idx = sorted_edges_idx[:int(prune_num_edges + num_masks)]\n            self.temporal_masks[prune_idx] = 0\n        return self.spatial_masks, self.temporal_masks"}
{"type": "source_file", "path": "AgentPrune/llm/__init__.py", "content": "from AgentPrune.llm.llm_registry import LLMRegistry\nfrom AgentPrune.llm.gpt_chat import GPTChat\n\n__all__ = [\"LLMRegistry\",\n           \"GPTChat\",]\n"}
{"type": "source_file", "path": "AgentPrune/prompt/gsm8k_prompt_set.py", "content": "from typing import Dict, Any\nimport itertools\nfrom AgentPrune.prompt.prompt_set import PromptSet\nfrom AgentPrune.prompt.prompt_set_registry import PromptSetRegistry\nfrom AgentPrune.prompt.common import get_combine_materials\n\nroles = itertools.cycle(['Math Solver',\n                         'Mathematical Analyst',\n                         'Programming Expert',\n                         'Inspector',])\n\nROLE_DESCRIPTION = {\n    \"Math Solver\": \n        \"You are a math expert. \"\n        \"You will be given a math problem and hints from other agents. \"\n        \"Give your own solving process step by step based on hints. \"\n        \"The last line of your output contains only the final result without any units, for example: The answer is 140\\n\"\n        \"You will be given some examples you may refer to.\",\n    \"Mathematical Analyst\":\n        \"You are a mathematical analyst. \"\n        \"You will be given a math problem, analysis and code from other agents. \"\n        \"You need to first analyze the problem-solving process step by step, where the variables are represented by letters. \"\n        \"Then you substitute the values into the analysis process to perform calculations and get the results.\"\n        \"The last line of your output contains only the final result without any units, for example: The answer is 140\\n\"\n        \"You will be given some examples you may refer to.\",\n    \"Programming Expert\":\n        \"You are a programming expert. \"\n        \"You will be given a math problem, analysis and code from other agents. \"\n        \"Integrate step-by-step reasoning and Python code to solve math problems. \"\n        \"Analyze the question and write functions to solve the problem. \"\n        \"The function should not take any arguments and use the final result as the return value. \"\n        \"The last line of code calls the function you wrote and assigns the return value to the \\(answer\\) variable. \"\n        \"Use a Python code block to write your response. For example:\\n```python\\ndef fun():\\n x = 10\\n y = 20\\n return x + y\\nanswer = fun()\\n```\\n\"\n        \"Do not include anything other than Python code blocks in your response.\"\n        \"You will be given some examples you may refer to.\",\n    \"Inspector\":\n        \"You are an Inspector. \"\n        \"You will be given a math problem, analysis and code from other agents. \"\n        \"Check whether the logic/calculation of the problem solving and analysis process is correct(if present). \"\n        \"Check whether the code corresponds to the solution analysis(if present). \"\n        \"Give your own solving process step by step based on hints. \"\n        \"The last line of your output contains only the final result without any units, for example: The answer is 140\\n\"\n        \"You will be given some examples you may refer to.\",\n}\n\n# This function is inspired by/derived from the implementation in the following GitHub repository:\n# Repository: https://github.com/chuanyang-Zheng/Progressive-Hint/blob/main/prompt/complex/complex_PHP_gsm8k.txt\n# Repository: https://github.com/microsoft/ToRA/blob/213c1c995038c73fab10343814df7a42f990f026/src/prompts/tora/gsm8k.md\n# Repository: https://github.com/microsoft/ToRA/blob/213c1c995038c73fab10343814df7a42f990f026/src/prompts/cot/gsm8k.md\nFEW_SHOT_DATA = {\n\"Math Solver\":\n\"\"\"\nQ: Angelo and Melanie want to plan how many hours over the next week they should study together for their test next week. \nThey have 2 chapters of their textbook to study and 4 worksheets to memorize. \nThey figure out that they should dedicate 3 hours to each chapter of their textbook and 1.5 hours for each worksheet. \nIf they plan to study no more than 4 hours each day, how many days should they plan to study total over the next week if they take a 10-minute break every hour, \ninclude 3 10-minute snack breaks each day, and 30 minutes for lunch each day? (Hint: The answer is near to 4).\n\nA: We know the Answer Hints: 4. With the Answer Hints: 4, we will answer the question. \nLet's think step by step. \nAngelo and Melanie think they should dedicate 3 hours to each of the 2 chapters, 3 hours x 2 chapters = 6 hours total.\nFor the worksheets they plan to dedicate 1.5 hours for each worksheet, 1.5 hours x 4 worksheets = 6 hours total.\nAngelo and Melanie need to start with planning 12 hours to study, at 4 hours a day, 12 / 4 = 3 days.\nHowever, they need to include time for breaks and lunch. Every hour they want to include a 10-minute break, \nso 12 total hours x 10 minutes = 120 extra minutes for breaks.\nThey also want to include 3 10-minute snack breaks, 3 x 10 minutes = 30 minutes.\nAnd they want to include 30 minutes for lunch each day, so 120 minutes for breaks + 30 minutes for snack breaks + 30 minutes for lunch = 180 minutes, or 180 / 60 minutes per hour = 3 extra hours.\nSo Angelo and Melanie want to plan 12 hours to study + 3 hours of breaks = 15 hours total.\nThey want to study no more than 4 hours each day, 15 hours / 4 hours each day = 3.75\nThey will need to plan to study 4 days to allow for all the time they need.\nThe answer is 4\n\nQ: Bella has two times as many marbles as frisbees. She also has 20 more frisbees than deck cards. If she buys 2/5 times more of each item, what would be the total number of the items she will have if she currently has 60 marbles? (Hint: The answer is near to 160,145).\nA: We know the Answer Hints: 160, 145. With the Answer Hints: 160, 145, we will answer the question.\nLet's think step by step\nWhen Bella buys 2/5 times more marbles, she'll have increased the number of marbles by 2/5*60 = 24\nThe total number of marbles she'll have is 60+24 = 84\nIf Bella currently has 60 marbles, and she has two times as many marbles as frisbees, she has 60/2 = 30 frisbees.\nIf Bella buys 2/5 times more frisbees, she'll have 2/5*30 = 12 more frisbees.\nThe total number of frisbees she'll have will increase to 30+12 = 42\nBella also has 20 more frisbees than deck cards, meaning she has 30-20 = 10 deck cards\nIf she buys 2/5 times more deck cards, she'll have 2/5*10 = 4 more deck cards.\nThe total number of deck cards she'll have is 10+4 = 14\nTogether, Bella will have a total of 14+42+84 = 140 items\nThe answer is 140\n\nQ: Susy goes to a large school with 800 students, while Sarah goes to a smaller school with only 300 students.  At the start of the school year, Susy had 100 social media followers.  She gained 40 new followers in the first week of the school year, half that in the second week, and half of that in the third week.  Sarah only had 50 social media followers at the start of the year, but she gained 90 new followers the first week, a third of that in the second week, and a third of that in the third week.  After three weeks, how many social media followers did the girl with the most total followers have? (Hint: The answer is near to 180, 160).\nA: We know the Answer Hints: 180, 160. With the Answer Hints: 180, 160, we will answer the question.\nLet's think step by step\nAfter one week, Susy has 100+40 = 140 followers.\nIn the second week, Susy gains 40/2 = 20 new followers.\nIn the third week, Susy gains 20/2 = 10 new followers.\nIn total, Susy finishes the three weeks with 140+20+10 = 170 total followers.\nAfter one week, Sarah has 50+90 = 140 followers.\nAfter the second week, Sarah gains 90/3 = 30 followers.\nAfter the third week, Sarah gains 30/3 = 10 followers.\nSo, Sarah finishes the three weeks with 140+30+10 = 180 total followers.\nThus, Sarah is the girl with the most total followers with a total of 180.\nThe answer is 180\n\"\"\",\n\n\"Mathematical Analyst\":\n\"\"\"\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today? \nA: ## Problem solving process analysis\n\nThere are {ori_tree_num} trees originally.\nThen there were {after_planted_tree_num} trees after some more were planted.\nSo the number of trees planted today {today_planted_num} is the number of trees after planting {after_planted_tree_num} minus the number of trees before planting {ori_tree_num}.\nThe answer is {today_planted_num} = {after_planted_tree_num} - {ori_tree_num}.\n\n## Actual analysis and solution process\n\nIn this question, {ori_tree_num} = 15 and {after_planted_tree_num} = 21.\nThere are 15 trees originally. \nThen there were 21 trees after some more were planted. \nSo the number of trees planted today must have been 21 - 15 = 6.\nThe answer is 6\n\nQ: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\nA:## Problem solving process analysis\n\nOriginally, Leah had {Leah_num} Leah_num chocolates.\nHer sister had {sister_num} chocolates.\nSo in total they had {all_num} = {Leah_num} + {sister_num} chocolates.\nAfter eating {eating_num} chocolates, the number of chocolates they have left {remain_num} is {all_num} minus {eating_num}. \nThe answer is {remain_num} = {all_num} - {eating_num}.\n\n## Actual analysis and solution process\n\nIn this question, {Leah_num} = 32, {sister_num} = 42 and {all_num} = 35.\nSo, in total they had 32 + 42 = 74 chocolates originally.\nAfter eating 35 chocolates, they had 74 - 35 = 39 chocolates.\nThe answer is 39\n\"\"\",\n\n\"Programming Expert\":\n\"\"\"\nQ: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\nA:\n```python\\n\ndef money_left():\n    money_initial = 23\n    bagels = 5\n    bagel_cost = 3\n    money_spent = bagels * bagel_cost\n    remaining_money = money_initial - money_spent\n    return remaining_money\n \nanswer = money_left()\n\\n```\n\nQ: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\nA:\n```python\\n\ndef remaining_golf_balls():\n    golf_balls_initial = 58\n    golf_balls_lost_tuesday = 23\n    golf_balls_lost_wednesday = 2\n    golf_balls_left = golf_balls_initial - golf_balls_lost_tuesday - golf_balls_lost_wednesday\n    remaining_golf_balls = golf_balls_left\n    return remaining_golf_balls\n\nanswer = remaining_golf_balls() \n\\n```\n\"\"\",\n\"Inspector\":\"\"\"\"\"\",\n}\n\n@PromptSetRegistry.register('gsm8k')\nclass GSM8KPromptSet(PromptSet):\n\n    @staticmethod\n    def get_role():\n        return next(roles)\n\n    @staticmethod\n    def get_constraint(role):\n        return ROLE_DESCRIPTION[role]\n    \n    @staticmethod\n    def get_format():\n        return \"natural language\"\n\n    @staticmethod\n    def get_answer_prompt(question,role=\"Mathematical Analyst\"):\n        # Format the question for the AI assistant to answer\n        return f\"{FEW_SHOT_DATA[role]}\\n\\nQ:{question}\"\n\n    @staticmethod\n    def get_decision_constraint():\n        return (\n        \"You will be given a math problem, analysis and code from other agents. \"\n        \"Please find the most reliable answer based on the analysis and results of other agents. \"\n        \"Give reasons for making decisions. \"\n        \"The last line of your output contains only the final result without any units, for example: The answer is 140\")\n    \n    @staticmethod\n    def get_decision_role():\n        return \"You are the top decision-maker.\"\n    \"Good at analyzing and summarizing mathematical problems, judging and summarizing other people's solutions, and giving final answers to math problems.\"\n    \n    @staticmethod\n    def get_decision_few_shot():\n        return \"\"\"\nQ:Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repairs.  This increased the value of the house by 150%.  How much profit did he make?\n\nA:After reviewing the analysis and results provided by the other agents, it appears that there is a discrepancy in the interpretation of the problem. The key point of contention is the base value on which the 150% increase is calculated.\n\nAgents 623T, 8GTW, and 7R9w have calculated the 150% increase based on the total investment (purchase price + repair costs), which is $130,000. They have then added this increase to the total investment to find the new value of the house, and subsequently calculated the profit by subtracting the total investment from the new value of the house. This approach leads to a profit calculation of $195,000.\n\nAgent 3qfQ, however, has interpreted the problem differently. This agent suggests that the 150% increase should be calculated based on the initial purchase price only, not the total investment. Following this method, the increased value is calculated as $80,000 (initial purchase price) + ($80,000 * 1.5), which equals $200,000. The profit is then calculated by subtracting the total investment ($130,000) from this increased value, resulting in a profit of $70,000.\n\nThe problem statement is ambiguous because it does not explicitly state whether the 150% increase is based on the initial purchase price alone or the total investment. However, the most common interpretation in real estate when referring to an increase in value due to repairs would be based on the initial purchase price, as the value increase is typically a reflection of the property's market value appreciation, not the sum of costs incurred.\n\nTherefore, based on the typical real estate valuation practice and the more common interpretation of such scenarios, Agent 3qfQ's approach seems to be the most reliable. The profit should be calculated based on the increased value from the initial purchase price, not the total investment.\n\nThe final result, based on the most reliable interpretation, is a profit of $70,000.\n\nThe answer is 70000\n\"\"\"\n    \n    @staticmethod\n    def get_react_prompt(question, solution, feedback):\n        return f\"\"\"Here is an unsuccessful attempt for solving the folloing question:\nQuestion:\n{question}\nAttempted Solution:\n{solution}\nFeedback:\\n{feedback}\nRewrite the code based on the feedback and the following question:\n{question}\"\"\"\n\n\n    @staticmethod\n    def get_query_prompt(question):\n        return (\n\"# Information Gathering for Question Resolution\\n\\n\"\n\"Evaluate if additional information is needed to answer the question. \"\n#\"If web search or file analysis is required, formulate specific queries to assist in finding the answer.\\n\\n\"\n\"If a web search or file analysis is necessary, outline specific clues or details to be searched for.\\n\\n\"\nf\"## ❓ Target Question:\\n{question}\\n\\n\"\n# \"## 🤔 Information Gathering:\\n\"\n# \"Identify if a web search or file reading is necessary and outline the approach.\"\n\"## 🔍 Clues for Investigation:\\n\"\n\"Identify critical clues and concepts within the question that are essential for finding the answer.\\n\"\n        )\n\n\n    @staticmethod\n    def get_file_analysis_prompt(query, file):\n        return (\n            # \"# File Analysis Required\\n\\n\"\n            # f\"## 🔍 Required Information to Extract:\\n---\\n{query}\\n---\\n\\n\"\n            # f\"## 📄 File Content for Analysis:\\n---\\n{file}\\n---\\n\\n\"\n            # \"## 🤔 Instructions:\\n\"\n            # \"Extract the specified information from the file. Example: 'Identify the main theme in the text.'\"\n\"# File Analysis Task\\n\\n\"\nf\"## 🔍 Information Extraction Objective:\\n---\\n{query}\\n---\\n\\n\"\nf\"## 📄 File Under Analysis:\\n---\\n{file}\\n---\\n\\n\"\n\"## 📝 Instructions:\\n\"\n\"1. Identify the key sections in the file relevant to the query.\\n\"\n\"2. Extract and summarize the necessary information from these sections.\\n\"\n\"3. Ensure the response is focused and directly addresses the query.\\n\"\n\"Example: 'Identify the main theme in the text.'\"\n        )\n\n\n    @staticmethod\n    def get_websearch_prompt(question, query):\n        return (\n            \"# Web Search Task\\n\\n\"\n            f\"## Original Question: \\n---\\n{question}\\n---\\n\\n\"\n            f\"## 🔍 Targeted Search Objective:\\n---\\n{query}\\n---\\n\\n\"\n            \"## 🌐 Simplified Search Instructions:\\n\"\n            \"Generate three specific search queries directly related to the original question. Each query should focus on key terms from the question. Format the output as a comma-separated list.\\n\"\n            \"For example, if the question is 'Who will be the next US president?', your queries could be: 'US presidential candidates, current US president, next US president'.\\n\"\n            \"Remember to format the queries as 'query1, query2, query3'.\"\n        )\n\n\n\n    @staticmethod\n    def get_adversarial_answer_prompt(question):\n        pass\n\n\n    @staticmethod\n    def get_distill_websearch_prompt(question, query, results):\n        return (\n            # \"# Summarization of Search Results\\n\\n\"\n            # \"## 🔍 Required Information for Summary:\\n---\\n{query}\\n---\\n\\n\"\n            # \"## 🌐 Search Results for Analysis:\\n---\\n{results}\\n---\\n\\n\"\n            # \"## ✏️ Instructions:\\n\"\n            # \"Summarize the key findings from the search results related to the query. \"\n            # \"Focus on relevant information. Example: 'Summary of key points...'\"\n\"# Summarization of Search Results\\n\\n\"\nf\"## Original question: \\n---\\n{question}\\n---\\n\\n\"\nf\"## 🔍 Required Information for Summary:\\n---\\n{query}\\n---\\n\\n\"\nf\"## 🌐 Analyzed Search Results:\\n---\\n{results}\\n---\\n\\n\"\n\"## 📝 Instructions for Summarization:\\n\"\n\"1. Review the provided search results and identify the most relevant information related to the question and query.\\n\"\n\"2. Extract and highlight the key findings, facts, or data points from these results.\\n\"\n\"3. Organize the summarized information in a coherent and logical manner.\\n\"\n\"4. Ensure the summary is concise and directly addresses the query, avoiding extraneous details.\\n\"  \n\"5. If the information from web search is useless, directly answer: \\\"No useful information from WebSearch\\\".\\n\"  \n        )\n\n\n    @staticmethod\n    def get_reflect_prompt(question, answer):\n        return (\n\"# Reflection on the Task\\n\\n\"\nf\"## 🤔 Reflection Question:\\n---\\n{question}\\n---\\n\\n\"\nf\"## 💡 Your Previous Answer:\\n---\\n{answer}\\n---\\n\\n\"\n\"## ✏️ Instructions:\\n\"\n\"Reflect on your answer process, considering the accuracy, method, and reasoning.\"\n        )\n\n\n    @staticmethod\n    def get_self_consistency(question: str, answers: list, constraint: str) -> str:\n        formatted_answers = \"\\n\".join([f\"Answer {index + 1}: {answer}\" for index, answer in enumerate(answers)])\n        return (\n            # \"# Self-Consistency Evaluation Task\\n\\n\"\n            # f\"## 🤔 Given Question:\\n---\\n{question}\\n---\\n\\n\"\n            # \"## 💡 Available Answers:\\n---\\n\"\n            # f\"{formatted_answers}\\n\"\n            # \"---\\n\\n\"\n            # \"## ✏️ Instructions:\\n\"\n            # \"Review the given answers and choose the most consistent one. \"\n            # \"If all answers differ, select the one you find most reliable. \"\n            # f\"Please keep following the constraints to answer the question: {constraint}.\"\n\"# Self-Consistency Evaluation Task\\n\\n\"\nf\"## 🤔 Question for Review:\\n---\\n{question}\\n---\\n\\n\"\nf\"## 💡 Reviewable Answers:\\n---\\n{formatted_answers}\\n---\\n\\n\"\n\"## 📋 Instructions for Selection:\\n\"\n\"1. Read each answer and assess how it addresses the question.\\n\"\n\"2. Compare the answers for their adherence to the given question's criteria and logical coherence.\\n\"\n\"3. Identify the answer that best aligns with the question's requirements and is the most logically consistent.\\n\"\n\"4. Ignore the candidate answers if they do not give a direct answer, for example, using 'unable to ...', 'as an AI ...'.\\n\"\n\"5. Copy the most suitable answer as it is, without modification, to maintain its original form.\\n\"\nf\"6. Adhere to the constraints: {constraint}.\\n\"\n\"Note: If no answer fully meets the criteria, choose and copy the one that is closest to the requirements.\"\n        )\n\n    @staticmethod\n    def get_select_best(question: str, answers: list, constraint: str) -> str:\n        formatted_answers = \"\\n\".join([f\"Answer {index + 1}: {answer}\" for index, answer in enumerate(answers)])\n        return (\n            # \"# Best Answer Evaluation Task\\n\\n\"\n            # f\"## 🤔 Given Question:\\n---\\n{question}\\n---\\n\\n\"\n            # \"## 💡 Available Answers:\\n---\\n\"\n            # f\"{formatted_answers}\\n\"\n            # \"---\\n\\n\"\n            # \"## ✏️ Instructions:\\n\"\n            # \"Review the given question and candidate answers and choose the most reasonable one. \"\n            # \"Please copy the original answer if you decide.\"\n            # f\"Please keep following the constraints to answer the question: {constraint}.\"\n\"# Best Answer Evaluation Task\\n\\n\"\nf\"## 🤔 Question:\\n---\\n{question}\\n---\\n\\n\"\nf\"## 💡 Candidate Answers for Evaluation:\\n---\\n{formatted_answers}\\n---\\n\\n\"\n\"## 📋 Evaluation Instructions:\\n\"\n\"1. Examine the question closely to understand its requirements.\\n\"\n\"2. Read each candidate answer thoroughly and assess its relevance and accuracy about the question.\\n\"\n\"3. Choose the answer that most accurately and completely addresses the question.\\n\"\n\"4. Ignore the candidate answers if they do not give a direct answer, for example, using 'unable to ...', 'as an AI ...'.\\n\"\n\"5. Copy the chosen answer exactly as it is presented, maintaining its original format.\\n\"\nf\"6. Adhere to the constraints: {constraint}.\\n\"\n\"Note: If none of the answers fully meet the question's criteria, select the one closest to fulfilling them.\"\n        )\n\n    @staticmethod\n    def get_combine_materials(materials: Dict[str, Any]) -> str:\n        return get_combine_materials(materials)\n\n"}
{"type": "source_file", "path": "AgentPrune/tools/reader/readers.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom openai import OpenAI\nimport pdb\n\n\"\"\"INSTALL\npip install openai --upgrade\npip install python-docx\npip install markdown\npip install PyPDF2\npip install openpyxl\npip install beautifulsoup4\npip install pylatexenc\npip install python-pptx\npip install xlrd\n\"\"\"\n\nimport json\nimport os\nimport pandas as pd\nimport charset_normalizer\nimport docx\nimport markdown\nimport PyPDF2\nimport openpyxl\nimport yaml\nimport zipfile\nimport subprocess\nfrom pathlib import Path\nfrom abc import ABC, abstractmethod\nfrom typing import Union, Any, Optional\nfrom bs4 import BeautifulSoup\nfrom pylatexenc.latex2text import LatexNodes2Text\nfrom pptx import Presentation\n\nfrom AgentPrune.utils.globals import Cost\n\nfrom dotenv import load_dotenv\nload_dotenv()\nimport aiohttp\nimport requests\nfrom openai import OpenAI, AsyncOpenAI\n\nOPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")\n\n\n# Refs: https://platform.openai.com/docs/api-reference\n# Refs: https://github.com/Significant-Gravitas/AutoGPT/blob/0e332c0c1221857f3ce96490f073c1c88bcbd367/autogpts/autogpt/autogpt/commands/file_operations_utils.py\n\nclass Reader(ABC):\n    @abstractmethod\n    def parse(self, file_path: Path) -> str:\n        \"\"\" To be overriden by the descendant class \"\"\"\n\n\nclass TXTReader(Reader):\n    def parse(self, file_path: Path) -> str:\n        content = charset_normalizer.from_path(file_path).best()\n        return str(content)\n    \nclass PDFReader(Reader):\n    def parse(self, file_path: Path) -> str:\n        content = PyPDF2.PdfReader(file_path)\n        text = \"\"\n        for page_idx in range(len(content.pages)):\n            text += f'Page {page_idx + 1}\\n' + content.pages[page_idx].extract_text()\n        return text\n    \nclass DOCXReader(Reader):\n    def parse(self, file_path: Path) -> str:\n        content = docx.Document(str(file_path))\n        text = \"\"\n        for i, para in enumerate(content.paragraphs):\n            text += f'Page {i + 1}:\\n' +  para.text\n        return text\n\nclass JSONReader(Reader):\n    def parse_file(file_path: Path) -> list:\n        try:\n            with open(file_path, \"r\") as f:\n                data = json.load(f)\n                #text = str(data)\n            return data#text\n        except:\n            return []\n    \n    def parse(self, file_path: Path) -> str:\n        try:\n            with open(file_path, \"r\") as f:\n                data = json.load(f)\n                text = str(data)\n            return text\n        except:\n            return ''\n        \nclass JSONLReader(Reader):\n    def parse_file(file_path: Path) -> list:\n        with open(file_path, \"r\",encoding='utf-8') as f:\n            lines = [json.loads(line) for line in f]\n            #text = '\\n'.join([str(line) for line in lines])\n        return lines #text\n    \n    def parse(file_path: Path) -> str:\n        with open(file_path, \"r\",encoding='utf-8') as f:\n            lines = [json.loads(line) for line in f]\n            text = '\\n'.join([str(line) for line in lines])\n        return text\n\nclass XMLReader(Reader):\n    def parse(self, file_path: Path) -> str:\n        with open(file_path, \"r\") as f:\n            data = BeautifulSoup(f, \"xml\")\n            text = data.get_text()\n        return text\n\nclass YAMLReader(Reader):\n    def parse(self, file_path: Path, return_str=True) -> Union[str, Any]:\n        with open(file_path, \"r\") as f:\n            data = yaml.load(f, Loader=yaml.FullLoader)\n            text = str(data)\n        if return_str:\n            return text\n        else:\n            return data\n    \nclass HTMLReader(Reader):\n    def parse(self, file_path: Path) -> str:\n        with open(file_path, \"r\") as f:\n            data = BeautifulSoup(f, \"html.parser\")\n            text = data.get_text()\n        return text\n    \nclass MarkdownReader(Reader):\n    def parse(self, file_path: Path) -> str:\n        with open(file_path, \"r\") as f:\n            data = markdown.markdown(f.read())\n            text = \"\".join(BeautifulSoup(data, \"html.parser\").findAll(string=True))\n        return text\n\nclass LaTexReader(Reader):\n    def parse(self, file_path: Path) -> str:\n        with open(file_path, \"r\") as f:\n            data = f.read()\n        text = LatexNodes2Text().latex_to_text(data)\n        return text\n\n\n\nclass AudioReader(Reader):\n    @staticmethod\n    def parse(file_path: Path) -> str:\n        client = OpenAI(api_key=OPENAI_API_KEY)\n        try:\n            client = OpenAI()\n            with open(file_path, \"rb\") as audio_file:\n                transcript = client.audio.translations.create(\n                    model=\"whisper-1\",\n                    file=audio_file\n                )\n            return transcript.text\n        except Exception as e:\n            return \"Error transcribing audio file.\"\n\nclass PPTXReader(Reader): \n    def parse(self, file_path: Path) -> str:\n        try:\n            pres = Presentation(str(file_path))\n            text = []\n            for slide_idx, slide in enumerate(pres.slides):\n                text.append(f\"Slide {slide_idx + 1}:\\n\")\n                for shape in slide.shapes:\n                    if hasattr(shape, \"text\"):\n                        text.append(shape.text)\n            return \"\\n\".join(text)\n        except Exception as e:\n            return \"Error reading PowerPoint file.\"\n\nclass ExcelReader(Reader):\n    def parse(self, file_path: Path) -> str:\n        try:\n            excel_data = pd.read_excel(file_path, sheet_name=None)\n\n            all_sheets_text = []\n            for sheet_name, data in excel_data.items():\n                all_sheets_text.append(f\"Sheet Name: {sheet_name}\\n{data.to_string()}\\n\")\n\n            return \"\\n\".join(all_sheets_text)\n        except Exception as e:\n            return \"Error reading Excel file.\"\n\nclass XLSXReader(Reader):\n    def parse(self, file_path: Path) -> str:\n        workbook = openpyxl.load_workbook(file_path, data_only=True)\n        text = \"\"\n\n        for sheet in workbook:\n            text += f\"\\nSheet: {sheet.title}\\n\"\n            for row in sheet.iter_rows(values_only=True):\n                row_data = [str(cell) if cell is not None else \"\" for cell in row]\n                text += \"\\t\".join(row_data) + \"\\n\"\n        \n        return text\n\nclass ZipReader(Reader):\n    def parse(self, file_path: Path) -> str:\n        #only support files that can be represented as text\n        try:\n            file_content = \"\"\n            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n                extract_dir = file_path[:-4] + '/'\n                zip_ref.extractall(extract_dir)\n                reader = FileReader()\n                for file_name in zip_ref.namelist():\n                    file_content += f'File {file_name}:\\n\"{reader.read_file(extract_dir + file_name)}\"\\n'\n            return file_content\n        \n        except zipfile.BadZipFile:\n            print(\"Invalid ZIP file.\")\n\n        except Exception as e:\n            print(f\"Error reading ZIP file: {e}\")\n\n\nclass PythonReader(Reader):\n    def parse(self, file_path: Path) -> str:\n        execution_result = \"\"\n        error = \"\"\n        file_content = \"\"\n        try:\n            completed_process = subprocess.run([\"python\", file_path], capture_output=True, text=True, check=True)\n            execution_result = \"Output:\\n\" + completed_process.stdout\n        except subprocess.CalledProcessError as e:\n            error = \"Error:\\n\" + e.stderr\n        except Exception as e:\n            print(f\"Error executing Python file: {e}\")\n\n        try:\n            with open(file_path, \"r\") as file:\n                file_content = \"\\nFile Content:\\n\" + file.read()\n        except Exception as e:\n            print(f\"Error reading Python file: {e}\")\n        return file_content, execution_result, error\n\n\n# Support 41 kinds of files.\nREADER_MAP = { \n    \".mp3\": AudioReader(),\n    \".m4a\": AudioReader(),\n    \".wav\": AudioReader(),\n    \".zip\": ZipReader(),\n    \".pptx\": PPTXReader(),\n    \".xlsx\": ExcelReader(),\n    \".xls\": ExcelReader(),\n    \".txt\": TXTReader(),\n    \".csv\": TXTReader(),\n    \".pdf\": PDFReader(),\n    \".docx\": DOCXReader(),\n    \".json\": JSONReader(),\n    \".jsonld\": JSONReader(),\n    \".jsonl\": JSONLReader(),\n    \".xml\": XMLReader(),\n    \".yaml\": YAMLReader(),\n    \".yml\": YAMLReader(),\n    \".html\": HTMLReader(),\n    \".htm\": HTMLReader(),\n    \".xhtml\": HTMLReader(),\n    \".md\": MarkdownReader(),\n    \".markdown\": MarkdownReader(),\n    \".tex\": LaTexReader(),\n    \".py\": PythonReader(),\n    \".pdb\": TXTReader(),\n}\n    \nclass FileReader:\n    def set_reader(self, suffix) -> None:\n        self.reader = READER_MAP[suffix]\n        print(f\"Setting Reader to {type(self.reader).__name__}\")\n\n    def read_file(self, file_path: Path, task=\"describe the file\") -> str:\n        suffix = '.' + file_path.split(\".\")[-1]\n        self.set_reader(suffix)\n        file_content = self.reader.parse(file_path)\n        print(f\"Reading file {file_path} using {type(self.reader).__name__}\")\n        return file_content\n    \n\nclass GeneralReader:\n    def __init__(self):\n        self.file_reader = FileReader()\n        self.name = \"General File Reader\"\n        self.description = \"\"\"A general file reader support to formats: 'py', 'java', 'cpp', 'c', 'js', \n                              'css', 'html', 'htm', 'xml', 'txt', 'jsonl', 'csv', 'json', \n                              'jsonld', 'jsonl', 'yaml', 'yml', 'xlsx', 'xls', 'jpg', 'png', \n                              'jpeg', 'gif', 'bmp', 'mp3', 'wav', 'ogg', 'mp4', 'avi', 'mkv', \n                              'mov', 'pdf', 'doc', 'docx', 'ppt', 'pptx', 'md', 'markdown', \n                              'tex', 'zip', 'tar', 'gz', '7z', 'rar'.\n                            \"\"\"\n\n    def read(self, task, file):\n\n        files_content = \"\"\n        file_content = self.file_reader.read_file(file, task)\n        suffix = file.split(\".\")[-1]\n\n        if suffix in ['py', 'java', 'cpp', 'c', 'js', 'css', 'html', 'htm', 'xml']:\n            files_content += f'\\nThe {suffix} file contains:\\n---\\n{file_content[0]}'\n            if file_content[1] != '':\n                files_content += f'\\nExecution result:\\n{file_content[1]}'\n            if file_content[2] != '':\n                files_content += f'\\nExecution error message:\\n{file_content[2]}'\n            files_content += '\\n---'\n\n        elif suffix in ['txt', 'jsonl', 'csv', 'json', 'jsonld', 'jsonl', 'yaml', 'yml', \n                        'xlsx', 'xls', 'jpg', 'png', 'jpeg', 'gif', 'bmp', 'mp3', 'wav', \n                        'ogg', 'mp4', 'avi', 'mkv', 'mov', 'pdf', 'doc', 'docx', 'ppt', \n                        'pptx', 'md', 'markdown', 'tex', 'zip', 'tar', 'gz', '7z', 'rar']:\n            files_content += f'\\nThe {suffix} file contains:\\n---\\n{file_content}\\n---'\n\n        return files_content\n    "}
{"type": "source_file", "path": "AgentPrune/llm/price.py", "content": "from AgentPrune.utils.globals import Cost, PromptTokens, CompletionTokens\nimport tiktoken\n# GPT-4:  https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo\n# GPT3.5: https://platform.openai.com/docs/models/gpt-3-5\n# DALL-E: https://openai.com/pricing\n\ndef cal_token(model:str, text:str):\n    encoder = tiktoken.encoding_for_model(model)\n    num_tokens = len(encoder.encode(text))\n    return num_tokens\n\ndef cost_count(prompt, response, model_name):\n    branch: str\n    prompt_len: int\n    completion_len: int\n    price: float\n\n    prompt_len = cal_token(model_name, prompt)\n    completion_len = cal_token(model_name, response)\n    if \"gpt-4\" in model_name:\n        branch = \"gpt-4\"\n        price = prompt_len * OPENAI_MODEL_INFO[branch][model_name][\"input\"] /1000 + \\\n                completion_len * OPENAI_MODEL_INFO[branch][model_name][\"output\"] /1000\n    elif \"gpt-3.5\" in model_name:\n        branch = \"gpt-3.5\"\n        price = prompt_len * OPENAI_MODEL_INFO[branch][model_name][\"input\"] /1000 + \\\n            completion_len * OPENAI_MODEL_INFO[branch][model_name][\"output\"] /1000\n    elif \"dall-e\" in model_name:\n        branch = \"dall-e\"\n        price = 0.0\n        prompt_len = 0\n        completion_len = 0\n    else:\n        branch = \"other\"\n        price = 0.0\n        prompt_len = 0\n        completion_len = 0\n\n    Cost.instance().value += price\n    PromptTokens.instance().value += prompt_len\n    CompletionTokens.instance().value += completion_len\n\n    # print(f\"Prompt Tokens: {prompt_len}, Completion Tokens: {completion_len}\")\n    return price, prompt_len, completion_len\n\nOPENAI_MODEL_INFO ={\n    \"gpt-4\": {\n        \"current_recommended\": \"gpt-4-1106-preview\",\n        \"gpt-4-0125-preview\": {\n            \"context window\": 128000, \n            \"training\": \"Jan 2024\", \n            \"input\": 0.01, \n            \"output\": 0.03\n        },      \n        \"gpt-4-1106-preview\": {\n            \"context window\": 128000, \n            \"training\": \"Apr 2023\", \n            \"input\": 0.01, \n            \"output\": 0.03\n        },\n        \"gpt-4-vision-preview\": {\n            \"context window\": 128000, \n            \"training\": \"Apr 2023\", \n            \"input\": 0.01, \n            \"output\": 0.03\n        },\n        \"gpt-4\": {\n            \"context window\": 8192, \n            \"training\": \"Sep 2021\", \n            \"input\": 0.03, \n            \"output\": 0.06\n        },\n        \"gpt-4-0314\": {\n            \"context window\": 8192, \n            \"training\": \"Sep 2021\", \n            \"input\": 0.03, \n            \"output\": 0.06\n        },\n        \"gpt-4-32k\": {\n            \"context window\": 32768, \n            \"training\": \"Sep 2021\", \n            \"input\": 0.06, \n            \"output\": 0.12\n        },\n        \"gpt-4-32k-0314\": {\n            \"context window\": 32768, \n            \"training\": \"Sep 2021\", \n            \"input\": 0.06, \n            \"output\": 0.12\n        },\n        \"gpt-4-0613\": {\n            \"context window\": 8192, \n            \"training\": \"Sep 2021\", \n            \"input\": 0.06, \n            \"output\": 0.12\n        },\n        \"gpt-4o\": {\n            \"context window\": 128000, \n            \"training\": \"Jan 2024\", \n            \"input\": 0.005, \n            \"output\": 0.015\n        }, \n    },\n    \"gpt-3.5\": {\n        \"current_recommended\": \"gpt-3.5-turbo-1106\",\n        \"gpt-3.5-turbo-0125\": {\n            \"context window\": 16385, \n            \"training\": \"Jan 2024\", \n            \"input\": 0.0010, \n            \"output\": 0.0020\n        },\n        \"gpt-3.5-turbo-1106\": {\n            \"context window\": 16385, \n            \"training\": \"Sep 2021\", \n            \"input\": 0.0010, \n            \"output\": 0.0020\n        },\n        \"gpt-3.5-turbo-instruct\": {\n            \"context window\": 4096, \n            \"training\": \"Sep 2021\", \n            \"input\": 0.0015, \n            \"output\": 0.0020\n        },\n        \"gpt-3.5-turbo\": {\n            \"context window\": 4096, \n            \"training\": \"Sep 2021\", \n            \"input\": 0.0015, \n            \"output\": 0.0020\n        },\n        \"gpt-3.5-turbo-0301\": {\n            \"context window\": 4096, \n            \"training\": \"Sep 2021\", \n            \"input\": 0.0015, \n            \"output\": 0.0020\n        },\n        \"gpt-3.5-turbo-0613\": {\n            \"context window\": 16384, \n            \"training\": \"Sep 2021\", \n            \"input\": 0.0015, \n            \"output\": 0.0020\n        },\n        \"gpt-3.5-turbo-16k-0613\": {\n            \"context window\": 16384, \n            \"training\": \"Sep 2021\", \n            \"input\": 0.0015, \n            \"output\": 0.0020\n        }\n    },\n    \"dall-e\": {\n        \"current_recommended\": \"dall-e-3\",\n        \"dall-e-3\": {\n            \"release\": \"Nov 2023\",\n            \"standard\": {\n                \"1024×1024\": 0.040,\n                \"1024×1792\": 0.080,\n                \"1792×1024\": 0.080\n            },\n            \"hd\": {\n                \"1024×1024\": 0.080,\n                \"1024×1792\": 0.120,\n                \"1792×1024\": 0.120\n            }\n        },\n        \"dall-e-2\": {\n            \"release\": \"Nov 2022\",\n            \"1024×1024\": 0.020,\n            \"512×512\": 0.018,\n            \"256×256\": 0.016\n        }\n    }\n}\n\n\n\n"}
{"type": "source_file", "path": "AgentPrune/prompt/prompt_set_registry.py", "content": "from typing import Type\nfrom class_registry import ClassRegistry\n\nfrom AgentPrune.prompt.prompt_set import PromptSet\n\nclass PromptSetRegistry:\n    registry = ClassRegistry()\n\n    @classmethod\n    def register(cls, *args, **kwargs):\n        return cls.registry.register(*args, **kwargs)\n    \n    @classmethod\n    def keys(cls):\n        return cls.registry.keys()\n\n    @classmethod\n    def get(cls, name: str, *args, **kwargs) -> PromptSet:\n        return cls.registry.get(name, *args, **kwargs)\n\n    @classmethod\n    def get_class(cls, name: str) -> Type:\n        return cls.registry.get_class(name)\n"}
{"type": "source_file", "path": "AgentPrune/graph/node.py", "content": "import shortuuid\nfrom typing import List, Any, Optional,Dict\nfrom abc import ABC, abstractmethod\nimport warnings\nimport asyncio\n\n\nclass Node(ABC):\n    \"\"\"\n    Represents a processing unit within a graph-based framework.\n\n    This class encapsulates the functionality for a node in a graph, managing\n    connections to other nodes, handling inputs and outputs, and executing\n    assigned operations. It supports both individual and aggregated processing modes.\n\n    Attributes:\n        id (uuid.UUID): Unique identifier for the node.\n        agent_type(str): Associated agent name for node-specific operations.\n        spatial_predecessors (List[Node]): Nodes that precede this node in the graph.\n        spatial_successors (List[Node]): Nodes that succeed this node in the graph.\n        inputs (List[Any]): Inputs to be processed by the node.\n        outputs (List[Any]): Results produced after node execution.\n        raw_inputs (List[Any]): The original input contains the question or math problem.\n        last_memory (Dict[str,List[Any]]): Input and output of the previous timestamp.\n        \n    Methods:\n        add_predecessor(operation): \n            Adds a node as a predecessor of this node, establishing a directed connection.\n        add_successor(operation): \n            Adds a node as a successor of this node, establishing a directed connection.\n        memory_update():\n            Update the last_memory.\n        get_spatial_info():\n            Get all of the info from spatial spatial_predecessors.\n        execute(**kwargs): \n            Processes the inputs through the node's operation, handling each input individually.\n        _execute(input, **kwargs): \n            An internal method that defines how a single input is processed by the node. This method should be implemented specifically for each node type.\n        _process_inputs(raw_inputs, spatial_info, temporal_info, **kwargs)->List[Any]:\n            An internal medthod to process the raw_input, the spatial info and temporal info to get the final inputs.\n    \"\"\"\n\n    def __init__(self, \n                 id: Optional[str],\n                 agent_name:str=\"\",\n                 domain:str=\"\", \n                 llm_name:str = \"\",\n                 ):\n        \"\"\"\n        Initializes a new Node instance.\n        \"\"\"\n        self.id:str = id if id is not None else shortuuid.ShortUUID().random(length=4)\n        self.agent_name:str = agent_name\n        self.domain:str = domain\n        self.llm_name:str = llm_name\n        self.spatial_predecessors: List[Node] = []\n        self.spatial_successors: List[Node] = []\n        self.temporal_predecessors: List[Node] = []\n        self.temporal_successors: List[Node] = []\n        self.inputs: List[Any] = []\n        self.outputs: List[Any] = []\n        self.raw_inputs: List[Any] = []\n        self.role = \"\"\n        self.last_memory: Dict[str,List[Any]] = {'inputs':[],'outputs':[],'raw_inputs':[]}\n        self.conversation_history : List[Dict] = [] # chat history of the whole conversation        \n\n    @property\n    def node_name(self):\n        return self.__class__.__name__\n    \n    def add_predecessor(self, operation: 'Node', st='spatial'):\n        if st == 'spatial' and operation not in self.spatial_predecessors:\n            self.spatial_predecessors.append(operation)\n            operation.spatial_successors.append(self)\n        elif st == 'temporal' and operation not in self.temporal_predecessors:\n            self.temporal_predecessors.append(operation)\n            operation.temporal_successors.append(self)\n\n    def add_successor(self, operation: 'Node', st='spatial'):\n        if st =='spatial' and operation not in self.spatial_successors:\n            self.spatial_successors.append(operation)\n            operation.spatial_predecessors.append(self)\n        elif st == 'temporal' and operation not in self.temporal_successors:\n            self.temporal_successors.append(operation)\n            operation.temporal_predecessors.append(self)\n\n    def remove_predecessor(self, operation: 'Node', st='spatial'):\n        if st =='spatial' and operation in self.spatial_predecessors:\n            self.spatial_predecessors.remove(operation)\n            operation.spatial_successors.remove(self)\n        elif st =='temporal' and operation in self.temporal_predecessors:\n            self.temporal_predecessors.remove(operation)\n            operation.temporal_successors.remove(self)\n\n    def remove_successor(self, operation: 'Node', st='spatial'):\n        if st =='spatial' and operation in self.spatial_successors:\n            self.spatial_successors.remove(operation)\n            operation.spatial_predecessors.remove(self)\n        elif st =='temporal' and operation in self.temporal_successors:\n            self.temporal_successors.remove(operation)\n            operation.temporal_predecessors.remove(self)\n\n    def clear_connections(self):\n        self.spatial_predecessors: List[Node] = []\n        self.spatial_successors: List[Node] = []\n        self.temporal_predecessors: List[Node] = []\n        self.temporal_successors: List[Node] = []        \n    \n    def update_memory(self):\n        self.last_memory['inputs'] = self.inputs\n        self.last_memory['outputs'] = self.outputs\n        self.last_memory['raw_inputs'] = self.raw_inputs\n\n    def get_spatial_info(self)->Dict[str,Dict]:\n        \"\"\" Return a dict that maps id to info. \"\"\"\n        spatial_info = {}\n        if self.spatial_predecessors is not None:\n            for predecessor in self.spatial_predecessors:\n                predecessor_outputs = predecessor.outputs\n                if isinstance(predecessor_outputs, list) and len(predecessor_outputs):\n                    predecessor_output = predecessor_outputs[-1]\n                elif isinstance(predecessor_outputs, list) and len(predecessor_outputs)==0:\n                    continue\n                else:\n                    predecessor_output = predecessor_outputs\n                spatial_info[predecessor.id] = {\"role\":predecessor.role,\"output\":predecessor_output}\n\n        return spatial_info\n\n    def get_temporal_info(self)->Dict[str,Any]:\n        temporal_info = {}\n        if self.temporal_predecessors is not None:\n            for predecessor in self.temporal_predecessors:\n                predecessor_outputs = predecessor.last_memory['outputs']\n                if isinstance(predecessor_outputs, list) and len(predecessor_outputs):\n                    predecessor_output = predecessor_outputs[-1]\n                elif isinstance(predecessor_outputs, list) and len(predecessor_outputs)==0:\n                    continue\n                else:\n                    predecessor_output = predecessor_outputs\n                temporal_info[predecessor.id] = {\"role\":predecessor.role,\"output\":predecessor_output}\n        \n        return temporal_info\n    \n    def execute(self, input:Any, **kwargs):\n        self.outputs = []\n        spatial_info:Dict[str,Dict] = self.get_spatial_info()\n        temporal_info:Dict[str,Dict] = self.get_temporal_info()\n        results = [self._execute(input, spatial_info, temporal_info, **kwargs)]\n\n        for result in results:\n            if not isinstance(result, list):\n                result = [result]\n            self.outputs.extend(result)\n        return self.outputs\n\n\n    async def async_execute(self, input:Any, **kwargs):\n        self.outputs = []\n        spatial_info:Dict[str,Any] = self.get_spatial_info()\n        temporal_info:Dict[str,Any] = self.get_temporal_info()\n        tasks = [asyncio.create_task(self._async_execute(input, spatial_info, temporal_info, **kwargs))]\n        results = await asyncio.gather(*tasks, return_exceptions=False)\n        for result in results:\n            if not isinstance(result, list):\n                result = [result]\n            self.outputs.extend(result)\n        return self.outputs\n               \n    @abstractmethod\n    def _execute(self, input:List[Any], spatial_info:Dict[str,Any], temporal_info:Dict[str,Any], **kwargs):\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Use the processed input to get the result \"\"\"\n\n    @abstractmethod\n    async def _async_execute(self, input:List[Any], spatial_info:Dict[str,Any], temporal_info:Dict[str,Any], **kwargs):\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Use the processed input to get the result \"\"\"\n\n    @abstractmethod\n    def _process_inputs(self, raw_inputs:List[Any], spatial_info:Dict[str,Any], temporal_info:Dict[str,Any], **kwargs)->List[Any]:\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Process the raw_inputs(most of the time is a List[Dict]) \"\"\"\n"}
{"type": "source_file", "path": "AgentPrune/agents/final_decision.py", "content": "from typing import List,Any,Dict\n\nfrom AgentPrune.graph.node import Node\nfrom AgentPrune.agents.agent_registry import AgentRegistry\nfrom AgentPrune.llm.llm_registry import LLMRegistry\nfrom AgentPrune.prompt.prompt_set_registry import PromptSetRegistry\nfrom AgentPrune.tools.coding.python_executor import PyExecutor\n\n@AgentRegistry.register('FinalWriteCode')\nclass FinalWriteCode(Node):\n    def __init__(self, id: str | None =None,  domain: str = \"\", llm_name: str = \"\",):\n        super().__init__(id, \"FinalWriteCode\" ,domain, llm_name)\n        self.llm = LLMRegistry.get(llm_name)\n        self.prompt_set = PromptSetRegistry.get(domain)\n\n    def extract_example(self, prompt: str) -> list:\n        prompt = prompt['task']\n        lines = (line.strip() for line in prompt.split('\\n') if line.strip())\n\n        results = []\n        lines_iter = iter(lines)\n        for line in lines_iter:\n            if line.startswith('>>>'):\n                function_call = line[4:]\n                expected_output = next(lines_iter, None)\n                if expected_output:\n                    results.append(f\"assert {function_call} == {expected_output}\")\n\n        return results\n    \n    def _process_inputs(self, raw_inputs:Dict[str,str], spatial_info:Dict[str,Any], temporal_info:Dict[str,Any], **kwargs)->List[Any]:\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Process the raw_inputs(most of the time is a List[Dict]) \"\"\"\n        self.role = self.prompt_set.get_decision_role()\n        self.constraint = self.prompt_set.get_decision_constraint()          \n        system_prompt = f\"{self.role}.\\n {self.constraint}\"\n        spatial_str = \"\"\n        for id, info in spatial_info.items():\n            if info['output'].startswith(\"```python\") and info['output'].endswith(\"```\"):  # is python code\n                self.internal_tests = self.extract_example(raw_inputs)\n                output = info['output'].lstrip(\"```python\\n\").rstrip(\"\\n```\")\n                is_solved, feedback, state = PyExecutor().execute(output, self.internal_tests, timeout=10)\n                spatial_str += f\"Agent {id} as a {info['role']}:\\n\\nThe code written by the agent is:\\n\\n{info['output']}\\n\\n Whether it passes internal testing? {is_solved}.\\n\\nThe feedback is:\\n\\n {feedback}.\\n\\n\"\n            else:\n                spatial_str += f\"Agent {id} as a {info['role']} provides the following info: {info['output']}\\n\\n\"\n        user_prompt = f\"The task is:\\n\\n{raw_inputs['task']}.\\n At the same time, the outputs and feedbacks of other agents are as follows:\\n\\n{spatial_str}\\n\\n\"\n        return system_prompt, user_prompt\n                \n    def _execute(self, input:Dict[str,str],  spatial_info:Dict[str,Any], temporal_info:Dict[str,Any],**kwargs):\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Use the processed input to get the result \"\"\"\n  \n        system_prompt, user_prompt = self._process_inputs(input, spatial_info, temporal_info)\n        message = [{'role':'system','content':system_prompt},{'role':'user','content':user_prompt}]\n        response = self.llm.gen(message)\n        return response\n    \n    async def _async_execute(self, input:Dict[str,str],  spatial_info:Dict[str,Any], temporal_info:Dict[str,Any],**kwargs):\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Use the processed input to get the result \"\"\"\n  \n        system_prompt, user_prompt = self._process_inputs(input, spatial_info, temporal_info)\n        message = [{'role':'system','content':system_prompt},{'role':'user','content':user_prompt}]\n        response = await self.llm.agen(message)\n        return response\n\n\n@AgentRegistry.register('FinalRefer')\nclass FinalRefer(Node):\n    def __init__(self, id: str | None =None,  domain: str = \"\", llm_name: str = \"\",):\n        super().__init__(id, \"FinalRefer\" ,domain, llm_name)\n        self.llm = LLMRegistry.get(llm_name)\n        self.prompt_set = PromptSetRegistry.get(domain)\n\n    def _process_inputs(self, raw_inputs:Dict[str,str], spatial_info:Dict[str,Any], temporal_info:Dict[str,Any], **kwargs)->List[Any]:\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Process the raw_inputs(most of the time is a List[Dict]) \"\"\"\n        self.role = self.prompt_set.get_decision_role()\n        self.constraint = self.prompt_set.get_decision_constraint()          \n        system_prompt = f\"{self.role}.\\n {self.constraint}\"\n        \n        spatial_str = \"\"\n        for id, info in spatial_info.items():\n            spatial_str += id + \": \" + info['output'] + \"\\n\\n\"\n        decision_few_shot = self.prompt_set.get_decision_few_shot()\n        user_prompt = f\"{decision_few_shot} The task is:\\n\\n {raw_inputs['task']}.\\n At the same time, the output of other agents is as follows:\\n\\n{spatial_str}\"\n        return system_prompt, user_prompt\n                \n    def _execute(self, input:Dict[str,str],  spatial_info:Dict[str,Any], temporal_info:Dict[str,Any],**kwargs):\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Use the processed input to get the result \"\"\"\n  \n        system_prompt, user_prompt = self._process_inputs(input, spatial_info, temporal_info)\n        message = [{'role':'system','content':system_prompt},{'role':'user','content':user_prompt}]\n        response = self.llm.gen(message)\n        return response\n    \n    async def _async_execute(self, input:Dict[str,str],  spatial_info:Dict[str,Any], temporal_info:Dict[str,Any],**kwargs):\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Use the processed input to get the result \"\"\"\n  \n        system_prompt, user_prompt = self._process_inputs(input, spatial_info, temporal_info)\n        message = [{'role':'system','content':system_prompt},{'role':'user','content':user_prompt}]\n        response = await self.llm.agen(message)\n        return response\n\n@AgentRegistry.register('FinalDirect')\nclass FinalDirect(Node):\n    def __init__(self, id: str | None =None,  domain: str = \"\", llm_name: str = \"\",):\n        \"\"\" Used for Directed IO \"\"\"\n        super().__init__(id, \"FinalDirect\")\n        self.prompt_set = PromptSetRegistry.get(domain)\n        \n    def _process_inputs(self, raw_inputs:Dict[str,str], spatial_info:Dict[str,Any], temporal_info:Dict[str,Any], **kwargs)->List[Any]:\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Process the raw_inputs(most of the time is a List[Dict]) \"\"\"\n        return None\n                \n    def _execute(self, input:Dict[str,str],  spatial_info:Dict[str,Any], temporal_info:Dict[str,Any],**kwargs):\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Use the processed input to get the result \"\"\"\n        output = \"\"\n        info_list = []\n        for info in spatial_info.values():\n            info_list.append(info['output'])\n        if len(info_list):\n            output = info_list[-1]\n        return output\n    \n    async def _async_execute(self, input:Dict[str,str],  spatial_info:Dict[str,Any], temporal_info:Dict[str,Any],**kwargs):\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Use the processed input to get the result \"\"\"\n        output = \"\"\n        info_list = []\n        for info in spatial_info.values():\n            info_list.append(info['output'])\n        if len(info_list):\n            output = info_list[-1]\n        return output\n\n\n@AgentRegistry.register('FinalMajorVote')\nclass FinalMajorVote(Node):\n    def __init__(self, id: str | None =None,  domain: str = \"\", llm_name: str = \"\",):\n        \"\"\" Used for Directed IO \"\"\"\n        super().__init__(id, \"FinalMajorVote\")\n        self.prompt_set = PromptSetRegistry.get(domain)\n        \n    def _process_inputs(self, raw_inputs:Dict[str,str], spatial_info:Dict[str,Any], temporal_info:Dict[str,Any], **kwargs)->List[Any]:\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Process the raw_inputs(most of the time is a List[Dict]) \"\"\"\n        return None\n    \n    def _execute(self, input:Dict[str,str],  spatial_info:Dict[str,Any], temporal_info:Dict[str,Any],**kwargs):\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Use the processed input to get the result \"\"\"\n        output_num = {}\n        max_output = \"\"\n        max_output_num = 0\n        for info in spatial_info.values():\n            processed_output = self.prompt_set.postprocess_answer(info['output'])\n            if processed_output in output_num:\n                output_num[processed_output] += 1\n            else:\n                output_num[processed_output] = 1\n            if output_num[processed_output] > max_output_num:\n                max_output = processed_output\n                max_output_num = output_num[processed_output]\n        return max_output\n    \n    async def _async_execute(self, input:Dict[str,str],  spatial_info:Dict[str,Any], temporal_info:Dict[str,Any],**kwargs):\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Use the processed input to get the result \"\"\"\n        output_num = {}\n        max_output = \"\"\n        max_output_num = 0\n        for info in spatial_info.values():\n            processed_output = self.prompt_set.postprocess_answer(info['output'])\n            if processed_output in output_num:\n                output_num[processed_output] += 1\n            else:\n                output_num[processed_output] = 1\n            if output_num[processed_output] > max_output_num:\n                max_output = processed_output\n                max_output_num = output_num[processed_output]\n        return max_output\n"}
{"type": "source_file", "path": "AgentPrune/tools/coding/executor_factory.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\nfrom AgentPrune.utils.log import logger\nfrom AgentPrune.environment.tools.coding.python_executor import PyExecutor\nfrom AgentPrune.environment.tools.coding.executor_types import Executor\n\nEXECUTOR_MAPPING = {\n    \"py\": PyExecutor,\n    \"python\": PyExecutor,\n}\n\ndef executor_factory(lang: str) -> Executor:\n\n    if lang not in EXECUTOR_MAPPING:\n        raise ValueError(f\"Invalid language for executor: {lang}\")\n\n    executor_class = EXECUTOR_MAPPING[lang]\n    return executor_class()"}
{"type": "source_file", "path": "AgentPrune/tools/coding/executor_types.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom typing import NamedTuple, List, Tuple\nfrom abc import ABC, abstractmethod\n\nclass ExecuteResult(NamedTuple):\n    is_passing: bool\n    feedback: str\n    state: Tuple[bool]\n\nclass Executor(ABC):\n    @abstractmethod\n    def execute(self, func: str, tests: List[str], timeout: int = 5) -> ExecuteResult:\n        ...\n\n    @abstractmethod\n    def evaluate(self, name: str, func: str, test: str, timeout: int = 5) -> bool:\n        ...\n\n"}
{"type": "source_file", "path": "AgentPrune/llm/format.py", "content": "import dataclasses\nfrom typing import List, Literal\n\nMessageRole = Literal[\"system\", \"user\", \"assistant\"]\n\n@dataclasses.dataclass()\nclass Message:\n    role: MessageRole\n    content: str\n\n@dataclasses.dataclass\nclass Status:\n    started: int = 0\n    in_progress: int = 0\n    succeeded: int = 0\n    failed: int = 0\n"}
{"type": "source_file", "path": "AgentPrune/tools/web/screenshot.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nRef: https://github.com/abi/screenshot-to-code/blob/main/backend/routes/screenshot.py\n\"\"\"\n\n\nimport base64\nfrom fastapi import APIRouter\nfrom pydantic import BaseModel\nimport httpx\n\nrouter = APIRouter()\n\ndef bytes_to_data_url(image_bytes: bytes, mime_type: str) -> str:\n    base64_image = base64.b64encode(image_bytes).decode(\"utf-8\")\n    return f\"data:{mime_type};base64,{base64_image}\"\n\n\nasync def capture_screenshot(target_url, api_key, device=\"desktop\") -> bytes:\n    api_base_url = \"https://api.screenshotone.com/take\"\n\n    params = {\n        \"access_key\": api_key,\n        \"url\": target_url,\n        \"full_page\": \"true\",\n        \"device_scale_factor\": \"1\",\n        \"format\": \"png\",\n        \"block_ads\": \"true\",\n        \"block_cookie_banners\": \"true\",\n        \"block_trackers\": \"true\",\n        \"cache\": \"false\",\n        \"viewport_width\": \"342\",\n        \"viewport_height\": \"684\",        \n    }\n\n    if device == \"desktop\":\n        params[\"viewport_width\"] = \"1280\"\n        params[\"viewport_height\"] = \"832\"\n\n    async with httpx.AsyncClient(timeout=60) as client:\n        response = await client.get(api_base_url, params=params)\n        if response.status_code == 200 and response.content:\n            return response.content\n        else:\n            raise Exception(\"Error taking screenshot\")\n        \n\nclass ScreenshotRequest(BaseModel):\n    url: str\n    apiKey: str\n\n\nclass ScreenshotResponse(BaseModel):\n    url: str\n\n"}
{"type": "source_file", "path": "AgentPrune/tools/search/search.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nfrom dotenv import load_dotenv\nfrom googleapiclient.discovery import build\nimport requests\nimport ast\nload_dotenv()\n\n\nclass GoogleSearchEngine():\n    def __init__(self) -> None:\n        load_dotenv()\n        self.api_key = os.getenv(\"GOOGLE_API_KEY\")\n        self.cse_id = os.getenv(\"GOOGLE_CSE_ID\")\n        self.service = build(\"customsearch\", \"v1\", developerKey=self.api_key)\n        \n    def search(self, query: str, num: int = 3):\n        try:\n            res = self.service.cse().list(q=query, cx=self.cse_id, num=num).execute()\n            return '\\n'.join([item['snippet'] for item in res['items']])\n        except:\n            return ''\n\n\nclass SearchAPIEngine():\n\n    def search(self, query: str, item_num: int = 3):\n            try:\n                url = \"https://www.searchapi.io/api/v1/search\"\n                params = {\n                \"engine\": \"google\",\n                \"q\": query,\n                \"api_key\": os.getenv(\"SEARCHAPI_API_KEY\")\n                }\n\n                response = ast.literal_eval(requests.get(url, params = params).text)\n\n            except:\n                return ''\n            \n            if 'knowledge_graph' in response.keys() and 'description' in response['knowledge_graph'].keys():\n                return response['knowledge_graph']['description']\n            if 'organic_results' in response.keys() and len(response['organic_results']) > 0:\n                \n                return '\\n'.join([res['snippet'] for res in response['organic_results'][:item_num]])\n            return ''\n\n\n\nif __name__ == \"__main__\":\n    print(SearchAPIEngine().search(\"Juergen Schmidhuber\"))"}
{"type": "source_file", "path": "AgentPrune/tools/vgen/dalle3.py", "content": "# This code is adapted from https://github.com/abi/screenshot-to-code/blob/5e3a174203dd6e59603c2fa944b14c7b398bfade/backend/image_generation.py\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport asyncio\nimport os\nimport re\nfrom openai import AsyncOpenAI\nfrom bs4 import BeautifulSoup\n\n\nasync def process_tasks(prompts, api_key):\n    tasks = [generate_image(prompt, api_key) for prompt in prompts]\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n\n    processed_results = []\n    for result in results:\n        if isinstance(result, Exception):\n            print(f\"An exeception occured: {result}\")\n            processed_results.append(None)\n        else:\n            processed_results.append(result)\n\n    return processed_results\n\n\nasync def generate_image(prompt, api_key):\n    client = AsyncOpenAI(api_key=api_key)\n    image_params = {\n        \"model\": \"dall-e-3\",\n        \"quality\": \"standard\",\n        \"style\": \"natural\",\n        \"n\": 1,\n        \"size\": \"1024x1024\",\n        \"prompt\": prompt,\n    }\n    res = await client.images.generate(**image_params)\n    return res.data[0].url\n\n\ndef extract_dimensions(url):\n    # Regular expression to match numbers in the format '300x200'\n    matches = re.findall(r\"(\\d+)x(\\d+)\", url)\n\n    if matches:\n        width, height = matches[0] # Extract the first match\n        width = int(width)\n        height = int(height)\n        return (width, height)\n    else:\n        return (100, 100)\n    \n\ndef create_alt_url_mapping(code):\n    soup = BeautifulSoup(code, \"html.parser\")\n    images = soup.find_all(\"img\")\n\n    mapping = {}\n\n    for image in images:\n        if not image[\"src\"].startswith(\"https://placehold.co\"):\n            mapping[image[\"alt\"]] = image[\"src\"]\n\n    return mapping\n\n\nasync def generate_images(code, api_key, image_cache):\n    # Fine all images\n    soup = BeautifulSoup(code, \"html.parser\")\n    images = soup.find_all(\"img\")\n\n    # Extract alt texts as image prompts\n    alts = []\n    for img in images:\n        # Only include URL if the image starts with htt[s://placehold.co\n        # and it's not already in the image_cache\n        if (\n            img[\"src\"].startswith(\"https://placehold.co\")\n            and image_cache.get(img.get(\"alt\")) is None\n        ):\n            alts.append(img.get(\"alt\", None))\n\n    # Exclude images with no alt text\n    alts = [alt for alt in alts if alt is not None]\n\n    # Remove deplicates\n    prompts = list(set(alts))\n\n    # Return early if there are no images to replace\n    if len(prompts) == 0:\n        return code\n    \n    # Generate images\n    results = await process_tasks(prompts, api_key)\n\n    # Create a dict mapping alt text to image URL\n    mapped_image_urls = dict(zip(prompts, results))\n\n    # Merge with image_cache\n    mapped_image_urls = {**mapped_image_urls, **image_cache}\n\n    # Replace old image URLs with the generated URLs\n    for img in images:\n        # Skip images that don't start with https://placehold.co (leave them alone)\n        if not img[\"src\"].startswith(\"https://placehold.co\"):\n            continue\n\n        new_url = mapped_image_urls[img.get(\"alt\")]\n\n        if new_url:\n            # Set width and height attributes\n            width, height = extract_dimensions(img[\"src\"])\n            img[\"width\"] = width\n            img[\"height\"] = height\n            # Replace img['src'] with the mapped image URL\n            img[\"src\"] = new_url\n        else:\n            print(\"Image generation failed for alt text:\" + img.get(\"alt\"))\n\n    # Return the modified HTML\n    # (need to prettify it because BeautifulSoup messes up the formatting)\n    return soup.prettify()\n"}
{"type": "source_file", "path": "AgentPrune/prompt/mmlu_prompt_set.py", "content": "from typing import Union, Dict, Any, List\nimport itertools\n\nfrom AgentPrune.prompt.prompt_set import PromptSet\nfrom AgentPrune.prompt.prompt_set_registry import PromptSetRegistry\nfrom AgentPrune.prompt.common import get_combine_materials\n\n\nroles = itertools.cycle(['Knowlegable Expert',\n                        #  'Wiki Searcher',\n                         'Critic',\n                         'Mathematician',\n                         'Psychologist',\n                         'Historian',\n                         'Doctor',\n                         'Lawyer',\n                         'Economist',\n                         'Programmer'])\n\n\nROLE_DESCRIPTION = {\n\"Knowlegable Expert\":\n\"\"\"\nYou are a knowlegable expert in question answering.\nPlease give several key entities that need to be searched in wikipedia to solve the problem. \nKey entities that need to be searched are included between two '@' when output, for example: @catfish effect@, @broken window effect@, @Shakespeare@.\nIf there is no entity in the question that needs to be searched in Wikipedia, you don't have to provide it\n\"\"\",\n\"Wiki Searcher\":\n\"\"\"\nYou will be given a question and a wikipedia overview of the key entities within it.\nPlease refer to them step by step to give your answer.\nAnd point out potential issues in other agent's analysis.\n\"\"\",\n\"Critic\":\n\"\"\"\nYou are an excellent critic.\nPlease point out potential issues in other agent's analysis point by point.\n\"\"\",\n\"Mathematician\":\n\"\"\"\nYou are a mathematician who is good at math games, arithmetic calculation, and long-term planning.\n\"\"\",\n\"Psychologist\":\n\"\"\"\nYou are a psychologist.\nYou are good at psychology, sociology, and philosophy.\nYou give people scientific suggestions that will make them feel better.\n\"\"\",\n\"Historian\":\n\"\"\"\nYou research and analyze cultural, economic, political, and social events in the past, collect data from primary sources and use it to develop theories about what happened during various periods of history.\n\"\"\",\n\"Doctor\":\n\"\"\"\nYou are a doctor and come up with creative treatments for illnesses or diseases.\nYou are able to recommend conventional medicines, herbal remedies and other natural alternatives. \nYou also consider the patient's age, lifestyle and medical history when providing your recommendations.\n\"\"\",\n\"Lawyer\":\n\"\"\"\nYou are good at law, politics, and history.\n\"\"\",\n\"Economist\":\n\"\"\"\nYou are good at economics, finance, and business.\nYou have experience on understanding charts while interpreting the macroeconomic environment prevailing across world economies.\n\"\"\",\n\"Programmer\":\n\"\"\"\nYou are good at computer science, engineering, and physics.\nYou have experience in designing and developing computer software and hardware.\n\"\"\",\n\"Fake\":\n\"\"\"\nYou are a liar who only tell lies.\n\"\"\",\n}\n\n\n@PromptSetRegistry.register('mmlu')\nclass MMLUPromptSet(PromptSet):\n    \"\"\"\n    MMLU prompt set for the 4-option qestion answering.\n    \"\"\"\n    @staticmethod\n    def get_role():\n        return next(roles)\n\n    @staticmethod\n    def get_decision_role():\n        return \"You are the top decision-maker and are good at analyzing and summarizing other people's opinions, finding errors and giving final answers.\"\n\n    @staticmethod\n    def get_constraint():\n        return \"\"\"\n            I will ask you a question.\n            I will also give you 4 answers enumerated as A, B, C and D.\n            Only one answer out of the offered 4 is correct.\n            You must choose the correct answer to the question.\n            Your response must be one of the 4 letters: A, B, C or D,\n            corresponding to the correct answer.\n            Your answer can refer to the answers of other agents provided to you.\n            Your reply must be less than 100 words but include your answer and a brief step by step analysis of the question.\n            The first line of your reply must contain only one letter(for example : A, B, C or D)\n        \"\"\"\n    \n    @staticmethod\n    def get_analyze_constraint(role):\n        return ROLE_DESCRIPTION[role] if role in ROLE_DESCRIPTION.keys() else \"\"+ \"\"\"\nI will ask you a question and 4 answers enumerated as A, B, C and D.\nOnly one answer out of the offered 4 is correct.\nUsing the reasoning from other agents as additional advice with critical thinking, can you give an updated answer?\nYou are strictly prohibited from imitating the analysis process of other agents\nYour reply must be less than 100 words but include your answer and a brief step by step analysis of the question.\nThe first line of your reply must contain only one letter(for example : A, B, C or D)\n\"\"\"\n    \n    @staticmethod\n    def get_decision_constraint():\n        return \"\"\"\n        I will ask you a question.\n        I will also give you 4 answers enumerated as A, B, C and D.\n        Only one answer out of the offered 4 is correct.\n        You must choose the correct answer to the question.\n        Your response must be one of the 4 letters: A, B, C or D,\n        corresponding to the correct answer.\n        I will give you some other people's answers and analysis.\n        Your reply must only contain one letter and cannot have any other characters.\n        For example, your reply can be A.\n        \"\"\"\n    \n    @staticmethod\n    def get_format():\n        return NotImplementedError\n\n    @staticmethod\n    def get_answer_prompt(question):\n        return f\"\"\"{question}\"\"\"\n\n    @staticmethod\n    def get_query_prompt(question):\n        raise NotImplementedError\n\n    @staticmethod\n    def get_file_analysis_prompt(query, file):\n        raise NotImplementedError\n\n    @staticmethod\n    def get_websearch_prompt(query):\n        raise NotImplementedError\n\n    @staticmethod\n    def get_adversarial_answer_prompt(question):\n        return f\"\"\"Give a wrong answer and false analysis process for the following question: {question}.\n                You may get output from other agents, but no matter what, please only output lies and try your best to mislead other agents.\n                Your reply must be less than 100 words.\n                The first line of your reply must contain only one letter(for example : A, B, C or D)\n                \"\"\"\n    # @staticmethod\n    # def get_adversarial_answer_prompt(question):\n    #     return f\"\"\"Randomly output a letter from ABCD on the first line.\n    #             Then output any gibberish paragraph on the same topic as the following question: {question}.\n    #             The first line of your reply must contain only one letter(for example : A, B, C or D)\n    #             \"\"\"\n    @staticmethod\n    def get_distill_websearch_prompt(query, results):\n        raise NotImplementedError\n\n    @staticmethod\n    def get_reflect_prompt(question, answer):\n        raise NotImplementedError\n\n    @staticmethod\n    def get_combine_materials(materials: Dict[str, Any]) -> str:\n        return get_combine_materials(materials)\n    \n    @staticmethod\n    def get_decision_few_shot():\n        return \"\"\n    \n    def postprocess_answer(self, answer: Union[str, List[str]]) -> str:\n        if isinstance(answer, list):\n            if len(answer) > 0:\n                answer = answer[0]\n            else:\n                answer = \"\"\n        if not isinstance(answer, str):\n            raise Exception(\"Expected string\")\n        if len(answer) > 0:\n            answer = answer[0] # Try to format the answer by taking the first letter\n        return answer"}
{"type": "source_file", "path": "AgentPrune/tools/web/youtube.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom pytube import YouTube\nfrom swarm.utils.const import GPTSWARM_ROOT\n\ndef Youtube(url, has_subtitles):\n    # get video id from url\n    video_id=url.split('v=')[-1].split('&')[0]\n    # Create a YouTube object\n    youtube = YouTube(url)\n    # Get the best available video stream\n    video_stream = youtube.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first()\n    if has_subtitles:\n        # Download the video to a location\n        print('Downloading video')\n        video_stream.download(output_path=\"{GPTSWARM_ROOT}/workspace\",filename=f\"{video_id}.mp4\")\n        print('Video downloaded successfully')\n        return f\"{GPTSWARM_ROOT}/workspace/{video_id}.mp4\"\n    else:\n        return video_stream.url "}
{"type": "source_file", "path": "AgentPrune/tools/search/wiki.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport wikipedia\nimport asyncio\n\nclass WikiSearch:\n    def __init__(self):\n        self.name = \"Wikipedia SearchEngine\"\n        self.description = \"Seach for an item in Wikipedia\"\n\n    def search(self, query):\n        result = wikipedia.search(query[:300], results=1, suggestion=True)\n        if len(result[0]) != 0:\n            return wikipedia.page(title=result[0]).content\n        \n        if result[1] is not None:\n            result = wikipedia.search(result[1], results=1)\n            return wikipedia.page(title=result[0]).content\n        \n        return None\n\nasync def get_wikipedia_summary(title):\n    try:\n        wikipedia.set_lang(\"en\")\n        summ = wikipedia.summary(title)\n        return summ\n    except wikipedia.exceptions.DisambiguationError as e:\n        return await get_wikipedia_summary(e.options[0])\n    except wikipedia.exceptions.PageError:\n        return \"\"\n\nasync def search_wiki(query):\n    wikipedia.set_lang(\"en\")\n    result = wikipedia.search(query, results=2, suggestion=True)\n    ret = \"\"\n    tasks = []\n    \n    if len(result[0]) != 0:\n        for res in result[0]:\n            tasks.append(get_wikipedia_summary(res))\n        summaries = await asyncio.gather(*tasks)\n        for res, summa in zip(result[0], summaries):\n            if len(summa):\n                ret += f\"The summary of {res} in Wikipedia is: {summa}\\n\"\n    if result[1] is not None:\n        summa = await get_wikipedia_summary(result[1])\n        if len(summa):\n            ret += f\"The summary of {result[1]} in Wikipedia is: {summa}\\n\"\n    return ret\n\n\nasync def search_wiki_main(queries):\n    tasks = [search_wiki(query) for query in queries]\n    results = await asyncio.gather(*tasks)\n    return results\n\nif __name__ == \"__main__\":\n    queries = [\"Python\", \"Asyncio\", \"Wikipedia\"]\n    asyncio.run(search_wiki_main(queries))"}
{"type": "source_file", "path": "AgentPrune/prompt/prompt_set.py", "content": "from typing import Dict, Any\nfrom abc import ABC, abstractmethod\n\n\nclass PromptSet(ABC):\n    \"\"\"\n    Abstract base class for a set of prompts.\n    \"\"\"\n    @staticmethod\n    @abstractmethod\n    def get_role() -> str:\n        \"\"\" TODO \"\"\"\n\n    @staticmethod\n    @abstractmethod\n    def get_constraint() -> str:\n        \"\"\" TODO \"\"\"\n\n    @staticmethod\n    @abstractmethod\n    def get_format() -> str:\n        \"\"\" TODO \"\"\"\n\n    @staticmethod\n    @abstractmethod\n    def get_answer_prompt(question) -> str:\n        \"\"\" TODO \"\"\"\n\n    @staticmethod\n    @abstractmethod\n    def get_adversarial_answer_prompt(question) -> str:\n        \"\"\" TODO \"\"\"\n\n    @staticmethod\n    @abstractmethod\n    def get_query_prompt(question) -> str:\n        \"\"\" TODO \"\"\"\n\n    @staticmethod\n    @abstractmethod\n    def get_file_analysis_prompt(query, file) -> str:\n        \"\"\" TODO \"\"\"\n\n    @staticmethod\n    @abstractmethod\n    def get_websearch_prompt(query) -> str:\n        \"\"\" TODO \"\"\"\n\n    @staticmethod\n    @abstractmethod\n    def get_distill_websearch_prompt(query, results) -> str:\n        \"\"\" TODO \"\"\"\n\n    @staticmethod\n    @abstractmethod\n    def get_reflect_prompt(question, answer) -> str:\n        \"\"\" TODO \"\"\"\n\n    @staticmethod\n    def get_react_prompt(question, solutions, feedback) -> str:\n        \"\"\" TODO \"\"\"\n\n    # @staticmethod\n    # @abstractmethod\n    # def get_self_consistency(materials: Dict[str, Any]) -> str:\n    #     \"\"\" TODO \"\"\"\n\n    # @staticmethod\n    # @abstractmethod\n    # def get_select_best(materials: Dict[str, Any]) -> str:\n    #     \"\"\" TODO \"\"\"\n\n    @staticmethod\n    @abstractmethod\n    def get_combine_materials(materials: Dict[str, Any]) -> str:\n        \"\"\" TODO \"\"\"\n\n    @staticmethod\n    @abstractmethod\n    def get_decision_constraint() ->str:\n        \"\"\" TODO \"\"\"\n        \n    @staticmethod\n    @abstractmethod\n    def get_decision_role() ->str:\n        \"\"\" TODO \"\"\"\n\n    @staticmethod\n    @abstractmethod\n    def get_decision_few_shot() ->str:\n        \"\"\" TODO \"\"\""}
{"type": "source_file", "path": "AgentPrune/tools/coding/executor_utils.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport json\nfrom threading import Thread\n\ndef timeout_handler(_, __):\n    raise TimeoutError()\n\n\ndef to_jsonl(dict_data, file_path):\n    with open(file_path, 'a') as file:\n        json_line = json.dumps(dict_data)\n        file.write(json_line + os.linesep)\n\n\nclass PropagatingThread(Thread):\n    def run(self):\n        self.exc = None\n        try:\n            if hasattr(self, '_Thread__target'):\n                # Thread uses name mangling prior to Python 3.\n                self.ret = self._Thread__target(*self._Thread__args, **self._Thread__kwargs)\n            else:\n                self.ret = self._target(*self._args, **self._kwargs)\n        except BaseException as e:\n            self.exc = e\n\n    def join(self, timeout=None):\n        super(PropagatingThread, self).join(timeout)\n        if self.exc:\n            raise self.exc\n        return self.ret\n    \n\ndef function_with_timeout(func, args, timeout):\n    result_container = []\n\n    def wrapper():\n        result_container.append(func(*args))\n\n    thread = PropagatingThread(target=wrapper)\n    thread.start()\n    thread.join(timeout)\n\n    if thread.is_alive():\n        raise TimeoutError()\n    else:\n        return result_container[0]\n    \n\n"}
{"type": "source_file", "path": "AgentPrune/tools/search/arXiv.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport arxiv\n\n\nclass ArxivSearch:\n    def __init__(self):\n        self.name = \"ArXiv Searcher\"\n        self.description = \"Search for a paper on ArXiv\"\n\n    def search(self, query=None, id_list=None, sort_by=arxiv.SortCriterion.Relevance, sort_order=arxiv.SortOrder.Descending):\n        search = arxiv.Search(query=query, id_list=id_list, max_results=1, sort_by=sort_by, sort_order=sort_order)\n        results = arxiv.Client().results(search)\n        paper = next(results, None)\n        \n        return paper\n"}
{"type": "source_file", "path": "AgentPrune/llm/gpt_chat.py", "content": "import aiohttp\nfrom typing import List, Union, Optional\nfrom tenacity import retry, wait_random_exponential, stop_after_attempt\nfrom typing import Dict, Any\nfrom dotenv import load_dotenv\nimport os\nfrom openai import OpenAI, AsyncOpenAI\n\nfrom AgentPrune.llm.format import Message\nfrom AgentPrune.llm.price import cost_count\nfrom AgentPrune.llm.llm import LLM\nfrom AgentPrune.llm.llm_registry import LLMRegistry\n\n\nOPENAI_API_KEYS = ['']\nBASE_URL = ''\n\nload_dotenv()\nMINE_BASE_URL = os.getenv('BASE_URL')\nMINE_API_KEY = os.getenv('API_KEY')\n\n\n@retry(wait=wait_random_exponential(max=300), stop=stop_after_attempt(3))\nasync def achat(\n    model: str,\n    msg: List[Dict],):\n    client = AsyncOpenAI(base_url = MINE_BASE_URL, api_key = MINE_API_KEY,)\n    chat_completion = await client.chat.completions.create(messages = msg,model = model,)\n    response = chat_completion.choices[0].message.content\n    return response\n    \n\n@LLMRegistry.register('GPTChat')\nclass GPTChat(LLM):\n\n    def __init__(self, model_name: str):\n        self.model_name = model_name\n\n    async def agen(\n        self,\n        messages: List[Message],\n        max_tokens: Optional[int] = None,\n        temperature: Optional[float] = None,\n        num_comps: Optional[int] = None,\n        ) -> Union[List[str], str]:\n\n        if max_tokens is None:\n            max_tokens = self.DEFAULT_MAX_TOKENS\n        if temperature is None:\n            temperature = self.DEFAULT_TEMPERATURE\n        if num_comps is None:\n            num_comps = self.DEFUALT_NUM_COMPLETIONS\n        \n        if isinstance(messages, str):\n            messages = [{'role':'user', 'content':'messages'}]\n        return await achat(self.model_name,messages)\n    \n    def gen(\n        self,\n        messages: List[Message],\n        max_tokens: Optional[int] = None,\n        temperature: Optional[float] = None,\n        num_comps: Optional[int] = None,\n    ) -> Union[List[str], str]:\n        pass"}
{"type": "source_file", "path": "AgentPrune/utils/globals.py", "content": "import sys\nimport random\nfrom typing import Union, Literal, List\n\nclass Singleton:\n    _instance = None\n\n    @classmethod\n    def instance(cls):\n        if cls._instance is None:\n            cls._instance = cls()\n        return cls._instance\n    \n    def reset(self):\n        self.value = 0.0\n\nclass Cost(Singleton):\n    def __init__(self):\n        self.value = 0.0\n\nclass PromptTokens(Singleton):\n    def __init__(self):\n        self.value = 0.0\n\nclass CompletionTokens(Singleton):\n    def __init__(self):\n        self.value = 0.0\n\nclass Time(Singleton):\n    def __init__(self):\n        self.value = \"\"\n\nclass Mode(Singleton):\n    def __init__(self):\n        self.value = \"\"\n"}
{"type": "source_file", "path": "dataset/__init__.py", "content": ""}
{"type": "source_file", "path": "AgentPrune/graph/autogen_graph.py", "content": "from typing import List,Any,Dict,Optional\n\nimport torch\nfrom AgentPrune.graph.graph import Graph\n\nclass GraphAutoGen(Graph):\n    def __init__(self, \n                domain: str,\n                llm_name: Optional[str],\n                agent_names: List[str],\n                decision_method: str,\n                optimized_spatial:bool = False,\n                initial_spatial_probability: float = 0.5,\n                fixed_spatial_masks:List[List[int]] = None,\n                optimized_temporal:bool = False,\n                initial_temporal_probability: float = 0.5,\n                fixed_temporal_masks:List[List[int]] = None,\n                node_kwargs:List[Dict] = None,):\n        super().__init__(domain,llm_name,agent_names,decision_method,\n                         optimized_spatial,initial_spatial_probability,fixed_spatial_masks,\n                         optimized_temporal,initial_temporal_probability,fixed_temporal_masks,node_kwargs)\n        self.chain_idx_list:List[int] = []\n        self.chain_str_list:List[str] = []\n\n    def construct_spatial_connection(self, temperature: float = 1.0, threshold: float = None,): # temperature must >= 1.0\n        self.clear_spatial_connection()\n        log_probs = [torch.tensor(0.0, requires_grad=self.optimized_spatial)]\n        last_node_idx = 0\n        for i, in_node in enumerate(self.nodes.keys()):\n            if i == 0:\n                self.chain_idx_list.append(i)\n                self.chain_str_list.append(in_node)\n            if i < last_node_idx:\n                continue\n            for j, out_node in enumerate(self.nodes.keys()):\n                if i >= j:\n                    continue\n                edge_idx = i*len(self.nodes)+j\n                edge_prob = torch.sigmoid(self.spatial_logits[edge_idx] / temperature)\n                if torch.rand(1) < edge_prob:\n                    self.nodes[in_node].add_successor(self.nodes[out_node],'spatial')\n                    log_probs.append(torch.log(edge_prob))\n                    last_node_idx = j\n                    self.chain_idx_list.append(j)\n                    self.chain_str_list.append(out_node)\n                    break\n                else:\n                    log_probs.append(torch.log(1 - edge_prob))\n        return torch.sum(torch.stack(log_probs))\n    \n    def construct_temporal_connection(self, round:int = 0, temperature: float = 1.0, threshold: float = None,):  # temperature must >= 1.0\n        self.clear_temporal_connection()\n        log_probs = [torch.tensor(0.0, requires_grad=self.optimized_temporal)]\n        if round == 0:\n            return torch.sum(torch.stack(log_probs))\n        first_agent_idx:int = self.chain_idx_list[0]\n        last_agent_idx:int = self.chain_idx_list[-1]\n        first_agent:str = self.chain_str_list[0]\n        last_agent:str = self.chain_str_list[-1]\n\n        edge_idx = first_agent_idx*len(self.nodes)+last_agent_idx\n        edge_prob = torch.sigmoid(self.temporal_logits[edge_idx] / temperature)\n        if torch.rand(1) < edge_prob:\n            self.nodes[first_agent].add_successor(self.nodes[last_agent],'temporal')\n            log_probs.append(torch.log(edge_prob))\n        else:\n            log_probs.append(torch.log(1 - edge_prob))\n        return torch.sum(torch.stack(log_probs))\n"}
{"type": "source_file", "path": "AgentPrune/tools/coding/python_executor.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport ast\nimport astunparse\nfrom typing import List\n\nfrom AgentPrune.tools.coding.executor_utils import function_with_timeout\nfrom AgentPrune.tools.coding.executor_types import ExecuteResult, Executor\n\n\ndef get_call_str(assert_statement: str) -> str:\n    ast_parsed = ast.parse(assert_statement)\n    try:\n        call_str = ast_parsed.body[0].test.left # type: ignore\n    except:\n        call_str = ast_parsed.body[0].test # type: ignore\n\n    return astunparse.unparse(call_str).strip()\n\ndef get_output(func: str, assert_statement: str, timeout: int = 5) -> str:\n    try:\n        exec(f\"from typing import *\\n{func}\", globals())\n        func_call = get_call_str(assert_statement)\n        output = function_with_timeout(eval, (func_call, globals()), timeout)\n        return output\n    except TimeoutError:\n        return \"TIMEOUT\"\n    except Exception as e:\n        return str(e)\n    \ndef execute_code_get_return(code: str):\n    local_vars = {}\n    try:\n        exec(code, {}, local_vars)\n        if 'answer' in local_vars:\n            return local_vars['answer']\n        else:\n            return None\n    except Exception as e:\n        return f\"Error occurred: {e}\"\n\nclass PyExecutor(Executor):\n    def execute(self, func: str, tests: List[str], timeout: int = 5, verbose: bool = True) -> ExecuteResult:\n        # Combine function code and assert statement\n        imports = 'from typing import *'\n        func_test_list = [f'{imports}\\n{func}\\n{test}' for test in tests]\n\n        # Run the tests and collect the results\n        success_tests = []\n        failed_tests = []\n        is_passing = True\n        num_tests = len(func_test_list)\n        for i in range(num_tests):\n            try:\n                function_with_timeout(exec, (func_test_list[i], globals()), timeout)\n                success_tests.append(tests[i])\n            except Exception:\n                output = get_output(func, tests[i], timeout=timeout)\n                failed_tests.append(f\"{tests[i]} # output: {output}\")\n                is_passing = False\n\n        state = [test in success_tests for test in tests]\n\n        feedback = \"Tests passed:\\n\" + \"\\n\".join(success_tests) + \"\\n\\nTests failed:\"\n        feedback += \"\\n\" + \"\\n\".join(failed_tests)\n        return is_passing, feedback, tuple(state)\n\n    def evaluate(self, name: str, func: str, test: str, timeout: int = 5) -> bool:\n        \"\"\"\n        Evaluates the implementation on Human-Eval Python.\n\n        probably should be written in a dataset-agnostic way but not now\n        \"\"\"\n        \n        code = f\"\"\"{func}\n\n{test}\n\ncheck({name})\n    \"\"\"\n        try:\n            function_with_timeout(exec, (code, globals()), timeout)\n            return True\n        except Exception:\n            return False\n        "}
{"type": "source_file", "path": "dataset/MMLU/download.py", "content": "import os\nimport requests\nimport tarfile\n\n\ndef download():\n\n    this_file_path = os.path.split(__file__)[0]\n    tar_path = os.path.join(this_file_path, \"data.tar\")\n    if not os.path.exists(tar_path):\n        url = \"https://people.eecs.berkeley.edu/~hendrycks/data.tar\"\n        print(f\"Downloading {url}\")\n        r = requests.get(url, allow_redirects=True)\n        with open(tar_path, 'wb') as f:\n            f.write(r.content)\n        print(f\"Saved to {tar_path}\")\n\n    data_path = os.path.join(this_file_path, \"data\")\n    if not os.path.exists(data_path):\n        tar = tarfile.open(tar_path)\n        tar.extractall(this_file_path)\n        tar.close()\n        print(f\"Saved to {data_path}\")\n\n\nif __name__ == \"__main__\":\n    download()\n"}
{"type": "source_file", "path": "AgentPrune/prompt/common.py", "content": "import re\nfrom typing import Dict, Any\n\n\ndef get_combine_materials(materials: Dict[str, Any], avoid_vague=True) -> str:\n    question = materials.get('task', 'No problem provided')\n\n    for key, value in materials.items():\n        if \"No useful information from WebSearch\" in value:\n            continue\n        if isinstance(value, list):\n            value = \"\\n\".join(value)\n        if not (isinstance(value, str) and isinstance(key, str)):\n            continue\n        value = value.strip(\"\\n\").strip()\n        if key != 'task' and value:\n            question += f\"\\n\\nReference information for {key}:\" + \\\n                        \"\\n----------------------------------------------\\n\" + \\\n                        f\"{value}\" + \\\n                        \"\\n----------------------------------------------\\n\\n\" \n\n    if avoid_vague:  \n        question += \"\\nProvide a specific answer. For questions with known answers, ensure to provide accurate and factual responses. \" + \\\n                    \"Avoid vague responses or statements like 'unable to...' that don't contribute to a definitive answer. \" + \\\n                    \"For example: if a question asks 'who will be the president of America', and the answer is currently unknown, you could suggest possibilities like 'Donald Trump', or 'Biden'. However, if the answer is known, provide the correct information.\"\n\n    return question\n\n"}
{"type": "source_file", "path": "AgentPrune/utils/log.py", "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport os\nimport sys\nfrom pathlib import Path\nfrom loguru import logger\nfrom AgentPrune.utils.const import AgentPrune_ROOT\n\ndef configure_logging(print_level: str = \"INFO\", logfile_level: str = \"DEBUG\") -> None:\n    \"\"\"\n    Configure the logging settings for the application.\n\n    Args:\n        print_level (str): The logging level for console output.\n        logfile_level (str): The logging level for file output.\n    \"\"\"\n    logger.remove()\n    logger.add(sys.stderr, level=print_level)\n    logger.add(AgentPrune_ROOT / 'logs/log.txt', level=logfile_level, rotation=\"10 MB\")\n\ndef initialize_log_file(experiment_name: str, time_stamp: str) -> Path:\n    \"\"\"\n    Initialize the log file with a start message and return its path.\n\n    Args:\n        mode (str): The mode of operation, used in the file path.\n        time_stamp (str): The current timestamp, used in the file path.\n\n    Returns:\n        Path: The path to the initialized log file.\n    \"\"\"\n    try:\n        log_file_path = AgentPrune_ROOT / f'result/{experiment_name}/logs/log_{time_stamp}.txt'\n        os.makedirs(log_file_path.parent, exist_ok=True)\n        with open(log_file_path, 'w') as file:\n            file.write(\"============ Start ============\\n\")\n    except OSError as error:\n        logger.error(f\"Error initializing log file: {error}\")\n        raise\n    return log_file_path\n\ndef swarmlog(sender: str, text: str, cost: float,  prompt_tokens: int, complete_tokens: int, log_file_path: str) -> None:\n    \"\"\"\n    Custom log function for swarm operations. Includes dynamic global variables.\n\n    Args:\n        sender (str): The name of the sender.\n        text (str): The text message to log.\n        cost (float): The cost associated with the operation.\n        result_file (Path, optional): Path to the result file. Default is None.\n        solution (list, optional): Solution data to be logged. Default is an empty list.\n    \"\"\"\n    # Directly reference global variables for dynamic values\n    formatted_message = (\n        f\"{sender} | 💵Total Cost: ${cost:.5f} | \"\n        f\"Prompt Tokens: {prompt_tokens} | \"\n        f\"Completion Tokens: {complete_tokens} | \\n {text}\"\n    )\n    logger.info(formatted_message)\n\n    try:\n        os.makedirs(log_file_path.parent, exist_ok=True)\n        with open(log_file_path, 'a') as file:\n            file.write(f\"{formatted_message}\\n\")\n    except OSError as error:\n        logger.error(f\"Error initializing log file: {error}\")\n        raise\n\n\ndef main():\n    configure_logging()\n    # Example usage of swarmlog with dynamic values\n    swarmlog(\"SenderName\", \"This is a test message.\", 0.123)\n\nif __name__ == \"__main__\":\n    main()\n\n"}
{"type": "source_file", "path": "AgentPrune/utils/const.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nfrom pathlib import Path\n\n\nAgentPrune_ROOT = Path(os.path.realpath(os.path.join(os.path.split(__file__)[0], \"../..\")))\n"}
{"type": "source_file", "path": "AgentPrune/agents/code_writing.py", "content": "from typing import List,Any,Dict\n\nfrom AgentPrune.graph.node import Node\nfrom AgentPrune.agents.agent_registry import AgentRegistry\nfrom AgentPrune.llm.llm_registry import LLMRegistry\nfrom AgentPrune.prompt.prompt_set_registry import PromptSetRegistry\nfrom AgentPrune.tools.coding.python_executor import PyExecutor\n\n@AgentRegistry.register('CodeWriting')\nclass CodeWriting(Node):\n    def __init__(self, id: str | None =None, role:str = None ,domain: str = \"\", llm_name: str = \"\",):\n        super().__init__(id, \"CodeWriting\" ,domain, llm_name)\n        self.llm = LLMRegistry.get(llm_name)\n        self.prompt_set = PromptSetRegistry.get(domain)\n        self.role = self.prompt_set.get_role() if role is None else role\n        self.constraint = self.prompt_set.get_constraint(self.role) \n        \n    def _process_inputs(self, raw_inputs:Dict[str,str], spatial_info:Dict[str,Dict], temporal_info:Dict[str,Dict], **kwargs)->List[Any]:\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Process the raw_inputs(most of the time is a List[Dict]) \"\"\"             \n        system_prompt = self.constraint\n        spatial_str = \"\"\n        temporal_str = \"\"\n        for id, info in spatial_info.items():\n            if info['output'].startswith(\"```python\") and info['output'].endswith(\"```\") and self.role != 'Normal Programmer' and self.role != 'Stupid Programmer':\n                output = info['output'].lstrip(\"```python\\n\").rstrip(\"\\n```\")\n                is_solved, feedback, state = PyExecutor().execute(output, self.internal_tests, timeout=10)\n                if is_solved and len(self.internal_tests):\n                    return \"is_solved\", info['output']\n                spatial_str += f\"Agent {id} as a {info['role']}:\\n\\nThe code written by the agent is:\\n\\n{info['output']}\\n\\n Whether it passes internal testing? {is_solved}.\\n\\nThe feedback is:\\n\\n {feedback}.\\n\\n\"\n            else:\n                spatial_str += f\"Agent {id} as a {info['role']} provides the following info: {info['output']}\\n\\n\"\n        for id, info in temporal_info.items():\n            if info['output'].startswith(\"```python\") and info['output'].endswith(\"```\") and self.role != 'Normal Programmer' and self.role != 'Stupid Programmer':\n                output = info['output'].lstrip(\"```python\\n\").rstrip(\"\\n```\")\n                is_solved, feedback, state = PyExecutor().execute(output, self.internal_tests, timeout=10)\n                if is_solved and len(self.internal_tests):\n                    return \"is_solved\", info['output']\n                temporal_str += f\"Agent {id} as a {info['role']}:\\n\\nThe code written by the agent is:\\n\\n{info['output']}\\n\\n Whether it passes internal testing? {is_solved}.\\n\\nThe feedback is:\\n\\n {feedback}.\\n\\n\"\n            else:\n                temporal_str += f\"Agent {id} as a {info['role']} provides the following info: {info['output']}\\n\\n\"\n        user_prompt = f\"The task is:\\n\\n{raw_inputs['task']}\\n\"\n        user_prompt += f\"At the same time, the outputs and feedbacks of other agents are as follows:\\n\\n{spatial_str} \\n\\n\" if len(spatial_str) else \"\"\n        user_prompt += f\"In the last round of dialogue, the outputs and feedbacks of some agents were: \\n\\n{temporal_str}\" if len(temporal_str) else \"\"\n        return system_prompt, user_prompt\n\n    def extract_example(self, prompt: str) -> list:\n        prompt = prompt['task']\n        lines = (line.strip() for line in prompt.split('\\n') if line.strip())\n\n        results = []\n        lines_iter = iter(lines)\n        for line in lines_iter:\n            if line.startswith('>>>'):\n                function_call = line[4:]\n                expected_output = next(lines_iter, None)\n                if expected_output:\n                    results.append(f\"assert {function_call} == {expected_output}\")\n\n        return results\n    \n    def _execute(self, input:Dict[str,str],  spatial_info:Dict[str,Any], temporal_info:Dict[str,Any],**kwargs):\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Use the processed input to get the result \"\"\"\n        self.internal_tests = self.extract_example(input)\n        system_prompt, user_prompt = self._process_inputs(input, spatial_info, temporal_info)\n        message = [{'role':'system','content':system_prompt},{'role':'user','content':user_prompt}]\n        response = self.llm.gen(message)\n        return response\n\n    async def _async_execute(self, input:Dict[str,str],  spatial_info:Dict[str,Any], temporal_info:Dict[str,Any],**kwargs):\n        \"\"\" To be overriden by the descendant class \"\"\"\n        \"\"\" Use the processed input to get the result \"\"\"\n        \"\"\" The input type of this node is Dict \"\"\"\n        self.internal_tests = self.extract_example(input)\n        system_prompt, user_prompt = self._process_inputs(input, spatial_info, temporal_info)\n        ## test\n        if system_prompt == \"is_solved\":\n            return user_prompt\n        message = [{'role':'system','content':system_prompt},{'role':'user','content':user_prompt}]\n        response = await self.llm.agen(message)\n        return response"}
{"type": "source_file", "path": "AgentPrune/utils/utils.py", "content": "import re\n\nANS_RE = re.compile(r\"#### (\\-?[0-9\\.\\,]+)\")\nINVALID_ANS = \"[invalid]\"\n\nN_SHOT = 8\nCOT_FLAG = True\nDEBUG = False\nANSWER_TRIGGER = \"The answer is\"\n\ndef extract_answer_from_output(completion):\n    match = ANS_RE.search(completion)\n    if match:\n        match_str = match.group(1).strip()\n        match_str = match_str.replace(\",\", \"\")\n        return match_str\n    else:\n        return INVALID_ANS\n\ndef is_correct(model_answer, answer):\n    gt_answer = extract_answer_from_output(answer)\n    assert gt_answer != INVALID_ANS\n    return model_answer == gt_answer\n\ndef clean_answer(model_pred):\n    model_pred = model_pred.lower()\n    preds = model_pred.split(ANSWER_TRIGGER.lower())\n    answer_flag = True if len(preds) > 1 else False\n    if answer_flag:\n        # Pick first answer with flag\n        pred = preds[1]\n    else:\n        # Pick last number without flag\n        pred = preds[-1]\n\n    pred = pred.replace(\",\", \"\")\n    pred = [s for s in re.findall(r\"-?\\d+\\.?\\d*\", pred)]\n\n    if len(pred) == 0:\n        return INVALID_ANS\n\n    if answer_flag:\n        # choose the first element in list\n        pred = pred[0]\n    else:\n        # choose the last element in list\n        pred = pred[-1]\n\n    # (For arithmetic tasks) if a word ends with period, it will be omitted ...\n    if pred[-1] == \".\":\n        pred = pred[:-1]\n\n    return pred\n\n"}
