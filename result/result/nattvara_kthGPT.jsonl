{"repo_info": {"repo_name": "kthGPT", "repo_owner": "nattvara", "repo_url": "https://github.com/nattvara/kthGPT"}}
{"type": "test_file", "path": "tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/conftest.py", "content": "from fastapi.testclient import TestClient\nfrom db.models import Lecture, Analysis\nfrom fakeredis import FakeStrictRedis\nfrom rq import Queue\nfrom PIL import Image\nimport subprocess\nimport tempfile\nimport peewee\nimport random\nimport pytest\nimport shutil\nimport json\nimport os\n\nfrom db.models import ImageUpload, ImageQuestion\nfrom config.settings import settings\nfrom db.schema import all_models\nimport api\n\nTEST_API_ENDPOINT = 'https://example.com'\nTEST_MATHPIX_APP_ID = 'some_id'\nTEST_MATHPIX_APP_KEY = 'some_key'\n\nTEST_DB_FILEPATH = '/tmp/kthgpt.test.db'\nTEST_STORAGE_DIRECTORY = '/tmp/kthgpt-test-filesystem'\n\nQUEUE_DEFAULT = 'fake_default'\nQUEUE_DOWNLOAD = 'fake_download'\nQUEUE_EXTRACT = 'fake_extract'\nQUEUE_TRANSCRIBE = 'fake_transcribe'\nQUEUE_SUMMARISE = 'fake_summarise'\nQUEUE_MONITORING = 'fake_monitoring'\nQUEUE_APPROVAL = 'fake_approval'\nQUEUE_GPT = 'fake_gpt'\nQUEUE_METADATA = 'fake_metadata'\nQUEUE_IMAGE = 'fake_image'\nQUEUE_IMAGE_METADATA = 'fake_image_metadata'\nQUEUE_CLASSIFICATIONS = 'fake_classifications'\n\n\ndef pytest_configure(config):\n    settings.STORAGE_DIRECTORY = TEST_STORAGE_DIRECTORY\n    settings.OPENAI_API_KEY = 'sk-xxx...'\n    settings.API_ENDPOINT = TEST_API_ENDPOINT\n\n    settings.MATHPIX_APP_ID = TEST_MATHPIX_APP_ID\n    settings.MATHPIX_APP_KEY = TEST_MATHPIX_APP_KEY\n    settings.MATHPIX_DAILY_OCR_REQUESTS_LIMIT = 42\n\n    if os.path.exists(TEST_DB_FILEPATH):\n        os.unlink(TEST_DB_FILEPATH)\n\n    db = peewee.SqliteDatabase(TEST_DB_FILEPATH)\n    db.create_tables(all_models)\n\n\ndef pytest_unconfigure(config):\n    if os.path.exists(TEST_DB_FILEPATH):\n        os.unlink(TEST_DB_FILEPATH)\n\n    if os.path.exists(TEST_STORAGE_DIRECTORY):\n        shutil.rmtree(TEST_STORAGE_DIRECTORY)\n\n\ndef get_fake_queue(name):\n    queue = Queue(\n        name=name,\n        is_async=False,\n        connection=FakeStrictRedis()\n    )\n    yield queue\n\n\n@pytest.fixture(autouse=True)\ndef run_around_tests(mocker):\n    db = peewee.SqliteDatabase(TEST_DB_FILEPATH)\n    setup(db)\n\n    mocker.patch('jobs.get_default_queue', return_value=get_fake_queue(QUEUE_DEFAULT))\n    mocker.patch('jobs.get_download_queue', return_value=get_fake_queue(QUEUE_DOWNLOAD))\n    mocker.patch('jobs.get_extract_queue', return_value=get_fake_queue(QUEUE_EXTRACT))\n    mocker.patch('jobs.get_transcribe_queue', return_value=get_fake_queue(QUEUE_TRANSCRIBE))\n    mocker.patch('jobs.get_summarise_queue', return_value=get_fake_queue(QUEUE_SUMMARISE))\n    mocker.patch('jobs.get_monitoring_queue', return_value=get_fake_queue(QUEUE_MONITORING))\n    mocker.patch('jobs.get_approval_queue', return_value=get_fake_queue(QUEUE_APPROVAL))\n    mocker.patch('jobs.get_metadata_queue', return_value=get_fake_queue(QUEUE_METADATA))\n    mocker.patch('jobs.get_image_queue', return_value=get_fake_queue(QUEUE_IMAGE))\n    mocker.patch('jobs.get_image_metadata_queue', return_value=get_fake_queue(QUEUE_IMAGE_METADATA))\n    mocker.patch('jobs.get_classifications_queue', return_value=get_fake_queue(QUEUE_CLASSIFICATIONS))\n\n    yield\n    teardown(db)\n\n\ndef setup(db):\n    db.create_tables(all_models)\n\n\ndef teardown(db):\n    db.drop_tables(all_models)\n\n\n@pytest.fixture\ndef api_client():\n    client = TestClient(api.get_app())\n    return client\n\n\n@pytest.fixture\ndef analysed_lecture():\n    id = 'some_id'\n\n    lecture = Lecture(\n        public_id=id,\n        language='sv',\n        approved=True,\n        title='A lecture',\n        description='some description',\n    )\n    lecture.save()\n\n    analysis = Analysis(lecture_id=lecture.id)\n    analysis.save()\n\n    save_dummy_summary_for_lecture(lecture)\n    save_dummy_transcript_for_lecture(lecture)\n\n    return lecture\n\n\ndef save_dummy_summary_for_lecture(lecture: Lecture):\n    summary_filename = lecture.summary_filename()\n    if os.path.isfile(summary_filename):\n        os.unlink(summary_filename)\n\n    with open(summary_filename, 'w+') as file:\n        file.write('some summary')\n\n    lecture.summary_filepath = summary_filename\n    lecture.save()\n\n\ndef save_dummy_transcript_for_lecture(lecture: Lecture):\n    transcript_dirname = lecture.transcript_dirname()\n    if os.path.exists(transcript_dirname):\n        shutil.rmtree(transcript_dirname, ignore_errors=True)\n\n    os.mkdir(transcript_dirname)\n\n    transcript_filename = f'{transcript_dirname}/{lecture.public_id}.mp3.pretty.txt'\n    with open(transcript_filename, 'w+') as file:\n        file.write('transcript content...')\n\n    lecture.transcript_filepath = transcript_dirname\n    lecture.save()\n\n\n@pytest.fixture\ndef image_upload(img_file):\n    _, extension = os.path.splitext(img_file)\n\n    image = ImageUpload(\n        public_id=ImageUpload.make_public_id(),\n        file_format=extension.replace('.', ''),\n        search_queries_sv=json.dumps(['fråga 1', 'fråga 2']),\n        search_queries_en=json.dumps(['query 1', 'query 2']),\n        parse_image_content_ok=True,\n        create_title_ok=True,\n        create_description_en_ok=True,\n        create_description_sv_ok=True,\n        create_search_queries_en_ok=True,\n        create_search_queries_sv_ok=True,\n        classify_subjects_ok=True,\n    )\n    image.save()\n\n    with open(image.get_filename(), 'wb+') as img:\n        with open(img_file, 'rb') as src:\n            img.write(src.read())\n\n    return image\n\n\n@pytest.fixture\ndef image_question(image_upload):\n    question = ImageQuestion(\n        public_id=ImageQuestion.make_public_id(),\n        image_upload_id=image_upload.id,\n        query_string='help me',\n    )\n    question.save()\n\n    return question\n\n\n@pytest.fixture\ndef mp4_file():\n    tf = tempfile.NamedTemporaryFile(\n        mode='w+',\n        delete=False,\n        suffix='.mp4'\n    )\n    length = 3  # length in seconds\n\n    cmd = [\n        'ffmpeg',\n        '-y',\n        '-f',\n        'lavfi',\n        '-i',\n        'testsrc=size=1920x1080:rate=1',\n        '-vf',\n        'hue=s=0',\n        '-vcodec',\n        'libx264',\n        '-preset',\n        'superfast',\n        '-tune',\n        'zerolatency',\n        '-pix_fmt',\n        'yuv420p',\n        '-t',\n        str(length),\n        '-movflags',\n        '+faststart',\n        tf.name,\n    ]\n\n    env = os.environ.copy()\n\n    process = subprocess.Popen(\n        cmd,\n        shell=False,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        env=env,\n    )\n    process.wait()\n\n    yield tf.name\n\n    tf.close()\n    os.unlink(tf.name)\n\n\n@pytest.fixture\ndef mp3_file():\n    return os.path.join(os.path.dirname(__file__), 'files') + '/example.mp3'\n\n\n@pytest.fixture\ndef img_file():\n    return os.path.join(os.path.dirname(__file__), 'files') + '/example.png'\n\n\n@pytest.fixture\ndef random_image_generator():\n    def create_random_image():\n        tf = tempfile.NamedTemporaryFile(\n            mode='w+',\n            delete=False,\n            suffix='.png'\n        )\n\n        width = 100\n        height = 100\n\n        img = Image.new('RGB', (width, height))\n\n        for x in range(width):\n            for y in range(height):\n                r = random.randint(0, 255)\n                g = random.randint(0, 255)\n                b = random.randint(0, 255)\n                img.putpixel((x, y), (r, g, b))\n\n        img.save(tf.name)\n        yield tf.name\n\n        tf.close()\n        os.unlink(tf.name)\n\n    return create_random_image\n\n\n@pytest.fixture\ndef make_mocked_classifier():\n    def helper_func(labels, err=None):\n        class ClassifierMock:\n            def __init__(self) -> None:\n                pass\n\n            def classify(self, text: str) -> list:\n                if err:\n                    raise err\n                return labels\n\n        return ClassifierMock()\n\n    return helper_func\n"}
{"type": "test_file", "path": "tests/feature/api/test_assignments.py", "content": "from unittest.mock import call\nfrom io import BytesIO\nfrom PIL import Image\nimport filecmp\n\nfrom config.settings import settings\nfrom db.crud import (\n    get_image_upload_by_public_id,\n)\n\n\ndef test_assignment_can_be_created_from_image_upload(mocker, api_client, img_file):\n    mocker.patch('jobs.schedule_parse_image_upload')\n\n    response = api_client.post(\n        '/assignments/image',\n        files={\n            'file': ('a_file_name.png', open(img_file, 'rb'), 'image/png')\n        }\n    )\n\n    image_id = response.json()['id']\n\n    upload = get_image_upload_by_public_id(image_id)\n\n    assert upload is not None\n\n\ndef test_assignments_will_not_save_the_same_image_twice(mocker, api_client, img_file):\n    mocker.patch('jobs.schedule_parse_image_upload')\n\n    def func():\n        response = api_client.post(\n            '/assignments/image',\n            files={\n                'file': ('a_file_name.png', open(img_file, 'rb'), 'image/png')\n            }\n        )\n        return response\n\n    response = func()\n    image_id_1 = response.json()['id']\n\n    response = func()\n    image_id_2 = response.json()['id']\n\n    assert image_id_1 == image_id_2\n\n\ndef test_assignments_saves_png_file_upload(mocker, api_client, img_file):\n    mocker.patch('jobs.schedule_parse_image_upload')\n\n    response = api_client.post(\n        '/assignments/image',\n        files={\n            'file': ('a_file_name.png', open(img_file, 'rb'), 'image/png')\n        }\n    )\n\n    image_id = response.json()['id']\n\n    upload = get_image_upload_by_public_id(image_id)\n    assert filecmp.cmp(img_file, upload.get_filename())\n\n\ndef test_assignments_can_retrieve_image(api_client, image_upload):\n    response = api_client.get(f'/assignments/image/{image_upload.public_id}/img')\n\n    image = Image.open(BytesIO(response.content))\n    assert image.format == 'PNG'\n\n\ndef test_can_retrieve_info_from_assignments_api(api_client, image_upload):\n    image_upload.text_content = 'foo'\n    image_upload.description_en = 'bar'\n    image_upload.description_sv = 'baz'\n    image_upload.save()\n\n    response = api_client.get(f'/assignments/image/{image_upload.public_id}')\n\n    assert response.json()['text_content'] == 'foo'\n    assert response.json()['description_en'] == 'bar'\n    assert response.json()['description_sv'] == 'baz'\n\n\ndef test_image_upload_schedules_parse_image_upload_pipeline(mocker, api_client, img_file):\n    schedule_parse_image_upload_mock = mocker.patch('jobs.schedule_parse_image_upload')\n\n    response = api_client.post(\n        '/assignments/image',\n        files={\n            'file': ('a_file_name.png', open(img_file, 'rb'), 'image/png')\n        }\n    )\n\n    image_id = response.json()['id']\n    image = get_image_upload_by_public_id(image_id)\n\n    assert schedule_parse_image_upload_mock.call_count == 1\n    assert schedule_parse_image_upload_mock.mock_calls[0] == call(image)\n\n\ndef test_image_upload_can_be_restarted(mocker, api_client, image_upload):\n    schedule_parse_image_upload_mock = mocker.patch('jobs.schedule_parse_image_upload')\n\n    response = api_client.post(\n        f'/assignments/image/{image_upload.public_id}?restart=true',\n    )\n\n    # only schedules pipeline if not successful\n    assert schedule_parse_image_upload_mock.call_count == 0\n    image_upload.create_description_sv_ok = False\n    image_upload.save()\n\n    response = api_client.post(\n        f'/assignments/image/{image_upload.public_id}?restart=true',\n    )\n\n    image_id = response.json()['id']\n    image = get_image_upload_by_public_id(image_id)\n\n    assert schedule_parse_image_upload_mock.call_count == 1\n    assert schedule_parse_image_upload_mock.mock_calls[0] == call(image)\n\n\ndef test_random_upload_of_subject_can_be_retrieved(mocker, api_client, image_upload):\n    image_upload.add_subject('Analysis and Calculus')\n\n    response = api_client.get('/assignments/image/random/Analysis and Calculus')\n\n    image_id = response.json()['id']\n    image = get_image_upload_by_public_id(image_id)\n\n    assert 'Analysis and Calculus' in image.subjects_list()\n\n\ndef test_assignments_respects_daily_rate_limit(mocker, api_client, random_image_generator):\n    mocker.patch('jobs.start_parse_image_upload_pipeline')\n\n    # Limit for this testcase should be 42\n    assert settings.MATHPIX_DAILY_OCR_REQUESTS_LIMIT == 42\n\n    def do_upload():\n        filename = next(random_image_generator())\n        response = api_client.post(\n            '/assignments/image',\n            files={\n                'file': ('a_file_name.png', open(filename, 'rb'), 'image/png')\n            }\n        )\n        return response\n\n    for _ in range(settings.MATHPIX_DAILY_OCR_REQUESTS_LIMIT):\n        response = do_upload()\n        assert response.status_code == 200\n\n    # Rate limit should now be exceeded\n    response = do_upload()\n    assert response.status_code == 500\n"}
{"type": "test_file", "path": "tests/feature/api/test_courses.py", "content": "from db.models import Course\n\n\ndef test_get_course_by_course_code(api_client):\n    course = Course(\n        course_code='SF1626',\n        swedish_name='Flervariabelanalys',\n        english_name='Calculus in Several Variables',\n        points='6.0 hp',\n        cycle='First cycle',\n    )\n    course.save()\n\n    response = api_client.get('/courses/SF1626')\n\n    assert response.json()['display_name'] == 'Flervariabelanalys'\n"}
{"type": "test_file", "path": "tests/feature/api/test_lectures.py", "content": "from db.models import Lecture, Analysis\n\n\ndef test_get_all_lectures(api_client):\n    id = 'some_id'\n\n    lecture = Lecture(public_id=id, language='sv')\n    lecture.save()\n\n    analysis = Analysis(lecture_id=lecture.id)\n    analysis.save()\n\n    response = api_client.get('/lectures')\n\n    assert response.json()[0]['public_id'] == id\n"}
{"type": "test_file", "path": "tests/feature/api/test_index.py", "content": "\n\ndef test_index_response(api_client):\n    response = api_client.get('/')\n\n    assert response.status_code == 200\n    assert response.json() == {'message': 'I am the API.'}\n"}
{"type": "test_file", "path": "tests/feature/api/test_query.py", "content": "\n\ndef test_lecture_query_can_be_made_about_lecture(mocker, api_client, analysed_lecture):\n    mocker.patch('tools.text.ai.gpt3', return_value='gpt-3 response')\n\n    response = api_client.post('/query/lecture', json={\n        'lecture_id': analysed_lecture.public_id,\n        'language': analysed_lecture.language,\n        'query_string': 'some interesting question',\n    })\n\n    assert response.json()['response'] == 'gpt-3 response'\n\n\ndef test_lecture_query_response_is_cached(mocker, api_client, analysed_lecture):\n    gpt3 = mocker.patch('tools.text.ai.gpt3', return_value='gpt-3 response')\n\n    def request():\n        return api_client.post('/query/lecture', json={\n            'lecture_id': analysed_lecture.public_id,\n            'language': analysed_lecture.language,\n            'query_string': 'some interesting question',\n        })\n\n    response = request()\n    response = request()\n\n    assert response.json()['response'] == 'gpt-3 response'\n    assert gpt3.call_count == 1\n\n\ndef test_lecture_query_response_cache_can_be_overridden(mocker, api_client, analysed_lecture):\n    gpt3 = mocker.patch('tools.text.ai.gpt3', return_value='gpt-3 response')\n\n    def request():\n        return api_client.post('/query/lecture', json={\n            'lecture_id': analysed_lecture.public_id,\n            'language': analysed_lecture.language,\n            'query_string': 'some interesting question',\n            'override_cache': True,\n        })\n\n    response = request()\n    response = request()\n\n    assert response.json()['response'] == 'gpt-3 response'\n    assert gpt3.call_count == 2\n\n\ndef test_query_response_cache_can_be_invalidated_for_lectures(mocker, api_client, analysed_lecture):\n    gpt3 = mocker.patch('tools.text.ai.gpt3', return_value='gpt-3 response')\n\n    def make_query(query_string: str):\n        return api_client.post('/query/lecture', json={\n            'lecture_id': analysed_lecture.public_id,\n            'language': analysed_lecture.language,\n            'query_string': query_string,\n        })\n\n    def make_requests():\n        make_query('some query')\n        make_query('some query')\n        make_query('some other query')\n        make_query('some third query')\n\n    make_requests()\n    assert gpt3.call_count == 3\n\n    # invalidate the cache\n    queries = analysed_lecture.queries()\n\n    for query in queries:\n        query.cache_is_valid = False\n        query.save()\n\n    make_requests()\n    assert gpt3.call_count == 6\n\n\ndef test_query_can_be_made_about_image_upload(mocker, api_client, image_upload):\n    mocker.patch('tools.text.ai.gpt3', return_value='gpt-3 response')\n\n    response = api_client.post('/query/image', json={\n        'image_id': image_upload.public_id,\n        'query_string': 'some interesting question',\n    })\n\n    assert response.json()['response'] == 'gpt-3 response'\n\n\ndef test_image_query_response_is_cached(mocker, api_client, image_upload):\n    gpt3 = mocker.patch('tools.text.ai.gpt3', return_value='gpt-3 response')\n\n    def request():\n        return api_client.post('/query/image', json={\n            'image_id': image_upload.public_id,\n            'query_string': 'some interesting question',\n        })\n\n    response = request()\n    response = request()\n\n    assert response.json()['response'] == 'gpt-3 response'\n    assert gpt3.call_count == 1\n\n\ndef test_image_query_response_cache_can_be_overridden(mocker, api_client, image_upload):\n    gpt3 = mocker.patch('tools.text.ai.gpt3', return_value='gpt-3 response')\n\n    def request():\n        return api_client.post('/query/image', json={\n            'image_id': image_upload.public_id,\n            'query_string': 'some interesting question',\n            'override_cache': True,\n        })\n\n    response = request()\n    response = request()\n\n    assert response.json()['response'] == 'gpt-3 response'\n    assert gpt3.call_count == 2\n\n\ndef test_query_response_cache_can_be_invalidated_for_images(mocker, api_client, image_upload):\n    gpt3 = mocker.patch('tools.text.ai.gpt3', return_value='gpt-3 response')\n\n    def make_query(query_string: str):\n        return api_client.post('/query/image', json={\n            'image_id': image_upload.public_id,\n            'query_string': query_string,\n        })\n\n    def make_requests():\n        make_query('some query')\n        make_query('some query')\n        make_query('some other query')\n        make_query('some third query')\n\n    make_requests()\n    assert gpt3.call_count == 3\n\n    # invalidate the cache\n    queries = image_upload.queries()\n\n    for query in queries:\n        query.cache_is_valid = False\n        query.save()\n\n    make_requests()\n    assert gpt3.call_count == 6\n"}
{"type": "test_file", "path": "tests/feature/api/test_search.py", "content": "from unittest.mock import call\nimport random\n\nfrom db.models import Lecture\nimport api.routers.search\n\n\ndef test_api_can_search_courses(mocker, api_client):\n    index = mocker.patch('index.courses.wildcard_search', return_value=[\n        {\n            'course_code': 'XX1337',\n            'display_name': 'Course name',\n            'lectures': None\n        }\n    ])\n\n    response = api_client.post('/search/course', json={\n        'query': 'XX1337',\n        'limit': 40,\n    })\n\n    assert response.json()[0]['course_code'] == 'XX1337'\n    assert index.call_count == 1\n    assert index.mock_calls[0] == call(\n        'XX1337',\n        include_lectures=None,  # should not include lecture count\n        lecture_count_above_or_equal=0,  # include all lectures\n        unlimited=False,\n        sort_by_lecture_count=False,\n        apply_filter=True\n    )\n\n\ndef test_limit_must_be_specified(api_client):\n    response = api_client.post('/search/course', json={\n        'query': 'XX1337',\n    })\n\n    assert response.status_code == 400\n    assert response.json()['detail'] == 'Please specify a limit to the search result'\n\n\ndef test_course_search_can_include_lecture_count(mocker, api_client):\n    index = mocker.patch('index.courses.wildcard_search', return_value=[\n        {\n            'course_code': 'XX1337',\n            'display_name': 'Course name',\n            'lectures': 123\n        }\n    ])\n\n    response = api_client.post('/search/course?include_lectures=true', json={\n        'query': 'XX1337',\n        'limit': 40,\n    })\n\n    assert response.json()[0]['lectures'] == 123\n    assert index.call_count == 1\n    assert index.mock_calls[0] == call(\n        'XX1337',\n        include_lectures=True,\n        lecture_count_above_or_equal=0,\n        unlimited=False,\n        sort_by_lecture_count=False,\n        apply_filter=True\n    )\n\n\ndef test_result_can_be_limited_to_non_courses_with_given_lecture_count(mocker, api_client):\n    index = mocker.patch('index.courses.wildcard_search', return_value=[\n        {\n            'course_code': 'XX1337',\n            'display_name': 'Course name',\n            'lectures': 123\n        }\n    ])\n\n    api_client.post('/search/course?lecture_count_above_or_equal=100', json={\n        'query': 'XX1337',\n        'limit': 40,\n    })\n\n    assert index.call_count == 1\n    assert index.mock_calls[0] == call(\n        'XX1337',\n        include_lectures=None,\n        lecture_count_above_or_equal=100,\n        unlimited=True,\n        sort_by_lecture_count=True,\n        apply_filter=True\n    )\n\n\ndef test_lecture_search_inside_course(mocker, api_client, analysed_lecture):\n    doc = analysed_lecture.to_doc()\n    doc['date'] = doc['date'].isoformat()\n    index = mocker.patch('index.lecture.search_in_course', return_value=[\n        doc,\n        doc,\n    ])\n\n    response = api_client.post('/search/course/XX1337', json={\n        'query': 'some query',\n    })\n\n    assert len(response.json()) == 2\n    assert index.call_count == 1\n    assert index.mock_calls[0] == call(\n        'some query',\n        'XX1337',\n        apply_filter=True,\n        source=None,\n        group=None,\n    )\n\n\ndef test_lecture_search_inside_no_course(mocker, api_client, analysed_lecture):\n    doc = analysed_lecture.to_doc()\n    doc['date'] = doc['date'].isoformat()\n    index = mocker.patch('index.lecture.search_in_course', return_value=[\n        doc,\n    ])\n\n    response = api_client.post('/search/course/no_course', json={\n        'query': 'some query',\n    })\n\n    assert len(response.json()) == 1\n    assert index.call_count == 1\n    assert index.mock_calls[0] == call(\n        'some query',\n        no_course=True,\n        apply_filter=True,\n        source=None,\n        group=None,\n    )\n\n\ndef test_lecture_search_inside_course_can_be_restricted_to_source(mocker, api_client, analysed_lecture):\n    doc = analysed_lecture.to_doc()\n    doc['date'] = doc['date'].isoformat()\n    index = mocker.patch('index.lecture.search_in_course', return_value=[\n        doc,\n        doc,\n    ])\n\n    response = api_client.post('/search/course/XX1337', json={\n        'query': 'some query',\n        'source': 'youtube',\n    })\n\n    assert len(response.json()) == 2\n    assert index.call_count == 1\n    assert index.mock_calls[0] == call(\n        'some query',\n        'XX1337',\n        apply_filter=True,\n        source='youtube',\n        group=None,\n    )\n\n\ndef test_lecture_search_inside_course_includes_kth_raw_with_kth_source(mocker, api_client, analysed_lecture):\n    doc = analysed_lecture.to_doc()\n    doc['date'] = doc['date'].isoformat()\n    index = mocker.patch('index.lecture.search_in_course', return_value=[\n        doc,\n        doc,\n    ])\n\n    response = api_client.post('/search/course/XX1337', json={\n        'query': 'some query',\n        'source': 'kth',\n    })\n\n    assert len(response.json()) == 4\n    assert index.call_count == 2\n    assert index.mock_calls[0] == call(\n        'some query',\n        'XX1337',\n        apply_filter=True,\n        source='kth',\n        group=None,\n    )\n    assert index.mock_calls[1] == call(\n        'some query',\n        'XX1337',\n        apply_filter=True,\n        source='kth_raw',\n        group=None,\n    )\n\n\ndef test_lecture_search_inside_course_can_be_restricted_to_group(mocker, api_client, analysed_lecture):\n    doc = analysed_lecture.to_doc()\n    doc['date'] = doc['date'].isoformat()\n    index = mocker.patch('index.lecture.search_in_course', return_value=[\n        doc,\n        doc,\n    ])\n\n    response = api_client.post('/search/course/XX1337', json={\n        'query': 'some query',\n        'group': 'some_group',\n    })\n\n    assert len(response.json()) == 2\n    assert index.call_count == 1\n    assert index.mock_calls[0] == call(\n        'some query',\n        'XX1337',\n        apply_filter=True,\n        source=None,\n        group='some_group',\n    )\n\n\ndef test_lecture_transcript_search(mocker, api_client, analysed_lecture):\n    doc = analysed_lecture.to_doc()\n    doc['date'] = doc['date'].isoformat()\n    doc['highlight'] = {\n        'transcript': [\n            '00:00 -> 00:30 foo',\n            '01:00 -> 01:20 <strong>match</strong>',\n        ],\n        'title': ['<strong>match</strong>'],\n    }\n\n    index = mocker.patch('index.lecture.search_in_transcripts_and_titles', return_value=[\n        doc,\n    ])\n\n    response = api_client.post('/search/lecture', json={\n        'query': 'some query',\n    })\n\n    assert len(response.json()) == 1\n    assert index.call_count == 1\n    assert response.json()[0]['highlight']['transcript'][0] == '00:00 -> 00:30 foo'\n\n\ndef test_image_question_can_be_created_for_uploaded_image(mocker, api_client, image_upload):\n    mocker.patch('tools.text.ai.gpt3', return_value='')\n    mocker.patch('index.lecture.search_in_transcripts_and_titles')\n\n    assert len(image_upload.questions()) == 0\n\n    api_client.post(\n        f'/search/image/{image_upload.public_id}/questions',\n        json={\n            'query': 'help me',\n        }\n    )\n\n    assert len(image_upload.questions()) == 1\n    assert image_upload.questions()[0].query_string == 'help me'\n\n\ndef test_image_question_runs_searches_for_each_query(mocker, api_client, image_upload):\n    mocker.patch('tools.text.ai.gpt3', return_value='')\n    index_mock = mocker.patch('index.lecture.search_in_transcripts_and_titles')\n\n    swedish_queries = len(image_upload.get_search_queries_sv())\n    english_queries = len(image_upload.get_search_queries_en())\n\n    api_client.post(\n        f'/search/image/{image_upload.public_id}/questions',\n        json={\n            'query': 'help me',\n        }\n    )\n\n    assert index_mock.call_count == swedish_queries + english_queries\n\n\ndef test_image_question_aggregates_the_top_hits(mocker, api_client, image_upload):\n    mocker.patch('tools.text.ai.gpt3', return_value='')\n\n    lecture1 = Lecture(\n        public_id='id-1',\n        language='sv',\n        approved=True,\n        title='A lecture',\n    )\n    lecture1.save()\n    lecture2 = Lecture(\n        public_id='id-2',\n        language='sv',\n        approved=True,\n        title='A lecture',\n    )\n    lecture2.save()\n\n    # Override the limit to match the sample below,\n    # with the given sample below, we only care about two docs (matching the created lectures)\n    api.routers.search.MAX_NUMBER_IMAGE_HITS = 2\n\n    # Create two sample of hits\n    # The documents at index 3 and 6 have way higher scores\n    # This test should prove that these two gets returned\n    sample_1 = [\n        {\n            '_id': 100,\n            '_score': 1.1,\n        },\n        {\n            '_id': 101,\n            '_score': 1.17,\n        },\n        {\n            '_id': lecture1.id,\n            '_score': 15.51,\n        },\n        {\n            '_id': 102,\n            '_score': 1.4,\n        },\n        {\n            '_id': 103,\n            '_score': 2.23,\n        },\n        {\n            '_id': lecture2.id,\n            '_score': 10.1253,\n        },\n        {\n            '_id': 104,\n            '_score': 1.3344,\n        },\n    ]\n    sample_2 = [\n        {\n            '_id': 100,\n            '_score': 3.42341,\n        },\n        {\n            '_id': 101,\n            '_score': 1.17342,\n        },\n        {\n            '_id': lecture1.id,\n            '_score': 27.92344,\n        },\n        {\n            '_id': 102,\n            '_score': 2.544,\n        },\n        {\n            '_id': 103,\n            '_score': 0.73234,\n        },\n        {\n            '_id': lecture2.id,\n            '_score': 20.177423,\n        },\n        {\n            '_id': 104,\n            '_score': 0.1,\n        },\n    ]\n\n    def random_sample(query: str, include_id=False, include_score=False):\n        random_number = random.randint(0, 1)\n        if random_number == 0:\n            return sample_1\n        return sample_2\n\n    index_mock = mocker.patch('index.lecture.search_in_transcripts_and_titles', side_effect=random_sample)\n\n    swedish_queries = len(image_upload.get_search_queries_sv())\n    english_queries = len(image_upload.get_search_queries_en())\n\n    response = api_client.post(\n        f'/search/image/{image_upload.public_id}/questions',\n        json={\n            'query': 'help me',\n        }\n    )\n\n    assert index_mock.call_count == swedish_queries + english_queries\n    assert len(response.json()['hits']) == 2\n    for hit in response.json()['hits']:\n        assert hit['lecture']['public_id'] in ['id-1', 'id-2']\n\n\ndef test_answer_to_question_hit_can_be_retrieved(\n    mocker,\n    api_client,\n    image_upload,\n    analysed_lecture,\n):\n    mocker.patch('index.lecture.search_in_transcripts_and_titles', return_value=[\n        {\n            '_id': analysed_lecture.id,\n            '_score': 3.14,\n        }\n    ])\n    mocker.patch('tools.text.ai.gpt3', return_value='you can find the answer here')\n\n    response = api_client.post(\n        f'/search/image/{image_upload.public_id}/questions',\n        json={\n            'query': 'help me',\n        }\n    )\n\n    question_id = response.json()['id']\n    hit = response.json()['hits'][0]\n\n    response = api_client.get(\n        f'/search/image/{image_upload.public_id}/questions/{question_id}/hits/{hit[\"id\"]}/answer',\n    )\n\n    assert response.json()['answer'] == 'you can find the answer here'\n\n\ndef test_relevance_of_hit_can_be_retrieved(\n    mocker,\n    api_client,\n    image_upload,\n    analysed_lecture,\n):\n    mocker.patch('index.lecture.search_in_transcripts_and_titles', return_value=[\n        {\n            '_id': analysed_lecture.id,\n            '_score': 3.14,\n        }\n    ])\n    mocker.patch('tools.text.ai.gpt3', return_value='this is relevant because')\n\n    response = api_client.post(\n        f'/search/image/{image_upload.public_id}/questions',\n        json={\n            'query': 'help me',\n        }\n    )\n\n    question_id = response.json()['id']\n    hit = response.json()['hits'][0]\n\n    response = api_client.get(\n        f'/search/image/{image_upload.public_id}/questions/{question_id}/hits/{hit[\"id\"]}/relevance',\n    )\n\n    assert response.json()['relevance'] == 'this is relevant because'\n"}
{"type": "test_file", "path": "tests/feature/jobs/pipelines/analyse_lecture/test_clean_lecture.py", "content": "from db.models import Lecture, Analysis, Message\nfrom db.crud import save_message_for_analysis\nfrom jobs.tasks.lecture import clean_lecture\n\n\ndef test_clean_lecture_deletes_all_but_last_message_for_lecture():\n    lecture = Lecture(\n        public_id='some_id',\n        language=Lecture.Language.SWEDISH,\n    )\n    lecture.save()\n    analysis = Analysis(lecture_id=lecture.id, state=Analysis.State.READY)\n    analysis.save()\n\n    save_message_for_analysis(analysis, 'some title', 'msg 1')\n    save_message_for_analysis(analysis, 'some title', 'msg 2')\n    save_message_for_analysis(analysis, 'some title', 'msg 3')\n    save_message_for_analysis(analysis, 'some title', 'msg 4')\n\n    clean_lecture.job(lecture.public_id, lecture.language)\n\n    messages = Message.filter(Message.analysis_id == analysis.id)\n    assert len(messages) == 1\n"}
{"type": "test_file", "path": "tests/feature/jobs/pipelines/analyse_lecture/test_download_lecture.py", "content": "import shutil\nimport os\n\nfrom jobs.pipelines.analyse_lecture import download_lecture\nfrom tools.ffmpeg.progress import ProgressFFmpeg\nfrom db.models import Lecture, Analysis\n\n\ndef test_download_of_kth_play_lecture_saves_mp4_file(mocker, mp4_file):\n    def create_download(\n        download_url: str,\n        output_filename: str,\n        progress: ProgressFFmpeg,\n    ):\n        shutil.copy(mp4_file, output_filename)\n\n    mocker.patch('tools.web.crawler.get_m3u8', return_value='https://example.com/index.m3u8')\n    mocker.patch('ffmpeg.probe', return_value={'format': {'duration': 10}})\n    mocker.patch('tools.web.downloader.run_ffmpeg_cmd', side_effect=create_download)\n\n    lecture = Lecture(\n        public_id='some_id',\n        language=Lecture.Language.SWEDISH,\n        approved=True,\n        source=Lecture.Source.KTH,\n    )\n    lecture.save()\n    analysis = Analysis(lecture_id=lecture.id)\n    analysis.save()\n\n    download_lecture.job(lecture.public_id, lecture.language)\n    lecture.refresh()\n\n    assert lecture.get_last_analysis().mp4_progress == 100\n    assert lecture.mp4_filepath == lecture.mp4_filename()\n    assert os.path.exists(lecture.mp4_filename())\n"}
{"type": "test_file", "path": "tests/feature/jobs/pipelines/analyse_lecture/test_extract_audio.py", "content": "import shutil\nimport os\n\nfrom jobs.pipelines.analyse_lecture import extract_audio\nfrom db.models import Lecture, Analysis\n\n\ndef test_extract_audio_job_creates_mp3_from_mp4(mocker, mp4_file, mp3_file):\n    def create_mp3(src_file: str, lecture: Lecture):\n        shutil.copy(mp3_file, lecture.mp3_filename())\n        return lecture.mp3_filename()\n\n    mocker.patch('tools.audio.extraction.extract_mp3_from_mp4', side_effect=create_mp3)\n\n    lecture = Lecture(\n        public_id='some_id',\n        language=Lecture.Language.SWEDISH,\n        approved=True,\n        source=Lecture.Source.KTH,\n        mp4_filepath=mp4_file,\n    )\n    lecture.save()\n    analysis = Analysis(lecture_id=lecture.id)\n    analysis.save()\n\n    extract_audio.job(lecture.public_id, lecture.language)\n    lecture.refresh()\n\n    assert lecture.get_last_analysis().mp3_progress == 100\n    assert lecture.mp3_filepath == lecture.mp3_filename()\n    assert os.path.exists(lecture.mp3_filename())\n"}
{"type": "test_file", "path": "tests/feature/jobs/pipelines/analyse_lecture/test_transcribe_lecture.py", "content": "from typing import Optional\nimport os\n\nfrom jobs.pipelines.analyse_lecture import transcribe_audio\nfrom db.models import Lecture, Analysis\nfrom config.settings import settings\n\n\ndef test_transcription_job_runs_local_transcription_if_device_is_cuda(mocker, mp3_file):\n    def create_download(\n        mp3_file: str,\n        lecture: Lecture,\n        analysis: Analysis,\n        output_dir: Optional[str] = None,\n        save_progress: bool = True\n    ):\n        os.mkdir(output_dir)\n        filename = f'{lecture.transcript_dirname()}/{lecture.public_id}.mp3.txt'\n        with open(filename, 'w+') as file:\n            file.write('some spoken words...')\n\n    settings.WHISPER_TRANSCRIPTION_DEVICE = 'cuda'\n\n    mocker.patch('db.models.lecture.Lecture.reindex')\n    transcribe_locally = mocker.patch('tools.audio.transcription.transcribe_locally', side_effect=create_download)\n\n    lecture = Lecture(\n        public_id='some_id',\n        language=Lecture.Language.SWEDISH,\n        approved=True,\n        source=Lecture.Source.KTH,\n        # needs mp3_path\n        mp3_filepath=mp3_file,\n    )\n    lecture.save()\n    analysis = Analysis(lecture_id=lecture.id)\n    analysis.save()\n\n    transcribe_audio.job(lecture.public_id, lecture.language)\n\n    lecture.refresh()\n\n    assert lecture.get_last_analysis().transcript_progress == 100\n    assert lecture.transcript_filepath == lecture.transcript_dirname()\n    assert os.path.exists(lecture.transcript_dirname())\n    assert transcribe_locally.call_count == 1\n\n\ndef test_transcription_job_runs_local_transcription_if_device_is_cpu(mocker, mp3_file):\n    def create_download(\n        mp3_file: str,\n        lecture: Lecture,\n        analysis: Analysis,\n        output_dir: Optional[str] = None,\n        save_progress: bool = True\n    ):\n        os.mkdir(output_dir)\n        filename = f'{lecture.transcript_dirname()}/{lecture.public_id}.mp3.txt'\n        with open(filename, 'w+') as file:\n            file.write('some spoken words...')\n\n    settings.WHISPER_TRANSCRIPTION_DEVICE = 'cpu'\n\n    mocker.patch('db.models.lecture.Lecture.reindex')\n    transcribe_locally = mocker.patch('tools.audio.transcription.transcribe_locally', side_effect=create_download)\n\n    lecture = Lecture(\n        public_id='some_id',\n        language=Lecture.Language.SWEDISH,\n        approved=True,\n        source=Lecture.Source.KTH,\n        # needs mp3_path\n        mp3_filepath=mp3_file,\n    )\n    lecture.save()\n    analysis = Analysis(lecture_id=lecture.id)\n    analysis.save()\n\n    transcribe_audio.job(lecture.public_id, lecture.language)\n\n    lecture.refresh()\n\n    assert lecture.get_last_analysis().transcript_progress == 100\n    assert lecture.transcript_filepath == lecture.transcript_dirname()\n    assert os.path.exists(lecture.transcript_dirname())\n    assert transcribe_locally.call_count == 1\n\n\ndef test_transcription_job_runs_calls_openai_if_device_is_openai(mocker, mp3_file):\n    def create_download(\n        mp3_file: str,\n        lecture: Lecture,\n        analysis: Analysis,\n        output_dir: Optional[str] = None,\n        save_progress: bool = True\n    ):\n        os.mkdir(output_dir)\n        filename = f'{lecture.transcript_dirname()}/{lecture.public_id}.mp3.txt'\n        with open(filename, 'w+') as file:\n            file.write('some spoken words...')\n\n    settings.WHISPER_TRANSCRIPTION_DEVICE = 'openai'\n\n    mocker.patch('db.models.lecture.Lecture.reindex')\n    transcribe_openai = mocker.patch('tools.audio.transcription.transcribe_openai', side_effect=create_download)\n\n    lecture = Lecture(\n        public_id='some_id',\n        language=Lecture.Language.SWEDISH,\n        approved=True,\n        source=Lecture.Source.KTH,\n        # needs mp3_path\n        mp3_filepath=mp3_file,\n    )\n    lecture.save()\n    analysis = Analysis(lecture_id=lecture.id)\n    analysis.save()\n\n    transcribe_audio.job(lecture.public_id, lecture.language)\n\n    lecture.refresh()\n\n    assert lecture.get_last_analysis().transcript_progress == 100\n    assert lecture.transcript_filepath == lecture.transcript_dirname()\n    assert os.path.exists(lecture.transcript_dirname())\n    assert transcribe_openai.call_count == 1\n"}
{"type": "test_file", "path": "tests/feature/jobs/pipelines/parse_image_upload/test_classify_subjects.py", "content": "from jobs.pipelines.parse_image_upload import (\n    classify_subjects\n)\n\n\ndef test_classify_subjects_job_creates_imageupload_subjects(mocker, make_mocked_classifier, image_upload):\n    labels = ['foo', 'bar']\n    mocker.patch(\n        'classifiers.SubjectMultipassClassifier.create_classifier_for',\n        return_value=make_mocked_classifier(labels)\n    )\n\n    classify_subjects.job(image_upload.public_id)\n    image_upload.refresh()\n\n    assert len(image_upload.subjects_list()) == 2\n    for label in labels:\n        assert label in image_upload.subjects_list()\n\n\ndef test_classify_subjects_saves_progress(mocker, make_mocked_classifier, image_upload):\n    labels = ['foo', 'bar']\n    mocker.patch(\n        'classifiers.SubjectMultipassClassifier.create_classifier_for',\n        return_value=make_mocked_classifier(labels)\n    )\n\n    classify_subjects.job(image_upload.public_id)\n    image_upload.refresh()\n\n    assert image_upload.classify_subjects_ok is True\n\n\ndef test_create_title_job_saves_failure_reason_on_error(mocker, make_mocked_classifier, image_upload):\n    labels = ['foo', 'bar']\n    err = ValueError('bang!')\n    mocker.patch(\n        'classifiers.SubjectMultipassClassifier.create_classifier_for',\n        return_value=make_mocked_classifier(labels, err)\n    )\n\n    try:\n        classify_subjects.job(image_upload.public_id)\n    except Exception:\n        pass\n\n    image_upload.refresh()\n\n    assert image_upload.classify_subjects_ok is False\n    assert image_upload.classify_subjects_failure_reason == 'bang!'\n"}
{"type": "test_file", "path": "tests/feature/jobs/pipelines/parse_image_upload/test_create_title.py", "content": "from typing import Optional\nfrom rq import Queue\n\nfrom jobs.pipelines.parse_image_upload import (\n    create_title\n)\n\n\ndef test_create_title_job_saves_title(mocker, image_upload):\n    title = 'a cool assignment'\n\n    mocker.patch('tools.text.ai.gpt3', return_value=title)\n\n    create_title.job(image_upload.public_id)\n    image_upload.refresh()\n\n    assert image_upload.title == title\n    assert image_upload.create_title_ok is True\n\n\ndef test_create_title_job_saves_failure_reason_on_error(mocker, image_upload):\n    def some_err(\n        prompt: str,\n        time_to_live: int = 60,\n        max_retries: int = 3,\n        retry_interval: list = [10, 30, 60],\n        queue_approval: Queue = None,\n        analysis_id: Optional[int] = None,\n        query_id: Optional[int] = None,\n        upload_id: Optional[int] = None,\n    ):\n        raise ValueError('wooooups')\n\n    mocker.patch('tools.text.ai.gpt3', side_effect=some_err)\n\n    try:\n        create_title.job(image_upload.public_id)\n    except Exception:\n        pass\n\n    image_upload.refresh()\n\n    assert image_upload.create_title_ok is False\n    assert image_upload.create_title_failure_reason == 'wooooups'\n"}
{"type": "test_file", "path": "tests/feature/jobs/pipelines/parse_image_upload/test_create_description.py", "content": "from typing import Optional\nfrom rq import Queue\n\nfrom jobs.pipelines.parse_image_upload import (\n    create_description\n)\n\n\ndef test_create_description_saves_english_description(mocker, image_upload):\n    description = 'this is a math assignment :)'\n\n    mocker.patch('tools.text.ai.gpt3', return_value=description)\n\n    create_description.job(image_upload.public_id, image_upload.Language.ENGLISH)\n    image_upload.refresh()\n\n    assert image_upload.description_en == description\n    assert image_upload.create_description_en_ok is True\n\n\ndef test_create_description_saves_swedish_description(mocker, image_upload):\n    description = 'skriv ett utryck för volymen av en köttbulle'\n\n    mocker.patch('tools.text.ai.gpt3', return_value=description)\n\n    create_description.job(image_upload.public_id, image_upload.Language.SWEDISH)\n    image_upload.refresh()\n\n    assert image_upload.description_sv == description\n    assert image_upload.create_description_sv_ok is True\n\n\ndef test_create_description_saves_error_on_english_error(mocker, image_upload):\n    def some_err(\n        prompt: str,\n        time_to_live: int = 60,\n        max_retries: int = 3,\n        retry_interval: list = [10, 30, 60],\n        queue_approval: Queue = None,\n        analysis_id: Optional[int] = None,\n        query_id: Optional[int] = None,\n        upload_id: Optional[int] = None,\n    ):\n        raise ValueError('wooooups')\n\n    mocker.patch('tools.text.ai.gpt3', side_effect=some_err)\n\n    try:\n        create_description.job(image_upload.public_id, image_upload.Language.ENGLISH)\n    except Exception:\n        pass\n\n    image_upload.refresh()\n\n    assert image_upload.create_description_en_ok is False\n    assert image_upload.create_description_en_failure_reason == 'wooooups'\n\n\ndef test_create_description_saves_error_on_swedish_error(mocker, image_upload):\n    def some_err(\n        prompt: str,\n        time_to_live: int = 60,\n        max_retries: int = 3,\n        retry_interval: list = [10, 30, 60],\n        queue_approval: Queue = None,\n        analysis_id: Optional[int] = None,\n        query_id: Optional[int] = None,\n        upload_id: Optional[int] = None,\n    ):\n        raise ValueError('wooooups')\n\n    mocker.patch('tools.text.ai.gpt3', side_effect=some_err)\n\n    try:\n        create_description.job(image_upload.public_id, image_upload.Language.SWEDISH)\n    except Exception:\n        pass\n\n    image_upload.refresh()\n\n    assert image_upload.create_description_sv_ok is False\n    assert image_upload.create_description_sv_failure_reason == 'wooooups'\n"}
{"type": "test_file", "path": "tests/feature/jobs/pipelines/parse_image_upload/test_create_search_queries.py", "content": "from typing import Optional\nfrom rq import Queue\n\nfrom jobs.pipelines.parse_image_upload import (\n    create_search_queries\n)\n\n\ndef test_create_search_queries_can_save_english_queries(mocker, image_upload):\n    response = '''\nenglish query 1\nenglish query 2\n'''.strip()\n\n    mocker.patch('tools.text.ai.gpt3', return_value=response)\n\n    create_search_queries.job(image_upload.public_id, image_upload.Language.ENGLISH)\n    image_upload.refresh()\n\n    assert image_upload.get_search_queries_en() == [\n        'english query 1',\n        'english query 2',\n    ]\n    assert image_upload.create_search_queries_en_ok is True\n\n\ndef test_create_search_queries_saves_error_on_failure_to_create_english_queries(mocker, image_upload):\n    def some_err(\n        prompt: str,\n        time_to_live: int = 60,\n        max_retries: int = 3,\n        retry_interval: list = [10, 30, 60],\n        queue_approval: Queue = None,\n        analysis_id: Optional[int] = None,\n        query_id: Optional[int] = None,\n        upload_id: Optional[int] = None,\n    ):\n        raise ValueError('wooooupsie')\n\n    mocker.patch('tools.text.ai.gpt3', side_effect=some_err)\n\n    try:\n        create_search_queries.job(image_upload.public_id, image_upload.Language.ENGLISH)\n    except Exception:\n        pass\n\n    image_upload.refresh()\n\n    assert image_upload.create_search_queries_en_ok is False\n    assert image_upload.create_search_queries_en_failure_reason == 'wooooupsie'\n\n\ndef test_create_search_queries_can_save_swedish_queries(mocker, image_upload):\n    response = '''\nsvenskt query 1\nsvenskt query 2\n'''.strip()\n\n    mocker.patch('tools.text.ai.gpt3', return_value=response)\n\n    create_search_queries.job(image_upload.public_id, image_upload.Language.SWEDISH)\n    image_upload.refresh()\n\n    assert image_upload.get_search_queries_sv() == [\n        'svenskt query 1',\n        'svenskt query 2',\n    ]\n    assert image_upload.create_search_queries_sv_ok is True\n\n\ndef test_create_search_queries_saves_error_on_failure_to_create_swedish_queries(mocker, image_upload):\n    def some_err(\n        prompt: str,\n        time_to_live: int = 60,\n        max_retries: int = 3,\n        retry_interval: list = [10, 30, 60],\n        queue_approval: Queue = None,\n        analysis_id: Optional[int] = None,\n        query_id: Optional[int] = None,\n        upload_id: Optional[int] = None,\n    ):\n        raise ValueError('wooooupsie doopsie')\n\n    mocker.patch('tools.text.ai.gpt3', side_effect=some_err)\n\n    try:\n        create_search_queries.job(image_upload.public_id, image_upload.Language.SWEDISH)\n    except Exception:\n        pass\n\n    image_upload.refresh()\n\n    assert image_upload.create_search_queries_sv_ok is False\n    assert image_upload.create_search_queries_sv_failure_reason == 'wooooupsie doopsie'\n"}
{"type": "test_file", "path": "tests/feature/jobs/pipelines/parse_image_upload/test_parse_image_content.py", "content": "from unittest.mock import call\nfrom decimal import Decimal\nimport json\n\nfrom jobs.pipelines.parse_image_upload import (\n    parse_image_content\n)\nfrom tests.conftest import (\n    TEST_API_ENDPOINT,\n    TEST_MATHPIX_APP_ID,\n    TEST_MATHPIX_APP_KEY\n)\nfrom tools.img.ocr import (\n    MATHPIX_PARAMS\n)\n\n\ndef test_parse_image_job_sends_request_to_mathpix(mocker, image_upload):\n    mock_response = mocker.Mock()\n    mock_response.status_code = 200\n    mock_response.json.return_value = {\n        'request_id': 'some_id',\n        'version': 'RSK-M111p1',\n        'image_width': 100,\n        'image_height': 200,\n        'is_printed': True,\n        'is_handwritten': False,\n        'auto_rotate_confidence': 0,\n        'auto_rotate_degrees': 0,\n        'confidence': 0.99951171875,\n        'confidence_rate': 0.9999023246709239,\n        'text': 'E=mc^2'\n    }\n    requests_mock = mocker.patch('requests.post', return_value=mock_response)\n\n    parse_image_content.job(image_upload.public_id)\n\n    image_upload.refresh()\n\n    assert image_upload.text_content == 'E=mc^2'\n    assert image_upload.parse_image_content_ok is True\n\n    # Request should include reachable link to the image\n    assert requests_mock.call_count == 1\n    mathpix_data = MATHPIX_PARAMS\n    mathpix_data['src'] = f'{TEST_API_ENDPOINT}assignments/image/{image_upload.public_id}/img'\n    assert requests_mock.mock_calls[0] == call(\n        url='https://api.mathpix.com/v3/text',\n        headers={\n            'App_id': TEST_MATHPIX_APP_ID,\n            'App_key': TEST_MATHPIX_APP_KEY,\n            'Content-Type': 'application/json',\n        },\n        data=json.dumps(mathpix_data)\n    )\n\n\ndef test_parse_image_job_saves_mathpix_metadata(mocker, image_upload):\n    mock_response = mocker.Mock()\n    mock_response.status_code = 200\n    mock_response.json.return_value = {\n        'request_id': 'some_id',\n        'version': 'RSK-M111p1',\n        'image_width': 100,\n        'image_height': 200,\n        'is_printed': True,\n        'is_handwritten': False,\n        'auto_rotate_confidence': 0,\n        'auto_rotate_degrees': 0,\n        'confidence': 0.99951171875,\n        'confidence_rate': 0.9999023246709239,\n        'text': 'E=mc^2'\n    }\n    mocker.patch('requests.post', return_value=mock_response)\n\n    parse_image_content.job(image_upload.public_id)\n    image_upload.refresh()\n\n    requests = image_upload.mathpix_requests()\n\n    assert len(requests) == 1\n    assert requests[0].request_id == 'some_id'\n    assert requests[0].version == 'RSK-M111p1'\n    assert requests[0].image_width == 100\n    assert requests[0].image_height == 200\n    assert requests[0].is_printed is True\n    assert requests[0].is_handwritten is False\n    assert requests[0].confidence == Decimal('0.99951171875')\n    assert requests[0].confidence_rate == Decimal('0.9999023246709239')\n\n\ndef test_parse_image_job_saves_error_on_mathpix_error(mocker, image_upload):\n    mock_response = mocker.Mock()\n    mock_response.status_code = 200  # for some reason actually does respond with a 200 when there is an error\n    mock_response.json.return_value = {\n        'request_id': 'some_id',\n        'version': 'RSK-M112',\n        'error': 'Cannot read image',\n        'error_info': {\n            'id': 'image_decode_error',\n            'message': 'Cannot read image'\n        }\n    }\n    mocker.patch('requests.post', return_value=mock_response)\n\n    try:\n        parse_image_content.job(image_upload.public_id)\n    except Exception:\n        pass\n\n    image_upload.refresh()\n\n    assert image_upload.parse_image_content_ok is False\n    assert image_upload.parse_image_content_failure_reason == 'Cannot read image'\n"}
{"type": "test_file", "path": "tests/feature/jobs/tasks/lecture/test_capture_preview.py", "content": "import shutil\n\nfrom jobs.tasks.lecture import capture_preview\nfrom db.models import Lecture\n\n\ndef test_download_of_kth_play_lecture_saves_mp4_file(mocker, mp4_file, img_file):\n    def save_photo_from_video(mp4_filepath: str, output_file: str, small=False):\n        shutil.copy(img_file, output_file)\n\n    func = mocker.patch('tools.video.img.save_photo_from_video', side_effect=save_photo_from_video)\n\n    lecture = Lecture(\n        public_id='some_id',\n        language=Lecture.Language.SWEDISH,\n        mp4_filepath=mp4_file,\n        approved=True,\n        source=Lecture.Source.KTH,\n    )\n    lecture.save()\n\n    capture_preview.job(lecture.public_id, lecture.language)\n    lecture.refresh()\n\n    assert lecture.img_preview == lecture.preview_filename()\n    assert lecture.img_preview_small == lecture.preview_small_filename()\n    assert func.call_count == 2\n"}
{"type": "test_file", "path": "tests/feature/jobs/tasks/lecture/test_classify_video.py", "content": "from typing import Optional\nimport tempfile\nimport shutil\nimport os\n\nfrom jobs.tasks.lecture import classify_video\nfrom db.models import Lecture, Analysis\n\n\ndef test_classification_can_approve_video(mocker, mp4_file, mp3_file):\n    tf = tempfile.NamedTemporaryFile(mode='w+', delete=False)\n    shutil.copy(mp3_file, tf.name)\n\n    def save_text(mp3_file: str, lecture: Lecture, output_dir: Optional[str] = None, save_progress: bool = True):\n        os.mkdir(output_dir)\n        filename = os.path.basename(mp3_file)\n        with open(f'{output_dir}/{filename}.txt', 'w+') as file:\n            file.write('One small step for man, one giant leap for mankind.')\n\n    classification = 'Recorded lecture'\n\n    mocker.patch('tools.youtube.download.download_mp3', return_value=tf.name)\n    mocker.patch('tools.audio.transcription.save_text', side_effect=save_text)\n    mocker.patch('tools.text.ai.gpt3', return_value=classification)\n    schedule_analysis_of_lecture_mock = mocker.patch('jobs.schedule_analysis_of_lecture')\n\n    lecture = Lecture(\n        public_id='some_id',\n        language=Lecture.Language.SWEDISH,\n        mp4_filepath=mp4_file,\n        source=Lecture.Source.YOUTUBE,\n    )\n    lecture.save()\n    analysis = Analysis(lecture_id=lecture.id)\n    analysis.save()\n\n    # Classify the video\n    classify_video.job(lecture.public_id, lecture.language)\n    lecture.refresh()\n\n    assert lecture.approved is True\n    assert schedule_analysis_of_lecture_mock.call_count == 1\n\n    os.unlink(tf.name)\n\n\ndef test_classification_can_deny_video(mocker, mp4_file, mp3_file):\n    tf = tempfile.NamedTemporaryFile(mode='w+', delete=False)\n    shutil.copy(mp3_file, tf.name)\n\n    def save_text(mp3_file: str, lecture: Lecture, output_dir: Optional[str] = None, save_progress: bool = True):\n        os.mkdir(output_dir)\n        filename = os.path.basename(mp3_file)\n        with open(f'{output_dir}/{filename}.txt', 'w+') as file:\n            file.write('One small step for man, one giant leap for mankind.')\n\n    classification = 'Some other category'\n\n    mocker.patch('tools.youtube.download.download_mp3', return_value=tf.name)\n    mocker.patch('tools.audio.transcription.save_text', side_effect=save_text)\n    mocker.patch('tools.text.ai.gpt3', return_value=classification)\n    schedule_analysis_of_lecture_mock = mocker.patch('jobs.schedule_analysis_of_lecture')\n    schedule_fetch_of_lecture_metadata_mock = mocker.patch('jobs.schedule_fetch_of_lecture_metadata')\n\n    lecture = Lecture(\n        public_id='some_id',\n        language=Lecture.Language.SWEDISH,\n        mp4_filepath=mp4_file,\n        source=Lecture.Source.YOUTUBE,\n    )\n    lecture.save()\n    analysis = Analysis(lecture_id=lecture.id)\n    analysis.save()\n\n    # Classify the video\n    classify_video.job(lecture.public_id, lecture.language)\n    lecture.refresh()\n\n    assert lecture.approved is False\n    assert schedule_analysis_of_lecture_mock.call_count == 0\n\n    # Even though the video got denied, we want to make sure it's easy to see\n    # which video that was\n    assert schedule_fetch_of_lecture_metadata_mock.call_count == 1\n\n    os.unlink(tf.name)\n"}
{"type": "test_file", "path": "tests/feature/jobs/tasks/lecture/test_fetch_metadata.py", "content": "from datetime import datetime\nimport pytest\nimport pytz\n\nfrom jobs.tasks.lecture import fetch_metadata\nfrom db.models import Lecture\n\n\ndef test_kth_video_title_is_fetched(mocker, mp4_file):\n    mocker.patch('tools.web.crawler.scrape_title_from_page', return_value='some title')\n    mocker.patch('tools.web.crawler.scrape_posted_date_from_kthplay')\n\n    lecture = Lecture(\n        public_id='some_id',\n        language=Lecture.Language.SWEDISH,\n        mp4_filepath=mp4_file,\n        approved=True,\n        source=Lecture.Source.KTH,\n    )\n    lecture.save()\n\n    fetch_metadata.job(lecture.public_id, lecture.language)\n    lecture.refresh()\n\n    assert lecture.title == 'some title'\n\n\ndef test_kth_video_date_is_fetched(mocker, mp4_file):\n    now = datetime.now()\n    tz = pytz.timezone('UTC')\n\n    mocker.patch('tools.web.crawler.scrape_title_from_page')\n    mocker.patch('tools.web.crawler.scrape_posted_date_from_kthplay', return_value=now)\n\n    lecture = Lecture(\n        public_id='some_id',\n        language=Lecture.Language.SWEDISH,\n        mp4_filepath=mp4_file,\n        approved=True,\n        source=Lecture.Source.KTH,\n    )\n    lecture.save()\n\n    fetch_metadata.job(lecture.public_id, lecture.language)\n    lecture.refresh()\n\n    lecture_date = tz.localize(lecture.date, is_dst=None)\n    mocked_date = tz.localize(now, is_dst=None)\n\n    assert lecture_date.isoformat(timespec='seconds') == mocked_date.isoformat(timespec='seconds')\n\n\ndef test_youtube_video_title_is_fetched(mocker, mp4_file):\n    mocker.patch('tools.web.crawler.scrape_title_from_page', return_value='some title')\n    mocker.patch('tools.youtube.metadata.get_upload_date')\n    mocker.patch('tools.youtube.metadata.get_channel_name')\n\n    lecture = Lecture(\n        public_id='some_id',\n        language=Lecture.Language.SWEDISH,\n        mp4_filepath=mp4_file,\n        approved=True,\n        source=Lecture.Source.YOUTUBE,\n    )\n    lecture.save()\n\n    fetch_metadata.job(lecture.public_id, lecture.language)\n    lecture.refresh()\n\n    assert lecture.title == 'some title'\n\n\ndef test_youtube_video_date_is_fetched(mocker, mp4_file):\n    now = datetime.now()\n    tz = pytz.timezone('UTC')\n\n    mocker.patch('tools.youtube.metadata.get_upload_date', return_value=now)\n    mocker.patch('tools.web.crawler.scrape_title_from_page')\n    mocker.patch('tools.youtube.metadata.get_channel_name')\n\n    lecture = Lecture(\n        public_id='some_id',\n        language=Lecture.Language.SWEDISH,\n        mp4_filepath=mp4_file,\n        approved=True,\n        source=Lecture.Source.YOUTUBE,\n    )\n    lecture.save()\n\n    fetch_metadata.job(lecture.public_id, lecture.language)\n    lecture.refresh()\n\n    lecture_date = tz.localize(lecture.date, is_dst=None)\n    mocked_date = tz.localize(now, is_dst=None)\n\n    assert lecture_date.isoformat(timespec='seconds') == mocked_date.isoformat(timespec='seconds')\n\n\ndef test_youtube_video_group_is_fetched(mocker, mp4_file):\n    mocker.patch('tools.youtube.metadata.get_channel_name', return_value='some_channel')\n    mocker.patch('tools.web.crawler.scrape_title_from_page')\n    mocker.patch('tools.youtube.metadata.get_upload_date')\n\n    lecture = Lecture(\n        public_id='some_id',\n        language=Lecture.Language.SWEDISH,\n        mp4_filepath=mp4_file,\n        approved=True,\n        source=Lecture.Source.YOUTUBE,\n    )\n    lecture.save()\n\n    fetch_metadata.job(lecture.public_id, lecture.language)\n    lecture.refresh()\n\n    assert lecture.group == 'some_channel'\n\n\ndef test_kth_raw_video_is_untouched(mp4_file):\n    lecture = Lecture(\n        public_id='some_id',\n        language=Lecture.Language.SWEDISH,\n        mp4_filepath=mp4_file,\n        approved=True,\n        source=Lecture.Source.KTH_RAW,\n        date=None,  # set to none here because is defaulted to now\n    )\n    lecture.save()\n\n    fetch_metadata.job(lecture.public_id, lecture.language)\n    lecture.refresh()\n\n    assert lecture.date is None\n    assert lecture.title is None\n\n\ndef test_job_raises_exception_if_source_type_is_unknown(mp4_file):\n    lecture = Lecture(\n        public_id='some_id',\n        language=Lecture.Language.SWEDISH,\n        mp4_filepath=mp4_file,\n        approved=True,\n        source='something_bad',\n    )\n    lecture.save()\n\n    with pytest.raises(ValueError, match='unknown source something_bad'):\n        fetch_metadata.job(lecture.public_id, lecture.language)\n"}
{"type": "test_file", "path": "tests/feature/jobs/tasks/lecture/test_lecture_classify_subjects.py", "content": "import pytest\n\nfrom jobs.tasks.lecture import classify_subjects\n\n\ndef test_classification_can_approve_video(mocker, make_mocked_classifier, analysed_lecture):\n    labels = ['some label', 'some other label']\n    mocker.patch(\n        'classifiers.SubjectMultipassClassifier.create_classifier_for',\n        return_value=make_mocked_classifier(labels)\n    )\n\n    # Classify the video\n    classify_subjects.job(analysed_lecture.public_id, analysed_lecture.language)\n    analysed_lecture.refresh()\n\n    assert len(analysed_lecture.subjects_list()) == 2\n    for label in labels:\n        assert label in analysed_lecture.subjects_list()\n\n\ndef test_lecture_cannot_be_classified_without_description(analysed_lecture):\n    analysed_lecture.description = None\n    analysed_lecture.save()\n\n    with pytest.raises(\n        ValueError,\n        match='cannot classify the subjects of a lecture without description'\n    ):\n        classify_subjects.job(\n            analysed_lecture.public_id,\n            analysed_lecture.language\n        )\n"}
{"type": "test_file", "path": "tests/feature/jobs/tasks/lecture/test_lecture_create_description.py", "content": "from jobs.tasks.lecture import create_description\n\n\ndef test_create_description_job_saves_description(mocker, analysed_lecture):\n    description = 'a lecture about the meaning of life'\n    mocker.patch('tools.text.ai.gpt3', return_value=description)\n\n    # Classify the video\n    create_description.job(analysed_lecture.public_id, analysed_lecture.language)\n    analysed_lecture.refresh()\n\n    assert analysed_lecture.description == description\n"}
{"type": "test_file", "path": "tests/unit/classifiers/test_subject.py", "content": "from unittest.mock import call\n\nfrom classifiers import SubjectClassifier\n\nTEXT_STRING = '''\nsome text mentioning a bunch of math jargon\n\nThe eigenvector corresponding to the maximum eigenvalue of the covariance matrix\nfacilitates dimensionality reduction in principal component analysis,\neffectively projecting the data onto a lower-dimensional subspace.\nMeanwhile, the Riemann zeta function, analytically continued to\nthe complex plane, yields nontrivial zeros that provide\ninsights into the distribution of prime numbers\nvia the Prime Number Theorem.\n'''\n\n\ndef test_subject_classifier_can_be_created():\n    classifier_1 = SubjectClassifier.create_classifier_for('lecture')\n    classifier_2 = SubjectClassifier.create_classifier_for('assignment')\n\n    assert classifier_1.what == 'lecture'\n    assert classifier_2.what == 'assignment'\n\n\ndef test_classification_can_be_make(mocker):\n    gpt_response = '''\nMathematics\nEngineering Sciences\n'''\n    gpt3_mock = mocker.patch('tools.text.ai.gpt3', return_value=gpt_response)\n\n    classifier = SubjectClassifier.create_classifier_for('lecture')\n\n    result = classifier.classify(TEXT_STRING)\n\n    assert result == ['Mathematics', 'Engineering Sciences']\n    assert gpt3_mock.call_count == 1\n\n\ndef test_classification_can_have_priority(mocker):\n    gpt_response = '''\nMathematics\nEngineering Sciences\n'''\n    gpt3_mock = mocker.patch('tools.text.ai.gpt3', return_value=gpt_response)\n\n    priority = SubjectClassifier.Priority.DEFAULT\n    classifier = SubjectClassifier.create_classifier_for('lecture', priority=priority)\n    classifier.classify(TEXT_STRING)\n\n    assert gpt3_mock.mock_calls[0] == call(\n        classifier.create_prompt(TEXT_STRING, 3),\n        time_to_live=60 * 2,\n        max_retries=4,\n        retry_interval=[\n            5,\n            15,\n            30,\n            60,\n        ],\n        upload_id=None,\n        analysis_id=None,\n    )\n\n    priority = SubjectClassifier.Priority.HIGH\n    classifier = SubjectClassifier.create_classifier_for('lecture', priority=priority)\n    classifier.classify(TEXT_STRING)\n\n    assert gpt3_mock.mock_calls[1] == call(\n        classifier.create_prompt(TEXT_STRING, 3),\n        time_to_live=60,\n        max_retries=5,\n        retry_interval=[\n            5,\n            10,\n            10,\n            10,\n            10,\n        ],\n        upload_id=None,\n        analysis_id=None,\n    )\n\n    priority = SubjectClassifier.Priority.LOW\n    classifier = SubjectClassifier.create_classifier_for('lecture', priority=priority)\n    classifier.classify(TEXT_STRING)\n\n    assert gpt3_mock.mock_calls[2] == call(\n        classifier.create_prompt(TEXT_STRING, 3),\n        time_to_live=60 * 60 * 10,\n        max_retries=10,\n        retry_interval=[\n            10,\n            30,\n            60,\n            2 * 60,\n            2 * 60,\n            10 * 60,\n            30 * 60,\n            2 * 60 * 60,\n            30 * 60,\n            30 * 60,\n        ],\n        upload_id=None,\n        analysis_id=None,\n    )\n\n\ndef test_classification_result_only_contains_labels(mocker):\n    gpt_response_with_bogus_classes = '''\nMathematics\nFoo\nBar\nEngineering Sciences\n'''\n\n    mocker.patch('tools.text.ai.gpt3', return_value=gpt_response_with_bogus_classes)\n\n    classifier = SubjectClassifier.create_classifier_for('lecture')\n\n    result = classifier.classify(TEXT_STRING)\n\n    assert result == ['Mathematics', 'Engineering Sciences']\n\n\ndef test_classifier_can_ignore_messy_gpt_output(mocker):\n    gpt_response = '''\n- Mathematics\n*Engineering Sciences\n-Communication\n(Computer Science)\n'''\n\n    mocker.patch('tools.text.ai.gpt3', return_value=gpt_response)\n\n    classifier = SubjectClassifier.create_classifier_for('lecture')\n\n    result = classifier.classify(TEXT_STRING)\n\n    assert result == ['Mathematics', 'Engineering Sciences', 'Communication', 'Computer Science']\n\n\ndef test_classifier_can_handle_classes_on_one_line(mocker):\n    gpt_response = 'Mathematics, Engineering Sciences'\n\n    mocker.patch('tools.text.ai.gpt3', return_value=gpt_response)\n\n    classifier = SubjectClassifier.create_classifier_for('lecture')\n\n    result = classifier.classify(TEXT_STRING)\n\n    assert result == ['Engineering Sciences', 'Mathematics']\n\n\ndef test_classifier_can_handle_classes_irrespective_of_casing(mocker):\n    gpt_response = '''\nmathematics\nENGINEERING ScienCES\nCommunication\n'''\n\n    mocker.patch('tools.text.ai.gpt3', return_value=gpt_response)\n\n    classifier = SubjectClassifier.create_classifier_for('lecture')\n\n    result = classifier.classify(TEXT_STRING)\n\n    assert result == ['Mathematics', 'Engineering Sciences', 'Communication']\n\n\ndef test_classification_can_have_upload_attached(mocker, image_upload):\n    gpt3_mock = mocker.patch('tools.text.ai.gpt3')\n\n    classifier = SubjectClassifier.create_classifier_for('lecture', upload=image_upload)\n    classifier.classify(TEXT_STRING)\n\n    assert gpt3_mock.mock_calls[0] == call(\n        classifier.create_prompt(TEXT_STRING, 3),\n        time_to_live=60 * 2,\n        max_retries=4,\n        retry_interval=[\n            5,\n            15,\n            30,\n            60,\n        ],\n        upload_id=image_upload.id,\n        analysis_id=None,\n    )\n\n\ndef test_classification_can_have_analysis_attached(mocker, analysed_lecture):\n    gpt3_mock = mocker.patch('tools.text.ai.gpt3')\n\n    analysis = analysed_lecture.get_last_analysis()\n\n    classifier = SubjectClassifier.create_classifier_for('lecture', analysis=analysis)\n    classifier.classify(TEXT_STRING)\n\n    assert gpt3_mock.mock_calls[0] == call(\n        classifier.create_prompt(TEXT_STRING, 3),\n        time_to_live=60 * 2,\n        max_retries=4,\n        retry_interval=[\n            5,\n            15,\n            30,\n            60,\n        ],\n        upload_id=None,\n        analysis_id=analysis.id,\n    )\n\n\ndef test_classifier_does_not_include_a_label_twice(mocker):\n    gpt_response = '''\nComputer Science\nSoftware Engineering and Security\nComputer Science\nTheoretical Computer Science\n'''\n\n    mocker.patch('tools.text.ai.gpt3', return_value=gpt_response)\n\n    classifier = SubjectClassifier.create_classifier_for('lecture')\n\n    result = classifier.classify(TEXT_STRING)\n\n    assert result == ['Computer Science', 'Software Engineering and Security', 'Theoretical Computer Science']\n\n\ndef test_labels_can_be_overriden(mocker):\n    new_labels = ['Foo', 'Bar', 'Baz']\n    gpt_response = '''\nMathematics\nFoo\nbar\nEngineering Sciences\n'''\n    mocker.patch('tools.text.ai.gpt3', return_value=gpt_response)\n\n    classifier = SubjectClassifier.create_classifier_for('lecture')\n    classifier.override_labels(new_labels)\n    result = classifier.classify(TEXT_STRING)\n\n    assert result == ['Foo', 'Bar']\n"}
{"type": "test_file", "path": "tests/unit/classifiers/test_subject_multipass.py", "content": "from unittest.mock import call\n\nfrom classifiers import SubjectMultipassClassifier\n\nTEXT_STRING = '''\nsome text mentioning a bunch of math jargon\n\nThe eigenvector corresponding to the maximum eigenvalue of the covariance matrix\nfacilitates dimensionality reduction in principal component analysis,\neffectively projecting the data onto a lower-dimensional subspace.\nMeanwhile, the Riemann zeta function, analytically continued to\nthe complex plane, yields nontrivial zeros that provide\ninsights into the distribution of prime numbers\nvia the Prime Number Theorem.\n'''\n\n\ndef test_subject_multipass_classifier_can_be_created():\n    classifier_1 = SubjectMultipassClassifier.create_classifier_for('lecture')\n    classifier_2 = SubjectMultipassClassifier.create_classifier_for('assignment')\n\n    assert classifier_1.what == 'lecture'\n    assert classifier_2.what == 'assignment'\n\n\ndef test_classification_use_subject_classifier(mocker):\n    mocker.patch('tools.text.ai.gpt3')\n\n    return_1 = ['Mathematics', 'Engineering Sciences']\n    subject_mock = mocker.patch('classifiers.subject.SubjectClassifier.classify', return_value=return_1)\n\n    classifier = SubjectMultipassClassifier.create_classifier_for('lecture')\n\n    result = classifier.classify(TEXT_STRING)\n\n    assert result == ['Mathematics', 'Engineering Sciences']\n    assert subject_mock.call_count == 1\n\n\ndef test_classification_can_sample_multiple_times(mocker):\n    mocker.patch('tools.text.ai.gpt3')\n\n    subject_mock = mocker.patch('classifiers.subject.SubjectClassifier.classify', return_value=['foo'])\n    classifier = SubjectMultipassClassifier.create_classifier_for(\n        'lecture',\n        samples=10\n    )\n\n    classifier.classify(TEXT_STRING)\n\n    assert subject_mock.call_count == 10 + 1  # one extra for the validation pass\n\n\ndef test_validation_pass_is_used_with_sampled_subjects(mocker):\n    return_1 = ['foo', 'bar']\n    return_2 = ['foo', 'baz']\n    return_3 = ['bat']\n    return_4 = ['foo']  # return of the final pass\n\n    mocker.patch('classifiers.subject.SubjectClassifier.classify', side_effect=[\n        return_1,\n        return_2,\n        return_3,\n        return_4,\n    ])\n    override_labels_mock = mocker.patch('classifiers.subject.SubjectClassifier.override_labels')\n\n    classifier = SubjectMultipassClassifier.create_classifier_for(\n        'lecture',\n        samples=3\n    )\n\n    result = classifier.classify(TEXT_STRING)\n\n    assert override_labels_mock.call_count == 1\n    assert override_labels_mock.mock_calls[0] == call(\n        ['foo', 'bar', 'baz', 'bat']\n    )\n    assert result == ['foo']\n\n\ndef test_last_pass_is_using_a_custom_validation_prompt(mocker):\n    classify_mock = mocker.patch('classifiers.subject.SubjectClassifier.classify', return_value=['foo'])\n\n    classifier = SubjectMultipassClassifier.create_classifier_for(\n        'lecture',\n        samples=3\n    )\n\n    classifier.classify(TEXT_STRING)\n\n    assert classify_mock.call_count == 4\n    assert list(classify_mock.mock_calls)[-1] == call(\n        TEXT_STRING,\n        0,\n        validation_prompt=True,\n    )\n\n\ndef test_last_pass_will_be_rerun_if_result_is_empty(mocker):\n    return_1 = ['foo']\n    return_2 = ['foo']\n    return_3 = ['foo']\n\n    # final passes\n    return_4 = []\n    return_5 = ['foo']\n    classify_mock = mocker.patch('classifiers.subject.SubjectClassifier.classify', side_effect=[\n        return_1,\n        return_2,\n        return_3,\n        return_4,\n        return_5,\n    ])\n\n    classifier = SubjectMultipassClassifier.create_classifier_for(\n        'lecture',\n        samples=3\n    )\n\n    classifier.classify(TEXT_STRING)\n\n    assert classify_mock.call_count == 5\n"}
{"type": "test_file", "path": "tests/unit/db/models/test_image_question.py", "content": "\nfrom db.crud import get_image_question_by_public_id\nfrom db.models import ImageQuestion, ImageQuestionHit\nimport tools.text.prompts as prompts\nimport tools.text.ai\n\n\ndef test_image_question_can_be_saved(image_upload):\n    public_id = ImageQuestion.make_public_id()\n\n    question = ImageQuestion(\n        public_id=public_id,\n        image_upload_id=image_upload.id,\n        query_string='help me',\n    )\n    question.save()\n\n    saved = get_image_question_by_public_id(public_id)\n    assert question.id == saved.id\n\n\ndef test_image_question_hit_can_be_saved(image_question, analysed_lecture):\n    hit = ImageQuestionHit(\n        public_id=ImageQuestionHit.make_public_id(),\n        image_question_id=image_question.id,\n        lecture_id=analysed_lecture.id,\n    )\n    hit.save()\n\n    assert hit.answer is None\n    assert hit.relevance is None\n    assert hit.cache_is_valid is True\n\n    # Test accessor from ImageQuestion\n    assert len(image_question.hits()) == 1\n    assert image_question.hits()[0].id == hit.id\n\n\ndef test_image_question_hit_can_compute_answer(mocker, image_question, analysed_lecture):\n    answer = 'some ai wizardry'\n    mocker.patch('tools.text.ai.gpt3', return_value=answer)\n\n    hit = ImageQuestionHit(\n        public_id=ImageQuestionHit.make_public_id(),\n        image_question_id=image_question.id,\n        lecture_id=analysed_lecture.id,\n    )\n    hit.save()\n\n    hit.create_answer(tools.text.ai, prompts)\n    hit.save()\n\n    assert hit.answer == answer\n\n\ndef test_image_question_hit_answer_is_cached(mocker, image_question, analysed_lecture):\n    answer = 'some ai wizardry'\n    gpt3_mock = mocker.patch('tools.text.ai.gpt3', return_value=answer)\n\n    hit = ImageQuestionHit(\n        public_id=ImageQuestionHit.make_public_id(),\n        image_question_id=image_question.id,\n        lecture_id=analysed_lecture.id,\n    )\n    hit.save()\n\n    def func():\n        hit.create_answer(tools.text.ai, prompts)\n        hit.save()\n\n    # call the function a few times\n    func()\n    func()\n    func()\n\n    assert hit.answer == answer\n    assert gpt3_mock.call_count == 1\n\n\ndef test_image_question_hit_answer_can_be_expired(mocker, image_question, analysed_lecture):\n    answer = 'some ai wizardry'\n    gpt3_mock = mocker.patch('tools.text.ai.gpt3', return_value=answer)\n\n    hit = ImageQuestionHit(\n        public_id=ImageQuestionHit.make_public_id(),\n        image_question_id=image_question.id,\n        lecture_id=analysed_lecture.id,\n    )\n    hit.save()\n\n    def func():\n        hit.create_answer(tools.text.ai, prompts)\n        hit.save()\n\n    func()\n    func()\n    func()\n\n    assert hit.answer == answer\n    assert gpt3_mock.call_count == 1\n\n    hit.cache_is_valid = False\n    hit.save()\n    func()\n    func()\n\n    assert hit.answer == answer\n    assert gpt3_mock.call_count == 2\n\n\ndef test_image_question_hit_can_compute_relevance(mocker, image_question, analysed_lecture):\n    relevance = 'the ai did wizardry'\n    mocker.patch('tools.text.ai.gpt3', return_value=relevance)\n\n    hit = ImageQuestionHit(\n        public_id=ImageQuestionHit.make_public_id(),\n        image_question_id=image_question.id,\n        lecture_id=analysed_lecture.id,\n    )\n    hit.save()\n\n    hit.create_relevance(tools.text.ai, prompts)\n    hit.save()\n\n    assert hit.relevance == relevance\n\n\ndef test_image_question_hit_relevance_is_cached(mocker, image_question, analysed_lecture):\n    relevance = 'the ai did wizardry'\n    gpt3_mock = mocker.patch('tools.text.ai.gpt3', return_value=relevance)\n\n    hit = ImageQuestionHit(\n        public_id=ImageQuestionHit.make_public_id(),\n        image_question_id=image_question.id,\n        lecture_id=analysed_lecture.id,\n    )\n    hit.save()\n\n    def func():\n        hit.create_relevance(tools.text.ai, prompts)\n        hit.save()\n\n    # call the function a few times\n    func()\n    func()\n    func()\n\n    assert hit.relevance == relevance\n    assert gpt3_mock.call_count == 1\n\n\ndef test_image_question_hit_relevance_can_be_expired(mocker, image_question, analysed_lecture):\n    relevance = 'the ai did wizardry'\n    gpt3_mock = mocker.patch('tools.text.ai.gpt3', return_value=relevance)\n\n    hit = ImageQuestionHit(\n        public_id=ImageQuestionHit.make_public_id(),\n        image_question_id=image_question.id,\n        lecture_id=analysed_lecture.id,\n    )\n    hit.save()\n\n    def func():\n        hit.create_relevance(tools.text.ai, prompts)\n        hit.save()\n\n    func()\n    func()\n    func()\n\n    assert hit.relevance == relevance\n    assert gpt3_mock.call_count == 1\n\n    hit.cache_is_valid = False\n    hit.save()\n    func()\n    func()\n\n    assert hit.relevance == relevance\n    assert gpt3_mock.call_count == 2\n"}
{"type": "test_file", "path": "tests/unit/db/test_crud.py", "content": "import db.crud as crud\nimport db.models as models\n\n\ndef test_get_all_courses_includes_courses():\n    course1 = models.Course(\n        course_code='SF1626',\n        swedish_name='Flervarre',\n        english_name='Multivariable Calculus',\n        points='7.5 hp',\n        cycle='First cycle',\n    )\n    course1.save()\n    course2 = models.Course(\n        course_code='SF2972',\n        swedish_name='Spelteori',\n        english_name='Game Theory',\n        points='7.5 hp',\n        cycle='First cycle',\n    )\n    course2.save()\n\n    courses = crud.get_all_courses()\n\n    assert len(courses) == 2\n\n    for c in [course1, course2]:\n        found = False\n        for course in courses:\n            if course.course_code == c.course_code:\n                found = True\n\n        assert found\n\n\ndef test_get_all_courses_includes_course_groups():\n    group1 = models.CourseGroup(\n        course_code='SF19XY',\n        swedish_name='Sannolikhetsteori och statistik',\n        english_name='Probability Theory and Statistics',\n        points='7.5 hp',\n        cycle='First cycle',\n    )\n    group1.save()\n\n    course1 = models.Course(\n        course_code='SF1912',\n        group_id=group1.id,\n        swedish_name='Sannolikhetsteori och statistik',\n        english_name='Probability Theory and Statistics',\n        points='7.5 hp',\n        cycle='First cycle',\n    )\n    course1.save()\n    course2 = models.Course(\n        course_code='SF1924',\n        group_id=group1.id,\n        swedish_name='Sannolikhetsteori och statistik',\n        english_name='Probability Theory and Statistics',\n        points='7.5 hp',\n        cycle='First cycle',\n    )\n    course2.save()\n    course3 = models.Course(\n        course_code='DD2350',\n        swedish_name='Algoritmer, datastrukturer och komplexitet',\n        english_name='Algorithms, Data Structures and Complexity',\n        points='9.5 hp',\n        cycle='First cycle',\n    )\n    course3.save()\n\n    courses = crud.get_all_courses()\n\n    assert len(courses) == 2\n\n    for c in [course1, course2]:\n        found = False\n        for course in courses:\n            if course.course_code == c.course_code:\n                found = True\n\n        # response should only include the group not it's courses\n        assert not found\n\n    for c in [group1, course3]:\n        found = False\n        for course in courses:\n            if course.course_code == c.course_code:\n                found = True\n\n        # response should contain both group and course without group\n        assert found\n"}
{"type": "test_file", "path": "tests/unit/index/test_lecture.py", "content": "import index.lecture as lecture_index\n\n\ndef test_search_in_course_performs_bool_query(mocker):\n    client = mocker.patch('index.client.search', return_value={\n        'hits': {\n            'hits': [\n                {\n                    'fields': {\n                        'title': ['Some title']\n                    }\n                }\n            ]\n        }\n    })\n\n    response = lecture_index.search_in_course('Some title', 'XX1337')\n\n    assert response[0]['title'] == 'Some title'\n\n    args = client.mock_calls[0][2]\n    assert 'bool' in args['body']['query']\n\n\ndef test_search_in_transcript_returns_highlighted_matches(mocker):\n    client = mocker.patch('index.client.search', return_value={\n        'hits': {\n            'hits': [\n                {\n                    'highlight': {\n                        'transcript': [\n                            '00:00 -> 00:30 foo',\n                            '01:00 -> 01:20 <strong>match</strong>',\n                        ]\n                    },\n                    'fields': {\n                        'title': ['Some title']\n                    }\n                }\n            ]\n        }\n    })\n\n    response = lecture_index.search_in_transcripts_and_titles('Some query string')\n\n    assert response[0]['title'] == 'Some title'\n    assert response[0]['highlight']['transcript'][0] == '00:00 -> 00:30 foo'\n\n    args = client.mock_calls[0][2]\n    assert 'highlight' in args['body']\n"}
{"type": "test_file", "path": "tests/unit/tools/video/test_img.py", "content": "from PIL import Image\nimport tempfile\nimport os\n\nfrom tools.video.img import save_photo_from_video\n\n\ndef test_photo_can_be_extracted_from_video(mp4_file):\n    img_filename = tempfile.mkdtemp() + '.png'\n\n    save_photo_from_video(mp4_file, img_filename)\n\n    assert os.path.exists(img_filename)\n\n    os.unlink(img_filename)\n\n\ndef test_low_res_photo_can_be_extracted_from_mp4(mp4_file):\n    img_filename = tempfile.mkdtemp() + '.png'\n\n    save_photo_from_video(mp4_file, img_filename, small=True)\n\n    img = Image.open(img_filename)\n    width, _ = img.size\n    assert os.path.exists(img_filename)\n    assert width == 180\n\n    os.unlink(img_filename)\n"}
{"type": "source_file", "path": "api/routers/assignments.py", "content": "from fastapi import Depends, APIRouter, HTTPException, UploadFile\nfrom fastapi.responses import FileResponse\nfrom typing import List, Optional\nfrom pydantic import BaseModel\nimport os\n\nfrom tools.files.paths import get_sha_of_binary_file_descriptor\nfrom db.models import ImageUpload\nfrom config.logger import log\nfrom db import get_db\nfrom db.crud import (\n    get_random_image_upload_by_subject,\n    get_image_upload_by_image_sha,\n    get_image_upload_by_public_id,\n)\nimport jobs\n\nMAX_NUMBER_IMAGE_HITS = 10\n\nrouter = APIRouter()\n\n\nclass ImageCreationOutputModel(BaseModel):\n    id: str\n\n\nclass ImageOutputModel(BaseModel):\n    id: str\n    created_at: str\n    modified_at: str\n    text_content: Optional[str]\n    title: Optional[str]\n    description_en: Optional[str]\n    description_sv: Optional[str]\n    subjects: List[str]\n    can_ask_question: bool\n    parse_image_upload_complete: bool\n    parse_image_content_ok: Optional[bool]\n    create_title_ok: Optional[bool]\n    create_description_en_ok: Optional[bool]\n    create_description_sv_ok: Optional[bool]\n    create_search_queries_en_ok: Optional[bool]\n    create_search_queries_sv_ok: Optional[bool]\n    classify_subjects_ok: Optional[bool]\n\n\n@router.post('/assignments/image', dependencies=[Depends(get_db)])\ndef search_image(file: UploadFile) -> ImageCreationOutputModel:\n    _, extension = os.path.splitext(file.filename)\n    extension = extension.replace('.', '')\n    extension = extension.lower()\n\n    allowed_extensions = [\n        'jpg',\n        'jpeg',\n        'jji',\n        'jpe',\n        'jif',\n        'jfif',\n        'heif',\n        'heic',\n        'png',\n        'gif',\n        'webp',\n        'tiff',\n        'tif',\n    ]\n\n    if extension not in allowed_extensions:\n        raise HTTPException(status_code=400, detail='Invalid image format, please provide an image in one of the following formats ' + ', '.join(allowed_extensions))  # noqa: E501\n\n    sha = get_sha_of_binary_file_descriptor(file.file)\n    image = get_image_upload_by_image_sha(sha)\n    should_start_parse_image_upload_pipeline = False\n\n    if image is None:\n        should_start_parse_image_upload_pipeline = True\n        image = ImageUpload(\n            public_id=ImageUpload.make_public_id(),\n            file_format=extension,\n        )\n        image.save()\n\n        image.save_image_data(file)\n\n    if not image.parse_image_content_ok:\n        should_start_parse_image_upload_pipeline = True\n\n    if not image.create_description_en_ok:\n        should_start_parse_image_upload_pipeline = True\n\n    if not image.create_title_ok:\n        should_start_parse_image_upload_pipeline = True\n\n    if not image.create_description_sv_ok:\n        should_start_parse_image_upload_pipeline = True\n\n    if not image.create_search_queries_en_ok:\n        should_start_parse_image_upload_pipeline = True\n\n    if not image.create_search_queries_sv_ok:\n        should_start_parse_image_upload_pipeline = True\n\n    if not image.classify_subjects_ok:\n        should_start_parse_image_upload_pipeline = True\n\n    if should_start_parse_image_upload_pipeline:\n        try:\n            jobs.schedule_parse_image_upload(image)\n        except jobs.ImageUploadQueueDailyLimitException as e:\n            log().error(e)\n            raise HTTPException(status_code=500, detail='Daily limit allowed by the system has been exceeded')  # noqa: E501\n\n    return {\n        'id': image.public_id,\n    }\n\n\n@router.get('/assignments/image/{public_id}', dependencies=[Depends(get_db)])\ndef get_image_info(public_id: str) -> ImageOutputModel:\n    upload = get_image_upload_by_public_id(public_id)\n\n    if upload is None:\n        raise HTTPException(status_code=404)\n\n    return upload.to_dict()\n\n\n@router.post('/assignments/image/{public_id}', dependencies=[Depends(get_db)])\ndef update_image_upload(\n    public_id: str,\n    restart: Optional[bool] = None,\n) -> ImageOutputModel:\n    upload = get_image_upload_by_public_id(public_id)\n\n    if upload is None:\n        raise HTTPException(status_code=404)\n\n    if restart and not upload.parse_image_upload_complete():\n        upload.clear_parse_results()\n        upload.save()\n        try:\n            jobs.schedule_parse_image_upload(upload)\n        except jobs.ImageUploadQueueDailyLimitException as e:\n            log().error(e)\n            raise HTTPException(status_code=500, detail='Daily limit allowed by the system has been exceeded')  # noqa: E501\n\n    return upload.to_dict()\n\n\n@router.get('/assignments/image/{public_id}/img', dependencies=[Depends(get_db)])\ndef get_image_data(public_id: str) -> FileResponse:\n    upload = get_image_upload_by_public_id(public_id)\n\n    if upload is None:\n        raise HTTPException(status_code=404)\n\n    return FileResponse(upload.get_filename())\n\n\n@router.get('/assignments/image/random/{subject}', dependencies=[Depends(get_db)])\ndef get_random_assignment_with_subject(subject: str) -> ImageOutputModel:\n    upload = get_random_image_upload_by_subject(subject)\n\n    if upload is None:\n        raise HTTPException(status_code=404)\n\n    return upload.to_dict()\n"}
{"type": "source_file", "path": "api/routers/courses.py", "content": "from fastapi import Depends, APIRouter, HTTPException\nfrom pydantic import BaseModel\n\nfrom db import get_db\nfrom db.crud import (\n    find_course_code,\n)\n\nrouter = APIRouter()\n\n\nclass CourseOutputModel(BaseModel):\n    course_code: str\n    display_name: str\n\n\n@router.get('/courses/{course_code}', dependencies=[Depends(get_db)])\ndef get_course(course_code: str) -> CourseOutputModel:\n    if course_code == 'no_course':\n        return {\n            'course_code': 'no_course',\n            'display_name': 'Untagged Lectures',\n        }\n\n    course = find_course_code(course_code)\n    if course is None:\n        raise HTTPException(status_code=404)\n\n    return course.to_small_dict()\n"}
{"type": "source_file", "path": "api/__init__.py", "content": "from fastapi.middleware.cors import CORSMiddleware\nfrom datetime import datetime, timedelta\nfrom logging.config import dictConfig\nfrom fastapi import FastAPI\n\nfrom config.logger import LogConfig, log\nfrom jobs import get_monitoring_queue\nfrom config.settings import settings\nimport jobs.tasks.save_queue_info\nfrom api.routers import (\n    index,\n    urls,\n    lectures,\n    courses,\n    query,\n    search,\n    stats,\n    assignments,\n)\n\n\ndef get_app():\n    app = FastAPI(title=settings.NAME, openapi_url=None)\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[str(origin) for origin in settings.BACKEND_CORS_ORIGINS],\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n    app.include_router(index.router)\n    app.include_router(urls.router)\n    app.include_router(lectures.router)\n    app.include_router(courses.router)\n    app.include_router(query.router)\n    app.include_router(search.router)\n    app.include_router(stats.router)\n    app.include_router(assignments.router)\n\n    return app\n\n\ndef main():\n    dictConfig(LogConfig().dict())\n    log().info('starting api')\n\n    app = get_app()\n\n    # Reset the monitoring queue and restart the workers\n    queue = next(get_monitoring_queue())\n    future = datetime.utcnow() + timedelta(days=1)\n    queue.scheduled_job_registry.remove_jobs(timestamp=future.timestamp())\n    jobs.tasks.save_queue_info.job()\n\n    return app\n"}
{"type": "source_file", "path": "api/routers/index.py", "content": "from fastapi import APIRouter\n\nrouter = APIRouter()\n\n\n@router.get('/')\nasync def index():\n    return {\n        'message': 'I am the API.'\n    }\n"}
{"type": "source_file", "path": "api/routers/lectures.py", "content": "from fastapi import Depends, APIRouter, HTTPException, Response\nfrom typing import Optional, Union, List\nfrom pydantic import BaseModel\nimport random as rand\n\nfrom db.models import Analysis\nfrom db import get_db\nfrom db.crud import (\n    create_relation_between_lecture_and_course_group,\n    find_relation_between_lecture_and_course_group,\n    create_relation_between_lecture_and_course,\n    find_relation_between_lecture_and_course,\n    get_lecture_by_public_id_and_language,\n    delete_lecture_course_relation,\n    get_all_denied_lectures,\n    get_all_failed_lectures,\n    get_unfinished_lectures,\n    find_course_code,\n    get_all_lectures,\n)\n\nrouter = APIRouter()\n\n\nclass MessageOutputModel(BaseModel):\n    timestamp: str\n    title: str\n    body: Union[str, None] = None\n\n\nclass AnalysisOutputModel(BaseModel):\n    analysis_id: int\n    created_at: str\n    modified_at: str\n    state: str\n    frozen: bool\n    last_message: Union[MessageOutputModel, None] = None\n    mp4_progress: int\n    mp3_progress: int\n    transcript_progress: int\n    summary_progress: int\n    overall_progress: int\n\n\nclass CourseOutputModel(BaseModel):\n    course_code: str\n    display_name: str\n\n\nclass HighlightsOutputModel(BaseModel):\n    transcript: Optional[List[str]]\n    title: Optional[List[str]]\n\n\nclass LectureOutputModel(BaseModel):\n    public_id: str\n    language: str\n    created_at: str\n    title: Union[str, None] = None\n    group: Union[str, None] = None\n    date: Union[str, None] = None\n    approved: Union[bool, None] = None\n    source: str\n    words: int\n    length: int\n    description: Union[str, None] = None\n    subjects: List[str]\n    preview_uri: Union[str, None] = None\n    preview_small_uri: Union[str, None] = None\n    transcript_uri: Union[str, None] = None\n    summary_uri: Union[str, None] = None\n    content_link: Union[str, None] = None\n    analysis: Union[AnalysisOutputModel, None] = None\n    courses: List[CourseOutputModel]\n    courses_can_change: bool\n\n\nclass LectureSummaryOutputModel(BaseModel):\n    public_id: str\n    language: str\n    source: str\n    created_at: Union[str, None] = None\n    title: Union[str, None] = None\n    group: Union[str, None] = None\n    date: Union[str, None] = None\n    state: Union[str, None] = None\n    frozen: Union[bool, None] = None\n    content_link: Union[str, None] = None\n    preview_uri: Union[str, None] = None\n    preview_small_uri: Union[str, None] = None\n    overall_progress: Union[int, None] = None\n    highlight: Union[HighlightsOutputModel, None] = None\n\n\n@router.get('/lectures', dependencies=[Depends(get_db)])\ndef get_all(\n    summary: Union[bool, None] = None,\n    only_unfinished: Union[bool, None] = None,\n    only_denied: Union[bool, None] = None,\n    only_failed: Union[bool, None] = None,\n    include_denied: Union[bool, None] = False,\n    include_failed: Union[bool, None] = False,\n    random: Union[bool, None] = None,\n) -> List[Union[LectureOutputModel, LectureSummaryOutputModel]]:\n    if only_unfinished:\n        lectures = get_unfinished_lectures()\n    elif only_denied:\n        include_denied = True\n        lectures = get_all_denied_lectures()\n    elif only_failed:\n        include_failed = True\n        lectures = get_all_failed_lectures()\n    else:\n        lectures = get_all_lectures()\n\n    out = []\n    for lecture in lectures:\n        has_analysis = lecture.get_last_analysis() is not None\n        if random:\n            if not has_analysis:\n                continue\n            if lecture.get_last_analysis().state != Analysis.State.READY:\n                continue\n\n        if not include_denied:\n            if not has_analysis:\n                continue\n            if lecture.get_last_analysis().state == Analysis.State.DENIED:\n                continue\n\n        if not include_failed:\n            if not has_analysis:\n                continue\n            if lecture.get_last_analysis().state == Analysis.State.FAILURE:\n                continue\n\n        if summary:\n            out.append(lecture.to_summary_dict())\n        else:\n            out.append(lecture.to_dict())\n\n    if random and len(out) == 0:\n        return []\n\n    if random:\n        index = rand.randint(0, len(out) - 1)\n        return [out[index]]\n\n    if summary:\n        out.reverse()\n\n    return out\n\n\n@router.get('/lectures/{public_id}/{language}', dependencies=[Depends(get_db)])\ndef get_lecture(public_id: str, language: str):\n    lecture = get_lecture_by_public_id_and_language(public_id, language)\n    if lecture is None:\n        raise HTTPException(status_code=404)\n\n    return lecture.to_dict()\n\n\n@router.get('/lectures/{public_id}/{language}/preview', dependencies=[Depends(get_db)])\ndef get_preview(public_id: str, language: str):\n    lecture = get_lecture_by_public_id_and_language(public_id, language)\n    if lecture is None:\n        raise HTTPException(status_code=404)\n\n    if lecture.img_preview is None:\n        raise HTTPException(status_code=404)\n\n    with open(lecture.img_preview, 'rb') as file:\n        image_bytes: bytes = file.read()\n        return Response(content=image_bytes, media_type='image/png')\n\n\n@router.get('/lectures/{public_id}/{language}/preview-small', dependencies=[Depends(get_db)])\ndef get_small_preview(public_id: str, language: str):\n    lecture = get_lecture_by_public_id_and_language(public_id, language)\n    if lecture is None:\n        raise HTTPException(status_code=404)\n\n    if lecture.img_preview_small is None:\n        raise HTTPException(status_code=404)\n\n    with open(lecture.img_preview_small, 'rb') as file:\n        image_bytes: bytes = file.read()\n        return Response(content=image_bytes, media_type='image/png')\n\n\n@router.get('/lectures/{public_id}/{language}/transcript', dependencies=[Depends(get_db)])\ndef get_transcript(public_id: str, language: str):\n    lecture = get_lecture_by_public_id_and_language(public_id, language)\n    if lecture is None:\n        raise HTTPException(status_code=404)\n\n    if lecture.transcript_filepath is None:\n        raise HTTPException(status_code=404)\n\n    return Response(\n        content=lecture.transcript_text(),\n        media_type='text/plain'\n    )\n\n\n@router.get('/lectures/{public_id}/{language}/summary', dependencies=[Depends(get_db)])\ndef get_summary(public_id: str, language: str):\n    lecture = get_lecture_by_public_id_and_language(public_id, language)\n    if lecture is None:\n        raise HTTPException(status_code=404)\n\n    if lecture.summary_filepath is None:\n        raise HTTPException(status_code=404)\n\n    return Response(\n        content=lecture.summary_text(),\n        media_type='text/plain'\n    )\n\n\nclass PostCourseInputModel(BaseModel):\n    course_code: str\n\n\n@router.post('/lectures/{public_id}/{language}/course', dependencies=[Depends(get_db)])\ndef add_course_to_lecture(public_id: str, language: str, input_data: PostCourseInputModel) -> LectureOutputModel:\n    lecture = get_lecture_by_public_id_and_language(public_id, language)\n    if lecture is None:\n        raise HTTPException(status_code=404)\n\n    if not lecture.courses_can_be_changed():\n        raise HTTPException(status_code=400, detail='A lectures courses can only be changed within the first day of being added')  # noqa: E501\n\n    code = input_data.course_code\n\n    course = find_course_code(code)\n    if course is None:\n        raise HTTPException(status_code=400, detail='invalid course code')\n\n    if lecture.has_course(code):\n        return lecture.to_dict()\n\n    limit = 10\n    if len(lecture.courses()) >= limit:\n        raise HTTPException(status_code=400, detail=f'Too many courses. A lecture cannot have more than {limit} courses.')\n\n    if course.source == course.Source.COURSE_GROUP:\n        create_relation_between_lecture_and_course_group(lecture.id, course.course_group_id)\n    elif course.source == course.Source.COURSE:\n        create_relation_between_lecture_and_course(lecture.id, course.course_id)\n\n    lecture.reindex()\n    course.reindex()\n\n    return lecture.to_dict()\n\n\n@router.delete('/lectures/{public_id}/{language}/course/{course_code}', dependencies=[Depends(get_db)])\ndef remove_course_to_lecture(public_id: str, language: str, course_code: str) -> LectureOutputModel:\n    lecture = get_lecture_by_public_id_and_language(public_id, language)\n    if lecture is None:\n        raise HTTPException(status_code=404)\n\n    if not lecture.courses_can_be_changed():\n        raise HTTPException(status_code=400, detail='A lectures courses can only be changed within the first day of being added')  # noqa: E501\n\n    course = find_course_code(course_code)\n    if course is None:\n        raise HTTPException(status_code=400, detail='invalid course code')\n\n    if not lecture.has_course(course_code):\n        raise HTTPException(status_code=404)\n\n    if course.source == course.Source.COURSE_GROUP:\n        relation = find_relation_between_lecture_and_course_group(lecture.id, course.course_group_id)\n    elif course.source == course.Source.COURSE:\n        relation = find_relation_between_lecture_and_course(lecture.id, course.course_id)\n\n    delete_lecture_course_relation(relation.id)\n\n    lecture.reindex()\n    course.reindex()\n\n    return lecture.to_dict()\n"}
{"type": "source_file", "path": "api/routers/query.py", "content": "from typing import Union\nfrom fastapi import Depends, APIRouter, HTTPException\nfrom pydantic import BaseModel\n\nfrom db.crud import (\n    get_lecture_by_public_id_and_language,\n    get_most_recent_lecture_query_by_sha,\n    get_most_recent_image_query_by_sha,\n    get_image_upload_by_public_id,\n    create_lecture_query,\n    create_image_query,\n)\nimport tools.text.prompts as prompts\nfrom config.logger import log\nfrom db.models import Query\nimport tools.text.ai as ai\nfrom db import get_db\n\n\nrouter = APIRouter()\n\n\nclass LectureInputModel(BaseModel):\n    lecture_id: str\n    language: str\n    query_string: str\n    override_cache: Union[bool, None] = None\n\n\nclass ImageInputModel(BaseModel):\n    image_id: str\n    query_string: str\n    override_cache: Union[bool, None] = None\n\n\nclass OutputModel(BaseModel):\n    response: str\n    cached: bool\n\n\n@router.post('/query/lecture', dependencies=[Depends(get_db)])\ndef new_lecture_query(input_data: LectureInputModel) -> OutputModel:\n    lecture = get_lecture_by_public_id_and_language(\n        input_data.lecture_id,\n        input_data.language\n    )\n\n    if lecture is None:\n        raise HTTPException(status_code=404)\n\n    query = get_most_recent_lecture_query_by_sha(lecture, Query.make_sha(input_data.query_string))\n\n    cached = True\n    should_create_new_query = False\n\n    if query is None:\n        should_create_new_query = True\n\n    elif query.response is None:\n        should_create_new_query = True\n\n    elif input_data.override_cache:\n        should_create_new_query = True\n\n    elif query.cache_is_valid is False:\n        should_create_new_query = True\n\n    if should_create_new_query:\n        cached = False\n        query = create_lecture_query(lecture, input_data.query_string)\n\n        if lecture.language == lecture.Language.ENGLISH:\n            prompt = prompts.create_query_text_english(query, lecture)\n        elif lecture.language == lecture.Language.SWEDISH:\n            prompt = prompts.create_query_text_swedish(query, lecture)\n        else:\n            raise ValueError(f'language {lecture.language} is not supported')\n\n        try:\n            response = ai.gpt3(\n                prompt,\n                time_to_live=60,\n                max_retries=2,\n                retry_interval=[10, 20],\n                query_id=query.id,\n            )\n        except ai.GPTException as e:\n            log().error(e)\n            raise HTTPException(\n                status_code=500,\n                detail=str(e)\n            )\n        except Exception as e:\n            log().error(e)\n            raise HTTPException(\n                status_code=500,\n                detail='Something went wrong when running GPT-3 query'\n            )\n\n        query.response = response\n        query.save()\n\n    query.count += 1\n    query.save()\n\n    return {\n        'response': query.response,\n        'cached': cached,\n    }\n\n\n@router.post('/query/image', dependencies=[Depends(get_db)])\ndef new_image_query(input_data: ImageInputModel) -> OutputModel:\n    upload = get_image_upload_by_public_id(\n        input_data.image_id,\n    )\n\n    if upload is None:\n        raise HTTPException(status_code=404)\n\n    query = get_most_recent_image_query_by_sha(upload, Query.make_sha(input_data.query_string))\n\n    cached = True\n    should_create_new_query = False\n\n    if query is None:\n        should_create_new_query = True\n\n    elif query.response is None:\n        should_create_new_query = True\n\n    elif input_data.override_cache:\n        should_create_new_query = True\n\n    elif query.cache_is_valid is False:\n        should_create_new_query = True\n\n    if should_create_new_query:\n        cached = False\n        query = create_image_query(upload, input_data.query_string)\n\n        prompt = prompts.create_query_text_for_image(query, upload)\n\n        try:\n            response = ai.gpt3(\n                prompt,\n                time_to_live=60,\n                max_retries=2,\n                retry_interval=[10, 20],\n                query_id=query.id,\n            )\n        except ai.GPTException as e:\n            log().error(e)\n            raise HTTPException(\n                status_code=500,\n                detail=str(e)\n            )\n        except Exception as e:\n            log().error(e)\n            raise HTTPException(\n                status_code=500,\n                detail='Something went wrong when running GPT-3 query'\n            )\n\n        query.response = response\n        query.save()\n\n    query.count += 1\n    query.save()\n\n    return {\n        'response': query.response,\n        'cached': cached,\n    }\n"}
{"type": "source_file", "path": "api/routers/search.py", "content": "from fastapi import Depends, APIRouter, HTTPException\nfrom typing import List, Optional\nfrom pydantic import BaseModel\n\nfrom api.routers.lectures import LectureSummaryOutputModel\nfrom db.models import ImageQuestion, ImageQuestionHit\nimport index.courses as courses_index\nimport tools.text.prompts as prompts\nfrom db.models import Lecture\nfrom config.logger import log\nfrom db import get_db\nimport index.lecture\nimport tools.text.ai\nfrom db.crud import (\n    get_most_recent_question_hit_by_lecture_and_question,\n    get_most_recent_image_question_by_sha,\n    get_image_question_hit_by_public_id,\n    get_image_upload_by_public_id,\n    get_lecture_by_id,\n)\n\nMAX_NUMBER_IMAGE_HITS = 10\n\nrouter = APIRouter()\n\n\nclass InputModelSearchCourse(BaseModel):\n    query: str\n    limit: Optional[int] = None\n\n\nclass InputModelSearchCourseCode(BaseModel):\n    query: str\n    source: Optional[str]\n    group: Optional[str]\n\n\nclass CourseOutputModel(BaseModel):\n    course_code: str\n    display_name: str\n    lectures: Optional[int] = None\n\n\nclass InputModelImageQuestion(BaseModel):\n    query: str\n\n\nclass HitOutputModel(BaseModel):\n    id: str\n    answer: Optional[str] = None\n    relevance: Optional[str] = None\n    lecture: Optional[LectureSummaryOutputModel]\n\n\nclass ImageQuestionOutputModel(BaseModel):\n    id: str\n    hits: List[HitOutputModel]\n\n\nclass LectureAnswerOutputModel(BaseModel):\n    answer: str\n\n\nclass LectureRelevanceOutputModel(BaseModel):\n    relevance: str\n\n\n@router.post('/search/course', dependencies=[Depends(get_db)])\ndef search_course(\n    input_data: InputModelSearchCourse,\n    include_lectures: Optional[bool] = None,\n    lecture_count_above_or_equal: Optional[int] = 0,\n) -> List[CourseOutputModel]:\n\n    sort_by_lecture_count = False\n    unlimited = False\n    if lecture_count_above_or_equal >= 1:\n        unlimited = True\n        sort_by_lecture_count = True\n\n    if not unlimited:\n        if input_data.limit is None:\n            raise HTTPException(status_code=400, detail='Please specify a limit to the search result')\n        if input_data.limit > 40:\n            raise HTTPException(status_code=400, detail='Cannot search for that many courses')\n\n    apply_filter = True\n    if input_data.query == '':\n        if not unlimited:\n            return []\n\n        apply_filter = False\n\n    response = courses_index.wildcard_search(\n        input_data.query,\n        include_lectures=include_lectures,\n        lecture_count_above_or_equal=lecture_count_above_or_equal,\n        unlimited=unlimited,\n        sort_by_lecture_count=sort_by_lecture_count,\n        apply_filter=apply_filter,\n    )\n\n    return response\n\n\n@router.post('/search/course/{course_code}', dependencies=[Depends(get_db)])\ndef search_course_lectures(\n    course_code: str,\n    input_data: InputModelSearchCourseCode,\n) -> List[LectureSummaryOutputModel]:\n\n    apply_filter = True\n    if input_data.query == '':\n        apply_filter = False\n\n    if course_code == 'no_course':\n        response = index.lecture.search_in_course(\n            input_data.query,\n            no_course=True,\n            apply_filter=apply_filter,\n            source=input_data.source,\n            group=input_data.group,\n        )\n        # Include kth_raw in kth searches\n        if input_data.source == Lecture.Source.KTH:\n            kth_raw_response = index.lecture.search_in_course(\n                input_data.query,\n                no_course=True,\n                apply_filter=apply_filter,\n                source=Lecture.Source.KTH_RAW,\n                group=input_data.group,\n            )\n            response = response + kth_raw_response\n        return response\n\n    response = index.lecture.search_in_course(\n        input_data.query,\n        course_code,\n        apply_filter=apply_filter,\n        source=input_data.source,\n        group=input_data.group,\n    )\n    # Include kth_raw in kth searches\n    if input_data.source == Lecture.Source.KTH:\n        kth_raw_response = index.lecture.search_in_course(\n            input_data.query,\n            course_code,\n            apply_filter=apply_filter,\n            source=Lecture.Source.KTH_RAW,\n            group=input_data.group,\n        )\n        response = response + kth_raw_response\n\n    return response\n\n\n@router.post('/search/lecture', dependencies=[Depends(get_db)])\ndef search_lecture(input_data: InputModelSearchCourseCode) -> List[LectureSummaryOutputModel]:\n    response = index.lecture.search_in_transcripts_and_titles(\n        input_data.query,\n    )\n\n    return response\n\n\n@router.post('/search/image/{image_public_id}/questions', dependencies=[Depends(get_db)])\ndef create_image_question(\n    image_public_id: str,\n    input_data: InputModelImageQuestion\n) -> ImageQuestionOutputModel:\n    upload = get_image_upload_by_public_id(image_public_id)\n\n    if upload is None:\n        raise HTTPException(status_code=404)\n\n    if not upload.parse_image_upload_complete():\n        raise HTTPException(status_code=409, detail='image has not finished parsing so question cannot be created, try again later.')  # noqa: E501\n\n    sha = ImageQuestion.make_sha(input_data.query)\n    question = get_most_recent_image_question_by_sha(upload, sha)\n\n    if question is None:\n        question = ImageQuestion(\n            public_id=ImageQuestion.make_public_id(),\n            image_upload_id=upload.id,\n            query_string=input_data.query,\n            query_hash=sha,\n        )\n        question.save()\n\n    docs = {}\n\n    def add_hits_to_docs(hits: list):\n        for hit in hits:\n            if hit['_id'] not in docs:\n                docs[hit['_id']] = []\n            docs[hit['_id']].append(hit['_score'])\n\n    for query in upload.get_search_queries_sv():\n        log().info(f'searching for (sv): {query}')\n        hits = index.lecture.search_in_transcripts_and_titles(query, include_id=True, include_score=True)\n        add_hits_to_docs(hits)\n\n    for query in upload.get_search_queries_en():\n        log().info(f'searching for (en): {query}')\n        hits = index.lecture.search_in_transcripts_and_titles(query, include_id=True, include_score=True)\n        add_hits_to_docs(hits)\n\n    total = {}\n    for doc in docs:\n        total[doc] = sum(docs[doc])\n\n    sorted_docs = sorted(total.items(), key=lambda x: x[1], reverse=True)\n    lectures = []\n    for (id, _) in sorted_docs:\n        if len(lectures) >= MAX_NUMBER_IMAGE_HITS:\n            break\n\n        lecture = get_lecture_by_id(id)\n        lectures.append(lecture)\n\n    hits = []\n    for lecture in lectures:\n        hit = get_most_recent_question_hit_by_lecture_and_question(lecture, question)\n\n        if hit is None:\n            hit = ImageQuestionHit(\n                public_id=ImageQuestionHit.make_public_id(),\n                image_question_id=question.id,\n                lecture_id=lecture.id,\n            )\n            hit.save()\n\n        hit.count += 1\n        hit.save()\n\n        hits.append(hit.to_dict())\n\n    return {\n        'id': question.public_id,\n        'hits': hits,\n    }\n\n\n@router.get('/search/image/{image_public_id}/questions/{question_public_id}/hits/{question_hit_public_id}/answer', dependencies=[Depends(get_db)])  # noqa: E501\ndef get_answer_to_question_hit(\n    image_public_id: str,\n    question_public_id: str,\n    question_hit_public_id: str,\n) -> LectureAnswerOutputModel:\n    upload = get_image_upload_by_public_id(image_public_id)\n\n    if upload is None:\n        raise HTTPException(status_code=404)\n\n    question = None\n    for q in upload.questions():\n        if q.public_id == question_public_id:\n            question = q\n\n    if question is None:\n        raise HTTPException(status_code=404)\n\n    hit = get_image_question_hit_by_public_id(question_hit_public_id)\n    if hit is None:\n        raise HTTPException(status_code=404)\n\n    hit.create_answer(tools.text.ai, prompts)\n    hit.save()\n\n    return {\n        'answer': hit.answer,\n    }\n\n\n@router.get('/search/image/{image_public_id}/questions/{question_public_id}/hits/{question_hit_public_id}/relevance', dependencies=[Depends(get_db)])  # noqa: E501\ndef get_relevance_of_question_hit(\n    image_public_id: str,\n    question_public_id: str,\n    question_hit_public_id: str,\n) -> LectureRelevanceOutputModel:\n    upload = get_image_upload_by_public_id(image_public_id)\n\n    if upload is None:\n        raise HTTPException(status_code=404)\n\n    question = None\n    for q in upload.questions():\n        if q.public_id == question_public_id:\n            question = q\n\n    if question is None:\n        raise HTTPException(status_code=404)\n\n    hit = get_image_question_hit_by_public_id(question_hit_public_id)\n    if hit is None:\n        raise HTTPException(status_code=404)\n\n    hit.create_relevance(tools.text.ai, prompts)\n    hit.save()\n\n    return {\n        'relevance': hit.relevance,\n    }\n"}
{"type": "source_file", "path": "api/routers/stats.py", "content": "from fastapi import Depends, APIRouter\nfrom pydantic import BaseModel\n\nimport index.courses as courses_index\nimport index.lecture as lecture_index\nfrom db import get_db\n\nrouter = APIRouter()\n\n\nclass StatsOutputModel(BaseModel):\n    courses: int\n    lectures: int\n    lectures_without_courses: int\n\n\n@router.get('/stats', dependencies=[Depends(get_db)])\ndef get_stats() -> StatsOutputModel:\n    return {\n        'courses': courses_index.at_least_one_lecture_count(),\n        'lectures': lecture_index.match_all_count(),\n        'lectures_without_courses': lecture_index.term_query_no_courses_count(),\n    }\n"}
{"type": "source_file", "path": "classifiers/__init__.py", "content": "# flake8: noqa\nfrom .subject import SubjectClassifier\nfrom .subject_multipass import SubjectMultipassClassifier\n"}
{"type": "source_file", "path": "classifiers/subject_multipass.py", "content": "from typing import List, Optional\n\nfrom db.models import Analysis, ImageUpload\nfrom .subject import SubjectClassifier\nfrom config.logger import log\n\n\nclass SubjectMultipassClassifier(SubjectClassifier):\n\n    def __init__(\n        self,\n        what: str,\n        priority: int,\n        upload: Optional[ImageUpload] = None,\n        analysis: Optional[Analysis] = None,\n        samples: Optional[int] = 1,\n    ):\n        super().__init__(what, priority, upload, analysis)\n        self.samples = samples\n        self.base_classifier = SubjectClassifier(what, priority, upload, analysis)\n\n    @staticmethod\n    def create_classifier_for(\n        what: str,\n        priority=SubjectClassifier.Priority.DEFAULT,\n        upload: Optional[ImageUpload] = None,\n        analysis: Optional[Analysis] = None,\n        samples: Optional[int] = 1,\n    ):\n        return SubjectMultipassClassifier(what, priority, upload, analysis, samples=samples)\n\n    def classify(self, string: str, target_number_of_labels: int = 3) -> List[str]:\n        subjects = []\n\n        for i in range(self.samples):\n            log().info(f'SubjectMultipassClassifier sample {i + 1}/{self.samples}')\n\n            classification = self.base_classifier.classify(string, 4)\n\n            for subject in classification:\n                if subject not in subjects:\n                    subjects.append(subject)\n\n        if self.samples == 1:\n            return subjects\n\n        self.base_classifier.override_labels(subjects)\n\n        validation_pass = self.base_classifier.classify(string, 0, validation_prompt=True)\n\n        # Sometimes yields an empty result or bad labels, re-running it will ask GPT-3 again,\n        # with a good chance of producing a better result\n        if len(validation_pass) == 0:\n            validation_pass = self.base_classifier.classify(string, 0, validation_prompt=True)\n\n        return validation_pass\n"}
{"type": "source_file", "path": "config/logger.py", "content": "from pydantic import BaseModel\nimport logging\n\nfrom config.settings import settings\n\n\nclass LogConfig(BaseModel):\n    LOGGER_NAME: str = settings.NAME\n    LOG_FORMAT: str = '%(levelprefix)s | %(asctime)s | %(message)s'\n    LOG_LEVEL: str = 'DEBUG'\n\n    version = 1\n    disable_existing_loggers = False\n    formatters = {\n        'default': {\n            '()': 'uvicorn.logging.DefaultFormatter',\n            'fmt': LOG_FORMAT,\n            'datefmt': '%Y-%m-%d %H:%M:%S',\n        },\n    }\n    handlers = {\n        'default': {\n            'formatter': 'default',\n            'class': 'logging.StreamHandler',\n            'stream': 'ext://sys.stderr',\n        },\n    }\n    loggers = {\n        LOGGER_NAME: {'handlers': ['default'], 'level': LOG_LEVEL},\n    }\n\n\ndef log(logger_name=settings.NAME):\n    return logging.getLogger(logger_name)\n"}
{"type": "source_file", "path": "config/settings.py", "content": "from typing import Any, Dict, List, Optional, Union\nfrom pydantic import AnyHttpUrl, BaseSettings, PostgresDsn, validator\n\n\nclass Settings(BaseSettings):\n    NAME: str\n    API_ENDPOINT: str\n    STORAGE_DIRECTORY: str\n\n    MATHPIX_APP_ID: str\n    MATHPIX_APP_KEY: str\n    MATHPIX_DAILY_OCR_REQUESTS_LIMIT: int\n\n    OPENAI_API_KEY: str\n    WHISPER_TRANSCRIPTION_DEVICE: str\n\n    BACKEND_CORS_ORIGINS: List[AnyHttpUrl] = []\n\n    @validator('BACKEND_CORS_ORIGINS', pre=True)\n    def assemble_cors_origins(cls, v: Union[str, List[str]]) -> Union[List[str], str]:\n        if isinstance(v, str) and not v.startswith('['):\n            return [i.strip() for i in v.split(',')]\n        elif isinstance(v, (list, str)):\n            return v\n        raise ValueError(v)\n\n    POSTGRES_SERVER: str\n    POSTGRES_PORT: str\n    POSTGRES_USER: str\n    POSTGRES_PASSWORD: str\n    POSTGRES_DB: str\n    DATABASE_URI: Optional[PostgresDsn] = None\n\n    @validator('DATABASE_URI', pre=True)\n    def assemble_db_connection(cls, v: Optional[str], values: Dict[str, Any]) -> Any:\n        if isinstance(v, str):\n            return v\n        return PostgresDsn.build(\n            scheme='postgresql',\n            user=values.get('POSTGRES_USER'),\n            password=values.get('POSTGRES_PASSWORD'),\n            host=values.get('POSTGRES_SERVER'),\n            port=values.get('POSTGRES_USER'),\n            path=f\"/{values.get('POSTGRES_DB') or ''}\",\n        )\n\n    REDIS_HOST: str\n    REDIS_PORT: str\n    REDIS_PASSWORD: Union[str, None] = None\n\n    OPENSEARCH_HOST: str\n    OPENSEARCH_PORT: str\n    OPENSEARCH_USERNAME: str\n    OPENSEARCH_PASSWORD: str\n\n    class Config:\n        case_sensitive = True\n        env_file = '.env'\n\n\nsettings = Settings()\n"}
{"type": "source_file", "path": "courses/__init__.py", "content": ""}
{"type": "source_file", "path": "db/__init__.py", "content": "from fastapi import Depends\n\n\nfrom .database import db, db_state_default\n\n\nasync def reset_db_state():\n    db._state._state.set(db_state_default.copy())\n    db._state.reset()\n\n\ndef get_db(db_state=Depends(reset_db_state)):\n    try:\n        db.connect()\n        yield\n    finally:\n        if not db.is_closed():\n            db.close()\n"}
{"type": "source_file", "path": "db/database.py", "content": "from contextvars import ContextVar\nimport peewee\nimport sys\n\nfrom config.settings import settings\n\n\ndb_state_default = {'closed': None, 'conn': None, 'ctx': None, 'transactions': None}\ndb_state = ContextVar('db_state', default=db_state_default.copy())\n\n\nclass PeeweeConnectionState(peewee._ConnectionState):\n    def __init__(self, **kwargs):\n        super().__setattr__('_state', db_state)\n        super().__init__(**kwargs)\n\n    def __setattr__(self, name, value):\n        self._state.get()[name] = value\n\n    def __getattr__(self, name):\n        return self._state.get()[name]\n\n\ndef create_postgres_database_connection():\n    return peewee.PostgresqlDatabase(\n        settings.POSTGRES_DB,\n        user=settings.POSTGRES_USER,\n        password=settings.POSTGRES_PASSWORD,\n        host=settings.POSTGRES_SERVER,\n        port=settings.POSTGRES_PORT,\n    )\n\n\ndef create_test_database_connection():\n    TEST_DB_FILEPATH = '/tmp/kthgpt.test.db'\n    return peewee.SqliteDatabase(TEST_DB_FILEPATH)\n\n\nif 'pytest' in sys.modules:\n    db = create_test_database_connection()\nelse:\n    db = create_postgres_database_connection()\n\n\ndb._state = PeeweeConnectionState()\n"}
{"type": "source_file", "path": "db/migrations/001_add_queue_info_table.py", "content": "\"\"\"Peewee migrations -- 001_add_queue_info_table.py.\"\"\"\n# flake8: noqa\n\nimport peewee as pw\nfrom peewee_migrate import Migrator\nfrom decimal import ROUND_HALF_EVEN\n\nfrom db.models import QueueInfo\nfrom db.database import db\n\n\ntry:\n    import playhouse.postgres_ext as pw_pext\nexcept ImportError:\n    pass\n\nSQL = pw.SQL\n\n\ndef migrate(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your migrations here.\"\"\"\n    db.create_tables([QueueInfo])\n\n\ndef rollback(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your rollback migrations here.\"\"\"\n    db.drop_tables([QueueInfo])\n"}
{"type": "source_file", "path": "db/migrations/002_add_approved_and_source_columns_to_lecture.py", "content": "\"\"\"Peewee migrations -- 002_add_approved_and_source_columns_to_lecture.py.\"\"\"\n# flake8: noqa\n\nimport peewee as pw\nfrom peewee_migrate import Migrator\n\nfrom db.models import Lecture\n\ntry:\n    import playhouse.postgres_ext as pw_pext\nexcept ImportError:\n    pass\n\nSQL = pw.SQL\n\n\ndef migrate(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your migrations here.\"\"\"\n    approved_field = pw.BooleanField(null=True)\n    source_field = pw.CharField(null=False, default=Lecture.Source.KTH)\n    migrator.add_fields(Lecture, approved=approved_field, source=source_field)\n    migrator.run()\n\n    lectures = Lecture.select()\n    for lecture in lectures:\n        lecture.approved = True\n        lecture.save()\n\n\ndef rollback(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your rollback migrations here.\"\"\"\n    migrator.remove_fields(Lecture, 'approved', cascade=True)\n    migrator.remove_fields(Lecture, 'source', cascade=True)\n"}
{"type": "source_file", "path": "db/migrations/003_add_token_usage_table.py", "content": "\"\"\"Peewee migrations -- 003_add_token_usage_table.py.\"\"\"\n# flake8: noqa\n\nimport peewee as pw\nfrom peewee_migrate import Migrator\n\nfrom db.models import TokenUsage\nfrom db.database import db\n\n\ntry:\n    import playhouse.postgres_ext as pw_pext\nexcept ImportError:\n    pass\n\nSQL = pw.SQL\n\n\ndef migrate(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your migrations here.\"\"\"\n    db.create_tables([TokenUsage])\n\n\ndef rollback(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your rollback migrations here.\"\"\"\n    db.drop_tables([TokenUsage])\n"}
{"type": "source_file", "path": "db/migrations/004_add_title_and_date_columns_to_lecture.py", "content": "\"\"\"Peewee migrations -- 004_add_title_and_date_columns_to_lecture.py.\"\"\"\n# flake8: noqa\n\nimport peewee as pw\nfrom peewee_migrate import Migrator\n\nfrom db.models import Lecture\n\ntry:\n    import playhouse.postgres_ext as pw_pext\nexcept ImportError:\n    pass\n\nSQL = pw.SQL\n\n\ndef migrate(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your migrations here.\"\"\"\n    title_field = pw.CharField(null=True)\n    date_field = pw.TimestampField(null=True)\n\n    migrator.add_fields(Lecture, title=title_field, date=date_field)\n    migrator.run()\n\n\ndef rollback(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your rollback migrations here.\"\"\"\n    migrator.remove_fields(Lecture, 'title', cascade=True)\n    migrator.remove_fields(Lecture, 'date', cascade=True)\n"}
{"type": "source_file", "path": "db/migrations/005_add_course_and_course_group_tables.py", "content": "\"\"\"Peewee migrations -- 005_add_course_and_course_group_tables.py.\"\"\"\n# flake8: noqa\n\nimport peewee as pw\nfrom peewee_migrate import Migrator\n\nfrom db.models import Course, CourseGroup\nfrom db.database import db\n\n\ntry:\n    import playhouse.postgres_ext as pw_pext\nexcept ImportError:\n    pass\n\nSQL = pw.SQL\n\n\ndef migrate(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your migrations here.\"\"\"\n    db.create_tables([Course, CourseGroup])\n\n\ndef rollback(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your rollback migrations here.\"\"\"\n    db.drop_tables([Course, CourseGroup])\n"}
{"type": "source_file", "path": "db/migrations/007_add_small_preview_to_lectures_table.py", "content": "\"\"\"Peewee migrations -- 007_add_small_preview_to_lectures_table.py.\"\"\"\n# flake8: noqa\n\nimport peewee as pw\nfrom peewee_migrate import Migrator\n\nfrom db.models import Lecture\n\ntry:\n    import playhouse.postgres_ext as pw_pext\nexcept ImportError:\n    pass\n\nSQL = pw.SQL\n\n\ndef migrate(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your migrations here.\"\"\"\n    field = pw.CharField(null=True)\n\n    migrator.add_fields(Lecture, img_preview_small=field)\n    migrator.run()\n\n\ndef rollback(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your rollback migrations here.\"\"\"\n    migrator.remove_fields(Lecture, 'img_preview_small', cascade=True)\n"}
{"type": "source_file", "path": "db/migrations/008_add_raw_content_link_column.py", "content": "\"\"\"Peewee migrations -- 008_add_raw_content_link_column.py.\"\"\"\n# flake8: noqa\n\nimport peewee as pw\nfrom peewee_migrate import Migrator\n\nfrom db.models import Lecture\n\ntry:\n    import playhouse.postgres_ext as pw_pext\nexcept ImportError:\n    pass\n\nSQL = pw.SQL\n\n\ndef migrate(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your migrations here.\"\"\"\n    field = pw.CharField(null=True)\n\n    migrator.add_fields(Lecture, raw_content_link=field)\n    migrator.run()\n\n\ndef rollback(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your rollback migrations here.\"\"\"\n    migrator.remove_fields(Lecture, 'raw_content_link', cascade=True)\n"}
{"type": "source_file", "path": "db/migrations/009_add_count_to_query_table.py", "content": "\"\"\"Peewee migrations -- 009_add_count_to_query_table.py.\"\"\"\n# flake8: noqa\n\nimport peewee as pw\nfrom peewee_migrate import Migrator\n\nfrom db.models import Query\n\ntry:\n    import playhouse.postgres_ext as pw_pext\nexcept ImportError:\n    pass\n\nSQL = pw.SQL\n\n\ndef migrate(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your migrations here.\"\"\"\n    field = pw.IntegerField(null=False, default=0)\n\n    migrator.add_fields(Query, count=field)\n    migrator.run()\n\n\ndef rollback(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your rollback migrations here.\"\"\"\n    migrator.remove_fields(Query, 'count', cascade=True)\n"}
{"type": "source_file", "path": "classifiers/subject.py", "content": "from typing import List, Optional\n\nfrom db.models import Analysis, ImageUpload\nimport tools.text.prompts as prompts\nimport tools.text.ai\n\nLABELS = [\n    # Architecture\n    'Sustainability Environmental Engineering',\n    'Architecture',\n    'History of Science, Technology Environment',\n\n    # Computer science\n    'Communication',\n    'Computer Science',\n    'Introductory Programming',\n    'Advanced Programming Techniques',\n    'Visualization and Graphics',\n    'Theoretical Computer Science',\n    'Software Engineering and Security',\n    'Media Technology Interaction Design',\n    'Speech, Music, Hearing',\n    'Machine Learning and AI',\n\n    # Electrical Engineering\n    'Control Theory',\n    'Communications',\n    'Intelligent Systems',\n    'Information Communication Technology',\n    'Electrical Engineering',\n    'Fusion Plasma Physics',\n\n    # Biotechnology\n    'Biotechnology',\n    'Biomedical Engineering Health Systems',\n    'Ergonomics and Work Environment',\n\n    # Chemistry\n    'Chemistry',\n    'Organic Chemistry',\n    'Inorganic Chemistry',\n    'Analytical and Physical Chemistry',\n    'Specialized and Applied Chemistry',\n\n    # Industrial engineering and learning sciences\n    'Energy Technology',\n    'Learning in Engineering Sciences',\n    'Industrial Engineering Management',\n    'Industrial Economics Management',\n    'Machine Design',\n    'Materials Science Engineering',\n    'Energy Technology',\n    'Sustainable Production Development',\n\n    # Engineering Sciences\n    'Engineering Sciences',\n\n    # Mathematics\n    'Mathematics',\n    'Basic Mathematics and Foundations',\n    'Analysis and Calculus',\n    'Algebra and Geometry',\n    'Probability Theory',\n    'Statistics',\n    'Applied Mathematics and Computational Methods'\n\n    # Physics\n    'Acoustics',\n    'Solid Mechanics',\n    'Mechanics',\n    'Physics',\n    'Theoretical Physics',\n    'Applied Physics',\n]\n\n\nclass SubjectClassifier:\n\n    def __init__(\n        self,\n        what: str,\n        priority: int,\n        upload: Optional[ImageUpload] = None,\n        analysis: Optional[Analysis] = None,\n    ) -> None:\n        self.what = what\n        self.priority = priority\n        self.upload = upload\n        self.analysis = analysis\n        self.custom_labels = None\n\n        self.lowercase_labels = {}\n        for (idx, label) in enumerate(self.get_labels()):\n            self.lowercase_labels[label.lower()] = idx\n\n    class Priority:\n        DEFAULT = 0\n        HIGH = 1\n        LOW = 2\n\n    @staticmethod\n    def create_classifier_for(\n        what: str,\n        priority=Priority.DEFAULT,\n        upload: Optional[ImageUpload] = None,\n        analysis: Optional[Analysis] = None,\n    ):\n        return SubjectClassifier(what, priority, upload, analysis)\n\n    def override_labels(self, labels: List[str]):\n        self.custom_labels = labels\n        self.lowercase_labels = {}\n        for (idx, label) in enumerate(labels):\n            self.lowercase_labels[label.lower()] = idx\n\n    def get_labels(self) -> List[str]:\n        if self.custom_labels is not None:\n            return self.custom_labels\n        return LABELS\n\n    def classify(\n        self,\n        string: str,\n        target_number_of_labels: int = 3,\n        validation_prompt=False\n    ) -> List[str]:\n        if validation_prompt:\n            prompt = self.create_validation_prompt(string)\n        else:\n            prompt = self.create_prompt(string, target_number_of_labels)\n\n        if self.priority == self.Priority.HIGH:\n            response = self.high_priority_request(prompt)\n        elif self.priority == self.Priority.LOW:\n            response = self.low_priority_request(prompt)\n        else:\n            response = self.default_priority_request(prompt)\n\n        classification = self.parse_response(response)\n        return classification\n\n    def create_prompt(self, text: str, target_number_of_labels: int) -> str:\n        return prompts.create_classification_prompt_for_subjects(\n            self.what,\n            target_number_of_labels,\n            self.get_labels(),\n            text,\n        )\n\n    def create_validation_prompt(self, text: str) -> str:\n        return prompts.create_validation_prompt_for_subjects(\n            self.what,\n            self.get_labels(),\n            text,\n        )\n\n    def parse_response(self, gpt_response: str) -> list:\n        out = []\n        lines = gpt_response.split('\\n')\n        for line in lines:\n            if line == '':\n                continue\n\n            for label in self.lowercase_labels:\n                if label in line.lower():\n                    original_case_label = self.get_labels()[self.lowercase_labels[label]]\n\n                    if original_case_label not in out:\n                        out.append(original_case_label)\n\n        return out\n\n    def default_priority_request(self, prompt: str) -> str:\n        upload_id = None\n        if self.upload is not None:\n            upload_id = self.upload.id\n\n        analysis_id = None\n        if self.analysis is not None:\n            analysis_id = self.analysis.id\n\n        return tools.text.ai.gpt3(\n            prompt,\n            time_to_live=60 * 2,  # 2 mins\n            max_retries=4,\n            retry_interval=[\n                5,\n                15,\n                30,\n                60,\n            ],\n            upload_id=upload_id,\n            analysis_id=analysis_id,\n        )\n\n    def high_priority_request(self, prompt: str) -> str:\n        upload_id = None\n        if self.upload is not None:\n            upload_id = self.upload.id\n\n        analysis_id = None\n        if self.analysis is not None:\n            analysis_id = self.analysis.id\n\n        return tools.text.ai.gpt3(\n            prompt,\n            time_to_live=60,  # 1 mins\n            max_retries=5,\n            retry_interval=[\n                5,\n                10,\n                10,\n                10,\n                10,\n            ],\n            upload_id=upload_id,\n            analysis_id=analysis_id,\n        )\n\n    def low_priority_request(self, prompt: str) -> str:\n        upload_id = None\n        if self.upload is not None:\n            upload_id = self.upload.id\n\n        analysis_id = None\n        if self.analysis is not None:\n            analysis_id = self.analysis.id\n\n        return tools.text.ai.gpt3(\n            prompt,\n            time_to_live=60 * 60 * 10,  # 5 hrs\n            max_retries=10,\n            retry_interval=[\n                10,\n                30,\n                60,\n                2 * 60,\n                2 * 60,\n                10 * 60,\n                30 * 60,\n                2 * 60 * 60,\n                30 * 60,\n                30 * 60,\n            ],\n            upload_id=upload_id,\n            analysis_id=analysis_id,\n        )\n"}
{"type": "source_file", "path": "db/crud.py", "content": "from datetime import datetime\nfrom typing import Union\nfrom peewee import fn\n\n\n# URL\ndef get_url_by_sha(sha: str):\n    from db.models.url import URL\n    return URL.filter(URL.url_hash == sha).first()\n\n\n# Lecture\ndef get_lecture_by_public_id_and_language(id: str, language: str):\n    from db.models.lecture import Lecture\n    return Lecture.filter(Lecture.public_id == id).filter(Lecture.language == language).first()\n\n\ndef get_lecture_by_id(id: int):\n    from db.models.lecture import Lecture\n    return Lecture.filter(Lecture.id == id).first()\n\n\ndef get_all_lectures():\n    from db.models.lecture import Lecture\n    query = Lecture.select().order_by(Lecture.created_at.asc())\n\n    lectures = []\n    for lecture in query:\n        lectures.append(lecture)\n\n    return lectures\n\n\ndef get_all_ready_lectures():\n    from db.models.lecture import Analysis\n    lectures = get_all_lectures()\n\n    out = []\n    for lecture in lectures:\n        if lecture.get_last_analysis().state == Analysis.State.READY:\n            out.append(lecture)\n\n    return out\n\n\ndef get_all_denied_lectures():\n    from db.models.lecture import Analysis\n    lectures = get_all_lectures()\n\n    out = []\n    for lecture in lectures:\n        if lecture.get_last_analysis().state == Analysis.State.DENIED:\n            out.append(lecture)\n\n    return out\n\n\ndef get_all_failed_lectures():\n    from db.models.lecture import Analysis\n    lectures = get_all_lectures()\n\n    out = []\n    for lecture in lectures:\n        if lecture.get_last_analysis().state == Analysis.State.FAILURE:\n            out.append(lecture)\n\n    return out\n\n\ndef get_unfinished_lectures():\n    from db.models.lecture import Analysis\n    lectures = get_all_lectures()\n\n    out = []\n    for lecture in lectures:\n        if lecture.get_last_analysis().state not in [\n            Analysis.State.READY,\n            Analysis.State.DENIED,\n        ]:\n            out.append(lecture)\n\n    return out\n\n\n# LectureSubject\ndef add_subject_with_name_and_reference_to_lecture(name: str, lecture_id: int):\n    from db.models import LectureSubject\n    subject = LectureSubject(\n        name=name,\n        lecture_id=lecture_id,\n    )\n    subject.save()\n    return subject\n\n\ndef get_lecture_subjects_by_lecture_id(id: int):\n    from db.models import LectureSubject\n    return LectureSubject.filter(LectureSubject.lecture_id == id)\n\n\n# Analysis\ndef get_all_analysis_for_lecture(lecture_id: int):\n    from db.models import Analysis\n    query = Analysis.filter(Analysis.lecture_id == lecture_id)\n\n    out = []\n    for a in query:\n        out.append(a)\n\n    return out\n\n\ndef delete_all_except_last_message_in_analysis(analysis_id: int):\n    from db.models import Analysis, Message\n    a = Analysis.get(analysis_id)\n    last_message = a.get_last_message()\n\n    Message.delete().where(\n        Message.id != last_message\n    ).where(\n        Message.analysis_id == analysis_id\n    ).execute()\n\n\n# Query\ndef get_most_recent_lecture_query_by_sha(lecture, sha: str):\n    from db.models.query import Query\n    return Query.filter(\n        Query.lecture_id == lecture.id\n    ).filter(\n        Query.query_hash == sha\n    ).filter(\n        Query.cache_is_valid == True  # noqa: E712\n    ).order_by(\n        Query.modified_at.desc()\n    ).first()\n\n\ndef get_most_recent_image_query_by_sha(image, sha: str):\n    from db.models.query import Query\n    return Query.filter(\n        Query.image_upload_id == image.id\n    ).filter(\n        Query.query_hash == sha\n    ).filter(\n        Query.cache_is_valid == True  # noqa: E712\n    ).order_by(\n        Query.modified_at.desc()\n    ).first()\n\n\ndef create_lecture_query(lecture, query_string: str):\n    from db.models.query import Query\n    query = Query(lecture_id=lecture.id, query_string=query_string)\n    query.save()\n    return query\n\n\ndef create_image_query(image, query_string: str):\n    from db.models.query import Query\n    query = Query(image_upload_id=image.id, query_string=query_string)\n    query.save()\n    return query\n\n\ndef find_all_queries_for_lecture(lecture):\n    from db.models.query import Query\n    return Query.select().where(Query.lecture_id == lecture.id)\n\n\ndef find_all_queries_for_image(image):\n    from db.models.query import Query\n    return Query.select().where(Query.image_upload_id == image.id)\n\n\n# Message\ndef save_message_for_analysis(analysis, title: str, body: Union[str, None] = None):\n    from db.models.message import Message\n    msg = Message(analysis_id=analysis.id, title=title, body=body)\n    msg.save()\n\n\n# Course\ndef find_course_by_course_code(code: str):\n    from db.models.course import Course\n    return Course.filter(Course.course_code == code).first()\n\n\ndef get_all_courses():\n    from db.models.course import Course, CourseGroup, CourseWrapper\n    out = []\n\n    courses = Course.filter(Course.group_id == None)  # noqa: E711\n    for course in courses:\n        out.append(CourseWrapper.from_course(course))\n\n    courses_groups = CourseGroup.select()\n    for group in courses_groups:\n        out.append(CourseWrapper.from_course_group(group))\n\n    return out\n\n\ndef find_course_code(course_code: str):\n    from db.models.course import Course, CourseGroup, CourseWrapper\n\n    course = CourseGroup.filter(CourseGroup.course_code == course_code).first()\n    if course is not None:\n        return CourseWrapper.from_course_group(course)\n\n    course = Course.filter(Course.course_code == course_code).first()\n    if course is not None:\n        return CourseWrapper.from_course(course)\n\n    return None\n\n\ndef find_all_courses_relations_for_course_group_id(id: int):\n    from db.models.course import CourseLectureRelation\n    relations = CourseLectureRelation.filter(CourseLectureRelation.group_id == id)\n    return relations\n\n\ndef find_all_courses_relations_for_course_id(id: int):\n    from db.models.course import CourseLectureRelation\n    relations = CourseLectureRelation.filter(CourseLectureRelation.course_id == id)\n    return relations\n\n\ndef find_all_courses_relations_for_lecture_id(id: int):\n    from db.models.course import CourseLectureRelation\n    relations = CourseLectureRelation.filter(CourseLectureRelation.lecture_id == id)\n    return relations\n\n\ndef find_all_courses_for_lecture_id(id: int):\n    from db.models.course import Course, CourseGroup, CourseWrapper, CourseLectureRelation\n    out = []\n    relations = CourseLectureRelation.filter(CourseLectureRelation.lecture_id == id)\n    for relation in relations:\n        if relation.course_id is not None:\n            out.append(CourseWrapper.from_course(\n                Course.get(id=relation.course_id))\n            )\n        elif relation.group_id is not None:\n            out.append(CourseWrapper.from_course_group(\n                CourseGroup.get(id=relation.group_id))\n            )\n\n    return out\n\n\ndef create_relation_between_lecture_and_course(lecture_id: int, course_id: int):\n    from db.models.course import CourseLectureRelation\n    relation = CourseLectureRelation(\n        lecture_id=lecture_id,\n        course_id=course_id,\n    )\n    relation.save()\n\n\ndef create_relation_between_lecture_and_course_group(lecture_id: int, group_id: int):\n    from db.models.course import CourseLectureRelation\n    relation = CourseLectureRelation(\n        lecture_id=lecture_id,\n        group_id=group_id\n    )\n    relation.save()\n\n\ndef find_relation_between_lecture_and_course(lecture_id: int, course_id: int):\n    from db.models.course import CourseLectureRelation\n    return CourseLectureRelation.filter(CourseLectureRelation.lecture_id == lecture_id).filter(CourseLectureRelation.course_id == course_id).first()  # noqa: E501\n\n\ndef find_relation_between_lecture_and_course_group(lecture_id: int, group_id: int):\n    from db.models.course import CourseLectureRelation\n    return CourseLectureRelation.filter(CourseLectureRelation.lecture_id == lecture_id).filter(CourseLectureRelation.group_id == group_id).first()  # noqa: E501\n\n\ndef delete_lecture_course_relation(id: int):\n    from db.models.course import CourseLectureRelation\n    return CourseLectureRelation.delete().where(CourseLectureRelation.id == id).execute()\n\n\n# ImageUpload\ndef get_image_upload_by_public_id(id: str):\n    from db.models import ImageUpload\n    return ImageUpload.filter(ImageUpload.public_id == id).first()\n\n\ndef get_image_upload_by_id(id: str):\n    from db.models import ImageUpload\n    return ImageUpload.filter(ImageUpload.id == id).first()\n\n\ndef get_image_upload_by_image_sha(sha: str):\n    from db.models import ImageUpload\n    return ImageUpload.filter(ImageUpload.image_sha == sha).first()\n\n\ndef get_random_image_upload_by_subject(subject: str):\n    from db.models import ImageUpload, ImageUploadSubject\n    return ImageUpload.select().join(\n        ImageUploadSubject\n    ).where(\n        ImageUploadSubject.image_upload_id == ImageUpload.id\n    ).filter(\n        ImageUploadSubject.name == subject\n    ).order_by(\n        fn.Random()\n    ).first()\n\n\ndef get_number_of_images_uploaded_today() -> int:\n    from db.models import ImageUpload\n    today_start = datetime.combine(datetime.utcnow().date(), datetime.min.time())\n    today_end = datetime.combine(datetime.utcnow().date(), datetime.max.time())\n\n    today_image_uploads = ImageUpload.select().where(\n        (ImageUpload.created_at >= today_start) & (ImageUpload.created_at <= today_end)\n    )\n\n    return today_image_uploads.count()\n\n\n# ImageUploadSubjects\ndef add_subject_with_name_and_reference_to_image_upload(name: str, upload_id: int):\n    from db.models import ImageUploadSubject\n    subject = ImageUploadSubject(\n        name=name,\n        image_upload_id=upload_id,\n    )\n    subject.save()\n    return subject\n\n\ndef get_image_upload_subjects_by_image_upload_id(id: int):\n    from db.models import ImageUploadSubject\n    return ImageUploadSubject.filter(ImageUploadSubject.image_upload_id == id)\n\n\n# ImageQuestion\ndef get_image_question_by_public_id(id: str):\n    from db.models import ImageQuestion\n    return ImageQuestion.filter(ImageQuestion.public_id == id).first()\n\n\ndef get_image_question_by_id(id: int):\n    from db.models import ImageQuestion\n    return ImageQuestion.filter(ImageQuestion.id == id).first()\n\n\ndef get_image_questions_by_image_upload_id(id: int):\n    from db.models import ImageQuestion\n    return ImageQuestion.filter(ImageQuestion.image_upload_id == id)\n\n\ndef get_most_recent_image_question_by_sha(upload, sha: str):\n    from db.models import ImageQuestion\n    return ImageQuestion.filter(\n        ImageQuestion.image_upload_id == upload.id\n    ).filter(\n        ImageQuestion.query_hash == sha\n    ).order_by(\n        ImageQuestion.modified_at.desc()\n    ).first()\n\n\n# ImageQuestionHit\ndef get_image_question_hit_by_public_id(id: str):\n    from db.models import ImageQuestionHit\n    return ImageQuestionHit.filter(ImageQuestionHit.public_id == id).first()\n\n\ndef get_most_recent_question_hit_by_lecture_and_question(lecture, image_question):\n    from db.models import ImageQuestionHit\n    return ImageQuestionHit.filter(\n        ImageQuestionHit.lecture_id == lecture.id\n    ).filter(\n        ImageQuestionHit.image_question_id == image_question.id\n    ).filter(\n        ImageQuestionHit.cache_is_valid == True  # noqa: E712\n    ).order_by(\n        ImageQuestionHit.modified_at.desc()\n    ).first()\n\n\ndef get_image_question_hits_by_image_question_id(id: int):\n    from db.models import ImageQuestionHit\n    return ImageQuestionHit.filter(ImageQuestionHit.image_question_id == id)\n\n\n# Mathpix requests\ndef get_mathpix_requests_by_image_upload_id(id: int):\n    from db.models import MathpixRequest\n    return MathpixRequest.filter(MathpixRequest.image_upload_id == id)\n"}
{"type": "source_file", "path": "db/cmd.py", "content": "\n\nfrom db.crud import get_all_lectures\n\n\ndef invalidate_all_query_caches():\n    print('invalidating all query caches')\n\n    lectures = get_all_lectures()\n    for lecture in lectures:\n        print(f'invalidating cache for lecture {lecture}: ', end='')\n        queries = lecture.queries()\n        print(f'found {len(queries)} queries', end='')\n        for query in queries:\n            query.cache_is_valid = False\n            query.save()\n        print(' done.')\n"}
{"type": "source_file", "path": "db/migrations/006_add_course_lecture_mapping_table.py", "content": "\"\"\"Peewee migrations -- 006_add_course_lecture_mapping_table.py.\"\"\"\n# flake8: noqa\n\nimport peewee as pw\nfrom peewee_migrate import Migrator\n\nfrom db.models import CourseLectureRelation\nfrom db.database import db\n\n\ntry:\n    import playhouse.postgres_ext as pw_pext\nexcept ImportError:\n    pass\n\nSQL = pw.SQL\n\n\ndef migrate(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your migrations here.\"\"\"\n    db.create_tables([CourseLectureRelation])\n\n\ndef rollback(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your rollback migrations here.\"\"\"\n    db.drop_tables([CourseLectureRelation])\n"}
{"type": "source_file", "path": "api/routers/urls.py", "content": "from fastapi import Depends, APIRouter, HTTPException\nfrom urllib.parse import urlparse, parse_qs\nfrom pydantic import BaseModel\nimport validators\nimport re\n\nfrom db.crud import (\n    get_lecture_by_public_id_and_language,\n    get_url_by_sha,\n)\nfrom jobs import schedule_analysis_of_lecture\nfrom db.models import URL, Lecture, Analysis\nfrom db import get_db\n\nfrom tools.web.crawler import get_m3u8\n\n\nrouter = APIRouter()\n\n\nclass InputModel(BaseModel):\n    url: str\n    language: str\n\n\nclass OutputModel(BaseModel):\n    uri: str\n\n\n@router.post('/url/kth', dependencies=[Depends(get_db)])\ndef parse_kth_url(\n    input_data: InputModel,\n) -> OutputModel:\n    if input_data.url.strip() == '':\n        raise HTTPException(status_code=400, detail='No URL provided, please enter a url such as: https://play.kth.se/media/0_4zo9e4nh')  # noqa: E501\n\n    if not validators.url(input_data.url):\n        raise HTTPException(status_code=400, detail='The URL you entered was not valid, please enter a url such as: https://play.kth.se/media/0_4zo9e4nh')  # noqa: E501\n\n    domain = urlparse(input_data.url).netloc\n    if not domain.endswith('play.kth.se'):\n        raise HTTPException(status_code=400, detail='The URL was not valid, it must be kth.play.se video, such as https://play.kth.se/media/0_4zo9e4nh')  # noqa: E501\n\n    if input_data.language == str(Lecture.Language.ENGLISH):\n        lang = Lecture.Language.ENGLISH\n    elif input_data.language == str(Lecture.Language.SWEDISH):\n        lang = Lecture.Language.SWEDISH\n    else:\n        raise HTTPException(status_code=400, detail='Unsupported language')\n\n    sha = URL.make_sha(input_data.url)\n    url = get_url_by_sha(sha)\n\n    if url is None:\n        url = URL(\n            url=input_data.url,\n            url_hash=URL.make_sha(input_data.url),\n        )\n\n        try:\n            content_url = urlparse(get_m3u8(input_data.url)).path\n        except Exception:\n            raise HTTPException(status_code=400, detail='Something went wrong while trying to get the lecture video, make sure its on the form https://play.kth.se/media/0_4zo9e4nh')  # noqa: E501\n\n        regex = r'^.*entryId\\/(\\w*)\\/.*$'\n        matches = re.finditer(regex, content_url, re.MULTILINE)\n        match = next(matches)\n\n        if not match:\n            raise HTTPException(status_code=400, detail='Could not find the lecture id')\n\n        url.lecture_id = match.group(1)\n        url.save()\n\n    lecture = get_lecture_by_public_id_and_language(url.lecture_id, lang)\n    if lecture is None:\n        lecture = Lecture(\n            public_id=url.lecture_id,\n            language=lang,\n            approved=True,\n            source=Lecture.Source.KTH,\n        )\n        lecture.save()\n\n    should_analyse = False\n    analysis = lecture.get_last_analysis()\n\n    if analysis is None:\n        should_analyse = True\n    else:\n        if analysis.state == Analysis.State.FAILURE:\n            should_analyse = True\n\n        if analysis.seems_to_have_crashed():\n            should_analyse = True\n\n    if should_analyse:\n        analysis = schedule_analysis_of_lecture(lecture)\n\n    return {\n        'uri': url.lecture_uri(lang)\n    }\n\n\n@router.post('/url/kth_raw', dependencies=[Depends(get_db)])\ndef parse_kth_raw_url(\n    input_data: InputModel,\n) -> OutputModel:\n    if input_data.url.strip() == '':\n        raise HTTPException(status_code=400, detail='No URL provided, please enter a url such as: https://vod-cache.kaltura.nordu.net/...')  # noqa: E501\n\n    if not validators.url(input_data.url):\n        raise HTTPException(status_code=400, detail='The URL you entered was not valid, please enter a url such as: https://vod-cache.kaltura.nordu.net/...')  # noqa: E501\n\n    domain = urlparse(input_data.url).netloc\n    if not domain.endswith('vod-cache.kaltura.nordu.net'):\n        raise HTTPException(status_code=400, detail='The URL was not valid, it must be kaltura.nordu.net video, such as https://vod-cache.kaltura.nordu.net/...')  # noqa: E501\n\n    if input_data.language == str(Lecture.Language.ENGLISH):\n        lang = Lecture.Language.ENGLISH\n    elif input_data.language == str(Lecture.Language.SWEDISH):\n        lang = Lecture.Language.SWEDISH\n    else:\n        raise HTTPException(status_code=400, detail='Unsupported language')\n\n    sha = URL.make_sha(input_data.url)\n    url = get_url_by_sha(sha)\n\n    if url is None:\n        url = URL(\n            url=input_data.url,\n            url_hash=URL.make_sha(input_data.url),\n        )\n\n        regex = r'^.*entryId\\/(\\w*)\\/.*$'\n        matches = re.finditer(regex, input_data.url, re.MULTILINE)\n        match = next(matches)\n\n        if not match:\n            raise HTTPException(status_code=400, detail='Could not find the lecture id')\n\n        url.lecture_id = match.group(1)\n        url.save()\n\n    lecture = get_lecture_by_public_id_and_language(url.lecture_id, lang)\n    if lecture is None:\n        lecture = Lecture(\n            public_id=url.lecture_id,\n            language=lang,\n            approved=True,\n            source=Lecture.Source.KTH_RAW,\n            raw_content_link=input_data.url,\n        )\n        lecture.save()\n\n    should_analyse = False\n    analysis = lecture.get_last_analysis()\n\n    if analysis is None:\n        should_analyse = True\n    else:\n        if analysis.state == Analysis.State.FAILURE:\n            should_analyse = True\n\n        if analysis.seems_to_have_crashed():\n            should_analyse = True\n\n    if should_analyse:\n        analysis = schedule_analysis_of_lecture(lecture)\n\n    return {\n        'uri': url.lecture_uri(lang)\n    }\n\n\n@router.post('/url/youtube', dependencies=[Depends(get_db)])\ndef parse_youtube_url(\n    input_data: InputModel,\n) -> OutputModel:\n    if input_data.url.strip() == '':\n        raise HTTPException(status_code=400, detail='No URL provided, please enter a url such as: https://www.youtube.com/watch?v=nnkCEN4suxs')  # noqa: E501\n\n    if not validators.url(input_data.url):\n        raise HTTPException(status_code=400, detail='The URL you entered was not valid, please enter a url such as: https://www.youtube.com/watch?v=nnkCEN4suxs')  # noqa: E501\n\n    domain = urlparse(input_data.url).netloc\n    if not domain.endswith('youtube.com'):\n        raise HTTPException(status_code=400, detail='The URL was not valid, it must be a youtube video, such as https://www.youtube.com/watch?v=nnkCEN4suxs')  # noqa: E501\n\n    query_params = parse_qs(urlparse(input_data.url).query)\n    if 'v' not in query_params:\n        raise HTTPException(status_code=400, detail='The URL you entered was not valid, its missing a video id, please enter a url such as: https://www.youtube.com/watch?v=nnkCEN4suxs')  # noqa: E501\n\n    if input_data.language == str(Lecture.Language.ENGLISH):\n        lang = Lecture.Language.ENGLISH\n    elif input_data.language == str(Lecture.Language.SWEDISH):\n        lang = Lecture.Language.SWEDISH\n    else:\n        raise HTTPException(status_code=400, detail='Unsupported language')\n\n    sha = URL.make_sha(input_data.url)\n    url = get_url_by_sha(sha)\n\n    if url is None:\n        url = URL(\n            url=input_data.url,\n            url_hash=URL.make_sha(input_data.url),\n            lecture_id=query_params['v'][0],\n        )\n        url.save()\n\n    lecture = get_lecture_by_public_id_and_language(url.lecture_id, lang)\n    if lecture is None:\n        lecture = Lecture(\n            public_id=url.lecture_id,\n            language=lang,\n            source=Lecture.Source.YOUTUBE\n        )\n        lecture.save()\n\n    should_analyse = False\n    analysis = lecture.get_last_analysis()\n\n    if analysis is None:\n        should_analyse = True\n    else:\n        if analysis.state == Analysis.State.FAILURE:\n            should_analyse = True\n\n        if analysis.seems_to_have_crashed():\n            should_analyse = True\n\n    if should_analyse:\n        analysis = schedule_analysis_of_lecture(lecture)\n\n    return {\n        'uri': url.lecture_uri(lang)\n    }\n"}
{"type": "source_file", "path": "db/migrations/014_add_upload_id_column_to_token_usage.py", "content": "\"\"\"Peewee migrations -- 014_add_upload_id_column_to_token_usage.py.\"\"\"\nimport peewee as pw\nfrom peewee_migrate import Migrator\n\nfrom db.models import TokenUsage\n\n\ndef migrate(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your migrations here.\"\"\"\n    field = pw.IntegerField(null=True, index=True)\n    migrator.add_fields(\n        TokenUsage,\n        upload_id=field,\n    )\n    migrator.run()\n\n\ndef rollback(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your rollback migrations here.\"\"\"\n    migrator.remove_fields(TokenUsage, 'upload_id', cascade=True)\n"}
{"type": "source_file", "path": "db/models/image_question.py", "content": "import hashlib\nimport peewee\nimport uuid\n\nfrom .image_upload import ImageUpload\nfrom .lecture import Lecture\nfrom .base import Base\nfrom db.crud import (\n    get_image_question_hits_by_image_question_id,\n    get_image_question_by_id,\n    get_image_upload_by_id,\n    get_lecture_by_id,\n)\n\n\nclass ImageQuestionHitException(Exception):\n    pass\n\n\nclass ImageQuestion(Base):\n    public_id = peewee.CharField(null=False, unique=True)\n    image_upload_id = peewee.ForeignKeyField(ImageUpload, null=False, backref='imageupload', on_delete='cascade')\n    query_string = peewee.TextField(null=False)\n    query_hash = peewee.CharField(index=True, null=True)\n\n    @staticmethod\n    def make_public_id() -> str:\n        return str(uuid.uuid4())\n\n    @staticmethod\n    def make_sha(string: str) -> str:\n        return hashlib.sha256(string.encode()).hexdigest()\n\n    def hits(self) -> list:\n        return list(get_image_question_hits_by_image_question_id(self.id))\n\n\nclass ImageQuestionHit(Base):\n    public_id = peewee.CharField(null=False, unique=True)\n    image_question_id = peewee.ForeignKeyField(ImageQuestion, null=False, backref='imagequestion', on_delete='cascade')\n    lecture_id = peewee.ForeignKeyField(Lecture, null=True, backref='lecture', on_delete='set null')\n    answer = peewee.TextField(null=True)\n    relevance = peewee.TextField(null=True)\n    cache_is_valid = peewee.BooleanField(null=False, default=True)\n    count = peewee.IntegerField(null=False, default=0)\n\n    @staticmethod\n    def make_public_id() -> str:\n        return str(uuid.uuid4())\n\n    def refresh(self):\n        update = ImageQuestionHit.get(self.id)\n        self.public_id = update.public_id\n        self.image_question_id = update.image_question_id\n        self.lecture_id = update.lecture_id\n        self.answer = update.answer\n        self.relevance = update.relevance\n        self.cache_is_valid = update.cache_is_valid\n        self.count = update.count\n\n    def create_answer(self, ai, prompts):\n        if self.answer is not None and self.cache_is_valid:\n            return\n\n        lecture = get_lecture_by_id(self.lecture_id)\n        if lecture is None:\n            raise ImageQuestionHitException(f'Lecture with id {self.lecture_id} not found')\n\n        question = get_image_question_by_id(self.image_question_id)\n        if question is None:\n            raise ImageQuestionHitException(f'ImageQuestion with id {self.image_question_id} not found')\n\n        upload = get_image_upload_by_id(question.image_upload_id)\n        if upload is None:\n            raise ImageQuestionHitException(f'ImageUpload with id {question.image_upload_id} not found')\n\n        prompt = prompts.create_prompt_to_answer_question_about_hit(\n            lecture,\n            upload,\n            question,\n        )\n\n        response = ai.gpt3(\n            prompt,\n            time_to_live=60,\n            max_retries=2,\n            retry_interval=[10, 20],\n            upload_id=upload.id,\n        )\n\n        self.refresh()\n        self.answer = response\n        self.cache_is_valid = True\n\n    def create_relevance(self, ai, prompts):\n        if self.relevance is not None and self.cache_is_valid:\n            return\n\n        lecture = get_lecture_by_id(self.lecture_id)\n        if lecture is None:\n            raise ImageQuestionHitException(f'Lecture with id {self.lecture_id} not found')\n\n        question = get_image_question_by_id(self.image_question_id)\n        if question is None:\n            raise ImageQuestionHitException(f'ImageQuestion with id {self.image_question_id} not found')\n\n        upload = get_image_upload_by_id(question.image_upload_id)\n        if upload is None:\n            raise ImageQuestionHitException(f'ImageUpload with id {question.image_upload_id} not found')\n\n        prompt = prompts.create_prompt_to_explain_hit_relevance(\n            lecture,\n            upload,\n            question,\n        )\n\n        response = ai.gpt3(\n            prompt,\n            time_to_live=60,\n            max_retries=2,\n            retry_interval=[10, 20],\n            upload_id=upload.id,\n        )\n\n        self.refresh()\n        self.relevance = response\n        self.cache_is_valid = True\n\n    def to_dict(self) -> dict:\n        lecture = get_lecture_by_id(self.lecture_id)\n        lecture_dict = None\n        if lecture is not None:\n            lecture_dict = lecture.to_dict()\n\n        return {\n            'id': self.public_id,\n            'lecture': lecture_dict,\n            'answer': self.answer,\n            'relevance': self.relevance,\n        }\n"}
{"type": "source_file", "path": "db/migrations/023_add_description_column_to_lecture.py", "content": "\"\"\"Peewee migrations -- 023_add_description_column_to_lecture.py.\"\"\"\nimport peewee as pw\nfrom peewee_migrate import Migrator\n\nfrom db.models import Lecture\n\n\ndef migrate(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your migrations here.\"\"\"\n    field = pw.TextField(null=True)\n    migrator.add_fields(\n        Lecture,\n        description=field,\n    )\n    migrator.run()\n\n\ndef rollback(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your rollback migrations here.\"\"\"\n    migrator.remove_fields(Lecture, 'description', cascade=True)\n"}
{"type": "source_file", "path": "db/migrations/012_add_image_uploads_table.py", "content": "\"\"\"Peewee migrations -- 012_add_image_uploads_table.py.\"\"\"\nimport peewee as pw\nfrom peewee_migrate import Migrator\n\nfrom db.models import ImageUpload\nfrom db.database import db\n\n\ndef migrate(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your migrations here.\"\"\"\n    db.create_tables([ImageUpload])\n\n\ndef rollback(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your rollback migrations here.\"\"\"\n    db.drop_tables([ImageUpload])\n"}
{"type": "source_file", "path": "db/models/course.py", "content": "from typing import List, Optional\nimport peewee\nimport re\n\nfrom db.crud import (\n    find_all_courses_relations_for_course_group_id,\n    find_all_courses_relations_for_course_id,\n)\nfrom .lecture import Lecture\nfrom .base import Base\n\n\n# for grouping courses such as SF19XY\nclass CourseGroup(Base):\n    course_code = peewee.CharField(null=False, index=True, unique=True)\n    swedish_name = peewee.CharField(null=False)\n    english_name = peewee.CharField(null=False)\n\n\n# crawled courses\nclass Course(Base):\n    course_code = peewee.CharField(null=False, index=True, unique=True)\n    group_id = peewee.ForeignKeyField(CourseGroup, null=True, backref='coursegroup')\n    swedish_name = peewee.CharField(null=False)\n    english_name = peewee.CharField(null=False)\n    points = peewee.CharField(null=False)\n    cycle = peewee.CharField(null=False)\n\n\nclass CourseLectureRelation(Base):\n    lecture_id = peewee.ForeignKeyField(Lecture, null=False, backref='lecture', on_delete='cascade')\n    course_id = peewee.ForeignKeyField(Course, null=True, backref='course', on_delete='cascade')\n    group_id = peewee.ForeignKeyField(CourseGroup, null=True, backref='coursegroup', on_delete='cascade')\n\n\n# Wrapper around both course group and courses\nclass CourseWrapper:\n\n    def __init__(\n        self,\n        course_code: str,\n        swedish_name: str,\n        english_name: str,\n        source: int,\n        points: Optional[str] = None,\n        cycle: Optional[str] = None,\n        alternate_course_codes: List[str] = [],\n        alternate_english_names: List[str] = [],\n        alternate_swedish_names: List[str] = [],\n        course_id: Optional[int] = None,\n        course_group_id: Optional[int] = None,\n    ) -> None:\n        self.course_code = course_code\n        self.swedish_name = swedish_name\n        self.english_name = english_name\n        self.points = points\n        self.cycle = cycle\n        self.source = source\n        self.alternate_course_codes = alternate_course_codes\n        self.alternate_english_names = alternate_english_names\n        self.alternate_swedish_names = alternate_swedish_names\n        self.course_id = course_id\n        self.course_group_id = course_group_id\n\n    class Source:\n        COURSE = 1\n        COURSE_GROUP = 2\n\n    def from_course(course: Course):\n        return CourseWrapper(\n            course_code=course.course_code,\n            swedish_name=course.swedish_name,\n            english_name=course.english_name,\n            points=course.points,\n            cycle=course.cycle,\n            source=CourseWrapper.Source.COURSE,\n            course_id=course.id,\n        )\n\n    def from_course_group(group: CourseGroup):\n        alternate_course_codes = []\n        alternate_english_names = []\n        alternate_swedish_names = []\n        courses = Course.filter(Course.group_id == group.id)\n        for course in courses:\n            alternate_course_codes.append(course.course_code)\n            alternate_english_names.append(course.english_name)\n            alternate_swedish_names.append(course.swedish_name)\n\n        return CourseWrapper(\n            course_code=group.course_code,\n            swedish_name=group.swedish_name,\n            english_name=group.english_name,\n            source=CourseWrapper.Source.COURSE_GROUP,\n            alternate_course_codes=alternate_course_codes,\n            alternate_english_names=alternate_english_names,\n            alternate_swedish_names=alternate_swedish_names,\n            course_group_id=group.id,\n        )\n\n    def reindex(self):\n        # Doing imports here to avoid circular imports\n        from jobs.tasks.course import index_course\n        from jobs import get_metadata_queue\n\n        q = next(get_metadata_queue())\n        q.enqueue(index_course.job, self.course_code)\n\n    def get_display_name(self) -> str:\n        regex = re.compile(r'\\D*(\\d)(\\d)*')\n        digit = re.search(regex, self.course_code).group(1).strip()\n        if int(digit) == 1:\n            return self.swedish_name\n        return self.english_name\n\n    def get_number_of_lectures(self):\n        if self.source == CourseWrapper.Source.COURSE:\n            return len(find_all_courses_relations_for_course_id(self.course_id))\n        if self.source == CourseWrapper.Source.COURSE_GROUP:\n            return len(find_all_courses_relations_for_course_group_id(self.course_group_id))\n\n    def to_doc(self) -> dict:\n        return {\n            'course_code': self.course_code,\n            'display_name': self.get_display_name(),\n            'swedish_name': self.swedish_name,\n            'english_name': self.english_name,\n            'points': self.points,\n            'cycle': self.cycle,\n            'alternate_course_codes': self.alternate_course_codes,\n            'alternate_english_names': self.alternate_english_names,\n            'alternate_swedish_names': self.alternate_swedish_names,\n            'lectures': self.get_number_of_lectures(),\n        }\n\n    def to_small_dict(self) -> dict:\n        return {\n            'course_code': self.course_code,\n            'display_name': self.get_display_name(),\n        }\n"}
{"type": "source_file", "path": "db/migrations/017_add_image_upload_id_column_to_query_table.py", "content": "\"\"\"Peewee migrations -- 017_add_image_upload_id_column_to_query_table.py.\"\"\"\nimport peewee as pw\nfrom peewee_migrate import Migrator\n\nfrom db.models import Query, ImageUpload\n\n\ndef migrate(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your migrations here.\"\"\"\n    image_upload_id = pw.ForeignKeyField(ImageUpload, null=True, backref='imageupload', on_delete='cascade')\n\n    migrator.add_fields(\n        Query,\n        image_upload_id=image_upload_id,\n    )\n    migrator.sql('ALTER TABLE query ALTER COLUMN lecture_id DROP NOT NULL;')\n    migrator.run()\n\n\ndef rollback(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your rollback migrations here.\"\"\"\n    migrator.sql('ALTER TABLE query ALTER COLUMN lecture_id SET NOT NULL;')\n    migrator.remove_fields(Query, 'image_upload_id', cascade=True)\n"}
{"type": "source_file", "path": "db/migrations/021_add_image_upload_subject_table.py", "content": "\"\"\"Peewee migrations -- 021_add_image_upload_subject_table.py.\"\"\"\nimport peewee as pw\nfrom peewee_migrate import Migrator\n\nfrom db.models import ImageUploadSubject\nfrom db.database import db\n\n\ndef migrate(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your migrations here.\"\"\"\n    db.create_tables([ImageUploadSubject])\n\n\ndef rollback(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your rollback migrations here.\"\"\"\n    db.drop_tables([ImageUploadSubject])\n"}
{"type": "source_file", "path": "db/migrations/011_add_more_cache_is_valid_column_to_query_table.py", "content": "\"\"\"Peewee migrations -- 011_add_more_cache_is_valid_column_to_query_table.py.\"\"\"\nimport peewee as pw\nfrom peewee_migrate import Migrator\n\nfrom db.models import Query\n\n\ndef migrate(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your migrations here.\"\"\"\n    field = pw.BooleanField(null=False, default=True)\n    migrator.add_fields(\n        Query,\n        cache_is_valid=field,\n    )\n    migrator.run()\n\n\ndef rollback(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your rollback migrations here.\"\"\"\n    migrator.remove_fields(Query, 'cache_is_valid', cascade=True)\n"}
{"type": "source_file", "path": "db/migrations/013_add_mathpix_requests_table.py", "content": "\"\"\"Peewee migrations -- 013_add_mathpix_requests_table.py.\"\"\"\nimport peewee as pw\nfrom peewee_migrate import Migrator\n\nfrom db.models import MathpixRequest\nfrom db.database import db\n\n\ndef migrate(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your migrations here.\"\"\"\n    db.create_tables([MathpixRequest])\n\n\ndef rollback(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your rollback migrations here.\"\"\"\n    db.drop_tables([MathpixRequest])\n"}
{"type": "source_file", "path": "db/models/__init__.py", "content": "# flake8: noqa\nfrom .url import URL\nfrom .lecture import Lecture, LectureSubject\nfrom .query import Query\nfrom .analysis import Analysis\nfrom .message import Message\nfrom .queue_info import QueueInfo\nfrom .token_usage import TokenUsage\nfrom .course import (\n    Course,\n    CourseGroup,\n    CourseWrapper,\n    CourseLectureRelation,\n)\nfrom .image_upload import ImageUpload, ImageUploadSubject\nfrom .image_question import ImageQuestion, ImageQuestionHit\nfrom .mathpix_request import MathpixRequest\n"}
{"type": "source_file", "path": "db/migrations/__init__.py", "content": "from peewee_migrate import Router\nimport sys\nimport os\n\nfrom db.database import db\nfrom config.logger import log\n\nMIGRATIONS_DIR = os.path.dirname(os.path.realpath(__file__))\n\nrouter = Router(db, migrate_dir=MIGRATIONS_DIR, logger=log())\n\n\ndef create_migration():\n    router.create(sys.argv[1])\n\n\ndef run_migrations():\n    # Run all unapplied migrations\n    router.run()\n\n\ndef rollback():\n    router.rollback()\n"}
{"type": "source_file", "path": "db/migrations/024_add_lecture_subject_table.py", "content": "\"\"\"Peewee migrations -- 024_add_lecture_subject_table.py.\"\"\"\nimport peewee as pw\nfrom peewee_migrate import Migrator\n\nfrom db.models import LectureSubject\nfrom db.database import db\n\n\ndef migrate(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your migrations here.\"\"\"\n    db.create_tables([LectureSubject])\n\n\ndef rollback(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your rollback migrations here.\"\"\"\n    db.drop_tables([LectureSubject])\n"}
{"type": "source_file", "path": "db/models/analysis.py", "content": "from datetime import datetime\nimport peewee\nimport pytz\n\nfrom .base import Base\nfrom .message import Message\n\n# 20 minutes\nFROZEN_LIMIT = 20 * 60\n\n\nclass Analysis(Base):\n    lecture_id = peewee.IntegerField(null=False, index=True)  # not sure how to make this a foreign key field, due to circular imports  # noqa: E501\n    state = peewee.CharField(null=False, default='waiting')\n    mp4_progress = peewee.IntegerField(null=False, default=0)\n    mp3_progress = peewee.IntegerField(null=False, default=0)\n    transcript_progress = peewee.IntegerField(null=False, default=0)\n    summary_progress = peewee.IntegerField(null=False, default=0)\n\n    class State:\n        WAITING = 'waiting'\n        IDLE = 'idle'\n        CLASSIFYING = 'classifying'\n        DENIED = 'denied'\n        FAILURE = 'failure'\n        DOWNLOADING = 'downloading'\n        EXTRACTING_AUDIO = 'extracting_audio'\n        TRANSCRIBING_LECTURE = 'transcribing_lecture'\n        SUMMARISING_LECTURE = 'summarising_lecture'\n        READY = 'ready'\n\n    def refresh(self):\n        update = Analysis.get(self.id)\n        self.state = update.state\n        self.mp4_progress = update.mp4_progress\n        self.mp3_progress = update.mp3_progress\n        self.transcript_progress = update.transcript_progress\n        self.summary_progress = update.summary_progress\n\n    def get_last_message(self):\n        return (Message\n                .filter(Message.analysis_id == self.id)\n                .order_by(Message.modified_at.desc())\n                .first())\n\n    def seems_to_have_crashed(self) -> bool:\n        if self.state == self.State.WAITING:\n            return False\n        if self.state == self.State.READY:\n            return False\n        if self.state == self.State.IDLE:\n            return False\n        if self.state == self.State.FAILURE:\n            return False\n\n        msg = self.get_last_message()\n        if msg is None:\n            return True\n\n        now = datetime.utcnow()\n        diff = (now - msg.created_at).total_seconds()\n\n        return diff > FROZEN_LIMIT\n\n    def overall_progress(self):\n        mp4_weight = 1\n        mp3_weight = 3\n        transcript_weight = 15\n        summary_weight = 10\n\n        weighted_mp4 = self.mp4_progress * mp4_weight\n        weighted_mp3 = self.mp3_progress * mp3_weight\n        weighted_transcript = self.transcript_progress * transcript_weight\n        weighted_summary = self.summary_progress * summary_weight\n\n        total_weight = mp4_weight + mp3_weight + transcript_weight + summary_weight\n        return int((weighted_mp4 + weighted_mp3 + weighted_transcript + weighted_summary) / total_weight)\n\n    def to_dict(self):\n        msg = self.get_last_message()\n        if msg is not None:\n            msg_dict = msg.to_dict()\n        else:\n            msg_dict = None\n\n        tz = pytz.timezone('UTC')\n        created_at = tz.localize(self.created_at, is_dst=None)\n        modified_at = tz.localize(self.modified_at, is_dst=None)\n\n        return {\n            'analysis_id': self.id,\n            'state': self.state,\n            'frozen': self.seems_to_have_crashed(),\n            'created_at': created_at.isoformat(),\n            'modified_at': modified_at.isoformat(),\n            'last_message': msg_dict,\n            'mp4_progress': self.mp4_progress,\n            'mp3_progress': self.mp3_progress,\n            'transcript_progress': self.transcript_progress,\n            'summary_progress': self.summary_progress,\n            'overall_progress': self.overall_progress(),\n        }\n"}
{"type": "source_file", "path": "db/migrations/010_add_more_queue_info_columns.py", "content": "\"\"\"Peewee migrations -- 010_add_more_queue_info_columns.py.\"\"\"\n# flake8: noqa\n\nimport peewee as pw\nfrom peewee_migrate import Migrator\n\nfrom db.models import QueueInfo\n\ntry:\n    import playhouse.postgres_ext as pw_pext\nexcept ImportError:\n    pass\n\nSQL = pw.SQL\n\n\ndef migrate(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your migrations here.\"\"\"\n    queue_approval_field = pw.IntegerField(null=False, default=0)\n    queue_metadata_field = pw.IntegerField(null=False, default=0)\n    queue_gpt_field = pw.IntegerField(null=False, default=0)\n\n    migrator.add_fields(\n        QueueInfo,\n        queue_approval=queue_approval_field,\n        queue_metadata=queue_metadata_field,\n        queue_gpt=queue_gpt_field,\n    )\n    migrator.run()\n\n\ndef rollback(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your rollback migrations here.\"\"\"\n    migrator.remove_fields(QueueInfo, 'queue_approval', cascade=True)\n    migrator.remove_fields(QueueInfo, 'queue_metadata', cascade=True)\n    migrator.remove_fields(QueueInfo, 'queue_gpt', cascade=True)\n"}
{"type": "source_file", "path": "db/migrations/015_add_image_question_table.py", "content": "\"\"\"Peewee migrations -- 015_add_image_question_table.py.\"\"\"\nimport peewee as pw\nfrom peewee_migrate import Migrator\n\nfrom db.models import ImageQuestion\nfrom db.database import db\n\n\ndef migrate(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your migrations here.\"\"\"\n    db.create_tables([ImageQuestion])\n\n\ndef rollback(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your rollback migrations here.\"\"\"\n    db.drop_tables([ImageQuestion])\n"}
{"type": "source_file", "path": "db/migrations/022_add_classify_subjects_columns_to_imageupload.py", "content": "\"\"\"Peewee migrations -- 022_add_classify_subjects_columns_to_imageupload.py.\"\"\"\nimport peewee as pw\nfrom peewee_migrate import Migrator\n\nfrom db.models import ImageUpload\n\n\ndef migrate(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your migrations here.\"\"\"\n    classify_subjects_ok = pw.BooleanField(null=True)\n    classify_subjects_failure_reason = pw.TextField(null=True)\n\n    migrator.add_fields(\n        ImageUpload,\n        classify_subjects_ok=classify_subjects_ok,\n        classify_subjects_failure_reason=classify_subjects_failure_reason,\n    )\n    migrator.run()\n\n\ndef rollback(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your rollback migrations here.\"\"\"\n    migrator.remove_fields(ImageUpload, 'classify_subjects_ok', cascade=True)\n    migrator.remove_fields(ImageUpload, 'classify_subjects_failure_reason', cascade=True)\n"}
{"type": "source_file", "path": "db/migrations/016_add_group_column_to_lecture_table.py", "content": "\"\"\"Peewee migrations -- 016_add_group_column_to_lecture_table.py.\"\"\"\nimport peewee as pw\nfrom peewee_migrate import Migrator\n\nfrom db.models import Lecture\n\n\ndef migrate(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your migrations here.\"\"\"\n    field = pw.CharField(null=True)\n    migrator.add_fields(\n        Lecture,\n        group=field,\n    )\n    migrator.run()\n\n\ndef rollback(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your rollback migrations here.\"\"\"\n    migrator.remove_fields(Lecture, 'group', cascade=True)\n"}
{"type": "source_file", "path": "db/models/base.py", "content": "from datetime import datetime\nimport peewee\n\nfrom db.database import db\n\n\nclass Base(peewee.Model):\n    created_at = peewee.TimestampField(default=datetime.utcnow, null=False)\n    modified_at = peewee.TimestampField(default=datetime.utcnow, null=False)\n\n    class Meta:\n        database = db\n\n    class Config:\n        orm_mode = True\n\n    def save(self, *args, **kwargs):\n        self.modified_at = datetime.utcnow()\n        return super(Base, self).save(*args, **kwargs)\n"}
{"type": "source_file", "path": "courses/cmd.py", "content": "from db.crud import find_course_by_course_code\nfrom tools.web.crawler import (\n    scrape_course_groups,\n    scrape_courses_from_group,\n)\nfrom db.models import Course\n\n\ndef fetch_kth_courses():\n    course_groups = scrape_course_groups()\n    print(f'found {len(course_groups)} course groups')\n\n    courses = {}\n\n    for url in course_groups:\n        print(f'crawling {url}')\n\n        courses_in_group = scrape_courses_from_group(url)\n        print(f'found {len(courses_in_group)} courses')\n\n        for course in courses_in_group:\n            code = course[0]\n            name = course[1]\n            points = course[2]\n            cycle = course[3]\n\n            if code not in courses:\n                courses[code] = {\n                    'english_name': '',\n                    'swedish_name': '',\n                    'cycle': '',\n                    'points': '',\n                }\n\n            if '?l=en' in url:\n                courses[code]['english_name'] = name\n                courses[code]['cycle'] = cycle\n                courses[code]['points'] = points\n\n            if '?l=sv' in url:\n                courses[code]['swedish_name'] = name\n\n    print(f'total courses found: {len(courses.keys())}')\n    print('saving courses to database')\n    new = 0\n    seen = 0\n    for course_code in courses:\n        course = find_course_by_course_code(course_code)\n        if course is None:\n            new += 1\n            course = Course(\n                course_code=course_code,\n                swedish_name=courses[course_code]['swedish_name'],\n                english_name=courses[course_code]['english_name'],\n                points=courses[course_code]['points'],\n                cycle=courses[course_code]['cycle'],\n            )\n        else:\n            seen += 1\n            course.course_code = course_code\n            course.swedish_name = courses[course_code]['swedish_name']\n            course.english_name = courses[course_code]['english_name']\n            course.points = courses[course_code]['points']\n            course.cycle = courses[course_code]['cycle']\n\n        course.save()\n\n    print(f'done. found {new} new courses, and updated {seen} existing courses.')\n"}
{"type": "source_file", "path": "db/migrations/019_add_query_hash_column_to_image_question_table.py", "content": "\"\"\"Peewee migrations -- 019_add_query_hash_column_to_image_question_table.py.\"\"\"\nimport peewee as pw\nfrom peewee_migrate import Migrator\n\nfrom db.models import ImageQuestion\n\n\ndef migrate(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your migrations here.\"\"\"\n    field = pw.CharField(index=True, null=True)\n    migrator.add_fields(\n        ImageQuestion,\n        query_hash=field,\n    )\n    migrator.run()\n\n\ndef rollback(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your rollback migrations here.\"\"\"\n    migrator.remove_fields(ImageQuestion, 'query_hash', cascade=True)\n"}
{"type": "source_file", "path": "db/migrations/018_add_image_question_hit_table.py", "content": "\"\"\"Peewee migrations -- 018_add_image_question_hit_table.py.\"\"\"\nimport peewee as pw\nfrom peewee_migrate import Migrator\n\nfrom db.models import ImageQuestionHit\nfrom db.database import db\n\n\ndef migrate(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your migrations here.\"\"\"\n    db.create_tables([ImageQuestionHit])\n\n\ndef rollback(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your rollback migrations here.\"\"\"\n    db.drop_tables([ImageQuestionHit])\n"}
{"type": "source_file", "path": "db/migrations/020_add_title_to_imageupload.py", "content": "\"\"\"Peewee migrations -- 020_add_title_to_imageupload.py.\"\"\"\nimport peewee as pw\nfrom peewee_migrate import Migrator\n\nfrom db.models import ImageUpload\n\n\ndef migrate(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your migrations here.\"\"\"\n    title = pw.CharField(null=True)\n    create_title_ok = pw.BooleanField(null=True)\n    create_title_failure_reason = pw.TextField(null=True)\n\n    migrator.add_fields(\n        ImageUpload,\n        title=title,\n        create_title_ok=create_title_ok,\n        create_title_failure_reason=create_title_failure_reason,\n    )\n    migrator.run()\n\n\ndef rollback(migrator: Migrator, database: pw.Database, fake=False, **kwargs):\n    \"\"\"Write your rollback migrations here.\"\"\"\n    migrator.remove_fields(ImageUpload, 'title', cascade=True)\n    migrator.remove_fields(ImageUpload, 'create_title_ok', cascade=True)\n    migrator.remove_fields(ImageUpload, 'create_title_failure_reason', cascade=True)\n"}
