{"repo_info": {"repo_name": "Pulsar", "repo_owner": "astramind-ai", "repo_url": "https://github.com/astramind-ai/Pulsar"}}
{"type": "source_file", "path": "app/api/__init__.py", "content": ""}
{"type": "source_file", "path": "app/__init__.py", "content": ""}
{"type": "source_file", "path": "app/db/lora/__init__.py", "content": ""}
{"type": "source_file", "path": "app/api/loras.py", "content": "import os\nimport shutil\nimport uuid\nfrom typing import Union\n\nfrom fastapi import APIRouter, Depends, Form, UploadFile, HTTPException\nfrom sqlalchemy import select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom starlette.responses import JSONResponse\nfrom vllm.lora.request import LoRARequest\n\nfrom app.db.auth.auth_db import get_current_user, set_last_model_lora, get_user\nfrom app.db.lora.lora_db import get_lora_list, get_lora, get_user_lora, hash_lora_str_id, is_lora_correct, \\\n    edit_lora_async, create_lora_async, delete_lora_async, get_user_lora_by_url, change_lora_id_and_owner\nfrom app.utils.models.model_paths import get_hf_path\nfrom app.db.model.auth import User\nfrom app.db.model.lora import LoRA\nfrom app.utils.database.get import get_db\nfrom app.utils.database.images import save_image, delete_image\nfrom app.utils.formatting.pydantic.request import ImageRequest\nfrom app.utils.log import setup_custom_logger\nfrom app.utils.models.hf_downloader import download_model_async, check_file_in_huggingface_repo\nfrom app.utils.models.list_model import list_loras_hf\nfrom app.utils.formatting.pydantic.privacy import PrivacyOptions\nfrom app.utils.server.image_fetch import get_image\nfrom app.utils.definitions import UPLOAD_DIRECTORY, MODEL_PATHS\nrouter = APIRouter()\n\nlogger = setup_custom_logger(__name__)\n\nlora_chat_template_dict = {}\n\n@router.get(\"/lora_image/{image_name}\")\nasync def get_router_image(image_name: str):\n    image_request = ImageRequest(image_name=image_name)\n    return await get_image(image_request)\n\n\n@router.post(\"/lora/download\")\nasync def download_lora(lora_id: str = Form(...),\n                        lora_name: str = Form(...),\n                        lora_description: str = Form(...),\n                        lora_image: Union[UploadFile, str] = Form(...),\n                        lora_url: str = Form(...),\n                        owner: str = Form(...),\n                        base_architecture: str = Form(None),\n                        current_user: User = Depends(get_current_user),\n                        db: AsyncSession = Depends(get_db)):\n    lora = await get_lora(db, lora_url)\n    image_name = await save_image(current_user, lora_image)\n    if lora:\n        if lora.id != lora_id and lora.owner == 'everyone':\n            lora.id = lora_id\n            lora.description = lora_description\n            lora.name = lora_name\n\n            lora.image = image_name\n            lora.owner = owner\n\n\n\n            if current_user not in lora.users:\n\n                db_user = await get_user(db, current_user.name)\n                lora.users.append(db_user)\n            else:\n                return JSONResponse(content={\"message\": \"You already have this LoRA\"})\n        else:\n            if current_user not in lora.users:\n                db_user = await get_user(db, current_user.name)\n                lora.users.append(db_user)\n            else:\n                return JSONResponse(content={\"message\": \"You already have this LoRA\"})\n        await db.commit()\n        return JSONResponse(content={\"message\": \"LoRA downloaded successfully\"})\n    # If it's new, download it\n    try:\n        await download_model_async(lora_url)\n\n        lora = LoRA(id=lora_id, name=lora_name.split(\"/\")[-1],\n                    url=lora_url, path=os.path.join(MODEL_PATHS, await get_hf_path(lora_url)),\n                    description=lora_description, users=[current_user], base_architecture=base_architecture,\n                    image=image_name,\n                    owner=owner)\n        db.add(lora)\n        await db.commit()\n        return JSONResponse(content={\"message\": \"LoRA downloaded successfully\"})\n    except Exception as e:\n        raise HTTPException(detail=f\"Error downloading LoRA: {str(e)}\", status_code=500)\n\n\n@router.put(\"/lora/edit\")\nasync def lora_edit(\n        lora_id: str = Form(...),\n        lora_name: str = Form(None),\n        tags: str = Form(None),\n        lora_description: str = Form(None),\n        image: Union[UploadFile, str] = Form(None),\n        privacy_settings: PrivacyOptions = Form(None),\n        online_id: str = Form(None),\n        current_user: User = Depends(get_current_user),\n        db: AsyncSession = Depends(get_db)):\n    lora = await get_user_lora(current_user, db, lora_id)\n    if not lora:\n        raise HTTPException(detail=\"LoRA not found\", status_code=404)\n\n    try:\n        if lora_name:\n            lora.name = lora_name\n\n        if lora_description:\n            lora.description = lora_description\n        if image:\n            new_filename = await save_image(current_user, image)\n            await delete_image(lora)\n            lora.image = new_filename\n        if online_id:\n            lora.id = online_id\n        db.add(lora)\n        await db.commit()\n        if privacy_settings and privacy_settings.value != \"local\":\n            try:\n                await edit_lora_async(db, tags=tags, name=lora_name, description=lora_description, image=image,\n                                      is_private=True if privacy_settings.value == \"private\" else False,\n                                      lora_id=lora.id)\n            except Exception as e:\n                raise HTTPException(detail=f\"Error pushing edited model online: {str(e)}\",\n                                    status_code=500)\n    except Exception as e:\n        await db.rollback()\n        raise HTTPException(detail=f\"Error editing model: {str(e)}\", status_code=500)\n    return JSONResponse(content={\"message\": \"Model edited successfully\"})\n\n\n@router.delete(\"/lora/delete\")\nasync def delete_lora(lora_id: str = Form(...),\n                      also_delete_online: bool = Form(False),\n                      current_user: User = Depends(get_current_user),\n                      db: AsyncSession = Depends(get_db)):\n    if also_delete_online:\n        await delete_lora_async(db, lora_id)\n\n    lora = await get_user_lora(current_user, db, lora_id)\n    if not lora and not also_delete_online:\n        return HTTPException(detail=\"Model not found are you sure you have the permissions to see this model?\",\n                             status_code=404)\n    elif not lora:\n        return JSONResponse(content={\"message\": \"Model deleted successfully\"})\n    try:\n        if len(lora.users) < 2:\n            await delete_image(lora)\n            await db.delete(lora)\n            if os.path.exists(lora.path):\n                shutil.rmtree(lora.path)\n        else:\n            lora.users = [user for user in lora.users if user.id != current_user.id]\n\n    except Exception as e:\n        await db.rollback()\n        raise HTTPException(detail=f\"Error deleting model: {str(e)}\", status_code=500)\n    await db.commit()\n\n    return JSONResponse(content={\"message\": \"Model deleted successfully\"})\n\n\n@router.post(\"/lora/create\")\nasync def create_lora(lora_name: str = Form(...),\n                      lora_url: str = Form(...),\n                      lora_tags: str = Form(None),\n                      lora_description: str = Form(...),\n                      lora_image: Union[UploadFile, str] = Form(None),\n                      privacy_settings: PrivacyOptions = Form(...),\n                      hf_model_id: str = Form(...),\n                      current_user: User = Depends(get_current_user),\n                      db: AsyncSession = Depends(get_db)):\n    lora = await get_user_lora_by_url(current_user, db, lora_url)\n    if lora:\n        if privacy_settings.value != \"local\":\n            try:\n                _ = await create_lora_async(db, tags=lora_tags, name=lora_name,\n                                            description=lora_description,\n                                            image=lora_image,\n                                            is_private=True if privacy_settings.value == \"private\" else False,\n                                            model_id=hf_model_id, url=lora_url)\n                return JSONResponse(content={\"message\": \"LoRA already exists locally, pushed online successfully\"})\n            except Exception as e:\n                raise HTTPException(status_code=500, detail=\"LoRA creation failed, \" + str(e))\n        else:\n            raise HTTPException(status_code=409, detail=\"LoRA url already exists\")\n    image_name = await save_image(current_user, lora_image)\n    await download_model_async(lora_url)\n    lora_path = os.path.join(MODEL_PATHS, await get_hf_path(\n        lora_url))\n    base_architecture = await check_file_in_huggingface_repo(lora_url, \"adapter_config.json\")\n    if not base_architecture:\n        raise HTTPException(status_code=500,\n                            detail=\"LoRA architecture could not be inferred, \"\n                                   \"please check if the model is a correct adapter model.\")\n    if privacy_settings.value != \"local\":\n        try:\n            json_response = await create_lora_async(db, tags=lora_tags, name=lora_name,\n                                                    description=lora_description,\n                                                    image=lora_image,\n                                                    is_private=True if privacy_settings.value == \"private\" else False,\n                                                    model_id=hf_model_id, url=lora_url)\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=\"LoRA creation failed, \" + str(e))\n        # use the id returned form the call\n        if not json_response.get('owner', None):\n            raise HTTPException(status_code=500,\n                                detail=\"LoRA creation failed, no ID returned from the main server\")\n        lora = LoRA(id=json_response['id'], name=lora_name, path=lora_path,\n                    owner=json_response['owner'], image=image_name, base_architecture=base_architecture,\n                    url=lora_url, description=lora_description,\n                    users=[current_user])\n    else:\n\n        lora = LoRA(id=uuid.uuid4().hex, name=lora_name, path=lora_path,\n                    owner=current_user.name, image=image_name, base_architecture=base_architecture,\n                    url=lora_url, description=lora_description,\n                    users=[current_user])\n\n    db.add(lora)\n    await db.commit()\n    await db.refresh(lora)\n    return {\"status\": \"LoRA created successfully\"}\n\n\n@router.get(\"/lora/list\")\nasync def show_available_models(db: AsyncSession = Depends(get_db), current_user: User = Depends(get_current_user)):\n    loras_list = await get_lora_list(current_user, db)\n    loras_final_list = []\n    for lora in loras_list:\n        loras = {}\n        for var in vars(lora):\n            if var != \"users\" and var != \"model\" and var != \"chats\" and var != \"completions\" and var != \"messages\":\n                loras[var] = getattr(lora, var)\n        loras_final_list.append(loras)\n\n    return {\"loras\": loras_final_list}\n\n\n@router.post(\"/lora/load\")\nasync def load_model(lora_url: str = Form(...),\n                     current_user: User = Depends(get_current_user),\n                     db: AsyncSession = Depends(get_db)):\n    global lora_chat_template_dict\n\n    lora = await get_user_lora_by_url(current_user, db, lora_url)\n    if not lora:\n        raise HTTPException(detail=\"LoRA not found\", status_code=404)\n    is_correct, error_maybe_template = await is_lora_correct(lora, db)\n    if not is_correct:\n        raise HTTPException(detail=error_maybe_template, status_code=404)\n\n    lora_int_id = await hash_lora_str_id(lora.url)\n    await set_last_model_lora(db, current_user, lora_id=lora.url)\n    lora_request = LoRARequest(lora_int_id=lora_int_id, lora_name=lora.url,\n                               lora_path=await list_loras_hf(lora.path))\n\n    try:\n        from app.core.engine import openai_serving_chat\n        lora_list = {lora.lora_int_id for lora in openai_serving_chat.lora_requests}\n        if lora_int_id in lora_list:\n            return JSONResponse(content={\"message\": \"Model already loaded\"}, status_code=200)\n        is_lora_loaded = openai_serving_chat.engine_client.engine.add_lora(lora_request)\n        if is_lora_loaded:\n            openai_serving_chat.lora_requests.append(lora_request)\n    except Exception as e:\n        return JSONResponse(content={\"message\": f\"Error loading model: {str(e)}\"}, status_code=500)\n\n    lora_chat_template_dict[lora.url] = error_maybe_template\n    return JSONResponse(content={\"message\": \"Model loaded successfully\" if is_lora_loaded else \"Could not be loaded\"},\n                        status_code=200)\n\n\n@router.post(\"/lora/unload\")\nasync def unload_model(lora_url: str = Form(...),\n                       current_user: User = Depends(get_current_user),\n                       db: AsyncSession = Depends(get_db)):\n    global lora_chat_template_dict\n\n    lora = await get_user_lora_by_url(current_user, db, lora_url)\n    if not lora:\n        return JSONResponse(content={\"message\": \"Model not found\"}, status_code=404)\n\n    lora_int_id = await hash_lora_str_id(lora.url)\n    await set_last_model_lora(db, current_user, lora_id='')\n\n    try:\n        from app.core.engine import openai_serving_chat\n        lora_list = {lora.lora_int_id for lora in openai_serving_chat.lora_requests}\n        if lora_int_id not in lora_list:\n            return JSONResponse(content={\"message\": \"Model not loaded\"}, status_code=404)\n        is_lora_unloaded = openai_serving_chat.engine_client.engine.remove_lora(lora_int_id)\n        if not is_lora_unloaded:\n            return JSONResponse(content={\"message\": \"Could not be unloaded\"}, status_code=404)\n        openai_serving_chat.lora_requests = [lora for lora in openai_serving_chat.lora_requests if\n                                             lora.lora_int_id != lora_int_id]\n    except Exception as e:\n        return JSONResponse(content={\"message\": f\"Error unloading model: {str(e)}\"}, status_code=500)\n\n    lora_chat_template_dict.pop(lora.url, None)\n    return JSONResponse(\n        content={\"message\": \"Model unloaded successfully\" if is_lora_unloaded else \"Could not be unloaded\"},\n        status_code=200)\n\n\n@router.post(\"/lora/push_online\")\nasync def push_lora_online(\n        lora_name: str = Form(...),\n        lora_local_id: str = Form(...),\n        lora_description: str = Form(None),\n        lora_url: str = Form(...),\n        id_model: str = Form(...),\n        lora_tags: str = Form(None),\n        image: Union[UploadFile, str] = Form(None),\n        is_private: bool = Form(False),\n        is_new_item: bool = Form(False),\n        current_user: User = Depends(get_current_user),\n        db: AsyncSession = Depends(get_db)):\n    if not is_new_item:\n        await edit_lora_async(db, lora_id=lora_local_id, name=lora_name, description=lora_description, image=image,\n                              is_private=is_private)\n    else:\n        try:\n            response = await create_lora_async(db, lora_name, lora_description, image, id_model, lora_url, is_private,\n                                               tags=lora_tags)\n            await change_lora_id_and_owner(db, lora_local_id, response['id'], response['owner'], current_user)\n        except Exception as e:\n            raise HTTPException(detail=f\"Error pushing lora online: {str(e)}\", status_code=500)\n    return JSONResponse(content={\"message\": \"LoRA pushed successfully\"})\n"}
{"type": "source_file", "path": "app/api/open_ai.py", "content": "import asyncio\nimport tempfile\nfrom typing import Optional, Set\n\nfrom fastapi import APIRouter, Request, HTTPException, Depends\nfrom fastapi.responses import JSONResponse, StreamingResponse\nfrom typing_extensions import assert_never\nfrom vllm.config import ModelConfig\nfrom vllm.engine.arg_utils import AsyncEngineArgs\nfrom vllm.entrypoints.openai.protocol import (ChatCompletionRequest,\n                                              ChatCompletionResponse,\n                                              CompletionRequest,\n                                              DetokenizeRequest,\n                                              DetokenizeResponse,\n                                              EmbeddingRequest,\n                                              EmbeddingResponse, ErrorResponse,\n                                              TokenizeRequest,\n                                              TokenizeResponse,\n                                              )\n# yapf: enable\nfrom vllm.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom vllm.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom vllm.entrypoints.openai.serving_embedding import OpenAIServingEmbedding\nfrom vllm.entrypoints.openai.serving_tokenization import (\n    OpenAIServingTokenization)\nfrom vllm.logger import init_logger\n\nfrom app.db.auth.auth_db import auth_user_with_local_exception\nfrom app.db.model.auth import User\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n\nengine_args: AsyncEngineArgs\nopenai_serving_chat: OpenAIServingChat\nopenai_serving_completion: OpenAIServingCompletion\nopenai_serving_embedding: OpenAIServingEmbedding\nopenai_serving_tokenization: OpenAIServingTokenization\nprometheus_multiproc_dir: tempfile.TemporaryDirectory\n\n# Cannot use __name__ (https://github.com/vllm-project/vllm/pull/4765)\nlogger = init_logger('vllm.entrypoints.openai.api_server')\n\n_running_tasks: Set[asyncio.Task] = set()\n\n\ndef load_serving_entrypoints() -> None:\n    global openai_serving_chat, openai_serving_completion, openai_serving_embedding, openai_serving_tokenization\n    from app.core.engine import openai_serving_tokenization, openai_serving_chat, openai_serving_embedding\n    openai_serving_tokenization, openai_serving_chat, openai_serving_embedding = (\n        openai_serving_tokenization, openai_serving_chat, openai_serving_embedding)\n\n\ndef model_is_embedding(model_name: str, trust_remote_code: bool,\n                       quantization: Optional[str]) -> bool:\n    return ModelConfig(model=model_name,\n                       tokenizer=model_name,\n                       tokenizer_mode=\"auto\",\n                       trust_remote_code=trust_remote_code,\n                       quantization=quantization,\n                       seed=0,\n                       dtype=\"auto\").embedding_mode\n\n\nrouter = APIRouter()\n\n\n@router.post(\"/tokenize\")\nasync def tokenize(request: TokenizeRequest, current_user: User = Depends(auth_user_with_local_exception)):\n    generator = await openai_serving_tokenization.create_tokenize(request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(content=generator.model_dump(),\n                            status_code=generator.code)\n    elif isinstance(generator, TokenizeResponse):\n        return JSONResponse(content=generator.model_dump())\n\n    assert_never(generator)\n\n\n@router.post(\"/detokenize\")\nasync def detokenize(request: DetokenizeRequest, current_user: User = Depends(auth_user_with_local_exception)):\n    generator = await openai_serving_tokenization.create_detokenize(request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(content=generator.model_dump(),\n                            status_code=generator.code)\n    elif isinstance(generator, DetokenizeResponse):\n        return JSONResponse(content=generator.model_dump())\n\n    assert_never(generator)\n\n\n@router.post(\"/v1/chat/completions\")\nasync def create_chat_completion(request: ChatCompletionRequest,\n                                 raw_request: Request, current_user: User = Depends(auth_user_with_local_exception)):\n    generator = await openai_serving_chat.create_chat_completion(\n        request, raw_request)\n\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(content=generator.model_dump(),\n                            status_code=generator.code)\n\n    elif isinstance(generator, ChatCompletionResponse):\n        return JSONResponse(content=generator.model_dump())\n\n    return StreamingResponse(content=generator, media_type=\"text/event-stream\")\n\n\n@router.post(\"/v1/completions\")\nasync def create_completion(request: CompletionRequest, raw_request: Request,\n                            current_user: User = Depends(auth_user_with_local_exception)):\n    return HTTPException(status_code=501, detail=\"Developers should use the /v1/chat/completions endpoint instead.\")\n\n\n@router.post(\"/v1/embeddings\")\nasync def create_embedding(request: EmbeddingRequest, raw_request: Request,\n                           current_user: User = Depends(auth_user_with_local_exception)):\n    generator = await openai_serving_embedding.create_embedding(\n        request, raw_request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(content=generator.model_dump(),\n                            status_code=generator.code)\n    elif isinstance(generator, EmbeddingResponse):\n        return JSONResponse(content=generator.model_dump())\n\n    assert_never(generator)\n"}
{"type": "source_file", "path": "app/db/chat/chat_db.py", "content": "import json\nimport uuid\nfrom collections import defaultdict\nfrom typing import List, Optional, Union\n\nfrom sqlalchemy import select, and_\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom ..model.chat import Chat, Message\nfrom ..model.personality import Personality\nfrom ...utils.definitions import ALLOWED_MESSAGE_FIELDS\n\n\nasync def _get_chat_history(db: AsyncSession, chat_id: str, up_to_message_id: str = None):\n    # Base query\n    query = select(Message).where(Message.chat_id == chat_id)\n\n    if up_to_message_id:\n        # Subquery to get the timestamp of the up_to_message_id\n        subquery = select(Message.timestamp).where(and_(\n            Message.chat_id == chat_id,\n            Message.id == up_to_message_id\n        )).scalar_subquery()\n\n        # Main query: get all messages up to and including the up_to_message_id\n        query = query.where(Message.timestamp <= subquery)\n\n    # Order by timestamp\n    query = query.order_by(Message.timestamp)\n\n    result = await db.execute(query)\n    return result.scalars().all()\n\n\nasync def async_unpack_chat_history(\n        db: AsyncSession,\n        chat: Union[Chat, str],\n        up_to_message_id: Optional[str] = None,\n        message_ids: Optional[List[str]] = None,\n        personality: Optional[Personality] = None,\n        full_history: bool = False\n) -> List[dict]:\n    # Get the chat history\n    chat_id = chat.id if isinstance(chat, Chat) else chat\n    messages = await _get_chat_history(db, chat_id, up_to_message_id)\n\n    # Unpack the messages\n    return unpack_messages(messages, personality, message_ids, full_history)\n\n\nasync def validate_uuid(uuid_id: str):\n    if len(uuid_id) != 32:\n        return False\n    return True\n\n\ndef unpack_multimodal_content(content: str) -> Union[List[dict], dict]:\n    if isinstance(content, str) and content.startswith(\"<_MultiModalContent_>\"):\n        try:\n            return json.loads(content[len(\"<_MultiModalContent_>\"):])\n        except json.JSONDecodeError:\n            return [{\"type\": \"text\", \"text\": content}]\n    return content  # Returns just the content if it is not multimodal\n\n\ndef unpack_messages(messages: List[Message], personality: Optional[Personality], message_ids: Optional[List[str]],\n                    full_history: Optional[bool] = True) -> List[dict]:\n    message_dict = {msg.id: msg for msg in messages}\n    message_id_set = set(message_ids) if message_ids else set()\n\n    # Group messages by their parent_message_id\n    message_groups = defaultdict(list)\n    for msg in messages:\n        parent_id = msg.parent_message_id or uuid.uuid4().hex\n        message_groups[parent_id].append(msg)\n\n    def get_message_chain(start_msg_id: str) -> List[dict]:\n        chain = []\n        current_msg_id = start_msg_id\n\n        while current_msg_id:\n            msg = message_dict.get(current_msg_id)\n            if not msg:\n                break\n\n            group = message_groups.get(msg.parent_message_id or uuid.uuid4().hex, [])\n\n            if len(group) > 1 and message_ids:\n                selected_msg = next((m for m in group if m.id in message_id_set), group[0])\n            else:\n                selected_msg = msg\n\n            if not (personality and selected_msg != messages[-2] and selected_msg.content.get(\n                    'role') == \"system\") and isinstance(selected_msg.content, dict):\n                unpacked_msg = selected_msg.content.copy()\n                unpacked_msg['id'] = selected_msg.id\n                unpacked_msg['version'] = selected_msg.version\n                unpacked_msg['parent_message_id'] = selected_msg.parent_message_id\n\n                # Gestione del contenuto multimodale\n                if 'content' in unpacked_msg:\n                    unpacked_msg['content'] = unpack_multimodal_content(unpacked_msg['content'])\n\n                chain.append(unpacked_msg)\n\n            current_msg_id = selected_msg.parent_message_id\n\n        return list(reversed(chain))  # Reverse to get chronological order\n\n    if messages:\n        if full_history:\n            full_messages_list = []\n            for message in messages:\n                message_dict = {}\n                for var in vars(message):\n                    if var in ALLOWED_MESSAGE_FIELDS:\n                        if var != 'content':\n                            message_dict[var] = vars(message)[var]\n                        else:\n                            message_attr = vars(message)[var]\n                            message_attr['content'] = unpack_multimodal_content(message_attr['content'])\n                            message_dict.update(unpack_multimodal_content(message_attr))  # it already returns a dict\n                full_messages_list.append(message_dict)\n            return full_messages_list\n\n        # Start from the last message and build the chain backwards\n        last_message = messages[-1]\n        return get_message_chain(last_message.id)\n    else:\n        return []\n"}
{"type": "source_file", "path": "app/core/engine.py", "content": "import asyncio\nimport copy\nimport gc\nfrom typing import Optional, Union\n\nimport torch\n#from faster_whisper import WhisperModel\nfrom vllm import AsyncLLMEngine\nfrom vllm.config import ModelConfig\nfrom vllm.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom vllm.entrypoints.openai.serving_embedding import OpenAIServingEmbedding\nfrom vllm.entrypoints.openai.serving_engine import BaseModelPath\nfrom vllm.entrypoints.openai.serving_tokenization import OpenAIServingTokenization\nfrom vllm.usage.usage_lib import UsageContext\n\nfrom .fallback.picker import pick_a_quantized_fallback\nfrom .whisper import get_optimal_whisper\nfrom ..hijacks.openai import ExtendedOpenAIServingChat\nfrom ..hijacks.vllm import ExtendedAsyncEngineArgs, ExtendedAsyncCompleteServerArgs\nfrom ..utils.log import setup_custom_logger\nfrom ..utils.models.tokenizer_template_inferrer import maybe_get_chat_template\nfrom ..utils.server.engine_utils import find_max_seq_len\n\n# Global variables\n#transcriber: Optional[WhisperModel] = None\nopenai_serving_chat: Optional[ExtendedOpenAIServingChat] = None\nopenai_serving_completion: Optional[OpenAIServingCompletion] = None\nopenai_serving_embedding: Optional[OpenAIServingEmbedding] = None\nopenai_serving_tokenization: Optional[OpenAIServingTokenization] = None\nasync_engine: Optional[AsyncLLMEngine] = None\nasync_engine_args: Optional[ExtendedAsyncEngineArgs] = None\nmodel_config: Optional[ModelConfig] = None\nis_lora_enabled: Optional[bool] = None\nis_model_vision: Optional[bool] = None\nimages_per_prompt: Optional[int] = None\n\nlogger = setup_custom_logger(__name__)\n\n\ndef delete_engine_model_from_vram() -> None:\n    \"\"\"Delete the model and all the gc's references to free the VRAM.\"\"\"\n    global async_engine, openai_serving_chat, openai_serving_completion\n    logger.debug(\"Deleting the engine and all the gc's references to free the VRAM\")\n    try:\n        del async_engine.engine.model_executor.driver_worker.model_runner\n        del async_engine.engine.model_executor.driver_worker\n        del async_engine\n        del openai_serving_chat, openai_serving_completion\n    except AttributeError:  # already unloaded\n        pass\n    gc.collect()\n    torch.cuda.synchronize()\n    torch.cuda.empty_cache()\n\n\ndef get_engine_args(args: Union[dict, ModelConfig]) -> Optional[ExtendedAsyncEngineArgs]:\n    \"\"\"Get engine arguments from either a dictionary or ModelConfig object.\"\"\"\n    if isinstance(args, dict):\n        return ExtendedAsyncEngineArgs(**args)\n    elif isinstance(args, ModelConfig):\n        return ExtendedAsyncEngineArgs.from_model_conf(vars(args))\n    return None\n\n\nasync def initialize_transcriber() -> None:\n    \"\"\"Initialize the transcriber. Not implemented in this version.\"\"\"\n    global transcriber\n    logger.info(\"Initializing the transcriber\")\n    transcriber = get_optimal_whisper()\n\n\nasync def initialize_engine(engine_args: ExtendedAsyncEngineArgs, usage_context: UsageContext.ENGINE_CONTEXT) -> None:\n    \"\"\"Initialize the engine with the given arguments.\"\"\"\n    global async_engine, model_config, async_engine_args, is_lora_enabled, is_model_vision, images_per_prompt\n    async_engine = None\n    async_engine_args = engine_args\n\n    retries = 0\n    original_engine_args = copy.copy(engine_args)\n    while not async_engine and retries <= 3:\n        try:\n            async_engine = AsyncLLMEngine.from_engine_args(engine_args, usage_context=usage_context)\n            tokenizer = await async_engine.get_tokenizer()\n            if not tokenizer.chat_template:\n                async_engine = None  # to throw AttributeError and to not delete the variable\n                raise RuntimeError(\n                    \"Chat template is not defined in the tokenizer, this is probably not an instruction tuned model. \"\n                    \"Please use another model\"\n                )\n            try:\n                model_config = await async_engine.get_model_config()\n            except RuntimeError:  # Handle if not within an async context\n                model_config = asyncio.run(async_engine.get_model_config())\n\n        except (torch.cuda.OutOfMemoryError, RuntimeError, ValueError) as e:\n            if retries < 3:\n                if await handle_specific_errors(e, engine_args):\n                    continue\n                log_message = f\"Failed to initialize the engine due to: {str(e)}. Retry {retries + 1}\"\n                logger.error(log_message)\n                delete_engine_model_from_vram()\n                if isinstance(e, torch.cuda.OutOfMemoryError):\n                    if not engine_args.max_model_len:\n                        # we do this so that at the next iter we can try to auto gauge the max_model_len\n                        engine_args.max_model_len = 32768\n                    else:\n                        engine_args.max_model_len = min(int(engine_args.max_model_len / 2), 8172)\n                    if retries < 4:\n                        retries += 1\n                        continue  # continue without decreasing the gpu mem utilization\n\n                engine_args.gpu_memory_utilization = engine_args.gpu_memory_utilization - 0.1\n                engine_args.swap_space += 2\n            else:\n                handle_final_retry(engine_args, original_engine_args, e)\n            retries += 1\n            continue  # Continue to retry initialization\n\n        except Exception as e:\n            logger.error(f\"Failed to initialize the engine due to: {str(e)}\")\n            raise RuntimeError(f\"Failed to initialize the engine due to: {str(e)}\") from e\n\n        if async_engine:\n            is_lora_enabled = engine_args.enable_lora\n            is_model_vision = async_engine.engine.model_config.multimodal_config is not None\n            images_per_prompt = (\n                async_engine.engine.model_config.multimodal_config.limit_per_prompt.get(\"image\", 0)\n                if is_model_vision\n                else 0\n            )\n        break  # Break the loop if engine is initialized successfully\n\n    if not async_engine:\n        logger.error(\n            f\"Model {engine_args.model} is too big for your current setup, \"\n            f\"please choose a quantized version or try with a different model\"\n        )\n        raise RuntimeError(\n            f\"Model {engine_args.model} is too big for your current setup, \"\n            f\"please choose a quantized version or try with a different model\"\n        )\n\n\nasync def handle_specific_errors(e: Exception, engine_args: ExtendedAsyncEngineArgs) -> bool:\n    \"\"\"Handle specific errors during engine initialization.\"\"\"\n    if any(sub_string in str(e) for sub_string in ['max_num_batched_tokens', 'does not support LoRA']):\n        engine_args.enable_lora = False\n        logger.error(\"Due to vllm kernel limitations, lora will be disabled for this model\")\n        delete_engine_model_from_vram()\n        return True\n    elif 'multimodal models' in str(e):\n        engine_args.limit_mm_per_prompt = None\n        logger.info(\"The previous configuration was for a MultiModal LLM, correcting configuration and retrying.\")\n        delete_engine_model_from_vram()\n        return True\n    elif 'is greater than the derived max_model_len' in str(e):\n        max_context_len = find_max_seq_len(str(e))\n        if max_context_len:\n            logger.error(f\"{str(e).split('.')[0]}. Automatically changing max_model_len to {max_context_len}\")\n            engine_args.max_model_len = max_context_len\n            delete_engine_model_from_vram()\n            return True\n    elif \"The model's max seq len\" in str(e):\n        max_context_len = find_max_seq_len(str(e))\n        if max_context_len:\n            logger.error(f\"{str(e).split('.')[0]}. Automatically changing max_model_len to {max_context_len}\")\n            engine_args.max_model_len = max_context_len\n            delete_engine_model_from_vram()\n            return True\n    elif 'Chat template is not defined in the tokenizer' in str(e):\n        logger.error(\"Chat template is not defined in the tokenizer, we'll to try infer it from the model config, this could lead to misconfigurations\")\n        model_template = await maybe_get_chat_template(engine_args.model)\n        if not model_template:\n            raise RuntimeError(\"Chat template is not defined in the tokenizer, \"\n                           \"this is probably not an instruction tuned model.  Please use another model\") from e\n        engine_args.chat_template = model_template\n        return True\n    elif 'No available memory for the cache blocks' in str(e):\n        max_context_len = int((engine_args.max_model_len or 65536) / 4)\n        logger.error(f\"{str(e).split('.')[0]}. Automatically changing max_model_len to {max_context_len}\")\n        engine_args.max_model_len = max_context_len\n        delete_engine_model_from_vram()\n        return True\n    return False\n\n\ndef handle_final_retry(engine_args: ExtendedAsyncEngineArgs,\n                       original_engine_args: ExtendedAsyncEngineArgs,\n                       e: Exception) -> None:\n    \"\"\"Handle the final retry attempt.\"\"\"\n    if engine_args.auto_quantized_fallback:\n        engine_args.model = pick_a_quantized_fallback(engine_args.quant_type_preference)\n        engine_args = original_engine_args\n    else:\n        logger.error(f\"Unable to initialize model after retries: {str(e)}\")\n        raise RuntimeError(f\"Unable to initialize model after retries: {str(e)}\") from e\n\n\ndef create_serving_instances(models: list, args: ExtendedAsyncCompleteServerArgs) -> None:\n    \"\"\"Create the serving instances for chat and completion.\"\"\"\n    global openai_serving_chat, openai_serving_completion, openai_serving_embedding, openai_serving_tokenization\n    served_model = [BaseModelPath(model_path='', name=model) for model in models]\n    openai_serving_chat = ExtendedOpenAIServingChat(\n        api_url=f\"http://{args.host}:{args.port}\",\n        engine_client=async_engine,\n        model_config=model_config,\n        base_model_paths= served_model,\n        response_role=args.response_role,\n        lora_modules=args.lora_modules,\n        chat_template=args.chat_template,\n        prompt_adapters=None,\n        request_logger=None\n    )\n    openai_serving_embedding = OpenAIServingEmbedding(\n        engine_client=async_engine,\n        model_config=model_config,\n        base_model_paths= served_model,\n        request_logger=None\n    )\n    openai_serving_tokenization = OpenAIServingTokenization(\n        engine_client=async_engine,\n        model_config=model_config,\n        base_model_paths= served_model,\n        lora_modules=args.lora_modules,\n        request_logger=None,\n        chat_template=args.chat_template\n    )\n    # openai_serving_completion = This is disabled since it is not used in the current implementation\n"}
{"type": "source_file", "path": "app/api/personalities.py", "content": "import json\nimport os\nimport uuid\nfrom json import JSONDecodeError\nfrom sqlite3 import IntegrityError\nfrom typing import Optional, Union\n\nimport aiofiles\nimport aiohttp\nfrom fastapi import APIRouter, Depends, UploadFile, Form, HTTPException\nfrom pydantic import ValidationError\nfrom sqlalchemy import select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom starlette.requests import Request\nfrom starlette.responses import JSONResponse, FileResponse\n\nfrom app.db.auth.auth_db import get_current_user, get_user\nfrom app.db.db_common import get_entity\nfrom app.db.model.auth import User\nfrom app.db.model.personality import Personality\nfrom app.db.personality.personality_db import set_personality_model_request_as_dict, \\\n    get_user_personality_list, \\\n    get_personality, create_personality_async, download_personality_from_server, get_personality_by_id, \\\n    edit_personality_async, delete_personality_async, change_personality_id_and_owner, modify_personality_fields\nfrom app.utils.formatting.personality.personality_preprompt import create_preprompt\nfrom app.utils.definitions import SERVER_URL\nfrom app.utils.database.get import get_db\nfrom app.utils.database.images import save_image, delete_image\nfrom app.utils.formatting.pydantic.request import ImageRequest\nfrom app.utils.log import setup_custom_logger\nfrom app.utils.formatting.pydantic.privacy import PrivacyOptions\nfrom app.utils.formatting.pydantic.personality import PersonalitySchema\nfrom app.utils.server.image_fetch import get_image\nfrom app.utils.definitions import UPLOAD_DIRECTORY\n\nrouter = APIRouter()\nlogger = setup_custom_logger(__name__)\n\n\n@router.get(\"/personality_image/{image_name}\")\nasync def get_router_image(image_name: str):\n    image_request = ImageRequest(image_name=image_name)\n    return await get_image(image_request)\n\n\n@router.post(\"/personality/create\")\nasync def add_personality(\n        raw_request: Request,\n        personality_name: str = Form(...),\n        personality_description: str = Form(...),\n        personality_image: Union[UploadFile, str] = Form(...),\n        # personality_describe_scene: bool = Form(...),\n        personality_tags: Optional[str] = Form(None),\n        personality_privacy: PrivacyOptions = Form(...),\n        auto_generate: bool = Form(...),\n        personality_data: Optional[str] = Form(None),\n        current_user: User = Depends(get_current_user),\n        db: AsyncSession = Depends(get_db)\n):\n    personality = await get_personality(db, personality_name, current_user.id)\n    if personality:\n        raise HTTPException(status_code=400, detail=\"Personality name already choosen\")\n    image_filename = await save_image(current_user, personality_image)\n    online_id, offline_id = None, None\n    # Generate personality data\n    if auto_generate:\n        from app.core.engine import openai_serving_chat\n        model_config = (await openai_serving_chat.engine_client.get_model_config())\n        model_name = model_config.model\n        personality_dict = await set_personality_model_request_as_dict(personality_name, personality_description)\n\n        max_attempts = 5\n        attempts = 0\n        success = False\n\n        while attempts < max_attempts and not success:\n            try:\n                preprompt = await create_preprompt(personality_dict, model_name, raw_request, openai_serving_chat)\n                personality_description = preprompt.get(\"description\")\n                personality_schema = PersonalitySchema(**preprompt).to_dict()\n                success = True\n            except Exception as e:\n                attempts += 1\n                if attempts >= max_attempts:\n                    logger.error(\n                        f\"Failed to generate valid personality after {max_attempts} attempts. Last error: {str(e)}\")\n                    raise HTTPException(status_code=400,\n                                        detail=f\"Failed to generate valid personality after {max_attempts} attempts. \"\n                                               f\"Last error: {str(e)}\")\n                else:\n                    logger.warn(f\"Attempt {attempts} failed. Retrying...\")\n\n        if success:\n            logger.info(f\"Successfully generated personality after {attempts + 1} attempt(s).\")\n        else:\n            raise HTTPException(status_code=500, detail=\"Unexpected error in personality generation.\")\n    # Validate user inputted personality data\n    else:\n        if not personality_data:\n            raise HTTPException(status_code=400, detail=\"Personality data is required for manual creation\")\n        try:\n            personality_data = json.loads(personality_data)\n        except JSONDecodeError as e:\n            raise HTTPException(status_code=400,\n                                detail=f\"Invalid personality data formatting, \"\n                                       f\"it should be a json compatible string: {str(e)}\")\n        try:\n            personality_data['name'] = personality_name\n            personality_schema = PersonalitySchema.from_form(personality_data).to_dict()\n        except ValidationError as e:\n            raise HTTPException(status_code=400, detail=f\"Invalid personality data: {str(e)}\")\n\n    if personality_privacy.value != \"local\":\n\n        json_response = await create_personality_async(db, tags=personality_tags,\n                                                       # describe_scene=personality_describe_scene,\n                                                       name=personality_name,\n                                                       description=personality_description,\n                                                       pre_prompt=personality_schema,\n                                                       image=personality_image,\n                                                       is_private=personality_privacy.value == \"private\")\n\n        # use the id returned form the call\n        if not json_response.get('id', None):\n            raise HTTPException(status_code=500,\n                                detail=\"Personality creation failed, no ID returned from the main server\")\n        online_id = json_response['id']\n        personality = Personality(users=[current_user],\n                                  # describe_scene=personality_describe_scene,\n                                  name=personality_name,\n                                  description=personality_description, pre_prompt=personality_schema,\n                                  image=image_filename,\n                                  id=online_id, owner=json_response['owner'])\n    else:\n        offline_id = uuid.uuid4().hex\n        personality = Personality(users=[current_user],\n                                  # describe_scene=personality_describe_scene,\n                                  name=personality_name,\n                                  description=personality_description, pre_prompt=personality_schema,\n                                  image=image_filename,\n                                  id=offline_id, owner=current_user.name)\n\n    db.add(personality)\n    await db.commit()\n    await db.refresh(personality)\n    return {\"status\": \"Personality created successfully\", \"personality_data\": personality_schema, \"id\": online_id or offline_id}\n\n\n@router.put(\"/personality/edit\")\nasync def update_personality(\n        personality_id: str = Form(...),\n        personality_name: Optional[str] = Form(None),\n        personality_description: Optional[str] = Form(None),\n        personality_image: Optional[UploadFile] = Form(None),\n        privacy_settings: Optional[PrivacyOptions] = Form(None),\n        personality_data: Optional[str] = Form(None),\n        current_user: User = Depends(get_current_user),\n        db: AsyncSession = Depends(get_db)\n):\n    personality = await get_personality_by_id(db, personality_id, current_user.id)\n    if not personality:\n        raise HTTPException(status_code=404, detail=\"Personality not found\")\n\n    if personality_name:\n        personality.name = personality_name\n    if personality_data:\n        try:\n            personality_data = json.loads(personality_data)\n        except JSONDecodeError as e:\n            raise HTTPException(status_code=400,\n                                detail=f\"Invalid personality data formatting, \"\n                                       f\"it should be a json compatible string: {str(e)}\")\n        previous_schema = personality.pre_prompt\n        preprompt = modify_personality_fields(PersonalitySchema.from_form(previous_schema), personality_data).to_dict()\n        personality.pre_prompt = json.dumps(preprompt)\n    if personality_description:\n        personality.description = personality_description\n    # if personality_describe_scene:\n    # personality.describe_scene = personality_describe_scene\n    if personality_image:\n        image_filename = await save_image(current_user, personality_image)\n        await delete_image(personality)\n        personality.image = image_filename\n    db.add(personality)\n    try:\n        await db.commit()\n    except IntegrityError:\n        raise HTTPException(status_code=400, detail=\"Personality name already choosen\")\n    if privacy_settings and privacy_settings.value != \"local\":\n        try:\n            await edit_personality_async(db,\n                                         # describe_scene=personality_describe_scene,\n                                         name=personality_name,\n                                         description=personality_description, pre_prompt=personality.pre_prompt,\n                                         image=personality_image,\n                                         is_private=True if privacy_settings.value == \"private\" else False,\n                                         personality_id=personality.id)\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e)\n                                )\n    return {\"message\": \"Personality updated successfully\"}\n\n\n@router.delete(\"/personality/delete\")\nasync def delete_personality(personality_id: str = Form(...),\n                             also_delete_online: bool = Form(False),\n                             current_user: User = Depends(get_current_user),\n                             db: AsyncSession = Depends(get_db)):\n    if also_delete_online:\n        await delete_personality_async(db, personality_id)\n\n    personality = await get_personality_by_id(db, personality_id, current_user.id)\n    if not personality and not also_delete_online:\n        raise HTTPException(status_code=404, detail=\"Personality not found\")\n    elif not personality:\n        return {\"message\": \"Personality deleted successfully\"}\n    await delete_image(personality)\n    await db.delete(personality)\n    await db.commit()\n\n    return {\"message\": \"Personality deleted successfully\"}\n\n\n@router.get(\"/personality/list\")\nasync def list_personality(current_user: User = Depends(get_current_user),\n                           db: AsyncSession = Depends(get_db)):\n    personality_list = await get_user_personality_list(db, current_user.id)\n    personality_final_list = []\n    for personality in personality_list:\n        personalities = {}\n        for var in vars(personality):\n            if var != \"users\":\n                personalities[var] = getattr(personality, var)\n        personality_final_list.append(personalities)\n\n    return {\"personalities\": personality_final_list}\n\n\n@router.post(\"/personality/download\")\nasync def download_personality(personality_id: str = Form(...),\n                               current_user: User = Depends(get_current_user),\n                               db: AsyncSession = Depends(get_db)):\n    personality = await get_personality_by_id(db, personality_id)\n    if personality:\n        if current_user.id not in [user.id for user in personality.users]:\n            # add the user to the personality\n            user = await get_user(db, current_user.name)\n            personality.users.append(user)\n            await db.commit()\n            return JSONResponse(content={\"message\": \"Personality downloaded successfully\"})\n        else:\n            raise HTTPException(status_code=404, detail=\"Personality already present\")\n    else:\n        personality_data = await download_personality_from_server(db, personality_id)\n\n        if not personality_data:\n            raise HTTPException(status_code=404, detail=\"Personality not found on the server\")\n\n        # Download and save the personality image\n        image_url = personality_data['image']\n\n        image_extension = image_url.split('.')[-1]\n        image_filename = f\"{current_user.name}_{uuid.uuid4().hex}.{image_extension}\"\n        save_path = os.path.join(UPLOAD_DIRECTORY, image_filename)\n\n        async with aiohttp.ClientSession() as session:\n            async with session.get(f\"{SERVER_URL}/personality_image/{image_url}\") as response:\n                if response.status == 200:\n                    async with aiofiles.open(save_path, mode='wb') as f:\n                        await f.write(await response.read())\n                else:\n                    raise HTTPException(status_code=500, detail=\"Failed to download personality image\")\n\n        personality = Personality(\n            id=personality_id,\n            users=[current_user],\n            # describe_scene=personality['describe_scene'],\n            name=personality_data['name'],\n            description=personality_data['description'],\n            pre_prompt=personality_data['pre_prompt'],\n            image=image_filename,\n            owner=personality_data['owner_name'])\n\n        db.add(personality)\n\n        try:\n            await db.commit()\n        except IntegrityError:\n            raise HTTPException(status_code=400, detail=\"Personality name already choosen\")\n\n    return {\"message\": \"Personality downloaded successfully\"}\n\n\n@router.post(\"/personality/push_online\")\nasync def push_lora_online(\n        personality_name: str = Form(...),\n        personality_local_id: str = Form(...),\n        personality_description: str = Form(...),\n        # personality_describe_scene: bool = Form(...),\n        tags: Optional[str] = Form(None),\n        pre_prompt: str = Form(...),\n        image: Union[UploadFile, str] = Form(None),\n        is_private: bool = Form(False),\n        is_new_item: bool = Form(False),\n        current_user: User = Depends(get_current_user),\n        db: AsyncSession = Depends(get_db)):\n    if not is_new_item:\n        await edit_personality_async(db, personality_local_id, personality_name, personality_description,\n                                     pre_prompt, image, is_private)\n    else:  # it does not exist\n        response = await create_personality_async(db, tags,\n                                                  personality_name, personality_description,\n                                                  pre_prompt, image, is_private)\n\n        await change_personality_id_and_owner(db, personality_local_id, response['id'], response['owner'], current_user)\n\n    return JSONResponse(content={\"message\": \"Personality pushed online successfully\"})\n"}
{"type": "source_file", "path": "app/api/personas.py", "content": "import os\nimport warnings\nfrom typing import Union\n\nfrom fastapi import APIRouter, Depends, UploadFile, Form, HTTPException\nfrom httpx import HTTPStatusError\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom starlette.responses import JSONResponse\n\nfrom app.db.auth.auth_db import get_current_user\nfrom app.db.model.auth import User\nfrom app.db.model.persona import Persona\nfrom app.db.persona.persona_db import get_persona, create_persona_async, \\\n    get_user_persona_list, get_persona_by_name, change_persona_id_and_owner, delete_persona_async, edit_persona_async\nfrom app.utils.database.get import get_db\nfrom app.utils.database.images import save_image, delete_image\nfrom app.utils.formatting.pydantic.privacy import PrivacyOptions\nfrom app.utils.formatting.pydantic.request import ImageRequest\nfrom app.utils.server.image_fetch import get_image\nfrom app.utils.definitions import UPLOAD_DIRECTORY\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*has conflict with protected namespace *\")\n\nrouter = APIRouter()\n\n\n@router.get(\"/persona_image/{image_name}\")\nasync def get_router_image(image_name: str):\n    image_request = ImageRequest(image_name=image_name)\n    return await get_image(image_request)\n\n\n@router.post(\"/persona/create\")\nasync def add_persona(persona_name: str = Form(...),\n                      persona_description: str = Form(...),\n                      persona_image: Union[UploadFile, str] = Form(...),\n                      personality_id: str = Form(...),\n                      lora_id: str = Form(None),\n                      model_id: str = Form(...),\n                      privacy_settings: PrivacyOptions = Form(...),\n                      current_user: User = Depends(get_current_user),\n                      db: AsyncSession = Depends(get_db)):\n    persona = await get_persona_by_name(db, persona_name, current_user.id)\n    if persona:\n        raise HTTPException(status_code=400, detail=\"persona name already choosen\")\n\n    new_filename = save_image(current_user, persona_image)\n\n    if privacy_settings != \"local\":\n        try:\n            await create_persona_async(db, name=persona_name, description=persona_description,\n                                       image=persona_image,\n                                       personality_id=personality_id,\n                                       lora_id=lora_id, model_id=model_id,\n                                       is_private=privacy_settings == \"private\")\n        except Exception as e:\n            if isinstance(e, HTTPException):\n                if \"does not exist\" in e.detail:\n                    e.detail += \". Please be sure you have synced the model/lora/personality with the online server\"\n                raise e\n            elif isinstance(e, HTTPStatusError):\n                if \"does not exist\" in e.response.json().get(\"message\", \"\"):\n                    e.response.json()[\"message\"] += \". Please be sure you have synced the model/lora/personality with the online server\"\n            raise HTTPException(status_code=500, detail=\"Online persona creation failed, \" + str(e))\n\n    # use the id returned form the call\n    persona = Persona(name=persona_name, description=persona_description,\n                      image=await new_filename,\n                      personality_id=personality_id,\n                      lora_id=lora_id, model_id=model_id,\n                      users=[current_user], owner=current_user.name\n                      )\n    db.add(persona)\n    await db.commit()\n    await db.refresh(persona)\n    return {\"status\": \"persona created successfully\"}\n\n\n@router.put(\"/persona/edit\")\nasync def update_persona(\n        persona_id: str = Form(...),\n        persona_name: str = Form(None),\n        persona_description: str = Form(None),\n        persona_image: Union[UploadFile, str] = Form(None),\n        lora_id: str = Form(None),\n        model_id: str = Form(None),\n        personality_id: str = Form(None),\n        privacy_settings: PrivacyOptions = Form(None),\n        current_user: User = Depends(get_current_user),\n        db: AsyncSession = Depends(get_db)):\n    persona = await get_persona(db, persona_id, current_user.id)\n\n    if not persona:\n        raise HTTPException(status_code=404, detail=\"persona not found\")\n    if persona_name:\n        persona.name = persona_name\n    if persona_description:\n        persona.description = persona_description\n    if persona_image:\n        new_filename = await save_image(current_user, persona_image)\n        await delete_image(persona)\n        persona.image = new_filename\n    if lora_id:\n        persona.lora_id = lora_id\n    if model_id:\n        persona.model_id = model_id\n    if personality_id:\n        persona.personality_id = personality_id\n\n    db.add(persona)\n    await db.commit()\n    if privacy_settings and privacy_settings.value != \"local\":\n        try:\n            await edit_persona_async(db, name=persona_name, description=persona_description, image=persona_image,\n                                     is_private=True if privacy_settings.value == \"private\" else False, persona_id=persona.id,\n                                     model_id=model_id, lora_id=lora_id, personality_id=personality_id)\n        except Exception as e:\n            raise HTTPException(detail=f\"Error pushing edited model online: {str(e)}\",\n                                status_code=500)\n    return {\"message\": \"persona updated successfully\"}\n\n\n@router.post(\"/persona/download\")\nasync def download_persona(\n        persona_id: str = Form(...),\n        persona_name: str = Form(...),\n        persona_description: str = Form(...),\n        persona_model_id: str = Form(...),\n        persona_personality_id: str = Form(...),\n        persona_lora_id: str = Form(None),\n        persona_image: Union[UploadFile, str] = Form(None),\n        persona_is_private: bool = Form(False),\n        current_user: User = Depends(get_current_user),\n        db: AsyncSession = Depends(get_db),\n):\n    persona = await get_persona(db, persona_id)\n    if persona:\n        if any([current_user.id not in user.id for user in persona.users]):\n            persona.users.append(current_user)\n            await db.commit()\n            return JSONResponse(content={\"message\": \"Persona downloaded successfully\"})\n        else:\n            raise HTTPException(status_code=409, detail=\"Persona already downloaded\")\n    image_filename = None\n    if persona_image:\n        image_filename = await save_image(current_user, persona_image)\n\n    persona = Persona(\n        id=persona_id,\n        user_username=current_user.name,\n        name=persona_name,\n        owner=current_user.name,\n        description=persona_description,\n        personality_id=persona_personality_id,\n        lora_id=persona_lora_id,\n        model_id=persona_model_id,\n        image=image_filename,\n        users=[current_user],\n    )\n    db.add(persona)\n    await db.commit()\n    return JSONResponse(content={\"message\": \"Persona downloaded successfully\"})\n\n\n@router.delete(\"/persona/delete\")\nasync def delete_persona(persona_id: str = Form(...),\n                         also_delete_online: bool = Form(False),\n                         current_user: User = Depends(get_current_user),\n                         db: AsyncSession = Depends(get_db)):\n    if also_delete_online:\n        await delete_persona_async(db, persona_id)\n\n    persona = await get_persona(db, persona_id, current_user.id)\n    if not persona and not also_delete_online:\n        raise HTTPException(status_code=404, detail=\"persona not found\")\n    elif not persona:\n        return {\"message\": \"persona deleted successfully\"}\n    await delete_image(persona)\n    await db.delete(persona)\n    await db.commit()\n    return {\"message\": \"persona deleted successfully\"}\n\n\n@router.get(\"/persona/list\")\nasync def list_persona(current_user: User = Depends(get_current_user),\n                       db: AsyncSession = Depends(get_db)):\n    persona_list = await get_user_persona_list(db, current_user.id)\n    persona_final_list = []\n    for persona in persona_list:\n        personalities = {}\n        for var in vars(persona):\n            if var != \"users\" and var != \"lora\" and var != \"model\" and var != \"personality\":\n                personalities[var] = getattr(persona, var)\n            personalities[var] = getattr(persona, var)\n        persona_final_list.append(personalities)\n\n    return {\"persona\": persona_final_list}\n\n\n@router.post(\"/persona/push_online\")\nasync def push_lora_online(\n        persona_name: str = Form(...),\n        persona_local_id: str = Form(...),\n        persona_description: str = Form(None),\n        id_model: str = Form(...),\n        id_personality: str = Form(...),\n        id_lora: str = Form(None),\n        image: Union[UploadFile, str] = Form(None),\n        is_private: bool = Form(False),\n        is_new_item: bool = Form(False),\n        current_user: User = Depends(get_current_user),\n        db: AsyncSession = Depends(get_db)):\n    if not is_new_item:\n        await edit_persona_async(db, persona_local_id, persona_name, persona_description, id_model, id_lora,\n                                 id_personality, image, is_private)\n    else:\n        response = await create_persona_async(db, is_private, id_personality, persona_name, persona_description, image,\n                                              id_lora, id_model)\n        try:\n            await change_persona_id_and_owner(db, persona_local_id, response['id'], response['owner'], current_user)\n        except Exception as e:\n            raise HTTPException(detail=f\"Error pushing persona online: {str(e)}\", status_code=500)\n    return JSONResponse(content={\"message\": \"Persona pushed online successfully\"})\n"}
{"type": "source_file", "path": "app/api/chats.py", "content": "import json\nimport uuid\n\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom sqlalchemy import select, func\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom starlette.requests import Request\nfrom starlette.responses import JSONResponse\nfrom vllm.entrypoints.openai.protocol import ErrorResponse, CompletionRequest\n\nfrom app.db.auth.auth_db import get_current_user\nfrom app.db.chat.chat_db import async_unpack_chat_history\nfrom app.db.lora.lora_db import establish_if_lora\nfrom app.db.ml_models.model_db import get_current_model\nfrom app.db.model.auth import User\nfrom app.db.model.chat import Chat, Message\nfrom app.db.personality.personality_db import format_dict_to_string, get_personality_by_id\nfrom app.hijacks.protocols.extended_oai import ExtendedChatCompletionRequest\nfrom app.hijacks.starlette import WrappedStreamingResponse\nfrom app.utils.formatting.chat.formatter import format_chat_response, extract_parameter_from_request\nfrom app.utils.formatting.chat.summerizer import populate_and_summarize_chat\nfrom app.utils.database.get import get_db\nfrom app.utils.formatting.personality.personality_preprompt import format_personality_preprompt\nfrom app.utils.log import setup_custom_logger\n\nrouter = APIRouter()\n\nlogger = setup_custom_logger(__name__)\n\n\n@router.put(\"/v1/chat/edit\")\nasync def edit_message(chat_id: str, summary: str,\n                       request: Request, db: AsyncSession = Depends(get_db),\n                       current_user: User = Depends(get_current_user)):\n    message = await db.execute(select(Chat).where(Chat.id == chat_id))\n    message = message.scalars().first()\n    if not message:\n        raise HTTPException(detail=\"No message found with this ID.\", status_code=404)\n    if message.user_id != current_user.id:\n        raise HTTPException(detail=\"You are not the owner of this message.\", status_code=403)\n    message.summary = summary\n    await db.commit()\n    return JSONResponse(content={\"message\": f\"Message {chat_id} edited\"})\n\n\n@router.delete(\"/v1/chat/delete\")\nasync def delete_message(chat_id: str, db: AsyncSession = Depends(get_db),\n                         current_user: User = Depends(get_current_user)):\n    message = await db.execute(select(Chat).where(Chat.id == chat_id))\n    message = message.scalars().first()\n    if not message:\n        raise HTTPException(detail=\"No message found with this ID.\", status_code=404)\n    if message.user_id != current_user.id:\n        raise HTTPException(detail=\"You are not the owner of this message.\", status_code=403)\n    await db.delete(message)\n    await db.commit()\n    return JSONResponse(content={\"message\": f\"Message {chat_id} deleted\"})\n\n\n@router.delete(\"/abort/{message_id}\")\nasync def abort(message_id: str):\n    from app.core.engine import openai_serving_chat\n    # the id here is from the stream response\n    await openai_serving_chat.engine_client.abort(message_id)\n    return JSONResponse(content={\"message\": f\"Message {message_id} aborted\"})\n\n\n@router.get(\"/v1/list/chats\")\nasync def list_chats(db: AsyncSession = Depends(get_db), current_user: User = Depends(get_current_user)):\n    chats = await db.execute(select(Chat).where(Chat.user_id == current_user.id))\n    chats = chats.scalars().all()\n    return JSONResponse(\n        content=[{\"chat_id\": chat.id, \"summary\": chat.summary, \"timestamp\": chat.timestamp.strftime(\"%Y%m%d%H%M%S\")} for\n                 chat in chats])\n\n\n@router.get(\"/v1/chat/history\")\nasync def get_chat_history(request: Request, db: AsyncSession = Depends(get_db),\n                           current_user: User = Depends(get_current_user)):  # noqa\n    chat_id = request.query_params.get('chat_id', None)\n    if not chat_id:\n        raise HTTPException(detail=\"chat_id is required\", status_code=400)\n    try:\n        history = await async_unpack_chat_history(db, chat_id, full_history=True)\n        return JSONResponse(content=history, status_code=200)\n    except Exception as e:\n        HTTPException(detail=str(e), status_code=500)\n\n\n@router.post(\"/v1/chat/pulsar/completions\")\nasync def create_chat_completion(\n        request: ExtendedChatCompletionRequest,\n        raw_request: Request,\n        db: AsyncSession = Depends(get_db),\n        current_user: User = Depends(get_current_user)\n):\n    from app.core.engine import openai_serving_chat\n    from app.api.loras import lora_chat_template_dict\n\n    (chat_id, personality_id, is_regeneration,\n     selected_messages_version_ids, sys_prompt,\n     request_dict) = await extract_parameter_from_request(request, [\"chat_id\", \"personality_id\",\n                                                                    \"is_regeneration\",\n                                                                    'selected_messages_version_ids',\n                                                                    'system_prompt', ])\n\n    if personality_id:\n        personality = await get_personality_by_id(db, personality_id, current_user.id)\n    else:\n        personality = None\n\n    model = await get_current_model(db)\n\n    is_new_chat = False\n\n    # Retrieve or initialize chat\n    if chat_id:\n        chat = await db.execute(select(Chat).where(Chat.id == chat_id))\n        chat = chat.scalars().first()\n        if not chat:\n            raise HTTPException(detail=\"No chat found with this ID.\", status_code=404)\n    else:\n        chat_id = uuid.uuid4().hex\n        chat = Chat(id=chat_id, user_id=current_user.id, model_id=model.name, timestamp=func.now())\n        db.add(chat)\n\n        is_new_chat = True\n\n    # Check model compatibility\n    is_lora = await establish_if_lora(request.model, db)\n    if not is_lora and (model.url != request.model):\n        raise HTTPException(detail=f\"Wrong model selected, the current loaded model is {model.name}\",\n                            status_code=400)\n\n    if is_lora:\n        request.chat_template = lora_chat_template_dict.get(request.model, None)\n\n    message = request.messages[0]\n\n    # add system prompt if in personality chat\n    if is_new_chat:\n\n        # message = await personality_message_hijack(message, context)\n        if sys_prompt != '':\n            initial_content = request.system_prompt.replace(\"{{personality_name}}\", personality.name).replace(\n                \"{{current_user}}\", current_user.name).replace(\"{{personality_attributes}}\",\n                                                                   format_dict_to_string(personality.pre_prompt)) \\\n                                                                   if personality else request.system_prompt\n        else:\n            initial_content = await format_personality_preprompt(personality, current_user) \\\n                if personality else \"You are an helpful AI Assistant.\"\n        system_message = Message(chat=chat, id=message[\"parent_message_id\"], model_id=model.name,\n                                 content={\"content\": initial_content, \"role\": \"system\"})\n\n        db.add(system_message)\n\n    if not is_regeneration:\n        if isinstance(message['content'], list):\n            message['content'] = \"<_MultiModalContent_>\" + json.dumps(\n                message['content'])  # we add a prefix to the message to identify it as multimodal\n\n        user_message = Message(chat=chat, id=message['id'],\n                               parent_message_id=message[\"parent_message_id\"], model_id=model.name,\n                               content={\"content\": message['content'], \"role\": message['role']})\n        db.add(user_message)\n\n    # Finalize chat session\n    await db.commit()\n\n    unpacked_history = await async_unpack_chat_history(db, chat, message[\"id\"], selected_messages_version_ids,\n                                                       personality)\n    request.messages = unpacked_history\n    response_id = uuid.uuid4().hex\n    try:\n        generator = await openai_serving_chat.generate_response(request, raw_request)\n        if isinstance(generator, ErrorResponse):\n            return JSONResponse(content=generator.model_dump(), status_code=generator.code)\n        if request.stream:\n            return WrappedStreamingResponse(generator, db, chat, response_id, message['id'], model.name,\n                                            media_type=\"text/event-stream\")\n\n        response = await format_chat_response(generator.model_dump())\n        response_msg = Message(chat=chat, id=response_id, parent_message_id=message['id'], model_id=model.name,\n                               content=response)\n        db.add(response_msg)\n        await db.commit()\n        generation = generator.model_dump()\n        generation['chat_id'] = chat_id\n        generation['id'] = response_id\n        return JSONResponse(content=generation)\n    except Exception as e:\n        await db.rollback()\n        raise HTTPException(detail=str(e), status_code=500)\n    finally:\n        if is_new_chat:\n            try:\n                await populate_and_summarize_chat(chat, db, openai_serving_chat, request, raw_request)\n            except Exception as e:\n                logger.error(f\"Error while summarizing chat: {e}\")\n\n\n@router.post(\"/v1/completions\")\nasync def create_completion(request: CompletionRequest, raw_request: Request, db: AsyncSession = Depends(get_db),\n                            current_user: User = Depends(get_current_user)):\n    \"\"\"\n    ATM non-functioning. This is a placeholder for the future.\n    \"\"\"\n    raise HTTPException(detail=\"Not implemented yet\", status_code=501)\n"}
{"type": "source_file", "path": "app/api/authorization.py", "content": "import os\nimport shutil\nimport uuid\n\nfrom fastapi import Depends, HTTPException, status, APIRouter, File, UploadFile, Form\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom starlette.responses import JSONResponse\n\n# Database and utility imports for user authentication and file handling.\nfrom app.db.auth.auth_db import authenticate_user, create_access_token, get_current_user, get_user, create_user, \\\n    get_current_user_for_login\nfrom app.db.model.auth import User\nfrom app.utils.database.get import get_db\nfrom app.utils.definitions import UPLOAD_DIRECTORY\nfrom app.utils.formatting.pydantic.request import ImageRequest\nfrom app.utils.formatting.pydantic.token import AuthToken\nfrom app.utils.server.image_fetch import get_image\n\nrouter = APIRouter()\n\n# Endpoint to retrieve user images by name.\n@router.get(\"/user_image/{image_name}\")\nasync def get_router_image(image_name: str):\n    image_request = ImageRequest(image_name=image_name)\n    return await get_image(image_request)\n\n# Endpoint for login and token generation.\n@router.post(\"/token\", response_model=AuthToken)\nasync def login_for_access_token(username: str = Form(...), validate_token=Depends(get_current_user_for_login), db: AsyncSession = Depends(get_db)):\n    user = await authenticate_user(db, username)\n    if not user:\n        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=\"User not found\")\n    access_token = create_access_token(data={\"sub\": user.name})\n    return JSONResponse({\"access_token\": access_token, \"token_type\": \"bearer\"})\n\n# Endpoint for updating user details, including username and profile picture.\n@router.put(\"/users/update\")\nasync def update_user(\n        username: str = Form(None),\n        pfp: UploadFile = File(None),\n        current_user: User = Depends(get_current_user),\n        db: AsyncSession = Depends(get_db)):\n    user = await get_user(db, current_user.name)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    # Update username if provided\n    if username:\n        user.name = username\n        # Update directory if username changes\n        img_path = next((file for file in set(os.listdir(UPLOAD_DIRECTORY)) if current_user.name in file), None)\n        if img_path:\n            new_img_path = img_path.replace(current_user.name, username)\n            if os.path.exists(os.path.join(UPLOAD_DIRECTORY, img_path)):\n                shutil.move(os.path.join(UPLOAD_DIRECTORY, img_path), os.path.join(UPLOAD_DIRECTORY, new_img_path))\n            user.image = new_img_path\n\n    # Update profile picture if provided\n    if pfp:\n        uid = uuid.uuid4().hex\n        file_location = os.path.join(UPLOAD_DIRECTORY, f\"{user.name}_{uid}_{pfp.filename}\")\n        with open(file_location, \"wb\") as buffer:\n            shutil.copyfileobj(pfp.file, buffer)\n        user.image = f\"{user.name}_{uid}_{pfp.filename}\"  # Store just the filename\n\n    db.add(user)\n    await db.commit()\n\n    return {\"message\": \"User updated successfully\", \"new_token\": create_access_token(data={\"sub\": user.name}),\n            \"username\": user.name,\n            \"profile_picture_url\": f\"/static/item_images/{user.image if user.image else ''}\"}\n\n# Endpoint to delete a user.\n@router.delete(\"/users/delete\")\nasync def delete_user(current_user: User = Depends(get_current_user), db: AsyncSession = Depends(get_db)):\n    user = await get_user(db, current_user.name)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n    if os.path.exists(os.path.join(UPLOAD_DIRECTORY, user.image)):\n        os.remove(os.path.join(UPLOAD_DIRECTORY, user.image))\n    await db.delete(user)\n    await db.commit()\n    return {\"message\": \"User account deleted successfully\"}\n\n# Endpoint for user registration.\n@router.post(\"/users/create\", response_model=AuthToken)\nasync def register_user(\n        username: str = Form(...),\n        pfp: UploadFile = File(...),\n        db: AsyncSession = Depends(get_db)\n):\n    db_user = await get_user(db, username)\n    uid = uuid.uuid4().hex\n    file_location = os.path.join(UPLOAD_DIRECTORY, f\"{username}_{uid}_{pfp.filename}\")\n    with open(file_location, \"wb\") as buffer:\n        shutil.copyfileobj(pfp.file, buffer)\n    if db_user:\n        raise HTTPException(status_code=400, detail=\"Username already registered\")\n    new_user = await create_user(db, username, f\"{username}_{uid}_{pfp.filename}\" if pfp else None)\n    # Optionally log the user in immediately after registration\n    access_token = create_access_token(data={\"sub\": new_user.name})\n    return {\"access_token\": access_token, \"token_type\": \"bearer\"}\n\n# Endpoint to list users, optionally filtered by username.\n@router.get(\"/users/list\")\nasync def list_users(username: str = Form(None), db: AsyncSession = Depends(get_db)):\n    if username:\n        user = get_user(db, username)\n        if not user:\n            raise HTTPException(status_code=404, detail=\"User not found\")\n        return user\n\n    users = await get_user(db, return_all=True)\n    return [{'id': user.id, 'username': user.name, 'pfp': user.image} for user in users]\n"}
{"type": "source_file", "path": "app/db/db_setup.py", "content": "import os\nimport secrets\nimport string\nimport uuid\n\nfrom dotenv import set_key\nfrom sqlalchemy import select, AsyncAdaptedQueuePool\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\nfrom sqlalchemy.orm import sessionmaker\n\nfrom app.db.migration.migrate import run_migrations\nfrom app.db.model.base import Base\nfrom app.db.model.lora import LoRA\nfrom app.db.model.ml_model import Model\nfrom app.db.model.token import Token\nfrom app.utils.models.gguf_util import extract_gguf_info_local\nfrom app.utils.log import setup_custom_logger\nfrom app.utils.models.hf_downloader import check_file_in_huggingface_repo\nfrom app.utils.models.list_model import list_models_paths_in_hf_cache, list_loras_hf\nfrom app.utils.definitions import MODEL_PATHS, DATABASE_URL\n\nlogger = setup_custom_logger(__name__)\n\n\nengine = create_async_engine(DATABASE_URL, echo=False,\n                             poolclass=AsyncAdaptedQueuePool,\n                             pool_size=20,\n                             max_overflow=40,\n                             pool_pre_ping=True,\n                             pool_recycle=360\n                             )\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine, class_=AsyncSession, expire_on_commit=False)\n\n\nasync def post_creation_task():\n    from app.db.ml_models.model_db import get_model_id_from_online\n    from app.db.lora.lora_db import get_lora_id_from_online\n    from app.db.ml_models.model_db import get_model\n    from app.utils.models.model_paths import get_hf_path\n    from app.db.lora.lora_db import get_orignal_model_name\n\n    try:\n        async with SessionLocal() as session:\n            models = await list_models_paths_in_hf_cache()\n            local_model_list = (await session.execute(select(Model.url))).scalars().all()\n            model_to_profile = [model for model in models if\n                                (model if isinstance(model, str) else model[0]) not in local_model_list]\n\n            for model in model_to_profile:\n\n                if isinstance(model, list):  # this is for gguf models and possibly exl2 in the future which have more\n                    # than one file per repo\n                    if any(['gguf' in model for model in model[1]]):\n                        base_arch = f\"gguf@{extract_gguf_info_local(model[1][0])['metadata']['general.architecture']}\"\n                    model_id = uuid.uuid4().hex\n                    paths = {variant_path.split('/')[-1]: variant_path for variant_path in model[1]}\n                    variants = ','.join(paths.keys())\n                    model = Model(id=model_id, url=model[0], owner='everyone', name=model[0].split(\"/\")[-1],\n                                  path=str(paths), working=True,\n                                  description=\"Model was pre downloaded from Hugging Face\",\n                                  base_architecture=base_arch,\n                                  variants=variants,\n                                  )\n                    session.add(model)\n\n                elif isinstance(model, str):\n                    model_url = model\n                    try:\n                        model_id = await get_model_id_from_online(session, model_url.split(\"/\")[-1],\n                                                                  \"Model added automatically\",\n                                                                  None, model_url, False)\n                    except Exception as e:\n                        if '401 Unauthorized' in str(e):\n                            logger.error(f\"You aren't logged in!: {e}\")\n                        else:\n                            logger.error(f\"Error while getting model id from online: {e}\")\n                        model_id = uuid.uuid4().hex\n                    try:\n\n                        # speed = get_speed(eng_args)\n                        base_arch = await check_file_in_huggingface_repo(model_url, \"config.json\")\n                        base_path = get_hf_path(model_url)\n                        model = Model(id=model_id, url=model_url, owner='everyone', name=model_url.split(\"/\")[-1],\n                                      path=os.path.join(MODEL_PATHS, await base_path),\n                                      # speed_value=await speed, working=True,\n                                      description=\"Model was pre downloaded from Hugging Face\",\n                                      base_architecture='Unknown' if not base_arch else base_arch)\n\n                        session.add(model)\n                    except Exception as e:\n                        logger.error(f\"Error while adding model {model_url} to the database: {e}\")\n                        model = Model(id=model_id, url=model_url, owner='everyone', name=model_url.split(\"/\")[-1],\n                                      path=os.path.join(MODEL_PATHS, await get_hf_path(model_url)),\n                                      working=False,\n                                      description=str(e), base_architecture=\"CausalLM\")\n                        session.add(model)\n\n            loras = list_loras_hf()\n            local_lora_list = (await session.execute(select(LoRA.url))).scalars().all()\n            loras_to_add = [lora for lora in await loras if lora not in local_lora_list]\n            for lora_url in loras_to_add:\n                original_model_name = await get_orignal_model_name(lora_url)\n                if original_model_name:\n                    model = await get_model(session, original_model_name)\n                else:\n                    model = None\n                try:\n                    lora_id = await get_lora_id_from_online(session, lora_url.split(\"/\")[-1],\n                                                            \"Model added automatically\",\n                                                            None, model.id, lora_url, False)\n                except Exception as e:\n                    if '401 Unauthorized' in str(e):\n                        logger.error(f\"You aren't logged in!: {e}\")\n                    else:\n                        logger.error(f\"Error while getting LoRA id from online: {e}\")\n                    lora_id = uuid.uuid4().hex\n                try:\n                    lora = LoRA(id=lora_id,\n                                name=lora_url.split(\"/\")[-1], url=lora_url, owner='everyone',\n                                path=os.path.join(MODEL_PATHS, await get_hf_path(lora_url)),\n                                base_architecture=await check_file_in_huggingface_repo(lora_url, \"adapter_config.json\"))\n                    session.add(lora)\n                except Exception as e:\n                    logger.error(f\"Error while adding LoRA {lora_url} to the database: {e}\")\n                    lora = LoRA(id=lora_id, name=lora_url.split(\"/\")[-1], url=lora_url, owner='everyone',\n                                path=os.path.join(MODEL_PATHS, await get_hf_path(lora_url)),\n                                description=str(e))\n                    session.add(lora)\n            await session.commit()\n\n    except Exception as e:\n        logger.error(f\"Error in post_creation_task: {e}\")\n    finally:\n        logger.info(\"Post creation task finished\")\n\n\nasync def ensure_default_token(session):\n    # look for the default token\n    result = await session.execute(select(Token).filter_by(id=1))\n    token = result.scalars().first()\n    if not token:\n        # if not, create a default token\n        token = Token(id=1)\n        session.add(token)\n        await session.commit()\n\n\nasync def init_db():\n    try:\n        run_migrations(DATABASE_URL.replace(\"+asyncpg\", \"\"),\n                       os.path.join(os.path.dirname(__file__), \"migration\", \"alembic.ini\"))\n        async with engine.begin() as conn:\n            await conn.run_sync(Base.metadata.create_all)\n        await post_creation_task()\n        async with SessionLocal() as session:\n            await ensure_default_token(session)\n    except Exception as e:\n        logger.error(f\"Error initializing database: {e}\")\n        raise\n"}
{"type": "source_file", "path": "app/core/__init__.py", "content": ""}
{"type": "source_file", "path": "app/db/auth/auth_db.py", "content": "import ipaddress\nimport os\nimport socket\nimport uuid\nfrom typing import List, Tuple, Union, Optional, Dict\n\nimport jwt\nfrom fastapi import Depends, HTTPException, status, Request\nfrom passlib.context import CryptContext\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom app.db.db_common import get_entity, oauth2_scheme\nfrom app.db.db_setup import SessionLocal\nfrom app.db.model.auth import User\nfrom app.utils.database.get import get_db\nfrom app.utils.decorators.cache import cache_unless_exception\nfrom app.utils.definitions import SECRET_KEY, ALGORITHM, LOCAL_TOKEN\nfrom app.utils.server.api_calls_to_main import make_api_request\n\n\n# Setup\npwd_context = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\n\ncredential_exception = HTTPException(\n    status_code=status.HTTP_401_UNAUTHORIZED,\n    detail=\"Could not validate credentials\",\n    headers={\"WWW-Authenticate\": \"Bearer\"},\n)\n\n@cache_unless_exception\nasync def validate_token(token: str = Depends(oauth2_scheme), ip: str = None):\n    \"\"\"\n    Validate the provided JWT token and local token for same-machine requests.\n\n    Args:\n        token (str): The JWT token to validate.\n        ip (str, optional): The IP address of the request, used for local token validation.\n\n    Returns:\n        str or dict: The username if valid or a dict for local users.\n\n    Raises:\n        HTTPException: If the token is invalid or does not match expected values.\n    \"\"\"\n    from app.core.engine import async_engine_args\n    try:\n        # We check if the request is form a local ip address\n        if is_from_local_network(ip):\n            # we check if the token is the local token\n            if token == LOCAL_TOKEN:\n                return {\"username\": \"local_user\", \"is_local\": True}\n            # we check if the user have chosen to allow local unauth request\n            elif async_engine_args.allow_unsafe_local_requests:\n                return {\"username\": \"local_user\", \"is_local\": True}\n            else:\n                # we try to decode the token\n                payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])\n                username: str = payload.get(\"sub\")\n                if username is None:\n                    raise credential_exception\n                return username\n\n        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])\n        username: str = payload.get(\"sub\")\n        if username is None:\n            raise credential_exception\n        return username\n    except jwt.PyJWTError:\n        raise credential_exception\n\n# User management functions\nasync def get_user(db: AsyncSession, username: str=None, return_all: bool = False) -> Optional[Union[User, List[User]]]:\n    \"\"\"\n    Retrieve a user or a list of users from the database.\n\n    Args:\n        db (AsyncSession): The database session.\n        username (str, optional): The username to query for.\n        return_all (bool, optional): Flag to return all users if True.\n\n    Returns:\n        Optional[Union[User, List[User]]]: A single user or list of users depending on the return_all flag.\n    \"\"\"\n    return await get_entity(db, User, name=username, return_all=return_all)\n\n\nasync def create_user(db: AsyncSession, username, image):\n    \"\"\"\n    Create a new user entry in the database.\n\n    Args:\n        db (AsyncSession): The database session.\n        username (str): The username for the new user.\n        image (str): The image filename associated with the new user.\n\n    Returns:\n        User: The newly created user object.\n    \"\"\"\n    db_user = User(id=str(uuid.uuid4()), name=username, image=image)\n    db.add(db_user)\n    await db.commit()\n    await db.refresh(db_user)\n    return db_user\n\n\nasync def authenticate_user(db: AsyncSession, username: str) -> Union[User, bool]:\n    \"\"\"\n    Authenticate a user by checking if the username exists in the database.\n\n    Args:\n        db (AsyncSession): The database session.\n        username (str): The username to authenticate.\n\n    Returns:\n        Union[User, bool]: The user object if authentication is successful, False otherwise.\n    \"\"\"\n    user = await get_user(db, username)\n    return user or False\n\n\nasync def set_last_model_lora(db: AsyncSession, current_user: User, model_id: str=None, lora_id: str=None):\n    \"\"\"\n    Set the last used model and LoRa IDs for a user.\n\n    Args:\n        db (AsyncSession): The database session.\n        current_user (User): The current user whose record is to be updated.\n        model_id (str, optional): The model ID to set.\n        lora_id (str, optional): The LoRA ID to set.\n\n    \"\"\"\n    user = await get_user(db, current_user.name)\n    if model_id is not None:\n        users = await get_user(db, return_all=True) # the model is set for all users\n        for user_ in users:\n            user_.last_model = model_id\n    if lora_id is not None:\n        user.last_lora = lora_id\n    await db.commit()\n\n\nasync def get_last_model_lora(db: AsyncSession, current_user: User) -> Dict:\n    \"\"\"\n    Retrieve the last used model and LoRa IDs for a user.\n\n    Args:\n        db (AsyncSession): The database session.\n        current_user (User): The user whose IDs are to be retrieved.\n\n    Returns:\n        Dict: A dictionary containing 'model' and 'lora' keys with corresponding IDs.\n    \"\"\"\n    user = await get_user(db, current_user.name)\n    return {'model': user.last_model, 'lora': user.last_lora}\n\n\n# Token management function\ndef create_access_token(data: dict) -> str:\n    \"\"\"\n    Create a JWT access token using the provided data.\n\n    Args:\n        data (dict): The data to encode in the JWT.\n\n    Returns:\n        str: The encoded JWT token.\n    \"\"\"\n    to_encode = data.copy()\n    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)\n\n\n\ndef get_allowed_networks() -> List[ipaddress.IPv4Network]:\n    \"\"\"\n    Get a list of allowed network ranges, including Docker internal network if specified.\n\n    Returns:\n        List[ipaddress.IPv4Network]: List of allowed network ranges.\n    \"\"\"\n    allowed_networks = [\n        ipaddress.ip_network(\"10.0.0.0/8\"),\n        ipaddress.ip_network(\"172.16.0.0/12\"),\n        ipaddress.ip_network(\"192.168.0.0/16\"),\n    ]\n\n    docker_internal = os.environ.get(\"DOCKER_INTERNAL_NETWORK\")\n    if docker_internal:\n        try:\n            allowed_networks.append(ipaddress.ip_network(docker_internal))\n        except ValueError:\n            print(f\"Warning: Invalid DOCKER_INTERNAL_NETWORK value: {docker_internal}\")\n\n    return allowed_networks\n\n\ndef get_localhost_addresses() -> List[str]:\n    \"\"\"\n    Retrieve a list of IP addresses associated with the localhost.\n\n    Returns:\n        List[str]: A list of localhost IP addresses.\n    \"\"\"\n    localhost_addresses = [\n        \"127.0.0.1\",\n        \"::1\",\n        \"0:0:0:0:0:0:0:1\",\n        \"localhost\"\n    ]\n    try:\n        local_hostname = socket.gethostname()\n        localhost_addresses.append(socket.gethostbyname(local_hostname))\n    except socket.gaierror:\n        pass\n    return localhost_addresses\n\n\ndef is_allowed_request(client_ip: str) -> bool:\n    \"\"\"\n    Check if a request is allowed based on its IP address.\n\n    This function checks if the request is from localhost, same machine,\n    or an allowed network (including Docker internal network if specified).\n\n    Args:\n        client_ip (str): The IP address of the incoming request.\n\n    Returns:\n        bool: True if the request is allowed, False otherwise.\n    \"\"\"\n    # Check for localhost and same machine\n    if client_ip in get_localhost_addresses():\n        return True\n\n    try:\n        ip = ipaddress.ip_address(client_ip)\n\n        # Check for IPv4 private networks\n        if ip.version == 4:\n            return any(ip in network for network in get_allowed_networks())\n\n        # Check for IPv6 Unique Local Addresses\n        elif ip.version == 6:\n            return ip in ipaddress.ip_network(\"fd00::/8\")\n\n    except ValueError:\n        # If the IP address is invalid, return False\n        return False\n\n    return False\n\n\nasync def ensure_local_request(request: Request):\n    \"\"\"\n    Ensure that the incoming request is from an allowed source.\n\n    This function checks if the request is from localhost, same machine,\n    Docker internal network, or a valid local network (when using host networking).\n\n    Args:\n        request (Request): The incoming request object.\n\n    Raises:\n        HTTPException: If the request is not from an allowed source.\n    \"\"\"\n    client_host = request.client.host\n\n    if is_allowed_request(client_host):\n        return True\n\n    # If the request is not from an allowed source, raise an exception\n    raise HTTPException(status_code=403, detail=\"Access denied. Request not from an allowed source.\")\n\n\n# Update these functions to use the new unified check\ndef is_from_local_network(client_ip: str) -> bool:\n    return is_allowed_request(client_ip)\n\n\nasync def get_current_user(token: str = Depends(oauth2_scheme)):\n    \"\"\"\n    Retrieve the current authenticated user based on the provided token.\n\n    Args:\n        token (str): The access token.\n\n    Returns:\n        User: The authenticated user object.\n\n    Raises:\n        HTTPException: If the user cannot be authenticated.\n    \"\"\"\n    username = await validate_token(token)\n    async with SessionLocal() as db:\n        user = await get_user(db, username=username)\n        if user is None:\n            raise credential_exception\n        return user\n\nasync def get_current_user_for_login(request: Request, token: str = Depends(oauth2_scheme)):\n    \"\"\"\n    Retrieve the current authenticated user based on the provided token.\n\n    Args:\n        token (str): The access token.\n        request (Request): The incoming request object.\n\n    Returns:\n        User: The authenticated user object.\n\n    Raises:\n        HTTPException: If the user cannot be authenticated.\n    \"\"\"\n    if not token == os.environ.get(\"LOCAL_TOKEN\") and not is_from_local_network(request.client.host):\n        raise credential_exception\n\n\nasync def auth_user_with_local_exception(request: Request, token: str = Depends(oauth2_scheme)):\n    \"\"\"\n    Authenticate a user or handle local user exceptions based on the incoming request and token.\n\n    Args:\n        request (Request): The incoming request object.\n        token (str): The access token.\n\n    Raises:\n        HTTPException: If the user cannot be authenticated.\n    \"\"\"\n    username = await validate_token(token, request.client.host)\n    if isinstance(username,dict) and username.get('username') == \"local_user\" and username.get('is_local'):\n        return\n    async with SessionLocal() as db:\n        user = await get_user(db, username=username)\n        if user is None:\n            raise credential_exception\n        return user\n\n\n# Online user configuration\nasync def get_online_user_configuration() -> Tuple[str, str, str]:\n    \"\"\"\n    Retrieve the online configuration for a user from the main API.\n\n    Returns:\n        Tuple[str, str, str]: A tuple containing username, email, and image URL.\n    \"\"\"\n    async for db in get_db():\n        response_json = await make_api_request(db, \"GET\", \"/users/me\")\n\n    return response_json['username'], response_json['email'], response_json['pfp']  # noqa\n"}
{"type": "source_file", "path": "app/db/db_common.py", "content": "from functools import wraps\nfrom typing import List, Optional, Union, Type, Any\n\nimport cachetools\nfrom fastapi.security import OAuth2PasswordBearer\n\nfrom sqlalchemy import select, or_, and_, Sequence, ColumnElement\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom fastapi import HTTPException, UploadFile, Depends\n\nfrom app.db.model.auth import User\nfrom app.db.model.ml_model import Model\nfrom app.db.model.lora import LoRA\nfrom app.db.model.personality import Personality\nfrom app.db.model.chat import Chat, Message\nfrom app.utils.definitions import ACCESS_TOKEN_EXPIRE_MINUTES\nfrom app.utils.server.api_calls_to_main import make_api_request\nfrom app.utils.database.images import save_image\n\nEntityType = Union[User, Model, LoRA, Personality, Chat, Message]\n\noauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"token\")\n\nasync def get_entity(db: AsyncSession, entity_class: Type[EntityType], id: str = None,\n                     user_id: Optional[str] = None, url: str =None, name: str=None,\n                     model_architecture:str =None, return_all: bool = False,\n                     added_conditions: Union[ColumnElement[bool], bool] =None) -> Optional[Union[Sequence[EntityType], EntityType]]:\n    conditions = []\n    if url:\n        conditions.append(entity_class.url == url)\n    if name:\n        conditions.append(entity_class.name == name)\n    if model_architecture:\n        conditions.append(entity_class.base_architecture == model_architecture)\n    if id:\n        conditions.append(entity_class.id == id)\n    if user_id and hasattr(entity_class, 'users'):\n        conditions.append(or_(entity_class.users.any(id=user_id), entity_class.owner == 'everyone'))\n    if added_conditions:\n        conditions.append(added_conditions)\n\n\n    # if either name, url or id is provoded we get one otherwise we get a list\n    query = select(entity_class).where(and_(*conditions))\n    result = await db.execute(query)\n\n    if (not name and not url and not id) or return_all:\n        return result.scalars().all()\n    return result.scalars().first()\n\nasync def ask_pulsar_for_id(db:AsyncSession, url: str, entity_type: str) -> Optional[str]:\n    try:\n        response = await make_api_request(db, \"GET\", f\"/{entity_type}/get_id_from_url\", {f\"{entity_type}_url\": url})\n    except HTTPException as e:\n        return None\n    return response[\"id\"]\n\n\nasync def create_entity_async(db: AsyncSession, entity_type: str, data: dict,\n                              image: Optional[UploadFile] = None) -> dict:\n    files = {}\n    if image:\n        files[f\"{entity_type}_image\"] = image.file\n    return await make_api_request(db, \"POST\", f\"/{entity_type}/create\", data, files)\n\n\nasync def edit_entity_async(db: AsyncSession, entity_type: str, entity_id: str, data: dict,\n                            image: Optional[UploadFile] = None) -> dict:\n    files = {}\n    if image:\n        files[f\"{entity_type}_image\"] = image.file\n    data[f\"{entity_type}_id\"] = entity_id\n    return await make_api_request(db, \"PUT\", f\"/{entity_type}/edit\", data, files)\n\n\nasync def delete_entity_async(db: AsyncSession, entity_type: str, entity_id: str) -> int:\n    await make_api_request(db, \"DELETE\", f\"/{entity_type}/delete\", {\"id\": entity_id})\n    return 200\n\n\nasync def change_entity_id_and_owner(db: AsyncSession, entity_class: Type[EntityType], local_id: str, new_id: str,\n                                     new_owner: str, current_user_username: str):\n    query = select(entity_class).where(\n        and_(or_(entity_class.users.any(username=current_user_username), entity_class.owner == 'everyone'),\n             entity_class.id == local_id)\n    )\n    result = await db.execute(query)\n    entity = result.scalars().first()\n    if not entity:\n        raise HTTPException(status_code=404, detail=f\"{entity_class.__name__} not found\")\n    entity.id = new_id\n    entity.owner = new_owner\n    db.add(entity)\n    await db.commit()\n\n\nasync def add_entity_to_db(db: AsyncSession, entity_class: Type[EntityType], data: dict, current_user: User,\n                           image: Optional[UploadFile] = None) -> EntityType:\n    image_filename = await save_image(current_user, image) if image else None\n    entity = entity_class(**data, image=image_filename, users=[current_user])\n    db.add(entity)\n    await db.commit()\n    await db.refresh(entity)\n    return entity\n\n\n# Specific functions that don't fit into the generic pattern\nasync def get_chat_history(db: AsyncSession, chat_id: str, up_to_message_id: Optional[str] = None) -> List[Message]:\n    query = select(Message).where(Message.chat_id == chat_id)\n    if up_to_message_id:\n        subquery = select(Message.timestamp).where(and_(\n            Message.chat_id == chat_id,\n            Message.id == up_to_message_id\n        )).scalar_subquery()\n        query = query.where(Message.timestamp <= subquery)\n    query = query.order_by(Message.timestamp)\n    result = await db.execute(query)\n    return result.scalars().all()\n\n\nasync def get_current_model(db: AsyncSession) -> Model:\n    from app.core.engine import async_engine\n    model_name = await async_engine.get_model_config()\n    result = await db.execute(select(Model).filter(Model.url == model_name.model))\n    return result.scalars().first()\n\n# Add more specific functions as needed"}
{"type": "source_file", "path": "app/core/fallback/picker.py", "content": "import os\nfrom typing import Any\n\nimport yaml\n\nfile_path = os.path.join(os.path.dirname(__file__), \"fallback.yml\")\n\n\ndef pick_a_quantized_fallback(quant_preference) -> Any:\n    \"\"\"\n    Pick from a fallback yaml file the name and link for a quantized model\n    :return:\n    \"\"\"\n    with open(file_path) as file:\n        data = yaml.load(file, Loader=yaml.FullLoader)\n        # check if the quantized_fallback key exists in the yaml file\n        try:\n            return data[\"model\"][quant_preference]\n        except KeyError:\n            return None\n"}
{"type": "source_file", "path": "app/core/error_checking/health_monitoring.py", "content": "import asyncio\nfrom typing import Optional\n\nfrom app.utils.log import setup_custom_logger\nfrom app.utils.server.restarter import restart\n\nlogger = setup_custom_logger(__name__)\n\n\nasync def continuously_monitor_server_for_errors(time_to_sleep: int = 60):\n    from app.core.engine import openai_serving_chat\n    while True:\n        try:\n            if openai_serving_chat:\n                await openai_serving_chat.engine_client.check_health()\n        except Exception as e:\n            logger.error(f\"The server is not functioning due to {e}, restarting\")\n            restart(dont_save_config=True)\n        await asyncio.sleep(time_to_sleep)  # Sleep for 60 seconds before next check\n\n\nclass ServerMonitor:\n    def __init__(self, check_interval: int = 20):\n        self.check_interval = check_interval\n        self.monitoring_task: Optional[asyncio.Task] = None\n\n    async def start_monitoring(self):\n        if self.monitoring_task is None or self.monitoring_task.done():\n            self.monitoring_task = asyncio.create_task(self._run_monitoring())\n            logger.info(\"Server monitoring started\")\n\n    async def stop_monitoring(self):\n        if self.monitoring_task and not self.monitoring_task.done():\n            self.monitoring_task.cancel()\n            try:\n                await self.monitoring_task\n            except asyncio.CancelledError:\n                pass\n            logger.info(\"Server monitoring stopped\")\n\n    async def _run_monitoring(self):\n        try:\n            await continuously_monitor_server_for_errors(self.check_interval)\n        except asyncio.CancelledError:\n            logger.info(\"Monitoring task was cancelled\")\n        except Exception as e:\n            logger.error(f\"An unexpected error occurred in the monitoring task: {e}\")\n\n\nasync def setup_server_monitoring(check_interval: int = 60):\n    monitor = ServerMonitor(check_interval)\n    await monitor.start_monitoring()\n    return monitor\n"}
{"type": "source_file", "path": "app/db/auth/__init__.py", "content": ""}
{"type": "source_file", "path": "app/db/chat/__init__.py", "content": ""}
{"type": "source_file", "path": "app/api/models.py", "content": "import asyncio\nimport os\nimport shutil\nimport warnings\nfrom typing import Union\n\nfrom fastapi import APIRouter, Depends, Form, UploadFile, HTTPException, BackgroundTasks\nfrom fastapi.responses import JSONResponse\nfrom huggingface_hub.utils import RepositoryNotFoundError\nfrom sqlalchemy import select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom starlette.responses import FileResponse\nfrom vllm.usage.usage_lib import UsageContext\n\nfrom app.core.engine import create_serving_instances\nfrom app.db.auth.auth_db import get_current_user, get_last_model_lora, \\\n    set_last_model_lora\nfrom app.db.lora.lora_db import get_lora_list, hash_lora_str_id, get_lora\nfrom app.db.ml_models.model_db import get_model_list, download_model_api_, \\\n    create_model_async, add_model_to_db, edit_model_async, \\\n    delete_model_async, change_model_id_and_owner, get_model_path_or_url, get_model\nfrom app.db.model.auth import User\nfrom app.db.model.ml_model import Model\nfrom app.hijacks.vllm import ExtendedAsyncCompleteServerArgs\nfrom app.middlewares.model_loader_block import set_block_requests\nfrom app.utils.database.get import get_db\nfrom app.utils.database.images import save_image, delete_image\nfrom app.utils.formatting.pydantic.request import ImageRequest\nfrom app.utils.log import setup_custom_logger\nfrom app.utils.formatting.pydantic.privacy import PrivacyOptions\nfrom app.utils.server.image_fetch import get_image\nfrom app.utils.server.restarter import restart\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*has conflict with protected namespace *\")\n\nrouter = APIRouter()\n\nlogger = setup_custom_logger(__name__)\n\n\n@router.get(\"/model_image/{image_name}\")\nasync def get_router_image(image_name: str):\n    image_request = ImageRequest(image_name=image_name)\n    return await get_image(image_request)\n\n\n@router.get(\"/model/list\")\nasync def show_available_models(\n        db: AsyncSession = Depends(get_db),\n        current_user: User = Depends(get_current_user)):\n    models = await get_model_list(db)\n    model_w_loras = {}\n    non_f_model = {}\n    for model in models:\n        if model.working:\n            loras = await get_lora_list(current_user, db, model.base_architecture)\n            model_w_loras[model.name] = {\n                \"name\": model.name, \"path\": model.path,\n                \"id\": model.id, \"image\": model.image,\n                \"model_description\": model.description,\n                \"model_url\": model.url, \"owner\": model.owner,\n                \"model_speed\": model.speed_value,\n                \"model_architecture\": model.base_architecture,\n                \"versions\": model.variants,\n                \"loras\": [lora.name for lora in loras]\n            }\n        else:\n            non_f_model[model.name] = {\"name\": model.name, \"path\": model.path,\n                                       \"versions\": model.variants,\n                                       \"id\": model.id, \"image\": model.image,\n                                       \"model_description\": model.description,\n                                       \"model_architecture\": model.base_architecture,\n                                       \"model_url\": model.url, \"owner\": model.owner\n                                       }\n\n    return JSONResponse(content={\"models\": model_w_loras, \"non_functional_models\": non_f_model})\n\n\n@router.get(\"/model/loaded\")\nasync def show_loaded_model(current_user: User = Depends(get_current_user), db: AsyncSession = Depends(get_db)):\n    from app.core.engine import openai_serving_chat, is_lora_enabled, is_model_vision, images_per_prompt\n    model_name = openai_serving_chat.engine_client.engine.get_model_config().model # noqa\n    last_conf = await get_last_model_lora(db, current_user)\n\n    if last_conf['model'] != model_name:\n        await set_last_model_lora(db, current_user, model_id=model_name)\n    model = await get_model(db, url=model_name)\n    if not model:\n        raise HTTPException(detail=\"Model not found\", status_code=404)\n    model_vars = vars(model)\n    final_dict = {}\n    for var in model_vars:\n        if var not in {'_sa_instance_state', 'chats', 'messages', 'completions', 'users', 'loras', 'metadata',\n                       'registry'}:\n            final_dict[var] = model_vars[var]\n    lora = None\n    if is_lora_enabled:\n        loras = openai_serving_chat.engine_client.engine.list_loras() # noqa\n\n        lora_url = last_conf['lora'] if any([lora == await hash_lora_str_id(last_conf['lora'])\n                                             for lora in loras if lora is not None]) else None\n        # we return the last lora if it still loaded\n\n        if lora_url:\n            lora_item = await get_lora(lora_url=lora_url, db=db)\n            if lora_item:\n                lora = {\"name\": lora_item.name, \"image\": lora_item.image,\n                        \"base_architecture\": lora_item.base_architecture,\n                        \"url\": lora_item.url, \"description\": lora_item.description,\n                        \"owner\": lora_item.owner}\n    return JSONResponse(content={\"model\": final_dict, \"lora\": lora, \"is_lora_enabled\": is_lora_enabled,\n                                 'is_vision': is_model_vision, \"images_per_prompt\": images_per_prompt})\n\n\n@router.post(\"/model/load\")\nasync def load_model(background_tasks: BackgroundTasks, model_url: str = Form(...), model_variant: str = Form(None),\n                     db: AsyncSession = Depends(get_db), current_user: User = Depends(get_current_user)):\n    from app.core.engine import (openai_serving_chat,\n                                 delete_engine_model_from_vram, initialize_engine, async_engine_args)\n    try:\n\n        old_conf = await openai_serving_chat.engine_client.get_model_config()\n\n        if model_url == old_conf.model:\n            raise HTTPException(detail=\"Model already loaded\", status_code=409)\n\n        is_model_predownloaded = (await db.execute(select(Model).where(Model.url == model_url))).scalars().first()\n\n        set_block_requests(True)\n\n        # Ensure that both engines are not currently running any tasks\n        while openai_serving_chat.engine_client.engine.has_unfinished_requests(): # noqa\n            await asyncio.sleep(0.1)\n\n        if not is_model_predownloaded:\n            return JSONResponse(content={\"error\": \"Model not found, is it installed?\"}, status_code=404)\n\n        # create a restoration server configuration\n        server_conf = ExtendedAsyncCompleteServerArgs.from_yaml(\"last.yml\")\n        server_conf.gpu_memory_utilization = async_engine_args.gpu_memory_utilization\n        # here because otherwise the gpu memory utilization would be calculated with the model still in vram\n        model_path = await get_model_path_or_url(is_model_predownloaded, model_variant)\n        # try to load the new model, if it fails reload the previous model\n        try:\n            delete_engine_model_from_vram()\n\n            server_conf.model = model_path\n            server_conf.tokenizer = model_url\n            server_conf.served_model_name = [model_url]\n            await initialize_engine(server_conf.get_async_eng_args(), UsageContext.OPENAI_API_SERVER)\n            create_serving_instances(server_conf.served_model_name, server_conf)\n\n            server_conf.save_to_yaml()\n            set_block_requests(False)\n\n        except Exception as e:\n            background_tasks.add_task(restart(server_conf, True))\n            return JSONResponse(\n                content={\"status\": f\"The model you select run into errors ({e}), defaulted back to the orignal model\"},\n                status_code=205)\n\n        await set_last_model_lora(db, current_user, model_id=is_model_predownloaded.url)\n        return JSONResponse(content={\"status\": \"Model loaded successfully\"}, status_code=200)\n    except RepositoryNotFoundError:\n        set_block_requests(False)\n        return JSONResponse(content={\n            \"error\": \"Model not found, please double check the repo name and if you have the right permissions\"},\n            status_code=404)\n    except HTTPException as e:\n        raise e\n    except Exception as e:\n        set_block_requests(False)\n        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n\n\n@router.post(\"/model/create\")\nasync def create_model(\n        background_tasks: BackgroundTasks,\n        model_name: str = Form(...),\n        model_description: str = Form(...),\n        model_image: Union[UploadFile, str] = Form(None),\n        model_url: str = Form(...),\n        model_tags: str = Form(None),\n        privacy_settings: PrivacyOptions = Form(...),\n        current_user: User = Depends(get_current_user),\n        db: AsyncSession = Depends(get_db)):\n    model = await get_model(db, url=model_url)\n    new_filename = ''\n    if model_image:\n        new_filename = await save_image(current_user, model_image)\n    if model:\n        # if we added the model via db the db init flow we need to fetch the online id and owner\n        if privacy_settings.value != \"local\":\n            try:\n                json_repsonse = await create_model_async(db, name=model_name, tags=model_tags,\n                                                         description=model_description,\n                                                         image=model_image,\n                                                         is_private=True if privacy_settings.value == \"private\"\n                                                         else False,\n                                                         url=model_url)\n                model.id = json_repsonse['id']\n                model.owner = json_repsonse['owner']\n                model.description = model_description\n                model.name = model_name\n                model.image = new_filename\n                await db.commit()\n                return {\"status\": \"Model created successfully\"}\n            except Exception as e:\n                raise HTTPException(status_code=500, detail=\"Model creation failed, \" + str(e))\n        else:\n            raise HTTPException(status_code=409, detail=\"Model url already exists\")\n\n    try:\n        model = await add_model_to_db(db, model_name, model_description, model_image, new_filename, model_url,\n                                      privacy_settings, current_user, model_tags, background_tasks)\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=\"Model creation failed, \" + str(e))\n\n    db.add(model)\n    await db.commit()\n    await db.refresh(model)\n    return {\"status\": \"Model created successfully\"}\n\n\n@router.put(\"/model/edit\")\nasync def edit_model(ml_model_id: str = Form(...),\n                     ml_model_name: str = Form(None),\n                     ml_model_description: str = Form(None),\n                     ml_model_image: Union[UploadFile, str] = Form(None),\n                     privacy_settings: PrivacyOptions = Form(None),\n                     online_id: str = Form(None),\n                     tags: str = Form(None),\n                     current_user: User = Depends(get_current_user),\n                     db: AsyncSession = Depends(get_db)):\n    model = await get_model(db, model_id=ml_model_id)\n    if not model:\n        raise HTTPException(status_code=404, detail=\"Model not found\")\n\n    if ml_model_description:\n        model.description = ml_model_description\n    if ml_model_name:\n        model.name = ml_model_name\n    if ml_model_image:\n        image_filename = await save_image(current_user, ml_model_image)\n        await delete_image(model)\n        model.image = image_filename\n    if online_id:\n        model.id = online_id\n    db.add(model)\n    await db.commit()\n    if privacy_settings and privacy_settings.value != \"local\":\n        try:\n            await edit_model_async(db, model.name, tags, ml_model_description, ml_model_image,\n                                   privacy_settings.value == 'private',\n                                   ml_model_id)\n        except Exception as e:\n            return JSONResponse(content={\"message\": f\"Error pushing edited model online: {str(e)}\"},\n                                status_code=500)\n\n    return JSONResponse(content={\"message\": \"Model edited successfully\"})\n\n\n@router.delete(\"/model/delete\")\nasync def delete_model(ml_model_id: str = Form(...),\n                       also_delete_loras: bool = Form(False),\n                       also_delete_online: bool = Form(False),\n                       current_user: User = Depends(get_current_user),\n                       db: AsyncSession = Depends(get_db)):\n    if also_delete_online:\n        await delete_model_async(db, ml_model_id)\n\n    model = await get_model(db, model_id=ml_model_id)\n    if not model and not also_delete_online:\n        raise HTTPException(status_code=404, detail=\"Model not found\")\n    elif not model:\n        return JSONResponse(content={\"message\": \"Model deleted successfully\"}, status_code=200)\n    # get all loras that use this model\n    loras_of_model = (await get_lora_list(current_user, db,\n                                          model.base_architecture)) if also_delete_loras else []\n    if len(model.users) < 2:\n        items = [model]\n        await db.delete(model)\n    else:\n        items = []\n        model.users = [user for user in model.users if user.id != current_user.id]\n\n    if also_delete_loras:\n        if len(loras_of_model) > 0:  # delete all loras that use this model from the database\n            for lora in loras_of_model:\n                if len(lora.users) < 2:\n                    items.append(lora)\n                else:\n                    lora.users = [user for user in lora.users if user.id != current_user.id]\n                await db.delete(lora)\n    try:\n        for item in items:  # delete all files associated with the model\n            if os.path.exists(item.path):\n                shutil.rmtree(item.path)\n                await delete_image(item)\n\n        await db.commit()\n\n        return JSONResponse({\"message\": \"Model deleted successfully\"}, 200)\n\n    except Exception as e:\n        await db.rollback()\n        return JSONResponse({\"error\": str(e)}, 500)\n\n\n@router.post(\"/model/download\")\nasync def download_model_api(\n        background_tasks: BackgroundTasks,\n        model_id: str = Form(...),\n        model: str = Form(...),\n        tags: str = Form(None),\n        model_base_architecture: str = Form(...),\n        model_description: str = Form(None),\n        file_variant: str = Form(None),\n        model_url: str = Form(...),\n        image: Union[UploadFile, str] = Form(None),\n        owner: str = Form(None),\n        current_user: User = Depends(get_current_user),\n        db: AsyncSession = Depends(get_db)):\n    return await download_model_api_(model_id=model_id, model_arch=model_base_architecture, model_name=model,\n                                     model_url=model_url, model_description=model_description, owner=owner, image=image,\n                                     current_user=current_user, file_variant=file_variant, tags=tags,\n                                     db=db, background_tasks=background_tasks)\n\n\n@router.post(\"/model/push_online\")\nasync def push_model_online(\n        model_name: str = Form(...),\n        model_local_id: str = Form(...),\n        model_description: str = Form(None),\n        tags: str = Form(None),\n        model_url: str = Form(...),\n        image: Union[UploadFile, str] = Form(None),\n        is_private: bool = Form(False),\n        is_new_item: bool = Form(False),\n        current_user: User = Depends(get_current_user),\n        db: AsyncSession = Depends(get_db)):\n    if not is_new_item:\n        await edit_model_async(db, model_name, tags, model_description, image, is_private, model_local_id)\n    else:\n        try:\n            response = await create_model_async(db, model_name, model_description, image, model_url, is_private, tags)\n            await change_model_id_and_owner(db, model_local_id,\n                                            response['id'], response['owner'],\n                                            current_user)\n\n        except Exception as e:\n            return JSONResponse(content={\"message\": f\"Error pushing model online: {str(e)}\"}, status_code=500)\n    return JSONResponse(content={\"message\": \"Model pushed successfully\"})\n"}
{"type": "source_file", "path": "app/db/__init__.py", "content": ""}
{"type": "source_file", "path": "app/core/whisper.py", "content": "import torch.cuda\n\ntry:\n    from faster_whisper import WhisperModel\nexcept ImportError:\n    WhisperModel = None\n\nMODEL_MAPPING = {\n    \"4\": \"tiny\",  # 200MB\n    \"5\": \"small\"  # 500MB\n}\n\n\ndef load_whisper(model_name: str = \"distil-large-v3\"):\n    return WhisperModel(model_name)\n\n\ndef get_optimal_whisper():\n    # get the available memory in gb\n    available_memory = torch.cuda.memory_reserved(0) / 1024 ** 3\n    # get the optimal model\n    for memory, model in MODEL_MAPPING.items():\n        if available_memory <= int(memory):\n            return load_whisper(model)\n"}
{"type": "source_file", "path": "app/api/reverse_proxy.py", "content": "import asyncio\r\nimport json\r\nimport os\r\nfrom concurrent.futures import ThreadPoolExecutor\r\n\r\nimport requests\r\nfrom fastapi import Request, Depends, APIRouter, HTTPException, Form\r\nfrom httpx import AsyncClient\r\nfrom sqlalchemy.ext.asyncio import AsyncSession\r\nfrom starlette.responses import StreamingResponse, Response, JSONResponse\r\n\r\nfrom app.db.auth.auth_db import get_current_user, get_online_user_configuration, create_access_token\r\nfrom app.db.model.auth import User\r\nfrom app.db.tokens.db_token import get_access_token, set_access_token, set_refresh_token, set_url_token, \\\r\n    get_refresh_token, set_local_url_online\r\nfrom app.utils.definitions import SERVER_URL\r\nfrom app.utils.database.get import get_db\r\nfrom app.utils.log import setup_custom_logger\r\nfrom app.utils.server.api_calls_to_main import refresh_lock\r\n\r\nglobal_username, global_email, global_pfp = None, None, None\r\n\r\nrouter = APIRouter()\r\n\r\nlogger = setup_custom_logger(__name__)\r\n\r\nthread_pool = ThreadPoolExecutor(\r\n    max_workers=os.cpu_count() * 2)  # Manages multiple frontend requests efficiently\r\n\r\n# Define an asynchronous lock to prevent simultaneous token refreshes\r\n\r\nasync def refresh_tokens(db: AsyncSession):\r\n    refresh_token = await get_refresh_token(db)\r\n    if not refresh_token:\r\n        raise HTTPException(status_code=401, detail=\"No refresh token found, please log in\")\r\n\r\n    headers = {\r\n        \"refresh-token\": f\"{refresh_token}\"\r\n    }\r\n\r\n    async with AsyncClient(timeout=120) as client:\r\n        response = await client.post(f\"{SERVER_URL}/refresh\", headers=headers)\r\n\r\n    if response.status_code == 200:\r\n        tokens_dict = response.json()\r\n        await set_access_token(tokens_dict[\"access_token\"], db)\r\n        await set_refresh_token(tokens_dict[\"refresh_token\"], db)\r\n        await set_url_token(tokens_dict[\"url_token\"], db)\r\n    else:\r\n        raise HTTPException(status_code=401, detail=\"Token refresh failed\")\r\n\r\n\r\n@router.api_route(\"/main/{full_path:path}\", methods=[\"GET\", \"POST\", \"PUT\", \"DELETE\"])\r\nasync def proxy_request(request: Request, full_path: str, current_user: User = Depends(get_current_user),\r\n                        db: AsyncSession = Depends(get_db)):\r\n    try:\r\n        access_token = await get_access_token(db)\r\n        if not access_token:\r\n            raise HTTPException(status_code=401, detail=\"No token found, please log in\")\r\n\r\n        old_access_token = access_token\r\n\r\n        headers = {\r\n            \"Authorization\": f\"Bearer {access_token}\",\r\n            **{key: value for key, value in request.headers.items() if key.lower() not in [\"host\", \"authorization\"]}\r\n        }\r\n        data = await request.body()\r\n        params = dict(request.query_params)\r\n\r\n        async def make_request(headers):\r\n            async with AsyncClient(timeout=120) as client:\r\n                req = client.build_request(\r\n                    method=request.method,\r\n                    url=f\"{SERVER_URL}/{full_path}\",\r\n                    headers=headers,\r\n                    content=data,\r\n                    params=params\r\n                )\r\n                return await client.send(req)\r\n\r\n        # Execute the request\r\n        response = await make_request(headers)\r\n\r\n        # Handle 401 Unauthorized response\r\n        if response.status_code == 401:\r\n            async with refresh_lock:\r\n                # Recheck if tokens have been refreshed by another coroutine\r\n                new_access_token = await get_access_token(db)\r\n                if new_access_token == old_access_token:\r\n                    # Refresh tokens since they are still old\r\n                    await refresh_tokens(db)\r\n                # Update headers with the new access token\r\n                headers[\"Authorization\"] = f\"Bearer {await get_access_token(db)}\"\r\n\r\n            # Retry the request with new tokens\r\n            response = await make_request(headers)\r\n\r\n            if response.status_code == 401:\r\n                raise HTTPException(status_code=401, detail=\"Unauthorized after token refresh\")\r\n\r\n        response.headers.pop(\"content-length\", None)\r\n        response.headers.pop(\"content-encoding\", None)  # We handle the encoding in the response\r\n\r\n        if response.headers.get(\"content-type\", \"\").startswith(\"image\"):\r\n            return StreamingResponse(content=response.iter_bytes(), media_type=response.headers.get(\"content-type\"),\r\n                                     status_code=response.status_code, headers=dict(response.headers))\r\n\r\n        try:\r\n            json_content = response.json()\r\n            if 'access_token' in json_content:\r\n                await set_access_token(json_content['access_token'], db)\r\n            if 'refresh_token' in json_content:\r\n                await set_refresh_token(json_content['refresh_token'], db)\r\n            if 'url_token' in json_content:\r\n                await set_url_token(json_content['url_token'], db)\r\n\r\n            json_content.pop('access_token', None)\r\n            json_content.pop('refresh_token', None)\r\n\r\n            return Response(content=json.dumps(json_content), media_type='application/json',\r\n                            status_code=response.status_code, headers=dict(response.headers))\r\n        except json.JSONDecodeError:\r\n            return Response(content=response.content, media_type=response.headers.get('content-type'),\r\n                            status_code=response.status_code, headers=dict(response.headers))\r\n\r\n    except Exception as e:\r\n        raise HTTPException(status_code=500, detail=str(e))\r\n\r\n\r\n@router.post(\"/online_login\")\r\nasync def online_login(username: str = Form(...), password: str = Form(...), db: AsyncSession = Depends(get_db)):\r\n    from server import localtunnel_url\r\n    global global_username, global_email, global_pfp\r\n\r\n    response = requests.post(f\"{SERVER_URL}/login\", data={\"username\": username, \"password\": password})\r\n    if response.status_code == 200:\r\n        tokens_dict = response.json()\r\n\r\n        await set_access_token(tokens_dict[\"access_token\"], db)\r\n        await set_refresh_token(tokens_dict[\"refresh_token\"], db)\r\n        await set_url_token(tokens_dict[\"url_token\"], db)\r\n        # Get the online profile information\r\n        try:\r\n            global_username, global_email, global_pfp = await get_online_user_configuration()\r\n        except Exception as e:\r\n            logger.error(f\"Error {e} getting online user configuration\")\r\n            global_username, global_email, global_pfp = (None, None, None)\r\n\r\n        try:\r\n            await set_local_url_online(localtunnel_url)  # Set the local URL online\r\n        except Exception as e:\r\n            logger.error(f\"Error {e} setting local url online\")\r\n        return JSONResponse(content={\"message\": \"Login successful\", \"access_token\": tokens_dict[\"access_token\"], \"local_token_for_login\": os.environ.get(\"LOCAL_TOKEN\")})\r\n    else:\r\n        return JSONResponse(content={\"message\": f\"Login failed, {response.content}\"}, status_code=401)\r\n\r\n\r\n@router.post(\"/online_logout\")\r\nasync def online_logout(current_user: User = Depends(get_current_user), db: AsyncSession = Depends(get_db)):\r\n    access_token = await get_access_token(db)\r\n    refresh_token = await get_refresh_token(db)\r\n    if not access_token or not refresh_token:\r\n        return JSONResponse(content={\"message\": \"No token found, please log in\"}, status_code=401)\r\n\r\n    headers = {\r\n        \"Authorization\": f\"Bearer {access_token}\",\r\n        \"refresh-token\": f\"{refresh_token}\"\r\n    }\r\n    await set_access_token(\"\", db)\r\n    await set_refresh_token(\"\", db)\r\n    await set_url_token(\"\", db)\r\n    response = requests.post(f\"{SERVER_URL}/logout\", headers=headers)\r\n    if response.status_code == 200:\r\n        return JSONResponse(content={\"message\": \"Logout successful\"})\r\n    else:\r\n        return JSONResponse(content={\"message\": \"Logout failed\"}, status_code=401)\r\n\r\n\r\n#@router.post(\"/set_tokens\") not used\r\nasync def set_tokens(access_token: str = Form(...), refresh_token: str = Form(...), url_token: str = Form(...),\r\n                     db: AsyncSession = Depends(get_db)):\r\n    await set_access_token(access_token, db)\r\n    await set_refresh_token(refresh_token, db)\r\n    await set_url_token(url_token, db)\r\n\r\n    return JSONResponse(content={\"message\": \"Tokens set\"})\r\n"}
{"type": "source_file", "path": "app/db/model/base.py", "content": "from sqlalchemy import Table, Column, String, ForeignKey\nfrom sqlalchemy.orm import declarative_base\n\nBase = declarative_base()\n\n# Many-to-Many between User and X\nuser_lora_association = Table('user_lora_association', Base.metadata,\n                              Column('user_id', String, ForeignKey('users.id'), primary_key=True),\n                              Column('lora_id', String, ForeignKey('loras.id'), primary_key=True)\n                              )\n\nuser_model_association = Table('user_model_association', Base.metadata,\n                               Column('user_id', String, ForeignKey('users.id'), primary_key=True),\n                               Column('model_id', String, ForeignKey('models.id'), primary_key=True)\n                               )\n\nuser_personality_association = Table('user_personality_association', Base.metadata,\n                                     Column('user_id', String, ForeignKey('users.id'), primary_key=True),\n                                     Column('personality_id', String, ForeignKey('personalities.id'), primary_key=True)\n                                     )\n\nuser_personas_association = Table('user_personas_association', Base.metadata,\n                                  Column('user_id', String, ForeignKey('users.id'), primary_key=True),\n                                  Column('personas_id', String, ForeignKey('personas.id'), primary_key=True)\n                                  )"}
{"type": "source_file", "path": "app/db/model/personality.py", "content": "from sqlalchemy import ForeignKey, Column, String, JSON\nfrom sqlalchemy.orm import relationship\n\nfrom app.db.model.base import Base, user_personality_association\n\n\nclass Personality(Base):\n    __tablename__ = \"personalities\"\n    id = Column(String, primary_key=True, unique=True, index=True)\n\n    name = Column(String, unique=True, index=True)\n    description = Column(String)\n    image = Column(String)\n    # describe_scene = Column(Boolean)\n    owner = Column(String)\n    pre_prompt = Column(JSON)\n\n    users = relationship(\"User\", secondary=user_personality_association, back_populates=\"personalities\",\n                         lazy='selectin')\n"}
{"type": "source_file", "path": "app/db/migration/alembic_/__init__.py", "content": ""}
{"type": "source_file", "path": "app/db/migration/alembic_/env.py", "content": "\nfrom alembic import context\nfrom sqlalchemy import engine_from_config\nfrom sqlalchemy import pool\nfrom app.db.model.base import Base\n\n# this is the Alembic Config object, which provides\n# access to the values within the .ini file in use.\nconfig = context.config\n\n# add your model's MetaData object here\n# for 'autogenerate' support\n# from myapp import mymodel\ntarget_metadata = Base.metadata\n\n\n# other values from the config, defined by the needs of env.py,\n# can be acquired:\n# my_important_option = config.get_main_option(\"my_important_option\")\n# ... etc.\n\n\ndef run_migrations_offline() -> None:\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL\n    and not an Engine, though an Engine is acceptable\n    here as well.  By skipping the Engine creation\n    we don't even need a DBAPI to be available.\n\n    Calls to context.execute() here emit the given string to the\n    script output.\n\n    \"\"\"\n    url = config.get_main_option(\"sqlalchemy.url\")\n    context.configure(\n        url=url,\n        target_metadata=target_metadata,\n        literal_binds=True,\n        dialect_opts={\"paramstyle\": \"named\"},\n        render_as_batch=True,\n    )\n\n    with context.begin_transaction():\n        context.run_migrations()\n\n\ndef run_migrations_online() -> None:\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine\n    and associate a connection with the context.\n\n    \"\"\"\n    connectable = engine_from_config(\n        config.get_section(config.config_ini_section, {}),\n        prefix=\"sqlalchemy.\",\n        poolclass=pool.NullPool,\n    )\n\n    with connectable.connect() as connection:\n        context.configure(\n            connection=connection,\n            target_metadata=target_metadata,\n            render_as_batch=True,\n        )\n\n        with context.begin_transaction():\n            context.run_migrations()\n\n\nif context.is_offline_mode():\n    run_migrations_offline()\nelse:\n    run_migrations_online()\n"}
{"type": "source_file", "path": "app/db/model/ml_model.py", "content": "from sqlalchemy import Column, String, Boolean, Float, event, text\nfrom sqlalchemy.orm import relationship\n\nfrom app.db.model.base import Base, user_model_association\n\n\nclass Model(Base):\n    __tablename__ = \"models\"\n    id = Column(String, index=True, unique=True)\n\n    name = Column(String, primary_key=True, unique=True, index=True)\n    path = Column(String)\n    owner = Column(String)\n    description = Column(String, nullable=True)\n    tags = Column(String, nullable=True)\n    image = Column(String, nullable=True)\n    working = Column(Boolean, default=True)\n    url = Column(String, unique=True, index=True)\n    base_architecture = Column(String)\n    variants = Column(String, nullable=True)\n\n    speed_value = Column(Float, nullable=True)\n\n    users = relationship(\"User\", secondary=user_model_association, back_populates=\"models\", lazy='selectin')\n    chats = relationship(\"Chat\", back_populates=\"model\", cascade=\"all, delete-orphan\", lazy='selectin')\n    completions = relationship(\"Completions\", back_populates=\"model\", lazy='selectin')\n    messages = relationship(\"Message\", back_populates=\"model\", lazy='selectin')\n\n\n# noinspection PyProtectedMember\n@event.listens_for(Model, 'before_update')\ndef receive_before_update(mapper, connection, target):\n    if hasattr(target, '_original_id') and target.id != target._original_id:\n        # L'ID è stato modificato, aggiorna tutte le tabelle correlate\n        for table in ['chats', 'completions', 'messages']:\n            stmt = text(f\"UPDATE {table} SET model_id = :new_id WHERE model_id = :old_id\")\n            connection.execute(stmt, {'new_id': target.id, 'old_id': target._original_id})\n\n        # Aggiorna anche la tabella di associazione user_model\n        stmt = text(\"UPDATE user_model_association SET model_id = :new_id WHERE model_id = :old_id\")\n        connection.execute(stmt, {'new_id': target.id, 'old_id': target._original_id})\n\n\n@event.listens_for(Model, 'load')\ndef receive_load(target, context):\n    target._original_id = target.id\n"}
{"type": "source_file", "path": "app/db/model/chat.py", "content": "\nfrom sqlalchemy import Column, Integer, String\nfrom sqlalchemy import ForeignKey, JSON, DateTime, func\nfrom sqlalchemy.orm import relationship\n\nfrom app.db.model.base import Base\n\n\nclass Message(Base):\n    __tablename__ = 'messages'\n    id = Column(String, unique=True, primary_key=True)\n    content = Column(JSON)\n    user_id = Column(String, ForeignKey('users.id'))\n    model_id = Column(String, ForeignKey('models.name'))\n    lora_id = Column(String, ForeignKey('loras.name'), nullable=True)\n    chat_id = Column(String, ForeignKey('chats.id'), nullable=True)\n    completion_id = Column(String, ForeignKey('completions.id'), nullable=True)\n    timestamp = Column(DateTime, default=func.now())\n    version = Column(Integer, default=1)\n    parent_message_id = Column(String, ForeignKey('messages.id'), nullable=True)\n\n    # Relationships\n    chat = relationship(\"Chat\", back_populates=\"messages\")\n    previous_message = relationship(\"Message\", remote_side=[id])\n    completion = relationship(\"Completions\", back_populates=\"messages\", lazy='selectin')\n    model = relationship(\"Model\", back_populates=\"messages\", lazy='selectin')\n    lora = relationship(\"LoRA\", back_populates=\"messages\", lazy='selectin')\n\n\nclass Chat(Base):\n    __tablename__ = \"chats\"\n    id = Column(String, primary_key=True, unique=True, index=True)\n    summary = Column(String, nullable=True)\n    timestamp = Column(DateTime, onupdate=func.now(), nullable=True)\n\n    user_id = Column(String, ForeignKey('users.id'))\n    model_id = Column(String, ForeignKey('models.name'))\n    lora_id = Column(String, ForeignKey('loras.name'), nullable=True)\n    personality = Column(String, ForeignKey('personalities.name'), nullable=True)\n\n    user = relationship(\"User\", back_populates=\"chats\")\n    model = relationship(\"Model\", back_populates=\"chats\")\n    lora = relationship(\"LoRA\", back_populates=\"chats\")\n    messages = relationship(\"Message\",\n                            back_populates=\"chat\",\n                            cascade=\"all, delete-orphan\",\n                            lazy='selectin',\n                            order_by=\"Message.timestamp\")\n\n\nclass Completions(Base):\n    __tablename__ = \"completions\"\n    id = Column(String, primary_key=True, index=True)\n\n    chat_id = Column(String, ForeignKey('chats.id'))\n    user_id = Column(String, ForeignKey('users.id'))\n    lora_id = Column(String, ForeignKey('loras.name'), nullable=True)\n    model_id = Column(String, ForeignKey('models.name'))\n\n    chat = relationship(\"Chat\")\n    user = relationship(\"User\")\n    lora = relationship(\"LoRA\")\n    model = relationship(\"Model\")\n    messages = relationship(\"Message\", back_populates=\"completion\", cascade=\"all, delete-orphan\", lazy='selectin')\n"}
{"type": "source_file", "path": "app/db/migration/migrate.py", "content": "import logging\nimport os\n\nfrom alembic import command\nfrom alembic.autogenerate import compare_metadata\nfrom alembic.config import Config\nfrom alembic.migration import MigrationContext\nfrom alembic.script import ScriptDirectory\nfrom sqlalchemy import create_engine, inspect, text\nfrom sqlalchemy.exc import SQLAlchemyError\n\nfrom app.db.model.base import Base\nfrom app.utils.log import setup_custom_logger\n\nlogger = setup_custom_logger(__name__)\n\ndef get_alembic_config(alembic_ini_path):\n    if not os.path.exists(alembic_ini_path):\n        raise FileNotFoundError(f\"Alembic configuration file not found: {alembic_ini_path}\")\n\n    alembic_cfg = Config(alembic_ini_path)\n    script_location = alembic_cfg.get_main_option(\"script_location\")\n    if not script_location:\n        raise ValueError(\"script_location not set in alembic.ini\")\n\n    if not os.path.isabs(script_location):\n        script_location = os.path.join(os.path.dirname(alembic_ini_path), script_location)\n\n    if not os.path.exists(script_location):\n        raise FileNotFoundError(f\"Migration script directory not found: {script_location}\")\n\n    alembic_cfg.set_main_option(\"script_location\", script_location)\n    return alembic_cfg\n\n\ndef get_current_revision(engine):\n    with engine.connect() as conn:\n        context = MigrationContext.configure(conn)\n        return context.get_current_revision()\n\n\ndef get_head_revision(alembic_cfg):\n    script = ScriptDirectory.from_config(alembic_cfg)\n    head_revision = script.get_current_head()\n    if head_revision is None:\n        logger.warning(\n            \"No migration scripts found. Ensure you have run 'alembic revision -m \\\"initial\\\"' to create your first migration.\")\n    return head_revision\n\n\ndef create_initial_revision(alembic_cfg):\n    try:\n        command.revision(alembic_cfg, message=\"Initial revision\", autogenerate=True)\n        logger.info(\"Created initial revision\")\n        return get_head_revision(alembic_cfg)\n    except Exception as e:\n        logger.error(f\"Error creating initial revision: {str(e)}\")\n        return None\n\n\ndef check_model_changes(engine, metadata):\n    with engine.connect() as connection:\n        context = MigrationContext.configure(connection)\n        diff = compare_metadata(context, metadata)\n        return bool(diff)\n\ndef safe_upgrade(engine, alembic_cfg):\n    current_rev = get_current_revision(engine)\n    head_rev = get_head_revision(alembic_cfg)\n\n    if head_rev is None:\n        logger.info(\"No migration scripts found. Creating initial revision.\")\n        head_rev = create_initial_revision(alembic_cfg)\n        if head_rev is None:\n            logger.error(\"Failed to create initial revision. Cannot proceed with upgrade.\")\n            return\n\n    if current_rev is None:\n        logger.info(\"No current revision found. Initializing with head revision.\")\n        with engine.begin() as conn:\n            conn.execute(text(f\"INSERT INTO alembic_version (version_num) VALUES ('{head_rev}')\"))\n        current_rev = head_rev\n\n    # Check for model changes\n    if check_model_changes(engine, Base.metadata):\n        logger.info(\"Detected changes in the Base model. Creating a new revision.\")\n        try:\n            with engine.begin() as connection:\n                alembic_cfg.attributes['connection'] = connection\n                revision = command.revision(alembic_cfg, message=\"Model changes\", autogenerate=True)\n            logger.info(f\"Created new revision: {revision}\")\n            head_rev = get_head_revision(alembic_cfg)  # Update head revision\n        except Exception as e:\n            logger.error(f\"Error creating revision for model changes: {str(e)}\")\n            return\n\n    if current_rev == head_rev:\n        logger.info(\"Database is up to date\")\n        return\n\n    try:\n        command.upgrade(alembic_cfg, \"head\")\n        logger.info(\"Successfully upgraded database to latest version\")\n    except SQLAlchemyError as e:\n        logger.error(f\"Error during migration: {str(e)}\")\n        logger.info(\"Attempting to roll back to previous version\")\n        try:\n            command.downgrade(alembic_cfg, f\"{current_rev}\")\n            logger.info(\"Successfully rolled back to previous version\")\n        except SQLAlchemyError as rollback_error:\n            logger.error(f\"Error during rollback: {str(rollback_error)}\")\n            logger.critical(\"Database is in an inconsistent state. Manual intervention required.\")\n\n\ndef run_migrations(db_url, alembic_ini_path):\n    engine = create_engine(db_url)\n\n    try:\n        alembic_cfg = get_alembic_config(alembic_ini_path)\n        alembic_cfg.set_main_option(\"sqlalchemy.url\", db_url)\n\n        with engine.begin() as connection:\n            alembic_cfg.attributes['connection'] = connection\n            safe_upgrade(engine, alembic_cfg)\n    except Exception as e:\n        logger.error(f\"Unexpected error during migration: {str(e)}\")\n    finally:\n        engine.dispose()"}
{"type": "source_file", "path": "app/db/migration/__init__.py", "content": ""}
{"type": "source_file", "path": "app/db/model/lora.py", "content": "from sqlalchemy import String, Column, event, text\nfrom sqlalchemy.orm import relationship\n\nfrom app.db.model.base import Base, user_lora_association\n\n\nclass LoRA(Base):\n    __tablename__ = \"loras\"\n    id = Column(String, unique=True, index=True)\n\n    name = Column(String, primary_key=True, index=True, unique=True)\n    path = Column(String)\n    owner = Column(String)\n    image = Column(String, nullable=True)\n    url = Column(String, unique=True)\n    description = Column(String, nullable=True)\n    base_architecture = Column(String)\n\n    users = relationship(\"User\", secondary=user_lora_association, back_populates=\"loras\", lazy='selectin')\n    chats = relationship(\"Chat\", back_populates=\"lora\", cascade=\"all, delete-orphan\", lazy='selectin')\n    completions = relationship(\"Completions\", back_populates=\"lora\", lazy='selectin')\n    messages = relationship(\"Message\", back_populates=\"lora\", cascade=\"all, delete-orphan\", lazy='selectin')\n\n\n# noinspection PyProtectedMember\n@event.listens_for(LoRA, 'before_update')\ndef receive_before_update(mapper, connection, target):\n    if hasattr(target, '_original_id') and target.id != target._original_id:\n        # L'ID è stato modificato, aggiorna tutte le tabelle correlate\n        for table in ['chats', 'completions', 'messages']:\n            stmt = text(f\"UPDATE {table} SET lora_id = :new_id WHERE lora_id = :old_id\")\n            connection.execute(stmt, {'new_id': target.id, 'old_id': target._original_id})\n\n        # Aggiorna anche la tabella di associazione user_lora\n        stmt = text(\"UPDATE user_lora_association SET lora_id = :new_id WHERE lora_id = :old_id\")\n        connection.execute(stmt, {'new_id': target.id, 'old_id': target._original_id})\n\n\n@event.listens_for(LoRA, 'load')\ndef receive_load(target, context):\n    target._original_id = target.id\n"}
{"type": "source_file", "path": "app/db/lora/lora_db.py", "content": "import hashlib\nimport json\nimport os\nfrom typing import Union, Optional, List\n\nfrom sqlalchemy import select, and_\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom starlette.datastructures import UploadFile\n\nfrom app.db.db_common import get_entity, ask_pulsar_for_id\nfrom app.db.ml_models.model_db import get_model\nfrom app.utils.models.model_paths import get_hf_path\nfrom app.db.model.auth import User\nfrom app.db.model.lora import LoRA\nfrom app.utils.log import setup_custom_logger\nfrom app.utils.models.list_model import list_loras_hf\nfrom app.utils.models.tokenizer_template_inferrer import maybe_get_chat_template\nfrom app.utils.server.api_calls_to_main import make_api_request\n\nlogger = setup_custom_logger(__name__)\n\nasync def get_lora_list(current_user: User, db: AsyncSession, model_arch: str = None) -> Union[LoRA, None, List[LoRA]]:\n    \"\"\"\n    Get a list of LoRA entities associated with the current user.\n\n    Args:\n        current_user (User): The user requesting the LoRA list.\n        db (AsyncSession): The database session.\n        model_arch (str, optional): The model architecture to filter by.\n\n    Returns:\n        Union[LoRA, None, List[LoRA]]: A list of LoRA entities or None.\n    \"\"\"\n    return await get_entity(db, LoRA, user_id=current_user.id, model_architecture=model_arch)\n\nasync def establish_if_lora(lora_url: str, db: AsyncSession) -> bool:\n    \"\"\"\n    Check if a LoRA entity exists in the database by name.\n\n    Args:\n        lora_url (str): The url of the LoRA entity.\n        db (AsyncSession): The database session.\n\n    Returns:\n        bool: True if the LoRA entity exists, False otherwise.\n    \"\"\"\n    loras = await get_entity(db, LoRA, url=lora_url, return_all=True)\n    return len(loras) > 0\n\nasync def get_lora(db: AsyncSession, lora_url: str = None, lora_name: str = None):\n    \"\"\"\n    Retrieve a LoRA entity by URL or name.\n\n    Args:\n        db (AsyncSession): The database session.\n        lora_url (str, optional): The URL of the LoRA entity.\n        lora_name (str, optional): The name of the LoRA entity.\n\n    Returns:\n        LoRA: The retrieved LoRA entity.\n    \"\"\"\n    return await get_entity(db, LoRA, url=lora_url if not lora_name else None, name=lora_name)\n\nasync def get_lora_id_from_online(db, name, description, image, model_id, url, is_private) -> str:\n    \"\"\"\n    Get or create a LoRA ID based on online status.\n\n    Args:\n        db (AsyncSession): The database session.\n        name (str): The name of the LoRA.\n        description (str): The description of the LoRA.\n        image (UploadFile): The image of the LoRA.\n        model_id (str): The model ID associated with the LoRA.\n        url (str): The URL of the LoRA.\n        is_private (bool): Whether the LoRA is private.\n\n    Returns:\n        str: The LoRA ID.\n    \"\"\"\n    lora_id = await ask_pulsar_for_id(db, url, 'lora')\n    if not lora_id:\n        response = await create_lora_async(db, name, description, image, model_id, url, is_private)\n        lora_id = response['id']\n    return lora_id\n\nasync def edit_lora_async(db: AsyncSession, name: str, description: str, image: UploadFile, is_private: bool, lora_id: str, tags: str=None):\n    \"\"\"\n    Edit a LoRA entity asynchronously.\n\n    Args:\n        db (AsyncSession): The database session.\n        name (str): The new name of the LoRA.\n        description (str): The new description of the LoRA.\n        image (UploadFile): The new image of the LoRA.\n        is_private (bool): Whether the LoRA is private.\n        lora_id (str): The ID of the LoRA to be edited.\n        tags (str, optional): Tags associated with the LoRA.\n\n    Returns:\n        dict: The response from the API call.\n    \"\"\"\n    files = {}\n    if image:\n        files[\"lora_image\"] = image.file\n    param_dict = {\n        \"lora_id\": lora_id,\n        \"lora_name\": name,\n        \"lora_description\": description,\n        \"lora_is_private\": is_private,\n    }\n    if tags:\n        param_dict.update({'lora_tags': tags})\n    response = await make_api_request(db, \"PUT\", \"/lora/edit\", param_dict, files)\n    return response\n\nasync def create_lora_async(db: AsyncSession, name: str, description: str, image: UploadFile, model_id: str, url: str, is_private: bool, tags: str=None):\n    \"\"\"\n    Create a LoRA entity asynchronously.\n\n    Args:\n        db (AsyncSession): The database session.\n        name (str): The name of the LoRA.\n        description (str): The description of the LoRA.\n        image (UploadFile): The image of the LoRA.\n        model_id (str): The model ID associated with the LoRA.\n        url (str): The URL of the LoRA.\n        is_private (bool): Whether the LoRA is private.\n        tags (str, optional): Tags associated with the LoRA.\n\n    Returns:\n        dict: The response from the API call.\n    \"\"\"\n    form_data = {\n        \"model_id\": model_id,\n        \"lora_private_token\": os.environ.get(\"PULSAR_HF_TOKEN\", None) if os.environ.get(\"PULSAR_SHOULD_SEND_PRIVATE_TOKEN\", False) else None,\n        \"lora_name\": name,\n        \"lora_description\": description,\n        \"lora_url\": url,\n        \"lora_tags\": tags,\n        \"lora_is_private\": str(is_private),\n    }\n\n    files = {}\n    if image:\n        files[\"lora_image\"] = image.file\n    return await make_api_request(db, \"POST\", \"/lora/create\", form_data, files)\n\nasync def delete_lora_async(db: AsyncSession, lora_id):\n    \"\"\"\n    Delete a LoRA entity asynchronously.\n\n    Args:\n        db (AsyncSession): The database session.\n        lora_id (str): The ID of the LoRA to be deleted.\n\n    Returns:\n        int: Status code indicating success (200).\n    \"\"\"\n    await make_api_request(db, \"DELETE\", \"/lora/delete\", {\"id\": lora_id})\n    return 200\n\nasync def get_user_lora(current_user: User, db: AsyncSession, lora_id: str = None, lora_name: str = None):\n    \"\"\"\n    Get a LoRA entity associated with the current user.\n\n    Args:\n        current_user (User): The current user.\n        db (AsyncSession): The database session.\n        lora_id (str, optional): The ID of the LoRA.\n        lora_name (str, optional): The name of the LoRA.\n\n    Returns:\n        LoRA: The retrieved LoRA entity.\n    \"\"\"\n    return await get_entity(db, LoRA, user_id=current_user.id, id=lora_id, name=lora_name)\n\nasync def change_lora_id_and_owner(db: AsyncSession, model_local_id: str, lora_id: str, owner: str, current_user: User):\n    \"\"\"\n    Change the LoRA ID and owner.\n\n    Args:\n        db (AsyncSession): The database session.\n        model_local_id (str): The local ID of the model.\n        lora_id (str): The new LoRA ID.\n        owner (str): The new owner of the LoRA.\n        current_user (User): The current user.\n\n    Raises:\n        Exception: If the LoRA entity is not found.\n    \"\"\"\n    lora = await get_entity(db, LoRA, id=model_local_id, user_id=current_user.id)\n    if not lora:\n        raise Exception(\"LoRA not found\")\n    lora.id = lora_id\n    lora.owner = owner\n    db.add(lora)\n    await db.commit()\n\nasync def get_user_lora_by_url(current_user: User, db: AsyncSession, url: str = None):\n    \"\"\"\n    Get a LoRA entity by URL for the current user.\n\n    Args:\n        current_user (User): The current user.\n        db (AsyncSession): The database session.\n        url (str, optional): The URL of the LoRA.\n\n    Returns:\n        LoRA: The retrieved LoRA entity.\n    \"\"\"\n    return await get_entity(db, LoRA, user_id=current_user.id, url=url)\n\nasync def hash_lora_str_id(text: str) -> Optional[int]:\n    \"\"\"\n    Hash a string to an integer, needed for the LoRA id in vllm.\n\n    This method uses the first 8 hex digits of the SHA-1 hash to generate the integer.\n\n    Args:\n        text (str): The input string to hash.\n\n    Returns:\n        Optional[int]: An integer representation of the truncated hash, or None if the text is empty.\n    \"\"\"\n    if not text:\n        return\n    hash_object = hashlib.sha1(text.encode())\n    hex_dig = hash_object.hexdigest()\n    return int(hex_dig[:8], 16)\n\nasync def is_lora_correct(lora: LoRA, db: AsyncSession):\n    \"\"\"\n    Check if a LoRA entity is correct and compatible with a given model.\n\n    Args:\n        lora (LoRA): The LoRA entity to check.\n        db (AsyncSession): The database session.\n\n    Returns:\n        tuple: A boolean indicating compatibility and a message or template.\n    \"\"\"\n    from app.core.engine import openai_serving_chat\n    model_url = (await openai_serving_chat.engine_client.get_model_config()).model\n    model = await get_model(db, model_url)\n    if not model:\n        error = \"Could not determine if the lora and the model architecture are compatible, this could lead to errors.\"\n        logger.error(error)\n        return False, error\n\n    query = select(LoRA).where(and_(LoRA.name == lora.name, LoRA.base_architecture == model.base_architecture))\n    loras = (await db.execute(query)).scalars().all()\n    if len(loras) > 0:\n        chat_template = await maybe_get_chat_template(lora.url)\n        if not chat_template:\n            error = \"Could not retrieve the chat template, this lora is unusable since transformer 0.44 unless you manually specify a chat template.\"\n            logger.error(error)\n            return False, error\n\n        return True, chat_template\n    return False, \"LoRA is not compatible with this model, please load a model with a correct architecture first.\"\n\nasync def get_orignal_model_name(lora_name):\n    \"\"\"\n    Get the original model name associated with a LoRA.\n\n    Args:\n        lora_name (str): The name of the LoRA.\n\n    Returns:\n        str: The base model name or path.\n    \"\"\"\n    model_dir = await list_loras_hf(await get_hf_path(lora_name))\n    with open(os.path.join(model_dir, 'adapter_config.json')) as f:\n        lora_conf = json.load(f)\n    return lora_conf.get(\"base_model_name_or_path\", None)"}
{"type": "source_file", "path": "app/db/model/__init__.py", "content": ""}
{"type": "source_file", "path": "app/db/model/persona.py", "content": "from sqlalchemy import ForeignKey, Column, String\nfrom sqlalchemy.orm import relationship\n\nfrom app.db.model.base import Base, user_personas_association\n\n\nclass Persona(Base):\n    __tablename__ = \"personas\"\n    id = Column(String, unique=True, index=True)\n    user_username = Column(String, ForeignKey('users.name'))\n    name = Column(String, primary_key=True, unique=True, index=True)\n    owner = Column(String)\n    description = Column(String)\n    personality_id = Column(String, ForeignKey('personalities.id'))\n    lora_id = Column(String, ForeignKey('loras.id'), nullable=True)\n    model_id = Column(String, ForeignKey('models.id'))\n\n    image = Column(String)\n\n    model = relationship(\"Model\")\n    lora = relationship(\"LoRA\")\n    personality = relationship(\"Personality\")\n    users = relationship(\"User\", secondary=user_personas_association, back_populates=\"personas\", lazy='selectin')\n"}
{"type": "source_file", "path": "app/db/personality/__init__.py", "content": ""}
{"type": "source_file", "path": "app/db/persona/persona_db.py", "content": "from typing import List, Optional\n\nfrom fastapi import UploadFile\nfrom sqlalchemy import select, or_, and_\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom app.db.model.persona import Persona\nfrom app.utils.server.api_calls_to_main import make_api_request\n\n\nasync def set_persona_model_request_as_dict(persona_name: str, persona_description: str) -> List[dict]:\n    return [{'content': 'Generate a detailed persona profile that addresses the following aspects:'\n                        ' Appearance, Persona, Motivations, Social Role, and Style Symbolism. '\n                        'Ensure each section is concise yet comprehensive.', 'role': 'system'},\n            {'content': f'Name: {persona_name}, Charachter Description:{persona_description} ', 'role': 'user'}]\n\n\nasync def change_persona_id_and_owner(db, model_local_id, persona_id, owner, current_user_username):\n    query = select(Persona).where(\n        and_(or_(Persona.users.any(username=current_user_username), Persona.owner == 'everyone'),\n             Persona.id == model_local_id)\n    )\n    result = await db.execute(query)\n    lora = result.scalars().first()\n    if not lora:\n        raise Exception(\"Persona not found\")\n    lora.id = persona_id\n    lora.owner = owner\n    db.add(lora)\n    await db.commit()\n\n\nasync def get_persona(db: AsyncSession, persona_id: str, user_id: str=None):\n    if user_id:\n        result = await db.execute(select(Persona).where(and_(Persona.id == persona_id, Persona.users.any(id=user_id))))\n    else:\n        result = await db.execute(select(Persona).where(Persona.id == persona_id))\n    return result.scalars().first()\n\n\nasync def get_persona_by_name(db: AsyncSession, name: str, user: str):\n    result = await db.execute(select(Persona).where(and_(Persona.name == name, Persona.users.any(id=user))))\n    return result.scalars().first()\n\n\nasync def create_persona_async(db, is_private, personality_id, name, description, lora_id, model_id, image):\n    data = {\"persona_name\": name,\n            \"persona_description\": description,\n            \"persona_lora_id\": lora_id,\n            \"persona_model_id\": model_id,\n            \"persona_personality_id\": personality_id,\n            \"persona_is_private\": is_private,\n            }\n    files = {}\n    if image:\n        files[\"persona_image\"] = image.file\n    return await make_api_request(db, \"POST\", \"/persona/create\", data, files)\n\n\nasync def edit_persona_async(\n        db: AsyncSession,\n        persona_id: str,\n        name: str,\n        description: str,\n        model_id: str,\n        lora_id: str,\n        personality_id: str,\n        image: Optional[UploadFile],\n        is_private: bool\n):\n    data = {\n        \"persona_id\": persona_id,\n        \"persona_name\": name,\n        \"persona_description\": description,\n        \"persona_model_id\": model_id,\n        'persona_personality_id': personality_id,\n        \"persona_lora_id\": lora_id,\n        # \"persona_describe_scene\": str(describe_scene),\n        \"persona_is_private\": str(is_private),\n    }\n\n    files = {}\n\n    if image:\n        files[\"persona_image\"] = image.file\n\n    return await make_api_request(db, \"PUT\", \"/persona/edit\", data, files)\n\n\nasync def delete_persona_async(db: AsyncSession, persona_id):\n    await make_api_request(db, \"DELETE\", \"/persona/delete\", {\"id\": persona_id})\n    return 200\n\n\nasync def get_user_persona_list(db: AsyncSession, user_id: str):\n    result = await db.execute(select(Persona).where(or_(Persona.users.any(id=user_id), Persona.owner=='everyone')))\n    return result.scalars().all()\n\n"}
{"type": "source_file", "path": "app/db/model/auth.py", "content": "from sqlalchemy import Column, String\nfrom sqlalchemy.orm import relationship\n\nfrom app.db.model.base import Base, user_model_association, user_personality_association, user_lora_association\n\n\nclass User(Base):\n    __tablename__ = 'users'\n    id = Column(String, unique=True, primary_key=True)\n    name = Column(String, unique=True, index=True)\n    image = Column(String, nullable=True)\n    last_lora = Column(String, nullable=True)\n    last_model = Column(String, nullable=True)\n\n    chats = relationship(\"Chat\", back_populates=\"user\", cascade=\"all, delete-orphan\", lazy='selectin')\n    personalities = relationship(\"Personality\", secondary=user_personality_association, back_populates=\"users\",\n                                 lazy='selectin')\n    models = relationship(\"Model\", secondary=user_model_association, back_populates=\"users\", lazy='selectin')\n    loras = (relationship(\"LoRA\", secondary=user_lora_association, back_populates=\"users\", lazy='selectin'))\n    personas = relationship(\"Persona\", back_populates=\"users\", lazy='selectin')\n"}
{"type": "source_file", "path": "app/db/ml_models/model_db.py", "content": "import asyncio\nimport os\nimport uuid\nfrom typing import Optional, List, Union\nimport ast\n\nfrom fastapi import UploadFile, BackgroundTasks, HTTPException\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom starlette.responses import JSONResponse\nfrom vllm.usage.usage_lib import UsageContext\n\nfrom app.core.engine import initialize_engine, create_serving_instances\nfrom app.db.auth.auth_db import get_user\nfrom app.db.db_common import get_entity\nfrom app.db.model.auth import User\nfrom app.db.model.ml_model import Model\nfrom app.hijacks.vllm import ExtendedAsyncCompleteServerArgs\nfrom app.middlewares.model_loader_block import set_block_requests\nfrom app.utils.database.images import save_image\nfrom app.utils.log import setup_custom_logger\nfrom app.utils.models.hf_downloader import download_model_async, check_file_in_huggingface_repo\nfrom app.utils.models.list_model import list_models_paths_in_hf_cache, format_repo_name_to_hf\nfrom app.utils.formatting.pydantic.privacy import PrivacyOptions\nfrom app.utils.models.model_paths import get_hf_path\nfrom app.utils.server.api_calls_to_main import make_api_request\nfrom app.utils.server.restarter import restart\nfrom app.utils.definitions import MODEL_PATHS\n\nlogger = setup_custom_logger(__name__)\n\n\nasync def get_model_path_or_url(model: Model, variant: str = None) -> str:\n    \"\"\"\n    Retrieve the path or URL of a model, optionally based on a variant.\n\n    Args:\n        model (Model): The model entity to query.\n        variant (str, optional): The specific variant of the model.\n\n    Returns:\n        str: The path or URL of the model.\n\n    Raises:\n        AssertionError: If the specified variant is not found.\n    \"\"\"\n    if variant:\n        model_path_dict = ast.literal_eval(model.path).get(variant, None)\n        assert model_path_dict, f\"Variant {variant} not found in model {model.name}\"\n        return model_path_dict\n    return model.url\n\n\nasync def get_model_list(db: AsyncSession) -> Optional[List[Model]]:\n    \"\"\"\n    Get a list of models associated with the current user.\n\n    Args:\n        current_user (User): The user whose models to retrieve.\n        db (AsyncSession): The database session.\n\n    Returns:\n        Optional[List[Model]]: A list of models or None if no models are found.\n    \"\"\"\n    return await get_entity(db, Model)\n\n\nasync def get_current_model(db: AsyncSession) -> Optional[Model]:\n    \"\"\"\n    Get the currently active model from the engine configuration.\n\n    Args:\n        db (AsyncSession): The database session.\n\n    Returns:\n        Optional[Model]: The currently active model or None if not found.\n    \"\"\"\n    from app.core.engine import async_engine\n    model_name = await async_engine.get_model_config()\n    return await get_entity(db, Model, url=model_name.model)\n\n\nasync def get_model(db: AsyncSession, url: str = None, model_id: str = None) -> Optional[Model]:\n    \"\"\"\n    Retrieve a model based on URL, model ID, or user.\n\n    Args:\n        db (AsyncSession): The database session.\n        url (str, optional): The URL of the model.\n        model_id (str, optional): The unique identifier of the model.\n        user (User, optional): The user associated with the model.\n\n    Returns:\n        Optional[Model]: The retrieved model or None if not found.\n    \"\"\"\n    return await get_entity(db, Model, url=url, id=model_id)\n\n\nasync def change_model_id_and_owner(db: AsyncSession, model_local_id: str, model_id: str, owner: str,\n                                    current_user: User):\n    \"\"\"\n    Change the model ID and owner for a specified model.\n\n    Args:\n        db (AsyncSession): The database session.\n        model_local_id (str): The local ID of the model to update.\n        model_id (str): The new global ID for the model.\n        owner (str): The new owner of the model.\n        current_user (User): The current user attempting the operation.\n\n    Raises:\n        Exception: If the model is not found in the database.\n    \"\"\"\n    model = await get_entity(db, Model, id=model_local_id, user_id=current_user.id)\n    if not model:\n        raise Exception(\"Model not found\")\n    model.id = model_id\n    model.owner = owner\n    db.add(model)\n    await db.commit()\n\n\nasync def download_model_api_(\n        model_id: str,\n        model_name: str,\n        model_arch: str,\n        owner: str,\n        model_description: str,\n        model_url: str,\n        file_variant: str,\n        tags: str,\n        image: Optional[UploadFile],\n        current_user: User,\n        db: AsyncSession,\n        background_tasks: Optional[BackgroundTasks],\n        _internal: bool = False,\n) -> Optional[Union[JSONResponse, HTTPException]]:\n    \"\"\"\n    Handles the API logic for downloading and updating a model, optionally with variant handling.\n\n    Args:\n        model_id (str): The global ID for the model.\n        model_name (str): The name of the model.\n        model_arch (str): The architecture of the model.\n        owner (str): The owner of the model.\n        model_description (str): The description of the model.\n        model_url (str): The URL of the model.\n        file_variant (str): The specific file variant to handle.\n        tags (str): The tags associated with the model.\n        image (UploadFile, optional): The image file associated with the model.\n        current_user (User): The current user attempting the operation.\n        db (AsyncSession): The database session.\n        background_tasks (BackgroundTasks, optional): Background tasks for asynchronous execution.\n        _internal (bool, optional): Internal flag to skip certain processing for internal calls.\n\n    Returns:\n        Optional[Union[JSONResponse, HTTPException]]: A success message or an HTTP error.\n    \"\"\"\n    model_in_db = await get_model(db, model_url)\n    image_filename = await save_image(current_user, image)\n\n    if model_in_db:\n        if model_in_db.id != model_id and model_in_db.owner == 'everyone':\n            model_in_db.id = model_id\n            model_in_db.owner = owner\n            model_in_db.name = model_name\n            model_in_db.base_architecture = model_arch\n            model_in_db.variants = file_variant\n            model_in_db.image = image_filename\n            model_in_db.description = model_description\n\n        if any([current_user.id not in user.id for user in model_in_db.users]):\n            db_user = await get_user(db, current_user.name)\n            model_in_db.users.append(db_user)\n            await db.commit()\n\n        elif file_variant and file_variant not in model_in_db.variants:\n            model_in_db.variants = f\"{model_in_db.variants},{file_variant}\"\n            await download_model_async(model_url, file_variant)\n\n            file_variant_path = await list_models_paths_in_hf_cache(\n                await format_repo_name_to_hf(model_url),\n                file_variant\n            )\n\n            old_path_dict = ast.literal_eval(model_in_db.path)\n            old_path_dict.update({file_variant: file_variant_path[0]})\n            model_in_db.path = str(old_path_dict)\n\n            await db.commit()\n        else:\n            return JSONResponse(content={\"message\": \"You already have this model\"})\n\n        return JSONResponse(content={\"message\": \"Model downloaded successfully\"})\n    try:\n        await download_model_async(model_url, file_variant)\n\n        model = Model(id=model_id, name=model_name, url=model_url, tags=tags,\n                      path=str(\n                          {file_variant: (await list_models_paths_in_hf_cache(await format_repo_name_to_hf(model_url),\n                                                                              file_variant))[0]}) if file_variant else\n                      (await list_models_paths_in_hf_cache(await format_repo_name_to_hf(model_url)))[0],\n                      working=True, description=model_description, users=[current_user],\n                      image=image_filename, variants=file_variant,\n                      owner=owner, base_architecture=model_arch)\n\n        db.add(model)\n        await db.commit()\n\n        if _internal:\n            return\n        return JSONResponse(content={\"message\": \"Model downloaded successfully\"}, status_code=200)\n    except Exception as e:\n        raise HTTPException(detail=f\"Error downloading model: {str(e)}\", status_code=500)\n\n\nasync def get_model_id_from_online(\n        db: AsyncSession,\n        name: str,\n        description: str,\n        image: Optional[UploadFile],\n        url: str,\n        is_private: bool) -> str:\n    \"\"\"\n    Retrieves or generates a unique model ID from an online server based on the model URL.\n\n    Args:\n        db (AsyncSession): The database session.\n        name (str): The name of the model.\n        description (str): The description of the model.\n        image (UploadFile, optional): The image associated with the model.\n        url (str): The URL of the model.\n        is_private (bool): Privacy flag indicating if the model is private.\n\n    Returns:\n        str: The unique model ID.\n    \"\"\"\n\n    async def ask_server_for_id(\n            url: str\n    ) -> Optional[str]:\n        params = {\n            \"model_url\": url\n        }\n        try:\n            response = await make_api_request(db, \"GET\", \"/model/get_id_from_url\", params)\n        except HTTPException:\n            return None\n        return response['id']\n\n    model_id = await ask_server_for_id(url)\n    if not model_id:\n        response = await create_model_async(db, name, description, image, url, is_private, '')\n        model_id = response['id']\n    return model_id\n\n\nasync def edit_model_async(\n        db: AsyncSession,\n        name: str,\n        model_tags: str,\n        description: str,\n        image: UploadFile,\n        is_private: bool,\n        model_id: str\n):\n    \"\"\"\n    Edits an existing model's metadata asynchronously.\n\n    Args:\n        db (AsyncSession): The database session.\n        name (str): The name of the model.\n        model_tags (str): Tags associated with the model.\n        description (str): The description of the model.\n        image (UploadFile): The image file to associate with the model.\n        is_private (bool): Flag indicating if the model is private.\n        model_id (str): The ID of the model being edited.\n\n    Returns:\n        int: HTTP status code indicating success (200).\n    \"\"\"\n    files = {}\n    if image:\n        files[\"model_image\"] = image.file\n    data = {\n        \"id_model\": model_id,\n        \"model_name\": name,\n        \"model_description\": description,\n        \"model_is_private\": is_private,\n        \"model_tags\": model_tags,\n    }\n    await make_api_request(db, \"PUT\", \"/model/edit\", data, files)\n    return 200\n\n\nasync def generate_entry_for_db(\n        model_id: str,\n        model_owner: str,\n        model_name: str,\n        model_description: str,\n        model_image: str,\n        model_url: str,\n        tags: str,\n        current_user: User,\n        background_tasks\n) -> Model:\n    \"\"\"\n    Generates a new model entry for the database including server configuration and loading into VRAM.\n\n    Args:\n        model_id (str): The model ID.\n        model_owner (str): The owner of the model.\n        model_name (str): The name of the model.\n        model_description (str): The description of the model.\n        model_image (str): The image filename for the model.\n        model_url (str): The URL of the model.\n        tags (str): Tags associated with the model.\n        current_user (User): The user associated with the model.\n        background_tasks (BackgroundTasks): Background tasks for handling long-running operations.\n\n    Returns:\n        Model: The newly created model object.\n    \"\"\"\n    from app.core.engine import (openai_serving_chat, delete_engine_model_from_vram, async_engine_args)\n    # create a restoration server configuration\n    server_conf = ExtendedAsyncCompleteServerArgs.from_yaml(\"last.yml\")\n    server_conf.gpu_memory_utilization = async_engine_args.gpu_memory_utilization\n\n    try:\n        model = Model(id=model_id, url=model_url, owner=model_owner,\n                      name=model_name, image=model_image, tags=tags,\n                      path=os.path.join(MODEL_PATHS, await get_hf_path(model_url)),\n                      # speed_value=await get_speed(eng_args), working=True,\n                      users=[current_user],\n                      description=model_description)\n    except Exception as e:\n        logger.error(f\"Error while adding model {model_url} to the database: {e}\")\n        model = Model(id=model_id, url=model_url, owner=model_owner,\n                      name=model_name, tags=tags,\n                      path=os.path.join(MODEL_PATHS, await get_hf_path(model_url)),\n                      working=False, users=[current_user],\n                      description=str(e))\n    # try to load the new model, if it fails reload the previous model\n    try:\n        set_block_requests(True)\n        while openai_serving_chat.engine_client.engine.has_unfinished_requests():  # noqa\n            await asyncio.sleep(0.1)\n\n        delete_engine_model_from_vram()\n        server_conf.model = model_url\n        server_conf.tokenizer = model_url\n        server_conf.served_model_name = [model_url]\n        await initialize_engine(server_conf.get_async_eng_args(), UsageContext.OPENAI_API_SERVER)\n        create_serving_instances(server_conf.served_model_name, server_conf)\n\n        server_conf.save_to_yaml()\n\n    except Exception as e:\n        background_tasks.add_task(restart(server_conf, True))\n        raise HTTPException(status_code=500, detail=\"Model loading had failed, restarting server, \" + str(e))\n    finally:\n        set_block_requests(False)\n    return model\n\n\nasync def add_model_to_db(\n        db: AsyncSession,\n        model_name: str,\n        model_description: str,\n        model_image: UploadFile,\n        model_image_name: str,\n        model_url: str,\n        privacy_settings: PrivacyOptions,\n        current_user: User,\n        tags: str, background_tasks\n):\n    \"\"\"\n    Adds a new model to the database and handles its download and configuration.\n\n    Args:\n        db (AsyncSession): The database session.\n        model_name (str): The name of the model.\n        model_description (str): The description of the model.\n        model_image (UploadFile): The image file for the model.\n        model_image_name (str): The name of the image file.\n        model_url (str): The URL of the model.\n        privacy_settings (PrivacyOptions): The privacy settings for the model.\n        current_user (User): The current user associated with the model.\n        tags (str): Tags associated with the model.\n        background_tasks (BackgroundTasks): Background tasks for asynchronous operations.\n\n    Returns:\n        Model: The newly created model object after successful addition to the database.\n    \"\"\"\n    base_architecture = await check_file_in_huggingface_repo(model_url, \"config.json\")\n    if not base_architecture:\n        raise HTTPException(status_code=404, detail=\"Model architecture could not be inferred\")\n    if privacy_settings.value != 'local':\n\n        json_response = await create_model_async(db, name=model_name, tags=tags,\n                                                 description=model_description,\n                                                 image=model_image,\n                                                 is_private=True if privacy_settings.value == \"private\" else False,\n                                                 url=model_url)\n\n        # use the id returned form the call\n        if not json_response.get('owner', None):\n            raise HTTPException(status_code=500,\n                                detail=\"Personality creation failed, no ID returned from the main server\")\n        try:\n            # download the model\n            await download_model_async(model_url)\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=\"Model creation failed due to a download issue \" + str(e))\n\n        return await generate_entry_for_db(model_id=json_response['id'], model_owner=json_response['owner'],\n                                           model_name=model_name, model_description=model_description,\n                                           model_image=model_image_name, tags=tags,\n                                           model_url=model_url, current_user=current_user,\n                                           background_tasks=background_tasks)\n    else:\n        return await generate_entry_for_db(model_id=str(uuid.uuid4()), model_owner=current_user.name,\n                                           model_name=model_name,\n                                           model_description=model_description, model_image=model_image_name,\n                                           model_url=model_url, tags=tags,\n                                           current_user=current_user, background_tasks=background_tasks)\n\n\nasync def create_model_async(\n        db: AsyncSession,\n        name: str,\n        description: str,\n        image: UploadFile,\n        url: str,\n        is_private: bool,\n        tags: str\n):\n    \"\"\"\n    Creates a new model entry asynchronously on the main server and returns the model details.\n\n    Args:\n        db (AsyncSession): The database session.\n        name (str): The name of the model.\n        description (str): The description of the model.\n        image (UploadFile): The image associated with the model.\n        url (str): The URL of the model.\n        is_private (bool): Indicates if the model is private.\n        tags (str): Tags associated with the model.\n\n    Returns:\n        dict: The response from the server with model details.\n    \"\"\"\n    files = {}\n    if image:\n        files[\"model_image\"] = image.file\n    data = {\n        \"model_name\": name,\n        \"model_description\": description,\n        \"tags\": tags,\n        \"model_token\": os.environ.get(\"PULSAR_HF_TOKEN\", None) if os.environ.get(\"PULSAR_SHOULD_SEND_PRIVATE_TOKEN\",\n                                                                                 False) else None,\n        \"model_url\": url,\n        \"model_is_private\": is_private,\n    }\n    return await make_api_request(db, \"POST\", \"/model/create\", data, files)\n\n\nasync def delete_model_async(\n        db: AsyncSession,\n        model_id: str\n):\n    \"\"\"\n    Deletes a model entry asynchronously from the database.\n\n    Args:\n        db (AsyncSession): The database session.\n        model_id (str): The ID of the model to be deleted.\n\n    Returns:\n        int: HTTP status code indicating success (200).\n    \"\"\"\n    await make_api_request(db, \"DELETE\", \"/model/delete\", {\"id\": model_id})\n    return 200\n"}
{"type": "source_file", "path": "app/db/ml_models/__init__.py", "content": ""}
{"type": "source_file", "path": "app/db/persona/__init__.py", "content": ""}
{"type": "source_file", "path": "app/db/model/token.py", "content": "from sqlalchemy import Column, Integer, String\n\nfrom app.db.model.base import Base\n\n\nclass Token(Base):\n    __tablename__ = \"token\"\n\n    id = Column(Integer, unique=True, primary_key=True)\n    access_token = Column(String, nullable=True)\n    refresh_token = Column(String, nullable=True)\n    url_token = Column(String, nullable=True)\n"}
{"type": "source_file", "path": "app/db/tokens/db_token.py", "content": "import base64\nimport json\nimport os\nimport time\n\nfrom cryptography.hazmat.primitives import serialization, hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom fastapi import Depends\nfrom sqlalchemy import select\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom app.db.model.token import Token\nfrom app.utils.database.get import get_db\nfrom app.utils.server.api_calls_to_main import make_api_request\n\n\nasync def get_access_token(db: AsyncSession):\n    token = await db.execute(select(Token).filter(Token.id == 1))\n    token = token.scalars().first()\n    if token is None:\n        return None\n    return token.access_token\n\n\nasync def get_refresh_token(db: AsyncSession):\n    token = await db.execute(select(Token).filter(Token.id == 1))\n    token = token.scalars().first()\n    if token is None:\n        return None\n    return token.refresh_token\n\n\nasync def set_access_token(access_token: str, db: AsyncSession):\n    token = await db.execute(select(Token).filter(Token.id == 1))\n    token = token.scalars().first()\n    token.access_token = access_token\n    await db.commit()\n\n\nasync def set_refresh_token(refresh_token: str, db: AsyncSession):\n    token = await db.execute(select(Token).filter(Token.id == 1))\n    token = token.scalars().first()\n    token.refresh_token = refresh_token\n    await db.commit()\n\n\nasync def get_setter_token(expiration_time=3600):\n    private_key = serialization.load_pem_private_key(os.environ.get('PULSAR_PRIVATE_KEY').replace(\"\\\\n\", \"\\n\").encode(), password=None)\n\n    payload = {\n        'user_id': os.environ.get('PULSAR_USER_ID'),\n        'exp': int(time.time()) + expiration_time\n    }\n\n    payload_bytes = json.dumps(payload).encode()\n    signature = private_key.sign(\n        payload_bytes,\n        padding.PSS(\n            mgf=padding.MGF1(hashes.SHA256()),\n            salt_length=padding.PSS.MAX_LENGTH\n        ),\n        hashes.SHA256()\n    )\n\n    token = base64.urlsafe_b64encode(payload_bytes + signature).decode()\n    return token\n\n\nasync def set_url_token(url_token: str, db: AsyncSession = Depends(get_db)):\n    token = await db.execute(select(Token).filter(Token.id == 1))\n    token = token.scalars().first()\n    token.url_token = url_token\n    await db.commit()\n\n\nasync def set_local_url_online(url: str):\n    async for db in get_db():\n        setter_token = await get_setter_token()\n\n        headers = {\n            \"token\": f\"{setter_token}\",\n            \"user-id\": f\"{os.environ.get('PULSAR_USER_ID')}\"\n        }\n\n        await make_api_request(db, \"PUT\", \"/update_url\", json_dict={\"url\": url}, headers=headers)\n\n    return 200  # if the request is successful\n"}
{"type": "source_file", "path": "app/db/tokens/__init__.py", "content": ""}
{"type": "source_file", "path": "app/hijacks/starlette.py", "content": "import json\nimport asyncio\nfrom starlette.responses import StreamingResponse\nfrom starlette.types import Send\nfrom app.db.model.chat import Message\nfrom app.utils.log import setup_custom_logger\n\nlogger = setup_custom_logger(__name__)\n\nclass WrappedStreamingResponse(StreamingResponse):\n    def __init__(self, content, db_session, chat, response_id, parent_message_id, model_id, *args,\n                 **kwargs):\n        self.db_session = db_session\n        self.chat = chat\n        self.response_id = response_id\n        self.parent_id = parent_message_id\n        self.model_id = model_id\n        self.accumulated_content = \"\"\n        super().__init__(content, *args, **kwargs)\n\n    def substitute_id(self, chunk):\n        if not isinstance(chunk, str):\n            return chunk\n        parts = chunk.split(\"data: \")\n        if len(parts) < 2:\n            return chunk\n        json_part = parts[1]\n        if json_part.strip() == \"[DONE]\":\n            return chunk\n        try:\n            data = json.loads(json_part)\n            data['chat_id'] = self.chat.id\n            data['id'] = self.response_id\n            updated_json_string = json.dumps(data)\n            return f\"data: {updated_json_string}\\n\\n\"\n        except json.JSONDecodeError:\n            return chunk\n\n    async def stream_response(self, send: Send) -> None:\n        await send(\n            {\n                \"type\": \"http.response.start\",\n                \"status\": self.status_code,\n                \"headers\": self.raw_headers,\n            }\n        )\n        try:\n            async for chunk in self.body_iterator:\n                chunk = self.substitute_id(chunk)\n                if not isinstance(chunk, bytes):\n                    chunk = chunk.encode(self.charset)\n\n                decoded_chunk = chunk.decode(self.charset)\n                self.accumulate_content(decoded_chunk)\n\n                await send({\"type\": \"http.response.body\", \"body\": chunk, \"more_body\": True})\n        except asyncio.CancelledError:\n            logger.warn(\"Stream cancelled by client\")\n            await self._safe_save_message_to_db()\n            raise\n        except Exception as e:\n            logger.warn(f\"Stream interrupted: {str(e)}\")\n        finally:\n            # Use asyncio.shield to ensure the save operation is not cancelled\n            try:\n                await asyncio.shield(self.save_message_to_db())\n            except Exception as e:\n                logger.error(f\"Error saving message to database: {e}\")\n            await send({\"type\": \"http.response.body\", \"body\": b\"\", \"more_body\": False})\n\n    def accumulate_content(self, chunk):\n        if 'INTERNAL-' in chunk:\n            return  # This is done in order to not log the internal process state\n        if chunk.startswith(\"data: \"):\n            content = chunk[6:].strip()  # Remove \"data: \" prefix\n            if content != \"[DONE]\":\n                try:\n                    data = json.loads(content)\n                    if 'choices' in data and len(data['choices']) > 0:\n                        delta = data['choices'][0].get('delta', {})\n                        if 'content' in delta:\n                            self.accumulated_content += delta['content']\n                except json.JSONDecodeError:\n                    logger.error(f\"Error parsing JSON content: {content}\")\n                    pass  # Ignore malformed JSON\n\n    async def save_message_to_db(self):\n        try:\n            async with self.db_session.begin():\n                if self.accumulated_content:\n                    new_message = Message(\n                        chat=self.chat,\n                        id=self.response_id,\n                        parent_message_id=self.parent_id,\n                        model_id=self.model_id,\n                        content={\"role\": \"assistant\", \"content\": self.accumulated_content}\n                    )\n                    self.db_session.add(new_message)\n        except Exception as e:\n            logger.error(f\"Error saving message to database: {str(e)}\")\n            raise\n"}
{"type": "source_file", "path": "app/db/personality/personality_db.py", "content": "import json\nfrom typing import List, Union\nfrom typing import Optional\n\nfrom fastapi import UploadFile\nfrom sqlalchemy import select, or_, and_\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import joinedload\n\nfrom app.db.model.personality import Personality\nfrom app.utils.formatting.pydantic.personality import PersonalitySchema\nfrom app.utils.server.api_calls_to_main import make_api_request\n\n\nasync def set_personality_model_request_as_dict(personality_name: str, personality_description: str) -> List[dict]:\n    return [{'content': 'Carefully answer the user request, be concise, precise and thoughtful.', 'role': 'system'},\n            {\n                'content': f'Based on this description: [Name: {personality_name}, Charachter Description:{personality_description}] answer the following question: ',\n                'role': 'user'}]\n\n\nasync def get_personality(db: AsyncSession, name: str, user: str):\n    result = await db.execute(select(Personality).where(and_(Personality.name == name, Personality.users.any(id=user))))\n    return result.scalars().first()\n\n\nasync def get_personality_by_id(db: AsyncSession, name: str, user: str = None):\n    conditions = [Personality.id == name]\n    if user:\n        conditions.append(or_(Personality.owner == 'everyone', Personality.users.any(id=user)))\n\n    result = await db.execute(\n        select(Personality)\n        .options(joinedload(Personality.users))\n        .where(and_(*conditions)\n               )\n    )\n    return result.scalars().first()\n\n\nasync def get_user_personality_list(db: AsyncSession, user_id: str):\n    result = await db.execute(select(Personality).where(Personality.users.any(id=user_id), ))\n    return result.scalars().all()\n\n\ndef format_dict_to_string(input_dict: Union[str, dict]):\n    \"\"\"\n    Converts a string into a dictionary and then into a string format where each key-value pair is\n    represented as 'key = value' on a new line.\n\n    Parameters:\n    - input_dict (dict): The dictionary to format.\n\n    Returns:\n    - str: A string representation of the dictionary with each item on a new line.\n    \"\"\"\n    if isinstance(input_dict, str):\n        input_dict = json.loads(input_dict)\n    formatted_items = [f\"{key} = {value}\" for key, value in input_dict.items()]\n    # Join all the items with a newline personality\n    result_string = \"\\n\".join(formatted_items)\n    return result_string\n\n\nasync def download_personality_from_server(db: AsyncSession, personality_id: str):\n    response = await make_api_request(db, \"GET\", f\"/personality/list?personality_id={personality_id}\")\n    return response['personality']\n\n\nasync def edit_personality_async(\n        db: AsyncSession,\n        # describe_scene: bool,\n        personality_id: str,\n        name: str,\n        description: str,\n        pre_prompt: str,\n        image: Optional[UploadFile],\n        is_private: bool\n):\n\n    data = {\n        \"personality_id\": personality_id,\n        \"personality_name\": name,\n        \"personality_description\": description,\n        \"personality_preprompt\": pre_prompt,\n        # \"personality_describe_scene\": str(describe_scene),\n        \"personality_is_private\": str(is_private),\n    }\n\n    files = {}\n\n    if image:\n        files[\"personality_image\"] = image.file\n    return await make_api_request(db, \"PUT\", \"/personality/edit\", data, files)\n\n\nasync def delete_personality_async(db: AsyncSession, personality_id):\n    await make_api_request(db, \"DELETE\", \"/personality/delete\", {\"id\": personality_id})\n    return 200\n\n\nasync def create_personality_async(\n        db: AsyncSession,\n        tags: str,\n        # describe_scene: bool,\n        name: str,\n        description: str,\n        pre_prompt: dict,\n        image: Optional[UploadFile],\n        is_private: bool\n):\n    if isinstance(pre_prompt, dict):\n        pre_prompt = json.dumps(pre_prompt)  # Ensure pre_prompt is JSON serialized\n\n    data = {\n        \"personality_name\": name,\n        \"personality_description\": description,\n        \"personality_preprompt\": pre_prompt,\n        \"personality_tags\": tags,\n        # \"personality_describe_scene\": str(describe_scene),\n        \"personality_is_private\": str(is_private),\n    }\n\n    files = {}\n\n    if image:\n        files[\"personality_image\"] = image.file\n\n    return await make_api_request(db, \"POST\", \"/personality/create\", data, files)\n\n\nasync def change_personality_id_and_owner(db, model_local_id, personality_id, owner, current_user_username):\n    query = select(Personality).where(\n        and_(or_(Personality.users.any(username=current_user_username), Personality.owner == 'everyone'),\n        Personality.id == model_local_id)\n    )\n    result = await db.execute(query)\n    personality = result.scalars().first()\n    if not personality:\n        raise Exception(\"Personality not found\")\n    personality.id = personality_id\n    personality.owner = owner\n    db.add(personality)\n    await db.commit()\n\n\n# Function to modify specific fields in the personality JSON\ndef modify_personality_fields(personality: PersonalitySchema, updates: dict):\n    for field, value in updates.items():\n        if hasattr(personality, field):\n            setattr(personality, field, value)\n    return personality\n"}
{"type": "source_file", "path": "app/hijacks/vllm.py", "content": "import dataclasses\nimport os\nimport warnings\nfrom dataclasses import dataclass\nfrom dataclasses import field\nfrom typing import List, Optional, Dict\n\nimport torch.cuda\nimport yaml\nfrom vllm import AsyncEngineArgs\n\nfrom app.utils.log import setup_custom_logger\nfrom app.utils.memory.cuda_mem import get_total_cuda_memory, get_used_cuda_memory\n\nlogger = setup_custom_logger(__name__)\n\n\ndef astra_parser_wrapper(parser):\n    \"\"\"\n    Wrap a parser to add arguments\n    :param parser:\n    :return:\n    \"\"\"\n    parser.description = \"vLLM Based OpenAI-Compatible RESTful API server By AstraMind AI\"\n    parser.add_argument(\"--auto-quantized-fallback\", type=bool, default=True,\n                        help=\"Automatically pick a quantized model if the model is too big\")\n    parser.add_argument(\"--quant-type-preference\", type=str, default=\"GPTQ\",\n                        help=\"The preference for quantization types, either GPTQ or AWQ\")\n    parser.add_argument(\"--use-config-file\", type=bool, default=True, help=\"Whether to use a server config file\")\n    parser.add_argument(\"--server-config-file\", type=str, default=\"last.yml\",\n                        help=\"The path to the server configuration file\")\n    parser.add_argument(\"--enable-tts\", type=bool, default=False, help=\"Whether to enable TTS\")\n    parser.add_argument(\"--enable-txt2img\", type=bool, default=False, help=\"Whether to enable txt2img\")\n    return parser\n\n\n@dataclass\nclass ExtendedAsyncEngineArgs(AsyncEngineArgs):\n    auto_quantized_fallback: bool = True\n    quant_type_preference: str = \"GPTQ\"\n    enable_tts: bool = False\n    enable_txt2img: bool = False\n    use_config_file: bool = True\n    server_config_file: str = \"last.yml\"\n    allow_unsafe_local_requests: bool = False\n    will_local_auth_token_rotate: bool = False\n    trust_remote_code = True\n\n    @classmethod\n    def from_model_conf(cls, config: dict):\n        field_names = {field.name for field in dataclasses.fields(cls)}\n        filtered_config = {k: v for k, v in config.items() if k in field_names}\n        eng_args = cls(**filtered_config)\n        eng_args.set_smart_gpu_memory_utilization()\n        return eng_args\n\n    @classmethod\n    def from_yaml(cls, file: str):\n        from server import CONFIG_FILE_PATH\n        path = os.path.join(CONFIG_FILE_PATH, file)\n        with open(path, \"r\") as f:\n            eng_args = cls.from_model_conf(yaml.safe_load(f))\n            return eng_args\n\n    def set_smart_gpu_memory_utilization(self):\n        total_memory = get_total_cuda_memory()\n        used_memory = get_used_cuda_memory()\n        if self.enforce_eager:\n            usage_percentage = (1 - (used_memory / total_memory)) * 0.98\n        else:\n            # we subtract the cuda graph additional memory usage\n            usage_percentage = ((1 - (used_memory / total_memory)) - (2.5 / (total_memory / 1024**3))) * 0.98\n\n        # TODO: Define actual whisper / tts memory usage based on the fixed execution load and the available memory\n        #whisper_memory_usage = 0.1 if self.enable_tts else 0\n        # TODO: Define actual txt2img memory usage\n        #txt2img_memory_usage = 0.15 if self.enable_txt2img else 0\n        #usage_percentage -= whisper_memory_usage + txt2img_memory_usage\n        self.gpu_memory_utilization = usage_percentage\n\n\n    def save_to_yaml(self):\n        from server import CONFIG_FILE_PATH\n        path = os.path.join(CONFIG_FILE_PATH, \"last.yml\")\n        with open(path, \"w\") as f:\n            yaml.dump(dataclasses.asdict(self), f)\n\n\n@dataclass\nclass ExtendedAsyncCompleteServerArgs(ExtendedAsyncEngineArgs):\n    host: Optional[str] = '127.0.0.1'\n    port: int = 40000\n    uvicorn_log_level: str = 'info'\n    allow_credentials: bool = False\n    allowed_origins: List[str] = field(default_factory=lambda: ['*'])\n    allowed_methods: List[str] = field(default_factory=lambda: ['*'])\n    allowed_headers: List[str] = field(default_factory=lambda: ['*'])\n    api_key: Optional[str] = None\n    lora_modules: Optional[Dict] = None\n    chat_template: Optional[str] = None\n    response_role: str = 'assistant'\n    ssl_keyfile: Optional[str] = None\n    ssl_certfile: Optional[str] = None\n    ssl_ca_certs: Optional[str] = None\n    ssl_cert_reqs: int = 0\n    root_path: Optional[str] = None\n    middleware: List = field(default_factory=list)\n\n    use_config_file: bool = True\n    server_config_file: str = \"last.yml\"\n\n    kv_cache_dtype = 'fp8'\n\n    max_lora_rank = 48\n    max_loras = 4\n    enable_lora = True\n\n    tunnel_type: Optional[str] = None\n    ngrok_auth_token = os.environ.get('PULSAR_NGROK_TOKEN', None)\n\n    def get_async_eng_args(self):\n        valid_fields = {f.name for f in dataclasses.fields(ExtendedAsyncEngineArgs)}\n        valid_data = {k: v for k, v in dataclasses.asdict(self).items() if k in valid_fields}\n        return ExtendedAsyncEngineArgs(**valid_data)\n\n    def define_optimal_lora_config(self):\n        usable_memory = get_total_cuda_memory() * self.gpu_memory_utilization\n        if usable_memory <= 6 * 1000 ** 3:\n            self.max_loras = 2\n            self.max_lora_rank = 32\n        elif usable_memory <= 12 * 1000 ** 3:\n            self.max_loras = 4\n            self.max_lora_rank = 32\n        elif usable_memory <= 24 * 1000 ** 3:\n            self.max_loras = 8\n            self.max_lora_rank = 32\n\n    def check_tunnel_config(self):\n        if self.tunnel_type == 'lt':\n            try:\n                import py_localtunnel # noqa\n            except ImportError:\n                logger.error(\"LocalTunnel is not installed, please install it using `pip install py-localtunnel`\")\n                return False\n        elif self.tunnel_type == 'ngrok':\n            try:\n                import ngrok # noqa\n            except ImportError:\n                logger.error(\"PyNgrok is not installed, please install it using `pip install ngrok`\")\n                return False\n\n    def check_gpu_arch_and_set_constraints(self):\n        \"\"\"\n        Check the GPU architecture and set constraints accordingly\n        \"\"\"\n        if torch.cuda.is_available():\n            device = torch.device(\"cuda\")\n            gpu_name = torch.cuda.get_device_name(device)\n            gpu_arch = torch.cuda.get_device_capability(device)\n            logger.debug(f\"GPU Name: {gpu_name}, GPU Capabilities: {gpu_arch[0]}.{gpu_arch[1]}\")\n            if float(f\"{gpu_arch[0]}.{gpu_arch[1]}\") < 7:\n                logger.warn(\"Your GPU does not support LoRA and bfloat16, they'll be disabled\")\n                self.dtype = \"float16\"\n                self.kv_cache_dtype = 'auto'\n                self.enable_lora = False\n            elif float(f\"{gpu_arch[0]}.{gpu_arch[1]}\") <= 7.5:  # gpu is not capable of any dtype but fp16, lora is enabled\n                self.dtype = \"float16\"\n                self.kv_cache_dtype = 'auto'\n\n            self.enable_lora = True\n        else:\n            logger.info(\"No GPU found, please check if this behaviour is desired\")\n\n    def __post_init__(self):\n        super().__post_init__()\n        self.check_gpu_arch_and_set_constraints()\n        if self.enable_lora:\n            self.define_optimal_lora_config()\n        self.served_model_name = [self.model]\n        if self.tokenizer != self.model:\n            logger.warn(\"The chosen tokenizer is different from the model, please be sure this is intended behaviour\")\n        self.check_tunnel_config()\n"}
{"type": "source_file", "path": "app/hijacks/__init__.py", "content": ""}
{"type": "source_file", "path": "app/hijacks/protocols/extended_oai.py", "content": "from typing import Union, List, Optional, Dict, Any\n\nimport openai.types.chat\nfrom pydantic import BaseModel\nfrom vllm.entrypoints.openai.protocol import ChatCompletionRequest,    CustomChatCompletionMessageParam\n\n\nclass ExtendedCustomChatCompletionMessageParam(CustomChatCompletionMessageParam):\n    id: Optional[int]\n\n\nChatCompletionMessageParam = Union[\n    openai.types.chat.ChatCompletionMessageParam,\n    CustomChatCompletionMessageParam,\n    ExtendedCustomChatCompletionMessageParam\n]\n\n\nclass ExtendedChatCompletionRequest(ChatCompletionRequest):\n    messages: List[ChatCompletionMessageParam]\n    personality_id: Optional[str] = None\n    chat_id: Optional[str] = None\n    system_prompt: Optional[str] = None\n    pulsar_boost: Optional[bool] = None\n    num_rollouts: Optional[int] = None\n    chat_history_cutoff_percentage: Optional[float] = None\n    max_depth: Optional[int] = None\n    is_regeneration: Optional[bool] = None\n\n    class Config:\n        extra = \"allow\"\n\n    def to_standard_request(self) -> ChatCompletionRequest:\n        # Create a dictionary of all fields that are also in ChatCompletionRequest\n        standard_fields = {}\n\n        for field, value in self.model_dump().items():\n            if field in ChatCompletionRequest.model_fields:\n                standard_fields[field] = value\n\n        if not standard_fields.get('tools', None) and standard_fields.get('tool_choice', 'none') == 'none':\n            standard_fields.pop('tool_choice', None)\n            standard_fields.pop('tools', None)\n        if standard_fields.get('top_logprobs', 0) == 0 and not standard_fields.get('logprobs', False):\n            standard_fields.pop('top_logprobs', None)\n            standard_fields.pop('logprobs', None)\n        # Handle messages conversion\n        standard_fields['messages'] = self._convert_messages(self.messages)\n\n        # Create and return a new ChatCompletionRequest\n        return ChatCompletionRequest(**standard_fields)\n\n    @staticmethod\n    def _convert_messages(messages: List[ChatCompletionMessageParam]) -> List[Dict[str, Any]]:\n        converted_messages = []\n        for msg in messages:\n            if isinstance(msg, (dict, BaseModel)):\n                # Convert to dict if it's a Pydantic model\n                msg_dict = msg.model_dump() if isinstance(msg, BaseModel) else msg\n                # Remove 'id' if present (as it's not in standard ChatCompletionMessageParam)\n                msg_dict.pop('id', None)\n                msg_dict.pop('version', None)\n                msg_dict.pop('parent_message_id', None)\n                converted_messages.append(msg_dict)\n            else:\n                # If it's already in a format accepted by ChatCompletionRequest, use it as is\n                converted_messages.append(msg)\n        return converted_messages\n\n"}
{"type": "source_file", "path": "app/hijacks/openai.py", "content": "import uuid\nfrom typing import Union, AsyncGenerator, Optional\n\nfrom starlette.requests import Request\nfrom vllm.entrypoints.openai.protocol import ErrorResponse, ChatCompletionResponse\nfrom vllm.entrypoints.openai.serving_chat import OpenAIServingChat\n\nfrom app.hijacks.protocols.extended_oai import ExtendedChatCompletionRequest\nfrom app.utils.formatting.chat.formatter import extract_parameter_from_request\nfrom app.utils.log import setup_custom_logger\nfrom app.services.logic_booster.pulsar_boost import PulsarBoost\n\nlogger = setup_custom_logger(__name__)\n\n\nclass ExtendedOpenAIServingChat(OpenAIServingChat):\n    def __init__(self, api_url, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.pulsar_boost_solver = PulsarBoost(api_url, self.model_config.model)\n\n    async def create_pulsar_chat_completion(\n            self,\n            request: ExtendedChatCompletionRequest,\n            raw_request: Optional[Request] = None,\n    ) -> Union[AsyncGenerator[str, None], ChatCompletionResponse, ErrorResponse]:\n        request.truncate_prompt_tokens = int(\n            float(request.chat_history_cutoff_percentage / 100)\n            * self.max_model_len ) # this set the \"history\" if the user does not set it himself\n        return await super().create_chat_completion(request, raw_request)\n\n    async def stream_chat_completion_with_rstar(\n            self,\n            request: ExtendedChatCompletionRequest,\n\n    ) -> Union[ErrorResponse, AsyncGenerator[str, None], ChatCompletionResponse]:\n        try:\n            request_id = f'chat-{uuid.uuid4()}'\n\n            return self.pulsar_boost_solver.process(\n                request,\n                request_id,\n\n            )\n\n        except Exception as e:\n            logger.error(f\"Error in streaming results: {str(e)}\")\n            return self.create_error_response(str(e))\n\n    async def generate_response(\n            self,\n            request: ExtendedChatCompletionRequest,\n            raw_request: Optional[Request] = None,\n    ) -> Union[ErrorResponse, AsyncGenerator[str, None], ChatCompletionResponse]:\n        (is_pulsar_boost, _) = await extract_parameter_from_request(request, ['pulsar_boost', ])\n        if is_pulsar_boost:\n            return await self.stream_chat_completion_with_rstar(request)\n        return await self.create_pulsar_chat_completion(request, raw_request)\n"}
