{"repo_info": {"repo_name": "scene_graph_commonsense", "repo_owner": "bowen-upenn", "repo_url": "https://github.com/bowen-upenn/scene_graph_commonsense"}}
{"type": "source_file", "path": "dataloader.py", "content": "import os\nimport numpy as np\nimport torch\nimport json\nfrom PIL import Image\nimport string\nimport tqdm\nimport torchvision\nfrom torchvision import transforms\nfrom collections import Counter\nfrom utils import *\nfrom dataset_utils import *\nimport cv2\nimport random\nfrom dataset_utils import TwoCropTransform\n\n\nclass PrepareVisualGenomeDataset(torch.utils.data.Dataset):\n    def __init__(self, annotations):\n        with open(annotations) as f:\n            self.annotations = json.load(f)\n\n    def __getitem__(self, idx):\n        return None\n\n    def __len__(self):\n        return len(self.annotations['images'])\n\n\nclass VisualGenomeDataset(torch.utils.data.Dataset):\n    def __init__(self, args, device, annotations, training):\n        self.args = args\n        self.device = device\n        self.training = training\n        self.image_dir = self.args['dataset']['image_dir']\n        self.annot_dir = self.args['dataset']['annot_dir']\n        self.subset_indices = None\n        with open(annotations) as f:\n            self.annotations = json.load(f)\n        self.image_transform = transforms.Compose([transforms.ToTensor(),\n                                                   transforms.Resize(size=600, max_size=1000, antialias=True)])\n        self.image_transform_to_tensor = transforms.ToTensor()\n        self.image_transform_square = transforms.Compose([transforms.ToTensor(),\n                                                     transforms.Resize((self.args['models']['image_size'], self.args['models']['image_size']), antialias=True)])\n        self.image_transform_square_jitter = transforms.Compose([transforms.ToTensor(),\n                                                            transforms.RandomApply([\n                                                                 transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n                                                            ], p=0.8),\n                                                            transforms.Resize((self.args['models']['image_size'], self.args['models']['image_size']), antialias=True)])\n        self.image_transform_contrastive = TwoCropTransform(self.image_transform_square, self.image_transform_square_jitter)\n        self.image_norm = transforms.Compose([transforms.Normalize((102.9801, 115.9465, 122.7717), (1.0, 1.0, 1.0))])\n\n        self.train_cs_step = 1  # 1 or 2\n        self.triplets_train_gt = {}\n        self.triplets_train_pseudo = {}\n        self.commonsense_aligned_triplets = {}\n        self.commonsense_violated_triplets = {}\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Dataloader Outputs:\n            image: an image in the Visual Genome dataset (to predict bounding boxes and labels in DETR-101)\n            image_depth: the estimated image depth map\n            categories: categories of all objects in the image\n            super_categories: super-categories of all objects in the image\n            masks: squared masks of all objects in the image\n            bbox: bounding boxes of all objects in the image\n            relationships: all target relationships in the image\n            subj_or_obj: the edge directions of all target relationships in the image\n        \"\"\"\n        annot_name = self.annotations['images'][idx]['file_name'][:-4] + '_annotations.pkl'\n        annot_path = os.path.join(self.annot_dir, annot_name)\n        if os.path.exists(annot_path):\n            curr_annot = torch.load(annot_path)\n        else:\n            return None\n\n        if self.args['training']['run_mode'] == 'prepare_cs' and self.train_cs_step == 2:\n            # load saved commonsense-aligned and violated triplets for each current image\n            annot_name_yes = 'cs_aligned_top10_gpt4v/' if self.args['models']['llm_model'] == 'gpt4v' else 'cs_aligned_top10_gpt3p5_temp/'\n            annot_name_yes += self.annotations['images'][idx]['file_name'][:-4] + '_pseudo_annotations.pkl'\n            annot_name_yes = os.path.join(self.annot_dir, annot_name_yes)\n            if os.path.exists(annot_name_yes):\n                curr_annot_yes = torch.load(annot_name_yes)\n            else:\n                return None\n            annot_name_no = 'cs_violated_top10_gpt4v/' if self.args['models']['llm_model'] == 'gpt4v' else 'cs_violated_top10_gpt3p5_temp/'\n            annot_name_no += self.annotations['images'][idx]['file_name'][:-4] + '_pseudo_annotations.pkl'\n            annot_name_no = os.path.join(self.annot_dir, annot_name_no)\n            if os.path.exists(annot_name_no):\n                curr_annot_no = torch.load(annot_name_no)\n            else:\n                return None\n            # print(annot_name_yes, annot_name_no)\n\n        image_path = os.path.join(self.image_dir, self.annotations['images'][idx]['file_name'])\n        image = cv2.imread(image_path)\n        width, height = image.shape[0], image.shape[1]\n\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = 255 * self.image_transform_contrastive(image)\n        image, image_aug = image[0], image[1]\n        image = self.image_norm(image)  # squared size that unifies the size of feature maps\n        image_aug = self.image_norm(image_aug)\n\n        if self.args['training']['run_mode'] == 'eval' or self.args['training']['run_mode'] == 'eval_cs':\n            del image_aug\n            # if self.args['training']['eval_mode'] != 'pc':\n            image_nonsq = Image.open(image_path).convert('RGB')  # keep original shape ratio, not reshaped to square\n            image_nonsq = 255 * self.image_transform(image_nonsq)[[2, 1, 0]]  # BGR\n            image_nonsq = self.image_norm(image_nonsq)\n\n        image_depth = curr_annot['image_depth'] if self.args['models']['use_depth'] \\\n            else torch.zeros(1, self.args['models']['feature_size'], self.args['models']['feature_size'])\n\n        categories = curr_annot['categories']\n        super_categories = curr_annot['super_categories']\n        # total in train: 60548, >20: 2651, >30: 209, >40: 23, >50: 4. Don't let rarely long data dominate the computation power.\n        if categories.shape[0] <= 1 or categories.shape[0] > 20:\n            return None\n        bbox = curr_annot['bbox']   # x_min, x_max, y_min, y_max\n\n        bbox_raw = bbox.clone() / self.args['models']['feature_size']\n        bbox_raw[:2] *= height\n        bbox_raw[2:] *= width\n        bbox_raw = bbox_raw.ceil().int()\n        if torch.any(bbox_raw[:, 1] - bbox_raw[:, 0] <= 0) or torch.any(bbox_raw[:, 3] - bbox_raw[:, 2] <= 0):\n            return None\n        bbox = bbox.int()\n\n        subj_or_obj = curr_annot['subj_or_obj']\n        relationships = curr_annot['relationships']\n\n        relationships_reordered = []\n        if self.args['dataset']['supcat_clustering'] == 'gpt2':\n            rel_reorder_dict = gpt2_index_map()\n        elif self.args['dataset']['supcat_clustering'] == 'bert':\n            rel_reorder_dict = bert_index_map()\n        elif self.args['dataset']['supcat_clustering'] == 'clip':\n            rel_reorder_dict = clip_index_map()\n        else: # motif\n            rel_reorder_dict = relation_class_freq2scat()\n\n        for rel in relationships:\n            rel[rel == 12] = 4      # wearing <- wears\n            relationships_reordered.append(rel_reorder_dict[rel])\n        relationships = relationships_reordered\n\n        if self.args['training']['run_mode'] == 'prepare_cs' and self.train_cs_step == 2:\n            self.accumulate_triplets(categories, relationships, subj_or_obj, bbox, curr_annot_yes, curr_annot_no)\n\n        \"\"\"\n        image: the image transformed to a squared shape of size self.args['models']['image_size'] (to generate a uniform-sized image features)\n        image_nonsq: the image transformed to a shape of size=600, max_size=1000 (used in SGCLS and SGDET to predict bounding boxes and labels in DETR-101)\n        image_aug: the image transformed to a squared shape of size self.args['models']['image_size'] with color jittering (used in contrastive learning only)\n        image_raw: the image transformed to tensor retaining its original shape (used in CLIP only)\n        \"\"\"\n\n        if self.args['training']['run_mode'] == 'eval' or self.args['training']['run_mode'] == 'eval_cs':\n            if self.args['training']['save_vis_results'] and self.args['training']['eval_mode'] == 'pc':\n                return image, image_nonsq, image_depth, categories, super_categories, bbox, relationships, subj_or_obj, annot_name, height, width, triplets, bbox_raw\n            else:\n                return image, image_nonsq, image_depth, categories, super_categories, bbox, relationships, subj_or_obj, annot_name\n        else:\n            return image, image_aug, image_depth, categories, super_categories, bbox, relationships, subj_or_obj, annot_name\n\n\n    def accumulate_triplets(self, categories, relationships, subj_or_obj, bbox, annot_name_yes, annot_name_no):\n        \"\"\"\n        This function is called iff run_mode == 'prepare_cs' and train_cs_step == 2\n        It accumulates all the commonsense-aligned and violated triplets for each current image\n        by gradually adding triplets into two dictionaries named self.commonsense_aligned_triplets and self.commonsense_violated_triplets during the inference\n        \"\"\"\n        for i, (rels, sos) in enumerate(zip(relationships, subj_or_obj)):\n            for j, (rel, so) in enumerate(zip(rels, sos)):\n                if so == 1:  # if subject\n                    key = (categories[i + 1].item(), rel.item(), categories[j].item())\n                elif so == 0:  # if object\n                    key = (categories[j].item(), rel.item(), categories[i + 1].item())\n                else:\n                    continue\n\n                # check if the key is already in the dictionary, if not, initialize the count to 0\n                if key not in self.triplets_train_gt:\n                    self.triplets_train_gt[key] = 0\n                self.triplets_train_gt[key] += 1\n\n        for edge in annot_name_yes:\n            subject_bbox, relation_id, object_bbox, _, _ = edge\n\n            # Match bbox for subject and object\n            subject_idx = match_bbox(subject_bbox, bbox, self.args['training']['eval_mode'])\n            object_idx = match_bbox(object_bbox, bbox, self.args['training']['eval_mode'])\n            if subject_idx == object_idx:\n                continue\n\n            if subject_idx is not None and object_idx is not None:\n                key = (categories[subject_idx].item(), relation_id, categories[object_idx].item())\n                # check if the key is already in the dictionary, if not, initialize the count to 0\n                if key not in self.commonsense_aligned_triplets:\n                    self.commonsense_aligned_triplets[key] = 0\n                self.commonsense_aligned_triplets[key] += 1\n\n        for edge in annot_name_no:\n            subject_bbox, relation_id, object_bbox, _, _ = edge\n\n            # Match bbox for subject and object\n            subject_idx = match_bbox(subject_bbox, bbox, self.args['training']['eval_mode'])\n            object_idx = match_bbox(object_bbox, bbox, self.args['training']['eval_mode'])\n            if subject_idx == object_idx:\n                continue\n\n            if subject_idx is not None and object_idx is not None:\n                key = (categories[subject_idx].item(), relation_id, categories[object_idx].item())\n                # check if the key is already in the dictionary, if not, initialize the count to 0\n                if key not in self.commonsense_violated_triplets:\n                    self.commonsense_violated_triplets[key] = 0\n                self.commonsense_violated_triplets[key] += 1\n\n\n    def save_all_triplets(self):\n        \"\"\"\n        This function is called iff run_mode == 'prepare_cs' and train_cs_step == 2\n        It saves the two dictionaries named self.commonsense_aligned_triplets and self.commonsense_violated_triplets as two .pt files at the end of the inference\n        \"\"\"\n        # add ground truth annotations to the commonsense_aligned_triplets\n        for k, v, in self.triplets_train_gt.items():\n            if k not in self.commonsense_aligned_triplets.keys():\n                self.commonsense_aligned_triplets[k] = v\n            else:\n                self.commonsense_aligned_triplets[k] += v\n        # remove ground truth annotations from the commonsense_violated_triplets\n        self.commonsense_violated_triplets = {k: v for k, v in self.commonsense_violated_triplets.items() if k not in self.triplets_train_gt.keys()}\n\n        print('len(self.triplets_train_gt), len(self.commonsense_violated_triplets), len(self.commonsense_aligned_triplets)',\n              len(self.triplets_train_gt), len(self.commonsense_violated_triplets), len(self.commonsense_aligned_triplets))\n        print('Saving triplets/commonsense_violated_triplets.pt and triplets/commonsense_aligned_triplets.pt')\n\n        if self.args['models']['llm_model'] == 'gpt4v':\n            torch.save(self.commonsense_violated_triplets, 'triplets/commonsense_violated_triplets_gpt4v.pt')\n            torch.save(self.commonsense_aligned_triplets, 'triplets/commonsense_aligned_triplets_gpt4v.pt')\n        else:\n            torch.save(self.commonsense_violated_triplets, 'triplets/commonsense_violated_triplets_gpt3p5_temp.pt')\n            torch.save(self.commonsense_aligned_triplets, 'triplets/commonsense_aligned_triplets_gpt3p5_temp.pt')\n\n\n    def __len__(self):\n        return len(self.annotations['images'])\n\n\n\nclass PrepareOpenImageV6Dataset(torch.utils.data.Dataset):\n    def __init__(self, args, annotations):\n        self.image_dir = \"../datasets/open_image_v6/images/\"\n        self.image_transform = transforms.Compose([transforms.ToTensor(),\n                                                   transforms.Resize((args['models']['image_size'], args['models']['image_size']))])\n        with open(annotations) as f:\n            self.annotations = json.load(f)\n\n    def __getitem__(self, idx):\n        rel = self.annotations[idx]['rel']\n        image_id = self.annotations[idx]['img_fn'] + '.jpg'\n        image_path = os.path.join(self.image_dir, image_id)\n        image = Image.open(image_path).convert('RGB')\n        image = self.image_transform(image)\n        return rel, image, self.annotations[idx]['img_fn']\n\n    def __len__(self):\n        return len(self.annotations)\n\n\nclass OpenImageV6Dataset(torch.utils.data.Dataset):\n    def __init__(self, args, device, annotations):\n        self.args = args\n        self.device = device\n        self.image_dir = \"../datasets/open_image_v6/images/\"\n        self.depth_dir = \"../datasets/open_image_v6/image_depths/\"\n        with open(annotations) as f:\n            self.annotations = json.load(f)\n        self.image_transform = transforms.Compose([transforms.ToTensor(),\n                                                   transforms.Resize(size=600, max_size=1000)])\n        self.image_transform_s = transforms.Compose([transforms.ToTensor(),\n                                                     transforms.Resize((self.args['models']['image_size'], self.args['models']['image_size']))])\n        self.image_norm = transforms.Compose([transforms.Normalize((103.530, 116.280, 123.675), (1.0, 1.0, 1.0))])\n        self.rel_super_dict = oiv6_reorder_by_super()\n\n    def __getitem__(self, idx):\n        # print('idx', idx, self.annotations[idx])\n        image_id = self.annotations[idx]['img_fn']\n        image_path = os.path.join(self.image_dir, image_id + '.jpg')\n\n        image = Image.open(image_path).convert('RGB')\n        h_img, w_img = self.annotations[idx]['img_size'][1], self.annotations[idx]['img_size'][0]\n\n        image = 255 * self.image_transform(image)[[2, 1, 0]]  # BGR\n        image = self.image_norm(image)  # original size that produce better bounding boxes\n        image_s = Image.open(image_path).convert('RGB')\n        image_s = 255 * self.image_transform_s(image_s)[[2, 1, 0]]  # BGR\n        image_s = self.image_norm(image_s)  # squared size that unifies the size of feature maps\n\n        if self.args['models']['use_depth']:\n            image_depth = torch.load(self.depth_dir + image_id + '_depth.pt')\n        else:\n            image_depth = torch.zeros(1, self.args['models']['feature_size'], self.args['models']['feature_size'])\n\n        categories = torch.tensor(self.annotations[idx]['det_labels'])\n        if len(categories) <= 1 or len(categories) > 20: # 25\n            return None\n\n        bbox = []\n        raw_bbox = self.annotations[idx]['bbox']    # x_min, y_min, x_max, y_max\n        masks = torch.zeros(len(raw_bbox), self.args['models']['feature_size'], self.args['models']['feature_size'], dtype=torch.uint8)\n        for i, b in enumerate(raw_bbox):\n            box = resize_boxes(b, (h_img, w_img), (self.args['models']['feature_size'], self.args['models']['feature_size']))\n            masks[i, box[0]:box[2], box[1]:box[3]] = 1\n            bbox.append([box[0], box[2], box[1], box[3]])  # x_min, x_max, y_min, y_max\n        bbox = torch.as_tensor(bbox)\n\n        raw_relations = self.annotations[idx]['rel']\n        relationships = []\n        subj_or_obj = []\n        for i in range(1, len(categories)):\n            relationships.append(-1 * torch.ones(i, dtype=torch.int64))\n            subj_or_obj.append(-1 * torch.ones(i, dtype=torch.float32))\n\n        for triplet in raw_relations:\n            # if curr is the subject\n            if triplet[0] > triplet[1]:\n                relationships[triplet[0]-1][triplet[1]] = self.rel_super_dict[triplet[2]]\n                subj_or_obj[triplet[0]-1][triplet[1]] = 1\n            # if curr is the object\n            elif triplet[0] < triplet[1]:\n                relationships[triplet[1]-1][triplet[0]] = self.rel_super_dict[triplet[2]]\n                subj_or_obj[triplet[1]-1][triplet[0]] = 0\n\n        return image, image_s, image_depth, categories, None, masks, bbox, relationships, subj_or_obj\n\n    def __len__(self):\n        return len(self.annotations)\n\n"}
{"type": "source_file", "path": "main.py", "content": "import numpy as np\nimport torch\nimport torchvision\nimport torch.optim as optim\nfrom torch.utils.data import Subset\nimport yaml\nimport os\nimport json\nimport torch.multiprocessing as mp\nimport argparse\nimport pickle\n\nfrom dataloader import VisualGenomeDataset, OpenImageV6Dataset\nfrom train_test import training\nfrom evaluate import eval_pc, eval_sgc, eval_sgd\n\n\nif __name__ == \"__main__\":\n    print('Torch', torch.__version__, 'Torchvision', torchvision.__version__)\n    # load hyperparameters\n    try:\n        with open('config.yaml', 'r') as file:\n            args = yaml.safe_load(file)\n    except Exception as e:\n        print('Error reading the config file')\n\n    # Command-line argument parsing\n    parser = argparse.ArgumentParser(description='Command line arguments')\n    parser.add_argument('--run_mode', type=str, default=None, help='Override run_mode (train, eval, prepare_cs, train_cs, eval_cs)')\n    parser.add_argument('--eval_mode', type=str, default=None, help='Override eval_mode (pc, sgc, sgd)')\n    parser.add_argument('--cluster', type=str, default=None, help='Override supcat_clustering (motif, gpt2, bert, clip)')\n    parser.add_argument('--hierar', dest='hierar', action='store_true', help='Set hierarchical_pred to True')\n    cmd_args = parser.parse_args()\n\n    # Override args from config.yaml with command-line arguments if provided\n    args['training']['run_mode'] = cmd_args.run_mode if cmd_args.run_mode is not None else args['training']['run_mode']\n    args['training']['eval_mode'] = cmd_args.eval_mode if cmd_args.eval_mode is not None else args['training']['eval_mode']\n    args['dataset']['supcat_clustering'] = cmd_args.cluster if cmd_args.cluster is not None else args['dataset']['supcat_clustering']\n    args['models']['hierarchical_pred'] = cmd_args.hierar if cmd_args.hierar is not None else args['models']['hierarchical_pred']\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    world_size = torch.cuda.device_count()\n    print('device', device)\n    print('torch.distributed.is_available', torch.distributed.is_available())\n    print('Using %d GPUs' % (torch.cuda.device_count()))\n    print(\"Running model\", args['models']['detr_or_faster_rcnn'])\n\n    # prepare datasets\n    if args['dataset']['dataset'] == 'vg':\n        args['models']['num_classes'] = 150\n        args['models']['num_relations'] = 50\n        args['models']['num_super_classes'] = 17\n\n        # we still use the name num_geometric, num_possessive, num_semantic for name consistency in this code repo\n        # but they are actually the number of clusters for each super category in the GPT-2, BERT, or CLIP clustering\n        if args['dataset']['supcat_clustering'] == 'gpt2':\n            args['models']['num_geometric'] = 9\n            args['models']['num_possessive'] = 32\n            args['models']['num_semantic'] = 9\n        elif args['dataset']['supcat_clustering'] == 'bert':\n            args['models']['num_geometric'] = 12\n            args['models']['num_possessive'] = 25\n            args['models']['num_semantic'] = 13\n        elif args['dataset']['supcat_clustering'] == 'clip':\n            args['models']['num_geometric'] = 27\n            args['models']['num_possessive'] = 15\n            args['models']['num_semantic'] = 8\n        else:   # if 'supcat_clustering' is 'motif', we follow the super-category definitions in the paper Neural Motifs\n            args['models']['num_geometric'] = 15\n            args['models']['num_possessive'] = 11\n            args['models']['num_semantic'] = 24\n\n        args['models']['detr101_pretrained'] = 'checkpoints/detr101_vg_ckpt.pth'\n\n        print(\"Loading the datasets...\")\n        train_dataset = VisualGenomeDataset(args, device, args['dataset']['annotation_train'], training=True)\n        test_dataset = VisualGenomeDataset(args, device, args['dataset']['annotation_test'], training=False)    # always evaluate on the original testing dataset\n\n    elif args['dataset']['dataset'] == 'oiv6':\n        args['models']['num_classes'] = 601\n        args['models']['num_relations'] = 30\n        args['models']['num_geometric'] = 4\n        args['models']['num_possessive'] = 2\n        args['models']['num_semantic'] = 24\n        args['models']['detr101_pretrained'] = 'checkpoints/detr101_oiv6_ckpt.pth'\n\n        print(\"Loading the datasets...\")\n        train_dataset = OpenImageV6Dataset(args, device, '../datasets/open_image_v6/annotations/oiv6-adjust/vrd-train-anno.json')\n        test_dataset = OpenImageV6Dataset(args, device, '../datasets/open_image_v6/annotations/oiv6-adjust/vrd-test-anno.json')\n    else:\n        print('Unknown dataset.')\n\n    torch.manual_seed(0)\n    train_subset_idx = torch.randperm(len(train_dataset))[:int(args['dataset']['percent_train'] * len(train_dataset))]\n    train_subset = Subset(train_dataset, train_subset_idx)\n    test_subset_idx = torch.randperm(len(test_dataset))[:int(args['dataset']['percent_test'] * len(test_dataset))]\n    test_subset = Subset(test_dataset, test_subset_idx)\n    print('num of train, test:', len(train_subset), len(test_subset))\n\n    print(args)\n    # select training or evaluation\n    if args['training']['run_mode'] == 'train' or args['training']['run_mode'] == 'train_cs':\n         mp.spawn(training, nprocs=world_size, args=(args, train_subset, test_subset))\n\n    elif args['training']['run_mode'] == 'prepare_cs':\n        \"\"\"\n        we have to collect commonsense-aligned and violated triplets only from the training dataset to prevent data leakage\n        the process is divided into two steps to avoid unexpected interrupts from OpenAI API connections\n        the first step requires model inference, but the second step only requires calling the __getitem__ function in dataloader\n        \"\"\"\n        # step 1: collect and save commonsense-aligned and violated triplets on the current baseline model for each image\n        mp.spawn(eval_pc, nprocs=world_size, args=(args, train_subset, train_dataset, 1))\n        # step 2: rerun it again but to accumulate all collected triplets from the two sets and save them into two .pt files\n        mp.spawn(eval_pc, nprocs=world_size, args=(args, train_subset, train_dataset, 2))\n\n    elif args['training']['run_mode'] == 'eval' or args['training']['run_mode'] == 'eval_cs':\n        # select evaluation mode\n        if args['training']['eval_mode'] == 'pc':    # predicate classification\n            mp.spawn(eval_pc, nprocs=world_size, args=(args, test_subset))\n        elif args['training']['eval_mode'] == 'sgc' and args['dataset']['dataset'] == 'vg':     # scene graph classification\n            mp.spawn(eval_sgc, nprocs=world_size, args=(args, test_subset))\n        elif args['training']['eval_mode'] == 'sgd' and args['dataset']['dataset'] == 'vg':     # scene graph detection\n            mp.spawn(eval_sgd, nprocs=world_size, args=(args, test_subset))\n        else:\n            print('Invalid arguments or not implemented.')\n    else:\n        print('Invalid arguments.')\n\n"}
{"type": "source_file", "path": "dataset_utils.py", "content": "import os\nimport numpy as np\nimport torch\nimport json\nfrom PIL import Image\nimport string\nimport tqdm\nimport torchvision\nfrom torchvision import transforms\nfrom collections import Counter\nfrom utils import *\nimport cv2\nfrom tqdm import tqdm\nimport json\n\n\nclass TwoCropTransform:\n    \"\"\"Create two crops of the same image\"\"\"\n    def __init__(self, transform1, transform2):\n        self.transform1 = transform1\n        self.transform2 = transform2\n\n    def __call__(self, x):\n        return self.transform1(x), self.transform2(x)\n\n\n# Dataset utils functions\ndef prepare_data_offline(args, data_loader, device, annot, image_transform, depth_estimator, start=0):\n    \"\"\"\n    This function organizes all information that VisualGenomeDataset __getitem__ function needs\n    to provide images, depth maps, ground-truth object categories and relationships.\n    An offline pre-process speeds avoids dealing with data preparations during the actual training process.\n    \"\"\"\n    with open(annot) as f:\n        annotations = json.load(f)\n        annotations['annotations'] = np.array(annotations['annotations'])\n        annotations['instances'] = np.array(annotations['instances'])\n\n    # processed_annotations = {}\n    curr_instance_start = 0\n    curr_relations_start = 0\n    for idx, _ in enumerate(tqdm(data_loader)):     # dataloader is only a placeholder who returns null every iter, need to gather all data right in this func\n        idx += start\n        '''\n        load instances\n        '''\n        # find all instances belonging to the current image\n        curr_instance = []\n        flag = False\n        gap = 0\n        for i in range(curr_instance_start, len(annotations['instances'])):\n            curr_instance_temp = annotations['instances'][i]['image_id']\n            if curr_instance_temp == annotations['images'][idx]['id']:\n                curr_instance.append(curr_instance_temp)\n                flag = True\n            elif flag:\n                break\n            else:\n                gap += 1\n        curr_instance = curr_instance_start + gap + np.nonzero(curr_instance)[0]  # indices of instances in curr image in annotations['instances']\n        if i != len(annotations['instances']) - 1:\n            curr_instance_start = i\n        num_instances = len(annotations['instances'][curr_instance])\n        # print('curr_instance', idx, curr_instance_start, curr_instance)\n\n        '''\n        load relationships\n        '''\n        # find all relationships belonging to the current image\n        curr_relations = []\n        flag = False\n        gap = 0\n        for i in range(curr_relations_start, len(annotations['annotations'])):\n            curr_relation_temp = annotations['annotations'][i]['image_id']\n            if curr_relation_temp == annotations['images'][idx]['id']:\n                curr_relations.append(curr_relation_temp)\n                flag = True\n            elif flag:\n                break\n            else:\n                gap += 1\n        curr_relations = curr_relations_start + gap + np.nonzero(curr_relations)[0]  # indices of instances in curr image in annotations['instances']\n        if i != len(annotations['annotations']) - 1:\n            curr_relations_start = i\n        num_relations = len(annotations['annotations'][curr_relations])\n        # print('curr_relations', idx, curr_relations_start, curr_relations)\n\n        if num_instances == 0 or num_relations == 0:\n            continue\n\n        '''\n        load image depth map\n        '''\n        image_path = os.path.join(args['dataset']['image_dir'], annotations['images'][idx]['file_name'])\n        image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n        h_img, w_img = image.shape[:2]\n        image = image_transform(image).view(1, 3, args['models']['image_size'], args['models']['image_size']).to(device)\n\n        depth_estimator = depth_estimator.cuda()\n        depth_estimator.eval()\n\n        with torch.no_grad():\n            image_depth = depth_estimator(image)  # size (1, 256, 256)\n\n        h_fea, w_fea = args['models']['feature_size'], args['models']['feature_size']\n\n        image_depth = torchvision.transforms.functional.resize(image_depth, args['models']['feature_size'])  # size (1, 64, 64)\n        image_depth = image_depth / (torch.max(image_depth) - torch.min(image_depth))\n        image_depth = image_depth.cpu()\n\n        '''\n        load bbox and categories of each instance in order\n        '''\n        areas = []\n        for inst in curr_instance:\n            areas.append(annotations['instances'][inst]['area'])\n        area_sorted, sorted_idices = torch.sort(torch.as_tensor(areas), descending=True)\n\n        masks = []\n        bbox = []\n        bbox_origin = []\n        categories = []\n        super_categories = []\n        for area_index in sorted_idices:\n            inst = curr_instance[area_index]\n\n            box = annotations['instances'][inst]['bbox']\n            bbox_origin.append([box[0], box[2], box[1], box[3]])\n            box = resize_boxes(box, (h_img, w_img), (h_fea, w_fea))\n            bbox.append([box[0], box[2], box[1], box[3]]) # x_min, x_max, y_min, y_max\n\n            mask = torch.zeros(h_fea, w_fea, dtype=torch.uint8)\n            mask[box[0]:box[2], box[1]:box[3]] = 1\n            masks.append(mask)\n\n            category = annotations['instances'][inst]['category_id']\n            categories.append(category)\n            super_category = annotations['instances'][inst]['super_category_id']\n            super_categories.append(torch.as_tensor(super_category, dtype=torch.int64))\n\n        categories = torch.as_tensor(categories, dtype=torch.int64)\n        bbox = torch.as_tensor(bbox, dtype=torch.float32)\n        bbox_origin = torch.as_tensor(bbox_origin, dtype=torch.float32)\n        masks = torch.stack(masks)\n\n        '''\n        load relationships in order\n        '''\n        curr_instance_id = [annotations['instances'][curr_instance[area_index]]['id'] for area_index in sorted_idices]  # object_id of instances in curr image\n\n        # prepare all relationships in order, iterate through all instances one by one, find its relationship to each of the previous instance\n        relation_id = [annotations['annotations'][curr_relations][i]['relation_id'] for i in range(num_relations)]\n        subject_id = [annotations['annotations'][curr_relations][i]['subject_id'] for i in range(num_relations)]\n        object_id = [annotations['annotations'][curr_relations][i]['object_id'] for i in range(num_relations)]\n\n        relationships = []\n        subj_or_obj = []\n\n        for i, inst_id in enumerate(curr_instance_id):\n            relationships_temp = []\n            subj_or_obj_temp = []  # 1 if curr is subject and prev is object, 0 if prev is subject and curr is object\n\n            for j, prev_inst_id in enumerate(curr_instance_id[:i]):\n                match1 = [(subject_id[k] == inst_id) and (object_id[k] == prev_inst_id) for k in range(num_relations)]\n                match_idx1 = np.nonzero(match1)[0]\n                match2 = [(object_id[k] == inst_id) and (subject_id[k] == prev_inst_id) for k in range(num_relations)]\n                match_idx2 = np.nonzero(match2)[0]\n\n                # there is only one relation per pair in the VG dataset\n                if len(match_idx1) > 0:  # if curr is subject\n                    relationships_temp.append(relation_id[match_idx1[0]])\n                    subj_or_obj_temp.append(1)\n\n                elif len(match_idx2) > 0:  # if curr is object\n                    relationships_temp.append(relation_id[match_idx2[0]])\n                    subj_or_obj_temp.append(0)\n\n                else:  # len(match_idx1) == 0 and len(match_idx2) == 0, no relationship\n                    relationships_temp.append(-1)\n                    subj_or_obj_temp.append(-1)\n\n            if len(relationships_temp) > 0:\n                relationships.append(torch.as_tensor(relationships_temp, dtype=torch.int64))\n                subj_or_obj.append(torch.as_tensor(subj_or_obj_temp, dtype=torch.float32))\n\n        data_annot = {\n            'image_depth': image_depth,\n            'curr_instance': curr_instance,\n            'num_relations': num_relations,\n            'categories': categories,\n            'super_categories': super_categories,\n            'masks': masks,\n            'bbox': bbox,\n            'bbox_origin': bbox_origin,\n            'relationships': relationships,\n            'subj_or_obj': subj_or_obj}\n\n        file_name_temp = annotations['images'][idx]['file_name'][:-4] + '_annotations.pkl'\n        file_name = os.path.join(args['dataset']['annot_dir'], file_name_temp)\n        torch.save(data_annot, file_name)\n\n\ndef prepare_depth_oiv6_offline(args, data_loader, device, depth_estimator):\n    saved_dir = \"../datasets/open_image_v6/image_depths/\"\n    all_labels = {i: 0 for i in range(30)}\n    rel_super_dict = oiv6_reorder_by_super()\n    for idx, data in enumerate(tqdm(data_loader)):\n        rel = data[0]\n        for r in rel:\n            all_labels[rel_super_dict[r.item()]] += 1\n\n        images, image_names = data[1], data[2]\n\n        depth_estimator = depth_estimator.to(device)\n        depth_estimator.eval()\n\n        with torch.no_grad():\n            image_depth = depth_estimator(images.to(device))  # size (1, 256, 256)\n        image_depth = image_depth.cpu()\n\n        for i in range(len(image_names)):\n            resized = torchvision.transforms.functional.resize(torch.unsqueeze(image_depth[i], 0), size=args['models']['feature_size'])  # size (1, 64, 64)\n            resized = resized / (torch.max(resized) - torch.min(resized))\n\n            saved_path = saved_dir + image_names[i] + '_depth.pt'\n            torch.save(resized, saved_path)\n\n    print([all_labels[key] for key in all_labels])\n\n\ndef find_zero_shot_triplet(train_annot, test_annot):\n    with open(train_annot) as f:\n        train_annotations = json.load(f)\n    with open(test_annot) as f:\n        test_annotations = json.load(f)\n\n    rel_reorder_dict = relation_class_freq2scat()\n\n    train_triplets = {}\n    test_triplets = {}\n    zero_shot_triplets = []\n\n    print(len(train_annotations['annotations']), len(test_annotations['annotations']))\n    # for idx, _ in enumerate(tqdm(train_loader)):\n    for idx in tqdm(range(len(train_annotations['annotations']))):\n        curr_annot = train_annotations['annotations'][idx]\n        curr_rel = curr_annot['relation_id']\n        if curr_rel == 12:\n            curr_rel = 4\n        curr_rel = rel_reorder_dict[curr_rel].item()\n        curr_triplet = str(curr_annot['category1']) + '_' + str(curr_rel) + '_' + str(curr_annot['category2'])\n        # if curr_triplet == '121_44_22':\n        #     print(\"train found\")\n        # if curr_triplet == '22_44_121':\n        #     print(\"train found2\")\n        if curr_triplet in train_triplets:\n            train_triplets[curr_triplet] += 1\n        else:\n            train_triplets[curr_triplet] = 1\n\n    # for idx, _ in enumerate(tqdm(test_loader)):\n    for idx in tqdm(range(len(test_annotations['annotations']))):\n        curr_annot = test_annotations['annotations'][idx]\n        curr_rel = curr_annot['relation_id']\n        if curr_rel == 12:\n            curr_rel = 4\n        curr_rel = rel_reorder_dict[curr_rel].item()\n        curr_triplet = str(curr_annot['category1']) + '_' + str(curr_rel) + '_' + str(curr_annot['category2'])\n        # if curr_triplet == '121_44_22':\n        #     print(\"test found\")\n        # if curr_triplet == '22_44_121':\n        #     print(\"test found2\")\n        if curr_triplet in test_triplets:\n            test_triplets[curr_triplet] += 1\n        else:\n            test_triplets[curr_triplet] = 1\n        # check if the test triplet has appeared in the training data or not\n        if (curr_triplet not in train_triplets) and (curr_triplet not in zero_shot_triplets):\n            zero_shot_triplets.append(curr_triplet)\n\n    # sanity check\n    for triplet in zero_shot_triplets:\n        assert triplet not in train_triplets\n        assert triplet in test_triplets\n    print('121_44_22' in train_triplets, '121_44_22' in test_triplets, '121_44_22' in zero_shot_triplets)\n\n    print(len(train_triplets), len(test_triplets), len(zero_shot_triplets))\n    torch.save(train_triplets, 'train_triplets.pt')\n    torch.save(test_triplets, 'test_triplets.pt')\n    torch.save(zero_shot_triplets, 'zero_shot_triplets.pt')\n\n\n# all functions below are built from the open-source code:\n# https://github.com/danfeiX/scene-graph-TF-release/blob/master/data_tools/vg_to_roidb.py\ndef merge_duplicate_boxes(raw_obj_data):\n    def IoU(b1, b2):\n        if b1[2] <= b2[0] or b1[3] <= b2[1] or b1[0] >= b2[2] or b1[1] >= b2[3]:\n            return 0\n        b1b2 = np.vstack([b1, b2])\n        minc = np.min(b1b2, 0)\n        maxc = np.max(b1b2, 0)\n        union_area = (maxc[2] - minc[0]) * (maxc[3] - minc[1])\n        int_area = (minc[2] - maxc[0]) * (minc[3] - maxc[1])\n        return float(int_area) / float(union_area)\n\n    def to_x1y1x2y2(obj):\n        x1 = obj['x']\n        y1 = obj['y']\n        x2 = obj['x'] + obj['w']\n        y2 = obj['y'] + obj['h']\n        return np.array([x1, y1, x2, y2], dtype=np.int32)\n\n    def inside(b1, b2):\n        return b1[0] >= b2[0] and b1[1] >= b2[1] \\\n               and b1[2] <= b2[2] and b1[3] <= b2[3]\n\n    def overlap(obj1, obj2):\n        b1 = to_x1y1x2y2(obj1)\n        b2 = to_x1y1x2y2(obj2)\n        iou = IoU(b1, b2)\n        if all(b1 == b2) or iou > 0.9:  # consider as the same box\n            return 1\n        elif (inside(b1, b2) or inside(b2, b1)) \\\n                and obj1['names'][0] == obj2['names'][0]:  # same object inside the other\n            return 2\n        elif iou > 0.6 and obj1['names'][0] == obj2['names'][0]:  # multiple overlapping same object\n            return 3\n        else:\n            return 0  # no overlap\n\n    num_merged = {1: 0, 2: 0, 3: 0}\n    print('merging boxes..')\n    for img in tqdm(raw_obj_data):\n        # mark objects to be merged and save their ids\n        objs = img['objects']\n        num_obj = len(objs)\n        for i in range(num_obj):\n            if 'M_TYPE' in objs[i]:  # has been merged\n                continue\n            merged_objs = []  # circular refs, but fine\n            for j in range(i + 1, num_obj):\n                if 'M_TYPE' in objs[j]:  # has been merged\n                    continue\n                overlap_type = overlap(objs[i], objs[j])\n                if overlap_type > 0:\n                    objs[j]['M_TYPE'] = overlap_type\n                    merged_objs.append(objs[j])\n            objs[i]['mobjs'] = merged_objs\n\n        # merge boxes\n        filtered_objs = []\n        merged_num_obj = 0\n        for obj in objs:\n            if 'M_TYPE' not in obj:\n                ids = [obj['object_id']]\n                dims = [to_x1y1x2y2(obj)]\n                prominent_type = 1\n                for mo in obj['mobjs']:\n                    ids.append(mo['object_id'])\n                    obj['names'].extend(mo['names'])\n                    dims.append(to_x1y1x2y2(mo))\n                    if mo['M_TYPE'] > prominent_type:\n                        prominent_type = mo['M_TYPE']\n                merged_num_obj += len(ids)\n                obj['ids'] = ids\n                mdims = np.zeros(4)\n                if prominent_type > 1:  # use extreme\n                    mdims[:2] = np.min(np.vstack(dims)[:, :2], 0)\n                    mdims[2:] = np.max(np.vstack(dims)[:, 2:], 0)\n                else:  # use mean\n                    mdims = np.mean(np.vstack(dims), 0)\n                obj['x'] = int(mdims[0])\n                obj['y'] = int(mdims[1])\n                obj['w'] = int(mdims[2] - mdims[0])\n                obj['h'] = int(mdims[3] - mdims[1])\n\n                num_merged[prominent_type] += len(obj['mobjs'])\n\n                obj['mobjs'] = None\n                obj['names'] = list(set(obj['names']))  # remove duplicates\n\n                filtered_objs.append(obj)\n            else:\n                assert 'mobjs' not in obj\n\n        img['objects'] = filtered_objs\n        assert (merged_num_obj == num_obj)\n\n    print('# merged boxes per merging type:')\n    print(num_merged)\n\n\ndef sentence_preprocess(phrase):\n    \"\"\" preprocess a sentence: lowercase, clean up weird chars, remove punctuation \"\"\"\n    replacements = {\n      '½': 'half',\n      '—' : '-',\n      '™': '',\n      '¢': 'cent',\n      'ç': 'c',\n      'û': 'u',\n      'é': 'e',\n      '°': ' degree',\n      'è': 'e',\n      '…': '',\n    }\n    phrase = phrase.strip()\n    for k, v in replacements.items():\n        phrase = phrase.replace(k, v)\n    return phrase.lower().translate(str.maketrans('', '', string.punctuation))\n\n\ndef preprocess_object_labels(data, alias_dict={}):\n    for img in data:\n        for obj in img['objects']:\n            obj['ids'] = [obj['object_id']]\n            names = []\n            for name in obj['names']:\n                label = sentence_preprocess(name)\n                if label in alias_dict:\n                    label = alias_dict[label]\n                names.append(label)\n            obj['names'] = names\n\n\ndef preprocess_predicates(data, alias_dict={}):\n    for img in data:\n        for relation in img['relationships']:\n            predicate = sentence_preprocess(relation['predicate'])\n            if predicate in alias_dict:\n                predicate = alias_dict[predicate]\n            relation['predicate'] = [predicate]\n\n            try:\n                sub_name = sentence_preprocess(relation['subject']['name'])\n            except:\n                sub_name = sentence_preprocess(relation['subject']['names'][0])\n            if sub_name in alias_dict:\n                sub_name = alias_dict[sub_name]\n            relation['subject']['names'] = [sub_name]\n\n            try:\n                obj_name = sentence_preprocess(relation['object']['name'])\n            except:\n                obj_name = sentence_preprocess(relation['object']['names'][0])\n            if obj_name in alias_dict:\n                obj_name = alias_dict[obj_name]\n            relation['object']['names'] = [obj_name]\n\n\ndef make_alias_dict(dict_file):\n    \"\"\"create an alias dictionary from a file\"\"\"\n    out_dict = {}\n    vocab = []\n    for line in open(dict_file, 'r'):\n        alias = line.strip('\\n').strip('\\r').split(',')\n        alias_target = alias[0] if alias[0] not in out_dict else out_dict[alias[0]]\n        for a in alias:\n            out_dict[a] = alias_target  # use the first term as the aliasing target\n        vocab.append(alias_target)\n    return out_dict, vocab\n\n\ndef make_list(list_file):\n    \"\"\"create a blacklist list from a file\"\"\"\n    return [line.strip('\\n').strip('\\r') for line in open(list_file)]\n\n\ndef obj_rel_cross_check(raw_obj_data, raw_relation_data):\n    \"\"\"\n    make sure all objects that are in relationship dataset\n    are in object dataset\n    \"\"\"\n    num_img = len(raw_obj_data)\n    num_correct = 0\n    total_rel = 0\n    for i in range(num_img):\n        assert(raw_obj_data[i]['image_id'] == raw_relation_data[i]['image_id'])\n        objs = raw_obj_data[i]['objects']\n        rels = raw_relation_data[i]['relationships']\n        ids = [obj['object_id'] for obj in objs]\n        for rel in rels:\n            if rel['subject']['object_id'] in ids and rel['object']['object_id'] in ids:\n                num_correct += 1\n            total_rel += 1\n    print('cross check: %i/%i relationship are correct' % (num_correct, total_rel))\n\n\ndef sync_objects(raw_obj_data, raw_relation_data):\n    num_img = len(raw_obj_data)\n    for i in range(num_img):\n        assert(raw_obj_data[i]['image_id'] == raw_relation_data[i]['image_id'])\n        objs = raw_obj_data[i]['objects']\n        rels = raw_relation_data[i]['relationships']\n\n        ids = [obj['object_id'] for obj in objs]\n        for rel in rels:\n            if rel['subject']['object_id'] not in ids:\n                rel_obj = rel['subject']\n                rel_obj['names'] = [rel_obj['name']]\n                objs.append(rel_obj)\n            if rel['object']['object_id'] not in ids:\n                rel_obj = rel['object']\n                rel_obj['names'] = [rel_obj['name']]\n                objs.append(rel_obj)\n\n        raw_obj_data[i]['objects'] = objs\n\n\ndef filter_object_boxes(raw_obj_data, images_id2area, area_frac_thresh=0.002):\n    \"\"\"\n    filter boxes by a box area-image area ratio threshold\n    \"\"\"\n    thresh_count = 0\n    all_count = 0\n    for i, img in enumerate(raw_obj_data):\n        filtered_obj = []\n        area = images_id2area[img['image_id']]\n        for obj in img['objects']:\n            if float(obj['h'] * obj['w']) > area * area_frac_thresh:\n                filtered_obj.append(obj)\n                thresh_count += 1\n            all_count += 1\n        img['objects'] = filtered_obj\n    print('box threshod: keeping %i/%i boxes' % (thresh_count, all_count))\n\n\ndef extract_object_token(raw_obj_data, num_tokens, obj_list=[], verbose=True):\n    \"\"\" Builds a set that contains the object names. Filters infrequent tokens. \"\"\"\n    token_counter = Counter()\n    for img in raw_obj_data:\n        for region in img['objects']:\n            for name in region['names']:\n                if not obj_list or name in obj_list:\n                    token_counter.update([name])\n    tokens = set()\n    # pick top N tokens\n    token_counter_return = {}\n    for token, count in token_counter.most_common():\n        tokens.add(token)\n        token_counter_return[token] = count\n        if len(tokens) == num_tokens:\n            break\n    if verbose:\n        print(('Keeping %d / %d objects'\n                  % (len(tokens), len(token_counter))))\n    return tokens, token_counter_return\n\n\ndef convert_all_gt_triplets_in_training():\n    data = torch.load('/tmp/datasets/vg_scene_graph_annot/test_triplets.pt')\n\n    # Get the relation and object mappings\n    relation_mapping = relation_by_super_class_int2str()\n    object_mapping = object_class_int2str()\n\n    # Create a new dictionary to store the converted data\n    converted_data = []\n\n    for key in tqdm(data.keys()):\n        # Parse the key\n        obj1, relation, obj2 = key.split('_')\n        obj1 = object_mapping[int(obj1)]\n        relation = relation_mapping[int(relation)]\n        obj2 = object_mapping[int(obj2)]\n\n        # Construct the description\n        description = f\"{obj1} {relation} {obj2}\"\n        converted_data.append(description)\n\n    # Save the converted data to a JSON file\n    with open('all_gt_triplets_in_training.json', 'w') as f:\n        json.dump(converted_data, f, indent=4)\n\n\ndef object_super_class():\n    return {'vehicle': 0, 'animal': 1, 'part': 2, 'person': 3, 'clothes': 4, 'food': 5, 'artifact': 6, 'location': 7, 'furniture': 8, 'flora': 9,\n            'building': 10, 'table': 11, 'structure': 12, 'door': 13, 'perosn': 14, 'laptop': 15, 'phone': 16}\n\n\ndef object_super_class_int2str():\n    return {0: 'vehicle', 1: 'animal', 2: 'part', 3: 'person', 4: 'clothes', 5: 'food', 6: 'artifact', 7: 'location', 8: 'furniture', 9: 'flora',\n            10: 'building', 11: 'table', 12: 'structure', 13: 'door', 14: 'perosn', 15: 'laptop', 16: 'phone'}\n\n\ndef object_class_int2str():\n    return {0: 'tree', 1: 'man', 2: 'window', 3: 'shirt', 4: 'building', 5: 'person', 6: 'sign', 7: 'leg', 8: 'head', 9: 'pole',\n            10: 'table', 11: 'woman', 12: 'hair', 13: 'hand', 14: 'car', 15: 'door', 16: 'leaf', 17: 'light', 18: 'pant', 19: 'fence',\n            20: 'ear', 21: 'shoe', 22: 'chair', 23: 'people', 24: 'plate', 25: 'arm', 26: 'glass', 27: 'jacket', 28: 'street', 29: 'sidewalk',\n            30: 'snow', 31: 'tail', 32: 'face', 33: 'wheel', 34: 'handle', 35: 'flower', 36: 'hat', 37: 'rock', 38: 'boy', 39: 'tile',\n            40: 'short', 41: 'bag', 42: 'roof', 43: 'letter', 44: 'girl', 45: 'umbrella', 46: 'helmet', 47: 'bottle', 48: 'branch', 49: 'tire',\n            50: 'plant', 51: 'train', 52: 'track', 53: 'nose', 54: 'boat', 55: 'post', 56: 'bench', 57: 'shelf', 58: 'wave', 59: 'box',\n            60: 'food', 61: 'pillow', 62: 'jean', 63: 'bus', 64: 'bowl', 65: 'eye', 66: 'trunk', 67: 'horse', 68: 'clock', 69: 'counter',\n            70: 'neck', 71: 'elephant', 72: 'giraffe', 73: 'mountain', 74: 'board', 75: 'house', 76: 'cabinet', 77: 'banana', 78: 'paper', 79: 'hill',\n            80: 'logo', 81: 'dog', 82: 'wing', 83: 'book', 84: 'bike', 85: 'coat', 86: 'seat', 87: 'truck', 88: 'glove', 89: 'zebra',\n            90: 'bird', 91: 'cup', 92: 'plane', 93: 'cap', 94: 'lamp', 95: 'motorcycle', 96: 'cow', 97: 'skateboard', 98: 'wire', 99: 'surfboard',\n            100: 'beach', 101: 'mouth', 102: 'sheep', 103: 'kite', 104: 'sink', 105: 'cat', 106: 'pizza', 107: 'bed', 108: 'animal', 109: 'ski',\n            110: 'curtain', 111: 'bear', 112: 'sock', 113: 'player', 114: 'flag', 115: 'finger', 116: 'windshield', 117: 'towel', 118: 'desk', 119: 'number',\n            120: 'railing', 121: 'lady', 122: 'stand', 123: 'vehicle', 124: 'child', 125: 'boot', 126: 'tower', 127: 'basket', 128: 'laptop', 129: 'engine',\n            130: 'vase', 131: 'toilet', 132: 'drawer', 133: 'racket', 134: 'tie', 135: 'pot', 136: 'paw', 137: 'airplane', 138: 'fork', 139: 'screen',\n            140: 'room', 141: 'guy', 142: 'orange', 143: 'phone', 144: 'fruit', 145: 'vegetable', 146: 'sneaker', 147: 'skier', 148: 'kid', 149: 'men'}\n\n\n# In our dataset, we arrange object labels by their frequency.\n# However, in the dataset used by the pretrained DETR-101 backbone, object labels are arranges by their alphabets\ndef object_class_alp2fre():\n    return {0: 137, 1: 108, 2: 25, 3: 41, 4: 77, 5: 127, 6: 100, 7: 111, 8: 107, 9: 56, 10: 84, 11: 90, 12: 74, 13: 54, 14: 83, 15: 125, 16: 47, 17: 64, 18: 59, 19: 38,\n            20: 48, 21: 4, 22: 63, 23: 76, 24: 93, 25: 14, 26: 105, 27: 22, 28: 124, 29: 68, 30: 85, 31: 69, 32: 96, 33: 91, 34: 110, 35: 118, 36: 81, 37: 15, 38: 132, 39: 20,\n            40: 71, 41: 129, 42: 65, 43: 32, 44: 19, 45: 115, 46: 114, 47: 35, 48: 60, 49: 138, 50: 144, 51: 72, 52: 44, 53: 26, 54: 88, 55: 141, 56: 12, 57: 13, 58: 34, 59: 36,\n            60: 8, 61: 46, 62: 79, 63: 67, 64: 75, 65: 27, 66: 62, 67: 148, 68: 103, 69: 121, 70: 94, 71: 128, 72: 16, 73: 7, 74: 43, 75: 17, 76: 80, 77: 1, 78: 149, 79: 95,\n            80: 73, 81: 101, 82: 70, 83: 53, 84: 119, 85: 142, 86: 18, 87: 78, 88: 136, 89: 23, 90: 5, 91: 143, 92: 61, 93: 106, 94: 92, 95: 50, 96: 24, 97: 113, 98: 9, 99: 55,\n            100: 135, 101: 133, 102: 120, 103: 37, 104: 42, 105: 140, 106: 139, 107: 86, 108: 102, 109: 57, 110: 3, 111: 21, 112: 40, 113: 29, 114: 6, 115: 104, 116: 97, 117: 109,\n            118: 147, 119: 146, 120: 30, 121: 112, 122: 122, 123: 28, 124: 99, 125: 10, 126: 31, 127: 134, 128: 39, 129: 49, 130: 131, 131: 117, 132: 126, 133: 52, 134: 51, 135: 0,\n            136: 87, 137: 66, 138: 45, 139: 130, 140: 145, 141: 123, 142: 58, 143: 33, 144: 2, 145: 116, 146: 82, 147: 98, 148: 11, 149: 89, 150: 150}\n\n\n# In our dataset, we arrange object labels by their frequency.\n# However, in the dataset used by the pretrained Faster-RCNN backbone, object labels are arranges by their alphabets\n# Everything is the same in DETR-101, except for the background label\ndef object_class_faster2fre():\n    return {1: 137, 2: 108, 3: 25, 4: 41, 5: 77, 6: 127, 7: 100, 8: 111, 9: 107, 10: 56, 11: 84, 12: 90, 13: 74, 14: 54, 15: 83, 16: 125, 17: 47, 18: 64, 19: 59, 20: 38,\n            21: 48, 22: 4, 23: 63, 24: 76, 25: 93, 26: 14, 27: 105, 28: 22, 29: 124, 30: 68, 31: 85, 32: 69, 33: 96, 34: 91, 35: 110, 36: 118, 37: 81, 38: 15, 39: 132, 40: 20,\n            41: 71, 42: 129, 43: 65, 44: 32, 45: 19, 46: 115, 47: 114, 48: 35, 49: 60, 50: 138, 51: 144, 52: 72, 53: 44, 54: 26, 55: 88, 56: 141, 57: 12, 58: 13, 59: 34, 60: 36,\n            61: 8, 62: 46, 63: 79, 64: 67, 65: 75, 66: 27, 67: 62, 68: 148, 69: 103, 70: 121, 71: 94, 72: 128, 73: 16, 74: 7, 75: 43, 76: 17, 77: 80, 78: 1, 79: 149, 80: 95,\n            81: 73, 82: 101, 83: 70, 84: 53, 85: 119, 86: 142, 87: 18, 88: 78, 89: 136, 90: 23, 91: 5, 92: 143, 93: 61, 94: 106, 95: 92, 96: 50, 97: 24, 98: 113, 99: 9, 100: 55,\n            101: 135, 102: 133, 103: 120, 104: 37, 105: 42, 106: 140, 107: 139, 108: 86, 109: 102, 110: 57, 111: 3, 112: 21, 113: 40, 114: 29, 115: 6, 116: 104, 117: 97, 118: 109,\n            119: 147, 120: 146, 121: 30, 122: 112, 123: 122, 124: 28, 125: 99, 126: 10, 127: 31, 128: 134, 129: 39, 130: 49, 131: 131, 132: 117, 133: 126, 134: 52, 135: 51, 136: 0,\n            137: 87, 138: 66, 139: 45, 140: 130, 141: 145, 142: 123, 143: 58, 144: 33, 145: 2, 146: 116, 147: 82, 148: 98, 149: 11, 150: 89, 0: 150}\n\n\ndef relation_class_by_freq():\n    return {0: 'on', 1: 'has', 2: 'in', 3: 'of', 4: 'wearing', 5: 'near', 6: 'with', 7: 'above', 8: 'holding', 9: 'behind',\n            10: 'under', 11: 'sitting on', 12: 'wears', 13: 'standing on', 14: 'in front of', 15: 'attached to', 16: 'at', 17: 'hanging from', 18: 'over', 19: 'for',\n            20: 'riding', 21: 'carrying', 22: 'eating', 23: 'walking on', 24: 'playing', 25: 'covering', 26: 'laying on', 27: 'along', 28: 'watching', 29: 'and',\n            30: 'between', 31: 'belonging to', 32: 'painted on', 33: 'against', 34: 'looking at', 35: 'from', 36: 'parked on', 37: 'to', 38: 'made of', 39: 'covered in',\n            40: 'mounted on', 41: 'says', 42: 'part of', 43: 'across', 44: 'flying in', 45: 'using', 46: 'on back of', 47: 'lying on', 48: 'growing on', 49: 'walking in'}\n\n\ndef relation_by_super_class_int2str():\n    return {0: 'above', 1: 'across', 2: 'against', 3: 'along', 4: 'and', 5: 'at', 6: 'behind', 7: 'between', 8: 'in', 9: 'in front of',\n            10: 'near', 11: 'on', 12: 'on back of', 13: 'over', 14: 'under', 15: 'belonging to', 16: 'for', 17: 'from', 18: 'has', 19: 'made of',\n            20: 'of', 21: 'part of', 22: 'to', 23: 'wearing', 24: 'wears', 25: 'with', 26: 'attached to', 27: 'carrying', 28: 'covered in', 29: 'covering',\n            30: 'eating', 31: 'flying in', 32: 'growing on', 33: 'hanging from', 34: 'holding', 35: 'laying on', 36: 'looking at', 37: 'lying on', 38: 'mounted on', 39: 'painted on',\n            40: 'parked on', 41: 'playing', 42: 'riding', 43: 'says', 44: 'sitting on', 45: 'standing on', 46: 'using', 47: 'walking in', 48: 'walking on', 49: 'watching'}\n\n\ndef relation_class_freq2scat():\n    return torch.tensor([11, 18, 8, 20, 23, 10, 25, 0, 34, 6, 14, 44, 24, 45, 9, 26, 5, 33, 13, 16,\n                         42, 27, 30, 48, 41, 29, 35, 3, 49, 4, 7, 15, 39, 2, 36, 17, 40, 22, 19, 28,\n                         38, 43, 21, 1, 31, 46, 12, 37, 32, 47, -1])\n\n\ndef preprocess_super_class(synset2cid, super_class_dict):\n    super_class_list = object_super_class()\n    sub2super_dict = {}\n    for line in open(super_class_dict, 'r'):\n        line = line.strip('\\n').strip('_').split(',')\n        sub_class = synset2cid[line[0]]\n        super_class = []\n        for item in line[1:]:\n            super_class.append(super_class_list[item])\n        sub2super_dict[sub_class] = super_class\n    return sub2super_dict\n\n\ndef find_top_caregories_relations_gqa():\n    all_categories = []\n    # all_relations = []\n\n    with open('../datasets/gqa/val_sceneGraphs.json') as f:\n        test_data = json.load(f)\n        for curr_image_idx in tqdm(test_data):\n            curr_image = test_data[curr_image_idx]['objects']\n            categories = [curr_image[curr_object]['name'] for curr_object in curr_image]\n            all_categories = all_categories + categories\n\n            # for curr_object in curr_image:\n            #     for curr_relation in curr_image[curr_object]['relations']:\n            #         all_relations.append(curr_relation['name'])\n\n    # with open('../datasets/gqa/train_sceneGraphs.json') as f:\n    #     train_data = json.load(f)\n    #     for curr_image_idx in tqdm(train_data):\n    #         curr_image = train_data[curr_image_idx]['objects']\n    #         categories = [curr_image[curr_object]['name'] for curr_object in curr_image]\n    #         all_categories = all_categories + categories\n    #\n    #         # for curr_object in curr_image:\n    #         #     for curr_relation in curr_image[curr_object]['relations']:\n    #         #         all_relations.append(curr_relation['name'])\n\n    print(len(all_categories), len(set(all_categories)))\n    all_categories_count = Counter(all_categories)\n    top_categories = all_categories_count.most_common(1704)\n    print('top_categories', len(top_categories))\n    # top_categories = [top_categories[i][0] for i in range(len(top_categories))]\n\n    # objects_idx2name_gqa = {i: top_categories[i] for i in range(len(top_categories))}\n    # torch.save(objects_idx2name_gqa, 'objects_idx2name_gqa.pt')\n    # objects_name2idx_gqa = {top_categories[i]: i for i in range(len(top_categories))}\n    # torch.save(objects_name2idx_gqa, 'objects_name2idx_gqa.pt')\n\n    # print(len(all_relations), len(set(all_relations)))\n    # all_relations_count = Counter(all_relations)\n    # top_relations = all_relations_count.most_common(52)\n    # top_relations = [top_relations[i][0] for i in range(len(top_relations))]\n\ndef object_name2label_gqa():\n    return {'window': 0, 'man': 1, 'shirt': 2, 'tree': 3, 'wall': 4, 'person': 5, 'sky': 6, 'building': 7, 'ground': 8, 'sign': 9,\n            'head': 10, 'pole': 11, 'hand': 12, 'grass': 13, 'hair': 14, 'leg': 15, 'car': 16, 'woman': 17, 'trees': 18, 'table': 19,\n            'leaves': 20, 'ear': 21, 'eye': 22, 'people': 23, 'pants': 24, 'water': 25, 'door': 26, 'fence': 27, 'nose': 28, 'wheel': 29,\n            'arm': 30, 'shoe': 31, 'clouds': 32, 'hat': 33, 'floor': 34, 'jacket': 35, 'chair': 36, 'leaf': 37, 'tail': 38, 'plate': 39,\n            'letter': 40, 'flower': 41, 'face': 42, 'road': 43, 'number': 44, 'windows': 45, 'cloud': 46, 'shorts': 47, 'sidewalk': 48, 'snow': 49,\n            'bag': 50, 'rock': 51, 'glass': 52, 'roof': 53, 'umbrella': 54, 'tire': 55, 'helmet': 56, 'boy': 57, 'logo': 58, 'jeans': 59,\n            'foot': 60, 'street': 61, 'cap': 62, 'boat': 63, 'bush': 64, 'mouth': 65, 'post': 66, 'girl': 67, 'flowers': 68, 'picture': 69,\n            'legs': 70, 'shoes': 71, 'bottle': 72, 'bus': 73, 'bench': 74, 'field': 75, 'pillow': 76, 'glasses': 77, 'mirror': 78, 'clock': 79,\n            'neck': 80, 'bowl': 81, 'dirt': 82, 'kite': 83, 'box': 84, 'train': 85, 'letters': 86, 'airplane': 87, 'bird': 88, 'food': 89,\n            'house': 90, 'lamp': 91, 'trunk': 92, 'cup': 93, 'coat': 94, 'horse': 95, 'street light': 96, 'shelf': 97, 'wing': 98, 'sheep': 99,\n            'paper': 100, 'book': 101, 'plant': 102, 'elephant': 103, 'branch': 104, 'dog': 105, 'giraffe': 106, 'counter': 107, 'motorcycle': 108, 'seat': 109,\n            'glove': 110, 'zebra': 111, 'skateboard': 112, 'banana': 113, 'eyes': 114, 'racket': 115, 'frame': 116, 'ceiling': 117, 'rocks': 118, 'surfboard': 119,\n            'truck': 120, 'bike': 121, 'wheels': 122, 'cabinet': 123, 'sink': 124, 'sand': 125, 'cow': 126, 'flag': 127, 'traffic light': 128, 'ball': 129,\n            'hands': 130, 'bushes': 131, 'feet': 132, 'child': 133, 'cat': 134, 'windshield': 135, 'bed': 136, 'finger': 137, 'stone': 138, 'hill': 139,\n            'word': 140, 'backpack': 141, 'basket': 142, 'player': 143, 'tie': 144, 'container': 145, 'paw': 146, 'vase': 147,  'buildings': 148, 'sock': 149}\n\ndef object_label2super_gqa():\n    return {0: [5], 1: [0], 2: [14], 3: [2], 4: [5], 5: [0], 6: [6], 7: [5], 8: [5, 15], 9: [13],\n            10: [0, 3, 11], 11: [13], 12: [0, 3, 11], 13: [6], 14: [0, 11], 15: [0, 3, 11], 16: [4], 17: [0], 18: [2], 19: [12],\n            20: [2, 11], 21: [0, 3, 11],  22: [0, 3, 11], 23: [0], 24: [14], 25: [6], 26: [5, 11], 27: [13], 28: [0, 3, 11], 29: [4, 11],\n            30: [0, 3, 11], 31: [14], 32: [6], 33: [14], 34: [5], 35: [14], 36: [12], 37: [2, 11, 15], 38: [3, 11], 39: [9, 13],\n            40: [13], 41: [15], 42: [0, 3, 11], 43: [6], 44: [13], 45: [5, 11], 46: [6], 47: [14], 48: [6], 49: [6],\n            50: [13], 51: [7], 52: [5, 13], 53: [5, 11], 54: [13], 55: [4, 11], 56: [14], 57: [0], 58: [13], 59: [14],\n            60: [0, 3, 11], 61: [6], 62: [14], 63: [4], 64: [14], 65: [0, 3, 11], 66: [13], 67: [0], 68: [15], 69: [13],\n            70: [0, 3, 11], 71: [14], 72: [13], 73: [4], 74: [12], 75: [6], 76: [12], 77: [14], 78: [12], 79: [12, 13],\n            80: [0, 3, 11], 81: [10, 13], 82: [7], 83: [13], 84: [13], 85: [4], 86: [13], 87: [4], 88: [3], 89: [1],\n            90: [5], 91: [12, 13], 92: [4], 93: [9, 10, 13], 94: [14], 95: [3, 4], 96: [13], 97: [12], 98: [3, 11], 99: [3],\n            100: [13], 101: [13], 102: [2], 103: [1, 7], 104: [2, 11], 105: [3], 106: [3], 107: [12], 108: [4], 109: [12],\n            110: [13], 111: [3], 112: [13], 113: [1, 8], 114: [0, 3, 11], 115: [13], 116: [12, 13], 117: [5], 118: [7], 119: [4, 13],\n            120: [4], 121: [4], 122: [4, 11], 123: [12], 124: [13], 125: [7], 126: [3], 127: [13], 128: [13], 129: [13],\n            130: [0, 3, 11], 131: [14], 132: [0, 3, 11], 133: [0], 134: [3], 135: [4, 11], 136: [12], 137: [0, 3, 11], 138: [7], 139: [6],\n            140: [13], 141: [9, 13], 142: [9, 13], 143: [0], 144: [14], 145: [9], 146: [3, 11], 147: [9, 13], 148: [5], 149: [14]}\n\ndef relation_name2label_gqa():\n    return {'to the left of': 0, 'to the right of': 1, 'on': 2, 'near': 3, 'in': 4, 'behind': 5, 'in front of': 6, 'holding': 7, 'on top of': 8, 'above': 9, 'next to': 10, 'below': 11,\n            'under': 12, 'on the side of': 13, 'beside': 14, 'inside': 15, 'at': 16, 'around': 17, 'on the front of': 18, 'on the back of': 19, 'wearing': 20, 'of': 21,\n            'with': 22, 'by': 23, 'contain': 24, 'filled with': 25, 'full of': 26, 'sitting on': 27, 'standing on': 28, 'carrying': 29, 'walking on': 30, 'riding': 31,\n            'standing in': 32, 'hanging on': 33, 'looking at': 34, 'covered by': 35, 'lying on': 36, 'watching': 37, 'eating': 38, 'covering': 39, 'hanging from': 40, 'riding on': 41,\n            'sitting in': 42, 'using': 43, 'parked on': 44, 'covered in': 45, 'walking in': 46, 'flying in': 47, 'crossing': 48, 'swinging': 49}\n\ndef oiv6_name2idx():\n    return {\"at\": 0, \"holds\": 1, \"wears\": 2, \"surf\": 3, \"hang\": 4, \"drink\": 5, \"holding_hands\": 6, \"on\": 7, \"ride\": 8, \"dance\": 9,\n            \"skateboard\": 10, \"catch\": 11, \"highfive\": 12, \"inside_of\": 13, \"eat\": 14, \"cut\": 15, \"contain\": 16, \"handshake\": 17, \"kiss\": 18, \"talk_on_phone\": 19,\n            \"interacts_with\": 20, \"under\": 21, \"hug\": 22, \"throw\": 23, \"hits\": 24, \"snowboard\": 25, \"kick\": 26, \"ski\": 27, \"plays\": 28, \"read\": 29}\n\ndef oiv6_reorder_by_super():\n    return {0:0, 1:6, 2:5, 3:7, 4:8, 5:9, 6:10, 7:1, 8: 11, 9:12,\n            10:13, 11:14, 12:15, 13:2, 14:16, 15:17, 16:4, 17:18, 18:19, 19:20,\n            20:21, 21:3, 22:22, 23:23, 24:24, 25:25, 26:26, 27:27, 28:28, 29:29}\n\n# Here are what we obtained from the token_embeddings.py\ndef gpt2_cluster_map():\n    return {0: ['standing on', 'walking on', 'painted on', 'looking at', 'mounted on', 'flying in', 'lying on', 'growing on', 'walking in'],\n            1: ['on', 'has', 'in', 'of', 'near', 'with', 'above', 'holding', 'behind', 'under', 'wears', 'in front of', 'attached to', 'at', 'over', 'for', 'riding', 'eating', 'playing', 'covering', 'along', 'watching', 'and', 'between', 'belonging to', 'against', 'from', 'to', 'says', 'across', 'using', 'on back of'],\n            2: ['wearing', 'sitting on', 'hanging from', 'carrying', 'laying on', 'parked on', 'made of', 'covered in', 'part of']} # [9, 32, 9]\ndef gpt2_index_map():\n    return torch.tensor([ 9, 10, 11, 12, 41, 13, 14, 15, 16, 17, 18, 42, 19,  0, 20, 21, 22, 43,\n                         23, 24, 25, 44, 26,  1, 27, 28, 45, 29, 30, 31, 32, 33,  2, 34,  3, 35,\n                         46, 36, 47, 48,  4, 37, 49, 38,  5, 39, 40,  6,  7,  8])\n# {0: 'standing on', 1: 'walking on', 2: 'painted on', 3: 'looking at', 4: 'mounted on', 5: 'flying in', 6: 'lying on', 7: 'growing on', 8: 'walking in', 9: 'on', 10: 'has', 11: 'in', 12: 'of', 13: 'near', 14: 'with', 15: 'above', 16: 'holding', 17: 'behind', 18: 'under', 19: 'wears', 20: 'in front of', 21: 'attached to', 22: 'at', 23: 'over', 24: 'for', 25: 'riding', 26: 'eating', 27: 'playing', 28: 'covering', 29: 'along', 30: 'watching', 31: 'and', 32: 'between', 33: 'belonging to', 34: 'against', 35: 'from', 36: 'to', 37: 'says', 38: 'across', 39: 'using', 40: 'on back of', 41: 'wearing', 42: 'sitting on', 43: 'hanging from', 44: 'carrying', 45: 'laying on', 46: 'parked on', 47: 'made of', 48: 'covered in', 49: 'part of'}\n\ndef bert_cluster_map():\n    return {0: ['behind', 'in front of', 'attached to', 'hanging from', 'painted on', 'parked on', 'made of', 'covered in', 'mounted on', 'part of', 'across', 'on back of'],\n            1: ['on', 'has', 'in', 'of', 'wearing', 'near', 'with', 'above', 'under', 'wears', 'at', 'over', 'for', 'riding', 'carrying', 'playing', 'covering', 'along', 'and', 'between', 'against', 'from', 'to', 'says', 'using'],\n            2: ['holding', 'sitting on', 'standing on', 'eating', 'walking on', 'laying on', 'watching', 'belonging to', 'looking at', 'flying in', 'lying on', 'growing on', 'walking in']}  # [12, 25, 13]\ndef bert_index_map():\n    return torch.tensor([12, 13, 14, 15, 16, 17, 18, 19, 37,  0, 20, 38, 21, 39,  1,  2, 22,  3,\n                         23, 24, 25, 26, 40, 41, 27, 28, 42, 29, 43, 30, 31, 44,  4, 32, 45, 33,\n                          5, 34,  6,  7,  8, 35,  9, 10, 46, 36, 11, 47, 48, 49])\n    # {0: 'behind', 1: 'in front of', 2: 'attached to', 3: 'hanging from', 4: 'painted on', 5: 'parked on', 6: 'made of', 7: 'covered in', 8: 'mounted on', 9: 'part of', 10: 'across', 11: 'on back of', 12: 'on', 13: 'has', 14: 'in', 15: 'of', 16: 'wearing', 17: 'near', 18: 'with', 19: 'above', 20: 'under', 21: 'wears', 22: 'at', 23: 'over', 24: 'for', 25: 'riding', 26: 'carrying', 27: 'playing', 28: 'covering', 29: 'along', 30: 'and', 31: 'between', 32: 'against', 33: 'from', 34: 'to', 35: 'says', 36: 'using', 37: 'holding', 38: 'sitting on', 39: 'standing on', 40: 'eating', 41: 'walking on', 42: 'laying on', 43: 'watching', 44: 'belonging to', 45: 'looking at', 46: 'flying in', 47: 'lying on', 48: 'growing on', 49: 'walking in'}\n\ndef clip_cluster_map():\n    return {0: ['wearing', 'near', 'with', 'above', 'holding', 'behind', 'under', 'wears', 'over', 'riding', 'carrying', 'eating', 'walking on', 'playing', 'covering', 'along', 'watching', 'between', 'against', 'from', 'made of', 'says', 'part of', 'across', 'using', 'growing on', 'walking in'],\n            1: ['sitting on', 'standing on', 'in front of', 'attached to', 'hanging from', 'laying on', 'belonging to', 'painted on', 'looking at', 'parked on', 'covered in', 'mounted on', 'flying in', 'on back of', 'lying on'],\n            2: ['on', 'has', 'in', 'of', 'at', 'for', 'and', 'to']}  # [27, 15, 8]\ndef clip_index_map():\n    return torch.tensor([42, 43, 44, 45,  0,  1,  2,  3,  4,  5,  6, 27,  7, 28, 29, 30, 46, 31,\n                          8, 47,  9, 10, 11, 12, 13, 14, 32, 15, 16, 48, 17, 33, 34, 18, 35, 19,\n                         36, 49, 20, 37, 38, 21, 22, 23, 39, 24, 40, 41, 25, 26])\n    # {0: 'wearing', 1: 'near', 2: 'with', 3: 'above', 4: 'holding', 5: 'behind', 6: 'under', 7: 'wears', 8: 'over', 9: 'riding', 10: 'carrying', 11: 'eating', 12: 'walking on', 13: 'playing', 14: 'covering', 15: 'along', 16: 'watching', 17: 'between', 18: 'against', 19: 'from', 20: 'made of', 21: 'says', 22: 'part of', 23: 'across', 24: 'using', 25: 'growing on', 26: 'walking in', 27: 'sitting on', 28: 'standing on', 29: 'in front of', 30: 'attached to', 31: 'hanging from', 32: 'laying on', 33: 'belonging to', 34: 'painted on', 35: 'looking at', 36: 'parked on', 37: 'covered in', 38: 'mounted on', 39: 'flying in', 40: 'on back of', 41: 'lying on', 42: 'on', 43: 'has', 44: 'in', 45: 'of', 46: 'at', 47: 'for', 48: 'and', 49: 'to'}\n\ndef clip_cluster_map_3dssg():\n    return {0: ['lying on', 'hanging on', 'leaning against', 'lying in', 'hanging in'],\n            1: ['supported by', 'behind', 'close by', 'bigger than', 'smaller than', 'higher than', 'lower than', 'same symmetry as', 'attached to', 'standing on', 'connected to', 'part of', 'belonging to', 'build in', 'standing in'],\n            2: ['left', 'right', 'front', 'inside', 'same as', 'cover']}  # [5, 15, 6]\ndef clip_index_map_3dssg():\n    return torch.tensor([0, 5, 20, 21, 22,  6,  7, 23,  8,  9, 10, 11, 12, 24, 13, 14,  0,  1,\n                         15,  2, 16, 17, 18, 19, 25,  3,  4])\n"}
{"type": "source_file", "path": "evaluate.py", "content": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nimport yaml\nimport os\nimport math\nimport torchvision\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport datetime\n\nfrom evaluator import *\nfrom model import *\nfrom utils import *\nfrom train_utils import *\nfrom dataset_utils import object_class_alp2fre\n\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12356'\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n\n\ndef eval_pc(gpu, args, test_subset, curr_dataset=None, prepare_cs_step=-1):\n    \"\"\"\n    This function evaluates the module on predicate classification tasks.\n    :param gpu: current gpu index\n    :param args: input arguments in config.yaml\n    :param test_subset: testing dataset\n    \"\"\"\n    rank = gpu\n    world_size = torch.cuda.device_count()\n    setup(rank, world_size)\n    print('rank', rank, 'torch.distributed.is_initialized', torch.distributed.is_initialized())\n\n    if args['training']['run_mode'] == 'prepare_cs':\n        curr_dataset.train_cs_step = prepare_cs_step\n\n    test_sampler = torch.utils.data.distributed.DistributedSampler(test_subset, num_replicas=world_size, rank=rank)\n    test_loader = torch.utils.data.DataLoader(test_subset, batch_size=args['training']['batch_size'], shuffle=False, collate_fn=collate_fn, num_workers=0, drop_last=True, sampler=test_sampler)\n    print(\"Finished loading the datasets...\")\n\n    start = []\n    test_record = []\n    with open(args['training']['result_path'] + 'test_results_' + str(rank) + '.json', 'w') as f:  # clear history logs\n        json.dump(start, f)\n\n    if args['models']['hierarchical_pred']:\n        relation_classifier = DDP(BayesianRelationClassifier(args=args, input_dim=args['models']['hidden_dim'], feature_size=args['models']['feature_size'],\n                                                             num_classes=args['models']['num_classes'], num_super_classes=args['models']['num_super_classes'],\n                                                             num_geometric=args['models']['num_geometric'], num_possessive=args['models']['num_possessive'],\n                                                             num_semantic=args['models']['num_semantic'])).to(rank)\n    else:\n        relation_classifier = DDP(FlatRelationClassifier(args=args, input_dim=args['models']['hidden_dim'], output_dim=args['models']['num_relations'],\n                                                         feature_size=args['models']['feature_size'], num_classes=args['models']['num_classes'])).to(rank)\n\n    detr = DDP(build_detr101(args)).to(rank)\n    relation_classifier.eval()\n\n    map_location = {'cuda:%d' % rank: 'cuda:%d' % 0}\n    if args['models']['hierarchical_pred']:\n        load_model_name = 'HierRelationModel_CS' if args['training']['run_mode'] == 'eval_cs' else 'HierRelationModel_Baseline'\n        load_model_name += '_' + args['dataset']['supcat_clustering'] + '_'\n        load_model_name = args['training']['checkpoint_path'] + load_model_name + str(args['training']['test_epoch']) + '_0' + '.pth'\n    else:\n        load_model_name = 'FlatRelationModel_CS' if args['training']['run_mode'] == 'eval_cs' else 'FlatRelationModel_Baseline'\n        load_model_name += '_' + args['dataset']['supcat_clustering'] + '_'\n        load_model_name = args['training']['checkpoint_path'] + load_model_name + str(args['training']['test_epoch']) + '_0' + '.pth'\n    if rank == 0:\n        print('Loading pretrained model from %s...' % load_model_name)\n    relation_classifier.load_state_dict(torch.load(load_model_name, map_location=map_location))\n\n    recall, mean_recall_top3, mean_recall, recall_zs, mean_recall_zs, wmap_rel, wmap_phrase = None, None, None, None, None, None, None\n    Recall = Evaluator(args=args, num_classes=args['models']['num_relations'], iou_thresh=0.5, top_k=[20, 50, 100])\n    Recall_top3 = None\n    if args['dataset']['dataset'] == 'vg':\n        Recall_top3 = Evaluator_Top3(args=args, num_classes=args['models']['num_relations'], iou_thresh=0.5, top_k=[20, 50, 100])\n    connectivity_recall, connectivity_precision, num_connected, num_not_connected, num_connected_pred = 0.0, 0.0, 0.0, 0.0, 0.0\n\n    print('Start Testing PC...')\n    with torch.no_grad():\n        for batch_count, data in enumerate(tqdm(test_loader), 0):\n            \"\"\"\n            PREPARE INPUT DATA\n            \"\"\"\n            try:\n                if args['training']['save_vis_results'] and args['training']['eval_mode'] == 'pc':\n                    images, images_raw, image_depth, categories, super_categories, bbox, relationships, subj_or_obj, annot_path, heights, widths, triplets, bbox_raw = data\n                else:\n                    images, _, image_depth, categories, super_categories, bbox, relationships, subj_or_obj, annot_path = data\n            except:\n                continue\n\n            if prepare_cs_step != 2:\n                # we need to run model inference unless we are in the running mode 'prepare_cs' and at step 2 when we have finished accumulating all triplets and need to save them\n                Recall.load_annotation_paths(annot_path)\n\n                image_feature = process_image_features(args, images, detr, rank)\n\n                categories = [category.to(rank) for category in categories]  # [batch_size][curr_num_obj, 1]\n                if super_categories[0] is not None:\n                    super_categories = [[sc.to(rank) for sc in super_category] for super_category in super_categories]  # [batch_size][curr_num_obj, [1 or more]]\n                image_depth = torch.stack([depth.to(rank) for depth in image_depth])\n                bbox = [box.to(rank) for box in bbox]  # [batch_size][curr_num_obj, 4]\n\n                masks = []\n                for i in range(len(bbox)):\n                    mask = torch.zeros(bbox[i].shape[0], args['models']['feature_size'], args['models']['feature_size'], dtype=torch.uint8).to(rank)\n                    for j, box in enumerate(bbox[i]):\n                        mask[j, int(bbox[i][j][2]):int(bbox[i][j][3]), int(bbox[i][j][0]):int(bbox[i][j][1])] = 1\n                    masks.append(mask)\n\n                \"\"\"\n                PREPARE TARGETS\n                \"\"\"\n                relations_target = []\n                direction_target = []\n                num_graph_iter = torch.as_tensor([len(mask) for mask in masks]) - 1\n                for graph_iter in range(max(num_graph_iter)):\n                    keep_in_batch = torch.nonzero(num_graph_iter > graph_iter).view(-1)\n                    relations_target.append(torch.vstack([relationships[i][graph_iter] for i in keep_in_batch]).T.to(rank))  # integer labels\n                    direction_target.append(torch.vstack([subj_or_obj[i][graph_iter] for i in keep_in_batch]).T.to(rank))\n\n                \"\"\"\n                FORWARD PASS THROUGH THE LOCAL PREDICTOR\n                \"\"\"\n                num_graph_iter = torch.as_tensor([len(mask) for mask in masks])\n                for graph_iter in range(max(num_graph_iter)):\n                    keep_in_batch = torch.nonzero(num_graph_iter > graph_iter).view(-1).to(rank)\n\n                    curr_graph_masks = torch.stack([torch.unsqueeze(masks[i][graph_iter], dim=0) for i in keep_in_batch])\n                    h_graph = torch.cat((image_feature[keep_in_batch] * curr_graph_masks, image_depth[keep_in_batch] * curr_graph_masks), dim=1)  # (bs, 256, 64, 64), (bs, 1, 64, 64)\n                    cat_graph = torch.tensor([torch.unsqueeze(categories[i][graph_iter], dim=0) for i in keep_in_batch]).to(rank)\n                    spcat_graph = [super_categories[i][graph_iter] for i in keep_in_batch] if super_categories[0] is not None else None\n                    bbox_graph = torch.stack([bbox[i][graph_iter] for i in keep_in_batch]).to(rank)\n\n                    for edge_iter in range(graph_iter):\n                        curr_edge_masks = torch.stack([torch.unsqueeze(masks[i][edge_iter], dim=0) for i in keep_in_batch])  # seg mask of every prev obj\n                        h_edge = torch.cat((image_feature[keep_in_batch] * curr_edge_masks, image_depth[keep_in_batch] * curr_edge_masks), dim=1)\n                        cat_edge = torch.tensor([torch.unsqueeze(categories[i][edge_iter], dim=0) for i in keep_in_batch]).to(rank)\n                        spcat_edge = [super_categories[i][edge_iter] for i in keep_in_batch] if super_categories[0] is not None else None\n                        bbox_edge = torch.stack([bbox[i][edge_iter] for i in keep_in_batch]).to(rank)\n\n                        # filter out subject-object pairs whose iou=0\n                        joint_intersect = torch.logical_or(curr_graph_masks, curr_edge_masks)\n                        joint_union = torch.logical_and(curr_graph_masks, curr_edge_masks)\n                        joint_iou = (torch.sum(torch.sum(joint_intersect, dim=-1), dim=-1) / torch.sum(torch.sum(joint_union, dim=-1), dim=-1)).flatten()\n                        joint_iou[torch.isinf(joint_iou)] = 0\n                        iou_mask = joint_iou > 0\n                        if torch.sum(iou_mask) == 0:\n                            continue\n                        # iou_mask = torch.ones(len(keep_in_batch), dtype=torch.bool).to(rank)\n\n                        \"\"\"\n                        FIRST DIRECTION\n                        \"\"\"\n                        curr_num_not_connected, curr_num_connected, curr_num_connected_pred, curr_connectivity_precision, curr_connectivity_recall = \\\n                            evaluate_one_direction(relation_classifier, args, h_graph, h_edge, cat_graph, cat_edge, spcat_graph, spcat_edge, bbox_graph, bbox_edge, iou_mask, rank, graph_iter, edge_iter, keep_in_batch,\n                                                   Recall, Recall_top3, relations_target, direction_target, batch_count, len(test_loader), first_direction=True)\n\n                        num_not_connected += curr_num_not_connected\n                        num_connected += curr_num_connected\n                        num_connected_pred += curr_num_connected_pred\n                        connectivity_precision += curr_connectivity_precision\n                        connectivity_recall += curr_connectivity_recall\n\n                        \"\"\"\n                        SECOND DIRECTION\n                        \"\"\"\n                        curr_num_not_connected, curr_num_connected, curr_num_connected_pred, curr_connectivity_precision, curr_connectivity_recall = \\\n                            evaluate_one_direction(relation_classifier, args, h_edge, h_graph, cat_edge, cat_graph, spcat_edge, spcat_graph, bbox_edge, bbox_graph, iou_mask, rank, graph_iter, edge_iter, keep_in_batch,\n                                                   Recall, Recall_top3, relations_target, direction_target, batch_count, len(test_loader), first_direction=False)\n\n                        num_not_connected += curr_num_not_connected\n                        num_connected += curr_num_connected\n                        num_connected_pred += curr_num_connected_pred\n                        connectivity_precision += curr_connectivity_precision\n                        connectivity_recall += curr_connectivity_recall\n\n                \"\"\"\n                EVALUATE AND PRINT CURRENT RESULTS\n                \"\"\"\n                if (batch_count % args['training']['eval_freq_test'] == 0) or (batch_count + 1 == len(test_loader)):\n                    if args['training']['save_vis_results'] and args['training']['eval_mode'] == 'pc':\n                        Recall.save_visualization_results(annot_path, triplets, heights, widths, images_raw, image_depth, bbox, categories, batch_count, top_k=15)\n\n                    if args['dataset']['dataset'] == 'vg':\n                        if args['training']['run_mode'] == 'prepare_cs':\n                            \"\"\"\n                            To achieve commonsense validation, we run the model on the training set, save the top-k predictions for each image,\n                            and then collect a commonsense-aligned set and a commonsense-violated set stored in the form of two .pt files. \n                            Note that the query process to GPT may be interrupted by API, budget limit, or Internet issues, \n                            so we opt to save the results for each image and collect the two sets afterwards.\n                            \"\"\"\n                            _, _, cache_hit_percentage = Recall.get_related_top_k_predictions_parallel(top_k=10)\n                            if batch_count + 1 == len(test_loader):\n                                print('cache hit percentage', cache_hit_percentage)\n                        else:\n                            recall, recall_per_class, mean_recall, recall_zs, _, mean_recall_zs = Recall.compute(per_class=True)\n                        if args['models']['hierarchical_pred']:\n                            recall_top3, _, mean_recall_top3 = Recall_top3.compute(per_class=True)\n                            Recall_top3.clear_data()\n                    else:\n                        recall, _, mean_recall, _, _, _ = Recall.compute(per_class=True)\n                        wmap_rel, wmap_phrase = Recall.compute_precision()\n\n                    if args['training']['run_mode'] != 'prepare_cs':\n                        if (batch_count % args['training']['print_freq_test'] == 0) or (batch_count + 1 == len(test_loader)):\n                            record_test_results(args, test_record, rank, args['training']['test_epoch'], recall_top3, recall, mean_recall_top3, mean_recall, recall_zs, mean_recall_zs,\n                                                connectivity_recall, num_connected, num_not_connected, connectivity_precision, num_connected_pred, wmap_rel, wmap_phrase)\n                    # clean up the evaluator\n                    Recall.clear_data()\n\n        # on the second step of prepare_cs, we accumulate all the commonsense-aligned and commonsense-violated sets from all images in the training dataset and save them as two .pt files\n        if args['training']['run_mode'] == 'prepare_cs' and prepare_cs_step == 2:\n            curr_dataset.save_all_triplets()\n\n        dist.monitored_barrier(timeout=datetime.timedelta(seconds=3600))\n\n    Recall.clear_gpt_cache()\n    dist.destroy_process_group()  # clean up\n    print('FINISHED TESTING PC\\n')\n\n\ndef eval_sgd(gpu, args, test_subset):\n    \"\"\"\n    This function evaluates the module on scene graph detection tasks.\n    :param gpu: current gpu index\n    :param args: input arguments in config.yaml\n    :param test_subset: testing dataset\n    \"\"\"\n    rank = gpu\n    world_size = torch.cuda.device_count()\n    setup(rank, world_size)\n    print('rank', rank, 'torch.distributed.is_initialized', torch.distributed.is_initialized())\n\n    test_sampler = torch.utils.data.distributed.DistributedSampler(test_subset, num_replicas=world_size, rank=rank)\n    test_loader = torch.utils.data.DataLoader(test_subset, batch_size=args['training']['batch_size'], shuffle=False, collate_fn=collate_fn, num_workers=0, drop_last=True, sampler=test_sampler)\n    print(\"Finished loading the datasets...\")\n\n    start = []\n    test_record = []\n    with open(args['training']['result_path'] + 'test_results_' + str(rank) + '.json', 'w') as f:  # clear history logs\n        json.dump(start, f)\n\n    if args['models']['hierarchical_pred']:\n        relation_classifier = DDP(BayesianRelationClassifier(args=args, input_dim=args['models']['hidden_dim'], feature_size=args['models']['feature_size'],\n                                                             num_classes=args['models']['num_classes'], num_super_classes=args['models']['num_super_classes'],\n                                                             num_geometric=args['models']['num_geometric'], num_possessive=args['models']['num_possessive'],\n                                                             num_semantic=args['models']['num_semantic'])).to(rank)\n    else:\n        relation_classifier = DDP(FlatRelationClassifier(args=args, input_dim=args['models']['hidden_dim'], output_dim=args['models']['num_relations'],\n                                                         feature_size=args['models']['feature_size'], num_classes=args['models']['num_classes'])).to(rank)\n\n    detr = DDP(build_detr101(args)).to(rank)\n    backbone = DDP(detr.module.backbone).to(rank)\n    input_proj = DDP(detr.module.input_proj).to(rank)\n    feature_encoder = DDP(detr.module.transformer.encoder).to(rank)\n\n    relation_classifier.eval()\n    backbone.eval()\n    input_proj.eval()\n    feature_encoder.eval()\n\n    map_location = {'cuda:%d' % rank: 'cuda:%d' % 0}\n    if args['models']['hierarchical_pred']:\n        load_model_name = 'HierRelationModel_CS' if args['training']['run_mode'] == 'eval_cs' else 'HierRelationModel_Baseline'\n        load_model_name += '_' + args['dataset']['supcat_clustering'] + '_'\n        load_model_name = args['training']['checkpoint_path'] + load_model_name + str(args['training']['test_epoch']) + '_0' + '.pth'\n    else:\n        load_model_name = 'FlatRelationModel_CS' if args['training']['run_mode'] == 'eval_cs' else 'FlatRelationModel_Baseline'\n        load_model_name += '_' + args['dataset']['supcat_clustering'] + '_'\n        load_model_name = args['training']['checkpoint_path'] + load_model_name + str(args['training']['test_epoch']) + '_0' + '.pth'\n    if rank == 0:\n        print('Loading pretrained model from %s...' % load_model_name)\n    relation_classifier.load_state_dict(torch.load(load_model_name, map_location=map_location))\n\n    connectivity_recall, connectivity_precision, num_connected, num_not_connected, num_connected_pred = 0.0, 0.0, 0.0, 0.0, 0.0\n    recall_top3, recall, mean_recall_top3, mean_recall, recall_zs, mean_recall_zs, wmap_rel, wmap_phrase = None, None, None, None, None, None, None, None\n\n    Recall = Evaluator(args=args, num_classes=args['models']['num_relations'], iou_thresh=0.5, top_k=[20, 50, 100])\n\n    sub2super_cat_dict = torch.load(args['dataset']['sub2super_cat_dict'])\n    object_class_alp2fre_dict = object_class_alp2fre()\n\n    print('Start Testing SGD...')\n    with torch.no_grad():\n        for batch_count, data in enumerate(tqdm(test_loader), 0):\n            \"\"\"\n            PREPARE INPUT DATA WITH PREDICTED OBJECT BOUNDING BOXES AND LABELS\n            \"\"\"\n            try:\n                images, image2, image_depth, categories_target, super_categories_target, bbox_target, relationships, subj_or_obj, _ = data\n            except:\n                continue\n\n            image_feature = process_image_features(args, images, detr, rank)\n\n            image_depth = torch.stack([depth.to(rank) for depth in image_depth])\n            categories_target = [category.to(rank) for category in categories_target]  # [batch_size][curr_num_obj, 1]\n            bbox_target = [box.to(rank) for box in bbox_target]  # [batch_size][curr_num_obj, 4]\n\n            image2 = [image.to(rank) for image in image2]\n            out_dict = detr(nested_tensor_from_tensor_list(image2))\n\n            logits_pred = torch.argmax(F.softmax(out_dict['pred_logits'], dim=2), dim=2)\n            has_object_pred = logits_pred < args['models']['num_classes']\n            logits_pred = torch.topk(F.softmax(out_dict['pred_logits'], dim=2), dim=2, k=args['models']['topk_cat'])[1].view(-1, 100, args['models']['topk_cat'])\n\n            logits_pred_value = torch.topk(F.softmax(out_dict['pred_logits'], dim=2), dim=2, k=args['models']['topk_cat'])[0].view(-1, 100, args['models']['topk_cat'])\n            cat_pred_confidence = [logits_pred_value[i, has_object_pred[i], :].flatten() for i in range(logits_pred_value.shape[0]) if torch.sum(has_object_pred[i]) > 0]\n\n            categories_pred = [logits_pred[i, has_object_pred[i], :].flatten() for i in range(logits_pred.shape[0]) if torch.sum(has_object_pred[i]) > 0]  # (batch_size, num_obj, 150)\n            # object category indices in pretrained DETR are different from our indices\n            for i in range(len(categories_pred)):\n                for j in range(len(categories_pred[i])):\n                    categories_pred[i][j] = object_class_alp2fre_dict[categories_pred[i][j].item()]     # at this moment, keep cat whose top2 == 150 for convenience\n            cat_mask = [categories_pred[i] != args['models']['num_classes'] for i in range(len(categories_pred))]\n\n            bbox_pred = [out_dict['pred_boxes'][i, has_object_pred[i]] for i in range(logits_pred.shape[0]) if torch.sum(has_object_pred[i]) > 0]  # convert from 0-1 to 0-32\n            for i in range(len(bbox_pred)):\n                # clone and calculate the new bounding box coordinates predicted by DETR\n                bbox_pred_c = bbox_pred[i].clone()\n                bbox_pred[i][:, [0, 2]] = bbox_pred_c[:, [0, 1]] - bbox_pred_c[:, [2, 3]] / 2\n                bbox_pred[i][:, [1, 3]] = bbox_pred_c[:, [0, 1]] + bbox_pred_c[:, [2, 3]] / 2\n                bbox_pred[i] = torch.clamp(bbox_pred[i], 0, 1)\n                bbox_pred[i] = (bbox_pred[i] * args['models']['feature_size']).repeat_interleave(args['models']['topk_cat'], dim=0)\n\n            masks_pred = []\n            for i in range(len(bbox_pred)):\n                mask_pred = torch.zeros(bbox_pred[i].shape[0], args['models']['feature_size'], args['models']['feature_size'], dtype=torch.uint8).to(rank)\n                for j, box in enumerate(bbox_pred[i]):\n                    mask_pred[j, int(bbox_pred[i][j][2]):int(bbox_pred[i][j][3]), int(bbox_pred[i][j][0]):int(bbox_pred[i][j][1])] = 1\n                masks_pred.append(mask_pred)\n\n            for i in range(len(categories_pred)):\n                categories_pred[i] = categories_pred[i][cat_mask[i]]\n                cat_pred_confidence[i] = cat_pred_confidence[i][cat_mask[i]]\n                bbox_pred[i] = bbox_pred[i][cat_mask[i]]\n                masks_pred[i] = masks_pred[i][cat_mask[i]]\n\n            # non-maximum suppression\n            for i in range(len(bbox_pred)):\n                bbox_pred[i] = bbox_pred[i][:, [0, 2, 1, 3]]\n\n                nms_keep_idx = None\n                for cls in torch.unique(categories_pred[i]):  # per class nms\n                    curr_class_idx = categories_pred[i] == cls\n                    curr_nms_keep_idx = torchvision.ops.nms(boxes=bbox_pred[i][curr_class_idx], scores=cat_pred_confidence[i][curr_class_idx],\n                                                            iou_threshold=args['models']['nms'])       # requires (x1, y1, x2, y2)\n                    if nms_keep_idx is None:\n                        nms_keep_idx = (torch.nonzero(curr_class_idx).flatten())[curr_nms_keep_idx]\n                    else:\n                        nms_keep_idx = torch.hstack((nms_keep_idx, (torch.nonzero(curr_class_idx).flatten())[curr_nms_keep_idx]))\n\n                bbox_pred[i] = bbox_pred[i][:, [0, 2, 1, 3]]       # convert back to (x1, x2, y1, y2)\n                categories_pred[i] = categories_pred[i][nms_keep_idx]\n                cat_pred_confidence[i] = cat_pred_confidence[i][nms_keep_idx]\n                bbox_pred[i] = bbox_pred[i][nms_keep_idx]\n                masks_pred[i] = masks_pred[i][nms_keep_idx]\n\n            # after nms\n            super_categories_pred = [[sub2super_cat_dict[c.item()] for c in categories_pred[i]] for i in range(len(categories_pred))]\n            super_categories_pred = [[torch.as_tensor(sc).to(rank) for sc in super_category] for super_category in super_categories_pred]\n\n            \"\"\"\n            PREPARE TARGETS\n            relations_target and direction_target: matched targets for each prediction\n            cat_subject_target, cat_object_target, bbox_subject_target, bbox_object_target, relation_target_origin: sets of original unmatched targets\n            \"\"\"\n            cat_subject_target, cat_object_target, bbox_subject_target, bbox_object_target, relation_target \\\n                = match_target_sgd(rank, relationships, subj_or_obj, categories_target, bbox_target)\n\n            \"\"\"\n            FORWARD PASS THROUGH THE LOCAL PREDICTOR\n            \"\"\"\n            num_graph_iter = torch.as_tensor([len(mask) for mask in masks_pred])\n            for graph_iter in range(max(num_graph_iter)):\n                keep_in_batch = torch.nonzero(num_graph_iter > graph_iter).view(-1).to(rank)\n\n                curr_graph_masks = torch.stack([torch.unsqueeze(masks_pred[i][graph_iter], dim=0) for i in keep_in_batch])\n                h_graph = torch.cat((image_feature[keep_in_batch] * curr_graph_masks, image_depth[keep_in_batch] * curr_graph_masks), dim=1)  # (bs, 256, 64, 64), (bs, 1, 64, 64)\n                cat_graph_pred = torch.tensor([torch.unsqueeze(categories_pred[i][graph_iter], dim=0) for i in keep_in_batch]).to(rank)\n                bbox_graph_pred = torch.stack([bbox_pred[i][graph_iter] for i in keep_in_batch]).to(rank)\n                cat_graph_confidence = torch.hstack([cat_pred_confidence[i][graph_iter] for i in keep_in_batch])\n\n                for edge_iter in range(graph_iter):\n                    curr_edge_masks = torch.stack([torch.unsqueeze(masks_pred[i][edge_iter], dim=0) for i in keep_in_batch])  # seg mask of every prev obj\n                    h_edge = torch.cat((image_feature[keep_in_batch] * curr_edge_masks, image_depth[keep_in_batch] * curr_edge_masks), dim=1)\n                    cat_edge_pred = torch.tensor([torch.unsqueeze(categories_pred[i][edge_iter], dim=0) for i in keep_in_batch]).to(rank)\n                    bbox_edge_pred = torch.stack([bbox_pred[i][edge_iter] for i in keep_in_batch]).to(rank)\n\n                    cat_edge_confidence = torch.hstack([cat_pred_confidence[i][edge_iter] for i in keep_in_batch])\n\n                    # filter out subject-object pairs whose iou=0\n                    joint_intersect = torch.logical_or(curr_graph_masks, curr_edge_masks)\n                    joint_union = torch.logical_and(curr_graph_masks, curr_edge_masks)\n                    joint_iou = (torch.sum(torch.sum(joint_intersect, dim=-1), dim=-1) / torch.sum(torch.sum(joint_union, dim=-1), dim=-1)).flatten()\n                    joint_iou[torch.isinf(joint_iou)] = 0\n                    iou_mask = joint_iou > 0\n\n                    if torch.sum(iou_mask) == 0:\n                        continue\n\n                    spcat_graph_pred = []    # they are not tensors but lists, which requires special mask manipulations\n                    spcat_edge_pred = []\n                    for count, i in enumerate(keep_in_batch):\n                        spcat_graph_pred.append(super_categories_pred[i][graph_iter])\n                        spcat_edge_pred.append(super_categories_pred[i][edge_iter])\n\n                    \"\"\"\n                    FIRST DIRECTION\n                    \"\"\"\n                    if args['models']['hierarchical_pred']:\n                        relation_1, relation_2, relation_3, super_relation, connectivity, _, _ = relation_classifier(h_graph, h_edge, cat_graph_pred, cat_edge_pred, spcat_graph_pred, spcat_edge_pred, rank)\n                        relation = torch.cat((relation_1, relation_2, relation_3), dim=1)\n                    else:\n                        relation, connectivity, _, _ = relation_classifier(h_graph, h_edge, cat_graph_pred, cat_edge_pred, spcat_graph_pred, spcat_edge_pred, rank)\n                        super_relation = None\n\n                    if (batch_count % args['training']['eval_freq_test'] == 0) or (batch_count + 1 == len(test_loader)):\n                        Recall.accumulate(keep_in_batch, relation, None, super_relation, torch.log(torch.sigmoid(connectivity[:, 0])),\n                                          cat_graph_pred, cat_edge_pred, None, None, bbox_graph_pred, bbox_edge_pred, None, None,\n                                          iou_mask, False, cat_graph_confidence, cat_edge_confidence)\n                    \"\"\"\n                    SECOND DIRECTION\n                    \"\"\"\n                    if args['models']['hierarchical_pred']:\n                        relation_1, relation_2, relation_3, super_relation, connectivity, _, _ = relation_classifier(h_edge, h_graph, cat_edge_pred, cat_graph_pred, spcat_edge_pred, spcat_graph_pred, rank)\n                        relation = torch.cat((relation_1, relation_2, relation_3), dim=1)\n                    else:\n                        relation, connectivity, _, _ = relation_classifier(h_edge, h_graph, cat_edge_pred, cat_graph_pred, spcat_edge_pred, spcat_graph_pred, rank)\n                        super_relation = None\n\n                    if (batch_count % args['training']['eval_freq_test'] == 0) or (batch_count + 1 == len(test_loader)):\n                        Recall.accumulate(keep_in_batch, relation, None, super_relation, torch.log(torch.sigmoid(connectivity[:, 0])),\n                                          cat_edge_pred, cat_graph_pred, None, None, bbox_edge_pred, bbox_graph_pred, None, None,\n                                          iou_mask, False, cat_edge_confidence, cat_graph_confidence)\n\n            if (batch_count % args['training']['eval_freq_test'] == 0) or (batch_count + 1 == len(test_loader)):\n                Recall.accumulate_target(relation_target, cat_subject_target, cat_object_target, bbox_subject_target, bbox_object_target)\n            \"\"\"\n            EVALUATE AND PRINT CURRENT RESULTS\n            \"\"\"\n            if (batch_count % args['training']['eval_freq_test'] == 0) or (batch_count + 1 == len(test_loader)):\n                recall, recall_per_class, mean_recall, recall_zs, _, mean_recall_zs = Recall.compute(per_class=True, predcls=False)\n                Recall.clear_data()\n\n                if (batch_count % args['training']['print_freq_test'] == 0) or (batch_count + 1 == len(test_loader)):\n                    record_test_results(args, test_record, rank, args['training']['test_epoch'], recall_top3, recall, mean_recall_top3, mean_recall, recall_zs, mean_recall_zs,\n                                        connectivity_recall, num_connected, num_not_connected, connectivity_precision, num_connected_pred, wmap_rel, wmap_phrase)\n\n        dist.monitored_barrier(timeout=datetime.timedelta(seconds=3600))\n\n    print('FINISHED TESTING SGD\\n')\n    dist.destroy_process_group()  # clean up\n\n\ndef eval_sgc(gpu, args, test_subset):\n    \"\"\"\n    This function evaluates the module on scene graph classification tasks.\n    :param gpu: current gpu index\n    :param args: input arguments in config.yaml\n    :param test_subset: testing dataset\n    \"\"\"\n    rank = gpu\n    world_size = torch.cuda.device_count()\n    setup(rank, world_size)\n    print('rank', rank, 'torch.distributed.is_initialized', torch.distributed.is_initialized())\n\n    test_sampler = torch.utils.data.distributed.DistributedSampler(test_subset, num_replicas=world_size, rank=rank)\n    test_loader = torch.utils.data.DataLoader(test_subset, batch_size=args['training']['batch_size'], shuffle=False, collate_fn=collate_fn, num_workers=0, drop_last=True, sampler=test_sampler)\n    print(\"Finished loading the datasets...\")\n\n    start = []\n    test_record = []\n    with open(args['training']['result_path'] + 'test_results_' + str(rank) + '.json', 'w') as f:  # clear history logs\n        json.dump(start, f)\n\n    if args['models']['hierarchical_pred']:\n        relation_classifier = DDP(BayesianRelationClassifier(args=args, input_dim=args['models']['hidden_dim'], feature_size=args['models']['feature_size'],\n                                                             num_classes=args['models']['num_classes'], num_super_classes=args['models']['num_super_classes'],\n                                                             num_geometric=args['models']['num_geometric'], num_possessive=args['models']['num_possessive'],\n                                                             num_semantic=args['models']['num_semantic'])).to(rank)\n    else:\n        relation_classifier = DDP(FlatRelationClassifier(args=args, input_dim=args['models']['hidden_dim'], output_dim=args['models']['num_relations'],\n                                                         feature_size=args['models']['feature_size'], num_classes=args['models']['num_classes'])).to(rank)\n\n    detr = DDP(build_detr101(args)).to(rank)\n    backbone = DDP(detr.module.backbone).to(rank)\n    input_proj = DDP(detr.module.input_proj).to(rank)\n    feature_encoder = DDP(detr.module.transformer.encoder).to(rank)\n\n    relation_classifier.eval()\n    backbone.eval()\n    input_proj.eval()\n    feature_encoder.eval()\n\n    map_location = {'cuda:%d' % rank: 'cuda:%d' % 0}\n    if args['models']['hierarchical_pred']:\n        load_model_name = 'HierRelationModel_CS' if args['training']['run_mode'] == 'eval_cs' else 'HierRelationModel_Baseline'\n        load_model_name += '_' + args['dataset']['supcat_clustering'] + '_'\n        load_model_name = args['training']['checkpoint_path'] + load_model_name + str(args['training']['test_epoch']) + '_0' + '.pth'\n    else:\n        load_model_name = 'FlatRelationModel_CS' if args['training']['run_mode'] == 'eval_cs' else 'FlatRelationModel_Baseline'\n        load_model_name += '_' + args['dataset']['supcat_clustering'] + '_'\n        load_model_name = args['training']['checkpoint_path'] + load_model_name + str(args['training']['test_epoch']) + '_0' + '.pth'\n    if rank == 0:\n        print('Loading pretrained model from %s...' % load_model_name)\n    relation_classifier.load_state_dict(torch.load(load_model_name, map_location=map_location))\n\n    Recall = Evaluator(args=args, num_classes=args['models']['num_relations'], iou_thresh=0.5, top_k=[20, 50, 100])\n\n    connectivity_recall, connectivity_precision, num_connected, num_not_connected, num_connected_pred = 0.0, 0.0, 0.0, 0.0, 0.0\n    recall_top3, recall, mean_recall_top3, mean_recall, recall_zs, mean_recall_zs, wmap_rel, wmap_phrase = None, None, None, None, None, None, None, None\n\n    sub2super_cat_dict = torch.load(args['dataset']['sub2super_cat_dict'])\n    object_class_alp2fre_dict = object_class_alp2fre()\n\n    print('Start Testing SGC...')\n    with torch.no_grad():\n        for batch_count, data in enumerate(tqdm(test_loader), 0):\n            \"\"\"\n            PREPARE INPUT DATA WITH PREDICTED OBJECT BOUNDING BOXES AND LABELS\n            \"\"\"\n            try:\n                images, image2, image_depth, categories_target, super_categories_target, bbox_target, relationships, subj_or_obj, _ = data\n            except:\n                continue\n\n            image_feature = process_image_features(args, images, detr, rank)\n\n            image_depth = torch.stack([depth.to(rank) for depth in image_depth])\n            categories_target = [category.to(rank) for category in categories_target]  # [batch_size][curr_num_obj, 1]\n            bbox_target = [box.to(rank) for box in bbox_target]  # [batch_size][curr_num_obj, 4]\n\n            image2 = [image.to(rank) for image in image2]\n            out_dict = detr(nested_tensor_from_tensor_list(image2))\n\n            logits_pred = torch.argmax(F.softmax(out_dict['pred_logits'], dim=2), dim=2)\n            has_object_pred = logits_pred < args['models']['num_classes']\n            logits_pred = torch.topk(F.softmax(out_dict['pred_logits'], dim=2), dim=2, k=args['models']['topk_cat'])[1].view(-1, 100, args['models']['topk_cat'])\n\n            logits_pred_value = torch.topk(F.softmax(out_dict['pred_logits'], dim=2), dim=2, k=args['models']['topk_cat'])[0].view(-1, 100, args['models']['topk_cat'])\n            cat_pred_confidence = [logits_pred_value[i, has_object_pred[i], :].flatten() for i in range(logits_pred_value.shape[0]) if torch.sum(has_object_pred[i]) > 0]\n\n            categories_pred = [logits_pred[i, has_object_pred[i], :].flatten() for i in range(logits_pred.shape[0]) if torch.sum(has_object_pred[i]) > 0]  # (batch_size, num_obj, 150)\n            # object category indices in pretrained DETR are different from our indices\n            for i in range(len(categories_pred)):\n                for j in range(len(categories_pred[i])):\n                    categories_pred[i][j] = object_class_alp2fre_dict[categories_pred[i][j].item()]     # at this moment, keep cat whose top2 == 150 for convenience\n            cat_mask = [categories_pred[i] != args['models']['num_classes'] for i in range(len(categories_pred))]\n\n            bbox_pred = [out_dict['pred_boxes'][i, has_object_pred[i]] for i in range(logits_pred.shape[0]) if torch.sum(has_object_pred[i]) > 0]  # convert from 0-1 to 0-32\n            for i in range(len(bbox_pred)):\n                # clone and calculate the new bounding box coordinates predicted by DETR\n                bbox_pred_c = bbox_pred[i].clone()\n                bbox_pred[i][:, [0, 2]] = bbox_pred_c[:, [0, 1]] - bbox_pred_c[:, [2, 3]] / 2\n                bbox_pred[i][:, [1, 3]] = bbox_pred_c[:, [0, 1]] + bbox_pred_c[:, [2, 3]] / 2\n                bbox_pred[i] = torch.clamp(bbox_pred[i], 0, 1)\n                bbox_pred[i] = (bbox_pred[i] * args['models']['feature_size']).repeat_interleave(args['models']['topk_cat'], dim=0)\n\n            for i in range(len(categories_pred)):\n                categories_pred[i] = categories_pred[i][cat_mask[i]]\n                cat_pred_confidence[i] = cat_pred_confidence[i][cat_mask[i]]\n                bbox_pred[i] = bbox_pred[i][cat_mask[i]]\n\n            # non-maximum suppression\n            for i in range(len(bbox_pred)):\n                bbox_pred[i] = bbox_pred[i][:, [0, 2, 1, 3]]\n\n                nms_keep_idx = None\n                for cls in torch.unique(categories_pred[i]):  # per class nms\n                    curr_class_idx = categories_pred[i] == cls\n                    curr_nms_keep_idx = torchvision.ops.nms(boxes=bbox_pred[i][curr_class_idx], scores=cat_pred_confidence[i][curr_class_idx],\n                                                            iou_threshold=args['models']['nms'])       # requires (x1, y1, x2, y2)\n                    if nms_keep_idx is None:\n                        nms_keep_idx = (torch.nonzero(curr_class_idx).flatten())[curr_nms_keep_idx]\n                    else:\n                        nms_keep_idx = torch.hstack((nms_keep_idx, (torch.nonzero(curr_class_idx).flatten())[curr_nms_keep_idx]))\n\n                bbox_pred[i] = bbox_pred[i][:, [0, 2, 1, 3]]       # convert back to (x1, x2, y1, y2)\n                categories_pred[i] = categories_pred[i][nms_keep_idx]\n                cat_pred_confidence[i] = cat_pred_confidence[i][nms_keep_idx]\n                bbox_pred[i] = bbox_pred[i][nms_keep_idx]\n\n            \"\"\"\n            PREPARE TARGETS\n            relations_target and direction_target: matched targets for each prediction\n            cat_subject_target, cat_object_target, bbox_subject_target, bbox_object_target, relation_target_origin: sets of original unmatched targets\n            \"\"\"\n            cat_subject_target, cat_object_target, bbox_subject_target, bbox_object_target, relation_target \\\n                = match_target_sgd(rank, relationships, subj_or_obj, categories_target, bbox_target)\n\n            \"\"\"\n            MATCH PREDICTED OBJECT LABELS FROM BOUNDING BOX IOUS\n            \"\"\"\n            categories_pred_matched, cat_pred_confidence_matched, bbox_target = match_object_categories(categories_pred, cat_pred_confidence, bbox_pred, bbox_target)\n            if categories_pred_matched is None or cat_pred_confidence_matched is None:\n                continue\n\n            # bbox_target = [torch.repeat_interleave(box, repeats=2, dim=0) for box in bbox_target]\n            assert len(categories_pred_matched[0]) == len(bbox_target[0])\n\n            # after nms\n            super_categories_pred = [[sub2super_cat_dict[c.item()] for c in categories_pred_matched[i]] for i in range(len(categories_pred_matched))]\n            super_categories_pred = [[torch.as_tensor(sc).to(rank) for sc in super_category] for super_category in super_categories_pred]\n\n            masks_target = []\n            for i in range(len(bbox_target)):\n                mask = torch.zeros(bbox_target[i].shape[0], args['models']['feature_size'], args['models']['feature_size'], dtype=torch.uint8).to(rank)\n                for j, box in enumerate(bbox_target[i]):\n                    mask[j, int(bbox_target[i][j][2]):int(bbox_target[i][j][3]), int(bbox_target[i][j][0]):int(bbox_target[i][j][1])] = 1\n                masks_target.append(mask)\n\n            \"\"\"\n            FORWARD PASS THROUGH THE LOCAL PREDICTOR\n            \"\"\"\n            num_graph_iter = torch.as_tensor([len(mask) for mask in masks_target])\n            for graph_iter in range(max(num_graph_iter)):\n                keep_in_batch = torch.nonzero(num_graph_iter > graph_iter).view(-1)\n\n                curr_graph_masks = torch.stack([torch.unsqueeze(masks_target[i][graph_iter], dim=0) for i in keep_in_batch])\n                h_graph = torch.cat((image_feature[keep_in_batch] * curr_graph_masks, image_depth[keep_in_batch] * curr_graph_masks), dim=1)  # (bs, 256, 64, 64), (bs, 1, 64, 64)\n                cat_graph_pred = torch.tensor([torch.unsqueeze(categories_pred_matched[i][graph_iter], dim=0) for i in keep_in_batch]).to(rank)\n                bbox_graph_pred = torch.stack([bbox_target[i][graph_iter] for i in keep_in_batch]).to(rank)    # use ground-truth bounding boxes\n                cat_graph_confidence = torch.hstack([cat_pred_confidence_matched[i][graph_iter] for i in keep_in_batch])\n\n                for edge_iter in range(graph_iter):\n                    curr_edge_masks = torch.stack([torch.unsqueeze(masks_target[i][edge_iter], dim=0) for i in keep_in_batch])  # seg mask of every prev obj\n                    h_edge = torch.cat((image_feature[keep_in_batch] * curr_edge_masks, image_depth[keep_in_batch] * curr_edge_masks), dim=1)\n                    cat_edge_pred = torch.tensor([torch.unsqueeze(categories_pred_matched[i][edge_iter], dim=0) for i in keep_in_batch]).to(rank)\n                    bbox_edge_pred = torch.stack([bbox_target[i][edge_iter] for i in keep_in_batch]).to(rank)    # use ground-truth bounding boxes\n                    cat_edge_confidence = torch.hstack([cat_pred_confidence_matched[i][edge_iter] for i in keep_in_batch])\n\n                    # filter out subject-object pairs whose iou=0\n                    joint_intersect = torch.logical_or(curr_graph_masks, curr_edge_masks)\n                    joint_union = torch.logical_and(curr_graph_masks, curr_edge_masks)\n                    joint_iou = (torch.sum(torch.sum(joint_intersect, dim=-1), dim=-1) / torch.sum(torch.sum(joint_union, dim=-1), dim=-1)).flatten()\n                    joint_iou[torch.isinf(joint_iou)] = 0\n                    iou_mask = joint_iou > 0\n                    if torch.sum(iou_mask) == 0:\n                        continue\n\n                    spcat_graph_pred = []  # they are not tensors but lists, which requires special mask manipulations\n                    spcat_edge_pred = []\n                    for count, i in enumerate(keep_in_batch):\n                        spcat_graph_pred.append(super_categories_pred[i][graph_iter])\n                        spcat_edge_pred.append(super_categories_pred[i][edge_iter])\n\n                    \"\"\"\n                    FIRST DIRECTION\n                    \"\"\"\n                    with torch.no_grad():\n                        if args['models']['hierarchical_pred']:\n                            relation_1, relation_2, relation_3, super_relation, connectivity, _, _ = relation_classifier(h_graph, h_edge, cat_graph_pred, cat_edge_pred, spcat_graph_pred, spcat_edge_pred, rank)\n                            relation = torch.cat((relation_1, relation_2, relation_3), dim=1)\n                        else:\n                            relation, connectivity, _, _ = relation_classifier(h_graph, h_edge, cat_graph_pred, cat_edge_pred, spcat_graph_pred, spcat_edge_pred, rank)\n                            super_relation = None\n\n                    if (batch_count % args['training']['eval_freq_test'] == 0) or (batch_count + 1 == len(test_loader)):\n                            Recall.accumulate(keep_in_batch, relation, None, super_relation, torch.log(torch.sigmoid(connectivity[:, 0])),\n                                              cat_graph_pred, cat_edge_pred, None, None, bbox_graph_pred, bbox_edge_pred, None, None,\n                                              iou_mask, False, cat_graph_confidence, cat_edge_confidence)\n                    \"\"\"\n                    SECOND DIRECTION\n                    \"\"\"\n                    with torch.no_grad():\n                        if args['models']['hierarchical_pred']:\n                            relation_1, relation_2, relation_3, super_relation, connectivity, _, _ = relation_classifier(h_edge, h_graph, cat_edge_pred, cat_graph_pred, spcat_edge_pred, spcat_graph_pred, rank)\n                            relation = torch.cat((relation_1, relation_2, relation_3), dim=1)\n                        else:\n                            relation, connectivity, _, _ = relation_classifier(h_edge, h_graph, cat_edge_pred, cat_graph_pred, spcat_edge_pred, spcat_graph_pred, rank)\n                            super_relation = None\n\n                    if (batch_count % args['training']['eval_freq_test'] == 0) or (batch_count + 1 == len(test_loader)):\n                        Recall.accumulate(keep_in_batch, relation, None, super_relation, torch.log(torch.sigmoid(connectivity[:, 0])),\n                                               cat_edge_pred, cat_graph_pred, None, None, bbox_edge_pred, bbox_graph_pred, None, None,\n                                               iou_mask, False, cat_edge_confidence, cat_graph_confidence)\n\n            if (batch_count % args['training']['eval_freq_test'] == 0) or (batch_count + 1 == len(test_loader)):\n                Recall.accumulate_target(relation_target, cat_subject_target, cat_object_target, bbox_subject_target, bbox_object_target)\n\n            \"\"\"\n            EVALUATE AND PRINT CURRENT RESULTS\n            \"\"\"\n            if (batch_count % args['training']['eval_freq_test'] == 0) or (batch_count + 1 == len(test_loader)):\n                recall, recall_per_class, mean_recall, recall_zs, _, mean_recall_zs = Recall.compute(per_class=True, predcls=False)\n                Recall.clear_data()\n\n            if (batch_count % args['training']['print_freq_test'] == 0) or (batch_count + 1 == len(test_loader)):\n                record_test_results(args, test_record, rank, args['training']['test_epoch'], recall_top3, recall, mean_recall_top3, mean_recall, recall_zs, mean_recall_zs,\n                                    connectivity_recall, num_connected, num_not_connected, connectivity_precision, num_connected_pred, wmap_rel, wmap_phrase)\n\n    dist.monitored_barrier()\n    print('FINISHED TESTING SGC\\n')\n\n"}
{"type": "source_file", "path": "preprocess.py", "content": "# reference:\n# https://github.com/ronghanghu/seg_every_thing\n# https://github.com/danfeiX/scene-graph-TF-release\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport json\nimport yaml\nimport numpy as np\nimport h5py\nfrom collections import Counter\nfrom dataloader import *\nfrom dataset_utils import *\n\n\n# load hyper-parameters\ntry:\n    with open ('config.yaml', 'r') as file:\n        args = yaml.safe_load(file)\nexcept Exception as e:\n    print('Error reading the config file')\n\nN_class = args['models']['num_classes']  # keep the top 150 classes\nN_relation = args['models']['num_relations']  # keep the top 50 relations\nraw_data_dir = args['dataset']['raw_annot_dir']\noutput_dir = raw_data_dir\n\n# Ensure an identical train-test split with the pretrained detr backbone and other works\n# using VG-SGG-with-attri.h5 from https://github.com/KaihuaTang/Scene-Graph-Benchmark.pytorch/blob/master/DATASET.md\nroi_h5 = h5py.File('/tmp/datasets/vg/VG-SGG-with-attri.h5', 'r')\ndata_split = roi_h5['split'][:]\nall_training_idx = np.where(data_split == 0)[0]\nall_testing_idx = np.where(data_split == 2)[0]\nassert len(all_training_idx) == 75651 and len(all_testing_idx) == 32422\n\n# ---------------------------------------------------------------------------- #\n# Load raw VG annotations and collect top-frequent synsets\n# ---------------------------------------------------------------------------- #\n\nwith open(raw_data_dir + 'image_data.json') as f:\n    raw_img_data = json.load(f)\nwith open(raw_data_dir + 'objects.json') as f:\n    raw_obj_data = json.load(f)\nwith open(raw_data_dir + 'relationships.json') as f:\n    raw_relation_data = json.load(f)\n\n# ---------------------------------------------------------------------------- #\n# Clean raw dataset\n# ---------------------------------------------------------------------------- #\n\n# same pre-processing as in https://github.com/danfeiX/scene-graph-TF-release/blob/master/data_tools/vg_to_roidb.py\nsync_objects(raw_obj_data, raw_relation_data)\nobj_rel_cross_check(raw_obj_data, raw_relation_data)\n\nobj_alias_dict, obj_vocab_list = make_alias_dict(args['dataset']['object_alias'])\npred_alias_dict, pred_vocab_list = make_alias_dict(args['dataset']['predicate_alias'])\npreprocess_object_labels(raw_obj_data, alias_dict=obj_alias_dict)\npreprocess_predicates(raw_relation_data, alias_dict=pred_alias_dict)\n\nimages_id2area = {img['image_id']: img['width'] * img['height'] for img in raw_img_data}\nfilter_object_boxes(raw_obj_data, images_id2area)\n\nmerge_duplicate_boxes(raw_obj_data)\n\n# ---------------------------------------------------------------------------- #\n# collect top-frequent synsets\n# ---------------------------------------------------------------------------- #\n\n# collect top frequent object synsets\nobj_list = make_list(args['dataset']['object_list'])\nall_synsets = [\n    name for img in raw_obj_data\n    for obj in img['objects'] for name in obj['names'] if name in obj_list]\nsynset_counter = Counter(all_synsets)\ntop_synsets_obj = [\n    synset for synset, _ in synset_counter.most_common(N_class)]\nprint('top_synsets_obj', top_synsets_obj)\n\n# collect top frequent relationship synsets\nall_synsets = [\n    synset for img in raw_relation_data\n    for obj in img['relationships'] for synset in obj['predicate']]\nsynset_counter = Counter(all_synsets)\ntop_synsets_relation = [\n    synset for synset, _ in synset_counter.most_common(N_relation)]\nprint('top_synsets_relation', top_synsets_relation)\n\n# ---------------------------------------------------------------------------- #\n# build \"image\"\n# ---------------------------------------------------------------------------- #\n\ncorrupted_ims = [1592, 1722, 4616, 4617]\nimages = [\n    {'id': img['image_id'],\n     'width': img['width'],\n     'height': img['height'],\n     'file_name': img['url'].replace('https://cs.stanford.edu/people/rak248/', ''),\n     'coco_id': img['coco_id']}\n    for img in raw_img_data if img['image_id'] not in corrupted_ims]\n\n# ---------------------------------------------------------------------------- #\n# build raw \"categories\"\n# ---------------------------------------------------------------------------- #\n\ncategories = [\n    {'id': n, 'name': synset} for n, synset in enumerate(top_synsets_obj)]\nsynset2cid = {c['name']: c['id'] for c in categories}\n\nsub2super_dict = preprocess_super_class(synset2cid, args['dataset']['object_types'])\ntorch.save(sub2super_dict, args['dataset']['sub2super_cat_dict'])\n\nrelationships = [\n    {'id': n, 'name': synset} for n, synset in enumerate(top_synsets_relation)]\nsynset2rid = {c['name']: c['id'] for c in relationships}\n# print('synset2rid', synset2rid)\n\n# ---------------------------------------------------------------------------- #\n# build \"instances\"\n# ---------------------------------------------------------------------------- #\n\ninstances = []\nskip_count_1, skip_count_2, skip_count_3, skip_count_4 = 0, 0, 0, 0\nave_num_obj = []\nfor idx, img in enumerate(raw_obj_data):\n    if img['image_id'] in corrupted_ims:\n        continue\n    image_area = images_id2area[img['image_id']]\n\n    num_objs = 0\n    for obj in img['objects']:\n        synsets = obj['names']\n        if len(synsets) == 0:\n            skip_count_1 += 1\n        elif len(synsets) > 1:\n            skip_count_2 += 1\n        elif synsets[0] not in synset2cid:\n            skip_count_3 += 1\n        else:\n            bbox = [obj['x'], obj['y'], obj['x']+obj['w'], obj['y']+obj['h']]\n            area = obj['w'] * obj['h']\n            if area <= image_area * args['dataset']['area_frac_thresh']:\n                skip_count_4 += 1\n                continue\n            cid = synset2cid[synsets[0]]\n            scid = sub2super_dict[cid]\n            ann = {'id': obj['object_id'],\n                   'image_id': img['image_id'],\n                   'category_id': cid,\n                   'super_category_id': scid,\n                   'bbox': bbox,\n                   'area': area}\n            instances.append(ann)\n            num_objs += 1\n\n    ave_num_obj.append(num_objs)\nave_num_obj = np.mean(ave_num_obj)\n\nprint('average num of obj per image:', ave_num_obj, 'skip_count:', skip_count_1, skip_count_2, skip_count_3, skip_count_4)\nprint('Done building instance annotations.')\n\n# ---------------------------------------------------------------------------- #\n# build \"annotations\" for relationships\n# ---------------------------------------------------------------------------- #\n\nannotations = []\nnum_relation_per_pair = []\nskip_count_1, skip_count_2, skip_count_3 = 0, 0, 0\nfor img in raw_relation_data:\n    for pair in img['relationships']:\n        synsets_relation = pair['predicate']\n        synsets_obj1 = pair['subject']['names']\n        synsets_obj2 = pair['object']['names']\n\n        if len(synsets_relation) == 0 or len(synsets_obj1) == 0 or len(synsets_obj2) == 0:\n            skip_count_1 += 1\n        elif len(synsets_obj1) > 1 or len(synsets_obj2) > 1:\n            skip_count_2 += 1\n        elif (synsets_relation[0] not in synset2rid) or (synsets_obj1[0] not in synset2cid) or (synsets_obj2[0] not in synset2cid):\n            skip_count_3 += 1\n        else:\n            rid = synset2rid[synsets_relation[0]]\n            oid1 = pair['subject']['object_id']\n            oid2 = pair['object']['object_id']\n            cid1 = synset2cid[synsets_obj1[0]]\n            cid2 = synset2cid[synsets_obj2[0]]\n            scid1 = sub2super_dict[cid1]\n            scid2 = sub2super_dict[cid2]\n\n            ann = {'image_id': img['image_id'],\n                   'relation_id': rid,\n                   'subject_id': oid1,\n                   'object_id': oid2,\n                   'category1': cid1,\n                   'category2': cid2,\n                   'super_category1': scid1,\n                   'super_category2': scid2}\n            annotations.append(ann)\nprint('Done building relationship annotations.', len(annotations), 'skip_count:', skip_count_1, skip_count_2, skip_count_3)\n\n# ---------------------------------------------------------------------------- #\n# Split into train and test\n# ---------------------------------------------------------------------------- #\n\n# Save the dataset splits\nimages_train = [images[i] for i in all_training_idx]\nimages_test = [images[i] for i in all_testing_idx]\nprint('len(images_train)', len(images_train), 'len(images_test)', len(images_test))\nassert len(images_train) == 75651 and len(images_test) == 32422\n\nannotations_train = [annotations[i] for i in all_training_idx]\nannotations_test = [annotations[i] for i in all_testing_idx]\ninstances_train = [instances[i] for i in all_training_idx]\ninstances_test = [instances[i] for i in all_testing_idx]\n\n# ---------------------------------------------------------------------------- #\n# Save to json file\n# ---------------------------------------------------------------------------- #\n\ndataset_vg3k_train = {\n    'images': images_train,\n    'annotations': annotations_train,\n    'categories': categories,\n    'instances': instances_train,\n    'relationships': relationships}\ndataset_vg3k_test = {\n    'images': images_test,\n    'annotations': annotations_test,\n    'categories': categories,\n    'instances': instances_test,\n    'relationships': relationships}\n\nwith open(output_dir + 'instances_vg_train.json', 'w') as f:\n    json.dump(dataset_vg3k_train, f)\nwith open(output_dir + 'instances_vg_test.json', 'w') as f:\n    json.dump(dataset_vg3k_test, f)\nprint('Done saving training and testing datasets.')\n\n\n# top_synsets_obj ['tree', 'man', 'window', 'shirt', 'building', 'person', 'sign', 'leg', 'head', 'pole', 'table', 'woman', 'hair',\n# 'hand', 'car', 'door', 'leaf', 'light', 'pant', 'fence', 'ear', 'shoe', 'chair', 'people', 'plate', 'arm', 'glass', 'jacket', 'street',\n# 'sidewalk', 'snow', 'tail', 'face', 'wheel', 'handle', 'flower', 'hat', 'rock', 'boy', 'tile', 'short', 'bag', 'roof', 'letter',\n# 'girl', 'umbrella', 'helmet', 'bottle', 'branch', 'tire', 'plant', 'train', 'track', 'nose', 'boat', 'post', 'bench', 'shelf', 'wave',\n# 'box', 'food', 'pillow', 'jean', 'bus', 'bowl', 'eye', 'trunk', 'horse', 'clock', 'counter', 'neck', 'elephant', 'giraffe', 'mountain',\n# 'board', 'house', 'cabinet', 'banana', 'paper', 'hill', 'logo', 'dog', 'wing', 'book', 'bike', 'coat', 'seat', 'truck', 'glove', 'zebra',\n# 'bird', 'cup', 'plane', 'cap', 'lamp', 'motorcycle', 'cow', 'skateboard', 'wire', 'surfboard', 'beach', 'mouth', 'sheep', 'kite', 'sink',\n# 'cat', 'pizza', 'bed', 'animal', 'ski', 'curtain', 'bear', 'sock', 'player', 'flag', 'finger', 'windshield', 'towel', 'desk', 'number',\n# 'railing', 'lady', 'stand', 'vehicle', 'child', 'boot', 'tower', 'basket', 'laptop', 'engine', 'vase', 'toilet', 'drawer', 'racket',\n# 'tie', 'pot', 'paw', 'airplane', 'fork', 'screen', 'room', 'guy', 'orange', 'phone', 'fruit', 'vegetable', 'sneaker', 'skier', 'kid', 'men']\n#\n# top_synsets_relation ['on', 'has', 'in', 'of', 'wearing', 'near', 'with', 'above', 'holding', 'behind', 'under', 'sitting on',\n# 'wears', 'standing on', 'in front of', 'attached to', 'at', 'hanging from', 'over', 'for', 'riding', 'carrying', 'eating',\n# 'walking on', 'playing', 'covering', 'laying on', 'along', 'watching', 'and', 'between', 'belonging to', 'painted on', 'against',\n# 'looking at', 'from', 'parked on', 'to', 'made of', 'covered in', 'mounted on', 'says', 'part of', 'across', 'flying in', 'using',\n# 'on back of', 'lying on', 'growing on', 'walking in']\n\n"}
{"type": "source_file", "path": "query_llm.py", "content": "import torch\nimport numpy as np\nfrom tqdm import tqdm\nimport os\nimport json\nimport openai\n# from openai import OpenAI\nfrom collections import OrderedDict\nimport re\nimport random\nimport cv2\nimport base64\nimport requests\n\n\nclass EdgeCache:\n    def __init__(self, max_cache_size):\n        self.cache = OrderedDict()\n        self.max_cache_size = max_cache_size\n        self.access_frequency = {}\n\n    def get(self, key):\n        return self.cache.get(key, None)\n\n    def put(self, key, value):\n        if key in self.cache:\n            # Move to end to show it was recently accessed\n            self.cache.move_to_end(key)\n            # Increase access frequency\n            self.access_frequency[key] += 1\n        else:\n            if len(self.cache) >= self.max_cache_size:\n                self._purge_least_frequent()\n            self.cache[key] = value\n            self.access_frequency[key] = 1\n\n    def _purge_least_frequent(self):\n        # Find the least frequently accessed item\n        least_frequent_key = min(self.access_frequency, key=self.access_frequency.get)\n        # Remove the least frequently accessed item\n        if least_frequent_key in self.cache:\n            del self.cache[least_frequent_key]\n        if least_frequent_key in self.access_frequency:\n            del self.access_frequency[least_frequent_key]\n\n    def cache_info(self):\n        return len(self.cache), self.max_cache_size\n\n\ndef batch_query_openai_gpt(predicted_edges, edge_cache, batch_size=4, cache_hits=0,\n                           annot_name=None, sub_bbox=None, obj_bbox=None, image_cache=None, image_dir=None):\n    # input arguments annot_name, sub_bbox, obj_bbox, image_cache, and image_dir are only required if using GPT-4V as the commonsense validator\n    use_vision = annot_name is not None\n    if use_vision:\n        batch_size = 1\n\n    total_edges = len(predicted_edges)\n    all_responses = []\n\n    for i in range(0, total_edges, batch_size):\n        batched_edges = predicted_edges[i: i + batch_size]\n        batched_edges_to_query = []\n\n        for edge in batched_edges:\n            if use_vision:  # do not use edge cache\n                batched_edges_to_query.append(edge)\n            else:\n                cached_response = edge_cache.get(edge)\n                if cached_response is not None and random.random() < 0.9:\n                    all_responses.append(cached_response)\n                    cache_hits += 1\n                    edge_cache.put(edge, cached_response)  # Update cache access frequency\n                else:\n                    batched_edges_to_query.append(edge)\n\n        if batched_edges_to_query:\n            if use_vision:\n                responses = _query_openai_gpt_4v(batched_edges_to_query, annot_name, sub_bbox[i], obj_bbox[i], image_cache, image_dir)\n            else:\n                responses = _batch_query_openai_gpt_3p5_instruct(batched_edges_to_query)\n\n            for edge, response in zip(batched_edges_to_query, responses):\n                if not use_vision:\n                    edge_cache.put(edge, response)\n                all_responses.append(response)\n\n    return all_responses, cache_hits\n\n\ndef _batch_query_openai_gpt_3p5_instruct(predicted_edges, verbose=False):\n    openai.api_key_path = 'openai_key.txt'\n    responses = torch.ones(len(predicted_edges)) * -1\n\n    prompts = []\n\n    # Prepare multiple variations of each prompt\n    prompt_variations = [\n        \"Is the relation '{}' generally make sense or a trivially true fact? Answer with 'Yes' or 'No' and justify your answer. A trivially true relation is still a 'Yes'.\",\n        \"Could there be either a {} or a {}s? Yes or No and justify your answer.\",\n        \"Regardless of whether it is basic or redundant, is the relation '{}' incorrect and is a mis-classification in scene graph generation? Show your reasoning and answer 'Yes' or 'No'.\",\n        \"Is the relation {} impossible in real world? Answer 'Yes' or 'No' and explain your answer.\"\n    ]\n\n    # For each predicted edge, create multiple prompts\n    for edge in predicted_edges:\n        for i, variation in enumerate(prompt_variations):\n            if i == 1:\n                prompts.append(variation.format(edge, edge))\n            else:\n                prompts.append(variation.format(edge))\n\n    # Call OpenAI with the batch of prompts\n    completions = openai.Completion.create(\n        model=\"gpt-3.5-turbo-instruct\",\n        prompt=prompts,\n        temperature=0,\n        max_tokens=100\n    )\n\n    # Gather responses and decide based on majority\n    for i, edge in enumerate(predicted_edges):\n        yes_votes = 0\n        no_votes = 0\n        for j in range(len(prompt_variations)):\n            completion_text = completions.choices[i * len(prompt_variations) + j].text\n            if verbose:\n                print(completion_text)\n            # completion_text = completions.choices[i * len(prompt_variations) + j].message\n\n            if j == 2 or j == 3:  # For the last two questions, we reverse the logic\n                if re.search(r'Yes', completion_text):\n                    no_votes += 1\n                elif re.search(r'No', completion_text):\n                    yes_votes += 1\n                else:\n                    no_votes += 1\n            else:\n                if re.search(r'Yes', completion_text):\n                    if j == 0:\n                        yes_votes += 2\n                    else:\n                        yes_votes += 1\n                else:\n                    if j == 0:\n                        no_votes += 2\n                    else:\n                        no_votes += 1\n\n        if yes_votes > no_votes:\n            if verbose:\n                print(f'predicted_edge {edge} [MAJORITY YES] {yes_votes} Yes votes vs {no_votes} No votes')\n            responses[i] = 1\n        else:\n            if verbose:\n                print(f'predicted_edge {edge} [MAJORITY NO] {no_votes} No votes vs {yes_votes} Yes votes')\n            responses[i] = -1\n\n    return responses\n\n\nclass ImageCache:\n    def __init__(self, image_size, feature_size):\n        self.cache = {}\n        self.image_size = image_size\n        self.feature_size = feature_size\n\n    def get_image(self, image_path, bbox=None):\n        if image_path not in self.cache:\n            image = cv2.imread(image_path)\n            image = cv2.resize(image, dsize=(self.image_size, self.image_size))\n\n            if bbox is not None:\n                x1, x2, y1, y2 = bbox[0], bbox[1], bbox[2], bbox[3]\n                image = image[y1:y2, x1:x2]\n\n            # Convert image to bytes for base64 encoding\n            _, buffer = cv2.imencode('.jpg', image)\n            image_bytes = np.array(buffer).tobytes()\n\n            self.cache[image_path] = base64.b64encode(image_bytes).decode('utf-8')\n        return self.cache[image_path]\n\n\ndef get_union_bbox(sub_bbox, obj_bbox):\n    # Calculate the smallest bounding box that contains both subject and object\n    x1 = min(sub_bbox[0], obj_bbox[0])\n    y1 = min(sub_bbox[1], obj_bbox[1])\n    x2 = max(sub_bbox[2], obj_bbox[2])\n    y2 = max(sub_bbox[3], obj_bbox[3])\n    return [x1, y1, x2, y2]\n\n\ndef _query_openai_gpt_4v(predicted_edges, annot_name, sub_bbox, obj_bbox, image_cache, image_dir, verbose=False):\n    with open(\"openai_key.txt\", \"r\") as api_key_file:\n        api_key = api_key_file.read()\n\n    responses = torch.ones(len(predicted_edges)) * -1\n\n    # GPT-4V does not support batch inference at this moment, but we keep the same structure as in _batch_query_openai_gpt_instruct for code simplicity\n    assert len(predicted_edges) == 1\n\n    for i, edge in enumerate(predicted_edges):\n        # Construct the path to the image and annotations\n        image_path = os.path.join(image_dir, annot_name[:-16] + '.jpg')\n\n        # Load and process image and annotations if they exist\n        if os.path.exists(image_path):\n            image_path = os.path.join(image_dir, annot_name[:-16] + '.jpg')\n            if verbose:\n                print('annot_name', annot_name, 'image_path', image_path, 'edge', edge)\n\n            sub_bbox *= image_cache.feature_size\n            obj_bbox *= image_cache.feature_size\n            union_bbox = get_union_bbox(sub_bbox, obj_bbox)\n            base64_image = image_cache.get_image(image_path, bbox=union_bbox)\n\n            # Form the prompt including the image.\n            # Due to the strong performance of the vision model, we omit multiple queries and majority vote to reduce costs\n            prompt = {\n                \"model\": \"gpt-4-vision-preview\",\n                \"messages\": [\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\"type\": \"text\", \"text\": \"Does the image contain a relation '{}'? Let us think about it step by step and answer with Yes or No in the end.\".format(edge)},\n                            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n                        ]\n                    }\n                ],\n                \"max_tokens\": 300\n            }\n\n            # Send request to OpenAI API\n            headers = {\n                \"Content-Type\": \"application/json\",\n                \"Authorization\": f\"Bearer {api_key}\"\n            }\n            response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=prompt)\n            response_json = response.json()\n\n            # Process the response\n            # Check if the response is valid and contains the expected data\n            if 'choices' in response_json and len(response_json['choices']) > 0:\n                completion_text = response_json['choices'][0].get('message', {}).get('content', '')\n\n                # Parse the response for 'Yes' or 'No'\n                if re.search(r'\\bYes\\b', completion_text, re.IGNORECASE):\n                    responses[i] = 1\n                else:\n                    responses[i] = -1  # 'No' or default to -1 if neither 'Yes' nor 'No' is found\n\n                if verbose:\n                    print(f'Edge: {edge}, Response: {completion_text}, Vote: {responses[i]}')\n        else:\n            responses[i] = -1  # or any other indicator for missing data\n\n    return responses\n\n"}
{"type": "source_file", "path": "sup_contrast/losses.py", "content": "from __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass SupConLossGraph(nn.Module):\n    def __init__(self, clip_model, tokenizer, all_labels_geometric, all_labels_possessive, all_labels_semantic, rank,\n                 num_geom=15, num_poss=11, num_sem=24, base_temperature=0.07):\n        super(SupConLossGraph, self).__init__()\n\n        self.clip_model = clip_model\n        self.tokenizer = tokenizer\n        self.base_temperature = base_temperature\n\n        self.num_geom = num_geom\n        self.num_poss = num_poss\n        self.num_sem = num_sem\n\n        # Initialize all possible embeddings under each supercategory\n        self.all_embeddings_geometric, self.all_embeddings_possessive, self.all_embeddings_semantic = self._initialize_embeddings(\n            all_labels_geometric, all_labels_possessive, all_labels_semantic, rank)\n\n    def _initialize_embeddings(self, all_labels_geometric, all_labels_possessive, all_labels_semantic, rank):\n        # Geometric embeddings\n        queries_geom = [f\"{label}\" for label in all_labels_geometric]\n        inputs_geom = self.tokenizer(queries_geom, padding=True, return_tensors=\"pt\").to(rank)\n        with torch.no_grad():\n            embeddings_geometric = self.clip_model.module.get_text_features(**inputs_geom)\n        embeddings_geometric = F.normalize(embeddings_geometric, p=2, dim=1)\n\n        # Possessive embeddings\n        queries_poss = [f\"{label}\" for label in all_labels_possessive]\n        inputs_poss = self.tokenizer(queries_poss, padding=True, return_tensors=\"pt\").to(rank)\n        with torch.no_grad():\n            embeddings_possessive = self.clip_model.module.get_text_features(**inputs_poss)\n        embeddings_possessive = F.normalize(embeddings_possessive, p=2, dim=1)\n\n        # Semantic embeddings\n        queries_sem = [f\"{label}\" for label in all_labels_semantic]\n        inputs_sem = self.tokenizer(queries_sem, padding=True, return_tensors=\"pt\").to(rank)\n        with torch.no_grad():\n            embeddings_semantic = self.clip_model.module.get_text_features(**inputs_sem)\n        embeddings_semantic = F.normalize(embeddings_semantic, p=2, dim=1)\n\n        return embeddings_geometric, embeddings_possessive, embeddings_semantic\n\n    def forward(self, predicted_txt_embeds, curr_relation_ids, rank, temperature=0.07):\n        batch_size = len(predicted_txt_embeds)\n        mean_log_contrasts = 0.0\n\n        for bid in range(batch_size):\n            curr_features = predicted_txt_embeds[bid].to(rank)\n            curr_features = F.normalize(curr_features, p=2, dim=0)  # without normalization, the exponential will be numerically too large\n            relation_id = curr_relation_ids[bid]\n\n            # Retrieve the initialized embeddings based on the relation_id\n            if relation_id < self.num_geom:\n                positive_anchors = self.all_embeddings_geometric[relation_id].to(rank)\n                negative_anchors = self.all_embeddings_geometric[~torch.isin(torch.arange(self.num_geom), relation_id)].to(rank)\n            elif self.num_geom <= relation_id < self.num_geom + self.num_poss:\n                positive_anchors = self.all_embeddings_possessive[relation_id - self.num_geom].to(rank)\n                negative_anchors = self.all_embeddings_possessive[~torch.isin(torch.arange(self.num_poss), relation_id - self.num_geom)].to(rank)\n            else:\n                positive_anchors = self.all_embeddings_semantic[relation_id - self.num_geom - self.num_poss].to(rank)\n                negative_anchors = self.all_embeddings_semantic[~torch.isin(torch.arange(self.num_sem), relation_id - self.num_geom - self.num_poss)].to(rank)\n\n            num_negative = negative_anchors.shape[0]\n\n            contrast_numerator = torch.sum(curr_features * positive_anchors)\n            contrast_numerator = torch.exp(contrast_numerator / temperature)\n\n            contrast_denominator = curr_features @ negative_anchors.T\n            contrast_denominator = torch.sum(torch.exp(contrast_denominator / temperature), dim=0)\n\n            log_contrasts = -1 * torch.sum(torch.log(contrast_numerator + 1e-7) - torch.log(contrast_denominator + 1e-7))\n            # log_contrasts /= -1 * num_negative\n            mean_log_contrasts += log_contrasts\n\n        loss = (temperature / self.base_temperature) * mean_log_contrasts\n        return loss\n\n\nclass SupConLossHierar(nn.Module):\n    def __init__(self, temperature=0.07, contrast_mode='all',\n                 base_temperature=0.07):\n        super(SupConLossHierar, self).__init__()\n        self.temperature = temperature\n        self.contrast_mode = contrast_mode\n        self.base_temperature = base_temperature\n\n    def get_parent_label(self, labels):\n        parent_labels = torch.clone(labels)\n        parent_labels[labels < 15] = 0\n        parent_labels[(labels >= 15) & (labels < 26)] = 1\n        parent_labels[labels >= 26] = 2\n        return parent_labels\n\n    def forward(self, rank, features, labels=None, mask=None):\n        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n        it degenerates to SimCLR unsupervised loss:\n        https://arxiv.org/pdf/2002.05709.pdf\n\n        Args:\n            features: hidden vector of shape [bsz, n_views, ...].\n            labels: ground truth of shape [bsz].\n            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n                has the same class as sample i. Can be asymmetric.\n        Returns:\n            A loss scalar.\n        \"\"\"\n        if len(features.shape) < 3:\n            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n                             'at least 3 dimensions are required')\n        if len(features.shape) > 3:\n            features = features.view(features.shape[0], features.shape[1], -1)\n\n        batch_size = features.shape[0]\n        if labels is not None and mask is not None:\n            raise ValueError('Cannot define both `labels` and `mask`')\n        elif labels is None and mask is None:\n            mask = torch.eye(batch_size, dtype=torch.float32).to(rank)\n        elif labels is not None:\n            labels = labels.contiguous().view(-1, 1)\n            if labels.shape[0] != batch_size:\n                raise ValueError('Num of labels does not match num of features')\n\n            # compute parent labels\n            parent_labels = self.get_parent_label(labels)\n            mask_same_parent = torch.eq(parent_labels, parent_labels.T).float().to(rank)  # 1 if same parent label\n\n            mask = torch.eq(labels, labels.T).float().to(rank)  # 1 if same label, 0 otherwise\n        else:\n            mask = mask.float().to(rank)\n\n        contrast_count = features.shape[1]\n        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n        if self.contrast_mode == 'one':\n            anchor_feature = features[:, 0]\n            anchor_count = 1\n        elif self.contrast_mode == 'all':\n            anchor_feature = contrast_feature\n            anchor_count = contrast_count\n        else:\n            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n\n        # compute logits\n        anchor_dot_contrast = torch.div(\n            torch.matmul(anchor_feature, contrast_feature.T),\n            self.temperature)\n        # for numerical stability\n        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n        logits = anchor_dot_contrast - logits_max.detach()\n\n        # tile mask\n        mask = mask.repeat(anchor_count, contrast_count)\n        mask_same_parent = mask_same_parent.repeat(anchor_count, contrast_count)\n\n        # mask-out self-contrast cases\n        logits_mask = torch.scatter(\n            torch.ones_like(mask),  # create a tensor of all ones with the same shape of mask\n            1,\n            torch.arange(batch_size * anchor_count).view(-1, 1).to(rank),   # fill the diagonal with zeros\n            0\n        )\n        mask = mask * logits_mask   # no need to contrast with itself (diff must be 0)\n\n        # compute log_prob, where only different labels under the same parent class appear in the denominator\n        logits_mask = logits_mask * mask_same_parent\n        exp_logits = torch.exp(logits) * logits_mask    # for each sample in the row, logits_mask selects all its negative samples\n        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True) + 1e-7)\n\n        # compute mean of log-likelihood over positive\n        mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-7)  # mask selects all positive (augmented) samples\n\n        # loss\n        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n        loss = loss.view(anchor_count, batch_size).mean()\n\n        return loss\n\n\nclass SupConLoss(nn.Module):\n    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n    def __init__(self, temperature=0.07, contrast_mode='all',\n                 base_temperature=0.07):\n        super(SupConLoss, self).__init__()\n        self.temperature = temperature\n        self.contrast_mode = contrast_mode\n        self.base_temperature = base_temperature\n\n    def forward(self, rank, features, labels=None, mask=None):\n        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n        it degenerates to SimCLR unsupervised loss:\n        https://arxiv.org/pdf/2002.05709.pdf\n\n        Args:\n            features: hidden vector of shape [bsz, n_views, ...].\n            labels: ground truth of shape [bsz].\n            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n                has the same class as sample i. Can be asymmetric.\n        Returns:\n            A loss scalar.\n        \"\"\"\n        if len(features.shape) < 3:\n            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n                             'at least 3 dimensions are required')\n        if len(features.shape) > 3:\n            features = features.view(features.shape[0], features.shape[1], -1)\n\n        batch_size = features.shape[0]\n        if labels is not None and mask is not None:\n            raise ValueError('Cannot define both `labels` and `mask`')\n        elif labels is None and mask is None:\n            mask = torch.eye(batch_size, dtype=torch.float32).to(rank)\n        elif labels is not None:\n            labels = labels.contiguous().view(-1, 1)\n            if labels.shape[0] != batch_size:\n                raise ValueError('Num of labels does not match num of features')\n            mask = torch.eq(labels, labels.T).float().to(rank)\n        else:\n            mask = mask.float().to(rank)\n\n        contrast_count = features.shape[1]\n        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n        if self.contrast_mode == 'one':\n            anchor_feature = features[:, 0]\n            anchor_count = 1\n        elif self.contrast_mode == 'all':\n            anchor_feature = contrast_feature\n            anchor_count = contrast_count\n        else:\n            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n\n        # compute logits\n        anchor_dot_contrast = torch.div(\n            torch.matmul(anchor_feature, contrast_feature.T),\n            self.temperature)\n        # for numerical stability\n        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n        logits = anchor_dot_contrast - logits_max.detach()\n        # print('test1', torch.mean(logits))\n\n        # tile mask\n        mask = mask.repeat(anchor_count, contrast_count)\n        # mask-out self-contrast cases\n        logits_mask = torch.scatter(\n            torch.ones_like(mask),\n            1,\n            torch.arange(batch_size * anchor_count).view(-1, 1).to(rank),\n            0\n        )\n        mask = mask * logits_mask\n\n        # compute log_prob\n        exp_logits = torch.exp(logits) * logits_mask\n        # print('test1', torch.mean(exp_logits))\n        # print('test2', torch.mean(torch.log(exp_logits.sum(1, keepdim=True))) + 1e-7)\n        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True) + 1e-7)\n        # print('test3', torch.mean(log_prob))\n\n        # compute mean of log-likelihood over positive\n        mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-7)\n        # print('test4', torch.mean(mean_log_prob_pos))\n\n        # loss\n        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n        # print('test5', torch.mean(loss))\n        loss = loss.view(anchor_count, batch_size).mean()\n        # print('test6', loss)\n\n        return loss\n"}
{"type": "source_file", "path": "prepare_datasets.py", "content": "import torch\nimport numpy as np\nimport json\nimport yaml\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import Subset\n\nfrom utils import collate_fn\nfrom dataloader import *\nfrom dataset_utils import *\n\n\n# load hyper-parameters\ntry:\n    with open ('config.yaml', 'r') as file:\n        args = yaml.safe_load(file)\nexcept Exception as e:\n    print('Error reading the config file')\n\nif args['dataset']['dataset'] == 'vg':\n    train_dataset = PrepareVisualGenomeDataset(args['dataset']['annotation_train'])\n    test_dataset  = PrepareVisualGenomeDataset(args['dataset']['annotation_test'])\nelse:\n    train_dataset = PrepareOpenImageV6Dataset(args, '../datasets/open_image_v6/annotations/oiv6-adjust/vrd-train-anno.json')\n    test_dataset = PrepareOpenImageV6Dataset(args, '../datasets/open_image_v6/annotations/oiv6-adjust/vrd-test-anno.json')\n    val_dataset = PrepareOpenImageV6Dataset(args, '../datasets/open_image_v6/annotations/oiv6-adjust/vrd-val-anno.json')\n    val_subset_idx = torch.arange(len(val_dataset))\n    val_subset = Subset(val_dataset, val_subset_idx)\n    val_loader = torch.utils.data.DataLoader(val_subset,  batch_size=1, shuffle=False)\n\ntorch.manual_seed(0)\ntest_start, train_start = 0, 0\ntrain_subset_idx = torch.arange(len(train_dataset))[train_start:]\ntrain_subset = Subset(train_dataset, train_subset_idx)\ntest_subset_idx = torch.arange(len(test_dataset))[test_start:]\ntest_subset = Subset(test_dataset, test_subset_idx)\nprint('num of train, test:', len(train_subset), len(test_subset))\n\ntrain_loader = torch.utils.data.DataLoader(train_subset, batch_size=1, shuffle=False, collate_fn=collate_fn)\ntest_loader = torch.utils.data.DataLoader(test_subset,  batch_size=1, shuffle=False, collate_fn=collate_fn)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nworld_size = torch.cuda.device_count()\nprint('device', device, world_size)\n\n# find_zero_shot_triplet(args['dataset']['annotation_train'], args['dataset']['annotation_test'])\n\nimage_transform = transforms.Compose([transforms.ToTensor(),\n                                      transforms.Resize((args['models']['image_size'], args['models']['image_size']))])\n\nmodel_type = args['models']['depth_model_type']\ndepth_estimator = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n\nif args['dataset']['dataset'] == 'vg':\n    print('Start gathering testing annotations...')\n    prepare_data_offline(args, test_loader, device, args['dataset']['annotation_test'], image_transform, depth_estimator, test_start)\n    print('Finished gathering testing annotations...')\n\n    print('Start gathering training annotations...')\n    prepare_data_offline(args, train_loader, device, args['dataset']['annotation_train'], image_transform, depth_estimator, train_start)\n    print('Finished gathering training annotations...')\n\nelse:\n    prepare_depth_oiv6_offline(args, test_loader, device, depth_estimator)\n    prepare_depth_oiv6_offline(args, train_loader, device, depth_estimator)\n"}
{"type": "source_file", "path": "evaluator.py", "content": "import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom tqdm import tqdm\nimport math\nimport os\nimport concurrent.futures\nfrom functools import partial\n\nfrom utils import *\nfrom query_llm import *\nfrom dataset_utils import relation_by_super_class_int2str, object_class_int2str\n\n\nclass Evaluator:\n    \"\"\"\n    The class evaluate the model performance on Recall@k and mean Recall@k evaluation metrics on predicate classification tasks.\n    In our hierarchical relationship scheme, each edge has three predictions per direction under three disjoint super-categories.\n    Therefore, each directed edge outputs three individual candidates to be ranked in the top k most confident predictions instead of one.\n    \"\"\"\n    def __init__(self, args, num_classes, iou_thresh, top_k, max_cache_size=10000):\n        self.args = args\n        self.hierar = args['models']['hierarchical_pred']\n        self.top_k = top_k\n        self.num_classes = num_classes\n        self.iou_thresh = iou_thresh\n        self.num_connected_target = 0.0\n        self.motif_total = 0.0\n        self.motif_correct = 0.0\n        self.result_dict = {20: 0.0, 50: 0.0, 100: 0.0}\n        self.result_per_class = {k: torch.tensor([0.0 for i in range(self.num_classes)]) for k in self.top_k}\n        self.num_conn_target_per_class = torch.tensor([0.0 for i in range(self.num_classes)])\n        self.feature_size = args['models']['feature_size']\n        self.run_mode = args['training']['run_mode']\n\n        if args['dataset']['dataset'] == 'vg':\n            self.train_triplets = torch.load(args['dataset']['train_triplets'])\n            self.test_triplets = torch.load(args['dataset']['test_triplets'])\n            self.zero_shot_triplets = torch.load(args['dataset']['zero_shot_triplets'])\n            self.result_dict_zs = {20: 0.0, 50: 0.0, 100: 0.0}\n            self.result_per_class_zs = {k: torch.tensor([0.0 for i in range(self.num_classes)]) for k in self.top_k}\n            self.num_connected_target_zs = 0.0\n            self.num_conn_target_per_class_zs = torch.tensor([0.0 for i in range(self.num_classes)])\n        elif args['dataset']['dataset'] == 'oiv6':\n            self.result_per_class_ap = torch.tensor([0.0 for i in range(self.num_classes)])\n            self.result_per_class_ap_union = torch.tensor([0.0 for i in range(self.num_classes)])\n            self.num_conn_target_per_class_ap = torch.tensor([0.0 for i in range(self.num_classes)])\n\n        self.which_in_batch = None\n        self.which_in_batch_target = None\n        # self.connected_pred = None\n        self.confidence = None\n        self.connectivity = None\n        self.relation_pred = None\n        self.relation_target = None\n\n        self.subject_cat_pred = None\n        self.object_cat_pred = None\n        self.subject_cat_target = None\n        self.object_cat_target = None\n\n        self.subject_bbox_pred = None\n        self.object_bbox_pred = None\n        self.subject_bbox_target = None\n        self.object_bbox_target = None\n\n        self.annotation_paths = None\n        self.cache_hits = 0\n        self.total_cache_queries = 0\n        self.max_cache_size = max_cache_size\n        self.cache = EdgeCache(max_cache_size=max_cache_size)\n        self.image_cache = ImageCache(image_size=self.args['models']['image_size'], feature_size=self.args['models']['feature_size'])\n        self.dict_relation_names = relation_by_super_class_int2str()\n        self.dict_object_names = object_class_int2str()\n\n        if self.args['models']['llm_model'] == 'gpt4v':\n            self.commonsense_aligned_triplets = torch.load('triplets/commonsense_aligned_triplets_gpt4v.pt') if args['training']['run_mode'] == 'train_cs' or self.run_mode == 'eval_cs' else None\n            self.commonsense_violated_triplets = torch.load('triplets/commonsense_violated_triplets_gpt4v.pt') if args['training']['run_mode'] == 'train_cs' or self.run_mode == 'eval_cs' else None\n        else:\n            self.commonsense_aligned_triplets = torch.load('triplets/commonsense_aligned_triplets.pt') if args['training']['run_mode'] == 'train_cs' or self.run_mode == 'eval_cs' else None\n            self.commonsense_violated_triplets = torch.load('triplets/commonsense_violated_triplets.pt') if args['training']['run_mode'] == 'train_cs' or self.run_mode == 'eval_cs' else None\n\n\n    def iou(self, bbox_target, bbox_pred):\n        mask_pred = torch.zeros(self.feature_size, self.feature_size)\n        mask_pred[int(bbox_pred[2]):int(bbox_pred[3]), int(bbox_pred[0]):int(bbox_pred[1])] = 1\n        mask_target = torch.zeros(self.feature_size, self.feature_size)\n        mask_target[int(bbox_target[2]):int(bbox_target[3]), int(bbox_target[0]):int(bbox_target[1])] = 1\n        intersect = torch.sum(torch.logical_and(mask_target, mask_pred))\n        union = torch.sum(torch.logical_or(mask_target, mask_pred))\n        if union == 0:\n            return 0\n        else:\n            return float(intersect) / float(union)\n\n\n    def iou_union(self, bbox_pred1, bbox_pred2, bbox_target1, bbox_target2):\n        mask_pred1 = torch.zeros(self.feature_size, self.feature_size)\n        mask_pred1[int(bbox_pred1[2]):int(bbox_pred1[3]), int(bbox_pred1[0]):int(bbox_pred1[1])] = 1\n        mask_pred2 = torch.zeros(self.feature_size, self.feature_size)\n        mask_pred2[int(bbox_pred2[2]):int(bbox_pred2[3]), int(bbox_pred2[0]):int(bbox_pred2[1])] = 1\n        mask_pred = torch.logical_or(mask_pred1, mask_pred2)\n\n        mask_target1 = torch.zeros(self.feature_size, self.feature_size)\n        mask_target1[int(bbox_target1[2]):int(bbox_target1[3]), int(bbox_target1[0]):int(bbox_target1[1])] = 1\n        mask_target2 = torch.zeros(self.feature_size, self.feature_size)\n        mask_target2[int(bbox_target2[2]):int(bbox_target2[3]), int(bbox_target2[0]):int(bbox_target2[1])] = 1\n        mask_target = torch.logical_or(mask_target1, mask_target2)\n\n        intersect = torch.sum(torch.logical_and(mask_target, mask_pred))\n        union = torch.sum(torch.logical_or(mask_target, mask_pred))\n        if union == 0:\n            return 0\n        else:\n            return float(intersect) / float(union)\n\n\n    def accumulate(self, which_in_batch, relation_pred, relation_target, super_relation_pred, connectivity,\n                   subject_cat_pred, object_cat_pred, subject_cat_target, object_cat_target,\n                   subject_bbox_pred, object_bbox_pred, subject_bbox_target, object_bbox_target, iou_mask,\n                   predcls=True, cat_subject_confidence=None, cat_object_confidence=None, height=None, width=None):\n\n        if self.relation_pred is None:\n            if not self.hierar:     # flat relationship prediction\n                self.which_in_batch = which_in_batch\n                self.connectivity = connectivity\n\n                self.confidence = torch.max(relation_pred, dim=1)[0] #+ connectivity\n                if not predcls:\n                    ins_pair_confidence = cat_subject_confidence + cat_object_confidence\n                    self.confidence += ins_pair_confidence\n                self.confidence[~iou_mask] = -math.inf\n\n                self.relation_pred = torch.argmax(relation_pred, dim=1)\n                self.subject_cat_pred = subject_cat_pred\n                self.object_cat_pred = object_cat_pred\n                self.subject_bbox_pred = subject_bbox_pred\n                self.object_bbox_pred = object_bbox_pred\n\n                if predcls:\n                    self.which_in_batch_target = which_in_batch\n                    self.relation_target = relation_target\n\n                    self.subject_cat_target = subject_cat_target\n                    self.object_cat_target = object_cat_target\n                    self.subject_bbox_target = subject_bbox_target\n                    self.object_bbox_target = object_bbox_target\n\n                if self.run_mode == 'train_cs' or self.run_mode == 'eval_cs':\n                    triplets = torch.hstack((self.subject_cat_pred.unsqueeze(1), self.relation_pred.unsqueeze(1), self.object_cat_pred.unsqueeze(1)))\n                    is_in_no_dict = torch.tensor([tuple(triplets[i].cpu().tolist()) in self.commonsense_violated_triplets for i in range(len(triplets))], device=self.confidence.device)\n                    not_in_yes_dict = torch.tensor([tuple(triplets[i].cpu().tolist()) not in self.commonsense_aligned_triplets for i in range(len(triplets))], device=self.confidence.device)\n                    self.confidence[not_in_yes_dict] = -math.inf\n                    self.confidence[is_in_no_dict] = -math.inf\n\n            else:\n                self.which_in_batch = which_in_batch.repeat(3)\n                self.connectivity = connectivity.repeat(3)\n\n                self.confidence = torch.hstack((torch.max(relation_pred[:, :self.args['models']['num_geometric']], dim=1)[0],\n                                                torch.max(relation_pred[:, self.args['models']['num_geometric']:\n                                                                           self.args['models']['num_geometric'] + self.args['models']['num_possessive']], dim=1)[0],\n                                                torch.max(relation_pred[:, self.args['models']['num_geometric'] + self.args['models']['num_possessive']:], dim=1)[0]))\n                if not predcls:\n                    ins_pair_confidence = cat_subject_confidence + cat_object_confidence\n                    self.confidence += ins_pair_confidence.repeat(3)\n                iou_mask = iou_mask.repeat(3)\n                self.confidence[~iou_mask] = -math.inf\n\n                self.relation_pred = torch.hstack((torch.argmax(relation_pred[:, :self.args['models']['num_geometric']], dim=1),\n                                                   torch.argmax(relation_pred[:, self.args['models']['num_geometric']:self.args['models']['num_geometric']+self.args['models']['num_possessive']], dim=1)\n                                                   + self.args['models']['num_geometric'],\n                                                   torch.argmax(relation_pred[:, self.args['models']['num_geometric']+self.args['models']['num_possessive']:], dim=1)\n                                                   + self.args['models']['num_geometric'] + self.args['models']['num_possessive']))\n\n                self.subject_cat_pred = subject_cat_pred.repeat(3)\n                self.object_cat_pred = object_cat_pred.repeat(3)\n                self.subject_bbox_pred = subject_bbox_pred.repeat(3, 1)\n                self.object_bbox_pred = object_bbox_pred.repeat(3, 1)\n\n                if predcls:\n                    self.which_in_batch_target = which_in_batch\n                    self.relation_target = relation_target\n                    self.subject_cat_target = subject_cat_target\n                    self.object_cat_target = object_cat_target\n                    self.subject_bbox_target = subject_bbox_target\n                    self.object_bbox_target = object_bbox_target\n\n                if self.run_mode == 'train_cs' or self.run_mode == 'eval_cs':\n                    triplets = torch.hstack((self.subject_cat_pred.unsqueeze(1), self.relation_pred.unsqueeze(1), self.object_cat_pred.unsqueeze(1)))\n                    is_in_no_dict = torch.tensor([tuple(triplets[i].cpu().tolist()) in self.commonsense_violated_triplets for i in range(len(triplets))], device=self.confidence.device)\n                    not_in_yes_dict = torch.tensor([tuple(triplets[i].cpu().tolist()) not in self.commonsense_aligned_triplets for i in range(len(triplets))], device=self.confidence.device)\n                    self.confidence[is_in_no_dict] = -math.inf\n                    self.confidence[not_in_yes_dict] = -math.inf\n\n        else:\n            if not self.hierar:     # flat relationship prediction\n                self.which_in_batch = torch.hstack((self.which_in_batch, which_in_batch))\n                confidence = torch.max(relation_pred, dim=1)[0] #+ connectivity\n                if not predcls:\n                    ins_pair_confidence = cat_subject_confidence + cat_object_confidence\n                    confidence += ins_pair_confidence\n                confidence[~iou_mask] = -math.inf\n\n                relation_pred = torch.argmax(relation_pred, dim=1)\n                self.relation_pred = torch.hstack((self.relation_pred, relation_pred))\n                self.subject_cat_pred = torch.hstack((self.subject_cat_pred, subject_cat_pred))\n                self.object_cat_pred = torch.hstack((self.object_cat_pred, object_cat_pred))\n                self.subject_bbox_pred = torch.vstack((self.subject_bbox_pred, subject_bbox_pred))\n                self.object_bbox_pred = torch.vstack((self.object_bbox_pred, object_bbox_pred))\n\n                if predcls:\n                    self.which_in_batch_target = torch.hstack((self.which_in_batch_target, which_in_batch))\n                    self.relation_target = torch.hstack((self.relation_target, relation_target))\n                    self.subject_cat_target = torch.hstack((self.subject_cat_target, subject_cat_target))\n                    self.object_cat_target = torch.hstack((self.object_cat_target, object_cat_target))\n                    self.subject_bbox_target = torch.vstack((self.subject_bbox_target, subject_bbox_target))\n                    self.object_bbox_target = torch.vstack((self.object_bbox_target, object_bbox_target))\n\n                if self.run_mode == 'train_cs' or self.run_mode == 'eval_cs':\n                    triplets = torch.hstack((subject_cat_pred.unsqueeze(1), relation_pred.unsqueeze(1), object_cat_pred.unsqueeze(1)))\n                    is_in_no_dict = torch.tensor([tuple(triplets[i].cpu().tolist()) in self.commonsense_violated_triplets for i in range(len(triplets))], device=self.confidence.device)\n                    not_in_yes_dict = torch.tensor([tuple(triplets[i].cpu().tolist()) not in self.commonsense_aligned_triplets for i in range(len(triplets))], device=self.confidence.device)\n                    confidence[is_in_no_dict] = -math.inf\n                    confidence[not_in_yes_dict] = -math.inf\n\n                self.confidence = torch.hstack((self.confidence, confidence))\n                self.connectivity = torch.hstack((self.connectivity, connectivity))\n\n            else:\n                self.which_in_batch = torch.hstack((self.which_in_batch, which_in_batch.repeat(3)))\n\n                relation_pred_candid = torch.hstack((torch.argmax(relation_pred[:, :self.args['models']['num_geometric']], dim=1),\n                                                     torch.argmax(relation_pred[:, self.args['models']['num_geometric']:self.args['models']['num_geometric']+self.args['models']['num_possessive']], dim=1)\n                                                     + self.args['models']['num_geometric'],\n                                                     torch.argmax(relation_pred[:, self.args['models']['num_geometric']+self.args['models']['num_possessive']:], dim=1)\n                                                     + self.args['models']['num_geometric'] + self.args['models']['num_possessive']))\n                self.relation_pred = torch.hstack((self.relation_pred, relation_pred_candid))\n                self.subject_cat_pred = torch.hstack((self.subject_cat_pred, subject_cat_pred.repeat(3)))\n                self.object_cat_pred = torch.hstack((self.object_cat_pred, object_cat_pred.repeat(3)))\n                self.subject_bbox_pred = torch.vstack((self.subject_bbox_pred, subject_bbox_pred.repeat(3, 1)))\n                self.object_bbox_pred = torch.vstack((self.object_bbox_pred, object_bbox_pred.repeat(3, 1)))\n\n                confidence = torch.hstack((torch.max(relation_pred[:, :self.args['models']['num_geometric']], dim=1)[0],\n                                           torch.max(relation_pred[:, self.args['models']['num_geometric']:self.args['models']['num_geometric'] + self.args['models']['num_possessive']], dim=1)[0],\n                                           torch.max(relation_pred[:, self.args['models']['num_geometric'] + self.args['models']['num_possessive']:], dim=1)[0]))\n                if not predcls:\n                    ins_pair_confidence = cat_subject_confidence + cat_object_confidence\n                    confidence += ins_pair_confidence.repeat(3)\n                iou_mask = iou_mask.repeat(3)\n                confidence[~iou_mask] = -math.inf\n\n                if predcls:\n                    self.which_in_batch_target = torch.hstack((self.which_in_batch_target, which_in_batch))\n                    self.relation_target = torch.hstack((self.relation_target, relation_target))\n                    self.subject_cat_target = torch.hstack((self.subject_cat_target, subject_cat_target))\n                    self.object_cat_target = torch.hstack((self.object_cat_target, object_cat_target))\n                    self.subject_bbox_target = torch.vstack((self.subject_bbox_target, subject_bbox_target))\n                    self.object_bbox_target = torch.vstack((self.object_bbox_target, object_bbox_target))\n\n                if self.run_mode == 'train_cs' or self.run_mode == 'eval_cs':\n                    triplets = torch.hstack((subject_cat_pred.repeat(3).unsqueeze(1), relation_pred_candid.unsqueeze(1), object_cat_pred.repeat(3).unsqueeze(1)))\n                    is_in_no_dict = torch.tensor([tuple(triplets[i].cpu().tolist()) in self.commonsense_violated_triplets for i in range(len(triplets))], device=self.confidence.device)\n                    not_in_yes_dict = torch.tensor([tuple(triplets[i].cpu().tolist()) not in self.commonsense_aligned_triplets for i in range(len(triplets))], device=self.confidence.device)\n                    confidence[is_in_no_dict] = -math.inf\n                    confidence[not_in_yes_dict] = -math.inf\n\n                self.confidence = torch.hstack((self.confidence, confidence))\n                self.connectivity = torch.hstack((self.connectivity, connectivity.repeat(3)))\n\n\n    def accumulate_target(self, relation_target, subject_cat_target, object_cat_target, subject_bbox_target, object_bbox_target):\n        self.relation_target = relation_target\n        self.subject_cat_target = subject_cat_target\n        self.object_cat_target = object_cat_target\n        self.subject_bbox_target = subject_bbox_target\n        self.object_bbox_target = object_bbox_target\n\n\n    def compute(self, per_class=False, predcls=True):\n        \"\"\"\n        A ground truth predicate is considered to match a hypothesized relationship iff the predicted relationship is correct,\n        the subject and object labels match, and the bounding boxes associated with the subject and object both have IOU>0.5 with the ground-truth boxes.\n        \"\"\"\n\n        \"\"\"\n        We calculate the recall scores for each image in a moving average fashion across the test dataset.\n        Otherwise, uncomment the following two lines and select batch size = 1 in the config file to view the recall on each individual image.\n        \"\"\"\n\n        recall_k_zs, recall_k_per_class_zs, mean_recall_k_zs = None, None, None\n        self.confidence += self.connectivity\n\n        for image in torch.unique(self.which_in_batch):  # image-wise\n            curr_image = self.which_in_batch == image\n            if self.which_in_batch_target is None:\n                curr_image_target = image\n                if self.relation_target[curr_image_target] is None:\n                    continue\n            else:\n                curr_image_target = self.which_in_batch_target == image\n            num_relation_pred = len(self.relation_pred[curr_image])\n            curr_confidence = self.confidence[curr_image]\n            sorted_inds = torch.argsort(curr_confidence, dim=0, descending=True)\n\n            for i in range(len(self.relation_target[curr_image_target])):\n                if self.relation_target[curr_image_target][i] == -1:  # if target is not connected\n                    continue\n                if self.args['dataset']['dataset'] == 'vg':\n                    curr_triplet = str(self.subject_cat_target[curr_image_target][i].item()) + '_' + str(self.relation_target[curr_image_target][i].item()) \\\n                                   + '_' + str(self.object_cat_target[curr_image_target][i].item())\n\n                # search in top k most confident predictions in each image\n                num_target = torch.sum(self.relation_target[curr_image_target] != -1)\n                this_k = min(self.top_k[-1], num_relation_pred)  # 100\n                keep_inds = sorted_inds[:this_k]\n\n                found = False   # found if any one of the three sub-models predict correctly\n                for j in range(len(keep_inds)):     # for each target <subject, relation, object> triple, find any match in the top k confident predictions\n                    if predcls:\n                        label_condition = (self.subject_cat_target[curr_image_target][i] == self.subject_cat_pred[curr_image][keep_inds][j] and\n                                           self.object_cat_target[curr_image_target][i] == self.object_cat_pred[curr_image][keep_inds][j])\n                    else:\n                        label_condition = (compare_object_cat(self.subject_cat_target[curr_image_target][i], self.subject_cat_pred[curr_image][keep_inds][j]) and\n                                           compare_object_cat(self.object_cat_target[curr_image_target][i], self.object_cat_pred[curr_image][keep_inds][j]))\n                    if label_condition:\n                        sub_iou = self.iou(self.subject_bbox_target[curr_image_target][i], self.subject_bbox_pred[curr_image][keep_inds][j])\n                        obj_iou = self.iou(self.object_bbox_target[curr_image_target][i], self.object_bbox_pred[curr_image][keep_inds][j])\n\n                        if sub_iou >= self.iou_thresh and obj_iou >= self.iou_thresh:\n                            if self.relation_target[curr_image_target][i] == self.relation_pred[curr_image][keep_inds][j]:\n                                for k in self.top_k:\n                                    if j >= k:\n                                        continue\n                                    self.result_dict[k] += 1.0\n                                    if per_class:\n                                        self.result_per_class[k][self.relation_target[curr_image_target][i]] += 1.0\n\n                                    # if zero shot\n                                    if self.args['dataset']['dataset'] == 'vg':\n                                        if curr_triplet in self.zero_shot_triplets:\n                                            assert curr_triplet not in self.train_triplets\n                                            self.result_dict_zs[k] += 1.0\n                                            if per_class:\n                                                self.result_per_class_zs[k][self.relation_target[curr_image_target][i]] += 1.0\n                                found = True\n                            if found:\n                                break\n\n                self.num_connected_target += 1.0\n                self.num_conn_target_per_class[self.relation_target[curr_image_target][i]] += 1.0\n                # if zero shot\n                if self.args['dataset']['dataset'] == 'vg':\n                    if curr_triplet in self.zero_shot_triplets:\n                        self.num_connected_target_zs += 1.0\n                        self.num_conn_target_per_class_zs[self.relation_target[curr_image_target][i]] += 1.0\n\n        recall_k = [self.result_dict[k] / max(self.num_connected_target, 1e-3) for k in self.top_k]\n        recall_k_per_class = [self.result_per_class[k] / self.num_conn_target_per_class for k in self.top_k]\n        mean_recall_k = [torch.nanmean(r) for r in recall_k_per_class]\n\n        if self.args['dataset']['dataset'] == 'vg':\n            recall_k_zs = [self.result_dict_zs[k] / max(self.num_connected_target_zs, 1e-3) for k in self.top_k]\n            recall_k_per_class_zs = [self.result_per_class_zs[k] / self.num_conn_target_per_class_zs for k in self.top_k]\n            mean_recall_k_zs = [torch.nanmean(r) for r in recall_k_per_class_zs]\n\n        return recall_k, recall_k_per_class, mean_recall_k, recall_k_zs, recall_k_per_class_zs, mean_recall_k_zs\n\n\n    def load_annotation_paths(self, annot_path):\n        self.annotation_paths = None    # reset\n        self.annotation_paths = annot_path\n\n\n    def _get_related_top_k_predictions(self, image, top_k):\n        curr_image = self.which_in_batch == image\n        curr_confidence = self.confidence[curr_image]\n        sorted_inds = torch.argsort(curr_confidence, dim=0, descending=True)\n\n        curr_predictions = []\n        curr_image_graph = []\n\n        if self.which_in_batch_target is None:\n            curr_image_target = image\n            if self.relation_target[curr_image_target] is None:\n                return\n        else:\n            curr_image_target = self.which_in_batch_target == image\n\n        for i in range(0, len(self.subject_cat_target[curr_image_target])):\n            if self.relation_target[curr_image_target][i] == -1:  # if target is not connected\n                continue\n            if len(curr_image_graph) >= 15:     # enforce efficiency\n                break\n\n            for j in range(min(top_k, len(sorted_inds))):\n                ind = sorted_inds[j]\n\n                subject_id_pred = self.subject_cat_pred[curr_image][ind].item()\n                object_id_pred = self.object_cat_pred[curr_image][ind].item()\n\n                # check if the predicted subject or object matches the target\n                if (self.subject_cat_target[curr_image_target][i] == subject_id_pred and torch.sum(torch.abs(self.subject_bbox_target[curr_image_target][i] - self.subject_bbox_pred[curr_image][ind])) == 0) \\\n                        or (self.object_cat_target[curr_image_target][i] == object_id_pred and torch.sum(torch.abs(self.object_bbox_target[curr_image_target][i] - self.object_bbox_pred[curr_image][ind])) == 0):\n\n                    relation_id = self.relation_pred[curr_image][ind].item()\n                    string = self.dict_object_names[subject_id_pred] + ' ' + self.dict_relation_names[relation_id] + ' ' + self.dict_object_names[object_id_pred]\n                    if string not in curr_predictions:\n                        # filter the edge by commonsense\n                        edge = [self.subject_bbox_pred[curr_image][ind].cpu().tolist(), relation_id, self.object_bbox_pred[curr_image][ind].cpu().tolist(), self.confidence[curr_image][ind].item(), j]\n                        curr_image_graph.append(edge)\n                        curr_predictions.append(string)\n\n                if len(curr_image_graph) >= 10:  # enforce efficiency\n                    break\n\n        if len(curr_image_graph) > 0:\n            if self.args['models']['llm_model'] == 'gpt4v':\n                responses, cache_hits = batch_query_openai_gpt(curr_predictions, self.cache, cache_hits=self.cache_hits, annot_name=self.annotation_paths[image],\n                                                               sub_bbox=self.subject_bbox_pred[curr_image], obj_bbox=self.object_bbox_pred[curr_image], image_cache=self.image_cache, image_dir=self.args['dataset']['image_dir'])\n            else:  # gpt3.5\n                responses, cache_hits = batch_query_openai_gpt(curr_predictions, self.cache, cache_hits=self.cache_hits)\n\n            # calculate cache hit percentage\n            self.cache_hits = cache_hits\n            self.total_cache_queries += len(curr_predictions)\n\n            valid_curr_image_graph = []\n            invalid_curr_image_graph = []\n            for i, response in enumerate(responses):\n                if response == 1:\n                    valid_curr_image_graph.append(curr_image_graph[i])\n                else:\n                    invalid_curr_image_graph.append(curr_image_graph[i])\n\n            annot_name = self.annotation_paths[image][:-16] + '_pseudo_annotations.pkl'\n            if self.args['models']['llm_model'] == 'gpt4v':\n                valid_annot_path = os.path.join(self.args['dataset']['annot_dir'], 'cs_aligned_top' + str(top_k) + '_gpt4v', annot_name)\n                invalid_annot_path = os.path.join(self.args['dataset']['annot_dir'], 'cs_violated_top' + str(top_k) + '_gpt4v', annot_name)\n            else:\n                valid_annot_path = os.path.join(self.args['dataset']['annot_dir'], 'cs_aligned_top' + str(top_k)+ '_gpt3p5_temp', annot_name)\n                invalid_annot_path = os.path.join(self.args['dataset']['annot_dir'], 'cs_violated_top' + str(top_k)+ '_gpt3p5_temp', annot_name)\n            torch.save(valid_curr_image_graph, valid_annot_path)\n            torch.save(invalid_curr_image_graph, invalid_annot_path)\n            # print(\"Saving annotations\", annot_name)\n\n        return curr_predictions, curr_image_graph\n\n\n    def get_related_top_k_predictions_parallel(self, top_k, save_to_annot=True):\n        self.dict_relation_names = relation_by_super_class_int2str()\n        self.dict_object_names = object_class_int2str()\n\n        with concurrent.futures.ThreadPoolExecutor() as executor:\n            results = list(executor.map(lambda image: self._get_related_top_k_predictions(image, top_k), torch.unique(self.which_in_batch)))\n\n        cache_hit_percentage = (self.cache_hits / self.total_cache_queries) * 100 if self.total_cache_queries > 0 else 0\n\n        if save_to_annot:\n            top_k_predictions = [item[0] for item in results]\n            top_k_image_graphs = [item[1] for item in results]\n            return top_k_predictions, top_k_image_graphs, cache_hit_percentage\n\n\n    def save_visualization_results(self, annot_path, triplets, heights, widths, images, image_depth, bboxes, categories, batch_count, top_k):\n        dict_relation_names = relation_by_super_class_int2str()\n        dict_object_names = object_class_int2str()\n        if self.which_in_batch is None:\n            return\n\n        for image in torch.unique(self.which_in_batch):  # image-wise\n            curr_image = self.which_in_batch == image\n            curr_confidence = self.confidence[curr_image]\n            sorted_inds = torch.argsort(curr_confidence, dim=0, descending=True)\n\n            # select the top k predictions\n            this_k = min(top_k, len(self.relation_pred[curr_image]))\n            keep_inds = sorted_inds[:this_k]\n\n            curr_image_graph = []\n\n            for ind in keep_inds:\n                subject_id = self.subject_cat_pred[curr_image][ind].item()\n                relation_id = self.relation_pred[curr_image][ind].item()\n                object_id = self.object_cat_pred[curr_image][ind].item()\n\n                subject_bbox = self.subject_bbox_pred[curr_image][ind].cpu() / self.feature_size\n                object_bbox = self.object_bbox_pred[curr_image][ind].cpu() / self.feature_size\n                height, width = heights[image], widths[image]\n                subject_bbox[:2] *= height\n                subject_bbox[2:] *= width\n                object_bbox[:2] *= height\n                object_bbox[2:] *= width\n                subject_bbox = subject_bbox.ceil().int()\n                object_bbox = object_bbox.ceil().int()\n\n                edge = {'edge': dict_object_names[subject_id] + ' ' + dict_relation_names[relation_id] + ' ' + dict_object_names[object_id],\n                        'subject_id': subject_id,\n                        'relation_id': relation_id,\n                        'object_id': object_id,\n                        'bbox_sub': subject_bbox.tolist(),\n                        'bbox_obj': object_bbox.tolist()}\n                curr_image_graph.append(edge)\n\n            vis_results = {'predicted_graph': curr_image_graph,\n                           'image_path': annot_path[image],\n                           'target_graph': triplets[image],\n                           'bboxes': bboxes[image],\n                           'categories': categories[image],\n                           'image': images[image],\n                           'image_depth': image_depth[image],\n                           'height': heights[image],\n                           'width': widths[image]}\n            # print('vis_results', vis_results)\n\n            annot_name = str(batch_count) + '_vis_results.pkl'\n            annot_path = os.path.join('results/visualization_results/cs', annot_name)\n            print('annot_path', annot_path)\n            torch.save(vis_results, annot_path)\n\n\n    def compute_precision(self):\n        for image in torch.unique(self.which_in_batch):  # image-wise\n            curr_image = self.which_in_batch == image\n            num_relation_pred = len(self.relation_pred[curr_image])\n            curr_confidence = self.confidence[curr_image]\n            sorted_inds = torch.argsort(curr_confidence, dim=0, descending=True)\n            this_k = min(20, num_relation_pred)  # 100\n            keep_inds = sorted_inds[:this_k]\n\n            for i in range(len(self.relation_pred[curr_image][keep_inds])):\n                found = False  # found if any one of the three sub-models predict correctly\n                found_union = False\n                for j in range(len(self.relation_target[curr_image])):\n                    if self.relation_target[curr_image][j] == -1:  # if target is not connected\n                        continue\n\n                    if (self.subject_cat_pred[curr_image][keep_inds][i] == self.subject_cat_target[curr_image][j]\n                            and self.object_cat_pred[curr_image][keep_inds][i] == self.object_cat_target[curr_image][j]):\n\n                        sub_iou = self.iou(self.subject_bbox_pred[curr_image][keep_inds][i], self.subject_bbox_target[curr_image][j])\n                        obj_iou = self.iou(self.object_bbox_pred[curr_image][keep_inds][i], self.object_bbox_target[curr_image][j])\n                        union_iou = self.iou_union(self.subject_bbox_pred[curr_image][keep_inds][i], self.object_bbox_pred[curr_image][keep_inds][i],\n                                                   self.subject_bbox_target[curr_image][j], self.object_bbox_target[curr_image][j])\n\n                        if self.relation_pred[curr_image][keep_inds][i] == self.relation_target[curr_image][j]:\n                            if sub_iou >= self.iou_thresh and obj_iou >= self.iou_thresh and found == False:\n                                self.result_per_class_ap[self.relation_pred[curr_image][keep_inds][i]] += 1.0\n                                found = True\n                            if union_iou >= self.iou_thresh and found_union == False:\n                                self.result_per_class_ap_union[self.relation_pred[curr_image][keep_inds][i]] += 1.0\n                                found_union = True\n\n                        if found and found_union:\n                            break\n\n                self.num_conn_target_per_class_ap[self.relation_pred[curr_image][keep_inds][i]] += 1.0\n\n        weight = get_weight_oiv6()\n        precision_per_class = self.result_per_class_ap / self.num_conn_target_per_class_ap\n        not_nan = torch.logical_not(torch.isnan(precision_per_class))\n        weighted_mean_precision = torch.nansum(precision_per_class * weight) / torch.sum(weight[not_nan])\n\n        precision_per_class_union = self.result_per_class_ap_union / self.num_conn_target_per_class_ap\n        weighted_mean_precision_union = torch.nansum(precision_per_class_union * weight) / torch.sum(weight[not_nan])\n        return weighted_mean_precision, weighted_mean_precision_union\n\n    def clear_data(self):\n        self.which_in_batch = None\n        self.confidence = None\n        self.connectivity = None\n        self.relation_pred = None\n        self.relation_target = None\n\n        self.subject_cat_pred = None\n        self.object_cat_pred = None\n        self.subject_cat_target = None\n        self.object_cat_target = None\n\n        self.subject_bbox_pred = None\n        self.object_bbox_pred = None\n        self.subject_bbox_target = None\n        self.object_bbox_target = None\n\n    def clear_gpt_cache(self):\n        self.cache = {}\n\n\nclass Evaluator_Top3:\n    \"\"\"\n    The class evaluate the model performance on Recall@k^{*} and mean Recall@k^{*} evaluation metrics on predicate classification tasks.\n    If any of the three super-category output heads correctly predicts the relationship, we score it as a match.\n    Top3 represents three argmax predicate from three disjoint super-categories, instead of the top 3 predicates under a flat classification.\n    \"\"\"\n    def __init__(self, args, num_classes, iou_thresh, top_k):\n        self.args = args\n        self.top_k = top_k\n        self.num_classes = num_classes\n        self.iou_thresh = iou_thresh\n        self.num_connected_target = 0.0\n        self.motif_total = 0.0\n        self.motif_correct = 0.0\n        self.result_dict = {20: 0.0, 50: 0.0, 100: 0.0}\n        self.result_dict_top1 = {20: 0.0, 50: 0.0, 100: 0.0}\n        self.result_per_class = {k: torch.tensor([0.0 for i in range(self.num_classes)]) for k in self.top_k}\n        self.result_per_class_top1 = {k: torch.tensor([0.0 for i in range(self.num_classes)]) for k in self.top_k}\n        self.num_conn_target_per_class = torch.tensor([0.0 for i in range(self.num_classes)])\n        self.feature_size = args['models']['feature_size']\n\n        self.which_in_batch = None\n        self.confidence = None\n        self.connectivity = None\n        self.relation_pred = None\n        self.relation_target = None\n        self.super_relation_pred = None\n\n        self.subject_cat_pred = None\n        self.object_cat_pred = None\n        self.subject_cat_target = None\n        self.object_cat_target = None\n\n        self.subject_bbox_pred = None\n        self.object_bbox_pred = None\n        self.subject_bbox_target = None\n        self.object_bbox_target = None\n\n    def iou(self, bbox_target, bbox_pred):\n        mask_pred = torch.zeros(self.feature_size, self.feature_size)\n        mask_pred[int(bbox_pred[2]):int(bbox_pred[3]), int(bbox_pred[0]):int(bbox_pred[1])] = 1\n        mask_target = torch.zeros(self.feature_size, self.feature_size)\n        mask_target[int(bbox_target[2]):int(bbox_target[3]), int(bbox_target[0]):int(bbox_target[1])] = 1\n        intersect = torch.sum(torch.logical_and(mask_target, mask_pred))\n        union = torch.sum(torch.logical_or(mask_target, mask_pred))\n        if union == 0:\n            return 0\n        else:\n            return float(intersect) / float(union)\n\n    def accumulate(self, which_in_batch, relation_pred, relation_target, super_relation_pred, connectivity,\n                   subject_cat_pred, object_cat_pred, subject_cat_target, object_cat_target,\n                   subject_bbox_pred, object_bbox_pred, subject_bbox_target, object_bbox_target, iou_mask):  # size (batch_size, num_relations_classes), (num_relations_classes)\n\n        if self.relation_pred is None:\n            self.which_in_batch = which_in_batch\n            self.connectivity = connectivity\n            self.confidence = torch.max(torch.vstack((torch.max(relation_pred[:, :self.args['models']['num_geometric']], dim=1)[0],\n                                                      torch.max(relation_pred[:, self.args['models']['num_geometric']:self.args['models']['num_geometric']+self.args['models']['num_possessive']], dim=1)[0],\n                                                      torch.max(relation_pred[:, self.args['models']['num_geometric']+self.args['models']['num_possessive']:], dim=1)[0])), dim=0)[0]  # in log space, [0] to take values\n            self.confidence[~iou_mask] = -math.inf\n            self.relation_pred = relation_pred\n            self.relation_target = relation_target\n            self.super_relation_pred = super_relation_pred\n\n            self.subject_cat_pred = subject_cat_pred\n            self.object_cat_pred = object_cat_pred\n            self.subject_cat_target = subject_cat_target\n            self.object_cat_target = object_cat_target\n\n            self.subject_bbox_pred = subject_bbox_pred\n            self.object_bbox_pred = object_bbox_pred\n            self.subject_bbox_target = subject_bbox_target\n            self.object_bbox_target = object_bbox_target\n        else:\n            self.which_in_batch = torch.hstack((self.which_in_batch, which_in_batch))\n            self.connectivity = torch.hstack((self.connectivity, connectivity))\n\n            confidence = torch.max(torch.vstack((torch.max(relation_pred[:, :self.args['models']['num_geometric']], dim=1)[0],\n                                                 torch.max(relation_pred[:, self.args['models']['num_geometric']:self.args['models']['num_geometric']+self.args['models']['num_possessive']], dim=1)[0],\n                                                 torch.max(relation_pred[:, self.args['models']['num_geometric']+self.args['models']['num_possessive']:], dim=1)[0])), dim=0)[0]  # in log space, [0] to take values\n            confidence[~iou_mask] = -math.inf\n            self.confidence = torch.hstack((self.confidence, confidence))\n\n            self.relation_pred = torch.vstack((self.relation_pred, relation_pred))\n            self.relation_target = torch.hstack((self.relation_target, relation_target))\n            self.super_relation_pred = torch.vstack((self.super_relation_pred, super_relation_pred))\n\n            self.subject_cat_pred = torch.hstack((self.subject_cat_pred, subject_cat_pred))\n            self.object_cat_pred = torch.hstack((self.object_cat_pred, object_cat_pred))\n            self.subject_cat_target = torch.hstack((self.subject_cat_target, subject_cat_target))\n            self.object_cat_target = torch.hstack((self.object_cat_target, object_cat_target))\n\n            self.subject_bbox_pred = torch.vstack((self.subject_bbox_pred, subject_bbox_pred))\n            self.object_bbox_pred = torch.vstack((self.object_bbox_pred, object_bbox_pred))\n            self.subject_bbox_target = torch.vstack((self.subject_bbox_target, subject_bbox_target))\n            self.object_bbox_target = torch.vstack((self.object_bbox_target, object_bbox_target))\n\n    def global_refine(self, refined_relation, connected_indices_accumulated):\n        # print('self.relation_pred', self.relation_pred.shape, 'connected_indices_accumulated', connected_indices_accumulated.shape)\n        # print('self.relation_pred[connected_indices_accumulated]', self.relation_pred[connected_indices_accumulated].shape, 'refined_relation', refined_relation.shape)\n        self.relation_pred[connected_indices_accumulated, :] = refined_relation\n\n        confidence = torch.max(torch.vstack((torch.max(refined_relation[:, :self.args['models']['num_geometric']], dim=1)[0],\n                                             torch.max(refined_relation[:, self.args['models']['num_geometric']:self.args['models']['num_geometric'] + self.args['models']['num_possessive']], dim=1)[0],\n                                             torch.max(refined_relation[:, self.args['models']['num_geometric'] + self.args['models']['num_possessive']:], dim=1)[0])), dim=0)[0]\n        self.confidence[connected_indices_accumulated] = confidence\n\n    def compute(self, per_class=False):\n        \"\"\"\n        A ground truth predicate is considered to match a hypothesized relationship iff the predicted relationship is correct,\n        the subject and object labels match, and the bounding boxes associated with the subject and object both have IOU>0.5 with the ground-truth boxes.\n        \"\"\"\n        self.confidence += self.connectivity\n\n        for image in torch.unique(self.which_in_batch):  # image-wise\n            curr_image = self.which_in_batch == image\n            num_relation_pred = len(self.relation_pred[curr_image])\n            curr_confidence = self.confidence[curr_image]\n\n            sorted_inds = torch.argsort(curr_confidence, dim=0, descending=True)\n\n            for i in range(len(self.relation_target[curr_image])):\n                if self.relation_target[curr_image][i] == -1:  # if target is not connected\n                    continue\n\n                # search in top k most confident predictions in each image\n                num_target = torch.sum(self.relation_target[curr_image] != -1)\n                this_k = min(self.top_k[-1], num_relation_pred)  # 100\n                keep_inds = sorted_inds[:this_k]\n\n                found = False   # found if any one of the three sub-models predict correctly\n                found_top1 = False  # found if only the most confident one of the three sub-models predict correctly\n                for j in range(len(keep_inds)):     # for each target <subject, relation, object> triple, find any match in the top k confident predictions\n                    if (self.subject_cat_target[curr_image][i] == self.subject_cat_pred[curr_image][keep_inds][j]\n                            and self.object_cat_target[curr_image][i] == self.object_cat_pred[curr_image][keep_inds][j]):\n\n                        sub_iou = self.iou(self.subject_bbox_target[curr_image][i], self.subject_bbox_pred[curr_image][keep_inds][j])\n                        obj_iou = self.iou(self.object_bbox_target[curr_image][i], self.object_bbox_pred[curr_image][keep_inds][j])\n\n                        if sub_iou >= self.iou_thresh and obj_iou >= self.iou_thresh:\n                            if not found:\n                                relation_pred_1 = self.relation_pred[curr_image][keep_inds][j][:self.args['models']['num_geometric']]  # geometric\n                                relation_pred_2 = self.relation_pred[curr_image][keep_inds][j][self.args['models']['num_geometric']:self.args['models']['num_geometric']\n                                                                                                                                    + self.args['models']['num_possessive']]  # possessive\n                                relation_pred_3 = self.relation_pred[curr_image][keep_inds][j][self.args['models']['num_geometric'] + self.args['models']['num_possessive']:]  # semantic\n                                if self.relation_target[curr_image][i] == torch.argmax(relation_pred_1) \\\n                                        or self.relation_target[curr_image][i] == torch.argmax(relation_pred_2) + self.args['models']['num_geometric'] \\\n                                        or self.relation_target[curr_image][i] == torch.argmax(relation_pred_3) + self.args['models']['num_geometric'] + self.args['models']['num_possessive']:\n                                    for k in self.top_k:\n                                        if j >= max(k, num_target):\n                                            continue\n                                        self.result_dict[k] += 1.0\n                                        if per_class:\n                                            self.result_per_class[k][self.relation_target[curr_image][i]] += 1.0\n                                    found = True\n\n                            if not found_top1:\n                                curr_super = torch.argmax(self.super_relation_pred[curr_image][keep_inds][j])\n                                relation_preds = [torch.argmax(self.relation_pred[curr_image][keep_inds][j][:self.args['models']['num_geometric']]),\n                                                  torch.argmax(self.relation_pred[curr_image][keep_inds][j][self.args['models']['num_geometric']:self.args['models']['num_geometric']\n                                                                                                            + self.args['models']['num_possessive']]) + self.args['models']['num_geometric'],\n                                                  torch.argmax(self.relation_pred[curr_image][keep_inds][j][self.args['models']['num_geometric'] + self.args['models']['num_possessive']:])\n                                                                                                            + self.args['models']['num_geometric'] + self.args['models']['num_possessive']]\n                                if self.relation_target[curr_image][i] == relation_preds[curr_super]:\n                                    for k in self.top_k:\n                                        if j >= max(k, num_target):\n                                            continue\n                                        self.result_dict_top1[k] += 1.0\n                                        if per_class:\n                                            self.result_per_class_top1[k][self.relation_target[curr_image][i]] += 1.0\n                                    found_top1 = True\n\n                            if found and found_top1:\n                                break\n\n                self.num_connected_target += 1.0\n                self.num_conn_target_per_class[self.relation_target[curr_image][i]] += 1.0\n\n        recall_k = [self.result_dict[k] / max(self.num_connected_target, 1e-3) for k in self.top_k]\n        recall_k_per_class = [self.result_per_class[k] / self.num_conn_target_per_class for k in self.top_k]\n        mean_recall_k = [torch.nanmean(r) for r in recall_k_per_class]\n        # recall_k_top1 = [self.result_dict_top1[k] / self.num_connected_target for k in self.top_k]\n        # mean_recall_k_top1 = [torch.nanmean(r) for r in recall_k_per_class_top1]\n        return recall_k, recall_k_per_class, mean_recall_k\n\n    def clear_data(self):\n        self.which_in_batch = None\n        self.confidence = None\n        self.relation_pred = None\n        self.connectivity = None\n        self.relation_target = None\n\n        self.subject_cat_pred = None\n        self.object_cat_pred = None\n        self.subject_cat_target = None\n        self.object_cat_target = None\n\n        self.subject_bbox_pred = None\n        self.object_bbox_pred = None\n        self.subject_bbox_target = None\n        self.object_bbox_target = None\n\n"}
{"type": "source_file", "path": "sup_contrast/networks/resnet_big.py", "content": "\"\"\"ResNet in PyTorch.\nImageNet-Style ResNet\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\nAdapted from: https://github.com/bearpaw/pytorch-classification\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1, is_last=False):\n        super(BasicBlock, self).__init__()\n        self.is_last = is_last\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion * planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion * planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        preact = out\n        out = F.relu(out)\n        if self.is_last:\n            return out, preact\n        else:\n            return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1, is_last=False):\n        super(Bottleneck, self).__init__()\n        self.is_last = is_last\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion * planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion * planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        preact = out\n        out = F.relu(out)\n        if self.is_last:\n            return out, preact\n        else:\n            return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, num_blocks, in_channel=3, zero_init_residual=False):\n        super(ResNet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves\n        # like an identity. This improves the model by 0.2~0.3% according to:\n        # https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for i in range(num_blocks):\n            stride = strides[i]\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x, layer=100):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.avgpool(out)\n        out = torch.flatten(out, 1)\n        return out\n\n\ndef resnet18(**kwargs):\n    return ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n\n\ndef resnet34(**kwargs):\n    return ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n\n\ndef resnet50(**kwargs):\n    return ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n\n\ndef resnet101(**kwargs):\n    return ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n\n\nmodel_dict = {\n    'resnet18': [resnet18, 512],\n    'resnet34': [resnet34, 512],\n    'resnet50': [resnet50, 2048],\n    'resnet101': [resnet101, 2048],\n}\n\n\nclass LinearBatchNorm(nn.Module):\n    \"\"\"Implements BatchNorm1d by BatchNorm2d, for SyncBN purpose\"\"\"\n    def __init__(self, dim, affine=True):\n        super(LinearBatchNorm, self).__init__()\n        self.dim = dim\n        self.bn = nn.BatchNorm2d(dim, affine=affine)\n\n    def forward(self, x):\n        x = x.view(-1, self.dim, 1, 1)\n        x = self.bn(x)\n        x = x.view(-1, self.dim)\n        return x\n\n\nclass SupConResNet(nn.Module):\n    \"\"\"backbone + projection head\"\"\"\n    def __init__(self, name='resnet50', head='mlp', feat_dim=128):\n        super(SupConResNet, self).__init__()\n        model_fun, dim_in = model_dict[name]\n        self.encoder = model_fun()\n        if head == 'linear':\n            self.head = nn.Linear(dim_in, feat_dim)\n        elif head == 'mlp':\n            self.head = nn.Sequential(\n                nn.Linear(dim_in, dim_in),\n                nn.ReLU(inplace=True),\n                nn.Linear(dim_in, feat_dim)\n            )\n        else:\n            raise NotImplementedError(\n                'head not supported: {}'.format(head))\n\n    def forward(self, x):\n        feat = self.encoder(x)\n        feat = F.normalize(self.head(feat), dim=1)\n        return feat\n\n\nclass SupCEResNet(nn.Module):\n    \"\"\"encoder + classifier\"\"\"\n    def __init__(self, name='resnet50', num_classes=10):\n        super(SupCEResNet, self).__init__()\n        model_fun, dim_in = model_dict[name]\n        self.encoder = model_fun()\n        self.fc = nn.Linear(dim_in, num_classes)\n\n    def forward(self, x):\n        return self.fc(self.encoder(x))\n\n\nclass LinearClassifier(nn.Module):\n    \"\"\"Linear classifier\"\"\"\n    def __init__(self, name='resnet50', num_classes=10):\n        super(LinearClassifier, self).__init__()\n        _, feat_dim = model_dict[name]\n        self.fc = nn.Linear(feat_dim, num_classes)\n\n    def forward(self, features):\n        return self.fc(features)\n"}
{"type": "source_file", "path": "sup_contrast/util.py", "content": "from __future__ import print_function\n\nimport math\nimport numpy as np\nimport torch\nimport torch.optim as optim\n\n\nclass TwoCropTransform:\n    \"\"\"Create two crops of the same image\"\"\"\n    def __init__(self, transform):\n        self.transform = transform\n\n    def __call__(self, x):\n        return [self.transform(x), self.transform(x)]\n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n\n\ndef adjust_learning_rate(args, optimizer, epoch):\n    lr = args.learning_rate\n    if args.cosine:\n        eta_min = lr * (args.lr_decay_rate ** 3)\n        lr = eta_min + (lr - eta_min) * (\n                1 + math.cos(math.pi * epoch / args.epochs)) / 2\n    else:\n        steps = np.sum(epoch > np.asarray(args.lr_decay_epochs))\n        if steps > 0:\n            lr = lr * (args.lr_decay_rate ** steps)\n\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n\ndef warmup_learning_rate(args, epoch, batch_id, total_batches, optimizer):\n    if args.warm and epoch <= args.warm_epochs:\n        p = (batch_id + (epoch - 1) * total_batches) / \\\n            (args.warm_epochs * total_batches)\n        lr = args.warmup_from + p * (args.warmup_to - args.warmup_from)\n\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n\n\ndef set_optimizer(opt, model):\n    optimizer = optim.SGD(model.parameters(),\n                          lr=opt.learning_rate,\n                          momentum=opt.momentum,\n                          weight_decay=opt.weight_decay)\n    return optimizer\n\n\ndef save_model(model, optimizer, opt, epoch, save_file):\n    print('==> Saving...')\n    state = {\n        'opt': opt,\n        'model': model.state_dict(),\n        'optimizer': optimizer.state_dict(),\n        'epoch': epoch,\n    }\n    torch.save(state, save_file)\n    del state\n"}
{"type": "source_file", "path": "sup_contrast/main_supcon.py", "content": "from __future__ import print_function\n\nimport os\nimport sys\nimport argparse\nimport time\nimport math\n\nimport tensorboard_logger as tb_logger\nimport torch\nimport torch.backends.cudnn as cudnn\nfrom torchvision import transforms, datasets\n\nfrom util import TwoCropTransform, AverageMeter\nfrom util import adjust_learning_rate, warmup_learning_rate\nfrom util import set_optimizer, save_model\nfrom networks.resnet_big import SupConResNet\nfrom losses import SupConLoss\n\ntry:\n    import apex\n    from apex import amp, optimizers\nexcept ImportError:\n    pass\n\n\ndef parse_option():\n    parser = argparse.ArgumentParser('argument for training')\n\n    parser.add_argument('--print_freq', type=int, default=10,\n                        help='print frequency')\n    parser.add_argument('--save_freq', type=int, default=50,\n                        help='save frequency')\n    parser.add_argument('--batch_size', type=int, default=256,\n                        help='batch_size')\n    parser.add_argument('--num_workers', type=int, default=16,\n                        help='num of workers to use')\n    parser.add_argument('--epochs', type=int, default=1000,\n                        help='number of training epochs')\n\n    # optimization\n    parser.add_argument('--learning_rate', type=float, default=0.05,\n                        help='learning rate')\n    parser.add_argument('--lr_decay_epochs', type=str, default='700,800,900',\n                        help='where to decay lr, can be a list')\n    parser.add_argument('--lr_decay_rate', type=float, default=0.1,\n                        help='decay rate for learning rate')\n    parser.add_argument('--weight_decay', type=float, default=1e-4,\n                        help='weight decay')\n    parser.add_argument('--momentum', type=float, default=0.9,\n                        help='momentum')\n\n    # model dataset\n    parser.add_argument('--model', type=str, default='resnet50')\n    parser.add_argument('--dataset', type=str, default='cifar10',\n                        choices=['cifar10', 'cifar100', 'path'], help='dataset')\n    parser.add_argument('--mean', type=str, help='mean of dataset in path in form of str tuple')\n    parser.add_argument('--std', type=str, help='std of dataset in path in form of str tuple')\n    parser.add_argument('--data_folder', type=str, default=None, help='path to custom dataset')\n    parser.add_argument('--size', type=int, default=32, help='parameter for RandomResizedCrop')\n\n    # method\n    parser.add_argument('--method', type=str, default='SupCon',\n                        choices=['SupCon', 'SimCLR'], help='choose method')\n\n    # temperature\n    parser.add_argument('--temp', type=float, default=0.07,\n                        help='temperature for loss function')\n\n    # other setting\n    parser.add_argument('--cosine', action='store_true',\n                        help='using cosine annealing')\n    parser.add_argument('--syncBN', action='store_true',\n                        help='using synchronized batch normalization')\n    parser.add_argument('--warm', action='store_true',\n                        help='warm-up for large batch training')\n    parser.add_argument('--trial', type=str, default='0',\n                        help='id for recording multiple runs')\n\n    opt = parser.parse_args()\n\n    # check if dataset is path that passed required arguments\n    if opt.dataset == 'path':\n        assert opt.data_folder is not None \\\n            and opt.mean is not None \\\n            and opt.std is not None\n\n    # set the path according to the environment\n    if opt.data_folder is None:\n        opt.data_folder = './datasets/'\n    opt.model_path = './save/SupCon/{}_models'.format(opt.dataset)\n    opt.tb_path = './save/SupCon/{}_tensorboard'.format(opt.dataset)\n\n    iterations = opt.lr_decay_epochs.split(',')\n    opt.lr_decay_epochs = list([])\n    for it in iterations:\n        opt.lr_decay_epochs.append(int(it))\n\n    opt.model_name = '{}_{}_{}_lr_{}_decay_{}_bsz_{}_temp_{}_trial_{}'.\\\n        format(opt.method, opt.dataset, opt.model, opt.learning_rate,\n               opt.weight_decay, opt.batch_size, opt.temp, opt.trial)\n\n    if opt.cosine:\n        opt.model_name = '{}_cosine'.format(opt.model_name)\n\n    # warm-up for large-batch training,\n    if opt.batch_size > 256:\n        opt.warm = True\n    if opt.warm:\n        opt.model_name = '{}_warm'.format(opt.model_name)\n        opt.warmup_from = 0.01\n        opt.warm_epochs = 10\n        if opt.cosine:\n            eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)\n            opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (\n                    1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2\n        else:\n            opt.warmup_to = opt.learning_rate\n\n    opt.tb_folder = os.path.join(opt.tb_path, opt.model_name)\n    if not os.path.isdir(opt.tb_folder):\n        os.makedirs(opt.tb_folder)\n\n    opt.save_folder = os.path.join(opt.model_path, opt.model_name)\n    if not os.path.isdir(opt.save_folder):\n        os.makedirs(opt.save_folder)\n\n    return opt\n\n\ndef set_loader(opt):\n    # construct data loader\n    if opt.dataset == 'cifar10':\n        mean = (0.4914, 0.4822, 0.4465)\n        std = (0.2023, 0.1994, 0.2010)\n    elif opt.dataset == 'cifar100':\n        mean = (0.5071, 0.4867, 0.4408)\n        std = (0.2675, 0.2565, 0.2761)\n    elif opt.dataset == 'path':\n        mean = eval(opt.mean)\n        std = eval(opt.std)\n    else:\n        raise ValueError('dataset not supported: {}'.format(opt.dataset))\n    normalize = transforms.Normalize(mean=mean, std=std)\n\n    train_transform = transforms.Compose([\n        transforms.RandomResizedCrop(size=opt.size, scale=(0.2, 1.)),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomApply([\n            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n        ], p=0.8),\n        transforms.RandomGrayscale(p=0.2),\n        transforms.ToTensor(),\n        normalize,\n    ])\n\n    if opt.dataset == 'cifar10':\n        train_dataset = datasets.CIFAR10(root=opt.data_folder,\n                                         transform=TwoCropTransform(train_transform),\n                                         download=True)\n    elif opt.dataset == 'cifar100':\n        train_dataset = datasets.CIFAR100(root=opt.data_folder,\n                                          transform=TwoCropTransform(train_transform),\n                                          download=True)\n    elif opt.dataset == 'path':\n        train_dataset = datasets.ImageFolder(root=opt.data_folder,\n                                            transform=TwoCropTransform(train_transform))\n    else:\n        raise ValueError(opt.dataset)\n\n    train_sampler = None\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=opt.batch_size, shuffle=(train_sampler is None),\n        num_workers=opt.num_workers, pin_memory=True, sampler=train_sampler)\n\n    return train_loader\n\n\ndef set_model(opt):\n    model = SupConResNet(name=opt.model)\n    criterion = SupConLoss(temperature=opt.temp)\n\n    # enable synchronized Batch Normalization\n    if opt.syncBN:\n        model = apex.parallel.convert_syncbn_model(model)\n\n    if torch.cuda.is_available():\n        if torch.cuda.device_count() > 1:\n            model.encoder = torch.nn.DataParallel(model.encoder)\n        model = model.cuda()\n        criterion = criterion.cuda()\n        cudnn.benchmark = True\n\n    return model, criterion\n\n\ndef train(train_loader, model, criterion, optimizer, epoch, opt):\n    \"\"\"one epoch training\"\"\"\n    model.train()\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n\n    end = time.time()\n    for idx, (images, labels) in enumerate(train_loader):\n        data_time.update(time.time() - end)\n\n        images = torch.cat([images[0], images[1]], dim=0)\n        if torch.cuda.is_available():\n            images = images.cuda(non_blocking=True)\n            labels = labels.cuda(non_blocking=True)\n        bsz = labels.shape[0]\n\n        # warm-up learning rate\n        warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)\n\n        # compute loss\n        features = model(images)\n        f1, f2 = torch.split(features, [bsz, bsz], dim=0)\n        features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)\n        if opt.method == 'SupCon':\n            loss = criterion(features, labels)\n        elif opt.method == 'SimCLR':\n            loss = criterion(features)\n        else:\n            raise ValueError('contrastive method not supported: {}'.\n                             format(opt.method))\n\n        # update metric\n        losses.update(loss.item(), bsz)\n\n        # SGD\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # print info\n        if (idx + 1) % opt.print_freq == 0:\n            print('Train: [{0}][{1}/{2}]\\t'\n                  'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                  'DT {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n                  'loss {loss.val:.3f} ({loss.avg:.3f})'.format(\n                   epoch, idx + 1, len(train_loader), batch_time=batch_time,\n                   data_time=data_time, loss=losses))\n            sys.stdout.flush()\n\n    return losses.avg\n\n\ndef main():\n    opt = parse_option()\n\n    # build data loader\n    train_loader = set_loader(opt)\n\n    # build model and criterion\n    model, criterion = set_model(opt)\n\n    # build optimizer\n    optimizer = set_optimizer(opt, model)\n\n    # tensorboard\n    logger = tb_logger.Logger(logdir=opt.tb_folder, flush_secs=2)\n\n    # training routine\n    for epoch in range(1, opt.epochs + 1):\n        adjust_learning_rate(opt, optimizer, epoch)\n\n        # train for one epoch\n        time1 = time.time()\n        loss = train(train_loader, model, criterion, optimizer, epoch, opt)\n        time2 = time.time()\n        print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))\n\n        # tensorboard logger\n        logger.log_value('loss', loss, epoch)\n        logger.log_value('learning_rate', optimizer.param_groups[0]['lr'], epoch)\n\n        if epoch % opt.save_freq == 0:\n            save_file = os.path.join(\n                opt.save_folder, 'ckpt_epoch_{epoch}.pth'.format(epoch=epoch))\n            save_model(model, optimizer, opt, epoch, save_file)\n\n    # save the last model\n    save_file = os.path.join(\n        opt.save_folder, 'last.pth')\n    save_model(model, optimizer, opt, opt.epochs, save_file)\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "model.py", "content": "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nfrom utils import process_super_class\n\n\nclass BayesianHead(nn.Module):\n    \"\"\"\n    The prediction head with a hierarchical classification when the optional transformer encoder is used.\n    \"\"\"\n\n    def __init__(self, input_dim=512, num_geometric=15, num_possessive=11, num_semantic=24, T1=1, T2=1, T3=1):\n        super(BayesianHead, self).__init__()\n        self.fc3_1 = nn.Linear(input_dim, num_geometric)\n        self.fc3_2 = nn.Linear(input_dim, num_possessive)\n        self.fc3_3 = nn.Linear(input_dim, num_semantic)\n        self.fc5 = nn.Linear(input_dim, 3)\n        self.T1 = T1\n        self.T2 = T2\n        self.T3 = T3\n\n    def forward(self, h):\n        super_relation = F.log_softmax(self.fc5(h), dim=1)\n\n        # By Bayes rule, log p(relation_n, super_n) = log p(relation_1 | super_1) + log p(super_1)\n        relation_1 = self.fc3_1(h)  # geometric\n        relation_1 = F.log_softmax(relation_1 / self.T1, dim=1) + super_relation[:, 0].view(-1, 1)\n        relation_2 = self.fc3_2(h)  # possessive\n        relation_2 = F.log_softmax(relation_2 / self.T2, dim=1) + super_relation[:, 1].view(-1, 1)\n        relation_3 = self.fc3_3(h)  # semantic\n        relation_3 = F.log_softmax(relation_3 / self.T3, dim=1) + super_relation[:, 2].view(-1, 1)\n        return relation_1, relation_2, relation_3, super_relation\n\n\nclass FlatRelationClassifier(nn.Module):\n    \"\"\"\n    The local prediction module with a flat classification.\n    \"\"\"\n\n    def __init__(self, args, input_dim=128, output_dim=50, feature_size=32, num_classes=150, num_super_classes=17):\n        super(FlatRelationClassifier, self).__init__()\n        self.num_classes = num_classes\n        self.num_super_classes = num_super_classes\n        self.conv1_1 = nn.Conv2d(2 * input_dim + 1, input_dim, kernel_size=1, stride=1, padding=0)\n        self.conv1_2 = nn.Conv2d(2 * input_dim + 1, input_dim, kernel_size=1, stride=1, padding=0)\n        self.conv2_1 = nn.Conv2d(2 * input_dim, 4 * input_dim, kernel_size=3, stride=1, padding=1)\n        self.conv3_1 = nn.Conv2d(4 * input_dim, 8 * input_dim, kernel_size=3, stride=1, padding=1)\n        self.dropout1 = nn.Dropout(p=0.5)\n        self.dropout2 = nn.Dropout(p=0.5)\n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.fc1 = nn.Linear(8 * input_dim * (feature_size // 4) ** 2, 4096)\n        if args['dataset']['dataset'] == 'vg':\n            self.fc2 = nn.Linear(4096 + 2 * (num_classes + num_super_classes), 512)\n        else:\n            self.fc2 = nn.Linear(4096 + 2 * num_classes, 512)\n        self.fc3 = nn.Linear(512, output_dim)\n        self.fc4 = nn.Linear(512, 1)\n\n    def conv_layers(self, h_sub, h_obj):\n        h_sub = torch.tanh(self.conv1_1(h_sub))\n        h_obj = torch.tanh(self.conv1_2(h_obj))\n        h = torch.cat((h_sub, h_obj), dim=1)  # (batch_size, 256, 32, 32)\n\n        h = F.relu(self.conv2_1(h))  # (batch_size, 512, 32, 32)\n        h = self.maxpool(h)  # (batch_size, 512, 16, 16)\n        h = F.relu(self.conv3_1(h))  # (batch_size, 1024,16, 16)\n        h = self.maxpool(h)  # (batch_size, 1024, 8,  8)\n\n        h = torch.reshape(h, (h.shape[0], -1))\n        h = self.dropout1(F.relu(self.fc1(h)))\n        return h\n\n    def concat_labels(self, h, c1, c2, s1, s2, rank, h_aug=None):\n        c1 = F.one_hot(c1, num_classes=self.num_classes)\n        c2 = F.one_hot(c2, num_classes=self.num_classes)\n        if s1 is not None:  # concatenate super-class labels as well\n            s1, s2 = process_super_class(s1, s2, self.num_super_classes, rank)\n            hc = torch.cat((h, c1, c2, s1, s2), dim=1)\n\n            if h_aug is not None:\n                h_aug = torch.cat((h_aug, c1, c2, s1, s2), dim=1)\n                h_aug = self.dropout2(F.relu(self.fc2(h_aug)))\n        else:\n            hc = torch.cat((h, c1, c2), dim=1)\n\n            if h_aug is not None:\n                h_aug = torch.cat((h_aug, c1, c2), dim=1)\n                h_aug = self.dropout2(F.relu(self.fc2(h_aug)))\n        return hc, h_aug\n\n    def forward(self, h_sub, h_obj, c1, c2, s1, s2, rank, h_sub_aug=None, h_obj_aug=None, one_hot=True):\n        h = self.conv_layers(h_sub, h_obj)\n        h_aug = self.conv_layers(h_sub_aug, h_obj_aug) if h_sub_aug is not None else None  # need data augmentation in contrastive learning\n        hc, pred_aug = self.concat_labels(h, c1, c2, s1, s2, rank, h_aug)\n\n        pred = self.dropout2(F.relu(self.fc2(hc)))\n        relation = self.fc3(pred)  # (batch_size, 50)\n        connectivity = self.fc4(pred)  # (batch_size, 1)\n        return relation, connectivity, pred, pred_aug\n\n\nclass BayesianRelationClassifier(nn.Module):\n    \"\"\"\n    The local prediction module with a hierarchical classification.\n    \"\"\"\n\n    def __init__(self, args, input_dim=128, feature_size=32, num_classes=150, num_super_classes=17, num_geometric=15,\n                 num_possessive=11, num_semantic=24, T1=1, T2=1, T3=1):\n        super(BayesianRelationClassifier, self).__init__()\n        self.input_dim = input_dim\n        self.num_classes = num_classes\n        self.num_super_classes = num_super_classes\n        self.conv1_1 = nn.Conv2d(2 * input_dim + 1, input_dim, kernel_size=1, stride=1, padding=0)\n        self.conv1_2 = nn.Conv2d(2 * input_dim + 1, input_dim, kernel_size=1, stride=1, padding=0)\n        self.conv2_1 = nn.Conv2d(2 * input_dim, 4 * input_dim, kernel_size=3, stride=1, padding=1)\n        self.conv3_1 = nn.Conv2d(4 * input_dim, 8 * input_dim, kernel_size=3, stride=1, padding=1)\n        self.dropout1 = nn.Dropout(p=0.5)\n        self.dropout2 = nn.Dropout(p=0.5)\n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.fc1 = nn.Linear(8 * input_dim * (feature_size // 4) ** 2, 4096)\n        if args['dataset']['dataset'] == 'vg':\n            self.fc2 = nn.Linear(4096 + 2 * (num_classes + num_super_classes), 512)\n        else:\n            self.fc2 = nn.Linear(4096 + 2 * num_classes, 512)\n        self.fc3_1 = nn.Linear(512, num_geometric)\n        self.fc3_2 = nn.Linear(512, num_possessive)\n        self.fc3_3 = nn.Linear(512, num_semantic)\n        self.fc4 = nn.Linear(512, 1)\n        self.fc5 = nn.Linear(512, 3)\n        self.T1 = T1\n        self.T2 = T2\n        self.T3 = T3\n\n    def conv_layers(self, h_sub, h_obj):\n        h_sub = torch.tanh(self.conv1_1(h_sub))\n        h_obj = torch.tanh(self.conv1_2(h_obj))\n        h = torch.cat((h_sub, h_obj), dim=1)  # (batch_size, 256, 32, 32)\n\n        h = F.relu(self.conv2_1(h))  # (batch_size, 512, 32, 32)\n        h = self.maxpool(h)  # (batch_size, 512, 16, 16)\n        h = F.relu(self.conv3_1(h))  # (batch_size, 1024,16, 16)\n        h = self.maxpool(h)  # (batch_size, 1024, 8,  8)\n\n        h = torch.reshape(h, (h.shape[0], -1))\n        h = self.dropout1(F.relu(self.fc1(h)))\n        return h\n\n    def concat_labels(self, h, c1, c2, s1, s2, rank, h_aug=None):\n        c1 = F.one_hot(c1, num_classes=self.num_classes)\n        c2 = F.one_hot(c2, num_classes=self.num_classes)\n        if s1 is not None:  # concatenate super-class labels as well\n            s1, s2 = process_super_class(s1, s2, self.num_super_classes, rank)\n            hc = torch.cat((h, c1, c2, s1, s2), dim=1)\n\n            if h_aug is not None:\n                h_aug = torch.cat((h_aug, c1, c2, s1, s2), dim=1)\n                h_aug = self.dropout2(F.relu(self.fc2(h_aug)))\n        else:\n            hc = torch.cat((h, c1, c2), dim=1)\n\n            if h_aug is not None:\n                h_aug = torch.cat((h_aug, c1, c2), dim=1)\n                h_aug = self.dropout2(F.relu(self.fc2(h_aug)))\n        return hc, h_aug\n\n    def forward(self, h_sub, h_obj, c1, c2, s1, s2, rank, h_sub_aug=None, h_obj_aug=None):\n        h = self.conv_layers(h_sub, h_obj)\n        h_aug = self.conv_layers(h_sub_aug, h_obj_aug) if h_sub_aug is not None else None  # need data augmentation in contrastive learning\n        hc, pred_aug = self.concat_labels(h, c1, c2, s1, s2, rank, h_aug)\n\n        pred = self.dropout2(F.relu(self.fc2(hc)))\n        connectivity = self.fc4(pred)  # (batch_size, 1)\n        super_relation = F.log_softmax(self.fc5(pred), dim=1)\n\n        relation_1 = self.fc3_1(pred)  # geometric\n        relation_1 = F.log_softmax(relation_1 / self.T1, dim=1) + super_relation[:, 0].view(-1, 1)\n        relation_2 = self.fc3_2(pred)  # possessive\n        relation_2 = F.log_softmax(relation_2 / self.T2, dim=1) + super_relation[:, 1].view(-1, 1)\n        relation_3 = self.fc3_3(pred)  # semantic\n        relation_3 = F.log_softmax(relation_3 / self.T3, dim=1) + super_relation[:, 2].view(-1, 1)\n\n        return relation_1, relation_2, relation_3, super_relation, connectivity, pred, pred_aug\n"}
{"type": "source_file", "path": "sup_contrast/main_ce.py", "content": "from __future__ import print_function\n\nimport os\nimport sys\nimport argparse\nimport time\nimport math\n\nimport tensorboard_logger as tb_logger\nimport torch\nimport torch.backends.cudnn as cudnn\nfrom torchvision import transforms, datasets\n\nfrom util import AverageMeter\nfrom util import adjust_learning_rate, warmup_learning_rate, accuracy\nfrom util import set_optimizer, save_model\nfrom networks.resnet_big import SupCEResNet\n\ntry:\n    import apex\n    from apex import amp, optimizers\nexcept ImportError:\n    pass\n\n\ndef parse_option():\n    parser = argparse.ArgumentParser('argument for training')\n\n    parser.add_argument('--print_freq', type=int, default=10,\n                        help='print frequency')\n    parser.add_argument('--save_freq', type=int, default=50,\n                        help='save frequency')\n    parser.add_argument('--batch_size', type=int, default=256,\n                        help='batch_size')\n    parser.add_argument('--num_workers', type=int, default=16,\n                        help='num of workers to use')\n    parser.add_argument('--epochs', type=int, default=500,\n                        help='number of training epochs')\n\n    # optimization\n    parser.add_argument('--learning_rate', type=float, default=0.2,\n                        help='learning rate')\n    parser.add_argument('--lr_decay_epochs', type=str, default='350,400,450',\n                        help='where to decay lr, can be a list')\n    parser.add_argument('--lr_decay_rate', type=float, default=0.1,\n                        help='decay rate for learning rate')\n    parser.add_argument('--weight_decay', type=float, default=1e-4,\n                        help='weight decay')\n    parser.add_argument('--momentum', type=float, default=0.9,\n                        help='momentum')\n\n    # model dataset\n    parser.add_argument('--model', type=str, default='resnet50')\n    parser.add_argument('--dataset', type=str, default='cifar10',\n                        choices=['cifar10', 'cifar100'], help='dataset')\n\n    # other setting\n    parser.add_argument('--cosine', action='store_true',\n                        help='using cosine annealing')\n    parser.add_argument('--syncBN', action='store_true',\n                        help='using synchronized batch normalization')\n    parser.add_argument('--warm', action='store_true',\n                        help='warm-up for large batch training')\n    parser.add_argument('--trial', type=str, default='0',\n                        help='id for recording multiple runs')\n\n    opt = parser.parse_args()\n\n    # set the path according to the environment\n    opt.data_folder = './datasets/'\n    opt.model_path = './save/SupCon/{}_models'.format(opt.dataset)\n    opt.tb_path = './save/SupCon/{}_tensorboard'.format(opt.dataset)\n\n    iterations = opt.lr_decay_epochs.split(',')\n    opt.lr_decay_epochs = list([])\n    for it in iterations:\n        opt.lr_decay_epochs.append(int(it))\n\n    opt.model_name = 'SupCE_{}_{}_lr_{}_decay_{}_bsz_{}_trial_{}'.\\\n        format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,\n               opt.batch_size, opt.trial)\n\n    if opt.cosine:\n        opt.model_name = '{}_cosine'.format(opt.model_name)\n\n    # warm-up for large-batch training,\n    if opt.batch_size > 256:\n        opt.warm = True\n    if opt.warm:\n        opt.model_name = '{}_warm'.format(opt.model_name)\n        opt.warmup_from = 0.01\n        opt.warm_epochs = 10\n        if opt.cosine:\n            eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)\n            opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (\n                    1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2\n        else:\n            opt.warmup_to = opt.learning_rate\n\n    opt.tb_folder = os.path.join(opt.tb_path, opt.model_name)\n    if not os.path.isdir(opt.tb_folder):\n        os.makedirs(opt.tb_folder)\n\n    opt.save_folder = os.path.join(opt.model_path, opt.model_name)\n    if not os.path.isdir(opt.save_folder):\n        os.makedirs(opt.save_folder)\n\n    if opt.dataset == 'cifar10':\n        opt.n_cls = 10\n    elif opt.dataset == 'cifar100':\n        opt.n_cls = 100\n    else:\n        raise ValueError('dataset not supported: {}'.format(opt.dataset))\n\n    return opt\n\n\ndef set_loader(opt):\n    # construct data loader\n    if opt.dataset == 'cifar10':\n        mean = (0.4914, 0.4822, 0.4465)\n        std = (0.2023, 0.1994, 0.2010)\n    elif opt.dataset == 'cifar100':\n        mean = (0.5071, 0.4867, 0.4408)\n        std = (0.2675, 0.2565, 0.2761)\n    else:\n        raise ValueError('dataset not supported: {}'.format(opt.dataset))\n    normalize = transforms.Normalize(mean=mean, std=std)\n\n    train_transform = transforms.Compose([\n        transforms.RandomResizedCrop(size=32, scale=(0.2, 1.)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        normalize,\n    ])\n\n    val_transform = transforms.Compose([\n        transforms.ToTensor(),\n        normalize,\n    ])\n\n    if opt.dataset == 'cifar10':\n        train_dataset = datasets.CIFAR10(root=opt.data_folder,\n                                         transform=train_transform,\n                                         download=True)\n        val_dataset = datasets.CIFAR10(root=opt.data_folder,\n                                       train=False,\n                                       transform=val_transform)\n    elif opt.dataset == 'cifar100':\n        train_dataset = datasets.CIFAR100(root=opt.data_folder,\n                                          transform=train_transform,\n                                          download=True)\n        val_dataset = datasets.CIFAR100(root=opt.data_folder,\n                                        train=False,\n                                        transform=val_transform)\n    else:\n        raise ValueError(opt.dataset)\n\n    train_sampler = None\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=opt.batch_size, shuffle=(train_sampler is None),\n        num_workers=opt.num_workers, pin_memory=True, sampler=train_sampler)\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=256, shuffle=False,\n        num_workers=8, pin_memory=True)\n\n    return train_loader, val_loader\n\n\ndef set_model(opt):\n    model = SupCEResNet(name=opt.model, num_classes=opt.n_cls)\n    criterion = torch.nn.CrossEntropyLoss()\n\n    # enable synchronized Batch Normalization\n    if opt.syncBN:\n        model = apex.parallel.convert_syncbn_model(model)\n\n    if torch.cuda.is_available():\n        if torch.cuda.device_count() > 1:\n            model = torch.nn.DataParallel(model)\n        model = model.cuda()\n        criterion = criterion.cuda()\n        cudnn.benchmark = True\n\n    return model, criterion\n\n\ndef train(train_loader, model, criterion, optimizer, epoch, opt):\n    \"\"\"one epoch training\"\"\"\n    model.train()\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n\n    end = time.time()\n    for idx, (images, labels) in enumerate(train_loader):\n        data_time.update(time.time() - end)\n\n        images = images.cuda(non_blocking=True)\n        labels = labels.cuda(non_blocking=True)\n        bsz = labels.shape[0]\n\n        # warm-up learning rate\n        warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)\n\n        # compute loss\n        output = model(images)\n        loss = criterion(output, labels)\n\n        # update metric\n        losses.update(loss.item(), bsz)\n        acc1, acc5 = accuracy(output, labels, topk=(1, 5))\n        top1.update(acc1[0], bsz)\n\n        # SGD\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # print info\n        if (idx + 1) % opt.print_freq == 0:\n            print('Train: [{0}][{1}/{2}]\\t'\n                  'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                  'DT {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n                  'loss {loss.val:.3f} ({loss.avg:.3f})\\t'\n                  'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n                   epoch, idx + 1, len(train_loader), batch_time=batch_time,\n                   data_time=data_time, loss=losses, top1=top1))\n            sys.stdout.flush()\n\n    return losses.avg, top1.avg\n\n\ndef validate(val_loader, model, criterion, opt):\n    \"\"\"validation\"\"\"\n    model.eval()\n\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n\n    with torch.no_grad():\n        end = time.time()\n        for idx, (images, labels) in enumerate(val_loader):\n            images = images.float().cuda()\n            labels = labels.cuda()\n            bsz = labels.shape[0]\n\n            # forward\n            output = model(images)\n            loss = criterion(output, labels)\n\n            # update metric\n            losses.update(loss.item(), bsz)\n            acc1, acc5 = accuracy(output, labels, topk=(1, 5))\n            top1.update(acc1[0], bsz)\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if idx % opt.print_freq == 0:\n                print('Test: [{0}/{1}]\\t'\n                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                      'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n                       idx, len(val_loader), batch_time=batch_time,\n                       loss=losses, top1=top1))\n\n    print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))\n    return losses.avg, top1.avg\n\n\ndef main():\n    best_acc = 0\n    opt = parse_option()\n\n    # build data loader\n    train_loader, val_loader = set_loader(opt)\n\n    # build model and criterion\n    model, criterion = set_model(opt)\n\n    # build optimizer\n    optimizer = set_optimizer(opt, model)\n\n    # tensorboard\n    logger = tb_logger.Logger(logdir=opt.tb_folder, flush_secs=2)\n\n    # training routine\n    for epoch in range(1, opt.epochs + 1):\n        adjust_learning_rate(opt, optimizer, epoch)\n\n        # train for one epoch\n        time1 = time.time()\n        loss, train_acc = train(train_loader, model, criterion, optimizer, epoch, opt)\n        time2 = time.time()\n        print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))\n\n        # tensorboard logger\n        logger.log_value('train_loss', loss, epoch)\n        logger.log_value('train_acc', train_acc, epoch)\n        logger.log_value('learning_rate', optimizer.param_groups[0]['lr'], epoch)\n\n        # evaluation\n        loss, val_acc = validate(val_loader, model, criterion, opt)\n        logger.log_value('val_loss', loss, epoch)\n        logger.log_value('val_acc', val_acc, epoch)\n\n        if val_acc > best_acc:\n            best_acc = val_acc\n\n        if epoch % opt.save_freq == 0:\n            save_file = os.path.join(\n                opt.save_folder, 'ckpt_epoch_{epoch}.pth'.format(epoch=epoch))\n            save_model(model, optimizer, opt, epoch, save_file)\n\n    # save the last model\n    save_file = os.path.join(\n        opt.save_folder, 'last.pth')\n    save_model(model, optimizer, opt, opt.epochs, save_file)\n\n    print('best accuracy: {:.2f}'.format(best_acc))\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "train_utils.py", "content": "import torch\nimport numpy as np\nfrom tqdm import tqdm\nimport os\nimport json\nfrom utils import *\n\n\ndef process_image_features(args, images, detr, rank):\n    images = torch.stack(images).to(rank)\n    image_feature, pos_embed = detr.module.backbone(nested_tensor_from_tensor_list(images))\n    src, mask = image_feature[-1].decompose()\n    src = detr.module.input_proj(src).flatten(2).permute(2, 0, 1)\n    pos_embed = pos_embed[-1].flatten(2).permute(2, 0, 1)\n    image_feature = detr.module.transformer.encoder(src, src_key_padding_mask=mask.flatten(1), pos=pos_embed)\n    image_feature = image_feature.permute(1, 2, 0)\n    image_feature = image_feature.view(-1, args['models']['num_img_feature'], args['models']['feature_size'], args['models']['feature_size'])\n    return image_feature\n\n\ndef train_one_direction(relation_classifier, args, h_sub, h_obj, cat_sub, cat_obj, spcat_sub, spcat_obj, bbox_sub, bbox_obj, h_sub_aug, h_obj_aug, iou_mask, rank, graph_iter, edge_iter, keep_in_batch,\n                        Recall, Recall_top3, criterion_relationship, criterion_connectivity, relations_target, direction_target, batch_count, hidden_cat_accumulated, hidden_cat_labels_accumulated,\n                        commonsense_aligned_triplets, commonsense_violated_triplets, len_train_loader, first_direction=True):\n\n    if args['models']['hierarchical_pred']:\n        relation_1, relation_2, relation_3, super_relation, connectivity, hidden, hidden_aug \\\n            = relation_classifier(h_sub, h_obj, cat_sub, cat_obj, spcat_sub, spcat_obj, rank, h_sub_aug, h_obj_aug)\n        relation = torch.cat((relation_1, relation_2, relation_3), dim=1)\n        hidden_cat = torch.cat((hidden.unsqueeze(1), hidden_aug.unsqueeze(1)), dim=1)\n\n    else:\n        relation, connectivity, hidden, hidden_aug = relation_classifier(h_sub, h_obj, cat_sub, cat_obj, spcat_sub, spcat_obj, rank, h_sub_aug, h_obj_aug)\n        hidden_cat = torch.cat((hidden.unsqueeze(1), hidden_aug.unsqueeze(1)), dim=1)\n        super_relation = None\n\n    if args['training']['run_mode'] == 'train_cs':\n        if args['models']['hierarchical_pred']:\n            relation_probs = torch.hstack((torch.max(F.softmax(relation_1, dim=1), dim=1)[0],\n                                           torch.max(F.softmax(relation_2, dim=1), dim=1)[0],\n                                           torch.max(F.softmax(relation_3, dim=1), dim=1)[0]))\n\n            relation_pred = torch.hstack((torch.argmax(relation_1, dim=1),\n                                          torch.argmax(relation_2, dim=1) + args['models']['num_geometric'],\n                                          torch.argmax(relation_3, dim=1) + args['models']['num_geometric'] + args['models']['num_possessive']))\n            triplets = torch.hstack((cat_sub.repeat(3).unsqueeze(1), relation_pred.unsqueeze(1), cat_obj.repeat(3).unsqueeze(1)))\n\n        else:\n            relation_probs = torch.max(F.softmax(relation, dim=1), dim=1)[0]\n            relation_pred = torch.argmax(relation, dim=1)\n            triplets = torch.hstack((cat_sub.unsqueeze(1), relation_pred.unsqueeze(1), cat_obj.unsqueeze(1)))\n\n        # evaluate on the commonsense for all predictions, regardless of whether they match with the ground truth or not\n        not_in_yes_dict = torch.tensor([tuple(triplets[i].cpu().tolist()) not in commonsense_aligned_triplets for i in range(len(triplets))], dtype=torch.bool).to(rank)\n        is_in_no_dict = torch.tensor([tuple(triplets[i].cpu().tolist()) in commonsense_violated_triplets for i in range(len(triplets))], dtype=torch.bool).to(rank)\n\n        loss_commonsense = 0.0\n        if relation_probs[not_in_yes_dict].numel() > 0:\n            loss_commonsense += args['training']['lambda_cs_weak'] * relation_probs[not_in_yes_dict].mean()\n        if relation_probs[is_in_no_dict].numel() > 0:\n            loss_commonsense += args['training']['lambda_cs_strong'] * relation_probs[is_in_no_dict].mean()\n    else:\n        loss_commonsense = 0.0\n\n    # evaluate on the connectivity\n    if first_direction:\n        not_connected = torch.where(direction_target[graph_iter - 1][edge_iter] != 1)[0]  # which data samples in curr keep_in_batch are not connected\n    else:\n        not_connected = torch.where(direction_target[graph_iter - 1][edge_iter] != 0)[0]\n    num_not_connected = len(not_connected)\n    temp = criterion_connectivity(connectivity[not_connected, 0], torch.zeros(len(not_connected)).to(rank))\n    loss_connectivity = 0.0 if torch.isnan(temp) else args['training']['lambda_not_connected'] * temp\n\n    if first_direction:\n        connected = torch.where(direction_target[graph_iter - 1][edge_iter] == 1)[0]  # which data samples in curr keep_in_batch are connected\n    else:\n        connected = torch.where(direction_target[graph_iter - 1][edge_iter] == 0)[0]\n    num_connected = len(connected)\n    connected_pred = torch.nonzero(torch.sigmoid(connectivity[:, 0]) >= 0.5).flatten()\n    connectivity_precision = torch.sum(relations_target[graph_iter - 1][edge_iter][connected_pred] != -1)\n    num_connected_pred = len(connected_pred)\n\n    connected_indices = torch.zeros(len(hidden_cat), dtype=torch.bool).to(rank)\n    hidden_cat = hidden_cat[connected]\n    connected_indices[connected] = 1\n\n    # evaluate on the relationships\n    loss_relationship = 0.0\n    connectivity_recall = 0.0\n    if len(connected) > 0:\n        temp = criterion_connectivity(connectivity[connected, 0], torch.ones(len(connected)).to(rank))\n        loss_connectivity = 0.0 if torch.isnan(temp) else temp\n        connectivity_recall = torch.sum(torch.round(torch.sigmoid(connectivity[connected, 0])))\n\n        loss_relationship = calculate_losses_on_relationships(args, relation, super_relation, connected, relations_target[graph_iter - 1][edge_iter], criterion_relationship)\n\n        hidden_cat_labels = relations_target[graph_iter - 1][edge_iter][connected]\n        for index, batch_index in enumerate(keep_in_batch[connected]):\n            hidden_cat_accumulated[batch_index].append(hidden_cat[index])\n            hidden_cat_labels_accumulated[batch_index].append(hidden_cat_labels[index])\n\n    # evaluate recall@k scores\n    relations_target_directed = relations_target[graph_iter - 1][edge_iter].clone()\n    relations_target_directed[not_connected] = -1\n\n    if (batch_count % args['training']['eval_freq'] == 0) or (batch_count + 1 == len_train_loader):\n        Recall.accumulate(keep_in_batch, relation, relations_target_directed, super_relation, torch.log(torch.sigmoid(connectivity[:, 0])),\n                          cat_sub, cat_obj, cat_sub, cat_obj, bbox_sub, bbox_obj, bbox_sub, bbox_obj, iou_mask)\n        if args['dataset']['dataset'] == 'vg' and args['models']['hierarchical_pred']:\n            Recall_top3.accumulate(keep_in_batch, relation, relations_target_directed, super_relation, torch.log(torch.sigmoid(connectivity[:, 0])),\n                                   cat_sub, cat_obj, cat_sub, cat_obj, bbox_sub, bbox_obj, bbox_sub, bbox_obj, iou_mask)\n\n    return loss_relationship, loss_connectivity, loss_commonsense, num_not_connected, num_connected, num_connected_pred, connectivity_precision, \\\n           connectivity_recall, hidden_cat_accumulated, hidden_cat_labels_accumulated\n\n\ndef calculate_losses_on_relationships(args, relation, super_relation, connected, curr_relations_target, criterion_relationship, pseudo_label_mask=None, lambda_pseudo=1):\n    loss_relationship = 0.0\n    is_hierarchical_pred = args['models']['hierarchical_pred']\n\n    # Only proceed if there are any connected edges to evaluate\n    if connected.numel() == 0:\n        return loss_relationship\n\n    # Compute super category losses if hierarchical_pred is enabled\n    if is_hierarchical_pred:\n        relation_1 = relation[:, :args['models']['num_geometric']]\n        relation_2 = relation[:, args['models']['num_geometric']:args['models']['num_geometric'] + args['models']['num_possessive']]\n        relation_3 = relation[:, args['models']['num_geometric'] + args['models']['num_possessive']:]\n        relation = [relation_1, relation_2, relation_3]\n\n        criterion_relationship_1, criterion_relationship_2, criterion_relationship_3, criterion_super_relationship = criterion_relationship\n        super_relation_target = super_relation_processing(args, connected, curr_relations_target)\n\n        # Compute losses for super relationships\n        loss_relationship += criterion_super_relationship(super_relation[connected], super_relation_target)\n\n        # Compute sub category losses\n        connected_1 = torch.nonzero(curr_relations_target[connected] < args['models']['num_geometric']).flatten()  # geometric\n        connected_2 = torch.nonzero(torch.logical_and(curr_relations_target[connected] >= args['models']['num_geometric'],\n                                                      curr_relations_target[connected] < args['models']['num_geometric'] + args['models']['num_possessive'])).flatten()  # possessive\n        connected_3 = torch.nonzero(curr_relations_target[connected] >= args['models']['num_geometric'] + args['models']['num_possessive']).flatten()  # semantic\n        connected_sub = [connected_1, connected_2, connected_3]\n\n        for i, (criterion_rel, offset) in enumerate(zip(\n                [criterion_relationship_1, criterion_relationship_2, criterion_relationship_3],\n                [0, args['models']['num_geometric'], args['models']['num_geometric'] + args['models']['num_possessive']]\n        )):\n            connected_i = connected_sub[i]\n\n            if connected_i.numel() > 0:  # Non-semi-supervised or non-empty connected indices\n                loss_relationship += criterion_rel(relation[i][connected][connected_i], curr_relations_target[connected][connected_i] - offset)\n\n    # Compute losses if not using hierarchical predictions\n    else:\n        loss_relationship += criterion_relationship(relation[connected], curr_relations_target[connected])\n\n    return loss_relationship\n\n\ndef evaluate_one_direction(relation_classifier, args, h_sub, h_obj, cat_sub, cat_obj, spcat_sub, spcat_obj, bbox_sub, bbox_obj, iou_mask, rank, graph_iter, edge_iter, keep_in_batch,\n                           Recall, Recall_top3, relations_target, direction_target, batch_count, len_test_loader, first_direction=True):\n    if args['models']['hierarchical_pred']:\n        relation_1, relation_2, relation_3, super_relation, connectivity, _, _ = relation_classifier(h_sub, h_obj, cat_sub, cat_obj, spcat_sub, spcat_obj, rank)\n        relation = torch.cat((relation_1, relation_2, relation_3), dim=1)\n    else:\n        relation, connectivity, _, _ = relation_classifier(h_sub, h_obj, cat_sub, cat_obj, spcat_sub, spcat_obj, rank)\n        super_relation = None\n\n    if first_direction:\n        not_connected = torch.where(direction_target[graph_iter - 1][edge_iter] != 1)[0]  # which data samples in curr keep_in_batch are not connected\n        connected = torch.where(direction_target[graph_iter - 1][edge_iter] == 1)[0]  # which data samples in curr keep_in_batch are connected\n    else:\n        not_connected = torch.where(direction_target[graph_iter - 1][edge_iter] != 0)[0]\n        connected = torch.where(direction_target[graph_iter - 1][edge_iter] == 0)[0]  # which data samples in curr keep_in_batch are connected\n    num_not_connected = len(not_connected)\n    num_connected = len(connected)\n    connected_pred = torch.nonzero(torch.sigmoid(connectivity[:, 0]) >= 0.5).flatten()\n    connectivity_precision = torch.sum(relations_target[graph_iter - 1][edge_iter][connected_pred] != -1)\n    num_connected_pred = len(connected_pred)\n\n    connectivity_recall = 0.0\n    if len(connected) > 0:\n        connectivity_recall = torch.sum(torch.round(torch.sigmoid(connectivity[connected, 0])))\n\n    # evaluate recall@k scores\n    relations_target_directed = relations_target[graph_iter - 1][edge_iter].clone()\n    relations_target_directed[not_connected] = -1\n\n    if (batch_count % args['training']['eval_freq_test'] == 0) or (batch_count + 1 == len_test_loader):\n        Recall.accumulate(keep_in_batch, relation, relations_target_directed, super_relation, torch.log(torch.sigmoid(connectivity[:, 0])),\n                          cat_sub, cat_obj, cat_sub, cat_obj, bbox_sub, bbox_obj, bbox_sub, bbox_obj, iou_mask)\n        if args['dataset']['dataset'] == 'vg' and args['models']['hierarchical_pred']:\n            Recall_top3.accumulate(keep_in_batch, relation, relations_target_directed, super_relation, torch.log(torch.sigmoid(connectivity[:, 0])),\n                                   cat_sub, cat_obj, cat_sub, cat_obj, bbox_sub, bbox_obj, bbox_sub, bbox_obj, iou_mask)\n\n    return num_not_connected, num_connected, num_connected_pred, connectivity_precision, connectivity_recall\n\n"}
{"type": "source_file", "path": "token_embeddings.py", "content": "from transformers import GPT2Tokenizer, GPT2Model, BertTokenizer, BertModel, CLIPTextModel, CLIPTokenizer\nimport torch\nfrom sklearn.cluster import KMeans\nimport numpy as np\nfrom dataset_utils import relation_class_by_freq\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\n\n\n# # This function is defined in dataset_utils. It is copied here for reference.\n# def relation_class_by_freq():\n#     return {0: 'on', 1: 'has', 2: 'in', 3: 'of', 4: 'wearing', 5: 'near', 6: 'with', 7: 'above', 8: 'holding', 9: 'behind',\n#             10: 'under', 11: 'sitting on', 12: 'wears', 13: 'standing on', 14: 'in front of', 15: 'attached to', 16: 'at', 17: 'hanging from', 18: 'over', 19: 'for',\n#             20: 'riding', 21: 'carrying', 22: 'eating', 23: 'walking on', 24: 'playing', 25: 'covering', 26: 'laying on', 27: 'along', 28: 'watching', 29: 'and',\n#             30: 'between', 31: 'belonging to', 32: 'painted on', 33: 'against', 34: 'looking at', 35: 'from', 36: 'parked on', 37: 'to', 38: 'made of', 39: 'covered in',\n#             40: 'mounted on', 41: 'says', 42: 'part of', 43: 'across', 44: 'flying in', 45: 'using', 46: 'on back of', 47: 'lying on', 48: 'growing on', 49: 'walking in'}\n\n\n# Get relation classes\n# Visual Genome\n# relation_classes = relation_class_by_freq()\n# 3D SSG\nwith open('3DSSG/vlsat/data/3DSSG_subset/relationships.txt', 'r') as file:\n    relations = [line.strip() for line in file]\nrelation_classes = {}\nfor i, relation in enumerate(relations):\n    relation_classes[i] = relation\nprint(relation_classes)\n\n# Initialize tokenizers and models for GPT-2, BERT, and CLIP\n# gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n# gpt2_model = GPT2Model.from_pretrained(\"gpt2\")\n#\n# bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n# bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n\nclip_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\nclip_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n\n# Function to get embeddings\ndef get_embeddings(model, tokenizer, sentences):\n    # Set padding token if not defined\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token if tokenizer.eos_token is not None else '[PAD]'\n\n    # Tokenize input\n    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n    # Get embeddings\n    with torch.no_grad():\n        outputs = model(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).numpy()\n\n\n# Get embeddings for each model\n# gpt2_embeddings = get_embeddings(gpt2_model, gpt2_tokenizer, list(relation_classes.values()))\n# bert_embeddings = get_embeddings(bert_model, bert_tokenizer, list(relation_classes.values()))\nclip_embeddings = get_embeddings(clip_model, clip_tokenizer, list(relation_classes.values()))\n\n\n# Function for clustering and index mapping\ndef cluster_and_map(embeddings, relation_names, n_clusters):\n    # Perform k-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=4).fit(embeddings)\n    # Create a dictionary to map original index to cluster\n    cluster_assignment = {i: cluster for i, cluster in enumerate(kmeans.labels_)}\n\n    # Sort relation classes by cluster and create a new index map\n    sorted_relations = sorted(enumerate(relation_names), key=lambda x: cluster_assignment[x[0]])\n\n    # Create a new index map (as a tensor) that maps original index to new sorted index\n    new_index_map = torch.zeros(len(relation_names), dtype=torch.long)\n    for new_idx, (original_idx, _) in enumerate(sorted_relations):\n        new_index_map[original_idx] = new_idx\n\n    # Map each relation name to its cluster center\n    cluster_map = {i: [relation_classes[key] for key in relation_classes.keys() if cluster_assignment[key] == i] for i in range(n_clusters)}\n    return cluster_map, new_index_map\n\n\n# Perform clustering and create index maps for each set of embeddings\nrelation_names = list(relation_classes.values())\nn_clusters = 3\n\n# gpt2_cluster_map, gpt2_index_map = cluster_and_map(gpt2_embeddings, relation_names, n_clusters)\n# bert_cluster_map, bert_index_map = cluster_and_map(bert_embeddings, relation_names, n_clusters)\nclip_cluster_map, clip_index_map = cluster_and_map(clip_embeddings, relation_names, n_clusters)\n# print('gpt2_cluster_map', [len(gpt2_cluster_map[key]) for key in gpt2_cluster_map.keys()], gpt2_cluster_map, '\\ngpt2_index_map', gpt2_index_map, '\\n\\n',\n#       'bert_cluster_map', [len(bert_cluster_map[key]) for key in bert_cluster_map.keys()], bert_cluster_map, '\\nbert_index_map', bert_index_map, '\\n\\n',\n#       'clip_cluster_map', [len(clip_cluster_map[key]) for key in clip_cluster_map.keys()], clip_cluster_map, '\\nclip_index_map', clip_index_map)\nprint('clip_cluster_map', [len(clip_cluster_map[key]) for key in clip_cluster_map.keys()], clip_cluster_map, '\\nclip_index_map', clip_index_map)\n\n\n# Function to plot t-SNE for given embeddings\ndef plot_tsne_embeddings(gpt2_emb, bert_emb, clip_emb, relation_classes):\n    # Perform t-SNE dimensionality reduction\n    tsne = TSNE(n_components=2, random_state=5)\n    gpt2_tsne = tsne.fit_transform(gpt2_emb)\n    bert_tsne = tsne.fit_transform(bert_emb)\n    clip_tsne = tsne.fit_transform(clip_emb)\n\n    # Dummy cluster assignments for illustration (replace with actual clustering method)\n    clusters = np.random.randint(0, 3, len(relation_classes))\n\n    # Create a figure with 3 subplots\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n    # Plot for GPT-2\n    for i, label in enumerate(relation_classes.values()):\n        axes[0].scatter(gpt2_tsne[i, 0], gpt2_tsne[i, 1], c=['r', 'g', 'b'][clusters[i]], label=label if i == 0 else \"\")\n        axes[0].text(gpt2_tsne[i, 0], gpt2_tsne[i, 1], label, fontsize=9)\n    axes[0].set_title('GPT-2 Embeddings')\n\n    # Plot for BERT\n    for i, label in enumerate(relation_classes.values()):\n        axes[1].scatter(bert_tsne[i, 0], bert_tsne[i, 1], c=['r', 'g', 'b'][clusters[i]], label=label if i == 0 else \"\")\n        axes[1].text(bert_tsne[i, 0], bert_tsne[i, 1], label, fontsize=9)\n    axes[1].set_title('BERT Embeddings')\n\n    # Plot for CLIP\n    for i, label in enumerate(relation_classes.values()):\n        axes[2].scatter(clip_tsne[i, 0], clip_tsne[i, 1], c=['r', 'g', 'b'][clusters[i]], label=label if i == 0 else \"\")\n        axes[2].text(clip_tsne[i, 0], clip_tsne[i, 1], label, fontsize=9)\n    axes[2].set_title('CLIP Embeddings')\n\n    # Adjust layout\n    plt.tight_layout()\n\n    # Save the figure\n    plt.savefig('tsne_embeddings.png')\n\n# plot_tsne_embeddings(gpt2_embeddings, bert_embeddings, clip_embeddings, relation_classes)"}
{"type": "source_file", "path": "utils.py", "content": "import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom tqdm import tqdm\nimport os\nimport json\nimport torchmetrics\nimport torch.nn as nn\nfrom torch import Tensor\nfrom typing import Optional, List\nimport torchvision\nimport math\nfrom collections import Counter, OrderedDict\nimport re\nimport random\n\n\ndef collate_fn(batch):\n    \"\"\"\n    This function solves the problem when some data samples in a batch are None.\n    :param batch: the current batch in dataloader\n    :return: a new batch with all non-None data samples\n    \"\"\"\n    batch = list(filter(lambda x: x is not None, batch))\n    return tuple(zip(*batch))\n\n\ndef super_relation_processing(args, connected, curr_relations_target):\n    # Clone the target relations for modification without affecting the original data\n    super_relation_target = curr_relations_target[connected].clone()\n    super_relation_target[super_relation_target < args['models']['num_geometric']] = 0\n    super_relation_target[torch.logical_and(super_relation_target >= args['models']['num_geometric'], super_relation_target < args['models']['num_geometric'] + args['models']['num_possessive'])] = 1\n    super_relation_target[super_relation_target >= args['models']['num_geometric'] + args['models']['num_possessive']] = 2\n\n    return super_relation_target\n\n\ndef resize_boxes(boxes, original_size, new_size):\n    \"\"\"\n    This function resizes an object bounding box.\n    :param boxes: original bounding box\n    :param original_size: original image size\n    :param new_size: target image size\n    :return: the resized bounding box\n    \"\"\"\n    ratios = [s / s_orig for s, s_orig in zip(new_size, original_size)]\n    ratio_height, ratio_width = ratios\n    xmin, ymin, xmax, ymax = boxes[0], boxes[1], boxes[2], boxes[3]\n\n    xmin = xmin * ratio_width\n    xmax = xmax * ratio_width\n    ymin = ymin * ratio_height\n    ymax = ymax * ratio_height\n\n    return [int(xmin), int(ymin), int(xmax), int(ymax)]\n\n\ndef iou(bbox_target, bbox_pred):\n    \"\"\"\n    This function calculates the IOU score between two bounding boxes.\n    :param bbox_target: target bounding box\n    :param bbox_pred: predicted bounding box\n    :return: the IOU score\n    \"\"\"\n    mask_pred = torch.zeros(32, 32)\n    mask_pred[int(bbox_pred[0]):int(bbox_pred[1]), int(bbox_pred[2]):int(bbox_pred[3])] = 1\n    mask_target = torch.zeros(32, 32)\n    mask_target[int(bbox_target[0]):int(bbox_target[1]), int(bbox_target[2]):int(bbox_target[3])] = 1\n    intersect = torch.sum(torch.logical_and(mask_target, mask_pred))\n    union = torch.sum(torch.logical_or(mask_target, mask_pred))\n    if union == 0:\n        return 0\n    else:\n        return float(intersect) / float(union)\n\n\ndef find_union_bounding_box(bbox1, bbox2):\n    # bbox expects format x_min, x_max, y_min, y_max\n    [x1min, x1max, y1min, y1max] = bbox1\n    [x2min, x2max, y2min, y2max] = bbox2\n    xmin = min(x1min, x2min)\n    xmax = max(x1max, x2max)\n    ymin = min(y1min, y2min)\n    ymax = max(y1max, y2max)\n    return xmin, xmax, ymin, ymax\n\n\ndef build_detr101(args):\n    \"\"\"\n    This function builds the DETR-101 object detection backbone.\n    It loads the model from source, change key names in the model state dict if needed,\n    and loads state dict from a pretrained checkpoint\n    :param args: input arguments in config.yaml file\n    :return: the pretrained model\n    \"\"\"\n    with open(args['models']['detr101_key_before'], 'r') as f:\n        name_before = f.readlines()\n        name_before = [line[:-1] for line in name_before]\n    with open(args['models']['detr101_key_after'], 'r') as f:\n        name_after = f.readlines()\n        name_after = [line[:-1] for line in name_after]\n\n    if args['dataset']['dataset'] == 'vg':\n        model_path = args['models']['detr101_pretrained_vg']\n    else:\n        model_path = args['models']['detr101_pretrained_oiv6']\n    model_param = torch.load(model_path)\n\n    keys = [key for key in model_param['model'] if key in name_before]\n    for idx, key in enumerate(keys):\n        model_param['model'][name_after[idx]] = model_param['model'].pop(key)\n        # print(idx, key, ' -> ', name_after[idx])\n\n    model = torch.hub.load('facebookresearch/detr:main', 'detr_resnet101', pretrained=False)\n    if args['dataset']['dataset'] == 'vg':\n        model.class_embed = nn.Linear(256, 151)\n    else:\n        model.class_embed = nn.Linear(256, 602)\n    model.load_state_dict(model_param['model'], strict=False) # every param except \"criterion.empty_weight\"\n    return model\n\n\ndef get_embeddings(model, tokenizer, sentences):\n    # Set padding token if not defined\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token if tokenizer.eos_token is not None else '[PAD]'\n\n    # Tokenize input\n    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n    # Get embeddings\n    with torch.no_grad():\n        outputs = model(**inputs)\n    return outputs.last_hidden_state.mean(dim=1)\n\n\ndef process_super_class(s1, s2, num_super_classes, rank):\n    sc1 = F.one_hot(torch.tensor([s[0] for s in s1]), num_classes=num_super_classes)\n    for i in range(1, 4):  # at most 4 diff super class for each subclass instance\n        idx = torch.nonzero(torch.tensor([len(s) == i + 1 for s in s1])).view(-1)\n        if len(idx) > 0:\n            sc1[idx] += F.one_hot(torch.tensor([s[i] for s in [s1[j] for j in idx]]), num_classes=num_super_classes)\n    sc2 = F.one_hot(torch.tensor([s[0] for s in s2]), num_classes=num_super_classes)\n    for i in range(1, 4):\n        idx = torch.nonzero(torch.tensor([len(s) == i + 1 for s in s2])).view(-1)\n        if len(idx) > 0:\n            sc2[idx] += F.one_hot(torch.tensor([s[i] for s in [s2[j] for j in idx]]), num_classes=num_super_classes)\n\n    sc1, sc2 = sc1.to(rank), sc2.to(rank)\n    return sc1, sc2\n\n\n# https://github.com/yrcong/RelTR/blob/main/util/misc.py\nclass NestedTensor(object):\n    def __init__(self, tensors, mask: Optional[Tensor]):\n        self.tensors = tensors\n        self.mask = mask\n\n    def to(self, device):\n        # type: (Device) -> NestedTensor # noqa\n        cast_tensor = self.tensors.to(device)\n        mask = self.mask\n        if mask is not None:\n            assert mask is not None\n            cast_mask = mask.to(device)\n        else:\n            cast_mask = None\n        return NestedTensor(cast_tensor, cast_mask)\n\n    def decompose(self):\n        return self.tensors, self.mask\n\n    def __repr__(self):\n        return str(self.tensors)\n\n# https://github.com/yrcong/RelTR/blob/main/util/misc.py\ndef _max_by_axis(the_list):\n    # type: (List[List[int]]) -> List[int]\n    maxes = the_list[0]\n    for sublist in the_list[1:]:\n        for index, item in enumerate(sublist):\n            maxes[index] = max(maxes[index], item)\n    return maxes\n\n# https://github.com/yrcong/RelTR/blob/main/util/misc.py\ndef nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n    if tensor_list[0].ndim == 3:\n        if torchvision._is_tracing():\n            # nested_tensor_from_tensor_list() does not export well to ONNX\n            # call _onnx_nested_tensor_from_tensor_list() instead\n            return _onnx_nested_tensor_from_tensor_list(tensor_list)\n\n        max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n        batch_shape = [len(tensor_list)] + max_size\n        b, c, h, w = batch_shape\n        dtype = tensor_list[0].dtype\n        device = tensor_list[0].device\n        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n        mask = torch.ones((b, h, w), dtype=torch.bool, device=device)\n        for img, pad_img, m in zip(tensor_list, tensor, mask):\n            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n            m[: img.shape[1], :img.shape[2]] = False\n    else:\n        raise ValueError('not supported')\n    return NestedTensor(tensor, mask)\n\n\ndef remove_ddp_module_in_weights(saved_state_dict):\n    # Handle key renaming for matching\n    renamed_state_dict = {}\n    for k, v in saved_state_dict.items():\n        # Remove 'module.' prefix if it exists\n        k = k.replace('module.', '')\n        renamed_state_dict[k] = v\n    return renamed_state_dict\n\n\ndef match_bbox(bbox_semi, bbox_raw, eval_mode):\n    \"\"\"\n    Returns the index of the bounding box from bbox_raw that most closely matches the pseudo bbox.\n    \"\"\"\n    if eval_mode == 'pc':\n        for idx, bbox in enumerate(bbox_raw):\n            if torch.sum(torch.abs(bbox - torch.as_tensor(bbox_semi))) == 0:\n                return idx\n        return None\n    else:\n        ious = calculate_iou_for_all(bbox_semi, bbox_raw)\n        return torch.argmax(ious).item()\n\n\ndef calculate_iou_for_all(box1, boxes):\n    \"\"\"\n    Calculate the Intersection over Union (IoU) of a bounding box with a set of bounding boxes.\n    \"\"\"\n    x1 = torch.max(box1[0], boxes[:, 0])\n    y1 = torch.max(box1[1], boxes[:, 1])\n    x2 = torch.min(box1[2], boxes[:, 2])\n    y2 = torch.min(box1[3], boxes[:, 3])\n\n    inter_area = torch.clamp(x2 - x1 + 1, 0) * torch.clamp(y2 - y1 + 1, 0)\n\n    box1_area = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n    boxes_area = (boxes[:, 2] - boxes[:, 0] + 1) * (boxes[:, 3] - boxes[:, 1] + 1)\n\n    union_area = box1_area + boxes_area - inter_area\n\n    return inter_area / union_area\n\n\ndef get_num_each_class():  # number of training data in total for each relationship class\n    return torch.tensor([712432, 277943, 251756, 146339, 136099, 96589, 66425, 47342, 42722,\n                         41363, 22596, 18643, 15457, 14185, 13715, 10191, 9903, 9894, 9317, 9145, 8856,\n                         5213, 4688, 4613, 3810, 3806, 3739, 3624, 3490, 3477, 3411, 3288, 3095, 3092,\n                         3083, 2945, 2721, 2517, 2380, 2312, 2253, 2241, 2065, 1996, 1973, 1925, 1914,\n                         1869, 1853, 1740])\n\n\ndef get_num_each_class_reordered(args):  # number of training data in total for each relationship class\n    if args['dataset']['dataset'] == 'vg':\n        return torch.tensor([47342, 1996, 3092, 3624, 3477, 9903, 41363, 3411, 251756,\n                             13715, 96589, 712432, 1914, 9317, 22596, 3288, 9145, 2945,\n                             277943, 2312, 146339, 2065, 2517, 136099, 15457, 66425, 10191,\n                             5213, 2312, 3806, 4688, 1973, 1853, 9894, 42722, 3739,\n                             3083, 1869, 2253, 3095, 2721, 3810, 8856, 2241, 18643,\n                             14185, 1925, 1740, 4613, 3490])\n    else:\n        return torch.tensor([150983, 7665, 841, 455, 9402, 52561, 145480, 157, 175, 77, 27, 4827, 1146, 198, 77, 1,\n                             12, 4, 43, 702, 8, 1111, 51, 43, 367, 10, 462, 11, 2094, 114])\n\ndef get_weight_oiv6():\n    num = torch.tensor([1974, 120, 27, 2, 284, 571, 2059, 8, 26, 2, 0, 163, 25, 30, 2, 0, 0, 1, 0, 17, 0, 29, 14, 4, 3, 0, 6, 0, 67, 5]) + 1\n    # freq = num / torch.sum(num)\n    # print(1 / freq)\n    return num\n\ndef get_distri_over_classes():  # distribution of training data over all relationship class\n    return torch.tensor([0.3482, 0.1358, 0.1230, 0.0715, 0.0665, 0.0472, 0.0325, 0.0231, 0.0209,\n                         0.0202, 0.0110, 0.0091, 0.0076, 0.0069, 0.0067, 0.0050, 0.0048, 0.0048,\n                         0.0046, 0.0045, 0.0043, 0.0025, 0.0023, 0.0023, 0.0019, 0.0019, 0.0018,\n                         0.0018, 0.0017, 0.0017, 0.0017, 0.0016, 0.0015, 0.0015, 0.0015, 0.0014,\n                         0.0013, 0.0012, 0.0012, 0.0011, 0.0011, 0.0011, 0.0010, 0.0010, 0.0010,\n                         0.0009, 0.0009, 0.0009, 0.0009, 0.0009])\n\n\ndef get_accumu_over_classes():  # accumulation of training data distributions over all relationship class\n    return torch.tensor([0.3482, 0.4840, 0.6070, 0.6785, 0.7450, 0.7922, 0.8247, 0.8478,\n                         0.8687, 0.8889, 0.8999, 0.9090, 0.9166, 0.9235, 0.9302, 0.9352, 0.9400,\n                         0.9448, 0.9494, 0.9539, 0.9582, 0.9607, 0.9630, 0.9653, 0.9672, 0.9691,\n                         0.9709, 0.9727, 0.9744, 0.9761, 0.9778, 0.9794, 0.9809, 0.9824, 0.9839,\n                         0.9853, 0.9866, 0.9878, 0.9890, 0.9901, 0.9912, 0.9923, 0.9933, 0.9943,\n                         0.9953, 0.9962, 0.9971, 0.9980, 0.9989, 0.9998])\n\n\ndef match_target_sgd(rank, relationships, subj_or_obj, categories_target, bbox_target):\n    \"\"\"\n    this function returns the target direction and relationship for scene graph detection, i.e., with predicted bbox and clf\n    the predicted bbox and target bbox might be two different sets of bbox\n    so we can not use the original sets of target direction and relationships\n    \"\"\"\n    cat_subject_target = []\n    cat_object_target = []\n    bbox_subject_target = []\n    bbox_object_target = []\n    relation_target = []     # the target relation for target sets of triplets, not for predicted sets\n    for image_idx in range(len(relationships)):\n        curr_cat_subject = None\n        curr_cat_object = None\n        curr_bbox_subject = None\n        curr_bbox_object = None\n        curr_relation_target = None\n\n        for graph_iter in range(len(relationships[image_idx])):\n            for edge_iter in range(graph_iter):\n                if subj_or_obj[image_idx][graph_iter-1][edge_iter] == 1:\n                    if curr_cat_subject is None:\n                        curr_cat_subject = torch.tensor([categories_target[image_idx][graph_iter]]).to(rank)\n                        curr_cat_object = torch.tensor([categories_target[image_idx][edge_iter]]).to(rank)\n                        curr_bbox_subject = bbox_target[image_idx][graph_iter]\n                        curr_bbox_object = bbox_target[image_idx][edge_iter]\n                        curr_relation_target = torch.tensor([relationships[image_idx][graph_iter-1][edge_iter]]).to(rank)\n                    else:\n                        curr_cat_subject = torch.hstack((curr_cat_subject, categories_target[image_idx][graph_iter]))\n                        curr_cat_object = torch.hstack((curr_cat_object, categories_target[image_idx][edge_iter]))\n                        curr_bbox_subject = torch.vstack((curr_bbox_subject, bbox_target[image_idx][graph_iter]))\n                        curr_bbox_object = torch.vstack((curr_bbox_object, bbox_target[image_idx][edge_iter]))\n                        curr_relation_target = torch.hstack((curr_relation_target, relationships[image_idx][graph_iter-1][edge_iter].to(rank)))\n\n                elif subj_or_obj[image_idx][graph_iter-1][edge_iter] == 0:\n                    if curr_cat_subject is None:\n                        curr_cat_subject = torch.tensor([categories_target[image_idx][edge_iter]]).to(rank)\n                        curr_cat_object = torch.tensor([categories_target[image_idx][graph_iter]]).to(rank)\n                        curr_bbox_subject = bbox_target[image_idx][edge_iter]\n                        curr_bbox_object = bbox_target[image_idx][graph_iter]\n                        curr_relation_target = torch.tensor([relationships[image_idx][graph_iter-1][edge_iter]]).to(rank)\n                    else:\n                        curr_cat_subject = torch.hstack((curr_cat_subject, categories_target[image_idx][edge_iter]))\n                        curr_cat_object = torch.hstack((curr_cat_object, categories_target[image_idx][graph_iter]))\n                        curr_bbox_subject = torch.vstack((curr_bbox_subject, bbox_target[image_idx][edge_iter]))\n                        curr_bbox_object = torch.vstack((curr_bbox_object, bbox_target[image_idx][graph_iter]))\n                        curr_relation_target = torch.hstack((curr_relation_target, relationships[image_idx][graph_iter-1][edge_iter].to(rank)))\n\n        cat_subject_target.append(curr_cat_subject)\n        cat_object_target.append(curr_cat_object)\n        if curr_relation_target is not None:\n            bbox_subject_target.append(curr_bbox_subject.view(-1, 4))\n            bbox_object_target.append(curr_bbox_object.view(-1, 4))\n        else:\n            bbox_subject_target.append(curr_bbox_subject)\n            bbox_object_target.append(curr_bbox_object)\n        relation_target.append(curr_relation_target)\n\n    return cat_subject_target, cat_object_target, bbox_subject_target, bbox_object_target, relation_target\n\n\ndef compare_object_cat(pred_cat, target_cat):\n    # man, person, woman, people, boy, girl, lady, child, kid, men  # tree, plant  # plane, airplane\n    equiv = [[1, 5, 11, 23, 38, 44, 121, 124, 148, 149], [0, 50], [92, 137]]\n    # vehicle -> car, bus, motorcycle, truck, vehicle\n    # animal -> zebra, sheep, horse, giraffe, elephant, dog, cow, cat, bird, bear, animal\n    # food -> vegetable, pizza, orange, fruit, banana, food\n    unsymm_equiv = {123: [14, 63, 95, 87, 123], 108: [89, 102, 67, 72, 71, 81, 96, 105, 90, 111, 108], 60: [145, 106, 142, 144, 77, 60]}\n\n    if pred_cat == target_cat:\n        return True\n    for group in equiv:\n        if pred_cat in group and target_cat in group:\n            return True\n    for key in unsymm_equiv:\n        if pred_cat == key and target_cat in unsymm_equiv[key]:\n            return True\n        elif target_cat == key and pred_cat in unsymm_equiv[key]:\n            return True\n    return False\n\n\ndef match_object_categories(categories_pred, cat_pred_confidence, bbox_pred, bbox_target):\n    \"\"\"\n    This function matches the predicted object category for each ground-truth bounding box.\n    For each ground-truth bounding box, the function finds the predicted bounding box with the largest IOU\n    and regards its predicted object category as the predicted object category of the ground-truth bounding box\n    :param categories_pred: a tensor of size N, where N is the number of predicted objects\n    :param cat_pred_confidence: a tensor of size N, where N is the number of predicted objects\n    :param bbox_pred: a tensor of size Nx4, where N is the number of predicted objects\n    :param bbox_target: a tensor of size Mx4, where M is the number of ground-truth objects\n    \"\"\"\n    categories_pred_matched = []\n    categories_pred_conf_matched = []\n    bbox_target_matched = bbox_target.copy()\n    if len(bbox_target) != len(bbox_pred):   # batch size\n        return None, None, None\n\n    for i in range(len(bbox_target)):\n        repeat_count = 0\n        assert len(categories_pred[i]) == len(bbox_pred[i])\n        curr_categories_pred_matched = []\n        curr_categories_pred_conf_matched = []\n\n        for k, curr_bbox_target in enumerate(bbox_target[i]):\n            all_ious = []\n            for j, curr_bbox_pred in enumerate(bbox_pred[i]):\n                all_ious.append(iou(curr_bbox_target, curr_bbox_pred))\n            if len(all_ious) < 2:\n                return None, None, None\n            top_ious = torch.topk(torch.tensor(all_ious), 2)\n\n            # if top two come from the same repeated bounding box\n            if top_ious[0][0] == top_ious[0][1]:\n                curr_categories_pred_matched.append(categories_pred[i][top_ious[1][0]])\n                curr_categories_pred_matched.append(categories_pred[i][top_ious[1][1]])\n                curr_categories_pred_conf_matched.append(cat_pred_confidence[i][top_ious[1][0]] * top_ious[0][0])\n                curr_categories_pred_conf_matched.append(cat_pred_confidence[i][top_ious[1][1]] * top_ious[0][1])\n                # repeat the curr_bbox_target\n                bbox_target_matched[i] = torch.cat([bbox_target_matched[i][:k+repeat_count], bbox_target_matched[i][k+repeat_count].view(1, 4),\n                                                    bbox_target_matched[i][k+repeat_count:]])\n                repeat_count += 1\n            else:\n                curr_categories_pred_matched.append(categories_pred[i][top_ious[1][0]])\n                curr_categories_pred_conf_matched.append(cat_pred_confidence[i][top_ious[1][0]] * top_ious[0][0])\n\n        categories_pred_matched.append(curr_categories_pred_matched)\n        categories_pred_conf_matched.append(curr_categories_pred_conf_matched)\n    return categories_pred_matched, categories_pred_conf_matched, bbox_target_matched\n\n\ndef record_train_results(args, record, rank, epoch, batch_count, lr, recall_top3, recall, mean_recall_top3, mean_recall, recall_zs, mean_recall_zs,\n                         running_losses, running_loss_relationship, running_loss_contrast, running_loss_connectivity, running_loss_commonsense,\n                         connectivity_recall, num_connected, num_not_connected, connectivity_precision, num_connected_pred, wmap_rel, wmap_phrase):\n\n    if args['dataset']['dataset'] == 'vg':\n        print('TRAIN, rank %d, epoch %d, batch %d, lr: %.7f, R@k: %.4f, %.4f, %.4f, mR@k: %.4f, %.4f, %.4f, loss: %.4f, %.4f, %.4f, %.4f.'\n              % (rank, epoch, batch_count, lr, recall[0], recall[1], recall[2], mean_recall[0], mean_recall[1], mean_recall[2],\n                 running_loss_relationship / (args['training']['print_freq'] * args['training']['batch_size']),\n                 running_loss_contrast / (args['training']['print_freq'] * args['training']['batch_size']),\n                 running_loss_connectivity / (args['training']['print_freq'] * args['training']['batch_size']),\n                 running_loss_commonsense / (args['training']['print_freq'] * args['training']['batch_size'])))\n\n        record.append({'rank': rank, 'epoch': epoch, 'batch': batch_count, 'lr': lr,\n                       'recall_relationship': [recall[0], recall[1], recall[2]],\n                       'mean_recall': [mean_recall[0].item(), mean_recall[1].item(), mean_recall[2].item()],\n                       'zero_shot_recall': [recall_zs[0], recall_zs[1], recall_zs[2]],\n                       'mean_zero_shot_recall': [mean_recall_zs[0].item(), mean_recall_zs[1].item(), mean_recall_zs[2].item()],\n                       'relationship_loss': running_loss_relationship.item() / (args['training']['print_freq'] * args['training']['batch_size'])})\n    else:\n        print('TRAIN, rank %d, epoch %d, batch %d, lr: %.7f, R@k: %.4f, %.4f, %.4f, mR@k: %.4f, %.4f, %.4f, '\n              'wmap_rel: %.4f, wmap_phrase: %.4f, loss: %.4f, %.4f, conn: %.4f, %.4f.'\n              % (rank, epoch, batch_count, lr, recall[0], recall[1], recall[2], mean_recall[0], mean_recall[1], mean_recall[2], wmap_rel, wmap_phrase,\n                 running_loss_relationship / (args['training']['print_freq'] * args['training']['batch_size']),\n                 running_loss_connectivity / (args['training']['print_freq'] * args['training']['batch_size']),\n                 connectivity_recall / (num_connected + 1e-5), connectivity_precision / (num_connected_pred + 1e-5)))\n\n        record.append({'rank': rank, 'epoch': epoch, 'batch': batch_count, 'lr': lr,\n                       'recall_relationship': [recall[0], recall[1], recall[2]],\n                       'mean_recall': [mean_recall[0].item(), mean_recall[1].item(), mean_recall[2].item()],\n                       'wmap_rel': wmap_rel.item(), 'wmap_phrase': wmap_phrase.item(),\n                       'connectivity_recall': connectivity_recall.item() / (num_connected + 1e-5), 'connectivity_precision': connectivity_precision.item() / (num_connected_pred + 1e-5),\n                       'total_losses': running_losses / (args['training']['print_freq'] * args['training']['batch_size']),\n                       'relationship_loss': running_loss_relationship.item() / (args['training']['print_freq'] * args['training']['batch_size']),\n                       'connectivity_loss': running_loss_connectivity.item() / (args['training']['print_freq'] * args['training']['batch_size']),\n                       'num_connected': num_connected, 'num_not_connected': num_not_connected})\n    with open(args['training']['result_path'] + 'train_results_' + str(rank) + '.json', 'w') as f:\n        json.dump(record, f)\n\n\ndef record_test_results(args, test_record, rank, epoch, recall_top3, recall, mean_recall_top3, mean_recall, recall_zs, mean_recall_zs,\n                        connectivity_recall, num_connected, num_not_connected, connectivity_precision, num_connected_pred, wmap_rel, wmap_phrase):\n\n    if args['dataset']['dataset'] == 'vg':\n        print('TEST, rank: %d, epoch: %d, R@k: %.4f, %.4f, %.4f, mR@k: %.4f, %.4f, %.4f'\n              % (rank, epoch, recall[0], recall[1], recall[2], mean_recall[0], mean_recall[1], mean_recall[2]))\n\n        test_record.append({'rank': rank, 'epoch': epoch, 'recall_relationship': [recall[0], recall[1], recall[2]],\n                            'mean_recall': [mean_recall[0].item(), mean_recall[1].item(), mean_recall[2].item()],\n                            'num_connected': num_connected, 'num_not_connected': num_not_connected})\n    else:\n        print('TEST, rank: %d, epoch: %d, R@k: %.4f, %.4f, %.4f, mR@k: %.4f, %.4f, %.4f, wmap_rel: %.4f, wmap_phrase: %.4f, conn: %.4f, %.4f.'\n              % (rank, epoch, recall[0], recall[1], recall[2], mean_recall[0], mean_recall[1], mean_recall[2], wmap_rel, wmap_phrase,\n                 connectivity_recall / (num_connected + 1e-5), connectivity_precision / (num_connected_pred + 1e-5)))\n\n        test_record.append({'rank': rank, 'epoch': epoch, 'recall_relationship': [recall[0], recall[1], recall[2]],\n                            'wmap_rel': wmap_rel.item(), 'wmap_phrase': wmap_phrase.item(),\n                            'connectivity_recall': connectivity_recall.item() / (num_connected + 1e-5),\n                            'connectivity_precision': connectivity_precision.item() / (num_connected_pred + 1e-5),\n                            'mean_recall': [mean_recall[0].item(), mean_recall[1].item(), mean_recall[2].item()],\n                            'num_connected': num_connected, 'num_not_connected': num_not_connected})\n\n        with open(args['training']['result_path'] + 'test_results_' + str(rank) + '.json', 'w') as f:  # append current logs\n            json.dump(test_record, f)\n"}
{"type": "source_file", "path": "sup_contrast/main_linear.py", "content": "from __future__ import print_function\n\nimport sys\nimport argparse\nimport time\nimport math\n\nimport torch\nimport torch.backends.cudnn as cudnn\n\nfrom main_ce import set_loader\nfrom util import AverageMeter\nfrom util import adjust_learning_rate, warmup_learning_rate, accuracy\nfrom util import set_optimizer\nfrom networks.resnet_big import SupConResNet, LinearClassifier\n\ntry:\n    import apex\n    from apex import amp, optimizers\nexcept ImportError:\n    pass\n\n\ndef parse_option():\n    parser = argparse.ArgumentParser('argument for training')\n\n    parser.add_argument('--print_freq', type=int, default=10,\n                        help='print frequency')\n    parser.add_argument('--save_freq', type=int, default=50,\n                        help='save frequency')\n    parser.add_argument('--batch_size', type=int, default=256,\n                        help='batch_size')\n    parser.add_argument('--num_workers', type=int, default=16,\n                        help='num of workers to use')\n    parser.add_argument('--epochs', type=int, default=100,\n                        help='number of training epochs')\n\n    # optimization\n    parser.add_argument('--learning_rate', type=float, default=0.1,\n                        help='learning rate')\n    parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90',\n                        help='where to decay lr, can be a list')\n    parser.add_argument('--lr_decay_rate', type=float, default=0.2,\n                        help='decay rate for learning rate')\n    parser.add_argument('--weight_decay', type=float, default=0,\n                        help='weight decay')\n    parser.add_argument('--momentum', type=float, default=0.9,\n                        help='momentum')\n\n    # model dataset\n    parser.add_argument('--model', type=str, default='resnet50')\n    parser.add_argument('--dataset', type=str, default='cifar10',\n                        choices=['cifar10', 'cifar100'], help='dataset')\n\n    # other setting\n    parser.add_argument('--cosine', action='store_true',\n                        help='using cosine annealing')\n    parser.add_argument('--warm', action='store_true',\n                        help='warm-up for large batch training')\n\n    parser.add_argument('--ckpt', type=str, default='',\n                        help='path to pre-trained model')\n\n    opt = parser.parse_args()\n\n    # set the path according to the environment\n    opt.data_folder = './datasets/'\n\n    iterations = opt.lr_decay_epochs.split(',')\n    opt.lr_decay_epochs = list([])\n    for it in iterations:\n        opt.lr_decay_epochs.append(int(it))\n\n    opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'.\\\n        format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,\n               opt.batch_size)\n\n    if opt.cosine:\n        opt.model_name = '{}_cosine'.format(opt.model_name)\n\n    # warm-up for large-batch training,\n    if opt.warm:\n        opt.model_name = '{}_warm'.format(opt.model_name)\n        opt.warmup_from = 0.01\n        opt.warm_epochs = 10\n        if opt.cosine:\n            eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)\n            opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (\n                    1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2\n        else:\n            opt.warmup_to = opt.learning_rate\n\n    if opt.dataset == 'cifar10':\n        opt.n_cls = 10\n    elif opt.dataset == 'cifar100':\n        opt.n_cls = 100\n    else:\n        raise ValueError('dataset not supported: {}'.format(opt.dataset))\n\n    return opt\n\n\ndef set_model(opt):\n    model = SupConResNet(name=opt.model)\n    criterion = torch.nn.CrossEntropyLoss()\n\n    classifier = LinearClassifier(name=opt.model, num_classes=opt.n_cls)\n\n    ckpt = torch.load(opt.ckpt, map_location='cpu')\n    state_dict = ckpt['model']\n\n    if torch.cuda.is_available():\n        if torch.cuda.device_count() > 1:\n            model.encoder = torch.nn.DataParallel(model.encoder)\n        else:\n            new_state_dict = {}\n            for k, v in state_dict.items():\n                k = k.replace(\"module.\", \"\")\n                new_state_dict[k] = v\n            state_dict = new_state_dict\n        model = model.cuda()\n        classifier = classifier.cuda()\n        criterion = criterion.cuda()\n        cudnn.benchmark = True\n\n        model.load_state_dict(state_dict)\n\n    return model, classifier, criterion\n\n\ndef train(train_loader, model, classifier, criterion, optimizer, epoch, opt):\n    \"\"\"one epoch training\"\"\"\n    model.eval()\n    classifier.train()\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n\n    end = time.time()\n    for idx, (images, labels) in enumerate(train_loader):\n        data_time.update(time.time() - end)\n\n        images = images.cuda(non_blocking=True)\n        labels = labels.cuda(non_blocking=True)\n        bsz = labels.shape[0]\n\n        # warm-up learning rate\n        warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)\n\n        # compute loss\n        with torch.no_grad():\n            features = model.encoder(images)\n        output = classifier(features.detach())\n        loss = criterion(output, labels)\n\n        # update metric\n        losses.update(loss.item(), bsz)\n        acc1, acc5 = accuracy(output, labels, topk=(1, 5))\n        top1.update(acc1[0], bsz)\n\n        # SGD\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # print info\n        if (idx + 1) % opt.print_freq == 0:\n            print('Train: [{0}][{1}/{2}]\\t'\n                  'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                  'DT {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n                  'loss {loss.val:.3f} ({loss.avg:.3f})\\t'\n                  'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n                   epoch, idx + 1, len(train_loader), batch_time=batch_time,\n                   data_time=data_time, loss=losses, top1=top1))\n            sys.stdout.flush()\n\n    return losses.avg, top1.avg\n\n\ndef validate(val_loader, model, classifier, criterion, opt):\n    \"\"\"validation\"\"\"\n    model.eval()\n    classifier.eval()\n\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n\n    with torch.no_grad():\n        end = time.time()\n        for idx, (images, labels) in enumerate(val_loader):\n            images = images.float().cuda()\n            labels = labels.cuda()\n            bsz = labels.shape[0]\n\n            # forward\n            output = classifier(model.encoder(images))\n            loss = criterion(output, labels)\n\n            # update metric\n            losses.update(loss.item(), bsz)\n            acc1, acc5 = accuracy(output, labels, topk=(1, 5))\n            top1.update(acc1[0], bsz)\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if idx % opt.print_freq == 0:\n                print('Test: [{0}/{1}]\\t'\n                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                      'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n                       idx, len(val_loader), batch_time=batch_time,\n                       loss=losses, top1=top1))\n\n    print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))\n    return losses.avg, top1.avg\n\n\ndef main():\n    best_acc = 0\n    opt = parse_option()\n\n    # build data loader\n    train_loader, val_loader = set_loader(opt)\n\n    # build model and criterion\n    model, classifier, criterion = set_model(opt)\n\n    # build optimizer\n    optimizer = set_optimizer(opt, classifier)\n\n    # training routine\n    for epoch in range(1, opt.epochs + 1):\n        adjust_learning_rate(opt, optimizer, epoch)\n\n        # train for one epoch\n        time1 = time.time()\n        loss, acc = train(train_loader, model, classifier, criterion,\n                          optimizer, epoch, opt)\n        time2 = time.time()\n        print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}'.format(\n            epoch, time2 - time1, acc))\n\n        # eval for one epoch\n        loss, val_acc = validate(val_loader, model, classifier, criterion, opt)\n        if val_acc > best_acc:\n            best_acc = val_acc\n\n    print('best accuracy: {:.2f}'.format(best_acc))\n\n\nif __name__ == '__main__':\n    main()\n"}
