{"repo_info": {"repo_name": "whitebox", "repo_owner": "squaredev-io", "repo_url": "https://github.com/squaredev-io/whitebox"}}
{"type": "test_file", "path": "whitebox/analytics/tests/__init__.py", "content": ""}
{"type": "test_file", "path": "whitebox/tests/v1/test_cron_tasks.py", "content": "import pytest\nfrom whitebox.tests.v1.conftest import get_order_number\nfrom fastapi import status\n\n\n@pytest.mark.order(get_order_number(\"cron_tasks_run_no_models\"))\ndef test_cron_tasks_no_models(client):\n    response = client.post(\"/v1/cron-tasks/run\")\n    assert response.status_code == status.HTTP_200_OK\n\n\n@pytest.mark.order(get_order_number(\"cron_tasks_run_no_inference\"))\ndef test_cron_tasks_no_inference(client):\n    response = client.post(\"/v1/cron-tasks/run\")\n    assert response.status_code == status.HTTP_200_OK\n\n\n@pytest.mark.order(get_order_number(\"cron_tasks_run_ok\"))\ndef test_cron_tasks_ok(client):\n    response = client.post(\"/v1/cron-tasks/run\")\n    assert response.status_code == status.HTTP_200_OK\n\n\n@pytest.mark.order(get_order_number(\"cron_tasks_run_after_x_time\"))\ndef test_cron_tasks_run_after_x_time(client):\n    response = client.post(\"/v1/cron-tasks/run\")\n    assert response.status_code == status.HTTP_200_OK\n"}
{"type": "test_file", "path": "whitebox/tests/v1/conftest.py", "content": "import shutil\nfrom fastapi.testclient import TestClient\nfrom pytest import fixture\nfrom sqlalchemy.orm import close_all_sessions\nfrom whitebox import crud\nfrom whitebox.core.settings import get_settings\nfrom whitebox.entities.Base import Base\nfrom whitebox.main import app\nfrom whitebox.sdk.whitebox import Whitebox\nfrom whitebox.tests.utils.maps import v1_test_order_map\nfrom whitebox.utils.passwords import decrypt_api_key\nfrom whitebox.core.db import SessionLocal, engine\n\nsettings = get_settings()\n\n\ndef get_order_number(task):\n    return v1_test_order_map.index(task)\n\n\n@fixture(scope=\"session\")\ndef client():\n    with TestClient(app) as client:\n        yield client\n\n\n@fixture(scope=\"session\")\ndef api_key():\n    db = SessionLocal()\n\n    user = crud.users.get_first_by_filter(db=db, username=\"admin\")\n    api_key = (\n        decrypt_api_key(user.api_key, settings.SECRET_KEY.encode())\n        if settings.SECRET_KEY\n        else user.api_key\n    )\n\n    yield api_key\n\n\n@fixture(scope=\"session\", autouse=True)\ndef drop_db():\n    yield\n    # Removes the folder \"test_model\" with the models trained during testing\n    test_model_path = settings.MODEL_PATH\n    shutil.rmtree(test_model_path)\n\n    close_all_sessions()\n    # Drops all test database tables\n    Base.metadata.drop_all(engine)\n\n\nclass TestsState:\n    user: dict = {}\n    model_binary: dict = {}\n    model_multi: dict = {}\n    model_multi_2: dict = {}\n    model_multi_3: dict = {}\n    model_regression: dict = {}\n    inference_row_multi: dict = {}\n    inference_row_binary: dict = {}\n    concept_drift_monitor: dict = {}\n\n\nstate = TestsState()\n\n\nclass TestsSDKState:\n    wb: Whitebox\n\n\nstate_sdk = TestsSDKState()\n"}
{"type": "test_file", "path": "whitebox/tests/v1/test_dataset_rows.py", "content": "from whitebox.tests.v1.mock_data import (\n    dataset_rows_single_row_column_payload,\n    dataset_rows_no_prediction_column_payload,\n    dataset_rows_one_prediction_value_payload,\n    dataset_rows_create_multi_class_payload,\n    dataset_rows_create_binary_payload,\n    dataset_rows_create_wrong_model_payload,\n    dataset_rows_create_reg_payload,\n)\nimport pytest\nfrom whitebox import schemas\nfrom whitebox.tests.v1.conftest import get_order_number, state\nfrom fastapi import status\n\n\n@pytest.mark.order(get_order_number(\"dataset_rows_wrong_training_dataset\"))\ndef test_dataset_rows_wrong_training_data(client, api_key):\n    response_single_row = client.post(\n        \"/v1/dataset-rows\",\n        json=list(\n            map(\n                lambda x: {**x, \"model_id\": state.model_multi[\"id\"]},\n                dataset_rows_single_row_column_payload,\n            )\n        ),\n        headers={\"api-key\": api_key},\n    )\n\n    response_no_prediction = client.post(\n        \"/v1/dataset-rows\",\n        json=list(\n            map(\n                lambda x: {**x, \"model_id\": state.model_multi[\"id\"]},\n                dataset_rows_no_prediction_column_payload,\n            )\n        ),\n        headers={\"api-key\": api_key},\n    )\n\n    response_one_prediction_value = client.post(\n        \"/v1/dataset-rows\",\n        json=list(\n            map(\n                lambda x: {**x, \"model_id\": state.model_multi[\"id\"]},\n                dataset_rows_one_prediction_value_payload,\n            )\n        ),\n        headers={\"api-key\": api_key},\n    )\n\n    assert response_single_row.status_code == status.HTTP_400_BAD_REQUEST\n    assert response_no_prediction.status_code == status.HTTP_400_BAD_REQUEST\n    assert response_one_prediction_value.status_code == status.HTTP_400_BAD_REQUEST\n\n\n@pytest.mark.order(get_order_number(\"dataset_rows_create\"))\ndef test_dataset_row_create_many(client, api_key):\n    response_model_multi = client.post(\n        \"/v1/dataset-rows\",\n        json=list(\n            map(\n                lambda x: {**x, \"model_id\": state.model_multi[\"id\"]},\n                dataset_rows_create_multi_class_payload,\n            )\n        ),\n        headers={\"api-key\": api_key},\n    )\n\n    response_model_binary = client.post(\n        \"/v1/dataset-rows\",\n        json=list(\n            map(\n                lambda x: {**x, \"model_id\": state.model_binary[\"id\"]},\n                dataset_rows_create_binary_payload,\n            )\n        ),\n        headers={\"api-key\": api_key},\n    )\n\n    response_model_reg = client.post(\n        \"/v1/dataset-rows\",\n        json=list(\n            map(\n                lambda x: {**x, \"model_id\": state.model_regression[\"id\"]},\n                dataset_rows_create_reg_payload,\n            )\n        ),\n        headers={\"api-key\": api_key},\n    )\n\n    response_wrong_model = client.post(\n        \"/v1/dataset-rows\",\n        json=(dataset_rows_create_wrong_model_payload),\n        headers={\"api-key\": api_key},\n    )\n\n    assert response_model_multi.status_code == status.HTTP_201_CREATED\n    assert response_model_binary.status_code == status.HTTP_201_CREATED\n    assert response_model_reg.status_code == status.HTTP_201_CREATED\n    assert response_wrong_model.status_code == status.HTTP_404_NOT_FOUND\n    validated = [schemas.DatasetRow(**m) for m in response_model_multi.json()]\n    validated = [schemas.DatasetRow(**m) for m in response_model_binary.json()]\n    validated = [schemas.DatasetRow(**m) for m in response_model_reg.json()]\n\n\n@pytest.mark.order(get_order_number(\"dataset_rows_get_model's_all\"))\ndef test_dataset_row_get_models_all(client, api_key):\n    response_model_multi = client.get(\n        f\"/v1/dataset-rows?model_id={state.model_multi['id']}\",\n        headers={\"api-key\": api_key},\n    )\n    response_model_binary = client.get(\n        f\"/v1/dataset-rows?model_id={state.model_binary['id']}\",\n        headers={\"api-key\": api_key},\n    )\n    response_model_not_found = client.get(\n        f\"/v1/dataset-rows?model_id=wrong_model_id\",\n        headers={\"api-key\": api_key},\n    )\n\n    assert response_model_multi.status_code == status.HTTP_200_OK\n    assert response_model_binary.status_code == status.HTTP_200_OK\n    assert response_model_not_found.status_code == status.HTTP_404_NOT_FOUND\n    validated = [schemas.DatasetRow(**m) for m in response_model_multi.json()]\n    validated = [schemas.DatasetRow(**m) for m in response_model_binary.json()]\n"}
{"type": "test_file", "path": "whitebox/tests/__init__.py", "content": ""}
{"type": "test_file", "path": "whitebox/tests/utils/maps.py", "content": "v1_test_order_map = [\n    \"health\",\n    \"models_no_api_key\",\n    \"models_wrong_api_key\",\n    \"cron_tasks_run_no_models\",\n    \"models_create\",\n    \"models_get_all\",\n    \"models_get\",\n    \"models_update\",\n    \"cron_tasks_run_no_inference\",\n    \"inference_rows_create\",\n    \"inference_rows_create_many\",\n    \"inference_rows_get_model's_all\",\n    \"inference_rows_get\",\n    \"dataset_rows_wrong_training_dataset\",\n    \"dataset_rows_create\",\n    \"dataset_rows_get_model's_all\",\n    \"inference_rows_xai\",\n    \"model_monitor_create\",\n    \"model_monitors_get_model_all\",\n    \"model_monitor_update\",\n    \"cron_tasks_run_ok\",\n    \"performance_metrics_get_model_all\",\n    \"drifting_metrics_get_model_all\",\n    \"model_integrity_metrics_get_model_all\",\n    \"alerts_get\",\n    \"inference_rows_create_many_after_x_time\",\n    \"cron_tasks_run_after_x_time\",\n    \"drifting_metrics_get_binary_model_after_x_time\",\n    \"model_monitor_delete\",\n    \"models_delete\",\n    # SDK tests\n    \"sdk_init\",\n    \"sdk_create_model\",\n    \"sdk_get_model\",\n    \"sdk_delete_model\",\n    \"sdk_log_training_dataset\",\n    \"sdk_log_inferences\",\n    \"sdk_create_model_monitor\",\n    \"sdk_update_model_monitor\",\n    \"sdk_delete_model_monitor\",\n    \"sdk_get_alerts\",\n    \"sdk_get_drifting_metrics\",\n    \"sdk_get_descriptive_statistics\",\n    \"sdk_get_performance_metrics\",\n    \"sdk_get_xai_row\",\n]\n"}
{"type": "test_file", "path": "whitebox/tests/v1/test_model_integrity_metrics.py", "content": "import pytest\nfrom whitebox import schemas\nfrom whitebox.tests.v1.conftest import get_order_number, state\nfrom fastapi import status\n\n\n@pytest.mark.order(get_order_number(\"model_integrity_metrics_get_model_all\"))\ndef test_model_integrity_metric_get_model_all(client, api_key):\n    response_multi = client.get(\n        f\"/v1/model-integrity-metrics?model_id={state.model_multi['id']}\",\n        headers={\"api-key\": api_key},\n    )\n    response_binary = client.get(\n        f\"/v1/model-integrity-metrics?model_id={state.model_binary['id']}\",\n        headers={\"api-key\": api_key},\n    )\n    response_wrong_model = client.get(\n        f\"/v1/model-integrity-metrics?model_id=wrong_model_id\",\n        headers={\"api-key\": api_key},\n    )\n\n    assert len(response_multi.json()) == 1\n    assert len(response_binary.json()) == 1\n\n    assert response_multi.status_code == status.HTTP_200_OK\n    assert response_binary.status_code == status.HTTP_200_OK\n    assert response_wrong_model.status_code == status.HTTP_404_NOT_FOUND\n\n    validated = [schemas.ModelIntegrityMetric(**m) for m in response_multi.json()]\n    validated = [schemas.ModelIntegrityMetric(**m) for m in response_binary.json()]\n"}
{"type": "test_file", "path": "whitebox/tests/v1/test_model_monitors.py", "content": "import pytest\nfrom whitebox.tests.v1.mock_data import (\n    model_monitor_accuracy_create_payload,\n    model_monitor_f1_create_payload,\n    model_monitor_data_drift_create_payload,\n    model_monitor_concept_drift_create_payload,\n    model_monitor_precision_create_payload,\n    model_monitor_r_square_create_payload,\n    model_monitor_no_threshold_create_payload,\n    model_monitor_no_feature_create_payload,\n    model_monitor_feature_same_as_target_create_payload,\n    model_monitor_feature_not_in_columns_create_payload,\n    model_monitor_update_payload,\n)\nfrom whitebox import schemas\nfrom whitebox.tests.v1.conftest import get_order_number, state\nfrom fastapi import status\n\n\n@pytest.mark.order(get_order_number(\"model_monitor_create\"))\ndef test_model_monitor_create(client, api_key):\n    accuracy_monitor = client.post(\n        \"/v1/model-monitors\",\n        json={\n            **model_monitor_accuracy_create_payload,\n            \"model_id\": state.model_multi[\"id\"],\n        },\n        headers={\"api-key\": api_key},\n    )\n\n    f1_monitor = client.post(\n        \"/v1/model-monitors\",\n        json={\n            **model_monitor_f1_create_payload,\n            \"model_id\": state.model_multi[\"id\"],\n        },\n        headers={\"api-key\": api_key},\n    )\n\n    data_drift_monitor = client.post(\n        \"/v1/model-monitors\",\n        json={\n            **model_monitor_data_drift_create_payload,\n            \"model_id\": state.model_binary[\"id\"],\n        },\n        headers={\"api-key\": api_key},\n    )\n\n    concept_drift_monitor = client.post(\n        \"/v1/model-monitors\",\n        json={\n            **model_monitor_concept_drift_create_payload,\n            \"model_id\": state.model_binary[\"id\"],\n        },\n        headers={\"api-key\": api_key},\n    )\n\n    precision_monitor = client.post(\n        \"/v1/model-monitors\",\n        json={\n            **model_monitor_precision_create_payload,\n            \"model_id\": state.model_binary[\"id\"],\n        },\n        headers={\"api-key\": api_key},\n    )\n\n    r_square_monitor = client.post(\n        \"/v1/model-monitors\",\n        json={\n            **model_monitor_r_square_create_payload,\n            \"model_id\": state.model_regression[\"id\"],\n        },\n        headers={\"api-key\": api_key},\n    )\n\n    wrong_model_monitor = client.post(\n        \"/v1/model-monitors\",\n        json={\n            **model_monitor_r_square_create_payload,\n            \"model_id\": \"wrong_model_id\",\n        },\n        headers={\"api-key\": api_key},\n    )\n\n    no_training_data = client.post(\n        \"/v1/model-monitors\",\n        json={\n            **model_monitor_data_drift_create_payload,\n            \"model_id\": state.model_multi_3[\"id\"],\n        },\n        headers={\"api-key\": api_key},\n    )\n\n    no_threshold = client.post(\n        \"/v1/model-monitors\",\n        json={\n            **model_monitor_no_threshold_create_payload,\n            \"model_id\": state.model_multi[\"id\"],\n        },\n        headers={\"api-key\": api_key},\n    )\n\n    no_feature = client.post(\n        \"/v1/model-monitors\",\n        json={\n            **model_monitor_no_feature_create_payload,\n            \"model_id\": state.model_multi[\"id\"],\n        },\n        headers={\"api-key\": api_key},\n    )\n\n    feature_same_as_target = client.post(\n        \"/v1/model-monitors\",\n        json={\n            **model_monitor_feature_same_as_target_create_payload,\n            \"model_id\": state.model_multi[\"id\"],\n        },\n        headers={\"api-key\": api_key},\n    )\n\n    feature_not_in_columns = client.post(\n        \"/v1/model-monitors\",\n        json={\n            **model_monitor_feature_not_in_columns_create_payload,\n            \"model_id\": state.model_multi[\"id\"],\n        },\n        headers={\"api-key\": api_key},\n    )\n\n    state.concept_drift_monitor = (\n        concept_drift_monitor_json\n    ) = concept_drift_monitor.json()\n\n    assert concept_drift_monitor.status_code == status.HTTP_201_CREATED\n    assert concept_drift_monitor_json[\"feature\"] == \"target\"\n    assert wrong_model_monitor.status_code == status.HTTP_404_NOT_FOUND\n    assert no_threshold.status_code == status.HTTP_400_BAD_REQUEST\n    assert no_feature.status_code == status.HTTP_400_BAD_REQUEST\n    assert feature_same_as_target.status_code == status.HTTP_400_BAD_REQUEST\n    assert feature_not_in_columns.status_code == status.HTTP_400_BAD_REQUEST\n    assert no_training_data.status_code == status.HTTP_404_NOT_FOUND\n    validated = schemas.ModelMonitor(**concept_drift_monitor_json)\n\n\n@pytest.mark.order(get_order_number(\"model_monitors_get_model_all\"))\ndef test_model_monitors_get_model_all(client, api_key):\n    response_multi = client.get(\n        f\"/v1/model-monitors?model_id={state.model_multi['id']}\",\n        headers={\"api-key\": api_key},\n    )\n    response_all = client.get(\n        f\"/v1/model-monitors\",\n        headers={\"api-key\": api_key},\n    )\n    response_wrong_model = client.get(\n        f\"/v1/model-monitors?model_id=wrong_model_id\",\n        headers={\"api-key\": api_key},\n    )\n\n    assert len(response_multi.json()) == 2\n    assert len(response_all.json()) == 6\n\n    assert response_multi.status_code == status.HTTP_200_OK\n    assert response_all.status_code == status.HTTP_200_OK\n    assert response_wrong_model.status_code == status.HTTP_404_NOT_FOUND\n\n    validated = [schemas.ModelMonitor(**m) for m in response_multi.json()]\n    validated = [schemas.ModelMonitor(**m) for m in response_all.json()]\n\n\n@pytest.mark.order(get_order_number(\"model_monitor_update\"))\ndef test_model_monitor_update(client, api_key):\n    response = client.put(\n        f\"/v1/model-monitors/{state.concept_drift_monitor['id']}\",\n        json=model_monitor_update_payload,\n        headers={\"api-key\": api_key},\n    )\n    response_wrong_model = client.put(\n        f\"/v1/model-monitors/wrong_model_id\",\n        json=model_monitor_update_payload,\n        headers={\"api-key\": api_key},\n    )\n\n    response_json = response.json()\n\n    assert response.status_code == status.HTTP_200_OK\n    assert response_json[\"lower_threshold\"] == None\n    assert response_wrong_model.status_code == status.HTTP_404_NOT_FOUND\n\n    validated = schemas.ModelMonitor(**response_json)\n\n\n@pytest.mark.order(get_order_number(\"model_monitor_delete\"))\ndef test_model_monitor_delete(client, api_key):\n    response = client.delete(\n        f\"/v1/model-monitors/{state.concept_drift_monitor['id']}\",\n        headers={\"api-key\": api_key},\n    )\n\n    wrong_monitor_response = client.delete(\n        f\"/v1/model-monitors/wrong_model_monitor_id\",\n        headers={\"api-key\": api_key},\n    )\n\n    assert response.status_code == status.HTTP_200_OK\n    assert wrong_monitor_response.status_code == status.HTTP_404_NOT_FOUND\n"}
{"type": "test_file", "path": "whitebox/tests/v1/mock_data.py", "content": "from datetime import datetime\nfrom whitebox.schemas.model import ModelType\nfrom sklearn.datasets import load_breast_cancer, load_wine, load_diabetes\nimport pandas as pd\nfrom copy import deepcopy\n\nfrom whitebox.schemas.modelMonitor import AlertSeverity, MonitorMetrics, MonitorStatus\n\nuser_create_payload = dict(username=\"admin\")\n\nmodel_binary_create_payload = dict(\n    name=\"Model Binary\",\n    description=\"Model description\",\n    type=ModelType.binary,\n    labels={\"label_1\": 0, \"label_2\": 1},\n    target_column=\"target\",\n    granularity=\"1D\",\n)\n\nmodel_multi_create_payload = dict(\n    name=\"Model 1 Multi\",\n    description=\"Model 1 description\",\n    type=ModelType.multi_class,\n    labels={\"label_1\": 0, \"label_2\": 1, \"label_3\": 2},\n    target_column=\"target\",\n    granularity=\"15T\",\n)\n\nmodel_multi_2_create_payload = dict(\n    name=\"Model 2 Multi\",\n    description=\"Model 2 description\",\n    type=ModelType.multi_class,\n    labels={\"label_1\": 0, \"label_2\": 1, \"label_3\": 2},\n    target_column=\"target\",\n    granularity=\"2H\",\n)\n\nmodel_multi_3_create_payload = dict(\n    name=\"Model 3\",\n    description=\"Model 3 description\",\n    type=ModelType.multi_class,\n    labels={\"label_1\": 0, \"label_2\": 1, \"label_3\": 2},\n    target_column=\"target\",\n    granularity=\"1W\",\n)\n\nmodel_regression_create_payload = dict(\n    name=\"Regression Model\",\n    description=\"Regression Model description\",\n    type=ModelType.regression,\n    target_column=\"target\",\n    granularity=\"1D\",\n)\n\nmodel_update_payload = dict(\n    name=\"Model 2 - categorical\",\n    description=\"Model 2 description updated\",\n)\n\n# dataset rows data for both binary and multiclass models\ndataset_rows_single_row_column_payload = [\n    {\n        \"nonprocessed\": {},\n        \"processed\": {\"additionalProp1\": 0, \"additionalProp2\": 0, \"target\": 1},\n    }\n]\n\ndataset_rows_no_prediction_column_payload = [\n    {\n        \"nonprocessed\": {},\n        \"processed\": {\"additionalProp1\": 0, \"additionalProp2\": 0},\n    },\n    {\n        \"nonprocessed\": {},\n        \"processed\": {\"additionalProp1\": 1, \"additionalProp2\": 0},\n    },\n]\n\ndataset_rows_one_prediction_value_payload = [\n    {\n        \"nonprocessed\": {},\n        \"processed\": {\"additionalProp1\": 0, \"additionalProp2\": 0, \"target\": 0},\n    },\n    {\n        \"nonprocessed\": {},\n        \"processed\": {\"additionalProp1\": 1, \"additionalProp2\": 0, \"target\": 0},\n    },\n]\n\n\ndf_load_multi = load_wine()\ndf_multi = pd.DataFrame(df_load_multi.data, columns=df_load_multi.feature_names)\ndf_multi[\"target\"] = df_load_multi.target\ndf_multi = df_multi.tail(100)\n\ndict_multi_data = df_multi.to_dict(orient=\"records\")\ndataset_rows_create_multi_class_payload = [\n    {\"processed\": x, \"nonprocessed\": x} for x in dict_multi_data\n]\n\ndf_load_binary = load_breast_cancer()\ndf_binary = pd.DataFrame(df_load_binary.data, columns=df_load_binary.feature_names)\ndf_binary[\"target\"] = df_load_binary.target\ndf_binary = df_binary.tail(100)\n\ndict_binary_data = df_binary.to_dict(orient=\"records\")\ndataset_rows_create_binary_payload = [\n    {\"processed\": x, \"nonprocessed\": x} for x in dict_binary_data\n]\n\ndf_load_reg = load_diabetes()\ndf_reg = pd.DataFrame(df_load_reg.data, columns=df_load_reg.feature_names)\ndf_reg[\"target\"] = df_load_reg.target\ndf_reg = df_reg.tail(100)\n\ndict_reg_data = df_reg.to_dict(orient=\"records\")\ndataset_rows_create_reg_payload = [\n    {\"processed\": x, \"nonprocessed\": x} for x in dict_reg_data\n]\n\ndataset_rows_create_wrong_model_payload = list(\n    (\n        dict(\n            model_id=\"wrong_model_id\",\n            nonprocessed={\"sex\": \"male\"},\n            processed={\"sex\": 0},\n        ),\n        dict(\n            model_id=\"wrong_model_id\",\n            nonprocessed={\"sex\": \"female\"},\n            processed={\"sex\": 1},\n        ),\n    )\n)\n\n\n# inference rows data for both binary and multiclaas models\ndf_multi_inference = df_multi.tail(10)\ndict_multi_inferences = df_multi_inference.to_dict(orient=\"records\")\nmulti_actuals = [0, 2, 0, 1, 2, 1, 1, 2, 0, 1]\ninference_row_create_many_multi_payload = [\n    {\n        \"timestamp\": str(datetime(2023, 3, 6, 12, 13, 25)),\n        \"processed\": x,\n        \"nonprocessed\": x,\n        \"actual\": multi_actuals[i],\n    }\n    for i, x in enumerate(dict_multi_inferences)\n]\n\ninference_row_create_single_row_payload = inference_row_create_many_multi_payload[0]\ninference_row_create_many_multi_no_actual_payload = deepcopy(\n    inference_row_create_many_multi_payload\n)\nfor x in inference_row_create_many_multi_no_actual_payload:\n    del x[\"actual\"]\n\ninference_row_create_many_multi_mixed_actuals_payload = (\n    inference_row_create_many_multi_no_actual_payload\n    + inference_row_create_many_multi_payload\n)\n\n# This is the body of the request coming from the sdk\ndf_binary_inference = df_binary.tail(10)\ndict_binary_inferences = df_binary_inference.to_dict(orient=\"records\")\nbinary_actuals = [0, 1, 1, 0, 1, 0, 1, 1, 1, 0]\ninference_row_create_many_binary_payload = [\n    {\n        \"timestamp\": str(datetime(2023, 3, 6, 12, 13, 25)),\n        \"processed\": x,\n        \"nonprocessed\": x,\n        \"actual\": binary_actuals[i],\n    }\n    for i, x in enumerate(dict_binary_inferences)\n]\n\ndf_reg_inference = df_reg.tail(10)\ndict_reg_inferences = df_reg_inference.to_dict(orient=\"records\")\nreg_actuals = [103, 20, 50, 64, 48, 198, 105, 138, 250, 57]\ninference_row_create_many_reg_payload = [\n    {\n        \"timestamp\": str(datetime(2023, 3, 6, 12, 13, 25)),\n        \"processed\": x,\n        \"nonprocessed\": x,\n        \"actual\": reg_actuals[i],\n    }\n    for i, x in enumerate(dict_reg_inferences)\n]\n\ntimestamps = pd.Series([\"2022-12-22T12:13:27.879738\"] * 10)\nmixed_actuals = pd.Series([0, 1, None, 1, 0, None, None, 1, 0, None])\n\nmodel_monitor_accuracy_create_payload = dict(\n    name=\"accuracy monitor\",\n    status=MonitorStatus.active,\n    metric=MonitorMetrics.accuracy,\n    lower_threshold=0.85,\n    severity=AlertSeverity.low,\n    email=\"example@whitebox.io\",\n)\n\nmodel_monitor_f1_create_payload = dict(\n    name=\"f1 Monitor\",\n    status=MonitorStatus.active,\n    metric=MonitorMetrics.f1,\n    lower_threshold=0.85,\n    severity=AlertSeverity.low,\n    email=\"example@whitebox.io\",\n)\n\nmodel_monitor_data_drift_create_payload = dict(\n    name=\"data drift monitor\",\n    status=MonitorStatus.active,\n    metric=MonitorMetrics.data_drift,\n    feature=\"concavity error\",\n    severity=AlertSeverity.low,\n    email=\"example@whitebox.io\",\n)\n\nmodel_monitor_concept_drift_create_payload = dict(\n    name=\"concept drift monitor\",\n    status=MonitorStatus.active,\n    metric=MonitorMetrics.concept_drift,\n    feature=\"concavity error\",\n    severity=AlertSeverity.low,\n    email=\"example@whitebox.io\",\n)\n\nmodel_monitor_precision_create_payload = dict(\n    name=\"precision monitor\",\n    status=MonitorStatus.active,\n    metric=MonitorMetrics.precision,\n    lower_threshold=0.85,\n    severity=AlertSeverity.low,\n    email=\"example@whitebox.io\",\n)\n\nmodel_monitor_r_square_create_payload = dict(\n    name=\"r_square monitor\",\n    status=MonitorStatus.active,\n    metric=MonitorMetrics.r_square,\n    lower_threshold=0.85,\n    severity=AlertSeverity.low,\n    email=\"example@whitebox.io\",\n)\n\nmodel_monitor_no_threshold_create_payload = dict(\n    name=\"no threshold monitor\",\n    status=MonitorStatus.active,\n    metric=MonitorMetrics.r_square,\n    severity=AlertSeverity.low,\n    email=\"example@whitebox.io\",\n)\n\nmodel_monitor_no_feature_create_payload = dict(\n    name=\"no feature monitor\",\n    status=MonitorStatus.active,\n    metric=MonitorMetrics.data_drift,\n    severity=AlertSeverity.low,\n    email=\"example@whitebox.io\",\n)\n\nmodel_monitor_feature_same_as_target_create_payload = dict(\n    name=\"feature same as target monitor\",\n    status=MonitorStatus.active,\n    metric=MonitorMetrics.data_drift,\n    feature=\"target\",\n    severity=AlertSeverity.low,\n    email=\"example@whitebox.io\",\n)\n\nmodel_monitor_feature_not_in_columns_create_payload = dict(\n    name=\"feature not in columns monitor\",\n    status=MonitorStatus.active,\n    metric=MonitorMetrics.data_drift,\n    feature=\"not_a_column\",\n    severity=AlertSeverity.low,\n    email=\"example@whitebox.io\",\n)\n\n\nmodel_monitor_update_payload = dict(\n    name=\"concept drift monitor updated\", lower_threshold=0.54\n)\n\nalert_payload = {\n    \"id\": \"c2a25e32-4e40-40da-a83f-fc299514866e\",\n    \"created_at\": \"2023-02-09T12:52:50.547557\",\n    \"updated_at\": \"2023-02-09T12:52:50.547557\",\n    \"model_id\": \"11737584-a356-4de5-924c-9894dfcd0e0d\",\n    \"model_monitor_id\": \"fa3a12f3-6801-4c25-8154-8b627e1d37b1\",\n    \"timestamp\": \"2023-02-09T12:52:50.547525\",\n    \"description\": 'Data drift found in \"concavity error\" feature.',\n}\n\nperformance_metrics_report_payload = [\n    {\n        \"id\": \"f7030044-e0c2-4493-8aea-f66e6564efb7\",\n        \"created_at\": \"2023-02-06T09:53:00.343616\",\n        \"updated_at\": \"2023-02-06T09:53:00.343616\",\n        \"model_id\": \"dbb7f384-cf46-4884-bca3-7787e049a3d5\",\n        \"timestamp\": \"2023-02-06T09:53:00.343548\",\n        \"accuracy\": 0.6,\n        \"precision\": 0.25,\n        \"recall\": 0.5,\n        \"f1\": 0.3333333333333333,\n        \"true_negative\": 5,\n        \"false_positive\": 3,\n        \"false_negative\": 1,\n        \"true_positive\": 1,\n    }\n]\n\ndescriptive_statistics_report_payload = [\n    {\n        \"id\": \"d4f18837-65f6-45c2-af7c-34dcdc4aabf9\",\n        \"created_at\": \"2023-02-06T09:53:00.370639\",\n        \"updated_at\": \"2023-02-06T09:53:00.370639\",\n        \"model_id\": \"bc01a212-b01e-48b4-aeb5-4a0a3cfa3c98\",\n        \"timestamp\": \"2023-02-06T09:53:00.370617\",\n        \"feature_metrics\": {\n            \"missing_count\": {\n                \"alcohol\": 0,\n                \"malic_acid\": 0,\n                \"ash\": 0,\n                \"alcalinity_of_ash\": 0,\n                \"magnesium\": 0,\n                \"total_phenols\": 0,\n                \"flavanoids\": 0,\n                \"nonflavanoid_phenols\": 0,\n                \"proanthocyanins\": 0,\n                \"color_intensity\": 0,\n                \"hue\": 0,\n                \"od280/od315_of_diluted_wines\": 0,\n                \"proline\": 0,\n                \"target\": 0,\n            },\n            \"non_missing_count\": {\n                \"alcohol\": 10,\n                \"malic_acid\": 10,\n                \"ash\": 10,\n                \"alcalinity_of_ash\": 10,\n                \"magnesium\": 10,\n                \"total_phenols\": 10,\n                \"flavanoids\": 10,\n                \"nonflavanoid_phenols\": 10,\n                \"proanthocyanins\": 10,\n                \"color_intensity\": 10,\n                \"hue\": 10,\n                \"od280/od315_of_diluted_wines\": 10,\n                \"proline\": 10,\n                \"target\": 10,\n            },\n            \"mean\": {\n                \"alcohol\": 13.379000000000001,\n                \"malic_acid\": 3.564,\n                \"ash\": 2.493,\n                \"alcalinity_of_ash\": 21.6,\n                \"magnesium\": 102.3,\n                \"total_phenols\": 1.6620000000000001,\n                \"flavanoids\": 0.699,\n                \"nonflavanoid_phenols\": 0.44499999999999995,\n                \"proanthocyanins\": 1.1889999999999998,\n                \"color_intensity\": 8.595999899999999,\n                \"hue\": 0.6399999999999999,\n                \"od280/od315_of_diluted_wines\": 1.6970000000000003,\n                \"proline\": 674.5,\n                \"target\": 2.0,\n            },\n            \"minimum\": {\n                \"alcohol\": 12.2,\n                \"malic_acid\": 2.39,\n                \"ash\": 2.26,\n                \"alcalinity_of_ash\": 19.0,\n                \"magnesium\": 86.0,\n                \"total_phenols\": 1.25,\n                \"flavanoids\": 0.49,\n                \"nonflavanoid_phenols\": 0.27,\n                \"proanthocyanins\": 0.64,\n                \"color_intensity\": 5.5,\n                \"hue\": 0.57,\n                \"od280/od315_of_diluted_wines\": 1.56,\n                \"proline\": 470.0,\n                \"target\": 2.0,\n            },\n            \"maximum\": {\n                \"alcohol\": 14.16,\n                \"malic_acid\": 5.65,\n                \"ash\": 2.86,\n                \"alcalinity_of_ash\": 25.0,\n                \"magnesium\": 120.0,\n                \"total_phenols\": 2.05,\n                \"flavanoids\": 0.96,\n                \"nonflavanoid_phenols\": 0.56,\n                \"proanthocyanins\": 1.54,\n                \"color_intensity\": 10.2,\n                \"hue\": 0.74,\n                \"od280/od315_of_diluted_wines\": 1.92,\n                \"proline\": 840.0,\n                \"target\": 2.0,\n            },\n            \"sum\": {\n                \"alcohol\": 133.79000000000002,\n                \"malic_acid\": 35.64,\n                \"ash\": 24.93,\n                \"alcalinity_of_ash\": 216.0,\n                \"magnesium\": 1023.0,\n                \"total_phenols\": 16.62,\n                \"flavanoids\": 6.989999999999999,\n                \"nonflavanoid_phenols\": 4.449999999999999,\n                \"proanthocyanins\": 11.889999999999999,\n                \"color_intensity\": 85.959999,\n                \"hue\": 6.3999999999999995,\n                \"od280/od315_of_diluted_wines\": 16.970000000000002,\n                \"proline\": 6745.0,\n                \"target\": 20.0,\n            },\n            \"standard_deviation\": {\n                \"alcohol\": 0.5907894906159238,\n                \"malic_acid\": 1.1073311258256142,\n                \"ash\": 0.2058613341278272,\n                \"alcalinity_of_ash\": 2.3664319132398464,\n                \"magnesium\": 11.80442478244681,\n                \"total_phenols\": 0.24334703157790474,\n                \"flavanoids\": 0.14224001624796806,\n                \"nonflavanoid_phenols\": 0.08396427811873335,\n                \"proanthocyanins\": 0.3045743842734572,\n                \"color_intensity\": 1.4311393049673125,\n                \"hue\": 0.05291502622129182,\n                \"od280/od315_of_diluted_wines\": 0.12356284950492914,\n                \"proline\": 130.39363481397396,\n                \"target\": 0.0,\n            },\n            \"variance\": {\n                \"alcohol\": 0.34903222222222274,\n                \"malic_acid\": 1.2261822222222223,\n                \"ash\": 0.04237888888888891,\n                \"alcalinity_of_ash\": 5.599999999999999,\n                \"magnesium\": 139.34444444444443,\n                \"total_phenols\": 0.059217777777777765,\n                \"flavanoids\": 0.02023222222222222,\n                \"nonflavanoid_phenols\": 0.0070500000000000024,\n                \"proanthocyanins\": 0.09276555555555556,\n                \"color_intensity\": 2.048159710222322,\n                \"hue\": 0.002800000000000001,\n                \"od280/od315_of_diluted_wines\": 0.015267777777777769,\n                \"proline\": 17002.5,\n                \"target\": 0.0,\n            },\n        },\n    }\n]\n\ndrifting_metrics_report_payload = [\n    {\n        \"id\": \"d11f2313-12e3-4533-b16d-7689175ad617\",\n        \"created_at\": \"2023-02-06T09:53:00.450466\",\n        \"updated_at\": \"2023-02-06T09:53:00.450466\",\n        \"model_id\": \"e8e5ff00-0212-4a1e-bcdb-0ef9183c867f\",\n        \"timestamp\": \"2023-02-06T09:53:00.450435\",\n        \"concept_drift_summary\": {\n            \"concept_drift_summary\": {\n                \"column_name\": \"target\",\n                \"column_type\": \"cat\",\n                \"stattest_name\": \"Z-test p_value\",\n                \"drift_score\": 0.0010364142456651404,\n                \"drift_detected\": True,\n                \"stattest_threshold\": 0.05,\n            },\n            \"column_correlation\": {\n                \"column_name\": \"target\",\n                \"current\": {},\n                \"reference\": {},\n            },\n        },\n        \"data_drift_summary\": {\n            \"number_of_columns\": 13,\n            \"number_of_drifted_columns\": 7,\n            \"share_of_drifted_columns\": 0.5384615384615384,\n            \"dataset_drift\": True,\n            \"drift_by_columns\": {\n                \"alcalinity_of_ash\": {\n                    \"column_name\": \"alcalinity_of_ash\",\n                    \"column_type\": \"num\",\n                    \"stattest_name\": \"K-S p_value\",\n                    \"drift_score\": 0.5163177458201748,\n                    \"drift_detected\": False,\n                    \"threshold\": 0.05,\n                },\n                \"alcohol\": {\n                    \"column_name\": \"alcohol\",\n                    \"column_type\": \"num\",\n                    \"stattest_name\": \"K-S p_value\",\n                    \"drift_score\": 0.0012612112534500996,\n                    \"drift_detected\": True,\n                    \"threshold\": 0.05,\n                },\n                \"ash\": {\n                    \"column_name\": \"ash\",\n                    \"column_type\": \"num\",\n                    \"stattest_name\": \"K-S p_value\",\n                    \"drift_score\": 0.23438597334285016,\n                    \"drift_detected\": False,\n                    \"threshold\": 0.05,\n                },\n                \"color_intensity\": {\n                    \"column_name\": \"color_intensity\",\n                    \"column_type\": \"num\",\n                    \"stattest_name\": \"K-S p_value\",\n                    \"drift_score\": 0.00015242693342585546,\n                    \"drift_detected\": True,\n                    \"threshold\": 0.05,\n                },\n                \"flavanoids\": {\n                    \"column_name\": \"flavanoids\",\n                    \"column_type\": \"num\",\n                    \"stattest_name\": \"K-S p_value\",\n                    \"drift_score\": 0.0003384276858653337,\n                    \"drift_detected\": True,\n                    \"threshold\": 0.05,\n                },\n                \"hue\": {\n                    \"column_name\": \"hue\",\n                    \"column_type\": \"num\",\n                    \"stattest_name\": \"K-S p_value\",\n                    \"drift_score\": 0.0002500181491633883,\n                    \"drift_detected\": True,\n                    \"threshold\": 0.05,\n                },\n                \"magnesium\": {\n                    \"column_name\": \"magnesium\",\n                    \"column_type\": \"num\",\n                    \"stattest_name\": \"K-S p_value\",\n                    \"drift_score\": 0.08450859981880006,\n                    \"drift_detected\": False,\n                    \"threshold\": 0.05,\n                },\n                \"malic_acid\": {\n                    \"column_name\": \"malic_acid\",\n                    \"column_type\": \"num\",\n                    \"stattest_name\": \"K-S p_value\",\n                    \"drift_score\": 0.033698127881912385,\n                    \"drift_detected\": True,\n                    \"threshold\": 0.05,\n                },\n                \"nonflavanoid_phenols\": {\n                    \"column_name\": \"nonflavanoid_phenols\",\n                    \"column_type\": \"num\",\n                    \"stattest_name\": \"K-S p_value\",\n                    \"drift_score\": 0.30861039864383094,\n                    \"drift_detected\": False,\n                    \"threshold\": 0.05,\n                },\n                \"od280/od315_of_diluted_wines\": {\n                    \"column_name\": \"od280/od315_of_diluted_wines\",\n                    \"column_type\": \"num\",\n                    \"stattest_name\": \"K-S p_value\",\n                    \"drift_score\": 0.0006076739224878145,\n                    \"drift_detected\": True,\n                    \"threshold\": 0.05,\n                },\n                \"proanthocyanins\": {\n                    \"column_name\": \"proanthocyanins\",\n                    \"column_type\": \"num\",\n                    \"stattest_name\": \"K-S p_value\",\n                    \"drift_score\": 0.09673060972050829,\n                    \"drift_detected\": False,\n                    \"threshold\": 0.05,\n                },\n                \"proline\": {\n                    \"column_name\": \"proline\",\n                    \"column_type\": \"num\",\n                    \"stattest_name\": \"K-S p_value\",\n                    \"drift_score\": 0.0328848734766648,\n                    \"drift_detected\": True,\n                    \"threshold\": 0.05,\n                },\n                \"total_phenols\": {\n                    \"column_name\": \"total_phenols\",\n                    \"column_type\": \"num\",\n                    \"stattest_name\": \"K-S p_value\",\n                    \"drift_score\": 0.082145120769578,\n                    \"drift_detected\": False,\n                    \"threshold\": 0.05,\n                },\n            },\n        },\n    }\n]\n\ninference_row_xai_payload = {\n    \"mean perimeter\": -0.23641989216827916,\n    \"mean concavity\": -0.16854638344893477,\n    \"worst radius\": -0.16657884627852115,\n    \"mean concave points\": -0.1260514980862101,\n    \"worst perimeter\": -0.08533726601274261,\n    \"worst texture\": -0.07905090544100012,\n    \"mean texture\": -0.05969642438812264,\n    \"worst concave points\": -0.03310756238929911,\n    \"mean radius\": -0.03244286722644983,\n    \"worst symmetry\": 0.02852296352595751,\n}\n"}
{"type": "test_file", "path": "whitebox/tests/v1/test_health.py", "content": "import pytest\nfrom whitebox import schemas\nfrom whitebox.tests.v1.conftest import get_order_number\nfrom fastapi import status\n\n\n@pytest.mark.order(get_order_number(\"health\"))\ndef test_health(client):\n    response = client.get(\"/v1/health\")\n    assert response.status_code == status.HTTP_200_OK\n    validated = schemas.HealthCheck(**response.json())\n"}
{"type": "test_file", "path": "whitebox/tests/v1/__init__.py", "content": ""}
{"type": "test_file", "path": "whitebox/tests/v1/test_performance_metrics.py", "content": "import pytest\nfrom whitebox import schemas\nfrom whitebox.tests.v1.conftest import get_order_number, state\nfrom fastapi import status\n\n\n@pytest.mark.order(get_order_number(\"performance_metrics_get_model_all\"))\ndef test_performance_metric_get_model_all(client, api_key):\n    response_multi = client.get(\n        f\"/v1/performance-metrics?model_id={state.model_multi['id']}\",\n        headers={\"api-key\": api_key},\n    )\n    response_binary = client.get(\n        f\"/v1/performance-metrics?model_id={state.model_binary['id']}\",\n        headers={\"api-key\": api_key},\n    )\n    response_reg = client.get(\n        f\"/v1/performance-metrics?model_id={state.model_regression['id']}\",\n        headers={\"api-key\": api_key},\n    )\n    response_wrong_model = client.get(\n        f\"/v1/performance-metrics?model_id=wrong_model_id\",\n        headers={\"api-key\": api_key},\n    )\n\n    assert response_multi.status_code == status.HTTP_200_OK\n    assert response_binary.status_code == status.HTTP_200_OK\n    assert response_reg.status_code == status.HTTP_200_OK\n    assert response_wrong_model.status_code == status.HTTP_404_NOT_FOUND\n\n    validated = [schemas.MultiClassificationMetrics(**m) for m in response_multi.json()]\n    validated = [\n        schemas.BinaryClassificationMetrics(**m) for m in response_binary.json()\n    ]\n    validated = [schemas.RegressionMetrics(**m) for m in response_reg.json()]\n"}
{"type": "test_file", "path": "whitebox/tests/v1/test_sdk.py", "content": "import pandas as pd\nimport pytest\nfrom whitebox.schemas.modelMonitor import AlertSeverity, MonitorMetrics, MonitorStatus\nfrom whitebox.sdk import Whitebox\nfrom whitebox.tests.v1.conftest import get_order_number, state, state_sdk\nfrom whitebox.tests.v1.mock_data import (\n    model_multi_create_payload,\n    timestamps,\n    mixed_actuals,\n    inference_row_xai_payload,\n    model_monitor_concept_drift_create_payload,\n    model_monitor_update_payload,\n    alert_payload,\n    drifting_metrics_report_payload,\n    descriptive_statistics_report_payload,\n    performance_metrics_report_payload,\n)\nimport requests_mock\nfrom fastapi import status\n\n\n@pytest.mark.order(get_order_number(\"sdk_init\"))\ndef test_sdk_init(client, api_key):\n    wb = Whitebox(host=client.base_url, api_key=api_key)\n    assert wb.host == client.base_url\n    assert wb.api_key == api_key\n    state_sdk.wb = wb\n\n\n@pytest.mark.order(get_order_number(\"sdk_create_model\"))\ndef test_sdk_create_model(client):\n    with requests_mock.Mocker() as m:\n        m.post(\n            url=f\"{state_sdk.wb.host}/v1/models\",\n            json=model_multi_create_payload,\n            headers={\"api-key\": state_sdk.wb.api_key},\n        )\n\n        model = state_sdk.wb.create_model(\n            name=model_multi_create_payload[\"name\"],\n            description=model_multi_create_payload[\"description\"],\n            labels=model_multi_create_payload[\"labels\"],\n            type=model_multi_create_payload[\"type\"],\n            target_column=model_multi_create_payload[\"target_column\"],\n            granularity=model_multi_create_payload[\"granularity\"],\n        )\n\n        assert model == model_multi_create_payload\n\n\n@pytest.mark.order(get_order_number(\"sdk_get_model\"))\ndef test_sdk_get_model(client):\n    mock_model_id = \"mock_model_id\"\n    with requests_mock.Mocker() as m:\n        m.get(\n            url=f\"{state_sdk.wb.host}/v1/models/{mock_model_id}\",\n            headers={\"api-key\": state_sdk.wb.api_key},\n            status_code=status.HTTP_404_NOT_FOUND,\n        )\n\n        not_found_result = state_sdk.wb.get_model(model_id=mock_model_id)\n\n        assert not_found_result == None\n\n        m.get(\n            url=f\"{state_sdk.wb.host}/v1/models/{mock_model_id}\",\n            headers={\"api-key\": state_sdk.wb.api_key},\n            json=state.model_multi,\n        )\n\n        model = state_sdk.wb.get_model(model_id=mock_model_id)\n\n        assert model == state.model_multi\n\n\n@pytest.mark.order(get_order_number(\"sdk_delete_model\"))\ndef test_sdk_delete_model(client):\n    mock_model_id = \"mock_model_id\"\n\n    with requests_mock.Mocker() as m:\n        m.delete(\n            url=f\"{state_sdk.wb.host}/v1/models/{mock_model_id}\",\n            headers={\"api-key\": state_sdk.wb.api_key},\n            status_code=status.HTTP_200_OK,\n        )\n\n        happy_result = state_sdk.wb.delete_model(model_id=mock_model_id)\n        assert happy_result == True\n\n        m.delete(\n            url=f\"{state_sdk.wb.host}/v1/models/{mock_model_id}\",\n            headers={\"api-key\": state_sdk.wb.api_key},\n            status_code=status.HTTP_400_BAD_REQUEST,\n        )\n\n        sad_result = state_sdk.wb.delete_model(model_id=mock_model_id)\n        assert sad_result == False\n\n\n@pytest.mark.order(get_order_number(\"sdk_log_training_dataset\"))\ndef test_sdk_log_training_dataset(client):\n    mock_model_id = \"mock_model_id\"\n    df = pd.read_csv(\"whitebox/analytics/data/testing/classification_test_data.csv\")\n\n    with requests_mock.Mocker() as m:\n        m.post(\n            url=f\"{state_sdk.wb.host}/v1/dataset-rows\",\n            headers={\"api-key\": state_sdk.wb.api_key},\n            status_code=status.HTTP_201_CREATED,\n        )\n\n        happy_result = state_sdk.wb.log_training_dataset(\n            model_id=mock_model_id, processed=df, non_processed=df\n        )\n        assert happy_result == True\n\n        m.post(\n            url=f\"{state_sdk.wb.host}/v1/dataset-rows\",\n            headers={\"api-key\": state_sdk.wb.api_key},\n            status_code=status.HTTP_400_BAD_REQUEST,\n        )\n\n        sad_result = state_sdk.wb.log_training_dataset(\n            model_id=mock_model_id, processed=df, non_processed=df\n        )\n        assert sad_result == False\n\n    # drop a row in df to test the dataframe length error handling\n    df2 = df.drop(df.index[0])\n    with pytest.raises(Exception) as e_info:\n        state_sdk.wb.log_inferences(\n            model_id=mock_model_id, processed=df, non_processed=df2\n        )\n\n\n@pytest.mark.order(get_order_number(\"sdk_log_inferences\"))\ndef test_sdk_log_inferences(client):\n    mock_model_id = \"mock_model_id\"\n    df = pd.read_csv(\"whitebox/analytics/data/testing/classification_test_data.csv\")\n\n    with requests_mock.Mocker() as m:\n        m.post(\n            url=f\"{state_sdk.wb.host}/v1/inference-rows/batch\",\n            headers={\"api-key\": state_sdk.wb.api_key},\n            status_code=status.HTTP_201_CREATED,\n        )\n\n        happy_result = state_sdk.wb.log_inferences(\n            model_id=mock_model_id,\n            processed=df,\n            non_processed=df,\n            timestamps=timestamps,\n            actuals=mixed_actuals,\n        )\n        assert happy_result == True\n\n        m.post(\n            url=f\"{state_sdk.wb.host}/v1/inference-rows/batch\",\n            headers={\"api-key\": state_sdk.wb.api_key},\n            status_code=status.HTTP_400_BAD_REQUEST,\n        )\n\n        sad_result = state_sdk.wb.log_inferences(\n            model_id=mock_model_id,\n            processed=df,\n            non_processed=df,\n            timestamps=timestamps,\n        )\n        assert sad_result == False\n\n    # drop a row in df to test the dataframe length error handling\n    df2 = df.drop(df.index[0])\n    with pytest.raises(Exception) as e_info:\n        state_sdk.wb.log_inferences(\n            model_id=mock_model_id,\n            processed=df,\n            non_processed=df2,\n            timestamps=timestamps,\n            actuals=mixed_actuals,\n        )\n\n\n@pytest.mark.order(get_order_number(\"sdk_create_model_monitor\"))\ndef test_sdk_create_model_monitor(client):\n    with requests_mock.Mocker() as m:\n        m.post(\n            url=f\"{state_sdk.wb.host}/v1/model-monitors\",\n            json=model_monitor_concept_drift_create_payload,\n            headers={\"api-key\": state_sdk.wb.api_key},\n        )\n\n        model_monitor = state_sdk.wb.create_model_monitor(\n            model_id=\"mock_model_id\",\n            name=\"test\",\n            status=MonitorStatus.active,\n            metric=MonitorMetrics.accuracy,\n            feature=\"feature1\",\n            lower_threshold=0.7,\n            severity=AlertSeverity.high,\n            email=\"jackie.chan@chinamail.io\",\n        )\n\n        assert model_monitor is not None\n\n\n@pytest.mark.order(get_order_number(\"sdk_update_model_monitor\"))\ndef test_sdk_update_model_monitor(client):\n    mock_model_monitor_id = \"mock_model_monitor_id\"\n\n    with requests_mock.Mocker() as m:\n        m.put(\n            url=f\"{state_sdk.wb.host}/v1/model-monitors/{mock_model_monitor_id}\",\n            json=model_monitor_update_payload,\n            headers={\"api-key\": state_sdk.wb.api_key},\n        )\n\n        happy_result = state_sdk.wb.update_model_monitor(\n            model_monitor_id=mock_model_monitor_id,\n            name=\"concept drift monitor updated\",\n            lower_threshold=0.54,\n        )\n\n        assert happy_result == True\n\n        m.put(\n            url=f\"{state_sdk.wb.host}/v1/model-monitors/{mock_model_monitor_id}\",\n            json=model_monitor_update_payload,\n            headers={\"api-key\": state_sdk.wb.api_key},\n            status_code=status.HTTP_400_BAD_REQUEST,\n        )\n\n        sad_result = state_sdk.wb.update_model_monitor(\n            model_monitor_id=mock_model_monitor_id,\n            name=\"concept drift monitor updated\",\n            lower_threshold=0.54,\n        )\n\n        assert sad_result == False\n\n\n@pytest.mark.order(get_order_number(\"sdk_delete_model_monitor\"))\ndef test_sdk_delete_model_monitor(client):\n    mock_model_monitor_id = \"mock_model_monitor_id\"\n\n    with requests_mock.Mocker() as m:\n        m.delete(\n            url=f\"{state_sdk.wb.host}/v1/model-monitors/{mock_model_monitor_id}\",\n            headers={\"api-key\": state_sdk.wb.api_key},\n            status_code=status.HTTP_200_OK,\n        )\n\n        happy_result = state_sdk.wb.delete_model_monitor(\n            model_monitor_id=mock_model_monitor_id\n        )\n        assert happy_result == True\n\n        m.delete(\n            url=f\"{state_sdk.wb.host}/v1/model-monitors/{mock_model_monitor_id}\",\n            headers={\"api-key\": state_sdk.wb.api_key},\n            status_code=status.HTTP_400_BAD_REQUEST,\n        )\n\n        sad_result = state_sdk.wb.delete_model_monitor(\n            model_monitor_id=mock_model_monitor_id\n        )\n        assert sad_result == False\n\n\n@pytest.mark.order(get_order_number(\"sdk_get_alerts\"))\ndef test_sdk_get_alerts(client):\n    mock_model_id = \"mock_model_id\"\n    with requests_mock.Mocker() as m:\n        m.get(\n            url=f\"{state_sdk.wb.host}/v1/alerts?model_id={mock_model_id}\",\n            headers={\"api-key\": state_sdk.wb.api_key},\n            status_code=status.HTTP_404_NOT_FOUND,\n        )\n\n        not_found_result = state_sdk.wb.get_alerts(model_id=mock_model_id)\n\n        assert not_found_result == None\n\n        m.get(\n            url=f\"{state_sdk.wb.host}/v1/alerts?model_id={mock_model_id}\",\n            headers={\"api-key\": state_sdk.wb.api_key},\n            json=alert_payload,\n        )\n\n        alert = state_sdk.wb.get_alerts(model_id=mock_model_id)\n\n        assert alert == alert_payload\n\n\n@pytest.mark.order(get_order_number(\"sdk_get_drifting_metrics\"))\ndef test_sdk_get_drifting_metrics(client):\n    mock_model_id = \"mock_model_id\"\n    with requests_mock.Mocker() as m:\n        m.get(\n            url=f\"{state_sdk.wb.host}/v1/drifting-metrics?model_id={mock_model_id}\",\n            headers={\"api-key\": state_sdk.wb.api_key},\n            status_code=status.HTTP_404_NOT_FOUND,\n        )\n\n        not_found_result = state_sdk.wb.get_drifting_metrics(model_id=mock_model_id)\n\n        assert not_found_result == None\n\n        m.get(\n            url=f\"{state_sdk.wb.host}/v1/drifting-metrics?model_id={mock_model_id}\",\n            headers={\"api-key\": state_sdk.wb.api_key},\n            json=drifting_metrics_report_payload,\n        )\n\n        drifting_report = state_sdk.wb.get_drifting_metrics(model_id=mock_model_id)\n\n        assert drifting_report == drifting_metrics_report_payload\n\n\n@pytest.mark.order(get_order_number(\"sdk_get_descriptive_statistics\"))\ndef test_sdk_get_descriptive_statistics(client):\n    mock_model_id = \"mock_model_id\"\n    with requests_mock.Mocker() as m:\n        m.get(\n            url=f\"{state_sdk.wb.host}/v1/model-integrity-metrics?model_id={mock_model_id}\",\n            headers={\"api-key\": state_sdk.wb.api_key},\n            status_code=status.HTTP_404_NOT_FOUND,\n        )\n\n        not_found_result = state_sdk.wb.get_descriptive_statistics(\n            model_id=mock_model_id\n        )\n\n        assert not_found_result == None\n\n        m.get(\n            url=f\"{state_sdk.wb.host}/v1/model-integrity-metrics?model_id={mock_model_id}\",\n            headers={\"api-key\": state_sdk.wb.api_key},\n            json=descriptive_statistics_report_payload,\n        )\n\n        descriptive_report = state_sdk.wb.get_descriptive_statistics(\n            model_id=mock_model_id\n        )\n\n        assert descriptive_report == descriptive_statistics_report_payload\n\n\n@pytest.mark.order(get_order_number(\"sdk_get_performance_metrics\"))\ndef test_sdk_get_performance_metrics(client):\n    mock_model_id = \"mock_model_id\"\n    with requests_mock.Mocker() as m:\n        m.get(\n            url=f\"{state_sdk.wb.host}/v1/performance-metrics?model_id={mock_model_id}\",\n            headers={\"api-key\": state_sdk.wb.api_key},\n            status_code=status.HTTP_404_NOT_FOUND,\n        )\n\n        not_found_result = state_sdk.wb.get_performance_metrics(model_id=mock_model_id)\n\n        assert not_found_result == None\n\n        m.get(\n            url=f\"{state_sdk.wb.host}/v1/performance-metrics?model_id={mock_model_id}\",\n            headers={\"api-key\": state_sdk.wb.api_key},\n            json=performance_metrics_report_payload,\n        )\n\n        performance_report = state_sdk.wb.get_performance_metrics(\n            model_id=mock_model_id\n        )\n\n        assert performance_report == performance_metrics_report_payload\n\n\n@pytest.mark.order(get_order_number(\"sdk_get_xai_row\"))\ndef test_sdk_get_xai_row(client):\n    mock_inference_id = \"mock_inference_id\"\n    with requests_mock.Mocker() as m:\n        m.get(\n            url=f\"{state_sdk.wb.host}/v1/inference-rows/{mock_inference_id}/xai\",\n            headers={\"api-key\": state_sdk.wb.api_key},\n            status_code=status.HTTP_404_NOT_FOUND,\n        )\n\n        not_found_result = state_sdk.wb.get_xai_row(inference_row_id=mock_inference_id)\n\n        assert not_found_result == None\n\n        m.get(\n            url=f\"{state_sdk.wb.host}/v1/inference-rows/{mock_inference_id}/xai\",\n            headers={\"api-key\": state_sdk.wb.api_key},\n            json=inference_row_xai_payload,\n        )\n\n        xai = state_sdk.wb.get_xai_row(inference_row_id=mock_inference_id)\n\n        assert xai == inference_row_xai_payload\n"}
{"type": "test_file", "path": "whitebox/tests/v1/test_alerts.py", "content": "import pytest\nfrom whitebox import schemas\nfrom whitebox.tests.v1.conftest import get_order_number, state\nfrom fastapi import status\n\n\n@pytest.mark.order(get_order_number(\"alerts_get\"))\ndef test_alerts_get_model_all(client, api_key):\n    response_all = client.get(\n        f\"/v1/alerts\",\n        headers={\"api-key\": api_key},\n    )\n\n    response_model_all = client.get(\n        f\"/v1/alerts?model_id={state.model_multi['id']}\",\n        headers={\"api-key\": api_key},\n    )\n\n    response_wrong_model = client.get(\n        f\"/v1/alerts?model_id=wrong_model_id\",\n        headers={\"api-key\": api_key},\n    )\n\n    assert len(response_all.json()) == 6\n\n    assert response_all.status_code == status.HTTP_200_OK\n    assert response_model_all.status_code == status.HTTP_200_OK\n    assert response_wrong_model.status_code == status.HTTP_404_NOT_FOUND\n\n    validated = [schemas.Alert(**m) for m in response_all.json()]\n    validated = [schemas.Alert(**m) for m in response_model_all.json()]\n"}
{"type": "test_file", "path": "whitebox/analytics/tests/test_pipelines.py", "content": "from whitebox.analytics.metrics.pipelines import *\nfrom whitebox.analytics.drift.pipelines import *\nfrom whitebox.analytics.models.pipelines import *\nfrom whitebox.analytics.xai_models.pipelines import *\nfrom unittest import TestCase\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.datasets import load_breast_cancer, load_wine, load_diabetes\n\ntest_model_id = \"test_model_id\"\n\ntest_metrics_df = pd.read_csv(\"whitebox/analytics/data/testing/metrics_test_data.csv\")\ntest_classification_df = pd.read_csv(\n    \"whitebox/analytics/data/testing/classification_test_data.csv\"\n)\ndrift_data = fetch_california_housing(as_frame=True)\ndrift_data = drift_data.frame\nreference = drift_data.head(500)\ncurrent = drift_data.iloc[1000:1200]\nreference_concept_drift = test_classification_df.head(5)\ncurrent_concept_drift = test_classification_df.tail(5)\nconcept_drift_detected_dataset = pd.read_csv(\n    \"whitebox/analytics/data/testing/udemy_fin_adj.csv\"\n)\nreference_concept_drift_detected = concept_drift_detected_dataset.head(1000)\ncurrent_concept_drift_detected = concept_drift_detected_dataset.tail(1000)\ndf_load_binary = load_breast_cancer()\ndf_binary = pd.DataFrame(df_load_binary.data, columns=df_load_binary.feature_names)\ndf_binary[\"target\"] = df_load_binary.target\ndf_binary_inference = df_binary.drop(columns=[\"target\"])\ndf_binary_inference = df_binary_inference.tail(10)\ndf_binary_inference_row1 = df_binary_inference.iloc[7]\ndf_binary_inference_row2 = df_binary_inference.iloc[3]\ndf_load_multi = load_wine()\ndf_multi = pd.DataFrame(df_load_multi.data, columns=df_load_multi.feature_names)\ndf_multi[\"target\"] = df_load_multi.target\ndf_multi_inference = df_multi.drop(columns=[\"target\"])\ndf_multi_inference = df_multi_inference.tail(10)\ndf_multi_inference_row1 = df_multi_inference.iloc[4]\ndf_multi_inference_row2 = df_multi_inference.iloc[2]\n\ntest_regression_df = pd.read_csv(\n    \"whitebox/analytics/data/testing/regression_test_data.csv\"\n)\ndf_load_reg = load_diabetes()\ndf_reg = pd.DataFrame(df_load_reg.data, columns=df_load_reg.feature_names)\ndf_reg[\"target\"] = df_load_reg.target\ndf_reg_inference = df_reg.drop(columns=[\"target\"])\ndf_reg_inference = df_reg_inference.tail(10)\ndf_reg_inference_row1 = df_reg_inference.iloc[7]\ndf_reg_inference_row2 = df_reg_inference.iloc[3]\n\n\nclass TestNodes:\n    def test_create_feature_metrics_pipeline(self):\n        features_metrics = dict(create_feature_metrics_pipeline(test_metrics_df))\n        missing_count = features_metrics[\"missing_count\"]\n        non_missing_count = features_metrics[\"non_missing_count\"]\n        mean = features_metrics[\"mean\"]\n        minimum = features_metrics[\"minimum\"]\n        maximum = features_metrics[\"maximum\"]\n        sum = features_metrics[\"sum\"]\n        standard_deviation = features_metrics[\"standard_deviation\"]\n        variance = features_metrics[\"variance\"]\n        TestCase().assertDictEqual(\n            {\"num1\": 1, \"num2\": 2, \"num3\": 0, \"cat1\": 1, \"cat2\": 2}, missing_count\n        )\n        TestCase().assertDictEqual(\n            {\"num1\": 9, \"num2\": 8, \"num3\": 10, \"cat1\": 9, \"cat2\": 8}, non_missing_count\n        )\n        TestCase().assertDictEqual(\n            {\"num1\": 156.33333333333334, \"num2\": 9.223817500000001, \"num3\": 1.0}, mean\n        )\n        TestCase().assertDictEqual({\"num1\": 0.0, \"num2\": 0.00054, \"num3\": 0.0}, minimum)\n        TestCase().assertDictEqual(\n            {\"num1\": 1000.0, \"num2\": 45.896, \"num3\": 2.0}, maximum\n        )\n        TestCase().assertDictEqual(\n            {\"num1\": 1407.0, \"num2\": 73.79054000000001, \"num3\": 10.0}, sum\n        )\n        TestCase().assertDictEqual(\n            {\n                \"num1\": 322.0283372624217,\n                \"num2\": 15.488918075768835,\n                \"num3\": 0.816496580927726,\n            },\n            standard_deviation,\n        )\n        TestCase().assertDictEqual(\n            {\n                \"num1\": 103702.25000000001,\n                \"num2\": 239.90658315787854,\n                \"num3\": 0.6666666666666666,\n            },\n            variance,\n        )\n\n    def test_create_binary_classification_evaluation_metrics_pipeline(self):\n        binary_metrics = dict(\n            create_binary_classification_evaluation_metrics_pipeline(\n                test_classification_df[\"y_testing_binary\"],\n                test_classification_df[\"y_prediction_binary\"],\n                labels=[0, 1],\n            )\n        )\n\n        binary_metrics_edge_case = dict(\n            create_binary_classification_evaluation_metrics_pipeline(\n                test_classification_df[\"y_testing_binary\"].tail(1),\n                test_classification_df[\"y_prediction_binary\"].tail(1),\n                labels=[0, 1],\n            )\n        )\n\n        assert binary_metrics_edge_case[\"true_positive\"] == 1\n        assert binary_metrics[\"accuracy\"] == 0.6\n        assert binary_metrics[\"precision\"] == 0.6\n        assert binary_metrics[\"recall\"] == 0.6\n        assert binary_metrics[\"f1\"] == 0.6\n        assert binary_metrics[\"true_negative\"] == 3\n        assert binary_metrics[\"false_positive\"] == 2\n        assert binary_metrics[\"false_negative\"] == 2\n        assert binary_metrics[\"true_positive\"] == 3\n\n    def test_create_multiple_classification_evaluation_metrics_pipeline(self):\n        multi_metrics = create_multiple_classification_evaluation_metrics_pipeline(\n            test_classification_df[\"y_testing_multi\"],\n            test_classification_df[\"y_prediction_multi\"],\n            labels=[0, 1, 2],\n        )\n\n        multi_metrics_edge_case = (\n            create_multiple_classification_evaluation_metrics_pipeline(\n                test_classification_df[\"y_testing_multi\"].tail(1),\n                test_classification_df[\"y_prediction_multi\"].tail(1),\n                labels=[0, 1, 2],\n            )\n        )\n\n        assert multi_metrics.accuracy == 0.6\n\n        TestCase().assertDictEqual(\n            {\n                \"true_negative\": 0,\n                \"false_positive\": 0,\n                \"false_negative\": 0,\n                \"true_positive\": 1,\n            },\n            multi_metrics_edge_case.confusion_matrix[\"class1\"].dict(),\n        )\n\n        TestCase().assertDictEqual(\n            {\"micro\": 0.6, \"macro\": 0.6444444444444445, \"weighted\": 0.64},\n            multi_metrics.precision.dict(),\n        )\n        TestCase().assertDictEqual(\n            {\"micro\": 0.6, \"macro\": 0.5833333333333334, \"weighted\": 0.6},\n            multi_metrics.recall.dict(),\n        )\n        TestCase().assertDictEqual(\n            {\"micro\": 0.6, \"macro\": 0.6, \"weighted\": 0.6066666666666667},\n            multi_metrics.f1.dict(),\n        )\n        TestCase().assertDictEqual(\n            {\n                \"true_negative\": 5,\n                \"false_positive\": 2,\n                \"false_negative\": 2,\n                \"true_positive\": 1,\n            },\n            multi_metrics.confusion_matrix[\"class0\"].dict(),\n        )\n        TestCase().assertDictEqual(\n            {\n                \"true_negative\": 4,\n                \"false_positive\": 2,\n                \"false_negative\": 1,\n                \"true_positive\": 3,\n            },\n            multi_metrics.confusion_matrix[\"class1\"].dict(),\n        )\n        TestCase().assertDictEqual(\n            {\n                \"true_negative\": 7,\n                \"false_positive\": 0,\n                \"false_negative\": 1,\n                \"true_positive\": 2,\n            },\n            multi_metrics.confusion_matrix[\"class2\"].dict(),\n        )\n\n    def test_create_regression_evaluation_metrics_pipeline(self):\n        reg_report = dict(\n            create_regression_evaluation_metrics_pipeline(\n                test_regression_df[\"y_test\"], test_regression_df[\"y_prediction\"]\n            )\n        )\n        rsq = reg_report[\"r_square\"]\n        mse = reg_report[\"mean_squared_error\"]\n        mae = reg_report[\"mean_absolute_error\"]\n        assert (rsq) == 0.9044\n        assert (mse) == 0.0071\n        assert (mae) == 0.037\n\n    def test_create_data_drift_pipeline(self):\n        data_drift_report = dict(run_data_drift_pipeline(reference, current))\n        assert data_drift_report[\"number_of_columns\"] == 9\n        assert data_drift_report[\"number_of_drifted_columns\"] == 7\n        assert (\n            round(\n                dict(data_drift_report[\"drift_by_columns\"][\"Population\"])[\n                    \"drift_score\"\n                ],\n                2,\n            )\n            == 0.06\n        )\n        assert (\n            dict(data_drift_report[\"drift_by_columns\"][\"Longitude\"])[\"drift_detected\"]\n            == True\n        )\n        assert (\n            dict(data_drift_report[\"drift_by_columns\"][\"AveBedrms\"])[\"drift_detected\"]\n            == False\n        )\n\n    def test_create_concept_drift_pipeline_drift_not_detected(self):\n        concept_drift_report = vars(\n            run_concept_drift_pipeline(\n                reference_concept_drift, current_concept_drift, \"y_testing_multi\"\n            )\n        )\n        assert list(concept_drift_report.keys()) == [\n            \"concept_drift_summary\",\n            \"column_correlation\",\n        ]\n        assert (\n            round(vars(concept_drift_report[\"concept_drift_summary\"])[\"drift_score\"], 3)\n            == 0.082\n        )\n        assert (\n            vars(concept_drift_report[\"concept_drift_summary\"])[\"drift_detected\"]\n            == False\n        )\n\n    def test_create_concept_drift_pipeline_drift_detected(self):\n        concept_drift_report = vars(\n            run_concept_drift_pipeline(\n                reference_concept_drift_detected,\n                current_concept_drift_detected,\n                \"discount_price__currency\",\n            )\n        )\n        assert list(concept_drift_report.keys()) == [\n            \"concept_drift_summary\",\n            \"column_correlation\",\n        ]\n        assert (\n            round(vars(concept_drift_report[\"concept_drift_summary\"])[\"drift_score\"], 3)\n            == 0.008\n        )\n        assert (\n            vars(concept_drift_report[\"concept_drift_summary\"])[\"drift_detected\"]\n            == True\n        )\n\n    def test_create_binary_classification_training_model_pipeline(self):\n        model, eval = create_binary_classification_training_model_pipeline(\n            df_binary, \"target\", test_model_id\n        )\n        eval_score = eval[\"roc_auc_score\"]\n        assert (round(eval_score, 3)) == 0.976\n\n    def test_create_multiclass_classification_training_model_pipeline(self):\n        model, eval = create_multiclass_classification_training_model_pipeline(\n            df_multi, \"target\", test_model_id\n        )\n        eval_score = eval[\"precision\"]\n        assert (round(eval_score, 2)) == 0.96\n\n    def test_create_regression_training_model_pipeline(self):\n        model, eval = create_regression_training_model_pipeline(\n            df_reg, \"target\", test_model_id\n        )\n        eval_score = eval[\"r2_score\"]\n        assert (eval_score) == 0.2576\n\n    def test_create_xai_pipeline_per_inference_row(self):\n        binary_class_report1 = create_xai_pipeline_per_inference_row(\n            df_binary, \"target\", df_binary_inference_row1, \"binary\", test_model_id\n        )\n        multi_class_report1 = create_xai_pipeline_per_inference_row(\n            df_multi, \"target\", df_multi_inference_row1, \"multi_class\", test_model_id\n        )\n        regression_report1 = create_xai_pipeline_per_inference_row(\n            df_reg, \"target\", df_reg_inference_row1, \"regression\", test_model_id\n        )\n        binary_class_report2 = create_xai_pipeline_per_inference_row(\n            df_binary, \"target\", df_binary_inference_row2, \"binary\", test_model_id\n        )\n        multi_class_report2 = create_xai_pipeline_per_inference_row(\n            df_multi, \"target\", df_multi_inference_row2, \"multi_class\", test_model_id\n        )\n        regression_report2 = create_xai_pipeline_per_inference_row(\n            df_reg, \"target\", df_reg_inference_row2, \"regression\", test_model_id\n        )\n\n        binary_contribution_check_one = binary_class_report1[\"worst perimeter\"]\n        binary_contribution_check_two = binary_class_report2[\"worst texture\"]\n        multi_contribution_check_one = multi_class_report1[\"hue\"]\n        multi_contribution_check_two = multi_class_report2[\"alcohol\"]\n        regression_contribution_check_one = regression_report1[\"sex\"]\n        regression_contribution_check_two = regression_report2[\"bp\"]\n\n        assert (round(binary_contribution_check_one, 3)) == -0.464\n        assert (round(binary_contribution_check_two, 1)) == -0.1\n        assert (round(multi_contribution_check_one, 2)) == -0.09\n        assert (round(multi_contribution_check_two, 3)) == 0.076\n        assert (round(regression_contribution_check_one, 2)) == 9.48\n        assert (round(regression_contribution_check_two, 3)) == 14.079\n"}
{"type": "test_file", "path": "whitebox/tests/unit_tests/test_unit.py", "content": "from whitebox.cron_tasks.shared import change_timestamp\nfrom datetime import datetime\n\n\nclass TestNodes:\n    def test_round_timestamp(self):\n        timestamp = datetime(2023, 3, 7, 15, 34, 23)\n        start_time = datetime(2023, 3, 6)\n\n        assert change_timestamp(timestamp, start_time, 15, \"T\") == datetime(\n            2023, 3, 7, 15, 45\n        )\n        assert change_timestamp(timestamp, start_time, 5, \"H\") == datetime(\n            2023, 3, 7, 16, 0\n        )\n        assert change_timestamp(timestamp, start_time, 2, \"D\") == datetime(2023, 3, 8)\n        assert change_timestamp(timestamp, start_time, 1, \"W\") == datetime(2023, 3, 13)\n"}
{"type": "test_file", "path": "whitebox/tests/v1/test_models.py", "content": "from whitebox.tests.v1.mock_data import (\n    model_binary_create_payload,\n    model_multi_create_payload,\n    model_multi_2_create_payload,\n    model_multi_3_create_payload,\n    model_regression_create_payload,\n    model_update_payload,\n)\nimport pytest\nfrom whitebox import schemas\nfrom whitebox.tests.v1.conftest import get_order_number, state\nfrom fastapi import status\n\n\n@pytest.mark.order(get_order_number(\"models_create\"))\ndef test_model_create(client, api_key):\n    response_binary = client.post(\n        \"/v1/models\",\n        json={**model_binary_create_payload},\n        headers={\"api-key\": api_key},\n    )\n\n    response_multi = client.post(\n        \"/v1/models\",\n        json={**model_multi_create_payload},\n        headers={\"api-key\": api_key},\n    )\n\n    response_multi_2 = client.post(\n        \"/v1/models\",\n        json={**model_multi_2_create_payload},\n        headers={\"api-key\": api_key},\n    )\n\n    response_multi_3 = client.post(\n        \"/v1/models\",\n        json={**model_multi_3_create_payload},\n        headers={\"api-key\": api_key},\n    )\n\n    response_regression = client.post(\n        \"/v1/models\",\n        json={**model_regression_create_payload},\n        headers={\"api-key\": api_key},\n    )\n\n    state.model_binary = response_binary.json()\n    state.model_multi = response_multi.json()\n    state.model_multi_2 = response_multi_2.json()\n    state.model_multi_3 = response_multi_3.json()\n    state.model_regression = response_regression.json()\n\n    assert response_binary.status_code == status.HTTP_201_CREATED\n    assert response_multi.status_code == status.HTTP_201_CREATED\n    assert response_multi_2.status_code == status.HTTP_201_CREATED\n    assert response_multi_3.status_code == status.HTTP_201_CREATED\n    assert response_regression.status_code == status.HTTP_201_CREATED\n\n    validated = schemas.Model(**response_binary.json())\n    validated = schemas.Model(**response_multi.json())\n    validated = schemas.Model(**response_multi_2.json())\n    validated = schemas.Model(**response_multi_3.json())\n    validated = schemas.Model(**response_regression.json())\n\n\n@pytest.mark.order(get_order_number(\"models_get_all\"))\ndef test_model_get_all(client, api_key):\n    response = client.get(f\"/v1/models\", headers={\"api-key\": api_key})\n    assert response.status_code == status.HTTP_200_OK\n    validated = [schemas.Model(**m) for m in response.json()]\n\n\n@pytest.mark.order(get_order_number(\"models_get\"))\ndef test_model_get(client, api_key):\n    response = client.get(\n        f\"/v1/models/{state.model_multi['id']}\", headers={\"api-key\": api_key}\n    )\n    response_wrong_model = client.get(\n        f\"/v1/models/wrong_model_id\", headers={\"api-key\": api_key}\n    )\n\n    assert response.status_code == status.HTTP_200_OK\n    assert response_wrong_model.status_code == status.HTTP_404_NOT_FOUND\n\n    validated = schemas.Model(**response.json())\n\n\n@pytest.mark.order(get_order_number(\"models_update\"))\ndef test_model_update(client, api_key):\n    response = client.put(\n        f\"/v1/models/{state.model_multi['id']}\",\n        json=model_update_payload,\n        headers={\"api-key\": api_key},\n    )\n    response_wrong_model = client.put(\n        f\"/v1/models/wrong_model_id\",\n        json=model_update_payload,\n        headers={\"api-key\": api_key},\n    )\n\n    assert response.status_code == status.HTTP_200_OK\n    assert response_wrong_model.status_code == status.HTTP_404_NOT_FOUND\n\n    validated = schemas.Model(**response.json())\n\n\n@pytest.mark.order(get_order_number(\"models_delete\"))\ndef test_model_delete(client, api_key):\n    response_binary = client.delete(\n        f\"/v1/models/{state.model_binary['id']}\", headers={\"api-key\": api_key}\n    )\n    response_multi = client.delete(\n        f\"/v1/models/{state.model_multi['id']}\", headers={\"api-key\": api_key}\n    )\n    response_multi_2 = client.delete(\n        f\"/v1/models/{state.model_multi_2['id']}\", headers={\"api-key\": api_key}\n    )\n    response_multi_3 = client.delete(\n        f\"/v1/models/{state.model_multi_3['id']}\", headers={\"api-key\": api_key}\n    )\n    response_regression = client.delete(\n        f\"/v1/models/{state.model_regression['id']}\", headers={\"api-key\": api_key}\n    )\n    response_no_model = client.delete(\n        f\"/v1/models/{state.model_binary['id']}\", headers={\"api-key\": api_key}\n    )\n\n    assert response_binary.status_code == status.HTTP_200_OK\n    assert response_multi.status_code == status.HTTP_200_OK\n    assert response_multi_2.status_code == status.HTTP_200_OK\n    assert response_multi_3.status_code == status.HTTP_200_OK\n    assert response_regression.status_code == status.HTTP_200_OK\n    assert response_no_model.status_code == status.HTTP_404_NOT_FOUND\n"}
{"type": "test_file", "path": "whitebox/tests/v1/test_inference_rows.py", "content": "from whitebox.tests.v1.mock_data import (\n    inference_row_create_single_row_payload,\n    inference_row_create_many_binary_payload,\n    inference_row_create_many_multi_payload,\n    inference_row_create_many_multi_no_actual_payload,\n    inference_row_create_many_multi_mixed_actuals_payload,\n    inference_row_create_many_reg_payload,\n)\nimport pytest\nfrom whitebox import schemas\nfrom whitebox.tests.v1.conftest import get_order_number, state\nfrom fastapi import status\n\n\n@pytest.mark.order(get_order_number(\"inference_rows_create\"))\ndef test_inference_row_create(client, api_key):\n    response = client.post(\n        \"/v1/inference-rows\",\n        json={\n            **inference_row_create_single_row_payload,\n            \"model_id\": state.model_multi[\"id\"],\n        },\n        headers={\"api-key\": api_key},\n    )\n    state.inference_row_multi = response.json()\n    assert response.status_code == status.HTTP_201_CREATED\n    validated = schemas.InferenceRow(**response.json())\n\n\n@pytest.mark.order(get_order_number(\"inference_rows_create_many\"))\ndef test_inference_row_create_many(client, api_key):\n    response_binary = client.post(\n        \"/v1/inference-rows/batch\",\n        json=list(\n            map(\n                lambda x: {**x, \"model_id\": state.model_binary[\"id\"]},\n                inference_row_create_many_binary_payload,\n            )\n        ),\n        headers={\"api-key\": api_key},\n    )\n\n    state.inference_row_binary = response_binary.json()[0]\n\n    response_multi = client.post(\n        \"/v1/inference-rows/batch\",\n        json=list(\n            map(\n                lambda x: {**x, \"model_id\": state.model_multi[\"id\"]},\n                inference_row_create_many_multi_payload,\n            )\n        ),\n        headers={\"api-key\": api_key},\n    )\n\n    response_multi_2 = client.post(\n        \"/v1/inference-rows/batch\",\n        json=list(\n            map(\n                lambda x: {**x, \"model_id\": state.model_multi_2[\"id\"]},\n                inference_row_create_many_multi_no_actual_payload,\n            )\n        ),\n        headers={\"api-key\": api_key},\n    )\n\n    response_multi_3 = client.post(\n        \"/v1/inference-rows/batch\",\n        json=list(\n            map(\n                lambda x: {**x, \"model_id\": state.model_multi_3[\"id\"]},\n                inference_row_create_many_multi_mixed_actuals_payload,\n            )\n        ),\n        headers={\"api-key\": api_key},\n    )\n\n    response_reg = client.post(\n        \"/v1/inference-rows/batch\",\n        json=list(\n            map(\n                lambda x: {**x, \"model_id\": state.model_regression[\"id\"]},\n                inference_row_create_many_reg_payload,\n            )\n        ),\n        headers={\"api-key\": api_key},\n    )\n\n    assert response_binary.status_code == status.HTTP_201_CREATED\n    assert response_multi.status_code == status.HTTP_201_CREATED\n    assert response_reg.status_code == status.HTTP_201_CREATED\n    validated = [schemas.InferenceRow(**m) for m in response_binary.json()]\n    validated = [schemas.InferenceRow(**m) for m in response_multi.json()]\n    validated = [schemas.InferenceRow(**m) for m in response_multi_2.json()]\n    validated = [schemas.InferenceRow(**m) for m in response_multi_3.json()]\n    validated = [schemas.InferenceRow(**m) for m in response_reg.json()]\n\n\n@pytest.mark.order(get_order_number(\"inference_rows_get_model's_all\"))\ndef test_inference_row_get_models_all(client, api_key):\n    response_multi = client.get(\n        f\"/v1/inference-rows?model_id={state.model_multi['id']}\",\n        headers={\"api-key\": api_key},\n    )\n    response_binary = client.get(\n        f\"/v1/inference-rows?model_id={state.model_binary['id']}\",\n        headers={\"api-key\": api_key},\n    )\n    response_wrong_model = client.get(\n        f\"/v1/inference-rows?model_id=wrong_model_id\",\n        headers={\"api-key\": api_key},\n    )\n    assert response_multi.status_code == status.HTTP_200_OK\n    assert response_binary.status_code == status.HTTP_200_OK\n    assert response_wrong_model.status_code == status.HTTP_404_NOT_FOUND\n    validated = [schemas.InferenceRow(**m) for m in response_multi.json()]\n    validated = [schemas.InferenceRow(**m) for m in response_binary.json()]\n\n\n@pytest.mark.order(get_order_number(\"inference_rows_get\"))\ndef test_inference_row_get(client, api_key):\n    response = client.get(\n        f\"/v1/inference-rows/{state.inference_row_multi['id']}\",\n        headers={\"api-key\": api_key},\n    )\n    response_wrong_inference = client.get(\n        f\"/v1/inference-rows/wrong_inference_id\",\n        headers={\"api-key\": api_key},\n    )\n    assert response.status_code == status.HTTP_200_OK\n    assert response_wrong_inference.status_code == status.HTTP_404_NOT_FOUND\n    validated = schemas.InferenceRow(**response.json())\n\n\n@pytest.mark.order(get_order_number(\"inference_rows_xai\"))\ndef test_inference_row_xai(client, api_key):\n    response = client.get(\n        f\"/v1/inference-rows/{state.inference_row_binary['id']}/xai\",\n        headers={\"api-key\": api_key},\n    )\n\n    response_wrong_inference = client.get(\n        f\"/v1/inference-rows/wrong_inference_id/xai\",\n        headers={\"api-key\": api_key},\n    )\n\n    assert response.status_code == status.HTTP_200_OK\n    assert response_wrong_inference.status_code == status.HTTP_404_NOT_FOUND\n\n\n@pytest.mark.order(get_order_number(\"inference_rows_create_many_after_x_time\"))\ndef test_inference_row_create_many_after_x_time(client, api_key):\n    response_binary = client.post(\n        \"/v1/inference-rows/batch\",\n        json=list(\n            map(\n                lambda x: {**x, \"model_id\": state.model_binary[\"id\"]},\n                inference_row_create_many_binary_payload,\n            )\n        ),\n        headers={\"api-key\": api_key},\n    )\n\n    response_multi = client.post(\n        \"/v1/inference-rows/batch\",\n        json=list(\n            map(\n                lambda x: {**x, \"model_id\": state.model_multi[\"id\"]},\n                inference_row_create_many_multi_payload,\n            )\n        ),\n        headers={\"api-key\": api_key},\n    )\n\n    response_reg = client.post(\n        \"/v1/inference-rows/batch\",\n        json=list(\n            map(\n                lambda x: {**x, \"model_id\": state.model_regression[\"id\"]},\n                inference_row_create_many_reg_payload,\n            )\n        ),\n        headers={\"api-key\": api_key},\n    )\n\n    assert response_binary.status_code == status.HTTP_201_CREATED\n    assert response_multi.status_code == status.HTTP_201_CREATED\n    assert response_reg.status_code == status.HTTP_201_CREATED\n    validated = [schemas.InferenceRow(**m) for m in response_binary.json()]\n    validated = [schemas.InferenceRow(**m) for m in response_multi.json()]\n    validated = [schemas.InferenceRow(**m) for m in response_reg.json()]\n"}
{"type": "test_file", "path": "whitebox/tests/v1/test_errors.py", "content": "import pytest\nfrom whitebox.tests.v1.conftest import get_order_number\nfrom fastapi import status\n\n\n@pytest.mark.order(get_order_number(\"models_no_api_key\"))\ndef test_model_no_api_key(client):\n    response = client.get(\"/v1/models\")\n\n    assert response.status_code == status.HTTP_422_UNPROCESSABLE_ENTITY\n\n\n@pytest.mark.order(get_order_number(\"models_wrong_api_key\"))\ndef test_model_wrong_api_key(client):\n    response = client.get(\n        \"/v1/models\",\n        headers={\"api-key\": \"1234567890\"},\n    )\n\n    assert response.status_code == status.HTTP_401_UNAUTHORIZED\n"}
{"type": "test_file", "path": "whitebox/tests/v1/test_drifting_metrics.py", "content": "import pytest\nfrom whitebox import schemas\nfrom whitebox.tests.v1.conftest import get_order_number, state\nfrom fastapi import status\n\n\n@pytest.mark.order(get_order_number(\"drifting_metrics_get_model_all\"))\ndef test_drifting_metric_get_model_all(client, api_key):\n    response_multi = client.get(\n        f\"/v1/drifting-metrics?model_id={state.model_multi['id']}\",\n        headers={\"api-key\": api_key},\n    )\n\n    response_binary = client.get(\n        f\"/v1/drifting-metrics?model_id={state.model_binary['id']}\",\n        headers={\"api-key\": api_key},\n    )\n    response_wrong_model = client.get(\n        f\"/v1/drifting-metrics?model_id=wrong_model_id\",\n        headers={\"api-key\": api_key},\n    )\n\n    response_binary_json = response_binary.json()\n    response_multi_json = response_multi.json()\n\n    assert len(response_multi_json) == 1\n    assert len(response_binary_json) == 1\n\n    assert response_multi_json[0][\"timestamp\"] == \"2023-03-06T12:15:00\"\n    assert response_binary_json[0][\"timestamp\"] == \"2023-03-07T00:00:00\"\n\n    assert response_multi.status_code == status.HTTP_200_OK\n    assert response_binary.status_code == status.HTTP_200_OK\n    assert response_wrong_model.status_code == status.HTTP_404_NOT_FOUND\n\n    validated = [schemas.DriftingMetric(**m) for m in response_multi_json]\n    validated = [schemas.DriftingMetric(**m) for m in response_binary_json]\n\n\n@pytest.mark.order(get_order_number(\"drifting_metrics_get_binary_model_after_x_time\"))\ndef test_drifting_metrics_get_binary_model_after_x_time(client, api_key):\n    response_binary = client.get(\n        f\"/v1/drifting-metrics?model_id={state.model_binary['id']}\",\n        headers={\"api-key\": api_key},\n    )\n\n    response_binary_json = response_binary.json()\n\n    assert len(response_binary_json) == 1\n\n    assert response_binary_json[0][\"timestamp\"] == \"2023-03-07T00:00:00\"\n\n    assert response_binary.status_code == status.HTTP_200_OK\n\n    validated = [schemas.DriftingMetric(**m) for m in response_binary_json]\n"}
{"type": "source_file", "path": "whitebox/core/settings.py", "content": "from functools import lru_cache\nfrom pydantic import BaseSettings\nimport os\n\n\nclass Settings(BaseSettings):\n    APP_NAME: str = \"\"\n    ENV: str = \"\"\n    DATABASE_URL: str = \"\"\n    VERSION: str = \"\"\n    MODEL_PATH: str = \"\"\n    SECRET_KEY: str = \"\"\n    GRANULARITY: str = \"\"\n\n    class Config:\n        env_file = f\".env.{os.getenv('ENV')}\" or \".env.dev\"\n\n\n@lru_cache()\ndef get_settings():\n    return Settings()\n\n\nclass CronSettings(Settings):\n    APP_NAME_CRON: str\n    METRICS_CRON: str\n\n    class Config:\n        env_file = f\".env.{os.getenv('ENV')}\" or \".env.dev\"\n\n\n@lru_cache()\ndef get_cron_settings():\n    return CronSettings()\n"}
{"type": "source_file", "path": "setup.py", "content": "from setuptools import setup, find_packages\nfrom pathlib import Path\n\n\nVERSION = \"0.0.20\"\n\nDESCRIPTION = \"Whitebox is an open source E2E ML monitoring platform with edge capabilities that plays nicely with kubernetes\"\nLONG_DESCRIPTION = (Path(__file__).parent / \"README.md\").read_text()\n\n# Setting up\nsetup(\n    # the name must match the folder name 'verysimplemodule'\n    name=\"whitebox-sdk\",\n    version=VERSION,\n    author=\"Squaredev\",\n    author_email=\"hello@squaredev.io\",\n    description=DESCRIPTION,\n    long_description=LONG_DESCRIPTION,  # add README.md\n    long_description_content_type=\"text/markdown\",\n    packages=find_packages(),\n    install_requires=[],  # add any additional packages that\n    # needs to be installed along with your package. Eg: 'caer'\n    keywords=[\"python\", \"model monitoring\", \"whitebox\", \"mlops\"],\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n    ],\n)\n"}
{"type": "source_file", "path": "whitebox/api/v1/inference_rows.py", "content": "from typing import Dict, List\nfrom whitebox.middleware.auth import authenticate_user\nfrom whitebox.schemas.inferenceRow import (\n    InferenceRow,\n    InferenceRowCreateDto,\n    InferenceRowPreDb,\n)\nfrom whitebox.analytics.xai_models.pipelines import (\n    create_xai_pipeline_per_inference_row,\n)\nimport pandas as pd\nfrom whitebox.schemas.user import User\nfrom fastapi import APIRouter, Depends, status\nfrom whitebox import crud\nfrom sqlalchemy.orm import Session\nfrom whitebox.core.db import get_db\nfrom whitebox.utils.errors import add_error_responses, errors\n\n\ninference_rows_router = APIRouter()\n\n\n@inference_rows_router.post(\n    \"/inference-rows\",\n    tags=[\"Inference Rows\"],\n    response_model=InferenceRow,\n    summary=\"Create an inference row\",\n    status_code=status.HTTP_201_CREATED,\n    responses=add_error_responses([400, 401]),\n)\nasync def create_row(\n    body: InferenceRowCreateDto,\n    db: Session = Depends(get_db),\n    authenticated_user: User = Depends(authenticate_user),\n) -> InferenceRow:\n    \"\"\"Inserts an inference row into the database.\"\"\"\n\n    updated_body = InferenceRowPreDb(**dict(body), is_used=False)\n\n    new_inference_row = crud.inference_rows.create(db=db, obj_in=updated_body)\n    return new_inference_row\n\n\n@inference_rows_router.post(\n    \"/inference-rows/batch\",\n    tags=[\"Inference Rows\"],\n    response_model=List[InferenceRow],\n    summary=\"Create many inference rows\",\n    status_code=status.HTTP_201_CREATED,\n    responses=add_error_responses([400, 401]),\n)\nasync def create_many_inference_rows(\n    body: List[InferenceRowCreateDto],\n    db: Session = Depends(get_db),\n    authenticated_user: User = Depends(authenticate_user),\n) -> List[InferenceRow]:\n    \"\"\"Inserts a set of inference rows into the database.\"\"\"\n\n    model = crud.models.get(db=db, _id=dict(body[0])[\"model_id\"])\n    if model:\n        for row in body:\n            if not model.target_column in row.processed:\n                return errors.bad_request(\n                    f'Column \"{model.target_column}\" was not found in some or any of the rows in provided inference dataset. Please try again!'\n                )\n\n        updated_body = [InferenceRowPreDb(**dict(x), is_used=False) for x in body]\n        new_inference_rows = crud.inference_rows.create_many(\n            db=db, obj_list=updated_body\n        )\n        return new_inference_rows\n    else:\n        return errors.not_found(f\"Model with id: {dict(body[0])['model_id']} not found\")\n\n\n@inference_rows_router.get(\n    \"/inference-rows\",\n    tags=[\"Inference Rows\"],\n    response_model=List[InferenceRow],\n    summary=\"Get all model's inference rows\",\n    status_code=status.HTTP_200_OK,\n    responses=add_error_responses([401, 404]),\n)\nasync def get_all_models_inference_rows(\n    model_id: str,\n    db: Session = Depends(get_db),\n    authenticated_user: User = Depends(authenticate_user),\n):\n    \"\"\"Fetches the inference rows of a specific model. A model id is required.\"\"\"\n\n    model = crud.models.get(db, model_id)\n    if model:\n        return crud.inference_rows.get_inference_rows_by_model(db=db, model_id=model_id)\n    else:\n        return errors.not_found(\"Model not found\")\n\n\n@inference_rows_router.get(\n    \"/inference-rows/{inference_row_id}\",\n    tags=[\"Inference Rows\"],\n    response_model=InferenceRow,\n    summary=\"Get inference row by id\",\n    status_code=status.HTTP_200_OK,\n    responses=add_error_responses([401, 404]),\n)\nasync def get_inference_row(\n    inference_row_id: str,\n    db: Session = Depends(get_db),\n    authenticated_user: User = Depends(authenticate_user),\n):\n    \"\"\"Fetches a specific inference row. An inference row id is required.\"\"\"\n\n    inference_row = crud.inference_rows.get(db=db, _id=inference_row_id)\n    if not inference_row:\n        return errors.not_found(\"Inference not found\")\n\n    return inference_row\n\n\n@inference_rows_router.get(\n    \"/inference-rows/{inference_row_id}/xai\",\n    tags=[\"Inference Rows\"],\n    response_model=Dict[str, float],\n    summary=\"Creates and fetches am explainability report for an inference row\",\n    status_code=status.HTTP_200_OK,\n    responses=add_error_responses([401, 404]),\n)\nasync def create_inference_row_xai_report(\n    inference_row_id: str,\n    db: Session = Depends(get_db),\n    authenticated_user: User = Depends(authenticate_user),\n):\n    \"\"\"\n    Given a specific inference row id, this endpoint produces an explainability report for this inference.\n    The XAI pipeline requires a set of dataset rows as a training set, a model and the inference row.\n    If one of those three is not found in the database, a 404 error is returned.\n    \"\"\"\n\n    inference_row = crud.inference_rows.get(db=db, _id=inference_row_id)\n    if not inference_row:\n        return errors.not_found(f\"Inference row with id {inference_row_id} not found\")\n\n    inference_row_df = pd.DataFrame([inference_row.processed])\n    inference_row_series = inference_row_df.drop(columns=[\"target\"]).iloc[0]\n\n    model = crud.models.get(db=db, _id=inference_row.model_id)\n    if not model:\n        return errors.not_found(f\"Model with id {inference_row.model_id} not found\")\n\n    dataset_rows = crud.dataset_rows.get_dataset_rows_by_model(db=db, model_id=model.id)\n    if not dataset_rows:\n        return errors.not_found(\n            f\"Dataset rows for model with id {inference_row.model_id} not found\"\n        )\n\n    dataset_rows_processed = [x.processed for x in dataset_rows]\n\n    xai_report = create_xai_pipeline_per_inference_row(\n        training_set=pd.DataFrame(dataset_rows_processed),\n        target=model.target_column,\n        inference_row=inference_row_series,\n        type_of_task=model.type,\n        model_id=model.id,\n    )\n\n    return xai_report\n"}
{"type": "source_file", "path": "whitebox/analytics/drift/pipelines.py", "content": "import pandas as pd\nimport json\nfrom evidently.report import Report\nfrom evidently.metric_preset import DataDriftPreset, TargetDriftPreset\nfrom whitebox.schemas.driftingMetric import DataDriftTable, ConceptDriftTable\n\n\ndef run_data_drift_pipeline(\n    reference_dataset: pd.DataFrame, current_dataset: pd.DataFrame\n) -> DataDriftTable:\n    \"\"\"\n    Two datasets are needed\n    The reference dataset serves as a benchmark.\n    An analysis happens comparing the current production data to the reference data.\n\n    The dataset should include the needed features to evaluate for drift.\n    The schema of both datasets should be identical.\n    - In the case of pandas DataFrame, all column names should be string\n    - All feature columns analyzed for drift should have the numerical type (np.number)\n    - Categorical data can be encoded as numerical labels and specified in the column_mapping.\n    - DateTime column is the only exception. If available, it can be used as the x-axis in the plots.\n\n    Potentially, any two datasets can be used for comparison. Only the reference dataset\n    will be used as a basis for comparison.\n\n    How it works\n\n    To estimate the data drift Evidently compares the distributions of each feature in the two datasets.\n    Evidently applies statistical tests to detect if the distribution has changed significantly. There is a default\n    logic to choosing the appropriate statistical test based on:\n    - feature type: categorical or numerical\n    - the number of observations in the reference dataset\n    - the number of unique values in the feature (n_unique)\n\n    For small data with <= 1000 observations in the reference dataset:\n    - For numerical features (n_unique > 5): two-sample Kolmogorov-Smirnov test.\n    - For categorical features or numerical features with n_unique <= 5: chi-squared test.\n    - For binary categorical features (n_unique <= 2), we use the proportion difference test for independent samples based on Z-score.\n\n    All tests use a 0.95 confidence level by default.\n\n    For larger data with > 1000 observations in the reference dataset:\n    - For numerical features (n_unique > 5): Wasserstein Distance.\n    - For categorical features or numerical with n_unique <= 5): JensenShannon divergence.\n\n    All tests use a threshold = 0.1 by default.\n\n    \"\"\"\n    drift_report = Report(metrics=[DataDriftPreset()])\n    drift_report.run(reference_data=reference_dataset, current_data=current_dataset)\n\n    initial_report = drift_report.json()\n    initial_report = json.loads(initial_report)\n\n    data_drift_report = {}\n    data_drift_report[\"drift_summary\"] = initial_report[\"metrics\"][1][\"result\"]\n\n    return DataDriftTable(**data_drift_report[\"drift_summary\"])\n\n\ndef run_concept_drift_pipeline(\n    reference_dataset: pd.DataFrame, current_dataset: pd.DataFrame, target_feature: str\n) -> ConceptDriftTable:\n    \"\"\"\n    To estimate the categorical target drift, we compare the distribution of the target in the two datasets.\n    This solution works for both binary and multi-class classification.\n    As this function works with keywords we have to explicitly define the target column. At the end of the function\n    we return the initial name of the feature.\n\n    There is a default logic to choosing the appropriate statistical test, based on:\n    - the number of observations in the reference dataset\n    - the number of unique values in the target (n_unique)\n\n    For small data with <= 1000 observations in the reference dataset:\n    - For categorical target with n_unique > 2: chi-squared test.\n    - For binary categorical target (n_unique <= 2), we use the proportion difference test for independent samples based on Z-score.\n\n    All tests use a 0.95 confidence level by default.\n\n    For larger data with > 1000 observations in the reference dataset we use JensenShannon divergence with a threshold = 0.1.\n\n    \"\"\"\n    reference_dataset.rename(columns={target_feature: \"target\"}, inplace=True)\n    current_dataset.rename(columns={target_feature: \"target\"}, inplace=True)\n    drift_report = Report(metrics=[TargetDriftPreset()])\n    drift_report.run(reference_data=reference_dataset, current_data=current_dataset)\n    initial_report = drift_report.json()\n    initial_report = json.loads(initial_report)\n    concept_drift_report = {}\n    concept_drift_report[\"concept_drift_summary\"] = initial_report[\"metrics\"][0][\n        \"result\"\n    ]\n    concept_drift_report[\"column_correlation\"] = initial_report[\"metrics\"][1][\"result\"]\n\n    return ConceptDriftTable(\n        concept_drift_summary=concept_drift_report[\"concept_drift_summary\"],\n        column_correlation=concept_drift_report[\"column_correlation\"],\n    )\n"}
{"type": "source_file", "path": "scripts/decrypt_api_key.py", "content": "import os\nimport sys\n\nsys.path.append(os.getcwd())\n\nfrom whitebox.utils.passwords import decrypt_api_key\nfrom whitebox.core.settings import get_settings\n\nvalue = sys.argv[1]\nsettings = get_settings()\n\nif __name__ == \"__main__\":\n    api_key = decrypt_api_key(value, settings.SECRET_KEY.encode())\n    print(api_key)\n"}
{"type": "source_file", "path": "whitebox/analytics/models/pipelines.py", "content": "import pandas as pd\nimport os\nfrom typing import Dict\nimport numpy as np\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, precision_score\nfrom sklearn import metrics\nimport joblib\nfrom whitebox.core.settings import get_settings\n\nsettings = get_settings()\n\nmodel_base_path = settings.MODEL_PATH\n\n\ndef create_binary_classification_training_model_pipeline(\n    training_dataset: pd.DataFrame, target: str, model_id: str\n) -> Dict[str, float]:\n    model_path = f\"{model_base_path}/{model_id}\"\n\n    # Create directory if it doesn't exist\n    os.makedirs(model_path, exist_ok=True)\n\n    \"\"\"\n    We first define what will be training set and the targeted column for our prediction\n\n    \"\"\"\n    Y = training_dataset[target]\n    X = training_dataset.drop(columns=[target])\n    \"\"\"\n    We split to test and training set by using a random_state of 0 in order our code to be \n    reproducible.\n    WARNING: We assume that the given dataset is preprocessed. That means than no preprocessing will be performed \n    by us. We have to revisit this step in the near future.\n    \n    \"\"\"\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, Y, test_size=0.3, random_state=0\n    )\n    \"\"\"\n    We use the default set of parameters which produce good results with our baseline dataset.\n    WARNING: In the near future we have to grid-search for the optimal parameters for training datasets\n    \n    The train of our model took locally less than 1 seconds.\n    Also we temp save the model in a pkl format.\n    WARNING: We have to revisit this step for optimise the resources cost.\n    \n    \"\"\"\n    clf = LGBMClassifier()\n    clf.fit(X_train, y_train)\n    joblib.dump(clf, f\"{model_path}/lgb_binary.pkl\")\n\n    \"\"\"\n    We make some predictions in the X_test and we find the class \n    there by rounding the output. After that we calculate the roc auc curve\n    score.\n    \n    \"\"\"\n\n    y_pred_1 = clf.predict(X_test)\n    y_pred_1 = y_pred_1.round(0)\n    y_pred_1 = y_pred_1.astype(int)\n\n    roc_score = roc_auc_score(y_test, y_pred_1)\n    binary_evaluation_report = {}\n    binary_evaluation_report[\"roc_auc_score\"] = roc_score\n\n    return clf, binary_evaluation_report\n\n\ndef create_multiclass_classification_training_model_pipeline(\n    training_dataset: pd.DataFrame, target: str, model_id: str\n) -> Dict[str, float]:\n    model_path = f\"{model_base_path}/{model_id}\"\n\n    # Create directory if it doesn't exist\n    os.makedirs(model_path, exist_ok=True)\n\n    \"\"\"\n    We first define what will be training set and the targeted column for our prediction\n\n    \"\"\"\n    Y = training_dataset[target]\n    X = training_dataset.drop(columns=[target])\n    \"\"\"\n    We split to test and training set by using a random_state of 0 in order our code to be \n    reproducible. \n    We load the dataset to lightgbm library.\n    WARNING: We assume that the given dataset is preprocessed. That means than no preprocessing will be performed \n    by us. We have to revisit this step in the near future.\n    \n    \"\"\"\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, Y, test_size=0.3, random_state=0\n    )\n    d_train = lgb.Dataset(X_train, label=y_train)\n    \"\"\"\n    We use a set of parameters which produce good results with our baseline dataset.\n    WARNING: In the near future we have to grid-search for the optimal parameters for training datasets\n    \n    \"\"\"\n\n    params = {}\n    params[\"verbose\"] = -1  # Remove logs\n    params[\"learning_rate\"] = 0.03\n    params[\"boosting_type\"] = \"gbdt\"  # GradientBoostingDecisionTree\n    params[\"objective\"] = \"multiclass\"  # Multi-class target feature\n    params[\"metric\"] = \"multi_logloss\"  # metric for multi-class\n    params[\"max_depth\"] = 10\n    params[\n        \"num_class\"\n    ] = 3  # no.of unique values in the target class not inclusive of the end value\n    \"\"\"\n    We train our model in 100 epochs - locally this took less than 2 seconds.\n    Also we temp save the model in a pkl format.\n    WARNING: We have to revisit this step for optimise the resources cost.\n    \n    \"\"\"\n    clf = lgb.train(params, d_train, 100)  # training the model on 100 epocs\n    joblib.dump(clf, f\"{model_path}/lgb_multi.pkl\")\n\n    \"\"\"\n    We make some predictions in the X_test and we find the class with the higher \n    probability there. After that we calculate the precision_score\n    \n    \"\"\"\n\n    y_pred_1 = clf.predict(X_test)\n    y_pred_1 = [np.argmax(line) for line in y_pred_1]\n    prec_score = precision_score(y_test, y_pred_1, average=None).mean()\n\n    multi_evaluation_report = {}\n    multi_evaluation_report[\"precision\"] = prec_score\n\n    return clf, multi_evaluation_report\n\n\ndef create_regression_training_model_pipeline(\n    training_dataset: pd.DataFrame, target: str, model_id: str\n) -> Dict[str, float]:\n    model_path = f\"{model_base_path}/{model_id}\"\n\n    # Create directory if it doesn't exist\n    os.makedirs(model_path, exist_ok=True)\n\n    \"\"\"\n    We first define what will be training set and the targeted column for our prediction\n\n    \"\"\"\n    Y = training_dataset[target]\n    X = training_dataset.drop(columns=[target])\n    \"\"\"\n    We split to test and training set by using a random_state of 0 in order our code to be \n    reproducible.\n    WARNING: We assume that the given dataset is preprocessed. That means than no preprocessing will be performed \n    by us. We have to revisit this step in the near future.\n    \n    \"\"\"\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, Y, test_size=0.3, random_state=0\n    )\n    \"\"\"\n    We use the default set of parameters which produce good results with our baseline dataset.\n    WARNING: In the near future we have to grid-search for the optimal parameters for training datasets\n    \n    The train of our model took locally less than 1 seconds.\n    Also we temp save the model in a pkl format.\n    WARNING: We have to revisit this step for optimise the resources cost.\n    \n    \"\"\"\n\n    reg = lgb.LGBMRegressor()\n    reg.fit(X_train, y_train)\n    joblib.dump(reg, f\"{model_path}/lgb_reg.pkl\")\n\n    \"\"\"\n    We make some predictions in the X_test and we find the class \n    there by rounding the output. After that we calculate the roc auc curve\n    score.\n    \n    \"\"\"\n\n    y_predicted = reg.predict(X_test)\n    r2_score = round(metrics.r2_score(y_test, y_predicted), 4)\n\n    reg_evaluation_report = {}\n    reg_evaluation_report[\"r2_score\"] = r2_score\n\n    return reg, reg_evaluation_report\n"}
{"type": "source_file", "path": "whitebox/api/v1/__init__.py", "content": "from fastapi import APIRouter\nfrom .health import health_router\n\nfrom .models import models_router\nfrom .dataset_rows import dataset_rows_router\nfrom .inference_rows import inference_rows_router\nfrom .performance_metrics import performance_metrics_router\nfrom .drifting_metrics import drifting_metrics_router\nfrom .model_integrity_metrics import model_integrity_metrics_router\nfrom .model_monitors import model_monitors_router\nfrom .alerts import alerts_router\nfrom .cron_tasks import cron_tasks_router\n\n\nv1_router = APIRouter()\nv1 = \"/v1\"\n\nv1_router.include_router(health_router, prefix=v1)\nv1_router.include_router(models_router, prefix=v1)\nv1_router.include_router(dataset_rows_router, prefix=v1)\nv1_router.include_router(inference_rows_router, prefix=v1)\nv1_router.include_router(performance_metrics_router, prefix=v1)\nv1_router.include_router(drifting_metrics_router, prefix=v1)\nv1_router.include_router(model_integrity_metrics_router, prefix=v1)\nv1_router.include_router(model_monitors_router, prefix=v1)\nv1_router.include_router(alerts_router, prefix=v1)\nv1_router.include_router(cron_tasks_router, prefix=v1)\n"}
{"type": "source_file", "path": "whitebox/api/v1/health.py", "content": "from whitebox.schemas.utils import HealthCheck\nfrom fastapi import APIRouter, status\n\nhealth_router = APIRouter()\n\n\n@health_router.get(\n    \"/health\",\n    tags=[\"Health\"],\n    response_model=HealthCheck,\n    summary=\"Health check the service\",\n    status_code=status.HTTP_200_OK,\n    response_description=\"Status of the service\",\n)\ndef health_check():\n    \"\"\"Responds with the status of the service.\"\"\"\n    return HealthCheck(status=\"OK\")\n"}
{"type": "source_file", "path": "whitebox/analytics/__init__.py", "content": ""}
{"type": "source_file", "path": "whitebox/api/v1/model_integrity_metrics.py", "content": "from typing import List\nfrom fastapi import APIRouter, Depends, status\nfrom whitebox import crud\nfrom sqlalchemy.orm import Session\nfrom whitebox.core.db import get_db\nfrom whitebox.middleware.auth import authenticate_user\nfrom whitebox.schemas.modelIntegrityMetric import ModelIntegrityMetric\nfrom whitebox.schemas.user import User\nfrom whitebox.utils.errors import add_error_responses, errors\n\n\nmodel_integrity_metrics_router = APIRouter()\n\n\n@model_integrity_metrics_router.get(\n    \"/model-integrity-metrics\",\n    tags=[\"Model Integrity Metrics\"],\n    response_model=List[ModelIntegrityMetric],\n    summary=\"Get all model's model integrity metrics\",\n    status_code=status.HTTP_200_OK,\n    responses=add_error_responses([401, 404]),\n)\nasync def get_all_models_model_integrity_metrics(\n    model_id: str,\n    db: Session = Depends(get_db),\n    authenticated_user: User = Depends(authenticate_user),\n):\n    \"\"\"Fetches the model integrity metrics of a specific model. A model id is required.\"\"\"\n\n    model = crud.models.get(db, model_id)\n    if model:\n        return crud.model_integrity_metrics.get_model_integrity_metrics_by_model(\n            db=db, model_id=model_id\n        )\n    else:\n        return errors.not_found(\"Model not found\")\n"}
{"type": "source_file", "path": "whitebox/api/__init__.py", "content": ""}
{"type": "source_file", "path": "whitebox/api/v1/docs.py", "content": "from whitebox.schemas.utils import ErrorResponse\n\n\ntags_metadata = [\n    {\n        \"name\": \"Health\",\n        \"description\": \"Health endpoints are used for checking the status of the service\",\n    },\n    {\n        \"name\": \"Models\",\n        \"description\": \"This set of endpoints handles the models that a user creates.\",\n    },\n    {\n        \"name\": \"Dataset Rows\",\n        \"description\": \"This set of endpoints handles the dataset rows.\",\n    },\n    {\n        \"name\": \"Inference Rows\",\n        \"description\": \"This set of endpoints handles a model's inference rows.\",\n    },\n    {\n        \"name\": \"Performance Metrics\",\n        \"description\": \"This set of endpoints handles a model's performance metrics.\",\n    },\n    {\n        \"name\": \"Drifting Metrics\",\n        \"description\": \"This set of endpoints handles a model's drifting metrics.\",\n    },\n    {\n        \"name\": \"Model Integrity Metrics\",\n        \"description\": \"This set of endpoints handles a model's integrity metrics.\",\n    },\n    {\n        \"name\": \"Model Monitors\",\n        \"description\": \"This set of endpoints handles a model's model monitors.\",\n    },\n    {\n        \"name\": \"Alerts\",\n        \"description\": \"This set of endpoints handles a model's alerts.\",\n    },\n    {\n        \"name\": \"Cron Tasks\",\n        \"description\": \"This is a helper endpoint to trigger cron tasks for tests.\",\n    },\n]\n\n\nbad_request: ErrorResponse = {\n    \"title\": \"BadRequest\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"error\": {\"title\": \"Error Message\", \"type\": \"string\"},\n        \"status_code\": {\"title\": \"Status code\", \"type\": \"integer\"},\n    },\n}\n\nvalidation_error: ErrorResponse = {\n    \"title\": \"HTTPValidationError\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"error\": {\"title\": \"Error Message\", \"type\": \"string\"},\n        \"status_code\": {\"title\": \"Status code\", \"type\": \"integer\"},\n    },\n}\n\nauthorization_error: ErrorResponse = {\n    \"title\": \"AuthorizationError\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"error\": {\"title\": \"Error Message\", \"type\": \"string\"},\n        \"status_code\": {\"title\": \"Status code\", \"type\": \"integer\"},\n    },\n}\n\nnot_found_error: ErrorResponse = {\n    \"title\": \"NotFoundError\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"error\": {\"title\": \"Error Message\", \"type\": \"string\"},\n        \"status_code\": {\"title\": \"Status code\", \"type\": \"integer\"},\n    },\n}\n\nconflict_error: ErrorResponse = {\n    \"title\": \"ConflictError\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"error\": {\"title\": \"Error Message\", \"type\": \"string\"},\n        \"status_code\": {\"title\": \"Status code\", \"type\": \"integer\"},\n    },\n}\n\nconflict_error: ErrorResponse = {\n    \"title\": \"ConflictError\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"error\": {\"title\": \"Error Message\", \"type\": \"string\"},\n        \"status_code\": {\"title\": \"Status code\", \"type\": \"integer\"},\n    },\n}\n\ncontent_gone: ErrorResponse = {\n    \"title\": \"ContentGone\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"error\": {\"title\": \"Error Message\", \"type\": \"string\"},\n        \"status_code\": {\"title\": \"Status code\", \"type\": \"integer\"},\n    },\n}\n"}
{"type": "source_file", "path": "whitebox/api/v1/model_monitors.py", "content": "from typing import List, Union\nfrom whitebox.middleware.auth import authenticate_user\nfrom whitebox.schemas.modelMonitor import (\n    ModelMonitor,\n    ModelMonitorCreateDto,\n    ModelMonitorUpdateDto,\n    MonitorMetrics,\n)\nfrom fastapi import APIRouter, Depends, status\nfrom whitebox import crud\nfrom sqlalchemy.orm import Session\nfrom whitebox.core.db import get_db\nfrom whitebox.schemas.user import User\nfrom whitebox.schemas.utils import StatusCode\nfrom whitebox.utils.errors import add_error_responses, errors\n\n\nmodel_monitors_router = APIRouter()\n\n\n@model_monitors_router.post(\n    \"/model-monitors\",\n    tags=[\"Model Monitors\"],\n    response_model=ModelMonitor,\n    summary=\"Create a model monitor\",\n    status_code=status.HTTP_201_CREATED,\n    responses=add_error_responses([400, 401, 409]),\n)\nasync def create_model_monitor(\n    body: ModelMonitorCreateDto,\n    db: Session = Depends(get_db),\n    authenticated_user: User = Depends(authenticate_user),\n) -> ModelMonitor:\n    \"\"\"Inserts a model monitor into the database.\"\"\"\n\n    model = crud.models.get(db, body.model_id)\n    if not model:\n        return errors.not_found(\"Model not found!\")\n\n    if body.metric in [MonitorMetrics.concept_drift, MonitorMetrics.data_drift]:\n        if body.metric == MonitorMetrics.concept_drift:\n            body.feature = model.target_column\n        else:\n            if not body.feature:\n                return errors.bad_request(f\"Please set a feature for the monitor!\")\n            # TODO This should get the feature columns from model.features when this field is\n            # automatically updated from the training dataset.\n            dataset_row = crud.dataset_rows.get_first_by_filter(db, model_id=model.id)\n            if not dataset_row:\n                return errors.not_found(\n                    f\"No training dataset found for model: {model.id}!\\\n                    Insert the taining dataset and then create a monitor!\"\n                )\n            features = dataset_row.processed\n            if body.feature not in features:\n                return errors.bad_request(\n                    f\"Monitored featured must be in the dataset's features!\"\n                )\n            if body.feature == model.target_column:\n                return errors.bad_request(\n                    f\"Monitored featured cannot be the target column in data drift!\"\n                )\n        body.lower_threshold = None\n    else:\n        if body.lower_threshold is None:\n            return errors.bad_request(f\"Please set a lower threshold for the monitor!\")\n        body.feature = None\n\n    new_model_monitor = crud.model_monitors.create(db=db, obj_in=body)\n    return new_model_monitor\n\n\n@model_monitors_router.get(\n    \"/model-monitors\",\n    tags=[\"Model Monitors\"],\n    response_model=List[ModelMonitor],\n    summary=\"Get all model's model monitors\",\n    status_code=status.HTTP_200_OK,\n    responses=add_error_responses([401, 404]),\n)\nasync def get_all_models_model_monitors(\n    model_id: Union[str, None] = None,\n    db: Session = Depends(get_db),\n    authenticated_user: User = Depends(authenticate_user),\n):\n    \"\"\"\n    Fetches model monitors from the databse.\n    \\n If a model id is provided, only the monitors for the specific model will be fetched.\n    \\n If a model id is not provided then all monitors from the database will be fetched.\n    \"\"\"\n\n    if model_id:\n        model = crud.models.get(db, model_id)\n        if model:\n            return crud.model_monitors.get_model_monitors_by_model(\n                db=db, model_id=model_id\n            )\n        else:\n            return errors.not_found(\"Model not found\")\n    else:\n        return crud.model_monitors.get_all(db=db)\n\n\n@model_monitors_router.put(\n    \"/model-monitors/{model_monitor_id}\",\n    tags=[\"Model Monitors\"],\n    response_model=ModelMonitor,\n    summary=\"Update model monitor\",\n    status_code=status.HTTP_200_OK,\n    responses=add_error_responses([400, 401, 404]),\n)\nasync def update_model_monitor(\n    model_monitor_id: str,\n    body: ModelMonitorUpdateDto,\n    db: Session = Depends(get_db),\n    authenticated_user: User = Depends(authenticate_user),\n) -> ModelMonitor:\n    \"\"\"Updates record of the model monitor with the specified id.\"\"\"\n\n    # Remove all unset properties (with None values) from the update object\n    filtered_body = {k: v for k, v in dict(body).items() if v is not None}\n\n    model_monitor = crud.model_monitors.get(db=db, _id=model_monitor_id)\n\n    if not model_monitor:\n        return errors.not_found(\"Model monitor not found!\")\n\n    if model_monitor.metric in [\n        MonitorMetrics.concept_drift,\n        MonitorMetrics.data_drift,\n    ]:\n        filtered_body[\"lower_threshold\"] = None\n\n    return crud.model_monitors.update(db=db, db_obj=model_monitor, obj_in=filtered_body)\n\n\n@model_monitors_router.delete(\n    \"/model-monitors/{model_monitor_id}\",\n    tags=[\"Model Monitors\"],\n    response_model=StatusCode,\n    summary=\"Delete model monitor\",\n    status_code=status.HTTP_200_OK,\n    responses=add_error_responses([401, 404]),\n)\nasync def delete_model_monitor(\n    model_monitor_id: str,\n    db: Session = Depends(get_db),\n    authenticated_user: User = Depends(authenticate_user),\n) -> StatusCode:\n    \"\"\"Deletes the model monitor with the specified id from the database.\"\"\"\n\n    model_monitor = crud.model_monitors.get(db=db, _id=model_monitor_id)\n    if not model_monitor:\n        return errors.not_found(\"Model monitor not found\")\n\n    crud.model_monitors.remove(db=db, _id=model_monitor_id)\n    return {\"status_code\": status.HTTP_200_OK}\n"}
{"type": "source_file", "path": "whitebox/analytics/xai_models/pipelines.py", "content": "import pandas as pd\nfrom typing import Dict\nimport joblib\nimport lime\nimport lime.lime_tabular\nfrom whitebox.analytics.models.pipelines import *\nfrom whitebox.core.settings import get_settings\nfrom whitebox.schemas.model import ModelType\n\n\nsettings = get_settings()\n\n\ndef create_xai_pipeline_per_inference_row(\n    training_set: pd.DataFrame,\n    target: str,\n    inference_row: pd.Series,\n    type_of_task: str,\n    model_id: str,\n) -> Dict[str, float]:\n    model_base_path = settings.MODEL_PATH\n    model_path = f\"{model_base_path}/{model_id}\"\n\n    xai_dataset = training_set.drop(columns=[target])\n    explainability_report = {}\n\n    # Make a mapping dict which will be used later to map the explainer index\n    # with the features names\n\n    mapping_dict = {}\n    for feature in range(0, len(xai_dataset.columns.tolist())):\n        mapping_dict[feature] = xai_dataset.columns.tolist()[feature]\n\n    # Expainability for both classifications tasks and regression\n    # We have again to revisit here in the future as in case we upload the model\n    # from the file system we don't care if it is binary or multiclass\n\n    if type_of_task == ModelType.multi_class:\n        # Giving the option of retrieving the local model\n\n        model = joblib.load(f\"{model_path}/lgb_multi.pkl\")\n        explainer = lime.lime_tabular.LimeTabularExplainer(\n            xai_dataset.values,\n            feature_names=xai_dataset.columns.values.tolist(),\n            mode=\"classification\",\n            random_state=1,\n        )\n\n        exp = explainer.explain_instance(inference_row, model.predict)\n        med_report = exp.as_map()\n        temp_dict = dict(list(med_report.values())[0])\n        explainability_report = {\n            mapping_dict[name]: val for name, val in temp_dict.items()\n        }\n\n    elif type_of_task == ModelType.binary:\n        # Giving the option of retrieving the local model\n\n        model = joblib.load(f\"{model_path}/lgb_binary.pkl\")\n        explainer = lime.lime_tabular.LimeTabularExplainer(\n            xai_dataset.values,\n            feature_names=xai_dataset.columns.values.tolist(),\n            mode=\"classification\",\n            random_state=1,\n        )\n\n        exp = explainer.explain_instance(inference_row, model.predict_proba)\n        med_report = exp.as_map()\n        temp_dict = dict(list(med_report.values())[0])\n        explainability_report = {\n            mapping_dict[name]: val for name, val in temp_dict.items()\n        }\n\n    elif type_of_task == ModelType.regression:\n        # Giving the option of retrieving the local model\n\n        model = joblib.load(f\"{model_path}/lgb_reg.pkl\")\n        explainer = lime.lime_tabular.LimeTabularExplainer(\n            xai_dataset.values,\n            feature_names=xai_dataset.columns.values.tolist(),\n            mode=\"regression\",\n            random_state=1,\n        )\n\n        exp = explainer.explain_instance(inference_row, model.predict)\n        med_report = exp.as_map()\n        temp_dict = dict(list(med_report.values())[0])\n        explainability_report = {\n            mapping_dict[name]: val for name, val in temp_dict.items()\n        }\n\n    return explainability_report\n"}
{"type": "source_file", "path": "whitebox/__init__.py", "content": "from whitebox.sdk import *\n"}
{"type": "source_file", "path": "whitebox/api/v1/drifting_metrics.py", "content": "from typing import List\nfrom fastapi import APIRouter, Depends, status\nfrom whitebox import crud\nfrom sqlalchemy.orm import Session\nfrom whitebox.core.db import get_db\nfrom whitebox.middleware.auth import authenticate_user\nfrom whitebox.schemas.driftingMetric import DriftingMetricBase\nfrom whitebox.schemas.user import User\nfrom whitebox.utils.errors import add_error_responses, errors\n\n\ndrifting_metrics_router = APIRouter()\n\n\n@drifting_metrics_router.get(\n    \"/drifting-metrics\",\n    tags=[\"Drifting Metrics\"],\n    response_model=List[DriftingMetricBase],\n    summary=\"Get all model's drifting metrics\",\n    status_code=status.HTTP_200_OK,\n    responses=add_error_responses([401, 404]),\n)\nasync def get_all_models_drifting_metrics(\n    model_id: str,\n    db: Session = Depends(get_db),\n    authenticated_user: User = Depends(authenticate_user),\n):\n    \"\"\"Fetches the drifting metrics of a specific model. A model id is required.\"\"\"\n\n    model = crud.models.get(db, model_id)\n    if model:\n        return crud.drifting_metrics.get_drifting_metrics_by_model(\n            db=db, model_id=model_id\n        )\n    else:\n        return errors.not_found(\"Model not found\")\n"}
{"type": "source_file", "path": "whitebox/analytics/metrics/pipelines.py", "content": "import pandas as pd\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom whitebox.analytics.metrics.functions import *\nfrom typing import List\nfrom whitebox.schemas.performanceMetric import (\n    BinaryClassificationMetricsPipelineResult,\n    MultiClassificationMetricsPipelineResult,\n    RegressionMetricsPipelineResult,\n)\nfrom whitebox.schemas.modelIntegrityMetric import FeatureMetrics\n\n\ndef create_feature_metrics_pipeline(\n    dataset: pd.DataFrame,\n) -> FeatureMetrics:\n    \"\"\"\n    Feature metrics basic calculation\n\n    Calculates the basic metrics of a given dataset\n\n    Parameters\n    ----------\n    dataset : pd.DataFrame\n        Given dataset for the calculation of metrics\n\n    Returns\n    -------\n    feature_metrics : Dict\n\n        The returned metrics are:\n            missing_count,\n            non_missing_count,\n            mean,\n            minimum,\n            maximum,\n            sum,\n            standard_deviation,\n            variance\n\n    \"\"\"\n    missing_count = dataset.isna().sum().to_dict()\n    non_missing_count = dataset.notna().sum().to_dict()\n    mean = dataset.mean(numeric_only=True).to_dict()\n    minimum = dataset.min(numeric_only=True).to_dict()\n    maximum = dataset.max(numeric_only=True).to_dict()\n    sum = dataset.sum(numeric_only=True).to_dict()\n    standard_deviation = dataset.std(numeric_only=True).to_dict()\n    variance = dataset.var(numeric_only=True).to_dict()\n\n    return FeatureMetrics(\n        **format_feature_metrics(\n            missing_count,\n            non_missing_count,\n            mean,\n            minimum,\n            maximum,\n            sum,\n            standard_deviation,\n            variance,\n        )\n    )\n\n\ndef create_binary_classification_evaluation_metrics_pipeline(\n    test_set: pd.Series, prediction_set: pd.Series, labels: List[int]\n) -> BinaryClassificationMetricsPipelineResult:\n    \"\"\"\n    Binary classification evaluation metrics\n\n    Calculates the evaluation metrics for binary classification\n    given two datasets\n\n    Parameters\n    ----------\n    test_set : pd.Series\n        Given ground truth dataset\n\n    prediction_set : pd.Series\n        Given predictions dataset\n\n    Returns\n    -------\n    evaluation_metrics : Dict\n\n        The returned metrics are:\n            accuracy,\n            precision,\n            recall,\n            f1,\n            tn,\n            fp,\n            fn,\n            tp\n\n    \"\"\"\n\n    accuracy = metrics.accuracy_score(test_set, prediction_set)\n    precision = metrics.precision_score(test_set, prediction_set)\n    recall = metrics.recall_score(test_set, prediction_set)\n    f1 = metrics.f1_score(test_set, prediction_set)\n    tn, fp, fn, tp = confusion_matrix(test_set, prediction_set, labels=labels).ravel()\n\n    return BinaryClassificationMetricsPipelineResult(\n        **format_evaluation_metrics_binary(\n            accuracy, precision, recall, f1, tn, fp, fn, tp\n        )\n    )\n\n\ndef create_multiple_classification_evaluation_metrics_pipeline(\n    test_set: pd.Series, prediction_set: pd.Series, labels: List[int]\n) -> MultiClassificationMetricsPipelineResult:\n    \"\"\"\n    Multiclass classification evaluation metrics\n\n    Calculates the evaluation metrics for multiclass classification\n    given two datasets\n\n    Parameters\n    ----------\n    test_set : pd.Series\n        Given ground truth dataset\n\n    prediction_set : pd.Series\n        Given predictions dataset\n\n    Returns\n    -------\n    evaluation_metrics : Dict\n\n        The returned metrics are:\n\n            accuracy,\n\n            precision_statistics\n                micro_precision,\n                macro_precision,\n                weighted_precision,\n\n            recall\n                micro_recall,\n                macro_recall,\n                weighted_recall,\n\n            f1\n                micro_f1,\n                macro_f1,\n                weighted_f1\n\n            conf_matrix\n                tn,\n                fp,\n                fn,\n                tp\n    \"\"\"\n    accuracy = metrics.accuracy_score(test_set, prediction_set)\n    micro_precision = metrics.precision_score(test_set, prediction_set, average=\"micro\")\n    macro_precision = metrics.precision_score(test_set, prediction_set, average=\"macro\")\n    weighted_precision = metrics.precision_score(\n        test_set, prediction_set, average=\"weighted\"\n    )\n    precision_statistics = {\n        \"micro\": micro_precision,\n        \"macro\": macro_precision,\n        \"weighted\": weighted_precision,\n    }\n\n    micro_recall = metrics.recall_score(test_set, prediction_set, average=\"micro\")\n    macro_recall = metrics.recall_score(test_set, prediction_set, average=\"macro\")\n    weighted_recall = metrics.recall_score(test_set, prediction_set, average=\"weighted\")\n    recall_statistics = {\n        \"micro\": micro_recall,\n        \"macro\": macro_recall,\n        \"weighted\": weighted_recall,\n    }\n\n    micro_f1 = metrics.f1_score(test_set, prediction_set, average=\"micro\")\n    macro_f1 = metrics.f1_score(test_set, prediction_set, average=\"macro\")\n    weighted_f1 = metrics.f1_score(test_set, prediction_set, average=\"weighted\")\n    f1_statistics = {\"micro\": micro_f1, \"macro\": macro_f1, \"weighted\": weighted_f1}\n    conf_matrix = confusion_for_multiclass(test_set, prediction_set, labels)\n\n    return MultiClassificationMetricsPipelineResult(\n        **format_evaluation_metrics_multiple(\n            accuracy,\n            precision_statistics,\n            recall_statistics,\n            f1_statistics,\n            conf_matrix,\n        )\n    )\n\n\ndef create_regression_evaluation_metrics_pipeline(\n    test_set: pd.Series, prediction_set: pd.Series\n) -> RegressionMetricsPipelineResult:\n    \"\"\"\n    Regression evaluation metrics\n\n    Calculates the evaluation metrics for regression\n    given two datasets\n\n    Parameters\n    ----------\n    test_set : pd.Series\n        Given ground truth dataset\n\n    prediction_set : pd.Series\n        Given predictions dataset\n\n    Returns\n    -------\n    evaluation_metrics : Dict\n\n        The returned metrics are:\n            r square,\n            mean square error,\n            mean absolute error\n\n    \"\"\"\n\n    rsq = round(metrics.r2_score(test_set, prediction_set), 4)\n    mse = round(metrics.mean_squared_error(test_set, prediction_set), 4)\n    mae = round(metrics.mean_absolute_error(test_set, prediction_set), 4)\n\n    return RegressionMetricsPipelineResult(\n        **format_evaluation_metrics_regression(\n            r_square=rsq, mean_squared_error=mse, mean_absolute_error=mae\n        )\n    )\n"}
{"type": "source_file", "path": "whitebox/core/__init__.py", "content": ""}
{"type": "source_file", "path": "whitebox/api/v1/alerts.py", "content": "from typing import List, Union\nfrom fastapi import APIRouter, Depends, status\nfrom whitebox import crud\nfrom sqlalchemy.orm import Session\nfrom whitebox.core.db import get_db\nfrom whitebox.middleware.auth import authenticate_user\nfrom whitebox.schemas.alert import Alert\nfrom whitebox.schemas.user import User\nfrom whitebox.utils.errors import add_error_responses, errors\n\n\nalerts_router = APIRouter()\n\n\n@alerts_router.get(\n    \"/alerts\",\n    tags=[\"Alerts\"],\n    response_model=List[Alert],\n    summary=\"Get all model's alerts\",\n    status_code=status.HTTP_200_OK,\n    responses=add_error_responses([401, 404]),\n)\nasync def get_alerts(\n    model_id: Union[str, None] = None,\n    db: Session = Depends(get_db),\n    authenticated_user: User = Depends(authenticate_user),\n):\n    \"\"\"\n    Fetches alerts from the databse.\n    \\n If a model id is provided, only the alerts for the specific model will be fetched.\n    \\n If a model id is not provided then all alerts from the database will be fetched.\n    \"\"\"\n\n    if model_id:\n        model = crud.models.get(db, model_id)\n        if model:\n            return crud.alerts.get_model_alerts_by_model(db=db, model_id=model_id)\n        else:\n            return errors.not_found(\"Model not found\")\n    else:\n        return crud.alerts.get_all(db=db)\n"}
{"type": "source_file", "path": "whitebox/api/v1/dataset_rows.py", "content": "import pandas as pd\nfrom typing import List\nfrom whitebox.analytics.models.pipelines import (\n    create_binary_classification_training_model_pipeline,\n    create_multiclass_classification_training_model_pipeline,\n    create_regression_training_model_pipeline,\n)\nfrom whitebox.middleware.auth import authenticate_user\nfrom whitebox.schemas.datasetRow import DatasetRow, DatasetRowCreate\nfrom whitebox.schemas.model import ModelType\nfrom whitebox.schemas.user import User\nfrom fastapi import APIRouter, BackgroundTasks, Depends, status\nfrom fastapi.encoders import jsonable_encoder\nfrom whitebox import crud\nfrom sqlalchemy.orm import Session\nfrom whitebox.core.db import get_db\nfrom whitebox.utils.errors import add_error_responses, errors\n\n\ndataset_rows_router = APIRouter()\n\n\n@dataset_rows_router.post(\n    \"/dataset-rows\",\n    tags=[\"Dataset Rows\"],\n    response_model=List[DatasetRow],\n    summary=\"Create dataset rows\",\n    status_code=status.HTTP_201_CREATED,\n    responses=add_error_responses([400, 401, 404, 409]),\n)\nasync def create_dataset_rows(\n    body: List[DatasetRowCreate],\n    background_tasks: BackgroundTasks,\n    db: Session = Depends(get_db),\n    authenticated_user: User = Depends(authenticate_user),\n) -> DatasetRow:\n    \"\"\"\n    Inserts a set of dataset rows into the database.\n    \\nWhen the dataset rows are successfully saved, the pipeline for training the model is triggered.\n    \"\"\"\n\n    if len(body) <= 1:\n        return errors.bad_request(\"Training dataset should be longer that 1 row!\")\n\n    model = crud.models.get(db=db, _id=dict(body[0])[\"model_id\"])\n    if model:\n        for row in body:\n            if not model.target_column in row.processed:\n                return errors.bad_request(\n                    f'Column \"{model.target_column}\" was not found in some or any of the rows in provided training dataset. Please try again!'\n                )\n\n        predictions = list(set(vars(x)[\"processed\"][model.target_column] for x in body))\n        if len(predictions) <= 1:\n            return errors.bad_request(\n                f'Training dataset\\'s \"{model.target_column}\" columns must have at least 2 different values!'\n            )\n\n        new_dataset_rows = crud.dataset_rows.create_many(db=db, obj_list=body)\n        processed_dataset_rows = [\n            x[\"processed\"] for x in jsonable_encoder(new_dataset_rows)\n        ]\n        processed_dataset_rows_pd = pd.DataFrame(processed_dataset_rows)\n\n        if model.type == ModelType.binary:\n            background_tasks.add_task(\n                create_binary_classification_training_model_pipeline,\n                processed_dataset_rows_pd,\n                model.target_column,\n                model.id,\n            )\n        elif model.type == ModelType.multi_class:\n            background_tasks.add_task(\n                create_multiclass_classification_training_model_pipeline,\n                processed_dataset_rows_pd,\n                model.target_column,\n                model.id,\n            )\n        elif model.type == ModelType.regression:\n            background_tasks.add_task(\n                create_regression_training_model_pipeline,\n                processed_dataset_rows_pd,\n                model.target_column,\n                model.id,\n            )\n        return new_dataset_rows\n    else:\n        return errors.not_found(f\"Model with id: {dict(body[0])['model_id']} not found\")\n\n\n@dataset_rows_router.get(\n    \"/dataset-rows\",\n    tags=[\"Dataset Rows\"],\n    response_model=List[DatasetRow],\n    summary=\"Get all model's dataset rows\",\n    status_code=status.HTTP_200_OK,\n    responses=add_error_responses([401, 404]),\n)\nasync def get_all_dataset_rows(\n    model_id: str,\n    db: Session = Depends(get_db),\n    authenticated_user: User = Depends(authenticate_user),\n):\n    \"\"\"Fetches the dataset rows of a specific model. A model id is required.\"\"\"\n\n    model = crud.models.get(db, model_id)\n    if model:\n        return crud.dataset_rows.get_dataset_rows_by_model(db=db, model_id=model_id)\n    else:\n        return errors.not_found(\"Model not found\")\n"}
{"type": "source_file", "path": "whitebox/core/manager.py", "content": "import asyncio\nimport functools\nfrom collections import deque\n\nfrom crontab import CronTab\nfrom whitebox.utils.exceptions import (\n    TaskNotFoundException,\n    TaskAlreadyRunningException,\n    TaskNotRunningException,\n)\nfrom whitebox.utils.logger import cronLogger as logger\nfrom whitebox.schemas.task import (\n    TaskDefinition,\n    RunningTask,\n    TaskLog,\n    TaskInfo,\n    TaskStatus,\n    State,\n    TaskRealTimeInfo,\n    EventType,\n)\nfrom typing import Callable, Optional, Coroutine, List, Dict, Deque, Union\n\nfrom functools import lru_cache\nimport datetime\n\nimport pytz\n\n\ndef now():\n    return datetime.datetime.utcnow().replace(tzinfo=pytz.utc)\n\n\nclass Task_Manager:\n    def __init__(self):\n        self._definitions: Dict[str, TaskDefinition] = {}\n        self._real_time: Dict[str, TaskRealTimeInfo] = {}\n        self._running_tasks: Dict[str, RunningTask] = {}\n        self._log_queue: Deque[TaskLog] = deque()\n\n        self._is_running: bool = False\n        self._is_shutting_down: bool = False\n\n        self._cleanup_tasks: List[asyncio.Task] = []\n\n    def clear(self):\n        \"\"\"Resets the\"\"\"\n        if self._is_running or self._is_shutting_down:\n            raise Exception(\"Cannot clear before shutdown\")\n        self._definitions: Dict[str, TaskDefinition] = {}\n        self._real_time: Dict[str, TaskRealTimeInfo] = {}\n        self._running_tasks: Dict[str, RunningTask] = {}\n        self._log_queue: Deque[TaskLog] = deque()\n\n    def register(\n        self,\n        async_callable: Callable[[], Coroutine],\n        crontab: Union[str, None] = None,\n        name: Union[str, None] = None,\n    ):\n        name = name or async_callable.__name__\n        if name in self._definitions:\n            raise Exception(f\"Task <{name}> already exists.\")\n\n        definition = TaskDefinition(\n            name=name, async_callable=async_callable, crontab=crontab, enabled=True\n        )\n        self._definitions[name] = definition\n        self._real_time[name] = TaskRealTimeInfo(\n            name=name, status=\"registered\", next_run_ts=None\n        )\n\n        self._log_event(\"task_registered\", definition.name)\n\n    def _log_event(\n        self, event_type: EventType, task_name: str, error: Union[str, None] = None\n    ):\n        self._log_queue.append(\n            TaskLog(\n                event_type=event_type,\n                task_name=task_name,\n                crontab=self._definitions[task_name].crontab,\n                enabled=self._definitions[task_name].enabled,\n                error=error,\n            )\n        )\n\n    def _get_task_definition(self, name: str) -> TaskDefinition:\n        try:\n            return self._definitions[name]\n        except KeyError as e:\n            raise TaskNotFoundException from e\n\n    def _get_running_task(self, name: str) -> RunningTask:\n        try:\n            return self._running_tasks[name]\n        except KeyError as e:\n            raise TaskNotRunningException from e\n\n    def _is_task_running(self, name: str) -> bool:\n        return name in self._running_tasks\n\n    def get_task_info(self, name: str) -> TaskInfo:\n        definition = self._get_task_definition(name)\n        return TaskInfo(\n            name=definition.name,\n            enabled=definition.enabled,\n            crontab=definition.crontab,\n            started_at=self._get_task_started_at(name),\n            stopped_at=self._get_task_stopped_at(name),\n            next_run_in=self._get_task_next_run_in(name),\n            previous_status=self._get_previous_status(name) or \"registered\",\n            status=self._get_task_status(name),\n        )\n\n    # TODO: Implement functionality\n    def _get_task_started_at(self, name: str):\n        rt_info = self._real_time.get(name)\n        if not rt_info:\n            raise TaskNotFoundException(\"task not found\")\n        return rt_info.started_at\n\n    # TODO: Implement functionality\n    def _get_task_stopped_at(self, name: str):\n        rt_info = self._real_time.get(name)\n        if not rt_info:\n            raise TaskNotFoundException(\"task not found\")\n        return rt_info.stopped_at\n\n    def _get_task_status(self, name: str) -> TaskStatus:\n        rt_info = self._real_time.get(name)\n        if not rt_info:\n            raise TaskNotFoundException(\"task not found\")\n        return rt_info.status\n\n    def _get_previous_status(self, name: str) -> TaskStatus:\n        rt_info = self._real_time.get(name)\n        if not rt_info:\n            raise TaskNotFoundException(\"task not found\")\n        return rt_info.previous_status\n\n    def _get_task_next_run_in(self, name: str) -> Optional[int]:\n        definition = self._get_task_definition(name)\n        if definition.crontab is None:\n            return None\n        return CronTab(definition.crontab).next(default_utc=True)\n\n    def _create_running_task(self, definition: TaskDefinition) -> RunningTask:\n        running_task = RunningTask(\n            task_definition=definition,\n            asyncio_task=asyncio.get_event_loop().create_task(\n                definition.async_callable()\n            ),\n            since=now().timestamp(),\n        )\n        running_task.asyncio_task.add_done_callback(\n            functools.partial(self._on_task_done, definition.name)\n        )\n        return running_task\n\n    async def cancel_task(self, name: str):\n        if not self._is_task_running(name):\n            raise TaskNotRunningException(\"task not running\")\n        logger.info(f\"Cancelling {name}\")\n        self._get_task_definition(name)\n        running_task = self._get_running_task(name)\n        cancelled = running_task.asyncio_task.cancel()\n        return cancelled\n\n    def run_task(self, name: str) -> None:\n        if self._is_task_running(name):\n            raise TaskAlreadyRunningException(\"task already running\")\n\n        definition = self._get_task_definition(name)\n        running_task = self._create_running_task(definition)\n        self._running_tasks[definition.name] = running_task\n\n        self._real_time[name] = TaskRealTimeInfo(\n            name=name,\n            status=\"running\",\n            previous_status=self._get_previous_status(name),\n            started_at=now().timestamp(),\n            next_run_ts=now().timestamp() + (self._get_task_next_run_in(name) or 0),\n        )\n\n    # TODO: Implement disable task functionality\n    def disable_task(self, name: str):\n        return False\n\n    # TODO: Implement enable task functionality\n    def enable_task(self, name: str):\n        return False\n\n    def get_all_tasks_info(self) -> List[TaskInfo]:\n        return [self.get_task_info(name) for name in self._definitions.keys()]\n\n    def _on_task_done(self, task_name: str, task: asyncio.Task) -> None:\n        definition = self._get_task_definition(task_name)\n        del self._running_tasks[task_name]\n\n        try:\n            exception = task.exception()\n        except asyncio.CancelledError:\n            self._log_event(\"task_cancelled\", definition.name)\n\n            self._real_time[task_name] = TaskRealTimeInfo(\n                name=task_name,\n                status=\"pending\",\n                previous_status=\"cancelled\",\n                next_run_ts=None,\n                started_at=self._get_task_started_at(task_name),\n                stopped_at=now().timestamp(),\n            )\n            task = asyncio.get_event_loop().create_task(\n                self.on_task_cancelled(task_name)\n            )\n            self._cleanup_tasks.append(task)\n            return\n\n        if exception:\n            self._log_event(\"task_failed\", definition.name, error=str(exception))\n\n            self._real_time[task_name] = TaskRealTimeInfo(\n                name=task_name,\n                status=\"pending\",\n                previous_status=\"failed\",\n                next_run_ts=None,\n                started_at=self._get_task_started_at(task_name),\n                stopped_at=now().timestamp(),\n            )\n            task = asyncio.get_event_loop().create_task(\n                self.on_task_exception(task_name, exception)\n            )\n            self._cleanup_tasks.append(task)\n            return\n\n        self._log_event(\"task_finished\", definition.name)\n\n        self._real_time[task_name] = TaskRealTimeInfo(\n            name=task_name,\n            status=\"pending\",\n            previous_status=\"finished\",\n            started_at=self._get_task_started_at(task_name),\n            stopped_at=now().timestamp(),\n            next_run_ts=now().timestamp() + self._get_task_next_run_in(task_name)\n            if definition.crontab\n            else None,\n        )\n        task = asyncio.get_event_loop().create_task(self.on_task_finished(task_name))\n        self._cleanup_tasks.append(task)\n        return\n\n    async def _on_task_started(self, task_name: str):\n        definition = self._get_task_definition(task_name)\n\n        self._log_event(\"task_started\", definition.name)\n        await self.on_task_started(task_name)\n\n    async def run(self, state: Union[State, None] = None):\n        if self._is_running:\n            logger.warning(\"Ignoring current calling of run(). Already running.\")\n            return\n\n        self._is_running = True\n        await self.on_startup()\n\n        if state:\n            for task_info in state.tasks_info:\n                if task_info.name in self._definitions:\n                    self._definitions[task_info.name].crontab = task_info.crontab\n                    self._definitions[task_info.name].enabled = task_info.enabled\n\n                    self._real_time[task_info.name].status = task_info.status\n\n        await self._run_ad_infinitum()\n\n    async def _run_ad_infinitum(self):\n        while True and self._is_running:\n            for task_name, rt_info in self._real_time.items():\n                this_time_ts = now().timestamp()\n\n                if rt_info.status == \"registered\":\n                    delta: int = self._get_task_next_run_in(task_name) or 0\n\n                    self._real_time[task_name] = TaskRealTimeInfo(\n                        name=task_name,\n                        status=\"pending\",\n                        previous_status=\"registered\",\n                        next_run_ts=now().timestamp() + delta,\n                    )\n                elif (\n                    not self._is_shutting_down\n                    and rt_info.status in [\"pending\", \"finished\"]\n                    and rt_info.next_run_ts is not None\n                    and rt_info.next_run_ts <= this_time_ts\n                ):\n                    self.run_task(task_name)\n                elif rt_info.status == \"running\" and not self._is_task_running(\n                    task_name\n                ):\n                    self.run_task(task_name)\n                else:  # rt_info.status in [\"cancelled\", \"failed\"]:\n                    ...\n            await asyncio.sleep(1.5)\n\n    async def shutdown(self):\n        await asyncio.sleep(2)\n        logger.info(\"Shutting down...\")\n        logger.info(f\"Cancelling {len(self._running_tasks)} running tasks...\")\n        self._is_shutting_down = True\n\n        for running_task in self._running_tasks.values():\n            await self.cancel_task(running_task.task_definition.name)\n            logger.debug(f\"Cancelled task {running_task.task_definition.name}\")\n\n        await asyncio.gather(*self._cleanup_tasks)\n        logger.debug(\"Cleanup tasks finished.\")\n\n        await self.on_shutdown()\n\n        self._is_running = False\n        self._is_shutting_down = False\n\n    async def on_task_started(self, task_name: str):\n        ...\n\n    async def on_task_exception(self, task_name: str, exception: BaseException):\n        ...\n\n    async def on_task_cancelled(self, task_name: str):\n        logger.info(f\"Cancelled {task_name}\")\n\n    async def on_task_finished(self, task_name: str):\n        logger.info(f\"Finished {task_name}\")\n\n    async def on_startup(self):\n        ...\n\n    async def on_shutdown(self):\n        ...\n\n    def state(self) -> State:\n        state = State(created_at=now(), tasks_info=self.get_all_tasks_info())\n        return state\n\n\n@lru_cache()\ndef get_task_manager():\n    return Task_Manager()\n"}
{"type": "source_file", "path": "whitebox/api/v1/performance_metrics.py", "content": "from typing import List, Union\nfrom fastapi import APIRouter, Depends, status\nfrom whitebox import crud\nfrom sqlalchemy.orm import Session\nfrom whitebox.core.db import get_db\nfrom whitebox.middleware.auth import authenticate_user\nfrom whitebox.schemas.performanceMetric import (\n    BinaryClassificationMetrics,\n    MultiClassificationMetrics,\n    RegressionMetrics,\n)\nfrom whitebox.schemas.user import User\nfrom whitebox.schemas.model import ModelType\nfrom whitebox.utils.errors import add_error_responses, errors\n\n\nperformance_metrics_router = APIRouter()\n\n\n@performance_metrics_router.get(\n    \"/performance-metrics\",\n    tags=[\"Performance Metrics\"],\n    response_model=Union[\n        List[BinaryClassificationMetrics],\n        List[MultiClassificationMetrics],\n        List[RegressionMetrics],\n    ],\n    summary=\"Get all model's performance metrics\",\n    status_code=status.HTTP_200_OK,\n    responses=add_error_responses([401, 404]),\n)\nasync def get_all_models_performance_metrics(\n    model_id: str,\n    db: Session = Depends(get_db),\n    authenticated_user: User = Depends(authenticate_user),\n):\n    \"\"\"Fetches the performance metrics of a specific model. A model id is required.\"\"\"\n\n    model = crud.models.get(db, model_id)\n    if model:\n        if model.type == ModelType.binary:\n            return crud.binary_classification_metrics.get_performance_metrics_by_model(\n                db=db, model_id=model_id\n            )\n        elif model.type == ModelType.multi_class:\n            return crud.multi_classification_metrics.get_performance_metrics_by_model(\n                db=db, model_id=model_id\n            )\n        elif model.type == ModelType.regression:\n            return crud.regression_metrics.get_performance_metrics_by_model(\n                db=db, model_id=model_id\n            )\n    else:\n        return errors.not_found(\"Model not found\")\n"}
{"type": "source_file", "path": "whitebox/api/v1/models.py", "content": "from typing import List\nfrom whitebox.middleware.auth import authenticate_user\nfrom whitebox.schemas.model import Model, ModelCreateDto, ModelUpdateDto\nfrom fastapi import APIRouter, Depends, status\nfrom whitebox import crud\nfrom sqlalchemy.orm import Session\nfrom whitebox.core.db import get_db\nfrom whitebox.schemas.utils import StatusCode\nfrom whitebox.schemas.user import User\nfrom whitebox.utils.errors import add_error_responses, errors\n\n\nmodels_router = APIRouter()\n\n\n@models_router.post(\n    \"/models\",\n    tags=[\"Models\"],\n    response_model=Model,\n    summary=\"Create model\",\n    status_code=status.HTTP_201_CREATED,\n    responses=add_error_responses([400, 401]),\n)\nasync def create_model(\n    body: ModelCreateDto,\n    db: Session = Depends(get_db),\n    authenticated_user: User = Depends(authenticate_user),\n) -> Model:\n    \"\"\"Inserts a model into the database\"\"\"\n\n    granularity = body.granularity\n\n    try:\n        granularity_amount = float(granularity[:-1])\n    except ValueError:\n        return errors.bad_request(\"Granularity amount that was given is not a number!\")\n\n    if not granularity_amount.is_integer():\n        return errors.bad_request(\n            \"Granularity amount should be an integer and not a float (e.g. 1D)!\"\n        )\n\n    granularity_type = granularity[-1]\n    if granularity_type not in [\"T\", \"H\", \"D\", \"W\"]:\n        return errors.bad_request(\n            \"Wrong granularity type. Accepted values: T (minutes), H (hours), D (days), W (weeks)\"\n        )\n\n    new_model = crud.models.create(db=db, obj_in=body)\n    return new_model\n\n\n@models_router.get(\n    \"/models\",\n    tags=[\"Models\"],\n    response_model=List[Model],\n    summary=\"Get all models\",\n    status_code=status.HTTP_200_OK,\n    responses=add_error_responses([401]),\n)\nasync def get_all_models(\n    db: Session = Depends(get_db), authenticated_user: User = Depends(authenticate_user)\n):\n    \"\"\"Fetches all models from the database\"\"\"\n\n    models_in_db = crud.models.get_all(db=db)\n    return models_in_db\n\n\n@models_router.get(\n    \"/models/{model_id}\",\n    tags=[\"Models\"],\n    response_model=Model,\n    summary=\"Get model by id\",\n    status_code=status.HTTP_200_OK,\n    responses=add_error_responses([401, 404]),\n)\nasync def get_model(\n    model_id: str,\n    db: Session = Depends(get_db),\n    authenticated_user: User = Depends(authenticate_user),\n):\n    \"\"\"Fetches the model with the specified id from the database\"\"\"\n\n    model = crud.models.get(db=db, _id=model_id)\n\n    if not model:\n        return errors.not_found(\"Model not found\")\n\n    return model\n\n\n@models_router.put(\n    \"/models/{model_id}\",\n    tags=[\"Models\"],\n    response_model=Model,\n    summary=\"Update model\",\n    status_code=status.HTTP_200_OK,\n    responses=add_error_responses([400, 401, 404]),\n)\nasync def update_model(\n    model_id: str,\n    body: ModelUpdateDto,\n    db: Session = Depends(get_db),\n    authenticated_user: User = Depends(authenticate_user),\n) -> Model:\n    \"\"\"Updates record of the model with the specified id\"\"\"\n\n    # Remove all unset properties (with None values) from the update object\n    filtered_body = {k: v for k, v in dict(body).items() if v is not None}\n\n    model = crud.models.get(db=db, _id=model_id)\n\n    if not model:\n        return errors.not_found(\"Model not found\")\n\n    return crud.models.update(db=db, db_obj=model, obj_in=filtered_body)\n\n\n@models_router.delete(\n    \"/models/{model_id}\",\n    tags=[\"Models\"],\n    response_model=StatusCode,\n    summary=\"Delete model\",\n    status_code=status.HTTP_200_OK,\n    responses=add_error_responses([401, 404]),\n)\nasync def delete_model(\n    model_id: str,\n    db: Session = Depends(get_db),\n    authenticated_user: User = Depends(authenticate_user),\n) -> StatusCode:\n    \"\"\"Deletes the model with the specified id from the database\"\"\"\n\n    model = crud.models.get(db=db, _id=model_id)\n    if not model:\n        return errors.not_found(\"Model not found\")\n\n    crud.models.remove(db=db, _id=model_id)\n    return {\"status_code\": status.HTTP_200_OK}\n"}
{"type": "source_file", "path": "whitebox/api/v1/cron_tasks.py", "content": "from whitebox.schemas.utils import HealthCheck\nfrom fastapi import APIRouter, status\nfrom whitebox.cron_tasks.monitoring_metrics import run_calculate_metrics_pipeline\nfrom whitebox.cron_tasks.monitoring_alerts import run_create_alerts_pipeline\n\n\ncron_tasks_router = APIRouter()\n\n\n@cron_tasks_router.post(\n    \"/cron-tasks/run\",\n    tags=[\"Cron Tasks\"],\n    summary=\"Helper endpoint\",\n    status_code=status.HTTP_200_OK,\n    response_description=\"Result of cron tasks\",\n)\nasync def run_cron():\n    \"\"\"A helper endpoint that triggers the metrics and alerts pipelines while testing.\"\"\"\n\n    await run_calculate_metrics_pipeline()\n    await run_create_alerts_pipeline()\n    return HealthCheck(status=\"OK\")\n"}
{"type": "source_file", "path": "whitebox/core/db.py", "content": "from whitebox.core.settings import get_settings\nimport databases\nimport sqlalchemy\nfrom sqlalchemy.orm import sessionmaker\nfrom whitebox.entities.Base import Base\nfrom whitebox.schemas.user import UserCreateDto\n\nfrom whitebox import crud\nfrom whitebox.utils.passwords import encrypt_api_key\nfrom whitebox.utils.logger import cronLogger as logger\n\nfrom secrets import token_hex\n\nsettings = get_settings()\ndatabase = databases.Database(settings.DATABASE_URL)\nengine = sqlalchemy.create_engine(settings.DATABASE_URL)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n\n\ndef get_db():\n    db = SessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\n\n\nasync def connect():\n    \"\"\"\n    Connect to DB\n    \"\"\"\n    Base.metadata.create_all(engine)\n    db = SessionLocal()\n\n    admin_exists = crud.users.get_first_by_filter(db=db, username=\"admin\")\n    if not admin_exists:\n        plain_api_key = token_hex(32)\n        secret_key = settings.SECRET_KEY\n        api_key = (\n            encrypt_api_key(plain_api_key, secret_key.encode())\n            if secret_key\n            else plain_api_key\n        )\n\n        obj_in = UserCreateDto(username=\"admin\", api_key=api_key)\n        crud.users.create(db=db, obj_in=obj_in)\n        logger.info(f\"Created username: admin, API key: {plain_api_key}\")\n    await database.connect()\n\n\nasync def close():\n    \"\"\"\n    Close DB Connection\n    \"\"\"\n    await database.disconnect()\n    # logging.info(\"Closed connection with DB\")\n"}
{"type": "source_file", "path": "whitebox/crud/alerts.py", "content": "from typing import Any, List\nfrom sqlalchemy.orm import Session\nfrom whitebox.crud.base import CRUDBase\nfrom whitebox.entities.Alert import Alert as AlertEntity\nfrom whitebox.schemas.alert import Alert\n\n\nclass CRUD(CRUDBase[Alert, Any, Any]):\n    def get_model_alerts_by_model(self, db: Session, *, model_id: str) -> List[Alert]:\n        return db.query(self.model).filter(AlertEntity.model_id == model_id).all()\n\n\nalerts = CRUD(AlertEntity)\n"}
{"type": "source_file", "path": "whitebox/cron_tasks/monitoring_alerts.py", "content": "from datetime import datetime\nimport time\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker, Session\n\nfrom whitebox import crud, entities\nfrom whitebox.core.settings import get_settings\nfrom whitebox.cron_tasks.shared import (\n    get_all_models,\n    get_latest_drift_metrics_report,\n    get_latest_performance_metrics_report,\n    get_active_model_monitors,\n)\nfrom whitebox.schemas.model import Model, ModelType\nfrom whitebox.schemas.modelMonitor import ModelMonitor, MonitorMetrics\nfrom whitebox.utils.logger import cronLogger as logger\n\nsettings = get_settings()\n\nengine = create_engine(settings.DATABASE_URL)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\ndb: Session = SessionLocal()\n\n\nasync def run_create_performance_metric_alert_pipeline(\n    model: Model, monitor: ModelMonitor\n):\n    \"\"\"\n    Run the pipeline to find any alerts for a metric in performance metrics\n    If one is found it is saved in the database\n    \"\"\"\n\n    last_performance_metrics_report = await get_latest_performance_metrics_report(\n        db, model\n    )\n\n    if not last_performance_metrics_report:\n        logger.info(\n            f\"No alert created for monitor: {monitor.id} because no performance report was found!\"\n        )\n        return\n\n    # Performance metrics reports for not multi_class models have the same format: metric: float.\n    # Same if the metric is accuracy.\n    if (\n        model.type is not ModelType.multi_class\n        or monitor.metric == MonitorMetrics.accuracy\n    ):\n        metric_value = vars(last_performance_metrics_report)[monitor.metric]\n    else:\n        metric_value = vars(last_performance_metrics_report)[monitor.metric][\"weighted\"]\n\n    if metric_value < monitor.lower_threshold:\n        new_alert = entities.Alert(\n            model_id=model.id,\n            model_monitor_id=monitor.id,\n            timestamp=str(datetime.utcnow()),\n            description=f\"{monitor.metric} fell below the threshold of {monitor.lower_threshold} at value {metric_value}.\",\n        )\n        crud.alerts.create(db, obj_in=new_alert)\n        logger.info(f\"Created alert for monitor {monitor.id}!\")\n\n\nasync def run_create_drift_alert_pipeline(model: Model, monitor: ModelMonitor):\n    \"\"\"\n    Run the pipeline to find any alerts for a metric in drift metrics\n    If one is found it is saved in the database\n    \"\"\"\n\n    last_drift_report = await get_latest_drift_metrics_report(db, model)\n\n    if not last_drift_report:\n        logger.info(\n            f\"No alert created for monitor: {monitor.id} because no drift report was found!\"\n        )\n        return\n\n    if monitor.metric == MonitorMetrics.data_drift:\n        drift_detected: bool = last_drift_report.data_drift_summary[\"drift_by_columns\"][\n            monitor.feature\n        ][\"drift_detected\"]\n    else:\n        drift_detected: bool = last_drift_report.concept_drift_summary[\n            \"concept_drift_summary\"\n        ][\"drift_detected\"]\n\n    if drift_detected:\n        new_alert = entities.Alert(\n            model_id=model.id,\n            model_monitor_id=monitor.id,\n            timestamp=str(datetime.utcnow()),\n            description=f'{monitor.metric.capitalize().replace(\"_\", \" \")} found in \"{monitor.feature}\" feature.',\n        )\n        crud.alerts.create(db, obj_in=new_alert)\n        logger.info(f\"Created alert for monitor {monitor.id}!\")\n\n\nasync def run_create_alerts_pipeline():\n    logger.info(\"Beginning Alerts pipeline for all models!\")\n    start = time.time()\n    engine.connect()\n\n    models = await get_all_models(db)\n    if not models:\n        logger.info(\"No models found! Skipping pipeline\")\n    else:\n        for model in models:\n            model_monitors = await get_active_model_monitors(db, model_id=model.id)\n            for monitor in model_monitors:\n                if monitor.metric in [\n                    MonitorMetrics.accuracy,\n                    MonitorMetrics.precision,\n                    MonitorMetrics.recall,\n                    MonitorMetrics.f1,\n                    MonitorMetrics.r_square,\n                    MonitorMetrics.mean_squared_error,\n                    MonitorMetrics.mean_absolute_error,\n                ]:\n                    await run_create_performance_metric_alert_pipeline(model, monitor)\n                elif (\n                    monitor.metric == MonitorMetrics.data_drift\n                    or monitor.metric == MonitorMetrics.concept_drift\n                ):\n                    await run_create_drift_alert_pipeline(model, monitor)\n\n    db.close()\n    end = time.time()\n    logger.info(\"Alerts pipeline ended for all models!\")\n    logger.info(\"Runtime of Alerts pipeline took {}\".format(end - start))\n"}
{"type": "source_file", "path": "whitebox/cron_tasks/tasks.py", "content": "import os\nfrom whitebox.core.manager import get_task_manager\nfrom whitebox.cron_tasks.monitoring_metrics import run_calculate_metrics_pipeline\nfrom whitebox.cron_tasks.monitoring_alerts import run_create_alerts_pipeline\n\ntask_manager = get_task_manager()\n\nmetrics_cron = os.getenv(\"METRICS_CRON\") or \"0 12 * * *\"\n\ntask_manager.register(\n    name=\"metrics_cron\",\n    async_callable=run_calculate_metrics_pipeline,\n    crontab=metrics_cron,\n)\n\ntask_manager.register(\n    name=\"alerts_cron\",\n    async_callable=run_create_alerts_pipeline,\n    crontab=metrics_cron,\n)\n"}
{"type": "source_file", "path": "whitebox/cron.py", "content": "from fastapi import FastAPI, Depends\nimport asyncio\nimport json\nfrom whitebox.utils.logger import cronLogger as logger\n\nfrom whitebox.core.settings import get_cron_settings\nfrom whitebox.cron_tasks.tasks import task_manager\nfrom fastapi.openapi.utils import get_openapi\n\n\nsettings = get_cron_settings()\ncron_app = FastAPI(title=settings.APP_NAME_CRON, redoc_url=\"/\")\n\n\n@cron_app.on_event(\"startup\")\nasync def init():\n    # Start task amanager\n    asyncio.get_event_loop().create_task(task_manager.run())\n\n\n@cron_app.on_event(\"shutdown\")\nasync def shutdown():\n    logger.info(\"App is shutting down...\")\n    logger.info(\"Task Manager is shutting down...\")\n    await task_manager.shutdown()\n\n\ndef app_openapi():\n    if cron_app.openapi_schema:\n        return cron_app.openapi_schema\n    openapi_schema = get_openapi(\n        title=\"Cron API\", version=settings.VERSION, routes=cron_app.routes\n    )\n\n    cron_app.openapi_schema = openapi_schema\n    return cron_app.openapi_schema\n\n\ncron_app.openapi = app_openapi\n"}
{"type": "source_file", "path": "whitebox/crud/__init__.py", "content": "from .drifting_metrics import *\nfrom .dataset_rows import *\nfrom .inference_rows import *\nfrom .model_integrity_metrics import *\nfrom .model_monitors import *\nfrom .performance_metrics import *\nfrom .alerts import *\nfrom .users import *\nfrom .models import *\nfrom .base import *\n"}
{"type": "source_file", "path": "whitebox/crud/inference_rows.py", "content": "from typing import Any, List\nfrom datetime import datetime\nfrom sqlalchemy import and_\nfrom sqlalchemy.orm import Session\nfrom whitebox.crud.base import CRUDBase\nfrom whitebox.schemas.inferenceRow import InferenceRow, InferenceRowPreDb\nfrom whitebox.entities.Inference import InferenceRow as InferenceRowEntity\n\n\nclass CRUD(CRUDBase[InferenceRow, InferenceRowPreDb, Any]):\n    def get_inference_rows_by_model(\n        self, db: Session, *, model_id: str\n    ) -> List[InferenceRow]:\n        return (\n            db.query(self.model).filter(InferenceRowEntity.model_id == model_id).all()\n        )\n\n    def get_unused_inference_rows(\n        self, db: Session, *, model_id: str\n    ) -> List[InferenceRow]:\n        return (\n            db.query(self.model)\n            .filter(\n                InferenceRowEntity.model_id == model_id,\n                InferenceRowEntity.is_used == False,\n            )\n            .all()\n        )\n\n    def get_inference_rows_betweet_dates(\n        self, db: Session, *, model_id: str, min_date: datetime, max_date: datetime\n    ) -> List[InferenceRow]:\n        return (\n            db.query(self.model)\n            .filter(\n                and_(\n                    InferenceRowEntity.model_id == model_id,\n                    InferenceRowEntity.is_used == True,\n                    InferenceRowEntity.timestamp >= min_date,\n                    InferenceRowEntity.timestamp < max_date,\n                )\n            )\n            .all()\n        )\n\n\ninference_rows = CRUD(InferenceRowEntity)\n"}
{"type": "source_file", "path": "whitebox/crud/performance_metrics.py", "content": "from typing import Any, List, Union\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy import asc, desc\nfrom whitebox.crud.base import CRUDBase\nfrom whitebox.entities.PerformanceMetric import (\n    BinaryClassificationMetrics as BinaryClassificationMetricsEntity,\n    MultiClassificationMetrics as MultiClassificationMetricsEntity,\n    RegressionMetrics as RegressionMetricsEntity,\n)\nfrom whitebox.schemas.performanceMetric import (\n    BinaryClassificationMetrics,\n    MultiClassificationMetrics,\n    RegressionMetrics,\n)\n\n\nclass CRUD(\n    CRUDBase[Union[BinaryClassificationMetrics, MultiClassificationMetrics], Any, Any]\n):\n    def get_performance_metrics_by_model(\n        self, db: Session, *, model_id: int\n    ) -> Union[\n        List[BinaryClassificationMetrics],\n        List[MultiClassificationMetrics],\n        List[RegressionMetrics],\n    ]:\n        return (\n            db.query(self.model)\n            .filter(self.model.model_id == model_id)\n            .order_by(asc(\"timestamp\"))\n            .all()\n        )\n\n    def get_latest_report_by_model(\n        self, db: Session, *, model_id: int\n    ) -> Union[\n        BinaryClassificationMetrics, MultiClassificationMetrics, RegressionMetrics\n    ]:\n        return (\n            db.query(self.model)\n            .filter(self.model.model_id == model_id)\n            .order_by(desc(\"created_at\"))\n            .first()\n        )\n\n\nbinary_classification_metrics = CRUD(BinaryClassificationMetricsEntity)\nmulti_classification_metrics = CRUD(MultiClassificationMetricsEntity)\nregression_metrics = CRUD(RegressionMetricsEntity)\n"}
{"type": "source_file", "path": "whitebox/entities/Alert.py", "content": "from sqlalchemy import Column, String, ForeignKey, DateTime\nfrom whitebox.entities.Base import Base\nfrom whitebox.utils.id_gen import generate_uuid\n\n\nclass Alert(Base):\n    __tablename__ = \"alerts\"\n\n    id = Column(String, primary_key=True, unique=True, default=generate_uuid)\n    model_id = model_id = Column(String, ForeignKey(\"models.id\", ondelete=\"CASCADE\"))\n    model_monitor_id = Column(\n        String, ForeignKey(\"model_monitors.id\", ondelete=\"CASCADE\")\n    )\n    timestamp = Column(DateTime)\n    description = Column(String)\n    created_at = Column(DateTime)\n    updated_at = Column(DateTime)\n"}
{"type": "source_file", "path": "whitebox/crud/drifting_metrics.py", "content": "from typing import Any, List\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy import asc, desc\nfrom whitebox.crud.base import CRUDBase\nfrom whitebox.entities.DriftingMetric import DriftingMetric as DriftingMetricEntity\nfrom whitebox.schemas.driftingMetric import DriftingMetric\n\n\nclass CRUD(CRUDBase[DriftingMetric, Any, Any]):\n    def get_drifting_metrics_by_model(\n        self, db: Session, *, model_id: str\n    ) -> List[DriftingMetric]:\n        return (\n            db.query(self.model)\n            .filter(DriftingMetricEntity.model_id == model_id)\n            .order_by(asc(\"timestamp\"))\n            .all()\n        )\n\n    def get_latest_report_by_model(\n        self, db: Session, *, model_id: int\n    ) -> DriftingMetric:\n        return (\n            db.query(self.model)\n            .filter(self.model.model_id == model_id)\n            .order_by(desc(\"created_at\"))\n            .first()\n        )\n\n\ndrifting_metrics = CRUD(DriftingMetricEntity)\n"}
{"type": "source_file", "path": "whitebox/crud/model_integrity_metrics.py", "content": "from typing import Any, List\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy import asc\nfrom whitebox.crud.base import CRUDBase\nfrom whitebox.entities.ModelIntegrityMetric import (\n    ModelIntegrityMetric as ModelIntegrityMetricEntity,\n)\nfrom whitebox.schemas.modelIntegrityMetric import (\n    ModelIntegrityMetricCreate,\n    ModelIntegrityMetric,\n)\n\n\nclass CRUD(CRUDBase[ModelIntegrityMetric, ModelIntegrityMetricCreate, Any]):\n    def get_model_integrity_metrics_by_model(\n        self, db: Session, *, model_id: str\n    ) -> List[ModelIntegrityMetric]:\n        return (\n            db.query(self.model)\n            .filter(ModelIntegrityMetricEntity.model_id == model_id)\n            .order_by(asc(\"timestamp\"))\n            .all()\n        )\n\n\nmodel_integrity_metrics = CRUD(ModelIntegrityMetricEntity)\n"}
{"type": "source_file", "path": "whitebox/cron_tasks/shared.py", "content": "from typing import Callable, Dict, List, Tuple, Union\nimport itertools\nimport pandas as pd\nimport datetime\nfrom sqlalchemy.orm import Session\nfrom whitebox import crud\nfrom whitebox.schemas.inferenceRow import InferenceRow\nfrom whitebox.schemas.model import Model, ModelType\nfrom whitebox.schemas.modelMonitor import ModelMonitor\nfrom whitebox.schemas.driftingMetric import DriftingMetric\nfrom whitebox.schemas.performanceMetric import (\n    BinaryClassificationMetrics,\n    MultiClassificationMetrics,\n)\n\n\nasync def get_model_dataset_rows_df(db: Session, model_id: str) -> pd.DataFrame:\n    dataset_rows_in_db = crud.dataset_rows.get_dataset_rows_by_model(\n        db=db, model_id=model_id\n    )\n    dataset_rows_processed = [x.processed for x in dataset_rows_in_db]\n    dataset_df = pd.DataFrame(dataset_rows_processed)\n    return dataset_df\n\n\nasync def get_unused_model_inference_rows(\n    db: Session, model_id: str\n) -> List[InferenceRow]:\n    return crud.inference_rows.get_unused_inference_rows(db=db, model_id=model_id)\n\n\nasync def group_inference_rows_by_timestamp(\n    inference_rows: List[InferenceRow],\n    last_time: datetime.datetime,\n    granularity_amount: int,\n    granularity_type: str,\n) -> List[Dict[datetime.datetime, List[InferenceRow]]]:\n    \"\"\"Create a list of dicts with all inferences grouped by timestamp\"\"\"\n\n    dict_inference_rows = [vars(x) for x in inference_rows]\n\n    updated_inferences_dict = []\n    for x in dict_inference_rows:\n        new_obj = {**x}\n        new_obj[\"timestamp\"] = change_timestamp(\n            x[\"timestamp\"], last_time, granularity_amount, granularity_type\n        )\n        updated_inferences_dict.append(new_obj)\n\n    updated_inferences = [InferenceRow(**x) for x in updated_inferences_dict]\n\n    key_func: Callable[[InferenceRow]] = lambda x: x.timestamp\n\n    grouped_inferences = [\n        {key: list(group)}\n        for key, group in itertools.groupby(updated_inferences, key_func)\n    ]\n\n    return grouped_inferences\n\n\nasync def seperate_inference_rows(\n    inference_rows: List[InferenceRow],\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series]:\n    inference_rows_processed = [x.processed for x in inference_rows]\n    inference_rows_nonprocessed = [x.nonprocessed for x in inference_rows]\n    inference_rows_actual = [x.actual for x in inference_rows]\n    processed_df = pd.DataFrame(inference_rows_processed)\n    nonprocessed_df = pd.DataFrame(inference_rows_nonprocessed)\n    actual_df = pd.Series(inference_rows_actual)\n\n    # TODO: check if the length of the dataframes is the same\n    return processed_df, nonprocessed_df, actual_df\n\n\nasync def set_inference_rows_to_used(db: Session, ids: List[str]) -> None:\n    \"\"\"Changes the \"is_used\" property of an inference row to True\"\"\"\n\n    for id in ids:\n        inference_to_update = crud.inference_rows.get(db=db, _id=id)\n        crud.inference_rows.update(\n            db=db, db_obj=inference_to_update, obj_in={\"is_used\": True}\n        )\n\n\nasync def get_all_models(db: Session) -> List[Model]:\n    models_in_db = crud.models.get_all(db)\n    return models_in_db\n\n\nasync def get_active_model_monitors(db: Session, model_id: str) -> List[ModelMonitor]:\n    model_monitors_in_db = crud.model_monitors.get_active_model_monitors_by_model(\n        db=db, model_id=model_id\n    )\n    return model_monitors_in_db\n\n\nasync def get_latest_performance_metrics_report(\n    db: Session, model: Model\n) -> Union[BinaryClassificationMetrics, MultiClassificationMetrics]:\n    if model.type == ModelType.binary:\n        last_report_in_db = (\n            crud.binary_classification_metrics.get_latest_report_by_model(\n                db, model_id=model.id\n            )\n        )\n    elif model.type == ModelType.multi_class:\n        last_report_in_db = (\n            crud.multi_classification_metrics.get_latest_report_by_model(\n                db, model_id=model.id\n            )\n        )\n    else:\n        last_report_in_db = crud.regression_metrics.get_latest_report_by_model(\n            db, model_id=model.id\n        )\n    return last_report_in_db\n\n\nasync def get_latest_drift_metrics_report(db: Session, model: Model) -> DriftingMetric:\n    last_report_in_db = crud.drifting_metrics.get_latest_report_by_model(\n        db, model_id=model.id\n    )\n    return last_report_in_db\n\n\ndef round_timestamp(\n    timestamp: datetime.datetime, granularity_type: str\n) -> datetime.datetime:\n    \"\"\"Rounds a timestamp depending on a given unit\n    (e.g. if the unit is D (day) it converts 2023-03-03 12:33:25.34432 into 2023-03-03 00:00:00)\n    \"\"\"\n\n    if granularity_type == \"T\":\n        timestamp = timestamp.replace(second=0, microsecond=0)\n    elif granularity_type == \"H\":\n        timestamp = timestamp.replace(minute=0, second=0, microsecond=0)\n    else:\n        timestamp = timestamp.replace(hour=0, minute=0, second=0, microsecond=0)\n\n    return timestamp\n\n\ndef convert_granularity_to_secs(granularity_amount: int, granularity_type: str) -> int:\n    \"\"\"Converts a granularity into seconds (e.g. 1 day in 1 * 86400 = 86400 seconds)\"\"\"\n\n    amount_of_seconds = {\"T\": 60, \"H\": 3600, \"D\": 86400, \"W\": 604800}\n\n    return granularity_amount * amount_of_seconds[granularity_type]\n\n\ndef change_timestamp(\n    timestamp: datetime.datetime,\n    start_time: datetime.datetime,\n    granularity_amount: int,\n    granularity_type: str,\n) -> datetime.datetime:\n    \"\"\"Converts a specific timestamp into it's group's timestamp based on the granularity and previous group timestamp.\\n\n    (E.g. a timestamp 2023-03-03 12:33:25.34432 when granularity is set to 2D and the previous group's timestamp is \\\n        2023-03-03 00:00:00 will be converted into 2023-03-05 00:00:00)\"\"\"\n\n    timestamp_in_seconds = round_timestamp(timestamp, granularity_type).timestamp()\n\n    granularity_in_seconds = convert_granularity_to_secs(\n        granularity_amount, granularity_type\n    )\n\n    start_time_in_seconds = start_time.timestamp()\n\n    time_difference = (\n        timestamp_in_seconds - start_time_in_seconds\n    ) // granularity_in_seconds\n\n    new_timestamp_in_seconds = (\n        time_difference + 1\n    ) * granularity_in_seconds + start_time_in_seconds\n\n    new_timestamp = datetime.datetime.fromtimestamp(new_timestamp_in_seconds)\n\n    return new_timestamp\n\n\ndef get_used_inference_for_reusage(\n    db: Session,\n    model_id: str,\n    inferences: List[InferenceRow],\n    start_time: datetime.datetime,\n    granularity_amount: int,\n    granularity_type: str,\n) -> List[InferenceRow]:\n    \"\"\"Collects already used inference rows to be grouped with new rows of the same timestamp group and be \\\n        reused to create new reports.\"\"\"\n\n    timestamps = [x.timestamp for x in inferences]\n\n    changed_timestamps = [\n        change_timestamp(x, start_time, granularity_amount, granularity_type)\n        for x in timestamps\n    ]\n    unique_timestamps = list(set(changed_timestamps))\n\n    granularity_in_seconds = convert_granularity_to_secs(\n        granularity_amount, granularity_type\n    )\n\n    all_used_inferences = []\n\n    for timestamp in unique_timestamps:\n        previous_timestamp = timestamp - datetime.timedelta(\n            seconds=granularity_in_seconds\n        )\n\n        used_inferences_for_timestamp = (\n            crud.inference_rows.get_inference_rows_betweet_dates(\n                db=db,\n                model_id=model_id,\n                min_date=previous_timestamp,\n                max_date=timestamp,\n            )\n        )\n\n        all_used_inferences += used_inferences_for_timestamp\n\n    return all_used_inferences\n"}
{"type": "source_file", "path": "whitebox/crud/model_monitors.py", "content": "from typing import Any, List\nfrom whitebox.crud.base import CRUDBase\nfrom sqlalchemy.orm import Session\nfrom whitebox.schemas.modelMonitor import (\n    ModelMonitor,\n    ModelMonitorCreateDto,\n    MonitorStatus,\n)\nfrom whitebox.entities.ModelMonitor import ModelMonitor as ModelMonitorEntity\n\n\nclass CRUD(CRUDBase[ModelMonitor, ModelMonitorCreateDto, Any]):\n    def get_model_monitors_by_model(\n        self, db: Session, *, model_id: str\n    ) -> List[ModelMonitor]:\n        return (\n            db.query(self.model).filter(ModelMonitorEntity.model_id == model_id).all()\n        )\n\n    def get_active_model_monitors_by_model(\n        self, db: Session, *, model_id: str\n    ) -> List[ModelMonitor]:\n        return (\n            db.query(self.model)\n            .filter(\n                ModelMonitorEntity.model_id == model_id,\n                ModelMonitorEntity.status == MonitorStatus.active,\n            )\n            .all()\n        )\n\n\nmodel_monitors = CRUD(ModelMonitorEntity)\n"}
{"type": "source_file", "path": "whitebox/crud/base.py", "content": "from typing import Any, Dict, Generic, List, Optional, Type, TypeVar, Union\nfrom fastapi.encoders import jsonable_encoder\nfrom pydantic import BaseModel\nfrom sqlalchemy.orm import Session\nfrom whitebox.core.db import Base\nimport datetime\n\nModelType = TypeVar(\"ModelType\", bound=Base)\nCreateSchemaType = TypeVar(\"CreateSchemaType\", bound=BaseModel)\nUpdateSchemaType = TypeVar(\"UpdateSchemaType\", bound=BaseModel)\n\n\nclass CRUDBase(Generic[ModelType, CreateSchemaType, UpdateSchemaType]):\n    def __init__(self, model: Type[ModelType]):\n        \"\"\"\n        CRUD object with default methods to Create, Read, Update, Delete (CRUD).\n\n        **Parameters**\n\n        * `model`: A SQLAlchemy model class\n        * `schema`: A Pydantic model (schema) class\n        \"\"\"\n        self.model = model\n\n    def get(self, db: Session, _id: str) -> Optional[ModelType]:\n        return db.query(self.model).filter(self.model.id == _id).first()\n\n    def get_all(\n        self, db: Session, *, skip: int = 0, limit: int = 100\n    ) -> List[ModelType]:\n        return db.query(self.model).offset(skip).limit(limit).all()\n\n    def get_first_by_filter(self, db: Session, **kwargs: Any) -> Optional[ModelType]:\n        return db.query(self.model).filter_by(**kwargs).first()\n\n    def create(self, db: Session, *, obj_in: CreateSchemaType) -> ModelType:\n        date_now = datetime.datetime.utcnow()\n        obj_in_data = jsonable_encoder(obj_in)\n        db_obj = self.model(**obj_in_data, created_at=date_now, updated_at=date_now)\n        db.add(db_obj)\n        db.commit()\n        db.refresh(db_obj)\n        return db_obj\n\n    def create_many(\n        self, db: Session, *, obj_list: List[CreateSchemaType]\n    ) -> List[ModelType]:\n        date_now = datetime.datetime.utcnow()\n        obj_list_in_data = jsonable_encoder(obj_list)\n        db_obj_list = list(\n            map(\n                lambda x: self.model(**x, created_at=date_now, updated_at=date_now),\n                obj_list_in_data,\n            )\n        )\n        db.add_all(db_obj_list)\n        db.commit()\n        for obj in db_obj_list:\n            db.refresh(obj)\n        return db_obj_list\n\n    def update(\n        self,\n        db: Session,\n        *,\n        db_obj: ModelType,\n        obj_in: Union[UpdateSchemaType, Dict[str, Any]]\n    ) -> ModelType:\n        date_now = datetime.datetime.utcnow()\n        obj_data = jsonable_encoder(db_obj)\n        if isinstance(obj_in, dict):\n            update_data = obj_in\n        else:\n            update_data = obj_in.dict(exclude_unset=True)\n        for field in obj_data:\n            if field in update_data:\n                setattr(db_obj, field, update_data[field])\n        setattr(db_obj, \"updated_at\", date_now)\n        db.commit()\n        db.refresh(db_obj)\n        return db_obj\n\n    def remove(self, db: Session, *, _id: str):\n        db.query(self.model).filter(self.model.id == _id).delete()\n        db.commit()\n        return\n"}
{"type": "source_file", "path": "whitebox/entities/Base.py", "content": "from typing import Any, Dict\nfrom sqlalchemy.ext.declarative import as_declarative, declared_attr\n\n\nclass_registry: Dict = {}\n\n\n@as_declarative(class_registry=class_registry)\nclass Base:\n    id: Any\n    __name__: str\n\n    # Generate __tablename__ automatically\n    @declared_attr\n    def __tablename__(cls) -> str:\n        return cls.__name__.lower()\n"}
{"type": "source_file", "path": "whitebox/crud/users.py", "content": "from typing import Any\nfrom whitebox.crud.base import CRUDBase\nfrom whitebox.schemas.user import User, UserCreateDto\nfrom whitebox.entities.User import User as UserEntity\n\n\nclass CRUD(CRUDBase[User, UserCreateDto, Any]):\n    pass\n\n\nusers = CRUD(UserEntity)\n"}
{"type": "source_file", "path": "whitebox/cron_tasks/monitoring_metrics.py", "content": "from datetime import datetime\nimport pandas as pd\nimport time\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker, Session\nfrom fastapi.encoders import jsonable_encoder\nfrom whitebox import crud, entities\nfrom whitebox.analytics.drift.pipelines import (\n    run_data_drift_pipeline,\n    run_concept_drift_pipeline,\n)\nfrom whitebox.analytics.metrics.pipelines import (\n    create_binary_classification_evaluation_metrics_pipeline,\n    create_feature_metrics_pipeline,\n    create_multiple_classification_evaluation_metrics_pipeline,\n    create_regression_evaluation_metrics_pipeline,\n)\nfrom whitebox.core.settings import get_settings\nfrom whitebox.cron_tasks.shared import (\n    get_all_models,\n    get_model_dataset_rows_df,\n    get_unused_model_inference_rows,\n    group_inference_rows_by_timestamp,\n    seperate_inference_rows,\n    set_inference_rows_to_used,\n    get_latest_drift_metrics_report,\n    round_timestamp,\n    get_used_inference_for_reusage,\n)\nfrom whitebox.schemas.model import Model, ModelType\nfrom whitebox.schemas.modelIntegrityMetric import ModelIntegrityMetricCreate\nfrom whitebox.utils.logger import cronLogger as logger\n\n\nsettings = get_settings()\n\nengine = create_engine(settings.DATABASE_URL)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\ndb: Session = SessionLocal()\n\n\nasync def run_calculate_drifting_metrics_pipeline(\n    model: Model, inference_processed_df: pd.DataFrame, timestamp: datetime\n):\n    \"\"\"\n    Run the pipeline to calculate the drifting metrics\n    After the metrics are calculated they are saved in the database\n    \"\"\"\n\n    training_processed_df = await get_model_dataset_rows_df(db, model_id=model.id)\n\n    if training_processed_df.empty:\n        logger.info(f\"Can't calculate data drift metrics for model {model.id}\")\n        return\n\n    logger.info(f\"Calculating drifting metrics for model {model.id}...\")\n\n    # We need to drop the target column from the data to calculate drifting metrics\n    processed_inference_dropped_target_df = inference_processed_df.drop(\n        [model.target_column], axis=1\n    )\n    processed_training_dropped_target_df = training_processed_df.drop(\n        [model.target_column], axis=1\n    )\n\n    data_drift_report = run_data_drift_pipeline(\n        processed_training_dropped_target_df, processed_inference_dropped_target_df\n    )\n    concept_drift_report = run_concept_drift_pipeline(\n        training_processed_df,\n        inference_processed_df,\n        model.target_column,\n    )\n\n    new_drifting_metric = entities.DriftingMetric(\n        timestamp=str(timestamp),\n        model_id=model.id,\n        concept_drift_summary=concept_drift_report,\n        data_drift_summary=data_drift_report,\n    )\n\n    existing_report = crud.drifting_metrics.get_first_by_filter(\n        db=db, model_id=model.id, timestamp=timestamp\n    )\n    if existing_report:\n        crud.drifting_metrics.update(\n            db=db, db_obj=existing_report, obj_in=jsonable_encoder(new_drifting_metric)\n        )\n    else:\n        crud.drifting_metrics.create(db, obj_in=new_drifting_metric)\n    logger.info(\"Drifting metrics calculated!\")\n\n\nasync def run_calculate_performance_metrics_pipeline(\n    model: Model,\n    inference_processed_df: pd.DataFrame,\n    actual_df: pd.DataFrame,\n    timestamp: datetime,\n):\n    \"\"\"\n    Run the pipeline to calculate the performance metrics\n    After the metrics are calculated they are saved in the database\n    \"\"\"\n\n    cleaned_actuals_df = actual_df.dropna()\n\n    if cleaned_actuals_df.empty:\n        logger.info(\n            f\"Can't calculate performance metrics for model {model.id} because no actuals were found!\"\n        )\n        return\n\n    if len(cleaned_actuals_df.index) != len(actual_df.index):\n        logger.info(\n            f\"Performance metrics will be calculated only for a portion of rows for model: {model.id}\\\n                because actuals were not provided for all inference rows!\"\n        )\n        inference_processed_df = inference_processed_df.iloc[cleaned_actuals_df.index]\n\n    inference_processed_df = inference_processed_df.reset_index(drop=True)\n    cleaned_actuals_df = cleaned_actuals_df.reset_index(drop=True)\n\n    if model.type is not ModelType.regression:\n        if not model.labels:\n            logger.info(\n                f\"Can't calculate performance metrics for model {model.id} because labels are required for binary and multi_class models!\"\n            )\n            return\n        labels = list(model.labels.values())\n\n    logger.info(f\"Calculating performance metrics for model {model.id}\")\n    if model.type == ModelType.binary:\n        binary_classification_metrics_report = (\n            create_binary_classification_evaluation_metrics_pipeline(\n                cleaned_actuals_df, inference_processed_df[model.target_column], labels\n            )\n        )\n\n        new_performance_metric = entities.BinaryClassificationMetrics(\n            model_id=model.id,\n            timestamp=str(timestamp),\n            **dict(binary_classification_metrics_report),\n        )\n\n        existing_report = crud.binary_classification_metrics.get_first_by_filter(\n            db=db, model_id=model.id, timestamp=timestamp\n        )\n        if existing_report:\n            crud.binary_classification_metrics.update(\n                db=db,\n                db_obj=existing_report,\n                obj_in=jsonable_encoder(new_performance_metric),\n            )\n        else:\n            crud.binary_classification_metrics.create(db, obj_in=new_performance_metric)\n\n    elif model.type == ModelType.multi_class:\n        multiclass_classification_metrics_report = (\n            create_multiple_classification_evaluation_metrics_pipeline(\n                cleaned_actuals_df, inference_processed_df[model.target_column], labels\n            )\n        )\n\n        new_performance_metric = entities.MultiClassificationMetrics(\n            model_id=model.id,\n            timestamp=str(timestamp),\n            **dict(multiclass_classification_metrics_report),\n        )\n\n        existing_report = crud.multi_classification_metrics.get_first_by_filter(\n            db=db, model_id=model.id, timestamp=timestamp\n        )\n        if existing_report:\n            crud.multi_classification_metrics.update(\n                db=db,\n                db_obj=existing_report,\n                obj_in=jsonable_encoder(new_performance_metric),\n            )\n        else:\n            crud.multi_classification_metrics.create(db, obj_in=new_performance_metric)\n\n    elif model.type == ModelType.regression:\n        regression_metrics_report = create_regression_evaluation_metrics_pipeline(\n            cleaned_actuals_df, inference_processed_df[model.target_column]\n        )\n\n        new_performance_metric = entities.RegressionMetrics(\n            model_id=model.id,\n            timestamp=str(timestamp),\n            **dict(regression_metrics_report),\n        )\n\n        existing_report = crud.regression_metrics.get_first_by_filter(\n            db=db, model_id=model.id, timestamp=timestamp\n        )\n        if existing_report:\n            crud.regression_metrics.update(\n                db=db,\n                db_obj=existing_report,\n                obj_in=jsonable_encoder(new_performance_metric),\n            )\n        else:\n            crud.regression_metrics.create(db, obj_in=new_performance_metric)\n\n    logger.info(\"Performance metrics calculated!\")\n\n\nasync def run_calculate_feature_metrics_pipeline(\n    model: Model, inference_processed_df: pd.DataFrame, timestamp: datetime\n):\n    \"\"\"\n    Run the pipeline to calculate the feature metrics\n    After the metrics are calculated they are saved in the database\n    \"\"\"\n\n    logger.info(f\"Calculating feature metrics for model {model.id}\")\n    feature_metrics_report = create_feature_metrics_pipeline(inference_processed_df)\n\n    if feature_metrics_report:\n        new_feature_metric = ModelIntegrityMetricCreate(\n            model_id=model.id,\n            timestamp=str(timestamp),\n            feature_metrics=feature_metrics_report,\n        )\n\n        existing_report = crud.model_integrity_metrics.get_first_by_filter(\n            db=db, model_id=model.id, timestamp=timestamp\n        )\n        if existing_report:\n            crud.model_integrity_metrics.update(\n                db=db,\n                db_obj=existing_report,\n                obj_in=jsonable_encoder(new_feature_metric),\n            )\n        else:\n            crud.model_integrity_metrics.create(db, obj_in=new_feature_metric)\n\n        logger.info(\"Feature metrics calculated!\")\n\n\nasync def run_calculate_metrics_pipeline():\n    logger.info(\"Beginning Metrics pipeline for all models!\")\n    start = time.time()\n    engine.connect()\n\n    models = await get_all_models(db)\n    if not models:\n        logger.info(\"No models found! Skipping pipeline\")\n    else:\n        for model in models:\n            granularity = model.granularity\n            granularity_amount = int(granularity[:-1])\n            granularity_type = granularity[-1]\n\n            last_report = await get_latest_drift_metrics_report(db, model)\n\n            # We need to get the last report's timestamp as a base of grouping unless there's no report produced.\n            # In this case, the base timestamp is considered the \"now\" rounded to the day so the intervals start from midnight\n            # e.g. 12:00, 12:15, 12:30, 12:45 and so on if granularity is 15T.\n            last_report_time = (\n                last_report.timestamp\n                if last_report\n                else round_timestamp(datetime.utcnow(), \"1D\")\n            )\n\n            unused_inference_rows_in_db = await get_unused_model_inference_rows(\n                db, model_id=model.id\n            )\n\n            if len(unused_inference_rows_in_db) == 0:\n                logger.info(\n                    f\"No new inferences found for model {model.id}! Continuing with next model...\"\n                )\n                continue\n            logger.info(f\"Executing Metrics pipeline for model {model.id}...\")\n\n            used_inferences = get_used_inference_for_reusage(\n                db,\n                model.id,\n                unused_inference_rows_in_db,\n                last_report_time,\n                granularity_amount,\n                granularity_type,\n            )\n\n            all_inferences = unused_inference_rows_in_db + used_inferences\n\n            grouped_inference_rows = await group_inference_rows_by_timestamp(\n                all_inferences,\n                last_report_time,\n                granularity_amount,\n                granularity_type,\n            )\n\n            for group in grouped_inference_rows:\n                for timestamp, inference_group in group.items():\n                    inference_rows_ids = [x.id for x in inference_group]\n                    (\n                        inference_processed_df,\n                        inference_nonprocessed_df,\n                        actual_df,\n                    ) = await seperate_inference_rows(inference_group)\n\n                    await run_calculate_drifting_metrics_pipeline(\n                        model, inference_processed_df, timestamp\n                    )\n\n                    await run_calculate_performance_metrics_pipeline(\n                        model, inference_processed_df, actual_df, timestamp\n                    )\n\n                    await run_calculate_feature_metrics_pipeline(\n                        model, inference_processed_df, timestamp\n                    )\n\n                    await set_inference_rows_to_used(db, inference_rows_ids)\n\n            logger.info(f\"Ended Metrics pipeline for model {model.id}...\")\n\n    db.close()\n    end = time.time()\n    logger.info(\"Metrics pipeline ended for all models!\")\n    logger.info(\"Runtime of Metrics pipeline took {}\".format(end - start))\n"}
{"type": "source_file", "path": "whitebox/crud/models.py", "content": "from whitebox.crud.base import CRUDBase\nfrom whitebox.schemas.model import Model, ModelCreateDto, ModelUpdateDto\nfrom whitebox.entities.Model import Model as ModelEntity\n\n\nclass CRUD(CRUDBase[Model, ModelCreateDto, ModelUpdateDto]):\n    pass\n\n\nmodels = CRUD(ModelEntity)\n"}
{"type": "source_file", "path": "whitebox/crud/dataset_rows.py", "content": "from typing import Any, List\nfrom sqlalchemy.orm import Session\nfrom whitebox.crud.base import CRUDBase\nfrom whitebox.schemas.datasetRow import DatasetRow, DatasetRowCreate\nfrom whitebox.entities.DatasetRow import DatasetRow as DatasetRowEntity\n\n\nclass CRUD(CRUDBase[DatasetRow, DatasetRowCreate, Any]):\n    def get_dataset_rows_by_model(\n        self, db: Session, *, model_id: str\n    ) -> List[DatasetRow]:\n        return db.query(self.model).filter(DatasetRowEntity.model_id == model_id).all()\n\n\ndataset_rows = CRUD(DatasetRowEntity)\n"}
{"type": "source_file", "path": "whitebox/entities/ModelIntegrityMetric.py", "content": "from sqlalchemy import Column, JSON, Float, ForeignKey, String, DateTime\nfrom whitebox.entities.Base import Base\nfrom whitebox.utils.id_gen import generate_uuid\n\n\nclass ModelIntegrityMetric(Base):\n    __tablename__ = \"model_integrity_metrics\"\n\n    id = Column(String, primary_key=True, unique=True, default=generate_uuid)\n    model_id = Column(String, ForeignKey(\"models.id\", ondelete=\"CASCADE\"))\n    timestamp = Column(DateTime)\n    feature_metrics = Column(JSON)\n    created_at = Column(DateTime)\n    updated_at = Column(DateTime)\n"}
{"type": "source_file", "path": "whitebox/entities/DatasetRow.py", "content": "from sqlalchemy import Column, String, ForeignKey, DateTime, JSON\nfrom whitebox.entities.Base import Base\nfrom whitebox.utils.id_gen import generate_uuid\n\n\nclass DatasetRow(Base):\n    __tablename__ = \"dataset_rows\"\n\n    id = Column(String, primary_key=True, unique=True, default=generate_uuid)\n    model_id = Column(String, ForeignKey(\"models.id\", ondelete=\"CASCADE\"))\n    nonprocessed = Column(JSON)\n    processed = Column(JSON)\n    created_at = Column(DateTime)\n    updated_at = Column(DateTime)\n"}
{"type": "source_file", "path": "whitebox/entities/Model.py", "content": "from sqlalchemy import Column, String, DateTime, JSON, Enum\nfrom whitebox.entities.Base import Base\nfrom whitebox.utils.id_gen import generate_uuid\nfrom sqlalchemy.orm import relationship\nfrom whitebox.schemas.model import ModelType\n\n\nclass Model(Base):\n    __tablename__ = \"models\"\n\n    id = Column(String, primary_key=True, unique=True, default=generate_uuid)\n    name = Column(String)\n    description = Column(String)\n    type = Column(\"type\", Enum(ModelType))\n    target_column = Column(String)\n    granularity = Column(String)\n    labels = Column(JSON, nullable=True)\n    created_at = Column(DateTime)\n    updated_at = Column(DateTime)\n\n    dataset_rows = relationship(\"DatasetRow\")\n    inference_rows = relationship(\"InferenceRow\")\n    binary_classification_metrics = relationship(\"BinaryClassificationMetrics\")\n    multi_classification_metrics = relationship(\"MultiClassificationMetrics\")\n    regression_metrics = relationship(\"RegressionMetrics\")\n    drifting_metrics = relationship(\"DriftingMetric\")\n    model_integrity_metrics = relationship(\"ModelIntegrityMetric\")\n    model_monitors = relationship(\"ModelMonitor\")\n"}
{"type": "source_file", "path": "whitebox/entities/DriftingMetric.py", "content": "from sqlalchemy import Column, ForeignKey, String, DateTime, JSON\nfrom whitebox.entities.Base import Base\nfrom whitebox.utils.id_gen import generate_uuid\n\n\nclass DriftingMetric(Base):\n    __tablename__ = \"drifting_metrics\"\n\n    id = Column(String, primary_key=True, unique=True, default=generate_uuid)\n    model_id = Column(String, ForeignKey(\"models.id\", ondelete=\"CASCADE\"))\n    timestamp = Column(DateTime)\n    concept_drift_summary = Column(JSON)\n    data_drift_summary = Column(JSON)\n    created_at = Column(DateTime)\n    updated_at = Column(DateTime)\n"}
{"type": "source_file", "path": "whitebox/analytics/metrics/functions.py", "content": "from sklearn.metrics import multilabel_confusion_matrix\nimport pandas as pd\nfrom typing import Dict, Union, List\n\n\ndef format_feature_metrics(\n    missing_count: Dict[str, int],\n    non_missing_count: Dict[str, int],\n    mean: Dict[str, float],\n    minimum: Dict[str, float],\n    maximum: Dict[str, float],\n    sum: Dict[str, float],\n    standard_deviation: Dict[str, float],\n    variance: Dict[str, float],\n) -> Dict[str, Union[int, float]]:\n    formated_metrics = {\n        \"missing_count\": missing_count,\n        \"non_missing_count\": non_missing_count,\n        \"mean\": mean,\n        \"minimum\": minimum,\n        \"maximum\": maximum,\n        \"sum\": sum,\n        \"standard_deviation\": standard_deviation,\n        \"variance\": variance,\n    }\n\n    return formated_metrics\n\n\ndef format_evaluation_metrics_binary(\n    accuracy: float,\n    precision: float,\n    recall: float,\n    f1: float,\n    tn: int,\n    fp: int,\n    fn: int,\n    tp: int,\n) -> Dict[str, Union[int, float]]:\n    formated_metrics_for_binary = {\n        \"accuracy\": accuracy,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,\n        \"true_negative\": tn,\n        \"false_positive\": fp,\n        \"false_negative\": fn,\n        \"true_positive\": tp,\n    }\n\n    return formated_metrics_for_binary\n\n\ndef format_evaluation_metrics_multiple(\n    accuracy: float,\n    precision_statistics: Dict[str, float],\n    recall_statistics: Dict[str, float],\n    f1_statistics: Dict[str, float],\n    conf_matrix: Dict[str, Dict[str, int]],\n) -> Dict[str, Union[float, Dict[str, Union[int, float]]]]:\n    formated_metrics_for_multiple = {\n        \"accuracy\": accuracy,\n        \"precision\": precision_statistics,\n        \"recall\": recall_statistics,\n        \"f1\": f1_statistics,\n        \"confusion_matrix\": conf_matrix,\n    }\n\n    return formated_metrics_for_multiple\n\n\ndef format_evaluation_metrics_regression(\n    r_square: float,\n    mean_squared_error: float,\n    mean_absolute_error: float,\n) -> Dict[str, Union[int, float]]:\n    formated_metrics_for_regression = {\n        \"r_square\": r_square,\n        \"mean_squared_error\": mean_squared_error,\n        \"mean_absolute_error\": mean_absolute_error,\n    }\n\n    return formated_metrics_for_regression\n\n\ndef confusion_for_multiclass(\n    test_set: pd.DataFrame, prediction_set: pd.DataFrame, labels: List[int]\n) -> Dict[str, Dict[str, int]]:\n    \"\"\"\n    Gets 2 datasets based on multiclass classification and calculates\n    the corresponding confusion matrix outputs tn, fp, fn, tp\n\n    Parameters\n    ----------\n    test_set : pd.DataFrame\n        Multiclass ground truth labels.\n\n    y_score : pd.DataFrame\n        Multiclass predicted labels.\n\n    Returns\n    -------\n    mult_dict : Dict\n\n    \"\"\"\n    cm = multilabel_confusion_matrix(test_set, prediction_set, labels=labels)\n    mult_dict = {}\n    class_key = 0\n    for i in cm:\n        tn, fp, fn, tp = i.ravel()\n        eval_dict = {\n            \"true_negative\": tn,\n            \"false_positive\": fp,\n            \"false_negative\": fn,\n            \"true_positive\": tp,\n        }\n        mult_dict[\"class{}\".format(class_key)] = eval_dict\n        class_key = class_key + 1\n    return mult_dict\n"}
{"type": "source_file", "path": "whitebox/entities/Inference.py", "content": "from sqlalchemy import Boolean, Column, String, ForeignKey, DateTime, JSON, Float\nfrom whitebox.entities.Base import Base\nfrom whitebox.utils.id_gen import generate_uuid\n\n\nclass InferenceRow(Base):\n    __tablename__ = \"inference_rows\"\n\n    id = Column(String, primary_key=True, unique=True, default=generate_uuid)\n    model_id = Column(String, ForeignKey(\"models.id\", ondelete=\"CASCADE\"))\n    timestamp = Column(DateTime)\n    nonprocessed = Column(JSON)\n    processed = Column(JSON)\n    is_used = Column(Boolean)\n    actual = Column(Float, nullable=True)\n\n    created_at = Column(DateTime)\n    updated_at = Column(DateTime)\n"}
{"type": "source_file", "path": ".github/scripts/bump_chart_version.py", "content": "import yaml\nimport sys\n\nwith open(\"helm_charts/whitebox/Chart.yaml\", \"r\") as f:\n    chart = yaml.safe_load(f)\n\nif len(sys.argv) < 2:\n    raise Exception(\"Version number not provided\")\n\n# If it starts with a v, remove it\nif sys.argv[1].startswith(\"v\"):\n    sys.argv[1] = sys.argv[1][1:]\n\nchart[\"version\"] = sys.argv[1]\n\nwith open(\"helm_charts/whitebox/Chart.yaml\", \"w\") as f:\n    yaml.dump(chart, f)\n\nprint(f\"Updated chart version to {sys.argv[1]}\")\n"}
