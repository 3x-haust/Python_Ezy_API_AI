{"repo_info": {"repo_name": "pytorch-vqgan", "repo_owner": "Shubhamai", "repo_url": "https://github.com/Shubhamai/pytorch-vqgan"}}
{"type": "test_file", "path": "test/test_dataloader.py", "content": "from dataloader import load_mnist\n\n\ndef test_load_mnist():\n\n    dataloader = load_mnist(batch_size=16)\n\n    for imgs in dataloader:\n        break\n\n    assert imgs.shape == (16, 1, 256, 256)"}
{"type": "test_file", "path": "test/test_vqgan.py", "content": "\"\"\"\nContains test functions for the VQGAN model. \n\"\"\"\n\n# Importing Libraries\nimport torch\n\nfrom vqgan import Encoder, Decoder, CodeBook, Discriminator\n\n\ndef test_encoder():\n\n    image_channels = 3\n    image_size = 256\n    latent_channels = 256\n    attn_resolution = 16\n\n    image = torch.randn(1, image_channels, image_size, image_size)\n\n    model = Encoder(\n        img_channels=image_channels,\n        image_size=image_size,\n        latent_channels=latent_channels,\n        attention_resolution=[attn_resolution],\n    )\n\n    output = model(image)\n\n    assert output.shape == (\n        1,\n        latent_channels,\n        attn_resolution,\n        attn_resolution,\n    ), \"Output of encoder does not match\"\n\n\ndef test_decoder():\n\n    img_channels = 3\n    img_size = 256\n    latent_channels = 256\n    latent_size = 16\n    attn_resolution = 16\n\n    latent = torch.randn(1, latent_channels, latent_size, latent_size)\n    model = Decoder(\n        img_channels=img_channels,\n        latent_size=latent_size,\n        latent_channels=latent_channels,\n        attention_resolution=[attn_resolution],\n    )\n\n    output = model(latent)\n\n    assert output.shape == (\n        1,\n        img_channels,\n        img_size,\n        img_size,\n    ), \"Output of decoder does not match\"\n\n\ndef test_codebook():\n\n    z = torch.randn(1, 256, 16, 16)\n\n    codebok = CodeBook(num_codebook_vectors=100, latent_dim=16)\n\n    z_q, min_distance_indices, loss = codebok(z)\n\n    assert z_q.shape == (1, 256, 16, 16), \"Output of codebook does not match\"\n\n\ndef test_discriminator():\n\n    image = torch.randn(1, 3, 256, 256)\n\n    model = Discriminator()\n\n\n    output = model(image)\n\n    assert output.shape == (1, 1, 30, 30), \"Output of discriminator does not match\"\n"}
{"type": "source_file", "path": "trainer/trainer.py", "content": "# Importing Libraries\nimport os\n\nimport torch\nimport torchvision\nfrom aim import Run\nfrom utils import reproducibility\n\nfrom trainer import TransformerTrainer, VQGANTrainer\n\n\nclass Trainer:\n    def __init__(\n        self,\n        vqgan: torch.nn.Module,\n        transformer: torch.nn.Module,\n        run: Run,\n        config: dict,\n        experiment_dir: str = \"experiments\",\n        seed: int = 42,\n        device: str = \"cuda\",\n    ) -> None:\n\n        self.vqgan = vqgan\n        self.transformer = transformer\n\n        self.run = run\n        self.config = config\n        self.experiment_dir = experiment_dir\n        self.seed = seed\n        self.device = device\n\n        print(f\"[INFO] Setting seed to {seed}\")\n        reproducibility(seed)\n\n        print(f\"[INFO] Results will be saved in {experiment_dir}\")\n        self.experiment_dir = experiment_dir\n\n    def train_vqgan(self, dataloader: torch.utils.data.DataLoader, epochs: int = 1):\n\n        print(f\"[INFO] Training VQGAN on {self.device} for {epochs} epoch(s).\")\n\n        self.vqgan.to(self.device)\n\n        self.vqgan_trainer = VQGANTrainer(\n            model=self.vqgan,\n            run=self.run,\n            device=self.device,\n            experiment_dir=self.experiment_dir,\n            **self.config[\"vqgan\"],\n        )\n\n        self.vqgan_trainer.train(\n            dataloader=dataloader,\n            epochs=epochs,\n        )\n\n        # Saving the model\n        self.vqgan.save_checkpoint(\n            os.path.join(self.experiment_dir, \"checkpoints\", \"vqgan.pt\")\n        )\n\n    def train_transformers(\n        self, dataloader: torch.utils.data.DataLoader, epochs: int = 1\n    ):\n\n        print(f\"[INFO] Training Transformer on {self.device} for {epochs} epoch(s).\")\n\n        self.vqgan.eval()\n        self.transformer = self.transformer.to(self.device)\n\n        self.transformer_trainer = TransformerTrainer(\n            model=self.transformer,\n            run=self.run,\n            device=self.device,\n            experiment_dir=self.experiment_dir,\n            **self.config[\"transformer\"],\n        )\n\n        self.transformer_trainer.train(dataloader=dataloader, epochs=epochs)\n\n        self.transformer.save_checkpoint(\n            os.path.join(self.experiment_dir, \"checkpoints\", \"transformer.pt\")\n        )\n\n    def generate_images(self, n_images: int = 5):\n\n        print(f\"[INFO] Generating {n_images} images...\")\n\n        self.vqgan.to(self.device)\n        self.transformer = self.transformer.to(self.device)\n\n\n        for i in range(n_images):\n            start_indices = torch.zeros((4, 0)).long().to(self.device)\n            sos_tokens = torch.ones(start_indices.shape[0], 1) * 0\n\n            sos_tokens = sos_tokens.long().to(self.device)\n            sample_indices = self.transformer.sample(\n                start_indices, sos_tokens, steps=256\n            )\n            sampled_imgs = self.transformer.z_to_image(sample_indices)\n            torchvision.utils.save_image(\n                sampled_imgs,\n                os.path.join(self.experiment_dir, f\"generated_{i}.jpg\"),\n                nrow=4,\n            )\n\n"}
{"type": "source_file", "path": "trainer/__init__.py", "content": "from .vqgan import VQGANTrainer\nfrom .transformer import TransformerTrainer\nfrom .trainer import Trainer"}
{"type": "source_file", "path": "trainer/transformer.py", "content": "# Importing Libraries\nimport torchvision\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom aim import Run, Image\n\n\nclass TransformerTrainer:\n    def __init__(\n        self,\n        model: nn.Module,\n        run: Run,\n        experiment_dir: str = \"experiments\",\n        device: str = \"cuda\",\n        learning_rate: float = 4.5e-06,\n        beta1: float = 0.9,\n        beta2: float = 0.95,\n    ):\n        self.run = run\n        self.experiment_dir = experiment_dir\n\n        self.model = model\n        self.device = device\n        self.optim = self.configure_optimizers(\n            learning_rate=learning_rate, beta1=beta1, beta2=beta2\n        )\n\n    def configure_optimizers(\n        self, learning_rate: float = 4.5e-06, beta1: float = 0.9, beta2: float = 0.95\n    ):\n        decay, no_decay = set(), set()\n        whitelist_weight_modules = (nn.Linear,)\n        blacklist_weight_modules = (nn.LayerNorm, nn.Embedding)\n\n        # Enabling weight decay to only certain layers\n        for mn, m in self.model.transformer.named_modules():\n            for pn, p in m.named_parameters():\n                fpn = f\"{mn}.{pn}\" if mn else pn\n\n                if pn.endswith(\"bias\"):\n                    no_decay.add(fpn)\n\n                elif pn.endswith(\"weight\") and isinstance(m, whitelist_weight_modules):\n                    decay.add(fpn)\n\n                elif pn.endswith(\"weight\") and isinstance(m, blacklist_weight_modules):\n                    no_decay.add(fpn)\n\n        no_decay.add(\"pos_emb\")\n\n        param_dict = {pn: p for pn, p in self.model.transformer.named_parameters()}\n\n        optim_groups = [\n            {\n                \"params\": [param_dict[pn] for pn in sorted(list(decay))],\n                \"weight_decay\": 0.01,\n            },\n            {\n                \"params\": [param_dict[pn] for pn in sorted(list(no_decay))],\n                \"weight_decay\": 0.0,\n            },\n        ]\n\n        optimizer = torch.optim.AdamW(\n            optim_groups, lr=learning_rate, betas=(beta1, beta2)\n        )\n        return optimizer\n\n    def train(self, dataloader: torch.utils.data.DataLoader, epochs: int):\n        for epoch in range(epochs):\n\n            for index, imgs in enumerate(dataloader):\n                self.optim.zero_grad()\n                imgs = imgs.to(device=self.device)\n                logits, targets = self.model(imgs)\n                loss = F.cross_entropy(\n                    logits.reshape(-1, logits.size(-1)), targets.reshape(-1)\n                )\n                loss.backward()\n                self.optim.step()\n\n                self.run.track(\n                    loss,\n                    name=\"Cross Entropy Loss\",\n                    step=index,\n                    context={\"stage\": \"transformer\"},\n                )\n\n                if index % 10 == 0:\n                    print(\n                        f\"Epoch: {epoch+1}/{epochs} | Batch: {index}/{len(dataloader)} | Cross Entropy Loss : {loss:.4f}\"\n                    )\n\n                    _, sampled_imgs = self.model.log_images(imgs[0][None])\n\n                    self.run.track(\n                        Image(\n                            torchvision.utils.make_grid(sampled_imgs)\n                            .mul(255)\n                            .add_(0.5)\n                            .clamp_(0, 255)\n                            .permute(1, 2, 0)\n                            .to(\"cpu\", torch.uint8)\n                            .numpy()\n                        ),\n                        name=\"Transformer Images\",\n                        step=index,\n                        context={\"stage\": \"transformer\"},\n                    )\n"}
{"type": "source_file", "path": "train.py", "content": "# Importing Libraries\nimport argparse\n\nimport yaml\nfrom aim import Run\n\nfrom dataloader import load_dataloader\nfrom trainer import Trainer\nfrom transformer import VQGANTransformer\nfrom vqgan import VQGAN\n\n\ndef main(args, config):\n\n    vqgan = VQGAN(**config[\"architecture\"][\"vqgan\"])\n    transformer = VQGANTransformer(\n        vqgan, **config[\"architecture\"][\"transformer\"], device=args.device\n    )\n    dataloader = load_dataloader(name=args.dataset_name)\n\n    run = Run(experiment=args.dataset_name)\n    run[\"hparams\"] = config\n\n    trainer = Trainer(\n        vqgan,\n        transformer,\n        run=run,\n        config=config[\"trainer\"],\n        seed=args.seed,\n        device=args.device,\n    )\n\n    trainer.train_vqgan(dataloader)\n    trainer.train_transformers(dataloader)\n    trainer.generate_images()\n\n\nif __name__ == \"__main__\":\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--config_path\",\n        type=str,\n        default=\"configs/default.yml\",\n        help=\"path to config file\",\n    )\n    parser.add_argument(\n        \"--dataset_name\",\n        type=str,\n        choices=[\"mnist\", \"cifar\", \"custom\"],\n        default=\"mnist\",\n        help=\"Dataset for the model\",\n    )\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        default=\"cuda\",\n        choices=[\"cpu\", \"cuda\"],\n        help=\"Device to train the model on\",\n    )\n    parser.add_argument(\n        \"--seed\",\n        type=str,\n        default=42,\n        help=\"Seed for Reproducibility\",\n    )\n\n    args = parser.parse_args()\n\n    args = parser.parse_args()\n    with open(args.config_path) as f:\n        config = yaml.load(f, Loader=yaml.FullLoader)\n\n    main(args, config)\n"}
{"type": "source_file", "path": "transformer/mingpt.py", "content": "\"\"\"\nfrom : https://github.com/dome272/VQGAN-pytorch/blob/main/mingpt.py\n    which is taken from: https://github.com/karpathy/minGPT/\nGPT model:\n- the initial stem consists of a combination of token encoding and a positional encoding\n- the meat of it is a uniform sequence of Transformer blocks\n    - each Transformer is a sequential combination of a 1-hidden-layer MLP block and a self-attention block\n    - all blocks feed into a central residual pathway similar to resnets\n- the final decoder is a linear projection into a vanilla Softmax classifier\n\"\"\"\n\nimport math\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n\nclass GPTConfig:\n    \"\"\" base GPT config, params common to all GPT versions \"\"\"\n    embd_pdrop = 0.1\n    resid_pdrop = 0.1\n    attn_pdrop = 0.1\n\n    def __init__(self, vocab_size, block_size, **kwargs):\n        self.vocab_size = vocab_size\n        self.block_size = block_size\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n\n\nclass CausalSelfAttention(nn.Module):\n    \"\"\"\n    A vanilla multi-head masked self-attention layer with a projection at the end.\n    It is possible to use torch.nn.MultiheadAttention here but I am including an\n    explicit implementation here to show that there is nothing too scary here.\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # key, query, value projections for all heads\n        self.key = nn.Linear(config.n_embd, config.n_embd)\n        self.query = nn.Linear(config.n_embd, config.n_embd)\n        self.value = nn.Linear(config.n_embd, config.n_embd)\n        # regularization\n        self.attn_drop = nn.Dropout(config.attn_pdrop)\n        self.resid_drop = nn.Dropout(config.resid_pdrop)\n        # output projection\n        self.proj = nn.Linear(config.n_embd, config.n_embd)\n        # causal mask to ensure that attention is only applied to the left in the input sequence\n        mask = torch.tril(torch.ones(config.block_size,\n                                     config.block_size))\n        if hasattr(config, \"n_unmasked\"):\n            mask[:config.n_unmasked, :config.n_unmasked] = 1\n        self.register_buffer(\"mask\", mask.view(1, 1, config.block_size, config.block_size))\n        self.n_head = config.n_head\n\n    def forward(self, x, layer_past=None):\n        B, T, C = x.size()\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n\n        present = torch.stack((k, v))\n        if layer_past is not None:\n            past_key, past_value = layer_past\n            k = torch.cat((past_key, k), dim=-2)\n            v = torch.cat((past_value, v), dim=-2)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        if layer_past is None:\n            att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n\n        att = F.softmax(att, dim=-1)\n        att = self.attn_drop(att)\n        y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.resid_drop(self.proj(y))\n        return y, present  # TODO: check that this does not break anything\n\n\nclass Block(nn.Module):\n    \"\"\" an unassuming Transformer block \"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(config.n_embd)\n        self.ln2 = nn.LayerNorm(config.n_embd)\n        self.attn = CausalSelfAttention(config)\n        self.mlp = nn.Sequential(\n            nn.Linear(config.n_embd, 4 * config.n_embd),\n            nn.GELU(),  # nice\n            nn.Linear(4 * config.n_embd, config.n_embd),\n            nn.Dropout(config.resid_pdrop),\n        )\n\n    def forward(self, x, layer_past=None, return_present=False):\n        # TODO: check that training still works\n        if return_present:\n            assert not self.training\n        # layer past: tuple of length two with B, nh, T, hs\n        attn, present = self.attn(self.ln1(x), layer_past=layer_past)\n\n        x = x + attn\n        x = x + self.mlp(self.ln2(x))\n        if layer_past is not None or return_present:\n            return x, present\n        return x\n\n\nclass GPT(nn.Module):\n    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n\n    def __init__(self, vocab_size, block_size, n_layer=12, n_head=8, n_embd=256,\n                 embd_pdrop=0., resid_pdrop=0., attn_pdrop=0., n_unmasked=0):\n        super().__init__()\n        config = GPTConfig(vocab_size=vocab_size, block_size=block_size,\n                           embd_pdrop=embd_pdrop, resid_pdrop=resid_pdrop, attn_pdrop=attn_pdrop,\n                           n_layer=n_layer, n_head=n_head, n_embd=n_embd,\n                           n_unmasked=n_unmasked)\n        # input embedding stem\n        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))  # 512 x 1024\n        self.drop = nn.Dropout(config.embd_pdrop)\n        # transformer\n        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n        # decoder head\n        self.ln_f = nn.LayerNorm(config.n_embd)\n        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n\n        self.block_size = config.block_size\n        self.apply(self._init_weights)\n        self.config = config\n\n    def get_block_size(self):\n        return self.block_size\n\n    def _init_weights(self, module):\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            module.weight.data.normal_(mean=0.0, std=0.02)\n            if isinstance(module, nn.Linear) and module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def forward(self, idx, embeddings=None):\n        token_embeddings = self.tok_emb(idx)  # each index maps to a (learnable) vector\n\n        if embeddings is not None:  # prepend explicit embeddings\n            token_embeddings = torch.cat((embeddings, token_embeddings), dim=1)\n\n        t = token_embeddings.shape[1]\n        assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n        position_embeddings = self.pos_emb[:, :t, :]  # each position maps to a (learnable) vector\n        x = self.drop(token_embeddings + position_embeddings)\n        x = self.blocks(x)\n        x = self.ln_f(x)\n        logits = self.head(x)\n\n        return logits, None\n\n\n\n\n\n"}
{"type": "source_file", "path": "utils/__init__.py", "content": "from .utils import weights_init, print_summary, generate_gif, clean_directory, reproducibility, collate_fn"}
{"type": "source_file", "path": "transformer/__init__.py", "content": "from .transformer import VQGANTransformer"}
{"type": "source_file", "path": "dataloader/load_dataloader.py", "content": "# Importing Libraries\nimport torch\n\nfrom dataloader import load_mnist, load_cifar10\n\n\ndef load_dataloader(\n    name: str = \"mnist\",\n    batch_size: int = 2,\n    image_size: int = 256,\n    num_workers: int = 4,\n    save_path: str = \"data\",\n) -> torch.utils.data.DataLoader:\n    \"\"\"Load the data loader for the given name.\n\n    Args:\n        name (str, optional): The name of the data loader. Defaults to \"mnist\".\n        batch_size (int, optional): The batch size. Defaults to 2.\n        image_size (int, optional): The image size. Defaults to 256.\n        num_workers (int, optional): The number of workers to use for the dataloader. Defaults to 4.\n        save_path (str, optional): The path to save the data to. Defaults to \"data\".\n\n    Returns:\n        torch.utils.data.DataLoader: The data loader.\n    \"\"\"\n\n    if name == \"mnist\":\n        return load_mnist(\n            batch_size=batch_size,\n            image_size=image_size,\n            num_workers=num_workers,\n            save_path=save_path,\n        )\n\n    elif name == \"cifar10\":\n        return load_cifar10(\n            batch_size=batch_size,\n            image_size=image_size,\n            num_workers=num_workers,\n            save_path=save_path,\n        )"}
{"type": "source_file", "path": "dataloader/cifar10.py", "content": "# Importing Libraries\nimport torch\nimport torchvision\n\nfrom utils import collate_fn\n\n\ndef load_cifar10(\n    batch_size: int = 16,\n    image_size: int = 28,\n    num_workers: int = 4,\n    save_path: str = \"data\",\n) -> torch.utils.data.DataLoader:\n    \"\"\"Load the Cifar 10 data and returns the dataloaders (train ). The data is downloaded if it does not exist.\n\n    Args:\n        batch_size (int): The batch size.\n        image_size (int): The image size.\n        num_workers (int): The number of workers to use for the dataloader.\n        save_path (str): The path to save the data to.\n\n    Returns:\n        torch.utils.data.DataLoader: The data loader.\n    \"\"\"\n\n    # Load the data\n    dataloader = torch.utils.data.DataLoader(\n        torchvision.datasets.CIFAR10(\n            root=save_path,\n            train=True,\n            download=True,\n            transform=torchvision.transforms.Compose(\n                [\n                    torchvision.transforms.Resize((image_size, image_size)),\n                    torchvision.transforms.ToTensor(),\n                    torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n                ]\n            ),\n        ),\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n        collate_fn=collate_fn,\n    )\n\n    return dataloader\n"}
{"type": "source_file", "path": "vqgan/__init__.py", "content": "from vqgan.encoder import Encoder\nfrom vqgan.decoder import Decoder\nfrom vqgan.codebook import CodeBook\nfrom vqgan.discriminator import Discriminator\nfrom vqgan.vqgan import VQGAN\n"}
{"type": "source_file", "path": "vqgan/codebook.py", "content": "\"\"\"\nhttps://github.com/dome272/VQGAN-pytorch/blob/main/codebook.py\n\nContains the implementation of the codebook for the VQGAN. \nWith each forward pass, it returns the loss, indices of min distance latent vectors between codebook and encoder output and latent vector with minimim distance. \n\"\"\"\n\n# Importing Libraries\nimport torch\nimport torch.nn as nn\n\n\nclass CodeBook(nn.Module):\n    \"\"\"\n    This is class, we are mostly implemented para 3.1 from the paper,\n\n    We generate the codebook from nn.Embeddings of given size and randomly initialize the weights in uniform distribution.\n\n    The `forward` method is mostly to calculates\n    1. the nearest vector in the codebook from the given latent vector by the encoder.\n    2. The index of the nearest vector in the codebook.\n    3. loss ( from eq. 4 ) ( except reconstruction loss )\n\n    Args:\n        num_codebook_vectors (int): Number of codebook vectors.\n        latent_dim (int): Latent dimension of individual vectors.\n        beta (int): Beta value for the commitment loss.\n    \"\"\"\n\n    def __init__(\n        self, num_codebook_vectors: int = 1024, latent_dim: int = 256, beta: int = 0.25\n    ):\n        super().__init__()\n\n        self.num_codebook_vectors = num_codebook_vectors\n        self.latent_dim = latent_dim\n        self.beta = beta\n\n        # creating the codebook, nn.Embedding here is simply a 2D array mainly for storing our embeddings, it's also learnable\n        self.codebook = nn.Embedding(num_codebook_vectors, latent_dim)\n\n        # Initializing the weights in codebook in uniform distribution\n        self.codebook.weight.data.uniform_(\n            -1 / num_codebook_vectors, 1 / num_codebook_vectors\n        )\n\n    def forward(self, z: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Calculates the loss and nearest vector in the codebook from the given latent vector.\n\n        We are mostly implementing the eq 2 and 4 ( except reconstruction loss ) from the paper.\n\n        Args:\n            z (torch.Tensor): Latent vector.\n        Returns:\n            torch.Tensor: Nearest vector in the codebook.\n            torch.Tensor: Index of the nearest vector in the codebook.\n            torch.Tensor: Loss ( except reconstruction loss ).\n        \"\"\"\n\n        # Channel to last dimension and copying the tensor to store it in a contiguous ( in a sequence ) way\n        z = z.permute(0, 2, 3, 1).contiguous()\n\n        z_flattened = z.view(\n            -1, self.latent_dim\n        )  # b*h*w * latent_dim, will look similar to codebook in fig 2 of the paper\n\n        # calculating the distance between the z to the vectors in flattened codebook, from eq. 2\n        # (a - b)^2 = a^2 + b^2 - 2ab\n        distance = (\n            torch.sum(\n                z_flattened**2, dim=1, keepdim=True\n            )  # keepdim = True to keep the same original shape after the sum\n            + torch.sum(self.codebook.weight**2, dim=1)\n            - 2\n            * torch.matmul(\n                z_flattened, self.codebook.weight.t()\n            )  # 2*dot(z, codebook.T)\n        )\n\n        # getting indices of vectors with minimum distance from the codebook\n        min_distance_indices = torch.argmin(distance, dim=1)\n\n        # getting the corresponding vector from the codebook\n        z_q = self.codebook(min_distance_indices).view(z.shape)\n\n        \"\"\"\n        this represent the equation 4 from the paper ( except the reconstruction loss ) . Thia loss will then be added\n        to GAN loss to create the final loss function for VQGAN, eq. 6 in the paper.\n\n\n        Note : In the first para of A. Changlog section of the paper,\n        they found a bug which resulted in beta equal to 1. here https://github.com/CompVis/taming-transformers/issues/57\n        just a note :)\n        \"\"\"\n        loss = torch.mean(\n            (z_q.detach() - z) ** 2\n            # detach() to avoid calculating gradient while backpropagating\n            + self.beta\n            * torch.mean(\n                (z_q - z.detach()) ** 2\n            )  # commitment loss, detach() to avoid calculating gradient while backpropagating\n        )\n\n        # Not sure why we need this, but it's in the original implementation and mentions for \"preserving gradients\"\n        z_q = z + (z_q - z).detach()\n\n        # reshapring to the original shape\n        z_q = z_q.permute(0, 3, 1, 2)\n\n        return z_q, min_distance_indices, loss\n"}
{"type": "source_file", "path": "trainer/vqgan.py", "content": "\"\"\"\nhttps://github.com/dome272/VQGAN-pytorch/blob/main/training_vqgan.py\n\"\"\"\n\n# Importing Libraries\nimport os\n\nimport imageio\nimport lpips\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torchvision\nfrom aim import Image, Run\nfrom utils import weights_init\nfrom vqgan import Discriminator\n\n\nclass VQGANTrainer:\n    \"\"\"Trainer class for VQGAN, contains step, train methods\"\"\"\n\n    def __init__(\n        self,\n        model: torch.nn.Module,\n        run: Run,\n        # Training parameters\n        device: str or torch.device = \"cuda\",\n        learning_rate: float = 2.25e-05,\n        beta1: float = 0.5,\n        beta2: float = 0.9,\n        # Loss parameters\n        perceptual_loss_factor: float = 1.0,\n        rec_loss_factor: float = 1.0,\n        # Discriminator parameters\n        disc_factor: float = 1.0,\n        disc_start: int = 100,\n        # Miscellaneous parameters\n        experiment_dir: str = \"./experiments\",\n        perceptual_model: str = \"vgg\",\n        save_every: int = 10,\n    ):\n\n        self.run = run\n        self.device = device\n\n        # VQGAN parameters\n        self.vqgan = model\n\n        # Discriminator parameters\n        self.discriminator = Discriminator(image_channels=self.vqgan.img_channels).to(\n            self.device\n        )\n        self.discriminator.apply(weights_init)\n\n        # Loss parameters\n        self.perceptual_loss = lpips.LPIPS(net=perceptual_model).to(self.device)\n\n        # Optimizers\n        self.opt_vq, self.opt_disc = self.configure_optimizers(\n            learning_rate=learning_rate, beta1=beta1, beta2=beta2\n        )\n\n        # Hyperprameters\n        self.disc_factor = disc_factor\n        self.disc_start = disc_start\n        self.perceptual_loss_factor = perceptual_loss_factor\n        self.rec_loss_factor = rec_loss_factor\n\n        # Save directory\n        self.expriment_save_dir = experiment_dir\n\n        # Miscellaneous\n        self.global_step = 0\n        self.sample_batch = None\n        self.gif_images = []\n        self.save_every = save_every\n\n    def configure_optimizers(\n        self, learning_rate: float = 2.25e-05, beta1: float = 0.5, beta2: float = 0.9\n    ):\n        opt_vq = torch.optim.Adam(\n            list(self.vqgan.encoder.parameters())\n            + list(self.vqgan.decoder.parameters())\n            + list(self.vqgan.codebook.parameters())\n            + list(self.vqgan.quant_conv.parameters())\n            + list(self.vqgan.post_quant_conv.parameters()),\n            lr=learning_rate,\n            eps=1e-08,\n            betas=(beta1, beta2),\n        )\n        opt_disc = torch.optim.Adam(\n            self.discriminator.parameters(),\n            lr=learning_rate,\n            eps=1e-08,\n            betas=(beta1, beta2),\n        )\n\n        return opt_vq, opt_disc\n\n    def step(self, imgs: torch.Tensor) -> torch.Tensor:\n        \"\"\"Performs a single training step from the dataloader images batch\n\n        For the VQGAN, it calculates the perceptual loss, reconstruction loss, and the codebook loss and does the backward pass.\n\n        For the discriminator, it calculates lambda for the discriminator loss and does the backward pass.\n\n        Args:\n            imgs: input tensor of shape (batch_size, channel, H, W)\n\n        Returns:\n            decoded_imgs: output tensor of shape (batch_size, channel, H, W)\n        \"\"\"\n\n        # Getting decoder output\n        decoded_images, _, q_loss = self.vqgan(imgs)\n\n        \"\"\"\n        =======================================================================================================================\n        VQ Loss\n        \"\"\"\n        perceptual_loss = self.perceptual_loss(imgs, decoded_images)\n        rec_loss = torch.abs(imgs - decoded_images)\n        perceptual_rec_loss = (\n            self.perceptual_loss_factor * perceptual_loss\n            + self.rec_loss_factor * rec_loss\n        )\n        perceptual_rec_loss = perceptual_rec_loss.mean()\n\n        \"\"\"\n        =======================================================================================================================\n        Discriminator Loss\n        \"\"\"\n        disc_real = self.discriminator(imgs)\n        disc_fake = self.discriminator(decoded_images)\n\n        disc_factor = self.vqgan.adopt_weight(\n            self.disc_factor, self.global_step, threshold=self.disc_start\n        )\n\n        g_loss = -torch.mean(disc_fake)\n\n        λ = self.vqgan.calculate_lambda(perceptual_rec_loss, g_loss)\n        vq_loss = perceptual_rec_loss + q_loss + disc_factor * λ * g_loss\n\n        d_loss_real = torch.mean(F.relu(1.0 - disc_real))\n        d_loss_fake = torch.mean(F.relu(1.0 + disc_fake))\n        gan_loss = disc_factor * 0.5 * (d_loss_real + d_loss_fake)\n\n        # ======================================================================================================================\n        # Tracking metrics\n\n        self.run.track(\n            perceptual_rec_loss,\n            name=\"Perceptual & Reconstruction loss\",\n            step=self.global_step,\n            context={\"stage\": \"vqgan\"},\n        )\n\n        self.run.track(\n            vq_loss, name=\"VQ Loss\", step=self.global_step, context={\"stage\": \"vqgan\"}\n        )\n        self.run.track(\n            gan_loss, name=\"GAN Loss\", step=self.global_step, context={\"stage\": \"vqgan\"}\n        )\n\n        # =======================================================================================================================\n        # Backpropagation\n\n        self.opt_vq.zero_grad()\n        vq_loss.backward(\n            retain_graph=True\n        )  # retain_graph is used to retain the computation graph for the discriminator loss\n\n        self.opt_disc.zero_grad()\n        gan_loss.backward()\n\n        self.opt_vq.step()\n        self.opt_disc.step()\n\n        return decoded_images, vq_loss, gan_loss\n\n    def train(\n        self,\n        dataloader: torch.utils.data.DataLoader,\n        epochs: int = 1,\n    ):\n        \"\"\"Trains the VQGAN for the given number of epochs\n\n        Args:\n            dataloader (torch.utils.data.DataLoader): dataloader to use.\n            epochs (int, optional): number of epochs to train for. Defaults to 100.\n        \"\"\"\n\n        for epoch in range(epochs):\n            for index, imgs in enumerate(dataloader):\n\n                # Training step\n                imgs = imgs.to(self.device)\n\n                decoded_images, vq_loss, gan_loss = self.step(imgs)\n\n                # Updating global step\n                self.global_step += 1\n\n                if index % self.save_every == 0:\n\n                    print(\n                        f\"Epoch: {epoch+1}/{epochs} | Batch: {index}/{len(dataloader)} | VQ Loss : {vq_loss:.4f} | Discriminator Loss: {gan_loss:.4f}\"\n                    )\n\n                    # Only saving the gif for the first 2000 save steps\n                    if self.global_step // self.save_every <= 2000:\n                        self.sample_batch = (\n                            imgs[:] if self.sample_batch is None else self.sample_batch\n                        )\n\n                        with torch.no_grad():\n                            \n                            \"\"\"\n                            Note : Lots of efficiency & cleaning needed here\n                            \"\"\"\n\n                            gif_img = (\n                                torchvision.utils.make_grid(\n                                    torch.cat(\n                                        (\n                                            self.sample_batch,\n                                            self.vqgan(self.sample_batch)[0],\n                                        ),\n                                    )\n                                )\n                                .detach()\n                                .cpu()\n                                .permute(1, 2, 0)\n                                .numpy()\n                            )\n\n                            gif_img = (gif_img - gif_img.min()) * (\n                                255 / (gif_img.max() - gif_img.min())\n                            )\n                            gif_img = gif_img.astype(np.uint8)\n\n                            self.run.track(\n                                Image(\n                                    torchvision.utils.make_grid(\n                                        torch.cat(\n                                            (\n                                                imgs,\n                                                decoded_images,\n                                            ),\n                                        )\n                                    ).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to('cpu', torch.uint8).numpy()\n                                ),\n                                name=\"VQGAN Reconstruction\",\n                                step=self.global_step,\n                                context={\"stage\": \"vqgan\"},\n                            )\n\n                            self.gif_images.append(gif_img)\n\n                        imageio.mimsave(\n                            os.path.join(self.expriment_save_dir, \"reconstruction.gif\"),\n                            self.gif_images,\n                            fps=5,\n                        )\n"}
{"type": "source_file", "path": "generate.py", "content": "# Importing Libraries\nimport argparse\n\nimport yaml\n\nfrom trainer import Trainer\nfrom transformer import VQGANTransformer\nfrom vqgan import VQGAN\n\n\ndef main(args, config):\n\n    vqgan = VQGAN(**config[\"architecture\"][\"vqgan\"])\n    vqgan.load_checkpoint(\"./experiments/checkpoints/vqgan.pt\")\n\n    transformer = VQGANTransformer(\n        vqgan, device=args.device, **config[\"architecture\"][\"transformer\"]\n    )\n    transformer.load_checkpoint(\"./experiments/checkpoints/transformer.pt\")\n    \n    trainer = Trainer(\n        vqgan,\n        transformer,\n        run=None,\n        config=config[\"trainer\"],\n        seed=args.seed,\n        device=args.device,\n    )\n\n    trainer.generate_images()\n\n\nif __name__ == \"__main__\":\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--config_path\",\n        type=str,\n        default=\"configs/default.yml\",\n        help=\"path to config file\",\n    )\n    parser.add_argument(\n        \"--dataset_name\",\n        type=str,\n        choices=[\"mnist\", \"cifar\", \"custom\"],\n        default=\"mnist\",\n        help=\"Dataset for the model\",\n    )\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        default=\"cuda\",\n        choices=[\"cpu\", \"cuda\"],\n        help=\"Device to train the model on\",\n    )\n    parser.add_argument(\n        \"--seed\",\n        type=str,\n        default=42,\n        help=\"Seed for Reproducibility\",\n    )\n\n    args = parser.parse_args()\n\n    args = parser.parse_args()\n    with open(args.config_path) as f:\n        config = yaml.load(f, Loader=yaml.FullLoader)\n\n    main(args, config)\n"}
{"type": "source_file", "path": "dataloader/__init__.py", "content": "from .mnist import load_mnist\nfrom .cifar10 import load_cifar10\nfrom .load_dataloader import load_dataloader"}
{"type": "source_file", "path": "vqgan/discriminator.py", "content": "\"\"\"\nPatchGAN Discriminator (https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py#L538)\n\n\nThis isn't a standward GAN discriminator, where the input is a batch of images and the output is a batch of real/fake labels.\n\n\nBut instead, PatchGAN discriminator is a network that takes a batch of images\nsplit into multiple patches \n\nfor ex. - 30-30 patches, 30 in x and 30 in y axis, similar to convolution kernels, \nand then runs them through a network to get a score of real/fake on those individual patches. \n\nex. - input size (1, 3, 256, 256) -> output size (1, 1, 30, 30)\n\n\"\"\"\n\nimport torch.nn as nn\n\n\nclass Discriminator(nn.Module):\n    \"\"\"  PatchGAN Discriminator\n\n\n    Args:\n        image_channels (int): Number of channels in the input image.\n        num_filters_last (int): Number of filters in the last layer of the discriminator.\n        n_layers (int): Number of layers in the discriminator.\n\n    \n    \"\"\"\n    \n    def __init__(self, image_channels:int=3, num_filters_last=64, n_layers=3):\n        super(Discriminator, self).__init__()\n\n        layers = [\n            nn.Conv2d(image_channels, num_filters_last, 4, 2, 1),\n            nn.LeakyReLU(0.2),\n        ]\n        num_filters_mult = 1\n\n        for i in range(1, n_layers + 1):\n            num_filters_mult_last = num_filters_mult\n            num_filters_mult = min(2**i, 8)\n            layers += [\n                nn.Conv2d(\n                    num_filters_last * num_filters_mult_last,\n                    num_filters_last * num_filters_mult,\n                    4,\n                    2 if i < n_layers else 1,\n                    1,\n                    bias=False,\n                ),\n                nn.BatchNorm2d(num_filters_last * num_filters_mult),\n                nn.LeakyReLU(0.2, True),\n            ]\n\n        layers.append(nn.Conv2d(num_filters_last * num_filters_mult, 1, 4, 1, 1))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)"}
{"type": "source_file", "path": "vqgan/common.py", "content": "\"\"\"\nhttps://github.com/dome272/VQGAN-pytorch/blob/main/helper.py\n\nThe file contains Swish, Group Norm, Residual & Non-Local Blocks,  Upsample and Downsample layer for VQGAN encoder and decoder blocks.\n\"\"\"\n\n# Importing Libraries\nimport torch\nimport torch.nn as nn\n\n\nclass Swish(nn.Module):\n    \"\"\"Swish activation function first introduced in the paper https://arxiv.org/abs/1710.05941v2\n    It has shown to be working better in many datasets as compares to ReLU.\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\n        return x * torch.sigmoid(x)\n\n\nclass GroupNorm(nn.Module):\n    \"\"\"Group Normalization is a method which normalizes the activation of the layer for better results across any batch size.\n    Note : Weight Standardization is also shown to given better results when added with group norm\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n    \"\"\"\n\n    def __init__(self, in_channels: int) -> None:\n        super().__init__()\n\n        # num_groups is according to the official code provided by the authors,\n        # eps is for numerical stability\n        # i think affine here is enabling learnable param for affine trasnform on calculated mean & standard deviation\n        self.group_norm = nn.GroupNorm(\n            num_groups=32, num_channels=in_channels, eps=1e-06, affine=True\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.group_norm(x)\n\n\nclass ResidualBlock(nn.Module):\n    \"\"\"Residual Block from the paper,\n    group norm -> swish -> conv -> group norm -> swish -> conv -> dropout -> conv -> skip connection\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n        out_channels (int): Number of channels in the output tensor.\n        dropout (float): Dropout probability.\n    \"\"\"\n\n    def __init__(self, in_channels:int, out_channels:int, dropout:float=0.0) -> None:\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n        self.block = nn.Sequential(\n            GroupNorm(in_channels),\n            Swish(),\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n            GroupNorm(out_channels),\n            Swish(),\n            nn.Dropout(dropout),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n        )\n\n        \"\"\"\n        In some cases, the shortcut connection needs to be added\n        to match the dimension of the input and the output for skip connection\n        \"\"\"\n        if in_channels != out_channels:\n            self.conv_shortcut = nn.Conv2d(\n                in_channels, out_channels, kernel_size=1, stride=1, padding=0\n            )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\n        # shortcut connection\n        if self.in_channels != self.out_channels:\n            return self.conv_shortcut(x) + self.block(x)\n        else:\n            return x + self.block(x)\n\n\nclass DownsampleBlock(nn.Module):\n    \"\"\"\n    Down sample block for the encoder. pad -> conv\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n    \"\"\"\n\n    def __init__(self, in_channels:int) -> None:\n        super().__init__()\n\n        # (0, 1, 0, 1) - pad on left, right, top, bottom, with respective size\n        self.pad = nn.ConstantPad2d((0, 1, 0, 1), value=0)  # and fill value of 0\n\n        self.conv = nn.Conv2d(\n            in_channels, in_channels, kernel_size=3, stride=2, padding=0\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\n        x = self.pad(x)\n\n        return self.conv(x)\n\n\nclass UpsampleBlock(nn.Module):\n    \"\"\"\n    Upsample block for the decoder. interpolate -> conv \n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n    \"\"\"\n\n    def __init__(self, in_channels:int) -> None:\n        super().__init__()\n\n        self.conv = nn.Conv2d(\n            in_channels, in_channels, kernel_size=3, stride=1, padding=1\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\n        x = torch.nn.functional.interpolate(x, scale_factor=2.0)\n\n        return self.conv(x)\n\n\nclass NonLocalBlock(nn.Module):\n    \"\"\"Attention mechanism similar to transformers but for CNNs, paper https://arxiv.org/abs/1805.08318\n\n    Args:\n        in_channels (int): Number of channels in the input tensor.\n    \"\"\"\n\n    def __init__(self, in_channels:int) -> None:\n        super().__init__()\n\n        self.in_channels = in_channels\n\n        # normalization layer\n        self.norm = GroupNorm(in_channels)\n\n        # query, key and value layers\n        self.q = nn.Conv2d(in_channels, in_channels, 1, 1, 0)\n        self.k = nn.Conv2d(in_channels, in_channels, 1, 1, 0)\n        self.v = nn.Conv2d(in_channels, in_channels, 1, 1, 0)\n\n        self.project_out = nn.Conv2d(in_channels, in_channels, 1, 1, 0)\n\n        self.softmax = nn.Softmax(dim=2)\n\n    def forward(self, x):\n\n        batch, _, height, width = x.size()\n\n        x = self.norm(x)\n\n        # query, key and value layers\n        q = self.q(x)\n        k = self.k(x)\n        v = self.v(x)\n\n        # resizing the output from 4D to 3D to generate attention map\n        q = q.reshape(batch, self.in_channels, height * width)\n        k = k.reshape(batch, self.in_channels, height * width)\n        v = v.reshape(batch, self.in_channels, height * width)\n\n        # transpose the query tensor for dot product\n        q = q.permute(0, 2, 1)\n\n        # main attention formula\n        scores = torch.bmm(q, k) * (self.in_channels**-0.5)\n        weights = self.softmax(scores)\n        weights = weights.permute(0, 2, 1)\n\n        attention = torch.bmm(v, weights)\n\n        # resizing the output from 3D to 4D to match the input\n        attention = attention.reshape(batch, self.in_channels, height, width)\n        attention = self.project_out(attention)\n\n        # adding the identity to the output\n        return x + attention\n"}
{"type": "source_file", "path": "dataloader/mnist.py", "content": "# Importing Libraries\nimport torch\nimport torchvision\n\nfrom utils import collate_fn\n\n\ndef load_mnist(\n    batch_size: int = 2,\n    image_size: int = 256,\n    num_workers: int = 4,\n    save_path: str = \"data\",\n) -> torch.utils.data.DataLoader:\n    \"\"\"Load the MNIST data and returns the dataloaders (train ). The data is downloaded if it does not exist.\n\n    Args:\n        batch_size (int): The batch size.\n        image_size (int): The image size.\n        num_workers (int): The number of workers to use for the dataloader.\n        save_path (str): The path to save the data to.\n\n    Returns:\n        torch.utils.data.DataLoader: The data loader.\n    \"\"\"\n\n    # Load the data\n    mnist_data = torchvision.datasets.MNIST(\n        root=save_path,\n        train=True,\n        download=True,\n        transform=torchvision.transforms.Compose(\n            [\n                torchvision.transforms.Resize((image_size, image_size)),\n                torchvision.transforms.Grayscale(num_output_channels=1),\n                torchvision.transforms.ToTensor(),\n                torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n            ]\n        ),\n    )\n\n    # Reduced set for faster training\n    mnist_data_reduced = torch.utils.data.Subset(mnist_data, list(range(0, 800)))\n\n    dataloader = torch.utils.data.DataLoader(\n        mnist_data_reduced,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n        collate_fn=collate_fn,\n    )\n\n    return dataloader\n"}
{"type": "source_file", "path": "utils/utils.py", "content": "# Importing Libraries\nimport glob\nimport os\nimport random\nimport shutil\n\nimport imageio\nimport numpy as np\nimport torch\nfrom torchsummary import summary\n\n\ndef print_summary(\n    model: torch.nn.Module,\n    input_data: torch.Tensor,\n    col_names: list = [\"input_size\", \"output_size\", \"num_params\"],\n    device: str = \"cpu\",\n    depth: int = 2,\n):\n    \"\"\"\n    Prints a summary of the model.\n    \"\"\"\n    return summary(\n        model, input_data=input_data, col_names=col_names, device=device, depth=depth\n    )\n\n\ndef weights_init(m):\n    \"\"\"Setting up the weights for the discriminator model.\n    This is mentioned in the original PatchGAN paper, in page 16, section 6.2 - Training Details\n\n    ```\n    All networks were trained from scratch. Weights were initialized from a Gaussian distribution with mean 0 and\n    standard deviation 0.02.\n    ```\n\n    Image-to-Image Translation with Conditional Adversarial Network - https://arxiv.org/pdf/1611.07004v3.pdf\n\n    Args:\n        m\n    \"\"\"\n\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0)\n\n\ndef generate_gif(imgs_path: str, save_path: str):\n    \"\"\"Generates a gif from a directory of images.\n\n    Args:\n        imgs_path: Path to the directory of images.\n        save_path: Path to save the gif.\n    \"\"\"\n\n    with imageio.get_writer(save_path, mode=\"I\") as writer:\n        for filename in glob.glob(imgs_path + \"/*.jpg\"):\n            image = imageio.imread(filename)\n            writer.append_data(image)\n\n\ndef clean_directory(directory: str):\n    \"\"\"Cleans a directory.\n    Args:\n        directory: Path to the directory.\n    \"\"\"\n\n    if os.path.exists(directory):\n        shutil.rmtree(directory)\n    os.mkdir(directory)\n\n\ndef reproducibility(seed: int = 42):\n    \"\"\"Set the random seed.\n\n    Args:\n        seed (int): The seed to use.\n\n    Returns:\n        None\n    \"\"\"\n\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n\n    np.random.seed(seed)\n    random.seed(seed)\n\n\ndef collate_fn(batch):\n    \"\"\"\n    Collate function for the dataloader like mnist or cifar10.\n    \"\"\"\n\n    imgs = torch.stack([img[0] for img in batch])\n\n    return imgs\n"}
{"type": "source_file", "path": "vqgan/decoder.py", "content": "\"\"\"\nhttps://github.com/dome272/VQGAN-pytorch/blob/main/decoder.py\n\nContains the decoder implementation of VQGAN.\n\nThe decoder architecture is also highly inspired by the - Denoising Diffusion Probabilistic Models - https://arxiv.org/abs/2006.11239\nAccording to the official implementation.  \n\"\"\"\n\n# Importing Libraries\nimport torch\nimport torch.nn as nn\n\nfrom vqgan.common import GroupNorm, NonLocalBlock, ResidualBlock, Swish, UpsampleBlock\n\n\nclass Decoder(nn.Module):\n    \"\"\"\n    The decoder part of the VQGAN.\n\n    The implementation is similar to the encoder but inverse, to produce an image from a latent vector.\n\n    Args:\n        img_channels (int): Number of channels in the output image.\n        latent_channels (int): Number of channels in the latent vector.\n        latent_size (int): Size of the latent vector.\n        intermediate_channels (list): List of channels in the intermediate layers.\n        num_residual_blocks (int): Number of residual blocks b/w each downsample block.\n        dropout (float): Dropout probability for residual blocks.\n        attention_resolution (list): tensor size ( height or width ) at which to add attention blocks\n    \"\"\"\n\n    def __init__(\n        self,\n        img_channels: int = 3,\n        latent_channels: int = 256,\n        latent_size: int = 16,\n        intermediate_channels: list = [128, 128, 256, 256, 512],\n        num_residual_blocks: int = 3,\n        dropout: float = 0.0,\n        attention_resolution: list = [16],\n    ):\n        super().__init__()\n\n        # Reverse the list to get the correct order of decoder layer channels\n        intermediate_channels = intermediate_channels[::-1]\n\n        # Appends all the layers to this list\n        layers = []\n\n        # Adding the first conv layer to increase the input channels to the first intermediate channels\n        in_channels = intermediate_channels[0]\n        layers.extend(\n            [\n                nn.Conv2d(\n                    latent_channels,\n                    intermediate_channels[0],\n                    kernel_size=3,\n                    stride=1,\n                    padding=1,\n                ),\n                ResidualBlock(\n                    in_channels=in_channels, out_channels=in_channels, dropout=dropout\n                ),\n                NonLocalBlock(in_channels=in_channels),\n                ResidualBlock(\n                    in_channels=in_channels, out_channels=in_channels, dropout=dropout\n                ),\n            ]\n        )\n\n        # Loop over the intermediate channels\n        for n in range(len(intermediate_channels)):\n            out_channels = intermediate_channels[n]\n\n            # adding the residual blocks\n            for _ in range(num_residual_blocks):\n                layers.append(ResidualBlock(in_channels, out_channels, dropout=dropout))\n                in_channels = out_channels\n\n                # adding the non local block\n                if latent_size in attention_resolution:\n                    layers.append(NonLocalBlock(in_channels))\n\n            # Due to conv in first layer, do not upsample\n            if n != 0:\n                layers.append(UpsampleBlock(in_channels=in_channels))\n                latent_size = latent_size * 2  # Upsample by a factor of 2\n\n        # Adding rest of the layers\n        layers.extend(\n            [\n                GroupNorm(in_channels=in_channels),\n                Swish(),\n                nn.Conv2d(\n                    in_channels, img_channels, kernel_size=3, stride=1, padding=1\n                ),\n            ]\n        )\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.model(x)\n"}
{"type": "source_file", "path": "vqgan/vqgan.py", "content": "\"\"\"\nhttps://github.com/dome272/VQGAN-pytorch/blob/main/vqgan.py\n\nImplementing the main VQGAN, containing forward pass, lambda calculation, and to \"enable\" discriminator loss after a certain number of global steps.\n\"\"\"\n\n# Importing Libraries\nimport torch\nimport torch.nn as nn\n\nfrom vqgan import Encoder\nfrom vqgan import Decoder\nfrom vqgan import CodeBook\n\n\nclass VQGAN(nn.Module):\n    \"\"\"\n    VQGAN class\n\n    Args:\n        img_channels (int, optional): Number of channels in the input image. Defaults to 3.\n        img_size (int, optional): Size of the input image. Defaults to 256.\n        latent_channels (int, optional): Number of channels in the latent vector. Defaults to 256.\n        latent_size (int, optional): Size of the latent vector. Defaults to 16.\n        intermediate_channels (list, optional): List of channels in the intermediate layers of encoder and decoder. Defaults to [128, 128, 256, 256, 512].\n        num_residual_blocks_encoder (int, optional): Number of residual blocks in the encoder. Defaults to 2.\n        num_residual_blocks_decoder (int, optional): Number of residual blocks in the decoder. Defaults to 3.\n        dropout (float, optional): Dropout probability. Defaults to 0.0.\n        attention_resolution (list, optional): Resolution of the attention mechanism. Defaults to [16].\n        num_codebook_vectors (int, optional): Number of codebook vectors. Defaults to 1024.\n    \"\"\"\n\n    def __init__(\n        self,\n        img_channels: int = 3,\n        img_size: int = 256,\n        latent_channels: int = 256,\n        latent_size: int = 16,\n        intermediate_channels: list = [128, 128, 256, 256, 512],\n        num_residual_blocks_encoder: int = 2,\n        num_residual_blocks_decoder: int = 3,\n        dropout: float = 0.0,\n        attention_resolution: list = [16],\n        num_codebook_vectors: int = 1024,\n    ):\n\n        super().__init__()\n        \n        self.img_channels = img_channels\n        self.num_codebook_vectors = num_codebook_vectors\n\n        self.encoder = Encoder(\n            img_channels=img_channels,\n            image_size=img_size,\n            latent_channels=latent_channels,\n            intermediate_channels=intermediate_channels[:], # shallow copy of the link\n            num_residual_blocks=num_residual_blocks_encoder,\n            dropout=dropout,\n            attention_resolution=attention_resolution,\n        )\n\n        self.decoder = Decoder(\n            img_channels=img_channels,\n            latent_channels=latent_channels,\n            latent_size=latent_size,\n            intermediate_channels=intermediate_channels[:], # shallow copy of the link\n            num_residual_blocks=num_residual_blocks_decoder,\n            dropout=dropout,\n            attention_resolution=attention_resolution,\n        )\n        self.codebook = CodeBook(\n            num_codebook_vectors=num_codebook_vectors, latent_dim=latent_channels\n        )\n\n        self.quant_conv = nn.Conv2d(latent_channels, latent_channels, 1)\n        self.post_quant_conv = nn.Conv2d(latent_channels, latent_channels, 1)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Performs a single step of training on the input tensor x\n\n        Args:\n            x (torch.Tensor): Input tensor to the encoder.\n\n        Returns:\n            torch.Tensor: Output tensor from the decoder.\n        \"\"\"\n\n        encoded_images = self.encoder(x)\n        quant_x = self.quant_conv(encoded_images)\n\n        codebook_mapping, codebook_indices, codebook_loss = self.codebook(quant_x)\n\n        post_quant_x = self.post_quant_conv(codebook_mapping)\n        decoded_images = self.decoder(post_quant_x)\n\n        return decoded_images, codebook_indices, codebook_loss\n\n    def encode(self, x: torch.Tensor) -> torch.Tensor:\n\n        x = self.encoder(x)\n        quant_x = self.quant_conv(x)\n\n        codebook_mapping, codebook_indices, q_loss = self.codebook(quant_x)\n\n        return codebook_mapping, codebook_indices, q_loss\n\n    def decode(self, x: torch.Tensor) -> torch.Tensor:\n\n        x = self.post_quant_conv(x)\n        x = self.decoder(x)\n\n        return x\n\n    def calculate_lambda(self, perceptual_loss, gan_loss):\n        \"\"\"Calculating lambda shown in the eq. 7 of the paper\n\n        Args:\n            perceptual_loss (torch.Tensor): Perceptual reconstruction loss.\n            gan_loss (torch.Tensor): loss from the GAN discriminator.\n        \"\"\"\n\n        last_layer = self.decoder.model[-1]\n        last_layer_weight = last_layer.weight\n\n        # Because we have multiple loss functions in the networks, retain graph helps to keep the computational graph for backpropagation\n        # https://stackoverflow.com/a/47174709\n        perceptual_loss_grads = torch.autograd.grad(\n            perceptual_loss, last_layer_weight, retain_graph=True\n        )[0]\n        gan_loss_grads = torch.autograd.grad(\n            gan_loss, last_layer_weight, retain_graph=True\n        )[0]\n\n        lmda = torch.norm(perceptual_loss_grads) / (torch.norm(gan_loss_grads) + 1e-4)\n        lmda = torch.clamp(\n            lmda, 0, 1e4\n        ).detach()  # Here, we are constraining the value of lambda between 0 and 1e4,\n\n        return 0.8 * lmda  # Note: not sure why we are multiplying it by 0.8... ?\n\n    @staticmethod\n    def adopt_weight(\n        disc_factor: float, i: int, threshold: int, value: float = 0.0\n    ) -> float:\n        \"\"\"Starting the discrimator later in training, so that our model has enough time to generate \"good-enough\" images to try to \"fool the discrimator\".\n\n        To do that, we before eaching a certain global step, set the discriminator factor by `value` ( default 0.0 ) .\n        This discriminator factor is then used to multiply the discriminator's loss.\n\n        Args:\n            disc_factor (float): This value is multiple to the discriminator's loss.\n            i (int): The current global step\n            threshold (int): The global step after which the `disc_factor` value is retured.\n            value (float, optional): The value of discriminator factor before the threshold is reached. Defaults to 0.0.\n\n        Returns:\n            float: The discriminator factor.\n        \"\"\"\n\n        if i < threshold:\n            disc_factor = value\n\n        return disc_factor\n\n    def load_checkpoint(self, path):\n        \"\"\"Loads the checkpoint from the given path.\"\"\"\n\n        self.load_state_dict(torch.load(path))\n\n    def save_checkpoint(self, path):\n        \"\"\"Saves the checkpoint to the given path.\"\"\"\n\n        torch.save(self.state_dict(), path)\n"}
{"type": "source_file", "path": "vqgan/encoder.py", "content": "\"\"\"\nhttps://github.com/dome272/VQGAN-pytorch/blob/main/encoder.py\n\nContains the encoder implementation of VQGAN. \n\nThe encoder is highly inspired by the - Denoising Diffusion Probabilistic Models - https://arxiv.org/abs/2006.11239\nAccording to the official implementation. \n\"\"\"\n\n# Importing Libraries\nimport torch\nimport torch.nn as nn\n\nfrom vqgan.common import DownsampleBlock, GroupNorm, NonLocalBlock, ResidualBlock, Swish\n\n\nclass Encoder(nn.Module):\n    \"\"\"\n    The encoder part of the VQGAN.\n\n    Args:\n        img_channels (int): Number of channels in the input image.\n        image_size (int): Size of the input image, only used in encoder (height or width ).\n        latent_channels (int): Number of channels in the latent vector.\n        intermediate_channels (list): List of channels in the intermediate layers.\n        num_residual_blocks (int): Number of residual blocks b/w each downsample block.\n        dropout (float): Dropout probability for residual blocks.\n        attention_resolution (list): tensor size ( height or width ) at which to add attention blocks\n    \"\"\"\n\n    def __init__(\n        self,\n        img_channels: int = 3,\n        image_size: int = 256,\n        latent_channels: int = 256,\n        intermediate_channels: list = [128, 128, 256, 256, 512],\n        num_residual_blocks: int = 2,\n        dropout: float = 0.0,\n        attention_resolution: list = [16],\n    ):\n        super().__init__()\n\n        # Inserting first intermediate channel to index 0\n        intermediate_channels.insert(0, intermediate_channels[0])\n\n        # Appends all the layers to this list\n        layers = []\n\n        # Addingt the first conv layer increase input channels to the first intermediate channels\n        layers.append(\n            nn.Conv2d(\n                img_channels,\n                intermediate_channels[0],\n                kernel_size=3,\n                stride=1,\n                padding=1,\n            )\n        )\n\n        # Loop over the intermediate channels except the last one\n        for n in range(len(intermediate_channels) - 1):\n            in_channels = intermediate_channels[n]\n            out_channels = intermediate_channels[n + 1]\n\n            # Adding the residual blocks for each channel\n            for _ in range(num_residual_blocks):\n                layers.append(ResidualBlock(in_channels, out_channels, dropout=dropout))\n                in_channels = out_channels\n\n                # Once we have downsampled the image to the size in attention resolution, we add attention blocks\n                if image_size in attention_resolution:\n                    layers.append(NonLocalBlock(in_channels))\n\n            # only downsample for the first n-2 layers, and decrease the input size by a factor of 2\n            if n != len(intermediate_channels) - 2:\n                layers.append(DownsampleBlock(in_channels=intermediate_channels[n + 1]))\n                image_size = image_size // 2  # Downsample by a factor of 2\n\n        in_channels = intermediate_channels[-1]\n        layers.extend(\n            [\n                ResidualBlock(\n                    in_channels=in_channels, out_channels=in_channels, dropout=dropout\n                ),\n                NonLocalBlock(in_channels=in_channels),\n                ResidualBlock(\n                    in_channels=in_channels, out_channels=in_channels, dropout=dropout\n                ),\n                GroupNorm(in_channels=in_channels),\n                Swish(),\n                # increase the channels upto the latent vector channels\n                nn.Conv2d(\n                    in_channels, latent_channels, kernel_size=3, stride=1, padding=1\n                ),\n            ]\n        )\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.model(x)\n"}
{"type": "source_file", "path": "transformer/transformer.py", "content": "\"\"\"\nhttps://github.com/dome272/VQGAN-pytorch/blob/main/transformer.py\n\"\"\"\n\n# Importing Libraries\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformer.mingpt import GPT\n\n\nclass VQGANTransformer(nn.Module):\n    def __init__(\n        self,\n        vqgan: nn.Module,\n        device: str = \"cuda\",\n        sos_token: int = 0,\n        pkeep: float = 0.5,\n        block_size: int = 512,\n        n_layer: int = 12,\n        n_head: int = 16,\n        n_embd: int = 1024,\n    ):\n        super().__init__()\n\n        self.sos_token = sos_token\n        self.device = device\n\n        self.vqgan = vqgan\n\n        self.transformer = GPT(\n            vocab_size=self.vqgan.num_codebook_vectors,\n            block_size=block_size,\n            n_layer=n_layer,\n            n_head=n_head,\n            n_embd=n_embd,\n        )\n\n        self.pkeep = pkeep\n\n    @torch.no_grad()\n    def encode_to_z(self, x: torch.tensor) -> torch.tensor:\n        \"\"\"Processes the input batch ( containing images ) to encoder and returning flattened quantized encodings\n\n        Args:\n            x (torch.tensor): the input batch b*c*h*w\n\n        Returns:\n            torch.tensor: the flattened quantized encodings\n        \"\"\"\n        quant_z, indices, _ = self.vqgan.encode(x)\n        indices = indices.view(quant_z.shape[0], -1)\n        return quant_z, indices\n\n    @torch.no_grad()\n    def z_to_image(\n        self, indices: torch.tensor, p1: int = 16, p2: int = 16\n    ) -> torch.Tensor:\n        \"\"\"Returns the decoded image from the indices for the codebook embeddings\n\n        Args:\n            indices (torch.tensor): the indices of the vectors in codebook to use for generating the decoder output\n            p1 (int, optional): encoding size. Defaults to 16.\n            p2 (int, optional): encoding size. Defaults to 16.\n\n        Returns:\n            torch.tensor: generated image from decoder\n        \"\"\"\n\n        ix_to_vectors = self.vqgan.codebook.codebook(indices).reshape(\n            indices.shape[0], p1, p2, 256\n        )\n        ix_to_vectors = ix_to_vectors.permute(0, 3, 1, 2)\n        image = self.vqgan.decode(ix_to_vectors)\n        return image\n\n    def forward(self, x:torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        transformer model forward pass \n\n        Args:\n            x (torch.tensor): Batch of images\n        \"\"\"\n\n        # Getting the codebook indices of the image\n        _, indices = self.encode_to_z(x)\n\n        # sos tokens, this will be needed when we will generate new and unseen images\n        sos_tokens = torch.ones(x.shape[0], 1) * self.sos_token\n        sos_tokens = sos_tokens.long().to(self.device)\n\n        # Generating a matrix of shape indices with 1s and 0s\n        mask = torch.bernoulli(\n            self.pkeep * torch.ones(indices.shape, device=indices.device)\n        )  # torch.bernoulli([0.5 ... 0.5]) -> [1, 0, 1, 1, 0, 0] ; p(1) - 0.5\n        mask = mask.round().to(dtype=torch.int64)\n\n        # Generate a vector containing randomlly indices\n        random_indices = torch.randint_like(\n            indices, high=self.transformer.config.vocab_size\n        )  # generating indices from the distribution\n\n        \"\"\"\n        indices - [3, 56, 72, 67, 45, 53, 78, 90]\n        mask - [1, 1, 0, 0, 1, 1, 1, 0]\n        random_indices - 15, 67, 27, 89, 92, 40, 91, 10]\n\n        new_indices - [ 3, 56,  0,  0, 45, 53, 78,  0] + [ 0,  0, 27, 89,  0,  0,  0, 10] => [ 3, 56, 27, 89, 45, 53, 78, 10]\n        \"\"\"\n        new_indices = mask * indices + (1 - mask) * random_indices\n\n        # Adding sos ( start of sentence ) token\n        new_indices = torch.cat((sos_tokens, new_indices), dim=1)\n\n        target = indices\n\n        logits, _ = self.transformer(new_indices[:, :-1])\n\n        return logits, target\n\n    def top_k_logits(self, logits: torch.Tensor, k: int) -> torch.Tensor:\n        \"\"\"\n\n        Args:\n            logits (torch.Tensor): predictions from the transformer\n            k (int): returning k highest values\n\n        Returns:\n            torch.Tensor: retuning tensor of same dimension as input keeping the top k entries\n        \"\"\"\n        v, ix = torch.topk(logits, k)\n        out = logits.clone()\n        out[out < v[..., [-1]]] = -float(\n            \"inf\"\n        )  # Setting all values except in topk to inf\n        return out\n\n    @torch.no_grad()\n    def sample(\n        self,\n        x: torch.Tensor,\n        c: torch.Tensor,\n        steps: int = 256,\n        temperature: float = 1.0,\n        top_k: int = 100,\n    ) -> torch.Tensor:\n        \"\"\"Generating sample indices from the transformer\n\n        Args:\n            x (torch.Tensor): the batch of images\n            c (torch.Tensor): sos token \n            steps (int, optional): the lenght of indices to generate. Defaults to 256.\n            temperature (float, optional): hyperparameter for minGPT model. Defaults to 1.0.\n            top_k (int, optional): keeping top k entries. Defaults to 100.\n\n        Returns:\n            torch.Tensor: _description_\n        \"\"\"\n\n        self.transformer.eval()\n\n        x = torch.cat((c, x), dim=1)  # Appending sos token\n        for k in range(steps):\n            logits, _ = self.transformer(x)  # Getting the predicted indices\n            logits = (\n                logits[:, -1, :] / temperature\n            )  # Getting the last prediction and scaling it by temperature\n\n            if top_k is not None:\n                logits = self.top_k_logits(logits, top_k)\n\n            probs = F.softmax(logits, dim=-1)\n\n            ix = torch.multinomial(\n                probs, num_samples=1\n            )  # Note : not sure what's happening here\n\n            x = torch.cat((x, ix), dim=1)\n\n        x = x[:, c.shape[1] :]  # Removing the sos token\n        self.transformer.train()\n        return x\n\n    @torch.no_grad()\n    def log_images(self, x:torch.Tensor):\n        \"\"\" Generating images using the transformer and decoder. Also uses encoder to complete partial images.   \n\n        Args:\n            x (torch.Tensor): batch of images\n\n        Returns:\n            Retures the input and generated image in dictionary and in a simple concatenated image\n        \"\"\"\n        log = dict()\n\n        _, indices = self.encode_to_z(x) # Getting the indices of the quantized encoding\n        sos_tokens = torch.ones(x.shape[0], 1) * self.sos_token\n        sos_tokens = sos_tokens.long().to(self.device)\n\n        start_indices = indices[:, : indices.shape[1] // 2]\n        sample_indices = self.sample(\n            start_indices, sos_tokens, steps=indices.shape[1] - start_indices.shape[1]\n        )\n        half_sample = self.z_to_image(sample_indices)\n\n        start_indices = indices[:, :0]\n        sample_indices = self.sample(start_indices, sos_tokens, steps=indices.shape[1])\n        full_sample = self.z_to_image(sample_indices)\n\n        x_rec = self.z_to_image(indices)\n\n        log[\"input\"] = x\n        log[\"rec\"] = x_rec\n        log[\"half_sample\"] = half_sample\n        log[\"full_sample\"] = full_sample\n\n        return log, torch.concat((x, x_rec, half_sample, full_sample))\n\n    def load_checkpoint(self, path):\n        \"\"\"Loads the checkpoint from the given path.\"\"\"\n\n        self.load_state_dict(torch.load(path))\n\n    def save_checkpoint(self, path):\n        \"\"\"Saves the checkpoint to the given path.\"\"\"\n\n        torch.save(self.state_dict(), path)\n"}
