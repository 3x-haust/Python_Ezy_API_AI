{"repo_info": {"repo_name": "toploc", "repo_owner": "PrimeIntellect-ai", "repo_url": "https://github.com/PrimeIntellect-ai/toploc"}}
{"type": "test_file", "path": "tests/test_ndd.py", "content": "import pytest\nimport random\nfrom toploc.C.csrc.ndd import compute_newton_coefficients, evaluate_polynomial\n\n\n@pytest.mark.parametrize(\n    \"x, y\",\n    [\n        ([0], [42]),\n        ([12, 11, 15], [0, 0, 0]),\n        ([1, 3, 5, 2], [1, 5, 4, 1]),\n        ([1, 8, 3], [1, 2, 3]),\n        ([100, 55, 2], [1, 0, 1]),\n    ],\n)\ndef test_newton_interpolation_specific(x: list[int], y: list[int]):\n    \"\"\"Test Newton interpolation with specific known values\"\"\"\n    x = [1, 3, 5, 2]\n    y = [1, 5, 4, 1]\n\n    # Compute interpolation coefficients\n    coeffs = compute_newton_coefficients(x, y)\n\n    # Verify interpolation at each point\n    for xi, yi in zip(x, y):\n        result = evaluate_polynomial(coeffs, xi)\n        assert result == yi, (\n            f\"Interpolation failed at x={xi}, expected {yi} but got {result}\"\n        )\n\n\ndef test_newton_interpolation_random():\n    \"\"\"Test Newton interpolation with random points\"\"\"\n    # Generate random unique x values\n    x_values = random.sample(range(0, 65497), random.randint(5, 1000))\n\n    # Generate random y values\n    y_values = [random.randint(0, 2**15) for _ in range(len(x_values))]\n\n    # Compute interpolation coefficients\n    coeffs = compute_newton_coefficients(x_values, y_values)\n\n    # Verify interpolation at each point\n    for xi, yi in zip(x_values, y_values):\n        result = evaluate_polynomial(coeffs, xi)\n        assert result == yi, (\n            f\"Interpolation failed at x={xi}, expected {yi} but got {result} {x_values, y_values, coeffs}\"\n        )\n\n\ndef test_error_conditions():\n    \"\"\"Test error handling\"\"\"\n    # Test with empty lists\n    with pytest.raises(Exception):\n        compute_newton_coefficients([], [])\n\n    # Test with mismatched lengths\n    with pytest.raises(Exception):\n        compute_newton_coefficients([1, 2], [1])\n"}
{"type": "test_file", "path": "tests/test_poly.py", "content": "import pytest\nimport torch\nimport base64\nfrom toploc.poly import (\n    find_injective_modulus,\n    build_proofs_bytes,\n    build_proofs_base64,\n    ProofPoly,\n    VerificationResult,\n    verify_proofs_bytes,\n    verify_proofs_base64,\n)\n\n\ndef test_find_injective_modulus():\n    \"\"\"Test finding injective modulus\"\"\"\n    x = torch.randint(0, 4_000_000_000, (100,)).tolist()\n    modulus = find_injective_modulus(x)\n    assert isinstance(modulus, int)\n    # Check that all values are unique under modulus\n    modded = [i % modulus for i in x]\n    assert len(set(modded)) == len(x)\n\n\n@pytest.fixture\ndef sample_poly():\n    return ProofPoly([1, 2, 3, 4], 65497)\n\n\ndef test_proof_poly_init(sample_poly):\n    \"\"\"Test initialization of ProofPoly\"\"\"\n    assert sample_poly.coeffs == [1, 2, 3, 4]\n    assert sample_poly.modulus == 65497\n\n\ndef test_proof_poly_call(sample_poly):\n    \"\"\"Test polynomial evaluation\"\"\"\n    x = 42\n    result = sample_poly(x)\n    assert isinstance(result, int)\n    assert result == (1 + 2 * x + 3 * x**2 + 4 * x**3) % 65497\n\n\ndef test_proof_poly_len(sample_poly):\n    \"\"\"Test length of polynomial\"\"\"\n    assert len(sample_poly) == 4\n\n\ndef test_proof_poly_null():\n    \"\"\"Test null polynomial creation\"\"\"\n    length = 5\n    null_poly = ProofPoly.null(length)\n    assert len(null_poly) == length\n    assert null_poly.modulus == 0\n    assert null_poly.coeffs == [0] * length\n\n\ndef test_proof_poly_from_points_list():\n    \"\"\"Test creation from list points\"\"\"\n    x = [1, 2, 3]\n    y = [4, 5, 6]\n    poly = ProofPoly.from_points(x, y)\n    assert isinstance(poly, ProofPoly)\n    assert len(poly.coeffs) > 0\n\n\ndef test_proof_poly_from_points_tensor():\n    \"\"\"Test creation from tensor points\"\"\"\n    x = torch.tensor([1, 2, 3])\n    y = torch.tensor([4, 5, 6])\n    poly = ProofPoly.from_points(x, y)\n    assert isinstance(poly, ProofPoly)\n    assert len(poly.coeffs) == 3\n    assert poly(1) == 4\n    assert poly(2) == 5\n    assert poly(3) == 6\n\n\ndef test_proof_poly_from_points_bfloat16():\n    \"\"\"Test creation from bfloat16 tensor\"\"\"\n    x = torch.tensor([1, 2, 3])\n    y = torch.tensor([4, 5, 6], dtype=torch.bfloat16)\n    poly = ProofPoly.from_points(x, y)\n    assert isinstance(poly, ProofPoly)\n    assert len(poly.coeffs) == 3\n\n\ndef test_proof_poly_to_base64(sample_poly):\n    \"\"\"Test base64 encoding\"\"\"\n    encoded = sample_poly.to_base64()\n    assert isinstance(encoded, str)\n    # Verify it's valid base64\n    base64.b64decode(encoded)\n\n\ndef test_proof_poly_to_bytes(sample_poly):\n    \"\"\"Test bytes conversion\"\"\"\n    byte_data = sample_poly.to_bytes()\n    assert isinstance(byte_data, bytes)\n    assert len(byte_data) > 0\n\n\ndef test_proof_poly_from_bytes(sample_poly):\n    \"\"\"Test creation from bytes\"\"\"\n    byte_data = sample_poly.to_bytes()\n    reconstructed = ProofPoly.from_bytes(byte_data)\n    assert reconstructed.coeffs == sample_poly.coeffs\n    assert reconstructed.modulus == sample_poly.modulus\n\n\ndef test_proof_poly_from_base64(sample_poly):\n    \"\"\"Test creation from base64\"\"\"\n    encoded = sample_poly.to_base64()\n    reconstructed = ProofPoly.from_base64(encoded)\n    assert reconstructed.coeffs == sample_poly.coeffs\n    assert reconstructed.modulus == sample_poly.modulus\n\n\ndef test_proof_poly_repr(sample_poly):\n    \"\"\"Test string representation\"\"\"\n    repr_str = repr(sample_poly)\n    assert isinstance(repr_str, str)\n    assert str(65497) in repr_str\n    assert str([1, 2, 3, 4]) in repr_str\n\n\n@pytest.fixture\ndef sample_activations():\n    torch.manual_seed(42)\n    DIM = 16\n    a = [torch.randn(3, DIM, dtype=torch.bfloat16)]\n    for _ in range(3 * 2 + 1):\n        a.append(torch.randn(DIM, dtype=torch.bfloat16))\n    return a\n\n\ndef test_build_proofs(sample_activations):\n    \"\"\"Test building proofs\"\"\"\n    proofs = build_proofs_bytes(sample_activations, decode_batching_size=2, topk=5)\n    assert isinstance(proofs, list)\n    assert all(isinstance(p, bytes) for p in proofs)\n    assert len(proofs) == 5\n\n\ndef test_build_proofs_base64(sample_activations):\n    \"\"\"Test building base64 proofs\"\"\"\n    proofs = build_proofs_base64(sample_activations, decode_batching_size=2, topk=5)\n    assert isinstance(proofs, list)\n    assert all(isinstance(p, str) for p in proofs)\n    # Verify each proof is valid base64\n    for proof in proofs:\n        base64.b64decode(proof)\n    assert len(proofs) == 5\n\n\ndef test_build_proofs_skip_prefill(sample_activations):\n    \"\"\"Test building proofs with skip_prefill\"\"\"\n    proofs = build_proofs_bytes(\n        sample_activations[1:], decode_batching_size=2, topk=5, skip_prefill=True\n    )\n    assert isinstance(proofs, list)\n    assert all(isinstance(p, bytes) for p in proofs)\n    assert len(proofs) == 4\n\n    proofs = build_proofs_base64(\n        torch.randn(17, 16, dtype=torch.bfloat16),\n        decode_batching_size=4,\n        topk=5,\n        skip_prefill=True,\n    )\n    assert isinstance(proofs, list)\n    assert all(isinstance(p, str) for p in proofs)\n    assert len(proofs) == 5\n\n\ndef test_build_proofs_error_handling():\n    \"\"\"Test error handling in proof building\"\"\"\n    invalid_activations = [\n        torch.randn(0, 16, dtype=torch.bfloat16),\n        torch.randn(16, dtype=torch.bfloat16),\n    ]\n    proofs = build_proofs_bytes(invalid_activations, decode_batching_size=2, topk=5)\n    assert isinstance(proofs, list)\n    assert all(isinstance(p, bytes) for p in proofs)\n\n    nullproof = ProofPoly.null(5).to_bytes()\n    assert all(p == nullproof for p in proofs)\n\n\ndef test_build_proofs_edge_cases(sample_activations):\n    \"\"\"Test edge cases for proof building\"\"\"\n    # Test with minimal topk\n    proofs_min = build_proofs_bytes(sample_activations, decode_batching_size=2, topk=1)\n    assert len(proofs_min) > 0\n\n    # Test with large batching size\n    proofs_large_batch = build_proofs_bytes(\n        sample_activations, decode_batching_size=10, topk=5\n    )\n    assert len(proofs_large_batch) > 0\n\n    # Test with only one prefill activation\n    proofs_one = build_proofs_bytes(\n        sample_activations[:1], decode_batching_size=2, topk=5\n    )\n    assert len(proofs_one) == 1\n\n    # Test with only one activation and skip_prefill\n    proofs_one_skip = build_proofs_bytes(\n        sample_activations[1:1], decode_batching_size=2, topk=5, skip_prefill=True\n    )\n    assert len(proofs_one_skip) == 0\n\n\ndef test_verify_proofs_bytes(sample_activations):\n    \"\"\"Test verification of proofs in bytes format\"\"\"\n    # Generate proofs in bytes format\n    proofs_bytes = build_proofs_bytes(\n        sample_activations, decode_batching_size=3, topk=4\n    )\n\n    results = verify_proofs_bytes(\n        [i * 1.01 for i in sample_activations],\n        proofs_bytes,\n        decode_batching_size=3,\n        topk=4,\n    )\n\n    assert isinstance(results, list)\n    assert all(isinstance(r, VerificationResult) for r in results)\n    assert len(results) == len(proofs_bytes)\n    assert all(r.exp_mismatches == 0 for r in results)\n    assert all(r.mant_err_mean > 0 and r.mant_err_mean <= 2 for r in results)\n    assert all(r.mant_err_median > 0 and r.mant_err_median <= 2 for r in results)\n\n\ndef test_verify_proofs_base64(sample_activations):\n    \"\"\"Test verification of proofs in base64 format\"\"\"\n    # Generate proofs in base64 format\n    proofs_base64 = build_proofs_base64(\n        sample_activations, decode_batching_size=2, topk=5\n    )\n\n    results = verify_proofs_base64(\n        sample_activations, proofs_base64, decode_batching_size=2, topk=5\n    )\n\n    assert isinstance(results, list)\n    assert all(isinstance(r, VerificationResult) for r in results)\n    assert len(results) == len(proofs_base64)\n    assert all(r.exp_mismatches == 0 for r in results)\n    assert all(r.mant_err_mean == 0 for r in results)\n    assert all(r.mant_err_median == 0 for r in results)\n\n\ndef test_verify_proofs_bytes_invalid(sample_activations):\n    # Generate proofs in bytes format\n    proofs_bytes = build_proofs_bytes(\n        sample_activations, decode_batching_size=3, topk=4\n    )\n\n    results = verify_proofs_bytes(\n        [i * 1.10 for i in sample_activations],\n        proofs_bytes,\n        decode_batching_size=3,\n        topk=4,\n    )\n\n    print(results)\n    assert isinstance(results, list)\n    assert all(isinstance(r, VerificationResult) for r in results)\n    assert len(results) == len(proofs_bytes)\n    assert all(r.exp_mismatches <= 2 for r in results)\n    assert all(r.mant_err_mean > 10 for r in results)\n    assert all(r.mant_err_median > 10 for r in results)\n\n\ndef test_verify_proofs_base64_no_intersection_invalid(sample_activations):\n    \"\"\"Test verification of invalid base64 proofs\"\"\"\n    # Generate invalid proofs in base64 format\n    proofs_base64 = build_proofs_base64(\n        sample_activations, decode_batching_size=2, topk=5\n    )\n\n    results = verify_proofs_base64(\n        [i * 4 for i in sample_activations],\n        proofs_base64,\n        decode_batching_size=2,\n        topk=5,\n    )\n\n    print(results)\n    assert isinstance(results, list)\n    assert all(isinstance(r, VerificationResult) for r in results)\n    assert len(results) == len(proofs_base64)\n    assert all(r.exp_mismatches == 5 for r in results)\n    assert all(r.mant_err_mean > 2**32 for r in results)\n    assert all(r.mant_err_median > 2**32 for r in results)\n\n\ndef test_verify_proofs_bytes_skip_prefill(sample_activations):\n    \"\"\"Test verification of invalid bytes proofs with skip_prefill\"\"\"\n    # Generate invalid proofs in bytes format\n    activations = torch.randn(16, 16, dtype=torch.bfloat16)\n    proofs_bytes = build_proofs_bytes(\n        activations, decode_batching_size=3, topk=4, skip_prefill=True\n    )\n\n    assert len(proofs_bytes) == 6\n\n    results = verify_proofs_bytes(\n        activations, proofs_bytes, decode_batching_size=3, topk=4, skip_prefill=True\n    )\n\n    assert isinstance(results, list)\n    assert all(isinstance(r, VerificationResult) for r in results)\n    assert len(results) == len(proofs_bytes)\n    assert all(r.exp_mismatches == 0 for r in results)\n    assert all(r.mant_err_mean == 0 for r in results)\n    assert all(r.mant_err_median == 0 for r in results)\n"}
{"type": "test_file", "path": "tests/test_utils.py", "content": "from toploc.C.csrc.utils import get_fp_parts\nimport torch\nimport time\nimport pytest\nimport tempfile\nfrom toploc.utils import sha256sum\n\n\n@pytest.mark.parametrize(\n    \"tensor_shape\",\n    [\n        (10_000,),\n        (1000, 3),\n        (1, 3, 5, 8, 1),\n    ],\n)\ndef test_get_fp32_parts(tensor_shape: tuple[int, ...]) -> None:\n    a = torch.randn(tensor_shape)\n    start_time = time.time()\n    exps, mantissas = get_fp_parts(a)\n    new_time = time.time() - start_time\n\n    ELEMENT_SIZE = 4\n\n    def get_exp_bits(fp32_str: str) -> int:\n        return int(fp32_str[1:9], 2)\n\n    def get_mant_bits(fp32_str: str) -> int:\n        return int(fp32_str[9:], 2)\n\n    def py_get_fp_parts(tensor: torch.FloatTensor) -> tuple[list[int], list[int]]:\n        \"\"\"\n        Given a tensor of floats, return the exponent and mantissa bits of each float.\n\n        Args:\n            tensor (torch.FloatTensor): A tensor of floats.\n\n        Returns:\n            tuple[list[int], list[int]]: A tuple containing the exponent and mantissa bits of each float.\n        \"\"\"\n        temp = None\n        bit_repr = []\n        for i, e in enumerate(tensor.untyped_storage()):\n            if i % ELEMENT_SIZE == 0:\n                if temp is not None:\n                    bit_repr.append(\"\".join(temp[::-1]))\n                temp = [bin(e)[2:].zfill(8)]\n            else:\n                temp.append(bin(e)[2:].zfill(8))\n        bit_repr.append(\"\".join(temp[::-1]))\n\n        prefill_exps = [get_exp_bits(i) for i in bit_repr]\n        prefill_mants = [get_mant_bits(i) for i in bit_repr]\n        return prefill_exps, prefill_mants\n\n    start_time = time.time()\n    ref_exps, ref_mants = py_get_fp_parts(a)\n    old_time = time.time() - start_time\n\n    assert exps == ref_exps\n    assert mantissas == ref_mants\n    assert new_time < old_time\n\n\n@pytest.mark.parametrize(\n    \"tensor_shape\",\n    [\n        (10_000,),\n        (1000, 3),\n        (1, 3, 5, 8, 1),\n    ],\n)\ndef test_get_bf16_parts(tensor_shape: tuple[int, ...]) -> None:\n    a = torch.randn(tensor_shape, dtype=torch.bfloat16)\n    start_time = time.time()\n    exps, mantissas = get_fp_parts(a)\n    new_time = time.time() - start_time\n\n    ELEMENT_SIZE = 2\n\n    def get_exp_bits(fp32_str: str) -> int:\n        return int(fp32_str[1:9], 2)\n\n    def get_mant_bits(fp32_str: str) -> int:\n        return int(fp32_str[9:], 2)\n\n    def py_get_fp_parts(tensor: torch.FloatTensor) -> tuple[list[int], list[int]]:\n        \"\"\"\n        Given a tensor of floats, return the exponent and mantissa bits of each float.\n\n        Args:\n            tensor (torch.FloatTensor): A tensor of floats.\n\n        Returns:\n            tuple[list[int], list[int]]: A tuple containing the exponent and mantissa bits of each float.\n        \"\"\"\n        temp = None\n        bit_repr = []\n        for i, e in enumerate(tensor.untyped_storage()):\n            if i % ELEMENT_SIZE == 0:\n                if temp is not None:\n                    bit_repr.append(\"\".join(temp[::-1]))\n                temp = [bin(e)[2:].zfill(8)]\n            else:\n                temp.append(bin(e)[2:].zfill(8))\n        bit_repr.append(\"\".join(temp[::-1]))\n\n        prefill_exps = [get_exp_bits(i) for i in bit_repr]\n        prefill_mants = [get_mant_bits(i) for i in bit_repr]\n        return prefill_exps, prefill_mants\n\n    start_time = time.time()\n    ref_exps, ref_mants = py_get_fp_parts(a)\n    old_time = time.time() - start_time\n\n    assert exps == ref_exps\n    assert mantissas == ref_mants\n    assert new_time < old_time\n\n\ndef test_sha256sum():\n    with tempfile.NamedTemporaryFile() as f:\n        f.write(b\"Hello, world!\" * 1000)\n        f.flush()\n        assert (\n            sha256sum(f.name)\n            == \"a8f764e70df94be2c911fb51b3d0c56c03882078dbdb215de8b7bd0374b0fb10\"\n        )\n"}
{"type": "source_file", "path": "bench/bench_ndd.py", "content": "\"\"\"\nSimplistic benchmark comparing Python and C++ implementations of ndd.\n\"\"\"\n\nimport timeit\nfrom toploc.C.csrc.ndd import (\n    compute_newton_coefficients as compute_newton_coefficients_cpp,\n)\nfrom toploc.ndd import compute_newton_coefficients as compute_newton_coefficients_py\n\nMOD_N = 65497\n\nn = 1000\nx = list(range(n))\ny = [i % MOD_N for i in range(n)]\n\nt_py = timeit.timeit(lambda: compute_newton_coefficients_py(x, y), number=1)\nprint(f\"Python (n={n}): {t_py:.2f}s\")\n\nt_cpp = timeit.timeit(lambda: compute_newton_coefficients_cpp(x, y), number=1)\nprint(f\"C++ (n={n}): {t_cpp:.2f}s\")\n"}
{"type": "source_file", "path": "setup.py", "content": "from setuptools import setup\nimport os\nimport platform\nfrom torch.utils.cpp_extension import BuildExtension, CppExtension\n\nCSRC_DIR = os.path.join(\"toploc\", \"C\", \"csrc\")\n\n# Define mac-specific compiler and linker flags\nextra_compile_args = [\"-O3\"]\nextra_link_args: list[str] = []\n\n# Add macOS specific flags\nif platform.system() == \"Darwin\":\n    # Enable support for both Intel and Apple Silicon\n    extra_compile_args.extend([\"-arch\", \"x86_64\", \"-arch\", \"arm64\"])\n    extra_link_args.extend([\"-arch\", \"x86_64\", \"-arch\", \"arm64\"])\n\n    # Add minimum deployment target for macOS\n    extra_compile_args.append(\"-mmacosx-version-min=10.13\")\n    extra_link_args.append(\"-mmacosx-version-min=10.13\")\n\nextensions = [\n    CppExtension(\n        name=\"toploc.C.csrc.ndd\",\n        sources=[os.path.join(CSRC_DIR, \"ndd.cpp\")],\n        extra_compile_args=extra_compile_args,\n        extra_link_args=extra_link_args,\n    ),\n    CppExtension(\n        name=\"toploc.C.csrc.utils\",\n        sources=[os.path.join(CSRC_DIR, \"utils.cpp\")],\n        extra_compile_args=extra_compile_args,\n        extra_link_args=extra_link_args,\n    ),\n]\n\nsetup(\n    name=\"toploc\",\n    ext_modules=extensions,\n    packages=[\"toploc\", \"toploc.C.csrc\"],\n    package_data={\n        \"toploc.C.csrc\": [\"*.pyi\"],  # Include .pyi files\n    },\n    cmdclass={\"build_ext\": BuildExtension},\n)\n"}
{"type": "source_file", "path": "toploc/utils.py", "content": "import hashlib\n\n\ndef sha256sum(filename: str, chunk_size: int = 65536) -> str:\n    \"\"\"Calculate the SHA-256 checksum of a file efficiently.\n\n    Args:\n        filename (str): Path to the file.\n        chunk_size (int, optional): Size of chunks read at a time. Defaults to 64 KB.\n\n    Returns:\n        str: The SHA-256 hash of the file as a hexadecimal string.\n    \"\"\"\n    sha256 = hashlib.sha256()\n    with open(filename, \"rb\", buffering=0) as f:\n        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n            sha256.update(memoryview(chunk))\n    return sha256.hexdigest()\n"}
{"type": "source_file", "path": "toploc/C/csrc/__init__.py", "content": "# We need to import torch here to set up the .so files\nimport torch  # noqa: F401\n"}
{"type": "source_file", "path": "toploc/poly.py", "content": "from typing import Union\nimport base64\nfrom toploc.C.csrc.ndd import compute_newton_coefficients, evaluate_polynomial\nfrom toploc.C.csrc.utils import get_fp_parts\nimport torch\nimport logging\nfrom dataclasses import dataclass\nfrom statistics import mean, median\n\nlogger = logging.getLogger(__name__)\n\n\ndef find_injective_modulus(x: list[int]) -> int:\n    for i in range(65497, 2**15, -1):\n        if len(set([j % i for j in x])) == len(x):\n            return i\n    raise ValueError(\"No injective modulus found!\")  # pragma: no cover\n\n\nclass ProofPoly:\n    def __init__(self, coeffs: list[int], modulus: int):\n        self.coeffs = coeffs\n        self.modulus = modulus\n\n    def __call__(self, x: int):\n        return evaluate_polynomial(self.coeffs, x % self.modulus)\n\n    def __len__(self):\n        return len(self.coeffs)\n\n    @classmethod\n    def null(cls, length: int) -> \"ProofPoly\":\n        return cls([0] * length, 0)\n\n    @classmethod\n    def from_points(\n        cls, x: Union[list[int], torch.Tensor], y: Union[list[int], torch.Tensor]\n    ) -> \"ProofPoly\":\n        if isinstance(x, torch.Tensor):\n            x = x.tolist()\n        if isinstance(y, torch.Tensor):\n            if y.dtype == torch.bfloat16:\n                y = y.view(dtype=torch.uint16).tolist()\n            elif y.dtype == torch.float32:\n                raise NotImplementedError(\n                    \"float32 not supported yet because interpolate has hardcode prime\"\n                )\n            else:\n                y = y.tolist()\n        modulus = find_injective_modulus(x)\n        x = [i % modulus for i in x]\n        return cls(compute_newton_coefficients(x, y), modulus)\n\n    def to_base64(self):\n        base64_encoded = base64.b64encode(self.to_bytes()).decode(\"utf-8\")\n        return base64_encoded\n\n    def to_bytes(self):\n        return self.modulus.to_bytes(2, byteorder=\"big\", signed=False) + b\"\".join(\n            coeff.to_bytes(2, byteorder=\"big\", signed=False) for coeff in self.coeffs\n        )\n\n    @classmethod\n    def from_bytes(cls, byte_data: bytes) -> \"ProofPoly\":\n        modulus = int.from_bytes(byte_data[:2], byteorder=\"big\", signed=False)\n        coeffs = [\n            int.from_bytes(byte_data[i : i + 2], byteorder=\"big\", signed=False)\n            for i in range(2, len(byte_data), 2)\n        ]\n        return cls(coeffs, modulus)\n\n    @classmethod\n    def from_base64(cls, base64_encoded: str) -> \"ProofPoly\":\n        byte_data = base64.b64decode(base64_encoded)\n        return cls.from_bytes(byte_data)\n\n    def __repr__(self) -> str:\n        return f\"ProofPoly[{self.modulus}]({self.coeffs})\"\n\n\ndef build_proofs(\n    activations: list[torch.Tensor],\n    decode_batching_size: int,\n    topk: int,\n    skip_prefill: bool = False,\n) -> list[ProofPoly]:\n    proofs = []\n\n    # In order to not crash, we return null proofs if there is an error\n    try:\n        # Prefill\n        if not skip_prefill:\n            flat_view = activations[0].view(-1)\n            topk_indices = flat_view.abs().topk(topk).indices\n            topk_values = flat_view[topk_indices]\n            proof = ProofPoly.from_points(topk_indices, topk_values)\n            proofs.append(proof)\n\n        # Batched Decode\n        for i in range(\n            0 if skip_prefill else 1, len(activations), decode_batching_size\n        ):\n            flat_view = torch.cat(\n                [i.view(-1) for i in activations[i : i + decode_batching_size]]\n            )\n            topk_indices = flat_view.abs().topk(topk).indices\n            topk_values = flat_view[topk_indices]\n            proof = ProofPoly.from_points(topk_indices, topk_values)\n            proofs.append(proof)\n    except Exception as e:\n        logger.error(f\"Error building proofs: {e}\")\n        proofs = [ProofPoly.null(topk)] * (\n            1 + (len(activations) - 1 + decode_batching_size) // decode_batching_size\n        )\n\n    return proofs\n\n\ndef build_proofs_bytes(\n    activations: list[torch.Tensor],\n    decode_batching_size: int,\n    topk: int,\n    skip_prefill: bool = False,\n) -> list[bytes]:\n    return [\n        proof.to_bytes()\n        for proof in build_proofs(activations, decode_batching_size, topk, skip_prefill)\n    ]\n\n\ndef build_proofs_base64(\n    activations: list[torch.Tensor],\n    decode_batching_size: int,\n    topk: int,\n    skip_prefill: bool = False,\n) -> list[str]:\n    return [\n        proof.to_base64()\n        for proof in build_proofs(activations, decode_batching_size, topk, skip_prefill)\n    ]\n\n\ndef batch_activations(\n    activations: list[torch.Tensor],\n    decode_batching_size: int,\n    skip_prefill: bool = False,\n) -> list[torch.Tensor]:\n    batches = []\n\n    # Prefill\n    if not skip_prefill:\n        flat_view = activations[0].view(-1)\n        batches.append(flat_view)\n\n    # Batched Decode\n    for i in range(0 if skip_prefill else 1, len(activations), decode_batching_size):\n        flat_view = torch.cat(\n            [i.view(-1) for i in activations[i : i + decode_batching_size]]\n        )\n        batches.append(flat_view)\n\n    return batches\n\n\n# NOTE (Jack): Attributes should always be a measure of error, increasing the further we are from the proof\n# This way, acceptance is always below the threshold and rejection is always above\n# e.g. exp_match is bad, exp_mismatch is good\n@dataclass\nclass VerificationResult:\n    exp_mismatches: int\n    mant_err_mean: float\n    mant_err_median: float\n\n\ndef verify_proofs(\n    activations: list[torch.Tensor],\n    proofs: list[ProofPoly],\n    decode_batching_size: int,\n    topk: int,\n    skip_prefill: bool = False,\n) -> list[VerificationResult]:\n    results = []\n    for proof, chunk in zip(\n        proofs,\n        batch_activations(\n            activations,\n            decode_batching_size=decode_batching_size,\n            skip_prefill=skip_prefill,\n        ),\n    ):\n        chunk = chunk.view(-1).cpu()\n        topk_indices = chunk.abs().topk(k=topk).indices.tolist()\n        topk_values = chunk[topk_indices]\n        proof_topk_values = torch.tensor(\n            [proof(i) for i in topk_indices], dtype=torch.uint16\n        ).view(dtype=torch.bfloat16)\n        exps, mants = get_fp_parts(proof_topk_values)\n        proof_exps, proof_mants = get_fp_parts(topk_values)\n\n        exp_mismatches = [i != j for i, j in zip(exps, proof_exps)]\n        mant_errs = [\n            abs(i - j) for i, j, k in zip(mants, proof_mants, exp_mismatches) if not k\n        ]\n        if len(mant_errs) > 0:\n            results.append(\n                VerificationResult(\n                    sum(exp_mismatches), mean(mant_errs), median(mant_errs)\n                )\n            )\n        else:\n            results.append(VerificationResult(sum(exp_mismatches), 2**64, 2**64))\n    return results\n\n\ndef verify_proofs_bytes(\n    activations: list[torch.Tensor],\n    proofs: list[bytes],\n    decode_batching_size: int,\n    topk: int,\n    skip_prefill: bool = False,\n) -> list[VerificationResult]:\n    return verify_proofs(\n        activations,\n        [ProofPoly.from_bytes(proof) for proof in proofs],\n        decode_batching_size,\n        topk,\n        skip_prefill,\n    )\n\n\ndef verify_proofs_base64(\n    activations: list[torch.Tensor],\n    proofs: list[str],\n    decode_batching_size: int,\n    topk: int,\n    skip_prefill: bool = False,\n) -> list[VerificationResult]:\n    return verify_proofs(\n        activations,\n        [ProofPoly.from_base64(proof) for proof in proofs],\n        decode_batching_size,\n        topk,\n        skip_prefill,\n    )\n"}
{"type": "source_file", "path": "toploc/__init__.py", "content": "# ruff: noqa: F401\nfrom toploc.poly import (\n    ProofPoly,\n    build_proofs,\n    build_proofs_bytes,\n    build_proofs_base64,\n    verify_proofs,\n    verify_proofs_bytes,\n    verify_proofs_base64,\n)\nfrom toploc.utils import sha256sum\n\n__version__ = \"0.0.0.dev1\"\n"}
