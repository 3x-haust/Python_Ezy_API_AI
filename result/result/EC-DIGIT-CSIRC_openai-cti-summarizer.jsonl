{"repo_info": {"repo_name": "openai-cti-summarizer", "repo_owner": "EC-DIGIT-CSIRC", "repo_url": "https://github.com/EC-DIGIT-CSIRC/openai-cti-summarizer"}}
{"type": "source_file", "path": "app/__init__.py", "content": ""}
{"type": "source_file", "path": "app/main.py", "content": "\"\"\"Main FastAPI file. Provides the app WSGI entry point.\"\"\"\nimport os\nimport sys\nimport tempfile\nfrom urllib.parse import urlparse\nfrom distutils.util import strtobool        # pylint: disable=deprecated-module\n\nimport requests\n\nimport fitz  # PyMuPDF\n\nimport uvicorn\nfrom fastapi import FastAPI, Request, Form, Depends, UploadFile, File\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.templating import Jinja2Templates\nfrom fastapi.staticfiles import StaticFiles\nfrom starlette.middleware.base import BaseHTTPMiddleware\nimport markdown\n\nfrom bs4 import BeautifulSoup\nfrom dotenv import load_dotenv, find_dotenv\n\nfrom summarizer import Summarizer       # pylint: ignore=import-error\nfrom auth import get_current_username   # pylint: ignore=import-error\n\nfrom settings import log                # pylint: ignore=import-error\n\n\n# first get the env parametting\nif not load_dotenv(find_dotenv(), verbose=True, override=False):     # read local .env file\n    log.warning(\"Could not find .env file! Assuming ENV vars work\")\n\ntry:\n    with open('../VERSION.txt', encoding='utf-8') as _f:\n        VERSION = _f.readline().rstrip('\\n')\nexcept Exception as e:\n    log.error(\"could not find VERSION.txt, bailing out.\")\n    sys.exit(-1)\n\n\napp = FastAPI(version=VERSION)\ntemplates = Jinja2Templates(directory=\"/templates\")\napp.mount(\"/static\", StaticFiles(directory=\"/static\"), name=\"static\")\nGO_AZURE = bool(strtobool(os.getenv('USE_AZURE', 'true')))\nOUTPUT_JSON = bool(strtobool(os.getenv('OUTPUT_JSON', 'false')))\nDRY_RUN = bool(strtobool(os.getenv('DRY_RUN', 'false')))\nOPENAI_MODEL = os.getenv('OPENAI_MODEL', 'gpt-3.5-turbo')\n\n# First detect if we should invoke OpenAI via MS Azure or directly\ntry:\n    GO_AZURE = bool(strtobool(os.getenv('USE_AZURE', 'false')))\nexcept Exception as e:\n    log.warning(\n        f\"Could not read 'USE_AZURE' env var. Reason: '{str(e)}'. Reverting to false.\")\n    GO_AZURE = False\n\n# print out settings\nlog.info(f\"{GO_AZURE=}\")\nlog.info(f\"{OUTPUT_JSON=}\")\nlog.info(f\"{DRY_RUN=}\")\nlog.info(f\"{OPENAI_MODEL=}\")\n\n\nclass HTTPSRedirectMiddleware(BaseHTTPMiddleware):\n    \"\"\"HTTP to HTTPS redirection\"\"\"\n    async def dispatch(self, request: Request, call_next):\n        if 'X-Forwarded-Proto' in request.headers and request.headers['X-Forwarded-Proto'] == 'https':\n            request.scope['scheme'] = 'https'\n        response = await call_next(request)\n        return response\n\n\napp.add_middleware(HTTPSRedirectMiddleware)\n\nsummarizer = Summarizer(go_azure=GO_AZURE, model=OPENAI_MODEL,\n                        max_tokens=8192, output_json=OUTPUT_JSON)\n\n\nasync def fetch_text_from_url(url: str) -> str:\n    \"\"\"Fetch the text behind url and try to extract it via beautiful soup.\n    Returns text or raises an exception.\n    \"\"\"\n    parsed_url = urlparse(url)\n    if not all([parsed_url.scheme, parsed_url.netloc]):\n        raise ValueError(\"Invalid URL\")\n\n    response = requests.get(url, timeout=5)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    text = soup.get_text()\n    return text\n\n\n@app.get(\"/\", response_class=HTMLResponse)\ndef get_index(request: Request, username: str = Depends(get_current_username)):\n    \"\"\"Return the default page.\"\"\"\n    return templates.TemplateResponse(\"index.html\", {\"request\": request, \"system_prompt\": os.environ['SYSTEM_PROMPT'], \"username\": username})\n\n\ndef convert_pdf_to_markdown(filename: str) -> str:\n    \"\"\"Convert a PDF file given by <filename> to markdown.\n\n    Args:\n      filename: str     the file on the filesystem\n\n    Returns:\n      markdown or \"\" empty string in case of error\n    \"\"\"\n    # Open the PDF file\n    doc = fitz.open(filename)\n\n    # Initialize a variable to hold the text\n    markdown_content = \"\"\n\n    # Iterate through each page of the PDF\n    for page_num in range(len(doc)):\n        # Get the page\n        page = doc.load_page(page_num)\n\n        # Extract text from the page\n        text = page.get_text()\n\n        # Add the text to our markdown content, followed by a page break\n        markdown_content += text + \"\\n\\n---\\n\\n\"\n\n    return markdown_content\n\n\n# The main POST method. Input can either be a URL or a PDF file or a textarea text\n@app.post(\"/\", response_class=HTMLResponse)\nasync def index(request: Request,           # request object\n                text: str = Form(None),     # the text in the textarea\n                url: str = Form(None),      # alternatively the URL\n                pdffile: UploadFile = File(None),\n                system_prompt: str = Form(None), model: str = Form('model'), token_count: int = Form(100),\n                username: str = Depends(get_current_username)):\n    \"\"\"HTTP POST method for the default page. This gets called when the user already HTTP POSTs a text which should be summarized.\"\"\"\n\n    if url:\n        log.warning(f\"Got request with url: {url[:20]}\")\n    elif pdffile:\n        log.warning(f\"Got request with pdffile: {pdffile.filename}\")\n    elif text:\n        log.warning(f\"Got request with text: {text[:100]}\")\n    else:\n        log.error(\"no pdffile, no text, no url. Bailing out.\")\n        error = \"Expected either url field or text field or a PDF file. Please specify one at least.\"\n        result = None\n        return templates.TemplateResponse(\"index.html\", {\"request\": request, \"text\": text, \"system_prompt\": system_prompt, \"result\": error, \"success\": False, \"username\": username}, status_code=400)\n\n    summarizer.model = model\n    summarizer.max_tokens = token_count\n\n    if url:\n        try:\n            text = await fetch_text_from_url(url)\n        except Exception as ex:\n            return templates.TemplateResponse(\"index.html\", {\"request\": request, \"text\": url, \"system_prompt\": system_prompt, \"result\": f\"Could not fetch URL. Reason {str(ex)}\", \"success\": False}, status_code=400)\n\n    elif pdffile:\n        log.warning(\"we got a pdffile\")\n        try:\n            suffix = \".pdf\"\n            with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:\n                tmp.write(pdffile.file.read())\n                tmp_pdf_path = tmp.name  # Temp file path\n                log.warning(f\"stored as {tmp_pdf_path}\")\n\n            # Convert PDF to Markdown\n            text = convert_pdf_to_markdown(tmp_pdf_path)\n            log.warning(f\"converted as {text[:100]}\")\n\n            # Cleanup the temporary file\n            os.unlink(tmp_pdf_path)\n        except Exception as ex:\n            return templates.TemplateResponse(\"index.html\", {\"request\": request, \"text\": text, \"system_prompt\": system_prompt, \"result\": f\"Could not process the PDF file. Reason {str(ex)}\", \"success\": False}, status_code=400)\n\n    # we got the text from the URL or the pdffile was converted... now check if we should actually summarize\n    if DRY_RUN:\n        result = \"This is a sample response, we are in dry-run mode. We don't want to waste money for querying the API.\"\n        error = None\n    else:\n        result, error = summarizer.summarize(text, system_prompt)\n\n    if error:\n        return templates.TemplateResponse(\"index.html\", {\"request\": request, \"text\": text, \"system_prompt\": system_prompt, \"result\": error, \"success\": False, \"username\": username}, status_code=400)\n\n    result = markdown.markdown(result)\n    return templates.TemplateResponse(\"index.html\", {\n        \"request\": request,\n        \"text\": text,\n        \"system_prompt\": system_prompt,\n        \"result\": result,\n        \"success\": True,\n        \"model\": model,\n        \"username\": username,\n        \"token_count\": token_count})\n\n\nif __name__ == \"__main__\":\n    uvicorn.run('main:app', host=\"localhost\", port=9999, reload=True)\n"}
{"type": "source_file", "path": "app/misc.py", "content": "\"\"\"\nmisc.py - collection of all kinds of stuff\n\"\"\"\n\nLORE_IPSUM = \"\"\"\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Malesuada pellentesque elit eget gravida cum. Sed id semper risus in hendrerit. Dui vivamus arcu felis bibendum ut tristique et. Felis imperdiet proin fermentum leo vel orci. Sit amet facilisis magna etiam tempor orci eu lobortis. Enim ut tellus elementum sagittis. Nulla at volutpat diam ut venenatis tellus in metus vulputate. Phasellus faucibus scelerisque eleifend donec pretium vulputate. Cras adipiscing enim eu turpis egestas pretium aenean. Tincidunt augue interdum velit euismod in pellentesque massa. Malesuada fames ac turpis egestas maecenas pharetra convallis posuere. Auctor augue mauris augue neque gravida in. Tempus imperdiet nulla malesuada pellentesque elit eget gravida cum. Morbi tristique senectus et netus et malesuada fames ac.\n\nAmet tellus cras adipiscing enim eu turpis egestas pretium. Orci nulla pellentesque dignissim enim sit. Suspendisse potenti nullam ac tortor vitae purus faucibus ornare. Enim neque volutpat ac tincidunt vitae semper quis. Ullamcorper eget nulla facilisi etiam. Proin sed libero enim sed. Tortor pretium viverra suspendisse potenti nullam ac tortor vitae. Adipiscing bibendum est ultricies integer quis auctor elit sed vulputate. Elit at imperdiet dui accumsan sit amet nulla facilisi. Augue eget arcu dictum varius duis. Tortor posuere ac ut consequat semper viverra nam libero. Phasellus faucibus scelerisque eleifend donec pretium vulputate sapien nec. Magna fermentum iaculis eu non diam phasellus vestibulum lorem.\n\nFaucibus interdum posuere lorem ipsum dolor sit. Nulla pellentesque dignissim enim sit. Tincidunt praesent semper feugiat nibh sed pulvinar proin gravida. Cras ornare arcu dui vivamus. Interdum varius sit amet mattis vulputate. Enim nulla aliquet porttitor lacus luctus. Vitae justo eget magna fermentum. Auctor elit sed vulputate mi sit amet mauris commodo quis. Congue eu consequat ac felis donec et odio pellentesque diam. Urna molestie at elementum eu facilisis sed. In metus vulputate eu scelerisque felis imperdiet proin. Sollicitudin ac orci phasellus egestas.\n\nConsectetur a erat nam at lectus urna duis convallis convallis. Justo donec enim diam vulputate ut pharetra sit. Sagittis purus sit amet volutpat consequat mauris. Placerat vestibulum lectus mauris ultrices. Feugiat nibh sed pulvinar proin. Placerat orci nulla pellentesque dignissim enim. Dui accumsan sit amet nulla facilisi. Magna sit amet purus gravida quis. Id donec ultrices tincidunt arcu non sodales. Volutpat sed cras ornare arcu dui.\n\nPharetra pharetra massa massa ultricies. Lacus luctus accumsan tortor posuere ac. Libero justo laoreet sit amet cursus. Posuere morbi leo urna molestie at elementum eu facilisis. Arcu ac tortor dignissim convallis. Euismod nisi porta lorem mollis aliquam ut. Id aliquet lectus proin nibh nisl condimentum id venenatis a. Praesent semper feugiat nibh sed pulvinar. Fermentum posuere urna nec tincidunt praesent semper feugiat nibh. Sed ullamcorper morbi tincidunt ornare massa eget egestas. Cras semper auctor neque vitae tempus quam pellentesque.\n\nNibh praesent tristique magna sit amet. Id porta nibh venenatis cras. Dictumst vestibulum rhoncus est pellentesque elit. Tempus imperdiet nulla malesuada pellentesque elit eget gravida cum. Purus gravida quis blandit turpis cursus in. Mattis pellentesque id nibh tortor id aliquet. A diam sollicitudin tempor id eu nisl nunc mi. At volutpat diam ut venenatis tellus in metus vulputate. Imperdiet dui accumsan sit amet nulla facilisi morbi. Morbi tristique senectus et netus. Sit amet massa vitae tortor condimentum. Fusce ut placerat orci nulla pellentesque. In hac habitasse platea dictumst vestibulum rhoncus est. Lectus urna duis convallis convallis tellus id. Euismod in pellentesque massa placerat duis ultricies. Vitae purus faucibus ornare suspendisse sed nisi lacus sed viverra. Imperdiet proin fermentum leo vel orci porta non. Auctor neque vitae tempus quam pellentesque nec nam.\n\nPharetra pharetra massa massa ultricies mi quis hendrerit dolor. Neque ornare aenean euismod elementum nisi quis eleifend quam adipiscing. Adipiscing at in tellus integer feugiat scelerisque varius. Maecenas ultricies mi eget mauris pharetra et. Luctus venenatis lectus magna fringilla. Aliquet risus feugiat in ante metus dictum at tempor commodo. Tellus at urna condimentum mattis pellentesque id nibh. Nec feugiat in fermentum posuere urna nec tincidunt. Adipiscing elit ut aliquam purus sit amet luctus. Lobortis feugiat vivamus at augue eget arcu dictum varius duis. Interdum consectetur libero id faucibus nisl tincidunt.\n\nMetus dictum at tempor commodo ullamcorper a lacus vestibulum. Nullam ac tortor vitae purus faucibus. Mattis rhoncus urna neque viverra justo. Et egestas quis ipsum suspendisse. Erat velit scelerisque in dictum non consectetur a erat nam. Pulvinar etiam non quam lacus suspendisse faucibus interdum posuere. Faucibus scelerisque eleifend donec pretium vulputate sapien. Nunc sed id semper risus in. Nibh nisl condimentum id venenatis a condimentum vitae sapien. Nullam vehicula ipsum a arcu cursus vitae congue.\n\nAenean vel elit scelerisque mauris. Leo vel fringilla est ullamcorper eget nulla facilisi etiam. Magna fermentum iaculis eu non diam phasellus vestibulum lorem sed. Lectus urna duis convallis convallis tellus id. Facilisi nullam vehicula ipsum a arcu cursus. Tincidunt eget nullam non nisi est sit. Dictum at tempor commodo ullamcorper a lacus vestibulum sed arcu. Blandit massa enim nec dui nunc mattis enim ut tellus. Duis ut diam quam nulla porttitor massa id neque. Blandit aliquam etiam erat velit scelerisque in dictum. At consectetur lorem donec massa sapien faucibus et. Tempor commodo ullamcorper a lacus vestibulum sed arcu non odio. Netus et malesuada fames ac turpis egestas integer. Consectetur lorem donec massa sapien. Urna porttitor rhoncus dolor purus non.\n\nIpsum dolor sit amet consectetur adipiscing elit ut aliquam. Purus sit amet luctus venenatis lectus magna fringilla. Pulvinar neque laoreet suspendisse interdum consectetur libero id faucibus nisl. Nulla facilisi morbi tempus iaculis urna id volutpat lacus laoreet. Orci dapibus ultrices in iaculis nunc sed augue lacus. Nulla facilisi nullam vehicula ipsum a arcu cursus vitae congue. Velit egestas dui id ornare arcu odio ut sem nulla. Sed odio morbi quis commodo. Sagittis orci a scelerisque purus semper eget duis at tellus. Nunc id cursus metus aliquam eleifend mi in nulla. Sit amet nisl suscipit adipiscing bibendum est ultricies. Felis eget nunc lobortis mattis aliquam.\n\"\"\""}
{"type": "source_file", "path": "app/auth.py", "content": "\"\"\"Authorization helper.\"\"\"\n\nimport os\n\nfrom fastapi import Depends, HTTPException\nfrom fastapi.security import HTTPBasic, HTTPBasicCredentials\n\n\nsecurity = HTTPBasic()\n\n# fake users to simulate authentication\nfake_users = {\n    os.getenv('BASIC_AUTH_USER'): os.getenv('BASIC_AUTH_PASSWORD')\n}\n\n\n# dependency to check if the credentials are valid\ndef get_current_username(credentials: HTTPBasicCredentials = Depends(security)):\n    \"\"\"Check if user in the allowed list\"\"\"\n    username = credentials.username\n    password = credentials.password\n    if username in fake_users and password == fake_users[username]:\n        return username\n    raise HTTPException(status_code=401, detail=\"Invalid credentials\")\n"}
{"type": "source_file", "path": "app/summarizer.py", "content": "\"\"\"The summarizer class, abstracting away the LLM.\"\"\"\nimport os\nfrom typing import Tuple\n\nimport openai\nfrom openai import AzureOpenAI\n\nfrom settings import log            # pylint: ignore=import-error\n\n# first get the env parametting\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv())      # read local .env file\n\n\nclass Summarizer:\n    \"\"\"Wrapper to summarize texts via OpenAI or MS Azure's OpenAI.\"\"\"\n\n    client: openai._base_client.BaseClient\n\n    def __init__(self, model: str, max_tokens: int, system_prompt: str = \"\", go_azure: bool = False, output_json: bool = False):\n        if system_prompt:\n            self.system_prompt = system_prompt\n        else:\n            self.system_prompt = \"You are a Cyber Threat Intelligence Analyst and need to summarise a report for upper management. The report shall be nicely formatted with two sections: one Executive Summary section and one 'TTPs and IoCs' section. The second section shall list all IP addresses, domains, URLs, tools and hashes (sha-1, sha256, md5, etc.) which can be found in the report. Nicely format the report as markdown. Use newlines between markdown headings.\"\n        self.model = model\n        self.max_tokens = max_tokens\n        self.go_azure = go_azure\n        self.output_json = output_json\n\n        if self.go_azure:\n            api_version = os.environ['OPENAI_API_VERSION']\n            azure_endpoint = os.environ['OPENAI_API_BASE']\n            azure_deployment = os.environ['ENGINE']\n            api_key = os.environ['AZURE_OPENAI_API_KEY']\n            log.debug(f\"\"\"\n                {api_version=},\n                {azure_endpoint=},\n                {azure_deployment=},\n                {api_key=}\n            \"\"\")\n            self.client = AzureOpenAI(api_version=os.environ['OPENAI_API_VERSION'],\n                                      azure_endpoint=os.environ['OPENAI_API_BASE'],\n                                      azure_deployment=os.environ['ENGINE'],\n                                      api_key=os.environ['AZURE_OPENAI_API_KEY'])\n\n            # TODO: The 'openai.api_base' option isn't read in the client API. You will need to pass it when you instantiate the client, e.g. 'OpenAI(api_base=os.environ['OPENAI_API_BASE'])'\n            # openai.api_base = os.environ['OPENAI_API_BASE']             # Your Azure OpenAI resource's endpoint value.\n            # \"2023-05-15\"\n\n            \"\"\"\n            openai.api_type = os.environ['OPENAI_API_TYPE']\n            openai.api_base = os.environ['OPENAI_API_BASE']             # \"https://devmartiopenai.openai.azure.com/\"\n            openai.api_version = os.environ['OPENAI_API_VERSION']       # \"2023-05-15\"\n            \"\"\"\n            log.info(f\"Using Azure client {self.client._version}\")\n        else:\n            self.client = openai.OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n\n    def summarize(self, text: str, system_prompt: str = \"\") -> Tuple[str, str]:\n        \"\"\"Send <text> to openAI and get a summary back.\n        Returns a tuple: error, message. Note that either error or message may be None.\n        \"\"\"\n        if not system_prompt:\n            system_prompt = self.system_prompt\n        messages = [\n            {\"role\": \"system\", \"content\": system_prompt},      # single shot\n            {\"role\": \"user\", \"content\": text}\n        ]\n\n        try:\n            if self.go_azure:\n                log.info(\"Using MS AZURE!\")\n                response = self.client.chat.completions.create(model=os.environ['ENGINE'],\n                                                               messages=messages,\n                                                               temperature=0.3,\n                                                               top_p=0.95,\n                                                               stop=None,\n                                                               max_tokens=self.max_tokens,\n                                                               n=1)\n            else:       # go directly via OpenAI's API\n                log.info(\"Using OpenAI directly!\")\n                if self.output_json:\n                    response_format = {\"type\": \"json_object\"}\n                else:\n                    response_format = None\n                response = self.client.chat.completions.create(model=self.model,\n                                                               messages=messages,\n                                                               temperature=0.3,\n                                                               top_p=0.95,\n                                                               stop=None,\n                                                               max_tokens=self.max_tokens,\n                                                               response_format=response_format,\n                                                               n=1)\n\n            log.debug(f\"Full Response (OpenAI): {response}\")\n            log.debug(f\"response.choices[0].text: {response.choices[0].message}\")\n            log.debug(response.model_dump_json(indent=2))\n            result = response.choices[0].message.content\n            error = None            # Or move the error handling back to main.py, not sure\n        except openai.APIConnectionError as e:\n            result = None\n            error = f\"The server could not be reached. Reason {e.__cause__}\"\n            log.error(error)\n        except openai.RateLimitError as e:\n            result = None\n            error = f\"A 429 status code was received; we should back off a bit. {str(e)}\"\n            log.error(error)\n        except openai.APIStatusError as e:\n            result = None\n            error = f\"Another non-200-range status code was received. Status code: {e.status_code}. \\n\\nResponse: {e.message}\"\n            log.error(error)\n        except Exception as e:\n            result = None\n            error = f\"Unknown error! Error = '{str(e)}'\"\n            log.error(error)\n\n        return result, error        # type: ignore\n"}
{"type": "source_file", "path": "quickview.py", "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n\"\"\"Quickview the plain_text ORKL field\"\"\"\n\nimport sys\nfrom pprint import pprint\n\nwith open(sys.argv[1]) as fp:\n    data = fp.readlines()\n    for line in data:\n        data2 = line.split('\\\\n')\n        data2 = list(filter(None, data2))\n        pprint(data2)\n"}
{"type": "source_file", "path": "app/settings.py", "content": "\"\"\"General settings config.\"\"\"\n\nimport logging\n\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n                    datefmt='%Y-%m-%d %H:%M:%S')\n\nlog = logging.getLogger(__name__)\n"}
