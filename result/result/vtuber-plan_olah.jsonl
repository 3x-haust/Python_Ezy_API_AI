{"repo_info": {"repo_name": "olah", "repo_owner": "vtuber-plan", "repo_url": "https://github.com/vtuber-plan/olah"}}
{"type": "test_file", "path": "tests/simple_test.py", "content": "import os\nimport subprocess\nimport time\n\nfrom huggingface_hub import snapshot_download\n\ndef test_dataset():\n    process = subprocess.Popen(['python', '-m', 'olah.server'])\n\n    os.environ['HF_ENDPOINT'] = 'http://localhost:8090'\n    snapshot_download(repo_id='Nerfgun3/bad_prompt', repo_type='dataset',\n                    local_dir='./dataset_dir', max_workers=8)\n\n    process.terminate()\n\ndef test_model():\n    process = subprocess.Popen(['python', '-m', 'olah.server'])\n\n    os.environ['HF_ENDPOINT'] = 'http://localhost:8090'\n    snapshot_download(repo_id='prajjwal1/bert-tiny', repo_type='model',\n                    local_dir='./model_dir', max_workers=8)\n\n    process.terminate()"}
{"type": "source_file", "path": "src/olah/__init__.py", "content": ""}
{"type": "source_file", "path": "src/olah/auth/__init__.py", "content": ""}
{"type": "source_file", "path": "src/olah/mirror/__init__.py", "content": ""}
{"type": "source_file", "path": "src/olah/configs.py", "content": "# coding=utf-8\n# Copyright 2024 XiaHan\n#\n# Use of this source code is governed by an MIT-style\n# license that can be found in the LICENSE file or at\n# https://opensource.org/licenses/MIT.\n\nfrom typing import Any, Dict, List, Literal, Optional, Union\nimport toml\nimport re\nimport fnmatch\n\nfrom olah.utils.disk_utils import convert_to_bytes\n\nDEFAULT_PROXY_RULES = [\n    {\"repo\": \"*\", \"allow\": True, \"use_re\": False},\n    {\"repo\": \"*/*\", \"allow\": True, \"use_re\": False},\n]\n\nDEFAULT_CACHE_RULES = [\n    {\"repo\": \"*\", \"allow\": True, \"use_re\": False},\n    {\"repo\": \"*/*\", \"allow\": True, \"use_re\": False},\n]\n\n\nclass OlahRule(object):\n    def __init__(self, repo: str = \"\", type: str = \"*\", allow: bool = False, use_re: bool = False) -> None:\n        self.repo = repo\n        self.type = type\n        self.allow = allow\n        self.use_re = use_re\n\n    @staticmethod\n    def from_dict(data: Dict[str, Any]) -> \"OlahRule\":\n        out = OlahRule()\n        if \"repo\" in data:\n            out.repo = data[\"repo\"]\n        if \"allow\" in data:\n            out.allow = data[\"allow\"]\n        if \"use_re\" in data:\n            out.use_re = data[\"use_re\"]\n        return out\n\n    def match(self, repo_name: str) -> bool:\n        if self.use_re:\n            return self.match_re(repo_name)\n        else:\n            return self.match_fn(repo_name)\n\n    def match_fn(self, repo_name: str) -> bool:\n        return fnmatch.fnmatch(repo_name, self.repo)\n\n    def match_re(self, repo_name: str) -> bool:\n        return re.match(self.repo, repo_name) is not None\n\n\nclass OlahRuleList(object):\n    def __init__(self) -> None:\n        self.rules: List[OlahRule] = []\n\n    @staticmethod\n    def from_list(data: List[Dict[str, Any]]) -> \"OlahRuleList\":\n        out = OlahRuleList()\n        for item in data:\n            out.rules.append(OlahRule.from_dict(item))\n        return out\n\n    def clear(self):\n        self.rules.clear()\n\n    def allow(self, repo_name: str) -> bool:\n        allow = False\n        for rule in self.rules:\n            if rule.match(repo_name):\n                allow = rule.allow\n        return allow\n\n\nclass OlahConfig(object):\n    def __init__(self, path: Optional[str] = None) -> None:\n\n        # basic\n        self.host: Union[List[str], str] = \"localhost\"\n        self.port = 8090\n        self.ssl_key = None\n        self.ssl_cert = None\n        self.repos_path = \"./repos\"\n        self.cache_size_limit: Optional[int] = None\n        self.cache_clean_strategy: Literal[\"LRU\", \"FIFO\", \"LARGE_FIRST\"] = \"LRU\"\n\n        self.hf_scheme: str = \"https\"\n        self.hf_netloc: str = \"huggingface.co\"\n        self.hf_lfs_netloc: str = \"cdn-lfs.huggingface.co\"\n\n        self.mirror_scheme: str = \"http\" if self.ssl_key is None else \"https\"\n        self.mirror_netloc: str = (\n            f\"{self.host if self._is_specific_addr(self.host) else 'localhost'}:{self.port}\"\n        )\n        self.mirror_lfs_netloc: str = (\n            f\"{self.host if self._is_specific_addr(self.host) else 'localhost'}:{self.port}\"\n        )\n\n        self.mirrors_path: List[str] = []\n\n        # accessibility\n        self.offline = False\n        self.proxy = OlahRuleList.from_list(DEFAULT_PROXY_RULES)\n        self.cache = OlahRuleList.from_list(DEFAULT_CACHE_RULES)\n\n        if path is not None:\n            self.read_toml(path)\n    \n    def _is_specific_addr(self, host: Union[List[str], str]) -> bool:\n        if isinstance(host, str):\n            return host not in ['0.0.0.0', '::']\n        else:\n            return False\n\n    def hf_url_base(self) -> str:\n        return f\"{self.hf_scheme}://{self.hf_netloc}\"\n\n    def hf_lfs_url_base(self) -> str:\n        return f\"{self.hf_scheme}://{self.hf_lfs_netloc}\"\n\n    def mirror_url_base(self) -> str:\n        return f\"{self.mirror_scheme}://{self.mirror_netloc}\"\n\n    def mirror_lfs_url_base(self) -> str:\n        return f\"{self.mirror_scheme}://{self.mirror_lfs_netloc}\"\n\n    def empty_str(self, s: str) -> Optional[str]:\n        if s == \"\":\n            return None\n        else:\n            return s\n\n    def read_toml(self, path: str) -> None:\n        config = toml.load(path)\n\n        if \"basic\" in config:\n            basic = config[\"basic\"]\n            self.host = basic.get(\"host\", self.host)\n            self.port = basic.get(\"port\", self.port)\n            self.ssl_key = self.empty_str(basic.get(\"ssl-key\", self.ssl_key))\n            self.ssl_cert = self.empty_str(basic.get(\"ssl-cert\", self.ssl_cert))\n            self.repos_path = basic.get(\"repos-path\", self.repos_path)\n            self.cache_size_limit = convert_to_bytes(basic.get(\"cache-size-limit\", self.cache_size_limit))\n            self.cache_clean_strategy = basic.get(\"cache-clean-strategy\", self.cache_clean_strategy)\n\n            self.hf_scheme = basic.get(\"hf-scheme\", self.hf_scheme)\n            self.hf_netloc = basic.get(\"hf-netloc\", self.hf_netloc)\n            self.hf_lfs_netloc = basic.get(\"hf-lfs-netloc\", self.hf_lfs_netloc)\n\n            self.mirror_scheme = basic.get(\"mirror-scheme\", self.mirror_scheme)\n            self.mirror_netloc = basic.get(\"mirror-netloc\", self.mirror_netloc)\n            self.mirror_lfs_netloc = basic.get(\n                \"mirror-lfs-netloc\", self.mirror_lfs_netloc\n            )\n\n            self.mirrors_path = basic.get(\"mirrors-path\", self.mirrors_path)\n\n        if \"accessibility\" in config:\n            accessibility = config[\"accessibility\"]\n            self.offline = accessibility.get(\"offline\", self.offline)\n            self.proxy = OlahRuleList.from_list(accessibility.get(\"proxy\", DEFAULT_PROXY_RULES))\n            self.cache = OlahRuleList.from_list(accessibility.get(\"cache\", DEFAULT_CACHE_RULES))\n"}
{"type": "source_file", "path": "src/olah/cache/olah_cache.py", "content": "# coding=utf-8\n# Copyright 2024 XiaHan\n#\n# Use of this source code is governed by an MIT-style\n# license that can be found in the LICENSE file or at\n# https://opensource.org/licenses/MIT.\n\nimport asyncio\nimport lzma\nimport mmap\nimport os\nimport string\nimport struct\nimport threading\nimport gzip\nfrom typing import BinaryIO, Dict, List, Optional\n\nimport aiofiles\nimport fastapi\nimport fastapi.concurrency\nimport portalocker\nfrom .bitset import Bitset\n\nCURRENT_OLAH_CACHE_VERSION = 9\n# Due to the download chunk settings: https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/constants.py#L37\nDEFAULT_BLOCK_SIZE = 50 * 1024 * 1024\nMAX_BLOCK_NUM = 8192\nDEFAULT_COMPRESSION_ALGO = 1\n\"\"\"\n0: no compression\n1: gzip\n2: lzma\n3: blosc\n4: zlib\n5: zstd\n6: ...\n\"\"\"\n\nclass OlahCacheHeader(object):\n    MAGIC_NUMBER = \"OLAH\".encode(\"ascii\")\n    HEADER_FIX_SIZE = 36\n\n    def __init__(\n        self,\n        version: int = CURRENT_OLAH_CACHE_VERSION,\n        block_size: int = DEFAULT_BLOCK_SIZE,\n        file_size: int = 0,\n        compression_algo: int = DEFAULT_COMPRESSION_ALGO,\n    ) -> None:\n        self._version = version\n        self._block_size = block_size\n        self._file_size = file_size\n        self._compression_algo = compression_algo\n\n        self._block_number = (file_size + block_size - 1) // block_size\n\n    @property\n    def version(self) -> int:\n        return self._version\n\n    @property\n    def block_size(self) -> int:\n        return self._block_size\n\n    @property\n    def file_size(self) -> int:\n        return self._file_size\n\n    @property\n    def block_number(self) -> int:\n        return self._block_number\n    \n    @property\n    def compression_algo(self) -> int:\n        return self._compression_algo\n\n    def get_header_size(self) -> int:\n        return self.HEADER_FIX_SIZE\n\n    def _valid_header(self) -> None:\n        if self._file_size > MAX_BLOCK_NUM * self._block_size:\n            raise Exception(\n                f\"The size of file {self._file_size} is out of the max capability of container ({MAX_BLOCK_NUM} * {self._block_size}).\"\n            )\n        if self._version < CURRENT_OLAH_CACHE_VERSION:\n            raise Exception(\n                f\"This Olah Cache file is created by older version Olah. Please remove cache files and retry.\"\n            )\n\n        if self._version > CURRENT_OLAH_CACHE_VERSION:\n            raise Exception(\n                f\"This Olah Cache file is created by newer version Olah. Please remove cache files and retry.\"\n            )\n\n    @staticmethod\n    def read(stream) -> \"OlahCacheHeader\":\n        obj = OlahCacheHeader()\n        try:\n            magic = struct.unpack(\n                \"<4s\", stream.read(4)\n            )\n        except struct.error:\n            raise Exception(\"File is not a Olah cache file.\")\n        if magic[0] != OlahCacheHeader.MAGIC_NUMBER:\n            raise Exception(\"File is not a Olah cache file.\")\n        \n        version, block_size, file_size, compression_algo = struct.unpack(\n            \"<QQQQ\", stream.read(OlahCacheHeader.HEADER_FIX_SIZE - 4)\n        )\n        obj._version = version\n        obj._block_size = block_size\n        obj._file_size = file_size\n        obj._compression_algo = compression_algo\n        \n        obj._block_number = (file_size + block_size - 1) // block_size\n\n        obj._valid_header()\n        return obj\n\n    def write(self, stream):\n        btyes_header = struct.pack(\n            \"<4sQQQQ\",\n            self.MAGIC_NUMBER,\n            self._version,\n            self._block_size,\n            self._file_size,\n            self._compression_algo,\n        )\n        stream.write(btyes_header)\n\n\nclass OlahCache(object):\n    def __init__(self, path: str, block_size: int = DEFAULT_BLOCK_SIZE) -> None:\n        self.path: Optional[str] = path\n        self.header: Optional[OlahCacheHeader] = None\n        self.is_open: bool = False\n\n        # Lock\n        self._header_lock = threading.Lock()\n        \n        # Path\n        self._meta_path = os.path.join(path, \"meta.bin\")\n        self._data_path = os.path.join(path, \"blocks/block_${block_index}.bin\")\n\n        self.open(path, block_size=block_size)\n\n    @staticmethod\n    def create(path: str, block_size: int = DEFAULT_BLOCK_SIZE):\n        return OlahCache(path, block_size=block_size)\n\n    def open(self, path: str, block_size: int = DEFAULT_BLOCK_SIZE):\n        if self.is_open:\n            raise Exception(\"This file has been open.\")\n        if self.path is None:\n            raise Exception(\"The file path is None.\")\n\n        if os.path.exists(path):\n            if not os.path.isdir(path):\n                raise Exception(\"The cache path shall be a folder instead of a file.\")\n            with self._header_lock:\n                with portalocker.Lock(self._meta_path, \"rb\", timeout=60, flags=portalocker.LOCK_SH) as f:\n                    f.seek(0)\n                    self.header = OlahCacheHeader.read(f)\n        else:\n            os.makedirs(self.path, exist_ok=True)\n            os.makedirs(os.path.join(self.path, \"blocks\"), exist_ok=True)\n            with self._header_lock:\n                # Create new file\n                with portalocker.Lock(self._meta_path, \"wb\", timeout=60, flags=portalocker.LOCK_EX) as f:\n                    f.seek(0)\n                    self.header = OlahCacheHeader(\n                        version=CURRENT_OLAH_CACHE_VERSION,\n                        block_size=block_size,\n                        file_size=0,\n                    )\n                    self.header.write(f)\n\n        self.is_open = True\n\n    def close(self):\n        if not self.is_open:\n            raise Exception(\"This file has been close.\")\n\n        self._flush_header()\n        self.path = None\n        self.header = None\n\n        self.is_open = False\n\n    def _flush_header(self):\n        if self.header is None:\n            raise Exception(\"The header of cache file is None\")\n        if self.path is None:\n            raise Exception(\"The path of cache file is None\")\n        with self._header_lock:\n            with portalocker.Lock(self._meta_path, \"rb+\", flags=portalocker.LOCK_EX) as f:\n                f.seek(0)\n                self.header.write(f)\n\n    def _get_file_size(self) -> int:\n        if self.header is None:\n            raise Exception(\"The header of cache file is None\")\n        with self._header_lock:\n            file_size = self.header.file_size\n        return file_size\n\n    def _get_block_number(self) -> int:\n        if self.header is None:\n            raise Exception(\"The header of cache file is None\")\n        with self._header_lock:\n            block_number = self.header.block_number\n        return block_number\n\n    def _get_block_size(self) -> int:\n        if self.header is None:\n            raise Exception(\"The header of cache file is None\")\n        with self._header_lock:\n            block_size = self.header.block_size\n        return block_size\n\n    def _get_header_size(self) -> int:\n        if self.header is None:\n            raise Exception(\"The header of cache file is None\")\n        with self._header_lock:\n            header_size = self.header.get_header_size()\n        return header_size\n\n    def _resize_header(self, block_num: int, file_size: int):\n        if self.header is None:\n            raise Exception(\"The header of cache file is None\")\n        with self._header_lock:\n            self.header._block_number = block_num\n            self.header._file_size = file_size\n            self.header._valid_header()\n\n    def _pad_block(self, raw_block: bytes) -> bytes:\n        if len(raw_block) < self._get_block_size():\n            block = raw_block + b\"\\x00\" * (self._get_block_size() - len(raw_block))\n        else:\n            block = raw_block\n        return block\n\n    def flush(self):\n        if not self.is_open:\n            raise Exception(\"This file has been close.\")\n        self._flush_header()\n\n    def has_block(self, block_index: int) -> bool:\n        block_path = string.Template(self._data_path).substitute(block_index=f\"{block_index:0>8}\")\n        return os.path.exists(block_path)\n\n    async def read_block(self, block_index: int) -> Optional[bytes]:\n        if not self.is_open:\n            raise Exception(\"This file has been closed.\")\n\n        if self.path is None:\n            raise Exception(\"The path of the cache file is None.\")\n        \n        if block_index >= self._get_block_number():\n            raise Exception(\"Invalid block index.\")\n        \n        if self.header is None:\n            raise Exception(\"The header of cache file is None\")\n\n        if not self.has_block(block_index=block_index):\n            return None\n        \n        block_path = string.Template(self._data_path).substitute(block_index=f\"{block_index:0>8}\")\n\n        with portalocker.Lock(block_path, \"rb\", timeout=60, flags=portalocker.LOCK_SH) as fh:\n            async with aiofiles.open(block_path, mode='rb') as f:\n                raw_block = await f.read(self._get_block_size())\n        \n        def decompression(block_data: bytes, compression_algo: int):\n            # compression\n            if compression_algo == 0:\n                return block_data\n            elif compression_algo == 1:\n                block_data = gzip.decompress(block_data)\n            elif compression_algo == 2:\n                lzma_dec = lzma.LZMADecompressor()\n                block_data = lzma_dec.decompress(block_data)\n            else:\n                raise Exception(\"Unsupported compression algorithm.\")\n            return block_data\n\n        raw_block = await fastapi.concurrency.run_in_threadpool(\n            decompression,\n            raw_block,\n            self.header.compression_algo\n        )\n\n        block = self._pad_block(raw_block)\n        return block\n\n    async def write_block(self, block_index: int, block_bytes: bytes) -> None:\n        if not self.is_open:\n            raise Exception(\"This file has been closed.\")\n        \n        if self.path is None:\n            raise Exception(\"The path of the cache file is None. \")\n\n        if block_index >= self._get_block_number():\n            raise Exception(\"Invalid block index.\")\n        \n        if self.header is None:\n            raise Exception(\"The header of cache file is None\")\n\n        if len(block_bytes) != self._get_block_size():\n            raise Exception(\"Block size does not match the cache's block size.\")\n        \n        # Truncation\n        if (block_index + 1) * self._get_block_size() > self._get_file_size():\n            real_block_bytes = block_bytes[\n                : self._get_file_size() - block_index * self._get_block_size()\n            ]\n        else:\n            real_block_bytes = block_bytes\n\n        def compression(block_data: bytes, compression_algo: int):\n            if compression_algo == 0:\n                return block_data\n            elif compression_algo == 1:\n                block_data = gzip.compress(block_data, compresslevel=4)\n            elif compression_algo == 2:\n                lzma_enc = lzma.LZMACompressor()\n                block_data = lzma_enc.compress(block_data)\n            else:\n                raise Exception(\"Unsupported compression algorithm.\")\n            return block_data\n\n        # Run in the default thread pool executor\n        real_block_bytes = await fastapi.concurrency.run_in_threadpool(\n            compression,\n            real_block_bytes,\n            self.header.compression_algo\n        )\n   \n        block_path = string.Template(self._data_path).substitute(block_index=f\"{block_index:0>8}\")\n\n        with portalocker.Lock(block_path, 'wb+', timeout=60, flags=portalocker.LOCK_EX) as fh:\n            async with aiofiles.open(block_path, mode='wb+') as f:\n                await f.write(real_block_bytes)\n\n        self._flush_header()\n\n    def _resize_file_size(self, file_size: int):\n        \"\"\"\n        Deprecation\n        \"\"\"\n        if not self.is_open:\n            raise Exception(\"This file has been closed.\")\n        \n        if self.path is None:\n            raise Exception(\"The path of the cache file is None. \")\n\n        if file_size == self._get_file_size():\n            return\n        if file_size < self._get_file_size():\n            raise Exception(\n                \"Invalid resize file size. New file size must be greater than the current file size.\"\n            )\n\n        with open(self.path, \"rb\") as f:\n            with mmap.mmap(f.fileno(), 0, mmap.MAP_SHARED, mmap.PROT_READ) as mm:\n                mm.seek(0, os.SEEK_END)\n                bin_size = mm.tell()\n\n        # FIXME: limit the resize method, because it may influence the _block_mask\n        new_bin_size = self._get_header_size() + file_size\n        with open(self.path, \"rb+\") as f:\n            with mmap.mmap(f.fileno(), 0, mmap.MAP_SHARED, mmap.PROT_WRITE) as mm:\n                mm.seek(new_bin_size - 1)\n                mm.write(b'\\0')\n                mm.truncate()\n                \n                # Extend file size (slow)\n                # mm.seek(0, os.SEEK_END)\n                # mm.write(b\"\\x00\" * (new_bin_size - bin_size))\n\n    def resize(self, file_size: int):\n        \"\"\"\n        Deprecation\n        \"\"\"\n        if not self.is_open:\n            raise Exception(\"This file has been closed.\")\n        bs = self._get_block_size()\n        new_block_num = (file_size + bs - 1) // bs\n        self._resize_header(new_block_num, file_size)\n        self._flush_header()\n"}
{"type": "source_file", "path": "src/olah/database/__init__.py", "content": ""}
{"type": "source_file", "path": "src/olah/constants.py", "content": "# coding=utf-8\n# Copyright 2024 XiaHan\n#\n# Use of this source code is governed by an MIT-style\n# license that can be found in the LICENSE file or at\n# https://opensource.org/licenses/MIT.\n\nimport os\n\n\nWORKER_API_TIMEOUT = 15\nCHUNK_SIZE = 4096\nLFS_FILE_BLOCK = 64 * 1024 * 1024\n\nDEFAULT_LOGGER_DIR = \"./logs\"\nOLAH_CODE_DIR = os.path.dirname(os.path.abspath(__file__))\n\nORIGINAL_LOC = \"oriloc\"\n\nfrom huggingface_hub.constants import (\n    REPO_TYPES_MAPPING,\n    HUGGINGFACE_CO_URL_TEMPLATE,\n    HUGGINGFACE_HEADER_X_REPO_COMMIT,\n    HUGGINGFACE_HEADER_X_LINKED_ETAG,\n    HUGGINGFACE_HEADER_X_LINKED_SIZE,\n)\n"}
{"type": "source_file", "path": "src/olah/database/models.py", "content": "# coding=utf-8\n# Copyright 2024 XiaHan\n#\n# Use of this source code is governed by an MIT-style\n# license that can be found in the LICENSE file or at\n# https://opensource.org/licenses/MIT.\n\nimport os\nfrom peewee import *\nimport datetime\n\nfrom olah.utils.olah_utils import get_olah_path\n\ndb_path = os.path.join(get_olah_path(), \"database.db\")\ndb = SqliteDatabase(db_path)\n\nclass BaseModel(Model):\n    class Meta:\n        database = db\n\nclass Token(BaseModel):\n    token = CharField(unique=True)\n    first_dt = DateTimeField()\n    last_dt = DateTimeField()\n\nclass DownloadLogs(BaseModel):\n    id = CharField(unique=True)\n    org = CharField()\n    repo = CharField()\n    path = CharField()\n    range_start = BigIntegerField()\n    range_end = BigIntegerField()\n    datetime = DateTimeField()\n    token = CharField()\n\nclass FileLevelLRU(BaseModel):\n    org = CharField()\n    repo = CharField()\n    path = CharField()\n    datetime = DateTimeField(default=datetime.datetime.now)\n\ndb.connect()\ndb.create_tables([\n    Token,\n    DownloadLogs,\n    FileLevelLRU,\n])\n"}
{"type": "source_file", "path": "src/olah/errors.py", "content": "# coding=utf-8\n# Copyright 2024 XiaHan\n#\n# Use of this source code is governed by an MIT-style\n# license that can be found in the LICENSE file or at\n# https://opensource.org/licenses/MIT.\n\nfrom fastapi import Response\nfrom fastapi.responses import JSONResponse\n\n\ndef error_repo_not_found() -> JSONResponse:\n    return JSONResponse(\n        content={\"error\": \"Repository not found\"},\n        headers={\n            \"x-error-code\": \"RepoNotFound\",\n            \"x-error-message\": \"Repository not found\",\n        },\n        status_code=401,\n    )\n\n\ndef error_page_not_found() -> JSONResponse:\n    return JSONResponse(\n        content={\"error\": \"Sorry, we can't find the page you are looking for.\"},\n        headers={\n            \"x-error-code\": \"RepoNotFound\",\n            \"x-error-message\": \"Sorry, we can't find the page you are looking for.\",\n        },\n        status_code=404,\n    )\n\n\ndef error_entry_not_found_branch(branch: str, path: str) -> Response:\n    return Response(\n        headers={\n            \"x-error-code\": \"EntryNotFound\",\n            \"x-error-message\": f'{path} does not exist on \"{branch}\"',\n        },\n        status_code=404,\n    )\n\n\ndef error_entry_not_found() -> Response:\n    return Response(\n        headers={\n            \"x-error-code\": \"EntryNotFound\",\n            \"x-error-message\": \"Entry not found\",\n        },\n        status_code=404,\n    )\n\n\ndef error_revision_not_found(revision: str) -> Response:\n    return JSONResponse(\n        content={\"error\": f\"Invalid rev id: {revision}\"},\n        headers={\n            \"x-error-code\": \"RevisionNotFound\",\n            \"x-error-message\": f\"Invalid rev id: {revision}\",\n        },\n        status_code=404,\n    )\n\n\n# Olah Custom Messages\ndef error_proxy_timeout() -> Response:\n    return Response(\n        headers={\n            \"x-error-code\": \"ProxyTimeout\",\n            \"x-error-message\": \"Proxy Timeout\",\n        },\n        status_code=504,\n    )\n\n\ndef error_proxy_invalid_data() -> Response:\n    return Response(\n        headers={\n            \"x-error-code\": \"ProxyInvalidData\",\n            \"x-error-message\": \"Proxy Invalid Data\",\n        },\n        status_code=504,\n    )\n"}
{"type": "source_file", "path": "src/olah/mirror/repos.py", "content": "# coding=utf-8\n# Copyright 2024 XiaHan\n#\n# Use of this source code is governed by an MIT-style\n# license that can be found in the LICENSE file or at\n# https://opensource.org/licenses/MIT.\nimport hashlib\nimport io\nimport os\nimport re\nfrom typing import Any, Dict, List, Union\nimport gitdb\nfrom git import Commit, Optional, Repo, Tree\nfrom git.objects.base import IndexObjUnion\nfrom gitdb.base import OStream\nimport yaml\n\nfrom olah.mirror.meta import RepoMeta\n\n\nclass LocalMirrorRepo(object):\n    def __init__(self, path: str, repo_type: str, org: str, repo: str) -> None:\n        self._path = path\n        self._repo_type = repo_type\n        self._org = org\n        self._repo = repo\n\n        self._git_repo = Repo(self._path)\n\n    def _sha256(self, text: Union[str, bytes]) -> str:\n        if isinstance(text, bytes) or isinstance(text, bytearray):\n            bin = text\n        elif isinstance(text, str):\n            bin = text.encode(\"utf-8\")\n        else:\n            raise Exception(\"Invalid sha256 param type.\")\n        sha256_hash = hashlib.sha256()\n        sha256_hash.update(bin)\n        hashed_string = sha256_hash.hexdigest()\n        return hashed_string\n\n    def _match_card(self, readme: str) -> str:\n        pattern = r\"\\s*---(.*?)---\"\n\n        match = re.match(pattern, readme, flags=re.S)\n\n        if match:\n            card_string = match.group(1)\n            return card_string\n        else:\n            return \"\"\n\n    def _remove_card(self, readme: str) -> str:\n        pattern = r\"\\s*---(.*?)---\"\n        out = re.sub(pattern, \"\", readme, flags=re.S)\n        return out\n\n    def _get_readme(self, commit: Commit) -> str:\n        if \"README.md\" not in commit.tree:\n            return \"\"\n        else:\n            out: bytes = commit.tree[\"README.md\"].data_stream.read()\n            return out.decode()\n\n    def _get_description(self, commit: Commit) -> str:\n        readme = self._get_readme(commit)\n        return self._remove_card(readme)\n\n    def _get_tree_filepaths_recursive(self, tree: Tree, include_dir: bool = False) -> List[str]:\n        out_paths = []\n        for entry in tree:\n            if entry.type == \"tree\":\n                out_paths.extend(self._get_tree_filepaths_recursive(entry))\n                if include_dir:\n                    out_paths.append(entry.path)\n            else:\n                out_paths.append(entry.path)\n        return out_paths\n\n    def _get_commit_filepaths_recursive(self, commit: Commit) -> List[str]:\n        return self._get_tree_filepaths_recursive(commit.tree)\n\n    def _get_path_info(self, entry: IndexObjUnion, expand: bool = False) -> Dict[str, Union[int, str]]:\n        lfs = False\n        if entry.type != \"tree\":\n            t = \"file\"\n            repr_size = entry.size\n            if repr_size > 120 and repr_size < 150:\n                # check lfs\n                lfs_data = entry.data_stream.read().decode(\"utf-8\")\n                match_groups = re.match(\n                    r\"version https://git-lfs\\.github\\.com/spec/v[0-9]\\noid sha256:([0-9a-z]{64})\\nsize ([0-9]+?)\\n\",\n                    lfs_data,\n                )\n                if match_groups is not None:\n                    lfs = True\n                    sha256 = match_groups.group(1)\n                    repr_size = int(match_groups.group(2))\n                    lfs_data = {\n                        \"oid\": sha256,\n                        \"size\": repr_size,\n                        \"pointerSize\": entry.size,\n                    }\n        else:\n            t = \"directory\"\n            repr_size = entry.size\n\n        if not lfs:\n            item = {\n                \"type\": t,\n                \"oid\": entry.hexsha,\n                \"size\": repr_size,\n                \"path\": entry.path,\n                \"name\": entry.name,\n            }\n        else:\n            item = {\n                \"type\": t,\n                \"oid\": entry.hexsha,\n                \"size\": repr_size,\n                \"path\": entry.path,\n                \"name\": entry.name,\n                \"lfs\": lfs_data,\n            }\n        if expand:\n            last_commit = next(self._git_repo.iter_commits(paths=entry.path, max_count=1))\n            item[\"lastCommit\"] = {\n                \"id\": last_commit.hexsha,\n                \"title\": last_commit.message,\n                \"date\": last_commit.committed_datetime.strftime(\n                    \"%Y-%m-%dT%H:%M:%S.%fZ\"\n                )\n            }\n            item[\"security\"] = {\n                \"blobId\": entry.hexsha,\n                \"name\": entry.name,\n                \"safe\": True,\n                \"indexed\": False,\n                \"avScan\": {\n                    \"virusFound\": False,\n                    \"virusNames\": None\n                },\n                \"pickleImportScan\": None\n            }\n        return item\n\n    def _get_tree_files(\n        self, tree: Tree, recursive: bool = False, expand: bool = False\n    ) -> List[Dict[str, Union[int, str]]]:\n        entries = []\n        for entry in tree:\n            entries.append(self._get_path_info(entry=entry, expand=expand))\n\n        if recursive:\n            for entry in tree:\n                if entry.type == \"tree\":\n                    entries.extend(self._get_tree_files(entry, recursive=recursive, expand=expand))\n        return entries\n\n    def _get_commit_files(self, commit: Commit) -> List[Dict[str, Union[int, str]]]:\n        return self._get_tree_files(commit.tree)\n\n    def _get_earliest_commit(self) -> Commit:\n        earliest_commit = None\n        earliest_commit_date = None\n\n        for commit in self._git_repo.iter_commits():\n            commit_date = commit.committed_datetime\n\n            if earliest_commit_date is None or commit_date < earliest_commit_date:\n                earliest_commit = commit\n                earliest_commit_date = commit_date\n\n        return earliest_commit\n\n    def get_index_object_by_path(\n        self, commit_hash: str, path: str\n    ) -> Optional[IndexObjUnion]:\n        try:\n            commit = self._git_repo.commit(commit_hash)\n        except gitdb.exc.BadName:\n            return None\n        path_part = path.split(\"/\")\n        path_part = [part for part in path_part if len(part.strip()) != 0]\n        tree = commit.tree\n        items = self._get_tree_files(tree=tree)\n        if len(path_part) == 0:\n            return None\n        for i, part in enumerate(path_part):\n            if i != len(path_part) - 1:\n                if part not in [\n                    item[\"name\"] for item in items if item[\"type\"] == \"directory\"\n                ]:\n                    return None\n            else:\n                if part not in [\n                    item[\"name\"] for item in items\n                ]:\n                    return None\n            tree = tree[part]\n            if tree.type == \"tree\":\n                items = self._get_tree_files(tree=tree, recursive=False)\n        return tree\n\n    def get_pathinfos(\n        self, commit_hash: str, paths: List[str]\n    ) -> Optional[List[Dict[str, Any]]]:\n        try:\n            commit = self._git_repo.commit(commit_hash)\n        except gitdb.exc.BadName:\n            return None\n\n        results = []\n        for path in paths:\n            index_obj = self.get_index_object_by_path(\n                commit_hash=commit_hash, path=path\n            )\n            if index_obj is not None:\n                results.append(self._get_path_info(index_obj))\n        \n        for r in results:\n            if \"name\" in r:\n                r.pop(\"name\")\n        return results\n\n    def get_tree(\n        self, commit_hash: str, path: str, recursive: bool = False, expand: bool = False\n    ) -> Optional[Dict[str, Any]]:\n        try:\n            commit = self._git_repo.commit(commit_hash)\n        except gitdb.exc.BadName:\n            return None\n\n        index_obj = self.get_index_object_by_path(commit_hash=commit_hash, path=path)\n        items = self._get_tree_files(tree=index_obj, recursive=recursive, expand=expand)\n        for r in items:\n            r.pop(\"name\")\n        return items\n    \n    def get_commits(self, commit_hash: str) -> Optional[Dict[str, Any]]:\n        try:\n            commit = self._git_repo.commit(commit_hash)\n        except gitdb.exc.BadName:\n            return None\n\n        parent_commits = [commit] + [each_commit for each_commit in commit.iter_parents()]\n        items = []\n        for each_commit in parent_commits:\n            item = {\n                \"id\": each_commit.hexsha,\n                \"title\": each_commit.message,\n                \"message\": \"\",\n                \"authors\": [],\n                \"date\": each_commit.committed_datetime.strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")\n            }\n            item[\"authors\"].append({\n                \"name\": each_commit.author.name,\n                \"avatar\": None\n            })\n            items.append(item)\n        return items\n\n    def get_meta(self, commit_hash: str) -> Optional[Dict[str, Any]]:\n        try:\n            commit = self._git_repo.commit(commit_hash)\n        except gitdb.exc.BadName:\n            return None\n        meta = RepoMeta()\n\n        meta._id = self._sha256(f\"{self._org}/{self._repo}/{commit.hexsha}\")\n        meta.id = f\"{self._org}/{self._repo}\"\n        meta.author = self._org\n        meta.sha = commit.hexsha\n        meta.lastModified = self._git_repo.head.commit.committed_datetime.strftime(\n            \"%Y-%m-%dT%H:%M:%S.%fZ\"\n        )\n        meta.private = False\n        meta.gated = False\n        meta.disabled = False\n        meta.tags = []\n        meta.description = self._get_description(commit)\n        meta.paperswithcode_id = None\n        meta.downloads = 0\n        meta.likes = 0\n        meta.cardData = yaml.load(\n            self._match_card(self._get_readme(commit)), Loader=yaml.CLoader\n        )\n        meta.siblings = [\n            {\"rfilename\": p} for p in self._get_commit_filepaths_recursive(commit)\n        ]\n        meta.createdAt = self._get_earliest_commit().committed_datetime.strftime(\n            \"%Y-%m-%dT%H:%M:%S.%fZ\"\n        )\n        return meta.to_dict()\n\n    def _contain_path(self, path: str, tree: Tree) -> bool:\n        norm_p = os.path.normpath(path).replace(\"\\\\\", \"/\")\n        parts = norm_p.split(\"/\")\n        for part in parts:\n            if all([t.name != part for t in tree]):\n                return False\n            else:\n                entry = tree[part]\n                if entry.type == \"tree\":\n                    tree = entry\n                else:\n                    tree = {}\n        return True\n\n    def get_file_head(self, commit_hash: str, path: str) -> Optional[Dict[str, Any]]:\n        try:\n            commit = self._git_repo.commit(commit_hash)\n        except gitdb.exc.BadName:\n            return None\n\n        if not self._contain_path(path, commit.tree):\n            return None\n        else:\n            header = {}\n            header[\"content-length\"] = str(commit.tree[path].data_stream.size)\n            header[\"x-repo-commit\"] = commit.hexsha\n            header[\"etag\"] = commit.tree[path].binsha.hex()\n            if (commit.tree[path].data_stream.size > 120) and (commit.tree[path].data_stream.size < 150):\n                lfs_data = commit.tree[path].data_stream.read().decode(\"utf-8\")\n                match_groups = re.match(\n                    r\"version https://git-lfs\\.github\\.com/spec/v[0-9]\\noid sha256:([0-9a-z]{64})\\nsize ([0-9]+?)\\n\",\n                    lfs_data,\n                )\n                if match_groups is not None:\n                    oid_sha256 = match_groups.group(1)\n                    objects_dir = os.path.join(self._git_repo.working_dir, '.git', 'lfs', 'objects')\n                    oid_dir = os.path.join(objects_dir, oid_sha256[:2], oid_sha256[2:4], oid_sha256)\n                    header[\"content-length\"] = str(os.path.getsize(oid_dir))\n                    with open(oid_dir, mode='rb') as lfs_file:\n                        header[\"etag\"] = self._sha256(lfs_file.read())\n\n            return header\n\n    def get_file(self, commit_hash: str, path: str) -> Optional[OStream]:\n        try:\n            commit = self._git_repo.commit(commit_hash)\n        except gitdb.exc.BadName:\n            return None\n\n        lfs = False\n        oid_dir = \"\"\n        if (commit.tree[path].size > 120) and (commit.tree[path].size < 150):\n            lfs_data = commit.tree[path].data_stream.read().decode(\"utf-8\")\n            match_groups = re.match(\n                    r\"version https://git-lfs\\.github\\.com/spec/v[0-9]\\noid sha256:([0-9a-z]{64})\\nsize ([0-9]+?)\\n\",\n                    lfs_data,\n            )\n            if match_groups is not None:\n                lfs = True\n                oid_sha256 = match_groups.group(1)\n                objects_dir = os.path.join(self._git_repo.working_dir, '.git', 'lfs', 'objects')\n                oid_dir = os.path.join(objects_dir, oid_sha256[:2], oid_sha256[2:4], oid_sha256)\n\n        def stream_wrapper(file_bytes: bytes):\n            file_stream = io.BytesIO(file_bytes)\n            while True:\n                chunk = file_stream.read(4096)\n                if len(chunk) == 0:\n                    break\n                else:\n                    yield chunk\n\n        if not self._contain_path(path, commit.tree):\n            return None\n        else:\n            if lfs:\n                with open(oid_dir, mode='rb') as lfs_file:\n                    return stream_wrapper(lfs_file.read())\n            else:\n                return stream_wrapper(commit.tree[path].data_stream.read())\n"}
{"type": "source_file", "path": "src/olah/proxy/lfs.py", "content": "# coding=utf-8\n# Copyright 2024 XiaHan\n#\n# Use of this source code is governed by an MIT-style\n# license that can be found in the LICENSE file or at\n# https://opensource.org/licenses/MIT.\n\nimport os\nfrom typing import Literal\nfrom fastapi import FastAPI, Header, Request\n\nfrom olah.proxy.files import _file_realtime_stream\nfrom olah.utils.file_utils import make_dirs\n\n\nasync def lfs_head_generator(\n    app, dir1: str, dir2: str, hash_repo: str, hash_file: str, request: Request\n):\n    # save\n    repos_path = app.state.app_settings.config.repos_path\n    head_path = os.path.join(\n        repos_path, f\"lfs/heads/{dir1}/{dir2}/{hash_repo}/{hash_file}\"\n    )\n    save_path = os.path.join(\n        repos_path, f\"lfs/files/{dir1}/{dir2}/{hash_repo}/{hash_file}\"\n    )\n    make_dirs(head_path)\n    make_dirs(save_path)\n\n    # use_cache = os.path.exists(head_path) and os.path.exists(save_path)\n    allow_cache = True\n\n    # proxy\n    return _file_realtime_stream(\n        app=app,\n        save_path=save_path,\n        head_path=head_path,\n        url=str(request.url),\n        request=request,\n        method=\"HEAD\",\n        allow_cache=allow_cache,\n        commit=None,\n    )\n\n\nasync def lfs_get_generator(\n    app, dir1: str, dir2: str, hash_repo: str, hash_file: str, request: Request\n):\n    # save\n    repos_path = app.state.app_settings.config.repos_path\n    head_path = os.path.join(\n        repos_path, f\"lfs/heads/{dir1}/{dir2}/{hash_repo}/{hash_file}\"\n    )\n    save_path = os.path.join(\n        repos_path, f\"lfs/files/{dir1}/{dir2}/{hash_repo}/{hash_file}\"\n    )\n    make_dirs(head_path)\n    make_dirs(save_path)\n\n    # use_cache = os.path.exists(head_path) and os.path.exists(save_path)\n    allow_cache = True\n\n    # proxy\n    return _file_realtime_stream(\n        app=app,\n        save_path=save_path,\n        head_path=head_path,\n        url=str(request.url),\n        request=request,\n        method=\"GET\",\n        allow_cache=allow_cache,\n        commit=None,\n    )\n"}
{"type": "source_file", "path": "src/olah/proxy/meta.py", "content": "# coding=utf-8\n# Copyright 2024 XiaHan\n#\n# Use of this source code is governed by an MIT-style\n# license that can be found in the LICENSE file or at\n# https://opensource.org/licenses/MIT.\n\nimport os\nimport shutil\nimport tempfile\nfrom typing import Dict, Literal, Optional, AsyncGenerator, Union\nfrom urllib.parse import urljoin\nfrom fastapi import FastAPI, Request\n\nimport httpx\nfrom olah.constants import CHUNK_SIZE, WORKER_API_TIMEOUT\n\nfrom olah.utils.cache_utils import read_cache_request, write_cache_request\nfrom olah.utils.rule_utils import check_cache_rules_hf\nfrom olah.utils.repo_utils import get_org_repo\nfrom olah.utils.file_utils import make_dirs\n\nasync def _meta_cache_generator(save_path: str) -> AsyncGenerator[Union[int, Dict[str, str], bytes], None]:\n    cache_rq = await read_cache_request(save_path)\n    yield cache_rq[\"headers\"]\n    yield cache_rq[\"content\"]\n\n\nasync def _meta_proxy_generator(\n    app: FastAPI,\n    headers: Dict[str, str],\n    meta_url: str,\n    method: str,\n    allow_cache: bool,\n    save_path: str,\n) -> AsyncGenerator[Union[int, Dict[str, str], bytes], None]:\n    async with httpx.AsyncClient(follow_redirects=True) as client:\n        content_chunks = []\n        async with client.stream(\n            method=method,\n            url=meta_url,\n            headers=headers,\n            timeout=WORKER_API_TIMEOUT,\n        ) as response:\n            response_status_code = response.status_code\n            response_headers = response.headers\n            yield response_headers\n\n            async for raw_chunk in response.aiter_raw():\n                if not raw_chunk:\n                    continue\n                content_chunks.append(raw_chunk)\n                yield raw_chunk\n\n        content = bytearray()\n        for chunk in content_chunks:\n            content += chunk\n\n        if allow_cache and response_status_code == 200:\n            await write_cache_request(\n                save_path, response_status_code, response_headers, bytes(content)\n            )\n\n\nasync def meta_generator(\n    app: FastAPI,\n    repo_type: Literal[\"models\", \"datasets\", \"spaces\"],\n    org: str,\n    repo: str,\n    commit: str,\n    override_cache: bool,\n    method: str,\n    authorization: Optional[str],\n) -> AsyncGenerator[Union[int, Dict[str, str], bytes], None]:\n    headers = {}\n    if authorization is not None:\n        headers[\"authorization\"] = authorization\n\n    org_repo = get_org_repo(org, repo)\n    # save\n    repos_path = app.state.app_settings.config.repos_path\n    save_dir = os.path.join(\n        repos_path, f\"api/{repo_type}/{org_repo}/revision/{commit}\"\n    )\n    save_path = os.path.join(save_dir, f\"meta_{method}.json\")\n    make_dirs(save_path)\n\n    use_cache = os.path.exists(save_path)\n    allow_cache = await check_cache_rules_hf(app, repo_type, org, repo)\n\n    org_repo = get_org_repo(org, repo)\n    meta_url = urljoin(\n        app.state.app_settings.config.hf_url_base(),\n        f\"/api/{repo_type}/{org_repo}/revision/{commit}\",\n    )\n    # proxy\n    if use_cache and not override_cache:\n        async for item in _meta_cache_generator(save_path):\n            yield item\n    else:\n        async for item in _meta_proxy_generator(\n            app, headers, meta_url, method, allow_cache, save_path\n        ):\n            yield item\n"}
{"type": "source_file", "path": "src/olah/proxy/commits.py", "content": "# coding=utf-8\n# Copyright 2024 XiaHan\n#\n# Use of this source code is governed by an MIT-style\n# license that can be found in the LICENSE file or at\n# https://opensource.org/licenses/MIT.\n\nimport os\nfrom typing import Dict, Literal, Mapping, Optional\nfrom urllib.parse import urljoin\nfrom fastapi import FastAPI, Request\n\nimport httpx\nfrom olah.constants import CHUNK_SIZE, WORKER_API_TIMEOUT\n\nfrom olah.utils.cache_utils import read_cache_request, write_cache_request\nfrom olah.utils.rule_utils import check_cache_rules_hf\nfrom olah.utils.repo_utils import get_org_repo\nfrom olah.utils.file_utils import make_dirs\n\n\nasync def _commits_cache_generator(save_path: str):\n    cache_rq = await read_cache_request(save_path)\n    yield cache_rq[\"status_code\"]\n    yield cache_rq[\"headers\"]\n    yield cache_rq[\"content\"]\n\n\nasync def _commits_proxy_generator(\n    app: FastAPI,\n    headers: Dict[str, str],\n    commits_url: str,\n    method: str,\n    params: Mapping[str, str],\n    allow_cache: bool,\n    save_path: str,\n):\n    async with httpx.AsyncClient(follow_redirects=True) as client:\n        content_chunks = []\n        async with client.stream(\n            method=method,\n            url=commits_url,\n            params=params,\n            headers=headers,\n            timeout=WORKER_API_TIMEOUT,\n        ) as response:\n            response_status_code = response.status_code\n            response_headers = response.headers\n            yield response_status_code\n            yield response_headers\n\n            async for raw_chunk in response.aiter_raw():\n                if not raw_chunk:\n                    continue\n                content_chunks.append(raw_chunk)\n                yield raw_chunk\n\n        content = bytearray()\n        for chunk in content_chunks:\n            content += chunk\n\n        if allow_cache and response_status_code == 200:\n            make_dirs(save_path)\n            await write_cache_request(\n                save_path, response_status_code, response_headers, bytes(content)\n            )\n\n\nasync def commits_generator(\n    app: FastAPI,\n    repo_type: Literal[\"models\", \"datasets\", \"spaces\"],\n    org: str,\n    repo: str,\n    commit: str,\n    override_cache: bool,\n    method: str,\n    authorization: Optional[str],\n):\n    headers = {}\n    if authorization is not None:\n        headers[\"authorization\"] = authorization\n\n    org_repo = get_org_repo(org, repo)\n    # save\n    repos_path = app.state.app_settings.config.repos_path\n    save_dir = os.path.join(\n        repos_path, f\"api/{repo_type}/{org_repo}/commits/{commit}\"\n    )\n    save_path = os.path.join(save_dir, f\"commits_{method}.json\")\n\n    use_cache = os.path.exists(save_path)\n    allow_cache = await check_cache_rules_hf(app, repo_type, org, repo)\n\n    org_repo = get_org_repo(org, repo)\n    commits_url = urljoin(\n        app.state.app_settings.config.hf_url_base(),\n        f\"/api/{repo_type}/{org_repo}/commits/{commit}\",\n    )\n    # proxy\n    if use_cache and not override_cache:\n        async for item in _commits_cache_generator(save_path):\n            yield item\n    else:\n        async for item in _commits_proxy_generator(\n            app, headers, commits_url, method, {}, allow_cache, save_path\n        ):\n            yield item\n"}
{"type": "source_file", "path": "src/olah/cache/__init__.py", "content": ""}
{"type": "source_file", "path": "src/olah/cache/bitset.py", "content": "# coding=utf-8\n# Copyright 2024 XiaHan\n#\n# Use of this source code is governed by an MIT-style\n# license that can be found in the LICENSE file or at\n# https://opensource.org/licenses/MIT.\n\n\nclass Bitset:\n    def __init__(self, size) -> None:\n        \"\"\"\n        Initializes a Bitset object with a given size.\n\n        Args:\n            size (int): The number of bits in the Bitset.\n        \"\"\"\n        self.size = size\n        self.bits = bytearray((0,) * ((size + 7) // 8))\n\n    def set(self, index: int) -> None:\n        \"\"\"\n        Sets the bit at the specified index to 1.\n\n        Args:\n            index (int): The index of the bit to be set.\n\n        Raises:\n            IndexError: If the index is out of range.\n        \"\"\"\n        if index < 0 or index >= self.size:\n            raise IndexError(\"Index out of range\")\n        byte_index = index // 8\n        bit_index = index % 8\n        self.bits[byte_index] |= 1 << bit_index\n\n    def clear(self, index: int) -> None:\n        \"\"\"\n        Sets the bit at the specified index to 0.\n\n        Args:\n            index (int): The index of the bit to be cleared.\n\n        Raises:\n            IndexError: If the index is out of range.\n        \"\"\"\n        if index < 0 or index >= self.size:\n            raise IndexError(\"Index out of range\")\n        self._resize_if_needed(index)\n        byte_index = index // 8\n        bit_index = index % 8\n        self.bits[byte_index] &= ~(1 << bit_index)\n\n    def test(self, index: int) -> None:\n        \"\"\"\n        Checks the value of the bit at the specified index.\n\n        Args:\n            index (int): The index of the bit to be checked.\n\n        Returns:\n            bool: True if the bit is set (1), False if the bit is cleared (0).\n\n        Raises:\n            IndexError: If the index is out of range.\n        \"\"\"\n        if index < 0 or index >= self.size:\n            raise IndexError(\"Index out of range\")\n        byte_index = index // 8\n        bit_index = index % 8\n        return bool(self.bits[byte_index] & (1 << bit_index))\n\n    def __str__(self):\n        \"\"\"\n        Returns a string representation of the Bitset.\n\n        Returns:\n            str: A string representation of the Bitset object, showing the binary representation of each byte.\n        \"\"\"\n        return \"\".join(bin(byte)[2:].zfill(8)[::-1] for byte in self.bits)\n"}
{"type": "source_file", "path": "src/olah/proxy/__init__.py", "content": ""}
{"type": "source_file", "path": "src/olah/proxy/files.py", "content": "# coding=utf-8\n# Copyright 2024 XiaHan\n#\n# Use of this source code is governed by an MIT-style\n# license that can be found in the LICENSE file or at\n# https://opensource.org/licenses/MIT.\n\nimport hashlib\nimport json\nimport os\nfrom typing import Dict, List, Literal, Optional, Tuple\nfrom fastapi import Request\nimport httpx\nfrom urllib.parse import urlparse, urljoin\n\nfrom olah.constants import (\n    CHUNK_SIZE,\n    WORKER_API_TIMEOUT,\n    HUGGINGFACE_HEADER_X_REPO_COMMIT,\n    HUGGINGFACE_HEADER_X_LINKED_ETAG,\n    HUGGINGFACE_HEADER_X_LINKED_SIZE,\n    ORIGINAL_LOC,\n)\nfrom olah.cache.olah_cache import OlahCache\nfrom olah.errors import error_entry_not_found, error_proxy_invalid_data, error_proxy_timeout\nfrom olah.proxy.pathsinfo import pathsinfo_generator\nfrom olah.utils.cache_utils import read_cache_request, write_cache_request\nfrom olah.utils.disk_utils import touch_file_access_time\nfrom olah.utils.url_utils import (\n    RemoteInfo,\n    add_query_param,\n    check_url_has_param_name,\n    get_all_ranges,\n    get_url_param_name,\n    get_url_tail,\n    parse_range_params,\n    remove_query_param,\n)\nfrom olah.utils.repo_utils import get_org_repo\nfrom olah.utils.rule_utils import check_cache_rules_hf\nfrom olah.utils.file_utils import make_dirs\nfrom olah.constants import CHUNK_SIZE, LFS_FILE_BLOCK, WORKER_API_TIMEOUT\nfrom olah.utils.zip_utils import Decompressor, decompress_data\n\n\ndef get_block_info(pos: int, block_size: int, file_size: int) -> Tuple[int, int, int]:\n    cur_block = pos // block_size\n    block_start_pos = cur_block * block_size\n    block_end_pos = min((cur_block + 1) * block_size, file_size)\n    return cur_block, block_start_pos, block_end_pos\n\n\ndef get_contiguous_ranges(\n    cache_file: OlahCache, start_pos: int, end_pos: int\n) -> List[Tuple[Tuple[int, int], bool]]:\n    start_block = start_pos // cache_file._get_block_size()\n    end_block = (end_pos - 1) // cache_file._get_block_size()\n\n    range_start_pos = start_pos\n    range_is_remote = not cache_file.has_block(start_block)\n    cur_pos = start_pos\n    # Get contiguous ranges: (range_start_pos, range_end_pos), is_remote\n    ranges_and_cache_list: List[Tuple[Tuple[int, int], bool]] = []\n    for cur_block in range(start_block, end_block + 1):\n        cur_block, block_start_pos, block_end_pos = get_block_info(\n            cur_pos, cache_file._get_block_size(), cache_file._get_file_size()\n        )\n\n        if cache_file.has_block(cur_block):\n            cur_is_remote = False\n        else:\n            cur_is_remote = True\n        if range_is_remote != cur_is_remote:\n            if range_start_pos < cur_pos:\n                ranges_and_cache_list.append(\n                    ((range_start_pos, cur_pos), range_is_remote)\n                )\n            range_start_pos = cur_pos\n            range_is_remote = cur_is_remote\n        cur_pos = block_end_pos\n\n    ranges_and_cache_list.append(((range_start_pos, end_pos), range_is_remote))\n    range_start_pos = end_pos\n    return ranges_and_cache_list\n\n\nasync def _get_file_range_from_cache(\n    cache_file: OlahCache, start_pos: int, end_pos: int\n):\n    start_block = start_pos // cache_file._get_block_size()\n    end_block = (end_pos - 1) // cache_file._get_block_size()\n    cur_pos = start_pos\n    for cur_block in range(start_block, end_block + 1):\n        _, block_start_pos, block_end_pos = get_block_info(\n            cur_pos, cache_file._get_block_size(), cache_file._get_file_size()\n        )\n        if not cache_file.has_block(cur_block):\n            raise Exception(\"Unknown exception: read block which has not been cached.\")\n        raw_block = await cache_file.read_block(cur_block)\n        chunk = raw_block[\n            max(start_pos, block_start_pos)\n            - block_start_pos : min(end_pos, block_end_pos)\n            - block_start_pos\n        ]\n        yield chunk\n        cur_pos += len(chunk)\n\n    if cur_pos != end_pos:\n        raise Exception(\"The cache range from {} to {} is incomplete.\")\n\n\nasync def _get_file_range_from_remote(\n    client: httpx.AsyncClient,\n    remote_info: RemoteInfo,\n    cache_file: OlahCache,\n    start_pos: int,\n    end_pos: int,\n):\n    headers = {}\n    if remote_info.headers.get(\"authorization\", None) is not None:\n        headers[\"authorization\"] = remote_info.headers.get(\"authorization\", None)\n    headers[\"range\"] = f\"bytes={start_pos}-{end_pos - 1}\"\n\n    chunk_bytes = 0\n    decompressor: Optional[Decompressor] = None\n    async with client.stream(\n        method=remote_info.method,\n        url=remote_info.url,\n        headers=headers,\n        timeout=WORKER_API_TIMEOUT,\n        follow_redirects=True,\n    ) as response:\n        status_code = response.status_code\n    \n        if status_code == 429:\n            raise Exception(\"Too many requests in a given amount of time.\")\n        \n        is_compressed = \"content-encoding\" in response.headers\n        if is_compressed:\n            decompressor = Decompressor(response.headers[\"content-encoding\"].split(\",\"))\n        \n        async for raw_chunk in response.aiter_raw():\n            if not raw_chunk:\n                continue\n            if is_compressed and decompressor is not None:\n                real_chunk = decompressor.decompress(raw_chunk)\n                yield real_chunk\n                chunk_bytes += len(real_chunk)\n            else:\n                yield raw_chunk\n                chunk_bytes += len(raw_chunk)\n\n        if is_compressed:\n            response_content_length = chunk_bytes\n        else:\n            response_content_length = int(response.headers[\"content-length\"])\n\n    # Post check\n    if end_pos - start_pos != response_content_length:\n        raise Exception(\n            f\"The content of the response is incomplete. File size: {cache_file._get_file_size()}. Start-end: {start_pos}-{end_pos}. Expected-{end_pos - start_pos}. Accepted-{response_content_length}\"\n        )\n\n\nasync def _file_chunk_get(\n    app,\n    save_path: str,\n    head_path: str,\n    client: httpx.AsyncClient,\n    method: str,\n    url: str,\n    headers: Dict[str, str],\n    allow_cache: bool,\n    file_size: int,\n):\n    # Redirect Chunks\n    if os.path.exists(save_path):\n        cache_file = OlahCache(save_path)\n    else:\n        cache_file = OlahCache.create(save_path)\n        cache_file.resize(file_size=file_size)\n    \n    # Refresh access time\n    touch_file_access_time(save_path)\n    \n    try:\n        unit, ranges, suffix = parse_range_params(headers.get(\"range\", f\"bytes={0}-{file_size-1}\"))\n        all_ranges = get_all_ranges(file_size, unit, ranges, suffix)\n\n        for start_pos, end_pos in all_ranges:\n            ranges_and_cache_list = get_contiguous_ranges(cache_file, start_pos, end_pos)\n            # Stream ranges\n            for (range_start_pos, range_end_pos), is_remote in ranges_and_cache_list:\n                # range_start_pos is zero-index and range_end_pos is exclusive\n                if is_remote:\n                    generator = _get_file_range_from_remote(\n                        client,\n                        RemoteInfo(method, url, headers),\n                        cache_file,\n                        range_start_pos,\n                        range_end_pos,\n                    )\n                else:\n                    generator = _get_file_range_from_cache(\n                        cache_file,\n                        range_start_pos,\n                        range_end_pos,\n                    )\n\n                cur_pos = range_start_pos\n                stream_cache = bytearray()\n                last_block, last_block_start_pos, last_block_end_pos = get_block_info(\n                    cur_pos, cache_file._get_block_size(), cache_file._get_file_size()\n                )\n                async for chunk in generator:\n                    if len(chunk) != 0:\n                        yield bytes(chunk)\n                        stream_cache += chunk\n                        cur_pos += len(chunk)\n\n                    cur_block = cur_pos // cache_file._get_block_size()\n\n                    if cur_block == last_block:\n                        continue\n                    split_pos = last_block_end_pos - max(\n                        last_block_start_pos, range_start_pos\n                    )\n                    raw_block = stream_cache[:split_pos]\n                    stream_cache = stream_cache[split_pos:]\n                    if len(raw_block) == cache_file._get_block_size():\n                        if not cache_file.has_block(last_block) and allow_cache:\n                            await cache_file.write_block(last_block, raw_block)\n                    last_block, last_block_start_pos, last_block_end_pos = get_block_info(\n                        cur_pos, cache_file._get_block_size(), cache_file._get_file_size()\n                    )\n\n                raw_block = stream_cache\n                if cur_block == cache_file._get_block_number() - 1:\n                    if (\n                        len(raw_block)\n                        == cache_file._get_file_size() % cache_file._get_block_size()\n                    ):\n                        raw_block += b\"\\x00\" * (\n                            cache_file._get_block_size() - len(raw_block)\n                        )\n                    last_block = cur_block\n                if len(raw_block) == cache_file._get_block_size():\n                    if not cache_file.has_block(last_block) and allow_cache:\n                        await cache_file.write_block(last_block, raw_block)\n\n                if cur_pos != range_end_pos:\n                    if is_remote:\n                        raise Exception(\n                            f\"The size of remote range ({range_end_pos - range_start_pos}) is different from sent size ({cur_pos - range_start_pos}).\"\n                        )\n                    else:\n                        raise Exception(\n                            f\"The size of cached range ({range_end_pos - range_start_pos}) is different from sent size ({cur_pos - range_start_pos}).\"\n                        )\n    finally:\n        cache_file.close()\n\n\nasync def _file_chunk_head(\n    app,\n    save_path: str,\n    head_path: str,\n    client: httpx.AsyncClient,\n    method: str,\n    url: str,\n    headers: Dict[str, str],\n    allow_cache: bool,\n    file_size: int,\n):\n    if not app.state.app_settings.config.offline:\n        async with client.stream(\n            method=method,\n            url=url,\n            headers=headers,\n            timeout=WORKER_API_TIMEOUT,\n        ) as response:\n            async for raw_chunk in response.aiter_raw():\n                if not raw_chunk:\n                    continue\n                yield raw_chunk\n    else:\n        yield b\"\"\n\n\nasync def _resource_etag(hf_url: str, authorization: Optional[str]=None, offline: bool = False) -> Optional[str]:\n    ret_etag = None\n    sha256_hash = hashlib.sha256()\n    sha256_hash.update(hf_url.encode(\"utf-8\"))\n    content_hash = sha256_hash.hexdigest()\n    if offline:\n        ret_etag = f'\"{content_hash[:32]}-10\"'\n    else:\n        etag_headers = {}\n        if authorization is not None:\n            etag_headers[\"authorization\"] = authorization\n        try:\n            async with httpx.AsyncClient() as client:\n                response = await client.request(\n                    method=\"head\",\n                    url=hf_url,\n                    headers=etag_headers,\n                    timeout=WORKER_API_TIMEOUT,\n                )\n            if \"etag\" in response.headers:\n                ret_etag = response.headers[\"etag\"]\n            else:\n                ret_etag = f'\"{content_hash[:32]}-10\"'\n        except httpx.TimeoutException:\n            ret_etag = None\n    return ret_etag\n\nasync def _file_realtime_stream(\n    app,\n    repo_type: Literal[\"models\", \"datasets\", \"spaces\"],\n    org: str,\n    repo: str,\n    file_path: str,\n    save_path: str,\n    head_path: str,\n    url: str,\n    request: Request,\n    method=\"GET\",\n    allow_cache=True,\n    commit: Optional[str] = None,\n):\n    if check_url_has_param_name(url, ORIGINAL_LOC):\n        clean_url = remove_query_param(url, ORIGINAL_LOC)\n        original_loc = get_url_param_name(url, ORIGINAL_LOC)\n\n        hf_loc = urlparse(original_loc)\n        if len(hf_loc.netloc) != 0:\n            hf_url = urljoin(\n                f\"{hf_loc.scheme}://{hf_loc.netloc}\", get_url_tail(clean_url)\n            )\n        else:\n            hf_url = urljoin(\n                app.state.app_settings.config.hf_lfs_url_base(), get_url_tail(clean_url)\n            )\n    else:\n        if urlparse(url).netloc in [\n            app.state.app_settings.config.hf_netloc,\n            app.state.app_settings.config.hf_lfs_netloc,\n        ]:\n            hf_url = url\n        else:\n            hf_url = urljoin(\n                app.state.app_settings.config.hf_lfs_url_base(), get_url_tail(url)\n            )\n\n    request_headers = {k: v for k, v in request.headers.items()}\n    if \"host\" in request_headers:\n        request_headers[\"host\"] = urlparse(hf_url).netloc\n\n    generator = pathsinfo_generator(\n        app,\n        repo_type,\n        org,\n        repo,\n        commit,\n        [file_path],\n        override_cache=False,\n        method=\"post\",\n        authorization=request.headers.get(\"authorization\", None),\n    )\n    status_code = await generator.__anext__()\n    headers = await generator.__anext__()\n    content = await generator.__anext__()\n    try:\n        pathsinfo = json.loads(content)\n    except json.JSONDecodeError:\n        response = error_proxy_invalid_data()\n        yield response.status_code\n        yield response.headers\n        yield response.body\n        return\n\n    if len(pathsinfo) == 0:\n        response = error_entry_not_found()\n        yield response.status_code\n        yield response.headers\n        yield response.body\n        return\n\n    if len(pathsinfo) != 1:\n        response = error_proxy_timeout()\n        yield response.status_code\n        yield response.headers\n        yield response.body\n        return\n\n    pathinfo = pathsinfo[0]\n    if \"size\" not in pathinfo:\n        response = error_proxy_timeout()\n        yield response.status_code\n        yield response.headers\n        yield response.body\n        return\n    file_size = pathinfo[\"size\"]\n\n    response_headers = {}\n    # Create content-length\n    unit, ranges, suffix = parse_range_params(request_headers.get(\"range\", f\"bytes={0}-{file_size-1}\"))\n    all_ranges = get_all_ranges(file_size, unit, ranges, suffix)\n    \n    response_headers[\"content-length\"] = str(sum(r[1] - r[0] for r in all_ranges))\n    if suffix is not None:\n        response_headers[\"content-range\"] = f\"bytes -{suffix}/{file_size}\"\n    else:\n        response_headers[\"content-range\"] = f\"bytes {','.join(f'{r[0]}-{r[1]-1}' for r in all_ranges)}/{file_size}\"\n    # Commit info\n    if commit is not None:\n        response_headers[HUGGINGFACE_HEADER_X_REPO_COMMIT.lower()] = commit\n    # Create fake headers when offline mode\n    etag = await _resource_etag(\n        hf_url=hf_url,\n        authorization=request.headers.get(\"authorization\", None),\n        offline=app.state.app_settings.config.offline,\n    )\n    response_headers[\"etag\"] = etag\n    \n    if etag is None:\n        error_response = error_proxy_timeout()\n        yield error_response.status_code\n        yield error_response.headers\n        yield error_response.body\n        return\n    else:\n        yield 200\n        yield response_headers\n\n    async with httpx.AsyncClient() as client:\n        if method.lower() == \"get\":\n            async for each_chunk in _file_chunk_get(\n                app=app,\n                save_path=save_path,\n                head_path=head_path,\n                client=client,\n                method=method,\n                url=hf_url,\n                headers=request_headers,\n                allow_cache=allow_cache,\n                file_size=file_size,\n            ):\n                yield each_chunk\n        elif method.lower() == \"head\":\n            async for each_chunk in _file_chunk_head(\n                app=app,\n                save_path=save_path,\n                head_path=head_path,\n                client=client,\n                method=method,\n                url=hf_url,\n                headers=request_headers,\n                allow_cache=allow_cache,\n                file_size=0,\n            ):\n                yield each_chunk\n        else:\n            raise Exception(f\"Unsupported method: {method}\")\n\n\nasync def file_get_generator(\n    app,\n    repo_type: Literal[\"models\", \"datasets\", \"spaces\"],\n    org: str,\n    repo: str,\n    commit: str,\n    file_path: str,\n    method: Literal[\"HEAD\", \"GET\"],\n    request: Request,\n):\n    org_repo = get_org_repo(org, repo)\n    # save\n    repos_path = app.state.app_settings.config.repos_path\n    head_path = os.path.join(\n        repos_path, f\"heads/{repo_type}/{org_repo}/resolve/{commit}/{file_path}\"\n    )\n    save_path = os.path.join(\n        repos_path, f\"files/{repo_type}/{org_repo}/resolve/{commit}/{file_path}\"\n    )\n    make_dirs(head_path)\n    make_dirs(save_path)\n\n    # use_cache = os.path.exists(head_path) and os.path.exists(save_path)\n    allow_cache = await check_cache_rules_hf(app, repo_type, org, repo)\n\n    # proxy\n    if repo_type == \"models\":\n        url = urljoin(\n            app.state.app_settings.config.hf_url_base(),\n            f\"/{org_repo}/resolve/{commit}/{file_path}\",\n        )\n    else:\n        url = urljoin(\n            app.state.app_settings.config.hf_url_base(),\n            f\"/{repo_type}/{org_repo}/resolve/{commit}/{file_path}\",\n        )\n    return _file_realtime_stream(\n        app=app,\n        repo_type=repo_type,\n        org=org,\n        repo=repo,\n        file_path=file_path,\n        save_path=save_path,\n        head_path=head_path,\n        url=url,\n        request=request,\n        method=method,\n        allow_cache=allow_cache,\n        commit=commit,\n    )\n\n\nasync def cdn_file_get_generator(\n    app,\n    repo_type: Literal[\"models\", \"datasets\", \"spaces\"],\n    org: str,\n    repo: str,\n    file_hash: str,\n    method: Literal[\"HEAD\", \"GET\"],\n    request: Request,\n):\n    headers = {k: v for k, v in request.headers.items()}\n    headers.pop(\"host\")\n\n    org_repo = get_org_repo(org, repo)\n    # save\n    repos_path = app.state.app_settings.config.repos_path\n    head_path = os.path.join(\n        repos_path, f\"heads/{repo_type}/{org_repo}/cdn/{file_hash}\"\n    )\n    save_path = os.path.join(\n        repos_path, f\"files/{repo_type}/{org_repo}/cdn/{file_hash}\"\n    )\n    make_dirs(head_path)\n    make_dirs(save_path)\n\n    # use_cache = os.path.exists(head_path) and os.path.exists(save_path)\n    allow_cache = await check_cache_rules_hf(app, repo_type, org, repo)\n\n    # proxy\n    # request_url = urlparse(str(request.url))\n    # if request_url.netloc == app.state.app_settings.config.hf_lfs_netloc:\n    #     redirected_url = urljoin(app.state.app_settings.config.mirror_lfs_url_base(), get_url_tail(request_url))\n    # else:\n    #     redirected_url = urljoin(app.state.app_settings.config.mirror_url_base(), get_url_tail(request_url))\n\n    return _file_realtime_stream(\n        app=app,\n        save_path=save_path,\n        head_path=head_path,\n        url=str(request.url),\n        request=request,\n        method=method,\n        allow_cache=allow_cache,\n    )\n"}
{"type": "source_file", "path": "src/olah/proxy/pathsinfo.py", "content": "# coding=utf-8\n# Copyright 2024 XiaHan\n#\n# Use of this source code is governed by an MIT-style\n# license that can be found in the LICENSE file or at\n# https://opensource.org/licenses/MIT.\n\nimport json\nimport os\nfrom typing import AsyncGenerator, Dict, List, Literal, Optional, Tuple, Union\nfrom urllib.parse import quote, urljoin\nfrom fastapi import FastAPI, Request\n\nimport httpx\nfrom olah.constants import CHUNK_SIZE, WORKER_API_TIMEOUT\n\nfrom olah.utils.cache_utils import read_cache_request, write_cache_request\nfrom olah.utils.rule_utils import check_cache_rules_hf\nfrom olah.utils.repo_utils import get_org_repo\nfrom olah.utils.file_utils import make_dirs\n\n\nasync def _pathsinfo_cache(save_path: str) -> Tuple[int, Dict[str, str], bytes]:\n    cache_rq = await read_cache_request(save_path)\n    return cache_rq[\"status_code\"], cache_rq[\"headers\"], cache_rq[\"content\"]\n\n\nasync def _pathsinfo_proxy(\n    app: FastAPI,\n    headers: Dict[str, str],\n    pathsinfo_url: str,\n    method: str,\n    path: str,\n    allow_cache: bool,\n    save_path: str,\n) -> Tuple[int, Dict[str, str], bytes]:\n    headers = {k: v for k, v in headers.items()}\n    if \"content-length\" in headers:\n        headers.pop(\"content-length\")\n    async with httpx.AsyncClient(follow_redirects=True) as client:\n        response = await client.request(\n            method=method,\n            url=pathsinfo_url,\n            headers=headers,\n            data={\"paths\": path},\n            timeout=WORKER_API_TIMEOUT,\n        )\n\n        if allow_cache and response.status_code == 200:\n            make_dirs(save_path)\n            await write_cache_request(\n                save_path,\n                response.status_code,\n                response.headers,\n                bytes(response.content),\n            )\n    return response.status_code, response.headers, response.content\n\n\nasync def pathsinfo_generator(\n    app: FastAPI,\n    repo_type: Literal[\"models\", \"datasets\", \"spaces\"],\n    org: str,\n    repo: str,\n    commit: str,\n    paths: List[str],\n    override_cache: bool,\n    method: str,\n    authorization: Optional[str],\n) -> AsyncGenerator[Union[int, Dict[str, str], bytes], None]:\n    headers = {}\n    if authorization is not None:\n        headers[\"authorization\"] = authorization\n\n    org_repo = get_org_repo(org, repo)\n    # save\n    repos_path = app.state.app_settings.config.repos_path\n\n    final_content = []\n    for path in paths:\n        save_dir = os.path.join(\n            repos_path, f\"api/{repo_type}/{org_repo}/paths-info/{commit}/{path}\"\n        )\n\n        save_path = os.path.join(save_dir, f\"paths-info_{method}.json\")\n\n        use_cache = os.path.exists(save_path)\n        allow_cache = await check_cache_rules_hf(app, repo_type, org, repo)\n\n        org_repo = get_org_repo(org, repo)\n        pathsinfo_url = urljoin(\n            app.state.app_settings.config.hf_url_base(),\n            f\"/api/{repo_type}/{org_repo}/paths-info/{commit}\",\n        )\n        # proxy\n        if use_cache and not override_cache:\n            status, headers, content = await _pathsinfo_cache(save_path)\n        else:\n            status, headers, content = await _pathsinfo_proxy(\n                app, headers, pathsinfo_url, method, path, allow_cache, save_path\n            )\n\n        try:\n            content_json = json.loads(content)\n        except json.JSONDecodeError:\n            continue\n        if status == 200 and isinstance(content_json, list):\n            final_content.extend(content_json)\n\n    yield 200\n    yield {'content-type': 'application/json'}\n    yield json.dumps(final_content, ensure_ascii=True)\n"}
{"type": "source_file", "path": "src/olah/utils/file_utils.py", "content": "# coding=utf-8\n# Copyright 2024 XiaHan\n#\n# Use of this source code is governed by an MIT-style\n# license that can be found in the LICENSE file or at\n# https://opensource.org/licenses/MIT.\n\nimport os\n\n\ndef make_dirs(path: str):\n    if os.path.isdir(path):\n        save_dir = path\n    else:\n        save_dir = os.path.dirname(path)\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir, exist_ok=True)\n"}
{"type": "source_file", "path": "src/olah/mirror/meta.py", "content": "# coding=utf-8\n# Copyright 2024 XiaHan\n#\n# Use of this source code is governed by an MIT-style\n# license that can be found in the LICENSE file or at\n# https://opensource.org/licenses/MIT.\n\n\nfrom typing import Any, Dict\n\n\nclass RepoMeta(object):\n    def __init__(self) -> None:\n        self._id = None\n        self.id = None\n        self.author = None\n        self.sha = None\n        self.lastModified = None\n        self.private = False\n        self.gated = False\n        self.disabled = False\n        self.tags = []\n        self.description = \"\"\n        self.paperswithcode_id = None\n        self.downloads = 0\n        self.likes = 0\n        self.cardData = None\n        self.siblings = None\n        self.createdAt = None\n\n    def to_dict(self) -> Dict[str, Any]:\n        return {\n            \"_id\": self._id,\n            \"id\": self.id,\n            \"author\": self.author,\n            \"sha\": self.sha,\n            \"lastModified\": self.lastModified,\n            \"private\": self.private,\n            \"gated\": self.gated,\n            \"disabled\": self.disabled,\n            \"tags\": self.tags,\n            \"description\": self.description,\n            \"paperswithcode_id\": self.paperswithcode_id,\n            \"downloads\": self.downloads,\n            \"likes\": self.likes,\n            \"cardData\": self.cardData,\n            \"siblings\": self.siblings,\n            \"createdAt\": self.createdAt,\n        }\n"}
{"type": "source_file", "path": "src/olah/server.py", "content": "# coding=utf-8\n# Copyright 2024 XiaHan\n#\n# Use of this source code is governed by an MIT-style\n# license that can be found in the LICENSE file or at\n# https://opensource.org/licenses/MIT.\n\nfrom contextlib import asynccontextmanager\nimport datetime\nimport os\nimport glob\nimport argparse\nimport sys\nimport time\nimport traceback\nfrom typing import Annotated, List, Literal, Optional, Sequence, Tuple, Union\nfrom urllib.parse import urljoin\nfrom fastapi import FastAPI, Header, Request, Form\nfrom fastapi.responses import (\n    FileResponse,\n    HTMLResponse,\n    StreamingResponse,\n    Response,\n    JSONResponse,\n)\nfrom fastapi.templating import Jinja2Templates\nfrom fastapi_utils.tasks import repeat_every\n\nimport git\nimport httpx\n\nfrom olah.proxy.commits import commits_generator\nfrom olah.proxy.pathsinfo import pathsinfo_generator\nfrom olah.proxy.tree import tree_generator\nfrom olah.utils.disk_utils import convert_bytes_to_human_readable, convert_to_bytes, get_folder_size, sort_files_by_access_time, sort_files_by_modify_time, sort_files_by_size\nfrom olah.utils.url_utils import clean_path\n\nBASE_SETTINGS = False\nif not BASE_SETTINGS:\n    try:\n        from pydantic import BaseSettings\n        BASE_SETTINGS = True\n    except ImportError:\n        BASE_SETTINGS = False\n\nif not BASE_SETTINGS:\n    try:\n        from pydantic_settings import BaseSettings\n        BASE_SETTINGS = True\n    except ImportError:\n        BASE_SETTINGS = False\n\nif not BASE_SETTINGS:\n    raise Exception(\"Cannot import BaseSettings from pydantic or pydantic-settings\")\n\nfrom olah.configs import OlahConfig\nfrom olah.errors import error_repo_not_found, error_page_not_found, error_revision_not_found\nfrom olah.mirror.repos import LocalMirrorRepo\nfrom olah.proxy.files import cdn_file_get_generator, file_get_generator\nfrom olah.proxy.lfs import lfs_get_generator, lfs_head_generator\nfrom olah.proxy.meta import meta_generator\nfrom olah.utils.rule_utils import check_proxy_rules_hf, get_org_repo\nfrom olah.utils.repo_utils import (\n    check_commit_hf,\n    get_commit_hf,\n    get_newest_commit_hf,\n    parse_org_repo,\n)\nfrom olah.constants import OLAH_CODE_DIR, REPO_TYPES_MAPPING\nfrom olah.utils.logging import build_logger\n\nlogger = None\n\n# ======================\n# Utilities\n# ======================\nasync def check_connection(url: str) -> bool:\n    try:\n        async with httpx.AsyncClient() as client:\n            response = await client.request(\n                method=\"HEAD\",\n                url=url,\n                timeout=10,\n            )\n        if response.status_code != 200:\n            return False\n        else:\n            return True\n    except httpx.TimeoutException:\n        return False\n\n# from pympler import tracker, classtracker\n# tr = tracker.SummaryTracker()\n# cr = classtracker.ClassTracker()\n# from olah.cache.bitset import Bitset\n# from olah.cache.olah_cache import OlahCache, OlahCacheHeader\n# cr.track_class(Bitset)\n# cr.track_class(OlahCacheHeader)\n# cr.track_class(OlahCache)\n\n@repeat_every(seconds=60*5)\nasync def check_hf_connection() -> None:\n    if app.state.app_settings.config.offline:\n        return\n    scheme = app.state.app_settings.config.hf_scheme\n    netloc = app.state.app_settings.config.hf_netloc\n    hf_online_status = await check_connection(\n        f\"{scheme}://{netloc}/datasets/Salesforce/wikitext/resolve/main/.gitattributes\"\n    )\n    if not hf_online_status:\n        print(\"Failed to reach Huggingface Site.\", file=sys.stderr)\n\n@repeat_every(seconds=60 * 60)\nasync def check_disk_usage() -> None:\n    if app.state.app_settings.config.offline:\n        return\n    if app.state.app_settings.config.cache_size_limit is None:\n        return\n\n    limit_size = app.state.app_settings.config.cache_size_limit\n    current_size = get_folder_size(app.state.app_settings.config.repos_path)\n\n    limit_size_h = convert_bytes_to_human_readable(limit_size)\n    current_size_h = convert_bytes_to_human_readable(current_size)\n\n    if current_size < limit_size:\n        return\n    print(\n        f\"Cache size exceeded! Limit: {limit_size_h}, Current: {current_size_h}.\"\n    )\n    print(\"Cleaning...\")\n    files_path = os.path.join(app.state.app_settings.config.repos_path, \"files\")\n    lfs_path = os.path.join(app.state.app_settings.config.repos_path, \"lfs\")\n\n    files: Sequence[Tuple[str, Union[int, datetime.datetime]]] = []\n    if app.state.app_settings.config.cache_clean_strategy == \"LRU\":\n        files = sort_files_by_access_time(files_path) + sort_files_by_access_time(\n            lfs_path\n        )\n        files = sorted(files, key=lambda x: x[1])\n    elif app.state.app_settings.config.cache_clean_strategy == \"FIFO\":\n        files = sort_files_by_modify_time(files_path) + sort_files_by_modify_time(\n            lfs_path\n        )\n        files = sorted(files, key=lambda x: x[1])\n    elif app.state.app_settings.config.cache_clean_strategy == \"LARGE_FIRST\":\n        files = sort_files_by_size(files_path) + sort_files_by_size(lfs_path)\n        files = sorted(files, key=lambda x: x[1], reverse=True)\n\n    for filepath, index in files:\n        if current_size < limit_size:\n            break\n        filesize = os.path.getsize(filepath)\n        os.remove(filepath)\n        current_size -= filesize\n        print(f\"Remove file: {filepath}. File Size: {convert_bytes_to_human_readable(filesize)}\")\n\n    current_size = get_folder_size(app.state.app_settings.config.repos_path)\n    current_size_h = convert_bytes_to_human_readable(current_size)\n    print(f\"Cleaning finished. Limit: {limit_size_h}, Current: {current_size_h}.\")\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # TODO: Check repo cache path\n    await check_hf_connection()\n    await check_disk_usage()\n    yield\n\n\n# ======================\n# Application\n# ======================\ncode_file_path = os.path.abspath(__file__)\napp = FastAPI(lifespan=lifespan, debug=False)\ntemplates = Jinja2Templates(directory=os.path.join(OLAH_CODE_DIR, \"static\"))\n\n\nclass AppSettings(BaseSettings):\n    # The address of the model controller.\n    config: OlahConfig = OlahConfig()\n\n\n# ======================\n# Exception handlers\n# ======================\n@app.exception_handler(404)\nasync def custom_404_handler(_, __):\n    return error_page_not_found()\n\n\n# ======================\n# File Meta Info API Hooks\n# See also: https://huggingface.co/docs/hub/api#repo-listing-api\n# ======================\nasync def meta_proxy_common(repo_type: Literal[\"models\", \"datasets\", \"spaces\"], org: str, repo: str, commit: str, method: str, authorization: Optional[str]) -> Response:\n    # FIXME: do not show the private repos to other user besides owner, even though the repo was cached\n    if repo_type not in REPO_TYPES_MAPPING.keys():\n        return error_page_not_found()\n    if not await check_proxy_rules_hf(app, repo_type, org, repo):\n        return error_repo_not_found()\n    # Check Mirror Path\n    for mirror_path in app.state.app_settings.config.mirrors_path:\n        git_path = os.path.join(mirror_path, repo_type, org, repo)\n        try:\n            git_path = os.path.join(mirror_path, repo_type, org or '', repo)\n            if os.path.exists(git_path):\n                local_repo = LocalMirrorRepo(git_path, repo_type, org, repo)\n                meta_data = local_repo.get_meta(commit)\n                if meta_data is None:\n                    continue\n                return JSONResponse(content=meta_data)\n        except git.exc.InvalidGitRepositoryError:\n            logger.warning(f\"Local repository {git_path} is not a valid git reposity.\")\n            continue\n\n    # Proxy the HF File Meta\n    try:\n        if not app.state.app_settings.config.offline:\n            if not await check_commit_hf(app, repo_type, org, repo, commit=None,\n                authorization=authorization,\n            ):\n                return error_repo_not_found()\n            if not await check_commit_hf(app, repo_type, org, repo, commit=commit,\n                authorization=authorization,\n            ):\n                return error_revision_not_found(revision=commit)\n        commit_sha = await get_commit_hf(app, repo_type, org, repo, commit=commit,\n            authorization=authorization,\n        )\n        if commit_sha is None:\n            return error_repo_not_found()\n        # if branch name and online mode, refresh branch info\n        if not app.state.app_settings.config.offline and commit_sha != commit:\n            generator = meta_generator(\n                app=app,\n                repo_type=repo_type,\n                org=org,\n                repo=repo,\n                commit=commit,\n                override_cache=True,\n                method=method,\n                authorization=authorization,\n            )\n            async for _ in generator:\n                pass\n            generator = meta_generator(\n                app=app,\n                repo_type=repo_type,\n                org=org,\n                repo=repo,\n                commit=commit_sha,\n                override_cache=True,\n                method=method,\n                authorization=authorization,\n            )\n        else:\n            generator = meta_generator(\n                app=app,\n                repo_type=repo_type,\n                org=org,\n                repo=repo,\n                commit=commit_sha,\n                override_cache=False,\n                method=method,\n                authorization=authorization,\n            )\n        headers = await generator.__anext__()\n        return StreamingResponse(generator, headers=headers)\n    except httpx.ConnectTimeout:\n        traceback.print_exc()\n        return Response(status_code=504)\n\n\n@app.head(\"/api/{repo_type}/{org_repo}\")\n@app.get(\"/api/{repo_type}/{org_repo}\")\nasync def meta_proxy(repo_type: str, org_repo: str, request: Request):\n    org, repo = parse_org_repo(org_repo)\n    if org is None and repo is None:\n        return error_repo_not_found()\n    if not app.state.app_settings.config.offline:\n        new_commit = await get_newest_commit_hf(\n            app,\n            repo_type,\n            org,\n            repo,\n            authorization=request.headers.get(\"authorization\", None),\n        )\n        if new_commit is None:\n            return error_repo_not_found()\n    else:\n        new_commit = \"main\"\n    return await meta_proxy_common(\n        repo_type=repo_type,\n        org=org,\n        repo=repo,\n        commit=new_commit,\n        method=request.method.lower(),\n        authorization=request.headers.get(\"authorization\", None),\n    )\n\n\n@app.head(\"/api/{repo_type}/{org}/{repo}\")\n@app.get(\"/api/{repo_type}/{org}/{repo}\")\nasync def meta_proxy(repo_type: str, org: str, repo: str, request: Request):\n    if not app.state.app_settings.config.offline:\n        new_commit = await get_newest_commit_hf(\n            app,\n            repo_type,\n            org,\n            repo,\n            authorization=request.headers.get(\"authorization\", None),\n        )\n        if new_commit is None:\n            return error_repo_not_found()\n    else:\n        new_commit = \"main\"\n    return await meta_proxy_common(\n        repo_type=repo_type,\n        org=org,\n        repo=repo,\n        commit=new_commit,\n        method=request.method.lower(),\n        authorization=request.headers.get(\"authorization\", None),\n    )\n\n\n@app.head(\"/api/{repo_type}/{org}/{repo}/revision/{commit}\")\n@app.get(\"/api/{repo_type}/{org}/{repo}/revision/{commit}\")\nasync def meta_proxy_commit2(\n    repo_type: str, org: str, repo: str, commit: str, request: Request\n):\n    return await meta_proxy_common(\n        repo_type=repo_type,\n        org=org,\n        repo=repo,\n        commit=commit,\n        method=request.method.lower(),\n        authorization=request.headers.get(\"authorization\", None),\n    )\n\n\n@app.head(\"/api/{repo_type}/{org_repo}/revision/{commit}\")\n@app.get(\"/api/{repo_type}/{org_repo}/revision/{commit}\")\nasync def meta_proxy_commit(\n    repo_type: str, org_repo: str, commit: str, request: Request\n):\n    org, repo = parse_org_repo(org_repo)\n    if org is None and repo is None:\n        return error_repo_not_found()\n\n    return await meta_proxy_common(\n        repo_type=repo_type,\n        org=org,\n        repo=repo,\n        commit=commit,\n        method=request.method.lower(),\n        authorization=request.headers.get(\"authorization\", None),\n    )\n\n\n# Git Tree\nasync def tree_proxy_common(\n    repo_type: str,\n    org: str,\n    repo: str,\n    commit: str,\n    path: str,\n    recursive: bool,\n    expand: bool,\n    method: str,\n    authorization: Optional[str]\n) -> Response:\n    # FIXME: do not show the private repos to other user besides owner, even though the repo was cached\n    path = clean_path(path)\n    if repo_type not in REPO_TYPES_MAPPING.keys():\n        return error_page_not_found()\n    if not await check_proxy_rules_hf(app, repo_type, org, repo):\n        return error_repo_not_found()\n    # Check Mirror Path\n    for mirror_path in app.state.app_settings.config.mirrors_path:\n        git_path = os.path.join(mirror_path, repo_type, org, repo)\n        try:\n            git_path = os.path.join(mirror_path, repo_type, org or '', repo)\n            if os.path.exists(git_path):\n                local_repo = LocalMirrorRepo(git_path, repo_type, org, repo)\n                tree_data = local_repo.get_tree(commit, path, recursive=recursive, expand=expand)\n                if tree_data is None:\n                    continue\n                return JSONResponse(content=tree_data)\n        except git.exc.InvalidGitRepositoryError:\n            logger.warning(f\"Local repository {git_path} is not a valid git reposity.\")\n            continue\n\n    # Proxy the HF File Meta\n    try:\n        if not app.state.app_settings.config.offline:\n            if not await check_commit_hf(app, repo_type, org, repo, commit=None,\n                authorization=authorization,\n            ):\n                return error_repo_not_found()\n            if not await check_commit_hf(app, repo_type, org, repo, commit=commit,\n                authorization=authorization,\n            ):\n                return error_revision_not_found(revision=commit)\n        commit_sha = await get_commit_hf(app, repo_type, org, repo, commit=commit,\n            authorization=authorization,\n        )\n        if commit_sha is None:\n            return error_repo_not_found()\n        # if branch name and online mode, refresh branch info\n        if not app.state.app_settings.config.offline and commit_sha != commit:\n            generator = tree_generator(\n                app=app,\n                repo_type=repo_type,\n                org=org,\n                repo=repo,\n                commit=commit,\n                path=path,\n                recursive=recursive,\n                expand=expand,\n                override_cache=True,\n                method=method,\n                authorization=authorization,\n            )\n            async for _ in generator:\n                pass\n            generator = tree_generator(\n                app=app,\n                repo_type=repo_type,\n                org=org,\n                repo=repo,\n                commit=commit_sha,\n                path=path,\n                recursive=recursive,\n                expand=expand,\n                override_cache=True,\n                method=method,\n                authorization=authorization,\n            )\n        else:\n            generator = tree_generator(\n                app=app,\n                repo_type=repo_type,\n                org=org,\n                repo=repo,\n                commit=commit_sha,\n                path=path,\n                recursive=recursive,\n                expand=expand,\n                override_cache=False,\n                method=method,\n                authorization=authorization,\n            )\n\n        status_code = await generator.__anext__()\n        headers = await generator.__anext__()\n        return StreamingResponse(generator, status_code=status_code, headers=headers)\n    except httpx.ConnectTimeout:\n        traceback.print_exc()\n        return Response(status_code=504)\n\n\n@app.head(\"/api/{repo_type}/{org}/{repo}/tree/{commit}/{file_path:path}\")\n@app.get(\"/api/{repo_type}/{org}/{repo}/tree/{commit}/{file_path:path}\")\nasync def tree_proxy_commit2(\n    repo_type: str,\n    org: str,\n    repo: str,\n    commit: str,\n    file_path: str,\n    request: Request,\n    recursive: bool = False,\n    expand: bool=False,\n):\n    return await tree_proxy_common(\n        repo_type=repo_type,\n        org=org,\n        repo=repo,\n        commit=commit,\n        path=file_path,\n        recursive=recursive,\n        expand=expand,\n        method=request.method.lower(),\n        authorization=request.headers.get(\"authorization\", None),\n    )\n\n\n@app.head(\"/api/{repo_type}/{org_repo}/tree/{commit}/{file_path:path}\")\n@app.get(\"/api/{repo_type}/{org_repo}/tree/{commit}/{file_path:path}\")\nasync def tree_proxy_commit(\n    repo_type: str,\n    org_repo: str,\n    commit: str,\n    file_path: str,\n    request: Request,\n    recursive: bool = False,\n    expand: bool=False,\n):\n    org, repo = parse_org_repo(org_repo)\n    if org is None and repo is None:\n        return error_repo_not_found()\n\n    return await tree_proxy_common(\n        repo_type=repo_type,\n        org=org,\n        repo=repo,\n        commit=commit,\n        path=file_path,\n        recursive=recursive,\n        expand=expand,\n        method=request.method.lower(),\n        authorization=request.headers.get(\"authorization\", None),\n    )\n\n# Git Pathsinfo\nasync def pathsinfo_proxy_common(repo_type: str, org: str, repo: str, commit: str, paths: List[str], method: str, authorization: Optional[str]) -> Response:\n    # TODO: the head method of meta apis\n    # FIXME: do not show the private repos to other user besides owner, even though the repo was cached\n    paths = [clean_path(path) for path in paths]\n    if repo_type not in REPO_TYPES_MAPPING.keys():\n        return error_page_not_found()\n    if not await check_proxy_rules_hf(app, repo_type, org, repo):\n        return error_repo_not_found()\n    # Check Mirror Path\n    for mirror_path in app.state.app_settings.config.mirrors_path:\n        git_path = os.path.join(mirror_path, repo_type, org, repo)\n        try:\n            git_path = os.path.join(mirror_path, repo_type, org or '', repo)\n            if os.path.exists(git_path):\n                local_repo = LocalMirrorRepo(git_path, repo_type, org, repo)\n                pathsinfo_data = local_repo.get_pathinfos(commit, paths)\n                if pathsinfo_data is None:\n                    continue\n                return JSONResponse(content=pathsinfo_data)\n        except git.exc.InvalidGitRepositoryError:\n            logger.warning(f\"Local repository {git_path} is not a valid git reposity.\")\n            continue\n\n    # Proxy the HF File pathsinfo\n    try:\n        if not app.state.app_settings.config.offline:\n            if not await check_commit_hf(app, repo_type, org, repo, commit=None,\n                authorization=authorization,\n            ):\n                return error_repo_not_found()\n            if not await check_commit_hf(app, repo_type, org, repo, commit=commit,\n                authorization=authorization,\n            ):\n                return error_revision_not_found(revision=commit)\n        commit_sha = await get_commit_hf(app, repo_type, org, repo, commit=commit,\n            authorization=authorization,\n        )\n        if commit_sha is None:\n            return error_repo_not_found()\n        if not app.state.app_settings.config.offline and commit_sha != commit:\n            generator = pathsinfo_generator(\n                app=app,\n                repo_type=repo_type,\n                org=org,\n                repo=repo,\n                commit=commit,\n                paths=paths,\n                override_cache=True,\n                method=method,\n                authorization=authorization,\n            )\n            async for _ in generator:\n                pass\n            generator = pathsinfo_generator(\n                app=app,\n                repo_type=repo_type,\n                org=org,\n                repo=repo,\n                commit=commit_sha,\n                paths=paths,\n                override_cache=True,\n                method=method,\n                authorization=authorization,\n            )\n        else:\n            generator = pathsinfo_generator(\n                app,\n                repo_type,\n                org,\n                repo,\n                commit_sha,\n                paths,\n                override_cache=False,\n                method=method,\n                authorization=authorization,\n            )\n        status_code = await generator.__anext__()\n        headers = await generator.__anext__()\n        return StreamingResponse(generator, status_code=status_code, headers=headers)\n    except httpx.ConnectTimeout:\n        traceback.print_exc()\n        return Response(status_code=504)\n\n\n@app.head(\"/api/{repo_type}/{org}/{repo}/paths-info/{commit}\")\n@app.post(\"/api/{repo_type}/{org}/{repo}/paths-info/{commit}\")\nasync def pathsinfo_proxy_commit2(\n    repo_type: str,\n    org: str,\n    repo: str,\n    commit: str,\n    paths: Annotated[List[str], Form()],\n    request: Request,\n):\n    return await pathsinfo_proxy_common(\n        repo_type=repo_type,\n        org=org,\n        repo=repo,\n        commit=commit,\n        paths=paths,\n        method=request.method.lower(),\n        authorization=request.headers.get(\"authorization\", None),\n    )\n\n\n@app.head(\"/api/{repo_type}/{org_repo}/paths-info/{commit}\")\n@app.post(\"/api/{repo_type}/{org_repo}/paths-info/{commit}\")\nasync def pathsinfo_proxy_commit(\n    repo_type: str,\n    org_repo: str,\n    commit: str,\n    paths: Annotated[List[str], Form()],\n    request: Request,\n):\n    org, repo = parse_org_repo(org_repo)\n    if org is None and repo is None:\n        return error_repo_not_found()\n\n    return await pathsinfo_proxy_common(\n        repo_type=repo_type,\n        org=org,\n        repo=repo,\n        commit=commit,\n        paths=paths,\n        method=request.method.lower(),\n        authorization=request.headers.get(\"authorization\", None),\n    )\n\n\n# Git Commits\nasync def commits_proxy_common(repo_type: str, org: str, repo: str, commit: str, method: str, authorization: Optional[str]) -> Response:\n    # FIXME: do not show the private repos to other user besides owner, even though the repo was cached\n    if repo_type not in REPO_TYPES_MAPPING.keys():\n        return error_page_not_found()\n    if not await check_proxy_rules_hf(app, repo_type, org, repo):\n        return error_repo_not_found()\n    # Check Mirror Path\n    for mirror_path in app.state.app_settings.config.mirrors_path:\n        try:\n            git_path = os.path.join(mirror_path, repo_type, org or '', repo)\n            if os.path.exists(git_path):\n                local_repo = LocalMirrorRepo(git_path, repo_type, org, repo)\n                commits_data = local_repo.get_commits(commit)\n                if commits_data is None:\n                    continue\n                return JSONResponse(content=commits_data)\n        except git.exc.InvalidGitRepositoryError:\n            logger.warning(f\"Local repository {git_path} is not a valid git reposity.\")\n            continue\n\n    # Proxy the HF File Commits\n    try:\n        if not app.state.app_settings.config.offline:\n            if not await check_commit_hf(app, repo_type, org, repo, commit=None,\n                authorization=authorization,\n            ):\n                return error_repo_not_found()\n            if not await check_commit_hf(app, repo_type, org, repo, commit=commit,\n                authorization=authorization,\n            ):\n                return error_revision_not_found(revision=commit)\n        commit_sha = await get_commit_hf(app, repo_type, org, repo, commit=commit,\n            authorization=authorization,\n        )\n        if commit_sha is None:\n            return error_repo_not_found()\n        # if branch name and online mode, refresh branch info\n        if not app.state.app_settings.config.offline and commit_sha != commit:\n            generator = commits_generator(\n                app=app,\n                repo_type=repo_type,\n                org=org,\n                repo=repo,\n                commit=commit,\n                override_cache=True,\n                method=method,\n                authorization=authorization,\n            )\n            async for _ in generator:\n                pass\n            generator = commits_generator(\n                app=app,\n                repo_type=repo_type,\n                org=org,\n                repo=repo,\n                commit=commit_sha,\n                override_cache=True,\n                method=method,\n                authorization=authorization,\n            )\n        else:\n            generator = commits_generator(\n                app=app,\n                repo_type=repo_type,\n                org=org,\n                repo=repo,\n                commit=commit_sha,\n                override_cache=False,\n                method=method,\n                authorization=authorization,\n            )\n        status_code = await generator.__anext__()\n        headers = await generator.__anext__()\n        return StreamingResponse(generator, status_code=status_code, headers=headers)\n    except httpx.ConnectTimeout:\n        traceback.print_exc()\n        return Response(status_code=504)\n\n\n@app.head(\"/api/{repo_type}/{org}/{repo}/commits/{commit}\")\n@app.get(\"/api/{repo_type}/{org}/{repo}/commits/{commit}\")\nasync def commits_proxy_commit2(\n    repo_type: str, org: str, repo: str, commit: str, request: Request\n):\n    return await commits_proxy_common(\n        repo_type=repo_type,\n        org=org,\n        repo=repo,\n        commit=commit,\n        method=request.method.lower(),\n        authorization=request.headers.get(\"authorization\", None),\n    )\n\n\n@app.head(\"/api/{repo_type}/{org_repo}/commits/{commit}\")\n@app.get(\"/api/{repo_type}/{org_repo}/commits/{commit}\")\nasync def commits_proxy_commit(\n    repo_type: str, org_repo: str, commit: str, request: Request\n):\n    org, repo = parse_org_repo(org_repo)\n    if org is None and repo is None:\n        return error_repo_not_found()\n\n    return await commits_proxy_common(\n        repo_type=repo_type,\n        org=org,\n        repo=repo,\n        commit=commit,\n        method=request.method.lower(),\n        authorization=request.headers.get(\"authorization\", None),\n    )\n\n\n# ======================\n# Authentication API Hooks\n# ======================\n@app.get(\"/api/whoami-v2\")\nasync def whoami_v2(request: Request):\n    \"\"\"\n    Sensitive Information!!! \n    \"\"\"\n    new_headers = {k.lower(): v for k, v in request.headers.items()}\n    new_headers[\"host\"] = app.state.app_settings.config.hf_netloc\n    async with httpx.AsyncClient() as client:\n        response = await client.request(\n            method=\"GET\",\n            url=urljoin(app.state.app_settings.config.hf_url_base(), \"/api/whoami-v2\"),\n            headers=new_headers,\n            timeout=10,\n        )\n    # final_content = decompress_data(response.headers.get(\"content-encoding\", None))\n    response_headers = {k.lower(): v for k, v in response.headers.items()}\n    if \"content-encoding\" in response_headers:\n        response_headers.pop(\"content-encoding\")\n    if \"content-length\" in response_headers:\n        response_headers.pop(\"content-length\")\n    return Response(\n        content=response.content,\n        status_code=response.status_code,\n        headers=response_headers,\n    )\n\n\n# ======================\n# File Head Hooks\n# ======================\nasync def file_head_common(\n    repo_type: str, org: str, repo: str, commit: str, file_path: str, request: Request\n) -> Response:\n    if repo_type not in REPO_TYPES_MAPPING.keys():\n        return error_page_not_found()\n    if not await check_proxy_rules_hf(app, repo_type, org, repo):\n        return error_repo_not_found()\n\n    # Check Mirror Path\n    for mirror_path in app.state.app_settings.config.mirrors_path:\n        git_path = os.path.join(mirror_path, repo_type, org, repo)\n        try:\n            git_path = os.path.join(mirror_path, repo_type, org or '', repo)\n            if os.path.exists(git_path):\n                local_repo = LocalMirrorRepo(git_path, repo_type, org, repo)\n                head = local_repo.get_file_head(commit_hash=commit, path=file_path)\n                if head is None:\n                    continue\n                return Response(headers=head)\n        except git.exc.InvalidGitRepositoryError:\n            logger.warning(f\"Local repository {git_path} is not a valid git reposity.\")\n            continue\n\n    # Proxy the HF File Head\n    try:\n        if not app.state.app_settings.config.offline and not await check_commit_hf(\n            app,\n            repo_type,\n            org,\n            repo,\n            commit=commit,\n            authorization=request.headers.get(\"authorization\", None),\n        ):\n            return error_repo_not_found()\n        commit_sha = await get_commit_hf(\n            app,\n            repo_type,\n            org,\n            repo,\n            commit=commit,\n            authorization=request.headers.get(\"authorization\", None),\n        )\n        if commit_sha is None:\n            return error_repo_not_found()\n        generator = await file_get_generator(\n            app,\n            repo_type,\n            org,\n            repo,\n            commit_sha,\n            file_path=file_path,\n            method=\"HEAD\",\n            request=request,\n        )\n        status_code = await generator.__anext__()\n        headers = await generator.__anext__()\n        return StreamingResponse(generator, headers=headers, status_code=status_code)\n    except httpx.ConnectTimeout:\n        traceback.print_exc()\n        return Response(status_code=504)\n\n\n@app.head(\"/{repo_type}/{org}/{repo}/resolve/{commit}/{file_path:path}\")\nasync def file_head3(\n    repo_type: str, org: str, repo: str, commit: str, file_path: str, request: Request\n):\n    return await file_head_common(\n        repo_type=repo_type,\n        org=org,\n        repo=repo,\n        commit=commit,\n        file_path=file_path,\n        request=request,\n    )\n\n\n@app.head(\"/{org_or_repo_type}/{repo_name}/resolve/{commit}/{file_path:path}\")\nasync def file_head2(\n    org_or_repo_type: str, repo_name: str, commit: str, file_path: str, request: Request\n):\n    if org_or_repo_type in REPO_TYPES_MAPPING.keys():\n        repo_type: str = org_or_repo_type\n        org, repo = parse_org_repo(repo_name)\n        if org is None and repo is None:\n            return error_repo_not_found()\n    else:\n        repo_type: str = \"models\"\n        org, repo = org_or_repo_type, repo_name\n\n    return await file_head_common(\n        repo_type=repo_type,\n        org=org,\n        repo=repo,\n        commit=commit,\n        file_path=file_path,\n        request=request,\n    )\n\n\n@app.head(\"/{org_repo}/resolve/{commit}/{file_path:path}\")\nasync def file_head(org_repo: str, commit: str, file_path: str, request: Request):\n    repo_type: str = \"models\"\n    org, repo = parse_org_repo(org_repo)\n    if org is None and repo is None:\n        return error_repo_not_found()\n    return await file_head_common(\n        repo_type=repo_type,\n        org=org,\n        repo=repo,\n        commit=commit,\n        file_path=file_path,\n        request=request,\n    )\n\n\n@app.head(\"/{org_repo}/{hash_file}\")\n@app.head(\"/{repo_type}/{org_repo}/{hash_file}\")\nasync def cdn_file_head(org_repo: str, hash_file: str, request: Request, repo_type: str = \"models\"):\n    org, repo = parse_org_repo(org_repo)\n    if org is None and repo is None:\n        return error_repo_not_found()\n\n    if not await check_proxy_rules_hf(app, repo_type, org, repo):\n        return error_repo_not_found()\n\n    try:\n        generator = await cdn_file_get_generator(app, repo_type, org, repo, hash_file, method=\"HEAD\", request=request)\n        status_code = await generator.__anext__()\n        headers = await generator.__anext__()\n        return StreamingResponse(generator, headers=headers, status_code=status_code)\n    except httpx.ConnectTimeout:\n        return Response(status_code=504)\n\n\n# ======================\n# File Hooks\n# ======================\nasync def file_get_common(\n    repo_type: str, org: str, repo: str, commit: str, file_path: str, request: Request\n) -> Response:\n    if repo_type not in REPO_TYPES_MAPPING.keys():\n        return error_page_not_found()\n    if not await check_proxy_rules_hf(app, repo_type, org, repo):\n        return error_repo_not_found()\n    # Check Mirror Path\n    for mirror_path in app.state.app_settings.config.mirrors_path:\n        try:\n            git_path = os.path.join(mirror_path, repo_type, org or '', repo)\n            if os.path.exists(git_path):\n                local_repo = LocalMirrorRepo(git_path, repo_type, org, repo)\n                content_stream = local_repo.get_file(commit_hash=commit, path=file_path)\n                if content_stream is None:\n                    continue\n                return StreamingResponse(content_stream)\n        except git.exc.InvalidGitRepositoryError:\n            logger.warning(f\"Local repository {git_path} is not a valid git reposity.\")\n            continue\n    try:\n        if not app.state.app_settings.config.offline and not await check_commit_hf(\n            app,\n            repo_type,\n            org,\n            repo,\n            commit=commit,\n            authorization=request.headers.get(\"authorization\", None),\n        ):\n            return error_repo_not_found()\n        commit_sha = await get_commit_hf(\n            app,\n            repo_type,\n            org,\n            repo,\n            commit=commit,\n            authorization=request.headers.get(\"authorization\", None),\n        )\n        if commit_sha is None:\n            return error_repo_not_found()\n        generator = await file_get_generator(\n            app,\n            repo_type,\n            org,\n            repo,\n            commit_sha,\n            file_path=file_path,\n            method=\"GET\",\n            request=request,\n        )\n        status_code = await generator.__anext__()\n        headers = await generator.__anext__()\n        return StreamingResponse(generator, headers=headers, status_code=status_code)\n    except httpx.ConnectTimeout:\n        traceback.print_exc()\n        return Response(status_code=504)\n\n\n@app.get(\"/{repo_type}/{org}/{repo}/resolve/{commit}/{file_path:path}\")\nasync def file_get3(\n    org: str, repo: str, commit: str, file_path: str, request: Request, repo_type: str\n):\n    return await file_get_common(\n        repo_type=repo_type,\n        org=org,\n        repo=repo,\n        commit=commit,\n        file_path=file_path,\n        request=request,\n    )\n\n\n@app.get(\"/{org_or_repo_type}/{repo_name}/resolve/{commit}/{file_path:path}\")\nasync def file_get2(\n    org_or_repo_type: str, repo_name: str, commit: str, file_path: str, request: Request\n):\n    if org_or_repo_type in REPO_TYPES_MAPPING.keys():\n        repo_type: str = org_or_repo_type\n        org, repo = parse_org_repo(repo_name)\n        if org is None and repo is None:\n            return error_repo_not_found()\n    else:\n        repo_type: str = \"models\"\n        org, repo = org_or_repo_type, repo_name\n\n    return await file_get_common(\n        repo_type=repo_type,\n        org=org,\n        repo=repo,\n        commit=commit,\n        file_path=file_path,\n        request=request,\n    )\n\n\n@app.get(\"/{org_repo}/resolve/{commit}/{file_path:path}\")\nasync def file_get(org_repo: str, commit: str, file_path: str, request: Request):\n    repo_type: str = \"models\"\n    org, repo = parse_org_repo(org_repo)\n    if org is None and repo is None:\n        return error_repo_not_found()\n\n    return await file_get_common(\n        repo_type=repo_type,\n        org=org,\n        repo=repo,\n        commit=commit,\n        file_path=file_path,\n        request=request,\n    )\n\n\n@app.get(\"/{org_repo}/{hash_file}\")\n@app.get(\"/{repo_type}/{org_repo}/{hash_file}\")\nasync def cdn_file_get(\n    org_repo: str, hash_file: str, request: Request, repo_type: str = \"models\"\n):\n    org, repo = parse_org_repo(org_repo)\n    if org is None and repo is None:\n        return error_repo_not_found()\n\n    if not await check_proxy_rules_hf(app, repo_type, org, repo):\n        return error_repo_not_found()\n    try:\n        generator = await cdn_file_get_generator(\n            app, repo_type, org, repo, hash_file, method=\"GET\", request=request\n        )\n        status_code = await generator.__anext__()\n        headers = await generator.__anext__()\n        return StreamingResponse(generator, headers=headers, status_code=status_code)\n    except httpx.ConnectTimeout:\n        return Response(status_code=504)\n\n\n# ======================\n# LFS Hooks\n# ======================\n@app.head(\"/repos/{dir1}/{dir2}/{hash_repo}/{hash_file}\")\nasync def lfs_head(dir1: str, dir2: str, hash_repo: str, hash_file: str, request: Request):\n    try:\n        generator = await lfs_head_generator(app, dir1, dir2, hash_repo, hash_file, request)\n        status_code = await generator.__anext__()\n        headers = await generator.__anext__()\n        return StreamingResponse(generator, headers=headers, status_code=status_code)\n    except httpx.ConnectTimeout:\n        return Response(status_code=504)\n\n@app.get(\"/repos/{dir1}/{dir2}/{hash_repo}/{hash_file}\")\nasync def lfs_get(dir1: str, dir2: str, hash_repo: str, hash_file: str, request: Request):\n    try:\n        generator = await lfs_get_generator(app, dir1, dir2, hash_repo, hash_file, request)\n        status_code = await generator.__anext__()\n        headers = await generator.__anext__()\n        return StreamingResponse(generator, headers=headers, status_code=status_code)\n    except httpx.ConnectTimeout:\n        return Response(status_code=504)\n\n\n# ======================\n# Web Page Hooks\n# ======================\n@app.get(\"/\", response_class=HTMLResponse)\nasync def index(request: Request):\n    return templates.TemplateResponse(\n        \"index.html\",\n        {\n            \"request\": request,\n            \"scheme\": app.state.app_settings.config.mirror_scheme,\n            \"netloc\": app.state.app_settings.config.mirror_netloc,\n        },\n    )\n\n@app.get(\"/repos\", response_class=HTMLResponse)\nasync def repos(request: Request):\n    datasets_repos = glob.glob(os.path.join(app.state.app_settings.config.repos_path, \"api/datasets/*/*\"))\n    models_repos = glob.glob(os.path.join(app.state.app_settings.config.repos_path, \"api/models/*/*\"))\n    spaces_repos = glob.glob(os.path.join(app.state.app_settings.config.repos_path, \"api/spaces/*/*\"))\n    datasets_repos = [get_org_repo(*repo.split(\"/\")[-2:]) for repo in datasets_repos]\n    models_repos = [get_org_repo(*repo.split(\"/\")[-2:]) for repo in models_repos]\n    spaces_repos = [get_org_repo(*repo.split(\"/\")[-2:]) for repo in spaces_repos]\n\n    return templates.TemplateResponse(\n        \"repos.html\",\n        {\n            \"request\": request,\n            \"datasets_repos\": datasets_repos,\n            \"models_repos\": models_repos,\n            \"spaces_repos\": spaces_repos,\n        },\n    )\n\n\ndef init():\n    parser = argparse.ArgumentParser(\n        description=\"Olah Huggingface Mirror Server.\"\n    )\n    parser.add_argument(\"--config\", \"-c\", type=str, default=\"\")\n    parser.add_argument(\"--host\", type=str, default=\"localhost\")\n    parser.add_argument(\"--port\", type=int, default=8090)\n    parser.add_argument(\"--hf-scheme\", type=str, default=\"https\", help=\"The scheme of huggingface site (http or https)\")\n    parser.add_argument(\"--hf-netloc\", type=str, default=\"huggingface.co\")\n    parser.add_argument(\"--hf-lfs-netloc\", type=str, default=\"cdn-lfs.huggingface.co\")\n    parser.add_argument(\"--mirror-scheme\", type=str, default=\"http\", help=\"The scheme of mirror site (http or https)\")\n    parser.add_argument(\"--mirror-netloc\", type=str, default=\"localhost:8090\")\n    parser.add_argument(\"--mirror-lfs-netloc\", type=str, default=\"localhost:8090\")\n    parser.add_argument(\"--has-lfs-site\", action=\"store_true\")\n    parser.add_argument(\"--ssl-key\", type=str, default=None, help=\"The SSL key file path, if HTTPS is used\")\n    parser.add_argument(\"--ssl-cert\", type=str, default=None, help=\"The SSL cert file path, if HTTPS is used\")\n    parser.add_argument(\"--repos-path\", type=str, default=\"./repos\", help=\"The folder to save cached repositories\")\n    parser.add_argument(\"--cache-size-limit\", type=str, default=\"\", help=\"The limit size of cache. (Example values: '100MB', '2GB', '500KB')\")\n    parser.add_argument(\"--cache-clean-strategy\", type=str, default=\"LRU\", help=\"The clean strategy of cache. ('LRU', 'FIFO', 'LARGE_FIRST')\")\n    parser.add_argument(\"--log-path\", type=str, default=\"./logs\", help=\"The folder to save logs\")\n    args = parser.parse_args()\n    \n    logger = build_logger(\"olah\", \"olah.log\", logger_dir=args.log_path)\n    \n    def is_default_value(args, arg_name):\n        if hasattr(args, arg_name):\n            arg_value = getattr(args, arg_name)\n            arg_default = parser.get_default(arg_name)\n            return arg_value == arg_default\n        return False\n\n    if args.config != \"\":\n        config = OlahConfig(args.config)\n    else:\n        config = OlahConfig()\n        \n        if not is_default_value(args, \"host\"):\n            config.host = args.host\n        if not is_default_value(args, \"port\"):\n            config.port = args.port\n        \n        if not is_default_value(args, \"ssl_key\"):\n            config.ssl_key = args.ssl_key\n        if not is_default_value(args, \"ssl_cert\"):\n            config.ssl_cert = args.ssl_cert\n        \n        if not is_default_value(args, \"repos_path\"):\n            config.repos_path = args.repos_path\n        if not is_default_value(args, \"hf_scheme\"):\n            config.hf_scheme = args.hf_scheme\n        if not is_default_value(args, \"hf_netloc\"):\n            config.hf_netloc = args.hf_netloc\n        if not is_default_value(args, \"hf_lfs_netloc\"):\n            config.hf_lfs_netloc = args.hf_lfs_netloc\n        if not is_default_value(args, \"mirror_scheme\"):\n            config.mirror_scheme = args.mirror_scheme\n        if not is_default_value(args, \"mirror_netloc\"):\n            config.mirror_netloc = args.mirror_netloc\n        if not is_default_value(args, \"mirror_lfs_netloc\"):\n            config.mirror_lfs_netloc = args.mirror_lfs_netloc\n        if not is_default_value(args, \"cache_size_limit\"):\n            config.cache_size_limit = convert_to_bytes(args.cache_size_limit)\n        if not is_default_value(args, \"cache_clean_strategy\"):\n            config.cache_clean_strategy = args.cache_clean_strategy\n        else:\n            if not args.has_lfs_site and not is_default_value(args, \"mirror_netloc\"):\n                config.mirror_lfs_netloc = args.mirror_netloc\n\n    if is_default_value(args, \"host\"):\n        args.host = config.host\n    if is_default_value(args, \"port\"):\n        args.port = config.port\n    if is_default_value(args, \"ssl_key\"):\n        args.ssl_key = config.ssl_key\n    if is_default_value(args, \"ssl_cert\"):\n        args.ssl_cert = config.ssl_cert\n    if is_default_value(args, \"repos_path\"):\n        args.repos_path = config.repos_path\n    \n    if is_default_value(args, \"hf_scheme\"):\n        args.hf_scheme = config.hf_scheme\n    if is_default_value(args, \"hf_netloc\"):\n        args.hf_netloc = config.hf_netloc\n    if is_default_value(args, \"hf_lfs_netloc\"):\n        args.hf_lfs_netloc = config.hf_lfs_netloc\n    if is_default_value(args, \"mirror_scheme\"):\n        args.mirror_scheme = config.mirror_scheme\n    if is_default_value(args, \"mirror_netloc\"):\n        args.mirror_netloc = config.mirror_netloc\n    if is_default_value(args, \"mirror_lfs_netloc\"):\n        args.mirror_lfs_netloc = config.mirror_lfs_netloc\n    \n    if is_default_value(args, \"cache_size_limit\"):\n        args.cache_size_limit = config.cache_size_limit\n    if is_default_value(args, \"cache_clean_strategy\"):\n        args.cache_clean_strategy = config.cache_clean_strategy\n\n    # Post processing\n    if \",\" in args.host:\n        args.host = args.host.split(\",\")\n    \n    args.mirror_scheme = config.mirror_scheme = \"http\" if args.ssl_key is None else \"https\"\n\n    print(args)\n    # Warnings\n    if config.cache_size_limit is not None:\n        logger.info(f\"\"\"\n======== WARNING ========\nDue to the cache_size_limit parameter being set, Olah will periodically delete cache files.\nPlease ensure that the cache directory specified in repos_path '{config.repos_path}' is correct.\nIncorrect settings may result in unintended file deletion and loss!!! !!!\n=========================\"\"\")\n        for i in range(10):\n            time.sleep(0.2)\n    \n    # Init app settings\n    app.state.app_settings = AppSettings(config=config)\n    return args\n\ndef main():\n    args = init()\n    if __name__ == \"__main__\":\n        import uvicorn\n        uvicorn.run(\n            \"olah.server:app\",\n            host=args.host,\n            port=args.port,\n            log_level=\"info\",\n            reload=False,\n            ssl_keyfile=args.ssl_key,\n            ssl_certfile=args.ssl_cert\n        )\n\ndef cli():\n    args = init()\n    import uvicorn\n    uvicorn.run(\n        \"olah.server:app\",\n        host=args.host,\n        port=args.port,\n        log_level=\"info\",\n        reload=False,\n        ssl_keyfile=args.ssl_key,\n        ssl_certfile=args.ssl_cert,\n    )\n\nif __name__ in [\"olah.server\", \"__main__\"]:\n    main()\n"}
{"type": "source_file", "path": "src/olah/cache/stat.py", "content": "# coding=utf-8\n# Copyright 2024 XiaHan\n#\n# Use of this source code is governed by an MIT-style\n# license that can be found in the LICENSE file or at\n# https://opensource.org/licenses/MIT.\n\nimport argparse\nimport os\nimport sys\nfrom olah.cache.olah_cache import OlahCache\n\ndef get_size_human(size: int) -> str:\n    if size > 1024 * 1024 * 1024:\n        return f\"{int(size / (1024 * 1024 * 1024)):.4f}GB\"\n    elif size > 1024 * 1024:\n        return f\"{int(size / (1024 * 1024)):.4f}MB\"\n    elif size > 1024:\n        return f\"{int(size / (1024)):.4f}KB\"\n    else:\n        return f\"{size:.4f}B\"\n\ndef insert_newlines(input_str, every=10):\n    return '\\n'.join(input_str[i:i+every] for i in range(0, len(input_str), every))\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Olah Cache Visualization Tool.\")\n    parser.add_argument(\"--file\", \"-f\", type=str, required=True, help=\"The path of Olah cache file\")\n    parser.add_argument(\"--export\", \"-e\", type=str, default=\"\", help=\"Export the cached file if all blocks are cached\")\n    args = parser.parse_args()\n    print(args)\n\n    with open(args.file, \"rb\") as f:\n        f.seek(0, os.SEEK_END)\n        bin_size = f.tell()\n    \n    try:\n        cache = OlahCache(args.file)\n    except Exception as e:\n        print(e)\n        sys.exit(1)\n    print(f\"File: {args.file}\")\n    print(f\"Olah Cache Version: {cache.header.version}\")\n    print(f\"File Size: {get_size_human(cache.header.file_size)}\")\n    print(f\"Cache Total Size: {get_size_human(bin_size)}\")\n    print(f\"Block Size: {cache.header.block_size}\")\n    print(f\"Block Number: {cache.header.block_number}\")\n    print(f\"Cache Status: \")\n    cache_status = cache.header.block_mask.__str__()[:cache.header._block_number]\n    print(insert_newlines(cache_status, every=50))\n\n    if args.export != \"\":\n        if all([c == \"1\" for c in cache_status]):\n            with open(args.file, \"rb\") as f:\n                f.seek(cache._get_header_size(), os.SEEK_SET)\n                with open(args.export, \"wb\") as fout:\n                    fout.write(f.read())\n        else:\n            print(\"Some blocks are not cached, so the export is skipped.\")"}
{"type": "source_file", "path": "src/olah/utils/disk_utils.py", "content": "# coding=utf-8\n# Copyright 2024 XiaHan\n#\n# Use of this source code is governed by an MIT-style\n# license that can be found in the LICENSE file or at\n# https://opensource.org/licenses/MIT.\n\nimport datetime\nimport os\n\nimport time\nfrom typing import List, Optional, Tuple\n\n\ndef get_folder_size(folder_path: str) -> int:\n    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(folder_path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            total_size += os.path.getsize(fp)\n    return total_size\n\ndef sort_files_by_access_time(folder_path: str) -> List[Tuple[str, datetime.datetime]]:\n    files = []\n\n    # Get all file paths and time\n    for dirpath, dirnames, filenames in os.walk(folder_path):\n        for f in filenames:\n            file_path = os.path.join(dirpath, f)\n            if not os.path.isfile(file_path):\n                continue\n            access_time = datetime.datetime.fromtimestamp(os.path.getatime(file_path))\n            files.append((file_path, access_time))\n    \n    # Sort by accesstime\n    sorted_files = sorted(files, key=lambda x: x[1])\n    \n    return sorted_files\n\ndef sort_files_by_modify_time(folder_path: str) -> List[Tuple[str, datetime.datetime]]:\n    files = []\n\n    # Get all file paths and time\n    for dirpath, dirnames, filenames in os.walk(folder_path):\n        for f in filenames:\n            file_path = os.path.join(dirpath, f)\n            if not os.path.isfile(file_path):\n                continue\n            access_time = datetime.datetime.fromtimestamp(os.path.getmtime(file_path))\n            files.append((file_path, access_time))\n    \n    # Sort by modify time\n    sorted_files = sorted(files, key=lambda x: x[1])\n    \n    return sorted_files\n\ndef sort_files_by_size(folder_path: str) -> List[Tuple[str, int]]:\n    files = []\n\n    # Get all file paths and sizes\n    for dirpath, dirnames, filenames in os.walk(folder_path):\n        for f in filenames:\n            file_path = os.path.join(dirpath, f)\n            if not os.path.isfile(file_path):\n                continue\n            file_size = os.path.getsize(file_path)\n            files.append((file_path, file_size))\n    \n    # Sort by file size\n    sorted_files = sorted(files, key=lambda x: x[1])\n    \n    return sorted_files\n\ndef touch_file_access_time(filename: str):\n    if not os.path.exists(filename):\n        return\n    now = time.time()\n    stat_info = os.stat(filename)\n    atime = stat_info.st_atime\n    mtime = stat_info.st_mtime\n    \n    os.utime(filename, times=(now, mtime))\n\ndef convert_to_bytes(size_str) -> Optional[int]:\n    size_str = size_str.strip().upper()\n    multipliers = {\n        \"K\": 1024,\n        \"M\": 1024**2,\n        \"G\": 1024**3,\n        \"T\": 1024**4,\n        \"KB\": 1024,\n        \"MB\": 1024**2,\n        \"GB\": 1024**3,\n        \"TB\": 1024**4,\n    }\n\n    for unit in multipliers:\n        if size_str.endswith(unit):\n            size = int(size_str[: -len(unit)])\n            return size * multipliers[unit]\n\n    # Default use bytes\n    try:\n        return int(size_str)\n    except ValueError:\n        return None\n\n\ndef convert_bytes_to_human_readable(bytes: int) -> str:\n    suffixes = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]\n    index = 0\n    while bytes >= 1024 and index < len(suffixes) - 1:\n        bytes /= 1024\n        index += 1\n    return f\"{bytes:.2f} {suffixes[index]}\"\n"}
{"type": "source_file", "path": "src/olah/utils/olah_utils.py", "content": "# coding=utf-8\n# Copyright 2024 XiaHan\n#\n# Use of this source code is governed by an MIT-style\n# license that can be found in the LICENSE file or at\n# https://opensource.org/licenses/MIT.\n\nimport platform\nimport os\n\n\ndef get_olah_path() -> str:\n    if platform.system() == \"Windows\":\n        olah_path = os.path.expanduser(\"~\\\\.olah\")\n    else:\n        olah_path = os.path.expanduser(\"~/.olah\")\n    return olah_path\n"}
{"type": "source_file", "path": "src/olah/utils/zip_utils.py", "content": "\"\"\"\nHandlers for Content-Encoding.\n\nSee: https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Encoding\n\"\"\"\n\nimport codecs\nimport io\nimport typing\nfrom typing import List, Optional, Union\nimport zlib\nimport brotli\nimport httpx\n\n\nclass DecodingError(httpx.RequestError):\n    \"\"\"\n    Decoding of the response failed, due to a malformed encoding.\n    \"\"\"\n\n\nclass ContentDecoder:\n    def decode(self, data: bytes) -> bytes:\n        raise NotImplementedError()  # pragma: no cover\n\n    def flush(self) -> bytes:\n        raise NotImplementedError()  # pragma: no cover\n\n\nclass IdentityDecoder(ContentDecoder):\n    \"\"\"\n    Handle unencoded data.\n    \"\"\"\n\n    def decode(self, data: bytes) -> bytes:\n        return data\n\n    def flush(self) -> bytes:\n        return b\"\"\n\n\nclass DeflateDecoder(ContentDecoder):\n    \"\"\"\n    Handle 'deflate' decoding.\n\n    See: https://stackoverflow.com/questions/1838699\n    \"\"\"\n\n    def __init__(self) -> None:\n        self.first_attempt = True\n        self.decompressor = zlib.decompressobj()\n\n    def decode(self, data: bytes) -> bytes:\n        was_first_attempt = self.first_attempt\n        self.first_attempt = False\n        try:\n            return self.decompressor.decompress(data)\n        except zlib.error as exc:\n            if was_first_attempt:\n                self.decompressor = zlib.decompressobj(-zlib.MAX_WBITS)\n                return self.decode(data)\n            raise DecodingError(str(exc)) from exc\n\n    def flush(self) -> bytes:\n        try:\n            return self.decompressor.flush()\n        except zlib.error as exc:  # pragma: no cover\n            raise DecodingError(str(exc)) from exc\n\n\nclass GZipDecoder(ContentDecoder):\n    \"\"\"\n    Handle 'gzip' decoding.\n\n    See: https://stackoverflow.com/questions/1838699\n    \"\"\"\n\n    def __init__(self) -> None:\n        self.decompressor = zlib.decompressobj(zlib.MAX_WBITS | 16)\n\n    def decode(self, data: bytes) -> bytes:\n        try:\n            return self.decompressor.decompress(data)\n        except zlib.error as exc:\n            raise DecodingError(str(exc)) from exc\n\n    def flush(self) -> bytes:\n        try:\n            return self.decompressor.flush()\n        except zlib.error as exc:  # pragma: no cover\n            raise DecodingError(str(exc)) from exc\n\n\nclass BrotliDecoder(ContentDecoder):\n    \"\"\"\n    Handle 'brotli' decoding.\n\n    Requires `pip install brotlipy`. See: https://brotlipy.readthedocs.io/\n        or   `pip install brotli`. See https://github.com/google/brotli\n    Supports both 'brotlipy' and 'Brotli' packages since they share an import\n    name. The top branches are for 'brotlipy' and bottom branches for 'Brotli'\n    \"\"\"\n\n    def __init__(self) -> None:\n        if brotli is None:  # pragma: no cover\n            raise ImportError(\n                \"Using 'BrotliDecoder', but neither of the 'brotlicffi' or 'brotli' \"\n                \"packages have been installed. \"\n                \"Make sure to install httpx using `pip install httpx[brotli]`.\"\n            ) from None\n\n        self.decompressor = brotli.Decompressor()\n        self.seen_data = False\n        self._decompress: typing.Callable[[bytes], bytes]\n        if hasattr(self.decompressor, \"decompress\"):\n            # The 'brotlicffi' package.\n            self._decompress = self.decompressor.decompress  # pragma: no cover\n        else:\n            # The 'brotli' package.\n            self._decompress = self.decompressor.process  # pragma: no cover\n\n    def decode(self, data: bytes) -> bytes:\n        if not data:\n            return b\"\"\n        self.seen_data = True\n        try:\n            return self._decompress(data)\n        except brotli.error as exc:\n            raise DecodingError(str(exc)) from exc\n\n    def flush(self) -> bytes:\n        if not self.seen_data:\n            return b\"\"\n        try:\n            if hasattr(self.decompressor, \"finish\"):\n                # Only available in the 'brotlicffi' package.\n\n                # As the decompressor decompresses eagerly, this\n                # will never actually emit any data. However, it will potentially throw\n                # errors if a truncated or damaged data stream has been used.\n                self.decompressor.finish()  # pragma: no cover\n            return b\"\"\n        except brotli.error as exc:  # pragma: no cover\n            raise DecodingError(str(exc)) from exc\n\n\nclass MultiDecoder(ContentDecoder):\n    \"\"\"\n    Handle the case where multiple encodings have been applied.\n    \"\"\"\n\n    def __init__(self, children: typing.Sequence[ContentDecoder]) -> None:\n        \"\"\"\n        'children' should be a sequence of decoders in the order in which\n        each was applied.\n        \"\"\"\n        # Note that we reverse the order for decoding.\n        self.children = list(reversed(children))\n\n    def decode(self, data: bytes) -> bytes:\n        for child in self.children:\n            data = child.decode(data)\n        return data\n\n    def flush(self) -> bytes:\n        data = b\"\"\n        for child in self.children:\n            data = child.decode(data) + child.flush()\n        return data\n\n\nSUPPORTED_DECODERS = {\n    \"identity\": IdentityDecoder,\n    \"gzip\": GZipDecoder,\n    \"deflate\": DeflateDecoder,\n    \"br\": BrotliDecoder,\n}\n\n\nclass Decompressor(object):\n    def __init__(self, algorithms: Union[str, List[str]]) -> None:\n        if isinstance(algorithms, str):\n            self.algorithms = [algorithms]\n        else:\n            self.algorithms = algorithms\n\n        self.decoders = []\n        for algo in self.algorithms:\n            algo = algo.strip().lower()\n            if algo in SUPPORTED_DECODERS:\n                self.decoders.append(SUPPORTED_DECODERS[algo]())\n            else:\n                print(f\"Unsupported compression algorithm: {algo}\")\n\n        self.decoder = MultiDecoder(self.decoders)\n\n    def decompress(self, raw_chunk: bytes) -> bytes:\n        return self.decoder.decode(raw_chunk)\n\n\ndef decompress_data(raw_data: bytes, content_encoding: Optional[str]) -> bytes:\n    # If result is compressed\n    if content_encoding is not None:\n        final_data = raw_data\n        algorithms = content_encoding.split(\",\")\n        for algo in algorithms:\n            algo = algo.strip().lower()\n            if algo == \"gzip\":\n                try:\n                    final_data = zlib.decompress(\n                        raw_data, zlib.MAX_WBITS | 16\n                    )  # 解压缩\n                except Exception as e:\n                    print(f\"Error decompressing gzip data: {e}\")\n            elif algo == \"compress\":\n                print(f\"Unsupported decompression algorithm: {algo}\")\n            elif algo == \"deflate\":\n                try:\n                    final_data = zlib.decompress(raw_data)\n                except Exception as e:\n                    print(f\"Error decompressing deflate data: {e}\")\n            elif algo == \"br\":\n                try:\n                    import brotli\n\n                    final_data = brotli.decompress(raw_data)\n                except Exception as e:\n                    print(f\"Error decompressing Brotli data: {e}\")\n            elif algo == \"zstd\":\n                try:\n                    import zstandard\n\n                    final_data = zstandard.ZstdDecompressor().decompress(raw_data)\n                except Exception as e:\n                    print(f\"Error decompressing Zstandard data: {e}\")\n            else:\n                print(f\"Unsupported compression algorithm: {algo}\")\n        return final_data\n    else:\n        return raw_data\n"}
{"type": "source_file", "path": "src/olah/utils/logging.py", "content": "# coding=utf-8\n# Copyright 2024 XiaHan\n#\n# Use of this source code is governed by an MIT-style\n# license that can be found in the LICENSE file or at\n# https://opensource.org/licenses/MIT.\n\nfrom asyncio import AbstractEventLoop\nimport json\nimport logging\nimport logging.handlers\nimport os\nimport platform\nimport re\nimport sys\nfrom typing import AsyncGenerator, Generator\nimport warnings\nfrom olah.constants import DEFAULT_LOGGER_DIR\n\nhandler = None\n\n\n# Define a custom formatter without color codes\nclass NoColorFormatter(logging.Formatter):\n    color_pattern = re.compile(r\"\\x1b[^m]*m\")  # Regex pattern to match color codes\n\n    def format(self, record):\n        message = super().format(record)\n        # Remove color codes from the log message\n        message = self.color_pattern.sub(\"\", message)\n        return message\n\n\ndef build_logger(\n    logger_name, logger_filename, logger_dir=DEFAULT_LOGGER_DIR\n) -> logging.Logger:\n    global handler\n\n    formatter = logging.Formatter(\n        fmt=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n    )\n\n    nocolor_formatter = NoColorFormatter(\n        fmt=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n    )\n\n    # Set the format of root handlers\n    if logging.getLogger().handlers is None or len(logging.getLogger().handlers) == 0:\n        if sys.version_info[1] >= 9:\n            # This is for windows\n            logging.basicConfig(level=logging.INFO, encoding=\"utf-8\")\n        else:\n            if platform.system() == \"Windows\":\n                warnings.warn(\n                    \"If you are running on Windows, \"\n                    \"we recommend you use Python >= 3.9 for UTF-8 encoding.\"\n                )\n            logging.basicConfig(level=logging.INFO)\n    logging.getLogger().handlers[0].setFormatter(formatter)\n\n    # Redirect stdout and stderr to loggers\n    stdout_logger = logging.getLogger(\"stdout\")\n    stdout_logger.setLevel(logging.DEBUG)\n    sl = StreamToLogger(stdout_logger, logging.INFO)\n    sys.stdout = sl\n\n    stderr_logger = logging.getLogger(\"stderr\")\n    stderr_logger.setLevel(logging.ERROR)\n    sl = StreamToLogger(stderr_logger, logging.ERROR)\n    sys.stderr = sl\n\n    # Get logger\n    logger = logging.getLogger(logger_name)\n    logger.setLevel(logging.DEBUG)\n\n    # Add a file handler for all loggers\n    if handler is None:\n        os.makedirs(logger_dir, exist_ok=True)\n        filename = os.path.join(logger_dir, logger_filename)\n        handler = logging.handlers.TimedRotatingFileHandler(\n            filename, when=\"H\", utc=True, encoding=\"utf-8\"\n        )\n        handler.setFormatter(nocolor_formatter)\n        handler.namer = lambda name: name.replace(\".log\", \"\") + \".log\"\n\n        for name, item in logging.root.manager.loggerDict.items():\n            if isinstance(item, logging.Logger):\n                item.addHandler(handler)\n\n    return logger\n\n\nclass StreamToLogger(object):\n    \"\"\"\n    Fake file-like stream object that redirects writes to a logger instance.\n    \"\"\"\n\n    def __init__(self, logger, log_level=logging.INFO):\n        self.terminal = sys.stdout\n        self.logger = logger\n        self.log_level = log_level\n        self.linebuf = \"\"\n\n    def __getattr__(self, attr):\n        try:\n            attr_value = getattr(self.terminal, attr)\n        except:\n            return None\n        return attr_value\n\n    def write(self, buf):\n        temp_linebuf = self.linebuf + buf\n        self.linebuf = \"\"\n        for line in temp_linebuf.splitlines(True):\n            # From the io.TextIOWrapper docs:\n            #   On output, if newline is None, any '\\n' characters written\n            #   are translated to the system default line separator.\n            # By default sys.stdout.write() expects '\\n' newlines and then\n            # translates them so this is still cross platform.\n            if line[-1] == \"\\n\":\n                encoded_message = line.encode(\"utf-8\", \"ignore\").decode(\"utf-8\")\n                self.logger.log(self.log_level, encoded_message.rstrip())\n            else:\n                self.linebuf += line\n\n    def flush(self):\n        if self.linebuf != \"\":\n            encoded_message = self.linebuf.encode(\"utf-8\", \"ignore\").decode(\"utf-8\")\n            self.logger.log(self.log_level, encoded_message.rstrip())\n        self.linebuf = \"\"\n\n\ndef iter_over_async(\n    async_gen: AsyncGenerator, event_loop: AbstractEventLoop\n) -> Generator:\n    \"\"\"\n    Convert async generator to sync generator\n\n    :param async_gen: the AsyncGenerator to convert\n    :param event_loop: the event loop to run on\n    :returns: Sync generator\n    \"\"\"\n    ait = async_gen.__aiter__()\n\n    async def get_next():\n        try:\n            obj = await ait.__anext__()\n            return False, obj\n        except StopAsyncIteration:\n            return True, None\n\n    while True:\n        done, obj = event_loop.run_until_complete(get_next())\n        if done:\n            break\n        yield obj\n"}
{"type": "source_file", "path": "src/olah/utils/cache_utils.py", "content": "# coding=utf-8\n# Copyright 2024 XiaHan\n#\n# Use of this source code is governed by an MIT-style\n# license that can be found in the LICENSE file or at\n# https://opensource.org/licenses/MIT.\n\n\nimport json\nfrom typing import Dict, Mapping, Union\n\n\nasync def write_cache_request(\n    save_path: str,\n    status_code: int,\n    headers: Union[Dict[str, str], Mapping],\n    content: bytes,\n) -> None:\n    \"\"\"\n    Write the request's status code, headers, and content to a cache file.\n\n    Args:\n        head_path (str): The path to the cache file.\n        status_code (int): The status code of the request.\n        headers (Dict[str, str]): The dictionary of response headers.\n        content (bytes): The content of the request.\n\n    Returns:\n        None\n    \"\"\"\n    if not isinstance(headers, dict):\n        headers = {k.lower(): v for k, v in headers.items()}\n    rq = {\n        \"status_code\": status_code,\n        \"headers\": headers,\n        \"content\": content.hex(),\n    }\n    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(json.dumps(rq, ensure_ascii=False))\n\n\nasync def read_cache_request(save_path: str) -> Dict[str, str]:\n    \"\"\"\n    Read the request's status code, headers, and content from a cache file.\n\n    Args:\n        save_path (str): The path to the cache file.\n\n    Returns:\n        Dict[str, str]: A dictionary containing the status code, headers, and content of the request.\n    \"\"\"\n    with open(save_path, \"r\", encoding=\"utf-8\") as f:\n        rq = json.loads(f.read())\n\n    rq[\"content\"] = bytes.fromhex(rq[\"content\"])\n    return rq\n"}
{"type": "source_file", "path": "src/olah/utils/__init__.py", "content": ""}
{"type": "source_file", "path": "src/olah/utils/rule_utils.py", "content": "# coding=utf-8\n# Copyright 2024 XiaHan\n#\n# Use of this source code is governed by an MIT-style\n# license that can be found in the LICENSE file or at\n# https://opensource.org/licenses/MIT.\n\n\nfrom typing import Dict, Literal, Optional, Tuple, Union\n\nfrom fastapi import FastAPI\nfrom olah.configs import OlahConfig\nfrom .repo_utils import get_org_repo\n\n\nasync def check_proxy_rules_hf(\n    app: FastAPI,\n    repo_type: Optional[Literal[\"models\", \"datasets\", \"spaces\"]],\n    org: Optional[str],\n    repo: str,\n) -> bool:\n    config: OlahConfig = app.state.app_settings.config\n    org_repo = get_org_repo(org, repo)\n    return config.proxy.allow(org_repo)\n\n\nasync def check_cache_rules_hf(\n    app: FastAPI,\n    repo_type: Optional[Literal[\"models\", \"datasets\", \"spaces\"]],\n    org: Optional[str],\n    repo: str,\n) -> bool:\n    config: OlahConfig = app.state.app_settings.config\n    org_repo = get_org_repo(org, repo)\n    return config.cache.allow(org_repo)\n"}
{"type": "source_file", "path": "src/olah/proxy/tree.py", "content": "# coding=utf-8\n# Copyright 2024 XiaHan\n#\n# Use of this source code is governed by an MIT-style\n# license that can be found in the LICENSE file or at\n# https://opensource.org/licenses/MIT.\n\nimport os\nfrom typing import Dict, Literal, Mapping, Optional, AsyncGenerator, Union\nfrom urllib.parse import urljoin\nfrom fastapi import FastAPI, Request\n\nimport httpx\nfrom olah.constants import CHUNK_SIZE, WORKER_API_TIMEOUT\n\nfrom olah.utils.cache_utils import read_cache_request, write_cache_request\nfrom olah.utils.rule_utils import check_cache_rules_hf\nfrom olah.utils.repo_utils import get_org_repo\nfrom olah.utils.file_utils import make_dirs\n\n\nasync def _tree_cache_generator(save_path: str) -> AsyncGenerator[Union[int, Dict[str, str], bytes], None]:\n    cache_rq = await read_cache_request(save_path)\n    yield cache_rq[\"status_code\"]\n    yield cache_rq[\"headers\"]\n    yield cache_rq[\"content\"]\n\nasync def _tree_proxy_generator(\n    app: FastAPI,\n    headers: Dict[str, str],\n    tree_url: str,\n    method: str,\n    params: Mapping[str, str],\n    allow_cache: bool,\n    save_path: str,\n) -> AsyncGenerator[Union[int, Dict[str, str], bytes], None]:\n    async with httpx.AsyncClient(follow_redirects=True) as client:\n        content_chunks = []\n        async with client.stream(\n            method=method,\n            url=tree_url,\n            params=params,\n            headers=headers,\n            timeout=WORKER_API_TIMEOUT,\n        ) as response:\n            response_status_code = response.status_code\n            response_headers = response.headers\n            yield response_status_code\n            yield response_headers\n\n            async for raw_chunk in response.aiter_raw():\n                if not raw_chunk:\n                    continue\n                content_chunks.append(raw_chunk)\n                yield raw_chunk\n\n        content = bytearray()\n        for chunk in content_chunks:\n            content += chunk\n\n        if allow_cache and response_status_code == 200:\n            make_dirs(save_path)\n            await write_cache_request(\n                save_path, response_status_code, response_headers, bytes(content)\n            )\n\n\nasync def tree_generator(\n    app: FastAPI,\n    repo_type: Literal[\"models\", \"datasets\", \"spaces\"],\n    org: str,\n    repo: str,\n    commit: str,\n    path: str,\n    recursive: bool,\n    expand: bool,\n    override_cache: bool,\n    method: str,\n    authorization: Optional[str],\n) -> AsyncGenerator[Union[int, Dict[str, str], bytes], None]:\n    headers = {}\n    if authorization is not None:\n        headers[\"authorization\"] = authorization\n\n    org_repo = get_org_repo(org, repo)\n    # save\n    repos_path = app.state.app_settings.config.repos_path\n    save_dir = os.path.join(\n        repos_path, f\"api/{repo_type}/{org_repo}/tree/{commit}/{path}\"\n    )\n    save_path = os.path.join(save_dir, f\"tree_{method}_recursive_{recursive}_expand_{expand}.json\")\n\n    use_cache = os.path.exists(save_path)\n    allow_cache = await check_cache_rules_hf(app, repo_type, org, repo)\n\n    org_repo = get_org_repo(org, repo)\n    tree_url = urljoin(\n        app.state.app_settings.config.hf_url_base(),\n        f\"/api/{repo_type}/{org_repo}/tree/{commit}/{path}\",\n    )\n    # proxy\n    if use_cache and not override_cache:\n        async for item in _tree_cache_generator(save_path):\n            yield item\n    else:\n        async for item in _tree_proxy_generator(\n            app, headers, tree_url, method, {\"recursive\": recursive, \"expand\": expand}, allow_cache, save_path\n        ):\n            yield item\n"}
{"type": "source_file", "path": "src/olah/utils/repo_utils.py", "content": "# coding=utf-8\n# Copyright 2024 XiaHan\n#\n# Use of this source code is governed by an MIT-style\n# license that can be found in the LICENSE file or at\n# https://opensource.org/licenses/MIT.\n\nimport datetime\nimport os\nimport glob\nimport tenacity\nfrom typing import Dict, Literal, Optional, Tuple, Union\nimport json\nfrom urllib.parse import urljoin\nimport httpx\nfrom olah.constants import WORKER_API_TIMEOUT\nfrom olah.utils.cache_utils import read_cache_request\n\n\ndef get_org_repo(org: Optional[str], repo: str) -> str:\n    \"\"\"\n    Constructs the organization/repository name.\n\n    Args:\n        org: The organization name (optional).\n        repo: The repository name.\n\n    Returns:\n        The organization/repository name as a string.\n\n    \"\"\"\n    if org is None:\n        org_repo = repo\n    else:\n        org_repo = f\"{org}/{repo}\"\n    return org_repo\n\n\ndef parse_org_repo(org_repo: str) -> Tuple[Optional[str], Optional[str]]:\n    \"\"\"\n    Parses the organization/repository name.\n\n    Args:\n        org_repo: The organization/repository name.\n\n    Returns:\n        A tuple containing the organization name and repository name.\n\n    \"\"\"\n    if \"/\" in org_repo and org_repo.count(\"/\") != 1:\n        return None, None\n    if \"/\" in org_repo:\n        org, repo = org_repo.split(\"/\")\n    else:\n        org = None\n        repo = org_repo\n    return org, repo\n\n\ndef get_meta_save_path(\n    repos_path: str, repo_type: str, org: Optional[str], repo: str, commit: str\n) -> str:\n    \"\"\"\n    Constructs the path to save the meta.json file.\n\n    Args:\n        repos_path: The base path where repositories are stored.\n        repo_type: The type of repository.\n        org: The organization name (optional).\n        repo: The repository name.\n        commit: The commit hash.\n\n    Returns:\n        The path to save the meta.json file as a string.\n\n    \"\"\"\n    org_repo = get_org_repo(org, repo)\n    return os.path.join(\n        repos_path, f\"api/{repo_type}/{org_repo}/revision/{commit}/meta_get.json\"\n    )\n\n\ndef get_meta_save_dir(\n    repos_path: str, repo_type: str, org: Optional[str], repo: str\n) -> str:\n    \"\"\"\n    Constructs the directory path to save the meta.json file.\n\n    Args:\n        repos_path: The base path where repositories are stored.\n        repo_type: The type of repository.\n        org: The organization name (optional).\n        repo: The repository name.\n\n    Returns:\n        The directory path to save the meta.json file as a string.\n\n    \"\"\"\n    org_repo = get_org_repo(org, repo)\n    return os.path.join(repos_path, f\"api/{repo_type}/{org_repo}/revision\")\n\n\ndef get_file_save_path(\n    repos_path: str,\n    repo_type: str,\n    org: Optional[str],\n    repo: str,\n    commit: str,\n    file_path: str,\n) -> str:\n    \"\"\"\n    Constructs the path to save a file in the repository.\n\n    Args:\n        repos_path: The base path where repositories are stored.\n        repo_type: The type of repository.\n        org: The organization name (optional).\n        repo: The repository name.\n        commit: The commit hash.\n        file_path: The path of the file within the repository.\n\n    Returns:\n        The path to save the file as a string.\n\n    \"\"\"\n    org_repo = get_org_repo(org, repo)\n    return os.path.join(\n        repos_path, f\"heads/{repo_type}/{org_repo}/resolve_head/{commit}/{file_path}\"\n    )\n\n\nasync def get_newest_commit_hf_offline(\n    app,\n    repo_type: Optional[Literal[\"models\", \"datasets\", \"spaces\"]],\n    org: str,\n    repo: str,\n) -> Optional[str]:\n    \"\"\"\n    Retrieves the newest commit hash for a repository in offline mode.\n\n    Args:\n        app: The application object.\n        repo_type: The type of repository.\n        org: The organization name.\n        repo: The repository name.\n\n    Returns:\n        The newest commit hash as a string.\n\n    \"\"\"\n    repos_path = app.state.app_settings.config.repos_path\n    save_dir = get_meta_save_dir(repos_path, repo_type, org, repo)\n    files = glob.glob(os.path.join(save_dir, \"*\", \"meta_head.json\"))\n\n    time_revisions = []\n    for file in files:\n        with open(file, \"r\", encoding=\"utf-8\") as f:\n            obj = json.loads(f.read())\n            datetime_object = datetime.datetime.fromisoformat(obj[\"lastModified\"])\n            time_revisions.append((datetime_object, obj[\"sha\"]))\n\n    time_revisions = sorted(time_revisions)\n    if len(time_revisions) == 0:\n        return None\n    else:\n        return time_revisions[-1][1]\n\n\nasync def get_newest_commit_hf(\n    app,\n    repo_type: Optional[Literal[\"models\", \"datasets\", \"spaces\"]],\n    org: Optional[str],\n    repo: str,\n    authorization: Optional[str] = None,\n) -> Optional[str]:\n    \"\"\"\n    Retrieves the newest commit hash for a repository.\n\n    Args:\n        app: The application object.\n        repo_type: The type of repository.\n        org: The organization name (optional).\n        repo: The repository name.\n\n    Returns:\n        The newest commit hash as a string, or None if it cannot be obtained.\n\n    \"\"\"\n    org_repo = get_org_repo(org, repo)\n    url = urljoin(\n        app.state.app_settings.config.hf_url_base(), f\"/api/{repo_type}/{org_repo}\"\n    )\n    if app.state.app_settings.config.offline:\n        return await get_newest_commit_hf_offline(app, repo_type, org, repo)\n    try:\n        async with httpx.AsyncClient() as client:\n            headers = {}\n            if authorization is not None:\n                headers[\"authorization\"] = authorization\n            response = await client.get(url, headers=headers, timeout=WORKER_API_TIMEOUT)\n            if response.status_code != 200:\n                return await get_newest_commit_hf_offline(app, repo_type, org, repo)\n            obj = json.loads(response.text)\n        return obj.get(\"sha\", None)\n    except httpx.TimeoutException as e:\n        return await get_newest_commit_hf_offline(app, repo_type, org, repo)\n\n\nasync def get_commit_hf_offline(\n    app,\n    repo_type: Optional[Literal[\"models\", \"datasets\", \"spaces\"]],\n    org: Optional[str],\n    repo: str,\n    commit: str,\n) -> Optional[str]:\n    \"\"\"\n    Retrieves the commit SHA for a given repository and commit from the offline cache.\n\n    This function is used when the application is in offline mode and the commit information is not available from the API.\n\n    Args:\n        app: The application instance.\n        repo_type: Optional. The type of repository (\"models\", \"datasets\", or \"spaces\").\n        org: Optional. The organization name for the repository.\n        repo: The name of the repository.\n        commit: The commit identifier.\n\n    Returns:\n        The commit SHA as a string if available in the offline cache, or None if the information is not cached.\n    \"\"\"\n    repos_path = app.state.app_settings.config.repos_path\n    save_path = get_meta_save_path(repos_path, repo_type, org, repo, commit)\n    if os.path.exists(save_path):\n        request_cache = await read_cache_request(save_path)\n        request_cache_json = json.loads(request_cache[\"content\"])\n        return request_cache_json[\"sha\"]\n    else:\n        return None\n\n\nasync def get_commit_hf(\n    app,\n    repo_type: Optional[Literal[\"models\", \"datasets\", \"spaces\"]],\n    org: Optional[str],\n    repo: str,\n    commit: str,\n    authorization: Optional[str] = None,\n) -> Optional[str]:\n    \"\"\"\n    Retrieves the commit SHA for a given repository and commit from the Hugging Face API.\n\n    Args:\n        app: The application instance.\n        repo_type: Optional. The type of repository (\"models\", \"datasets\", or \"spaces\").\n        org: Optional. The organization name for the repository.\n        repo: The name of the repository.\n        commit: The commit identifier.\n        authorization: Optional. The authorization token for accessing the API.\n\n    Returns:\n        The commit SHA as a string, or None if the commit cannot be retrieved.\n\n    Raises:\n        This function does not raise any explicit exceptions but may propagate exceptions from underlying functions.\n    \"\"\"\n    org_repo = get_org_repo(org, repo)\n    url = urljoin(\n        app.state.app_settings.config.hf_url_base(),\n        f\"/api/{repo_type}/{org_repo}/revision/{commit}\",\n    )\n    if app.state.app_settings.config.offline:\n        return await get_commit_hf_offline(app, repo_type, org, repo, commit)\n    try:\n        headers = {}\n        if authorization is not None:\n            headers[\"authorization\"] = authorization\n        async with httpx.AsyncClient() as client:\n            response = await client.get(\n                url, headers=headers, timeout=WORKER_API_TIMEOUT, follow_redirects=True\n            )\n            if response.status_code not in [200, 307]:\n                return await get_commit_hf_offline(app, repo_type, org, repo, commit)\n            obj = json.loads(response.text)\n        return obj.get(\"sha\", None)\n    except:\n        return await get_commit_hf_offline(app, repo_type, org, repo, commit)\n\n\n@tenacity.retry(stop=tenacity.stop_after_attempt(3))\nasync def check_commit_hf(\n    app,\n    repo_type: Optional[Literal[\"models\", \"datasets\", \"spaces\"]],\n    org: Optional[str],\n    repo: str,\n    commit: Optional[str] = None,\n    authorization: Optional[str] = None,\n) -> bool:\n    \"\"\"\n    Checks the commit status of a repository in the Hugging Face ecosystem.\n\n    Args:\n        app: The application object.\n        repo_type: The type of repository (models, datasets, or spaces).\n        org: The organization name (optional).\n        repo: The repository name.\n        commit: The commit hash (optional).\n        authorization: The authorization token (optional).\n\n    Returns:\n        A boolean indicating if the commit is valid (status code 200 or 307) or not.\n\n    \"\"\"\n    org_repo = get_org_repo(org, repo)\n    if commit is None:\n        url = urljoin(\n            app.state.app_settings.config.hf_url_base(), f\"/api/{repo_type}/{org_repo}\"\n        )\n    else:\n        url = urljoin(\n            app.state.app_settings.config.hf_url_base(),\n            f\"/api/{repo_type}/{org_repo}/revision/{commit}\",\n        )\n\n    headers = {}\n    if authorization is not None:\n        headers[\"authorization\"] = authorization\n    async with httpx.AsyncClient() as client:\n        response = await client.request(method=\"HEAD\", url=url, headers=headers, timeout=WORKER_API_TIMEOUT)\n        status_code = response.status_code\n    return status_code in [200, 307]\n"}
{"type": "source_file", "path": "src/olah/utils/url_utils.py", "content": "# coding=utf-8\n# Copyright 2024 XiaHan\n#\n# Use of this source code is governed by an MIT-style\n# license that can be found in the LICENSE file or at\n# https://opensource.org/licenses/MIT.\n\nimport datetime\nimport os\nimport glob\nfrom typing import Dict, List, Literal, Optional, Tuple, Union\nimport json\nfrom urllib.parse import ParseResult, urlencode, urljoin, urlparse, parse_qs, urlunparse\nimport httpx\nfrom olah.configs import OlahConfig\nfrom olah.constants import WORKER_API_TIMEOUT\n\n\ndef get_url_tail(parsed_url: Union[str, ParseResult]) -> str:\n    \"\"\"\n    Extracts the tail of a URL, including path, parameters, query, and fragment.\n\n    Args:\n        parsed_url (Union[str, ParseResult]): The parsed URL or a string URL.\n\n    Returns:\n        str: The tail of the URL, including path, parameters, query, and fragment.\n    \"\"\"\n    if isinstance(parsed_url, str):\n        parsed_url = urlparse(parsed_url)\n    url_tail = parsed_url.path\n    if len(parsed_url.params) != 0:\n        url_tail += f\";{parsed_url.params}\"\n    if len(parsed_url.query) != 0:\n        url_tail += f\"?{parsed_url.query}\"\n    if len(parsed_url.fragment) != 0:\n        url_tail += f\"#{parsed_url.fragment}\"\n    return url_tail\n\n\ndef parse_content_range(content_range: str) -> Tuple[str, Optional[int], Optional[int], Optional[int]]:\n    \"\"\"\n    Parses a Content-Range header string and extracts the unit, start position, end position, and resource size.\n\n    Args:\n        content_range (str): The Content-Range header string, e.g., \"bytes 0-999/1000\".\n\n    Returns:\n        Tuple[str, Optional[int], Optional[int], Optional[int]]: A tuple containing:\n            - unit (str): The unit of the range, typically \"bytes\".\n            - start_pos (Optional[int]): The starting position of the range. None if the range is \"*\".\n            - end_pos (Optional[int]): The ending position of the range. None if the range is \"*\".\n            - resource_size (Optional[int]): The total size of the resource. None if the size is unknown.\n\n    Raises:\n        Exception: If the range unit is invalid or the range format is incorrect.\n    \"\"\"\n    if content_range.startswith(\"bytes \"):\n        unit = \"bytes\"\n        content_range_part = content_range[len(\"bytes \"):]\n    else:\n        raise Exception(\"Invalid range unit\")\n\n    \n    if \"/\" in content_range_part:\n        data_range, resource_size = content_range_part.split(\"/\", maxsplit=1)\n        resource_size = int(resource_size)\n    else:\n        data_range = content_range_part\n        resource_size = None\n    \n    if \"-\" in data_range:\n        start_pos, end_pos = data_range.split(\"-\")\n        start_pos, end_pos = int(start_pos), int(end_pos)\n    elif \"*\" == data_range.strip():\n        start_pos, end_pos = None, None\n    else:\n        raise Exception(\"Invalid range\")\n    return unit, start_pos, end_pos, resource_size\n\n\ndef parse_range_params(range_header: str) -> Tuple[str, List[Tuple[Optional[int], Optional[int]]], Optional[int]]:\n    \"\"\"\n    Parses the HTTP Range request header and returns the unit and a list of ranges.\n\n    Args:\n        range_header (str): The HTTP Range request header string, e.g., \"bytes=0-499\" or \"bytes=200-999, 2000-2499, 9500-\".\n\n    Returns:\n        Tuple[str, List[Tuple[int, int]], Optional[int]]: A tuple containing the unit (e.g., \"bytes\") and a list of ranges.\n            Each range is represented as a tuple of start and end positions. If the end position is not specified,\n            it is set to None. For suffix-length ranges (e.g., \"-500\"), the start position is negative.\n\n    Raises:\n        ValueError: If the Range header is empty or has an invalid format.\n    \"\"\"\n    if not range_header:\n        raise ValueError(\"Range header cannot be empty\")\n\n    # Split the unit and range specifiers\n    parts = range_header.split('=')\n    if len(parts) != 2:\n        raise ValueError(\"Invalid Range header format\")\n\n    unit = parts[0].strip()  # Get the unit, typically \"bytes\"\n    range_specifiers = parts[1].strip()  # Get the range part\n    \n    if range_specifiers.startswith(\"-\") and range_specifiers[1:].isdigit():\n        return unit, [], int(range_specifiers[1:])\n\n    # Parse multiple ranges\n    range_list = []\n    for range_spec in range_specifiers.split(','):\n        range_spec = range_spec.strip()\n        if '-' not in range_spec:\n            raise ValueError(\"Invalid range specifier\")\n\n        start, end = range_spec.split('-')\n        start = start.strip()\n        end = end.strip()\n\n        # Handle suffix-length ranges (e.g., \"-500\")\n        if not start and end:\n            range_list.append((None, int(end)))  # Negative start indicates suffix-length\n            continue\n\n        # Handle open-ended ranges (e.g., \"500-\")\n        if not end and start:\n            range_list.append((int(start), None))\n            continue\n\n        # Handle full ranges (e.g., \"200-999\")\n        if start and end:\n            range_list.append((int(start), int(end)))\n            continue\n\n        # If neither start nor end is provided, it's invalid\n        raise ValueError(\"Invalid range specifier\")\n\n    return unit, range_list, None\n\n\ndef get_all_ranges(file_size: int, unit: str, ranges: List[Tuple[Optional[int], Optional[int]]], suffix: Optional[int]) -> List[Tuple[int, int]]:\n    all_ranges: List[Tuple[int, int]] = []\n    if suffix is not None:\n        all_ranges.append((file_size - suffix, file_size))\n    else:\n        for r in ranges:\n            r_start = r[0] if r[0] is not None else 0\n            r_end = r[1] if r[1] is not None else file_size - 1\n            start_pos = max(0, r_start)\n            end_pos = min(file_size - 1, r_end)\n            if end_pos < start_pos:\n                continue\n            all_ranges.append((start_pos, end_pos + 1))\n    return all_ranges\n\n\nclass RemoteInfo(object):\n    def __init__(self, method: str, url: str, headers: Dict[str, str]) -> None:\n        \"\"\"\n        Represents information about a remote request.\n\n        Args:\n            method (str): The HTTP method of the request.\n            url (str): The URL of the request.\n            headers (Dict[str, str]): The headers of the request.\n        \"\"\"\n        self.method = method\n        self.url = url\n        self.headers = headers\n\n\ndef check_url_has_param_name(url: str, param_name: str) -> bool:\n    \"\"\"\n    Checks if a URL contains a specific query parameter.\n\n    Args:\n        url (str): The URL to check.\n        param_name (str): The name of the query parameter.\n\n    Returns:\n        bool: True if the URL contains the parameter, False otherwise.\n    \"\"\"\n    parsed_url = urlparse(url)\n    query_params = parse_qs(parsed_url.query)\n    return param_name in query_params\n\n\ndef get_url_param_name(url: str, param_name: str) -> Optional[str]:\n    \"\"\"\n    Retrieves the value of a specific query parameter from a URL.\n\n    Args:\n        url (str): The URL to retrieve the parameter from.\n        param_name (str): The name of the query parameter.\n\n    Returns:\n        Optional[str]: The value of the query parameter if found, None otherwise.\n    \"\"\"\n    parsed_url = urlparse(url)\n    query_params = parse_qs(parsed_url.query)\n    original_location = query_params.get(param_name)\n    if original_location:\n        return original_location[0]\n    else:\n        return None\n\n\ndef add_query_param(url: str, param_name: str, param_value: str) -> str:\n    \"\"\"\n    Adds a query parameter to a URL.\n\n    Args:\n        url (str): The URL to add the parameter to.\n        param_name (str): The name of the query parameter.\n        param_value (str): The value of the query parameter.\n\n    Returns:\n        str: The modified URL with the added query parameter.\n    \"\"\"\n    parsed_url = urlparse(url)\n    query_params = parse_qs(parsed_url.query)\n\n    query_params[param_name] = [param_value]\n\n    new_query = urlencode(query_params, doseq=True)\n    new_url = urlunparse(parsed_url._replace(query=new_query))\n\n    return new_url\n\n\ndef remove_query_param(url: str, param_name: str) -> str:\n    \"\"\"\n    Removes a query parameter from a URL.\n\n    Args:\n        url (str): The URL to remove the parameter from.\n        param_name (str): The name of the query parameter.\n\n    Returns:\n        str: The modified URL with the parameter removed.\n    \"\"\"\n    parsed_url = urlparse(url)\n    query_params = parse_qs(parsed_url.query)\n\n    if param_name in query_params:\n        del query_params[param_name]\n\n    new_query = urlencode(query_params, doseq=True)\n    new_url = urlunparse(parsed_url._replace(query=new_query))\n\n    return new_url\n\n\ndef clean_path(path: str) -> str:\n    while \"..\" in path:\n        path = path.replace(\"..\", \"\")\n    path = path.replace(\"\\\\\", \"/\")\n    while \"//\" in path:\n        path = path.replace(\"//\", \"/\")\n    return path"}
