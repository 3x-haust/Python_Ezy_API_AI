{"repo_info": {"repo_name": "hai-platform", "repo_owner": "HFAiLab", "repo_url": "https://github.com/HFAiLab/hai-platform"}}
{"type": "source_file", "path": "api/query/optimized/task/__init__.py", "content": "\n\nfrom .implement import *\n"}
{"type": "source_file", "path": "api/query/optimized/__init__.py", "content": "\n"}
{"type": "source_file", "path": "api/depends/default.py", "content": "\nimport ujson\n\nfrom fastapi import HTTPException, Request\n\nfrom conf.flags import USER_ROLE\nfrom server_model.selector import AioUserSelector\n\n\ndef get_api_user_with_token(allowed_groups=[], allowed_scopes=[], **kwargs):\n    async def __func(request: Request):\n        token = None\n        if request.url.query:\n            req_query = {q.split('=')[0]: q.split('=')[1] for q in request.url.query.split('&') if '=' in q}\n            token = req_query.get('token')\n        if token is None:\n            try:\n                token = ujson.loads(await request.body())['token']\n            except Exception:\n                pass\n        if token is None:\n                raise HTTPException(status_code=403, detail={\n                    'success': 0,\n                    'msg': '需要指定 token'\n                })\n        user = await AioUserSelector.from_token(token=token)\n        if user is None:\n            expired_user = await AioUserSelector.from_token(token=token, allow_expired=True)\n            if expired_user is not None:\n                raise HTTPException(status_code=401, detail={\n                    'success': 0,\n                    'msg': '该 token 已经过期了，请重新提供凭证'\n                })\n            raise HTTPException(status_code=403, detail={\n                'success': 0,\n                'msg': '根据 token 未找到用户'\n            })\n        if not user.active:\n            raise HTTPException(status_code=401, detail={\n                'success': 0,\n                'msg': '您的账号为不活跃状态，无法访问集群服务'\n            })\n        if len(allowed_groups) > 0 and not user.in_any_group(allowed_groups):\n            raise HTTPException(status_code=401, detail={\n                'success': 0,\n                'msg': '您所在的权限组不允许使用该接口'\n            })\n        if len(allowed_scopes) > 0 and user.access.access_scope not in allowed_scopes:\n            raise HTTPException(status_code=401, detail={\n                'success': 0,\n                'msg': f'您当前的 access_scope({user.access.access_scope}) 不允许使用该接口'\n            })\n        for attr, value in kwargs.items():\n            try:\n                assert getattr(user, attr) == value\n            except Exception:\n                raise HTTPException(status_code=401, detail={\n                    'success': 0,\n                    'msg': '您无权访问该接口'\n                })\n        return user\n    return __func\n\n\ndef get_non_external_api_user_with_token(allowed_groups=[], allowed_scopes=[], **kwargs):\n    return get_api_user_with_token(allowed_groups=[USER_ROLE.INTERNAL] + allowed_groups, allowed_scopes=allowed_scopes, **kwargs)\n\n"}
{"type": "source_file", "path": "api/query/optimized/task/default.py", "content": "\n\nasync def get_task_distribute_api():\n    return {\n        'success': 1,\n        'result': {},\n        'msg': 'not implemented'\n    }\n"}
{"type": "source_file", "path": "api/operation/implement.py", "content": "\n\nfrom .default import *\nfrom .custom import *\n\n\nimport git\nimport os.path\nimport re\nfrom git import Repo\nfrom typing import TYPE_CHECKING, List\n\nfrom base_model.base_task import BaseTask\nfrom base_model.training_task import TrainingTask\nfrom conf import CONF\nfrom conf.flags import RunJobCode, BACKEND_UPGRADE, QUE_STATUS, TASK_TYPE, TASK_PRIORITY, TASK_OP_CODE\nfrom db import a_redis as redis, MarsDB\nfrom server_model.auto_task_impl import AutoTaskApiImpl\nfrom server_model.task_impl import AioDbOperationImpl\nfrom server_model.user import User\nfrom utils import convert_to_external_task\nfrom logm import logger\nfrom api.task_schema import TaskSchema, TaskService\n\n\nif TYPE_CHECKING:\n    from server_model.user import User\n\n\nasync def operate_task_base(\n        operate_user: str,\n        task: TrainingTask,\n        task_op_code=TASK_OP_CODE.STOP,\n        restart_delay: int = 0,\n        remote_apply: bool = False\n):\n    \"\"\"\n\n    @param operate_user:\n    @param task:\n    @param task_op_code: TASK_OP_CODE\n    @param restart_delay\n    @param remote_apply\n    @return:\n    \"\"\"\n    t = task\n    job_info = t.job_info\n    if t.queue_status == QUE_STATUS.FINISHED:\n        return {\n            'success': 1,\n            'msg': f'{job_info} 已经停止，不再[{task_op_code.value}]',\n        }\n\n    # 只能关闭队列中的 upgraded 任务\n    if t.backend == BACKEND_UPGRADE:\n        return {\n            'success': 0,\n            'msg': f'{t.job_info}: 执行停止出错, 不能操作升级任务'\n        }\n\n    if t.queue_status == QUE_STATUS.QUEUED:\n        if task_op_code == TASK_OP_CODE.SUSPEND:\n            return {\n                'success': 1,\n                'msg': f'{job_info} 已经 [{QUE_STATUS.QUEUED}]，不再[{task_op_code.value}]',\n            }\n        # STOP\n        # queue 的任务，需要标记成 finished\n        await t.re_impl(AioDbOperationImpl).update(fields=('queue_status', ),  values=(QUE_STATUS.FINISHED, ), remote_apply=remote_apply)\n        return {\n            'success': 1,\n            'msg': f'{job_info} 并没有在运行',\n        }\n\n    # QUE_STATUS.SCHEDULED:\n    try:\n        t.re_impl(AutoTaskApiImpl)\n        if task_op_code in {TASK_OP_CODE.STOP, TASK_OP_CODE.FAIL, TASK_OP_CODE.SUCCEED}:\n            msg = await t.stop(task_op_code=task_op_code)\n        else:\n            if task.user_name != operate_user:\n                msg = f'{operate_user} 试图打断 {task.user_name} 的任务 [{task.nb_name}][{task.id}]'\n                logger.warning(msg)\n                return {'success': 0, 'msg': '无权操作他人的任务'}\n            msg = await t.suspend(restart_delay)\n    except Exception as e:\n        msg = f'{job_info} 执行[{task_op_code.value}]时出现错误，请联系系统组: {e}'\n        return {\n            'success': 0,\n            'msg': msg,\n        }\n\n    return {\n        'success': 1,\n        'msg': f'{job_info} 已对 id {t.id} 执行 [{task_op_code.value}] 命令，原来状态{t.chain_status}，返回信息 {msg}',\n    }\n\n\nasync def check_restart(lt):\n    stop_code = 0\n    if await redis.get(f'ban:{lt.user_name}:{lt.nb_name}:{lt.chain_id}'):\n        return False\n    recorded_stop_code = await redis.get(f'lifecycle:{lt.id}:stop_code')\n    if not recorded_stop_code:\n        return False\n    for code in recorded_stop_code.decode().strip().split('\\n'):\n        stop_code |= int(code)\n    return 1 < stop_code < 64 or stop_code >= 2048\n\n\ndef get_service_config_error(service: TaskService, services: List[TaskService]):\n    # name check\n    if sum([svc.name == service.name for svc in services]) > 1:\n        return f'service name 有重复：{service.name}'\n    if re.fullmatch('[a-z0-9]([-a-z0-9]*[a-z0-9])?', service.name) is None:\n        return f\"{service.name} 服务的命名不合法：只能包含小写字母、数字，以及 '-'；必须以字母数字开头和结尾\"\n\n    # port check\n    if service.type != 'local':\n        if service.rank != [0]:\n            return f'{service.name} 服务类型不为 local, 仅支持在 rank=0 节点启动' # 涉及 nodeport 和 ingress, 不能多节点\n        if service.port is None:\n            return f'{service.name} 服务类型不为 local，必须指定端口'\n        if service.port not in range(1, 65536):\n            return f'{service.name} 服务的端口不合法：需要在 [1, 65535] 范围内'\n        for existing_svc in services:\n            if existing_svc != service and service.port == existing_svc.port:\n                return f'{service.name} 服务设定的端口 {service.port} 与 {existing_svc.name} 服务的端口冲突'\n    elif service.port is not None:\n        return f'{service.name} 服务类型为 local，不能指定端口'\n\n    # startup_script check\n    if service.startup_script and any(token in service.startup_script for token in [';', '\\n']):\n        return f'启动命令仅支持单条 bash 命令，复杂指令请编写 bash 脚本运行。({service.name} 服务启动命令包含 \";\" 或 \"\\\\n\")'\n\n    # type check\n    if service.type not in ['http', 'tcp', 'local']:\n        return f'不支持的服务类型：{service.type}'\n    return None\n\n\ndef check_services_config_get_err(services, user: User):\n    # 为内建服务设置端口, 以供后续检验端口是否有重复\n    for service in filter(lambda svc: svc.name in CONF.jupyter.builtin_services, services):\n        if any(x is not None for x in [service.port, service.type, service.startup_script]):\n            return f'\"{service.name}\" 是系统保留的内建服务名, 自定义服务请不要以此命名'\n        service.port = CONF.jupyter.builtin_services.get(service.name).get('port', None)\n    # 检查自定义服务的quota和参数\n    for service in filter(lambda svc: svc.name not in CONF.jupyter.builtin_services, services):\n        allowed_ports = user.quota.custom_service(service.type)\n        if '*' not in allowed_ports and str(service.port) not in allowed_ports:\n            return f'无权自定义服务 {service.type}:{service.port}'\n        if (err_msg := get_service_config_error(service, services)) is not None:\n            return err_msg\n    return None\n\n\nasync def create_base_task(task: BaseTask, tags: list = [], remote_apply: bool = False):\n    try:\n        task = await task.re_impl(AioDbOperationImpl).create(remote_apply=remote_apply)\n        async with MarsDB() as conn:\n            for tag in tags:\n                await task.tag_task(tag, a_db_conn=conn)\n    except Exception as e:\n        if \"already exists\" in str(e):\n            return {\n                'success': RunJobCode.EXISTS.value,\n                'msg': f'您名字为 {task.nb_name} 的任务正在运行，不能重复创建，请稍后重试'\n            }\n        else:\n            logger.exception(e)\n            return {\n                'success': RunJobCode.FATAL.value,\n                'msg': '未能在数据库中成功创建队列，请联系系统组',\n            }\n    task: TrainingTask = convert_to_external_task(task)\n    return {\n        'success': RunJobCode.QUEUED.value,\n        'msg': '任务创建队列成功，请等待调度',\n        'task': task.trait_dict()\n    }\n\n\nasync def create_task_base_queue_v2(user: User, task_schema: TaskSchema = None, raw_task_schema: dict = None, remote_apply: bool = False):\n    \"\"\"\n\n    :param task_schema:\n    :param user:\n    :param raw_task_schema: 原始的任务 schema 为 json 的 string\n    :param remote_apply: 创建任务是否要等到从库都返回了才成功\n    :return: dict(success, msg)\n    \"\"\"\n    if raw_task_schema is None:\n        raw_task_schema = task_schema.dict()\n    if task_schema is None:\n        task_schema = TaskSchema.parse_obj(raw_task_schema)\n    raw_task_schema.pop('token', None)\n\n    fatal_response = lambda msg: {'success': RunJobCode.FATAL.value, 'msg': msg}\n    # 基础校验\n    unsupported_chars = ['(', ')']\n    if any(illegal_chars := list(c for c in unsupported_chars if c in task_schema.name)):\n        return fatal_response(f'name 包含不支持的命名字符：{illegal_chars}')\n    if task_schema.version != 2:\n        return fatal_response('task config 版本不对，应该是 [2]')\n    if task_schema.task_type not in TASK_TYPE.all_task_types():\n        return fatal_response('无效的task_type')\n    if task_schema.priority not in TASK_PRIORITY.values():\n        return fatal_response('无效的 priority')\n    if task_schema.spec is None:\n        return fatal_response('必须指定 task spec')\n    if len(task_schema.spec.workspace) > 255:\n        return fatal_response('workspace 长度不应超过 255')\n    if ' ' in task_schema.spec.workspace:\n        return fatal_response('workspace 不允许有空格')\n    if task_schema.resource.node_count <= 0:\n        return fatal_response('节点数必须大于 0')\n    config_json = {}\n    # git 相关校验和配置补全\n    # -- 任务指定记录 workspace 中的 git revision\n    if task_schema.options.get('record_git_revision', False):\n        if task_schema.spec.git_remote_repo + task_schema.spec.git_target_revision != '':\n            return fatal_response('指定记录 workspace git revision 时不能指定 git_remote_repo/git_target_revision')\n        if not os.path.isdir(task_schema.spec.workspace):\n            return fatal_response('指定的 workspace 不是合法目录')\n        try:\n            repo = Repo(task_schema.spec.workspace)\n        except git.InvalidGitRepositoryError:\n            return fatal_response(f'指定的 workspace 不是合法的 git repo: {task_schema.spec.workspace}')\n        if repo.is_dirty() or any(repo.untracked_files):\n            return fatal_response(f'指定的 workspace 中当前有未提交的修改')\n        remote_repo, current_rev = repo.remote().url, repo.head.object.hexsha\n        config_json['git_commit_sha'] = current_rev\n        config_json['git_remote_repo'] = remote_repo\n    # -- 任务指定 git repo 作为 workspace\n    if task_schema.spec.git_remote_repo != '':\n        if task_schema.spec.workspace:\n            return fatal_response('指定 git repo 时不能指定 workspace')\n        task_schema.spec.workspace = '${HOME}/experiment_repo'\n        config_json['git_commit_sha'] = ''\n        config_json['git_remote_repo'] = task_schema.spec.git_remote_repo\n    elif task_schema.spec.workspace is None:\n        return fatal_response('不指定 git repo 时, 必须指定 workspace')\n    # 组装成数据库需要的 code file\n    code_file = os.path.join(task_schema.spec.workspace, task_schema.spec.entrypoint) + ' ' + task_schema.spec.parameters\n    if len(code_file) > 2047:\n        return fatal_response('运行命令 [workspace + entrypoint + params] 长度不应超过 2047')\n    # 不按照分组判断，以免有人换分组名字无限提交\n    MAX_TASKS = 10000\n    ts_count = (await MarsDB().a_execute(\"\"\"\n        select count(*) from \"unfinished_task_ng\"\n        where \"user_name\" = %s\n    \"\"\", (user.user_name, ))).fetchall()[0][0]\n    if ts_count >= MAX_TASKS:\n        return fatal_response(f'您提交的[未运行完成任务]已经超过了 [{MAX_TASKS}] 个')\n\n    # 获取 group\n    group = task_schema.resource.group\n    if (not task_schema.resource.group) or task_schema.resource.group.lower() == 'default':\n        group = CONF.scheduler.default_group\n    client_group = group\n    schedule_zone = None\n    if '#' in group:\n        group, schedule_zone = group.split('#')\n\n    # schedule_zone 相关\n    override_node_resource = task_schema.options.get('override_node_resource', None)\n\n    # 用户相关\n    if user.is_external:\n        task_schema.priority = TASK_PRIORITY.AUTO.value\n\n    # 对 image 进行处理, template 表示系统内建镜像；train_image 表示用户自定义镜像\n    if task_schema.resource.image is not None and '/' in task_schema.resource.image:\n        template = 'train_image:' + task_schema.resource.image.split('/')[-1]\n        train_image = task_schema.resource.image\n    else:\n        template = task_schema.resource.image or 'default'\n        train_image = None\n    if template in ['default', 'DEFAULT']:\n        user_train_envs = user.quota.train_environments\n        if len(user_train_envs) == 0:\n            return fatal_response('至少要有一个可用的 train_environments')\n        template = user_train_envs[0]\n    if (err_msg := await check_environment_get_err(train_image, template, user)) is not None:\n        return fatal_response(err_msg)\n\n    # service 配置校验\n    if (err_msg := check_services_config_get_err(task_schema.services, user)) is not None:\n        return fatal_response(err_msg)\n\n    # tag 校验\n    if any(t.startswith('_') for t in task_schema.options.get('tags', [])):\n        return fatal_response('任务 tag 不能以下划线开头')\n\n    # 对 train task 的处理\n    # if task_schema.task_type == TASK_TYPE.TRAINING_TASK:\n    #     # 只需要对 training 任务判断 quota\n    #     available_priority = user.quota.available_priority(group)\n    #     if task_schema.priority not in available_priority.values():\n    #         if len(available_priority.values()) > 0:\n    #             task_schema.priority = min(available_priority.values())\n    #         else:\n    #             msg = ' , '.join(\n    #                 f'{p_value} ({p_name})'\n    #                 for p_name, p_value in\n    #                 sorted(available_priority.items(), key=lambda p: p[1])\n    #             )\n    #             return {\n    #                 'success': RunJobCode.FATAL.value,\n    #                 'msg': f'无效的优先级，可选为：{msg}'\n    #             }\n\n    # 先构造一个基础的 task\n    task = BaseTask(\n        implement_cls=AioDbOperationImpl, id=None, nb_name=task_schema.name,\n        user_name=user.user_name, code_file=code_file,\n        workspace=task_schema.spec.workspace, group=group, nodes=task_schema.resource.node_count,\n        restart_count=0, # delete me\n        backend=template,\n        queue_status=QUE_STATUS.QUEUED, priority=task_schema.priority,\n        chain_id=None,   # delete me\n        task_type=task_schema.task_type,\n        whole_life_state=task_schema.options.get('whole_life_state', 0),\n        mount_code=task_schema.options.get('mount_code', 2),\n        # options 包容万物\n        assigned_nodes=task_schema.options.get('assigned_nodes', []),\n    )\n    task.user = user\n    task.config_json = {\n        'client_group': client_group,\n        'whole_life_state': task_schema.options.get('whole_life_state', 0),\n        'environments': task_schema.spec.environments,\n        'schedule_zone': schedule_zone,\n        'train_image': train_image,  # 用于 client 端获取镜像完整 URL\n        'override_node_resource': override_node_resource,\n        'schema': raw_task_schema,  # 保存提交时候的样子\n        **config_json\n    }\n    result = await process_create_task(task_schema=task_schema, task=task)\n    if result.get('success', 0) != 1:\n        return result\n    task = result['task']\n    # 给任务打 tag\n    tags = task_schema.options.get('tags', [])\n    if isinstance(tags, str):\n        tags = tags.split(',')\n    tags = [str(t) for t in tags if t] if isinstance(tags, list) else []\n    if task_schema.options.get('shared_in_group', False):\n        tags.append(user.shared_task_tag)\n    return await create_base_task(task, tags=tags, remote_apply=remote_apply)\n"}
{"type": "source_file", "path": "api/register/__init__.py", "content": "\n\nfrom .implement import *\n"}
{"type": "source_file", "path": "api/query/optimized/resource.py", "content": "\n\nimport copy\nfrom collections import defaultdict\n\nfrom fastapi import Depends\n\nfrom api.depends import get_api_user_with_token\nfrom conf.flags import USER_ROLE, ALL_USER_ROLES\nfrom k8s.async_v1_api import async_get_nodes_df\nfrom server_model.user import User\n\n\nasync def get_train_images(user: User = Depends(get_api_user_with_token())):\n    \"\"\" 获取内部的 train_image, 有两个来源，一个是用户组自己的 images，另一个是内建的 train_environment 表 \"\"\"\n    return {\n        'success': 1,\n        'result': await user.image.async_get()\n    }\n\n\nasync def get_nodes_detail_api(user: User = Depends(get_api_user_with_token())):\n    resource_df = await async_get_nodes_df(monitor=True)\n    return {\n        'success': 1,\n        'result': resource_df.to_dict('records')\n    }\n\n\ndef get_node_type_template():\n    count_template = {\n        'total': 0,\n        'service': 0,\n        'dev_and_release': {\n            'total': 0,\n            'release': 0,\n            'dev': 0,\n        },\n        'train': {\n            'total': 0,\n            'schedulable': {\n                'total': 0,\n                'free': 0,\n                'working': 0,\n                **{f'{role}_working': 0 for role in ALL_USER_ROLES}\n            },\n            'unschedulable': 0,\n        },\n        'err': 0,\n        'exclusive': 0,\n    }\n    return {\n        'count': copy.deepcopy(count_template),\n        'count_schedule_zone': defaultdict(lambda: copy.deepcopy(count_template)),\n        'detail': {\n            'err': defaultdict(int),\n            'service': defaultdict(int),\n            'exclusive': defaultdict(int),\n            'train': {\n                'working': {role: defaultdict(int) for role in ALL_USER_ROLES},\n                'free': defaultdict(int),\n                'unschedulable': defaultdict(int),\n            }\n        }\n    }\n\n\n# todo: 这个接口需要删掉，逻辑写到前端更自然\ndef get_nodes_overview_impl(nodes: list, for_monitor: bool):\n    overview = defaultdict(lambda: get_node_type_template())\n    for node in nodes:\n        overview[node['type']]['count']['total'] += 1\n        overview[node['type']]['count_schedule_zone'][node['schedule_zone']]['total'] += 1\n        if node['current_category'] == 'service':\n            overview[node['type']]['count']['service'] += 1\n            overview[node['type']]['count_schedule_zone'][node['schedule_zone']]['service'] += 1\n            overview[node['type']]['detail']['service'][node['use']] += 1\n        if node['current_category'] in ['release', 'dev']:\n            overview[node['type']]['count']['dev_and_release']['total'] += 1\n            overview[node['type']]['count_schedule_zone'][node['schedule_zone']]['dev_and_release']['total'] += 1\n            if node['current_category'] == 'release':\n                overview[node['type']]['count']['dev_and_release']['release'] += 1\n                overview[node['type']]['count_schedule_zone'][node['schedule_zone']]['dev_and_release']['release'] += 1\n            else:\n                overview[node['type']]['count']['dev_and_release']['dev'] += 1\n                overview[node['type']]['count_schedule_zone'][node['schedule_zone']]['dev_and_release']['dev'] += 1\n        if node['current_category'] == 'training':\n            overview[node['type']]['count']['train']['total'] += 1\n            overview[node['type']]['count_schedule_zone'][node['schedule_zone']]['train']['total'] += 1\n            if node['status'] == 'Ready':\n                overview[node['type']]['count']['train']['schedulable']['total'] += 1\n                overview[node['type']]['count_schedule_zone'][node['schedule_zone']]['train']['schedulable']['total'] += 1\n                if node['working'] is None:\n                    overview[node['type']]['count']['train']['schedulable']['free'] += 1\n                    overview[node['type']]['count_schedule_zone'][node['schedule_zone']]['train']['schedulable']['free'] += 1\n                    overview[node['type']]['detail']['train']['free'][node['group']] += 1\n                else:\n                    overview[node['type']]['count']['train']['schedulable']['working'] += 1\n                    overview[node['type']]['count_schedule_zone'][node['schedule_zone']]['train']['schedulable']['working'] += 1\n                    if (role := node['working_user_role']) in ALL_USER_ROLES:\n                        overview[node['type']]['count']['train']['schedulable'][f'{role}_working'] += 1\n                        overview[node['type']]['count_schedule_zone'][node['schedule_zone']]['train']['schedulable'][f'{role}_working'] += 1\n                        overview[node['type']]['detail']['train']['working'][role][node['working_user']] += 1\n            else:\n                overview[node['type']]['count']['train']['unschedulable'] += 1\n                overview[node['type']]['count_schedule_zone'][node['schedule_zone']]['train']['unschedulable'] += 1\n                overview[node['type']]['detail']['train']['unschedulable'][node['group']] += 1\n        if node['current_category'] == 'err':\n            overview[node['type']]['count']['err'] += 1\n            overview[node['type']]['count_schedule_zone'][node['schedule_zone']]['err'] += 1\n            overview[node['type']]['detail']['err'][node['group']] += 1\n        if node['current_category'] == 'exclusive':\n            overview[node['type']]['count']['exclusive'] += 1\n            overview[node['type']]['count_schedule_zone'][node['schedule_zone']]['exclusive'] += 1\n            overview[node['type']]['detail']['exclusive'][node['group']] += 1\n    if not for_monitor:\n        to_ret = overview['gpu']['count']\n        # 公开的展示接口，隐藏调度细节\n        for role in ALL_USER_ROLES:\n            del to_ret['train']['schedulable'][f'{role}_working']\n        return to_ret\n    return overview\n\n\nasync def get_nodes_overview_api(user: User = Depends(get_api_user_with_token())):\n    nodes = (await get_nodes_detail_api(user))['result']\n    return {\n        'success': 1,\n        'result': {\n            'nodes': nodes,\n            'overview': get_nodes_overview_impl(nodes, True)\n        }\n    }\n\n\nasync def get_cluster_overview_for_client_api(user: User = Depends(get_api_user_with_token())):\n    try:\n        nodes = (await get_nodes_detail_api(user))['result']\n        base = get_nodes_overview_impl(nodes, True)\n        types = ('cpu', 'gpu')\n        ret = {t: {} for t in types}\n        for typ in types:\n            if typ not in base:\n                ret[typ] = {t: 0 for t in ('other', 'usage_rate', 'total', 'working', 'free')}\n                continue\n            data = base[typ]['count']\n            others_count = data['dev_and_release']['total'] + data['err'] + data['train']['unschedulable'] + data['service'] + data['exclusive']\n            usage_rate = 0 if data['train']['total'] == 0 else (data['train']['total'] - data['train']['schedulable']['free'] ) / data['train']['total']\n            ret[typ] = {\n                'other': others_count,\n                'usage_rate': usage_rate,\n                'total': data['total'],\n                'working': data['train']['schedulable']['working'],\n                'free': data['train']['schedulable']['free'],\n            }\n        resp = {\n            'success': 1,\n            'result': ret['gpu'].copy()\n        }\n        resp['result']['gpu_detail'] = ret['gpu']\n        if not user.is_external:\n            resp['result']['cpu_detail'] = ret['cpu']\n        return resp\n    except Exception as e:\n        print(f'Internal calculation error: {str(e)}')\n        return {\n              'success': 0,\n              'msg': 'Internal calculation error'\n        }\n\n"}
{"type": "source_file", "path": "api/register/implement.py", "content": "\n\nfrom .default import *\nfrom .custom import *\n\n\nimport api.task.experiment as at_exp\nimport api.task.port as at_port\nimport api.task.service_task as at_service_task\nimport api.user.admin as au_admin\nimport api.user.user as au_user\nimport api.resource.cluster as ar_cluster\nimport api.training as a_train\nimport api.query.optimized.task as aq_optimized_task\nimport api.query.optimized.user as aq_optimized_user\nimport api.query.optimized.resource as aq_optimized_resource\nimport api.query.optimized.service_task as aq_optimized_service_task\nimport api.user.access as au_access\nimport api.resource.cloud_storage as ar_cloud_storage\nimport api.resource.storage as ar_storage\n\n\nif 'operating' in REG_SERVERS:\n    app.post('/operating/task/create')(at_exp.create_task_v2)\n    app.post('/operating/task/resume')(at_exp.resume_task)\n    app.post('/operating/task/stop')(at_exp.stop_task)\n    app.post('/operating/task/suspend')(at_exp.suspend_task_by_name)\n    app.post('/operating/task/tag')(at_exp.tag_task)\n    app.post('/operating/task/untag')(at_exp.untag_task)\n    app.post('/operating/task/share')(at_exp.share_task)\n    app.post('/operating/task/unshare')(at_exp.unshare_task)\n    app.post('/operating/task/fail')(a_train.fail_task)\n    app.post('/operating/task/priority/update')(at_exp.update_priority)\n    app.post('/operating/task/group/update')(at_exp.switch_group)\n    app.post('/operating/task/service_control')(at_exp.service_control_api)\n    app.post('/operating/task/restart_log/set')(at_exp.set_task_restart_log_api)\n\n    app.post('/operating/task/artifact/map')(at_exp.map_task_artifact)\n    app.post('/operating/task/artifact/unmap')(at_exp.unmap_task_artifact)\n\n    app.post('/operating/user/tag/delete')(at_exp.delete_tags)\n    app.post('/operating/user/training_quota/update')(au_user.set_user_gpu_quota)\n    app.post('/operating/user/training_quota_limit/update')(au_admin.set_user_gpu_quota_limit)\n    app.post('/operating/user/access_token/create')(au_access.create_access_token)\n    app.post('/operating/user/access_token/delete')(au_access.delete_access_token)\n    app.post('/operating/user/active/update')(au_admin.set_user_active_state_api)\n    app.post('/operating/user/create')(au_admin.create_user_api)\n    app.post('/operating/user/group/update')(au_admin.update_user_group)\n    app.post('/operating/user/artifact/createupdate')(au_user.create_update_user_artifact)\n    app.post('/operating/user/artifact/delete')(au_user.delete_user_artifact)\n\n    app.post('/operating/service_task/delete')(at_service_task.delete_task_api)\n    app.post('/operating/service_task/move_node')(at_service_task.move_node_api)\n\n    app.post('/operating/node/state/update')(ar_cluster.change_node_state_api)\n    app.post('/operating/node/host_info/update')(ar_cluster.update_host_info_api)\n    app.post('/operating/node/host_info/create')(ar_cluster.create_host_info_api)\n    app.post('/operating/node/host_info/delete')(ar_cluster.delete_host_info_api)\n    app.post('/operating/node/label')(ar_cluster.label_node_api)\n\n    app.post('/operating/mount_point/create')(ar_storage.create_mount_point)\n    app.post('/operating/mount_point/delete')(ar_storage.delete_mount_point)\n\n    app.post('/operating/storage/add_monitor_dir')(ar_storage.add_monitor_dir)\n\n\nif 'ugc' in REG_SERVERS:\n    app.post('/ugc/user/nodeport/create')(at_port.node_port_svc)\n    app.post('/ugc/user/nodeport/delete')(at_port.delete_node_port_svc)\n    app.post('/ugc/user/nodeport/bind')(at_port.bind_node_port_svc)\n    app.post('/ugc/user/train_image/list')(aq_optimized_resource.get_train_images)\n\n    app.post('/ugc/cloud/cluster_files/list')(ar_cloud_storage.list_cluster_files)\n\n\nif 'query' in REG_SERVERS:\n    app.post('/query/task')(aq_optimized_task.get_task_api)\n    app.post('/query/task/log')(at_exp.task_node_log_api)\n    app.post('/query/task/sys_log')(at_exp.task_sys_log_api)\n    app.post('/query/task/log/search')(at_exp.task_search_in_global)\n    app.post('/query/task/ssh_ip')(at_port.task_ssh_ip)\n    app.post('/query/task/list')(aq_optimized_task.get_tasks_api)\n    app.post('/query/task/list_all_unfinished')(aq_optimized_task.get_running_tasks_api)\n    app.post('/query/task/list_all_with_priority')(aq_optimized_task.get_tasks_overview)\n    app.post('/query/task/container_monitor_stats/list')(aq_optimized_task.get_task_container_monitor_stats_api)\n    app.post('/query/task/time_range_overview')(aq_optimized_task.get_time_range_schedule_info_api)\n    app.post('/query/task/get_task_on_node')(at_exp.get_task_on_node_api)\n\n    app.post('/query/task/artifact/get')(at_exp.get_task_artifact_mapping)\n    app.post('/query/task/artifact/list')(at_exp.get_all_task_artifact_mapping)\n    app.post('/query/user/artifact/get')(au_user.get_user_artifact)\n\n    app.post('/query/service_task/list')(aq_optimized_service_task.data_api)\n    app.post('/query/service_task/list_all')(aq_optimized_service_task.all_tasks_api)\n\n    app.post('/query/user/info')(aq_optimized_user.get_user_api)\n    app.post('/query/user/list_all')(aq_optimized_user.get_all_user_api)\n    app.post('/query/user/quota/list')(au_user.get_user_all_quota)\n    app.post('/query/user/training_quota')(aq_optimized_user.get_user_node_quota_api)\n    app.post('/query/user/training_quota/get_used')(aq_optimized_user.get_quota_used_api)\n    app.post('/query/user/training_quota/list_all')(aq_optimized_user.get_all_user_node_quota_api)\n    app.post('/query/user/training_quota/list')(au_admin.get_all_user_priority_quota)\n    ##### 兼容接口, 保留几个版本\n    app.post('/query/user/training_quota/internal_list_all')(au_admin.get_internal_user_priority_quota)\n    app.post('/query/user/training_quota/external_list_all')(au_admin.get_external_user_priority_quota)\n    #####\n    app.post('/query/user/access_token/list')(au_access.list_access_token)\n    app.post('/query/user/tag/list')(at_exp.get_task_tags)\n    app.post('/query/user/nodeport/list')(at_port.get_node_port_svc_list)\n\n    app.post('/query/node/list')(ar_cluster.cluster_df)\n    app.post('/query/node/host_info')(ar_cluster.get_host_info_api)\n    app.post('/query/node/overview')(aq_optimized_resource.get_nodes_overview_api)\n    app.post('/query/node/client_overview')(aq_optimized_resource.get_cluster_overview_for_client_api)\n\n    app.post('/query/storage/get_by_task')(ar_storage.get_storage_by_task)\n\n\nif 'monitor' in REG_SERVERS:\n    app.post('/monitor/task/chain_perf_series')(at_exp.chain_perf_series_api)\n    app.post('/monitor/user/storage/list')(ar_storage.get_user_storage_list)\n\n\nlogger.info(f'通过 server.py 初始化了 app: {app}')\n"}
{"type": "source_file", "path": "api/resource/image/default.py", "content": "\n\nasync def hfai_image_load():\n    return {'success': 1, 'msg': 'not implemented'}\n\n\nasync def hfai_image_update_status():\n    return {'success': 1, 'msg': 'not implemented'}\n\n\nasync def hfai_image_list():\n    return {'success': 1, 'data': [], 'msg': 'not implemented'}\n\n\nasync def hfai_image_delete():\n    return {'success': 1, 'msg': 'not implemented'}\n"}
{"type": "source_file", "path": "api/resource/__init__.py", "content": ""}
{"type": "source_file", "path": "api/query/__init__.py", "content": "\n"}
{"type": "source_file", "path": "api/resource/cloud_storage/default.py", "content": "\nfrom conf import CONF\n\nasync def get_sts_token():\n    return {'success': 1, f'{CONF.cloud.storage.provider}': {}, 'msg': 'not implemented'}\n\n\nasync def set_external_user_cloud_storage_quota():\n    return {'success': 1, 'msg': 'not implemented'}\n\n\nasync def set_sync_status():\n    return {'success': 1, 'msg': 'not implemented'}\n\n\nasync def get_sync_status():\n    return {'success': 1, 'data': [], 'msg': 'not implemented'}\n\n\nasync def delete_files():\n    return {'success': 1, 'msg': 'not implemented'}\n\n\nasync def list_cluster_files():\n    return []\n\n\nasync def sync_to_cluster():\n    return {'success': 1, 'msg': 'not implemented', 'index': None, 'dst_path': None}\n\n\nasync def sync_to_cluster_status():\n    return {'success': 1, 'status': None, 'msg': 'not implemented'}\n\n\nasync def sync_from_cluster():\n    return {'success': 1, 'msg': 'not implemented', 'index': None}\n\n\nasync def sync_from_cluster_status():\n    return {'success': 1, 'status': None, 'msg': 'not implemented'}\n"}
{"type": "source_file", "path": "api/operation/__init__.py", "content": "\n\nfrom .implement import *\n"}
{"type": "source_file", "path": "api/depends/implement.py", "content": "\nfrom .default import *\nfrom .custom import *\n\nfrom typing import List\n\nimport ujson\nfrom fastapi import Depends, HTTPException, Request\nfrom pydantic import BaseModel\n\nfrom base_model.training_task import TrainingTask\nfrom conf.flags import TASK_TYPE, USER_ROLE\nfrom server_model.selector import AioTrainingTaskSelector, AioUserSelector, AioBaseTaskSelector\nfrom server_model.user import User\n\n\nJUPYTER_ADMIN_GROUP = 'hub_admin'   # 能够操作其他人容器的权限组\nTASK_ADMIN_GROUP = 'task_admin'     # 能够操作其他人任务的权限组\n\n\nclass RestTask(BaseModel):\n    code_file: str\n    workspace: str = ''\n    token: str\n    # 用于训练任务\n    nb_name: str = None\n    environments: dict = {}\n    nodes: int = None\n    group: str = None\n    includes: str = ''\n    template: str = 'DEFAULT'\n    is_queue_job: int = 0\n    priority: int = 0\n    can_restart: int = 0\n    task_type: str = TASK_TYPE.TRAINING_TASK\n    force_restart1: int = 0\n    # 用于升级任务\n    upgrade_nodes: List[str] = None\n    upgrade_name: str = None\n    version: str = None\n    include_failed: int = 0\n    whole_life_state: int = 0\n    mount_code: int = 2\n    schedule_zone: str = None\n    train_image: str = None  # 和 template 相对，指的是用户使用哪个 image 来运行\n    options: dict = {}  # create_experiment 里的 options\n\n\nasync def request_limitation():\n    if True:\n        return\n\n\ndef get_internal_api_user_with_token(allowed_groups=[], allowed_scopes=[], **kwargs):\n    return get_api_user_with_token(allowed_groups=[USER_ROLE.INTERNAL] + allowed_groups, allowed_scopes=allowed_scopes, **kwargs)\n\n\nasync def get_api_user_with_name(user_name: str = None):\n    if user_name is None:\n        raise HTTPException(status_code=401, detail={\n            'success': 0,\n            'msg': '非法请求'\n        })\n    user = await AioUserSelector.find_one(user_name=user_name)\n    if user is None:\n        raise HTTPException(status_code=401, detail={\n            'success': 0,\n            'msg': '根据user_name未找到用户'\n        })\n    return user\n\n\ndef check_user_access_to_task(task: TrainingTask, user: User, allow_shared_task=False):\n    if task.task_type == TASK_TYPE.JUPYTER_TASK:\n        # jupyter 任务, 允许 admin 操作\n        if task.user_name != user.user_name and not user.in_any_group([JUPYTER_ADMIN_GROUP, TASK_ADMIN_GROUP]):\n            raise HTTPException(status_code=403, detail='无权操作其他人的容器')\n    elif task.user_name != user.user_name and not user.in_group(TASK_ADMIN_GROUP) \\\n            and not (allow_shared_task and user.shared_task_tag in task.tags):\n        msg = f'[未授权操作] [{user.user_name}] 不能操作 [{task.user_name}] 的任务[{task.job_info}]，已经阻断!'\n        raise HTTPException(status_code=403, detail=msg)\n\n\ndef get_api_task(check_user=True, chain_task=True, allow_shared_task=False):\n    async def __func(user: User = Depends(get_api_user_with_token()), chain_id: str = None, nb_name: str = None, id: int = None) -> TrainingTask:\n        aio_selector = AioTrainingTaskSelector if chain_task else AioBaseTaskSelector\n        if id is not None:\n            t: TrainingTask = await aio_selector.find_one(None, id=id)\n        elif chain_id is not None:\n            t: TrainingTask = await aio_selector.find_one(None, chain_id=chain_id)\n        else:\n            t: TrainingTask = await aio_selector.find_one(None, nb_name=nb_name, user_name=user.user_name)\n        if t is None:\n            raise HTTPException(status_code=401, detail={\n                'success': 0,\n                'msg': f'no task of user [{user.user_name}] with id [{id}] or chain_id [{chain_id}] or nb_name [{nb_name}]!'\n            })\n        if check_user:\n            check_user_access_to_task(task=t, user=user, allow_shared_task=allow_shared_task)\n        if t.user_name == user.user_name:\n            t.user = user\n        return t\n    return __func\n\n\nasync def get_new_nb_name(nb_name: str):\n    return nb_name[len('DL_CLUSTER_'):]\n\n\nclass API_NOTES(BaseModel):\n    content: str\n\n\nclass ChainIds(BaseModel):\n    chain_id_list: str = ''\n    saved_path: str = ''\n"}
{"type": "source_file", "path": "api/depends/__init__.py", "content": "\nfrom .implement import *\n"}
{"type": "source_file", "path": "api/query/optimized/service_task/default.py", "content": "\n\nasync def extra_data(user):\n    ''' `/query/service_task/list` 接口中的补充信息 '''\n    return {}\n"}
{"type": "source_file", "path": "api/resource/dataset/implement.py", "content": "\n\nfrom .default import *\nfrom .custom import *\n"}
{"type": "source_file", "path": "api/resource/image/implement.py", "content": "\n\nfrom .default import *\nfrom .custom import *\n"}
{"type": "source_file", "path": "api/query/optimized/service_task/__init__.py", "content": "\nfrom .implement import *\n"}
{"type": "source_file", "path": "api/resource/dataset/__init__.py", "content": "\n\nfrom .implement import *\n"}
{"type": "source_file", "path": "api/resource/monitor/default.py", "content": "\n\nasync def get_node_summary_series_api():\n    return []\n\n\nasync def fetch_health_dict_api():\n    return {}\n\n\nasync def get_daily_stats_api():\n    return []\n\n\nasync def get_history_storage_stat_api():\n    return []\n\n\nasync def get_all_storage_quota_api():\n    return {}\n\n\nasync def get_user_monitor_info():\n    return {'success': 1, 'result': {}, 'msg': 'not implemented'}\n"}
{"type": "source_file", "path": "api/resource/cloud_storage/implement.py", "content": "\n\nfrom .default import *\nfrom .custom import *\n"}
{"type": "source_file", "path": "api/resource/monitor/__init__.py", "content": "\n\nfrom .implement import *\n"}
{"type": "source_file", "path": "api/resource/cluster/__init__.py", "content": "\nfrom .implement import *\n"}
{"type": "source_file", "path": "api/app.py", "content": "import binascii\n\nimport multiprocessing\nimport os\nimport re\nimport urllib.parse\nimport uuid\nimport base64\nimport aiohttp\nfrom datetime import datetime\n\nimport prometheus_fastapi_instrumentator as pfi\nfrom fastapi import FastAPI, Request\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom starlette.exceptions import HTTPException as StarletteHTTPException\nfrom starlette.responses import JSONResponse as StarletteJSONResponse\nfrom conf import CONF\nfrom logm import logger\nfrom db import MarsDB\nfrom roman_parliament import register_parliament\nfrom server_model.user_data import initialize_user_data_roaming\n\ntry:\n    with os.popen('git describe --abbrev=0 2>/dev/null') as p:\n        client_tag = p.read().strip()\nexcept:\n    client_tag = ''\nif not client_tag:\n    client_tag = 'NotFound'\nprint(f'client_tag: {client_tag}', flush=True)\n\nmodule_name = os.environ.get('MODULE_NAME', '')\nif module_name == 'server':  # 理论上应该在server.py里写，但uvicorn有bug，先这么写\n    num_worker = CONF.server_workers.operating\n    is_main_process = multiprocessing.current_process().name == 'MainProcess'\n    worker_rank = int(multiprocessing.current_process().name.split('-')[-1]) if not is_main_process else 0\n    if num_worker == 1 or not is_main_process:  # 单进程的server 或者多个进程且不是主进程，都参与会议\n        if os.environ.get('REPLICA_RANK') == '0' and (num_worker == 1 or worker_rank == 1):\n            os.environ['USER_DATA_SYNC_POINT'] = '1'  # 固定 server-0 的第一个 worker 进程是 USER_DATA 的同步点\n        register_parliament()\n\n\napp = FastAPI()\n\n\nif module_name == 'server':\n    # 在 server 上启动 swagger-ui\n    from swagger_ui import api_doc\n    api_doc(\n        app,\n        config_path='api/openapi_specification/user_api.yaml',\n        url_prefix='/swagger',\n        title='MarsV2 User API',\n        parameters={\n            'persistAuthorization': 'true',\n            'tryItOutEnabled': 'true',\n        }\n    )\n\n\nALL_USERS = {}\n\n\ndef update_all_users(token):\n    global ALL_USERS\n    for user_name, token in MarsDB().execute(\"\"\"select \"user_name\", \"token\" from \"user\" where \"token\" = %s \"\"\", (token, )).fetchall():\n        ALL_USERS[token] = user_name\n\n\ndef query_mask(url: str, query_key: str, keep: float = 0.2):\n    if query_key in url:\n        query_key_index = url.index(query_key)\n        if '&' in url[query_key_index:]:\n            next_query_index = url[query_key_index:].index('&') + query_key_index\n        else:\n            next_query_index = len(url)\n        query_value = url[query_key_index + len(query_key):next_query_index]\n        keep_chars = int(len(query_value) * keep)\n        query_value = query_value[:keep_chars] + '???' + query_value[-keep_chars:]\n        return url[:query_key_index] + query_key + query_value + query_mask(url[next_query_index:], query_key, keep)\n    else:\n        return url\n\n# 无实际业务功能的api\nlog_ignore_api = ['/metrics', '/api_server_status']\n# 耗时的api\nwarning_ignore_api = [\n    '/list_cluster_files', '/sync_to_cluster', '/sync_from_cluster', '/delete_files',\n    '/ugc/list_cluster_files', '/ugc/sync_to_cluster', '/ugc/sync_from_cluster', '/ugc/delete_files',\n    '/ugc/cloud/cluster_files/list'\n]\n\nbff_report_url = CONF.try_get('server_url.raw_bff.external')\nbff_report_url = f'{bff_report_url}/agg_fetion/report/cluster' if bff_report_url else None\n\n\n@app.middleware(\"http\")\n@logger.catch\nasync def log_requests(request: Request, call_next):\n    if request.url.query:\n        url = urllib.parse.unquote(f'{request.url.path}?{request.url.query}')\n    else:\n        url = urllib.parse.unquote(request.url.path)\n    if 'token=' in url:\n        url = query_mask(url, 'token=')\n    if 'access_token=' in url:\n        url = query_mask(url, 'access_token=')\n    # 日志中不暴露 token\n    if 'get_user_info/' in url:\n        url = re.sub(r'get_user_info/([^/]+)/(.+)', r'get_user_info/\\2?token=\\1', url)\n        url = query_mask(url, 'token=')\n    if 'get_worker_user_info/' in url:\n        url = re.sub(r'get_worker_user_info/(.+)', r'get_worker_user_info?token=\\1', url)\n        url = query_mask(url, 'token=')\n    if 'set_user_gpu_quota/' in url:\n        url = re.sub(r'set_user_gpu_quota/([^/]+)/(.+)', r'set_user_gpu_quota/\\2?token=\\1', url)\n        url = query_mask(url, 'token=')\n    log = logger.info\n    if url in log_ignore_api:\n        log = logger.trace\n    start_time = datetime.now()\n    log(f'[REQ] - {request.method} - \"{url}\"')\n    try:\n        response = await call_next(request)\n        exc = None\n    except Exception as e:\n        exc = e\n    end_time = datetime.now()\n    seconds = (end_time - start_time).total_seconds()\n    if request.url.path not in warning_ignore_api:\n        if seconds > 5:\n            log = logger.warning\n        if seconds > 10:\n            log = logger.error\n    if exc is None:\n        log(f'[RES] - {request.method} - \"{url}\" - [{response.status_code}] - {seconds * 1000:.2f}ms')\n    if bff_report_url and os.environ.get('DISABLE_BFF_ALERT', 'False') != 'True':\n        if exc is not None or seconds > 5:\n            report_data = {\n                \"payload\": {\n                    \"method\": request.method,\n                    \"url\": request.url.path,\n                    \"status_code\": response.status_code if exc is None else 500,\n                    \"response_time\": round(seconds * 1000, 2)\n                }\n            }\n            try:\n                async with aiohttp.ClientSession() as session:\n                    async with await session.post(url=bff_report_url, json=report_data, timeout=1) as r:\n                        pass\n            except Exception as e:\n                print('上报信息出现错误：', e)\n    if exc is not None:\n        raise exc\n    return response\n\n\n@app.middleware(\"http\")\nasync def add_client_version(request: Request, call_next):\n    response = await call_next(request)\n    response.headers['client-version'] = client_tag\n    return response\n\nhost = os.environ.get('HOSTNAME', 'no_env')\ntry:\n    host = host.split('-')[1]\nexcept Exception:\n    pass\n\n\n@app.middleware('http')\nasync def add_log_context(request: Request, call_next):\n    token = None\n    if request.url.query:\n        req_query = {q.split('=')[0]: q.split('=')[1] for q in request.url.query.split('&') if '=' in q}\n        token = req_query.get('token')\n    if token is not None and ALL_USERS.get(token) is None:\n        update_all_users(token)\n    req_user = ALL_USERS.get(token, 'NA') if token else 'NA'\n    if token is not None and token.startswith('ACCESS-'):\n        split_tokens = token.split('-')\n        try:\n            decoded_users = base64.b16decode(split_tokens[1].upper().encode()).decode()\n        except binascii.Error:\n            decoded_users = '(Invalid Token)'\n        for sep_char in ['#', '-']:\n            try:\n                from_user_name, access_user_name = decoded_users.split(sep_char)\n                if access_user_name != from_user_name:\n                    req_user = f'{access_user_name}({from_user_name})'\n                else:\n                    req_user = access_user_name\n                break\n            except Exception:\n                pass\n        else:\n            req_user = decoded_users\n    # pass to call_next\n    request.state.mars_token = token\n    request.state.mars_user = req_user\n\n    uid = f\"{str(uuid.uuid4())[:6]}-{datetime.now().strftime('%m%d_%H%M')}-{host}-{req_user}\"\n    with logger.contextualize(uuid=uid):\n        try:\n            response = await call_next(request)\n            return response\n        except Exception as e:\n            logger.error(e)\n\n\n@app.exception_handler(StarletteHTTPException)\nasync def http_exception_handler(request, exc):\n    if isinstance(exc.detail, str):\n        _res_json = {\n            'success': 0,\n            'msg': exc.detail\n        }\n    elif isinstance(exc.detail, dict):\n        _res_json = exc.detail\n    else:\n        raise Exception('无法处理的 HTTPException 错误类型：', exc.detail)\n    return StarletteJSONResponse(_res_json, status_code=exc.status_code)\n\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=['*'],\n    allow_credentials=True,\n    allow_methods=['*'],\n    allow_headers=['*'],\n)\n\ninstrumentator = pfi.Instrumentator(\n    should_group_status_codes=True,\n    should_ignore_untemplated=True,\n    should_instrument_requests_inprogress=True,\n    inprogress_labels=True,\n)\n\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    if module_name == 'server':\n        initialize_user_data_roaming(tables_to_subscribe='*')\n    instrumentator.instrument(app).expose(app)\n\n\n@app.get('/api_server_status')\nasync def api_server_status():\n    \"\"\"\n    判断 api server 是不是 work 了\n    @return:\n    \"\"\"\n    return {\n        'success': 1,\n        'status': 'running'\n    }\n"}
{"type": "source_file", "path": "api/resource/storage/implement.py", "content": "from .default import *\nfrom .custom import *\n\nfrom fastapi import Depends, HTTPException, Body\nfrom logm import logger\nfrom pydantic import BaseModel, validator\nfrom typing import List, Optional, Dict\n\nfrom api.depends import get_api_user_with_token\nfrom conf.flags import ALL_USER_ROLES\nfrom db import MarsDB\nfrom server_model.user import User\nfrom server_model.user_data import UserWithAllGroupsTable\nfrom server_model.selector import AioUserSelector, AioBaseTaskSelector\nfrom server_model.task_impl import AioDbOperationImpl, SingleTaskImpl\n\n\nWARNING_NOTE = '(使用 force=true 忽略告警)'\n\n\nclass MountPoint(BaseModel):\n    host_path: str\n    mount_path: str\n    owners: List[str]\n    conditions: List[str]\n    mount_type: str\n    read_only: Optional[bool] = None\n\n    @validator('owners')\n    def ensure_single_owner(cls, v):\n        if len(v) != 1:\n            raise ValueError('owners 仅支持长度为1的列表, 如有需要可添加多条记录')\n        return v\n\n    @validator('conditions')\n    def conditions_no_or(cls, vlist):\n        for v in vlist:\n            if ' or ' in v:\n                raise ValueError('conditions 不允许使用 or 运算符, 如有需要可添加多条记录')\n            if ' and ' in v:\n                raise ValueError('conditions 不允许使用 and 运算符, 请通过拆分为多个 condition 的方式实现 and 逻辑')\n        return vlist\n\n\nasync def get_user_storage_list(user: User = Depends(get_api_user_with_token())):\n    \"\"\"获取用户的可用挂载点路径\"\"\"\n    return {\n        'success': 1,\n        'storages': await user.storage.async_get()\n    }\n\n\nasync def _insert_mount_point(mount_point: MountPoint, action: str):\n    sql = 'insert into \"storage\"(\"host_path\", \"mount_path\", \"owners\", \"conditions\", \"mount_type\", \"read_only\", \"action\")' \\\n          'values (%s, %s, %s, %s, %s, %s, %s)'\n    params = (\n        mount_point.host_path, mount_point.mount_path, mount_point.owners, mount_point.conditions,\n        mount_point.mount_type, mount_point.read_only, action)\n    logger.info(f'插入挂载记录 {mount_point} action={action}')\n    return await MarsDB().a_execute(sql, params)\n\n\nasync def _get_mount_points(mount_point: MountPoint, action: str = None):\n    sql = 'select * from \"storage\"' \\\n          'where \"host_path\"=%s and \"mount_path\"=%s and %s && \"owners\"::text[] and \"active\"' \\\n          ' and \"conditions\"::text[] @> %s and \"conditions\"::text[] <@ %s'\n    params = [mount_point.host_path, mount_point.mount_path, mount_point.owners, mount_point.conditions, mount_point.conditions]\n    if action is not None:\n        sql += ' and action = %s'\n        params.append(action)\n    return await MarsDB().a_execute(sql, tuple(params))\n\n\nasync def _delete_mount_points(mount_point: MountPoint, action: str = None):\n    sql = 'delete from \"storage\"' \\\n          'where \"host_path\"=%s and \"mount_path\"=%s and %s && \"owners\"::text[] and \"active\"' \\\n          ' and \"conditions\"::text[] @> %s and \"conditions\"::text[] <@ %s'\n    params = [mount_point.host_path, mount_point.mount_path, mount_point.owners, mount_point.conditions, mount_point.conditions]\n    if action is not None:\n        sql += ' and action = %s'\n        params.append(action)\n    logger.info(f'删除挂载记录 {mount_point} action={action}')\n    return await MarsDB().a_execute(sql, tuple(params))\n\n\nasync def check_conflict(mount_point: MountPoint):\n    \"\"\" 提前检查变更后可能存在的冲突, 即 同优先级存在多条 mount_path 相同的挂载记录的情况 \"\"\"\n    owner = mount_point.owners[0]\n    mount_info = lambda m: f'[{m.host_path}]->[{m.mount_path}], owners={m.owners}, conditions={m.conditions}, action={m.action}'\n\n    # 筛选同优先级、可能冲突的 conditions 条件\n    conditions_where, conditions_params = \\\n        (''' \"conditions\" = '{}' ''', tuple()) if len(mount_point.conditions) == 0 else \\\n        ('(\"conditions\"::text[] <@ %s or \"conditions\"::text[] @> %s)', (mount_point.conditions, mount_point.conditions))\n\n    if (await AioUserSelector.find_one(user_name=owner)) is not None:\n        # owner 是用户, 则查 owners 同为该用户、属于同一个优先级 rank 的挂载记录, 其在特定 conditions 会发生冲突\n        sql = f'select * from \"storage\" where \"mount_path\" = %s and %s=any(\"owners\") and \"active\" and {conditions_where}'\n        records = await MarsDB().a_execute(sql, params=(mount_point.mount_path, owner, *conditions_params))\n        if len(records := records.fetchall()) > 0:\n            return f'将添加的挂载记录与以下记录冲突:\\n' + '\\n'.join(mount_info(r) for r in records)\n    else:\n        user_df = await UserWithAllGroupsTable.async_df\n        # owner 是用户组, 先查其包含的所有用户\n        users = user_df[user_df.user_groups.apply(lambda gs: owner in gs)].user_name.tolist()\n        # 再查这些用户属于的所有组\n        groups = user_df[user_df.user_name.apply(lambda uname: uname in users)].user_groups.explode().tolist()\n        groups = list(set(groups))\n        # 再查 owners 是这些组、属于同一个优先级 rank 的挂载记录, 其在特定 conditions 下会在特定用户身上发生冲突\n        sql = f'select * from \"storage\" where \"mount_path\" = %s and \"owners\"::text[] && %s and \"active\" and {conditions_where}'\n        records = await MarsDB().a_execute(sql, params=(mount_point.mount_path, groups, *conditions_params))\n        if len(records := records.fetchall()) > 0:\n            msg = '将添加的挂载记录与以下记录冲突:\\n'\n            for r in records:\n                msg += mount_info(r) + ', '\n                overlap_users = user_df[user_df.user_groups.apply(lambda gs: owner in gs and r.owners[0] in gs)].user_name.tolist()\n                if len(overlap_users) > 5:\n                    msg += '影响: ' + ','.join(overlap_users[:5]) + f'...(共{len(overlap_users)})\\n'\n                else:\n                    msg += '影响: ' + ','.join(overlap_users) + '\\n'\n            return msg\n\n\nasync def create_mount_point(mount_point: MountPoint, force: bool = False,\n                             user: User = Depends(get_api_user_with_token(allowed_groups=['ops', 'cluster_manager']))):\n    if mount_point.read_only is None:\n        raise HTTPException(status_code=400, detail='添加挂载点时必须指定 read_only')\n\n    # 1. 尝试找到 action=remove 的挂载点, 之前若 remove 过, 本次的操作是撤销 remove 动作\n    records = (await _get_mount_points(mount_point)).fetchall()\n    if len(records) > 1:\n        raise HTTPException(status_code=400,\n                            detail='当前 (host_path, mount_path, owners, conditions) 对应多条挂载记录, 需要人工检查')\n    elif len(records) > 0:\n        # 指定参数存在挂载记录\n        if records[0].action == 'add':\n            raise HTTPException(status_code=400,\n                                detail='当前 (host_path, mount_path, owners, conditions) 已存在 action=add 的挂载记录')\n        await _delete_mount_points(mount_point, action='remove')\n        return {\n            'success': 1,\n            'msg': '指定参数存在 action=remove 的挂载记录, 现已删除'\n        }\n\n    # 2. 指定参数不存在挂载记录, 添加之前进行校验\n    if not force:\n        user_or_group = mount_point.owners[0]\n        user = await AioUserSelector.find_one(user_name=user_or_group)\n        group = await MarsDB().a_execute('select 1 from \"user_group\" where \"group\" = %s', (user_or_group,))\n        if user is None and len(group.fetchall()) == 0 and user_or_group not in (['public'] + ALL_USER_ROLES):\n            return {'success': 0, 'msg': f'Warning: 指定的 owner [{user_or_group}] 不是现有的用户名或用户组. ' + WARNING_NOTE}\n        if user is not None:\n            # 指定用户时, 检查是否已经通过用户组获得了挂载\n            target_mount = mount_point.copy(update={'owners': user.group_list})\n            records = await _get_mount_points(mount_point=target_mount, action='add')\n            if (record := records.fetchone()) is not None:\n                return {'success': 0, 'msg': f'Warning: 指定用户 {user.user_name} 已经通过 {record.owners[0]} 组获得了该挂载点. ' + WARNING_NOTE}\n        if (err_msg := await security_check(mount_point)) is not None:     # 安全相关的检验\n            return {'success': 0, 'msg': f'Warning: 安全检查未通过 {WARNING_NOTE}:\\n{err_msg}'}\n        if (err_msg := await check_conflict(mount_point)) is not None:\n            return {'success': 0, 'msg': err_msg}\n\n    try:\n        await _insert_mount_point(mount_point=mount_point, action='add')\n    except Exception as e:\n        logger.exception(e)\n        return {'success': 0, 'msg': f'添加 action=add 的挂载记录失败: {e}'}\n    return {\n        'success': 1,\n        'msg': '添加 action=add 的挂载记录成功'\n    }\n\n\nasync def delete_mount_point(mount_point: MountPoint, force: bool = False,\n                             user: User = Depends(get_api_user_with_token(allowed_groups=['ops', 'cluster_manager']))):\n    # 0. 含有通配符时可以直接写入\n    if '*' in mount_point.host_path or '*' in mount_point.mount_path:\n        try:\n            await _insert_mount_point(mount_point=mount_point, action='remove')\n        except Exception as e:\n            logger.exception(e)\n            return {'success': 0, 'msg': f'直接添加含有通配符的挂载记录失败: {e}'}\n        return {\n            'success': 1,\n            'msg': '添加 action=remove 的挂载记录成功'\n        }\n\n    # 1. 尝试找到 action=add 的挂载点, 之前若 add 过, 本次的操作是撤销 add 动作\n    records = (await _get_mount_points(mount_point, action='add')).fetchall()\n    if len(records) > 0:\n        await _delete_mount_points(mount_point, action='add')\n        return {\n            'success': 1,\n            'msg': '找到了 action=add 的挂载记录并删除成功'\n        }\n\n    # 2. 之前没有 add 过, 但 owner 是用户时, 先检查其用户组是否挂载了这个 host_path, 尽量避免无意义的 remove 动作\n    if not force and (user := await AioUserSelector.find_one(user_name=mount_point.owners[0])) is not None:\n        target_mount = mount_point.copy(update={'owners': user.group_list})\n        records = await _get_mount_points(target_mount, action='add')\n        if records.fetchone() is None:\n            return {'success': 0, 'msg': f'Warning: 指定用户 {user.user_name} 在指定 condition 下没有该挂载点. ' + WARNING_NOTE}\n\n    # 3. 添加 remove 动作\n    # 检查冲突\n    if not force and (err_msg := await check_conflict(mount_point)) is not None:\n        return {'success': 0, 'msg': err_msg}\n\n    try:\n        await _insert_mount_point(mount_point=mount_point, action='remove')\n    except Exception as e:\n        logger.exception(e)\n        return {'success': 0, 'msg': f'添加 action=remove 的挂载记录失败 {e}'}\n    return {'success': 1, 'msg': '添加 action=remove 的挂载记录成功'}\n\n\nclass MonitorDirectory(BaseModel):\n    type: str\n    host_path: str\n    tag: str = ''\n    user_name: str = ''\n    inode_id: int = None\n\n\nasync def add_monitor_dir(monitor_dir: MonitorDirectory,\n                          user: User = Depends(get_api_user_with_token(allowed_groups=['ops', 'cluster_manager']))):\n    if monitor_dir.type not in ['weka', 'ceph', '3fs', '3fs-cpu']:\n        raise HTTPException(400, detail=f'不支持的 type {monitor_dir.type}')\n    sql = '''\n        insert into storage_monitor_dir(\"type\", \"host_path\", \"tag\", \"user_name\", \"inode_id\")\n        values (%s, %s, %s, %s, %s)\n    '''\n    try:\n        await MarsDB().a_execute(sql, params=(\n            monitor_dir.type, monitor_dir.host_path, monitor_dir.tag, monitor_dir.user_name, monitor_dir.inode_id))\n    except Exception as e:\n        logger.exception(e)\n        return {'success': 0, 'msg': f'添加 monitor dir 失败: {e}'}\n    return {'success': 1, 'msg': '添加成功'}\n\n\nasync def get_storage_by_task(task_id: int, overwrite_attributes: Dict = Body(default={}),\n                              user: User = Depends(get_api_user_with_token(allowed_groups=['ops', 'platform']))):\n    \"\"\"\n    查询任务容器的挂载点列表, 只包含数据库中配置的挂载, 不包括处理 sidecar 等起任务时动态添加的挂载\n    overwrite_attributes: 可以覆盖部分任务的属性方便测试\n    \"\"\"\n    task = await AioBaseTaskSelector.find_one(None, id=task_id)\n    if task is None:\n        raise HTTPException(status_code=400, detail=f'任务 ID {task_id} 不存在')\n    task_traits = task.trait_dict()\n    if any(missing_key := list(k for k in overwrite_attributes if k not in task_traits)):\n        raise HTTPException(status_code=400, detail=f'任务属性 {missing_key} 不存在')\n    for k, v in overwrite_attributes.items():\n        setattr(task, k, v)\n    task_user = await AioUserSelector.find_one(user_name=task.user_name)\n    await task_user.storage.create_storage_df()\n    storages = task_user.storage.personal_storage(task=task)\n    for storage in storages:\n        del storage['name'] # 这个属性不需要, 只有起任务的时候需要\n    return {'success': 1, 'result': storages}\n"}
{"type": "source_file", "path": "api/resource/storage/default.py", "content": "\nfrom __future__ import annotations\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from .implement import MountPoint\n\n\nasync def update_cluster_venv():\n    return {'success': 1, 'msg': 'not implemented', 'path': None}\n\n\nasync def get_user_weka_usage():\n    return {'success': 1, 'result': {}, 'msg': 'not implemented'}\n\n\nasync def get_3fs_monitor_dir_api():\n    return []\n\n\nasync def get_external_user_storage_usage():\n    return {'success': 1, 'result': {}, 'msg': 'not implemented'}\n\n\nasync def security_check(mount_point: MountPoint):\n    \"\"\" 添加挂载点时的安全校验 \"\"\"\n    pass\n"}
{"type": "source_file", "path": "api/task/service_task/__init__.py", "content": "\n\nfrom .implement import *\n"}
{"type": "source_file", "path": "api/resource/monitor/implement.py", "content": "\n\nfrom .default import *\nfrom .custom import *\n"}
{"type": "source_file", "path": "api/task/monitor/implement.py", "content": "\n\nfrom .default import *\nfrom .custom import *\n"}
{"type": "source_file", "path": "api/task/experiment/default.py", "content": "\n\nasync def create_task():\n    return {\n        'success': 1,\n        'msg': 'not implemented'\n    }\n\n\nasync def validate_task():\n    return {\n        'success': 1,\n        'msg': 'not implemented'\n    }\n\n\nasync def validate_nodes():\n    return {\n        'success': 1,\n        'msg': 'not implemented'\n    }\n\n\nasync def haiprof_task():\n    return {\n        'success': 1,\n        'msg': 'not implemented'\n    }\n\n\nasync def switch_schedule_zone():\n    return {\n        'success': 1,\n        'msg': 'not implemented'\n    }\n\n\nasync def task_sys_log_api():\n    return {\n        'success': 1,\n        'msg': 'not implemented',\n        'data': ''\n    }\n\n\nasync def task_container_log_api():\n    return {\n        'success': 1,\n        'msg': 'not implemented',\n        'result': {\n            'data': ''\n        }\n    }\n"}
{"type": "source_file", "path": "api/task/service_task/default.py", "content": "\nasync def checkpoint_api():\n    return {\n        'success': 1,\n        'result': \"success\",\n        'msg': 'not implemented'\n    }\n"}
{"type": "source_file", "path": "api/task/port.py", "content": "\nfrom fastapi import Depends, HTTPException\n\nfrom api.depends import get_api_task, get_internal_api_user_with_token, get_non_external_api_user_with_token\nfrom base_model.training_task import TrainingTask\nfrom server_model.selector import AioUserSelector\nfrom server_model.user import User\nfrom utils import run_cmd_aio\nfrom conf import CONF\nfrom logm import logger\n\n# TODO(role): 是否开放 nodeport\n\nasync def node_port_svc(task: TrainingTask = Depends(get_api_task()),\n                          usage: str = 'ssh', rank: int = 0,\n                          dist_port: int = 22, user: User = Depends(get_internal_api_user_with_token())):\n    assert rank >= 0\n    try:\n        result = await user.nodeport.async_create(task=task, alias=usage, dist_port=dist_port, rank=rank)\n    except Exception as exception:\n        logger.exception(exception)\n        raise HTTPException(status_code=400, detail={'success': 0, 'msg': str(exception)})\n    else:\n        result['msg'] = '端口已经暴露' if result['existed'] else '暴露端口成功'\n        del result['existed']\n        return {'success': 1, 'result': result}\n\n\nasync def delete_node_port_svc(task: TrainingTask = Depends(get_api_task()),\n                               rank: int = 0, dist_port: int = 22,\n                               usage: str = None,   # `usage` is deprecated\n                               user: User = Depends(get_internal_api_user_with_token())):\n    assert rank >= 0\n    try:\n        await user.nodeport.async_delete(task, dist_port=dist_port, rank=rank)\n    except Exception as exception:\n        logger.exception(exception)\n        raise HTTPException(status_code=400, detail={'success': 0, 'msg': str(exception)})\n    else:\n        return {'success': 1, 'msg': '删除成功'}\n\n\nasync def get_node_port_svc_list(user: User = Depends(get_internal_api_user_with_token())):\n    return {'success': 1, 'result': await user.nodeport.async_get()}\n\n\nasync def task_ssh_ip(pod_name: str, user: User = Depends(get_non_external_api_user_with_token())):\n    ip = (await run_cmd_aio(f\"kubectl -n {user.config.task_namespace} get pod {pod_name} -o wide | grep -v NAME | awk '{{print $6}}'\"))[0].decode().replace(\"\\n\", \"\")\n    return {\n            'success': 1,\n            'ip': ip\n        }\n\n\nasync def bind_node_port_svc(user_name: str, alias: str, dist_port: int, src_port: int, nb_name: str, rank: int = 0,\n                             random_on_conflict: bool = True,\n                             api_user: User = Depends(get_internal_api_user_with_token(allowed_groups=['cluster_manager', 'account_manager']))):\n    if (user := await AioUserSelector.find_one(user_name=user_name)) is None:\n        raise HTTPException(404, detail=f'用户 [{user_name}] 不存在')\n    try:\n        res = await user.nodeport.async_bind(alias=alias, dist_port=dist_port, src_port=src_port, nb_name=nb_name, rank=rank,\n                                             random_on_conflict=random_on_conflict)\n    except Exception as e:\n        return {'success': 0, 'msg': f'创建 nodeport 失败: {e}'}\n    return {'success': 1, 'msg': '创建成功', 'result': res}\n"}
{"type": "source_file", "path": "api/task/monitor/default.py", "content": "\n\nasync def syslog_api():\n    return {'success': 1, 'msg': 'not implemented'}\n"}
{"type": "source_file", "path": "api/task/__init__.py", "content": ""}
{"type": "source_file", "path": "api/operation/default.py", "content": "\nfrom typing import Dict\n\nfrom api.task_schema import TaskSchema\nfrom base_model.base_task import BaseTask\nfrom server_model.selector import TrainImageSelector, AioTrainEnvironmentSelector\n\n\nasync def check_environment_get_err(train_image, template, user):\n    if train_image is None:\n        # 使用内建镜像\n        if (await AioTrainEnvironmentSelector.find_one(env_name=template)) is None:\n            return f'内建镜像 [{template}] 不存在, 请检查拼写'\n        if template not in user.quota.train_environments:\n            return f'用户没有使用内建镜像 [{template}] 的权限'\n    else:\n        if len(train_image.split('/')) != 3:\n            return 'train_image 格式不正确, 请检查. 仅支持镜像 URL, 请参考 hfai client 文档.'\n        # 使用自定义镜像\n        valid_image_urls = await TrainImageSelector.a_find_user_group_image_urls(shared_group=user.shared_group, status='loaded')\n        if train_image not in valid_image_urls:\n            return f'用户所在的组 [{user.shared_group}] 不存在镜像 [{train_image}] 或镜像仍在加载, 请使用命令 `hfai images list` 检查'\n    return None\n\n\nasync def create_task_base_queue(*args, **kwargs):\n    return {\n        'success': 1,\n        'msg': 'not implemented',\n        'task': {}\n    }\n\n\nasync def check_sidecar_get_err(task_schema: TaskSchema, task: BaseTask):\n    return None\n\n\nasync def process_create_task(task_schema: TaskSchema, task: BaseTask) -> Dict:\n    return {\n        'success': 1,\n        'task': task\n    }\n"}
{"type": "source_file", "path": "api/__init__.py", "content": "\n\nfrom base_model.utils import setup_custom_finder\n\nsetup_custom_finder()\n"}
{"type": "source_file", "path": "api/resource/dataset/default.py", "content": "\n\nasync def clone_dataset():\n    return {'success': 1, 'msg': 'not implemented'}\n\n\nasync def clone_dataset_update_status():\n    return {'success': 1, 'msg': 'not implemented'}\n\n\nasync def clone_dataset_get_status():\n    return {'success': 1, 'status': None, 'msg': 'not implemented'}\n"}
{"type": "source_file", "path": "api/task/experiment/__init__.py", "content": "\n\nfrom .implement import *\n"}
{"type": "source_file", "path": "api/resource/cluster/implement.py", "content": "\nfrom .default import *\nfrom .custom import *\n\nfrom itertools import chain\nfrom typing import Optional, List, Union\n\nimport pandas as pd\nfrom typing import List\nfrom fastapi import Depends, HTTPException\nfrom kubernetes.client import ApiException\nfrom pydantic import BaseModel\n\nfrom api.depends import get_api_user_with_token, get_internal_api_user_with_token\nfrom conf import MARS_GROUP_FLAG, CONF\nfrom conf.flags import MOUNT_CODE\nfrom db import MarsDB\nfrom k8s.async_v1_api import async_get_nodes_df, async_read_node, async_set_node_label\nfrom server_model.user import User\n\n\nasync def cluster_df(monitor: bool = True, user: User = Depends(get_api_user_with_token())):\n    \"\"\"\n    获取集群的状态，接口返回 dataframe\n    @return:\n    \"\"\"\n    df = await async_get_nodes_df(monitor=monitor)\n    await user.quota.create_quota_df()\n    df = post_process_cluster_df(df, user)\n    return dict(\n        success=1,\n        cluster_df=df.to_dict('records'),\n        containers=user.quota.train_environments,\n        mount_code=MOUNT_CODE\n    )\n\n\nasync def change_node_state_api(node_name: str, state: str, user: User = Depends(\n    get_api_user_with_token(allowed_groups=['cluster_manager', 'ops']))):\n    try:\n        node = await async_read_node(node_name)\n    except ApiException as e:\n        if e.status == 404:\n            raise HTTPException(404, detail=f'要操作的节点 [{node_name}] 不存在')\n        else: raise HTTPException(400, detail=f'读取节点信息失败') from e\n    if state not in ['enabled', 'disabled']:\n        raise HTTPException(400, detail=f'不合法的目标状态 [{state}]')\n\n    err_group = CONF.scheduler.error_node_meta_group\n    group_prefix = f'{err_group}.manually_disabled_by_'\n    mars_group = node.metadata.labels[MARS_GROUP_FLAG]\n    if state == 'disabled':\n        if mars_group.startswith(err_group):\n            raise HTTPException(400, detail=f'节点 [{node_name}] 未启用 (group={mars_group})')\n        new_group = group_prefix + user.user_name\n    else:   # enabled\n        if not mars_group.startswith(group_prefix):\n            raise HTTPException(400, detail=f'节点 [{node_name}] 未通过本接口禁用 (group={mars_group})')\n        origin_group = (await MarsDB().a_execute('select \"origin_group\" from \"host\" where node = %s', (node_name, ))).fetchone()[0]\n        new_group = origin_group\n    res = await async_set_node_label(node_name, MARS_GROUP_FLAG, new_group)\n    return {\n        'success': res,\n        'msg': f'设置节点状态为{state}' + '成功' if res else '失败'\n    }\n\n\nclass HostInfo(BaseModel):\n    node: str\n    gpu_num: int = None\n    type: str = None\n    use: str = None\n    origin_group: str = None\n    room: str = None\n    schedule_zone: str = None\n    flags: List[str] = []\n\n\nasync def update_host_info_api(node: HostInfo, user: User = Depends(\n        get_api_user_with_token(allowed_groups=['cluster_manager', 'ops']))):\n    update_attrs = {k: v for k, v in node.dict().items() if v is not None and k != 'node'}\n    if len(update_attrs) == 0:\n        raise HTTPException(status_code=400, detail=f'必须至少指定一个要更新的属性')\n    assigns = ','.join(f'\"{k}\"=%s' for k in update_attrs)\n    sql = f'update \"host\" set {assigns} where \"node\" = %s'\n    if (await MarsDB().a_execute(sql, (*update_attrs.values(), node.node))).rowcount == 0:\n        raise HTTPException(404, detail=f'未找到指定的节点 [{node.node}]')\n    return {\n        'success': 1,\n        'msg': '更新成功'\n    }\n\n\nasync def delete_host_info_api(node: str, user: User = Depends(\n        get_api_user_with_token(allowed_groups=['cluster_manager', 'ops']))):\n    sql = 'delete from \"host\" where \"node\" = %s'\n    if (await MarsDB().a_execute(sql, (node,))).rowcount == 0:\n        raise HTTPException(404, detail=f'未找到指定的节点 [{node}]')\n    return {\n        'success': 1,\n        'msg': '删除成功'\n    }\n\n\nasync def create_host_info_api(node: HostInfo, user: User = Depends(\n        get_api_user_with_token(allowed_groups=['cluster_manager', 'ops']))):\n    nodes = [node]  # 暂时只支持单条处理\n    for node in nodes:\n        if any([v is None for v in node.dict().values()]):\n            raise HTTPException(status_code=400, detail=f'创建 host info 时必须指定全部属性')\n\n    sql = 'select node from \"host\" where \"node\" in (' + ','.join(['%s'] * len(nodes)) + ')'\n    if len(dups := (await MarsDB().a_execute(sql=sql, params=tuple(node.node for node in nodes))).fetchall()) > 0:\n        return {'success': 0, 'msg': f'以下节点已经存在: {[dup.node for dup in dups]}'}\n\n    sql = 'insert into \"host\"(\"node\", \"gpu_num\", \"type\", \"use\", \"origin_group\", \"room\", \"schedule_zone\") values ' \\\n        + ','.join([f'(%s, {int(node.gpu_num)}, %s, %s, %s, %s, %s)' for node in nodes])\n    params = tuple(chain.from_iterable(\n        [node.node, node.type, node.use, node.origin_group, node.room, node.schedule_zone] for node in nodes))\n    await MarsDB().a_execute(sql=sql, params=params)\n    return {'success': 1, 'msg': '创建成功'}\n\n\nasync def get_host_info_api(node_regex: str = '', type: str = None, use: str = None, origin_group: str = None,\n                            schedule_zone: str = None,\n                            user: User = Depends(get_internal_api_user_with_token())):\n    filter = {'type': type, 'use': use, 'origin_group': origin_group, 'schedule_zone': schedule_zone}\n    where = ' '.join(f'and {k} = %s' for k in filter if filter[k] is not None)\n    params = [node_regex] + list(v for v in filter.values() if v is not None)\n    sql = f'''\n        select \"node\", \"gpu_num\", \"type\", \"use\", \"origin_group\", \"schedule_zone\"\n        from \"host\" where \"node\" ~ %s {where}\n    '''\n    rows = (await MarsDB().a_execute(sql, tuple(params))).fetchall()\n    return {\n        'success': 1,\n        'result': {\n            'data': [dict(row) for row in rows]\n        }\n    }\n\n\nasync def label_node_api(node_name: str, label: str, user: User = Depends(get_api_user_with_token(allowed_groups=['cluster_manager', 'ops']))):\n    try:\n        await async_read_node(node_name)\n    except ApiException as e:\n        if e.status == 404:\n            raise HTTPException(404, detail=f'要操作的节点 [{node_name}] 不存在')\n        else: raise HTTPException(400, detail=f'读取节点信息失败') from e\n    res = await async_set_node_label(node_name, MARS_GROUP_FLAG, label)\n    return {\n        'success': res,\n        'msg': f'设置节点标签为{label}' + '成功' if res else '失败'\n    }\n"}
{"type": "source_file", "path": "api/resource/cluster/default.py", "content": "\ndef post_process_cluster_df(df, user):\n    return df\n"}
{"type": "source_file", "path": "api/task/monitor/__init__.py", "content": "\n\nfrom .implement import *\n"}
{"type": "source_file", "path": "api/task/experiment/implement.py", "content": "\nfrom .default import *\nfrom .custom import *\n\nimport ujson\nimport ciso8601\nimport datetime\nimport inspect\nimport json\nimport pickle\nimport time\nimport urllib\nfrom typing import Optional, List\nfrom fastapi import Depends, Request, Query, HTTPException\n\nfrom logm import logger\nfrom api.depends import get_api_user_with_token, get_api_task, JUPYTER_ADMIN_GROUP, check_user_access_to_task\nfrom api.task_schema import TaskSchema\nfrom api.task.service_task import create_service_task, VISIBLE_TASK_TAG\nfrom api.operation import operate_task_base, create_task_base_queue_v2\nfrom base_model.training_task import TrainingTask\nfrom conf.flags import TASK_OP_CODE, TASK_PRIORITY, QUE_STATUS, STOP_CODE, EXP_STATUS, TASK_TYPE\nfrom db import MarsDB\nfrom db import a_redis as redis\nfrom server_model.auto_task_impl import AutoTaskApiImpl\nfrom server_model.selector import AioUserSelector\nfrom server_model.task_impl import AioDbOperationImpl\nfrom server_model.training_task_impl import TaskApiImpl, DashboardApiImpl\nfrom server_model.user import User\nfrom server_model.task_runtime_config import TaskRuntimeConfig\nfrom utils import convert_to_external_node, convert_to_external_task, get_task_node_idx_log\n\n\nasync def create_task_v2(\n        request: Request,\n        task_schema: TaskSchema,\n        user_name: str = None,\n        api_user: User = Depends(get_api_user_with_token()),\n    ):\n        if user_name is not None and api_user.user_name != user_name:     # 为其他人创建任务\n            # 目前只允许 hub admin 为他人创建开发容器\n            if task_schema.task_type != TASK_TYPE.JUPYTER_TASK or not api_user.in_group(JUPYTER_ADMIN_GROUP):\n                raise HTTPException(403, detail='无权为他人创建任务')\n            user = await AioUserSelector.find_one(user_name=user_name)\n            if user is None:\n                raise HTTPException(404, detail=f'用户 [{user_name}] 不存在')\n        else:\n            user = api_user\n\n        if task_schema.task_type == TASK_TYPE.JUPYTER_TASK:\n            return await create_service_task(user=user, task_schema=task_schema, raw_task_schema=await request.json())\n        else:\n            return await create_task_base_queue_v2(user=user, task_schema=task_schema, raw_task_schema=await request.json(), remote_apply=True)\n\n\nasync def resume_task(\n    task: TrainingTask = Depends(get_api_task()),\n    user: User = Depends(get_api_user_with_token())\n):\n    task.re_impl(AioDbOperationImpl)\n    try:\n        await redis.delete(f'ban:{task.user_name}:{task.nb_name}:{task.chain_id}')\n        task = await task.resume(remote_apply=True)\n    except Exception as e:\n        if \"already exists\" in str(e):\n            return {\n                'success': 0,\n                'msg': f'您名字为 {task.nb_name} 的任务正在运行，不能重复创建，请稍后重试'\n            }\n        else:\n            logger.exception(e)\n            return {\n                'success': 0,\n                'msg': '未能在数据库中成功创建队列，请联系系统组',\n            }\n    task: TrainingTask = convert_to_external_task(task)\n    return {\n        'success': 1,\n        'msg': '直接插入队列成功，请等待调度',\n        'task': task.trait_dict()\n    }\n\n\nasync def stop_task(op: TASK_OP_CODE = TASK_OP_CODE.STOP, task: TrainingTask = Depends(get_api_task())):\n    await redis.set(f'ban:{task.user_name}:{task.nb_name}:{task.chain_id}', 1)\n    res = await operate_task_base(operate_user=task.user_name, task=task, task_op_code=op, remote_apply=False)\n    return res\n\n\nasync def suspend_task_by_name(\n        task: TrainingTask = Depends(get_api_task()),\n        restart_delay: int = 0,\n        version: str = \"new\",   # 兼容 jupyter, 前端更新后删除\n):\n    res = await operate_task_base(operate_user=task.user_name, task=task, task_op_code=TASK_OP_CODE.SUSPEND, restart_delay=restart_delay, remote_apply=False)\n    return res\n\n\nasync def task_node_log_api(task: TrainingTask = Depends(get_api_task(allow_shared_task=True)), rank: int = 0,\n                            last_seen: str = 'null', service: str = None):\n    try:\n        last_seen = json.loads(last_seen)\n    except:\n        last_seen = None\n    if last_seen:\n        try:\n            last_seen['timestamp'] = datetime.datetime.strptime(last_seen['timestamp'], \"%Y-%m-%dT%H:%M:%S.%f\")\n        except:\n            last_seen['timestamp'] = datetime.datetime.strptime(last_seen['timestamp'], \"%Y-%m-%dT%H:%M:%S\")\n    task.re_impl(AutoTaskApiImpl)\n    res = await task.log(rank, last_seen=last_seen, service=service)\n    # 兜底逻辑，任务没启动就失败了，日志文件都没有，标记 stop\n    if task.queue_status == QUE_STATUS.FINISHED and res['stop_code'] == STOP_CODE.NO_STOP and res['data'] == '还没产生日志':\n        res['stop_code'] = STOP_CODE.STOP\n    return res\n\n\nasync def task_sys_log_api(task: TrainingTask = Depends(get_api_task())):\n    res = await task.re_impl(TaskApiImpl).sys_log()\n    # if not user.is_internal:\n    #     res['data'] = ''  # 系统错误日志里含节点信息，先不给外部用户看\n    return res\n\n\nasync def task_search_in_global(content, task: TrainingTask = Depends(get_api_task()), user=Depends(get_api_user_with_token())):\n    content = urllib.parse.unquote(content)\n    res = await task.re_impl(TaskApiImpl).search_in_global(content)\n    return res\n\n\nasync def chain_perf_series_api(task: TrainingTask = Depends(get_api_task(allow_shared_task=True)),\n                                user: User = Depends(get_api_user_with_token()), typ: str = 'gpu', rank: int = 0, data_interval: Optional[str]= '5min'):\n    # data_interval 使用query参数传入\n    if data_interval not in ('1min', '5min'):\n        data_interval = '5min'\n    try:\n        data = await task.re_impl(DashboardApiImpl).get_chain_time_series(typ, rank, data_interval=data_interval)\n        if user.is_external:\n            for item in data:\n                if 'node' in item:\n                    item['node'] = convert_to_external_node(item['node'], 'rank', item['rank'])\n        return {\n                'success': 1,\n                'data': data\n        }\n    except ValueError as e:\n        return{\n            'success':0,\n            'msg': str(e)\n        }\n\n\nasync def tag_task(tag: str, task: TrainingTask = Depends(get_api_task())):\n    if tag.startswith('_'):\n        return {'success': 0, 'msg': '以下划线开头的 tag 是系统保留 tag, 请使用其他命名'}\n    await task.re_impl(AioDbOperationImpl).tag_task(tag, remote_apply=True)\n    return {\n        'success': 1,\n        'msg': f'训练任务[{task.job_info}] 设置 tag {tag} 标记成功'\n    }\n\n\nasync def untag_task(tag: str, task: TrainingTask = Depends(get_api_task())):\n    if tag.startswith('_'):\n        return {'success': 0, 'msg': '以下划线开头的 tag 是系统保留 tag, 无法删除'}\n    await task.re_impl(AioDbOperationImpl).untag_task(tag, remote_apply=True)\n    return {\n        'success': 1,\n        'msg': f'训练任务[{task.job_info}] 取消 tag {tag} 标记成功'\n    }\n\n\nasync def delete_tags(tag: List[str] = Query(default=None), user: User = Depends(get_api_user_with_token())):\n    if tag is None:\n        return {\n            'success': 0,\n            'msg': '请指定要删除的 tag'\n        }\n    if any(t.startswith('_') for t in tag):\n        return {'success': 0, 'msg': '以下划线开头的 tag 是系统保留 tag, 无法删除'}\n    await MarsDB().a_execute(f\"\"\"\n    delete from \"task_tag\" where \"user_name\" = '{user.user_name}' and tag in ('{\"','\".join(tag)}')\n    \"\"\")\n    return {\n        'success': 1,\n        'msg': f'成功删除 tag {tag}'\n    }\n\n\nasync def get_task_tags(user: User = Depends(get_api_user_with_token())):\n    tags = [\n        r.tag for r in\n        await MarsDB().a_execute(f\"\"\"select distinct \"tag\" from \"task_tag\" where user_name = '{user.user_name}' \"\"\")\n    ]\n    tags = [t for t in tags if not t.startswith('_')]\n    return {\n        'success': 1,\n        'result': tags\n    }\n\n\nasync def share_task(task: TrainingTask = Depends(get_api_task())):\n    await task.re_impl(AioDbOperationImpl).tag_task(task.user.shared_task_tag, remote_apply=True)\n    return {\n        'success': 1,\n        'msg': f'训练任务[{task.job_info}] 设置为组内共享成功'\n    }\n\n\nasync def unshare_task(task: TrainingTask = Depends(get_api_task())):\n    await task.re_impl(AioDbOperationImpl).untag_task(task.user.shared_task_tag, remote_apply=True)\n    return {\n        'success': 1,\n        'msg': f'训练任务[{task.job_info}] 取消组内共享成功'\n    }\n\n\nasync def map_task_artifact(artifact_name: str, artifact_version: str, direction: str, task: TrainingTask = Depends(get_api_task())):\n    err_msg = await task.re_impl(AioDbOperationImpl).map_task_artifact(artifact_name, artifact_version, direction, remote_apply=True)\n    if err_msg:\n        return {\n            'success': 0,\n            'msg': err_msg\n        }\n    return {\n        'success': 1,\n        'msg': f'任务[{task.job_info}] 设置 {\"input\" if input else \"output\"} artifact {artifact_name}:{artifact_version} 成功'\n    }\n\n\nasync def unmap_task_artifact(direction: str, task: TrainingTask = Depends(get_api_task())):\n    await task.re_impl(AioDbOperationImpl).unmap_task_artifact(direction, remote_apply=True)\n    return {\n        'success': 1,\n        'msg': f'任务[{task.job_info}] 移除 artifact 成功'\n    }\n\n\nasync def get_task_artifact_mapping(task: TrainingTask = Depends(get_api_task(check_user=False))):\n    artifact = await task.re_impl(AioDbOperationImpl).get_task_artifact(remote_apply=True)\n    return {\n        'success': 1,\n        'msg': artifact\n    }\n\n\nasync def get_all_task_artifact_mapping(artifact_name: str = '', artifact_version: str = 'default', page: int = 1, page_size: int = 1000, days: int = 90, user: User = Depends(get_api_user_with_token())):\n    now = datetime.datetime.now()\n    last = now - datetime.timedelta(days=days)\n    artifact_info = [f'{user.user_name}:{artifact_name}:{artifact_version}', f'{user.shared_group}:{artifact_name}:{artifact_version}']\n    _sql = f\"\"\"\n        select \"user_name\", \"chain_id\", \"nb_name\", \"task_ids\", \"in_artifact\", \"out_artifact\"\n        from \"task_artifact_mapping\"\n        where (\"user_name\" = '{user.user_name}' or\n              \"chain_id\" in (select \"chain_id\" from \"task_tag\" where \"tag\" = '{user.shared_task_tag}'))\n              and (\"created_at\" between '{last.strftime(\"%Y-%m-%d %H:%M:%S\")}' and '{now.strftime(\"%Y-%m-%d %H:%M:%S\")}')\n              {f'''and (\"in_artifact\" in ('{artifact_info[0]}', '{artifact_info[1]}') or \"out_artifact\" in ('{artifact_info[0]}', '{artifact_info[1]}')) ''' if artifact_name else ''}\n        order by \"user_name\", \"updated_at\" desc\n        {f'limit {page_size} offset {page_size * (page - 1)}' if page else ''}\n    \"\"\"\n    result = (await MarsDB().a_execute(_sql)).fetchall()\n    ret = list()\n    # 隐藏map到私有artifact的记录\n    for row in result:\n        row = row._asdict()\n        for index in ['in_artifact', 'out_artifact']:\n            if row[index]:\n                in_arr = row[index].split(':')\n                if in_arr[0] not in ['', user.user_name, user.shared_group]:\n                    row[index] = '***'\n                else:\n                    row[index] = ':'.join(in_arr[1:])\n        if row['in_artifact'] in ['', '***'] and row['out_artifact'] in ['', '***']:\n            continue\n        ret.append(row)\n    return {\n        'success': 1,\n        'msg': ret\n    }\n\n\nasync def a_is_api_limited(key=None, waiting_seconds: int = 10) -> bool:\n    \"\"\"\n    api限流，本次操作后waiting_seconds秒内再次操作会返回True（被限制），否则返回False（未被限制）\n    :param key:\n    :param waiting_seconds: 单位为秒\n    :return:\n    \"\"\"\n    try:\n        key = f'{inspect.getframeinfo(inspect.currentframe().f_back)[2]}{key}'\n        exist_key = await redis.exists(key)\n        if exist_key:\n            return True\n        else:\n            await redis.set(key, waiting_seconds)\n            await redis.expire(key, waiting_seconds)\n            return False\n    except:\n        return False\n\n\nasync def update_priority(\n        priority: int = None,\n        custom_rank: float = None,\n        real_priority: int = None,\n        t: TrainingTask = Depends(get_api_task(check_user=False)),\n        api_user: User = Depends(get_api_user_with_token()),\n):\n    if t.queue_status == QUE_STATUS.FINISHED:\n        return {\n            'success': 0,\n            'msg': f'不能更新已经结束任务的优先级'\n        }\n    if t.user_name != api_user.user_name and not api_user.in_group('cluster_manager'):\n        return HTTPException(403, detail='无权操作他人任务')\n    t.re_impl(AutoTaskApiImpl)  # for t.user\n    if real_priority is not None:\n        priority = real_priority\n    if priority is None and custom_rank is None:\n        return {\n            'success': 0,\n            'msg': f'必须指定要更新的字段'\n        }\n    # 限流的时间\n    waiting_seconds = 10\n    if await a_is_api_limited(key=t.id, waiting_seconds=waiting_seconds):\n        return {\n            'success': 0,\n            'msg': f'该任务在{waiting_seconds}秒内已经更新过优先级'\n        }\n    if priority is not None:\n        if t.user.is_external:\n            priority = -1\n        try:\n            priority = int(priority)\n            if priority in TASK_PRIORITY.external_priorities():\n                raise Exception(f'{[p.name for p in TASK_PRIORITY.external_priorities()]} 优先级仅支持外部用户使用')\n            elif priority not in TASK_PRIORITY.internal_priorities(with_auto=True):\n                raise Exception('优先级设置不对, 请参考 hfai.client.EXP_PRIORITY')\n        except Exception as e:\n            return {'success': 0, 'msg': f'优先级有误: {e}'}\n    runtime_config_json = {\n        'update_priority_called': True\n    }\n    if real_priority is not None:\n        await MarsDB().a_execute(\"\"\"\n        update \"task_ng\" set \"config_json\" = jsonb_set(\"config_json\", '{ schema,priority }', %s)\n        where \"id\" = %s\n        \"\"\", (ujson.dumps(real_priority), t.id))\n        runtime_config_json['update_real_priority'] = real_priority\n    if priority is not None:\n        await t.re_impl(AioDbOperationImpl).update(('priority', ), (priority, ), remote_apply=False)\n        runtime_config_json['update_priority'] = priority\n    if custom_rank is not None:\n        runtime_config_json['custom_rank'] = custom_rank\n    await TaskRuntimeConfig(t).a_insert('runtime_priority', runtime_config_json, chain=True, update=True)\n    return {\n        'success': 1,\n        'msg': f'成功修改 [{t.user_name}][{t.nb_name}] 的{\" priority 为 \" + str(priority) if priority is not None else \"\"}'\n               f'{\" custom_rank 为 \" + str(custom_rank) if custom_rank is not None else \"\"}'\n               f'{\" real_priority 为 \" + str(real_priority) if real_priority is not None else \"\"}',\n        'timestamp': time.time()\n    }\n\n\nasync def switch_group(group: str = None, task: TrainingTask = Depends(get_api_task())):\n    \"\"\"\n    修改任务的分组， 会在返回的 data 中提供 task 的 chain_id\n    :param group:\n    :param task:\n    :return:\n    \"\"\"\n    await MarsDB().a_execute(\"\"\"\n    update \"task_ng\" set \"group\" = %s where \"chain_id\" = %s\n    \"\"\", (group, task.chain_id))\n    return {\n        'success': 1,\n        'data': {\n            'chain_id': task.chain_id\n        }\n    }\n\n\nasync def get_task_on_node_api(node: str, tick : datetime.datetime, with_log : bool = False, log_context_in_sec=60,\n                               user: User = Depends(get_api_user_with_token(allowed_groups=['ops', 'system', 'platform']))):\n    \"\"\"\n    根据时间戳查询当时在节点上运行的训练任务, 并返回指定时间前后的任务日志\n    \"\"\"\n    sql = f'''\n        select\n            \"task_ng\".\"id\", \"task_ng\".\"user_name\", \"task_ng\".\"nb_name\", \"task_ng\".\"begin_at\", \"task_ng\".\"end_at\",\n            \"task_ng\".\"assigned_nodes\", \"task_ng\".\"group\"\n        from \"pod_ng\"\n        inner join \"task_ng\" on \"task_ng\".\"id\" = \"pod_ng\".\"task_id\"\n        where\n            \"task_ng\".\"task_type\" = %s and\n            \"pod_ng\".\"node\" = %s and\n            \"pod_ng\".\"begin_at\" < %s and\n            (\"pod_ng\".\"end_at\" > %s or \"pod_ng\".\"status\" not in (%s, %s, %s))\n    '''\n    res = await MarsDB().a_execute(sql, (TASK_TYPE.TRAINING_TASK, node, tick, tick, *EXP_STATUS.FINISHED))\n    if with_log:\n        task_logs = []\n        for task in res:\n            user = await AioUserSelector.find_one(user_name=task.user_name)\n            node_rank = [rank for rank, rank_node in enumerate(task.assigned_nodes) if rank_node == node][0]\n            node_log = await get_task_node_idx_log(task.id, user, node_idx=node_rank)\n            task_log = ''\n            for line in node_log['data'].splitlines():\n                try:\n                    ts = ciso8601.parse_datetime(line[1:27])\n                except:\n                    continue\n                if abs((ts - tick).total_seconds()) <= log_context_in_sec:\n                    task_log += line + '\\n'\n            task_logs.append(task_log)\n        additional_response = {'logs': task_logs}\n    else:\n        additional_response = {}\n\n    return {\n        'success': 1,\n        # 目前只查训练任务正常情况下最多只有一个任务, 这里先以 list 形式返回, 方便之后有需求的话支持 background task / jupyter 之类的\n        'result': [{**r} for r in res],\n        **additional_response,\n    }\n\n\nasync def service_control_api(service: str, action: str, task: TrainingTask = Depends(get_api_task())):\n    task.re_impl(AutoTaskApiImpl)\n    if task.runtime_config_json.get('service_task', {}).get('version', 0) < 1:\n        return {\n            'success': 0,\n            'msg': '容器版本较老, 不支持此功能, 请重启容器后重试'\n        }\n    if action not in ['start', 'stop', 'restart']:\n        raise HTTPException(400, detail=f'不支持的操作: {action}')\n    msg = {'service': service, 'action': action}\n    await redis.lpush(f'manager_service_control:{task.id}', pickle.dumps(msg))\n    return {\n        'success': 1,\n        'msg': '已成功发送信号, 检查服务日志以查看操作结果'\n    }\n\n\nasync def set_task_restart_log_api(rule: str, reason: str, result: str, task: TrainingTask = Depends(get_api_task(chain_task=False))):\n    task.re_impl(AioDbOperationImpl)\n    return await task.set_restart_log(rule, reason, result)\n"}
{"type": "source_file", "path": "api/query/optimized/user.py", "content": "\n\nimport pandas as pd\nfrom fastapi import Depends, HTTPException\n\nfrom api.depends import get_api_user_with_token, get_internal_api_user_with_token\nfrom api.user.priority_utils import check_permission\nfrom api.user.admin import verify_quota_permission\nfrom server_model.user import User\nfrom server_model.user_data import UserWithAllGroupsTable, SchedulerUserTable, UserAllGroupsTable, UserAllQuotaTable\nfrom conf.flags import ALL_USER_ROLES\n\n\nasync def get_user_api(user: User = Depends(get_api_user_with_token())):\n    return {\n        'success': 1,\n        'result': await user.async_get_info()\n    }\n\n\nasync def get_all_user_api(user: User = Depends(get_internal_api_user_with_token())):\n    user_df = await UserWithAllGroupsTable.async_df\n    users = user_df.to_dict('records')\n    for user in users:\n        user.pop('token')\n    return {\n        'success': 1,\n        'result': users\n    }\n\n\nasync def get_user_node_quota_api(role: str , user: User = Depends(get_api_user_with_token())):\n    \"\"\" 获取配置到用户账号名下的节点 quota 和 node limit, 不包括从 group/shared_group 中继承的数据 \"\"\"\n    if role not in ALL_USER_ROLES:\n        return {'success': 0, 'msg': f'不存在的 role {role}'}\n    if not verify_quota_permission(role, user):\n        raise HTTPException(403, detail='无权操作')\n    quota_df = await SchedulerUserTable.async_df\n    quota_df = quota_df[(quota_df.user_name == quota_df.hit_group) & (quota_df.role == role)]\n    json_agg = lambda k, v: lambda df: pd.DataFrame([[df[[k, v]].set_index(k).to_dict()[v]]], columns=[v])\n    quota_df = quota_df \\\n        .groupby(['user_name', 'role', 'group', 'resource']) \\\n            .apply(json_agg(k='priority', v='quota')).reset_index(-1, drop=True).reset_index() \\\n        .groupby(['user_name', 'role', 'group']) \\\n            .apply(json_agg(k='resource', v='quota')).reset_index(-1, drop=True).reset_index() \\\n        .groupby(['user_name', 'role']) \\\n            .apply(json_agg(k='group', v='quota')).reset_index(-1, drop=True).reset_index()\n    user_group_df = await UserAllGroupsTable.async_df\n    quota_df = quota_df.merge(user_group_df, how='left', on='user_name')\n\n    return {\n        'success': 1,\n        'result': quota_df.to_dict('records')\n    }\n\n\nasync def get_all_user_node_quota_api(user: User=Depends(get_internal_api_user_with_token(allowed_groups=['internal_quota_limit_editor']))):\n    \"\"\" 获取全部用户的所有节点 quota 和 node limit \"\"\"\n    def process_df(df, prefix):\n        df = df[df.resource.str.startswith(prefix)].copy()\n        df['resource'] = df.resource.str.slice(len(prefix))\n        return df.groupby('user_name').apply(lambda x: x.set_index('resource').to_dict()['quota']).to_dict()\n\n    quota_df = await UserAllQuotaTable.async_df\n    quota_df = quota_df[['user_name', 'resource', 'quota']]\n    node_quota, node_limit = process_df(quota_df, prefix='node-'), process_df(quota_df, prefix='node_limit')\n    node_limit_extra = {}\n    for user_name in list(node_limit.keys()):\n        node_limit_extra[user_name] = {}\n        for key in list(node_limit[user_name].keys()):\n            resource, group, priority = key.split('-')\n            replaced_key = f'{group}-{priority}'\n            node_limit[user_name][replaced_key] = node_limit[user_name][key]\\\n                if replaced_key not in node_limit[user_name] else\\\n                min(node_limit[user_name][key], node_limit[user_name][replaced_key])\n            node_limit_extra[user_name][f'{replaced_key}{resource.strip(\"-\")}'] = node_limit[user_name].pop(key)\n    res = {\n        user_name: {'node': node_quota.get(user_name, {}), 'node_limit': node_limit.get(user_name, {}), 'node_limit_extra': node_limit_extra.get(user_name, {})}\n        for user_name in quota_df.user_name.tolist()\n    }\n    return {'success': 1, 'result': res}\n\n\nasync def get_quota_used_api(user: User = Depends(get_api_user_with_token())):\n    worker_user_info = {}\n    worker_user_info['user_name'] = user.user_name\n    worker_user_info['quota'] = user.quota.node_quota\n    worker_user_info['quota_limit'] = user.quota.node_quota_limit\n    worker_user_info['quota_limit_extra'] = user.quota.node_quota_limit_extra\n    worker_user_info['all_quota'] = {k.replace('node-', ''): v for k, v in user.quota.node_quota.items()}\n    worker_user_info['already_used_quota'] = await user.quota.async_get_used_quota()\n    return {\n        'success': 1,\n        'result': worker_user_info\n    }\n"}
{"type": "source_file", "path": "api/resource/cloud_storage/__init__.py", "content": "\n\nfrom .implement import *\n"}
{"type": "source_file", "path": "api/resource/image/__init__.py", "content": "\n\nfrom .implement import *\n"}
{"type": "source_file", "path": "api/query/optimized/service_task/implement.py", "content": "\nfrom .default import *\nfrom .custom import *\n\nimport asyncio\n\nimport pandas as pd\nfrom fastapi import Depends\n\nfrom conf import CONF\nfrom api.task.service_task import QUOTA_SPOT_JUPYTER, QUOTA_DEDICATED_JUPYTER, get_spot_jupyter_status, VISIBLE_TASK_TAG\nfrom api.depends import JUPYTER_ADMIN_GROUP, get_api_user_with_token\nfrom conf.flags import QUE_STATUS, TASK_TYPE, EXP_STATUS\nfrom db import MarsDB\nfrom k8s.async_v1_api import async_get_nodes_df\nfrom monitor.utils import async_redis_cached\nfrom server_model.user import User\nfrom server_model.user_impl.user_access import ACCESS_SCOPE\nfrom server_model.user_data import UserAccessTokenTable\n\n\ndef construct_sql(filters=None, fullmatch_filters=None, universal_filter_keywords=None, select_count=False):\n    filters, fullmatch_filters = filters or dict(), fullmatch_filters or dict()\n    make_filter_sql = lambda columns: 'and'.join([\n        f''' \"{column}\" ~ '{filters[column]}' '''\n        for column in columns if filters.get(column) is not None\n    ] + [\n        f''' \"{column}\" = '{fullmatch_filters[column]}' '''\n        for column in columns if fullmatch_filters.get(column) is not None\n    ])\n    inner_filter_sql = make_filter_sql(columns=['group', 'nb_name'])\n    outer_filter_sql = make_filter_sql(columns=['node', 'user_name', 'status'])\n\n    if universal_filter_keywords is not None and len(universal_filter_keywords) > 0:\n        universal_filter_sql = 'and'.join(\n            f''' \"res\".\"all_filter_fields\" ~ '{keyword}' '''\n            for keyword in universal_filter_keywords\n        )\n        if len(outer_filter_sql) == 0:\n            outer_filter_sql = universal_filter_sql\n        else:\n            outer_filter_sql += ' and ' + universal_filter_sql\n\n    return f\"\"\"\n        select {'count(*)' if select_count else '*'} from (\n            select *, concat_ws(' ', \"user_name\", \"nb_name\", \"node\", \"group\", \"status\") as \"all_filter_fields\"\n            from (\n                select\n                    \"task_ng\".*,\n                    \"assigned_nodes\"[1] as \"node\",\n                    \"t_rc\".\"runtime_config_json\",\n                    case\n                        when \"pod\".\"status\" is null then\n                            case\n                                when \"task_ng\".\"queue_status\" = 'finished' then 'stopped'\n                                else \"task_ng\".\"queue_status\"\n                            end\n                        when \"pod\".\"status\" like '%_terminating' then 'terminating'\n                        when \"pod\".\"status\" = any('{{ {\",\".join(EXP_STATUS.FINISHED)} }}'::varchar[]) then 'stopped'\n                        else \"pod\".\"status\"\n                    end as \"status\"\n                from \"task_ng\"\n                inner join \"task_tag\" \"tt\" on \"task_ng\".\"chain_id\" = \"tt\".\"chain_id\"\n                left join \"pod_ng\" \"pod\" on \"pod\".\"task_id\" = \"task_ng\".\"id\" and \"pod\".\"job_id\" = 0\n                left join (\n                    select \"id\", coalesce(jsonb_object_agg(\"source\", \"config_json\") filter ( where \"source\" is not null ), '{{}}'::jsonb) as \"runtime_config_json\"\n                    from (\n                        select \"task_ng\".\"id\", \"tr\".\"source\", \"tr\".\"config_json\"\n                        from \"task_ng\"\n                        inner join \"task_tag\" \"tt\" on \"task_ng\".\"chain_id\" = \"tt\".\"chain_id\"\n                        left join \"task_runtime_config\" \"tr\" on \"tr\".\"task_id\" = \"task_ng\".\"id\" or \"tr\".\"chain_id\" = \"task_ng\".\"chain_id\"\n                        where \"task_type\" = '{TASK_TYPE.JUPYTER_TASK}' and last_task = true and \"tt\".\"tag\" = '{VISIBLE_TASK_TAG}'\n                    ) as \"t\" group by \"id\"\n                ) \"t_rc\" on \"t_rc\".\"id\" = \"task_ng\".\"id\"\n                where \"task_type\" = 'jupyter' and \"last_task\" = true and \"tt\".\"tag\" = '{VISIBLE_TASK_TAG}' \n                    {('and ' + inner_filter_sql) if inner_filter_sql else ''}\n            ) as \"res_before_concat\"\n        ) as \"res\" {outer_filter_sql and f'where {outer_filter_sql}'}\n    \"\"\"\n\n\nasync def async_get_jupyter_task_df(limit=1000, offset=0, filters=None, fullmatch_filters=None, universal_filter_keywords=None):\n    sql = f\"\"\"\n        {construct_sql(filters, fullmatch_filters, universal_filter_keywords, select_count=False)}\n    \"\"\"\n    res = await MarsDB().a_execute(sql)\n    df = pd.DataFrame([{**r} for r in res])\n    if len(df) == 0:\n        df = pd.DataFrame([], columns=['id', 'nb_name', 'user_name', 'code_file', 'workspace', 'config_json',\n                                       'group', 'nodes', 'assigned_nodes', 'node', 'restart_count', 'whole_life_state',\n                                       'first_id', 'backend', 'task_type', 'queue_status', 'notes', 'priority',\n                                       'chain_id', 'stop_code', 'suspend_code', 'mount_code',\n                                       'suspend_updated_at', 'begin_at', 'end_at', 'created_at',\n                                       'worker_status', 'status'])\n    df = df.sort_values('id', ascending=False)\n    df = df[offset:offset+limit]\n    last_checkpoint_list = []\n    # 一次性拿所有的 image\n    sql = \"\"\"\n    select distinct on (\"user_name\", \"description\") * \n    from \"user_image\" \n    order by \"user_name\", \"description\", \"updated_at\" desc\n    \"\"\"\n    res = await MarsDB().a_execute(sql)\n    image_dict = {f'{r.user_name}-{r.description}': {**r} for r in res}\n    for _, row in df.iterrows():\n        # 拿 image 只需要用户名\n        last_checkpoint_image = image_dict.get(f'{row.user_name}-{row.nb_name}')\n        last_checkpoint_list.append(last_checkpoint_image['updated_at'].strftime(\"%Y-%m-%d %H:%M:%S\") if last_checkpoint_image else None)\n    df['last_checkpoint'] = last_checkpoint_list\n    return df\n\n\ndef get_jupyter_tasks_info(jupyter_task_df: pd.DataFrame, node_ip):\n    jupyter_tasks = dict()\n    for _, row in jupyter_task_df.iterrows():\n        task_name = f'{row.user_name}/{row.nb_name}'\n        task_info = dict(row.to_dict())\n        # Task 的全字段\n        jupyter_tasks[task_name] = task_info\n        # 添加额外字段\n        task_node_ip =  node_ip.get(row.node)\n        services = task_info['runtime_config_json'].get('service_task', {}).get('services', {}) or task_info['config_json'].get('services', {})   # deprecating\n        jupyter_tasks[task_name].update({\n            # ServiceTask 特有的字段\n            'node_ip': task_node_ip if row.queue_status == QUE_STATUS.SCHEDULED else None,\n            # Deprecating (为了兼容旧接口的额外字段)\n            'builtin_services':\n                [{'name': svc_name, **svc} for svc_name, svc in services.items() if svc_name in CONF.jupyter.builtin_services],\n            'custom_services':\n                [{'name': svc_name, **svc} for svc_name, svc in services.items() if svc_name not in CONF.jupyter.builtin_services],\n        })\n    return jupyter_tasks\n\n\nasync def async_get_num_jupyter_tasks(filters=None, fullmatch_filters=None, universal_filter_keywords=None):\n    sql = construct_sql(filters, fullmatch_filters, universal_filter_keywords, select_count=True)\n    res = await MarsDB().a_execute(sql)\n    return next(res).count\n\n\n@async_redis_cached(ttl_in_sec=5)\nasync def async_get_cluster_status():\n    # 计算各个 group 的剩余资源\n    all_jupyter_task_df = await async_get_jupyter_task_df()\n    cluster_df = await async_get_nodes_df()\n    cluster_df = cluster_df[(cluster_df.working.apply(lambda w: w is None).astype(bool) | (cluster_df.working == TASK_TYPE.JUPYTER_TASK)) & (cluster_df.status == 'Ready')]\n    for _, row in all_jupyter_task_df[all_jupyter_task_df.queue_status == QUE_STATUS.SCHEDULED].iterrows():\n        cluster_df.loc[cluster_df.name == row.node, 'memory'] -= row.config_json['memory'] << 30\n    not_working_dict = cluster_df[cluster_df.working.apply(lambda w: w is None).astype(bool)].groupby('group').name.count().to_dict()\n    allocatable_memory_dict = cluster_df.groupby('group').memory.max().to_dict()\n    cpu_dict = cluster_df.groupby('group').cpu.max().to_dict()\n    resource = {}\n    for group in cpu_dict:\n        resource[group] = {\n            'not_working': not_working_dict.get(group, 0),\n            'allocatable': allocatable_memory_dict.get(group, 0) >> 30,\n            'max_cpu_core': cpu_dict.get(group, 0),\n        }\n    node_ip = cluster_df[['name', 'internal_ip']].set_index('name').to_dict().get('internal_ip', {})\n    return {'resource': resource, 'node_ip': node_ip}\n\n\nasync def data_api(user: User = Depends(get_api_user_with_token())):\n    # 处理 Jupyter quota\n    await user.quota.create_quota_df()\n    jupyter_quota = user.quota.jupyter_quota\n    tmp = {}\n    for g in sorted(jupyter_quota.keys(), key=lambda k: ~k.startswith(CONF.jupyter.shared_node_group_prefix)):\n        jupyter_quota[g]['allocatable'] = 0\n        jupyter_quota[g]['running'] = 0\n        jupyter_quota[g]['not_working'] = 0\n        tmp[g] = jupyter_quota[g]\n    jupyter_quota = tmp\n\n    user_jupyter_task_df = await async_get_jupyter_task_df(fullmatch_filters={'user_name': user.user_name})\n    for _, row in user_jupyter_task_df.iterrows():\n        if not jupyter_quota.get(row.group):\n            jupyter_quota[row.group] = {\n                'cpu': 0,\n                'memory': 0,\n                'quota': 0,\n                'allocatable': 0,\n                'running': 0,\n                'not_working': 0\n            }\n        if row.status != 'stopped':\n            jupyter_quota[row.group]['running'] += 1\n\n    cluster_status = await async_get_cluster_status()\n    for g in jupyter_quota:\n        if g in cluster_status['resource']:\n            jupyter_quota[g].update(cluster_status['resource'][g])\n    jupyter_tasks = get_jupyter_tasks_info(user_jupyter_task_df, cluster_status['node_ip'])\n\n    # 处理 node port 信息\n    nodeports = await user.nodeport.async_get()\n    for nb_name, node_port_list in nodeports.items():\n        node_ip = jupyter_tasks.get(f'{user.user_name}/{nb_name}', {}).get('node_ip', None)\n        for port in node_port_list:\n            port['ip'] = node_ip\n\n    result = {\n        'tasks': list(jupyter_tasks.values()),\n        'quota': jupyter_quota,\n        'nodeports': nodeports,\n        'nodeport_quota': user.nodeport.quota_info(),\n        'environments': user.quota.train_environments,\n        'admin': user.in_group(JUPYTER_ADMIN_GROUP),\n        'can_suspend': user.in_group('can_suspend'),\n    }\n    result.update(await extra_data(user))\n    if user.is_external:\n        result.update({\n            'spot_jupyter_quota': int(user.quota.quota(QUOTA_SPOT_JUPYTER)),\n            'spot_jupyter_status': await get_spot_jupyter_status(),\n            'dedicated_jupyter_quota': int(user.quota.quota(QUOTA_DEDICATED_JUPYTER)),\n        })\n\n    return {\n        'success': 1,\n        'msg': 'success',\n        'result': result\n    }\n\n\nasync def all_tasks_api(user: User = Depends(get_api_user_with_token(allowed_groups=[JUPYTER_ADMIN_GROUP])),\n                        page: int = 1, page_size: int = 1000,\n                        user_name: str = None, nb_name: str = None, node: str = None,\n                        group: str = None, status: str = None, universal_filter_keywords: str = None):\n    cluster_df_task = asyncio.create_task(async_get_nodes_df())\n    filters = {'user_name':user_name, 'nb_name':nb_name, 'node':node, 'group':group, 'status':status}\n    uni_keywords = universal_filter_keywords.split() if universal_filter_keywords is not None else []\n    jupyter_df_task = asyncio.create_task(async_get_jupyter_task_df(\n        limit=page_size, offset=(page-1)*page_size, filters=filters,\n        universal_filter_keywords=uni_keywords\n    ))\n    jupyter_count_task = asyncio.create_task(\n        async_get_num_jupyter_tasks(filters=filters, universal_filter_keywords=uni_keywords)\n    )\n    # 为 task 增加 token 信息方便直接访问 Jupyter\n    user_token_df = await UserAccessTokenTable.async_df\n    user_token_df = user_token_df[\n        (user_token_df.access_user_name == user_token_df.from_user_name) &\n        (user_token_df.access_scope == ACCESS_SCOPE.ALL) &\n        user_token_df.active\n    ]\n    user_token = {user_name: token for user_name, token in zip(user_token_df['access_user_name'], user_token_df['access_token'])}\n    jupyter_task_df = await jupyter_df_task\n    jupyter_task_df['token'] = jupyter_task_df.user_name.apply(lambda u: user_token.get(u, None))\n    # admin 接口不在前端轮询查看是否可以访问\n    jupyter_task_df.status = jupyter_task_df.status.apply(lambda s: s if s != 'standby' else 'ready')\n    return {\n        'success': 1,\n        'msg': 'success',\n        'result': {\n            'tasks': list(get_jupyter_tasks_info(jupyter_task_df, await cluster_df_task).values()),\n            'total_count': await jupyter_count_task,\n        }\n    }\n\n"}
{"type": "source_file", "path": "api/query/optimized/task/implement.py", "content": "\n\nfrom .default import *\nfrom .custom import *\n\nimport datetime\nfrom typing import List\nfrom fastapi import Depends, Query\n\nfrom api.depends import get_api_user_with_token, get_internal_api_user_with_token\nfrom base_model.training_task import TrainingTask\nfrom conf import CONF\nfrom conf.flags import QUE_STATUS, CHAIN_STATUS, TASK_TYPE\nfrom db import MarsDB\nfrom server_model.auto_task_impl import AutoTaskApiImpl\nfrom server_model.user import User\nfrom utils import convert_to_external_task\ntry:\n    from monitor import async_get_container_monitor_stats\nexcept:\n    pass\n\n\nasync def get_chain_tasks_in_query_db(\n        sql_where_and,\n        sql_where_and_args,\n        page: int = None,\n        page_size: int = None,\n        user: User = None,\n        select_pods=False,\n        order_by='id',\n        count=True\n):\n    sql = f\"\"\"\n    select \n        \"t\".*, \"task_ng\".*, coalesce(\"tt\".\"tags\", '{{}}')::varchar[] as \"tags\",\n        case \n            when \"task_ng\".\"queue_status\" = '{QUE_STATUS.FINISHED}' then '{CHAIN_STATUS.FINISHED}'\n            when \"task_ng\".\"queue_status\" = '{QUE_STATUS.QUEUED}' and \"task_ng\".\"id\" != \"task_ng\".\"first_id\" then '{CHAIN_STATUS.SUSPENDED}'\n            when \"task_ng\".\"queue_status\" = '{QUE_STATUS.QUEUED}' and \"task_ng\".\"id\" = \"task_ng\".\"first_id\" then '{CHAIN_STATUS.WAITING_INIT}'\n            when \"task_ng\".\"queue_status\" = '{QUE_STATUS.SCHEDULED}' then '{CHAIN_STATUS.RUNNING}'\n        end as \"chain_status\",\n        \"task_ng\".\"config_json\" as \"config_json\",\n        coalesce(jsonb_object_agg(\"tr\".\"source\", \"tr\".\"config_json\") filter ( where \"tr\".\"source\" is not null ), '{{}}'::jsonb) as \"runtime_config_json\"\n    from \"task_ng\"\n    inner join (\n        select\n            max(\"id\") as \"max_id\",\n            array_agg(\"id\" order by \"id\") as \"id_list\",\n            array_agg(\"queue_status\" order by \"id\") as \"queue_status_list\",\n            array_agg(\"begin_at\" order by \"id\") as \"begin_at_list\",\n            array_agg(\"end_at\" order by \"id\") as \"end_at_list\",\n            array_agg(\"created_at\" order by \"id\") as \"created_at_list\",\n            array_agg(\"stop_code\" order by \"id\") as \"stop_code_list\",\n            array_agg(\"suspend_code\" order by \"id\") as \"suspend_code_list\",\n            array_agg(\"whole_life_state\" order by \"id\") as \"whole_life_state_list\",\n            array_agg(\"worker_status\" order by \"id\") as \"worker_status_list\"\n        from \"task_ng\"\n        where \"task_ng\".\"chain_id\" in (\n            select\n                \"chain_id\"\n            from \"task_ng\"\n            where \"last_task\" {sql_where_and}\n            order by \"{order_by}\" desc\n            {'' if page is None else f'limit {page_size} offset {page_size * (page - 1)}'}\n        )\n        group by \"task_ng\".\"chain_id\"\n    ) as \"t\" on \"t\".\"max_id\" = \"task_ng\".\"id\"\n    left join (\n        select array_agg(\"task_tag\".\"tag\") as \"tags\", \"chain_id\"\n        from \"task_tag\"\n        group by \"chain_id\"\n    ) \"tt\" on \"tt\".\"chain_id\" = \"task_ng\".\"chain_id\"\n    left join \"task_runtime_config\" \"tr\" on \"tr\".\"task_id\" = \"task_ng\".\"id\" or \"tr\".\"chain_id\" = \"task_ng\".\"chain_id\"\n    group by\n        \"task_ng\".\"id\", \"t\".\"max_id\", \"t\".\"id_list\", \"t\".\"queue_status_list\", \"t\".\"begin_at_list\", \"t\".\"end_at_list\",\n        \"t\".\"created_at_list\", \"t\".\"stop_code_list\", \"t\".\"suspend_code_list\", \"t\".\"whole_life_state_list\",\n        \"t\".\"worker_status_list\", \"tt\".\"tags\"\n    order by \"{order_by}\" desc\n    \"\"\"\n    results = (await MarsDB().a_execute(sql, sql_where_and_args)).fetchall()\n    if count:\n        count_sql = f\"\"\"\n            select\n                count(*)\n            from \"task_ng\"\n            where \"last_task\" {sql_where_and}\n            \"\"\"\n        total_count = (await MarsDB().a_execute(count_sql, sql_where_and_args)).fetchall()[0]['count']\n    else:\n        total_count = len(results)\n    res = []\n    for r in results:\n        task = TrainingTask(AutoTaskApiImpl, **{**r})\n        if select_pods:\n            await task.aio_select_pods()\n        if user is not None and user.is_external:\n            task = convert_to_external_task(task)\n        res.append(task.trait_dict())\n    return res, total_count\n\n\nasync def get_tasks_api(\n        page: int,\n        page_size: int,\n        task_type: List[str] = Query(default=None),\n        nb_name_pattern: str = None,\n        worker_status: List[str] = Query(default=None),\n        queue_status: List[str] = Query(default=None),\n        tag: List[str] = Query(default=None),\n        excluded_tag: List[str] = Query(default=None),\n        group: List[str] = Query(default=None),\n        only_star: bool = False,\n        shared_task: str = 'exclude',\n        select_pods: bool = True,\n        created_start_time: str = None,\n        created_end_time: str = None,\n        order_by: str = 'id',\n        user: User = Depends(get_api_user_with_token())\n):\n    if order_by not in {'id', 'first_id'}:\n        return {\n            'success': 0,\n            'msg': f'order_by 只能是 id / first_id'\n        }\n    if page <= 0:\n        return {\n            'success': 0,\n            'msg': 'page 必须大于等于 1'\n        }\n    if page_size <= 0 or page_size > 100:\n        return {\n            'success': 0,\n            'msg': 'page_size 需要为 1 ~ 100'\n        }\n    if any((created_start_time, created_end_time)) and not all((created_start_time, created_end_time)):\n        return {\n            'success': 0,\n            'msg': 'created_start_time 和 created_end_time 必须同时为空 / 不为空'\n        }\n    if created_start_time is not None and created_end_time is not None:\n        try:\n            created_start_time = datetime.datetime.fromisoformat(created_start_time)\n            created_end_time = datetime.datetime.fromisoformat(created_end_time)\n        except Exception:\n            return {\n                'success': 0,\n                'msg': 'created_start_time / created_end_time 格式不正确，需要为 isoformat'\n            }\n    if shared_task not in {'exclude', 'only'}:\n        return {\n            'success': 0,\n            'msg': '共享任务筛选仅支持 exclude/only 选项'\n        }\n    if shared_task == 'only':\n        sql_where_and = ' and \"chain_id\" in (select \"chain_id\" from \"task_tag\" where \"tag\" = %s) '\n        sql_where_and_args = (user.shared_task_tag, )\n    else:\n        sql_where_and = ' and \"user_name\" = %s '\n        sql_where_and_args = (user.user_name, )\n    if task_type is not None:\n        sql_where_and += f''' and \"task_type\" in ({\",\".join(\"%s\" for _ in task_type)}) '''\n        sql_where_and_args += tuple(task_type)\n    if group is not None:\n        client_groups = [g for g in group if '#' in g]\n        groups = [g.split('#')[0] if '#' in g else g for g in group]\n        sql_where_and += f''' and \"group\" in ({\",\".join(\"%s\" for _ in groups)}) '''\n        sql_where_and_args += tuple(groups)\n        # 有 # 的 groups\n        if len(client_groups) > 0:\n            sql_where_and += f''' and \"config_json\"->>'client_group' in ({\",\".join(\"%s\" for _ in client_groups)}) '''\n            sql_where_and_args += tuple(client_groups)\n    if nb_name_pattern is not None:\n        sql_where_and += ' and (\"nb_name\" like %s or \"id\"::varchar like %s or \"chain_id\" like %s) '\n        sql_where_and_args += (f'%{nb_name_pattern}%', f'{nb_name_pattern}%', f'%{nb_name_pattern}%')\n    if worker_status is not None:\n        sql_where_and += f''' and \"worker_status\" in ({\",\".join(\"%s\" for _ in worker_status)}) '''\n        sql_where_and_args += tuple(worker_status)\n    if queue_status is not None:\n        sql_where_and += f''' and \"queue_status\" in ({\",\".join(\"%s\" for _ in queue_status)}) '''\n        sql_where_and_args += tuple(queue_status)\n    if only_star and tag is None:\n        tag = ['star']\n    if tag is not None:\n        sql_where_and += f' and \"chain_id\" in (select \"chain_id\" from \"task_tag\" where \"user_name\" = %s and \"tag\" in ({\",\".join(\"%s\" for _ in tag)})) '\n        sql_where_and_args += (user.user_name, ) + tuple(tag)\n    if excluded_tag is not None:\n        sql_where_and += f' and \"chain_id\" not in (select \"chain_id\" from \"task_tag\" where \"user_name\" = %s and \"tag\" in ({\",\".join(\"%s\" for _ in excluded_tag)})) '\n        sql_where_and_args += (user.user_name, ) + tuple(excluded_tag)\n    if created_start_time is not None and created_end_time is not None:\n        sql_where_and += f\"\"\"\n        and \"chain_id\" in (select \"chain_id\" from \"task_ng\" where \"user_name\" = %s and \"created_at\" > %s and \"created_at\" < %s and \"restart_count\" = 0)\n        \"\"\"\n        sql_where_and_args += (user.user_name, created_start_time, created_end_time)\n    results, total_count = await get_chain_tasks_in_query_db(\n        sql_where_and=sql_where_and,\n        sql_where_and_args=sql_where_and_args,\n        page=page,\n        page_size=page_size,\n        user=user,\n        select_pods=select_pods,\n        order_by=order_by\n    )\n    return {\n        'success': 1,\n        'result': {\n            'tasks': results,\n            'total': total_count\n        }\n    }\n\n\nasync def get_task_api(\n        id: int = None,\n        chain_id: str = None,\n        nb_name: str = None,\n        user: User = Depends(get_api_user_with_token())\n):\n    sql_where_and = ' and (\"user_name\" = %s or \"chain_id\" in (select \"chain_id\" from \"task_tag\" where \"tag\" = %s)) '\n    sql_where_and_args = (user.user_name, user.shared_task_tag)\n    if id is not None:\n        sql_where_and += ' and \"chain_id\" in (select \"chain_id\" from \"task_ng\" where \"id\" = %s) '\n        sql_where_and_args += (id, )\n    elif chain_id is not None:\n        sql_where_and += ' and \"chain_id\" = %s '\n        sql_where_and_args += (chain_id, )\n    elif nb_name is not None:\n        sql_where_and += ' and (\"nb_name\" = %s and \"user_name\" = %s)'\n        sql_where_and_args += (nb_name, user.user_name, )\n    else:\n        return {\n            'success': 0,\n            'msg': '必须指定 id, chain_id 或者 nb_name'\n        }\n    results, _ = await get_chain_tasks_in_query_db(sql_where_and=sql_where_and, sql_where_and_args=sql_where_and_args, page=1, page_size=1, user=user, select_pods=True, count=False)\n    if len(results) == 0:\n        return {\n            'success': 0,\n            'msg': '没有符合条件的任务'\n        }\n    return {\n            'success': 1,\n            'result': {\n                'task': results[0]\n            }\n        }\n\n\nasync def get_time_range_schedule_info_api(start_time: str, end_time: str, user: User = Depends(get_api_user_with_token())):\n    extra_join = '' if not user.is_external else \"\"\"\n    inner join \"user\" on \"user\".\"user_name\" = \"task_ng\".\"user_name\" and \"user\".\"role\" = 'external'\n    \"\"\"\n    sql = f\"\"\"\n    select\n       count(*) as \"count\", {'\"group\"' if not user.is_external else ''' 'training' as \"group\"'''}, 'created' as \"tag\"\n    from \"task_ng\"\n    {extra_join}\n    where\n        \"created_at\" >= '{start_time}' and \"created_at\" < '{end_time}' and\n        \"id\" = \"first_id\" and \"task_type\" = '{TASK_TYPE.TRAINING_TASK}' {'' if not user.is_external else f''' and \"group\" = '{CONF.scheduler.default_group}' '''}\n    group by \"group\"\n    union all\n    select\n           count(*) as \"count\", {'\"group\"' if not user.is_external else ''' 'training' as \"group\"'''}, 'finished' as \"tag\"\n    from \"task_ng\"\n    {extra_join}\n    where\n        \"created_at\" >= '{start_time}' and \"created_at\" < '{end_time}' and\n        \"queue_status\" = '{QUE_STATUS.FINISHED}' and task_type = '{TASK_TYPE.TRAINING_TASK}' and\n        \"chain_id\" not in (select \"chain_id\" from \"task_ng\" where \"queue_status\" = '{QUE_STATUS.QUEUED}' or \"queue_status\" = '{QUE_STATUS.SCHEDULED}') {'' if not user.is_external else f''' and \"group\" = '{CONF.scheduler.default_group}' '''}\n    group by \"group\"\n    \"\"\"\n    results = await MarsDB().a_execute(sql)\n    res = {\n        'created': 0,\n        'finished': 0,\n        'detail': {\n            'created': [],\n            'finished': []\n        }\n    }\n    for r in results.fetchall():\n        count, group, tag = r['count'], r['group'], r['tag']\n        if tag == 'created':\n            res['created'] += count\n            res['detail']['created'].append({\n                'count': count,\n                'group': group\n            })\n        elif tag == 'finished':\n            res['finished'] += count\n            res['detail']['finished'].append({\n                'count': count,\n                'group': group\n            })\n    return {\n        'success': 1,\n        'result': res\n    }\n\n\nasync def get_running_tasks_api(\n        task_type: List[str] = Query(default=None),\n        user: User = Depends(get_internal_api_user_with_token())\n):\n    if task_type is None:\n        task_type = TASK_TYPE.all_task_types()\n    results = await MarsDB().a_execute(f\"\"\"\n    select \n        \"user\".\"role\" as \"user_role\", \"unfinished_task_ng\".\"id\", \"unfinished_task_ng\".\"nb_name\", \"unfinished_task_ng\".\"queue_status\",\n        \"unfinished_task_ng\".\"user_name\", \"unfinished_task_ng\".\"priority\", \"unfinished_task_ng\".\"assigned_nodes\",\n        \"unfinished_task_ng\".\"nodes\", \"unfinished_task_ng\".\"backend\", \"unfinished_task_ng\".\"begin_at\", \"unfinished_task_ng\".\"created_at\",\n        \"unfinished_task_ng\".\"group\", \"unfinished_task_ng\".\"chain_id\", \"unfinished_task_ng\".\"task_type\", \"unfinished_task_ng\".\"first_id\",\n        \"chain_task_ng\".\"begin_at_list\", \"chain_task_ng\".\"end_at_list\",\n        case \n            when \"unfinished_task_ng\".\"queue_status\" = '{QUE_STATUS.FINISHED}' then '{CHAIN_STATUS.FINISHED}'\n            when \"unfinished_task_ng\".\"queue_status\" = '{QUE_STATUS.QUEUED}' and \"unfinished_task_ng\".\"id\" != \"unfinished_task_ng\".\"first_id\" then '{CHAIN_STATUS.SUSPENDED}'\n            when \"unfinished_task_ng\".\"queue_status\" = '{QUE_STATUS.QUEUED}' and \"unfinished_task_ng\".\"id\" = \"unfinished_task_ng\".\"first_id\" then '{CHAIN_STATUS.WAITING_INIT}'\n            when \"unfinished_task_ng\".\"queue_status\" = '{QUE_STATUS.SCHEDULED}' then '{CHAIN_STATUS.RUNNING}'\n        end as \"chain_status\",\n        \"unfinished_task_ng\".\"config_json\" as \"config_json\",\n        coalesce(jsonb_object_agg(\"tr\".\"source\", \"tr\".\"config_json\") filter ( where \"tr\".\"source\" is not null ), '{{}}'::jsonb) as \"runtime_config_json\"\n    from \"unfinished_task_ng\"\n    inner join (\n        select\n            max(\"id\") as \"max_id\"\n        from \"unfinished_task_ng\"\n        where \"chain_id\" in (\n            select distinct on (\"chain_id\", \"first_id\") \"chain_id\"\n            from \"unfinished_task_ng\"\n            where \"task_type\" in ({\",\".join(\"%s\" for _ in task_type)}) and (\"queue_status\" = '{QUE_STATUS.QUEUED}' or \"queue_status\" = '{QUE_STATUS.SCHEDULED}')\n        )\n        group by \"chain_id\"\n    ) as \"t\" on \"t\".\"max_id\" = \"unfinished_task_ng\".\"id\"\n    inner join \"user\" on \"user\".\"user_name\" = \"unfinished_task_ng\".\"user_name\"\n    left join \"task_runtime_config\" \"tr\" on \"tr\".\"task_id\" = \"unfinished_task_ng\".\"id\" or \"tr\".\"chain_id\" = \"unfinished_task_ng\".\"chain_id\"\n    left join (\n        select\n            max(\"id\") as \"max_id\",\n            array_agg(\"begin_at\" order by \"id\") as \"begin_at_list\",\n            array_agg(\"end_at\" order by \"id\") as \"end_at_list\"\n        from \"task_ng\"\n        where \"task_ng\".\"chain_id\" in (\n            select\n                \"chain_id\"\n            from \"unfinished_task_ng\"\n        )\n        group by \"task_ng\".\"chain_id\"\n    ) as \"chain_task_ng\" on \"chain_task_ng\".\"max_id\" = \"unfinished_task_ng\".\"id\"\n    group by \"unfinished_task_ng\".\"id\",\"unfinished_task_ng\".\"first_id\", \"user\".\"role\",\n        \"chain_task_ng\".\"max_id\", \"chain_task_ng\".\"begin_at_list\", \"chain_task_ng\".\"end_at_list\"\n    order by \"first_id\" desc\n    \"\"\", tuple(task_type))\n    return {\n        'success': 1,\n        'result': results.fetchall()\n    }\n\n\nasync def get_task_container_monitor_stats_api(user: User = Depends(get_api_user_with_token())):\n    res = await async_get_container_monitor_stats()\n    return {\n        'success': 1,\n        'result': res\n    }\n\n\nasync def get_tasks_overview(user: User = Depends(get_api_user_with_token())):\n    result = await MarsDB().a_execute(\"\"\"\n    select\n        coalesce((config_json->'running_priority'->-1->'priority')::integer, \"priority\") as \"priority\", \"queue_status\", count(*)\n    from \"unfinished_task_ng\"\n    group by coalesce((config_json->'running_priority'->-1->'priority')::integer, \"priority\"), \"queue_status\"\n    order by coalesce((config_json->'running_priority'->-1->'priority')::integer, \"priority\") desc, \"queue_status\" \n    \"\"\")\n    return {\n        'success': 1,\n        'result': [{**r} for r in result]\n    }\n"}
{"type": "source_file", "path": "api/register/default.py", "content": "\n\nimport os\nimport functools\nimport logging\n\nfrom api.app import app\n\n\nlogger = logging.getLogger(\"uvicorn\")  # stdout req and res\napp.post = functools.partial(app.post)\nREG_SERVERS = {s for s in os.environ['SERVER'].split(',')}\n"}
{"type": "source_file", "path": "api/resource/storage/__init__.py", "content": "\n\nfrom .implement import *\n"}
