{"repo_info": {"repo_name": "PyBench", "repo_owner": "Mercury7353", "repo_url": "https://github.com/Mercury7353/PyBench"}}
{"type": "source_file", "path": "inference.py", "content": "import json\nimport os\nimport traceback\n\nimport fire\nimport matplotlib\n#from langchain_experimental.tools.python.tool import PythonAstREPLTool\nfrom loguru import logger\nfrom yaml import safe_load\n\nfrom llms import build_llm\nfrom llms.utils import message2dict\nfrom utils.assistant import GPT\nfrom utils.output_parser import parse_code_action\nfrom utils.save_notebook import generate_notebook, save_as_ipynb\n\nmatplotlib.use(\"Agg\")\nimport nbformat\nfrom nbclient import NotebookClient\nimport traceback\nimport time\nimport ruamel.yaml\n\ndef execute_code(code_str: str, Kernel, nb):\n    nb.cells.append(nbformat.v4.new_code_cell(code_str))\n    total_cells = len(nb.cells)\n    cell = nb.cells[-1]\n    client = NotebookClient(nb, allow_errors=True)\n    client.kc = Kernel\n    print(cell)\n\n    try:\n        client.reset_execution_trackers()\n        client.execute_cell(cell=cell, cell_index=-1)\n    except Exception as e:\n        traceback.print_exc()\n        error_message = str(e)\n        return error_message\n\n    outputs = nb.cells[-1]['outputs']\n\n    result = \"\"\n    for output in outputs:\n        if output.output_type == \"stream\":  # 如果输出是标准输出或标准错误\n            result += output.text\n        elif output.output_type == \"execute_result\":  # 如果输出是执行结果\n            result += str(output['data']['text/plain'])\n        elif output.output_type == \"error\":  # 如果输出是错误\n            result += \"There are some errors in the code. All variables in this cell should be redefined. Please Debug:\\n\"\n            result += \"Error: \" + str(output.ename) + \"\\n\"\n            result += str(output.evalue) + \"\\n\"\n\n    return result\n\n\nimport os\nimport json\nimport threading\nimport traceback\nfrom concurrent.futures import ThreadPoolExecutor\nimport fire\nfrom ruamel.yaml import safe_load\nimport nbformat\nfrom nbconvert.preprocessors import ExecutePreprocessor\nfrom nbclient import NotebookClient\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# 全局线程锁\nlock = threading.Lock()\n\ndef process_task(task, config, llm, system_prompt_template, max_turns, output_path, processed_ids):\n    index = task[\"index\"]\n    cell_path=\"Cells\"\n    if index in processed_ids:\n        return\n\n    file_path = \",\".join(task[\"file_paths\"])\n    user_query = task[\"user\"]\n\n    nb = nbformat.v4.new_notebook()\n    client = NotebookClient(nb, allow_errors=True)\n    client.km = client.create_kernel_manager()\n    client.start_new_kernel()\n    client.start_new_kernel_client()\n    Kernel = client.kc\n\n    logger.info(\"==\" * 80)\n    logger.info(f\"Task index: {index}\")\n    logger.info(f\"Task:\\n{user_query}\")\n    logger.info(f\"File path: {file_path}\")\n\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt_template},\n        {\"role\": \"user\", \"content\": f\"[INFO]The data is uploaded to {file_path}\"},\n        {\"role\": \"user\", \"content\": user_query},\n    ]\n    cells = [\n        {\"role\": \"system\", \"text\": system_prompt_template},\n        {\"role\": \"user\", \"text\": f\"[INFO]The data is uploaded to {file_path}\"},\n        {\"role\": \"user\", \"text\": user_query},\n    ]\n\n    try:\n        for count in range(max_turns):  # max 5 turn interaction\n            logger.info(\"--\" * 10 + f\"Round: {count}\" + \"--\" * 10)\n            #logger.info(f\"input messages: {messages}\")\n            try:\n                out_msg, debug_info = llm.generate(messages)\n            except:\n                break\n            #logger.info(f\"output msg: {message2dict(out_msg)}\")\n\n            reasoning, code_script = parse_code_action(\n                out_msg.content,\n                mode=config[\"mode\"],\n                code_start_token=config[\"code_start_token\"],\n                code_end_token=config[\"code_end_token\"],\n                tool_call_token=config[\"tool_call_token\"],\n            )\n            #logger.info(f\"Reasoning: {reasoning}\")\n            #logger.info(f\"Code script: {code_script}\")\n            if code_script is None or code_script.strip() == \"\":\n                messages.append({\"role\": \"assistant\", \"content\": reasoning})\n                cells.append({\"role\": \"assistant\", \"text\": reasoning})\n            else:\n                messages.append(\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": out_msg.content,\n                    }\n                )\n                cells.append({\"role\": \"assistant\", \"text\": reasoning})\n\n            if code_script is None or code_script.strip() == \"\":\n                break\n\n            code_response = execute_code(code_script, Kernel, nb)\n            #logger.info(f\"Code response:\\n{code_response}\")\n            messages.append({\"role\": \"user\", \"content\": \"[INFO]This is a Code Interpreter Message:\\n\" + code_response})\n            cells.append(\n                {\n                    \"role\": \"assistant\",\n                    \"code\": code_script,\n                    \"result\": code_response,\n                }\n            )\n\n        save_as_ipynb(generate_notebook(cells), f\"{cell_path}/{index}.ipynb\")\n        item = {\"messages\": messages}\n        item.update(task)\n    except Exception:\n        logger.error(traceback.format_exc())\n        save_as_ipynb(generate_notebook(cells), f\"{cell_path}/{index}.ipynb\")\n        item = {\"messages\": messages}\n        item.update(task)\n    finally:\n        client._cleanup_kernel()\n\n    \n    with open(output_path, 'a') as f:\n        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n        print(\"======Write Json=======\")\n\ndef main(config_path: str, task_path: str, output_path: str):\n    os.makedirs(\"Check\", exist_ok=True)\n    os.makedirs(\"output\", exist_ok=True)\n    logger.info(\"started\")\n    yaml = ruamel.yaml.YAML()\n    config=yaml.load(open(config_path, \"r\"))\n    logger.info(\"config loaded\")\n    llm = build_llm(config[\"llm\"][\"type\"], config[\"llm\"][\"args\"])\n    logger.info(\"llm built\")\n    system_prompt_template = config[\"system_prompt_template\"]\n    max_turns = config[\"max_turns\"]\n    test_data = json.load(open(task_path, \"r\"))\n    logger.info(f\"total tasks: {len(test_data)}\")\n    if os.path.exists(output_path):\n        processed_ids = set(\n            [json.loads(line)[\"index\"] for line in open(output_path, \"r\")]\n        )\n    else:\n        processed_ids = set()\n\n    with ThreadPoolExecutor(max_workers=16) as executor:\n        futures = [\n            executor.submit(process_task, task, config, llm, system_prompt_template, max_turns, output_path, processed_ids)\n            for task in test_data\n        ]\n        for future in futures:\n            future.result()  # 等待所有任务完成\n\n    logger.info(\"finished\")\n\nif __name__ == \"__main__\":\n    fire.Fire(main)\n"}
{"type": "source_file", "path": "utils/calculate_avg_step.py", "content": "# 如果failed，则步数算10？\n'''\n计算NLPython Instruct 的 统计信息\n条数\n平均轮数分布\n总token数\n'''\nimport json\n\nfrom collections import Counter\n    \n#model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\ndef read_lines(lines):\n    data = []\n    m = {}\n    for line in lines:\n        try:\n            x = json.loads(line)\n            m[x[\"index\"]] = line\n            data.append(x)\n        except:\n            pass\n    data = sorted(data, key=lambda x: x[\"index\"])\n    lines = [json.dumps(item, ensure_ascii=False, indent=4) for item in data]\n    return lines, m\ndef count_tokens(text):\n    # 初始化tokenizer\n    \n    # 对文本进行编码，然后计算token数量\n    tokens = tokenizer.encode(text, add_special_tokens=True)\n    return len(tokens)\n\n\n\ndef read_jsonl(filename):\n    file1 = open(filename)\n    lines1 = file1.readlines()\n    lines,_=read_lines(lines1)\n    return lines\n\n\ndef save_jsonl(data, filename):\n    with open(filename, \"w\") as f:\n        for x in data:\n            print(json.dumps(x, ensure_ascii=False), file=f)\n\ndef count_elements_in_intervals(elements):\n    # 定义区间\n    intervals = {'<=2': 0, '[4,6]': 0, '[7,9]': 0, '>=10': 0}\n\n    for element in elements:\n        if element <= 3:\n            intervals['<=2'] += 1\n        elif 4 <= element <= 6:\n            intervals['[4,6]'] += 1\n        elif 7 <= element <= 9:\n            intervals['[7,9]'] += 1\n        elif element >= 10:\n            intervals['>=10'] += 1\n\n    return intervals\n\nimport json\n\ndef read_jsonl(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        return file.readlines()\n\ndef statistic_info(compare_path, solution_path):\n    compare_data = read_jsonl(compare_path)\n    solution_data = read_jsonl(solution_path)\n    \n    # 将solution_data转换为index到内容的映射，便于后续查找\n    solution_dict = {}\n    for line in solution_data:\n        try:\n            solution = json.loads(line)\n            solution_dict[solution[\"index\"]] = solution\n        except:\n            print(\"Error load\")\n            continue\n    \n    print(\"Length of Compare Data:\", len(compare_data))\n    \n    turns_dis = []\n    total_token_length = 0\n    \n    for line in compare_data:\n        try:\n            compare = json.loads(line)\n            index = compare[\"index\"]\n        except:\n            continue\n        \n        # 根据index找到对应的solution\n        if index in solution_dict:\n            solution = solution_dict[index]\n            try:\n                decision = compare[\"Decision\"][\"Pass\"][\"Agent2\"]# 1 for 3.5 , 2 for anothers\n                # 如果是failed，则步数直接置为10\n                if decision == \"Failed\":\n                    temp_len = 10\n                else:\n                    convs = solution[\"messages\"]\n                    temp_len = sum(1 for conv in convs if conv['role'] == 'assistant')\n            except KeyError:\n                print(f\"Missing keys in data with index {index}\")\n                continue\n        else:\n            print(f\"No matching solution found for index {index}\")\n            continue\n        \n        turns_dis.append(temp_len)\n    \n    # 假设这里的count_elements_in_intervals是之前定义好的，用于统计区间内元素的分布\n    dis = count_elements_in_intervals(turns_dis)\n    print(\"Distribution of turns:\\n\", dis)  \n    print(\"Average turns:\\n\", sum(turns_dis) / len(compare_data))\n   \n    #print(\"Average token length:\\n\", total_token_length / len(compare_data) if compare_data else 0)\n\n# 假设count_elements_in_intervals是一个已经定义好的函数，这里就不再实现它了\n\n\n\nfile_list=[(\"/data/zyl7353/codeinterpreterbenchmark/codeinterpreter_ultrachat_ablation.jsonl\",\"/data/zyl7353/codeinterpreterbenchmark/compare_codeinterpreter_ultrachat_ablation.jsonl\"),\n(\"/data/zyl7353/codeinterpreterbenchmark/codeinterpreter_cpt_jupyter.jsonl\",\"/data/zyl7353/codeinterpreterbenchmark/compare_codeinterpreter_cpt.jsonl\"),\n(\"/data/zyl7353/codeinterpreterbenchmark/Llama3_70b_Instruct0613.jsonl\",\"/data/zyl7353/codeinterpreterbenchmark/compare_Llama3_70b_instruct_v1.jsonl\"),\n(\"/data/zyl7353/codeinterpreterbenchmark/codellama_70b_instruct.jsonl\",\"/data/zyl7353/codeinterpreterbenchmark/compare_codellama_70b_instruct.jsonl\"),\n(\"/data/zyl7353/codeinterpreterbenchmark/codellama34binstruct.jsonl\",\"/data/zyl7353/codeinterpreterbenchmark/compare_codellama34binstruct.jsonl\"),\n(\"/data/zyl7353/codeinterpreterbenchmark/deepseekcoder_33b.jsonl\",\"/data/zyl7353/codeinterpreterbenchmark/compare_deepseekcoder33b_instruct.jsonl\")]#file_list=[\"NLPython.jsonl\"]\nfor paths in file_list:\n    print(\"============================\")\n    compare_path,solution_path=paths[1],paths[0]\n    print(solution_path,\"\\n\")\n    \n    statistic_info(compare_path,solution_path)\n    break\n    #except:\n        \n        #print(\"Format Error\")\n\n\n\n    "}
{"type": "source_file", "path": "llms/schema.py", "content": "from __future__ import annotations\n\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional\n\nfrom pydantic import BaseModel\nfrom typing_extensions import Literal\n\n\nclass RoleType(Enum):\n    SYSTEM = \"system\"\n    USER = \"user\"\n    ASSISTANT = \"assistant\"\n    TOOL = \"tool\"\n    KNOWLEDGE = \"knowledge\"\n\n\nclass CriticCategory(Enum):\n    POSITIVE: str = \"POSITIVE\"\n    INCONFIDENT: str = \"INCONFIDENT\"\n    NEGATIVE: str = \"NEGATIVE\"\n\n\nclass DataType(Enum):\n    \"\"\"function call中，参数支持的数据类型\n    包括整数、字符串、小数、布尔值、列表、字典\n    \"\"\"\n\n    INTEGER: str = \"integer\"\n    STRING: str = \"string\"\n    NUMBER: str = \"number\"\n    BOOLEAN: str = \"boolean\"\n    ARRAY: str = \"array\"\n    OBJECT: str = \"object\"\n\n\nclass ToolProperty(BaseModel):\n    \"\"\"定义工具时，参数的定义\"\"\"\n\n    type: DataType  # 参数类别\n    description: Optional[str] = None  # 参数描述\n    enum: Optional[List] = None  # 枚举类时才有值\n    items: Optional[ToolProperty] = None  # type == array时，items才有值\n    # type == object时，properties才有值\n    properties: Optional[Dict[str, ToolProperty]] = None\n    required: Optional[List[str]] = None  # type == object时，required才有值\n\n\nclass FunctionDefinition(BaseModel):\n    \"\"\"\n    ADC Function definition, a custom version of the OpenAI Function definition.\n    https://github.com/openai/openai-python/blob/main/src/openai/types/shared/function_definition.py\n    \"\"\"\n\n    name: str\n    \"\"\"The name of the function to be called.\n\n    Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length\n    of 64.\n    \"\"\"\n    description: Optional[str] = None\n    \"\"\"\n    A description of what the function does, used by the model to choose when and\n    how to call the function.\n    \"\"\"\n    parameters: ToolProperty  # Optional[FunctionParameters] = None\n    \"\"\"The parameters the functions accepts, described as a JSON Schema object.\n\n    See the\n    [guide](https://platform.openai.com/docs/guides/text-generation/function-calling)\n    for examples, and the\n    [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for\n    documentation about the format.\n\n    Omitting `parameters` defines a function with an empty parameter list.\n    \"\"\"\n\n\nclass Tool(BaseModel):\n    type: Literal[\"function\"]\n    function: FunctionDefinition\n\n\nclass Function(BaseModel):\n    arguments: str\n    \"\"\"\n    The arguments to call the function with, as generated by the model in JSON\n    format. Note that the model does not always generate valid JSON, and may\n    hallucinate parameters not defined by your function schema. Validate the\n    arguments in your code before calling your function.\n    \"\"\"\n\n    name: str\n    \"\"\"The name of the function to call.\"\"\"\n\n\nclass ToolCall(BaseModel):\n    id: str\n    \"\"\"The ID of the tool call.\"\"\"\n\n    type: Literal[\"function\"]\n    \"\"\"The type of the tool. Currently, only `function` is supported.\"\"\"\n\n    function: Function\n    \"\"\"The function that the model called.\"\"\"\n\n\nclass Message(BaseModel):\n    role: RoleType\n    content: Optional[str] = None\n    tool_calls: Optional[List[ToolCall]] = None  # 可选，在role=assistant时可选\n    # 可选，在role=tool时才有，必须与之前某个Message中的ToolCall中的id相同\n    tool_call_id: Optional[str] = None\n\n\nclass DebugInfo(BaseModel):\n    info: Optional[Dict[str, Any]] = None\n    exception: Optional[str] = None\n    traceback: Optional[str] = None\n\n\nclass Messages(BaseModel):\n    messages: List[Message]\n\n\nclass CriticResults(BaseModel):\n    critic_results: List[CriticCategory]\n\n\nclass Tools(BaseModel):\n    tools: List[Tool]\n\n\nclass LLMInput(BaseModel):\n    messages: List[Message]\n    tools: Optional[List[Tool]] = None\n\n\nclass TracerDB(BaseModel):\n    timestamp: datetime\n    trace_id: str\n    config_id: str\n    round: int\n    current_task: str\n    module_type: str\n    history_messages: List[Message]\n    debug_info: DebugInfo\n\n\nclass ModuleType(Enum):\n    RESPONDER = \"responder\"\n    TOOL = \"tool\"\n    CRITIC = \"critic\"\n    USERINPUTGENERATOR = \"user_input_generator\"\n    SYSTEMPROMPTGENERATOR = \"system_prompt_generator\"\n    USERPROFILEGENERATOR = \"user_profile_generator\"\n\n\nclass LLMArgs(BaseModel):\n    args: Dict\n\n\nclass LLMDB(BaseModel):\n    config_id: Optional[str] = \"\"\n    module_type: ModuleType\n    llm_type: str\n    llm_args: LLMArgs\n    input: LLMInput\n    output: Message\n\n\nclass MessageDB(BaseModel):\n    trace_id: str\n    config_id: str\n    messages: List[Message]\n    critic_results: List[CriticCategory]\n\n\nclass ConfigDB(BaseModel):\n    config_id: str\n    config: str\n\n\nclass WokerRunStatus(Enum):\n    sucess = 1\n    error = 0\n\n\nclass RetryInfo(BaseModel):\n    timestamp: datetime\n    task_id: str\n    module_type: str\n    history_messages: List[Message]\n    debug_info: DebugInfo\n\n\nclass WorkerRes(BaseModel):\n    worker_run_state: WokerRunStatus\n    trace_id: str\n    messages: List[Message]\n    critic_results: List[CriticCategory]\n    traces: List[RetryInfo]\n"}
{"type": "source_file", "path": "llms/base_llm.py", "content": "from abc import ABC, abstractmethod\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nfrom llms.schema import DebugInfo, Message, Tool\n\nLLMConfig = Dict[str, Any]\n\n\nclass BaseLLM(ABC):\n    def __init__(self, llm_config: LLMConfig):\n        self.config = llm_config\n\n    @abstractmethod\n    def generate(\n        self,\n        messages: List[Union[Message, Dict]],\n        tools: Optional[List[Union[Tool, Dict]]] = None,\n    ) -> Tuple[Message, DebugInfo]:\n        \"\"\"接受用户传入的messages和tools[可选]，返回新的message和debug_info\n        Args:\n            messages: chatml messages\n            tools = openai tools description\n        Returns:\n            new_message, debug_info\n        \"\"\"\n        ...\n"}
{"type": "source_file", "path": "utils/extract_raw_data.py", "content": "import json\nfrom glob import glob\nimport pandas as pd\n\n\ndef find_fuzzy_file(file_name, row_idx):\n    files = []\n    \n    if isinstance(file_name, str):\n        file_name = file_name.replace(\"📄\", \"\")\n        for file in glob(f\"./data/{file_name}.*\"):\n            files.append(file)\n    for file in glob(f\"./data/{row_idx}.*\"):\n        files.append(file)\n    return files\n\n\ndf = pd.read_excel(\"./data/meta/工具学习能力分类.xlsx\")\ndata = []\ncategory1, category2, category3 = None, None, None\n\"\"\"\n                    能力大项  能力分项   具体能力                                               附带文件                                      system prompt                                                 问题  Unnamed: 6 Unnamed: 7\n0  简单数据分析处理\\n（pandas）  数据清洗  去除重复项                           📄yearly_deaths_by_clinic  You are a proficient Data Scientist who good a...  Could you help me clean the given dataset? Esp...         NaN        NaN\n1                 NaN   NaN    NaN  📄Week 40 - US Christmas Tree Sales - 2010 to 2016  You are a proficient Data Scientist who good a...                                   帮我处理一下这个数据里面的重复值         NaN        NaN\n2                 NaN   NaN   去除空值                             📄accessories_organizer  You are a proficient Data Scientist who good a...                    Let's get rid of the null value         NaN        NaN\n3                 NaN   NaN    NaN       📄ThrowbackDataThursday - 202001 - Ozone Hole  You are a proficient Data Scientist who good a...                        请帮我做一下简单的数据预处理，检查空值，重复值和异常值         NaN        NaN\n4                 NaN   NaN  去除异常值                                    📄activity_clean  You are a proficient Data Scientist who good a...             Please detect and handle with outliers         NaN        NaN\n\"\"\"\nfor i, row in df.iterrows():\n    # print(row.to_dict())\n    if isinstance(row[\"能力大项\"], str) and row[\"能力大项\"].strip() != \"\":\n        category1 = row[\"能力大项\"]\n    if isinstance(row[\"能力分项\"], str) and row[\"能力分项\"].strip() != \"\":\n        category2 = row[\"能力分项\"]\n    if isinstance(row[\"具体能力\"], str) and row[\"具体能力\"].strip() != \"\":\n        category3 = row[\"具体能力\"]\n    question = row[\"问题\"]\n    if not isinstance(question, str) or question.strip() == \"\":\n        attach = row[\"附带文件\"]\n        files = find_fuzzy_file(attach, i + 2)\n        if isinstance(attach, str):\n            data[-1][\"attachments\"].append(attach)\n        data[-1][\"file_paths\"] += files\n    else:\n        attach = row[\"附带文件\"]\n        files = find_fuzzy_file(attach, i + 2)\n        data.append(\n            {\n                \"index\": str(i + 2),\n                \"category1\": category1,\n                \"category2\": category2,\n                \"category3\": category3,\n                \"user\": question,\n                \"file_paths\": files,\n                \"attachments\": [],\n            }\n        )\n        if isinstance(attach, str):\n            data[-1][\"attachments\"].append(attach)\njson.dump(data, open(\"./data/meta/task.json\", \"w\"), ensure_ascii=False, indent=4)\n"}
{"type": "source_file", "path": "llms/exceptions.py", "content": "# ADC LLMs Exceptions\nclass LLMError(Exception):\n    \"\"\"Base exception class for my_package errors.\"\"\"\n\n    ...\n\n\nclass FunctionCallDeprecationError(LLMError):\n    \"\"\"Raised when OpenaiAPI returns a message including 'function_call' field.\"\"\"\n\n    ...\n\n\nclass APIKeyPoolEmptyError(Exception):\n    \"\"\"Raised when client pool is empty.\"\"\"\n\n    ...\n"}
{"type": "source_file", "path": "llms/api_key_pool.py", "content": "\"\"\"# Hyper-parameters set according to OpenAI's site:\nhttps://platform.openai.com/docs/guides/rate-limits\n\n## OpenAI defines 5 rate limits type.\nRPM: Rate per minute\nTPM: Tokens per minute\nRPD: Rate per day\nTPD: Tokens per day\nIPM: Image per minute (useless for us now)\n\n## Rate limits (Tier 1 as example)\nThis is a high level summary and there are per-model exceptions to these limits\n(e.g. some legacy models or models with larger context windows have different\nrate limits). To view the exact rate limits per model for your account, visit\nthe limits section of your account settings.\n\nMODEL\t                RPM\t    RPD\t    TPM\t        BATCH QUEUE LIMIT\ngpt-4-turbo\t            500\t    -\t    30,000\t    90,000\ngpt-4\t                500\t    10,000\t10,000\t    100,000\ngpt-3.5-turbo\t        3,500\t10,000\t60,000\t    200,000\ntext-embedding-3-large\t3,000\t-\t    1,000,000\t3,000,000\ntext-embedding-3-small\t3,000\t-\t    1,000,000\t3,000,000\ntext-embedding-ada-002\t3,000\t-\t    1,000,000\t3,000,000\n\n## Price:\nhttps://openai.com/api/pricing/\n|Model                  |Input                  |Output|\n|-|-|-|\n|gpt-4-turbo-2024-04-09 |US$10.00 / 1M tokens   |US$ 30.00 / 1M tokens|\n|gpt-4                  |US$30.00 / 1M tokens   |US$ 60.00 / 1M tokens|\n|gpt-4-32k              |US$60.00 / 1M tokens   |US$120.00 / 1M tokens|\n|gpt-3.5-turbo-0125     |US$ 0.50 / 1M tokens   |US$  1.50 / 1M tokens|\n|gpt-3.5-turbo-instruct |US$ 1.50 / 1M tokens   |US$  2.00 / 1M tokens|\n\nNOTE: The rate limits are subject to change at any time.\nThis module provides a thread-safe singleton class `ClientPool` that\nmanages a pool of OpenAI API clients. It is designed to be used as a\ncontext manager to ensure that clients are properly released back to the\npool when they are no longer in use.\n\nThe `ClientPool` class is a singleton, meaning that only one instance of it\ncan exist at any given time. This is achieved by using the `ThreadSafeSingleton`\nmetaclass, which ensures that only one instance of the class is created\nregardless of the number of threads.\n\nExample:\n    get_from_pool = ClientPool([\n        {\"key\": \"YOUR_API_KEY_1\", \"url\": \"https://api.openai.com/v1\"},\n        {\"key\": \"YOUR_API_KEY_2\", \"url\": \"https://api.openai.com/v1\"},\n        {\"key\": \"YOUR_API_KEY_3\", \"url\": \"https://api.openai.com/v1\"},\n    ])\n\n    with get_from_pool as client_args:\n        client = OpenAI(**client_args)\n        completion = client.chat.completions.create(**model_args)\n        return completion.choices[0].message\n\"\"\"\n\nimport random\nimport time\nfrom abc import abstractmethod\nfrom heapq import heapify, heappop, heappush\nfrom typing import Dict, List, NewType, Optional\n\nfrom loguru import logger\nfrom openai import RateLimitError\n\nfrom llms.exceptions import APIKeyPoolEmptyError\n\nA_DAY_IN_SECONDS = 24 * 60 * 60\nA_MINITE_IN_SECONDS = 60\n\nRPM_INTERVAL = 500\nTPM_QUOTA = 30_000\n\nINPUT_PRICE_PER_TOKEN = 10\nOUTPUT_PRICE_PER_TOKEN = 30\n\nClientArgs = NewType(\"ClientArgs\", Dict[str, str | List[str]])\n\n\nclass Resource:\n    def __init__(\n        self,\n        api_key: str,\n        base_url: str,\n        organization: Optional[str] = None,\n        project: Optional[str] = None,\n        max_retries: int = 2,\n        timeout: Optional[float] = None,\n    ) -> None:\n        # # client args\n        self.client_args: ClientArgs = {\n            \"api_key\": api_key,\n            \"base_url\": self._enforce_trailing_slash(base_url),\n            \"organization\": organization,\n            \"project\": project,\n            \"max_retries\": max_retries,\n            \"timeout\": timeout,\n        }\n\n        # priority items\n        self._last_used_time: float = time.time()\n        self._request_count: int = 0\n        self._prompt_tokens: int = 0\n        self._completion_tokens: int = 0\n        self._total_tokens: int = 0\n        # self._client = OpenAI(api_key=self.api_key, base_url=self.base_url)\n\n    def __lt__(self, other: \"Resource\") -> bool:\n        \"\"\"Compare two resources based on request rate, token usage and price\n        Args:\n            other (Resource): The other resource to compare with\n        Returns:\n            bool: True if this resource has a lower request rate and token usage\n            than the other resource, False otherwise\n        \"\"\"\n        if abs(self._last_used_time - other._last_used_time) > A_MINITE_IN_SECONDS:\n            return self._last_used_time < other._last_used_time\n        elif abs(self._request_count - other._request_count) > RPM_INTERVAL:\n            return self._request_count < other._request_count\n        else:\n            return (\n                self._prompt_tokens * INPUT_PRICE_PER_TOKEN\n                + self._completion_tokens * OUTPUT_PRICE_PER_TOKEN\n            ) < (\n                other._prompt_tokens * INPUT_PRICE_PER_TOKEN\n                + other._completion_tokens * OUTPUT_PRICE_PER_TOKEN\n            )\n\n    def __gt__(self, other: \"Resource\") -> bool:\n        \"\"\"Compare two resources based on request rate and token usage\n        Args:\n            other (Resource): The other resource to compare with\n        Returns:\n            bool: True if this resource has a higher request rate and token usage\n            than the other resource, False otherwise\n        \"\"\"\n        return not self.__lt__(other)\n\n    def _enforce_trailing_slash(self, url: str) -> str:\n        if not url.endswith(\"/\"):\n            url += \"/\"\n        return url\n\n    def update(self, usage: Dict[str, int]) -> \"Resource\":\n        current_time = time.time()\n        assert self._last_used_time <= current_time\n        assert (\n            usage[\"prompt_tokens\"] + usage[\"completion_tokens\"] == usage[\"total_tokens\"]\n        )\n        if current_time - self._last_used_time > A_MINITE_IN_SECONDS:\n            self._request_count = 1\n            self._prompt_tokens = usage[\"prompt_tokens\"]\n            self._completion_tokens = usage[\"completion_tokens\"]\n        else:\n            self._request_count += 1\n            self._prompt_tokens += usage[\"prompt_tokens\"]\n            self._completion_tokens += usage[\"completion_tokens\"]\n\n        return self\n\n\nclass Pool:\n    def __init__(self, client_args: ClientArgs) -> None:\n        assert \"api_keys\" in client_args or \"api_key\" in client_args\n        assert not (\"api_keys\" in client_args and \"api_key\" in client_args)\n\n        if \"api_key\" in client_args:\n            self.queue: List[Resource] = [Resource(**client_args)]\n        elif \"api_keys\" in client_args:\n            self.queue: List[Resource] = [\n                Resource(\n                    api_key=api_key,\n                    base_url=client_args[\"base_url\"],\n                    organization=client_args.get(\"organization\", None),\n                    project=client_args.get(\"project\", None),\n                    max_retries=client_args.get(\"max_retries\", 2),\n                    timeout=client_args.get(\"timeout\", None),\n                )\n                for api_key in client_args[\"api_keys\"]\n            ]\n\n        self._index: int = 0\n\n    def __len__(self) -> int:\n        return len(self.queue)\n\n    @abstractmethod\n    def __enter__(self, **kwargs) -> ClientArgs:\n        \"\"\"Get a client from the pool and return it as a context manager.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __exit__(self, exc_type, exc_value, exc_traceback) -> bool:\n        \"\"\"Put the resource back in the pool.\"\"\"\n        raise NotImplementedError\n\n    def _remove_cur_arg(self) -> None:\n        logger.info(f\"removing api key {self.queue[self._index]}\")\n        _ = self.client_args.pop(self._index)\n        if len(self) == 0:\n            raise APIKeyPoolEmptyError(\"api key pool is empty\")\n\n\nclass PQPool(Pool):\n    def __init__(self, client_args: ClientArgs) -> None:\n        super().__init__(client_args)  # unlimited size\n\n        # Initialize the heap with resources\n        heapify(self.queue)\n\n    def __enter__(self, **kwargs) -> ClientArgs:\n        \"\"\"Get a client from the pool and return it as a context manager.\"\"\"\n\n        if len(self) == 0:\n            raise APIKeyPoolEmptyError(\"api key pool is empty\")\n\n        self._cur_resource = heappop(self.queue)\n\n        return self._cur_resource.client_args\n\n    def __exit__(self, exc_type, exc_value, exc_traceback) -> bool:\n        \"\"\"Put the resource back in the pool.\"\"\"\n\n        # TODO how to update the usage?\n        self._cur_usage = {\n            \"prompt_tokens\": 0,\n            \"completion_tokens\": 0,\n            \"total_tokens\": 0,\n        }\n        heappush(self.queue, self._cur_resource.update(self._cur_usage))\n        if exc_type is not None:\n            # Log the exception or handle it as needed\n            logger.error(f\"Client Pool Error: {exc_type}, {exc_value}\")\n            return False\n        return True\n\n\nclass RandomPool(Pool):\n    def __init__(self, client_args: ClientArgs) -> None:\n        super().__init__(client_args)\n        logger.info(f\"initializing RandomPool with {len(client_args)} api keys\")\n        logger.info(f\"client args: {client_args}\")\n\n    def __enter__(self, *args, **kwargs) -> ClientArgs:\n        \"\"\"Get a client from the pool and return it as a context manager.\"\"\"\n\n        if len(self) == 0:\n            raise APIKeyPoolEmptyError(\"api key pool is empty\")\n\n        self._index = random.randint(0, len(self) - 1)\n        return self.queue[self._index].client_args\n\n    def __exit__(self, exc_type, exc_value, exc_traceback) -> bool:\n        \"\"\"Put the resource back in the pool.\"\"\"\n\n        if exc_value is not None:\n            if (\n                exc_type is RateLimitError\n                and exc_value.status_code == 429\n                and judge_quota_exceeded(exc_value.message)\n            ):\n                self._remove_cur_arg()\n            return False\n        return True\n\n\n# class SpinPool(metaclass=Singleton):\nclass SpinPool(Pool):\n    def __init__(self, client_args: ClientArgs) -> None:\n        super().__init__(client_args)\n        random.shuffle(self.queue)\n\n    def __enter__(self, *arg, **kwargs) -> ClientArgs:\n        \"\"\"Get a client from the pool and return it as a context manager.\"\"\"\n\n        if len(self) == 0:\n            raise APIKeyPoolEmptyError(\"api key pool is empty\")\n\n        return self.queue[self._index].client_args\n\n    def __exit__(self, exc_type, exc_value, exc_traceback) -> bool:\n        \"\"\"Put the resource back in the pool.\"\"\"\n\n        if exc_value is None:\n            self._index = (self._index + 1) % len(self)\n            return True\n        else:\n            if (\n                exc_type is RateLimitError\n                and exc_value.status_code == 429\n                and judge_quota_exceeded(exc_value.message)\n            ):\n                self._remove_cur_arg()\n            else:\n                self._index = (self._index + 1) % len(self)\n            return False\n\n\ndef judge_quota_exceeded(message: str) -> bool:\n    return any(word in message.lower() for word in (\"quota\", \"bill\", \"额度\", \"余额\"))\n"}
{"type": "source_file", "path": "utils/LlamaCompletions.py", "content": "from openai import OpenAI\nfrom jinja2 import Environment, PackageLoader, select_autoescape\nfrom jinja2 import Template\n\nclass LlamaCompletion():\n    def __init__(self) -> None:\n        self.client=OpenAI(\n            base_url=\"http://localhost:8001/v1\",\n            api_key=\"token-abc123\",\n        )\n        \n        self.model =\"/data/zyl7353/models/codeinterpreter_0529-hf\"\n        \n        self.template=Template('''{% for message in messages %}{{'<|begin_of_text|>' + message['role'] + '\\n' + message['content']}}{% if (loop.last and add_generation_prompt) or not loop.last %}{{ '<|end_of_text|>' + '\\n'}}{% endif %}{% endfor %}\n{% if add_generation_prompt and messages[-1]['role'] != 'assistant' %}{{ '<|begin_of_text|>assistant\\n' }}{% endif %}''')\n       \n        \n        \n    def chat(self,messages):\n        message_string=self.format_message(messages)\n        \n        message_string=message_string.rsplit(\"<|execute_start|>\",1)[0]+\"<|execute_start|>\"\n        \n        #message_string+=\"\\n<|begin_of_text|>assistant\\n\"\n        print(message_string)\n        print(\"----=====================-----\\n\")\n        completion = self.client.completions.create(\n        model=self.model,\n        prompt=message_string,\n        max_tokens=8192,\n        echo=False,\n        stream=False,\n        temperature=0.2\n        )\n        ans=\"\"\n        for choice in completion.choices:\n            print(choice.text)\n            ans+=choice.text.strip()\n            \n        return ans\n    \n    def format_message(self,messages):\n        rendered = self.template.render(messages=messages, add_generation_prompt=True)\n        return rendered\n        \n\nif __name__==\"__main__\":\n    L=LlamaCompletion()\n    rsp=L.chat([{\"role\":\"system\",\"content\":\"you are a helpful assistant\"},{\"role\":\"user\",\"content\":\"Analyse the dataset\"},{\"role\":\"assistant\",\"content\":\"Analyse: I'am going to write python code to load the dataset at ./data.csv.\\n<|execute_start|>\\n\"}])\n    print(rsp)\n\n"}
{"type": "source_file", "path": "utils/save_notebook.py", "content": "import json\n\n\ndef generate_notebook(cells):\n    new_cells = [\n        generate_notebook_one_cell(\n            cell[\"role\"],\n            text=cell.get(\"text\", None),\n            code=cell.get(\"code\", None),\n            result=cell.get(\"result\", None),\n        )\n        for cell in cells\n    ]\n    return new_cells\n\n\ndef generate_notebook_one_cell(role, text=None, code=None, result=None):\n    if text is not None and code is None and result is None:\n        # 仅传入文本内容，生成Markdown单元格\n        cell = {\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": f\"<|{role}|>\\n\\n{text}\"}\n    elif code is not None and result is not None:\n        # 传入文本、代码和执行结果，生成Code单元格\n        cell = {\n            \"cell_type\": \"code\",\n            \"execution_count\": None,\n            \"metadata\": {},\n            \"outputs\": [{\"name\": \"stdout\", \"text\": [result], \"output_type\": \"stream\"}],\n            \"source\": [code],\n        }\n    else:\n        raise ValueError(\"Invalid combination of parameters.\")\n\n    return cell\n\n\ndef save_as_ipynb(cells, filename):\n    notebook = {\n        \"cells\": cells,\n        \"metadata\": {\n            \"kernelspec\": {\n                \"display_name\": \"Python 3\",\n                \"language\": \"python\",\n                \"name\": \"python3\",\n            },\n            \"language_info\": {\n                \"codemirror_mode\": {\"name\": \"ipython\", \"version\": 3},\n                \"file_extension\": \".py\",\n                \"mimetype\": \"text/x-python\",\n                \"name\": \"python\",\n                \"nbconvert_exporter\": \"python\",\n                \"pygments_lexer\": \"ipython3\",\n                \"version\": \"3.7.11\",\n            },\n        },\n        \"nbformat\": 4,\n        \"nbformat_minor\": 5,\n    }\n\n    with open(filename, \"w\") as file:\n        json.dump(notebook, file, indent=2)\n\n    print(f\"Notebook saved as {filename}\")\n\n\nif __name__ == \"__main__\":\n    messasges = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a proficient Data Scientist who good at data analysis.\",\n        },\n        {\"role\": \"user\", \"content\": \"What is the result of 2^1000\"},\n        {\"role\": \"assistant\", \"content\": \"The result is:\"},\n    ]\n"}
{"type": "source_file", "path": "llms/openai_api.py", "content": "import traceback\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nimport openai\nfrom loguru import logger\nfrom openai.types.chat.chat_completion import (\n    ChatCompletionMessage as OpenAIMessage,\n)\nfrom tenacity import retry, stop_after_attempt, wait_random\n\nfrom llms.api_key_pool import SpinPool\nfrom llms.base_llm import BaseLLM, LLMConfig\nfrom llms.schema import DebugInfo, Message, Tool\nfrom llms.utils import message2dict, tool2dict\n\n\nclass OpenAIAPI(BaseLLM):\n    def __init__(self, config: LLMConfig) -> None:\n        super().__init__(config)\n        self.pool = SpinPool(config[\"client_args\"])\n\n    def get_client_args(self):\n        return self.pool\n\n    def generate(\n        self,\n        messages: List[Union[Message, Dict]],\n        tools: Optional[List[Union[Tool, Dict]]] = None,\n    ) -> Tuple[Message, DebugInfo]:\n        if len(messages) > 0 and isinstance(messages[0], Message):\n            kwargs = dict(\n                messages=[message2dict(msg) for msg in messages],\n            )\n        elif isinstance(messages[0], dict):\n            kwargs = dict(\n                messages=messages,\n            )\n        kwargs.update(self.config[\"model_args\"])\n        if tools is not None:\n            if len(tools) > 0 and isinstance(tools[0], Tool):\n                kwargs[\"tools\"] = [tool2dict(tool) for tool in tools]\n                kwargs[\"tool_choice\"] = \"auto\"\n            else:\n                kwargs[\"tools\"] = tools\n                kwargs[\"tool_choice\"] = \"auto\"\n\n        new_message, debug_info = self._post_request(kwargs)\n\n        new_message = Message(**new_message.model_dump())\n        return new_message, debug_info\n\n    @retry(stop=stop_after_attempt(10), wait=wait_random(min=5, max=10))\n    def _post_request(self, kwargs: Dict[str, Any]) -> Tuple[OpenAIMessage, DebugInfo]:\n        try:\n            with self.get_client_args() as client_args:\n                logger.debug(f\"getting client args: {client_args}\")\n                client = openai.OpenAI(\n                    api_key=client_args.get(\"api_key\", None),\n                    base_url=client_args.get(\"base_url\", None),\n                )\n                completion = client.chat.completions.create(**kwargs)\n                logger.debug(f\"{completion}\")\n        except openai.RateLimitError as e:\n            logger.warning(f\"OpenAI rate limit error: {e}\")\n            raise e\n        except openai.APIError as e:\n            logger.error(f\"OpenAI API error: {e}\")\n            raise e\n        except Exception as e:\n            logger.error(f\"Unexpected error: {e}, type{e}, args{e.args}\")\n            logger.error(f\"Traceback: {traceback.format_exc()}\")\n            raise e\n\n        new_message = completion.choices[0].message\n        return new_message, DebugInfo(\n            info={\"OpenAICompletion\": completion.model_dump()}\n        )\n"}
{"type": "source_file", "path": "llms/__init__.py", "content": "from llms.base_llm import BaseLLM\n\nfrom llms.openai_api import OpenAIAPI\nfrom typing import Dict, Any\n\n__all__ = [\"BaseLLM\", \"OpenAIAPI\"]\n\n\ndef build_llm(llm_type: str, config: Dict[str, Any]) -> BaseLLM:\n    \"\"\"build llm from config\n\n    Args:\n        llm_type (str): llm type, only support  openai\n        config (Dict[str, Any]): the dict of llm config\n\n    Raises:\n        ValueError: llm_type should be  openai\n\n    Returns:\n        BaseLLM:  OpenAIAPI\n    \"\"\"\n    \n    if llm_type == \"openai\":\n        llm = OpenAIAPI(config)\n    else:\n        raise ValueError(\"llm_type should be llmcenter or openai\")\n    return llm\n"}
{"type": "source_file", "path": "utils/output_parser.py", "content": "\"\"\"output parser for code interpreter\"\"\"\n\nimport ast\nfrom typing import Tuple\n\n\ndef parse_code_action(\n    output: str,\n    mode: str = \"prompt\",\n    code_start_token: str = \"```\\npython\\n\",\n    code_end_token: str = \"```\",\n    tool_call_token: str = \"<|tool_call|>\",\n) -> Tuple[str, str]:\n    \"\"\"parse output from code interpreter\n\n    Args:\n        output (str): the output from code interpreter\n        mode: the mode of the output, could be prompt, functioncall, assistant\n        code_start_token: the token code script starts with, only used in prompt mode\n        code_end_token: the token code script ends with, only used in prompt mode\n        tool_call_token: the token for tool call, only used in prompt mode\n\n    Returns:\n        Tuple[str, str]: reasoning and code action\n    \"\"\"\n    if mode == \"prompt\":\n        return extract_code(output, code_start_token, code_end_token)\n    elif mode == \"functioncall\":\n        rsp = fc2dict(output, tool_call_token)\n        if \"tool_calls\" in rsp and len(rsp[\"tool_calls\"]) > 0:\n            return rsp[\"content\"], rsp[\"tool_calls\"][0][\"arguments\"][\"code\"]\n        else:\n            return rsp[\"content\"], \"\"\n    elif mode == \"assistant\":\n        raise NotImplementedError(\"assistant mode is not implemented yet\")\n    else:\n        raise ValueError(f\"mode {mode} is not supported\")\n\n\ndef extract_code(\n    rsp: str, code_start_token: str = \"```\\npython\\n\", code_end_token: str = \"```\"\n) -> Tuple[str, str]:\n    \"\"\"extract code from assistant content\n\n    Args:\n        rsp (str): the response content from assistant\n        code_start_token (str, optional): the token code script starts with. Defaults to \"```\\npython\".\n        code_end_token (str, optional): the token code script ends with. Defaults to \"```\".\n\n    Returns:\n        Tuple[str, str]: reasoning and code action\n    \"\"\"\n    # TODO: implement the code extraction logic using different code_start_token and code_end_token\n    rsp = str(rsp)\n\n    start_index = rsp.find(code_start_token)\n    if start_index == -1:\n        return rsp, \"\"\n\n    start_index += len(code_start_token)\n    end_index = rsp.find(code_end_token, start_index)\n    if end_index == -1:\n        return rsp, \"\"\n\n    return rsp[:start_index].replace(code_start_token, \"\").strip(), rsp[\n        start_index:end_index\n    ].strip()\n\n\nimport ast\nimport re\nimport json\n\ndef convert_function_call_to_json(string):\n    try:\n        tool_calls = []\n        x = ast.parse(string)\n        for tool in x.body:\n            function_name = tool.value.func.id\n            function_args = {}\n            for keyword in tool.value.keywords:\n                function_args[keyword.arg] = ast.literal_eval(keyword.value)\n            this_one = {\"name\": function_name, \"arguments\": function_args}\n            tool_calls.append(this_one)\n        return tool_calls\n    except Exception:\n        return []\nimport json\n\ndef extract_code_from_arguments(arguments_str):\n    try:\n        arguments_dict = json.loads(arguments_str)\n        return arguments_dict.get(\"code\", \"\")\n    except json.JSONDecodeError:\n        return \"\"\n\ndef fc2dict(sequence: str, spliter=\"<|tool_call|>\"):\n    if spliter in sequence:\n        content, tool_call_string = sequence.split(spliter, 1)\n        try:\n            # 找到第一个 { 和最后一个 }\n            start_idx = tool_call_string.find('{')\n            end_idx = tool_call_string.rfind('}')\n            if start_idx != -1 and end_idx != -1:\n                arguments_str = tool_call_string[start_idx:end_idx + 1]\n                print(\"Arg:\",arguments_str)\n                arguments_str=arguments_str.replace(\"\\n\",\"\\\\n\")\n                #code_content = extract_code_from_arguments(arguments_str)\n                tool_call_dict = {\n                    \"name\": \"execute_python\",\n                    \"arguments\": json.loads(arguments_str)\n                }\n                tool_calls = [tool_call_dict]\n            else:\n                tool_calls = []\n            return {\n                \"content\": content.strip(),\n                \"tool_calls\": tool_calls,\n                \"role\": \"assistant\",\n            }\n        except Exception as e:\n            print(f\"Error: {e}\")\n            return {\"content\": content.strip(), \"role\": \"assistant\"}\n    else:\n        return {\"content\": sequence.strip(), \"role\": \"assistant\"}\n\n# 示例用法\nsequence = '''To fulfill your request, I will perform the following steps:\n\n1. Read the dataset from the provided path.\n2. Extract the necessary data for the radar chart.\n3. Create a radar chart using the extracted data.\n\nLet's start by reading the dataset.\n\nAction:\n\n\n<|tool_call|>execute_python({\"code\":\"import pandas as pd\\n\\n# Read the dataset\\ndata_path = './data/radar.csv'\\ndf = pd.read_csv(data_path)\\ndf.head()\"})\\n'''\nresult = fc2dict(sequence)\nprint(result)\n"}
{"type": "source_file", "path": "utils/draw_win_tie_lose.py", "content": "#!/usr/bin/env python\n# encoding: utf-8\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# 创建数据框\ndata = {\n    'Model': ['GPT-4', 'GPT-3.5','Llama3-8b-instruct', 'CodeQwen7b-chat', \"CodeActAgent-Llama-2-7b\", \"CodeActAgent-Mistral-7b-v0.1\", \"OpenCodeInterpreter-DS-6.7b\", \"Qwen2-7B-Instruct\", \"Py-Llama3-v3\",\"Llama3-70b-Instruct\"],\n    'Win': [117, 71.5, 67, 77, 42, 44, 54, 85, 74,100],\n    'Tie': [3, 0, 1, 2, 2, 3, 1, 2, 3,1],\n    'Lose': [23, 71.5, 75, 64, 99, 96, 88, 56, 53,42]\n}\ndf = pd.DataFrame(data)\n\n# 按Lose的数量降序排列\ndf.sort_values(by='Lose', inplace=True, ascending=False)\n\n# 数据归一化处理\ndf[['Win', 'Tie', 'Lose']] = df[['Win', 'Tie', 'Lose']].div(df[['Win', 'Tie', 'Lose']].sum(axis=1), axis=0)\n\n# 设置颜色，并增加透明度\ncolors = ['#1E90FF', 'lightgrey', '#FF4500']\n\n# 初始化绘图，调整figsize以增大条形图的宽度\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# 绘制堆积条形图\nbottom = [0] * len(df)\nfor i, col in enumerate(['Win', 'Tie', 'Lose']):\n    bars = ax.barh(df['Model'], df[col], left=bottom, color=colors[i], label=col)\n    bottom = bottom + df[col]\n    if col == 'Win':\n        for bar, value in zip(bars, df[col]):\n            # 计算文本位置\n            text_x = bar.get_x() + bar.get_width() / 2\n            ax.text(text_x, bar.get_y() + bar.get_height()/2, f\"{value:.1%}\", va='center', ha='center')\n\n# 设置标题和标签\nax.set_title('Win-Tie-Lose Evaluation of Different Models against GPT-3.5')\nax.set_xlabel('Percentage')\nax.set_ylabel('Model')\n\n# 设置X轴为百分比\nax.set_xlim(0, 1)\nax.set_xticklabels(['{:.0%}'.format(x) for x in ax.get_xticks()])\n\n# 去掉图的边框\nfor spine in ax.spines.values():\n    spine.set_visible(False)\n\n# 添加图例并放在右边\nax.legend(title='Result', loc='center left', bbox_to_anchor=(1, 0.5))\n\n# 显示图形\nplt.tight_layout()\nplt.savefig(\"./WLRate.png\")\n"}
{"type": "source_file", "path": "llms/utils.py", "content": "from typing import Dict, Optional\n\nfrom llms.schema import DataType, Message, Tool, ToolProperty\n\n\ndef property2dict(property_: ToolProperty) -> Dict:\n    \"\"\"把传入的ToolProperty转换为OpenAI接口需要的dict格式\n\n    Args:\n        property (ToolProperty): ToolProperty对话\n\n    Returns:\n        Dict: 参数的dict形式描述\n    \"\"\"\n    new_property = {\"type\": property_.type.value}\n    if property_.description is not None:\n        new_property[\"description\"] = property_.description\n    if property_.type == DataType.OBJECT:\n        new_property[\"properties\"] = {\n            key: property2dict(value) for key, value in property_.properties.items()\n        }\n        if property_.required is not None:\n            new_property[\"required\"] = property_.required\n    elif property_.type == DataType.ARRAY:\n        new_property[\"items\"] = property2dict(property_.items)\n    elif property_.enum is not None:\n        new_property[\"enum\"] = property_.enum\n    return new_property\n\n\ndef tool2dict(tool: Tool) -> Dict[str, str]:\n    parameters = property2dict(tool.function.parameters)\n    parameters[\"type\"] = DataType.OBJECT.name\n    if \"required\" not in parameters:\n        parameters[\"required\"] = []\n    new_tool = {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": tool.function.name,\n            \"description\": tool.function.description,\n            \"parameters\": parameters,\n        },\n    }\n    return new_tool\n\n\ndef message2dict(\n    message: Message,\n) -> Dict[str, Optional[str | list[Dict[str, str]]]]:\n    ret = dict(\n        role=message.role.value,\n        content=message.content,\n        tool_calls=[call.model_dump() for call in message.tool_calls]\n        if message.tool_calls is not None\n        else None,\n        tool_call_id=message.tool_call_id,\n    )\n    ret = {k: v for k, v in ret.items() if v is not None}\n    return ret\n"}
