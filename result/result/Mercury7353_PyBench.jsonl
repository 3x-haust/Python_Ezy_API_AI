{"repo_info": {"repo_name": "PyBench", "repo_owner": "Mercury7353", "repo_url": "https://github.com/Mercury7353/PyBench"}}
{"type": "source_file", "path": "inference.py", "content": "import json\nimport os\nimport traceback\n\nimport fire\nimport matplotlib\n#from langchain_experimental.tools.python.tool import PythonAstREPLTool\nfrom loguru import logger\nfrom yaml import safe_load\n\nfrom llms import build_llm\nfrom llms.utils import message2dict\nfrom utils.assistant import GPT\nfrom utils.output_parser import parse_code_action\nfrom utils.save_notebook import generate_notebook, save_as_ipynb\n\nmatplotlib.use(\"Agg\")\nimport nbformat\nfrom nbclient import NotebookClient\nimport traceback\nimport time\nimport ruamel.yaml\n\ndef execute_code(code_str: str, Kernel, nb):\n    nb.cells.append(nbformat.v4.new_code_cell(code_str))\n    total_cells = len(nb.cells)\n    cell = nb.cells[-1]\n    client = NotebookClient(nb, allow_errors=True)\n    client.kc = Kernel\n    print(cell)\n\n    try:\n        client.reset_execution_trackers()\n        client.execute_cell(cell=cell, cell_index=-1)\n    except Exception as e:\n        traceback.print_exc()\n        error_message = str(e)\n        return error_message\n\n    outputs = nb.cells[-1]['outputs']\n\n    result = \"\"\n    for output in outputs:\n        if output.output_type == \"stream\":  # å¦‚æœè¾“å‡ºæ˜¯æ ‡å‡†è¾“å‡ºæˆ–æ ‡å‡†é”™è¯¯\n            result += output.text\n        elif output.output_type == \"execute_result\":  # å¦‚æœè¾“å‡ºæ˜¯æ‰§è¡Œç»“æœ\n            result += str(output['data']['text/plain'])\n        elif output.output_type == \"error\":  # å¦‚æœè¾“å‡ºæ˜¯é”™è¯¯\n            result += \"There are some errors in the code. All variables in this cell should be redefined. Please Debug:\\n\"\n            result += \"Error: \" + str(output.ename) + \"\\n\"\n            result += str(output.evalue) + \"\\n\"\n\n    return result\n\n\nimport os\nimport json\nimport threading\nimport traceback\nfrom concurrent.futures import ThreadPoolExecutor\nimport fire\nfrom ruamel.yaml import safe_load\nimport nbformat\nfrom nbconvert.preprocessors import ExecutePreprocessor\nfrom nbclient import NotebookClient\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# å…¨å±€çº¿ç¨‹é”\nlock = threading.Lock()\n\ndef process_task(task, config, llm, system_prompt_template, max_turns, output_path, processed_ids):\n    index = task[\"index\"]\n    cell_path=\"Cells\"\n    if index in processed_ids:\n        return\n\n    file_path = \",\".join(task[\"file_paths\"])\n    user_query = task[\"user\"]\n\n    nb = nbformat.v4.new_notebook()\n    client = NotebookClient(nb, allow_errors=True)\n    client.km = client.create_kernel_manager()\n    client.start_new_kernel()\n    client.start_new_kernel_client()\n    Kernel = client.kc\n\n    logger.info(\"==\" * 80)\n    logger.info(f\"Task index: {index}\")\n    logger.info(f\"Task:\\n{user_query}\")\n    logger.info(f\"File path: {file_path}\")\n\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt_template},\n        {\"role\": \"user\", \"content\": f\"[INFO]The data is uploaded to {file_path}\"},\n        {\"role\": \"user\", \"content\": user_query},\n    ]\n    cells = [\n        {\"role\": \"system\", \"text\": system_prompt_template},\n        {\"role\": \"user\", \"text\": f\"[INFO]The data is uploaded to {file_path}\"},\n        {\"role\": \"user\", \"text\": user_query},\n    ]\n\n    try:\n        for count in range(max_turns):  # max 5 turn interaction\n            logger.info(\"--\" * 10 + f\"Round: {count}\" + \"--\" * 10)\n            #logger.info(f\"input messages: {messages}\")\n            try:\n                out_msg, debug_info = llm.generate(messages)\n            except:\n                break\n            #logger.info(f\"output msg: {message2dict(out_msg)}\")\n\n            reasoning, code_script = parse_code_action(\n                out_msg.content,\n                mode=config[\"mode\"],\n                code_start_token=config[\"code_start_token\"],\n                code_end_token=config[\"code_end_token\"],\n                tool_call_token=config[\"tool_call_token\"],\n            )\n            #logger.info(f\"Reasoning: {reasoning}\")\n            #logger.info(f\"Code script: {code_script}\")\n            if code_script is None or code_script.strip() == \"\":\n                messages.append({\"role\": \"assistant\", \"content\": reasoning})\n                cells.append({\"role\": \"assistant\", \"text\": reasoning})\n            else:\n                messages.append(\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": out_msg.content,\n                    }\n                )\n                cells.append({\"role\": \"assistant\", \"text\": reasoning})\n\n            if code_script is None or code_script.strip() == \"\":\n                break\n\n            code_response = execute_code(code_script, Kernel, nb)\n            #logger.info(f\"Code response:\\n{code_response}\")\n            messages.append({\"role\": \"user\", \"content\": \"[INFO]This is a Code Interpreter Message:\\n\" + code_response})\n            cells.append(\n                {\n                    \"role\": \"assistant\",\n                    \"code\": code_script,\n                    \"result\": code_response,\n                }\n            )\n\n        save_as_ipynb(generate_notebook(cells), f\"{cell_path}/{index}.ipynb\")\n        item = {\"messages\": messages}\n        item.update(task)\n    except Exception:\n        logger.error(traceback.format_exc())\n        save_as_ipynb(generate_notebook(cells), f\"{cell_path}/{index}.ipynb\")\n        item = {\"messages\": messages}\n        item.update(task)\n    finally:\n        client._cleanup_kernel()\n\n    \n    with open(output_path, 'a') as f:\n        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n        print(\"======Write Json=======\")\n\ndef main(config_path: str, task_path: str, output_path: str):\n    os.makedirs(\"Check\", exist_ok=True)\n    os.makedirs(\"output\", exist_ok=True)\n    logger.info(\"started\")\n    yaml = ruamel.yaml.YAML()\n    config=yaml.load(open(config_path, \"r\"))\n    logger.info(\"config loaded\")\n    llm = build_llm(config[\"llm\"][\"type\"], config[\"llm\"][\"args\"])\n    logger.info(\"llm built\")\n    system_prompt_template = config[\"system_prompt_template\"]\n    max_turns = config[\"max_turns\"]\n    test_data = json.load(open(task_path, \"r\"))\n    logger.info(f\"total tasks: {len(test_data)}\")\n    if os.path.exists(output_path):\n        processed_ids = set(\n            [json.loads(line)[\"index\"] for line in open(output_path, \"r\")]\n        )\n    else:\n        processed_ids = set()\n\n    with ThreadPoolExecutor(max_workers=16) as executor:\n        futures = [\n            executor.submit(process_task, task, config, llm, system_prompt_template, max_turns, output_path, processed_ids)\n            for task in test_data\n        ]\n        for future in futures:\n            future.result()  # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ\n\n    logger.info(\"finished\")\n\nif __name__ == \"__main__\":\n    fire.Fire(main)\n"}
{"type": "source_file", "path": "utils/calculate_avg_step.py", "content": "# å¦‚æœfailedï¼Œåˆ™æ­¥æ•°ç®—10ï¼Ÿ\n'''\nè®¡ç®—NLPython Instruct çš„ ç»Ÿè®¡ä¿¡æ¯\næ¡æ•°\nå¹³å‡è½®æ•°åˆ†å¸ƒ\næ€»tokenæ•°\n'''\nimport json\n\nfrom collections import Counter\n    \n#model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\ndef read_lines(lines):\n    data = []\n    m = {}\n    for line in lines:\n        try:\n            x = json.loads(line)\n            m[x[\"index\"]] = line\n            data.append(x)\n        except:\n            pass\n    data = sorted(data, key=lambda x: x[\"index\"])\n    lines = [json.dumps(item, ensure_ascii=False, indent=4) for item in data]\n    return lines, m\ndef count_tokens(text):\n    # åˆå§‹åŒ–tokenizer\n    \n    # å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ï¼Œç„¶åè®¡ç®—tokenæ•°é‡\n    tokens = tokenizer.encode(text, add_special_tokens=True)\n    return len(tokens)\n\n\n\ndef read_jsonl(filename):\n    file1 = open(filename)\n    lines1 = file1.readlines()\n    lines,_=read_lines(lines1)\n    return lines\n\n\ndef save_jsonl(data, filename):\n    with open(filename, \"w\") as f:\n        for x in data:\n            print(json.dumps(x, ensure_ascii=False), file=f)\n\ndef count_elements_in_intervals(elements):\n    # å®šä¹‰åŒºé—´\n    intervals = {'<=2': 0, '[4,6]': 0, '[7,9]': 0, '>=10': 0}\n\n    for element in elements:\n        if element <= 3:\n            intervals['<=2'] += 1\n        elif 4 <= element <= 6:\n            intervals['[4,6]'] += 1\n        elif 7 <= element <= 9:\n            intervals['[7,9]'] += 1\n        elif element >= 10:\n            intervals['>=10'] += 1\n\n    return intervals\n\nimport json\n\ndef read_jsonl(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        return file.readlines()\n\ndef statistic_info(compare_path, solution_path):\n    compare_data = read_jsonl(compare_path)\n    solution_data = read_jsonl(solution_path)\n    \n    # å°†solution_dataè½¬æ¢ä¸ºindexåˆ°å†…å®¹çš„æ˜ å°„ï¼Œä¾¿äºåç»­æŸ¥æ‰¾\n    solution_dict = {}\n    for line in solution_data:\n        try:\n            solution = json.loads(line)\n            solution_dict[solution[\"index\"]] = solution\n        except:\n            print(\"Error load\")\n            continue\n    \n    print(\"Length of Compare Data:\", len(compare_data))\n    \n    turns_dis = []\n    total_token_length = 0\n    \n    for line in compare_data:\n        try:\n            compare = json.loads(line)\n            index = compare[\"index\"]\n        except:\n            continue\n        \n        # æ ¹æ®indexæ‰¾åˆ°å¯¹åº”çš„solution\n        if index in solution_dict:\n            solution = solution_dict[index]\n            try:\n                decision = compare[\"Decision\"][\"Pass\"][\"Agent2\"]# 1 for 3.5 , 2 for anothers\n                # å¦‚æœæ˜¯failedï¼Œåˆ™æ­¥æ•°ç›´æ¥ç½®ä¸º10\n                if decision == \"Failed\":\n                    temp_len = 10\n                else:\n                    convs = solution[\"messages\"]\n                    temp_len = sum(1 for conv in convs if conv['role'] == 'assistant')\n            except KeyError:\n                print(f\"Missing keys in data with index {index}\")\n                continue\n        else:\n            print(f\"No matching solution found for index {index}\")\n            continue\n        \n        turns_dis.append(temp_len)\n    \n    # å‡è®¾è¿™é‡Œçš„count_elements_in_intervalsæ˜¯ä¹‹å‰å®šä¹‰å¥½çš„ï¼Œç”¨äºç»Ÿè®¡åŒºé—´å†…å…ƒç´ çš„åˆ†å¸ƒ\n    dis = count_elements_in_intervals(turns_dis)\n    print(\"Distribution of turns:\\n\", dis)  \n    print(\"Average turns:\\n\", sum(turns_dis) / len(compare_data))\n   \n    #print(\"Average token length:\\n\", total_token_length / len(compare_data) if compare_data else 0)\n\n# å‡è®¾count_elements_in_intervalsæ˜¯ä¸€ä¸ªå·²ç»å®šä¹‰å¥½çš„å‡½æ•°ï¼Œè¿™é‡Œå°±ä¸å†å®ç°å®ƒäº†\n\n\n\nfile_list=[(\"/data/zyl7353/codeinterpreterbenchmark/codeinterpreter_ultrachat_ablation.jsonl\",\"/data/zyl7353/codeinterpreterbenchmark/compare_codeinterpreter_ultrachat_ablation.jsonl\"),\n(\"/data/zyl7353/codeinterpreterbenchmark/codeinterpreter_cpt_jupyter.jsonl\",\"/data/zyl7353/codeinterpreterbenchmark/compare_codeinterpreter_cpt.jsonl\"),\n(\"/data/zyl7353/codeinterpreterbenchmark/Llama3_70b_Instruct0613.jsonl\",\"/data/zyl7353/codeinterpreterbenchmark/compare_Llama3_70b_instruct_v1.jsonl\"),\n(\"/data/zyl7353/codeinterpreterbenchmark/codellama_70b_instruct.jsonl\",\"/data/zyl7353/codeinterpreterbenchmark/compare_codellama_70b_instruct.jsonl\"),\n(\"/data/zyl7353/codeinterpreterbenchmark/codellama34binstruct.jsonl\",\"/data/zyl7353/codeinterpreterbenchmark/compare_codellama34binstruct.jsonl\"),\n(\"/data/zyl7353/codeinterpreterbenchmark/deepseekcoder_33b.jsonl\",\"/data/zyl7353/codeinterpreterbenchmark/compare_deepseekcoder33b_instruct.jsonl\")]#file_list=[\"NLPython.jsonl\"]\nfor paths in file_list:\n    print(\"============================\")\n    compare_path,solution_path=paths[1],paths[0]\n    print(solution_path,\"\\n\")\n    \n    statistic_info(compare_path,solution_path)\n    break\n    #except:\n        \n        #print(\"Format Error\")\n\n\n\n    "}
{"type": "source_file", "path": "llms/schema.py", "content": "from __future__ import annotations\n\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional\n\nfrom pydantic import BaseModel\nfrom typing_extensions import Literal\n\n\nclass RoleType(Enum):\n    SYSTEM = \"system\"\n    USER = \"user\"\n    ASSISTANT = \"assistant\"\n    TOOL = \"tool\"\n    KNOWLEDGE = \"knowledge\"\n\n\nclass CriticCategory(Enum):\n    POSITIVE: str = \"POSITIVE\"\n    INCONFIDENT: str = \"INCONFIDENT\"\n    NEGATIVE: str = \"NEGATIVE\"\n\n\nclass DataType(Enum):\n    \"\"\"function callä¸­ï¼Œå‚æ•°æ”¯æŒçš„æ•°æ®ç±»å‹\n    åŒ…æ‹¬æ•´æ•°ã€å­—ç¬¦ä¸²ã€å°æ•°ã€å¸ƒå°”å€¼ã€åˆ—è¡¨ã€å­—å…¸\n    \"\"\"\n\n    INTEGER: str = \"integer\"\n    STRING: str = \"string\"\n    NUMBER: str = \"number\"\n    BOOLEAN: str = \"boolean\"\n    ARRAY: str = \"array\"\n    OBJECT: str = \"object\"\n\n\nclass ToolProperty(BaseModel):\n    \"\"\"å®šä¹‰å·¥å…·æ—¶ï¼Œå‚æ•°çš„å®šä¹‰\"\"\"\n\n    type: DataType  # å‚æ•°ç±»åˆ«\n    description: Optional[str] = None  # å‚æ•°æè¿°\n    enum: Optional[List] = None  # æšä¸¾ç±»æ—¶æ‰æœ‰å€¼\n    items: Optional[ToolProperty] = None  # type == arrayæ—¶ï¼Œitemsæ‰æœ‰å€¼\n    # type == objectæ—¶ï¼Œpropertiesæ‰æœ‰å€¼\n    properties: Optional[Dict[str, ToolProperty]] = None\n    required: Optional[List[str]] = None  # type == objectæ—¶ï¼Œrequiredæ‰æœ‰å€¼\n\n\nclass FunctionDefinition(BaseModel):\n    \"\"\"\n    ADC Function definition, a custom version of the OpenAI Function definition.\n    https://github.com/openai/openai-python/blob/main/src/openai/types/shared/function_definition.py\n    \"\"\"\n\n    name: str\n    \"\"\"The name of the function to be called.\n\n    Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length\n    of 64.\n    \"\"\"\n    description: Optional[str] = None\n    \"\"\"\n    A description of what the function does, used by the model to choose when and\n    how to call the function.\n    \"\"\"\n    parameters: ToolProperty  # Optional[FunctionParameters] = None\n    \"\"\"The parameters the functions accepts, described as a JSON Schema object.\n\n    See the\n    [guide](https://platform.openai.com/docs/guides/text-generation/function-calling)\n    for examples, and the\n    [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for\n    documentation about the format.\n\n    Omitting `parameters` defines a function with an empty parameter list.\n    \"\"\"\n\n\nclass Tool(BaseModel):\n    type: Literal[\"function\"]\n    function: FunctionDefinition\n\n\nclass Function(BaseModel):\n    arguments: str\n    \"\"\"\n    The arguments to call the function with, as generated by the model in JSON\n    format. Note that the model does not always generate valid JSON, and may\n    hallucinate parameters not defined by your function schema. Validate the\n    arguments in your code before calling your function.\n    \"\"\"\n\n    name: str\n    \"\"\"The name of the function to call.\"\"\"\n\n\nclass ToolCall(BaseModel):\n    id: str\n    \"\"\"The ID of the tool call.\"\"\"\n\n    type: Literal[\"function\"]\n    \"\"\"The type of the tool. Currently, only `function` is supported.\"\"\"\n\n    function: Function\n    \"\"\"The function that the model called.\"\"\"\n\n\nclass Message(BaseModel):\n    role: RoleType\n    content: Optional[str] = None\n    tool_calls: Optional[List[ToolCall]] = None  # å¯é€‰ï¼Œåœ¨role=assistantæ—¶å¯é€‰\n    # å¯é€‰ï¼Œåœ¨role=toolæ—¶æ‰æœ‰ï¼Œå¿…é¡»ä¸ä¹‹å‰æŸä¸ªMessageä¸­çš„ToolCallä¸­çš„idç›¸åŒ\n    tool_call_id: Optional[str] = None\n\n\nclass DebugInfo(BaseModel):\n    info: Optional[Dict[str, Any]] = None\n    exception: Optional[str] = None\n    traceback: Optional[str] = None\n\n\nclass Messages(BaseModel):\n    messages: List[Message]\n\n\nclass CriticResults(BaseModel):\n    critic_results: List[CriticCategory]\n\n\nclass Tools(BaseModel):\n    tools: List[Tool]\n\n\nclass LLMInput(BaseModel):\n    messages: List[Message]\n    tools: Optional[List[Tool]] = None\n\n\nclass TracerDB(BaseModel):\n    timestamp: datetime\n    trace_id: str\n    config_id: str\n    round: int\n    current_task: str\n    module_type: str\n    history_messages: List[Message]\n    debug_info: DebugInfo\n\n\nclass ModuleType(Enum):\n    RESPONDER = \"responder\"\n    TOOL = \"tool\"\n    CRITIC = \"critic\"\n    USERINPUTGENERATOR = \"user_input_generator\"\n    SYSTEMPROMPTGENERATOR = \"system_prompt_generator\"\n    USERPROFILEGENERATOR = \"user_profile_generator\"\n\n\nclass LLMArgs(BaseModel):\n    args: Dict\n\n\nclass LLMDB(BaseModel):\n    config_id: Optional[str] = \"\"\n    module_type: ModuleType\n    llm_type: str\n    llm_args: LLMArgs\n    input: LLMInput\n    output: Message\n\n\nclass MessageDB(BaseModel):\n    trace_id: str\n    config_id: str\n    messages: List[Message]\n    critic_results: List[CriticCategory]\n\n\nclass ConfigDB(BaseModel):\n    config_id: str\n    config: str\n\n\nclass WokerRunStatus(Enum):\n    sucess = 1\n    error = 0\n\n\nclass RetryInfo(BaseModel):\n    timestamp: datetime\n    task_id: str\n    module_type: str\n    history_messages: List[Message]\n    debug_info: DebugInfo\n\n\nclass WorkerRes(BaseModel):\n    worker_run_state: WokerRunStatus\n    trace_id: str\n    messages: List[Message]\n    critic_results: List[CriticCategory]\n    traces: List[RetryInfo]\n"}
{"type": "source_file", "path": "llms/base_llm.py", "content": "from abc import ABC, abstractmethod\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nfrom llms.schema import DebugInfo, Message, Tool\n\nLLMConfig = Dict[str, Any]\n\n\nclass BaseLLM(ABC):\n    def __init__(self, llm_config: LLMConfig):\n        self.config = llm_config\n\n    @abstractmethod\n    def generate(\n        self,\n        messages: List[Union[Message, Dict]],\n        tools: Optional[List[Union[Tool, Dict]]] = None,\n    ) -> Tuple[Message, DebugInfo]:\n        \"\"\"æ¥å—ç”¨æˆ·ä¼ å…¥çš„messageså’Œtools[å¯é€‰]ï¼Œè¿”å›æ–°çš„messageå’Œdebug_info\n        Args:\n            messages: chatml messages\n            tools = openai tools description\n        Returns:\n            new_message, debug_info\n        \"\"\"\n        ...\n"}
{"type": "source_file", "path": "utils/extract_raw_data.py", "content": "import json\nfrom glob import glob\nimport pandas as pd\n\n\ndef find_fuzzy_file(file_name, row_idx):\n    files = []\n    \n    if isinstance(file_name, str):\n        file_name = file_name.replace(\"ğŸ“„\", \"\")\n        for file in glob(f\"./data/{file_name}.*\"):\n            files.append(file)\n    for file in glob(f\"./data/{row_idx}.*\"):\n        files.append(file)\n    return files\n\n\ndf = pd.read_excel(\"./data/meta/å·¥å…·å­¦ä¹ èƒ½åŠ›åˆ†ç±».xlsx\")\ndata = []\ncategory1, category2, category3 = None, None, None\n\"\"\"\n                    èƒ½åŠ›å¤§é¡¹  èƒ½åŠ›åˆ†é¡¹   å…·ä½“èƒ½åŠ›                                               é™„å¸¦æ–‡ä»¶                                      system prompt                                                 é—®é¢˜  Unnamed: 6 Unnamed: 7\n0  ç®€å•æ•°æ®åˆ†æå¤„ç†\\nï¼ˆpandasï¼‰  æ•°æ®æ¸…æ´—  å»é™¤é‡å¤é¡¹                           ğŸ“„yearly_deaths_by_clinic  You are a proficient Data Scientist who good a...  Could you help me clean the given dataset? Esp...         NaN        NaN\n1                 NaN   NaN    NaN  ğŸ“„Week 40 - US Christmas Tree Sales - 2010 to 2016  You are a proficient Data Scientist who good a...                                   å¸®æˆ‘å¤„ç†ä¸€ä¸‹è¿™ä¸ªæ•°æ®é‡Œé¢çš„é‡å¤å€¼         NaN        NaN\n2                 NaN   NaN   å»é™¤ç©ºå€¼                             ğŸ“„accessories_organizer  You are a proficient Data Scientist who good a...                    Let's get rid of the null value         NaN        NaN\n3                 NaN   NaN    NaN       ğŸ“„ThrowbackDataThursday - 202001 - Ozone Hole  You are a proficient Data Scientist who good a...                        è¯·å¸®æˆ‘åšä¸€ä¸‹ç®€å•çš„æ•°æ®é¢„å¤„ç†ï¼Œæ£€æŸ¥ç©ºå€¼ï¼Œé‡å¤å€¼å’Œå¼‚å¸¸å€¼         NaN        NaN\n4                 NaN   NaN  å»é™¤å¼‚å¸¸å€¼                                    ğŸ“„activity_clean  You are a proficient Data Scientist who good a...             Please detect and handle with outliers         NaN        NaN\n\"\"\"\nfor i, row in df.iterrows():\n    # print(row.to_dict())\n    if isinstance(row[\"èƒ½åŠ›å¤§é¡¹\"], str) and row[\"èƒ½åŠ›å¤§é¡¹\"].strip() != \"\":\n        category1 = row[\"èƒ½åŠ›å¤§é¡¹\"]\n    if isinstance(row[\"èƒ½åŠ›åˆ†é¡¹\"], str) and row[\"èƒ½åŠ›åˆ†é¡¹\"].strip() != \"\":\n        category2 = row[\"èƒ½åŠ›åˆ†é¡¹\"]\n    if isinstance(row[\"å…·ä½“èƒ½åŠ›\"], str) and row[\"å…·ä½“èƒ½åŠ›\"].strip() != \"\":\n        category3 = row[\"å…·ä½“èƒ½åŠ›\"]\n    question = row[\"é—®é¢˜\"]\n    if not isinstance(question, str) or question.strip() == \"\":\n        attach = row[\"é™„å¸¦æ–‡ä»¶\"]\n        files = find_fuzzy_file(attach, i + 2)\n        if isinstance(attach, str):\n            data[-1][\"attachments\"].append(attach)\n        data[-1][\"file_paths\"] += files\n    else:\n        attach = row[\"é™„å¸¦æ–‡ä»¶\"]\n        files = find_fuzzy_file(attach, i + 2)\n        data.append(\n            {\n                \"index\": str(i + 2),\n                \"category1\": category1,\n                \"category2\": category2,\n                \"category3\": category3,\n                \"user\": question,\n                \"file_paths\": files,\n                \"attachments\": [],\n            }\n        )\n        if isinstance(attach, str):\n            data[-1][\"attachments\"].append(attach)\njson.dump(data, open(\"./data/meta/task.json\", \"w\"), ensure_ascii=False, indent=4)\n"}
{"type": "source_file", "path": "llms/exceptions.py", "content": "# ADC LLMs Exceptions\nclass LLMError(Exception):\n    \"\"\"Base exception class for my_package errors.\"\"\"\n\n    ...\n\n\nclass FunctionCallDeprecationError(LLMError):\n    \"\"\"Raised when OpenaiAPI returns a message including 'function_call' field.\"\"\"\n\n    ...\n\n\nclass APIKeyPoolEmptyError(Exception):\n    \"\"\"Raised when client pool is empty.\"\"\"\n\n    ...\n"}
{"type": "source_file", "path": "llms/api_key_pool.py", "content": "\"\"\"# Hyper-parameters set according to OpenAI's site:\nhttps://platform.openai.com/docs/guides/rate-limits\n\n## OpenAI defines 5 rate limits type.\nRPM: Rate per minute\nTPM: Tokens per minute\nRPD: Rate per day\nTPD: Tokens per day\nIPM: Image per minute (useless for us now)\n\n## Rate limits (Tier 1 as example)\nThis is a high level summary and there are per-model exceptions to these limits\n(e.g. some legacy models or models with larger context windows have different\nrate limits). To view the exact rate limits per model for your account, visit\nthe limits section of your account settings.\n\nMODEL\t                RPM\t    RPD\t    TPM\t        BATCH QUEUE LIMIT\ngpt-4-turbo\t            500\t    -\t    30,000\t    90,000\ngpt-4\t                500\t    10,000\t10,000\t    100,000\ngpt-3.5-turbo\t        3,500\t10,000\t60,000\t    200,000\ntext-embedding-3-large\t3,000\t-\t    1,000,000\t3,000,000\ntext-embedding-3-small\t3,000\t-\t    1,000,000\t3,000,000\ntext-embedding-ada-002\t3,000\t-\t    1,000,000\t3,000,000\n\n## Price:\nhttps://openai.com/api/pricing/\n|Model                  |Input                  |Output|\n|-|-|-|\n|gpt-4-turbo-2024-04-09 |US$10.00 / 1M tokens   |US$ 30.00 / 1M tokens|\n|gpt-4                  |US$30.00 / 1M tokens   |US$ 60.00 / 1M tokens|\n|gpt-4-32k              |US$60.00 / 1M tokens   |US$120.00 / 1M tokens|\n|gpt-3.5-turbo-0125     |US$ 0.50 / 1M tokens   |US$  1.50 / 1M tokens|\n|gpt-3.5-turbo-instruct |US$ 1.50 / 1M tokens   |US$  2.00 / 1M tokens|\n\nNOTE: The rate limits are subject to change at any time.\nThis module provides a thread-safe singleton class `ClientPool` that\nmanages a pool of OpenAI API clients. It is designed to be used as a\ncontext manager to ensure that clients are properly released back to the\npool when they are no longer in use.\n\nThe `ClientPool` class is a singleton, meaning that only one instance of it\ncan exist at any given time. This is achieved by using the `ThreadSafeSingleton`\nmetaclass, which ensures that only one instance of the class is created\nregardless of the number of threads.\n\nExample:\n    get_from_pool = ClientPool([\n        {\"key\": \"YOUR_API_KEY_1\", \"url\": \"https://api.openai.com/v1\"},\n        {\"key\": \"YOUR_API_KEY_2\", \"url\": \"https://api.openai.com/v1\"},\n        {\"key\": \"YOUR_API_KEY_3\", \"url\": \"https://api.openai.com/v1\"},\n    ])\n\n    with get_from_pool as client_args:\n        client = OpenAI(**client_args)\n        completion = client.chat.completions.create(**model_args)\n        return completion.choices[0].message\n\"\"\"\n\nimport random\nimport time\nfrom abc import abstractmethod\nfrom heapq import heapify, heappop, heappush\nfrom typing import Dict, List, NewType, Optional\n\nfrom loguru import logger\nfrom openai import RateLimitError\n\nfrom llms.exceptions import APIKeyPoolEmptyError\n\nA_DAY_IN_SECONDS = 24 * 60 * 60\nA_MINITE_IN_SECONDS = 60\n\nRPM_INTERVAL = 500\nTPM_QUOTA = 30_000\n\nINPUT_PRICE_PER_TOKEN = 10\nOUTPUT_PRICE_PER_TOKEN = 30\n\nClientArgs = NewType(\"ClientArgs\", Dict[str, str | List[str]])\n\n\nclass Resource:\n    def __init__(\n        self,\n        api_key: str,\n        base_url: str,\n        organization: Optional[str] = None,\n        project: Optional[str] = None,\n        max_retries: int = 2,\n        timeout: Optional[float] = None,\n    ) -> None:\n        # # client args\n        self.client_args: ClientArgs = {\n            \"api_key\": api_key,\n            \"base_url\": self._enforce_trailing_slash(base_url),\n            \"organization\": organization,\n            \"project\": project,\n            \"max_retries\": max_retries,\n            \"timeout\": timeout,\n        }\n\n        # priority items\n        self._last_used_time: float = time.time()\n        self._request_count: int = 0\n        self._prompt_tokens: int = 0\n        self._completion_tokens: int = 0\n        self._total_tokens: int = 0\n        # self._client = OpenAI(api_key=self.api_key, base_url=self.base_url)\n\n    def __lt__(self, other: \"Resource\") -> bool:\n        \"\"\"Compare two resources based on request rate, token usage and price\n        Args:\n            other (Resource): The other resource to compare with\n        Returns:\n            bool: True if this resource has a lower request rate and token usage\n            than the other resource, False otherwise\n        \"\"\"\n        if abs(self._last_used_time - other._last_used_time) > A_MINITE_IN_SECONDS:\n            return self._last_used_time < other._last_used_time\n        elif abs(self._request_count - other._request_count) > RPM_INTERVAL:\n            return self._request_count < other._request_count\n        else:\n            return (\n                self._prompt_tokens * INPUT_PRICE_PER_TOKEN\n                + self._completion_tokens * OUTPUT_PRICE_PER_TOKEN\n            ) < (\n                other._prompt_tokens * INPUT_PRICE_PER_TOKEN\n                + other._completion_tokens * OUTPUT_PRICE_PER_TOKEN\n            )\n\n    def __gt__(self, other: \"Resource\") -> bool:\n        \"\"\"Compare two resources based on request rate and token usage\n        Args:\n            other (Resource): The other resource to compare with\n        Returns:\n            bool: True if this resource has a higher request rate and token usage\n            than the other resource, False otherwise\n        \"\"\"\n        return not self.__lt__(other)\n\n    def _enforce_trailing_slash(self, url: str) -> str:\n        if not url.endswith(\"/\"):\n            url += \"/\"\n        return url\n\n    def update(self, usage: Dict[str, int]) -> \"Resource\":\n        current_time = time.time()\n        assert self._last_used_time <= current_time\n        assert (\n            usage[\"prompt_tokens\"] + usage[\"completion_tokens\"] == usage[\"total_tokens\"]\n        )\n        if current_time - self._last_used_time > A_MINITE_IN_SECONDS:\n            self._request_count = 1\n            self._prompt_tokens = usage[\"prompt_tokens\"]\n            self._completion_tokens = usage[\"completion_tokens\"]\n        else:\n            self._request_count += 1\n            self._prompt_tokens += usage[\"prompt_tokens\"]\n            self._completion_tokens += usage[\"completion_tokens\"]\n\n        return self\n\n\nclass Pool:\n    def __init__(self, client_args: ClientArgs) -> None:\n        assert \"api_keys\" in client_args or \"api_key\" in client_args\n        assert not (\"api_keys\" in client_args and \"api_key\" in client_args)\n\n        if \"api_key\" in client_args:\n            self.queue: List[Resource] = [Resource(**client_args)]\n        elif \"api_keys\" in client_args:\n            self.queue: List[Resource] = [\n                Resource(\n                    api_key=api_key,\n                    base_url=client_args[\"base_url\"],\n                    organization=client_args.get(\"organization\", None),\n                    project=client_args.get(\"project\", None),\n                    max_retries=client_args.get(\"max_retries\", 2),\n                    timeout=client_args.get(\"timeout\", None),\n                )\n                for api_key in client_args[\"api_keys\"]\n            ]\n\n        self._index: int = 0\n\n    def __len__(self) -> int:\n        return len(self.queue)\n\n    @abstractmethod\n    def __enter__(self, **kwargs) -> ClientArgs:\n        \"\"\"Get a client from the pool and return it as a context manager.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __exit__(self, exc_type, exc_value, exc_traceback) -> bool:\n        \"\"\"Put the resource back in the pool.\"\"\"\n        raise NotImplementedError\n\n    def _remove_cur_arg(self) -> None:\n        logger.info(f\"removing api key {self.queue[self._index]}\")\n        _ = self.client_args.pop(self._index)\n        if len(self) == 0:\n            raise APIKeyPoolEmptyError(\"api key pool is empty\")\n\n\nclass PQPool(Pool):\n    def __init__(self, client_args: ClientArgs) -> None:\n        super().__init__(client_args)  # unlimited size\n\n        # Initialize the heap with resources\n        heapify(self.queue)\n\n    def __enter__(self, **kwargs) -> ClientArgs:\n        \"\"\"Get a client from the pool and return it as a context manager.\"\"\"\n\n        if len(self) == 0:\n            raise APIKeyPoolEmptyError(\"api key pool is empty\")\n\n        self._cur_resource = heappop(self.queue)\n\n        return self._cur_resource.client_args\n\n    def __exit__(self, exc_type, exc_value, exc_traceback) -> bool:\n        \"\"\"Put the resource back in the pool.\"\"\"\n\n        # TODO how to update the usage?\n        self._cur_usage = {\n            \"prompt_tokens\": 0,\n            \"completion_tokens\": 0,\n            \"total_tokens\": 0,\n        }\n        heappush(self.queue, self._cur_resource.update(self._cur_usage))\n        if exc_type is not None:\n            # Log the exception or handle it as needed\n            logger.error(f\"Client Pool Error: {exc_type}, {exc_value}\")\n            return False\n        return True\n\n\nclass RandomPool(Pool):\n    def __init__(self, client_args: ClientArgs) -> None:\n        super().__init__(client_args)\n        logger.info(f\"initializing RandomPool with {len(client_args)} api keys\")\n        logger.info(f\"client args: {client_args}\")\n\n    def __enter__(self, *args, **kwargs) -> ClientArgs:\n        \"\"\"Get a client from the pool and return it as a context manager.\"\"\"\n\n        if len(self) == 0:\n            raise APIKeyPoolEmptyError(\"api key pool is empty\")\n\n        self._index = random.randint(0, len(self) - 1)\n        return self.queue[self._index].client_args\n\n    def __exit__(self, exc_type, exc_value, exc_traceback) -> bool:\n        \"\"\"Put the resource back in the pool.\"\"\"\n\n        if exc_value is not None:\n            if (\n                exc_type is RateLimitError\n                and exc_value.status_code == 429\n                and judge_quota_exceeded(exc_value.message)\n            ):\n                self._remove_cur_arg()\n            return False\n        return True\n\n\n# class SpinPool(metaclass=Singleton):\nclass SpinPool(Pool):\n    def __init__(self, client_args: ClientArgs) -> None:\n        super().__init__(client_args)\n        random.shuffle(self.queue)\n\n    def __enter__(self, *arg, **kwargs) -> ClientArgs:\n        \"\"\"Get a client from the pool and return it as a context manager.\"\"\"\n\n        if len(self) == 0:\n            raise APIKeyPoolEmptyError(\"api key pool is empty\")\n\n        return self.queue[self._index].client_args\n\n    def __exit__(self, exc_type, exc_value, exc_traceback) -> bool:\n        \"\"\"Put the resource back in the pool.\"\"\"\n\n        if exc_value is None:\n            self._index = (self._index + 1) % len(self)\n            return True\n        else:\n            if (\n                exc_type is RateLimitError\n                and exc_value.status_code == 429\n                and judge_quota_exceeded(exc_value.message)\n            ):\n                self._remove_cur_arg()\n            else:\n                self._index = (self._index + 1) % len(self)\n            return False\n\n\ndef judge_quota_exceeded(message: str) -> bool:\n    return any(word in message.lower() for word in (\"quota\", \"bill\", \"é¢åº¦\", \"ä½™é¢\"))\n"}
{"type": "source_file", "path": "utils/LlamaCompletions.py", "content": "from openai import OpenAI\nfrom jinja2 import Environment, PackageLoader, select_autoescape\nfrom jinja2 import Template\n\nclass LlamaCompletion():\n    def __init__(self) -> None:\n        self.client=OpenAI(\n            base_url=\"http://localhost:8001/v1\",\n            api_key=\"token-abc123\",\n        )\n        \n        self.model =\"/data/zyl7353/models/codeinterpreter_0529-hf\"\n        \n        self.template=Template('''{% for message in messages %}{{'<|begin_of_text|>' + message['role'] + '\\n' + message['content']}}{% if (loop.last and add_generation_prompt) or not loop.last %}{{ '<|end_of_text|>' + '\\n'}}{% endif %}{% endfor %}\n{% if add_generation_prompt and messages[-1]['role'] != 'assistant' %}{{ '<|begin_of_text|>assistant\\n' }}{% endif %}''')\n       \n        \n        \n    def chat(self,messages):\n        message_string=self.format_message(messages)\n        \n        message_string=message_string.rsplit(\"<|execute_start|>\",1)[0]+\"<|execute_start|>\"\n        \n        #message_string+=\"\\n<|begin_of_text|>assistant\\n\"\n        print(message_string)\n        print(\"----=====================-----\\n\")\n        completion = self.client.completions.create(\n        model=self.model,\n        prompt=message_string,\n        max_tokens=8192,\n        echo=False,\n        stream=False,\n        temperature=0.2\n        )\n        ans=\"\"\n        for choice in completion.choices:\n            print(choice.text)\n            ans+=choice.text.strip()\n            \n        return ans\n    \n    def format_message(self,messages):\n        rendered = self.template.render(messages=messages, add_generation_prompt=True)\n        return rendered\n        \n\nif __name__==\"__main__\":\n    L=LlamaCompletion()\n    rsp=L.chat([{\"role\":\"system\",\"content\":\"you are a helpful assistant\"},{\"role\":\"user\",\"content\":\"Analyse the dataset\"},{\"role\":\"assistant\",\"content\":\"Analyse: I'am going to write python code to load the dataset at ./data.csv.\\n<|execute_start|>\\n\"}])\n    print(rsp)\n\n"}
{"type": "source_file", "path": "utils/save_notebook.py", "content": "import json\n\n\ndef generate_notebook(cells):\n    new_cells = [\n        generate_notebook_one_cell(\n            cell[\"role\"],\n            text=cell.get(\"text\", None),\n            code=cell.get(\"code\", None),\n            result=cell.get(\"result\", None),\n        )\n        for cell in cells\n    ]\n    return new_cells\n\n\ndef generate_notebook_one_cell(role, text=None, code=None, result=None):\n    if text is not None and code is None and result is None:\n        # ä»…ä¼ å…¥æ–‡æœ¬å†…å®¹ï¼Œç”ŸæˆMarkdownå•å…ƒæ ¼\n        cell = {\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": f\"<|{role}|>\\n\\n{text}\"}\n    elif code is not None and result is not None:\n        # ä¼ å…¥æ–‡æœ¬ã€ä»£ç å’Œæ‰§è¡Œç»“æœï¼Œç”ŸæˆCodeå•å…ƒæ ¼\n        cell = {\n            \"cell_type\": \"code\",\n            \"execution_count\": None,\n            \"metadata\": {},\n            \"outputs\": [{\"name\": \"stdout\", \"text\": [result], \"output_type\": \"stream\"}],\n            \"source\": [code],\n        }\n    else:\n        raise ValueError(\"Invalid combination of parameters.\")\n\n    return cell\n\n\ndef save_as_ipynb(cells, filename):\n    notebook = {\n        \"cells\": cells,\n        \"metadata\": {\n            \"kernelspec\": {\n                \"display_name\": \"Python 3\",\n                \"language\": \"python\",\n                \"name\": \"python3\",\n            },\n            \"language_info\": {\n                \"codemirror_mode\": {\"name\": \"ipython\", \"version\": 3},\n                \"file_extension\": \".py\",\n                \"mimetype\": \"text/x-python\",\n                \"name\": \"python\",\n                \"nbconvert_exporter\": \"python\",\n                \"pygments_lexer\": \"ipython3\",\n                \"version\": \"3.7.11\",\n            },\n        },\n        \"nbformat\": 4,\n        \"nbformat_minor\": 5,\n    }\n\n    with open(filename, \"w\") as file:\n        json.dump(notebook, file, indent=2)\n\n    print(f\"Notebook saved as {filename}\")\n\n\nif __name__ == \"__main__\":\n    messasges = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a proficient Data Scientist who good at data analysis.\",\n        },\n        {\"role\": \"user\", \"content\": \"What is the result of 2^1000\"},\n        {\"role\": \"assistant\", \"content\": \"The result is:\"},\n    ]\n"}
{"type": "source_file", "path": "llms/openai_api.py", "content": "import traceback\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nimport openai\nfrom loguru import logger\nfrom openai.types.chat.chat_completion import (\n    ChatCompletionMessage as OpenAIMessage,\n)\nfrom tenacity import retry, stop_after_attempt, wait_random\n\nfrom llms.api_key_pool import SpinPool\nfrom llms.base_llm import BaseLLM, LLMConfig\nfrom llms.schema import DebugInfo, Message, Tool\nfrom llms.utils import message2dict, tool2dict\n\n\nclass OpenAIAPI(BaseLLM):\n    def __init__(self, config: LLMConfig) -> None:\n        super().__init__(config)\n        self.pool = SpinPool(config[\"client_args\"])\n\n    def get_client_args(self):\n        return self.pool\n\n    def generate(\n        self,\n        messages: List[Union[Message, Dict]],\n        tools: Optional[List[Union[Tool, Dict]]] = None,\n    ) -> Tuple[Message, DebugInfo]:\n        if len(messages) > 0 and isinstance(messages[0], Message):\n            kwargs = dict(\n                messages=[message2dict(msg) for msg in messages],\n            )\n        elif isinstance(messages[0], dict):\n            kwargs = dict(\n                messages=messages,\n            )\n        kwargs.update(self.config[\"model_args\"])\n        if tools is not None:\n            if len(tools) > 0 and isinstance(tools[0], Tool):\n                kwargs[\"tools\"] = [tool2dict(tool) for tool in tools]\n                kwargs[\"tool_choice\"] = \"auto\"\n            else:\n                kwargs[\"tools\"] = tools\n                kwargs[\"tool_choice\"] = \"auto\"\n\n        new_message, debug_info = self._post_request(kwargs)\n\n        new_message = Message(**new_message.model_dump())\n        return new_message, debug_info\n\n    @retry(stop=stop_after_attempt(10), wait=wait_random(min=5, max=10))\n    def _post_request(self, kwargs: Dict[str, Any]) -> Tuple[OpenAIMessage, DebugInfo]:\n        try:\n            with self.get_client_args() as client_args:\n                logger.debug(f\"getting client args: {client_args}\")\n                client = openai.OpenAI(\n                    api_key=client_args.get(\"api_key\", None),\n                    base_url=client_args.get(\"base_url\", None),\n                )\n                completion = client.chat.completions.create(**kwargs)\n                logger.debug(f\"{completion}\")\n        except openai.RateLimitError as e:\n            logger.warning(f\"OpenAI rate limit error: {e}\")\n            raise e\n        except openai.APIError as e:\n            logger.error(f\"OpenAI API error: {e}\")\n            raise e\n        except Exception as e:\n            logger.error(f\"Unexpected error: {e}, type{e}, args{e.args}\")\n            logger.error(f\"Traceback: {traceback.format_exc()}\")\n            raise e\n\n        new_message = completion.choices[0].message\n        return new_message, DebugInfo(\n            info={\"OpenAICompletion\": completion.model_dump()}\n        )\n"}
{"type": "source_file", "path": "llms/__init__.py", "content": "from llms.base_llm import BaseLLM\n\nfrom llms.openai_api import OpenAIAPI\nfrom typing import Dict, Any\n\n__all__ = [\"BaseLLM\", \"OpenAIAPI\"]\n\n\ndef build_llm(llm_type: str, config: Dict[str, Any]) -> BaseLLM:\n    \"\"\"build llm from config\n\n    Args:\n        llm_type (str): llm type, only support  openai\n        config (Dict[str, Any]): the dict of llm config\n\n    Raises:\n        ValueError: llm_type should be  openai\n\n    Returns:\n        BaseLLM:  OpenAIAPI\n    \"\"\"\n    \n    if llm_type == \"openai\":\n        llm = OpenAIAPI(config)\n    else:\n        raise ValueError(\"llm_type should be llmcenter or openai\")\n    return llm\n"}
{"type": "source_file", "path": "utils/output_parser.py", "content": "\"\"\"output parser for code interpreter\"\"\"\n\nimport ast\nfrom typing import Tuple\n\n\ndef parse_code_action(\n    output: str,\n    mode: str = \"prompt\",\n    code_start_token: str = \"```\\npython\\n\",\n    code_end_token: str = \"```\",\n    tool_call_token: str = \"<|tool_call|>\",\n) -> Tuple[str, str]:\n    \"\"\"parse output from code interpreter\n\n    Args:\n        output (str): the output from code interpreter\n        mode: the mode of the output, could be prompt, functioncall, assistant\n        code_start_token: the token code script starts with, only used in prompt mode\n        code_end_token: the token code script ends with, only used in prompt mode\n        tool_call_token: the token for tool call, only used in prompt mode\n\n    Returns:\n        Tuple[str, str]: reasoning and code action\n    \"\"\"\n    if mode == \"prompt\":\n        return extract_code(output, code_start_token, code_end_token)\n    elif mode == \"functioncall\":\n        rsp = fc2dict(output, tool_call_token)\n        if \"tool_calls\" in rsp and len(rsp[\"tool_calls\"]) > 0:\n            return rsp[\"content\"], rsp[\"tool_calls\"][0][\"arguments\"][\"code\"]\n        else:\n            return rsp[\"content\"], \"\"\n    elif mode == \"assistant\":\n        raise NotImplementedError(\"assistant mode is not implemented yet\")\n    else:\n        raise ValueError(f\"mode {mode} is not supported\")\n\n\ndef extract_code(\n    rsp: str, code_start_token: str = \"```\\npython\\n\", code_end_token: str = \"```\"\n) -> Tuple[str, str]:\n    \"\"\"extract code from assistant content\n\n    Args:\n        rsp (str): the response content from assistant\n        code_start_token (str, optional): the token code script starts with. Defaults to \"```\\npython\".\n        code_end_token (str, optional): the token code script ends with. Defaults to \"```\".\n\n    Returns:\n        Tuple[str, str]: reasoning and code action\n    \"\"\"\n    # TODO: implement the code extraction logic using different code_start_token and code_end_token\n    rsp = str(rsp)\n\n    start_index = rsp.find(code_start_token)\n    if start_index == -1:\n        return rsp, \"\"\n\n    start_index += len(code_start_token)\n    end_index = rsp.find(code_end_token, start_index)\n    if end_index == -1:\n        return rsp, \"\"\n\n    return rsp[:start_index].replace(code_start_token, \"\").strip(), rsp[\n        start_index:end_index\n    ].strip()\n\n\nimport ast\nimport re\nimport json\n\ndef convert_function_call_to_json(string):\n    try:\n        tool_calls = []\n        x = ast.parse(string)\n        for tool in x.body:\n            function_name = tool.value.func.id\n            function_args = {}\n            for keyword in tool.value.keywords:\n                function_args[keyword.arg] = ast.literal_eval(keyword.value)\n            this_one = {\"name\": function_name, \"arguments\": function_args}\n            tool_calls.append(this_one)\n        return tool_calls\n    except Exception:\n        return []\nimport json\n\ndef extract_code_from_arguments(arguments_str):\n    try:\n        arguments_dict = json.loads(arguments_str)\n        return arguments_dict.get(\"code\", \"\")\n    except json.JSONDecodeError:\n        return \"\"\n\ndef fc2dict(sequence: str, spliter=\"<|tool_call|>\"):\n    if spliter in sequence:\n        content, tool_call_string = sequence.split(spliter, 1)\n        try:\n            # æ‰¾åˆ°ç¬¬ä¸€ä¸ª { å’Œæœ€åä¸€ä¸ª }\n            start_idx = tool_call_string.find('{')\n            end_idx = tool_call_string.rfind('}')\n            if start_idx != -1 and end_idx != -1:\n                arguments_str = tool_call_string[start_idx:end_idx + 1]\n                print(\"Arg:\",arguments_str)\n                arguments_str=arguments_str.replace(\"\\n\",\"\\\\n\")\n                #code_content = extract_code_from_arguments(arguments_str)\n                tool_call_dict = {\n                    \"name\": \"execute_python\",\n                    \"arguments\": json.loads(arguments_str)\n                }\n                tool_calls = [tool_call_dict]\n            else:\n                tool_calls = []\n            return {\n                \"content\": content.strip(),\n                \"tool_calls\": tool_calls,\n                \"role\": \"assistant\",\n            }\n        except Exception as e:\n            print(f\"Error: {e}\")\n            return {\"content\": content.strip(), \"role\": \"assistant\"}\n    else:\n        return {\"content\": sequence.strip(), \"role\": \"assistant\"}\n\n# ç¤ºä¾‹ç”¨æ³•\nsequence = '''To fulfill your request, I will perform the following steps:\n\n1. Read the dataset from the provided path.\n2. Extract the necessary data for the radar chart.\n3. Create a radar chart using the extracted data.\n\nLet's start by reading the dataset.\n\nAction:\n\n\n<|tool_call|>execute_python({\"code\":\"import pandas as pd\\n\\n# Read the dataset\\ndata_path = './data/radar.csv'\\ndf = pd.read_csv(data_path)\\ndf.head()\"})\\n'''\nresult = fc2dict(sequence)\nprint(result)\n"}
{"type": "source_file", "path": "utils/draw_win_tie_lose.py", "content": "#!/usr/bin/env python\n# encoding: utf-8\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# åˆ›å»ºæ•°æ®æ¡†\ndata = {\n    'Model': ['GPT-4', 'GPT-3.5','Llama3-8b-instruct', 'CodeQwen7b-chat', \"CodeActAgent-Llama-2-7b\", \"CodeActAgent-Mistral-7b-v0.1\", \"OpenCodeInterpreter-DS-6.7b\", \"Qwen2-7B-Instruct\", \"Py-Llama3-v3\",\"Llama3-70b-Instruct\"],\n    'Win': [117, 71.5, 67, 77, 42, 44, 54, 85, 74,100],\n    'Tie': [3, 0, 1, 2, 2, 3, 1, 2, 3,1],\n    'Lose': [23, 71.5, 75, 64, 99, 96, 88, 56, 53,42]\n}\ndf = pd.DataFrame(data)\n\n# æŒ‰Loseçš„æ•°é‡é™åºæ’åˆ—\ndf.sort_values(by='Lose', inplace=True, ascending=False)\n\n# æ•°æ®å½’ä¸€åŒ–å¤„ç†\ndf[['Win', 'Tie', 'Lose']] = df[['Win', 'Tie', 'Lose']].div(df[['Win', 'Tie', 'Lose']].sum(axis=1), axis=0)\n\n# è®¾ç½®é¢œè‰²ï¼Œå¹¶å¢åŠ é€æ˜åº¦\ncolors = ['#1E90FF', 'lightgrey', '#FF4500']\n\n# åˆå§‹åŒ–ç»˜å›¾ï¼Œè°ƒæ•´figsizeä»¥å¢å¤§æ¡å½¢å›¾çš„å®½åº¦\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# ç»˜åˆ¶å †ç§¯æ¡å½¢å›¾\nbottom = [0] * len(df)\nfor i, col in enumerate(['Win', 'Tie', 'Lose']):\n    bars = ax.barh(df['Model'], df[col], left=bottom, color=colors[i], label=col)\n    bottom = bottom + df[col]\n    if col == 'Win':\n        for bar, value in zip(bars, df[col]):\n            # è®¡ç®—æ–‡æœ¬ä½ç½®\n            text_x = bar.get_x() + bar.get_width() / 2\n            ax.text(text_x, bar.get_y() + bar.get_height()/2, f\"{value:.1%}\", va='center', ha='center')\n\n# è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾\nax.set_title('Win-Tie-Lose Evaluation of Different Models against GPT-3.5')\nax.set_xlabel('Percentage')\nax.set_ylabel('Model')\n\n# è®¾ç½®Xè½´ä¸ºç™¾åˆ†æ¯”\nax.set_xlim(0, 1)\nax.set_xticklabels(['{:.0%}'.format(x) for x in ax.get_xticks()])\n\n# å»æ‰å›¾çš„è¾¹æ¡†\nfor spine in ax.spines.values():\n    spine.set_visible(False)\n\n# æ·»åŠ å›¾ä¾‹å¹¶æ”¾åœ¨å³è¾¹\nax.legend(title='Result', loc='center left', bbox_to_anchor=(1, 0.5))\n\n# æ˜¾ç¤ºå›¾å½¢\nplt.tight_layout()\nplt.savefig(\"./WLRate.png\")\n"}
{"type": "source_file", "path": "llms/utils.py", "content": "from typing import Dict, Optional\n\nfrom llms.schema import DataType, Message, Tool, ToolProperty\n\n\ndef property2dict(property_: ToolProperty) -> Dict:\n    \"\"\"æŠŠä¼ å…¥çš„ToolPropertyè½¬æ¢ä¸ºOpenAIæ¥å£éœ€è¦çš„dictæ ¼å¼\n\n    Args:\n        property (ToolProperty): ToolPropertyå¯¹è¯\n\n    Returns:\n        Dict: å‚æ•°çš„dictå½¢å¼æè¿°\n    \"\"\"\n    new_property = {\"type\": property_.type.value}\n    if property_.description is not None:\n        new_property[\"description\"] = property_.description\n    if property_.type == DataType.OBJECT:\n        new_property[\"properties\"] = {\n            key: property2dict(value) for key, value in property_.properties.items()\n        }\n        if property_.required is not None:\n            new_property[\"required\"] = property_.required\n    elif property_.type == DataType.ARRAY:\n        new_property[\"items\"] = property2dict(property_.items)\n    elif property_.enum is not None:\n        new_property[\"enum\"] = property_.enum\n    return new_property\n\n\ndef tool2dict(tool: Tool) -> Dict[str, str]:\n    parameters = property2dict(tool.function.parameters)\n    parameters[\"type\"] = DataType.OBJECT.name\n    if \"required\" not in parameters:\n        parameters[\"required\"] = []\n    new_tool = {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": tool.function.name,\n            \"description\": tool.function.description,\n            \"parameters\": parameters,\n        },\n    }\n    return new_tool\n\n\ndef message2dict(\n    message: Message,\n) -> Dict[str, Optional[str | list[Dict[str, str]]]]:\n    ret = dict(\n        role=message.role.value,\n        content=message.content,\n        tool_calls=[call.model_dump() for call in message.tool_calls]\n        if message.tool_calls is not None\n        else None,\n        tool_call_id=message.tool_call_id,\n    )\n    ret = {k: v for k, v in ret.items() if v is not None}\n    return ret\n"}
