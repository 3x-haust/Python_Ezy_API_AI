{"repo_info": {"repo_name": "BrahmaSumm-Community-Edition", "repo_owner": "balajivis", "repo_url": "https://github.com/balajivis/BrahmaSumm-Community-Edition"}}
{"type": "test_file", "path": "tests/test_chunking.py", "content": "import pytest\nfrom src.chunking.textchunking import ChunkManager  # Adjust import based on your project structure\n\n@pytest.fixture\ndef chunk_manager():\n    # Create an instance of ChunkManager with a sample configuration\n    return ChunkManager(config_path='config/config.yaml')\n\ndef test_preprocess_text(chunk_manager):\n    text = \"This is a test.\\n\\nThis is a second paragraph with non-ASCII: ñ\"\n    processed_text = chunk_manager.preprocess_text(text)\n    \n    # Ensure multiple newlines are handled, and non-ASCII characters are removed\n    assert \"\\n\\n\" in processed_text, \"Preprocessing should preserve paragraph breaks\"\n    assert \"ñ\" not in processed_text, \"Preprocessing should remove non-ASCII characters\"\n\ndef test_flexible_chunk(chunk_manager):\n    text = \"\"\"\n    **BrahmaSumm** is an advanced document summarization and visualization tool designed to streamline document management, knowledge base creation, and chatbot enhancement. By leveraging cutting-edge chunking and clustering techniques, BrahmaSumm reduces token usage sent to Large Language Models (LLMs) by up to 99%, while maintaining the quality of content. The tool provides intuitive document processing, stunning visualizations, and efficient querying across multiple formats.\n\n    ## Features (v0.1)\n    - **Multi-format support**: Summarize and visualize content from PDFs, YouTube videos, audio files, HTML, spreadsheets, and Google Drive folders.\n    - **Clustering-based summarization**: BrahmaSumm intelligently chunks and clusters documents, extracting key insights while preserving quality.\n    - **UMAP visualization**: View your documents in an intuitive, visual format that highlights clusters and relationships within the content.\n    - **Token reduction**: Reduce the token count sent to LLMs by up to 99% with BrahmaSumm's efficient clustering algorithms.\n    - **Extract Tables, Images, and Text**: Seamlessly extract and summarize data from tables, images, and text within documents.\n    - **Vectorization for querying**: Enable powerful document querying by vectorizing content for efficient search and retrieval.  \"\"\"\n    chunk_manager.flexible_chunk(text)\n\n    chunks = chunk_manager.get_chunks()\n    \n    # Assert there are multiple chunks and each chunk has reasonable word counts\n    assert len(chunks) > 1, \"Text should be chunked into multiple chunks\"\n    word_counts = chunk_manager.get_word_count_per_chunk()\n    \n    # Ensure each chunk has word count between 75% and 125% of the target\n    min_words = int(chunk_manager.target_words * (1 - chunk_manager.flexibility))\n    max_words = 2*int(chunk_manager.target_words * (1 + chunk_manager.flexibility))\n\n    for count in word_counts:\n        assert min_words <= count <= max_words, \"Each chunk should meet word count flexibility range\"\n\ndef test_get_word_count_per_chunk(chunk_manager):\n    text = \"This is a test chunk.\\n\\nThis is another test chunk.\"\n    chunk_manager.flexible_chunk(text)\n    \n    word_counts = chunk_manager.get_word_count_per_chunk()\n    \n    # Ensure the correct word count for each chunk is returned\n    assert all(isinstance(count, int) for count in word_counts), \"Word count should be an integer\"\n    assert sum(word_counts) > 0, \"Word count should be greater than 0\"\n\ndef test_get_total_chunks(chunk_manager):\n    text = \"This is a test chunk.\\n\\nThis is another test chunk.\"\n    chunk_manager.flexible_chunk(text)\n    \n    total_chunks = chunk_manager.get_total_chunks()\n    \n    # Ensure the total number of chunks is correct\n    assert total_chunks > 0, \"Total number of chunks should be greater than 0\"\n\ndef test_get_total_words(chunk_manager):\n    text = \"This is a test chunk with several words.\\n\\nThis is another test chunk with some more words.\"\n    chunk_manager.flexible_chunk(text)\n    \n    total_words = chunk_manager.get_total_words()\n    \n    # Ensure the total word count across all chunks is correct\n    assert total_words == len(text.split()), \"Total word count should match the word count of the input text\""}
{"type": "test_file", "path": "tests/test_models.py", "content": "import os\nimport pytest\nfrom src.models.models import ModelManager\nfrom unittest.mock import MagicMock, patch\n\n@pytest.fixture\ndef mock_config(tmp_path):\n    # Create a temporary configuration file for testing\n    config_content = \"\"\"\n    llm_model: \"llama-3.1-70b-versatile\"\n    embedding_model: \"mxbai-embed-large\"\n    \"\"\"\n    config_file = tmp_path / \"config.yaml\"\n    config_file.write_text(config_content)\n    return str(config_file)\n\n@pytest.fixture\ndef model_manager(mock_config):\n    # Initialize the ModelManager with a mock config\n    return ModelManager(mock_config)\n\n@patch(\"src.models.models.ChatGroq\")\ndef test_load_llm_groq(mock_llm_groq, model_manager):\n    # Mock the Groq LLM model load\n    mock_llm_groq.return_value = MagicMock()\n\n    llm_groq = model_manager.load_llm_groq()\n\n    assert llm_groq is not None, \"LLM Groq model should be loaded\"\n    mock_llm_groq.assert_called_once_with(\n        model_name=\"llama-3.1-70b-versatile\", api_key=os.getenv(\"GROQ_API_KEY\")\n    )\n\n@patch(\"src.models.models.OllamaEmbeddings\")\ndef test_load_embedding_model(mock_ollama_embeddings, model_manager):\n    # Mock the embedding model load\n    mock_ollama_embeddings.return_value = MagicMock()\n\n    embedding_model = model_manager.load_embedding_model()\n\n    assert embedding_model is not None, \"Embedding model should be loaded\"\n    mock_ollama_embeddings.assert_called_once_with(model=\"mxbai-embed-large\")\n\n@patch(\"src.models.models.ChatGroq\")\ndef test_count_tokens(mock_llm_groq, model_manager):\n    # Mock the LLM token counting method\n    mock_llm_groq.return_value.get_num_tokens.return_value = 5\n    model_manager.llm_groq = mock_llm_groq.return_value\n\n    token_count = model_manager.count_tokens(\"Hello world.\")\n    \n    assert token_count == 5, \"Token count should be 5\"\n    mock_llm_groq.return_value.get_num_tokens.assert_called_once_with(\"Hello world.\")"}
{"type": "test_file", "path": "tests/test_clustering.py", "content": "import pytest\nimport numpy as np\nfrom src.clustering.clustering import ClusterManager\nfrom src.models.models import ModelManager  # Import the actual ModelManager\n\n@pytest.fixture\ndef model_manager():\n    # Initialize the ModelManager with the config\n    config_path = 'config/config.yaml'\n    return ModelManager(config_path)\n\n@pytest.fixture\ndef cluster_manager(model_manager):\n    # Load the real embedding model\n    embedding_model = model_manager.load_embedding_model()\n\n    # Initialize the ClusterManager with the real embedding model\n    config_path = 'config/config.yaml'\n    return ClusterManager(embedding_model, config_path)\n\ndef test_initialization(cluster_manager):\n    # Test if ClusterManager initializes properly with the given config and embedding model\n    assert cluster_manager.embedding_model is not None, \"Embedding model should be initialized\"\n    assert isinstance(cluster_manager.config, dict), \"Config should be a dictionary\"\n    assert cluster_manager.vectors == [], \"Vectors should be initialized as an empty list\"\n\ndef test_embed_documents_with_progress(cluster_manager):\n    # Real document chunks to embed\n    chunks = [\"This is the first chunk.\", \"This is the second chunk.\", \"This is the third chunk.\"]\n\n    # Embed the documents\n    cluster_manager.embed_documents_with_progress(chunks)\n\n    # Ensure that vectors are populated and are NumPy arrays\n    assert len(cluster_manager.vectors) == 3, \"There should be 3 embedded vectors\"\n    assert all(isinstance(vec, list) for vec in cluster_manager.vectors), \"Each vector should be a list\"\n\ndef test_cluster_document(cluster_manager):\n    # Embed some actual text chunks to get real vectors\n    chunks = [\"This is the first chunk.\", \"This is the second chunk.\", \"This is the third chunk.\", \n              \"This is the fourth chunk.\", \"This is the fifth chunk.\"]\n    cluster_manager.embed_documents_with_progress(chunks)\n    \n    # Get the length of the embedding (list length) dynamically\n    embedding_dim = len(cluster_manager.vectors[0])  # Extract the embedding dimension from list length\n\n    # Perform clustering\n    labels, centers = cluster_manager.cluster_document(n_clusters=2)\n\n    # Ensure the correct number of clusters and centers are returned\n    assert len(labels) == len(chunks), \"There should be a label for each vector\"\n    \n    # Ensure that the shape of the cluster centers matches the embedding dimension\n    assert len(centers) == 2, \"There should be 2 cluster centers\"\n    assert all(len(center) == embedding_dim for center in centers), f\"Each cluster center should have {embedding_dim} dimensions\"\n\ndef test_find_n_closest_representatives(cluster_manager):\n    # Embed some actual text chunks to get real vectors\n    chunks = [\"This is the first chunk.\", \"This is the second chunk.\", \"This is the third chunk.\",\n              \"This is the fourth chunk.\", \"This is the fifth chunk.\"]\n    cluster_manager.embed_documents_with_progress(chunks)\n\n    # Perform clustering\n    cluster_manager.cluster_document(n_clusters=2)\n\n    # Find the closest representatives\n    representatives = cluster_manager.find_n_closest_representatives(n=2)\n\n    # Ensure the correct number of representatives are returned\n    assert len(representatives) == 2, \"There should be representatives for each of the 2 clusters\"\n    for cluster_label, closest_indices in representatives:\n        assert len(closest_indices) == 2, \"Each cluster should have 2 closest representatives\""}
{"type": "test_file", "path": "tests/test_summarizer.py", "content": "import pytest\nfrom src.summarize import Summarizer \n\n@pytest.fixture(scope=\"module\")\ndef summarizer():\n    # This will set up the summarizer once for all tests\n    config_path = 'config/config.yaml'\n    return Summarizer(config_path)\n\ndef test_find_suitable_theme(summarizer):\n    result = summarizer.find_suitable_theme(\"Who is John Galt!\")\n    assert isinstance(result, str)  # Check if result is a string\n    assert len(result) > 0  # Check if result is not empty\n\ndef test_summary(summarizer):\n    # Run the summarizer and store the summary for later tests\n    summary = summarizer('https://www.whitehouse.gov/state-of-the-union-2024/',\"web\")\n    assert isinstance(summary, str)\n    assert len(summary) > 0\n\ndef test_analysis(summarizer):\n    # This test depends on `test_summary` and expects a valid summary\n    chunk_words, total_chunks, total_words, total_tokens, tokens_sent_tokens = summarizer.get_analysis()\n    # Ensure all elements in chunk_words are integers\n    assert all(isinstance(word_count, int) for word_count in chunk_words), \"All elements in chunk_words should be integers\"\n    \n    # Ensure no element in chunk_words is zero (if that's expected)\n    assert all(word_count > 0 for word_count in chunk_words[:-1]), \"No word count should be zero except possibly the last element\"\n\n    # Check that the length of chunk_words is what you expect (optional)\n    assert len(chunk_words) > 0, \"chunk_words should not be empty\"\n\n    assert isinstance(total_chunks, int)\n    assert isinstance(total_words, int)\n    assert isinstance(total_tokens, int)\n    assert isinstance(tokens_sent_tokens, int)"}
{"type": "test_file", "path": "tests/test_report.py", "content": "import pytest\nimport os\nfrom src.outputs.report_generate import create_final_report\n\n@pytest.fixture\ndef realistic_data():\n    \"\"\"\n    Fixture to create a realistic data dictionary for the report.\n    \"\"\"\n    summary = \"\"\"<h1>State of the Union Address 2024: Key Points</h1>\n\n        <p>The President's State of the Union address highlighted various key points, including:</p>\n\n        <h2>Economy and Jobs</h2>\n\n        <ul>\n        <li>The President emphasized that the economy is \"the envy of the world\" with 15 million new jobs created in three years.</li>\n        <li>Unemployment is at a 50-year low, and a record 16 million Americans are starting small businesses.</li>\n        <li>Historic job growth and small-business growth for Black, Hispanic, and Asian Americans.</li>\n        <li>800,000 new manufacturing jobs in America and counting.</li>\n        </ul>\n\n        <h2>Education and Student Loans</h2>\n\n        <ul>\n        <li>The President wants to make college more affordable by increasing Pell Grants for working- and middle-class families.</li>\n        <li>Record investments in HBCUs and minority-serving institutions, including Hispanic institutions.</li>\n        <li>Fixed two student loan programs to reduce the burden of student debt for nearly 4 million Americans.</li>\n        <li>Wants to give public school teachers a raise.</li>\n        </ul>\n\n        <h2>Healthcare and Prescription Drugs</h2>\n\n        <ul>\n        <li>Americans pay more for prescription drugs than anywhere in the world, which the President wants to change.</li>\n        <li>Capped the cost of insulin at $35 a month for every American who needs it.</li>\n        <li>Gave Medicare the power to negotiate lower prices on prescription drugs, saving seniors and taxpayers money.</li>\n        <li>Protected and strengthened the Affordable Care Act, and wants to make tax credits for working families permanent.</li>\n        </ul>\n\n        <h2>Reproductive Freedom and Women's Rights</h2>\n\n        <ul>\n        <li>The President reiterated his support for reproductive freedom and the right to choose.</li>\n        <li>Wants to restore Roe v. Wade as the law of the land.</li>\n        <li>Guarantee the right to IVF nationwide.</li>\n        </ul>\n\n        <h2>Foreign Policy and Humanitarian Aid</h2>\n\n        <ul>\n        <li>The President emphasized the need for humanitarian assistance in Gaza.</li>\n        <li>Directed the U.S. military to lead an emergency mission to establish a temporary pier in the Mediterranean to receive large shipments of aid.</li>\n        <li>Called on Israel to allow more aid into Gaza and protect humanitarian workers.</li>\n        </ul>\n\n        <h2>Immigration and Border Security</h2>\n\n        <ul>\n        <li>The President wants to tackle the backlog of 2 million immigration cases.</li>\n        <li>Wants to hire 1,500 more security agents and officers, 100 more immigration judges, and 4,300 more asylum officers.</li>\n        <li>Proposed a bipartisan bill to bring order to the border and save lives.</li>\n        </ul>\n\n        <h2>Taxes and Fairness</h2>\n\n        <ul>\n        <li>The President wants to make the wealthy and big corporations pay their fair share.</li>\n        <li>Proposed raising the corporate minimum tax to at least 21%.</li>\n        <li>Wants to end tax breaks for Big Pharma, Big Oil, private jets, and massive executive pay.</li>\n        </ul>\n    \"\"\"\n    test_data = {\n        'summary': summary,\n        'chunk_words': [76, 79, 76, 75, 75, 75, 75, 90, 100, 75, 92, 124, 75, 76, 75, 98, 113, 87, 94, 90],\n        'total_chunks': 123,\n        'total_words': 10920,\n        'total_tokens': 16895,\n        'tokens_sent_tokens': 362,\n        'labels': [9, 9, 9, 9, 4, 4, 4, 2, 2, 2, 2, 1, 3, 6, 6, 2, 3, 7, 0, 7, 7, 7, 7, 7, 8, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n          2, 2, 1, 1, 1, 1, 1, 2, 2, 3, 6, 6, 6, 0, 6, 0, 6, 6, 6, 2, 0, 3, 3, 3, 3, 3, 0, 7, 0, 0, 0, 0, 0, 0, 7, 7, 7,\n          7, 7, 7, 3, 7, 7, 0, 0, 7, 8, 8, 8, 8, 8, 8, 2, 2, 1, 2, 2, 0, 8, 8, 8, 1, 8, 8, 5, 5, 5, 5, 5, 5, 5, 2, 2, 2,\n          3, 3, 2, 2, 6, 2, 2, 2, 2, 2, 9, 1],\n        'themes': {0: 'Education and job accessibility.', 1: \"Women's reproductive rights empowerment.\", \n                   2: 'Unity and national optimism.', 3: 'Affordability of prescription drugs.',\n                   4: 'Engagement metrics, unrelated content.', 5: 'Humanitarian aid in Gaza.', \n                   6: 'Economic revival through collaboration.', 7: 'Wealth inequality and taxation.', \n                   8: 'Immigration reform and security.', 9: 'US Government Administration'},\n        'umap_image_path': 'reports/umap_clusters.png'  # Assume the UMAP image already exists\n    }\n    return  test_data\n\ndef test_create_final_report(realistic_data):\n    \"\"\"\n    Test to check if the PDF report is generated successfully in a persistent location.\n    \"\"\"\n    # Step 1: Define the report path (set to a persistent location for manual inspection)\n    report_path = 'reports/test_final_report.pdf'\n    \n    # Step 2: Call the create_final_report function\n    create_final_report(realistic_data, report_path=report_path)\n    \n    # Step 3: Check if the PDF file was created\n    assert os.path.exists(report_path), \"The final report PDF should be generated.\"\n    \n    # Step 4: Open the generated PDF manually for visual inspection\n    print(f\"Generated PDF report saved at: {os.path.abspath(report_path)}\")\n\n# Run with pytest"}
{"type": "source_file", "path": "services/spread_sheet_insight_generator.py", "content": "import pandas as pd\nimport yaml\nfrom dotenv import load_dotenv\nfrom pandasai import SmartDataframe\nfrom langchain_groq.chat_models import ChatGroq\nfrom langchain_ollama import OllamaLLM \n\n# Load environment variables\nload_dotenv()\n\n# Function to load configuration from YAML file\ndef load_config(config_path=\"config\\config.yaml\"):\n    with open(config_path, 'r') as file:\n        config = yaml.safe_load(file)\n    return config\n\n# Load configuration\nconfig = load_config()\n\n# Function to select the LLM based on the config.yaml file\ndef initialize_llm(config):\n    if config['llm_provider'] == \"groq\":\n        print(f\"Using Groq LLM Model: {config['llm_model']}\")\n        return ChatGroq(model=config['llm_model'])\n    elif config['llm_provider'] == \"ollama\":\n        print(f\"Using Ollama LLM Model: {config['llm_model']}\")\n        return OllamaLLM(model=config['llm_model']) \n    else:\n        raise ValueError(\"Unsupported LLM provider specified in config.yaml.\")\n\n# Initialize the LLM based on configuration\nllm = initialize_llm(config)\n\n# Load the dataset\ntry:\n    df = pd.read_csv(\"samples\\Train.csv\")  # Specify correct path to the CSV file\nexcept FileNotFoundError:\n    raise Exception(\"CSV file not found. Please check the path to the dataset.\")\n\n# Function to generate key insights from the dataset\ndef generate_insights(dataframe):\n    # Initialize SmartDataframe with the selected LLM\n    sdf = SmartDataframe(dataframe, config={\"llm\": llm})\n\n    print(\"===== Data Insights Report =====\\n\")\n\n    # 1. Basic Information: Displays general info about the dataset\n    print(\"1. Basic Information:\\n\")\n    print(dataframe.info(), \"\\n\")\n\n    # 2. Missing Values: Summarizes the number of missing values in each column\n    print(\"2. Missing Values Summary:\\n\")\n    print(dataframe.isnull().sum(), \"\\n\")\n\n    # 3. Descriptive Statistics: Generates summary statistics for numeric columns\n    print(\"3. Descriptive Statistics (Numeric Columns):\\n\")\n    print(dataframe.describe(), \"\\n\")\n\n    # 4. Correlation Matrix (Plotting using LLM)\n    print(\"4. Correlation Matrix:\\n\")\n    correlation_matrix_prompt = \"Create a plot of the correlation matrix.\"\n    correlation_matrix = sdf.chat(correlation_matrix_prompt)\n    print(correlation_matrix)\n\n    # 5. Outlier Detection: Detects any outliers in the dataset using LLM\n    print(\"\\n5. Outlier Detection:\\n\")\n    outlier_prompt = \"Identify any outliers in the dataset.\"\n    outliers = sdf.chat(outlier_prompt)\n    print(outliers)\n\n    # 6. Value Counts for Categorical Columns: Displays the distribution of values for each categorical column\n    print(\"\\n6. Categorical Columns Distribution:\\n\")\n    categorical_cols = dataframe.select_dtypes(include=['object']).columns\n    if len(categorical_cols) > 0:\n        for col in categorical_cols:\n            print(f\"Value counts for {col}:\\n\", dataframe[col].value_counts(), \"\\n\")\n\n            # Create a plot of the distribution of values for the categorical column\n            plot_prompt = f\"Create a plot showing the distribution of {col}.\"\n            plot_output = sdf.chat(plot_prompt)\n            print(plot_output)\n    else:\n        print(\"No categorical columns found.\\n\")\n\n    # 7. Time-Series Trend Analysis: If 'date' column exists, plots trends over time\n    if 'date' in dataframe.columns:\n        print(\"\\n7. Time-Series Trend Analysis:\\n\")\n        trend_prompt = \"Plot the trends in the data over time.\"\n        trends = sdf.chat(trend_prompt)\n        print(trends)\n    else:\n        print(\"\\nNo date column detected for trend analysis.\\n\")\n\n# Call the function to generate insights\ngenerate_insights(df)\n"}
{"type": "source_file", "path": "src/chunking/audiochunking.py", "content": "from pydub import AudioSegment\n\ndef split_audio(audio_file_path, chunk_length_ms=30000):  # Default is 30 seconds\n    \"\"\"\n    Splits the audio file into chunks of specified length.\n    \n    :param audio_file_path: Path to the audio file.\n    :param chunk_length_ms: Length of each chunk in milliseconds.\n    :return: A list of audio chunks.\n    \"\"\"\n    audio = AudioSegment.from_file(audio_file_path)\n    chunks = [audio[i:i + chunk_length_ms] for i in range(0, len(audio), chunk_length_ms)]\n    return chunks"}
{"type": "source_file", "path": "src/__init__.py", "content": ""}
{"type": "source_file", "path": "docs/conf.py", "content": "\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath('./src')) \n\nproject = 'BrahmaSumm'\ncopyright = '2024, Balaji Viswanathan'\nauthor = 'Balaji Viswanathan'\nrelease = '0.1'\n\nextensions = [\n    'sphinx.ext.autodoc',  # Auto-generates documentation from docstrings\n    'sphinx.ext.napoleon'  # Supports NumPy and Google style docstrings\n]\n\ntemplates_path = ['_templates']\nexclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']\n\n# -- Options for HTML output -------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output\n\nhtml_theme = 'alabaster'\nhtml_static_path = ['_static']\n"}
{"type": "source_file", "path": "src/chunking/textchunking.py", "content": "import re\nimport yaml\nimport logging\n\n# Set up logger\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass ChunkManager:\n    \"\"\"\n    The ChunkManager class is responsible for dividing large text documents into chunks\n    based on word count, while preserving sentence and paragraph boundaries.\n    It ensures that chunks are created flexibly based on a target word count, \n    allowing for slight variations in chunk size.\n    \"\"\"\n\n    def __init__(self, config_path):\n        \"\"\"\n        Initializes ChunkManager by loading the configuration.\n\n        :param config_path: Path to the configuration file containing chunking parameters.\n        \"\"\"\n        logger.info(\"Initializing ChunkManager with configuration from %s\", config_path)\n        with open(config_path, 'r') as file:\n            self.config = yaml.safe_load(file)\n\n        self.flexibility = self.config.get('chunk_flexibility', 0.25)  # Default to 25% flexibility\n        self.target_words = self.config.get('target_words', 100)       # Default to 100 words per chunk\n        self.chunks = []\n        logger.info(\"ChunkManager initialized with target_words=%d and flexibility=%.2f\", self.target_words, self.flexibility)\n\n    def preprocess_text(self, text):\n        \"\"\"\n        Cleans the input text by replacing multiple newlines and removing non-ASCII characters.\n        \n        :param text: The raw text to be preprocessed.\n        :return: Cleaned text.\n        \"\"\"\n        logger.info(\"Preprocessing text: removing extra newlines and non-ASCII characters\")\n        text = re.sub(r'\\n+', '\\n\\n', text)\n        text = re.sub(r'\\t+', '\\t', text)\n        text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n        logger.debug(\"Preprocessed text: %s\", text[:100])  # Log a preview of the preprocessed text\n        return text\n\n    def flexible_chunk(self, text, target_words=None, flexibility=None):\n        \"\"\"\n        Divides the text into chunks based on a target word count and flexible chunk size.\n        Ensures that each chunk contains a minimum of 75% and a maximum of 125% of the target word count.\n\n        :param text: The preprocessed text to be chunked.\n        :param target_words: Optional; target word count per chunk. If not provided, the default is used.\n        :param flexibility: Optional; the percentage flexibility for chunk size. If not provided, the default is used.\n        \"\"\"\n        \n        if target_words is None:\n            target_words = self.target_words\n        if flexibility is None:\n            flexibility = self.flexibility\n            \n        # coutn words in text\n        text_word_count = len(text.split())        \n        if text_word_count < 100:\n            logger.warning(\"Text is too short to be chunked\")\n            target_words = 25\n            \n        elif text_word_count < 800:\n            target_words = 50\n            \n        print(\"Target words\",text_word_count, target_words, flexibility)\n\n        min_words = int(target_words * (1 - flexibility))  # Minimum words per chunk (75%)\n        max_words = int(target_words * (1 + flexibility))  # Maximum words per chunk (125%)\n\n        logger.info(\"Chunking text with target_words=%d, flexibility=%.2f, min_words=%d, max_words=%d\", \n                    target_words, flexibility, min_words, max_words)\n\n        paragraphs = re.split(r'\\n\\n', text)  # Split by paragraphs\n        chunks = []\n        current_chunk = []\n        current_word_count = 0\n\n        def finalize_chunk(force=False):\n            \"\"\"Finalizes the current chunk if it meets the minimum word count or if forced.\"\"\"\n            if current_chunk and (force or current_word_count >= min_words):\n                logger.debug(\"Finalizing chunk with %d words\", current_word_count)\n                chunks.append(' '.join(current_chunk))\n\n        def process_paragraph(paragraph):\n            \"\"\"Processes a paragraph, splitting it into sentences and adding to chunks.\"\"\"\n            nonlocal current_chunk, current_word_count\n            sentences = re.split(r'(?<=[.!?]) +', paragraph)  # Split by sentence\n            for sentence in sentences:\n                sentence_word_count = len(sentence.split())\n\n                # If adding the sentence keeps the chunk under the max limit, add it\n                if current_word_count + sentence_word_count <= max_words:\n                    current_chunk.append(sentence)\n                    current_word_count += sentence_word_count\n\n                # If the chunk is at or above the minimum, finalize it\n                if current_word_count >= min_words:\n                    finalize_chunk()\n                    current_chunk = []\n                    current_word_count = 0\n\n                # If adding the sentence exceeds the max, finalize and start a new chunk\n                elif current_word_count + sentence_word_count > max_words:\n                    finalize_chunk()\n                    current_chunk = [sentence]\n                    current_word_count = sentence_word_count\n\n        # Process each paragraph and chunk the text accordingly\n        for paragraph in paragraphs:\n            para_word_count = len(paragraph.split())\n            logger.debug(\"Processing paragraph with %d words\", para_word_count)\n\n            # If the paragraph itself is smaller than the max size, add it as a chunk\n            if para_word_count <= max_words:\n                current_chunk.append(paragraph)\n                current_word_count += para_word_count\n                if current_word_count >= min_words:\n                    finalize_chunk()\n                    current_chunk = []\n                    current_word_count = 0\n            else:\n                process_paragraph(paragraph)\n\n        # Finalize any remaining text as the last chunk\n        finalize_chunk(force=True)\n        self.chunks = chunks\n        logger.info(\"Chunking completed with %d chunks\", len(chunks))\n\n    def get_word_count_per_chunk(self):\n        \"\"\"\n        Returns the word count for each chunk.\n\n        :return: List of word counts per chunk.\n        \"\"\"\n        word_counts = [len(chunk.split()) for chunk in self.chunks]\n        logger.debug(\"Word counts per chunk: %s\", word_counts)\n        return word_counts\n\n    def get_total_chunks(self):\n        \"\"\"\n        Returns the total number of chunks created.\n\n        :return: Total number of chunks.\n        \"\"\"\n        total_chunks = len(self.chunks)\n        logger.info(\"Total chunks: %d\", total_chunks)\n        return total_chunks\n\n    def get_chunks(self):\n        \"\"\"\n        Returns the list of text chunks.\n\n        :return: List of chunks.\n        \"\"\"\n        return self.chunks\n\n    def get_total_words(self):\n        \"\"\"\n        Returns the total word count across all chunks.\n\n        :return: Total word count.\n        \"\"\"\n        total_words = sum(self.get_word_count_per_chunk())\n        logger.info(\"Total word count: %d\", total_words)\n        return total_words\n\n\nif __name__ == '__main__':\n    # Example usage of ChunkManager\n    text = \"\"\"\n    Lorem ipsum dolor sit amet, consectetur adipiscing elit. \n    Nulla euismod, nisl eget aliquam ultricies, nunc nisl ultricies \n    nunc, sit amet aliquam nisl nunc eget nisl.\n    \n    Lorem ipsum dolor sit amet, consectetur adipiscing elit. \n    Nulla euismod, nisl eget aliquam ultricies, nunc nisl ultricies \n    nunc, sit amet aliquam nisl nunc eget nisl.\n    \"\"\"\n    chunk_manager = ChunkManager('config/config.yaml')\n    \n    processed_text = chunk_manager.preprocess_text(text)\n    chunk_manager.flexible_chunk(processed_text)\n    \n    logger.info(\"Word count per chunk: %s\", chunk_manager.get_word_count_per_chunk())\n    logger.info(\"Total chunks: %d\", chunk_manager.get_total_chunks())\n    logger.info(\"Total words: %d\", chunk_manager.get_total_words())\n    logger.debug(\"Chunks: %s\", chunk_manager.get_chunks())"}
{"type": "source_file", "path": "src/doc_loaders/doc_loader.py", "content": "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader, DirectoryLoader\n# from .multimedia_loader import MultimediaLoader\nimport logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass DocumentLoader:\n    def __init__(self, source: str, type: str, filter: str = None):\n        \"\"\"\n        Initialize the DocumentLoader with a source.\n\n        Parameters\n        ----------\n        source : str\n            The path to a document file or a URL to a webpage.\n        \"\"\"\n        self.source = source\n        self.type = type\n        self.glob = filter\n        self.text = \"\"\n\n    def __call__(self) -> str:\n        self.load()\n        return self.text\n\n    def load(self) -> None:\n        \"\"\"\n        Load text content from the source. Detects if the source is a PDF or a webpage.\n\n        Raises\n        ------\n        ValueError\n            If the source is not a valid PDF file or a URL.\n        \"\"\"\n        if self.source.lower().endswith('.pdf'):\n            self._load_pdf()\n        elif self.source.lower().startswith('http'):\n            self._load_webpage()\n        elif self.type == \"directory\":\n            self._load_directory()\n        # else:\n            # loader = MultimediaLoader(self.source)\n            # loader()\n        \n       \n\n    def _load_pdf(self) -> None:\n        \"\"\"\n        Load text content from a PDF file.\n        \"\"\"\n        loader = PyPDFLoader(self.source)\n        docs = loader.load()\n        self.text = ''.join(doc.page_content for doc in docs)\n\n    def _load_webpage(self) -> None:\n        \"\"\"\n        Load text content from a webpage.\n        \"\"\"\n        loader = WebBaseLoader(self.source)\n        docs = loader.load()\n        self.text = docs[0].page_content if docs else \"\"\n    \n    def _load_directory(self) -> None:\n        loader = DirectoryLoader(self.source, self.glob)\n        docs = loader.load()\n        print(docs)\n\n\n\nif __name__ == \"__main__\":\n    # Load text from a PDF file\n    #pdf_loader = DocumentLoader(\"https://arxiv.org/pdf/2105.01697.pdf\",\"pdf\")\n    #print(pdf_loader())  \n\n    # Load text from a webpage\n    #web_loader = DocumentLoader(\"https://arxiv.org/abs/2105.01697\",\"web\")\n    #print(web_loader())\n    \n    dir_loader = DocumentLoader(\"/Users/balajiviswanathan/Invento/pathak\",\"directory\",\"**/*.xlsx\")\n    print(dir_loader())"}
{"type": "source_file", "path": "src/outputs/report_generate.py", "content": "import os\nfrom reportlab.pdfgen import canvas\nfrom reportlab.lib.styles import getSampleStyleSheet\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, PageBreak, Table, TableStyle\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.lib.units import inch\nfrom PIL import Image as PILImage\nfrom reportlab.lib import colors\n\ndef handle_li_tags(html_text, list_type=\"ul\"):\n    \"\"\"\n    Converts <li> HTML tags into ReportLab-compatible bullets or numbered lists.\n    \n    :param html_text: The HTML text containing <li> tags\n    :param list_type: 'ul' for unordered list (bullets), 'ol' for ordered list (numbers)\n    :return: Processed string with bullet points or numbering\n    \"\"\"\n    if list_type == \"ul\":\n        # Replace <li> tags with bullet points\n        html_text = html_text.replace(\"<li>\", '<br/>\\u2022 ').replace(\"</li>\", \"\")\n    elif list_type == \"ol\":\n        # Replace <li> tags with numbered list (this assumes items are numbered sequentially)\n        items = html_text.split(\"<li>\")\n        for i, item in enumerate(items):\n            if \"</li>\" in item:\n                items[i] = f\"<br/>{i}. \" + item.split(\"</li>\")[0]\n        html_text = \"\".join(items)\n\n    return html_text\n\ndef convert_html_to_reportlab_compatible(html_text):\n    # Replace HTML tags with ReportLab-compatible tags\n    html_text = html_text.replace(\"<h1>\", '<br /><br /><b><font size=\"15\">').replace(\"</h1>\", \"</font></b><br />\")\n    html_text = html_text.replace(\"<h2>\", '<br /><br /><b><font size=\"13\">').replace(\"</h2>\", \"</font></b><br />\")\n    html_text = html_text.replace(\"<p>\", \"<br />\").replace(\"</p>\", \"<br />\")  # Handle paragraphs\n    html_text = handle_li_tags(html_text, list_type=\"ul\")\n    html_text = html_text.replace(\"<ul>\", \"\").replace(\"</ul>\", \"\")\n    html_text = html_text.replace(\"<ol>\", \"\").replace(\"</ol>\", \"\")\n    return html_text\n\n\ndef create_final_report(data: dict, report_path='reports/final_report.pdf'):\n    \"\"\"\n    Generates a final report containing the analysis, summary, UMAP clusters, and themes.\n\n    :param data: Dictionary containing necessary values for the report\n    :param report_path: Path to save the final report PDF.\n    \"\"\"\n    # 1. Retrieve data from the dictionary\n    summary = convert_html_to_reportlab_compatible(data.get('summary', 'No summary available'))\n\n    chunk_words = data.get('chunk_words', [])\n    total_chunks = data.get('total_chunks', 0)\n    total_words = data.get('total_words', 0)\n    total_tokens = data.get('total_tokens', 0)\n    tokens_sent_tokens = data.get('tokens_sent_tokens', 0)\n    umap_image_path = data.get('umap_image_path', 'reports/umap_clusters.png')\n    labels = data.get('labels', [])\n    themes = data.get('themes', {})\n\n    # Convert chunk words to a comma-separated string\n    chunk_words_str = \", \".join(map(str, chunk_words))\n\n    # 2. Create PDF document\n    doc = SimpleDocTemplate(report_path, pagesize=letter)\n\n    # 3. Define styles\n    c = canvas.Canvas(report_path, pagesize=letter)\n    width, height = letter\n    styles = getSampleStyleSheet()\n    normal_style = styles['Normal']\n    title_style = styles['Title']\n\n    # 4. Build content for the PDF\n    content = []\n\n    # Title\n    content.append(Paragraph(\"Document Analysis Report\", title_style))\n    content.append(Spacer(1, 0.25 * inch))\n\n    # Summary of key numbers\n    content.append(Paragraph(f\"Total Chunks: {total_chunks}\", normal_style))\n    content.append(Paragraph(f\"Total Words: {total_words}\", normal_style))\n    content.append(Paragraph(f\"Total Tokens: {total_tokens}\", normal_style))\n    content.append(Paragraph(f\"Tokens Sent to LLM: {tokens_sent_tokens}\", normal_style))\n    content.append(Spacer(1, 0.25 * inch))\n\n    # Add the chunk words with wrapping\n    # Commented for now...\n    #chunk_words_para = Paragraph(f\"Words per chunk: {chunk_words_str}\", normal_style)\n    #content.append(chunk_words_para)\n    #content.append(Spacer(1, 0.25 * inch))\n\n    # Add the summary with line breaks respected\n    content.append(Paragraph(\"Summary:\", title_style))\n    summary_para = Paragraph(summary, normal_style)\n    content.append(summary_para)\n    content.append(PageBreak())\n\n    # 5. Create the topic visualization\n    clusters = {}\n    for chunk_idx, cluster_label in enumerate(labels):\n        if cluster_label not in clusters:\n            clusters[cluster_label] = []\n        clusters[cluster_label].append(f\"{chunk_idx + 1}\")\n\n    # Sort the clusters by cluster label and also sort the chunk numbers within each cluster\n    sorted_clusters = {cluster_label: sorted(chunks, key=lambda x: int(x)) for cluster_label, chunks in sorted(clusters.items())}\n\n    print(sorted_clusters)\n    # Create the data table for the PDF\n    data_table = [[\"Theme\", \"Chunks\"]]\n    \n    # Fill table with clusters, their themes, and chunk lists\n    for cluster_label, chunks in clusters.items():\n        theme = themes.get(cluster_label, f\"Cluster {cluster_label}\")\n        theme_paragraph = Paragraph(theme, styles['Normal'])\n        chunk_list = \", \".join(chunks)  # Concatenate all chunks into a single string\n        chunks_paragraph = Paragraph(chunk_list, styles['Normal'])\n        data_table.append([theme_paragraph, chunks_paragraph])\n        \n    #print(data_table)\n    # Generate the PDF report\n    doc = SimpleDocTemplate(report_path, pagesize=letter)\n\n    # Add a title to the PDF\n    title = Paragraph(\"Document Cluster Overview\", styles['Title'])\n    content.append(title)\n    \n    # Create the table\n    table = Table(data_table, colWidths=[2 * inch, 4 * inch])\n    \n    # Add some style to the table\n    table.setStyle(TableStyle([\n        ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n        ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n        ('FONTSIZE', (0, 0), (-1, -1), 12),\n        ('BACKGROUND', (0, 1), (-1, -1), colors.beige),\n        ('BOX', (0, 0), (-1, -1), 2, colors.black),\n        ('GRID', (0, 0), (-1, -1), 1, colors.black),\n    ]))\n    # Ensure text wrapping within each cell\n    table.wrapOn(c, width, height)\n    table.drawOn(c, 1 * inch, height - 6 * inch)  \n    \n    content.append(table)\n    content.append(PageBreak())\n    \n    # 6. Insert UMAP cluster image (if it exists)\n    if os.path.exists(umap_image_path):\n        content.append(Paragraph(\"Topic themes and clusters\", title_style))\n        content.append(Spacer(1, 0.2 * inch))\n        \n        # GEt the aspect ratio of the image\n        img = PILImage.open(umap_image_path)\n        aspect_ratio = img.width / img.height\n        img_width = width * 0.8\n        img_height = img_width / aspect_ratio\n        \n        print(f\"Image size: {img_width} x {img_height}\")\n\n        # Adjust image width and height to fit the page, keeping aspect ratio\n        umap_image = Image(umap_image_path, img_width, img_height )\n        content.append(umap_image)\n        content.append(Spacer(1, 0.5 * inch))\n        #content.append(PageBreak())\n\n    # 6. Build the document\n    doc.build(content)\n    print(f\"Report saved at {report_path}\")"}
{"type": "source_file", "path": "src/visualize/visualize.py", "content": "import umap\nimport matplotlib.pyplot as plt\n\nclass Visualizer:\n    def __init__(self, config_path):\n        pass\n    \n    def print_labels_in_grid(self,labels):\n        \"\"\"\n        Print the cluster labels in a grid format.\n        :param labels: The list of labels.\n        :param row_length: Number of labels to display per row.\n        \"\"\"\n        print(\"Visualizing the document by topic clusters\")\n        row_length=30\n        if len(labels) > 500:\n            row_length=50\n            \n        if len(labels) <= row_length:\n            print(labels)\n            return\n        \n        for i in range(0, len(labels), row_length):\n            # Slice the labels into rows and print them\n            print(labels[i:i+row_length])\n            \n\n    def plot_clusters_with_umap(self, vectors, themes, labels, n_neighbors=25, min_dist=0.001, spread=0.8, length=12, width=5, output_image='umap_clusters.png'):\n        \"\"\"\n        Plot clusters using UMAP and label them with their corresponding themes, then save to PNG.\n\n        :param vectors: The vector embeddings of the chunks\n        :param themes: A dictionary with cluster labels as keys and themes as values\n        :param labels: The cluster labels for each vector embedding\n        :param n_neighbors: UMAP parameter that controls the number of neighbors to consider\n        :param min_dist: UMAP parameter that controls how closely UMAP packs points together\n        :param spread: UMAP parameter to control how spread out the clusters are\n        :param length: The length of the plot figure\n        :param width: The width of the plot figure\n        :param output_image: Path to save the PNG file of the plot\n        \"\"\"\n        # Step 1: Apply UMAP to reduce the dimensionality of vectors\n        umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, spread=spread, random_state=42)\n        embedding = umap_model.fit_transform(vectors)  # This gives a 2D embedding\n\n        # Step 2: Prepare to plot the clusters\n        plt.figure(figsize=(length, width))\n\n        # Step 3: Plot each cluster with its corresponding theme label\n        unique_labels = list(set(labels))  # Get unique cluster labels\n\n        for cluster_label in unique_labels:\n            # Get the indices of vectors that belong to this cluster\n            cluster_indices = [i for i, lbl in enumerate(labels) if lbl == cluster_label]\n\n            # Get the 2D UMAP coordinates for this cluster\n            cluster_embedding = embedding[cluster_indices]\n\n            # Get the theme for this cluster, fall back to cluster number if missing\n            theme_key = f\"Cluster {cluster_label}\"\n            theme_label = themes.get(cluster_label, f\"Cluster {cluster_label}\")\n\n            # Debugging: Check if the theme matches the cluster\n            #print(f\"Plotting cluster {cluster_label}: Theme = {theme_label}\")\n            #print(f\"Cluster {cluster_label} theme exists: {theme_key in themes}\")\n\n            # Plot the cluster points with the correct theme label\n            plt.scatter(cluster_embedding[:, 0], cluster_embedding[:, 1], label=theme_label, s=50)\n\n        # Step 4: Add labels and title to the plot\n        plt.title('Clusters Visualized with UMAP', fontsize=12)\n        plt.legend(loc='best', title=\"Themes\")\n        plt.grid(True)\n\n        # Step 5: Save the plot to a PNG file\n        plt.savefig(output_image, format='png')\n\n        # Step 6: Show the plot (optional)\n        plt.show()"}
{"type": "source_file", "path": "src/summarize.py", "content": "import logging\nimport yaml\nimport json\nfrom models.models import ModelManager\nfrom chunking.textchunking import ChunkManager\nfrom doc_loaders.doc_loader import DocumentLoader\nfrom clustering.clustering import ClusterManager\nfrom visualize.visualize import Visualizer\nfrom outputs.report_generate import create_final_report\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass Summarizer:\n    def __init__(self, config_path):\n        \"\"\"\n        Initializes the Summarizer class with models, chunking manager, clustering, and visualization.\n        Loads the prompts and the necessary models as per the configuration.\n        \"\"\"\n        self.model_manager = ModelManager(config_path)\n        self.prompts = self.load_prompts()\n        # self.model_manager.load_llm()\n        self.model_manager.load_embedding_model()\n        self.chunk_manager = ChunkManager(config_path)\n        self.cluster_manager = ClusterManager(self.model_manager.embedding_model, config_path)\n        self.visualizer = Visualizer(config_path)\n\n    def load_prompts(self):\n        \"\"\"\n        Loads prompts from the specified YAML configuration file.\n        \n        :param config_path: Path to the YAML file containing prompts\n        :return: Loaded prompts dictionary\n        \"\"\"\n        with open('config/prompts.yaml', 'r') as file:\n            return yaml.safe_load(file)\n\n    def __call__(self, source: str, type: str) -> dict:\n        \"\"\"\n        Processes the input document through loading, chunking, clustering, and summarizing.\n        It returns a dictionary with all necessary data for report generation.\n\n        :param source: The source document (URL or file path)\n        :return: A dictionary containing the final summary, analysis, UMAP cluster details, and themes.\n        \"\"\"\n        # Step 1: Load the document\n        logger.info(\"Loading document...\")\n        doc_loader = DocumentLoader(source,type)\n        text = doc_loader()\n\n        # Step 2: Preprocess and chunk the document\n        logger.info(\"Chunking text...\")\n        self.processed_text = self.chunk_manager.preprocess_text(text)\n        self.chunk_manager.flexible_chunk(self.processed_text)\n        chunks = self.chunk_manager.get_chunks()\n\n        # Step 3: Embed the document and run clustering\n        logger.info(\"Embedding and clustering...\")\n        self.cluster_manager.embed_documents_with_progress(chunks)\n        labels, cluster_centers = self.cluster_manager.cluster_document()\n        logger.info(f\"Number of clusters: {len(cluster_centers)}\")\n\n        # Step 4: Find representatives and themes for each cluster\n        representatives = self.cluster_manager.find_n_closest_representatives()\n        logger.info(\"Finding themes for each cluster...\")\n        themes, cluster_content = self.find_themes_for_clusters_slow(chunks, representatives)\n        \n      \n        # Step 5: Generate UMAP visualization\n        logger.info(\"Creating the visualization...\")\n        #print(\"Labels:\", labels)\n        #print(\"Themes keys:\", themes.keys())\n        \n        self.visualizer.plot_clusters_with_umap(\n            self.cluster_manager.vectors, \n            themes, \n            labels, \n            n_neighbors=25, \n            min_dist=0.001, \n            spread=0.8, \n            length=12, \n            width=8, \n            output_image='reports/umap_clusters.png'\n        )\n\n        # Step 6: Generate the final summary using LLM\n        logger.info(\"Creating the final summary...\")\n        self.combined_content = \" \".join(cluster_content.values())\n        prompt = self.prompts['create_summary_prompt'].format(combined_content=self.combined_content)\n        \n        final_summary = self.model_manager.llm.invoke(prompt).content\n      \n        # Step 7: Perform analysis on the document\n        chunk_words, total_chunks, total_words, total_tokens, tokens_sent_tokens = self.get_analysis()\n\n        # Step 8: Populate the data dictionary\n        data = {\n            'summary': final_summary,\n            'labels': labels,\n            'chunk_words': chunk_words,\n            'total_chunks': total_chunks,\n            'total_words': total_words,\n            'total_tokens': total_tokens,\n            'tokens_sent_tokens': tokens_sent_tokens,\n            'themes': themes,\n            'umap_image_path': 'reports/umap_clusters.png'\n        }\n        \n        \n\n        return data\n\n    def get_analysis(self):\n        \"\"\"\n        Provides detailed analysis of the processed document, including chunk sizes, total tokens, and word counts.\n\n        :return: Tuple containing chunk words, total chunks, total words, total tokens, and tokens sent to LLM\n        \"\"\"\n        total_tokens = self.model_manager.count_tokens(self.processed_text)\n        chunk_words = self.chunk_manager.get_word_count_per_chunk()\n        total_chunks = self.chunk_manager.get_total_chunks()\n        total_words = self.chunk_manager.get_total_words()\n        tokens_sent_tokens = self.model_manager.count_tokens(self.combined_content)\n        \n        return chunk_words, total_chunks, total_words, total_tokens, tokens_sent_tokens\n\n    def find_suitable_theme(self, chunk_text):\n        \"\"\"\n        Uses LLM to extract the most relevant theme from the given chunk.\n\n        :param chunk_text: A chunk of text from the document\n        :return: The extracted theme for the given chunk\n        \"\"\"\n        prompt = self.prompts['find_suitable_theme_prompt'].format(chunk_text=chunk_text)\n        logger.info(\"Finding suitable theme for chunk: %s\", chunk_text)\n        return self.model_manager.llm.invoke(prompt).content\n\n    def find_themes_for_clusters_slow(self, chunks, representatives):\n        \"\"\"\n        Finds a suitable theme for each cluster and combines the chunks for each representative.\n\n        :param chunks: The chunked text from the document\n        :param representatives: The representative chunks closest to the cluster centers\n        :return: A dictionary of themes for each cluster and combined content for each cluster\n        \"\"\"\n        themes = {}\n        cluster_content = {}\n\n        for cluster_label, representative_indices in representatives:\n            first_representative_chunk = chunks[representative_indices[0]]\n            theme = self.find_suitable_theme(first_representative_chunk)\n            \n            themes[cluster_label] = theme  # Store the theme in the dictionary\n            logger.info(\"Found theme for cluster %s: %s\", cluster_label, theme)\n\n            # Combine chunks for this cluster\n            combined_chunks = \" \".join([chunks[index] for index in representative_indices])\n            cluster_content[cluster_label] = combined_chunks\n\n        print(themes)\n        return themes, cluster_content\n      \n    # TODO: Fix this -- current unused due to various issues in formatting\n    def find_themes_for_clusters(self, chunks, representatives):\n        \"\"\"\n        Finds suitable themes for all clusters in a single LLM call and combines the chunks for each representative.\n\n        :param chunks: The chunked text from the document\n        :param representatives: The representative chunks closest to the cluster centers\n        :return: A dictionary of themes for each cluster and combined content for each cluster\n        \"\"\"\n        # Step 1: Prepare the text chunks for the LLM prompt\n        clusters_data = {}\n        cluster_content = {}\n        themes = {}\n        \n        for cluster_label, representative_indices in representatives:\n            # Get the first representative chunk to represent the cluster\n            first_representative_chunk = chunks[representative_indices[0]]\n            \n            # For theme we just need the first representative, but the full list of chunks for the cluster\n            combined_chunks = \" \".join([chunks[index] for index in representative_indices])\n            cluster_content[cluster_label] = combined_chunks\n            \n            # Prepare the data for each cluster\n            clusters_data[cluster_label] = {\n                \"representative_text\": first_representative_chunk,\n                \"combined_text\": \" \".join([chunks[index] for index in representative_indices])\n            }\n\n        # Step 2: Build the LLM prompt\n        prompt = self.prompts['find_suitable_theme_prompt_multiple'].format(first_representative_chunk=first_representative_chunk)\n        \n        # Step 3: Call the LLM once for all clusters\n        response = self.model_manager.llm.invoke(prompt).content\n\n        \n        print(response)\n\n        # Step 4: Process and clean the response\n        # LLM might return extra text alongside JSON, so let's clean it\n        start_idx = response.find(\"{\")  # Find the start of the JSON\n        end_idx = response.find(\"}\")  # Find the end of the JSON\n        if start_idx == -1 or end_idx == -1:\n            print(\"No valid JSON found in the LLM response\")\n            return {}, {}\n        \n        # Extract the JSON part of the response\n        json_response = response[start_idx:end_idx+1]\n\n        # Step 5: Parse the JSON response\n        try:\n            parsed_response = json.loads(json_response)\n        except json.JSONDecodeError as e:\n            print(f\"Failed to parse JSON: {e}\")\n            return {}, {}\n\n\n         # Step 6: Initialize dictionaries for themes and summaries\n        # Iterate over the parsed response and store the themes and summaries\n        for cluster_label, cluster_data in parsed_response.items():\n            theme = cluster_data.get(\"theme\", \"No theme available\")\n            themes[cluster_label] = theme\n\n        \n        print(themes)\n        return themes, cluster_content\n      \n\ndef main():\n    config_path = 'config/config.yaml'\n    summarizer = Summarizer(config_path)\n    print(summarizer.find_suitable_theme(\"Who is John Galt!\"))\n\n    data = summarizer('https://medium.com/@balajivis/whats-so-challenging-about-building-chatbots-drawing-lessons-from-the-trenches-1ca7343c6e3d',\"web\")\n    # data = summarizer('https://abc7.com/read-harris-trump-presidential-debate-transcript/15289001/','web')\n    # data = summarizer('https://www.whitehouse.gov/state-of-the-union-2024/',\"web\")\n    #data = summarizer('https://d18rn0p25nwr6d.cloudfront.net/CIK-0001921963/77018dae-bae9-4c33-8eaf-fa6685991719.pdf',\"pdf\")\n    \n    create_final_report(data,report_path='reports/final_report.pdf')\n    \n    print(data[\"summary\"])\n\n    chunk_words, total_chunks, total_words, total_tokens, tokens_sent_tokens = summarizer.get_analysis()\n    print(f\"Total chunks: {total_chunks}\\n Total words: {total_words}\\n Total tokens in original text: {total_tokens}\\n Total tokens sent to LLM: {tokens_sent_tokens}\")\n\nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "src/utils/__init__.py", "content": ""}
{"type": "source_file", "path": "src/topic_themes/themes.py", "content": ""}
{"type": "source_file", "path": "src/visualize/__init__.py", "content": ""}
{"type": "source_file", "path": "src/doc_loaders/multimedia_loader.py", "content": "# We will assume that the models are cached already\n\nimport os\nimport sys \nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))\nimport tempfile\nfrom tqdm import tqdm\nimport logging\nfrom src.chunking.audiochunking import split_audio\nimport fast_whisper\nfrom pytubefix import YouTube\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass MultimediaLoader:\n    def __init__(self, source: str):\n        \"\"\"\n        Initialize the DocumentLoader with a source.\n\n        Parameters\n        ----------\n        source : str\n            The path to a PDF file or a URL to a webpage.\n        \"\"\"\n        self.source = source\n        self.text = \"\"\n        self.model = whisper.load_model(\"tiny\")\n        \n    def __call__(self) -> str:\n        \"\"\"\n        Call the object to load the text from the source.\n\n        Returns\n        -------\n        str\n            The loaded text content.\n        \"\"\"\n        self.load()\n        return self.text\n\n    def load(self) -> None:\n        \"\"\"\n        Load text content from the source. Detects if the source is a PDF or a webpage.\n\n        Raises\n        ------\n        ValueError\n            If the source is not a valid PDF file or a URL.\n        \"\"\"\n        if self.source.lower().endswith('.mp3'):\n            logger.info(\"Loading audio...\")\n            self._load_audio(self.source)\n        elif self.source.startswith(\"https://www.youtube.com/watch?v=\"):\n            logger.info(\"Loading youtube...\")\n            self._load_yt()\n        else:\n            raise ValueError(\"Invalid source. Must be a PDF file or a URL.\")\n\n    def _load_yt(self) -> None:\n        \"\"\"\n        Load YouTube video content from the source.\n\n        Raises\n        ------\n        ValueError\n            If the source is not a valid YT URL.\n        \"\"\"\n        # check for valid yt url\n        if not self.source.startswith(\"https://www.youtube.com/watch?v=\"):\n            raise ValueError(\"Invalid YT URL. Must be a valid YouTube URL.\")\n        \n        logger.info(\"Loading YouTube video...\")\n        yt = YouTube(self.source)\n        audio = yt.streams.filter(only_audio=True).first()\n        \n        # Create a temporary directory\n        temp_dir = tempfile.gettempdir()\n        yt_audio_path = os.path.join(temp_dir, f\"yt_audio.wav\")\n        #store audio in wav format\n        audio.download(filename=yt_audio_path)\n        \n        logger.info(\"Sending the audio...\")\n        self._load_audio(yt_audio_path)\n        os.remove(yt_audio_path)\n        \n    def _load_audio(self,audio) -> None:\n        \"\"\"\n        Trascribe audio content from the source.\n        \"\"\"\n        logger.info(\"Splitting audio into chunks...\")\n        audio_chunks = split_audio(audio)\n        print(f\"Number of audio chunks: {len(audio_chunks)}\")\n        # Give a progress bar\n        for i, chunk in tqdm(enumerate(audio_chunks), desc=\"Transcribing audio chunks\", total=len(audio_chunks)):\n            wav_file = self.audio_chunk_to_wav(chunk, i)\n            \n            result = self.model.transcribe(wav_file)\n            #self.text += result[\"start\"] + \"-\" + result[\"end\"] + \":\" + result[\"text\"] + \"\\n\"\n            self.text += result[\"text\"]\n        \n            print(f\"Chunk {i+1}: {result['text']}\")\n        os.remove(wav_file)\n    \n    def audio_chunk_to_wav(self,chunk, chunk_index):\n        \"\"\"\n        Converts an audio chunk to a WAV file.\n\n        :param chunk: An AudioSegment chunk.\n        :param chunk_index: Index of the chunk to generate a unique file name.\n        :return: The path to the WAV file.\n        \"\"\"\n        # Create a temporary directory\n        temp_dir = tempfile.gettempdir()\n        wav_file_path = os.path.join(temp_dir, f\"chunk_{chunk_index}.wav\")\n        \n        # Export the chunk to a wav file\n        chunk.export(wav_file_path, format=\"wav\")\n        \n        return wav_file_path\n\nif __name__ == \"__main__\":\n    # We will be using a sample from librivox for the demo\n    # https://librivox.org/julius-caesar-by-william-shakespeare/\n    #loader = MultimediaLoader(\"samples/juliuscaesar_01_shakespeare_64kb.mp3\")\n    loader =  MultimediaLoader(\"https://www.youtube.com/watch?v=4Prc1UfuokY\")\n    text = loader()\n    print(text)\n\n"}
{"type": "source_file", "path": "src/clustering/clustering.py", "content": "import logging\nfrom tqdm import tqdm\nimport yaml\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\n# Set up logger\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass ClusterManager:\n    \"\"\"\n    The ClusterManager class is responsible for embedding document chunks\n    and clustering them to identify themes or groups. It uses KMeans clustering\n    and provides utilities for finding representative chunks close to the cluster centers.\n    \"\"\"\n\n    def __init__(self, embedding_model, config_path):\n        \"\"\"\n        Initializes the ClusterManager with an embedding model and configuration file.\n\n        :param embedding_model: Model used for embedding document chunks.\n        :param config_path: Path to the YAML configuration file.\n        \"\"\"\n        self.embedding_model = embedding_model\n        self.config = yaml.safe_load(open(config_path, 'r'))\n        self.vectors = []\n        logger.info(\"ClusterManager initialized with config from %s\", config_path)\n\n    def embed_documents_with_progress(self, chunks, batch_size=None):\n        \"\"\"\n        Embed document chunks with progress tracking, using batch processing.\n\n        :param chunks: List of document chunks to embed.\n        :param batch_size: Number of chunks to process in each batch. If None, uses the config value.\n        \"\"\"\n        if batch_size is None:\n            batch_size = self.config.get('embed_batch_size', 10)\n        \n        logger.info(\"Embedding %d document chunks in batches of %d\", len(chunks), batch_size)\n\n        # Embed chunks in batches with progress tracking\n        for i in tqdm(range(0, len(chunks), batch_size), desc=\"Embedding documents\"):\n            batch_chunks = chunks[i:i + batch_size]\n            batch_embeddings = self.embedding_model.embed_documents(batch_chunks)\n            self.vectors.extend(batch_embeddings)\n\n        logger.info(\"Completed embedding for %d chunks\", len(chunks))\n\n    def get_vectors(self):\n        \"\"\"\n        Returns the embedded vectors generated from document chunks.\n\n        :return: List of embedded vectors.\n        \"\"\"\n        return self.vectors\n\n    def cluster_document(self, n_clusters=None):\n        \"\"\"\n        Clusters the embedded document vectors using KMeans.\n\n        :param n_clusters: Number of clusters to form. If None, uses the config value.\n        :return: Tuple of cluster labels and cluster centers.\n        \"\"\"\n        if n_clusters is None:\n            n_clusters = self.config.get('n_clusters', 5)\n        \n        if n_clusters > len(self.vectors):\n            logger.warning(\"Requested %d clusters, but only %d vectors are available. Adjusting number of clusters.\", n_clusters, len(self.vectors))\n            n_clusters = len(self.vectors)\n\n        logger.info(\"Clustering %d vectors into %d clusters\", len(self.vectors), n_clusters)\n\n        self.kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=\"auto\")\n        self.labels = self.kmeans.fit_predict(self.vectors)\n\n        logger.info(\"Clustering completed. %d clusters formed.\", n_clusters)\n        return self.labels, self.kmeans.cluster_centers_\n\n    def find_n_closest_representatives(self, n=None):\n        \"\"\"\n        Finds the 'n' closest document chunks to each cluster center.\n\n        :param n: Number of closest chunks to return for each cluster. If None, uses the config value.\n        :return: List of tuples (cluster_label, indices of closest chunks for each cluster).\n        \"\"\"\n        cluster_centers = self.kmeans.cluster_centers_\n        \n        if n is None:\n            n = self.config.get('n_closest_representatives', 3)\n        \n        if n > len(cluster_centers):\n            logger.warning(\"Requested %d representatives, but only %d clusters available. Adjusting number of representatives.\", n, len(cluster_centers))\n            n = len(cluster_centers)\n\n        logger.info(\"Finding %d closest representatives for each of the %d clusters\", n, len(cluster_centers))\n\n        num_clusters = cluster_centers.shape[0]  # Number of clusters\n        representatives_with_labels = []\n\n        # Loop through each cluster center and find closest chunks\n        for i in range(num_clusters):\n            distances = np.linalg.norm(self.vectors - cluster_centers[i], axis=1)\n            closest_indices = np.argsort(distances)[:n]\n\n            logger.debug(\"Cluster %d: Closest %d chunks found\", i, n)\n            representatives_with_labels.append((i, closest_indices))\n\n        logger.info(\"Closest representatives found for all clusters.\")\n        return representatives_with_labels"}
{"type": "source_file", "path": "src/chunking/__init__.py", "content": ""}
{"type": "source_file", "path": "src/clustering/__init__.py", "content": ""}
{"type": "source_file", "path": "setup.py", "content": "from setuptools import setup, find_packages\n\n# Function to read requirements.txt\ndef parse_requirements(filename):\n    \"\"\"Load requirements from a requirements.txt file.\"\"\"\n    with open(filename, \"r\") as f:\n        return f.read().splitlines()\n\nsetup(\n    name=\"BrahmaSumm\",\n    version=\"0.1\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=parse_requirements(\"requirements.txt\")\n)"}
{"type": "source_file", "path": "src/doc_loaders/__init__.py", "content": ""}
{"type": "source_file", "path": "src/outputs/__init__.py", "content": ""}
{"type": "source_file", "path": "src/topic_themes/__init__.py", "content": ""}
{"type": "source_file", "path": "src/utils/utils.py", "content": ""}
{"type": "source_file", "path": "src/models/__init__.py", "content": ""}
{"type": "source_file", "path": "src/models/models.py", "content": "import os\nimport yaml\nimport logging\nfrom dotenv import load_dotenv\nfrom langchain_groq import ChatGroq\nfrom langchain_community.embeddings import OllamaEmbeddings\nfrom langchain_ollama import ChatOllama\nfrom langchain_openai import OpenAI\nfrom langchain_openai import AzureOpenAI\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass ModelManager:\n    def __init__(self, config_path):\n        \"\"\"\n        Initialize ModelManager by loading configuration and setting up models.\n        :param config_path: Path to the YAML configuration file.\n        \"\"\"\n        load_dotenv()\n        \n        try:\n            with open(config_path, 'r') as file:\n                self.config = yaml.safe_load(file)\n            logger.info(\"Configuration loaded successfully from %s\", config_path)\n        except Exception as e:\n            logger.error(\"Failed to load configuration: %s\", e)\n            raise\n\n        self.llm = None\n        self.embedding_model = None\n        \n        if self.config['llm_provider'] == 'groq':\n            self.load_llm_groq()\n            \n        if self.config['llm_provider'] == 'ollama':\n            self.load_llm_ollama()\n            \n        if self.config['llm_provider'] == 'openai':\n            self.load_llm_openai()\n        \n        \n    # def get_llm_response(self):\n    #     if self.config['llm_provider'] == 'groq':\n    #         return self.llm_groq.invoke(prompt).content\n    #     if self.config['llm_provider'] == 'ollama':\n    #         return self.llm_ollama(prompt)\n    #     if self.config['llm_provider'] == 'openai':\n    #         return self.llm_openai(prompt)\n\n    def load_llm_groq(self):\n        \"\"\"\n        Lazily loads the LLM Groq model based on the configuration if it hasn't been loaded yet.\n        :return: The loaded LLM Groq model.\n        \"\"\"\n        if not self.llm:\n            try:\n                logger.info(\"Loading Groq LLM model...\")\n                self.llm = ChatGroq(\n                    model_name=self.config['llm_model'],\n                    api_key=os.getenv(\"GROQ_API_KEY\")\n                )\n                logger.info(\"Groq LLM model loaded successfully.\")\n                \n            except KeyError as e:\n                logger.error(\"Missing required config key for LLM: %s\", e)\n                raise\n            except Exception as e:\n                logger.error(\"Error loading Groq LLM model: %s\", e)\n                raise\n    \n    def load_llm_openai(self):\n        \"\"\"\n        Loads the OpenAI model.\n        :return: The loaded OpenAI model.\n        \"\"\"\n        if not self.llm:\n            try:\n                logger.info(\"Loading OpenAI model...\")\n                # openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n                # os.environ[\"OPENAI_API_VERSION\"] = \"2023-12-01-preview\"\n                # os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\n                # os.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\n                print(\"AZURE_OPENAI_DEPLOYMENT:\", os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"))\n                load_dotenv()\n                # print(\"AZURE_OPENAI_DEPLOYMENT:\", os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"))\n                self.llm = AzureOpenAI(\n                    deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),  # The deployment name of the model\n                    )\n                logger.info(\"OpenAI model loaded successfully.\")\n                \n            except KeyError as e:\n                logger.error(\"Missing required config key for OpenAI LLM: %s\", e)\n                raise\n            except Exception as e:\n                logger.error(\"Error loading OpenAI LLM model: %s\", e)\n                raise\n            \n    def load_llm_ollama(self):\n        \"\"\"\n        Loads the Ollama LLM model.\n        :return: The loaded Ollama LLM model.\n        \"\"\"\n        if not self.llm:\n            try:\n                logger.info(\"Loading Ollama LLM model...\")\n                \n                self.llm = ChatOllama(\n                                model=self.config['llm_model'],\n                                temperature=0,\n                                )\n                logger.info(\"Ollama LLM model loaded successfully.\")\n                          \n            except KeyError as e:\n                logger.error(\"Missing required config key for Ollama LLM: %s\", e)\n                raise\n            except Exception as e:\n                logger.error(\"Error loading Ollama LLM model: %s\", e)\n                raise\n\n    def load_embedding_model(self):\n        \"\"\"\n        Lazily loads the Hugging Face embedding model based on the configuration if it hasn't been loaded yet.\n        :return: The loaded embedding model.\n        \"\"\"\n        if not self.embedding_model:\n            try:\n                logger.info(\"Loading embedding model...\")\n                self.embedding_model = OllamaEmbeddings(model=self.config['embedding_model'])\n                logger.info(\"Embedding model loaded successfully.\")\n            except KeyError as e:\n                logger.error(\"Missing required config key for embedding model: %s\", e)\n                raise\n            except Exception as e:\n                logger.error(\"Error loading embedding model: %s\", e)\n                raise\n        return self.embedding_model\n\n    def count_tokens(self, text):\n        \"\"\"\n        Counts the number of tokens in the given text using the Groq LLM.\n        :param text: The text to count tokens for.\n        :return: Number of tokens in the text.\n        \"\"\"\n        if not self.llm:\n            logger.warning(\"Groq LLM model is not loaded. Loading the model first...\")\n            self.load_llm_groq()\n        \n        try:\n            num_tokens = self.llm.get_num_tokens(text)\n            logger.info(\"Successfully counted %d tokens for the given text.\", num_tokens)\n            return num_tokens\n        except Exception as e:\n            logger.error(\"Error counting tokens: %s\", e)\n            raise\n\n\n# Main function for testing the ModelManager class\nif __name__ == '__main__':\n    try:\n        model_manager = ModelManager('config/config.yaml')\n        model_manager.load_embedding_model()\n\n        # print(\"Token count:\", model_manager.count_tokens(\"Hello world.\"))\n        print(\"Groq LLM Response:\", model_manager.llm.invoke(\"Hello world.\"))\n        print(\"Embedding Result:\", model_manager.embedding_model.embed_documents([\"Hello world.\"]))\n        # print(\"LLM content:\", model_manager.get_llm_response(\"Hello world.\"))\n    except Exception as e:\n        logger.error(\"An error occurred: %s\", e)"}
