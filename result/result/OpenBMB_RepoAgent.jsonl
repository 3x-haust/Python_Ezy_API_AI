{"repo_info": {"repo_name": "RepoAgent", "repo_owner": "OpenBMB", "repo_url": "https://github.com/OpenBMB/RepoAgent"}}
{"type": "test_file", "path": "tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/test_change_detector.py", "content": "import os\nimport unittest\n\nfrom git import Repo\n\nfrom repo_agent.change_detector import ChangeDetector\n\n\nclass TestChangeDetector(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        # 定义测试仓库的路径\n        cls.test_repo_path = os.path.join(os.path.dirname(__file__), 'test_repo')\n\n        # 如果测试仓库文件夹不存在，则创建它\n        if not os.path.exists(cls.test_repo_path):\n            os.makedirs(cls.test_repo_path)\n\n        # 初始化 Git 仓库\n        cls.repo = Repo.init(cls.test_repo_path)\n\n        # 配置 Git 用户信息\n        cls.repo.git.config('user.email', 'ci@example.com')\n        cls.repo.git.config('user.name', 'CI User')\n\n        # 创建一些测试文件\n        with open(os.path.join(cls.test_repo_path, 'test_file.py'), 'w') as f:\n            f.write('print(\"Hello, Python\")')\n        \n        with open(os.path.join(cls.test_repo_path, 'test_file.md'), 'w') as f:\n            f.write('# Hello, Markdown')\n\n        # 模拟 Git 操作：添加和提交文件\n        cls.repo.git.add(A=True)\n        cls.repo.git.commit('-m', 'Initial commit')\n\n    def test_get_staged_pys(self):\n        # 创建一个新的 Python 文件并暂存\n        new_py_file = os.path.join(self.test_repo_path, 'new_test_file.py')\n        with open(new_py_file, 'w') as f:\n            f.write('print(\"New Python File\")')\n        self.repo.git.add(new_py_file)\n\n        # 使用 ChangeDetector 检查暂存文件\n        change_detector = ChangeDetector(self.test_repo_path)\n        staged_files = change_detector.get_staged_pys()\n\n        # 断言新文件在暂存文件列表中\n        self.assertIn('new_test_file.py', [os.path.basename(path) for path in staged_files])\n\n        print(f\"\\ntest_get_staged_pys: Staged Python files: {staged_files}\")\n\n\n    def test_get_unstaged_mds(self):\n        # 修改一个 Markdown 文件但不暂存\n        md_file = os.path.join(self.test_repo_path, 'test_file.md')\n        with open(md_file, 'a') as f:\n            f.write('\\nAdditional Markdown content')\n\n        # 使用 ChangeDetector 获取未暂存的 Markdown 文件\n        change_detector = ChangeDetector(self.test_repo_path)\n        unstaged_files = change_detector.get_to_be_staged_files()\n\n        # 断言修改的文件在未暂存文件列表中\n        self.assertIn('test_file.md', [os.path.basename(path) for path in unstaged_files])\n\n        print(f\"\\ntest_get_unstaged_mds: Unstaged Markdown files: {unstaged_files}\")\n\n\n    def test_add_unstaged_mds(self):\n        # 确保有一个未暂存的 Markdown 文件\n        self.test_get_unstaged_mds()\n\n        # 使用 ChangeDetector 添加未暂存的 Markdown 文件\n        change_detector = ChangeDetector(self.test_repo_path)\n        change_detector.add_unstaged_files()\n\n        # 检查文件是否被暂存\n        unstaged_files_after_add = change_detector.get_to_be_staged_files()\n\n        # 断言暂存操作后没有未暂存的 Markdown 文件\n        self.assertEqual(len(unstaged_files_after_add), 0)\n\n        remaining_unstaged_files = len(unstaged_files_after_add)\n        print(f\"\\ntest_add_unstaged_mds: Number of remaining unstaged Markdown files after add: {remaining_unstaged_files}\")\n\n\n    @classmethod\n    def tearDownClass(cls):\n        # 清理测试仓库\n        cls.repo.close()\n        os.system('rm -rf ' + cls.test_repo_path)\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/test_json_handler.py", "content": "import unittest\nfrom unittest.mock import mock_open, patch\n\nfrom ..repo_agent.chat_with_repo.json_handler import (\n    JsonFileProcessor,  # Adjust the import according to your project structure\n)\n\n\nclass TestJsonFileProcessor(unittest.TestCase):\n\n    def setUp(self):\n        self.processor = JsonFileProcessor(\"test.json\")\n\n    @patch(\"builtins.open\", new_callable=mock_open, read_data='{\"files\": [{\"objects\": [{\"md_content\": \"content1\"}]}]}')\n    def test_read_json_file(self, mock_file):\n        # Test read_json_file method\n        data = self.processor.read_json_file()\n        self.assertEqual(data, {\"files\": [{\"objects\": [{\"md_content\": \"content1\"}]}]})\n        mock_file.assert_called_with(\"test.json\", \"r\", encoding=\"utf-8\")\n\n    @patch.object(JsonFileProcessor, 'read_json_file')\n    def test_extract_md_contents(self, mock_read_json):\n        # Test extract_md_contents method\n        mock_read_json.return_value = {\"files\": [{\"objects\": [{\"md_content\": \"content1\"}]}]}\n        md_contents = self.processor.extract_md_contents()\n        self.assertIn(\"content1\", md_contents)\n\n    @patch(\"builtins.open\", new_callable=mock_open, read_data='{\"name\": \"test\", \"files\": [{\"name\": \"file1\"}]}')\n    def test_search_in_json_nested(self, mock_file):\n        # Test search_in_json_nested method\n        result = self.processor.search_in_json_nested(\"test.json\", \"file1\")\n        self.assertEqual(result, {\"name\": \"file1\"})\n        mock_file.assert_called_with(\"test.json\", \"r\", encoding=\"utf-8\")\n\n    # Additional tests for error handling (FileNotFoundError, JSONDecodeError, etc.) can be added here\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/test_structure_tree.py", "content": "import os\nfrom collections import defaultdict\n\n\ndef build_path_tree(who_reference_me, reference_who, doc_item_path):\n    def tree():\n        return defaultdict(tree)\n    path_tree = tree()\n\n    for path_list in [who_reference_me, reference_who]:\n        for path in path_list:\n            parts = path.split(os.sep)\n            node = path_tree\n            for part in parts:\n                node = node[part]\n\n    # 处理 doc_item_path\n    parts = doc_item_path.split(os.sep)\n    parts[-1] = '✳️' + parts[-1]  # 在最后一个对象前面加上星号\n    node = path_tree\n    for part in parts:\n        node = node[part]\n\n    def tree_to_string(tree, indent=0):\n        s = ''\n        for key, value in sorted(tree.items()):\n            s += '    ' * indent + key + '\\n'\n            if isinstance(value, dict):\n                s += tree_to_string(value, indent + 1)\n        return s\n\n    return tree_to_string(path_tree)\n\n\nif \"__name__ == main\":\n    who_reference_me = [\n        \"repo_agent/file_handler.py/FileHandler/__init__\",\n        \"repo_agent/runner.py/need_to_generate\"\n    ]\n    reference_who = [\n        \"repo_agent/file_handler.py/FileHandler/__init__\",\n        \"repo_agent/runner.py/need_to_generate\",\n    ]\n\n    doc_item_path = 'tests/test_change_detector.py/TestChangeDetector'\n\n    result = build_path_tree(who_reference_me,reference_who,doc_item_path)\n    print(result)\n"}
{"type": "source_file", "path": "display/book_tools/generate_summary_from_book.py", "content": "import os\nimport re\nimport sys\n\n\ndef create_readme_if_not_exist(dire):\n    readme_path = os.path.join(dire, 'README.md')\n\n    if not os.path.exists(readme_path):\n        with open(readme_path, 'w') as readme_file:\n            dirname = os.path.basename(dire)\n            readme_file.write('# {}\\n'.format(dirname))\n\n\n# def output_markdown(dire, base_dir, output_file, iter_depth=0):\n#     for filename in os.listdir(dire):\n#         print('add readme ', filename)\n#         file_or_path = os.path.join(dire, filename)\n#         if os.path.isdir(file_or_path):\n#             create_readme_if_not_exist(file_or_path)\n#\n#     for filename in os.listdir(dire):\n#         print('deal with ', filename)\n#         file_or_path = os.path.join(dire, filename)\n#         if os.path.isdir(file_or_path):\n#             # create_readme_if_not_exist(file_or_path)\n#\n#             if markdown_file_in_dir(file_or_path):\n#                 output_file.write('  ' * iter_depth + '- ' + filename + '\\n')\n#                 output_markdown(file_or_path, base_dir, output_file,\n#                                 iter_depth + 1)\n#         else:\n#             if is_markdown_file(filename):\n#                 if (filename not in ['SUMMARY.md',\n#                                      'README.md']\n#                         or iter_depth != 0):\n#                     output_file.write('  ' * iter_depth +\n#                                       '- [{}]({})\\n'.format(is_markdown_file(filename),\n#                                                             os.path.join(os.path.relpath(dire, base_dir),\n#                                                                          filename)))\n\ndef output_markdown(dire, base_dir, output_file, iter_depth=0):\n    for filename in os.listdir(dire):\n        print('add readme ', filename)\n        file_or_path = os.path.join(dire, filename)\n        if os.path.isdir(file_or_path):\n            create_readme_if_not_exist(file_or_path)\n\n    for filename in os.listdir(dire):\n        print('deal with ', filename)\n        file_or_path = os.path.join(dire, filename)\n        if os.path.isdir(file_or_path):\n            # Check if README.md exists in the directory\n            readme_path = os.path.join(file_or_path, 'README.md')\n            if os.path.exists(readme_path):\n                # If README.md exists, create a markdown link to it\n                relative_path = os.path.join(os.path.relpath(file_or_path, base_dir), 'README.md')\n                output_file.write('  ' * iter_depth + '- [{}]({})\\n'.format(filename, relative_path))\n            # Recursively call output_markdown for nested directories\n            output_markdown(file_or_path, base_dir, output_file, iter_depth + 1)\n        else:\n            if is_markdown_file(filename):\n                if filename not in ['SUMMARY.md', 'README.md'] or iter_depth != 0 and filename not in ['README.md']:\n                    relative_path = os.path.join(os.path.relpath(dire, base_dir), filename)\n                    output_file.write('  ' * iter_depth + '- [{}]({})\\n'.format(is_markdown_file(filename), relative_path))\n\n\n\ndef markdown_file_in_dir(dire):\n    for root, dirs, files in os.walk(dire):\n        for filename in files:\n            if re.search('.md$|.markdown$', filename):\n                return True\n    return False\n\n\ndef is_markdown_file(filename):\n    match = re.search('.md$|.markdown$', filename)\n    if not match:\n        return False\n    elif len(match.group()) is len('.md'):\n        return filename[:-3]\n    elif len(match.group()) is len('.markdown'):\n        return filename[:-9]\n\n\ndef main():\n    book_name = sys.argv[1]\n\n    # mkdir the book folder\n    dir_input = os.path.join('./books', book_name, 'src')\n\n    # check the dst_dir\n    if not os.path.exists(dir_input):\n        print(dir_input)\n        os.makedirs(dir_input)\n    # Ensure the directory exists or create it\n    if not os.path.exists(dir_input):\n        os.makedirs(dir_input)\n\n    # Then proceed to create the file\n    output_path = os.path.join(dir_input, 'SUMMARY.md')\n    output = open(output_path, 'w')\n    # output = open(os.path.join(dir_input, 'SUMMARY.md'), 'w')\n    output.write('# Summary\\n\\n')\n    output_markdown(dir_input, dir_input, output)\n\n    print('GitBook auto summary finished:) ')\n    return 0\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "repo_agent/__init__.py", "content": ""}
{"type": "source_file", "path": "display/book_tools/generate_repoagent_books.py", "content": "import os\nimport shutil\nimport sys\n\n\ndef main():\n    markdown_docs_folder = sys.argv[1]\n    book_name = sys.argv[2]\n    repo_path = sys.argv[3]\n\n    # mkdir the book folder\n    dst_dir = os.path.join('./books', book_name, 'src')\n    docs_dir = os.path.join(repo_path, markdown_docs_folder)\n\n    # check the dst_dir\n    if not os.path.exists(dst_dir):\n        os.makedirs(dst_dir)\n        print(\"mkdir %s\" % dst_dir)\n\n    # cp the Markdown_Docs_folder to dst_dir\n    for item in os.listdir(docs_dir):\n        src_path = os.path.join(docs_dir, item)\n        dst_path = os.path.join(dst_dir, item)\n\n        # check the src_path\n        if os.path.isdir(src_path):\n            # if the src_path is a folder, use shutil.copytree to copy\n            shutil.copytree(src_path, dst_path)\n            print(\"copytree %s to %s\" % (src_path, dst_path))\n        else:\n            # if the src_path is a file, use shutil.copy2 to copy\n            shutil.copy2(src_path, dst_path)\n            print(\"copy2 %s to %s\" % (src_path, dst_path))\n\n    def create_book_readme_if_not_exist(dire):\n        readme_path = os.path.join(dire, 'README.md')\n\n        if not os.path.exists(readme_path):\n            with open(readme_path, 'w') as readme_file:\n                readme_file.write('# {}\\n'.format(book_name))\n\n    # create book README.md if not exist\n    create_book_readme_if_not_exist(dst_dir)\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "repo_agent/utils/gitignore_checker.py", "content": "import fnmatch\nimport os\n\n\nclass GitignoreChecker:\n    def __init__(self, directory: str, gitignore_path: str):\n        \"\"\"\n        Initialize the GitignoreChecker with a specific directory and the path to a .gitignore file.\n\n        Args:\n            directory (str): The directory to be checked.\n            gitignore_path (str): The path to the .gitignore file.\n        \"\"\"\n        self.directory = directory\n        self.gitignore_path = gitignore_path\n        self.folder_patterns, self.file_patterns = self._load_gitignore_patterns()\n\n    def _load_gitignore_patterns(self) -> tuple:\n        \"\"\"\n        Load and parse the .gitignore file, then split the patterns into folder and file patterns.\n\n        If the specified .gitignore file is not found, fall back to the default path.\n\n        Returns:\n            tuple: A tuple containing two lists - one for folder patterns and one for file patterns.\n        \"\"\"\n        try:\n            with open(self.gitignore_path, \"r\", encoding=\"utf-8\") as file:\n                gitignore_content = file.read()\n        except FileNotFoundError:\n            # Fallback to the default .gitignore path if the specified file is not found\n            default_path = os.path.join(\n                os.path.dirname(__file__), \"..\", \"..\", \".gitignore\"\n            )\n            with open(default_path, \"r\", encoding=\"utf-8\") as file:\n                gitignore_content = file.read()\n\n        patterns = self._parse_gitignore(gitignore_content)\n        return self._split_gitignore_patterns(patterns)\n\n    @staticmethod\n    def _parse_gitignore(gitignore_content: str) -> list:\n        \"\"\"\n        Parse the .gitignore content and return patterns as a list.\n\n        Args:\n            gitignore_content (str): The content of the .gitignore file.\n\n        Returns:\n            list: A list of patterns extracted from the .gitignore content.\n        \"\"\"\n        patterns = []\n        for line in gitignore_content.splitlines():\n            line = line.strip()\n            if line and not line.startswith(\"#\"):\n                patterns.append(line)\n        return patterns\n\n    @staticmethod\n    def _split_gitignore_patterns(gitignore_patterns: list) -> tuple:\n        \"\"\"\n        Split the .gitignore patterns into folder patterns and file patterns.\n\n        Args:\n            gitignore_patterns (list): A list of patterns from the .gitignore file.\n\n        Returns:\n            tuple: Two lists, one for folder patterns and one for file patterns.\n        \"\"\"\n        folder_patterns = []\n        file_patterns = []\n        for pattern in gitignore_patterns:\n            if pattern.endswith(\"/\"):\n                folder_patterns.append(pattern.rstrip(\"/\"))\n            else:\n                file_patterns.append(pattern)\n        return folder_patterns, file_patterns\n\n    @staticmethod\n    def _is_ignored(path: str, patterns: list, is_dir: bool = False) -> bool:\n        \"\"\"\n        Check if the given path matches any of the patterns.\n\n        Args:\n            path (str): The path to check.\n            patterns (list): A list of patterns to check against.\n            is_dir (bool): True if the path is a directory, False otherwise.\n\n        Returns:\n            bool: True if the path matches any pattern, False otherwise.\n        \"\"\"\n        for pattern in patterns:\n            if fnmatch.fnmatch(path, pattern):\n                return True\n            if is_dir and pattern.endswith(\"/\") and fnmatch.fnmatch(path, pattern[:-1]):\n                return True\n        return False\n\n    def check_files_and_folders(self) -> list:\n        \"\"\"\n        Check all files and folders in the given directory against the split gitignore patterns.\n        Return a list of files that are not ignored and have the '.py' extension.\n        The returned file paths are relative to the self.directory.\n\n        Returns:\n            list: A list of paths to files that are not ignored and have the '.py' extension.\n        \"\"\"\n        not_ignored_files = []\n        for root, dirs, files in os.walk(self.directory):\n            dirs[:] = [\n                d\n                for d in dirs\n                if not self._is_ignored(d, self.folder_patterns, is_dir=True)\n            ]\n\n            for file in files:\n                file_path = os.path.join(root, file)\n                relative_path = os.path.relpath(file_path, self.directory)\n                if not self._is_ignored(\n                    file, self.file_patterns\n                ) and file_path.endswith(\".py\"):\n                    not_ignored_files.append(relative_path)\n\n        return not_ignored_files\n\n\n# Example usage:\n# gitignore_checker = GitignoreChecker('path_to_directory', 'path_to_gitignore_file')\n# not_ignored_files = gitignore_checker.check_files_and_folders()\n# print(not_ignored_files)\n"}
{"type": "source_file", "path": "repo_agent/chat_with_repo/__init__.py", "content": "# repo_agent/chat_with_repo/__init__.py\n\nfrom .main import main\n"}
{"type": "source_file", "path": "repo_agent/file_handler.py", "content": "# FileHandler 类，实现对文件的读写操作，这里的文件包括markdown文件和python文件\n# repo_agent/file_handler.py\nimport ast\nimport json\nimport os\n\nimport git\nfrom colorama import Fore, Style\nfrom tqdm import tqdm\n\nfrom repo_agent.log import logger\nfrom repo_agent.settings import SettingsManager\nfrom repo_agent.utils.gitignore_checker import GitignoreChecker\nfrom repo_agent.utils.meta_info_utils import latest_verison_substring\n\n\nclass FileHandler:\n    \"\"\"\n    历变更后的文件的循环中，为每个变更后文件（也就是当前文件）创建一个实例\n    \"\"\"\n\n    def __init__(self, repo_path, file_path):\n        self.file_path = file_path  # 这里的file_path是相对于仓库根目录的路径\n        self.repo_path = repo_path\n\n        setting = SettingsManager.get_setting()\n\n        self.project_hierarchy = (\n            setting.project.target_repo / setting.project.hierarchy_name\n        )\n\n    def read_file(self):\n        \"\"\"\n        Read the file content\n\n        Returns:\n            str: The content of the current changed file\n        \"\"\"\n        abs_file_path = os.path.join(self.repo_path, self.file_path)\n\n        with open(abs_file_path, \"r\", encoding=\"utf-8\") as file:\n            content = file.read()\n        return content\n\n    def get_obj_code_info(\n        self, code_type, code_name, start_line, end_line, params, file_path=None\n    ):\n        \"\"\"\n        Get the code information for a given object.\n\n        Args:\n            code_type (str): The type of the code.\n            code_name (str): The name of the code.\n            start_line (int): The starting line number of the code.\n            end_line (int): The ending line number of the code.\n            parent (str): The parent of the code.\n            file_path (str, optional): The file path. Defaults to None.\n\n        Returns:\n            dict: A dictionary containing the code information.\n        \"\"\"\n\n        code_info = {}\n        code_info[\"type\"] = code_type\n        code_info[\"name\"] = code_name\n        code_info[\"md_content\"] = []\n        code_info[\"code_start_line\"] = start_line\n        code_info[\"code_end_line\"] = end_line\n        code_info[\"params\"] = params\n\n        with open(\n            os.path.join(\n                self.repo_path, file_path if file_path != None else self.file_path\n            ),\n            \"r\",\n            encoding=\"utf-8\",\n        ) as code_file:\n            lines = code_file.readlines()\n            code_content = \"\".join(lines[start_line - 1 : end_line])\n            # 获取对象名称在第一行代码中的位置\n            name_column = lines[start_line - 1].find(code_name)\n            # 判断代码中是否有return字样\n            if \"return\" in code_content:\n                have_return = True\n            else:\n                have_return = False\n\n            code_info[\"have_return\"] = have_return\n            # # 使用 json.dumps 来转义字符串，并去掉首尾的引号\n            # code_info['code_content'] = json.dumps(code_content)[1:-1]\n            code_info[\"code_content\"] = code_content\n            code_info[\"name_column\"] = name_column\n\n        return code_info\n\n    def write_file(self, file_path, content):\n        \"\"\"\n        Write content to a file.\n\n        Args:\n            file_path (str): The relative path of the file.\n            content (str): The content to be written to the file.\n        \"\"\"\n        # 确保file_path是相对路径\n        if file_path.startswith(\"/\"):\n            # 移除开头的 '/'\n            file_path = file_path[1:]\n\n        abs_file_path = os.path.join(self.repo_path, file_path)\n        os.makedirs(os.path.dirname(abs_file_path), exist_ok=True)\n        with open(abs_file_path, \"w\", encoding=\"utf-8\") as file:\n            file.write(content)\n\n    def get_modified_file_versions(self):\n        \"\"\"\n        Get the current and previous versions of the modified file.\n\n        Returns:\n            tuple: A tuple containing the current version and the previous version of the file.\n        \"\"\"\n        repo = git.Repo(self.repo_path)\n\n        # Read the file in the current working directory (current version)\n        current_version_path = os.path.join(self.repo_path, self.file_path)\n        with open(current_version_path, \"r\", encoding=\"utf-8\") as file:\n            current_version = file.read()\n\n        # Get the file version from the last commit (previous version)\n        commits = list(repo.iter_commits(paths=self.file_path, max_count=1))\n        previous_version = None\n        if commits:\n            commit = commits[0]\n            try:\n                previous_version = (\n                    (commit.tree / self.file_path).data_stream.read().decode(\"utf-8\")\n                )\n            except KeyError:\n                previous_version = None  # The file may be newly added and not present in previous commits\n\n        return current_version, previous_version\n\n    def get_end_lineno(self, node):\n        \"\"\"\n        Get the end line number of a given node.\n\n        Args:\n            node: The node for which to find the end line number.\n\n        Returns:\n            int: The end line number of the node. Returns -1 if the node does not have a line number.\n        \"\"\"\n        if not hasattr(node, \"lineno\"):\n            return -1  # 返回-1表示此节点没有行号\n\n        end_lineno = node.lineno\n        for child in ast.iter_child_nodes(node):\n            child_end = getattr(child, \"end_lineno\", None) or self.get_end_lineno(child)\n            if child_end > -1:  # 只更新当子节点有有效行号时\n                end_lineno = max(end_lineno, child_end)\n        return end_lineno\n\n    def add_parent_references(self, node, parent=None):\n        \"\"\"\n        Adds a parent reference to each node in the AST.\n\n        Args:\n            node: The current node in the AST.\n\n        Returns:\n            None\n        \"\"\"\n        for child in ast.iter_child_nodes(node):\n            child.parent = node\n            self.add_parent_references(child, node)\n\n    def get_functions_and_classes(self, code_content):\n        \"\"\"\n        Retrieves all functions, classes, their parameters (if any), and their hierarchical relationships.\n        Output Examples: [('FunctionDef', 'AI_give_params', 86, 95, None, ['param1', 'param2']), ('ClassDef', 'PipelineEngine', 97, 104, None, []), ('FunctionDef', 'get_all_pys', 99, 104, 'PipelineEngine', ['param1'])]\n        On the example above, PipelineEngine is the Father structure for get_all_pys.\n\n        Args:\n            code_content: The code content of the whole file to be parsed.\n\n        Returns:\n            A list of tuples containing the type of the node (FunctionDef, ClassDef, AsyncFunctionDef),\n            the name of the node, the starting line number, the ending line number, the name of the parent node, and a list of parameters (if any).\n        \"\"\"\n        tree = ast.parse(code_content)\n        self.add_parent_references(tree)\n        functions_and_classes = []\n        for node in ast.walk(tree):\n            if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):\n                # if node.name == \"recursive_check\":\n                #     import pdb; pdb.set_trace()\n                start_line = node.lineno\n                end_line = self.get_end_lineno(node)\n                # def get_recursive_parent_name(node):\n                #     now = node\n                #     while \"parent\" in dir(now):\n                #         if isinstance(now.parent, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):\n                #             assert 'name' in dir(now.parent)\n                #             return now.parent.name\n                #         now = now.parent\n                #     return None\n                # parent_name = get_recursive_parent_name(node)\n                parameters = (\n                    [arg.arg for arg in node.args.args] if \"args\" in dir(node) else []\n                )\n                all_names = [item[1] for item in functions_and_classes]\n                # (parent_name == None or parent_name in all_names) and\n                functions_and_classes.append(\n                    (type(node).__name__, node.name, start_line, end_line, parameters)\n                )\n        return functions_and_classes\n\n    def generate_file_structure(self, file_path):\n        \"\"\"\n        Generates the file structure for the given file path.\n\n        Args:\n            file_path (str): The relative path of the file.\n\n        Returns:\n            dict: A dictionary containing the file path and the generated file structure.\n\n        Output example:\n        {\n            \"function_name\": {\n                \"type\": \"function\",\n                \"start_line\": 10,\n                ··· ···\n                \"end_line\": 20,\n                \"parent\": \"class_name\"\n            },\n            \"class_name\": {\n                \"type\": \"class\",\n                \"start_line\": 5,\n                ··· ···\n                \"end_line\": 25,\n                \"parent\": None\n            }\n        }\n        \"\"\"\n        with open(os.path.join(self.repo_path, file_path), \"r\", encoding=\"utf-8\") as f:\n            content = f.read()\n            structures = self.get_functions_and_classes(content)\n            file_objects = []  # 以列表的形式存储\n            for struct in structures:\n                structure_type, name, start_line, end_line, params = struct\n                code_info = self.get_obj_code_info(\n                    structure_type, name, start_line, end_line, params, file_path\n                )\n                file_objects.append(code_info)\n\n        return file_objects\n\n    def generate_overall_structure(self, file_path_reflections, jump_files) -> dict:\n        \"\"\"获取目标仓库的文件情况，通过AST-walk获取所有对象等情况。\n        对于jump_files: 不会parse，当做不存在\n        \"\"\"\n        repo_structure = {}\n        gitignore_checker = GitignoreChecker(\n            directory=self.repo_path,\n            gitignore_path=os.path.join(self.repo_path, \".gitignore\"),\n        )\n\n        bar = tqdm(gitignore_checker.check_files_and_folders())\n        for not_ignored_files in bar:\n            normal_file_names = not_ignored_files\n            if not_ignored_files in jump_files:\n                print(\n                    f\"{Fore.LIGHTYELLOW_EX}[File-Handler] Unstaged AddFile, ignore this file: {Style.RESET_ALL}{normal_file_names}\"\n                )\n                continue\n            elif not_ignored_files.endswith(latest_verison_substring):\n                print(\n                    f\"{Fore.LIGHTYELLOW_EX}[File-Handler] Skip Latest Version, Using Git-Status Version]: {Style.RESET_ALL}{normal_file_names}\"\n                )\n                continue\n            # elif not_ignored_files.endswith(latest_version):\n            #     \"\"\"如果某文件被删除但没有暂存，文件系统有fake_file但没有对应的原始文件\"\"\"\n            #     for k,v in file_path_reflections.items():\n            #         if v == not_ignored_files and not os.path.exists(os.path.join(setting.project.target_repo, not_ignored_files)):\n            #             print(f\"{Fore.LIGHTYELLOW_EX}[Unstaged DeleteFile] load fake-file-content: {Style.RESET_ALL}{k}\")\n            #             normal_file_names = k #原来的名字\n            #             break\n            #     if normal_file_names == not_ignored_files:\n            #         continue\n\n            # if not_ignored_files in file_path_reflections.keys():\n            #     not_ignored_files = file_path_reflections[not_ignored_files] #获取fake_file_path\n            #     print(f\"{Fore.LIGHTYELLOW_EX}[Unstaged ChangeFile] load fake-file-content: {Style.RESET_ALL}{normal_file_names}\")\n\n            try:\n                repo_structure[normal_file_names] = self.generate_file_structure(\n                    not_ignored_files\n                )\n            except Exception as e:\n                logger.error(\n                    f\"Alert: An error occurred while generating file structure for {not_ignored_files}: {e}\"\n                )\n                continue\n            bar.set_description(f\"generating repo structure: {not_ignored_files}\")\n        return repo_structure\n\n    def convert_to_markdown_file(self, file_path=None):\n        \"\"\"\n        Converts the content of a file to markdown format.\n\n        Args:\n            file_path (str, optional): The relative path of the file to be converted. If not provided, the default file path, which is None, will be used.\n\n        Returns:\n            str: The content of the file in markdown format.\n\n        Raises:\n            ValueError: If no file object is found for the specified file path in project_hierarchy.json.\n        \"\"\"\n        with open(self.project_hierarchy, \"r\", encoding=\"utf-8\") as f:\n            json_data = json.load(f)\n\n        if file_path is None:\n            file_path = self.file_path\n\n        # Find the file object in json_data that matches file_path\n\n        file_dict = json_data.get(file_path)\n\n        if file_dict is None:\n            raise ValueError(\n                f\"No file object found for {self.file_path} in project_hierarchy.json\"\n            )\n\n        markdown = \"\"\n        parent_dict = {}\n        objects = sorted(file_dict.values(), key=lambda obj: obj[\"code_start_line\"])\n        for obj in objects:\n            if obj[\"parent\"] is not None:\n                parent_dict[obj[\"name\"]] = obj[\"parent\"]\n        current_parent = None\n        for obj in objects:\n            level = 1\n            parent = obj[\"parent\"]\n            while parent is not None:\n                level += 1\n                parent = parent_dict.get(parent)\n            if level == 1 and current_parent is not None:\n                markdown += \"***\\n\"\n            current_parent = obj[\"name\"]\n            params_str = \"\"\n            if obj[\"type\"] in [\"FunctionDef\", \"AsyncFunctionDef\"]:\n                params_str = \"()\"\n                if obj[\"params\"]:\n                    params_str = f\"({', '.join(obj['params'])})\"\n            markdown += f\"{'#' * level} {obj['type']} {obj['name']}{params_str}:\\n\"\n            markdown += (\n                f\"{obj['md_content'][-1] if len(obj['md_content']) >0 else ''}\\n\"\n            )\n        markdown += \"***\\n\"\n\n        return markdown\n"}
{"type": "source_file", "path": "repo_agent/chat_engine.py", "content": "from llama_index.llms.openai_like import OpenAILike\n\nfrom repo_agent.doc_meta_info import DocItem\nfrom repo_agent.log import logger\nfrom repo_agent.prompt import chat_template\nfrom repo_agent.settings import SettingsManager\n\n\nclass ChatEngine:\n    \"\"\"\n    ChatEngine is used to generate the doc of functions or classes.\n    \"\"\"\n\n    def __init__(self, project_manager):\n        setting = SettingsManager.get_setting()\n\n        self.llm = OpenAILike(\n            api_key=setting.chat_completion.openai_api_key.get_secret_value(),\n            api_base=setting.chat_completion.openai_base_url,\n            timeout=setting.chat_completion.request_timeout,\n            model=setting.chat_completion.model,\n            temperature=setting.chat_completion.temperature,\n            max_retries=1,\n            is_chat_model=True,\n        )\n\n    def build_prompt(self, doc_item: DocItem):\n        \"\"\"Builds and returns the system and user prompts based on the DocItem.\"\"\"\n        setting = SettingsManager.get_setting()\n\n        code_info = doc_item.content\n        referenced = len(doc_item.who_reference_me) > 0\n\n        code_type = code_info[\"type\"]\n        code_name = code_info[\"name\"]\n        code_content = code_info[\"code_content\"]\n        have_return = code_info[\"have_return\"]\n        file_path = doc_item.get_full_name()\n\n        def get_referenced_prompt(doc_item: DocItem) -> str:\n            if len(doc_item.reference_who) == 0:\n                return \"\"\n            prompt = [\n                \"\"\"As you can see, the code calls the following objects, their code and docs are as following:\"\"\"\n            ]\n            for reference_item in doc_item.reference_who:\n                instance_prompt = (\n                    f\"\"\"obj: {reference_item.get_full_name()}\\nDocument: \\n{reference_item.md_content[-1] if len(reference_item.md_content) > 0 else 'None'}\\nRaw code:```\\n{reference_item.content['code_content'] if 'code_content' in reference_item.content.keys() else ''}\\n```\"\"\"\n                    + \"=\" * 10\n                )\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n\n        def get_referencer_prompt(doc_item: DocItem) -> str:\n            if len(doc_item.who_reference_me) == 0:\n                return \"\"\n            prompt = [\n                \"\"\"Also, the code has been called by the following objects, their code and docs are as following:\"\"\"\n            ]\n            for referencer_item in doc_item.who_reference_me:\n                instance_prompt = (\n                    f\"\"\"obj: {referencer_item.get_full_name()}\\nDocument: \\n{referencer_item.md_content[-1] if len(referencer_item.md_content) > 0 else 'None'}\\nRaw code:```\\n{referencer_item.content['code_content'] if 'code_content' in referencer_item.content.keys() else 'None'}\\n```\"\"\"\n                    + \"=\" * 10\n                )\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n\n        def get_relationship_description(referencer_content, reference_letter):\n            if referencer_content and reference_letter:\n                return \"And please include the reference relationship with its callers and callees in the project from a functional perspective\"\n            elif referencer_content:\n                return \"And please include the relationship with its callers in the project from a functional perspective.\"\n            elif reference_letter:\n                return \"And please include the relationship with its callees in the project from a functional perspective.\"\n            else:\n                return \"\"\n\n        code_type_tell = \"Class\" if code_type == \"ClassDef\" else \"Function\"\n        parameters_or_attribute = (\n            \"attributes\" if code_type == \"ClassDef\" else \"parameters\"\n        )\n        have_return_tell = (\n            \"**Output Example**: Mock up a possible appearance of the code's return value.\"\n            if have_return\n            else \"\"\n        )\n        combine_ref_situation = (\n            \"and combine it with its calling situation in the project,\"\n            if referenced\n            else \"\"\n        )\n\n        referencer_content = get_referencer_prompt(doc_item)\n        reference_letter = get_referenced_prompt(doc_item)\n        has_relationship = get_relationship_description(\n            referencer_content, reference_letter\n        )\n\n        project_structure_prefix = \", and the related hierarchical structure of this project is as follows (The current object is marked with an *):\"\n\n        return chat_template.format_messages(\n            combine_ref_situation=combine_ref_situation,\n            file_path=file_path,\n            project_structure_prefix=project_structure_prefix,\n            code_type_tell=code_type_tell,\n            code_name=code_name,\n            code_content=code_content,\n            have_return_tell=have_return_tell,\n            has_relationship=has_relationship,\n            reference_letter=reference_letter,\n            referencer_content=referencer_content,\n            parameters_or_attribute=parameters_or_attribute,\n            language=setting.project.language,\n        )\n\n    def generate_doc(self, doc_item: DocItem):\n        \"\"\"Generates documentation for a given DocItem.\"\"\"\n        messages = self.build_prompt(doc_item)\n\n        try:\n            response = self.llm.chat(messages)\n            logger.debug(f\"LLM Prompt Tokens: {response.raw.usage.prompt_tokens}\")  # type: ignore\n            logger.debug(\n                f\"LLM Completion Tokens: {response.raw.usage.completion_tokens}\"  # type: ignore\n            )\n            logger.debug(\n                f\"Total LLM Token Count: {response.raw.usage.total_tokens}\"  # type: ignore\n            )\n            return response.message.content\n        except Exception as e:\n            logger.error(f\"Error in llamaindex chat call: {e}\")\n            raise\n"}
{"type": "source_file", "path": "repo_agent/settings.py", "content": "from enum import StrEnum\nfrom typing import Optional\n\nfrom iso639 import Language, LanguageNotFoundError\nfrom pydantic import (\n    DirectoryPath,\n    Field,\n    HttpUrl,\n    PositiveFloat,\n    PositiveInt,\n    SecretStr,\n    field_validator,\n)\nfrom pydantic_settings import BaseSettings\nfrom pathlib import Path\n\n\nclass LogLevel(StrEnum):\n    DEBUG = \"DEBUG\"\n    INFO = \"INFO\"\n    WARNING = \"WARNING\"\n    ERROR = \"ERROR\"\n    CRITICAL = \"CRITICAL\"\n\n\nclass ProjectSettings(BaseSettings):\n    target_repo: DirectoryPath = \"\"  # type: ignore\n    hierarchy_name: str = \".project_doc_record\"\n    markdown_docs_name: str = \"markdown_docs\"\n    ignore_list: list[str] = []\n    language: str = \"English\"\n    max_thread_count: PositiveInt = 4\n    log_level: LogLevel = LogLevel.INFO\n\n    @field_validator(\"language\")\n    @classmethod\n    def validate_language_code(cls, v: str) -> str:\n        try:\n            language_name = Language.match(v).name\n            return language_name  # Returning the resolved language name\n        except LanguageNotFoundError:\n            raise ValueError(\n                \"Invalid language input. Please enter a valid ISO 639 code or language name.\"\n            )\n\n    @field_validator(\"log_level\", mode=\"before\")\n    @classmethod\n    def set_log_level(cls, v: str) -> LogLevel:\n        if isinstance(v, str):\n            v = v.upper()  # Convert input to uppercase\n        if (\n            v in LogLevel._value2member_map_\n        ):  # Check if the converted value is in enum members\n            return LogLevel(v)\n        raise ValueError(f\"Invalid log level: {v}\")\n\n\nclass ChatCompletionSettings(BaseSettings):\n    model: str = \"gpt-4o-mini\"  # NOTE: No model restrictions for user flexibility, but it's recommended to use models with a larger context window.\n    temperature: PositiveFloat = 0.2\n    request_timeout: PositiveInt = 60\n    openai_base_url: str = \"https://api.openai.com/v1\"\n    openai_api_key: SecretStr = Field(..., exclude=True)\n\n    @field_validator(\"openai_base_url\", mode=\"before\")\n    @classmethod\n    def convert_base_url_to_str(cls, openai_base_url: HttpUrl) -> str:\n        return str(openai_base_url)\n\n\nclass Setting(BaseSettings):\n    project: ProjectSettings = {}  # type: ignore\n    chat_completion: ChatCompletionSettings = {}  # type: ignore\n\n\nclass SettingsManager:\n    _setting_instance: Optional[Setting] = (\n        None  # Private class attribute, initially None\n    )\n\n    @classmethod\n    def get_setting(cls):\n        if cls._setting_instance is None:\n            cls._setting_instance = Setting()\n        return cls._setting_instance\n\n    @classmethod\n    def initialize_with_params(\n        cls,\n        target_repo: Path,\n        markdown_docs_name: str,\n        hierarchy_name: str,\n        ignore_list: list[str],\n        language: str,\n        max_thread_count: int,\n        log_level: str,\n        model: str,\n        temperature: float,\n        request_timeout: int,\n        openai_base_url: str,\n    ):\n        project_settings = ProjectSettings(\n            target_repo=target_repo,\n            hierarchy_name=hierarchy_name,\n            markdown_docs_name=markdown_docs_name,\n            ignore_list=ignore_list,\n            language=language,\n            max_thread_count=max_thread_count,\n            log_level=LogLevel(log_level),\n        )\n\n        chat_completion_settings = ChatCompletionSettings(\n            model=model,\n            temperature=temperature,\n            request_timeout=request_timeout,\n            openai_base_url=openai_base_url,\n        )\n\n        cls._setting_instance = Setting(\n            project=project_settings,\n            chat_completion=chat_completion_settings,\n        )\n\n\nif __name__ == \"__main__\":\n    setting = SettingsManager.get_setting()\n    print(setting.model_dump())\n"}
{"type": "source_file", "path": "repo_agent/chat_with_repo/vector_store_manager.py", "content": "import chromadb\nfrom llama_index.core import (\n    Document,\n    StorageContext,\n    VectorStoreIndex,\n    get_response_synthesizer,\n)\nfrom llama_index.core.node_parser import (\n    SemanticSplitterNodeParser,\n    SentenceSplitter,\n)\nfrom llama_index.core.query_engine import RetrieverQueryEngine\nfrom llama_index.core.retrievers import VectorIndexRetriever\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.vector_stores.chroma import ChromaVectorStore\n\nfrom repo_agent.log import logger\n\n\nclass VectorStoreManager:\n    def __init__(self, top_k, llm):\n        \"\"\"\n        Initialize the VectorStoreManager.\n        \"\"\"\n        self.query_engine = None  # Initialize as None\n        self.chroma_db_path = \"./chroma_db\"  # Path to Chroma database\n        self.collection_name = \"test\"  # Default collection name\n        self.similarity_top_k = top_k\n        self.llm = llm\n\n    def create_vector_store(self, md_contents, meta_data, api_key, api_base):\n        \"\"\"\n        Add markdown content and metadata to the index.\n        \"\"\"\n        if not md_contents or not meta_data:\n            logger.warning(\"No content or metadata provided. Skipping.\")\n            return\n\n        # Ensure lengths match\n        min_length = min(len(md_contents), len(meta_data))\n        md_contents = md_contents[:min_length]\n        meta_data = meta_data[:min_length]\n\n        logger.debug(f\"Number of markdown contents: {len(md_contents)}\")\n        logger.debug(f\"Number of metadata entries: {len(meta_data)}\")\n\n        # Initialize Chroma client and collection\n        db = chromadb.PersistentClient(path=self.chroma_db_path)\n        chroma_collection = db.get_or_create_collection(self.collection_name)\n\n        # Define embedding model\n        embed_model = OpenAIEmbedding(\n            model_name=\"text-embedding-3-large\",\n            api_key=api_key,\n            api_base=api_base,\n        )\n\n        # Initialize semantic chunker (SimpleNodeParser)\n        logger.debug(\"Initializing semantic chunker (SimpleNodeParser).\")\n        splitter = SemanticSplitterNodeParser(\n            buffer_size=1, breakpoint_percentile_threshold=95, embed_model=embed_model\n        )\n        base_splitter = SentenceSplitter(chunk_size=1024)\n\n        documents = [\n            Document(text=content, extra_info=meta)\n            for content, meta in zip(md_contents, meta_data)\n        ]\n\n        all_nodes = []\n        for i, doc in enumerate(documents):\n            logger.debug(\n                f\"Processing document {i+1}: Content length={len(doc.get_text())}\"\n            )\n\n            try:\n                # Try semantic splitting first\n                nodes = splitter.get_nodes_from_documents([doc])\n                logger.debug(f\"Document {i+1} split into {len(nodes)} semantic chunks.\")\n\n            except Exception as e:\n                # Fallback to baseline sentence splitting\n                logger.warning(\n                    f\"Semantic splitting failed for document {i+1}, falling back to SentenceSplitter. Error: {e}\"\n                )\n                nodes = base_splitter.get_nodes_from_documents([doc])\n                logger.debug(f\"Document {i+1} split into {len(nodes)} sentence chunks.\")\n\n            all_nodes.extend(nodes)\n\n        if not all_nodes:\n            logger.warning(\"No valid nodes to add to the index after chunking.\")\n            return\n\n        logger.debug(f\"Number of valid chunks: {len(all_nodes)}\")\n\n        # Set up ChromaVectorStore and load data\n        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n        index = VectorStoreIndex(\n            all_nodes, storage_context=storage_context, embed_model=embed_model\n        )\n        retriever = VectorIndexRetriever(\n            index=index, similarity_top_k=self.similarity_top_k, embed_model=embed_model\n        )\n\n        response_synthesizer = get_response_synthesizer(llm=self.llm)\n\n        # Set the query engine\n        self.query_engine = RetrieverQueryEngine(\n            retriever=retriever,\n            response_synthesizer=response_synthesizer,\n        )\n\n        logger.info(f\"Vector store created and loaded with {len(documents)} documents.\")\n\n    def query_store(self, query):\n        \"\"\"\n        Query the vector store for relevant documents.\n        \"\"\"\n        if not self.query_engine:\n            logger.error(\n                \"Query engine is not initialized. Please create a vector store first.\"\n            )\n            return []\n\n        # Query the vector store\n        logger.debug(f\"Querying vector store with: {query}\")\n        results = self.query_engine.query(query)\n\n        # Extract relevant information from results\n        return [{\"text\": results.response, \"metadata\": results.metadata}]\n"}
{"type": "source_file", "path": "repo_agent/chat_with_repo/json_handler.py", "content": "import json\nimport sys\n\nfrom repo_agent.log import logger\n\n\nclass JsonFileProcessor:\n    def __init__(self, file_path):\n        self.file_path = file_path\n\n    def read_json_file(self):\n        try:\n            with open(self.file_path, \"r\", encoding=\"utf-8\") as file:\n                data = json.load(file)\n            return data\n        except FileNotFoundError:\n            logger.exception(f\"File not found: {self.file_path}\")\n            sys.exit(1)\n\n    def extract_data(self):\n        # Load JSON data from a file\n        json_data = self.read_json_file()\n        md_contents = []\n        extracted_contents = []\n        # Iterate through each file in the JSON data\n        for file, items in json_data.items():\n            # Check if the value is a list (new format)\n            if isinstance(items, list):\n                # Iterate through each item in the list\n                for item in items:\n                    # Check if 'md_content' exists and is not empty\n                    if \"md_content\" in item and item[\"md_content\"]:\n                        # Append the first element of 'md_content' to the result list\n                        md_contents.append(item[\"md_content\"][0])\n                        # Build a dictionary containing the required information\n                        item_dict = {\n                            \"type\": item.get(\"type\", \"UnknownType\"),\n                            \"name\": item.get(\"name\", \"Unnamed\"),\n                            \"code_start_line\": item.get(\"code_start_line\", -1),\n                            \"code_end_line\": item.get(\"code_end_line\", -1),\n                            \"have_return\": item.get(\"have_return\", False),\n                            \"code_content\": item.get(\"code_content\", \"NoContent\"),\n                            \"name_column\": item.get(\"name_column\", 0),\n                            \"item_status\": item.get(\"item_status\", \"UnknownStatus\"),\n                            # Adapt or remove fields based on new structure requirements\n                        }\n                        extracted_contents.append(item_dict)\n        return md_contents, extracted_contents\n\n    def recursive_search(self, data_item, search_text, code_results, md_results):\n        if isinstance(data_item, dict):\n            # Direct comparison is removed as there's no direct key==search_text in the new format\n            for key, value in data_item.items():\n                # Recursively search through dictionary values and lists\n                if isinstance(value, (dict, list)):\n                    self.recursive_search(value, search_text, code_results, md_results)\n        elif isinstance(data_item, list):\n            for item in data_item:\n                # Now we check for the 'name' key in each item of the list\n                if isinstance(item, dict) and item.get(\"name\") == search_text:\n                    # If 'code_content' exists, append it to results\n                    if \"code_content\" in item:\n                        code_results.append(item[\"code_content\"])\n                        md_results.append(item[\"md_content\"])\n                # Recursive call in case of nested lists or dicts\n                self.recursive_search(item, search_text, code_results, md_results)\n\n    def search_code_contents_by_name(self, file_path, search_text):\n        # Attempt to retrieve code from the JSON file\n        try:\n            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n                data = json.load(file)\n                code_results = []\n                md_results = []  # List to store matching items' code_content and md_content\n                self.recursive_search(data, search_text, code_results, md_results)\n                # 确保无论结果如何都返回两个值\n                if code_results or md_results:\n                    return code_results, md_results\n                else:\n                    return [\"No matching item found.\"], [\"No matching item found.\"]\n        except FileNotFoundError:\n            return \"File not found.\"\n        except json.JSONDecodeError:\n            return \"Invalid JSON file.\"\n        except Exception as e:\n            return f\"An error occurred: {e}\"\n\n\nif __name__ == \"__main__\":\n    processor = JsonFileProcessor(\"database.json\")\n    md_contents, extracted_contents = processor.extract_data()\n"}
{"type": "source_file", "path": "repo_agent/doc_meta_info.py", "content": "\"\"\"存储doc对应的信息，同时处理引用的关系\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport os\nimport threading\nfrom dataclasses import dataclass, field\nfrom enum import Enum, auto, unique\nfrom pathlib import Path\nfrom typing import Any, Callable, Dict, List, Optional\n\nimport jedi\nfrom colorama import Fore, Style\nfrom prettytable import PrettyTable\nfrom tqdm import tqdm\n\nfrom repo_agent.file_handler import FileHandler\nfrom repo_agent.log import logger\nfrom repo_agent.multi_task_dispatch import Task, TaskManager\nfrom repo_agent.settings import SettingsManager\nfrom repo_agent.utils.meta_info_utils import latest_verison_substring\n\n\n@unique\nclass EdgeType(Enum):\n    reference_edge = auto()  # 一个obj引用另一个obj\n    subfile_edge = auto()  # 一个 文件/文件夹 属于一个文件夹\n    file_item_edge = auto()  # 一个 obj 属于一个文件\n\n\n@unique\nclass DocItemType(Enum):\n    # 对可能的对象文档类型进行定义（分不同细粒度）\n    _repo = auto()  # 根节点，需要生成readme\n    _dir = auto()\n    _file = auto()\n    _class = auto()\n    _class_function = auto()\n    _function = auto()  # 文件内的常规function\n    _sub_function = auto()  # function内的定义的subfunction\n    _global_var = auto()\n\n    def to_str(self):\n        if self == DocItemType._class:\n            return \"ClassDef\"\n        elif self == DocItemType._function:\n            return \"FunctionDef\"\n        elif self == DocItemType._class_function:\n            return \"FunctionDef\"\n        elif self == DocItemType._sub_function:\n            return \"FunctionDef\"\n        # assert False, f\"{self.name}\"\n        return self.name\n\n    def print_self(self):\n        color = Fore.WHITE\n        if self == DocItemType._dir:\n            color = Fore.GREEN\n        elif self == DocItemType._file:\n            color = Fore.YELLOW\n        elif self == DocItemType._class:\n            color = Fore.RED\n        elif self in [\n            DocItemType._function,\n            DocItemType._sub_function,\n            DocItemType._class_function,\n        ]:\n            color = Fore.BLUE\n        return color + self.name + Style.RESET_ALL\n\n    def get_edge_type(self, from_item_type: DocItemType, to_item_type: DocItemType):\n        pass\n\n\n@unique\nclass DocItemStatus(Enum):\n    doc_up_to_date = auto()  # 无需生成文档\n    doc_has_not_been_generated = auto()  # 文档还未生成，需要生成\n    code_changed = auto()  # 源码被修改了，需要改文档\n    add_new_referencer = auto()  # 添加了新的引用者\n    referencer_not_exist = auto()  # 曾经引用他的obj被删除了，或者不再引用他了\n\n\ndef need_to_generate(doc_item: DocItem, ignore_list: List[str] = []) -> bool:\n    \"\"\"只生成item的，文件及更高粒度都跳过。另外如果属于一个blacklist的文件也跳过\"\"\"\n    if doc_item.item_status == DocItemStatus.doc_up_to_date:\n        return False\n    rel_file_path = doc_item.get_full_name()\n    if doc_item.item_type in [\n        DocItemType._file,\n        DocItemType._dir,\n        DocItemType._repo,\n    ]:  # 暂时不生成file及以上的doc\n        return False\n    doc_item = doc_item.father\n    while doc_item:\n        if doc_item.item_type == DocItemType._file:\n            # 如果当前文件在忽略列表中，或者在忽略列表某个文件路径下，则跳过\n            if any(\n                rel_file_path.startswith(ignore_item) for ignore_item in ignore_list\n            ):\n                return False\n            else:\n                return True\n        doc_item = doc_item.father\n    return False\n\n\n@dataclass\nclass DocItem:\n    item_type: DocItemType = DocItemType._class_function\n    item_status: DocItemStatus = DocItemStatus.doc_has_not_been_generated\n\n    obj_name: str = \"\"  # 对象的名字\n    code_start_line: int = -1\n    code_end_line: int = -1\n    md_content: List[str] = field(default_factory=list)  # 存储不同版本的doc\n    content: Dict[Any, Any] = field(default_factory=dict)  # 原本存储的信息\n\n    children: Dict[str, DocItem] = field(default_factory=dict)  # 子对象\n    father: Any[DocItem] = None\n\n    depth: int = 0\n    tree_path: List[DocItem] = field(default_factory=list)  # 一整条链路，从root开始\n    max_reference_ansce: Any[DocItem] = None\n\n    reference_who: List[DocItem] = field(default_factory=list)  # 他引用了谁\n    who_reference_me: List[DocItem] = field(default_factory=list)  # 谁引用了他\n    special_reference_type: List[bool] = field(default_factory=list)\n\n    reference_who_name_list: List[str] = field(\n        default_factory=list\n    )  # 他引用了谁，这个可能是老版本\n    who_reference_me_name_list: List[str] = field(\n        default_factory=list\n    )  # 谁引用了他，这个可能是老版本的\n\n    has_task: bool = False\n\n    multithread_task_id: int = -1  # 在多线程中的task_id\n\n    @staticmethod\n    def has_ans_relation(now_a: DocItem, now_b: DocItem):\n        \"\"\"Check if there is an ancestor relationship between two nodes and return the earlier node if exists.\n\n        Args:\n            now_a (DocItem): The first node.\n            now_b (DocItem): The second node.\n\n        Returns:\n            DocItem or None: The earlier node if an ancestor relationship exists, otherwise None.\n        \"\"\"\n        if now_b in now_a.tree_path:\n            return now_b\n        if now_a in now_b.tree_path:\n            return now_a\n        return None\n\n    def get_travel_list(self):\n        \"\"\"按照先序遍历的顺序，根节点在第一个\"\"\"\n        now_list = [self]\n        for _, child in self.children.items():\n            now_list = now_list + child.get_travel_list()\n        return now_list\n\n    def check_depth(self):\n        \"\"\"\n        Recursively calculates the depth of the node in the tree.\n\n        Returns:\n            int: The depth of the node.\n        \"\"\"\n        if len(self.children) == 0:\n            self.depth = 0\n            return self.depth\n        max_child_depth = 0\n        for _, child in self.children.items():\n            child_depth = child.check_depth()\n            max_child_depth = max(child_depth, max_child_depth)\n        self.depth = max_child_depth + 1\n        return self.depth\n\n    def parse_tree_path(self, now_path):\n        \"\"\"\n        Recursively parses the tree path by appending the current node to the given path.\n\n        Args:\n            now_path (list): The current path in the tree.\n\n        Returns:\n            None\n        \"\"\"\n        self.tree_path = now_path + [self]\n        for key, child in self.children.items():\n            child.parse_tree_path(self.tree_path)\n\n    def get_file_name(self):\n        full_name = self.get_full_name()\n        return full_name.split(\".py\")[0] + \".py\"\n\n    def get_full_name(self, strict=False):\n        \"\"\"获取从下到上所有的obj名字\n\n        Returns:\n            str: 从下到上所有的obj名字，以斜杠分隔\n        \"\"\"\n        if self.father == None:\n            return self.obj_name\n        name_list = []\n        now = self\n        while now != None:\n            self_name = now.obj_name\n            if strict:\n                for name, item in self.father.children.items():\n                    if item == now:\n                        self_name = name\n                        break\n                if self_name != now.obj_name:\n                    self_name = self_name + \"(name_duplicate_version)\"\n            name_list = [self_name] + name_list\n            now = now.father\n\n        name_list = name_list[1:]\n        return \"/\".join(name_list)\n\n    def find(self, recursive_file_path: list) -> Optional[DocItem]:\n        \"\"\"\n        从repo根节点根据path_list找到对应的文件, 否则返回False\n\n        Args:\n            recursive_file_path (list): The list of file paths to search for.\n\n        Returns:\n            Optional[DocItem]: The corresponding file if found, otherwise None.\n        \"\"\"\n        assert self.item_type == DocItemType._repo\n        pos = 0\n        now = self\n        while pos < len(recursive_file_path):\n            if not recursive_file_path[pos] in now.children.keys():\n                return None\n            now = now.children[recursive_file_path[pos]]\n            pos += 1\n        return now\n\n    @staticmethod\n    def check_has_task(now_item: DocItem, ignore_list: List[str] = []):\n        if need_to_generate(now_item, ignore_list=ignore_list):\n            now_item.has_task = True\n        for _, child in now_item.children.items():\n            DocItem.check_has_task(child, ignore_list)\n            now_item.has_task = child.has_task or now_item.has_task\n\n    def print_recursive(\n        self,\n        indent=0,\n        print_content=False,\n        diff_status=False,\n        ignore_list: List[str] = [],\n    ):\n        \"\"\"递归打印repo对象\"\"\"\n\n        def print_indent(indent=0):\n            if indent == 0:\n                return \"\"\n            return \"  \" * indent + \"|-\"\n\n        print_obj_name = self.obj_name\n\n        setting = SettingsManager.get_setting()\n\n        if self.item_type == DocItemType._repo:\n            print_obj_name = setting.project.target_repo\n        if diff_status and need_to_generate(self, ignore_list=ignore_list):\n            print(\n                print_indent(indent)\n                + f\"{self.item_type.print_self()}: {print_obj_name} : {self.item_status.name}\",\n            )\n        else:\n            print(\n                print_indent(indent)\n                + f\"{self.item_type.print_self()}: {print_obj_name}\",\n            )\n        for child_name, child in self.children.items():\n            if diff_status and child.has_task == False:\n                continue\n            child.print_recursive(\n                indent=indent + 1,\n                print_content=print_content,\n                diff_status=diff_status,\n                ignore_list=ignore_list,\n            )\n\n\ndef find_all_referencer(\n    repo_path, variable_name, file_path, line_number, column_number, in_file_only=False\n):\n    \"\"\"复制过来的之前的实现\"\"\"\n    script = jedi.Script(path=os.path.join(repo_path, file_path))\n    try:\n        if in_file_only:\n            references = script.get_references(\n                line=line_number, column=column_number, scope=\"file\"\n            )\n        else:\n            references = script.get_references(line=line_number, column=column_number)\n        # 过滤出变量名为 variable_name 的引用，并返回它们的位置\n        variable_references = [ref for ref in references if ref.name == variable_name]\n        # if variable_name == \"need_to_generate\":\n        #     import pdb; pdb.set_trace()\n        return [\n            (os.path.relpath(ref.module_path, repo_path), ref.line, ref.column)\n            for ref in variable_references\n            if not (ref.line == line_number and ref.column == column_number)\n        ]\n    except Exception as e:\n        # 打印错误信息和相关参数\n        logger.error(f\"Error occurred: {e}\")\n        logger.error(\n            f\"Parameters: variable_name={variable_name}, file_path={file_path}, line_number={line_number}, column_number={column_number}\"\n        )\n        return []\n\n\n@dataclass\nclass MetaInfo:\n    repo_path: Path = \"\"  # type: ignore\n    document_version: str = (\n        \"\"  # 随时间变化，\"\"代表没完成，否则对应一个目标仓库的commit hash\n    )\n    target_repo_hierarchical_tree: \"DocItem\" = field(\n        default_factory=lambda: DocItem()\n    )  # 整个repo的文件结构\n    white_list: Any[List] = None\n\n    fake_file_reflection: Dict[str, str] = field(default_factory=dict)\n    jump_files: List[str] = field(default_factory=list)\n    deleted_items_from_older_meta: List[List] = field(default_factory=list)\n\n    in_generation_process: bool = False\n\n    checkpoint_lock: threading.Lock = threading.Lock()\n\n    @staticmethod\n    def init_meta_info(file_path_reflections, jump_files) -> MetaInfo:\n        \"\"\"从一个仓库path中初始化metainfo\"\"\"\n\n        setting = SettingsManager.get_setting()\n\n        project_abs_path = setting.project.target_repo\n        print(\n            f\"{Fore.LIGHTRED_EX}Initializing MetaInfo: {Style.RESET_ALL}from {project_abs_path}\"\n        )\n        file_handler = FileHandler(project_abs_path, None)\n        repo_structure = file_handler.generate_overall_structure(\n            file_path_reflections, jump_files\n        )\n        metainfo = MetaInfo.from_project_hierarchy_json(repo_structure)\n        metainfo.repo_path = project_abs_path\n        metainfo.fake_file_reflection = file_path_reflections\n        metainfo.jump_files = jump_files\n        return metainfo\n\n    @staticmethod\n    def from_checkpoint_path(checkpoint_dir_path: Path) -> MetaInfo:\n        \"\"\"从已有的metainfo dir里面读取metainfo\"\"\"\n        setting = SettingsManager.get_setting()\n\n        project_hierarchy_json_path = checkpoint_dir_path / \"project_hierarchy.json\"\n\n        with open(project_hierarchy_json_path, \"r\", encoding=\"utf-8\") as reader:\n            project_hierarchy_json = json.load(reader)\n        metainfo = MetaInfo.from_project_hierarchy_json(project_hierarchy_json)\n\n        with open(\n            checkpoint_dir_path / \"meta-info.json\", \"r\", encoding=\"utf-8\"\n        ) as reader:\n            meta_data = json.load(reader)\n            metainfo.repo_path = setting.project.target_repo\n\n            metainfo.document_version = meta_data[\"doc_version\"]\n            metainfo.fake_file_reflection = meta_data[\"fake_file_reflection\"]\n            metainfo.jump_files = meta_data[\"jump_files\"]\n            metainfo.in_generation_process = meta_data[\"in_generation_process\"]\n            metainfo.deleted_items_from_older_meta = meta_data[\n                \"deleted_items_from_older_meta\"\n            ]\n\n        print(f\"{Fore.CYAN}Loading MetaInfo:{Style.RESET_ALL} {checkpoint_dir_path}\")\n        return metainfo\n\n    def checkpoint(self, target_dir_path: str | Path, flash_reference_relation=False):\n        \"\"\"\n        Save the MetaInfo object to the specified directory.\n\n        Args:\n            target_dir_path (str | Path): The path to the target directory where the MetaInfo will be saved.\n            flash_reference_relation (bool, optional): Whether to include flash reference relation in the saved MetaInfo. Defaults to False.\n        \"\"\"\n        with self.checkpoint_lock:\n            # 转换 target_dir_path 为 Path 对象\n            target_dir = Path(target_dir_path)\n            logger.debug(f\"Checkpointing MetaInfo to directory: {target_dir}\")\n\n            # 打印保存成功的信息\n            print(f\"{Fore.GREEN}MetaInfo is Refreshed and Saved{Style.RESET_ALL}\")\n\n            # 创建目录（如果不存在）\n            if not target_dir.exists():\n                target_dir.mkdir(parents=True, exist_ok=True)\n                logger.debug(f\"Created directory: {target_dir}\")\n\n            # 保存 project_hierarchy.json 文件\n            now_hierarchy_json = self.to_hierarchy_json(\n                flash_reference_relation=flash_reference_relation\n            )\n            hierarchy_file = target_dir / \"project_hierarchy.json\"\n            try:\n                with hierarchy_file.open(\"w\", encoding=\"utf-8\") as writer:\n                    json.dump(now_hierarchy_json, writer, indent=2, ensure_ascii=False)\n                logger.debug(f\"Saved hierarchy JSON to {hierarchy_file}\")\n            except IOError as e:\n                logger.error(f\"Failed to save hierarchy JSON to {hierarchy_file}: {e}\")\n\n            # 保存 meta-info.json 文件\n            meta_info_file = target_dir / \"meta-info.json\"\n            meta = {\n                \"doc_version\": self.document_version,\n                \"in_generation_process\": self.in_generation_process,\n                \"fake_file_reflection\": self.fake_file_reflection,\n                \"jump_files\": self.jump_files,\n                \"deleted_items_from_older_meta\": self.deleted_items_from_older_meta,\n            }\n            try:\n                with meta_info_file.open(\"w\", encoding=\"utf-8\") as writer:\n                    json.dump(meta, writer, indent=2, ensure_ascii=False)\n                logger.debug(f\"Saved meta-info JSON to {meta_info_file}\")\n            except IOError as e:\n                logger.error(f\"Failed to save meta-info JSON to {meta_info_file}: {e}\")\n\n    def print_task_list(self, task_dict: Dict[Task]):\n        \"\"\"打印\"\"\"\n        task_table = PrettyTable(\n            [\"task_id\", \"Doc Generation Reason\", \"Path\", \"dependency\"]\n        )\n        for task_id, task_info in task_dict.items():\n            remain_str = \"None\"\n            if task_info.dependencies != []:\n                remain_str = \",\".join(\n                    [str(d_task.task_id) for d_task in task_info.dependencies]\n                )\n                if len(remain_str) > 20:\n                    remain_str = remain_str[:8] + \"...\" + remain_str[-8:]\n            task_table.add_row(\n                [\n                    task_id,\n                    task_info.extra_info.item_status.name,\n                    task_info.extra_info.get_full_name(strict=True),\n                    remain_str,\n                ]\n            )\n        # print(\"Remain tasks to be done\")\n        print(task_table)\n\n    def get_all_files(self) -> List[DocItem]:\n        \"\"\"获取所有的file节点\"\"\"\n        files = []\n\n        def walk_tree(now_node):\n            if now_node.item_type == DocItemType._file:\n                files.append(now_node)\n            for _, child in now_node.children.items():\n                walk_tree(child)\n\n        walk_tree(self.target_repo_hierarchical_tree)\n        return files\n\n    def find_obj_with_lineno(self, file_node: DocItem, start_line_num) -> DocItem:\n        \"\"\"每个DocItem._file，对于所有的行，建立他们对应的对象是谁\n        一个行属于这个obj的范围，并且没法属于他的儿子的范围了\"\"\"\n        now_node = file_node\n        # if\n        assert now_node != None\n        while len(now_node.children) > 0:\n            find_qualify_child = False\n            for _, child in now_node.children.items():\n                assert child.content != None\n                if (\n                    child.content[\"code_start_line\"] <= start_line_num\n                    and child.content[\"code_end_line\"] >= start_line_num\n                ):\n                    now_node = child\n                    find_qualify_child = True\n                    break\n            if not find_qualify_child:\n                return now_node\n        return now_node\n\n    def parse_reference(self):\n        \"\"\"双向提取所有引用关系\"\"\"\n        file_nodes = self.get_all_files()\n\n        white_list_file_names, white_list_obj_names = (\n            [],\n            [],\n        )  # 如果指定白名单，只处理白名单上的双向引用关系\n        if self.white_list != None:\n            white_list_file_names = [cont[\"file_path\"] for cont in self.white_list]\n            white_list_obj_names = [cont[\"id_text\"] for cont in self.white_list]\n\n        for file_node in tqdm(file_nodes, desc=\"parsing bidirectional reference\"):\n            \"\"\"检测一个文件内的所有引用信息，只能检测引用该文件内某个obj的其他内容。\n            1. 如果某个文件是jump-files，就不应该出现在这个循环里\n            2. 如果检测到的引用信息来源于一个jump-files, 忽略它\n            3. 如果检测到一个引用来源于fake-file,则认为他的母文件是原来的文件\n            \"\"\"\n            assert not file_node.get_full_name().endswith(latest_verison_substring)\n\n            ref_count = 0\n            rel_file_path = file_node.get_full_name()\n            assert rel_file_path not in self.jump_files\n\n            if white_list_file_names != [] and (\n                file_node.get_file_name() not in white_list_file_names\n            ):  # 如果有白名单，只parse白名单里的对象\n                continue\n\n            def walk_file(now_obj: DocItem):\n                \"\"\"在文件内遍历所有变量\"\"\"\n                nonlocal ref_count, white_list_file_names\n                in_file_only = False\n                if white_list_obj_names != [] and (\n                    now_obj.obj_name not in white_list_obj_names\n                ):\n                    in_file_only = True  # 作为加速，如果有白名单，白名单obj同文件夹下的也parse，但是只找同文件内的引用\n\n                reference_list = find_all_referencer(\n                    repo_path=self.repo_path,\n                    variable_name=now_obj.obj_name,\n                    file_path=rel_file_path,\n                    line_number=now_obj.content[\"code_start_line\"],\n                    column_number=now_obj.content[\"name_column\"],\n                    in_file_only=in_file_only,\n                )\n                for referencer_pos in reference_list:  # 对于每个引用\n                    referencer_file_ral_path = referencer_pos[0]\n                    if referencer_file_ral_path in self.fake_file_reflection.values():\n                        \"\"\"检测到的引用者来自于unstaged files，跳过该引用\"\"\"\n                        print(\n                            f\"{Fore.LIGHTBLUE_EX}[Reference From Unstaged Version, skip]{Style.RESET_ALL} {referencer_file_ral_path} -> {now_obj.get_full_name()}\"\n                        )\n                        continue\n                    elif referencer_file_ral_path in self.jump_files:\n                        \"\"\"检测到的引用者来自于untracked files，跳过该引用\"\"\"\n                        print(\n                            f\"{Fore.LIGHTBLUE_EX}[Reference From Unstracked Version, skip]{Style.RESET_ALL} {referencer_file_ral_path} -> {now_obj.get_full_name()}\"\n                        )\n                        continue\n\n                    target_file_hiera = referencer_file_ral_path.split(\"/\")\n                    # for file_hiera_id in range(len(target_file_hiera)):\n                    #     if target_file_hiera[file_hiera_id].endswith(fake_file_substring):\n                    #         prefix = \"/\".join(target_file_hiera[:file_hiera_id+1])\n                    #         find_in_reflection = False\n                    #         for real, fake in self.fake_file_reflection.items():\n                    #             if fake == prefix:\n                    #                 print(f\"{Fore.BLUE}Find Reference in Fake-File: {Style.RESET_ALL}{referencer_file_ral_path} {Fore.BLUE}referred{Style.RESET_ALL} {now_obj.item_type.name} {now_obj.get_full_name()}\")\n                    #                 target_file_hiera = real.split(\"/\") + target_file_hiera[file_hiera_id+1:]\n                    #                 find_in_reflection = True\n                    #                 break\n                    #         assert find_in_reflection\n                    #         break\n\n                    referencer_file_item = self.target_repo_hierarchical_tree.find(\n                        target_file_hiera\n                    )\n                    if referencer_file_item == None:\n                        print(\n                            f'{Fore.LIGHTRED_EX}Error: Find \"{referencer_file_ral_path}\"(not in target repo){Style.RESET_ALL} referenced {now_obj.get_full_name()}'\n                        )\n                        continue\n                    referencer_node = self.find_obj_with_lineno(\n                        referencer_file_item, referencer_pos[1]\n                    )\n                    if referencer_node.obj_name == now_obj.obj_name:\n                        logger.info(\n                            f\"Jedi find {now_obj.get_full_name()} with name_duplicate_reference, skipped\"\n                        )\n                        continue\n                    # if now_obj.get_full_name() == \"repo_agent/runner.py/Runner/run\":\n                    #     import pdb; pdb.set_trace()\n                    if DocItem.has_ans_relation(now_obj, referencer_node) == None:\n                        # 不考虑祖先节点之间的引用\n                        if now_obj not in referencer_node.reference_who:\n                            special_reference_type = (\n                                referencer_node.item_type\n                                in [\n                                    DocItemType._function,\n                                    DocItemType._sub_function,\n                                    DocItemType._class_function,\n                                ]\n                            ) and referencer_node.code_start_line == referencer_pos[1]\n                            referencer_node.special_reference_type.append(\n                                special_reference_type\n                            )\n                            referencer_node.reference_who.append(now_obj)\n                            now_obj.who_reference_me.append(referencer_node)\n                            ref_count += 1\n                for _, child in now_obj.children.items():\n                    walk_file(child)\n\n            for _, child in file_node.children.items():\n                walk_file(child)\n            # logger.info(f\"find {ref_count} refer-relation in {file_node.get_full_name()}\")\n\n    def get_task_manager(self, now_node: DocItem, task_available_func) -> TaskManager:\n        \"\"\"先写一个退化的版本，只考虑拓扑引用关系\"\"\"\n        doc_items = now_node.get_travel_list()\n        if self.white_list != None:\n\n            def in_white_list(item: DocItem):\n                for cont in self.white_list:\n                    if (\n                        item.get_file_name() == cont[\"file_path\"]\n                        and item.obj_name == cont[\"id_text\"]\n                    ):\n                        return True\n                return False\n\n            doc_items = list(filter(in_white_list, doc_items))\n        doc_items = list(filter(task_available_func, doc_items))\n        doc_items = sorted(doc_items, key=lambda x: x.depth)  # 叶子节点在前面\n        deal_items = []\n        task_manager = TaskManager()\n        bar = tqdm(total=len(doc_items), desc=\"parsing topology task-list\")\n        while doc_items:\n            min_break_level = 1e7\n            target_item = None\n            for item in doc_items:\n                \"\"\"一个任务依赖于所有引用者和他的子节点,我们不能保证引用不成环(也许有些仓库的废代码会出现成环)。\n                这时就只能选择一个相对来说遵守程度最好的了\n                有特殊情况func-def中的param def可能会出现循环引用\n                另外循环引用真实存在，对于一些bind类的接口真的会发生，比如：\n                ChatDev/WareHouse/Gomoku_HumanAgentInteraction_20230920135038/main.py里面的: on-click、show-winner、restart\n                \"\"\"\n                best_break_level = 0\n                second_best_break_level = 0\n                for _, child in item.children.items():  # 父亲依赖儿子的关系是一定要走的\n                    if task_available_func(child) and (child not in deal_items):\n                        best_break_level += 1\n                for referenced, special in zip(\n                    item.reference_who, item.special_reference_type\n                ):\n                    if task_available_func(referenced) and (\n                        referenced not in deal_items\n                    ):\n                        best_break_level += 1\n                    if (\n                        task_available_func(referenced)\n                        and (not special)\n                        and (referenced not in deal_items)\n                    ):\n                        second_best_break_level += 1\n                if best_break_level == 0:\n                    min_break_level = -1\n                    target_item = item\n                    break\n                if second_best_break_level < min_break_level:\n                    target_item = item\n                    min_break_level = second_best_break_level\n\n            if min_break_level > 0:\n                print(\n                    f\"circle-reference(second-best still failed), level={min_break_level}: {target_item.get_full_name()}\"\n                )\n\n            item_denp_task_ids = []\n            for _, child in target_item.children.items():\n                if child.multithread_task_id != -1:\n                    assert child.multithread_task_id in task_manager.task_dict.keys()\n                    item_denp_task_ids.append(child.multithread_task_id)\n            for referenced_item in target_item.reference_who:\n                if referenced_item.multithread_task_id in task_manager.task_dict.keys():\n                    item_denp_task_ids.append(referenced_item.multithread_task_id)\n            item_denp_task_ids = list(set(item_denp_task_ids))  # 去重\n            if task_available_func == None or task_available_func(target_item):\n                task_id = task_manager.add_task(\n                    dependency_task_id=item_denp_task_ids, extra=target_item\n                )\n                target_item.multithread_task_id = task_id\n            deal_items.append(target_item)\n            doc_items.remove(target_item)\n            bar.update(1)\n\n        return task_manager\n\n    def get_topology(self, task_available_func) -> TaskManager:\n        \"\"\"计算repo中所有对象的拓扑顺序\"\"\"\n        self.parse_reference()\n        task_manager = self.get_task_manager(\n            self.target_repo_hierarchical_tree, task_available_func=task_available_func\n        )\n        return task_manager\n\n    def _map(self, deal_func: Callable):\n        \"\"\"将所有节点进行同一个操作\"\"\"\n\n        def travel(now_item: DocItem):\n            deal_func(now_item)\n            for _, child in now_item.children.items():\n                travel(child)\n\n        travel(self.target_repo_hierarchical_tree)\n\n    def load_doc_from_older_meta(self, older_meta: MetaInfo):\n        \"\"\"older_meta是老版本的、已经生成doc的meta info\"\"\"\n        logger.info(\"merge doc from an older version of metainfo\")\n        root_item = self.target_repo_hierarchical_tree  # 新版的根节点\n        deleted_items = []\n\n        def find_item(now_item: DocItem) -> Optional[DocItem]:\n            \"\"\"\n            Find an item in the new version of meta based on its original item.\n\n            Args:\n                now_item (DocItem): The original item to be found in the new version of meta.\n\n            Returns:\n                Optional[DocItem]: The corresponding item in the new version of meta if found, otherwise None.\n            \"\"\"\n            nonlocal root_item\n            if now_item.father == None:  # The root node can always be found\n                return root_item\n            father_find_result = find_item(now_item.father)\n            if not father_find_result:\n                return None\n            # 注意：这里需要考虑 now_item.obj_name可能会有重名，并不一定等于\n            real_name = None\n            for child_real_name, temp_item in now_item.father.children.items():\n                if temp_item == now_item:\n                    real_name = child_real_name\n                    break\n            assert real_name != None\n            # if real_name != now_item.obj_name:\n            #     import pdb; pdb.set_trace()\n            if real_name in father_find_result.children.keys():\n                result_item = father_find_result.children[real_name]\n                return result_item\n            return None\n\n        def travel(now_older_item: DocItem):  # 只寻找源码是否被修改的信息\n            # if now_older_item.get_full_name() == \"autogen/_pydantic.py/type2schema\":\n            #     import pdb; pdb.set_trace()\n            result_item = find_item(now_older_item)\n            if not result_item:  # 新版文件中找不到原来的item，就回退\n                deleted_items.append(\n                    [now_older_item.get_full_name(), now_older_item.item_type.name]\n                )\n                return\n            result_item.md_content = now_older_item.md_content\n            result_item.item_status = now_older_item.item_status\n            # if result_item.obj_name == \"run\":\n            #     import pdb; pdb.set_trace()\n            if \"code_content\" in now_older_item.content.keys():\n                assert \"code_content\" in result_item.content.keys()\n                if (\n                    now_older_item.content[\"code_content\"]\n                    != result_item.content[\"code_content\"]\n                ):  # 源码被修改了\n                    result_item.item_status = DocItemStatus.code_changed\n\n            for _, child in now_older_item.children.items():\n                travel(child)\n\n        travel(older_meta.target_repo_hierarchical_tree)\n\n        \"\"\"接下来，parse现在的双向引用，观察谁的引用者改了\"\"\"\n        self.parse_reference()\n\n        def travel2(now_older_item: DocItem):\n            result_item = find_item(now_older_item)\n            if not result_item:  # 新版文件中找不到原来的item，就回退\n                return\n            \"\"\"result_item引用的人是否变化了\"\"\"\n            new_reference_names = [\n                name.get_full_name(strict=True) for name in result_item.who_reference_me\n            ]\n            old_reference_names = now_older_item.who_reference_me_name_list\n            # if now_older_item.get_full_name() == \"autogen/_pydantic.py/type2schema\":\n            #     import pdb; pdb.set_trace()\n            if not (set(new_reference_names) == set(old_reference_names)) and (\n                result_item.item_status == DocItemStatus.doc_up_to_date\n            ):\n                if set(new_reference_names) <= set(\n                    old_reference_names\n                ):  # 旧的referencer包含新的referencer\n                    result_item.item_status = DocItemStatus.referencer_not_exist\n                else:\n                    result_item.item_status = DocItemStatus.add_new_referencer\n            for _, child in now_older_item.children.items():\n                travel2(child)\n\n        travel2(older_meta.target_repo_hierarchical_tree)\n\n        self.deleted_items_from_older_meta = deleted_items\n\n    @staticmethod\n    def from_project_hierarchy_path(repo_path: str) -> MetaInfo:\n        \"\"\"project_hierarchy_json全是压平的文件，递归的文件目录都在最终的key里面, 把他转换到我们的数据结构\"\"\"\n        project_hierarchy_json_path = os.path.join(repo_path, \"project_hierarchy.json\")\n        logger.info(f\"parsing from {project_hierarchy_json_path}\")\n        if not os.path.exists(project_hierarchy_json_path):\n            raise NotImplementedError(\"Invalid operation detected\")\n\n        with open(project_hierarchy_json_path, \"r\", encoding=\"utf-8\") as reader:\n            project_hierarchy_json = json.load(reader)\n        return MetaInfo.from_project_hierarchy_json(project_hierarchy_json)\n\n    def to_hierarchy_json(self, flash_reference_relation=False):\n        \"\"\"\n        Convert the document metadata to a hierarchical JSON representation.\n\n        Args:\n            flash_reference_relation (bool): If True, the latest bidirectional reference relations will be written back to the meta file.\n\n        Returns:\n            dict: A dictionary representing the hierarchical JSON structure of the document metadata.\n        \"\"\"\n        hierachy_json = {}\n        file_item_list = self.get_all_files()\n        for file_item in file_item_list:\n            file_hierarchy_content = []\n\n            def walk_file(now_obj: DocItem):\n                nonlocal file_hierarchy_content, flash_reference_relation\n                temp_json_obj = now_obj.content\n                temp_json_obj[\"name\"] = now_obj.obj_name\n                temp_json_obj[\"type\"] = now_obj.item_type.to_str()\n                temp_json_obj[\"md_content\"] = now_obj.md_content\n                temp_json_obj[\"item_status\"] = now_obj.item_status.name\n\n                if flash_reference_relation:\n                    temp_json_obj[\"who_reference_me\"] = [\n                        cont.get_full_name(strict=True)\n                        for cont in now_obj.who_reference_me\n                    ]\n                    temp_json_obj[\"reference_who\"] = [\n                        cont.get_full_name(strict=True)\n                        for cont in now_obj.reference_who\n                    ]\n                    temp_json_obj[\"special_reference_type\"] = (\n                        now_obj.special_reference_type\n                    )\n                else:\n                    temp_json_obj[\"who_reference_me\"] = (\n                        now_obj.who_reference_me_name_list\n                    )\n                    temp_json_obj[\"reference_who\"] = now_obj.reference_who_name_list\n                    # temp_json_obj[\"special_reference_type\"] =\n                file_hierarchy_content.append(temp_json_obj)\n\n                for _, child in now_obj.children.items():\n                    walk_file(child)\n\n            for _, child in file_item.children.items():\n                walk_file(child)\n            hierachy_json[file_item.get_full_name()] = file_hierarchy_content\n        return hierachy_json\n\n    @staticmethod\n    def from_project_hierarchy_json(project_hierarchy_json) -> MetaInfo:\n        setting = SettingsManager.get_setting()\n\n        target_meta_info = MetaInfo(\n            # repo_path=repo_path,\n            target_repo_hierarchical_tree=DocItem(  # 根节点\n                item_type=DocItemType._repo,\n                obj_name=\"full_repo\",\n            )\n        )\n\n        for file_name, file_content in tqdm(\n            project_hierarchy_json.items(), desc=\"parsing parent relationship\"\n        ):\n            # 首先parse file archi\n            if not os.path.exists(os.path.join(setting.project.target_repo, file_name)):\n                logger.info(f\"deleted content: {file_name}\")\n                continue\n            elif (\n                os.path.getsize(os.path.join(setting.project.target_repo, file_name))\n                == 0\n            ):\n                logger.info(f\"blank content: {file_name}\")\n                continue\n\n            recursive_file_path = file_name.split(\"/\")\n            pos = 0\n            now_structure = target_meta_info.target_repo_hierarchical_tree\n            while pos < len(recursive_file_path) - 1:\n                if recursive_file_path[pos] not in now_structure.children.keys():\n                    now_structure.children[recursive_file_path[pos]] = DocItem(\n                        item_type=DocItemType._dir,\n                        md_content=\"\",\n                        obj_name=recursive_file_path[pos],\n                    )\n                    now_structure.children[\n                        recursive_file_path[pos]\n                    ].father = now_structure\n                now_structure = now_structure.children[recursive_file_path[pos]]\n                pos += 1\n            if recursive_file_path[-1] not in now_structure.children.keys():\n                now_structure.children[recursive_file_path[pos]] = DocItem(\n                    item_type=DocItemType._file,\n                    obj_name=recursive_file_path[-1],\n                )\n                now_structure.children[recursive_file_path[pos]].father = now_structure\n\n            # 然后parse file内容\n            assert type(file_content) == list\n            file_item = target_meta_info.target_repo_hierarchical_tree.find(\n                recursive_file_path\n            )\n            assert file_item.item_type == DocItemType._file\n            \"\"\"用类线段树的方式：\n            1.先parse所有节点，再找父子关系\n            2.一个节点的父节点，所有包含他的code范围的节点里的，最小的节点\n            复杂度是O(n^2)\n            3.最后来处理节点的type问题\n            \"\"\"\n\n            obj_item_list: List[DocItem] = []\n            for value in file_content:\n                obj_doc_item = DocItem(\n                    obj_name=value[\"name\"],\n                    content=value,\n                    md_content=value[\"md_content\"],\n                    code_start_line=value[\"code_start_line\"],\n                    code_end_line=value[\"code_end_line\"],\n                )\n                if \"item_status\" in value.keys():\n                    obj_doc_item.item_status = DocItemStatus[value[\"item_status\"]]\n                if \"reference_who\" in value.keys():\n                    obj_doc_item.reference_who_name_list = value[\"reference_who\"]\n                if \"special_reference_type\" in value.keys():\n                    obj_doc_item.special_reference_type = value[\n                        \"special_reference_type\"\n                    ]\n                if \"who_reference_me\" in value.keys():\n                    obj_doc_item.who_reference_me_name_list = value[\"who_reference_me\"]\n                obj_item_list.append(obj_doc_item)\n\n            # 接下里寻找可能的父亲\n            for item in obj_item_list:\n                potential_father = None\n                for other_item in obj_item_list:\n\n                    def code_contain(item, other_item) -> bool:\n                        if (\n                            other_item.code_end_line == item.code_end_line\n                            and other_item.code_start_line == item.code_start_line\n                        ):\n                            return False\n                        if (\n                            other_item.code_end_line < item.code_end_line\n                            or other_item.code_start_line > item.code_start_line\n                        ):\n                            return False\n                        return True\n\n                    if code_contain(item, other_item):\n                        if potential_father == None or (\n                            (other_item.code_end_line - other_item.code_start_line)\n                            < (\n                                potential_father.code_end_line\n                                - potential_father.code_start_line\n                            )\n                        ):\n                            potential_father = other_item\n\n                if potential_father == None:\n                    potential_father = file_item\n                item.father = potential_father\n                child_name = item.obj_name\n                if child_name in potential_father.children.keys():\n                    # 如果存在同层次的重名问题，就重命名成 xxx_i的形式\n                    now_name_id = 0\n                    while (\n                        child_name + f\"_{now_name_id}\"\n                    ) in potential_father.children.keys():\n                        now_name_id += 1\n                    child_name = child_name + f\"_{now_name_id}\"\n                    logger.warning(\n                        f\"Name duplicate in {file_item.get_full_name()}: rename to {item.obj_name}->{child_name}\"\n                    )\n                potential_father.children[child_name] = item\n                # print(f\"{potential_father.get_full_name()} -> {item.get_full_name()}\")\n\n            def change_items(now_item: DocItem):\n                if now_item.item_type != DocItemType._file:\n                    if now_item.content[\"type\"] == \"ClassDef\":\n                        now_item.item_type = DocItemType._class\n                    elif now_item.content[\"type\"] == \"FunctionDef\":\n                        now_item.item_type = DocItemType._function\n                        if now_item.father.item_type == DocItemType._class:\n                            now_item.item_type = DocItemType._class_function\n                        elif now_item.father.item_type in [\n                            DocItemType._function,\n                            DocItemType._sub_function,\n                        ]:\n                            now_item.item_type = DocItemType._sub_function\n                for _, child in now_item.children.items():\n                    change_items(child)\n\n            change_items(file_item)\n\n        target_meta_info.target_repo_hierarchical_tree.parse_tree_path(now_path=[])\n        target_meta_info.target_repo_hierarchical_tree.check_depth()\n        return target_meta_info\n\n\nif __name__ == \"__main__\":\n    repo_path = \"some_repo_path\"\n    meta = MetaInfo.from_project_hierarchy_json(repo_path)\n    meta.target_repo_hierarchical_tree.print_recursive()\n    topology_list = meta.get_topology()\n"}
{"type": "source_file", "path": "repo_agent/chat_with_repo/main.py", "content": "import time\n\nfrom repo_agent.chat_with_repo.gradio_interface import GradioInterface\nfrom repo_agent.chat_with_repo.rag import RepoAssistant\nfrom repo_agent.log import logger\nfrom repo_agent.settings import SettingsManager\n\n\ndef main():\n    logger.info(\"Initializing the RepoAgent chat with doc module.\")\n\n    # Load settings\n    setting = SettingsManager.get_setting()\n\n    api_key = setting.chat_completion.openai_api_key.get_secret_value()\n    api_base = str(setting.chat_completion.openai_base_url)\n    db_path = (\n        setting.project.target_repo\n        / setting.project.hierarchy_name\n        / \"project_hierarchy.json\"\n    )\n\n    # Initialize RepoAssistant\n    assistant = RepoAssistant(api_key, api_base, db_path)\n\n    # Extract data\n    md_contents, meta_data = assistant.json_data.extract_data()\n\n    # Create vector store and measure runtime\n    logger.info(\"Starting vector store creation...\")\n    start_time = time.time()\n    assistant.vector_store_manager.create_vector_store(\n        md_contents, meta_data, api_key, api_base\n    )\n    elapsed_time = time.time() - start_time\n    logger.info(f\"Vector store created successfully in {elapsed_time:.2f} seconds.\")\n\n    # Launch Gradio interface\n    GradioInterface(assistant.respond)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "repo_agent/chat_with_repo/gradio_interface.py", "content": "import gradio as gr\nimport markdown\n\nfrom repo_agent.log import logger\n\n\nclass GradioInterface:\n    def __init__(self, respond_function):\n        self.respond = respond_function\n        self.cssa = \"\"\"\n                <style>\n                        .outer-box {\n                            border: 1px solid #333; /* 外框的边框颜色和大小 */\n                            border-radius: 10px; /* 外框的边框圆角效果 */\n                            padding: 10px; /* 外框的内边距 */\n                        }\n\n                        .title {\n                            margin-bottom: 10px; /* 标题和内框之间的距离 */\n                        }\n\n                        .inner-box {\n                            border: 1px solid #555; /* 内框的边框颜色和大小 */\n                            border-radius: 5px; /* 内框的边框圆角效果 */\n                            padding: 10px; /* 内框的内边距 */\n                        }\n\n                        .content {\n                            white-space: pre-wrap; /* 保留空白符和换行符 */\n                            font-size: 16px; /* 内容文字大小 */\n                            height: 405px;\n                            overflow: auto;\n                        }\n                    </style>\n                    <div class=\"outer-box\"\">\n        \n        \"\"\"\n        self.cssb = \"\"\"\n                        </div>\n                    </div>\n                </div>\n        \"\"\"\n        self.setup_gradio_interface()\n\n    def wrapper_respond(self, msg_input, system_input):\n        # 调用原来的 respond 函数\n        msg, output1, output2, output3, code, codex = self.respond(\n            msg_input, system_input\n        )\n        output1 = markdown.markdown(str(output1))\n        output2 = markdown.markdown(str(output2))\n        code = markdown.markdown(str(code))\n        output1 = (\n            self.cssa\n            + \"\"\"\n                          <div class=\"title\">Response</div>\n                            <div class=\"inner-box\">\n                                <div class=\"content\">\n                \"\"\"\n            + str(output1)\n            + \"\"\"\n                        </div>\n                    </div>\n                </div>\n                \"\"\"\n        )\n        output2 = (\n            self.cssa\n            + \"\"\"\n                          <div class=\"title\">Embedding Recall</div>\n                            <div class=\"inner-box\">\n                                <div class=\"content\">\n                \"\"\"\n            + str(output2)\n            + self.cssb\n        )\n        code = (\n            self.cssa\n            + \"\"\"\n                          <div class=\"title\">Code</div>\n                            <div class=\"inner-box\">\n                                <div class=\"content\">\n                \"\"\"\n            + str(code)\n            + self.cssb\n        )\n\n        return msg, output1, output2, output3, code, codex\n\n    def clean(self):\n        msg = \"\"\n        output1 = gr.HTML(\n            self.cssa\n            + \"\"\"\n                                        <div class=\"title\">Response</div>\n                                            <div class=\"inner-box\">\n                                                <div class=\"content\">\n                      \n                                            \"\"\"\n            + self.cssb\n        )\n        output2 = gr.HTML(\n            self.cssa\n            + \"\"\"\n                                        <div class=\"title\">Embedding Recall</div>\n                                            <div class=\"inner-box\">\n                                                <div class=\"content\">\n                                    \n                                            \"\"\"\n            + self.cssb\n        )\n        output3 = \"\"\n        code = gr.HTML(\n            self.cssa\n            + \"\"\"\n                                        <div class=\"title\">Code</div>\n                                            <div class=\"inner-box\">\n                                                <div class=\"content\">\n                                   \n                                            \"\"\"\n            + self.cssb\n        )\n        codex = \"\"\n        return msg, output1, output2, output3, code, codex\n\n    def setup_gradio_interface(self):\n        with gr.Blocks() as demo:\n            gr.Markdown(\"\"\"\n                # RepoAgent: Chat with doc\n            \"\"\")\n            with gr.Tab(\"main chat\"):\n                with gr.Row():\n                    with gr.Column():\n                        msg = gr.Textbox(label=\"Question Input\", lines=4)\n                        system = gr.Textbox(\n                            label=\"(Optional)insturction editing\", lines=4\n                        )\n                        btn = gr.Button(\"Submit\")\n                        btnc = gr.ClearButton()\n                        btnr = gr.Button(\"record\")\n\n                    output1 = gr.HTML(\n                        self.cssa\n                        + \"\"\"\n                                        <div class=\"title\">Response</div>\n                                            <div class=\"inner-box\">\n                                                <div class=\"content\">\n                      \n                                            \"\"\"\n                        + self.cssb\n                    )\n                with gr.Row():\n                    with gr.Column():\n                        # output2 = gr.Textbox(label = \"Embedding recall\")\n                        output2 = gr.HTML(\n                            self.cssa\n                            + \"\"\"\n                                        <div class=\"title\">Embedding Recall</div>\n                                            <div class=\"inner-box\">\n                                                <div class=\"content\">\n                                    \n                                            \"\"\"\n                            + self.cssb\n                        )\n                    code = gr.HTML(\n                        self.cssa\n                        + \"\"\"\n                                        <div class=\"title\">Code</div>\n                                            <div class=\"inner-box\">\n                                                <div class=\"content\">\n                                   \n                                            \"\"\"\n                        + self.cssb\n                    )\n                    with gr.Row():\n                        with gr.Column():\n                            output3 = gr.Textbox(label=\"key words\", lines=2)\n                            output4 = gr.Textbox(label=\"key words code\", lines=14)\n\n            btn.click(\n                self.wrapper_respond,\n                inputs=[msg, system],\n                outputs=[msg, output1, output2, output3, code, output4],\n            )\n            btnc.click(\n                self.clean, outputs=[msg, output1, output2, output3, code, output4]\n            )\n            msg.submit(\n                self.wrapper_respond,\n                inputs=[msg, system],\n                outputs=[msg, output1, output2, output3, code, output4],\n            )  # Press enter to submit\n\n        gr.close_all()\n        demo.queue().launch(share=False, height=800)\n\n\n# 使用方法\nif __name__ == \"__main__\":\n\n    def respond_function(msg, system):\n        RAG = \"\"\"\n\n        \n        \"\"\"\n        return msg, RAG, \"Embedding_recall_output\", \"Key_words_output\", \"Code_output\"\n\n    gradio_interface = GradioInterface(respond_function)\n"}
{"type": "source_file", "path": "repo_agent/log.py", "content": "# repo_agent/log.py\nimport inspect\nimport logging\nimport sys\n\nfrom loguru import logger\n\nlogger = logger.opt(colors=True)\n\"\"\"\nRepoAgent 日志记录器对象。\n\n默认信息:\n- 格式: `[%(asctime)s %(name)s] %(levelname)s: %(message)s`\n- 等级: `INFO` ，根据 `CONFIG[\"log_level\"]` 配置改变\n- 输出: 输出至 stdout\n\n用法示例:\n    ```python\n    from repo_agent.log import logger\n    \n    # 基本消息记录\n    logger.info(\"It <green>works</>!\") # 使用颜色\n\n    # 记录异常信息\n    try:\n        1 / 0\n    except ZeroDivisionError:\n        # 使用 `logger.exception` 可以在记录异常消息时自动附加异常的堆栈跟踪信息。\n        logger.exception(\"ZeroDivisionError occurred\")\n\n    # 记录调试信息\n    logger.debug(f\"Debugging info: {some_debug_variable}\")\n\n    # 记录警告信息\n    logger.warning(\"This is a warning message\")\n\n    # 记录错误信息\n    logger.error(\"An error occurred\")\n    ```\n\n\"\"\"\n\n\nclass InterceptHandler(logging.Handler):\n    def emit(self, record: logging.LogRecord) -> None:\n        # Get corresponding Loguru level if it exists.\n        level: str | int\n        try:\n            level = logger.level(record.levelname).name\n        except ValueError:\n            level = record.levelno\n\n        # Find caller from where originated the logged message.\n        frame, depth = inspect.currentframe(), 0\n        while frame and (depth == 0 or frame.f_code.co_filename == logging.__file__):\n            frame = frame.f_back\n            depth += 1\n\n        logger.opt(depth=depth, exception=record.exc_info).log(\n            level, record.getMessage()\n        )\n\n\ndef set_logger_level_from_config(log_level):\n    \"\"\"\n    Configures the loguru logger with specified log level and integrates it with the standard logging module.\n\n    Args:\n        log_level (str): The log level to set for loguru (e.g., \"DEBUG\", \"INFO\", \"WARNING\").\n\n    This function:\n    - Removes any existing loguru handlers to ensure a clean slate.\n    - Adds a new handler to loguru, directing output to stderr with the specified level.\n      - `enqueue=True` ensures thread-safe logging by using a queue, helpful in multi-threaded contexts.\n      - `backtrace=False` minimizes detailed traceback to prevent overly verbose output.\n      - `diagnose=False` suppresses additional loguru diagnostic information for more concise logs.\n    - Redirects the standard logging output to loguru using the InterceptHandler, allowing loguru to handle\n      all logs consistently across the application.\n    \"\"\"\n    logger.remove()\n    logger.add(\n        sys.stderr, level=log_level, enqueue=True, backtrace=False, diagnose=False\n    )\n\n    # Intercept standard logging\n    logging.basicConfig(handlers=[InterceptHandler()], level=0, force=True)\n\n    logger.success(f\"Log level set to {log_level}!\")\n"}
{"type": "source_file", "path": "repo_agent/chat_with_repo/text_analysis_tool.py", "content": "from llama_index.core.llms.function_calling import FunctionCallingLLM\nfrom llama_index.llms.openai import OpenAI\n\nfrom repo_agent.chat_with_repo.json_handler import JsonFileProcessor\n\n\nclass TextAnalysisTool:\n    def __init__(self, llm: FunctionCallingLLM, db_path):\n        self.jsonsearch = JsonFileProcessor(db_path)\n        self.llm = llm\n        self.db_path = db_path\n\n    def keyword(self, query):\n        prompt = f\"Please provide a list of Code keywords according to the following query, please output no more than 3 keywords, Input: {query}, Output:\"\n        response = self.llm.complete(prompt)\n        return response\n\n    def tree(self, query):\n        prompt = f\"Please analyze the following text and generate a tree structure based on its hierarchy:\\n\\n{query}\"\n        response = self.llm.complete(prompt)\n        return response\n\n    def format_chat_prompt(self, message, instruction):\n        prompt = f\"System:{instruction}\\nUser: {message}\\nAssistant:\"\n        return prompt\n\n    def queryblock(self, message):\n        search_result, md = self.jsonsearch.search_code_contents_by_name(\n            self.db_path, message\n        )\n        return search_result, md\n\n    def list_to_markdown(self, search_result):\n        markdown_str = \"\"\n        # 遍历列表，将每个元素转换为Markdown格式的项\n        for index, content in enumerate(search_result, start=1):\n            # 添加到Markdown字符串中，每个项后跟一个换行符\n            markdown_str += f\"{index}. {content}\\n\\n\"\n\n        return markdown_str\n\n    def nerquery(self, message):\n        instrcution = \"\"\"\nExtract the most relevant class or function base on the following instrcution:\n\nThe output must strictly be a pure function name or class name, without any additional characters.\nFor example:\nPure function names: calculateSum, processData\nPure class names: MyClass, DataProcessor\nThe output function name or class name should be only one.\n        \"\"\"\n        query = f\"{instrcution}\\n\\nThe input is shown as bellow:\\n{message}\\n\\nAnd now directly give your Output:\"\n        response = self.llm.complete(query)\n        # logger.debug(f\"Input: {message}, Output: {response}\")\n        return response\n\n\nif __name__ == \"__main__\":\n    api_base = \"https://api.openai.com/v1\"\n    api_key = \"your_api_key\"\n    log_file = \"your_logfile_path\"\n    llm = OpenAI(api_key=api_key, api_base=api_base)\n    db_path = \"your_database_path\"\n    test = TextAnalysisTool(llm, db_path)\n"}
{"type": "source_file", "path": "repo_agent/chat_with_repo/rag.py", "content": "import json\n\nfrom llama_index.llms.openai import OpenAI\n\nfrom repo_agent.chat_with_repo.json_handler import JsonFileProcessor\nfrom repo_agent.chat_with_repo.prompt import (\n    query_generation_template,\n    rag_ar_template,\n    rag_template,\n    relevance_ranking_chat_template,\n)\nfrom repo_agent.chat_with_repo.text_analysis_tool import TextAnalysisTool\nfrom repo_agent.chat_with_repo.vector_store_manager import VectorStoreManager\nfrom repo_agent.log import logger\n\n\nclass RepoAssistant:\n    def __init__(self, api_key, api_base, db_path):\n        self.db_path = db_path\n        self.md_contents = []\n\n        self.weak_model = OpenAI(\n            api_key=api_key,\n            api_base=api_base,\n            model=\"gpt-4o-mini\",\n        )\n        self.strong_model = OpenAI(\n            api_key=api_key,\n            api_base=api_base,\n            model=\"gpt-4o\",\n        )\n        self.textanslys = TextAnalysisTool(self.weak_model, db_path)\n        self.json_data = JsonFileProcessor(db_path)\n        self.vector_store_manager = VectorStoreManager(top_k=5, llm=self.weak_model)\n\n    def generate_queries(self, query_str: str, num_queries: int = 4):\n        fmt_prompt = query_generation_template.format(\n            num_queries=num_queries - 1, query=query_str\n        )\n        response = self.weak_model.complete(fmt_prompt)\n        queries = response.text.split(\"\\n\")\n        return queries\n\n    def rerank(self, query, docs):  # 这里要防止返回值格式上出问题\n        response = self.weak_model.chat(\n            response_format={\"type\": \"json_object\"},\n            temperature=0,\n            messages=relevance_ranking_chat_template.format_messages(\n                query=query, docs=docs\n            ),\n        )\n        scores = json.loads(response.message.content)[\"documents\"]  # type: ignore\n        logger.debug(f\"scores: {scores}\")\n        sorted_data = sorted(scores, key=lambda x: x[\"relevance_score\"], reverse=True)\n        top_5_contents = [doc[\"content\"] for doc in sorted_data[:5]]\n        return top_5_contents\n\n    def rag(self, query, retrieved_documents):\n        rag_prompt = rag_template.format(\n            query=query, information=\"\\n\\n\".join(retrieved_documents)\n        )\n        response = self.weak_model.complete(rag_prompt)\n        return response.text\n\n    def list_to_markdown(self, list_items):\n        markdown_content = \"\"\n\n        # 对于列表中的每个项目，添加一个带数字的列表项\n        for index, item in enumerate(list_items, start=1):\n            markdown_content += f\"{index}. {item}\\n\"\n\n        return markdown_content\n\n    def rag_ar(self, query, related_code, embedding_recall, project_name):\n        rag_ar_prompt = rag_ar_template.format_messages(\n            query=query,\n            related_code=related_code,\n            embedding_recall=embedding_recall,\n            project_name=project_name,\n        )\n        response = self.strong_model.chat(rag_ar_prompt)\n        return response.message.content\n\n    def respond(self, message, instruction):\n        \"\"\"\n        Respond to a user query by processing input, querying the vector store,\n        reranking results, and generating a final response.\n        \"\"\"\n        logger.debug(\"Starting response generation.\")\n\n        # Step 1: Format the chat prompt\n        prompt = self.textanslys.format_chat_prompt(message, instruction)\n        logger.debug(f\"Formatted prompt: {prompt}\")\n\n        questions = self.textanslys.keyword(prompt)\n        logger.debug(f\"Generated keywords from prompt: {questions}\")\n\n        # Step 2: Generate additional queries\n        prompt_queries = self.generate_queries(prompt, 3)\n        logger.debug(f\"Generated queries: {prompt_queries}\")\n\n        all_results = []\n        all_documents = []\n\n        # Step 3: Query the VectorStoreManager for each query\n        for query in prompt_queries:\n            logger.debug(f\"Querying vector store with: {query}\")\n            query_results = self.vector_store_manager.query_store(query)\n            logger.debug(f\"Results for query '{query}': {query_results}\")\n            all_results.extend(query_results)\n\n        # Step 4: Deduplicate results by content\n        unique_results = {result[\"text\"]: result for result in all_results}.values()\n        unique_documents = [result[\"text\"] for result in unique_results]\n        logger.debug(f\"Unique documents: {unique_documents}\")\n\n        unique_code = [\n            result.get(\"metadata\", {}).get(\"code_content\") for result in unique_results\n        ]\n        logger.debug(f\"Unique code content: {unique_code}\")\n\n        # Step 5: Rerank documents based on relevance\n        retrieved_documents = self.rerank(message, unique_documents)\n        logger.debug(f\"Reranked documents: {retrieved_documents}\")\n\n        # Step 6: Generate a response using RAG (Retrieve and Generate)\n        response = self.rag(prompt, retrieved_documents)\n        chunkrecall = self.list_to_markdown(retrieved_documents)\n        logger.debug(f\"RAG-generated response: {response}\")\n        logger.debug(f\"Markdown chunk recall: {chunkrecall}\")\n\n        bot_message = str(response)\n        logger.debug(f\"Initial bot_message: {bot_message}\")\n\n        # Step 7: Perform NER and queryblock processing\n        keyword = str(self.textanslys.nerquery(bot_message))\n        keywords = str(self.textanslys.nerquery(str(prompt) + str(questions)))\n        logger.debug(f\"Extracted keywords: {keyword}, {keywords}\")\n\n        codez, mdz = self.textanslys.queryblock(keyword)\n        codey, mdy = self.textanslys.queryblock(keywords)\n\n        # Ensure all returned items are lists\n        codez = codez if isinstance(codez, list) else [codez]\n        mdz = mdz if isinstance(mdz, list) else [mdz]\n        codey = codey if isinstance(codey, list) else [codey]\n        mdy = mdy if isinstance(mdy, list) else [mdy]\n\n        # Step 8: Merge and deduplicate results\n        codex = list(dict.fromkeys(codez + codey))\n        md = list(dict.fromkeys(mdz + mdy))\n        unique_mdx = list(set([item for sublist in md for item in sublist]))\n        uni_codex = list(dict.fromkeys(codex))\n        uni_md = list(dict.fromkeys(unique_mdx))\n\n        # Convert to Markdown format\n        codex_md = self.textanslys.list_to_markdown(uni_codex)\n        retrieved_documents = list(dict.fromkeys(retrieved_documents + uni_md))\n\n        # Final rerank and response generation\n        retrieved_documents = self.rerank(message, retrieved_documents[:6])\n        logger.debug(f\"Final retrieved documents after rerank: {retrieved_documents}\")\n\n        uni_code = self.rerank(\n            message, list(dict.fromkeys(uni_codex + unique_code))[:6]\n        )\n        logger.debug(f\"Final unique code after rerank: {uni_code}\")\n\n        unique_code_md = self.textanslys.list_to_markdown(unique_code)\n        logger.debug(f\"Unique code in Markdown: {unique_code_md}\")\n\n        # Generate final response using RAG_AR\n        bot_message = self.rag_ar(prompt, uni_code, retrieved_documents, \"test\")\n        logger.debug(f\"Final bot_message after RAG_AR: {bot_message}\")\n\n        return message, bot_message, chunkrecall, questions, unique_code_md, codex_md\n"}
{"type": "source_file", "path": "repo_agent/utils/meta_info_utils.py", "content": "import itertools\nimport os\n\nimport git\nfrom colorama import Fore, Style\n\nfrom repo_agent.log import logger\nfrom repo_agent.settings import SettingsManager\n\nlatest_verison_substring = \"_latest_version.py\"\n\n\ndef make_fake_files():\n    \"\"\"根据git status检测暂存区信息。如果有文件：\n    1. 新增文件，没有add。无视\n    2. 修改文件内容，没有add，原始文件重命名为fake_file，新建原本的文件名内容为git status中的文件内容\n    3. 删除文件，没有add，原始文件重命名为fake_file，新建原本的文件名内容为git status中的文件内容\n    注意: 目标仓库的文件不能以latest_verison_substring结尾\n    \"\"\"\n    delete_fake_files()\n    setting = SettingsManager.get_setting()\n\n    repo = git.Repo(setting.project.target_repo)\n    unstaged_changes = repo.index.diff(None)  # 在git status里，但是有修改没提交\n    untracked_files = repo.untracked_files  # 在文件系统里，但没在git里的文件\n\n    jump_files = []  # 这里面的内容不parse、不生成文档，并且引用关系也不计算他们\n    for file_name in untracked_files:\n        if file_name.endswith(\".py\"):\n            print(\n                f\"{Fore.LIGHTMAGENTA_EX}[SKIP untracked files]: {Style.RESET_ALL}{file_name}\"\n            )\n            jump_files.append(file_name)\n    for diff_file in unstaged_changes.iter_change_type(\n        \"A\"\n    ):  # 新增的、没有add的文件，都不处理\n        if diff_file.a_path.endswith(latest_verison_substring):\n            logger.error(\n                \"FAKE_FILE_IN_GIT_STATUS detected! suggest to use `delete_fake_files` and re-generate document\"\n            )\n            exit()\n        jump_files.append(diff_file.a_path)\n\n    file_path_reflections = {}\n    for diff_file in itertools.chain(\n        unstaged_changes.iter_change_type(\"M\"), unstaged_changes.iter_change_type(\"D\")\n    ):  # 获取修改过的文件\n        if diff_file.a_path.endswith(latest_verison_substring):\n            logger.error(\n                \"FAKE_FILE_IN_GIT_STATUS detected! suggest to use `delete_fake_files` and re-generate document\"\n            )\n            exit()\n        now_file_path = diff_file.a_path  # 针对repo_path的相对路径\n        if now_file_path.endswith(\".py\"):\n            raw_file_content = diff_file.a_blob.data_stream.read().decode(\"utf-8\")\n            latest_file_path = now_file_path[:-3] + latest_verison_substring\n            if os.path.exists(os.path.join(setting.project.target_repo, now_file_path)):\n                os.rename(\n                    os.path.join(setting.project.target_repo, now_file_path),\n                    os.path.join(setting.project.target_repo, latest_file_path),\n                )\n\n                print(\n                    f\"{Fore.LIGHTMAGENTA_EX}[Save Latest Version of Code]: {Style.RESET_ALL}{now_file_path} -> {latest_file_path}\"\n                )\n            else:\n                print(\n                    f\"{Fore.LIGHTMAGENTA_EX}[Create Temp-File for Deleted(But not Staged) Files]: {Style.RESET_ALL}{now_file_path} -> {latest_file_path}\"\n                )\n                with open(\n                    os.path.join(setting.project.target_repo, latest_file_path), \"w\"\n                ) as writer:\n                    pass\n            with open(\n                os.path.join(setting.project.target_repo, now_file_path), \"w\"\n            ) as writer:\n                writer.write(raw_file_content)\n            file_path_reflections[now_file_path] = latest_file_path  # real指向fake\n    return file_path_reflections, jump_files\n\n\ndef delete_fake_files():\n    \"\"\"在任务执行完成以后，删除所有的fake_file\"\"\"\n    setting = SettingsManager.get_setting()\n\n    def gci(filepath):\n        # 遍历filepath下所有文件，包括子目录\n        files = os.listdir(filepath)\n        for fi in files:\n            fi_d = os.path.join(filepath, fi)\n            if os.path.isdir(fi_d):\n                gci(fi_d)\n            elif fi_d.endswith(latest_verison_substring):\n                origin_name = fi_d.replace(latest_verison_substring, \".py\")\n                os.remove(origin_name)\n                if os.path.getsize(fi_d) == 0:\n                    print(\n                        f\"{Fore.LIGHTRED_EX}[Deleting Temp File]: {Style.RESET_ALL}{fi_d[len(str(setting.project.target_repo)):]}, {origin_name[len(str(setting.project.target_repo)):]}\"\n                    )  # type: ignore\n                    os.remove(fi_d)\n                else:\n                    print(\n                        f\"{Fore.LIGHTRED_EX}[Recovering Latest Version]: {Style.RESET_ALL}{origin_name[len(str(setting.project.target_repo)):]} <- {fi_d[len(str(setting.project.target_repo)):]}\"\n                    )  # type: ignore\n                    os.rename(fi_d, origin_name)\n\n    gci(setting.project.target_repo)\n"}
{"type": "source_file", "path": "repo_agent/multi_task_dispatch.py", "content": "from __future__ import annotations\n\nimport random\nimport threading\nimport time\nfrom typing import Any, Callable, Dict, List\n\nfrom colorama import Fore, Style\n\n\nclass Task:\n    def __init__(self, task_id: int, dependencies: List[Task], extra_info: Any = None):\n        self.task_id = task_id\n        self.extra_info = extra_info\n        self.dependencies = dependencies\n        self.status = 0  # 任务状态：0未开始，1正在进行，2已经完成，3出错了\n\n\nclass TaskManager:\n    def __init__(self):\n        \"\"\"\n        Initialize a MultiTaskDispatch object.\n\n        This method initializes the MultiTaskDispatch object by setting up the necessary attributes.\n\n        Attributes:\n        - task_dict (Dict[int, Task]): A dictionary that maps task IDs to Task objects.\n        - task_lock (threading.Lock): A lock used for thread synchronization when accessing the task_dict.\n        - now_id (int): The current task ID.\n        - query_id (int): The current query ID.\n        - sync_func (None): A placeholder for a synchronization function.\n\n        \"\"\"\n        self.task_dict: Dict[int, Task] = {}\n        self.task_lock = threading.Lock()\n        self.now_id = 0\n        self.query_id = 0\n\n    @property\n    def all_success(self) -> bool:\n        return len(self.task_dict) == 0\n\n    def add_task(self, dependency_task_id: List[int], extra=None) -> int:\n        \"\"\"\n        Adds a new task to the task dictionary.\n\n        Args:\n            dependency_task_id (List[int]): List of task IDs that the new task depends on.\n            extra (Any, optional): Extra information associated with the task. Defaults to None.\n\n        Returns:\n            int: The ID of the newly added task.\n        \"\"\"\n        with self.task_lock:\n            depend_tasks = [self.task_dict[task_id] for task_id in dependency_task_id]\n            self.task_dict[self.now_id] = Task(\n                task_id=self.now_id, dependencies=depend_tasks, extra_info=extra\n            )\n            self.now_id += 1\n            return self.now_id - 1\n\n    def get_next_task(self, process_id: int):\n        \"\"\"\n        Get the next task for a given process ID.\n\n        Args:\n            process_id (int): The ID of the process.\n\n        Returns:\n            tuple: A tuple containing the next task object and its ID.\n                   If there are no available tasks, returns (None, -1).\n        \"\"\"\n        with self.task_lock:\n            self.query_id += 1\n            for task_id in self.task_dict.keys():\n                ready = (\n                    len(self.task_dict[task_id].dependencies) == 0\n                ) and self.task_dict[task_id].status == 0\n                if ready:\n                    self.task_dict[task_id].status = 1\n                    print(\n                        f\"{Fore.RED}[process {process_id}]{Style.RESET_ALL}: get task({task_id}), remain({len(self.task_dict)})\"\n                    )\n                    return self.task_dict[task_id], task_id\n            return None, -1\n\n    def mark_completed(self, task_id: int):\n        \"\"\"\n        Marks a task as completed and removes it from the task dictionary.\n\n        Args:\n            task_id (int): The ID of the task to mark as completed.\n\n        \"\"\"\n        with self.task_lock:\n            target_task = self.task_dict[task_id]\n            for task in self.task_dict.values():\n                if target_task in task.dependencies:\n                    task.dependencies.remove(target_task)\n            self.task_dict.pop(task_id)  # 从任务字典中移除\n\n\ndef worker(task_manager, process_id: int, handler: Callable):\n    \"\"\"\n    Worker function that performs tasks assigned by the task manager.\n\n    Args:\n        task_manager: The task manager object that assigns tasks to workers.\n        process_id (int): The ID of the current worker process.\n        handler (Callable): The function that handles the tasks.\n\n    Returns:\n        None\n    \"\"\"\n    while True:\n        if task_manager.all_success:\n            return\n        task, task_id = task_manager.get_next_task(process_id)\n        if task is None:\n            time.sleep(0.5)\n            continue\n        # print(f\"will perform task: {task_id}\")\n        handler(task.extra_info)\n        task_manager.mark_completed(task.task_id)\n        # print(f\"task complete: {task_id}\")\n\n\nif __name__ == \"__main__\":\n    task_manager = TaskManager()\n\n    def some_function():  # 随机睡一会\n        time.sleep(random.random() * 3)\n\n    # 添加任务，例如：\n    i1 = task_manager.add_task(some_function, [])  # type: ignore\n    i2 = task_manager.add_task(some_function, [])  # type: ignore\n    i3 = task_manager.add_task(some_function, [i1])  # type: ignore\n    i4 = task_manager.add_task(some_function, [i2, i3])  # type: ignore\n    i5 = task_manager.add_task(some_function, [i2, i3])  # type: ignore\n    i6 = task_manager.add_task(some_function, [i1])  # type: ignore\n\n    threads = [threading.Thread(target=worker, args=(task_manager,)) for _ in range(4)]\n    for thread in threads:\n        thread.start()\n    for thread in threads:\n        thread.join()\n"}
{"type": "source_file", "path": "repo_agent/project_manager.py", "content": "import os\n\nimport jedi\n\n\nclass ProjectManager:\n    def __init__(self, repo_path, project_hierarchy):\n        self.repo_path = repo_path\n        self.project = jedi.Project(self.repo_path)\n        self.project_hierarchy = os.path.join(\n            self.repo_path, project_hierarchy, \"project_hierarchy.json\"\n        )\n\n    def get_project_structure(self):\n        \"\"\"\n        Returns the structure of the project by recursively walking through the directory tree.\n\n        Returns:\n            str: The project structure as a string.\n        \"\"\"\n\n        def walk_dir(root, prefix=\"\"):\n            structure.append(prefix + os.path.basename(root))\n            new_prefix = prefix + \"  \"\n            for name in sorted(os.listdir(root)):\n                if name.startswith(\".\"):  # 忽略隐藏文件和目录\n                    continue\n                path = os.path.join(root, name)\n                if os.path.isdir(path):\n                    walk_dir(path, new_prefix)\n                elif os.path.isfile(path) and name.endswith(\".py\"):\n                    structure.append(new_prefix + name)\n\n        structure = []\n        walk_dir(self.repo_path)\n        return \"\\n\".join(structure)\n\n    def build_path_tree(self, who_reference_me, reference_who, doc_item_path):\n        from collections import defaultdict\n\n        def tree():\n            return defaultdict(tree)\n\n        path_tree = tree()\n\n        # 构建 who_reference_me 和 reference_who 的树\n        for path_list in [who_reference_me, reference_who]:\n            for path in path_list:\n                parts = path.split(os.sep)\n                node = path_tree\n                for part in parts:\n                    node = node[part]\n\n        # 处理 doc_item_path\n        parts = doc_item_path.split(os.sep)\n        parts[-1] = \"✳️\" + parts[-1]  # 在最后一个对象前面加上星号\n        node = path_tree\n        for part in parts:\n            node = node[part]\n\n        def tree_to_string(tree, indent=0):\n            s = \"\"\n            for key, value in sorted(tree.items()):\n                s += \"    \" * indent + key + \"\\n\"\n                if isinstance(value, dict):\n                    s += tree_to_string(value, indent + 1)\n            return s\n\n        return tree_to_string(path_tree)\n\n\nif __name__ == \"__main__\":\n    project_manager = ProjectManager(repo_path=\"\", project_hierarchy=\"\")\n    print(project_manager.get_project_structure())\n"}
{"type": "source_file", "path": "repo_agent/prompt.py", "content": "from llama_index.core import ChatPromptTemplate\nfrom llama_index.core.llms import ChatMessage, MessageRole\n\ndoc_generation_instruction = (\n    \"You are an AI documentation assistant, and your task is to generate documentation based on the given code of an object. \"\n    \"The purpose of the documentation is to help developers and beginners understand the function and specific usage of the code.\\n\\n\"\n    \"Currently, you are in a project{project_structure_prefix}\\n\"\n    \"{project_structure}\\n\\n\"\n    \"The path of the document you need to generate in this project is {file_path}.\\n\"\n    'Now you need to generate a document for a {code_type_tell}, whose name is \"{code_name}\".\\n\\n'\n    \"The content of the code is as follows:\\n\"\n    \"{code_content}\\n\\n\"\n    \"{reference_letter}\\n\"\n    \"{referencer_content}\\n\\n\"\n    \"Please generate a detailed explanation document for this object based on the code of the target object itself {combine_ref_situation}.\\n\\n\"\n    \"Please write out the function of this {code_type_tell} in bold plain text, followed by a detailed analysis in plain text \"\n    \"(including all details), in language {language} to serve as the documentation for this part of the code.\\n\\n\"\n    \"The standard format is as follows:\\n\\n\"\n    \"**{code_name}**: The function of {code_name} is XXX. (Only code name and one sentence function description are required)\\n\"\n    \"**{parameters_or_attribute}**: The {parameters_or_attribute} of this {code_type_tell}.\\n\"\n    \"· parameter1: XXX\\n\"\n    \"· parameter2: XXX\\n\"\n    \"· ...\\n\"\n    \"**Code Description**: The description of this {code_type_tell}.\\n\"\n    \"(Detailed and CERTAIN code analysis and description...{has_relationship})\\n\"\n    \"**Note**: Points to note about the use of the code\\n\"\n    \"{have_return_tell}\\n\\n\"\n    \"Please note:\\n\"\n    \"- Any part of the content you generate SHOULD NOT CONTAIN Markdown hierarchical heading and divider syntax.\\n\"\n    \"- Write mainly in the desired language. If necessary, you can write with some English words in the analysis and description \"\n    \"to enhance the document's readability because you do not need to translate the function name or variable name into the target language.\\n\"\n)\n\ndocumentation_guideline = (\n    \"Keep in mind that your audience is document readers, so use a deterministic tone to generate precise content and don't let them know \"\n    \"you're provided with code snippet and documents. AVOID ANY SPECULATION and inaccurate descriptions! Now, provide the documentation \"\n    \"for the target object in {language} in a professional way.\"\n)\n\n\nmessage_templates = [\n    ChatMessage(content=doc_generation_instruction, role=MessageRole.SYSTEM),\n    ChatMessage(\n        content=documentation_guideline,\n        role=MessageRole.USER,\n    ),\n]\n\nchat_template = ChatPromptTemplate(message_templates=message_templates)\n"}
{"type": "source_file", "path": "repo_agent/chat_with_repo/prompt.py", "content": "from llama_index.core import ChatPromptTemplate, PromptTemplate\nfrom llama_index.core.llms import ChatMessage, MessageRole\n\n# Query Generation Prompt\nquery_generation_prompt_str = (\n    \"You are a helpful assistant that generates multiple search queries based on a \"\n    \"single input query. Generate {num_queries} search queries, one on each line, \"\n    \"related to the following input query:\\n\"\n    \"Query: {query}\\n\"\n    \"Queries:\\n\"\n)\nquery_generation_template = PromptTemplate(query_generation_prompt_str)\n\n# Relevance Ranking Prompt\nrelevance_ranking_instruction = (\n    \"You are an expert relevance ranker. Given a list of documents and a query, your job is to determine how relevant each document is for answering the query. \"\n    \"Your output is JSON, which is a list of documents. Each document has two fields, content and relevance_score. relevance_score is from 0.0 to 100.0. \"\n    \"Higher relevance means higher score.\"\n)\nrelevance_ranking_guideline = \"Query: {query} Docs: {docs}\"\n\nrelevance_ranking_message_template = [\n    ChatMessage(content=relevance_ranking_instruction, role=MessageRole.SYSTEM),\n    ChatMessage(\n        content=relevance_ranking_guideline,\n        role=MessageRole.USER,\n    ),\n]\nrelevance_ranking_chat_template = ChatPromptTemplate(\n    message_templates=relevance_ranking_message_template\n)\n\n# RAG (Retrieve and Generate) Prompt\nrag_prompt_str = (\n    \"You are a helpful assistant in repository Q&A. Users will ask questions about something contained in a repository. \"\n    \"You will be shown the user's question, and the relevant information from the repository. Answer the user's question only with information given.\\n\\n\"\n    \"Question: {query}.\\n\\n\"\n    \"Information: {information}\"\n)\nrag_template = PromptTemplate(rag_prompt_str)\n\n# RAG_AR (Advanced RAG) Prompt\nrag_ar_prompt_str = (\n    \"You are a helpful Repository-Level Software Q&A assistant. Your task is to answer users' questions based on the given information about a software repository, \"\n    \"including related code and documents.\\n\\n\"\n    \"Currently, you're in the {project_name} project. The user's question is:\\n\"\n    \"{query}\\n\\n\"\n    \"Now, you are given related code and documents as follows:\\n\\n\"\n    \"-------------------Code-------------------\\n\"\n    \"Some most likely related code snippets recalled by the retriever are:\\n\"\n    \"{related_code}\\n\\n\"\n    \"-------------------Document-------------------\\n\"\n    \"Some most relevant documents recalled by the retriever are:\\n\"\n    \"{embedding_recall}\\n\\n\"\n    \"Please note:   \\n\"\n    \"1. All the provided recall results are related to the current project {project_name}. Please filter useful information according to the user's question and provide corresponding answers or solutions.\\n\"\n    \"2. Ensure that your responses are accurate and detailed. Present specific answers in a professional manner and tone.\\n\"\n    \"3. The user's question may be asked in any language. You must respond **in the same language** as the user's question, even if the input language is not English.\\n\"\n    \"4. If you find the user's question completely unrelated to the provided information or if you believe you cannot provide an accurate answer, kindly decline. Note: DO NOT fabricate any non-existent information.\\n\\n\"\n    \"Now, focusing on the user's query, and incorporating the given information to offer a specific, detailed, and professional answer IN THE SAME LANGUAGE AS the user's question.\"\n)\n\n\nrag_ar_template = PromptTemplate(rag_ar_prompt_str)\n"}
{"type": "source_file", "path": "repo_agent/main.py", "content": "from importlib import metadata\n\nimport click\nfrom pydantic import ValidationError\n\nfrom repo_agent.doc_meta_info import DocItem, MetaInfo\nfrom repo_agent.log import logger, set_logger_level_from_config\nfrom repo_agent.runner import Runner, delete_fake_files\nfrom repo_agent.settings import SettingsManager, LogLevel\nfrom repo_agent.utils.meta_info_utils import delete_fake_files, make_fake_files\n\ntry:\n    version_number = metadata.version(\"repoagent\")\nexcept metadata.PackageNotFoundError:\n    version_number = \"0.0.0\"\n\n\n@click.group()\n@click.version_option(version_number)\ndef cli():\n    \"\"\"An LLM-Powered Framework for Repository-level Code Documentation Generation.\"\"\"\n    pass\n\n\ndef handle_setting_error(e: ValidationError):\n    \"\"\"Handle configuration errors for settings.\"\"\"\n    # 输出更详细的字段缺失信息，使用颜色区分\n    for error in e.errors():\n        field = error[\"loc\"][-1]\n        if error[\"type\"] == \"missing\":\n            message = click.style(\n                f\"Missing required field `{field}`. Please set the `{field}` environment variable.\",\n                fg=\"yellow\",\n            )\n        else:\n            message = click.style(error[\"msg\"], fg=\"yellow\")\n        click.echo(message, err=True, color=True)\n\n    # 使用 ClickException 优雅地退出程序\n    raise click.ClickException(\n        click.style(\n            \"Program terminated due to configuration errors.\", fg=\"red\", bold=True\n        )\n    )\n\n\n@cli.command()\n@click.option(\n    \"--model\",\n    \"-m\",\n    default=\"gpt-4o-mini\",\n    show_default=True,\n    help=\"Specifies the model to use for completion.\",\n    type=str,\n)\n@click.option(\n    \"--temperature\",\n    \"-t\",\n    default=0.2,\n    show_default=True,\n    help=\"Sets the generation temperature for the model. Lower values make the model more deterministic.\",\n    type=float,\n)\n@click.option(\n    \"--request-timeout\",\n    \"-r\",\n    default=60,\n    show_default=True,\n    help=\"Defines the timeout in seconds for the API request.\",\n    type=int,\n)\n@click.option(\n    \"--base-url\",\n    \"-b\",\n    default=\"https://api.openai.com/v1\",\n    show_default=True,\n    help=\"The base URL for the API calls.\",\n    type=str,\n)\n@click.option(\n    \"--target-repo-path\",\n    \"-tp\",\n    default=\"\",\n    show_default=True,\n    help=\"The file system path to the target repository. This path is used as the root for documentation generation.\",\n    type=click.Path(file_okay=False),\n)\n@click.option(\n    \"--hierarchy-path\",\n    \"-hp\",\n    default=\".project_doc_record\",\n    show_default=True,\n    help=\"The name or path for the project hierarchy file, used to organize documentation structure.\",\n    type=str,\n)\n@click.option(\n    \"--markdown-docs-path\",\n    \"-mdp\",\n    default=\"markdown_docs\",\n    show_default=True,\n    help=\"The folder path where Markdown documentation will be stored or generated.\",\n    type=str,\n)\n@click.option(\n    \"--ignore-list\",\n    \"-i\",\n    default=\"\",\n    help=\"A comma-separated list of files or directories to ignore during documentation generation.\",\n)\n@click.option(\n    \"--language\",\n    \"-l\",\n    default=\"English\",\n    show_default=True,\n    help=\"The ISO 639 code or language name for the documentation. \",\n    type=str,\n)\n@click.option(\n    \"--max-thread-count\",\n    \"-mtc\",\n    default=4,\n    show_default=True,\n)\n@click.option(\n    \"--log-level\",\n    \"-ll\",\n    default=\"INFO\",\n    show_default=True,\n    help=\"Sets the logging level (e.g., DEBUG, INFO, WARNING, ERROR, CRITICAL) for the application. Default is INFO.\",\n    type=click.Choice([level.value for level in LogLevel], case_sensitive=False),\n)\n@click.option(\n    \"--print-hierarchy\",\n    \"-pr\",\n    is_flag=True,\n    show_default=True,\n    default=False,\n    help=\"If set, prints the hierarchy of the target repository when finished running the main task.\",\n)\ndef run(\n    model,\n    temperature,\n    request_timeout,\n    base_url,\n    target_repo_path,\n    hierarchy_path,\n    markdown_docs_path,\n    ignore_list,\n    language,\n    max_thread_count,\n    log_level,\n    print_hierarchy,\n):\n    \"\"\"Run the program with the specified parameters.\"\"\"\n    try:\n        # Fetch and validate the settings using the SettingsManager\n        setting = SettingsManager.initialize_with_params(\n            target_repo=target_repo_path,\n            hierarchy_name=hierarchy_path,\n            markdown_docs_name=markdown_docs_path,\n            ignore_list=[item.strip() for item in ignore_list.split(\",\") if item],\n            language=language,\n            log_level=log_level,\n            model=model,\n            temperature=temperature,\n            request_timeout=request_timeout,\n            openai_base_url=base_url,\n            max_thread_count=max_thread_count,\n        )\n        set_logger_level_from_config(log_level=log_level)\n    except ValidationError as e:\n        handle_setting_error(e)\n        return\n\n    # 如果设置成功，则运行任务\n    runner = Runner()\n    runner.run()\n    logger.success(\"Documentation task completed.\")\n    if print_hierarchy:\n        runner.meta_info.target_repo_hierarchical_tree.print_recursive()\n        logger.success(\"Hierarchy printed.\")\n\n\n@cli.command()\ndef clean():\n    \"\"\"Clean the fake files generated by the documentation process.\"\"\"\n    delete_fake_files()\n    logger.success(\"Fake files have been cleaned up.\")\n\n\n@cli.command()\ndef diff():\n    \"\"\"Check for changes and print which documents will be updated or generated.\"\"\"\n    try:\n        # Fetch and validate the settings using the SettingsManager\n        setting = SettingsManager.get_setting()\n    except ValidationError as e:\n        handle_setting_error(e)\n        return\n\n    runner = Runner()\n    if runner.meta_info.in_generation_process:  # 如果不是在生成过程中，就开始检测变更\n        click.echo(\"This command only supports pre-check\")\n        raise click.Abort()\n\n    file_path_reflections, jump_files = make_fake_files()\n    new_meta_info = MetaInfo.init_meta_info(file_path_reflections, jump_files)\n    new_meta_info.load_doc_from_older_meta(runner.meta_info)\n    delete_fake_files()\n\n    DocItem.check_has_task(\n        new_meta_info.target_repo_hierarchical_tree,\n        ignore_list=setting.project.ignore_list,\n    )\n    if new_meta_info.target_repo_hierarchical_tree.has_task:\n        click.echo(\"The following docs will be generated/updated:\")\n        new_meta_info.target_repo_hierarchical_tree.print_recursive(\n            diff_status=True, ignore_list=setting.project.ignore_list\n        )\n    else:\n        click.echo(\"No docs will be generated/updated, check your source-code update\")\n\n\n@cli.command()\ndef chat_with_repo():\n    \"\"\"\n    Start an interactive chat session with the repository.\n    \"\"\"\n    try:\n        # Fetch and validate the settings using the SettingsManager\n        setting = SettingsManager.get_setting()\n    except ValidationError as e:\n        # Handle configuration errors if the settings are invalid\n        handle_setting_error(e)\n        return\n\n    from repo_agent.chat_with_repo import main\n\n    main()\n\n\nif __name__ == \"__main__\":\n    cli()\n"}
{"type": "source_file", "path": "repo_agent/__main__.py", "content": "from .main import cli\n\nif __name__ == \"__main__\":\n    cli()\n"}
{"type": "source_file", "path": "repo_agent/change_detector.py", "content": "import os\nimport re\nimport subprocess\n\nimport git\nfrom colorama import Fore, Style\n\nfrom repo_agent.file_handler import FileHandler\nfrom repo_agent.settings import SettingsManager\n\n\nclass ChangeDetector:\n    \"\"\"\n    这个类需要处理文件的差异和变更检测，它可能会用到 FileHandler 类来访问文件系统。\n    ChangeDetector 类的核心在于能够识别自上次提交以来文件的变更。\n    \"\"\"\n\n    def __init__(self, repo_path):\n        \"\"\"\n        Initializes a ChangeDetector object.\n\n        Parameters:\n        repo_path (str): The path to the repository.\n\n        Returns:\n        None\n        \"\"\"\n        self.repo_path = repo_path\n        self.repo = git.Repo(repo_path)\n\n    def get_staged_pys(self):\n        \"\"\"\n        Get added python files in the repository that have been staged.\n\n        This function only tracks the changes of Python files in Git that have been staged,\n        i.e., the files that have been added using `git add`.\n\n        Returns:\n            dict: A dictionary of changed Python files, where the keys are the file paths and the values are booleans indicating whether the file is newly created or not.\n\n        \"\"\"\n        repo = self.repo\n        staged_files = {}\n        # Detect Staged Changes\n        # Please note! The logic of the GitPython library is different from git. Here, the R=True parameter is used to reverse the version comparison logic.\n        # In the GitPython library, repo.index.diff('HEAD') compares the staging area (index) as the new state with the original HEAD commit (old state). This means that if there is a new file in the current staging area, it will be shown as non-existent in HEAD, i.e., \"deleted\".\n        # R=True reverses this logic, correctly treating the last commit (HEAD) as the old state and comparing it with the current staging area (new state) (Index). In this case, a new file in the staging area will correctly show as added because it does not exist in HEAD.\n        diffs = repo.index.diff(\"HEAD\", R=True)\n\n        for diff in diffs:\n            if diff.change_type in [\"A\", \"M\"] and diff.a_path.endswith(\".py\"):\n                is_new_file = diff.change_type == \"A\"\n                staged_files[diff.a_path] = is_new_file\n\n        return staged_files\n\n    def get_file_diff(self, file_path, is_new_file):\n        \"\"\"\n        The function's purpose is to retrieve the changes made to a specific file. For new files, it uses git diff --staged to get the differences.\n        Args:\n            file_path (str): The relative path of the file\n            is_new_file (bool): Indicates whether the file is a new file\n        Returns:\n            list: List of changes made to the file\n        \"\"\"\n        repo = self.repo\n\n        if is_new_file:\n            # For new files, first add them to the staging area.\n            add_command = f\"git -C {repo.working_dir} add {file_path}\"\n            subprocess.run(add_command, shell=True, check=True)\n\n            # Get the diff from the staging area.\n            diffs = repo.git.diff(\"--staged\", file_path).splitlines()\n        else:\n            # For non-new files, get the diff from HEAD.\n            diffs = repo.git.diff(\"HEAD\", file_path).splitlines()\n\n        return diffs\n\n    def parse_diffs(self, diffs):\n        \"\"\"\n        Parse the difference content, extract the added and deleted object information, the object can be a class or a function.\n        Output example: {'added': [(86, '    '), (87, '    def to_json_new(self, comments = True):'), (88, '        data = {'), (89, '            \"name\": self.node_name,')...(95, '')], 'removed': []}\n        In the above example, PipelineEngine and AI_give_params are added objects, and there are no deleted objects.\n        But the addition here does not mean that it is a newly added object, because in git diff, the modification of a line is represented as deletion and addition in diff.\n        So for the modified content, it will also be represented as this object has undergone an added operation.\n\n        If you need to know clearly that an object is newly added, you need to use the get_added_objs() function.\n        Args:\n            diffs (list): A list containing difference content. Obtained by the get_file_diff() function inside the class.\n\n        Returns:\n            dict: A dictionary containing added and deleted line information, the format is {'added': set(), 'removed': set()}\n        \"\"\"\n        changed_lines = {\"added\": [], \"removed\": []}\n        line_number_current = 0\n        line_number_change = 0\n\n        for line in diffs:\n            # 检测行号信息，例如 \"@@ -43,33 +43,40 @@\"\n            line_number_info = re.match(r\"@@ \\-(\\d+),\\d+ \\+(\\d+),\\d+ @@\", line)\n            if line_number_info:\n                line_number_current = int(line_number_info.group(1))\n                line_number_change = int(line_number_info.group(2))\n                continue\n\n            if line.startswith(\"+\") and not line.startswith(\"+++\"):\n                changed_lines[\"added\"].append((line_number_change, line[1:]))\n                line_number_change += 1\n            elif line.startswith(\"-\") and not line.startswith(\"---\"):\n                changed_lines[\"removed\"].append((line_number_current, line[1:]))\n                line_number_current += 1\n            else:\n                # 对于没有变化的行，两者的行号都需要增加\n                line_number_current += 1\n                line_number_change += 1\n\n        return changed_lines\n\n    # TODO: The key issue is that the changed line numbers correspond to the old function names (i.e., those removed) and the new function names (i.e., those added), and the current implementation does not handle this correctly.\n    # We need a way to associate the changed line numbers with their function or class names before and after the change. One method is to build a mapping before processing changed_lines, which can map the names after the change back to the names before the change based on the line number.\n    # Then, in the identify_changes_in_structure function, this mapping can be used to correctly identify the changed structure.\n    def identify_changes_in_structure(self, changed_lines, structures):\n        \"\"\"\n        Identify the structure of the function or class where changes have occurred: Traverse all changed lines, for each line, it checks whether this line is between the start line and the end line of a structure (function or class).\n        If so, then this structure is considered to have changed, and its name and the name of the parent structure are added to the corresponding set in the result dictionary changes_in_structures (depending on whether this line is added or deleted).\n\n        Output example: {'added': {('PipelineAutoMatNode', None), ('to_json_new', 'PipelineAutoMatNode')}, 'removed': set()}\n\n        Args:\n            changed_lines (dict): A dictionary containing the line numbers where changes have occurred, {'added': [(line number, change content)], 'removed': [(line number, change content)]}\n            structures (list): The received is a list of function or class structures from get_functions_and_classes, each structure is composed of structure type, name, start line number, end line number, and parent structure name.\n\n        Returns:\n            dict: A dictionary containing the structures where changes have occurred, the key is the change type, and the value is a set of structure names and parent structure names.\n                Possible change types are 'added' (new) and 'removed' (removed).\n        \"\"\"\n        changes_in_structures = {\"added\": set(), \"removed\": set()}\n        for change_type, lines in changed_lines.items():\n            for line_number, _ in lines:\n                for (\n                    structure_type,\n                    name,\n                    start_line,\n                    end_line,\n                    parent_structure,\n                ) in structures:\n                    if start_line <= line_number <= end_line:\n                        changes_in_structures[change_type].add((name, parent_structure))\n        return changes_in_structures\n\n    # TODO:可能有错，需要单元测试覆盖； 可能有更好的实现方式\n    def get_to_be_staged_files(self):\n        \"\"\"\n        This method retrieves all unstaged files in the repository that meet one of the following conditions:\n        1. The file, when its extension is changed to .md, corresponds to a file that is already staged.\n        2. The file's path is the same as the 'project_hierarchy' field in the CONFIG.\n\n        It returns a list of the paths of these files.\n\n        :return: A list of relative file paths to the repo that are either modified but not staged, or untracked, and meet one of the conditions above.\n        \"\"\"\n        # 已经更改但是暂未暂存的文件，这里只能是.md文件，因为作者不提交的.py文件（即使发生变更）我们不做处理。\n        to_be_staged_files = []\n        # staged_files是已经暂存的文件，通常这里是作者做了更改后git add 的.py文件 或其他文件\n        staged_files = [item.a_path for item in self.repo.index.diff(\"HEAD\")]\n        print(\n            f\"{Fore.LIGHTYELLOW_EX}target_repo_path{Style.RESET_ALL}: {self.repo_path}\"\n        )\n        print(\n            f\"{Fore.LIGHTMAGENTA_EX}already_staged_files{Style.RESET_ALL}:{staged_files}\"\n        )\n\n        setting = SettingsManager.get_setting()\n\n        project_hierarchy = setting.project.hierarchy_name\n        # diffs是所有未暂存更改文件的列表。这些更改文件是相对于工作区（working directory）的，也就是说，它们是自上次提交（commit）以来在工作区发生的更改，但还没有被添加到暂存区（staging area）\n        # 比如原本存在的md文件现在由于代码的变更发生了更新，就会标记为未暂存diff\n        diffs = self.repo.index.diff(None)\n        # untracked_files是一个包含了所有未跟踪文件的列表。比如说用户添加了新的.py文件后项目自己生成的对应.md文档。它们是在工作区中存在但还没有被添加到暂存区（staging area）的文件。\n        # untracked_files中的文件路径是绝对路径\n        untracked_files = self.repo.untracked_files\n        print(f\"{Fore.LIGHTCYAN_EX}untracked_files{Style.RESET_ALL}: {untracked_files}\")\n\n        # 处理untrack_files中的内容\n        for untracked_file in untracked_files:\n            # 连接repo_path和untracked_file以获取完整的绝对路径\n            if untracked_file.startswith(setting.project.markdown_docs_name):\n                to_be_staged_files.append(untracked_file)\n            continue\n            print(f\"rel_untracked_file:{rel_untracked_file}\")\n            # import pdb; pdb.set_trace()\n            # 判断这个文件的类型：\n            if rel_untracked_file.endswith(\".md\"):\n                # 把rel_untracked_file从CONFIG['Markdown_Docs_folder']中拆离出来。判断是否能跟暂存区中的某一个.py文件对应上\n                rel_untracked_file = os.path.relpath(\n                    rel_untracked_file, setting.project.markdown_docs_name\n                )\n                corresponding_py_file = os.path.splitext(rel_untracked_file)[0] + \".py\"\n                print(\n                    f\"corresponding_py_file in untracked_files:{corresponding_py_file}\"\n                )\n                if corresponding_py_file in staged_files:\n                    # 如果是，那么就把这个md文件也加入到unstaged_files中\n                    to_be_staged_files.append(\n                        os.path.join(\n                            self.repo_path.lstrip(\"/\"),\n                            setting.project.markdown_docs_name,\n                            rel_untracked_file,\n                        )\n                    )\n            elif rel_untracked_file == project_hierarchy:\n                to_be_staged_files.append(rel_untracked_file)\n\n        # 处理已追踪但是未暂存的内容\n        unstaged_files = [diff.b_path for diff in diffs]\n        print(f\"{Fore.LIGHTCYAN_EX}unstaged_files{Style.RESET_ALL}: {unstaged_files}\")\n\n        for unstaged_file in unstaged_files:\n            # 连接repo_path和unstaged_file以获取完整的绝对路径\n            if unstaged_file.startswith(\n                setting.project.markdown_docs_name\n            ) or unstaged_file.startswith(setting.project.hierarchy_name):\n                # abs_unstaged_file = os.path.join(self.repo_path, unstaged_file)\n                # # # 获取相对于仓库根目录的相对路径\n                # # rel_unstaged_file = os.path.relpath(abs_unstaged_file, self.repo_path)\n                to_be_staged_files.append(unstaged_file)\n            elif unstaged_file == project_hierarchy:  # project_hierarchy永远add\n                to_be_staged_files.append(unstaged_file)\n            continue\n            abs_unstaged_file = os.path.join(self.repo_path, unstaged_file)\n            # 获取相对于仓库根目录的相对路径\n            rel_unstaged_file = os.path.relpath(abs_unstaged_file, self.repo_path)\n            print(f\"rel_unstaged_file:{rel_unstaged_file}\")\n            # 如果它是md文件\n            if unstaged_file.endswith(\".md\"):\n                # 把rel_unstaged_file从CONFIG['Markdown_Docs_folder']中拆离出来。判断是否能跟暂存区中的某一个.py文件对应上\n                rel_unstaged_file = os.path.relpath(\n                    rel_unstaged_file, setting.project.markdown_docs_name\n                )\n                corresponding_py_file = os.path.splitext(rel_unstaged_file)[0] + \".py\"\n                print(f\"corresponding_py_file:{corresponding_py_file}\")\n                if corresponding_py_file in staged_files:\n                    # 如果是，那么就把这个md文件也加入到unstaged_files中\n                    to_be_staged_files.append(\n                        os.path.join(\n                            self.repo_path.lstrip(\"/\"),\n                            setting.project.markdown_docs_name,\n                            rel_unstaged_file,\n                        )\n                    )\n            elif unstaged_file == project_hierarchy:  # project_hierarchy永远add\n                to_be_staged_files.append(unstaged_file)\n        print(\n            f\"{Fore.LIGHTRED_EX}newly_staged_files{Style.RESET_ALL}: {to_be_staged_files}\"\n        )\n        return to_be_staged_files\n\n    def add_unstaged_files(self):\n        \"\"\"\n        Add unstaged files which meet the condition to the staging area.\n        \"\"\"\n        unstaged_files_meeting_conditions = self.get_to_be_staged_files()\n        for file_path in unstaged_files_meeting_conditions:\n            add_command = f\"git -C {self.repo.working_dir} add {file_path}\"\n            subprocess.run(add_command, shell=True, check=True)\n        return unstaged_files_meeting_conditions\n\n\nif __name__ == \"__main__\":\n    repo_path = \"/path/to/your/repo/\"\n    change_detector = ChangeDetector(repo_path)\n    changed_files = change_detector.get_staged_pys()\n    print(f\"\\nchanged_files:{changed_files}\\n\\n\")\n    for file_path, is_new_file in changed_files.items():\n        changed_lines = change_detector.parse_diffs(\n            change_detector.get_file_diff(file_path, is_new_file)\n        )\n        # print(\"changed_lines:\",changed_lines)\n        file_handler = FileHandler(repo_path=repo_path, file_path=file_path)\n        changes_in_pyfile = change_detector.identify_changes_in_structure(\n            changed_lines,\n            file_handler.get_functions_and_classes(file_handler.read_file()),\n        )\n        print(f\"Changes in {file_path} Structures:{changes_in_pyfile}\\n\")\n"}
{"type": "source_file", "path": "repo_agent/chat_with_repo/__main__.py", "content": "from .main import main\n\nmain()\n"}
{"type": "source_file", "path": "repo_agent/runner.py", "content": "import json\nimport os\nimport shutil\nimport subprocess\nimport threading\nimport time\nfrom concurrent.futures import ThreadPoolExecutor\nfrom functools import partial\nfrom pathlib import Path\n\nfrom colorama import Fore, Style\nfrom tqdm import tqdm\n\nfrom repo_agent.change_detector import ChangeDetector\nfrom repo_agent.chat_engine import ChatEngine\nfrom repo_agent.doc_meta_info import DocItem, DocItemStatus, MetaInfo, need_to_generate\nfrom repo_agent.file_handler import FileHandler\nfrom repo_agent.log import logger\nfrom repo_agent.multi_task_dispatch import worker\nfrom repo_agent.project_manager import ProjectManager\nfrom repo_agent.settings import SettingsManager\nfrom repo_agent.utils.meta_info_utils import delete_fake_files, make_fake_files\n\n\nclass Runner:\n    def __init__(self):\n        self.setting = SettingsManager.get_setting()\n        self.absolute_project_hierarchy_path = (\n            self.setting.project.target_repo / self.setting.project.hierarchy_name\n        )\n\n        self.project_manager = ProjectManager(\n            repo_path=self.setting.project.target_repo,\n            project_hierarchy=self.setting.project.hierarchy_name,\n        )\n        self.change_detector = ChangeDetector(\n            repo_path=self.setting.project.target_repo\n        )\n        self.chat_engine = ChatEngine(project_manager=self.project_manager)\n\n        if not self.absolute_project_hierarchy_path.exists():\n            file_path_reflections, jump_files = make_fake_files()\n            self.meta_info = MetaInfo.init_meta_info(file_path_reflections, jump_files)\n            self.meta_info.checkpoint(\n                target_dir_path=self.absolute_project_hierarchy_path\n            )\n        else:  # 如果存在全局结构信息文件夹.project_hierarchy，就从中加载\n            self.meta_info = MetaInfo.from_checkpoint_path(\n                self.absolute_project_hierarchy_path\n            )\n\n        self.meta_info.checkpoint(  # 更新白名单后也要重新将全局信息写入到.project_doc_record文件夹中\n            target_dir_path=self.absolute_project_hierarchy_path\n        )\n        self.runner_lock = threading.Lock()\n\n    def get_all_pys(self, directory):\n        \"\"\"\n        Get all Python files in the given directory.\n\n        Args:\n            directory (str): The directory to search.\n\n        Returns:\n            list: A list of paths to all Python files.\n        \"\"\"\n        python_files = []\n\n        for root, dirs, files in os.walk(directory):\n            for file in files:\n                if file.endswith(\".py\"):\n                    python_files.append(os.path.join(root, file))\n\n        return python_files\n\n    def generate_doc_for_a_single_item(self, doc_item: DocItem):\n        \"\"\"为一个对象生成文档\"\"\"\n        try:\n            if not need_to_generate(doc_item, self.setting.project.ignore_list):\n                print(\n                    f\"Content ignored/Document generated, skipping: {doc_item.get_full_name()}\"\n                )\n            else:\n                print(\n                    f\" -- Generating document  {Fore.LIGHTYELLOW_EX}{doc_item.item_type.name}: {doc_item.get_full_name()}{Style.RESET_ALL}\"\n                )\n                response_message = self.chat_engine.generate_doc(\n                    doc_item=doc_item,\n                )\n                doc_item.md_content.append(response_message)  # type: ignore\n                doc_item.item_status = DocItemStatus.doc_up_to_date\n                self.meta_info.checkpoint(\n                    target_dir_path=self.absolute_project_hierarchy_path\n                )\n        except Exception:\n            logger.exception(\n                f\"Document generation failed after multiple attempts, skipping: {doc_item.get_full_name()}\"\n            )\n            doc_item.item_status = DocItemStatus.doc_has_not_been_generated\n\n    def first_generate(self):\n        \"\"\"\n        生成所有文档，完成后刷新并保存文件系统中的文档信息。\n        \"\"\"\n        logger.info(\"Starting to generate documentation\")\n        check_task_available_func = partial(\n            need_to_generate, ignore_list=self.setting.project.ignore_list\n        )\n        task_manager = self.meta_info.get_topology(check_task_available_func)\n        before_task_len = len(task_manager.task_dict)\n\n        if not self.meta_info.in_generation_process:\n            self.meta_info.in_generation_process = True\n            logger.info(\"Init a new task-list\")\n        else:\n            logger.info(\"Load from an existing task-list\")\n        self.meta_info.print_task_list(task_manager.task_dict)\n\n        try:\n            # 创建并启动线程\n            threads = [\n                threading.Thread(\n                    target=worker,\n                    args=(\n                        task_manager,\n                        process_id,\n                        self.generate_doc_for_a_single_item,\n                    ),\n                )\n                for process_id in range(self.setting.project.max_thread_count)\n            ]\n            for thread in threads:\n                thread.start()\n            for thread in threads:\n                thread.join()\n\n            # 所有任务完成后刷新文档\n            self.markdown_refresh()\n\n            # 更新文档版本\n            self.meta_info.document_version = (\n                self.change_detector.repo.head.commit.hexsha\n            )\n            self.meta_info.in_generation_process = False\n            self.meta_info.checkpoint(\n                target_dir_path=self.absolute_project_hierarchy_path\n            )\n            logger.info(\n                f\"Successfully generated {before_task_len - len(task_manager.task_dict)} documents.\"\n            )\n\n        except BaseException as e:\n            logger.error(\n                f\"An error occurred: {e}. {before_task_len - len(task_manager.task_dict)} docs are generated at this time\"\n            )\n\n    def markdown_refresh(self):\n        \"\"\"刷新最新的文档信息到markdown格式文件夹中\"\"\"\n        with self.runner_lock:\n            # 定义 markdown 文件夹路径\n            markdown_folder = (\n                Path(self.setting.project.target_repo)\n                / self.setting.project.markdown_docs_name\n            )\n\n            # 删除并重新创建目录\n            if markdown_folder.exists():\n                logger.debug(f\"Deleting existing contents of {markdown_folder}\")\n                shutil.rmtree(markdown_folder)\n            markdown_folder.mkdir(parents=True, exist_ok=True)\n            logger.debug(f\"Created markdown folder at {markdown_folder}\")\n\n        # 遍历文件列表生成 markdown\n        file_item_list = self.meta_info.get_all_files()\n        logger.debug(f\"Found {len(file_item_list)} files to process.\")\n\n        for file_item in tqdm(file_item_list):\n            # 检查文档内容\n            def recursive_check(doc_item) -> bool:\n                if doc_item.md_content:\n                    return True\n                for child in doc_item.children.values():\n                    if recursive_check(child):\n                        return True\n                return False\n\n            if not recursive_check(file_item):\n                logger.debug(\n                    f\"No documentation content for: {file_item.get_full_name()}, skipping.\"\n                )\n                continue\n\n            # 生成 markdown 内容\n            markdown = \"\"\n            for child in file_item.children.values():\n                markdown += self.to_markdown(child, 2)\n\n            if not markdown:\n                logger.warning(\n                    f\"No markdown content generated for: {file_item.get_full_name()}\"\n                )\n                continue\n\n            # 确定并创建文件路径\n            file_path = Path(\n                self.setting.project.markdown_docs_name\n            ) / file_item.get_file_name().replace(\".py\", \".md\")\n            abs_file_path = self.setting.project.target_repo / file_path\n            logger.debug(f\"Writing markdown to: {abs_file_path}\")\n\n            # 确保目录存在\n            abs_file_path.parent.mkdir(parents=True, exist_ok=True)\n            logger.debug(f\"Ensured directory exists: {abs_file_path.parent}\")\n\n            # 使用锁保护文件写入操作\n            with self.runner_lock:\n                for attempt in range(3):  # 最多重试3次\n                    try:\n                        with open(abs_file_path, \"w\", encoding=\"utf-8\") as file:\n                            file.write(markdown)\n                        logger.debug(f\"Successfully wrote to {abs_file_path}\")\n                        break\n                    except IOError as e:\n                        logger.error(\n                            f\"Failed to write {abs_file_path} on attempt {attempt + 1}: {e}\"\n                        )\n                        time.sleep(1)  # 延迟再试\n\n        logger.info(\n            f\"Markdown documents have been refreshed at {self.setting.project.markdown_docs_name}\"\n        )\n\n    def to_markdown(self, item, now_level: int) -> str:\n        \"\"\"将文件内容转化为 markdown 格式的文本\"\"\"\n        markdown_content = (\n            \"#\" * now_level + f\" {item.item_type.to_str()} {item.obj_name}\"\n        )\n        if \"params\" in item.content.keys() and item.content[\"params\"]:\n            markdown_content += f\"({', '.join(item.content['params'])})\"\n        markdown_content += \"\\n\"\n        if item.md_content:\n            markdown_content += f\"{item.md_content[-1]}\\n\"\n        else:\n            markdown_content += \"Doc is waiting to be generated...\\n\"\n        for child in item.children.values():\n            markdown_content += self.to_markdown(child, now_level + 1)\n            markdown_content += \"***\\n\"\n        return markdown_content\n\n    def git_commit(self, commit_message):\n        try:\n            subprocess.check_call(\n                [\"git\", \"commit\", \"--no-verify\", \"-m\", commit_message],\n                shell=True,\n            )\n        except subprocess.CalledProcessError as e:\n            print(f\"An error occurred while trying to commit {str(e)}\")\n\n    def run(self):\n        \"\"\"\n        Runs the document update process.\n\n        This method detects the changed Python files, processes each file, and updates the documents accordingly.\n\n        Returns:\n            None\n        \"\"\"\n\n        if self.meta_info.document_version == \"\":\n            # 根据document version自动检测是否仍在最初生成的process里(是否为第一次生成)\n            self.first_generate()  # 如果是第一次做文档生成任务，就通过first_generate生成所有文档\n            self.meta_info.checkpoint(\n                target_dir_path=self.absolute_project_hierarchy_path,\n                flash_reference_relation=True,\n            )  # 这一步将生成后的meta信息（包含引用关系）写入到.project_doc_record文件夹中\n            return\n\n        if (\n            not self.meta_info.in_generation_process\n        ):  # 如果不是在生成过程中，就开始检测变更\n            logger.info(\"Starting to detect changes.\")\n\n            \"\"\"采用新的办法\n            1.新建一个project-hierachy\n            2.和老的hierarchy做merge,处理以下情况：\n            - 创建一个新文件：需要生成对应的doc\n            - 文件、对象被删除：对应的doc也删除(按照目前的实现，文件重命名算是删除再添加)\n            - 引用关系变了：对应的obj-doc需要重新生成\n            \n            merge后的new_meta_info中：\n            1.新建的文件没有文档，因此metainfo merge后还是没有文档\n            2.被删除的文件和obj，本来就不在新的meta里面，相当于文档被自动删除了\n            3.只需要观察被修改的文件，以及引用关系需要被通知的文件去重新生成文档\"\"\"\n            file_path_reflections, jump_files = make_fake_files()\n            new_meta_info = MetaInfo.init_meta_info(file_path_reflections, jump_files)\n            new_meta_info.load_doc_from_older_meta(self.meta_info)\n\n            self.meta_info = new_meta_info  # 更新自身的meta_info信息为new的信息\n            self.meta_info.in_generation_process = True  # 将in_generation_process设置为True，表示检测到变更后Generating document 的过程中\n\n        # 处理任务队列\n        check_task_available_func = partial(\n            need_to_generate, ignore_list=self.setting.project.ignore_list\n        )\n\n        task_manager = self.meta_info.get_task_manager(\n            self.meta_info.target_repo_hierarchical_tree,\n            task_available_func=check_task_available_func,\n        )\n\n        for item_name, item_type in self.meta_info.deleted_items_from_older_meta:\n            print(\n                f\"{Fore.LIGHTMAGENTA_EX}[Dir/File/Obj Delete Dected]: {Style.RESET_ALL} {item_type} {item_name}\"\n            )\n        self.meta_info.print_task_list(task_manager.task_dict)\n        if task_manager.all_success:\n            logger.info(\n                \"No tasks in the queue, all documents are completed and up to date.\"\n            )\n\n        threads = [\n            threading.Thread(\n                target=worker,\n                args=(task_manager, process_id, self.generate_doc_for_a_single_item),\n            )\n            for process_id in range(self.setting.project.max_thread_count)\n        ]\n        for thread in threads:\n            thread.start()\n        for thread in threads:\n            thread.join()\n\n        self.meta_info.in_generation_process = False\n        self.meta_info.document_version = self.change_detector.repo.head.commit.hexsha\n\n        self.meta_info.checkpoint(\n            target_dir_path=self.absolute_project_hierarchy_path,\n            flash_reference_relation=True,\n        )\n        logger.info(f\"Doc has been forwarded to the latest version\")\n\n        self.markdown_refresh()\n        delete_fake_files()\n\n        logger.info(f\"Starting to git-add DocMetaInfo and newly generated Docs\")\n        time.sleep(1)\n\n        # 将run过程中更新的Markdown文件（未暂存）添加到暂存区\n        git_add_result = self.change_detector.add_unstaged_files()\n\n        if len(git_add_result) > 0:\n            logger.info(\n                f\"Added {[file for file in git_add_result]} to the staging area.\"\n            )\n\n        # self.git_commit(f\"Update documentation for {file_handler.file_path}\") # 提交变更\n\n    def add_new_item(self, file_handler, json_data):\n        \"\"\"\n        Add new projects to the JSON file and generate corresponding documentation.\n\n        Args:\n            file_handler (FileHandler): The file handler object for reading and writing files.\n            json_data (dict): The JSON data storing the project structure information.\n\n        Returns:\n            None\n        \"\"\"\n        file_dict = {}\n        # 因为是新增的项目，所以这个文件里的所有对象都要写一个文档\n        for (\n            structure_type,\n            name,\n            start_line,\n            end_line,\n            parent,\n            params,\n        ) in file_handler.get_functions_and_classes(file_handler.read_file()):\n            code_info = file_handler.get_obj_code_info(\n                structure_type, name, start_line, end_line, parent, params\n            )\n            response_message = self.chat_engine.generate_doc(code_info, file_handler)\n            md_content = response_message.content\n            code_info[\"md_content\"] = md_content\n            # 文件对象file_dict中添加一个新的对象\n            file_dict[name] = code_info\n\n        json_data[file_handler.file_path] = file_dict\n        # 将新的项写入json文件\n        with open(self.project_manager.project_hierarchy, \"w\", encoding=\"utf-8\") as f:\n            json.dump(json_data, f, indent=4, ensure_ascii=False)\n        logger.info(\n            f\"The structural information of the newly added file {file_handler.file_path} has been written into a JSON file.\"\n        )\n        # 将变更部分的json文件内容转换成markdown内容\n        markdown = file_handler.convert_to_markdown_file(\n            file_path=file_handler.file_path\n        )\n        # 将markdown内容写入.md文件\n        file_handler.write_file(\n            os.path.join(\n                self.project_manager.repo_path,\n                self.setting.project.markdown_docs_name,\n                file_handler.file_path.replace(\".py\", \".md\"),\n            ),\n            markdown,\n        )\n        logger.info(f\"已生成新增文件 {file_handler.file_path} 的Markdown文档。\")\n\n    def process_file_changes(self, repo_path, file_path, is_new_file):\n        \"\"\"\n        This function is called in the loop of detected changed files. Its purpose is to process changed files according to the absolute file path, including new files and existing files.\n        Among them, changes_in_pyfile is a dictionary that contains information about the changed structures. An example format is: {'added': {'add_context_stack', '__init__'}, 'removed': set()}\n\n        Args:\n            repo_path (str): The path to the repository.\n            file_path (str): The relative path to the file.\n            is_new_file (bool): Indicates whether the file is new or not.\n\n        Returns:\n            None\n        \"\"\"\n\n        file_handler = FileHandler(\n            repo_path=repo_path, file_path=file_path\n        )  # 变更文件的操作器\n        # 获取整个py文件的代码\n        source_code = file_handler.read_file()\n        changed_lines = self.change_detector.parse_diffs(\n            self.change_detector.get_file_diff(file_path, is_new_file)\n        )\n        changes_in_pyfile = self.change_detector.identify_changes_in_structure(\n            changed_lines, file_handler.get_functions_and_classes(source_code)\n        )\n        logger.info(f\"检测到变更对象：\\n{changes_in_pyfile}\")\n\n        # 判断project_hierarchy.json文件中能否找到对应.py文件路径的项\n        with open(self.project_manager.project_hierarchy, \"r\", encoding=\"utf-8\") as f:\n            json_data = json.load(f)\n\n        # 如果找到了对应文件\n        if file_handler.file_path in json_data:\n            # 更新json文件中的内容\n            json_data[file_handler.file_path] = self.update_existing_item(\n                json_data[file_handler.file_path], file_handler, changes_in_pyfile\n            )\n            # 将更新后的file写回到json文件中\n            with open(\n                self.project_manager.project_hierarchy, \"w\", encoding=\"utf-8\"\n            ) as f:\n                json.dump(json_data, f, indent=4, ensure_ascii=False)\n\n            logger.info(f\"已更新{file_handler.file_path}文件的json结构信息。\")\n\n            # 将变更部分的json文件内容转换成markdown内容\n            markdown = file_handler.convert_to_markdown_file(\n                file_path=file_handler.file_path\n            )\n            # 将markdown内容写入.md文件\n            file_handler.write_file(\n                os.path.join(\n                    self.setting.project.markdown_docs_name,\n                    file_handler.file_path.replace(\".py\", \".md\"),\n                ),\n                markdown,\n            )\n            logger.info(f\"已更新{file_handler.file_path}文件的Markdown文档。\")\n\n        # 如果没有找到对应的文件，就添加一个新的项\n        else:\n            self.add_new_item(file_handler, json_data)\n\n        # 将run过程中更新的Markdown文件（未暂存）添加到暂存区\n        git_add_result = self.change_detector.add_unstaged_files()\n\n        if len(git_add_result) > 0:\n            logger.info(f\"已添加 {[file for file in git_add_result]} 到暂存区\")\n\n        # self.git_commit(f\"Update documentation for {file_handler.file_path}\") # 提交变更\n\n    def update_existing_item(self, file_dict, file_handler, changes_in_pyfile):\n        \"\"\"\n        Update existing projects.\n\n        Args:\n            file_dict (dict): A dictionary containing file structure information.\n            file_handler (FileHandler): The file handler object.\n            changes_in_pyfile (dict): A dictionary containing information about the objects that have changed in the file.\n\n        Returns:\n            dict: The updated file structure information dictionary.\n        \"\"\"\n        new_obj, del_obj = self.get_new_objects(file_handler)\n\n        # 处理被删除的对象\n        for obj_name in del_obj:  # 真正被删除的对象\n            if obj_name in file_dict:\n                del file_dict[obj_name]\n                logger.info(f\"已删除 {obj_name} 对象。\")\n\n        referencer_list = []\n\n        # 生成文件的结构信息，获得当前文件中的所有对象， 这里其实就是文件更新之后的结构了\n        current_objects = file_handler.generate_file_structure(file_handler.file_path)\n\n        current_info_dict = {obj[\"name\"]: obj for obj in current_objects.values()}\n\n        # 更新全局文件结构信息，比如代码起始行\\终止行等\n        for current_obj_name, current_obj_info in current_info_dict.items():\n            if current_obj_name in file_dict:\n                # 如果当前对象在旧对象列表中存在，更新旧对象的信息\n                file_dict[current_obj_name][\"type\"] = current_obj_info[\"type\"]\n                file_dict[current_obj_name][\"code_start_line\"] = current_obj_info[\n                    \"code_start_line\"\n                ]\n                file_dict[current_obj_name][\"code_end_line\"] = current_obj_info[\n                    \"code_end_line\"\n                ]\n                file_dict[current_obj_name][\"parent\"] = current_obj_info[\"parent\"]\n                file_dict[current_obj_name][\"name_column\"] = current_obj_info[\n                    \"name_column\"\n                ]\n            else:\n                # 如果当前对象在旧对象列表中不存在，将新对象添加到旧对象列表中\n                file_dict[current_obj_name] = current_obj_info\n\n        # 对于每一个对象：获取其引用者列表\n        for obj_name, _ in changes_in_pyfile[\"added\"]:\n            for current_object in current_objects.values():  # 引入new_objects的目的是获取到find_all_referencer中必要的参数信息。在changes_in_pyfile['added']中只有对象和其父级结构的名称，缺少其他参数\n                if (\n                    obj_name == current_object[\"name\"]\n                ):  # 确保只有当added中的对象名称匹配new_objects时才添加引用者\n                    # 获取每个需要生成文档的对象的引用者\n                    referencer_obj = {\n                        \"obj_name\": obj_name,\n                        \"obj_referencer_list\": self.project_manager.find_all_referencer(\n                            variable_name=current_object[\"name\"],\n                            file_path=file_handler.file_path,\n                            line_number=current_object[\"code_start_line\"],\n                            column_number=current_object[\"name_column\"],\n                        ),\n                    }\n                    referencer_list.append(\n                        referencer_obj\n                    )  # 对于每一个正在处理的对象，添加他的引用者字典到全部对象的应用者列表中\n\n        with ThreadPoolExecutor(max_workers=5) as executor:\n            # 通过线程池并发执行\n            futures = []\n            for changed_obj in changes_in_pyfile[\"added\"]:  # 对于每一个待处理的对象\n                for ref_obj in referencer_list:\n                    if (\n                        changed_obj[0] == ref_obj[\"obj_name\"]\n                    ):  # 在referencer_list中找到它的引用者字典！\n                        future = executor.submit(\n                            self.update_object,\n                            file_dict,\n                            file_handler,\n                            changed_obj[0],\n                            ref_obj[\"obj_referencer_list\"],\n                        )\n                        print(\n                            f\"正在生成 {Fore.CYAN}{file_handler.file_path}{Style.RESET_ALL}中的{Fore.CYAN}{changed_obj[0]}{Style.RESET_ALL}对象文档.\"\n                        )\n                        futures.append(future)\n\n            for future in futures:\n                future.result()\n\n        # 更新传入的file参数\n        return file_dict\n\n    def update_object(self, file_dict, file_handler, obj_name, obj_referencer_list):\n        \"\"\"\n        Generate documentation content and update corresponding field information of the object.\n\n        Args:\n            file_dict (dict): A dictionary containing old object information.\n            file_handler: The file handler.\n            obj_name (str): The object name.\n            obj_referencer_list (list): The list of object referencers.\n\n        Returns:\n            None\n        \"\"\"\n        if obj_name in file_dict:\n            obj = file_dict[obj_name]\n            response_message = self.chat_engine.generate_doc(\n                obj, file_handler, obj_referencer_list\n            )\n            obj[\"md_content\"] = response_message.content\n\n    def get_new_objects(self, file_handler):\n        \"\"\"\n        The function gets the added and deleted objects by comparing the current version and the previous version of the .py file.\n\n        Args:\n            file_handler (FileHandler): The file handler object.\n\n        Returns:\n            tuple: A tuple containing the added and deleted objects, in the format (new_obj, del_obj)\n\n        Output example:\n            new_obj: ['add_context_stack', '__init__']\n            del_obj: []\n        \"\"\"\n        current_version, previous_version = file_handler.get_modified_file_versions()\n        parse_current_py = file_handler.get_functions_and_classes(current_version)\n        parse_previous_py = (\n            file_handler.get_functions_and_classes(previous_version)\n            if previous_version\n            else []\n        )\n\n        current_obj = {f[1] for f in parse_current_py}\n        previous_obj = {f[1] for f in parse_previous_py}\n\n        new_obj = list(current_obj - previous_obj)\n        del_obj = list(previous_obj - current_obj)\n        return new_obj, del_obj\n\n\nif __name__ == \"__main__\":\n    runner = Runner()\n\n    runner.run()\n\n    logger.info(\"文档任务完成。\")\n"}
