{"repo_info": {"repo_name": "CCExpert", "repo_owner": "Meize0729", "repo_url": "https://github.com/Meize0729/CCExpert"}}
{"type": "test_file", "path": "llava/serve/test_message.py", "content": "import argparse\nimport json\n\nimport requests\n\nfrom llava.conversation import default_conversation\n\n\ndef main():\n    if args.worker_address:\n        worker_addr = args.worker_address\n    else:\n        controller_addr = args.controller_address\n        ret = requests.post(controller_addr + \"/refresh_all_workers\")\n        ret = requests.post(controller_addr + \"/list_models\")\n        models = ret.json()[\"models\"]\n        models.sort()\n        print(f\"Models: {models}\")\n\n        ret = requests.post(controller_addr + \"/get_worker_address\", json={\"model\": args.model_name})\n        worker_addr = ret.json()[\"address\"]\n        print(f\"worker_addr: {worker_addr}\")\n\n    if worker_addr == \"\":\n        return\n\n    conv = default_conversation.copy()\n    conv.append_message(conv.roles[0], args.message)\n    prompt = conv.get_prompt()\n\n    headers = {\"User-Agent\": \"LLaVA Client\"}\n    pload = {\n        \"model\": args.model_name,\n        \"prompt\": prompt,\n        \"max_new_tokens\": args.max_new_tokens,\n        \"temperature\": 0.7,\n        \"stop\": conv.sep,\n    }\n    response = requests.post(worker_addr + \"/worker_generate_stream\", headers=headers, json=pload, stream=True)\n\n    print(prompt.replace(conv.sep, \"\\n\"), end=\"\")\n    for chunk in response.iter_lines(chunk_size=8192, decode_unicode=False, delimiter=b\"\\0\"):\n        if chunk:\n            data = json.loads(chunk.decode(\"utf-8\"))\n            output = data[\"text\"].split(conv.sep)[-1]\n            print(output, end=\"\\r\")\n    print(\"\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--controller-address\", type=str, default=\"http://localhost:21001\")\n    parser.add_argument(\"--worker-address\", type=str)\n    parser.add_argument(\"--model-name\", type=str, default=\"facebook/opt-350m\")\n    parser.add_argument(\"--max-new-tokens\", type=int, default=32)\n    parser.add_argument(\"--message\", type=str, default=\"Tell me a story with more than 1000 words.\")\n    args = parser.parse_args()\n\n    main()\n"}
{"type": "source_file", "path": "llava/constants.py", "content": "CONTROLLER_HEART_BEAT_EXPIRATION = 30\nWORKER_HEART_BEAT_INTERVAL = 15\n\nLOGDIR = \".\"\n\n# Model Constants\nIGNORE_INDEX = -100\nIMAGE_TOKEN_INDEX = -200\nDEFAULT_IMAGE_TOKEN = \"<image>\"\nDEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\nDEFAULT_IM_START_TOKEN = \"<im_start>\"\nDEFAULT_IM_END_TOKEN = \"<im_end>\"\n"}
{"type": "source_file", "path": "eval_func/cider/cider.py", "content": "# Filename: cider.py\n#\n# Description: Describes the class to compute the CIDEr (Consensus-Based Image Description Evaluation) Metric \n#               by Vedantam, Zitnick, and Parikh (http://arxiv.org/abs/1411.5726)\n#\n# Creation Date: Sun Feb  8 14:16:54 2015\n#\n# Authors: Ramakrishna Vedantam <vrama91@vt.edu> and Tsung-Yi Lin <tl483@cornell.edu>\n\nfrom eval_func.cider.cider_scorer import CiderScorer\nimport pdb\n\nclass Cider:\n    \"\"\"\n    Main Class to compute the CIDEr metric \n\n    \"\"\"\n    def __init__(self, test=None, refs=None, n=4, sigma=6.0):\n        # set cider to sum over 1 to 4-grams\n        self._n = n\n        # set the standard deviation parameter for gaussian penalty\n        self._sigma = sigma\n\n    def compute_score(self, gts, res):\n        \"\"\"\n        Main function to compute CIDEr score\n        :param  hypo_for_image (dict) : dictionary with key <image> and value <tokenized hypothesis / candidate sentence>\n                ref_for_image (dict)  : dictionary with key <image> and value <tokenized reference sentence>\n        :return: cider (float) : computed CIDEr score for the corpus \n        \"\"\"\n\n        cider_scorer = CiderScorer(n=self._n, sigma=self._sigma)\n\n        for i in range(len(res)):\n            hypo = res[i]\n            ref = gts[i]\n\n            # Sanity check.\n            assert(type(hypo) is list)\n            assert(len(hypo) == 1)\n            assert(type(ref) is list)\n            assert(len(ref) > 0)\n\n            cider_scorer += (hypo[0], ref)\n\n        (score, scores) = cider_scorer.compute_score()\n\n        return score, scores\n\n    def method(self):\n        return \"CIDEr\""}
{"type": "source_file", "path": "llava/__init__.py", "content": ""}
{"type": "source_file", "path": "eval_func/bleu/bleu_scorer.py", "content": "#!/usr/bin/env python\n\n# bleu_scorer.py\n# David Chiang <chiang@isi.edu>\n\n# Copyright (c) 2004-2006 University of Maryland. All rights\n# reserved. Do not redistribute without permission from the\n# author. Not for commercial use.\n\n# Modified by: \n# Hao Fang <hfang@uw.edu>\n# Tsung-Yi Lin <tl483@cornell.edu>\n\n'''Provides:\ncook_refs(refs, n=4): Transform a list of reference sentences as strings into a form usable by cook_test().\ncook_test(test, refs, n=4): Transform a test sentence as a string (together with the cooked reference sentences) into a form usable by score_cooked().\n'''\n\nimport copy\nimport sys, math, re\nfrom collections import defaultdict\n\ndef precook(s, n=4, out=False):\n    \"\"\"Takes a string as input and returns an object that can be given to\n    either cook_refs or cook_test. This is optional: cook_refs and cook_test\n    can take string arguments as well.\"\"\"\n    words = s.split()\n    counts = defaultdict(int)\n    for k in range(1,n+1):\n        for i in range(len(words)-k+1):\n            ngram = tuple(words[i:i+k])\n            counts[ngram] += 1\n    return (len(words), counts)\n\ndef cook_refs(refs, eff=None, n=4): ## lhuang: oracle will call with \"average\"\n    '''Takes a list of reference sentences for a single segment\n    and returns an object that encapsulates everything that BLEU\n    needs to know about them.'''\n\n    reflen = []\n    maxcounts = {}\n    for ref in refs:\n        rl, counts = precook(ref, n)\n        reflen.append(rl)\n        for (ngram,count) in counts.items():\n            maxcounts[ngram] = max(maxcounts.get(ngram,0), count)\n\n    # Calculate effective reference sentence length.\n    if eff == \"shortest\":\n        reflen = min(reflen)\n    elif eff == \"average\":\n        reflen = float(sum(reflen))/len(reflen)\n\n    ## lhuang: N.B.: leave reflen computaiton to the very end!!\n    \n    ## lhuang: N.B.: in case of \"closest\", keep a list of reflens!! (bad design)\n\n    return (reflen, maxcounts)\n\ndef cook_test(test, xxx_todo_changeme, eff=None, n=4):\n    '''Takes a test sentence and returns an object that\n    encapsulates everything that BLEU needs to know about it.'''\n    (reflen, refmaxcounts) = xxx_todo_changeme\n    testlen, counts = precook(test, n, True)\n\n    result = {}\n\n    # Calculate effective reference sentence length.\n    \n    if eff == \"closest\":\n        result[\"reflen\"] = min((abs(l-testlen), l) for l in reflen)[1]\n    else: ## i.e., \"average\" or \"shortest\" or None\n        result[\"reflen\"] = reflen\n\n    result[\"testlen\"] = testlen\n\n    result[\"guess\"] = [max(0,testlen-k+1) for k in range(1,n+1)]\n\n    result['correct'] = [0]*n\n    for (ngram, count) in counts.items():\n        result[\"correct\"][len(ngram)-1] += min(refmaxcounts.get(ngram,0), count)\n\n    return result\n\nclass BleuScorer(object):\n    \"\"\"Bleu scorer.\n    \"\"\"\n\n    __slots__ = \"n\", \"crefs\", \"ctest\", \"_score\", \"_ratio\", \"_testlen\", \"_reflen\", \"special_reflen\"\n    # special_reflen is used in oracle (proportional effective ref len for a node).\n\n    def copy(self):\n        ''' copy the refs.'''\n        new = BleuScorer(n=self.n)\n        new.ctest = copy.copy(self.ctest)\n        new.crefs = copy.copy(self.crefs)\n        new._score = None\n        return new\n\n    def __init__(self, test=None, refs=None, n=4, special_reflen=None):\n        ''' singular instance '''\n\n        self.n = n\n        self.crefs = []\n        self.ctest = []\n        self.cook_append(test, refs)\n        self.special_reflen = special_reflen\n\n    def cook_append(self, test, refs):\n        '''called by constructor and __iadd__ to avoid creating new instances.'''\n        \n        if refs is not None:\n            self.crefs.append(cook_refs(refs))\n            if test is not None:\n                cooked_test = cook_test(test, self.crefs[-1])\n                self.ctest.append(cooked_test) ## N.B.: -1\n            else:\n                self.ctest.append(None) # lens of crefs and ctest have to match\n\n        self._score = None ## need to recompute\n\n    def ratio(self, option=None):\n        self.compute_score(option=option)\n        return self._ratio\n\n    def score_ratio(self, option=None):\n        '''return (bleu, len_ratio) pair'''\n        return (self.fscore(option=option), self.ratio(option=option))\n\n    def score_ratio_str(self, option=None):\n        return \"%.4f (%.2f)\" % self.score_ratio(option)\n\n    def reflen(self, option=None):\n        self.compute_score(option=option)\n        return self._reflen\n\n    def testlen(self, option=None):\n        self.compute_score(option=option)\n        return self._testlen        \n\n    def retest(self, new_test):\n        if type(new_test) is str:\n            new_test = [new_test]\n        assert len(new_test) == len(self.crefs), new_test\n        self.ctest = []\n        for t, rs in zip(new_test, self.crefs):\n            self.ctest.append(cook_test(t, rs))\n        self._score = None\n\n        return self\n\n    def rescore(self, new_test):\n        ''' replace test(s) with new test(s), and returns the new score.'''\n        \n        return self.retest(new_test).compute_score()\n\n    def size(self):\n        assert len(self.crefs) == len(self.ctest), \"refs/test mismatch! %d<>%d\" % (len(self.crefs), len(self.ctest))\n        return len(self.crefs)\n\n    def __iadd__(self, other):\n        '''add an instance (e.g., from another sentence).'''\n\n        if type(other) is tuple:\n            ## avoid creating new BleuScorer instances\n            self.cook_append(other[0], other[1])\n        else:\n            assert self.compatible(other), \"incompatible BLEUs.\"\n            self.ctest.extend(other.ctest)\n            self.crefs.extend(other.crefs)\n            self._score = None ## need to recompute\n\n        return self        \n\n    def compatible(self, other):\n        return isinstance(other, BleuScorer) and self.n == other.n\n\n    def single_reflen(self, option=\"average\"):\n        return self._single_reflen(self.crefs[0][0], option)\n\n    def _single_reflen(self, reflens, option=None, testlen=None):\n        \n        if option == \"shortest\":\n            reflen = min(reflens)\n        elif option == \"average\":\n            reflen = float(sum(reflens))/len(reflens)\n        elif option == \"closest\":\n            reflen = min((abs(l-testlen), l) for l in reflens)[1]\n        else:\n            assert False, \"unsupported reflen option %s\" % option\n\n        return reflen\n\n    def recompute_score(self, option=None, verbose=0):\n        self._score = None\n        return self.compute_score(option, verbose)\n        \n    def compute_score(self, option=None, verbose=0):\n        n = self.n\n        small = 1e-9\n        tiny = 1e-15 ## so that if guess is 0 still return 0\n        bleu_list = [[] for _ in range(n)]\n\n        if self._score is not None:\n            return self._score\n\n        if option is None:\n            option = \"average\" if len(self.crefs) == 1 else \"closest\"\n\n        self._testlen = 0\n        self._reflen = 0\n        totalcomps = {'testlen':0, 'reflen':0, 'guess':[0]*n, 'correct':[0]*n}\n\n        # for each sentence\n        for comps in self.ctest:            \n            testlen = comps['testlen']\n            self._testlen += testlen\n\n            if self.special_reflen is None: ## need computation\n                reflen = self._single_reflen(comps['reflen'], option, testlen)\n            else:\n                reflen = self.special_reflen\n\n            self._reflen += reflen\n                \n            for key in ['guess','correct']:\n                for k in range(n):\n                    totalcomps[key][k] += comps[key][k]\n\n            # append per image bleu score\n            bleu = 1.\n            for k in range(n):\n                bleu *= (float(comps['correct'][k]) + tiny) \\\n                        /(float(comps['guess'][k]) + small) \n                bleu_list[k].append(bleu ** (1./(k+1)))\n            ratio = (testlen + tiny) / (reflen + small) ## N.B.: avoid zero division\n            if ratio < 1:\n                for k in range(n):\n                    bleu_list[k][-1] *= math.exp(1 - 1/ratio)\n\n            # if verbose > 1:\n            #     print(comps, reflen)\n\n        totalcomps['reflen'] = self._reflen\n        totalcomps['testlen'] = self._testlen\n\n        bleus = []\n        bleu = 1.\n        for k in range(n):\n            bleu *= float(totalcomps['correct'][k] + tiny) \\\n                    / (totalcomps['guess'][k] + small)\n            bleus.append(bleu ** (1./(k+1)))\n        ratio = (self._testlen + tiny) / (self._reflen + small) ## N.B.: avoid zero division\n        if ratio < 1:\n            for k in range(n):\n                bleus[k] *= math.exp(1 - 1/ratio)\n\n        # if verbose > 0:\n        #     print(totalcomps)\n        #     print(\"ratio:\", ratio)\n\n        self._score = bleus\n        return self._score, bleu_list\n"}
{"type": "source_file", "path": "eval_func/rouge/__init__.py", "content": "__author__ = 'vrama91'\n"}
{"type": "source_file", "path": "eval_func/meteor/meteor.py", "content": "#!/usr/bin/env python\n\n# Python wrapper for METEOR implementation, by Xinlei Chen\n# Acknowledge Michael Denkowski for the generous discussion and help\n\nimport os\nimport sys\nimport subprocess\nimport threading\n\n# Assumes meteor-1.5.jar is in the same directory as meteor.py.  Change as needed.\nMETEOR_JAR = 'meteor-1.5.jar'\n\n\n# print METEOR_JAR\n\nclass Meteor:\n\n    def __init__(self):\n        self.env = os.environ\n        self.env['LC_ALL'] = 'en_US.UTF_8'\n        self.meteor_cmd = ['java', '-jar', '-Xmx2G', METEOR_JAR, \\\n                           '-', '-', '-stdio', '-l', 'en', '-norm']\n        self.meteor_p = subprocess.Popen(self.meteor_cmd, \\\n                                         cwd=os.path.dirname(os.path.abspath(__file__)), \\\n                                         stdin=subprocess.PIPE, \\\n                                         stdout=subprocess.PIPE, \\\n                                         stderr=subprocess.PIPE,\n                                         env=self.env, universal_newlines=True, bufsize=1)\n        # Used to guarantee thread safety\n        self.lock = threading.Lock()\n\n    def compute_score(self, gts, res):\n\n        scores = []\n\n        eval_line = 'EVAL'\n        self.lock.acquire()\n        for i in range(len(res)):\n            assert (len(res[i]) == 1)\n            try:\n                stat = self._stat(res[i][0], gts[i])\n                eval_line += ' ||| {}'.format(stat)\n            except:\n                continue\n        # # Send to METEOR\n        # self.meteor_p.stdin.write(eval_line + '\\n')\n\n        # # Collect segment scores\n        # for i in range(len(res)):\n        #     score = float(self.meteor_p.stdout.readline().strip())\n        #     scores.append(score)\n\n        # # Final score\n        # final_score = float(self.meteor_p.stdout.readline().strip())\n        # self.lock.release()\n        \n        self.meteor_p.stdin.write('{}\\n'.format(eval_line))\n        self.meteor_p.stdin.flush()\n        for i in range(len(res)):\n            try:\n                scores.append(float(self.meteor_p.stdout.readline().strip()))\n            except:\n                continue\n        final_score = float(self.meteor_p.stdout.readline().strip())\n        self.lock.release()\n        return final_score, scores\n\n    def method(self):\n        return \"METEOR\"\n\n    def _stat(self, hypothesis_str, reference_list):\n        # # SCORE ||| reference 1 words ||| reference n words ||| hypothesis words\n        # hypothesis_str = hypothesis_str.replace('|||', '').replace('  ', ' ')\n        # score_line = ' ||| '.join(('SCORE', ' ||| '.join(reference_list), hypothesis_str))\n        # self.meteor_p.stdin.write(score_line + '\\n')\n        # return self.meteor_p.stdout.readline().strip()\n        # SCORE ||| reference 1 words ||| reference n words ||| hypothesis words\n        hypothesis_str = hypothesis_str.replace('|||','').replace('  ',' ')\n        score_line = ' ||| '.join(('SCORE', ' ||| '.join(reference_list), hypothesis_str))\n        self.meteor_p.stdin.write('{}\\n'.format(score_line))\n        self.meteor_p.stdin.flush()\n        raw = self.meteor_p.stdout.readline().strip()\n        numbers = [str(int(float(n))) for n in raw.split()]\n        return ' '.join(numbers)\n\n    def __del__(self):\n        self.lock.acquire()\n        self.meteor_p.stdin.close()\n        self.meteor_p.kill()\n        self.meteor_p.wait()\n        self.lock.release()"}
{"type": "source_file", "path": "eval_func/cider/__init__.py", "content": "__author__ = 'tylin'\n"}
{"type": "source_file", "path": "eval_func/rouge/rouge.py", "content": "#!/usr/bin/env python\n\n#\n\n# File Name : rouge.py\n\n#\n\n# Description : Computes ROUGE-L metric as described by Lin and Hovey (2004)\n\n#\n\n# Creation Date : 2015-01-07 06:03\n\n# Author : Ramakrishna Vedantam <vrama91@vt.edu>\n\n\nimport numpy as np\n\nimport pdb\n\n\ndef my_lcs(string, sub):\n    \"\"\"\n\n    Calculates longest common subsequence for a pair of tokenized strings\n\n    :param string : list of str : tokens from a string split using whitespace\n\n    :param sub : list of str : shorter string, also split using whitespace\n\n    :returns: length (list of int): length of the longest common subsequence between the two strings\n\n\n\n    Note: my_lcs only gives length of the longest common subsequence, not the actual LCS\n\n    \"\"\"\n\n    if (len(string) < len(sub)):\n        sub, string = string, sub\n\n    lengths = [[0 for i in range(0, len(sub) + 1)] for j in range(0, len(string) + 1)]\n\n    for j in range(1, len(sub) + 1):\n\n        for i in range(1, len(string) + 1):\n\n            if (string[i - 1] == sub[j - 1]):\n\n                lengths[i][j] = lengths[i - 1][j - 1] + 1\n\n            else:\n\n                lengths[i][j] = max(lengths[i - 1][j], lengths[i][j - 1])\n\n    return lengths[len(string)][len(sub)]\n\n\nclass Rouge():\n    '''\n\n    Class for computing ROUGE-L score for a set of candidate sentences for the MS COCO test set\n\n\n\n    '''\n\n    def __init__(self):\n\n        # vrama91: updated the value below based on discussion with Hovey\n\n        self.beta = 1.2\n\n    def calc_score(self, candidate, refs):\n\n        \"\"\"\n\n        Compute ROUGE-L score given one candidate and references for an image\n\n        :param candidate: str : candidate sentence to be evaluated\n\n        :param refs: list of str : COCO reference sentences for the particular image to be evaluated\n\n        :returns score: int (ROUGE-L score for the candidate evaluated against references)\n\n        \"\"\"\n\n\n        assert (len(candidate) == 1)\n\n        assert (len(refs) > 0)\n\n        prec = []\n\n        rec = []\n\n        # split into tokens\n\n        token_c = candidate[0].split(\" \")\n\n        for reference in refs:\n            # split into tokens\n            hh =1\n\n            token_r = reference.split(\" \")\n\n            # compute the longest common subsequence\n\n            lcs = my_lcs(token_r, token_c)\n\n            prec.append(lcs / float(len(token_c)))\n\n            rec.append(lcs / float(len(token_r)))\n\n        prec_max = max(prec)\n\n        rec_max = max(rec)\n\n        if (prec_max != 0 and rec_max != 0):\n\n            score = ((1 + self.beta ** 2) * prec_max * rec_max) / float(rec_max + self.beta ** 2 * prec_max)\n\n        else:\n\n            score = 0.0\n\n        return score\n\n    def compute_score(self, references, hypotheses):\n\n        \"\"\"\n\n        Computes Rouge-L score given a set of reference and candidate sentences for the dataset\n\n        Invoked by evaluate_captions.py\n\n        :param hypo_for_image: dict : candidate / test sentences with \"image name\" key and \"tokenized sentences\" as values\n\n        :param ref_for_image: dict : reference MS-COCO sentences with \"image name\" key and \"tokenized sentences\" as values\n\n        :returns: average_score: float (mean ROUGE-L score computed by averaging scores for all the images)\n\n        \"\"\"\n\n        # assert (gts.keys() == res.keys())\n        #\n        # imgIds = gts.keys()\n\n        score = []\n\n        for i in range(len(hypotheses)):\n            hypo = hypotheses[i]\n            ref = references[i]\n\n            score.append(self.calc_score(hypo, ref))\n\n            # Sanity check.\n\n            assert (type(hypo) is list)\n\n            assert (len(hypo) == 1)\n\n            assert (type(ref) is list)\n\n            assert (len(ref) > 0)\n\n        average_score = np.mean(np.array(score))\n\n        return average_score, np.array(score)\n\n    def method(self):\n\n        return \"Rouge\""}
{"type": "source_file", "path": "eval_func/cider/cider_scorer.py", "content": "#!/usr/bin/env python\n# Tsung-Yi Lin <tl483@cornell.edu>\n# Ramakrishna Vedantam <vrama91@vt.edu>\n\nimport copy\nfrom collections import defaultdict\nimport numpy as np\nimport pdb\nimport math\n\ndef precook(s, n=4, out=False):\n    \"\"\"\n    Takes a string as input and returns an object that can be given to\n    either cook_refs or cook_test. This is optional: cook_refs and cook_test\n    can take string arguments as well.\n    :param s: string : sentence to be converted into ngrams\n    :param n: int    : number of ngrams for which representation is calculated\n    :return: term frequency vector for occuring ngrams\n    \"\"\"\n    words = s.split()\n    counts = defaultdict(int)\n    for k in range(1,n+1):\n        for i in range(len(words)-k+1):\n            ngram = tuple(words[i:i+k])\n            counts[ngram] += 1\n    return counts\n\ndef cook_refs(refs, n=4): ## lhuang: oracle will call with \"average\"\n    '''Takes a list of reference sentences for a single segment\n    and returns an object that encapsulates everything that BLEU\n    needs to know about them.\n    :param refs: list of string : reference sentences for some image\n    :param n: int : number of ngrams for which (ngram) representation is calculated\n    :return: result (list of dict)\n    '''\n    return [precook(ref, n) for ref in refs]\n\ndef cook_test(test, n=4):\n    '''Takes a test sentence and returns an object that\n    encapsulates everything that BLEU needs to know about it.\n    :param test: list of string : hypothesis sentence for some image\n    :param n: int : number of ngrams for which (ngram) representation is calculated\n    :return: result (dict)\n    '''\n    return precook(test, n, True)\n\nclass CiderScorer(object):\n    \"\"\"CIDEr scorer.\n    \"\"\"\n\n    def copy(self):\n        ''' copy the refs.'''\n        new = CiderScorer(n=self.n)\n        new.ctest = copy.copy(self.ctest)\n        new.crefs = copy.copy(self.crefs)\n        return new\n\n    def __init__(self, test=None, refs=None, n=4, sigma=6.0):\n        ''' singular instance '''\n        self.n = n\n        self.sigma = sigma\n        self.crefs = []\n        self.ctest = []\n        self.document_frequency = defaultdict(float)\n        self.cook_append(test, refs)\n        self.ref_len = None\n\n    def cook_append(self, test, refs):\n        '''called by constructor and __iadd__ to avoid creating new instances.'''\n\n        if refs is not None:\n            self.crefs.append(cook_refs(refs))\n            if test is not None:\n                self.ctest.append(cook_test(test)) ## N.B.: -1\n            else:\n                self.ctest.append(None) # lens of crefs and ctest have to match\n\n    def size(self):\n        assert len(self.crefs) == len(self.ctest), \"refs/test mismatch! %d<>%d\" % (len(self.crefs), len(self.ctest))\n        return len(self.crefs)\n\n    def __iadd__(self, other):\n        '''add an instance (e.g., from another sentence).'''\n\n        if type(other) is tuple:\n            ## avoid creating new CiderScorer instances\n            self.cook_append(other[0], other[1])\n        else:\n            self.ctest.extend(other.ctest)\n            self.crefs.extend(other.crefs)\n\n        return self\n    def compute_doc_freq(self):\n        '''\n        Compute term frequency for reference data.\n        This will be used to compute idf (inverse document frequency later)\n        The term frequency is stored in the object\n        :return: None\n        '''\n        for refs in self.crefs:\n            # refs, k ref captions of one image\n            for ngram in set([ngram for ref in refs for (ngram,count) in ref.items()]):\n                self.document_frequency[ngram] += 1\n            # maxcounts[ngram] = max(maxcounts.get(ngram,0), count)\n\n    def compute_cider(self):\n        def counts2vec(cnts):\n            \"\"\"\n            Function maps counts of ngram to vector of tfidf weights.\n            The function returns vec, an array of dictionary that store mapping of n-gram and tf-idf weights.\n            The n-th entry of array denotes length of n-grams.\n            :param cnts:\n            :return: vec (array of dict), norm (array of float), length (int)\n            \"\"\"\n            vec = [defaultdict(float) for _ in range(self.n)]\n            length = 0\n            norm = [0.0 for _ in range(self.n)]\n            for (ngram, term_freq) in cnts.items():\n                # give word count 1 if it doesn't appear in reference corpus\n                df = np.log(max(1.0, self.document_frequency[ngram]))\n                # ngram index\n                n = len(ngram)-1\n                # tf (term_freq) * idf (precomputed idf) for n-grams\n                vec[n][ngram] = float(term_freq)*(self.ref_len - df)\n                # compute norm for the vector.  the norm will be used for computing similarity\n                norm[n] += pow(vec[n][ngram], 2)\n\n                if n == 1:\n                    length += term_freq\n            norm = [np.sqrt(n) for n in norm]\n            return vec, norm, length\n\n        def sim(vec_hyp, vec_ref, norm_hyp, norm_ref, length_hyp, length_ref):\n            '''\n            Compute the cosine similarity of two vectors.\n            :param vec_hyp: array of dictionary for vector corresponding to hypothesis\n            :param vec_ref: array of dictionary for vector corresponding to reference\n            :param norm_hyp: array of float for vector corresponding to hypothesis\n            :param norm_ref: array of float for vector corresponding to reference\n            :param length_hyp: int containing length of hypothesis\n            :param length_ref: int containing length of reference\n            :return: array of score for each n-grams cosine similarity\n            '''\n            delta = float(length_hyp - length_ref)\n            # measure consine similarity\n            val = np.array([0.0 for _ in range(self.n)])\n            for n in range(self.n):\n                # ngram\n                for (ngram,count) in vec_hyp[n].items():\n                    # vrama91 : added clipping\n                    val[n] += min(vec_hyp[n][ngram], vec_ref[n][ngram]) * vec_ref[n][ngram]\n\n                if (norm_hyp[n] != 0) and (norm_ref[n] != 0):\n                    val[n] /= (norm_hyp[n]*norm_ref[n])\n\n                assert(not math.isnan(val[n]))\n                # vrama91: added a length based gaussian penalty\n                val[n] *= np.e**(-(delta**2)/(2*self.sigma**2))\n            return val\n\n        # compute log reference length\n        self.ref_len = np.log(float(len(self.crefs)))\n        if len(self.crefs) == 1:\n            self.ref_len = 1\n        scores = []\n        for test, refs in zip(self.ctest, self.crefs):\n            # compute vector for test captions\n            vec, norm, length = counts2vec(test)\n            # compute vector for ref captions\n            score = np.array([0.0 for _ in range(self.n)])\n            for ref in refs:\n                vec_ref, norm_ref, length_ref = counts2vec(ref)\n                score += sim(vec, vec_ref, norm, norm_ref, length, length_ref)\n            # change by vrama91 - mean of ngram scores, instead of sum\n            score_avg = np.mean(score)\n            # divide by number of references\n            score_avg /= len(refs)\n            # multiply score by 10\n            score_avg *= 10.0\n            # append score of an image to the score list\n            scores.append(score_avg)\n        return scores\n\n    def compute_score(self, option=None, verbose=0):\n        # compute idf\n        self.compute_doc_freq()\n        # assert to check document frequency\n        assert(len(self.ctest) >= max(self.document_frequency.values()))\n        # compute cider score\n        score = self.compute_cider()\n        # debug\n        # print score\n        return np.mean(np.array(score)), np.array(score)\n"}
{"type": "source_file", "path": "docs/onevision_trial.py", "content": "from llava.model.builder import load_pretrained_model\nfrom llava.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token\nfrom llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IGNORE_INDEX\nfrom llava.conversation import conv_templates, SeparatorStyle\n\nfrom PIL import Image\nimport requests\nimport copy\nimport torch\n\nimport sys\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\npretrained = \"lmms-lab/llava-onevision-qwen2-0.5b-si\"\nmodel_name = \"llava_qwen\"\ndevice = \"cuda\"\ndevice_map = \"auto\"\ntokenizer, model, image_processor, max_length = load_pretrained_model(pretrained, None, model_name, device_map=device_map)  # Add any other thing you want to pass in llava_model_args\n\nmodel.eval()\n\nurl = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\nimage_tensor = process_images([image], image_processor, model.config)\nimage_tensor = [_image.to(dtype=torch.float16, device=device) for _image in image_tensor]\n\nconv_template = \"qwen_1_5\"  # Make sure you use correct chat template for different models\nquestion = DEFAULT_IMAGE_TOKEN + \"\\nWhat is shown in this image?\"\nconv = copy.deepcopy(conv_templates[conv_template])\nconv.append_message(conv.roles[0], question)\nconv.append_message(conv.roles[1], None)\nprompt_question = conv.get_prompt()\n\ninput_ids = tokenizer_image_token(prompt_question, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(device)\nimage_sizes = [image.size]\n\n\ncont = model.generate(\n    input_ids,\n    images=image_tensor,\n    image_sizes=image_sizes,\n    do_sample=False,\n    temperature=0,\n    max_new_tokens=4096,\n)\ntext_outputs = tokenizer.batch_decode(cont, skip_special_tokens=True)\nprint(text_outputs)\n\nfrom threading import Thread\nfrom transformers import TextIteratorStreamer\nimport json\n\nurl = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\nimage_tensor = process_images([image], image_processor, model.config)\nimage_tensor = [_image.to(dtype=torch.float16, device=device) for _image in image_tensor]\n\nconv_template = \"qwen_1_5\"\nquestion = DEFAULT_IMAGE_TOKEN + \"\\nWhat is shown in this image?\"\nconv = copy.deepcopy(conv_templates[conv_template])\nconv.append_message(conv.roles[0], question)\nconv.append_message(conv.roles[1], None)\nprompt_question = conv.get_prompt()\n\ninput_ids = tokenizer_image_token(prompt_question, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(device)\nimage_sizes = [image.size]\n\nmax_context_length = getattr(model.config, \"max_position_embeddings\", 2048)\nnum_image_tokens = question.count(DEFAULT_IMAGE_TOKEN) * model.get_vision_tower().num_patches\n\nstreamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True, timeout=15)\n\nmax_new_tokens = min(4096, max_context_length - input_ids.shape[-1] - num_image_tokens)\n\nif max_new_tokens < 1:\n    print(\n        json.dumps(\n            {\n                \"text\": question + \"Exceeds max token length. Please start a new conversation, thanks.\",\n                \"error_code\": 0,\n            }\n        )\n    )\nelse:\n    gen_kwargs = {\n        \"do_sample\": False,\n        \"temperature\": 0,\n        \"max_new_tokens\": max_new_tokens,\n        \"images\": image_tensor,\n        \"image_sizes\": image_sizes,\n    }\n\n    thread = Thread(\n        target=model.generate,\n        kwargs=dict(\n            inputs=input_ids,\n            streamer=streamer,\n            **gen_kwargs,\n        ),\n    )\n    thread.start()\n\n    generated_text = \"\"\n    for new_text in streamer:\n        generated_text += new_text\n        sys.stdout.write(new_text)\n        sys.stdout.flush()\n\n    print(\"\\nFinal output:\", generated_text)\n"}
{"type": "source_file", "path": "eval_func/meteor/__init__.py", "content": "__author__ = 'tylin'\n"}
{"type": "source_file", "path": "eval_func/bleu/__init__.py", "content": "__author__ = 'tylin'\n"}
{"type": "source_file", "path": "eval_func/bleu/bleu.py", "content": "#!/usr/bin/env python\n# \n# File Name : bleu.py\n#\n# Description : Wrapper for BLEU scorer.\n#\n# Creation Date : 06-01-2015\n# Last Modified : Thu 19 Mar 2015 09:13:28 PM PDT\n# Authors : Hao Fang <hfang@uw.edu> and Tsung-Yi Lin <tl483@cornell.edu>\n\nfrom .bleu_scorer import BleuScorer\n\n\nclass Bleu:\n    def __init__(self, n=4):\n        # default compute Blue score up to 4\n        self._n = n\n        self._hypo_for_image = {}\n        self.ref_for_image = {}\n\n    def compute_score(self, gts, res):\n\n        bleu_scorer = BleuScorer(n=self._n)\n        for i in range(len(res)):\n            hypo = res[i]\n            ref = gts[i]\n\n            # Sanity check.\n            assert(type(hypo) is list)\n            assert(len(hypo) == 1)\n            assert(type(ref) is list)\n            assert(len(ref) >= 1)\n\n            bleu_scorer += (hypo[0], ref)\n\n        #score, scores = bleu_scorer.compute_score(option='shortest')\n        score, scores = bleu_scorer.compute_score(option='closest', verbose=1)\n        #score, scores = bleu_scorer.compute_score(option='average', verbose=1)\n\n        # return (bleu, bleu_info)\n        return score, scores\n\n    def method(self):\n        return \"Bleu\"\n"}
{"type": "source_file", "path": "eval_func/LB_intention_mllm/evaluate_for_lb_brand_intention_mllm.py", "content": "import json\nimport pandas as pd\nimport numpy as np\nimport os\nimport argparse\nfrom sklearn.metrics import precision_recall_curve, classification_report, confusion_matrix\n\ndef main(file_path):\n    data = []\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            data.append(json.loads(line))\n\n    # Extract true labels and predicted logits\n    y_true = []\n    logits = []\n    for item in data:\n        try:\n            y_true.append(item['gt_label'])\n            logits.append([float(x) for x in item['logits'].split('\\t')])\n        except:\n            item = item[0]\n            y_true.append(item['gt_label'])\n            logits.append([float(x) for x in item['logits'].split('\\t')])            \n\n    # Convert labels to numbers\n    labels = ['<A>', '<B>', '<C>', '<D>', '<E>', '<F>']\n    y_true = [labels.index(label) for label in y_true]\n\n    # Use argmax to compute overall six-class predictions\n    logits = np.array(logits)\n    y_pred = np.argmax(logits, axis=1)\n\n    # Generate classification report as a dictionary\n    report_dict = classification_report(y_true, y_pred, target_names=labels, zero_division=0, output_dict=True)\n    # Convert dictionary to DataFrame for easier formatting\n    report_df = pd.DataFrame(report_dict).transpose()\n    # Convert the support column to integers\n    report_df['support'] = report_df['support'].astype(int)\n    # Remove the accuracy row\n    report_df = report_df.drop(index='accuracy')\n    # Format the floating-point numbers to the desired number of decimal places\n    report_df[['precision', 'recall', 'f1-score']] = report_df[['precision', 'recall', 'f1-score']].applymap(lambda x: f\"{x:.4f}\")\n\n    overall_report = report_df\n    overall_conf_matrix = confusion_matrix(y_true, y_pred)\n\n    # Calculate P95R, P90R, P80R, P70R, R95P, R90P metrics for each class\n    metrics = {label: {'P70R': None, 'P80R': None, 'P90R': None, 'P95R': None, 'R95P': None, 'R90P': None} for label in labels}\n\n    for i in range(len(labels)):\n        class_logits = logits[:, i]\n        y_true_bin = (np.array(y_true) == i).astype(int)\n        \n        # Calculate precision-recall curve\n        precisions, recalls, thresholds = precision_recall_curve(y_true_bin, class_logits)\n\n        # Calculate P95R, P90R, P80R, P70R\n        for precision_target in [0.70, 0.80, 0.90, 0.95]:\n            best_index = np.argmin(np.abs(precisions - precision_target))\n            if best_index < len(thresholds):\n                metrics[labels[i]][f'P{int(precision_target * 100)}R'] = {\n                    'Precision': precisions[best_index],\n                    'Recall': recalls[best_index],\n                    'F1-score': 2 * (precisions[best_index] * recalls[best_index]) / (precisions[best_index] + recalls[best_index] + 1e-9),\n                    'Threshold': thresholds[best_index]\n                }\n\n        # Calculate R95P and R90P\n        for recall_target in [0.95, 0.90]:\n            best_index = np.argmin(np.abs(recalls - recall_target))\n            if best_index < len(thresholds):\n                metrics[labels[i]][f'R{int(recall_target * 100)}P'] = {\n                    'Precision': precisions[best_index],\n                    'Recall': recalls[best_index],\n                    'F1-score': 2 * (precisions[best_index] * recalls[best_index]) / (precisions[best_index] + recalls[best_index] + 1e-9),\n                    'Threshold': thresholds[best_index]\n                }\n\n    # Prepare output content\n    output = []\n    output.append(\"Overall PR:\")\n    output.append(overall_report.to_string())\n    output.append(\"Overall Confusion Matrix:\")\n    output.append(str(overall_conf_matrix))\n    output.append(\"\")\n\n    for label in labels:\n        output.append(f\"{label}:\")\n        for metric, values in metrics[label].items():\n            if values is not None:\n                output.append(f\"  {metric} - Precision: {values['Precision']:.3f}, Recall: {values['Recall']:.3f}, F1-score: {values['F1-score']:.3f}, Threshold: {values['Threshold']:.3f}\")\n            else:\n                output.append(f\"  {metric} - Precision: , Recall: , F1-score: , Threshold: \")\n        output.append(\"\")\n\n    output_text = \"\\n\".join(output)\n\n    # Print results\n    print(output_text)\n\n    # Save to metrics.txt\n    metrics_file_path = os.path.join(os.path.dirname(file_path), 'metrics.txt')\n    with open(metrics_file_path, 'w', encoding='utf-8') as f:\n        f.write(output_text)\n    \nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='Evaluate metrics for LB-MLLM.')\n    parser.add_argument('file_path', type=str, help='Path to the JSONL file containing the test results.')\n    args = parser.parse_args()\n    main(args.file_path)\n"}
{"type": "source_file", "path": "llava/conversation.py", "content": "import dataclasses\nfrom enum import auto, Enum\nfrom typing import List, Any, Dict, Union, Tuple\nimport re\nimport base64\nfrom io import BytesIO\nfrom PIL import Image\nfrom transformers import AutoTokenizer\n\n\nclass SeparatorStyle(Enum):\n    \"\"\"Different separator style.\"\"\"\n\n    SINGLE = auto()\n    TWO = auto()\n    MPT = auto()\n    PLAIN = auto()\n    CHATML = auto()\n    LLAMA_2 = auto()\n    LLAMA_3 = auto()\n    QWEN = auto()\n    GEMMA = auto()\n\n\n@dataclasses.dataclass\nclass Conversation:\n    \"\"\"A class that keeps all conversation history.\"\"\"\n\n    system: str\n    roles: List[str]\n    messages: List[List[str]]\n    offset: int\n    sep_style: SeparatorStyle = SeparatorStyle.SINGLE\n    sep: str = \"###\"\n    sep2: str = None\n    version: str = \"Unknown\"\n\n    tokenizer_id: str = \"\"\n    tokenizer: Any = None\n    # Stop criteria (the default one is EOS token)\n    stop_str: Union[str, List[str]] = None\n    # Stops generation if meeting any token in this list\n    stop_token_ids: List[int] = None\n\n    skip_next: bool = False\n\n    def get_prompt(self):\n        messages = self.messages\n        if len(messages) > 0 and type(messages[0][1]) is tuple:\n            messages = self.messages.copy()\n            init_role, init_msg = messages[0].copy()\n            init_msg = init_msg[0]\n            if \"mmtag\" in self.version:\n                init_msg = init_msg.replace(\"<image>\", \"\").strip()\n                messages[0] = (init_role, init_msg)\n                messages.insert(0, (self.roles[0], \"<Image><image></Image>\"))\n                messages.insert(1, (self.roles[1], \"Received.\"))\n            elif not init_msg.startswith(\"<image>\"):\n                init_msg = init_msg.replace(\"<image>\", \"\").strip()\n                messages[0] = (init_role, \"<image>\\n\" + init_msg)\n            else:\n                messages[0] = (init_role, init_msg)\n\n        if self.sep_style == SeparatorStyle.SINGLE:\n            ret = self.system + self.sep\n            for role, message in messages:\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    ret += role + \": \" + message + self.sep\n                else:\n                    ret += role + \":\"\n\n        elif self.sep_style == SeparatorStyle.TWO:\n            seps = [self.sep, self.sep2]\n            ret = self.system + seps[0]\n            for i, (role, message) in enumerate(messages):\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    ret += role + \": \" + message + seps[i % 2]\n                else:\n                    ret += role + \":\"\n\n        elif self.sep_style == SeparatorStyle.CHATML:\n            ret = \"\" if self.system == \"\" else self.system + self.sep + \"\\n\"\n            for role, message in messages:\n                if message:\n                    if type(message) is tuple:\n                        message, images, _ = message\n                        message = \"<image>\" * len(images) + message\n                    ret += role + \"\\n\" + message + self.sep + \"\\n\"\n                else:\n                    ret += role + \"\\n\"\n            return ret\n\n        elif self.sep_style == SeparatorStyle.LLAMA_3:\n            chat_template_messages = [{\"role\": \"system\", \"content\": self.system}]\n            for role, message in messages:\n                if message:\n                    if type(message) is tuple:\n                        message, images = message\n                        message = \"<image>\" * len(images) + message\n                    chat_template_messages.append({\"role\": role, \"content\": message})\n\n            # print(chat_template_messages)\n            return self.tokenizer.apply_chat_template(chat_template_messages, tokenize=False, add_generation_prompt=True)\n            # ret = \"\" if self.system == \"\" else self.system + self.sep + \"\\n\"\n            # for role, message in messages:\n            #     if message:\n            #         if type(message) is tuple:\n            #             message, images = message\n            #             message = \"<image>\" * len(images) + message\n            #         ret += role + \"\\n\" + message + self.sep + \"\\n\"\n            #     else:\n            #         ret += role + \"\\n\"\n            # return ret\n\n        elif self.sep_style == SeparatorStyle.MPT:\n            ret = self.system + self.sep\n            for role, message in messages:\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    ret += role + message + self.sep\n                else:\n                    ret += role\n\n        elif self.sep_style == SeparatorStyle.GEMMA:\n            ret = \"\"\n            for i, (role, message) in enumerate(messages):\n                assert role == self.roles[i % 2], \"Conversation should alternate user/assistant/user/assistant/...\"\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    ret += role + message + self.sep\n                else:\n                    ret += role\n\n        elif self.sep_style == SeparatorStyle.LLAMA_2:\n            wrap_sys = lambda msg: f\"<<SYS>>\\n{msg}\\n<</SYS>>\\n\\n\" if len(msg) > 0 else msg\n            wrap_inst = lambda msg: f\"[INST] {msg} [/INST]\"\n            ret = \"\"\n\n            for i, (role, message) in enumerate(messages):\n                if i == 0:\n                    assert message, \"first message should not be none\"\n                    assert role == self.roles[0], \"first message should come from user\"\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    if i == 0:\n                        message = wrap_sys(self.system) + message\n                    if i % 2 == 0:\n                        message = wrap_inst(message)\n                        ret += self.sep + message\n                    else:\n                        ret += \" \" + message + \" \" + self.sep2\n                else:\n                    ret += \"\"\n            ret = ret.lstrip(self.sep)\n\n        elif self.sep_style == SeparatorStyle.PLAIN:\n            seps = [self.sep, self.sep2]\n            ret = self.system\n            for i, (role, message) in enumerate(messages):\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    ret += message + seps[i % 2]\n                else:\n                    ret += \"\"\n        else:\n            raise ValueError(f\"Invalid style: {self.sep_style}\")\n\n        return ret\n\n    def append_message(self, role, message):\n        self.messages.append([role, message])\n\n    def process_image(self, image, image_process_mode, return_pil=False, image_format=\"PNG\"):\n        if image_process_mode == \"Pad\":\n\n            def expand2square(pil_img, background_color=(122, 116, 104)):\n                width, height = pil_img.size\n                if width == height:\n                    return pil_img\n                elif width > height:\n                    result = Image.new(pil_img.mode, (width, width), background_color)\n                    result.paste(pil_img, (0, (width - height) // 2))\n                    return result\n                else:\n                    result = Image.new(pil_img.mode, (height, height), background_color)\n                    result.paste(pil_img, ((height - width) // 2, 0))\n                    return result\n\n            image = expand2square(image)\n        elif image_process_mode in [\"Default\", \"Crop\"]:\n            pass\n        elif image_process_mode == \"Resize\":\n            image = image.resize((336, 336))\n        else:\n            raise ValueError(f\"Invalid image_process_mode: {image_process_mode}\")\n\n        if type(image) is not Image.Image:\n            image = Image.open(image).convert(\"RGB\")\n\n        max_hw, min_hw = max(image.size), min(image.size)\n        aspect_ratio = max_hw / min_hw\n        max_len, min_len = 672, 448\n        shortest_edge = int(min(max_len / aspect_ratio, min_len, min_hw))\n        longest_edge = int(shortest_edge * aspect_ratio)\n        W, H = image.size\n        if H > W:\n            H, W = longest_edge, shortest_edge\n        else:\n            H, W = shortest_edge, longest_edge\n        image = image.resize((W, H))\n        if return_pil:\n            return image\n        else:\n            buffered = BytesIO()\n            image.save(buffered, format=image_format)\n            img_b64_str = base64.b64encode(buffered.getvalue()).decode()\n            return img_b64_str\n\n    def get_images(self, return_pil=False, return_path=False):\n        images = []\n        for i, (role, msg) in enumerate(self.messages[self.offset :]):\n            if i % 2 == 0:\n                if type(msg) is tuple:\n                    msg, image, image_process_mode = msg\n                    if type(image) != list:\n                        image = [image]\n                    for img in image:\n                        if not return_path and self.is_image_file(img):\n                            img = self.process_image(img, image_process_mode, return_pil=return_pil)\n                        else:\n                            images.append(img)\n        return images\n\n    def is_image_file(self, filename):\n        image_extensions = [\".png\", \".jpg\", \".jpeg\", \".gif\", \".bmp\", \".tiff\", \".webp\"]\n        return any(filename.lower().endswith(ext) for ext in image_extensions)\n\n    def is_video_file(self, filename):\n        video_extensions = [\".mp4\", \".mov\", \".avi\", \".mkv\", \".wmv\", \".flv\", \".mpeg\", \".mpg\"]\n        return any(filename.lower().endswith(ext) for ext in video_extensions)\n\n    def to_gradio_chatbot(self):\n        ret = []\n        for i, (role, msg) in enumerate(self.messages[self.offset :]):\n            if i % 2 == 0:\n                if type(msg) is tuple:\n                    msg, image, image_process_mode = msg\n                    if type(image) != list:\n                        image = [image]\n                    if len(image) == 1:\n                        msg = \"<image>\\n\" + msg.replace(\"<image>\", \"\").strip()\n                    else:\n                        msg = re.sub(r\"(<image>)\\n(?=<image>)\", r\"\\1 \", msg)\n\n                    img_str_list = []                         \n                    for img in image:\n                        if self.is_image_file(img):\n                            img_b64_str = self.process_image(img, \"Default\", return_pil=False, image_format=\"JPEG\")\n                            img_str = f'<img src=\"data:image/jpeg;base64,{img_b64_str}\" style=\"max-width: 256px; max-height: 256px; width: auto; height: auto; object-fit: contain;\"/>'\n                            img_str_list.append(img_str)\n                        elif self.is_video_file(img):\n                            ret.append(((img,), None))\n\n                    msg = msg.strip()\n                    img_place_holder = \"\"\n                    for img_str in img_str_list:\n                        img_place_holder += f\"{img_str}\\n\\n\"\n\n                    if len(img_str_list) > 0:\n                        msg = f\"{img_place_holder}\\n\\n{msg}\"\n\n                    if len(msg) > 0:\n                        ret.append([msg, None])\n                else:\n                    ret.append([msg, None])\n            else:\n                ret[-1][-1] = msg\n        return ret\n\n    def copy(self):\n        return Conversation(system=self.system, roles=self.roles, messages=[[x, y] for x, y in self.messages], offset=self.offset, sep_style=self.sep_style, sep=self.sep, sep2=self.sep2, version=self.version)\n\n    def dict(self):\n        if len(self.get_images()) > 0:\n            return {\n                \"system\": self.system,\n                \"roles\": self.roles,\n                \"messages\": [[x, y[0] if type(y) is tuple else y] for x, y in self.messages],\n                \"offset\": self.offset,\n                \"sep\": self.sep,\n                \"sep2\": self.sep2,\n            }\n        return {\n            \"system\": self.system,\n            \"roles\": self.roles,\n            \"messages\": self.messages,\n            \"offset\": self.offset,\n            \"sep\": self.sep,\n            \"sep2\": self.sep2,\n        }\n\n\nconv_vicuna_v0 = Conversation(\n    system=\"A chat between a curious human and an artificial intelligence assistant. \" \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n    roles=(\"Human\", \"Assistant\"),\n    messages=[\n        [\"Human\", \"What are the key differences between renewable and non-renewable energy sources?\"],\n        [\n            \"Assistant\",\n            \"Renewable energy sources are those that can be replenished naturally in a relatively \"\n            \"short amount of time, such as solar, wind, hydro, geothermal, and biomass. \"\n            \"Non-renewable energy sources, on the other hand, are finite and will eventually be \"\n            \"depleted, such as coal, oil, and natural gas. Here are some key differences between \"\n            \"renewable and non-renewable energy sources:\\n\"\n            \"1. Availability: Renewable energy sources are virtually inexhaustible, while non-renewable \"\n            \"energy sources are finite and will eventually run out.\\n\"\n            \"2. Environmental impact: Renewable energy sources have a much lower environmental impact \"\n            \"than non-renewable sources, which can lead to air and water pollution, greenhouse gas emissions, \"\n            \"and other negative effects.\\n\"\n            \"3. Cost: Renewable energy sources can be more expensive to initially set up, but they typically \"\n            \"have lower operational costs than non-renewable sources.\\n\"\n            \"4. Reliability: Renewable energy sources are often more reliable and can be used in more remote \"\n            \"locations than non-renewable sources.\\n\"\n            \"5. Flexibility: Renewable energy sources are often more flexible and can be adapted to different \"\n            \"situations and needs, while non-renewable sources are more rigid and inflexible.\\n\"\n            \"6. Sustainability: Renewable energy sources are more sustainable over the long term, while \"\n            \"non-renewable sources are not, and their depletion can lead to economic and social instability.\\n\",\n        ],\n    ],\n    offset=2,\n    sep_style=SeparatorStyle.SINGLE,\n    sep=\"###\",\n)\n\nconv_vicuna_v1 = Conversation(\n    system=\"A chat between a curious user and an artificial intelligence assistant. \" \"The assistant gives helpful, detailed, and polite answers to the user's questions.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"v1\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.TWO,\n    sep=\" \",\n    sep2=\"</s>\",\n)\n\nconv_llama_2 = Conversation(\n    system=\"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"llama_v2\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.LLAMA_2,\n    sep=\"<s>\",\n    sep2=\"</s>\",\n)\n\nconv_llava_llama_2 = Conversation(\n    system=\"You are a helpful language and vision assistant. \" \"You are able to understand the visual content that the user provides, \" \"and assist the user with a variety of tasks using natural language.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"llama_v2\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.LLAMA_2,\n    sep=\"<s>\",\n    sep2=\"</s>\",\n)\n\n# conv_llava_llama_3 = Conversation(\n#     system=\"You are a helpful language and vision assistant. \" \"You are able to understand the visual content that the user provides, \" \"and assist the user with a variety of tasks using natural language.\",\n#     roles=(\"user\", \"assistant\"),\n#     version=\"llama_v3\",\n#     messages=[],\n#     offset=0,\n#     sep=\"<|eot_id|>\",\n#     sep_style=SeparatorStyle.LLAMA_3,\n#     tokenizer_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n#     tokenizer=AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\"),\n#     stop_token_ids=[128009],\n# )\n\nconv_mistral_instruct = Conversation(\n    system=\"\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"llama_v2\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.LLAMA_2,\n    sep=\"\",\n    sep2=\"</s>\",\n)\n\nconv_llava_llama_2_simple = Conversation(\n    system=\"Answer the questions about the visual content that the user provides.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"llama_v2\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.LLAMA_2,\n    sep=\"<s>\",\n    sep2=\"</s>\",\n)\n\nconv_llava_llama_2_mmtag = Conversation(\n    system=\"Answer the questions about the visual content that the user provides.\" \"The visual content will be provided with the following format: <Image>visual content</Image>.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"llama_v2_mmtag\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.LLAMA_2,\n    sep=\"<s>\",\n    sep2=\"</s>\",\n)\n\nconv_mpt = Conversation(\n    system=\"\"\"<|im_start|>system\nA conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers.\"\"\",\n    roles=(\"<|im_start|>user\\n\", \"<|im_start|>assistant\\n\"),\n    version=\"mpt\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.MPT,\n    sep=\"<|im_end|>\",\n)\n\nconv_qwen = Conversation(\n    system=\"\"\"<|im_start|>system\nYou are a helpful assistant.\"\"\",\n    roles=(\"<|im_start|>user\", \"<|im_start|>assistant\"),\n    version=\"qwen\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.CHATML,\n    sep=\"<|im_end|>\",\n)\n\nconv_gemma_instruct = Conversation(system=\"\", roles=(\"<start_of_turn>user\\n\", \"<start_of_turn>model\\n\"), version=\"gemma\", messages=[], offset=0, sep_style=SeparatorStyle.GEMMA, sep=\"<end_of_turn>\\n\")\n\nconv_llava_plain = Conversation(\n    system=\"\",\n    roles=(\"\", \"\"),\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.PLAIN,\n    sep=\"\\n\",\n)\n\nconv_llava_v0 = Conversation(\n    system=\"A chat between a curious human and an artificial intelligence assistant. \" \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n    roles=(\"Human\", \"Assistant\"),\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.SINGLE,\n    sep=\"###\",\n)\n\nconv_llava_v0_mmtag = Conversation(\n    system=\"A chat between a curious user and an artificial intelligence assistant. \"\n    \"The assistant is able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\"\n    \"The visual content will be provided with the following format: <Image>visual content</Image>.\",\n    roles=(\"Human\", \"Assistant\"),\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.SINGLE,\n    sep=\"###\",\n    version=\"v0_mmtag\",\n)\n\nconv_llava_v1 = Conversation(\n    system=\"A chat between a curious human and an artificial intelligence assistant. \" \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"v1\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.TWO,\n    sep=\" \",\n    sep2=\"</s>\",\n)\n\nconv_llava_v1_mmtag = Conversation(\n    system=\"A chat between a curious user and an artificial intelligence assistant. \"\n    \"The assistant is able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\"\n    \"The visual content will be provided with the following format: <Image>visual content</Image>.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.TWO,\n    sep=\" \",\n    sep2=\"</s>\",\n    version=\"v1_mmtag\",\n)\n\nconv_mistral_orca = Conversation(\n    system=\"\"\"<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI. Write out your reasoning step-by-step to be sure you get the right answers!\"\"\",\n    roles=(\"<|im_start|>user\\n\", \"<|im_start|>assistant\\n\"),\n    version=\"mpt\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.MPT,\n    sep=\"<|im_end|>\",\n)\n\nconv_mistral_zephyr = Conversation(\n    system=\"\"\"<|system|>\nYou are a helpful AI assistant.\"\"\",\n    roles=(\"<|user|>\\n\", \"<|assistant|>\\n\"),\n    version=\"mpt\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.MPT,\n    sep=\"</s>\",\n)\n\nconv_mistral_direct = Conversation(\n    system=\"\"\"<|im_start|>system\nAnswer the questions.\"\"\",\n    roles=(\"<|im_start|>user\\n\", \"<|im_start|>assistant\\n\"),\n    version=\"mpt\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.MPT,\n    sep=\"<|im_end|>\",\n)\n\nconv_chatml_direct = Conversation(\n    system=\"\"\"<|im_start|>system\nAnswer the questions.\"\"\",\n    roles=(\"<|im_start|>user\\n\", \"<|im_start|>assistant\\n\"),\n    version=\"mpt\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.MPT,\n    sep=\"<|im_end|>\",\n)\n\ndefault_conversation = conv_vicuna_v0\nconv_templates = {\n    \"default\": conv_vicuna_v0,\n    \"v0\": conv_vicuna_v0,\n    \"v1\": conv_vicuna_v1,\n    \"vicuna_v1\": conv_vicuna_v1,\n    \"llama_2\": conv_llama_2,\n    \"mistral_instruct\": conv_mistral_instruct,\n    \"mistral_orca\": conv_mistral_orca,\n    \"mistral_zephyr\": conv_mistral_zephyr,\n    \"mistral_direct\": conv_mistral_direct,\n    \"plain\": conv_llava_plain,\n    \"v0_plain\": conv_llava_plain,\n    \"chatml_direct\": conv_chatml_direct,\n    \"llava_v0\": conv_llava_v0,\n    \"llava_v0_mmtag\": conv_llava_v0_mmtag,\n    \"llava_v1\": conv_llava_v1,\n    \"llava_v1_mmtag\": conv_llava_v1_mmtag,\n    \"llava_llama_2\": conv_llava_llama_2,\n    # \"llava_llama_3\": conv_llava_llama_3,\n    \"llava_llama_2_simple\": conv_llava_llama_2_simple,\n    \"llava_llama_2_mmtag\": conv_llava_llama_2_mmtag,\n    \"llava_mistral_instruct\": conv_mistral_instruct,\n    \"mpt\": conv_mpt,\n    \"qwen_1_5\": conv_qwen,\n    \"qwen_2\": conv_qwen,\n    \"gemma_instruct\": conv_gemma_instruct,\n}\n\n\nif __name__ == \"__main__\":\n    print(default_conversation.get_prompt())\n"}
{"type": "source_file", "path": "llava/model/__init__.py", "content": "import os\n\nAVAILABLE_MODELS = {\n    \"llava_llama\": \"LlavaLlamaForCausalLM, LlavaConfig\",\n    \"llava_qwen\": \"LlavaQwenForCausalLM, LlavaQwenConfig\",\n    \"llava_mistral\": \"LlavaMistralForCausalLM, LlavaMistralConfig\",\n    \"llava_mixtral\": \"LlavaMixtralForCausalLM, LlavaMixtralConfig\",\n    \"llava_qwen_cc\": \"LlavaQwenCCForCausalLM, LlavaQwenCCConfig\",\n    # \"llava_qwen_moe\": \"LlavaQwenMoeForCausalLM, LlavaQwenMoeConfig\",    \n    # Add other models as needed\n}\n\nfor model_name, model_classes in AVAILABLE_MODELS.items():\n    try:\n        exec(f\"from .language_model.{model_name} import {model_classes}\")\n        print(f\"Successfully imported {model_name} from llava.language_model.{model_name}\")\n    except Exception as e:\n        print(f\"Failed to import {model_name} from llava.language_model.{model_name}. Error: {e}\")"}
{"type": "source_file", "path": "llava/model/language_model/llava_qwen_moe.py", "content": "#    Copyright 2024 Hao Zhang\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom typing import List, Optional, Tuple, Union, Dict\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\n\nimport transformers\nfrom transformers import AutoConfig, AutoModelForCausalLM\n\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nfrom transformers.generation.utils import GenerateOutput\n\n# from ...constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\nfrom llava.model.llava_arch import LlavaMetaModel, LlavaMetaForCausalLM\nfrom transformers import Qwen2MoeConfig, Qwen2MoeModel, Qwen2MoeForCausalLM\n\n# from .qwen.modeling_qwen import QWenLMHeadModel, QWenModel\n# from .qwen.configuration_qwen import QWenConfig\n\n\nclass LlavaQwenMoeConfig(Qwen2MoeConfig):\n    model_type = \"llava_qwen_moe\"\n\n\nclass LlavaQwenMoeModel(LlavaMetaModel, Qwen2MoeModel):\n    config_class = LlavaQwenMoeConfig\n\n    def __init__(self, config: Qwen2MoeConfig):\n        super(LlavaQwenMoeModel, self).__init__(config)\n\n\nclass LlavaQwenMoeForCausalLM(Qwen2MoeForCausalLM, LlavaMetaForCausalLM):\n    config_class = LlavaQwenMoeConfig\n\n    def __init__(self, config):\n        # super(Qwen2MoeForCausalLM, self).__init__(config)\n        Qwen2MoeForCausalLM.__init__(self, config)\n        config.model_type = \"llava_qwen_moe\"\n        config.rope_scaling = None\n\n        self.model = LlavaQwenMoeModel(config)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.model\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        images: Optional[torch.FloatTensor] = None,\n        image_sizes: Optional[List[List[int]]] = None,\n        return_dict: Optional[bool] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        dpo_forward: Optional[bool] = False,\n        cache_position=None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n\n        if inputs_embeds is None:\n            (input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels) = self.prepare_inputs_labels_for_multimodal(input_ids, position_ids, attention_mask, past_key_values, labels, images, modalities, image_sizes)\n\n        if dpo_forward:\n            outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n            hidden_states = outputs[0]\n            logits = self.lm_head(hidden_states)\n            return logits, labels\n\n        else:\n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                labels=labels,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs: Optional[torch.Tensor] = None,\n        images: Optional[torch.Tensor] = None,\n        image_sizes: Optional[torch.Tensor] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        **kwargs,\n    ) -> Union[GenerateOutput, torch.LongTensor]:\n        position_ids = kwargs.pop(\"position_ids\", None)\n        attention_mask = kwargs.pop(\"attention_mask\", None)\n        if \"inputs_embeds\" in kwargs:\n            raise NotImplementedError(\"`inputs_embeds` is not supported\")\n\n        if images is not None:\n            (inputs, position_ids, attention_mask, _, inputs_embeds, _) = self.prepare_inputs_labels_for_multimodal(inputs, position_ids, attention_mask, None, None, images, modalities, image_sizes=image_sizes)\n        else:\n            inputs_embeds = self.get_model().embed_tokens(inputs)\n\n        return super().generate(position_ids=position_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, **kwargs)\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n        images = kwargs.pop(\"images\", None)\n        image_sizes = kwargs.pop(\"image_sizes\", None)\n        inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs)\n        if images is not None:\n            inputs[\"images\"] = images\n        if image_sizes is not None:\n            inputs[\"image_sizes\"] = image_sizes\n        return inputs\n\n\nAutoConfig.register(\"llava_qwen_moe\", LlavaQwenMoeConfig)\nAutoModelForCausalLM.register(LlavaQwenMoeConfig, LlavaQwenMoeForCausalLM)\n"}
{"type": "source_file", "path": "llava/model/consolidate.py", "content": "\"\"\"\nUsage:\npython3 -m llava.model.consolidate --src ~/model_weights/llava-7b --dst ~/model_weights/llava-7b_consolidate\n\"\"\"\n\nimport argparse\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom llava.model import *\nfrom llava.model.utils import auto_upgrade\n\n\ndef consolidate_ckpt(src_path, dst_path):\n    print(\"Loading model\")\n    auto_upgrade(src_path)\n    src_model = AutoModelForCausalLM.from_pretrained(src_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n    src_tokenizer = AutoTokenizer.from_pretrained(src_path, use_fast=False)\n    src_model.save_pretrained(dst_path)\n    src_tokenizer.save_pretrained(dst_path)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--src\", type=str, required=True)\n    parser.add_argument(\"--dst\", type=str, required=True)\n\n    args = parser.parse_args()\n\n    consolidate_ckpt(args.src, args.dst)\n"}
{"type": "source_file", "path": "llava/model/language_model/llava_mistral.py", "content": "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\n\nfrom transformers import AutoConfig, AutoModelForCausalLM, MistralConfig, MistralModel, MistralForCausalLM, GenerationConfig\n\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nfrom transformers.generation.utils import GenerateOutput\n\nfrom ..llava_arch import LlavaMetaModel, LlavaMetaForCausalLM\n\n\nclass LlavaMistralConfig(MistralConfig):\n    model_type = \"llava_mistral\"\n    temperature: float = 0.0  # reset to 0.0, previously 0.9 for Vicuna\n    max_new_tokens: int = 1024\n    do_sample: bool = False\n    top_p: Optional[float] = None\n\n\nclass LlavaMistralModel(LlavaMetaModel, MistralModel):\n    config_class = LlavaMistralConfig\n\n    def __init__(self, config: MistralConfig):\n        super(LlavaMistralModel, self).__init__(config)\n\n\nclass LlavaMistralForCausalLM(MistralForCausalLM, LlavaMetaForCausalLM):\n    config_class = LlavaMistralConfig\n\n    def __init__(self, config):\n        super(MistralForCausalLM, self).__init__(config)\n\n        config.model_type = \"llava_mistral\"\n        config.rope_scaling = None\n\n        self.model = LlavaMistralModel(config)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.model\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        images: Optional[torch.FloatTensor] = None,\n        image_sizes: Optional[List[List[int]]] = None,\n        return_dict: Optional[bool] = None,\n        cache_position=None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n\n        if inputs_embeds is None:\n            (input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels) = self.prepare_inputs_labels_for_multimodal(input_ids, position_ids, attention_mask, past_key_values, labels, images, image_sizes)\n\n        return super().forward(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            labels=labels,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs: Optional[torch.Tensor] = None,\n        images: Optional[torch.Tensor] = None,\n        image_sizes: Optional[torch.Tensor] = None,\n        **kwargs,\n    ) -> Union[GenerateOutput, torch.LongTensor]:\n        position_ids = kwargs.pop(\"position_ids\", None)\n        attention_mask = kwargs.pop(\"attention_mask\", None)\n        if \"inputs_embeds\" in kwargs:\n            raise NotImplementedError(\"`inputs_embeds` is not supported\")\n\n        if images is not None:\n            (inputs, position_ids, attention_mask, _, inputs_embeds, _) = self.prepare_inputs_labels_for_multimodal(inputs, position_ids, attention_mask, None, None, images, image_sizes=image_sizes)\n        else:\n            inputs_embeds = self.get_model().embed_tokens(inputs)\n\n        return super().generate(position_ids=position_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, **kwargs)\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n        images = kwargs.pop(\"images\", None)\n        image_sizes = kwargs.pop(\"image_sizes\", None)\n        inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs)\n        if images is not None:\n            inputs[\"images\"] = images\n        if image_sizes is not None:\n            inputs[\"image_sizes\"] = image_sizes\n        return inputs\n\n\nAutoConfig.register(\"llava_mistral\", LlavaMistralConfig)\nAutoModelForCausalLM.register(LlavaMistralConfig, LlavaMistralForCausalLM)\n"}
{"type": "source_file", "path": "llava/model/language_model/llava_gemma.py", "content": "#    Copyright 2024 Duc Q. Nguyen, Haotian Liu and Bo Li\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\n\nfrom transformers import AutoConfig, AutoModelForCausalLM, GemmaConfig, GemmaModel, GemmaForCausalLM\n\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nfrom transformers.generation.utils import GenerateOutput\n\nfrom ..llava_arch import LlavaMetaModel, LlavaMetaForCausalLM\n\n\nclass LlavaGemmaConfig(GemmaConfig):\n    model_type = \"llava_gemma\"\n\n\nclass LlavaGemmaModel(LlavaMetaModel, GemmaModel):\n    config_class = LlavaGemmaConfig\n\n    def __init__(self, config: GemmaConfig):\n        super(LlavaGemmaModel, self).__init__(config)\n\n\nclass LlavaGemmaForCausalLM(GemmaForCausalLM, LlavaMetaForCausalLM):\n    config_class = LlavaGemmaConfig\n\n    def __init__(self, config):\n        super(GemmaForCausalLM, self).__init__(config)\n        self.model = LlavaGemmaModel(config)\n\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.model\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        images: Optional[torch.FloatTensor] = None,\n        image_sizes: Optional[List[List[int]]] = None,\n        return_dict: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n\n        if inputs_embeds is None:\n            (input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels) = self.prepare_inputs_labels_for_multimodal(input_ids, position_ids, attention_mask, past_key_values, labels, images, image_sizes)\n\n        return super().forward(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            labels=labels,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            cache_position=cache_position,\n        )\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs: Optional[torch.Tensor] = None,\n        images: Optional[torch.Tensor] = None,\n        image_sizes: Optional[torch.Tensor] = None,\n        **kwargs,\n    ) -> Union[GenerateOutput, torch.LongTensor]:\n        position_ids = kwargs.pop(\"position_ids\", None)\n        attention_mask = kwargs.pop(\"attention_mask\", None)\n        if \"inputs_embeds\" in kwargs:\n            raise NotImplementedError(\"`inputs_embeds` is not supported\")\n\n        if images is not None:\n            (inputs, position_ids, attention_mask, _, inputs_embeds, _) = self.prepare_inputs_labels_for_multimodal(inputs, position_ids, attention_mask, None, None, images, image_sizes=image_sizes)\n        else:\n            inputs_embeds = self.get_model().embed_tokens(inputs)\n\n        return super().generate(position_ids=position_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, **kwargs)\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n        images = kwargs.pop(\"images\", None)\n        image_sizes = kwargs.pop(\"image_sizes\", None)\n        inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs)\n        if images is not None:\n            inputs[\"images\"] = images\n        if image_sizes is not None:\n            inputs[\"image_sizes\"] = image_sizes\n        return inputs\n\n\nAutoConfig.register(\"llava_gemma\", LlavaGemmaConfig)\nAutoModelForCausalLM.register(LlavaGemmaConfig, LlavaGemmaForCausalLM)\n"}
{"type": "source_file", "path": "llava/model/language_model/llava_qwen_cc.py", "content": "#    Copyright 2024 Hao Zhang\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom typing import List, Optional, Tuple, Union, Dict\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\n\nimport transformers\nfrom transformers import AutoConfig, AutoModelForCausalLM, LlamaConfig, LlamaModel, LlamaForCausalLM\n\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nfrom transformers.generation.utils import GenerateOutput\n\n# from ...constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\nfrom llava.model.llava_arch_cc import LlavaMetaCCModel, LlavaMetaCCForCausalLM\nfrom transformers import Qwen2Config, Qwen2Model, Qwen2ForCausalLM\n\n# from .qwen.modeling_qwen import QWenLMHeadModel, QWenModel\n# from .qwen.configuration_qwen import QWenConfig\n\n\nclass LlavaQwenCCConfig(Qwen2Config):\n    model_type = \"llava_qwen_cc\"\n\n\nclass LlavaQwenCCModel(LlavaMetaCCModel, Qwen2Model):\n    config_class = LlavaQwenCCConfig\n\n    def __init__(self, config: Qwen2Config):\n        super(LlavaQwenCCModel, self).__init__(config)\n\n\nclass LlavaQwenCCForCausalLM(Qwen2ForCausalLM, LlavaMetaCCForCausalLM):\n    config_class = LlavaQwenCCConfig\n\n    def __init__(self, config):\n        # super(Qwen2ForCausalLM, self).__init__(config)\n        Qwen2ForCausalLM.__init__(self, config)\n        config.model_type = \"llava_qwen_cc\"\n        config.rope_scaling = None\n\n        self.model = LlavaQwenCCModel(config)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.model\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        images: Optional[torch.FloatTensor] = None,\n        image_sizes: Optional[List[List[int]]] = None,\n        return_dict: Optional[bool] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        dpo_forward: Optional[bool] = False,\n        cache_position=None,\n        **kwargs,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n\n        if inputs_embeds is None:\n            (input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels) = self.prepare_inputs_labels_for_multimodal(input_ids, position_ids, attention_mask, past_key_values, labels, images, modalities, image_sizes)\n\n        if dpo_forward:\n            outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n            hidden_states = outputs[0]\n            logits = self.lm_head(hidden_states)\n            return logits, labels\n\n        else:\n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                labels=labels,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs: Optional[torch.Tensor] = None,\n        images: Optional[torch.Tensor] = None,\n        image_sizes: Optional[torch.Tensor] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        **kwargs,\n    ) -> Union[GenerateOutput, torch.LongTensor]:\n        position_ids = kwargs.pop(\"position_ids\", None)\n        attention_mask = kwargs.pop(\"attention_mask\", None)\n        if \"inputs_embeds\" in kwargs:\n            raise NotImplementedError(\"`inputs_embeds` is not supported\")\n\n        is_draw_pictures = kwargs.pop(\"is_draw_pictures\", False)\n        if is_draw_pictures:\n            (inputs, position_ids, attention_mask, _, inputs_embeds, _, last_stage_features, combined_emb, change_emb) = self.prepare_inputs_labels_for_multimodal(inputs, position_ids, attention_mask, None, None, images, modalities, image_sizes=image_sizes, is_draw_pictures=is_draw_pictures)\n            text_generation = super().generate(position_ids=position_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, **kwargs)\n            return text_generation, last_stage_features, combined_emb, change_emb\n        else:\n            if images is not None:\n                (inputs, position_ids, attention_mask, _, inputs_embeds, _) = self.prepare_inputs_labels_for_multimodal(inputs, position_ids, attention_mask, None, None, images, modalities, image_sizes=image_sizes)\n            else:\n                inputs_embeds = self.get_model().embed_tokens(inputs)\n            return super().generate(position_ids=position_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, **kwargs)\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n        images = kwargs.pop(\"images\", None)\n        image_sizes = kwargs.pop(\"image_sizes\", None)\n        inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs)\n        if images is not None:\n            inputs[\"images\"] = images\n        if image_sizes is not None:\n            inputs[\"image_sizes\"] = image_sizes\n        return inputs\n\n\nAutoConfig.register(\"llava_qwen_cc\", LlavaQwenCCConfig)\nAutoModelForCausalLM.register(LlavaQwenCCConfig, LlavaQwenCCForCausalLM)\n"}
{"type": "source_file", "path": "llava/model/builder.py", "content": "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nimport os\nimport warnings\nimport shutil\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\nimport torch\nfrom llava.model import *\nfrom llava.constants import DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\nfrom llava.utils import rank0_print\n\n\ndef load_pretrained_model(model_path, model_base, model_name, load_8bit=False, load_4bit=False, device_map=\"auto\", attn_implementation=\"flash_attention_2\", customized_config=None, overwrite_config=None, **kwargs):\n    kwargs[\"device_map\"] = device_map\n\n    if load_8bit:\n        kwargs[\"load_in_8bit\"] = True\n    elif load_4bit:\n        kwargs[\"load_in_4bit\"] = True\n        kwargs[\"quantization_config\"] = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\")\n    else:\n        kwargs[\"torch_dtype\"] = torch.float16\n\n    if customized_config is not None:\n        kwargs[\"config\"] = customized_config\n\n    if \"multimodal\" in kwargs:\n        if kwargs[\"multimodal\"] is True:\n            is_multimodal = True\n            kwargs.pop(\"multimodal\")\n    else:\n        is_multimodal = False\n\n    if \"llava\" in model_name.lower() or is_multimodal:\n        # Load LLaVA model\n        if \"lora\" in model_name.lower() and model_base is None:\n            warnings.warn(\n                \"There is `lora` in model name but no `model_base` is provided. If you are loading a LoRA model, please provide the `model_base` argument. Detailed instruction: https://github.com/haotian-liu/LLaVA#launch-a-model-worker-lora-weights-unmerged.\"\n            )\n        if \"lora\" in model_name.lower() and model_base is not None:\n            lora_cfg_pretrained = AutoConfig.from_pretrained(model_path)\n            tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n            rank0_print(\"Loading LLaVA from base model...\")\n            if \"mixtral\" in model_name.lower():\n                from llava.model.language_model.llava_mixtral import LlavaMixtralConfig\n\n                lora_cfg_pretrained = LlavaMixtralConfig.from_pretrained(model_path)\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                model = LlavaMixtralForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n            elif \"mistral\" in model_name.lower():\n                from llava.model.language_model.llava_mistral import LlavaMistralConfig\n\n                lora_cfg_pretrained = LlavaMistralConfig.from_pretrained(model_path)\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                model = LlavaMistralForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n            elif \"gemma\" in model_name.lower():\n                from llava.model.language_model.llava_gemma import LlavaGemmaConfig\n\n                lora_cfg_pretrained = LlavaGemmaConfig.from_pretrained(model_path)\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                model = LlavaGemmaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n            else:\n                from llava.model.language_model.llava_llama import LlavaConfig\n\n                lora_cfg_pretrained = LlavaConfig.from_pretrained(model_path)\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n\n            token_num, tokem_dim = model.lm_head.out_features, model.lm_head.in_features\n            if model.lm_head.weight.shape[0] != token_num:\n                model.lm_head.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\n                model.model.embed_tokens.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\n\n            rank0_print(\"Loading additional LLaVA weights...\")\n            if os.path.exists(os.path.join(model_path, \"non_lora_trainables.bin\")):\n                non_lora_trainables = torch.load(os.path.join(model_path, \"non_lora_trainables.bin\"), map_location=\"cpu\")\n            else:\n                # this is probably from HF Hub\n                from huggingface_hub import hf_hub_download\n\n                def load_from_hf(repo_id, filename, subfolder=None):\n                    cache_file = hf_hub_download(repo_id=repo_id, filename=filename, subfolder=subfolder)\n                    return torch.load(cache_file, map_location=\"cpu\")\n\n                non_lora_trainables = load_from_hf(model_path, \"non_lora_trainables.bin\")\n            non_lora_trainables = {(k[11:] if k.startswith(\"base_model.\") else k): v for k, v in non_lora_trainables.items()}\n            if any(k.startswith(\"model.model.\") for k in non_lora_trainables):\n                non_lora_trainables = {(k[6:] if k.startswith(\"model.\") else k): v for k, v in non_lora_trainables.items()}\n            model.load_state_dict(non_lora_trainables, strict=False)\n\n            from peft import PeftModel\n\n            rank0_print(\"Loading LoRA weights...\")\n            model = PeftModel.from_pretrained(model, model_path)\n            rank0_print(\"Merging LoRA weights...\")\n            model = model.merge_and_unload()\n            rank0_print(\"Model is loaded...\")\n        elif model_base is not None:  # this may be mm projector only, loading projector with preset language mdoel\n            rank0_print(f\"Loading LLaVA from base model {model_base}...\")\n            if \"mixtral\" in model_name.lower():\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                cfg_pretrained = AutoConfig.from_pretrained(model_path)\n                model = LlavaMixtralForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n            elif \"mistral\" in model_name.lower() or \"zephyr\" in model_name.lower():\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                cfg_pretrained = AutoConfig.from_pretrained(model_path)\n                model = LlavaMistralForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n            elif \"gemma\" in model_name.lower():\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                cfg_pretrained = AutoConfig.from_pretrained(model_path)\n                model = LlavaGemmaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n            elif (\n                \"wizardlm-2\" in model_name.lower()\n                and \"vicuna\" in model_name.lower()\n                or \"llama\" in model_name.lower()\n                or \"yi\" in model_name.lower()\n                or \"nous-hermes\" in model_name.lower()\n                or \"llava-v1.6-34b\" in model_name.lower()\n                or \"llava-v1.5\" in model_name.lower()\n            ):\n                from llava.model.language_model.llava_llama import LlavaConfig\n\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n                if customized_config is None:\n                    llava_cfg = LlavaConfig.from_pretrained(model_path)\n                    if \"v1.5\" in model_name.lower():\n                        llava_cfg.delay_load = True  # a workaround for correctly loading v1.5 models\n                else:\n                    llava_cfg = customized_config\n\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                llava_cfg = LlavaConfig.from_pretrained(model_path)\n                model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=llava_cfg, **kwargs)\n            else:\n                raise ValueError(f\"Model {model_name} not supported\")\n\n            mm_projector_weights = torch.load(os.path.join(model_path, \"mm_projector.bin\"), map_location=\"cpu\")\n            mm_projector_weights = {k: v.to(torch.float16) for k, v in mm_projector_weights.items()}\n            model.load_state_dict(mm_projector_weights, strict=False)\n        else:\n            rank0_print(f\"Loaded LLaVA model: {model_path}\")\n            if \"cc\" in model_name.lower():\n                tokenizer = AutoTokenizer.from_pretrained(model_path)\n                from llava.model.language_model.llava_qwen_cc import LlavaQwenCCConfig\n                if overwrite_config is not None:\n                    llava_cfg = LlavaQwenCCConfig.from_pretrained(model_path)\n                    rank0_print(f\"Overwriting config with {overwrite_config}\")\n                    for k, v in overwrite_config.items():\n                        setattr(llava_cfg, k, v)\n                    model = LlavaQwenCCForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, config=llava_cfg, **kwargs)\n            elif \"mixtral\" in model_name.lower():\n                from llava.model.language_model.llava_mixtral import LlavaMixtralConfig\n\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n                if customized_config is None:\n                    llava_cfg = LlavaMixtralConfig.from_pretrained(model_path)\n                else:\n                    llava_cfg = customized_config\n\n                if overwrite_config is not None:\n                    rank0_print(f\"Overwriting config with {overwrite_config}\")\n                    for k, v in overwrite_config.items():\n                        setattr(llava_cfg, k, v)\n\n                tokenizer = AutoTokenizer.from_pretrained(model_path)\n                model = LlavaMixtralForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, config=llava_cfg, **kwargs)\n\n            elif \"mistral\" in model_name.lower() or \"zephyr\" in model_name.lower():\n                tokenizer = AutoTokenizer.from_pretrained(model_path)\n                model = LlavaMistralForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, **kwargs)\n            elif (\n                \"wizardlm-2\" in model_name.lower()\n                and \"vicuna\" in model_name.lower()\n                or \"llama\" in model_name.lower()\n                or \"yi\" in model_name.lower()\n                or \"nous-hermes\" in model_name.lower()\n                or \"llava-v1.6-34b\" in model_name.lower()\n                or \"llava-v1.5\" in model_name.lower()\n            ):\n                from llava.model.language_model.llava_llama import LlavaConfig\n\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n                if customized_config is None:\n                    llava_cfg = LlavaConfig.from_pretrained(model_path)\n                    if \"v1.5\" in model_name.lower():\n                        llava_cfg.delay_load = True  # a workaround for correctly loading v1.5 models\n                else:\n                    llava_cfg = customized_config\n\n                if overwrite_config is not None:\n                    rank0_print(f\"Overwriting config with {overwrite_config}\")\n                    for k, v in overwrite_config.items():\n                        setattr(llava_cfg, k, v)\n\n                model = LlavaLlamaForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, config=llava_cfg, **kwargs)\n\n            elif \"qwen\" in model_name.lower() or \"quyen\" in model_name.lower():\n                tokenizer = AutoTokenizer.from_pretrained(model_path)\n                if \"moe\" in model_name.lower() or \"A14B\" in model_name.lower():\n                    from llava.model.language_model.llava_qwen_moe import LlavaQwenMoeConfig\n                    if overwrite_config is not None:\n                        llava_cfg = LlavaQwenMoeConfig.from_pretrained(model_path)\n                        rank0_print(f\"Overwriting config with {overwrite_config}\")\n                        for k, v in overwrite_config.items():\n                            setattr(llava_cfg, k, v)\n                        model = LlavaQwenMoeForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, config=llava_cfg, **kwargs)\n                    else:\n                        model = LlavaQwenMoeForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, **kwargs)\n\n                else:\n                    from llava.model.language_model.llava_qwen import LlavaQwenConfig\n                    if overwrite_config is not None:\n                        llava_cfg = LlavaQwenConfig.from_pretrained(model_path)\n                        rank0_print(f\"Overwriting config with {overwrite_config}\")\n                        for k, v in overwrite_config.items():\n                            setattr(llava_cfg, k, v)\n                        model = LlavaQwenForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, config=llava_cfg, **kwargs)\n                    else:\n                        model = LlavaQwenForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, **kwargs)\n\n            elif \"gemma\" in model_name.lower():\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n                cfg_pretrained = AutoConfig.from_pretrained(model_path)\n                model = LlavaGemmaForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, config=cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n            else:\n                try:\n                    from llava.model.language_model.llava_llama import LlavaConfig\n\n                    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n                    if customized_config is None:\n                        llava_cfg = LlavaConfig.from_pretrained(model_path)\n                        if \"v1.5\" in model_path.lower():\n                            llava_cfg.delay_load = True  # a workaround for correctly loading v1.5 models\n                    else:\n                        llava_cfg = customized_config\n\n                    if overwrite_config is not None:\n                        rank0_print(f\"Overwriting config with {overwrite_config}\")\n                        for k, v in overwrite_config.items():\n                            setattr(llava_cfg, k, v)\n                    model = LlavaLlamaForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, config=llava_cfg, **kwargs)\n                except:\n                    raise ValueError(f\"Model {model_name} not supported\")\n\n    else:\n        # Load language model\n        if model_base is not None:\n            # PEFT model\n            from peft import PeftModel\n\n            tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n            model = AutoModelForCausalLM.from_pretrained(model_base, torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map=\"auto\")\n            print(f\"Loading LoRA weights from {model_path}\")\n            model = PeftModel.from_pretrained(model, model_path)\n            print(f\"Merging weights\")\n            model = model.merge_and_unload()\n            print(\"Convert to FP16...\")\n            model.to(torch.float16)\n        else:\n            use_fast = False\n            if \"mpt\" in model_name.lower().replace(\"prompt\", \"\"):\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n                model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, trust_remote_code=True, **kwargs)\n            else:\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n                model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\n    \n    rank0_print(f\"Model Class: {model.__class__.__name__}\")\n    image_processor = None\n\n    if \"llava\" in model_name.lower() or is_multimodal:\n        mm_use_im_start_end = getattr(model.config, \"mm_use_im_start_end\", False)\n        mm_use_im_patch_token = getattr(model.config, \"mm_use_im_patch_token\", True)\n        if mm_use_im_patch_token:\n            tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\n        if mm_use_im_start_end:\n            tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\n        model.resize_token_embeddings(len(tokenizer))\n\n        vision_tower = model.get_vision_tower()\n        if not vision_tower.is_loaded:\n            vision_tower.load_model(device_map=device_map)\n        if device_map != \"auto\":\n            vision_tower.to(device=\"cuda\", dtype=torch.float16)\n        image_processor = vision_tower.image_processor\n\n    if hasattr(model.config, \"max_sequence_length\"):\n        context_len = model.config.max_sequence_length\n    elif hasattr(model.config, \"max_position_embeddings\"):\n        context_len = model.config.max_position_embeddings\n    elif hasattr(model.config, \"tokenizer_model_max_length\"):\n        context_len = model.config.tokenizer_model_max_length\n    else:\n        context_len = 2048\n\n    return tokenizer, model, image_processor, context_len\n"}
{"type": "source_file", "path": "llava/mm_utils.py", "content": "from PIL import Image\nfrom io import BytesIO\nimport base64\nimport math\nimport ast\nimport re\nimport torch\nfrom transformers import StoppingCriteria\nfrom llava.constants import IMAGE_TOKEN_INDEX\n\n\ndef resize_and_center_crop(image, shortest_edge_length):\n    # Calculate new dimensions and resize\n    aspect_ratio = float(image.width) / float(image.height)\n    if aspect_ratio > 1:\n        new_width = int(shortest_edge_length * aspect_ratio)\n        new_height = shortest_edge_length\n    else:\n        new_width = shortest_edge_length\n        new_height = int(shortest_edge_length / aspect_ratio)\n    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)\n\n    # Calculate the position and perform the center crop\n    left = (new_width - shortest_edge_length) / 2\n    top = (new_height - shortest_edge_length) / 2\n    right = (new_width + shortest_edge_length) / 2\n    bottom = (new_height + shortest_edge_length) / 2\n    cropped_image = resized_image.crop((left, top, right, bottom))\n\n    return cropped_image\n\n\ndef auto_pad_images(image, grid_params):\n    assert isinstance(image, Image.Image), \"Input should be a Pillow Image\"\n    assert len(grid_params) > 0, \"Grid parameters should not be empty\"\n\n    # Step 1: Calculate and find the closest aspect ratio\n    input_width, input_height = image.size\n    input_aspect_ratio = input_width / input_height\n    candidate_resolutions = [(w / h, w, h) for w in grid_params for h in grid_params]\n    closest_aspect_ratio = min(candidate_resolutions, key=lambda x: abs(input_aspect_ratio - x[0]))\n\n    candidate_resolutions = [(x[1], x[2]) for x in candidate_resolutions if abs(x[0] - closest_aspect_ratio[0]) < 1e-3]\n\n    target_resolution = min(candidate_resolutions, key=lambda res: abs(max(input_width, input_height) / max(res) - 1))\n\n    resize_width, resize_height = target_resolution\n    if input_width > input_height:\n        resize_height = int(resize_width / input_aspect_ratio)\n    else:\n        resize_width = int(resize_height * input_aspect_ratio)\n    resized_image = image.resize((resize_width, resize_height), Image.ANTIALIAS)\n\n    # Step 5: Pad the resized image if necessary to match the target resolution\n    pad_width = target_resolution[0] - resize_width\n    pad_height = target_resolution[1] - resize_height\n    padded_image = Image.new(\"RGB\", target_resolution, color=(0, 0, 0))\n    padded_image.paste(resized_image, (pad_width // 2, pad_height // 2))\n\n    return padded_image\n\n\ndef extract_patches(image, patch_size, overlap_ratio):\n    assert isinstance(image, Image.Image), \"Input should be a Pillow Image\"\n    assert patch_size > 0, \"Patch size should be greater than 0\"\n    assert 0 <= overlap_ratio < 1, \"Overlap ratio should be between 0 and 1\"\n\n    W, H = image.size\n    patches = []\n\n    stride = int(patch_size * (1 - overlap_ratio))\n\n    num_patches_y = (H - patch_size) // stride + 1\n    num_patches_x = (W - patch_size) // stride + 1\n\n    y_start = (H - (num_patches_y - 1) * stride - patch_size) // 2\n    x_start = (W - (num_patches_x - 1) * stride - patch_size) // 2\n\n    for y in range(y_start, y_start + num_patches_y * stride, stride):\n        for x in range(x_start, x_start + num_patches_x * stride, stride):\n            patch = image.crop((x, y, x + patch_size, y + patch_size))\n            patches.append(patch)\n\n    return patches\n\n\ndef process_highres_image_crop_split(image, data_args, processor=None):\n    crop_resolution = data_args.image_crop_resolution\n    split_resolution = data_args.image_split_resolution\n    if processor is None:\n        processor = data_args.image_processor\n    image_crop = resize_and_center_crop(image, crop_resolution)\n    image_patches = extract_patches(image_crop, patch_size=split_resolution, overlap_ratio=0)\n    image_patches = [processor.preprocess(image_patch, return_tensors=\"pt\")[\"pixel_values\"][0] for image_patch in image_patches]\n    return torch.stack(image_patches, dim=0)\n\n\ndef process_highres_image(image, processor, grid_pinpoints):\n    grid_params = [int(x) for x in grid_pinpoints.split(\",\")]\n    width_height = max(image.size)\n    fit_grid_params = [x for x in grid_params if x >= width_height]\n    if len(fit_grid_params) == 0:\n        select_size = max(grid_params)\n    else:\n        select_size = min(fit_grid_params)\n    # FIXME: always select the 448\n    select_size = max(grid_params)\n    image_padded = expand2square(image, tuple(int(x * 255) for x in processor.image_mean))\n\n    # FIXME: this seems to be a bug that it always resizes instead of padding\n    image_original_resize = image.resize((processor.size[\"shortest_edge\"], processor.size[\"shortest_edge\"]))\n    image_padded = image_padded.resize((select_size, select_size))\n    image_patches = extract_patches(image_padded, patch_size=processor.size[\"shortest_edge\"], overlap_ratio=0)\n    image_patches = [image_original_resize] + image_patches\n    image_patches = [processor.preprocess(image_patch, return_tensors=\"pt\")[\"pixel_values\"][0] for image_patch in image_patches]\n    return torch.stack(image_patches, dim=0)\n\n\ndef select_best_resolution(original_size, possible_resolutions):\n    \"\"\"\n    Selects the best resolution from a list of possible resolutions based on the original size.\n\n    Args:\n        original_size (tuple): The original size of the image in the format (width, height).\n        possible_resolutions (list): A list of possible resolutions in the format [(width1, height1), (width2, height2), ...].\n\n    Returns:\n        tuple: The best fit resolution in the format (width, height).\n    \"\"\"\n    original_width, original_height = original_size\n    best_fit = None\n    max_effective_resolution = 0\n    min_wasted_resolution = float(\"inf\")\n\n    for width, height in possible_resolutions:\n        # Calculate the downscaled size to keep the aspect ratio\n        scale = min(width / original_width, height / original_height)\n        downscaled_width, downscaled_height = int(original_width * scale), int(original_height * scale)\n\n        # Calculate effective and wasted resolutions\n        effective_resolution = min(downscaled_width * downscaled_height, original_width * original_height)\n        wasted_resolution = (width * height) - effective_resolution\n\n        if effective_resolution > max_effective_resolution or (effective_resolution == max_effective_resolution and wasted_resolution < min_wasted_resolution):\n            max_effective_resolution = effective_resolution\n            min_wasted_resolution = wasted_resolution\n            best_fit = (width, height)\n\n    return best_fit\n\n\ndef resize_and_pad_image(image, target_resolution):\n    \"\"\"\n    Resize and pad an image to a target resolution while maintaining aspect ratio.\n\n    Args:\n        image (PIL.Image.Image): The input image.\n        target_resolution (tuple): The target resolution (width, height) of the image.\n\n    Returns:\n        PIL.Image.Image: The resized and padded image.\n    \"\"\"\n    original_width, original_height = image.size\n    target_width, target_height = target_resolution\n\n    # Determine which dimension (width or height) to fill\n    scale_w = target_width / original_width\n    scale_h = target_height / original_height\n\n    if scale_w < scale_h:\n        # Width will be filled completely\n        new_width = target_width\n        new_height = min(math.ceil(original_height * scale_w), target_height)\n    else:\n        # Height will be filled completely\n        new_height = target_height\n        new_width = min(math.ceil(original_width * scale_h), target_width)\n\n    # Resize the image\n    resized_image = image.resize((new_width, new_height))\n\n    # Create a new image with the target size and paste the resized image onto it\n    new_image = Image.new(\"RGB\", (target_width, target_height), (0, 0, 0))\n    paste_x = (target_width - new_width) // 2\n    paste_y = (target_height - new_height) // 2\n    new_image.paste(resized_image, (paste_x, paste_y))\n\n    return new_image\n\n\ndef divide_to_patches(image, patch_size):\n    \"\"\"\n    Divides an image into patches of a specified size.\n\n    Args:\n        image (PIL.Image.Image): The input image.\n        patch_size (int): The size of each patch.\n\n    Returns:\n        list: A list of PIL.Image.Image objects representing the patches.\n    \"\"\"\n    patches = []\n    width, height = image.size\n    for i in range(0, height, patch_size):\n        for j in range(0, width, patch_size):\n            box = (j, i, j + patch_size, i + patch_size)\n            patch = image.crop(box)\n            patches.append(patch)\n\n    return patches\n\n\ndef get_anyres_image_grid_shape(image_size, grid_pinpoints, patch_size):\n    \"\"\"\n    Calculate the shape of the image patch grid after the preprocessing for images of any resolution.\n\n    Args:\n        image_size (tuple): The size of the input image in the format (width, height).\n        grid_pinpoints (str): A string representation of a list of possible resolutions.\n        patch_size (int): The size of each image patch.\n\n    Returns:\n        tuple: The shape of the image patch grid in the format (width, height).\n    \"\"\"\n    if isinstance(grid_pinpoints, str) and \"x\" in grid_pinpoints:\n        assert patch_size in [224, 336, 384, 448, 512], \"patch_size should be in [224, 336, 384, 448, 512]\"\n        # Use regex to extract the range from the input string\n        matches = re.findall(r\"\\((\\d+)x(\\d+)\\)\", grid_pinpoints)\n        range_start = tuple(map(int, matches[0]))\n        range_end = tuple(map(int, matches[-1]))\n        # Generate a matrix of tuples from (range_start[0], range_start[1]) to (range_end[0], range_end[1])\n        grid_pinpoints = [(i, j) for i in range(range_start[0], range_end[0] + 1) for j in range(range_start[1], range_end[1] + 1)]\n        # Multiply all elements by patch_size\n        grid_pinpoints = [[dim * patch_size for dim in pair] for pair in grid_pinpoints]\n    if type(grid_pinpoints) is list:\n        possible_resolutions = grid_pinpoints\n    else:\n        possible_resolutions = ast.literal_eval(grid_pinpoints)\n    width, height = select_best_resolution(image_size, possible_resolutions)\n    return width // patch_size, height // patch_size\n\n\ndef process_anyres_image(image, processor, grid_pinpoints):\n    \"\"\"\n    Process an image with variable resolutions.\n\n    Args:\n        image (PIL.Image.Image): The input image to be processed.\n        processor: The image processor object.\n        grid_pinpoints (str): A string representation of a list of possible resolutions.\n\n    Returns:\n        torch.Tensor: A tensor containing the processed image patches.\n    \"\"\"\n    # Convert grid_pinpoints from string to list\n    if isinstance(grid_pinpoints, str) and \"x\" in grid_pinpoints:\n        try:\n            patch_size = processor.size[0]\n        except Exception as e:\n            patch_size = processor.size[\"shortest_edge\"]\n        assert patch_size in [224, 336, 384, 448, 512], \"patch_size should be in [224, 336, 384, 448, 512]\"\n        # Use regex to extract the range from the input string\n        matches = re.findall(r\"\\((\\d+)x(\\d+)\\)\", grid_pinpoints)\n        range_start = tuple(map(int, matches[0]))\n        range_end = tuple(map(int, matches[-1]))\n        # Generate a matrix of tuples from (range_start[0], range_start[1]) to (range_end[0], range_end[1])\n        grid_pinpoints = [(i, j) for i in range(range_start[0], range_end[0] + 1) for j in range(range_start[1], range_end[1] + 1)]\n        # Multiply all elements by patch_size\n        grid_pinpoints = [[dim * patch_size for dim in pair] for pair in grid_pinpoints]\n\n    if type(grid_pinpoints) is list:\n        possible_resolutions = grid_pinpoints\n    else:\n        possible_resolutions = ast.literal_eval(grid_pinpoints)\n    best_resolution = select_best_resolution(image.size, possible_resolutions)\n    image_padded = resize_and_pad_image(image, best_resolution)\n\n    patches = divide_to_patches(image_padded, processor.crop_size[\"height\"])\n\n    # FIXME: this seems to be a bug that it resizes instead of pad.\n    # but to keep it consistent with previous, i will keep it as it is\n    # TODO: uncomment below to ablate with the padding\n    if isinstance(processor.size, dict):\n        shortest_edge = processor.size[\"shortest_edge\"]\n    else:\n        shortest_edge = min(processor.size)\n    image_original_resize = image.resize((shortest_edge, shortest_edge))\n    # image_padded_square = expand2square(image, tuple(int(x*255) for x in processor.image_mean))\n    # image_original_resize = image_padded_square.resize((processor.size['shortest_edge'], processor.size['shortest_edge']))\n\n    image_patches = [image_original_resize] + patches\n    image_patches = [processor.preprocess(image_patch, return_tensors=\"pt\")[\"pixel_values\"][0] for image_patch in image_patches]\n    return torch.stack(image_patches, dim=0)\n\n\ndef load_image_from_base64(image):\n    return Image.open(BytesIO(base64.b64decode(image)))\n\n\ndef expand2square(pil_img, background_color):\n    width, height = pil_img.size\n    if width == height:\n        return pil_img\n    elif width > height:\n        result = Image.new(pil_img.mode, (width, width), background_color)\n        result.paste(pil_img, (0, (width - height) // 2))\n        return result\n    else:\n        result = Image.new(pil_img.mode, (height, height), background_color)\n        result.paste(pil_img, ((height - width) // 2, 0))\n        return result\n\n\ndef process_images(images, image_processor, model_cfg):\n    image_aspect_ratio = getattr(model_cfg, \"image_aspect_ratio\", None)\n    new_images = []\n    if image_aspect_ratio == \"highres\":\n        for image in images:\n            image = process_highres_image(image, image_processor, model_cfg.image_grid_pinpoints)\n            new_images.append(image)\n    elif image_aspect_ratio == \"anyres\" or \"anyres_max\" in image_aspect_ratio:\n        for image in images:\n            image = process_anyres_image(image, image_processor, model_cfg.image_grid_pinpoints)\n            new_images.append(image)\n    elif image_aspect_ratio == \"crop_split\":\n        for image in images:\n            image = process_highres_image_crop_split(image, model_cfg, image_processor)\n            new_images.append(image)\n    elif image_aspect_ratio == \"pad\":\n        for image in images:\n            image = expand2square(image, tuple(int(x * 255) for x in image_processor.image_mean))\n            image = image_processor.preprocess(image, return_tensors=\"pt\")[\"pixel_values\"][0]\n            new_images.append(image)\n    else:\n        return image_processor.preprocess(images, return_tensors=\"pt\")[\"pixel_values\"]\n    if all(x.shape == new_images[0].shape for x in new_images):\n        new_images = torch.stack(new_images, dim=0)\n    return new_images\n\n\ndef tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None):\n    prompt_chunks = [tokenizer(chunk).input_ids for chunk in prompt.split(\"<image>\")]\n\n    def insert_separator(X, sep):\n        return [ele for sublist in zip(X, [sep] * len(X)) for ele in sublist][:-1]\n\n    input_ids = []\n    offset = 0\n    if len(prompt_chunks) > 0 and len(prompt_chunks[0]) > 0 and prompt_chunks[0][0] == tokenizer.bos_token_id:\n        offset = 1\n        input_ids.append(prompt_chunks[0][0])\n\n    for x in insert_separator(prompt_chunks, [image_token_index] * (offset + 1)):\n        input_ids.extend(x[offset:])\n\n    if return_tensors is not None:\n        if return_tensors == \"pt\":\n            return torch.tensor(input_ids, dtype=torch.long)\n        raise ValueError(f\"Unsupported tensor type: {return_tensors}\")\n    return input_ids\n\ndef get_model_name_from_path(model_path):\n    model_path = model_path.strip(\"/\")\n    model_paths = model_path.split(\"/\")\n    if model_paths[-1].startswith(\"checkpoint-\"):\n        return model_paths[-2] + \"_\" + model_paths[-1]\n    else:\n        return model_paths[-1]\n\n\nclass KeywordsStoppingCriteria(StoppingCriteria):\n    def __init__(self, keywords, tokenizer, input_ids):\n        self.keywords = keywords\n        self.keyword_ids = []\n        for keyword in keywords:\n            cur_keyword_ids = tokenizer(keyword).input_ids\n            if len(cur_keyword_ids) > 1 and cur_keyword_ids[0] == tokenizer.bos_token_id:\n                cur_keyword_ids = cur_keyword_ids[1:]\n            self.keyword_ids.append(torch.tensor(cur_keyword_ids))\n        self.tokenizer = tokenizer\n        self.start_len = input_ids.shape[1]\n\n    def __call__(self, output_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n        assert output_ids.shape[0] == 1, \"Only support batch size 1 (yet)\"  # TODO\n        offset = min(output_ids.shape[1] - self.start_len, 3)\n        self.keyword_ids = [keyword_id.to(output_ids.device) for keyword_id in self.keyword_ids]\n        for keyword_id in self.keyword_ids:\n            if output_ids[0, -keyword_id.shape[0] :] == keyword_id:\n                return True\n        outputs = self.tokenizer.batch_decode(output_ids[:, -offset:], skip_special_tokens=True)[0]\n        for keyword in self.keywords:\n            if keyword in outputs:\n                return True\n        return False\n"}
{"type": "source_file", "path": "llava/dataset/cc_dataset.py", "content": "import ast\nimport os\nimport copy\nfrom dataclasses import dataclass, field\nimport json\nimport logging\nimport pathlib\nfrom typing import Dict, Optional, Sequence, List\nfrom PIL import Image, ImageFile\nfrom packaging import version\nimport numpy as np\n\nimport time\nimport random\nimport yaml\nimport math\nimport re\nimport torch\n\nimport transformers\nimport tokenizers\nimport deepspeed\n\nfrom transformers import AutoConfig\nfrom torch.utils.data import Dataset\nfrom llava.constants import IGNORE_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IMAGE_TOKEN_INDEX\nfrom llava.train.llava_trainer import LLaVATrainer\n\nfrom llava import conversation as conversation_lib\nfrom llava.model import *\nfrom llava.mm_utils import process_highres_image, process_anyres_image, process_highres_image_crop_split, tokenizer_image_token\nfrom llava.utils import rank0_print, process_video_with_pyav, process_video_with_decord\n\ntorch.multiprocessing.set_sharing_strategy(\"file_system\")\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\nlocal_rank = None\n\nIS_TOKENIZER_GREATER_THAN_0_14 = version.parse(tokenizers.__version__) >= version.parse(\"0.14\")\n\n\ndef _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n    \"\"\"Tokenize a list of strings.\"\"\"\n    tokenized_list = [\n        tokenizer(\n            text,\n            return_tensors=\"pt\",\n            padding=\"longest\",\n            max_length=tokenizer.model_max_length,\n            truncation=True,\n        )\n        for text in strings\n    ]\n    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n    input_ids_lens = labels_lens = [tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list]\n    return dict(\n        input_ids=input_ids,\n        labels=labels,\n        input_ids_lens=input_ids_lens,\n        labels_lens=labels_lens,\n    )\n\n\ndef _mask_targets(target, tokenized_lens, speakers):\n    # cur_idx = 0\n    cur_idx = tokenized_lens[0]\n    tokenized_lens = tokenized_lens[1:]\n    target[:cur_idx] = IGNORE_INDEX\n    for tokenized_len, speaker in zip(tokenized_lens, speakers):\n        if speaker == \"human\":\n            target[cur_idx + 2 : cur_idx + tokenized_len] = IGNORE_INDEX\n        cur_idx += tokenized_len\n\n\ndef _add_speaker_and_signal(header, source, get_conversation=True):\n    \"\"\"Add speaker and start/end signal on each round.\"\"\"\n    BEGIN_SIGNAL = \"### \"\n    END_SIGNAL = \"\\n\"\n    conversation = header\n    for sentence in source:\n        from_str = sentence[\"from\"]\n        if from_str.lower() == \"human\":\n            from_str = conversation_lib.default_conversation.roles[0]\n        elif from_str.lower() == \"gpt\":\n            from_str = conversation_lib.default_conversation.roles[1]\n        else:\n            from_str = \"unknown\"\n        sentence[\"value\"] = BEGIN_SIGNAL + from_str + \": \" + sentence[\"value\"] + END_SIGNAL\n        if get_conversation:\n            conversation += sentence[\"value\"]\n    conversation += BEGIN_SIGNAL\n    return conversation\n\n\ndef preprocess_multimodal(sources: Sequence[str], data_args) -> Dict:\n    is_multimodal = data_args.is_multimodal\n    if not is_multimodal:\n        return sources\n\n    for source in sources:\n        for sentence in source:\n            # TODO maybe this should be changed for interleaved data?\n            # if DEFAULT_IMAGE_TOKEN in sentence[\"value\"] and not sentence[\"value\"].startswith(DEFAULT_IMAGE_TOKEN):\n            # only check for num_im=1\n            num_im = len(re.findall(DEFAULT_IMAGE_TOKEN, sentence[\"value\"]))\n            if num_im == 1 and DEFAULT_IMAGE_TOKEN in sentence[\"value\"] and not sentence[\"value\"].startswith(DEFAULT_IMAGE_TOKEN):\n                sentence[\"value\"] = sentence[\"value\"].replace(DEFAULT_IMAGE_TOKEN, \"\").strip()\n                sentence[\"value\"] = DEFAULT_IMAGE_TOKEN + \"\\n\" + sentence[\"value\"]\n                sentence[\"value\"] = sentence[\"value\"].strip()\n                if \"mmtag\" in conversation_lib.default_conversation.version:\n                    sentence[\"value\"] = sentence[\"value\"].replace(DEFAULT_IMAGE_TOKEN, \"<Image>\" + DEFAULT_IMAGE_TOKEN + \"</Image>\")\n            replace_token = DEFAULT_IMAGE_TOKEN\n            if data_args.mm_use_im_start_end:\n                replace_token = DEFAULT_IM_START_TOKEN + replace_token + DEFAULT_IM_END_TOKEN\n            sentence[\"value\"] = sentence[\"value\"].replace(DEFAULT_IMAGE_TOKEN, replace_token)\n\n            # For videoInstruct-100k noisy_data. TODO: Ask Yuanhan to clean the data instead of leaving the noise code here.\n            sentence[\"value\"] = sentence[\"value\"].replace(\"QA_GT_caption_based_noisy\", \"\")\n\n    return sources\n\n\ndef preprocess_llama_2(sources, tokenizer: transformers.PreTrainedTokenizer, has_image: bool = False) -> Dict:\n    conv = conversation_lib.default_conversation.copy()\n    roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n\n    # Apply prompt templates\n    conversations = []\n    for i, source in enumerate(sources):\n        if roles[source[0][\"from\"]] != conv.roles[0]:\n            # Skip the first one if it is not from human\n            source = source[1:]\n\n        conv.messages = []\n        for j, sentence in enumerate(source):\n            role = roles[sentence[\"from\"]]\n            assert role == conv.roles[j % 2], f\"{i}\"\n            conv.append_message(role, sentence[\"value\"])\n        conversations.append(conv.get_prompt())\n\n    # Tokenize conversations\n\n    if has_image:\n        input_ids = torch.stack([tokenizer_image_token(prompt, tokenizer, return_tensors=\"pt\") for prompt in conversations], dim=0)\n    else:\n        input_ids = tokenizer(\n            conversations,\n            return_tensors=\"pt\",\n            padding=\"longest\",\n            max_length=tokenizer.model_max_length,\n            truncation=True,\n        ).input_ids\n\n    targets = input_ids.clone()\n\n    assert conv.sep_style == conversation_lib.SeparatorStyle.LLAMA_2\n\n    # Mask targets\n    sep = \"[/INST] \"\n    for conversation, target in zip(conversations, targets):\n        total_len = int(target.ne(tokenizer.pad_token_id).sum())\n\n        rounds = conversation.split(conv.sep2)\n        cur_len = 1\n        target[:cur_len] = IGNORE_INDEX\n        for i, rou in enumerate(rounds):\n            if rou == \"\":\n                break\n\n            parts = rou.split(sep)\n            if len(parts) != 2:\n                break\n            parts[0] += sep\n\n            if has_image:\n                round_len = len(tokenizer_image_token(rou, tokenizer))\n                instruction_len = len(tokenizer_image_token(parts[0], tokenizer)) - 2\n            else:\n                round_len = len(tokenizer(rou).input_ids)\n                instruction_len = len(tokenizer(parts[0]).input_ids) - 2\n\n            target[cur_len : cur_len + instruction_len] = IGNORE_INDEX\n\n            cur_len += round_len\n        target[cur_len:] = IGNORE_INDEX\n\n        if cur_len < tokenizer.model_max_length:\n            if cur_len != total_len:\n                target[:] = IGNORE_INDEX\n                print(f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\" f\" (ignored)\")\n\n    return dict(\n        input_ids=input_ids,\n        labels=targets,\n    )\n\n\ndef preprocess_gemma(sources: List[List[Dict[str, str]]], tokenizer: transformers.PreTrainedTokenizer, has_image: bool = False) -> Dict:\n    conv: conversation_lib.Conversation = conversation_lib.default_conversation.copy()\n    roles: Dict[str, str] = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n\n    # Apply prompt templates\n    conversations: List[str] = []\n    for i, source in enumerate(sources):\n        if roles[source[0][\"from\"]] != conv.roles[0]:\n            # Skip the first one if it is not from human\n            source: List[Dict[str, str]] = source[1:]\n\n        conv.messages = []\n        for j, sentence in enumerate(source):\n            role: str = roles[sentence[\"from\"]]\n            assert role == conv.roles[j % 2], f\"{i}\"\n            conv.append_message(role, sentence[\"value\"])\n        conversations.append(conv.get_prompt())\n\n    # Tokenize conversations\n    if has_image:\n        input_ids: torch.Tensor = torch.stack([tokenizer_image_token(prompt, tokenizer, return_tensors=\"pt\") for prompt in conversations], dim=0)\n    else:\n        input_ids: torch.Tensor = tokenizer(\n            conversations,\n            return_tensors=\"pt\",\n            padding=\"longest\",\n            max_length=tokenizer.model_max_length,\n            truncation=True,\n        ).input_ids\n\n    targets: torch.Tensor = input_ids.clone()\n    assert conv.sep_style == conversation_lib.SeparatorStyle.GEMMA\n\n    # Mask target\n    sep: str = conv.sep + conv.roles[1]\n    for conversation, target in zip(conversations, targets):\n        total_len: int = int(target.ne(tokenizer.pad_token_id).sum())\n\n        rounds: List[str] = conversation.split(conv.sep)\n        re_rounds = []\n        for conv_idx in range(0, len(rounds), 2):\n            re_rounds.append(conv.sep.join(rounds[conv_idx : conv_idx + 2]))\n\n        cur_len = 1  # Ignore <bos>\n        target[:cur_len] = IGNORE_INDEX\n        for i, rou in enumerate(re_rounds):\n            if rou == \"\":\n                break\n\n            parts = rou.split(sep)\n            if len(parts) != 2:\n                break\n            parts[0] += sep  # Re-append sep because split on this\n            # Now \"\".join(parts)==rou\n\n            if has_image:\n                round_len = len(tokenizer_image_token(rou, tokenizer)) - 1  # Ignore <bos>\n                instruction_len = len(tokenizer_image_token(parts[0], tokenizer)) - 1  # Ignore <bos>\n            else:\n                round_len = len(tokenizer(rou).input_ids) - 1  # Ignore <bos>\n                instruction_len = len(tokenizer(parts[0]).input_ids) - 1  # Ignore <bos>\n\n            round_len += 2  # sep: <end_of_turn>\\n takes 2 tokens\n            target[cur_len : cur_len + instruction_len] = IGNORE_INDEX\n            cur_len += round_len\n\n        target[cur_len:] = IGNORE_INDEX\n\n        if cur_len < tokenizer.model_max_length:\n            if cur_len != total_len:\n                target[:] = IGNORE_INDEX\n                print(f\"warning: tokenization mismatch: {cur_len} vs. {total_len}.\" f\" (ignored)\")\n\n    return dict(\n        input_ids=input_ids,\n        labels=targets,\n    )\n\n\ndef preprocess_qwen(sources, tokenizer: transformers.PreTrainedTokenizer, has_image: bool = False, has_change_feature=False, max_len=2048, system_message: str = \"You are a helpful assistant.\") -> Dict:\n    # roles = {\"human\": \"<|im_start|>user\", \"gpt\": \"<|im_start|>assistant\"}\n    roles = {\"human\": \"user\", \"gpt\": \"assistant\"}\n\n    # Add image tokens to tokenizer as a special tokens\n    # Use a deepcopy of tokenizer so that we don't modify on the tokenizer\n    tokenizer = copy.deepcopy(tokenizer)\n    # When there is actually an image, we add the image tokens as a special token\n    if has_image:\n        tokenizer.add_tokens([\"<image>\"], special_tokens=True)\n    # if has_change_feature:\n    #     tokenizer.add_tokens([\"<change_feature>\"], special_tokens=True)\n    \n    image_token_index = tokenizer.convert_tokens_to_ids(\"<image>\")\n    # change_feature_token_index = tokenizer.convert_tokens_to_ids(\"<change_feature>\")\n    im_start = tokenizer.encode(\"<|im_start|>\")[0]\n    im_end = tokenizer.encode(\"<|im_end|>\")[0]\n    # im_start, im_end = tokenizer.additional_special_tokens_ids\n    # unmask_tokens = [\"<|im_start|>\", \"<|im_start|>\", \"\\n\"]\n    unmask_tokens_idx =  [198, im_start, im_end]\n    nl_tokens = tokenizer(\"\\n\").input_ids\n    \n    # Reset Qwen chat templates so that it won't include system message every time we apply\n    chat_template = \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n    tokenizer.chat_template = chat_template\n\n    # _system = tokenizer(\"system\").input_ids + nl_tokens\n    # _user = tokenizer(\"user\").input_ids + nl_tokens\n    # _assistant = tokenizer(\"assistant\").input_ids + nl_tokens\n\n    # Apply prompt templates\n    input_ids, targets = [], []\n    for i, source in enumerate(sources):\n        if roles[source[0][\"from\"]] != roles[\"human\"]:\n            source = source[1:]\n\n        input_id, target = [], []\n\n        # New version, use apply chat template\n        # Build system message for each sentence\n        input_id += tokenizer.apply_chat_template([{\"role\" : \"system\", \"content\" : system_message}])\n        target += [IGNORE_INDEX] * len(input_id)\n\n        for conv in source:\n            # Make sure llava data can load\n            try:\n                role = conv[\"role\"]\n                content = conv[\"content\"]\n            except:\n                role = conv[\"from\"]\n                content = conv[\"value\"]\n\n            role =  roles.get(role, role)\n            \n            conv = [{\"role\" : role, \"content\" : content}]\n            encode_id = tokenizer.apply_chat_template(conv)\n            input_id += encode_id\n            if role in [\"user\", \"system\"]:\n                target += [IGNORE_INDEX] * len(encode_id)\n            else:\n                target += encode_id\n        \n\n                    \n        assert len(input_id) == len(target), f\"{len(input_id)} != {len(target)}\"\n        for idx, encode_id in enumerate(input_id):\n            if encode_id in unmask_tokens_idx:\n                target[idx] = encode_id\n            if encode_id == image_token_index:\n                input_id[idx] = IMAGE_TOKEN_INDEX\n            # if encode_id == change_feature_token_index:\n            #     input_id[idx] = CHANGE_FEATURE_TOKEN_INDEX\n        input_ids.append(input_id)\n        targets.append(target)\n    input_ids = torch.tensor(input_ids, dtype=torch.long)\n    targets = torch.tensor(targets, dtype=torch.long)\n    return dict(\n        input_ids=input_ids,  # tensor(bs x seq_len)\n        labels=targets,  # tensor(bs x seq_len)\n    )\n\n\ndef preprocess_llama3(\n    sources,\n    tokenizer: transformers.PreTrainedTokenizer,\n    has_image: bool = False,\n    max_len=2048,\n    system_message: str = \"You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\",\n) -> Dict:\n    # roles = {\"human\": \"<|start_header_id|>user<|end_header_id|>\", \"gpt\": \"<|start_header_id|>assistant<|end_header_id|>\"}\n    roles = {\"human\": \"user\", \"gpt\": \"assistant\"}\n\n    # Add image tokens to tokenizer as a special tokens\n    # Use a deepcopy of tokenizer so that we don't modify on the tokenizer\n    tokenizer = copy.deepcopy(tokenizer)\n    # When there is actually an image, we add the image tokens as a special token\n    if has_image:\n        tokenizer.add_tokens([\"<image>\"], special_tokens=True)\n    image_token_index = tokenizer.convert_tokens_to_ids(\"<image>\")\n    bos_token_id = tokenizer.convert_tokens_to_ids(\"<|begin_of_text|>\")\n    start_header_id = tokenizer.convert_tokens_to_ids(\"<|start_header_id|>\")\n    end_header_id = tokenizer.convert_tokens_to_ids(\"<|end_header_id|>\")\n    eot_id = tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n\n    unmask_tokens = [\"<|begin_of_text|>\", \"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\", \"\\n\\n\"]\n    unmask_tokens_idx = [tokenizer.convert_tokens_to_ids(tok) for tok in unmask_tokens]\n\n    # After update, calling tokenizer of llama3 will\n    # auto add bos id for the tokens. ヽ(｀⌒´)ﾉ\n    def safe_tokenizer_llama3(text):\n        input_ids = tokenizer(text).input_ids\n        if input_ids[0] == bos_token_id:\n            input_ids = input_ids[1:]\n        return input_ids\n\n    nl_tokens = tokenizer.convert_tokens_to_ids(\"\\n\\n\")\n    # Apply prompt templates\n    input_ids, targets = [], []\n    for i, source in enumerate(sources):\n        if roles[source[0][\"from\"]] != roles[\"human\"]:\n            source = source[1:]\n\n        input_id, target = [], []\n\n        # New version, use apply chat template\n        # Build system message for each sentence\n        input_id += tokenizer.apply_chat_template([{\"role\" : \"system\", \"content\" : system_message}])\n        target += [IGNORE_INDEX] * len(input_id)\n\n        for conv in source:\n            # Make sure llava data can load\n            try:\n                role = conv[\"role\"]\n                content = conv[\"content\"]\n            except:\n                role = conv[\"from\"]\n                content = conv[\"value\"]\n\n            role =  roles.get(role, role)\n            \n            conv = [{\"role\" : role, \"content\" : content}]\n            # First is bos token we don't need here\n            encode_id = tokenizer.apply_chat_template(conv)[1:]\n            input_id += encode_id\n            if role in [\"user\", \"system\"]:\n                target += [IGNORE_INDEX] * len(encode_id)\n            else:\n                target += encode_id\n        \n\n                    \n        assert len(input_id) == len(target), f\"{len(input_id)} != {len(target)}\"\n        for idx, encode_id in enumerate(input_id):\n            if encode_id in unmask_tokens_idx:\n                target[idx] = encode_id\n            if encode_id == image_token_index:\n                input_id[idx] = IMAGE_TOKEN_INDEX\n        input_ids.append(input_id)\n        targets.append(target)\n    input_ids = torch.tensor(input_ids, dtype=torch.long)\n    targets = torch.tensor(targets, dtype=torch.long)\n\n    return dict(\n        input_ids=input_ids,  # tensor(bs x seq_len)\n        labels=targets,  # tensor(bs x seq_len)\n    )\n\n\ndef preprocess_v1(sources, tokenizer: transformers.PreTrainedTokenizer, has_image: bool = False) -> Dict:\n    conv = conversation_lib.default_conversation.copy()\n    roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n\n    # Apply prompt templates\n    conversations = []\n    for i, source in enumerate(sources):\n        if roles[source[0][\"from\"]] != conv.roles[0]:\n            # Skip the first one if it is not from human\n            source = source[1:]\n\n        conv.messages = []\n        for j, sentence in enumerate(source):\n            role = roles[sentence[\"from\"]]\n            assert role == conv.roles[j % 2], f\"{i}\"\n            conv.append_message(role, sentence[\"value\"])\n        conversations.append(conv.get_prompt())\n\n    # Tokenize conversations\n\n    if has_image:\n        input_ids = torch.stack([tokenizer_image_token(prompt, tokenizer, return_tensors=\"pt\") for prompt in conversations], dim=0)\n    else:\n        input_ids = tokenizer(\n            conversations,\n            return_tensors=\"pt\",\n            padding=\"longest\",\n            max_length=tokenizer.model_max_length,\n            truncation=True,\n        ).input_ids\n\n    targets = input_ids.clone()\n\n    assert conv.sep_style == conversation_lib.SeparatorStyle.TWO\n\n    # Mask targets\n    sep = conv.sep + conv.roles[1] + \": \"\n    for conversation, target in zip(conversations, targets):\n        total_len = int(target.ne(tokenizer.pad_token_id).sum())\n\n        rounds = conversation.split(conv.sep2)\n        cur_len = 1\n        target[:cur_len] = IGNORE_INDEX\n        for i, rou in enumerate(rounds):\n            if rou == \"\":\n                break\n\n            parts = rou.split(sep)\n            if len(parts) != 2:\n                break\n            parts[0] += sep\n\n            if has_image:\n                round_len = len(tokenizer_image_token(rou, tokenizer))\n                instruction_len = len(tokenizer_image_token(parts[0], tokenizer)) - 2\n            else:\n                round_len = len(tokenizer(rou).input_ids)\n                instruction_len = len(tokenizer(parts[0]).input_ids) - 2\n\n            if i != 0 and not tokenizer.legacy and IS_TOKENIZER_GREATER_THAN_0_14:\n                round_len -= 1\n                instruction_len -= 1\n\n            target[cur_len : cur_len + instruction_len] = IGNORE_INDEX\n\n            cur_len += round_len\n        target[cur_len:] = IGNORE_INDEX\n\n        if cur_len < tokenizer.model_max_length:\n            if cur_len != total_len:\n                target[:] = IGNORE_INDEX\n                print(f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\" f\" (ignored)\")\n\n    return dict(\n        input_ids=input_ids,\n        labels=targets,\n    )\n\n\ndef preprocess_mpt(sources, tokenizer: transformers.PreTrainedTokenizer, has_image: bool = False) -> Dict:\n    conv = conversation_lib.default_conversation.copy()\n    roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n\n    # Apply prompt templates\n    conversations = []\n    for i, source in enumerate(sources):\n        if roles[source[0][\"from\"]] != conv.roles[0]:\n            # Skip the first one if it is not from human\n            source = source[1:]\n\n        conv.messages = []\n        for j, sentence in enumerate(source):\n            role = roles[sentence[\"from\"]]\n            assert role == conv.roles[j % 2], f\"{i}\"\n            conv.append_message(role, sentence[\"value\"])\n        conversations.append(conv.get_prompt())\n\n    # Tokenize conversations\n\n    if has_image:\n        input_ids = torch.stack([tokenizer_image_token(prompt, tokenizer, return_tensors=\"pt\") for prompt in conversations], dim=0)\n    else:\n        input_ids = tokenizer(\n            conversations,\n            return_tensors=\"pt\",\n            padding=\"longest\",\n            max_length=tokenizer.model_max_length,\n            truncation=True,\n        ).input_ids\n\n    targets = input_ids.clone()\n    assert conv.sep_style == conversation_lib.SeparatorStyle.MPT\n\n    # Mask targets\n    sep = conv.sep + conv.roles[1]\n    for conversation, target in zip(conversations, targets):\n        total_len = int(target.ne(tokenizer.pad_token_id).sum())\n\n        rounds = conversation.split(conv.sep)\n        re_rounds = [conv.sep.join(rounds[:3])]  # system + user + gpt\n        for conv_idx in range(3, len(rounds), 2):\n            re_rounds.append(conv.sep.join(rounds[conv_idx : conv_idx + 2]))  # user + gpt\n        cur_len = 1\n        target[:cur_len] = IGNORE_INDEX\n        for i, rou in enumerate(re_rounds):\n            if rou == \"\":\n                break\n\n            parts = rou.split(sep)\n            if len(parts) != 2:\n                break\n            parts[0] += sep\n\n            if has_image:\n                round_len = len(tokenizer_image_token(rou, tokenizer))\n                instruction_len = len(tokenizer_image_token(parts[0], tokenizer)) - 1\n            else:\n                round_len = len(tokenizer(rou).input_ids)\n                instruction_len = len(tokenizer(parts[0]).input_ids) - 1\n\n            if i != 0 and getattr(tokenizer, \"legacy\", False) and IS_TOKENIZER_GREATER_THAN_0_14:\n                round_len += 1\n                instruction_len += 1\n\n            target[cur_len : cur_len + instruction_len] = IGNORE_INDEX\n\n            cur_len += round_len\n        target[cur_len:] = IGNORE_INDEX\n\n        if cur_len < tokenizer.model_max_length:\n            if cur_len != total_len:\n                target[:] = IGNORE_INDEX\n                print(f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\" f\"(#turns={len(re_rounds)} ignored)\")\n\n    return dict(\n        input_ids=input_ids,\n        labels=targets,\n    )\n\n\ndef preprocess_plain(\n    sources: Sequence[str],\n    tokenizer: transformers.PreTrainedTokenizer,\n) -> Dict:\n    # add end signal and concatenate together\n    conversations = []\n    for source in sources:\n        assert len(source) == 2\n        assert DEFAULT_IMAGE_TOKEN in source[0][\"value\"]\n        source[0][\"value\"] = DEFAULT_IMAGE_TOKEN\n        conversation = source[0][\"value\"] + source[1][\"value\"] + conversation_lib.default_conversation.sep\n        conversations.append(conversation)\n    # tokenize conversations\n    input_ids = [tokenizer_image_token(prompt, tokenizer, return_tensors=\"pt\") for prompt in conversations]\n    targets = copy.deepcopy(input_ids)\n    for target, source in zip(targets, sources):\n        tokenized_len = len(tokenizer_image_token(source[0][\"value\"], tokenizer))\n        target[:tokenized_len] = IGNORE_INDEX\n\n    return dict(input_ids=input_ids, labels=targets)\n\n\ndef preprocess(sources: Sequence[str], tokenizer: transformers.PreTrainedTokenizer, has_image: bool = False) -> Dict:\n    \"\"\"\n    Given a list of sources, each is a conversation list. This transform:\n    1. Add signal '### ' at the beginning each sentence, with end signal '\\n';\n    2. Concatenate conversations together;\n    3. Tokenize the concatenated conversation;\n    4. Make a deepcopy as the target. Mask human words with IGNORE_INDEX.\n    \"\"\"\n    if conversation_lib.default_conversation.sep_style == conversation_lib.SeparatorStyle.PLAIN:\n        return preprocess_plain(sources, tokenizer)\n    if conversation_lib.default_conversation.sep_style == conversation_lib.SeparatorStyle.LLAMA_2:\n        return preprocess_llama_2(sources, tokenizer, has_image=has_image)\n    if conversation_lib.default_conversation.version.startswith(\"v1\"):\n        return preprocess_v1(sources, tokenizer, has_image=has_image)\n    if conversation_lib.default_conversation.version == \"mpt\":\n        return preprocess_mpt(sources, tokenizer, has_image=has_image)\n    if conversation_lib.default_conversation.version == \"qwen\":\n        return preprocess_qwen(sources, tokenizer, has_image=has_image)\n    if conversation_lib.default_conversation.version == \"gemma\":\n        return preprocess_gemma(sources, tokenizer, has_image=has_image)\n    if conversation_lib.default_conversation.version == \"llama_v3\":\n        return preprocess_llama3(sources, tokenizer, has_image=has_image)\n    # add end signal and concatenate together\n    conversations = []\n    for source in sources:\n        header = f\"{conversation_lib.default_conversation.system}\\n\\n\"\n        conversation = _add_speaker_and_signal(header, source)\n        conversations.append(conversation)\n\n    # tokenize conversations\n    def get_tokenize_len(prompts):\n        return [len(tokenizer_image_token(prompt, tokenizer)) for prompt in prompts]\n\n    if has_image:\n        input_ids = [tokenizer_image_token(prompt, tokenizer, return_tensors=\"pt\") for prompt in conversations]\n    else:\n        conversations_tokenized = _tokenize_fn(conversations, tokenizer)\n        input_ids = conversations_tokenized[\"input_ids\"]\n\n    targets = copy.deepcopy(input_ids)\n    for target, source in zip(targets, sources):\n        if has_image:\n            tokenized_lens = get_tokenize_len([header] + [s[\"value\"] for s in source])\n        else:\n            tokenized_lens = _tokenize_fn([header] + [s[\"value\"] for s in source], tokenizer)[\"input_ids_lens\"]\n        speakers = [sentence[\"from\"] for sentence in source]\n        _mask_targets(target, tokenized_lens, speakers)\n\n    return dict(input_ids=input_ids, labels=targets)\n\n\nclass LazySupervisedDataset_CC(Dataset):\n    def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer, data_args, is_eval=False):\n        super(LazySupervisedDataset_CC, self).__init__()\n        self.tokenizer = tokenizer\n        self.data_args = data_args\n        self.data_args.image_folder = None\n\n        self.list_data_dict = []\n        # Handle multiple JSON/JSONL files specified in the data_path\n        self.load_data(data_path)\n        self.loop_format = getattr(self.data_args, \"loop_format\", \"random_choice\")\n        if is_eval: self.loop_format = \"random_choice\"\n        if self.loop_format == \"flatten_list\":\n            new_data_list = []\n            for data_dict in self.list_data_dict:\n                for cc in data_dict['change_caption']:\n                    new_data_list.append({\n                        \"uid\": data_dict['uid'],\n                        \"change_flag\": data_dict.get('change_flag', -1),\n                        \"img1_path\": data_dict['img1_path'],\n                        \"img2_path\": data_dict['img2_path'],\n                        \"change_caption\": [cc],\n                    })\n            self.list_data_dict = new_data_list\n        if not is_eval:\n            random.shuffle(self.list_data_dict)\n\n        rank0_print(f\"Loaded {len(self.list_data_dict)} samples from {data_path}\")\n        rank0_print(\"Formatting inputs...Skip in lazy mode\")\n\n        self.question = \"This is the Image1 <image>. This is the second Image2 <image>.\\nWhat difference happened from Image1 to Image2?\"\n        \n    def __len__(self):\n        return len(self.list_data_dict)\n\n    def load_data(self, data_path):\n        if \"[\" in data_path and \"]\" in data_path:\n            paths = data_path.strip(\"[\").strip(\"]\").split(\",\")\n            for path in paths:\n                self.load_single_file(path.strip())\n        elif \"{\" in data_path and \"}\" in data_path:\n            base_path, file_pattern = re.match(r\"^(.*)\\{(.*)\\}\\.(jsonl|json)$\", data_path).groups()\n            file_names = file_pattern.split(\",\")\n            print(f\"Loading {file_names} from {base_path}\")\n            self.data_args.dataset_paths = []\n            for file_name in file_names:\n                full_path = f\"{base_path}{file_name}.json\"\n                if os.path.exists(full_path):\n                    self.data_args.dataset_paths.append(full_path)\n                    print(f\"Loading {full_path}\")\n                    self.load_single_file(full_path)\n                else:\n                    print(f\"path {full_path} not exists, skipped!!!\")\n        elif data_path.endswith(\".yaml\"):\n            self.load_yaml_file(data_path)\n        else:\n            # Fallback to original pattern matching logic\n            self.load_single_file(data_path)\n\n    def load_single_file(self, file_path):\n        if isinstance(file_path, list):\n            for path in file_path:\n                self.load_single_file(path)\n            return\n\n        print(f\"Loading {file_path}\")\n        if file_path.endswith(\".jsonl\"):\n            slice_data_dict = []\n            with open(file_path, \"r\") as file:\n                for line in file:\n                    try:\n                        slice_data_dict.append(json.loads(line.strip()))\n                    except Exception as e:\n                        print(f\"Error during loading {file_path}: {e}\")\n            print(f\"Loaded {len(slice_data_dict)} samples from {file_path}\")\n\n        elif file_path.endswith(\".json\"):\n            with open(file_path, \"r\") as file:\n                slice_data_dict = json.load(file)\n            print(f\"Loaded {len(slice_data_dict)} samples from {file_path}\")\n\n        else:\n            raise ValueError(f\"Unsupported file type: {file_path}\")\n\n        self.list_data_dict.extend(slice_data_dict)\n\n    def load_yaml_file(self, yaml_path):\n        with open(yaml_path, \"r\") as file:\n            yaml_data = yaml.safe_load(file)\n            datasets = yaml_data.get(\"datasets\")\n            self.data_args.dataset_paths = [dataset.get(\"json_path\") for dataset in datasets]\n            for dataset in datasets:\n                json_path = dataset.get(\"json_path\")\n                sampling_strategy = dataset.get(\"sampling_strategy\", \"all\")\n                self.apply_sampling_strategy(json_path, sampling_strategy)\n\n    def apply_sampling_strategy(self, json_path, sampling_strategy):\n        print(f\"Loading {json_path} with {sampling_strategy} sampling strategy\")\n        if json_path.endswith(\".jsonl\"):\n            cur_data_dict = []\n            with open(json_path, \"r\") as json_file:\n                for line in json_file:\n                    cur_data_dict.append(json.loads(line.strip()))\n        elif json_path.endswith(\".json\"):\n            with open(json_path, \"r\") as json_file:\n                cur_data_dict = json.load(json_file)\n        else:\n            raise ValueError(f\"Unsupported file type: {json_path}\")\n\n        if \":\" in sampling_strategy:\n            sampling_strategy, sampling_number = sampling_strategy.split(\":\")\n            if \"%\" in sampling_number:\n                sampling_number = math.ceil(int(sampling_number.split(\"%\")[0]) * len(cur_data_dict) / 100)\n            else:\n                sampling_number = int(sampling_number)\n\n            if sampling_strategy == \"first\":\n                cur_data_dict = cur_data_dict[:sampling_number]\n            elif sampling_strategy == \"end\":\n                cur_data_dict = cur_data_dict[-sampling_number:]\n            elif sampling_strategy == \"random\":\n                random.shuffle(cur_data_dict)\n                cur_data_dict = cur_data_dict[:sampling_number]\n\n        print(f\"Loaded {len(cur_data_dict)} samples from {json_path}\")\n        self.list_data_dict.extend(cur_data_dict)\n    \n    @property\n    def lengths(self):\n        length_list = []\n        for sample in self.list_data_dict:\n            img_tokens = 729\n            tmp_length = len(self.question) + img_tokens * 2 + len(random.choice(sample['change_caption']))\n            length_list.append(tmp_length)\n        return length_list\n\n    @property\n    def modality_lengths(self):\n        length_list = []\n        for sample in self.list_data_dict:\n            tmp_length = len(self.question) + len(random.choice(sample['change_caption']))\n            length_list.append(tmp_length)\n        return length_list\n\n    def process_image(self, image_file, overwrite_image_aspect_ratio=None):\n        image_folder = None\n        processor = self.data_args.image_processor\n        # print(f\"\\n\\nInspecting the image path, folder = {image_folder}, image={image_file}\\n\\n\")\n        try:\n            if image_folder is not None:\n                image = Image.open(os.path.join(image_folder, image_file)).convert(\"RGB\")\n            else:\n                image = Image.open(image_file).convert(\"RGB\")\n        except Exception as exn:\n            print(f\"Failed to open image {image_file}. Exception:\", exn)\n            raise exn\n\n        image_size = image.size\n        image_aspect_ratio = self.data_args.image_aspect_ratio\n        if overwrite_image_aspect_ratio is not None:\n            image_aspect_ratio = overwrite_image_aspect_ratio\n        if image_aspect_ratio == \"highres\":\n            image = process_highres_image(image, self.data_args.image_processor, self.data_args.image_grid_pinpoints)\n        elif image_aspect_ratio == \"anyres\" or \"anyres_max\" in image_aspect_ratio:\n            image = process_anyres_image(image, self.data_args.image_processor, self.data_args.image_grid_pinpoints)\n        elif image_aspect_ratio == \"crop_split\":\n            image = process_highres_image_crop_split(image, self.data_args)\n        elif image_aspect_ratio == \"pad\":\n            def expand2square(pil_img, background_color):\n                width, height = pil_img.size\n                if width == height:\n                    return pil_img\n                elif width > height:\n                    result = Image.new(pil_img.mode, (width, width), background_color)\n                    result.paste(pil_img, (0, (width - height) // 2))\n                    return result\n                else:\n                    result = Image.new(pil_img.mode, (height, height), background_color)\n                    result.paste(pil_img, ((height - width) // 2, 0))\n                    return result\n\n            image = expand2square(image, tuple(int(x * 255) for x in processor.image_mean))\n            image = processor.preprocess(image, return_tensors=\"pt\")[\"pixel_values\"][0]\n        else:\n            image = processor.preprocess(image, return_tensors=\"pt\")[\"pixel_values\"][0]\n        return image, image_size, \"image\"\n\n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n        # TODO: define number of retries somewhere else\n        num_base_retries = 3\n        num_final_retries = 300\n\n        # try the current sample first\n        for attempt_idx in range(num_base_retries):\n            try:\n                sample = self._get_item(i)\n                return sample\n            except Exception as e:\n                # sleep 1s in case it is a cloud disk issue\n                print(f\"[Try #{attempt_idx}] Failed to fetch sample {i}. Exception:\", e)\n                time.sleep(1)\n\n        # try other samples, in case it is file corruption issue\n        for attempt_idx in range(num_base_retries):\n            try:\n                next_index = min(i + 1, len(self.list_data_dict) - 1)\n                # sample_idx = random.choice(range(len(self)))\n                sample = self._get_item(next_index)\n                return sample\n            except Exception as e:\n                # no need to sleep\n                print(f\"[Try other #{attempt_idx}] Failed to fetch sample {next_index}. Exception:\", e)\n                pass\n\n        try:\n            sample = self._get_item(i)\n            return sample\n        except Exception as e:\n            raise e\n\n    def _get_item(self, i) -> Dict[str, torch.Tensor]:\n        sources = self.list_data_dict[i]\n        if isinstance(i, int):\n            sources = [sources]\n        assert len(sources) == 1, \"Don't know why it is wrapped to a list\"  # FIXME\n        conversation = [\n            {\n                \"from\": \"human\",\n                \"value\": self.question\n            },\n            {\n                \"from\": \"gpt\",\n                \"value\": random.choice(sources[0]['change_caption'])\n            }\n        ]\n\n        img1, img2 = self.list_data_dict[i]['img1_path'], self.list_data_dict[i]['img2_path']\n        image_file = [img1, img2]\n        image = [self.process_image(f) for f in image_file]\n        sources = preprocess_multimodal(copy.deepcopy([conversation]), self.data_args)\n        \n        has_image = True\n        data_dict = preprocess(sources, self.tokenizer, has_image=has_image)\n\n        if isinstance(i, int):\n            data_dict = dict(input_ids=data_dict[\"input_ids\"][0], labels=data_dict[\"labels\"][0])\n        data_dict[\"image\"] = image\n        data_dict[\"uid\"] = self.list_data_dict[i].get(\"uid\", i)\n        extra_info = dict(change_flag=self.list_data_dict[i]['change_flag'], change_caption=self.list_data_dict[i]['change_caption'], conversation=conversation, img1_path=img1, img2_path=img2)\n        data_dict['extra_info'] = extra_info\n\n        return data_dict\n\n\n@dataclass\nclass DataCollatorForSupervisedDataset(object):\n    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n\n    tokenizer: transformers.PreTrainedTokenizer\n\n    def pad_sequence(self, input_ids, batch_first, padding_value):\n        if self.tokenizer.padding_side == \"left\":\n            input_ids = [torch.flip(_input_ids, [0]) for _input_ids in input_ids]\n        input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=batch_first, padding_value=padding_value)\n        if self.tokenizer.padding_side == \"left\":\n            input_ids = torch.flip(input_ids, [1])\n        return input_ids\n\n    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n        # input_ids, labels, ids = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\", \"id\"))\n        input_ids = [_input_ids[: self.tokenizer.model_max_length] for _input_ids in input_ids]\n        labels = [_labels[: self.tokenizer.model_max_length] for _labels in labels]\n        if self.tokenizer.pad_token_id is None:\n            # self.tokenizer.pad_token_id = self.tokenizer.eos_token_id  # FIXME: this could only be triggered for llama3 model.\n            self.tokenizer.pad_token_id = 0 # This gets the best result. Don't know why.\n        input_ids = self.pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n        labels = self.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n        batch = dict(input_ids=input_ids, labels=labels.long() if labels.dtype == torch.int32 else labels, attention_mask=input_ids.ne(self.tokenizer.pad_token_id))\n        # batch = dict(input_ids=input_ids, labels=labels, attention_mask=input_ids.ne(self.tokenizer.pad_token_id), ids=ids)\n\n        if \"image\" in instances[0]:\n            images = [instance[\"image\"] for instance in instances]\n\n            batch[\"image_sizes\"] = [im[1] for im_list in images for im in im_list]\n            batch[\"modalities\"] = [im[2] for im_list in images for im in im_list]\n            images = [im[0] for im_list in images for im in im_list]\n\n            # if all(x is not None and x.shape == images[0].shape for x in images):\n                # Image: (N, P, C, H, W)\n                # Video: (N, F, C, H, W)\n            #     batch[\"images\"] = torch.stack(images)\n            # else:\n            batch[\"images\"] = images\n\n        if \"prompt\" in instances[0]:\n            batch[\"prompts\"] = [instance[\"prompt\"] for instance in instances]\n\n        try:\n            if \"uid\" in instances[0]:\n                batch[\"uid\"] = [instance[\"uid\"] for instance in instances]\n            if \"extra_info\" in instances[0]:\n                batch[\"extra_info\"] = [instance[\"extra_info\"] for instance in instances]\n            if \"gt_label\" in instances[0]:\n                batch[\"gt_label\"] = [instance[\"gt_label\"] for instance in instances]\n        except:\n            pass\n\n        return batch\n\n\ndef make_supervised_data_module_cc(tokenizer: transformers.PreTrainedTokenizer, data_args) -> Dict:\n    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n    train_dataset = LazySupervisedDataset_CC(tokenizer=tokenizer, data_path=data_args.data_path, data_args=data_args)\n    eval_dataset = LazySupervisedDataset_CC(tokenizer=tokenizer, data_path=data_args.eval_data_path, data_args=data_args, is_eval=True)\n    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n    return dict(train_dataset=train_dataset, eval_dataset=eval_dataset, data_collator=data_collator)"}
{"type": "source_file", "path": "llava/model/llava_arch.py", "content": "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom abc import ABC, abstractmethod\n\nimport math\nimport re\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .multimodal_encoder.builder import build_vision_tower\nfrom .multimodal_resampler.builder import build_vision_resampler\nfrom .multimodal_projector.builder import build_vision_projector\n\nfrom llava.constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n\nfrom llava.mm_utils import get_anyres_image_grid_shape\nfrom llava.utils import rank0_print, rank_print\nimport random\n\n\nclass LlavaMetaModel:\n\n    def __init__(self, config):\n        super(LlavaMetaModel, self).__init__(config)\n\n        if hasattr(config, \"mm_vision_tower\"):\n            delay_load = getattr(config, \"delay_load\", False)\n            self.vision_tower = build_vision_tower(config, delay_load=delay_load)\n            self.vision_resampler = build_vision_resampler(config, vision_tower=self.vision_tower)\n            self.mm_projector = build_vision_projector(config, vision_cfg=self.vision_tower.config)\n\n            if \"unpad\" in getattr(config, \"mm_patch_merge_type\", \"\"):\n                self.image_newline = nn.Parameter(torch.empty(config.hidden_size, dtype=self.dtype))\n\n    def get_vision_tower(self):\n        vision_tower = getattr(self, \"vision_tower\", None)\n        if type(vision_tower) is list:\n            vision_tower = vision_tower[0]\n        return vision_tower\n\n    def initialize_vision_modules(self, model_args, fsdp=None):\n        vision_tower = model_args.vision_tower\n        mm_vision_select_layer = model_args.mm_vision_select_layer\n        mm_vision_select_feature = model_args.mm_vision_select_feature\n        pretrain_mm_mlp_adapter = model_args.pretrain_mm_mlp_adapter\n        mm_patch_merge_type = model_args.mm_patch_merge_type\n\n        self.config.mm_vision_tower = vision_tower\n        self.config.vision_tower_pretrained = getattr(model_args, \"vision_tower_pretrained\", \"\")\n\n        if self.get_vision_tower() is None:\n            vision_tower = build_vision_tower(model_args)\n            vision_resampler = build_vision_resampler(model_args, vision_tower=vision_tower)\n            for k, v in vision_resampler.config.items():\n                setattr(self.config, k, v)\n\n            if fsdp is not None and len(fsdp) > 0:\n                self.vision_tower = [vision_tower]\n                self.vision_resampler = [vision_resampler]\n            else:\n                self.vision_tower = vision_tower\n                self.vision_resampler = vision_resampler\n        else:\n            if fsdp is not None and len(fsdp) > 0:\n                vision_resampler = self.vision_resampler[0]\n                vision_tower = self.vision_tower[0]\n            else:\n                vision_resampler = self.vision_resampler\n                vision_tower = self.vision_tower\n            vision_tower.load_model()\n\n            # In case it is frozen by LoRA\n            for p in self.vision_resampler.parameters():\n                p.requires_grad = True\n\n        self.config.use_mm_proj = True\n        self.config.mm_projector_type = getattr(model_args, \"mm_projector_type\", \"linear\")\n        self.config.mm_hidden_size = getattr(vision_resampler, \"hidden_size\", vision_tower.hidden_size)\n        self.config.mm_vision_select_layer = mm_vision_select_layer\n        self.config.mm_vision_select_feature = mm_vision_select_feature\n        self.config.mm_patch_merge_type = mm_patch_merge_type\n\n        if getattr(self, \"mm_projector\", None) is None:\n            self.mm_projector = build_vision_projector(self.config, vision_cfg=vision_tower.config)\n\n            if \"unpad\" in mm_patch_merge_type:\n                embed_std = 1 / torch.sqrt(torch.tensor(self.config.hidden_size, dtype=self.dtype))\n                self.image_newline = nn.Parameter(torch.randn(self.config.hidden_size, dtype=self.dtype) * embed_std)\n        else:\n            # In case it is frozen by LoRA\n            for p in self.mm_projector.parameters():\n                p.requires_grad = True\n\n        if pretrain_mm_mlp_adapter is not None:\n            mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location=\"cpu\")\n\n            def get_w(weights, keyword):\n                return {k.split(keyword + \".\")[1]: v for k, v in weights.items() if keyword in k}\n\n            incompatible_keys = self.mm_projector.load_state_dict(get_w(mm_projector_weights, \"mm_projector\"))\n            rank0_print(f\"Loaded mm projector weights from {pretrain_mm_mlp_adapter}. Incompatible keys: {incompatible_keys}\")\n            incompatible_keys = self.vision_resampler.load_state_dict(get_w(mm_projector_weights, \"vision_resampler\"), strict=False)\n            rank0_print(f\"Loaded vision resampler weights from {pretrain_mm_mlp_adapter}. Incompatible keys: {incompatible_keys}\")\n\n\ndef unpad_image(tensor, original_size):\n    \"\"\"\n    Unpads a PyTorch tensor of a padded and resized image.\n\n    Args:\n    tensor (torch.Tensor): The image tensor, assumed to be in CxHxW format.\n    original_size (tuple): The original size of the image (height, width).\n\n    Returns:\n    torch.Tensor: The unpadded image tensor.\n    \"\"\"\n    original_width, original_height = original_size\n    current_height, current_width = tensor.shape[1:]\n\n    # Compute aspect ratios\n    original_aspect_ratio = original_width / original_height\n    current_aspect_ratio = current_width / current_height\n\n    # Determine padding size and direction\n    if original_aspect_ratio > current_aspect_ratio:\n        # Padding was added to the height\n        scale_factor = current_width / original_width\n        new_height = int(original_height * scale_factor)\n        padding = (current_height - new_height) // 2\n        unpadded_tensor = tensor[:, padding : current_height - padding, :]\n    else:\n        # Padding was added to the width\n        scale_factor = current_height / original_height\n        new_width = int(original_width * scale_factor)\n        padding = (current_width - new_width) // 2\n        unpadded_tensor = tensor[:, :, padding : current_width - padding]\n\n    return unpadded_tensor\n\n\nclass LlavaMetaForCausalLM(ABC):\n\n    @abstractmethod\n    def get_model(self):\n        pass\n\n    def get_vision_tower(self):\n        return self.get_model().get_vision_tower()\n\n    def get_2dPool(self, image_feature):\n        height = width = self.get_vision_tower().num_patches_per_side\n        num_frames, num_tokens, num_dim = image_feature.shape\n        image_feature = image_feature.view(num_frames, height, width, -1)\n        image_feature = image_feature.permute(0, 3, 1, 2).contiguous()\n        # image_feature = nn.functional.max_pool2d(image_feature, self.config.mm_spatial_pool_stride)\n        if self.config.mm_spatial_pool_mode == \"average\":\n            image_feature = nn.functional.avg_pool2d(image_feature, self.config.mm_spatial_pool_stride)\n        elif self.config.mm_spatial_pool_mode == \"max\":\n            image_feature = nn.functional.max_pool2d(image_feature, self.config.mm_spatial_pool_stride)\n        elif self.config.mm_spatial_pool_mode == \"bilinear\":\n            height, weight = image_feature.shape[2:]\n            scaled_shape = [math.ceil(height / 2), math.ceil(weight / 2)]\n            image_feature = nn.functional.interpolate(image_feature, size=scaled_shape, mode='bilinear')\n\n        else:\n            raise ValueError(f\"Unexpected mm_spatial_pool_mode: {self.config.mm_spatial_pool_mode}\")\n        image_feature = image_feature.permute(0, 2, 3, 1)\n        image_feature = image_feature.view(num_frames, -1, num_dim)\n        return image_feature\n\n    def encode_images(self, images):\n        image_features = self.get_model().get_vision_tower()(images)\n        # image_features = self.get_model().vision_resampler(image_features, images=images)\n        image_features = self.get_model().mm_projector(image_features)\n        return image_features\n    \n    def encode_multimodals(self, videos_or_images, video_idx_in_batch, split_sizes=None):\n        videos_or_images_features = self.get_model().get_vision_tower()(videos_or_images)\n        per_videos_or_images_features = torch.split(videos_or_images_features, split_sizes, dim=0)  # tuple, (dim_1, 576, 4096)\n        all_videos_or_images_features = []\n\n        for idx, feat in enumerate(per_videos_or_images_features):\n            feat = self.get_model().mm_projector(feat)\n            if idx in video_idx_in_batch:\n                feat = self.get_2dPool(feat)\n            all_videos_or_images_features.append(feat)\n        return all_videos_or_images_features\n\n    def prepare_inputs_labels_for_multimodal(self, input_ids, position_ids, attention_mask, past_key_values, labels, images, modalities=[\"image\"], image_sizes=None):\n        vision_tower = self.get_vision_tower()\n        # rank_print(modalities)\n        if vision_tower is None or images is None or input_ids.shape[1] == 1:\n            return input_ids, position_ids, attention_mask, past_key_values, None, labels\n        \n        if type(images) is list or images.ndim == 5:\n            if type(images) is list:\n                images = [x.unsqueeze(0) if x.ndim == 3 else x for x in images]\n\n            video_idx_in_batch = []\n            for _ in range(len(modalities)):\n                if modalities[_] == \"video\":\n                    video_idx_in_batch.append(_)\n\n            images_list = []\n            for image in images:\n                if image.ndim == 4:\n                    images_list.append(image)\n                else:\n                    images_list.append(image.unsqueeze(0))\n\n            concat_images = torch.cat([image for image in images_list], dim=0)\n            split_sizes = [image.shape[0] for image in images_list]\n            encoded_image_features = self.encode_images(concat_images)\n            \n\n            # This is a list, each element is [num_images, patch * patch, dim]\n            # rank_print(f\"Concat images : {concat_images.shape}\")\n            encoded_image_features = torch.split(encoded_image_features, split_sizes)\n            image_features = []\n            for idx, image_feat in enumerate(encoded_image_features):\n                if idx in video_idx_in_batch:\n                    image_features.append(self.get_2dPool(image_feat))\n                else:\n                    image_features.append(image_feat)\n            # image_features = self.encode_multimodals(concat_images, video_idx_in_batch, split_sizes)\n            # rank_print(f\"Encoded image feats : {[x.shape for x in image_features]}\")\n            # image_features = torch.split(image_features, split_sizes, dim=0)\n            mm_patch_merge_type = getattr(self.config, \"mm_patch_merge_type\", \"flat\")\n            image_aspect_ratio = getattr(self.config, \"image_aspect_ratio\", \"square\")\n\n            if mm_patch_merge_type == \"flat\":\n                image_features = [x.flatten(0, 1) for x in image_features]\n            elif mm_patch_merge_type == \"force_avgpool2x2\":\n                processed_list = []\n                for tensor in image_features:\n                    # 将 [1, 729, 896] 转换为 [1, 896, 27, 27]\n                    reshaped_tensor = tensor.view(1, 896, 27, 27)\n                    # 执行大小为2的平均池化\n                    pooled_tensor = F.avg_pool2d(reshaped_tensor, kernel_size=2)\n                    # 将池化后的张量转换为 [1, 169, 896]\n                    final_tensor = pooled_tensor.view(1, -1, 896)\n\n                    processed_list.append(final_tensor)\n                image_features = [x.flatten(0, 1) for x in processed_list]\n\n            elif mm_patch_merge_type.startswith(\"spatial\"):\n                new_image_features = []\n                for image_idx, image_feature in enumerate(image_features):\n                    # FIXME: now assume the image is square, and split to 2x2 patches\n                    # num_patches = h * w, where h = w = sqrt(num_patches)\n                    # currently image_feature is a tensor of shape (4, num_patches, hidden_size)\n                    # we want to first unflatten it to (2, 2, h, w, hidden_size)\n                    # rank0_print(\"At least we are reaching here\")\n                    if image_idx in video_idx_in_batch:  # video operations\n                        # rank0_print(\"Video\")\n                        if \"unpad\" in mm_patch_merge_type:\n                            # image_feature = image_feature.permute(2, 0, 1).contiguous()\n                            # image_feature =  torch.cat((image_feature, self.model.image_newline[:, None, None].expand(*image_feature.shape[:-1], 1).to(image_feature.device)), dim=-1)\n                            # image_feature = image_feature.permute(1, 2, 0).contiguous()\n                            image_feature = image_feature.flatten(0, 1)\n                            image_feature = torch.cat((image_feature, self.model.image_newline[None].to(image_feature.device)), dim=0)\n\n                    elif image_feature.shape[0] > 1:  # multi patches and multi images operations\n                        # rank0_print(\"Single-images\")\n                        base_image_feature = image_feature[0]\n                        image_feature = image_feature[1:]\n                        height = width = self.get_vision_tower().num_patches_per_side\n                        assert height * width == base_image_feature.shape[0]\n\n                        if \"anyres_max\" in image_aspect_ratio:\n                            matched_anyres_max_num_patches = re.match(r\"anyres_max_(\\d+)\", image_aspect_ratio)\n                            if matched_anyres_max_num_patches:\n                                max_num_patches = int(matched_anyres_max_num_patches.group(1))\n\n                        if image_aspect_ratio == \"anyres\" or \"anyres_max\" in image_aspect_ratio:\n                            if hasattr(self.get_vision_tower(), \"image_size\"):\n                                vision_tower_image_size = self.get_vision_tower().image_size\n                            else:\n                                raise ValueError(\"vision_tower_image_size is not found in the vision tower.\")\n                            try:\n                                num_patch_width, num_patch_height = get_anyres_image_grid_shape(image_sizes[image_idx], self.config.image_grid_pinpoints, vision_tower_image_size)\n                            except Exception as e:\n                                rank0_print(f\"Error: {e}\")\n                                num_patch_width, num_patch_height = 2, 2\n                            image_feature = image_feature.view(num_patch_height, num_patch_width, height, width, -1)\n                        else:\n                            image_feature = image_feature.view(2, 2, height, width, -1)\n\n                        if \"maxpool2x2\" in mm_patch_merge_type:\n                            image_feature = image_feature.permute(4, 0, 2, 1, 3).contiguous()\n                            image_feature = image_feature.flatten(1, 2).flatten(2, 3)\n                            image_feature = nn.functional.max_pool2d(image_feature, 2)\n                            image_feature = image_feature.flatten(1, 2).transpose(0, 1)\n                        elif \"unpad\" in mm_patch_merge_type and \"anyres_max\" in image_aspect_ratio and matched_anyres_max_num_patches:\n                            unit = image_feature.shape[2]\n                            image_feature = image_feature.permute(4, 0, 2, 1, 3).contiguous()\n                            image_feature = image_feature.flatten(1, 2).flatten(2, 3)\n                            image_feature = unpad_image(image_feature, image_sizes[image_idx])\n                            c, h, w = image_feature.shape\n                            times = math.sqrt(h * w / (max_num_patches * unit**2))\n                            if times > 1.1:\n                                image_feature = image_feature[None]\n                                image_feature = nn.functional.interpolate(image_feature, [int(h // times), int(w // times)], mode=\"bilinear\")[0]\n                            image_feature = torch.cat((image_feature, self.model.image_newline[:, None, None].expand(*image_feature.shape[:-1], 1).to(image_feature.device)), dim=-1)\n                            image_feature = image_feature.flatten(1, 2).transpose(0, 1)\n                        elif \"unpad\" in mm_patch_merge_type:\n                            image_feature = image_feature.permute(4, 0, 2, 1, 3).contiguous()\n                            image_feature = image_feature.flatten(1, 2).flatten(2, 3)\n                            image_feature = unpad_image(image_feature, image_sizes[image_idx])\n                            image_feature = torch.cat((image_feature, self.model.image_newline[:, None, None].expand(*image_feature.shape[:-1], 1).to(image_feature.device)), dim=-1)\n                            image_feature = image_feature.flatten(1, 2).transpose(0, 1)\n                        else:\n                            image_feature = image_feature.permute(0, 2, 1, 3, 4).contiguous()\n                            image_feature = image_feature.flatten(0, 3)\n                        if \"nobase\" in mm_patch_merge_type:\n                            pass\n                        else:\n                            image_feature = torch.cat((base_image_feature, image_feature), dim=0)\n                    else:  # single image operations\n                        image_feature = image_feature[0]\n                        if \"unpad\" in mm_patch_merge_type:\n                            image_feature = torch.cat((image_feature, self.model.image_newline[None]), dim=0)\n\n                    new_image_features.append(image_feature)\n                image_features = new_image_features\n            else:\n                raise ValueError(f\"Unexpected mm_patch_merge_type: {self.config.mm_patch_merge_type}\")\n        else:\n            image_features = self.encode_images(images)\n\n        # TODO: image start / end is not implemented here to support pretraining.\n        if getattr(self.config, \"tune_mm_mlp_adapter\", False) and getattr(self.config, \"mm_use_im_start_end\", False):\n            raise NotImplementedError\n        # rank_print(f\"Total images : {len(image_features)}\")\n\n        # Let's just add dummy tensors if they do not exist,\n        # it is a headache to deal with None all the time.\n        # But it is not ideal, and if you have a better idea,\n        # please open an issue / submit a PR, thanks.\n        _labels = labels\n        _position_ids = position_ids\n        _attention_mask = attention_mask\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids, dtype=torch.bool)\n        else:\n            attention_mask = attention_mask.bool()\n        if position_ids is None:\n            position_ids = torch.arange(0, input_ids.shape[1], dtype=torch.long, device=input_ids.device)\n        if labels is None:\n            labels = torch.full_like(input_ids, IGNORE_INDEX)\n\n        # remove the padding using attention_mask -- FIXME\n        _input_ids = input_ids\n        input_ids = [cur_input_ids[cur_attention_mask] for cur_input_ids, cur_attention_mask in zip(input_ids, attention_mask)]\n        labels = [cur_labels[cur_attention_mask] for cur_labels, cur_attention_mask in zip(labels, attention_mask)]\n\n        new_input_embeds = []\n        new_labels = []\n        cur_image_idx = 0\n        # rank_print(\"Inserting Images embedding\")\n\n        for batch_idx, cur_input_ids in enumerate(input_ids):\n            num_images = (cur_input_ids == IMAGE_TOKEN_INDEX).sum()\n            # rank0_print(num_images)\n            if num_images == 0:\n                cur_image_features = image_features[cur_image_idx]\n                cur_input_embeds_1 = self.get_model().embed_tokens(cur_input_ids)\n                cur_input_embeds = torch.cat([cur_input_embeds_1, cur_image_features[0:0]], dim=0)\n                new_input_embeds.append(cur_input_embeds)\n                new_labels.append(labels[batch_idx])\n                cur_image_idx += 1\n                continue\n\n            image_token_indices = [-1] + torch.where(cur_input_ids == IMAGE_TOKEN_INDEX)[0].tolist() + [cur_input_ids.shape[0]]\n            cur_input_ids_noim = []\n            cur_labels = labels[batch_idx]\n            cur_labels_noim = []\n            for i in range(len(image_token_indices) - 1):\n                cur_input_ids_noim.append(cur_input_ids[image_token_indices[i] + 1 : image_token_indices[i + 1]])\n                cur_labels_noim.append(cur_labels[image_token_indices[i] + 1 : image_token_indices[i + 1]])\n            split_sizes = [x.shape[0] for x in cur_labels_noim]\n            cur_input_embeds = self.get_model().embed_tokens(torch.cat(cur_input_ids_noim))\n            cur_input_embeds_no_im = torch.split(cur_input_embeds, split_sizes, dim=0)\n            cur_new_input_embeds = []\n            cur_new_labels = []\n\n            for i in range(num_images + 1):\n                cur_new_input_embeds.append(cur_input_embeds_no_im[i])\n                cur_new_labels.append(cur_labels_noim[i])\n                if i < num_images:\n                    try:\n                        cur_image_features = image_features[cur_image_idx]\n                    except IndexError:\n                        cur_image_features = image_features[cur_image_idx - 1]\n                    cur_image_idx += 1\n                    cur_new_input_embeds.append(cur_image_features)\n                    cur_new_labels.append(torch.full((cur_image_features.shape[0],), IGNORE_INDEX, device=cur_labels.device, dtype=cur_labels.dtype))\n\n            cur_new_input_embeds = [x.to(self.device) for x in cur_new_input_embeds]\n\n            cur_new_input_embeds = torch.cat(cur_new_input_embeds)\n            cur_new_labels = torch.cat(cur_new_labels)\n\n            new_input_embeds.append(cur_new_input_embeds)\n            new_labels.append(cur_new_labels)\n\n        # Truncate sequences to max length as image embeddings can make the sequence longer\n        tokenizer_model_max_length = getattr(self.config, \"tokenizer_model_max_length\", None)\n        # rank_print(\"Finishing Inserting\")\n\n        new_input_embeds = [x[:tokenizer_model_max_length] for x, modality in zip(new_input_embeds, modalities)]\n        new_labels = [x[:tokenizer_model_max_length] for x, modality in zip(new_labels, modalities)]\n        # TODO: Hard code for control loss spike\n        # if tokenizer_model_max_length is not None:\n        #     new_input_embeds = [x[:4096] if modality != \"video\" else x[:tokenizer_model_max_length] for x, modality in zip(new_input_embeds, modalities)]\n        #     new_labels = [x[:4096] if modality != \"video\" else x[:tokenizer_model_max_length] for x, modality in zip(new_labels, modalities)]\n\n        # Combine them\n        max_len = max(x.shape[0] for x in new_input_embeds)\n        batch_size = len(new_input_embeds)\n\n        new_input_embeds_padded = []\n        new_labels_padded = torch.full((batch_size, max_len), IGNORE_INDEX, dtype=new_labels[0].dtype, device=new_labels[0].device)\n        attention_mask = torch.zeros((batch_size, max_len), dtype=attention_mask.dtype, device=attention_mask.device)\n        position_ids = torch.zeros((batch_size, max_len), dtype=position_ids.dtype, device=position_ids.device)\n        # rank0_print(\"Prepare pos id\")\n\n        for i, (cur_new_embed, cur_new_labels) in enumerate(zip(new_input_embeds, new_labels)):\n            cur_len = cur_new_embed.shape[0]\n            if getattr(self.config, \"tokenizer_padding_side\", \"right\") == \"left\":\n                new_input_embeds_padded.append(torch.cat((torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device), cur_new_embed), dim=0))\n                if cur_len > 0:\n                    new_labels_padded[i, -cur_len:] = cur_new_labels\n                    attention_mask[i, -cur_len:] = True\n                    position_ids[i, -cur_len:] = torch.arange(0, cur_len, dtype=position_ids.dtype, device=position_ids.device)\n            else:\n                new_input_embeds_padded.append(torch.cat((cur_new_embed, torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device)), dim=0))\n                if cur_len > 0:\n                    new_labels_padded[i, :cur_len] = cur_new_labels\n                    attention_mask[i, :cur_len] = True\n                    position_ids[i, :cur_len] = torch.arange(0, cur_len, dtype=position_ids.dtype, device=position_ids.device)\n\n        new_input_embeds = torch.stack(new_input_embeds_padded, dim=0)\n        # rank0_print(\"tokenizer padding\")\n\n        if _labels is None:\n            new_labels = None\n        else:\n            new_labels = new_labels_padded\n\n        if _attention_mask is None:\n            attention_mask = None\n        else:\n            attention_mask = attention_mask.to(dtype=_attention_mask.dtype)\n\n        if _position_ids is None:\n            position_ids = None\n        if getattr(self.config, \"use_pos_skipping\", False) and self.training:\n            position_ids = torch.arange(new_input_embeds.size(1), device=new_input_embeds.device).unsqueeze(0).to(new_input_embeds.device)\n            split_position = random.randint(0, new_input_embeds.size(1))\n            left_add = random.randint(0, self.config.pos_skipping_range)\n            right_add = random.randint(left_add, self.config.pos_skipping_range)\n            position_ids[:, :split_position] += left_add\n            position_ids[:, split_position:] += right_add\n        # rank0_print(\"Finish preparing\")\n        return None, position_ids, attention_mask, past_key_values, new_input_embeds, new_labels\n\n    def initialize_vision_tokenizer(self, model_args, tokenizer):\n        if model_args.mm_use_im_patch_token:\n            tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\n            self.resize_token_embeddings(len(tokenizer))\n\n        if model_args.mm_use_im_start_end:\n            num_new_tokens = tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\n            self.resize_token_embeddings(len(tokenizer))\n\n            if num_new_tokens > 0:\n                input_embeddings = self.get_input_embeddings().weight.data\n                output_embeddings = self.get_output_embeddings().weight.data\n\n                input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n                output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n\n                input_embeddings[-num_new_tokens:] = input_embeddings_avg\n                output_embeddings[-num_new_tokens:] = output_embeddings_avg\n\n            if model_args.tune_mm_mlp_adapter:\n                for p in self.get_input_embeddings().parameters():\n                    p.requires_grad = True\n                for p in self.get_output_embeddings().parameters():\n                    p.requires_grad = False\n\n            if model_args.pretrain_mm_mlp_adapter:\n                mm_projector_weights = torch.load(model_args.pretrain_mm_mlp_adapter, map_location=\"cpu\")\n                embed_tokens_weight = mm_projector_weights[\"model.embed_tokens.weight\"]\n                assert num_new_tokens == 2\n                if input_embeddings.shape == embed_tokens_weight.shape:\n                    input_embeddings[-num_new_tokens:] = embed_tokens_weight[-num_new_tokens:]\n                elif embed_tokens_weight.shape[0] == num_new_tokens:\n                    input_embeddings[-num_new_tokens:] = embed_tokens_weight\n                else:\n                    raise ValueError(f\"Unexpected embed_tokens_weight shape. Pretrained: {embed_tokens_weight.shape}. Current: {input_embeddings.shape}. Numer of new tokens: {num_new_tokens}.\")\n        elif model_args.mm_use_im_patch_token:\n            if model_args.tune_mm_mlp_adapter:\n                for p in self.get_input_embeddings().parameters():\n                    p.requires_grad = False\n                for p in self.get_output_embeddings().parameters():\n                    p.requires_grad = False"}
{"type": "source_file", "path": "llava/eval/model_vqa.py", "content": "import argparse\nimport torch\nimport os\nimport json\nfrom tqdm import tqdm\nimport shortuuid\n\nfrom llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\nfrom llava.conversation import conv_templates, SeparatorStyle\nfrom llava.model.builder import load_pretrained_model\nfrom llava.utils import disable_torch_init\nfrom llava.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\n\nfrom llava.constants import IGNORE_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IMAGE_TOKEN_INDEX\nfrom typing import Dict, Optional, Sequence, List\nimport transformers\nimport re\n\nfrom PIL import Image\nimport math\n\n\ndef split_list(lst, n):\n    \"\"\"Split a list into n (roughly) equal-sized chunks\"\"\"\n    chunk_size = math.ceil(len(lst) / n)  # integer division\n    return [lst[i:i+chunk_size] for i in range(0, len(lst), chunk_size)]\n\n\ndef get_chunk(lst, n, k):\n    chunks = split_list(lst, n)\n    return chunks[k]\n\ndef preprocess_qwen(sources, tokenizer: transformers.PreTrainedTokenizer, has_image: bool = False, max_len=2048, system_message: str = \"You are a helpful assistant.\") -> Dict:\n    roles = {\"human\": \"<|im_start|>user\", \"gpt\": \"<|im_start|>assistant\"}\n\n    im_start, im_end = tokenizer.additional_special_tokens_ids\n    nl_tokens = tokenizer(\"\\n\").input_ids\n    _system = tokenizer(\"system\").input_ids + nl_tokens\n    _user = tokenizer(\"user\").input_ids + nl_tokens\n    _assistant = tokenizer(\"assistant\").input_ids + nl_tokens\n\n    # Apply prompt templates\n    input_ids, targets = [], []\n\n    source = sources\n    if roles[source[0][\"from\"]] != roles[\"human\"]:\n        source = source[1:]\n\n    input_id, target = [], []\n    system = [im_start] + _system + tokenizer(system_message).input_ids + [im_end] + nl_tokens\n    input_id += system\n    target += [im_start] + [IGNORE_INDEX] * (len(system) - 3) + [im_end] + nl_tokens\n    assert len(input_id) == len(target)\n    for j, sentence in enumerate(source):\n        role = roles[sentence[\"from\"]]\n        if has_image and sentence[\"value\"] is not None and \"<image>\" in sentence[\"value\"]:\n            num_image = len(re.findall(DEFAULT_IMAGE_TOKEN, sentence[\"value\"]))\n            texts = sentence[\"value\"].split('<image>')\n            _input_id = tokenizer(role).input_ids + nl_tokens \n            for i,text in enumerate(texts):\n                _input_id += tokenizer(text).input_ids \n                if i<len(texts)-1:\n                    _input_id += [IMAGE_TOKEN_INDEX] + nl_tokens\n            _input_id += [im_end] + nl_tokens\n            assert sum([i==IMAGE_TOKEN_INDEX for i in _input_id])==num_image\n        else:\n            if sentence[\"value\"] is None:\n                _input_id = tokenizer(role).input_ids + nl_tokens\n            else:\n                _input_id = tokenizer(role).input_ids + nl_tokens + tokenizer(sentence[\"value\"]).input_ids + [im_end] + nl_tokens\n        input_id += _input_id\n        if role == \"<|im_start|>user\":\n            _target = [im_start] + [IGNORE_INDEX] * (len(_input_id) - 3) + [im_end] + nl_tokens\n        elif role == \"<|im_start|>assistant\":\n            _target = [im_start] + [IGNORE_INDEX] * len(tokenizer(role).input_ids) + _input_id[len(tokenizer(role).input_ids) + 1 : -2] + [im_end] + nl_tokens\n        else:\n            raise NotImplementedError\n        target += _target\n\n    input_ids.append(input_id)\n    targets.append(target)\n    input_ids = torch.tensor(input_ids, dtype=torch.long)\n    targets = torch.tensor(targets, dtype=torch.long)\n    return input_ids\n\ndef eval_model(args):\n    \n    # Model\n    disable_torch_init()\n    model_path = os.path.expanduser(args.model_path)\n    model_name = get_model_name_from_path(model_path)\n    tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, args.model_base, model_name)\n\n    # Data\n    with open(os.path.expanduser(args.question_file)) as f:\n        questions = json.load(f)\n    questions = get_chunk(questions, args.num_chunks, args.chunk_idx)\n    answers_file = os.path.expanduser(args.answers_file)\n    os.makedirs(os.path.dirname(answers_file), exist_ok=True)\n    ans_file = open(answers_file, \"w\")\n    \n    for line in tqdm(questions):\n        idx = line[\"sample_id\"]\n        question_type = line[\"metadata\"][\"question_type\"]\n        dataset_name = line[\"metadata\"][\"dataset\"]\n        gt = line[\"conversations\"][1][\"value\"]\n\n        image_files = line[\"image\"]\n        qs = line[\"conversations\"][0][\"value\"]\n        cur_prompt = args.extra_prompt + qs\n\n        args.conv_mode = \"qwen_1_5\"\n\n        conv = conv_templates[args.conv_mode].copy()\n        conv.append_message(conv.roles[0], qs)\n        conv.append_message(conv.roles[1], None)\n        prompt = conv.get_prompt()\n\n        input_ids = preprocess_qwen([line[\"conversations\"][0],{'from': 'gpt','value': None}], tokenizer, has_image=True).cuda()\n        img_num = list(input_ids.squeeze()).count(IMAGE_TOKEN_INDEX)\n\n        image_tensors = []\n        for image_file in image_files:\n            image = Image.open(os.path.join(args.image_folder, image_file))\n            image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values']\n            image_tensors.append(image_tensor.half().cuda())\n        # image_tensors = torch.cat(image_tensors, dim=0)\n\n        stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n        keywords = [stop_str]\n        stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n\n        with torch.inference_mode():\n            output_ids = model.generate(\n                input_ids,\n                images=image_tensors,\n                do_sample=True if args.temperature > 0 else False,\n                temperature=args.temperature,\n                top_p=args.top_p,\n                num_beams=args.num_beams,\n                # no_repeat_ngram_size=3,\n                max_new_tokens=1024,\n                use_cache=True)\n\n        \n        outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\n        outputs = outputs.strip()\n        if outputs.endswith(stop_str):\n            outputs = outputs[:-len(stop_str)]\n        outputs = outputs.strip()\n\n        ans_id = shortuuid.uuid()\n        ans_file.write(json.dumps({\n                                   \"dataset\": dataset_name,\n                                   \"sample_id\": idx,\n                                   \"prompt\": cur_prompt,\n                                   \"pred_response\": outputs,\n                                   \"gt_response\": gt,\n                                   \"shortuuid\": ans_id,\n                                   \"model_id\": model_name,\n                                   \"question_type\": question_type,\n                                   }) + \"\\n\")\n        ans_file.flush()\n\n        if len(line[\"conversations\"]) > 2:\n\n            for i in range(2, len(line[\"conversations\"]), 2):\n                input_ids = torch.cat((input_ids, output_ids), dim=1)\n\n                gt = line[\"conversations\"][i + 1][\"value\"]\n                qs = line[\"conversations\"][i][\"value\"]\n                cur_prompt = args.extra_prompt + qs\n\n                args.conv_mode = \"qwen_1_5\"\n\n                conv = conv_templates[args.conv_mode].copy()\n                conv.append_message(conv.roles[0], qs)\n                conv.append_message(conv.roles[1], None)\n                prompt = conv.get_prompt()\n\n                input_ids_new = preprocess_qwen([line[\"conversations\"][i],{'from': 'gpt','value': None}], tokenizer, has_image=True).cuda()\n                input_ids = torch.cat((input_ids, input_ids_new), dim=1)\n                img_num = list(input_ids_new.squeeze()).count(IMAGE_TOKEN_INDEX)\n\n                stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n                keywords = [stop_str]\n                stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n\n                with torch.inference_mode():\n                    output_ids = model.generate(\n                        input_ids,\n                        images=image_tensors,\n                        do_sample=True if args.temperature > 0 else False,\n                        temperature=args.temperature,\n                        top_p=args.top_p,\n                        num_beams=args.num_beams,\n                        # no_repeat_ngram_size=3,\n                        max_new_tokens=1024,\n                        use_cache=True)\n        \n                outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\n                outputs = outputs.strip()\n                if outputs.endswith(stop_str):\n                    outputs = outputs[:-len(stop_str)]\n                outputs = outputs.strip()\n\n                ans_id = shortuuid.uuid()\n                ans_file.write(json.dumps({\n                                        \"dataset\": dataset_name,\n                                        \"sample_id\": idx,\n                                        \"prompt\": cur_prompt,\n                                        \"pred_response\": outputs,\n                                        \"gt_response\": gt,\n                                        \"shortuuid\": ans_id,\n                                        \"model_id\": model_name,\n                                        \"question_type\": question_type,\n                                        }) + \"\\n\")\n                ans_file.flush()\n\n\n    ans_file.close()\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model-path\", type=str, default=\"facebook/opt-350m\")\n    parser.add_argument(\"--model-base\", type=str, default=None)\n    parser.add_argument(\"--image-folder\", type=str, default=\"\")\n    parser.add_argument(\"--extra-prompt\", type=str, default=\"\")\n    parser.add_argument(\"--question-file\", type=str, default=\"tables/question.jsonl\")\n    parser.add_argument(\"--answers-file\", type=str, default=\"answer.jsonl\")\n    parser.add_argument(\"--conv-mode\", type=str, default=\"llava_v1\")\n    parser.add_argument(\"--num-chunks\", type=int, default=1)\n    parser.add_argument(\"--chunk-idx\", type=int, default=0)\n    parser.add_argument(\"--temperature\", type=float, default=0.2)\n    parser.add_argument(\"--top_p\", type=float, default=None)\n    parser.add_argument(\"--num_beams\", type=int, default=1)\n    parser.add_argument(\"--test_size\", type=int, default=10000000)\n    args = parser.parse_args()\n\n    eval_model(args)"}
{"type": "source_file", "path": "llava/eval/evaluate_interleave.py", "content": "import re\nfrom rouge import Rouge\nimport argparse\nimport os\nimport json\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n\nspot_the_diff = [\"Spot-the-Diff\", \"Birds-to-Words\", \"CLEVR-Change\"]\nimage_edit_instruct = [\"IEdit\", \"HQ-Edit\", \"MagicBrush\"]\nvisual_story_telling = [\"AESOP\", \"FlintstonesSV\", \"PororoSV\", \"VIST\"]\nvisual_cloze = [\"COMICS_Dialogue\", \"RecipeQA_VisualCloze\"]\ntext_rich_vqa = [\"WebQA\", \"TQA\", \"OCR-VQA\", \"DocVQA\"]\nmulti_image_vqa = [\"MIT-States_StateCoherence\", \"MIT-States_PropertyCoherence\", \"VISION\", \"RecipeQA_ImageCoherence\"]\n\npuzzle = [\"RAVEN\"]\nnlrv2 = [\"NLVR2_Mantis\"]\nqbench = [\"QBench\"]\n\nclass Eval:\n    def __init__(self):\n        self.periodStrip = re.compile(\"(?!<=\\d)(\\.)(?!\\d)\")\n        self.commaStrip = re.compile(\"(\\d)(\\,)(\\d)\")\n        self.punct = [\n            \";\",\n            r\"/\",\n            \"[\",\n            \"]\",\n            '\"',\n            \"{\",\n            \"}\",\n            \"(\",\n            \")\",\n            \"=\",\n            \"+\",\n            \"\\\\\",\n            \"_\",\n            \"-\",\n            \">\",\n            \"<\",\n            \"@\",\n            \"`\",\n            \",\",\n            \"?\",\n            \"!\",\n        ]\n        \n    def processPunctuation(self, inText):\n        outText = inText\n        for p in self.punct:\n            if (p + \" \" in inText or \" \" + p in inText) or (\n                re.search(self.commaStrip, inText) != None\n            ):\n                outText = outText.replace(p, \"\")\n            else:\n                outText = outText.replace(p, \" \")\n        outText = self.periodStrip.sub(\"\", outText, re.UNICODE)\n        return outText\n    \n    def process(self, answer):\n        answer = answer.replace(\"\\n\", \" \")\n        answer = answer.replace(\"\\t\", \" \")\n        answer = answer.strip()\n        answer = self.processPunctuation(answer)\n        answer = answer.strip('\\'')\n        answer = answer.strip('\\\"')\n        answer = answer.strip(')')\n        answer = answer.strip('(')\n        answer = answer.strip().lower()\n        return answer\n\n    def evaluate_rouge(self,preds):\n        rouge = Rouge()\n        acc = {'f': []}\n        eval_list = []\n        for i, res in enumerate(preds):\n            sample_id = res['sample_id']\n            # print(sample_id)\n            gt_ans = self.process(res[\"gt_response\"])\n            pred_ans = self.process(res[\"pred_response\"])\n            # assert gt_ans != ''\n\n            if gt_ans == '':\n                continue\n            \n            if pred_ans == '':\n                s = 0\n            else:\n                if len(pred_ans) > 512:\n                    pred_ans = pred_ans[0: 512]\n                s = rouge.get_scores(pred_ans, gt_ans)[0]['rouge-l']['f']\n            acc['f'].append(s)\n            eval_list.append({'id':str(sample_id),'score':str(round(s,3))})\n        results = {'Rouge-L f': np.mean(acc['f'])}\n        return results,eval_list\n\n\n    def judge_multi_choice(self,sample):\n        sample_id = sample['sample_id']\n        gt_ans = sample[\"gt_response\"]\n        pred_ans = sample[\"pred_response\"]\n\n        if \":\" in pred_ans:\n            a_list = pred_ans.split(\":\")\n            a_list = [a.strip() for a in a_list ]\n            for a in a_list:\n                if len(a) == 1 and a[-1] in [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"]:\n                    pred_ans = a\n\n        if pred_ans == gt_ans:\n            return 1\n        else:\n            return 0\n\n    def process_sample(self,sample):\n        sample[\"gt_response\"] = self.process(sample[\"gt_response\"])\n        sample[\"pred_response\"] = self.process(sample[\"pred_response\"])\n\n    def evaluate_multichoice(self, preditions):\n        correct = 0\n        eval_list = []\n        for i, sample in enumerate(preditions):\n            self.process_sample(sample)\n            score = self.judge_multi_choice(sample)\n            sample_id = sample['sample_id']\n            sample['result'] = score\n            eval_list.append({'id':str(sample_id),'score':str(score)})\n            correct+=score\n        return {'Accuracy':correct/len(preditions)},eval_list\n\n    def evaluate_multi_choice_image(self,preditions):\n        correct = 0\n        eval_list = []\n        for i,sample in enumerate(preditions):\n            gt_ans = self.process(sample[\"gt_response\"])\n            pred_ans = self.process(sample[\"pred_response\"])\n            sample_id = sample['sample_id']\n\n            if \":\" in pred_ans:\n                a_list = pred_ans.split(\":\")\n                a_list = [a.strip() for a in a_list ]\n                for a in a_list:\n                    if len(a) == 1 and a[-1] in [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"]:\n                        pred_ans = a\n\n            if gt_ans == pred_ans:\n                score = 1\n            else:\n                score = 0\n            sample_id = sample['sample_id']\n            sample['result'] = score\n            eval_list.append({'id':str(sample_id),'score':str(score)})\n            correct+=score\n        return {'Accuracy':correct/len(preditions)},eval_list\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--result-dir', type=str, required=True)\n\n    args = parser.parse_args()\n    \n    result_file = os.path.join(args.result_dir, \"result.jsonl\")\n\n    if not os.path.exists(result_file):\n        print('No prediction file found')\n        exit(0)\n    with open(result_file, 'r') as f:\n        preds_all = [json.loads(line) for line in f]\n    \n    preds_all_dict = dict()\n    for pred in preds_all:\n        if pred[\"dataset\"] not in preds_all_dict:\n            preds_all_dict[pred[\"dataset\"]] = list()\n        preds_all_dict[pred[\"dataset\"]].append(pred)\n\n    image_choice_dataset_list = [\"recipeqa-RecipeQA_VisualCloze\", \"RecipeQA_ImageCoherence\", \"COMICS_Panel\"]\n    E = Eval()\n\n    eval_result_list = dict()\n    eval_result_list_detail = dict()\n\n    for dataset in preds_all_dict:\n        \n        preds = preds_all_dict[dataset]\n        question_type = preds[0][\"question_type\"]\n   \n        if question_type == 'open-ended':\n            eval_result, eval_list = E.evaluate_rouge(preds)\n\n        elif question_type == 'multi-choice' or dataset == 'nlrv2':\n            if dataset in image_choice_dataset_list:\n                eval_result, eval_list = E.evaluate_multi_choice_image(preds)\n            else:\n                eval_result, eval_list = E.evaluate_multichoice(preds)\n\n        else:\n            eval_result = 'Dataset not supported'\n            print('Dataset not supported')\n            exit(0)\n\n        print(dataset, end = ':  ')\n        print(eval_result)\n\n        eval_result_list[dataset] = eval_result\n        eval_result_list_detail[dataset] = eval_list\n\n    os.makedirs(args.result_dir, exist_ok=True)\n    with open(os.path.join(args.result_dir, 'eval_dataset.json'), 'w') as f:\n        json.dump(eval_result_list, f, indent=4)\n\n    with open(os.path.join(args.result_dir,'eval_dataset_details.json'), 'w') as f:\n        json.dump(eval_result_list_detail, f, indent=4)\n\n\n    eval_cat_list = dict()\n    print()\n\n    # spot_the_diff\n    score = 0\n    count = 0\n    for dataset in eval_result_list:\n        if dataset in spot_the_diff:\n            count += 1\n            score += list(eval_result_list[dataset].values())[0]\n    if count > 0:\n        score /= count\n        eval_cat_list[\"spot_the_diff\"] = score\n        print(\"spot_the_diff\", end = ':  ')\n        print('{:.2f}'.format(100 * score))\n\n    # image_edit_instruct\n    score = 0\n    count = 0\n    for dataset in eval_result_list:\n        if dataset in image_edit_instruct:\n            count += 1\n            score += list(eval_result_list[dataset].values())[0]\n    if count > 0:\n        score /= count\n        eval_cat_list[\"image_edit_instruct\"] = score\n        print(\"image_edit_instruct\", end = ':  ')\n        print('{:.2f}'.format(100 * score))\n\n    # visual_story_telling\n    score = 0\n    count = 0\n    for dataset in eval_result_list:\n        if dataset in visual_story_telling:\n            count += 1\n            score += list(eval_result_list[dataset].values())[0]\n    if count > 0:\n        score /= count\n        eval_cat_list[\"visual_story_telling\"] = score\n        print(\"visual_story_telling\", end = ':  ')\n        print('{:.2f}'.format(100 * score))\n\n    # visual_cloze\n    score = 0\n    count = 0\n    for dataset in eval_result_list:\n        if dataset in visual_cloze:\n            count += 1\n            score += list(eval_result_list[dataset].values())[0]\n    if count > 0:\n        score /= count\n        eval_cat_list[\"visual_cloze\"] = score\n        print(\"visual_cloze\", end = ':  ')\n        print('{:.2f}'.format(100 * score))\n\n    # text_rich_vqa\n    score = 0\n    count = 0\n    for dataset in eval_result_list:\n        if dataset in text_rich_vqa:\n            count += 1\n            score += list(eval_result_list[dataset].values())[0]\n    if count > 0:\n        score /= count\n        eval_cat_list[\"text_rich_vqa\"] = score\n        print(\"text_rich_vqa\", end = ':  ')\n        print('{:.2f}'.format(100 * score))\n\n    # multi_image_vqa\n    score = 0\n    count = 0\n    for dataset in eval_result_list:\n        if dataset in multi_image_vqa:\n            count += 1\n            score += list(eval_result_list[dataset].values())[0]\n    if count > 0:\n        score /= count\n        eval_cat_list[\"multi_image_vqa\"] = score\n        print(\"multi_image_vqa\", end = ':  ')\n        print('{:.2f}'.format(100 * score))\n\n    # puzzle\n    score = 0\n    count = 0\n    for dataset in eval_result_list:\n        if dataset in puzzle:\n            count += 1\n            score += list(eval_result_list[dataset].values())[0]\n    if count > 0:\n        score /= count\n        eval_cat_list[\"puzzle\"] = score\n        print(\"puzzle\", end = ':  ')\n        print('{:.2f}'.format(100 * score))\n\n    # nlrv2\n    score = 0\n    count = 0\n    for dataset in eval_result_list:\n        if dataset in nlrv2:\n            count += 1\n            score += list(eval_result_list[dataset].values())[0]\n    if count > 0:\n        score /= count\n        eval_cat_list[\"nlrv2\"] = score\n        print(\"nlrv2\", end = ':  ')\n        print('{:.2f}'.format(100 * score))\n\n    # qbench\n    score = 0\n    count = 0\n    for dataset in eval_result_list:\n        if dataset in qbench:\n            count += 1\n            score += list(eval_result_list[dataset].values())[0]\n    if count > 0:\n        score /= count\n        eval_cat_list[\"qbench\"] = score\n        print(\"qbench\", end = ':  ')\n        print('{:.2f}'.format(100 * score))\n\n    with open(os.path.join(args.result_dir,'eval_cat.json'), 'w') as f:\n        json.dump(eval_cat_list, f, indent=4)"}
{"type": "source_file", "path": "llava/dataset/llava_dataset.py", "content": "import ast\nimport os\nimport copy\nfrom dataclasses import dataclass, field\nimport json\nimport logging\nimport pathlib\nfrom typing import Dict, Optional, Sequence, List\nfrom PIL import Image, ImageFile\nfrom packaging import version\nimport numpy as np\n\nimport time\nimport random\nimport yaml\nimport math\nimport re\nimport torch\n\nimport transformers\nimport tokenizers\nimport deepspeed\n\nfrom transformers import AutoConfig\nfrom torch.utils.data import Dataset\nfrom llava.constants import IGNORE_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IMAGE_TOKEN_INDEX\nfrom llava.train.llava_trainer import LLaVATrainer\n\nfrom llava import conversation as conversation_lib\nfrom llava.model import *\nfrom llava.mm_utils import process_highres_image, process_anyres_image, process_highres_image_crop_split, tokenizer_image_token\nfrom llava.utils import rank0_print, process_video_with_pyav, process_video_with_decord\n\ntorch.multiprocessing.set_sharing_strategy(\"file_system\")\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\nlocal_rank = None\n\nIS_TOKENIZER_GREATER_THAN_0_14 = version.parse(tokenizers.__version__) >= version.parse(\"0.14\")\n\n\ndef _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n    \"\"\"Tokenize a list of strings.\"\"\"\n    tokenized_list = [\n        tokenizer(\n            text,\n            return_tensors=\"pt\",\n            padding=\"longest\",\n            max_length=tokenizer.model_max_length,\n            truncation=True,\n        )\n        for text in strings\n    ]\n    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n    input_ids_lens = labels_lens = [tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list]\n    return dict(\n        input_ids=input_ids,\n        labels=labels,\n        input_ids_lens=input_ids_lens,\n        labels_lens=labels_lens,\n    )\n\n\ndef _mask_targets(target, tokenized_lens, speakers):\n    # cur_idx = 0\n    cur_idx = tokenized_lens[0]\n    tokenized_lens = tokenized_lens[1:]\n    target[:cur_idx] = IGNORE_INDEX\n    for tokenized_len, speaker in zip(tokenized_lens, speakers):\n        if speaker == \"human\":\n            target[cur_idx + 2 : cur_idx + tokenized_len] = IGNORE_INDEX\n        cur_idx += tokenized_len\n\n\ndef _add_speaker_and_signal(header, source, get_conversation=True):\n    \"\"\"Add speaker and start/end signal on each round.\"\"\"\n    BEGIN_SIGNAL = \"### \"\n    END_SIGNAL = \"\\n\"\n    conversation = header\n    for sentence in source:\n        from_str = sentence[\"from\"]\n        if from_str.lower() == \"human\":\n            from_str = conversation_lib.default_conversation.roles[0]\n        elif from_str.lower() == \"gpt\":\n            from_str = conversation_lib.default_conversation.roles[1]\n        else:\n            from_str = \"unknown\"\n        sentence[\"value\"] = BEGIN_SIGNAL + from_str + \": \" + sentence[\"value\"] + END_SIGNAL\n        if get_conversation:\n            conversation += sentence[\"value\"]\n    conversation += BEGIN_SIGNAL\n    return conversation\n\n\ndef preprocess_multimodal(sources: Sequence[str], data_args) -> Dict:\n    is_multimodal = data_args.is_multimodal\n    if not is_multimodal:\n        return sources\n\n    for source in sources:\n        for sentence in source:\n            # TODO maybe this should be changed for interleaved data?\n            # if DEFAULT_IMAGE_TOKEN in sentence[\"value\"] and not sentence[\"value\"].startswith(DEFAULT_IMAGE_TOKEN):\n            # only check for num_im=1\n            num_im = len(re.findall(DEFAULT_IMAGE_TOKEN, sentence[\"value\"]))\n            if num_im == 1 and DEFAULT_IMAGE_TOKEN in sentence[\"value\"] and not sentence[\"value\"].startswith(DEFAULT_IMAGE_TOKEN):\n                sentence[\"value\"] = sentence[\"value\"].replace(DEFAULT_IMAGE_TOKEN, \"\").strip()\n                sentence[\"value\"] = DEFAULT_IMAGE_TOKEN + \"\\n\" + sentence[\"value\"]\n                sentence[\"value\"] = sentence[\"value\"].strip()\n                if \"mmtag\" in conversation_lib.default_conversation.version:\n                    sentence[\"value\"] = sentence[\"value\"].replace(DEFAULT_IMAGE_TOKEN, \"<Image>\" + DEFAULT_IMAGE_TOKEN + \"</Image>\")\n            replace_token = DEFAULT_IMAGE_TOKEN\n            if data_args.mm_use_im_start_end:\n                replace_token = DEFAULT_IM_START_TOKEN + replace_token + DEFAULT_IM_END_TOKEN\n            sentence[\"value\"] = sentence[\"value\"].replace(DEFAULT_IMAGE_TOKEN, replace_token)\n\n            # For videoInstruct-100k noisy_data. TODO: Ask Yuanhan to clean the data instead of leaving the noise code here.\n            sentence[\"value\"] = sentence[\"value\"].replace(\"QA_GT_caption_based_noisy\", \"\")\n\n    return sources\n\n\ndef preprocess_llama_2(sources, tokenizer: transformers.PreTrainedTokenizer, has_image: bool = False) -> Dict:\n    conv = conversation_lib.default_conversation.copy()\n    roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n\n    # Apply prompt templates\n    conversations = []\n    for i, source in enumerate(sources):\n        if roles[source[0][\"from\"]] != conv.roles[0]:\n            # Skip the first one if it is not from human\n            source = source[1:]\n\n        conv.messages = []\n        for j, sentence in enumerate(source):\n            role = roles[sentence[\"from\"]]\n            assert role == conv.roles[j % 2], f\"{i}\"\n            conv.append_message(role, sentence[\"value\"])\n        conversations.append(conv.get_prompt())\n\n    # Tokenize conversations\n\n    if has_image:\n        input_ids = torch.stack([tokenizer_image_token(prompt, tokenizer, return_tensors=\"pt\") for prompt in conversations], dim=0)\n    else:\n        input_ids = tokenizer(\n            conversations,\n            return_tensors=\"pt\",\n            padding=\"longest\",\n            max_length=tokenizer.model_max_length,\n            truncation=True,\n        ).input_ids\n\n    targets = input_ids.clone()\n\n    assert conv.sep_style == conversation_lib.SeparatorStyle.LLAMA_2\n\n    # Mask targets\n    sep = \"[/INST] \"\n    for conversation, target in zip(conversations, targets):\n        total_len = int(target.ne(tokenizer.pad_token_id).sum())\n\n        rounds = conversation.split(conv.sep2)\n        cur_len = 1\n        target[:cur_len] = IGNORE_INDEX\n        for i, rou in enumerate(rounds):\n            if rou == \"\":\n                break\n\n            parts = rou.split(sep)\n            if len(parts) != 2:\n                break\n            parts[0] += sep\n\n            if has_image:\n                round_len = len(tokenizer_image_token(rou, tokenizer))\n                instruction_len = len(tokenizer_image_token(parts[0], tokenizer)) - 2\n            else:\n                round_len = len(tokenizer(rou).input_ids)\n                instruction_len = len(tokenizer(parts[0]).input_ids) - 2\n\n            target[cur_len : cur_len + instruction_len] = IGNORE_INDEX\n\n            cur_len += round_len\n        target[cur_len:] = IGNORE_INDEX\n\n        if cur_len < tokenizer.model_max_length:\n            if cur_len != total_len:\n                target[:] = IGNORE_INDEX\n                print(f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\" f\" (ignored)\")\n\n    return dict(\n        input_ids=input_ids,\n        labels=targets,\n    )\n\n\ndef preprocess_gemma(sources: List[List[Dict[str, str]]], tokenizer: transformers.PreTrainedTokenizer, has_image: bool = False) -> Dict:\n    conv: conversation_lib.Conversation = conversation_lib.default_conversation.copy()\n    roles: Dict[str, str] = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n\n    # Apply prompt templates\n    conversations: List[str] = []\n    for i, source in enumerate(sources):\n        if roles[source[0][\"from\"]] != conv.roles[0]:\n            # Skip the first one if it is not from human\n            source: List[Dict[str, str]] = source[1:]\n\n        conv.messages = []\n        for j, sentence in enumerate(source):\n            role: str = roles[sentence[\"from\"]]\n            assert role == conv.roles[j % 2], f\"{i}\"\n            conv.append_message(role, sentence[\"value\"])\n        conversations.append(conv.get_prompt())\n\n    # Tokenize conversations\n    if has_image:\n        input_ids: torch.Tensor = torch.stack([tokenizer_image_token(prompt, tokenizer, return_tensors=\"pt\") for prompt in conversations], dim=0)\n    else:\n        input_ids: torch.Tensor = tokenizer(\n            conversations,\n            return_tensors=\"pt\",\n            padding=\"longest\",\n            max_length=tokenizer.model_max_length,\n            truncation=True,\n        ).input_ids\n\n    targets: torch.Tensor = input_ids.clone()\n    assert conv.sep_style == conversation_lib.SeparatorStyle.GEMMA\n\n    # Mask target\n    sep: str = conv.sep + conv.roles[1]\n    for conversation, target in zip(conversations, targets):\n        total_len: int = int(target.ne(tokenizer.pad_token_id).sum())\n\n        rounds: List[str] = conversation.split(conv.sep)\n        re_rounds = []\n        for conv_idx in range(0, len(rounds), 2):\n            re_rounds.append(conv.sep.join(rounds[conv_idx : conv_idx + 2]))\n\n        cur_len = 1  # Ignore <bos>\n        target[:cur_len] = IGNORE_INDEX\n        for i, rou in enumerate(re_rounds):\n            if rou == \"\":\n                break\n\n            parts = rou.split(sep)\n            if len(parts) != 2:\n                break\n            parts[0] += sep  # Re-append sep because split on this\n            # Now \"\".join(parts)==rou\n\n            if has_image:\n                round_len = len(tokenizer_image_token(rou, tokenizer)) - 1  # Ignore <bos>\n                instruction_len = len(tokenizer_image_token(parts[0], tokenizer)) - 1  # Ignore <bos>\n            else:\n                round_len = len(tokenizer(rou).input_ids) - 1  # Ignore <bos>\n                instruction_len = len(tokenizer(parts[0]).input_ids) - 1  # Ignore <bos>\n\n            round_len += 2  # sep: <end_of_turn>\\n takes 2 tokens\n            target[cur_len : cur_len + instruction_len] = IGNORE_INDEX\n            cur_len += round_len\n\n        target[cur_len:] = IGNORE_INDEX\n\n        if cur_len < tokenizer.model_max_length:\n            if cur_len != total_len:\n                target[:] = IGNORE_INDEX\n                print(f\"warning: tokenization mismatch: {cur_len} vs. {total_len}.\" f\" (ignored)\")\n\n    return dict(\n        input_ids=input_ids,\n        labels=targets,\n    )\n\n\ndef preprocess_qwen(sources, tokenizer: transformers.PreTrainedTokenizer, has_image: bool = False, max_len=2048, system_message: str = \"You are a helpful assistant.\") -> Dict:\n    # roles = {\"human\": \"<|im_start|>user\", \"gpt\": \"<|im_start|>assistant\"}\n    roles = {\"human\": \"user\", \"gpt\": \"assistant\"}\n\n    # Add image tokens to tokenizer as a special tokens\n    # Use a deepcopy of tokenizer so that we don't modify on the tokenizer\n    tokenizer = copy.deepcopy(tokenizer)\n    # When there is actually an image, we add the image tokens as a special token\n    if has_image:\n        tokenizer.add_tokens([\"<image>\"], special_tokens=True)\n\n    image_token_index = tokenizer.convert_tokens_to_ids(\"<image>\")\n    im_start = tokenizer.encode(\"<|im_start|>\")[0]\n    im_end = tokenizer.encode(\"<|im_end|>\")[0]\n    # im_start, im_end = tokenizer.additional_special_tokens_ids\n    # unmask_tokens = [\"<|im_start|>\", \"<|im_start|>\", \"\\n\"]\n    unmask_tokens_idx =  [198, im_start, im_end]\n    nl_tokens = tokenizer(\"\\n\").input_ids\n\n    # Reset Qwen chat templates so that it won't include system message every time we apply\n    chat_template = \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n    tokenizer.chat_template = chat_template\n\n    # _system = tokenizer(\"system\").input_ids + nl_tokens\n    # _user = tokenizer(\"user\").input_ids + nl_tokens\n    # _assistant = tokenizer(\"assistant\").input_ids + nl_tokens\n\n    # Apply prompt templates\n    input_ids, targets = [], []\n    for i, source in enumerate(sources):\n        if roles[source[0][\"from\"]] != roles[\"human\"]:\n            source = source[1:]\n\n        input_id, target = [], []\n\n        # New version, use apply chat template\n        # Build system message for each sentence\n        input_id += tokenizer.apply_chat_template([{\"role\" : \"system\", \"content\" : system_message}])\n        target += [IGNORE_INDEX] * len(input_id)\n\n        for conv in source:\n            # Make sure llava data can load\n            try:\n                role = conv[\"role\"]\n                content = conv[\"content\"]\n            except:\n                role = conv[\"from\"]\n                content = conv[\"value\"]\n\n            role =  roles.get(role, role)\n            \n            conv = [{\"role\" : role, \"content\" : content}]\n            encode_id = tokenizer.apply_chat_template(conv)\n            input_id += encode_id\n            if role in [\"user\", \"system\"]:\n                target += [IGNORE_INDEX] * len(encode_id)\n            else:\n                target += encode_id\n        \n\n        assert len(input_id) == len(target), f\"{len(input_id)} != {len(target)}\"\n        for idx, encode_id in enumerate(input_id):\n            if encode_id in unmask_tokens_idx:\n                target[idx] = encode_id\n            if encode_id == image_token_index:\n                input_id[idx] = IMAGE_TOKEN_INDEX\n        input_ids.append(input_id)\n        targets.append(target)\n    input_ids = torch.tensor(input_ids, dtype=torch.long)\n    targets = torch.tensor(targets, dtype=torch.long)\n\n    return dict(\n        input_ids=input_ids,  # tensor(bs x seq_len)\n        labels=targets,  # tensor(bs x seq_len)\n    )\n\n\ndef preprocess_llama3(\n    sources,\n    tokenizer: transformers.PreTrainedTokenizer,\n    has_image: bool = False,\n    max_len=2048,\n    system_message: str = \"You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\",\n) -> Dict:\n    # roles = {\"human\": \"<|start_header_id|>user<|end_header_id|>\", \"gpt\": \"<|start_header_id|>assistant<|end_header_id|>\"}\n    roles = {\"human\": \"user\", \"gpt\": \"assistant\"}\n\n    # Add image tokens to tokenizer as a special tokens\n    # Use a deepcopy of tokenizer so that we don't modify on the tokenizer\n    tokenizer = copy.deepcopy(tokenizer)\n    # When there is actually an image, we add the image tokens as a special token\n    if has_image:\n        tokenizer.add_tokens([\"<image>\"], special_tokens=True)\n    image_token_index = tokenizer.convert_tokens_to_ids(\"<image>\")\n    bos_token_id = tokenizer.convert_tokens_to_ids(\"<|begin_of_text|>\")\n    start_header_id = tokenizer.convert_tokens_to_ids(\"<|start_header_id|>\")\n    end_header_id = tokenizer.convert_tokens_to_ids(\"<|end_header_id|>\")\n    eot_id = tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n\n    unmask_tokens = [\"<|begin_of_text|>\", \"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\", \"\\n\\n\"]\n    unmask_tokens_idx = [tokenizer.convert_tokens_to_ids(tok) for tok in unmask_tokens]\n\n    # After update, calling tokenizer of llama3 will\n    # auto add bos id for the tokens. ヽ(｀⌒´)ﾉ\n    def safe_tokenizer_llama3(text):\n        input_ids = tokenizer(text).input_ids\n        if input_ids[0] == bos_token_id:\n            input_ids = input_ids[1:]\n        return input_ids\n\n    nl_tokens = tokenizer.convert_tokens_to_ids(\"\\n\\n\")\n    # Apply prompt templates\n    input_ids, targets = [], []\n    for i, source in enumerate(sources):\n        if roles[source[0][\"from\"]] != roles[\"human\"]:\n            source = source[1:]\n\n        input_id, target = [], []\n\n        # New version, use apply chat template\n        # Build system message for each sentence\n        input_id += tokenizer.apply_chat_template([{\"role\" : \"system\", \"content\" : system_message}])\n        target += [IGNORE_INDEX] * len(input_id)\n\n        for conv in source:\n            # Make sure llava data can load\n            try:\n                role = conv[\"role\"]\n                content = conv[\"content\"]\n            except:\n                role = conv[\"from\"]\n                content = conv[\"value\"]\n\n            role =  roles.get(role, role)\n            \n            conv = [{\"role\" : role, \"content\" : content}]\n            # First is bos token we don't need here\n            encode_id = tokenizer.apply_chat_template(conv)[1:]\n            input_id += encode_id\n            if role in [\"user\", \"system\"]:\n                target += [IGNORE_INDEX] * len(encode_id)\n            else:\n                target += encode_id\n        \n\n                    \n        assert len(input_id) == len(target), f\"{len(input_id)} != {len(target)}\"\n        for idx, encode_id in enumerate(input_id):\n            if encode_id in unmask_tokens_idx:\n                target[idx] = encode_id\n            if encode_id == image_token_index:\n                input_id[idx] = IMAGE_TOKEN_INDEX\n        input_ids.append(input_id)\n        targets.append(target)\n    input_ids = torch.tensor(input_ids, dtype=torch.long)\n    targets = torch.tensor(targets, dtype=torch.long)\n\n    return dict(\n        input_ids=input_ids,  # tensor(bs x seq_len)\n        labels=targets,  # tensor(bs x seq_len)\n    )\n\n\ndef preprocess_v1(sources, tokenizer: transformers.PreTrainedTokenizer, has_image: bool = False) -> Dict:\n    conv = conversation_lib.default_conversation.copy()\n    roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n\n    # Apply prompt templates\n    conversations = []\n    for i, source in enumerate(sources):\n        if roles[source[0][\"from\"]] != conv.roles[0]:\n            # Skip the first one if it is not from human\n            source = source[1:]\n\n        conv.messages = []\n        for j, sentence in enumerate(source):\n            role = roles[sentence[\"from\"]]\n            assert role == conv.roles[j % 2], f\"{i}\"\n            conv.append_message(role, sentence[\"value\"])\n        conversations.append(conv.get_prompt())\n\n    # Tokenize conversations\n\n    if has_image:\n        input_ids = torch.stack([tokenizer_image_token(prompt, tokenizer, return_tensors=\"pt\") for prompt in conversations], dim=0)\n    else:\n        input_ids = tokenizer(\n            conversations,\n            return_tensors=\"pt\",\n            padding=\"longest\",\n            max_length=tokenizer.model_max_length,\n            truncation=True,\n        ).input_ids\n\n    targets = input_ids.clone()\n\n    assert conv.sep_style == conversation_lib.SeparatorStyle.TWO\n\n    # Mask targets\n    sep = conv.sep + conv.roles[1] + \": \"\n    for conversation, target in zip(conversations, targets):\n        total_len = int(target.ne(tokenizer.pad_token_id).sum())\n\n        rounds = conversation.split(conv.sep2)\n        cur_len = 1\n        target[:cur_len] = IGNORE_INDEX\n        for i, rou in enumerate(rounds):\n            if rou == \"\":\n                break\n\n            parts = rou.split(sep)\n            if len(parts) != 2:\n                break\n            parts[0] += sep\n\n            if has_image:\n                round_len = len(tokenizer_image_token(rou, tokenizer))\n                instruction_len = len(tokenizer_image_token(parts[0], tokenizer)) - 2\n            else:\n                round_len = len(tokenizer(rou).input_ids)\n                instruction_len = len(tokenizer(parts[0]).input_ids) - 2\n\n            if i != 0 and not tokenizer.legacy and IS_TOKENIZER_GREATER_THAN_0_14:\n                round_len -= 1\n                instruction_len -= 1\n\n            target[cur_len : cur_len + instruction_len] = IGNORE_INDEX\n\n            cur_len += round_len\n        target[cur_len:] = IGNORE_INDEX\n\n        if cur_len < tokenizer.model_max_length:\n            if cur_len != total_len:\n                target[:] = IGNORE_INDEX\n                print(f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\" f\" (ignored)\")\n\n    return dict(\n        input_ids=input_ids,\n        labels=targets,\n    )\n\n\ndef preprocess_mpt(sources, tokenizer: transformers.PreTrainedTokenizer, has_image: bool = False) -> Dict:\n    conv = conversation_lib.default_conversation.copy()\n    roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n\n    # Apply prompt templates\n    conversations = []\n    for i, source in enumerate(sources):\n        if roles[source[0][\"from\"]] != conv.roles[0]:\n            # Skip the first one if it is not from human\n            source = source[1:]\n\n        conv.messages = []\n        for j, sentence in enumerate(source):\n            role = roles[sentence[\"from\"]]\n            assert role == conv.roles[j % 2], f\"{i}\"\n            conv.append_message(role, sentence[\"value\"])\n        conversations.append(conv.get_prompt())\n\n    # Tokenize conversations\n\n    if has_image:\n        input_ids = torch.stack([tokenizer_image_token(prompt, tokenizer, return_tensors=\"pt\") for prompt in conversations], dim=0)\n    else:\n        input_ids = tokenizer(\n            conversations,\n            return_tensors=\"pt\",\n            padding=\"longest\",\n            max_length=tokenizer.model_max_length,\n            truncation=True,\n        ).input_ids\n\n    targets = input_ids.clone()\n    assert conv.sep_style == conversation_lib.SeparatorStyle.MPT\n\n    # Mask targets\n    sep = conv.sep + conv.roles[1]\n    for conversation, target in zip(conversations, targets):\n        total_len = int(target.ne(tokenizer.pad_token_id).sum())\n\n        rounds = conversation.split(conv.sep)\n        re_rounds = [conv.sep.join(rounds[:3])]  # system + user + gpt\n        for conv_idx in range(3, len(rounds), 2):\n            re_rounds.append(conv.sep.join(rounds[conv_idx : conv_idx + 2]))  # user + gpt\n        cur_len = 1\n        target[:cur_len] = IGNORE_INDEX\n        for i, rou in enumerate(re_rounds):\n            if rou == \"\":\n                break\n\n            parts = rou.split(sep)\n            if len(parts) != 2:\n                break\n            parts[0] += sep\n\n            if has_image:\n                round_len = len(tokenizer_image_token(rou, tokenizer))\n                instruction_len = len(tokenizer_image_token(parts[0], tokenizer)) - 1\n            else:\n                round_len = len(tokenizer(rou).input_ids)\n                instruction_len = len(tokenizer(parts[0]).input_ids) - 1\n\n            if i != 0 and getattr(tokenizer, \"legacy\", False) and IS_TOKENIZER_GREATER_THAN_0_14:\n                round_len += 1\n                instruction_len += 1\n\n            target[cur_len : cur_len + instruction_len] = IGNORE_INDEX\n\n            cur_len += round_len\n        target[cur_len:] = IGNORE_INDEX\n\n        if cur_len < tokenizer.model_max_length:\n            if cur_len != total_len:\n                target[:] = IGNORE_INDEX\n                print(f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\" f\"(#turns={len(re_rounds)} ignored)\")\n\n    return dict(\n        input_ids=input_ids,\n        labels=targets,\n    )\n\n\ndef preprocess_plain(\n    sources: Sequence[str],\n    tokenizer: transformers.PreTrainedTokenizer,\n) -> Dict:\n    # add end signal and concatenate together\n    conversations = []\n    for source in sources:\n        assert len(source) == 2\n        assert DEFAULT_IMAGE_TOKEN in source[0][\"value\"]\n        source[0][\"value\"] = DEFAULT_IMAGE_TOKEN\n        conversation = source[0][\"value\"] + source[1][\"value\"] + conversation_lib.default_conversation.sep\n        conversations.append(conversation)\n    # tokenize conversations\n    input_ids = [tokenizer_image_token(prompt, tokenizer, return_tensors=\"pt\") for prompt in conversations]\n    targets = copy.deepcopy(input_ids)\n    for target, source in zip(targets, sources):\n        tokenized_len = len(tokenizer_image_token(source[0][\"value\"], tokenizer))\n        target[:tokenized_len] = IGNORE_INDEX\n\n    return dict(input_ids=input_ids, labels=targets)\n\n\ndef preprocess(sources: Sequence[str], tokenizer: transformers.PreTrainedTokenizer, has_image: bool = False) -> Dict:\n    \"\"\"\n    Given a list of sources, each is a conversation list. This transform:\n    1. Add signal '### ' at the beginning each sentence, with end signal '\\n';\n    2. Concatenate conversations together;\n    3. Tokenize the concatenated conversation;\n    4. Make a deepcopy as the target. Mask human words with IGNORE_INDEX.\n    \"\"\"\n    if conversation_lib.default_conversation.sep_style == conversation_lib.SeparatorStyle.PLAIN:\n        return preprocess_plain(sources, tokenizer)\n    if conversation_lib.default_conversation.sep_style == conversation_lib.SeparatorStyle.LLAMA_2:\n        return preprocess_llama_2(sources, tokenizer, has_image=has_image)\n    if conversation_lib.default_conversation.version.startswith(\"v1\"):\n        return preprocess_v1(sources, tokenizer, has_image=has_image)\n    if conversation_lib.default_conversation.version == \"mpt\":\n        return preprocess_mpt(sources, tokenizer, has_image=has_image)\n    if conversation_lib.default_conversation.version == \"qwen\":\n        return preprocess_qwen(sources, tokenizer, has_image=has_image)\n    if conversation_lib.default_conversation.version == \"gemma\":\n        return preprocess_gemma(sources, tokenizer, has_image=has_image)\n    if conversation_lib.default_conversation.version == \"llama_v3\":\n        return preprocess_llama3(sources, tokenizer, has_image=has_image)\n    # add end signal and concatenate together\n    conversations = []\n    for source in sources:\n        header = f\"{conversation_lib.default_conversation.system}\\n\\n\"\n        conversation = _add_speaker_and_signal(header, source)\n        conversations.append(conversation)\n\n    # tokenize conversations\n    def get_tokenize_len(prompts):\n        return [len(tokenizer_image_token(prompt, tokenizer)) for prompt in prompts]\n\n    if has_image:\n        input_ids = [tokenizer_image_token(prompt, tokenizer, return_tensors=\"pt\") for prompt in conversations]\n    else:\n        conversations_tokenized = _tokenize_fn(conversations, tokenizer)\n        input_ids = conversations_tokenized[\"input_ids\"]\n\n    targets = copy.deepcopy(input_ids)\n    for target, source in zip(targets, sources):\n        if has_image:\n            tokenized_lens = get_tokenize_len([header] + [s[\"value\"] for s in source])\n        else:\n            tokenized_lens = _tokenize_fn([header] + [s[\"value\"] for s in source], tokenizer)[\"input_ids_lens\"]\n        speakers = [sentence[\"from\"] for sentence in source]\n        _mask_targets(target, tokenized_lens, speakers)\n\n    return dict(input_ids=input_ids, labels=targets)\n\n\nclass LazySupervisedDataset(Dataset):\n    def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer, data_args):\n        super(LazySupervisedDataset, self).__init__()\n        self.tokenizer = tokenizer\n        self.data_args = data_args\n        self.list_data_dict = []\n        # Handle multiple JSON files specified in the data_path\n        self.load_data(data_path)\n        random.shuffle(self.list_data_dict)\n\n\n        rank0_print(f\"Loaded {len(self.list_data_dict)} samples from {data_path}\")\n        rank0_print(\"Formatting inputs...Skip in lazy mode\")\n\n    def __len__(self):\n        return len(self.list_data_dict)\n\n    def load_data(self, data_path):\n        if \"[\" in data_path and \"]\" in data_path:\n            paths = data_path.strip(\"[\").strip(\"]\").split(\",\")\n            for path in paths:\n                self.load_single_file(path.strip())\n        elif \"{\" in data_path and \"}\" in data_path:\n            base_path, file_pattern = re.match(r\"^(.*)\\{(.*)\\}\\.(jsonl|json)$\", data_path).groups()\n            file_names = file_pattern.split(\",\")\n            print(f\"Loading {file_names} from {base_path}\")\n            self.data_args.dataset_paths = []\n            for file_name in file_names:\n                full_path = f\"{base_path}{file_name}.json\"\n                if os.path.exists(full_path):\n                    self.data_args.dataset_paths.append(full_path)\n                    print(f\"Loading {full_path}\")\n                    self.load_single_file(full_path)\n                else:\n                    print(f\"path {full_path} not exists, skipped!!!\")\n        elif data_path.endswith(\".yaml\"):\n            self.load_yaml_file(data_path)\n        else:\n            # Fallback to original pattern matching logic\n            self.load_single_file(data_path)\n\n    def load_single_file(self, file_path):\n        if isinstance(file_path, list):\n            for path in file_path:\n                self.load_single_file(path)\n            return\n\n        print(f\"Loading {file_path}\")\n        if file_path.endswith(\".jsonl\"):\n            slice_data_dict = []\n            with open(file_path, \"r\") as file:\n                for line in file:\n                    try:\n                        slice_data_dict.append(json.loads(line.strip()))\n                    except Exception as e:\n                        print(f\"Error during loading {file_path}: {e}\")\n            print(f\"Loaded {len(slice_data_dict)} samples from {file_path}\")\n\n        elif file_path.endswith(\".json\"):\n            with open(file_path, \"r\") as file:\n                slice_data_dict = json.load(file)\n            print(f\"Loaded {len(slice_data_dict)} samples from {file_path}\")\n\n        else:\n            raise ValueError(f\"Unsupported file type: {file_path}\")\n\n        self.list_data_dict.extend(slice_data_dict)\n\n    def load_yaml_file(self, yaml_path):\n        with open(yaml_path, \"r\") as file:\n            yaml_data = yaml.safe_load(file)\n            datasets = yaml_data.get(\"datasets\")\n            self.data_args.dataset_paths = [dataset.get(\"json_path\") for dataset in datasets]\n            for dataset in datasets:\n                json_path = dataset.get(\"json_path\")\n                sampling_strategy = dataset.get(\"sampling_strategy\", \"all\")\n                self.apply_sampling_strategy(json_path, sampling_strategy)\n\n    def apply_sampling_strategy(self, json_path, sampling_strategy):\n        print(f\"Loading {json_path} with {sampling_strategy} sampling strategy\")\n        if json_path.endswith(\".jsonl\"):\n            cur_data_dict = []\n            with open(json_path, \"r\") as json_file:\n                for line in json_file:\n                    cur_data_dict.append(json.loads(line.strip()))\n        elif json_path.endswith(\".json\"):\n            with open(json_path, \"r\") as json_file:\n                cur_data_dict = json.load(json_file)\n        else:\n            raise ValueError(f\"Unsupported file type: {json_path}\")\n\n        if \":\" in sampling_strategy:\n            sampling_strategy, sampling_number = sampling_strategy.split(\":\")\n            if \"%\" in sampling_number:\n                sampling_number = math.ceil(int(sampling_number.split(\"%\")[0]) * len(cur_data_dict) / 100)\n            else:\n                sampling_number = int(sampling_number)\n\n            if sampling_strategy == \"first\":\n                cur_data_dict = cur_data_dict[:sampling_number]\n            elif sampling_strategy == \"end\":\n                cur_data_dict = cur_data_dict[-sampling_number:]\n            elif sampling_strategy == \"random\":\n                random.shuffle(cur_data_dict)\n                cur_data_dict = cur_data_dict[:sampling_number]\n\n        print(f\"Loaded {len(cur_data_dict)} samples from {json_path}\")\n        self.list_data_dict.extend(cur_data_dict)\n    \n    @property\n    def lengths(self):\n        length_list = []\n        for sample in self.list_data_dict:\n            img_tokens = 128 if \"image\" in sample else 0\n            length_list.append(sum(len(conv[\"value\"].split()) for conv in sample[\"conversations\"]) + img_tokens)\n        return length_list\n\n    @property\n    def modality_lengths(self):\n        length_list = []\n        for sample in self.list_data_dict:\n            cur_len = sum(len(conv[\"value\"].split()) for conv in sample[\"conversations\"])\n            assert cur_len > 0, f\"Conversation length is 0 for {sample}\"\n            if \"image\" in sample or \"video\" in sample or self.data_args.early_mix_text:\n                length_list.append(cur_len)\n            else:\n                length_list.append(-cur_len)\n        return length_list\n\n    def process_image(self, image_file, overwrite_image_aspect_ratio=None):\n        image_folder = self.data_args.image_folder\n        processor = self.data_args.image_processor\n        # print(f\"\\n\\nInspecting the image path, folder = {image_folder}, image={image_file}\\n\\n\")\n        try:\n            if image_folder is not None:\n                image = Image.open(os.path.join(image_folder, image_file)).convert(\"RGB\")\n            else:\n                image = Image.open(image_file).convert(\"RGB\")\n        except Exception as exn:\n            print(f\"Failed to open image {image_file}. Exception:\", exn)\n            raise exn\n\n        image_size = image.size\n        image_aspect_ratio = self.data_args.image_aspect_ratio\n        if overwrite_image_aspect_ratio is not None:\n            image_aspect_ratio = overwrite_image_aspect_ratio\n        if image_aspect_ratio == \"highres\":\n            image = process_highres_image(image, self.data_args.image_processor, self.data_args.image_grid_pinpoints)\n        elif image_aspect_ratio == \"anyres\" or \"anyres_max\" in image_aspect_ratio:\n            image = process_anyres_image(image, self.data_args.image_processor, self.data_args.image_grid_pinpoints)\n        elif image_aspect_ratio == \"crop_split\":\n            image = process_highres_image_crop_split(image, self.data_args)\n        elif image_aspect_ratio == \"pad\":\n\n            def expand2square(pil_img, background_color):\n                width, height = pil_img.size\n                if width == height:\n                    return pil_img\n                elif width > height:\n                    result = Image.new(pil_img.mode, (width, width), background_color)\n                    result.paste(pil_img, (0, (width - height) // 2))\n                    return result\n                else:\n                    result = Image.new(pil_img.mode, (height, height), background_color)\n                    result.paste(pil_img, ((height - width) // 2, 0))\n                    return result\n\n            image = expand2square(image, tuple(int(x * 255) for x in processor.image_mean))\n            image = processor.preprocess(image, return_tensors=\"pt\")[\"pixel_values\"][0]\n        else:\n            image = processor.preprocess(image, return_tensors=\"pt\")[\"pixel_values\"][0]\n        return image, image_size, \"image\"\n\n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n        # TODO: define number of retries somewhere else\n        num_base_retries = 3\n        num_final_retries = 300\n\n        # try the current sample first\n        for attempt_idx in range(num_base_retries):\n            try:\n                sample = self._get_item(i)\n                return sample\n            except Exception as e:\n                # sleep 1s in case it is a cloud disk issue\n                print(f\"[Try #{attempt_idx}] Failed to fetch sample {i}. Exception:\", e)\n                time.sleep(1)\n\n        # try other samples, in case it is file corruption issue\n        for attempt_idx in range(num_base_retries):\n            try:\n                # next_index = min(i + 1, len(self.list_data_dict) - 1)\n                sample_idx = random.choice(range(len(self)))\n                if  sample_idx == i:\n                    next_index = random.choice(range(len(self)))\n                else:\n                    next_index = sample_idx\n                sample = self._get_item(next_index)\n                return sample\n            except Exception as e:\n                # no need to sleep\n                print(f\"[Try other #{attempt_idx}] Failed to fetch sample {next_index}. Exception:\", e)\n                pass\n\n        try:\n            sample = self._get_item(i)\n            return sample\n        except Exception as e:\n            raise e\n\n    def _get_item(self, i) -> Dict[str, torch.Tensor]:\n        sources = self.list_data_dict[i]\n        if isinstance(i, int):\n            sources = [sources]\n        assert len(sources) == 1, \"Don't know why it is wrapped to a list\"  # FIXME\n        \n        if \"image\" in sources[0]:\n            image_file = self.list_data_dict[i][\"image\"]\n            if type(image_file) is list:\n                image = [self.process_image(f) for f in image_file]\n                # Handling multi images\n                # overwrite to process with simple pad \n                # if len(image_file) > 1:\n                #     image = [self.process_image(f, \"pad\") for f in image_file]\n                #     image = [[im[0], im[1], \"image\"] for im in image]\n            else:\n                image = [self.process_image(image_file)]\n            sources = preprocess_multimodal(copy.deepcopy([e[\"conversations\"] for e in sources]), self.data_args)\n\n        elif \"video\" in sources[0]:\n            video_file = self.list_data_dict[i][\"video\"]\n            video_folder = self.data_args.video_folder\n            if video_folder is not None:\n                video_file = os.path.join(video_folder, video_file)\n            else:\n                video_file = video_file\n            suffix = video_file.split(\".\")[-1]\n            if not os.path.exists(video_file):\n                print(\"File {} not exist!\".format(video_file))\n\n            try:\n                if \"shareVideoGPTV\" in video_file:\n                    frame_files = [os.path.join(video_file, f) for f in os.listdir(video_file) if os.path.isfile(os.path.join(video_file, f))]\n                    frame_files.sort()  # Ensure the frames are sorted if they are named sequentially\n\n                    # TODO: Hard CODE: Determine the indices for uniformly sampling 10 frames\n                    num_frames_to_sample = 10\n                    total_frames = len(frame_files)\n                    sampled_indices = np.linspace(0, total_frames - 1, num_frames_to_sample, dtype=int)\n\n                    # Read and store the sampled frames\n                    video = []\n                    for idx in sampled_indices:\n                        frame_path = frame_files[idx]\n                        try:\n                            with Image.open(frame_path) as img:\n                                frame = img.convert(\"RGB\")\n                                video.append(frame)\n                        except IOError:\n                            print(f\"Failed to read frame at path: {frame_path}\")\n                else:\n                    video = process_video_with_decord(video_file, self.data_args)\n\n                processor = self.data_args.image_processor\n                image = processor.preprocess(video, return_tensors=\"pt\")[\"pixel_values\"]\n                image = [(image, video[0].size, \"video\")]\n                sources = preprocess_multimodal(copy.deepcopy([e[\"conversations\"] for e in sources]), self.data_args)\n            except Exception as e:\n                print(f\"Error: {e}\")\n                print(f\"Failed to read video file: {video_file}\")\n                return self._get_item(i + 1)\n        else:\n            sources = copy.deepcopy([e[\"conversations\"] for e in sources])\n\n        has_image = (\"image\" in self.list_data_dict[i]) or (\"video\" in self.list_data_dict[i])\n        data_dict = preprocess(sources, self.tokenizer, has_image=has_image)\n\n        if \"prompt\" in data_dict:\n            prompt = data_dict[\"prompt\"]\n        else:\n            prompt = None\n\n        if isinstance(i, int):\n            data_dict = dict(input_ids=data_dict[\"input_ids\"][0], labels=data_dict[\"labels\"][0])\n\n        # image exist in the data\n        if \"image\" in self.list_data_dict[i]:\n            data_dict[\"image\"] = image\n        elif \"video\" in self.list_data_dict[i]:\n            data_dict[\"image\"] = image\n        elif self.data_args.is_multimodal:\n            # image does not exist in the data, but the model is multimodal\n            crop_size = self.data_args.image_processor.crop_size\n            data_dict[\"image\"] = [\n                (torch.zeros(1, 3, crop_size[\"height\"], crop_size[\"width\"]), (crop_size[\"width\"], crop_size[\"height\"]), \"text\"),\n            ]\n        # prompt exist in the data\n        if prompt is not None:\n            data_dict[\"prompt\"] = prompt\n\n        data_dict[\"id\"] = self.list_data_dict[i].get(\"id\", i)\n        try:\n            if \"id\" in self.list_data_dict[i]:\n                data_dict[\"id\"] = self.list_data_dict[i][\"id\"]\n            if \"extra_info\" in self.list_data_dict[i]:\n                data_dict[\"extra_info\"] = self.list_data_dict[i]['extra_info']\n            if \"gt_label\" in self.list_data_dict[i]:\n                data_dict[\"gt_label\"] = self.list_data_dict[i]['gt_label']\n        except:\n            pass\n        return data_dict\n\n\n@dataclass\nclass DataCollatorForSupervisedDataset(object):\n    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n\n    tokenizer: transformers.PreTrainedTokenizer\n\n    def pad_sequence(self, input_ids, batch_first, padding_value):\n        if self.tokenizer.padding_side == \"left\":\n            input_ids = [torch.flip(_input_ids, [0]) for _input_ids in input_ids]\n        input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=batch_first, padding_value=padding_value)\n        if self.tokenizer.padding_side == \"left\":\n            input_ids = torch.flip(input_ids, [1])\n        return input_ids\n\n    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n        # input_ids, labels, ids = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\", \"id\"))\n        input_ids = [_input_ids[: self.tokenizer.model_max_length] for _input_ids in input_ids]\n        labels = [_labels[: self.tokenizer.model_max_length] for _labels in labels]\n        if self.tokenizer.pad_token_id is None:\n            # self.tokenizer.pad_token_id = self.tokenizer.eos_token_id  # FIXME: this could only be triggered for llama3 model.\n            self.tokenizer.pad_token_id = 0 # This gets the best result. Don't know why.\n        input_ids = self.pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n        labels = self.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n        batch = dict(input_ids=input_ids, labels=labels.long() if labels.dtype == torch.int32 else labels, attention_mask=input_ids.ne(self.tokenizer.pad_token_id))\n        # batch = dict(input_ids=input_ids, labels=labels, attention_mask=input_ids.ne(self.tokenizer.pad_token_id), ids=ids)\n\n        if \"image\" in instances[0]:\n            images = [instance[\"image\"] for instance in instances]\n\n            batch[\"image_sizes\"] = [im[1] for im_list in images for im in im_list]\n            batch[\"modalities\"] = [im[2] for im_list in images for im in im_list]\n            images = [im[0] for im_list in images for im in im_list]\n\n            # if all(x is not None and x.shape == images[0].shape for x in images):\n                # Image: (N, P, C, H, W)\n                # Video: (N, F, C, H, W)\n            #     batch[\"images\"] = torch.stack(images)\n            # else:\n            batch[\"images\"] = images\n\n        if \"prompt\" in instances[0]:\n            batch[\"prompts\"] = [instance[\"prompt\"] for instance in instances]\n\n        try:\n            if \"id\" in instances[0]:\n                batch[\"id\"] = [instance[\"id\"] for instance in instances]\n            if \"extra_info\" in instances[0]:\n                batch[\"extra_info\"] = [instance[\"extra_info\"] for instance in instances]\n            if \"gt_label\" in instances[0]:\n                batch[\"gt_label\"] = [instance[\"gt_label\"] for instance in instances]\n        except:\n            pass\n        return batch\n\n\ndef make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer, data_args) -> Dict:\n    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n    train_dataset = LazySupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path, data_args=data_args)\n    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n    return dict(train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator)"}
{"type": "source_file", "path": "llava/model/language_model/modeling_llama.py", "content": "# coding=utf-8\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" PyTorch LLaMA model.\"\"\"\nimport math\nimport warnings\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.activations import ACT2FN\nfrom transformers.cache_utils import Cache, DynamicCache, StaticCache\nfrom transformers.modeling_outputs import (\n    BaseModelOutputWithPast,\n    CausalLMOutputWithPast,\n    QuestionAnsweringModelOutput,\n    SequenceClassifierOutputWithPast,\n)\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.pytorch_utils import ALL_LAYERNORM_LAYERS\nfrom transformers.utils import (\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    is_flash_attn_2_available,\n    is_flash_attn_greater_or_equal_2_10,\n    logging,\n    replace_return_docstrings,\n)\nfrom transformers.models.llama.configuration_llama import LlamaConfig\n\nif is_flash_attn_2_available():\n    from flash_attn import flash_attn_func, flash_attn_varlen_func\n    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa\n\n\nlogger = logging.get_logger(__name__)\n\n_CONFIG_FOR_DOC = \"LlamaConfig\"\n\n\ndef _get_unpad_data(attention_mask):\n    seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)\n    indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()\n    max_seqlen_in_batch = seqlens_in_batch.max().item()\n    cu_seqlens = F.pad(torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0))\n    return (\n        indices,\n        cu_seqlens,\n        max_seqlen_in_batch,\n    )\n\n\nclass LlamaRMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        LlamaRMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        input_dtype = hidden_states.dtype\n        hidden_states = hidden_states.to(torch.float32)\n        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * hidden_states.to(input_dtype)\n\n\nALL_LAYERNORM_LAYERS.append(LlamaRMSNorm)\n\n\nclass LlamaRotaryEmbedding(nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n        super().__init__()\n        self.scaling_factor = scaling_factor\n        self.dim = dim\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n        # For BC we register cos and sin cached\n        self.max_seq_len_cached = max_position_embeddings\n        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n        t = t / self.scaling_factor\n        freqs = torch.outer(t, self.inv_freq)\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.register_buffer(\"_cos_cached\", emb.cos().to(torch.get_default_dtype()), persistent=False)\n        self.register_buffer(\"_sin_cached\", emb.sin().to(torch.get_default_dtype()), persistent=False)\n\n    @property\n    def sin_cached(self):\n        logger.warning_once(\"The sin_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use \" \"the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\")\n        return self._sin_cached\n\n    @property\n    def cos_cached(self):\n        logger.warning_once(\"The cos_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use \" \"the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\")\n        return self._cos_cached\n\n    @torch.no_grad()\n    def forward(self, x, position_ids, seq_len=None):\n        if seq_len is not None:\n            logger.warning_once(\"The `seq_len` argument is deprecated and unused. It will be removed in v4.39.\")\n\n        # x: [bs, num_attention_heads, seq_len, head_size]\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n        position_ids_expanded = position_ids[:, None, :].float()\n        # Force float32 since bfloat16 loses precision on long contexts\n        # See https://github.com/huggingface/transformers/pull/29285\n        device_type = x.device.type\n        device_type = device_type if isinstance(device_type, str) else \"cpu\"\n        with torch.autocast(device_type=device_type, enabled=False):\n            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n            emb = torch.cat((freqs, freqs), dim=-1)\n            cos = emb.cos()\n            sin = emb.sin()\n        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n\n\nclass LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):\n    \"\"\"LlamaRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n\n    def forward(self, x, position_ids, seq_len=None):\n        # difference to the original RoPE: a scaling factor is aplied to the position ids\n        position_ids = position_ids.float() / self.scaling_factor\n        cos, sin = super().forward(x, position_ids, seq_len)\n        return cos, sin\n\n\nclass LlamaDynamicNTKScalingRotaryEmbedding(LlamaRotaryEmbedding):\n    \"\"\"LlamaRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n\n    def forward(self, x, position_ids, seq_len=None):\n        # difference to the original RoPE: inv_freq is recomputed when the sequence length > original length\n        seq_len = torch.max(position_ids) + 1\n        if seq_len > self.max_position_embeddings:\n            base = self.base * ((self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)) ** (self.dim / (self.dim - 2))\n            inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(x.device) / self.dim))\n            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: this may break with compilation\n\n        cos, sin = super().forward(x, position_ids, seq_len)\n        return cos, sin\n\n\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n\n    Args:\n        q (`torch.Tensor`): The query tensor.\n        k (`torch.Tensor`): The key tensor.\n        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n        sin (`torch.Tensor`): The sine part of the rotary embedding.\n        position_ids (`torch.Tensor`, *optional*):\n            Deprecated and unused.\n        unsqueeze_dim (`int`, *optional*, defaults to 1):\n            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n    Returns:\n        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n    \"\"\"\n    cos = cos.unsqueeze(unsqueeze_dim)\n    sin = sin.unsqueeze(unsqueeze_dim)\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\n\nclass LlamaMLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.intermediate_size = config.intermediate_size\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n        self.act_fn = ACT2FN[config.hidden_act]\n\n    def forward(self, x):\n        if self.config.pretraining_tp > 1:\n            slice = self.intermediate_size // self.config.pretraining_tp\n            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)\n            up_proj_slices = self.up_proj.weight.split(slice, dim=0)\n            down_proj_slices = self.down_proj.weight.split(slice, dim=1)\n\n            gate_proj = torch.cat([F.linear(x, gate_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1)\n            up_proj = torch.cat([F.linear(x, up_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1)\n\n            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)\n            down_proj = [F.linear(intermediate_states[i], down_proj_slices[i]) for i in range(self.config.pretraining_tp)]\n            down_proj = sum(down_proj)\n        else:\n            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n\n        return down_proj\n\n\ndef repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n    \"\"\"\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n\n\nclass LlamaAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        if layer_idx is None:\n            logger.warning_once(\n                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n                \"when creating this class.\"\n            )\n\n        self.attention_dropout = config.attention_dropout\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\" f\" and `num_heads`: {self.num_heads}).\")\n\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)\n        self._init_rope()\n\n    def _init_rope(self):\n        if self.config.rope_scaling is None:\n            self.rotary_emb = LlamaRotaryEmbedding(\n                self.head_dim,\n                max_position_embeddings=self.max_position_embeddings,\n                base=self.rope_theta,\n            )\n        else:\n            scaling_type = self.config.rope_scaling[\"type\"]\n            scaling_factor = self.config.rope_scaling[\"factor\"]\n            if scaling_type == \"linear\":\n                self.rotary_emb = LlamaLinearScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    scaling_factor=scaling_factor,\n                    base=self.rope_theta,\n                )\n            elif scaling_type == \"dynamic\":\n                self.rotary_emb = LlamaDynamicNTKScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    scaling_factor=scaling_factor,\n                    base=self.rope_theta,\n                )\n            else:\n                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n\n        if self.config.pretraining_tp > 1:\n            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n            query_slices = self.q_proj.weight.split((self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0)\n            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n\n            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n            query_states = torch.cat(query_states, dim=-1)\n\n            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n            key_states = torch.cat(key_states, dim=-1)\n\n            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n            value_states = torch.cat(value_states, dim=-1)\n\n        else:\n            query_states = self.q_proj(hidden_states)\n            key_states = self.k_proj(hidden_states)\n            value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        past_key_value = getattr(self, \"past_key_value\", past_key_value)\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; position_ids needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n        if attention_mask is not None:  # no matter the length, we just slice it\n            causal_mask = attention_mask\n            if cache_position is not None:\n                causal_mask = attention_mask[:, :, cache_position, : key_states.shape[-2]]\n            attn_weights = attn_weights + causal_mask\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\" f\" {attn_output.size()}\")\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        if self.config.pretraining_tp > 1:\n            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n        else:\n            attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\nclass LlamaRingFlashAttention2(LlamaAttention):\n    \"\"\"\n    Llama flash attention module. This module inherits from `LlamaAttention` as the weights of the module stays\n    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n    flash attention and deal with padding tokens in case the input contains any of them.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        output_attentions = False\n\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        # Flash attention requires the input to have the shape\n        # batch_size x seq_length x head_dim x hidden_dim\n        # therefore we just need to keep the original shape\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        past_key_value = getattr(self, \"past_key_value\", past_key_value)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; position_ids needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n        # to be able to avoid many of these transpose/reshape/view.\n        query_states = query_states.transpose(1, 2)\n        key_states = key_states.transpose(1, 2)\n        value_states = value_states.transpose(1, 2)\n\n        dropout_rate = self.attention_dropout if self.training else 0.0\n\n        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n        # therefore the input hidden states gets silently casted in float32. Hence, we need\n        # cast them back in the correct dtype just to be sure everything works as expected.\n        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n        # in fp32. (LlamaRMSNorm handles it correctly)\n\n        input_dtype = query_states.dtype\n        if input_dtype == torch.float32:\n            if torch.is_autocast_enabled():\n                target_dtype = torch.get_autocast_gpu_dtype()\n            # Handle the case where the model is quantized\n            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                target_dtype = self.config._pre_quantization_dtype\n            else:\n                target_dtype = self.q_proj.weight.dtype\n\n            logger.warning_once(\n                f\"The input hidden states seems to be silently casted in float32, this might be related to\" f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\" f\" {target_dtype}.\"\n            )\n\n            query_states = query_states.to(target_dtype)\n            key_states = key_states.to(target_dtype)\n            value_states = value_states.to(target_dtype)\n\n        attn_output = self._flash_attention_forward(query_states, key_states, value_states, attention_mask, q_len, dropout=dropout_rate)\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n    def _flash_attention_forward(self, query_states, key_states, value_states, attention_mask, query_length, dropout=0.0, softmax_scale=None):\n        \"\"\"\n        Calls the forward method of Flash Attention - if the input hidden states contain at least one padding token\n        first unpad the input, then computes the attention scores and pad the final attention scores.\n\n        Args:\n            query_states (`torch.Tensor`):\n                Input query states to be passed to Flash Attention API\n            key_states (`torch.Tensor`):\n                Input key states to be passed to Flash Attention API\n            value_states (`torch.Tensor`):\n                Input value states to be passed to Flash Attention API\n            attention_mask (`torch.Tensor`):\n                The padding mask - corresponds to a tensor of size `(batch_size, seq_len)` where 0 stands for the\n                position of padding tokens and 1 for the position of non-padding tokens.\n            dropout (`int`, *optional*):\n                Attention dropout\n            softmax_scale (`float`, *optional*):\n                The scaling of QK^T before applying softmax. Default to 1 / sqrt(head_dim)\n        \"\"\"\n        if not self._flash_attn_uses_top_left_mask:\n            causal = self.is_causal\n        else:\n            # TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1. For details, please see the comment in LlamaFlashAttention2 __init__.\n            causal = self.is_causal and query_length != 1\n\n        # Contains at least one padding token in the sequence\n        if attention_mask is not None:\n            batch_size = query_states.shape[0]\n            query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = self._upad_input(query_states, key_states, value_states, attention_mask, query_length)\n\n            cu_seqlens_q, cu_seqlens_k = cu_seq_lens\n            max_seqlen_in_batch_q, max_seqlen_in_batch_k = max_seq_lens\n\n            attn_output_unpad = zigzag_ring_flash_attn_varlen_func(\n                query_states,\n                key_states,\n                value_states,\n                cu_seqlens_q=cu_seqlens_q,\n                cu_seqlens_k=cu_seqlens_k,\n                max_seqlen_q=max_seqlen_in_batch_q,\n                max_seqlen_k=max_seqlen_in_batch_k,\n                dropout_p=dropout,\n                softmax_scale=softmax_scale,\n                causal=causal,\n            )\n\n            attn_output = pad_input(attn_output_unpad, indices_q, batch_size, query_length)\n        else:\n            # pack qkv\n            # query_states: (batch_size, seqlen, nheads, headdim)\n            # qkv: (batch_size, seqlen, 3, nheads, headdim)\n            qkv = torch.stack([query_states, key_states, value_states], dim=2)\n            attn_output = zigzag_ring_flash_attn_qkvpacked_func(qkv, dropout, softmax_scale, causal=causal)\n\n        return attn_output\n\n    def _upad_input(self, query_layer, key_layer, value_layer, attention_mask, query_length):\n        indices_k, cu_seqlens_k, max_seqlen_in_batch_k = _get_unpad_data(attention_mask)\n        batch_size, kv_seq_len, num_key_value_heads, head_dim = key_layer.shape\n\n        key_layer = index_first_axis(key_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k)\n        value_layer = index_first_axis(value_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k)\n        if query_length == kv_seq_len:\n            query_layer = index_first_axis(query_layer.reshape(batch_size * kv_seq_len, self.num_heads, head_dim), indices_k)\n            cu_seqlens_q = cu_seqlens_k\n            max_seqlen_in_batch_q = max_seqlen_in_batch_k\n            indices_q = indices_k\n        elif query_length == 1:\n            max_seqlen_in_batch_q = 1\n            cu_seqlens_q = torch.arange(batch_size + 1, dtype=torch.int32, device=query_layer.device)  # There is a memcpy here, that is very bad.\n            indices_q = cu_seqlens_q[:-1]\n            query_layer = query_layer.squeeze(1)\n        else:\n            # The -q_len: slice assumes left padding.\n            attention_mask = attention_mask[:, -query_length:]\n            query_layer, indices_q, cu_seqlens_q, max_seqlen_in_batch_q = unpad_input(query_layer, attention_mask)\n\n        return (\n            query_layer,\n            key_layer,\n            value_layer,\n            indices_q,\n            (cu_seqlens_q, cu_seqlens_k),\n            (max_seqlen_in_batch_q, max_seqlen_in_batch_k),\n        )\n\n\nclass LlamaFlashAttention2(LlamaAttention):\n    \"\"\"\n    Llama flash attention module. This module inherits from `LlamaAttention` as the weights of the module stays\n    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n    flash attention and deal with padding tokens in case the input contains any of them.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        output_attentions = False\n\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        # Flash attention requires the input to have the shape\n        # batch_size x seq_length x head_dim x hidden_dim\n        # therefore we just need to keep the original shape\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        past_key_value = getattr(self, \"past_key_value\", past_key_value)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; position_ids needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n        # to be able to avoid many of these transpose/reshape/view.\n        query_states = query_states.transpose(1, 2)\n        key_states = key_states.transpose(1, 2)\n        value_states = value_states.transpose(1, 2)\n\n        dropout_rate = self.attention_dropout if self.training else 0.0\n\n        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n        # therefore the input hidden states gets silently casted in float32. Hence, we need\n        # cast them back in the correct dtype just to be sure everything works as expected.\n        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n        # in fp32. (LlamaRMSNorm handles it correctly)\n\n        input_dtype = query_states.dtype\n        if input_dtype == torch.float32:\n            if torch.is_autocast_enabled():\n                target_dtype = torch.get_autocast_gpu_dtype()\n            # Handle the case where the model is quantized\n            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                target_dtype = self.config._pre_quantization_dtype\n            else:\n                target_dtype = self.q_proj.weight.dtype\n\n            logger.warning_once(\n                f\"The input hidden states seems to be silently casted in float32, this might be related to\" f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\" f\" {target_dtype}.\"\n            )\n\n            query_states = query_states.to(target_dtype)\n            key_states = key_states.to(target_dtype)\n            value_states = value_states.to(target_dtype)\n\n        attn_output = self._flash_attention_forward(query_states, key_states, value_states, attention_mask, q_len, dropout=dropout_rate)\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n    def _flash_attention_forward(self, query_states, key_states, value_states, attention_mask, query_length, dropout=0.0, softmax_scale=None):\n        \"\"\"\n        Calls the forward method of Flash Attention - if the input hidden states contain at least one padding token\n        first unpad the input, then computes the attention scores and pad the final attention scores.\n\n        Args:\n            query_states (`torch.Tensor`):\n                Input query states to be passed to Flash Attention API\n            key_states (`torch.Tensor`):\n                Input key states to be passed to Flash Attention API\n            value_states (`torch.Tensor`):\n                Input value states to be passed to Flash Attention API\n            attention_mask (`torch.Tensor`):\n                The padding mask - corresponds to a tensor of size `(batch_size, seq_len)` where 0 stands for the\n                position of padding tokens and 1 for the position of non-padding tokens.\n            dropout (`int`, *optional*):\n                Attention dropout\n            softmax_scale (`float`, *optional*):\n                The scaling of QK^T before applying softmax. Default to 1 / sqrt(head_dim)\n        \"\"\"\n        if not self._flash_attn_uses_top_left_mask:\n            causal = self.is_causal\n        else:\n            # TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1. For details, please see the comment in LlamaFlashAttention2 __init__.\n            causal = self.is_causal and query_length != 1\n\n        # Contains at least one padding token in the sequence\n        if attention_mask is not None:\n            batch_size = query_states.shape[0]\n            query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = self._upad_input(query_states, key_states, value_states, attention_mask, query_length)\n\n            cu_seqlens_q, cu_seqlens_k = cu_seq_lens\n            max_seqlen_in_batch_q, max_seqlen_in_batch_k = max_seq_lens\n\n            attn_output_unpad = flash_attn_varlen_func(\n                query_states,\n                key_states,\n                value_states,\n                cu_seqlens_q=cu_seqlens_q,\n                cu_seqlens_k=cu_seqlens_k,\n                max_seqlen_q=max_seqlen_in_batch_q,\n                max_seqlen_k=max_seqlen_in_batch_k,\n                dropout_p=dropout,\n                softmax_scale=softmax_scale,\n                causal=causal,\n            )\n\n            attn_output = pad_input(attn_output_unpad, indices_q, batch_size, query_length)\n        else:\n            attn_output = flash_attn_func(query_states, key_states, value_states, dropout, softmax_scale=softmax_scale, causal=causal)\n\n        return attn_output\n\n    def _upad_input(self, query_layer, key_layer, value_layer, attention_mask, query_length):\n        indices_k, cu_seqlens_k, max_seqlen_in_batch_k = _get_unpad_data(attention_mask)\n        batch_size, kv_seq_len, num_key_value_heads, head_dim = key_layer.shape\n\n        key_layer = index_first_axis(key_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k)\n        value_layer = index_first_axis(value_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k)\n        if query_length == kv_seq_len:\n            query_layer = index_first_axis(query_layer.reshape(batch_size * kv_seq_len, self.num_heads, head_dim), indices_k)\n            cu_seqlens_q = cu_seqlens_k\n            max_seqlen_in_batch_q = max_seqlen_in_batch_k\n            indices_q = indices_k\n        elif query_length == 1:\n            max_seqlen_in_batch_q = 1\n            cu_seqlens_q = torch.arange(batch_size + 1, dtype=torch.int32, device=query_layer.device)  # There is a memcpy here, that is very bad.\n            indices_q = cu_seqlens_q[:-1]\n            query_layer = query_layer.squeeze(1)\n        else:\n            # The -q_len: slice assumes left padding.\n            attention_mask = attention_mask[:, -query_length:]\n            query_layer, indices_q, cu_seqlens_q, max_seqlen_in_batch_q = unpad_input(query_layer, attention_mask)\n\n        return (\n            query_layer,\n            key_layer,\n            value_layer,\n            indices_q,\n            (cu_seqlens_q, cu_seqlens_k),\n            (max_seqlen_in_batch_q, max_seqlen_in_batch_k),\n        )\n\n\nclass LlamaSdpaAttention(LlamaAttention):\n    \"\"\"\n    Llama attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n    `LlamaAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n    SDPA API.\n    \"\"\"\n\n    # Adapted from LlamaAttention.forward\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        if output_attentions:\n            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n            logger.warning_once(\n                \"LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n            )\n            return super().forward(\n                hidden_states=hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_value=past_key_value,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n                cache_position=cache_position,\n            )\n\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        # In case static cache is used, it is an instance attribute.\n        past_key_value = getattr(self, \"past_key_value\", past_key_value)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; position_ids needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        causal_mask = attention_mask\n        if attention_mask is not None and cache_position is not None:\n            causal_mask = causal_mask[:, :, cache_position, : key_states.shape[-2]]\n\n        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n        if query_states.device.type == \"cuda\" and causal_mask is not None:\n            query_states = query_states.contiguous()\n            key_states = key_states.contiguous()\n            value_states = value_states.contiguous()\n\n        attn_output = torch.nn.functional.scaled_dot_product_attention(\n            query_states,\n            key_states,\n            value_states,\n            attn_mask=causal_mask,\n            dropout_p=self.attention_dropout if self.training else 0.0,\n        )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        attn_output = attn_output.view(bsz, q_len, self.hidden_size)\n\n        attn_output = self.o_proj(attn_output)\n\n        return attn_output, None, past_key_value\n\n\ntry:\n    from ring_flash_attn import zigzag_ring_flash_attn_qkvpacked_func, zigzag_ring_flash_attn_varlen_func\nexcept ImportError:\n    print(\"Please install the ring-flash-attn package\")\n\nLLAMA_ATTENTION_CLASSES = {\n    \"eager\": LlamaAttention,\n    \"flash_attention_2\": LlamaFlashAttention2,\n    \"ring_flash_attention_2\": LlamaRingFlashAttention2,\n    \"sdpa\": LlamaSdpaAttention,\n}\n\n\nclass LlamaDecoderLayer(nn.Module):\n    def __init__(self, config: LlamaConfig, layer_idx: int):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n\n        self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n\n        self.mlp = LlamaMLP(config)\n        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n            attention_mask (`torch.FloatTensor`, *optional*):\n                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n                query_sequence_length, key_sequence_length)` if default attention is used.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            use_cache (`bool`, *optional*):\n                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                (see `past_key_values`).\n            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n        \"\"\"\n        if \"padding_mask\" in kwargs:\n            warnings.warn(\"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\")\n\n        residual = hidden_states\n\n        hidden_states = self.input_layernorm(hidden_states)\n\n        # Self Attention\n        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_value=past_key_value,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n            cache_position=cache_position,\n            **kwargs,\n        )\n        hidden_states = residual + hidden_states\n\n        # Fully Connected\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (self_attn_weights,)\n\n        if use_cache:\n            outputs += (present_key_value,)\n\n        return outputs\n\n\nLLAMA_START_DOCSTRING = r\"\"\"\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Parameters:\n        config ([`LlamaConfig`]):\n            Model configuration class with all the parameters of the model. Initializing with a config file does not\n            load the weights associated with the model, only the configuration. Check out the\n            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaPreTrainedModel(PreTrainedModel):\n    config_class = LlamaConfig\n    base_model_prefix = \"model\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"LlamaDecoderLayer\"]\n    _skip_keys_device_placement = [\"past_key_values\", \"causal_mask\"]\n    _supports_flash_attn_2 = True\n    _supports_sdpa = True\n    _supports_cache_class = True\n\n    def _init_weights(self, module):\n        std = self.config.initializer_range\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n\n    def _setup_cache(self, cache_cls, max_batch_size, max_cache_len: Optional[int] = None):\n        if self.config._attn_implementation == \"flash_attention_2\" and cache_cls == StaticCache:\n            raise ValueError(\"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \" \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\")\n\n        if max_cache_len > self.model.causal_mask.shape[-1] or self.device != self.model.causal_mask.device:\n            causal_mask = torch.full((max_cache_len, max_cache_len), fill_value=True, device=self.device, dtype=torch.bool)\n            self.register_buffer(\"causal_mask\", torch.triu(causal_mask, diagonal=1), persistent=False)\n\n        for layer in self.model.layers:\n            device = layer.input_layernorm.weight.device\n            if hasattr(self.config, \"_pre_quantization_dtype\"):\n                dtype = self.config._pre_quantization_dtype\n            else:\n                dtype = layer.self_attn.o_proj.weight.dtype\n            layer.self_attn.past_key_value = cache_cls(self.config, max_batch_size, max_cache_len, device=device, dtype=dtype)\n\n    def _reset_cache(self):\n        for layer in self.model.layers:\n            layer.self_attn.past_key_value = None\n\n\nLLAMA_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n            it.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n            `past_key_values`).\n\n            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n            information on the default strategy.\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n            config.n_positions - 1]`.\n\n            [What are position IDs?](../glossary#position-ids)\n        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n\n            Two formats are allowed:\n            - a [`~cache_utils.Cache`] instance;\n            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n            cache format.\n\n            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n            legacy cache format will be returned.\n\n            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n            of shape `(batch_size, sequence_length)`.\n        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n            model's internal embedding lookup matrix.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaModel(LlamaPreTrainedModel):\n    \"\"\"\n    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n\n    Args:\n        config: LlamaConfig\n    \"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n        self.layers = nn.ModuleList([LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)])\n        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.gradient_checkpointing = False\n\n        # Register a causal mask to separate causal and padding mask creation. Merging happens in the attention class.\n        # NOTE: This is not friendly with TorchScript, ONNX, ExportedProgram serialization for very large `max_position_embeddings`.\n        causal_mask = torch.full((config.max_position_embeddings, config.max_position_embeddings), fill_value=True, dtype=torch.bool)\n        self.register_buffer(\"causal_mask\", torch.triu(causal_mask, diagonal=1), persistent=False)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Union[Tuple, BaseModelOutputWithPast]:\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if (input_ids is None) ^ (inputs_embeds is not None):\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\")\n\n        if self.gradient_checkpointing and self.training and use_cache:\n            logger.warning_once(\"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\")\n            use_cache = False\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)\n\n        past_seen_tokens = 0\n        if use_cache:  # kept for BC (cache positions)\n            if not isinstance(past_key_values, StaticCache):\n                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n                past_seen_tokens = past_key_values.get_seq_length()\n\n        if cache_position is None:\n            if isinstance(past_key_values, StaticCache):\n                raise ValueError(\"cache_position is a required argument when using StaticCache.\")\n            cache_position = torch.arange(past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device)\n\n        if position_ids is None:\n            position_ids = cache_position.unsqueeze(0)\n\n        causal_mask = self._update_causal_mask(attention_mask, inputs_embeds)\n\n        # embed positions\n        hidden_states = inputs_embeds\n\n        # decoder layers\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n        next_decoder_cache = None\n\n        for decoder_layer in self.layers:\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            if self.gradient_checkpointing and self.training:\n                layer_outputs = self._gradient_checkpointing_func(\n                    decoder_layer.__call__,\n                    hidden_states,\n                    causal_mask,\n                    position_ids,\n                    past_key_values,\n                    output_attentions,\n                    use_cache,\n                    cache_position,\n                )\n            else:\n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    attention_mask=causal_mask,\n                    position_ids=position_ids,\n                    past_key_value=past_key_values,\n                    output_attentions=output_attentions,\n                    use_cache=use_cache,\n                    cache_position=cache_position,\n                )\n\n            hidden_states = layer_outputs[0]\n\n            if use_cache:\n                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n\n        hidden_states = self.norm(hidden_states)\n\n        # add hidden states from the last decoder layer\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        next_cache = None\n        if use_cache:\n            next_cache = next_decoder_cache.to_legacy_cache() if isinstance(next_decoder_cache, Cache) else next_decoder_cache\n        if not return_dict:\n            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n        return BaseModelOutputWithPast(\n            last_hidden_state=hidden_states,\n            past_key_values=next_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n        )\n\n    # TODO: As of torch==2.2.0, the `attention_mask` passed to the model in `generate` is 2D and of dynamic length even when the static\n    # KV cache is used. This is an issue for torch.compile which then recaptures cudagraphs at each decode steps due to the dynamic shapes.\n    # (`recording cudagraph tree for symint key 13`, etc.), which is VERY slow. A workaround is `@torch.compiler.disable`, but this prevents using\n    # `fullgraph=True`. See more context in https://github.com/huggingface/transformers/pull/29114\n    def _update_causal_mask(self, attention_mask, input_tensor):\n        if self.config._attn_implementation == \"flash_attention_2\":\n            if attention_mask is not None and 0.0 in attention_mask:\n                return attention_mask\n            return None\n\n        batch_size, seq_length = input_tensor.shape[:2]\n        dtype = input_tensor.dtype\n        device = input_tensor.device\n\n        # support going beyond cached `max_position_embedding`\n        if seq_length > self.causal_mask.shape[-1]:\n            causal_mask = torch.full((2 * self.causal_mask.shape[-1], 2 * self.causal_mask.shape[-1]), fill_value=1)\n            self.register_buffer(\"causal_mask\", torch.triu(causal_mask, diagonal=1), persistent=False)\n\n        # We use the current dtype to avoid any overflows\n        min_dtype = torch.finfo(dtype).min\n        causal_mask = self.causal_mask[None, None, :, :].repeat(batch_size, 1, 1, 1).to(dtype) * min_dtype\n\n        causal_mask = causal_mask.to(dtype=dtype, device=device)\n        if attention_mask is not None and attention_mask.dim() == 2:\n            mask_length = attention_mask.shape[-1]\n            padding_mask = causal_mask[..., :mask_length].eq(0.0) * attention_mask[:, None, None, :].eq(0.0)\n            causal_mask[..., :mask_length] = causal_mask[..., :mask_length].masked_fill(padding_mask, min_dtype)\n\n        if self.config._attn_implementation == \"sdpa\" and attention_mask is not None and attention_mask.device.type == \"cuda\":\n            # TODO: For dynamo, rather use a check on fullgraph=True once this is possible (https://github.com/pytorch/pytorch/pull/120400).\n            is_tracing = torch.jit.is_tracing() or isinstance(input_tensor, torch.fx.Proxy) or (hasattr(torch, \"_dynamo\") and torch._dynamo.is_compiling())\n            if not is_tracing and torch.any(attention_mask != 1):\n                # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n                # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n                # Details: https://github.com/pytorch/pytorch/issues/110213\n                causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n\n        return causal_mask\n\n\nclass LlamaForCausalLM(LlamaPreTrainedModel):\n    _tied_weights_keys = [\"lm_head.weight\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = LlamaModel(config)\n        self.vocab_size = config.vocab_size\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def set_decoder(self, decoder):\n        self.model = decoder\n\n    def get_decoder(self):\n        return self.model\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n        r\"\"\"\n        Args:\n            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n\n        Returns:\n\n        Example:\n\n        ```python\n        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n\n        >>> model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\n        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n        >>> # Generate\n        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n        ```\"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            cache_position=cache_position,\n        )\n\n        hidden_states = outputs[0]\n        if self.config.pretraining_tp > 1:\n            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n            logits = torch.cat(logits, dim=-1)\n        else:\n            logits = self.lm_head(hidden_states)\n        logits = logits.float()\n\n        loss = None\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss()\n            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n            shift_labels = shift_labels.view(-1)\n            # Enable model parallelism\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return CausalLMOutputWithPast(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs):\n        past_length = 0\n        if past_key_values is not None:\n            if isinstance(past_key_values, Cache):\n                cache_length = past_key_values.get_seq_length()\n                past_length = past_key_values.seen_tokens\n                max_cache_length = past_key_values.get_max_length()\n            else:\n                cache_length = past_length = past_key_values[0][0].shape[2]\n                max_cache_length = None\n\n            # Keep only the unprocessed tokens:\n            # 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where\n            # some of the inputs are exclusively passed as part of the cache (e.g. when passing input_embeds as\n            # input)\n            if attention_mask is not None and attention_mask.shape[1] > input_ids.shape[1]:\n                input_ids = input_ids[:, -(attention_mask.shape[1] - past_length) :]\n            # 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard\n            # input_ids based on the past_length.\n            elif past_length < input_ids.shape[1]:\n                input_ids = input_ids[:, past_length:]\n            # 3 - Otherwise (past_length >= input_ids.shape[1]), let's assume input_ids only has unprocessed tokens.\n\n            # If we are about to go beyond the maximum cache length, we need to crop the input attention mask.\n            if max_cache_length is not None and attention_mask is not None and cache_length + input_ids.shape[1] > max_cache_length:\n                attention_mask = attention_mask[:, -max_cache_length:]\n\n        position_ids = kwargs.get(\"position_ids\", None)\n        if attention_mask is not None and position_ids is None:\n            # create position_ids on the fly for batch generation\n            position_ids = attention_mask.long().cumsum(-1) - 1\n            position_ids.masked_fill_(attention_mask == 0, 1)\n            if past_key_values:\n                position_ids = position_ids[:, -input_ids.shape[1] :]\n\n        if self.generation_config.cache_implementation == \"static\":\n            # generation with static cache\n            cache_position = kwargs.get(\"cache_position\", None)\n            if cache_position is None:\n                past_length = 0\n            else:\n                past_length = cache_position[-1] + 1\n            input_ids = input_ids[:, past_length:]\n            position_ids = position_ids[:, past_length:]\n\n        # TODO @gante we should only keep a `cache_position` in generate, and do +=1.\n        # same goes for position ids. Could also help with continued generation.\n        input_length = position_ids.shape[-1] if position_ids is not None else input_ids.shape[-1]\n        cache_position = torch.arange(past_length, past_length + input_length, device=input_ids.device)\n        position_ids = position_ids.contiguous() if position_ids is not None else None\n\n        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n        if inputs_embeds is not None and past_key_values is None:\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            # The `contiguous()` here is necessary to have a static stride during decoding. torchdynamo otherwise\n            # recompiles graphs as the stride of the inputs is a guard. Ref: https://github.com/huggingface/transformers/pull/29114\n            # TODO: use `next_tokens` directly instead.\n            model_inputs = {\"input_ids\": input_ids.contiguous()}\n\n        model_inputs.update(\n            {\n                \"position_ids\": position_ids,\n                \"cache_position\": cache_position,\n                \"past_key_values\": past_key_values,\n                \"use_cache\": kwargs.get(\"use_cache\"),\n                \"attention_mask\": attention_mask,\n            }\n        )\n        return model_inputs\n\n    @staticmethod\n    def _reorder_cache(past_key_values, beam_idx):\n        reordered_past = ()\n        for layer_past in past_key_values:\n            reordered_past += (tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),)\n        return reordered_past\n\n\n@add_start_docstrings(\n    \"\"\"\n    The LLaMa Model transformer with a sequence classification head on top (linear layer).\n\n    [`LlamaForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n    (e.g. GPT-2) do.\n\n    Since it does classification on the last token, it requires to know the position of the last token. If a\n    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n    each row of the batch).\n    \"\"\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaForSequenceClassification(LlamaPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.model = LlamaModel(config)\n        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        transformer_outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        hidden_states = transformer_outputs[0]\n        logits = self.score(hidden_states)\n\n        if input_ids is not None:\n            batch_size = input_ids.shape[0]\n        else:\n            batch_size = inputs_embeds.shape[0]\n\n        if self.config.pad_token_id is None and batch_size != 1:\n            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n        if self.config.pad_token_id is None:\n            sequence_lengths = -1\n        else:\n            if input_ids is not None:\n                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n                sequence_lengths = sequence_lengths.to(logits.device)\n            else:\n                sequence_lengths = -1\n\n        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n\n        loss = None\n        if labels is not None:\n            labels = labels.to(logits.device)\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = \"regression\"\n                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\"\n                else:\n                    self.config.problem_type = \"multi_label_classification\"\n\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss()\n                if self.num_labels == 1:\n                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n                else:\n                    loss = loss_fct(pooled_logits, labels)\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n            elif self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(pooled_logits, labels)\n        if not return_dict:\n            output = (pooled_logits,) + transformer_outputs[1:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutputWithPast(\n            loss=loss,\n            logits=pooled_logits,\n            past_key_values=transformer_outputs.past_key_values,\n            hidden_states=transformer_outputs.hidden_states,\n            attentions=transformer_outputs.attentions,\n        )\n\n\n@add_start_docstrings(\n    \"\"\"\nThe Llama Model transformer with a span classification head on top for extractive question-answering tasks like\nSQuAD (a linear layer on top of the hidden-states output to compute `span start logits` and `span end logits`).\n    \"\"\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaForQuestionAnswering(LlamaPreTrainedModel):\n    base_model_prefix = \"transformer\"\n\n    # Copied from transformers.models.bloom.modeling_bloom.BloomForQuestionAnswering.__init__ with Bloom->Llama\n    def __init__(self, config):\n        super().__init__(config)\n        self.transformer = LlamaModel(config)\n        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.transformer.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.transformer.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        start_positions: Optional[torch.LongTensor] = None,\n        end_positions: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n        r\"\"\"\n        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n            are not taken into account for computing the loss.\n        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n            are not taken into account for computing the loss.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.transformer(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1).contiguous()\n        end_logits = end_logits.squeeze(-1).contiguous()\n\n        total_loss = None\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1).to(start_logits.device)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1).to(end_logits.device)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions = start_positions.clamp(0, ignored_index)\n            end_positions = end_positions.clamp(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n\n        if not return_dict:\n            output = (start_logits, end_logits) + outputs[2:]\n            return ((total_loss,) + output) if total_loss is not None else output\n\n        return QuestionAnsweringModelOutput(\n            loss=total_loss,\n            start_logits=start_logits,\n            end_logits=end_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n"}
{"type": "source_file", "path": "llava/model/language_model/llava_llama.py", "content": "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\n\nfrom transformers import AutoConfig, AutoModelForCausalLM, LlamaConfig\n\nfrom torch.nn import CrossEntropyLoss\n\n\n# , LlamaModel, LlamaForCausalLM, GenerationConfig\n# from .modeling_llama import LlamaModel, LlamaForCausalLM\nfrom transformers import LlamaModel, LlamaForCausalLM\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nfrom transformers.generation.utils import GenerateOutput\n\nfrom llava.model.llava_arch import LlavaMetaModel, LlavaMetaForCausalLM\n\n\nclass LlavaConfig(LlamaConfig):\n    model_type = \"llava_llama\"\n    temperature: float = 0.0  # reset to 0.0, previously 0.9 for Vicuna\n    max_new_tokens: int = 1024\n    do_sample: bool = False\n    top_p: Optional[float] = None\n    # rope_scaling: Optional[dict] = {}\n\n\nclass LlavaLlamaModel(LlavaMetaModel, LlamaModel):\n    config_class = LlavaConfig\n\n    def __init__(self, config: LlamaConfig):\n        super(LlavaLlamaModel, self).__init__(config)\n\n\nclass LlavaLlamaForCausalLM(LlamaForCausalLM, LlavaMetaForCausalLM):\n    config_class = LlavaConfig\n\n    def __init__(self, config):\n        LlamaForCausalLM.__init__(self, config)\n\n        # configure default generation settings\n        config.model_type = \"llava_llama\"\n        # config.rope_scaling = None\n\n        self.model = LlavaLlamaModel(config)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.model\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        images: Optional[torch.FloatTensor] = None,\n        image_sizes: Optional[List[List[int]]] = None,\n        return_dict: Optional[bool] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        dpo_forward: Optional[bool] = None,\n        cache_position=None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n\n        if inputs_embeds is None:\n            (input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels) = self.prepare_inputs_labels_for_multimodal(input_ids, position_ids, attention_mask, past_key_values, labels, images, modalities, image_sizes)\n\n        if dpo_forward:\n            outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n            hidden_states = outputs[0]\n            logits = self.lm_head(hidden_states)\n            return logits, labels\n\n        else:\n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                labels=labels,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs: Optional[torch.Tensor] = None,\n        images: Optional[torch.Tensor] = None,\n        image_sizes: Optional[torch.Tensor] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        **kwargs,\n    ) -> Union[GenerateOutput, torch.LongTensor]:\n        modalities = kwargs.pop(\"modalities\", None) if \"modalities\" in kwargs and modalities is None else modalities\n        position_ids = kwargs.pop(\"position_ids\", None)\n        attention_mask = kwargs.pop(\"attention_mask\", None)\n        if \"inputs_embeds\" in kwargs:\n            raise NotImplementedError(\"`inputs_embeds` is not supported\")\n\n        if images is not None:\n            (inputs, position_ids, attention_mask, _, inputs_embeds, _) = self.prepare_inputs_labels_for_multimodal(inputs, position_ids, attention_mask, None, None, images, modalities, image_sizes=image_sizes)\n        else:\n            inputs_embeds = self.get_model().embed_tokens(inputs)\n\n        return super().generate(position_ids=position_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, **kwargs)\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n        images = kwargs.pop(\"images\", None)\n        image_sizes = kwargs.pop(\"image_sizes\", None)\n        inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs)\n        if images is not None:\n            inputs[\"images\"] = images\n        if image_sizes is not None:\n            inputs[\"image_sizes\"] = image_sizes\n        return inputs\n\n\nAutoConfig.register(\"llava_llama\", LlavaConfig)\nAutoModelForCausalLM.register(LlavaConfig, LlavaLlamaForCausalLM)\n"}
{"type": "source_file", "path": "llava/model/apply_delta.py", "content": "\"\"\"\nUsage:\npython3 -m fastchat.model.apply_delta --base ~/model_weights/llama-7b --target ~/model_weights/vicuna-7b --delta lmsys/vicuna-7b-delta\n\"\"\"\n\nimport argparse\n\nimport torch\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom llava import LlavaLlamaForCausalLM\n\n\ndef apply_delta(base_model_path, target_model_path, delta_path):\n    print(\"Loading base model\")\n    base = AutoModelForCausalLM.from_pretrained(base_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n\n    print(\"Loading delta\")\n    delta = LlavaLlamaForCausalLM.from_pretrained(delta_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n    delta_tokenizer = AutoTokenizer.from_pretrained(delta_path)\n\n    print(\"Applying delta\")\n    for name, param in tqdm(delta.state_dict().items(), desc=\"Applying delta\"):\n        if name not in base.state_dict():\n            assert name in [\"model.mm_projector.weight\", \"model.mm_projector.bias\"], f\"{name} not in base model\"\n            continue\n        if param.data.shape == base.state_dict()[name].shape:\n            param.data += base.state_dict()[name]\n        else:\n            assert name in [\"model.embed_tokens.weight\", \"lm_head.weight\"], f\"{name} dimension mismatch: {param.data.shape} vs {base.state_dict()[name].shape}\"\n            bparam = base.state_dict()[name]\n            param.data[: bparam.shape[0], : bparam.shape[1]] += bparam\n\n    print(\"Saving target model\")\n    delta.save_pretrained(target_model_path)\n    delta_tokenizer.save_pretrained(target_model_path)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--base-model-path\", type=str, required=True)\n    parser.add_argument(\"--target-model-path\", type=str, required=True)\n    parser.add_argument(\"--delta-path\", type=str, required=True)\n\n    args = parser.parse_args()\n\n    apply_delta(args.base_model_path, args.target_model_path, args.delta_path)\n"}
{"type": "source_file", "path": "llava/model/llava_arch_cc.py", "content": "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom abc import ABC, abstractmethod\n\nimport math\nimport re\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .multimodal_encoder.builder import build_vision_tower\nfrom .multimodal_resampler.builder import build_vision_resampler\nfrom .multimodal_projector.builder import build_vision_projector\n\nfrom llava.constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n\nfrom llava.mm_utils import get_anyres_image_grid_shape\nfrom llava.utils import rank0_print, rank_print\nimport random\n\n\nclass LlavaMetaCCModel:\n\n    def __init__(self, config):\n        super(LlavaMetaCCModel, self).__init__(config)\n\n        if hasattr(config, \"mm_vision_tower\"):\n            delay_load = getattr(config, \"delay_load\", False)\n            self.vision_tower = build_vision_tower(config, delay_load=delay_load)\n            self.vision_resampler = build_vision_resampler(config, vision_tower=self.vision_tower)\n            # self.vision_resampler = nn.Identity()\n            self.mm_projector = build_vision_projector(config, vision_cfg=self.vision_tower.config)\n            self.mm_cc_projector = build_vision_projector(config, vision_cfg=self.vision_tower.config)\n\n            if \"unpad\" in getattr(config, \"mm_patch_merge_type\", \"\"):\n                self.image_newline = nn.Parameter(torch.empty(config.hidden_size, dtype=self.dtype))\n        \n    def get_vision_tower(self):\n        vision_tower = getattr(self, \"vision_tower\", None)\n        if type(vision_tower) is list:\n            vision_tower = vision_tower[0]\n        return vision_tower\n\n    def initialize_vision_modules(self, model_args, fsdp=None):\n        vision_tower = model_args.vision_tower\n        mm_vision_select_layer = model_args.mm_vision_select_layer\n        mm_vision_select_feature = model_args.mm_vision_select_feature\n        pretrain_mm_mlp_adapter = model_args.pretrain_mm_mlp_adapter\n        mm_patch_merge_type = model_args.mm_patch_merge_type\n\n        self.config.mm_vision_tower = vision_tower\n        self.config.vision_tower_pretrained = getattr(model_args, \"vision_tower_pretrained\", \"\")\n\n        if self.get_vision_tower() is None:\n            vision_tower = build_vision_tower(model_args)\n            vision_resampler = build_vision_resampler(model_args, vision_tower=vision_tower)\n            for k, v in vision_resampler.config.items():\n                setattr(self.config, k, v)\n\n            if fsdp is not None and len(fsdp) > 0:\n                self.vision_tower = [vision_tower]\n                self.vision_resampler = [vision_resampler]\n            else:\n                self.vision_tower = vision_tower\n                self.vision_resampler = vision_resampler\n        else:\n            if fsdp is not None and len(fsdp) > 0:\n                vision_resampler = self.vision_resampler[0]\n                vision_tower = self.vision_tower[0]\n            else:\n                vision_resampler = self.vision_resampler\n                vision_tower = self.vision_tower\n            vision_tower.load_model()\n\n            # In case it is frozen by LoRA\n            for p in self.vision_resampler.parameters():\n                p.requires_grad = True\n\n        self.config.use_mm_proj = True\n        self.config.mm_projector_type = getattr(model_args, \"mm_projector_type\", \"linear\")\n        self.config.mm_hidden_size = getattr(vision_resampler, \"hidden_size\", vision_tower.hidden_size)\n        self.config.mm_vision_select_layer = mm_vision_select_layer\n        self.config.mm_vision_select_feature = mm_vision_select_feature\n        self.config.mm_patch_merge_type = mm_patch_merge_type\n\n        if getattr(self, \"mm_projector\", None) is None:\n            self.mm_projector = build_vision_projector(self.config, vision_cfg=vision_tower.config)\n\n            if \"unpad\" in mm_patch_merge_type:\n                embed_std = 1 / torch.sqrt(torch.tensor(self.config.hidden_size, dtype=self.dtype))\n                self.image_newline = nn.Parameter(torch.randn(self.config.hidden_size, dtype=self.dtype) * embed_std)\n        else:\n            # In case it is frozen by LoRA\n            for p in self.mm_projector.parameters():\n                p.requires_grad = True\n\n        if pretrain_mm_mlp_adapter is not None:\n            mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location=\"cpu\")\n\n            def get_w(weights, keyword):\n                return {k.split(keyword + \".\")[1]: v for k, v in weights.items() if keyword in k}\n\n            incompatible_keys = self.mm_projector.load_state_dict(get_w(mm_projector_weights, \"mm_projector\"))\n            rank0_print(f\"Loaded mm projector weights from {pretrain_mm_mlp_adapter}. Incompatible keys: {incompatible_keys}\")\n            incompatible_keys = self.vision_resampler.load_state_dict(get_w(mm_projector_weights, \"vision_resampler\"), strict=False)\n            rank0_print(f\"Loaded vision resampler weights from {pretrain_mm_mlp_adapter}. Incompatible keys: {incompatible_keys}\")\n\n\ndef unpad_image(tensor, original_size):\n    \"\"\"\n    Unpads a PyTorch tensor of a padded and resized image.\n\n    Args:\n    tensor (torch.Tensor): The image tensor, assumed to be in CxHxW format.\n    original_size (tuple): The original size of the image (height, width).\n\n    Returns:\n    torch.Tensor: The unpadded image tensor.\n    \"\"\"\n    original_width, original_height = original_size\n    current_height, current_width = tensor.shape[1:]\n\n    # Compute aspect ratios\n    original_aspect_ratio = original_width / original_height\n    current_aspect_ratio = current_width / current_height\n\n    # Determine padding size and direction\n    if original_aspect_ratio > current_aspect_ratio:\n        # Padding was added to the height\n        scale_factor = current_width / original_width\n        new_height = int(original_height * scale_factor)\n        padding = (current_height - new_height) // 2\n        unpadded_tensor = tensor[:, padding : current_height - padding, :]\n    else:\n        # Padding was added to the width\n        scale_factor = current_height / original_height\n        new_width = int(original_width * scale_factor)\n        padding = (current_width - new_width) // 2\n        unpadded_tensor = tensor[:, :, padding : current_width - padding]\n\n    return unpadded_tensor\n\n\nclass LlavaMetaCCForCausalLM(ABC):\n\n    @abstractmethod\n    def get_model(self):\n        pass\n\n    def get_vision_tower(self):\n        return self.get_model().get_vision_tower()\n\n    def get_2dPool(self, image_feature):\n        height = width = self.get_vision_tower().num_patches_per_side\n        num_frames, num_tokens, num_dim = image_feature.shape\n        image_feature = image_feature.view(num_frames, height, width, -1)\n        image_feature = image_feature.permute(0, 3, 1, 2).contiguous()\n        # image_feature = nn.functional.max_pool2d(image_feature, self.config.mm_spatial_pool_stride)\n        if self.config.mm_spatial_pool_mode == \"average\":\n            image_feature = nn.functional.avg_pool2d(image_feature, self.config.mm_spatial_pool_stride)\n        elif self.config.mm_spatial_pool_mode == \"max\":\n            image_feature = nn.functional.max_pool2d(image_feature, self.config.mm_spatial_pool_stride)\n        elif self.config.mm_spatial_pool_mode == \"bilinear\":\n            height, weight = image_feature.shape[2:]\n            scaled_shape = [math.ceil(height / 2), math.ceil(weight / 2)]\n            image_feature = nn.functional.interpolate(image_feature, size=scaled_shape, mode='bilinear')\n\n        else:\n            raise ValueError(f\"Unexpected mm_spatial_pool_mode: {self.config.mm_spatial_pool_mode}\")\n        image_feature = image_feature.permute(0, 2, 3, 1)\n        image_feature = image_feature.view(num_frames, -1, num_dim)\n        return image_feature\n\n    def encode_images(self, images):\n        image_features = self.get_model().get_vision_tower()(images)\n        # image_features = self.get_model().vision_resampler(image_features, images=images)  # No resampler here\n        image_features = self.get_model().mm_projector(image_features)\n        return image_features\n\n    def encode_images_and_cc_features(self, images):  # images是concat bs得到的tensor shape\n        image_features = self.get_model().get_vision_tower()(images)\n        last_stage_features = image_features[-1]\n        multi_stage_features_A = [image_features[i][0::2] for i in range(len(image_features))]\n        multi_stage_features_B = [image_features[i][1::2] for i in range(len(image_features))]\n\n        imageA_emb_after_tokenreduce, imageB_emb_after_tokenreduce, change_emb_combined = self.get_model().vision_resampler(multi_stage_features_A, multi_stage_features_B)\n        combined_emb = torch.empty_like(torch.cat([imageA_emb_after_tokenreduce, imageB_emb_after_tokenreduce], dim=0))\n        combined_emb[0::2, :, :] = imageA_emb_after_tokenreduce  # 将 imageA_emb 放回奇数位置\n        combined_emb[1::2, :, :] = imageB_emb_after_tokenreduce  # 将 imageB_emb 放回偶数位置\n        images_features_after_projector = self.get_model().mm_projector(combined_emb)\n        return images_features_after_projector, change_emb_combined, last_stage_features, combined_emb  # 经过projector后的视觉特征, 提取到的变化特征, vision tower提出出的特征最后一层, 经过CCExpert后的图像特征\n\n    def encode_multimodals(self, videos_or_images, video_idx_in_batch, split_sizes=None):\n        videos_or_images_features = self.get_model().get_vision_tower()(videos_or_images)\n        per_videos_or_images_features = torch.split(videos_or_images_features, split_sizes, dim=0)  # tuple, (dim_1, 576, 4096)\n        all_videos_or_images_features = []\n\n        for idx, feat in enumerate(per_videos_or_images_features):\n            feat = self.get_model().mm_projector(feat)\n            if idx in video_idx_in_batch:\n                feat = self.get_2dPool(feat)\n            all_videos_or_images_features.append(feat)\n        return all_videos_or_images_features\n\n    def prepare_inputs_labels_for_multimodal(self, input_ids, position_ids, attention_mask, past_key_values, labels, images, modalities=[\"image\"], image_sizes=None, is_draw_pictures=False):\n        vision_tower = self.get_vision_tower()\n        # rank_print(modalities)\n        if vision_tower is None or images is None or input_ids.shape[1] == 1:\n            return input_ids, position_ids, attention_mask, past_key_values, None, labels\n        if type(images) is list or images.ndim == 5:\n            if type(images) is list:\n                images = [x.unsqueeze(0) if x.ndim == 3 else x for x in images]\n\n            video_idx_in_batch = []\n            for _ in range(len(modalities)):\n                if modalities[_] == \"video\":\n                    video_idx_in_batch.append(_)\n\n            images_list = []\n            for image in images:\n                if image.ndim == 4:\n                    images_list.append(image)\n                else:\n                    images_list.append(image.unsqueeze(0))\n            concat_images = torch.cat([image for image in images_list], dim=0)\n            image_features, change_features, last_stage_features, combined_emb = self.encode_images_and_cc_features(concat_images)\n            image_features = [t.unsqueeze(0) for t in image_features]\n            image_features = [x.flatten(0, 1) for x in image_features]\n        else:\n            image_features = self.encode_images(images)\n\n        # TODO: image start / end is not implemented here to support pretraining.\n        if getattr(self.config, \"tune_mm_mlp_adapter\", False) and getattr(self.config, \"mm_use_im_start_end\", False):\n            raise NotImplementedError\n        # rank_print(f\"Total images : {len(image_features)}\")\n\n        # Let's just add dummy tensors if they do not exist,\n        # it is a headache to deal with None all the time.\n        # But it is not ideal, and if you have a better idea,\n        # please open an issue / submit a PR, thanks.\n        _labels = labels\n        _position_ids = position_ids\n        _attention_mask = attention_mask\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids, dtype=torch.bool)\n        else:\n            attention_mask = attention_mask.bool()\n        if position_ids is None:\n            position_ids = torch.arange(0, input_ids.shape[1], dtype=torch.long, device=input_ids.device)\n        if labels is None:\n            labels = torch.full_like(input_ids, IGNORE_INDEX)\n\n        # remove the padding using attention_mask -- FIXME\n        _input_ids = input_ids\n        input_ids = [cur_input_ids[cur_attention_mask] for cur_input_ids, cur_attention_mask in zip(input_ids, attention_mask)]\n        labels = [cur_labels[cur_attention_mask] for cur_labels, cur_attention_mask in zip(labels, attention_mask)]\n\n        new_input_embeds = []\n        new_labels = []\n        cur_image_idx = 0\n        # rank_print(\"Inserting Images embedding\")\n        for batch_idx, cur_input_ids in enumerate(input_ids):\n            num_images = (cur_input_ids == IMAGE_TOKEN_INDEX).sum()\n            # rank0_print(num_images)\n            if num_images == 0:\n                cur_image_features = image_features[cur_image_idx]\n                cur_input_embeds_1 = self.get_model().embed_tokens(cur_input_ids)\n                cur_input_embeds = torch.cat([cur_input_embeds_1, cur_image_features[0:0]], dim=0)\n                new_input_embeds.append(cur_input_embeds)\n                new_labels.append(labels[batch_idx])\n                cur_image_idx += 1\n                continue\n\n            image_token_indices = [-1] + torch.where(cur_input_ids == IMAGE_TOKEN_INDEX)[0].tolist() + [cur_input_ids.shape[0]]\n            cur_input_ids_noim = []\n            cur_labels = labels[batch_idx]\n            cur_labels_noim = []\n            for i in range(len(image_token_indices) - 1):\n                cur_input_ids_noim.append(cur_input_ids[image_token_indices[i] + 1 : image_token_indices[i + 1]])\n                cur_labels_noim.append(cur_labels[image_token_indices[i] + 1 : image_token_indices[i + 1]])\n            split_sizes = [x.shape[0] for x in cur_labels_noim]\n            cur_input_embeds = self.get_model().embed_tokens(torch.cat(cur_input_ids_noim))\n            cur_input_embeds_no_im = torch.split(cur_input_embeds, split_sizes, dim=0)\n            cur_new_input_embeds = []\n            cur_new_labels = []\n\n            for i in range(num_images + 1):\n                cur_new_input_embeds.append(cur_input_embeds_no_im[i])\n                cur_new_labels.append(cur_labels_noim[i])\n                if i < num_images:\n                    try:\n                        cur_image_features = image_features[cur_image_idx]\n                    except IndexError:\n                        cur_image_features = image_features[cur_image_idx - 1]\n                    cur_image_idx += 1\n                    cur_new_input_embeds.append(cur_image_features)\n                    cur_new_labels.append(torch.full((cur_image_features.shape[0],), IGNORE_INDEX, device=cur_labels.device, dtype=cur_labels.dtype))\n\n            cur_new_input_embeds = [x.to(self.device) for x in cur_new_input_embeds]\n\n            cur_new_input_embeds = torch.cat(cur_new_input_embeds)\n            cur_new_labels = torch.cat(cur_new_labels)\n\n            new_input_embeds.append(cur_new_input_embeds)\n            new_labels.append(cur_new_labels)\n\n        # Truncate sequences to max length as image embeddings can make the sequence longer\n        tokenizer_model_max_length = getattr(self.config, \"tokenizer_model_max_length\", None)\n        # rank_print(\"Finishing Inserting\")\n\n        new_input_embeds = [x[:tokenizer_model_max_length] for x, modality in zip(new_input_embeds, modalities)]\n        new_labels = [x[:tokenizer_model_max_length] for x, modality in zip(new_labels, modalities)]\n\n        # TODO: Hard code for control loss spike\n        if tokenizer_model_max_length is not None:\n            new_input_embeds = [x[:4096] if modality != \"video\" else x[:tokenizer_model_max_length] for x, modality in zip(new_input_embeds, modalities)]\n            new_labels = [x[:4096] if modality != \"video\" else x[:tokenizer_model_max_length] for x, modality in zip(new_labels, modalities)]\n\n        # Combine them\n        max_len = max(x.shape[0] for x in new_input_embeds)\n        batch_size = len(new_input_embeds)\n\n        new_input_embeds_padded = []\n        new_labels_padded = torch.full((batch_size, max_len), IGNORE_INDEX, dtype=new_labels[0].dtype, device=new_labels[0].device)\n        attention_mask = torch.zeros((batch_size, max_len), dtype=attention_mask.dtype, device=attention_mask.device)\n        position_ids = torch.zeros((batch_size, max_len), dtype=position_ids.dtype, device=position_ids.device)\n        # rank0_print(\"Prepare pos id\")\n\n        for i, (cur_new_embed, cur_new_labels) in enumerate(zip(new_input_embeds, new_labels)):\n            cur_len = cur_new_embed.shape[0]\n            if getattr(self.config, \"tokenizer_padding_side\", \"right\") == \"left\":\n                new_input_embeds_padded.append(torch.cat((torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device), cur_new_embed), dim=0))\n                if cur_len > 0:\n                    new_labels_padded[i, -cur_len:] = cur_new_labels\n                    attention_mask[i, -cur_len:] = True\n                    position_ids[i, -cur_len:] = torch.arange(0, cur_len, dtype=position_ids.dtype, device=position_ids.device)\n            else:\n                new_input_embeds_padded.append(torch.cat((cur_new_embed, torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device)), dim=0))\n                if cur_len > 0:\n                    new_labels_padded[i, :cur_len] = cur_new_labels\n                    attention_mask[i, :cur_len] = True\n                    position_ids[i, :cur_len] = torch.arange(0, cur_len, dtype=position_ids.dtype, device=position_ids.device)\n\n        new_input_embeds = torch.stack(new_input_embeds_padded, dim=0)\n        # rank0_print(\"tokenizer padding\")\n\n        if _labels is None:\n            new_labels = None\n        else:\n            new_labels = new_labels_padded\n\n        if _attention_mask is None:\n            attention_mask = None\n        else:\n            attention_mask = attention_mask.to(dtype=_attention_mask.dtype)\n\n        if _position_ids is None:\n            position_ids = None\n        if getattr(self.config, \"use_pos_skipping\", False) and self.training:\n            position_ids = torch.arange(new_input_embeds.size(1), device=new_input_embeds.device).unsqueeze(0).to(new_input_embeds.device)\n            split_position = random.randint(0, new_input_embeds.size(1))\n            left_add = random.randint(0, self.config.pos_skipping_range)\n            right_add = random.randint(left_add, self.config.pos_skipping_range)\n            position_ids[:, :split_position] += left_add\n            position_ids[:, split_position:] += right_add\n        # rank0_print(\"Finish preparing\")\n        if is_draw_pictures:\n            return None, position_ids, attention_mask, past_key_values, new_input_embeds, new_labels, last_stage_features, combined_emb, change_features\n        else:\n            return None, position_ids, attention_mask, past_key_values, new_input_embeds, new_labels\n\n    def initialize_vision_tokenizer(self, model_args, tokenizer):\n        if model_args.mm_use_im_patch_token:\n            tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\n            self.resize_token_embeddings(len(tokenizer))\n\n        if model_args.mm_use_im_start_end:\n            num_new_tokens = tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\n            self.resize_token_embeddings(len(tokenizer))\n\n            if num_new_tokens > 0:\n                input_embeddings = self.get_input_embeddings().weight.data\n                output_embeddings = self.get_output_embeddings().weight.data\n\n                input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n                output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n\n                input_embeddings[-num_new_tokens:] = input_embeddings_avg\n                output_embeddings[-num_new_tokens:] = output_embeddings_avg\n\n            if model_args.tune_mm_mlp_adapter:\n                for p in self.get_input_embeddings().parameters():\n                    p.requires_grad = True\n                for p in self.get_output_embeddings().parameters():\n                    p.requires_grad = False\n\n            if model_args.pretrain_mm_mlp_adapter:\n                mm_projector_weights = torch.load(model_args.pretrain_mm_mlp_adapter, map_location=\"cpu\")\n                embed_tokens_weight = mm_projector_weights[\"model.embed_tokens.weight\"]\n                assert num_new_tokens == 2\n                if input_embeddings.shape == embed_tokens_weight.shape:\n                    input_embeddings[-num_new_tokens:] = embed_tokens_weight[-num_new_tokens:]\n                elif embed_tokens_weight.shape[0] == num_new_tokens:\n                    input_embeddings[-num_new_tokens:] = embed_tokens_weight\n                else:\n                    raise ValueError(f\"Unexpected embed_tokens_weight shape. Pretrained: {embed_tokens_weight.shape}. Current: {input_embeddings.shape}. Numer of new tokens: {num_new_tokens}.\")\n        elif model_args.mm_use_im_patch_token:\n            if model_args.tune_mm_mlp_adapter:\n                for p in self.get_input_embeddings().parameters():\n                    p.requires_grad = False\n                for p in self.get_output_embeddings().parameters():\n                    p.requires_grad = False"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/builder.py", "content": "import os\nfrom .clip_encoder import CLIPVisionTower\nfrom .imagebind import ImageBindWrapper\nfrom .open_clip_encoder import OpenCLIPVisionTower\nfrom .hf_vision import HFVisionTower\nfrom .siglip_encoder import SigLipVisionTower\nfrom .clip_encoder import CLIPVisionTower, CLIPVisionTowerS2\n\n# from .eva_clip.eva_clip_encoder import EvaClipVisionTower\n# from .dev_eva_clip.eva_vit import EvaViTWrapper\n\n\ndef build_vision_tower(vision_tower_cfg, **kwargs):\n    vision_tower = getattr(vision_tower_cfg, \"mm_vision_tower\", getattr(vision_tower_cfg, \"vision_tower\", None))\n    is_absolute_path_exists = os.path.exists(vision_tower)\n    use_s2 = getattr(vision_tower_cfg, \"s2\", False)\n    if is_absolute_path_exists or vision_tower.startswith(\"openai\") or vision_tower.startswith(\"laion\") or \"ShareGPT4V\" in vision_tower:\n        if use_s2:\n            return CLIPVisionTowerS2(vision_tower, args=vision_tower_cfg, **kwargs)\n        else:\n            return CLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n    elif \"siglip\" in vision_tower:\n        return SigLipVisionTower(vision_tower, vision_tower_cfg=vision_tower_cfg, **kwargs)\n    elif vision_tower.startswith(\"hf:\"):\n        return HFVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n    elif vision_tower in [\"imagebind_huge\"]:\n        return ImageBindWrapper(vision_tower, args=vision_tower_cfg, **kwargs)\n    elif vision_tower.startswith(\"open_clip_hub\"):\n        return OpenCLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n    # elif \"internal-eva\" in vision_tower.lower() or \"eva02\" in vision_tower.lower():\n    #     return EvaClipVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n    # elif vision_tower in [\"EVA-CLIP-8B\", \"EVA-CLIP-8B-plus\"]:\n    #     return EvaViTWrapper(vision_tower, args=vision_tower_cfg, **kwargs)\n\n    raise ValueError(f\"Unknown vision tower: {vision_tower}\")\n"}
{"type": "source_file", "path": "llava/model/language_model/llava_qwen.py", "content": "#    Copyright 2024 Hao Zhang\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom typing import List, Optional, Tuple, Union, Dict\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\n\nimport transformers\nfrom transformers import AutoConfig, AutoModelForCausalLM, LlamaConfig, LlamaModel, LlamaForCausalLM\n\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nfrom transformers.generation.utils import GenerateOutput\n\n# from ...constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\nfrom llava.model.llava_arch import LlavaMetaModel, LlavaMetaForCausalLM\nfrom transformers import Qwen2Config, Qwen2Model, Qwen2ForCausalLM\n\n# from .qwen.modeling_qwen import QWenLMHeadModel, QWenModel\n# from .qwen.configuration_qwen import QWenConfig\n\n\nclass LlavaQwenConfig(Qwen2Config):\n    model_type = \"llava_qwen\"\n\n\nclass LlavaQwenModel(LlavaMetaModel, Qwen2Model):\n    config_class = LlavaQwenConfig\n\n    def __init__(self, config: Qwen2Config):\n        super(LlavaQwenModel, self).__init__(config)\n\n\nclass LlavaQwenForCausalLM(Qwen2ForCausalLM, LlavaMetaForCausalLM):\n    config_class = LlavaQwenConfig\n\n    def __init__(self, config):\n        # super(Qwen2ForCausalLM, self).__init__(config)\n        Qwen2ForCausalLM.__init__(self, config)\n        config.model_type = \"llava_qwen\"\n        config.rope_scaling = None\n\n        self.model = LlavaQwenModel(config)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.model\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        images: Optional[torch.FloatTensor] = None,\n        image_sizes: Optional[List[List[int]]] = None,\n        return_dict: Optional[bool] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        dpo_forward: Optional[bool] = False,\n        cache_position=None,\n        **kwargs,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n\n        if inputs_embeds is None:\n            (input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels) = self.prepare_inputs_labels_for_multimodal(input_ids, position_ids, attention_mask, past_key_values, labels, images, modalities, image_sizes)\n\n        if dpo_forward:\n            outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n            hidden_states = outputs[0]\n            logits = self.lm_head(hidden_states)\n            return logits, labels\n\n        else:\n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                labels=labels,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs: Optional[torch.Tensor] = None,\n        images: Optional[torch.Tensor] = None,\n        image_sizes: Optional[torch.Tensor] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        **kwargs,\n    ) -> Union[GenerateOutput, torch.LongTensor]:\n        position_ids = kwargs.pop(\"position_ids\", None)\n        attention_mask = kwargs.pop(\"attention_mask\", None)\n        if \"inputs_embeds\" in kwargs:\n            raise NotImplementedError(\"`inputs_embeds` is not supported\")\n\n        if images is not None:\n            (inputs, position_ids, attention_mask, _, inputs_embeds, _) = self.prepare_inputs_labels_for_multimodal(inputs, position_ids, attention_mask, None, None, images, modalities, image_sizes=image_sizes)\n        else:\n            inputs_embeds = self.get_model().embed_tokens(inputs)\n\n        return super().generate(position_ids=position_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, **kwargs)\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n        images = kwargs.pop(\"images\", None)\n        image_sizes = kwargs.pop(\"image_sizes\", None)\n        inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs)\n        if images is not None:\n            inputs[\"images\"] = images\n        if image_sizes is not None:\n            inputs[\"image_sizes\"] = image_sizes\n        return inputs\n\n\nAutoConfig.register(\"llava_qwen\", LlavaQwenConfig)\nAutoModelForCausalLM.register(LlavaQwenConfig, LlavaQwenForCausalLM)\n"}
{"type": "source_file", "path": "llava/model/language_model/llava_mixtral.py", "content": "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\n\nfrom transformers import AutoConfig, AutoModelForCausalLM, MixtralConfig, MixtralModel, MixtralForCausalLM, GenerationConfig\n\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nfrom transformers.generation.utils import GenerateOutput\n\nfrom ..llava_arch import LlavaMetaModel, LlavaMetaForCausalLM\n\n\nclass LlavaMixtralConfig(MixtralConfig):\n    model_type = \"llava_mixtral\"\n\n\nclass LlavaMixtralModel(LlavaMetaModel, MixtralModel):\n    config_class = LlavaMixtralConfig\n\n    def __init__(self, config: MixtralConfig):\n        super(LlavaMixtralModel, self).__init__(config)\n\n\nclass LlavaMixtralForCausalLM(MixtralForCausalLM, LlavaMetaForCausalLM):\n    config_class = LlavaMixtralConfig\n\n    def __init__(self, config):\n        super(MixtralForCausalLM, self).__init__(config)\n\n        config.model_type = \"llava_mixtral\"\n        config.rope_scaling = None\n        self.model = LlavaMixtralModel(config)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.model\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        images: Optional[torch.FloatTensor] = None,\n        image_sizes: Optional[List[List[int]]] = None,\n        return_dict: Optional[bool] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        dpo_forward: Optional[bool] = None,\n        cache_position=None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n\n        if inputs_embeds is None:\n            (input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels) = self.prepare_inputs_labels_for_multimodal(input_ids, position_ids, attention_mask, past_key_values, labels, images, modalities, image_sizes)\n\n        if dpo_forward:\n            outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n            hidden_states = outputs[0]\n            logits = self.lm_head(hidden_states)\n            return logits, labels\n\n        else:\n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                labels=labels,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs: Optional[torch.Tensor] = None,\n        images: Optional[torch.Tensor] = None,\n        image_sizes: Optional[torch.Tensor] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        **kwargs,\n    ) -> Union[GenerateOutput, torch.LongTensor]:\n        position_ids = kwargs.pop(\"position_ids\", None)\n        attention_mask = kwargs.pop(\"attention_mask\", None)\n        if \"inputs_embeds\" in kwargs:\n            raise NotImplementedError(\"`inputs_embeds` is not supported\")\n\n        if images is not None:\n            (inputs, position_ids, attention_mask, _, inputs_embeds, _) = self.prepare_inputs_labels_for_multimodal(inputs, position_ids, attention_mask, None, None, images, modalities, image_sizes=image_sizes)\n        else:\n            inputs_embeds = self.get_model().embed_tokens(inputs)\n\n        return super().generate(position_ids=position_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, **kwargs)\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n        images = kwargs.pop(\"images\", None)\n        image_sizes = kwargs.pop(\"image_sizes\", None)\n        inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs)\n        if images is not None:\n            inputs[\"images\"] = images\n        if image_sizes is not None:\n            inputs[\"image_sizes\"] = image_sizes\n        return inputs\n\n\nAutoConfig.register(\"llava_mixtral\", LlavaMixtralConfig)\nAutoModelForCausalLM.register(LlavaMixtralConfig, LlavaMixtralForCausalLM)\n"}
{"type": "source_file", "path": "llava/model/language_model/llava_mpt.py", "content": "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom typing import Optional, Tuple\n\nimport torch\n\nfrom transformers import AutoConfig, AutoModelForCausalLM, MptConfig, MptForCausalLM, MptModel, GenerationConfig\nfrom llava.model.llava_arch import LlavaMetaModel, LlavaMetaForCausalLM\n\n\nclass LlavaMptConfig(MptConfig):\n    model_type = \"llava_mpt\"\n\n\nclass LlavaMptModel(LlavaMetaModel, MptModel):\n    config_class = LlavaMptConfig\n\n    def __init__(self, config: MptConfig):\n        config.hidden_size = config.d_model\n        super(LlavaMptModel, self).__init__(config)\n\n    def embed_tokens(self, x):\n        return self.wte(x)\n\n\nclass LlavaMptForCausalLM(MptForCausalLM, LlavaMetaForCausalLM):\n    config_class = LlavaMptConfig\n    supports_gradient_checkpointing = True\n\n    def __init__(self, config):\n        super(MptForCausalLM, self).__init__(config)\n\n        config.model_type = \"llava_mpt\"\n        config.rope_scaling = None\n        self.generation_config = GenerationConfig(\n            temperature=0.0,\n            max_new_tokens=1024,\n            do_sample=False,\n            top_p=None,\n        )\n\n        self.transformer = LlavaMptModel(config)\n        self.lm_head = torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.transformer\n\n    def _set_gradient_checkpointing(self, module, value=False):\n        if isinstance(module, LlavaMptModel):\n            module.gradient_checkpointing = value\n\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        inputs_embeds: Optional[torch.Tensor] = None,\n        labels: Optional[torch.Tensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        cache_position=None,\n        images=None,\n    ):\n\n        input_ids, attention_mask, past_key_values, inputs_embeds, labels = self.prepare_inputs_labels_for_multimodal(input_ids, attention_mask, past_key_values, labels, images)\n\n        return super().forward(\n            input_ids,\n            past_key_values=past_key_values,\n            attention_mask=attention_mask,\n            inputs_embeds=inputs_embeds,\n            labels=labels,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n        images = kwargs.pop(\"images\", None)\n        _inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs)\n        _inputs[\"images\"] = images\n        return _inputs\n\n\nAutoConfig.register(\"llava_mpt\", LlavaMptConfig)\nAutoModelForCausalLM.register(LlavaMptConfig, LlavaMptForCausalLM)\n"}
{"type": "source_file", "path": "llava/model/make_delta.py", "content": "\"\"\"\nUsage:\npython3 -m llava.model.make_delta --base ~/model_weights/llama-7b --target ~/model_weights/llava-7b --delta ~/model_weights/llava-7b-delta --hub-repo-id liuhaotian/llava-7b-delta\n\"\"\"\n\nimport argparse\n\nimport torch\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom llava.model.utils import auto_upgrade\n\n\ndef make_delta(base_model_path, target_model_path, delta_path, hub_repo_id):\n    print(\"Loading base model\")\n    base = AutoModelForCausalLM.from_pretrained(base_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n\n    print(\"Loading target model\")\n    auto_upgrade(target_model_path)\n    target = AutoModelForCausalLM.from_pretrained(target_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n\n    print(\"Calculating delta\")\n    for name, param in tqdm(target.state_dict().items(), desc=\"Calculating delta\"):\n        if name not in base.state_dict():\n            assert name in [\"model.mm_projector.weight\", \"model.mm_projector.bias\"], f\"{name} not in base model\"\n            continue\n        if param.data.shape == base.state_dict()[name].shape:\n            param.data -= base.state_dict()[name]\n        else:\n            assert name in [\"model.embed_tokens.weight\", \"lm_head.weight\"], f\"{name} dimension mismatch: {param.data.shape} vs {base.state_dict()[name].shape}\"\n            bparam = base.state_dict()[name]\n            param.data[: bparam.shape[0], : bparam.shape[1]] -= bparam\n\n    print(\"Saving delta\")\n    if hub_repo_id:\n        kwargs = {\"push_to_hub\": True, \"repo_id\": hub_repo_id}\n    else:\n        kwargs = {}\n    target.save_pretrained(delta_path, **kwargs)\n    target_tokenizer = AutoTokenizer.from_pretrained(target_model_path)\n    target_tokenizer.save_pretrained(delta_path, **kwargs)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--base-model-path\", type=str, required=True)\n    parser.add_argument(\"--target-model-path\", type=str, required=True)\n    parser.add_argument(\"--delta-path\", type=str, required=True)\n    parser.add_argument(\"--hub-repo-id\", type=str, default=None)\n    args = parser.parse_args()\n\n    make_delta(args.base_model_path, args.target_model_path, args.delta_path, args.hub_repo_id)\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/loss.py", "content": "import math\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\ntry:\n    import torch.distributed.nn\n    from torch import distributed as dist\n\n    has_distributed = True\nexcept ImportError:\n    has_distributed = False\n\ntry:\n    import horovod.torch as hvd\nexcept ImportError:\n    hvd = None\n\nfrom timm.loss import LabelSmoothingCrossEntropy\n\n\ndef gather_features(image_features, text_features, local_loss=False, gather_with_grad=False, rank=0, world_size=1, use_horovod=False):\n    assert has_distributed, \"torch.distributed did not import correctly, please use a PyTorch version with support.\"\n    if use_horovod:\n        assert hvd is not None, \"Please install horovod\"\n        if gather_with_grad:\n            all_image_features = hvd.allgather(image_features)\n            all_text_features = hvd.allgather(text_features)\n        else:\n            with torch.no_grad():\n                all_image_features = hvd.allgather(image_features)\n                all_text_features = hvd.allgather(text_features)\n            if not local_loss:\n                # ensure grads for local rank when all_* features don't have a gradient\n                gathered_image_features = list(all_image_features.chunk(world_size, dim=0))\n                gathered_text_features = list(all_text_features.chunk(world_size, dim=0))\n                gathered_image_features[rank] = image_features\n                gathered_text_features[rank] = text_features\n                all_image_features = torch.cat(gathered_image_features, dim=0)\n                all_text_features = torch.cat(gathered_text_features, dim=0)\n    else:\n        # We gather tensors from all gpus\n        if gather_with_grad:\n            all_image_features = torch.cat(torch.distributed.nn.all_gather(image_features), dim=0)\n            all_text_features = torch.cat(torch.distributed.nn.all_gather(text_features), dim=0)\n            # all_image_features = torch.cat(torch.distributed.nn.all_gather(image_features, async_op=True), dim=0)\n            # all_text_features = torch.cat(torch.distributed.nn.all_gather(text_features, async_op=True), dim=0)\n        else:\n            gathered_image_features = [torch.zeros_like(image_features) for _ in range(world_size)]\n            gathered_text_features = [torch.zeros_like(text_features) for _ in range(world_size)]\n            dist.all_gather(gathered_image_features, image_features)\n            dist.all_gather(gathered_text_features, text_features)\n            if not local_loss:\n                # ensure grads for local rank when all_* features don't have a gradient\n                gathered_image_features[rank] = image_features\n                gathered_text_features[rank] = text_features\n            all_image_features = torch.cat(gathered_image_features, dim=0)\n            all_text_features = torch.cat(gathered_text_features, dim=0)\n\n    return all_image_features, all_text_features\n\n\nclass ClipLoss(nn.Module):\n\n    def __init__(\n        self,\n        local_loss=False,\n        gather_with_grad=False,\n        cache_labels=False,\n        rank=0,\n        world_size=1,\n        use_horovod=False,\n        smoothing=0.0,\n    ):\n        super().__init__()\n        self.local_loss = local_loss\n        self.gather_with_grad = gather_with_grad\n        self.cache_labels = cache_labels\n        self.rank = rank\n        self.world_size = world_size\n        self.use_horovod = use_horovod\n        self.label_smoothing_cross_entropy = LabelSmoothingCrossEntropy(smoothing=smoothing) if smoothing > 0 else None\n\n        # cache state\n        self.prev_num_logits = 0\n        self.labels = {}\n\n    def forward(self, image_features, text_features, logit_scale=1.0):\n        device = image_features.device\n        if self.world_size > 1:\n            all_image_features, all_text_features = gather_features(image_features, text_features, self.local_loss, self.gather_with_grad, self.rank, self.world_size, self.use_horovod)\n\n            if self.local_loss:\n                logits_per_image = logit_scale * image_features @ all_text_features.T\n                logits_per_text = logit_scale * text_features @ all_image_features.T\n            else:\n                logits_per_image = logit_scale * all_image_features @ all_text_features.T\n                logits_per_text = logits_per_image.T\n        else:\n            logits_per_image = logit_scale * image_features @ text_features.T\n            logits_per_text = logit_scale * text_features @ image_features.T\n        # calculated ground-truth and cache if enabled\n        num_logits = logits_per_image.shape[0]\n        if self.prev_num_logits != num_logits or device not in self.labels:\n            labels = torch.arange(num_logits, device=device, dtype=torch.long)\n            if self.world_size > 1 and self.local_loss:\n                labels = labels + num_logits * self.rank\n            if self.cache_labels:\n                self.labels[device] = labels\n                self.prev_num_logits = num_logits\n        else:\n            labels = self.labels[device]\n\n        if self.label_smoothing_cross_entropy:\n            total_loss = (self.label_smoothing_cross_entropy(logits_per_image, labels) + self.label_smoothing_cross_entropy(logits_per_text, labels)) / 2\n        else:\n            total_loss = (F.cross_entropy(logits_per_image, labels) + F.cross_entropy(logits_per_text, labels)) / 2\n\n        acc = None\n        i2t_acc = (logits_per_image.argmax(-1) == labels).sum() / len(logits_per_image)\n        t2i_acc = (logits_per_text.argmax(-1) == labels).sum() / len(logits_per_text)\n        acc = {\"i2t\": i2t_acc, \"t2i\": t2i_acc}\n        return total_loss, acc\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/modified_resnet.py", "content": "from collections import OrderedDict\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom .utils import freeze_batch_norm_2d\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1):\n        super().__init__()\n\n        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.act1 = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.act2 = nn.ReLU(inplace=True)\n\n        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()\n\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.act3 = nn.ReLU(inplace=True)\n\n        self.downsample = None\n        self.stride = stride\n\n        if stride > 1 or inplanes != planes * Bottleneck.expansion:\n            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1\n            self.downsample = nn.Sequential(OrderedDict([(\"-1\", nn.AvgPool2d(stride)), (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)), (\"1\", nn.BatchNorm2d(planes * self.expansion))]))\n\n    def forward(self, x: torch.Tensor):\n        identity = x\n\n        out = self.act1(self.bn1(self.conv1(x)))\n        out = self.act2(self.bn2(self.conv2(out)))\n        out = self.avgpool(out)\n        out = self.bn3(self.conv3(out))\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.act3(out)\n        return out\n\n\nclass AttentionPool2d(nn.Module):\n    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):\n        super().__init__()\n        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim**2 + 1, embed_dim) / embed_dim**0.5)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n        self.num_heads = num_heads\n\n    def forward(self, x):\n        x = x.reshape(x.shape[0], x.shape[1], x.shape[2] * x.shape[3]).permute(2, 0, 1)  # NCHW -> (HW)NC\n        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC\n        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC\n        x, _ = F.multi_head_attention_forward(\n            query=x,\n            key=x,\n            value=x,\n            embed_dim_to_check=x.shape[-1],\n            num_heads=self.num_heads,\n            q_proj_weight=self.q_proj.weight,\n            k_proj_weight=self.k_proj.weight,\n            v_proj_weight=self.v_proj.weight,\n            in_proj_weight=None,\n            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),\n            bias_k=None,\n            bias_v=None,\n            add_zero_attn=False,\n            dropout_p=0.0,\n            out_proj_weight=self.c_proj.weight,\n            out_proj_bias=self.c_proj.bias,\n            use_separate_proj_weight=True,\n            training=self.training,\n            need_weights=False,\n        )\n\n        return x[0]\n\n\nclass ModifiedResNet(nn.Module):\n    \"\"\"\n    A ResNet class that is similar to torchvision's but contains the following changes:\n    - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\n    - The final pooling layer is a QKV attention instead of an average pool\n    \"\"\"\n\n    def __init__(self, layers, output_dim, heads, image_size=224, width=64):\n        super().__init__()\n        self.output_dim = output_dim\n        self.image_size = image_size\n\n        # the 3-layer stem\n        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(width // 2)\n        self.act1 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(width // 2)\n        self.act2 = nn.ReLU(inplace=True)\n        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(width)\n        self.act3 = nn.ReLU(inplace=True)\n        self.avgpool = nn.AvgPool2d(2)\n\n        # residual layers\n        self._inplanes = width  # this is a *mutable* variable used during construction\n        self.layer1 = self._make_layer(width, layers[0])\n        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)\n        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)\n        self.layer4 = self._make_layer(width * 8, layers[3], stride=2)\n\n        embed_dim = width * 32  # the ResNet feature dimension\n        self.attnpool = AttentionPool2d(image_size // 32, embed_dim, heads, output_dim)\n\n        self.init_parameters()\n\n    def _make_layer(self, planes, blocks, stride=1):\n        layers = [Bottleneck(self._inplanes, planes, stride)]\n\n        self._inplanes = planes * Bottleneck.expansion\n        for _ in range(1, blocks):\n            layers.append(Bottleneck(self._inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def init_parameters(self):\n        if self.attnpool is not None:\n            std = self.attnpool.c_proj.in_features**-0.5\n            nn.init.normal_(self.attnpool.q_proj.weight, std=std)\n            nn.init.normal_(self.attnpool.k_proj.weight, std=std)\n            nn.init.normal_(self.attnpool.v_proj.weight, std=std)\n            nn.init.normal_(self.attnpool.c_proj.weight, std=std)\n\n        for resnet_block in [self.layer1, self.layer2, self.layer3, self.layer4]:\n            for name, param in resnet_block.named_parameters():\n                if name.endswith(\"bn3.weight\"):\n                    nn.init.zeros_(param)\n\n    def lock(self, unlocked_groups=0, freeze_bn_stats=False):\n        assert unlocked_groups == 0, \"partial locking not currently supported for this model\"\n        for param in self.parameters():\n            param.requires_grad = False\n        if freeze_bn_stats:\n            freeze_batch_norm_2d(self)\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        # FIXME support for non-transformer\n        pass\n\n    def stem(self, x):\n        x = self.act1(self.bn1(self.conv1(x)))\n        x = self.act2(self.bn2(self.conv2(x)))\n        x = self.act3(self.bn3(self.conv3(x)))\n        x = self.avgpool(x)\n        return x\n\n    def forward(self, x):\n        x = self.stem(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.attnpool(x)\n\n        return x\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/openai.py", "content": "\"\"\" OpenAI pretrained model functions\n\nAdapted from https://github.com/openai/CLIP. Originally MIT License, Copyright (c) 2021 OpenAI.\n\"\"\"\n\nimport os\nimport warnings\nfrom typing import List, Optional, Union\n\nimport torch\n\nfrom .model import build_model_from_openai_state_dict, convert_weights_to_lp, get_cast_dtype\nfrom .pretrained import get_pretrained_url, list_pretrained_models_by_tag, download_pretrained_from_url\n\n__all__ = [\"list_openai_models\", \"load_openai_model\"]\n\n\ndef list_openai_models() -> List[str]:\n    \"\"\"Returns the names of available CLIP models\"\"\"\n    return list_pretrained_models_by_tag(\"openai\")\n\n\ndef load_openai_model(\n    name: str,\n    precision: Optional[str] = None,\n    device: Optional[Union[str, torch.device]] = None,\n    jit: bool = True,\n    cache_dir: Optional[str] = None,\n):\n    \"\"\"Load a CLIP model\n\n    Parameters\n    ----------\n    name : str\n        A model name listed by `clip.available_models()`, or the path to a model checkpoint containing the state_dict\n    precision: str\n        Model precision, if None defaults to 'fp32' if device == 'cpu' else 'fp16'.\n    device : Union[str, torch.device]\n        The device to put the loaded model\n    jit : bool\n        Whether to load the optimized JIT model (default) or more hackable non-JIT model.\n    cache_dir : Optional[str]\n        The directory to cache the downloaded model weights\n\n    Returns\n    -------\n    model : torch.nn.Module\n        The CLIP model\n    preprocess : Callable[[PIL.Image], torch.Tensor]\n        A torchvision transform that converts a PIL image into a tensor that the returned model can take as its input\n    \"\"\"\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    if precision is None:\n        precision = \"fp32\" if device == \"cpu\" else \"fp16\"\n\n    if get_pretrained_url(name, \"openai\"):\n        model_path = download_pretrained_from_url(get_pretrained_url(name, \"openai\"), cache_dir=cache_dir)\n    elif os.path.isfile(name):\n        model_path = name\n    else:\n        raise RuntimeError(f\"Model {name} not found; available models = {list_openai_models()}\")\n\n    try:\n        # loading JIT archive\n        model = torch.jit.load(model_path, map_location=device if jit else \"cpu\").eval()\n        state_dict = None\n    except RuntimeError:\n        # loading saved state dict\n        if jit:\n            warnings.warn(f\"File {model_path} is not a JIT archive. Loading as a state dict instead\")\n            jit = False\n        state_dict = torch.load(model_path, map_location=\"cpu\")\n\n    if not jit:\n        # Build a non-jit model from the OpenAI jitted model state dict\n        cast_dtype = get_cast_dtype(precision)\n        try:\n            model = build_model_from_openai_state_dict(state_dict or model.state_dict(), cast_dtype=cast_dtype)\n        except KeyError:\n            sd = {k[7:]: v for k, v in state_dict[\"state_dict\"].items()}\n            model = build_model_from_openai_state_dict(sd, cast_dtype=cast_dtype)\n\n        # model from OpenAI state dict is in manually cast fp16 mode, must be converted for AMP/fp32/bf16 use\n        model = model.to(device)\n        if precision.startswith(\"amp\") or precision == \"fp32\":\n            model.float()\n        elif precision == \"bf16\":\n            convert_weights_to_lp(model, dtype=torch.bfloat16)\n\n        return model\n\n    # patch the device names\n    device_holder = torch.jit.trace(lambda: torch.ones([]).to(torch.device(device)), example_inputs=[])\n    device_node = [n for n in device_holder.graph.findAllNodes(\"prim::Constant\") if \"Device\" in repr(n)][-1]\n\n    def patch_device(module):\n        try:\n            graphs = [module.graph] if hasattr(module, \"graph\") else []\n        except RuntimeError:\n            graphs = []\n\n        if hasattr(module, \"forward1\"):\n            graphs.append(module.forward1.graph)\n\n        for graph in graphs:\n            for node in graph.findAllNodes(\"prim::Constant\"):\n                if \"value\" in node.attributeNames() and str(node[\"value\"]).startswith(\"cuda\"):\n                    node.copyAttributes(device_node)\n\n    model.apply(patch_device)\n    patch_device(model.encode_image)\n    patch_device(model.encode_text)\n\n    # patch dtype to float32 (typically for CPU)\n    if precision == \"fp32\":\n        float_holder = torch.jit.trace(lambda: torch.ones([]).float(), example_inputs=[])\n        float_input = list(float_holder.graph.findNode(\"aten::to\").inputs())[1]\n        float_node = float_input.node()\n\n        def patch_float(module):\n            try:\n                graphs = [module.graph] if hasattr(module, \"graph\") else []\n            except RuntimeError:\n                graphs = []\n\n            if hasattr(module, \"forward1\"):\n                graphs.append(module.forward1.graph)\n\n            for graph in graphs:\n                for node in graph.findAllNodes(\"aten::to\"):\n                    inputs = list(node.inputs())\n                    for i in [1, 2]:  # dtype can be the second or third argument to aten::to()\n                        if inputs[i].node()[\"value\"] == 5:\n                            inputs[i].node().copyAttributes(float_node)\n\n        model.apply(patch_float)\n        patch_float(model.encode_image)\n        patch_float(model.encode_text)\n        model.float()\n\n    # ensure image_size attr available at consistent location for both jit and non-jit\n    model.visual.image_size = model.input_resolution.item()\n    return model\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/model.py", "content": "\"\"\" CLIP Model\n\nAdapted from https://github.com/openai/CLIP. Originally MIT License, Copyright (c) 2021 OpenAI.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, Union\nfrom functools import partial\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\ntry:\n    from .hf_model import HFTextEncoder\nexcept:\n    HFTextEncoder = None\nfrom .modified_resnet import ModifiedResNet\nfrom .timm_model import TimmModel\nfrom .eva_vit_model import EVAVisionTransformer\nfrom .transformer import LayerNorm, QuickGELU, Attention, VisionTransformer, TextTransformer\n\ntry:\n    from apex.normalization import FusedLayerNorm\nexcept:\n    FusedLayerNorm = LayerNorm\n    # print(\"Please 'pip install apex'\")\n\ntry:\n    import xformers.ops as xops\nexcept ImportError:\n    xops = None\n    # print(\"Please 'pip install xformers'\")\n\n\nclass RMSnorm(nn.Module):\n    \"\"\"\n    adepted from transformers T5LayerNorm\n    \"\"\"\n\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        Construct a layernorm module in the T5 style. No bias and no subtraction of mean.\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        # T5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean\n        # Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated\n        # w/o mean and there is no bias. Additionally we want to make sure that the accumulation for\n        # half-precision inputs is done in fp32\n\n        variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n\n        # convert into half-precision if necessary\n        if self.weight.dtype in [torch.float16, torch.bfloat16]:\n            hidden_states = hidden_states.to(self.weight.dtype)\n\n        return self.weight * hidden_states\n\n\n@dataclass\nclass CLIPVisionCfg:\n    layers: Union[Tuple[int, int, int, int], int] = 12\n    width: int = 768\n    head_width: int = 64\n    mlp_ratio: float = 4.0\n    patch_size: int = 16\n    image_size: Union[Tuple[int, int], int] = 224\n    ls_init_value: Optional[float] = None  # layer scale initial value\n    patch_dropout: float = 0.0  # what fraction of patches to dropout during training (0 would mean disabled and no patches dropped) - 0.5 to 0.75 recommended in the paper for optimal results\n    global_average_pool: bool = False  # whether to global average pool the last embedding layer, instead of using CLS token (https://arxiv.org/abs/2205.01580)\n    drop_path_rate: Optional[float] = None  # drop path rate\n    timm_model_name: str = None  # a valid model name overrides layers, width, patch_size\n    timm_model_pretrained: bool = False  # use (imagenet) pretrained weights for named model\n    timm_pool: str = \"avg\"  # feature pooling for timm model ('abs_attn', 'rot_attn', 'avg', '')\n    timm_proj: str = \"linear\"  # linear projection for timm model output ('linear', 'mlp', '')\n    timm_proj_bias: bool = False  # enable bias final projection\n    eva_model_name: str = None  # a valid eva model name overrides layers, width, patch_size\n    qkv_bias: bool = True\n    fusedLN: bool = False\n    xattn: bool = False\n    postnorm: bool = False\n    rope: bool = False\n    pt_hw_seq_len: int = 16  # 224/14\n    intp_freq: bool = False\n    naiveswiglu: bool = False\n    subln: bool = False\n    use_rms_norm: bool = False\n\n\n@dataclass\nclass CLIPTextCfg:\n    context_length: int = 77\n    vocab_size: int = 49408\n    width: int = 512\n    heads: int = 8\n    layers: int = 12\n    ls_init_value: Optional[float] = None  # layer scale initial value\n    hf_model_name: str = None\n    hf_tokenizer_name: str = None\n    hf_model_pretrained: bool = True\n    proj: str = \"mlp\"\n    pooler_type: str = \"mean_pooler\"\n    masked_language_modeling: bool = False\n    fusedLN: bool = False\n    xattn: bool = False\n    attn_mask: bool = True\n\n\ndef get_cast_dtype(precision: str):\n    cast_dtype = None\n    if precision == \"bf16\":\n        cast_dtype = torch.bfloat16\n    elif precision == \"fp16\":\n        cast_dtype = torch.float16\n    return cast_dtype\n\n\ndef _build_vision_tower(embed_dim: int, vision_cfg: CLIPVisionCfg, quick_gelu: bool = False, cast_dtype: Optional[torch.dtype] = None):\n    if isinstance(vision_cfg, dict):\n        vision_cfg = CLIPVisionCfg(**vision_cfg)\n\n    # OpenAI models are pretrained w/ QuickGELU but native nn.GELU is both faster and more\n    # memory efficient in recent PyTorch releases (>= 1.10).\n    # NOTE: timm models always use native GELU regardless of quick_gelu flag.\n    act_layer = QuickGELU if quick_gelu else nn.GELU\n\n    if vision_cfg.eva_model_name:\n        vision_heads = vision_cfg.width // vision_cfg.head_width\n\n        norm_layer = RMSnorm if vision_cfg.use_rms_norm else LayerNorm\n\n        visual = EVAVisionTransformer(\n            img_size=vision_cfg.image_size,\n            patch_size=vision_cfg.patch_size,\n            num_classes=embed_dim,\n            use_mean_pooling=vision_cfg.global_average_pool,  # False\n            init_values=vision_cfg.ls_init_value,\n            patch_dropout=vision_cfg.patch_dropout,\n            embed_dim=vision_cfg.width,\n            depth=vision_cfg.layers,\n            num_heads=vision_heads,\n            mlp_ratio=vision_cfg.mlp_ratio,\n            qkv_bias=vision_cfg.qkv_bias,\n            drop_path_rate=vision_cfg.drop_path_rate,\n            norm_layer=partial(norm_layer, eps=1e-6),\n            xattn=vision_cfg.xattn,\n            rope=vision_cfg.rope,\n            postnorm=vision_cfg.postnorm,\n            pt_hw_seq_len=vision_cfg.pt_hw_seq_len,  # 224/14\n            intp_freq=vision_cfg.intp_freq,\n            naiveswiglu=vision_cfg.naiveswiglu,\n            subln=vision_cfg.subln,\n        )\n    elif vision_cfg.timm_model_name:\n        visual = TimmModel(\n            vision_cfg.timm_model_name, pretrained=vision_cfg.timm_model_pretrained, pool=vision_cfg.timm_pool, proj=vision_cfg.timm_proj, proj_bias=vision_cfg.timm_proj_bias, embed_dim=embed_dim, image_size=vision_cfg.image_size\n        )\n        act_layer = nn.GELU  # so that text transformer doesn't use QuickGELU w/ timm models\n    elif isinstance(vision_cfg.layers, (tuple, list)):\n        vision_heads = vision_cfg.width * 32 // vision_cfg.head_width\n        visual = ModifiedResNet(layers=vision_cfg.layers, output_dim=embed_dim, heads=vision_heads, image_size=vision_cfg.image_size, width=vision_cfg.width)\n    else:\n        vision_heads = vision_cfg.width // vision_cfg.head_width\n        norm_layer = LayerNormFp32 if cast_dtype in (torch.float16, torch.bfloat16) else LayerNorm\n        visual = VisionTransformer(\n            image_size=vision_cfg.image_size,\n            patch_size=vision_cfg.patch_size,\n            width=vision_cfg.width,\n            layers=vision_cfg.layers,\n            heads=vision_heads,\n            mlp_ratio=vision_cfg.mlp_ratio,\n            ls_init_value=vision_cfg.ls_init_value,\n            patch_dropout=vision_cfg.patch_dropout,\n            global_average_pool=vision_cfg.global_average_pool,\n            output_dim=embed_dim,\n            act_layer=act_layer,\n            norm_layer=norm_layer,\n        )\n\n    return visual\n\n\ndef _build_text_tower(\n    embed_dim: int,\n    text_cfg: CLIPTextCfg,\n    quick_gelu: bool = False,\n    cast_dtype: Optional[torch.dtype] = None,\n):\n    if isinstance(text_cfg, dict):\n        text_cfg = CLIPTextCfg(**text_cfg)\n\n    if text_cfg.hf_model_name:\n        text = HFTextEncoder(text_cfg.hf_model_name, output_dim=embed_dim, tokenizer_name=text_cfg.hf_tokenizer_name, proj=text_cfg.proj, pooler_type=text_cfg.pooler_type, masked_language_modeling=text_cfg.masked_language_modeling)\n    else:\n        act_layer = QuickGELU if quick_gelu else nn.GELU\n        norm_layer = LayerNorm\n\n        text = TextTransformer(\n            context_length=text_cfg.context_length,\n            vocab_size=text_cfg.vocab_size,\n            width=text_cfg.width,\n            heads=text_cfg.heads,\n            layers=text_cfg.layers,\n            ls_init_value=text_cfg.ls_init_value,\n            output_dim=embed_dim,\n            act_layer=act_layer,\n            norm_layer=FusedLayerNorm if text_cfg.fusedLN else norm_layer,\n            xattn=text_cfg.xattn,\n            attn_mask=text_cfg.attn_mask,\n        )\n    return text\n\n\nclass CLIP(nn.Module):\n    def __init__(\n        self,\n        embed_dim: int,\n        vision_cfg: CLIPVisionCfg,\n        text_cfg: CLIPTextCfg,\n        quick_gelu: bool = False,\n        cast_dtype: Optional[torch.dtype] = None,\n    ):\n        super().__init__()\n        self.visual = _build_vision_tower(embed_dim, vision_cfg, quick_gelu, cast_dtype)\n\n        text = _build_text_tower(embed_dim, text_cfg, quick_gelu, cast_dtype)\n        self.transformer = text.transformer\n        self.vocab_size = text.vocab_size\n        self.token_embedding = text.token_embedding\n        self.positional_embedding = text.positional_embedding\n        self.ln_final = text.ln_final\n        self.text_projection = text.text_projection\n        self.register_buffer(\"attn_mask\", text.attn_mask, persistent=False)\n\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n\n    def lock_image_tower(self, unlocked_groups=0, freeze_bn_stats=False):\n        # lock image tower as per LiT - https://arxiv.org/abs/2111.07991\n        self.visual.lock(unlocked_groups=unlocked_groups, freeze_bn_stats=freeze_bn_stats)\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.visual.set_grad_checkpointing(enable)\n        self.transformer.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\"logit_scale\"}\n\n    def encode_image(self, image, normalize: bool = False):\n        features = self.visual(image)\n        return F.normalize(features, dim=-1) if normalize else features\n\n    def encode_text(self, text, normalize: bool = False):\n        cast_dtype = self.transformer.get_cast_dtype()\n\n        x = self.token_embedding(text).to(cast_dtype)  # [batch_size, n_ctx, d_model]\n\n        x = x + self.positional_embedding.to(cast_dtype)\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.transformer(x, attn_mask=self.attn_mask)\n        x = x.permute(1, 0, 2)  # LND -> NLD\n        x = self.ln_final(x)  # [batch_size, n_ctx, transformer.width]\n        # take features from the eot embedding (eot_token is the highest number in each sequence)\n        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n        return F.normalize(x, dim=-1) if normalize else x\n\n    def forward(self, image, text):\n        image_features = self.encode_image(image, normalize=True)\n        text_features = self.encode_text(text, normalize=True)\n        return image_features, text_features, self.logit_scale.exp()\n\n\nclass CustomCLIP(nn.Module):\n    def __init__(\n        self,\n        embed_dim: int,\n        vision_cfg: CLIPVisionCfg,\n        text_cfg: CLIPTextCfg,\n        quick_gelu: bool = False,\n        cast_dtype: Optional[torch.dtype] = None,\n        itm_task: bool = False,\n    ):\n        super().__init__()\n        self.visual = _build_vision_tower(embed_dim, vision_cfg, quick_gelu, cast_dtype)\n        self.text = _build_text_tower(embed_dim, text_cfg, quick_gelu, cast_dtype)\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n\n    def lock_image_tower(self, unlocked_groups=0, freeze_bn_stats=False):\n        # lock image tower as per LiT - https://arxiv.org/abs/2111.07991\n        self.visual.lock(unlocked_groups=unlocked_groups, freeze_bn_stats=freeze_bn_stats)\n\n    def lock_text_tower(self, unlocked_layers: int = 0, freeze_layer_norm: bool = True):\n        self.text.lock(unlocked_layers, freeze_layer_norm)\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.visual.set_grad_checkpointing(enable)\n        self.text.set_grad_checkpointing(enable)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\"logit_scale\"}\n\n    def encode_image(self, image, normalize: bool = False):\n        features = self.visual(image)\n        return F.normalize(features, dim=-1) if normalize else features\n\n    def encode_text(self, text, normalize: bool = False):\n        features = self.text(text)\n        return F.normalize(features, dim=-1) if normalize else features\n\n    def forward(self, image, text):\n        image_features = self.encode_image(image, normalize=True)\n        text_features = self.encode_text(text, normalize=True)\n        return image_features, text_features, self.logit_scale.exp()\n\n\ndef convert_weights_to_lp(model: nn.Module, dtype=torch.float16):\n    \"\"\"Convert applicable model parameters to low-precision (bf16 or fp16)\"\"\"\n\n    def _convert_weights(l):\n\n        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n            l.weight.data = l.weight.data.to(dtype)\n            if l.bias is not None:\n                l.bias.data = l.bias.data.to(dtype)\n\n        if isinstance(l, (nn.MultiheadAttention, Attention)):\n            for attr in [*[f\"{s}_proj_weight\" for s in [\"in\", \"q\", \"k\", \"v\"]], \"in_proj_bias\", \"bias_k\", \"bias_v\"]:\n                tensor = getattr(l, attr, None)\n                if tensor is not None:\n                    tensor.data = tensor.data.to(dtype)\n\n        if isinstance(l, nn.Parameter):\n            l.data = l.data.to(dtype)\n\n        for name in [\"text_projection\", \"proj\"]:\n            if hasattr(l, name) and isinstance(l, nn.Parameter):\n                attr = getattr(l, name, None)\n                if attr is not None:\n                    attr.data = attr.data.to(dtype)\n\n    model.apply(_convert_weights)\n\n\nconvert_weights_to_fp16 = convert_weights_to_lp  # backwards compat\n\n\n# used to maintain checkpoint compatibility\ndef convert_to_custom_text_state_dict(state_dict: dict):\n    if \"text_projection\" in state_dict:\n        # old format state_dict, move text tower -> .text\n        new_state_dict = {}\n        for k, v in state_dict.items():\n            if any(k.startswith(p) for p in (\"text_projection\", \"positional_embedding\", \"token_embedding\", \"transformer\", \"ln_final\", \"logit_scale\")):\n                k = \"text.\" + k\n            new_state_dict[k] = v\n        return new_state_dict\n    return state_dict\n\n\ndef build_model_from_openai_state_dict(\n    state_dict: dict,\n    quick_gelu=True,\n    cast_dtype=torch.float16,\n):\n    vit = \"visual.proj\" in state_dict\n\n    if vit:\n        vision_width = state_dict[\"visual.conv1.weight\"].shape[0]\n        vision_layers = len([k for k in state_dict.keys() if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")])\n        vision_patch_size = state_dict[\"visual.conv1.weight\"].shape[-1]\n        grid_size = round((state_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5)\n        image_size = vision_patch_size * grid_size\n    else:\n        counts: list = [len(set(k.split(\".\")[2] for k in state_dict if k.startswith(f\"visual.layer{b}\"))) for b in [1, 2, 3, 4]]\n        vision_layers = tuple(counts)\n        vision_width = state_dict[\"visual.layer1.0.conv1.weight\"].shape[0]\n        output_width = round((state_dict[\"visual.attnpool.positional_embedding\"].shape[0] - 1) ** 0.5)\n        vision_patch_size = None\n        assert output_width**2 + 1 == state_dict[\"visual.attnpool.positional_embedding\"].shape[0]\n        image_size = output_width * 32\n\n    embed_dim = state_dict[\"text_projection\"].shape[1]\n    context_length = state_dict[\"positional_embedding\"].shape[0]\n    vocab_size = state_dict[\"token_embedding.weight\"].shape[0]\n    transformer_width = state_dict[\"ln_final.weight\"].shape[0]\n    transformer_heads = transformer_width // 64\n    transformer_layers = len(set(k.split(\".\")[2] for k in state_dict if k.startswith(f\"transformer.resblocks\")))\n\n    vision_cfg = CLIPVisionCfg(\n        layers=vision_layers,\n        width=vision_width,\n        patch_size=vision_patch_size,\n        image_size=image_size,\n    )\n    text_cfg = CLIPTextCfg(context_length=context_length, vocab_size=vocab_size, width=transformer_width, heads=transformer_heads, layers=transformer_layers)\n    model = CLIP(\n        embed_dim,\n        vision_cfg=vision_cfg,\n        text_cfg=text_cfg,\n        quick_gelu=quick_gelu,  # OpenAI models were trained with QuickGELU\n        cast_dtype=cast_dtype,\n    )\n\n    for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n        state_dict.pop(key, None)\n\n    convert_weights_to_fp16(model)  # OpenAI state dicts are partially converted to float16\n    model.load_state_dict(state_dict)\n    return model.eval()\n\n\ndef trace_model(model, batch_size=256, device=torch.device(\"cpu\")):\n    model.eval()\n    image_size = model.visual.image_size\n    example_images = torch.ones((batch_size, 3, image_size, image_size), device=device)\n    example_text = torch.zeros((batch_size, model.context_length), dtype=torch.int, device=device)\n    model = torch.jit.trace_module(model, inputs=dict(forward=(example_images, example_text), encode_text=(example_text,), encode_image=(example_images,)))\n    model.visual.image_size = image_size\n    return model\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/factory.py", "content": "import json\nimport logging\nimport os\nimport pathlib\nimport re\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom typing import Optional, Tuple, Union, Dict, Any\nimport torch\n\ntry:\n    import deepspeed\nexcept ImportError:\n    deepspeed = None\n\nfrom .constants import OPENAI_DATASET_MEAN, OPENAI_DATASET_STD\nfrom .model import CLIP, CustomCLIP, convert_weights_to_lp, convert_to_custom_text_state_dict, get_cast_dtype\nfrom .openai import load_openai_model\nfrom .pretrained import is_pretrained_cfg, get_pretrained_cfg, download_pretrained, list_pretrained_tags_by_model\nfrom .transform import image_transform\nfrom .tokenizer import HFTokenizer, tokenize\nfrom .utils import resize_clip_pos_embed, resize_evaclip_pos_embed, resize_visual_pos_embed, resize_eva_pos_embed\n\n\n_MODEL_CONFIG_PATHS = [Path(__file__).parent / f\"model_configs/\"]\n_MODEL_CONFIGS = {}  # directory (model_name: config) of model architecture configs\n\n\ndef _natural_key(string_):\n    return [int(s) if s.isdigit() else s for s in re.split(r\"(\\d+)\", string_.lower())]\n\n\ndef _rescan_model_configs():\n    global _MODEL_CONFIGS\n\n    config_ext = (\".json\",)\n    config_files = []\n    for config_path in _MODEL_CONFIG_PATHS:\n        if config_path.is_file() and config_path.suffix in config_ext:\n            config_files.append(config_path)\n        elif config_path.is_dir():\n            for ext in config_ext:\n                config_files.extend(config_path.glob(f\"*{ext}\"))\n\n    for cf in config_files:\n        with open(cf, \"r\", encoding=\"utf8\") as f:\n            model_cfg = json.load(f)\n            if all(a in model_cfg for a in (\"embed_dim\", \"vision_cfg\", \"text_cfg\")):\n                _MODEL_CONFIGS[cf.stem] = model_cfg\n\n    _MODEL_CONFIGS = dict(sorted(_MODEL_CONFIGS.items(), key=lambda x: _natural_key(x[0])))\n\n\n_rescan_model_configs()  # initial populate of model config registry\n\n\ndef list_models():\n    \"\"\"enumerate available model architectures based on config files\"\"\"\n    return list(_MODEL_CONFIGS.keys())\n\n\ndef add_model_config(path):\n    \"\"\"add model config path or file and update registry\"\"\"\n    if not isinstance(path, Path):\n        path = Path(path)\n    _MODEL_CONFIG_PATHS.append(path)\n    _rescan_model_configs()\n\n\ndef get_model_config(model_name):\n    if model_name in _MODEL_CONFIGS:\n        return deepcopy(_MODEL_CONFIGS[model_name])\n    else:\n        return None\n\n\ndef get_tokenizer(model_name):\n    config = get_model_config(model_name)\n    tokenizer = HFTokenizer(config[\"text_cfg\"][\"hf_tokenizer_name\"]) if \"hf_tokenizer_name\" in config[\"text_cfg\"] else tokenize\n    return tokenizer\n\n\n# loading openai CLIP weights when is_openai=True for training\ndef load_state_dict(checkpoint_path: str, map_location: str = \"cpu\", model_key: str = \"model|module|state_dict\", is_openai: bool = False, skip_list: list = []):\n    if is_openai:\n        model = torch.jit.load(checkpoint_path, map_location=\"cpu\").eval()\n        state_dict = model.state_dict()\n        for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n            state_dict.pop(key, None)\n    else:\n        checkpoint = torch.load(checkpoint_path, map_location=map_location)\n        for mk in model_key.split(\"|\"):\n            if isinstance(checkpoint, dict) and mk in checkpoint:\n                state_dict = checkpoint[mk]\n                break\n            else:\n                state_dict = checkpoint\n        if next(iter(state_dict.items()))[0].startswith(\"module\"):\n            state_dict = {k[7:]: v for k, v in state_dict.items()}\n\n    for k in skip_list:\n        if k in list(state_dict.keys()):\n            logging.info(f\"Removing key {k} from pretrained checkpoint\")\n            del state_dict[k]\n\n    if os.getenv(\"RoPE\") == \"1\":\n        for k in list(state_dict.keys()):\n            if \"freqs_cos\" in k or \"freqs_sin\" in k:\n                del state_dict[k]\n    return state_dict\n\n\ndef load_checkpoint(model, checkpoint_path, model_key=\"model|module|state_dict\", strict=True):\n    state_dict = load_state_dict(checkpoint_path, model_key=model_key, is_openai=False)\n    # detect old format and make compatible with new format\n    if \"positional_embedding\" in state_dict and not hasattr(model, \"positional_embedding\"):\n        state_dict = convert_to_custom_text_state_dict(state_dict)\n    if \"text.logit_scale\" in state_dict and hasattr(model, \"logit_scale\"):\n        state_dict[\"logit_scale\"] = state_dict[\"text.logit_scale\"]\n        del state_dict[\"text.logit_scale\"]\n\n    # resize_clip_pos_embed for CLIP and open CLIP\n    if \"visual.positional_embedding\" in state_dict:\n        resize_clip_pos_embed(state_dict, model)\n    # specified to eva_vit_model\n    elif \"visual.pos_embed\" in state_dict:\n        resize_evaclip_pos_embed(state_dict, model)\n\n    # resize_clip_pos_embed(state_dict, model)\n    incompatible_keys = model.load_state_dict(state_dict, strict=strict)\n    logging.info(f\"incompatible_keys.missing_keys: {incompatible_keys.missing_keys}\")\n    return incompatible_keys\n\n\ndef load_clip_visual_state_dict(checkpoint_path: str, map_location: str = \"cpu\", is_openai: bool = False, skip_list: list = []):\n    state_dict = load_state_dict(checkpoint_path, map_location=map_location, is_openai=is_openai, skip_list=skip_list)\n\n    for k in list(state_dict.keys()):\n        if not k.startswith(\"visual.\"):\n            del state_dict[k]\n    for k in list(state_dict.keys()):\n        if k.startswith(\"visual.\"):\n            new_k = k[7:]\n            state_dict[new_k] = state_dict[k]\n            del state_dict[k]\n    return state_dict\n\n\ndef load_clip_text_state_dict(checkpoint_path: str, map_location: str = \"cpu\", is_openai: bool = False, skip_list: list = []):\n    state_dict = load_state_dict(checkpoint_path, map_location=map_location, is_openai=is_openai, skip_list=skip_list)\n\n    for k in list(state_dict.keys()):\n        if k.startswith(\"visual.\"):\n            del state_dict[k]\n    return state_dict\n\n\ndef get_pretrained_tag(pretrained_model):\n    pretrained_model = pretrained_model.lower()\n    if \"laion\" in pretrained_model or \"open_clip\" in pretrained_model:\n        return \"open_clip\"\n    elif \"openai\" in pretrained_model:\n        return \"clip\"\n    elif \"eva\" in pretrained_model and \"clip\" in pretrained_model:\n        return \"eva_clip\"\n    else:\n        return \"other\"\n\n\ndef load_zero_partitions(model, state_dict, is_deepspeed_zero3_enabled, pretrained_model_path, ignore_mismatched_sizes=False):\n    \"\"\"\n    adept from pytorch lightning and transformers\n    with deepspeed.zero.Init():\n        model = MyModel()\n    state_dict = torch.load(model_path, map_location=\"cpu\")\n    load_zero_partitions(model, prefix=\"\")\n    \"\"\"\n\n    # because zero3 puts placeholders in model params, this context\n    # manager gathers (unpartitions) the params of the current layer, then loads from\n    # the state dict and then re-partitions them again\n    model_state_dict = model.state_dict()\n    expected_keys = list(model_state_dict.keys())\n    loaded_keys = list(state_dict.keys())\n    missing_keys = list(set(expected_keys) - set(loaded_keys))\n    unexpected_keys = list(set(loaded_keys) - set(expected_keys))\n\n    # Mistmatched keys contains tuples key/shape1/shape2 of weights in the checkpoint that have a shape not\n    # matching the weights in the model.\n    mismatched_keys = []\n    if ignore_mismatched_sizes:\n        for checkpoint_key in loaded_keys:\n            model_key = checkpoint_key\n\n            if model_key in model_state_dict and state_dict[checkpoint_key].shape != model_state_dict[model_key].shape:\n                mismatched_keys.append((checkpoint_key, state_dict[checkpoint_key].shape, model_state_dict[model_key].shape))\n                del state_dict[checkpoint_key]\n    # copy state_dict so _load_from_state_dict can modify it\n    metadata = getattr(state_dict, \"_metadata\", None)\n    state_dict = state_dict.copy()\n    if metadata is not None:\n        state_dict._metadata = metadata\n\n    error_msgs = []\n\n    # PyTorch's `_load_from_state_dict` does not copy parameters in a module's descendants\n    # so we need to apply the function recursively.\n    def load(module, prefix=\"\"):\n        local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n        args = (state_dict, prefix, local_metadata, True, [], [], error_msgs)\n        if is_deepspeed_zero3_enabled:\n            # because zero3 puts placeholders in model params, this context\n            # manager gathers (unpartitions) the params of the current layer, then loads from\n            # the state dict and then re-partitions them again\n            with deepspeed.zero.GatheredParameters(list(module.parameters(recurse=False)), modifier_rank=0):\n                if torch.distributed.get_rank() == 0:\n                    module._load_from_state_dict(*args)\n        else:\n            module._load_from_state_dict(*args)\n\n        for name, child in module._modules.items():\n            if child is not None:\n                load(child, prefix + name + \".\")\n\n    # Make sure we are able to load base models as well as derived models (with heads)\n    start_prefix = \"\"\n    model_to_load = model\n    load(model_to_load, prefix=start_prefix)\n    del state_dict\n    if len(error_msgs) > 0:\n        error_msg = \"\\n\\t\".join(error_msgs)\n        if \"size mismatch\" in error_msg:\n            error_msg += \"\\n\\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\"\n        raise RuntimeError(f\"Error(s) in loading state_dict for {model.__class__.__name__}:\\n\\t{error_msg}\")\n    if len(unexpected_keys) > 0:\n        logging.warning(\n            f\"Some weights of the model checkpoint at {pretrained_model_path} were not used when\"\n            f\" initializing {model.__class__.__name__}: {unexpected_keys}\\n- This IS expected if you are\"\n            f\" initializing {model.__class__.__name__} from the checkpoint of a model trained on another task or\"\n            \" with another architecture (e.g. initializing a BertForSequenceClassification model from a\"\n            \" BertForPreTraining model).\\n- This IS NOT expected if you are initializing\"\n            f\" {model.__class__.__name__} from the checkpoint of a model that you expect to be exactly identical\"\n            \" (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\"\n        )\n    else:\n        logging.info(f\"All model checkpoint weights were used when initializing {model.__class__.__name__}.\\n\")\n    if len(missing_keys) > 0:\n        logging.warning(\n            f\"Some weights of {model.__class__.__name__} were not initialized from the model checkpoint at\"\n            f\" {pretrained_model_path} and are newly initialized: {missing_keys}\\nYou should probably\"\n            \" TRAIN this model on a down-stream task to be able to use it for predictions and inference.\"\n        )\n    elif len(mismatched_keys) == 0:\n        logging.info(\n            f\"All the weights of {model.__class__.__name__} were initialized from the model checkpoint at\"\n            f\" {pretrained_model_path}.\\nIf your task is similar to the task the model of the checkpoint\"\n            f\" was trained on, you can already use {model.__class__.__name__} for predictions without further\"\n            \" training.\"\n        )\n    if len(mismatched_keys) > 0:\n        mismatched_warning = \"\\n\".join([f\"- {key}: found shape {shape1} in the checkpoint and {shape2} in the model instantiated\" for key, shape1, shape2 in mismatched_keys])\n        logging.warning(\n            f\"Some weights of {model.__class__.__name__} were not initialized from the model checkpoint at\"\n            f\" {pretrained_model_path} and are newly initialized because the shapes did not\"\n            f\" match:\\n{mismatched_warning}\\nYou should probably TRAIN this model on a down-stream task to be able\"\n            \" to use it for predictions and inference.\"\n        )\n\n\ndef load_pretrained_checkpoint(model, visual_checkpoint_path, text_checkpoint_path, strict=True, visual_model=None, text_model=None, model_key=\"model|module|state_dict\", skip_list=[]):\n    visual_tag = get_pretrained_tag(visual_model)\n    text_tag = get_pretrained_tag(text_model)\n\n    logging.info(f\"num of model state_dict keys: {len(model.state_dict().keys())}\")\n    visual_incompatible_keys, text_incompatible_keys = None, None\n    if visual_checkpoint_path:\n        if visual_tag == \"eva_clip\" or visual_tag == \"open_clip\":\n            visual_state_dict = load_clip_visual_state_dict(visual_checkpoint_path, is_openai=False, skip_list=skip_list)\n        elif visual_tag == \"clip\":\n            visual_state_dict = load_clip_visual_state_dict(visual_checkpoint_path, is_openai=True, skip_list=skip_list)\n        else:\n            visual_state_dict = load_state_dict(visual_checkpoint_path, model_key=model_key, is_openai=False, skip_list=skip_list)\n\n        # resize_clip_pos_embed for CLIP and open CLIP\n        if \"positional_embedding\" in visual_state_dict:\n            resize_visual_pos_embed(visual_state_dict, model)\n        # specified to EVA model\n        elif \"pos_embed\" in visual_state_dict:\n            resize_eva_pos_embed(visual_state_dict, model)\n\n        visual_incompatible_keys = model.visual.load_state_dict(visual_state_dict, strict=strict)\n        logging.info(f\"num of loaded visual_state_dict keys: {len(visual_state_dict.keys())}\")\n        logging.info(f\"visual_incompatible_keys.missing_keys: {visual_incompatible_keys.missing_keys}\")\n\n    if text_checkpoint_path:\n        if text_tag == \"eva_clip\" or text_tag == \"open_clip\":\n            text_state_dict = load_clip_text_state_dict(text_checkpoint_path, is_openai=False, skip_list=skip_list)\n        elif text_tag == \"clip\":\n            text_state_dict = load_clip_text_state_dict(text_checkpoint_path, is_openai=True, skip_list=skip_list)\n        else:\n            text_state_dict = load_state_dict(visual_checkpoint_path, model_key=model_key, is_openai=False, skip_list=skip_list)\n\n        text_incompatible_keys = model.text.load_state_dict(text_state_dict, strict=strict)\n\n        logging.info(f\"num of loaded text_state_dict keys: {len(text_state_dict.keys())}\")\n        logging.info(f\"text_incompatible_keys.missing_keys: {text_incompatible_keys.missing_keys}\")\n\n    return visual_incompatible_keys, text_incompatible_keys\n\n\ndef create_model(\n    model_name: str,\n    pretrained: Optional[str] = None,\n    precision: str = \"fp32\",\n    device: Union[str, torch.device] = \"cpu\",\n    jit: bool = False,\n    force_quick_gelu: bool = False,\n    force_custom_clip: bool = False,\n    force_patch_dropout: Optional[float] = None,\n    pretrained_image: str = \"\",\n    pretrained_text: str = \"\",\n    pretrained_hf: bool = True,\n    pretrained_visual_model: str = None,\n    pretrained_text_model: str = None,\n    cache_dir: Optional[str] = None,\n    skip_list: list = [],\n):\n    model_name = model_name.replace(\"/\", \"-\")  # for callers using old naming with / in ViT names\n    if isinstance(device, str):\n        device = torch.device(device)\n\n    if pretrained and pretrained.lower() == \"openai\":\n        logging.info(f\"Loading pretrained {model_name} from OpenAI.\")\n        model = load_openai_model(\n            model_name,\n            precision=precision,\n            device=device,\n            jit=jit,\n            cache_dir=cache_dir,\n        )\n    else:\n        model_cfg = get_model_config(model_name)\n        if model_cfg is not None:\n            logging.info(f\"Loaded {model_name} model config.\")\n        else:\n            logging.error(f\"Model config for {model_name} not found; available models {list_models()}.\")\n            raise RuntimeError(f\"Model config for {model_name} not found.\")\n\n        if \"rope\" in model_cfg.get(\"vision_cfg\", {}):\n            if model_cfg[\"vision_cfg\"][\"rope\"]:\n                os.environ[\"RoPE\"] = \"1\"\n        else:\n            os.environ[\"RoPE\"] = \"0\"\n\n        if force_quick_gelu:\n            # override for use of QuickGELU on non-OpenAI transformer models\n            model_cfg[\"quick_gelu\"] = True\n\n        if force_patch_dropout is not None:\n            # override the default patch dropout value\n            model_cfg[\"vision_cfg\"][\"patch_dropout\"] = force_patch_dropout\n\n        cast_dtype = get_cast_dtype(precision)\n        custom_clip = model_cfg.pop(\"custom_text\", False) or force_custom_clip or (\"hf_model_name\" in model_cfg[\"text_cfg\"])\n\n        if custom_clip:\n            if \"hf_model_name\" in model_cfg.get(\"text_cfg\", {}):\n                model_cfg[\"text_cfg\"][\"hf_model_pretrained\"] = pretrained_hf\n            model = CustomCLIP(**model_cfg, cast_dtype=cast_dtype)\n        else:\n            model = CLIP(**model_cfg, cast_dtype=cast_dtype)\n\n        pretrained_cfg = {}\n        if pretrained:\n            checkpoint_path = \"\"\n            pretrained_cfg = get_pretrained_cfg(model_name, pretrained)\n            if pretrained_cfg:\n                checkpoint_path = download_pretrained(pretrained_cfg, cache_dir=cache_dir)\n            elif os.path.exists(pretrained):\n                checkpoint_path = pretrained\n\n            if checkpoint_path:\n                logging.info(f\"Loading pretrained {model_name} weights ({pretrained}).\")\n                load_checkpoint(model, checkpoint_path, model_key=\"model|module|state_dict\", strict=False)\n            else:\n                error_str = f\"Pretrained weights ({pretrained}) not found for model {model_name}.\" f\"Available pretrained tags ({list_pretrained_tags_by_model(model_name)}.\"\n                logging.warning(error_str)\n                raise RuntimeError(error_str)\n        else:\n            visual_checkpoint_path = \"\"\n            text_checkpoint_path = \"\"\n\n            if pretrained_image:\n                pretrained_visual_model = pretrained_visual_model.replace(\"/\", \"-\")  # for callers using old naming with / in ViT names\n                pretrained_image_cfg = get_pretrained_cfg(pretrained_visual_model, pretrained_image)\n                if \"timm_model_name\" in model_cfg.get(\"vision_cfg\", {}):\n                    # pretrained weight loading for timm models set via vision_cfg\n                    model_cfg[\"vision_cfg\"][\"timm_model_pretrained\"] = True\n                elif pretrained_image_cfg:\n                    visual_checkpoint_path = download_pretrained(pretrained_image_cfg, cache_dir=cache_dir)\n                elif os.path.exists(pretrained_image):\n                    visual_checkpoint_path = pretrained_image\n                else:\n                    logging.warning(f\"Pretrained weights ({visual_checkpoint_path}) not found for model {model_name}.visual.\")\n                    raise RuntimeError(f\"Pretrained weights ({visual_checkpoint_path}) not found for model {model_name}.visual.\")\n\n            if pretrained_text:\n                pretrained_text_model = pretrained_text_model.replace(\"/\", \"-\")  # for callers using old naming with / in ViT names\n                pretrained_text_cfg = get_pretrained_cfg(pretrained_text_model, pretrained_text)\n                if pretrained_image_cfg:\n                    text_checkpoint_path = download_pretrained(pretrained_text_cfg, cache_dir=cache_dir)\n                elif os.path.exists(pretrained_text):\n                    text_checkpoint_path = pretrained_text\n                else:\n                    logging.warning(f\"Pretrained weights ({text_checkpoint_path}) not found for model {model_name}.text.\")\n                    raise RuntimeError(f\"Pretrained weights ({text_checkpoint_path}) not found for model {model_name}.text.\")\n\n            if visual_checkpoint_path:\n                logging.info(f\"Loading pretrained {model_name}.visual weights ({visual_checkpoint_path}).\")\n            if text_checkpoint_path:\n                logging.info(f\"Loading pretrained {model_name}.text weights ({text_checkpoint_path}).\")\n\n            if visual_checkpoint_path or text_checkpoint_path:\n                load_pretrained_checkpoint(model, visual_checkpoint_path, text_checkpoint_path, strict=False, visual_model=pretrained_visual_model, text_model=pretrained_text_model, model_key=\"model|module|state_dict\", skip_list=skip_list)\n\n        if \"fp16\" in precision or \"bf16\" in precision:\n            logging.info(f\"convert precision to {precision}\")\n            model = model.to(torch.bfloat16) if \"bf16\" in precision else model.to(torch.float16)\n\n        # model.to(device=device)\n\n        # set image / mean metadata from pretrained_cfg if available, or use default\n        model.visual.image_mean = pretrained_cfg.get(\"mean\", None) or OPENAI_DATASET_MEAN\n        model.visual.image_std = pretrained_cfg.get(\"std\", None) or OPENAI_DATASET_STD\n\n        if jit:\n            model = torch.jit.script(model)\n\n    return model\n\n\ndef create_model_and_transforms(\n    model_name: str,\n    pretrained: Optional[str] = None,\n    precision: str = \"fp32\",\n    device: Union[str, torch.device] = \"cpu\",\n    jit: bool = False,\n    force_quick_gelu: bool = False,\n    force_custom_clip: bool = False,\n    force_patch_dropout: Optional[float] = None,\n    pretrained_image: str = \"\",\n    pretrained_text: str = \"\",\n    pretrained_hf: bool = True,\n    pretrained_visual_model: str = None,\n    pretrained_text_model: str = None,\n    image_mean: Optional[Tuple[float, ...]] = None,\n    image_std: Optional[Tuple[float, ...]] = None,\n    cache_dir: Optional[str] = None,\n    skip_list: list = [],\n):\n    model = create_model(\n        model_name,\n        pretrained,\n        precision=precision,\n        device=device,\n        jit=jit,\n        force_quick_gelu=force_quick_gelu,\n        force_custom_clip=force_custom_clip,\n        force_patch_dropout=force_patch_dropout,\n        pretrained_image=pretrained_image,\n        pretrained_text=pretrained_text,\n        pretrained_hf=pretrained_hf,\n        pretrained_visual_model=pretrained_visual_model,\n        pretrained_text_model=pretrained_text_model,\n        cache_dir=cache_dir,\n        skip_list=skip_list,\n    )\n\n    image_mean = image_mean or getattr(model.visual, \"image_mean\", None)\n    image_std = image_std or getattr(model.visual, \"image_std\", None)\n    preprocess_train = image_transform(model.visual.image_size, is_train=True, mean=image_mean, std=image_std)\n    preprocess_val = image_transform(model.visual.image_size, is_train=False, mean=image_mean, std=image_std)\n\n    return model, preprocess_train, preprocess_val\n\n\ndef create_model_from_pretrained(\n    model_name: str,\n    pretrained: str,\n    precision: str = \"fp32\",\n    device: Union[str, torch.device] = \"cpu\",\n    jit: bool = False,\n    force_quick_gelu: bool = False,\n    force_custom_clip: bool = False,\n    force_patch_dropout: Optional[float] = None,\n    return_transform: bool = True,\n    image_mean: Optional[Tuple[float, ...]] = None,\n    image_std: Optional[Tuple[float, ...]] = None,\n    cache_dir: Optional[str] = None,\n    is_frozen: bool = False,\n):\n    if not is_pretrained_cfg(model_name, pretrained) and not os.path.exists(pretrained):\n        raise RuntimeError(f\"{pretrained} is not a valid pretrained cfg or checkpoint for {model_name}.\" f\" Use open_clip.list_pretrained() to find one.\")\n\n    model = create_model(\n        model_name,\n        pretrained,\n        precision=precision,\n        device=device,\n        jit=jit,\n        force_quick_gelu=force_quick_gelu,\n        force_custom_clip=force_custom_clip,\n        force_patch_dropout=force_patch_dropout,\n        cache_dir=cache_dir,\n    )\n\n    if is_frozen:\n        for param in model.parameters():\n            param.requires_grad = False\n\n    if not return_transform:\n        return model\n\n    image_mean = image_mean or getattr(model.visual, \"image_mean\", None)\n    image_std = image_std or getattr(model.visual, \"image_std\", None)\n    preprocess = image_transform(model.visual.image_size, is_train=False, mean=image_mean, std=image_std)\n\n    return model, preprocess\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/hf_model.py", "content": "\"\"\" huggingface model adapter\n\nWraps HuggingFace transformers (https://github.com/huggingface/transformers) models for use as a text tower in CLIP model.\n\"\"\"\n\nimport re\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch import TensorType\n\ntry:\n    import transformers\n    from transformers import AutoModel, AutoModelForMaskedLM, AutoTokenizer, AutoConfig, PretrainedConfig\n    from transformers.modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, BaseModelOutputWithPoolingAndCrossAttentions\nexcept ImportError as e:\n    transformers = None\n\n    class BaseModelOutput:\n        pass\n\n    class PretrainedConfig:\n        pass\n\n\nfrom .hf_configs import arch_dict\n\n\n# utils\ndef _camel2snake(s):\n    return re.sub(r\"(?<!^)(?=[A-Z])\", \"_\", s).lower()\n\n\n# TODO: ?last - for gpt-like models\n_POOLERS = {}\n\n\ndef register_pooler(cls):\n    \"\"\"Decorator registering pooler class\"\"\"\n    _POOLERS[_camel2snake(cls.__name__)] = cls\n    return cls\n\n\n@register_pooler\nclass MeanPooler(nn.Module):\n    \"\"\"Mean pooling\"\"\"\n\n    def forward(self, x: BaseModelOutput, attention_mask: TensorType):\n        masked_output = x.last_hidden_state * attention_mask.unsqueeze(-1)\n        return masked_output.sum(dim=1) / attention_mask.sum(-1, keepdim=True)\n\n\n@register_pooler\nclass MaxPooler(nn.Module):\n    \"\"\"Max pooling\"\"\"\n\n    def forward(self, x: BaseModelOutput, attention_mask: TensorType):\n        masked_output = x.last_hidden_state.masked_fill(attention_mask.unsqueeze(-1), -torch.inf)\n        return masked_output.max(1).values\n\n\n@register_pooler\nclass ClsPooler(nn.Module):\n    \"\"\"CLS token pooling\"\"\"\n\n    def __init__(self, use_pooler_output=True):\n        super().__init__()\n        self.cls_token_position = 0\n        self.use_pooler_output = use_pooler_output\n\n    def forward(self, x: BaseModelOutput, attention_mask: TensorType):\n\n        if self.use_pooler_output and isinstance(x, (BaseModelOutputWithPooling, BaseModelOutputWithPoolingAndCrossAttentions)) and (x.pooler_output is not None):\n            return x.pooler_output\n\n        return x.last_hidden_state[:, self.cls_token_position, :]\n\n\nclass HFTextEncoder(nn.Module):\n    \"\"\"HuggingFace model adapter\"\"\"\n\n    def __init__(self, model_name_or_path: str, output_dim: int, tokenizer_name: str = None, config: PretrainedConfig = None, pooler_type: str = None, proj: str = None, pretrained: bool = True, masked_language_modeling: bool = False):\n        super().__init__()\n\n        self.output_dim = output_dim\n\n        # TODO: find better way to get this information\n        uses_transformer_pooler = pooler_type == \"cls_pooler\"\n\n        if transformers is None:\n            raise RuntimeError(\"Please `pip install transformers` to use pre-trained HuggingFace models\")\n        if config is None:\n            self.config = AutoConfig.from_pretrained(model_name_or_path)\n            if masked_language_modeling:\n                create_func, model_args = (AutoModelForMaskedLM.from_pretrained, model_name_or_path) if pretrained else (AutoModelForMaskedLM.from_config, self.config)\n            else:\n                create_func, model_args = (AutoModel.from_pretrained, model_name_or_path) if pretrained else (AutoModel.from_config, self.config)\n            # TODO: do all model configs have this attribute? PretrainedConfig does so yes??\n            if hasattr(self.config, \"is_encoder_decoder\") and self.config.is_encoder_decoder:\n                self.transformer = create_func(model_args)\n                self.transformer = self.transformer.encoder\n            else:\n                self.transformer = create_func(model_args, add_pooling_layer=uses_transformer_pooler)\n        else:\n            self.config = config\n            if masked_language_modeling:\n                self.transformer = AutoModelForMaskedLM.from_config(config)\n            else:\n                self.transformer = AutoModel.from_config(config)\n\n        if pooler_type is None:  # get default arch pooler\n            self.pooler = _POOLERS[(arch_dict[self.config.model_type][\"pooler\"])]()\n        else:\n            self.pooler = _POOLERS[pooler_type]()\n\n        d_model = getattr(self.config, arch_dict[self.config.model_type][\"config_names\"][\"width\"])\n        if (d_model == output_dim) and (proj is None):  # do we always need a proj?\n            self.proj = nn.Identity()\n        elif proj == \"linear\":\n            self.proj = nn.Linear(d_model, output_dim, bias=False)\n        elif proj == \"mlp\":\n            hidden_size = (d_model + output_dim) // 2\n            self.proj = nn.Sequential(\n                nn.Linear(d_model, hidden_size, bias=False),\n                nn.GELU(),\n                nn.Linear(hidden_size, output_dim, bias=False),\n            )\n\n        # self.itm_proj = nn.Linear(d_model, 2, bias=False)\n        # self.mlm_proj = nn.Linear(d_model, self.config.vocab_size), bias=False)\n        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n\n    # def forward_itm(self, x:TensorType, image_embeds:TensorType) -> TensorType:\n    #     image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(x.device)\n    #     attn_mask = (x != self.config.pad_token_id).long()\n    #     out = self.transformer(\n    #         input_ids=x,\n    #         attention_mask=attn_mask,\n    #         encoder_hidden_states = image_embeds,\n    #         encoder_attention_mask = image_atts,\n    #         )\n    #     pooled_out = self.pooler(out, attn_mask)\n\n    #     return self.itm_proj(pooled_out)\n\n    def mask(self, input_ids, vocab_size, device, targets=None, masked_indices=None, probability_matrix=None):\n        if masked_indices is None:\n            masked_indices = torch.bernoulli(probability_matrix).bool()\n\n        masked_indices[input_ids == self.tokenizer.pad_token_id] = False\n        masked_indices[input_ids == self.tokenizer.cls_token_id] = False\n\n        if targets is not None:\n            targets[~masked_indices] = -100  # We only compute loss on masked tokens\n\n        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n        indices_replaced = torch.bernoulli(torch.full(input_ids.shape, 0.8)).bool() & masked_indices\n        input_ids[indices_replaced] = self.tokenizer.mask_token_id\n\n        # 10% of the time, we replace masked input tokens with random word\n        indices_random = torch.bernoulli(torch.full(input_ids.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n        random_words = torch.randint(vocab_size, input_ids.shape, dtype=torch.long).to(device)\n        input_ids[indices_random] = random_words[indices_random]\n        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n\n        if targets is not None:\n            return input_ids, targets\n        else:\n            return input_ids\n\n    def forward_mlm(self, input_ids, image_embeds, mlm_probability=0.25):\n        labels = input_ids.clone()\n        attn_mask = (input_ids != self.config.pad_token_id).long()\n        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(input_ids.device)\n        vocab_size = getattr(self.config, arch_dict[self.config.model_type][\"config_names\"][\"vocab_size\"])\n        probability_matrix = torch.full(labels.shape, mlm_probability)\n        input_ids, labels = self.mask(input_ids, vocab_size, input_ids.device, targets=labels, probability_matrix=probability_matrix)\n        mlm_output = self.transformer(\n            input_ids,\n            attention_mask=attn_mask,\n            encoder_hidden_states=image_embeds,\n            encoder_attention_mask=image_atts,\n            return_dict=True,\n            labels=labels,\n        )\n        return mlm_output.loss\n        # mlm_output = self.transformer(input_ids,\n        #                 attention_mask = attn_mask,\n        #                 encoder_hidden_states = image_embeds,\n        #                 encoder_attention_mask = image_atts,\n        #                 return_dict = True,\n        #             ).last_hidden_state\n        # logits = self.mlm_proj(mlm_output)\n\n        # # logits = logits[:, :-1, :].contiguous().view(-1, vocab_size)\n        # logits = logits[:, 1:, :].contiguous().view(-1, vocab_size)\n        # labels = labels[:, 1:].contiguous().view(-1)\n\n        # mlm_loss = F.cross_entropy(\n        #     logits,\n        #     labels,\n        #     # label_smoothing=0.1,\n        # )\n        # return mlm_loss\n\n    def forward(self, x: TensorType) -> TensorType:\n        attn_mask = (x != self.config.pad_token_id).long()\n        out = self.transformer(input_ids=x, attention_mask=attn_mask)\n        pooled_out = self.pooler(out, attn_mask)\n\n        return self.proj(pooled_out)\n\n    def lock(self, unlocked_layers: int = 0, freeze_layer_norm: bool = True):\n        if not unlocked_layers:  # full freezing\n            for n, p in self.transformer.named_parameters():\n                p.requires_grad = (not freeze_layer_norm) if \"LayerNorm\" in n.split(\".\") else False\n            return\n\n        encoder = self.transformer.encoder if hasattr(self.transformer, \"encoder\") else self.transformer\n        layer_list = getattr(encoder, arch_dict[self.config.model_type][\"config_names\"][\"layer_attr\"])\n        print(f\"Unlocking {unlocked_layers}/{len(layer_list) + 1} layers of hf model\")\n        embeddings = getattr(self.transformer, arch_dict[self.config.model_type][\"config_names\"][\"token_embeddings_attr\"])\n        modules = [embeddings, *layer_list][:-unlocked_layers]\n        # freeze layers\n        for module in modules:\n            for n, p in module.named_parameters():\n                p.requires_grad = (not freeze_layer_norm) if \"LayerNorm\" in n.split(\".\") else False\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.transformer.gradient_checkpointing_enable()\n\n    def get_num_layers(self):\n        encoder = self.transformer.encoder if hasattr(self.transformer, \"encoder\") else self.transformer\n        layer_list = getattr(encoder, arch_dict[self.config.model_type][\"config_names\"][\"layer_attr\"])\n        return len(layer_list)\n\n    def init_parameters(self):\n        pass\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/hf_configs.py", "content": "# HF architecture dict:\narch_dict = {\n    # https://huggingface.co/docs/transformers/model_doc/roberta#roberta\n    \"roberta\": {\n        \"config_names\": {\n            \"context_length\": \"max_position_embeddings\",\n            \"vocab_size\": \"vocab_size\",\n            \"width\": \"hidden_size\",\n            \"heads\": \"num_attention_heads\",\n            \"layers\": \"num_hidden_layers\",\n            \"layer_attr\": \"layer\",\n            \"token_embeddings_attr\": \"embeddings\",\n        },\n        \"pooler\": \"mean_pooler\",\n    },\n    # https://huggingface.co/docs/transformers/model_doc/xlm-roberta#transformers.XLMRobertaConfig\n    \"xlm-roberta\": {\n        \"config_names\": {\n            \"context_length\": \"max_position_embeddings\",\n            \"vocab_size\": \"vocab_size\",\n            \"width\": \"hidden_size\",\n            \"heads\": \"num_attention_heads\",\n            \"layers\": \"num_hidden_layers\",\n            \"layer_attr\": \"layer\",\n            \"token_embeddings_attr\": \"embeddings\",\n        },\n        \"pooler\": \"mean_pooler\",\n    },\n    # https://huggingface.co/docs/transformers/model_doc/mt5#mt5\n    \"mt5\": {\n        \"config_names\": {\n            # unlimited seqlen\n            # https://github.com/google-research/text-to-text-transfer-transformer/issues/273\n            # https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/t5/modeling_t5.py#L374\n            \"context_length\": \"\",\n            \"vocab_size\": \"vocab_size\",\n            \"width\": \"d_model\",\n            \"heads\": \"num_heads\",\n            \"layers\": \"num_layers\",\n            \"layer_attr\": \"block\",\n            \"token_embeddings_attr\": \"embed_tokens\",\n        },\n        \"pooler\": \"mean_pooler\",\n    },\n    \"bert\": {\n        \"config_names\": {\n            \"context_length\": \"max_position_embeddings\",\n            \"vocab_size\": \"vocab_size\",\n            \"width\": \"hidden_size\",\n            \"heads\": \"num_attention_heads\",\n            \"layers\": \"num_hidden_layers\",\n            \"layer_attr\": \"layer\",\n            \"token_embeddings_attr\": \"embeddings\",\n        },\n        \"pooler\": \"mean_pooler\",\n    },\n}\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/eva_vit_model.py", "content": "# --------------------------------------------------------\n# Adapted from  https://github.com/microsoft/unilm/tree/master/beit\n# --------------------------------------------------------\nimport math\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntry:\n    from timm.models.layers import drop_path, to_2tuple, trunc_normal_\nexcept:\n    from timm.layers import drop_path, to_2tuple, trunc_normal_\n\nfrom .transformer import PatchDropout\nfrom .rope import VisionRotaryEmbedding, VisionRotaryEmbeddingFast\n\nif os.getenv(\"ENV_TYPE\") == \"deepspeed\":\n    try:\n        from deepspeed.runtime.activation_checkpointing.checkpointing import checkpoint\n    except:\n        from torch.utils.checkpoint import checkpoint\nelse:\n    from torch.utils.checkpoint import checkpoint\n\ntry:\n    import xformers.ops as xops\nexcept ImportError:\n    xops = None\n    # print(\"Please 'pip install xformers'\")\n\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n\n    def extra_repr(self) -> str:\n        return \"p={}\".format(self.drop_prob)\n\n\nclass Mlp(nn.Module):\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        drop=0.0,\n        subln=False,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n\n        self.ffn_ln = norm_layer(hidden_features) if subln else nn.Identity()\n\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        # x = self.drop(x)\n        # commit this for the orignal BERT implement\n        x = self.ffn_ln(x)\n\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass SwiGLU(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.SiLU, drop=0.0, norm_layer=nn.LayerNorm, subln=False):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n\n        self.w1 = nn.Linear(in_features, hidden_features)\n        self.w2 = nn.Linear(in_features, hidden_features)\n\n        self.act = act_layer()\n        self.ffn_ln = norm_layer(hidden_features) if subln else nn.Identity()\n        self.w3 = nn.Linear(hidden_features, out_features)\n\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x1 = self.w1(x)\n        x2 = self.w2(x)\n        hidden = self.act(x1) * x2\n        x = self.ffn_ln(hidden)\n        x = self.w3(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.0, proj_drop=0.0, window_size=None, attn_head_dim=None, xattn=False, rope=None, subln=False, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        if attn_head_dim is not None:\n            head_dim = attn_head_dim\n        all_head_dim = head_dim * self.num_heads\n        self.scale = qk_scale or head_dim**-0.5\n\n        self.subln = subln\n        if self.subln:\n            self.q_proj = nn.Linear(dim, all_head_dim, bias=False)\n            self.k_proj = nn.Linear(dim, all_head_dim, bias=False)\n            self.v_proj = nn.Linear(dim, all_head_dim, bias=False)\n        else:\n            self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n\n        if qkv_bias:\n            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n        else:\n            self.q_bias = None\n            self.v_bias = None\n\n        if window_size:\n            self.window_size = window_size\n            self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n            self.relative_position_bias_table = nn.Parameter(torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n            # cls to token & token 2 cls & cls to cls\n\n            # get pair-wise relative position index for each token inside the window\n            coords_h = torch.arange(window_size[0])\n            coords_w = torch.arange(window_size[1])\n            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n            relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n            relative_coords[:, :, 1] += window_size[1] - 1\n            relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n            relative_position_index = torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n            relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n            relative_position_index[0, 0:] = self.num_relative_distance - 3\n            relative_position_index[0:, 0] = self.num_relative_distance - 2\n            relative_position_index[0, 0] = self.num_relative_distance - 1\n\n            self.register_buffer(\"relative_position_index\", relative_position_index)\n        else:\n            self.window_size = None\n            self.relative_position_bias_table = None\n            self.relative_position_index = None\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.inner_attn_ln = norm_layer(all_head_dim) if subln else nn.Identity()\n        # self.proj = nn.Linear(all_head_dim, all_head_dim)\n        self.proj = nn.Linear(all_head_dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.xattn = xattn\n        self.xattn_drop = attn_drop\n\n        self.rope = rope\n\n    def forward(self, x, rel_pos_bias=None, attn_mask=None):\n        B, N, C = x.shape\n        if self.subln:\n            q = F.linear(input=x, weight=self.q_proj.weight, bias=self.q_bias)\n            k = F.linear(input=x, weight=self.k_proj.weight, bias=None)\n            v = F.linear(input=x, weight=self.v_proj.weight, bias=self.v_bias)\n\n            q = q.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)  # B, num_heads, N, C\n            k = k.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n            v = v.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n        else:\n\n            qkv_bias = None\n            if self.q_bias is not None:\n                qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n\n            qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n            qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)  # 3, B, num_heads, N, C\n            q, k, v = qkv[0], qkv[1], qkv[2]\n\n        if self.rope:\n            # slightly fast impl\n            q_t = q[:, :, 1:, :]\n            ro_q_t = self.rope(q_t)\n            q = torch.cat((q[:, :, :1, :], ro_q_t), -2).type_as(v)\n\n            k_t = k[:, :, 1:, :]\n            ro_k_t = self.rope(k_t)\n            k = torch.cat((k[:, :, :1, :], ro_k_t), -2).type_as(v)\n\n        if self.xattn:\n            q = q.permute(0, 2, 1, 3)  # B, num_heads, N, C -> B, N, num_heads, C\n            k = k.permute(0, 2, 1, 3)\n            v = v.permute(0, 2, 1, 3)\n\n            x = xops.memory_efficient_attention(\n                q,\n                k,\n                v,\n                p=self.xattn_drop,\n                scale=self.scale,\n            )\n            x = x.reshape(B, N, -1)\n            x = self.inner_attn_ln(x)\n            x = self.proj(x)\n            x = self.proj_drop(x)\n        else:\n            q = q * self.scale\n            attn = q @ k.transpose(-2, -1)\n\n            if self.relative_position_bias_table is not None:\n                relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(self.window_size[0] * self.window_size[1] + 1, self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n                relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n                attn = attn + relative_position_bias.unsqueeze(0).type_as(attn)\n\n            if rel_pos_bias is not None:\n                attn = attn + rel_pos_bias.type_as(attn)\n\n            if attn_mask is not None:\n                attn_mask = attn_mask.bool()\n                attn = attn.masked_fill(~attn_mask[:, None, None, :], float(\"-inf\"))\n\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n\n            x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n            x = self.inner_attn_ln(x)\n            x = self.proj(x)\n            x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        init_values=None,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        window_size=None,\n        attn_head_dim=None,\n        xattn=False,\n        rope=None,\n        postnorm=False,\n        subln=False,\n        naiveswiglu=False,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim, xattn=xattn, rope=rope, subln=subln, norm_layer=norm_layer\n        )\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n\n        if naiveswiglu:\n            self.mlp = SwiGLU(\n                in_features=dim,\n                hidden_features=mlp_hidden_dim,\n                subln=subln,\n                norm_layer=norm_layer,\n            )\n        else:\n            self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, subln=subln, drop=drop)\n\n        if init_values is not None and init_values > 0:\n            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n        else:\n            self.gamma_1, self.gamma_2 = None, None\n\n        self.postnorm = postnorm\n\n    def forward(self, x, rel_pos_bias=None, attn_mask=None):\n        if self.gamma_1 is None:\n            if self.postnorm:\n                x = x + self.drop_path(self.norm1(self.attn(x, rel_pos_bias=rel_pos_bias, attn_mask=attn_mask)))\n                x = x + self.drop_path(self.norm2(self.mlp(x)))\n            else:\n                x = x + self.drop_path(self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias, attn_mask=attn_mask))\n                x = x + self.drop_path(self.mlp(self.norm2(x)))\n        else:\n            if self.postnorm:\n                x = x + self.drop_path(self.gamma_1 * self.norm1(self.attn(x, rel_pos_bias=rel_pos_bias, attn_mask=attn_mask)))\n                x = x + self.drop_path(self.gamma_2 * self.norm2(self.mlp(x)))\n            else:\n                x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias, attn_mask=attn_mask))\n                x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n        return x\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\"Image to Patch Embedding\"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x, **kwargs):\n        B, C, H, W = x.shape\n        # FIXME look at relaxing size constraints\n        assert H == self.img_size[0] and W == self.img_size[1], f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x\n\n\nclass RelativePositionBias(nn.Module):\n\n    def __init__(self, window_size, num_heads):\n        super().__init__()\n        self.window_size = window_size\n        self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n        self.relative_position_bias_table = nn.Parameter(torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n        # cls to token & token 2 cls & cls to cls\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(window_size[0])\n        coords_w = torch.arange(window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n        relative_position_index = torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n        relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        relative_position_index[0, 0:] = self.num_relative_distance - 3\n        relative_position_index[0:, 0] = self.num_relative_distance - 2\n        relative_position_index[0, 0] = self.num_relative_distance - 1\n\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n    def forward(self):\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(self.window_size[0] * self.window_size[1] + 1, self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n        return relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n\n\nclass EVAVisionTransformer(nn.Module):\n    \"\"\"Vision Transformer with support for patch or hybrid CNN input stage\"\"\"\n\n    def __init__(\n        self,\n        img_size=224,\n        patch_size=16,\n        in_chans=3,\n        num_classes=1000,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.0,\n        norm_layer=nn.LayerNorm,\n        init_values=None,\n        patch_dropout=0.0,\n        use_abs_pos_emb=True,\n        use_rel_pos_bias=False,\n        use_shared_rel_pos_bias=False,\n        rope=False,\n        use_mean_pooling=True,\n        init_scale=0.001,\n        grad_checkpointing=False,\n        xattn=False,\n        postnorm=False,\n        pt_hw_seq_len=16,\n        intp_freq=False,\n        naiveswiglu=False,\n        subln=False,\n    ):\n        super().__init__()\n        self.image_size = img_size\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n\n        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        # self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        if use_abs_pos_emb:\n            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        else:\n            self.pos_embed = None\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        if use_shared_rel_pos_bias:\n            self.rel_pos_bias = RelativePositionBias(window_size=self.patch_embed.patch_shape, num_heads=num_heads)\n        else:\n            self.rel_pos_bias = None\n\n        if rope:\n            half_head_dim = embed_dim // num_heads // 2\n            hw_seq_len = img_size // patch_size\n            self.rope = VisionRotaryEmbeddingFast(\n                dim=half_head_dim,\n                pt_seq_len=pt_hw_seq_len,\n                ft_seq_len=hw_seq_len if intp_freq else None,\n                # patch_dropout=patch_dropout\n            )\n        else:\n            self.rope = None\n\n        self.naiveswiglu = naiveswiglu\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        self.use_rel_pos_bias = use_rel_pos_bias\n        self.blocks = nn.ModuleList(\n            [\n                Block(\n                    dim=embed_dim,\n                    num_heads=num_heads,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    drop=drop_rate,\n                    attn_drop=attn_drop_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                    init_values=init_values,\n                    window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None,\n                    xattn=xattn,\n                    rope=self.rope,\n                    postnorm=postnorm,\n                    subln=subln,\n                    naiveswiglu=naiveswiglu,\n                )\n                for i in range(depth)\n            ]\n        )\n        self.norm = nn.Identity() if use_mean_pooling else norm_layer(embed_dim)\n        self.fc_norm = norm_layer(embed_dim) if use_mean_pooling else None\n        self.head = nn.Linear(embed_dim, num_classes, bias=qkv_bias) if num_classes > 0 else nn.Identity()\n\n        if self.pos_embed is not None:\n            trunc_normal_(self.pos_embed, std=0.02)\n\n        trunc_normal_(self.cls_token, std=0.02)\n\n        self.apply(self._init_weights)\n        self.fix_init_weight()\n\n        if isinstance(self.head, nn.Linear):\n            trunc_normal_(self.head.weight, std=0.02)\n            self.head.weight.data.mul_(init_scale)\n            if self.head.bias is not None:\n                self.head.bias.data.mul_(init_scale)\n\n        # setting a patch_dropout of 0. would mean it is disabled and this function would be the identity fn\n        self.patch_dropout = PatchDropout(patch_dropout) if patch_dropout > 0.0 else nn.Identity()\n\n        self.grad_checkpointing = grad_checkpointing\n\n    def fix_init_weight(self):\n        def rescale(param, layer_id):\n            param.div_(math.sqrt(2.0 * layer_id))\n\n        for layer_id, layer in enumerate(self.blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            if self.naiveswiglu:\n                rescale(layer.mlp.w3.weight.data, layer_id + 1)\n            else:\n                rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n\n    def get_cast_dtype(self) -> torch.dtype:\n        return self.blocks[0].mlp.fc2.weight.dtype\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def get_num_layers(self):\n        return len(self.blocks)\n\n    def lock(self, unlocked_groups=0, freeze_bn_stats=False):\n        assert unlocked_groups == 0, \"partial locking not currently supported for this model\"\n        for param in self.parameters():\n            param.requires_grad = False\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\"pos_embed\", \"cls_token\"}\n\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=\"\"):\n        self.num_classes = num_classes\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x, return_all_features=False):\n\n        x = self.patch_embed(x)\n        batch_size, seq_len, _ = x.size()\n\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        x = torch.cat((cls_tokens, x), dim=1)\n        if self.pos_embed is not None:\n            x = x + self.pos_embed\n        x = self.pos_drop(x)\n\n        # a patch_dropout of 0. would mean it is disabled and this function would do nothing but return what was passed in\n        # if os.getenv(\"RoPE\") == \"1\":\n        #     if self.training and not isinstance(self.patch_dropout, nn.Identity):\n        #         x, patch_indices_keep = self.patch_dropout(x)\n        #         self.rope.forward = partial(self.rope.forward, patch_indices_keep=patch_indices_keep)\n        #     else:\n        #         self.rope.forward = partial(self.rope.forward, patch_indices_keep=None)\n        #         x = self.patch_dropout(x)\n        # else:\n        x = self.patch_dropout(x)\n\n        rel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None\n        for blk in self.blocks:\n            if self.grad_checkpointing:\n                x = checkpoint(blk, x, (rel_pos_bias,))\n            else:\n                x = blk(x, rel_pos_bias=rel_pos_bias)\n\n        if not return_all_features:\n            x = self.norm(x)\n            if self.fc_norm is not None:\n                return self.fc_norm(x.mean(1))\n            else:\n                return x[:, 0]\n        return x\n\n    def forward(self, x, return_all_features=False):\n        if return_all_features:\n            return self.forward_features(x, return_all_features)\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/rope.py", "content": "from math import pi\nimport torch\nfrom torch import nn\nfrom einops import rearrange, repeat\nimport logging\n\n\ndef broadcat(tensors, dim=-1):\n    num_tensors = len(tensors)\n    shape_lens = set(list(map(lambda t: len(t.shape), tensors)))\n    assert len(shape_lens) == 1, \"tensors must all have the same number of dimensions\"\n    shape_len = list(shape_lens)[0]\n    dim = (dim + shape_len) if dim < 0 else dim\n    dims = list(zip(*map(lambda t: list(t.shape), tensors)))\n    expandable_dims = [(i, val) for i, val in enumerate(dims) if i != dim]\n    assert all([*map(lambda t: len(set(t[1])) <= 2, expandable_dims)]), \"invalid dimensions for broadcastable concatentation\"\n    max_dims = list(map(lambda t: (t[0], max(t[1])), expandable_dims))\n    expanded_dims = list(map(lambda t: (t[0], (t[1],) * num_tensors), max_dims))\n    expanded_dims.insert(dim, (dim, dims[dim]))\n    expandable_shapes = list(zip(*map(lambda t: t[1], expanded_dims)))\n    tensors = list(map(lambda t: t[0].expand(*t[1]), zip(tensors, expandable_shapes)))\n    return torch.cat(tensors, dim=dim)\n\n\ndef rotate_half(x):\n    x = rearrange(x, \"... (d r) -> ... d r\", r=2)\n    x1, x2 = x.unbind(dim=-1)\n    x = torch.stack((-x2, x1), dim=-1)\n    return rearrange(x, \"... d r -> ... (d r)\")\n\n\nclass VisionRotaryEmbedding(nn.Module):\n    def __init__(\n        self,\n        dim,\n        pt_seq_len,\n        ft_seq_len=None,\n        custom_freqs=None,\n        freqs_for=\"lang\",\n        theta=10000,\n        max_freq=10,\n        num_freqs=1,\n    ):\n        super().__init__()\n        if custom_freqs:\n            freqs = custom_freqs\n        elif freqs_for == \"lang\":\n            freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n        elif freqs_for == \"pixel\":\n            freqs = torch.linspace(1.0, max_freq / 2, dim // 2) * pi\n        elif freqs_for == \"constant\":\n            freqs = torch.ones(num_freqs).float()\n        else:\n            raise ValueError(f\"unknown modality {freqs_for}\")\n\n        if ft_seq_len is None:\n            ft_seq_len = pt_seq_len\n        t = torch.arange(ft_seq_len) / ft_seq_len * pt_seq_len\n\n        freqs_h = torch.einsum(\"..., f -> ... f\", t, freqs)\n        freqs_h = repeat(freqs_h, \"... n -> ... (n r)\", r=2)\n\n        freqs_w = torch.einsum(\"..., f -> ... f\", t, freqs)\n        freqs_w = repeat(freqs_w, \"... n -> ... (n r)\", r=2)\n\n        freqs = broadcat((freqs_h[:, None, :], freqs_w[None, :, :]), dim=-1)\n\n        self.register_buffer(\"freqs_cos\", freqs.cos())\n        self.register_buffer(\"freqs_sin\", freqs.sin())\n\n        logging.info(f\"Shape of rope freq: {self.freqs_cos.shape}\")\n\n    def forward(self, t, start_index=0):\n        rot_dim = self.freqs_cos.shape[-1]\n        end_index = start_index + rot_dim\n        assert rot_dim <= t.shape[-1], f\"feature dimension {t.shape[-1]} is not of sufficient size to rotate in all the positions {rot_dim}\"\n        t_left, t, t_right = t[..., :start_index], t[..., start_index:end_index], t[..., end_index:]\n        t = (t * self.freqs_cos) + (rotate_half(t) * self.freqs_sin)\n\n        return torch.cat((t_left, t, t_right), dim=-1)\n\n\nclass VisionRotaryEmbeddingFast(nn.Module):\n    def __init__(self, dim, pt_seq_len, ft_seq_len=None, custom_freqs=None, freqs_for=\"lang\", theta=10000, max_freq=10, num_freqs=1, patch_dropout=0.0):\n        super().__init__()\n        if custom_freqs:\n            freqs = custom_freqs\n        elif freqs_for == \"lang\":\n            freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n        elif freqs_for == \"pixel\":\n            freqs = torch.linspace(1.0, max_freq / 2, dim // 2) * pi\n        elif freqs_for == \"constant\":\n            freqs = torch.ones(num_freqs).float()\n        else:\n            raise ValueError(f\"unknown modality {freqs_for}\")\n\n        if ft_seq_len is None:\n            ft_seq_len = pt_seq_len\n        t = torch.arange(ft_seq_len) / ft_seq_len * pt_seq_len\n\n        freqs = torch.einsum(\"..., f -> ... f\", t, freqs)\n        freqs = repeat(freqs, \"... n -> ... (n r)\", r=2)\n        freqs = broadcat((freqs[:, None, :], freqs[None, :, :]), dim=-1)\n\n        freqs_cos = freqs.cos().view(-1, freqs.shape[-1])\n        freqs_sin = freqs.sin().view(-1, freqs.shape[-1])\n\n        self.patch_dropout = patch_dropout\n\n        self.register_buffer(\"freqs_cos\", freqs_cos)\n        self.register_buffer(\"freqs_sin\", freqs_sin)\n\n        logging.info(f\"Shape of rope freq: {self.freqs_cos.shape}\")\n\n    def forward(self, t, patch_indices_keep=None):\n        if patch_indices_keep is not None:\n            batch = t.size()[0]\n            batch_indices = torch.arange(batch)\n            batch_indices = batch_indices[..., None]\n\n            freqs_cos = repeat(self.freqs_cos, \"i j -> n i m j\", n=t.shape[0], m=t.shape[1])\n            freqs_sin = repeat(self.freqs_sin, \"i j -> n i m j\", n=t.shape[0], m=t.shape[1])\n\n            freqs_cos = freqs_cos[batch_indices, patch_indices_keep]\n            freqs_cos = rearrange(freqs_cos, \"n i m j -> n m i j\")\n            freqs_sin = freqs_sin[batch_indices, patch_indices_keep]\n            freqs_sin = rearrange(freqs_sin, \"n i m j -> n m i j\")\n\n            return t * freqs_cos + rotate_half(t) * freqs_sin\n\n        return t * self.freqs_cos + rotate_half(t) * self.freqs_sin\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/constants.py", "content": "OPENAI_DATASET_MEAN = (0.48145466, 0.4578275, 0.40821073)\nOPENAI_DATASET_STD = (0.26862954, 0.26130258, 0.27577711)\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/__init__.py", "content": "from .constants import OPENAI_DATASET_MEAN, OPENAI_DATASET_STD\nfrom .factory import create_model, create_model_and_transforms, create_model_from_pretrained, get_tokenizer\nfrom .factory import list_models, add_model_config, get_model_config, load_checkpoint\nfrom .loss import ClipLoss\nfrom .model import CLIP, CustomCLIP, CLIPTextCfg, CLIPVisionCfg, convert_weights_to_lp, convert_weights_to_fp16, trace_model, get_cast_dtype\nfrom .openai import load_openai_model, list_openai_models\nfrom .pretrained import list_pretrained, list_pretrained_models_by_tag, list_pretrained_tags_by_model, get_pretrained_url, download_pretrained_from_url, is_pretrained_cfg, get_pretrained_cfg, download_pretrained\nfrom .tokenizer import SimpleTokenizer, tokenize\nfrom .transform import image_transform\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/pretrained.py", "content": "import hashlib\nimport os\nimport urllib\nimport warnings\nfrom typing import Dict, Union\n\nfrom tqdm import tqdm\n\ntry:\n    from huggingface_hub import hf_hub_download\n\n    _has_hf_hub = True\nexcept ImportError:\n    hf_hub_download = None\n    _has_hf_hub = False\n\n\ndef _pcfg(url=\"\", hf_hub=\"\", filename=\"\", mean=None, std=None):\n    return dict(\n        url=url,\n        hf_hub=hf_hub,\n        mean=mean,\n        std=std,\n    )\n\n\n_VITB32 = dict(\n    openai=_pcfg(\"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\"),\n    laion400m_e31=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-quickgelu-laion400m_e31-d867053b.pt\"),\n    laion400m_e32=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-quickgelu-laion400m_e32-46683a32.pt\"),\n    laion2b_e16=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-laion2b_e16-af8dbd0c.pth\"),\n    laion2b_s34b_b79k=_pcfg(hf_hub=\"laion/CLIP-ViT-B-32-laion2B-s34B-b79K/\"),\n)\n\n_VITB32_quickgelu = dict(\n    openai=_pcfg(\"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\"),\n    laion400m_e31=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-quickgelu-laion400m_e31-d867053b.pt\"),\n    laion400m_e32=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-quickgelu-laion400m_e32-46683a32.pt\"),\n)\n\n_VITB16 = dict(\n    openai=_pcfg(\"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\"),\n    laion400m_e31=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_16-laion400m_e31-00efa78f.pt\"),\n    laion400m_e32=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_16-laion400m_e32-55e67d44.pt\"),\n    laion2b_s34b_b88k=_pcfg(hf_hub=\"laion/CLIP-ViT-B-16-laion2B-s34B-b88K/\"),\n)\n\n_EVAB16 = dict(\n    eva=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_B_psz14to16.pt\"),\n    eva02=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_B_psz14to16.pt\"),\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_B_psz16_s8B.pt\"),\n    eva02_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_B_psz16_s8B.pt\"),\n)\n\n_VITB16_PLUS_240 = dict(\n    laion400m_e31=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_16_plus_240-laion400m_e31-8fb26589.pt\"),\n    laion400m_e32=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_16_plus_240-laion400m_e32-699c4b84.pt\"),\n)\n\n_VITL14 = dict(\n    openai=_pcfg(\"https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt\"),\n    laion400m_e31=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_l_14-laion400m_e31-69988bb6.pt\"),\n    laion400m_e32=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_l_14-laion400m_e32-3d133497.pt\"),\n    laion2b_s32b_b82k=_pcfg(hf_hub=\"laion/CLIP-ViT-L-14-laion2B-s32B-b82K/\", mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n)\n\n_EVAL14 = dict(\n    eva=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_L_psz14.pt\"),\n    eva02=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_L_psz14.pt\"),\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_L_psz14_s4B.pt\"),\n    eva02_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_L_psz14_s4B.pt\"),\n)\n\n_VITL14_336 = dict(\n    openai=_pcfg(\"https://openaipublic.azureedge.net/clip/models/3035c92b350959924f9f00213499208652fc7ea050643e8b385c2dac08641f02/ViT-L-14-336px.pt\"),\n)\n\n_EVAL14_336 = dict(\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_L_336_psz14_s6B.pt\"),\n    eva02_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_L_336_psz14_s6B.pt\"),\n    eva_clip_224to336=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_L_psz14_224to336.pt\"),\n    eva02_clip_224to336=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_L_psz14_224to336.pt\"),\n)\n\n_VITH14 = dict(\n    laion2b_s32b_b79k=_pcfg(hf_hub=\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K/\"),\n)\n\n_VITg14 = dict(\n    laion2b_s12b_b42k=_pcfg(hf_hub=\"laion/CLIP-ViT-g-14-laion2B-s12B-b42K/\"),\n    laion2b_s34b_b88k=_pcfg(hf_hub=\"laion/CLIP-ViT-g-14-laion2B-s34B-b88K/\"),\n)\n\n_EVAg14 = dict(\n    eva=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/\"),\n    eva01=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA01_g_psz14.pt\"),\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA01_CLIP_g_14_psz14_s11B.pt\"),\n    eva01_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA01_CLIP_g_14_psz14_s11B.pt\"),\n)\n\n_EVAg14_PLUS = dict(\n    eva=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/\"),\n    eva01=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA01_g_psz14.pt\"),\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA01_CLIP_g_14_plus_psz14_s11B.pt\"),\n    eva01_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA01_CLIP_g_14_plus_psz14_s11B.pt\"),\n)\n\n_VITbigG14 = dict(\n    laion2b_s39b_b160k=_pcfg(hf_hub=\"laion/CLIP-ViT-bigG-14-laion2B-39B-b160k/\"),\n)\n\n_EVAbigE14 = dict(\n    eva=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_E_psz14.pt\"),\n    eva02=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_E_psz14.pt\"),\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_E_psz14_s4B.pt\"),\n    eva02_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_E_psz14_s4B.pt\"),\n)\n\n_EVAbigE14_PLUS = dict(\n    eva=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_E_psz14.pt\"),\n    eva02=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_E_psz14.pt\"),\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_E_psz14_plus_s9B.pt\"),\n    eva02_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_E_psz14_plus_s9B.pt\"),\n)\n\n_EVA_8B = dict(\n    eva=_pcfg(hf_hub=\"BAAI/EVA-CLIP-8B/EVA_8B_psz14.bin\"),\n    eva_clip=_pcfg(hf_hub=\"BAAI/EVA-CLIP-8B/EVA_CLIP_8B_psz14_s9B.pt\"),\n)\n\n_EVA_8B_PLUS = dict(\n    eva_clip=_pcfg(hf_hub=\"BAAI/EVA-CLIP-8B-448/EVA_CLIP_8B_psz14_plus_s0.6B.pt\"),\n)\n\n\n_PRETRAINED = {\n    # \"ViT-B-32\": _VITB32,\n    \"OpenaiCLIP-B-32\": _VITB32,\n    \"OpenCLIP-B-32\": _VITB32,\n    # \"ViT-B-32-quickgelu\": _VITB32_quickgelu,\n    \"OpenaiCLIP-B-32-quickgelu\": _VITB32_quickgelu,\n    \"OpenCLIP-B-32-quickgelu\": _VITB32_quickgelu,\n    # \"ViT-B-16\": _VITB16,\n    \"OpenaiCLIP-B-16\": _VITB16,\n    \"OpenCLIP-B-16\": _VITB16,\n    \"EVA02-B-16\": _EVAB16,\n    \"EVA02-CLIP-B-16\": _EVAB16,\n    # \"ViT-B-16-plus-240\": _VITB16_PLUS_240,\n    \"OpenCLIP-B-16-plus-240\": _VITB16_PLUS_240,\n    # \"ViT-L-14\": _VITL14,\n    \"OpenaiCLIP-L-14\": _VITL14,\n    \"OpenCLIP-L-14\": _VITL14,\n    \"EVA02-L-14\": _EVAL14,\n    \"EVA02-CLIP-L-14\": _EVAL14,\n    # \"ViT-L-14-336\": _VITL14_336,\n    \"OpenaiCLIP-L-14-336\": _VITL14_336,\n    \"EVA02-CLIP-L-14-336\": _EVAL14_336,\n    # \"ViT-H-14\": _VITH14,\n    # \"ViT-g-14\": _VITg14,\n    \"OpenCLIP-H-14\": _VITH14,\n    \"OpenCLIP-g-14\": _VITg14,\n    \"EVA01-CLIP-g-14\": _EVAg14,\n    \"EVA01-CLIP-g-14-plus\": _EVAg14_PLUS,\n    # \"ViT-bigG-14\": _VITbigG14,\n    \"OpenCLIP-bigG-14\": _VITbigG14,\n    \"EVA02-CLIP-bigE-14\": _EVAbigE14,\n    \"EVA02-CLIP-bigE-14-plus\": _EVAbigE14_PLUS,\n    \"EVA-CLIP-8B\": _EVA_8B,\n    \"EVA-CLIP-8B-448\": _EVA_8B_PLUS,\n    \"EVA-CLIP-8B-plus\": _EVA_8B_PLUS,\n}\n\n\ndef _clean_tag(tag: str):\n    # normalize pretrained tags\n    return tag.lower().replace(\"-\", \"_\")\n\n\ndef list_pretrained(as_str: bool = False):\n    \"\"\"returns list of pretrained models\n    Returns a tuple (model_name, pretrain_tag) by default or 'name:tag' if as_str == True\n    \"\"\"\n    return [\":\".join([k, t]) if as_str else (k, t) for k in _PRETRAINED.keys() for t in _PRETRAINED[k].keys()]\n\n\ndef list_pretrained_models_by_tag(tag: str):\n    \"\"\"return all models having the specified pretrain tag\"\"\"\n    models = []\n    tag = _clean_tag(tag)\n    for k in _PRETRAINED.keys():\n        if tag in _PRETRAINED[k]:\n            models.append(k)\n    return models\n\n\ndef list_pretrained_tags_by_model(model: str):\n    \"\"\"return all pretrain tags for the specified model architecture\"\"\"\n    tags = []\n    if model in _PRETRAINED:\n        tags.extend(_PRETRAINED[model].keys())\n    return tags\n\n\ndef is_pretrained_cfg(model: str, tag: str):\n    if model not in _PRETRAINED:\n        return False\n    return _clean_tag(tag) in _PRETRAINED[model]\n\n\ndef get_pretrained_cfg(model: str, tag: str):\n    if model not in _PRETRAINED:\n        return {}\n    model_pretrained = _PRETRAINED[model]\n    return model_pretrained.get(_clean_tag(tag), {})\n\n\ndef get_pretrained_url(model: str, tag: str):\n    cfg = get_pretrained_cfg(model, _clean_tag(tag))\n    return cfg.get(\"url\", \"\")\n\n\ndef download_pretrained_from_url(\n    url: str,\n    cache_dir: Union[str, None] = None,\n):\n    if not cache_dir:\n        cache_dir = os.path.expanduser(\"~/.cache/clip\")\n    os.makedirs(cache_dir, exist_ok=True)\n    filename = os.path.basename(url)\n\n    if \"openaipublic\" in url:\n        expected_sha256 = url.split(\"/\")[-2]\n    elif \"mlfoundations\" in url:\n        expected_sha256 = os.path.splitext(filename)[0].split(\"-\")[-1]\n    else:\n        expected_sha256 = \"\"\n\n    download_target = os.path.join(cache_dir, filename)\n\n    if os.path.exists(download_target) and not os.path.isfile(download_target):\n        raise RuntimeError(f\"{download_target} exists and is not a regular file\")\n\n    if os.path.isfile(download_target):\n        if expected_sha256:\n            if hashlib.sha256(open(download_target, \"rb\").read()).hexdigest().startswith(expected_sha256):\n                return download_target\n            else:\n                warnings.warn(f\"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file\")\n        else:\n            return download_target\n\n    with urllib.request.urlopen(url) as source, open(download_target, \"wb\") as output:\n        with tqdm(total=int(source.headers.get(\"Content-Length\")), ncols=80, unit=\"iB\", unit_scale=True) as loop:\n            while True:\n                buffer = source.read(8192)\n                if not buffer:\n                    break\n\n                output.write(buffer)\n                loop.update(len(buffer))\n\n    if expected_sha256 and not hashlib.sha256(open(download_target, \"rb\").read()).hexdigest().startswith(expected_sha256):\n        raise RuntimeError(f\"Model has been downloaded but the SHA256 checksum does not not match\")\n\n    return download_target\n\n\ndef has_hf_hub(necessary=False):\n    if not _has_hf_hub and necessary:\n        # if no HF Hub module installed, and it is necessary to continue, raise error\n        raise RuntimeError(\"Hugging Face hub model specified but package not installed. Run `pip install huggingface_hub`.\")\n    return _has_hf_hub\n\n\ndef download_pretrained_from_hf(\n    model_id: str,\n    filename: str = \"open_clip_pytorch_model.bin\",\n    revision=None,\n    cache_dir: Union[str, None] = None,\n):\n    has_hf_hub(True)\n    cached_file = hf_hub_download(model_id, filename, revision=revision, cache_dir=cache_dir)\n    return cached_file\n\n\ndef download_pretrained(\n    cfg: Dict,\n    force_hf_hub: bool = False,\n    cache_dir: Union[str, None] = None,\n):\n    target = \"\"\n    if not cfg:\n        return target\n\n    download_url = cfg.get(\"url\", \"\")\n    download_hf_hub = cfg.get(\"hf_hub\", \"\")\n    if download_hf_hub and force_hf_hub:\n        # use HF hub even if url exists\n        download_url = \"\"\n\n    if download_url:\n        target = download_pretrained_from_url(download_url, cache_dir=cache_dir)\n    elif download_hf_hub:\n        has_hf_hub(True)\n        # we assume the hf_hub entries in pretrained config combine model_id + filename in\n        # 'org/model_name/filename.pt' form. To specify just the model id w/o filename and\n        # use 'open_clip_pytorch_model.bin' default, there must be a trailing slash 'org/model_name/'.\n        model_id, filename = os.path.split(download_hf_hub)\n        if filename:\n            target = download_pretrained_from_hf(model_id, filename=filename, cache_dir=cache_dir)\n        else:\n            target = download_pretrained_from_hf(model_id, cache_dir=cache_dir)\n\n    return target\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/clip_encoder.py", "content": "import torch\nimport torch.nn as nn\nfrom llava.utils import rank0_print\nfrom transformers import CLIPVisionModel, CLIPImageProcessor, CLIPVisionConfig\n\ntry:\n    from s2wrapper import forward as multiscale_forward\nexcept:\n    pass\n\n\nclass CLIPVisionTower(nn.Module):\n    def __init__(self, vision_tower, args, delay_load=False):\n        super().__init__()\n\n        self.is_loaded = False\n\n        self.vision_tower_name = vision_tower\n        self.select_layer = args.mm_vision_select_layer\n        self.select_feature = getattr(args, \"mm_vision_select_feature\", \"patch\")\n\n        if not delay_load:\n            rank0_print(f\"Loading vision tower: {vision_tower}\")\n            self.load_model()\n        elif getattr(args, \"unfreeze_mm_vision_tower\", False):\n            # TODO: better detector is needed.\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `unfreeze_mm_vision_tower`: True.\")\n            self.load_model()\n        elif hasattr(args, \"mm_tunable_parts\") and \"mm_vision_tower\" in args.mm_tunable_parts:\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `mm_tunable_parts` contains `mm_vision_tower`.\")\n            self.load_model()\n        else:\n            self.cfg_only = CLIPVisionConfig.from_pretrained(self.vision_tower_name)\n\n    def load_model(self, device_map=None):\n        if self.is_loaded:\n            rank0_print(\"{} is already loaded, `load_model` called again, skipping.\".format(self.vision_tower_name))\n            return\n        print(self.vision_tower_name)\n        self.image_processor = CLIPImageProcessor.from_pretrained(self.vision_tower_name)\n        self.vision_tower = CLIPVisionModel.from_pretrained(self.vision_tower_name, device_map=device_map)\n        self.vision_tower.requires_grad_(False)\n\n        self.is_loaded = True\n\n    def feature_select(self, image_forward_outs):\n        select_feature_type = self.select_feature\n\n        if self.select_feature in [\"slicefour_patch\", \"slicefour_cls_patch\"]:\n            select_every_k_layer = len(image_forward_outs.hidden_states) // 4\n            image_features = torch.cat([image_forward_outs.hidden_states[i] for i in range(select_every_k_layer + self.select_layer, len(image_forward_outs.hidden_states), select_every_k_layer)], dim=-1)\n            select_feature_type = select_feature_type.replace(\"slicefour_\", \"\")\n        elif self.select_feature in [\"slice_m25811_f6_patch\", \"slice_m25811_f6_cls_patch\"]:\n            select_layers = [-2, -5, -8, -11, 6]\n            image_features = torch.cat([image_forward_outs.hidden_states[i] for i in select_layers], dim=-1)\n            select_feature_type = select_feature_type.replace(\"slice_m25811_f6_\", \"\")\n        else:\n            image_features = image_forward_outs.hidden_states[self.select_layer]\n\n        if select_feature_type == \"patch\":\n            image_features = image_features[:, 1:]\n        elif select_feature_type == \"cls_patch\":\n            image_features = image_features\n        else:\n            raise ValueError(f\"Unexpected select feature: {select_feature_type}\")\n        return image_features\n\n    def forward(self, images):\n        if type(images) is list:\n            image_features = []\n            for image in images:\n                image_forward_out = self.vision_tower(image.to(device=self.device, dtype=self.dtype).unsqueeze(0), output_hidden_states=True)\n                image_feature = self.feature_select(image_forward_out).to(image.dtype)\n                image_features.append(image_feature)\n        else:\n            image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\n            image_features = self.feature_select(image_forward_outs).to(images.dtype)\n\n        return image_features\n\n    @property\n    def dummy_feature(self):\n        return torch.zeros(1, self.hidden_size, device=self.device, dtype=self.dtype)\n\n    @property\n    def dtype(self):\n        return self.vision_tower.dtype\n\n    @property\n    def device(self):\n        return self.vision_tower.device\n\n    @property\n    def config(self):\n        if self.is_loaded:\n            return self.vision_tower.config\n        else:\n            return self.cfg_only\n\n    @property\n    def hidden_size(self):\n        _hidden_size = self.config.hidden_size\n        if \"slicefour\" in self.select_feature:\n            _hidden_size *= 4\n        if \"slice_m25811_f6\" in self.select_feature:\n            _hidden_size *= 5\n        return _hidden_size\n\n    @property\n    def num_patches_per_side(self):\n        return self.config.image_size // self.config.patch_size\n\n    @property\n    def num_patches(self):\n        _num_patches = (self.config.image_size // self.config.patch_size) ** 2\n        if \"cls_patch\" in self.select_feature:\n            _num_patches += 1\n        return _num_patches\n\n    @property\n    def image_size(self):\n        return self.config.image_size\n\n\nclass CLIPVisionTowerS2(CLIPVisionTower):\n    def __init__(self, vision_tower, args, delay_load=False):\n\n        self.s2_scales = getattr(args, \"s2_scales\", \"336,672,1008\")\n        self.s2_scales = list(map(int, self.s2_scales.split(\",\")))\n        self.s2_scales.sort()\n        self.s2_split_size = self.s2_scales[0]\n        self.s2_image_size = self.s2_scales[-1]\n\n        super().__init__(vision_tower, args, delay_load)\n\n        # change resize/crop size in preprocessing to the largest image size in s2_scale\n        if not delay_load or getattr(args, \"unfreeze_mm_vision_tower\", False):\n            self.image_processor.size[\"shortest_edge\"] = self.s2_image_size\n            self.image_processor.crop_size[\"height\"] = self.image_processor.crop_size[\"width\"] = self.s2_image_size\n\n    def load_model(self, device_map=None):\n        if self.is_loaded:\n            rank0_print(\"{} is already loaded, `load_model` called again, skipping.\".format(self.vision_tower_name))\n            return\n\n        self.image_processor = CLIPImageProcessor.from_pretrained(self.vision_tower_name)\n        print(self.vision_tower_name)\n        self.vision_tower = CLIPVisionModel.from_pretrained(self.vision_tower_name, device_map=device_map)\n        self.vision_tower.requires_grad_(False)\n\n        self.image_processor.size[\"shortest_edge\"] = self.s2_image_size\n        self.image_processor.crop_size[\"height\"] = self.image_processor.crop_size[\"width\"] = self.s2_image_size\n\n        self.is_loaded = True\n\n    def forward_feature(self, images):\n        image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\n        image_features = self.feature_select(image_forward_outs).to(images.dtype)\n        return image_features\n\n    def forward(self, images):\n        if type(images) is list:\n            image_features = []\n            for image in images:\n                image_feature = multiscale_forward(self.forward_feature, image.unsqueeze(0), img_sizes=self.s2_scales, max_split_size=self.s2_split_size, split_forward=True)\n                image_features.append(image_feature)\n        else:\n            image_features = multiscale_forward(self.forward_feature, images, img_sizes=self.s2_scales, max_split_size=self.s2_split_size, split_forward=True)\n\n        return image_features\n\n    @property\n    def hidden_size(self):\n        return self.config.hidden_size * len(self.s2_scales)\n"}
