{"repo_info": {"repo_name": "BlueSuffix", "repo_owner": "Vinsonzyh", "repo_url": "https://github.com/Vinsonzyh/BlueSuffix"}}
{"type": "test_file", "path": "LLaVA/llava/serve/test_message.py", "content": "import argparse\nimport json\n\nimport requests\n\nfrom llava.conversation import default_conversation\n\n\ndef main():\n    if args.worker_address:\n        worker_addr = args.worker_address\n    else:\n        controller_addr = args.controller_address\n        ret = requests.post(controller_addr + \"/refresh_all_workers\")\n        ret = requests.post(controller_addr + \"/list_models\")\n        models = ret.json()[\"models\"]\n        models.sort()\n        print(f\"Models: {models}\")\n\n        ret = requests.post(controller_addr + \"/get_worker_address\",\n            json={\"model\": args.model_name})\n        worker_addr = ret.json()[\"address\"]\n        print(f\"worker_addr: {worker_addr}\")\n\n    if worker_addr == \"\":\n        return\n\n    conv = default_conversation.copy()\n    conv.append_message(conv.roles[0], args.message)\n    prompt = conv.get_prompt()\n\n    headers = {\"User-Agent\": \"LLaVA Client\"}\n    pload = {\n        \"model\": args.model_name,\n        \"prompt\": prompt,\n        \"max_new_tokens\": args.max_new_tokens,\n        \"temperature\": 0.7,\n        \"stop\": conv.sep,\n    }\n    response = requests.post(worker_addr + \"/worker_generate_stream\", headers=headers,\n            json=pload, stream=True)\n\n    print(prompt.replace(conv.sep, \"\\n\"), end=\"\")\n    for chunk in response.iter_lines(chunk_size=8192, decode_unicode=False, delimiter=b\"\\0\"):\n        if chunk:\n            data = json.loads(chunk.decode(\"utf-8\"))\n            output = data[\"text\"].split(conv.sep)[-1]\n            print(output, end=\"\\r\")\n    print(\"\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--controller-address\", type=str, default=\"http://localhost:21001\")\n    parser.add_argument(\"--worker-address\", type=str)\n    parser.add_argument(\"--model-name\", type=str, default=\"facebook/opt-350m\")\n    parser.add_argument(\"--max-new-tokens\", type=int, default=32)\n    parser.add_argument(\"--message\", type=str, default=\n        \"Tell me a story with more than 1000 words.\")\n    args = parser.parse_args()\n\n    main()\n"}
{"type": "source_file", "path": "Image_Purifier/guided_diffusion/train_util.py", "content": "# ---------------------------------------------------------------\n# Taken from the following link as is from:\n# https://github.com/openai/guided-diffusion/blob/main/guided_diffusion/train_util.py\n#\n# The license for the original version of this file can be\n# found in this directory (LICENSE_GUIDED_DIFFUSION).\n# ---------------------------------------------------------------\n\nimport copy\nimport functools\nimport os\n\nimport blobfile as bf\nimport torch as th\nimport torch.distributed as dist\nfrom torch.nn.parallel.distributed import DistributedDataParallel as DDP\nfrom torch.optim import AdamW\n\nfrom . import dist_util, logger\nfrom .fp16_util import MixedPrecisionTrainer\nfrom .nn import update_ema\nfrom .resample import LossAwareSampler, UniformSampler\n\n# For ImageNet experiments, this was a good default value.\n# We found that the lg_loss_scale quickly climbed to\n# 20-21 within the first ~1K steps of training.\nINITIAL_LOG_LOSS_SCALE = 20.0\n\n\nclass TrainLoop:\n    def __init__(\n        self,\n        *,\n        model,\n        diffusion,\n        data,\n        batch_size,\n        microbatch,\n        lr,\n        ema_rate,\n        log_interval,\n        save_interval,\n        resume_checkpoint,\n        use_fp16=False,\n        fp16_scale_growth=1e-3,\n        schedule_sampler=None,\n        weight_decay=0.0,\n        lr_anneal_steps=0,\n    ):\n        self.model = model\n        self.diffusion = diffusion\n        self.data = data\n        self.batch_size = batch_size\n        self.microbatch = microbatch if microbatch > 0 else batch_size\n        self.lr = lr\n        self.ema_rate = (\n            [ema_rate]\n            if isinstance(ema_rate, float)\n            else [float(x) for x in ema_rate.split(\",\")]\n        )\n        self.log_interval = log_interval\n        self.save_interval = save_interval\n        self.resume_checkpoint = resume_checkpoint\n        self.use_fp16 = use_fp16\n        self.fp16_scale_growth = fp16_scale_growth\n        self.schedule_sampler = schedule_sampler or UniformSampler(diffusion)\n        self.weight_decay = weight_decay\n        self.lr_anneal_steps = lr_anneal_steps\n\n        self.step = 0\n        self.resume_step = 0\n        self.global_batch = self.batch_size * dist.get_world_size()\n\n        self.sync_cuda = th.cuda.is_available()\n\n        self._load_and_sync_parameters()\n        self.mp_trainer = MixedPrecisionTrainer(\n            model=self.model,\n            use_fp16=self.use_fp16,\n            fp16_scale_growth=fp16_scale_growth,\n        )\n\n        self.opt = AdamW(\n            self.mp_trainer.master_params, lr=self.lr, weight_decay=self.weight_decay\n        )\n        if self.resume_step:\n            self._load_optimizer_state()\n            # Model was resumed, either due to a restart or a checkpoint\n            # being specified at the command line.\n            self.ema_params = [\n                self._load_ema_parameters(rate) for rate in self.ema_rate\n            ]\n        else:\n            self.ema_params = [\n                copy.deepcopy(self.mp_trainer.master_params)\n                for _ in range(len(self.ema_rate))\n            ]\n\n        if th.cuda.is_available():\n            self.use_ddp = True\n            self.ddp_model = DDP(\n                self.model,\n                device_ids=[dist_util.dev()],\n                output_device=dist_util.dev(),\n                broadcast_buffers=False,\n                bucket_cap_mb=128,\n                find_unused_parameters=False,\n            )\n        else:\n            if dist.get_world_size() > 1:\n                logger.warn(\n                    \"Distributed training requires CUDA. \"\n                    \"Gradients will not be synchronized properly!\"\n                )\n            self.use_ddp = False\n            self.ddp_model = self.model\n\n    def _load_and_sync_parameters(self):\n        resume_checkpoint = find_resume_checkpoint() or self.resume_checkpoint\n\n        if resume_checkpoint:\n            self.resume_step = parse_resume_step_from_filename(resume_checkpoint)\n            if dist.get_rank() == 0:\n                logger.log(f\"loading model from checkpoint: {resume_checkpoint}...\")\n                self.model.load_state_dict(\n                    dist_util.load_state_dict(\n                        resume_checkpoint, map_location=dist_util.dev()\n                    )\n                )\n\n        dist_util.sync_params(self.model.parameters())\n\n    def _load_ema_parameters(self, rate):\n        ema_params = copy.deepcopy(self.mp_trainer.master_params)\n\n        main_checkpoint = find_resume_checkpoint() or self.resume_checkpoint\n        ema_checkpoint = find_ema_checkpoint(main_checkpoint, self.resume_step, rate)\n        if ema_checkpoint:\n            if dist.get_rank() == 0:\n                logger.log(f\"loading EMA from checkpoint: {ema_checkpoint}...\")\n                state_dict = dist_util.load_state_dict(\n                    ema_checkpoint, map_location=dist_util.dev()\n                )\n                ema_params = self.mp_trainer.state_dict_to_master_params(state_dict)\n\n        dist_util.sync_params(ema_params)\n        return ema_params\n\n    def _load_optimizer_state(self):\n        main_checkpoint = find_resume_checkpoint() or self.resume_checkpoint\n        opt_checkpoint = bf.join(\n            bf.dirname(main_checkpoint), f\"opt{self.resume_step:06}.pt\"\n        )\n        if bf.exists(opt_checkpoint):\n            logger.log(f\"loading optimizer state from checkpoint: {opt_checkpoint}\")\n            state_dict = dist_util.load_state_dict(\n                opt_checkpoint, map_location=dist_util.dev()\n            )\n            self.opt.load_state_dict(state_dict)\n\n    def run_loop(self):\n        while (\n            not self.lr_anneal_steps\n            or self.step + self.resume_step < self.lr_anneal_steps\n        ):\n            batch, cond = next(self.data)\n            self.run_step(batch, cond)\n            if self.step % self.log_interval == 0:\n                logger.dumpkvs()\n            if self.step % self.save_interval == 0:\n                self.save()\n                # Run for a finite amount of time in integration tests.\n                if os.environ.get(\"DIFFUSION_TRAINING_TEST\", \"\") and self.step > 0:\n                    return\n            self.step += 1\n        # Save the last checkpoint if it wasn't already saved.\n        if (self.step - 1) % self.save_interval != 0:\n            self.save()\n\n    def run_step(self, batch, cond):\n        self.forward_backward(batch, cond)\n        took_step = self.mp_trainer.optimize(self.opt)\n        if took_step:\n            self._update_ema()\n        self._anneal_lr()\n        self.log_step()\n\n    def forward_backward(self, batch, cond):\n        self.mp_trainer.zero_grad()\n        for i in range(0, batch.shape[0], self.microbatch):\n            micro = batch[i : i + self.microbatch].to(dist_util.dev())\n            micro_cond = {\n                k: v[i : i + self.microbatch].to(dist_util.dev())\n                for k, v in cond.items()\n            }\n            last_batch = (i + self.microbatch) >= batch.shape[0]\n            t, weights = self.schedule_sampler.sample(micro.shape[0], dist_util.dev())\n\n            compute_losses = functools.partial(\n                self.diffusion.training_losses,\n                self.ddp_model,\n                micro,\n                t,\n                model_kwargs=micro_cond,\n            )\n\n            if last_batch or not self.use_ddp:\n                losses = compute_losses()\n            else:\n                with self.ddp_model.no_sync():\n                    losses = compute_losses()\n\n            if isinstance(self.schedule_sampler, LossAwareSampler):\n                self.schedule_sampler.update_with_local_losses(\n                    t, losses[\"loss\"].detach()\n                )\n\n            loss = (losses[\"loss\"] * weights).mean()\n            log_loss_dict(\n                self.diffusion, t, {k: v * weights for k, v in losses.items()}\n            )\n            self.mp_trainer.backward(loss)\n\n    def _update_ema(self):\n        for rate, params in zip(self.ema_rate, self.ema_params):\n            update_ema(params, self.mp_trainer.master_params, rate=rate)\n\n    def _anneal_lr(self):\n        if not self.lr_anneal_steps:\n            return\n        frac_done = (self.step + self.resume_step) / self.lr_anneal_steps\n        lr = self.lr * (1 - frac_done)\n        for param_group in self.opt.param_groups:\n            param_group[\"lr\"] = lr\n\n    def log_step(self):\n        logger.logkv(\"step\", self.step + self.resume_step)\n        logger.logkv(\"samples\", (self.step + self.resume_step + 1) * self.global_batch)\n\n    def save(self):\n        def save_checkpoint(rate, params):\n            state_dict = self.mp_trainer.master_params_to_state_dict(params)\n            if dist.get_rank() == 0:\n                logger.log(f\"saving model {rate}...\")\n                if not rate:\n                    filename = f\"model{(self.step+self.resume_step):06d}.pt\"\n                else:\n                    filename = f\"ema_{rate}_{(self.step+self.resume_step):06d}.pt\"\n                with bf.BlobFile(bf.join(get_blob_logdir(), filename), \"wb\") as f:\n                    th.save(state_dict, f)\n\n        save_checkpoint(0, self.mp_trainer.master_params)\n        for rate, params in zip(self.ema_rate, self.ema_params):\n            save_checkpoint(rate, params)\n\n        if dist.get_rank() == 0:\n            with bf.BlobFile(\n                bf.join(get_blob_logdir(), f\"opt{(self.step+self.resume_step):06d}.pt\"),\n                \"wb\",\n            ) as f:\n                th.save(self.opt.state_dict(), f)\n\n        dist.barrier()\n\n\ndef parse_resume_step_from_filename(filename):\n    \"\"\"\n    Parse filenames of the form path/to/modelNNNNNN.pt, where NNNNNN is the\n    checkpoint's number of steps.\n    \"\"\"\n    split = filename.split(\"model\")\n    if len(split) < 2:\n        return 0\n    split1 = split[-1].split(\".\")[0]\n    try:\n        return int(split1)\n    except ValueError:\n        return 0\n\n\ndef get_blob_logdir():\n    # You can change this to be a separate path to save checkpoints to\n    # a blobstore or some external drive.\n    return logger.get_dir()\n\n\ndef find_resume_checkpoint():\n    # On your infrastructure, you may want to override this to automatically\n    # discover the latest checkpoint on your blob storage, etc.\n    return None\n\n\ndef find_ema_checkpoint(main_checkpoint, step, rate):\n    if main_checkpoint is None:\n        return None\n    filename = f\"ema_{rate}_{(step):06d}.pt\"\n    path = bf.join(bf.dirname(main_checkpoint), filename)\n    if bf.exists(path):\n        return path\n    return None\n\n\ndef log_loss_dict(diffusion, ts, losses):\n    for key, values in losses.items():\n        logger.logkv_mean(key, values.mean().item())\n        # Log the quantiles (four quartiles, in particular).\n        for sub_t, sub_loss in zip(ts.cpu().numpy(), values.detach().cpu().numpy()):\n            quartile = int(4 * sub_t / diffusion.num_timesteps)\n            logger.logkv_mean(f\"{key}_q{quartile}\", sub_loss)\n"}
{"type": "source_file", "path": "Image_Purifier/guided_diffusion/resample.py", "content": "# ---------------------------------------------------------------\n# Taken from the following link as is from:\n# https://github.com/openai/guided-diffusion/blob/main/guided_diffusion/resample.py\n#\n# The license for the original version of this file can be\n# found in this directory (LICENSE_GUIDED_DIFFUSION).\n# ---------------------------------------------------------------\n\nfrom abc import ABC, abstractmethod\n\nimport numpy as np\nimport torch as th\nimport torch.distributed as dist\n\n\ndef create_named_schedule_sampler(name, diffusion):\n    \"\"\"\n    Create a ScheduleSampler from a library of pre-defined samplers.\n\n    :param name: the name of the sampler.\n    :param diffusion: the diffusion object to sample for.\n    \"\"\"\n    if name == \"uniform\":\n        return UniformSampler(diffusion)\n    elif name == \"loss-second-moment\":\n        return LossSecondMomentResampler(diffusion)\n    else:\n        raise NotImplementedError(f\"unknown schedule sampler: {name}\")\n\n\nclass ScheduleSampler(ABC):\n    \"\"\"\n    A distribution over timesteps in the diffusion process, intended to reduce\n    variance of the objective.\n\n    By default, samplers perform unbiased importance sampling, in which the\n    objective's mean is unchanged.\n    However, subclasses may override sample() to change how the resampled\n    terms are reweighted, allowing for actual changes in the objective.\n    \"\"\"\n\n    @abstractmethod\n    def weights(self):\n        \"\"\"\n        Get a numpy array of weights, one per diffusion step.\n\n        The weights needn't be normalized, but must be positive.\n        \"\"\"\n\n    def sample(self, batch_size, device):\n        \"\"\"\n        Importance-sample timesteps for a batch.\n\n        :param batch_size: the number of timesteps.\n        :param device: the torch device to save to.\n        :return: a tuple (timesteps, weights):\n                 - timesteps: a tensor of timestep indices.\n                 - weights: a tensor of weights to scale the resulting losses.\n        \"\"\"\n        w = self.weights()\n        p = w / np.sum(w)\n        indices_np = np.random.choice(len(p), size=(batch_size,), p=p)\n        indices = th.from_numpy(indices_np).long().to(device)\n        weights_np = 1 / (len(p) * p[indices_np])\n        weights = th.from_numpy(weights_np).float().to(device)\n        return indices, weights\n\n\nclass UniformSampler(ScheduleSampler):\n    def __init__(self, diffusion):\n        self.diffusion = diffusion\n        self._weights = np.ones([diffusion.num_timesteps])\n\n    def weights(self):\n        return self._weights\n\n\nclass LossAwareSampler(ScheduleSampler):\n    def update_with_local_losses(self, local_ts, local_losses):\n        \"\"\"\n        Update the reweighting using losses from a model.\n\n        Call this method from each rank with a batch of timesteps and the\n        corresponding losses for each of those timesteps.\n        This method will perform synchronization to make sure all of the ranks\n        maintain the exact same reweighting.\n\n        :param local_ts: an integer Tensor of timesteps.\n        :param local_losses: a 1D Tensor of losses.\n        \"\"\"\n        batch_sizes = [\n            th.tensor([0], dtype=th.int32, device=local_ts.device)\n            for _ in range(dist.get_world_size())\n        ]\n        dist.all_gather(\n            batch_sizes,\n            th.tensor([len(local_ts)], dtype=th.int32, device=local_ts.device),\n        )\n\n        # Pad all_gather batches to be the maximum batch size.\n        batch_sizes = [x.item() for x in batch_sizes]\n        max_bs = max(batch_sizes)\n\n        timestep_batches = [th.zeros(max_bs).to(local_ts) for bs in batch_sizes]\n        loss_batches = [th.zeros(max_bs).to(local_losses) for bs in batch_sizes]\n        dist.all_gather(timestep_batches, local_ts)\n        dist.all_gather(loss_batches, local_losses)\n        timesteps = [\n            x.item() for y, bs in zip(timestep_batches, batch_sizes) for x in y[:bs]\n        ]\n        losses = [x.item() for y, bs in zip(loss_batches, batch_sizes) for x in y[:bs]]\n        self.update_with_all_losses(timesteps, losses)\n\n    @abstractmethod\n    def update_with_all_losses(self, ts, losses):\n        \"\"\"\n        Update the reweighting using losses from a model.\n\n        Sub-classes should override this method to update the reweighting\n        using losses from the model.\n\n        This method directly updates the reweighting without synchronizing\n        between workers. It is called by update_with_local_losses from all\n        ranks with identical arguments. Thus, it should have deterministic\n        behavior to maintain state across workers.\n\n        :param ts: a list of int timesteps.\n        :param losses: a list of float losses, one per timestep.\n        \"\"\"\n\n\nclass LossSecondMomentResampler(LossAwareSampler):\n    def __init__(self, diffusion, history_per_term=10, uniform_prob=0.001):\n        self.diffusion = diffusion\n        self.history_per_term = history_per_term\n        self.uniform_prob = uniform_prob\n        self._loss_history = np.zeros(\n            [diffusion.num_timesteps, history_per_term], dtype=np.float64\n        )\n        self._loss_counts = np.zeros([diffusion.num_timesteps], dtype=np.int)\n\n    def weights(self):\n        if not self._warmed_up():\n            return np.ones([self.diffusion.num_timesteps], dtype=np.float64)\n        weights = np.sqrt(np.mean(self._loss_history ** 2, axis=-1))\n        weights /= np.sum(weights)\n        weights *= 1 - self.uniform_prob\n        weights += self.uniform_prob / len(weights)\n        return weights\n\n    def update_with_all_losses(self, ts, losses):\n        for t, loss in zip(ts, losses):\n            if self._loss_counts[t] == self.history_per_term:\n                # Shift out the oldest loss term.\n                self._loss_history[t, :-1] = self._loss_history[t, 1:]\n                self._loss_history[t, -1] = loss\n            else:\n                self._loss_history[t, self._loss_counts[t]] = loss\n                self._loss_counts[t] += 1\n\n    def _warmed_up(self):\n        return (self._loss_counts == self.history_per_term).all()\n"}
{"type": "source_file", "path": "Image_Purifier/guided_diffusion/gaussian_diffusion.py", "content": "# ---------------------------------------------------------------\n# Taken from the following link as is from:\n# https://github.com/openai/guided-diffusion/blob/main/guided_diffusion/gaussian_diffusion.py\n#\n# The license for the original version of this file can be\n# found in this directory (LICENSE_GUIDED_DIFFUSION).\n# ---------------------------------------------------------------\n\n\"\"\"\nThis code started out as a PyTorch port of Ho et al's diffusion models:\nhttps://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/diffusion_utils_2.py\n\nDocstrings have been added, as well as DDIM sampling and a new collection of beta schedules.\n\"\"\"\n\nimport enum\nimport math\n\nimport numpy as np\nimport torch as th\n\nfrom .nn import mean_flat\nfrom .losses import normal_kl, discretized_gaussian_log_likelihood\n\n\ndef get_named_beta_schedule(schedule_name, num_diffusion_timesteps):\n    \"\"\"\n    Get a pre-defined beta schedule for the given name.\n\n    The beta schedule library consists of beta schedules which remain similar\n    in the limit of num_diffusion_timesteps.\n    Beta schedules may be added, but should not be removed or changed once\n    they are committed to maintain backwards compatibility.\n    \"\"\"\n    if schedule_name == \"linear\":\n        # Linear schedule from Ho et al, extended to work for any number of\n        # diffusion steps.\n        scale = 1000 / num_diffusion_timesteps\n        beta_start = scale * 0.0001\n        beta_end = scale * 0.02\n        return np.linspace(\n            beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64\n        )\n    elif schedule_name == \"cosine\":\n        return betas_for_alpha_bar(\n            num_diffusion_timesteps,\n            lambda t: math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2,\n        )\n    else:\n        raise NotImplementedError(f\"unknown beta schedule: {schedule_name}\")\n\n\ndef betas_for_alpha_bar(num_diffusion_timesteps, alpha_bar, max_beta=0.999):\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function,\n    which defines the cumulative product of (1-beta) over time from t = [0,1].\n\n    :param num_diffusion_timesteps: the number of betas to produce.\n    :param alpha_bar: a lambda that takes an argument t from 0 to 1 and\n                      produces the cumulative product of (1-beta) up to that\n                      part of the diffusion process.\n    :param max_beta: the maximum beta to use; use values lower than 1 to\n                     prevent singularities.\n    \"\"\"\n    betas = []\n    for i in range(num_diffusion_timesteps):\n        t1 = i / num_diffusion_timesteps\n        t2 = (i + 1) / num_diffusion_timesteps\n        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n    return np.array(betas)\n\n\nclass ModelMeanType(enum.Enum):\n    \"\"\"\n    Which type of output the model predicts.\n    \"\"\"\n\n    PREVIOUS_X = enum.auto()  # the model predicts x_{t-1}\n    START_X = enum.auto()  # the model predicts x_0\n    EPSILON = enum.auto()  # the model predicts epsilon\n\n\nclass ModelVarType(enum.Enum):\n    \"\"\"\n    What is used as the model's output variance.\n\n    The LEARNED_RANGE option has been added to allow the model to predict\n    values between FIXED_SMALL and FIXED_LARGE, making its job easier.\n    \"\"\"\n\n    LEARNED = enum.auto()\n    FIXED_SMALL = enum.auto()\n    FIXED_LARGE = enum.auto()\n    LEARNED_RANGE = enum.auto()\n\n\nclass LossType(enum.Enum):\n    MSE = enum.auto()  # use raw MSE loss (and KL when learning variances)\n    RESCALED_MSE = (\n        enum.auto()\n    )  # use raw MSE loss (with RESCALED_KL when learning variances)\n    KL = enum.auto()  # use the variational lower-bound\n    RESCALED_KL = enum.auto()  # like KL, but rescale to estimate the full VLB\n\n    def is_vb(self):\n        return self == LossType.KL or self == LossType.RESCALED_KL\n\n\nclass GaussianDiffusion:\n    \"\"\"\n    Utilities for training and sampling diffusion models.\n\n    Ported directly from here, and then adapted over time to further experimentation.\n    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/diffusion_utils_2.py#L42\n\n    :param betas: a 1-D numpy array of betas for each diffusion timestep,\n                  starting at T and going to 1.\n    :param model_mean_type: a ModelMeanType determining what the model outputs.\n    :param model_var_type: a ModelVarType determining how variance is output.\n    :param loss_type: a LossType determining the loss function to use.\n    :param rescale_timesteps: if True, pass floating point timesteps into the\n                              model so that they are always scaled like in the\n                              original paper (0 to 1000).\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        betas,\n        model_mean_type,\n        model_var_type,\n        loss_type,\n        rescale_timesteps=False,\n    ):\n        self.model_mean_type = model_mean_type\n        self.model_var_type = model_var_type\n        self.loss_type = loss_type\n        self.rescale_timesteps = rescale_timesteps\n\n        # Use float64 for accuracy.\n        betas = np.array(betas, dtype=np.float64)\n        self.betas = betas\n        assert len(betas.shape) == 1, \"betas must be 1-D\"\n        assert (betas > 0).all() and (betas <= 1).all()\n\n        self.num_timesteps = int(betas.shape[0])\n\n        alphas = 1.0 - betas\n        self.alphas_cumprod = np.cumprod(alphas, axis=0)\n        self.alphas_cumprod_prev = np.append(1.0, self.alphas_cumprod[:-1])\n        self.alphas_cumprod_next = np.append(self.alphas_cumprod[1:], 0.0)\n        assert self.alphas_cumprod_prev.shape == (self.num_timesteps,)\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        self.sqrt_alphas_cumprod = np.sqrt(self.alphas_cumprod)\n        self.sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - self.alphas_cumprod)\n        self.log_one_minus_alphas_cumprod = np.log(1.0 - self.alphas_cumprod)\n        self.sqrt_recip_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod)\n        self.sqrt_recipm1_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod - 1)\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        self.posterior_variance = (\n            betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n        )\n        # log calculation clipped because the posterior variance is 0 at the\n        # beginning of the diffusion chain.\n        self.posterior_log_variance_clipped = np.log(\n            np.append(self.posterior_variance[1], self.posterior_variance[1:])\n        )\n        self.posterior_mean_coef1 = (\n            betas * np.sqrt(self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n        )\n        self.posterior_mean_coef2 = (\n            (1.0 - self.alphas_cumprod_prev)\n            * np.sqrt(alphas)\n            / (1.0 - self.alphas_cumprod)\n        )\n\n    def q_mean_variance(self, x_start, t):\n        \"\"\"\n        Get the distribution q(x_t | x_0).\n\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \"\"\"\n        mean = (\n            _extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n        )\n        variance = _extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = _extract_into_tensor(\n            self.log_one_minus_alphas_cumprod, t, x_start.shape\n        )\n        return mean, variance, log_variance\n\n    def q_sample(self, x_start, t, noise=None):\n        \"\"\"\n        Diffuse the data for a given number of diffusion steps.\n\n        In other words, sample from q(x_t | x_0).\n\n        :param x_start: the initial data batch.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :param noise: if specified, the split-out normal noise.\n        :return: A noisy version of x_start.\n        \"\"\"\n        if noise is None:\n            noise = th.randn_like(x_start)\n        assert noise.shape == x_start.shape\n        return (\n            _extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n            + _extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n            * noise\n        )\n\n    def q_posterior_mean_variance(self, x_start, x_t, t):\n        \"\"\"\n        Compute the mean and variance of the diffusion posterior:\n\n            q(x_{t-1} | x_t, x_0)\n\n        \"\"\"\n        assert x_start.shape == x_t.shape\n        posterior_mean = (\n            _extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start\n            + _extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = _extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = _extract_into_tensor(\n            self.posterior_log_variance_clipped, t, x_t.shape\n        )\n        assert (\n            posterior_mean.shape[0]\n            == posterior_variance.shape[0]\n            == posterior_log_variance_clipped.shape[0]\n            == x_start.shape[0]\n        )\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def p_mean_variance(\n        self, model, x, t, clip_denoised=True, denoised_fn=None, model_kwargs=None\n    ):\n        \"\"\"\n        Apply the model to get p(x_{t-1} | x_t), as well as a prediction of\n        the initial x, x_0.\n\n        :param model: the model, which takes a signal and a batch of timesteps\n                      as input.\n        :param x: the [N x C x ...] tensor at time t.\n        :param t: a 1-D Tensor of timesteps.\n        :param clip_denoised: if True, clip the denoised signal into [-1, 1].\n        :param denoised_fn: if not None, a function which applies to the\n            x_start prediction before it is used to sample. Applies before\n            clip_denoised.\n        :param model_kwargs: if not None, a dict of extra keyword arguments to\n            pass to the model. This can be used for conditioning.\n        :return: a dict with the following keys:\n                 - 'mean': the model mean output.\n                 - 'variance': the model variance output.\n                 - 'log_variance': the log of 'variance'.\n                 - 'pred_xstart': the prediction for x_0.\n        \"\"\"\n        if model_kwargs is None:\n            model_kwargs = {}\n\n        B, C = x.shape[:2]\n        assert t.shape == (B,)\n        model_output = model(x, self._scale_timesteps(t), **model_kwargs)\n\n        if self.model_var_type in [ModelVarType.LEARNED, ModelVarType.LEARNED_RANGE]:\n            assert model_output.shape == (B, C * 2, *x.shape[2:])\n            model_output, model_var_values = th.split(model_output, C, dim=1)\n            if self.model_var_type == ModelVarType.LEARNED:\n                model_log_variance = model_var_values\n                model_variance = th.exp(model_log_variance)\n            else:\n                min_log = _extract_into_tensor(\n                    self.posterior_log_variance_clipped, t, x.shape\n                )\n                max_log = _extract_into_tensor(np.log(self.betas), t, x.shape)\n                # The model_var_values is [-1, 1] for [min_var, max_var].\n                frac = (model_var_values + 1) / 2\n                model_log_variance = frac * max_log + (1 - frac) * min_log\n                model_variance = th.exp(model_log_variance)\n        else:\n            model_variance, model_log_variance = {\n                # for fixedlarge, we set the initial (log-)variance like so\n                # to get a better decoder log likelihood.\n                ModelVarType.FIXED_LARGE: (\n                    np.append(self.posterior_variance[1], self.betas[1:]),\n                    np.log(np.append(self.posterior_variance[1], self.betas[1:])),\n                ),\n                ModelVarType.FIXED_SMALL: (\n                    self.posterior_variance,\n                    self.posterior_log_variance_clipped,\n                ),\n            }[self.model_var_type]\n            model_variance = _extract_into_tensor(model_variance, t, x.shape)\n            model_log_variance = _extract_into_tensor(model_log_variance, t, x.shape)\n\n        def process_xstart(x):\n            if denoised_fn is not None:\n                x = denoised_fn(x)\n            if clip_denoised:\n                return x.clamp(-1, 1)\n            return x\n\n        if self.model_mean_type == ModelMeanType.PREVIOUS_X:\n            pred_xstart = process_xstart(\n                self._predict_xstart_from_xprev(x_t=x, t=t, xprev=model_output)\n            )\n            model_mean = model_output\n        elif self.model_mean_type in [ModelMeanType.START_X, ModelMeanType.EPSILON]:\n            if self.model_mean_type == ModelMeanType.START_X:\n                pred_xstart = process_xstart(model_output)\n            else:\n                pred_xstart = process_xstart(\n                    self._predict_xstart_from_eps(x_t=x, t=t, eps=model_output)\n                )\n            model_mean, _, _ = self.q_posterior_mean_variance(\n                x_start=pred_xstart, x_t=x, t=t\n            )\n        else:\n            raise NotImplementedError(self.model_mean_type)\n\n        assert (\n            model_mean.shape == model_log_variance.shape == pred_xstart.shape == x.shape\n        )\n        return {\n            \"mean\": model_mean,\n            \"variance\": model_variance,\n            \"log_variance\": model_log_variance,\n            \"pred_xstart\": pred_xstart,\n        }\n\n    def _predict_xstart_from_eps(self, x_t, t, eps):\n        assert x_t.shape == eps.shape\n        return (\n            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t\n            - _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * eps\n        )\n\n    def _predict_xstart_from_xprev(self, x_t, t, xprev):\n        assert x_t.shape == xprev.shape\n        return (  # (xprev - coef2*x_t) / coef1\n            _extract_into_tensor(1.0 / self.posterior_mean_coef1, t, x_t.shape) * xprev\n            - _extract_into_tensor(\n                self.posterior_mean_coef2 / self.posterior_mean_coef1, t, x_t.shape\n            )\n            * x_t\n        )\n\n    def _predict_eps_from_xstart(self, x_t, t, pred_xstart):\n        return (\n            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t\n            - pred_xstart\n        ) / _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n\n    def _scale_timesteps(self, t):\n        if self.rescale_timesteps:\n            return t.float() * (1000.0 / self.num_timesteps)\n        return t\n\n    def condition_mean(self, cond_fn, p_mean_var, x, t, model_kwargs=None):\n        \"\"\"\n        Compute the mean for the previous step, given a function cond_fn that\n        computes the gradient of a conditional log probability with respect to\n        x. In particular, cond_fn computes grad(log(p(y|x))), and we want to\n        condition on y.\n\n        This uses the conditioning strategy from Sohl-Dickstein et al. (2015).\n        \"\"\"\n        gradient = cond_fn(x, self._scale_timesteps(t), **model_kwargs)\n        new_mean = (\n            p_mean_var[\"mean\"].float() + p_mean_var[\"variance\"] * gradient.float()\n        )\n        return new_mean\n\n    def condition_score(self, cond_fn, p_mean_var, x, t, model_kwargs=None):\n        \"\"\"\n        Compute what the p_mean_variance output would have been, should the\n        model's score function be conditioned by cond_fn.\n\n        See condition_mean() for details on cond_fn.\n\n        Unlike condition_mean(), this instead uses the conditioning strategy\n        from Song et al (2020).\n        \"\"\"\n        alpha_bar = _extract_into_tensor(self.alphas_cumprod, t, x.shape)\n\n        eps = self._predict_eps_from_xstart(x, t, p_mean_var[\"pred_xstart\"])\n        eps = eps - (1 - alpha_bar).sqrt() * cond_fn(\n            x, self._scale_timesteps(t), **model_kwargs\n        )\n\n        out = p_mean_var.copy()\n        out[\"pred_xstart\"] = self._predict_xstart_from_eps(x, t, eps)\n        out[\"mean\"], _, _ = self.q_posterior_mean_variance(\n            x_start=out[\"pred_xstart\"], x_t=x, t=t\n        )\n        return out\n\n    def p_sample(\n        self,\n        model,\n        x,\n        t,\n        clip_denoised=True,\n        denoised_fn=None,\n        cond_fn=None,\n        model_kwargs=None,\n    ):\n        \"\"\"\n        Sample x_{t-1} from the model at the given timestep.\n\n        :param model: the model to sample from.\n        :param x: the current tensor at x_{t-1}.\n        :param t: the value of t, starting at 0 for the first diffusion step.\n        :param clip_denoised: if True, clip the x_start prediction to [-1, 1].\n        :param denoised_fn: if not None, a function which applies to the\n            x_start prediction before it is used to sample.\n        :param cond_fn: if not None, this is a gradient function that acts\n                        similarly to the model.\n        :param model_kwargs: if not None, a dict of extra keyword arguments to\n            pass to the model. This can be used for conditioning.\n        :return: a dict containing the following keys:\n                 - 'sample': a random sample from the model.\n                 - 'pred_xstart': a prediction of x_0.\n        \"\"\"\n        out = self.p_mean_variance(\n            model,\n            x,\n            t,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            model_kwargs=model_kwargs,\n        )\n        noise = th.randn_like(x)\n        nonzero_mask = (\n            (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))\n        )  # no noise when t == 0\n        if cond_fn is not None:\n            out[\"mean\"] = self.condition_mean(\n                cond_fn, out, x, t, model_kwargs=model_kwargs\n            )\n        sample = out[\"mean\"] + nonzero_mask * th.exp(0.5 * out[\"log_variance\"]) * noise\n        return {\"sample\": sample, \"pred_xstart\": out[\"pred_xstart\"]}\n\n    def p_sample_loop(\n        self,\n        model,\n        shape,\n        noise=None,\n        clip_denoised=True,\n        denoised_fn=None,\n        cond_fn=None,\n        model_kwargs=None,\n        device=None,\n        progress=False,\n    ):\n        \"\"\"\n        Generate samples from the model.\n\n        :param model: the model module.\n        :param shape: the shape of the samples, (N, C, H, W).\n        :param noise: if specified, the noise from the encoder to sample.\n                      Should be of the same shape as `shape`.\n        :param clip_denoised: if True, clip x_start predictions to [-1, 1].\n        :param denoised_fn: if not None, a function which applies to the\n            x_start prediction before it is used to sample.\n        :param cond_fn: if not None, this is a gradient function that acts\n                        similarly to the model.\n        :param model_kwargs: if not None, a dict of extra keyword arguments to\n            pass to the model. This can be used for conditioning.\n        :param device: if specified, the device to create the samples on.\n                       If not specified, use a model parameter's device.\n        :param progress: if True, show a tqdm progress bar.\n        :return: a non-differentiable batch of samples.\n        \"\"\"\n        final = None\n        for sample in self.p_sample_loop_progressive(\n            model,\n            shape,\n            noise=noise,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            cond_fn=cond_fn,\n            model_kwargs=model_kwargs,\n            device=device,\n            progress=progress,\n        ):\n            final = sample\n        return final[\"sample\"]\n\n    def p_sample_loop_progressive(\n        self,\n        model,\n        shape,\n        noise=None,\n        clip_denoised=True,\n        denoised_fn=None,\n        cond_fn=None,\n        model_kwargs=None,\n        device=None,\n        progress=False,\n    ):\n        \"\"\"\n        Generate samples from the model and yield intermediate samples from\n        each timestep of diffusion.\n\n        Arguments are the same as p_sample_loop().\n        Returns a generator over dicts, where each dict is the return value of\n        p_sample().\n        \"\"\"\n        if device is None:\n            device = next(model.parameters()).device\n        assert isinstance(shape, (tuple, list))\n        if noise is not None:\n            img = noise\n        else:\n            img = th.randn(*shape, device=device)\n        indices = list(range(self.num_timesteps))[::-1]\n\n        if progress:\n            # Lazy import so that we don't depend on tqdm.\n            from tqdm.auto import tqdm\n\n            indices = tqdm(indices)\n\n        for i in indices:\n            t = th.tensor([i] * shape[0], device=device)\n            with th.no_grad():\n                out = self.p_sample(\n                    model,\n                    img,\n                    t,\n                    clip_denoised=clip_denoised,\n                    denoised_fn=denoised_fn,\n                    cond_fn=cond_fn,\n                    model_kwargs=model_kwargs,\n                )\n                yield out\n                img = out[\"sample\"]\n\n    def ddim_sample(\n        self,\n        model,\n        x,\n        t,\n        clip_denoised=True,\n        denoised_fn=None,\n        cond_fn=None,\n        model_kwargs=None,\n        eta=0.0,\n    ):\n        \"\"\"\n        Sample x_{t-1} from the model using DDIM.\n\n        Same usage as p_sample().\n        \"\"\"\n        out = self.p_mean_variance(\n            model,\n            x,\n            t,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            model_kwargs=model_kwargs,\n        )\n        if cond_fn is not None:\n            out = self.condition_score(cond_fn, out, x, t, model_kwargs=model_kwargs)\n\n        # Usually our model outputs epsilon, but we re-derive it\n        # in case we used x_start or x_prev prediction.\n        eps = self._predict_eps_from_xstart(x, t, out[\"pred_xstart\"])\n\n        alpha_bar = _extract_into_tensor(self.alphas_cumprod, t, x.shape)\n        alpha_bar_prev = _extract_into_tensor(self.alphas_cumprod_prev, t, x.shape)\n        sigma = (\n            eta\n            * th.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar))\n            * th.sqrt(1 - alpha_bar / alpha_bar_prev)\n        )\n        # Equation 12.\n        noise = th.randn_like(x)\n        mean_pred = (\n            out[\"pred_xstart\"] * th.sqrt(alpha_bar_prev)\n            + th.sqrt(1 - alpha_bar_prev - sigma ** 2) * eps\n        )\n        nonzero_mask = (\n            (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))\n        )  # no noise when t == 0\n        sample = mean_pred + nonzero_mask * sigma * noise\n        return {\"sample\": sample, \"pred_xstart\": out[\"pred_xstart\"]}\n\n    def ddim_reverse_sample(\n        self,\n        model,\n        x,\n        t,\n        clip_denoised=True,\n        denoised_fn=None,\n        model_kwargs=None,\n        eta=0.0,\n    ):\n        \"\"\"\n        Sample x_{t+1} from the model using DDIM reverse ODE.\n        \"\"\"\n        assert eta == 0.0, \"Reverse ODE only for deterministic path\"\n        out = self.p_mean_variance(\n            model,\n            x,\n            t,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            model_kwargs=model_kwargs,\n        )\n        # Usually our model outputs epsilon, but we re-derive it\n        # in case we used x_start or x_prev prediction.\n        eps = (\n            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x.shape) * x\n            - out[\"pred_xstart\"]\n        ) / _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x.shape)\n        alpha_bar_next = _extract_into_tensor(self.alphas_cumprod_next, t, x.shape)\n\n        # Equation 12. reversed\n        mean_pred = (\n            out[\"pred_xstart\"] * th.sqrt(alpha_bar_next)\n            + th.sqrt(1 - alpha_bar_next) * eps\n        )\n\n        return {\"sample\": mean_pred, \"pred_xstart\": out[\"pred_xstart\"]}\n\n    def ddim_sample_loop(\n        self,\n        model,\n        shape,\n        noise=None,\n        clip_denoised=True,\n        denoised_fn=None,\n        cond_fn=None,\n        model_kwargs=None,\n        device=None,\n        progress=False,\n        eta=0.0,\n    ):\n        \"\"\"\n        Generate samples from the model using DDIM.\n\n        Same usage as p_sample_loop().\n        \"\"\"\n        final = None\n        for sample in self.ddim_sample_loop_progressive(\n            model,\n            shape,\n            noise=noise,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            cond_fn=cond_fn,\n            model_kwargs=model_kwargs,\n            device=device,\n            progress=progress,\n            eta=eta,\n        ):\n            final = sample\n        return final[\"sample\"]\n\n    def ddim_sample_loop_progressive(\n        self,\n        model,\n        shape,\n        noise=None,\n        clip_denoised=True,\n        denoised_fn=None,\n        cond_fn=None,\n        model_kwargs=None,\n        device=None,\n        progress=False,\n        eta=0.0,\n    ):\n        \"\"\"\n        Use DDIM to sample from the model and yield intermediate samples from\n        each timestep of DDIM.\n\n        Same usage as p_sample_loop_progressive().\n        \"\"\"\n        if device is None:\n            device = next(model.parameters()).device\n        assert isinstance(shape, (tuple, list))\n        if noise is not None:\n            img = noise\n        else:\n            img = th.randn(*shape, device=device)\n        indices = list(range(self.num_timesteps))[::-1]\n\n        if progress:\n            # Lazy import so that we don't depend on tqdm.\n            from tqdm.auto import tqdm\n\n            indices = tqdm(indices)\n\n        for i in indices:\n            t = th.tensor([i] * shape[0], device=device)\n            with th.no_grad():\n                out = self.ddim_sample(\n                    model,\n                    img,\n                    t,\n                    clip_denoised=clip_denoised,\n                    denoised_fn=denoised_fn,\n                    cond_fn=cond_fn,\n                    model_kwargs=model_kwargs,\n                    eta=eta,\n                )\n                yield out\n                img = out[\"sample\"]\n\n    def _vb_terms_bpd(\n        self, model, x_start, x_t, t, clip_denoised=True, model_kwargs=None\n    ):\n        \"\"\"\n        Get a term for the variational lower-bound.\n\n        The resulting units are bits (rather than nats, as one might expect).\n        This allows for comparison to other papers.\n\n        :return: a dict with the following keys:\n                 - 'output': a shape [N] tensor of NLLs or KLs.\n                 - 'pred_xstart': the x_0 predictions.\n        \"\"\"\n        true_mean, _, true_log_variance_clipped = self.q_posterior_mean_variance(\n            x_start=x_start, x_t=x_t, t=t\n        )\n        out = self.p_mean_variance(\n            model, x_t, t, clip_denoised=clip_denoised, model_kwargs=model_kwargs\n        )\n        kl = normal_kl(\n            true_mean, true_log_variance_clipped, out[\"mean\"], out[\"log_variance\"]\n        )\n        kl = mean_flat(kl) / np.log(2.0)\n\n        decoder_nll = -discretized_gaussian_log_likelihood(\n            x_start, means=out[\"mean\"], log_scales=0.5 * out[\"log_variance\"]\n        )\n        assert decoder_nll.shape == x_start.shape\n        decoder_nll = mean_flat(decoder_nll) / np.log(2.0)\n\n        # At the first timestep return the decoder NLL,\n        # otherwise return KL(q(x_{t-1}|x_t,x_0) || p(x_{t-1}|x_t))\n        output = th.where((t == 0), decoder_nll, kl)\n        return {\"output\": output, \"pred_xstart\": out[\"pred_xstart\"]}\n\n    def training_losses(self, model, x_start, t, model_kwargs=None, noise=None):\n        \"\"\"\n        Compute training losses for a single timestep.\n\n        :param model: the model to evaluate loss on.\n        :param x_start: the [N x C x ...] tensor of inputs.\n        :param t: a batch of timestep indices.\n        :param model_kwargs: if not None, a dict of extra keyword arguments to\n            pass to the model. This can be used for conditioning.\n        :param noise: if specified, the specific Gaussian noise to try to remove.\n        :return: a dict with the key \"loss\" containing a tensor of shape [N].\n                 Some mean or variance settings may also have other keys.\n        \"\"\"\n        if model_kwargs is None:\n            model_kwargs = {}\n        if noise is None:\n            noise = th.randn_like(x_start)\n        x_t = self.q_sample(x_start, t, noise=noise)\n\n        terms = {}\n\n        if self.loss_type == LossType.KL or self.loss_type == LossType.RESCALED_KL:\n            terms[\"loss\"] = self._vb_terms_bpd(\n                model=model,\n                x_start=x_start,\n                x_t=x_t,\n                t=t,\n                clip_denoised=False,\n                model_kwargs=model_kwargs,\n            )[\"output\"]\n            if self.loss_type == LossType.RESCALED_KL:\n                terms[\"loss\"] *= self.num_timesteps\n        elif self.loss_type == LossType.MSE or self.loss_type == LossType.RESCALED_MSE:\n            model_output = model(x_t, self._scale_timesteps(t), **model_kwargs)\n\n            if self.model_var_type in [\n                ModelVarType.LEARNED,\n                ModelVarType.LEARNED_RANGE,\n            ]:\n                B, C = x_t.shape[:2]\n                assert model_output.shape == (B, C * 2, *x_t.shape[2:])\n                model_output, model_var_values = th.split(model_output, C, dim=1)\n                # Learn the variance using the variational bound, but don't let\n                # it affect our mean prediction.\n                frozen_out = th.cat([model_output.detach(), model_var_values], dim=1)\n                terms[\"vb\"] = self._vb_terms_bpd(\n                    model=lambda *args, r=frozen_out: r,\n                    x_start=x_start,\n                    x_t=x_t,\n                    t=t,\n                    clip_denoised=False,\n                )[\"output\"]\n                if self.loss_type == LossType.RESCALED_MSE:\n                    # Divide by 1000 for equivalence with initial implementation.\n                    # Without a factor of 1/1000, the VB term hurts the MSE term.\n                    terms[\"vb\"] *= self.num_timesteps / 1000.0\n\n            target = {\n                ModelMeanType.PREVIOUS_X: self.q_posterior_mean_variance(\n                    x_start=x_start, x_t=x_t, t=t\n                )[0],\n                ModelMeanType.START_X: x_start,\n                ModelMeanType.EPSILON: noise,\n            }[self.model_mean_type]\n            assert model_output.shape == target.shape == x_start.shape\n            terms[\"mse\"] = mean_flat((target - model_output) ** 2)\n            if \"vb\" in terms:\n                terms[\"loss\"] = terms[\"mse\"] + terms[\"vb\"]\n            else:\n                terms[\"loss\"] = terms[\"mse\"]\n        else:\n            raise NotImplementedError(self.loss_type)\n\n        return terms\n\n    def _prior_bpd(self, x_start):\n        \"\"\"\n        Get the prior KL term for the variational lower-bound, measured in\n        bits-per-dim.\n\n        This term can't be optimized, as it only depends on the encoder.\n\n        :param x_start: the [N x C x ...] tensor of inputs.\n        :return: a batch of [N] KL values (in bits), one per batch element.\n        \"\"\"\n        batch_size = x_start.shape[0]\n        t = th.tensor([self.num_timesteps - 1] * batch_size, device=x_start.device)\n        qt_mean, _, qt_log_variance = self.q_mean_variance(x_start, t)\n        kl_prior = normal_kl(\n            mean1=qt_mean, logvar1=qt_log_variance, mean2=0.0, logvar2=0.0\n        )\n        return mean_flat(kl_prior) / np.log(2.0)\n\n    def calc_bpd_loop(self, model, x_start, clip_denoised=True, model_kwargs=None):\n        \"\"\"\n        Compute the entire variational lower-bound, measured in bits-per-dim,\n        as well as other related quantities.\n\n        :param model: the model to evaluate loss on.\n        :param x_start: the [N x C x ...] tensor of inputs.\n        :param clip_denoised: if True, clip denoised samples.\n        :param model_kwargs: if not None, a dict of extra keyword arguments to\n            pass to the model. This can be used for conditioning.\n\n        :return: a dict containing the following keys:\n                 - total_bpd: the total variational lower-bound, per batch element.\n                 - prior_bpd: the prior term in the lower-bound.\n                 - vb: an [N x T] tensor of terms in the lower-bound.\n                 - xstart_mse: an [N x T] tensor of x_0 MSEs for each timestep.\n                 - mse: an [N x T] tensor of epsilon MSEs for each timestep.\n        \"\"\"\n        device = x_start.device\n        batch_size = x_start.shape[0]\n\n        vb = []\n        xstart_mse = []\n        mse = []\n        for t in list(range(self.num_timesteps))[::-1]:\n            t_batch = th.tensor([t] * batch_size, device=device)\n            noise = th.randn_like(x_start)\n            x_t = self.q_sample(x_start=x_start, t=t_batch, noise=noise)\n            # Calculate VLB term at the current timestep\n            with th.no_grad():\n                out = self._vb_terms_bpd(\n                    model,\n                    x_start=x_start,\n                    x_t=x_t,\n                    t=t_batch,\n                    clip_denoised=clip_denoised,\n                    model_kwargs=model_kwargs,\n                )\n            vb.append(out[\"output\"])\n            xstart_mse.append(mean_flat((out[\"pred_xstart\"] - x_start) ** 2))\n            eps = self._predict_eps_from_xstart(x_t, t_batch, out[\"pred_xstart\"])\n            mse.append(mean_flat((eps - noise) ** 2))\n\n        vb = th.stack(vb, dim=1)\n        xstart_mse = th.stack(xstart_mse, dim=1)\n        mse = th.stack(mse, dim=1)\n\n        prior_bpd = self._prior_bpd(x_start)\n        total_bpd = vb.sum(dim=1) + prior_bpd\n        return {\n            \"total_bpd\": total_bpd,\n            \"prior_bpd\": prior_bpd,\n            \"vb\": vb,\n            \"xstart_mse\": xstart_mse,\n            \"mse\": mse,\n        }\n\n\ndef _extract_into_tensor(arr, timesteps, broadcast_shape):\n    \"\"\"\n    Extract values from a 1-D numpy array for a batch of indices.\n\n    :param arr: the 1-D numpy array.\n    :param timesteps: a tensor of indices into the array to extract.\n    :param broadcast_shape: a larger shape of K dimensions with the batch\n                            dimension equal to the length of timesteps.\n    :return: a tensor of shape [batch_size, 1, ...] where the shape has K dims.\n    \"\"\"\n    res = th.from_numpy(arr).to(device=timesteps.device)[timesteps].float()\n    while len(res.shape) < len(broadcast_shape):\n        res = res[..., None]\n    return res.expand(broadcast_shape)\n"}
{"type": "source_file", "path": "Image_Purifier/guided_diffusion/losses.py", "content": "# ---------------------------------------------------------------\n# Taken from the following link as is from:\n# https://github.com/openai/guided-diffusion/blob/main/guided_diffusion/losses.py\n#\n# The license for the original version of this file can be\n# found in this directory (LICENSE_GUIDED_DIFFUSION).\n# ---------------------------------------------------------------\n\n\"\"\"\nHelpers for various likelihood-based losses. These are ported from the original\nHo et al. diffusion models codebase:\nhttps://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/utils.py\n\"\"\"\n\nimport numpy as np\n\nimport torch as th\n\n\ndef normal_kl(mean1, logvar1, mean2, logvar2):\n    \"\"\"\n    Compute the KL divergence between two gaussians.\n\n    Shapes are automatically broadcasted, so batches can be compared to\n    scalars, among other use cases.\n    \"\"\"\n    tensor = None\n    for obj in (mean1, logvar1, mean2, logvar2):\n        if isinstance(obj, th.Tensor):\n            tensor = obj\n            break\n    assert tensor is not None, \"at least one argument must be a Tensor\"\n\n    # Force variances to be Tensors. Broadcasting helps convert scalars to\n    # Tensors, but it does not work for th.exp().\n    logvar1, logvar2 = [\n        x if isinstance(x, th.Tensor) else th.tensor(x).to(tensor)\n        for x in (logvar1, logvar2)\n    ]\n\n    return 0.5 * (\n        -1.0\n        + logvar2\n        - logvar1\n        + th.exp(logvar1 - logvar2)\n        + ((mean1 - mean2) ** 2) * th.exp(-logvar2)\n    )\n\n\ndef approx_standard_normal_cdf(x):\n    \"\"\"\n    A fast approximation of the cumulative distribution function of the\n    standard normal.\n    \"\"\"\n    return 0.5 * (1.0 + th.tanh(np.sqrt(2.0 / np.pi) * (x + 0.044715 * th.pow(x, 3))))\n\n\ndef discretized_gaussian_log_likelihood(x, *, means, log_scales):\n    \"\"\"\n    Compute the log-likelihood of a Gaussian distribution discretizing to a\n    given image.\n\n    :param x: the target images. It is assumed that this was uint8 values,\n              rescaled to the range [-1, 1].\n    :param means: the Gaussian mean Tensor.\n    :param log_scales: the Gaussian log stddev Tensor.\n    :return: a tensor like x of log probabilities (in nats).\n    \"\"\"\n    assert x.shape == means.shape == log_scales.shape\n    centered_x = x - means\n    inv_stdv = th.exp(-log_scales)\n    plus_in = inv_stdv * (centered_x + 1.0 / 255.0)\n    cdf_plus = approx_standard_normal_cdf(plus_in)\n    min_in = inv_stdv * (centered_x - 1.0 / 255.0)\n    cdf_min = approx_standard_normal_cdf(min_in)\n    log_cdf_plus = th.log(cdf_plus.clamp(min=1e-12))\n    log_one_minus_cdf_min = th.log((1.0 - cdf_min).clamp(min=1e-12))\n    cdf_delta = cdf_plus - cdf_min\n    log_probs = th.where(\n        x < -0.999,\n        log_cdf_plus,\n        th.where(x > 0.999, log_one_minus_cdf_min, th.log(cdf_delta.clamp(min=1e-12))),\n    )\n    assert log_probs.shape == x.shape\n    return log_probs\n"}
{"type": "source_file", "path": "Image_Purifier/guided_diffusion/script_util.py", "content": "# ---------------------------------------------------------------\n# Taken from the following link as is from:\n# https://github.com/openai/guided-diffusion/blob/main/guided_diffusion/script_util.py\n#\n# The license for the original version of this file can be\n# found in this directory (LICENSE_GUIDED_DIFFUSION).\n# ---------------------------------------------------------------\n\nimport argparse\nimport inspect\n\nfrom . import gaussian_diffusion as gd\nfrom .respace import SpacedDiffusion, space_timesteps\nfrom .unet import SuperResModel, UNetModel, EncoderUNetModel\n\nNUM_CLASSES = 1000\n\n\ndef diffusion_defaults():\n    \"\"\"\n    Defaults for image and classifier training.\n    \"\"\"\n    return dict(\n        learn_sigma=False,\n        diffusion_steps=1000,\n        noise_schedule=\"linear\",\n        timestep_respacing=\"\",\n        use_kl=False,\n        predict_xstart=False,\n        rescale_timesteps=False,\n        rescale_learned_sigmas=False,\n    )\n\n\ndef classifier_defaults():\n    \"\"\"\n    Defaults for classifier models.\n    \"\"\"\n    return dict(\n        image_size=64,\n        classifier_use_fp16=False,\n        classifier_width=128,\n        classifier_depth=2,\n        classifier_attention_resolutions=\"32,16,8\",  # 16\n        classifier_use_scale_shift_norm=True,  # False\n        classifier_resblock_updown=True,  # False\n        classifier_pool=\"attention\",\n    )\n\n\ndef model_and_diffusion_defaults():\n    \"\"\"\n    Defaults for image training.\n    \"\"\"\n    res = dict(\n        image_size=64,\n        num_channels=128,\n        num_res_blocks=2,\n        num_heads=4,\n        num_heads_upsample=-1,\n        num_head_channels=-1,\n        attention_resolutions=\"16,8\",\n        channel_mult=\"\",\n        dropout=0.0,\n        class_cond=False,\n        use_checkpoint=False,\n        use_scale_shift_norm=True,\n        resblock_updown=False,\n        use_fp16=False,\n        use_new_attention_order=False,\n    )\n    res.update(diffusion_defaults())\n    return res\n\n\ndef classifier_and_diffusion_defaults():\n    res = classifier_defaults()\n    res.update(diffusion_defaults())\n    return res\n\n\ndef create_model_and_diffusion(\n    image_size,\n    class_cond,\n    learn_sigma,\n    num_channels,\n    num_res_blocks,\n    channel_mult,\n    num_heads,\n    num_head_channels,\n    num_heads_upsample,\n    attention_resolutions,\n    dropout,\n    diffusion_steps,\n    noise_schedule,\n    timestep_respacing,\n    use_kl,\n    predict_xstart,\n    rescale_timesteps,\n    rescale_learned_sigmas,\n    use_checkpoint,\n    use_scale_shift_norm,\n    resblock_updown,\n    use_fp16,\n    use_new_attention_order,\n):\n    model = create_model(\n        image_size,\n        num_channels,\n        num_res_blocks,\n        channel_mult=channel_mult,\n        learn_sigma=learn_sigma,\n        class_cond=class_cond,\n        use_checkpoint=use_checkpoint,\n        attention_resolutions=attention_resolutions,\n        num_heads=num_heads,\n        num_head_channels=num_head_channels,\n        num_heads_upsample=num_heads_upsample,\n        use_scale_shift_norm=use_scale_shift_norm,\n        dropout=dropout,\n        resblock_updown=resblock_updown,\n        use_fp16=use_fp16,\n        use_new_attention_order=use_new_attention_order,\n    )\n    diffusion = create_gaussian_diffusion(\n        steps=diffusion_steps,\n        learn_sigma=learn_sigma,\n        noise_schedule=noise_schedule,\n        use_kl=use_kl,\n        predict_xstart=predict_xstart,\n        rescale_timesteps=rescale_timesteps,\n        rescale_learned_sigmas=rescale_learned_sigmas,\n        timestep_respacing=timestep_respacing,\n    )\n    return model, diffusion\n\n\ndef create_model(\n    image_size,\n    num_channels,\n    num_res_blocks,\n    channel_mult=\"\",\n    learn_sigma=False,\n    class_cond=False,\n    use_checkpoint=False,\n    attention_resolutions=\"16\",\n    num_heads=1,\n    num_head_channels=-1,\n    num_heads_upsample=-1,\n    use_scale_shift_norm=False,\n    dropout=0,\n    resblock_updown=False,\n    use_fp16=False,\n    use_new_attention_order=False,\n):\n    if channel_mult == \"\":\n        if image_size == 512:\n            channel_mult = (0.5, 1, 1, 2, 2, 4, 4)\n        elif image_size == 256:\n            channel_mult = (1, 1, 2, 2, 4, 4)\n        elif image_size == 128:\n            channel_mult = (1, 1, 2, 3, 4)\n        elif image_size == 64:\n            channel_mult = (1, 2, 3, 4)\n        else:\n            raise ValueError(f\"unsupported image size: {image_size}\")\n    else:\n        channel_mult = tuple(int(ch_mult) for ch_mult in channel_mult.split(\",\"))\n\n    attention_ds = []\n    for res in attention_resolutions.split(\",\"):\n        attention_ds.append(image_size // int(res))\n\n    return UNetModel(\n        image_size=image_size,\n        in_channels=3,\n        model_channels=num_channels,\n        out_channels=(3 if not learn_sigma else 6),\n        num_res_blocks=num_res_blocks,\n        attention_resolutions=tuple(attention_ds),\n        dropout=dropout,\n        channel_mult=channel_mult,\n        num_classes=(NUM_CLASSES if class_cond else None),\n        use_checkpoint=use_checkpoint,\n        use_fp16=use_fp16,\n        num_heads=num_heads,\n        num_head_channels=num_head_channels,\n        num_heads_upsample=num_heads_upsample,\n        use_scale_shift_norm=use_scale_shift_norm,\n        resblock_updown=resblock_updown,\n        use_new_attention_order=use_new_attention_order,\n    )\n\n\ndef create_classifier_and_diffusion(\n    image_size,\n    classifier_use_fp16,\n    classifier_width,\n    classifier_depth,\n    classifier_attention_resolutions,\n    classifier_use_scale_shift_norm,\n    classifier_resblock_updown,\n    classifier_pool,\n    learn_sigma,\n    diffusion_steps,\n    noise_schedule,\n    timestep_respacing,\n    use_kl,\n    predict_xstart,\n    rescale_timesteps,\n    rescale_learned_sigmas,\n):\n    classifier = create_classifier(\n        image_size,\n        classifier_use_fp16,\n        classifier_width,\n        classifier_depth,\n        classifier_attention_resolutions,\n        classifier_use_scale_shift_norm,\n        classifier_resblock_updown,\n        classifier_pool,\n    )\n    diffusion = create_gaussian_diffusion(\n        steps=diffusion_steps,\n        learn_sigma=learn_sigma,\n        noise_schedule=noise_schedule,\n        use_kl=use_kl,\n        predict_xstart=predict_xstart,\n        rescale_timesteps=rescale_timesteps,\n        rescale_learned_sigmas=rescale_learned_sigmas,\n        timestep_respacing=timestep_respacing,\n    )\n    return classifier, diffusion\n\n\ndef create_classifier(\n    image_size,\n    classifier_use_fp16,\n    classifier_width,\n    classifier_depth,\n    classifier_attention_resolutions,\n    classifier_use_scale_shift_norm,\n    classifier_resblock_updown,\n    classifier_pool,\n):\n    if image_size == 512:\n        channel_mult = (0.5, 1, 1, 2, 2, 4, 4)\n    elif image_size == 256:\n        channel_mult = (1, 1, 2, 2, 4, 4)\n    elif image_size == 128:\n        channel_mult = (1, 1, 2, 3, 4)\n    elif image_size == 64:\n        channel_mult = (1, 2, 3, 4)\n    else:\n        raise ValueError(f\"unsupported image size: {image_size}\")\n\n    attention_ds = []\n    for res in classifier_attention_resolutions.split(\",\"):\n        attention_ds.append(image_size // int(res))\n\n    return EncoderUNetModel(\n        image_size=image_size,\n        in_channels=3,\n        model_channels=classifier_width,\n        out_channels=1000,\n        num_res_blocks=classifier_depth,\n        attention_resolutions=tuple(attention_ds),\n        channel_mult=channel_mult,\n        use_fp16=classifier_use_fp16,\n        num_head_channels=64,\n        use_scale_shift_norm=classifier_use_scale_shift_norm,\n        resblock_updown=classifier_resblock_updown,\n        pool=classifier_pool,\n    )\n\n\ndef sr_model_and_diffusion_defaults():\n    res = model_and_diffusion_defaults()\n    res[\"large_size\"] = 256\n    res[\"small_size\"] = 64\n    arg_names = inspect.getfullargspec(sr_create_model_and_diffusion)[0]\n    for k in res.copy().keys():\n        if k not in arg_names:\n            del res[k]\n    return res\n\n\ndef sr_create_model_and_diffusion(\n    large_size,\n    small_size,\n    class_cond,\n    learn_sigma,\n    num_channels,\n    num_res_blocks,\n    num_heads,\n    num_head_channels,\n    num_heads_upsample,\n    attention_resolutions,\n    dropout,\n    diffusion_steps,\n    noise_schedule,\n    timestep_respacing,\n    use_kl,\n    predict_xstart,\n    rescale_timesteps,\n    rescale_learned_sigmas,\n    use_checkpoint,\n    use_scale_shift_norm,\n    resblock_updown,\n    use_fp16,\n):\n    model = sr_create_model(\n        large_size,\n        small_size,\n        num_channels,\n        num_res_blocks,\n        learn_sigma=learn_sigma,\n        class_cond=class_cond,\n        use_checkpoint=use_checkpoint,\n        attention_resolutions=attention_resolutions,\n        num_heads=num_heads,\n        num_head_channels=num_head_channels,\n        num_heads_upsample=num_heads_upsample,\n        use_scale_shift_norm=use_scale_shift_norm,\n        dropout=dropout,\n        resblock_updown=resblock_updown,\n        use_fp16=use_fp16,\n    )\n    diffusion = create_gaussian_diffusion(\n        steps=diffusion_steps,\n        learn_sigma=learn_sigma,\n        noise_schedule=noise_schedule,\n        use_kl=use_kl,\n        predict_xstart=predict_xstart,\n        rescale_timesteps=rescale_timesteps,\n        rescale_learned_sigmas=rescale_learned_sigmas,\n        timestep_respacing=timestep_respacing,\n    )\n    return model, diffusion\n\n\ndef sr_create_model(\n    large_size,\n    small_size,\n    num_channels,\n    num_res_blocks,\n    learn_sigma,\n    class_cond,\n    use_checkpoint,\n    attention_resolutions,\n    num_heads,\n    num_head_channels,\n    num_heads_upsample,\n    use_scale_shift_norm,\n    dropout,\n    resblock_updown,\n    use_fp16,\n):\n    _ = small_size  # hack to prevent unused variable\n\n    if large_size == 512:\n        channel_mult = (1, 1, 2, 2, 4, 4)\n    elif large_size == 256:\n        channel_mult = (1, 1, 2, 2, 4, 4)\n    elif large_size == 64:\n        channel_mult = (1, 2, 3, 4)\n    else:\n        raise ValueError(f\"unsupported large size: {large_size}\")\n\n    attention_ds = []\n    for res in attention_resolutions.split(\",\"):\n        attention_ds.append(large_size // int(res))\n\n    return SuperResModel(\n        image_size=large_size,\n        in_channels=3,\n        model_channels=num_channels,\n        out_channels=(3 if not learn_sigma else 6),\n        num_res_blocks=num_res_blocks,\n        attention_resolutions=tuple(attention_ds),\n        dropout=dropout,\n        channel_mult=channel_mult,\n        num_classes=(NUM_CLASSES if class_cond else None),\n        use_checkpoint=use_checkpoint,\n        num_heads=num_heads,\n        num_head_channels=num_head_channels,\n        num_heads_upsample=num_heads_upsample,\n        use_scale_shift_norm=use_scale_shift_norm,\n        resblock_updown=resblock_updown,\n        use_fp16=use_fp16,\n    )\n\n\ndef create_gaussian_diffusion(\n    *,\n    steps=1000,\n    learn_sigma=False,\n    sigma_small=False,\n    noise_schedule=\"linear\",\n    use_kl=False,\n    predict_xstart=False,\n    rescale_timesteps=False,\n    rescale_learned_sigmas=False,\n    timestep_respacing=\"\",\n):\n    betas = gd.get_named_beta_schedule(noise_schedule, steps)\n    if use_kl:\n        loss_type = gd.LossType.RESCALED_KL\n    elif rescale_learned_sigmas:\n        loss_type = gd.LossType.RESCALED_MSE\n    else:\n        loss_type = gd.LossType.MSE\n    if not timestep_respacing:\n        timestep_respacing = [steps]\n    return SpacedDiffusion(\n        use_timesteps=space_timesteps(steps, timestep_respacing),\n        betas=betas,\n        model_mean_type=(\n            gd.ModelMeanType.EPSILON if not predict_xstart else gd.ModelMeanType.START_X\n        ),\n        model_var_type=(\n            (\n                gd.ModelVarType.FIXED_LARGE\n                if not sigma_small\n                else gd.ModelVarType.FIXED_SMALL\n            )\n            if not learn_sigma\n            else gd.ModelVarType.LEARNED_RANGE\n        ),\n        loss_type=loss_type,\n        rescale_timesteps=rescale_timesteps,\n    )\n\n\ndef add_dict_to_argparser(parser, default_dict):\n    for k, v in default_dict.items():\n        v_type = type(v)\n        if v is None:\n            v_type = str\n        elif isinstance(v, bool):\n            v_type = str2bool\n        parser.add_argument(f\"--{k}\", default=v, type=v_type)\n\n\ndef args_to_dict(args, keys):\n    return {k: getattr(args, k) for k in keys}\n\n\ndef str2bool(v):\n    \"\"\"\n    https://stackoverflow.com/questions/15008758/parsing-boolean-values-with-argparse\n    \"\"\"\n    if isinstance(v, bool):\n        return v\n    if v.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n        return True\n    elif v.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\"boolean value expected\")\n"}
{"type": "source_file", "path": "Image_Purifier/guided_diffusion/fp16_util.py", "content": "# ---------------------------------------------------------------\n# Taken from the following link as is from:\n# https://github.com/openai/guided-diffusion/blob/main/guided_diffusion/fp16_util.py\n#\n# The license for the original version of this file can be\n# found in this directory (LICENSE_GUIDED_DIFFUSION).\n# ---------------------------------------------------------------\n\n\"\"\"\nHelpers to train with 16-bit precision.\n\"\"\"\n\nimport numpy as np\nimport torch as th\nimport torch.nn as nn\nfrom torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors\n\nfrom . import logger\n\nINITIAL_LOG_LOSS_SCALE = 20.0\n\n\ndef convert_module_to_f16(l):\n    \"\"\"\n    Convert primitive modules to float16.\n    \"\"\"\n    if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Conv3d)):\n        l.weight.data = l.weight.data.half()\n        if l.bias is not None:\n            l.bias.data = l.bias.data.half()\n\n\ndef convert_module_to_f32(l):\n    \"\"\"\n    Convert primitive modules to float32, undoing convert_module_to_f16().\n    \"\"\"\n    if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Conv3d)):\n        l.weight.data = l.weight.data.float()\n        if l.bias is not None:\n            l.bias.data = l.bias.data.float()\n\n\ndef make_master_params(param_groups_and_shapes):\n    \"\"\"\n    Copy model parameters into a (differently-shaped) list of full-precision\n    parameters.\n    \"\"\"\n    master_params = []\n    for param_group, shape in param_groups_and_shapes:\n        master_param = nn.Parameter(\n            _flatten_dense_tensors(\n                [param.detach().float() for (_, param) in param_group]\n            ).view(shape)\n        )\n        master_param.requires_grad = True\n        master_params.append(master_param)\n    return master_params\n\n\ndef model_grads_to_master_grads(param_groups_and_shapes, master_params):\n    \"\"\"\n    Copy the gradients from the model parameters into the master parameters\n    from make_master_params().\n    \"\"\"\n    for master_param, (param_group, shape) in zip(\n        master_params, param_groups_and_shapes\n    ):\n        master_param.grad = _flatten_dense_tensors(\n            [param_grad_or_zeros(param) for (_, param) in param_group]\n        ).view(shape)\n\n\ndef master_params_to_model_params(param_groups_and_shapes, master_params):\n    \"\"\"\n    Copy the master parameter data back into the model parameters.\n    \"\"\"\n    # Without copying to a list, if a generator is passed, this will\n    # silently not copy any parameters.\n    for master_param, (param_group, _) in zip(master_params, param_groups_and_shapes):\n        for (_, param), unflat_master_param in zip(\n            param_group, unflatten_master_params(param_group, master_param.view(-1))\n        ):\n            param.detach().copy_(unflat_master_param)\n\n\ndef unflatten_master_params(param_group, master_param):\n    return _unflatten_dense_tensors(master_param, [param for (_, param) in param_group])\n\n\ndef get_param_groups_and_shapes(named_model_params):\n    named_model_params = list(named_model_params)\n    scalar_vector_named_params = (\n        [(n, p) for (n, p) in named_model_params if p.ndim <= 1],\n        (-1),\n    )\n    matrix_named_params = (\n        [(n, p) for (n, p) in named_model_params if p.ndim > 1],\n        (1, -1),\n    )\n    return [scalar_vector_named_params, matrix_named_params]\n\n\ndef master_params_to_state_dict(\n    model, param_groups_and_shapes, master_params, use_fp16\n):\n    if use_fp16:\n        state_dict = model.state_dict()\n        for master_param, (param_group, _) in zip(\n            master_params, param_groups_and_shapes\n        ):\n            for (name, _), unflat_master_param in zip(\n                param_group, unflatten_master_params(param_group, master_param.view(-1))\n            ):\n                assert name in state_dict\n                state_dict[name] = unflat_master_param\n    else:\n        state_dict = model.state_dict()\n        for i, (name, _value) in enumerate(model.named_parameters()):\n            assert name in state_dict\n            state_dict[name] = master_params[i]\n    return state_dict\n\n\ndef state_dict_to_master_params(model, state_dict, use_fp16):\n    if use_fp16:\n        named_model_params = [\n            (name, state_dict[name]) for name, _ in model.named_parameters()\n        ]\n        param_groups_and_shapes = get_param_groups_and_shapes(named_model_params)\n        master_params = make_master_params(param_groups_and_shapes)\n    else:\n        master_params = [state_dict[name] for name, _ in model.named_parameters()]\n    return master_params\n\n\ndef zero_master_grads(master_params):\n    for param in master_params:\n        param.grad = None\n\n\ndef zero_grad(model_params):\n    for param in model_params:\n        # Taken from https://pytorch.org/docs/stable/_modules/torch/optim/optimizer.html#Optimizer.add_param_group\n        if param.grad is not None:\n            param.grad.detach_()\n            param.grad.zero_()\n\n\ndef param_grad_or_zeros(param):\n    if param.grad is not None:\n        return param.grad.data.detach()\n    else:\n        return th.zeros_like(param)\n\n\nclass MixedPrecisionTrainer:\n    def __init__(\n        self,\n        *,\n        model,\n        use_fp16=False,\n        fp16_scale_growth=1e-3,\n        initial_lg_loss_scale=INITIAL_LOG_LOSS_SCALE,\n    ):\n        self.model = model\n        self.use_fp16 = use_fp16\n        self.fp16_scale_growth = fp16_scale_growth\n\n        self.model_params = list(self.model.parameters())\n        self.master_params = self.model_params\n        self.param_groups_and_shapes = None\n        self.lg_loss_scale = initial_lg_loss_scale\n\n        if self.use_fp16:\n            self.param_groups_and_shapes = get_param_groups_and_shapes(\n                self.model.named_parameters()\n            )\n            self.master_params = make_master_params(self.param_groups_and_shapes)\n            self.model.convert_to_fp16()\n\n    def zero_grad(self):\n        zero_grad(self.model_params)\n\n    def backward(self, loss: th.Tensor):\n        if self.use_fp16:\n            loss_scale = 2 ** self.lg_loss_scale\n            (loss * loss_scale).backward()\n        else:\n            loss.backward()\n\n    def optimize(self, opt: th.optim.Optimizer):\n        if self.use_fp16:\n            return self._optimize_fp16(opt)\n        else:\n            return self._optimize_normal(opt)\n\n    def _optimize_fp16(self, opt: th.optim.Optimizer):\n        logger.logkv_mean(\"lg_loss_scale\", self.lg_loss_scale)\n        model_grads_to_master_grads(self.param_groups_and_shapes, self.master_params)\n        grad_norm, param_norm = self._compute_norms(grad_scale=2 ** self.lg_loss_scale)\n        if check_overflow(grad_norm):\n            self.lg_loss_scale -= 1\n            logger.log(f\"Found NaN, decreased lg_loss_scale to {self.lg_loss_scale}\")\n            zero_master_grads(self.master_params)\n            return False\n\n        logger.logkv_mean(\"grad_norm\", grad_norm)\n        logger.logkv_mean(\"param_norm\", param_norm)\n\n        self.master_params[0].grad.mul_(1.0 / (2 ** self.lg_loss_scale))\n        opt.step()\n        zero_master_grads(self.master_params)\n        master_params_to_model_params(self.param_groups_and_shapes, self.master_params)\n        self.lg_loss_scale += self.fp16_scale_growth\n        return True\n\n    def _optimize_normal(self, opt: th.optim.Optimizer):\n        grad_norm, param_norm = self._compute_norms()\n        logger.logkv_mean(\"grad_norm\", grad_norm)\n        logger.logkv_mean(\"param_norm\", param_norm)\n        opt.step()\n        return True\n\n    def _compute_norms(self, grad_scale=1.0):\n        grad_norm = 0.0\n        param_norm = 0.0\n        for p in self.master_params:\n            with th.no_grad():\n                param_norm += th.norm(p, p=2, dtype=th.float32).item() ** 2\n                if p.grad is not None:\n                    grad_norm += th.norm(p.grad, p=2, dtype=th.float32).item() ** 2\n        return np.sqrt(grad_norm) / grad_scale, np.sqrt(param_norm)\n\n    def master_params_to_state_dict(self, master_params):\n        return master_params_to_state_dict(\n            self.model, self.param_groups_and_shapes, master_params, self.use_fp16\n        )\n\n    def state_dict_to_master_params(self, state_dict):\n        return state_dict_to_master_params(self.model, state_dict, self.use_fp16)\n\n\ndef check_overflow(value):\n    return (value == float(\"inf\")) or (value == -float(\"inf\")) or (value != value)\n"}
{"type": "source_file", "path": "Image_Purifier/guided_diffusion/respace.py", "content": "# ---------------------------------------------------------------\n# Taken from the following link as is from:\n# https://github.com/openai/guided-diffusion/blob/main/guided_diffusion/respace.py\n#\n# The license for the original version of this file can be\n# found in this directory (LICENSE_GUIDED_DIFFUSION).\n# ---------------------------------------------------------------\n\nimport numpy as np\nimport torch as th\n\nfrom .gaussian_diffusion import GaussianDiffusion\n\n\ndef space_timesteps(num_timesteps, section_counts):\n    \"\"\"\n    Create a list of timesteps to use from an original diffusion process,\n    given the number of timesteps we want to take from equally-sized portions\n    of the original process.\n\n    For example, if there's 300 timesteps and the section counts are [10,15,20]\n    then the first 100 timesteps are strided to be 10 timesteps, the second 100\n    are strided to be 15 timesteps, and the final 100 are strided to be 20.\n\n    If the stride is a string starting with \"ddim\", then the fixed striding\n    from the DDIM paper is used, and only one section is allowed.\n\n    :param num_timesteps: the number of diffusion steps in the original\n                          process to divide up.\n    :param section_counts: either a list of numbers, or a string containing\n                           comma-separated numbers, indicating the step count\n                           per section. As a special case, use \"ddimN\" where N\n                           is a number of steps to use the striding from the\n                           DDIM paper.\n    :return: a set of diffusion steps from the original process to use.\n    \"\"\"\n    if isinstance(section_counts, str):\n        if section_counts.startswith(\"ddim\"):\n            desired_count = int(section_counts[len(\"ddim\") :])\n            for i in range(1, num_timesteps):\n                if len(range(0, num_timesteps, i)) == desired_count:\n                    return set(range(0, num_timesteps, i))\n            raise ValueError(\n                f\"cannot create exactly {num_timesteps} steps with an integer stride\"\n            )\n        section_counts = [int(x) for x in section_counts.split(\",\")]\n    size_per = num_timesteps // len(section_counts)\n    extra = num_timesteps % len(section_counts)\n    start_idx = 0\n    all_steps = []\n    for i, section_count in enumerate(section_counts):\n        size = size_per + (1 if i < extra else 0)\n        if size < section_count:\n            raise ValueError(\n                f\"cannot divide section of {size} steps into {section_count}\"\n            )\n        if section_count <= 1:\n            frac_stride = 1\n        else:\n            frac_stride = (size - 1) / (section_count - 1)\n        cur_idx = 0.0\n        taken_steps = []\n        for _ in range(section_count):\n            taken_steps.append(start_idx + round(cur_idx))\n            cur_idx += frac_stride\n        all_steps += taken_steps\n        start_idx += size\n    return set(all_steps)\n\n\nclass SpacedDiffusion(GaussianDiffusion):\n    \"\"\"\n    A diffusion process which can skip steps in a base diffusion process.\n\n    :param use_timesteps: a collection (sequence or set) of timesteps from the\n                          original diffusion process to retain.\n    :param kwargs: the kwargs to create the base diffusion process.\n    \"\"\"\n\n    def __init__(self, use_timesteps, **kwargs):\n        self.use_timesteps = set(use_timesteps)\n        self.timestep_map = []\n        self.original_num_steps = len(kwargs[\"betas\"])\n\n        base_diffusion = GaussianDiffusion(**kwargs)  # pylint: disable=missing-kwoa\n        last_alpha_cumprod = 1.0\n        new_betas = []\n        for i, alpha_cumprod in enumerate(base_diffusion.alphas_cumprod):\n            if i in self.use_timesteps:\n                new_betas.append(1 - alpha_cumprod / last_alpha_cumprod)\n                last_alpha_cumprod = alpha_cumprod\n                self.timestep_map.append(i)\n        kwargs[\"betas\"] = np.array(new_betas)\n        super().__init__(**kwargs)\n\n    def p_mean_variance(\n        self, model, *args, **kwargs\n    ):  # pylint: disable=signature-differs\n        return super().p_mean_variance(self._wrap_model(model), *args, **kwargs)\n\n    def training_losses(\n        self, model, *args, **kwargs\n    ):  # pylint: disable=signature-differs\n        return super().training_losses(self._wrap_model(model), *args, **kwargs)\n\n    def condition_mean(self, cond_fn, *args, **kwargs):\n        return super().condition_mean(self._wrap_model(cond_fn), *args, **kwargs)\n\n    def condition_score(self, cond_fn, *args, **kwargs):\n        return super().condition_score(self._wrap_model(cond_fn), *args, **kwargs)\n\n    def _wrap_model(self, model):\n        if isinstance(model, _WrappedModel):\n            return model\n        return _WrappedModel(\n            model, self.timestep_map, self.rescale_timesteps, self.original_num_steps\n        )\n\n    def _scale_timesteps(self, t):\n        # Scaling is done by the wrapped model.\n        return t\n\n\nclass _WrappedModel:\n    def __init__(self, model, timestep_map, rescale_timesteps, original_num_steps):\n        self.model = model\n        self.timestep_map = timestep_map\n        self.rescale_timesteps = rescale_timesteps\n        self.original_num_steps = original_num_steps\n\n    def __call__(self, x, ts, **kwargs):\n        map_tensor = th.tensor(self.timestep_map, device=ts.device, dtype=ts.dtype)\n        new_ts = map_tensor[ts]\n        if self.rescale_timesteps:\n            new_ts = new_ts.float() * (1000.0 / self.original_num_steps)\n        return self.model(x, new_ts, **kwargs)\n"}
{"type": "source_file", "path": "Image_Purifier/guided_diffusion/nn.py", "content": "# ---------------------------------------------------------------\n# Taken from the following link as is from:\n# https://github.com/openai/guided-diffusion/blob/main/guided_diffusion/nn.py\n#\n# The license for the original version of this file can be\n# found in this directory (LICENSE_GUIDED_DIFFUSION).\n# ---------------------------------------------------------------\n\n\"\"\"\nVarious utilities for neural networks.\n\"\"\"\n\nimport math\n\nimport torch as th\nimport torch.nn as nn\n\n\n# PyTorch 1.7 has SiLU, but we support PyTorch 1.5.\nclass SiLU(nn.Module):\n    def forward(self, x):\n        return x * th.sigmoid(x)\n\n\nclass GroupNorm32(nn.GroupNorm):\n    def forward(self, x):\n        return super().forward(x.float()).type(x.dtype)\n\n\ndef conv_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D convolution module.\n    \"\"\"\n    if dims == 1:\n        return nn.Conv1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.Conv2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.Conv3d(*args, **kwargs)\n    raise ValueError(f\"unsupported dimensions: {dims}\")\n\n\ndef linear(*args, **kwargs):\n    \"\"\"\n    Create a linear module.\n    \"\"\"\n    return nn.Linear(*args, **kwargs)\n\n\ndef avg_pool_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D average pooling module.\n    \"\"\"\n    if dims == 1:\n        return nn.AvgPool1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.AvgPool2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.AvgPool3d(*args, **kwargs)\n    raise ValueError(f\"unsupported dimensions: {dims}\")\n\n\ndef update_ema(target_params, source_params, rate=0.99):\n    \"\"\"\n    Update target parameters to be closer to those of source parameters using\n    an exponential moving average.\n\n    :param target_params: the target parameter sequence.\n    :param source_params: the source parameter sequence.\n    :param rate: the EMA rate (closer to 1 means slower).\n    \"\"\"\n    for targ, src in zip(target_params, source_params):\n        targ.detach().mul_(rate).add_(src, alpha=1 - rate)\n\n\ndef zero_module(module):\n    \"\"\"\n    Zero out the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().zero_()\n    return module\n\n\ndef scale_module(module, scale):\n    \"\"\"\n    Scale the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().mul_(scale)\n    return module\n\n\ndef mean_flat(tensor):\n    \"\"\"\n    Take the mean over all non-batch dimensions.\n    \"\"\"\n    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n\n\ndef normalization(channels):\n    \"\"\"\n    Make a standard normalization layer.\n\n    :param channels: number of input channels.\n    :return: an nn.Module for normalization.\n    \"\"\"\n    return GroupNorm32(32, channels)\n\n\ndef timestep_embedding(timesteps, dim, max_period=10000):\n    \"\"\"\n    Create sinusoidal timestep embeddings.\n\n    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n                      These may be fractional.\n    :param dim: the dimension of the output.\n    :param max_period: controls the minimum frequency of the embeddings.\n    :return: an [N x dim] Tensor of positional embeddings.\n    \"\"\"\n    half = dim // 2\n    freqs = th.exp(\n        -math.log(max_period) * th.arange(start=0, end=half, dtype=th.float32) / half\n    ).to(device=timesteps.device)\n    args = timesteps[:, None].float() * freqs[None]\n    embedding = th.cat([th.cos(args), th.sin(args)], dim=-1)\n    if dim % 2:\n        embedding = th.cat([embedding, th.zeros_like(embedding[:, :1])], dim=-1)\n    return embedding\n\n\ndef checkpoint(func, inputs, params, flag):\n    \"\"\"\n    Evaluate a function without caching intermediate activations, allowing for\n    reduced memory at the expense of extra compute in the backward pass.\n\n    :param func: the function to evaluate.\n    :param inputs: the argument sequence to pass to `func`.\n    :param params: a sequence of parameters `func` depends on but does not\n                   explicitly take as arguments.\n    :param flag: if False, disable gradient checkpointing.\n    \"\"\"\n    if flag:\n        args = tuple(inputs) + tuple(params)\n        return CheckpointFunction.apply(func, len(inputs), *args)\n    else:\n        return func(*inputs)\n\n\nclass CheckpointFunction(th.autograd.Function):\n    @staticmethod\n    def forward(ctx, run_function, length, *args):\n        ctx.run_function = run_function\n        ctx.input_tensors = list(args[:length])\n        ctx.input_params = list(args[length:])\n        with th.no_grad():\n            output_tensors = ctx.run_function(*ctx.input_tensors)\n        return output_tensors\n\n    @staticmethod\n    def backward(ctx, *output_grads):\n        ctx.input_tensors = [x.detach().requires_grad_(True) for x in ctx.input_tensors]\n        with th.enable_grad():\n            # Fixes a bug where the first op in run_function modifies the\n            # Tensor storage in place, which is not allowed for detach()'d\n            # Tensors.\n            shallow_copies = [x.view_as(x) for x in ctx.input_tensors]\n            output_tensors = ctx.run_function(*shallow_copies)\n        input_grads = th.autograd.grad(\n            output_tensors,\n            ctx.input_tensors + ctx.input_params,\n            output_grads,\n            allow_unused=True,\n        )\n        del ctx.input_tensors\n        del ctx.input_params\n        del output_tensors\n        return (None, None) + input_grads\n"}
{"type": "source_file", "path": "Image_Purifier/guided_diffusion/dist_util.py", "content": "# ---------------------------------------------------------------\n# Taken from the following link as is from:\n# https://github.com/openai/guided-diffusion/blob/main/guided_diffusion/dist_util.py\n#\n# The license for the original version of this file can be\n# found in this directory (LICENSE_GUIDED_DIFFUSION).\n# ---------------------------------------------------------------\n\n\"\"\"\nHelpers for distributed training.\n\"\"\"\n\nimport io\nimport os\nimport socket\n\nimport blobfile as bf\nfrom mpi4py import MPI\nimport torch as th\nimport torch.distributed as dist\n\n# Change this to reflect your cluster layout.\n# The GPU for a given rank is (rank % GPUS_PER_NODE).\nGPUS_PER_NODE = 8\n\nSETUP_RETRY_COUNT = 3\n\n\ndef setup_dist():\n    \"\"\"\n    Setup a distributed process group.\n    \"\"\"\n    if dist.is_initialized():\n        return\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{MPI.COMM_WORLD.Get_rank() % GPUS_PER_NODE}\"\n\n    comm = MPI.COMM_WORLD\n    backend = \"gloo\" if not th.cuda.is_available() else \"nccl\"\n\n    if backend == \"gloo\":\n        hostname = \"localhost\"\n    else:\n        hostname = socket.gethostbyname(socket.getfqdn())\n    os.environ[\"MASTER_ADDR\"] = comm.bcast(hostname, root=0)\n    os.environ[\"RANK\"] = str(comm.rank)\n    os.environ[\"WORLD_SIZE\"] = str(comm.size)\n\n    port = comm.bcast(_find_free_port(), root=0)\n    os.environ[\"MASTER_PORT\"] = str(port)\n    dist.init_process_group(backend=backend, init_method=\"env://\")\n\n\ndef dev():\n    \"\"\"\n    Get the device to use for torch.distributed.\n    \"\"\"\n    if th.cuda.is_available():\n        return th.device(f\"cuda\")\n    return th.device(\"cpu\")\n\n\ndef load_state_dict(path, **kwargs):\n    \"\"\"\n    Load a PyTorch file without redundant fetches across MPI ranks.\n    \"\"\"\n    chunk_size = 2 ** 30  # MPI has a relatively small size limit\n    if MPI.COMM_WORLD.Get_rank() == 0:\n        with bf.BlobFile(path, \"rb\") as f:\n            data = f.read()\n        num_chunks = len(data) // chunk_size\n        if len(data) % chunk_size:\n            num_chunks += 1\n        MPI.COMM_WORLD.bcast(num_chunks)\n        for i in range(0, len(data), chunk_size):\n            MPI.COMM_WORLD.bcast(data[i : i + chunk_size])\n    else:\n        num_chunks = MPI.COMM_WORLD.bcast(None)\n        data = bytes()\n        for _ in range(num_chunks):\n            data += MPI.COMM_WORLD.bcast(None)\n\n    return th.load(io.BytesIO(data), **kwargs)\n\n\ndef sync_params(params):\n    \"\"\"\n    Synchronize a sequence of Tensors across ranks from rank 0.\n    \"\"\"\n    for p in params:\n        with th.no_grad():\n            dist.broadcast(p, 0)\n\n\ndef _find_free_port():\n    try:\n        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        s.bind((\"\", 0))\n        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        return s.getsockname()[1]\n    finally:\n        s.close()\n"}
{"type": "source_file", "path": "Image_Purifier/guided_diffusion/image_datasets.py", "content": "# ---------------------------------------------------------------\n# Taken from the following link as is from:\n# https://github.com/openai/guided-diffusion/blob/main/guided_diffusion/image_datasets.py\n#\n# The license for the original version of this file can be\n# found in this directory (LICENSE_GUIDED_DIFFUSION).\n# ---------------------------------------------------------------\n\nimport math\nimport random\n\nfrom PIL import Image\nimport blobfile as bf\nfrom mpi4py import MPI\nimport numpy as np\nfrom torch.utils.data import DataLoader, Dataset\n\n\ndef load_data(\n    *,\n    data_dir,\n    batch_size,\n    image_size,\n    class_cond=False,\n    deterministic=False,\n    random_crop=False,\n    random_flip=True,\n):\n    \"\"\"\n    For a dataset, create a generator over (images, kwargs) pairs.\n\n    Each images is an NCHW float tensor, and the kwargs dict contains zero or\n    more keys, each of which map to a batched Tensor of their own.\n    The kwargs dict can be used for class labels, in which case the key is \"y\"\n    and the values are integer tensors of class labels.\n\n    :param data_dir: a dataset directory.\n    :param batch_size: the batch size of each returned pair.\n    :param image_size: the size to which images are resized.\n    :param class_cond: if True, include a \"y\" key in returned dicts for class\n                       label. If classes are not available and this is true, an\n                       exception will be raised.\n    :param deterministic: if True, yield results in a deterministic order.\n    :param random_crop: if True, randomly crop the images for augmentation.\n    :param random_flip: if True, randomly flip the images for augmentation.\n    \"\"\"\n    if not data_dir:\n        raise ValueError(\"unspecified data directory\")\n    all_files = _list_image_files_recursively(data_dir)\n    classes = None\n    if class_cond:\n        # Assume classes are the first part of the filename,\n        # before an underscore.\n        class_names = [bf.basename(path).split(\"_\")[0] for path in all_files]\n        sorted_classes = {x: i for i, x in enumerate(sorted(set(class_names)))}\n        classes = [sorted_classes[x] for x in class_names]\n    dataset = ImageDataset(\n        image_size,\n        all_files,\n        classes=classes,\n        shard=MPI.COMM_WORLD.Get_rank(),\n        num_shards=MPI.COMM_WORLD.Get_size(),\n        random_crop=random_crop,\n        random_flip=random_flip,\n    )\n    if deterministic:\n        loader = DataLoader(\n            dataset, batch_size=batch_size, shuffle=False, num_workers=1, drop_last=True\n        )\n    else:\n        loader = DataLoader(\n            dataset, batch_size=batch_size, shuffle=True, num_workers=1, drop_last=True\n        )\n    while True:\n        yield from loader\n\n\ndef _list_image_files_recursively(data_dir):\n    results = []\n    for entry in sorted(bf.listdir(data_dir)):\n        full_path = bf.join(data_dir, entry)\n        ext = entry.split(\".\")[-1]\n        if \".\" in entry and ext.lower() in [\"jpg\", \"jpeg\", \"png\", \"gif\"]:\n            results.append(full_path)\n        elif bf.isdir(full_path):\n            results.extend(_list_image_files_recursively(full_path))\n    return results\n\n\nclass ImageDataset(Dataset):\n    def __init__(\n        self,\n        resolution,\n        image_paths,\n        classes=None,\n        shard=0,\n        num_shards=1,\n        random_crop=False,\n        random_flip=True,\n    ):\n        super().__init__()\n        self.resolution = resolution\n        self.local_images = image_paths[shard:][::num_shards]\n        self.local_classes = None if classes is None else classes[shard:][::num_shards]\n        self.random_crop = random_crop\n        self.random_flip = random_flip\n\n    def __len__(self):\n        return len(self.local_images)\n\n    def __getitem__(self, idx):\n        path = self.local_images[idx]\n        with bf.BlobFile(path, \"rb\") as f:\n            pil_image = Image.open(f)\n            pil_image.load()\n        pil_image = pil_image.convert(\"RGB\")\n\n        if self.random_crop:\n            arr = random_crop_arr(pil_image, self.resolution)\n        else:\n            arr = center_crop_arr(pil_image, self.resolution)\n\n        if self.random_flip and random.random() < 0.5:\n            arr = arr[:, ::-1]\n\n        arr = arr.astype(np.float32) / 127.5 - 1\n\n        out_dict = {}\n        if self.local_classes is not None:\n            out_dict[\"y\"] = np.array(self.local_classes[idx], dtype=np.int64)\n        return np.transpose(arr, [2, 0, 1]), out_dict\n\n\ndef center_crop_arr(pil_image, image_size):\n    # We are not on a new enough PIL to support the `reducing_gap`\n    # argument, which uses BOX downsampling at powers of two first.\n    # Thus, we do it by hand to improve downsample quality.\n    while min(*pil_image.size) >= 2 * image_size:\n        pil_image = pil_image.resize(\n            tuple(x // 2 for x in pil_image.size), resample=Image.BOX\n        )\n\n    scale = image_size / min(*pil_image.size)\n    pil_image = pil_image.resize(\n        tuple(round(x * scale) for x in pil_image.size), resample=Image.BICUBIC\n    )\n\n    arr = np.array(pil_image)\n    crop_y = (arr.shape[0] - image_size) // 2\n    crop_x = (arr.shape[1] - image_size) // 2\n    return arr[crop_y : crop_y + image_size, crop_x : crop_x + image_size]\n\n\ndef random_crop_arr(pil_image, image_size, min_crop_frac=0.8, max_crop_frac=1.0):\n    min_smaller_dim_size = math.ceil(image_size / max_crop_frac)\n    max_smaller_dim_size = math.ceil(image_size / min_crop_frac)\n    smaller_dim_size = random.randrange(min_smaller_dim_size, max_smaller_dim_size + 1)\n\n    # We are not on a new enough PIL to support the `reducing_gap`\n    # argument, which uses BOX downsampling at powers of two first.\n    # Thus, we do it by hand to improve downsample quality.\n    while min(*pil_image.size) >= 2 * smaller_dim_size:\n        pil_image = pil_image.resize(\n            tuple(x // 2 for x in pil_image.size), resample=Image.BOX\n        )\n\n    scale = smaller_dim_size / min(*pil_image.size)\n    pil_image = pil_image.resize(\n        tuple(round(x * scale) for x in pil_image.size), resample=Image.BICUBIC\n    )\n\n    arr = np.array(pil_image)\n    crop_y = random.randrange(arr.shape[0] - image_size + 1)\n    crop_x = random.randrange(arr.shape[1] - image_size + 1)\n    return arr[crop_y : crop_y + image_size, crop_x : crop_x + image_size]\n"}
{"type": "source_file", "path": "Image_Purifier/guided_diffusion/__init__.py", "content": "# ---------------------------------------------------------------\n# Taken from the following link as is from:\n# https://github.com/openai/guided-diffusion/blob/main/guided_diffusion/__init__.py\n#\n# The license for the original version of this file can be\n# found in this directory (LICENSE_GUIDED_DIFFUSION).\n# ---------------------------------------------------------------\n\n\"\"\"\nCodebase for \"Improved Denoising Diffusion Probabilistic Models\".\n\"\"\"\n"}
{"type": "source_file", "path": "Image_Purifier/ddpm/unet_ddpm.py", "content": "# ---------------------------------------------------------------\n# Taken from the following link as is from:\n# https://github.com/ermongroup/SDEdit/blob/main/models/diffusion.py\n#\n# The license for the original version of this file can be\n# found in this directory (LICENSE_UNET_DDPM).\n# ---------------------------------------------------------------\n\nimport math\nimport torch\nimport torch.nn as nn\n\n\ndef get_timestep_embedding(timesteps, embedding_dim):\n    \"\"\"\n    This matches the implementation in Denoising Diffusion Probabilistic Models:\n    From Fairseq.\n    Build sinusoidal embeddings.\n    This matches the implementation in tensor2tensor, but differs slightly\n    from the description in Section 3.5 of \"Attention Is All You Need\".\n    \"\"\"\n    assert len(timesteps.shape) == 1\n\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = emb.to(device=timesteps.device)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n    return emb\n\n\ndef nonlinearity(x):\n    # swish\n    return x * torch.sigmoid(x)\n\n\ndef Normalize(in_channels):\n    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n\n\nclass Upsample(nn.Module):\n    def __init__(self, in_channels, with_conv):\n        super().__init__()\n        self.with_conv = with_conv\n        if self.with_conv:\n            self.conv = torch.nn.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)\n\n    def forward(self, x):\n        x = torch.nn.functional.interpolate(\n            x, scale_factor=2.0, mode=\"nearest\")\n        if self.with_conv:\n            x = self.conv(x)\n        return x\n\n\nclass Downsample(nn.Module):\n    def __init__(self, in_channels, with_conv):\n        super().__init__()\n        self.with_conv = with_conv\n        if self.with_conv:\n            # no asymmetric padding in torch conv, must do it ourselves\n            self.conv = torch.nn.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=3,\n                                        stride=2,\n                                        padding=0)\n\n    def forward(self, x):\n        if self.with_conv:\n            pad = (0, 1, 0, 1)\n            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0)\n            x = self.conv(x)\n        else:\n            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n        return x\n\n\nclass ResnetBlock(nn.Module):\n    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n                 dropout, temb_channels=512):\n        super().__init__()\n        self.in_channels = in_channels\n        out_channels = in_channels if out_channels is None else out_channels\n        self.out_channels = out_channels\n        self.use_conv_shortcut = conv_shortcut\n\n        self.norm1 = Normalize(in_channels)\n        self.conv1 = torch.nn.Conv2d(in_channels,\n                                     out_channels,\n                                     kernel_size=3,\n                                     stride=1,\n                                     padding=1)\n        self.temb_proj = torch.nn.Linear(temb_channels,\n                                         out_channels)\n        self.norm2 = Normalize(out_channels)\n        self.dropout = torch.nn.Dropout(dropout)\n        self.conv2 = torch.nn.Conv2d(out_channels,\n                                     out_channels,\n                                     kernel_size=3,\n                                     stride=1,\n                                     padding=1)\n        if self.in_channels != self.out_channels:\n            if self.use_conv_shortcut:\n                self.conv_shortcut = torch.nn.Conv2d(in_channels,\n                                                     out_channels,\n                                                     kernel_size=3,\n                                                     stride=1,\n                                                     padding=1)\n            else:\n                self.nin_shortcut = torch.nn.Conv2d(in_channels,\n                                                    out_channels,\n                                                    kernel_size=1,\n                                                    stride=1,\n                                                    padding=0)\n\n    def forward(self, x, temb):\n        h = x\n        h = self.norm1(h)\n        h = nonlinearity(h)\n        h = self.conv1(h)\n\n        h = h + self.temb_proj(nonlinearity(temb))[:, :, None, None]\n\n        h = self.norm2(h)\n        h = nonlinearity(h)\n        h = self.dropout(h)\n        h = self.conv2(h)\n\n        if self.in_channels != self.out_channels:\n            if self.use_conv_shortcut:\n                x = self.conv_shortcut(x)\n            else:\n                x = self.nin_shortcut(x)\n\n        return x + h\n\n\nclass AttnBlock(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.in_channels = in_channels\n\n        self.norm = Normalize(in_channels)\n        self.q = torch.nn.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.k = torch.nn.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.v = torch.nn.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.proj_out = torch.nn.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=1,\n                                        stride=1,\n                                        padding=0)\n\n    def forward(self, x):\n        h_ = x\n        h_ = self.norm(h_)\n        q = self.q(h_)\n        k = self.k(h_)\n        v = self.v(h_)\n\n        # compute attention\n        b, c, h, w = q.shape\n        q = q.reshape(b, c, h * w)\n        q = q.permute(0, 2, 1)  # b,hw,c\n        k = k.reshape(b, c, h * w)  # b,c,hw\n        w_ = torch.bmm(q, k)  # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n        w_ = w_ * (int(c) ** (-0.5))\n        w_ = torch.nn.functional.softmax(w_, dim=2)\n\n        # attend to values\n        v = v.reshape(b, c, h * w)\n        w_ = w_.permute(0, 2, 1)  # b,hw,hw (first hw of k, second of q)\n        # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n        h_ = torch.bmm(v, w_)\n        h_ = h_.reshape(b, c, h, w)\n\n        h_ = self.proj_out(h_)\n\n        return x + h_\n\n\nclass Model(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        ch, out_ch, ch_mult = config.model.ch, config.model.out_ch, tuple(config.model.ch_mult)\n        num_res_blocks = config.model.num_res_blocks\n        attn_resolutions = config.model.attn_resolutions\n        dropout = config.model.dropout\n        in_channels = config.model.in_channels\n        resolution = config.data.image_size\n        resamp_with_conv = config.model.resamp_with_conv\n\n        self.ch = ch\n        self.temb_ch = self.ch * 4\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.resolution = resolution\n        self.in_channels = in_channels\n\n        # timestep embedding\n        self.temb = nn.Module()\n        self.temb.dense = nn.ModuleList([\n            torch.nn.Linear(self.ch,\n                            self.temb_ch),\n            torch.nn.Linear(self.temb_ch,\n                            self.temb_ch),\n        ])\n\n        # downsampling\n        self.conv_in = torch.nn.Conv2d(in_channels,\n                                       self.ch,\n                                       kernel_size=3,\n                                       stride=1,\n                                       padding=1)\n\n        curr_res = resolution\n        in_ch_mult = (1,) + ch_mult\n        self.down = nn.ModuleList()\n        block_in = None\n        for i_level in range(self.num_resolutions):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_in = ch * in_ch_mult[i_level]\n            block_out = ch * ch_mult[i_level]\n            for i_block in range(self.num_res_blocks):\n                block.append(ResnetBlock(in_channels=block_in,\n                                         out_channels=block_out,\n                                         temb_channels=self.temb_ch,\n                                         dropout=dropout))\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(AttnBlock(block_in))\n            down = nn.Module()\n            down.block = block\n            down.attn = attn\n            if i_level != self.num_resolutions - 1:\n                down.downsample = Downsample(block_in, resamp_with_conv)\n                curr_res = curr_res // 2\n            self.down.append(down)\n\n        # middle\n        self.mid = nn.Module()\n        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n        self.mid.attn_1 = AttnBlock(block_in)\n        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n\n        # upsampling\n        self.up = nn.ModuleList()\n        for i_level in reversed(range(self.num_resolutions)):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_out = ch * ch_mult[i_level]\n            skip_in = ch * ch_mult[i_level]\n            for i_block in range(self.num_res_blocks + 1):\n                if i_block == self.num_res_blocks:\n                    skip_in = ch * in_ch_mult[i_level]\n                block.append(ResnetBlock(in_channels=block_in + skip_in,\n                                         out_channels=block_out,\n                                         temb_channels=self.temb_ch,\n                                         dropout=dropout))\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(AttnBlock(block_in))\n            up = nn.Module()\n            up.block = block\n            up.attn = attn\n            if i_level != 0:\n                up.upsample = Upsample(block_in, resamp_with_conv)\n                curr_res = curr_res * 2\n            self.up.insert(0, up)  # prepend to get consistent order\n\n        # end\n        self.norm_out = Normalize(block_in)\n        self.conv_out = torch.nn.Conv2d(block_in,\n                                        out_ch,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)\n\n    def forward(self, x, t):\n        assert x.shape[2] == x.shape[3] == self.resolution\n\n        # timestep embedding\n        temb = get_timestep_embedding(t, self.ch)\n        temb = self.temb.dense[0](temb)\n        temb = nonlinearity(temb)\n        temb = self.temb.dense[1](temb)\n\n        # downsampling\n        hs = [self.conv_in(x)]\n        for i_level in range(self.num_resolutions):\n            for i_block in range(self.num_res_blocks):\n                h = self.down[i_level].block[i_block](hs[-1], temb)\n                if len(self.down[i_level].attn) > 0:\n                    h = self.down[i_level].attn[i_block](h)\n                hs.append(h)\n            if i_level != self.num_resolutions - 1:\n                hs.append(self.down[i_level].downsample(hs[-1]))\n\n        # middle\n        h = hs[-1]\n        h = self.mid.block_1(h, temb)\n        h = self.mid.attn_1(h)\n        h = self.mid.block_2(h, temb)\n\n        # upsampling\n        for i_level in reversed(range(self.num_resolutions)):\n            for i_block in range(self.num_res_blocks + 1):\n                h = self.up[i_level].block[i_block](\n                    torch.cat([h, hs.pop()], dim=1), temb)\n                if len(self.up[i_level].attn) > 0:\n                    h = self.up[i_level].attn[i_block](h)\n            if i_level != 0:\n                h = self.up[i_level].upsample(h)\n\n        # end\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h)\n        return h\n"}
{"type": "source_file", "path": "Image_Purifier/guided_diffusion/logger.py", "content": "# ---------------------------------------------------------------\n# Taken from the following link as is from:\n# https://github.com/openai/guided-diffusion/blob/main/guided_diffusion/logger.py\n#\n# The license for the original version of this file can be\n# found in this directory (LICENSE_GUIDED_DIFFUSION).\n# ---------------------------------------------------------------\n\n\"\"\"\nLogger copied from OpenAI baselines to avoid extra RL-based dependencies:\nhttps://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/logger.py\n\"\"\"\n\nimport os\nimport sys\nimport shutil\nimport os.path as osp\nimport json\nimport time\nimport datetime\nimport tempfile\nimport warnings\nfrom collections import defaultdict\nfrom contextlib import contextmanager\n\nDEBUG = 10\nINFO = 20\nWARN = 30\nERROR = 40\n\nDISABLED = 50\n\n\nclass KVWriter(object):\n    def writekvs(self, kvs):\n        raise NotImplementedError\n\n\nclass SeqWriter(object):\n    def writeseq(self, seq):\n        raise NotImplementedError\n\n\nclass HumanOutputFormat(KVWriter, SeqWriter):\n    def __init__(self, filename_or_file):\n        if isinstance(filename_or_file, str):\n            self.file = open(filename_or_file, \"wt\")\n            self.own_file = True\n        else:\n            assert hasattr(filename_or_file, \"read\"), (\n                \"expected file or str, got %s\" % filename_or_file\n            )\n            self.file = filename_or_file\n            self.own_file = False\n\n    def writekvs(self, kvs):\n        # Create strings for printing\n        key2str = {}\n        for (key, val) in sorted(kvs.items()):\n            if hasattr(val, \"__float__\"):\n                valstr = \"%-8.3g\" % val\n            else:\n                valstr = str(val)\n            key2str[self._truncate(key)] = self._truncate(valstr)\n\n        # Find max widths\n        if len(key2str) == 0:\n            print(\"WARNING: tried to write empty key-value dict\")\n            return\n        else:\n            keywidth = max(map(len, key2str.keys()))\n            valwidth = max(map(len, key2str.values()))\n\n        # Write out the data\n        dashes = \"-\" * (keywidth + valwidth + 7)\n        lines = [dashes]\n        for (key, val) in sorted(key2str.items(), key=lambda kv: kv[0].lower()):\n            lines.append(\n                \"| %s%s | %s%s |\"\n                % (key, \" \" * (keywidth - len(key)), val, \" \" * (valwidth - len(val)))\n            )\n        lines.append(dashes)\n        self.file.write(\"\\n\".join(lines) + \"\\n\")\n\n        # Flush the output to the file\n        self.file.flush()\n\n    def _truncate(self, s):\n        maxlen = 30\n        return s[: maxlen - 3] + \"...\" if len(s) > maxlen else s\n\n    def writeseq(self, seq):\n        seq = list(seq)\n        for (i, elem) in enumerate(seq):\n            self.file.write(elem)\n            if i < len(seq) - 1:  # add space unless this is the last one\n                self.file.write(\" \")\n        self.file.write(\"\\n\")\n        self.file.flush()\n\n    def close(self):\n        if self.own_file:\n            self.file.close()\n\n\nclass JSONOutputFormat(KVWriter):\n    def __init__(self, filename):\n        self.file = open(filename, \"wt\")\n\n    def writekvs(self, kvs):\n        for k, v in sorted(kvs.items()):\n            if hasattr(v, \"dtype\"):\n                kvs[k] = float(v)\n        self.file.write(json.dumps(kvs) + \"\\n\")\n        self.file.flush()\n\n    def close(self):\n        self.file.close()\n\n\nclass CSVOutputFormat(KVWriter):\n    def __init__(self, filename):\n        self.file = open(filename, \"w+t\")\n        self.keys = []\n        self.sep = \",\"\n\n    def writekvs(self, kvs):\n        # Add our current row to the history\n        extra_keys = list(kvs.keys() - self.keys)\n        extra_keys.sort()\n        if extra_keys:\n            self.keys.extend(extra_keys)\n            self.file.seek(0)\n            lines = self.file.readlines()\n            self.file.seek(0)\n            for (i, k) in enumerate(self.keys):\n                if i > 0:\n                    self.file.write(\",\")\n                self.file.write(k)\n            self.file.write(\"\\n\")\n            for line in lines[1:]:\n                self.file.write(line[:-1])\n                self.file.write(self.sep * len(extra_keys))\n                self.file.write(\"\\n\")\n        for (i, k) in enumerate(self.keys):\n            if i > 0:\n                self.file.write(\",\")\n            v = kvs.get(k)\n            if v is not None:\n                self.file.write(str(v))\n        self.file.write(\"\\n\")\n        self.file.flush()\n\n    def close(self):\n        self.file.close()\n\n\nclass TensorBoardOutputFormat(KVWriter):\n    \"\"\"\n    Dumps key/value pairs into TensorBoard's numeric format.\n    \"\"\"\n\n    def __init__(self, dir):\n        os.makedirs(dir, exist_ok=True)\n        self.dir = dir\n        self.step = 1\n        prefix = \"events\"\n        path = osp.join(osp.abspath(dir), prefix)\n        import tensorflow as tf\n        from tensorflow.python import pywrap_tensorflow\n        from tensorflow.core.util import event_pb2\n        from tensorflow.python.util import compat\n\n        self.tf = tf\n        self.event_pb2 = event_pb2\n        self.pywrap_tensorflow = pywrap_tensorflow\n        self.writer = pywrap_tensorflow.EventsWriter(compat.as_bytes(path))\n\n    def writekvs(self, kvs):\n        def summary_val(k, v):\n            kwargs = {\"tag\": k, \"simple_value\": float(v)}\n            return self.tf.Summary.Value(**kwargs)\n\n        summary = self.tf.Summary(value=[summary_val(k, v) for k, v in kvs.items()])\n        event = self.event_pb2.Event(wall_time=time.time(), summary=summary)\n        event.step = (\n            self.step\n        )  # is there any reason why you'd want to specify the step?\n        self.writer.WriteEvent(event)\n        self.writer.Flush()\n        self.step += 1\n\n    def close(self):\n        if self.writer:\n            self.writer.Close()\n            self.writer = None\n\n\ndef make_output_format(format, ev_dir, log_suffix=\"\"):\n    os.makedirs(ev_dir, exist_ok=True)\n    if format == \"stdout\":\n        return HumanOutputFormat(sys.stdout)\n    elif format == \"log\":\n        return HumanOutputFormat(osp.join(ev_dir, \"log%s.txt\" % log_suffix))\n    elif format == \"json\":\n        return JSONOutputFormat(osp.join(ev_dir, \"progress%s.json\" % log_suffix))\n    elif format == \"csv\":\n        return CSVOutputFormat(osp.join(ev_dir, \"progress%s.csv\" % log_suffix))\n    elif format == \"tensorboard\":\n        return TensorBoardOutputFormat(osp.join(ev_dir, \"tb%s\" % log_suffix))\n    else:\n        raise ValueError(\"Unknown format specified: %s\" % (format,))\n\n\n# ================================================================\n# API\n# ================================================================\n\n\ndef logkv(key, val):\n    \"\"\"\n    Log a value of some diagnostic\n    Call this once for each diagnostic quantity, each iteration\n    If called many times, last value will be used.\n    \"\"\"\n    get_current().logkv(key, val)\n\n\ndef logkv_mean(key, val):\n    \"\"\"\n    The same as logkv(), but if called many times, values averaged.\n    \"\"\"\n    get_current().logkv_mean(key, val)\n\n\ndef logkvs(d):\n    \"\"\"\n    Log a dictionary of key-value pairs\n    \"\"\"\n    for (k, v) in d.items():\n        logkv(k, v)\n\n\ndef dumpkvs():\n    \"\"\"\n    Write all of the diagnostics from the current iteration\n    \"\"\"\n    return get_current().dumpkvs()\n\n\ndef getkvs():\n    return get_current().name2val\n\n\ndef log(*args, level=INFO):\n    \"\"\"\n    Write the sequence of args, with no separators, to the console and output files (if you've configured an output file).\n    \"\"\"\n    get_current().log(*args, level=level)\n\n\ndef debug(*args):\n    log(*args, level=DEBUG)\n\n\ndef info(*args):\n    log(*args, level=INFO)\n\n\ndef warn(*args):\n    log(*args, level=WARN)\n\n\ndef error(*args):\n    log(*args, level=ERROR)\n\n\ndef set_level(level):\n    \"\"\"\n    Set logging threshold on current logger.\n    \"\"\"\n    get_current().set_level(level)\n\n\ndef set_comm(comm):\n    get_current().set_comm(comm)\n\n\ndef get_dir():\n    \"\"\"\n    Get directory that log files are being written to.\n    will be None if there is no output directory (i.e., if you didn't call start)\n    \"\"\"\n    return get_current().get_dir()\n\n\nrecord_tabular = logkv\ndump_tabular = dumpkvs\n\n\n@contextmanager\ndef profile_kv(scopename):\n    logkey = \"wait_\" + scopename\n    tstart = time.time()\n    try:\n        yield\n    finally:\n        get_current().name2val[logkey] += time.time() - tstart\n\n\ndef profile(n):\n    \"\"\"\n    Usage:\n    @profile(\"my_func\")\n    def my_func(): code\n    \"\"\"\n\n    def decorator_with_name(func):\n        def func_wrapper(*args, **kwargs):\n            with profile_kv(n):\n                return func(*args, **kwargs)\n\n        return func_wrapper\n\n    return decorator_with_name\n\n\n# ================================================================\n# Backend\n# ================================================================\n\n\ndef get_current():\n    if Logger.CURRENT is None:\n        _configure_default_logger()\n\n    return Logger.CURRENT\n\n\nclass Logger(object):\n    DEFAULT = None  # A logger with no output files. (See right below class definition)\n    # So that you can still log to the terminal without setting up any output files\n    CURRENT = None  # Current logger being used by the free functions above\n\n    def __init__(self, dir, output_formats, comm=None):\n        self.name2val = defaultdict(float)  # values this iteration\n        self.name2cnt = defaultdict(int)\n        self.level = INFO\n        self.dir = dir\n        self.output_formats = output_formats\n        self.comm = comm\n\n    # Logging API, forwarded\n    # ----------------------------------------\n    def logkv(self, key, val):\n        self.name2val[key] = val\n\n    def logkv_mean(self, key, val):\n        oldval, cnt = self.name2val[key], self.name2cnt[key]\n        self.name2val[key] = oldval * cnt / (cnt + 1) + val / (cnt + 1)\n        self.name2cnt[key] = cnt + 1\n\n    def dumpkvs(self):\n        if self.comm is None:\n            d = self.name2val\n        else:\n            d = mpi_weighted_mean(\n                self.comm,\n                {\n                    name: (val, self.name2cnt.get(name, 1))\n                    for (name, val) in self.name2val.items()\n                },\n            )\n            if self.comm.rank != 0:\n                d[\"dummy\"] = 1  # so we don't get a warning about empty dict\n        out = d.copy()  # Return the dict for unit testing purposes\n        for fmt in self.output_formats:\n            if isinstance(fmt, KVWriter):\n                fmt.writekvs(d)\n        self.name2val.clear()\n        self.name2cnt.clear()\n        return out\n\n    def log(self, *args, level=INFO):\n        if self.level <= level:\n            self._do_log(args)\n\n    # Configuration\n    # ----------------------------------------\n    def set_level(self, level):\n        self.level = level\n\n    def set_comm(self, comm):\n        self.comm = comm\n\n    def get_dir(self):\n        return self.dir\n\n    def close(self):\n        for fmt in self.output_formats:\n            fmt.close()\n\n    # Misc\n    # ----------------------------------------\n    def _do_log(self, args):\n        for fmt in self.output_formats:\n            if isinstance(fmt, SeqWriter):\n                fmt.writeseq(map(str, args))\n\n\ndef get_rank_without_mpi_import():\n    # check environment variables here instead of importing mpi4py\n    # to avoid calling MPI_Init() when this module is imported\n    for varname in [\"PMI_RANK\", \"OMPI_COMM_WORLD_RANK\"]:\n        if varname in os.environ:\n            return int(os.environ[varname])\n    return 0\n\n\ndef mpi_weighted_mean(comm, local_name2valcount):\n    \"\"\"\n    Copied from: https://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/common/mpi_util.py#L110\n    Perform a weighted average over dicts that are each on a different node\n    Input: local_name2valcount: dict mapping key -> (value, count)\n    Returns: key -> mean\n    \"\"\"\n    all_name2valcount = comm.gather(local_name2valcount)\n    if comm.rank == 0:\n        name2sum = defaultdict(float)\n        name2count = defaultdict(float)\n        for n2vc in all_name2valcount:\n            for (name, (val, count)) in n2vc.items():\n                try:\n                    val = float(val)\n                except ValueError:\n                    if comm.rank == 0:\n                        warnings.warn(\n                            \"WARNING: tried to compute mean on non-float {}={}\".format(\n                                name, val\n                            )\n                        )\n                else:\n                    name2sum[name] += val * count\n                    name2count[name] += count\n        return {name: name2sum[name] / name2count[name] for name in name2sum}\n    else:\n        return {}\n\n\ndef configure(dir=None, format_strs=None, comm=None, log_suffix=\"\"):\n    \"\"\"\n    If comm is provided, average all numerical stats across that comm\n    \"\"\"\n    if dir is None:\n        dir = os.getenv(\"OPENAI_LOGDIR\")\n    if dir is None:\n        dir = osp.join(\n            tempfile.gettempdir(),\n            datetime.datetime.now().strftime(\"openai-%Y-%m-%d-%H-%M-%S-%f\"),\n        )\n    assert isinstance(dir, str)\n    dir = os.path.expanduser(dir)\n    os.makedirs(os.path.expanduser(dir), exist_ok=True)\n\n    rank = get_rank_without_mpi_import()\n    if rank > 0:\n        log_suffix = log_suffix + \"-rank%03i\" % rank\n\n    if format_strs is None:\n        if rank == 0:\n            format_strs = os.getenv(\"OPENAI_LOG_FORMAT\", \"stdout,log,csv\").split(\",\")\n        else:\n            format_strs = os.getenv(\"OPENAI_LOG_FORMAT_MPI\", \"log\").split(\",\")\n    format_strs = filter(None, format_strs)\n    output_formats = [make_output_format(f, dir, log_suffix) for f in format_strs]\n\n    Logger.CURRENT = Logger(dir=dir, output_formats=output_formats, comm=comm)\n    if output_formats:\n        log(\"Logging to %s\" % dir)\n\n\ndef _configure_default_logger():\n    configure()\n    Logger.DEFAULT = Logger.CURRENT\n\n\ndef reset():\n    if Logger.CURRENT is not Logger.DEFAULT:\n        Logger.CURRENT.close()\n        Logger.CURRENT = Logger.DEFAULT\n        log(\"Reset logger\")\n\n\n@contextmanager\ndef scoped_configure(dir=None, format_strs=None, comm=None):\n    prevlogger = Logger.CURRENT\n    configure(dir=dir, format_strs=format_strs, comm=comm)\n    try:\n        yield\n    finally:\n        Logger.CURRENT.close()\n        Logger.CURRENT = prevlogger\n\n"}
{"type": "source_file", "path": "Image_Purifier/runners/diffpure_guided.py", "content": "# ---------------------------------------------------------------\n# Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.\n#\n# This work is licensed under the NVIDIA Source Code License\n# for DiffPure. To view a copy of this license, see the LICENSE file.\n# ---------------------------------------------------------------\n\nimport os\nimport random\n\nimport torch\nimport torchvision.utils as tvu\n\nfrom guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n\n\nclass GuidedDiffusion(torch.nn.Module):\n    def __init__(self, args, config, device=None, model_dir='/path/to/Image_purifier/pretrained'):\n        super().__init__()\n        self.args = args\n        self.config = config\n        if device is None:\n            device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n        self.device = device\n\n        # load model\n        model_config = model_and_diffusion_defaults()\n        model_config.update(vars(self.config.model))\n        print(f'model_config: {model_config}')\n        model, diffusion = create_model_and_diffusion(**model_config)\n        model.load_state_dict(torch.load(f'{model_dir}/256x256_diffusion_uncond.pt', map_location='cpu'))\n        model.requires_grad_(False).eval().to(self.device)\n\n        if model_config['use_fp16']:\n            model.convert_to_fp16()\n\n        self.model = model\n        self.diffusion = diffusion\n        self.betas = torch.from_numpy(diffusion.betas).float().to(self.device)\n\n    def image_editing_sample(self, img, bs_id=0, tag=None):\n        with torch.no_grad():\n            assert isinstance(img, torch.Tensor)\n            batch_size = img.shape[0]\n\n            if tag is None:\n                tag = 'rnd' + str(random.randint(0, 10000))\n            out_dir = os.path.join(self.args.log_dir, 'bs' + str(bs_id) + '_' + tag)\n\n            assert img.ndim == 4, img.ndim\n            img = img.to(self.device)\n            x0 = img\n\n            if bs_id < 2:\n                os.makedirs(out_dir, exist_ok=True)\n                tvu.save_image((x0 + 1) * 0.5, os.path.join(out_dir, f'original_input.png'))\n\n            xs = []\n            for it in range(self.args.sample_step):\n                e = torch.randn_like(x0)\n                total_noise_levels = self.args.t\n                a = (1 - self.betas).cumprod(dim=0)\n                x = x0 * a[total_noise_levels - 1].sqrt() + e * (1.0 - a[total_noise_levels - 1]).sqrt()\n\n                if bs_id < 2:\n                    tvu.save_image((x + 1) * 0.5, os.path.join(out_dir, f'init_{it}.png'))\n\n                for i in reversed(range(total_noise_levels)):\n                    t = torch.tensor([i] * batch_size, device=self.device)\n\n                    x = self.diffusion.p_sample(self.model, x, t,\n                                                clip_denoised=True,\n                                                denoised_fn=None,\n                                                cond_fn=None,\n                                                model_kwargs=None)[\"sample\"]\n\n                    # added intermediate step vis\n                    if (i - 99) % 100 == 0 and bs_id < 2:\n                        tvu.save_image((x + 1) * 0.5, os.path.join(out_dir, f'noise_t_{i}_{it}.png'))\n\n                x0 = x\n\n                if bs_id < 2:\n                    torch.save(x0, os.path.join(out_dir, f'samples_{it}.pth'))\n                    tvu.save_image((x0 + 1) * 0.5, os.path.join(out_dir, f'samples_{it}.png'))\n\n                xs.append(x0)\n\n            return torch.cat(xs, dim=0)\n"}
{"type": "source_file", "path": "Image_Purifier/score_sde/op/fused_act.py", "content": "# ---------------------------------------------------------------\n# Taken from the following link as is from:\n# https://github.com/yang-song/score_sde_pytorch/blob/main/op/fused_act.py\n#\n# The license for the original version of this file can be\n# found in the `score_sde` directory (LICENSE_SCORE_SDE).\n# ---------------------------------------------------------------\n\nimport os\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.autograd import Function\nfrom torch.utils.cpp_extension import load\n\n\nmodule_path = os.path.dirname(__file__)\nfused = load(\n    \"fused\",\n    sources=[\n        os.path.join(module_path, \"fused_bias_act.cpp\"),\n        os.path.join(module_path, \"fused_bias_act_kernel.cu\"),\n    ],\n)\n\n\nclass FusedLeakyReLUFunctionBackward(Function):\n    @staticmethod\n    def forward(ctx, grad_output, out, negative_slope, scale):\n        ctx.save_for_backward(out)\n        ctx.negative_slope = negative_slope\n        ctx.scale = scale\n\n        empty = grad_output.new_empty(0)\n\n        grad_input = fused.fused_bias_act(\n            grad_output, empty, out, 3, 1, negative_slope, scale\n        )\n\n        dim = [0]\n\n        if grad_input.ndim > 2:\n            dim += list(range(2, grad_input.ndim))\n\n        grad_bias = grad_input.sum(dim).detach()\n\n        return grad_input, grad_bias\n\n    @staticmethod\n    def backward(ctx, gradgrad_input, gradgrad_bias):\n        out, = ctx.saved_tensors\n        gradgrad_out = fused.fused_bias_act(\n            gradgrad_input, gradgrad_bias, out, 3, 1, ctx.negative_slope, ctx.scale\n        )\n\n        return gradgrad_out, None, None, None\n\n\nclass FusedLeakyReLUFunction(Function):\n    @staticmethod\n    def forward(ctx, input, bias, negative_slope, scale):\n        empty = input.new_empty(0)\n        out = fused.fused_bias_act(input, bias, empty, 3, 0, negative_slope, scale)\n        ctx.save_for_backward(out)\n        ctx.negative_slope = negative_slope\n        ctx.scale = scale\n\n        return out\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        out, = ctx.saved_tensors\n\n        grad_input, grad_bias = FusedLeakyReLUFunctionBackward.apply(\n            grad_output, out, ctx.negative_slope, ctx.scale\n        )\n\n        return grad_input, grad_bias, None, None\n\n\nclass FusedLeakyReLU(nn.Module):\n    def __init__(self, channel, negative_slope=0.2, scale=2 ** 0.5):\n        super().__init__()\n\n        self.bias = nn.Parameter(torch.zeros(channel))\n        self.negative_slope = negative_slope\n        self.scale = scale\n\n    def forward(self, input):\n        return fused_leaky_relu(input, self.bias, self.negative_slope, self.scale)\n\n\ndef fused_leaky_relu(input, bias, negative_slope=0.2, scale=2 ** 0.5):\n    if input.device.type == \"cpu\":\n        rest_dim = [1] * (input.ndim - bias.ndim - 1)\n        return (\n            F.leaky_relu(\n                input + bias.view(1, bias.shape[0], *rest_dim), negative_slope=0.2\n            )\n            * scale\n        )\n\n    else:\n        return FusedLeakyReLUFunction.apply(input, bias, negative_slope, scale)\n"}
{"type": "source_file", "path": "Image_Purifier/score_sde/op/__init__.py", "content": "# ---------------------------------------------------------------\n# Taken from the following link as is from:\n# https://github.com/yang-song/score_sde_pytorch/blob/main/op/__init__.py\n#\n# The license for the original version of this file can be\n# found in the `score_sde` directory (LICENSE_SCORE_SDE).\n# ---------------------------------------------------------------\n\nfrom .fused_act import FusedLeakyReLU, fused_leaky_relu\nfrom .upfirdn2d import upfirdn2d\n"}
{"type": "source_file", "path": "Image_Purifier/score_sde/models/ncsnv2.py", "content": "# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# pylint: skip-file\n\"\"\"The NCSNv2 model.\"\"\"\nimport torch\nimport torch.nn as nn\nimport functools\n\nfrom .utils import get_sigmas, register_model\nfrom .layers import (CondRefineBlock, RefineBlock, ResidualBlock, ncsn_conv3x3,\n                     ConditionalResidualBlock, get_act)\nfrom .normalization import get_normalization\n\nCondResidualBlock = ConditionalResidualBlock\nconv3x3 = ncsn_conv3x3\n\n\ndef get_network(config):\n  if config.data.image_size < 96:\n    return functools.partial(NCSNv2, config=config)\n  elif 96 <= config.data.image_size <= 128:\n    return functools.partial(NCSNv2_128, config=config)\n  elif 128 < config.data.image_size <= 256:\n    return functools.partial(NCSNv2_256, config=config)\n  else:\n    raise NotImplementedError(\n      f'No network suitable for {config.data.image_size}px implemented yet.')\n\n\n@register_model(name='ncsnv2_64')\nclass NCSNv2(nn.Module):\n  def __init__(self, config):\n    super().__init__()\n    self.centered = config.data.centered\n    self.norm = get_normalization(config)\n    self.nf = nf = config.model.nf\n\n    self.act = act = get_act(config)\n    self.register_buffer('sigmas', torch.tensor(get_sigmas(config)))\n    self.config = config\n\n    self.begin_conv = nn.Conv2d(config.data.channels, nf, 3, stride=1, padding=1)\n\n    self.normalizer = self.norm(nf, config.model.num_scales)\n    self.end_conv = nn.Conv2d(nf, config.data.channels, 3, stride=1, padding=1)\n\n    self.res1 = nn.ModuleList([\n      ResidualBlock(self.nf, self.nf, resample=None, act=act,\n                    normalization=self.norm),\n      ResidualBlock(self.nf, self.nf, resample=None, act=act,\n                    normalization=self.norm)]\n    )\n\n    self.res2 = nn.ModuleList([\n      ResidualBlock(self.nf, 2 * self.nf, resample='down', act=act,\n                    normalization=self.norm),\n      ResidualBlock(2 * self.nf, 2 * self.nf, resample=None, act=act,\n                    normalization=self.norm)]\n    )\n\n    self.res3 = nn.ModuleList([\n      ResidualBlock(2 * self.nf, 2 * self.nf, resample='down', act=act,\n                    normalization=self.norm, dilation=2),\n      ResidualBlock(2 * self.nf, 2 * self.nf, resample=None, act=act,\n                    normalization=self.norm, dilation=2)]\n    )\n\n    if config.data.image_size == 28:\n      self.res4 = nn.ModuleList([\n        ResidualBlock(2 * self.nf, 2 * self.nf, resample='down', act=act,\n                      normalization=self.norm, adjust_padding=True, dilation=4),\n        ResidualBlock(2 * self.nf, 2 * self.nf, resample=None, act=act,\n                      normalization=self.norm, dilation=4)]\n      )\n    else:\n      self.res4 = nn.ModuleList([\n        ResidualBlock(2 * self.nf, 2 * self.nf, resample='down', act=act,\n                      normalization=self.norm, adjust_padding=False, dilation=4),\n        ResidualBlock(2 * self.nf, 2 * self.nf, resample=None, act=act,\n                      normalization=self.norm, dilation=4)]\n      )\n\n    self.refine1 = RefineBlock([2 * self.nf], 2 * self.nf, act=act, start=True)\n    self.refine2 = RefineBlock([2 * self.nf, 2 * self.nf], 2 * self.nf, act=act)\n    self.refine3 = RefineBlock([2 * self.nf, 2 * self.nf], self.nf, act=act)\n    self.refine4 = RefineBlock([self.nf, self.nf], self.nf, act=act, end=True)\n\n  def _compute_cond_module(self, module, x):\n    for m in module:\n      x = m(x)\n    return x\n\n  def forward(self, x, y):\n    if not self.centered:\n      h = 2 * x - 1.\n    else:\n      h = x\n\n    output = self.begin_conv(h)\n\n    layer1 = self._compute_cond_module(self.res1, output)\n    layer2 = self._compute_cond_module(self.res2, layer1)\n    layer3 = self._compute_cond_module(self.res3, layer2)\n    layer4 = self._compute_cond_module(self.res4, layer3)\n\n    ref1 = self.refine1([layer4], layer4.shape[2:])\n    ref2 = self.refine2([layer3, ref1], layer3.shape[2:])\n    ref3 = self.refine3([layer2, ref2], layer2.shape[2:])\n    output = self.refine4([layer1, ref3], layer1.shape[2:])\n\n    output = self.normalizer(output)\n    output = self.act(output)\n    output = self.end_conv(output)\n\n    used_sigmas = self.sigmas[y].view(x.shape[0], *([1] * len(x.shape[1:])))\n\n    output = output / used_sigmas\n\n    return output\n\n\n@register_model(name='ncsn')\nclass NCSN(nn.Module):\n  def __init__(self, config):\n    super().__init__()\n    self.centered = config.data.centered\n    self.norm = get_normalization(config)\n    self.nf = nf = config.model.nf\n    self.act = act = get_act(config)\n    self.config = config\n\n    self.begin_conv = nn.Conv2d(config.data.channels, nf, 3, stride=1, padding=1)\n\n    self.normalizer = self.norm(nf, config.model.num_scales)\n    self.end_conv = nn.Conv2d(nf, config.data.channels, 3, stride=1, padding=1)\n\n    self.res1 = nn.ModuleList([\n      ConditionalResidualBlock(self.nf, self.nf, config.model.num_scales, resample=None, act=act,\n                               normalization=self.norm),\n      ConditionalResidualBlock(self.nf, self.nf, config.model.num_scales, resample=None, act=act,\n                               normalization=self.norm)]\n    )\n\n    self.res2 = nn.ModuleList([\n      ConditionalResidualBlock(self.nf, 2 * self.nf, config.model.num_scales, resample='down', act=act,\n                               normalization=self.norm),\n      ConditionalResidualBlock(2 * self.nf, 2 * self.nf, config.model.num_scales, resample=None, act=act,\n                               normalization=self.norm)]\n    )\n\n    self.res3 = nn.ModuleList([\n      ConditionalResidualBlock(2 * self.nf, 2 * self.nf, config.model.num_scales, resample='down', act=act,\n                               normalization=self.norm, dilation=2),\n      ConditionalResidualBlock(2 * self.nf, 2 * self.nf, config.model.num_scales, resample=None, act=act,\n                               normalization=self.norm, dilation=2)]\n    )\n\n    if config.data.image_size == 28:\n      self.res4 = nn.ModuleList([\n        ConditionalResidualBlock(2 * self.nf, 2 * self.nf, config.model.num_scales, resample='down', act=act,\n                                 normalization=self.norm, adjust_padding=True, dilation=4),\n        ConditionalResidualBlock(2 * self.nf, 2 * self.nf, config.model.num_scales, resample=None, act=act,\n                                 normalization=self.norm, dilation=4)]\n      )\n    else:\n      self.res4 = nn.ModuleList([\n        ConditionalResidualBlock(2 * self.nf, 2 * self.nf, config.model.num_scales, resample='down', act=act,\n                                 normalization=self.norm, adjust_padding=False, dilation=4),\n        ConditionalResidualBlock(2 * self.nf, 2 * self.nf, config.model.num_scales, resample=None, act=act,\n                                 normalization=self.norm, dilation=4)]\n      )\n\n    self.refine1 = CondRefineBlock([2 * self.nf], 2 * self.nf, config.model.num_scales, self.norm, act=act, start=True)\n    self.refine2 = CondRefineBlock([2 * self.nf, 2 * self.nf], 2 * self.nf, config.model.num_scales, self.norm, act=act)\n    self.refine3 = CondRefineBlock([2 * self.nf, 2 * self.nf], self.nf, config.model.num_scales, self.norm, act=act)\n    self.refine4 = CondRefineBlock([self.nf, self.nf], self.nf, config.model.num_scales, self.norm, act=act, end=True)\n\n  def _compute_cond_module(self, module, x, y):\n    for m in module:\n      x = m(x, y)\n    return x\n\n  def forward(self, x, y):\n    if not self.centered:\n      h = 2 * x - 1.\n    else:\n      h = x\n\n    output = self.begin_conv(h)\n\n    layer1 = self._compute_cond_module(self.res1, output, y)\n    layer2 = self._compute_cond_module(self.res2, layer1, y)\n    layer3 = self._compute_cond_module(self.res3, layer2, y)\n    layer4 = self._compute_cond_module(self.res4, layer3, y)\n\n    ref1 = self.refine1([layer4], y, layer4.shape[2:])\n    ref2 = self.refine2([layer3, ref1], y, layer3.shape[2:])\n    ref3 = self.refine3([layer2, ref2], y, layer2.shape[2:])\n    output = self.refine4([layer1, ref3], y, layer1.shape[2:])\n\n    output = self.normalizer(output, y)\n    output = self.act(output)\n    output = self.end_conv(output)\n\n    return output\n\n\n@register_model(name='ncsnv2_128')\nclass NCSNv2_128(nn.Module):\n  \"\"\"NCSNv2 model architecture for 128px images.\"\"\"\n  def __init__(self, config):\n    super().__init__()\n    self.centered = config.data.centered\n    self.norm = get_normalization(config)\n    self.nf = nf = config.model.nf\n    self.act = act = get_act(config)\n    self.register_buffer('sigmas', torch.tensor(get_sigmas(config)))\n    self.config = config\n\n    self.begin_conv = nn.Conv2d(config.data.channels, nf, 3, stride=1, padding=1)\n    self.normalizer = self.norm(nf, config.model.num_scales)\n\n    self.end_conv = nn.Conv2d(nf, config.data.channels, 3, stride=1, padding=1)\n\n    self.res1 = nn.ModuleList([\n      ResidualBlock(self.nf, self.nf, resample=None, act=act,\n                    normalization=self.norm),\n      ResidualBlock(self.nf, self.nf, resample=None, act=act,\n                    normalization=self.norm)]\n    )\n\n    self.res2 = nn.ModuleList([\n      ResidualBlock(self.nf, 2 * self.nf, resample='down', act=act,\n                    normalization=self.norm),\n      ResidualBlock(2 * self.nf, 2 * self.nf, resample=None, act=act,\n                    normalization=self.norm)]\n    )\n\n    self.res3 = nn.ModuleList([\n      ResidualBlock(2 * self.nf, 2 * self.nf, resample='down', act=act,\n                    normalization=self.norm),\n      ResidualBlock(2 * self.nf, 2 * self.nf, resample=None, act=act,\n                    normalization=self.norm)]\n    )\n\n    self.res4 = nn.ModuleList([\n      ResidualBlock(2 * self.nf, 4 * self.nf, resample='down', act=act,\n                    normalization=self.norm, dilation=2),\n      ResidualBlock(4 * self.nf, 4 * self.nf, resample=None, act=act,\n                    normalization=self.norm, dilation=2)]\n    )\n\n    self.res5 = nn.ModuleList([\n      ResidualBlock(4 * self.nf, 4 * self.nf, resample='down', act=act,\n                    normalization=self.norm, dilation=4),\n      ResidualBlock(4 * self.nf, 4 * self.nf, resample=None, act=act,\n                    normalization=self.norm, dilation=4)]\n    )\n\n    self.refine1 = RefineBlock([4 * self.nf], 4 * self.nf, act=act, start=True)\n    self.refine2 = RefineBlock([4 * self.nf, 4 * self.nf], 2 * self.nf, act=act)\n    self.refine3 = RefineBlock([2 * self.nf, 2 * self.nf], 2 * self.nf, act=act)\n    self.refine4 = RefineBlock([2 * self.nf, 2 * self.nf], self.nf, act=act)\n    self.refine5 = RefineBlock([self.nf, self.nf], self.nf, act=act, end=True)\n\n  def _compute_cond_module(self, module, x):\n    for m in module:\n      x = m(x)\n    return x\n\n  def forward(self, x, y):\n    if not self.centered:\n      h = 2 * x - 1.\n    else:\n      h = x\n\n    output = self.begin_conv(h)\n\n    layer1 = self._compute_cond_module(self.res1, output)\n    layer2 = self._compute_cond_module(self.res2, layer1)\n    layer3 = self._compute_cond_module(self.res3, layer2)\n    layer4 = self._compute_cond_module(self.res4, layer3)\n    layer5 = self._compute_cond_module(self.res5, layer4)\n\n    ref1 = self.refine1([layer5], layer5.shape[2:])\n    ref2 = self.refine2([layer4, ref1], layer4.shape[2:])\n    ref3 = self.refine3([layer3, ref2], layer3.shape[2:])\n    ref4 = self.refine4([layer2, ref3], layer2.shape[2:])\n    output = self.refine5([layer1, ref4], layer1.shape[2:])\n\n    output = self.normalizer(output)\n    output = self.act(output)\n    output = self.end_conv(output)\n\n    used_sigmas = self.sigmas[y].view(x.shape[0], *([1] * len(x.shape[1:])))\n\n    output = output / used_sigmas\n\n    return output\n\n\n@register_model(name='ncsnv2_256')\nclass NCSNv2_256(nn.Module):\n  \"\"\"NCSNv2 model architecture for 256px images.\"\"\"\n  def __init__(self, config):\n    super().__init__()\n    self.centered = config.data.centered\n    self.norm = get_normalization(config)\n    self.nf = nf = config.model.nf\n    self.act = act = get_act(config)\n    self.register_buffer('sigmas', torch.tensor(get_sigmas(config)))\n    self.config = config\n\n    self.begin_conv = nn.Conv2d(config.data.channels, nf, 3, stride=1, padding=1)\n    self.normalizer = self.norm(nf, config.model.num_scales)\n\n    self.end_conv = nn.Conv2d(nf, config.data.channels, 3, stride=1, padding=1)\n\n    self.res1 = nn.ModuleList([\n      ResidualBlock(self.nf, self.nf, resample=None, act=act,\n                    normalization=self.norm),\n      ResidualBlock(self.nf, self.nf, resample=None, act=act,\n                    normalization=self.norm)]\n    )\n\n    self.res2 = nn.ModuleList([\n      ResidualBlock(self.nf, 2 * self.nf, resample='down', act=act,\n                    normalization=self.norm),\n      ResidualBlock(2 * self.nf, 2 * self.nf, resample=None, act=act,\n                    normalization=self.norm)]\n    )\n\n    self.res3 = nn.ModuleList([\n      ResidualBlock(2 * self.nf, 2 * self.nf, resample='down', act=act,\n                    normalization=self.norm),\n      ResidualBlock(2 * self.nf, 2 * self.nf, resample=None, act=act,\n                    normalization=self.norm)]\n    )\n\n    self.res31 = nn.ModuleList([\n      ResidualBlock(2 * self.nf, 2 * self.nf, resample='down', act=act,\n                    normalization=self.norm),\n      ResidualBlock(2 * self.nf, 2 * self.nf, resample=None, act=act,\n                    normalization=self.norm)]\n    )\n\n    self.res4 = nn.ModuleList([\n      ResidualBlock(2 * self.nf, 4 * self.nf, resample='down', act=act,\n                    normalization=self.norm, dilation=2),\n      ResidualBlock(4 * self.nf, 4 * self.nf, resample=None, act=act,\n                    normalization=self.norm, dilation=2)]\n    )\n\n    self.res5 = nn.ModuleList([\n      ResidualBlock(4 * self.nf, 4 * self.nf, resample='down', act=act,\n                    normalization=self.norm, dilation=4),\n      ResidualBlock(4 * self.nf, 4 * self.nf, resample=None, act=act,\n                    normalization=self.norm, dilation=4)]\n    )\n\n    self.refine1 = RefineBlock([4 * self.nf], 4 * self.nf, act=act, start=True)\n    self.refine2 = RefineBlock([4 * self.nf, 4 * self.nf], 2 * self.nf, act=act)\n    self.refine3 = RefineBlock([2 * self.nf, 2 * self.nf], 2 * self.nf, act=act)\n    self.refine31 = RefineBlock([2 * self.nf, 2 * self.nf], 2 * self.nf, act=act)\n    self.refine4 = RefineBlock([2 * self.nf, 2 * self.nf], self.nf, act=act)\n    self.refine5 = RefineBlock([self.nf, self.nf], self.nf, act=act, end=True)\n\n  def _compute_cond_module(self, module, x):\n    for m in module:\n      x = m(x)\n    return x\n\n  def forward(self, x, y):\n    if not self.centered:\n      h = 2 * x - 1.\n    else:\n      h = x\n\n    output = self.begin_conv(h)\n\n    layer1 = self._compute_cond_module(self.res1, output)\n    layer2 = self._compute_cond_module(self.res2, layer1)\n    layer3 = self._compute_cond_module(self.res3, layer2)\n    layer31 = self._compute_cond_module(self.res31, layer3)\n    layer4 = self._compute_cond_module(self.res4, layer31)\n    layer5 = self._compute_cond_module(self.res5, layer4)\n\n    ref1 = self.refine1([layer5], layer5.shape[2:])\n    ref2 = self.refine2([layer4, ref1], layer4.shape[2:])\n    ref31 = self.refine31([layer31, ref2], layer31.shape[2:])\n    ref3 = self.refine3([layer3, ref31], layer3.shape[2:])\n    ref4 = self.refine4([layer2, ref3], layer2.shape[2:])\n    output = self.refine5([layer1, ref4], layer1.shape[2:])\n\n    output = self.normalizer(output)\n    output = self.act(output)\n    output = self.end_conv(output)\n\n    used_sigmas = self.sigmas[y].view(x.shape[0], *([1] * len(x.shape[1:])))\n\n    output = output / used_sigmas\n\n    return output"}
{"type": "source_file", "path": "Image_Purifier/score_sde/models/__init__.py", "content": "# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom . import  ncsnpp\n"}
{"type": "source_file", "path": "Image_Purifier/score_sde/models/layers.py", "content": "# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# pylint: skip-file\n\"\"\"Common layers for defining score networks.\n\"\"\"\nimport math\nimport string\nfrom functools import partial\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom .normalization import ConditionalInstanceNorm2dPlus\n\n\ndef get_act(config):\n  \"\"\"Get activation functions from the config file.\"\"\"\n\n  if config.model.nonlinearity.lower() == 'elu':\n    return nn.ELU()\n  elif config.model.nonlinearity.lower() == 'relu':\n    return nn.ReLU()\n  elif config.model.nonlinearity.lower() == 'lrelu':\n    return nn.LeakyReLU(negative_slope=0.2)\n  elif config.model.nonlinearity.lower() == 'swish':\n    return nn.SiLU()\n  else:\n    raise NotImplementedError('activation function does not exist!')\n\n\ndef ncsn_conv1x1(in_planes, out_planes, stride=1, bias=True, dilation=1, init_scale=1., padding=0):\n  \"\"\"1x1 convolution. Same as NCSNv1/v2.\"\"\"\n  conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=bias, dilation=dilation,\n                   padding=padding)\n  init_scale = 1e-10 if init_scale == 0 else init_scale\n  conv.weight.data *= init_scale\n  conv.bias.data *= init_scale\n  return conv\n\n\ndef variance_scaling(scale, mode, distribution,\n                     in_axis=1, out_axis=0,\n                     dtype=torch.float32,\n                     device='cpu'):\n  \"\"\"Ported from JAX. \"\"\"\n\n  def _compute_fans(shape, in_axis=1, out_axis=0):\n    receptive_field_size = np.prod(shape) / shape[in_axis] / shape[out_axis]\n    fan_in = shape[in_axis] * receptive_field_size\n    fan_out = shape[out_axis] * receptive_field_size\n    return fan_in, fan_out\n\n  def init(shape, dtype=dtype, device=device):\n    fan_in, fan_out = _compute_fans(shape, in_axis, out_axis)\n    if mode == \"fan_in\":\n      denominator = fan_in\n    elif mode == \"fan_out\":\n      denominator = fan_out\n    elif mode == \"fan_avg\":\n      denominator = (fan_in + fan_out) / 2\n    else:\n      raise ValueError(\n        \"invalid mode for variance scaling initializer: {}\".format(mode))\n    variance = scale / denominator\n    if distribution == \"normal\":\n      return torch.randn(*shape, dtype=dtype, device=device) * np.sqrt(variance)\n    elif distribution == \"uniform\":\n      return (torch.rand(*shape, dtype=dtype, device=device) * 2. - 1.) * np.sqrt(3 * variance)\n    else:\n      raise ValueError(\"invalid distribution for variance scaling initializer\")\n\n  return init\n\n\ndef default_init(scale=1.):\n  \"\"\"The same initialization used in DDPM.\"\"\"\n  scale = 1e-10 if scale == 0 else scale\n  return variance_scaling(scale, 'fan_avg', 'uniform')\n\n\nclass Dense(nn.Module):\n  \"\"\"Linear layer with `default_init`.\"\"\"\n  def __init__(self):\n    super().__init__()\n\n\ndef ddpm_conv1x1(in_planes, out_planes, stride=1, bias=True, init_scale=1., padding=0):\n  \"\"\"1x1 convolution with DDPM initialization.\"\"\"\n  conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, padding=padding, bias=bias)\n  conv.weight.data = default_init(init_scale)(conv.weight.data.shape)\n  nn.init.zeros_(conv.bias)\n  return conv\n\n\ndef ncsn_conv3x3(in_planes, out_planes, stride=1, bias=True, dilation=1, init_scale=1., padding=1):\n  \"\"\"3x3 convolution with PyTorch initialization. Same as NCSNv1/NCSNv2.\"\"\"\n  init_scale = 1e-10 if init_scale == 0 else init_scale\n  conv = nn.Conv2d(in_planes, out_planes, stride=stride, bias=bias,\n                   dilation=dilation, padding=padding, kernel_size=3)\n  conv.weight.data *= init_scale\n  conv.bias.data *= init_scale\n  return conv\n\n\ndef ddpm_conv3x3(in_planes, out_planes, stride=1, bias=True, dilation=1, init_scale=1., padding=1):\n  \"\"\"3x3 convolution with DDPM initialization.\"\"\"\n  conv = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=padding,\n                   dilation=dilation, bias=bias)\n  conv.weight.data = default_init(init_scale)(conv.weight.data.shape)\n  nn.init.zeros_(conv.bias)\n  return conv\n\n  ###########################################################################\n  # Functions below are ported over from the NCSNv1/NCSNv2 codebase:\n  # https://github.com/ermongroup/ncsn\n  # https://github.com/ermongroup/ncsnv2\n  ###########################################################################\n\n\nclass CRPBlock(nn.Module):\n  def __init__(self, features, n_stages, act=nn.ReLU(), maxpool=True):\n    super().__init__()\n    self.convs = nn.ModuleList()\n    for i in range(n_stages):\n      self.convs.append(ncsn_conv3x3(features, features, stride=1, bias=False))\n    self.n_stages = n_stages\n    if maxpool:\n      self.pool = nn.MaxPool2d(kernel_size=5, stride=1, padding=2)\n    else:\n      self.pool = nn.AvgPool2d(kernel_size=5, stride=1, padding=2)\n\n    self.act = act\n\n  def forward(self, x):\n    x = self.act(x)\n    path = x\n    for i in range(self.n_stages):\n      path = self.pool(path)\n      path = self.convs[i](path)\n      x = path + x\n    return x\n\n\nclass CondCRPBlock(nn.Module):\n  def __init__(self, features, n_stages, num_classes, normalizer, act=nn.ReLU()):\n    super().__init__()\n    self.convs = nn.ModuleList()\n    self.norms = nn.ModuleList()\n    self.normalizer = normalizer\n    for i in range(n_stages):\n      self.norms.append(normalizer(features, num_classes, bias=True))\n      self.convs.append(ncsn_conv3x3(features, features, stride=1, bias=False))\n\n    self.n_stages = n_stages\n    self.pool = nn.AvgPool2d(kernel_size=5, stride=1, padding=2)\n    self.act = act\n\n  def forward(self, x, y):\n    x = self.act(x)\n    path = x\n    for i in range(self.n_stages):\n      path = self.norms[i](path, y)\n      path = self.pool(path)\n      path = self.convs[i](path)\n\n      x = path + x\n    return x\n\n\nclass RCUBlock(nn.Module):\n  def __init__(self, features, n_blocks, n_stages, act=nn.ReLU()):\n    super().__init__()\n\n    for i in range(n_blocks):\n      for j in range(n_stages):\n        setattr(self, '{}_{}_conv'.format(i + 1, j + 1), ncsn_conv3x3(features, features, stride=1, bias=False))\n\n    self.stride = 1\n    self.n_blocks = n_blocks\n    self.n_stages = n_stages\n    self.act = act\n\n  def forward(self, x):\n    for i in range(self.n_blocks):\n      residual = x\n      for j in range(self.n_stages):\n        x = self.act(x)\n        x = getattr(self, '{}_{}_conv'.format(i + 1, j + 1))(x)\n\n      x += residual\n    return x\n\n\nclass CondRCUBlock(nn.Module):\n  def __init__(self, features, n_blocks, n_stages, num_classes, normalizer, act=nn.ReLU()):\n    super().__init__()\n\n    for i in range(n_blocks):\n      for j in range(n_stages):\n        setattr(self, '{}_{}_norm'.format(i + 1, j + 1), normalizer(features, num_classes, bias=True))\n        setattr(self, '{}_{}_conv'.format(i + 1, j + 1), ncsn_conv3x3(features, features, stride=1, bias=False))\n\n    self.stride = 1\n    self.n_blocks = n_blocks\n    self.n_stages = n_stages\n    self.act = act\n    self.normalizer = normalizer\n\n  def forward(self, x, y):\n    for i in range(self.n_blocks):\n      residual = x\n      for j in range(self.n_stages):\n        x = getattr(self, '{}_{}_norm'.format(i + 1, j + 1))(x, y)\n        x = self.act(x)\n        x = getattr(self, '{}_{}_conv'.format(i + 1, j + 1))(x)\n\n      x += residual\n    return x\n\n\nclass MSFBlock(nn.Module):\n  def __init__(self, in_planes, features):\n    super().__init__()\n    assert isinstance(in_planes, list) or isinstance(in_planes, tuple)\n    self.convs = nn.ModuleList()\n    self.features = features\n\n    for i in range(len(in_planes)):\n      self.convs.append(ncsn_conv3x3(in_planes[i], features, stride=1, bias=True))\n\n  def forward(self, xs, shape):\n    sums = torch.zeros(xs[0].shape[0], self.features, *shape, device=xs[0].device)\n    for i in range(len(self.convs)):\n      h = self.convs[i](xs[i])\n      h = F.interpolate(h, size=shape, mode='bilinear', align_corners=True)\n      sums += h\n    return sums\n\n\nclass CondMSFBlock(nn.Module):\n  def __init__(self, in_planes, features, num_classes, normalizer):\n    super().__init__()\n    assert isinstance(in_planes, list) or isinstance(in_planes, tuple)\n\n    self.convs = nn.ModuleList()\n    self.norms = nn.ModuleList()\n    self.features = features\n    self.normalizer = normalizer\n\n    for i in range(len(in_planes)):\n      self.convs.append(ncsn_conv3x3(in_planes[i], features, stride=1, bias=True))\n      self.norms.append(normalizer(in_planes[i], num_classes, bias=True))\n\n  def forward(self, xs, y, shape):\n    sums = torch.zeros(xs[0].shape[0], self.features, *shape, device=xs[0].device)\n    for i in range(len(self.convs)):\n      h = self.norms[i](xs[i], y)\n      h = self.convs[i](h)\n      h = F.interpolate(h, size=shape, mode='bilinear', align_corners=True)\n      sums += h\n    return sums\n\n\nclass RefineBlock(nn.Module):\n  def __init__(self, in_planes, features, act=nn.ReLU(), start=False, end=False, maxpool=True):\n    super().__init__()\n\n    assert isinstance(in_planes, tuple) or isinstance(in_planes, list)\n    self.n_blocks = n_blocks = len(in_planes)\n\n    self.adapt_convs = nn.ModuleList()\n    for i in range(n_blocks):\n      self.adapt_convs.append(RCUBlock(in_planes[i], 2, 2, act))\n\n    self.output_convs = RCUBlock(features, 3 if end else 1, 2, act)\n\n    if not start:\n      self.msf = MSFBlock(in_planes, features)\n\n    self.crp = CRPBlock(features, 2, act, maxpool=maxpool)\n\n  def forward(self, xs, output_shape):\n    assert isinstance(xs, tuple) or isinstance(xs, list)\n    hs = []\n    for i in range(len(xs)):\n      h = self.adapt_convs[i](xs[i])\n      hs.append(h)\n\n    if self.n_blocks > 1:\n      h = self.msf(hs, output_shape)\n    else:\n      h = hs[0]\n\n    h = self.crp(h)\n    h = self.output_convs(h)\n\n    return h\n\n\nclass CondRefineBlock(nn.Module):\n  def __init__(self, in_planes, features, num_classes, normalizer, act=nn.ReLU(), start=False, end=False):\n    super().__init__()\n\n    assert isinstance(in_planes, tuple) or isinstance(in_planes, list)\n    self.n_blocks = n_blocks = len(in_planes)\n\n    self.adapt_convs = nn.ModuleList()\n    for i in range(n_blocks):\n      self.adapt_convs.append(\n        CondRCUBlock(in_planes[i], 2, 2, num_classes, normalizer, act)\n      )\n\n    self.output_convs = CondRCUBlock(features, 3 if end else 1, 2, num_classes, normalizer, act)\n\n    if not start:\n      self.msf = CondMSFBlock(in_planes, features, num_classes, normalizer)\n\n    self.crp = CondCRPBlock(features, 2, num_classes, normalizer, act)\n\n  def forward(self, xs, y, output_shape):\n    assert isinstance(xs, tuple) or isinstance(xs, list)\n    hs = []\n    for i in range(len(xs)):\n      h = self.adapt_convs[i](xs[i], y)\n      hs.append(h)\n\n    if self.n_blocks > 1:\n      h = self.msf(hs, y, output_shape)\n    else:\n      h = hs[0]\n\n    h = self.crp(h, y)\n    h = self.output_convs(h, y)\n\n    return h\n\n\nclass ConvMeanPool(nn.Module):\n  def __init__(self, input_dim, output_dim, kernel_size=3, biases=True, adjust_padding=False):\n    super().__init__()\n    if not adjust_padding:\n      conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride=1, padding=kernel_size // 2, bias=biases)\n      self.conv = conv\n    else:\n      conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride=1, padding=kernel_size // 2, bias=biases)\n\n      self.conv = nn.Sequential(\n        nn.ZeroPad2d((1, 0, 1, 0)),\n        conv\n      )\n\n  def forward(self, inputs):\n    output = self.conv(inputs)\n    output = sum([output[:, :, ::2, ::2], output[:, :, 1::2, ::2],\n                  output[:, :, ::2, 1::2], output[:, :, 1::2, 1::2]]) / 4.\n    return output\n\n\nclass MeanPoolConv(nn.Module):\n  def __init__(self, input_dim, output_dim, kernel_size=3, biases=True):\n    super().__init__()\n    self.conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride=1, padding=kernel_size // 2, bias=biases)\n\n  def forward(self, inputs):\n    output = inputs\n    output = sum([output[:, :, ::2, ::2], output[:, :, 1::2, ::2],\n                  output[:, :, ::2, 1::2], output[:, :, 1::2, 1::2]]) / 4.\n    return self.conv(output)\n\n\nclass UpsampleConv(nn.Module):\n  def __init__(self, input_dim, output_dim, kernel_size=3, biases=True):\n    super().__init__()\n    self.conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride=1, padding=kernel_size // 2, bias=biases)\n    self.pixelshuffle = nn.PixelShuffle(upscale_factor=2)\n\n  def forward(self, inputs):\n    output = inputs\n    output = torch.cat([output, output, output, output], dim=1)\n    output = self.pixelshuffle(output)\n    return self.conv(output)\n\n\nclass ConditionalResidualBlock(nn.Module):\n  def __init__(self, input_dim, output_dim, num_classes, resample=1, act=nn.ELU(),\n               normalization=ConditionalInstanceNorm2dPlus, adjust_padding=False, dilation=None):\n    super().__init__()\n    self.non_linearity = act\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    self.resample = resample\n    self.normalization = normalization\n    if resample == 'down':\n      if dilation > 1:\n        self.conv1 = ncsn_conv3x3(input_dim, input_dim, dilation=dilation)\n        self.normalize2 = normalization(input_dim, num_classes)\n        self.conv2 = ncsn_conv3x3(input_dim, output_dim, dilation=dilation)\n        conv_shortcut = partial(ncsn_conv3x3, dilation=dilation)\n      else:\n        self.conv1 = ncsn_conv3x3(input_dim, input_dim)\n        self.normalize2 = normalization(input_dim, num_classes)\n        self.conv2 = ConvMeanPool(input_dim, output_dim, 3, adjust_padding=adjust_padding)\n        conv_shortcut = partial(ConvMeanPool, kernel_size=1, adjust_padding=adjust_padding)\n\n    elif resample is None:\n      if dilation > 1:\n        conv_shortcut = partial(ncsn_conv3x3, dilation=dilation)\n        self.conv1 = ncsn_conv3x3(input_dim, output_dim, dilation=dilation)\n        self.normalize2 = normalization(output_dim, num_classes)\n        self.conv2 = ncsn_conv3x3(output_dim, output_dim, dilation=dilation)\n      else:\n        conv_shortcut = nn.Conv2d\n        self.conv1 = ncsn_conv3x3(input_dim, output_dim)\n        self.normalize2 = normalization(output_dim, num_classes)\n        self.conv2 = ncsn_conv3x3(output_dim, output_dim)\n    else:\n      raise Exception('invalid resample value')\n\n    if output_dim != input_dim or resample is not None:\n      self.shortcut = conv_shortcut(input_dim, output_dim)\n\n    self.normalize1 = normalization(input_dim, num_classes)\n\n  def forward(self, x, y):\n    output = self.normalize1(x, y)\n    output = self.non_linearity(output)\n    output = self.conv1(output)\n    output = self.normalize2(output, y)\n    output = self.non_linearity(output)\n    output = self.conv2(output)\n\n    if self.output_dim == self.input_dim and self.resample is None:\n      shortcut = x\n    else:\n      shortcut = self.shortcut(x)\n\n    return shortcut + output\n\n\nclass ResidualBlock(nn.Module):\n  def __init__(self, input_dim, output_dim, resample=None, act=nn.ELU(),\n               normalization=nn.InstanceNorm2d, adjust_padding=False, dilation=1):\n    super().__init__()\n    self.non_linearity = act\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    self.resample = resample\n    self.normalization = normalization\n    if resample == 'down':\n      if dilation > 1:\n        self.conv1 = ncsn_conv3x3(input_dim, input_dim, dilation=dilation)\n        self.normalize2 = normalization(input_dim)\n        self.conv2 = ncsn_conv3x3(input_dim, output_dim, dilation=dilation)\n        conv_shortcut = partial(ncsn_conv3x3, dilation=dilation)\n      else:\n        self.conv1 = ncsn_conv3x3(input_dim, input_dim)\n        self.normalize2 = normalization(input_dim)\n        self.conv2 = ConvMeanPool(input_dim, output_dim, 3, adjust_padding=adjust_padding)\n        conv_shortcut = partial(ConvMeanPool, kernel_size=1, adjust_padding=adjust_padding)\n\n    elif resample is None:\n      if dilation > 1:\n        conv_shortcut = partial(ncsn_conv3x3, dilation=dilation)\n        self.conv1 = ncsn_conv3x3(input_dim, output_dim, dilation=dilation)\n        self.normalize2 = normalization(output_dim)\n        self.conv2 = ncsn_conv3x3(output_dim, output_dim, dilation=dilation)\n      else:\n        # conv_shortcut = nn.Conv2d ### Something wierd here.\n        conv_shortcut = partial(ncsn_conv1x1)\n        self.conv1 = ncsn_conv3x3(input_dim, output_dim)\n        self.normalize2 = normalization(output_dim)\n        self.conv2 = ncsn_conv3x3(output_dim, output_dim)\n    else:\n      raise Exception('invalid resample value')\n\n    if output_dim != input_dim or resample is not None:\n      self.shortcut = conv_shortcut(input_dim, output_dim)\n\n    self.normalize1 = normalization(input_dim)\n\n  def forward(self, x):\n    output = self.normalize1(x)\n    output = self.non_linearity(output)\n    output = self.conv1(output)\n    output = self.normalize2(output)\n    output = self.non_linearity(output)\n    output = self.conv2(output)\n\n    if self.output_dim == self.input_dim and self.resample is None:\n      shortcut = x\n    else:\n      shortcut = self.shortcut(x)\n\n    return shortcut + output\n\n\n###########################################################################\n# Functions below are ported over from the DDPM codebase:\n#  https://github.com/hojonathanho/diffusion/blob/master/diffusion_tf/nn.py\n###########################################################################\n\ndef get_timestep_embedding(timesteps, embedding_dim, max_positions=10000):\n  assert len(timesteps.shape) == 1  # and timesteps.dtype == tf.int32\n  half_dim = embedding_dim // 2\n  # magic number 10000 is from transformers\n  emb = math.log(max_positions) / (half_dim - 1)\n  # emb = math.log(2.) / (half_dim - 1)\n  emb = torch.exp(torch.arange(half_dim, dtype=torch.float32, device=timesteps.device) * -emb)\n  # emb = tf.range(num_embeddings, dtype=jnp.float32)[:, None] * emb[None, :]\n  # emb = tf.cast(timesteps, dtype=jnp.float32)[:, None] * emb[None, :]\n  emb = timesteps.float()[:, None] * emb[None, :]\n  emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n  if embedding_dim % 2 == 1:  # zero pad\n    emb = F.pad(emb, (0, 1), mode='constant')\n  assert emb.shape == (timesteps.shape[0], embedding_dim)\n  return emb\n\n\ndef _einsum(a, b, c, x, y):\n  einsum_str = '{},{}->{}'.format(''.join(a), ''.join(b), ''.join(c))\n  return torch.einsum(einsum_str, x, y)\n\n\ndef contract_inner(x, y):\n  \"\"\"tensordot(x, y, 1).\"\"\"\n  x_chars = list(string.ascii_lowercase[:len(x.shape)])\n  y_chars = list(string.ascii_lowercase[len(x.shape):len(y.shape) + len(x.shape)])\n  y_chars[0] = x_chars[-1]  # first axis of y and last of x get summed\n  out_chars = x_chars[:-1] + y_chars[1:]\n  return _einsum(x_chars, y_chars, out_chars, x, y)\n\n\nclass NIN(nn.Module):\n  def __init__(self, in_dim, num_units, init_scale=0.1):\n    super().__init__()\n    self.W = nn.Parameter(default_init(scale=init_scale)((in_dim, num_units)), requires_grad=True)\n    self.b = nn.Parameter(torch.zeros(num_units), requires_grad=True)\n\n  def forward(self, x):\n    x = x.permute(0, 2, 3, 1)\n    y = contract_inner(x, self.W) + self.b\n    return y.permute(0, 3, 1, 2)\n\n\nclass AttnBlock(nn.Module):\n  \"\"\"Channel-wise self-attention block.\"\"\"\n  def __init__(self, channels):\n    super().__init__()\n    self.GroupNorm_0 = nn.GroupNorm(num_groups=32, num_channels=channels, eps=1e-6)\n    self.NIN_0 = NIN(channels, channels)\n    self.NIN_1 = NIN(channels, channels)\n    self.NIN_2 = NIN(channels, channels)\n    self.NIN_3 = NIN(channels, channels, init_scale=0.)\n\n  def forward(self, x):\n    B, C, H, W = x.shape\n    h = self.GroupNorm_0(x)\n    q = self.NIN_0(h)\n    k = self.NIN_1(h)\n    v = self.NIN_2(h)\n\n    w = torch.einsum('bchw,bcij->bhwij', q, k) * (int(C) ** (-0.5))\n    w = torch.reshape(w, (B, H, W, H * W))\n    w = F.softmax(w, dim=-1)\n    w = torch.reshape(w, (B, H, W, H, W))\n    h = torch.einsum('bhwij,bcij->bchw', w, v)\n    h = self.NIN_3(h)\n    return x + h\n\n\nclass Upsample(nn.Module):\n  def __init__(self, channels, with_conv=False):\n    super().__init__()\n    if with_conv:\n      self.Conv_0 = ddpm_conv3x3(channels, channels)\n    self.with_conv = with_conv\n\n  def forward(self, x):\n    B, C, H, W = x.shape\n    h = F.interpolate(x, (H * 2, W * 2), mode='nearest')\n    if self.with_conv:\n      h = self.Conv_0(h)\n    return h\n\n\nclass Downsample(nn.Module):\n  def __init__(self, channels, with_conv=False):\n    super().__init__()\n    if with_conv:\n      self.Conv_0 = ddpm_conv3x3(channels, channels, stride=2, padding=0)\n    self.with_conv = with_conv\n\n  def forward(self, x):\n    B, C, H, W = x.shape\n    # Emulate 'SAME' padding\n    if self.with_conv:\n      x = F.pad(x, (0, 1, 0, 1))\n      x = self.Conv_0(x)\n    else:\n      x = F.avg_pool2d(x, kernel_size=2, stride=2, padding=0)\n\n    assert x.shape == (B, C, H // 2, W // 2)\n    return x\n\n\nclass ResnetBlockDDPM(nn.Module):\n  \"\"\"The ResNet Blocks used in DDPM.\"\"\"\n  def __init__(self, act, in_ch, out_ch=None, temb_dim=None, conv_shortcut=False, dropout=0.1):\n    super().__init__()\n    if out_ch is None:\n      out_ch = in_ch\n    self.GroupNorm_0 = nn.GroupNorm(num_groups=32, num_channels=in_ch, eps=1e-6)\n    self.act = act\n    self.Conv_0 = ddpm_conv3x3(in_ch, out_ch)\n    if temb_dim is not None:\n      self.Dense_0 = nn.Linear(temb_dim, out_ch)\n      self.Dense_0.weight.data = default_init()(self.Dense_0.weight.data.shape)\n      nn.init.zeros_(self.Dense_0.bias)\n\n    self.GroupNorm_1 = nn.GroupNorm(num_groups=32, num_channels=out_ch, eps=1e-6)\n    self.Dropout_0 = nn.Dropout(dropout)\n    self.Conv_1 = ddpm_conv3x3(out_ch, out_ch, init_scale=0.)\n    if in_ch != out_ch:\n      if conv_shortcut:\n        self.Conv_2 = ddpm_conv3x3(in_ch, out_ch)\n      else:\n        self.NIN_0 = NIN(in_ch, out_ch)\n    self.out_ch = out_ch\n    self.in_ch = in_ch\n    self.conv_shortcut = conv_shortcut\n\n  def forward(self, x, temb=None):\n    B, C, H, W = x.shape\n    assert C == self.in_ch\n    out_ch = self.out_ch if self.out_ch else self.in_ch\n    h = self.act(self.GroupNorm_0(x))\n    h = self.Conv_0(h)\n    # Add bias to each feature map conditioned on the time embedding\n    if temb is not None:\n      h += self.Dense_0(self.act(temb))[:, :, None, None]\n    h = self.act(self.GroupNorm_1(h))\n    h = self.Dropout_0(h)\n    h = self.Conv_1(h)\n    if C != out_ch:\n      if self.conv_shortcut:\n        x = self.Conv_2(x)\n      else:\n        x = self.NIN_0(x)\n    return x + h"}
{"type": "source_file", "path": "LLaVA/llava/__init__.py", "content": "from .model import LlavaLlamaForCausalLM\n"}
{"type": "source_file", "path": "Image_Purifier/score_sde/models/ddpm.py", "content": "# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# pylint: skip-file\n\"\"\"DDPM model.\n\nThis code is the pytorch equivalent of:\nhttps://github.com/hojonathanho/diffusion/blob/master/diffusion_tf/models/unet.py\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport functools\n\nfrom . import utils, layers, normalization\n\nRefineBlock = layers.RefineBlock\nResidualBlock = layers.ResidualBlock\nResnetBlockDDPM = layers.ResnetBlockDDPM\nUpsample = layers.Upsample\nDownsample = layers.Downsample\nconv3x3 = layers.ddpm_conv3x3\nget_act = layers.get_act\nget_normalization = normalization.get_normalization\ndefault_initializer = layers.default_init\n\n\n@utils.register_model(name='ddpm')\nclass DDPM(nn.Module):\n  def __init__(self, config):\n    super().__init__()\n    self.act = act = get_act(config)\n    self.register_buffer('sigmas', torch.tensor(utils.get_sigmas(config)))\n\n    self.nf = nf = config.model.nf\n    ch_mult = config.model.ch_mult\n    self.num_res_blocks = num_res_blocks = config.model.num_res_blocks\n    self.attn_resolutions = attn_resolutions = config.model.attn_resolutions\n    dropout = config.model.dropout\n    resamp_with_conv = config.model.resamp_with_conv\n    self.num_resolutions = num_resolutions = len(ch_mult)\n    self.all_resolutions = all_resolutions = [config.data.image_size // (2 ** i) for i in range(num_resolutions)]\n\n    AttnBlock = functools.partial(layers.AttnBlock)\n    self.conditional = conditional = config.model.conditional\n    ResnetBlock = functools.partial(ResnetBlockDDPM, act=act, temb_dim=4 * nf, dropout=dropout)\n    if conditional:\n      # Condition on noise levels.\n      modules = [nn.Linear(nf, nf * 4)]\n      modules[0].weight.data = default_initializer()(modules[0].weight.data.shape)\n      nn.init.zeros_(modules[0].bias)\n      modules.append(nn.Linear(nf * 4, nf * 4))\n      modules[1].weight.data = default_initializer()(modules[1].weight.data.shape)\n      nn.init.zeros_(modules[1].bias)\n\n    self.centered = config.data.centered\n    channels = config.data.num_channels\n\n    # Downsampling block\n    modules.append(conv3x3(channels, nf))\n    hs_c = [nf]\n    in_ch = nf\n    for i_level in range(num_resolutions):\n      # Residual blocks for this resolution\n      for i_block in range(num_res_blocks):\n        out_ch = nf * ch_mult[i_level]\n        modules.append(ResnetBlock(in_ch=in_ch, out_ch=out_ch))\n        in_ch = out_ch\n        if all_resolutions[i_level] in attn_resolutions:\n          modules.append(AttnBlock(channels=in_ch))\n        hs_c.append(in_ch)\n      if i_level != num_resolutions - 1:\n        modules.append(Downsample(channels=in_ch, with_conv=resamp_with_conv))\n        hs_c.append(in_ch)\n\n    in_ch = hs_c[-1]\n    modules.append(ResnetBlock(in_ch=in_ch))\n    modules.append(AttnBlock(channels=in_ch))\n    modules.append(ResnetBlock(in_ch=in_ch))\n\n    # Upsampling block\n    for i_level in reversed(range(num_resolutions)):\n      for i_block in range(num_res_blocks + 1):\n        out_ch = nf * ch_mult[i_level]\n        modules.append(ResnetBlock(in_ch=in_ch + hs_c.pop(), out_ch=out_ch))\n        in_ch = out_ch\n      if all_resolutions[i_level] in attn_resolutions:\n        modules.append(AttnBlock(channels=in_ch))\n      if i_level != 0:\n        modules.append(Upsample(channels=in_ch, with_conv=resamp_with_conv))\n\n    assert not hs_c\n    modules.append(nn.GroupNorm(num_channels=in_ch, num_groups=32, eps=1e-6))\n    modules.append(conv3x3(in_ch, channels, init_scale=0.))\n    self.all_modules = nn.ModuleList(modules)\n\n    self.scale_by_sigma = config.model.scale_by_sigma\n\n  def forward(self, x, labels):\n    modules = self.all_modules\n    m_idx = 0\n    if self.conditional:\n      # timestep/scale embedding\n      timesteps = labels\n      temb = layers.get_timestep_embedding(timesteps, self.nf)\n      temb = modules[m_idx](temb)\n      m_idx += 1\n      temb = modules[m_idx](self.act(temb))\n      m_idx += 1\n    else:\n      temb = None\n\n    if self.centered:\n      # Input is in [-1, 1]\n      h = x\n    else:\n      # Input is in [0, 1]\n      h = 2 * x - 1.\n\n    # Downsampling block\n    hs = [modules[m_idx](h)]\n    m_idx += 1\n    for i_level in range(self.num_resolutions):\n      # Residual blocks for this resolution\n      for i_block in range(self.num_res_blocks):\n        h = modules[m_idx](hs[-1], temb)\n        m_idx += 1\n        if h.shape[-1] in self.attn_resolutions:\n          h = modules[m_idx](h)\n          m_idx += 1\n        hs.append(h)\n      if i_level != self.num_resolutions - 1:\n        hs.append(modules[m_idx](hs[-1]))\n        m_idx += 1\n\n    h = hs[-1]\n    h = modules[m_idx](h, temb)\n    m_idx += 1\n    h = modules[m_idx](h)\n    m_idx += 1\n    h = modules[m_idx](h, temb)\n    m_idx += 1\n\n    # Upsampling block\n    for i_level in reversed(range(self.num_resolutions)):\n      for i_block in range(self.num_res_blocks + 1):\n        h = modules[m_idx](torch.cat([h, hs.pop()], dim=1), temb)\n        m_idx += 1\n      if h.shape[-1] in self.attn_resolutions:\n        h = modules[m_idx](h)\n        m_idx += 1\n      if i_level != 0:\n        h = modules[m_idx](h)\n        m_idx += 1\n\n    assert not hs\n    h = self.act(modules[m_idx](h))\n    m_idx += 1\n    h = modules[m_idx](h)\n    m_idx += 1\n    assert m_idx == len(modules)\n\n    if self.scale_by_sigma:\n      # Divide the output by sigmas. Useful for training with the NCSN loss.\n      # The DDPM loss scales the network output by sigma in the loss function,\n      # so no need of doing it here.\n      used_sigmas = self.sigmas[labels, None, None, None]\n      h = h / used_sigmas\n\n    return h\n"}
{"type": "source_file", "path": "Image_Purifier/score_sde/models/up_or_down_sampling.py", "content": "# ---------------------------------------------------------------\n# Taken from the following link as is from:\n# https://github.com/yang-song/score_sde_pytorch/blob/main/models/up_or_down_sampling.py\n#\n# The license for the original version of this file can be\n# found in the `score_sde` directory (LICENSE_SCORE_SDE).\n# ---------------------------------------------------------------\n\n\"\"\"Layers used for up-sampling or down-sampling images.\n\nMany functions are ported from https://github.com/NVlabs/stylegan2.\n\"\"\"\n\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom ..op import upfirdn2d\n\n\n# Function ported from StyleGAN2\ndef get_weight(module,\n               shape,\n               weight_var='weight',\n               kernel_init=None):\n  \"\"\"Get/create weight tensor for a convolution or fully-connected layer.\"\"\"\n\n  return module.param(weight_var, kernel_init, shape)\n\n\nclass Conv2d(nn.Module):\n  \"\"\"Conv2d layer with optimal upsampling and downsampling (StyleGAN2).\"\"\"\n\n  def __init__(self, in_ch, out_ch, kernel, up=False, down=False,\n               resample_kernel=(1, 3, 3, 1),\n               use_bias=True,\n               kernel_init=None):\n    super().__init__()\n    assert not (up and down)\n    assert kernel >= 1 and kernel % 2 == 1\n    self.weight = nn.Parameter(torch.zeros(out_ch, in_ch, kernel, kernel))\n    if kernel_init is not None:\n      self.weight.data = kernel_init(self.weight.data.shape)\n    if use_bias:\n      self.bias = nn.Parameter(torch.zeros(out_ch))\n\n    self.up = up\n    self.down = down\n    self.resample_kernel = resample_kernel\n    self.kernel = kernel\n    self.use_bias = use_bias\n\n  def forward(self, x):\n    if self.up:\n      x = upsample_conv_2d(x, self.weight, k=self.resample_kernel)\n    elif self.down:\n      x = conv_downsample_2d(x, self.weight, k=self.resample_kernel)\n    else:\n      x = F.conv2d(x, self.weight, stride=1, padding=self.kernel // 2)\n\n    if self.use_bias:\n      x = x + self.bias.reshape(1, -1, 1, 1)\n\n    return x\n\n\ndef naive_upsample_2d(x, factor=2):\n  _N, C, H, W = x.shape\n  x = torch.reshape(x, (-1, C, H, 1, W, 1))\n  x = x.repeat(1, 1, 1, factor, 1, factor)\n  return torch.reshape(x, (-1, C, H * factor, W * factor))\n\n\ndef naive_downsample_2d(x, factor=2):\n  _N, C, H, W = x.shape\n  x = torch.reshape(x, (-1, C, H // factor, factor, W // factor, factor))\n  return torch.mean(x, dim=(3, 5))\n\n\ndef upsample_conv_2d(x, w, k=None, factor=2, gain=1):\n  \"\"\"Fused `upsample_2d()` followed by `tf.nn.conv2d()`.\n\n     Padding is performed only once at the beginning, not between the\n     operations.\n     The fused op is considerably more efficient than performing the same\n     calculation\n     using standard TensorFlow ops. It supports gradients of arbitrary order.\n     Args:\n       x:            Input tensor of the shape `[N, C, H, W]` or `[N, H, W,\n         C]`.\n       w:            Weight tensor of the shape `[filterH, filterW, inChannels,\n         outChannels]`. Grouped convolution can be performed by `inChannels =\n         x.shape[0] // numGroups`.\n       k:            FIR filter of the shape `[firH, firW]` or `[firN]`\n         (separable). The default is `[1] * factor`, which corresponds to\n         nearest-neighbor upsampling.\n       factor:       Integer upsampling factor (default: 2).\n       gain:         Scaling factor for signal magnitude (default: 1.0).\n\n     Returns:\n       Tensor of the shape `[N, C, H * factor, W * factor]` or\n       `[N, H * factor, W * factor, C]`, and same datatype as `x`.\n  \"\"\"\n\n  assert isinstance(factor, int) and factor >= 1\n\n  # Check weight shape.\n  assert len(w.shape) == 4\n  convH = w.shape[2]\n  convW = w.shape[3]\n  inC = w.shape[1]\n  outC = w.shape[0]\n\n  assert convW == convH\n\n  # Setup filter kernel.\n  if k is None:\n    k = [1] * factor\n  k = _setup_kernel(k) * (gain * (factor ** 2))\n  p = (k.shape[0] - factor) - (convW - 1)\n\n  stride = (factor, factor)\n\n  # Determine data dimensions.\n  stride = [1, 1, factor, factor]\n  output_shape = ((_shape(x, 2) - 1) * factor + convH, (_shape(x, 3) - 1) * factor + convW)\n  output_padding = (output_shape[0] - (_shape(x, 2) - 1) * stride[0] - convH,\n                    output_shape[1] - (_shape(x, 3) - 1) * stride[1] - convW)\n  assert output_padding[0] >= 0 and output_padding[1] >= 0\n  num_groups = _shape(x, 1) // inC\n\n  # Transpose weights.\n  w = torch.reshape(w, (num_groups, -1, inC, convH, convW))\n  w = w[..., ::-1, ::-1].permute(0, 2, 1, 3, 4)\n  w = torch.reshape(w, (num_groups * inC, -1, convH, convW))\n\n  x = F.conv_transpose2d(x, w, stride=stride, output_padding=output_padding, padding=0)\n  ## Original TF code.\n  # x = tf.nn.conv2d_transpose(\n  #     x,\n  #     w,\n  #     output_shape=output_shape,\n  #     strides=stride,\n  #     padding='VALID',\n  #     data_format=data_format)\n  ## JAX equivalent\n\n  return upfirdn2d(x, torch.tensor(k, device=x.device),\n                   pad=((p + 1) // 2 + factor - 1, p // 2 + 1))\n\n\ndef conv_downsample_2d(x, w, k=None, factor=2, gain=1):\n  \"\"\"Fused `tf.nn.conv2d()` followed by `downsample_2d()`.\n\n    Padding is performed only once at the beginning, not between the operations.\n    The fused op is considerably more efficient than performing the same\n    calculation\n    using standard TensorFlow ops. It supports gradients of arbitrary order.\n    Args:\n        x:            Input tensor of the shape `[N, C, H, W]` or `[N, H, W,\n          C]`.\n        w:            Weight tensor of the shape `[filterH, filterW, inChannels,\n          outChannels]`. Grouped convolution can be performed by `inChannels =\n          x.shape[0] // numGroups`.\n        k:            FIR filter of the shape `[firH, firW]` or `[firN]`\n          (separable). The default is `[1] * factor`, which corresponds to\n          average pooling.\n        factor:       Integer downsampling factor (default: 2).\n        gain:         Scaling factor for signal magnitude (default: 1.0).\n\n    Returns:\n        Tensor of the shape `[N, C, H // factor, W // factor]` or\n        `[N, H // factor, W // factor, C]`, and same datatype as `x`.\n  \"\"\"\n\n  assert isinstance(factor, int) and factor >= 1\n  _outC, _inC, convH, convW = w.shape\n  assert convW == convH\n  if k is None:\n    k = [1] * factor\n  k = _setup_kernel(k) * gain\n  p = (k.shape[0] - factor) + (convW - 1)\n  s = [factor, factor]\n  x = upfirdn2d(x, torch.tensor(k, device=x.device),\n                pad=((p + 1) // 2, p // 2))\n  return F.conv2d(x, w, stride=s, padding=0)\n\n\ndef _setup_kernel(k):\n  k = np.asarray(k, dtype=np.float32)\n  if k.ndim == 1:\n    k = np.outer(k, k)\n  k /= np.sum(k)\n  assert k.ndim == 2\n  assert k.shape[0] == k.shape[1]\n  return k\n\n\ndef _shape(x, dim):\n  return x.shape[dim]\n\n\ndef upsample_2d(x, k=None, factor=2, gain=1):\n  r\"\"\"Upsample a batch of 2D images with the given filter.\n\n    Accepts a batch of 2D images of the shape `[N, C, H, W]` or `[N, H, W, C]`\n    and upsamples each image with the given filter. The filter is normalized so\n    that\n    if the input pixels are constant, they will be scaled by the specified\n    `gain`.\n    Pixels outside the image are assumed to be zero, and the filter is padded\n    with\n    zeros so that its shape is a multiple of the upsampling factor.\n    Args:\n        x:            Input tensor of the shape `[N, C, H, W]` or `[N, H, W,\n          C]`.\n        k:            FIR filter of the shape `[firH, firW]` or `[firN]`\n          (separable). The default is `[1] * factor`, which corresponds to\n          nearest-neighbor upsampling.\n        factor:       Integer upsampling factor (default: 2).\n        gain:         Scaling factor for signal magnitude (default: 1.0).\n\n    Returns:\n        Tensor of the shape `[N, C, H * factor, W * factor]`\n  \"\"\"\n  assert isinstance(factor, int) and factor >= 1\n  if k is None:\n    k = [1] * factor\n  k = _setup_kernel(k) * (gain * (factor ** 2))\n  p = k.shape[0] - factor\n  return upfirdn2d(x, torch.tensor(k, device=x.device),\n                   up=factor, pad=((p + 1) // 2 + factor - 1, p // 2))\n\n\ndef downsample_2d(x, k=None, factor=2, gain=1):\n  r\"\"\"Downsample a batch of 2D images with the given filter.\n\n    Accepts a batch of 2D images of the shape `[N, C, H, W]` or `[N, H, W, C]`\n    and downsamples each image with the given filter. The filter is normalized\n    so that\n    if the input pixels are constant, they will be scaled by the specified\n    `gain`.\n    Pixels outside the image are assumed to be zero, and the filter is padded\n    with\n    zeros so that its shape is a multiple of the downsampling factor.\n    Args:\n        x:            Input tensor of the shape `[N, C, H, W]` or `[N, H, W,\n          C]`.\n        k:            FIR filter of the shape `[firH, firW]` or `[firN]`\n          (separable). The default is `[1] * factor`, which corresponds to\n          average pooling.\n        factor:       Integer downsampling factor (default: 2).\n        gain:         Scaling factor for signal magnitude (default: 1.0).\n\n    Returns:\n        Tensor of the shape `[N, C, H // factor, W // factor]`\n  \"\"\"\n\n  assert isinstance(factor, int) and factor >= 1\n  if k is None:\n    k = [1] * factor\n  k = _setup_kernel(k) * gain\n  p = k.shape[0] - factor\n  return upfirdn2d(x, torch.tensor(k, device=x.device),\n                   down=factor, pad=((p + 1) // 2, p // 2))\n"}
{"type": "source_file", "path": "Image_Purifier/score_sde/op/upfirdn2d.py", "content": "# ---------------------------------------------------------------\n# Taken from the following link as is from:\n# https://github.com/yang-song/score_sde_pytorch/blob/main/op/upfirdn2d.py\n#\n# The license for the original version of this file can be\n# found in the `score_sde` directory (LICENSE_SCORE_SDE).\n# ---------------------------------------------------------------\n\nimport os\n\nimport torch\nfrom torch.nn import functional as F\nfrom torch.autograd import Function\nfrom torch.utils.cpp_extension import load\n\n\nmodule_path = os.path.dirname(__file__)\nupfirdn2d_op = load(\n    \"upfirdn2d\",\n    sources=[\n        os.path.join(module_path, \"upfirdn2d.cpp\"),\n        os.path.join(module_path, \"upfirdn2d_kernel.cu\"),\n    ],\n)\n\n\nclass UpFirDn2dBackward(Function):\n    @staticmethod\n    def forward(\n        ctx, grad_output, kernel, grad_kernel, up, down, pad, g_pad, in_size, out_size\n    ):\n\n        up_x, up_y = up\n        down_x, down_y = down\n        g_pad_x0, g_pad_x1, g_pad_y0, g_pad_y1 = g_pad\n\n        grad_output = grad_output.reshape(-1, out_size[0], out_size[1], 1)\n\n        grad_input = upfirdn2d_op.upfirdn2d(\n            grad_output,\n            grad_kernel,\n            down_x,\n            down_y,\n            up_x,\n            up_y,\n            g_pad_x0,\n            g_pad_x1,\n            g_pad_y0,\n            g_pad_y1,\n        )\n        grad_input = grad_input.view(in_size[0], in_size[1], in_size[2], in_size[3])\n\n        ctx.save_for_backward(kernel)\n\n        pad_x0, pad_x1, pad_y0, pad_y1 = pad\n\n        ctx.up_x = up_x\n        ctx.up_y = up_y\n        ctx.down_x = down_x\n        ctx.down_y = down_y\n        ctx.pad_x0 = pad_x0\n        ctx.pad_x1 = pad_x1\n        ctx.pad_y0 = pad_y0\n        ctx.pad_y1 = pad_y1\n        ctx.in_size = in_size\n        ctx.out_size = out_size\n\n        return grad_input\n\n    @staticmethod\n    def backward(ctx, gradgrad_input):\n        kernel, = ctx.saved_tensors\n\n        gradgrad_input = gradgrad_input.reshape(-1, ctx.in_size[2], ctx.in_size[3], 1)\n\n        gradgrad_out = upfirdn2d_op.upfirdn2d(\n            gradgrad_input,\n            kernel,\n            ctx.up_x,\n            ctx.up_y,\n            ctx.down_x,\n            ctx.down_y,\n            ctx.pad_x0,\n            ctx.pad_x1,\n            ctx.pad_y0,\n            ctx.pad_y1,\n        )\n        # gradgrad_out = gradgrad_out.view(ctx.in_size[0], ctx.out_size[0], ctx.out_size[1], ctx.in_size[3])\n        gradgrad_out = gradgrad_out.view(\n            ctx.in_size[0], ctx.in_size[1], ctx.out_size[0], ctx.out_size[1]\n        )\n\n        return gradgrad_out, None, None, None, None, None, None, None, None\n\n\nclass UpFirDn2d(Function):\n    @staticmethod\n    def forward(ctx, input, kernel, up, down, pad):\n        up_x, up_y = up\n        down_x, down_y = down\n        pad_x0, pad_x1, pad_y0, pad_y1 = pad\n\n        kernel_h, kernel_w = kernel.shape\n        batch, channel, in_h, in_w = input.shape\n        ctx.in_size = input.shape\n\n        input = input.reshape(-1, in_h, in_w, 1)\n\n        ctx.save_for_backward(kernel, torch.flip(kernel, [0, 1]))\n\n        out_h = (in_h * up_y + pad_y0 + pad_y1 - kernel_h) // down_y + 1\n        out_w = (in_w * up_x + pad_x0 + pad_x1 - kernel_w) // down_x + 1\n        ctx.out_size = (out_h, out_w)\n\n        ctx.up = (up_x, up_y)\n        ctx.down = (down_x, down_y)\n        ctx.pad = (pad_x0, pad_x1, pad_y0, pad_y1)\n\n        g_pad_x0 = kernel_w - pad_x0 - 1\n        g_pad_y0 = kernel_h - pad_y0 - 1\n        g_pad_x1 = in_w * up_x - out_w * down_x + pad_x0 - up_x + 1\n        g_pad_y1 = in_h * up_y - out_h * down_y + pad_y0 - up_y + 1\n\n        ctx.g_pad = (g_pad_x0, g_pad_x1, g_pad_y0, g_pad_y1)\n\n        out = upfirdn2d_op.upfirdn2d(\n            input, kernel, up_x, up_y, down_x, down_y, pad_x0, pad_x1, pad_y0, pad_y1\n        )\n        # out = out.view(major, out_h, out_w, minor)\n        out = out.view(-1, channel, out_h, out_w)\n\n        return out\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        kernel, grad_kernel = ctx.saved_tensors\n\n        grad_input = UpFirDn2dBackward.apply(\n            grad_output,\n            kernel,\n            grad_kernel,\n            ctx.up,\n            ctx.down,\n            ctx.pad,\n            ctx.g_pad,\n            ctx.in_size,\n            ctx.out_size,\n        )\n\n        return grad_input, None, None, None, None\n\n\ndef upfirdn2d(input, kernel, up=1, down=1, pad=(0, 0)):\n    if input.device.type == \"cpu\":\n        out = upfirdn2d_native(\n            input, kernel, up, up, down, down, pad[0], pad[1], pad[0], pad[1]\n        )\n\n    else:\n        out = UpFirDn2d.apply(\n            input, kernel, (up, up), (down, down), (pad[0], pad[1], pad[0], pad[1])\n        )\n\n    return out\n\n\ndef upfirdn2d_native(\n    input, kernel, up_x, up_y, down_x, down_y, pad_x0, pad_x1, pad_y0, pad_y1\n):\n    _, channel, in_h, in_w = input.shape\n    input = input.reshape(-1, in_h, in_w, 1)\n\n    _, in_h, in_w, minor = input.shape\n    kernel_h, kernel_w = kernel.shape\n\n    out = input.view(-1, in_h, 1, in_w, 1, minor)\n    out = F.pad(out, [0, 0, 0, up_x - 1, 0, 0, 0, up_y - 1])\n    out = out.view(-1, in_h * up_y, in_w * up_x, minor)\n\n    out = F.pad(\n        out, [0, 0, max(pad_x0, 0), max(pad_x1, 0), max(pad_y0, 0), max(pad_y1, 0)]\n    )\n    out = out[\n        :,\n        max(-pad_y0, 0) : out.shape[1] - max(-pad_y1, 0),\n        max(-pad_x0, 0) : out.shape[2] - max(-pad_x1, 0),\n        :,\n    ]\n\n    out = out.permute(0, 3, 1, 2)\n    out = out.reshape(\n        [-1, 1, in_h * up_y + pad_y0 + pad_y1, in_w * up_x + pad_x0 + pad_x1]\n    )\n    w = torch.flip(kernel, [0, 1]).view(1, 1, kernel_h, kernel_w)\n    out = F.conv2d(out, w)\n    out = out.reshape(\n        -1,\n        minor,\n        in_h * up_y + pad_y0 + pad_y1 - kernel_h + 1,\n        in_w * up_x + pad_x0 + pad_x1 - kernel_w + 1,\n    )\n    out = out.permute(0, 2, 3, 1)\n    out = out[:, ::down_y, ::down_x, :]\n\n    out_h = (in_h * up_y + pad_y0 + pad_y1 - kernel_h) // down_y + 1\n    out_w = (in_w * up_x + pad_x0 + pad_x1 - kernel_w) // down_x + 1\n\n    return out.view(-1, channel, out_h, out_w)\n"}
{"type": "source_file", "path": "Image_Purifier/score_sde/sde_lib.py", "content": "# ---------------------------------------------------------------\n# Taken from the following link as is from:\n# https://github.com/yang-song/score_sde_pytorch/blob/main/sde_lib.py\n#\n# The license for the original version of this file can be\n# found in this directory (LICENSE_SCORE_SDE).\n# ---------------------------------------------------------------\n\n\"\"\"Abstract SDE classes, Reverse SDE, and VE/VP SDEs.\"\"\"\nimport abc\nimport torch\nimport numpy as np\n\n\nclass SDE(abc.ABC):\n  \"\"\"SDE abstract class. Functions are designed for a mini-batch of inputs.\"\"\"\n\n  def __init__(self, N):\n    \"\"\"Construct an SDE.\n\n    Args:\n      N: number of discretization time steps.\n    \"\"\"\n    super().__init__()\n    self.N = N\n\n  @property\n  @abc.abstractmethod\n  def T(self):\n    \"\"\"End time of the SDE.\"\"\"\n    pass\n\n  @abc.abstractmethod\n  def sde(self, x, t):\n    pass\n\n  @abc.abstractmethod\n  def marginal_prob(self, x, t):\n    \"\"\"Parameters to determine the marginal distribution of the SDE, $p_t(x)$.\"\"\"\n    pass\n\n  @abc.abstractmethod\n  def prior_sampling(self, shape):\n    \"\"\"Generate one sample from the prior distribution, $p_T(x)$.\"\"\"\n    pass\n\n  @abc.abstractmethod\n  def prior_logp(self, z):\n    \"\"\"Compute log-density of the prior distribution.\n\n    Useful for computing the log-likelihood via probability flow ODE.\n\n    Args:\n      z: latent code\n    Returns:\n      log probability density\n    \"\"\"\n    pass\n\n  def discretize(self, x, t):\n    \"\"\"Discretize the SDE in the form: x_{i+1} = x_i + f_i(x_i) + G_i z_i.\n\n    Useful for reverse diffusion sampling and probabiliy flow sampling.\n    Defaults to Euler-Maruyama discretization.\n\n    Args:\n      x: a torch tensor\n      t: a torch float representing the time step (from 0 to `self.T`)\n\n    Returns:\n      f, G\n    \"\"\"\n    dt = 1 / self.N\n    drift, diffusion = self.sde(x, t)\n    f = drift * dt\n    G = diffusion * torch.sqrt(torch.tensor(dt, device=t.device))\n    return f, G\n\n  def reverse(self, score_fn, probability_flow=False):\n    \"\"\"Create the reverse-time SDE/ODE.\n\n    Args:\n      score_fn: A time-dependent score-based model that takes x and t and returns the score.\n      probability_flow: If `True`, create the reverse-time ODE used for probability flow sampling.\n    \"\"\"\n    N = self.N\n    T = self.T\n    sde_fn = self.sde\n    discretize_fn = self.discretize\n\n    # Build the class for reverse-time SDE.\n    class RSDE(self.__class__):\n      def __init__(self):\n        self.N = N\n        self.probability_flow = probability_flow\n\n      @property\n      def T(self):\n        return T\n\n      def sde(self, x, t):\n        \"\"\"Create the drift and diffusion functions for the reverse SDE/ODE.\"\"\"\n        drift, diffusion = sde_fn(x, t)\n        score = score_fn(x, t)\n        drift = drift - diffusion[:, None, None, None] ** 2 * score * (0.5 if self.probability_flow else 1.)\n        # Set the diffusion function to zero for ODEs.\n        diffusion = 0. if self.probability_flow else diffusion\n        return drift, diffusion\n\n      def discretize(self, x, t):\n        \"\"\"Create discretized iteration rules for the reverse diffusion sampler.\"\"\"\n        f, G = discretize_fn(x, t)\n        rev_f = f - G[:, None, None, None] ** 2 * score_fn(x, t) * (0.5 if self.probability_flow else 1.)\n        rev_G = torch.zeros_like(G) if self.probability_flow else G\n        return rev_f, rev_G\n\n    return RSDE()\n\n\nclass VPSDE(SDE):\n  def __init__(self, beta_min=0.1, beta_max=20, N=1000):\n    \"\"\"Construct a Variance Preserving SDE.\n\n    Args:\n      beta_min: value of beta(0)\n      beta_max: value of beta(1)\n      N: number of discretization steps\n    \"\"\"\n    super().__init__(N)\n    self.beta_0 = beta_min\n    self.beta_1 = beta_max\n    self.N = N\n    self.discrete_betas = torch.linspace(beta_min / N, beta_max / N, N)\n    self.alphas = 1. - self.discrete_betas\n    self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n    self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n    self.sqrt_1m_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod)\n\n  @property\n  def T(self):\n    return 1\n\n  def sde(self, x, t):\n    beta_t = self.beta_0 + t * (self.beta_1 - self.beta_0)\n    drift = -0.5 * beta_t[:, None, None, None] * x\n    diffusion = torch.sqrt(beta_t)\n    return drift, diffusion\n\n  def marginal_prob(self, x, t):\n    log_mean_coeff = -0.25 * t ** 2 * (self.beta_1 - self.beta_0) - 0.5 * t * self.beta_0\n    mean = torch.exp(log_mean_coeff[:, None, None, None]) * x\n    std = torch.sqrt(1. - torch.exp(2. * log_mean_coeff))\n    return mean, std\n\n  def prior_sampling(self, shape):\n    return torch.randn(*shape)\n\n  def prior_logp(self, z):\n    shape = z.shape\n    N = np.prod(shape[1:])\n    logps = -N / 2. * np.log(2 * np.pi) - torch.sum(z ** 2, dim=(1, 2, 3)) / 2.\n    return logps\n\n  def discretize(self, x, t):\n    \"\"\"DDPM discretization.\"\"\"\n    timestep = (t * (self.N - 1) / self.T).long()\n    beta = self.discrete_betas.to(x.device)[timestep]\n    alpha = self.alphas.to(x.device)[timestep]\n    sqrt_beta = torch.sqrt(beta)\n    f = torch.sqrt(alpha)[:, None, None, None] * x - x\n    G = sqrt_beta\n    return f, G\n\n\nclass subVPSDE(SDE):\n  def __init__(self, beta_min=0.1, beta_max=20, N=1000):\n    \"\"\"Construct the sub-VP SDE that excels at likelihoods.\n\n    Args:\n      beta_min: value of beta(0)\n      beta_max: value of beta(1)\n      N: number of discretization steps\n    \"\"\"\n    super().__init__(N)\n    self.beta_0 = beta_min\n    self.beta_1 = beta_max\n    self.N = N\n\n  @property\n  def T(self):\n    return 1\n\n  def sde(self, x, t):\n    beta_t = self.beta_0 + t * (self.beta_1 - self.beta_0)\n    drift = -0.5 * beta_t[:, None, None, None] * x\n    discount = 1. - torch.exp(-2 * self.beta_0 * t - (self.beta_1 - self.beta_0) * t ** 2)\n    diffusion = torch.sqrt(beta_t * discount)\n    return drift, diffusion\n\n  def marginal_prob(self, x, t):\n    log_mean_coeff = -0.25 * t ** 2 * (self.beta_1 - self.beta_0) - 0.5 * t * self.beta_0\n    mean = torch.exp(log_mean_coeff)[:, None, None, None] * x\n    std = 1 - torch.exp(2. * log_mean_coeff)\n    return mean, std\n\n  def prior_sampling(self, shape):\n    return torch.randn(*shape)\n\n  def prior_logp(self, z):\n    shape = z.shape\n    N = np.prod(shape[1:])\n    return -N / 2. * np.log(2 * np.pi) - torch.sum(z ** 2, dim=(1, 2, 3)) / 2.\n\n\nclass VESDE(SDE):\n  def __init__(self, sigma_min=0.01, sigma_max=50, N=1000):\n    \"\"\"Construct a Variance Exploding SDE.\n\n    Args:\n      sigma_min: smallest sigma.\n      sigma_max: largest sigma.\n      N: number of discretization steps\n    \"\"\"\n    super().__init__(N)\n    self.sigma_min = sigma_min\n    self.sigma_max = sigma_max\n    self.discrete_sigmas = torch.exp(torch.linspace(np.log(self.sigma_min), np.log(self.sigma_max), N))\n    self.N = N\n\n  @property\n  def T(self):\n    return 1\n\n  def sde(self, x, t):\n    sigma = self.sigma_min * (self.sigma_max / self.sigma_min) ** t\n    drift = torch.zeros_like(x)\n    diffusion = sigma * torch.sqrt(torch.tensor(2 * (np.log(self.sigma_max) - np.log(self.sigma_min)),\n                                                device=t.device))\n    return drift, diffusion\n\n  def marginal_prob(self, x, t):\n    std = self.sigma_min * (self.sigma_max / self.sigma_min) ** t\n    mean = x\n    return mean, std\n\n  def prior_sampling(self, shape):\n    return torch.randn(*shape) * self.sigma_max\n\n  def prior_logp(self, z):\n    shape = z.shape\n    N = np.prod(shape[1:])\n    return -N / 2. * np.log(2 * np.pi * self.sigma_max ** 2) - torch.sum(z ** 2, dim=(1, 2, 3)) / (2 * self.sigma_max ** 2)\n\n  def discretize(self, x, t):\n    \"\"\"SMLD(NCSN) discretization.\"\"\"\n    timestep = (t * (self.N - 1) / self.T).long()\n    sigma = self.discrete_sigmas.to(t.device)[timestep]\n    adjacent_sigma = torch.where(timestep == 0, torch.zeros_like(t),\n                                 self.discrete_sigmas[timestep - 1].to(t.device))\n    f = torch.zeros_like(x)\n    G = torch.sqrt(sigma ** 2 - adjacent_sigma ** 2)\n    return f, G"}
{"type": "source_file", "path": "Image_Purifier/utils.py", "content": "# ---------------------------------------------------------------\n# Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.\n#\n# This work is licensed under the NVIDIA Source Code License\n# for DiffPure. To view a copy of this license, see the LICENSE file.\n# ---------------------------------------------------------------\n\nimport sys\nimport argparse\nfrom typing import Any\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\n\nfrom robustbench import load_model\nimport data\n\n\ndef compute_n_params(model, return_str=True):\n    tot = 0\n    for p in model.parameters():\n        w = 1\n        for x in p.shape:\n            w *= x\n        tot += w\n    if return_str:\n        if tot >= 1e6:\n            return '{:.1f}M'.format(tot / 1e6)\n        else:\n            return '{:.1f}K'.format(tot / 1e3)\n    else:\n        return tot\n\n\nclass Logger(object):\n    \"\"\"\n    Redirect stderr to stdout, optionally print stdout to a file,\n    and optionally force flushing on both stdout and the file.\n    \"\"\"\n\n    def __init__(self, file_name: str = None, file_mode: str = \"w\", should_flush: bool = True):\n        self.file = None\n\n        if file_name is not None:\n            self.file = open(file_name, file_mode)\n\n        self.should_flush = should_flush\n        self.stdout = sys.stdout\n        self.stderr = sys.stderr\n\n        sys.stdout = self\n        sys.stderr = self\n\n    def __enter__(self) -> \"Logger\":\n        return self\n\n    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n        self.close()\n\n    def write(self, text: str) -> None:\n        \"\"\"Write text to stdout (and a file) and optionally flush.\"\"\"\n        if len(text) == 0: # workaround for a bug in VSCode debugger: sys.stdout.write(''); sys.stdout.flush() => crash\n            return\n\n        if self.file is not None:\n            self.file.write(text)\n\n        self.stdout.write(text)\n\n        if self.should_flush:\n            self.flush()\n\n    def flush(self) -> None:\n        \"\"\"Flush written text to both stdout and a file, if open.\"\"\"\n        if self.file is not None:\n            self.file.flush()\n\n        self.stdout.flush()\n\n    def close(self) -> None:\n        \"\"\"Flush, close possible files, and remove stdout/stderr mirroring.\"\"\"\n        self.flush()\n\n        # if using multiple loggers, prevent closing in wrong order\n        if sys.stdout is self:\n            sys.stdout = self.stdout\n        if sys.stderr is self:\n            sys.stderr = self.stderr\n\n        if self.file is not None:\n            self.file.close()\n\n\ndef dict2namespace(config):\n    namespace = argparse.Namespace()\n    for key, value in config.items():\n        if isinstance(value, dict):\n            new_value = dict2namespace(value)\n        else:\n            new_value = value\n        setattr(namespace, key, new_value)\n    return namespace\n\n\ndef str2bool(v):\n    if isinstance(v, bool):\n        return v\n    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n        return True\n    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError('Boolean value expected.')\n\n\ndef update_state_dict(state_dict, idx_start=9):\n\n    from collections import OrderedDict\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        name = k[idx_start:]  # remove 'module.0.' of dataparallel\n        new_state_dict[name]=v\n\n    return new_state_dict\n\n\n# ------------------------------------------------------------------------\ndef get_accuracy(model, x_orig, y_orig, bs=64, device=torch.device('cuda:0')):\n    n_batches = x_orig.shape[0] // bs\n    acc = 0.\n    for counter in range(n_batches):\n        x = x_orig[counter * bs:min((counter + 1) * bs, x_orig.shape[0])].clone().to(device)\n        y = y_orig[counter * bs:min((counter + 1) * bs, x_orig.shape[0])].clone().to(device)\n        output = model(x)\n        acc += (output.max(1)[1] == y).float().sum()\n\n    return (acc / x_orig.shape[0]).item()\n\n\ndef get_image_classifier(classifier_name):\n    class _Wrapper_ResNet(nn.Module):\n        def __init__(self, resnet):\n            super().__init__()\n            self.resnet = resnet\n            self.mu = torch.Tensor([0.485, 0.456, 0.406]).float().view(3, 1, 1)\n            self.sigma = torch.Tensor([0.229, 0.224, 0.225]).float().view(3, 1, 1)\n\n        def forward(self, x):\n            x = (x - self.mu.to(x.device)) / self.sigma.to(x.device)\n            return self.resnet(x)\n\n    if 'imagenet' in classifier_name:\n        if 'resnet18' in classifier_name:\n            print('using imagenet resnet18...')\n            model = models.resnet18(pretrained=True).eval()\n        elif 'resnet50' in classifier_name:\n            print('using imagenet resnet50...')\n            model = models.resnet50(pretrained=True).eval()\n        elif 'resnet101' in classifier_name:\n            print('using imagenet resnet101...')\n            model = models.resnet101(pretrained=True).eval()\n        elif 'wideresnet-50-2' in classifier_name:\n            print('using imagenet wideresnet-50-2...')\n            model = models.wide_resnet50_2(pretrained=True).eval()\n        elif 'deit-s' in classifier_name:\n            print('using imagenet deit-s...')\n            model = torch.hub.load('facebookresearch/deit:main', 'deit_small_patch16_224', pretrained=True).eval()\n        else:\n            raise NotImplementedError(f'unknown {classifier_name}')\n\n        wrapper_resnet = _Wrapper_ResNet(model)\n\n    elif 'cifar10' in classifier_name:\n        if 'wideresnet-28-10' in classifier_name:\n            print('using cifar10 wideresnet-28-10...')\n            model = load_model(model_name='Standard', dataset='cifar10', threat_model='Linf')  # pixel in [0, 1]\n\n        elif 'wrn-28-10-at0' in classifier_name:\n            print('using cifar10 wrn-28-10-at0...')\n            model = load_model(model_name='Gowal2021Improving_28_10_ddpm_100m', dataset='cifar10',\n                               threat_model='Linf')  # pixel in [0, 1]\n\n        elif 'wrn-28-10-at1' in classifier_name:\n            print('using cifar10 wrn-28-10-at1...')\n            model = load_model(model_name='Gowal2020Uncovering_28_10_extra', dataset='cifar10',\n                               threat_model='Linf')  # pixel in [0, 1]\n\n        elif 'wrn-70-16-at0' in classifier_name:\n            print('using cifar10 wrn-70-16-at0...')\n            model = load_model(model_name='Gowal2021Improving_70_16_ddpm_100m', dataset='cifar10',\n                               threat_model='Linf')  # pixel in [0, 1]\n\n        elif 'wrn-70-16-at1' in classifier_name:\n            print('using cifar10 wrn-70-16-at1...')\n            model = load_model(model_name='Rebuffi2021Fixing_70_16_cutmix_extra', dataset='cifar10',\n                               threat_model='Linf')  # pixel in [0, 1]\n\n        elif 'wrn-70-16-L2-at1' in classifier_name:\n            print('using cifar10 wrn-70-16-L2-at1...')\n            model = load_model(model_name='Rebuffi2021Fixing_70_16_cutmix_extra', dataset='cifar10',\n                               threat_model='L2')  # pixel in [0, 1]\n\n        elif 'wideresnet-70-16' in classifier_name:\n            print('using cifar10 wideresnet-70-16 (dm_wrn-70-16)...')\n            from robustbench.model_zoo.architectures.dm_wide_resnet import DMWideResNet, Swish\n            model = DMWideResNet(num_classes=10, depth=70, width=16, activation_fn=Swish)  # pixel in [0, 1]\n\n            model_path = 'pretrained/cifar10/wresnet-76-10/weights-best.pt'\n            print(f\"=> loading wideresnet-70-16 checkpoint '{model_path}'\")\n            model.load_state_dict(update_state_dict(torch.load(model_path)['model_state_dict']))\n            model.eval()\n            print(f\"=> loaded wideresnet-70-16 checkpoint\")\n\n        elif 'resnet-50' in classifier_name:\n            print('using cifar10 resnet-50...')\n            from classifiers.cifar10_resnet import ResNet50\n            model = ResNet50()  # pixel in [0, 1]\n\n            model_path = 'pretrained/cifar10/resnet-50/weights.pt'\n            print(f\"=> loading resnet-50 checkpoint '{model_path}'\")\n            model.load_state_dict(update_state_dict(torch.load(model_path), idx_start=7))\n            model.eval()\n            print(f\"=> loaded resnet-50 checkpoint\")\n\n        elif 'wrn-70-16-dropout' in classifier_name:\n            print('using cifar10 wrn-70-16-dropout (standard wrn-70-16-dropout)...')\n            from classifiers.cifar10_resnet import WideResNet_70_16_dropout\n            model = WideResNet_70_16_dropout()  # pixel in [0, 1]\n\n            model_path = 'pretrained/cifar10/wrn-70-16-dropout/weights.pt'\n            print(f\"=> loading wrn-70-16-dropout checkpoint '{model_path}'\")\n            model.load_state_dict(update_state_dict(torch.load(model_path), idx_start=7))\n            model.eval()\n            print(f\"=> loaded wrn-70-16-dropout checkpoint\")\n\n        else:\n            raise NotImplementedError(f'unknown {classifier_name}')\n\n        wrapper_resnet = model\n\n    elif 'celebahq' in classifier_name:\n        attribute = classifier_name.split('__')[-1]  # `celebahq__Smiling`\n        ckpt_path = f'pretrained/celebahq/{attribute}/net_best.pth'\n        from classifiers.attribute_classifier import ClassifierWrapper\n        model = ClassifierWrapper(attribute, ckpt_path=ckpt_path)\n        wrapper_resnet = model\n    else:\n        raise NotImplementedError(f'unknown {classifier_name}')\n\n    return wrapper_resnet\n\n\ndef load_data(args, adv_batch_size):\n    if 'imagenet' in args.domain:\n        val_dir = './dataset/imagenet_lmdb/val'  # using imagenet lmdb data\n        val_transform = data.get_transform(args.domain, 'imval', base_size=224)\n        val_data = data.imagenet_lmdb_dataset_sub(val_dir, transform=val_transform,\n                                                  num_sub=args.num_sub, data_seed=args.data_seed)\n        n_samples = len(val_data)\n        val_loader = DataLoader(val_data, batch_size=n_samples, shuffle=False, pin_memory=True, num_workers=4)\n        x_val, y_val = next(iter(val_loader))\n    elif 'cifar10' in args.domain:\n        data_dir = './dataset'\n        transform = transforms.Compose([transforms.ToTensor()])\n        val_data = data.cifar10_dataset_sub(data_dir, transform=transform,\n                                            num_sub=args.num_sub, data_seed=args.data_seed)\n        n_samples = len(val_data)\n        val_loader = DataLoader(val_data, batch_size=n_samples, shuffle=False, pin_memory=True, num_workers=4)\n        x_val, y_val = next(iter(val_loader))\n    elif 'celebahq' in args.domain:\n        data_dir = './dataset/celebahq'\n        attribute = args.classifier_name.split('__')[-1]  # `celebahq__Smiling`\n        val_transform = data.get_transform('celebahq', 'imval')\n        clean_dset = data.get_dataset('celebahq', 'val', attribute, root=data_dir, transform=val_transform,\n                                      fraction=2, data_seed=args.data_seed)  # data_seed randomizes here\n        loader = DataLoader(clean_dset, batch_size=adv_batch_size, shuffle=False,\n                            pin_memory=True, num_workers=4)\n        x_val, y_val = next(iter(loader))  # [0, 1], 256x256\n    else:\n        raise NotImplementedError(f'Unknown domain: {args.domain}!')\n\n    print(f'x_val shape: {x_val.shape}')\n    x_val, y_val = x_val.contiguous().requires_grad_(True), y_val.contiguous()\n    print(f'x (min, max): ({x_val.min()}, {x_val.max()})')\n\n    return x_val, y_val\n"}
{"type": "source_file", "path": "Image_Purifier/image_purifier.py", "content": "import argparse\nimport logging\nimport yaml\nimport os\nimport time\n\nimport random\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom PIL import Image\nfrom torchvision.utils import save_image\nfrom torchvision import transforms\nfrom torchvision.transforms.transforms import Resize\n\n\nimport utils\nfrom utils import str2bool, get_accuracy, get_image_classifier, load_data\n\nfrom runners.diffpure_guided import GuidedDiffusion\n\nclass SDE_Adv_Model(nn.Module):\n    def __init__(self, args, config):\n        super().__init__()\n        self.args = args\n\n        # image classifier\n        #self.classifier = get_image_classifier(args.classifier_name).to(config.device)\n\n        # diffusion model\n        print(f'diffusion_type: {args.diffusion_type}')\n        if args.diffusion_type == 'ddpm':\n            self.runner = GuidedDiffusion(args, config, device=config.device)\n        else:\n            raise NotImplementedError('unknown diffusion type')\n\n        # use `counter` to record the the sampling time every 5 NFEs (note we hardcoded print freq to 5,\n        # and you may want to change the freq)\n        self.register_buffer('counter', torch.zeros(1, device=config.device))\n        self.tag = None\n\n    def reset_counter(self):\n        self.counter = torch.zeros(1, dtype=torch.int, device=config.device)\n\n    def set_tag(self, tag=None):\n        self.tag = tag\n\n    def forward(self, x):\n        counter = self.counter.item()\n        if counter % 5 == 0:\n            print(f'diffusion times: {counter}')\n\n        # imagenet [3, 224, 224] -> [3, 256, 256] -> [3, 224, 224]\n        if 'imagenet' in self.args.domain:\n            x = F.interpolate(x, size=(256, 256), mode='bilinear', align_corners=False)\n\n        x_re = self.runner.image_editing_sample((x - 0.5) * 2, bs_id=counter, tag=self.tag)\n\n        if 'imagenet' in self.args.domain:\n            x_re = F.interpolate(x_re, size=(224, 224), mode='bilinear', align_corners=False)\n\n        if counter % 5 == 0:\n            print(f'x shape (before diffusion models): {x.shape}')\n            print(f'x shape (before classifier): {x_re.shape}')\n            print(\"Sampling time per batch: {:0>2}:{:05.2f}\".format(int(minutes), seconds))\n\n        out = (x_re + 1) * 0.5\n\n        self.counter += 1\n\n        return out\n    \n    \ndef parse_args_and_config():\n    parser = argparse.ArgumentParser(description=\"Image Purifier\")\n    parser.add_argument('--config', type=str, required=True, help='Path to the config file')\n    parser.add_argument('--data_seed', type=int, default=0, help='Random seed')\n    parser.add_argument('--seed', type=int, default=1234, help='Random seed')\n    parser.add_argument('--exp', type=str, default='exp', help='Path for saving running related data.')\n    parser.add_argument('--verbose', type=str, default='info', help='Verbose level: info | debug | warning | critical')\n    parser.add_argument('-i', '--image_folder', type=str, default='images', help=\"The folder name of samples\")\n    parser.add_argument('--ni', action='store_true', help=\"No interaction. Suitable for Slurm Job launcher\")\n    parser.add_argument('--sample_step', type=int, default=1, help='Total sampling steps')\n    parser.add_argument('--t', type=int, default=400, help='Sampling noise scale')\n    parser.add_argument('--t_delta', type=int, default=15, help='Perturbation range of sampling noise scale')\n    parser.add_argument('--rand_t', type=str2bool, default=False, help='Decide if randomize sampling noise scale')\n    parser.add_argument('--diffusion_type', type=str, default='ddpm', help='[ddpm, sde]')\n    parser.add_argument('--score_type', type=str, default='guided_diffusion', help='[guided_diffusion, score_sde]')\n    parser.add_argument('--eot_iter', type=int, default=20, help='only for rand version of autoattack')\n    parser.add_argument('--use_bm', action='store_true', help='whether to use brownian motion')\n\n    # LDSDE\n    parser.add_argument('--sigma2', type=float, default=1e-3, help='LDSDE sigma2')\n    parser.add_argument('--lambda_ld', type=float, default=1e-2, help='lambda_ld')\n    parser.add_argument('--eta', type=float, default=5., help='LDSDE eta')\n    parser.add_argument('--step_size', type=float, default=1e-3, help='step size for ODE Euler method')\n\n    # adv\n    parser.add_argument('--domain', type=str, default='imagenet', help='imagenet')\n    parser.add_argument('--classifier_name', type=str, default='Eyeglasses', help='which classifier to use')\n    parser.add_argument('--partition', type=str, default='val')\n    parser.add_argument('--adv_batch_size', type=int, default=64)\n    parser.add_argument('--attack_type', type=str, default='square')\n    parser.add_argument('--lp_norm', type=str, default='Linf', choices=['Linf', 'L2'])\n    parser.add_argument('--attack_version', type=str, default='custom')\n\n    parser.add_argument('--num_sub', type=int, default=1000, help='imagenet subset')\n    parser.add_argument('--adv_eps', type=float, default=0.07)\n    parser.add_argument('--log_dir', type=str, required=True)\n\n    args = parser.parse_args()\n\n    # parse config file\n    config_absolute_path = '/path/to/Image_Purifier'\n    with open(os.path.join(config_absolute_path, args.config), 'r') as f:\n        config = yaml.safe_load(f)\n    new_config = utils.dict2namespace(config)\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    new_config.device = device\n\n    # set random seed\n    torch.manual_seed(args.seed)\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(args.seed)\n\n    torch.backends.cudnn.benchmark = True\n    \n    \n    return args, new_config\n\n\nimg = Image.open('/path/to/the/jailbreak_image')\n\n\ndef diffpure(args, config):\n    model = SDE_Adv_Model(args, config)\n    model = model.eval().to(config.device)\n    print('Model loaded!')\n    image_size = config.model.image_size\n    mean = [0.485, 0.456, 0.406]\n    std = [0.229, 0.224, 0.225]\n    transform = transforms.Compose([\n        transforms.Resize((image_size, image_size)),\n        transforms.ToTensor(),\n        #transforms.Normalize(mean, std)\n    ])\n    image = transform(img).unsqueeze(0)\n    print('Processing image!')\n    with torch.no_grad():\n        output_image = model(image)\n    transform_back = transforms.ToPILImage()\n    image = transform_back(output_image.squeeze(0))\n    image.save('/path/to/the/purified_image')\n    print('Finished!')\n    \n    \n    \n    \nif __name__ == '__main__':\n    args, config = parse_args_and_config()\n    # os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu_ids\n    diffpure(args, config)\n"}
{"type": "source_file", "path": "Image_Purifier/score_sde/models/ema.py", "content": "# ---------------------------------------------------------------\n# Taken from the following link as is from:\n# https://github.com/yang-song/score_sde_pytorch/blob/main/models/ema.py\n#\n# The license for the original version of this file can be\n# found in the `score_sde` directory (LICENSE_SCORE_SDE).\n# ---------------------------------------------------------------\n\n# Modified from https://raw.githubusercontent.com/fadel/pytorch_ema/master/torch_ema/ema.py\n\nfrom __future__ import division\nfrom __future__ import unicode_literals\n\nimport torch\n\n\n# Partially based on: https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/training/moving_averages.py\nclass ExponentialMovingAverage:\n  \"\"\"\n  Maintains (exponential) moving average of a set of parameters.\n  \"\"\"\n\n  def __init__(self, parameters, decay, use_num_updates=True):\n    \"\"\"\n    Args:\n      parameters: Iterable of `torch.nn.Parameter`; usually the result of\n        `model.parameters()`.\n      decay: The exponential decay.\n      use_num_updates: Whether to use number of updates when computing\n        averages.\n    \"\"\"\n    if decay < 0.0 or decay > 1.0:\n      raise ValueError('Decay must be between 0 and 1')\n    self.decay = decay\n    self.num_updates = 0 if use_num_updates else None\n    self.shadow_params = [p.clone().detach()\n                          for p in parameters if p.requires_grad]\n    self.collected_params = []\n\n  def update(self, parameters):\n    \"\"\"\n    Update currently maintained parameters.\n\n    Call this every time the parameters are updated, such as the result of\n    the `optimizer.step()` call.\n\n    Args:\n      parameters: Iterable of `torch.nn.Parameter`; usually the same set of\n        parameters used to initialize this object.\n    \"\"\"\n    decay = self.decay\n    if self.num_updates is not None:\n      self.num_updates += 1\n      decay = min(decay, (1 + self.num_updates) / (10 + self.num_updates))\n    one_minus_decay = 1.0 - decay\n    with torch.no_grad():\n      parameters = [p for p in parameters if p.requires_grad]\n      for s_param, param in zip(self.shadow_params, parameters):\n        s_param.sub_(one_minus_decay * (s_param - param))\n\n  def copy_to(self, parameters):\n    \"\"\"\n    Copy current parameters into given collection of parameters.\n\n    Args:\n      parameters: Iterable of `torch.nn.Parameter`; the parameters to be\n        updated with the stored moving averages.\n    \"\"\"\n    parameters = [p for p in parameters if p.requires_grad]\n    for s_param, param in zip(self.shadow_params, parameters):\n      if param.requires_grad:\n        param.data.copy_(s_param.data)\n\n  def store(self, parameters):\n    \"\"\"\n    Save the current parameters for restoring later.\n\n    Args:\n      parameters: Iterable of `torch.nn.Parameter`; the parameters to be\n        temporarily stored.\n    \"\"\"\n    self.collected_params = [param.clone() for param in parameters]\n\n  def restore(self, parameters):\n    \"\"\"\n    Restore the parameters stored with the `store` method.\n    Useful to validate the model with EMA parameters without affecting the\n    original optimization process. Store the parameters before the\n    `copy_to` method. After validation (or model saving), use this to\n    restore the former parameters.\n\n    Args:\n      parameters: Iterable of `torch.nn.Parameter`; the parameters to be\n        updated with the stored parameters.\n    \"\"\"\n    for c_param, param in zip(self.collected_params, parameters):\n      param.data.copy_(c_param.data)\n\n  def state_dict(self):\n    return dict(decay=self.decay, num_updates=self.num_updates,\n                shadow_params=self.shadow_params)\n\n  def load_state_dict(self, state_dict):\n    self.decay = state_dict['decay']\n    self.num_updates = state_dict['num_updates']\n    self.shadow_params = state_dict['shadow_params']"}
{"type": "source_file", "path": "Image_Purifier/score_sde/models/ncsnpp.py", "content": "# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# pylint: skip-file\n\nfrom . import utils, layers, layerspp, normalization\nimport torch.nn as nn\nimport functools\nimport torch\nimport numpy as np\n\nResnetBlockDDPM = layerspp.ResnetBlockDDPMpp\nResnetBlockBigGAN = layerspp.ResnetBlockBigGANpp\nCombine = layerspp.Combine\nconv3x3 = layerspp.conv3x3\nconv1x1 = layerspp.conv1x1\nget_act = layers.get_act\nget_normalization = normalization.get_normalization\ndefault_initializer = layers.default_init\n\n\n@utils.register_model(name='ncsnpp')\nclass NCSNpp(nn.Module):\n  \"\"\"NCSN++ model\"\"\"\n\n  def __init__(self, config):\n    super().__init__()\n    self.config = config\n    self.act = act = get_act(config)\n    self.register_buffer('sigmas', torch.tensor(utils.get_sigmas(config)))\n\n    self.nf = nf = config.model.nf\n    ch_mult = config.model.ch_mult\n    self.num_res_blocks = num_res_blocks = config.model.num_res_blocks\n    self.attn_resolutions = attn_resolutions = config.model.attn_resolutions\n    dropout = config.model.dropout\n    resamp_with_conv = config.model.resamp_with_conv\n    self.num_resolutions = num_resolutions = len(ch_mult)\n    self.all_resolutions = all_resolutions = [config.data.image_size // (2 ** i) for i in range(num_resolutions)]\n\n    self.conditional = conditional = config.model.conditional  # noise-conditional\n    fir = config.model.fir\n    fir_kernel = config.model.fir_kernel\n    self.skip_rescale = skip_rescale = config.model.skip_rescale\n    self.resblock_type = resblock_type = config.model.resblock_type.lower()\n    self.progressive = progressive = config.model.progressive.lower()\n    self.progressive_input = progressive_input = config.model.progressive_input.lower()\n    self.embedding_type = embedding_type = config.model.embedding_type.lower()\n    init_scale = config.model.init_scale\n    assert progressive in ['none', 'output_skip', 'residual']\n    assert progressive_input in ['none', 'input_skip', 'residual']\n    assert embedding_type in ['fourier', 'positional']\n    combine_method = config.model.progressive_combine.lower()\n    combiner = functools.partial(Combine, method=combine_method)\n\n    modules = []\n    # timestep/noise_level embedding; only for continuous training\n    if embedding_type == 'fourier':\n      # Gaussian Fourier features embeddings.\n      assert config.training.continuous, \"Fourier features are only used for continuous training.\"\n\n      modules.append(layerspp.GaussianFourierProjection(\n        embedding_size=nf, scale=config.model.fourier_scale\n      ))\n      embed_dim = 2 * nf\n\n    elif embedding_type == 'positional':\n      embed_dim = nf\n\n    else:\n      raise ValueError(f'embedding type {embedding_type} unknown.')\n\n    if conditional:\n      modules.append(nn.Linear(embed_dim, nf * 4))\n      modules[-1].weight.data = default_initializer()(modules[-1].weight.shape)\n      nn.init.zeros_(modules[-1].bias)\n      modules.append(nn.Linear(nf * 4, nf * 4))\n      modules[-1].weight.data = default_initializer()(modules[-1].weight.shape)\n      nn.init.zeros_(modules[-1].bias)\n\n    AttnBlock = functools.partial(layerspp.AttnBlockpp,\n                                  init_scale=init_scale,\n                                  skip_rescale=skip_rescale)\n\n    Upsample = functools.partial(layerspp.Upsample,\n                                 with_conv=resamp_with_conv, fir=fir, fir_kernel=fir_kernel)\n\n    if progressive == 'output_skip':\n      self.pyramid_upsample = layerspp.Upsample(fir=fir, fir_kernel=fir_kernel, with_conv=False)\n    elif progressive == 'residual':\n      pyramid_upsample = functools.partial(layerspp.Upsample,\n                                           fir=fir, fir_kernel=fir_kernel, with_conv=True)\n\n    Downsample = functools.partial(layerspp.Downsample,\n                                   with_conv=resamp_with_conv, fir=fir, fir_kernel=fir_kernel)\n\n    if progressive_input == 'input_skip':\n      self.pyramid_downsample = layerspp.Downsample(fir=fir, fir_kernel=fir_kernel, with_conv=False)\n    elif progressive_input == 'residual':\n      pyramid_downsample = functools.partial(layerspp.Downsample,\n                                             fir=fir, fir_kernel=fir_kernel, with_conv=True)\n\n    if resblock_type == 'ddpm':\n      ResnetBlock = functools.partial(ResnetBlockDDPM,\n                                      act=act,\n                                      dropout=dropout,\n                                      init_scale=init_scale,\n                                      skip_rescale=skip_rescale,\n                                      temb_dim=nf * 4)\n\n    elif resblock_type == 'biggan':\n      ResnetBlock = functools.partial(ResnetBlockBigGAN,\n                                      act=act,\n                                      dropout=dropout,\n                                      fir=fir,\n                                      fir_kernel=fir_kernel,\n                                      init_scale=init_scale,\n                                      skip_rescale=skip_rescale,\n                                      temb_dim=nf * 4)\n\n    else:\n      raise ValueError(f'resblock type {resblock_type} unrecognized.')\n\n    # Downsampling block\n\n    channels = config.data.num_channels\n    if progressive_input != 'none':\n      input_pyramid_ch = channels\n\n    modules.append(conv3x3(channels, nf))\n    hs_c = [nf]\n\n    in_ch = nf\n    for i_level in range(num_resolutions):\n      # Residual blocks for this resolution\n      for i_block in range(num_res_blocks):\n        out_ch = nf * ch_mult[i_level]\n        modules.append(ResnetBlock(in_ch=in_ch, out_ch=out_ch))\n        in_ch = out_ch\n\n        if all_resolutions[i_level] in attn_resolutions:\n          modules.append(AttnBlock(channels=in_ch))\n        hs_c.append(in_ch)\n\n      if i_level != num_resolutions - 1:\n        if resblock_type == 'ddpm':\n          modules.append(Downsample(in_ch=in_ch))\n        else:\n          modules.append(ResnetBlock(down=True, in_ch=in_ch))\n\n        if progressive_input == 'input_skip':\n          modules.append(combiner(dim1=input_pyramid_ch, dim2=in_ch))\n          if combine_method == 'cat':\n            in_ch *= 2\n\n        elif progressive_input == 'residual':\n          modules.append(pyramid_downsample(in_ch=input_pyramid_ch, out_ch=in_ch))\n          input_pyramid_ch = in_ch\n\n        hs_c.append(in_ch)\n\n    in_ch = hs_c[-1]\n    modules.append(ResnetBlock(in_ch=in_ch))\n    modules.append(AttnBlock(channels=in_ch))\n    modules.append(ResnetBlock(in_ch=in_ch))\n\n    pyramid_ch = 0\n    # Upsampling block\n    for i_level in reversed(range(num_resolutions)):\n      for i_block in range(num_res_blocks + 1):\n        out_ch = nf * ch_mult[i_level]\n        modules.append(ResnetBlock(in_ch=in_ch + hs_c.pop(),\n                                   out_ch=out_ch))\n        in_ch = out_ch\n\n      if all_resolutions[i_level] in attn_resolutions:\n        modules.append(AttnBlock(channels=in_ch))\n\n      if progressive != 'none':\n        if i_level == num_resolutions - 1:\n          if progressive == 'output_skip':\n            modules.append(nn.GroupNorm(num_groups=min(in_ch // 4, 32),\n                                        num_channels=in_ch, eps=1e-6))\n            modules.append(conv3x3(in_ch, channels, init_scale=init_scale))\n            pyramid_ch = channels\n          elif progressive == 'residual':\n            modules.append(nn.GroupNorm(num_groups=min(in_ch // 4, 32),\n                                        num_channels=in_ch, eps=1e-6))\n            modules.append(conv3x3(in_ch, in_ch, bias=True))\n            pyramid_ch = in_ch\n          else:\n            raise ValueError(f'{progressive} is not a valid name.')\n        else:\n          if progressive == 'output_skip':\n            modules.append(nn.GroupNorm(num_groups=min(in_ch // 4, 32),\n                                        num_channels=in_ch, eps=1e-6))\n            modules.append(conv3x3(in_ch, channels, bias=True, init_scale=init_scale))\n            pyramid_ch = channels\n          elif progressive == 'residual':\n            modules.append(pyramid_upsample(in_ch=pyramid_ch, out_ch=in_ch))\n            pyramid_ch = in_ch\n          else:\n            raise ValueError(f'{progressive} is not a valid name')\n\n      if i_level != 0:\n        if resblock_type == 'ddpm':\n          modules.append(Upsample(in_ch=in_ch))\n        else:\n          modules.append(ResnetBlock(in_ch=in_ch, up=True))\n\n    assert not hs_c\n\n    if progressive != 'output_skip':\n      modules.append(nn.GroupNorm(num_groups=min(in_ch // 4, 32),\n                                  num_channels=in_ch, eps=1e-6))\n      modules.append(conv3x3(in_ch, channels, init_scale=init_scale))\n\n    self.all_modules = nn.ModuleList(modules)\n\n  def forward(self, x, time_cond):\n    # timestep/noise_level embedding; only for continuous training\n    modules = self.all_modules\n    m_idx = 0\n    if self.embedding_type == 'fourier':\n      # Gaussian Fourier features embeddings.\n      used_sigmas = time_cond\n      temb = modules[m_idx](torch.log(used_sigmas))\n      m_idx += 1\n\n    elif self.embedding_type == 'positional':\n      # Sinusoidal positional embeddings.\n      timesteps = time_cond\n      used_sigmas = self.sigmas[time_cond.long()]\n      temb = layers.get_timestep_embedding(timesteps, self.nf)\n\n    else:\n      raise ValueError(f'embedding type {self.embedding_type} unknown.')\n\n    if self.conditional:\n      temb = modules[m_idx](temb)\n      m_idx += 1\n      temb = modules[m_idx](self.act(temb))\n      m_idx += 1\n    else:\n      temb = None\n\n    if not self.config.data.centered:\n      # If input data is in [0, 1]\n      x = 2 * x - 1.\n\n    # Downsampling block\n    input_pyramid = None\n    if self.progressive_input != 'none':\n      input_pyramid = x\n\n    hs = [modules[m_idx](x)]\n    m_idx += 1\n    for i_level in range(self.num_resolutions):\n      # Residual blocks for this resolution\n      for i_block in range(self.num_res_blocks):\n        h = modules[m_idx](hs[-1], temb)\n        m_idx += 1\n        if h.shape[-1] in self.attn_resolutions:\n          h = modules[m_idx](h)\n          m_idx += 1\n\n        hs.append(h)\n\n      if i_level != self.num_resolutions - 1:\n        if self.resblock_type == 'ddpm':\n          h = modules[m_idx](hs[-1])\n          m_idx += 1\n        else:\n          h = modules[m_idx](hs[-1], temb)\n          m_idx += 1\n\n        if self.progressive_input == 'input_skip':\n          input_pyramid = self.pyramid_downsample(input_pyramid)\n          h = modules[m_idx](input_pyramid, h)\n          m_idx += 1\n\n        elif self.progressive_input == 'residual':\n          input_pyramid = modules[m_idx](input_pyramid)\n          m_idx += 1\n          if self.skip_rescale:\n            input_pyramid = (input_pyramid + h) / np.sqrt(2.)\n          else:\n            input_pyramid = input_pyramid + h\n          h = input_pyramid\n\n        hs.append(h)\n\n    h = hs[-1]\n    h = modules[m_idx](h, temb)\n    m_idx += 1\n    h = modules[m_idx](h)\n    m_idx += 1\n    h = modules[m_idx](h, temb)\n    m_idx += 1\n\n    pyramid = None\n\n    # Upsampling block\n    for i_level in reversed(range(self.num_resolutions)):\n      for i_block in range(self.num_res_blocks + 1):\n        h = modules[m_idx](torch.cat([h, hs.pop()], dim=1), temb)\n        m_idx += 1\n\n      if h.shape[-1] in self.attn_resolutions:\n        h = modules[m_idx](h)\n        m_idx += 1\n\n      if self.progressive != 'none':\n        if i_level == self.num_resolutions - 1:\n          if self.progressive == 'output_skip':\n            pyramid = self.act(modules[m_idx](h))\n            m_idx += 1\n            pyramid = modules[m_idx](pyramid)\n            m_idx += 1\n          elif self.progressive == 'residual':\n            pyramid = self.act(modules[m_idx](h))\n            m_idx += 1\n            pyramid = modules[m_idx](pyramid)\n            m_idx += 1\n          else:\n            raise ValueError(f'{self.progressive} is not a valid name.')\n        else:\n          if self.progressive == 'output_skip':\n            pyramid = self.pyramid_upsample(pyramid)\n            pyramid_h = self.act(modules[m_idx](h))\n            m_idx += 1\n            pyramid_h = modules[m_idx](pyramid_h)\n            m_idx += 1\n            pyramid = pyramid + pyramid_h\n          elif self.progressive == 'residual':\n            pyramid = modules[m_idx](pyramid)\n            m_idx += 1\n            if self.skip_rescale:\n              pyramid = (pyramid + h) / np.sqrt(2.)\n            else:\n              pyramid = pyramid + h\n            h = pyramid\n          else:\n            raise ValueError(f'{self.progressive} is not a valid name')\n\n      if i_level != 0:\n        if self.resblock_type == 'ddpm':\n          h = modules[m_idx](h)\n          m_idx += 1\n        else:\n          h = modules[m_idx](h, temb)\n          m_idx += 1\n\n    assert not hs\n\n    if self.progressive == 'output_skip':\n      h = pyramid\n    else:\n      h = self.act(modules[m_idx](h))\n      m_idx += 1\n      h = modules[m_idx](h)\n      m_idx += 1\n\n    assert m_idx == len(modules)\n    if self.config.model.scale_by_sigma:\n      used_sigmas = used_sigmas.reshape((x.shape[0], *([1] * len(x.shape[1:]))))\n      h = h / used_sigmas\n\n    return h\n"}
{"type": "source_file", "path": "Image_Purifier/score_sde/models/utils.py", "content": "# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"All functions and modules related to model definition.\n\"\"\"\n\nimport torch\nfrom score_sde import sde_lib\nimport numpy as np\n\n_MODELS = {}\n\n\ndef register_model(cls=None, *, name=None):\n    \"\"\"A decorator for registering model classes.\"\"\"\n\n    def _register(cls):\n        if name is None:\n            local_name = cls.__name__\n        else:\n            local_name = name\n        if local_name in _MODELS:\n            raise ValueError(f'Already registered model with name: {local_name}')\n        _MODELS[local_name] = cls\n        return cls\n\n    if cls is None:\n        return _register\n    else:\n        return _register(cls)\n\n\ndef get_model(name):\n    return _MODELS[name]\n\n\ndef get_sigmas(config):\n    \"\"\"Get sigmas --- the set of noise levels for SMLD from config files.\n    Args:\n      config: A ConfigDict object parsed from the config file\n    Returns:\n      sigmas: a jax numpy arrary of noise levels\n    \"\"\"\n    sigmas = np.exp(\n        np.linspace(np.log(config.model.sigma_max), np.log(config.model.sigma_min), config.model.num_scales))\n\n    return sigmas\n\n\ndef get_ddpm_params(config):\n    \"\"\"Get betas and alphas --- parameters used in the original DDPM paper.\"\"\"\n    num_diffusion_timesteps = 1000\n    # parameters need to be adapted if number of time steps differs from 1000\n    beta_start = config.model.beta_min / config.model.num_scales\n    beta_end = config.model.beta_max / config.model.num_scales\n    betas = np.linspace(beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64)\n\n    alphas = 1. - betas\n    alphas_cumprod = np.cumprod(alphas, axis=0)\n    sqrt_alphas_cumprod = np.sqrt(alphas_cumprod)\n    sqrt_1m_alphas_cumprod = np.sqrt(1. - alphas_cumprod)\n\n    return {\n        'betas': betas,\n        'alphas': alphas,\n        'alphas_cumprod': alphas_cumprod,\n        'sqrt_alphas_cumprod': sqrt_alphas_cumprod,\n        'sqrt_1m_alphas_cumprod': sqrt_1m_alphas_cumprod,\n        'beta_min': beta_start * (num_diffusion_timesteps - 1),\n        'beta_max': beta_end * (num_diffusion_timesteps - 1),\n        'num_diffusion_timesteps': num_diffusion_timesteps\n    }\n\n\ndef create_model(config):\n    \"\"\"Create the score model.\"\"\"\n    model_name = config.model.name\n    score_model = get_model(model_name)(config)\n    # score_model = score_model.to(config.device)\n    # score_model = torch.nn.DataParallel(score_model)\n    return score_model\n\n\ndef get_model_fn(model, train=False):\n    \"\"\"Create a function to give the output of the score-based model.\n\n    Args:\n      model: The score model.\n      train: `True` for training and `False` for evaluation.\n\n    Returns:\n      A model function.\n    \"\"\"\n\n    def model_fn(x, labels):\n        \"\"\"Compute the output of the score-based model.\n\n        Args:\n          x: A mini-batch of input data.\n          labels: A mini-batch of conditioning variables for time steps. Should be interpreted differently\n            for different models.\n\n        Returns:\n          A tuple of (model output, new mutable states)\n        \"\"\"\n        if not train:\n            model.eval()\n            return model(x, labels)\n        else:\n            model.train()\n            return model(x, labels)\n\n    return model_fn\n\n\ndef get_score_fn(sde, model, train=False, continuous=False):\n    \"\"\"Wraps `score_fn` so that the model output corresponds to a real time-dependent score function.\n\n    Args:\n      sde: An `sde_lib.SDE` object that represents the forward SDE.\n      model: A score model.\n      train: `True` for training and `False` for evaluation.\n      continuous: If `True`, the score-based model is expected to directly take continuous time steps.\n\n    Returns:\n      A score function.\n    \"\"\"\n    model_fn = get_model_fn(model, train=train)\n\n    if isinstance(sde, sde_lib.VPSDE) or isinstance(sde, sde_lib.subVPSDE):\n        def score_fn(x, t):\n            # Scale neural network output by standard deviation and flip sign\n            if continuous or isinstance(sde, sde_lib.subVPSDE):\n                # For VP-trained models, t=0 corresponds to the lowest noise level\n                # The maximum value of time embedding is assumed to 999 for\n                # continuously-trained models.\n                labels = t * 999\n                score = model_fn(x, labels)\n                std = sde.marginal_prob(torch.zeros_like(x), t)[1]\n            else:\n                # For VP-trained models, t=0 corresponds to the lowest noise level\n                labels = t * (sde.N - 1)\n                score = model_fn(x, labels)\n                std = sde.sqrt_1m_alphas_cumprod.to(labels.device)[labels.long()]\n\n            score = -score / std[:, None, None, None]\n            return score\n\n    elif isinstance(sde, sde_lib.VESDE):\n        def score_fn(x, t):\n            if continuous:\n                labels = sde.marginal_prob(torch.zeros_like(x), t)[1]\n            else:\n                # For VE-trained models, t=0 corresponds to the highest noise level\n                labels = sde.T - t\n                labels *= sde.N - 1\n                labels = torch.round(labels).long()\n\n            score = model_fn(x, labels)\n            return score\n\n    else:\n        raise NotImplementedError(f\"SDE class {sde.__class__.__name__} not yet supported.\")\n\n    return score_fn\n\n\ndef to_flattened_numpy(x):\n    \"\"\"Flatten a torch tensor `x` and convert it to numpy.\"\"\"\n    return x.detach().cpu().numpy().reshape((-1,))\n\n\ndef from_flattened_numpy(x, shape):\n    \"\"\"Form a torch tensor with the given `shape` from a flattened numpy array `x`.\"\"\"\n    return torch.from_numpy(x.reshape(shape))\n"}
{"type": "source_file", "path": "LLaVA/llava/constants.py", "content": "CONTROLLER_HEART_BEAT_EXPIRATION = 30\nWORKER_HEART_BEAT_INTERVAL = 15\n\nLOGDIR = \".\"\n\n# Model Constants\nIGNORE_INDEX = -100\nIMAGE_TOKEN_INDEX = -200\nDEFAULT_IMAGE_TOKEN = \"<image>\"\nDEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\nDEFAULT_IM_START_TOKEN = \"<im_start>\"\nDEFAULT_IM_END_TOKEN = \"<im_end>\"\nIMAGE_PLACEHOLDER = \"<image-placeholder>\"\n"}
{"type": "source_file", "path": "Image_Purifier/score_sde/sampling.py", "content": "# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# pylint: skip-file\n# pytype: skip-file\n\"\"\"Various sampling methods.\"\"\"\nimport functools\n\nimport torch\nimport numpy as np\nimport abc\n\nfrom .models.utils import from_flattened_numpy, to_flattened_numpy, get_score_fn\nfrom scipy import integrate\nfrom . import sde_lib\nfrom .models import utils as mutils\n\n_CORRECTORS = {}\n_PREDICTORS = {}\n\n\ndef register_predictor(cls=None, *, name=None):\n  \"\"\"A decorator for registering predictor classes.\"\"\"\n\n  def _register(cls):\n    if name is None:\n      local_name = cls.__name__\n    else:\n      local_name = name\n    if local_name in _PREDICTORS:\n      raise ValueError(f'Already registered model with name: {local_name}')\n    _PREDICTORS[local_name] = cls\n    return cls\n\n  if cls is None:\n    return _register\n  else:\n    return _register(cls)\n\n\ndef register_corrector(cls=None, *, name=None):\n  \"\"\"A decorator for registering corrector classes.\"\"\"\n\n  def _register(cls):\n    if name is None:\n      local_name = cls.__name__\n    else:\n      local_name = name\n    if local_name in _CORRECTORS:\n      raise ValueError(f'Already registered model with name: {local_name}')\n    _CORRECTORS[local_name] = cls\n    return cls\n\n  if cls is None:\n    return _register\n  else:\n    return _register(cls)\n\n\ndef get_predictor(name):\n  return _PREDICTORS[name]\n\n\ndef get_corrector(name):\n  return _CORRECTORS[name]\n\n\ndef get_sampling_fn(config, sde, shape, inverse_scaler, eps):\n  \"\"\"Create a sampling function.\n\n  Args:\n    config: A `ml_collections.ConfigDict` object that contains all configuration information.\n    sde: A `sde_lib.SDE` object that represents the forward SDE.\n    shape: A sequence of integers representing the expected shape of a single sample.\n    inverse_scaler: The inverse data normalizer function.\n    eps: A `float` number. The reverse-time SDE is only integrated to `eps` for numerical stability.\n\n  Returns:\n    A function that takes random states and a replicated training state and outputs samples with the\n      trailing dimensions matching `shape`.\n  \"\"\"\n\n  sampler_name = config.sampling.method\n  # Probability flow ODE sampling with black-box ODE solvers\n  if sampler_name.lower() == 'ode':\n    sampling_fn = get_ode_sampler(sde=sde,\n                                  shape=shape,\n                                  inverse_scaler=inverse_scaler,\n                                  denoise=config.sampling.noise_removal,\n                                  eps=eps,\n                                  device=config.device)\n  # Predictor-Corrector sampling. Predictor-only and Corrector-only samplers are special cases.\n  elif sampler_name.lower() == 'pc':\n    predictor = get_predictor(config.sampling.predictor.lower())\n    corrector = get_corrector(config.sampling.corrector.lower())\n    sampling_fn = get_pc_sampler(sde=sde,\n                                 shape=shape,\n                                 predictor=predictor,\n                                 corrector=corrector,\n                                 inverse_scaler=inverse_scaler,\n                                 snr=config.sampling.snr,\n                                 n_steps=config.sampling.n_steps_each,\n                                 probability_flow=config.sampling.probability_flow,\n                                 continuous=config.training.continuous,\n                                 denoise=config.sampling.noise_removal,\n                                 eps=eps,\n                                 device=config.device)\n  else:\n    raise ValueError(f\"Sampler name {sampler_name} unknown.\")\n\n  return sampling_fn\n\n\nclass Predictor(abc.ABC):\n  \"\"\"The abstract class for a predictor algorithm.\"\"\"\n\n  def __init__(self, sde, score_fn, probability_flow=False):\n    super().__init__()\n    self.sde = sde\n    # Compute the reverse SDE/ODE\n    self.rsde = sde.reverse(score_fn, probability_flow)\n    self.score_fn = score_fn\n\n  @abc.abstractmethod\n  def update_fn(self, x, t):\n    \"\"\"One update of the predictor.\n\n    Args:\n      x: A PyTorch tensor representing the current state\n      t: A Pytorch tensor representing the current time step.\n\n    Returns:\n      x: A PyTorch tensor of the next state.\n      x_mean: A PyTorch tensor. The next state without random noise. Useful for denoising.\n    \"\"\"\n    pass\n\n\nclass Corrector(abc.ABC):\n  \"\"\"The abstract class for a corrector algorithm.\"\"\"\n\n  def __init__(self, sde, score_fn, snr, n_steps):\n    super().__init__()\n    self.sde = sde\n    self.score_fn = score_fn\n    self.snr = snr\n    self.n_steps = n_steps\n\n  @abc.abstractmethod\n  def update_fn(self, x, t):\n    \"\"\"One update of the corrector.\n\n    Args:\n      x: A PyTorch tensor representing the current state\n      t: A PyTorch tensor representing the current time step.\n\n    Returns:\n      x: A PyTorch tensor of the next state.\n      x_mean: A PyTorch tensor. The next state without random noise. Useful for denoising.\n    \"\"\"\n    pass\n\n\n@register_predictor(name='euler_maruyama')\nclass EulerMaruyamaPredictor(Predictor):\n  def __init__(self, sde, score_fn, probability_flow=False):\n    super().__init__(sde, score_fn, probability_flow)\n\n  def update_fn(self, x, t):\n    dt = -1. / self.rsde.N\n    z = torch.randn_like(x)\n    drift, diffusion = self.rsde.sde(x, t)\n    x_mean = x + drift * dt\n    x = x_mean + diffusion[:, None, None, None] * np.sqrt(-dt) * z\n    return x, x_mean\n\n\n@register_predictor(name='reverse_diffusion')\nclass ReverseDiffusionPredictor(Predictor):\n  def __init__(self, sde, score_fn, probability_flow=False):\n    super().__init__(sde, score_fn, probability_flow)\n\n  def update_fn(self, x, t):\n    f, G = self.rsde.discretize(x, t)\n    z = torch.randn_like(x)\n    x_mean = x - f\n    x = x_mean + G[:, None, None, None] * z\n    return x, x_mean\n\n\n@register_predictor(name='ancestral_sampling')\nclass AncestralSamplingPredictor(Predictor):\n  \"\"\"The ancestral sampling predictor. Currently only supports VE/VP SDEs.\"\"\"\n\n  def __init__(self, sde, score_fn, probability_flow=False):\n    super().__init__(sde, score_fn, probability_flow)\n    if not isinstance(sde, sde_lib.VPSDE) and not isinstance(sde, sde_lib.VESDE):\n      raise NotImplementedError(f\"SDE class {sde.__class__.__name__} not yet supported.\")\n    assert not probability_flow, \"Probability flow not supported by ancestral sampling\"\n\n  def vesde_update_fn(self, x, t):\n    sde = self.sde\n    timestep = (t * (sde.N - 1) / sde.T).long()\n    sigma = sde.discrete_sigmas[timestep]\n    adjacent_sigma = torch.where(timestep == 0, torch.zeros_like(t), sde.discrete_sigmas.to(t.device)[timestep - 1])\n    score = self.score_fn(x, t)\n    x_mean = x + score * (sigma ** 2 - adjacent_sigma ** 2)[:, None, None, None]\n    std = torch.sqrt((adjacent_sigma ** 2 * (sigma ** 2 - adjacent_sigma ** 2)) / (sigma ** 2))\n    noise = torch.randn_like(x)\n    x = x_mean + std[:, None, None, None] * noise\n    return x, x_mean\n\n  def vpsde_update_fn(self, x, t):\n    sde = self.sde\n    timestep = (t * (sde.N - 1) / sde.T).long()\n    beta = sde.discrete_betas.to(t.device)[timestep]\n    score = self.score_fn(x, t)\n    x_mean = (x + beta[:, None, None, None] * score) / torch.sqrt(1. - beta)[:, None, None, None]\n    noise = torch.randn_like(x)\n    x = x_mean + torch.sqrt(beta)[:, None, None, None] * noise\n    return x, x_mean\n\n  def update_fn(self, x, t):\n    if isinstance(self.sde, sde_lib.VESDE):\n      return self.vesde_update_fn(x, t)\n    elif isinstance(self.sde, sde_lib.VPSDE):\n      return self.vpsde_update_fn(x, t)\n\n\n@register_predictor(name='none')\nclass NonePredictor(Predictor):\n  \"\"\"An empty predictor that does nothing.\"\"\"\n\n  def __init__(self, sde, score_fn, probability_flow=False):\n    pass\n\n  def update_fn(self, x, t):\n    return x, x\n\n\n@register_corrector(name='langevin')\nclass LangevinCorrector(Corrector):\n  def __init__(self, sde, score_fn, snr, n_steps):\n    super().__init__(sde, score_fn, snr, n_steps)\n    if not isinstance(sde, sde_lib.VPSDE) \\\n        and not isinstance(sde, sde_lib.VESDE) \\\n        and not isinstance(sde, sde_lib.subVPSDE):\n      raise NotImplementedError(f\"SDE class {sde.__class__.__name__} not yet supported.\")\n\n  def update_fn(self, x, t):\n    sde = self.sde\n    score_fn = self.score_fn\n    n_steps = self.n_steps\n    target_snr = self.snr\n    if isinstance(sde, sde_lib.VPSDE) or isinstance(sde, sde_lib.subVPSDE):\n      timestep = (t * (sde.N - 1) / sde.T).long()\n      alpha = sde.alphas.to(t.device)[timestep]\n    else:\n      alpha = torch.ones_like(t)\n\n    for i in range(n_steps):\n      grad = score_fn(x, t)\n      noise = torch.randn_like(x)\n      grad_norm = torch.norm(grad.reshape(grad.shape[0], -1), dim=-1).mean()\n      noise_norm = torch.norm(noise.reshape(noise.shape[0], -1), dim=-1).mean()\n      step_size = (target_snr * noise_norm / grad_norm) ** 2 * 2 * alpha\n      x_mean = x + step_size[:, None, None, None] * grad\n      x = x_mean + torch.sqrt(step_size * 2)[:, None, None, None] * noise\n\n    return x, x_mean\n\n\n@register_corrector(name='ald')\nclass AnnealedLangevinDynamics(Corrector):\n  \"\"\"The original annealed Langevin dynamics predictor in NCSN/NCSNv2.\n\n  We include this corrector only for completeness. It was not directly used in our paper.\n  \"\"\"\n\n  def __init__(self, sde, score_fn, snr, n_steps):\n    super().__init__(sde, score_fn, snr, n_steps)\n    if not isinstance(sde, sde_lib.VPSDE) \\\n        and not isinstance(sde, sde_lib.VESDE) \\\n        and not isinstance(sde, sde_lib.subVPSDE):\n      raise NotImplementedError(f\"SDE class {sde.__class__.__name__} not yet supported.\")\n\n  def update_fn(self, x, t):\n    sde = self.sde\n    score_fn = self.score_fn\n    n_steps = self.n_steps\n    target_snr = self.snr\n    if isinstance(sde, sde_lib.VPSDE) or isinstance(sde, sde_lib.subVPSDE):\n      timestep = (t * (sde.N - 1) / sde.T).long()\n      alpha = sde.alphas.to(t.device)[timestep]\n    else:\n      alpha = torch.ones_like(t)\n\n    std = self.sde.marginal_prob(x, t)[1]\n\n    for i in range(n_steps):\n      grad = score_fn(x, t)\n      noise = torch.randn_like(x)\n      step_size = (target_snr * std) ** 2 * 2 * alpha\n      x_mean = x + step_size[:, None, None, None] * grad\n      x = x_mean + noise * torch.sqrt(step_size * 2)[:, None, None, None]\n\n    return x, x_mean\n\n\n@register_corrector(name='none')\nclass NoneCorrector(Corrector):\n  \"\"\"An empty corrector that does nothing.\"\"\"\n\n  def __init__(self, sde, score_fn, snr, n_steps):\n    pass\n\n  def update_fn(self, x, t):\n    return x, x\n\n\ndef shared_predictor_update_fn(x, t, sde, model, predictor, probability_flow, continuous):\n  \"\"\"A wrapper that configures and returns the update function of predictors.\"\"\"\n  score_fn = mutils.get_score_fn(sde, model, train=False, continuous=continuous)\n  if predictor is None:\n    # Corrector-only sampler\n    predictor_obj = NonePredictor(sde, score_fn, probability_flow)\n  else:\n    predictor_obj = predictor(sde, score_fn, probability_flow)\n  return predictor_obj.update_fn(x, t)\n\n\ndef shared_corrector_update_fn(x, t, sde, model, corrector, continuous, snr, n_steps):\n  \"\"\"A wrapper tha configures and returns the update function of correctors.\"\"\"\n  score_fn = mutils.get_score_fn(sde, model, train=False, continuous=continuous)\n  if corrector is None:\n    # Predictor-only sampler\n    corrector_obj = NoneCorrector(sde, score_fn, snr, n_steps)\n  else:\n    corrector_obj = corrector(sde, score_fn, snr, n_steps)\n  return corrector_obj.update_fn(x, t)\n\n\ndef get_pc_sampler(sde, shape, predictor, corrector, inverse_scaler, snr,\n                   n_steps=1, probability_flow=False, continuous=False,\n                   denoise=True, eps=1e-3, device='cuda'):\n  \"\"\"Create a Predictor-Corrector (PC) sampler.\n\n  Args:\n    sde: An `sde_lib.SDE` object representing the forward SDE.\n    shape: A sequence of integers. The expected shape of a single sample.\n    predictor: A subclass of `sampling.Predictor` representing the predictor algorithm.\n    corrector: A subclass of `sampling.Corrector` representing the corrector algorithm.\n    inverse_scaler: The inverse data normalizer.\n    snr: A `float` number. The signal-to-noise ratio for configuring correctors.\n    n_steps: An integer. The number of corrector steps per predictor update.\n    probability_flow: If `True`, solve the reverse-time probability flow ODE when running the predictor.\n    continuous: `True` indicates that the score model was continuously trained.\n    denoise: If `True`, add one-step denoising to the final samples.\n    eps: A `float` number. The reverse-time SDE and ODE are integrated to `epsilon` to avoid numerical issues.\n    device: PyTorch device.\n\n  Returns:\n    A sampling function that returns samples and the number of function evaluations during sampling.\n  \"\"\"\n  # Create predictor & corrector update functions\n  predictor_update_fn = functools.partial(shared_predictor_update_fn,\n                                          sde=sde,\n                                          predictor=predictor,\n                                          probability_flow=probability_flow,\n                                          continuous=continuous)\n  corrector_update_fn = functools.partial(shared_corrector_update_fn,\n                                          sde=sde,\n                                          corrector=corrector,\n                                          continuous=continuous,\n                                          snr=snr,\n                                          n_steps=n_steps)\n\n  def pc_sampler(model):\n    \"\"\" The PC sampler funciton.\n\n    Args:\n      model: A score model.\n    Returns:\n      Samples, number of function evaluations.\n    \"\"\"\n    with torch.no_grad():\n      # Initial sample\n      x = sde.prior_sampling(shape).to(device)\n      timesteps = torch.linspace(sde.T, eps, sde.N, device=device)\n\n      for i in range(sde.N):\n        t = timesteps[i]\n        vec_t = torch.ones(shape[0], device=t.device) * t\n        x, x_mean = corrector_update_fn(x, vec_t, model=model)\n        x, x_mean = predictor_update_fn(x, vec_t, model=model)\n\n      return inverse_scaler(x_mean if denoise else x), sde.N * (n_steps + 1)\n\n  return pc_sampler\n\n\ndef get_ode_sampler(sde, shape, inverse_scaler,\n                    denoise=False, rtol=1e-5, atol=1e-5,\n                    method='RK45', eps=1e-3, device='cuda'):\n  \"\"\"Probability flow ODE sampler with the black-box ODE solver.\n\n  Args:\n    sde: An `sde_lib.SDE` object that represents the forward SDE.\n    shape: A sequence of integers. The expected shape of a single sample.\n    inverse_scaler: The inverse data normalizer.\n    denoise: If `True`, add one-step denoising to final samples.\n    rtol: A `float` number. The relative tolerance level of the ODE solver.\n    atol: A `float` number. The absolute tolerance level of the ODE solver.\n    method: A `str`. The algorithm used for the black-box ODE solver.\n      See the documentation of `scipy.integrate.solve_ivp`.\n    eps: A `float` number. The reverse-time SDE/ODE will be integrated to `eps` for numerical stability.\n    device: PyTorch device.\n\n  Returns:\n    A sampling function that returns samples and the number of function evaluations during sampling.\n  \"\"\"\n\n  def denoise_update_fn(model, x):\n    score_fn = get_score_fn(sde, model, train=False, continuous=True)\n    # Reverse diffusion predictor for denoising\n    predictor_obj = ReverseDiffusionPredictor(sde, score_fn, probability_flow=False)\n    vec_eps = torch.ones(x.shape[0], device=x.device) * eps\n    _, x = predictor_obj.update_fn(x, vec_eps)\n    return x\n\n  def drift_fn(model, x, t):\n    \"\"\"Get the drift function of the reverse-time SDE.\"\"\"\n    score_fn = get_score_fn(sde, model, train=False, continuous=True)\n    rsde = sde.reverse(score_fn, probability_flow=True)\n    return rsde.sde(x, t)[0]\n\n  def ode_sampler(model, z=None):\n    \"\"\"The probability flow ODE sampler with black-box ODE solver.\n\n    Args:\n      model: A score model.\n      z: If present, generate samples from latent code `z`.\n    Returns:\n      samples, number of function evaluations.\n    \"\"\"\n    with torch.no_grad():\n      # Initial sample\n      if z is None:\n        # If not represent, sample the latent code from the prior distibution of the SDE.\n        x = sde.prior_sampling(shape).to(device)\n      else:\n        x = z\n\n      def ode_func(t, x):\n        x = from_flattened_numpy(x, shape).to(device).type(torch.float32)\n        vec_t = torch.ones(shape[0], device=x.device) * t\n        drift = drift_fn(model, x, vec_t)\n        return to_flattened_numpy(drift)\n\n      # Black-box ODE solver for the probability flow ODE\n      solution = integrate.solve_ivp(ode_func, (sde.T, eps), to_flattened_numpy(x),\n                                     rtol=rtol, atol=atol, method=method)\n      nfe = solution.nfev\n      x = torch.tensor(solution.y[:, -1]).reshape(shape).to(device).type(torch.float32)\n\n      # Denoising is equivalent to running one predictor step without adding noise\n      if denoise:\n        x = denoise_update_fn(model, x)\n\n      x = inverse_scaler(x)\n      return x, nfe\n\n  return ode_sampler\n"}
{"type": "source_file", "path": "Image_Purifier/score_sde/losses.py", "content": "# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"All functions related to loss computation and optimization.\n\"\"\"\n\nimport torch\nimport torch.optim as optim\nimport numpy as np\nfrom .models import utils as mutils\nfrom .sde_lib import VESDE, VPSDE\n\n\ndef get_optimizer(config, params):\n  \"\"\"Returns a flax optimizer object based on `config`.\"\"\"\n  if config.optim.optimizer == 'Adam':\n    optimizer = optim.Adam(params, lr=config.optim.lr, betas=(config.optim.beta1, 0.999), eps=config.optim.eps,\n                           weight_decay=config.optim.weight_decay)\n  else:\n    raise NotImplementedError(\n      f'Optimizer {config.optim.optimizer} not supported yet!')\n\n  return optimizer\n\n\ndef optimization_manager(config):\n  \"\"\"Returns an optimize_fn based on `config`.\"\"\"\n\n  def optimize_fn(optimizer, params, step, lr=config.optim.lr,\n                  warmup=config.optim.warmup,\n                  grad_clip=config.optim.grad_clip):\n    \"\"\"Optimizes with warmup and gradient clipping (disabled if negative).\"\"\"\n    if warmup > 0:\n      for g in optimizer.param_groups:\n        g['lr'] = lr * np.minimum(step / warmup, 1.0)\n    if grad_clip >= 0:\n      torch.nn.utils.clip_grad_norm_(params, max_norm=grad_clip)\n    optimizer.step()\n\n  return optimize_fn\n\n\ndef get_sde_loss_fn(sde, train, reduce_mean=True, continuous=True, likelihood_weighting=True, eps=1e-5):\n  \"\"\"Create a loss function for training with arbirary SDEs.\n\n  Args:\n    sde: An `sde_lib.SDE` object that represents the forward SDE.\n    train: `True` for training loss and `False` for evaluation loss.\n    reduce_mean: If `True`, average the loss across data dimensions. Otherwise sum the loss across data dimensions.\n    continuous: `True` indicates that the model is defined to take continuous time steps. Otherwise it requires\n      ad-hoc interpolation to take continuous time steps.\n    likelihood_weighting: If `True`, weight the mixture of score matching losses\n      according to https://arxiv.org/abs/2101.09258; otherwise use the weighting recommended in our paper.\n    eps: A `float` number. The smallest time step to sample from.\n\n  Returns:\n    A loss function.\n  \"\"\"\n  reduce_op = torch.mean if reduce_mean else lambda *args, **kwargs: 0.5 * torch.sum(*args, **kwargs)\n\n  def loss_fn(model, batch):\n    \"\"\"Compute the loss function.\n\n    Args:\n      model: A score model.\n      batch: A mini-batch of training data.\n\n    Returns:\n      loss: A scalar that represents the average loss value across the mini-batch.\n    \"\"\"\n    score_fn = mutils.get_score_fn(sde, model, train=train, continuous=continuous)\n    t = torch.rand(batch.shape[0], device=batch.device) * (sde.T - eps) + eps\n    z = torch.randn_like(batch)\n    mean, std = sde.marginal_prob(batch, t)\n    perturbed_data = mean + std[:, None, None, None] * z\n    score = score_fn(perturbed_data, t)\n\n    if not likelihood_weighting:\n      losses = torch.square(score * std[:, None, None, None] + z)\n      losses = reduce_op(losses.reshape(losses.shape[0], -1), dim=-1)\n    else:\n      g2 = sde.sde(torch.zeros_like(batch), t)[1] ** 2\n      losses = torch.square(score + z / std[:, None, None, None])\n      losses = reduce_op(losses.reshape(losses.shape[0], -1), dim=-1) * g2\n\n    loss = torch.mean(losses)\n    return loss\n\n  return loss_fn\n\n\ndef get_smld_loss_fn(vesde, train, reduce_mean=False):\n  \"\"\"Legacy code to reproduce previous results on SMLD(NCSN). Not recommended for new work.\"\"\"\n  assert isinstance(vesde, VESDE), \"SMLD training only works for VESDEs.\"\n\n  # Previous SMLD models assume descending sigmas\n  smld_sigma_array = torch.flip(vesde.discrete_sigmas, dims=(0,))\n  reduce_op = torch.mean if reduce_mean else lambda *args, **kwargs: 0.5 * torch.sum(*args, **kwargs)\n\n  def loss_fn(model, batch):\n    model_fn = mutils.get_model_fn(model, train=train)\n    labels = torch.randint(0, vesde.N, (batch.shape[0],), device=batch.device)\n    sigmas = smld_sigma_array.to(batch.device)[labels]\n    noise = torch.randn_like(batch) * sigmas[:, None, None, None]\n    perturbed_data = noise + batch\n    score = model_fn(perturbed_data, labels)\n    target = -noise / (sigmas ** 2)[:, None, None, None]\n    losses = torch.square(score - target)\n    losses = reduce_op(losses.reshape(losses.shape[0], -1), dim=-1) * sigmas ** 2\n    loss = torch.mean(losses)\n    return loss\n\n  return loss_fn\n\n\ndef get_ddpm_loss_fn(vpsde, train, reduce_mean=True):\n  \"\"\"Legacy code to reproduce previous results on DDPM. Not recommended for new work.\"\"\"\n  assert isinstance(vpsde, VPSDE), \"DDPM training only works for VPSDEs.\"\n\n  reduce_op = torch.mean if reduce_mean else lambda *args, **kwargs: 0.5 * torch.sum(*args, **kwargs)\n\n  def loss_fn(model, batch):\n    model_fn = mutils.get_model_fn(model, train=train)\n    labels = torch.randint(0, vpsde.N, (batch.shape[0],), device=batch.device)\n    sqrt_alphas_cumprod = vpsde.sqrt_alphas_cumprod.to(batch.device)\n    sqrt_1m_alphas_cumprod = vpsde.sqrt_1m_alphas_cumprod.to(batch.device)\n    noise = torch.randn_like(batch)\n    perturbed_data = sqrt_alphas_cumprod[labels, None, None, None] * batch + \\\n                     sqrt_1m_alphas_cumprod[labels, None, None, None] * noise\n    score = model_fn(perturbed_data, labels)\n    losses = torch.square(score - noise)\n    losses = reduce_op(losses.reshape(losses.shape[0], -1), dim=-1)\n    loss = torch.mean(losses)\n    return loss\n\n  return loss_fn\n\n\ndef get_step_fn(sde, train, optimize_fn=None, reduce_mean=False, continuous=True, likelihood_weighting=False):\n  \"\"\"Create a one-step training/evaluation function.\n\n  Args:\n    sde: An `sde_lib.SDE` object that represents the forward SDE.\n    optimize_fn: An optimization function.\n    reduce_mean: If `True`, average the loss across data dimensions. Otherwise sum the loss across data dimensions.\n    continuous: `True` indicates that the model is defined to take continuous time steps.\n    likelihood_weighting: If `True`, weight the mixture of score matching losses according to\n      https://arxiv.org/abs/2101.09258; otherwise use the weighting recommended by our paper.\n\n  Returns:\n    A one-step function for training or evaluation.\n  \"\"\"\n  if continuous:\n    loss_fn = get_sde_loss_fn(sde, train, reduce_mean=reduce_mean,\n                              continuous=True, likelihood_weighting=likelihood_weighting)\n  else:\n    assert not likelihood_weighting, \"Likelihood weighting is not supported for original SMLD/DDPM training.\"\n    if isinstance(sde, VESDE):\n      loss_fn = get_smld_loss_fn(sde, train, reduce_mean=reduce_mean)\n    elif isinstance(sde, VPSDE):\n      loss_fn = get_ddpm_loss_fn(sde, train, reduce_mean=reduce_mean)\n    else:\n      raise ValueError(f\"Discrete training for {sde.__class__.__name__} is not recommended.\")\n\n  def step_fn(state, batch):\n    \"\"\"Running one step of training or evaluation.\n\n    This function will undergo `jax.lax.scan` so that multiple steps can be pmapped and jit-compiled together\n    for faster execution.\n\n    Args:\n      state: A dictionary of training information, containing the score model, optimizer,\n       EMA status, and number of optimization steps.\n      batch: A mini-batch of training/evaluation data.\n\n    Returns:\n      loss: The average loss value of this state.\n    \"\"\"\n    model = state['model']\n    if train:\n      optimizer = state['optimizer']\n      optimizer.zero_grad()\n      loss = loss_fn(model, batch)\n      loss.backward()\n      optimize_fn(optimizer, model.parameters(), step=state['step'])\n      state['step'] += 1\n      state['ema'].update(model.parameters())\n    else:\n      with torch.no_grad():\n        ema = state['ema']\n        ema.store(model.parameters())\n        ema.copy_to(model.parameters())\n        loss = loss_fn(model, batch)\n        ema.restore(model.parameters())\n\n    return loss\n\n  return step_fn\n"}
{"type": "source_file", "path": "LLaVA/llava/conversation.py", "content": "import dataclasses\nfrom enum import auto, Enum\nfrom typing import List, Tuple\nimport base64\nfrom io import BytesIO\nfrom PIL import Image\n\n\nclass SeparatorStyle(Enum):\n    \"\"\"Different separator style.\"\"\"\n    SINGLE = auto()\n    TWO = auto()\n    MPT = auto()\n    PLAIN = auto()\n    LLAMA_2 = auto()\n\n\n@dataclasses.dataclass\nclass Conversation:\n    \"\"\"A class that keeps all conversation history.\"\"\"\n    system: str\n    roles: List[str]\n    messages: List[List[str]]\n    offset: int\n    sep_style: SeparatorStyle = SeparatorStyle.SINGLE\n    sep: str = \"###\"\n    sep2: str = None\n    version: str = \"Unknown\"\n\n    skip_next: bool = False\n\n    def get_prompt(self):\n        messages = self.messages\n        if len(messages) > 0 and type(messages[0][1]) is tuple:\n            messages = self.messages.copy()\n            init_role, init_msg = messages[0].copy()\n            init_msg = init_msg[0].replace(\"<image>\", \"\").strip()\n            if 'mmtag' in self.version:\n                messages[0] = (init_role, init_msg)\n                messages.insert(0, (self.roles[0], \"<Image><image></Image>\"))\n                messages.insert(1, (self.roles[1], \"Received.\"))\n            else:\n                messages[0] = (init_role, \"<image>\\n\" + init_msg)\n\n        if self.sep_style == SeparatorStyle.SINGLE:\n            ret = self.system + self.sep\n            for role, message in messages:\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    ret += role + \": \" + message + self.sep\n                else:\n                    ret += role + \":\"\n        elif self.sep_style == SeparatorStyle.TWO:\n            seps = [self.sep, self.sep2]\n            ret = self.system + seps[0]\n            for i, (role, message) in enumerate(messages):\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    ret += role + \": \" + message + seps[i % 2]\n                else:\n                    ret += role + \":\"\n        elif self.sep_style == SeparatorStyle.MPT:\n            ret = self.system + self.sep\n            for role, message in messages:\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    ret += role + message + self.sep\n                else:\n                    ret += role\n        elif self.sep_style == SeparatorStyle.LLAMA_2:\n            wrap_sys = lambda msg: f\"<<SYS>>\\n{msg}\\n<</SYS>>\\n\\n\" if len(msg) > 0 else msg\n            wrap_inst = lambda msg: f\"[INST] {msg} [/INST]\"\n            ret = \"\"\n\n            for i, (role, message) in enumerate(messages):\n                if i == 0:\n                    assert message, \"first message should not be none\"\n                    assert role == self.roles[0], \"first message should come from user\"\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    if i == 0: message = wrap_sys(self.system) + message\n                    if i % 2 == 0:\n                        message = wrap_inst(message)\n                        ret += self.sep + message\n                    else:\n                        ret += \" \" + message + \" \" + self.sep2\n                else:\n                    ret += \"\"\n            ret = ret.lstrip(self.sep)\n        elif self.sep_style == SeparatorStyle.PLAIN:\n            seps = [self.sep, self.sep2]\n            ret = self.system\n            for i, (role, message) in enumerate(messages):\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    ret += message + seps[i % 2]\n                else:\n                    ret += \"\"\n        else:\n            raise ValueError(f\"Invalid style: {self.sep_style}\")\n\n        return ret\n\n    def append_message(self, role, message):\n        self.messages.append([role, message])\n\n    def process_image(self, image, image_process_mode, return_pil=False, image_format='PNG', max_len=1344, min_len=672):\n        if image_process_mode == \"Pad\":\n            def expand2square(pil_img, background_color=(122, 116, 104)):\n                width, height = pil_img.size\n                if width == height:\n                    return pil_img\n                elif width > height:\n                    result = Image.new(pil_img.mode, (width, width), background_color)\n                    result.paste(pil_img, (0, (width - height) // 2))\n                    return result\n                else:\n                    result = Image.new(pil_img.mode, (height, height), background_color)\n                    result.paste(pil_img, ((height - width) // 2, 0))\n                    return result\n            image = expand2square(image)\n        elif image_process_mode in [\"Default\", \"Crop\"]:\n            pass\n        elif image_process_mode == \"Resize\":\n            image = image.resize((336, 336))\n        else:\n            raise ValueError(f\"Invalid image_process_mode: {image_process_mode}\")\n        if max(image.size) > max_len:\n            max_hw, min_hw = max(image.size), min(image.size)\n            aspect_ratio = max_hw / min_hw\n            shortest_edge = int(min(max_len / aspect_ratio, min_len, min_hw))\n            longest_edge = int(shortest_edge * aspect_ratio)\n            W, H = image.size\n            if H > W:\n                H, W = longest_edge, shortest_edge\n            else:\n                H, W = shortest_edge, longest_edge\n            image = image.resize((W, H))\n        if return_pil:\n            return image\n        else:\n            buffered = BytesIO()\n            image.save(buffered, format=image_format)\n            img_b64_str = base64.b64encode(buffered.getvalue()).decode()\n            return img_b64_str\n\n    def get_images(self, return_pil=False):\n        images = []\n        for i, (role, msg) in enumerate(self.messages[self.offset:]):\n            if i % 2 == 0:\n                if type(msg) is tuple:\n                    msg, image, image_process_mode = msg\n                    image = self.process_image(image, image_process_mode, return_pil=return_pil)\n                    images.append(image)\n        return images\n\n    def to_gradio_chatbot(self):\n        ret = []\n        for i, (role, msg) in enumerate(self.messages[self.offset:]):\n            if i % 2 == 0:\n                if type(msg) is tuple:\n                    msg, image, image_process_mode = msg\n                    img_b64_str = self.process_image(\n                        image, \"Default\", return_pil=False,\n                        image_format='JPEG')\n                    img_str = f'<img src=\"data:image/jpeg;base64,{img_b64_str}\" alt=\"user upload image\" />'\n                    msg = img_str + msg.replace('<image>', '').strip()\n                    ret.append([msg, None])\n                else:\n                    ret.append([msg, None])\n            else:\n                ret[-1][-1] = msg\n        return ret\n\n    def copy(self):\n        return Conversation(\n            system=self.system,\n            roles=self.roles,\n            messages=[[x, y] for x, y in self.messages],\n            offset=self.offset,\n            sep_style=self.sep_style,\n            sep=self.sep,\n            sep2=self.sep2,\n            version=self.version)\n\n    def dict(self):\n        if len(self.get_images()) > 0:\n            return {\n                \"system\": self.system,\n                \"roles\": self.roles,\n                \"messages\": [[x, y[0] if type(y) is tuple else y] for x, y in self.messages],\n                \"offset\": self.offset,\n                \"sep\": self.sep,\n                \"sep2\": self.sep2,\n            }\n        return {\n            \"system\": self.system,\n            \"roles\": self.roles,\n            \"messages\": self.messages,\n            \"offset\": self.offset,\n            \"sep\": self.sep,\n            \"sep2\": self.sep2,\n        }\n\n\nconv_vicuna_v0 = Conversation(\n    system=\"A chat between a curious human and an artificial intelligence assistant. \"\n           \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n    roles=(\"Human\", \"Assistant\"),\n    messages=(\n        (\"Human\", \"What are the key differences between renewable and non-renewable energy sources?\"),\n        (\"Assistant\",\n            \"Renewable energy sources are those that can be replenished naturally in a relatively \"\n            \"short amount of time, such as solar, wind, hydro, geothermal, and biomass. \"\n            \"Non-renewable energy sources, on the other hand, are finite and will eventually be \"\n            \"depleted, such as coal, oil, and natural gas. Here are some key differences between \"\n            \"renewable and non-renewable energy sources:\\n\"\n            \"1. Availability: Renewable energy sources are virtually inexhaustible, while non-renewable \"\n            \"energy sources are finite and will eventually run out.\\n\"\n            \"2. Environmental impact: Renewable energy sources have a much lower environmental impact \"\n            \"than non-renewable sources, which can lead to air and water pollution, greenhouse gas emissions, \"\n            \"and other negative effects.\\n\"\n            \"3. Cost: Renewable energy sources can be more expensive to initially set up, but they typically \"\n            \"have lower operational costs than non-renewable sources.\\n\"\n            \"4. Reliability: Renewable energy sources are often more reliable and can be used in more remote \"\n            \"locations than non-renewable sources.\\n\"\n            \"5. Flexibility: Renewable energy sources are often more flexible and can be adapted to different \"\n            \"situations and needs, while non-renewable sources are more rigid and inflexible.\\n\"\n            \"6. Sustainability: Renewable energy sources are more sustainable over the long term, while \"\n            \"non-renewable sources are not, and their depletion can lead to economic and social instability.\\n\")\n    ),\n    offset=2,\n    sep_style=SeparatorStyle.SINGLE,\n    sep=\"###\",\n)\n\nconv_vicuna_v1 = Conversation(\n    system=\"A chat between a curious user and an artificial intelligence assistant. \"\n    \"The assistant gives helpful, detailed, and polite answers to the user's questions.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"v1\",\n    messages=(),\n    offset=0,\n    sep_style=SeparatorStyle.TWO,\n    sep=\" \",\n    sep2=\"</s>\",\n)\n\nconv_llama_2 = Conversation(\n    system=\"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"llama_v2\",\n    messages=(),\n    offset=0,\n    sep_style=SeparatorStyle.LLAMA_2,\n    sep=\"<s>\",\n    sep2=\"</s>\",\n)\n\nconv_llava_llama_2 = Conversation(\n    system=\"You are a helpful language and vision assistant. \"\n           \"You are able to understand the visual content that the user provides, \"\n           \"and assist the user with a variety of tasks using natural language.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"llama_v2\",\n    messages=(),\n    offset=0,\n    sep_style=SeparatorStyle.LLAMA_2,\n    sep=\"<s>\",\n    sep2=\"</s>\",\n)\n\nconv_mpt = Conversation(\n    system=\"\"\"<|im_start|>system\nA conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers.\"\"\",\n    roles=(\"<|im_start|>user\\n\", \"<|im_start|>assistant\\n\"),\n    version=\"mpt\",\n    messages=(),\n    offset=0,\n    sep_style=SeparatorStyle.MPT,\n    sep=\"<|im_end|>\",\n)\n\nconv_llava_plain = Conversation(\n    system=\"\",\n    roles=(\"\", \"\"),\n    messages=(\n    ),\n    offset=0,\n    sep_style=SeparatorStyle.PLAIN,\n    sep=\"\\n\",\n)\n\nconv_llava_v0 = Conversation(\n    system=\"A chat between a curious human and an artificial intelligence assistant. \"\n           \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n    roles=(\"Human\", \"Assistant\"),\n    messages=(\n    ),\n    offset=0,\n    sep_style=SeparatorStyle.SINGLE,\n    sep=\"###\",\n)\n\nconv_llava_v0_mmtag = Conversation(\n    system=\"A chat between a curious user and an artificial intelligence assistant. \"\n           \"The assistant is able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\"\n           \"The visual content will be provided with the following format: <Image>visual content</Image>.\",\n    roles=(\"Human\", \"Assistant\"),\n    messages=(\n    ),\n    offset=0,\n    sep_style=SeparatorStyle.SINGLE,\n    sep=\"###\",\n    version=\"v0_mmtag\",\n)\n\nconv_llava_v1 = Conversation(\n    system=\"A chat between a curious human and an artificial intelligence assistant. \"\n           \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"v1\",\n    messages=(),\n    offset=0,\n    sep_style=SeparatorStyle.TWO,\n    sep=\" \",\n    sep2=\"</s>\",\n)\n\nconv_llava_v1_mmtag = Conversation(\n    system=\"A chat between a curious user and an artificial intelligence assistant. \"\n           \"The assistant is able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\"\n           \"The visual content will be provided with the following format: <Image>visual content</Image>.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    messages=(),\n    offset=0,\n    sep_style=SeparatorStyle.TWO,\n    sep=\" \",\n    sep2=\"</s>\",\n    version=\"v1_mmtag\",\n)\n\nconv_mistral_instruct = Conversation(\n    system=\"\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"llama_v2\",\n    messages=(),\n    offset=0,\n    sep_style=SeparatorStyle.LLAMA_2,\n    sep=\"\",\n    sep2=\"</s>\",\n)\n\nconv_chatml_direct = Conversation(\n    system=\"\"\"<|im_start|>system\nAnswer the questions.\"\"\",\n    roles=(\"<|im_start|>user\\n\", \"<|im_start|>assistant\\n\"),\n    version=\"mpt\",\n    messages=(),\n    offset=0,\n    sep_style=SeparatorStyle.MPT,\n    sep=\"<|im_end|>\",\n)\n\ndefault_conversation = conv_vicuna_v1\nconv_templates = {\n    \"default\": conv_vicuna_v0,\n    \"v0\": conv_vicuna_v0,\n    \"v1\": conv_vicuna_v1,\n    \"vicuna_v1\": conv_vicuna_v1,\n    \"llama_2\": conv_llama_2,\n    \"mistral_instruct\": conv_mistral_instruct,\n    \"chatml_direct\": conv_chatml_direct,\n    \"mistral_direct\": conv_chatml_direct,\n\n    \"plain\": conv_llava_plain,\n    \"v0_plain\": conv_llava_plain,\n    \"llava_v0\": conv_llava_v0,\n    \"v0_mmtag\": conv_llava_v0_mmtag,\n    \"llava_v1\": conv_llava_v1,\n    \"v1_mmtag\": conv_llava_v1_mmtag,\n    \"llava_llama_2\": conv_llava_llama_2,\n\n    \"mpt\": conv_mpt,\n}\n\n\nif __name__ == \"__main__\":\n    print(default_conversation.get_prompt())\n"}
{"type": "source_file", "path": "Image_Purifier/score_sde/models/layerspp.py", "content": "# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# pylint: skip-file\n\"\"\"Layers for defining NCSN++.\n\"\"\"\nfrom . import layers\nfrom . import up_or_down_sampling\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\n\nconv1x1 = layers.ddpm_conv1x1\nconv3x3 = layers.ddpm_conv3x3\nNIN = layers.NIN\ndefault_init = layers.default_init\n\n\nclass GaussianFourierProjection(nn.Module):\n  \"\"\"Gaussian Fourier embeddings for noise levels.\"\"\"\n\n  def __init__(self, embedding_size=256, scale=1.0):\n    super().__init__()\n    self.W = nn.Parameter(torch.randn(embedding_size) * scale, requires_grad=False)\n\n  def forward(self, x):\n    x_proj = x[:, None] * self.W[None, :] * 2 * np.pi\n    return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n\n\nclass Combine(nn.Module):\n  \"\"\"Combine information from skip connections.\"\"\"\n\n  def __init__(self, dim1, dim2, method='cat'):\n    super().__init__()\n    self.Conv_0 = conv1x1(dim1, dim2)\n    self.method = method\n\n  def forward(self, x, y):\n    h = self.Conv_0(x)\n    if self.method == 'cat':\n      return torch.cat([h, y], dim=1)\n    elif self.method == 'sum':\n      return h + y\n    else:\n      raise ValueError(f'Method {self.method} not recognized.')\n\n\nclass AttnBlockpp(nn.Module):\n  \"\"\"Channel-wise self-attention block. Modified from DDPM.\"\"\"\n\n  def __init__(self, channels, skip_rescale=False, init_scale=0.):\n    super().__init__()\n    self.GroupNorm_0 = nn.GroupNorm(num_groups=min(channels // 4, 32), num_channels=channels,\n                                  eps=1e-6)\n    self.NIN_0 = NIN(channels, channels)\n    self.NIN_1 = NIN(channels, channels)\n    self.NIN_2 = NIN(channels, channels)\n    self.NIN_3 = NIN(channels, channels, init_scale=init_scale)\n    self.skip_rescale = skip_rescale\n\n  def forward(self, x):\n    B, C, H, W = x.shape\n    h = self.GroupNorm_0(x)\n    q = self.NIN_0(h)\n    k = self.NIN_1(h)\n    v = self.NIN_2(h)\n\n    w = torch.einsum('bchw,bcij->bhwij', q, k) * (int(C) ** (-0.5))\n    w = torch.reshape(w, (B, H, W, H * W))\n    w = F.softmax(w, dim=-1)\n    w = torch.reshape(w, (B, H, W, H, W))\n    h = torch.einsum('bhwij,bcij->bchw', w, v)\n    h = self.NIN_3(h)\n    if not self.skip_rescale:\n      return x + h\n    else:\n      return (x + h) / np.sqrt(2.)\n\n\nclass Upsample(nn.Module):\n  def __init__(self, in_ch=None, out_ch=None, with_conv=False, fir=False,\n               fir_kernel=(1, 3, 3, 1)):\n    super().__init__()\n    out_ch = out_ch if out_ch else in_ch\n    if not fir:\n      if with_conv:\n        self.Conv_0 = conv3x3(in_ch, out_ch)\n    else:\n      if with_conv:\n        self.Conv2d_0 = up_or_down_sampling.Conv2d(in_ch, out_ch,\n                                                 kernel=3, up=True,\n                                                 resample_kernel=fir_kernel,\n                                                 use_bias=True,\n                                                 kernel_init=default_init())\n    self.fir = fir\n    self.with_conv = with_conv\n    self.fir_kernel = fir_kernel\n    self.out_ch = out_ch\n\n  def forward(self, x):\n    B, C, H, W = x.shape\n    if not self.fir:\n      h = F.interpolate(x, (H * 2, W * 2), 'nearest')\n      if self.with_conv:\n        h = self.Conv_0(h)\n    else:\n      if not self.with_conv:\n        h = up_or_down_sampling.upsample_2d(x, self.fir_kernel, factor=2)\n      else:\n        h = self.Conv2d_0(x)\n\n    return h\n\n\nclass Downsample(nn.Module):\n  def __init__(self, in_ch=None, out_ch=None, with_conv=False, fir=False,\n               fir_kernel=(1, 3, 3, 1)):\n    super().__init__()\n    out_ch = out_ch if out_ch else in_ch\n    if not fir:\n      if with_conv:\n        self.Conv_0 = conv3x3(in_ch, out_ch, stride=2, padding=0)\n    else:\n      if with_conv:\n        self.Conv2d_0 = up_or_down_sampling.Conv2d(in_ch, out_ch,\n                                                 kernel=3, down=True,\n                                                 resample_kernel=fir_kernel,\n                                                 use_bias=True,\n                                                 kernel_init=default_init())\n    self.fir = fir\n    self.fir_kernel = fir_kernel\n    self.with_conv = with_conv\n    self.out_ch = out_ch\n\n  def forward(self, x):\n    B, C, H, W = x.shape\n    if not self.fir:\n      if self.with_conv:\n        x = F.pad(x, (0, 1, 0, 1))\n        x = self.Conv_0(x)\n      else:\n        x = F.avg_pool2d(x, 2, stride=2)\n    else:\n      if not self.with_conv:\n        x = up_or_down_sampling.downsample_2d(x, self.fir_kernel, factor=2)\n      else:\n        x = self.Conv2d_0(x)\n\n    return x\n\n\nclass ResnetBlockDDPMpp(nn.Module):\n  \"\"\"ResBlock adapted from DDPM.\"\"\"\n\n  def __init__(self, act, in_ch, out_ch=None, temb_dim=None, conv_shortcut=False,\n               dropout=0.1, skip_rescale=False, init_scale=0.):\n    super().__init__()\n    out_ch = out_ch if out_ch else in_ch\n    self.GroupNorm_0 = nn.GroupNorm(num_groups=min(in_ch // 4, 32), num_channels=in_ch, eps=1e-6)\n    self.Conv_0 = conv3x3(in_ch, out_ch)\n    if temb_dim is not None:\n      self.Dense_0 = nn.Linear(temb_dim, out_ch)\n      self.Dense_0.weight.data = default_init()(self.Dense_0.weight.data.shape)\n      nn.init.zeros_(self.Dense_0.bias)\n    self.GroupNorm_1 = nn.GroupNorm(num_groups=min(out_ch // 4, 32), num_channels=out_ch, eps=1e-6)\n    self.Dropout_0 = nn.Dropout(dropout)\n    self.Conv_1 = conv3x3(out_ch, out_ch, init_scale=init_scale)\n    if in_ch != out_ch:\n      if conv_shortcut:\n        self.Conv_2 = conv3x3(in_ch, out_ch)\n      else:\n        self.NIN_0 = NIN(in_ch, out_ch)\n\n    self.skip_rescale = skip_rescale\n    self.act = act\n    self.out_ch = out_ch\n    self.conv_shortcut = conv_shortcut\n\n  def forward(self, x, temb=None):\n    h = self.act(self.GroupNorm_0(x))\n    h = self.Conv_0(h)\n    if temb is not None:\n      h += self.Dense_0(self.act(temb))[:, :, None, None]\n    h = self.act(self.GroupNorm_1(h))\n    h = self.Dropout_0(h)\n    h = self.Conv_1(h)\n    if x.shape[1] != self.out_ch:\n      if self.conv_shortcut:\n        x = self.Conv_2(x)\n      else:\n        x = self.NIN_0(x)\n    if not self.skip_rescale:\n      return x + h\n    else:\n      return (x + h) / np.sqrt(2.)\n\n\nclass ResnetBlockBigGANpp(nn.Module):\n  def __init__(self, act, in_ch, out_ch=None, temb_dim=None, up=False, down=False,\n               dropout=0.1, fir=False, fir_kernel=(1, 3, 3, 1),\n               skip_rescale=True, init_scale=0.):\n    super().__init__()\n\n    out_ch = out_ch if out_ch else in_ch\n    self.GroupNorm_0 = nn.GroupNorm(num_groups=min(in_ch // 4, 32), num_channels=in_ch, eps=1e-6)\n    self.up = up\n    self.down = down\n    self.fir = fir\n    self.fir_kernel = fir_kernel\n\n    self.Conv_0 = conv3x3(in_ch, out_ch)\n    if temb_dim is not None:\n      self.Dense_0 = nn.Linear(temb_dim, out_ch)\n      self.Dense_0.weight.data = default_init()(self.Dense_0.weight.shape)\n      nn.init.zeros_(self.Dense_0.bias)\n\n    self.GroupNorm_1 = nn.GroupNorm(num_groups=min(out_ch // 4, 32), num_channels=out_ch, eps=1e-6)\n    self.Dropout_0 = nn.Dropout(dropout)\n    self.Conv_1 = conv3x3(out_ch, out_ch, init_scale=init_scale)\n    if in_ch != out_ch or up or down:\n      self.Conv_2 = conv1x1(in_ch, out_ch)\n\n    self.skip_rescale = skip_rescale\n    self.act = act\n    self.in_ch = in_ch\n    self.out_ch = out_ch\n\n  def forward(self, x, temb=None):\n    h = self.act(self.GroupNorm_0(x))\n\n    if self.up:\n      if self.fir:\n        h = up_or_down_sampling.upsample_2d(h, self.fir_kernel, factor=2)\n        x = up_or_down_sampling.upsample_2d(x, self.fir_kernel, factor=2)\n      else:\n        h = up_or_down_sampling.naive_upsample_2d(h, factor=2)\n        x = up_or_down_sampling.naive_upsample_2d(x, factor=2)\n    elif self.down:\n      if self.fir:\n        h = up_or_down_sampling.downsample_2d(h, self.fir_kernel, factor=2)\n        x = up_or_down_sampling.downsample_2d(x, self.fir_kernel, factor=2)\n      else:\n        h = up_or_down_sampling.naive_downsample_2d(h, factor=2)\n        x = up_or_down_sampling.naive_downsample_2d(x, factor=2)\n\n    h = self.Conv_0(h)\n    # Add bias to each feature map conditioned on the time embedding\n    if temb is not None:\n      h += self.Dense_0(self.act(temb))[:, :, None, None]\n    h = self.act(self.GroupNorm_1(h))\n    h = self.Dropout_0(h)\n    h = self.Conv_1(h)\n\n    if self.in_ch != self.out_ch or self.up or self.down:\n      x = self.Conv_2(x)\n\n    if not self.skip_rescale:\n      return x + h\n    else:\n      return (x + h) / np.sqrt(2.)\n"}
{"type": "source_file", "path": "LLaVA/llava/eval/eval_science_qa_gpt4_requery.py", "content": "import argparse\nimport json\nimport os\nimport re\nimport random\nfrom collections import defaultdict\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--base-dir', type=str)\n    parser.add_argument('--gpt4-result', type=str)\n    parser.add_argument('--requery-result', type=str)\n    parser.add_argument('--our-result', type=str)\n    parser.add_argument('--output-result', type=str)\n    parser.add_argument('--split', type=str, default='test')\n    parser.add_argument('--options', type=list, default=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n    return parser.parse_args()\n\n\ndef convert_caps(results):\n    fakecaps = []\n    for result in results:\n        image_id = result['question_id']\n        caption = result['text']\n        fakecaps.append({\"image_id\": int(image_id), \"caption\": caption})\n    return fakecaps\n\n\ndef get_pred_idx(prediction, choices, options):\n    \"\"\"\n    Get the index (e.g. 2) from the prediction (e.g. 'C')\n    \"\"\"\n    if prediction in options[:len(choices)]:\n        return options.index(prediction)\n    else:\n        return random.choice(range(len(choices)))\n\n\nif __name__ == \"__main__\":\n    args = get_args()\n\n    base_dir = args.base_dir\n    split_indices = json.load(open(os.path.join(base_dir, \"pid_splits.json\")))[args.split]\n    problems = json.load(open(os.path.join(base_dir, \"problems.json\")))\n    our_predictions = [json.loads(line) for line in open(args.our_result)]\n    our_predictions = {pred['question_id']: pred for pred in our_predictions}\n    split_problems = {idx: problems[idx] for idx in split_indices}\n\n    requery_predictions = [json.loads(line) for line in open(args.requery_result)]\n    requery_predictions = {pred['question_id']: pred for pred in requery_predictions}\n\n    gpt4_predictions = json.load(open(args.gpt4_result))['outputs']\n\n    results = defaultdict(lambda: 0)\n\n    sqa_results = {}\n    sqa_results['acc'] = None\n    sqa_results['correct'] = None\n    sqa_results['count'] = None\n    sqa_results['results'] = {}\n    sqa_results['outputs'] = {}\n\n    for prob_id, prob in split_problems.items():\n        if prob_id not in our_predictions:\n            assert False\n        if prob_id not in gpt4_predictions:\n            assert False\n        our_pred = our_predictions[prob_id]['text']\n        gpt4_pred = gpt4_predictions[prob_id]\n        if prob_id not in requery_predictions:\n            results['missing_requery'] += 1\n            requery_pred = \"MISSING\"\n        else:\n            requery_pred = requery_predictions[prob_id]['text']\n\n        pattern = re.compile(r'The answer is ([A-Z]).')\n        our_res = pattern.findall(our_pred)\n        if len(our_res) == 1:\n            our_answer = our_res[0]  # 'A', 'B', ...\n        else:\n            our_answer = \"FAILED\"\n\n        requery_res = pattern.findall(requery_pred)\n        if len(requery_res) == 1:\n            requery_answer = requery_res[0]  # 'A', 'B', ...\n        else:\n            requery_answer = \"FAILED\"\n\n        gpt4_res = pattern.findall(gpt4_pred)\n        if len(gpt4_res) == 1:\n            gpt4_answer = gpt4_res[0]  # 'A', 'B', ...\n        else:\n            gpt4_answer = \"FAILED\"\n\n        our_pred_idx = get_pred_idx(our_answer, prob['choices'], args.options)\n        gpt4_pred_idx = get_pred_idx(gpt4_answer, prob['choices'], args.options)\n        requery_pred_idx = get_pred_idx(requery_answer, prob['choices'], args.options)\n\n        results['total'] += 1\n\n        if gpt4_answer == 'FAILED':\n            results['gpt4_failed'] += 1\n            if gpt4_pred_idx == prob['answer']:\n                results['gpt4_correct'] += 1\n            if our_pred_idx == prob['answer']:\n                results['gpt4_ourvisual_correct'] += 1\n        elif gpt4_pred_idx == prob['answer']:\n            results['gpt4_correct'] += 1\n            results['gpt4_ourvisual_correct'] += 1\n\n        if our_pred_idx == prob['answer']:\n            results['our_correct'] += 1\n\n        if requery_answer == 'FAILED':\n            sqa_results['results'][prob_id] = our_pred_idx\n            if our_pred_idx == prob['answer']:\n                results['requery_correct'] += 1\n        else:\n            sqa_results['results'][prob_id] = requery_pred_idx\n            if requery_pred_idx == prob['answer']:\n                results['requery_correct'] += 1\n            else:\n                print(f\"\"\"\nQuestion ({args.options[prob['answer']]}): {our_predictions[prob_id]['prompt']}\nOur ({our_answer}): {our_pred}\nGPT-4 ({gpt4_answer}): {gpt4_pred}\nRequery ({requery_answer}): {requery_pred}\nprint(\"=====================================\")\n\"\"\")\n\n        if gpt4_pred_idx == prob['answer'] or our_pred_idx == prob['answer']:\n            results['correct_upperbound'] += 1\n\n    total = results['total']\n    print(f'Total: {total}, Our-Correct: {results[\"our_correct\"]}, Accuracy: {results[\"our_correct\"] / total * 100:.2f}%')\n    print(f'Total: {total}, GPT-4-Correct: {results[\"gpt4_correct\"]}, Accuracy: {results[\"gpt4_correct\"] / total * 100:.2f}%')\n    print(f'Total: {total}, GPT-4 NO-ANS (RANDOM): {results[\"gpt4_failed\"]}, Percentage: {results[\"gpt4_failed\"] / total * 100:.2f}%')\n    print(f'Total: {total}, GPT-4-OursVisual-Correct: {results[\"gpt4_ourvisual_correct\"]}, Accuracy: {results[\"gpt4_ourvisual_correct\"] / total * 100:.2f}%')\n    print(f'Total: {total}, Requery-Correct: {results[\"requery_correct\"]}, Accuracy: {results[\"requery_correct\"] / total * 100:.2f}%')\n    print(f'Total: {total}, Correct upper: {results[\"correct_upperbound\"]}, Accuracy: {results[\"correct_upperbound\"] / total * 100:.2f}%')\n\n    sqa_results['acc'] = results[\"requery_correct\"] / total * 100\n    sqa_results['correct'] = results[\"requery_correct\"]\n    sqa_results['count'] = total\n\n    with open(args.output_result, 'w') as f:\n        json.dump(sqa_results, f, indent=2)\n\n"}
{"type": "source_file", "path": "Image_Purifier/score_sde/models/normalization.py", "content": "# coding=utf-8\n# Copyright 2020 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Normalization layers.\"\"\"\nimport torch.nn as nn\nimport torch\nimport functools\n\n\ndef get_normalization(config, conditional=False):\n  \"\"\"Obtain normalization modules from the config file.\"\"\"\n  norm = config.model.normalization\n  if conditional:\n    if norm == 'InstanceNorm++':\n      return functools.partial(ConditionalInstanceNorm2dPlus, num_classes=config.model.num_classes)\n    else:\n      raise NotImplementedError(f'{norm} not implemented yet.')\n  else:\n    if norm == 'InstanceNorm':\n      return nn.InstanceNorm2d\n    elif norm == 'InstanceNorm++':\n      return InstanceNorm2dPlus\n    elif norm == 'VarianceNorm':\n      return VarianceNorm2d\n    elif norm == 'GroupNorm':\n      return nn.GroupNorm\n    else:\n      raise ValueError('Unknown normalization: %s' % norm)\n\n\nclass ConditionalBatchNorm2d(nn.Module):\n  def __init__(self, num_features, num_classes, bias=True):\n    super().__init__()\n    self.num_features = num_features\n    self.bias = bias\n    self.bn = nn.BatchNorm2d(num_features, affine=False)\n    if self.bias:\n      self.embed = nn.Embedding(num_classes, num_features * 2)\n      self.embed.weight.data[:, :num_features].uniform_()  # Initialise scale at N(1, 0.02)\n      self.embed.weight.data[:, num_features:].zero_()  # Initialise bias at 0\n    else:\n      self.embed = nn.Embedding(num_classes, num_features)\n      self.embed.weight.data.uniform_()\n\n  def forward(self, x, y):\n    out = self.bn(x)\n    if self.bias:\n      gamma, beta = self.embed(y).chunk(2, dim=1)\n      out = gamma.view(-1, self.num_features, 1, 1) * out + beta.view(-1, self.num_features, 1, 1)\n    else:\n      gamma = self.embed(y)\n      out = gamma.view(-1, self.num_features, 1, 1) * out\n    return out\n\n\nclass ConditionalInstanceNorm2d(nn.Module):\n  def __init__(self, num_features, num_classes, bias=True):\n    super().__init__()\n    self.num_features = num_features\n    self.bias = bias\n    self.instance_norm = nn.InstanceNorm2d(num_features, affine=False, track_running_stats=False)\n    if bias:\n      self.embed = nn.Embedding(num_classes, num_features * 2)\n      self.embed.weight.data[:, :num_features].uniform_()  # Initialise scale at N(1, 0.02)\n      self.embed.weight.data[:, num_features:].zero_()  # Initialise bias at 0\n    else:\n      self.embed = nn.Embedding(num_classes, num_features)\n      self.embed.weight.data.uniform_()\n\n  def forward(self, x, y):\n    h = self.instance_norm(x)\n    if self.bias:\n      gamma, beta = self.embed(y).chunk(2, dim=-1)\n      out = gamma.view(-1, self.num_features, 1, 1) * h + beta.view(-1, self.num_features, 1, 1)\n    else:\n      gamma = self.embed(y)\n      out = gamma.view(-1, self.num_features, 1, 1) * h\n    return out\n\n\nclass ConditionalVarianceNorm2d(nn.Module):\n  def __init__(self, num_features, num_classes, bias=False):\n    super().__init__()\n    self.num_features = num_features\n    self.bias = bias\n    self.embed = nn.Embedding(num_classes, num_features)\n    self.embed.weight.data.normal_(1, 0.02)\n\n  def forward(self, x, y):\n    vars = torch.var(x, dim=(2, 3), keepdim=True)\n    h = x / torch.sqrt(vars + 1e-5)\n\n    gamma = self.embed(y)\n    out = gamma.view(-1, self.num_features, 1, 1) * h\n    return out\n\n\nclass VarianceNorm2d(nn.Module):\n  def __init__(self, num_features, bias=False):\n    super().__init__()\n    self.num_features = num_features\n    self.bias = bias\n    self.alpha = nn.Parameter(torch.zeros(num_features))\n    self.alpha.data.normal_(1, 0.02)\n\n  def forward(self, x):\n    vars = torch.var(x, dim=(2, 3), keepdim=True)\n    h = x / torch.sqrt(vars + 1e-5)\n\n    out = self.alpha.view(-1, self.num_features, 1, 1) * h\n    return out\n\n\nclass ConditionalNoneNorm2d(nn.Module):\n  def __init__(self, num_features, num_classes, bias=True):\n    super().__init__()\n    self.num_features = num_features\n    self.bias = bias\n    if bias:\n      self.embed = nn.Embedding(num_classes, num_features * 2)\n      self.embed.weight.data[:, :num_features].uniform_()  # Initialise scale at N(1, 0.02)\n      self.embed.weight.data[:, num_features:].zero_()  # Initialise bias at 0\n    else:\n      self.embed = nn.Embedding(num_classes, num_features)\n      self.embed.weight.data.uniform_()\n\n  def forward(self, x, y):\n    if self.bias:\n      gamma, beta = self.embed(y).chunk(2, dim=-1)\n      out = gamma.view(-1, self.num_features, 1, 1) * x + beta.view(-1, self.num_features, 1, 1)\n    else:\n      gamma = self.embed(y)\n      out = gamma.view(-1, self.num_features, 1, 1) * x\n    return out\n\n\nclass NoneNorm2d(nn.Module):\n  def __init__(self, num_features, bias=True):\n    super().__init__()\n\n  def forward(self, x):\n    return x\n\n\nclass InstanceNorm2dPlus(nn.Module):\n  def __init__(self, num_features, bias=True):\n    super().__init__()\n    self.num_features = num_features\n    self.bias = bias\n    self.instance_norm = nn.InstanceNorm2d(num_features, affine=False, track_running_stats=False)\n    self.alpha = nn.Parameter(torch.zeros(num_features))\n    self.gamma = nn.Parameter(torch.zeros(num_features))\n    self.alpha.data.normal_(1, 0.02)\n    self.gamma.data.normal_(1, 0.02)\n    if bias:\n      self.beta = nn.Parameter(torch.zeros(num_features))\n\n  def forward(self, x):\n    means = torch.mean(x, dim=(2, 3))\n    m = torch.mean(means, dim=-1, keepdim=True)\n    v = torch.var(means, dim=-1, keepdim=True)\n    means = (means - m) / (torch.sqrt(v + 1e-5))\n    h = self.instance_norm(x)\n\n    if self.bias:\n      h = h + means[..., None, None] * self.alpha[..., None, None]\n      out = self.gamma.view(-1, self.num_features, 1, 1) * h + self.beta.view(-1, self.num_features, 1, 1)\n    else:\n      h = h + means[..., None, None] * self.alpha[..., None, None]\n      out = self.gamma.view(-1, self.num_features, 1, 1) * h\n    return out\n\n\nclass ConditionalInstanceNorm2dPlus(nn.Module):\n  def __init__(self, num_features, num_classes, bias=True):\n    super().__init__()\n    self.num_features = num_features\n    self.bias = bias\n    self.instance_norm = nn.InstanceNorm2d(num_features, affine=False, track_running_stats=False)\n    if bias:\n      self.embed = nn.Embedding(num_classes, num_features * 3)\n      self.embed.weight.data[:, :2 * num_features].normal_(1, 0.02)  # Initialise scale at N(1, 0.02)\n      self.embed.weight.data[:, 2 * num_features:].zero_()  # Initialise bias at 0\n    else:\n      self.embed = nn.Embedding(num_classes, 2 * num_features)\n      self.embed.weight.data.normal_(1, 0.02)\n\n  def forward(self, x, y):\n    means = torch.mean(x, dim=(2, 3))\n    m = torch.mean(means, dim=-1, keepdim=True)\n    v = torch.var(means, dim=-1, keepdim=True)\n    means = (means - m) / (torch.sqrt(v + 1e-5))\n    h = self.instance_norm(x)\n\n    if self.bias:\n      gamma, alpha, beta = self.embed(y).chunk(3, dim=-1)\n      h = h + means[..., None, None] * alpha[..., None, None]\n      out = gamma.view(-1, self.num_features, 1, 1) * h + beta.view(-1, self.num_features, 1, 1)\n    else:\n      gamma, alpha = self.embed(y).chunk(2, dim=-1)\n      h = h + means[..., None, None] * alpha[..., None, None]\n      out = gamma.view(-1, self.num_features, 1, 1) * h\n    return out\n"}
{"type": "source_file", "path": "LLaVA/llava/eval/eval_gpt_review_bench.py", "content": "import argparse\nimport json\nimport os\n\nimport openai\nimport time\n\nNUM_SECONDS_TO_SLEEP = 0.5\n\n\ndef get_eval(content: str, max_tokens: int):\n    while True:\n        try:\n            response = openai.ChatCompletion.create(\n                model='gpt-4-0314',\n                messages=[{\n                    'role': 'system',\n                    'content': 'You are a helpful and precise assistant for checking the quality of the answer.'\n                }, {\n                    'role': 'user',\n                    'content': content,\n                }],\n                temperature=0.2,  # TODO: figure out which temperature is best for evaluation\n                max_tokens=max_tokens,\n            )\n            break\n        except openai.error.RateLimitError:\n            pass\n        except Exception as e:\n            print(e)\n        time.sleep(NUM_SECONDS_TO_SLEEP)\n\n    return response['choices'][0]['message']['content']\n\n\ndef parse_score(review):\n    try:\n        score_pair = review.split('\\n')[0]\n        score_pair = score_pair.replace(',', ' ')\n        sp = score_pair.split(' ')\n        if len(sp) == 2:\n            return [float(sp[0]), float(sp[1])]\n        else:\n            print('error', review)\n            return [-1, -1]\n    except Exception as e:\n        print(e)\n        print('error', review)\n        return [-1, -1]\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='ChatGPT-based QA evaluation.')\n    parser.add_argument('-q', '--question')\n    parser.add_argument('-c', '--context')\n    parser.add_argument('-a', '--answer-list', nargs='+', default=[])\n    parser.add_argument('-r', '--rule')\n    parser.add_argument('-o', '--output')\n    parser.add_argument('--max-tokens', type=int, default=1024, help='maximum number of tokens produced in the output')\n    args = parser.parse_args()\n\n    f_q = open(os.path.expanduser(args.question))\n    f_ans1 = open(os.path.expanduser(args.answer_list[0]))\n    f_ans2 = open(os.path.expanduser(args.answer_list[1]))\n    rule_dict = json.load(open(os.path.expanduser(args.rule), 'r'))\n\n    if os.path.isfile(os.path.expanduser(args.output)):\n        cur_reviews = [json.loads(line) for line in open(os.path.expanduser(args.output))]\n    else:\n        cur_reviews = []\n\n    review_file = open(f'{args.output}', 'a')\n\n    context_list = [json.loads(line) for line in open(os.path.expanduser(args.context))]\n    image_to_context = {context['image']: context for context in context_list}\n\n    handles = []\n    idx = 0\n    for ques_js, ans1_js, ans2_js in zip(f_q, f_ans1, f_ans2):\n        ques = json.loads(ques_js)\n        ans1 = json.loads(ans1_js)\n        ans2 = json.loads(ans2_js)\n\n        inst = image_to_context[ques['image']]\n\n        if isinstance(inst['caption'], list):\n            cap_str = '\\n'.join(inst['caption'])\n        else:\n            cap_str = inst['caption']\n\n        category = 'llava_bench_' + json.loads(ques_js)['category']\n        if category in rule_dict:\n            rule = rule_dict[category]\n        else:\n            assert False, f\"Visual QA category not found in rule file: {category}.\"\n        prompt = rule['prompt']\n        role = rule['role']\n        content = (f'[Context]\\n{cap_str}\\n\\n'\n                   f'[Question]\\n{ques[\"text\"]}\\n\\n'\n                   f'[{role} 1]\\n{ans1[\"text\"]}\\n\\n[End of {role} 1]\\n\\n'\n                   f'[{role} 2]\\n{ans2[\"text\"]}\\n\\n[End of {role} 2]\\n\\n'\n                   f'[System]\\n{prompt}\\n\\n')\n        cur_js = {\n            'id': idx+1,\n            'question_id': ques['question_id'],\n            'answer1_id': ans1.get('answer_id', ans1['question_id']),\n            'answer2_id': ans2.get('answer_id', ans2['answer_id']),\n            'category': category\n        }\n        if idx >= len(cur_reviews):\n            review = get_eval(content, args.max_tokens)\n            scores = parse_score(review)\n            cur_js['content'] = review\n            cur_js['tuple'] = scores\n            review_file.write(json.dumps(cur_js) + '\\n')\n            review_file.flush()\n        else:\n            print(f'Skipping {idx} as we already have it.')\n        idx += 1\n        print(idx)\n    review_file.close()\n"}
{"type": "source_file", "path": "LLaVA/llava/eval/eval_science_qa_gpt4.py", "content": "import argparse\nimport json\nimport os\nimport re\nimport random\nfrom collections import defaultdict\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--base-dir', type=str)\n    parser.add_argument('--gpt4-result', type=str)\n    parser.add_argument('--our-result', type=str)\n    parser.add_argument('--split', type=str, default='test')\n    parser.add_argument('--options', type=list, default=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n    return parser.parse_args()\n\n\ndef convert_caps(results):\n    fakecaps = []\n    for result in results:\n        image_id = result['question_id']\n        caption = result['text']\n        fakecaps.append({\"image_id\": int(image_id), \"caption\": caption})\n    return fakecaps\n\n\ndef get_pred_idx(prediction, choices, options):\n    \"\"\"\n    Get the index (e.g. 2) from the prediction (e.g. 'C')\n    \"\"\"\n    if prediction in options[:len(choices)]:\n        return options.index(prediction)\n    else:\n        return random.choice(range(len(choices)))\n\n\nif __name__ == \"__main__\":\n    args = get_args()\n\n    base_dir = args.base_dir\n    split_indices = json.load(open(os.path.join(base_dir, \"pid_splits.json\")))[args.split]\n    problems = json.load(open(os.path.join(base_dir, \"problems.json\")))\n    our_predictions = [json.loads(line) for line in open(args.our_result)]\n    our_predictions = {pred['question_id']: pred for pred in our_predictions}\n    split_problems = {idx: problems[idx] for idx in split_indices}\n\n    gpt4_predictions = json.load(open(args.gpt4_result))['outputs']\n\n    results = defaultdict(lambda: 0)\n\n    for prob_id, prob in split_problems.items():\n        if prob_id not in our_predictions:\n            continue\n        if prob_id not in gpt4_predictions:\n            continue\n        our_pred = our_predictions[prob_id]['text']\n        gpt4_pred = gpt4_predictions[prob_id]\n\n        pattern = re.compile(r'The answer is ([A-Z]).')\n        our_res = pattern.findall(our_pred)\n        if len(our_res) == 1:\n            our_answer = our_res[0]  # 'A', 'B', ...\n        else:\n            our_answer = \"FAILED\"\n        gpt4_res = pattern.findall(gpt4_pred)\n        if len(gpt4_res) == 1:\n            gpt4_answer = gpt4_res[0]  # 'A', 'B', ...\n        else:\n            gpt4_answer = \"FAILED\"\n\n        our_pred_idx = get_pred_idx(our_answer, prob['choices'], args.options)\n        gpt4_pred_idx = get_pred_idx(gpt4_answer, prob['choices'], args.options)\n\n        if gpt4_answer == 'FAILED':\n            results['gpt4_failed'] += 1\n            # continue\n            gpt4_pred_idx = our_pred_idx\n            # if our_pred_idx != prob['answer']:\n            #     print(our_predictions[prob_id]['prompt'])\n            #     print('-----------------')\n            #     print(f'LECTURE: {prob[\"lecture\"]}')\n            #     print(f'SOLUTION: {prob[\"solution\"]}')\n            #     print('=====================')\n        else:\n            # continue\n            pass\n        # gpt4_pred_idx = our_pred_idx\n\n        if gpt4_pred_idx == prob['answer']:\n            results['correct'] += 1\n        else:\n            results['incorrect'] += 1\n\n\n        if gpt4_pred_idx == prob['answer'] or our_pred_idx == prob['answer']:\n            results['correct_upperbound'] += 1\n\n    correct = results['correct']\n    total = results['correct'] + results['incorrect']\n    print(f'Total: {total}, Correct: {correct}, Accuracy: {correct / total * 100:.2f}%')\n    print(f'Total: {total}, Correct (upper): {results[\"correct_upperbound\"]}, Accuracy: {results[\"correct_upperbound\"] / total * 100:.2f}%')\n    print(f'Total: {total}, GPT-4 NO-ANS (RANDOM): {results[\"gpt4_failed\"]}, Percentage: {results[\"gpt4_failed\"] / total * 100:.2f}%')\n\n"}
{"type": "source_file", "path": "LLaVA/llava/eval/eval_gpt_review_visual.py", "content": "import argparse\nimport json\nimport os\n\nimport openai\nimport time\n\nNUM_SECONDS_TO_SLEEP = 0.5\n\n\ndef get_eval(content: str, max_tokens: int):\n    while True:\n        try:\n            response = openai.ChatCompletion.create(\n                model='gpt-4-0314',\n                messages=[{\n                    'role': 'system',\n                    'content': 'You are a helpful and precise assistant for checking the quality of the answer.'\n                }, {\n                    'role': 'user',\n                    'content': content,\n                }],\n                temperature=0.2,  # TODO: figure out which temperature is best for evaluation\n                max_tokens=max_tokens,\n            )\n            break\n        except openai.error.RateLimitError:\n            pass\n        except Exception as e:\n            print(e)\n        time.sleep(NUM_SECONDS_TO_SLEEP)\n\n    return response['choices'][0]['message']['content']\n\n\ndef parse_score(review):\n    try:\n        score_pair = review.split('\\n')[0]\n        score_pair = score_pair.replace(',', ' ')\n        sp = score_pair.split(' ')\n        if len(sp) == 2:\n            return [float(sp[0]), float(sp[1])]\n        else:\n            print('error', review)\n            return [-1, -1]\n    except Exception as e:\n        print(e)\n        print('error', review)\n        return [-1, -1]\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='ChatGPT-based QA evaluation.')\n    parser.add_argument('-q', '--question')\n    parser.add_argument('-c', '--context')\n    parser.add_argument('-a', '--answer-list', nargs='+', default=[])\n    parser.add_argument('-r', '--rule')\n    parser.add_argument('-o', '--output')\n    parser.add_argument('--max-tokens', type=int, default=1024, help='maximum number of tokens produced in the output')\n    args = parser.parse_args()\n\n    f_q = open(os.path.expanduser(args.question))\n    f_ans1 = open(os.path.expanduser(args.answer_list[0]))\n    f_ans2 = open(os.path.expanduser(args.answer_list[1]))\n    rule_dict = json.load(open(os.path.expanduser(args.rule), 'r'))\n\n    if os.path.isfile(os.path.expanduser(args.output)):\n        cur_reviews = [json.loads(line) for line in open(os.path.expanduser(args.output))]\n    else:\n        cur_reviews = []\n\n    review_file = open(f'{args.output}', 'a')\n\n    context_list = [json.loads(line) for line in open(os.path.expanduser(args.context))]\n    image_to_context = {context['image']: context for context in context_list}\n\n    handles = []\n    idx = 0\n    for ques_js, ans1_js, ans2_js in zip(f_q, f_ans1, f_ans2):\n        ques = json.loads(ques_js)\n        ans1 = json.loads(ans1_js)\n        ans2 = json.loads(ans2_js)\n\n        inst = image_to_context[ques['image']]\n        cap_str = '\\n'.join(inst['captions'])\n        box_str = '\\n'.join([f'{instance[\"category\"]}: {instance[\"bbox\"]}' for instance in inst['instances']])\n\n        category = json.loads(ques_js)['category']\n        if category in rule_dict:\n            rule = rule_dict[category]\n        else:\n            assert False, f\"Visual QA category not found in rule file: {category}.\"\n        prompt = rule['prompt']\n        role = rule['role']\n        content = (f'[Context]\\n{cap_str}\\n\\n{box_str}\\n\\n'\n                   f'[Question]\\n{ques[\"text\"]}\\n\\n'\n                   f'[{role} 1]\\n{ans1[\"text\"]}\\n\\n[End of {role} 1]\\n\\n'\n                   f'[{role} 2]\\n{ans2[\"text\"]}\\n\\n[End of {role} 2]\\n\\n'\n                   f'[System]\\n{prompt}\\n\\n')\n        cur_js = {\n            'id': idx+1,\n            'question_id': ques['question_id'],\n            'answer1_id': ans1.get('answer_id', ans1['question_id']),\n            'answer2_id': ans2.get('answer_id', ans2['answer_id']),\n            'category': category\n        }\n        if idx >= len(cur_reviews):\n            review = get_eval(content, args.max_tokens)\n            scores = parse_score(review)\n            cur_js['content'] = review\n            cur_js['tuple'] = scores\n            review_file.write(json.dumps(cur_js) + '\\n')\n            review_file.flush()\n        else:\n            print(f'Skipping {idx} as we already have it.')\n        idx += 1\n        print(idx)\n    review_file.close()\n"}
{"type": "source_file", "path": "LLaVA/llava/eval/generate_webpage_data_from_table.py", "content": "\"\"\"Generate json file for webpage.\"\"\"\nimport json\nimport os\nimport re\n\n# models = ['llama', 'alpaca', 'gpt35', 'bard']\nmodels = ['vicuna']\n\n\ndef read_jsonl(path: str, key: str=None):\n    data = []\n    with open(os.path.expanduser(path)) as f:\n        for line in f:\n            if not line:\n                continue\n            data.append(json.loads(line))\n    if key is not None:\n        data.sort(key=lambda x: x[key])\n        data = {item[key]: item for item in data}\n    return data\n\n\ndef trim_hanging_lines(s: str, n: int) -> str:\n    s = s.strip()\n    for _ in range(n):\n        s = s.split('\\n', 1)[1].strip()\n    return s\n\n\nif __name__ == '__main__':\n    questions = read_jsonl('table/question.jsonl', key='question_id')\n\n    # alpaca_answers = read_jsonl('table/answer/answer_alpaca-13b.jsonl', key='question_id')\n    # bard_answers = read_jsonl('table/answer/answer_bard.jsonl', key='question_id')\n    # gpt35_answers = read_jsonl('table/answer/answer_gpt35.jsonl', key='question_id')\n    # llama_answers = read_jsonl('table/answer/answer_llama-13b.jsonl', key='question_id')\n    vicuna_answers = read_jsonl('table/answer/answer_vicuna-13b.jsonl', key='question_id')\n    ours_answers = read_jsonl('table/results/llama-13b-hf-alpaca.jsonl', key='question_id')\n\n    review_vicuna = read_jsonl('table/review/review_vicuna-13b_llama-13b-hf-alpaca.jsonl', key='question_id')\n    # review_alpaca = read_jsonl('table/review/review_alpaca-13b_vicuna-13b.jsonl', key='question_id')\n    # review_bard = read_jsonl('table/review/review_bard_vicuna-13b.jsonl', key='question_id')\n    # review_gpt35 = read_jsonl('table/review/review_gpt35_vicuna-13b.jsonl', key='question_id')\n    # review_llama = read_jsonl('table/review/review_llama-13b_vicuna-13b.jsonl', key='question_id')\n\n    records = []\n    for qid in questions.keys():\n        r = {\n            'id': qid,\n            'category': questions[qid]['category'],\n            'question': questions[qid]['text'],\n            'answers': {\n                # 'alpaca': alpaca_answers[qid]['text'],\n                # 'llama': llama_answers[qid]['text'],\n                # 'bard': bard_answers[qid]['text'],\n                # 'gpt35': gpt35_answers[qid]['text'],\n                'vicuna': vicuna_answers[qid]['text'],\n                'ours': ours_answers[qid]['text'],\n            },\n            'evaluations': {\n                # 'alpaca': review_alpaca[qid]['text'],\n                # 'llama': review_llama[qid]['text'],\n                # 'bard': review_bard[qid]['text'],\n                'vicuna': review_vicuna[qid]['content'],\n                # 'gpt35': review_gpt35[qid]['text'],\n            },\n            'scores': {\n                'vicuna': review_vicuna[qid]['tuple'],\n                # 'alpaca': review_alpaca[qid]['score'],\n                # 'llama': review_llama[qid]['score'],\n                # 'bard': review_bard[qid]['score'],\n                # 'gpt35': review_gpt35[qid]['score'],\n            },\n        }\n\n        # cleanup data\n        cleaned_evals = {}\n        for k, v in r['evaluations'].items():\n            v = v.strip()\n            lines = v.split('\\n')\n            # trim the first line if it's a pair of numbers\n            if re.match(r'\\d+[, ]+\\d+', lines[0]):\n                lines = lines[1:]\n            v = '\\n'.join(lines)\n            cleaned_evals[k] = v.replace('Assistant 1', \"**Assistant 1**\").replace('Assistant 2', '**Assistant 2**')\n\n        r['evaluations'] = cleaned_evals\n        records.append(r)\n\n    # Reorder the records, this is optional\n    for r in records:\n        if r['id'] <= 20:\n            r['id'] += 60\n        else:\n            r['id'] -= 20\n    for r in records:\n        if r['id'] <= 50:\n            r['id'] += 10\n        elif 50 < r['id'] <= 60:\n            r['id'] -= 50\n    for r in records:\n        if r['id'] == 7:\n            r['id'] = 1\n        elif r['id'] < 7:\n            r['id'] += 1 \n\n    records.sort(key=lambda x: x['id'])\n\n    # Write to file\n    with open('webpage/data.json', 'w') as f:\n        json.dump({'questions': records, 'models': models}, f, indent=2)\n"}
{"type": "source_file", "path": "LLaVA/llava/eval/eval_textvqa.py", "content": "import os\nimport argparse\nimport json\nimport re\n\nfrom llava.eval.m4c_evaluator import TextVQAAccuracyEvaluator\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--annotation-file', type=str)\n    parser.add_argument('--result-file', type=str)\n    parser.add_argument('--result-dir', type=str)\n    return parser.parse_args()\n\n\ndef prompt_processor(prompt):\n    if prompt.startswith('OCR tokens: '):\n        pattern = r\"Question: (.*?) Short answer:\"\n        match = re.search(pattern, prompt, re.DOTALL)\n        question = match.group(1)\n    elif 'Reference OCR token: ' in prompt and len(prompt.split('\\n')) == 3:\n        if prompt.startswith('Reference OCR token:'):\n            question = prompt.split('\\n')[1]\n        else:\n            question = prompt.split('\\n')[0]\n    elif len(prompt.split('\\n')) == 2:\n        question = prompt.split('\\n')[0]\n    else:\n        assert False\n\n    return question.lower()\n\n\ndef eval_single(annotation_file, result_file):\n    experiment_name = os.path.splitext(os.path.basename(result_file))[0]\n    print(experiment_name)\n    annotations = json.load(open(annotation_file))['data']\n    annotations = {(annotation['image_id'], annotation['question'].lower()): annotation for annotation in annotations}\n    results = [json.loads(line) for line in open(result_file)]\n\n    pred_list = []\n    for result in results:\n        annotation = annotations[(result['question_id'], prompt_processor(result['prompt']))]\n        pred_list.append({\n            \"pred_answer\": result['text'],\n            \"gt_answers\": annotation['answers'],\n        })\n\n    evaluator = TextVQAAccuracyEvaluator()\n    print('Samples: {}\\nAccuracy: {:.2f}%\\n'.format(len(pred_list), 100. * evaluator.eval_pred_list(pred_list)))\n\n\nif __name__ == \"__main__\":\n    args = get_args()\n\n    if args.result_file is not None:\n        eval_single(args.annotation_file, args.result_file)\n\n    if args.result_dir is not None:\n        for result_file in sorted(os.listdir(args.result_dir)):\n            if not result_file.endswith('.jsonl'):\n                print(f'Skipping {result_file}')\n                continue\n            eval_single(args.annotation_file, os.path.join(args.result_dir, result_file))\n"}
{"type": "source_file", "path": "LLaVA/llava/eval/m4c_evaluator.py", "content": "# Copyright (c) Facebook, Inc. and its affiliates.\nimport re\n\nfrom tqdm import tqdm\n\n\nclass EvalAIAnswerProcessor:\n    \"\"\"\n    Processes an answer similar to Eval AI\n        copied from\n        https://github.com/facebookresearch/mmf/blob/c46b3b3391275b4181567db80943473a89ab98ab/pythia/tasks/processors.py#L897\n    \"\"\"\n\n    CONTRACTIONS = {\n        \"aint\": \"ain't\",\n        \"arent\": \"aren't\",\n        \"cant\": \"can't\",\n        \"couldve\": \"could've\",\n        \"couldnt\": \"couldn't\",\n        \"couldn'tve\": \"couldn't've\",\n        \"couldnt've\": \"couldn't've\",\n        \"didnt\": \"didn't\",\n        \"doesnt\": \"doesn't\",\n        \"dont\": \"don't\",\n        \"hadnt\": \"hadn't\",\n        \"hadnt've\": \"hadn't've\",\n        \"hadn'tve\": \"hadn't've\",\n        \"hasnt\": \"hasn't\",\n        \"havent\": \"haven't\",\n        \"hed\": \"he'd\",\n        \"hed've\": \"he'd've\",\n        \"he'dve\": \"he'd've\",\n        \"hes\": \"he's\",\n        \"howd\": \"how'd\",\n        \"howll\": \"how'll\",\n        \"hows\": \"how's\",\n        \"Id've\": \"I'd've\",\n        \"I'dve\": \"I'd've\",\n        \"Im\": \"I'm\",\n        \"Ive\": \"I've\",\n        \"isnt\": \"isn't\",\n        \"itd\": \"it'd\",\n        \"itd've\": \"it'd've\",\n        \"it'dve\": \"it'd've\",\n        \"itll\": \"it'll\",\n        \"let's\": \"let's\",\n        \"maam\": \"ma'am\",\n        \"mightnt\": \"mightn't\",\n        \"mightnt've\": \"mightn't've\",\n        \"mightn'tve\": \"mightn't've\",\n        \"mightve\": \"might've\",\n        \"mustnt\": \"mustn't\",\n        \"mustve\": \"must've\",\n        \"neednt\": \"needn't\",\n        \"notve\": \"not've\",\n        \"oclock\": \"o'clock\",\n        \"oughtnt\": \"oughtn't\",\n        \"ow's'at\": \"'ow's'at\",\n        \"'ows'at\": \"'ow's'at\",\n        \"'ow'sat\": \"'ow's'at\",\n        \"shant\": \"shan't\",\n        \"shed've\": \"she'd've\",\n        \"she'dve\": \"she'd've\",\n        \"she's\": \"she's\",\n        \"shouldve\": \"should've\",\n        \"shouldnt\": \"shouldn't\",\n        \"shouldnt've\": \"shouldn't've\",\n        \"shouldn'tve\": \"shouldn't've\",\n        \"somebody'd\": \"somebodyd\",\n        \"somebodyd've\": \"somebody'd've\",\n        \"somebody'dve\": \"somebody'd've\",\n        \"somebodyll\": \"somebody'll\",\n        \"somebodys\": \"somebody's\",\n        \"someoned\": \"someone'd\",\n        \"someoned've\": \"someone'd've\",\n        \"someone'dve\": \"someone'd've\",\n        \"someonell\": \"someone'll\",\n        \"someones\": \"someone's\",\n        \"somethingd\": \"something'd\",\n        \"somethingd've\": \"something'd've\",\n        \"something'dve\": \"something'd've\",\n        \"somethingll\": \"something'll\",\n        \"thats\": \"that's\",\n        \"thered\": \"there'd\",\n        \"thered've\": \"there'd've\",\n        \"there'dve\": \"there'd've\",\n        \"therere\": \"there're\",\n        \"theres\": \"there's\",\n        \"theyd\": \"they'd\",\n        \"theyd've\": \"they'd've\",\n        \"they'dve\": \"they'd've\",\n        \"theyll\": \"they'll\",\n        \"theyre\": \"they're\",\n        \"theyve\": \"they've\",\n        \"twas\": \"'twas\",\n        \"wasnt\": \"wasn't\",\n        \"wed've\": \"we'd've\",\n        \"we'dve\": \"we'd've\",\n        \"weve\": \"we've\",\n        \"werent\": \"weren't\",\n        \"whatll\": \"what'll\",\n        \"whatre\": \"what're\",\n        \"whats\": \"what's\",\n        \"whatve\": \"what've\",\n        \"whens\": \"when's\",\n        \"whered\": \"where'd\",\n        \"wheres\": \"where's\",\n        \"whereve\": \"where've\",\n        \"whod\": \"who'd\",\n        \"whod've\": \"who'd've\",\n        \"who'dve\": \"who'd've\",\n        \"wholl\": \"who'll\",\n        \"whos\": \"who's\",\n        \"whove\": \"who've\",\n        \"whyll\": \"why'll\",\n        \"whyre\": \"why're\",\n        \"whys\": \"why's\",\n        \"wont\": \"won't\",\n        \"wouldve\": \"would've\",\n        \"wouldnt\": \"wouldn't\",\n        \"wouldnt've\": \"wouldn't've\",\n        \"wouldn'tve\": \"wouldn't've\",\n        \"yall\": \"y'all\",\n        \"yall'll\": \"y'all'll\",\n        \"y'allll\": \"y'all'll\",\n        \"yall'd've\": \"y'all'd've\",\n        \"y'alld've\": \"y'all'd've\",\n        \"y'all'dve\": \"y'all'd've\",\n        \"youd\": \"you'd\",\n        \"youd've\": \"you'd've\",\n        \"you'dve\": \"you'd've\",\n        \"youll\": \"you'll\",\n        \"youre\": \"you're\",\n        \"youve\": \"you've\",\n    }\n\n    NUMBER_MAP = {\n        \"none\": \"0\",\n        \"zero\": \"0\",\n        \"one\": \"1\",\n        \"two\": \"2\",\n        \"three\": \"3\",\n        \"four\": \"4\",\n        \"five\": \"5\",\n        \"six\": \"6\",\n        \"seven\": \"7\",\n        \"eight\": \"8\",\n        \"nine\": \"9\",\n        \"ten\": \"10\",\n    }\n    ARTICLES = [\"a\", \"an\", \"the\"]\n    PERIOD_STRIP = re.compile(r\"(?!<=\\d)(\\.)(?!\\d)\")\n    COMMA_STRIP = re.compile(r\"(?<=\\d)(\\,)+(?=\\d)\")\n    PUNCTUATIONS = [\n        \";\",\n        r\"/\",\n        \"[\",\n        \"]\",\n        '\"',\n        \"{\",\n        \"}\",\n        \"(\",\n        \")\",\n        \"=\",\n        \"+\",\n        \"\\\\\",\n        \"_\",\n        \"-\",\n        \">\",\n        \"<\",\n        \"@\",\n        \"`\",\n        \",\",\n        \"?\",\n        \"!\",\n    ]\n\n    def __init__(self, *args, **kwargs):\n        pass\n\n    def word_tokenize(self, word):\n        word = word.lower()\n        word = word.replace(\",\", \"\").replace(\"?\", \"\").replace(\"'s\", \" 's\")\n        return word.strip()\n\n    def process_punctuation(self, in_text):\n        out_text = in_text\n        for p in self.PUNCTUATIONS:\n            if (p + \" \" in in_text or \" \" + p in in_text) or (\n                re.search(self.COMMA_STRIP, in_text) is not None\n            ):\n                out_text = out_text.replace(p, \"\")\n            else:\n                out_text = out_text.replace(p, \" \")\n        out_text = self.PERIOD_STRIP.sub(\"\", out_text, re.UNICODE)\n        return out_text\n\n    def process_digit_article(self, in_text):\n        out_text = []\n        temp_text = in_text.lower().split()\n        for word in temp_text:\n            word = self.NUMBER_MAP.setdefault(word, word)\n            if word not in self.ARTICLES:\n                out_text.append(word)\n            else:\n                pass\n        for word_id, word in enumerate(out_text):\n            if word in self.CONTRACTIONS:\n                out_text[word_id] = self.CONTRACTIONS[word]\n        out_text = \" \".join(out_text)\n        return out_text\n\n    def __call__(self, item):\n        item = self.word_tokenize(item)\n        item = item.replace(\"\\n\", \" \").replace(\"\\t\", \" \").strip()\n        item = self.process_punctuation(item)\n        item = self.process_digit_article(item)\n        return item\n\n\nclass TextVQAAccuracyEvaluator:\n    def __init__(self):\n        self.answer_processor = EvalAIAnswerProcessor()\n\n    def _compute_answer_scores(self, raw_answers):\n        \"\"\"\n        compute the accuracy (soft score) of human answers\n        \"\"\"\n        answers = [self.answer_processor(a) for a in raw_answers]\n        assert len(answers) == 10\n        gt_answers = list(enumerate(answers))\n        unique_answers = set(answers)\n        unique_answer_scores = {}\n\n        for unique_answer in unique_answers:\n            accs = []\n            for gt_answer in gt_answers:\n                other_answers = [item for item in gt_answers if item != gt_answer]\n                matching_answers = [\n                    item for item in other_answers if item[1] == unique_answer\n                ]\n                acc = min(1, float(len(matching_answers)) / 3)\n                accs.append(acc)\n            unique_answer_scores[unique_answer] = sum(accs) / len(accs)\n\n        return unique_answer_scores\n\n    def eval_pred_list(self, pred_list):\n        pred_scores = []\n        for entry in tqdm(pred_list):\n            pred_answer = self.answer_processor(entry[\"pred_answer\"])\n            unique_answer_scores = self._compute_answer_scores(entry[\"gt_answers\"])\n            score = unique_answer_scores.get(pred_answer, 0.0)\n            pred_scores.append(score)\n\n        accuracy = sum(pred_scores) / len(pred_scores)\n        return accuracy\n\n\nclass STVQAAccuracyEvaluator:\n    def __init__(self):\n        self.answer_processor = EvalAIAnswerProcessor()\n\n    def eval_pred_list(self, pred_list):\n        pred_scores = []\n        for entry in pred_list:\n            pred_answer = self.answer_processor(entry[\"pred_answer\"])\n            gts = [self.answer_processor(a) for a in entry[\"gt_answers\"]]\n            score = 1.0 if pred_answer in gts else 0.0\n            pred_scores.append(score)\n\n        accuracy = sum(pred_scores) / len(pred_scores)\n        return accuracy\n\n\nclass STVQAANLSEvaluator:\n    def __init__(self):\n        import editdistance  # install with `pip install editdistance`\n\n        self.get_edit_distance = editdistance.eval\n\n    def get_anls(self, s1, s2):\n        s1 = s1.lower().strip()\n        s2 = s2.lower().strip()\n        iou = 1 - self.get_edit_distance(s1, s2) / max(len(s1), len(s2))\n        anls = iou if iou >= 0.5 else 0.0\n        return anls\n\n    def eval_pred_list(self, pred_list):\n        pred_scores = []\n        for entry in pred_list:\n            anls = max(\n                self.get_anls(entry[\"pred_answer\"], gt) for gt in entry[\"gt_answers\"]\n            )\n            pred_scores.append(anls)\n\n        accuracy = sum(pred_scores) / len(pred_scores)\n        return accuracy\n\n\nclass TextCapsBleu4Evaluator:\n    def __init__(self):\n        # The following script requires Java 1.8.0 and pycocotools installed.\n        # The pycocoevalcap can be installed with pip as\n        # pip install git+https://github.com/ronghanghu/coco-caption.git@python23\n        # Original pycocoevalcap code is at https://github.com/tylin/coco-caption\n        # but has no python3 support yet.\n        try:\n            from pycocoevalcap.bleu.bleu import Bleu\n            from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\n        except ModuleNotFoundError:\n            print(\n                \"Please install pycocoevalcap module using \"\n                \"pip install git+https://github.com/ronghanghu/coco-caption.git@python23\"  # noqa\n            )\n            raise\n\n        self.tokenizer = PTBTokenizer()\n        self.scorer = Bleu(4)\n\n    def eval_pred_list(self, pred_list):\n        # Create reference and hypotheses captions.\n        gts = {}\n        res = {}\n        for idx, entry in enumerate(pred_list):\n            gts[idx] = [{\"caption\": a} for a in entry[\"gt_answers\"]]\n            res[idx] = [{\"caption\": entry[\"pred_answer\"]}]\n\n        gts = self.tokenizer.tokenize(gts)\n        res = self.tokenizer.tokenize(res)\n        score, _ = self.scorer.compute_score(gts, res)\n\n        bleu4 = score[3]  # score is (Bleu-1, Bleu-2, Bleu-3, Bleu-4)\n        return bleu4\n"}
{"type": "source_file", "path": "LLaVA/llava/eval/eval_pope.py", "content": "import os\nimport json\nimport argparse\n\ndef eval_pope(answers, label_file):\n    label_list = [json.loads(q)['label'] for q in open(label_file, 'r')]\n\n    for answer in answers:\n        text = answer['text']\n\n        # Only keep the first sentence\n        if text.find('.') != -1:\n            text = text.split('.')[0]\n\n        text = text.replace(',', '')\n        words = text.split(' ')\n        if 'No' in words or 'not' in words or 'no' in words:\n            answer['text'] = 'no'\n        else:\n            answer['text'] = 'yes'\n\n    for i in range(len(label_list)):\n        if label_list[i] == 'no':\n            label_list[i] = 0\n        else:\n            label_list[i] = 1\n\n    pred_list = []\n    for answer in answers:\n        if answer['text'] == 'no':\n            pred_list.append(0)\n        else:\n            pred_list.append(1)\n\n    pos = 1\n    neg = 0\n    yes_ratio = pred_list.count(1) / len(pred_list)\n\n    TP, TN, FP, FN = 0, 0, 0, 0\n    for pred, label in zip(pred_list, label_list):\n        if pred == pos and label == pos:\n            TP += 1\n        elif pred == pos and label == neg:\n            FP += 1\n        elif pred == neg and label == neg:\n            TN += 1\n        elif pred == neg and label == pos:\n            FN += 1\n\n    print('TP\\tFP\\tTN\\tFN\\t')\n    print('{}\\t{}\\t{}\\t{}'.format(TP, FP, TN, FN))\n\n    precision = float(TP) / float(TP + FP)\n    recall = float(TP) / float(TP + FN)\n    f1 = 2*precision*recall / (precision + recall)\n    acc = (TP + TN) / (TP + TN + FP + FN)\n    print('Accuracy: {}'.format(acc))\n    print('Precision: {}'.format(precision))\n    print('Recall: {}'.format(recall))\n    print('F1 score: {}'.format(f1))\n    print('Yes ratio: {}'.format(yes_ratio))\n    print('%.3f, %.3f, %.3f, %.3f, %.3f' % (f1, acc, precision, recall, yes_ratio) )\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--annotation-dir\", type=str)\n    parser.add_argument(\"--question-file\", type=str)\n    parser.add_argument(\"--result-file\", type=str)\n    args = parser.parse_args()\n\n    questions = [json.loads(line) for line in open(args.question_file)]\n    questions = {question['question_id']: question for question in questions}\n    answers = [json.loads(q) for q in open(args.result_file)]\n    for file in os.listdir(args.annotation_dir):\n        assert file.startswith('coco_pope_')\n        assert file.endswith('.json')\n        category = file[10:-5]\n        cur_answers = [x for x in answers if questions[x['question_id']]['category'] == category]\n        print('Category: {}, # samples: {}'.format(category, len(cur_answers)))\n        eval_pope(cur_answers, os.path.join(args.annotation_dir, file))\n        print(\"====================================\")\n"}
{"type": "source_file", "path": "LLaVA/llava/eval/eval_science_qa.py", "content": "import argparse\nimport json\nimport os\nimport re\nimport random\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--base-dir', type=str)\n    parser.add_argument('--result-file', type=str)\n    parser.add_argument('--output-file', type=str)\n    parser.add_argument('--output-result', type=str)\n    parser.add_argument('--split', type=str, default='test')\n    parser.add_argument('--options', type=list, default=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n    return parser.parse_args()\n\n\ndef convert_caps(results):\n    fakecaps = []\n    for result in results:\n        image_id = result['question_id']\n        caption = result['text']\n        fakecaps.append({\"image_id\": int(image_id), \"caption\": caption})\n    return fakecaps\n\n\ndef get_pred_idx(prediction, choices, options):\n    \"\"\"\n    Get the index (e.g. 2) from the prediction (e.g. 'C')\n    \"\"\"\n    if prediction in options[:len(choices)]:\n        return options.index(prediction)\n    else:\n        return -1\n        return random.choice(range(len(choices)))\n\n\nif __name__ == \"__main__\":\n    args = get_args()\n\n    base_dir = args.base_dir\n    split_indices = json.load(open(os.path.join(base_dir, \"pid_splits.json\")))[args.split]\n    problems = json.load(open(os.path.join(base_dir, \"problems.json\")))\n    predictions = [json.loads(line) for line in open(args.result_file)]\n    predictions = {pred['question_id']: pred for pred in predictions}\n    split_problems = {idx: problems[idx] for idx in split_indices}\n\n    results = {'correct': [], 'incorrect': []}\n    sqa_results = {}\n    sqa_results['acc'] = None\n    sqa_results['correct'] = None\n    sqa_results['count'] = None\n    sqa_results['results'] = {}\n    sqa_results['outputs'] = {}\n\n    for prob_id, prob in split_problems.items():\n        if prob_id not in predictions:\n            pred = {'text': 'FAILED', 'prompt': 'Unknown'}\n            pred_text = 'FAILED'\n        else:\n            pred = predictions[prob_id]\n            pred_text = pred['text']\n\n        if pred_text in args.options:\n            answer = pred_text\n        elif len(pred_text) >= 3 and pred_text[0] in args.options and pred_text[1:3] == \". \":\n            answer = pred_text[0]\n        else:\n            pattern = re.compile(r'The answer is ([A-Z]).')\n            res = pattern.findall(pred_text)\n            if len(res) == 1:\n                answer = res[0]  # 'A', 'B', ...\n            else:\n                answer = \"FAILED\"\n\n        pred_idx = get_pred_idx(answer, prob['choices'], args.options)\n\n        analysis = {\n            'question_id': prob_id,\n            'parsed_ans': answer,\n            'ground_truth': args.options[prob['answer']],\n            'question': pred['prompt'],\n            'pred': pred_text,\n            'is_multimodal': '<image>' in pred['prompt'],\n        }\n\n        sqa_results['results'][prob_id] = get_pred_idx(answer, prob['choices'], args.options)\n        sqa_results['outputs'][prob_id] = pred_text\n\n        if pred_idx == prob['answer']:\n            results['correct'].append(analysis)\n        else:\n            results['incorrect'].append(analysis)\n\n    correct = len(results['correct'])\n    total = len(results['correct']) + len(results['incorrect'])\n\n    ###### IMG ######\n    multimodal_correct = len([x for x in results['correct'] if x['is_multimodal']])\n    multimodal_incorrect = len([x for x in results['incorrect'] if x['is_multimodal']])\n    multimodal_total = multimodal_correct + multimodal_incorrect\n    ###### IMG ######\n\n    print(f'Total: {total}, Correct: {correct}, Accuracy: {correct / total * 100:.2f}%, IMG-Accuracy: {multimodal_correct / multimodal_total * 100:.2f}%')\n\n    sqa_results['acc'] = correct / total * 100\n    sqa_results['correct'] = correct\n    sqa_results['count'] = total\n\n    with open(args.output_file, 'w') as f:\n        json.dump(results, f, indent=2)\n    with open(args.output_result, 'w') as f:\n        json.dump(sqa_results, f, indent=2)\n"}
{"type": "source_file", "path": "LLaVA/llava/eval/eval_gpt_review.py", "content": "import argparse\nimport json\nimport os\n\nimport openai\nimport tqdm\nimport ray\nimport time\n\nNUM_SECONDS_TO_SLEEP = 3\n\n@ray.remote(num_cpus=4)\ndef get_eval(content: str, max_tokens: int):\n    while True:\n        try:\n            response = openai.ChatCompletion.create(\n                model='gpt-4',\n                messages=[{\n                    'role': 'system',\n                    'content': 'You are a helpful and precise assistant for checking the quality of the answer.'\n                }, {\n                    'role': 'user',\n                    'content': content,\n                }],\n                temperature=0.2,  # TODO: figure out which temperature is best for evaluation\n                max_tokens=max_tokens,\n            )\n            break\n        except openai.error.RateLimitError:\n            pass\n        except Exception as e:\n            print(e)\n        time.sleep(NUM_SECONDS_TO_SLEEP)\n\n    print('success!')\n    return response['choices'][0]['message']['content']\n\n\ndef parse_score(review):\n    try:\n        score_pair = review.split('\\n')[0]\n        score_pair = score_pair.replace(',', ' ')\n        sp = score_pair.split(' ')\n        if len(sp) == 2:\n            return [float(sp[0]), float(sp[1])]\n        else:\n            print('error', review)\n            return [-1, -1]\n    except Exception as e:\n        print(e)\n        print('error', review)\n        return [-1, -1]\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='ChatGPT-based QA evaluation.')\n    parser.add_argument('-q', '--question')\n    # parser.add_argument('-a', '--answer')\n    parser.add_argument('-a', '--answer-list', nargs='+', default=[])\n    parser.add_argument('-r', '--rule')\n    parser.add_argument('-o', '--output')\n    parser.add_argument('--max-tokens', type=int, default=1024, help='maximum number of tokens produced in the output')\n    args = parser.parse_args()\n\n    ray.init()\n\n    f_q = open(os.path.expanduser(args.question))\n    f_ans1 = open(os.path.expanduser(args.answer_list[0]))\n    f_ans2 = open(os.path.expanduser(args.answer_list[1]))\n    rule_dict = json.load(open(os.path.expanduser(args.rule), 'r'))\n\n    review_file = open(f'{args.output}', 'w')\n\n    js_list = []\n    handles = []\n    idx = 0\n    for ques_js, ans1_js, ans2_js in zip(f_q, f_ans1, f_ans2):\n        # if idx == 1:\n        #     break\n\n        ques = json.loads(ques_js)\n        ans1 = json.loads(ans1_js)\n        ans2 = json.loads(ans2_js)\n\n        category = json.loads(ques_js)['category']\n        if category in rule_dict:\n            rule = rule_dict[category]\n        else:\n            rule = rule_dict['default']\n        prompt = rule['prompt']\n        role = rule['role']\n        content = (f'[Question]\\n{ques[\"text\"]}\\n\\n'\n                   f'[{role} 1]\\n{ans1[\"text\"]}\\n\\n[End of {role} 1]\\n\\n'\n                   f'[{role} 2]\\n{ans2[\"text\"]}\\n\\n[End of {role} 2]\\n\\n'\n                   f'[System]\\n{prompt}\\n\\n')\n        js_list.append({\n            'id': idx+1,\n            'question_id': ques['question_id'],\n            'answer1_id': ans1['answer_id'],\n            'answer2_id': ans2['answer_id'],\n            'category': category})\n        idx += 1\n        handles.append(get_eval.remote(content, args.max_tokens))\n        # To avoid the rate limit set by OpenAI\n        time.sleep(NUM_SECONDS_TO_SLEEP)\n\n    reviews = ray.get(handles)\n    for idx, review in enumerate(reviews):\n        scores = parse_score(review)\n        js_list[idx]['content'] = review\n        js_list[idx]['tuple'] = scores\n        review_file.write(json.dumps(js_list[idx]) + '\\n')\n    review_file.close()\n"}
{"type": "source_file", "path": "Image_Purifier/guided_diffusion/unet.py", "content": "# ---------------------------------------------------------------\n# Taken from the following link as is from:\n# https://github.com/openai/guided-diffusion/blob/main/guided_diffusion/unet.py\n#\n# The license for the original version of this file can be\n# found in this directory (LICENSE_GUIDED_DIFFUSION).\n# ---------------------------------------------------------------\n\nfrom abc import abstractmethod\n\nimport math\n\nimport numpy as np\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .fp16_util import convert_module_to_f16, convert_module_to_f32\nfrom .nn import (\n    checkpoint,\n    conv_nd,\n    linear,\n    avg_pool_nd,\n    zero_module,\n    normalization,\n    timestep_embedding,\n)\n\n\nclass AttentionPool2d(nn.Module):\n    \"\"\"\n    Adapted from CLIP: https://github.com/openai/CLIP/blob/main/clip/model.py\n    \"\"\"\n\n    def __init__(\n        self,\n        spacial_dim: int,\n        embed_dim: int,\n        num_heads_channels: int,\n        output_dim: int = None,\n    ):\n        super().__init__()\n        self.positional_embedding = nn.Parameter(\n            th.randn(embed_dim, spacial_dim ** 2 + 1) / embed_dim ** 0.5\n        )\n        self.qkv_proj = conv_nd(1, embed_dim, 3 * embed_dim, 1)\n        self.c_proj = conv_nd(1, embed_dim, output_dim or embed_dim, 1)\n        self.num_heads = embed_dim // num_heads_channels\n        self.attention = QKVAttention(self.num_heads)\n\n    def forward(self, x):\n        b, c, *_spatial = x.shape\n        x = x.reshape(b, c, -1)  # NC(HW)\n        x = th.cat([x.mean(dim=-1, keepdim=True), x], dim=-1)  # NC(HW+1)\n        x = x + self.positional_embedding[None, :, :].to(x.dtype)  # NC(HW+1)\n        x = self.qkv_proj(x)\n        x = self.attention(x)\n        x = self.c_proj(x)\n        return x[:, :, 0]\n\n\nclass TimestepBlock(nn.Module):\n    \"\"\"\n    Any module where forward() takes timestep embeddings as a second argument.\n    \"\"\"\n\n    @abstractmethod\n    def forward(self, x, emb):\n        \"\"\"\n        Apply the module to `x` given `emb` timestep embeddings.\n        \"\"\"\n\n\nclass TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n    \"\"\"\n    A sequential module that passes timestep embeddings to the children that\n    support it as an extra input.\n    \"\"\"\n\n    def forward(self, x, emb):\n        for layer in self:\n            if isinstance(layer, TimestepBlock):\n                x = layer(x, emb)\n            else:\n                x = layer(x)\n        return x\n\n\nclass Upsample(nn.Module):\n    \"\"\"\n    An upsampling layer with an optional convolution.\n\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n                 upsampling occurs in the inner-two dimensions.\n    \"\"\"\n\n    def __init__(self, channels, use_conv, dims=2, out_channels=None):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.dims = dims\n        if use_conv:\n            self.conv = conv_nd(dims, self.channels, self.out_channels, 3, padding=1)\n\n    def forward(self, x):\n        assert x.shape[1] == self.channels\n        if self.dims == 3:\n            x = F.interpolate(\n                x, (x.shape[2], x.shape[3] * 2, x.shape[4] * 2), mode=\"nearest\"\n            )\n        else:\n            x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n        if self.use_conv:\n            x = self.conv(x)\n        return x\n\n\nclass Downsample(nn.Module):\n    \"\"\"\n    A downsampling layer with an optional convolution.\n\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n                 downsampling occurs in the inner-two dimensions.\n    \"\"\"\n\n    def __init__(self, channels, use_conv, dims=2, out_channels=None):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.dims = dims\n        stride = 2 if dims != 3 else (1, 2, 2)\n        if use_conv:\n            self.op = conv_nd(\n                dims, self.channels, self.out_channels, 3, stride=stride, padding=1\n            )\n        else:\n            assert self.channels == self.out_channels\n            self.op = avg_pool_nd(dims, kernel_size=stride, stride=stride)\n\n    def forward(self, x):\n        assert x.shape[1] == self.channels\n        return self.op(x)\n\n\nclass ResBlock(TimestepBlock):\n    \"\"\"\n    A residual block that can optionally change the number of channels.\n\n    :param channels: the number of input channels.\n    :param emb_channels: the number of timestep embedding channels.\n    :param dropout: the rate of dropout.\n    :param out_channels: if specified, the number of out channels.\n    :param use_conv: if True and out_channels is specified, use a spatial\n        convolution instead of a smaller 1x1 convolution to change the\n        channels in the skip connection.\n    :param dims: determines if the signal is 1D, 2D, or 3D.\n    :param use_checkpoint: if True, use gradient checkpointing on this module.\n    :param up: if True, use this block for upsampling.\n    :param down: if True, use this block for downsampling.\n    \"\"\"\n\n    def __init__(\n        self,\n        channels,\n        emb_channels,\n        dropout,\n        out_channels=None,\n        use_conv=False,\n        use_scale_shift_norm=False,\n        dims=2,\n        use_checkpoint=False,\n        up=False,\n        down=False,\n    ):\n        super().__init__()\n        self.channels = channels\n        self.emb_channels = emb_channels\n        self.dropout = dropout\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.use_checkpoint = use_checkpoint\n        self.use_scale_shift_norm = use_scale_shift_norm\n\n        self.in_layers = nn.Sequential(\n            normalization(channels),\n            nn.SiLU(),\n            conv_nd(dims, channels, self.out_channels, 3, padding=1),\n        )\n\n        self.updown = up or down\n\n        if up:\n            self.h_upd = Upsample(channels, False, dims)\n            self.x_upd = Upsample(channels, False, dims)\n        elif down:\n            self.h_upd = Downsample(channels, False, dims)\n            self.x_upd = Downsample(channels, False, dims)\n        else:\n            self.h_upd = self.x_upd = nn.Identity()\n\n        self.emb_layers = nn.Sequential(\n            nn.SiLU(),\n            linear(\n                emb_channels,\n                2 * self.out_channels if use_scale_shift_norm else self.out_channels,\n            ),\n        )\n        self.out_layers = nn.Sequential(\n            normalization(self.out_channels),\n            nn.SiLU(),\n            nn.Dropout(p=dropout),\n            zero_module(\n                conv_nd(dims, self.out_channels, self.out_channels, 3, padding=1)\n            ),\n        )\n\n        if self.out_channels == channels:\n            self.skip_connection = nn.Identity()\n        elif use_conv:\n            self.skip_connection = conv_nd(\n                dims, channels, self.out_channels, 3, padding=1\n            )\n        else:\n            self.skip_connection = conv_nd(dims, channels, self.out_channels, 1)\n\n    def forward(self, x, emb):\n        \"\"\"\n        Apply the block to a Tensor, conditioned on a timestep embedding.\n\n        :param x: an [N x C x ...] Tensor of features.\n        :param emb: an [N x emb_channels] Tensor of timestep embeddings.\n        :return: an [N x C x ...] Tensor of outputs.\n        \"\"\"\n        return checkpoint(\n            self._forward, (x, emb), self.parameters(), self.use_checkpoint\n        )\n\n    def _forward(self, x, emb):\n        if self.updown:\n            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n            h = in_rest(x)\n            h = self.h_upd(h)\n            x = self.x_upd(x)\n            h = in_conv(h)\n        else:\n            h = self.in_layers(x)\n        emb_out = self.emb_layers(emb).type(h.dtype)\n        while len(emb_out.shape) < len(h.shape):\n            emb_out = emb_out[..., None]\n        if self.use_scale_shift_norm:\n            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n            scale, shift = th.chunk(emb_out, 2, dim=1)\n            h = out_norm(h) * (1 + scale) + shift\n            h = out_rest(h)\n        else:\n            h = h + emb_out\n            h = self.out_layers(h)\n        return self.skip_connection(x) + h\n\n\nclass AttentionBlock(nn.Module):\n    \"\"\"\n    An attention block that allows spatial positions to attend to each other.\n\n    Originally ported from here, but adapted to the N-d case.\n    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\n    \"\"\"\n\n    def __init__(\n        self,\n        channels,\n        num_heads=1,\n        num_head_channels=-1,\n        use_checkpoint=False,\n        use_new_attention_order=False,\n    ):\n        super().__init__()\n        self.channels = channels\n        if num_head_channels == -1:\n            self.num_heads = num_heads\n        else:\n            assert (\n                channels % num_head_channels == 0\n            ), f\"q,k,v channels {channels} is not divisible by num_head_channels {num_head_channels}\"\n            self.num_heads = channels // num_head_channels\n        self.use_checkpoint = use_checkpoint\n        self.norm = normalization(channels)\n        self.qkv = conv_nd(1, channels, channels * 3, 1)\n        if use_new_attention_order:\n            # split qkv before split heads\n            self.attention = QKVAttention(self.num_heads)\n        else:\n            # split heads before split qkv\n            self.attention = QKVAttentionLegacy(self.num_heads)\n\n        self.proj_out = zero_module(conv_nd(1, channels, channels, 1))\n\n    def forward(self, x):\n        return checkpoint(self._forward, (x,), self.parameters(), True)\n\n    def _forward(self, x):\n        b, c, *spatial = x.shape\n        x = x.reshape(b, c, -1)\n        qkv = self.qkv(self.norm(x))\n        h = self.attention(qkv)\n        h = self.proj_out(h)\n        return (x + h).reshape(b, c, *spatial)\n\n\ndef count_flops_attn(model, _x, y):\n    \"\"\"\n    A counter for the `thop` package to count the operations in an\n    attention operation.\n    Meant to be used like:\n        macs, params = thop.profile(\n            model,\n            inputs=(inputs, timestamps),\n            custom_ops={QKVAttention: QKVAttention.count_flops},\n        )\n    \"\"\"\n    b, c, *spatial = y[0].shape\n    num_spatial = int(np.prod(spatial))\n    # We perform two matmuls with the same number of ops.\n    # The first computes the weight matrix, the second computes\n    # the combination of the value vectors.\n    matmul_ops = 2 * b * (num_spatial ** 2) * c\n    model.total_ops += th.DoubleTensor([matmul_ops])\n\n\nclass QKVAttentionLegacy(nn.Module):\n    \"\"\"\n    A module which performs QKV attention. Matches legacy QKVAttention + input/ouput heads shaping\n    \"\"\"\n\n    def __init__(self, n_heads):\n        super().__init__()\n        self.n_heads = n_heads\n\n    def forward(self, qkv):\n        \"\"\"\n        Apply QKV attention.\n\n        :param qkv: an [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.\n        :return: an [N x (H * C) x T] tensor after attention.\n        \"\"\"\n        bs, width, length = qkv.shape\n        assert width % (3 * self.n_heads) == 0\n        ch = width // (3 * self.n_heads)\n        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(ch, dim=1)\n        scale = 1 / math.sqrt(math.sqrt(ch))\n        weight = th.einsum(\n            \"bct,bcs->bts\", q * scale, k * scale\n        )  # More stable with f16 than dividing afterwards\n        weight = th.softmax(weight.float(), dim=-1).type(weight.dtype)\n        a = th.einsum(\"bts,bcs->bct\", weight, v)\n        return a.reshape(bs, -1, length)\n\n    @staticmethod\n    def count_flops(model, _x, y):\n        return count_flops_attn(model, _x, y)\n\n\nclass QKVAttention(nn.Module):\n    \"\"\"\n    A module which performs QKV attention and splits in a different order.\n    \"\"\"\n\n    def __init__(self, n_heads):\n        super().__init__()\n        self.n_heads = n_heads\n\n    def forward(self, qkv):\n        \"\"\"\n        Apply QKV attention.\n\n        :param qkv: an [N x (3 * H * C) x T] tensor of Qs, Ks, and Vs.\n        :return: an [N x (H * C) x T] tensor after attention.\n        \"\"\"\n        bs, width, length = qkv.shape\n        assert width % (3 * self.n_heads) == 0\n        ch = width // (3 * self.n_heads)\n        q, k, v = qkv.chunk(3, dim=1)\n        scale = 1 / math.sqrt(math.sqrt(ch))\n        weight = th.einsum(\n            \"bct,bcs->bts\",\n            (q * scale).view(bs * self.n_heads, ch, length),\n            (k * scale).view(bs * self.n_heads, ch, length),\n        )  # More stable with f16 than dividing afterwards\n        weight = th.softmax(weight.float(), dim=-1).type(weight.dtype)\n        a = th.einsum(\"bts,bcs->bct\", weight, v.reshape(bs * self.n_heads, ch, length))\n        return a.reshape(bs, -1, length)\n\n    @staticmethod\n    def count_flops(model, _x, y):\n        return count_flops_attn(model, _x, y)\n\n\nclass UNetModel(nn.Module):\n    \"\"\"\n    The full UNet model with attention and timestep embedding.\n\n    :param in_channels: channels in the input Tensor.\n    :param model_channels: base channel count for the model.\n    :param out_channels: channels in the output Tensor.\n    :param num_res_blocks: number of residual blocks per downsample.\n    :param attention_resolutions: a collection of downsample rates at which\n        attention will take place. May be a set, list, or tuple.\n        For example, if this contains 4, then at 4x downsampling, attention\n        will be used.\n    :param dropout: the dropout probability.\n    :param channel_mult: channel multiplier for each level of the UNet.\n    :param conv_resample: if True, use learned convolutions for upsampling and\n        downsampling.\n    :param dims: determines if the signal is 1D, 2D, or 3D.\n    :param num_classes: if specified (as an int), then this model will be\n        class-conditional with `num_classes` classes.\n    :param use_checkpoint: use gradient checkpointing to reduce memory usage.\n    :param num_heads: the number of attention heads in each attention layer.\n    :param num_heads_channels: if specified, ignore num_heads and instead use\n                               a fixed channel width per attention head.\n    :param num_heads_upsample: works with num_heads to set a different number\n                               of heads for upsampling. Deprecated.\n    :param use_scale_shift_norm: use a FiLM-like conditioning mechanism.\n    :param resblock_updown: use residual blocks for up/downsampling.\n    :param use_new_attention_order: use a different attention pattern for potentially\n                                    increased efficiency.\n    \"\"\"\n\n    def __init__(\n        self,\n        image_size,\n        in_channels,\n        model_channels,\n        out_channels,\n        num_res_blocks,\n        attention_resolutions,\n        dropout=0,\n        channel_mult=(1, 2, 4, 8),\n        conv_resample=True,\n        dims=2,\n        num_classes=None,\n        use_checkpoint=False,\n        use_fp16=False,\n        num_heads=1,\n        num_head_channels=-1,\n        num_heads_upsample=-1,\n        use_scale_shift_norm=False,\n        resblock_updown=False,\n        use_new_attention_order=False,\n    ):\n        super().__init__()\n\n        if num_heads_upsample == -1:\n            num_heads_upsample = num_heads\n\n        self.image_size = image_size\n        self.in_channels = in_channels\n        self.model_channels = model_channels\n        self.out_channels = out_channels\n        self.num_res_blocks = num_res_blocks\n        self.attention_resolutions = attention_resolutions\n        self.dropout = dropout\n        self.channel_mult = channel_mult\n        self.conv_resample = conv_resample\n        self.num_classes = num_classes\n        self.use_checkpoint = use_checkpoint\n        self.dtype = th.float16 if use_fp16 else th.float32\n        self.num_heads = num_heads\n        self.num_head_channels = num_head_channels\n        self.num_heads_upsample = num_heads_upsample\n\n        time_embed_dim = model_channels * 4\n        self.time_embed = nn.Sequential(\n            linear(model_channels, time_embed_dim),\n            nn.SiLU(),\n            linear(time_embed_dim, time_embed_dim),\n        )\n\n        if self.num_classes is not None:\n            self.label_emb = nn.Embedding(num_classes, time_embed_dim)\n\n        ch = input_ch = int(channel_mult[0] * model_channels)\n        self.input_blocks = nn.ModuleList(\n            [TimestepEmbedSequential(conv_nd(dims, in_channels, ch, 3, padding=1))]\n        )\n        self._feature_size = ch\n        input_block_chans = [ch]\n        ds = 1\n        for level, mult in enumerate(channel_mult):\n            for _ in range(num_res_blocks):\n                layers = [\n                    ResBlock(\n                        ch,\n                        time_embed_dim,\n                        dropout,\n                        out_channels=int(mult * model_channels),\n                        dims=dims,\n                        use_checkpoint=use_checkpoint,\n                        use_scale_shift_norm=use_scale_shift_norm,\n                    )\n                ]\n                ch = int(mult * model_channels)\n                if ds in attention_resolutions:\n                    layers.append(\n                        AttentionBlock(\n                            ch,\n                            use_checkpoint=use_checkpoint,\n                            num_heads=num_heads,\n                            num_head_channels=num_head_channels,\n                            use_new_attention_order=use_new_attention_order,\n                        )\n                    )\n                self.input_blocks.append(TimestepEmbedSequential(*layers))\n                self._feature_size += ch\n                input_block_chans.append(ch)\n            if level != len(channel_mult) - 1:\n                out_ch = ch\n                self.input_blocks.append(\n                    TimestepEmbedSequential(\n                        ResBlock(\n                            ch,\n                            time_embed_dim,\n                            dropout,\n                            out_channels=out_ch,\n                            dims=dims,\n                            use_checkpoint=use_checkpoint,\n                            use_scale_shift_norm=use_scale_shift_norm,\n                            down=True,\n                        )\n                        if resblock_updown\n                        else Downsample(\n                            ch, conv_resample, dims=dims, out_channels=out_ch\n                        )\n                    )\n                )\n                ch = out_ch\n                input_block_chans.append(ch)\n                ds *= 2\n                self._feature_size += ch\n\n        self.middle_block = TimestepEmbedSequential(\n            ResBlock(\n                ch,\n                time_embed_dim,\n                dropout,\n                dims=dims,\n                use_checkpoint=use_checkpoint,\n                use_scale_shift_norm=use_scale_shift_norm,\n            ),\n            AttentionBlock(\n                ch,\n                use_checkpoint=use_checkpoint,\n                num_heads=num_heads,\n                num_head_channels=num_head_channels,\n                use_new_attention_order=use_new_attention_order,\n            ),\n            ResBlock(\n                ch,\n                time_embed_dim,\n                dropout,\n                dims=dims,\n                use_checkpoint=use_checkpoint,\n                use_scale_shift_norm=use_scale_shift_norm,\n            ),\n        )\n        self._feature_size += ch\n\n        self.output_blocks = nn.ModuleList([])\n        for level, mult in list(enumerate(channel_mult))[::-1]:\n            for i in range(num_res_blocks + 1):\n                ich = input_block_chans.pop()\n                layers = [\n                    ResBlock(\n                        ch + ich,\n                        time_embed_dim,\n                        dropout,\n                        out_channels=int(model_channels * mult),\n                        dims=dims,\n                        use_checkpoint=use_checkpoint,\n                        use_scale_shift_norm=use_scale_shift_norm,\n                    )\n                ]\n                ch = int(model_channels * mult)\n                if ds in attention_resolutions:\n                    layers.append(\n                        AttentionBlock(\n                            ch,\n                            use_checkpoint=use_checkpoint,\n                            num_heads=num_heads_upsample,\n                            num_head_channels=num_head_channels,\n                            use_new_attention_order=use_new_attention_order,\n                        )\n                    )\n                if level and i == num_res_blocks:\n                    out_ch = ch\n                    layers.append(\n                        ResBlock(\n                            ch,\n                            time_embed_dim,\n                            dropout,\n                            out_channels=out_ch,\n                            dims=dims,\n                            use_checkpoint=use_checkpoint,\n                            use_scale_shift_norm=use_scale_shift_norm,\n                            up=True,\n                        )\n                        if resblock_updown\n                        else Upsample(ch, conv_resample, dims=dims, out_channels=out_ch)\n                    )\n                    ds //= 2\n                self.output_blocks.append(TimestepEmbedSequential(*layers))\n                self._feature_size += ch\n\n        self.out = nn.Sequential(\n            normalization(ch),\n            nn.SiLU(),\n            zero_module(conv_nd(dims, input_ch, out_channels, 3, padding=1)),\n        )\n\n    def convert_to_fp16(self):\n        \"\"\"\n        Convert the torso of the model to float16.\n        \"\"\"\n        self.input_blocks.apply(convert_module_to_f16)\n        self.middle_block.apply(convert_module_to_f16)\n        self.output_blocks.apply(convert_module_to_f16)\n\n    def convert_to_fp32(self):\n        \"\"\"\n        Convert the torso of the model to float32.\n        \"\"\"\n        self.input_blocks.apply(convert_module_to_f32)\n        self.middle_block.apply(convert_module_to_f32)\n        self.output_blocks.apply(convert_module_to_f32)\n\n    def forward(self, x, timesteps, y=None):\n        \"\"\"\n        Apply the model to an input batch.\n\n        :param x: an [N x C x ...] Tensor of inputs.\n        :param timesteps: a 1-D batch of timesteps.\n        :param y: an [N] Tensor of labels, if class-conditional.\n        :return: an [N x C x ...] Tensor of outputs.\n        \"\"\"\n        assert (y is not None) == (\n            self.num_classes is not None\n        ), \"must specify y if and only if the model is class-conditional\"\n\n        hs = []\n        emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))\n\n        if self.num_classes is not None:\n            assert y.shape == (x.shape[0],)\n            emb = emb + self.label_emb(y)\n\n        h = x.type(self.dtype)\n        for module in self.input_blocks:\n            h = module(h, emb)\n            hs.append(h)\n        h = self.middle_block(h, emb)\n        for module in self.output_blocks:\n            h = th.cat([h, hs.pop()], dim=1)\n            h = module(h, emb)\n        h = h.type(x.dtype)\n        return self.out(h)\n\n\nclass SuperResModel(UNetModel):\n    \"\"\"\n    A UNetModel that performs super-resolution.\n\n    Expects an extra kwarg `low_res` to condition on a low-resolution image.\n    \"\"\"\n\n    def __init__(self, image_size, in_channels, *args, **kwargs):\n        super().__init__(image_size, in_channels * 2, *args, **kwargs)\n\n    def forward(self, x, timesteps, low_res=None, **kwargs):\n        _, _, new_height, new_width = x.shape\n        upsampled = F.interpolate(low_res, (new_height, new_width), mode=\"bilinear\")\n        x = th.cat([x, upsampled], dim=1)\n        return super().forward(x, timesteps, **kwargs)\n\n\nclass EncoderUNetModel(nn.Module):\n    \"\"\"\n    The half UNet model with attention and timestep embedding.\n\n    For usage, see UNet.\n    \"\"\"\n\n    def __init__(\n        self,\n        image_size,\n        in_channels,\n        model_channels,\n        out_channels,\n        num_res_blocks,\n        attention_resolutions,\n        dropout=0,\n        channel_mult=(1, 2, 4, 8),\n        conv_resample=True,\n        dims=2,\n        use_checkpoint=False,\n        use_fp16=False,\n        num_heads=1,\n        num_head_channels=-1,\n        num_heads_upsample=-1,\n        use_scale_shift_norm=False,\n        resblock_updown=False,\n        use_new_attention_order=False,\n        pool=\"adaptive\",\n    ):\n        super().__init__()\n\n        if num_heads_upsample == -1:\n            num_heads_upsample = num_heads\n\n        self.in_channels = in_channels\n        self.model_channels = model_channels\n        self.out_channels = out_channels\n        self.num_res_blocks = num_res_blocks\n        self.attention_resolutions = attention_resolutions\n        self.dropout = dropout\n        self.channel_mult = channel_mult\n        self.conv_resample = conv_resample\n        self.use_checkpoint = use_checkpoint\n        self.dtype = th.float16 if use_fp16 else th.float32\n        self.num_heads = num_heads\n        self.num_head_channels = num_head_channels\n        self.num_heads_upsample = num_heads_upsample\n\n        time_embed_dim = model_channels * 4\n        self.time_embed = nn.Sequential(\n            linear(model_channels, time_embed_dim),\n            nn.SiLU(),\n            linear(time_embed_dim, time_embed_dim),\n        )\n\n        ch = int(channel_mult[0] * model_channels)\n        self.input_blocks = nn.ModuleList(\n            [TimestepEmbedSequential(conv_nd(dims, in_channels, ch, 3, padding=1))]\n        )\n        self._feature_size = ch\n        input_block_chans = [ch]\n        ds = 1\n        for level, mult in enumerate(channel_mult):\n            for _ in range(num_res_blocks):\n                layers = [\n                    ResBlock(\n                        ch,\n                        time_embed_dim,\n                        dropout,\n                        out_channels=int(mult * model_channels),\n                        dims=dims,\n                        use_checkpoint=use_checkpoint,\n                        use_scale_shift_norm=use_scale_shift_norm,\n                    )\n                ]\n                ch = int(mult * model_channels)\n                if ds in attention_resolutions:\n                    layers.append(\n                        AttentionBlock(\n                            ch,\n                            use_checkpoint=use_checkpoint,\n                            num_heads=num_heads,\n                            num_head_channels=num_head_channels,\n                            use_new_attention_order=use_new_attention_order,\n                        )\n                    )\n                self.input_blocks.append(TimestepEmbedSequential(*layers))\n                self._feature_size += ch\n                input_block_chans.append(ch)\n            if level != len(channel_mult) - 1:\n                out_ch = ch\n                self.input_blocks.append(\n                    TimestepEmbedSequential(\n                        ResBlock(\n                            ch,\n                            time_embed_dim,\n                            dropout,\n                            out_channels=out_ch,\n                            dims=dims,\n                            use_checkpoint=use_checkpoint,\n                            use_scale_shift_norm=use_scale_shift_norm,\n                            down=True,\n                        )\n                        if resblock_updown\n                        else Downsample(\n                            ch, conv_resample, dims=dims, out_channels=out_ch\n                        )\n                    )\n                )\n                ch = out_ch\n                input_block_chans.append(ch)\n                ds *= 2\n                self._feature_size += ch\n\n        self.middle_block = TimestepEmbedSequential(\n            ResBlock(\n                ch,\n                time_embed_dim,\n                dropout,\n                dims=dims,\n                use_checkpoint=use_checkpoint,\n                use_scale_shift_norm=use_scale_shift_norm,\n            ),\n            AttentionBlock(\n                ch,\n                use_checkpoint=use_checkpoint,\n                num_heads=num_heads,\n                num_head_channels=num_head_channels,\n                use_new_attention_order=use_new_attention_order,\n            ),\n            ResBlock(\n                ch,\n                time_embed_dim,\n                dropout,\n                dims=dims,\n                use_checkpoint=use_checkpoint,\n                use_scale_shift_norm=use_scale_shift_norm,\n            ),\n        )\n        self._feature_size += ch\n        self.pool = pool\n        if pool == \"adaptive\":\n            self.out = nn.Sequential(\n                normalization(ch),\n                nn.SiLU(),\n                nn.AdaptiveAvgPool2d((1, 1)),\n                zero_module(conv_nd(dims, ch, out_channels, 1)),\n                nn.Flatten(),\n            )\n        elif pool == \"attention\":\n            assert num_head_channels != -1\n            self.out = nn.Sequential(\n                normalization(ch),\n                nn.SiLU(),\n                AttentionPool2d(\n                    (image_size // ds), ch, num_head_channels, out_channels\n                ),\n            )\n        elif pool == \"spatial\":\n            self.out = nn.Sequential(\n                nn.Linear(self._feature_size, 2048),\n                nn.ReLU(),\n                nn.Linear(2048, self.out_channels),\n            )\n        elif pool == \"spatial_v2\":\n            self.out = nn.Sequential(\n                nn.Linear(self._feature_size, 2048),\n                normalization(2048),\n                nn.SiLU(),\n                nn.Linear(2048, self.out_channels),\n            )\n        else:\n            raise NotImplementedError(f\"Unexpected {pool} pooling\")\n\n    def convert_to_fp16(self):\n        \"\"\"\n        Convert the torso of the model to float16.\n        \"\"\"\n        self.input_blocks.apply(convert_module_to_f16)\n        self.middle_block.apply(convert_module_to_f16)\n\n    def convert_to_fp32(self):\n        \"\"\"\n        Convert the torso of the model to float32.\n        \"\"\"\n        self.input_blocks.apply(convert_module_to_f32)\n        self.middle_block.apply(convert_module_to_f32)\n\n    def forward(self, x, timesteps):\n        \"\"\"\n        Apply the model to an input batch.\n\n        :param x: an [N x C x ...] Tensor of inputs.\n        :param timesteps: a 1-D batch of timesteps.\n        :return: an [N x K] Tensor of outputs.\n        \"\"\"\n        emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))\n\n        results = []\n        h = x.type(self.dtype)\n        for module in self.input_blocks:\n            h = module(h, emb)\n            if self.pool.startswith(\"spatial\"):\n                results.append(h.type(x.dtype).mean(dim=(2, 3)))\n        h = self.middle_block(h, emb)\n        if self.pool.startswith(\"spatial\"):\n            results.append(h.type(x.dtype).mean(dim=(2, 3)))\n            h = th.cat(results, axis=-1)\n            return self.out(h)\n        else:\n            h = h.type(x.dtype)\n            return self.out(h)\n"}
{"type": "source_file", "path": "LLaVA/llava/eval/model_vqa_mmbench.py", "content": "import argparse\nimport torch\nimport os\nimport json\nimport pandas as pd\nfrom tqdm import tqdm\nimport shortuuid\n\nfrom llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\nfrom llava.conversation import conv_templates, SeparatorStyle\nfrom llava.model.builder import load_pretrained_model\nfrom llava.utils import disable_torch_init\nfrom llava.mm_utils import tokenizer_image_token, process_images, load_image_from_base64, get_model_name_from_path\n\nfrom PIL import Image\nimport math\n\n\nall_options = ['A', 'B', 'C', 'D']\n\n\ndef split_list(lst, n):\n    \"\"\"Split a list into n (roughly) equal-sized chunks\"\"\"\n    chunk_size = math.ceil(len(lst) / n)  # integer division\n    return [lst[i:i+chunk_size] for i in range(0, len(lst), chunk_size)]\n\n\ndef get_chunk(lst, n, k):\n    chunks = split_list(lst, n)\n    return chunks[k]\n\n\ndef is_none(value):\n    if value is None:\n        return True\n    if type(value) is float and math.isnan(value):\n        return True\n    if type(value) is str and value.lower() == 'nan':\n        return True\n    if type(value) is str and value.lower() == 'none':\n        return True\n    return False\n\ndef get_options(row, options):\n    parsed_options = []\n    for option in options:\n        option_value = row[option]\n        if is_none(option_value):\n            break\n        parsed_options.append(option_value)\n    return parsed_options\n\n\ndef eval_model(args):\n    # Model\n    disable_torch_init()\n    model_path = os.path.expanduser(args.model_path)\n    model_name = get_model_name_from_path(model_path)\n    tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, args.model_base, model_name)\n\n    questions = pd.read_table(os.path.expanduser(args.question_file))\n    questions = get_chunk(questions, args.num_chunks, args.chunk_idx)\n    answers_file = os.path.expanduser(args.answers_file)\n    os.makedirs(os.path.dirname(answers_file), exist_ok=True)\n    ans_file = open(answers_file, \"w\")\n\n    if 'plain' in model_name and 'finetune' not in model_name.lower() and 'mmtag' not in args.conv_mode:\n        args.conv_mode = args.conv_mode + '_mmtag'\n        print(f'It seems that this is a plain model, but it is not using a mmtag prompt, auto switching to {args.conv_mode}.')\n\n    for index, row in tqdm(questions.iterrows(), total=len(questions)):\n        options = get_options(row, all_options)\n        cur_option_char = all_options[:len(options)]\n\n        if args.all_rounds:\n            num_rounds = len(options)\n        else:\n            num_rounds = 1\n\n        for round_idx in range(num_rounds):\n            idx = row['index']\n            question = row['question']\n            hint = row['hint']\n            image = load_image_from_base64(row['image'])\n            if not is_none(hint):\n                question = hint + '\\n' + question\n            for option_char, option in zip(all_options[:len(options)], options):\n                question = question + '\\n' + option_char + '. ' + option\n            qs = cur_prompt = question\n            if model.config.mm_use_im_start_end:\n                qs = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + qs\n            else:\n                qs = DEFAULT_IMAGE_TOKEN + '\\n' + qs\n\n            if args.single_pred_prompt:\n                if args.lang == 'cn':\n                    qs = qs + '\\n' + \"\"\n                else:\n                    qs = qs + '\\n' + \"Answer with the option's letter from the given choices directly.\"\n\n            conv = conv_templates[args.conv_mode].copy()\n            conv.append_message(conv.roles[0], qs)\n            conv.append_message(conv.roles[1], None)\n            prompt = conv.get_prompt()\n\n            input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\n\n            image_tensor = process_images([image], image_processor, model.config)[0]\n\n            with torch.inference_mode():\n                output_ids = model.generate(\n                    input_ids,\n                    images=image_tensor.unsqueeze(0).half().cuda(),\n                    image_sizes=[image.size],\n                    do_sample=True if args.temperature > 0 else False,\n                    temperature=args.temperature,\n                    top_p=args.top_p,\n                    num_beams=args.num_beams,\n                    # no_repeat_ngram_size=3,\n                    max_new_tokens=1024,\n                    use_cache=True)\n\n            outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n\n            ans_id = shortuuid.uuid()\n            ans_file.write(json.dumps({\"question_id\": idx,\n                                    \"round_id\": round_idx,\n                                    \"prompt\": cur_prompt,\n                                    \"text\": outputs,\n                                    \"options\": options,\n                                    \"option_char\": cur_option_char,\n                                    \"answer_id\": ans_id,\n                                    \"model_id\": model_name,\n                                    \"metadata\": {}}) + \"\\n\")\n            ans_file.flush()\n\n            # rotate options\n            options = options[1:] + options[:1]\n            cur_option_char = cur_option_char[1:] + cur_option_char[:1]\n    ans_file.close()\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model-path\", type=str, default=\"facebook/opt-350m\")\n    parser.add_argument(\"--model-base\", type=str, default=None)\n    parser.add_argument(\"--image-folder\", type=str, default=\"\")\n    parser.add_argument(\"--question-file\", type=str, default=\"tables/question.jsonl\")\n    parser.add_argument(\"--answers-file\", type=str, default=\"answer.jsonl\")\n    parser.add_argument(\"--conv-mode\", type=str, default=\"llava_v1\")\n    parser.add_argument(\"--num-chunks\", type=int, default=1)\n    parser.add_argument(\"--chunk-idx\", type=int, default=0)\n    parser.add_argument(\"--temperature\", type=float, default=0.2)\n    parser.add_argument(\"--top_p\", type=float, default=None)\n    parser.add_argument(\"--num_beams\", type=int, default=1)\n    parser.add_argument(\"--all-rounds\", action=\"store_true\")\n    parser.add_argument(\"--single-pred-prompt\", action=\"store_true\")\n    parser.add_argument(\"--lang\", type=str, default=\"en\")\n    args = parser.parse_args()\n\n    eval_model(args)\n"}
{"type": "source_file", "path": "LLaVA/llava/eval/model_vqa_loader.py", "content": "import argparse\nimport torch\nimport os\nimport json\nfrom tqdm import tqdm\nimport shortuuid\n\nfrom llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\nfrom llava.conversation import conv_templates, SeparatorStyle\nfrom llava.model.builder import load_pretrained_model\nfrom llava.utils import disable_torch_init\nfrom llava.mm_utils import tokenizer_image_token, process_images, get_model_name_from_path\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom PIL import Image\nimport math\n\n\ndef split_list(lst, n):\n    \"\"\"Split a list into n (roughly) equal-sized chunks\"\"\"\n    chunk_size = math.ceil(len(lst) / n)  # integer division\n    return [lst[i:i+chunk_size] for i in range(0, len(lst), chunk_size)]\n\n\ndef get_chunk(lst, n, k):\n    chunks = split_list(lst, n)\n    return chunks[k]\n\n\n# Custom dataset class\nclass CustomDataset(Dataset):\n    def __init__(self, questions, image_folder, tokenizer, image_processor, model_config):\n        self.questions = questions\n        self.image_folder = image_folder\n        self.tokenizer = tokenizer\n        self.image_processor = image_processor\n        self.model_config = model_config\n\n    def __getitem__(self, index):\n        line = self.questions[index]\n        image_file = line[\"image\"]\n        qs = line[\"text\"]\n        if self.model_config.mm_use_im_start_end:\n            qs = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + qs\n        else:\n            qs = DEFAULT_IMAGE_TOKEN + '\\n' + qs\n\n        conv = conv_templates[args.conv_mode].copy()\n        conv.append_message(conv.roles[0], qs)\n        conv.append_message(conv.roles[1], None)\n        prompt = conv.get_prompt()\n\n        image = Image.open(os.path.join(self.image_folder, image_file)).convert('RGB')\n        image_tensor = process_images([image], self.image_processor, self.model_config)[0]\n\n        input_ids = tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt')\n\n        return input_ids, image_tensor, image.size\n\n    def __len__(self):\n        return len(self.questions)\n\n\ndef collate_fn(batch):\n    input_ids, image_tensors, image_sizes = zip(*batch)\n    input_ids = torch.stack(input_ids, dim=0)\n    image_tensors = torch.stack(image_tensors, dim=0)\n    return input_ids, image_tensors, image_sizes\n\n\n# DataLoader\ndef create_data_loader(questions, image_folder, tokenizer, image_processor, model_config, batch_size=1, num_workers=4):\n    assert batch_size == 1, \"batch_size must be 1\"\n    dataset = CustomDataset(questions, image_folder, tokenizer, image_processor, model_config)\n    data_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, collate_fn=collate_fn)\n    return data_loader\n\n\ndef eval_model(args):\n    # Model\n    disable_torch_init()\n    model_path = os.path.expanduser(args.model_path)\n    model_name = get_model_name_from_path(model_path)\n    tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, args.model_base, model_name)\n\n    questions = [json.loads(q) for q in open(os.path.expanduser(args.question_file), \"r\")]\n    questions = get_chunk(questions, args.num_chunks, args.chunk_idx)\n    answers_file = os.path.expanduser(args.answers_file)\n    os.makedirs(os.path.dirname(answers_file), exist_ok=True)\n    ans_file = open(answers_file, \"w\")\n\n    if 'plain' in model_name and 'finetune' not in model_name.lower() and 'mmtag' not in args.conv_mode:\n        args.conv_mode = args.conv_mode + '_mmtag'\n        print(f'It seems that this is a plain model, but it is not using a mmtag prompt, auto switching to {args.conv_mode}.')\n\n    data_loader = create_data_loader(questions, args.image_folder, tokenizer, image_processor, model.config)\n\n    for (input_ids, image_tensor, image_sizes), line in tqdm(zip(data_loader, questions), total=len(questions)):\n        idx = line[\"question_id\"]\n        cur_prompt = line[\"text\"]\n\n        input_ids = input_ids.to(device='cuda', non_blocking=True)\n\n        with torch.inference_mode():\n            output_ids = model.generate(\n                input_ids,\n                images=image_tensor.to(dtype=torch.float16, device='cuda', non_blocking=True),\n                image_sizes=image_sizes,\n                do_sample=True if args.temperature > 0 else False,\n                temperature=args.temperature,\n                top_p=args.top_p,\n                num_beams=args.num_beams,\n                max_new_tokens=args.max_new_tokens,\n                use_cache=True)\n\n        outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n\n        ans_id = shortuuid.uuid()\n        ans_file.write(json.dumps({\"question_id\": idx,\n                                   \"prompt\": cur_prompt,\n                                   \"text\": outputs,\n                                   \"answer_id\": ans_id,\n                                   \"model_id\": model_name,\n                                   \"metadata\": {}}) + \"\\n\")\n        # ans_file.flush()\n    ans_file.close()\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model-path\", type=str, default=\"facebook/opt-350m\")\n    parser.add_argument(\"--model-base\", type=str, default=None)\n    parser.add_argument(\"--image-folder\", type=str, default=\"\")\n    parser.add_argument(\"--question-file\", type=str, default=\"tables/question.jsonl\")\n    parser.add_argument(\"--answers-file\", type=str, default=\"answer.jsonl\")\n    parser.add_argument(\"--conv-mode\", type=str, default=\"llava_v1\")\n    parser.add_argument(\"--num-chunks\", type=int, default=1)\n    parser.add_argument(\"--chunk-idx\", type=int, default=0)\n    parser.add_argument(\"--temperature\", type=float, default=0.2)\n    parser.add_argument(\"--top_p\", type=float, default=None)\n    parser.add_argument(\"--num_beams\", type=int, default=1)\n    parser.add_argument(\"--max_new_tokens\", type=int, default=128)\n    args = parser.parse_args()\n\n    eval_model(args)\n"}
{"type": "source_file", "path": "LLaVA/llava/eval/model_vqa.py", "content": "import argparse\nimport torch\nimport os\nimport json\nfrom tqdm import tqdm\nimport shortuuid\n\nfrom llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\nfrom llava.conversation import conv_templates, SeparatorStyle\nfrom llava.model.builder import load_pretrained_model\nfrom llava.utils import disable_torch_init\nfrom llava.mm_utils import tokenizer_image_token, process_images, get_model_name_from_path\n\nfrom PIL import Image\nimport math\n\n\ndef split_list(lst, n):\n    \"\"\"Split a list into n (roughly) equal-sized chunks\"\"\"\n    chunk_size = math.ceil(len(lst) / n)  # integer division\n    return [lst[i:i+chunk_size] for i in range(0, len(lst), chunk_size)]\n\n\ndef get_chunk(lst, n, k):\n    chunks = split_list(lst, n)\n    return chunks[k]\n\n\ndef eval_model(args):\n    # Model\n    disable_torch_init()\n    model_path = os.path.expanduser(args.model_path)\n    model_name = get_model_name_from_path(model_path)\n    tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, args.model_base, model_name)\n\n    questions = [json.loads(q) for q in open(os.path.expanduser(args.question_file), \"r\")]\n    questions = get_chunk(questions, args.num_chunks, args.chunk_idx)\n    answers_file = os.path.expanduser(args.answers_file)\n    os.makedirs(os.path.dirname(answers_file), exist_ok=True)\n    ans_file = open(answers_file, \"w\")\n    for line in tqdm(questions):\n        idx = line[\"question_id\"]\n        image_file = line[\"image\"]\n        qs = line[\"text\"]\n        cur_prompt = qs\n        if model.config.mm_use_im_start_end:\n            qs = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + qs\n        else:\n            qs = DEFAULT_IMAGE_TOKEN + '\\n' + qs\n\n        conv = conv_templates[args.conv_mode].copy()\n        conv.append_message(conv.roles[0], qs)\n        conv.append_message(conv.roles[1], None)\n        prompt = conv.get_prompt()\n\n        input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\n\n        image = Image.open(os.path.join(args.image_folder, image_file)).convert('RGB')\n        image_tensor = process_images([image], image_processor, model.config)[0]\n\n        with torch.inference_mode():\n            output_ids = model.generate(\n                input_ids,\n                images=image_tensor.unsqueeze(0).half().cuda(),\n                image_sizes=[image.size],\n                do_sample=True if args.temperature > 0 else False,\n                temperature=args.temperature,\n                top_p=args.top_p,\n                num_beams=args.num_beams,\n                # no_repeat_ngram_size=3,\n                max_new_tokens=1024,\n                use_cache=True)\n\n        outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n\n        ans_id = shortuuid.uuid()\n        ans_file.write(json.dumps({\"question_id\": idx,\n                                   \"prompt\": cur_prompt,\n                                   \"text\": outputs,\n                                   \"answer_id\": ans_id,\n                                   \"model_id\": model_name,\n                                   \"metadata\": {}}) + \"\\n\")\n        ans_file.flush()\n    ans_file.close()\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model-path\", type=str, default=\"facebook/opt-350m\")\n    parser.add_argument(\"--model-base\", type=str, default=None)\n    parser.add_argument(\"--image-folder\", type=str, default=\"\")\n    parser.add_argument(\"--question-file\", type=str, default=\"tables/question.jsonl\")\n    parser.add_argument(\"--answers-file\", type=str, default=\"answer.jsonl\")\n    parser.add_argument(\"--conv-mode\", type=str, default=\"llava_v1\")\n    parser.add_argument(\"--num-chunks\", type=int, default=1)\n    parser.add_argument(\"--chunk-idx\", type=int, default=0)\n    parser.add_argument(\"--temperature\", type=float, default=0.2)\n    parser.add_argument(\"--top_p\", type=float, default=None)\n    parser.add_argument(\"--num_beams\", type=int, default=1)\n    args = parser.parse_args()\n\n    eval_model(args)\n"}
{"type": "source_file", "path": "LLaVA/llava/eval/model_qa.py", "content": "import argparse\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteria\nimport torch\nimport os\nimport json\nfrom tqdm import tqdm\nimport shortuuid\n\nfrom llava.conversation import default_conversation\nfrom llava.utils import disable_torch_init\n\n\n@torch.inference_mode()\ndef eval_model(model_name, questions_file, answers_file):\n    # Model\n    disable_torch_init()\n    model_name = os.path.expanduser(model_name)\n    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n    model = AutoModelForCausalLM.from_pretrained(model_name,\n        torch_dtype=torch.float16).cuda()\n\n\n    ques_file = open(os.path.expanduser(questions_file), \"r\")\n    ans_file = open(os.path.expanduser(answers_file), \"w\")\n    for i, line in enumerate(tqdm(ques_file)):\n        idx = json.loads(line)[\"question_id\"]\n        qs = json.loads(line)[\"text\"]\n        cat = json.loads(line)[\"category\"]\n        conv = default_conversation.copy()\n        conv.append_message(conv.roles[0], qs)\n        prompt = conv.get_prompt()\n        inputs = tokenizer([prompt])\n        input_ids = torch.as_tensor(inputs.input_ids).cuda()\n        output_ids = model.generate(\n            input_ids,\n            do_sample=True,\n            use_cache=True,\n            temperature=0.7,\n            max_new_tokens=1024,)\n        outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\n        try:\n            index = outputs.index(conv.sep, len(prompt))\n        except ValueError:\n            outputs += conv.sep\n            index = outputs.index(conv.sep, len(prompt))\n\n        outputs = outputs[len(prompt) + len(conv.roles[1]) + 2:index].strip()\n        ans_id = shortuuid.uuid()\n        ans_file.write(json.dumps({\"question_id\": idx,\n                                   \"text\": outputs,\n                                   \"answer_id\": ans_id,\n                                   \"model_id\": model_name,\n                                   \"metadata\": {}}) + \"\\n\")\n        ans_file.flush()\n    ans_file.close()\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model-name\", type=str, default=\"facebook/opt-350m\")\n    parser.add_argument(\"--question-file\", type=str, default=\"tables/question.jsonl\")\n    parser.add_argument(\"--answers-file\", type=str, default=\"answer.jsonl\")\n    args = parser.parse_args()\n\n    eval_model(args.model_name, args.question_file, args.answers_file)\n"}
