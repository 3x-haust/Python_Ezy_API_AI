{"repo_info": {"repo_name": "LibMoE", "repo_owner": "Fsoft-AIC", "repo_url": "https://github.com/Fsoft-AIC/LibMoE"}}
{"type": "test_file", "path": "evaluate/miscs/test_llava.py", "content": "import requests\nfrom PIL import Image\n\nimport torch\nfrom transformers import AutoProcessor, LlavaForConditionalGeneration\n\nmodel_id = \"llava-hf/llava-1.5-7b-hf\"\n\nprompt_1 = \"USER: <image>\\nWhat does this image show?\\nASSISTANT:\"\nprompt_2 = \"USER: <image> <image> \\nWhat is the difference between these two images?\\nASSISTANT:\"\nimage_file_1 = \"image1.png\"\nimage_file_2 = \"image2.png\"\nmodel = LlavaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.float16, low_cpu_mem_usage=True, use_flash_attention_2=True).to(0)\nprocessor = AutoProcessor.from_pretrained(model_id)\nraw_image_1 = Image.open(image_file_1)\nraw_image_2 = Image.open(image_file_2)\ninputs = processor([prompt_1, prompt_2], [raw_image_1, raw_image_1, raw_image_2], padding=True, return_tensors=\"pt\").to(0, torch.float16)\nimport pdb\n\npdb.set_trace()\noutput = model.generate(**inputs, max_new_tokens=200, do_sample=False)\nprint(processor.batch_decode(output, skip_special_tokens=True))\n"}
{"type": "test_file", "path": "evaluate/miscs/test_scienceqa.py", "content": "from datasets import load_dataset\n\ndataset = load_dataset(\"Otter-AI/ScienceQA\", trust_remote_code=True)[\"test\"]\nfor doc in dataset:\n    print(doc[\"id\"])\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/models/idefics2.py", "content": "import torch\n\nfrom tqdm import tqdm\nfrom lmms_eval import utils\nfrom lmms_eval.api.instance import Instance\nfrom lmms_eval.api.model import lmms\nfrom lmms_eval.api.registry import register_model\nfrom accelerate import Accelerator, DistributedType\nfrom accelerate.state import AcceleratorState\nfrom typing import List, Optional, Union, Tuple\nfrom transformers import Idefics2ForConditionalGeneration, AutoProcessor\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom loguru import logger as eval_logger\n\nDEFAULT_IMAGE_TOKEN = \"<image>\"\ntry:\n    import flash_attn\n\n    best_fit_attn_implementation = \"flash_attention_2\"\nexcept ImportError:\n    best_fit_attn_implementation = \"eager\"\n\n\n@register_model(\"idefics2\")\nclass Idefics2(lmms):\n    \"\"\"\n    Idefics2 Model for Hugging Face Transformers: https://github.com/huggingface/transformers/blob/main/src/transformers/models/idefics2/modeling_idefics2.py\n\n    Example usage:\n\n    accelerate launch --num_processes=8 -m lmms_eval \\\n        --model idefics2 \\\n        --model_args pretrained=HuggingFaceM4/idefics2-8b \\\n        --tasks mme \\\n        --batch_size 1 \\\n        --output_path ./logs/ \\\n        --log_samples\n    \"\"\"\n\n    def __init__(\n        self,\n        pretrained: str = \"HuggingFaceM4/idefics2-8b\",\n        revision: str = \"main\",\n        device: str = \"cuda\",\n        dtype: Optional[Union[str, torch.dtype]] = \"float16\",\n        batch_size: int = 1,\n        trust_remote_code: Optional[bool] = False,\n        attn_implementation: Optional[str] = best_fit_attn_implementation,\n        device_map: str = \"\",\n        use_cache: bool = True,\n        do_image_splitting: bool = False,\n        **kwargs,\n    ) -> None:\n        super().__init__()\n        # Do not use kwargs for now\n        assert kwargs == {}, f\"Unexpected kwargs: {kwargs}\"\n\n        accelerator = Accelerator()\n        if accelerator.num_processes > 1 and device_map == \"\":\n            self._device = torch.device(f\"cuda:{accelerator.local_process_index}\")\n            self.device_map = f\"cuda:{accelerator.local_process_index}\"\n        else:\n            self._device = torch.device(device)\n            self.device_map = device_map\n        if isinstance(dtype, str) and dtype != \"auto\":\n            dtype = getattr(torch, dtype)\n        self._model = Idefics2ForConditionalGeneration.from_pretrained(pretrained, revision=revision, torch_dtype=dtype, device_map=self.device_map, trust_remote_code=trust_remote_code, attn_implementation=attn_implementation)\n        self._processor = AutoProcessor.from_pretrained(pretrained, do_image_splitting=do_image_splitting, revision=revision, trust_remote_code=trust_remote_code)\n\n        self._tokenizer = self._processor.tokenizer\n        self._config = self._model.config\n        self.batch_size_per_gpu = int(batch_size)\n        self.use_cache = use_cache\n        if accelerator.num_processes > 1 and device_map == \"\":\n            assert accelerator.distributed_type in [DistributedType.FSDP, DistributedType.MULTI_GPU, DistributedType.DEEPSPEED], \"Unsupported distributed type provided. Only DDP and FSDP are supported.\"\n            # If you want to use DistributedType.DEEPSPEED, you have to run accelerate config before using the model\n            # Also, you have to select zero stage 0 (equivalent to DDP) in order to make the prepare model works\n            # I tried to set different parameters in the kwargs to let default zero 2 stage works, but it didn't work.\n            if accelerator.distributed_type == DistributedType.DEEPSPEED:\n                kwargs = {\n                    \"train_micro_batch_size_per_gpu\": self.batch_size_per_gpu,\n                    \"train_batch_size\": self.batch_size_per_gpu * accelerator.num_processes,\n                }\n                AcceleratorState().deepspeed_plugin.deepspeed_config_process(must_match=True, **kwargs)\n                eval_logger.info(\"Detected that you are using DistributedType.DEEPSPEED. Make sure you run `accelerate config` and set zero stage to 0\")\n            if accelerator.distributed_type == DistributedType.FSDP or accelerator.distributed_type == DistributedType.DEEPSPEED:\n                self._model = accelerator.prepare(self.model)\n            else:\n                self._model = accelerator.prepare_model(self.model, evaluation_mode=True)\n            self.accelerator = accelerator\n            if self.accelerator.is_local_main_process:\n                eval_logger.info(f\"Using {accelerator.num_processes} devices with data parallelism\")\n            self._rank = self.accelerator.local_process_index\n            self._world_size = self.accelerator.num_processes\n        elif accelerator.num_processes == 1 and device_map == \"auto\":\n            eval_logger.info(f\"Using {accelerator.num_processes} devices with pipeline parallelism\")\n            self._rank = 0\n            self._word_size = 1\n        else:\n            eval_logger.info(f\"Using single device: {self._device}\")\n            self.model.to(self._device)\n            self._rank = 0\n            self._word_size = 1\n\n    @property\n    def config(self):\n        # return the associated transformers.AutoConfig for the given pretrained model.\n        return self._config\n\n    @property\n    def tokenizer(self):\n        return self._tokenizer\n\n    @property\n    def model(self):\n        # returns the model, unwrapping it if using Accelerate\n        if hasattr(self, \"accelerator\"):\n            return self.accelerator.unwrap_model(self._model)\n        else:\n            return self._model\n\n    @property\n    def eot_token_id(self):\n        # we use EOT because end of *text* is more accurate for what we're doing than end of *sentence*\n        return self.tokenizer.eos_token_id\n\n    @property\n    def max_length(self):\n        return self._max_length\n\n    @property\n    def batch_size(self):\n        return self.batch_size_per_gpu\n\n    @property\n    def device(self):\n        return self._device\n\n    @property\n    def rank(self):\n        return self._rank\n\n    @property\n    def world_size(self):\n        return self._world_size\n\n    def tok_encode(self, string: str, left_truncate_len=None, add_special_tokens=None) -> List[int]:\n        \"\"\" \"\"\"\n        add_special_tokens = False if add_special_tokens is None else add_special_tokens\n        encoding = self.tokenizer.encode(string, add_special_tokens=add_special_tokens)\n        # left-truncate the encoded context to be at most `left_truncate_len` tokens long\n        if left_truncate_len:\n            encoding = encoding[-left_truncate_len:]\n        return encoding\n\n    def tok_decode(self, tokens):\n        return self.tokenizer.decode(tokens)\n\n    def loglikelihood(self, requests: List[Instance]) -> List[Tuple[float, bool]]:\n        raise NotImplementedError(\"Loglikelihood is not implemented for Idefics2 model\")\n\n    def flatten(self, input):\n        new_list = []\n        for i in input:\n            for j in i:\n                new_list.append(j)\n        return new_list\n\n    def generate_until(self, requests: List[Instance]) -> List[str]:\n        res = []\n\n        def _collate(x):\n            # the negative sign on len(toks) sorts descending - this has a few advantages:\n            # - time estimates will always be over not underestimates, which is more useful for planning\n            # - to know the size of a batch when going through the list, you know the first one is always the batch\n            #   padded context length. this is useful to simplify the batching logic and more importantly to make\n            #   automatic adaptive batches much much easier to implement\n            # - any OOMs will happen right away rather than near the end\n            toks = self.tok_encode(x[0])\n            return -len(toks), x[0]\n\n        # we group requests by their generation_kwargs,\n        # so that we don't try to execute e.g. greedy sampling and temp=0.8 sampling\n        # in the same batch.\n        re_ords = utils.Collator([reg.args for reg in requests], _collate, grouping=True)\n        chunks = re_ords.get_batched(n=self.batch_size, batch_fn=None)\n        num_iters = len(requests) // self.batch_size if len(requests) % self.batch_size == 0 else len(requests) // self.batch_size + 1\n        pbar = tqdm(total=num_iters, disable=(self.rank != 0), desc=\"Model Responding\")\n        for chunk in chunks:\n            contexts, all_gen_kwargs, doc_to_visuals, doc_id, tasks, splits = zip(*chunk)\n            visuals = [doc_to_visual(self.task_dict[task][split][ids]) for ids, task, split, doc_to_visual in zip(doc_id, tasks, splits, doc_to_visuals)]\n            # we assume all gen kwargs in the batch are the same\n            # this is safe to assume because the `grouper` object ensures it.\n            gen_kwargs = all_gen_kwargs[0]\n            #\n            until = gen_kwargs.pop(\"until\", None)\n            image_aspect_ratio = gen_kwargs.pop(\"image_aspect_ratio\", None)\n            if \"max_new_tokens\" not in gen_kwargs:\n                gen_kwargs[\"max_new_tokens\"] = 1024\n            if \"temperature\" not in gen_kwargs:\n                gen_kwargs[\"temperature\"] = 0\n\n            prompts = []\n            for context, visual in zip(contexts, visuals):\n                content = []\n                if DEFAULT_IMAGE_TOKEN not in context:\n                    for image in visual:\n                        content.append({\"type\": \"image\"})\n                content.append({\"type\": \"text\", \"text\": context})\n                message = [{\"role\": \"user\", \"content\": content}]\n                prompt = self._processor.apply_chat_template(message, add_generation_prompt=True)\n                prompts.append(prompt)\n            inputs = self._processor(text=prompts, images=visuals, padding=True, return_tensors=\"pt\")\n            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n            output_ids = self.model.generate(**inputs, **gen_kwargs)\n            # only retain the generated text\n            for output_id, input_id in zip(output_ids, inputs[\"input_ids\"]):\n                generated_id = output_id[len(input_id) :]\n                generated_text = self.tokenizer.decode(generated_id, skip_special_tokens=True)\n\n                res.append(generated_text)\n            pbar.update(1)\n        # reorder this group of results back to original unsorted form\n        res = re_ords.get_original(res)\n\n        pbar.close()\n        return res\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/models/from_log.py", "content": "import json\nimport os\nimport re\n\nfrom datetime import datetime\nfrom typing import List, Tuple\nfrom tqdm import tqdm\nfrom lmms_eval.api.registry import register_model\nfrom lmms_eval.api.model import lmms\nfrom lmms_eval.api.instance import Instance\nfrom accelerate import Accelerator, DistributedType\n\nfrom loguru import logger as eval_logger\n\n\n@register_model(\"from_log\")\nclass FromLog(lmms):\n    def __init__(\n        self,\n        logs: str = \"logs\",\n        model_name: str = None,\n        model_args: str = None,\n        have_limits: bool = False,\n        **kwargs,\n    ) -> None:\n        super().__init__()\n\n        self.logs = {}\n\n        log_folders = logs.split(\",\")\n\n        def matched_model(_model_args):\n            if model_name and model_name != _model_args[\"model\"]:\n                return False\n\n            if model_args:\n                _model_args_list = model_args.split(\",\")\n\n                for _model_arg in _model_args_list:\n                    if _model_arg not in _model_args[\"model_args\"]:\n                        return False\n\n            if not have_limits and _model_args[\"limit\"] is not None:\n                return False\n\n            return True\n\n        for log_folder in log_folders:\n            for root, dirs, files in os.walk(log_folder):\n                for file in files:\n                    if file.endswith(\".json\"):\n                        try:\n                            log_file = os.path.join(root, file)\n\n                            with open(log_file, \"r\") as f:\n                                log_data = json.load(f)\n\n                            # check if model is matched\n                            _model_args = log_data[\"args\"]\n                            if not matched_model(_model_args):\n                                raise Exception(\"Model not matched\")\n\n                            # load logs\n                            logs = {}\n                            for data in log_data[\"logs\"]:\n                                id = data[\"doc_id\"]\n                                response = data[\"resps\"][0]\n                                logs[id] = response\n\n                            task = log_data[\"model_configs\"][\"task\"]\n\n                            pattern = re.compile(r\"\\d{4}_\\d{4}\")\n\n                            if \"time\" in log_data:\n                                log_time = log_data[\"time\"]\n                            elif pattern.search(os.path.abspath(log_file)):\n                                log_time = pattern.findall(os.path.abspath(log_file))[-1]\n                            else:\n                                log_time = \"unknown\"\n\n                            if task not in self.logs or (self.logs[task][\"time\"] == \"unknown\" or datetime.strptime(log_time, \"%m%d_%H%M\") > datetime.strptime(self.logs[task][\"time\"], \"%m%d_%H%M\")):\n                                self.logs[task] = {\"time\": log_time, \"logs\": logs}\n\n                        except Exception as e:\n                            pass\n\n        accelerator = Accelerator()\n        if accelerator.num_processes > 1:\n            assert accelerator.distributed_type in [DistributedType.FSDP, DistributedType.MULTI_GPU, DistributedType.DEEPSPEED], \"Unsupported distributed type provided. Only DDP and FSDP are supported.\"\n            self.accelerator = accelerator\n            if self.accelerator.is_local_main_process:\n                eval_logger.info(f\"Using {accelerator.num_processes} devices with data parallelism\")\n            self._rank = self.accelerator.local_process_index\n            self._world_size = self.accelerator.num_processes\n        else:\n            self.accelerator = accelerator\n            self._rank = self.accelerator.local_process_index\n            self._world_size = self.accelerator.num_processes\n\n        self.device = self.accelerator.device\n\n    def generate_until(self, requests) -> List[str]:\n        res = []\n        pbar = tqdm(total=len(requests), disable=(self.rank != 0), desc=\"Model Responding\")\n\n        for contexts, gen_kwargs, doc_to_visual, doc_id, task, split in [reg.args for reg in requests]:\n            response = self.logs[task][\"logs\"][doc_id]\n            res.append(response[0])\n            pbar.update(1)\n\n        pbar.close()\n        return res\n\n    def loglikelihood(self, requests: List[Instance]) -> List[Tuple[float, bool]]:\n        # TODO\n        assert False, \"not support\"\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/models/longva.py", "content": "from accelerate import Accelerator, DistributedType, InitProcessGroupKwargs\nfrom accelerate.state import AcceleratorState\nfrom transformers import AutoConfig\n\nimport math\nimport torch\n\ntorch.backends.cuda.matmul.allow_tf32 = True\n\nfrom tqdm import tqdm\nfrom datetime import timedelta\nfrom decord import VideoReader, cpu\nimport numpy as np\n\nimport copy\nimport PIL\nfrom typing import List, Optional, Union, Tuple\nfrom packaging import version\nimport warnings\nimport logging\n\nwarnings.filterwarnings(\"ignore\")\n\neval_logger = logging.getLogger(\"lmms-eval\")\n\nfrom lmms_eval import utils\nfrom lmms_eval.api.instance import Instance\nfrom lmms_eval.api.model import lmms\nfrom lmms_eval.api.registry import register_model\nfrom lmms_eval.models.model_utils.load_video import read_video_pyav\n\ntry:\n    from longva.model.builder import load_pretrained_model\n    from longva.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token, KeywordsStoppingCriteria\n    from longva.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IGNORE_INDEX\n    from longva.conversation import conv_templates, SeparatorStyle\n\nexcept Exception as e:\n    eval_logger.debug(\"longva is not installed. Please install longva to use this model.\\nError: %s\" % e)\n\n# inference implementation for attention, can be \"sdpa\", \"eager\", \"flash_attention_2\". Seems FA2 is not effective during inference: https://discuss.huggingface.co/t/flash-attention-has-no-effect-on-inference/73453/5\n# if is_flash_attn_2_available:\n#     best_fit_attn_implementation = \"flash_attention_2\" # flash_attn has a bug that says: ERROR Error query and key must have the same dtype in generating\n\nif version.parse(torch.__version__) >= version.parse(\"2.1.2\"):\n    best_fit_attn_implementation = \"sdpa\"\nelse:\n    best_fit_attn_implementation = \"eager\"\n\n\n@register_model(\"longva\")\nclass LongVA(lmms):\n    def __init__(\n        self,\n        pretrained: str = \"lmms-lab/LongVA-7B\",\n        truncation: Optional[bool] = True,\n        device: Optional[str] = \"cuda:0\",\n        batch_size: Optional[Union[int, str]] = 1,\n        model_name: Optional[str] = None,\n        attn_implementation: Optional[str] = best_fit_attn_implementation,\n        device_map: Optional[str] = \"cuda:0\",\n        conv_template: Optional[str] = \"vicuna_v1\",\n        use_cache: Optional[bool] = True,\n        truncate_context: Optional[bool] = False,  # whether to truncate the context in generation, set it False for LLaVA-1.6\n        customized_config: Optional[str] = None,  # ends in json\n        max_frames_num: Optional[int] = 32,\n        mm_spatial_pool_stride: Optional[int] = 2,\n        mm_spatial_pool_mode: Optional[str] = \"average\",\n        token_strategy: Optional[str] = \"single\",  # could be \"single\" or \"multiple\", \"multiple\" denotes adding multiple <image> tokens for each frame\n        video_decode_backend: str = \"pyav\",\n        **kwargs,\n    ) -> None:\n        super().__init__()\n        # Do not use kwargs for now\n        assert kwargs == {}, f\"Unexpected kwargs: {kwargs}\"\n\n        accelerator_kwargs = InitProcessGroupKwargs(timeout=timedelta(weeks=52))\n        accelerator = Accelerator(kwargs_handlers=[accelerator_kwargs])\n        self.accelerator = accelerator\n        if accelerator.num_processes > 1:\n            self._device = torch.device(f\"cuda:{accelerator.local_process_index}\")\n            self.device_map = f\"cuda:{accelerator.local_process_index}\"\n        elif accelerator.num_processes == 1 and device_map == \"auto\":\n            self._device = torch.device(device)\n            self.device_map = device_map\n        else:\n            self._device = torch.device(f\"cuda:{accelerator.local_process_index}\")\n            self.device_map = f\"cuda:{accelerator.local_process_index}\"\n\n        llava_model_args = {\n            \"multimodal\": True,\n        }\n        if customized_config is not None:\n            llava_model_args[\"customized_config\"] = customized_config\n        if attn_implementation is not None:\n            llava_model_args[\"attn_implementation\"] = attn_implementation\n        if \"use_flash_attention_2\" in kwargs:\n            llava_model_args[\"use_flash_attention_2\"] = kwargs[\"use_flash_attention_2\"]\n        model_name = model_name if model_name is not None else get_model_name_from_path(pretrained)\n\n        self.pretrained = pretrained\n        self.token_strategy = token_strategy\n        self.max_frames_num = max_frames_num\n        self.mm_spatial_pool_stride = mm_spatial_pool_stride\n        self.mm_spatial_pool_mode = mm_spatial_pool_mode\n        self.video_decode_backend = video_decode_backend\n\n        overwrite_config = {}\n        overwrite_config[\"mm_spatial_pool_stride\"] = self.mm_spatial_pool_stride\n        overwrite_config[\"mm_spatial_pool_mode\"] = self.mm_spatial_pool_mode\n        cfg_pretrained = AutoConfig.from_pretrained(self.pretrained)\n\n        llava_model_args[\"overwrite_config\"] = overwrite_config\n        try:\n            # Try to load the model with the multimodal argument\n            self._tokenizer, self._model, self._image_processor, self._max_length = load_pretrained_model(pretrained, None, model_name, device_map=self.device_map, **llava_model_args)\n        except TypeError:\n            # for older versions of LLaVA that don't have multimodal argument\n            llava_model_args.pop(\"multimodal\", None)\n            self._tokenizer, self._model, self._image_processor, self._max_length = load_pretrained_model(pretrained, None, model_name, device_map=self.device_map, **llava_model_args)\n\n        self._config = self._model.config\n        self.model.eval()\n        self.model.tie_weights()\n        self.truncation = truncation\n        self.batch_size_per_gpu = int(batch_size)\n        self.conv_template = conv_template\n        self.use_cache = use_cache\n        self.truncate_context = truncate_context\n        assert self.batch_size_per_gpu == 1, \"Llava currently does not support batched generation. See https://github.com/haotian-liu/LLaVA/issues/754. HF Llava also has this issue.\"\n\n        if accelerator.num_processes > 1:\n            assert accelerator.distributed_type in [DistributedType.FSDP, DistributedType.MULTI_GPU, DistributedType.DEEPSPEED], \"Unsupported distributed type provided. Only DDP and FSDP are supported.\"\n            # If you want to use DistributedType.DEEPSPEED, you have to run accelerate config before using the model\n            # Also, you have to select zero stage 0 (equivalent to DDP) in order to make the prepare model works\n            # I tried to set different parameters in the kwargs to let default zero 2 stage works, but it didn't work.\n            if accelerator.distributed_type == DistributedType.DEEPSPEED:\n                kwargs = {\n                    \"train_micro_batch_size_per_gpu\": self.batch_size_per_gpu,\n                    \"train_batch_size\": self.batch_size_per_gpu * accelerator.num_processes,\n                }\n                AcceleratorState().deepspeed_plugin.deepspeed_config_process(must_match=True, **kwargs)\n                eval_logger.info(\"Detected that you are using DistributedType.DEEPSPEED. Make sure you run `accelerate config` and set zero stage to 0\")\n\n            if accelerator.distributed_type == DistributedType.FSDP or accelerator.distributed_type == DistributedType.DEEPSPEED:\n                self._model = accelerator.prepare(self.model)\n            else:\n                self._model = accelerator.prepare_model(self.model, evaluation_mode=True)\n            self.accelerator = accelerator\n            if self.accelerator.is_local_main_process:\n                eval_logger.info(f\"Using {accelerator.num_processes} devices with data parallelism\")\n            self._rank = self.accelerator.local_process_index\n            self._world_size = self.accelerator.num_processes\n\n        elif accelerator.num_processes == 1 and device_map == \"auto\":\n            eval_logger.info(f\"Using {accelerator.num_processes} devices with tensor parallelism\")\n            self._rank = 0\n            self._word_size = 1\n\n        else:\n            eval_logger.info(f\"Using single device: {self._device}\")\n            self.model.to(self._device)\n            self._rank = 0\n            self._world_size = 1\n\n    @property\n    def config(self):\n        # return the associated transformers.AutoConfig for the given pretrained model.\n        return self._config\n\n    @property\n    def tokenizer(self):\n        return self._tokenizer\n\n    @property\n    def model(self):\n        # returns the model, unwrapping it if using Accelerate\n        if hasattr(self, \"accelerator\"):\n            return self.accelerator.unwrap_model(self._model)\n        else:\n            return self._model\n\n    @property\n    def eot_token_id(self):\n        # we use EOT because end of *text* is more accurate for what we're doing than end of *sentence*\n        return self.tokenizer.eos_token_id\n\n    @property\n    def max_length(self):\n        return self._max_length\n\n    def pad_sequence(self, input_ids, batch_first, padding_value):\n        if self.tokenizer.padding_side == \"left\":\n            input_ids = [torch.flip(_input_ids, [0]) for _input_ids in input_ids]\n        input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=batch_first, padding_value=padding_value)\n        if self.tokenizer.padding_side == \"left\":\n            input_ids = torch.flip(input_ids, [1])\n        return input_ids\n\n    @property\n    def batch_size(self):\n        return self.batch_size_per_gpu\n\n    @property\n    def device(self):\n        return self._device\n\n    @property\n    def rank(self):\n        return self._rank\n\n    @property\n    def world_size(self):\n        return self._world_size\n\n    def tok_encode(self, string: str, left_truncate_len=None, add_special_tokens=None) -> List[int]:\n        \"\"\" \"\"\"\n        add_special_tokens = False if add_special_tokens is None else add_special_tokens\n        encoding = self.tokenizer.encode(string, add_special_tokens=add_special_tokens)\n        # left-truncate the encoded context to be at most `left_truncate_len` tokens long\n        if left_truncate_len:\n            encoding = encoding[-left_truncate_len:]\n        return encoding\n\n    def tok_decode(self, tokens):\n        try:\n            return self.tokenizer.decode(tokens)\n        except:\n            return self.tokenizer.decode([tokens])\n\n    def loglikelihood(self, requests: List[Instance]) -> List[Tuple[float, bool]]:\n        # TODO\n        res = []\n        pbar = tqdm(total=len(requests), disable=(self.rank != 0), desc=\"Model Responding\")\n\n        for contexts, doc_to_target, doc_to_visual, doc_id, task, split in [reg.args for reg in requests]:\n            # encode, pad, and truncate contexts for this batch\n            if type(doc_to_target) == str:\n                continuation = doc_to_target\n            else:\n                continuation = doc_to_target(self.task_dict[task][split][doc_id])\n            visuals = [doc_to_visual(self.task_dict[task][split][doc_id])]\n            visuals = self.flatten(visuals)\n            image_sizes = [[visual.size[0], visual.size[1]] for visual in visuals]\n            if visuals:\n                image = process_images(visuals, self._image_processor, self._config)\n                if type(image) is list:\n                    image = [_image.to(dtype=torch.float16, device=self.device) for _image in image]\n                else:\n                    image = image.to(dtype=torch.float16, device=self.device)\n            else:\n                image = None\n\n            prompts_input = contexts[0] if isinstance(contexts, list) else contexts\n\n            if image is not None and len(image) != 0 and DEFAULT_IMAGE_TOKEN not in prompts_input:\n                \"\"\"\n                Three senarios:\n                1. No image, and there for, no image token should be added.\n                2. image token is already specified in the context, so we don't need to add it.\n                3. image token is not specified in the context and there is image inputs, so we need to add it. In this case, we add the image token at the beginning of the context and add a new line.\n                \"\"\"\n                image_tokens = [DEFAULT_IMAGE_TOKEN] * len(visuals)\n                image_tokens = \" \".join(image_tokens)\n                prompts_input = image_tokens + \"\\n\" + (contexts[0] if isinstance(contexts, list) else contexts)\n\n            # This is much safer for llama3, as we now have some object type in it\n            if \"llama_3\" in self.conv_template:\n                conv = copy.deepcopy(conv_templates[self.conv_template])\n            else:\n                conv = conv_templates[self.conv_template].copy()\n\n            conv.append_message(conv.roles[0], prompts_input)\n            conv.append_message(conv.roles[1], None)\n            prompt = conv.get_prompt()\n            pad_token_id = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id\n            contxt_id = tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(self.device)\n            # Add the answer of the second role\n            conv.messages[1][1] = continuation\n\n            prompt = conv.get_prompt()\n            input_ids = tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(self.device)\n            labels = input_ids.clone()\n            # Context part no need to calculate for loss\n            labels[0, : contxt_id.shape[1]] = -100\n            with torch.inference_mode():\n                outputs = self.model(input_ids=input_ids, labels=labels, images=image, use_cache=True, image_sizes=image_sizes)\n            loss = outputs[\"loss\"]\n            # loss = torch.exp(loss)\n            logits = outputs[\"logits\"]\n            greedy_tokens = logits.argmax(dim=-1)\n            cont_toks = input_ids[:, contxt_id.shape[1] :]  # [1, seq]\n            greedy_tokens = greedy_tokens[:, contxt_id.shape[1] : input_ids.shape[1]]  # [1, seq]\n            max_equal = (greedy_tokens == cont_toks).all()\n            res.append((float(loss.item()), bool(max_equal)))\n            pbar.update(1)\n\n        pbar.close()\n        return res\n\n    def flatten(self, input):\n        new_list = []\n        for i in input:\n            for j in i:\n                new_list.append(j)\n        return new_list\n\n    def load_video(self, video_path, max_frames_num):\n        if type(video_path) == str:\n            vr = VideoReader(video_path, ctx=cpu(0))\n        else:\n            vr = VideoReader(video_path[0], ctx=cpu(0))\n        total_frame_num = len(vr)\n        uniform_sampled_frames = np.linspace(0, total_frame_num - 1, max_frames_num, dtype=int)\n        frame_idx = uniform_sampled_frames.tolist()\n        spare_frames = vr.get_batch(frame_idx).asnumpy()\n        return spare_frames  # (frames, height, width, channels)\n\n    def generate_until(self, requests: List[Instance]) -> List[str]:\n        res = []\n\n        def _collate(x):\n            # the negative sign on len(toks) sorts descending - this has a few advantages:\n            # - time estimates will always be over not underestimates, which is more useful for planning\n            # - to know the size of a batch when going through the list, you know the first one is always the batch\n            #   padded context length. this is useful to simplify the batching logic and more importantly to make\n            #   automatic adaptive batches much much easier to implement\n            # - any OOMs will happen right away rather than near the end\n            toks = self.tok_encode(x[0])\n            return -len(toks), x[0]\n\n        # we group requests by their generation_kwargs,\n        # so that we don't try to execute e.g. greedy sampling and temp=0.8 sampling\n        # in the same batch.\n        re_ords = utils.Collator([reg.args for reg in requests], _collate, grouping=True)\n        chunks = re_ords.get_batched(n=self.batch_size, batch_fn=None)\n        num_iters = len(requests) // self.batch_size if len(requests) % self.batch_size == 0 else len(requests) // self.batch_size + 1\n        pbar = tqdm(total=num_iters, disable=(self.rank != 0), desc=\"Model Responding\")\n        for chunk in chunks:\n            batched_contexts, all_gen_kwargs, batched_doc_to_visual, batched_doc_id, batched_task, batched_split = zip(*chunk)\n            task = batched_task[0]\n            split = batched_split[0]\n            batched_visuals = [batched_doc_to_visual[0](self.task_dict[task][split][ids]) for ids in batched_doc_id]  # [B, N]\n            flattened_visuals = self.flatten(batched_visuals)  # [B*N]\n            assert len(batched_visuals) == 1\n\n            # we assume all gen kwargs in the batch are the same\n            # this is safe to assume because the `grouper` object ensures it.\n            gen_kwargs = all_gen_kwargs[0]\n            if \"until\" in gen_kwargs:\n                gen_kwargs.pop(\"until\")\n\n            question_input = []\n\n            for visual, context in zip(batched_visuals, batched_contexts):\n                if \"image_aspect_ratio\" in gen_kwargs.keys() and \"image_aspect_ratio\" not in self._config.__dict__:\n                    # here we should pop it out of gen_kwargs so that it doesn't get passed to the model for next step of generation\n                    self._config.image_aspect_ratio = gen_kwargs.pop(\"image_aspect_ratio\")\n                    eval_logger.info(f\"Setting image aspect ratio: {self._config.image_aspect_ratio}\")\n\n                # encode, pad, and truncate contexts for this batch\n                if type(visual[0]) == PIL.Image.Image:  # For image task\n                    image_tensor = process_images(visual, self._image_processor, self._config)\n                    if type(image_tensor) is list:\n                        image_tensor = [_image.to(dtype=torch.float16, device=self.device) for _image in image_tensor]\n                    else:\n                        image_tensor = image_tensor.to(dtype=torch.float16, device=self.device)\n\n                    task_type = \"image\"\n\n                elif type(visual[0]) == str:  # For video task\n                    image_tensor = []\n                    try:\n                        if self.video_decode_backend == \"decord\":\n                            frames = self.load_video(visual, self.max_frames_num)\n                        elif self.video_decode_backend == \"pyav\":\n                            frames = read_video_pyav(visual[0], num_frm=self.max_frames_num)\n                        frames = self._image_processor.preprocess(frames, return_tensors=\"pt\")[\"pixel_values\"].half().cuda()\n                        image_tensor.append(frames)\n                    except Exception as e:\n                        eval_logger.error(f\"Error {e} in loading video\")\n                        image_tensor = None\n\n                    task_type = \"video\"\n\n                if image_tensor is not None and len(image_tensor) != 0 and DEFAULT_IMAGE_TOKEN not in context:\n                    \"\"\"\n                    Three senarios:\n                    1. No image, and there for, no image token should be added.\n                    2. image token is already specified in the context, so we don't need to add it.\n                    3. image token is not specified in the context and there is image inputs, so we need to add it. In this case, we add the image token at the beginning of the context and add a new line.\n                    4. For video tasks, we could add a <image> token or multiple <image> tokens for each frame in the context. This depends on the training strategy and should balance in test to decide which is better\n                    \"\"\"\n                    if task_type == \"image\":\n                        image_tokens = [DEFAULT_IMAGE_TOKEN] * len(visual) if isinstance(visual, list) else [DEFAULT_IMAGE_TOKEN]\n                    elif task_type == \"video\":\n                        image_tokens = [DEFAULT_IMAGE_TOKEN] * len(frames) if self.token_strategy == \"multiple\" else [DEFAULT_IMAGE_TOKEN]\n\n                    image_tokens = \" \".join(image_tokens)\n                    question = image_tokens + \"\\n\" + context\n                else:\n                    question = context\n\n                # This is much safer for llama3, as we now have some object type in it\n                if \"llama_3\" in self.conv_template:\n                    conv = copy.deepcopy(conv_templates[self.conv_template])\n                else:\n                    conv = conv_templates[self.conv_template].copy()\n                conv.append_message(conv.roles[0], question)\n                conv.append_message(conv.roles[1], None)\n                prompt_question = conv.get_prompt()\n                question_input.append(prompt_question)\n\n            # preconfigure gen_kwargs with defaults\n            if \"max_new_tokens\" not in gen_kwargs:\n                gen_kwargs[\"max_new_tokens\"] = 1024\n            if \"temperature\" not in gen_kwargs:\n                gen_kwargs[\"temperature\"] = 0\n            if \"do_sample\" not in gen_kwargs:\n                gen_kwargs[\"do_sample\"] = False\n            if \"top_p\" not in gen_kwargs:\n                gen_kwargs[\"top_p\"] = None\n            if \"num_beams\" not in gen_kwargs:\n                gen_kwargs[\"num_beams\"] = 1\n\n            input_ids_list = [tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\") for prompt in question_input]\n            pad_token_ids = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id\n            input_ids = self.pad_sequence(input_ids_list, batch_first=True, padding_value=pad_token_ids).to(self.device)\n            attention_masks = input_ids.ne(pad_token_ids).to(self.device)\n\n            if task_type == \"image\":\n                gen_kwargs[\"image_sizes\"] = [flattened_visuals[idx].size for idx in range(len(flattened_visuals))]\n            elif task_type == \"video\":\n                stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n                keywords = [stop_str]\n                stopping_criteria = KeywordsStoppingCriteria(keywords, self.tokenizer, input_ids)\n                gen_kwargs[\"modalities\"] = [\"video\"]\n                gen_kwargs[\"stopping_criteria\"] = [stopping_criteria]\n                self._config.mm_spatial_pool_stride = self.mm_spatial_pool_stride\n                self._config.mm_spatial_pool_mode = self.mm_spatial_pool_mode\n\n            # These steps are not in LLaVA's original code, but are necessary for generation to work\n            # TODO: attention to this major generation step...\n            if \"image_aspect_ratio\" in gen_kwargs.keys():\n                gen_kwargs.pop(\"image_aspect_ratio\")\n            try:\n                with torch.inference_mode():\n                    cont = self.model.generate(input_ids, attention_mask=attention_masks, pad_token_id=pad_token_ids, images=image_tensor, use_cache=self.use_cache, **gen_kwargs)\n\n                text_outputs = self.tokenizer.batch_decode(cont, skip_special_tokens=True)\n            except Exception as e:\n                raise e\n\n            text_outputs = [response.strip() for response in text_outputs]\n            res.extend(text_outputs)\n            self.cache_hook.add_partial(\"generate_until\", (context, gen_kwargs), text_outputs)\n            pbar.update(1)\n            # reorder this group of results back to original unsorted form\n        res = re_ords.get_original(res)\n\n        pbar.close()\n        return res\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/api/__init__.py", "content": ""}
{"type": "source_file", "path": "evaluate/lmms_eval/models/fuyu.py", "content": "import warnings\n\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\")\n\nfrom accelerate import Accelerator, DistributedType\nfrom transformers import FuyuForCausalLM, AutoTokenizer, FuyuImageProcessor, FuyuProcessor\nfrom lmms_eval.api.model import lmms\nfrom lmms_eval.api.registry import register_model\nimport torch\nfrom PIL import Image\nfrom typing import List, Optional, Union, Tuple\nfrom lmms_eval import utils\nfrom lmms_eval.api.instance import Instance\nfrom tqdm import tqdm\nfrom accelerate import Accelerator, DistributedType\nfrom accelerate.state import AcceleratorState\n\nfrom loguru import logger as eval_logger\n\n\n@register_model(\"fuyu\")\nclass Fuyu(lmms):\n    \"\"\"\n    Fuyu Model\n    \"\"\"\n\n    def __init__(\n        self,\n        pretrained: str = \"adept/fuyu-8b\",\n        device: Optional[str] = \"cuda\",\n        max_new_tokens: int = 256,\n        batch_size: Optional[Union[int, str]] = 1,\n        **kwargs,\n    ) -> None:\n        super().__init__()\n        # Do not use kwargs for now\n        assert kwargs == {}, f\"Unexpected kwargs: {kwargs}\"\n\n        accelerator = Accelerator()\n        if accelerator.num_processes > 1:\n            self._device = torch.device(f\"cuda:{accelerator.local_process_index}\")\n        else:\n            self._device = device\n\n        self._model = FuyuForCausalLM.from_pretrained(pretrained, torch_dtype=torch.bfloat16, device_map=self.device)\n        self.model.eval()\n        self.model.tie_weights()\n        self._tokenizer = AutoTokenizer.from_pretrained(pretrained)\n        self._config = self.model.config\n\n        self.image_processor = FuyuImageProcessor()\n        self.processor = FuyuProcessor(image_processor=self.image_processor, tokenizer=self.tokenizer)\n        self.max_new_tokens = max_new_tokens\n        self.batch_size_per_gpu = int(batch_size)\n        accelerator = Accelerator()\n        if accelerator.num_processes > 1:\n            assert accelerator.distributed_type in [DistributedType.FSDP, DistributedType.MULTI_GPU, DistributedType.DEEPSPEED], \"Unsupported distributed type provided. Only DDP and FSDP are supported.\"\n            # If you want to use DistributedType.DEEPSPEED, you have to run accelerate config before using the model\n            # Also, you have to select zero stage 0 (equivalent to DDP) in order to make the prepare model works\n            # I tried to set different parameters in the kwargs to let default zero 2 stage works, but it didn't work.\n            if accelerator.distributed_type == DistributedType.DEEPSPEED:\n                kwargs = {\n                    \"train_micro_batch_size_per_gpu\": self.batch_size_per_gpu,\n                    \"train_batch_size\": self.batch_size_per_gpu * accelerator.num_processes,\n                }\n                AcceleratorState().deepspeed_plugin.deepspeed_config_process(must_match=True, **kwargs)\n                eval_logger.info(\"Detected that you are using DistributedType.DEEPSPEED. Make sure you run `accelerate config` and set zero stage to 0\")\n            if accelerator.distributed_type == DistributedType.FSDP or accelerator.distributed_type == DistributedType.DEEPSPEED:\n                self._model = accelerator.prepare(self.model)\n            else:\n                self._model = accelerator.prepare_model(self.model, evaluation_mode=True)\n            self.accelerator = accelerator\n            if self.accelerator.is_local_main_process:\n                eval_logger.info(f\"Using {accelerator.num_processes} devices with data parallelism\")\n            self._rank = self.accelerator.local_process_index\n            self._world_size = self.accelerator.num_processes\n        else:\n            self.model.to(self._device)\n            self._rank = 0\n            self._word_size = 1\n\n        \"\"\"if accelerator.num_processes > 1:\n            assert accelerator.distributed_type in [\n                DistributedType.FSDP,\n                DistributedType.MULTI_GPU,\n            ], \"Unsupported distributed type provided. Only DDP and FSDP are supported.\"\n            if accelerator.distributed_type == DistributedType.FSDP:\n                self._model = accelerator.prepare(self.model)\n            else:\n                self._model = accelerator.prepare_model(self.model, evaluation_mode=True)\n            self.accelerator = accelerator\n            if self.accelerator.is_local_main_process:\n                eval_logger.info(f\"Using {accelerator.num_processes} devices with data parallelism\")\n            self._rank = self.accelerator.local_process_index\n            self._world_size = self.accelerator.num_processes\"\"\"\n\n    @property\n    def config(self):\n        # return the associated transformers.AutoConfig for the given pretrained model.\n        return self._config\n\n    @property\n    def tokenizer(self):\n        return self._tokenizer\n\n    @property\n    def model(self):\n        # returns the model, unwrapping it if using Accelerate\n        if hasattr(self, \"accelerator\"):\n            return self.accelerator.unwrap_model(self._model)\n        else:\n            return self._model\n\n    @property\n    def eot_token_id(self):\n        # we use EOT because end of *text* is more accurate for what we're doing than end of *sentence*\n        return self.tokenizer.eos_token_id\n\n    @property\n    def max_length(self):\n        # Assuming max_length is the sum of max context tokens and max new tokens\n        return self.tokenizer.model_max_length\n\n    @property\n    def batch_size(self):\n        return self.batch_size_per_gpu\n\n    @property\n    def device(self):\n        return self._device\n\n    @property\n    def rank(self):\n        return self._rank\n\n    @property\n    def world_size(self):\n        return self._world_size\n\n    def flatten(self, input, only_get_first=False):\n        new_list = []\n        for i in input:\n            for j in i:\n                new_list.append(j)\n                if only_get_first:\n                    break\n        return new_list\n\n    def generate_until(self, requests: List[Instance]) -> List[str]:\n        res = []\n\n        def _collate(x):\n            # the negative sign on len(toks) sorts descending - this has a few advantages:\n            # - time estimates will always be over not underestimates, which is more useful for planning\n            # - to know the size of a batch when going through the list, you know the first one is always the batch\n            #   padded context length. this is useful to simplify the batching logic and more importantly to make\n            #   automatic adaptive batches much much easier to implement\n            # - any OOMs will happen right away rather than near the end\n            toks = self.tok_encode(x[0])\n            return -len(toks), x[0]\n\n        re_ords = utils.Collator([reg.args for reg in requests], _collate, grouping=True)\n        chunks = re_ords.get_batched(n=self.batch_size, batch_fn=None)\n        num_iters = len(requests) // self.batch_size if len(requests) % self.batch_size == 0 else len(requests) // self.batch_size + 1\n        pbar = tqdm(total=num_iters, disable=(self.rank != 0), desc=\"Model Responding\")\n\n        for chunk in chunks:\n            contexts, all_gen_kwargs, doc_to_visual, doc_id, task, split = zip(*chunk)\n            task = task[0]\n            split = split[0]\n            visuals = [doc_to_visual[0](self.task_dict[task][split][ids]) for ids in doc_id]\n            visuals = self.flatten(visuals, only_get_first=True)\n            gen_kwargs = all_gen_kwargs[0]\n\n            # if isinstance(visuals[0], list):\n            #     visuals = [visuals[idx][0] for idx in range(len(visuals))]  # get the first image in multi-image scenarios.\n\n            # assert len(contexts) == self.batch_size_per_gpu, f\"Expected contexts batch size {self.batch_size_per_gpu}, got {len(contexts)}\"\n            # assert len(visuals) == self.batch_size_per_gpu, f\"Expected visuals batch size {self.batch_size_per_gpu}, got {len(visuals)}\"\n            formatted_contexts = [f\"{context}\\n\" for context in contexts]\n            model_inputs = self.processor(text=formatted_contexts, images=visuals, device=self.device)\n            for k, v in model_inputs.items():\n                model_inputs[k] = v.to(self.device, non_blocking=True) if isinstance(v, torch.Tensor) else [vv.to(self.device, non_blocking=True) for vv in v]\n\n            for index in range(len(model_inputs[\"image_patches\"])):\n                model_inputs[\"image_patches\"][index] = model_inputs[\"image_patches\"][index].to(dtype=next(self.model.parameters()).dtype)\n\n            # preconfigure gen_kwargs with defaults\n            gen_kwargs[\"image_sizes\"] = [visuals[idx].size for idx in range(len(visuals))]\n            if \"max_new_tokens\" not in gen_kwargs:\n                gen_kwargs[\"max_new_tokens\"] = 256\n            if \"temperature\" not in gen_kwargs:\n                gen_kwargs[\"temperature\"] = 0\n            if \"top_p\" not in gen_kwargs:\n                gen_kwargs[\"top_p\"] = None\n            if \"num_beams\" not in gen_kwargs:\n                gen_kwargs[\"num_beams\"] = 1\n            # generation_output = self.model.generate(\n            #     **model_inputs, temperature=gen_kwargs[\"temperature\"], max_new_tokens=gen_kwargs[\"max_new_tokens\"], top_p=gen_kwargs[\"top_p\"], num_beams=gen_kwargs[\"num_beams\"], pad_token_id=self.tokenizer.eos_token_id\n            # )\n            generation_output = self.model.generate(**model_inputs, max_new_tokens=gen_kwargs[\"max_new_tokens\"], pad_token_id=self.tokenizer.eos_token_id)\n            generation_texts = self.processor.batch_decode(generation_output, skip_special_tokens=True)\n            response = [gen_text.split(\"\\x04\")[1].strip(\" \").strip(\"\\n\") for gen_text in generation_texts]\n            res.extend(response)\n            pbar.update(1)\n\n        pbar.close()\n        return res\n\n    def loglikelihood(self, requests: List[Instance]) -> List[Tuple[float, bool]]:\n        # TODO\n        res = []\n        pbar = tqdm(total=len(requests), disable=(self.rank != 0), desc=\"Model Responding\")\n\n        for contexts, doc_to_target, doc_to_visual, doc_id, task, split in [reg.args for reg in requests]:\n            # encode, pad, and truncate contexts for this batch\n            if type(doc_to_target) == str:\n                continuation = doc_to_target\n            else:\n                continuation = doc_to_target(self.task_dict[task][split][doc_id])\n            visuals = [doc_to_visual(self.task_dict[task][split][doc_id])]\n            visuals = self.flatten(visuals)\n            formatted_contexts = [f\"{contexts}\\n\"]\n            formatted_continuation = [f\"{contexts}\\n{continuation}\"]\n            model_inputs = self.processor(text=formatted_continuation, images=visuals, device=self.device)\n            for k, v in model_inputs.items():\n                model_inputs[k] = v.to(self.device, non_blocking=True) if isinstance(v, torch.Tensor) else [vv.to(self.device, non_blocking=True) for vv in v]\n\n            for index in range(len(model_inputs[\"image_patches\"])):\n                model_inputs[\"image_patches\"][index] = model_inputs[\"image_patches\"][index].to(dtype=next(self.model.parameters()).dtype)\n\n            labels = model_inputs[\"input_ids\"].clone()\n            contxt_id = self.processor(text=formatted_contexts, return_tensors=\"pt\")[\"input_ids\"]\n            labels[: len(contxt_id)] = -100\n            with torch.inference_mode():\n                outputs = self.model(**model_inputs, labels=labels)\n            loss = outputs[\"loss\"]\n            # loss = torch.exp(loss)\n            logits = outputs[\"logits\"]\n            greedy_tokens = logits.argmax(dim=-1)\n            cont_toks = model_inputs[\"input_ids\"][:, contxt_id.shape[1] :]  # [1, seq]\n            greedy_tokens = greedy_tokens[:, contxt_id.shape[1] : model_inputs[\"input_ids\"].shape[1]]  # [1, seq]\n            max_equal = (greedy_tokens == cont_toks).all()\n            res.append((float(loss.item()), bool(max_equal)))\n            pbar.update(1)\n\n        pbar.close()\n        return res\n\n    def tok_encode(self, string: str, left_truncate_len=None, add_special_tokens=None) -> List[int]:\n        \"\"\" \"\"\"\n        add_special_tokens = False if add_special_tokens is None else add_special_tokens\n        encoding = self.tokenizer.encode(string, add_special_tokens=add_special_tokens)\n        # left-truncate the encoded context to be at most `left_truncate_len` tokens long\n        if left_truncate_len:\n            encoding = encoding[-left_truncate_len:]\n        return encoding\n\n    def tok_decode(self, tokens):\n        return self.tokenizer.decode(tokens)\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/models/mplug_owl_video/configuration_mplug_owl.py", "content": "# coding=utf-8\n# Copyright 2022 x-plug and The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" MplugOwl model configuration \"\"\"\nimport copy\nimport os\nfrom typing import Union\n\nfrom transformers.configuration_utils import PretrainedConfig\nfrom transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\nfrom transformers.models.auto import CONFIG_MAPPING\n\nfrom loguru import logger\n\nMPLUG_OWL_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n    \"MAGAer13/mplug-owl-llama-7b\": \"https://huggingface.co/MAGAer13/mplug-owl-llama-7b/resolve/main/config.json\",\n    # See all MplugOwl models at https://huggingface.co/models?filter=mplug_owl\n}\n\n\nclass MplugOwlVisionConfig(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`MplugOwlVisionModel`]. It is used to instantiate a\n     mPLUG-Owl vision encoder according to the specified arguments, defining the model architecture. Instantiating a\n     configuration defaults will yield a similar configuration to that of the mPLUG-Owl\n     [x-plug/x_plug-llama-7b](https://huggingface.co/x-plug/x_plug-llama-7b) architecture.\n\n     Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n     documentation from [`PretrainedConfig`] for more information.\n\n     Args:\n         hidden_size (`int`, *optional*, defaults to 768):\n             Dimensionality of the encoder layers and the pooler layer.\n         intermediate_size (`int`, *optional*, defaults to 3072):\n             Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n         num_hidden_layers (`int`, *optional*, defaults to 12):\n             Number of hidden layers in the Transformer encoder.\n         num_attention_heads (`int`, *optional*, defaults to 12):\n             Number of attention heads for each attention layer in the Transformer encoder.\n         image_size (`int`, *optional*, defaults to 224):\n             The size (resolution) of each image.\n         patch_size (`int`, *optional*, defaults to 32):\n             The size (resolution) of each patch.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"quick_gelu\"`):\n             The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n             `\"relu\"`, `\"selu\"` and `\"gelu_new\"` ``\"quick_gelu\"` are supported.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-5):\n             The epsilon used by the layer normalization layers.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n         initializer_range (`float`, *optional*, defaults to 0.02):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         initializer_factor (`float`, *optional*, defaults to 1):\n             A factor for initializing all weight matrices (should be kept to 1, used internally for initialization\n             testing).\n\n\n     ```\"\"\"\n\n    model_type = \"mplug_owl_vision_model\"\n\n    def __init__(\n        self,\n        hidden_size=1024,\n        intermediate_size=4096,\n        projection_dim=768,\n        num_hidden_layers=24,\n        num_attention_heads=16,\n        num_channels=3,\n        image_size=224,\n        patch_size=14,\n        hidden_act=\"quick_gelu\",\n        layer_norm_eps=1e-6,\n        attention_dropout=0.0,\n        initializer_range=0.02,\n        initializer_factor=1.0,\n        use_flash_attn=False,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.projection_dim = projection_dim\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.num_channels = num_channels\n        self.patch_size = patch_size\n        self.image_size = image_size\n        self.initializer_range = initializer_range\n        self.initializer_factor = initializer_factor\n        self.attention_dropout = attention_dropout\n        self.layer_norm_eps = layer_norm_eps\n        self.hidden_act = hidden_act\n        self.use_flash_attn = use_flash_attn\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n\n        # get the vision config dict if we are loading from MplugOwlConfig\n        if config_dict.get(\"model_type\") == \"mplug-owl\":\n            config_dict = config_dict[\"vision_config\"]\n\n        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n            logger.warning(f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \" f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\")\n\n        return cls.from_dict(config_dict, **kwargs)\n\n\nclass MplugOwlVisualAbstractorConfig(PretrainedConfig):\n    model_type = \"mplug_owl_visual_abstract\"\n\n    def __init__(\n        self,\n        hidden_size=1024,  #\n        num_hidden_layers=6,  #\n        num_attention_heads=16,  #\n        intermediate_size=4096,  #\n        attention_probs_dropout_prob=0.1,  #\n        initializer_range=0.02,\n        layer_norm_eps=1e-6,  #\n        encoder_hidden_size=1024,  #\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.hidden_size = hidden_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.intermediate_size = intermediate_size\n        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n        self.initializer_range = initializer_range\n        self.layer_norm_eps = layer_norm_eps\n        self.encoder_hidden_size = encoder_hidden_size\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n\n        # get the visual_abstractor config dict if we are loading from MplugOwlConfig\n        if config_dict.get(\"model_type\") == \"mplug-owl\":\n            config_dict = config_dict[\"abstractor_config\"]\n\n        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n            logger.warning(f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \" f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\")\n\n        return cls.from_dict(config_dict, **kwargs)\n\n\nclass MplugOwlConfig(PretrainedConfig):\n    r\"\"\"\n    [`MplugOwlConfig`] is the configuration class to store the configuration of a [`MplugOwlForConditionalGeneration`]. It is\n    used to instantiate a mPLUG-Owl model according to the specified arguments, defining the vision model, Q-Former model\n    and language model configs. Instantiating a configuration with the defaults will yield a similar configuration to\n    that of the mPLUG-Owl [x-plug/x_plug-llama-7b](https://huggingface.co/x-plug/x_plug-llama-7b) architecture.\n\n    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PretrainedConfig`] for more information.\n\n    Args:\n        vision_config (`dict`, *optional*):\n            Dictionary of configuration options used to initialize [`MplugOwlVisionConfig`].\n        visual_abstractor_config (`dict`, *optional*):\n            Dictionary of configuration options used to initialize [`MplugOwlVisualAbstractorConfig`].\n        text_config (`dict`, *optional*):\n            Dictionary of configuration options used to initialize any [`PretrainedConfig`].\n        num_query_tokens (`int`, *optional*, defaults to 32):\n            The number of query tokens passed through the Transformer.\n\n        kwargs (*optional*):\n            Dictionary of keyword arguments.\n\n    Example:\n\n    ```python\n    >>> from transformers import (\n    ...     MplugOwlVisionConfig,\n    ...     MplugOwlVisualAbstractorConfig,\n    ...     OPTConfig,\n    ...     MplugOwlConfig,\n    ...     MplugOwlForConditionalGeneration,\n    ... )\n\n    >>> # Initializing a MplugOwlConfig with x-plug/x_plug-llama-7b style configuration\n    >>> configuration = MplugOwlConfig()\n\n    >>> # Initializing a MplugOwlForConditionalGeneration (with random weights) from the x-plug/x_plug-llama-7b style configuration\n    >>> model = MplugOwlForConditionalGeneration(configuration)\n\n    >>> # Accessing the model configuration\n    >>> configuration = model.config\n\n    >>> # We can also initialize a MplugOwlConfig from a MplugOwlVisionConfig, MplugOwlVisualAbstractorConfig and any PretrainedConfig\n\n    >>> # Initializing mPLUG-Owl vision, mPLUG-Owl Q-Former and language model configurations\n    >>> vision_config = MplugOwlVisionConfig()\n    >>> visual_abstractor_config = MplugOwlVisualAbstractorConfig()\n    >>> text_config = OPTConfig()\n\n    >>> config = MplugOwlConfig.from_text_vision_configs(vision_config, visual_abstractor_config, text_config)\n    ```\"\"\"\n\n    model_type = \"mplug-owl\"\n    is_composition = True\n\n    def __init__(self, vision_config=None, visual_abstractor_config=None, text_config=None, num_query_tokens=64, **kwargs):\n        super().__init__(**kwargs)\n        if vision_config is None:\n            vision_config = MplugOwlVisionConfig().to_dict()\n            logger.info(\"vision_config is None.\")\n\n        if visual_abstractor_config is None:\n            visual_abstractor_config = {}\n            logger.info(\"abstractor_config is None. \")\n\n        if text_config is None:\n            # we use LLAMA 7b by default\n            from ..llama.configuration_llama import LlamaConfig\n\n            text_config = LlamaConfig(pad_token_id=2).to_dict()\n            logger.info(\"text_config is None.\")\n\n        self.vision_config = MplugOwlVisionConfig(**vision_config)\n        self.visual_abstractor_config = MplugOwlVisualAbstractorConfig(**visual_abstractor_config)\n        # self.visual_abstractor_config.layer_norm_eps = 1e-6\n        text_model_type = text_config[\"model_type\"] if \"model_type\" in text_config else \"llama\"\n        self.text_config = CONFIG_MAPPING[text_model_type](**text_config)\n\n        self.tie_word_embeddings = self.text_config.tie_word_embeddings\n        self.is_encoder_decoder = self.text_config.is_encoder_decoder\n\n        self.num_query_tokens = num_query_tokens\n        # self.visual_abstractor_config.encoder_hidden_size = self.vision_config.hidden_size\n        self.use_decoder_only_language_model = self.text_config.model_type in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n        self.initializer_factor = 1.0\n        self.initializer_range = 0.02\n\n        for attr in dir(self.text_config):\n            if not hasattr(self, attr):\n                setattr(self, attr, getattr(self.text_config, attr))\n\n    @classmethod\n    def from_vision_visual_abstractor_text_configs(\n        cls,\n        vision_config: MplugOwlVisionConfig,\n        visual_abstractor_config: MplugOwlVisualAbstractorConfig,\n        text_config: PretrainedConfig,\n        **kwargs,\n    ):\n        r\"\"\"\n        Instantiate a [`MplugOwlConfig`] (or a derived class) from a mPLUG-Owl vision model, Q-Former and language model\n        configurations.\n\n        Returns:\n            [`MplugOwlConfig`]: An instance of a configuration object\n        \"\"\"\n\n        return cls(\n            vision_config=vision_config.to_dict(),\n            visual_abstractor_config=visual_abstractor_config.to_dict(),\n            text_config=text_config.to_dict(),\n            **kwargs,\n        )\n\n    def to_dict(self):\n        \"\"\"\n        Serializes this instance to a Python dictionary. Override the default [`~PretrainedConfig.to_dict`].\n\n        Returns:\n            `Dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,\n        \"\"\"\n        output = copy.deepcopy(self.__dict__)\n        output[\"vision_config\"] = self.vision_config.to_dict()\n        output[\"visual_abstractor_config\"] = self.visual_abstractor_config.to_dict()\n        output[\"text_config\"] = self.text_config.to_dict()\n        output[\"model_type\"] = self.__class__.model_type\n        return output\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/models/mantis.py", "content": "import torch\n\ntorch.backends.cuda.matmul.allow_tf32 = True\n\n\nimport copy\nfrom tqdm import tqdm\nfrom datetime import timedelta\n\nfrom lmms_eval import utils\nfrom lmms_eval.api.instance import Instance\nfrom lmms_eval.api.model import lmms\nfrom lmms_eval.api.registry import register_model\nfrom lmms_eval.utils import stop_sequences_criteria\n\nfrom accelerate import Accelerator, DistributedType, InitProcessGroupKwargs\nfrom accelerate.state import AcceleratorState\nfrom typing import List, Optional, Union, Tuple\nfrom packaging import version\nimport warnings\n\nfrom loguru import logger as eval_logger\n\nwarnings.filterwarnings(\"ignore\")\n\ntry:\n    from mantis.models.mllava import LlavaForConditionalGeneration, MLlavaProcessor\n    from mantis.models.mfuyu import MFuyuForCausalLM, MFuyuProcessor\n    from mantis.models.conversation import conv_mllava_v1 as default_conv, conv_templates\n    \nexcept Exception as e:\n    eval_logger.debug(\"Mantis is not installed. Please install Mantis to use this model.\\nError: %s\" % e)\n    \ntry:\n    from transformers import AutoModelForVision2Seq, AutoProcessor\nexcept Exception as e:\n    eval_logger.debug(\"Upgrade transformers to use Mantis's idefics model.\\nError: %s\" % e)\n\n# inference implementation for attention, can be \"sdpa\", \"eager\", \"flash_attention_2\". Seems FA2 is not effective during inference: https://discuss.huggingface.co/t/flash-attention-has-no-effect-on-inference/73453/5\n# if is_flash_attn_2_available:\n#     best_fit_attn_implementation = \"flash_attention_2\" # flash_attn has a bug that says: ERROR Error query and key must have the same dtype in generating\n\ntry:\n    import flash_attn\n    \n    best_fit_attn_implementation = \"flash_attention_2\"\nexcept ImportError:\n    best_fit_attn_implementation = \"eager\"\n\nDEFAULT_IMAGE_TOKEN = \"<image>\"\n\n@register_model(\"mantis\")\nclass Mantis(lmms):\n    \"\"\"\n    Mantis Model\n    This implementation is adpated from the Llava model from llava.py and the Idefics model from idefics.py\n    \"\"\"\n\n    def __init__(\n        self,\n        pretrained: str = \"TIGER-Lab/Mantis-8B-siglip-llama3\",\n        truncation: Optional[bool] = True,\n        device: Optional[str] = \"cuda:0\",\n        dtype: Optional[Union[str, torch.dtype]] = \"float16\",\n        batch_size: Optional[Union[int, str]] = 1,\n        attn_implementation=best_fit_attn_implementation,\n        device_map=\"cuda:0\",\n        use_cache=True,\n        truncate_context=False,  # whether to truncate the context in generation, set it False for LLaVA-1.6\n        **kwargs,\n    ) -> None:\n        super().__init__()\n        # Do not use kwargs for now\n        assert kwargs == {}, f\"Unexpected kwargs: {kwargs}\"\n\n        accelerator_kwargs = InitProcessGroupKwargs(timeout=timedelta(weeks=52))\n        accelerator = Accelerator(kwargs_handlers=[accelerator_kwargs])\n        if accelerator.num_processes > 1:\n            self._device = torch.device(f\"cuda:{accelerator.local_process_index}\")\n            self.device_map = f\"cuda:{accelerator.local_process_index}\"\n        elif accelerator.num_processes == 1 and device_map == \"auto\":\n            self._device = torch.device(device)\n            self.device_map = device_map\n        else:\n            self._device = torch.device(f\"cuda:{accelerator.local_process_index}\")\n            self.device_map = f\"cuda:{accelerator.local_process_index}\"\n        \n        self._is_idefics = \"idefics\" in pretrained.lower()\n        if isinstance(dtype, str) and dtype != \"auto\":\n            dtype = getattr(torch, dtype)\n        \n        # Here we load the \"non-idefics\" Mantis model.\n        if not self._is_idefics:\n            if 'fuyu' in pretrained.lower():\n                self._processor = MFuyuProcessor.from_pretrained(pretrained)\n                self._model = MFuyuForCausalLM.from_pretrained(pretrained, device_map=self.device_map, attn_implementation=attn_implementation, torch_dtype=dtype)\n            else:\n                self._processor = MLlavaProcessor.from_pretrained(pretrained)\n                self._model = LlavaForConditionalGeneration.from_pretrained(pretrained, device_map=self.device_map, attn_implementation=attn_implementation, torch_dtype=dtype)\n            \n        else:\n            self._processor = AutoProcessor.from_pretrained(pretrained)\n            self._model = AutoModelForVision2Seq.from_pretrained(pretrained, device_map=self.device_map, torch_dtype=dtype)\n        eval_logger.info(f\"Using {type(self._model)} to instantiate the Mantis model.\")\n        \n        self._tokenizer = self._processor.tokenizer\n        \n        self._config = self._model.config\n        self.model.eval()\n        self.model.tie_weights()\n        self.truncation = truncation\n        self.batch_size_per_gpu = int(batch_size)\n        self.use_cache = use_cache\n        self.truncate_context = truncate_context\n        \n        if accelerator.num_processes > 1:\n            assert accelerator.distributed_type in [DistributedType.FSDP, DistributedType.MULTI_GPU, DistributedType.DEEPSPEED], \"Unsupported distributed type provided. Only DDP and FSDP are supported.\"\n            # If you want to use DistributedType.DEEPSPEED, you have to run accelerate config before using the model\n            # Also, you have to select zero stage 0 (equivalent to DDP) in order to make the prepare model works\n            # I tried to set different parameters in the kwargs to let default zero 2 stage works, but it didn't work.\n            if accelerator.distributed_type == DistributedType.DEEPSPEED:\n                kwargs = {\n                    \"train_micro_batch_size_per_gpu\": self.batch_size_per_gpu,\n                    \"train_batch_size\": self.batch_size_per_gpu * accelerator.num_processes,\n                }\n                AcceleratorState().deepspeed_plugin.deepspeed_config_process(must_match=True, **kwargs)\n                eval_logger.info(\"Detected that you are using DistributedType.DEEPSPEED. Make sure you run `accelerate config` and set zero stage to 0\")\n\n            if accelerator.distributed_type == DistributedType.FSDP or accelerator.distributed_type == DistributedType.DEEPSPEED:\n                self._model = accelerator.prepare(self.model)\n            else:\n                self._model = accelerator.prepare_model(self.model, evaluation_mode=True)\n            self.accelerator = accelerator\n            if self.accelerator.is_local_main_process:\n                eval_logger.info(f\"Using {accelerator.num_processes} devices with data parallelism\")\n            self._rank = self.accelerator.local_process_index\n            self._world_size = self.accelerator.num_processes\n        elif accelerator.num_processes == 1 and device_map == \"auto\":\n            eval_logger.info(f\"Using {accelerator.num_processes} devices with tensor parallelism\")\n            self._rank = 0\n            self._word_size = 1\n        else:\n            eval_logger.info(f\"Using single device: {self._device}\")\n            self.model.to(self._device)\n            self._rank = 0\n            self._world_size = 1\n\n    @property\n    def config(self):\n        # return the associated transformers.AutoConfig for the given pretrained model.\n        return self._config\n\n    @property\n    def tokenizer(self):\n        return self._tokenizer\n\n    @property\n    def model(self):\n        # returns the model, unwrapping it if using Accelerate\n        if hasattr(self, \"accelerator\"):\n            return self.accelerator.unwrap_model(self._model)\n        else:\n            return self._model\n\n    @property\n    def eot_token_id(self):\n        # we use EOT because end of *text* is more accurate for what we're doing than end of *sentence*\n        return self.tokenizer.eos_token_id\n\n    @property\n    def max_length(self):\n        return self._max_length\n\n    def pad_sequence(self, input_ids, batch_first, padding_value):\n        if self.tokenizer.padding_side == \"left\":\n            input_ids = [torch.flip(_input_ids, [0]) for _input_ids in input_ids]\n        input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=batch_first, padding_value=padding_value)\n        if self.tokenizer.padding_side == \"left\":\n            input_ids = torch.flip(input_ids, [1])\n        return input_ids\n\n    @property\n    def batch_size(self):\n        return self.batch_size_per_gpu\n\n    @property\n    def device(self):\n        return self._device\n\n    @property\n    def rank(self):\n        return self._rank\n\n    @property\n    def world_size(self):\n        return self._world_size\n\n    def tok_encode(self, string: str, left_truncate_len=None, add_special_tokens=None) -> List[int]:\n        \"\"\" \"\"\"\n        add_special_tokens = False if add_special_tokens is None else add_special_tokens\n        encoding = self.tokenizer.encode(string, add_special_tokens=add_special_tokens)\n        # left-truncate the encoded context to be at most `left_truncate_len` tokens long\n        if left_truncate_len:\n            encoding = encoding[-left_truncate_len:]\n        return encoding\n\n    def tok_decode(self, tokens):\n        try:\n            return self.tokenizer.decode(tokens)\n        except:\n            return self.tokenizer.decode([tokens])\n\n    def loglikelihood(self, requests: List[Instance]) -> List[Tuple[float, bool]]:\n        raise NotImplementedError\n\n    def flatten(self, input):\n        new_list = []\n        for i in input:\n            for j in i:\n                new_list.append(j)\n        return new_list\n\n    def generate_until(self, requests: List[Instance]) -> List[str]:\n        res = []\n    \n        def _collate(x):\n            # the negative sign on len(toks) sorts descending - this has a few advantages:\n            # - time estimates will always be over not underestimates, which is more useful for planning\n            # - to know the size of a batch when going through the list, you know the first one is always the batch\n            #   padded context length. this is useful to simplify the batching logic and more importantly to make\n            #   automatic adaptive batches much much easier to implement\n            # - any OOMs will happen right away rather than near the end\n            toks = self.tok_encode(x[0])\n            return -len(toks), x[0]\n\n        # we group requests by their generation_kwargs,\n        # so that we don't try to execute e.g. greedy sampling and temp=0.8 sampling\n        # in the same batch.\n        re_ords = utils.Collator([reg.args for reg in requests], _collate, grouping=True)\n        chunks = re_ords.get_batched(n=self.batch_size, batch_fn=None)\n        num_iters = len(requests) // self.batch_size if len(requests) % self.batch_size == 0 else len(requests) // self.batch_size + 1\n        pbar = tqdm(total=num_iters, disable=(self.rank != 0), desc=\"Model Responding\")\n        for chunk in chunks:\n            contexts, all_gen_kwargs, doc_to_visuals, doc_id, tasks, splits = zip(*chunk)\n            visuals = [doc_to_visual(self.task_dict[task][split][ids]) for ids, task, split, doc_to_visual in zip(doc_id, tasks, splits, doc_to_visuals)]\n        \n            # we assume all gen kwargs in the batch are the same\n            # this is safe to assume because the `grouper` object ensures it.\n            gen_kwargs = all_gen_kwargs[0]\n            \n            until = gen_kwargs.pop(\"until\", None)\n            image_aspect_ratio = gen_kwargs.pop(\"image_aspect_ratio\", None)\n\n            if \"max_new_tokens\" not in gen_kwargs:\n                gen_kwargs[\"max_new_tokens\"] = 1024\n            if \"temperature\" not in gen_kwargs:\n                gen_kwargs[\"temperature\"] = 0\n\n            # prompts_input = contexts[0]\n\n            prompts = []\n            for visual, context in zip(visuals, contexts):\n                if self._is_idefics:\n                    # Follow the idefics implementation: \n                    content = []\n                    if DEFAULT_IMAGE_TOKEN not in context:\n                        for _ in visual:\n                            content.append({\"type\": \"image\"})\n                    content.append({\"type\": \"text\", \"text\": context})\n                    message = [{\"role\": \"user\", \"content\": content}]\n                    prompt = self._processor.apply_chat_template(message, add_generation_prompt=True)\n                    prompts.append(prompt)\n                else:\n                    # We follow the Mantis code base: https://github.com/TIGER-AI-Lab/Mantis/blob/main/mantis/models/mllava/utils.py#L33 to make sure they are consistent\n                    # Users don't need to define chat template as it is done here\n                    if \"llama-3\" in self._model.language_model.name_or_path.lower():\n                        conv = conv_templates['llama_3']\n                        terminators = [\n                            self._processor.tokenizer.eos_token_id,\n                            self._processor.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n                        ]\n                    else:\n                        conv = default_conv\n                        terminators = None\n                        \n                    gen_kwargs[\"eos_token_id\"] = terminators\n                    \n                    conv = conv.copy()\n                    conv.append_message(conv.roles[0], context)\n                    conv.append_message(conv.roles[1], \"\")\n                    prompt = conv.get_prompt()\n                    prompts.append(prompt)\n            inputs = self._processor(images=visuals, text=prompts, return_tensors=\"pt\", truncation=True)\n            if \"image_patches\" in inputs.keys():\n                inputs[\"image_patches\"] = inputs[\"image_patches\"][0] # FIXME: Fuyu model would return a list instead of a pytorch tensor. This weird behavior needs fixing.\n            inputs = {k: v.to(self.device) for k, v in inputs.items()}    \n            \n            output_ids = self.model.generate(**inputs, **gen_kwargs)\n            for output_id, input_id in zip(output_ids, inputs[\"input_ids\"]):\n                generated_id = output_id[len(input_id) :]\n                generated_text = self.tokenizer.decode(generated_id, skip_special_tokens=True)\n\n                res.append(generated_text)\n\n            # self.cache_hook.add_partial(\"generate_until\", (context, gen_kwargs), text_outputs)\n            pbar.update(1)\n            # reorder this group of results back to original unsorted form\n        res = re_ords.get_original(res)\n\n        pbar.close()\n        return res\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/filters/transformation.py", "content": "from lmms_eval.api.filter import Filter\n\n\nclass LowercaseFilter(Filter):\n    def __init__(self) -> None:\n        pass\n\n    def apply(self, resps, docs):\n        def filter_set(inst):\n            return [resp.lower() for resp in inst]\n\n        return [filter_set(resp) for resp in resps]\n\n\nclass UppercaseFilter(Filter):\n    def __init__(self) -> None:\n        pass\n\n    def apply(self, resps, docs):\n        def filter_set(inst):\n            return [resp.upper() for resp in inst]\n\n        return [filter_set(resp) for resp in resps]\n\n\nclass MapFilter(Filter):\n    def __init__(self, mapping_dict: dict = {}, default_value=None) -> None:\n        \"\"\"\n        Initializes the MapFilter with a given mapping dictionary and default value.\n\n        Args:\n        - mapping_dict (dict): A dictionary containing the key-value mappings.\n                               Default is an empty dictionary.\n        - default_value (Any): The value to be returned when a key is not found in the mapping_dict.\n                               Default is None.\n\n        Example:\n        mapper = MapFilter({'A': 1, 'B': 2}, default_value=0)\n        \"\"\"\n        assert isinstance(mapping_dict, dict), \"Provided mapping_dict is not a dictionary\"\n        self.mapping_dict = mapping_dict\n        self.default_value = default_value\n\n    def apply(self, resps, docs):\n        def filter_set(inst):\n            return [self.mapping_dict.get(resp, self.default_value) for resp in inst]\n\n        return [filter_set(resp) for resp in resps]\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/models/mplug_owl_video/processing_mplug_owl.py", "content": "import re\nimport torch\nimport torch.utils.checkpoint\n\nfrom transformers.processing_utils import ProcessorMixin\nfrom transformers.tokenization_utils_base import BatchEncoding\nfrom transformers.models.clip.image_processing_clip import CLIPImageProcessor\nfrom .tokenization_mplug_owl import MplugOwlTokenizer\n\nfrom decord import VideoReader\nimport numpy as np\nfrom PIL import Image\nfrom lmms_eval.models.model_utils.load_video import read_video_pyav\n\n\ndef get_index(num_frames, num_segments):\n    seg_size = float(num_frames - 1) / num_segments\n    start = int(seg_size / 2)\n    offsets = np.array([start + int(np.round(seg_size * idx)) for idx in range(num_segments)])\n    return offsets\n\n\ndef load_video(path, num_frames=4):\n    \"\"\"vr = VideoReader(path, height=224, width=224)\n    total_frames = len(vr)\n    frame_indices = get_index(total_frames, num_frames)\n    images_group = list()\n    for frame_index in frame_indices:\n        img = Image.fromarray(vr[frame_index].asnumpy()).convert(\"RGB\")\n        images_group.append(img)\n    return images_group\"\"\"\n    # Change a bit here from the original code\n    # I use pyav instead of decord because it is much more safer\n    # The operations here are the same, we load video and return a list of PIL Image\n    # Load video frames\n    video_frames = read_video_pyav(path, num_frm=num_frames)\n    target_h, target_w = 224, 224\n    # If image shape is not as target, resize it\n    if video_frames.shape[-3] != target_h or video_frames.shape[-2] != target_w:\n        video_frames = torch.from_numpy(video_frames).permute(0, 3, 1, 2).float()\n        video_frames = torch.nn.functional.interpolate(video_frames, size=(target_h, target_w))\n        video_frames = video_frames.permute(0, 2, 3, 1).to(torch.uint8).numpy()\n    video_frames = [Image.fromarray(frame) for frame in video_frames]\n    if len(video_frames) > num_frames:\n        video_frames = video_frames[:num_frames]\n    return video_frames\n\n\nclass MplugOwlProcessor(ProcessorMixin):\n    attributes = []\n    tokenizer_class = \"MplugOwlTokenizer\"\n\n    def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n        super().__init__(**kwargs)\n        self.tokens_to_generate = 0\n        self.image_processor = image_processor\n        self.tokenizer = tokenizer\n        self.add_BOS = True\n\n    def __call__(self, text=None, images=None, videos=None, num_frames=4, return_tensors=None, **kwargs):\n        if text is None and images is None:\n            raise ValueError(\"You have to specify either text or images. Both cannot be none.\")\n\n        if text is not None:\n            encoding = tokenize_prompts(\n                prompts=text,\n                tokens_to_generate=self.tokens_to_generate,\n                add_BOS=self.add_BOS,\n                tokenizer=self.tokenizer,\n                ignore_dist=True,\n                **kwargs,\n            )\n            # encoding = self.tokenizer(text, return_tensors=return_tensors, **kwargs)\n\n        if images is not None:\n            image_features = self.image_processor(images, return_tensors=return_tensors, **kwargs)\n\n        if videos is not None:\n            video_features = []\n            for video in videos:\n                video_frames = load_video(video, num_frames)\n                video_feature = self.image_processor(video_frames, return_tensors=return_tensors, **kwargs)[\"pixel_values\"]\n                video_features.append(video_feature)\n            video_features = torch.stack(video_features, dim=0)\n            video_features = video_features.permute(0, 2, 1, 3, 4)\n\n        if text is not None and images is not None:\n            encoding[\"pixel_values\"] = image_features.pixel_values\n            return encoding\n        if text is not None and videos is not None:\n            encoding[\"video_pixel_values\"] = video_features\n            return encoding\n        elif text is not None:\n            return encoding\n        elif images is not None:\n            return BatchEncoding(data=dict(**image_features), tensor_type=return_tensors)\n        else:\n            return BatchEncoding(data=dict(video_pixel_values=video_pixel_values), tensor_type=return_tensors)\n\n    def batch_decode(self, skip_special_tokens=True, *args, **kwargs):\n        \"\"\"\n        This method forwards all its arguments to CLIPTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n        refer to the docstring of this method for more information.\n        \"\"\"\n        return self.tokenizer.batch_decode(*args, skip_special_tokens=skip_special_tokens, **kwargs)\n\n    def decode(self, skip_special_tokens=True, *args, **kwargs):\n        \"\"\"\n        This method forwards all its arguments to CLIPTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n        the docstring of this method for more information.\n        \"\"\"\n        return self.tokenizer.decode(*args, skip_special_tokens=skip_special_tokens, **kwargs)\n\n\nclass MplugOwlImageProcessor(CLIPImageProcessor):\n    pass\n\n\ndef detokenize_generations(tokens_gpu_tensor, lengths_gpu_tensor, return_segments, tokenizer):\n    \"\"\"Detokenize the generated tokens.\"\"\"\n\n    prompts_plus_generations = []\n    if return_segments:\n        prompts_plus_generations_segments = []\n\n    tokens = tokens_gpu_tensor.cpu().numpy().tolist()\n    lengths = lengths_gpu_tensor.cpu().numpy().tolist()\n    for sequence_tokens, length in zip(tokens, lengths):\n        sequence_tokens = sequence_tokens[:length]\n        prompts_plus_generations.append(tokenizer.detokenize(sequence_tokens))\n        if return_segments:\n            from tokenizers.decoders import Metaspace\n\n            if hasattr(tokenizer, \"tokenizer\"):\n                if isinstance(tokenizer.tokenizer.decoder, Metaspace):\n                    words = tokenizer.tokenizer.decode(sequence_tokens)\n                else:\n                    words = []\n                    for token in sequence_tokens:\n                        word = tokenizer.tokenizer.decoder[token]\n                        word = bytearray([tokenizer.tokenizer.byte_decoder[c] for c in word]).decode(\"utf-8\", errors=\"replace\")\n                        words.append(word)\n                prompts_plus_generations_segments.append(words)\n            else:\n                words = tokenizer.detokenize(sequence_tokens)\n                # else:\n                #     words = []\n                #     for token in sequence_tokens:\n                #         word = tokenizer.tokenizer.decoder[token]\n                #         word = bytearray(\n                #             [tokenizer.tokenizer.byte_decoder[c] for c in word]).decode(\n                #                 'utf-8', errors='replace')\n                #         words.append(word)\n                prompts_plus_generations_segments.append(words)\n\n    if return_segments:\n        return tokens, prompts_plus_generations, prompts_plus_generations_segments\n\n    return tokens, prompts_plus_generations\n\n\ndef tokenize_prompts(prompts=None, tokens_to_generate=None, add_BOS=None, rank=0, tokenizer=None, ignore_dist=False, **kwargs):\n    \"\"\"Tokenize prompts and make them avaiable on all ranks.\"\"\"\n\n    # On all ranks set to None so we can pass them to functions\n    prompts_tokens_cuda_long_tensor = None\n    prompts_length_cuda_long_tensor = None\n\n    # On the specified rank, build the above.\n    attention_mask = None\n    if ignore_dist or torch.distributed.get_rank() == rank:\n        assert prompts is not None\n        assert tokens_to_generate is not None\n        # Tensor of tokens padded and their unpadded length.\n        prompts_tokens_cuda_long_tensor, prompts_length_cuda_long_tensor, attention_mask = _tokenize_prompts_and_batch(prompts, tokens_to_generate, add_BOS, tokenizer, **kwargs)\n        # We need the sizes of these tensors for the boradcast\n        [\n            prompts_tokens_cuda_long_tensor.size(0),  # Batch size\n            prompts_tokens_cuda_long_tensor.size(1),\n        ]  # Sequence lenght\n\n    return {\n        \"input_ids\": prompts_tokens_cuda_long_tensor,\n        \"attention_mask\": attention_mask,\n        # \"prompt_length\": prompts_length_cuda_long_tensor,\n    }\n\n\ndef _tokenize_prompts_and_batch(prompts, tokens_to_generate, add_BOS, tokenizer, **kwargs):\n    \"\"\"Given a set of prompts and number of tokens to generate:\n    - tokenize prompts\n    - set the sequence length to be the max of length of prompts\n      plus the number of tokens we would like to generate\n    - pad all the sequences to this length so we can convert them\n      into a 2D tensor.\n    \"\"\"\n\n    # Tokenize all the prompts.\n    # if add_BOS:\n    #     prompts_tokens = [[tokenizer.bos] + tokenizer.tokenize(prompt)\n    #                       for prompt in prompts]\n    # else:\n    #     prompts_tokens = [tokenizer.tokenize(prompt) for prompt in prompts]\n\n    prompts_tokens = [_tokenize_prompt(prompt, tokenizer, add_BOS, **kwargs) for prompt in prompts]\n\n    # Now we have a list of list of tokens which each list has a different\n    # size. We want to extend this list to:\n    #   - incorporate the tokens that need to be generated\n    #   - make all the sequences equal length.\n    # Get the prompts length.\n    prompts_length = [len(prompt_tokens) for prompt_tokens in prompts_tokens]\n    # Get the max prompts length.\n    max_prompt_len = max(prompts_length)\n    # Number of tokens in the each sample of the batch.\n    samples_length = max_prompt_len + tokens_to_generate\n    # Now update the list of list to be of the same size: samples_length.\n    for prompt_tokens, prompt_length in zip(prompts_tokens, prompts_length):\n        padding_size = samples_length - prompt_length\n        prompt_tokens.extend([tokenizer.eos_token_id] * padding_size)\n\n    # Now we are in a structured format, we can convert to tensors.\n    prompts_tokens_tensor = torch.LongTensor(prompts_tokens)\n    prompts_length_tensor = torch.LongTensor(prompts_length)\n    attention_mask = torch.zeros(prompts_tokens_tensor.shape[:2])\n    for i, l in enumerate(prompts_length_tensor):\n        attention_mask[i, :l] = 1\n    return prompts_tokens_tensor, prompts_length_tensor, attention_mask\n\n\ndef _tokenize_prompt(prompt, tokenizer, add_BOS=False, media_info={\"<image>\": 65, \"<|video|>\": 65}, **kwargs):\n    media_tokens = {k: -int(i + 1) for i, k in enumerate(media_info.keys())}\n    media_lengths = media_info.copy()\n\n    if add_BOS:\n        prompt_chunk = [tokenizer.bos_token_id]\n    else:\n        prompt_chunk = []\n\n    # Pure Text\n    if all([media_token not in prompt for media_token in media_tokens.keys()]):\n        enc_chunk = prompt_chunk + tokenizer(prompt, add_special_tokens=False, **kwargs)[\"input_ids\"]\n\n    # Multi-Modal Text\n    else:\n        enc_chunk = prompt_chunk\n        pattern = \"|\".join(map(re.escape, list(media_tokens.keys())))\n        chunk_strs = re.split(f\"({pattern})\", prompt)\n        chunk_strs = [x for x in chunk_strs if len(x) > 0]\n        for idx, chunk_str in enumerate(chunk_strs):\n            if chunk_str in media_tokens:\n                enc_chunk += [media_tokens[chunk_str]] * media_lengths[chunk_str]\n            else:\n                tmp_chunk = tokenizer(chunk_str, add_special_tokens=False)[\"input_ids\"]\n                # if idx < len(chunk_strs) - 1: # Last chunk should not have eos\n                #     tmp_chunk += [tokenizer.eod_id]\n                enc_chunk += tmp_chunk\n    return enc_chunk\n\n\nif __name__ == \"__main__\":\n    pass\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/evaluator.py", "content": "import os\nimport time\nimport random\nimport itertools\nimport json\nimport collections\nimport sys\nimport inspect\nfrom tqdm import tqdm\n\nimport torch\n\nimport numpy as np\nfrom datasets import Image, Sequence\n\nimport lmms_eval.api\nimport lmms_eval.tasks\nimport lmms_eval.models\nimport lmms_eval.api.metrics\nimport lmms_eval.api.registry\n\nfrom lmms_eval.utils import (\n    positional_deprecated,\n    run_task_tests,\n    make_table,\n    create_iterator,\n    get_git_commit_hash,\n    simple_parse_args_string,\n)\n\nfrom loguru import logger as eval_logger\n\n\n@positional_deprecated\ndef simple_evaluate(\n    model,\n    model_args=None,\n    tasks=[],\n    num_fewshot=None,\n    batch_size=None,\n    device=None,\n    limit=None,\n    bootstrap_iters: int = 100000,\n    check_integrity: bool = False,\n    show_task_to_terminal: bool = False,\n    log_samples: bool = True,\n    gen_kwargs: str = None,\n    cli_args=None,  # Bo: put args into more functions (cost 48 Bytes per call)\n    predict_only: bool = False,\n    return_id_experts: bool = False,\n    layers_expert_selection: list = []\n\n):\n    \"\"\"Instantiate and evaluate a model on a list of tasks.\n\n    :param model: Union[str, LMM]\n        Name of model or LMM object, see lmms_eval.models.get_model\n    :param model_args: Optional[str]\n        String arguments for each model class, see LMM.create_from_arg_string.\n        Ignored if `model` argument is a LMM object.\n    :param tasks: list[Union[str, Task]]\n        List of task names or Task objects. Task objects will be taken to have name task.EVAL_HARNESS_NAME if defined and type(task).__name__ otherwise.\n    :param num_fewshot: int\n        Number of examples in few-shot context\n    :param batch_size: int or str, optional\n        Batch size for model\n    :param device: str, optional\n        PyTorch device (e.g. \"cpu\" or \"cuda:0\") for running models\n    :param limit: int or float, optional\n        Limit the number of examples per task (only use this for testing), If <1, limit is a percentage of the total number of examples.\n    :param bootstrap_iters:\n        Number of iterations for bootstrap statistics\n    :param check_integrity: bool\n        Whether to run the relevant part of the test suite for the tasks\n    :param show_task_to_terminal: bool\n        If True, write out an example document and model input for checking task integrity\n    :param log_samples: bool\n        If True, write out all model outputs and documents for per-sample measurement and post-hoc analysis\n    :param gen_kwargs: str\n        String arguments for model generation\n        Ignored for all tasks with loglikelihood output_type\n    :return\n        Dictionary of results\n    \"\"\"\n    random.seed(0)\n    np.random.seed(1234)\n    torch.manual_seed(1234)  # TODO: this may affect training runs that are run with evaluation mid-run.\n    print(tasks)\n    assert tasks != [], \"No tasks specified, or no tasks found. Please verify the task names.\"\n\n    if gen_kwargs:\n        gen_kwargs = simple_parse_args_string(gen_kwargs)\n        eval_logger.warning(f\"generation_kwargs specified through cli, these settings will be used over set parameters in yaml tasks.\")\n        if gen_kwargs == \"\":\n            gen_kwargs = None\n\n    if model_args is None:\n        model_args = \"\"\n    lm = lmms_eval.api.registry.get_model(model).create_from_arg_string(\n        model_args,\n        {\n            \"batch_size\": batch_size,\n            \"device\": device,\n        },\n    )\n\n    task_dict = lmms_eval.tasks.get_task_dict(tasks, model_name=model)\n    for task_name in task_dict.keys():\n        task_obj = task_dict[task_name]\n        if type(task_obj) == tuple:\n            group, task_obj = task_obj\n            if task_obj is None:\n                continue\n        lm.task_dict[task_name] = task_obj.dataset\n\n        config = task_obj._config\n        if config[\"output_type\"] == \"generate_until\" and gen_kwargs:\n            config[\"generation_kwargs\"].update(gen_kwargs)\n\n        if predict_only:\n            log_samples = True\n            eval_logger.info(f\"Processing {task_name} in output-only mode. Metrics will not be calculated!\")\n            # we have to change the class properties post-hoc. This is pretty hacky.\n            task_obj.override_metric(metric_name=\"bypass\")\n\n        if num_fewshot is not None:\n            if config[\"num_fewshot\"] == 0:\n                eval_logger.info(f\"num_fewshot has been set to 0 for {task_name} in its config. Manual configuration will be ignored.\")\n            else:\n                default_num_fewshot = config[\"num_fewshot\"]\n                eval_logger.warning(f\"Overwriting default num_fewshot of {task_name} from {default_num_fewshot} to {num_fewshot}\")\n\n                task_obj._config[\"num_fewshot\"] = num_fewshot\n\n    if check_integrity:\n        run_task_tests(task_list=tasks)\n\n    results = evaluate(\n        lm=lm,\n        task_dict=task_dict,\n        limit=limit,\n        bootstrap_iters=bootstrap_iters,\n        show_task_to_terminal=show_task_to_terminal,\n        log_samples=log_samples,\n        cli_args=cli_args,\n        return_id_experts = return_id_experts,\n        layers_expert_selection = layers_expert_selection\n    )\n\n    if lm.rank == 0:\n        # add info about the model and few shot config\n        results[\"model_configs\"] = {\n            \"model\": model if isinstance(model, str) else model.model.config._name_or_path,\n            \"model_args\": model_args,\n            \"batch_size\": batch_size,\n            \"device\": device,\n            \"limit\": limit,\n            \"bootstrap_iters\": bootstrap_iters,\n            \"gen_kwargs\": gen_kwargs,\n        }\n        results[\"git_hash\"] = get_git_commit_hash()\n        return results\n    else:\n        return None\n\n\ndecontaminate_suffix = \"_decontaminate\"\n\n\n@positional_deprecated\ndef evaluate(\n    lm,\n    task_dict,\n    limit=None,\n    bootstrap_iters: int = 100000,\n    show_task_to_terminal: bool = False,\n    log_samples: bool = True,\n    cli_args=None,\n    return_id_experts = False,\n    layers_expert_selection: list = []\n\n):\n    \"\"\"Instantiate and evaluate a model on a list of tasks.\n\n    :param lm: obj\n        Language Model\n    :param task_dict: dict[str, Task]\n        Dictionary of tasks. Tasks will be taken to have name type(task).config.task .\n    :param limit: int, optional\n        Limit the number of examples per task (only use this for testing)\n    :param bootstrap_iters:\n        Number of iterations for bootstrap statistics\n    :param show_task_to_terminal: bool\n        If True, write out an example document and model input for checking task integrity\n    :param log_samples: bool\n        If True, write out all model outputs and documents for per-sample measurement and post-hoc analysis\n    :return\n        Dictionary of results\n    \"\"\"\n\n    # stores the final result for each task, for each metric/filter pair.\n    results = collections.defaultdict(dict)\n    # Tracks each task's version.\n    versions = collections.defaultdict(dict)\n    # Tracks the YAML configs of all chosen tasks.\n    configs = collections.defaultdict(dict)\n    # logs info about each document evaluated.\n    samples = collections.defaultdict(list)\n    # tracks all Instances/requests a model must generate output on.\n    requests = collections.defaultdict(list)\n    # Aggregated task scores presented with groups\n    results_agg = collections.defaultdict(dict)\n    # Aggregated groups scores only\n    groups_agg = collections.defaultdict(dict)\n    # stores the amount to pad out reqs per req. type so that\n    # number of fwd passes per distributed rank is equal\n    padding_requests = collections.defaultdict(int)\n    # store the hierarchy to do proper ordering\n    task_hierarchy = collections.defaultdict(list)\n    # store the ordering of tasks and groups\n    task_order = collections.defaultdict(int)\n    task_group_alias = collections.defaultdict(dict)\n    # store num-fewshot value per task\n    num_fewshot = collections.defaultdict(int)\n    # get lists of each type of request\n    for task_name, task in task_dict.items():\n        if type(task) == tuple:\n            group_name, task = task\n            task_hierarchy[group_name].append(task_name)\n            versions[group_name] = \"N/A\"\n\n        else:\n            group_name = None\n            task_hierarchy[task_name] = []\n\n        if task is None:\n            continue\n\n        versions[task_name] = task.VERSION\n        configs[task_name] = dict(task.dump_config())\n\n        if \"num_fewshot\" in configs[task_name]:\n            n_shot = configs[task_name][\"num_fewshot\"]\n        else:\n            n_shot = 0\n        num_fewshot[task_name] = n_shot\n\n        if \"task_alias\" in configs[task_name]:\n            task_group_alias[task_name] = configs[task_name][\"task_alias\"]\n\n        if (\"group_alias\" in configs[task_name]) and (group_name not in task_group_alias) and (group_name is not None):\n            task_group_alias[group_name] = configs[task_name][\"group_alias\"]\n\n        if limit is not None:\n            if task.has_test_docs():\n                task_docs = task.test_docs()\n            elif task.has_validation_docs():\n                task_docs = task.validation_docs()\n            else:\n                raise RuntimeError(\"Task has neither test_docs nor validation_docs\")\n            limit = int(len(task_docs) * limit) if limit < 1.0 else int(limit)\n\n        task.build_all_requests(limit=limit, rank=lm.rank, world_size=lm.world_size)\n\n        eval_logger.debug(f\"Task: {task_name}; number of requests on rank {lm.rank}: {len(task.instances)}\")\n\n        if show_task_to_terminal:\n            for inst in task.instances:\n                # print the prompt for the first few documents\n                if inst.doc_id < 1:\n                    eval_logger.info(\n                        f\"Task: {task_name}; document {inst.doc_id}; context prompt (starting on next line):\\\n\\n{inst.args[0]}\\n(end of prompt on previous line)\\ntarget string or answer choice index (starting on next line):\\n{task.doc_to_target(inst.doc)}\\n(end of target on previous line)\"\n                    )\n                    eval_logger.info(f\"Request: {str(inst)}\")\n\n        # aggregate Instances by LMM method requested to get output.\n        for instance in task.instances:\n            reqtype = instance.request_type\n            requests[reqtype].append(instance)\n\n        if lm.world_size > 1:\n            instances_rnk = torch.tensor(len(task._instances), device=lm.device)\n            gathered_item = lm.accelerator.gather(instances_rnk).cpu().detach().numpy().tolist()\n\n            # compute number of pseudobatches to pad with (FSDP/DDP require even batches among ranks)\n            numpad = max(gathered_item) - gathered_item[lm.rank]\n            padding_requests[task.OUTPUT_TYPE] += numpad\n\n    ### Run LMM on inputs, get all outputs ###\n    # execute each type of request\n    \n    for reqtype, reqs in requests.items():\n        eval_logger.info(\"Running {} requests\".format(reqtype))\n        # create `K` copies of each request `req` based off `K = req.repeats`\n        cloned_reqs = []\n        \n        for req in reqs:\n            cloned_reqs.extend([req] * req.repeats)\n\n        if (lm.world_size > 1) and (padding_requests[reqtype] > 0):\n            for _ in range(padding_requests[reqtype]):\n                cloned_reqs.extend([req] * req.repeats)\n        # run requests through model\n        resps, vision_id_experts, mlp_id_experts = getattr(lm, reqtype)(cloned_reqs, return_id_experts = return_id_experts, layers_expert_selection =  layers_expert_selection)  # Choiszt run generate until\n        if return_id_experts:\n            # put responses from model into a list of length K for each request.\n            for x, vision_id_expert, mlp_id_expert, req in zip(resps, vision_id_experts,mlp_id_experts, cloned_reqs):\n                req.resps.append(x)\n                req.vision_id_experts.append(vision_id_expert)\n                req.mlp_id_experts.append(mlp_id_expert)\n        else:\n            # put responses from model into a list of length K for each request.\n            for x, req in zip(resps, cloned_reqs):\n                req.resps.append(x)\n              \n        \n        if lm.world_size > 1:\n            lm.accelerator.wait_for_everyone()\n    \n    ### Postprocess outputs ###\n    # TODO: del model here, maybe (idea: allow user to specify device of e.g. reward model separately)\n    for task_name, task in task_dict.items():\n        if type(task) == tuple:\n            group, task = task\n            if task is None:\n                continue\n        task.apply_filters()\n\n    ### Collect values of metrics on all datapoints ###\n    vals = collections.defaultdict(list)\n\n    # unpack results and sort back in order and return control to Task\n    for task_name, task in task_dict.items():\n        if type(task) == tuple:\n            group, task = task\n            if task is None:\n                continue\n        # TODO: make it possible to use a different metric per filter\n        # iterate over different filters used\n        for key in task.instances[0].filtered_resps.keys():\n            # hack: remove image columns to speed avoid loading images and speed up postprocessing\n            # reason: doc_iterator will actually load image if it's in the doc.\n            docs = task.test_docs() if task.has_test_docs() else task.validation_docs()\n            if \"d170\" not in task_name and \"dc100\" not in task_name and \"dc200\" not in task_name and \"llava_wilder\" not in task_name and \"livebench\" not in task_name and \"wildvision\" not in task_name:\n                remove_cols = []\n                features = docs.features\n                # If it is an Image instance or a Sequence of Image instance. Remove it\n                for feature in features:\n                    if isinstance(features[feature], Image):\n                        remove_cols.append(feature)\n                    elif isinstance(features[feature], Sequence) and isinstance(features[feature].feature, Image):\n                        remove_cols.append(feature)\n                if remove_cols:\n                    docs = docs.remove_columns(remove_cols)\n\n            ####################### Processing with Full Docs Mode #######################\n            if task_name in [\"videochatgpt_consistency\"]:\n                full_docs = True\n            else:\n                full_docs = False\n\n            doc_iterator = itertools.islice(enumerate(docs), lm.rank, limit, lm.world_size)\n            # Instead of converting the iterator to a list, use `itertools.tee` to create a parallel iterator for counting\n            # doc_iterator, doc_iterator_for_counting = itertools.tee(doc_iterator)\n            # Don't use above one, this would crash if doc_iterator_for_counting contains too many objects and very slow\n            doc_iterator_for_counting = itertools.islice(range(len(task.test_docs())), lm.rank, limit, lm.world_size) if task.has_test_docs() else itertools.islice(range(len(task.validation_docs())), lm.rank, limit, lm.world_size)\n            total_docs = sum(1 for _ in doc_iterator_for_counting)\n            pbar = tqdm(total=total_docs, desc=f\"Postprocessing\", disable=(lm.rank != 0))\n            for doc_id, doc in doc_iterator:\n                # subset instances to only this document id ; sort by idx\n                requests = list(filter(lambda x: x.doc_id == doc_id, task.instances))\n                requests.sort(key=lambda x: x.idx)\n                if full_docs:\n                    metrics = task.process_results(doc, [req.filtered_resps[key] for req in requests], full_docs=docs)\n                else:\n                    metrics = task.process_results(doc, [req.filtered_resps[key] for req in requests])\n                if log_samples:\n                    target = task.doc_to_target(doc)\n                    example = {\n                        \"doc_id\": doc_id,\n                        \"target\": target,\n                        \"doc\": doc,\n                        \"arguments\": [tuple(a for a in req.args if isinstance(a, (int, str))) for req in requests],  # do not include image\n                        \"resps\": [req.resps for req in requests],\n                        \"filtered_resps\": [req.filtered_resps[key] for req in requests],\n                        \"mlp_id_experts\": [req.mlp_id_experts for req in requests],\n                        \"vision_id_experts\": [req.vision_id_experts for req in requests],\n                        \"domain\": [req.domain for req in requests],\n                        \n                    }\n                    example.update(metrics)\n                    samples[task_name].append(example)\n                for metric, value in metrics.items():\n                    vals[(task_name, key, metric)].append(value)\n                pbar.update(1)\n\n            pbar.close()\n\n    if lm.world_size > 1:\n        # if multigpu, then gather data across all ranks\n        # first gather logged samples across all ranks\n        for task_name, task_samples in list(samples.items()):\n            full_samples = [None] * lm.world_size\n            torch.distributed.all_gather_object(full_samples, task_samples)\n            samples[task_name] = list(itertools.chain.from_iterable(full_samples))\n        # then collect metrics across all ranks\n        vals_torch = collections.defaultdict(list)\n        for (task_name, key, metric), items in vals.items():\n            numitem = 0\n            if type(items[0]) == tuple:\n                numitem = len(items[0])\n\n            if isinstance(items[0], (str, list, dict)):\n                # handle the string case\n                gathered_items = [None] * lm.accelerator.num_processes\n                torch.distributed.all_gather_object(gathered_items, items)\n\n                gathered_item = list(itertools.chain.from_iterable(gathered_items))\n            else:\n                # distributed gather requires all ranks to have same dimensions\n                # so we pad out with float32 min value\n                pad_value = torch.finfo(torch.float32).min\n                metrics_tensor = torch.tensor(items, device=lm.device)\n\n                original_dtype = metrics_tensor.dtype  # store original dtype\n                torch_device_tensor = lm.accelerator.pad_across_processes(metrics_tensor.to(torch.float32), pad_index=pad_value)\n                gathered_item = lm.accelerator.gather(torch_device_tensor)\n\n                if numitem > 0:\n                    gathered_filtered = gathered_item[gathered_item[:, 0] != pad_value]\n                else:\n                    gathered_filtered = gathered_item[gathered_item != pad_value]\n\n                gathered_item = gathered_filtered.to(original_dtype).cpu().detach().numpy().tolist()\n                # reconvert if we were passed a tuple of values\n                if numitem > 0:\n                    gathered_item = [tuple(g) for g in gathered_item]\n\n            if lm.rank == 0:\n                vals_torch[(task_name, key, metric)] = gathered_item\n\n        vals = vals_torch\n        # Ensure all ranks wait for rank 0 to finish aggregation\n        torch.distributed.barrier()\n\n    # Synchronize processes with a temp file in case the evluation metric requires gpus\n    # TODO: fix barriers' taking up gpu computation\n    os.makedirs(cli_args.output_path, exist_ok=True)\n    if os.path.exists(f\"{cli_args.output_path}/rank{int(os.environ.get('RANK', 0))}_metric_eval_done.txt\"):\n        os.remove(f\"{cli_args.output_path}/rank{int(os.environ.get('RANK', 0))}_metric_eval_done.txt\")\n\n    if lm.rank == 0:\n        ### Get task ordering for correct sample-wide aggregation\n        group_to_task = {}\n        for group in task_hierarchy.keys():\n            if group not in task_order:\n                task_order[group] = 0\n\n            if len(task_hierarchy[group]) > 0:\n                group_to_task[group] = task_hierarchy[group].copy()\n\n            for task in task_hierarchy[group]:\n                if task in task_order:\n                    task_order[task] += 1\n                else:\n                    task_order[task] = 1 + task_order[group]\n\n                if task in task_hierarchy:\n                    group_to_task[group].remove(task)\n                    group_to_task[group].extend(task_hierarchy[task])\n\n        task_to_group = {}\n        for group in group_to_task:\n            for task in group_to_task[group]:\n                if task in task_to_group:\n                    task_to_group[task].append(group)\n                else:\n                    task_to_group[task] = [group]\n\n        ### Aggregate results over all datapoints ###\n        # aggregate results ; run bootstrap CIs\n        for (task_name, key, metric), items in vals.items():\n            task = task_dict[task_name]\n            metric_key = metric + \",\" + key\n\n            if type(task) == tuple:\n                group_name, task = task\n            else:\n                group_name = None\n\n            if metric not in task.aggregation():\n                continue\n\n            agg_fn = task.aggregation()[metric]\n\n            # Bo: for models that need to know the args to save to correct path\n            if inspect.getfullargspec(agg_fn).args == [\"results\", \"args\"]:\n                results[task_name][metric_key] = agg_fn(items, cli_args)\n            else:\n                # Bo: for models only need agg items\n                results[task_name][metric_key] = agg_fn(items)\n\n            results[task_name][\"samples\"] = len(items)\n\n            # hotfix: bleu, chrf, ter seem to be really expensive to bootstrap\n            # so we run them less iterations. still looking for a cleaner way to do this\n            if bootstrap_iters > 0:\n                stderr = lmms_eval.api.metrics.stderr_for_metric(\n                    metric=task.aggregation()[metric],\n                    bootstrap_iters=min(bootstrap_iters, 100) if metric in [\"bleu\", \"chrf\", \"ter\"] else bootstrap_iters,\n                )\n\n                if stderr is not None and len(items) > 1:\n                    results[task_name][metric + \"_stderr\" + \",\" + key] = stderr(items)\n                else:\n                    results[task_name][metric + \"_stderr\" + \",\" + key] = \"N/A\"\n\n        if bool(results):\n            for group, task_list in reversed(task_hierarchy.items()):\n                if task_list == []:\n                    total_size = results[group][\"samples\"]\n                else:\n                    total_size = 0\n\n                    for task in task_list:\n                        metrics = results[task]\n\n                        current_size = metrics.pop(\"samples\")\n                        # TODO: There should be a way for users\n                        #       to toggle between weighted and\n                        #       unweighted averaging\n                        # For unweighted averaging, use:\n                        #     current_size = 1\n\n                        all_stderr = []\n                        for metric in [key for key in metrics.keys() if \"_stderr\" not in key]:\n                            stderr = \"_stderr,\".join(metric.split(\",\"))\n                            stderr_score = results[task][stderr]\n                            var_score = stderr_score**2 if stderr_score != \"N/A\" else 0\n                            metric_score = results[task][metric]\n\n                            all_stderr.append(stderr)\n\n                            if metric_score is None:\n                                results[group][metric] = None\n                                results[group][stderr] = 0\n                                continue\n\n                            if metric in results[group]:\n                                if isinstance(results[group][metric], str) == False:\n                                    results[group][metric] = (results[group][metric] * total_size + metric_score * current_size) / (total_size + current_size)\n                                    # $$s_z^2 = \\frac{(n-1) s_x^2 + (m-1) s_y^2}{n+m-1} + \\frac{nm(\\bar x - \\bar y)^2}{(n+m)(n+m-1)}.$$\n                                    results[group][stderr] = ((total_size - 1) * results[group][stderr] + (current_size - 1) * var_score) / (total_size + current_size - 1) + total_size * current_size / (\n                                        (total_size + current_size) * (total_size + current_size - 1)\n                                    ) * (results[group][metric] - metric_score) ** 2\n                                else:\n                                    # accuracy = re.search(r'acc: ([\\d.]+)%', results[group][metric]).group(1)\n                                    # score = re.search(r'score: ([\\d.]+)', results[group][metric]).group(1)\n                                    # group_accuracy = float(accuracy)\n                                    # group_score = float(score)\n                                    # group_accuracy = (group_accuracy * total_size + metric_score * current_size) / total_size\n                                    # group_score = (group_score * total_size + metric_score * current_size) / total_size\n                                    # results[group][metric] = \"Acc: \" + str(group_accuracy) + \" Score: \" + str(group_score)\n                                    results[group][metric] = \"group_results\"\n                                    results[group][stderr] = 0\n                            else:\n                                results[group][metric] = metric_score\n                                results[group][stderr] = var_score\n\n                        total_size += current_size\n\n                    for stderr in all_stderr:\n                        results[group][stderr] = np.sqrt(results[group][stderr])\n\n                results[group][\"samples\"] = total_size\n\n        def print_tasks(task_hierarchy, task_order, task_version, task_group_alias):\n            results_agg = collections.defaultdict(dict)\n            groups_agg = collections.defaultdict(dict)\n            for group_name, task_list in task_hierarchy.items():\n                order = task_order[group_name]\n                results_agg[group_name] = results[group_name].copy()\n                results_agg[group_name][\"tab\"] = order\n\n                if (order < max(task_order.values())) and (len(task_list) > 0):\n                    groups_agg[group_name] = results[group_name].copy()\n                    groups_agg[group_name][\"tab\"] = order\n\n                if task_list != []:\n                    for task in sorted(task_list):\n                        if task in task_hierarchy:\n                            _task_hierarchy = {task: task_hierarchy[task]}\n                        else:\n                            _task_hierarchy = {task: []}\n\n                        _results_agg, _groups_agg, task_version = print_tasks(_task_hierarchy, task_order, task_version, task_group_alias)\n\n                        results_agg = {**results_agg, **_results_agg}\n                        groups_agg = {**groups_agg, **_groups_agg}\n\n            return results_agg, groups_agg, task_version\n\n        results_agg, groups_agg, versions = print_tasks(task_hierarchy, task_order, versions, task_group_alias)\n\n        for task in results_agg:\n            task_results = results_agg[task]\n\n            if \"samples\" in task_results:\n                task_results.pop(\"samples\")\n\n            tab_string = \"\"\n            if \"tab\" in task_results:\n                tab = task_results.pop(\"tab\")\n                tab_string = \" \" * tab + \"- \" if tab > 0 else \"\"\n\n            if task in task_group_alias:\n                task_alias = task_group_alias[task]\n                results_agg[task][\"alias\"] = tab_string + task_alias\n            else:\n                results_agg[task][\"alias\"] = tab_string + task\n\n        for group in groups_agg:\n            group_results = groups_agg[group]\n\n            if \"samples\" in group_results:\n                group_results.pop(\"samples\")\n\n            tab_string = \"\"\n            if \"tab\" in group_results:\n                tab = group_results.pop(\"tab\")\n                tab_string = \" \" * tab + \"- \" if tab > 0 else \"\"\n\n            if group in task_group_alias:\n                group_alias = task_group_alias[group]\n                groups_agg[group][\"alias\"] = tab_string + group_alias\n            else:\n                groups_agg[group][\"alias\"] = tab_string + group\n\n        for group_name, task_list in task_hierarchy.items():\n            if task_list != []:\n                num_fewshot[group_name] = num_fewshot[task_list[0]]\n\n        results_dict = {\n            \"results\": dict(results_agg.items()),\n            **({\"groups\": dict(groups_agg.items())} if bool(groups_agg) else {}),\n            \"configs\": dict(sorted(configs.items())),\n            \"versions\": dict(sorted(versions.items())),\n            \"n-shot\": dict(sorted(num_fewshot.items())),\n        }\n        if log_samples:\n            results_dict[\"samples\"] = dict(samples)\n    else:\n        results_dict = None\n    \n    with open(f\"{cli_args.output_path}/rank{int(os.environ.get('RANK', 0))}_metric_eval_done.txt\", 'w') as f:\n        f.write(f\"rank {int(os.environ.get('RANK', 0))} eval done\")\n    while len([file for file in os.listdir(cli_args.output_path) if file.endswith('metric_eval_done.txt')]) < lm.accelerator.num_processes:\n        time.sleep(1)\n\n    return results_dict\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/models/internvl2.py", "content": "from typing import List, Tuple\nfrom lmms_eval.api.instance import Instance\nfrom decord import VideoReader, cpu\nimport torch\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom torchvision.transforms.functional import InterpolationMode\nimport numpy as np\nfrom transformers import AutoModel, AutoTokenizer\nfrom lmms_eval.api.registry import register_model\nfrom accelerate import Accelerator, DistributedType\nfrom lmms_eval.api.model import lmms\nfrom tqdm import tqdm\nimport logging\n\neval_logger = logging.getLogger(\"eval_logger\")\n\nIMAGENET_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_STD = (0.229, 0.224, 0.225)\n\nDEFAULT_GEN_KWARGS = dict(\n    num_beams=1,\n    max_new_tokens=1024,\n    do_sample=False,\n)\n\n\ndef build_transform(input_size):\n    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n    transform = T.Compose([T.Lambda(lambda img: img.convert(\"RGB\") if img.mode != \"RGB\" else img), T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC), T.ToTensor(), T.Normalize(mean=MEAN, std=STD)])\n    return transform\n\n\ndef find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n    best_ratio_diff = float(\"inf\")\n    best_ratio = (1, 1)\n    area = width * height\n    for ratio in target_ratios:\n        target_aspect_ratio = ratio[0] / ratio[1]\n        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n        if ratio_diff < best_ratio_diff:\n            best_ratio_diff = ratio_diff\n            best_ratio = ratio\n        elif ratio_diff == best_ratio_diff:\n            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n                best_ratio = ratio\n    return best_ratio\n\n\ndef dynamic_preprocess(image, min_num=1, max_num=6, image_size=448, use_thumbnail=False):\n    orig_width, orig_height = image.size\n    aspect_ratio = orig_width / orig_height\n\n    # calculate the existing image aspect ratio\n    target_ratios = set((i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if i * j <= max_num and i * j >= min_num)\n    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n\n    # find the closest aspect ratio to the target\n    target_aspect_ratio = find_closest_aspect_ratio(aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n\n    # calculate the target width and height\n    target_width = image_size * target_aspect_ratio[0]\n    target_height = image_size * target_aspect_ratio[1]\n    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n\n    # resize the image\n    resized_img = image.resize((target_width, target_height))\n    processed_images = []\n    for i in range(blocks):\n        box = ((i % (target_width // image_size)) * image_size, (i // (target_width // image_size)) * image_size, ((i % (target_width // image_size)) + 1) * image_size, ((i // (target_width // image_size)) + 1) * image_size)\n        # split the image\n        split_img = resized_img.crop(box)\n        processed_images.append(split_img)\n    assert len(processed_images) == blocks\n    if use_thumbnail and len(processed_images) != 1:\n        thumbnail_img = image.resize((image_size, image_size))\n        processed_images.append(thumbnail_img)\n    return processed_images\n\n\ndef load_image(image, input_size=448, max_num=6):\n    transform = build_transform(input_size=input_size)\n    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n    pixel_values = [transform(image) for image in images]\n    pixel_values = torch.stack(pixel_values)\n    return pixel_values\n\n\ndef get_index(bound, fps, max_frame, first_idx=0, num_segments=32):\n    if bound:\n        start, end = bound[0], bound[1]\n    else:\n        start, end = -100000, 100000\n    start_idx = max(first_idx, round(start * fps))\n    end_idx = min(round(end * fps), max_frame)\n    seg_size = float(end_idx - start_idx) / num_segments\n    frame_indices = np.array([int(start_idx + (seg_size / 2) + np.round(seg_size * idx)) for idx in range(num_segments)])\n    return frame_indices\n\n\ndef load_video(video_path, bound=None, input_size=448, max_num=1, num_segments=32):\n    vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\n    max_frame = len(vr) - 1\n    fps = float(vr.get_avg_fps())\n\n    pixel_values_list, num_patches_list = [], []\n    transform = build_transform(input_size=input_size)\n    frame_indices = get_index(bound, fps, max_frame, first_idx=0, num_segments=num_segments)\n    for frame_index in frame_indices:\n        img = Image.fromarray(vr[frame_index].asnumpy()).convert(\"RGB\")\n        img = dynamic_preprocess(img, image_size=input_size, use_thumbnail=True, max_num=max_num)\n        pixel_values = [transform(tile) for tile in img]\n        pixel_values = torch.stack(pixel_values)\n        num_patches_list.append(pixel_values.shape[0])\n        pixel_values_list.append(pixel_values)\n    pixel_values = torch.cat(pixel_values_list)\n    return pixel_values, num_patches_list\n\n\nfrom datetime import timedelta\nfrom accelerate.state import AcceleratorState\nfrom accelerate.utils import InitProcessGroupKwargs\n\n\n@register_model(\"internvl2\")\nclass InternVL2(lmms):\n    def __init__(\n        self,\n        pretrained: str = \"OpenGVLab/InternVL2-2B\",\n        modality: str = \"image\",\n        device: str = \"cuda:0\",\n        device_map: str = \"cuda:0\",\n        batch_size: str = \"1\",\n        **kwargs,\n    ):\n        super().__init__()\n\n        self.path = pretrained\n        self.model = AutoModel.from_pretrained(self.path, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True, trust_remote_code=True).eval().cuda()\n        self.tokenizer = AutoTokenizer.from_pretrained(self.path, trust_remote_code=True)\n\n        batch_size = int(batch_size)\n        assert batch_size == 1, f\"Batch size should be 1 for InternVL2, but got {batch_size}.\"\n        self.batch_size_per_gpu = batch_size\n\n        accelerator_kwargs = InitProcessGroupKwargs(timeout=timedelta(weeks=52))\n        accelerator = Accelerator(kwargs_handlers=[accelerator_kwargs])\n        self.accelerator = accelerator\n        if accelerator.num_processes > 1:\n            self._device = torch.device(f\"cuda:{accelerator.local_process_index}\")\n            self.device_map = f\"cuda:{accelerator.local_process_index}\"\n        elif accelerator.num_processes == 1 and device_map == \"auto\":\n            self._device = torch.device(device)\n            self.device_map = device_map\n        else:\n            self._device = torch.device(f\"cuda:{accelerator.local_process_index}\")\n            self.device_map = f\"cuda:{accelerator.local_process_index}\"\n\n        if accelerator.num_processes > 1:\n            assert accelerator.distributed_type in [DistributedType.FSDP, DistributedType.MULTI_GPU, DistributedType.DEEPSPEED], \"Unsupported distributed type provided. Only DDP and FSDP are supported.\"\n            # If you want to use DistributedType.DEEPSPEED, you have to run accelerate config before using the model\n            # Also, you have to select zero stage 0 (equivalent to DDP) in order to make the prepare model works\n            # I tried to set different parameters in the kwargs to let default zero 2 stage works, but it didn't work.\n            if accelerator.distributed_type == DistributedType.DEEPSPEED:\n                kwargs = {\n                    \"train_micro_batch_size_per_gpu\": self.batch_size_per_gpu,\n                    \"train_batch_size\": self.batch_size_per_gpu * accelerator.num_processes,\n                }\n                AcceleratorState().deepspeed_plugin.deepspeed_config_process(must_match=True, **kwargs)\n                eval_logger.info(\"Detected that you are using DistributedType.DEEPSPEED. Make sure you run `accelerate config` and set zero stage to 0\")\n\n            if accelerator.distributed_type == DistributedType.FSDP or accelerator.distributed_type == DistributedType.DEEPSPEED:\n                self._model = accelerator.prepare(self.model)\n            else:\n                self._model = accelerator.prepare_model(self.model, evaluation_mode=True)\n            self.accelerator = accelerator\n            if self.accelerator.is_local_main_process:\n                eval_logger.info(f\"Using {accelerator.num_processes} devices with data parallelism\")\n            self._rank = self.accelerator.local_process_index\n            self._world_size = self.accelerator.num_processes\n        elif accelerator.num_processes == 1 and device_map == \"auto\":\n            eval_logger.info(f\"Using {accelerator.num_processes} devices with tensor parallelism\")\n            self._rank = 0\n            self._word_size = 1\n        else:\n            eval_logger.info(f\"Using single device: {self._device}\")\n            self.model.to(self._device)\n            self._rank = 0\n            self._world_size = 1\n\n        self.device = self._device\n        self.modality = modality\n\n    def flatten(self, input):\n        new_list = []\n        for i in input:\n            for j in i:\n                new_list.append(j)\n        return new_list\n\n    def generate_until(self, requests) -> List[str]:\n        res = []\n        pbar = tqdm(total=len(requests), disable=(self.rank != 0), desc=\"Model Responding\")\n\n        for contexts, gen_kwargs, doc_to_visual, doc_id, task, split in [reg.args for reg in requests]:\n            if \"until\" in gen_kwargs:\n                gen_kwargs.pop(\"until\")\n\n            for k, v in DEFAULT_GEN_KWARGS.items():\n                if k not in gen_kwargs:\n                    gen_kwargs[k] = v\n\n            visuals = [doc_to_visual(self.task_dict[task][split][doc_id])]\n            visuals = self.flatten(visuals)\n            if self.modality == \"image\":\n                visuals = [load_image(visual).to(torch.bfloat16).cuda() for visual in visuals]\n                pixel_values = torch.cat(visuals, dim=0)\n                num_patches_list = [visual.size(0) for visual in visuals]\n                if visuals:\n                    image_tokens = [\"<image>\"] * len(visuals)\n                    image_tokens = \" \".join(image_tokens)\n                    contexts = image_tokens + \"\\n\" + contexts\n                response, history = self.model.chat(self.tokenizer, pixel_values, contexts, gen_kwargs, num_patches_list=num_patches_list, history=None, return_history=True)\n\n            elif self.modality == \"video\":\n                assert len(visuals) == 1, f\"Only one video is supported, but got {len(visuals)} videos.\"\n                video_path = visuals[0]\n                pixel_values, num_patches_list = load_video(video_path, num_segments=8, max_num=1)\n                pixel_values = pixel_values.to(torch.bfloat16).cuda()\n                video_prefix = \"\".join([f\"Frame{i+1}: <image>\\n\" for i in range(len(num_patches_list))])\n                question = video_prefix + contexts\n                response, history = self.model.chat(self.tokenizer, pixel_values, question, gen_kwargs, num_patches_list=num_patches_list, history=None, return_history=True)\n            res.append(response)\n            pbar.update(1)\n        pbar.close()\n        return res\n\n    def loglikelihood(self, requests: List[Instance]) -> List[Tuple[float, bool]]:\n        assert False, \"Not implemented yet.\"\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/api/samplers.py", "content": "class ContextSampler:\n    def __init__(self, docs, task, fewshot_indices=None, rnd=None) -> None:\n        self.rnd = rnd\n        assert self.rnd, \"must pass rnd to FewShotSampler!\"\n\n        self.task = task\n        self.config = task._config\n\n        self.target_delimiter = self.config.target_delimiter\n        self.fewshot_delimiter = self.config.fewshot_delimiter\n\n        self.doc_to_text = self.task.doc_to_text\n        self.doc_to_target = self.task.doc_to_target\n        self.doc_to_choice = self.task.doc_to_choice\n\n        self.docs = docs  # HF dataset split, provided by task._fewshot_docs()\n        if fewshot_indices:  # subset few-shot docs from\n            self.docs = self.docs.select(fewshot_indices)\n\n    def get_context(self, doc, num_fewshot):\n        # draw an extra fewshot sample if using same split as evaluating on\n        n_samples = num_fewshot + 1 if self.config.fewshot_split == self.config.test_split else num_fewshot\n\n        # draw `n_samples` docs from fewshot_docs\n        fewshotex = self.sample(n_samples)\n\n        # get rid of the doc that's the one we're evaluating, if it's in the fewshot\n        # TODO: should we just stop people from using fewshot from same split as evaluating?\n        selected_docs = [x for x in fewshotex if x != doc][:num_fewshot]\n\n        labeled_examples = (\n            self.fewshot_delimiter.join(\n                [\n                    # TODO: is separating doc_to_text and doc_to_target by one space always desired?\n                    (self.doc_to_text(doc) if (self.config.doc_to_choice is None or type(self.doc_to_text(doc)) is str) else self.doc_to_choice(doc)[self.doc_to_text(doc)])\n                    + self.target_delimiter\n                    + (\n                        str(self.doc_to_target(doc)[0])\n                        if type(self.doc_to_target(doc)) is list\n                        else self.doc_to_target(doc)\n                        if (self.config.doc_to_choice is None or type(self.doc_to_target(doc)) is str)\n                        else str(self.doc_to_choice(doc)[self.doc_to_target(doc)])\n                    )\n                    for doc in selected_docs\n                ]\n            )\n            + self.fewshot_delimiter\n        )\n\n        return labeled_examples\n\n    def sample(self, n):\n        \"\"\"\n        Draw `n` samples from our fewshot docs. This method should be overridden by subclasses.\n        \"\"\"\n\n        return self.rnd.sample(self.docs, n)\n\n\nclass FirstNSampler(ContextSampler):\n    def sample(self, n) -> None:\n        \"\"\"\n        Draw the first `n` samples in order from the specified split.\n        Used for tasks with \"canonical\" ordered fewshot examples, such as MMLU and CMMLU.\n        \"\"\"\n        assert n <= len(self.docs), f\"Error: number of fewshot samples requested exceeds the {len(self.docs)} that are available.\"\n        return self.docs[:n]\n\n\nclass BalancedSampler(ContextSampler):\n    def sample(self, n) -> None:\n        \"\"\"\n        TODO: this should return approximately class-balanced samples from our fewshot examples.\n        TODO: what order should they be in? maybe random?\n        \"\"\"\n\n        pass\n\n\nclass ManualSampler(ContextSampler):\n    def sample(self, n) -> None:\n        \"\"\" \"\"\"\n        pass\n\n\nSAMPLER_REGISTRY = {\n    \"default\": ContextSampler,\n    \"first_n\": FirstNSampler,\n}\n\n\ndef get_sampler(name):\n    try:\n        return SAMPLER_REGISTRY[name]\n    except KeyError:\n        raise ValueError(f\"Attempted to use contextsampler '{name}', but no sampling strategy for this name found! Supported model names: {', '.join(SAMPLER_REGISTRY.keys())}\")\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/models/tinyllava.py", "content": "import torch\n\ntorch.backends.cuda.matmul.allow_tf32 = True\n\nimport copy\nfrom tqdm import tqdm\nfrom datetime import timedelta\n\nfrom lmms_eval import utils\nfrom lmms_eval.api.instance import Instance\nfrom lmms_eval.api.model import lmms\nfrom lmms_eval.api.registry import register_model\nfrom lmms_eval.utils import stop_sequences_criteria\n\nfrom accelerate import Accelerator, DistributedType, InitProcessGroupKwargs\nfrom accelerate.state import AcceleratorState\nfrom typing import List, Optional, Union, Tuple\nfrom packaging import version\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom loguru import logger as eval_logger\n\ntry:\n    from tinyllava.model import load_pretrained_model\n    from tinyllava.data import ImagePreprocess, TextPreprocess\n    from tinyllava.utils.constants import DEFAULT_IMAGE_TOKEN\n    from tinyllava.utils.message import Message\nexcept Exception as e:\n    eval_logger.debug(\"TinyLLaVA_Factory is not installed. Please install TinyLLaVA_Factory to use this model.\\nError: %s\" % e)\n\n# inference implementation for attention, can be \"sdpa\", \"eager\", \"flash_attention_2\". Seems FA2 is not effective during inference: https://discuss.huggingface.co/t/flash-attention-has-no-effect-on-inference/73453/5\n# if is_flash_attn_2_available:\n#     best_fit_attn_implementation = \"flash_attention_2\" # flash_attn has a bug that says: ERROR Error query and key must have the same dtype in generating\n\nif version.parse(torch.__version__) >= version.parse(\"2.1.2\"):\n    best_fit_attn_implementation = \"sdpa\"\nelse:\n    best_fit_attn_implementation = \"eager\"\n\n\n@register_model(\"tinyllava\")\nclass TinyLlava(lmms):\n    \"\"\"\n    TinyLlava Model\n    \"\"\"\n\n    def __init__(\n        self,\n        pretrained: str = \"tinyllava/TinyLLaVA-Phi-2-SigLIP-3.1B\",\n        device: Optional[str] = \"cuda:0\",\n        batch_size: Optional[Union[int, str]] = 1,\n        device_map=\"cuda:0\",\n        conv_mode=\"phi\",  # TODO\n        use_cache=True,\n        **kwargs,\n    ) -> None:\n        super().__init__()\n        # Do not use kwargs for now\n        assert kwargs == {}, f\"Unexpected kwargs: {kwargs}\"\n\n        accelerator_kwargs = InitProcessGroupKwargs(timeout=timedelta(weeks=52))\n        accelerator = Accelerator(kwargs_handlers=[accelerator_kwargs])\n        if accelerator.num_processes > 1:\n            self._device = torch.device(f\"cuda:{accelerator.local_process_index}\")\n            self.device_map = f\"cuda:{accelerator.local_process_index}\"\n        elif accelerator.num_processes == 1 and device_map == \"auto\":\n            self._device = torch.device(device)\n            self.device_map = device_map\n        else:\n            self._device = torch.device(f\"cuda:{accelerator.local_process_index}\")\n            self.device_map = f\"cuda:{accelerator.local_process_index}\"\n\n        self._model, self._tokenizer, self._image_processor, self._max_length = load_pretrained_model(pretrained, device_map=self.device_map)\n        data_args = self._model.config\n        self._image_processor = ImagePreprocess(self._image_processor, data_args)\n        assert self._tokenizer.padding_side == \"right\", \"Not sure but seems like `right` is a natural choice for padding?\"\n        self._text_processor = TextPreprocess(self._tokenizer, conv_mode)\n\n        self._config = self._model.config\n        self.model.eval()\n        self.model.tie_weights()\n        # self.truncation = truncation\n        self.batch_size_per_gpu = int(batch_size)\n        # self.conv_template = conv_template\n        self.use_cache = use_cache\n        # self.truncate_context = truncate_context\n\n        # assert self.batch_size_per_gpu == 1, \"Llava currently does not support batched generation. See https://github.com/haotian-liu/LLaVA/issues/754. HF Llava also has this issue.\"\n        if accelerator.num_processes > 1:\n            assert accelerator.distributed_type in [DistributedType.FSDP, DistributedType.MULTI_GPU, DistributedType.DEEPSPEED], \"Unsupported distributed type provided. Only DDP and FSDP are supported.\"\n            # If you want to use DistributedType.DEEPSPEED, you have to run accelerate config before using the model\n            # Also, you have to select zero stage 0 (equivalent to DDP) in order to make the prepare model works\n            # I tried to set different parameters in the kwargs to let default zero 2 stage works, but it didn't work.\n            if accelerator.distributed_type == DistributedType.DEEPSPEED:\n                kwargs = {\n                    \"train_micro_batch_size_per_gpu\": self.batch_size_per_gpu,\n                    \"train_batch_size\": self.batch_size_per_gpu * accelerator.num_processes,\n                }\n                AcceleratorState().deepspeed_plugin.deepspeed_config_process(must_match=True, **kwargs)\n                eval_logger.info(\"Detected that you are using DistributedType.DEEPSPEED. Make sure you run `accelerate config` and set zero stage to 0\")\n\n            if accelerator.distributed_type == DistributedType.FSDP or accelerator.distributed_type == DistributedType.DEEPSPEED:\n                self._model = accelerator.prepare(self.model)\n            else:\n                self._model = accelerator.prepare_model(self.model, evaluation_mode=True)\n            self.accelerator = accelerator\n            if self.accelerator.is_local_main_process:\n                eval_logger.info(f\"Using {accelerator.num_processes} devices with data parallelism\")\n            self._rank = self.accelerator.local_process_index\n            self._world_size = self.accelerator.num_processes\n        elif accelerator.num_processes == 1 and device_map == \"auto\":\n            eval_logger.info(f\"Using {accelerator.num_processes} devices with tensor parallelism\")\n            self._rank = 0\n            self._word_size = 1\n        else:\n            eval_logger.info(f\"Using single device: {self._device}\")\n            self.model.to(self._device)\n            self._rank = 0\n            self._world_size = 1\n\n    @property\n    def config(self):\n        # return the associated transformers.AutoConfig for the given pretrained model.\n        return self._config\n\n    @property\n    def tokenizer(self):\n        return self._tokenizer\n\n    @property\n    def model(self):\n        # returns the model, unwrapping it if using Accelerate\n        if hasattr(self, \"accelerator\"):\n            return self.accelerator.unwrap_model(self._model)\n        else:\n            return self._model\n\n    @property\n    def eot_token_id(self):\n        # we use EOT because end of *text* is more accurate for what we're doing than end of *sentence*\n        return self.tokenizer.eos_token_id\n\n    @property\n    def max_length(self):\n        return self._max_length\n\n    def pad_sequence(self, input_ids, batch_first, padding_value):\n        if self.tokenizer.padding_side == \"left\":\n            input_ids = [torch.flip(_input_ids, [0]) for _input_ids in input_ids]\n        input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=batch_first, padding_value=padding_value)\n        if self.tokenizer.padding_side == \"left\":\n            input_ids = torch.flip(input_ids, [1])\n        return input_ids\n\n    @property\n    def batch_size(self):\n        return self.batch_size_per_gpu\n\n    @property\n    def device(self):\n        return self._device\n\n    @property\n    def rank(self):\n        return self._rank\n\n    @property\n    def world_size(self):\n        return self._world_size\n\n    def tok_encode(self, string: str, left_truncate_len=None, add_special_tokens=None) -> List[int]:\n        \"\"\" \"\"\"\n        add_special_tokens = False if add_special_tokens is None else add_special_tokens\n        encoding = self.tokenizer.encode(string, add_special_tokens=add_special_tokens)\n        # left-truncate the encoded context to be at most `left_truncate_len` tokens long\n        if left_truncate_len:\n            encoding = encoding[-left_truncate_len:]\n        return encoding\n\n    def tok_decode(self, tokens):\n        try:\n            return self.tokenizer.decode(tokens)\n        except:\n            return self.tokenizer.decode([tokens])\n\n    def flatten(self, input):\n        new_list = []\n        for i in input:\n            for j in i:\n                new_list.append(j)\n        return new_list\n\n    def loglikelihood(self, requests: List[Instance]) -> List[Tuple[float, bool]]:\n        # TODO\n        res = []\n        pbar = tqdm(total=len(requests), disable=(self.rank != 0), desc=\"Model Responding\")\n\n        for contexts, doc_to_target, doc_to_visual, doc_id, task, split in [reg.args for reg in requests]:\n            # encode, pad, and truncate contexts for this batch\n            if type(doc_to_target) == str:\n                continuation = doc_to_target\n            else:\n                continuation = doc_to_target(self.task_dict[task][split][doc_id])\n            visuals = [doc_to_visual(self.task_dict[task][split][doc_id])]\n            visuals = self.flatten(visuals)\n            image_sizes = [[visual.size[0], visual.size[1]] for visual in visuals]\n            if visuals:\n                # https://github.com/zjysteven/TinyLLaVA_Factory/blob/main/tinyllava/data/image_preprocess.py\n                # tinyllava's image processor seems to take each individual image as input\n                image = [self._image_processor(v) for v in visuals]\n                if type(image) is list:\n                    image = [_image.to(dtype=torch.float16, device=self.device) for _image in image]\n                    # as of 2024/06, tinyllava only accepts `images` input to be a tensor\n                    image = torch.stack(image)\n                else:\n                    image = image.to(dtype=torch.float16, device=self.device)\n            else:\n                image = None\n\n            prompts_input = contexts[0] if isinstance(contexts, list) else contexts\n\n            if image is not None and len(image) != 0 and DEFAULT_IMAGE_TOKEN not in prompts_input:\n                \"\"\"\n                Three senarios:\n                1. No image, and there for, no image token should be added.\n                2. image token is already specified in the context, so we don't need to add it.\n                3. image token is not specified in the context and there is image inputs, so we need to add it. In this case, we add the image token at the beginning of the context and add a new line.\n                \"\"\"\n                image_tokens = [DEFAULT_IMAGE_TOKEN] * len(visuals)\n                image_tokens = \" \".join(image_tokens)\n                prompts_input = image_tokens + \"\\n\" + (contexts[0] if isinstance(contexts, list) else contexts)\n\n            msg = Message()\n            msg.add_message(prompts_input)\n            contxt_id = self._text_processor(msg.messages, mode=\"eval\")[\"input_ids\"]\n            # Add the answer of the second role\n            msg._messages[1][\"value\"] = continuation\n            input_ids = self._text_processor(msg.messages, mode=\"eval\")[\"input_ids\"]\n\n            labels = input_ids.clone()\n            # Context part no need to calculate for loss\n            labels[0, : contxt_id.shape[1]] = -100\n\n            with torch.inference_mode():\n                outputs = self.model(input_ids=input_ids, labels=labels, images=image, use_cache=True, image_sizes=image_sizes)\n            loss = outputs[\"loss\"]\n            # loss = torch.exp(loss)\n            logits = outputs[\"logits\"]\n            greedy_tokens = logits.argmax(dim=-1)\n            cont_toks = input_ids[:, contxt_id.shape[1] :]  # [1, seq]\n            greedy_tokens = greedy_tokens[:, contxt_id.shape[1] : input_ids.shape[1]]  # [1, seq]\n            max_equal = (greedy_tokens == cont_toks).all()\n            res.append((float(loss.item()), bool(max_equal)))\n            pbar.update(1)\n        pbar.close()\n        return res\n\n    def generate_until(self, requests: List[Instance]) -> List[str]:\n        res = []\n\n        def _collate(x):\n            # the negative sign on len(toks) sorts descending - this has a few advantages:\n            # - time estimates will always be over not underestimates, which is more useful for planning\n            # - to know the size of a batch when going through the list, you know the first one is always the batch\n            #   padded context length. this is useful to simplify the batching logic and more importantly to make\n            #   automatic adaptive batches much much easier to implement\n            # - any OOMs will happen right away rather than near the end\n            toks = self.tok_encode(x[0])\n            return -len(toks), x[0]\n\n        # we group requests by their generation_kwargs,\n        # so that we don't try to execute e.g. greedy sampling and temp=0.8 sampling\n        # in the same batch.\n        re_ords = utils.Collator([reg.args for reg in requests], _collate, grouping=True)\n        chunks = re_ords.get_batched(n=self.batch_size, batch_fn=None)\n        num_iters = len(requests) // self.batch_size if len(requests) % self.batch_size == 0 else len(requests) // self.batch_size + 1\n        pbar = tqdm(total=num_iters, disable=(self.rank != 0), desc=\"Model Responding\")\n        for chunk in chunks:\n            contexts, all_gen_kwargs, doc_to_visual, doc_id, task, split = zip(*chunk)\n            task = task[0]\n            split = split[0]\n            batched_visuals = [doc_to_visual[0](self.task_dict[task][split][ids]) for ids in doc_id]  # [B, N]\n            flattened_visuals = self.flatten(batched_visuals)  # [B*N]\n            # we assume all gen kwargs in the batch are the same\n            # this is safe to assume because the `grouper` object ensures it.\n            gen_kwargs = all_gen_kwargs[0]\n\n            # Set default values for until and max_new_tokens\n            until = [self.tok_decode(self.eot_token_id)]\n\n            # Update values from gen_kwargs if present\n            if \"until\" in gen_kwargs:\n                until = gen_kwargs.pop(\"until\")\n                if isinstance(until, str):\n                    until = [until]\n                elif not isinstance(until, list):\n                    raise ValueError(f\"Expected `gen_kwargs['until']` to be of type Union[str,list] but got {type(until)}\")\n\n            if \"image_aspect_ratio\" in gen_kwargs.keys() and \"image_aspect_ratio\" not in self._config.__dict__:\n                # here we should pop it out of gen_kwargs so that it doesn't get passed to the model for next step of generation\n                self._config.image_aspect_ratio = gen_kwargs.pop(\"image_aspect_ratio\")\n                eval_logger.info(f\"Setting image aspect ratio: {self._config.image_aspect_ratio}\")\n            # encode, pad, and truncate contexts for this batch\n            if flattened_visuals:\n                image_tensor = [self._image_processor(v) for v in flattened_visuals]\n                if type(image_tensor) is list:\n                    image_tensor = [_image.to(dtype=torch.float16, device=self.device) for _image in image_tensor]\n                    # as of 2024/06, tinyllava only accepts `images` input to be a tensor\n                    image_tensor = torch.stack(image_tensor)\n                else:\n                    image_tensor = image_tensor.to(dtype=torch.float16, device=self.device)\n            else:\n                image_tensor = None\n\n            # prompts_input = contexts[0]\n\n            question_input = []\n\n            for visual, context in zip(batched_visuals, contexts):\n                if image_tensor is not None and len(image_tensor) != 0 and DEFAULT_IMAGE_TOKEN not in context:\n                    \"\"\"\n                    Three senarios:\n                    1. No image, and there for, no image token should be added.\n                    2. image token is already specified in the context, so we don't need to add it.\n                    3. image token is not specified in the context and there is image inputs, so we need to add it. In this case, we add the image token at the beginning of the context and add a new line.\n                    \"\"\"\n                    image_tokens = [DEFAULT_IMAGE_TOKEN] * len(visual) if isinstance(visual, list) else [DEFAULT_IMAGE_TOKEN]\n                    image_tokens = \" \".join(image_tokens)\n                    question = image_tokens + \"\\n\" + context\n                else:\n                    question = context\n\n                msg = Message()\n                msg.add_message(question)\n                prompt_question = self._text_processor(msg.messages, mode=\"eval\")[\"prompt\"]\n                question_input.append(prompt_question)\n\n            # The above for loop has bugs. When there is no visuals, e.g. pure text,\n            # there will be no for loop execute resulting in an empty question_input (because no visuals)\n            # Scenario 1 won't even be execute\n            if len(flattened_visuals) == 0:\n                for context in contexts:\n                    question = context\n                    msg = Message()\n                    msg.add_message(question)\n                    prompt_question = self._text_processor(msg.messages, mode=\"eval\")[\"prompt\"]\n                    question_input.append(prompt_question)\n\n            # input_ids = tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(self.device)\n            # preconfigure gen_kwargs with defaults\n            gen_kwargs[\"image_sizes\"] = [flattened_visuals[idx].size for idx in range(len(flattened_visuals))]\n            if \"max_new_tokens\" not in gen_kwargs:\n                gen_kwargs[\"max_new_tokens\"] = 1024\n            if \"temperature\" not in gen_kwargs:\n                gen_kwargs[\"temperature\"] = 0\n            if \"top_p\" not in gen_kwargs:\n                gen_kwargs[\"top_p\"] = None\n            if \"num_beams\" not in gen_kwargs:\n                gen_kwargs[\"num_beams\"] = 1\n\n            # input_ids_list = [tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\") for prompt in question_input]\n            input_ids_list = [self._text_processor.template.tokenizer_image_token(prompt, self.tokenizer, return_tensors=\"pt\") for prompt in question_input]\n            pad_token_ids = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id\n            input_ids = self.pad_sequence(input_ids_list, batch_first=True, padding_value=pad_token_ids).to(self.device)\n            attention_masks = input_ids.ne(pad_token_ids).to(self.device)\n            # These steps are not in LLaVA's original code, but are necessary for generation to work\n            # TODO: attention to this major generation step...\n            try:\n                cont = self.model.generate(\n                    input_ids,\n                    attention_mask=attention_masks,\n                    pad_token_id=pad_token_ids,\n                    images=image_tensor,\n                    image_sizes=gen_kwargs[\"image_sizes\"],\n                    do_sample=True if gen_kwargs[\"temperature\"] > 0 else False,\n                    temperature=gen_kwargs[\"temperature\"],\n                    top_p=gen_kwargs[\"top_p\"],\n                    num_beams=gen_kwargs[\"num_beams\"],\n                    max_new_tokens=gen_kwargs[\"max_new_tokens\"],\n                    use_cache=self.use_cache,\n                )\n                text_outputs = self.tokenizer.batch_decode(cont, skip_special_tokens=True)\n            except Exception as e:\n                raise e\n                eval_logger.error(f\"Error {e} in generating\")\n                cont = \"\"\n                text_outputs = [\"\"]\n\n            # cont_toks_list = cont.tolist()\n            # for cont_toks, context in zip(cont_toks_list, contexts):\n            # discard context + left-padding toks if using causal decoder-only LMM\n            # if self.truncate_context:\n            #     cont_toks = cont_toks[input_ids.shape[1] :]\n            # use secondary stop seqs to cut off should-have-been-stopped content post-hoc\n            # if self.truncate_context:\n            #     for term in until:\n            #         if len(term) > 0:\n            #             # ignore '' separator,\n            #             # for seq2seq case where self.tok_decode(self.eot_token_id) = ''\n            #             text_outputs = text_outputs.split(term)[0]\n            res.extend(text_outputs)\n            self.cache_hook.add_partial(\"generate_until\", (context, gen_kwargs), text_outputs)\n            pbar.update(1)\n            # reorder this group of results back to original unsorted form\n        res = re_ords.get_original(res)\n\n        pbar.close()\n        return res\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/api/filter.py", "content": "from dataclasses import dataclass\nfrom typing import List\n\nfrom lmms_eval.api.instance import Instance\nfrom datasets import Dataset\n\n\nclass Filter:\n    \"\"\"\n    Filter classes operate on a per-task level.\n    They take all model outputs (`instance.resps` for all `task.instances`)\n    across all instances of a task, and perform operations.\n    In a single run, one can configure any number of separate filters or lists of filters.\n\n    \"\"\"\n\n    def __init__(self, *args, **kwargs) -> None:\n        \"\"\"\n        Can define custom behavior here, if an individual instantiation of a Filter class should have state.\n        \"\"\"\n\n    def apply(self, resps, docs):\n        \"\"\"\n        Defines the operation to perform on a list of the `inst.resps` properties of `Instance` objects.\n        Should return the list of (filtered) response lists *in the same order as they were input*, e.g.\n        if pass in [<inst.resps for instance 0>, <inst.resps for instance 1>] should return\n        [<filtered resps for instance 0>, <filtered resps for instance 1>]\n        \"\"\"\n        return resps\n\n\n@dataclass\nclass FilterEnsemble:\n    \"\"\"\n    FilterEnsemble creates a pipeline applying multiple filters.\n    Its intended usage is to stack multiple post-processing steps in order.\n    `task.apply_filters` should use a list of FilterEnsemble classes that it stores, to apply each\n    pipeline separately.\n    \"\"\"\n\n    name: str\n    filters: List[Filter]\n\n    def apply(self, instances: List[Instance], docs: List[Dataset]) -> None:\n        resps = [inst.resps for inst in instances]  # operate just on the model responses\n        for f in self.filters:\n            # apply filters in sequence\n            resps = f.apply(resps, docs)\n\n        # add the end results after filtering to filtered_requests of their respective source instances.\n        # has key `self.name`: each FilterEnsemble applied in a given run should use a different name.\n        for inst, resp in zip(instances, resps):\n            inst.filtered_resps[self.name] = resp\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/filters/selection.py", "content": "from collections import Counter\n\nfrom lmms_eval.api.filter import Filter\n\n\nclass TakeFirstFilter(Filter):\n    def __init__(self) -> None:\n        \"\"\"\n        Can define custom behavior here, if an individual instantiation of a Filter class should have state.\n        \"\"\"\n\n    def apply(self, resps, docs):\n        \"\"\"\n        Assuming each entry of `resps` is a list of model responses, we discard all but the first response.\n        \"\"\"\n        return map(lambda r: r[0], resps)\n\n\nclass TakeKFilter(Filter):\n    def __init__(self, *args, **kwargs) -> None:\n        self.k = kwargs.pop(\"k\")\n\n        super().__init__(*args, **kwargs)\n\n    def apply(self, resps, docs):\n        # check we have at least k responses per doc, else we can't take the first k\n        assert len(resps[0]) >= self.k, f\"Need at least {self.k} responses per doc to take first {self.k}, but got {len(resps[0])} only! Please increase TaskConfig.repeats .\"\n        return map(lambda r: r[: self.k], resps)\n\n\nclass MajorityVoteFilter(Filter):\n    def __init__(self) -> None:\n        \"\"\"\n        Can define custom behavior here, if an individual instantiation of a Filter class should have state.\n        \"\"\"\n\n    def apply(self, resps, docs):\n        \"\"\"\n        Each entry of `resps` is a list of model responses.\n        We select the response that occurs most frequently in each entry of `resps`.\n        \"\"\"\n\n        def select_majority(resp):\n            counts = Counter(resp)\n            vote = counts.most_common(1)[0][0]\n            return vote\n\n        return map(lambda r: [select_majority(r)], resps)\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/models/llava.py", "content": "import torch\n\ntorch.backends.cuda.matmul.allow_tf32 = True\n\n\nimport copy\nfrom tqdm import tqdm\nfrom datetime import timedelta\n\nfrom lmms_eval import utils\nfrom lmms_eval.api.instance import Instance\nfrom lmms_eval.api.model import lmms\nfrom lmms_eval.api.registry import register_model\nfrom lmms_eval.utils import stop_sequences_criteria\n\nfrom accelerate import Accelerator, DistributedType, InitProcessGroupKwargs\nfrom accelerate.state import AcceleratorState\nfrom typing import List, Optional, Union, Tuple\nfrom packaging import version\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom loguru import logger as eval_logger\nimport sys\nfrom moe_model.model.builder import load_pretrained_model\nfrom moe_model.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token\nfrom moe_model.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\nfrom moe_model.conversation import conv_templates\nimport os\n#Setup env\nif \"libmoe\" in os.environ['TOOLKIT_DIR'].lower():\n    sys.path.append(f\"{os.environ['TOOLKIT_DIR']}\")\nelse: \n    sys.path.append(f\"{os.environ['TOOLKIT_DIR']}/LibMoE\")\n\n# inference implementation for attention, can be \"sdpa\", \"eager\", \"flash_attention_2\". Seems FA2 is not effective during inference: https://discuss.huggingface.co/t/flash-attention-has-no-effect-on-inference/73453/5\n# if is_flash_attn_2_available:\n#     best_fit_attn_implementation = \"flash_attention_2\" # flash_attn has a bug that says: ERROR Error query and key must have the same dtype in generating\n\nif version.parse(torch.__version__) >= version.parse(\"2.1.2\"):\n    best_fit_attn_implementation = \"sdpa\"\nelse:\n    best_fit_attn_implementation = \"eager\"\n\n\nimport torch\n\ntorch.backends.cuda.matmul.allow_tf32 = True\n\n\nimport copy\nfrom tqdm import tqdm\nfrom datetime import timedelta\n\nfrom lmms_eval import utils\nfrom lmms_eval.api.instance import Instance\nfrom lmms_eval.api.model import lmms\nfrom lmms_eval.api.registry import register_model\nfrom lmms_eval.utils import stop_sequences_criteria\n\nfrom accelerate import Accelerator, DistributedType, InitProcessGroupKwargs\nfrom accelerate.state import AcceleratorState\nfrom typing import List, Optional, Union, Tuple\nfrom packaging import version\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom loguru import logger as eval_logger\n\n@register_model(\"llava\")\nclass Llava(lmms):\n    \"\"\"\n    Llava Model\n    \"\"\"\n\n    def __init__(\n        self,\n        pretrained: str = \"liuhaotian/llava-v1.5-7b\",\n        truncation: Optional[bool] = True,\n        device: Optional[str] = \"cuda:0\",\n        batch_size: Optional[Union[int, str]] = 1,\n        model_name=None,\n        attn_implementation=best_fit_attn_implementation,\n        device_map=\"cuda:0\",\n        conv_template=\"vicuna_v1\",\n        use_cache=True,\n        tie_weights: bool = True,\n        truncate_context=False,  # whether to truncate the context in generation, set it False for LLaVA-1.6\n        customized_config=None,  # ends in json\n        **kwargs,\n    ) -> None:\n        super().__init__()\n        # Do not use kwargs for now\n        assert kwargs == {}, f\"Unexpected kwargs: {kwargs}\"\n        self.conv_template = conv_template\n        accelerator_kwargs = InitProcessGroupKwargs(timeout=timedelta(weeks=52))\n        accelerator = Accelerator(kwargs_handlers=[accelerator_kwargs])\n        self.accelerator = accelerator\n        print(f\"accelerator.num_processes: {accelerator.num_processes}\")\n        if accelerator.num_processes > 1:\n            self._device = torch.device(f\"cuda:{accelerator.local_process_index}\")\n            self.device_map = f\"cuda:{accelerator.local_process_index}\"\n        elif accelerator.num_processes == 1 and device_map == \"auto\":\n            self._device = torch.device(device)\n            self.device_map = device_map\n        else:\n            self._device = torch.device(f\"cuda:{accelerator.local_process_index}\")\n            self.device_map = f\"cuda:{accelerator.local_process_index}\"\n\n        model_name = model_name if model_name is not None else get_model_name_from_path(pretrained)\n        try:\n            # Try to load the model with the multimodal argument\n            self._tokenizer, self._model, self._image_processor, self._max_length = load_pretrained_model(\n                model_path = pretrained,\n                model_base = None,\n                model_name = pretrained,\n                use_flash_attn = True,\n                device_map = \"cuda:0\"\n                \n            )\n            self._model.config.training = False\n        except Exception as e:\n            print(f\"Model is not MultiModal LLM: {e}\")\n            # # for older versions of LLaVA that don't have multimodal argument\n            # llava_model_args.pop(\"multimodal\", None)\n            # self._tokenizer, self._model, self._image_processor, self._max_length = load_pretrained_model(pretrained, None, model_name, device_map=self.device_map, **llava_model_args)\n        # self.model = self._model\n        self._config = self._model.config\n        self.model.eval()\n        # if tie_weights:\n        #     self.model.tie_weights()\n\n        self.truncation = truncation\n        self.batch_size_per_gpu = int(batch_size)\n        self.conv_template = conv_template\n        self.use_cache = use_cache\n        self.truncate_context = truncate_context\n        # assert self.batch_size_per_gpu == 1, \"Llava currently does not support batched generation. See https://github.com/haotian-liu/LLaVA/issues/754. HF Llava also has this issue.\"\n        if accelerator.num_processes > 1:\n            assert accelerator.distributed_type in [DistributedType.FSDP, DistributedType.MULTI_GPU, DistributedType.DEEPSPEED], \"Unsupported distributed type provided. Only DDP and FSDP are supported.\"\n            # If you want to use DistributedType.DEEPSPEED, you have to run accelerate config before using the model\n            # Also, you have to select zero stage 0 (equivalent to DDP) in order to make the prepare model works\n            # I tried to set different parameters in the kwargs to let default zero 2 stage works, but it didn't work.\n            if accelerator.distributed_type == DistributedType.DEEPSPEED:\n                kwargs = {\n                    \"train_micro_batch_size_per_gpu\": self.batch_size_per_gpu,\n                    \"train_batch_size\": self.batch_size_per_gpu * accelerator.num_processes,\n                }\n                AcceleratorState().deepspeed_plugin.deepspeed_config_process(must_match=True, **kwargs)\n                eval_logger.info(\"Detected that you are using DistributedType.DEEPSPEED. Make sure you run `accelerate config` and set zero stage to 0\")\n\n            if accelerator.distributed_type == DistributedType.FSDP or accelerator.distributed_type == DistributedType.DEEPSPEED:\n                self._model = accelerator.prepare(self.model)\n            else:\n                self._model = accelerator.prepare_model(self.model, evaluation_mode=True)\n            self.accelerator = accelerator\n            if self.accelerator.is_local_main_process:\n                eval_logger.info(f\"Using {accelerator.num_processes} devices with data parallelism\")\n            self._rank = self.accelerator.local_process_index\n            self._world_size = self.accelerator.num_processes\n        elif accelerator.num_processes == 1 and device_map == \"auto\":\n            eval_logger.info(f\"Using {accelerator.num_processes} devices with tensor parallelism\")\n            self._rank = 0\n            self._word_size = 1\n        else:\n            eval_logger.info(f\"Using single device: {self._device}\")\n            self.model.to(self._device)\n            self._rank = 0\n            self._world_size = 1\n\n    @property\n    def config(self):\n        # return the associated transformers.AutoConfig for the given pretrained model.\n        return self._config\n\n    @property\n    def tokenizer(self):\n        return self._tokenizer\n\n    @property\n    def model(self):\n        # returns the model, unwrapping it if using Accelerate\n        if hasattr(self, \"accelerator\"):\n            return self.accelerator.unwrap_model(self._model)\n        else:\n            return self._model\n\n    @property\n    def eot_token_id(self):\n        # we use EOT because end of *text* is more accurate for what we're doing than end of *sentence*\n        return self.tokenizer.eos_token_id\n\n    @property\n    def max_length(self):\n        return self._max_length\n\n    def pad_sequence(self, input_ids, batch_first, padding_value):\n        if self.tokenizer.padding_side == \"left\":\n            input_ids = [torch.flip(_input_ids, [0]) for _input_ids in input_ids]\n        input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=batch_first, padding_value=padding_value)\n        if self.tokenizer.padding_side == \"left\":\n            input_ids = torch.flip(input_ids, [1])\n        return input_ids\n\n    @property\n    def batch_size(self):\n        return self.batch_size_per_gpu\n\n    @property\n    def device(self):\n        return self._device\n\n    @property\n    def rank(self):\n        return self._rank\n\n    @property\n    def world_size(self):\n        return self._world_size\n\n    def tok_encode(self, string: str, left_truncate_len=None, add_special_tokens=None) -> List[int]:\n        \"\"\" \"\"\"\n        add_special_tokens = False if add_special_tokens is None else add_special_tokens\n        encoding = self.tokenizer.encode(string, add_special_tokens=add_special_tokens)\n        # left-truncate the encoded context to be at most `left_truncate_len` tokens long\n        if left_truncate_len:\n            encoding = encoding[-left_truncate_len:]\n        return encoding\n\n    def tok_decode(self, tokens):\n        try:\n            return self.tokenizer.decode(tokens)\n        except:\n            return self.tokenizer.decode([tokens])\n\n    def loglikelihood(self, requests: List[Instance]) -> List[Tuple[float, bool]]:\n        # TODO\n        res = []\n        pbar = tqdm(total=len(requests), disable=(self.rank != 0), desc=\"Model Responding\")\n\n        for contexts, doc_to_target, doc_to_visual, doc_id, task, split in [reg.args for reg in requests]:\n            # encode, pad, and truncate contexts for this batch\n            if type(doc_to_target) == str:\n                continuation = doc_to_target\n            else:\n                continuation = doc_to_target(self.task_dict[task][split][doc_id])\n            visuals = [doc_to_visual(self.task_dict[task][split][doc_id])]\n            visuals = self.flatten(visuals)\n            image_sizes = [[visual.size[0], visual.size[1]] for visual in visuals]\n            if visuals:\n                image = process_images(visuals, self._image_processor, self._config)\n                if type(image) is list:\n                    image = [_image.to(dtype=torch.float16, device=self.device) for _image in image]\n                else:\n                    image = image.to(dtype=torch.float16, device=self.device)\n            else:\n                image = None\n\n            prompts_input = contexts[0] if isinstance(contexts, list) else contexts\n\n            if image is not None and len(image) != 0 and DEFAULT_IMAGE_TOKEN not in prompts_input:\n                \"\"\"\n                Three senarios:\n                1. No image, and there for, no image token should be added.\n                2. image token is already specified in the context, so we don't need to add it.\n                3. image token is not specified in the context and there is image inputs, so we need to add it. In this case, we add the image token at the beginning of the context and add a new line.\n                \"\"\"\n                image_tokens = [DEFAULT_IMAGE_TOKEN] * len(visuals)\n                image_tokens = \" \".join(image_tokens)\n                prompts_input = image_tokens + \"\\n\" + (contexts[0] if isinstance(contexts, list) else contexts)\n\n            # This is much safer for llama3, as we now have some object type in it\n            if \"llama_3\" in self.conv_template:\n                conv = copy.deepcopy(conv_templates[self.conv_template])\n            else:\n                conv = conv_templates[self.conv_template].copy()\n            conv.append_message(conv.roles[0], prompts_input)\n            conv.append_message(conv.roles[1], None)\n            prompt = conv.get_prompt()\n            pad_token_id = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id\n            contxt_id = tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(self.device)\n            # Add the answer of the second role\n            conv.messages[1][1] = continuation\n\n            prompt = conv.get_prompt()\n            input_ids = tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(self.device)\n            labels = input_ids.clone()\n            # Context part no need to calculate for loss\n            labels[0, : contxt_id.shape[1]] = -100\n            with torch.inference_mode():\n                outputs = self.model(input_ids=input_ids, labels=labels, images=image, use_cache=True, image_sizes=image_sizes)\n            loss = outputs[\"loss\"]\n            # loss = torch.exp(loss)\n            logits = outputs[\"logits\"]\n            greedy_tokens = logits.argmax(dim=-1)\n            cont_toks = input_ids[:, contxt_id.shape[1] :]  # [1, seq]\n            greedy_tokens = greedy_tokens[:, contxt_id.shape[1] : input_ids.shape[1]]  # [1, seq]\n            max_equal = (greedy_tokens == cont_toks).all()\n            res.append((float(loss.item()), bool(max_equal)))\n            pbar.update(1)\n        pbar.close()\n        return res\n\n    def flatten(self, input):\n        new_list = []\n        for i in input:\n            for j in i:\n                new_list.append(j)\n        return new_list\n\n    def generate_until(self, requests: List[Instance], **kwargs) -> List[str]:\n        res = []\n        vision_id_experts, mlp_id_experts = [], []\n        def _collate(x):\n            # the negative sign on len(toks) sorts descending - this has a few advantages:\n            # - time estimates will always be over not underestimates, which is more useful for planning\n            # - to know the size of a batch when going through the list, you know the first one is always the batch\n            #   padded context length. this is useful to simplify the batching logic and more importantly to make\n            #   automatic adaptive batches much much easier to implement\n            # - any OOMs will happen right away rather than near the end\n            toks = self.tok_encode(x[0])\n            return -len(toks), x[0]\n\n        # we group requests by their generation_kwargs,\n        # so that we don't try to execute e.g. greedy sampling and temp=0.8 sampling\n        # in the same batch.\n        re_ords = utils.Collator([reg.args for reg in requests], _collate, grouping=True)\n        chunks = re_ords.get_batched(n=self.batch_size, batch_fn=None)\n        num_iters = len(requests) // self.batch_size if len(requests) % self.batch_size == 0 else len(requests) // self.batch_size + 1\n        pbar = tqdm(total=num_iters, disable=(self.rank != 0), desc=\"Model Responding\")\n        for chunk in chunks:\n            contexts, all_gen_kwargs, doc_to_visual, doc_id, task, split = zip(*chunk)\n            task = task[0]\n            split = split[0]\n            batched_visuals = [doc_to_visual[0](self.task_dict[task][split][ids]) for ids in doc_id]  # [B, N]\n            flattened_visuals = self.flatten(batched_visuals)  # [B*N]\n            # we assume all gen kwargs in the batch are the same\n            # this is safe to assume because the `grouper` object ensures it.\n            gen_kwargs = all_gen_kwargs[0]\n\n            # Set default values for until and max_new_tokens\n            until = [self.tok_decode(self.eot_token_id)]\n\n            # Update values from gen_kwargs if present\n            if \"until\" in gen_kwargs:\n                until = gen_kwargs.pop(\"until\")\n                if isinstance(until, str):\n                    until = [until]\n                elif not isinstance(until, list):\n                    raise ValueError(f\"Expected `gen_kwargs['until']` to be of type Union[str,list] but got {type(until)}\")\n\n            if \"image_aspect_ratio\" in gen_kwargs.keys() and \"image_aspect_ratio\" not in self._config.__dict__:\n                # here we should pop it out of gen_kwargs so that it doesn't get passed to the model for next step of generation\n                self._config.image_aspect_ratio = gen_kwargs.pop(\"image_aspect_ratio\")\n                eval_logger.info(f\"Setting image aspect ratio: {self._config.image_aspect_ratio}\")\n            # encode, pad, and truncate contexts for this batch\n            if flattened_visuals:\n                image_tensor = process_images(flattened_visuals, self._image_processor, self._config)\n                if type(image_tensor) is list:\n                    image_tensor = [_image.to(dtype=torch.float16, device=self.device) for _image in image_tensor]\n                else:\n                    image_tensor = image_tensor.to(dtype=torch.float16, device=self.device)\n            else:\n                image_tensor = None\n\n            # prompts_input = contexts[0]\n\n            question_input = []\n            for visual, context in zip(batched_visuals, contexts):\n                if image_tensor is not None and len(image_tensor) != 0 and DEFAULT_IMAGE_TOKEN not in context:\n                    \"\"\"\n                    Three senarios:\n                    1. No image, and there for, no image token should be added.\n                    2. image token is already specified in the context, so we don't need to add it.\n                    3. image token is not specified in the context and there is image inputs, so we need to add it. In this case, we add the image token at the beginning of the context and add a new line.\n                    \"\"\"\n                    image_tokens = [DEFAULT_IMAGE_TOKEN] * len(visual) if isinstance(visual, list) else [DEFAULT_IMAGE_TOKEN]\n                    image_tokens = \" \".join(image_tokens)\n                    question = image_tokens + \"\\n\" + context\n                else:\n                    question = context\n                # This is much safer for llama3, as we now have some object type in it\n                if \"llama_3\" in self.conv_template:\n                    conv = copy.deepcopy(conv_templates[self.conv_template])\n                else:\n                    conv = conv_templates[self.conv_template].copy()\n                conv.append_message(conv.roles[0], question)\n                conv.append_message(conv.roles[1], None)\n                prompt_question = conv.get_prompt()\n                question_input.append(prompt_question)\n\n            # input_ids = tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(self.device)\n            # preconfigure gen_kwargs with defaults\n            gen_kwargs[\"image_sizes\"] = [flattened_visuals[idx].size for idx in range(len(flattened_visuals))]\n            if \"max_new_tokens\" not in gen_kwargs:\n                gen_kwargs[\"max_new_tokens\"] = 1024\n            if \"temperature\" not in gen_kwargs:\n                gen_kwargs[\"temperature\"] = 0\n            if \"top_p\" not in gen_kwargs:\n                gen_kwargs[\"top_p\"] = None\n            if \"num_beams\" not in gen_kwargs:\n                gen_kwargs[\"num_beams\"] = 1\n\n            input_ids_list = [tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\") for prompt in question_input]\n            # pad_token_ids = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id\n            pad_token_ids = self.tokenizer.eos_token_id\n            input_ids = self.pad_sequence(input_ids_list, batch_first=True, padding_value=pad_token_ids).to(self.device)\n            attention_masks = input_ids.ne(pad_token_ids).to(self.device)\n            # These steps are not in LLaVA's original code, but are necessary for generation to work\n            # TODO: attention to this major generation step...\n            try:\n                self.model.eval()\n                with torch.inference_mode():\n                    cont,  vision_id_expert_tmp, mlp_id_expert = self.model.generate(\n                        input_ids,\n                        attention_mask=attention_masks,\n                        pad_token_id=pad_token_ids,\n                        images=image_tensor,\n                        image_sizes=gen_kwargs[\"image_sizes\"],\n                        do_sample=True if gen_kwargs[\"temperature\"] > 0 else False,\n                        temperature=gen_kwargs[\"temperature\"],\n                        top_p=gen_kwargs[\"top_p\"],\n                        num_beams=gen_kwargs[\"num_beams\"],\n                        max_new_tokens=gen_kwargs[\"max_new_tokens\"],\n                        use_cache=self.use_cache,\n                        return_id_experts = kwargs['return_id_experts']\n                    )\n                vision_id_expert = {}\n                if kwargs['return_id_experts'] == True:\n                    \n                    for i in vision_id_expert_tmp.keys():\n                        try:\n                            if int(i) in kwargs['layers_expert_selection'] or len(kwargs['layers_expert_selection']) == 0 :\n                                vision_id_expert[i] = vision_id_expert_tmp[i].squeeze().tolist()\n                        except Exception as e:\n                            print(f\"Error processing vision expert selection at layer {i}: {e}\")\n                            break\n                    try:\n                        mlp_id_expert =  mlp_id_expert.squeeze().tolist()\n                    except Exception as e:\n                        print(f\"Error processing mlp expert selection at layer {i}: {e}\")\n                        break\n                    \n                text_outputs = self.tokenizer.batch_decode(cont, skip_special_tokens=True)\n                if isinstance(text_outputs, list):\n                    for i in range(len(text_outputs)):\n                        if \"<|end|>\" in text_outputs[i]:\n                        \n                            text_outputs[i] = text_outputs[i].replace(\"<|end|>\", \"\").strip()\n                        if \"assistant\\n\" in text_outputs[i]:\n                            text_outputs[i] = text_outputs[i].replace(\"assistant\\n\", \"\").strip()\n                    if text_outputs[i] == '':\n                        print(\"\\n\")\n                        print(question_input[i])\n                elif isinstance(text_outputs, str):\n                    text_outputs = text_outputs.replace(\"<|end|>\", \"\").strip()\n                    text_outputs = text_outputs.replace(\"assistant\\n\", \"\").strip()\n                else:\n                    raise TypeError(f\"Unexpected type for text_outputs: {type(text_outputs)}\")\n                # print(text_outputs)\n            except Exception as e:\n                raise e\n                eval_logger.error(f\"Error {e} in generating\")\n                cont = \"\"\n                text_outputs = [\"\"]\n\n            # cont_toks_list = cont.tolist()\n            # for cont_toks, context in zip(cont_toks_list, contexts):\n            # discard context + left-padding toks if using causal decoder-only LMM\n            # if self.truncate_context:\n            #     cont_toks = cont_toks[input_ids.shape[1] :]\n            # use secondary stop seqs to cut off should-have-been-stopped content post-hoc\n            # if self.truncate_context:\n            #     for term in until:\n            #         if len(term) > 0:\n            #             # ignore '' separator,\n            #             # for seq2seq case where self.tok_decode(self.eot_token_id) = ''\n            #             text_outputs = text_outputs.split(term)[0]\n            res.extend(text_outputs)\n            vision_id_experts.append(vision_id_expert)\n            mlp_id_experts.append(mlp_id_expert)\n            self.cache_hook.add_partial(\"generate_until\", (context, gen_kwargs), text_outputs)\n            \n            pbar.update(1)\n            # reorder this group of results back to original unsorted form\n        res = re_ords.get_original(res)\n        vision_id_experts = re_ords.get_original(vision_id_experts)\n        mlp_id_experts = re_ords.get_original(mlp_id_experts)\n        pbar.close()\n        return res, vision_id_experts, mlp_id_experts"}
{"type": "source_file", "path": "evaluate/lmms_eval/api/task.py", "content": "import abc\nimport ast\nimport itertools\nimport json\n\nimport os\nimport random\nimport re\nimport shutil\nimport subprocess\nfrom collections.abc import Callable\nfrom dataclasses import dataclass, field, asdict\nfrom glob import glob\nfrom typing import Any, List, Union\n\nimport datasets\nimport numpy as np\nfrom PIL import ImageFile\nfrom datasets import DownloadConfig, Image, Sequence\nfrom huggingface_hub import snapshot_download\nfrom tenacity import retry, stop_after_attempt, wait_fixed, stop_after_delay\nfrom tqdm import tqdm\n\nfrom accelerate import Accelerator\nfrom lmms_eval import utils\nfrom lmms_eval.api import samplers\nfrom lmms_eval.api.instance import Instance\nfrom lmms_eval.api.registry import (\n    AGGREGATION_REGISTRY,\n    DEFAULT_METRIC_REGISTRY,\n    METRIC_REGISTRY,\n    OUTPUT_TYPE_REGISTRY,\n    get_aggregation,\n    get_metric,\n    get_metric_aggregation,\n    is_higher_better,\n)\nfrom lmms_eval.filters import build_filter_ensemble\n\nfrom loguru import logger as eval_logger\n\n# HuggingfaceM4/NoCaps contains truncated image in test split\n# Include this inside code block to avoid error\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nALL_OUTPUT_TYPES = [\n    \"loglikelihood\",\n    \"multiple_choice\",\n    \"generate_until\",\n]\n\n\n@dataclass\nclass TaskConfig(dict):\n    # task naming/registry\n    task: str = None\n    task_alias: str = None\n    group: Union[str, list] = None\n    group_alias: Union[str, list] = None\n    # HF dataset options.\n    # which dataset to use,\n    # and what splits for what purpose\n    dataset_path: str = None\n    dataset_name: str = None\n    dataset_kwargs: dict = None\n    training_split: str = None\n    validation_split: str = None\n    test_split: str = None\n    fewshot_split: str = None  # TODO: assert that this not None if num_fewshot > 0. (?) assert if this is same split as one evaling (?)\n    # formatting / prompting options.\n    # see docs/advanced_task_guide.md for more info\n    process_docs: Callable = None\n    doc_to_visual: Union[Callable, str] = None\n    doc_to_text: Union[Callable, str] = None\n    doc_to_target: Union[Callable, str] = None\n    doc_to_choice: Union[Callable, str, dict, list] = None\n    process_results: Union[Callable, str] = None\n    use_prompt: str = None\n    description: str = \"\"\n    target_delimiter: str = \" \"\n    fewshot_delimiter: str = \"\\n\\n\"\n    fewshot_config: dict = None\n    # runtime configuration options\n    num_fewshot: int = None\n    # scoring options\n    metric_list: list = None\n    output_type: str = \"generate_until\"\n    generation_kwargs: dict = None\n    repeats: int = 1\n    filter_list: Union[str, list] = None\n    should_decontaminate: bool = False\n    doc_to_decontamination_query: str = None\n\n    metadata: Union[str, list] = None  # by default, not used in the code. allows for users to pass arbitrary info to tasks\n\n    model_specific_prompt_kwargs: dict = None\n    model_specific_generation_kwargs: dict = None\n    model_specific_target_kwargs: dict = None\n\n    def __post_init__(self) -> None:\n        if self.dataset_path and os.path.exists(os.path.dirname(self.dataset_path)):\n            import inspect\n            from importlib import import_module\n\n            # self.dataset_path = inspect.getfile(import_module(self.dataset_path))\n\n        if self.generation_kwargs is not None:\n            if self.output_type != \"generate_until\":\n                eval_logger.warning(f\"[{self.task}] passed `generation_kwargs`, but not using `output_type: generate_until`!\")\n                assert self.output_type != \"generate_until\"\n\n            if \"temperature\" in self.generation_kwargs:\n                self.generation_kwargs[\"temperature\"] = float(self.generation_kwargs[\"temperature\"])\n\n            if \"until\" not in self.generation_kwargs:\n                self.generation_kwargs[\"until\"] = [self.fewshot_delimiter]\n        else:\n            if self.output_type == \"generate_until\":\n                # ensure that we greedily generate in absence of explicit arguments otherwise\n                self.generation_kwargs = {\n                    \"until\": None if self.fewshot_delimiter is None else [self.fewshot_delimiter],\n                    \"do_sample\": False,\n                }\n\n        # TODO: how to make TaskConfigs be de- and re-serializable, even when using the !function constructor?\n\n    def __getitem__(self, item):\n        return getattr(self, item)\n\n    def __setitem__(self, item, value):\n        return setattr(self, item, value)\n\n    def to_dict(self):\n        \"\"\"dumps the current config as a dictionary object, as a printable format.\n        null fields will not be printed.\n        Used for dumping results alongside full task configuration\n\n        :return: dict\n            A printable dictionary version of the TaskConfig object.\n\n        # TODO: should any default value in the TaskConfig not be printed?\n        \"\"\"\n        cfg_dict = asdict(self)\n        # remove values that are `None`\n        for k, v in list(cfg_dict.items()):\n            if v is None:\n                cfg_dict.pop(k)\n            elif isinstance(v, Callable):\n                # TODO: this should handle Promptsource template objects as a separate case?\n                cfg_dict[k] = str(v)\n        return cfg_dict\n\n\nclass Task(abc.ABC):\n    \"\"\"A task represents an entire benchmark including its dataset, problems,\n    answers, and evaluation methods. See BoolQ for a simple example implementation\n\n    A `doc` can be any python object which represents one instance of evaluation.\n    This is usually a dictionary e.g.\n        {\"question\": ..., \"answer\": ...} or\n        {\"question\": ..., question, answer)\n    \"\"\"\n\n    VERSION = None\n\n    # The name of the `Task` benchmark as denoted in the HuggingFace datasets Hub\n    # or a path to a custom `datasets` loading script.\n    DATASET_PATH: str = None\n\n    # The name of a subset within `DATASET_PATH`.\n    DATASET_NAME: str = None\n\n    OUTPUT_TYPE: str = None\n\n    def __init__(\n        self,\n        data_dir=None,\n        cache_dir=None,\n        download_mode=None,\n        config=None,\n    ) -> None:\n        \"\"\"\n        :param data_dir: str\n            Stores the path to a local folder containing the `Task`'s data files.\n            Use this to specify the path to manually downloaded data (usually when\n            the dataset is not publicly accessible).\n        :param cache_dir: str\n            The directory to read/write the `Task` dataset. This follows the\n            HuggingFace `datasets` API with the default cache directory located at:\n                `~/.cache/huggingface/datasets`\n            NOTE: You can change the cache location globally for a given process\n            to another directory:\n                `export HF_DATASETS_CACHE=\"/path/to/another/directory\"`\n        :param download_mode: datasets.DownloadMode\n            How to treat pre-existing `Task` downloads and data.\n            - `datasets.DownloadMode.REUSE_DATASET_IF_EXISTS`\n                Reuse download and reuse dataset.\n            - `datasets.DownloadMode.REUSE_CACHE_IF_EXISTS`\n                Reuse download with fresh dataset.\n            - `datasets.DownloadMode.FORCE_REDOWNLOAD`\n                Fresh download and fresh dataset.\n        \"\"\"\n        self.download(data_dir, cache_dir, download_mode)\n        self._training_docs = None\n        self._fewshot_docs = None\n        self._instances = None\n\n        self._config = TaskConfig({**config}) if config else TaskConfig()\n\n        self._filters = [build_filter_ensemble(\"none\", [[\"take_first\", None]])]\n\n    def download(self, data_dir=None, cache_dir=None, download_mode=None) -> None:\n        \"\"\"Downloads and returns the task dataset.\n        Override this method to download the dataset from a custom API.\n\n        :param data_dir: str\n            Stores the path to a local folder containing the `Task`'s data files.\n            Use this to specify the path to manually downloaded data (usually when\n            the dataset is not publicly accessible).\n        :param cache_dir: str\n            The directory to read/write the `Task` dataset. This follows the\n            HuggingFace `datasets` API with the default cache directory located at:\n                `~/.cache/huggingface/datasets`\n            NOTE: You can change the cache location globally for a given process\n            by setting the shell environment variable, `HF_DATASETS_CACHE`,\n            to another directory:\n                `export HF_DATASETS_CACHE=\"/path/to/another/directory\"`\n        :param download_mode: datasets.DownloadMode\n            How to treat pre-existing `Task` downloads and data.\n            - `datasets.DownloadMode.REUSE_DATASET_IF_EXISTS`\n                Reuse download and reuse dataset.\n            - `datasets.DownloadMode.REUSE_CACHE_IF_EXISTS`\n                Reuse download with fresh dataset.\n            - `datasets.DownloadMode.FORCE_REDOWNLOAD`\n                Fresh download and fresh dataset.\n        \"\"\"\n        self.dataset = datasets.load_dataset(\n            path=self.DATASET_PATH,\n            name=self.DATASET_NAME,\n            data_dir=data_dir,\n            cache_dir=cache_dir,\n            download_mode=download_mode,\n        )\n        self.dataset_no_image = datasets.load_dataset(\n            path=self.DATASET_PATH,\n            name=self.DATASET_NAME,\n            data_dir=data_dir,\n            cache_dir=cache_dir,\n            download_mode=download_mode,\n        )\n        for doc_name in self.dataset_no_image:\n            remove_cols = []\n            features = self.dataset_no_image[doc_name].features\n            # If it is an Image instance or a Sequence of Image instance. Remove it\n            for feature in features:\n                if isinstance(features[feature], Image):\n                    remove_cols.append(feature)\n                elif isinstance(features[feature], Sequence) and isinstance(features[feature].feature, Image):\n                    remove_cols.append(feature)\n            for remove_col in remove_cols:\n                self.dataset_no_image[doc_name] = self.dataset_no_image[doc_name].remove_columns(remove_col)\n\n    @property\n    def config(self):\n        \"\"\"Returns the TaskConfig associated with this class.\"\"\"\n        return self._config\n\n    @abc.abstractmethod\n    def has_training_docs(self):\n        \"\"\"Whether the task has a training set\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def has_validation_docs(self):\n        \"\"\"Whether the task has a validation set\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def has_test_docs(self):\n        \"\"\"Whether the task has a test set\"\"\"\n        pass\n\n    def training_docs(self):\n        \"\"\"\n        :return: Iterable[obj]\n            A iterable of any object, that doc_to_text can handle\n        \"\"\"\n        return []\n\n    def validation_docs(self):\n        \"\"\"\n        :return: Iterable[obj]\n            A iterable of any object, that doc_to_text can handle\n        \"\"\"\n        return []\n\n    def test_docs(self):\n        \"\"\"\n        :return: Iterable[obj]\n            A iterable of any object, that doc_to_text can handle\n        \"\"\"\n        return []\n\n    def fewshot_docs(self):\n        \"\"\"\n        :return: Iterable[obj]\n            A iterable of any object, that doc_to_text can handle\n        \"\"\"\n        if self.has_training_docs():\n            return self.training_docs()\n        elif self.has_validation_docs():\n            return self.validation_docs()\n        else:\n            if self.config.num_fewshot is not None:\n                eval_logger.warning(\"has_training_docs and has_validation_docs are False\" \", using test_docs as fewshot_docs but this is not recommended.\")\n            return self.test_docs()\n\n    def _process_doc(self, doc):\n        \"\"\"\n        Override this to process (detokenize, strip, replace, etc.) individual\n        documents. This can be used in a map over documents of a data split.\n        E.g. `map(self._process_doc, self.dataset[\"validation\"])`\n\n        :return: dict\n            The processed version of the specified `doc`.\n        \"\"\"\n        return doc\n\n    @property\n    def instances(self):\n        \"\"\"After calling `task.build_all_requests()`, tasks\n        maintain a list of the dataset instances which will be evaluated.\n        \"\"\"\n        return self._instances\n\n    def fewshot_examples(self, k, rnd):\n        if self._training_docs is None:\n            self._training_docs = list(self.training_docs())\n\n        return rnd.sample(self._training_docs, k)\n\n    def doc_to_decontamination_query(self, doc) -> None:\n        print(\"Override doc_to_decontamination_query with document specific decontamination query.\")\n        assert False\n\n    @abc.abstractmethod\n    def doc_to_text(self, doc):\n        pass\n\n    @abc.abstractmethod\n    def doc_to_target(self, doc):\n        pass\n\n    # @profile\n    def build_all_requests(self, limit=None, rank=None, world_size=None) -> None:\n        \"\"\"Build a set of Instances for a task, and store them in task.instances\"\"\"\n        if self.has_test_docs():\n            docs = self.test_docs()\n            split = self.config.test_split\n        elif self.has_validation_docs():\n            docs = self.validation_docs()\n            split = self.config.validation_split\n        else:\n            assert False, f\"Task dataset (path={self.DATASET_PATH}, name={self.DATASET_NAME}) must have valid or test docs!\"\n\n        eval_logger.info(f\"Building contexts for task {self.CONFIG.task} on rank {rank}...\")\n        instances = []\n        doc_id_iterator = utils.create_iterator([i for i in range(len(docs))], rank, world_size, limit)\n        doc_id_iterator, doc_id_iterator_counting = itertools.tee(doc_id_iterator)\n        total_docs = sum(1 for _ in doc_id_iterator_counting)\n        pbar = tqdm(total=total_docs, desc=f\"Building context\", disable=(rank != 0))\n        for doc_id in doc_id_iterator:\n            # sample fewshot context #TODO: need to offset doc_id by rank now!\n            fewshot_ctx = self.fewshot_context(doc_id, 0 if self.config.num_fewshot is None else self.config.num_fewshot, self.config.training_split if self.has_training_docs() else split)\n\n            # TODO: we should override self.config.repeats if doing greedy gen so users don't waste time+compute\n            inst = self.construct_requests(doc_id=doc_id, ctx=fewshot_ctx, metadata=(self.config[\"task\"], doc_id, self.config.repeats), split=split)\n\n            if not isinstance(inst, list):\n                inst = [inst]\n\n            instances.extend(inst)\n            pbar.update(1)\n\n        pbar.close()\n        self._instances = instances\n        assert len(self._instances) != 0, \"task.build_requests() did not find any docs!\"\n\n    @abc.abstractmethod\n    def construct_requests(self, doc_id, ctx, **kwargs):\n        \"\"\"Uses RequestFactory to construct Requests and returns an iterable of\n        Requests which will be sent to the LMM.\n\n        :param doc_id: int\n            The index of a document within `self.test_docs()` or `self.validation_docs()`,\n            whichever is the main split used.\n        :param ctx: str\n            The context string, generated by fewshot_context. This includes the natural\n            language description, as well as the few shot examples, and the question\n            part of the document for `doc`.\n        :param repeats: int\n        TODO: update this docstring\n            The number of times each instance in a dataset is inferred on. Defaults to 1,\n            can be increased for techniques like majority voting.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def process_results(self, doc, results):\n        \"\"\"Take a single document and the LMM results and evaluates, returning a\n        dict where keys are the names of submetrics and values are the values of\n        the metric for that one document\n\n        :param doc:\n            The document as returned from training_docs, validation_docs, or test_docs.\n        :param results:\n            The results of the requests created in construct_requests.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def aggregation(self):\n        \"\"\"\n        :returns: {str: [metric_score] -> float}\n            A dictionary where keys are the names of submetrics and values are\n            functions that aggregate a list of metric scores\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def higher_is_better(self):\n        \"\"\"\n        :returns: {str: bool}\n            A dictionary where keys are the names of submetrics and values are\n            whether a higher value of the submetric is better\n        \"\"\"\n        pass\n\n    @classmethod\n    def count_bytes(cls, doc):\n        \"\"\"Used for byte-level perplexity metrics in rolling loglikelihood\"\"\"\n        return len(doc.encode(\"utf-8\"))\n\n    @utils.positional_deprecated\n    def fewshot_context(\n        self,\n        doc_id,\n        num_fewshot,\n        split,\n        rnd=random.Random(1234),\n        description=None,\n    ):\n        \"\"\"Returns a fewshot context string that is made up of a prepended description\n        (if provided), the `num_fewshot` number of examples, and an appended prompt example.\n\n        :param doc_id: int\n            The document id as returned from training_docs, validation_docs, or test_docs.\n        :param num_fewshot: int\n            The number of fewshot examples to provide in the returned context string.\n        :param split: str\n            The split of the document to retrieve from the dataset\n        :param rnd: random.Random\n            The pseudo-random number generator used to randomly sample examples.\n            WARNING: This is currently a required arg although it's optionalized with a default `None`.\n        :param description: str\n            The task's description that will be prepended to the fewshot examples.\n        :returns: str\n            The fewshot context.\n        \"\"\"\n        assert rnd is not None, \"A `random.Random` generator argument must be provided to `rnd`\"\n\n        description = description if description else \"\"\n        doc = self.dataset_no_image[split][doc_id]\n\n        if num_fewshot == 0:\n            labeled_examples = \"\"\n        else:\n            # for sets with no training docs, draw from other set *but ensure no overlap with current doc*\n            if self.has_training_docs():\n                fewshotex = self.fewshot_examples(k=num_fewshot, rnd=rnd)\n            else:\n                if self._fewshot_docs is None:\n                    self._fewshot_docs = list(self.validation_docs() if self.has_validation_docs() else self.test_docs())\n\n                fewshotex = rnd.sample(self._fewshot_docs, num_fewshot + 1)\n\n                # get rid of the doc that's the one we're evaluating, if it's in the fewshot\n                fewshotex = [x for x in fewshotex if x != doc][:num_fewshot]\n\n            labeled_examples = \"\\n\\n\".join([self.doc_to_text(doc) + self.doc_to_target(doc) for doc in fewshotex]) + \"\\n\\n\"\n\n        example = self.doc_to_text(doc)\n        return description + labeled_examples + example\n\n    def apply_filters(self):\n        if hasattr(self, \"_filters\"):\n            for f in self._filters:\n                f.apply(self._instances, None)\n        else:\n            eval_logger.warning(\"No filter defined, passing through instances\")\n            return self._instances\n\n    def dump_config(self) -> dict:\n        \"\"\"Returns a dictionary representing the task's config.\n\n        :returns: str\n            The fewshot context.\n        \"\"\"\n        # TODO: this should only return the overrides applied to a non-YAML task's configuration.\n        # (num_fewshot)\n        return self.config.to_dict()\n\n    def override_metric(self, metric_name: str) -> None:\n        \"\"\"\n        Override the default metrics used for evaluation with custom metrics.\n\n        Parameters:\n        - metric_name (str): The name of the custom metric to override. Should be registered in api.metrics.\n        \"\"\"\n        (\n            self._metric_fn_list,\n            self._aggregation_list,\n            self._metric_fn_kwargs,\n            self._higher_is_better,\n        ) = ({}, {}, {}, {})\n        self._metric_fn_list[metric_name] = get_metric(metric_name)\n        self._aggregation_list[metric_name] = get_metric_aggregation(metric_name)\n        self._higher_is_better[metric_name] = is_higher_better(metric_name)\n        self._metric_fn_kwargs[metric_name] = {}\n        if not isinstance(self, ConfigurableTask):\n            self.process_results = lambda x, y: {metric_name: get_metric(metric_name)}\n            self.aggregation = lambda: {metric_name: get_metric_aggregation(metric_name)}\n        setattr(self._config, \"metric_list\", [{\"metric\": metric_name}])\n        setattr(self._config, \"process_results\", None)\n\n\nclass ConfigurableTask(Task):\n    VERSION = \"Yaml\"\n    OUTPUT_TYPE = None\n    CONFIG = None\n\n    def __init__(self, model_name) -> None:  # TODO no super() call here\n        # Get pre-configured attributes\n        self._config = self.CONFIG\n        # different model requires different prompt, we have to take those into account.\n\n        self.model_name = model_name\n        self._prepare_model_specific_config()\n\n        assert self.config.output_type in ALL_OUTPUT_TYPES\n        self.OUTPUT_TYPE = self.config.output_type\n\n        self.DATASET_PATH = self.config.dataset_path\n\n        if self.config.dataset_name is not None:\n            self.DATASET_NAME = self.config.dataset_name\n\n        self._prepare_metric_and_aggregation()\n\n        self.download(self.config.dataset_kwargs)\n        self._training_docs = None\n        self._fewshot_docs = None\n\n        if self.config.filter_list is not None:\n            self._filters = []\n            for filter_config in self.config.filter_list:\n                for filter_pipeline in filter_config:\n                    filter_name = filter_config[\"name\"]\n                    filter_functions = filter_config[\"filter\"]\n                    components = []\n                    for function in filter_functions:\n                        kwargs = {key: function[key] for key in function if key != \"function\"}\n                        components.append([function[\"function\"], kwargs])\n                    filter_pipeline = build_filter_ensemble(filter_name, components)\n                self._filters.append(filter_pipeline)\n        else:\n            self._filters = [build_filter_ensemble(\"none\", [[\"take_first\", None]])]\n        if self.config.fewshot_config is not None:\n            self.sampler = samplers.get_sampler(self.config.fewshot_config.get(\"sampler\", \"default\") if self.config.fewshot_config else \"default\")(list(self.fewshot_docs()), self, rnd=random.Random(1234))\n\n        if self.has_test_docs():\n            self.task_docs = self.test_docs()\n        elif self.has_validation_docs():\n            self.task_docs = self.validation_docs()\n        else:\n            assert False, f\"Task dataset (path={self.DATASET_PATH}, name={self.DATASET_NAME}) must have valid or test docs!\"\n\n        # Test One Doc\n        self.features = list(self.task_docs.features.keys())\n        self.multiple_input = 0\n        self.multiple_target = 0\n        test_doc = self.task_docs[0]\n        test_text = self.doc_to_text(test_doc)\n        test_target = self.doc_to_target(test_doc)\n\n        if self.config.doc_to_choice is not None:\n            test_choice = self.doc_to_choice(test_doc)\n            if type(test_choice) is not list:\n                eval_logger.error(\"doc_to_choice must return list\")\n            else:\n                num_choice = len(test_choice)\n\n            if type(test_text) is int:\n                self.multiple_input = num_choice\n        else:\n            test_choice = None\n\n        if type(test_target) is list:\n            self.multiple_target = len(test_target)\n        else:\n            if (type(test_target) is int) and (test_choice is not None):\n                test_target = test_choice[test_target]\n            else:\n                test_target = str(test_target)\n\n        if test_choice is not None:\n            check_choices = test_choice\n        else:\n            check_choices = [test_target]\n        if self.config.doc_to_choice is not None:\n            for choice in check_choices:\n                choice_has_whitespace = True if choice[0].isspace() else False\n                delimiter_has_whitespace = True if self.config.target_delimiter.rstrip() != self.config.target_delimiter else False\n\n                if delimiter_has_whitespace and choice_has_whitespace:\n                    eval_logger.warning(f'Both target_delimiter and target choice: \"{choice}\" have whitespace')\n                elif (not delimiter_has_whitespace) and (not choice_has_whitespace):\n                    eval_logger.warning(f'Both target_delimiter \"{self.config.target_delimiter}\" and target choice: \"{choice}\" do not have whitespace, ignore if the language you are evaluating on does not require/use whitespace')\n\n    def _prepare_model_specific_config(self):\n        self.model_specific_prompt_kwargs = self.config.model_specific_prompt_kwargs\n        if self.model_specific_prompt_kwargs is not None:\n            if self.model_name in self.model_specific_prompt_kwargs:\n                self.model_specific_prompt_kwargs = self.model_specific_prompt_kwargs[self.model_name]\n            else:\n                self.model_specific_prompt_kwargs = self.model_specific_prompt_kwargs.get(\"default\", None)\n\n        self.model_specific_target_kwargs = self.config.model_specific_target_kwargs\n        if self.model_specific_target_kwargs is not None:\n            if self.model_name in self.model_specific_target_kwargs:\n                self.model_specific_target_kwargs = self.model_specific_target_kwargs[self.model_name]\n            else:\n                self.model_specific_target_kwargs = self.model_specific_target_kwargs.get(\"default\", None)\n        self.model_specific_generation_kwargs = self.config.model_specific_generation_kwargs\n        if self.model_specific_generation_kwargs is not None:\n            if self.model_name in self.model_specific_generation_kwargs:\n                self.model_specific_generation_kwargs = self.model_specific_generation_kwargs[self.model_name]\n            else:\n                self.model_specific_generation_kwargs = self.model_specific_generation_kwargs.get(\"default\", {})\n\n            self.config.generation_kwargs.update(self.model_specific_generation_kwargs)\n\n    def _prepare_metric_and_aggregation(self):\n        self._metric_fn_list = {}\n        self._metric_fn_kwargs = {}\n        self._aggregation_list = {}\n        self._higher_is_better = {}\n\n        if self.config.metric_list is None:\n            # TODO: handle this in TaskConfig.__post_init__ ?\n            _metric_list = DEFAULT_METRIC_REGISTRY[self.config.output_type]\n\n            for metric_name in _metric_list:\n                self._metric_fn_list[metric_name] = METRIC_REGISTRY[metric_name]\n                self._metric_fn_kwargs[metric_name] = {}\n                self._aggregation_list[metric_name] = get_metric_aggregation(metric_name)\n                self._higher_is_better[metric_name] = is_higher_better(metric_name)\n        else:\n            for metric_config in self.config.metric_list:\n                assert \"metric\" in metric_config\n                metric_name = metric_config[\"metric\"]\n                kwargs = {key: metric_config[key] for key in metric_config if key not in [\"metric\", \"aggregation\", \"higher_is_better\"]}\n\n                if self.config.process_results is not None:\n                    self._metric_fn_list[metric_name] = None\n                    self._metric_fn_kwargs[metric_name] = {}\n                elif callable(metric_name):\n                    metric_fn = metric_name.__call__\n                    metric_name = metric_name.__name__\n                    self._metric_fn_list[metric_name] = metric_fn\n                    self._metric_fn_kwargs[metric_name] = kwargs\n                else:\n                    self._metric_fn_list[metric_name] = METRIC_REGISTRY[metric_name]\n                    self._metric_fn_kwargs[metric_name] = kwargs\n\n                if \"aggregation\" in metric_config:\n                    agg_name = metric_config[\"aggregation\"]\n                    if type(agg_name) == str:\n                        self._aggregation_list[metric_name] = get_aggregation(agg_name)\n                    elif callable(agg_name):\n                        self._aggregation_list[metric_name] = metric_config[\"aggregation\"]\n                else:\n                    INV_AGG_REGISTRY = {v: k for k, v in AGGREGATION_REGISTRY.items()}\n                    metric_agg = get_metric_aggregation(metric_name)\n                    eval_logger.warning(f\"[Task: {self._config.task}] metric {metric_name} is defined, but aggregation is not. \" f\"using default \" f\"aggregation={INV_AGG_REGISTRY[metric_agg]}\")\n                    self._aggregation_list[metric_name] = metric_agg\n\n                if \"higher_is_better\" in metric_config:\n                    self._higher_is_better[metric_name] = metric_config[\"higher_is_better\"]\n                else:\n                    eval_logger.warning(f\"[Task: {self._config.task}] metric {metric_name} is defined, but higher_is_better is not. \" f\"using default \" f\"higher_is_better={is_higher_better(metric_name)}\")\n                    self._higher_is_better[metric_name] = is_higher_better(metric_name)\n\n    @retry(stop=(stop_after_attempt(5) | stop_after_delay(60)), wait=wait_fixed(2))\n    def download(self, dataset_kwargs=None) -> None:\n        # If the dataset is a video dataset,\n        # Recursively search whether their is a zip and unzip it to the huggingface home\n        download_config = DownloadConfig()\n        download_config.max_retries = dataset_kwargs.get(\"max_retries\", 10) if dataset_kwargs is not None else 10\n        download_config.num_proc = dataset_kwargs.get(\"num_proc\", 8) if dataset_kwargs is not None else 8\n        download_config.local_files_only = dataset_kwargs.get(\"local_files_only\", False) if dataset_kwargs is not None else False\n        if dataset_kwargs is not None:\n            if \"From_YouTube\" in dataset_kwargs:\n\n                def _download_from_youtube(path):\n                    try:\n                        for video in tqdm(self.all_dataset[split]):\n                            video_id = video[\"videoID\"]\n                            target_path = os.path.join(path, f\"{video_id}.mp4\")\n                            assert shutil.which(\"yt-dlp\") is not None, \"yt-dlp must be installed and available in the system's PATH\"\n                            command = f\"yt-dlp -o {target_path} -f mp4 https://www.youtube.com/watch?v={video_id}\"\n                            subprocess.run(command, shell=True)\n                        with open(os.path.join(cache_path, f\"{task}_download_status.json\"), \"w\") as f:\n                            f.write(json.dumps({task: \"downloaded\"}))\n                    except Exception as e:\n                        eval_logger.error(f\"Error while downloading {task} data: {e}\")\n                        with open(os.path.join(cache_path, f\"{task}_download_status.json\"), \"w\") as f:\n                            f.write(json.dumps({task: \"not downloaded\"}))\n\n                hf_home = os.getenv(\"HF_HOME\", \"~/.cache/huggingface/\")\n                accelerator = Accelerator()\n                if accelerator.is_main_process:\n                    dataset_kwargs.pop(\"From_YouTube\")\n                    self.all_dataset = datasets.load_dataset(\n                        path=self.DATASET_PATH,\n                        name=self.DATASET_NAME,\n                        download_mode=datasets.DownloadMode.REUSE_DATASET_IF_EXISTS,\n                        **dataset_kwargs if dataset_kwargs is not None else {},\n                    )\n                    dataset_kwargs[\"From_YouTube\"] = True\n                    cache_path = snapshot_download(repo_id=self.DATASET_PATH, repo_type=\"dataset\")  # download_parquet\n                    split = vars(self.config)[\"test_split\"]\n                    task = vars(self.config)[\"task\"]\n\n                    video_path = os.path.join(hf_home, task)\n                    if os.path.exists(os.path.join(cache_path, f\"{task}_download_status.json\")):\n                        download_status = json.load(open(os.path.join(cache_path, f\"{task}_download_status.json\"), \"r\"))\n                        if download_status[task] == \"downloaded\":\n                            eval_logger.info(f\"Data for {task} already download!\")\n                        else:\n                            eval_logger.info(f\"Start downloading YouTube data to {video_path}...\")\n                            _download_from_youtube(video_path)\n                    else:\n                        eval_logger.info(f\"Start downloading YouTube data to {video_path}...\")\n                        _download_from_youtube(video_path)\n\n                accelerator.wait_for_everyone()\n                if \"builder_script\" in dataset_kwargs:\n                    builder_script = dataset_kwargs[\"builder_script\"]\n                    self.DATASET_PATH = os.path.join(cache_path, builder_script)\n                    dataset_kwargs.pop(\"builder_script\")\n\n                downloaded_video_ids = [i.split(\".mp4\")[0] for i in os.listdir(os.path.expanduser(video_path)) if i.endswith(\".mp4\")]\n                # Filtered the existing dataset with the downloaded video ids\n                self.dataset = datasets.DatasetDict({split: self.all_dataset[split].filter(lambda x: x[\"videoID\"] in downloaded_video_ids)})\n\n                self.dataset_no_image = self.dataset\n                dataset_kwargs.pop(\"From_YouTube\")\n                return\n\n            if \"video\" in dataset_kwargs and dataset_kwargs[\"video\"]:\n                hf_home = os.getenv(\"HF_HOME\", \"~/.cache/huggingface/\")\n                cache_dir = dataset_kwargs[\"cache_dir\"]\n                cache_dir = os.path.join(hf_home, cache_dir)\n                accelerator = Accelerator()\n                if accelerator.is_main_process:\n                    force_download = dataset_kwargs.get(\"force_download\", False)\n                    force_unzip = dataset_kwargs.get(\"force_unzip\", False)\n                    cache_path = snapshot_download(repo_id=self.DATASET_PATH, repo_type=\"dataset\", force_download=force_download, etag_timeout=60)\n                    zip_files = glob(os.path.join(cache_path, \"**/*.zip\"), recursive=True)\n                    tar_files = glob(os.path.join(cache_path, \"**/*.tar*\"), recursive=True)\n\n                    def unzip_video_data(zip_file):\n                        import zipfile\n\n                        with zipfile.ZipFile(zip_file, \"r\") as zip_ref:\n                            zip_ref.extractall(cache_dir)\n                            eval_logger.info(f\"Extracted all files from {zip_file} to {cache_dir}\")\n\n                    def untar_video_data(tar_file):\n                        import tarfile\n\n                        with tarfile.open(tar_file, \"r\") as tar_ref:\n                            tar_ref.extractall(cache_dir)\n                            eval_logger.info(f\"Extracted all files from {tar_file} to {cache_dir}\")\n\n                    def concat_tar_parts(tar_parts, output_tar):\n                        with open(output_tar, \"wb\") as out_tar:\n                            from tqdm import tqdm\n\n                            for part in tqdm(sorted(tar_parts)):\n                                with open(part, \"rb\") as part_file:\n                                    out_tar.write(part_file.read())\n                        eval_logger.info(f\"Concatenated parts {tar_parts} into {output_tar}\")\n\n                    # Unzip zip files if needed\n                    if force_unzip or (not os.path.exists(cache_dir) and len(zip_files) > 0):\n                        for zip_file in zip_files:\n                            unzip_video_data(zip_file)\n\n                    # Concatenate and extract tar files if needed\n                    if force_unzip or (not os.path.exists(cache_dir) and len(tar_files) > 0):\n                        tar_parts_dict = {}\n\n                        # Group tar parts together\n                        for tar_file in tar_files:\n                            base_name = tar_file.split(\".tar\")[0]\n                            if base_name not in tar_parts_dict:\n                                tar_parts_dict[base_name] = []\n                            tar_parts_dict[base_name].append(tar_file)\n\n                        # Concatenate and untar split parts\n                        for base_name, parts in tar_parts_dict.items():\n                            eval_logger.info(f\"Extracting following tar files: {parts}\")\n                            output_tar = base_name + \".tar\"\n                            if not os.path.exists(output_tar):\n                                eval_logger.info(f\"Start concatenating tar files\")\n\n                                concat_tar_parts(parts, output_tar)\n                                eval_logger.info(f\"Finish concatenating tar files\")\n\n                            if not os.path.exists(os.path.join(cache_dir, os.path.basename(base_name))):\n                                untar_video_data(output_tar)\n\n                accelerator.wait_for_everyone()\n                dataset_kwargs.pop(\"cache_dir\")\n                dataset_kwargs.pop(\"video\")\n\n            if \"builder_script\" in dataset_kwargs:\n                builder_script = dataset_kwargs[\"builder_script\"]\n                self.DATASET_PATH = os.path.join(cache_path, builder_script)\n                dataset_kwargs.pop(\"builder_script\")\n\n            if \"force_download\" in dataset_kwargs:\n                dataset_kwargs.pop(\"force_download\")\n\n            if \"force_unzip\" in dataset_kwargs:\n                dataset_kwargs.pop(\"force_unzip\")\n\n            if \"local_files_only\" in dataset_kwargs:\n                dataset_kwargs.pop(\"local_files_only\")\n\n        self.dataset = datasets.load_dataset(\n            path=self.DATASET_PATH,\n            name=self.DATASET_NAME,\n            download_mode=datasets.DownloadMode.REUSE_DATASET_IF_EXISTS,\n            download_config=download_config,\n            **dataset_kwargs if dataset_kwargs is not None else {},\n        )\n        self.dataset_no_image = datasets.load_dataset(\n            path=self.DATASET_PATH,\n            name=self.DATASET_NAME,\n            download_mode=datasets.DownloadMode.REUSE_DATASET_IF_EXISTS,\n            download_config=download_config,\n            **dataset_kwargs if dataset_kwargs is not None else {},\n        )\n        for doc_name in self.dataset_no_image:\n            remove_cols = []\n            features = self.dataset_no_image[doc_name].features\n            # If it is an Image instance or a Sequence of Image instance. Remove it\n            for feature in features:\n                if isinstance(features[feature], Image):\n                    remove_cols.append(feature)\n                elif isinstance(features[feature], Sequence) and isinstance(features[feature].feature, Image):\n                    remove_cols.append(feature)\n            for remove_col in remove_cols:\n                self.dataset_no_image[doc_name] = self.dataset_no_image[doc_name].remove_columns(remove_col)\n\n    def has_training_docs(self) -> bool:\n        if self.config.training_split is not None:\n            return True\n        else:\n            return False\n\n    def has_validation_docs(self) -> bool:\n        if self.config.validation_split is not None:\n            return True\n        else:\n            return False\n\n    def has_test_docs(self) -> bool:\n        if self.config.test_split is not None:\n            return True\n        else:\n            return False\n\n    def training_docs(self) -> datasets.Dataset:\n        if self.has_training_docs():\n            if self.config.process_docs is not None:\n                return self.config.process_docs(self.dataset[self.config.training_split])\n            return self.dataset[self.config.training_split]\n\n    def validation_docs(self) -> datasets.Dataset:\n        if self.has_validation_docs():\n            if self.config.process_docs is not None:\n                return self.config.process_docs(self.dataset[self.config.validation_split])\n            return self.dataset[self.config.validation_split]\n\n    def test_docs(self) -> datasets.Dataset:\n        if self.has_test_docs():\n            if self.config.process_docs is not None:\n                return self.config.process_docs(self.dataset[self.config.test_split])\n            return self.dataset[self.config.test_split]\n\n    def fewshot_docs(self):\n        if self.config.fewshot_split is not None:\n            return self.dataset[self.config.fewshot_split]\n        else:\n            if (self.config.num_fewshot is not None) and (self.config.num_fewshot > 0):\n                eval_logger.warning(f\"Task '{self.config.task}': \" \"num_fewshot > 0 but fewshot_split is None. \" \"using preconfigured rule.\")\n            return super().fewshot_docs()\n\n    @utils.positional_deprecated\n    def fewshot_context(self, doc_id, num_fewshot, split):\n        \"\"\"Returns a fewshot context string that is made up of a prepended description\n        (if provided), the `num_fewshot` number of examples, and an appended prompt example.\n\n        :param doc_id: str\n            The document id as returned from training_docs, validation_docs, or test_docs.\n        :param num_fewshot: int\n            The number of fewshot examples to provide in the returned context string.\n        :returns: str\n            The fewshot context.\n        \"\"\"\n        doc = self.dataset_no_image[split][doc_id]\n        if num_fewshot == 0:\n            # always prepend the (possibly empty) task description\n            labeled_examples = self.config.description\n        else:\n            labeled_examples = self.config.description + self.sampler.get_context(doc, num_fewshot)\n        example = self.doc_to_text(doc)\n        if type(example) == str:\n            return labeled_examples + example\n        elif type(example) == list:\n            return [labeled_examples + ex for ex in example]\n        elif type(example) == int:\n            if self.config.doc_to_choice is not None:\n                choices = self.doc_to_choice(doc)\n                return labeled_examples + choices[example]\n            else:\n                return labeled_examples + str(example)\n\n    def apply_filters(self):\n        if hasattr(self, \"_filters\"):\n            for f in self._filters:\n                f.apply(self._instances, self.task_docs)\n        else:\n            eval_logger.warning(\"No filter defined, passing through instances\")\n            return self._instances\n\n    def should_decontaminate(self):\n        return self.config.should_decontaminate\n\n    def doc_to_decontamination_query(self, doc):\n        if self.config.should_decontaminate:\n            if self.config.doc_to_decontamination_query is None:\n                return self.doc_to_text(doc)\n            else:\n                doc_to_decontamination_query = self.config.doc_to_decontamination_query\n                if doc_to_decontamination_query in self.features:\n                    return doc[doc_to_decontamination_query]\n                elif callable(doc_to_decontamination_query):\n                    return doc_to_decontamination_query(doc)\n                else:\n                    return ast.literal_eval(utils.apply_template(self.config.doc_to_decontamination_query, doc))\n\n    def _process_doc(self, doc):\n        \"\"\"\n        Override this to process (detokenize, strip, replace, etc.) individual\n        documents. This can be used in a map over documents of a data split.\n        E.g. `map(self._process_doc, self.dataset[\"validation\"])`\n\n        :return: dict\n            The processed version of the specified `doc`.\n        \"\"\"\n        return doc\n\n    def doc_to_text(self, doc):\n        doc_to_text = self.config.doc_to_text\n\n        if type(doc_to_text) == int:\n            return doc_to_text\n        elif type(doc_to_text) == str:\n            if doc_to_text in self.features:\n                # if self.config.doc_to_choice is not None:\n                #     return self.doc_to_choice(doc)[doc[doc_to_text]]\n                # else:\n                return doc[doc_to_text]\n            else:\n                text_string = utils.apply_template(doc_to_text, doc)\n                if text_string.isdigit() and self._config.doc_to_choice is not None:\n                    return ast.literal_eval(text_string)\n                else:\n                    return text_string\n        elif callable(doc_to_text):\n            return (\n                doc_to_text(doc, self.model_specific_prompt_kwargs)\n                if self.model_specific_prompt_kwargs is not None\n                else doc_to_text(\n                    doc,\n                )\n            )\n        # Used when applying a Promptsource template\n        elif hasattr(doc_to_text, \"apply\"):\n            applied_prompt = doc_to_text.apply(doc)\n            if len(applied_prompt) == 2:\n                return applied_prompt[0]\n            else:\n                eval_logger.warning(\"Applied prompt returns empty string\")\n                return self.config.fewshot_delimiter\n        else:\n            print(type(doc_to_text))\n            raise TypeError\n\n    def doc_to_target(self, doc: dict) -> Union[int, str, list]:\n        doc_to_target = self.config.doc_to_target\n\n        if type(doc_to_target) == int:\n            return doc_to_target\n        elif type(doc_to_target) == str:\n            if doc_to_target in self.features:\n                # if self.config.doc_to_choice is not None:\n                #     return self.doc_to_choice(doc)[doc[doc_to_target]]\n                # else:\n                return doc[doc_to_target]\n            else:\n                target_string = utils.apply_template(doc_to_target, doc)\n                if target_string.isdigit() and self._config.doc_to_choice is not None:\n                    return ast.literal_eval(target_string)\n                elif len(target_string) >= 2 and (target_string[0] == \"[\") and (target_string[-1] == \"]\"):\n                    try:\n                        return ast.literal_eval(target_string)\n                    except (SyntaxError, ValueError):\n                        return target_string\n                else:\n                    return target_string\n        elif type(doc_to_target) == list:\n            return doc_to_target\n        elif callable(doc_to_target):\n            return doc_to_target(doc, self.model_specific_target_kwargs) if self.model_specific_target_kwargs is not None else doc_to_target(doc)\n        # Used when applying a Promptsource template\n        elif hasattr(doc_to_target, \"apply\"):\n            applied_prompt = doc_to_target.apply(doc)\n            if len(applied_prompt) == 2:\n                return applied_prompt[1]\n            else:\n                eval_logger.warning(\"Applied prompt returns empty string\")\n                return self.config.fewshot_delimiter\n        else:\n            raise TypeError\n\n    def doc_to_visual(self, doc: dict) -> Union[int, str, list]:\n        self.config.doc_to_visual\n        if type(self.config.doc_to_visual) == str:\n            assert self.config.doc_to_visual in self.features\n            # Single image. Still return a list for consistency.\n            return [doc[self.config.doc_to_visual]]\n        else:\n            assert callable(self.config.doc_to_visual)\n            return self.config.doc_to_visual(doc)\n\n    def doc_to_choice(self, doc: Any) -> List[str]:\n        if self.config.doc_to_choice is None:\n            eval_logger.error(\"doc_to_choice was called but not set in config\")\n        else:\n            doc_to_choice = self.config.doc_to_choice\n\n        if type(doc_to_choice) == str:\n            if doc_to_choice in self.features:\n                return doc[doc_to_choice]\n            else:\n                return ast.literal_eval(utils.apply_template(doc_to_choice, doc))\n        elif type(doc_to_choice) == list:\n            return doc_to_choice\n        elif type(doc_to_choice) == dict:\n            return list(doc_to_choice.values())\n        elif callable(doc_to_choice):\n            return doc_to_choice(doc)\n        elif hasattr(doc_to_choice, \"get_answer_choices_list\"):\n            return doc_to_choice.get_answer_choices_list(doc)\n        else:\n            raise TypeError\n\n    def construct_requests(self, doc_id: int, ctx: str, **kwargs) -> Union[List[Instance], Instance]:\n        split = kwargs.get(\"split\")\n        kwargs.pop(\"split\")\n        if self.OUTPUT_TYPE == \"loglikelihood\":\n            arguments = (ctx, self.doc_to_target, self.doc_to_visual, doc_id, self.config.task, split)\n        elif self.OUTPUT_TYPE == \"multiple_choice\":\n            doc = self.dataset[split][doc_id]\n            choices = self.doc_to_choice(doc)\n            target_delimiter = self.config.target_delimiter\n            if self.multiple_input:\n                # If there are multiple inputs, choices are placed in the ctx\n                cont = self.doc_to_target(doc)\n                arguments = [(ctx, f\"{target_delimiter}{cont}\", self.doc_to_visual, doc_id, self.config.task, split) for ctx in choices]\n            else:\n                # Otherwise they are placed in the continuation\n                arguments = [(ctx, f\"{target_delimiter}{cont}\", self.doc_to_visual, doc_id, self.config.task, split) for cont in choices]\n            request_list = [\n                Instance(\n                    request_type=\"loglikelihood\",\n                    # doc=doc,\n                    arguments=arg,\n                    idx=i,\n                    **kwargs,\n                )\n                for i, arg in enumerate(arguments)\n            ]\n            # TODO: we should raise a warning telling users this will at most ~2x runtime.\n            if \"acc_mutual_info\" in self._metric_fn_list.keys():\n                # if we are calculating multiple choice accuracy\n                # using mutual information instead of raw loglikelihood as metric, need unconditional lls.\n\n                # here mutual info refers to calculating\n                # log(P(choice|ctx) / P(choice)) = log(P(choice|ctx)) - log(P(choice))\n                # in other words normalizing by subtracting the unconditional logprob of each choice.\n                request_list.extend(\n                    [\n                        Instance(\n                            request_type=\"loglikelihood\",\n                            # doc=doc,\n                            arguments=(\"\", \"{}\".format(choice)),\n                            idx=i,\n                            **kwargs,\n                        )\n                        for i, choice in enumerate(choices)\n                    ]\n                )\n            return request_list\n\n        elif self.OUTPUT_TYPE == \"generate_until\":\n            arguments = (ctx, self.config.generation_kwargs, self.doc_to_visual, doc_id, self.config.task, split)\n        ins = Instance(request_type=self.OUTPUT_TYPE, arguments=arguments, idx=0, **kwargs)\n        if ins.task_name == \"mme\":\n            ins.domain = self.dataset_no_image[split][doc_id]['category']\n\n        return ins\n\n    # TODO: we add a full_docs interface here for some evaluations that needs to access the full datasets during process_results function. we may have better ways to handle this.\n    @retry(stop=(stop_after_attempt(5) | stop_after_delay(1200)), wait=wait_fixed(2))\n    def process_results(self, doc, results, full_docs=None):\n        if self.OUTPUT_TYPE == \"generate_until\":\n            results[0] = results[0].strip()\n\n        kwargs = {}\n        if full_docs is not None:\n            kwargs[\"full_docs\"] = full_docs\n        if callable(self.config.process_results):\n            return self.config.process_results(doc, results, **kwargs)\n\n        result_dict = {}\n        use_metric = list(self._metric_fn_list.keys())\n        if self.OUTPUT_TYPE == \"loglikelihood\":\n            results = results[0]\n            ll, is_greedy = results\n            return {\n                **({\"perplexity\": ll} if \"perplexity\" in use_metric else {}),\n                **({\"acc\": int(is_greedy)} if \"acc\" in use_metric else {}),\n            }\n        elif self.OUTPUT_TYPE == \"multiple_choice\":\n            lls, is_greedy = zip(*results)\n\n            # retrieve choices in List[str] form, to compute choice lengths, etc.\n            choices = self.doc_to_choice(doc)\n            completion_len = np.array([float(len(i)) for i in choices])\n\n            if 2 * len(choices) == len(lls) and \"acc_mutual_info\" in self._metric_fn_list.keys():\n                # then we are doing mutual info.\n                # this stores the \"dryrun\" / unconditional answer loglikelihoods\n                lls_unconditional = lls[1::2]\n                assert len(lls_unconditional) == len(choices)\n                # and this stores our \"regular\" conditional loglikelihoods\n                lls = lls[::2]\n\n            pred = np.argmax(lls)\n            pred_norm = np.argmax(lls / completion_len)\n\n            if self.multiple_input:\n                gold = self.doc_to_text(doc)\n            else:\n                gold = self.doc_to_target(doc)\n\n            gold_index_error = False\n            if type(gold) is list:\n                gold = [i if i < len(choices) else -100 for i in gold]\n                if -100 in gold:\n                    gold_index_error = True\n            else:\n                if type(gold) is int:\n                    gold = gold if gold < len(choices) else -100\n                elif type(gold) is str:\n                    gold = choices.index(gold) if gold in choices else -100\n\n                if gold == -100:\n                    gold_index_error = True\n\n            if gold_index_error:\n                eval_logger.warning(f\"Label index was not in within range of available choices,\" f\"Sample:\\n\\n{doc}\\n\\n\")\n\n            if self.multiple_target:\n                acc = 1.0 if pred in gold else 0.0\n                acc_norm = 1.0 if pred_norm in gold else 0.0\n                exact_match = int(any([is_greedy[i] if i != -100 else 0 for i in gold]))\n            else:\n                acc = 1.0 if pred == gold else 0.0\n                acc_norm = 1.0 if pred_norm == gold else 0.0\n                # TODO: this gets score of 0 on arc_challenge for pythia-70m. need to test that this works properly\n                exact_match = int(is_greedy[gold]) if gold != -100 else 0\n\n            result_dict = {\n                **({\"acc\": acc} if \"acc\" in use_metric else {}),\n                **({\"f1\": (gold, pred)} if \"f1\" in use_metric else {}),\n                **({\"mcc\": (gold, pred)} if \"mcc\" in use_metric else {}),\n                **({\"acc_norm\": acc_norm} if \"acc_norm\" in use_metric else {}),\n                **({\"exact_match\": exact_match} if \"exact_match\" in use_metric else {}),\n            }\n\n            if \"acc_mutual_info\" in use_metric:\n                lls_mutual_info = [ll_c - ll_u for ll_c, ll_u in zip(lls, lls_unconditional)]\n                acc_mutual_info = 1.0 if np.argmax(lls_mutual_info) == gold else 0.0\n                result_dict[\"acc_mutual_info\"] = acc_mutual_info\n\n        elif self.OUTPUT_TYPE == \"generate_until\":\n            gold = self.doc_to_target(doc)\n            result = results[0]\n            if self.config.doc_to_choice is not None:\n                # If you set doc_to_choice,\n                # it assumes that doc_to_target returns a number.\n                choices = self.doc_to_choice(doc)\n                gold = choices[gold]\n            # we expect multiple_targets to be a list.\n            elif self.multiple_target:\n                gold = list(gold)\n            elif type(gold) != type(result):\n                # cast gold to the same type as result\n                gold = type(result)(gold)\n\n            for metric in self._metric_fn_list.keys():\n                if self.multiple_target:\n                    # in the case where we have multiple targets,\n                    # return true if any are true\n                    # TODO: this may break for multipLe_target, non zero-or-1 metrics\n                    scores = []\n                    if not isinstance(gold, list):\n                        # sometimes, a multiple_target dataset has exceptions where one doc has only one string answer\n                        # print(gold)\n                        gold = [gold]\n                    for gold_option in gold:\n                        try:\n                            result_score = self._metric_fn_list[metric](\n                                references=[gold_option],\n                                predictions=[result],\n                                **self._metric_fn_kwargs[metric],\n                            )\n                        except TypeError:  # TODO: this is hacky and I don't want to do it\n                            result_score = self._metric_fn_list[metric]([gold_option, result])\n                        if isinstance(result_score, dict):\n                            # TODO: this handles the case where HF evaluate returns a dict.\n                            result_score = result_score[metric]\n                        scores.append(result_score)\n                    if any(scores):\n                        result_score = 1.0\n                    else:\n                        result_score = 0.0\n                else:\n                    try:\n                        result_score = self._metric_fn_list[metric](\n                            references=[gold],\n                            predictions=[result],\n                            **self._metric_fn_kwargs[metric],\n                        )\n                    except TypeError:  # needed for now in order to use a different interface between our own metrics and HF Evaluate metrics\n                        result_score = self._metric_fn_list[metric]([gold, result])\n                    if isinstance(result_score, dict):\n                        # TODO: this handles the case where HF evaluate returns a dict.\n                        result_score = result_score[metric]\n                result_dict[metric] = result_score\n        else:\n            raise ValueError(\n                f\"Passed invalid output_type '{self.OUTPUT_TYPE}' ! Please use one of \",\n                \"'loglikelihood','generate_until' or 'multiple_choice'\",\n            )\n\n        return result_dict\n\n    def aggregation(self):\n        return self._aggregation_list\n\n    def higher_is_better(self):\n        return self._higher_is_better\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/models/mplug_owl_video/tokenization_mplug_owl.py", "content": "# coding=utf-8\n# Copyright 2022 x-plug and The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Tokenization classes for MplugOwl.\"\"\"\n\nfrom transformers.models.llama.tokenization_llama import LlamaTokenizer\n\nfrom loguru import logger\n\nVOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\"}\n\nPRETRAINED_VOCAB_FILES_MAP = {\n    \"vocab_file\": {\n        \"MAGAer13/mplug-owl-llama-7b\": \"https://huggingface.co/MAGAer13/mplug-owl-llama-7b/resolve/main/vocab.txt\",\n    },\n}\n\nPRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n    \"MAGAer13/mplug-owl-llama-7b\": 2048,\n}\n\n\nclass MplugOwlTokenizer(LlamaTokenizer):\n    def __init__(\n        self,\n        vocab_file,\n        unk_token=\"<unk>\",\n        bos_token=\"<s>\",\n        eos_token=\"</s>\",\n        pad_token=\"<unk>\",\n        sp_model_kwargs=None,\n        add_bos_token=False,\n        add_eos_token=False,\n        clean_up_tokenization_spaces=False,\n        **kwargs,\n    ):\n        super().__init__(\n            vocab_file,\n            unk_token,\n            bos_token,\n            eos_token,\n            pad_token,\n            sp_model_kwargs,\n            add_bos_token,\n            add_eos_token,\n            clean_up_tokenization_spaces,\n            **kwargs,\n        )\n        self.eod_id = self.eos_token_id\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/models/minicpm_v.py", "content": "import torch\n\nfrom tqdm import tqdm\nfrom lmms_eval import utils\nfrom lmms_eval.api.instance import Instance\nfrom lmms_eval.api.model import lmms\nfrom lmms_eval.api.registry import register_model\nfrom accelerate import Accelerator, DistributedType\nfrom accelerate.state import AcceleratorState\nfrom typing import List, Optional, Union, Tuple\nfrom transformers import AutoModel, AutoTokenizer\n\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom loguru import logger as eval_logger\n\n\n@register_model(\"minicpm_v\")\nclass MiniCPM_V(lmms):\n    \"\"\"\n    MiniCPM_V Model\n    \"\"\"\n\n    def __init__(\n        self,\n        pretrained: str = \"openbmb/MiniCPM-V\",\n        device: Optional[str] = \"cuda\",\n        dtype: Optional[Union[str, torch.dtype]] = torch.bfloat16,\n        batch_size: Optional[Union[int, str]] = 1,\n        trust_remote_code: Optional[bool] = True,\n        **kwargs,\n    ) -> None:\n        super().__init__()\n        # Do not use kwargs for now\n        assert kwargs == {}, f\"Unexpected kwargs: {kwargs}\"\n\n        accelerator = Accelerator()\n        if accelerator.num_processes > 1:\n            self._device = torch.device(f\"cuda:{accelerator.local_process_index}\")\n        else:\n            self._device = device\n        self._model = AutoModel.from_pretrained(pretrained, trust_remote_code=trust_remote_code, torch_dtype=dtype, device_map=self._device).to(dtype)\n        self._tokenizer = AutoTokenizer.from_pretrained(pretrained, trust_remote_code=trust_remote_code)\n        self._config = self._model.config\n        self.model.eval()\n        self.model.tie_weights()\n        self.batch_size_per_gpu = int(batch_size)\n        if accelerator.num_processes > 1:\n            assert accelerator.distributed_type in [DistributedType.FSDP, DistributedType.MULTI_GPU, DistributedType.DEEPSPEED], \"Unsupported distributed type provided. Only DDP and FSDP are supported.\"\n            # If you want to use DistributedType.DEEPSPEED, you have to run accelerate config before using the model\n            # Also, you have to select zero stage 0 (equivalent to DDP) in order to make the prepare model works\n            # I tried to set different parameters in the kwargs to let default zero 2 stage works, but it didn't work.\n            if accelerator.distributed_type == DistributedType.DEEPSPEED:\n                kwargs = {\n                    \"train_micro_batch_size_per_gpu\": self.batch_size_per_gpu,\n                    \"train_batch_size\": self.batch_size_per_gpu * accelerator.num_processes,\n                }\n                AcceleratorState().deepspeed_plugin.deepspeed_config_process(must_match=True, **kwargs)\n                eval_logger.info(\"Detected that you are using DistributedType.DEEPSPEED. Make sure you run `accelerate config` and set zero stage to 0\")\n            if accelerator.distributed_type == DistributedType.FSDP or accelerator.distributed_type == DistributedType.DEEPSPEED:\n                self._model = accelerator.prepare(self.model)\n            else:\n                self._model = accelerator.prepare_model(self.model, evaluation_mode=True)\n            self.accelerator = accelerator\n            if self.accelerator.is_local_main_process:\n                eval_logger.info(f\"Using {accelerator.num_processes} devices with data parallelism\")\n            self._rank = self.accelerator.local_process_index\n            self._world_size = self.accelerator.num_processes\n        else:\n            self.model.to(self._device)\n            self._rank = 0\n            self._word_size = 1\n\n    @property\n    def config(self):\n        # return the associated transformers.AutoConfig for the given pretrained model.\n        return self._config\n\n    @property\n    def tokenizer(self):\n        return self._tokenizer\n\n    @property\n    def model(self):\n        # returns the model, unwrapping it if using Accelerate\n        if hasattr(self, \"accelerator\"):\n            return self.accelerator.unwrap_model(self._model)\n        else:\n            return self._model\n\n    @property\n    def eot_token_id(self):\n        # we use EOT because end of *text* is more accurate for what we're doing than end of *sentence*\n        return self.tokenizer.eos_token_id\n\n    @property\n    def max_length(self):\n        return self._max_length\n\n    @property\n    def batch_size(self):\n        return self.batch_size_per_gpu\n\n    @property\n    def device(self):\n        return self._device\n\n    @property\n    def rank(self):\n        return self._rank\n\n    @property\n    def world_size(self):\n        return self._world_size\n\n    def tok_encode(self, string: str, left_truncate_len=None, add_special_tokens=None) -> List[int]:\n        \"\"\" \"\"\"\n        add_special_tokens = False if add_special_tokens is None else add_special_tokens\n        encoding = self.tokenizer.encode(string, add_special_tokens=add_special_tokens)\n        # left-truncate the encoded context to be at most `left_truncate_len` tokens long\n        if left_truncate_len:\n            encoding = encoding[-left_truncate_len:]\n        return encoding\n\n    def tok_decode(self, tokens):\n        return self.tokenizer.decode(tokens)\n\n    def loglikelihood(self, requests: List[Instance]) -> List[Tuple[float, bool]]:\n        # TODO\n        assert False, \"We have not implemented this function for MiniCPM_V yet\"\n\n    def flatten(self, input):\n        new_list = []\n        for i in input:\n            for j in i:\n                new_list.append(j)\n        return new_list\n\n    def generate_until(self, requests: List[Instance]) -> List[str]:\n        res = []\n\n        def _collate(x):\n            # the negative sign on len(toks) sorts descending - this has a few advantages:\n            # - time estimates will always be over not underestimates, which is more useful for planning\n            # - to know the size of a batch when going through the list, you know the first one is always the batch\n            #   padded context length. this is useful to simplify the batching logic and more importantly to make\n            #   automatic adaptive batches much much easier to implement\n            # - any OOMs will happen right away rather than near the end\n            toks = self.tok_encode(x[0])\n            return -len(toks), x[0]\n\n        # we group requests by their generation_kwargs,\n        # so that we don't try to execute e.g. greedy sampling and temp=0.8 sampling\n        # in the same batch.\n        re_ords = utils.Collator([reg.args for reg in requests], _collate, grouping=True)\n        chunks = re_ords.get_batched(n=self.batch_size, batch_fn=None)\n        num_iters = len(requests) // self.batch_size if len(requests) % self.batch_size == 0 else len(requests) // self.batch_size + 1\n        pbar = tqdm(total=num_iters, disable=(self.rank != 0), desc=\"Model Responding\")\n        for chunk in chunks:\n            contexts, all_gen_kwargs, doc_to_visual, doc_id, task, split = zip(*chunk)\n            task = task[0]\n            split = split[0]\n            visuals = [doc_to_visual[0](self.task_dict[task][split][ids]) for ids in doc_id]\n            visuals = self.flatten(visuals)\n            # we assume all gen kwargs in the batch are the same\n            # this is safe to assume because the `grouper` object ensures it.\n            gen_kwargs = all_gen_kwargs[0]\n\n            # Set default values for until and max_new_tokens\n            until = [self.tok_decode(self.eot_token_id)]\n\n            # Update values from gen_kwargs if present\n            if \"until\" in gen_kwargs:\n                until = gen_kwargs.pop(\"until\")\n                if isinstance(until, str):\n                    until = [until]\n                elif not isinstance(until, list):\n                    raise ValueError(f\"Expected `gen_kwargs['until']` to be of type Union[str,list] but got {type(until)}\")\n            assert self.batch_size_per_gpu == 1, \"Do not support batch_size_per_gpu > 1 for now\"\n            assert len(visuals) == 1, \"MiniCPM_V interface does not support bn_image > 1 for now\"\n            context = contexts[0]\n            if \"<image>\" in context:\n                # minicpm does not expect the <image> tag\n                context = context.replace(\"<image>\", \"\")\n            msgs = [{\"role\": \"user\", \"content\": context}]\n\n            gen_kwargs[\"image_sizes\"] = [visuals[idx].size for idx in range(len(visuals))]\n            if \"max_new_tokens\" not in gen_kwargs:\n                gen_kwargs[\"max_new_tokens\"] = 1024\n            if \"temperature\" not in gen_kwargs:\n                gen_kwargs[\"temperature\"] = 0\n            if \"top_p\" not in gen_kwargs:\n                gen_kwargs[\"top_p\"] = None\n            if \"num_beams\" not in gen_kwargs:\n                gen_kwargs[\"num_beams\"] = 1\n            try:\n                # ominicpm does not give much information on how they do eval so I just use the chat format.\n                response, context, _ = self.model.chat(\n                    image=visuals[0],\n                    msgs=msgs,\n                    context=None,\n                    tokenizer=self.tokenizer,\n                    sampling=True if gen_kwargs[\"temperature\"] > 0 else False,\n                    temperature=gen_kwargs[\"temperature\"],\n                    top_p=gen_kwargs[\"top_p\"],\n                    num_beams=gen_kwargs[\"num_beams\"],\n                    max_new_tokens=gen_kwargs[\"max_new_tokens\"],\n                )\n            except Exception as e:\n                eval_logger.error(f\"Error {e} in generating\")\n                cont = \"\"\n            res.append(response)\n            self.cache_hook.add_partial(\"generate_until\", (context, gen_kwargs), response)\n            pbar.update(1)\n            # reorder this group of results back to original unsorted form\n        res = re_ords.get_original(res)\n\n        pbar.close()\n        return res\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/models/reka.py", "content": "from PIL import Image\nfrom io import BytesIO\nfrom copy import deepcopy\nimport numpy as np\nimport os\nimport base64\nfrom typing import List, Tuple\nfrom tqdm import tqdm\nimport requests as url_requests\nimport time\n\nimport json\n\nfrom lmms_eval.api.instance import Instance\nfrom lmms_eval.api.model import lmms\nfrom lmms_eval.api.registry import register_model\nfrom accelerate import Accelerator, DistributedType\n\nNUM_SECONDS_TO_SLEEP = 30\n\nfrom loguru import logger\n\neval_logger = logger\n\ntry:\n    from reka.client import Reka as RekaClient\n    from reka import ChatMessage\n    from decord import VideoReader, cpu\nexcept Exception as e:\n    eval_logger.warning(f\"Error importing reka: {e}\")\n\n\n@register_model(\"reka\")\nclass Reka(lmms):\n    def __init__(\n        self,\n        model_version: str = \"reka-edge\",\n        modality: str = \"image\",\n        max_frames_num: int = 5,\n        timeout: int = 120,\n        continual_mode: bool = False,\n        response_persistent_folder: str = None,  # We will cache the Gemini API response in this path and use it for future requests\n        **kwargs,\n    ) -> None:\n        super().__init__()\n        self.model_version = model_version\n        self.modality = modality\n        self.max_frames_num = max_frames_num\n        self.timeout = timeout\n        self.continual_mode = continual_mode\n        if self.continual_mode:\n            if response_persistent_folder is None:\n                raise ValueError(\"Continual mode requires a persistent path for the response. Please provide a valid path.\")\n\n            os.makedirs(response_persistent_folder, exist_ok=True)\n            self.response_persistent_folder = response_persistent_folder\n            self.response_persistent_file = os.path.join(self.response_persistent_folder, f\"{self.model_version}_response.json\")\n\n            if os.path.exists(self.response_persistent_file):\n                with open(self.response_persistent_file, \"r\") as f:\n                    self.response_cache = json.load(f)\n                self.cache_mode = \"resume\"\n            else:\n                self.response_cache = {}\n                self.cache_mode = \"start\"\n\n        self.reka = RekaClient(api_key=os.getenv(\"REKA_API_KEY\", \"YOUR_API_KEY\"))\n\n        accelerator = Accelerator()\n        if accelerator.num_processes > 1:\n            assert accelerator.distributed_type in [DistributedType.FSDP, DistributedType.MULTI_GPU, DistributedType.DEEPSPEED], \"Unsupported distributed type provided. Only DDP and FSDP are supported.\"\n            self.accelerator = accelerator\n            if self.accelerator.is_local_main_process:\n                eval_logger.info(f\"Using {accelerator.num_processes} devices with data parallelism\")\n            self._rank = self.accelerator.local_process_index\n            self._world_size = self.accelerator.num_processes\n        else:\n            self.accelerator = accelerator\n            self._rank = self.accelerator.local_process_index\n            self._world_size = self.accelerator.num_processes\n\n        self.device = self.accelerator.device\n\n    def encode_image(self, image):\n        if type(image) == list:\n            media_urls = []\n            for img in image:\n                output_buffer = BytesIO()\n                img.save(output_buffer, format=\"PNG\")\n                byte_data = output_buffer.getvalue()\n                base64_str = base64.b64encode(byte_data).decode(\"utf-8\")\n                media_urls.append(f\"data:image/jpeg;base64,{base64_str}\")\n            return media_urls\n        else:\n            output_buffer = BytesIO()\n            image.save(output_buffer, format=\"PNG\")\n            byte_data = output_buffer.getvalue()\n            base64_str = base64.b64encode(byte_data).decode(\"utf-8\")\n\n            return f\"data:image/jpeg;base64,{base64_str}\"\n\n    def encode_video(self, video_path):\n        vr = VideoReader(video_path, ctx=cpu(0))\n        total_frame_num = len(vr)\n        uniform_sampled_frames = np.linspace(0, total_frame_num - 1, self.max_frames_num, dtype=int)\n        frame_idx = uniform_sampled_frames.tolist()\n        frames = vr.get_batch(frame_idx).asnumpy()\n\n        base64_frames = []\n        for frame in frames:\n            img = Image.fromarray(frame)\n            output_buffer = BytesIO()\n            img.save(output_buffer, format=\"PNG\")\n            byte_data = output_buffer.getvalue()\n            base64_str = base64.b64encode(byte_data).decode(\"utf-8\")\n            base64_frames.append(f\"data:image/jpeg;base64,{base64_str}\")\n\n        return base64_frames\n\n    def generate_until(self, requests) -> List[str]:\n        res = []\n        pbar = tqdm(total=len(requests), disable=(self.rank != 0), desc=\"Model Responding\")\n\n        for context, gen_kwargs, doc_to_visual, doc_id, task, split in [reg.args for reg in requests]:\n            if self.continual_mode is True and self.cache_mode == \"resume\":\n                doc_uuid = f\"{task}___{split}___{doc_id}\"\n                if doc_uuid in self.response_cache:\n                    response_text = self.response_cache[doc_uuid]\n                    if response_text:\n                        res.append(response_text)\n                        pbar.update(1)\n                        continue\n\n            visual = doc_to_visual(self.task_dict[task][split][doc_id])\n\n            message_content = []\n\n            if self.modality == \"image\":\n                media_urls = self.encode_image(visual)\n                message_content.append({\"type\": \"text\", \"text\": context})\n                for media_url in media_urls:\n                    message_content.append({\"type\": \"image_url\", \"image_url\": media_url})\n            elif self.modality == \"video\":\n                message_content.append({\"type\": \"text\", \"text\": context})\n                assert len(visual) == 1, \"Reka only supports one video per request\"\n                media_urls = self.encode_video(visual[0])\n                assert len(media_urls) == self.max_frames_num, f\"Reka only supports {self.max_frames_num} frames per request\"\n                for media_url in media_urls:\n                    message_content.append({\"type\": \"image_url\", \"image_url\": media_url})\n\n            if \"max_new_tokens\" not in gen_kwargs:\n                gen_kwargs[\"max_new_tokens\"] = 1024\n            if \"temperature\" not in gen_kwargs:\n                gen_kwargs[\"temperature\"] = 0\n            if \"top_p\" not in gen_kwargs:\n                gen_kwargs[\"top_p\"] = None\n            if \"num_beams\" not in gen_kwargs:\n                gen_kwargs[\"num_beams\"] = 1\n\n            for attempt in range(5):\n                try:\n                    response = self.reka.chat.create(\n                        messages=[\n                            ChatMessage(\n                                role=\"user\",\n                                content=message_content,\n                            )\n                        ],\n                        model=self.model_version,\n                    )\n                    response_text = response.responses[0].message.content.strip()\n                    break  # If successful, break out of the loop\n\n                except Exception as e:\n                    eval_logger.info(f\"Attempt {attempt + 1} failed with error: {str(e)}\")\n                    if attempt < 5 - 1:  # If we have retries left, sleep and then continue to next attempt\n                        time.sleep(NUM_SECONDS_TO_SLEEP)\n                    else:  # If this was the last attempt, log and return empty\n                        eval_logger.error(f\"All 5 attempts failed. Last error message: {str(e)}\")\n                        response_text = \"\"\n\n            res.append(response_text)\n            pbar.update(1)\n            if self.continual_mode is True:  # Cache the response\n                doc_uuid = f\"{task}___{split}___{doc_id}\"\n                self.response_cache[doc_uuid] = response_text\n                with open(self.response_persistent_file, \"w\") as f:\n                    json.dump(self.response_cache, f)\n\n        pbar.close()\n        return res\n\n    def loglikelihood(self, requests: List[Instance]) -> List[Tuple[float, bool]]:\n        # TODO\n        assert False, \"Reka not support loglikelihood\"\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/models/llama_vid.py", "content": "import os\nfrom accelerate import Accelerator, DistributedType, InitProcessGroupKwargs\nfrom accelerate.state import AcceleratorState\nfrom typing import List, Optional, Union, Tuple\nimport torch\nfrom tqdm import tqdm\nfrom decord import VideoReader, cpu\nimport numpy as np\nimport math\nfrom datetime import timedelta\nfrom transformers import AutoConfig\nfrom huggingface_hub import snapshot_download\nimport requests\n\nfrom lmms_eval import utils\nfrom lmms_eval.api.instance import Instance\nfrom lmms_eval.api.model import lmms\nfrom lmms_eval.api.registry import register_model\nfrom lmms_eval.utils import stop_sequences_criteria\nfrom lmms_eval.models.model_utils.load_video import read_video_pyav\n\nimport subprocess\n\nfrom loguru import logger as eval_logger\n\ntry:\n    from llamavid.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n    from llamavid.conversation import conv_templates, SeparatorStyle\n    from llamavid.model.builder import load_pretrained_model\n    from llava.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\nexcept ImportError:\n    eval_logger.debug(\"LLaMA-Video is not installed. Please install LLaMA-Video to use this model.\")\n\n\n@register_model(\"llama_vid\")\nclass LLaMAVid(lmms):\n    def __init__(\n        self,\n        pretrained: str = \"YanweiLi/llama-vid-7b-full-224-video-fps-1\",\n        truncation: Optional[bool] = True,\n        device: Optional[str] = \"cuda:0\",\n        dtype: Optional[Union[str, torch.dtype]] = \"auto\",\n        batch_size: Optional[Union[int, str]] = 1,\n        trust_remote_code: Optional[bool] = False,\n        revision=None,\n        attn_implementation=(\n            \"sdpa\" if torch.__version__ > \"2.1.2\" else \"eager\"\n        ),  # inference implementation for attention, can be \"sdpa\", \"eager\", \"flash_attention_2\". Seems FA2 is not effective during inference: https://discuss.huggingface.co/t/flash-attention-has-no-effect-on-inference/73453/5\n        device_map=\"cuda:0\",\n        conv_template=\"vicuna_v1\",\n        use_cache=True,\n        truncate_context=False,\n        num_frames: int = 100,\n        **kwargs,\n    ) -> None:\n        super().__init__()\n\n        accelerator_kwargs = InitProcessGroupKwargs(timeout=timedelta(weeks=52))\n        accelerator = Accelerator(kwargs_handlers=[accelerator_kwargs])\n        if accelerator.num_processes > 1:\n            self._device = torch.device(f\"cuda:{accelerator.local_process_index}\")\n            self.device_map = f\"cuda:{accelerator.local_process_index}\"\n        elif accelerator.num_processes == 1 and device_map == \"auto\":\n            self._device = torch.device(device)\n            self.device_map = device_map\n        else:\n            self._device = torch.device(f\"cuda:{accelerator.local_process_index}\")\n            self.device_map = f\"cuda:{accelerator.local_process_index}\"\n\n        self.pretrained = pretrained\n        self.model_path = snapshot_download(self.pretrained)\n        self.model_name = get_model_name_from_path(pretrained)\n        self.num_frames = num_frames\n        if not os.path.exists(\"./model_zoo/LAVIS/eva_vit_g.pth\") and accelerator.is_main_process:\n            eval_logger.info(\"\\n\\n Eva Encoder is not found for LLaMA-VID. Download automatically to the folder ./model_zoo/LAVIS\")\n            cache_path = \"model_zoo/LAVIS\"\n            os.makedirs(cache_path, exist_ok=True)\n            subprocess.run([\"wget https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/eva_vit_g.pth -O ./model_zoo/LAVIS/eva_vit_g.pth\"], shell=True)\n\n        accelerator.wait_for_everyone()\n        self._tokenizer, self._model, self.image_processor, self._max_length = load_pretrained_model(\n            self.model_path,\n            None,\n            self.model_name,\n            device_map=self.device_map,\n        )\n\n        self._config = self._model.config\n        self.model.eval()\n        self.model.tie_weights()\n        self.truncation = truncation\n        self.batch_size_per_gpu = int(batch_size)\n        self.conv_template = conv_template\n        self.use_cache = use_cache\n        self.truncate_context = truncate_context\n        # assert self.batch_size_per_gpu == 1, \"Llava currently does not support batched generation. See https://github.com/haotian-liu/LLaVA/issues/754. HF Llava also has this issue.\"\n        if accelerator.num_processes > 1:\n            assert accelerator.distributed_type in [DistributedType.FSDP, DistributedType.MULTI_GPU, DistributedType.DEEPSPEED], \"Unsupported distributed type provided. Only DDP and FSDP are supported.\"\n            # If you want to use DistributedType.DEEPSPEED, you have to run accelerate config before using the model\n            # Also, you have to select zero stage 0 (equivalent to DDP) in order to make the prepare model works\n            # I tried to set different parameters in the kwargs to let default zero 2 stage works, but it didn't work.\n            if accelerator.distributed_type == DistributedType.DEEPSPEED:\n                kwargs = {\n                    \"train_micro_batch_size_per_gpu\": self.batch_size_per_gpu,\n                    \"train_batch_size\": self.batch_size_per_gpu * accelerator.num_processes,\n                }\n                AcceleratorState().deepspeed_plugin.deepspeed_config_process(must_match=True, **kwargs)\n                eval_logger.info(\"Detected that you are using DistributedType.DEEPSPEED. Make sure you run `accelerate config` and set zero stage to 0\")\n            if accelerator.distributed_type == DistributedType.FSDP or accelerator.distributed_type == DistributedType.DEEPSPEED:\n                self._model = accelerator.prepare(self.model)\n            else:\n                self._model = accelerator.prepare_model(self.model, evaluation_mode=True)\n            self.accelerator = accelerator\n            if self.accelerator.is_local_main_process:\n                eval_logger.info(f\"Using {accelerator.num_processes} devices with data parallelism\")\n            self._rank = self.accelerator.local_process_index\n            self._world_size = self.accelerator.num_processes\n        elif accelerator.num_processes == 1 and device_map == \"auto\":\n            eval_logger.info(f\"Using {accelerator.num_processes} devices with tensor parallelism\")\n            self._rank = 0\n            self._word_size = 1\n        else:\n            eval_logger.info(f\"Using single device: {self._device}\")\n            self.model.to(self._device)\n            self._rank = 0\n            self._world_size = 1\n\n    def download_file(self, url, folder_path):\n        # Create the folder if it doesn't exist\n        if not os.path.exists(folder_path):\n            os.makedirs(folder_path)\n\n        # Extract filename from URL\n        filename = url.split(\"/\")[-1]\n\n        # Define path to save the file\n        file_path = os.path.join(folder_path, filename)\n\n        # Send a GET request to the URL\n        response = requests.get(url)\n\n        # Check if request was successful (status code 200)\n        if response.status_code == 200:\n            # Save the file to the specified folder\n            with open(file_path, \"wb\") as f:\n                f.write(response.content)\n            print(f\"File downloaded successfully to {file_path}\")\n        else:\n            print(f\"Failed to download file. Status code: {response.status_code}\")\n\n    @property\n    def config(self):\n        # return the associated transformers.AutoConfig for the given pretrained model.\n        return self._config\n\n    @property\n    def tokenizer(self):\n        return self._tokenizer\n\n    @property\n    def model(self):\n        # returns the model, unwrapping it if using Accelerate\n        if hasattr(self, \"accelerator\"):\n            return self.accelerator.unwrap_model(self._model)\n        else:\n            return self._model\n\n    @property\n    def eot_token_id(self):\n        # we use EOT because end of *text* is more accurate for what we're doing than end of *sentence*\n        return self.tokenizer.eos_token_id\n\n    @property\n    def max_length(self):\n        return self._max_length\n\n    def tok_encode(self, string: str, left_truncate_len=None, add_special_tokens=None) -> List[int]:\n        \"\"\" \"\"\"\n        add_special_tokens = False if add_special_tokens is None else add_special_tokens\n        encoding = self.tokenizer.encode(string, add_special_tokens=add_special_tokens)\n        # left-truncate the encoded context to be at most `left_truncate_len` tokens long\n        if left_truncate_len:\n            encoding = encoding[-left_truncate_len:]\n        return encoding\n\n    def tok_decode(self, tokens):\n        return self.tokenizer.decode(tokens)\n\n    def load_video(self, video_path):\n        vr = VideoReader(video_path, ctx=cpu(0))\n        total_frame_num = len(vr)\n        fps = round(vr.get_avg_fps())\n        frame_idx = [i for i in range(0, len(vr), fps)]\n        spare_frames = vr.get_batch(frame_idx).asnumpy()\n        return spare_frames\n\n    def flatten(self, input):\n        new_list = []\n        for i in input:\n            for j in i:\n                new_list.append(j)\n        return new_list\n\n    def generate_until(self, requests) -> List[str]:\n        res = []\n        pbar = tqdm(total=len(requests), disable=(self.rank != 0), desc=\"Model Responding\")\n\n        for contexts, gen_kwargs, doc_to_visual, doc_id, task, split in [reg.args for reg in requests]:\n            # encode, pad, and truncate contexts for this batch\n            visuals = [doc_to_visual(self.task_dict[task][split][doc_id])]\n            visuals = self.flatten(visuals)\n            videos = []\n            for visual in visuals:\n                video = read_video_pyav(visual, num_frm=self.num_frames)\n                video = self.image_processor.preprocess(video, return_tensors=\"pt\")[\"pixel_values\"].half().cuda()\n                video = [video]\n                videos += video\n            qs = contexts\n            if self.model.config.mm_use_im_start_end:\n                qs = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + \"\\n\" + qs\n            else:\n                qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\n\n            conv = conv_templates[self.conv_template].copy()\n            conv.append_message(conv.roles[0], qs)\n            conv.append_message(conv.roles[1], None)\n            prompt = conv.get_prompt()\n\n            input_ids = tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).cuda()\n\n            stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n            keywords = [stop_str]\n            stopping_criteria = KeywordsStoppingCriteria(keywords, self.tokenizer, input_ids)\n\n            cur_prompt = contexts\n            with torch.inference_mode():\n                self.model.update_prompt([[cur_prompt]])\n                output_ids = self.model.generate(input_ids, images=video, do_sample=True, temperature=0.2, max_new_tokens=1024, use_cache=True, stopping_criteria=[stopping_criteria])\n\n            input_token_len = input_ids.shape[1]\n            n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()\n            if n_diff_input_output > 0:\n                print(f\"[Warning] {n_diff_input_output} output_ids are not the same as the input_ids\")\n            outputs = self.tokenizer.batch_decode(output_ids[:, input_token_len:], skip_special_tokens=True)[0]\n            outputs = outputs.strip()\n            if outputs.endswith(stop_str):\n                outputs = outputs[: -len(stop_str)]\n            outputs = outputs.strip()\n            pbar.update(1)\n            res.append(outputs)\n\n        return res\n\n    def loglikelihood(self, requests: List[Instance]) -> List[Tuple[float, bool]]:\n        return super().loglikelihood(requests)\n\n    @property\n    def batch_size(self):\n        return self.batch_size_per_gpu\n\n    @property\n    def device(self):\n        return self._device\n\n    @property\n    def rank(self):\n        return self._rank\n\n    @property\n    def world_size(self):\n        return self._world_size\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/models/model_utils/qwen/qwen_generate_utils.py", "content": "# Copyright (c) Alibaba Cloud.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"Generation support.\"\"\"\nimport warnings\n\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\")\n\nfrom typing import Tuple, List, Union, Iterable\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom transformers import PreTrainedTokenizer\nfrom transformers.generation import LogitsProcessor\n\nfrom loguru import logger\n\n# Types.\nHistoryType = List[Tuple[str, str]]\nTokensType = List[int]\nBatchTokensType = List[List[int]]\n\n\ndef pad_batch(batch: BatchTokensType, pad_id: int, seq_length: int) -> BatchTokensType:\n    for tokens in batch:\n        context_length = len(tokens)\n        if context_length < seq_length:\n            tokens.extend([pad_id] * (seq_length - context_length))\n    return batch\n\n\ndef get_ltor_masks_and_position_ids(\n    data,\n    eod_token,\n    reset_position_ids,\n    reset_attention_mask,\n    eod_mask_loss,\n):\n    \"\"\"Build masks and position id for left to right model.\"\"\"\n\n    # Extract batch size and sequence length.\n    micro_batch_size, seq_length = data.size()\n\n    # Attention mask (lower triangular).\n    if reset_attention_mask:\n        att_mask_batch = micro_batch_size\n    else:\n        att_mask_batch = 1\n    attention_mask = torch.tril(torch.ones((att_mask_batch, seq_length, seq_length), device=data.device)).view(att_mask_batch, 1, seq_length, seq_length)\n\n    # Loss mask.\n    loss_mask = torch.ones(data.size(), dtype=torch.float, device=data.device)\n    if eod_mask_loss:\n        loss_mask[data == eod_token] = 0.0\n\n    # Position ids.\n    position_ids = torch.arange(seq_length, dtype=torch.long, device=data.device)\n    position_ids = position_ids.unsqueeze(0).expand_as(data)\n    # We need to clone as the ids will be modifed based on batch index.\n    if reset_position_ids:\n        position_ids = position_ids.clone()\n\n    if reset_position_ids or reset_attention_mask:\n        # Loop through the batches:\n        for b in range(micro_batch_size):\n            # Find indecies where EOD token is.\n            eod_index = position_ids[b, data[b] == eod_token]\n            # Detach indecies from positions if going to modify positions.\n            if reset_position_ids:\n                eod_index = eod_index.clone()\n\n            # Loop through EOD indecies:\n            prev_index = 0\n            for j in range(eod_index.size()[0]):\n                i = eod_index[j]\n                # Mask attention loss.\n                if reset_attention_mask:\n                    attention_mask[b, 0, (i + 1) :, : (i + 1)] = 0\n                # Reset positions.\n                if reset_position_ids:\n                    position_ids[b, (i + 1) :] -= i + 1 - prev_index\n                    prev_index = i + 1\n\n    # Convert attention mask to binary:\n    attention_mask = attention_mask < 0.5\n\n    return attention_mask, loss_mask, position_ids\n\n\ndef get_batch(context_tokens: torch.LongTensor, eod_id: int):\n    \"\"\"Generate batch from context tokens.\"\"\"\n    # Move to GPU.\n    tokens = context_tokens.contiguous().to(context_tokens.device)\n    # Get the attention mask and postition ids.\n    attention_mask, _, position_ids = get_ltor_masks_and_position_ids(\n        tokens,\n        eod_id,\n        reset_position_ids=False,\n        reset_attention_mask=False,\n        eod_mask_loss=False,\n    )\n    return tokens, attention_mask, position_ids\n\n\ndef get_stop_words_ids(chat_format, tokenizer):\n    if chat_format == \"raw\":\n        stop_words_ids = [tokenizer.encode(\"Human:\"), [tokenizer.eod_id]]\n    elif chat_format == \"chatml\":\n        stop_words_ids = [[tokenizer.im_end_id], [tokenizer.im_start_id]]\n    else:\n        raise NotImplementedError(f\"Unknown chat format {chat_format!r}\")\n    return stop_words_ids\n\n\ndef make_context(\n    tokenizer: PreTrainedTokenizer,\n    query: str,\n    history: List[Tuple[str, str]] = None,\n    system: str = \"\",\n    max_window_size: int = 6144,\n    chat_format: str = \"chatml\",\n):\n    if history is None:\n        history = []\n\n    if chat_format == \"chatml\":\n        im_start, im_end = \"<|im_start|>\", \"<|im_end|>\"\n        im_start_tokens = [tokenizer.im_start_id]\n        im_end_tokens = [tokenizer.im_end_id]\n        nl_tokens = tokenizer.encode(\"\\n\")\n\n        def _tokenize_str(role, content):\n            return f\"{role}\\n{content}\", tokenizer.encode(role, allowed_special=set(tokenizer.IMAGE_ST)) + nl_tokens + tokenizer.encode(content, allowed_special=set(tokenizer.IMAGE_ST))\n\n        system_text, system_tokens_part = _tokenize_str(\"system\", system)\n        system_tokens = im_start_tokens + system_tokens_part + im_end_tokens\n\n        raw_text = \"\"\n        context_tokens = []\n\n        for turn_query, turn_response in reversed(history):\n            query_text, query_tokens_part = _tokenize_str(\"user\", turn_query)\n            query_tokens = im_start_tokens + query_tokens_part + im_end_tokens\n            if turn_response is not None:\n                response_text, response_tokens_part = _tokenize_str(\"assistant\", turn_response)\n                response_tokens = im_start_tokens + response_tokens_part + im_end_tokens\n\n                next_context_tokens = nl_tokens + query_tokens + nl_tokens + response_tokens\n                prev_chat = f\"\\n{im_start}{query_text}{im_end}\\n{im_start}{response_text}{im_end}\"\n            else:\n                next_context_tokens = nl_tokens + query_tokens + nl_tokens\n                prev_chat = f\"\\n{im_start}{query_text}{im_end}\\n\"\n\n            current_context_size = len(system_tokens) + len(next_context_tokens) + len(context_tokens)\n            if current_context_size < max_window_size:\n                context_tokens = next_context_tokens + context_tokens\n                raw_text = prev_chat + raw_text\n            else:\n                break\n\n        context_tokens = system_tokens + context_tokens\n        raw_text = f\"{im_start}{system_text}{im_end}\" + raw_text\n        context_tokens += nl_tokens + im_start_tokens + _tokenize_str(\"user\", query)[1] + im_end_tokens + nl_tokens + im_start_tokens + tokenizer.encode(\"assistant\") + nl_tokens\n        raw_text += f\"\\n{im_start}user\\n{query}{im_end}\\n{im_start}assistant\\n\"\n\n    elif chat_format == \"raw\":\n        raw_text = query\n        context_tokens = tokenizer.encode(raw_text)\n    else:\n        raise NotImplementedError(f\"Unknown chat format {chat_format!r}\")\n\n    return raw_text, context_tokens\n\n\ndef _decode_default(\n    tokens: List[int],\n    *,\n    stop_words: List[str],\n    eod_words: List[str],\n    tokenizer: PreTrainedTokenizer,\n    raw_text_len: int,\n    verbose: bool = False,\n    return_end_reason: bool = False,\n    errors: str = \"replace\",\n):\n    trim_decode_tokens = tokenizer.decode(tokens, errors=errors)[raw_text_len:]\n    if verbose:\n        print(\"\\nRaw Generate: \", trim_decode_tokens)\n\n    end_reason = f\"Gen length {len(tokens)}\"\n    for stop_word in stop_words:\n        trim_decode_tokens = trim_decode_tokens.replace(stop_word, \"\").strip()\n    for eod_word in eod_words:\n        if eod_word in trim_decode_tokens:\n            end_reason = f\"Gen {eod_word!r}\"\n        trim_decode_tokens = trim_decode_tokens.split(eod_word)[0]\n    trim_decode_tokens = trim_decode_tokens.strip()\n    if verbose:\n        print(\"\\nEnd Reason:\", end_reason)\n        print(\"\\nGenerate: \", trim_decode_tokens)\n\n    if return_end_reason:\n        return trim_decode_tokens, end_reason\n    else:\n        return trim_decode_tokens\n\n\ndef _decode_chatml(\n    tokens: List[int], *, stop_words: List[str], eod_token_ids: List[int], tokenizer: PreTrainedTokenizer, raw_text_len: int, context_length: int, verbose: bool = False, return_end_reason: bool = False, errors: str = \"replace\"\n):\n    end_reason = f\"Gen length {len(tokens)}\"\n    eod_token_idx = context_length\n    for eod_token_idx in range(context_length, len(tokens)):\n        if tokens[eod_token_idx] in eod_token_ids:\n            end_reason = f\"Gen {tokenizer.decode([tokens[eod_token_idx]])!r}\"\n            break\n\n    trim_decode_tokens = tokenizer.decode(tokens[:eod_token_idx], errors=errors)[raw_text_len:]\n    if verbose:\n        print(\"\\nRaw Generate w/o EOD:\", tokenizer.decode(tokens, errors=errors)[raw_text_len:])\n        print(\"\\nRaw Generate:\", trim_decode_tokens)\n        print(\"\\nEnd Reason:\", end_reason)\n    for stop_word in stop_words:\n        trim_decode_tokens = trim_decode_tokens.replace(stop_word, \"\").strip()\n    trim_decode_tokens = trim_decode_tokens.strip()\n    if verbose:\n        print(\"\\nGenerate:\", trim_decode_tokens)\n\n    if return_end_reason:\n        return trim_decode_tokens, end_reason\n    else:\n        return trim_decode_tokens\n\n\ndef decode_tokens(\n    tokens: Union[torch.LongTensor, TokensType],\n    tokenizer: PreTrainedTokenizer,\n    raw_text_len: int,\n    context_length: int,\n    chat_format: str,\n    verbose: bool = False,\n    return_end_reason: bool = False,\n    errors: str = \"replace\",\n) -> str:\n    if torch.is_tensor(tokens):\n        tokens = tokens.cpu().numpy().tolist()\n\n    if chat_format == \"chatml\":\n        return _decode_chatml(\n            tokens,\n            stop_words=[],\n            eod_token_ids=[tokenizer.im_start_id, tokenizer.im_end_id],\n            tokenizer=tokenizer,\n            raw_text_len=raw_text_len,\n            context_length=context_length,\n            verbose=verbose,\n            return_end_reason=return_end_reason,\n            errors=errors,\n        )\n    elif chat_format == \"raw\":\n        return _decode_default(\n            tokens,\n            stop_words=[\"<|endoftext|>\"],\n            eod_words=[\"<|endoftext|>\"],\n            tokenizer=tokenizer,\n            raw_text_len=raw_text_len,\n            verbose=verbose,\n            return_end_reason=return_end_reason,\n            errors=errors,\n        )\n    else:\n        raise NotImplementedError(f\"Unknown chat format {chat_format!r}\")\n\n\nclass StopWordsLogitsProcessor(LogitsProcessor):\n    \"\"\"\n    :class:`transformers.LogitsProcessor` that enforces that when specified sequences appear, stop geration.\n    Args:\n        stop_words_ids (:obj:`List[List[int]]`):\n            List of list of token ids of stop ids. In order to get the tokens of the words\n            that should not appear in the generated text, use :obj:`tokenizer(bad_word,\n            add_prefix_space=True).input_ids`.\n        eos_token_id (:obj:`int`):\n            The id of the `end-of-sequence` token.\n    \"\"\"\n\n    def __init__(self, stop_words_ids: Iterable[Iterable[int]], eos_token_id: int):\n        if not isinstance(stop_words_ids, List) or len(stop_words_ids) == 0:\n            raise ValueError(f\"`stop_words_ids` has to be a non-emtpy list, but is {stop_words_ids}.\")\n        if any(not isinstance(bad_word_ids, list) for bad_word_ids in stop_words_ids):\n            raise ValueError(f\"`stop_words_ids` has to be a list of lists, but is {stop_words_ids}.\")\n        if any(any((not isinstance(token_id, (int, np.integer)) or token_id < 0) for token_id in stop_word_ids) for stop_word_ids in stop_words_ids):\n            raise ValueError(f\"Each list in `stop_words_ids` has to be a list of positive integers, but is {stop_words_ids}.\")\n\n        self.stop_words_ids = list(filter(lambda bad_token_seq: bad_token_seq != [eos_token_id], stop_words_ids))\n        self.eos_token_id = eos_token_id\n        for stop_token_seq in self.stop_words_ids:\n            assert len(stop_token_seq) > 0, \"Stop words token sequences {} cannot have an empty list\".format(stop_words_ids)\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n        stopped_samples = self._calc_stopped_samples(input_ids)\n        for i, should_stop in enumerate(stopped_samples):\n            if should_stop:\n                scores[i, self.eos_token_id] = float(2**15)\n        return scores\n\n    def _tokens_match(self, prev_tokens: torch.LongTensor, tokens: List[int]) -> bool:\n        if len(tokens) == 0:\n            # if bad word tokens is just one token always ban it\n            return True\n        elif len(tokens) > len(prev_tokens):\n            # if bad word tokens are longer then prev input_ids they can't be equal\n            return False\n        elif prev_tokens[-len(tokens) :].tolist() == tokens:\n            # if tokens match\n            return True\n        else:\n            return False\n\n    def _calc_stopped_samples(self, prev_input_ids: Iterable[int]) -> Iterable[int]:\n        stopped_samples = []\n        for prev_input_ids_slice in prev_input_ids:\n            match = False\n            for stop_token_seq in self.stop_words_ids:\n                if self._tokens_match(prev_input_ids_slice, stop_token_seq):\n                    # if tokens do not match continue\n                    match = True\n                    break\n            stopped_samples.append(match)\n\n        return stopped_samples\n\n\ndef top_k_logits(logits, top_k=0, top_p=0.0, filter_value=-float(\"Inf\")):\n    \"\"\"This function has been mostly taken from huggingface conversational\n    ai code at\n        https://medium.com/huggingface/how-to-build-a-state-of-the-art-\n             conversational-ai-with-transfer-learning-2d818ac26313\"\"\"\n\n    if top_k > 0:\n        # Remove all tokens with a probability less than the\n        # last token of the top-k\n        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n        logits[indices_to_remove] = filter_value\n\n    if top_p > 0.0:\n        # Cconvert to 1D\n        sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)\n        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n\n        # Remove tokens with cumulative probability above the threshold\n        sorted_indices_to_remove = cumulative_probs > top_p\n        # Shift the indices to the right to keep also the first token\n        # above the threshold\n        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n        sorted_indices_to_remove[..., 0] = 0\n        for i in range(sorted_indices.size(0)):\n            indices_to_remove = sorted_indices[i][sorted_indices_to_remove[i]]\n            logits[i][indices_to_remove] = filter_value\n\n    return logits\n\n\ndef switch(val1, val2, boolean):\n    boolean = boolean.type_as(val1)\n    return (1 - boolean) * val1 + boolean * val2\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/api/instance.py", "content": "from dataclasses import dataclass, field\nfrom typing import Literal, Tuple\n\n\n@dataclass\nclass Instance:\n    request_type: Literal[\"loglikelihood\", \"generate_until\"]\n    arguments: tuple\n    idx: int\n    metadata: Tuple[str, int, int] = field(default_factory=lambda: (None, None, None))  # TODO: better typehints here\n    resps: list = field(default_factory=list)\n    vision_id_experts: list = field(default_factory=list)\n    mlp_id_experts: list = field(default_factory=list)\n    filtered_resps: dict = field(default_factory=dict)\n\n    # initialized after init\n    task_name: str = None\n    doc_id: str = None\n    repeats: str = None\n    doc: dict = None\n    # special domain of a sample (ex: category of the MME: code_reasoning, ...)\n    domain: str = None\n\n    def __post_init__(self) -> None:\n        # unpack metadata field\n        self.task_name, self.doc_id, self.repeats = self.metadata\n\n    @property\n    def args(self):\n        \"\"\"\n        Returns (string,) where `string` is the string to calculate loglikelihood over\n        \"\"\"\n        return self.arguments if isinstance(self.arguments, tuple) else (self.arguments,)\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/models/mplug_owl_video.py", "content": "from accelerate import Accelerator, DistributedType, InitProcessGroupKwargs\nfrom accelerate.state import AcceleratorState\nfrom typing import List, Optional, Union, Tuple\nimport torch\nfrom transformers import AutoTokenizer\nfrom tqdm import tqdm\nfrom datetime import timedelta\n\nfrom lmms_eval.api.instance import Instance\nfrom lmms_eval.api.model import lmms\nfrom lmms_eval.api.registry import register_model\n\nfrom lmms_eval.models.mplug_owl_video.modeling_mplug_owl import MplugOwlForConditionalGeneration\nfrom lmms_eval.models.mplug_owl_video.processing_mplug_owl import MplugOwlImageProcessor, MplugOwlProcessor\n\n\nfrom loguru import logger\n\neval_logger = logger\n\n\n@register_model(\"mplug_owl_video\")\nclass mplug_Owl(lmms):\n    def __init__(\n        self,\n        pretrained: str = \"MAGAer13/mplug-owl-llama-7b-video\",\n        device: Optional[str] = \"cuda:0\",\n        dtype: Optional[Union[str, torch.dtype]] = \"auto\",\n        batch_size: Optional[Union[int, str]] = 1,\n        device_map=\"cuda:0\",\n        num_frames: Union[str, int] = 4,\n        **kwargs,\n    ) -> None:\n        \"\"\"\n        Install instructions:\n        1. Install lmms-eval\n        cd lmms-eval\n        pip install -e .;\n        2. Install other packages with restricted versions\n        pip install av sentencepiece protobuf==3.20 transformers==4.28.1 einops;\n        \"\"\"\n        super().__init__()\n\n        accelerator_kwargs = InitProcessGroupKwargs(timeout=timedelta(weeks=52))\n        accelerator = Accelerator(kwargs_handlers=[accelerator_kwargs])\n        if accelerator.num_processes > 1:\n            self._device = torch.device(f\"cuda:{accelerator.local_process_index}\")\n            self.device_map = f\"cuda:{accelerator.local_process_index}\"\n        elif accelerator.num_processes == 1 and device_map == \"auto\":\n            self._device = torch.device(device)\n            self.device_map = device_map\n        else:\n            self._device = torch.device(f\"cuda:{accelerator.local_process_index}\")\n            self.device_map = f\"cuda:{accelerator.local_process_index}\"\n\n        # import pdb; pdb.set_trace()\n        # This is very slow. Their issue, not mine\n        # Also, keep transformers in version 4.28.1\n        # They put a Config object inside a config object, this is not acceptable\n        # for transformers == 4.39.1, object type not serializable\n        # Protobuf needs to be in 3.20.x otherwise error\n        # ヽ(｀Д´)ﾉ\n        self._model = MplugOwlForConditionalGeneration.from_pretrained(\n            pretrained,\n            torch_dtype=torch.bfloat16,\n        )\n        self.image_processor = MplugOwlImageProcessor.from_pretrained(pretrained)\n        self._tokenizer = AutoTokenizer.from_pretrained(pretrained)\n        self.processor = MplugOwlProcessor(self.image_processor, self.tokenizer)\n        self.model.eval()\n        self.batch_size_per_gpu = batch_size\n        self.num_frames = num_frames\n\n        self.model.to(self.device)\n\n        if accelerator.num_processes > 1:\n            assert accelerator.distributed_type in [DistributedType.FSDP, DistributedType.MULTI_GPU, DistributedType.DEEPSPEED], \"Unsupported distributed type provided. Only DDP and FSDP are supported.\"\n            # If you want to use DistributedType.DEEPSPEED, you have to run accelerate config before using the model\n            # Also, you have to select zero stage 0 (equivalent to DDP) in order to make the prepare model works\n            # I tried to set different parameters in the kwargs to let default zero 2 stage works, but it didn't work.\n            if accelerator.distributed_type == DistributedType.DEEPSPEED:\n                kwargs = {\n                    \"train_micro_batch_size_per_gpu\": self.batch_size_per_gpu,\n                    \"train_batch_size\": self.batch_size_per_gpu * accelerator.num_processes,\n                }\n                AcceleratorState().deepspeed_plugin.deepspeed_config_process(must_match=True, **kwargs)\n                eval_logger.info(\"Detected that you are using DistributedType.DEEPSPEED. Make sure you run `accelerate config` and set zero stage to 0\")\n            if accelerator.distributed_type == DistributedType.FSDP or accelerator.distributed_type == DistributedType.DEEPSPEED:\n                self._model = accelerator.prepare(self.model)\n            else:\n                self._model = accelerator.prepare_model(self.model, evaluation_mode=True)\n            self.accelerator = accelerator\n            if self.accelerator.is_local_main_process:\n                eval_logger.info(f\"Using {accelerator.num_processes} devices with data parallelism\")\n            self._rank = self.accelerator.local_process_index\n            self._world_size = self.accelerator.num_processes\n        else:\n            eval_logger.info(f\"Using single device: {self._device}\")\n            self.model.to(self._device)\n            self._rank = 0\n            self._world_size = 1\n\n    @property\n    def config(self):\n        # return the associated transformers.AutoConfig for the given pretrained model.\n        return self._config\n\n    @property\n    def tokenizer(self):\n        return self._tokenizer\n\n    @property\n    def model(self):\n        # returns the model, unwrapping it if using Accelerate\n        if hasattr(self, \"accelerator\"):\n            return self.accelerator.unwrap_model(self._model)\n        else:\n            return self._model\n\n    @property\n    def eot_token_id(self):\n        # we use EOT because end of *text* is more accurate for what we're doing than end of *sentence*\n        return self.tokenizer.eos_token_id\n\n    @property\n    def max_length(self):\n        return self._max_length\n\n    @property\n    def batch_size(self):\n        return self.batch_size_per_gpu\n\n    @property\n    def device(self):\n        return self._device\n\n    @property\n    def rank(self):\n        return self._rank\n\n    @property\n    def world_size(self):\n        return self._world_size\n\n    def flatten(self, input):\n        new_list = []\n        for i in input:\n            for j in i:\n                new_list.append(j)\n        return new_list\n\n    def format_prompt(self, question):\n        prompts = [f\" <|video|> Question : {question} Answer : \"]\n        return prompts\n\n    def generate_until(self, requests) -> List[str]:\n        res = []\n        pbar = tqdm(total=len(requests), disable=(self.rank != 0), desc=\"Model Responding\")\n\n        for contexts, gen_kwargs, doc_to_visual, doc_id, task, split in [reg.args for reg in requests]:\n            # encode, pad, and truncate contexts for this batch\n            visuals = [doc_to_visual(self.task_dict[task][split][doc_id])]\n            visuals = self.flatten(visuals)\n            inputs = self.processor(text=self.format_prompt(contexts), videos=visuals, num_frames=self.num_frames, return_tensors=\"pt\")\n            pixel_values_videos = inputs[\"video_pixel_values\"]\n            if pixel_values_videos.shape[2] != self.num_frames:\n                empty_frames = torch.zeros((1, pixel_values_videos.shape[1], self.num_frames - pixel_values_videos.shape[2], *pixel_values_videos.shape[3:]), dtype=pixel_values_videos.dtype)\n                pixel_values_videos = torch.cat([pixel_values_videos, empty_frames], dim=2)\n                inputs[\"video_pixel_values\"] = pixel_values_videos\n            inputs = {k: v.bfloat16() if v.dtype == torch.float else v for k, v in inputs.items()}\n            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n\n            if \"max_new_tokens\" in gen_kwargs:\n                gen_kwargs[\"max_length\"] = gen_kwargs[\"max_new_tokens\"]\n            if \"max_new_tokens\" not in gen_kwargs:\n                gen_kwargs[\"max_length\"] = 128\n            if \"do_sample\" not in gen_kwargs:\n                gen_kwargs[\"do_sample\"] = False\n            if \"top_k\" not in gen_kwargs:\n                gen_kwargs[\"top_k\"] = 1\n\n            generate_kwargs = {\"do_sample\": gen_kwargs[\"do_sample\"], \"top_k\": gen_kwargs[\"top_k\"], \"max_length\": gen_kwargs[\"max_length\"]}\n\n            with torch.no_grad():\n                outputs = self.model.generate(**inputs, **generate_kwargs)\n            sentence = self.tokenizer.decode(outputs.tolist()[0], skip_special_tokens=True)\n            pbar.update(1)\n            res.append(sentence)\n        pbar.close()\n        return res\n\n    def loglikelihood(self, requests: List[Instance]) -> List[Tuple[float, bool]]:\n        return super().loglikelihood(requests)\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/filters/decontamination.py", "content": "from lmms_eval.api.filter import Filter\n\n\nclass DecontaminationFilter(Filter):\n    \"\"\"\n    A filter which evaluates\n    \"\"\"\n\n    name = \"track_decontamination\"\n\n    def __init__(self, path) -> None:\n        \"\"\"\n\n        TODO: make sure only ever run one time on the train set (should this be cached as a class var? keyed by value for \"path\").\n        should further cache result on a given (task_name, doc_id)\n        \"\"\"\n        self._decontam_results = None\n\n    def apply(self, resps, docs) -> None:\n        \"\"\"\n        Return {\"no_contamination\", \"only_contamination\"} keys for the 2 different subsets\n        \"\"\"\n        pass\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/models/batch_gpt4.py", "content": "# Standard library imports\nfrom copy import deepcopy\nfrom io import BytesIO\nimport base64\n\nimport os\nimport time\nimport json\n\n# Related third-party imports\nfrom accelerate import Accelerator, DistributedType, InitProcessGroupKwargs\nfrom accelerate.state import AcceleratorState\nimport numpy as np\nfrom PIL import Image\nimport requests as url_requests\nfrom tqdm import tqdm\nfrom openai import OpenAI\n\n# Local application/library specific imports\nfrom lmms_eval.api.instance import Instance\nfrom lmms_eval.api.model import lmms\nfrom lmms_eval.api.registry import register_model\nfrom loguru import logger as eval_logger\n\n# Conditional imports\ntry:\n    from decord import VideoReader, cpu\nexcept ImportError:\n    eval_logger.warning(\"Decord is not installed. Video input will not be supported.\")\n\n# Constants and global configurations\nAPI_TYPE = os.getenv(\"API_TYPE\", \"openai\")\nNUM_SECONDS_TO_SLEEP = 5\n\nif API_TYPE == \"openai\":\n    API_URL = os.getenv(\"OPENAI_API_URL\", \"https://api.openai.com/v1/chat/completions\")\n    API_KEY = os.getenv(\"OPENAI_API_KEY\", \"YOUR_API_KEY\")\n    headers = {\n        \"Authorization\": f\"Bearer {API_KEY}\",\n        \"Content-Type\": \"application/json\",\n    }\nelif API_TYPE == \"azure\":\n    API_URL = os.getenv(\"AZURE_ENDPOINT\", \"https://api.cognitive.microsoft.com/sts/v1.0/issueToken\")\n    API_KEY = os.getenv(\"AZURE_API_KEY\", \"YOUR_API_KEY\")\n    headers = {\n        \"api-key\": API_KEY,\n        \"Content-Type\": \"application/json\",\n    }\nelse:\n    API_URL = \"YOUR_API_URL\"\n    API_KEY = \"YOUR_API_KEY\"\n\n\n@register_model(\"batch_gpt4\")\nclass BatchGPT4(lmms):\n    def __init__(\n        self,\n        model_version: str = \"gpt-4o\",\n        api_key: str = API_KEY,\n        api_url: str = API_URL,\n        modality: str = \"image\",\n        max_frames_num: int = 10,\n        timeout: int = 120,\n        **kwargs,\n    ) -> None:\n        super().__init__()\n        # Manually set a image token for GPT4V so that we can search for it\n        # and split the text and image\n        # Here we just use the same token as llava for convenient\n        self.model_version = model_version\n        self.modality = modality\n        self.max_frames_num = max_frames_num\n        self.image_token = \"<image>\"\n        self.timeout = timeout\n\n        self.api_key = api_key\n        self.api_url = api_url\n        self.client = OpenAI(api_key=api_key)\n\n        accelerator = Accelerator()\n        assert accelerator.state.local_process_index == 0, \"BatchGPT4 does not support distributed inference.\"\n        assert accelerator.state.num_processes == 1, \"BatchGPT4 does not support distributed inference.\"\n\n    # Function to encode the image\n    def encode_image(self, image: Image):\n        output_buffer = BytesIO()\n        image.save(output_buffer, format=\"PNG\")\n        byte_data = output_buffer.getvalue()\n        base64_str = base64.b64encode(byte_data).decode(\"utf-8\")\n        return base64_str\n\n    # Function to encode the video\n    def encode_video(self, video_path, for_get_frames_num):\n        vr = VideoReader(video_path, ctx=cpu(0))\n        total_frame_num = len(vr)\n        uniform_sampled_frames = np.linspace(0, total_frame_num - 1, for_get_frames_num, dtype=int)\n        frame_idx = uniform_sampled_frames.tolist()\n        frames = vr.get_batch(frame_idx).asnumpy()\n\n        base64_frames = []\n        for frame in frames:\n            img = Image.fromarray(frame)\n            output_buffer = BytesIO()\n            img.save(output_buffer, format=\"PNG\")\n            byte_data = output_buffer.getvalue()\n            base64_str = base64.b64encode(byte_data).decode(\"utf-8\")\n            base64_frames.append(base64_str)\n\n        return base64_frames\n\n    def flatten(self, input):\n        new_list = []\n        for i in input:\n            for j in i:\n                new_list.append(j)\n        return new_list\n\n    def generate_until(self, requests):\n        # Prepare the batch requests data\n        requests_data = {}\n        pbar = tqdm(total=len(requests), disable=(self.rank != 0), desc=\"Batch Preparing\")\n        for idx, (contexts, gen_kwargs, doc_to_visual, doc_id, task, split) in enumerate([reg.args for reg in requests]):\n            visuals = [doc_to_visual(self.task_dict[task][split][doc_id])]\n            visuals = self.flatten(visuals)\n            imgs = []\n            for visual in visuals:\n                if self.modality == \"image\":\n                    img = self.encode_image(visual)\n                    imgs.append(img)\n                elif self.modality == \"video\":\n                    frames = self.encode_video(visual, self.max_frames_num)\n                    imgs.extend(frames)\n\n            messages = []\n            if self.image_token not in contexts:\n                messages.append({\"role\": \"user\", \"content\": contexts})\n                for img in imgs:\n                    messages.append({\"role\": \"user\", \"content\": f\"data:image/jpeg;base64,{img}\"})\n            else:\n                contexts_split = contexts.split(self.image_token)\n                for idx, context in enumerate(contexts_split):\n                    if idx < len(imgs):\n                        messages.append({\"role\": \"user\", \"content\": context})\n                        messages.append({\"role\": \"user\", \"content\": f\"data:image/jpeg;base64,{imgs[idx]}\"})\n                if len(contexts_split) > len(imgs):\n                    messages.append({\"role\": \"user\", \"content\": contexts_split[-1]})\n\n            requests_data[f\"request-{idx}\"] = {\"model\": self.model_version, \"messages\": messages, \"max_tokens\": gen_kwargs.get(\"max_new_tokens\", 1024)}\n            pbar.update(1)\n\n        file_path = os.getenv(\"HF_HOME\", \"~/.cache/huggingface\") + f\"/batchinput_{len(requests_data)}.jsonl\"\n        file_path = self.create_batch_input_file(requests_data, file_path)\n        file_id = self.upload_input_file(file_path)\n\n        batch_response = self.create_batch(file_id, metadata={\"description\": \"Batch Processing for GPT-4\"})\n        batch_status = self.check_batch_status(batch_response.id)\n        while True:\n            batch_status = self.check_batch_status(batch_response.id)\n            if batch_status.status == \"completed\":\n                eval_logger.info(\"Batch processing completed.\")\n                batch_results = self.retrieve_batch_results(batch_status.output_file_id)\n                res = [result[\"response\"][\"choices\"][0][\"message\"][\"content\"] for result in json.loads(batch_results)]\n                return res\n            elif batch_status.status == \"failed\":\n                eval_logger.info(\"Batch processing failed.\")\n                res = [\"Batch failed\"] * len(requests)\n                return res\n            else:\n                eval_logger.info(f\"Batch status: {batch_status.status}. Retrying in {NUM_SECONDS_TO_SLEEP} seconds.\")\n                time.sleep(NUM_SECONDS_TO_SLEEP)\n\n    def loglikelihood(self, requests):\n        # TODO\n        assert False, \"GPT4V not support\"\n\n    def create_batch_input_file(self, requests_data, file_path=\"batchinput.jsonl\"):\n        with open(file_path, \"w\") as file:\n            for request_id, data in requests_data.items():\n                json_record = json.dumps({\"custom_id\": request_id, \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": data})\n                file.write(json_record + \"\\n\")\n        return file_path\n\n    def upload_input_file(self, file_path):\n        with open(file_path, \"rb\") as file:\n            response = self.client.files.create(file=file, purpose=\"batch\")\n        return response.id\n\n    def create_batch(self, file_id, metadata=None):\n        if metadata is None:\n            metadata = {}\n        response = self.client.batches.create(input_file_id=file_id, endpoint=\"/v1/chat/completions\", completion_window=\"24h\", metadata=metadata)\n        return response\n\n    def check_batch_status(self, batch_id):\n        return self.client.batches.retrieve(batch_id)\n\n    def retrieve_batch_results(self, file_id):\n        return self.client.files.content(file_id)\n\n    def cancel_batch(self, batch_id):\n        return self.client.batches.cancel(batch_id)\n\n    def list_batches(self, limit=10):\n        return self.client.batches.list(limit=limit)\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/models/gemini_api.py", "content": "import io\nimport os\nimport time\n\nimport json\n\nfrom PIL import Image\nfrom typing import List, Tuple\nfrom tqdm import tqdm\nfrom lmms_eval.api.registry import register_model\nfrom lmms_eval.api.model import lmms\nfrom lmms_eval.api.instance import Instance\nfrom accelerate import Accelerator, DistributedType\n\nfrom loguru import logger as eval_logger\n\ntry:\n    import google.generativeai as genai\n    from google.generativeai.types import HarmCategory, HarmBlockThreshold\n\n    NUM_SECONDS_TO_SLEEP = 30\n    GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n    genai.configure(api_key=GOOGLE_API_KEY)\n\nexcept Exception as e:\n    eval_logger.error(f\"Error importing generativeai: {str(e)}\")\n    genai = None\n\n\n@register_model(\"gemini_api\")\nclass GeminiAPI(lmms):\n    def __init__(\n        self,\n        model_version: str = \"gemini-1.5-pro\",\n        modality: str = \"image\",\n        timeout: int = 120,\n        continual_mode: bool = False,\n        response_persistent_folder: str = None,  # We will cache the Gemini API response in this path and use it for future requests\n        **kwargs,\n    ) -> None:\n        super().__init__()\n        self.model_version = model_version\n        self.timeout = timeout\n        self.model = genai.GenerativeModel(model_version)\n        self.continual_mode = continual_mode\n        if self.continual_mode and response_persistent_folder is None:\n            raise ValueError(\"Continual mode requires a persistent path for the response. We will cache the Gemini API response in this path and use it for future requests. Please provide a valid path.\")\n        self.response_persistent_folder = response_persistent_folder\n        if not os.path.exists(self.response_persistent_folder):\n            os.makedirs(self.response_persistent_folder)\n        self.response_persistent_file = os.path.join(self.response_persistent_folder, f\"{self.model_version}_response.json\")\n\n        if os.path.exists(self.response_persistent_file):\n            with open(self.response_persistent_file, \"r\") as f:\n                self.response_cache = json.load(f)\n            self.cache_mode = \"resume\"\n        else:\n            self.response_cache = {}\n            self.cache_mode = \"start\"\n\n        accelerator = Accelerator()\n        if accelerator.num_processes > 1:\n            assert self.continual_mode is False, \"Continual mode is not supported with distributed inference.\"\n            assert accelerator.distributed_type in [DistributedType.FSDP, DistributedType.MULTI_GPU, DistributedType.DEEPSPEED], \"Unsupported distributed type provided. Only DDP and FSDP are supported.\"\n            self.accelerator = accelerator\n            if self.accelerator.is_local_main_process:\n                eval_logger.info(f\"Using {accelerator.num_processes} devices with data parallelism\")\n            self._rank = self.accelerator.local_process_index\n            self._world_size = self.accelerator.num_processes\n        else:\n            self.accelerator = accelerator\n            self._rank = self.accelerator.local_process_index\n            self._world_size = self.accelerator.num_processes\n\n        self.device = self.accelerator.device\n\n        self.modality = modality\n\n    def flatten(self, input):\n        new_list = []\n        for i in input:\n            for j in i:\n                new_list.append(j)\n        return new_list\n\n    def get_image_size(self, image):\n        # Create a BytesIO object to store the image bytes\n        img_byte_array = io.BytesIO()\n\n        # Save the image to the BytesIO object\n        image.save(img_byte_array, format=\"PNG\")\n\n        # Get the size of the BytesIO object\n        img_size = img_byte_array.tell()\n\n        return img_size\n\n    def encode_video(self, video_path):\n        uploaded_obj = genai.upload_file(path=video_path)\n        time.sleep(5)\n        return uploaded_obj\n\n    def convert_video(self, images):\n        for idx, img in enumerate(images):\n            if self.modality == \"video\" and isinstance(img, str):\n                try:\n                    images[idx] = self.encode_video(img)\n                except Exception as e:\n                    eval_logger.error(f\"Error converting video: {str(e)}\")\n        return images\n\n    def generate_until(self, requests) -> List[str]:\n        res = []\n        pbar = tqdm(total=len(requests), disable=(self.rank != 0), desc=\"Model Responding\")\n\n        def get_uuid(task, split, doc_id):\n            return f\"{task}___{split}___{doc_id}\"\n\n        for contexts, gen_kwargs, doc_to_visual, doc_id, task, split in [reg.args for reg in requests]:\n            if self.continual_mode is True and self.cache_mode == \"resume\":\n                doc_uuid = get_uuid(task, split, doc_id)\n                if doc_uuid in self.response_cache:\n                    content = self.response_cache[doc_uuid]\n                    if content:\n                        res.append(content)\n                        pbar.update(1)\n                        continue\n\n            if \"max_new_tokens\" not in gen_kwargs:\n                gen_kwargs[\"max_new_tokens\"] = 1024\n            if \"temperature\" not in gen_kwargs:\n                gen_kwargs[\"temperature\"] = 0\n\n            config = genai.GenerationConfig(\n                max_output_tokens=gen_kwargs[\"max_new_tokens\"],\n                temperature=gen_kwargs[\"temperature\"],\n            )\n\n            visuals = [doc_to_visual(self.task_dict[task][split][doc_id])]\n            visuals = self.flatten(visuals)\n            visuals = self.convert_video(visuals)\n\n            message = [contexts] + visuals\n\n            for attempt in range(5):\n                try:\n                    content = self.model.generate_content(\n                        message,\n                        generation_config=config,\n                        safety_settings={\n                            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n                            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n                            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n                            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n                        },\n                    )\n                    content = content.text\n                    break\n                except Exception as e:\n                    eval_logger.info(f\"Attempt {attempt + 1} failed with error: {str(e)}\")\n                    if isinstance(e, ValueError):\n                        try:\n                            eval_logger.info(f\"Prompt feed_back: {content.prompt_feedback}\")\n                            content = \"\"\n                            break\n                        except Exception:\n                            pass\n                    if attempt < 5 - 1:  # If we have retries left, sleep and then continue to next attempt\n                        time.sleep(NUM_SECONDS_TO_SLEEP)\n                    else:  # If this was the last attempt, log and return empty\n                        eval_logger.error(f\"All 5 attempts failed. Last error message: {str(e)}\")\n                        content = \"\"\n            res.append(content)\n            pbar.update(1)\n\n            if self.continual_mode is True:  # Cache the response\n                doc_uuid = get_uuid(task, split, doc_id)\n                self.response_cache[doc_uuid] = content\n                with open(self.response_persistent_file, \"w\") as f:\n                    json.dump(self.response_cache, f)\n\n        pbar.close()\n        return res\n\n    def loglikelihood(self, requests: List[Instance]) -> List[Tuple[float, bool]]:\n        # TODO\n        assert False, \"Gemini API not support\"\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/models/mplug_owl_video/modeling_mplug_owl.py", "content": "# coding=utf-8\n# Copyright 2022 x-plug The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" PyTorch MplugOwl model. \"\"\"\n\nimport math\nfrom typing import Any, Optional, Tuple, Union\nfrom loguru import logger\n\ntry:\n    from flash_attn.flash_attn_interface import flash_attn_unpadded_func\n\n    flash_attn_func = flash_attn_unpadded_func\nexcept:\n    flash_attn_func = None\n    logger.warning(\"Error importing flash_attn in mplug_owl. Please install flash-attn first.\")\n\nimport math\nfrom dataclasses import dataclass\nfrom typing import Any, Optional, Tuple, Union\n\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\nimport einops\n\nfrom transformers.modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, BaseModelOutputWithPastAndCrossAttentions\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\nfrom transformers.utils import (\n    ModelOutput,\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    replace_return_docstrings,\n)\nfrom transformers.models.auto import AutoModelForCausalLM\nfrom .configuration_mplug_owl import MplugOwlConfig, MplugOwlVisionConfig, MplugOwlVisualAbstractorConfig\n\n_CHECKPOINT_FOR_DOC = \"MAGAer13/mplug-owl-llama-7b\"\n_CONFIG_FOR_DOC = \"MplugOwlConfig\"\n\n\nMPLUG_OWL_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"MAGAer13/mplug-owl-llama-7b\",\n    # See all MplugOwl models at https://huggingface.co/models?filter=mplug_owl\n]\n\n\n@dataclass\nclass MplugOwlForConditionalGenerationModelOutput(ModelOutput):\n    \"\"\"\n    Class defining the outputs of [`MPlugOwlForConditionalGeneration`].\n\n    Args:\n        loss (`torch.FloatTensor`, *optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n            Language modeling loss from the language model.\n        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n            Prediction scores of the language modeling head of the language model.\n        vision_outputs (`BaseModelOutputWithPooling`):\n            Outputs of the vision encoder.\n\n        language_model_outputs (`CausalLMOutputWithPast` or `Seq2SeqLMOutput`):\n            Outputs of the language model.\n    \"\"\"\n\n    loss: Optional[Tuple[torch.FloatTensor]] = None\n    logits: Optional[Tuple[torch.FloatTensor]] = None\n    vision_outputs: Optional[torch.FloatTensor] = None\n    language_model_outputs: Optional[Tuple[torch.FloatTensor]] = None\n\n    def to_tuple(self) -> Tuple[Any]:\n        return tuple(self[k] if k not in [\"vision_outputs\", \"language_model_outputs\"] else getattr(self, k).to_tuple() for k in self.keys())\n\n\ndef get_ltor_masks_and_position_ids_from_embeddings(data):\n    \"\"\"Build masks and position id for left to right model.\"\"\"\n\n    # Extract batch size and sequence length.\n    micro_batch_size, seq_length = data.size()[:2]\n\n    # Attention mask (lower triangular).\n    att_mask_batch = 1\n    attention_mask = torch.tril(torch.ones((att_mask_batch, seq_length, seq_length), device=data.device)).view(att_mask_batch, 1, seq_length, seq_length)\n\n    # Loss mask.\n    loss_mask = torch.ones(data.size()[:2], dtype=torch.float, device=data.device)\n\n    # Position ids.\n    position_ids = torch.arange(seq_length, dtype=torch.long, device=data.device)\n    position_ids = position_ids.unsqueeze(0).expand_as(data[..., 0])\n\n    # Convert attention mask to binary:\n    attention_mask = attention_mask < 0.5\n\n    return attention_mask, loss_mask, position_ids\n\n\nclass MplugOwlVisionEmbeddings(nn.Module):\n    def __init__(self, config: MplugOwlVisionConfig):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.image_size = config.image_size\n        self.patch_size = config.patch_size\n\n        self.cls_token = nn.Parameter(torch.randn(1, 1, self.hidden_size))\n\n        self.patch_embed = nn.Conv2d(\n            in_channels=3,\n            out_channels=self.hidden_size,\n            kernel_size=self.patch_size,\n            stride=self.patch_size,\n            bias=False,\n        )\n\n        self.num_patches = (self.image_size // self.patch_size) ** 2\n\n        self.position_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, self.hidden_size))\n\n        self.pre_layernorm = LayerNormFp32(self.hidden_size, eps=config.layer_norm_eps)\n\n    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n        # [B, C, T, H, W] or [B, C, H, W]\n        batch_size = pixel_values.size(0)\n        T = pixel_values.size(2) if pixel_values.dim() > 4 else 1\n        if T > 1:\n            pixel_values = einops.rearrange(pixel_values, \"b c t h w -> (b t) c h w\")\n        image_embeds = self.patch_embed(pixel_values)\n        image_embeds = image_embeds.flatten(2).transpose(1, 2)\n\n        class_embeds = self.cls_token.expand(batch_size * T, 1, -1).to(image_embeds.dtype)\n        embeddings = torch.cat([class_embeds, image_embeds], dim=1)\n        embeddings = embeddings + self.position_embedding[:, : embeddings.size(1)].to(image_embeds.dtype)\n        embeddings = self.pre_layernorm(embeddings)\n        embeddings = einops.rearrange(embeddings, \"(b t) n d -> b t n d\", b=batch_size)\n        return embeddings\n\n\nclass LayerNormFp32(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm to handle fp16 (by casting to float32 and back).\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def forward(self, x: torch.Tensor):\n        output = torch.nn.functional.layer_norm(\n            x.float(),\n            self.normalized_shape,\n            self.weight.float() if self.weight is not None else None,\n            self.bias.float() if self.bias is not None else None,\n            self.eps,\n        )\n        return output.type_as(x)\n\n\nclass QuickGELU(nn.Module):\n    def forward(self, x: torch.Tensor):\n        return x * torch.sigmoid(1.702 * x)\n\n\nclass MplugOwlVisionLocalTemporal(nn.Module):\n    def __init__(self, config):\n        super(MplugOwlVisionLocalTemporal, self).__init__()\n\n        self.image_size = config.image_size\n        self.patch_size = config.patch_size\n        self.num_patches = 1 + (self.image_size // self.patch_size) ** 2\n        self.hidden_size = config.hidden_size\n        d_bottleneck = self.hidden_size // 2\n\n        self.ln = LayerNormFp32(self.hidden_size)\n        self.down_proj = nn.Conv3d(self.hidden_size, d_bottleneck, kernel_size=1, stride=1, padding=0)\n        self.conv = nn.Conv3d(d_bottleneck, d_bottleneck, kernel_size=(3, 1, 1), stride=1, padding=(1, 0, 0), groups=d_bottleneck)\n        self.up_proj = nn.Conv3d(d_bottleneck, self.hidden_size, kernel_size=1, stride=1, padding=0)\n\n        nn.init.constant_(self.up_proj.weight, 0)\n        nn.init.constant_(self.up_proj.bias, 0)\n\n        self.activation_func = QuickGELU()\n\n    def forward(self, x):\n        # [b, t, s, c]\n        T = x.size(1)\n        H = int((self.num_patches - 1) ** 0.5)\n        cls_token, x = x[:, :, 0:1], x[:, :, 1:]\n        x = self.ln(x)\n        x = einops.rearrange(x, \"b t (h w) c -> b c t h w\", h=H)\n        x = self.down_proj(x)\n        if self.conv.weight.dtype == torch.bfloat16:\n            x = torch.nn.functional.conv3d(x.half(), self.conv.weight.half(), bias=self.conv.bias.half(), stride=1, padding=(1, 0, 0), groups=self.conv.weight.shape[0]).to(cls_token.dtype)\n        else:\n            x = self.conv(x)\n        x = self.activation_func(x)\n        x = self.up_proj(x)\n        x = einops.rearrange(x, \"b c t h w -> b t (h w) c\")\n        x = torch.cat([cls_token, x], dim=2)\n        return x\n\n\nclass MplugOwlVisionAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        if self.head_dim * self.num_heads != self.hidden_size:\n            raise ValueError(f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size} and `num_heads`:\" f\" {self.num_heads}).\")\n        self.scale = self.head_dim**-0.5\n        self.dropout = nn.Dropout(config.attention_dropout)\n\n        self.query_key_value = nn.Linear(self.hidden_size, 3 * self.hidden_size)\n        self.dense = nn.Linear(self.hidden_size, self.hidden_size)\n\n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        head_mask: Optional[torch.Tensor] = None,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n\n        bsz, seq_len, embed_dim = hidden_states.size()\n\n        mixed_qkv = self.query_key_value(hidden_states)\n\n        mixed_qkv = mixed_qkv.reshape(bsz, seq_len, self.num_heads, 3, embed_dim // self.num_heads).permute(3, 0, 2, 1, 4)  # [3, b, np, sq, hn]\n        query_states, key_states, value_states = (\n            mixed_qkv[0],\n            mixed_qkv[1],\n            mixed_qkv[2],\n        )\n        # if self.config.use_flash_attn and flash_attn_func is not None:\n        if False:\n            # [b*sq, np, hn]\n            query_states = query_states.permute(0, 2, 1, 3).contiguous()\n            query_states = query_states.view(query_states.size(0) * query_states.size(1), query_states.size(2), -1)\n\n            key_states = key_states.permute(0, 2, 1, 3).contiguous()\n            key_states = key_states.view(key_states.size(0) * key_states.size(1), key_states.size(2), -1)\n\n            value_states = value_states.permute(0, 2, 1, 3).contiguous()\n            value_states = value_states.view(value_states.size(0) * value_states.size(1), value_states.size(2), -1)\n\n            cu_seqlens = torch.arange(0, (bsz + 1) * seq_len, step=seq_len, dtype=torch.int32, device=query_states.device)\n\n            context_layer = flash_attn_func(\n                query_states,\n                key_states,\n                value_states,\n                cu_seqlens,\n                cu_seqlens,\n                seq_len,\n                seq_len,\n                self.dropout if self.training else 0.0,\n                softmax_scale=self.scale,\n                causal=False,\n                return_attn_probs=False,\n            )\n            # [b*sq, np, hn] => [b, sq, np, hn]\n            context_layer = context_layer.view(bsz, seq_len, context_layer.size(1), context_layer.size(2))\n        else:\n            # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n            attention_scores = torch.matmul(query_states, key_states.transpose(-1, -2))\n\n            attention_scores = attention_scores * self.scale\n\n            # Normalize the attention scores to probabilities.\n            attention_probs = torch.softmax(attention_scores, dim=-1)\n\n            # This is actually dropping out entire tokens to attend to, which might\n            # seem a bit unusual, but is taken from the original Transformer paper.\n            attention_probs = self.dropout(attention_probs)\n\n            # Mask heads if we want to\n            if head_mask is not None:\n                attention_probs = attention_probs * head_mask\n\n            context_layer = torch.matmul(attention_probs, value_states).permute(0, 2, 1, 3)\n\n        new_context_layer_shape = context_layer.size()[:-2] + (self.hidden_size,)\n        context_layer = context_layer.reshape(new_context_layer_shape)\n\n        output = self.dense(context_layer)\n\n        outputs = (output, attention_probs) if output_attentions else (output, None)\n\n        return outputs\n\n\nclass MplugOwlMLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.activation_fn = QuickGELU()\n        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        hidden_states = self.fc1(hidden_states)\n        hidden_states = self.activation_fn(hidden_states)\n        hidden_states = self.fc2(hidden_states)\n        return hidden_states\n\n\nclass MplugOwlVisionEncoderLayer(nn.Module):\n    def __init__(self, config: MplugOwlVisionConfig):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        self.temporal = MplugOwlVisionLocalTemporal(config)\n        self.self_attn = MplugOwlVisionAttention(config)\n        self.input_layernorm = LayerNormFp32(self.hidden_size, eps=config.layer_norm_eps)\n        self.mlp = MplugOwlMLP(config)\n        self.post_attention_layernorm = LayerNormFp32(self.hidden_size, eps=config.layer_norm_eps)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: torch.Tensor,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[torch.FloatTensor]:\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, time, seq_len, embed_dim)`\n            attention_mask (`torch.FloatTensor`): attention mask of size\n                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n                `(config.encoder_attention_heads,)`.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n        \"\"\"\n        B, T = hidden_states.size(0), hidden_states.size(1)\n        if T > 1:\n            hidden_states = hidden_states + self.temporal(hidden_states)\n        hidden_states = einops.rearrange(hidden_states, \"b t n d -> (b t) n d\")\n\n        residual = hidden_states\n\n        hidden_states = self.input_layernorm(hidden_states)\n        hidden_states, attn_weights = self.self_attn(\n            hidden_states=hidden_states,\n            head_mask=attention_mask,\n            output_attentions=output_attentions,\n        )\n        hidden_states = hidden_states + residual\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n\n        hidden_states = hidden_states + residual\n        hidden_states = einops.rearrange(hidden_states, \"(b t) n d -> b t n d\", b=B)\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (attn_weights,)\n\n        return outputs\n\n\nclass MplugOwlPreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n\n    config_class = MplugOwlConfig\n    base_model_prefix = \"mplug_owl\"\n    supports_gradient_checkpointing = True\n    _keys_to_ignore_on_load_missing = [\n        r\"position_ids\",\n        r\"language_model.encoder.embed_tokens.weight\",\n        r\"language_model.decoder.embed_tokens.weight\",\n        r\"language_model.lm_head.weight\",\n    ]\n    _no_split_modules = [\n        \"MplugOwlVisionEncoderLayer\",\n        \"LlamaDecoderLayer\",\n        \"MplugOwlVisualAbstractorLayer\",\n        \"LlamaForCausalLM\",\n        \"Parameter\",\n    ]\n    _keep_in_fp32_modules = [\"wo\"]\n\n    def _init_weights(self, module):\n        \"\"\"Initialize the weights\"\"\"\n        factor = self.config.initializer_range\n        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Embedding) or isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=factor)\n            if hasattr(module, \"bias\") and module.bias is not None:\n                module.bias.data.zero_()\n\n        if isinstance(module, MplugOwlVisionEmbeddings):\n            if hasattr(self.config, \"vision_config\"):\n                factor = self.config.vision_config.initializer_range\n            nn.init.trunc_normal_(module.position_embedding, mean=0.0, std=factor)\n            nn.init.trunc_normal_(module.cls_token, mean=0.0, std=factor)\n\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        elif isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n        elif isinstance(module, nn.Parameter):\n            raise ValueError\n            nn.init.trunc_normal_(module.data, mean=0.0, std=factor)\n\n    def _set_gradient_checkpointing(self, module, value=False):\n        if isinstance(module, MplugOwlVisionEncoder):\n            module.gradient_checkpointing = value\n\n\nMPLUG_OWL_START_DOCSTRING = r\"\"\"\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Parameters:\n        config ([`MplugOwlConfig`]): Model configuration class with all the parameters of the model.\n            Initializing with a config file does not load the weights associated with the model, only the\n            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\n\nMPLUG_OWL_VISION_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n            Pixel values. Pixel values can be obtained using [`MplugOwlProcessor`]. See [`MplugOwlProcessor.__call__`] for\n            details.\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"\n\nMPLUG_OWL_TEXT_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n            it. Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details. [What are input IDs?](../glossary#input-ids)\n        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n            [What are attention masks?](../glossary#attention-mask)\n        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n            Indices of decoder input sequence tokens in the vocabulary.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are decoder input IDs?](../glossary#decoder-input-ids)\n\n            T5 uses the `pad_token_id` as the starting token for `decoder_input_ids` generation. If `past_key_values`\n            is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).\n\n            To know more on how to prepare `decoder_input_ids` for pretraining take a look at [T5\n            Training](./t5#training).\n        decoder_attention_mask (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n            be used by default.\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"\n\nMPLUG_OWL_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n            Pixel values. Pixel values can be obtained using [`MplugOwlProcessor`]. See [`MplugOwlProcessor.__call__`] for\n            details.\n\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Indices of input sequence tokens in the vocabulary of the language model. Input tokens can optionally be\n            provided to serve as text prompt, which the language model can continue.\n\n            Indices can be obtained using [`MplugOwlProcessor`]. See [`MplugOwlProcessor.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n\n        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n            Indices of decoder input sequence tokens in the vocabulary of the language model. Only relevant in case an\n            encoder-decoder language model (like T5) is used.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details. [What are decoder input IDs?](../glossary#decoder-input-ids)\n\n        decoder_attention_mask (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n            be used by default.\n\n            Only relevant in case an encoder-decoder language model (like T5) is used.\n\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"\n\n\nclass MplugOwlVisionEncoder(nn.Module):\n    \"\"\"\n    Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n    [`MplugOwlVisionEncoderLayer`].\n\n    Args:\n        config (`MplugOwlVisionConfig`):\n            The corresponding vision configuration for the `MplugOwlEncoder`.\n    \"\"\"\n\n    def __init__(self, config: MplugOwlVisionConfig):\n        super().__init__()\n        self.config = config\n        self.layers = nn.ModuleList([MplugOwlVisionEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n        self.gradient_checkpointing = False\n\n    def forward(\n        self,\n        inputs_embeds,\n        attention_mask: Optional[torch.Tensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, BaseModelOutput]:\n        r\"\"\"\n        Args:\n            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n                Embedded representation of the inputs. Should be float, not int tokens.\n            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n\n                [What are attention masks?](../glossary#attention-mask)\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            output_hidden_states (`bool`, *optional*):\n                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n                for more detail.\n            return_dict (`bool`, *optional*):\n                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        encoder_states = () if output_hidden_states else None\n        all_attentions = () if output_attentions else None\n\n        hidden_states = inputs_embeds\n        for idx, encoder_layer in enumerate(self.layers):\n            if output_hidden_states:\n                encoder_states = encoder_states + (hidden_states,)\n            if self.gradient_checkpointing and self.training:\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        return module(*inputs, output_attentions)\n\n                    return custom_forward\n\n                layer_outputs = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(encoder_layer),\n                    hidden_states,\n                    attention_mask,\n                )\n            else:\n                layer_outputs = encoder_layer(\n                    hidden_states,\n                    attention_mask,\n                    output_attentions=output_attentions,\n                )\n\n            hidden_states = layer_outputs[0]\n\n            if output_attentions:\n                all_attentions = all_attentions + (layer_outputs[1],)\n\n        if output_hidden_states:\n            encoder_states = encoder_states + (hidden_states,)\n\n        if not return_dict:\n            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n        return BaseModelOutput(last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions)\n\n\nclass MplugOwlVisionModel(MplugOwlPreTrainedModel):\n    main_input_name = \"pixel_values\"\n    config_class = MplugOwlVisionConfig\n\n    def __init__(self, config: MplugOwlVisionConfig):\n        super().__init__(config)\n        self.config = config\n        self.hidden_size = config.hidden_size\n\n        self.embeddings = MplugOwlVisionEmbeddings(config)\n        self.encoder = MplugOwlVisionEncoder(config)\n        self.post_layernorm = LayerNormFp32(self.hidden_size, eps=config.layer_norm_eps)\n\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(MPLUG_OWL_VISION_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=MplugOwlVisionConfig)\n    def forward(\n        self,\n        pixel_values: Optional[torch.FloatTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n        r\"\"\"\n        Returns:\n\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if pixel_values is None:\n            raise ValueError(\"You have to specify pixel_values\")\n\n        hidden_states = self.embeddings(pixel_values)  # [B, T, N, D]\n\n        encoder_outputs = self.encoder(\n            inputs_embeds=hidden_states,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        last_hidden_state = encoder_outputs[0]\n        last_hidden_state = self.post_layernorm(last_hidden_state)\n\n        pooled_output = last_hidden_state[:, :, 0, :].mean(1)\n        pooled_output = self.post_layernorm(pooled_output)\n\n        if not return_dict:\n            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n\n        return BaseModelOutputWithPooling(\n            last_hidden_state=last_hidden_state,\n            pooler_output=pooled_output,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n        )\n\n    def get_input_embeddings(self):\n        return self.embeddings\n\n\nclass MplugOwlVisualAbstractorMLP(nn.Module):\n    def __init__(self, config: MplugOwlVisualAbstractorConfig):\n        super().__init__()\n        self.config = config\n        in_features = config.hidden_size\n        hidden_features = config.intermediate_size\n        if hidden_features != 2816:\n            hidden_features = int(2 * hidden_features / 3)\n            multiple_of = 256\n            hidden_features = multiple_of * ((hidden_features + multiple_of - 1) // multiple_of)\n        self.act = nn.SiLU()\n\n        self.w1 = nn.Linear(in_features, hidden_features)\n        self.w2 = nn.Linear(hidden_features, in_features)\n        self.w3 = nn.Linear(in_features, hidden_features)\n        self.ffn_ln = LayerNormFp32(hidden_features, eps=config.layer_norm_eps)\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        hidden_states = self.act(self.w1(hidden_states)) * self.w3(hidden_states)\n        hidden_states = self.ffn_ln(hidden_states)\n        hidden_states = self.w2(hidden_states)\n        return hidden_states\n\n\nclass MplugOwlVisualAbstractorMultiHeadAttention(nn.Module):\n    def __init__(self, config: MplugOwlVisualAbstractorConfig):\n        super().__init__()\n        self.config = config\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\"The hidden size (%d) is not a multiple of the number of attention heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.encoder_hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.encoder_hidden_size, self.all_head_size)\n\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n        self.save_attention = False\n\n    def save_attn_gradients(self, attn_gradients):\n        self.attn_gradients = attn_gradients\n\n    def get_attn_gradients(self):\n        return self.attn_gradients\n\n    def save_attention_map(self, attention_map):\n        self.attention_map = attention_map\n\n    def get_attention_map(self):\n        return self.attention_map\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_value=None,\n        output_attentions=False,\n    ):\n        # If this is instantiated as a cross-attention module, the keys\n        # and values come from an encoder; the attention mask needs to be\n        # such that the encoder's padding tokens are not attended to.\n        key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n        value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n        attention_mask = encoder_attention_mask\n\n        mixed_query_layer = self.query(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n\n        past_key_value = (key_layer, value_layer)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n            attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n\n        if self.save_attention:\n            self.save_attention_map(attention_probs)\n            attention_probs.register_hook(self.save_attn_gradients)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs_dropped = self.dropout(attention_probs)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attention_probs_dropped = attention_probs_dropped * head_mask\n\n        context_layer = torch.matmul(attention_probs_dropped, value_layer)\n\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n\n        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n\n        outputs = outputs + (past_key_value,)\n        return outputs\n\n\nclass MplugOwlVisualAbstractorCrossOutput(nn.Module):\n    def __init__(self, config: MplugOwlVisualAbstractorConfig):\n        super().__init__()\n        dim = config.hidden_size\n        self.out_proj = nn.Linear(dim, dim, bias=True)\n        self.norm2 = LayerNormFp32(dim)\n        self.mlp = MplugOwlVisualAbstractorMLP(config)\n\n    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n        input_tensor = input_tensor + self.out_proj(hidden_states)\n        input_tensor = input_tensor + self.mlp(self.norm2(input_tensor))\n        return input_tensor\n\n\nclass MplugOwlVisualAbstractorAttention(nn.Module):\n    def __init__(self, config: MplugOwlVisualAbstractorConfig):\n        super().__init__()\n        self.attention = MplugOwlVisualAbstractorMultiHeadAttention(config)\n        self.output = MplugOwlVisualAbstractorCrossOutput(config)\n        self.pruned_heads = set()\n        self.norm1 = LayerNormFp32(config.hidden_size)\n        self.normk = LayerNormFp32(config.hidden_size)\n\n    def prune_heads(self, heads):\n        if len(heads) == 0:\n            return\n        heads, index = find_pruneable_heads_and_indices(heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads)\n\n        # Prune linear layers\n        self.attention.query = prune_linear_layer(self.attention.query, index)\n        self.attention.key = prune_linear_layer(self.attention.key, index)\n        self.attention.value = prune_linear_layer(self.attention.value, index)\n        self.output.dense = prune_linear_layer(self.output.out_proj, index, dim=1)\n\n        # Update hyper params and store pruned heads\n        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n        self.pruned_heads = self.pruned_heads.union(heads)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[torch.Tensor]:\n        # HACK we apply norm on q and k\n        hidden_states = self.norm1(hidden_states)\n        encoder_hidden_states = self.normk(encoder_hidden_states)\n        encoder_hidden_states = torch.cat([hidden_states, encoder_hidden_states], dim=1)\n        encoder_attention_mask = torch.cat([attention_mask, encoder_attention_mask], dim=-1)\n        self_outputs = self.attention(\n            hidden_states,\n            attention_mask,\n            head_mask,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            past_key_value,\n            output_attentions,\n        )\n        attention_output = self.output(self_outputs[0], hidden_states)\n        # add attentions if we output them\n        outputs = (attention_output,) + self_outputs[1:]\n        return outputs\n\n\nclass MplugOwlVisualAbstractorLayer(nn.Module):\n    def __init__(self, config, layer_idx):\n        super().__init__()\n        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n        self.seq_len_dim = 1\n\n        self.layer_idx = layer_idx\n\n        self.crossattention = MplugOwlVisualAbstractorAttention(config)\n        self.has_cross_attention = True\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        output_attentions=False,\n    ):\n        if encoder_hidden_states is None:\n            raise ValueError(\"encoder_hidden_states must be given for cross-attention layers\")\n        cross_attention_outputs = self.crossattention(\n            hidden_states,\n            attention_mask,\n            head_mask,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            output_attentions=output_attentions,\n        )\n        query_attention_output = cross_attention_outputs[0]\n\n        outputs = (query_attention_output,)\n        return outputs\n\n\nclass MplugOwlVisualAbstractorEncoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.layers = nn.ModuleList([MplugOwlVisualAbstractorLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)])\n        self.gradient_checkpointing = False\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_values=None,\n        output_attentions=False,\n        output_hidden_states=False,\n        return_dict=True,\n    ):\n        all_hidden_states = () if output_hidden_states else None\n\n        for i in range(self.config.num_hidden_layers):\n            layer_module = self.layers[i]\n            if output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states,)\n\n            layer_head_mask = head_mask[i] if head_mask is not None else None\n            past_key_value = past_key_values[i] if past_key_values is not None else None\n\n            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        return module(*inputs, past_key_value, output_attentions)\n\n                    return custom_forward\n\n                layer_outputs = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(layer_module),\n                    hidden_states,\n                    attention_mask,\n                    layer_head_mask,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                )\n            else:\n                layer_outputs = layer_module(\n                    hidden_states,\n                    attention_mask,\n                    layer_head_mask,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                    output_attentions,\n                )\n\n            hidden_states = layer_outputs[0]\n\n        return BaseModelOutput(\n            last_hidden_state=hidden_states,\n        )\n\n\nclass MplugOwlVisualAbstractorModel(MplugOwlPreTrainedModel):\n    def __init__(self, config: MplugOwlVisualAbstractorConfig, language_hidden_size):\n        super().__init__(config)\n        self.config = config\n\n        self.encoder = MplugOwlVisualAbstractorEncoder(config)\n        self.visual_fc = torch.nn.Linear(config.hidden_size, language_hidden_size)\n        self.temporal_visual_fc = torch.nn.Linear(config.hidden_size, language_hidden_size)\n        self.vit_eos = torch.nn.Parameter(torch.randn(1, 1, language_hidden_size))\n        nn.init.trunc_normal_(self.vit_eos, mean=0.0, std=self.config.initializer_range)\n        self.post_init()\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    def get_extended_attention_mask(\n        self,\n        attention_mask: torch.Tensor,\n        input_shape: Tuple[int],\n        device: torch.device,\n    ) -> torch.Tensor:\n        \"\"\"\n        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n\n        Arguments:\n            attention_mask (`torch.Tensor`):\n                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n            input_shape (`Tuple[int]`):\n                The shape of the input to the model.\n            device: (`torch.device`):\n                The device of the input to the model.\n\n        Returns:\n            `torch.Tensor` The extended attention mask, with a the same dtype as `attention_mask.dtype`.\n        \"\"\"\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        if attention_mask.dim() == 3:\n            extended_attention_mask = attention_mask[:, None, :, :]\n        elif attention_mask.dim() == 2:\n            # Provided a padding mask of dimensions [batch_size, seq_length]\n            # - the model is an encoder, so make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n            extended_attention_mask = attention_mask[:, None, None, :]\n        else:\n            raise ValueError(\"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(input_shape, attention_mask.shape))\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n        return extended_attention_mask\n\n    def forward(\n        self,\n        query_embeds,\n        temporal_query_embeds=None,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_values=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, `optional`):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, `optional`):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of:\n            shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`): Contains precomputed key and\n            value hidden states of the attention blocks. Can be used to speed up decoding. If `past_key_values` are\n            used, the user can optionally input only the last `decoder_input_ids` (those that don't have their past key\n            value states given to this model) of shape `(batch_size, 1)` instead of all `decoder_input_ids` of shape\n            `(batch_size, sequence_length)`.\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        T = encoder_hidden_states.size(1)\n        if T == 1 or temporal_query_embeds is None:\n            embedding_output = query_embeds\n        else:\n            embedding_output = torch.cat([query_embeds, temporal_query_embeds], dim=1)\n        input_shape = embedding_output.size()[:-1]\n        batch_size, seq_length = input_shape\n        device = embedding_output.device\n\n        encoder_hidden_states = einops.rearrange(encoder_hidden_states, \"b t n d -> b (t n) d\")\n\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        if attention_mask is None:\n            attention_mask = torch.ones((embedding_output.shape[0], embedding_output.shape[1]), dtype=torch.long, device=embedding_output.device)\n        extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape, device)\n\n        # If a 2D or 3D attention mask is provided for the cross-attention\n        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n        if encoder_hidden_states is not None:\n            if type(encoder_hidden_states) == list:\n                encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states[0].size()\n            else:\n                (\n                    encoder_batch_size,\n                    encoder_sequence_length,\n                    _,\n                ) = encoder_hidden_states.size()\n            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n\n            if type(encoder_attention_mask) == list:\n                encoder_extended_attention_mask = [self.invert_attention_mask(mask) for mask in encoder_attention_mask]\n            elif encoder_attention_mask is None:\n                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n                encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n            else:\n                encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n        else:\n            encoder_extended_attention_mask = None\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n        encoder_outputs = self.encoder(\n            embedding_output,\n            attention_mask=extended_attention_mask,\n            head_mask=head_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_extended_attention_mask,\n            past_key_values=past_key_values,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        sequence_output = encoder_outputs[0]\n        pooled_output = sequence_output[:, 0, :]\n\n        if T == 1 or temporal_query_embeds is None:\n            temporal_sequence_output = None\n        else:\n            temporal_sequence_output = sequence_output[:, query_embeds.size(1) :]\n            sequence_output = sequence_output[:, : query_embeds.size(1)]\n\n        sequence_output = self.visual_fc(sequence_output)\n        if temporal_sequence_output is not None:\n            sequence_output += self.temporal_visual_fc(temporal_sequence_output)\n        sequence_output = torch.cat([sequence_output, self.vit_eos.repeat(sequence_output.shape[0], 1, 1)], dim=1)\n\n        return BaseModelOutputWithPooling(\n            last_hidden_state=sequence_output,\n            pooler_output=pooled_output,\n            hidden_states=encoder_outputs.hidden_states,\n        )\n\n\n@add_start_docstrings(\n    \"\"\"\n    mPLUG-Owl Model for generating text and image features. The model consists of a vision encoder, Querying Transformer\n    (Q-Former) and a language model.\n    \"\"\",\n    MPLUG_OWL_START_DOCSTRING,\n)\nclass MplugOwlModel(MplugOwlPreTrainedModel):\n    config_class = MplugOwlConfig\n    main_input_name = \"pixel_values\"\n\n    def __init__(self, config: MplugOwlConfig, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n\n        self.vision_model = MplugOwlVisionModel(config.vision_config)\n\n        self.query_tokens = nn.Parameter(torch.zeros(1, config.num_query_tokens, config.visual_abstractor_config.hidden_size))\n        self.temporal_query_tokens = nn.Parameter(torch.zeros(1, config.num_query_tokens, config.visual_abstractor_config.hidden_size))\n        self.abstractor = MplugOwlVisualAbstractorModel(config.visual_abstractor_config, config.text_config.hidden_size)\n\n        # if config.use_decoder_only_language_model:\n        # from llama.modeling_llama import LlamaForCausalLM\n        language_model = AutoModelForCausalLM.from_config(config.text_config)\n        # else:\n        #     language_model = AutoModelForSeq2SeqLM.from_config(config.text_config)\n        self.language_model = language_model\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.language_model.get_input_embeddings()\n\n    def set_input_embeddings(self, value):\n        self.language_model.set_input_embeddings(value)\n\n    def set_output_embeddings(self, new_embeddings):\n        self.language_model.set_output_embeddings(new_embeddings)\n\n    def get_output_embeddings(self) -> nn.Module:\n        return self.language_model.get_output_embeddings()\n\n    def get_encoder(self):\n        return self.language_model.get_encoder()\n\n    def get_decoder(self):\n        return self.language_model.get_decoder()\n\n    def _tie_weights(self):\n        if not self.config.use_decoder_only_language_model:\n            self.language_model.encoder.embed_tokens = self.language_model.shared\n            self.language_model.decoder.embed_tokens = self.language_model.shared\n\n    def get_text_features(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        decoder_input_ids: Optional[torch.Tensor] = None,\n        decoder_attention_mask: Optional[torch.Tensor] = None,\n        labels: Optional[torch.Tensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ):\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if self.config.use_decoder_only_language_model:\n            text_outputs = self.language_model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n        else:\n            inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n\n            text_outputs = self.language_model(\n                inputs_embeds=inputs_embeds,\n                attention_mask=attention_mask,\n                decoder_input_ids=decoder_input_ids,\n                decoder_attention_mask=decoder_attention_mask,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n                labels=labels,\n            )\n\n        return text_outputs\n\n    def get_image_features(\n        self,\n        pixel_values: Optional[torch.FloatTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ):\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        vision_outputs = self.vision_model(\n            pixel_values=pixel_values,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        return vision_outputs\n\n\ndef get_media_indices(my_list):\n    if isinstance(my_list, torch.Tensor):\n        my_list = my_list.cpu().tolist()\n    result = []\n    for i in range(len(my_list)):\n        if i == 0 and my_list[i] < 0:\n            result.append(i)\n        elif my_list[i] != my_list[i - 1] and my_list[i] < 0:\n            result.append(i)\n    return result\n\n\ndef get_media_types(tensors, positions):\n    if isinstance(tensors, torch.Tensor):\n        tensors = tensors.cpu().tolist()\n    result = []\n    for pos in positions:\n        result.append(tensors[pos])\n    return result\n\n\n@add_start_docstrings(\n    \"\"\"\n    mPLUG-Owl Model for generating text given an image and an optional text prompt.\n    \"\"\",\n    MPLUG_OWL_START_DOCSTRING,\n)\nclass MplugOwlForConditionalGeneration(MplugOwlPreTrainedModel):\n    config_class = MplugOwlConfig\n    main_input_name = \"pixel_values\"\n\n    def __init__(self, config: MplugOwlConfig):\n        super().__init__(config)\n\n        self.vision_model = MplugOwlVisionModel(config.vision_config)\n\n        self.query_tokens = nn.Parameter(torch.zeros(1, config.num_query_tokens, config.visual_abstractor_config.hidden_size))\n        self.temporal_query_tokens = nn.Parameter(torch.zeros(1, config.num_query_tokens, config.visual_abstractor_config.hidden_size))\n        self.abstractor = MplugOwlVisualAbstractorModel(config.visual_abstractor_config, config.text_config.hidden_size)\n\n        # if config.use_decoder_only_language_model:\n        # from llama.modeling_llama import LlamaForCausalLM\n        language_model = AutoModelForCausalLM.from_config(config.text_config)\n        # else:\n        #     language_model = AutoModelForSeq2SeqLM.from_config(config.text_config)\n        self.language_model = language_model\n\n        # Initialize weights and apply final processing\n        self.post_init()\n        self.main_input_name = \"input_ids\"\n        from transformers import GenerationConfig\n\n        self.generation_config = GenerationConfig(max_length=512, do_sample=True, top_k=3, pad_token_id=0, unk_token_id=0, bos_token_id=1, eos_token_id=2)\n\n        # Hack Bloom\n        if config.text_config.model_type == \"bloom\":\n            bound_method = bloom_forward.__get__(self.language_model.transformer, self.language_model.transformer.__class__)\n            setattr(self.language_model.transformer, \"forward\", bound_method)\n\n    def get_input_embeddings(self):\n        return self.language_model.get_input_embeddings()\n\n    def set_input_embeddings(self, value):\n        self.language_model.set_input_embeddings(value)\n\n    def set_output_embeddings(self, new_embeddings):\n        self.language_model.set_output_embeddings(new_embeddings)\n\n    def get_output_embeddings(self) -> nn.Module:\n        return self.language_model.get_output_embeddings()\n\n    def get_encoder(self):\n        return self.language_model.get_encoder()\n\n    def get_decoder(self):\n        return self.language_model.get_decoder()\n\n    def _tie_weights(self):\n        if not self.config.use_decoder_only_language_model:\n            self.language_model.encoder.embed_tokens = self.language_model.shared\n            self.language_model.decoder.embed_tokens = self.language_model.shared\n\n    def _preprocess_accelerate(self):\n        r\"\"\"\n        Some pre-processing hacks to make the model `accelerate` compatible. Check\n        https://github.com/huggingface/transformers/pull/21707 for more details.\n        \"\"\"\n        hf_device_map = self.hf_device_map\n\n        if len(hf_device_map) > 1 and \"language_model\" not in hf_device_map and torch.cuda.device_count() > 1:\n            # warn users about unexpected behavior when using multi-GPU + mPLUG-Owl + `accelerate`.\n            logger.warning(\n                \"The `language_model` is not in the `hf_device_map` dictionary and you are running your script\"\n                \" in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`.\"\n                \" Please pass a `device_map` that contains `language_model` to remove this warning.\"\n                \" Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for\"\n                \" more details on creating a `device_map` for large models.\",\n            )\n\n        if hasattr(self.language_model, \"_hf_hook\"):\n            self.language_model._hf_hook.io_same_device = True  # For `generate` compatibility\n\n    @add_start_docstrings_to_model_forward(MPLUG_OWL_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=MplugOwlForConditionalGenerationModelOutput, config_class=MplugOwlVisionConfig)\n    def forward(\n        self,\n        pixel_values: torch.FloatTensor,\n        video_pixel_values: torch.FloatTensor,\n        input_ids: torch.FloatTensor,\n        num_images,\n        num_videos,\n        non_padding_mask: Optional[torch.LongTensor] = None,\n        non_media_mask: Optional[torch.LongTensor] = None,\n        prompt_mask: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.LongTensor] = None,\n        decoder_input_ids: Optional[torch.LongTensor] = None,\n        decoder_attention_mask: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        labels: Optional[torch.LongTensor] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, MplugOwlForConditionalGenerationModelOutput]:\n        r\"\"\"\n        Returns:\n\n        Examples:\n\n        Image captioning (without providing a text prompt):\n\n        ```python\n        >>> from PIL import Image\n        >>> import requests\n        >>> from transformers import MplugOwlProcessor, MplugOwlForConditionalGeneration\n        >>> import torch\n\n        >>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n        >>> processor = MplugOwlProcessor.from_pretrained(\"x-plug/x_plug-llama-7b\")\n        >>> model = MplugOwlForConditionalGeneration.from_pretrained(\n        ...     \"x-plug/x_plug-llama-7b\", torch_dtype=torch.float16\n        ... )\n        >>> model.to(device)  # doctest: +IGNORE_RESULT\n\n        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n        >>> image = Image.open(requests.get(url, stream=True).raw)\n\n        >>> inputs = processor(images=image, return_tensors=\"pt\").to(device, torch.float16)\n\n        >>> generated_ids = model.generate(**inputs)\n        >>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n        >>> print(generated_text)\n        two cats laying on a couch\n        ```\n\n        Visual question answering (prompt = question):\n\n        ```python\n        >>> from PIL import Image\n        >>> import requests\n        >>> from transformers import MplugOwlProcessor, MplugOwlForConditionalGeneration\n        >>> import torch\n\n        >>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n        >>> processor = MplugOwlProcessor.from_pretrained(\"x-plug/x_plug-llama-7b\")\n        >>> model = MplugOwlForConditionalGeneration.from_pretrained(\n        ...     \"x-plug/x_plug-llama-7b\", torch_dtype=torch.float16\n        ... )\n        >>> model.to(device)  # doctest: +IGNORE_RESULT\n\n        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n        >>> image = Image.open(requests.get(url, stream=True).raw)\n\n        >>> prompt = \"Question: how many cats are there? Answer:\"\n        >>> inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n\n        >>> generated_ids = model.generate(**inputs)\n        >>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n        >>> print(generated_text)\n        two\n        ```\"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # get text embedding\n        text_tokens_ = input_ids.clone()\n        batch_size = input_ids.shape[0]\n        # labels = text_tokens_[:, 1:].clone().contiguous()\n\n        media_token_indices = [\n            # [:-1] since we would not use the last token for embedding\n            get_media_indices(text_tokens_[i][:-1])\n            for i in range(batch_size)\n        ]\n\n        media_token_types = [get_media_types(text_tokens_[i][:-1], media_token_indices[i]) for i in range(batch_size)]\n\n        text_tokens_[text_tokens_ < 0] = 1  # Not used\n        # text_tokens = text_tokens_[:, :-1].contiguous()\n        text_embeds = self.get_input_embeddings()(text_tokens_)  # Temporally Embedding\n\n        if pixel_values is not None:\n            image_embeds = self.vision_model(pixel_values, return_dict=True).last_hidden_state\n\n            image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n            query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n            temporal_query_tokens = self.temporal_query_tokens.expand(image_embeds.shape[0], -1, -1)\n\n            query_features = self.abstractor(\n                query_embeds=query_tokens,\n                encoder_hidden_states=image_embeds,\n                encoder_attention_mask=image_attention_mask,\n            )[\"last_hidden_state\"]\n            img_seq_length = query_features.shape[1]\n\n        if video_pixel_values is not None:\n            video_embeds = self.vision_model(video_pixel_values, return_dict=True).last_hidden_state\n\n            video_attention_mask = torch.ones(video_embeds.size()[:-1], dtype=torch.long, device=video_embeds.device)\n            video_attention_mask = einops.rearrange(video_attention_mask, \"b t n -> b (t n)\")\n            query_tokens = self.query_tokens.expand(video_embeds.shape[0], -1, -1)\n            temporal_query_tokens = self.temporal_query_tokens.expand(video_embeds.shape[0], -1, -1)\n\n            video_query_features = self.abstractor(\n                query_embeds=query_tokens,\n                temporal_query_embeds=temporal_query_tokens,\n                encoder_hidden_states=video_embeds,\n                encoder_attention_mask=video_attention_mask,\n            )[\"last_hidden_state\"]\n            vid_seq_length = video_query_features.shape[1]\n\n        num_images_per_sample = num_images.long().cpu().tolist()\n        num_videos_per_sample = num_videos.long().cpu().tolist()\n\n        text_chunk_embeds = []\n        img_idx = 0\n        for b in range(batch_size):\n            start = 0\n            result = []\n            if len(media_token_indices[b]) > 0:\n                for i, pos in enumerate(media_token_indices[b]):\n                    if pos > start:\n                        result.append(text_embeds[b, start:pos])\n                    result.append(query_features[img_idx + i])\n                    start = pos + img_seq_length\n            if start < text_embeds.shape[1]:\n                result.append(text_embeds[b, start:])\n\n            img_idx += num_images_per_sample[b]\n            text_chunk_embeds.append(torch.cat(result, dim=0))\n\n        # Actual Input Embeddings\n        input_embeds = torch.stack(text_chunk_embeds, dim=0)\n\n        # if pixel_values is None and self.language_model.is_gradient_checkpointing:\n        #     # Hack here when gradient checkpoint is enable.\n        #     # Keep the compute graph static\n        #     image_embeds = self.vision_model(torch.zeros(1,3,224,224,device=input_embeds.device,dtype=input_embeds.dtype), return_dict=True).last_hidden_state\n        #     query_tokens = self.query_tokens.expand(\n        #         image_embeds.shape[0], -1, -1)\n        #     query_features = self.abstractor(query_embeds=query_tokens,\n        #     encoder_hidden_states=image_embeds,)['last_hidden_state']\n\n        #     input_embeds = input_embeds + query_features.mean()*0\n\n        # Create causal mask and position ids\n        _, loss_mask, position_ids = get_ltor_masks_and_position_ids_from_embeddings(input_embeds)\n\n        # Calculate the loss_mask\n        non_padding_mask = non_padding_mask.long()\n        non_media_mask = non_media_mask.long()\n        prompt_mask = prompt_mask.long()  # TODO How to deal with prompt mask\n        # from icecream import ic\n        # non_padding_mask = non_padding_mask[:,:-1]\n        # non_media_mask = non_media_mask[:,:-1]\n        # prompt_mask = prompt_mask[:,:-1]\n        # attention_mask = attention_mask[:,:-1]\n        loss_mask = loss_mask[:, :-1]\n\n        loss_mask = loss_mask * non_padding_mask * non_media_mask * prompt_mask\n        labels[:, 1:][loss_mask != 1] = -100\n        # Forward into GPT\n        outputs = self.language_model(\n            inputs_embeds=input_embeds,\n            attention_mask=attention_mask,\n            labels=labels,\n            return_dict=return_dict,\n            output_attentions=self.config.output_attentions,\n        )\n        # outputs.loss = (outputs.loss * loss_mask.view(-1)\n        #                 ).sum()/loss_mask.sum()\n        return outputs\n\n    @torch.no_grad()\n    def generate(\n        self,\n        pixel_values: torch.FloatTensor = None,\n        video_pixel_values: torch.FloatTensor = None,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.LongTensor] = None,\n        isdecoder=True,\n        **generate_kwargs,\n    ) -> torch.LongTensor:\n        \"\"\"\n        Overrides `generate` function to be able to use the model as a conditional generator.\n\n        Args:\n            pixel_values (`torch.FloatTensor` of shape (batch_size, num_channels, height, width)):\n                Input images to be processed.\n            input_ids (`torch.LongTensor` of shape (batch_size, sequence_length), *optional*):\n                The sequence used as a prompt for the generation.\n            attention_mask (`torch.LongTensor` of shape (batch_size, sequence_length), *optional*):\n                Mask to avoid performing attention on padding token indices\n\n        Returns:\n            captions (list): A list of strings of length batch_size * num_captions.\n        \"\"\"\n        if input_ids is None:\n            return self.language_model.generate(attention_mask=attention_mask, **generate_kwargs)\n\n        if attention_mask is None:\n            attention_mask = input_ids.new_ones(*input_ids.shape)\n\n        batch_size = input_ids.size(0)\n        media_token_indices = [get_media_indices(input_ids[i]) for i in range(batch_size)]\n        media_token_types = [get_media_types(input_ids[i], media_token_indices[i]) for i in range(batch_size)]\n        num_images_per_sample = [len([y for y in x if y == -1]) for x in media_token_types]\n        num_videos_per_sample = [len([y for y in x if y < -1]) for x in media_token_types]\n        input_ids = input_ids.clone()  # prevent inplace modify\n        input_ids[input_ids < 0] = 0  # Not used\n\n        if hasattr(self, \"hf_device_map\"):\n            # preprocess for `accelerate`\n            self._preprocess_accelerate()\n        batch_size = input_ids.shape[0]\n        # get text embedding\n        inputs_embeds = self.get_input_embeddings()(input_ids)\n        if hasattr(self.language_model, \"transformer\") and hasattr(self.language_model.transformer, \"word_embeddings_layernorm\"):\n            inputs_embeds = self.language_model.transformer.word_embeddings_layernorm(inputs_embeds)\n        # get visual embedding\n        if pixel_values is not None:\n            pixel_values = pixel_values.to(input_ids.device)\n            with torch.no_grad():\n                image_embeds = self.vision_model(pixel_values, return_dict=True).last_hidden_state\n                image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n                query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n                query_outputs = self.abstractor(\n                    query_embeds=query_tokens,\n                    encoder_hidden_states=image_embeds,\n                    encoder_attention_mask=image_attention_mask,\n                    return_dict=True,\n                )\n                query_output = query_outputs[\"last_hidden_state\"]\n                image_embeds = query_output\n            img_seq_length = image_embeds.shape[1]\n\n        if video_pixel_values is not None:\n            video_pixel_values = video_pixel_values.to(input_ids.device)\n            with torch.no_grad():\n                video_embeds = self.vision_model(video_pixel_values, return_dict=True).last_hidden_state\n                video_attention_mask = torch.ones(video_embeds.size()[:-1], dtype=torch.long, device=video_embeds.device)\n                video_attention_mask = einops.rearrange(video_attention_mask, \"b t n -> b (t n)\")\n                query_tokens = self.query_tokens.expand(video_embeds.shape[0], -1, -1)\n                temporal_query_tokens = self.temporal_query_tokens.expand(video_embeds.shape[0], -1, -1)\n                query_outputs = self.abstractor(\n                    query_embeds=query_tokens,\n                    temporal_query_embeds=temporal_query_tokens,\n                    encoder_hidden_states=video_embeds,\n                    encoder_attention_mask=video_attention_mask,\n                    return_dict=True,\n                )\n                query_output = query_outputs[\"last_hidden_state\"]\n                video_embeds = query_output\n            vid_seq_length = video_embeds.shape[1]\n\n        # ===================\n        # Get actual input embeddings\n        # ===================\n        text_chunk_embeds = []\n        text_chunk_attns = []\n        img_idx = 0\n        vid_idx = 0\n\n        for b in range(batch_size):\n            start = 0\n            result = []\n            result_attn = []\n            for i, pos in enumerate(media_token_indices[b]):\n                curr_image_idx, curr_video_idx = 0, 0\n                if pos > start:\n                    result.append(inputs_embeds[b, start:pos])\n                    result_attn.append(attention_mask[b, start:pos])\n                if media_token_types[b][i] == -1:\n                    result.append(image_embeds[img_idx + curr_image_idx])\n                    result_attn.append(torch.ones(image_embeds[img_idx + curr_image_idx].shape[0], device=inputs_embeds.device))\n                    start = pos + img_seq_length\n                    curr_image_idx += 1\n                else:\n                    result.append(video_embeds[vid_idx + curr_video_idx])\n                    result_attn.append(torch.ones(video_embeds[img_idx + curr_video_idx].shape[0], device=inputs_embeds.device))\n                    start = pos + vid_seq_length\n                    curr_video_idx += 1\n            if start < inputs_embeds.shape[1]:\n                result.append(inputs_embeds[b, start:])\n                result_attn.append(attention_mask[b, start:])\n\n            img_idx += num_images_per_sample[b]\n            vid_idx += num_videos_per_sample[b]\n            text_chunk_embeds.append(torch.cat(result, dim=0))\n            text_chunk_attns.append(torch.cat(result_attn, dim=0))\n        inputs_embeds = torch.stack(text_chunk_embeds, dim=0)\n        attention_mask = torch.stack(text_chunk_attns, dim=0)\n\n        outputs = self.language_model.generate(\n            inputs_embeds=inputs_embeds,\n            # input_ids=input_ids,\n            attention_mask=attention_mask,\n            **generate_kwargs,\n        )\n\n        return outputs\n\n    def prepare_inputs_for_generation(self, input_ids, pixel_values=None, video_pixel_values=None, past_key_values=None, attention_mask=None, **model_kwargs):\n        input_shape = input_ids.shape\n        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n        if attention_mask is None:\n            attention_mask = input_ids.new_ones(input_shape)\n\n        # # cut decoder_input_ids if past_key_values is used\n        # if past_key_values is not None:\n        #     input_ids = input_ids[:, -1:]\n\n        return {\n            \"input_ids\": input_ids,\n            \"pixel_values\": pixel_values,\n            \"video_pixel_values\": video_pixel_values,\n            \"attention_mask\": attention_mask,\n            # \"past_key_values\": past_key_values,\n            # \"encoder_hidden_states\": model_kwargs.get(\"encoder_hidden_states\", None),\n            # \"encoder_attention_mask\": model_kwargs.get(\"encoder_attention_mask\", None),\n            \"is_decoder\": True,\n        }\n\n\ndef bloom_forward(\n    self,\n    input_ids: Optional[torch.LongTensor] = None,\n    past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,\n    attention_mask: Optional[torch.Tensor] = None,\n    head_mask: Optional[torch.LongTensor] = None,\n    inputs_embeds: Optional[torch.LongTensor] = None,\n    use_cache: Optional[bool] = None,\n    output_attentions: Optional[bool] = None,\n    output_hidden_states: Optional[bool] = None,\n    return_dict: Optional[bool] = None,\n    **deprecated_arguments,\n) -> Union[Tuple[torch.Tensor, ...], BaseModelOutputWithPastAndCrossAttentions]:\n    if deprecated_arguments.pop(\"position_ids\", False) is not False:\n        # `position_ids` could have been `torch.Tensor` or `None` so defaulting pop to `False` allows to detect if users were passing explicitly `None`\n        warnings.warn(\n            \"`position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore\" \" passing `position_ids`.\",\n            FutureWarning,\n        )\n    if len(deprecated_arguments) > 0:\n        raise ValueError(f\"Got unexpected arguments: {deprecated_arguments}\")\n\n    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n    output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n    use_cache = use_cache if use_cache is not None else self.config.use_cache\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    if input_ids is not None and inputs_embeds is not None:\n        raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n    elif input_ids is not None:\n        batch_size, seq_length = input_ids.shape\n    elif inputs_embeds is not None:\n        batch_size, seq_length, _ = inputs_embeds.shape\n    else:\n        raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n    if past_key_values is None:\n        past_key_values = tuple([None] * len(self.h))\n\n    # Prepare head mask if needed\n    # 1.0 in head_mask indicate we keep the head\n    # attention_probs has shape batch_size x num_heads x N x N\n    # head_mask has shape n_layer x batch x num_heads x N x N\n    head_mask = self.get_head_mask(head_mask, self.config.n_layer)\n\n    if inputs_embeds is None:\n        inputs_embeds = self.word_embeddings(input_ids)\n        inputs_embeds = self.word_embeddings_layernorm(inputs_embeds)\n\n    hidden_states = inputs_embeds\n\n    presents = () if use_cache else None\n    all_self_attentions = () if output_attentions else None\n    all_hidden_states = () if output_hidden_states else None\n\n    if self.gradient_checkpointing and self.training:\n        if use_cache:\n            logger.warning_once(\"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\")\n            use_cache = False\n\n    # Compute alibi tensor: check build_alibi_tensor documentation\n    seq_length_with_past = seq_length\n    past_key_values_length = 0\n    if past_key_values[0] is not None:\n        past_key_values_length = past_key_values[0][0].shape[2]\n        seq_length_with_past = seq_length_with_past + past_key_values_length\n    if attention_mask is None:\n        attention_mask = torch.ones((batch_size, seq_length_with_past), device=hidden_states.device)\n    else:\n        attention_mask = attention_mask.to(hidden_states.device)\n\n    alibi = self.build_alibi_tensor(attention_mask, self.num_heads, dtype=hidden_states.dtype)\n\n    causal_mask = self._prepare_attn_mask(\n        attention_mask,\n        input_shape=(batch_size, seq_length),\n        past_key_values_length=past_key_values_length,\n    )\n\n    for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):\n        if output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states,)\n\n        if self.gradient_checkpointing and self.training:\n\n            def create_custom_forward(module):\n                def custom_forward(*inputs):\n                    # None for past_key_value\n                    return module(*inputs, use_cache=use_cache, output_attentions=output_attentions)\n\n                return custom_forward\n\n            outputs = torch.utils.checkpoint.checkpoint(\n                create_custom_forward(block),\n                hidden_states,\n                alibi,\n                causal_mask,\n                layer_past,\n                head_mask[i],\n            )\n        else:\n            outputs = block(\n                hidden_states,\n                layer_past=layer_past,\n                attention_mask=causal_mask,\n                head_mask=head_mask[i],\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                alibi=alibi,\n            )\n\n        hidden_states = outputs[0]\n        if use_cache is True:\n            presents = presents + (outputs[1],)\n\n        if output_attentions:\n            all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n\n    # Add last hidden state\n    hidden_states = self.ln_f(hidden_states)\n\n    if output_hidden_states:\n        all_hidden_states = all_hidden_states + (hidden_states,)\n\n    if not return_dict:\n        return tuple(v for v in [hidden_states, presents, all_hidden_states, all_self_attentions] if v is not None)\n\n    return BaseModelOutputWithPastAndCrossAttentions(\n        last_hidden_state=hidden_states,\n        past_key_values=presents,\n        hidden_states=all_hidden_states,\n        attentions=all_self_attentions,\n    )\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/filters/extraction.py", "content": "import re\nimport sys\nimport unicodedata\nfrom lmms_eval.api.filter import Filter\n\n\nclass WhitespaceFilter(Filter):\n    \"\"\" \"\"\"\n\n    def __init__(self) -> None:\n        pass\n\n    def apply(self, resps, docs):\n        def filter_set(inst):\n            filtered_resp = []\n            for resp in inst:\n                if resp.startswith(\" \"):\n                    resp = resp[1:]\n\n                filtered_resp.append(resp)\n\n            return filtered_resp\n\n        filtered_resps = [filter_set(resp) for resp in resps]\n\n        return filtered_resps\n\n\nclass RegexFilter(Filter):\n    \"\"\" \"\"\"\n\n    def __init__(\n        self,\n        regex_pattern: str = r\"#### (\\-?[0-9\\.\\,]+)\",\n        group_select=0,\n        fallback: str = \"[invalid]\",\n    ) -> None:\n        \"\"\"\n        pass a string `regex` to run `re.compile(r\"regex\")` on.\n        `fallback` defines the output returned if no matches for the regex are located.\n        \"\"\"\n        self.regex_pattern = regex_pattern\n        self.regex = re.compile(regex_pattern)\n        self.group_select = group_select\n        self.fallback = fallback\n\n    def apply(self, resps, docs):\n        # here, we assume we have a list, in which each element is\n        # a list of model responses for some particular input/target pair.\n        # so we process each of these (same input/target response sets)\n        # independently (and keep them a list.)\n        def filter_set(inst):\n            filtered = []\n            for resp in inst:\n                match = self.regex.findall(resp)\n                if match:\n                    match = match[self.group_select]\n                    if isinstance(match, tuple):\n                        match = [m for m in match if m][0]\n                    match = match.strip()\n                else:\n                    match = self.fallback\n                filtered.append(match)\n            return filtered\n\n        # print(resps)\n        filtered_resps = list(map(lambda x: filter_set(x), resps))\n        # print(filtered_resps)\n\n        return filtered_resps\n\n\nclass MultiChoiceRegexFilter(RegexFilter):\n    \"\"\"\n    A filter used to extract a model's answer on multiple choice questions with\n    letter answers. assumes each document has a \"choices\" field\n    containing the list of answer choices and that the answer label symbols\n    are of the form (A), (B), (C), ... or A, B, C.\n    \"\"\"\n\n    def __init__(\n        self,\n        regex_pattern: str = r\"#### (\\-?[0-9\\.\\,]+)\",\n        group_select=0,\n        fallback: str = \"[invalid]\",\n        ignore_case=False,\n        ignore_punctuation=False,\n        regexes_to_ignore=None,\n    ) -> None:\n        \"\"\"\n        regex_pattern: The basic regex pattern to use. If fails to match, we will use the customized match procedure\n                        - step 1 : We parse the choices between ([A-Z])s then try to find these choices in the response.\n                        - step 2 : We parse the choice with regex :[\\s]*([A-?]), where ? varies by number of choices.\n        group_select: Selects the (group_select)th match from the findall result.\n        ignore_case: Ignores the case during step 1 matching\n        ignore_punctuation: Remove the punctuation during step 1 matching\n        regexes_to_ignore: Remove these regexes during step 1 matching\n        \"\"\"\n        super().__init__(regex_pattern, group_select, fallback)\n        self.ignore_case = ignore_case\n        self.ignore_punctuation = ignore_punctuation\n        self.regexes_to_ignore = regexes_to_ignore\n\n    def apply(self, resps, docs):\n        # here, we assume we have a list, in which each element is\n        # a list of model responses for some particular input/target pair.\n        # so we process each of these (same input/target response sets)\n        # independently (and keep them a list.)\n\n        def find_match(regex, resp, convert_dict={}):\n            match = regex.findall(resp)\n            if match:\n                match = match[self.group_select]\n                if isinstance(match, tuple):\n                    match = [m for m in match if m][0]\n                match = match.strip()\n                if match and match in convert_dict:\n                    match = convert_dict[match]\n            return match\n\n        punct_tbl = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith(\"P\"))\n\n        def filter_ignores(st):\n            if self.regexes_to_ignore is not None:\n                for s in self.regexes_to_ignore:\n                    st = re.sub(s, \"\", st)\n\n            if self.ignore_case:\n                st = st.lower()\n\n            if self.ignore_punctuation:\n                # https://stackoverflow.com/a/266162\n                st = st.translate(punct_tbl)\n            return st\n\n        filtered_resps = []\n\n        for r, doc in zip(resps, docs):\n            fallback_regexes = []\n            choice_to_alpha = {}\n            next_alpha = \"A\"\n\n            without_paren_fallback_regexes = []\n            without_paren_to_target = {}\n\n            choices = doc[\"choices\"]\n            for c in choices:\n                m = filter_ignores(c.strip())\n                fallback_regexes.append(f\"{re.escape(m)}\")\n                choice_to_alpha[m] = f\"({next_alpha})\"\n\n                without_paren_fallback_regexes.append(next_alpha)\n                without_paren_to_target[next_alpha] = f\"({next_alpha})\"\n\n                next_alpha = chr(ord(next_alpha) + 1)\n            fallback_regex = re.compile(\"|\".join(fallback_regexes))\n            without_paren_fallback_regex = \"|\".join(without_paren_fallback_regexes)\n            without_paren_fallback_regex = re.compile(f\":[\\s]*({without_paren_fallback_regex})\")\n\n            filtered = []\n            for resp in r:\n                match = find_match(self.regex, resp)\n                if not match:\n                    match = find_match(fallback_regex, filter_ignores(resp), choice_to_alpha)\n                    if not match:\n                        match = find_match(without_paren_fallback_regex, resp, without_paren_to_target)\n                if not match:\n                    match = self.fallback\n                filtered.append(match)\n            filtered_resps.append(filtered)\n\n        return filtered_resps\n\n\nclass ExtendedRegexFilter(RegexFilter):\n    punct_tbl = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith(\"P\"))\n\n    def __init__(\n        self,\n        regex_pattern: str = r\"#### (\\-?[0-9\\.\\,]+)\",\n        group_select=0,\n        fallback: str = \"[invalid]\",\n        ignore_case=False,\n        ignore_punctuation=False,\n        regexes_to_ignore=None,\n    ) -> None:\n        super().__init__(regex_pattern, group_select, fallback)\n        self.ignore_case = ignore_case\n        self.ignore_punctuation = ignore_punctuation\n        self.regexes_to_ignore = regexes_to_ignore\n\n    def filter_ignores(self, st):\n        if self.regexes_to_ignore is not None:\n            for s in self.regexes_to_ignore:\n                st = re.sub(s, \"\", st)\n\n        if self.ignore_case:\n            st = st.lower()\n\n        if self.ignore_punctuation:\n            # https://stackoverflow.com/a/266162\n            st = st.translate(self.punct_tbl)\n        return st\n\n    def find_match(self, regex, resp, convert_dict={}):\n        match = regex.findall(resp)\n        if match:\n            match = match[self.group_select]\n            if isinstance(match, tuple):\n                match = [m for m in match if m][0]\n            match = match.strip()\n            if match and match in convert_dict:\n                match = convert_dict[match]\n        return match\n\n\n# Designed for the AI2D/RealworldQA dataset\nclass SimpleMultiChoiceRegexFilter(ExtendedRegexFilter):\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        regex_pattern: The basic regex pattern to use. If fails to match, we will use the customized match procedure\n                        - step 1 : We parse the choices between ([A-Z])s then try to find these choices in the response.\n                        - step 2 : We parse the choice with regex :[\\s]*([A-?]), where ? varies by number of choices.\n        group_select: Selects the (group_select)th match from the findall result.\n        ignore_case: Ignores the case during step 1 matching\n        ignore_punctuation: Remove the punctuation during step 1 matching\n        regexes_to_ignore: Remove these regexes during step 1 matching\n        \"\"\"\n        super().__init__(*args, **kwargs)\n\n    def apply(self, resps, docs):\n        # here, we assume we have a list, in which each element is\n        # a list of model responses for some particular input/target pair.\n        # so we process each of these (same input/target response sets)\n        # independently (and keep them a list.)\n\n        filtered_resps = []\n\n        for r, doc in zip(resps, docs):\n            fallback_regexes = []\n            choice_to_alpha = {}\n            next_alpha = \"A\"\n\n            without_paren_fallback_regexes = []\n            without_paren_to_target = {}\n\n            # Regex to extract multiple choice options from the question\n            multiple_choices_regex = re.compile(r\"\\b([A-Z])\\.\\s+([^\\n]*)\")\n            matches = multiple_choices_regex.findall(doc[\"question\"])\n\n            # Build regex patterns and mappings for each choice\n            for m in matches:\n                choice_text = m[1].strip()\n                fallback_regexes.append(f\"{re.escape(choice_text)}\")\n                choice_to_alpha[choice_text] = next_alpha\n\n                next_alpha = chr(ord(next_alpha) + 1)\n\n            # Compile regex to match any of the extracted choices\n            fallback_regex = re.compile(\"|\".join(fallback_regexes))\n\n            # Process each response\n            filtered = []\n            for resp in r:\n                # Remove any punctuation and extra spaces\n                cleaned_resp = re.sub(r\"[^\\w\\s]\", \"\", resp).strip()\n                # Try to match cleaned response with the choice text\n                match = fallback_regex.search(cleaned_resp)\n                if match and match.group() in choice_to_alpha:\n                    # Map the matched choice text back to its corresponding letter\n                    filtered.append(choice_to_alpha[match.group()])\n                else:\n                    # If no match, return the cleaned response\n                    filtered.append(cleaned_resp)\n\n            filtered_resps.append(filtered[0])\n\n        return filtered_resps\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/models/llava_sglang.py", "content": "import torch\nimport random\n\ntorch.backends.cuda.matmul.allow_tf32 = True\n\n\nfrom tqdm import tqdm\nfrom datetime import timedelta\n\nfrom lmms_eval import utils\nfrom lmms_eval.api.instance import Instance\nfrom lmms_eval.api.model import lmms\nfrom lmms_eval.api.registry import register_model\n\nfrom typing import List, Optional, Union, Tuple\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport tempfile\n\nfrom loguru import logger as eval_logger\n\ntry:\n    import sglang as sgl\n    from sglang.lang.chat_template import get_chat_template\nexcept ImportError:\n    eval_logger.debug(\"SGLang is not installed. If you want to use llava_sglang, please install it using pip install 'sglang[all]' \")\n\nif torch.__version__ > \"2.1.2\":\n    best_fit_attn_implementation = \"sdpa\"\nelse:\n    best_fit_attn_implementation = \"eager\"\n\n\n@register_model(\"llava_sglang\")\nclass LlavaSglang(lmms):\n    \"\"\"\n    Llava Sglang Model\n    \"\"\"\n\n    def __init__(\n        self,\n        pretrained: str = \"liuhaotian/llava-v1.5-7b\",\n        tokenizer: str = \"llava-hf/llava-1.5-7b-hf\",\n        tp_size: int = 1,\n        parallel: Optional[Union[int, str]] = 64,\n        conv_template=\"vicuna_v1.1\",\n        **kwargs,\n    ) -> None:\n        super().__init__()\n        self.pretrained = pretrained\n        self.tokenizer = tokenizer\n        self.tp_size = tp_size\n        self.conv_template = conv_template\n        # torch.multiprocessing.set_start_method(\"spawn\")\n\n        # accelerator_kwargs = InitProcessGroupKwargs(timeout=timedelta(weeks=52))\n        # accelerator = Accelerator(kwargs_handlers=[accelerator_kwargs])\n        # assert accelerator.num_processes == 1, \"Llava-sglang does not support multi-processes yet (it does support tensor parallelism).\"\n        self._rank = 0\n        self._world_size = 1\n        self.parallel = parallel\n\n    def loglikelihood(self, requests: List[Instance]) -> List[Tuple[float, bool]]:\n        raise NotImplementedError(\"Llava-sglang does not support loglikelihood evaluation yet\")\n\n    def generate_until(self, requests: List[Instance]) -> List[str]:\n        torch.multiprocessing.set_start_method(\"spawn\", force=True)\n        runtime = sgl.Runtime(model_path=self.pretrained, tokenizer_path=self.tokenizer, tp_size=self.tp_size, port=random.randint(10000, 50000))\n        runtime.endpoint.chat_template = get_chat_template(self.conv_template)\n        sgl.set_default_backend(runtime)\n\n        @sgl.function\n        def image_qa(s, image_file, question):\n            s += sgl.user(sgl.image(image_file) + question)\n            s += sgl.assistant(sgl.gen(\"answer\"))\n\n        res = []\n\n        def _collate(x):\n            # the negative sign on len(toks) sorts descending - this has a few advantages:\n            # - time estimates will always be over not underestimates, which is more useful for planning\n            # - to know the size of a batch when going through the list, you know the first one is always the batch\n            #   padded context length. this is useful to simplify the batching logic and more importantly to make\n            #   automatic adaptive batches much much easier to implement\n            # - any OOMs will happen right away rather than near the end\n            toks = x[0].split(\" \")\n            return -len(toks), x[0]\n\n        # we group requests by their generation_kwargs,\n        # so that we don't try to execute e.g. greedy sampling and temp=0.8 sampling\n        # in the same batch.\n        re_ords = utils.Collator([reg.args for reg in requests], _collate, grouping=True)\n        chunks = re_ords.get_batched(n=self.parallel, batch_fn=None)\n        num_iters = len(requests) // self.parallel if len(requests) % self.parallel == 0 else len(requests) // self.parallel + 1\n        pbar = tqdm(total=num_iters, disable=(self.rank != 0), desc=\"Model Responding\")\n        for chunk in chunks:\n            contexts, all_gen_kwargs, doc_to_visuals, doc_id, tasks, splits = zip(*chunk)\n            batched_visuals = [doc_to_visual(self.task_dict[task][split][ids]) for ids, task, split, doc_to_visual in zip(doc_id, tasks, splits, doc_to_visuals)]  # [B, N]\n            # we assume all gen kwargs in the batch are the same\n            # this is safe to assume because the `grouper` object ensures it.\n            gen_kwargs = all_gen_kwargs[0]\n            if \"max_new_tokens\" not in gen_kwargs:\n                gen_kwargs[\"max_new_tokens\"] = 1024\n            if \"temperature\" not in gen_kwargs:\n                gen_kwargs[\"temperature\"] = 0\n            if \"top_p\" not in gen_kwargs:\n                gen_kwargs[\"top_p\"] = 1.0\n            if \"num_beams\" not in gen_kwargs:\n                gen_kwargs[\"num_beams\"] = 1\n            assert gen_kwargs[\"num_beams\"] == 1\n\n            def save_image_to_temp_file(image):\n                temp_file = tempfile.NamedTemporaryFile(suffix=\".jpeg\", delete=True)\n                image.save(temp_file.name)\n                return temp_file\n\n            def prepare_arguments_parallel(contexts, batched_visuals, max_workers=64):\n                arguments = [None] * len(contexts)  # Initialize with placeholders\n                tmp_files = [None] * len(contexts)  # Initialize with placeholders\n\n                with ThreadPoolExecutor(max_workers=max_workers) as executor:\n                    # Associate each future with its index and content\n                    future_to_info = {executor.submit(save_image_to_temp_file, pil_list[0]): (index, context, pil_list) for index, (context, pil_list) in enumerate(zip(contexts, batched_visuals))}\n\n                    for future in as_completed(future_to_info):\n                        index, context, pil_list = future_to_info[future]\n                        if len(pil_list) > 1:\n                            eval_logger.warning(\"Llava-sglang only supports one visual input per question. Using the first visual input.\")\n                        try:\n                            temp_file = future.result()\n                            arguments[index] = {\n                                \"image_file\": temp_file.name,\n                                \"question\": context,\n                            }\n                            tmp_files[index] = temp_file\n                        except Exception as exc:\n                            print(f\"Generated an exception: {exc}\")\n\n                # Filter out any None values in case of exceptions\n                arguments = [arg for arg in arguments if arg is not None]\n                tmp_files = [tmp_file for tmp_file in tmp_files if tmp_file is not None]\n\n                return arguments, tmp_files\n\n            arguments, tmp_files = prepare_arguments_parallel(contexts, batched_visuals, self.parallel)\n            states = image_qa.run_batch(arguments, temperature=gen_kwargs[\"temperature\"], max_new_tokens=gen_kwargs[\"max_new_tokens\"], top_p=gen_kwargs[\"top_p\"], num_threads=self.parallel, progress_bar=False)\n\n            text_outputs = [state[\"answer\"].strip() for state in states]\n            # clean up the temporary files\n            for tmp_file in tmp_files:\n                tmp_file.close()\n            res.extend(text_outputs)\n            pbar.update(1)\n            # reorder this group of results back to original unsorted form\n        res = re_ords.get_original(res)\n\n        pbar.close()\n        runtime.shutdown()\n        return res\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/__main__.py", "content": "import importlib\nimport os\nimport yaml\nimport sys\nimport json\n\nimport traceback\nimport argparse\nimport numpy as np\nimport datetime\n\nimport warnings\nimport traceback\n\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)\n\nfrom accelerate import Accelerator\nfrom accelerate.utils import InitProcessGroupKwargs\nfrom pathlib import Path\nfrom typing import Union\nimport hashlib\n\nfrom lmms_eval import evaluator, utils\nfrom lmms_eval.tasks import initialize_tasks, include_path, get_task_dict\nfrom lmms_eval.api.registry import ALL_TASKS\nfrom lmms_eval.logging_utils import WandbLogger\nfrom loguru import logger as eval_logger\n\n\ndef _handle_non_serializable(o):\n    if isinstance(o, np.int64) or isinstance(o, np.int32):\n        return int(o)\n    elif isinstance(o, set):\n        return list(o)\n    else:\n        return str(o)\n\n\ndef parse_eval_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter)\n    parser.add_argument(\"--config\", default=\"\", help=\"Path to a yaml file specifying all eval arguments, will ignore cli arguments if specified\")\n    parser.add_argument(\"--model\", default=\"hf\", help=\"Name of model e.g. `hf`\")\n    parser.add_argument(\n        \"--tasks\",\n        default=None,\n        help=\"To get full list of tasks, use the command lmms-eval --tasks list\",\n    )\n    parser.add_argument(\n        \"--model_args\",\n        default=\"\",\n        help=\"String arguments for model, e.g. `pretrained=EleutherAI/pythia-160m,dtype=float32`\",\n    )\n    parser.add_argument(\n        \"--num_fewshot\",\n        type=int,\n        default=None,\n        help=\"Number of examples in few-shot context\",\n    )\n    parser.add_argument(\"--batch_size\", type=str, default=1)\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        default=None,\n        help=\"Device to use (e.g. cuda, cuda:0, cpu)\",\n    )\n    parser.add_argument(\n        \"--output_path\",\n        default=None,\n        type=str,\n        metavar=\"= [dir/file.jsonl] [DIR]\",\n        help=\"The path to the output file where the result metrics will be saved. If the path is a directory and log_samples is true, the results will be saved in the directory. Else the parent directory will be used.\",\n    )\n    parser.add_argument(\n        \"--limit\",\n        type=float,\n        default=None,\n        help=\"Limit the number of examples per task. \" \"If <1, limit is a percentage of the total number of examples.\",\n    )\n    parser.add_argument(\n        \"--check_integrity\",\n        action=\"store_true\",\n        help=\"Whether to run the relevant part of the test suite for the tasks\",\n    )\n    parser.add_argument(\n        \"--show_task_to_terminal\",\n        action=\"store_true\",\n        default=False,\n        help=\"Prints the prompt for the first few documents\",\n    )\n    parser.add_argument(\n        \"--log_samples\",\n        action=\"store_true\",\n        default=False,\n        help=\"If True, write out all model outputs and documents for per-sample measurement and post-hoc analysis\",\n    )\n    parser.add_argument(\n        \"--wandb_log_samples\",\n        action=\"store_true\",\n        default=False,\n        help=\"If True, write out all model outputs and documents for per-sample measurement and post-hoc analysis to Weights and Biases\",\n    )\n    parser.add_argument(\n        \"--log_samples_suffix\",\n        type=str,\n        default=\"model_outputs\",\n        help=\"Specify a suffix for the log_samples file name.\",\n    )\n    parser.add_argument(\n        \"--predict_only\",\n        \"-x\",\n        action=\"store_true\",\n        default=False,\n        help=\"Use with --log_samples. Only model outputs will be saved and metrics will not be evaluated.\",\n    )\n    parser.add_argument(\n        \"--show_config\",\n        action=\"store_true\",\n        default=False,\n        help=\"If True, shows the the full config of all tasks at the end of the evaluation.\",\n    )\n    parser.add_argument(\n        \"--include_path\",\n        type=str,\n        default=None,\n        help=\"Additional path to include if there are external tasks to include.\",\n    )\n    parser.add_argument(\n        \"--gen_kwargs\",\n        default=\"\",\n        help=(\"String arguments for model generation on greedy_until tasks,\" \" e.g. `temperature=0,top_k=0,top_p=0`\"),\n    )\n    parser.add_argument(\n        \"--verbosity\",\n        type=str,\n        default=\"INFO\",\n        help=\"Log error when tasks are not registered.\",\n    )\n    parser.add_argument(\n        \"--wandb_args\",\n        default=\"\",\n        help=\"Comma separated string arguments passed to wandb.init, e.g. `project=lmms-eval,job_type=eval\",\n    )\n    parser.add_argument(\n        \"--timezone\",\n        default=\"Asia/Singapore\",\n        help=\"Timezone for datetime string, e.g. Asia/Singapore, America/New_York, America/Los_Angeles\",\n    )\n    parser.add_argument(\n        \"--return_id_experts\",\n        help=\"Return id experts of each input (True/False)\",\n        choices=[True, False],\n        type=lambda x: (str(x).lower() == 'true'),\n        default=False\n    )\n    parser.add_argument(\n        \"--layers_expert_selection\",\n        help=\"List of layer IDs for expert selection.\",\n        type=int,  # Ensure each input is cast to an integer\n        nargs='+',  # Accepts one or more arguments as a list\n        default=[]  # Default value if no input is provided\n    )\n    args = parser.parse_args()\n    return args\n\n\ndef cli_evaluate(args: Union[argparse.Namespace, None] = None) -> None:\n    from huggingface_hub import login\n    from envparse import env\n    env.read_envfile()\n    login(token=env('KEY_HF'))\n    if not args:\n        args = parse_eval_args()\n    # Check if no arguments were passed after parsing\n    if len(sys.argv) == 1:\n        print(\"┌───────────────────────────────────────────────────────────────────────────────┐\")\n        print(\"│ Please provide arguments to evaluate the model. e.g.                          │\")\n        print(\"│ `lmms-eval --model llava --model_path liuhaotian/llava-v1.6-7b --tasks okvqa` │\")\n        print(\"│ Use `lmms-eval --help` for more information.                                  │\")\n        print(\"└───────────────────────────────────────────────────────────────────────────────┘\")\n        sys.exit(1)\n    args.output_path = \"./logs/\"\n    # reset logger\n    eval_logger.remove()\n    eval_logger.add(sys.stdout, colorize=True, level=args.verbosity)\n    eval_logger.info(f\"Verbosity set to {args.verbosity}\")\n    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n    args_list = []\n    results_list = []\n    if args.config:\n        if not os.path.exists(args.config):\n            raise ValueError(f\"Config file does not exist: {args.config}\")\n\n        with open(args.config, \"r\") as file:\n            config_args = yaml.safe_load(file)\n        config_args = [config_args] if type(config_args) != list else config_args\n        # multiple configs, create args list first\n        for config in config_args:\n            args_copy = argparse.Namespace(**vars(args))\n            for key, value in config.items():\n                setattr(args_copy, key, value)\n            args_list.append(args_copy)\n    else:\n        args_list.append(args)\n\n    # initialize Accelerator\n    kwargs_handler = InitProcessGroupKwargs(timeout=datetime.timedelta(seconds=60000))\n    accelerator = Accelerator(kwargs_handlers=[kwargs_handler])\n    if accelerator.is_main_process:\n        is_main_process = True\n    else:\n        is_main_process = False\n\n    for args in args_list:\n        try:\n            if is_main_process and args.wandb_args:  # thoughtfully we should only init wandb once, instead of multiple ranks to avoid network traffics and unwanted behaviors.\n                wandb_logger = WandbLogger(args)\n\n            results, samples = cli_evaluate_single(args)\n            results_list.append(results)\n\n            accelerator.wait_for_everyone()\n            if is_main_process and args.wandb_args:\n                wandb_logger.post_init(results)\n                wandb_logger.log_eval_result()\n                if args.wandb_log_samples and samples is not None:\n                    wandb_logger.log_eval_samples(samples)\n\n                wandb_logger.finish()\n\n        except Exception as e:\n            traceback.print_exc()\n            eval_logger.error(f\"Error during evaluation: {e}\")\n            traceback.print_exc()\n            results_list.append(None)\n\n    for args, results in zip(args_list, results_list):\n        # cli_evaluate will return none if the process is not the main process (rank 0)\n        if results is not None:\n            print_results(args, results)\n\n\ndef cli_evaluate_single(args: Union[argparse.Namespace, None] = None) -> None:\n    initialize_tasks(args.verbosity)\n\n    if args.predict_only:\n        args.log_samples = True\n    if (args.log_samples or args.predict_only) and not args.output_path:\n        raise ValueError(\"Specify --output_path if providing --log_samples or --predict_only\")\n    if args.limit:\n        eval_logger.warning(\" --limit SHOULD ONLY BE USED FOR TESTING.\" \"REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\")\n    if args.include_path is not None:\n        eval_logger.info(f\"Including path: {args.include_path}\")\n        include_path(args.include_path)\n\n    if os.environ.get(\"LMMS_EVAL_PLUGINS\", None):\n        for plugin in os.environ[\"LMMS_EVAL_PLUGINS\"].split(\",\"):\n            package_tasks_location = importlib.util.find_spec(f\"{plugin}.tasks\").submodule_search_locations[0]\n            eval_logger.info(f\"Including path: {args.include_path}\")\n            include_path(package_tasks_location)\n\n    if args.tasks is None:\n        task_names = ALL_TASKS\n    elif args.tasks == \"list\":\n        eval_logger.info(\"Available Tasks:\\n - {}\".format(f\"\\n - \".join(sorted(ALL_TASKS))))\n        sys.exit()\n    elif args.tasks == \"list_with_num\":\n        log_message = (\n            \"\\n\" + \"=\" * 70 + \"\\n\" + \"\\n\\tYou are trying to check all the numbers in each task.\" + \"\\n\\tThis action will download the complete dataset.\" + \"\\n\\tIf the results are not clear initially, call this again.\" + \"\\n\\n\" + \"=\" * 70\n        )\n        eval_logger.info(log_message)\n        for task_name in sorted(ALL_TASKS):\n            try:\n                task_dict = get_task_dict([task_name], model_name=\"llava\")\n                task_obj = task_dict[task_name]\n                if type(task_obj) == tuple:\n                    group, task_obj = task_obj\n                    if task_obj is None:\n                        continue\n                eval_logger.info(f\"\\nTask : {task_obj.config.task}\\n - #num : {len(task_obj.test_docs()) if task_obj.has_test_docs() else len(task_obj.validation_docs())}\")\n            except Exception as e:\n                eval_logger.debug(f\"\\nTask : {task_name} fail to load \\n Exception : \\n {e}\")\n        sys.exit()\n    else:\n        tasks_list = args.tasks.split(\",\")\n        eval_logger.info(f\"Evaluating on {len(tasks_list)} tasks.\")\n        task_names = utils.pattern_match(tasks_list, ALL_TASKS)\n        task_missing = [task for task in tasks_list if task not in task_names and \"*\" not in task]  # we don't want errors if a wildcard (\"*\") task name was used\n\n        if task_missing:\n            missing = \", \".join(task_missing)\n            eval_logger.error(\n                f\"Tasks were not found: {missing}. Try `lmms-eval --tasks list` for list of available tasks\",\n            )\n            # eval_logger.warn(f\"Tasks {missing} were not found. Try `lmms-eval --tasks list` for list of available tasks.\")\n\n    eval_logger.info(f\"Selected Tasks: {task_names}\")\n\n    # set datetime before evaluation\n    datetime_str = utils.get_datetime_str(timezone=args.timezone)\n    if args.output_path:\n        if args.log_samples_suffix and len(args.log_samples_suffix) > 15:\n            eval_logger.warning(\"The suffix for log_samples is too long. It is recommended to keep it under 15 characters.\")\n            args.log_samples_suffix = args.log_samples_suffix[:5] + \"...\" + args.log_samples_suffix[-5:]\n\n        hash_input = f\"{args.model_args}\".encode(\"utf-8\")\n        hash_output = hashlib.sha256(hash_input).hexdigest()[:6]\n        path = Path(args.output_path)\n        path = path.expanduser().resolve().joinpath(f\"{datetime_str}_{args.log_samples_suffix}_{args.model}_model_args_{hash_output}\")\n        args.output_path = path\n\n    elif args.log_samples and not args.output_path:\n        assert args.output_path, \"Specify --output_path\"\n    results = evaluator.simple_evaluate(\n        model=args.model,\n        model_args=args.model_args,\n        tasks=task_names,\n        num_fewshot=args.num_fewshot,\n        batch_size=args.batch_size,\n        device=args.device,\n        limit=args.limit,\n        check_integrity=args.check_integrity,\n        show_task_to_terminal=args.show_task_to_terminal,\n        log_samples=args.log_samples,\n        gen_kwargs=args.gen_kwargs,\n        cli_args=args,\n        predict_only=args.predict_only,\n        return_id_experts = args.return_id_experts,\n        layers_expert_selection = args.layers_expert_selection\n    )\n\n    if results is not None:\n        if args.log_samples:\n            samples = results.pop(\"samples\")\n        else:\n            samples = None\n        dumped = json.dumps(results, indent=4, default=_handle_non_serializable)\n        if args.show_config:\n            print(dumped)\n\n        if args.output_path:\n            args.output_path.mkdir(parents=True, exist_ok=True)\n            result_file_path = path.joinpath(\"results.json\")\n            if result_file_path.exists():\n                eval_logger.warning(f\"Output file {result_file_path} already exists and will be overwritten.\")\n\n            result_file_path.open(\"w\").write(dumped)\n            if args.log_samples:\n                for task_name, config in results[\"configs\"].items():\n                    filename = args.output_path.joinpath(f\"{task_name}.json\")\n                    # Structure the data with 'args' and 'logs' keys\n                    data_to_dump = {\"args\": vars(args), \"model_configs\": config, \"logs\": sorted(samples[task_name], key=lambda x: x[\"doc_id\"]), \"time\": datetime_str}\n                    samples_dumped = json.dumps(data_to_dump, indent=4, default=_handle_non_serializable, ensure_ascii=False)\n                    filename.open(\"w\", encoding=\"utf-8\").write(samples_dumped)\n                    eval_logger.info(f\"Saved samples to {filename}\")\n\n        return results, samples\n    return None, None\n\n\ndef print_results(args, results):\n    print(f\"{args.model} ({args.model_args}),\\ngen_kwargs: ({args.gen_kwargs}),\\nlimit: {args.limit},\\nnum_fewshot: {args.num_fewshot},\\nbatch_size: {args.batch_size}\")\n    print(evaluator.make_table(results))\n    if \"groups\" in results:\n        print(evaluator.make_table(results, \"groups\"))\n\n\nif __name__ == \"__main__\":\n    cli_evaluate()\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/api/registry.py", "content": "from lmms_eval.api.model import lmms\n\nfrom typing import Callable, Dict\nimport evaluate as hf_evaluate\n\nfrom loguru import logger as eval_logger\n\nMODEL_REGISTRY = {}\n\ndef register_model(*names):\n    # either pass a list or a single alias.\n    # function receives them as a tuple of strings\n    def decorate(cls):\n        for name in names:\n            assert issubclass(cls, lmms), f\"Model '{name}' ({cls.__name__}) must extend lmms class\"\n\n            if name in MODEL_REGISTRY:\n                try:\n                    if MODEL_REGISTRY[name] != cls:\n                        assert name not in MODEL_REGISTRY, f\"Model named '{name}' conflicts with existing model! Please register with a non-conflicting alias instead. \\n {cls} \\n Models: {MODEL_REGISTRY}\"\n                except AssertionError as e:\n                    f\"Error: {e} \\n Infor: \\n Model named '{name}' conflicts with existing model! Please register with a non-conflicting alias instead. \\n {cls} \\n Models: {MODEL_REGISTRY}\"\n            MODEL_REGISTRY[name] = cls\n        return cls\n\n    return decorate\n\n\ndef get_model(model_name):\n    try:\n        return MODEL_REGISTRY[model_name]\n    except KeyError:\n        raise ValueError(f\"Attempted to load model '{model_name}', but no model for this name found! Supported model names: {', '.join(MODEL_REGISTRY.keys())}\")\n\n\nTASK_REGISTRY = {}  # Key: task name, Value: task ConfigurableTask class\nGROUP_REGISTRY = {}  # Key: group name, Value: list of task names or group names\nALL_TASKS = set()  # Set of all task names and group names\nfunc2task_index = {}  # Key: task ConfigurableTask class, Value: task name\n\n\ndef register_task(name):\n    def decorate(fn):\n        assert name not in TASK_REGISTRY, f\"task named '{name}' conflicts with existing registered task!\"\n\n        TASK_REGISTRY[name] = fn\n        ALL_TASKS.add(name)\n        func2task_index[fn.__name__] = name\n        return fn\n\n    return decorate\n\n\ndef register_group(name):\n    def decorate(fn):\n        func_name = func2task_index[fn.__name__]\n        if name in GROUP_REGISTRY:\n            GROUP_REGISTRY[name].append(func_name)\n        else:\n            GROUP_REGISTRY[name] = [func_name]\n            ALL_TASKS.add(name)\n        return fn\n\n    return decorate\n\n\nOUTPUT_TYPE_REGISTRY = {}\nMETRIC_REGISTRY = {}\nMETRIC_AGGREGATION_REGISTRY = {}\nAGGREGATION_REGISTRY = {}\nHIGHER_IS_BETTER_REGISTRY = {}\n\nDEFAULT_METRIC_REGISTRY = {\n    \"loglikelihood\": [\n        \"perplexity\",\n        \"acc\",\n    ],\n    \"multiple_choice\": [\"acc\", \"acc_norm\"],\n    \"generate_until\": [\"exact_match\"],\n}\n\n\ndef register_metric(**args):\n    # TODO: do we want to enforce a certain interface to registered metrics?\n    def decorate(fn):\n        assert \"metric\" in args\n        name = args[\"metric\"]\n\n        for key, registry in [\n            (\"metric\", METRIC_REGISTRY),\n            (\"higher_is_better\", HIGHER_IS_BETTER_REGISTRY),\n            (\"aggregation\", METRIC_AGGREGATION_REGISTRY),\n        ]:\n            if key in args:\n                value = args[key]\n                assert value not in registry, f\"{key} named '{value}' conflicts with existing registered {key}!\"\n\n                if key == \"metric\":\n                    registry[name] = fn\n                elif key == \"aggregation\":\n                    registry[name] = AGGREGATION_REGISTRY[value]\n                else:\n                    registry[name] = value\n\n        return fn\n\n    return decorate\n\n\ndef get_metric(name: str, hf_evaluate_metric=False) -> Callable:\n    print(METRIC_REGISTRY)\n    if not hf_evaluate_metric:\n        if name in METRIC_REGISTRY:\n            return METRIC_REGISTRY[name]\n        else:\n            eval_logger.warning(f\"Could not find registered metric '{name}' in lm-eval, searching in HF Evaluate library...\")\n\n    try:\n        metric_object = hf_evaluate.load(name)\n        return metric_object.compute\n    except Exception:\n        eval_logger.error(\n            f\"{name} not found in the evaluate library! Please check https://huggingface.co/evaluate-metric\",\n        )\n\n\ndef register_aggregation(name):\n    def decorate(fn):\n        assert name not in AGGREGATION_REGISTRY, f\"aggregation named '{name}' conflicts with existing registered aggregation!\"\n\n        AGGREGATION_REGISTRY[name] = fn\n        return fn\n\n    return decorate\n\n\ndef get_aggregation(name):\n    try:\n        return AGGREGATION_REGISTRY[name]\n    except KeyError:\n        eval_logger.warning(\n            \"{} not a registered aggregation metric!\".format(name),\n        )\n\n\ndef get_metric_aggregation(name):\n    try:\n        return METRIC_AGGREGATION_REGISTRY[name]\n    except KeyError:\n        eval_logger.warning(\n            \"{} metric is not assigned a default aggregation!\".format(name),\n        )\n\n\ndef is_higher_better(metric_name):\n    try:\n        return HIGHER_IS_BETTER_REGISTRY[metric_name]\n    except KeyError:\n        eval_logger.warning(f\"higher_is_better not specified for metric '{metric_name}'!\")\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/models/model_utils/load_video.py", "content": "import av\nfrom av.codec.context import CodecContext\nimport numpy as np\n\n\n# This one is faster\ndef record_video_length_stream(container, indices):\n    frames = []\n    start_index = indices[0]\n    end_index = indices[-1]\n    for i, frame in enumerate(container.decode(video=0)):\n        if i > end_index:\n            break\n        if i >= start_index and i in indices:\n            frames.append(frame)\n    return frames\n\n\n# This one works for all types of video\ndef record_video_length_packet(container):\n    frames = []\n    # https://github.com/PyAV-Org/PyAV/issues/1269\n    # https://www.cnblogs.com/beyond-tester/p/17641872.html\n    # context = CodecContext.create(\"libvpx-vp9\", \"r\")\n    for packet in container.demux(video=0):\n        for frame in packet.decode():\n            frames.append(frame)\n    return frames\n\n\ndef read_video_pyav(video_path, num_frm=8):\n    container = av.open(video_path)\n\n    if \"webm\" not in video_path and \"mkv\" not in video_path:\n        # For mp4, we try loading with stream first\n        try:\n            container = av.open(video_path)\n            total_frames = container.streams.video[0].frames\n            sampled_frm = min(total_frames, num_frm)\n            indices = np.linspace(0, total_frames - 1, sampled_frm, dtype=int)\n            frames = record_video_length_stream(container, indices)\n        except:\n            container = av.open(video_path)\n            frames = record_video_length_packet(container)\n            total_frames = len(frames)\n            sampled_frm = min(total_frames, num_frm)\n            indices = np.linspace(0, total_frames - 1, sampled_frm, dtype=int)\n            frames = [frames[i] for i in indices]\n    else:\n        container = av.open(video_path)\n        frames = record_video_length_packet(container)\n        total_frames = len(frames)\n        sampled_frm = min(total_frames, num_frm)\n        indices = np.linspace(0, total_frames - 1, sampled_frm, dtype=int)\n        frames = [frames[i] for i in indices]\n    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/logging_utils.py", "content": "# Code mostly from: https://github.com/EleutherAI/lm-evaluation-harness/pull/1339, credit to: https://github.com/ayulockin\nimport copy\n\nimport re\nimport os\nimport json\nimport glob\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Literal, Tuple, Union\nfrom packaging.version import Version\nfrom lmms_eval import utils\nimport tenacity\nfrom loguru import logger\n\ntry:\n    import wandb\n\n    assert Version(wandb.__version__) >= Version(\"0.13.6\")\n    if Version(wandb.__version__) < Version(\"0.13.6\"):\n        wandb.require(\"report-editing:v0\")\nexcept Exception as e:\n    logger.warning(\"To use the wandb reporting functionality please install wandb>=0.13.6.\\n\" \"To install the latest version of wandb run `pip install wandb --upgrade`\\n\" f\"{e}\")\n\n\ndef remove_none_pattern(input_string):\n    # Define the pattern to match ',none' at the end of the string\n    pattern = re.compile(r\",none$\")\n\n    # Use sub() to replace ',none' with an empty string\n    result = re.sub(pattern, \"\", input_string)\n\n    # check if the input_string changed\n    removed = result != input_string\n\n    return result, removed\n\n\ndef _handle_non_serializable(o: Any) -> Union[int, str, list]:\n    \"\"\"Handle non-serializable objects by converting them to serializable types.\n\n    Args:\n        o (Any): The object to be handled.\n\n    Returns:\n        Union[int, str, list]: The converted object. If the object is of type np.int64 or np.int32,\n            it will be converted to int. If the object is of type set, it will be converted\n            to a list. Otherwise, it will be converted to str.\n    \"\"\"\n    if isinstance(o, np.int64) or isinstance(o, np.int32):\n        return int(o)\n    elif isinstance(o, set):\n        return list(o)\n    else:\n        return str(o)\n\n\ndef get_wandb_printer() -> Literal[\"Printer\"]:\n    \"\"\"Returns a wandb printer instance for pretty stdout.\"\"\"\n    from wandb.sdk.lib.printer import get_printer\n    from wandb.sdk.wandb_settings import Settings\n\n    printer = get_printer(Settings()._jupyter)\n    return printer\n\n\n# class WandbLogger:\nclass WandbLogger:\n    def __init__(self, args):\n        self.wandb_args = utils.simple_parse_args_string(args.wandb_args)\n        self.args = args\n        self.all_args_dict = vars(args)\n        self.printer = get_wandb_printer()\n        try:\n            self.init_run()\n        except Exception as e:\n            logger.warning(f\"Failed to initialize W&B run: {e}\")\n            os.environ[\"WANDB_MODE\"] = \"offline\"\n            self.init_run()\n\n    def finish(self):\n        self.run.finish()\n\n    @tenacity.retry(wait=tenacity.wait_fixed(5), stop=tenacity.stop_after_attempt(5))\n    def init_run(self):\n        if \"name\" not in self.wandb_args:\n            if \"config\" in self.all_args_dict and self.all_args_dict[\"config\"] != \"\":\n                self.wandb_args[\"name\"] = self.all_args_dict[\"config\"].split(\"/\")[-1].replace(\".yaml\", \"\") + \"/\" + self.args.log_samples_suffix\n            else:\n                task_names = self.args.tasks.replace(\",\", \"/\")\n                self.wandb_args[\"name\"] = f\"{self.args.model}/<{task_names}>/{self.args.log_samples_suffix}\"\n                if self.args.num_fewshot:\n                    self.wandb_args[\"name\"] += f\"_{self.args.num_fewshot}shot\"\n        if \"project\" not in self.wandb_args:\n            self.wandb_args[\"project\"] = \"lmms-eval\"\n        # initialize a W&B run\n        self.run = wandb.init(**self.wandb_args)\n\n    def post_init(self, results: Dict[str, Any]) -> None:\n        self.results: Dict[str, Any] = copy.deepcopy(results)\n        self.task_names: List[str] = list(results.get(\"results\", {}).keys())\n        self.group_names: List[str] = list(results.get(\"groups\", {}).keys())\n\n    def _get_config(self) -> Dict[str, Any]:\n        \"\"\"Get configuration parameters.\"\"\"\n        self.task_configs = self.results.get(\"configs\", {})\n        cli_configs = self.results.get(\"config\", {})\n        configs = {\n            \"task_configs\": self.task_configs,\n            \"cli_configs\": cli_configs,\n        }\n\n        return configs\n\n    def _sanitize_results_dict(self) -> Tuple[Dict[str, str], Dict[str, Any]]:\n        \"\"\"Sanitize the results dictionary.\"\"\"\n        _results = copy.deepcopy(self.results.get(\"results\", dict()))\n        _results[\"model_configs\"] = self.results.get(\"model_configs\", dict())\n\n        # Remove None from the metric string name\n        tmp_results = copy.deepcopy(_results)\n        for task_name in self.task_names:\n            task_result = tmp_results.get(task_name, dict())\n            for metric_name, metric_value in task_result.items():\n                _metric_name, removed = remove_none_pattern(metric_name)\n                if removed:\n                    _results[task_name][_metric_name] = metric_value\n                    _results[task_name].pop(metric_name)\n\n        # remove string valued keys from the results dict\n        wandb_summary = {}\n        for task in self.task_names:\n            task_result = _results.get(task, dict())\n            for metric_name, metric_value in task_result.items():\n                if isinstance(metric_value, str):\n                    wandb_summary[f\"{task}/{metric_name}\"] = metric_value\n\n        wandb_summary[\"model_configs\"] = self.results.get(\"model_configs\", dict())\n        for summary_metric, summary_value in wandb_summary.items():\n            if summary_metric != \"model_configs\":\n                _task, _summary_metric = summary_metric.split(\"/\")\n                _results[_task].pop(_summary_metric)\n\n        tmp_results = copy.deepcopy(_results)\n        for task_name, task_results in tmp_results.items():\n            if task_name != \"model_configs\":\n                for metric_name, metric_value in task_results.items():\n                    _results[f\"{task_name}/{metric_name}\"] = metric_value\n                    _results[task_name].pop(metric_name)\n        for task in self.task_names:\n            _results.pop(task)\n\n        return wandb_summary, _results\n\n    def _log_results_as_table(self) -> None:\n        \"\"\"Generate and log evaluation results as a table to W&B.\"\"\"\n        columns = [\n            \"Model\",\n            \"Args\",\n            \"Tasks\",\n            \"Version\",\n            \"Filter\",\n            \"num_fewshot\",\n            \"Metric\",\n            \"Value\",\n            \"Stderr\",\n        ]\n\n        def make_table(columns: List[str], key: str = \"results\"):\n            table = wandb.Table(columns=columns)\n            results = copy.deepcopy(self.results)\n\n            model_name = results.get(\"model_configs\").get(\"model\")\n            model_args = results.get(\"model_configs\").get(\"model_args\")\n\n            for k, dic in results.get(key).items():\n                if k in self.group_names and not key == \"groups\":\n                    continue\n                version = results.get(\"versions\").get(k)\n                if version == \"N/A\":\n                    version = None\n                n = results.get(\"n-shot\").get(k)\n\n                for (mf), v in dic.items():\n                    m, _, f = mf.partition(\",\")\n                    if m.endswith(\"_stderr\"):\n                        continue\n                    if m == \"alias\":\n                        continue\n\n                    if m + \"_stderr\" + \",\" + f in dic:\n                        se = dic[m + \"_stderr\" + \",\" + f]\n                        if se != \"N/A\":\n                            se = \"%.4f\" % se\n                        data = [model_name, model_args, k, version, f, n, m, str(v), str(se)]\n                        if key == \"groups\":\n                            data = [self.group_names] + data\n                        table.add_data(*data)\n                    else:\n                        data = [model_name, model_args, k, version, f, n, m, str(v), \"\"]\n                        if key == \"groups\":\n                            data = [self.group_names] + data\n                        table.add_data(*data)\n\n            return table\n\n        # log the complete eval result to W&B Table\n        table = make_table(columns, \"results\")\n        self.run.log({\"evaluation/eval_results\": table})\n\n        if \"groups\" in self.results.keys():\n            table = make_table([\"Groups\"] + columns, \"groups\")\n            self.run.log({\"evaluation/group_eval_results\": table})\n\n    def _log_results_as_artifact(self) -> None:\n        \"\"\"Log results as JSON artifact to W&B.\"\"\"\n        dumped = json.dumps(self.results, indent=2, default=_handle_non_serializable, ensure_ascii=False)\n        artifact = wandb.Artifact(\"results\", type=\"eval_results\")\n        with artifact.new_file(\"results.json\", mode=\"w\", encoding=\"utf-8\") as f:\n            f.write(dumped)\n        self.run.log_artifact(artifact)\n\n    def log_eval_result(self) -> None:\n        \"\"\"Log evaluation results to W&B.\"\"\"\n        # Log configs to wandb\n        configs = self._get_config()\n        self.run.config.update(configs, allow_val_change=True)\n\n        wandb_summary, self.wandb_results = self._sanitize_results_dict()\n        # update wandb.run.summary with items that were removed\n        self.run.summary.update(wandb_summary)\n        # Log the evaluation metrics to wandb\n        self.run.log(self.wandb_results)\n        # Log the evaluation metrics as W&B Table\n        self._log_results_as_table()\n        # Log the results dict as json to W&B Artifacts\n        self._log_results_as_artifact()\n\n    def _generate_dataset(self, data: List[Dict[str, Any]], config: Dict[str, Any]) -> pd.DataFrame:\n        \"\"\"Generate a dataset from evaluation data.\n\n        Args:\n            data (List[Dict[str, Any]]): The data to generate a dataset for.\n            config (Dict[str, Any]): The configuration of the task.\n\n        Returns:\n            pd.DataFrame: A dataframe that is ready to be uploaded to W&B.\n        \"\"\"\n        ids = [x[\"doc_id\"] for x in data]\n        labels = [x[\"target\"] for x in data]\n        instance = [\"\"] * len(ids)\n        resps = [\"\"] * len(ids)\n        filtered_resps = [\"\"] * len(ids)\n        model_outputs = {}\n\n        metrics_list = config[\"metric_list\"]\n        metrics = {}\n        for metric in metrics_list:\n            metric = metric.get(\"metric\")\n            if metric in [\"word_perplexity\", \"byte_perplexity\", \"bits_per_byte\"]:\n                metrics[f\"{metric}_loglikelihood\"] = [x[metric][0] for x in data]\n                if metric in [\"byte_perplexity\", \"bits_per_byte\"]:\n                    metrics[f\"{metric}_bytes\"] = [x[metric][1] for x in data]\n                else:\n                    metrics[f\"{metric}_words\"] = [x[metric][1] for x in data]\n            else:\n                metrics[metric] = [x[metric] for x in data]\n\n        if config[\"output_type\"] == \"loglikelihood\":\n            instance = [x[\"arguments\"][0][0] for x in data]\n            labels = [x[\"arguments\"][0][1] for x in data]\n            resps = [f'log probability of continuation is {x[\"resps\"][0][0][0]} ' + \"\\n\\n\" + \"continuation will {} generated with greedy sampling\".format(\"not be\" if not x[\"resps\"][0][0][1] else \"be\") for x in data]\n            filtered_resps = [f'log probability of continuation is {x[\"filtered_resps\"][0][0]} ' + \"\\n\\n\" + \"continuation will {} generated with greedy sampling\".format(\"not be\" if not x[\"filtered_resps\"][0][1] else \"be\") for x in data]\n        elif config[\"output_type\"] == \"multiple_choice\":\n            instance = [x[\"arguments\"][0][0] for x in data]\n            choices = [\"\\n\".join([f\"{idx}. {y[1]}\" for idx, y in enumerate(x[\"arguments\"])]) for x in data]\n            resps = [np.argmax([n[0][0] for n in x[\"resps\"]]) for x in data]\n            filtered_resps = [np.argmax([n[0] for n in x[\"filtered_resps\"]]) for x in data]\n        elif config[\"output_type\"] == \"generate_until\":\n            instance = [x[\"arguments\"][0][0] for x in data]\n            resps = [x[\"resps\"][0][0] for x in data]\n            filtered_resps = [x[\"filtered_resps\"][0] for x in data]\n\n        model_outputs[\"raw_predictions\"] = resps\n        model_outputs[\"filtered_predictions\"] = filtered_resps\n\n        df_data = {\n            \"id\": ids,\n            \"data\": instance,\n        }\n        if config[\"output_type\"] == \"multiple_choice\":\n            df_data[\"choices\"] = choices\n\n        tmp_data = {\n            \"input_len\": [len(x) for x in instance],\n            \"labels\": labels,\n            \"output_type\": config[\"output_type\"],\n        }\n        df_data.update(tmp_data)\n        df_data.update(model_outputs)\n        df_data.update(metrics)\n\n        return pd.DataFrame(df_data)\n\n    def _log_samples_as_artifact(self, data: List[Dict[str, Any]], task_name: str) -> None:\n        # log the samples as an artifact\n        dumped = json.dumps(\n            data,\n            indent=2,\n            default=_handle_non_serializable,\n            ensure_ascii=False,\n        )\n        artifact = wandb.Artifact(f\"{task_name}\", type=\"samples_by_task\")\n        with artifact.new_file(f\"{task_name}_eval_samples.json\", mode=\"w\", encoding=\"utf-8\") as f:\n            f.write(dumped)\n        self.run.log_artifact(artifact)\n        # artifact.wait()\n\n    def log_eval_samples(self, samples: Dict[str, List[Dict[str, Any]]]) -> None:\n        \"\"\"Log evaluation samples to W&B.\n\n        Args:\n            samples (Dict[str, List[Dict[str, Any]]]): Evaluation samples for each task.\n        \"\"\"\n        task_names: List[str] = [x for x in self.task_names if x not in self.group_names]\n\n        ungrouped_tasks = []\n        tasks_by_groups = {}\n\n        for task_name in task_names:\n            group_names = self.task_configs[task_name].get(\"group\", None)\n            if group_names:\n                if isinstance(group_names, str):\n                    group_names = [group_names]\n\n                for group_name in group_names:\n                    if not tasks_by_groups.get(group_name):\n                        tasks_by_groups[group_name] = [task_name]\n                    else:\n                        tasks_by_groups[group_name].append(task_name)\n            else:\n                ungrouped_tasks.append(task_name)\n\n        for task_name in ungrouped_tasks:\n            eval_preds = samples[task_name]\n\n            # log the samples as a W&B Table\n            df = self._generate_dataset(eval_preds, self.task_configs.get(task_name))\n            self.run.log({f\"{task_name}_eval_results\": df})\n\n            # log the samples as a json file as W&B Artifact\n            self._log_samples_as_artifact(eval_preds, task_name)\n\n        for group, grouped_tasks in tasks_by_groups.items():\n            grouped_df = pd.DataFrame()\n            for task_name in grouped_tasks:\n                eval_preds = samples[task_name]\n                df = self._generate_dataset(eval_preds, self.task_configs.get(task_name))\n                df[\"group\"] = group\n                df[\"task\"] = task_name\n                grouped_df = pd.concat([grouped_df, df], ignore_index=True)\n\n                # log the samples as a json file as W&B Artifact\n                self._log_samples_as_artifact(eval_preds, task_name)\n\n            self.run.log({f\"{group}_eval_results\": grouped_df})\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/models/qwen_vl.py", "content": "import torch\n\nfrom tqdm import tqdm\nfrom lmms_eval import utils\nfrom lmms_eval.api.instance import Instance\nfrom lmms_eval.api.model import lmms\nfrom lmms_eval.api.registry import register_model\nfrom lmms_eval.models.model_utils.qwen.qwen_generate_utils import make_context\nfrom accelerate import Accelerator, DistributedType\nfrom typing import List, Optional, Union, Tuple\nimport uuid\nimport os\n\nimport warnings\n\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\")\n\nfrom loguru import logger as eval_logger\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n\n@register_model(\"qwen_vl\")\nclass Qwen_VL(lmms):\n    \"\"\"\n    Qwen_VL Model\n    https://github.com/QwenLM/Qwen-VL/blob/master/eval_mm/evaluate_vqa.py\n    \"\"\"\n\n    def __init__(\n        self,\n        pretrained: str = \"Qwen/Qwen-VL\",\n        device: Optional[str] = \"cuda\",\n        batch_size: Optional[Union[int, str]] = 1,\n        trust_remote_code: Optional[bool] = True,\n        use_cache=True,\n        **kwargs,\n    ) -> None:\n        super().__init__()\n        # Do not use kwargs for now\n        assert kwargs == {}, f\"Unexpected kwargs: {kwargs}\"\n\n        accelerator = Accelerator()\n        if accelerator.num_processes > 1:\n            self._device = torch.device(f\"cuda:{accelerator.local_process_index}\")\n        else:\n            self._device = device\n        self._model = AutoModelForCausalLM.from_pretrained(pretrained, device_map=self._device, trust_remote_code=trust_remote_code).eval()\n        self._tokenizer = AutoTokenizer.from_pretrained(pretrained, trust_remote_code=trust_remote_code)\n        self.tokenizer.padding_side = \"left\"\n        self.tokenizer.pad_token_id = self.tokenizer.eod_id\n        self.prompt = \"<img>{}</img>{}\"\n        self._config = self._model.config\n        self.model.tie_weights()\n        self.batch_size_per_gpu = int(batch_size)\n        self.use_cache = use_cache\n        if accelerator.num_processes > 1:\n            assert accelerator.distributed_type in [\n                DistributedType.FSDP,\n                DistributedType.MULTI_GPU,\n            ], \"Unsupported distributed type provided. Only DDP and FSDP are supported.\"\n            if accelerator.distributed_type == DistributedType.FSDP:\n                self._model = accelerator.prepare(self.model)\n            else:\n                self._model = accelerator.prepare_model(self.model, evaluation_mode=True)\n            self.accelerator = accelerator\n            if self.accelerator.is_local_main_process:\n                eval_logger.info(f\"Using {accelerator.num_processes} devices with data parallelism\")\n            self._rank = self.accelerator.local_process_index\n            self._world_size = self.accelerator.num_processes\n        else:\n            self.model.to(self._device)\n            self._rank = 0\n            self._word_size = 1\n\n    @property\n    def config(self):\n        # return the associated transformers.AutoConfig for the given pretrained model.\n        return self._config\n\n    @property\n    def tokenizer(self):\n        return self._tokenizer\n\n    @property\n    def model(self):\n        # returns the model, unwrapping it if using Accelerate\n        if hasattr(self, \"accelerator\"):\n            return self.accelerator.unwrap_model(self._model)\n        else:\n            return self._model\n\n    @property\n    def eot_token_id(self):\n        # we use EOT because end of *text* is more accurate for what we're doing than end of *sentence*\n        return self.tokenizer.eod_id\n\n    @property\n    def max_length(self):\n        return self._max_length\n\n    # should be deleted since max_new_tokens is decided by gen_kwargs not a model property\n    # @property\n    # def max_new_tokens(self) -> int:\n    #     return 256\n\n    @property\n    def batch_size(self):\n        return self.batch_size_per_gpu\n\n    @property\n    def device(self):\n        return self._device\n\n    @property\n    def rank(self):\n        return self._rank\n\n    @property\n    def world_size(self):\n        return self._world_size\n\n    def loglikelihood(self, requests: List[Instance]) -> List[Tuple[float, bool]]:\n        res = []\n        pbar = tqdm(total=len(requests), disable=(self.rank != 0), desc=\"Model Responding\")\n\n        for contexts, doc_to_target, doc_to_visual, doc_id, task, split in [reg.args for reg in requests]:\n            # encode, pad, and truncate contexts for this batch\n            if type(doc_to_target) == str:\n                continuation = doc_to_target\n            else:\n                continuation = doc_to_target(self.task_dict[task][split][doc_id])\n            visuals = [doc_to_visual(self.task_dict[task][split][doc_id])]\n            visuals = self.flatten(visuals)\n            query = []\n            visual_paths = []\n            for visual in visuals:\n                name = uuid.uuid4().hex.upper()[0:6]\n                visual.save(f\"/tmp/{name}.png\")\n                visual_paths.append(f\"/tmp/{name}.png\")\n                query.append({\"image\": f\"/tmp/{name}.png\"})\n\n            # Make a copy for query to save context (text that needs to be masked)\n            context_query = [_ for _ in query]\n            context_query.append({\"text\": contexts})\n            query.append({\"text\": contexts + continuation})\n\n            context_query = self.tokenizer.from_list_format(context_query)\n            query = self.tokenizer.from_list_format(query)\n\n            raw_contxt_text, context_tokens = make_context(\n                self.tokenizer, context_query, history=None, system=\"You are a helpful assistant\", max_window_size=self.model.generation_config.max_window_size, chat_format=self.model.generation_config.chat_format\n            )\n            context_tokens = torch.tensor([context_tokens])\n\n            raw_continuation_text, continuation_tokens = make_context(\n                self.tokenizer, query, history=None, system=\"You are a helpful assistant\", max_window_size=self.model.generation_config.max_window_size, chat_format=self.model.generation_config.chat_format\n            )\n            continuation_tokens = torch.tensor([continuation_tokens]).to(self.model.device)\n            attn_mask = torch.ones_like(continuation_tokens).to(self.model.device)\n            labels = continuation_tokens.clone().to(self.model.device)\n            labels[:, : context_tokens.shape[1]] = -100\n            with torch.inference_mode():\n                outputs = self.model(input_ids=continuation_tokens, labels=labels, attention_mask=attn_mask)\n            loss = outputs.loss\n            logits = outputs[\"logits\"]\n            greedy_tokens = logits.argmax(dim=-1)\n            cont_toks = continuation_tokens[:, context_tokens.shape[1] :]\n            greedy_tokens = greedy_tokens[:, context_tokens.shape[1] : continuation_tokens.shape[1]]  # [1, seq]\n            max_equal = (greedy_tokens == cont_toks).all()\n            res.append((float(loss.item()), bool(max_equal)))\n            pbar.update(1)\n\n        pbar.close()\n        return res\n\n    def flatten(self, input):\n        new_list = []\n        for i in input:\n            for j in i:\n                new_list.append(j)\n        return new_list\n\n    def generate_until(self, requests: List[Instance]) -> List[str]:\n        res = []\n\n        def _collate(x):\n            # the negative sign on len(toks) sorts descending - this has a few advantages:\n            # - time estimates will always be over not underestimates, which is more useful for planning\n            # - to know the size of a batch when going through the list, you know the first one is always the batch\n            #   padded context length. this is useful to simplify the batching logic and more importantly to make\n            #   automatic adaptive batches much much easier to implement\n            # - any OOMs will happen right away rather than near the end\n            toks = self.tokenizer.encode(x[0])\n            return -len(toks), x[0]\n\n        pbar = tqdm(total=len(requests), disable=(self.rank != 0), desc=\"Model Responding\")\n        # we group requests by their generation_kwargs,\n        # so that we don't try to execute e.g. greedy sampling and temp=0.8 sampling\n        # in the same batch.\n        re_ords = utils.Collator([reg.args for reg in requests], _collate, grouping=True)\n        chunks = re_ords.get_batched(n=self.batch_size, batch_fn=None)\n        for chunk in chunks:\n            contexts, all_gen_kwargs, doc_to_visual, doc_id, task, split = zip(*chunk)\n            task = task[0]\n            split = split[0]\n            visuals = [doc_to_visual[0](self.task_dict[task][split][ids]) for ids in doc_id]\n            visuals = self.flatten(visuals)\n            visual_paths = []\n            # save images to /tmp, name generated by hash function\n            # qwen accept image path. Have to do it here....\n            for visual in visuals:\n                name = uuid.uuid4().hex.upper()[0:6]\n                visual.save(f\"/tmp/{name}.png\")\n                visual_paths.append(f\"/tmp/{name}.png\")\n\n            # we assume all gen kwargs in the batch are the same\n            # this is safe to assume because the `grouper` object ensures it.\n            gen_kwargs = all_gen_kwargs[0]\n\n            # Set default values for until and max_new_tokens\n            until = [self.tokenizer.decode(self.eot_token_id)]\n\n            # Update values from gen_kwargs if present\n            if \"until\" in gen_kwargs:\n                until = gen_kwargs.pop(\"until\")\n                if isinstance(until, str):\n                    until = [until]\n                elif not isinstance(until, list):\n                    raise ValueError(f\"Expected `gen_kwargs['until']` to be of type Union[str,list] but got {type(until)}\")\n\n            if isinstance(contexts, tuple):\n                contexts = list(contexts)\n\n            for i in range(len(contexts)):\n                if \"<image>\" in contexts[i]:\n                    contexts[i] = contexts[i].replace(\"<image>\", \"\")\n\n            # Similar to llava, is visual paths has len 0\n            # Then nothing will be executed\n            query = []\n            if len(visual_paths) == 0:\n                for context in contexts:\n                    query.append({\"text\": context})\n            else:\n                for visual_path, context in zip(visual_paths, contexts):\n                    query.append({\"image\": visual_path})\n                    query.append({\"text\": context})\n\n            questions = self.tokenizer.from_list_format(query)\n            input_ids = self.tokenizer(questions, return_tensors=\"pt\", padding=\"longest\")\n\n            # preconfigure gen_kwargs with defaults\n            if \"image_sizes\" not in gen_kwargs:\n                try:\n                    gen_kwargs[\"image_sizes\"] = [visuals[0].size]\n                except:\n                    gen_kwargs[\"image_sizes\"] = None\n            if \"max_new_tokens\" not in gen_kwargs:\n                gen_kwargs[\"max_new_tokens\"] = 1024\n            if \"temperature\" not in gen_kwargs:\n                gen_kwargs[\"temperature\"] = 0\n            if \"top_p\" not in gen_kwargs:\n                gen_kwargs[\"top_p\"] = None\n            if \"num_beams\" not in gen_kwargs:\n                gen_kwargs[\"num_beams\"] = 1\n\n            pad_token_id = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eod_id\n\n            cont = self.model.generate(\n                input_ids.input_ids.to(self.device),\n                attention_mask=input_ids.attention_mask.to(self.device),\n                eos_token_id=self.tokenizer.eod_id,\n                pad_token_id=pad_token_id,\n                do_sample=True if gen_kwargs[\"temperature\"] > 0 else False,\n                temperature=gen_kwargs[\"temperature\"],\n                top_p=gen_kwargs[\"top_p\"],\n                num_beams=gen_kwargs[\"num_beams\"],\n                max_new_tokens=gen_kwargs[\"max_new_tokens\"],\n                use_cache=self.use_cache,\n                # kwargs=gen_kwargs\n            )\n\n            cont_toks_list = cont.tolist()\n            for cont_toks, context in zip(cont_toks_list, contexts):\n                # discard context + left-padding toks if using causal decoder-only LMM\n                cont_toks = cont_toks[input_ids.input_ids.shape[1] :]\n                text_outputs = self.tokenizer.decode(cont_toks, skip_special_tokens=True).strip()\n                for term in until:\n                    if len(term) > 0:\n                        # ignore '' separator,\n                        # for seq2seq case where self.tok_decode(self.eot_token_id) = ''\n                        text_outputs = text_outputs.split(term)[0]\n\n                res.append(text_outputs)\n\n                self.cache_hook.add_partial(\"generate_until\", (context, gen_kwargs), text_outputs)\n                # remove visuals from tmp\n                for visual_path in visual_paths:\n                    try:\n                        os.remove(visual_path)\n                    except:\n                        pass\n                pbar.update(1)\n            # reorder this group of results back to original unsorted form\n        res = re_ords.get_original(res)\n\n        pbar.close()\n        return res\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/models/llava_vid.py", "content": "from accelerate import Accelerator, DistributedType, InitProcessGroupKwargs\nfrom accelerate.state import AcceleratorState\nfrom typing import List, Optional, Union, Tuple\nimport torch\nfrom tqdm import tqdm\nfrom decord import VideoReader, cpu\nimport numpy as np\nimport math\nfrom datetime import timedelta\nfrom transformers import AutoConfig\nimport copy\n\nfrom lmms_eval.api.instance import Instance\nfrom lmms_eval.api.model import lmms\nfrom lmms_eval.api.registry import register_model\nfrom lmms_eval.models.model_utils.load_video import read_video_pyav\n\nfrom loguru import logger as eval_logger\n\ntry:\n    from llavavid.model.builder import load_pretrained_model\n    from llavavid.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\n    from llavavid.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IGNORE_INDEX\n    from llavavid.conversation import conv_templates, SeparatorStyle\n    from llavavid.mm_utils import tokenizer_image_token_qwen_merge, preprocess_qwen, preprocess_llama3\nexcept ImportError:\n    eval_logger.debug(\"LLaVA-Video is not installed. Please install LLaVA-Video to use this model.\")\n\nfrom llavavid.model.language_model.llava_qwen import LlavaQwenConfig\nfrom llavavid.model.language_model.llava_llama import LlavaConfig\n\nAutoConfig.register(\"llava_qwen\", LlavaQwenConfig)\nAutoConfig.register(\"llava_llama\", LlavaConfig)\n\n\n@register_model(\"llavavid\")\nclass LlavaVid(lmms):\n    \"\"\"\n    LlavaVid Model\n    \"\"\"\n\n    def __init__(\n        self,\n        pretrained: str = \"liuhaotian/llava-v1.5-7b\",\n        truncation: Optional[bool] = True,\n        device: Optional[str] = \"cuda:0\",\n        batch_size: Optional[Union[int, str]] = 1,\n        attn_implementation=(\n            \"sdpa\" if torch.__version__ >= \"2.1.2\" else \"eager\"\n        ),  # inference implementation for attention, can be \"sdpa\", \"eager\", \"flash_attention_2\". Seems FA2 is not effective during inference: https://discuss.huggingface.co/t/flash-attention-has-no-effect-on-inference/73453/5\n        device_map=\"cuda:0\",\n        conv_template=\"vicuna_v1\",\n        use_cache=True,\n        truncate_context=False,  # whether to truncate the context in generation, set it False for LLaVA-1.6\n        max_frames_num: int = 3,\n        mm_resampler_type: str = \"spatial_pool\",\n        mm_spatial_pool_stride: int = 2,\n        mm_spatial_pool_out_channels: int = 1024,\n        mm_spatial_pool_mode: str = \"average\",\n        mm_resampler_location:str = \"before\",\n        mm_newline_position: str = \"grid\",\n        overwrite: bool = True,\n        video_decode_backend: str = \"pyav\",\n        delay_load: bool = False,\n        tie_weights: bool = True,\n        **kwargs,\n    ) -> None:\n        super().__init__()\n        assert kwargs == {}, f\"Unexpected kwargs: {kwargs}\"\n\n        accelerator_kwargs = InitProcessGroupKwargs(timeout=timedelta(weeks=52))\n        accelerator = Accelerator(kwargs_handlers=[accelerator_kwargs])\n        if accelerator.num_processes > 1:\n            self._device = torch.device(f\"cuda:{accelerator.local_process_index}\")\n            self.device_map = f\"cuda:{accelerator.local_process_index}\"\n        elif accelerator.num_processes == 1 and device_map == \"auto\":\n            self._device = torch.device(device)\n            self.device_map = device_map\n        else:\n            self._device = torch.device(f\"cuda:{accelerator.local_process_index}\")\n            self.device_map = f\"cuda:{accelerator.local_process_index}\"\n\n        self.pretrained = pretrained\n        self.model_name = get_model_name_from_path(pretrained)\n        self.video_decode_backend = video_decode_backend\n        # self._config = AutoConfig.from_pretrained(self.pretrained)\n        self.overwrite = overwrite\n        self.mm_resampler_type = mm_resampler_type\n        self.mm_spatial_pool_stride = int(mm_spatial_pool_stride)\n        self.mm_spatial_pool_out_channels = int(mm_spatial_pool_out_channels)\n        self.mm_spatial_pool_mode = mm_spatial_pool_mode\n        self.max_frames_num = int(max_frames_num)\n\n        self.mm_resampler_location = mm_resampler_location\n        self.mm_newline_position=mm_newline_position\n        self.delay_load = delay_load\n\n        if self.overwrite == True:\n            overwrite_config = {}\n            overwrite_config[\"mm_resampler_type\"] = self.mm_resampler_type\n            overwrite_config[\"mm_spatial_pool_stride\"] = self.mm_spatial_pool_stride\n            overwrite_config[\"mm_spatial_pool_out_channels\"] = self.mm_spatial_pool_out_channels\n            overwrite_config[\"mm_spatial_pool_mode\"] = self.mm_spatial_pool_mode\n            overwrite_config[\"mm_pooling_position\"] = self.mm_resampler_location\n            overwrite_config[\"mm_newline_position\"] = self.mm_newline_position\n            overwrite_config[\"add_faster_video\"] = False\n            overwrite_config[\"delay_load\"] = self.delay_load\n            # overwrite_config[\"attn_implementation\"] = attn_implementation\n\n            cfg_pretrained = AutoConfig.from_pretrained(self.pretrained)\n\n            if cfg_pretrained.architectures[0] == \"LlavaLlamaForCausalLM\":  # Ugly code, only used in  vicuna that needs ROPE\n                if \"224\" in cfg_pretrained.mm_vision_tower:\n                    least_token_number = self.max_frames_num * (16 // self.mm_spatial_pool_stride) ** 2 + 1000\n                else:\n                    least_token_number = self.max_frames_num * (24 // self.mm_spatial_pool_stride) ** 2 + 1000\n\n                scaling_factor = math.ceil(least_token_number / 4096)\n                if scaling_factor >= 2:\n                    overwrite_config[\"rope_scaling\"] = {\"factor\": float(scaling_factor), \"type\": \"linear\"}\n                    overwrite_config[\"max_sequence_length\"] = 4096 * scaling_factor\n                    overwrite_config[\"tokenizer_model_max_length\"] = 4096 * scaling_factor\n\n            if \"v1.5\" in pretrained:  # A hardcode solution here to load v1.5 model, otherwise it will use LlavaConfig from hf transformers\n                from transformers import AutoTokenizer\n                from llavavid.model.language_model.llava_llama import LlavaConfig, LlavaLlamaForCausalLM\n\n                self._tokenizer = AutoTokenizer.from_pretrained(pretrained, use_fast=False)\n                cfg_pretrained = LlavaConfig.from_pretrained(pretrained)\n                if overwrite_config is not None:\n                    eval_logger.log(f\"Overwriting config with {overwrite_config}\")\n                    for k, v in overwrite_config.items():\n                        setattr(cfg_pretrained, k, v)\n                kwargs[\"torch_dtype\"] = torch.float16\n                self._model = LlavaLlamaForCausalLM.from_pretrained(pretrained, low_cpu_mem_usage=True, config=cfg_pretrained, device_map=self.device_map, **kwargs)\n                vision_tower = self._model.get_vision_tower()\n                if not vision_tower.is_loaded:\n                    vision_tower.load_model(device_map=self.device_map)\n                if self.device_map != \"auto\":\n                    vision_tower.to(device=\"cuda\", dtype=torch.float16)\n                self._image_processor = vision_tower.image_processor\n\n                if hasattr(self._model.config, \"max_sequence_length\"):\n                    self._max_length = self._model.config.max_sequence_length\n                else:\n                    self._max_length = 2048\n            else:\n                self._tokenizer, self._model, self._image_processor, self._max_length = load_pretrained_model(pretrained, None, self.model_name, device_map=self.device_map, overwrite_config=overwrite_config)\n        else:\n            self._tokenizer, self._model, self._image_processor, self._max_length = load_pretrained_model(\n                pretrained,\n                None,\n                self.model_name,\n                device_map=self.device_map,\n            )\n\n        self._config = self._model.config\n        self.model.eval()\n        if tie_weights:\n            self.model.tie_weights()\n        self.truncation = truncation\n        self.batch_size_per_gpu = int(batch_size)\n        self.conv_template = conv_template\n        self.use_cache = use_cache\n        self.truncate_context = truncate_context\n        # assert self.batch_size_per_gpu == 1, \"Llava currently does not support batched generation. See https://github.com/haotian-liu/LLaVA/issues/754. HF Llava also has this issue.\"\n        if accelerator.num_processes > 1:\n            assert accelerator.distributed_type in [DistributedType.FSDP, DistributedType.MULTI_GPU, DistributedType.DEEPSPEED], \"Unsupported distributed type provided. Only DDP and FSDP are supported.\"\n            # If you want to use DistributedType.DEEPSPEED, you have to run accelerate config before using the model\n            # Also, you have to select zero stage 0 (equivalent to DDP) in order to make the prepare model works\n            # I tried to set different parameters in the kwargs to let default zero 2 stage works, but it didn't work.\n            if accelerator.distributed_type == DistributedType.DEEPSPEED:\n                kwargs = {\n                    \"train_micro_batch_size_per_gpu\": self.batch_size_per_gpu,\n                    \"train_batch_size\": self.batch_size_per_gpu * accelerator.num_processes,\n                }\n                AcceleratorState().deepspeed_plugin.deepspeed_config_process(must_match=True, **kwargs)\n                eval_logger.info(\"Detected that you are using DistributedType.DEEPSPEED. Make sure you run `accelerate config` and set zero stage to 0\")\n            if accelerator.distributed_type == DistributedType.FSDP or accelerator.distributed_type == DistributedType.DEEPSPEED:\n                self._model = accelerator.prepare(self.model)\n            else:\n                self._model = accelerator.prepare_model(self.model, evaluation_mode=True)\n            self.accelerator = accelerator\n            if self.accelerator.is_local_main_process:\n                eval_logger.info(f\"Using {accelerator.num_processes} devices with data parallelism\")\n            self._rank = self.accelerator.local_process_index\n            self._world_size = self.accelerator.num_processes\n        elif accelerator.num_processes == 1 and device_map == \"auto\":\n            eval_logger.info(f\"Using {accelerator.num_processes} devices with tensor parallelism\")\n            self._rank = 0\n            self._word_size = 1\n        else:\n            eval_logger.info(f\"Using single device: {self._device}\")\n            self.model.to(self._device)\n            self._rank = 0\n            self._world_size = 1\n\n    @property\n    def config(self):\n        # return the associated transformers.AutoConfig for the given pretrained model.\n        return self._config\n\n    @property\n    def tokenizer(self):\n        return self._tokenizer\n\n    @property\n    def model(self):\n        # returns the model, unwrapping it if using Accelerate\n        if hasattr(self, \"accelerator\"):\n            return self.accelerator.unwrap_model(self._model)\n        else:\n            return self._model\n\n    @property\n    def eot_token_id(self):\n        # we use EOT because end of *text* is more accurate for what we're doing than end of *sentence*\n        return self.tokenizer.eos_token_id\n\n    @property\n    def max_length(self):\n        return self._max_length\n\n    def pad_sequence(self, input_ids, batch_first, padding_value):\n        if self.tokenizer.padding_side == \"left\":\n            input_ids = [torch.flip(_input_ids, [0]) for _input_ids in input_ids]\n        input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=batch_first, padding_value=padding_value)\n        if self.tokenizer.padding_side == \"left\":\n            input_ids = torch.flip(input_ids, [1])\n        return input_ids\n\n    @property\n    def batch_size(self):\n        return self.batch_size_per_gpu\n\n    @property\n    def device(self):\n        return self._device\n\n    @property\n    def rank(self):\n        return self._rank\n\n    @property\n    def world_size(self):\n        return self._world_size\n\n    def tok_encode(self, string: str, left_truncate_len=None, add_special_tokens=None) -> List[int]:\n        \"\"\" \"\"\"\n        add_special_tokens = False if add_special_tokens is None else add_special_tokens\n        encoding = self.tokenizer.encode(string, add_special_tokens=add_special_tokens)\n        # left-truncate the encoded context to be at most `left_truncate_len` tokens long\n        if left_truncate_len:\n            encoding = encoding[-left_truncate_len:]\n        return encoding\n\n    def load_video(self, video_path, max_frames_num):\n        vr = VideoReader(video_path, ctx=cpu(0))\n        total_frame_num = len(vr)\n        # fps = round(vr.get_avg_fps())\n        # frame_idx = [i for i in range(0, len(vr), fps)]\n        uniform_sampled_frames = np.linspace(0, total_frame_num - 1, max_frames_num, dtype=int)\n        frame_idx = uniform_sampled_frames.tolist()\n        spare_frames = vr.get_batch(frame_idx).asnumpy()\n        return spare_frames  # (frames, height, width, channels)\n\n    def tok_decode(self, tokens):\n        return self.tokenizer.decode(tokens)\n\n    def loglikelihood(self, requests: List[Instance]) -> List[Tuple[float, bool]]:\n        res = []\n        pbar = tqdm(total=len(requests), disable=(self.rank != 0), desc=\"Model Responding\")\n\n        for contexts, doc_to_target, doc_to_visual, doc_id, task, split in [reg.args for reg in requests]:\n            # encode, pad, and truncate contexts for this batch\n            if type(doc_to_target) == str:\n                continuation = doc_to_target\n            else:\n                continuation = doc_to_target(self.task_dict[task][split][doc_id])\n            visuals = [doc_to_visual(self.task_dict[task][split][doc_id])]\n            visuals = self.flatten(visuals)\n            videos = []\n            for visual in visuals:\n                video = self.load_video(visual, self.max_frames_num)\n                video = self._image_processor.preprocess(video, return_tensors=\"pt\")[\"pixel_values\"].half().cuda()\n                videos.append(video)\n\n            qs = contexts\n            if self.model.config.mm_use_im_start_end:\n                qs = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + \"\\n\" + qs\n            else:\n                qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\n\n            conv = conv_templates[self.conv_template].copy()\n            conv.append_message(conv.roles[0], qs)\n            conv.append_message(conv.roles[1], None)\n            prompt = conv.get_prompt()\n\n            contxt_id = tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(self.device)\n\n            conv = conv_templates[self.conv_template].copy()\n            conv.append_message(conv.roles[0], qs)\n            conv.append_message(conv.roles[1], continuation)\n            prompt = conv.get_prompt()\n\n            input_ids = tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).cuda()\n            attention_masks = input_ids.ne(self.tokenizer.pad_token_id).long().cuda()\n\n            labels = input_ids.clone()\n            # Context part no need to calculate for loss\n            labels[0, : contxt_id.shape[1]] = -100\n\n            with torch.inference_mode():\n                outputs = self.model(input_ids=input_ids, labels=labels, images=videos, modalities=\"video\")\n\n            loss = outputs[\"loss\"]\n            # loss = torch.exp(loss)\n            logits = outputs[\"logits\"]\n            greedy_tokens = logits.argmax(dim=-1)\n            cont_toks = input_ids[:, contxt_id.shape[1] :]  # [1, seq]\n            greedy_tokens = greedy_tokens[:, contxt_id.shape[1] : input_ids.shape[1]]  # [1, seq]\n            max_equal = (greedy_tokens == cont_toks).all()\n            res.append((float(loss.item()), bool(max_equal)))\n            pbar.update(1)\n        pbar.close()\n        return res\n\n    def flatten(self, input):\n        new_list = []\n        for i in input:\n            for j in i:\n                new_list.append(j)\n        return new_list\n\n    def generate_until(self, requests) -> List[str]:\n        res = []\n        pbar = tqdm(total=len(requests), disable=(self.rank != 0), desc=\"Model Responding\")\n\n        for contexts, gen_kwargs, doc_to_visual, doc_id, task, split in [reg.args for reg in requests]:\n            # encode, pad, and truncate contexts for this batch\n            visuals = [doc_to_visual(self.task_dict[task][split][doc_id])]\n            visuals = self.flatten(visuals)\n            videos = []\n            try:\n                for visual in visuals:\n                    if self.video_decode_backend == \"decord\":\n                        video = self.load_video(visual, self.max_frames_num)\n                    elif self.video_decode_backend == \"pyav\":\n                        video = read_video_pyav(visual, num_frm=self.max_frames_num)\n                    # video = self.load_video(visual, self.max_frames_num)\n                    video = self._image_processor.preprocess(video, return_tensors=\"pt\")[\"pixel_values\"].half().cuda()\n                    videos.append(video)\n            except Exception as e:\n                eval_logger.info(f\"{e}\")\n                eval_logger.info(f\"Video {visuals} can not load, check the source\")\n                video_path = \"\\n\".join(visuals)\n                res.append(f\"Video {video_path} can not load, check the source\")\n                pbar.update(1)\n                continue\n\n            qs = contexts\n            if self.model.config.mm_use_im_start_end:\n                qs = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + \"\\n\" + qs\n            else:\n                qs = DEFAULT_IMAGE_TOKEN * len(videos) + \"\\n\" + qs\n\n            # This is much safer for llama3, as we now have some object type in it\n            if \"llama_3\" in self.conv_template:\n                conv = copy.deepcopy(conv_templates[self.conv_template])\n            else:\n                conv = conv_templates[self.conv_template].copy()\n\n            conv.append_message(conv.roles[0], qs)\n            conv.append_message(conv.roles[1], None)\n            prompt = conv.get_prompt()\n\n            input_ids = tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).cuda()\n            pad_token_ids = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id\n            if \"llama_3\" in self.conv_template:\n                pad_token_ids = 0  # lmms-lab/llama3-llava-8b is trained on this pad token id. You may need to customize this for other models.\n            attention_masks = input_ids.ne(pad_token_ids).long().cuda()\n\n            stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n            keywords = [stop_str]\n            stopping_criteria = KeywordsStoppingCriteria(keywords, self.tokenizer, input_ids)\n\n            cur_prompt = contexts\n\n            if \"max_new_tokens\" not in gen_kwargs:\n                gen_kwargs[\"max_new_tokens\"] = 1024\n            if \"temperature\" not in gen_kwargs:\n                gen_kwargs[\"temperature\"] = 0\n            if \"top_p\" not in gen_kwargs:\n                gen_kwargs[\"top_p\"] = None\n            if \"num_beams\" not in gen_kwargs:\n                gen_kwargs[\"num_beams\"] = 1\n            with torch.inference_mode():\n                output_ids = self.model.generate(\n                    inputs=input_ids,\n                    images=videos,\n                    attention_mask=attention_masks,\n                    modalities=\"video\",\n                    use_cache=self.use_cache,\n                    stopping_criteria=[stopping_criteria],\n                    do_sample=True if gen_kwargs[\"temperature\"] > 0 else False,\n                    temperature=gen_kwargs[\"temperature\"],\n                    top_p=gen_kwargs[\"top_p\"],\n                    num_beams=gen_kwargs[\"num_beams\"],\n                    max_new_tokens=gen_kwargs[\"max_new_tokens\"],\n                )\n                # output_ids = model.generate(inputs=input_ids, images=video, attention_mask=attention_masks, modalities=\"video\", do_sample=True, temperature=0.2, use_cache=True, stopping_criteria=[stopping_criteria])\n\n            outputs = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n            res.append(outputs)\n            pbar.update(1)\n        return res\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/models/claude.py", "content": "from io import BytesIO\nfrom copy import deepcopy\nimport os\nimport base64\nimport json\nfrom typing import List, Tuple, Union\nfrom tqdm import tqdm\nimport time\n\nfrom lmms_eval.api.instance import Instance\nfrom lmms_eval.api.model import lmms\nfrom lmms_eval.api.registry import register_model\n\nfrom accelerate import Accelerator, DistributedType\n\nfrom PIL import Image\n\nNUM_SECONDS_TO_SLEEP = 5\n\nfrom loguru import logger\n\neval_logger = logger\n\ntry:\n    import anthropic\n    from decord import VideoReader, cpu\n    import numpy as np\nexcept Exception as e:\n    eval_logger.warning(f\"Error importing claude: {e}\")\n\nAPI_URL = os.getenv(\"ANTHROPIC_API_URL\", \"https://api.anthropic.com/v1/complete\")\nAPI_KEY = os.getenv(\"ANTHROPIC_API_KEY\", \"YOUR_API_KEY\")\n\n\n@register_model(\"claude\")\nclass Claude(lmms):\n    def __init__(\n        self,\n        model_version: str = \"claude-3-opus-20240229\",\n        image_token: str = \"<image>\",  # Use to separate interleaved image and text\n        system_prompt: str = \"\",  # Whether you want some special system prompt here\n        modality: str = \"image\",\n        max_frames_num: int = 10,\n        continual_mode: bool = False,\n        response_persistent_folder: str = None,\n        **kwargs,\n    ) -> None:\n        super().__init__()\n        self.model_version = model_version\n        self.image_token = image_token\n        self.system_prompt = system_prompt\n        self.modality = modality\n        self.max_frames_num = max_frames_num\n\n        self.continual_mode = continual_mode\n        if self.continual_mode:\n            if response_persistent_folder is None:\n                raise ValueError(\"Continual mode requires a persistent path for the response. Please provide a valid path.\")\n\n            os.makedirs(response_persistent_folder, exist_ok=True)\n            self.response_persistent_folder = response_persistent_folder\n            self.response_persistent_file = os.path.join(self.response_persistent_folder, f\"{self.model_version}_response.json\")\n\n            if os.path.exists(self.response_persistent_file):\n                with open(self.response_persistent_file, \"r\") as f:\n                    self.response_cache = json.load(f)\n                self.cache_mode = \"resume\"\n            else:\n                self.response_cache = {}\n                self.cache_mode = \"start\"\n\n        accelerator = Accelerator()\n        if accelerator.num_processes > 1:\n            assert accelerator.distributed_type in [DistributedType.FSDP, DistributedType.MULTI_GPU, DistributedType.DEEPSPEED], \"Unsupported distributed type provided. Only DDP and FSDP are supported.\"\n            self.accelerator = accelerator\n            if self.accelerator.is_local_main_process:\n                eval_logger.info(f\"Using {accelerator.num_processes} devices with data parallelism\")\n            self._rank = self.accelerator.local_process_index\n            self._world_size = self.accelerator.num_processes\n        else:\n            self.accelerator = accelerator\n            self._rank = self.accelerator.local_process_index\n            self._world_size = self.accelerator.num_processes\n\n        self.device = self.accelerator.device\n\n    def encode_image(self, image):\n        output_buffer = BytesIO()\n        image.save(output_buffer, format=\"JPEG\")\n        byte_data = output_buffer.getvalue()\n        base64_str = base64.b64encode(byte_data).decode(\"utf-8\")\n        return base64_str\n\n    def flatten(self, input):\n        new_list = []\n        for i in input:\n            for j in i:\n                new_list.append(j)\n        return new_list\n\n    def get_image_size(self, image):\n        # Create a BytesIO object to store the image bytes\n        img_byte_array = BytesIO()\n\n        # Save the image to the BytesIO object\n        image.save(img_byte_array, format=\"PNG\")\n\n        # Get the size of the BytesIO object\n        img_size = img_byte_array.tell()\n\n        return img_size\n\n    # The max file size is 5MB for claude\n    def shrink_image_to_file_size(self, img: Image, max_file_size=4838990) -> Image:\n        # Get the current size of the image\n        original_size = self.get_image_size(img)\n\n        # If the image size is already smaller than the desired size, return\n        if original_size <= max_file_size:\n            return img\n\n        # Calculate the ratio to shrink the image\n        # Somehow I found out sqrt ratio is not enough to shrink the image\n        # below threshold, so I guess we do more\n        shrink_ratio = min(0.9, max_file_size / original_size)\n\n        # Resize the image with the calculated ratio\n        new_width = int(img.width * shrink_ratio)\n        new_height = int(img.height * shrink_ratio)\n        img = img.resize((new_width, new_height), Image.LANCZOS)\n\n        return self.shrink_image_to_file_size(img, max_file_size)\n\n    def encode_video(self, video_path):\n        vr = VideoReader(video_path, ctx=cpu(0))\n        total_frame_num = len(vr)\n        uniform_sampled_frames = np.linspace(0, total_frame_num - 1, self.max_frames_num, dtype=int)\n        frame_idx = uniform_sampled_frames.tolist()\n        frames = vr.get_batch(frame_idx).asnumpy()\n\n        base64_frames = []\n        for frame in frames:\n            img = Image.fromarray(frame)\n            output_buffer = BytesIO()\n            img.save(output_buffer, format=\"JPEG\")\n            byte_data = output_buffer.getvalue()\n            base64_str = base64.b64encode(byte_data).decode(\"utf-8\")\n            base64_frames.append(f\"{base64_str}\")\n\n        return base64_frames\n\n    def generate_until(self, requests) -> List[str]:\n        client = anthropic.Anthropic()\n\n        res = []\n        pbar = tqdm(total=len(requests), disable=(self.rank != 0), desc=\"Model Responding\")\n\n        empty_image_block = {\n            \"type\": \"image\",\n            \"source\": {\n                \"type\": \"base64\",\n                \"media_type\": \"image/jpeg\",\n            },\n        }\n        empty_text_block = {\"type\": \"text\"}\n        empty_messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [],\n            }\n        ]\n\n        for contexts, gen_kwargs, doc_to_visual, doc_id, task, split in [reg.args for reg in requests]:\n            ###################### CONTINUAL MODE ######################\n            if self.continual_mode is True and self.cache_mode == \"resume\":\n                doc_uuid = f\"{task}___{split}___{doc_id}\"\n                if doc_uuid in self.response_cache:\n                    response_text = self.response_cache[doc_uuid]\n                    if response_text:\n                        res.append(response_text)\n                        pbar.update(1)\n                        continue\n\n            visuals = [doc_to_visual(self.task_dict[task][split][doc_id])]\n            visuals = self.flatten(visuals)\n            imgs = []\n            for visual in visuals:\n                if isinstance(visual, str) and os.path.exists(visual):  # Assuming visual is a path to a video\n                    visual = self.encode_video(visual)\n                    for img in visual:\n                        imgs.append(img)\n                else:\n                    visual = self.shrink_image_to_file_size(visual)\n                    img = self.encode_image(visual)\n                    imgs.append(img)\n\n            messages = deepcopy(empty_messages)\n\n            if self.image_token not in contexts:\n                for img in imgs:\n                    image_block = deepcopy(empty_image_block)\n                    image_block[\"source\"][\"data\"] = img\n                    messages[0][\"content\"].append(image_block)\n                text_block = deepcopy(empty_text_block)\n                text_block[\"text\"] = contexts\n                messages[0][\"content\"].append(text_block)\n            else:\n                contexts = contexts.split(self.image_token)\n                for idx, img in enumerate(imgs):\n                    text_block = deepcopy(empty_text_block)\n                    image_block = deepcopy(empty_image_block)\n                    text_block[\"text\"] = contexts\n                    messages[0][\"content\"].append(text_block)\n                    image_block[\"source\"][\"data\"] = img\n                    messages[0][\"content\"].append(image_block)\n\n                # If n image tokens are in the contexts\n                # contexts will be splitted into n+1 chunks\n                # Manually add it into the messages\n                text_block = deepcopy(empty_text_block)\n                text_block[\"text\"] = contexts\n                messages[\"content\"].append(text_block)\n\n            if \"max_new_tokens\" not in gen_kwargs:\n                gen_kwargs[\"max_new_tokens\"] = 1024\n            if gen_kwargs[\"max_new_tokens\"] > 4096:\n                gen_kwargs[\"max_new_tokens\"] = 4096\n            if \"temperature\" not in gen_kwargs:\n                gen_kwargs[\"temperature\"] = 0\n            if \"top_p\" not in gen_kwargs or gen_kwargs[\"top_p\"] is None:\n                gen_kwargs[\"top_p\"] = 1\n            if \"num_beams\" not in gen_kwargs:\n                gen_kwargs[\"num_beams\"] = 1\n\n            for attempt in range(5):\n                try:\n                    message = client.messages.create(model=self.model_version, max_tokens=gen_kwargs[\"max_new_tokens\"], system=self.system_prompt, temperature=gen_kwargs[\"temperature\"], top_p=gen_kwargs[\"top_p\"], messages=messages)\n                except Exception as e:\n                    eval_logger.info(f\"Attempt {attempt + 1} failed with error: {str(e)}\")\n                    if attempt < 5 - 1:  # If we have retries left, sleep and then continue to next attempt\n                        time.sleep(NUM_SECONDS_TO_SLEEP)\n                    else:  # If this was the last attempt, log and return empty\n                        eval_logger.error(f\"All 5 attempts failed. Last error message: {str(e)}\")\n                        res.append(\"\")\n                        pbar.update(1)\n                        continue\n\n            response_text = message.content[0].text\n            res.append(message.content[0].text)\n            pbar.update(1)\n\n            ###################### CONTINUAL MODE ######################\n            if self.continual_mode is True:  # Cache the response\n                response_text = message.content[0].text\n                doc_uuid = f\"{task}___{split}___{doc_id}\"\n                self.response_cache[doc_uuid] = response_text\n                with open(self.response_persistent_file, \"w\") as f:\n                    json.dump(self.response_cache, f)\n\n        pbar.close()\n\n        return res\n\n    def loglikelihood(self, requests: List[Instance]) -> List[Tuple[float, bool]]:\n        assert False, \"Not supported for claude\"\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/models/qwen_vl_api.py", "content": "from io import BytesIO\nfrom copy import deepcopy\nimport os\nimport base64\nfrom typing import List, Tuple, Union\nfrom tqdm import tqdm\nimport requests as url_requests\nimport time\n\n\nfrom lmms_eval.api.instance import Instance\nfrom lmms_eval.api.model import lmms\nfrom lmms_eval.api.registry import register_model\nfrom lmms_eval import utils\n\nfrom PIL import Image\n\nNUM_SECONDS_TO_SLEEP = 5\nfrom loguru import logger as eval_logger\n\ntry:\n    import dashscope\nexcept:\n    eval_logger.debug(\"Can not import Dashscope\")\n\nAPI_KEY = os.getenv(\"DASHSCOPE_API_KEY\", \"YOUR_API_KEY\")\n\n\n@register_model(\"qwen-vl-api\")\nclass Qwen_VL_API(lmms):\n    def __init__(\n        self,\n        model_version: str = \"qwen-vl-max\",\n        image_token: str = \"<image>\",  # Use to separate interleaved image and text\n        system_prompt: str = \"\",  # Whether you want some special system prompt here\n        tmp_folder: str = \"./tmp\",  # Due to qwen's api restriction,\n        **kwargs,\n    ) -> None:\n        super().__init__()\n\n        self.model_version = model_version\n        self.image_token = image_token\n        self.system_prompt = system_prompt\n        self.tmp_folder = tmp_folder\n\n    @property\n    def rank(self):\n        return self._rank\n\n    @property\n    def world_size(self):\n        return self._world_size\n\n    def generate_until(self, requests) -> List[str]:\n        res = []\n        pbar = tqdm(total=len(requests), disable=(self.rank != 0), desc=\"Model Responding\")\n        os.makedirs(self.tmp_folder, exist_ok=True)\n\n        for contexts, gen_kwargs, doc_to_visual, doc_id, task, split in [reg.args for reg in requests]:\n            # encode, pad, and truncate contexts for this batch\n            visuals = [doc_to_visual(self.task_dict[task][split][doc_id])]\n            visuals = self.flatten(visuals)\n            imgs = []\n\n            for idx, visual in enumerate(visuals):\n                visual.save(os.path.join(self.tmp_folder, f\"tmp_{idx}_{self.rank}_{self.world_size}.jpg\"))\n                imgs.append(os.path.join(self.tmp_folder, f\"tmp_{idx}_{self.rank}_{self.world_size}.jpg\"))\n\n            messages = [{\"role\": \"user\", \"content\": []}]\n\n            if self.image_token not in contexts:\n                for img in imgs:\n                    messages[0][\"content\"].append({\"image\": img})\n                messages[0][\"content\"].append({\"text\": contexts})\n            else:\n                contexts = contexts.split(self.image_token)\n\n                for idx, img in enumerate(imgs):\n                    messages[0][\"content\"].append({\"text\": contexts[idx]})\n                    messages[0][\"content\"].append({\"image\": img})\n                messages[0][\"content\"].append({\"text\": contexts[-1]})\n\n            if \"max_new_tokens\" not in gen_kwargs or gen_kwargs[\"max_new_tokens\"] > 1500:\n                gen_kwargs[\"max_new_tokens\"] = 1024\n            if \"temperature\" not in gen_kwargs:\n                gen_kwargs[\"temperature\"] = 0\n            if \"top_p\" not in gen_kwargs:\n                gen_kwargs[\"top_p\"] = None\n            if \"num_beams\" not in gen_kwargs:\n                gen_kwargs[\"num_beams\"] = 1\n\n            for attempt in range(5):\n                try:\n                    response_data = dashscope.MultiModalConversation.call(model=self.model_version, messages=messages, api_key=API_KEY, max_length=gen_kwargs[\"max_new_tokens\"])\n                except Exception as e:\n                    eval_logger.info(f\"Attempt {attempt + 1} failed with error: {str(e)}\")\n                    if attempt < 5 - 1:  # If we have retries left, sleep and then continue to next attempt\n                        time.sleep(NUM_SECONDS_TO_SLEEP)\n                    else:  # If this was the last attempt, log and return empty\n                        eval_logger.error(f\"All 5 attempts failed. Last error message: {str(e)}\")\n                        res.append(\"\")\n                        pbar.update(1)\n                        continue\n            try:\n                res.append(response_data[\"output\"][\"choices\"][0][\"message\"][\"content\"][0][\"text\"].strip())\n            except Exception as e:\n                eval_logger.error(f\"Error {e} happens when parsing input.\")\n                eval_logger.error(f\"{response_data}\")\n                res.append(\"\")\n            pbar.update(1)\n\n        pbar.close()\n\n        return res\n\n    def loglikelihood(self, requests: List[Instance]) -> List[Tuple[float, bool]]:\n        assert False, \"Not supported for claude\"\n\n    def flatten(self, input):\n        new_list = []\n        for i in input:\n            for j in i:\n                new_list.append(j)\n        return new_list\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/models/internvl.py", "content": "import os\nfrom accelerate import Accelerator, DistributedType, InitProcessGroupKwargs\nfrom accelerate.state import AcceleratorState\nfrom typing import List, Optional, Union, Tuple\nimport torch\nfrom tqdm import tqdm\nimport numpy as np\nimport math\nfrom datetime import timedelta\nfrom transformers import AutoConfig\nfrom huggingface_hub import snapshot_download\nimport requests\n\nfrom lmms_eval import utils\nfrom lmms_eval.api.instance import Instance\nfrom lmms_eval.api.model import lmms\nfrom lmms_eval.api.registry import register_model\nfrom lmms_eval.utils import stop_sequences_criteria\nfrom PIL import Image\n\nimport subprocess\nfrom pathlib import Path\n\nwd = Path(__file__).parent.parent.parent.resolve()\nimport sys\n\nsys.path.append(os.path.join(str(wd), \"InternVL\", \"internvl_chat\"))\nfrom loguru import logger as eval_logger\n\nif not hasattr(eval_logger, \"internvl_warning_logged\"):\n    eval_logger.internvl_warning_logged = False\n\ntry:\n    from internvl.model.internlm2.modeling_internlm2 import InternLM2ForCausalLM\n    from internvl.model.internvl_chat.configuration_internvl_chat import InternVLChatConfig\n    from internvl.model.internvl_chat.modeling_intern_vit import InternVisionModel\n    from internvl.model.internvl_chat import InternVLChatModel\n    from internvl.train.dataset import build_transform, dynamic_preprocess\nexcept ImportError:\n    eval_logger.debug(\"InternVL is not installed. Please install InternVL to use this model.\")\n    if not eval_logger.internvl_warning_logged:\n        eval_logger.debug(\"InternVL is not installed. Please install InternVL to use this model.\")\n        eval_logger.internvl_warning_logged = True\n\nimport warnings\nfrom typing import Any, List, Optional, Tuple, Union\n\nimport torch.utils.checkpoint\n\nfrom peft import LoraConfig, get_peft_model\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\nfrom transformers import AutoModel, GenerationConfig, LlamaForCausalLM, LlamaTokenizer\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers import AutoTokenizer\nimport re\nfrom huggingface_hub import snapshot_download\n\n\n@register_model(\"internvl\")\nclass InternVLChat(lmms):\n    # config_class = InternVLChatConfig\n    main_input_name = \"pixel_values\"\n    _no_split_modules = [\"InternVisionEncoderLayer\", \"LlamaDecoderLayer\"]\n\n    \"\"\"\n    0. Install lmms-eval\n    cd lmms-eval\n    pip install -e .\n\n    How to Install InternVL:\n    1. Clone the InternVL repository:\n    git clone https://github.com/OpenGVLab/InternVL.git\n\n    2. Install the requirements:\n    pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118\n\n    3. Install flash-attn==2.3.6:\n    pip install flash-attn==2.3.6 --no-build-isolation\n    \"\"\"\n\n    \"\"\"\n    How to download the pretrained model:\n    1. Download the pretrained model from hugginface:\n    cd pretrained/\n    # pip install -U huggingface_hub\n    huggingface-cli download --resume-download --local-dir-use-symlinks False OpenGVLab/InternVL-Chat-V1-5 --local-dir InternVL-Chat-V1-5\n\n    2. the pretrained model should be in the following directory:\n    pretrained\n    └── InternVL-Chat-V1-5\n    \"\"\"\n\n    #\n    # The above steps can be optional, I add snapshot download, so now can just use hf repo_id\n    # model_args pretrained=OpenGVLab/InternVL-Chat-V1-5\n    #\n\n    \"\"\"\n    InternVL-Chat-V1-5 Model for OpenGVLab https://github.com/OpenGVLab/InternVL/blob/main/internvl_chat/internvl/model/internvl_chat/modeling_internvl_chat.py\n    Example usage:\n\n    accelerate launch --num_processes=8 --main_process_port 12345 -m lmms_eval \\\n        --model internvl \\\n        --model_args pretrained=OpenGVLab/InternVL-Chat-V1-5 \\\n        --tasks llava_wilder_small \\\n        --batch_size 1 \\\n        --output_path ./logs/ \\\n        --log_samples\n    \"\"\"\n\n    def __init__(\n        self,\n        config=None,\n        pretrained: str = \"OpenGVLab/InternVL-Chat-V1-5\",\n        truncation: Optional[bool] = True,\n        device: Optional[str] = \"cuda:0\",\n        dtype: Optional[Union[str, torch.dtype]] = \"auto\",\n        batch_size: Optional[Union[int, str]] = 1,\n        trust_remote_code: Optional[bool] = False,\n        revision=None,\n        device_map=\"cuda:0\",\n        conv_template=\"vicuna_v1\",\n        use_cache=True,\n        truncate_context=False,  # whether to truncate the context in generation, set it False for LLaVA-1.6\n        customized_config=None,  # ends in json\n        dynamic=True,\n        load_in_8bit=False,\n        vision_model=None,\n        language_model=None,\n        max_num=12,\n        **kwargs,\n    ) -> None:\n        super().__init__()\n\n        assert kwargs == {}, f\"Unexpected kwargs: {kwargs}\"\n\n        accelerator_kwargs = InitProcessGroupKwargs(timeout=timedelta(weeks=52))\n        accelerator = Accelerator(kwargs_handlers=[accelerator_kwargs])\n        if accelerator.num_processes > 1:\n            self._device = torch.device(f\"cuda:{accelerator.local_process_index}\")\n            self.device_map = f\"cuda:{accelerator.local_process_index}\"\n        elif accelerator.num_processes == 1 and device_map == \"auto\":\n            self._device = torch.device(device)\n            self.device_map = device_map\n        else:\n            self._device = torch.device(f\"cuda:{accelerator.local_process_index}\")\n            self.device_map = f\"cuda:{accelerator.local_process_index}\"\n\n        self.dynamic = dynamic  # dynamic image_size\n        self.max_num = max_num\n        if accelerator.is_main_process:\n            cache_dir = snapshot_download(repo_id=pretrained, cache_dir=\"cache_dir\", local_dir=\"cache_dir\", local_dir_use_symlinks=False)\n        accelerator.wait_for_everyone()\n        # So what I did is that I let main process to download the repo, and then\n        # other process can just simply read from this repo\n        cache_dir = snapshot_download(repo_id=pretrained, cache_dir=\"cache_dir\", local_dir=\"cache_dir\", local_dir_use_symlinks=False)\n        config = InternVLChatConfig.from_pretrained(cache_dir)\n        tokenizer = AutoTokenizer.from_pretrained(cache_dir, trust_remote_code=True, use_fast=False)\n        model = InternVLChatModel.from_pretrained(cache_dir, low_cpu_mem_usage=True, config=config, torch_dtype=torch.bfloat16, load_in_8bit=load_in_8bit).eval()\n        if not load_in_8bit:\n            model = model.cuda()\n        # self.model=model\n        # self.device=self._device\n        self._tokenizer = tokenizer\n        # self.tokenizer=tokenizer\n        self._model = model\n        self._config = self._model.config\n        self.use_thumbnail = self.model.config.use_thumbnail\n        self.model.eval()\n        self.model.tie_weights()\n        self.truncation = truncation\n        self.batch_size_per_gpu = int(batch_size)\n        self.conv_template = conv_template\n        self.use_cache = use_cache\n        self.truncate_context = truncate_context\n        if accelerator.num_processes > 1:\n            assert accelerator.distributed_type in [DistributedType.FSDP, DistributedType.MULTI_GPU, DistributedType.DEEPSPEED], \"Unsupported distributed type provided. Only DDP and FSDP are supported.\"\n            # If you want to use DistributedType.DEEPSPEED, you have to run accelerate config before using the model\n            # Also, you have to select zero stage 0 (equivalent to DDP) in order to make the prepare model works\n            # I tried to set different parameters in the kwargs to let default zero 2 stage works, but it didn't work.\n            if accelerator.distributed_type == DistributedType.DEEPSPEED:\n                kwargs = {\n                    \"train_micro_batch_size_per_gpu\": self.batch_size_per_gpu,\n                    \"train_batch_size\": self.batch_size_per_gpu * accelerator.num_processes,\n                }\n                AcceleratorState().deepspeed_plugin.deepspeed_config_process(must_match=True, **kwargs)\n                eval_logger.info(\"Detected that you are using DistributedType.DEEPSPEED. Make sure you run `accelerate config` and set zero stage to 0\")\n\n            if accelerator.distributed_type == DistributedType.FSDP or accelerator.distributed_type == DistributedType.DEEPSPEED:\n                self._model = accelerator.prepare(self.model)\n            else:\n                self._model = accelerator.prepare_model(self.model, evaluation_mode=True)\n            self.accelerator = accelerator\n            if self.accelerator.is_local_main_process:\n                eval_logger.info(f\"Using {accelerator.num_processes} devices with data parallelism\")\n            self._rank = self.accelerator.local_process_index\n            self._world_size = self.accelerator.num_processes\n        elif accelerator.num_processes == 1 and device_map == \"auto\":\n            eval_logger.info(f\"Using {accelerator.num_processes} devices with tensor parallelism\")\n            self._rank = 0\n            self._word_size = 1\n        else:\n            eval_logger.info(f\"Using single device: {self._device}\")\n            self.model.to(self._device)\n            self._rank = 0\n            self._world_size = 1\n\n        # from internvl model\n\n        self.image_size = config.force_image_size or config.vision_config.image_size\n\n    def wrap_backbone_lora(self, r=128, lora_alpha=256, lora_dropout=0.05):\n        lora_config = LoraConfig(\n            r=r,\n            target_modules=[\"attn.qkv\", \"attn.proj\", \"mlp.fc1\", \"mlp.fc2\"],\n            lora_alpha=lora_alpha,\n            lora_dropout=lora_dropout,\n        )\n        self.vision_model = get_peft_model(self.vision_model, lora_config)\n        self.vision_model.print_trainable_parameters()\n\n    def wrap_llm_lora(self, r=128, lora_alpha=256, lora_dropout=0.05):\n        lora_config = LoraConfig(\n            r=r, target_modules=[\"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.o_proj\", \"mlp.gate_proj\", \"mlp.down_proj\", \"mlp.up_proj\"], lora_alpha=lora_alpha, lora_dropout=lora_dropout, task_type=\"CAUSAL_LM\"\n        )\n        self.language_model = get_peft_model(self.language_model, lora_config)\n        self.language_model.enable_input_require_grads()\n        self.language_model.print_trainable_parameters()\n\n    def pixel_shuffle(self, x, scale_factor=0.5):\n        n, w, h, c = x.size()\n        # N, W, H, C --> N, W, H * scale, C // scale\n        x = x.view(n, w, int(h * scale_factor), int(c / scale_factor))\n        # N, W, H * scale, C // scale --> N, H * scale, W, C // scale\n        x = x.permute(0, 2, 1, 3).contiguous()\n        # N, H * scale, W, C // scale --> N, H * scale, W * scale, C // (scale ** 2)\n        x = x.view(n, int(h * scale_factor), int(w * scale_factor), int(c / (scale_factor * scale_factor)))\n        if self.ps_version == \"v1\":\n            warnings.warn(\"In ps_version 'v1', the height and width have not been swapped back, \" \"which results in a transposed image.\")\n        else:\n            x = x.permute(0, 2, 1, 3).contiguous()\n        return x\n\n    def noised_embed(self, vit_embeds, noise_alpha=5):\n        dims = torch.tensor(vit_embeds.size(1) * vit_embeds.size(2))\n        mag_norm = noise_alpha / torch.sqrt(dims)\n        noise = torch.zeros_like(vit_embeds).uniform_(-mag_norm, mag_norm)\n        return vit_embeds + noise\n\n    def extract_feature(self, pixel_values):\n        if self.select_layer == -1:\n            vit_embeds = self.vision_model(pixel_values=pixel_values, output_hidden_states=False, return_dict=True).last_hidden_state\n        else:\n            vit_embeds = self.vision_model(pixel_values=pixel_values, output_hidden_states=True, return_dict=True).hidden_states[self.select_layer]\n        vit_embeds = vit_embeds[:, 1:, :]\n\n        if self.training and self.neftune_alpha is not None:\n            vit_embeds = self.noised_embed(vit_embeds, self.neftune_alpha)\n\n        h = w = int(vit_embeds.shape[1] ** 0.5)\n        vit_embeds = vit_embeds.reshape(vit_embeds.shape[0], h, w, -1)\n        vit_embeds = self.pixel_shuffle(vit_embeds, scale_factor=self.downsample_ratio)\n        vit_embeds = vit_embeds.reshape(vit_embeds.shape[0], -1, vit_embeds.shape[-1])\n        vit_embeds = self.mlp1(vit_embeds)  # .to(pixel_values.device)\n        return vit_embeds\n\n    def multi_image_chat(self, tokenizer, pixel_values, image_counts, question, generation_config, history=None, return_history=False, IMG_START_TOKEN=\"<img>\", IMG_END_TOKEN=\"</img>\", IMG_CONTEXT_TOKEN=\"<IMG_CONTEXT>\"):\n        img_context_token_id = tokenizer.convert_tokens_to_ids(IMG_CONTEXT_TOKEN)\n        self.img_context_token_id = img_context_token_id\n        if tokenizer.convert_tokens_to_ids(\"<|im_end|>\") != 0:\n            eos_token_id = tokenizer.convert_tokens_to_ids(\"<|im_end|>\")  # 92542, InternLM2\n        else:\n            eos_token_id = tokenizer.eos_token_id\n\n        from internvl.conversation import get_conv_template\n\n        template = get_conv_template(self.template)\n\n        if history is None:\n            history = []\n            image_tokens = \"\"\n            image_bs = pixel_values.shape[0]\n            # print(f\"dynamic ViT batch size: {image_bs}, image_counts: {image_counts}\")\n            for idx, image_count in enumerate(image_counts):\n                image_tokens += f\"<image {idx+1}> (图{idx+1}):\" + IMG_START_TOKEN + IMG_CONTEXT_TOKEN * self.num_image_token * image_count + IMG_END_TOKEN\n            question = image_tokens + \"\\n\" + question\n        else:\n            for old_question, old_answer in history:\n                template.append_message(template.roles[0], old_question)\n                template.append_message(template.roles[1], old_answer)\n        template.append_message(template.roles[0], question)\n        template.append_message(template.roles[1], None)\n        query = template.get_prompt()\n        model_inputs = tokenizer(query, return_tensors=\"pt\")\n        input_ids = model_inputs[\"input_ids\"].cuda()\n        attention_mask = model_inputs[\"attention_mask\"].cuda()\n        generation_config[\"eos_token_id\"] = eos_token_id\n\n        generation_output = self.generate(pixel_values=pixel_values, input_ids=input_ids, attention_mask=attention_mask, **generation_config)\n        response = tokenizer.batch_decode(generation_output, skip_special_tokens=True)[0]\n        response = response.split(\"<|im_end|>\")[0].strip()  # for InternLM2\n        history.append((question, response))\n        if return_history:\n            return response, history\n        else:\n            query_to_print = query.replace(image_tokens, \"<image>\")\n            # print(query_to_print, response)\n            return response\n        return response\n\n    @property\n    def tokenizer(self):\n        return self._tokenizer\n\n    @property\n    def model(self):\n        # returns the model, unwrapping it if using Accelerate\n        if hasattr(self, \"accelerator\"):\n            return self.accelerator.unwrap_model(self._model)\n        else:\n            return self._model\n\n    @property\n    def batch_size(self):\n        return self.batch_size_per_gpu\n\n    @property\n    def device(self):\n        return self._device\n\n    @property\n    def rank(self):\n        return self._rank\n\n    @property\n    def world_size(self):\n        return self._world_size\n\n    def tok_encode(self, string: str, left_truncate_len=None, add_special_tokens=None) -> List[int]:\n        \"\"\" \"\"\"\n        add_special_tokens = False if add_special_tokens is None else add_special_tokens\n        encoding = self.tokenizer.encode(string, add_special_tokens=add_special_tokens)\n        # left-truncate the encoded context to be at most `left_truncate_len` tokens long\n        if left_truncate_len:\n            encoding = encoding[-left_truncate_len:]\n        return encoding\n\n    def tok_decode(self, tokens):\n        try:\n            return self.tokenizer.decode(tokens)\n        except:\n            return self.tokenizer.decode([tokens])\n\n    def post_processing(self, response):\n        response = response.replace(\"\\n\", \"\").replace(\"不是\", \"No\").replace(\"是\", \"Yes\").replace(\"否\", \"No\")\n        response = response.lower().replace(\"true\", \"yes\").replace(\"false\", \"no\")\n        pattern = re.compile(r\"[\\u4e00-\\u9fa5]\")\n        response = re.sub(pattern, \"\", response)\n        return response\n\n    @torch.no_grad()\n    def generate(\n        self,\n        pixel_values: Optional[torch.FloatTensor] = None,\n        input_ids: Optional[torch.FloatTensor] = None,\n        attention_mask: Optional[torch.LongTensor] = None,\n        visual_features: Optional[torch.FloatTensor] = None,\n        generation_config: Optional[GenerationConfig] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        **generate_kwargs,\n    ) -> torch.LongTensor:\n        assert self.img_context_token_id is not None\n        if pixel_values is not None:\n            if visual_features is not None:\n                vit_embeds = visual_features\n            else:\n                vit_embeds = self.extract_feature(pixel_values)\n\n            input_embeds = self.language_model.get_input_embeddings()(input_ids)\n            B, N, C = input_embeds.shape\n            input_embeds = input_embeds.reshape(B * N, C)\n\n            input_ids = input_ids.reshape(B * N)\n            selected = input_ids == self.img_context_token_id\n            assert selected.sum() != 0\n            input_embeds[selected] = vit_embeds.reshape(-1, C).to(input_embeds.device)\n\n            input_embeds = input_embeds.reshape(B, N, C)\n        else:\n            input_embeds = self.language_model.get_input_embeddings()(input_ids)\n\n        outputs = self.language_model.generate(\n            inputs_embeds=input_embeds,\n            attention_mask=attention_mask,\n            generation_config=generation_config,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            use_cache=True,\n            **generate_kwargs,\n        )\n\n        return outputs\n\n    def flatten(self, input):\n        new_list = []\n        for i in input:\n            for j in i:\n                new_list.append(j)\n        return new_list\n\n    def load_image(self, flattened_visuals, input_size=224):\n        assert flattened_visuals[0].mode == \"RGB\"\n        image = flattened_visuals[0].convert(\"RGB\")\n        transform = build_transform(is_train=False, input_size=input_size)\n        if self.dynamic:\n            images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=self.use_thumbnail, max_num=self.max_num)\n        else:\n            images = [image]\n        pixel_values = [transform(image) for image in images]\n        pixel_values = torch.stack(pixel_values)\n        return pixel_values\n\n    def generate_until(self, requests: List[Instance]) -> List[str]:\n        res = []\n\n        def _collate(x):\n            # the negative sign on len(toks) sorts descending - this has a few advantages:\n            # - time estimates will always be over not underestimates, which is more useful for planning\n            # - to know the size of a batch when going through the list, you know the first one is always the batch\n            #   padded context length. this is useful to simplify the batching logic and more importantly to make\n            #   automatic adaptive batches much much easier to implement\n            # - any OOMs will happen right away rather than near the end\n            toks = self.tok_encode(x[0])\n            return -len(toks), x[0]\n\n        # we group requests by their generation_kwargs,\n        # so that we don't try to execute e.g. greedy sampling and temp=0.8 sampling\n        # in the same batch.\n        re_ords = utils.Collator([reg.args for reg in requests], _collate, grouping=True)\n        chunks = re_ords.get_batched(n=self.batch_size, batch_fn=None)\n        num_iters = len(requests) // self.batch_size if len(requests) % self.batch_size == 0 else len(requests) // self.batch_size + 1\n        pbar = tqdm(total=num_iters, disable=(self.rank != 0), desc=\"Model Responding\")\n        for chunk in chunks:\n            contexts, all_gen_kwargs, doc_to_visual, doc_id, task, split = zip(*chunk)\n            task = task[0]\n            split = split[0]\n            batched_visuals = [doc_to_visual[0](self.task_dict[task][split][ids]) for ids in doc_id]  # [B, N]\n            flattened_visuals = self.flatten(batched_visuals)\n            pixel_values = self.load_image(flattened_visuals, self.image_size).cuda().to(torch.bfloat16)\n            gen_kwargs = all_gen_kwargs[0]\n\n            if \"max_new_tokens\" not in gen_kwargs:\n                gen_kwargs[\"max_new_tokens\"] = 1024\n            if \"temperature\" not in gen_kwargs:\n                gen_kwargs[\"temperature\"] = 0\n            if \"top_p\" not in gen_kwargs:\n                gen_kwargs[\"top_p\"] = None\n            if \"num_beams\" not in gen_kwargs:\n                gen_kwargs[\"num_beams\"] = 1\n\n            generation_config = dict(\n                do_sample=False,\n                top_k=50,\n                top_p=gen_kwargs[\"top_p\"],\n                num_beams=gen_kwargs[\"num_beams\"],\n                max_new_tokens=gen_kwargs[\"max_new_tokens\"],\n                eos_token_id=self.tokenizer.eos_token_id,\n            )\n            question = contexts[0]\n            response = self.model.chat(tokenizer=self.tokenizer, pixel_values=pixel_values, question=question, generation_config=generation_config)\n            # TODO(choiszt) try batch_chat for multiple inputs\n            response = self.post_processing(response)\n            res.append(response)\n            self.cache_hook.add_partial(\"generate_until\", (question, gen_kwargs), response)\n            pbar.update(1)\n        res = re_ords.get_original(res)\n        return res\n        # print(chunk)\n\n    def loglikelihood(self, requests: List[Instance]) -> List[Tuple[float, bool]]:\n        pass\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/models/llava_hf.py", "content": "import torch\n\nfrom tqdm import tqdm\nfrom lmms_eval import utils\nfrom lmms_eval.api.instance import Instance\nfrom lmms_eval.api.model import lmms\nfrom lmms_eval.api.registry import register_model\nfrom accelerate import Accelerator, DistributedType\nfrom accelerate.state import AcceleratorState\nfrom typing import List, Optional, Union, Tuple\nfrom transformers import LlavaForConditionalGeneration, LlavaNextForConditionalGeneration, AutoProcessor\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom loguru import logger as eval_logger\n\nDEFAULT_IMAGE_TOKEN = \"<image>\"\n\n# Default chat for llava-hf/llava-1.5 models: https://huggingface.co/collections/llava-hf/llava-15-65f762d5b6941db5c2ba07e0\nVICUNA_CHAT_TEMPLATE = \"{% for message in messages %}{% if loop.index0 == 0 %}A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {{ message['content'] }} {% elif message['role'] == 'user' %}USER: {{ message['content'] }} {% else %} ASSISTANT: {{ message['content'] }}{{ eos_token }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'ASSISTANT:' }}{% endif %}\"\n\n\n@register_model(\"llava_hf\")\nclass LlavaHf(lmms):\n    \"\"\"\n    Llava Model for Hugging Face Transformers: https://huggingface.co/docs/transformers/v4.39.3/en/model_doc/llava\n\n    Adapted from the InstructBLIP model in lmms_eval/models/instructblip.py\n\n    Example usage:\n\n    accelerate launch --num_processes=8 --main_process_port 12345 -m lmms_eval \\\n        --model llava_hf \\\n        --model_args pretrained=llava-hf/llava-1.5-7b-hf \\\n        --tasks seedbench \\\n        --batch_size 1 \\\n        --output_path ./logs/ \\\n        --log_samples\n    \"\"\"\n\n    def __init__(\n        self,\n        pretrained: str = \"llava-hf/llava-1.5-7b-hf\",\n        revision: str = \"main\",\n        device: str = \"cuda\",\n        dtype: Optional[Union[str, torch.dtype]] = \"auto\",\n        batch_size: int = 1,\n        trust_remote_code: Optional[bool] = False,\n        attn_implementation: Optional[str] = None,\n        device_map: str = \"\",\n        chat_template: Optional[str] = None,\n        use_cache: bool = True,\n        specified_eot_token_id: Optional[int] = None,\n        **kwargs,\n    ) -> None:\n        super().__init__()\n        # Do not use kwargs for now\n        assert kwargs == {}, f\"Unexpected kwargs: {kwargs}\"\n\n        accelerator = Accelerator()\n        if accelerator.num_processes > 1 and device_map == \"\":\n            self._device = torch.device(f\"cuda:{accelerator.local_process_index}\")\n            self.device_map = f\"cuda:{accelerator.local_process_index}\"\n        else:\n            self._device = torch.device(device)\n            self.device_map = device_map\n        if isinstance(dtype, str) and dtype != \"auto\":\n            dtype = getattr(torch, dtype)\n\n        if \"1.5\" in pretrained:\n            self._model = LlavaForConditionalGeneration.from_pretrained(pretrained, revision=revision, torch_dtype=dtype, device_map=self.device_map, trust_remote_code=trust_remote_code, attn_implementation=attn_implementation)\n        elif \"1.6\" in pretrained:\n            self._model = LlavaNextForConditionalGeneration.from_pretrained(pretrained, revision=revision, torch_dtype=dtype, device_map=self.device_map, trust_remote_code=trust_remote_code, attn_implementation=attn_implementation)\n        else:\n            eval_logger.info(\"Not sure whether you use 1.5 or 1.6. Use 1.5 by default. This might cause bugs if you are actually using 1.6\")\n            self._model = LlavaForConditionalGeneration.from_pretrained(pretrained, revision=revision, torch_dtype=dtype, device_map=self.device_map, trust_remote_code=trust_remote_code, attn_implementation=attn_implementation)\n\n        self.pretrained = pretrained\n        self._image_processor = AutoProcessor.from_pretrained(pretrained, revision=revision, trust_remote_code=trust_remote_code)\n        # Pad from left for batched generation: https://huggingface.co/docs/transformers/v4.39.3/en/model_doc/llava#usage-tips\n        self._image_processor.tokenizer.padding_side = \"left\"\n        self._tokenizer = self._image_processor.tokenizer\n        self._config = self._model.config\n        self.batch_size_per_gpu = int(batch_size)\n        self.chat_template = chat_template\n        self.use_cache = use_cache\n        self.specified_eot_token_id = specified_eot_token_id\n        if accelerator.num_processes > 1 and device_map == \"\":\n            assert accelerator.distributed_type in [DistributedType.FSDP, DistributedType.MULTI_GPU, DistributedType.DEEPSPEED], \"Unsupported distributed type provided. Only DDP and FSDP are supported.\"\n            # If you want to use DistributedType.DEEPSPEED, you have to run accelerate config before using the model\n            # Also, you have to select zero stage 0 (equivalent to DDP) in order to make the prepare model works\n            # I tried to set different parameters in the kwargs to let default zero 2 stage works, but it didn't work.\n            if accelerator.distributed_type == DistributedType.DEEPSPEED:\n                kwargs = {\n                    \"train_micro_batch_size_per_gpu\": self.batch_size_per_gpu,\n                    \"train_batch_size\": self.batch_size_per_gpu * accelerator.num_processes,\n                }\n                AcceleratorState().deepspeed_plugin.deepspeed_config_process(must_match=True, **kwargs)\n                eval_logger.info(\"Detected that you are using DistributedType.DEEPSPEED. Make sure you run `accelerate config` and set zero stage to 0\")\n            if accelerator.distributed_type == DistributedType.FSDP or accelerator.distributed_type == DistributedType.DEEPSPEED:\n                self._model = accelerator.prepare(self.model)\n            else:\n                self._model = accelerator.prepare_model(self.model, evaluation_mode=True)\n            self.accelerator = accelerator\n            if self.accelerator.is_local_main_process:\n                eval_logger.info(f\"Using {accelerator.num_processes} devices with data parallelism\")\n            self._rank = self.accelerator.local_process_index\n            self._world_size = self.accelerator.num_processes\n        elif accelerator.num_processes == 1 and device_map == \"auto\":\n            eval_logger.info(f\"Using {accelerator.num_processes} devices with pipeline parallelism\")\n            self._rank = 0\n            self._word_size = 1\n        else:\n            eval_logger.info(f\"Using single device: {self._device}\")\n            self.model.to(self._device)\n            self._rank = 0\n            self._word_size = 1\n        self.accelerator = accelerator\n\n    @property\n    def config(self):\n        # return the associated transformers.AutoConfig for the given pretrained model.\n        return self._config\n\n    @property\n    def tokenizer(self):\n        return self._tokenizer\n\n    @property\n    def model(self):\n        # returns the model, unwrapping it if using Accelerate\n        if hasattr(self, \"accelerator\"):\n            return self.accelerator.unwrap_model(self._model)\n        else:\n            return self._model\n\n    @property\n    def eot_token_id(self):\n        # we use EOT because end of *text* is more accurate for what we're doing than end of *sentence*\n        return self.tokenizer.eos_token_id\n\n    @property\n    def max_length(self):\n        return self._max_length\n\n    @property\n    def batch_size(self):\n        return self.batch_size_per_gpu\n\n    @property\n    def device(self):\n        return self._device\n\n    @property\n    def rank(self):\n        return self._rank\n\n    @property\n    def world_size(self):\n        return self._world_size\n\n    def tok_encode(self, string: str, left_truncate_len=None, add_special_tokens=None) -> List[int]:\n        \"\"\" \"\"\"\n        add_special_tokens = False if add_special_tokens is None else add_special_tokens\n        encoding = self.tokenizer.encode(string, add_special_tokens=add_special_tokens)\n        # left-truncate the encoded context to be at most `left_truncate_len` tokens long\n        if left_truncate_len:\n            encoding = encoding[-left_truncate_len:]\n        return encoding\n\n    def tok_decode(self, tokens):\n        return self.tokenizer.decode(tokens)\n\n    def loglikelihood(self, requests: List[Instance]) -> List[Tuple[float, bool]]:\n        res = []\n        pbar = tqdm(total=len(requests), disable=(self.rank != 0), desc=\"Model Responding\")\n\n        for context, doc_to_target, doc_to_visual, doc_id, task, split in [reg.args for reg in requests]:\n            # encode, pad, and truncate contexts for this batch\n            if type(doc_to_target) == str:\n                continuation = doc_to_target\n            else:\n                continuation = doc_to_target(self.task_dict[task][split][doc_id])\n            visuals = [doc_to_visual(self.task_dict[task][split][doc_id])]\n            visuals = self.flatten(visuals)\n\n            image_tokens = [DEFAULT_IMAGE_TOKEN] * len(visuals)\n            image_tokens = \" \".join(image_tokens)\n            context = f\"{image_tokens}\\n{context}\"\n            # Apply chat template\n            messages = [{\"role\": \"user\", \"content\": context}, {\"role\": \"assistant\", \"content\": continuation}]\n            if self.chat_template is not None:\n                self.tokenizer.chat_template = self.chat_template\n                prompt = self.tokenizer.apply_chat_template(messages[:-1], tokenize=False, add_generation_prompt=True)\n                prompt_and_continuation = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n            elif self.tokenizer.chat_template is not None:\n                prompt = self.tokenizer.apply_chat_template(messages[:-1], tokenize=False, add_generation_prompt=True)\n                prompt_and_continuation = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n            else:\n                self.tokenizer.chat_template = VICUNA_CHAT_TEMPLATE\n                prompt = self.tokenizer.apply_chat_template(messages[:-1], tokenize=False, add_generation_prompt=True)\n                prompt_and_continuation = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n\n            formatted_contexts = [prompt]\n            formatted_continuation = [prompt_and_continuation]\n            model_inputs = self._image_processor(text=formatted_continuation, images=visuals).to(self._device, self.model.dtype)\n            labels = model_inputs[\"input_ids\"].clone()\n            contxt_id = self._image_processor(text=formatted_contexts, return_tensors=\"pt\")[\"input_ids\"]\n            labels[: len(contxt_id)] = -100\n\n            if self.accelerator.is_main_process and doc_id % 100 == 0:\n                eval_logger.debug(f\"Prompt for doc ID {doc_id}:\\n\\n{formatted_contexts[0]}\\n\")\n                eval_logger.debug(f\"Prompt and continuation for doc ID {doc_id}:\\n\\n{formatted_continuation[0]}\\n\")\n\n            with torch.inference_mode():\n                outputs = self.model(**model_inputs, labels=labels)\n            loss = outputs[\"loss\"]\n            logits = outputs[\"logits\"]\n            greedy_tokens = logits.argmax(dim=-1)\n            cont_toks = model_inputs[\"input_ids\"][:, contxt_id.shape[1] :]  # [1, seq]\n            greedy_tokens = greedy_tokens[:, contxt_id.shape[1] : model_inputs[\"input_ids\"].shape[1]]  # [1, seq]\n            max_equal = (greedy_tokens == cont_toks).all()\n            res.append((float(loss.item()), bool(max_equal)))\n            pbar.update(1)\n\n        pbar.close()\n        return res\n\n    def flatten(self, input):\n        new_list = []\n        for i in input:\n            for j in i:\n                new_list.append(j)\n        return new_list\n\n    def generate_until(self, requests: List[Instance]) -> List[str]:\n        res = []\n\n        def _collate(x):\n            # the negative sign on len(toks) sorts descending - this has a few advantages:\n            # - time estimates will always be over not underestimates, which is more useful for planning\n            # - to know the size of a batch when going through the list, you know the first one is always the batch\n            #   padded context length. this is useful to simplify the batching logic and more importantly to make\n            #   automatic adaptive batches much much easier to implement\n            # - any OOMs will happen right away rather than near the end\n            toks = self.tok_encode(x[0])\n            return -len(toks), x[0]\n\n        # we group requests by their generation_kwargs,\n        # so that we don't try to execute e.g. greedy sampling and temp=0.8 sampling\n        # in the same batch.\n        re_ords = utils.Collator([reg.args for reg in requests], _collate, grouping=True)\n        chunks = re_ords.get_batched(n=self.batch_size, batch_fn=None)\n        num_iters = len(requests) // self.batch_size if len(requests) % self.batch_size == 0 else len(requests) // self.batch_size + 1\n        pbar = tqdm(total=num_iters, disable=(self.rank != 0), desc=\"Model Responding\")\n        for chunk in chunks:\n            contexts, all_gen_kwargs, doc_to_visual, doc_id, task, split = zip(*chunk)\n            task = task[0]\n            split = split[0]\n            visuals = [doc_to_visual[0](self.task_dict[task][split][ids]) for ids in doc_id]\n            visuals = self.flatten(visuals)\n            # we assume all gen kwargs in the batch are the same\n            # this is safe to assume because the `grouper` object ensures it.\n            gen_kwargs = all_gen_kwargs[0]\n\n            # Set default values for until and max_new_tokens\n            until = [self.tok_decode(self.eot_token_id)]\n\n            # Update values from gen_kwargs if present\n            if \"until\" in gen_kwargs:\n                until = gen_kwargs.pop(\"until\")\n                if isinstance(until, str):\n                    until = [until]\n                elif not isinstance(until, list):\n                    raise ValueError(f\"Expected `gen_kwargs['until']` to be of type Union[str,list] but got {type(until)}\")\n            assert self.batch_size_per_gpu == 1, \"Do not support batch_size_per_gpu > 1 for now\"\n            context = contexts[0]\n\n            # Some benchmarks like MME do not contain image tokens, so we prepend them to the prompt.\n            if DEFAULT_IMAGE_TOKEN not in context:\n                image_tokens = [DEFAULT_IMAGE_TOKEN] * len(visuals)\n                image_tokens = \" \".join(image_tokens)\n                context = f\"{image_tokens}\\n{context}\"\n            # Apply chat template\n            messages = [{\"role\": \"user\", \"content\": context}]\n            if self.chat_template is not None:\n                self.tokenizer.chat_template = self.chat_template\n                text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n            elif self.tokenizer.chat_template is not None:\n                text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n            else:\n                self.tokenizer.chat_template = VICUNA_CHAT_TEMPLATE\n                text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n            if self.accelerator.is_main_process and doc_id[0] % 100 == 0:\n                eval_logger.debug(f\"Prompt for doc ID {doc_id[0]}:\\n\\n{text}\\n\")\n\n            inputs = self._image_processor(images=visuals, text=text, return_tensors=\"pt\").to(self._device, self.model.dtype)\n\n            gen_kwargs[\"image_sizes\"] = [visuals[idx].size for idx in range(len(visuals))]\n            if \"max_new_tokens\" not in gen_kwargs:\n                gen_kwargs[\"max_new_tokens\"] = 1024\n            if \"temperature\" not in gen_kwargs:\n                gen_kwargs[\"temperature\"] = 0\n            if \"top_p\" not in gen_kwargs:\n                gen_kwargs[\"top_p\"] = None\n            if \"num_beams\" not in gen_kwargs:\n                gen_kwargs[\"num_beams\"] = 1\n            try:\n                cont = self.model.generate(\n                    **inputs,\n                    do_sample=True if gen_kwargs[\"temperature\"] > 0 else False,\n                    temperature=gen_kwargs[\"temperature\"],\n                    top_p=gen_kwargs[\"top_p\"],\n                    num_beams=gen_kwargs[\"num_beams\"],\n                    max_new_tokens=gen_kwargs[\"max_new_tokens\"],\n                    use_cache=self.use_cache,\n                    pad_token_id=self.tokenizer.eos_token_id,\n                    eos_token_id=self.specified_eot_token_id,\n                )\n                cont = cont[:, inputs[\"input_ids\"].shape[-1]:]\n            except Exception as e:\n                eval_logger.error(f\"Error {e} in generating\")\n                cont = \"\"\n            text_outputs = self.tokenizer.batch_decode(cont, skip_special_tokens=True)[0]\n            if self.accelerator.is_main_process and doc_id[0] % 100 == 0:\n                eval_logger.debug(f\"Generated text for doc ID {doc_id[0]}:\\n\\n{text_outputs}\\n\")\n\n            res.append(text_outputs)\n            self.cache_hook.add_partial(\"generate_until\", (context, gen_kwargs), text_outputs)\n            pbar.update(1)\n        # reorder this group of results back to original unsorted form\n        res = re_ords.get_original(res)\n\n        pbar.close()\n        return res\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/models/mplug_owl_video/__init__.py", "content": "# Copyright 2020 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom typing import TYPE_CHECKING\n\nfrom transformers.utils import OptionalDependencyNotAvailable, _LazyModule, is_tokenizers_available, is_torch_available\n\n\n_import_structure = {\n    \"configuration_mplug_owl\": [\"MPLUG_OWL_PRETRAINED_CONFIG_ARCHIVE_MAP\", \"MplugOwlConfig\"],\n    \"processing_mplug_owl\": [\"MplugOwlImageProcessor\", \"MplugOwlProcessor\"],\n    \"tokenization_mplug_owl\": [\"MplugOwlTokenizer\"],\n}\n\ntry:\n    if not is_tokenizers_available():\n        raise OptionalDependencyNotAvailable()\nexcept OptionalDependencyNotAvailable:\n    pass\n\n\ntry:\n    if not is_torch_available():\n        raise OptionalDependencyNotAvailable()\nexcept OptionalDependencyNotAvailable:\n    pass\nelse:\n    _import_structure[\"modeling_mplug_owl\"] = [\n        \"MPLUG_OWL_PRETRAINED_MODEL_ARCHIVE_LIST\",\n        \"MplugOwlForConditionalGeneration\",\n        \"MplugOwlModel\",\n    ]\n\n\nif TYPE_CHECKING:\n    from .configuration_mplug_owl import MPLUG_OWL_PRETRAINED_CONFIG_ARCHIVE_MAP, MplugOwlConfig\n    from .tokenization_mplug_owl import MplugOwlTokenizer\n\n    try:\n        if not is_tokenizers_available():\n            raise OptionalDependencyNotAvailable()\n    except OptionalDependencyNotAvailable:\n        pass\n\n    try:\n        if not is_torch_available():\n            raise OptionalDependencyNotAvailable()\n    except OptionalDependencyNotAvailable:\n        pass\n    else:\n        from .modeling_mplug_owl import (\n            MPLUG_OWL_PRETRAINED_MODEL_ARCHIVE_LIST,\n            MplugOwlForConditionalGeneration,\n            MplugOwlModel,\n            MplugOwlPreTrainedModel,\n        )\n\n\nelse:\n    import sys\n\n    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n\nfrom .configuration_mplug_owl import *\nfrom .modeling_mplug_owl import *\nfrom .processing_mplug_owl import *\nfrom .tokenization_mplug_owl import *\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/__init__.py", "content": ""}
{"type": "source_file", "path": "evaluate/lmms_eval/models/instructblip.py", "content": "import torch\n\nimport copy\nfrom tqdm import tqdm\nfrom lmms_eval import utils\nfrom lmms_eval.api.instance import Instance\nfrom lmms_eval.api.model import lmms\nfrom lmms_eval.api.registry import register_model\nfrom lmms_eval.tasks.mmmu.utils_group_img import process_images\nfrom accelerate import Accelerator, DistributedType\nfrom accelerate.state import AcceleratorState\nfrom typing import List, Optional, Union, Tuple\nimport transformers\nfrom transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration\n\nfrom lmms_eval.utils import stop_sequences_criteria\n\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom loguru import logger as eval_logger\n\n\n@register_model(\"instructblip\")\nclass InstructBLIP(lmms):\n    \"\"\"\n    InstructBLIP Model\n    \"\"\"\n\n    def __init__(\n        self,\n        pretrained: str = \"Salesforce/instructblip-vicuna-7b\",\n        device: Optional[str] = \"cuda\",\n        dtype: Optional[Union[str, torch.dtype]] = \"auto\",\n        batch_size: Optional[Union[int, str]] = 1,\n        **kwargs,\n    ) -> None:\n        super().__init__()\n        # Do not use kwargs for now\n        assert kwargs == {}, f\"Unexpected kwargs: {kwargs}\"\n\n        accelerator = Accelerator()\n        if accelerator.num_processes > 1:\n            self._device = torch.device(f\"cuda:{accelerator.local_process_index}\")\n        else:\n            self._device = device\n        self._model = InstructBlipForConditionalGeneration.from_pretrained(pretrained, device_map=self._device)\n        self._image_processor = InstructBlipProcessor.from_pretrained(pretrained)\n        self._tokenizer = self._image_processor.tokenizer\n        self._config = self._model.config\n        self.model.eval()\n        self.model.tie_weights()\n        self.batch_size_per_gpu = int(batch_size)\n        if accelerator.num_processes > 1:\n            assert accelerator.distributed_type in [DistributedType.FSDP, DistributedType.MULTI_GPU, DistributedType.DEEPSPEED], \"Unsupported distributed type provided. Only DDP and FSDP are supported.\"\n            # If you want to use DistributedType.DEEPSPEED, you have to run accelerate config before using the model\n            # Also, you have to select zero stage 0 (equivalent to DDP) in order to make the prepare model works\n            # I tried to set different parameters in the kwargs to let default zero 2 stage works, but it didn't work.\n            if accelerator.distributed_type == DistributedType.DEEPSPEED:\n                kwargs = {\n                    \"train_micro_batch_size_per_gpu\": self.batch_size_per_gpu,\n                    \"train_batch_size\": self.batch_size_per_gpu * accelerator.num_processes,\n                }\n                AcceleratorState().deepspeed_plugin.deepspeed_config_process(must_match=True, **kwargs)\n                eval_logger.info(\"Detected that you are using DistributedType.DEEPSPEED. Make sure you run `accelerate config` and set zero stage to 0\")\n            if accelerator.distributed_type == DistributedType.FSDP or accelerator.distributed_type == DistributedType.DEEPSPEED:\n                self._model = accelerator.prepare(self.model)\n            else:\n                self._model = accelerator.prepare_model(self.model, evaluation_mode=True)\n            self.accelerator = accelerator\n            if self.accelerator.is_local_main_process:\n                eval_logger.info(f\"Using {accelerator.num_processes} devices with data parallelism\")\n            self._rank = self.accelerator.local_process_index\n            self._world_size = self.accelerator.num_processes\n        else:\n            self.model.to(self._device)\n            self._rank = 0\n            self._word_size = 1\n\n    @property\n    def config(self):\n        # return the associated transformers.AutoConfig for the given pretrained model.\n        return self._config\n\n    @property\n    def tokenizer(self):\n        return self._tokenizer\n\n    @property\n    def model(self):\n        # returns the model, unwrapping it if using Accelerate\n        if hasattr(self, \"accelerator\"):\n            return self.accelerator.unwrap_model(self._model)\n        else:\n            return self._model\n\n    @property\n    def eot_token_id(self):\n        # we use EOT because end of *text* is more accurate for what we're doing than end of *sentence*\n        return self.tokenizer.eos_token_id\n\n    @property\n    def max_length(self):\n        return self._max_length\n\n    @property\n    def batch_size(self):\n        return self.batch_size_per_gpu\n\n    @property\n    def device(self):\n        return self._device\n\n    @property\n    def rank(self):\n        return self._rank\n\n    @property\n    def world_size(self):\n        return self._world_size\n\n    def tok_encode(self, string: str, left_truncate_len=None, add_special_tokens=None) -> List[int]:\n        \"\"\" \"\"\"\n        add_special_tokens = False if add_special_tokens is None else add_special_tokens\n        encoding = self.tokenizer.encode(string, add_special_tokens=add_special_tokens)\n        # left-truncate the encoded context to be at most `left_truncate_len` tokens long\n        if left_truncate_len:\n            encoding = encoding[-left_truncate_len:]\n        return encoding\n\n    def tok_decode(self, tokens):\n        return self.tokenizer.decode(tokens)\n\n    def loglikelihood(self, requests: List[Instance]) -> List[Tuple[float, bool]]:\n        # TODO\n        assert False, \"We have not implemented this function for InstructBLIP yet\"\n\n    def flatten(self, input):\n        new_list = []\n        for i in input:\n            for j in i:\n                new_list.append(j)\n        return new_list\n\n    def generate_until(self, requests: List[Instance]) -> List[str]:\n        res = []\n\n        def _collate(x):\n            # the negative sign on len(toks) sorts descending - this has a few advantages:\n            # - time estimates will always be over not underestimates, which is more useful for planning\n            # - to know the size of a batch when going through the list, you know the first one is always the batch\n            #   padded context length. this is useful to simplify the batching logic and more importantly to make\n            #   automatic adaptive batches much much easier to implement\n            # - any OOMs will happen right away rather than near the end\n            toks = self.tok_encode(x[0])\n            return -len(toks), x[0]\n\n        # we group requests by their generation_kwargs,\n        # so that we don't try to execute e.g. greedy sampling and temp=0.8 sampling\n        # in the same batch.\n        re_ords = utils.Collator([reg.args for reg in requests], _collate, grouping=True)\n        chunks = re_ords.get_batched(n=self.batch_size, batch_fn=None)\n        num_iters = len(requests) // self.batch_size if len(requests) % self.batch_size == 0 else len(requests) // self.batch_size + 1\n        pbar = tqdm(total=num_iters, disable=(self.rank != 0), desc=\"Model Responding\")\n        for chunk in chunks:\n            contexts, all_gen_kwargs, doc_to_visual, doc_id, task, split = zip(*chunk)\n            task = task[0]\n            split = split[0]\n            visuals = [doc_to_visual[0](self.task_dict[task][split][ids]) for ids in doc_id]\n            visuals = self.flatten(visuals)\n            # we assume all gen kwargs in the batch are the same\n            # this is safe to assume because the `grouper` object ensures it.\n            gen_kwargs = all_gen_kwargs[0]\n\n            # Set default values for until and max_new_tokens\n            until = [self.tok_decode(self.eot_token_id)]\n\n            # Update values from gen_kwargs if present\n            if \"until\" in gen_kwargs:\n                until = gen_kwargs.pop(\"until\")\n                if isinstance(until, str):\n                    until = [until]\n                elif not isinstance(until, list):\n                    raise ValueError(f\"Expected `gen_kwargs['until']` to be of type Union[str,list] but got {type(until)}\")\n            assert self.batch_size_per_gpu == 1, \"Do not support batch_size_per_gpu > 1 for now\"\n            context = contexts[0]\n            if \"<image>\" in context:\n                # instruct blip does not expect the <image> tag\n                context = context.replace(\"<image>\", \"\")\n            # Set trunction equals true here, the max length for qformer tokenizer is 512\n            # if not truncate, some questions will cause size mismatch\n            # The transformer implementation can't handle multi images for blip\n            # Concat it into one image\n            if len(visuals) > 1:\n                visuals = [process_images(visuals)]\n            inputs = self._image_processor(images=visuals, text=context, return_tensors=\"pt\", truncation=True).to(self.device)\n\n            gen_kwargs[\"image_sizes\"] = [visuals[idx].size for idx in range(len(visuals))]\n            if \"max_new_tokens\" not in gen_kwargs:\n                gen_kwargs[\"max_new_tokens\"] = 1024\n            if \"temperature\" not in gen_kwargs:\n                gen_kwargs[\"temperature\"] = 0\n            if \"top_p\" not in gen_kwargs:\n                gen_kwargs[\"top_p\"] = None\n            if \"num_beams\" not in gen_kwargs:\n                gen_kwargs[\"num_beams\"] = 1\n            try:\n                cont = self.model.generate(\n                    **inputs,\n                    do_sample=True if gen_kwargs[\"temperature\"] > 0 else False,\n                    temperature=gen_kwargs[\"temperature\"],\n                    top_p=gen_kwargs[\"top_p\"],\n                    num_beams=gen_kwargs[\"num_beams\"],\n                    max_new_tokens=gen_kwargs[\"max_new_tokens\"],\n                )\n            except Exception as e:\n                eval_logger.error(f\"Error {e} in generating\")\n                cont = \"\"\n            text_outputs = self.tokenizer.batch_decode(cont, skip_special_tokens=True)[0].strip()\n            res.append(text_outputs)\n            self.cache_hook.add_partial(\"generate_until\", (context, gen_kwargs), text_outputs)\n            pbar.update(1)\n            # reorder this group of results back to original unsorted form\n        res = re_ords.get_original(res)\n\n        pbar.close()\n        return res\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/filters/__init__.py", "content": "from lmms_eval.api.filter import FilterEnsemble, Filter\nfrom . import selection\nfrom . import extraction\nfrom . import transformation\n\n\nFILTER_REGISTRY = {\n    \"take_first\": selection.TakeFirstFilter,\n    \"regex\": extraction.RegexFilter,\n    \"majority_vote\": selection.MajorityVoteFilter,\n    \"take_first_k\": selection.TakeKFilter,\n    \"remove_whitespace\": extraction.WhitespaceFilter,\n    \"lowercase\": transformation.LowercaseFilter,\n    \"uppercase\": transformation.UppercaseFilter,\n    \"map\": transformation.MapFilter,\n    \"multi_choice_regex\": extraction.MultiChoiceRegexFilter,\n    # TODO: implement this filter. either it should take in an arbitrary \"scoring\"/reward function\n    # that takes an input and returns a scalar and then should select the max reward,\n    # or should implement different filters for different ways of handling a reward model's inference.\n    # \"arg_max\": selection.ArgMaxFilter,\n}\n\n\ndef get_filter(filter_name):\n    if filter_name in FILTER_REGISTRY:\n        return FILTER_REGISTRY[filter_name]\n    else:\n        return filter_name\n\n\ndef build_filter_ensemble(filter_name, components):\n    \"\"\"\n    Create a filtering pipeline.\n    \"\"\"\n    filters = []\n    for function, kwargs in components:\n        if kwargs is None:\n            f = get_filter(function)()\n        else:\n            # create a filter given its name in the registry\n            f = get_filter(function)(**kwargs)  # TODO: pass kwargs to filters properly\n        # add the filter as a pipeline step\n        filters.append(f)\n\n    return FilterEnsemble(name=filter_name, filters=filters)\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/api/model.py", "content": "import abc\nimport os\n\nfrom typing import Union, List, Tuple, Optional, Type, TypeVar\nfrom sqlitedict import SqliteDict\nimport json\nimport hashlib\nfrom lmms_eval.api.instance import Instance\nfrom tqdm import tqdm\nfrom lmms_eval import utils\n\n\nfrom loguru import logger as eval_logger\n\nT = TypeVar(\"T\", bound=\"lmms\")\n\n\nclass lmms(abc.ABC):\n    def __init__(self) -> None:\n        \"\"\"Defines the interface that should be implemented by all lmms subclasses.\n        lmmss are assumed to take image-text as input and yield strings as output\n        (inputs/outputs should be tokenization-agnostic.)\n        \"\"\"\n        # set rank and world size to a single process, by default.\n        self._rank = 0\n        self._world_size = 1\n        self.cache_hook = CacheHook(None)\n        self.task_dict = {}\n\n    @abc.abstractmethod\n    def loglikelihood(self, requests: List[Instance]) -> List[Tuple[float, bool]]:\n        \"\"\"Compute log-likelihood of generating a continuation from a context.\n        Downstream tasks should attempt to use loglikelihood instead of other\n        LMM calls whenever possible.\n\n        :param requests: list[Instance]\n            A list of Instance objects, with property `args` which returns a tuple (context, continuation).\n            `context: str`\n                Context string. Implementations of LMM must be able to handle an\n                empty context string.\n            `continuation: str`\n                The continuation over which log likelihood will be calculated. If\n                there is a word boundary, the space should be in the continuation.\n                For example, context=\"hello\" continuation=\" world\" is correct.\n            'visual_list: list[dict]'\n                Visual input to the model. Can be None.\n\n        :return: list[tuple[float, bool]]\n            A list of pairs (logprob, isgreedy)\n            `logprob: float`\n                The log probability of `continuation`.\n            `isgreedy`:\n                Whether `continuation` would be generated by greedy sampling from `context`.\n        \"\"\"\n        pass\n\n    # TODO: Add an optional max length\n    @abc.abstractmethod\n    def generate_until(self, requests) -> List[str]:\n        \"\"\"Generate greedily until a stopping sequence\n\n        :param requests: list[Instance]\n            A list of Instance objects with property `args` which returns a tuple (context, until).\n            context: str\n                Context string\n            generation_kwargs: dict\n                Generation Kwargs\n            'visual_list: list[dict]'\n                Visual input to the model. Can be None.\n        :return: list[str]\n            A list of strings continuation\n            continuation: str\n                The generated continuation.\n        \"\"\"\n        pass\n\n    @classmethod\n    def create_from_arg_string(cls: Type[T], arg_string: str, additional_config: Optional[dict] = None) -> T:\n        \"\"\"\n        Creates an instance of the LMM class using the given argument string and additional config.\n\n        Parameters:\n        - arg_string: A string containing arguments in the format key1=value1,key2=value2.\n        - additional_config: Optional dictionary containing additional configuration parameters.\n\n        Returns:\n        - Instance of the LMM class.\n        \"\"\"\n        additional_config = {} if additional_config is None else additional_config\n        args = utils.simple_parse_args_string(arg_string)\n        args2 = {k: v for k, v in additional_config.items() if v is not None}\n        return cls(**args, **args2)\n\n    @property\n    def rank(self):\n        # used in the case of parallelism. Hardcoded to\n        # ensure no errors arise using API models which do\n        # not support multi-device parallelism nor expect it.\n        return self._rank\n\n    @property\n    def world_size(self):\n        # used in the case of parallelism. Hardcoded to\n        # ensure no errors arise using API models which do\n        # not support multi-device parallelism nor expect it.\n        return self._world_size\n\n    def set_cache_hook(self, cache_hook) -> None:\n        self.cache_hook = cache_hook\n\n\n### SQLite-based caching of LMM responses\ndef hash_args(attr, args):\n    dat = json.dumps([attr] + list(args))\n    return hashlib.sha256(dat.encode(\"utf-8\")).hexdigest()\n\n\nclass CacheHook:\n    def __init__(self, cachinglm) -> None:\n        if cachinglm is None:\n            self.dbdict = None\n            return\n\n        self.dbdict = cachinglm.dbdict\n\n    def add_partial(self, attr, req, res) -> None:\n        if self.dbdict is None:\n            return\n        hsh = hash_args(attr, req)\n        self.dbdict[hsh] = res\n\n\nclass CachingLMM:\n    def __init__(self, lm, cache_db) -> None:\n        \"\"\"LMM wrapper that returns cached results if they exist, and uses the underlying LMM if not.\n\n        :param lm: LMM\n            Underlying LMM\n        :param cache_db: str\n            Path to cache db\n        \"\"\"\n        self.lm = lm\n        self.cache_db = cache_db\n        if os.path.dirname(cache_db):\n            os.makedirs(os.path.dirname(cache_db), exist_ok=True)\n        self.dbdict = SqliteDict(cache_db, autocommit=True)\n\n        # add hook to lm\n        lm.set_cache_hook(self.get_cache_hook())\n\n    def __getattr__(self, attr):\n        lm_attr = getattr(self.lm, attr)\n        if not callable(lm_attr):\n            return lm_attr\n\n        def fn(requests):\n            res = []\n            remaining_reqs = []\n            warned = False\n            # figure out which ones are cached and which ones are new\n            eval_logger.info(f\"Loading '{attr}' responses from cache '{self.cache_db}' where possible...\")\n            for req in tqdm(requests):\n                hsh = hash_args(attr, req.args)\n                if attr == \"generate_until\" and req.args[1].get(\"do_sample\", False):\n                    # when we are doing non-greedy generation, don't use the cache\n                    # (else every \"randomly sampled\" generation would be identical for repeats > 1).\n                    if not warned:\n                        eval_logger.warning(f\"Arguments to lm.generate_until() '{req.args[1]}' include non-deterministic sampling. Caching will not be performed for such requests.\")\n                        warned = True\n                    res.append(None)\n                    remaining_reqs.append(req)\n                elif hsh in self.dbdict:\n                    ob = self.dbdict[hsh]\n\n                    assert ob is not None\n\n                    res.append(ob)\n                else:\n                    res.append(None)\n                    remaining_reqs.append(req)\n\n            # actually run the LMM on the requests that do not have cached results\n            rem_res = getattr(self.lm, attr)(remaining_reqs)\n\n            # stick the new ones back into the list and also cache any of the new ones\n            resptr = 0\n            for req, r in zip(remaining_reqs, rem_res):\n                while res[resptr] is not None:\n                    resptr += 1\n\n                res[resptr] = r\n\n                # caching\n                hsh = hash_args(attr, req.args)\n                self.dbdict[hsh] = r\n            self.dbdict.commit()\n\n            return res\n\n        return fn\n\n    def get_cache_hook(self):\n        return CacheHook(self)\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/models/__init__.py", "content": "import importlib\nimport os\nimport hf_transfer\nfrom loguru import logger\nimport sys\n\nimport hf_transfer\n\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n\nlogger.remove()\nlogger.add(sys.stdout, level=\"WARNING\")\nsys.path.append(f\"{os.environ['TOOLKIT_DIR']}/toolkitmoe\")\n\nAVAILABLE_MODELS = {\n    \"llava\": \"Llava\",\n    # \"qwen_vl\": \"Qwen_VL\",\n    # \"fuyu\": \"Fuyu\",\n    # \"batch_gpt4\": \"BatchGPT4\",\n    # \"gpt4v\": \"GPT4V\",\n    # \"instructblip\": \"InstructBLIP\",\n    # \"minicpm_v\": \"MiniCPM_V\",\n    # \"llava_vid\": \"LlavaVid\",\n    # \"videoChatGPT\": \"VideoChatGPT\",\n    # \"llama_vid\": \"LLaMAVid\",\n    # \"video_llava\": \"VideoLLaVA\",\n    # \"xcomposer2_4KHD\": \"XComposer2_4KHD\",\n    # \"claude\": \"Claude\",\n    # \"qwen_vl_api\": \"Qwen_VL_API\",\n    # \"llava_sglang\": \"LlavaSglang\",\n    # \"idefics2\": \"Idefics2\",\n    # \"internvl\": \"InternVLChat\",\n    # \"internvl2\": \"InternVL2\",\n    # \"gemini_api\": \"GeminiAPI\",\n    # \"reka\": \"Reka\",\n    # \"from_log\": \"FromLog\",\n    # \"mplug_owl_video\": \"mplug_Owl\",\n    # \"phi3v\": \"Phi3v\",\n    # \"tinyllava\": \"TinyLlava\",\n    # \"llava_hf\": \"LlavaHf\",\n    # \"longva\": \"LongVA\",\n    # \"llava_hf\": \"LlavaHf\",\n    # \"longva\": \"LongVA\",\n    # \"vila\": \"VILA\",\n    # \"mantis\": \"Mantis\"\n}\n\nfor model_name, model_class in AVAILABLE_MODELS.items():\n    try:\n        exec(f\"from lmms_eval.models.{model_name} import {model_class}\")\n    except ImportError as e:\n        logger.warning(f\"Failed to import {model_class} from {model_name}: {e}\")\n\nif os.environ.get(\"LMMS_EVAL_PLUGINS\", None):\n    # Allow specifying other packages to import models from\n    for plugin in os.environ[\"LMMS_EVAL_PLUGINS\"].split(\",\"):\n        m = importlib.import_module(f\"{plugin}.models\")\n        for model_name, model_class in getattr(m, \"AVAILABLE_MODELS\").items():\n            try:\n                exec(f\"from {plugin}.models.{model_name} import {model_class}\")\n            except ImportError:\n                logger.warning(f\"Failed to import {model_class} from {model_name}\")\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/models/phi3v.py", "content": "import torch\n\nfrom accelerate import Accelerator, DistributedType\nfrom lmms_eval import utils\nfrom lmms_eval.api.instance import Instance\nfrom lmms_eval.api.model import lmms\nfrom lmms_eval.api.registry import register_model\nfrom tqdm import tqdm\nfrom transformers import AutoModelForCausalLM\nfrom transformers import AutoProcessor\nfrom typing import List, Optional, Tuple, Union\n\nfrom loguru import logger as eval_logger\n\n\n@register_model(\"phi3v\")\nclass Phi3v(lmms):\n    \"\"\"\n    This class implements inference for the microsoft/Phi-3-vision-128k-instruct model.\n    To learn more about this model please visit the following links:\n    1. https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\n    2. https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/\n    3. https://github.com/microsoft/Phi-3CookBook\n\n    NOTE: This class was adapted from quen_vl.py and llava_hf.py.\n\n    Example:\n\n    accelerate launch --num_processes=4 -m lmms_eval --model phi3v --tasks mmmu_val \\\n        --batch_size 1 --log_samples --log_samples_suffix phi3v_mmmu --output_path ./logs/\n    \"\"\"\n\n    def __init__(\n        self,\n        model_id_name: str = \"microsoft/Phi-3-vision-128k-instruct\",\n        device: str = \"cuda\",\n        dtype: Optional[Union[str, torch.dtype]] = \"auto\",\n        batch_size: int = 1,\n        trust_remote_code: Optional[bool] = True,\n        use_cache: bool = True,\n        **kwargs,\n    ) -> None:\n        super().__init__()\n        # Do not use kwargs for now\n        assert kwargs == {}, f\"Unexpected kwargs: {kwargs}\"\n        # Setup accelerator.\n        accelerator = Accelerator()\n        if accelerator.num_processes > 1:\n            self._device = torch.device(f\"cuda:{accelerator.local_process_index}\")\n        else:\n            self._device = device\n        # Load model.\n        self._model = AutoModelForCausalLM.from_pretrained(model_id_name, device_map=device, trust_remote_code=trust_remote_code, torch_dtype=dtype)\n        self._processor = AutoProcessor.from_pretrained(model_id_name, trust_remote_code=trust_remote_code)\n        self._processor.tokenizer.padding_side = \"left\"\n        self._tokenizer = self._processor.tokenizer\n        self._config = self._model.config\n        self.batch_size_per_gpu = int(batch_size)\n        assert self.batch_size_per_gpu == 1, \"batch_size_per_gpu > 1 is not supported for now.\"\n        self.use_cache = use_cache\n        if accelerator.num_processes > 1:\n            distributed_type_list = [DistributedType.FSDP, DistributedType.MULTI_GPU, DistributedType.DEEPSPEED]\n            assert accelerator.distributed_type in distributed_type_list, \"Unsupported distributed type provided. Only DDP and FSDP are supported.\"\n            if accelerator.distributed_type == DistributedType.FSDP:\n                self._model = accelerator.prepare(self.model)\n            else:\n                self._model = accelerator.prepare_model(self.model, evaluation_mode=True)\n            self.accelerator = accelerator\n            if self.accelerator.is_local_main_process:\n                eval_logger.info(f\"Using {accelerator.num_processes} devices with data parallelism\")\n            self._rank = self.accelerator.local_process_index\n            self._world_size = self.accelerator.num_processes\n        else:\n            eval_logger.info(f\"Using single device: {self._device}\")\n            self.model.to(self._device)\n            self._rank = 0\n            self._word_size = 1\n\n    @property\n    def config(self):\n        # return the associated transformers.AutoConfig for the given pretrained model.\n        return self._config\n\n    @property\n    def tokenizer(self):\n        return self._tokenizer\n\n    @property\n    def model(self):\n        # returns the model, unwrapping it if using Accelerate\n        if hasattr(self, \"accelerator\"):\n            return self.accelerator.unwrap_model(self._model)\n        else:\n            return self._model\n\n    @property\n    def eot_token_id(self):\n        # we use EOT because end of *text* is more accurate for what we're doing than end of *sentence*\n        return self.tokenizer.eos_token_id\n\n    @property\n    def max_length(self):\n        return self._max_length\n\n    @property\n    def batch_size(self):\n        return self.batch_size_per_gpu\n\n    @property\n    def device(self):\n        return self._device\n\n    @property\n    def rank(self):\n        return self._rank\n\n    @property\n    def world_size(self):\n        return self._world_size\n\n    def flatten(self, input):\n        new_list = []\n        for i in input:\n            for j in i:\n                new_list.append(j)\n        return new_list\n\n    def loglikelihood(self, requests: List[Instance]) -> List[Tuple[float, bool]]:\n        raise NotImplementedError(\"Not implemented for Phi3v.\")\n\n    def generate_until(self, requests: List[Instance]) -> List[str]:\n        res = []\n\n        def _collate(x):\n            # the negative sign on len(toks) sorts descending - this has a few advantages:\n            # - time estimates will always be over not underestimates, which is more useful for planning\n            # - to know the size of a batch when going through the list, you know the first one is always the batch\n            #   padded context length. this is useful to simplify the batching logic and more importantly to make\n            #   automatic adaptive batches much much easier to implement\n            # - any OOMs will happen right away rather than near the end\n            toks = self.tokenizer.encode(x[0])\n            return -len(toks), x[0]\n\n        pbar = tqdm(total=len(requests), disable=(self.rank != 0), desc=\"Model Responding\")\n        # we group requests by their generation_kwargs,\n        # so that we don't try to execute e.g. greedy sampling and temp=0.8 sampling\n        # in the same batch.\n        re_ords = utils.Collator([reg.args for reg in requests], _collate, grouping=True)\n        chunks = re_ords.get_batched(n=self.batch_size, batch_fn=None)\n        for chunk in chunks:\n            contexts, all_gen_kwargs, doc_to_visual, doc_id, task, split = zip(*chunk)\n            task = task[0]\n            split = split[0]\n            visuals = [doc_to_visual[0](self.task_dict[task][split][ids]) for ids in doc_id]\n            visuals = self.flatten(visuals)\n            # We assume all gen kwargs in the batch are the same\n            # this is safe to assume because the `grouper` object ensures it.\n            gen_kwargs = all_gen_kwargs[0]\n            # Set default values for until and max_new_tokens\n            until = [self.tokenizer.decode(self.eot_token_id)]\n            # Update values from gen_kwargs if present\n            if \"until\" in gen_kwargs:\n                until = gen_kwargs.pop(\"until\")\n                if isinstance(until, str):\n                    until = [until]\n                elif not isinstance(until, list):\n                    raise ValueError(f\"Expected `gen_kwargs['until']` to be of type Union[str,list] but got {type(until)}\")\n            if isinstance(contexts, tuple):\n                contexts = list(contexts)\n            for i in range(len(contexts)):\n                if \"<image>\" in contexts[i]:\n                    query = \"\" + contexts[i]\n                    img_placeholder_count = 1\n                    while \"<image>\" in query:\n                        query = query.replace(\"<image>\", f\"<|image_{img_placeholder_count}|>\", 1)\n                        img_placeholder_count += 1\n                else:\n                    query = \"\"\n                    for placeholder_id in range(len(visuals)):\n                        query += f\"<|image_{placeholder_id+1}|>\\n\"\n                    query += contexts[i]\n                messages = [{\"role\": \"user\", \"content\": query}]\n                contexts[i] = self._tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n            assert len(contexts) == 1\n            #\n            context = contexts[0]\n            input_ids = self._processor(text=context, images=visuals, return_tensors=\"pt\").to(self._device, self.model.dtype)\n            # Setting default parameters.\n            if \"max_new_tokens\" not in gen_kwargs:\n                gen_kwargs[\"max_new_tokens\"] = 1024\n            if \"temperature\" not in gen_kwargs:\n                gen_kwargs[\"temperature\"] = 0\n            if \"top_p\" not in gen_kwargs:\n                gen_kwargs[\"top_p\"] = None\n            if \"num_beams\" not in gen_kwargs:\n                gen_kwargs[\"num_beams\"] = 1\n            # Generate answer.\n            pad_token_id = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eod_id\n            generate_ids = self.model.generate(\n                **input_ids,\n                eos_token_id=self.tokenizer.eos_token_id,\n                pad_token_id=pad_token_id,\n                do_sample=True if gen_kwargs[\"temperature\"] > 0 else False,\n                temperature=gen_kwargs[\"temperature\"],\n                top_p=gen_kwargs[\"top_p\"],\n                num_beams=gen_kwargs[\"num_beams\"],\n                max_new_tokens=gen_kwargs[\"max_new_tokens\"],\n                use_cache=self.use_cache,\n            )\n            generate_ids = generate_ids[:, input_ids[\"input_ids\"].shape[1] :]\n            response = self._processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n            res.append(response)\n            self.cache_hook.add_partial(\"generate_until\", (context, gen_kwargs), response)\n            pbar.update(1)\n        # reorder this group of results back to original unsorted form\n        res = re_ords.get_original(res)\n        pbar.close()\n        return res\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/api/metrics.py", "content": "import math\nfrom collections.abc import Iterable\n\nimport numpy as np\nimport sacrebleu\nimport sklearn.metrics\nimport random\nimport evaluate\nimport torch\n\nfrom lmms_eval.api.registry import register_metric, register_aggregation\nfrom loguru import logger as eval_logger\n\n\n# Register Aggregations First\n@register_aggregation(\"bypass\")\ndef bypass_agg(arr):\n    return 999\n\n\n@register_aggregation(\"mean\")\ndef mean(arr):\n    return sum(arr) / len(arr)\n\n\n@register_aggregation(\"median\")\ndef median(arr):\n    return arr[len(arr) // 2]\n\n\n# Certain metrics must be calculated across all documents in a benchmark.\n# We use them as aggregation metrics, paired with no-op passthrough metric fns.\n@register_aggregation(\"perplexity\")\ndef perplexity(items):\n    # return math.exp(-mean(items))\n    items = torch.exp(torch.tensor(items)).tolist()\n    return sum(items) / len(items)\n\n\n@register_aggregation(\"weighted_perplexity\")\ndef weighted_perplexity(items):\n    return math.exp(-weighted_mean(items))\n\n\n@register_aggregation(\"bits_per_byte\")\ndef bits_per_byte(items):\n    return -weighted_mean(items) / math.log(2)\n\n\n@register_aggregation(\"f1\")\ndef f1_score(items):\n    unzipped_list = list(zip(*items))\n    golds = unzipped_list[0]\n    preds = unzipped_list[1]\n    fscore = sklearn.metrics.f1_score(golds, preds)\n\n    return np.max(fscore)\n\n\n@register_aggregation(\"matthews_corrcoef\")\ndef matthews_corrcoef(items):\n    unzipped_list = list(zip(*items))\n    golds = unzipped_list[0]\n    preds = unzipped_list[1]\n    # print(preds)\n    return sklearn.metrics.matthews_corrcoef(golds, preds)\n\n\n@register_aggregation(\"bleu\")\ndef bleu(items):\n    \"\"\"The Bilingual Evaluation Understudy Score, or BLEU for short, is a metric\n    for evaluating a generated sentence to a reference sentence. It counts matching\n    n-grams in the candidate translation to n-grams in the reference text, where\n    1-gram or unigram would be each token and a bigram comparison would be each\n    word pair. The comparison is made regardless of word order\n    Source: https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n    Paper: https://www.aclweb.org/anthology/P02-1040/\n\n    Higher is better\n    \"\"\"\n    refs = list(zip(*items))[0]\n    preds = list(zip(*items))[1]\n    refs, preds = _sacreformat(refs, preds)\n    return sacrebleu.corpus_bleu(preds, refs).score\n\n\n@register_aggregation(\"chrf\")\ndef chrf(items):\n    \"\"\"chrF++ is a tool for automatic evaluation of machine translation output\n    based on character n-gram precision and recall enhanced with word n-grams.\n    Source: https://github.com/m-popovic/chrF\n    Paper: https://www.aclweb.org/anthology/W15-3049.pdf\n\n    Higher is better  # TODO I think\n    \"\"\"\n    refs = list(zip(*items))[0]\n    preds = list(zip(*items))[1]\n    refs, preds = _sacreformat(refs, preds)\n    return sacrebleu.corpus_chrf(preds, refs).score\n\n\n@register_aggregation(\"ter\")\ndef ter(items):\n    \"\"\"Translation Error Rate is an error metric for machine translation that\n    measures the number of edits required to change a system output into one\n    of the references\n    Source: http://www.cs.umd.edu/~snover/tercom/\n    Paper: http://mt-archive.info/AMTA-2006-Snover.pdf\n\n    Lower is better\n    \"\"\"\n    refs = list(zip(*items))[0]\n    preds = list(zip(*items))[1]\n    refs, preds = _sacreformat(refs, preds)\n    return sacrebleu.corpus_ter(preds, refs).score\n\n\n@register_metric(\n    metric=\"acc\",\n    higher_is_better=True,\n    output_type=[\"loglikelihood\", \"multiple_choice\"],\n    aggregation=\"mean\",\n)\ndef acc_fn(items):  # This is a passthrough function\n    return items\n\n\n@register_metric(\n    metric=\"acc_norm\",\n    higher_is_better=True,\n    output_type=[\"loglikelihood\", \"multiple_choice\"],\n    aggregation=\"mean\",\n)\ndef acc_norm_fn(items):  # This is a passthrough function\n    return items\n\n\n@register_metric(\n    metric=\"acc_mutual_info\",\n    higher_is_better=True,\n    output_type=\"multiple_choice\",\n    aggregation=\"mean\",\n)\ndef acc_mutual_info_fn(items):  # This is a passthrough function\n    return items\n\n\nexact_match = evaluate.load(\"exact_match\")\n\n\n@register_metric(\n    metric=\"exact_match\",\n    higher_is_better=True,\n    output_type=\"generate_until\",\n    aggregation=\"mean\",\n)\ndef exact_match_fn(**kwargs):\n    return exact_match.compute(**kwargs)\n\n\n@register_metric(\n    metric=\"perplexity\",\n    higher_is_better=False,\n    output_type=\"loglikelihood\",\n    aggregation=\"perplexity\",\n)\ndef perplexity_fn(items):  # This is a passthrough function\n    return items\n\n\ndef levenshtein_distance(s1, s2):\n    if len(s1) > len(s2):\n        s1, s2 = s2, s1\n\n    distances = range(len(s1) + 1)\n    for i2, c2 in enumerate(s2):\n        distances_ = [i2 + 1]\n        for i1, c1 in enumerate(s1):\n            if c1 == c2:\n                distances_.append(distances[i1])\n            else:\n                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n        distances = distances_\n    return distances[-1]\n\n\n@register_metric(\n    metric=\"anls\",\n    higher_is_better=True,\n    output_type=\"generate_until\",\n    aggregation=\"mean\",\n)\ndef anls(\n    references,\n    predictions,\n    thresh_hold=0.5,\n):  # This is a passthrough function\n    \"\"\"https://github.com/QwenLM/Qwen-VL/blob/master/eval_mm/infographicsvqa_eval.py\"\"\"\n    values = []\n    for answer in references:\n        # preprocess both the answers - gt and prediction\n        gt_answer = \" \".join(answer.strip().lower().split())\n        det_answer = \" \".join(predictions[0].strip().lower().split())\n\n        # dist = levenshtein_distance(answer.lower(), detObject['answer'].lower())\n        dist = levenshtein_distance(gt_answer, det_answer)\n        length = max(len(answer.upper()), len(predictions[0].upper()))\n        values.append(0.0 if length == 0 else float(dist) / float(length))\n\n    question_result = 1 - min(values)\n\n    if question_result < thresh_hold:\n        question_result = 0\n    return {\"anls\": question_result}\n\n\ndef pop_stddev(arr):\n    mu = mean(arr)\n    return math.sqrt(sum([(x - mu) ** 2 for x in arr]) / len(arr))\n\n\ndef sample_stddev(arr):\n    mu = mean(arr)\n    return math.sqrt(sum([(x - mu) ** 2 for x in arr]) / (len(arr) - 1))\n\n\ndef mean_stderr(arr):\n    return sample_stddev(arr) / math.sqrt(len(arr))\n\n\n@register_metric(\n    metric=\"bypass\",\n    higher_is_better=True,\n    output_type=[\"loglikelihood\", \"multiple_choice\", \"generate_until\"],\n    aggregation=\"bypass\",\n)\ndef bypass(items):\n    return items\n\n\n@register_metric(\n    metric=\"mcc\",\n    higher_is_better=True,\n    output_type=\"multiple_choice\",\n    aggregation=\"matthews_corrcoef\",\n)\ndef mcc_fn(items):  # This is a passthrough function\n    return items\n\n\n@register_metric(\n    metric=\"f1\",\n    higher_is_better=True,\n    output_type=\"multiple_choice\",\n    aggregation=\"f1\",\n)\ndef f1_fn(items):  # This is a passthrough function\n    return items\n\n\n@register_metric(\n    metric=\"bleu\",\n    higher_is_better=True,\n    output_type=\"generate_until\",\n    aggregation=\"bleu\",\n)\ndef bleu_fn(items):  # This is a passthrough function\n    return items\n\n\n@register_metric(\n    metric=\"chrf\",\n    higher_is_better=True,\n    output_type=\"generate_until\",\n    aggregation=\"chrf\",\n)\ndef chrf_fn(items):  # This is a passthrough function\n    return items\n\n\n@register_metric(\n    metric=\"ter\",\n    higher_is_better=True,\n    output_type=\"generate_until\",\n    aggregation=\"ter\",\n)\ndef ter_fn(items):  # This is a passthrough function\n    return items\n\n\n@register_metric(\n    metric=\"acc_all\",\n    higher_is_better=True,\n    output_type=\"loglikelihood\",\n    aggregation=\"mean\",\n)\ndef acc_all(items):\n    # Only count as correct if all answers are labeled correctly for each question\n    question_scoring_dict = {}\n    preds = list(zip(*items))[0]\n    docs = list(zip(*items))[1]\n\n    for doc, pred in zip(docs, preds):\n        paragraph_id = doc[\"idx\"][\"paragraph\"]\n        question_id = doc[\"idx\"][\"question\"]\n        if (paragraph_id, question_id) not in question_scoring_dict:\n            question_scoring_dict[(paragraph_id, question_id)] = []\n\n        gold_label = doc[\"label\"] == 1\n\n        question_scoring_dict[(paragraph_id, question_id)].append(gold_label == pred)\n    acc = np.mean([int(all(x)) for x in question_scoring_dict.values()])\n    return acc\n\n\ndef acc_all_stderr(items):\n    # Only count as correct if all answers are labeled correctly for each question\n    question_scoring_dict = {}\n    preds = list(zip(*items))[0]\n    docs = list(zip(*items))[1]\n\n    for doc, pred in zip(docs, preds):\n        question_id = doc[\"idx\"][\"question\"]\n        if question_id not in question_scoring_dict:\n            question_scoring_dict[question_id] = []\n\n        gold_label = doc[\"label\"] == 1\n        question_scoring_dict[question_id].append(gold_label == pred)\n\n    acc = mean_stderr([int(all(x)) for x in question_scoring_dict.values()])\n    return acc\n\n\ndef metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n    \"\"\"Compute max metric between prediction and each ground truth.\"\"\"\n    scores_for_ground_truths = []\n    for ground_truth in ground_truths:\n        score = metric_fn(prediction, ground_truth)\n        scores_for_ground_truths.append(score)\n    return max(scores_for_ground_truths)\n\n\ndef weighted_mean(items):\n    a, b = zip(*items)\n    return sum(a) / sum(b)\n\n\ndef is_non_str_iterable(obj):\n    return isinstance(obj, Iterable) and not isinstance(obj, str)\n\n\ndef _sacreformat(refs, preds):\n    \"\"\"Format refs and preds for sacrebleu corpus calculation. It is very particular\"\"\"\n    # Sacrebleu expects (List[str], List[List[str])\n    #   e.g. sacrebleu.corpus_bleu([pred_t], [[ref1_stream], [ref2_stream], ...])\n\n    # Note [ref1_stream] is the first reference for each pred.\n    # So lists are size N and (M, N) for N preds and M possible refs for each pred\n    # This is a different order of dimensions that I would expect\n\n    # We expect refs to be List[str] or List[List[str]], the outer list corresponding to preds\n    # Must become List[List[str]] with the inner list corresponding to preds\n    if not is_non_str_iterable(refs):\n        refs = list(refs)\n    if not is_non_str_iterable(refs[0]):\n        refs = [[ref] for ref in refs]\n    refs = list(zip(*refs))\n    # Note the number of refs in each ref list much match the number of preds\n\n    # We expect preds to be List[str] or List[List[str]]. Must become List[str]\n    if not is_non_str_iterable(preds):\n        preds = list(preds)\n    if is_non_str_iterable(preds[0]):\n        assert len(preds[0]) == 1, f\"Pred must be a str, was {preds[0]}\"\n        preds = [pred[0] for pred in preds]\n\n    return refs, preds\n\n\n# stderr stuff\n\n\nclass _bootstrap_internal:\n    def __init__(self, f, n) -> None:\n        self.f = f\n        self.n = n\n\n    def __call__(self, v):\n        i, xs = v\n        rnd = random.Random()\n        rnd.seed(i)\n        res = []\n        for _ in range(self.n):\n            res.append(self.f(rnd.choices(xs, k=len(xs))))\n        return res\n\n\ndef bootstrap_stderr(f, xs, iters):\n    import multiprocessing as mp\n\n    pool = mp.Pool(mp.cpu_count())\n    # this gives a biased estimate of the stderr (i.e w/ the mean, it gives something\n    # equivalent to stderr calculated without Bessel's correction in the stddev.\n    # Unfortunately, I haven't been able to figure out what the right correction is\n    # to make the bootstrap unbiased - i considered multiplying by sqrt(n/(n-1)) but\n    # that would be ad-hoc and I can't prove that that would actually be an unbiased estimator)\n    # Thankfully, shouldn't matter because our samples are pretty big usually anyways\n    res = []\n    chunk_size = min(1000, iters)\n    from tqdm import tqdm\n\n    print(\"bootstrapping for stddev:\", f.__name__)\n    for bootstrap in tqdm(\n        pool.imap(\n            _bootstrap_internal(f, chunk_size),\n            [(i, xs) for i in range(iters // chunk_size)],\n        ),\n        total=iters // chunk_size,\n    ):\n        # sample w replacement\n        res.extend(bootstrap)\n\n    pool.close()\n    return sample_stddev(res)\n\n\ndef stderr_for_metric(metric, bootstrap_iters):\n    bootstrappable = [\n        median,\n        matthews_corrcoef,\n        f1_score,\n        perplexity,\n        bleu,\n        chrf,\n        ter,\n    ]\n\n    if metric in bootstrappable:\n        return lambda x: bootstrap_stderr(metric, x, iters=bootstrap_iters)\n\n    stderr = {mean: mean_stderr, acc_all: acc_all_stderr}\n\n    return stderr.get(metric, None)\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/models/video_chatgpt.py", "content": "import os\nfrom lmms_eval.api.instance import Instance\nfrom lmms_eval.api.model import lmms\nfrom lmms_eval.api.registry import register_model\n\nfrom accelerate import Accelerator, DistributedType, InitProcessGroupKwargs\nfrom accelerate.state import AcceleratorState\nfrom huggingface_hub import snapshot_download\nimport torch\nfrom PIL import Image\n\nfrom datetime import timedelta\nfrom typing import List, Tuple, Optional, Union\nfrom tqdm import tqdm\n\nfrom loguru import logger\n\neval_logger = logger\n\ntry:\n    from lmms_eval.models.video_chatgpt.eval.model_utils import load_video, initialize_model\n    from lmms_eval.models.video_chatgpt.inference import video_chatgpt_infer, video_chatgpt_infer_ppl, get_spatio_temporal_features_torch\nexcept ImportError:\n    eval_logger.warning(\"Failed to import video_chatgpt modules\")\n\nfrom lmms_eval.models.model_utils.load_video import read_video_pyav\n\n\n@register_model(\"video_chatgpt\")\nclass VideoChatGPT(lmms):\n    def __init__(\n        self,\n        batch_size: Optional[Union[int, str]] = 1,\n        projection_path: str = \"MBZUAI/Video-ChatGPT-7B\",\n        model_path: str = \"mmaaz60/LLaVA-7B-Lightening-v1-1\",\n        device_map=\"cuda:0\",\n        device: Optional[str] = \"cuda:0\",\n        num_frm: Optional[Union[int, str]] = 100,\n    ) -> None:\n        super().__init__()\n        self.batch_size_per_gpu = int(batch_size)\n        self.num_frm = int(num_frm)\n        accelerator_kwargs = InitProcessGroupKwargs(timeout=timedelta(weeks=52))\n        accelerator = Accelerator(kwargs_handlers=[accelerator_kwargs])\n        if accelerator.num_processes > 1:\n            self._device = torch.device(f\"cuda:{accelerator.local_process_index}\")\n            self.device_map = f\"cuda:{accelerator.local_process_index}\"\n        elif accelerator.num_processes == 1 and device_map == \"auto\":\n            self._device = torch.device(device)\n            self.device_map = device_map\n        else:\n            self._device = torch.device(f\"cuda:{accelerator.local_process_index}\")\n            self.device_map = f\"cuda:{accelerator.local_process_index}\"\n        try:\n            self.model, self.vision_tower, self.tokenizer, self.image_processor, self.video_token_len = initialize_model(model_path, projection_path, device=self.device)\n        except:\n            eval_logger.info(\"Does not find the model from the path you provide, try downloading from the hf repo.\")\n            model_path = snapshot_download(repo_id=model_path)\n            projection_path = os.path.join(snapshot_download(repo_id=projection_path), \"video_chatgpt-7B.bin\")\n            self.model, self.vision_tower, self.tokenizer, self.image_processor, self.video_token_len = initialize_model(model_path, projection_path, device=self.device)\n\n        if accelerator.num_processes > 1:\n            assert accelerator.distributed_type in [DistributedType.FSDP, DistributedType.MULTI_GPU, DistributedType.DEEPSPEED], \"Unsupported distributed type provided. Only DDP and FSDP are supported.\"\n            # If you want to use DistributedType.DEEPSPEED, you have to run accelerate config before using the model\n            # Also, you have to select zero stage 0 (equivalent to DDP) in order to make the prepare model works\n            # I tried to set different parameters in the kwargs to let default zero 2 stage works, but it didn't work.\n            if accelerator.distributed_type == DistributedType.DEEPSPEED:\n                kwargs = {\n                    \"train_micro_batch_size_per_gpu\": self.batch_size_per_gpu,\n                    \"train_batch_size\": self.batch_size_per_gpu * accelerator.num_processes,\n                }\n                AcceleratorState().deepspeed_plugin.deepspeed_config_process(must_match=True, **kwargs)\n                eval_logger.info(\"Detected that you are using DistributedType.DEEPSPEED. Make sure you run `accelerate config` and set zero stage to 0\")\n            if accelerator.distributed_type == DistributedType.FSDP or accelerator.distributed_type == DistributedType.DEEPSPEED:\n                self._model = accelerator.prepare(self.model)\n            else:\n                self._model = accelerator.prepare_model(self.model, evaluation_mode=True)\n            self.accelerator = accelerator\n            if self.accelerator.is_local_main_process:\n                eval_logger.info(f\"Using {accelerator.num_processes} devices with data parallelism\")\n            self._rank = self.accelerator.local_process_index\n            self._world_size = self.accelerator.num_processes\n        elif accelerator.num_processes == 1 and device_map == \"auto\":\n            eval_logger.info(f\"Using {accelerator.num_processes} devices with tensor parallelism\")\n            self._rank = 0\n            self._word_size = 1\n        else:\n            eval_logger.info(f\"Using single device: {self._device}\")\n            self.model.to(self._device)\n            self._rank = 0\n            self._world_size = 1\n\n    def flatten(self, input):\n        new_list = []\n        for i in input:\n            for j in i:\n                new_list.append(j)\n        return new_list\n\n    def generate_until(self, requests) -> List[str]:\n        res = []\n        pbar = tqdm(total=len(requests), disable=(self.rank != 0), desc=\"Model Responding\")\n\n        for contexts, gen_kwargs, doc_to_visual, doc_id, task, split in [reg.args for reg in requests]:\n            # encode, pad, and truncate contexts for this batch\n            visuals = [doc_to_visual(self.task_dict[task][split][doc_id])]\n            visuals = self.flatten(visuals)\n            # videos = []\n            for visual in visuals:\n                video_frames = read_video_pyav(visual, num_frm=self.num_frm)\n                target_h, target_w = 224, 224\n                # If image shape is not as target, resize it\n                if video_frames.shape[-3] != target_h or video_frames.shape[-2] != target_w:\n                    video_frames = torch.from_numpy(video_frames).permute(0, 3, 1, 2).float()\n                    video_frames = torch.nn.functional.interpolate(video_frames, size=(target_h, target_w))\n                    video_frames = video_frames.permute(0, 2, 3, 1).to(torch.uint8).numpy()\n                video_frames = [Image.fromarray(frame) for frame in video_frames]\n                if len(video_frames) > self.num_frm:\n                    video_frames = video_frames[: self.num_frm]\n                # VideoChatGPT load video return a list of PIL Image\n                # videos += video_frames\n\n            output = video_chatgpt_infer(\n                video_frames, contexts, conv_mode=\"video-chatgpt_v1\", model=self.model, vision_tower=self.vision_tower, tokenizer=self.tokenizer, image_processor=self.image_processor, video_token_len=self.video_token_len\n            )\n\n            res.append(output)\n            pbar.update(1)\n\n        return res\n\n    def loglikelihood(self, requests: List[Instance]) -> List[Tuple[float, bool]]:\n        res = []\n        pbar = tqdm(total=len(requests), disable=(self.rank != 0), desc=\"Model Responding\")\n\n        for contexts, doc_to_target, doc_to_visual, doc_id, task, split in [reg.args for reg in requests]:\n            # encode, pad, and truncate contexts for this batch\n            if type(doc_to_target) == str:\n                continuation = doc_to_target\n            else:\n                continuation = doc_to_target(self.task_dict[task][split][doc_id])\n            visuals = [doc_to_visual(self.task_dict[task][split][doc_id])]\n            visuals = self.flatten(visuals)\n            videos = []\n            for visual in visuals:\n                video_frames = load_video(visual, num_frm=self.num_frm)\n                # VideoChatGPT load video return a list of PIL Image\n                videos += video_frames\n            image_tensor = self.image_processor.preprocess(videos, return_tensors=\"pt\")[\"pixel_values\"]\n\n            # Move image tensor to GPU and reduce precision to half\n            image_tensor = image_tensor.half().to(self.device)\n\n            # Generate video spatio-temporal features\n            with torch.no_grad():\n                image_forward_outs = self.vision_tower(image_tensor, output_hidden_states=True)\n                frame_features = image_forward_outs.hidden_states[-2][:, 1:]  # Use second to last layer as in LLaVA\n            video_spatio_temporal_features = get_spatio_temporal_features_torch(frame_features).cuda()\n\n            outputs, input_ids, context_ids = video_chatgpt_infer_ppl(\n                # video_frames,\n                contexts,\n                continuation,\n                conv_mode=\"video-chatgpt_v1\",\n                model=self.model,\n                vision_tower=self.vision_tower,\n                tokenizer=self.tokenizer,\n                image_processor=self.image_processor,\n                video_token_len=self.video_token_len,\n                video_spatio_temporal_features=video_spatio_temporal_features,\n            )\n\n            loss = outputs[\"loss\"]\n            # loss = torch.exp(loss)\n            logits = outputs[\"logits\"]\n            greedy_tokens = logits.argmax(dim=-1)\n            cont_toks = input_ids[:, context_ids.shape[1] :]  # [1, seq]\n            greedy_tokens = greedy_tokens[:, context_ids.shape[1] : input_ids.shape[1]]  # [1, seq]\n            max_equal = (greedy_tokens == cont_toks).all()\n            res.append((float(loss.item()), bool(max_equal)))\n            pbar.update(1)\n        pbar.close()\n        return res\n\n    @property\n    def batch_size(self):\n        return self.batch_size_per_gpu\n\n    @property\n    def device(self):\n        return self._device\n\n    @property\n    def rank(self):\n        return self._rank\n\n    @property\n    def world_size(self):\n        return self._world_size\n"}
{"type": "source_file", "path": "evaluate/lmms_eval/models/model_utils/__init__.py", "content": ""}
